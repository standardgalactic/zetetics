In the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
Data-Free Model Extraction
Jean-Baptiste Truong*
Worcester Polytechnic Institute
jtruong2@wpi.edu
Pratyush Maini*†
Indian Institute of Technology Delhi
pratyush.maini@gmail.com
Robert J. Walls
Worcester Polytechnic Institute
rjwalls@wpi.edu
Nicolas Papernot
University of Toronto and Vector Institute
nicolas.papernot@utoronto.ca
Abstract
Current model extraction attacks assume that the adver-
sary has access to a surrogate dataset with characteris-
tics similar to the proprietary data used to train the vic-
tim model. This requirement precludes the use of existing
model extraction techniques on valuable models, such as
those trained on rare or hard to acquire datasets. In con-
trast, we propose data-free model extraction methods that
do not require a surrogate dataset. Our approach adapts
techniques from the area of data-free knowledge transfer
for model extraction. As part of our study, we identify that
the choice of loss is critical to ensuring that the extracted
model is an accurate replica of the victim model. Further-
more, we address difﬁculties arising from the adversary’s
limited access to the victim model in a black-box setting.
For example, we recover the model’s logits from its prob-
ability predictions to approximate gradients. We ﬁnd that
the proposed data-free model extraction approach achieves
high-accuracy with reasonable query complexity – 0.99×
and 0.92× the victim model accuracy on SVHN and CIFAR-
10 datasets given 2M and 20M queries respectively.
1. Introduction
Machine learning (ML) and deep learning, in particu-
lar, often require large amounts of training data to achieve
high performance on a particular task [39].
Curating
such data necessitates signiﬁcant time and monetary invest-
ment [17, 12]. Thus, the resulting ML model becomes valu-
able intellectual property, especially when considering the
computing resources and human expertise required [5, 13].
Often to monetize these models, companies make them
available as a service via APIs over the web (MLaaS). These
models are also deployed to end-user devices, making their
*equal contribution
†Work done while an intern at the Vector Institute.
predictions directly accessible to customers. However, the
exposure of the model’s predictions represents a signiﬁcant
risk as an adversary can leverage this information to steal
the model’s knowledge [26, 41, 7, 32, 31, 11, 28, 19]. The
threat of such model extraction attacks is two-fold: adver-
saries may use the stolen model for monetary gains or as a
reconnaissance step to mount further attacks [33, 37].
While model extraction is in many ways similar to model
distillation, it differs in that the victim’s proprietary train-
ing set is not accessible to the adversary. To stage a model
extraction attack, the adversary typically queries the victim
using samples from a surrogate dataset with semantic or dis-
tributional similarity to the original training set [31]. In the
classiﬁcation setting, the victim’s response may be limited
to the most-likely label [6] or include conﬁdence values for
different class labels [19]. The number of queries—i.e., the
query complexity—is also an important consideration for
the adversary. The greater the query complexity, the higher
the cost of the attack—unless the victim model is available
ofﬂine (e.g., deployed on-device).
In this work, we ﬁrst demonstrate in Section 3 that the
success of current established practices for model extrac-
tion, which often take the form of distillation, depends on
the closeness of the surrogate distribution to the victim’s
proprietary training distribution. This ﬁnding has important
implications for the practicality of existing model extraction
techniques.
To remedy this, we propose techniques for data-free
model extraction (DFME). In short, we demonstrate the fea-
sibility of extracting ML models without any knowledge of
the distribution of the proprietary training data. In practice,
gathering a surrogate dataset for the purpose of model ex-
traction can be a very expensive process, both in terms of
the time and money required to curate it. In particular, the
most valuable models are often those for which it is most
challenging to curate an appropriate surrogate dataset, i.e.,
when the victim model’s value arises from its proprietary
1
arXiv:2011.14779v2  [cs.LG]  31 Mar 2021

dataset. Our work builds on recent advances in data-free
knowledge distillation, which involve a generative model to
synthesize queries that maximize disagreement between the
student and teacher models [27, 14]. Here, the teacher is
the victim model whereas the student is the stolen extracted
model. We innovate on two fronts: the choice of loss to
quantify student-teacher disagreement and an approach for
training the generator without the ability to backpropagate
through the teacher to compute its gradients (because we
only have black-box access to the victim/teacher predictions
in our setting). We observe that it is essential to ensure the
stability of the loss computed, and ﬁnd that the ℓ1 norm loss
is particularly conducive to data-free model extraction. We
also demonstrate that using inexpensive gradient approxi-
mation (based on the victim model’s outputs) is sufﬁcient
to train a generative model that produces queries relevant
to distill the knowledge of a victim to a student model. In
summary, our main contributions are:
• We
demonstrate
in
Section
3
that
successful
distillation-based model extraction attacks require the
adversary to sample queries from a surrogate dataset
whose distribution is close to the victim training data.
• In Section 4, we propose data-free model extraction
(DFME) to extract ML models without knowledge of
private training data, and only using the victim’s black-
box predictions. As a by-product of DFME needing
to approximate gradients of the victim, this leads us to
present a method for recovering per-example logits out
of the probability vector output by a ML model.
• We validate1 our DFME technique in Section 5 on the
SVHN and CIFAR10 datasets and successfully extract
a model with 0.99x the victim accuracy with only 2M
queries for SVHN, and 0.92x the victim accuracy with
20M queries for CIFAR10.
• An ablation study of our approach in Section 6 pro-
vides two key insights: (1) measuring disagreement
between the victim and extracted models with the ℓ1
norm achieves higher extraction accuracy than losses
previously considered in the literature; (2) weak gradi-
ent estimates yield sufﬁcient signal to train a generator
despite only having access to the victim’s predictions.
2. Related Work
We covered the seminal results in model extraction based
on surrogate datasets in the introduction. Here, we discuss
data-free knowledge distillation—the technique that under-
lies our approach to data-free model extraction—as well as
the rudiments of generative modeling and gradient approx-
imation required to understand our method.
1Code and models for reproducing our work can be found at
https://github.com/cake-lab/datafree-model-extraction
2.1. Data-Free Knowledge Distillation
Knowledge distillation aims to compress, i.e., transfer,
the knowledge of a (larger) teacher model to a (smaller) stu-
dent model [3, 18]. It was originally introduced to reduce
the size of models deployed on devices with limited compu-
tational resources. Since then, this line of work has attracted
a lot of attention [47, 16, 34, 45, 46]. While the model
owner usually performs knowledge distillation, the original
dataset used to train the teacher model may not be available
during distillation [27], e.g., because the dataset is too large
or conﬁdential. Therefore, others have proposed distilla-
tion techniques that leverage a surrogate dataset with a sim-
ilar feature space or distribution [25, 31]. Others proposed
techniques that altogether remove the need for a surrogate
dataset, i.e., data-free knowledge distillation [14, 27]. Tech-
niques addressing data-free knowledge distillation have re-
lied on training a generative model to synthesize the queries
that the student makes to the teacher [10, 27].
The success of data-free knowledge distillation hints at
the feasibility of data-free model extraction. Kariyappa et
al. observe this as well in concurrent work [20]. They also
tackle data-free model extraction through the synthesis of
queries by a generative model. Key differences include our
loss formulation and optimizer choice (see Section 4). We
show in Sections 5 and 6 that our approach consistently out-
performs theirs.
2.2. Generative Models
Model extraction through data-free distillation involves
the generation of training data with which the student (i.e.,
adversary) queries the teacher (i.e., victim) model. Naively,
one could generate these queries randomly [27, 14].
In
this paper, we instead build on a min-max game between
two adversaries that try to optimize opposite loss functions.
This approach is analogous to the optimization performed
in Generative Adversarial Networks (GANs) [15] to train
the generator and discriminator. Here, we use GANs in a
fashion analogous to their application to semi-supervised
learning [36]: our student and teacher models, in conjunc-
tion, play the discriminator’s role. The key difference here
is that GANs are generally trained to recover an underly-
ing ﬁxed data distribution. However, our generator chases
a moving target: the distribution of data which is most in-
dicative of the discrepancies between the decision surfaces
of the current student model and its teacher model.
2.3. Black-box Gradient Approximation
Zeroth-order optimization is a common approach to ap-
proximating gradients [43, 29, 8, 24]. Such techniques have
previously been used to mount attacks against ML mod-
els in a black-box setting, e.g., to craft adversarial exam-
ples [42, 8, 4]. Various gradient estimation methods solve
2

0.0
0.2
0.4
0.6
0.8
1.0
Interpolation coefficient 
40
60
80
100
Test accuracy (%)      
MNIST surrogate
SVHN surrogate
Figure 1. Dataset Interpolation with CIFAR10 as target. λ = 0
implies that the inputs are sampled from the target distribution,
while λ = 1 implies sampling from the surrogate.
different trade-offs between query complexity and the qual-
ity of the gradient estimate [42, 8, 4]. We use the forward
differences [44] method for its relatively low query utiliza-
tion, and systematically study the impact of its main param-
eter (e.g. the number of random directions) in Section 4.3.
3. How Hard is it to Find a Surrogate Dataset?
To motivate the need for data-free approaches to model
extraction, we evaluate if an adversary must ensure that the
distribution of its surrogate dataset is close to that of the
victim’s training dataset. We hypothesize that in the ab-
sence of this condition, distillation-based model extraction
will return a poor approximation of the victim. We perform
an analysis on the closeness of the distributions along three
axes: (1) similarity in feature space, (2) marginal proba-
bility distribution of inputs, and (3) class-conditional prob-
ability distribution of the inputs. In our experiments, we
attempt to steal ML models trained on CIFAR10 [21] and
SVHN [30] using various surrogate datasets that align dif-
ferently with the axes deﬁned above. We study in details
the experimental setting, optimization problem, surrogate
datasets and hyperparameters in Appendix A.
Our experiments support our hypothesis. For instance, in
case of CIFAR10, with a victim model of accuracy 95.5%,
extracting it using CIFAR100 [21] as surrogate dataset re-
sults in extraction accuracy of 93.5%. This can be largely
attributed to the fact that both the CIFAR10 and CIFAR100
datasets are subsets from the same TinyImages [40] dataset.
However, on using SVHN as surrogate dataset, the model
extraction performance dropped remarkably, attaining a
maximum of 66.6% across all the hyperparameters tried. In
the extreme scenario when querying the CIFAR10 teacher
with MNIST [23]–a dataset with disjoint feature space both
in terms of number of pixels, and number of channels)—
accuracy did not improve beyond 37.2%.
On the contrary, we notice that the victim trained on
the SVHN dataset is much easier for the adversary to ex-
tract. Surprisingly, even when the victim is queried with
completely random inputs, the extracted model attains an
accuracy of over 84% on the original SVHN test set. We
hypothesize that this observation is linked to how the digit
classiﬁcation task, at the root of SVHN, is a simpler task
for neural networks to solve, and the underlying representa-
tions (hence, not being as complex as for CIFAR10) can be
learnt even when queried over random inputs.
While these correlations agree with our hypothesis, these
experiments can not systematically quantify the distance
between two distributions (viz. the surrogate and the tar-
get). To more systematically understand how the shift away
from the target distribution affects extraction performance
we interpolated inputs (xin) from the surrogate (xs) and tar-
get (xt) datasets, s.t. xin = (1 −λ) · xt + λ · xs. Figure 1
shows the decrease in extraction accuracy as the distribution
diverges from target (CIFAR10) for two different surrogate
datasets (SVHN and MNIST).
We make two conclusions from our observations: (1) the
success of distillation-based model extraction largely de-
pends on the complexity of the task that the victim model
aims to solve; and (2) similarity to source domain appears
to be critical for extracting ML models that solve com-
plex tasks. We posit that it may be nearly as expensive for
the adversary to extract such models with a good surrogate
dataset, as is training from scratch. A weaker or non-task
speciﬁc dataset may have lesser costs, but has high accu-
racy trade-offs.
4. Data-Free Model Extraction
The goal of model extraction is to train a student model S
to match the predictions of the victim V on its private target
domain DV. That is to say, ﬁnd the student model’s param-
eters θS that minimize the probability of errors between the
student and victim predictions S(x) and V(x) ∀x ∈DV:
arg min
θS
Px∼DV

arg max
i
Vi(x) ̸= arg max
i
Si(x)

(1)
Since the victim’s domain, DV, is not publicly available,
the proposed data-free model extraction attack minimizes
the student’s error on a synthesized dataset, DS. The error
is minimized by optimizing a loss function, L, which mea-
sures disagreement between the victim and student:
arg min
θS
Ex∼DS [L(V(x), S(x))]
(2)
This section describes how we minimize the number of
queries made to the victim model with a novel query gener-
ation process, and how we train the student model itself.
4.1. Overview
The overall attack setup is inspired by Generative Adver-
sarial Networks [15]. A generator (G) model is responsible
for crafting some input images, and the student model S
3

Data flow
Gradient approximation
Backprobagation
Generator
Victim
Student
Loss
Black-box access only
Figure 2. Date-Free Model Extraction Attack Diagram
serves as a discriminator while trained to match the victim
V predictions on these images. In this setting, the two ad-
versaries are S and G, which respectively try to minimize
and maximize the disagreement between S and V.
The data ﬂow is shown as a black arrow in Figure 2: a
vector of random noise z is sampled from a standard nor-
mal distribution and fed into G which produces an image x.
Then the victim V and student S each perform inference on
x to ﬁnally compute the loss function L.
During the back-propagation phase (shown with red ar-
rows) gradients from two different sources need to be com-
puted: the gradients of L with regards to the student’s pa-
rameters θS and the gradient of L with regards to the gen-
erator’s parameters θG. Because the victim is only accessi-
ble as a black-box, it is not possible to propagate gradients
through it. The dashed arrow indicates the need for gradient
approximation (see Section 4.3).
Student.
Prior work on knowledge distillation showed
that a student model S can learn from a teacher and reach
high accuracy even though its architecture is smaller and
different [9, 27]. Therefore, in the context of model extrac-
tion, the adversary only needs to select a model architecture
which has sufﬁcient capacity. This does not require knowl-
edge of the victim architecture but rather generic knowledge
of architectural choices made for the task solved by the vic-
tim (e.g., a convolutional neural network is appropriate for
an object recognition task). In our work, we used a student
with ResNet-18-8x architecture for model extraction.
The loss function L is used to measure the disagreement
between S and V. For this function, we use the ℓ1 norm
loss between victim and student logits (i.e. pre-softmax ac-
tivations), li(x) and si(x) respectively. This requires us to
recover the logits from the softmax outputs, since the ad-
versary only has access to the later. We introduce an ap-
proach for doing so and further elaborate on the choice of
L is detailed in Subsection 4.2. It is important to note that
the gradient of the loss with regard to the student’s weights
θS does not require gradients of V since the victim’s predic-
tions don’t depend on the weights θS.
Algorithm 1: Data-Free Model Extraction
Input: Query budget Q, generator iterations nG,
student iterations nS, learning rate η,
random directions m, step size ϵ
Result: Trained S
while Q > 0 do
for i = 1 . . . nG do
z ∼N(0, 1)
x = G(z; θG)
approximate gradient ∇θGL(x)
θG = θG −η∇θGL(x)
end
for i = 1 . . . nS do
z ∼N(0, 1)
x = G(z; θG)
compute V(x), S(x), L(x), ∇θSL(x)
θS = θS −η∇θSL(x)
end
update remaining query budget Q
end
Generator.
The generator model G is used to synthesize
images that maximize the disagreement between S and V.
The loss function used for G is the same as for S except
that the goal is to maximize it. From this setting emerges an
adversarial game in which S and G compete to respectively
maximize and minimize the same function. In other words,
the student is trained to match the victim’s predictions and
the generator is trained to generate difﬁcult examples for the
student. The adversarial game can be written as:
min
S
max
G
Ez∼N(0,1) [L(V(G(z)), S(G(z)))]
(3)
As shown in Figure 2, computing the gradient of L with re-
gard to θG requires gradients of V. As we only have access
to V as a black-box, gradient approximation techniques are
required. These techniques are discussed in Section 4.3.
Algorithm.
Each iteration alternates training the genera-
tor G and student S. To ﬁnely tune the balance between G
and S training, each of these training phases is repeated nG
and nS times, respectively, before moving on to the next
epoch. While setting nG higher allows G to train faster
and to produce more difﬁcult examples for S, it can also
be wasteful if S does not see enough examples. The trade-
off between nG and nS is an additional hyperparameter that
needs tuning. The additional hyperparameters m and ϵ are
related to gradient approximation (see Section 4.3).
4.2. Loss function
Here we discuss different loss functions to measure the
disagreement between V and S. These losses are commonly
4

used in the knowledge distillation literature given the simi-
larity with the model extraction task [9, 14]. The choice of
the loss function is key to the outcome of the attack since
gradients computed through S and V can easily impede the
convergence of optimizers, e.g., if they vanish because the
wrong loss function is used.
Kullback–Leibler (KL) Divergence
Most prior work in
model distillation optimized over the KL divergence be-
tween the student and the teacher [9, 18, 22]. As a result,
KL divergence between the outputs of S and V is a natural
candidate for the loss function to train the student network.
For a probability distribution over K classes indexed by i,
the KL divergence loss for a single image x is deﬁned as:
LKL(x) =
K
X
i=1
Vi(x) log
Vi(x)
Si(x)

(4)
However, as the student model matches more closely the
victim model, the KL divergence loss tends to suffer from
vanishing gradients [14]. Hypothesis 1 suggests that LKL
can make it difﬁcult to achieve convergence while training
G (refer to Appendix D for justiﬁcation). Speciﬁcally, back-
propagating such vanishing gradients through the generator
can harm its learning. We conﬁrm this through empirical
evaluation as well in Section 6.1.
Hypothesis 1. The gradients of the KL divergence loss with
respect to the image x should be small compared to the gra-
dients of the ℓ1 norm loss when S converges to V:
∥∇xLKL(x)∥≪
S→V ∥∇xLℓ1(x)∥
The ℓ1 norm loss.
To prevent gradients from vanishing,
we use the ℓ1 norm loss (Lℓ1) computed with the victim
and student logits vi and si where i ∈{1...K} and K is
the number of classes. This was previously found by Fang
et al. to prevent gradients from vanishing in knowledge dis-
tillation [14]. Even though Lℓ1 is not differentiable every-
where, it does not suffer from the vanishing gradients issue
and yields better results in practice (see Sec. 6.1). Lastly,
the probabilities output by V need to be transformed into
logits to be used in Lℓ1. We describe how to perform logit
approximation in Appendix B, and evaluate in Sec. 6.3.
Lℓ1(x) =
K
X
i=1
|vi −si|
(5)
4.3. Gradient Approximation
Because only black-box access is provided for V, the op-
timizer aims at maximizing a function for which it only has
an evaluation oracle. Yet, in order to train G, gradients of
the loss with regards to G’s parameters ∇θGL must be com-
puted. Thus, we approximate gradients by interacting with
the oracle: we maximize L with zeroth-order optimization.
4.3.1
Images as a Proxy
The number of parameters in G is typically large (millions
of parameters) and it would be very query-expensive for a
zeroth-order optimizer to get accurate gradient estimations
on this large space. Instead, one can approximate gradients
with regards to the input images x, and then back-propagate
this gradient through G [20]. This way the dimensionality of
gradients being approximated is much smaller, which yields
more accurate zeroth-order approximations.
Additionally the oracle might only accept images that lie
within a pre-deﬁned input domain, for example [−1, 1]d.
To force G to respect this constraint, we use a hyperbolic
tangent activation at the end of the generator architecture.
Furthermore, zeroth-order gradients approximation meth-
ods usually evaluate the function in the neighborhood of a
given point, which can result in query images slightly out-
side the input domain. To avoid this, we approximate gradi-
ents with regard to the pre-activation images (i.e. just before
the hyperbolic tangent function is applied).
4.3.2
Forward Differences Method
The Forward Differences method approximates gradients
by computing directional derivatives Duif(x) of a function
f at a point x along m random directions ui. The direc-
tional derivatives are computed by measuring the variation
of f a small step of size ϵ in the direction ui. They are then
averaged to form an estimator of the gradient ∇FWDf(x).
In a way, each directional derivative brings some amount of
information about true gradient. The estimator being more
accurate as the number of random directions increases.
∇FWDf(x) = 1
m
m
X
i=1
df(x + ϵui) −f(x)
ϵ
ui
(6)
The main advantage of this method is that the number of
query directions m may be chosen independently of the in-
put space dimensionality, offering a trade-off between query
utilization and gradient accuracy. This makes it an appeal-
ing candidate for DFME [20]. The inﬂuence of the number
of query directions m is further described in Section 6.2.
Finite differences, an alternative gradient approximation
method used when crafting adversarial examples [4], re-
quires too many queries per gradient estimate to be viable
for data-free model extraction.
5. Experimental Validation
We evaluate data-free model extraction (DFME) against
victim models trained SVHN and CIFAR-10. We show that
the resulting student models can reach high accuracy (e.g.,
95.2% on SVHN) even when the generator only has access
to inaccurate gradient estimates. Later in Section 6, we per-
form an ablation study and evaluate the impact of each at-
5

tack component on the ﬁnal student model accuracy and on
the query budget Q.
5.1. Datasets and Architectures
We evaluate the effectiveness of the proposed DFME
method on two datasets: SVHN and CIFAR-10. For each
dataset, the victim model architecture is a ResNet-34-8x.
These victim models were trained during 50 epochs for
SVHN and 200 for CIFAR-10 with SGD at an initial learn-
ing rate of 0.1, decayed by a factor of 10 at 50% of training.
We use ResNet-18-8x as the architecture for our student
model. This is inspired by previous works in knowledge
distillation [14] that show how a smaller student is sufﬁcient
to distill the knowledge of a larger teacher. The network
was trained with a batch size of 256 with SGD, with an
initial learning rate of 0.1, a weight-decay of 5.10−4, and a
learning rate scheduler that multiplies the learning rate by a
factor 0.3 at 0.1×, 0.3×, and 0.5× the total training epochs.
The default query budget Q is 2M for SVHN, and 20M for
CIFAR-10 in our experiments.
The generator used three convolutional layers, inter-
leaved with linear up-sampling layers, batch normalization
layers, and ReLU activations for all layers except the last
one. The ﬁnal activation function was the hyperbolic tan-
gent function to output values in the range [-1,1] (see Sec-
tion 4.3). It was also trained with a batch size of 256, but
using an Adam optimizer with an initial learning rate of
5.10−4 which is decayed by a factor 0.3 at 10%, 30%, and
50% of the training.
For gradient approximation we sample m = 1 random
directions and a step size ϵ = 10−3.
5.2. Results
We compare the performance of different extraction at-
tacks in Table 1. We measure the ratio between the stu-
dent’s accuracy and the victim’s accuracy on the victim’s
test set.
This helps compare the performance of DFME
across different datasets. The student model’s normalized
accuracy is reported for each dataset and extraction method
evaluated—our approach (DFME), our approach with KL
divergence loss (DFME-KL), our approach without logit
correction (Log-Probabilities), and concurrent work [20]
(MAZE). Further, we perform DFME with a range of query
budgets and reported the accuracy in Figure 3.
Without any knowledge of the original training distri-
bution, the proposed DFME method achieved as high as
88.1% (0.92x target) of accuracy with Q = 20M and 89.9%
(0.94x target) with Q = 30M. The accuracy of the extracted
model exceeds that reported in concurrent work which we
refer to as MAZE in our results [20].
However, in our
best-efforts at reproducing their results with the details in
the paper,2 we were unsuccessful in achieving the same re-
2The authors declined to share their code upon request.
0
5
10
15
20
25
30
Query budget (M)
0
25
50
75
100
Accuracy (%)
CIFAR10
SVHN
Figure 3. Test accuracy wrt query budget, for SVHN and CIFAR10
ported accuracy, and were only able to achieve an accuracy
of 45.6% (0.48x target) at best on the student model. In ad-
dition, MAZE reports that they were unable to learn when
using extremely few directions (such as m = 1) for the gra-
dient approximation with CIFAR10, whereas we ﬁnd that
weak gradient approximations are beneﬁcial to reduce the
overall query budget of successful attacks.
We observed similar results for the SVHN victim model:
reaching as high as 95.2% (0.99x target) accuracy with only
2M queries. The task for SVHN is much simpler than CI-
FAR10 given that a model with 84% (0.87x target) accu-
racy can be extracted from just random noise. The proposed
method allows one to achieve far higher accuracy.
One limitation of our study is that the reported query
budgets do not include the cost of hyperparameter tuning.
This is an important direction for future work as prelim-
inary experiments suggest that extraction accuracy can be
sensitive to the choice of hyperparameters.
6. Ablation Studies
Our work systematically transitions from a data-free
knowledge distillation paradigm [14, 27] to a data-free
model extraction scenario. The main challenges in this tran-
sition were (1) to surpass the need for true gradients for
training the student; (2) the lack of access to true victim
logits; and (3) the need to restrict the query complexity of
the attacks (to reduce the cost of stealing). With this goal,
we made speciﬁc choices with regards to (a) the loss func-
tion; (b) gradient approximation; and (c) logit access. In
this section, we detail the impact of each of these choices to
the ﬁnal performance of our proposed DFME method.
6.1. Choice of Loss Function
The choice of loss is of paramount importance to a suc-
cessful extraction. In our DFME approach, the choice of
loss involves similar factors to those outlined in research
on GANs: multiple works have discussed the problem of
vanishing gradients as the discriminator becomes strong in
case of GAN training [1, 2]. For our DFME approach, we
minimize the ℓ1 distance between the output logits of the
student and the teacher. We ﬁnd that this signiﬁcantly im-
proves convergence and stability over other possible losses,
such as the KL divergence chosen in [20].
6

Dataset (budget)
Victim accuracy
DFME
DFME-KL
MAZE* [20]
Log-Probabilities
CIFAR10 (20M)
95.5%
88.1% (0.92×)
76.7% (0.80×)
45.6% (0.48×)
73.2% (0.77×)
SVHN (2M)
96.2%
95.2% (0.99×)
84.7% (0.88×)
91.1% (0.95×)
94.4% (0.98×)
Table 1. Accuracy and normalized accuracy of data-free model extraction methods. Results for ‘MAZE’ reﬂect our best-effort reproduction.
0
25
50
75
100
125
150
175
200
Epoch (x50)
20
40
60
80
Accuracy (%)
KL divergence
1 norm
Figure 4. Test accuracy as training progresses for ℓ1 and KL diver-
gence losses.
We perform DFME in the same setting to evaluate the
difference between the KL divergence and ℓ1 losses. Be-
low, we draw comparisons based on two metrics: (1) Final
accuracy attained by the student at the end of a ﬁxed num-
ber of queries as well as the learning curves of the student;
and (2) The norm of gradients of the loss with respect to the
input image as the training progresses.
Test Accuracy.
The key metric of interest for this com-
parison is the normalized accuracy of the student model at
the end of a designated query budget Q of 20M queries for
CIFAR10 and 2M queries for SVHN. Table 1 shows that
using the ℓ1 loss achieves signiﬁcantly better test accuracies
compared to the KL divergence loss. For instance, on CI-
FAR10 the accuracy improves from 76.7% to 88.1% when
switching from KL divergence to the ℓ1 loss. We also vi-
sualize a learning curve for CIFAR10 in Figure 4: the KL
divergence objective slows converge and tappers off earlier,
even when the student has yet to plateau.
Gradient Vanishing.
The KL divergence loss suffers
from vanishing gradients, as explained in Section 4.2. In
DFME, these gradients are used to update the generator’s
parameters and are thus essential to synthesize queries
which extract more information from the victim. In Fig-
ure 5 we empirically demonstrate that as the student accu-
racy approaches that of the victim model, the gradients of
the KL divergence loss with respect to the input image re-
duce signiﬁcantly in norm. The same decay is slower and
less signiﬁcant in case of the ℓ1 loss. We hypothesize that
these vanishing gradients are the cause for degraded accu-
racy when using the KL divergence loss.
0
25
50
75
100
125
150
175
200
Epoch (x50)
0.0000
0.0025
0.0050
0.0075
0.0100
0.0125
Norm gradient
KL divergence loss
1 norm loss
Figure 5. Norm of gradients with respect to the input image, for
the KL divergence and ℓ1 norm losses.
m
1
3
5
8
10
No. of Queries
10.04
10.02
16.33
13.80
20.00
Table 2. Minimum queries (in millions) to reach 85% accuracy on
CIFAR10, for different number of gradient approximation steps.
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Query budget consumed (M)
20
40
60
80
Accuracy (%)
m=1
m=3
m=5
m=8
m=10
Figure 6. Accuracy of the model during training for different num-
ber of gradient approximation steps, m.
Model
MTL (±2.3e-6)
MC (±2.3e-6)
LP (±1.3)
Resnet-34-8x
-1.24e-5
1.24e-5
4.98
Densenet 121
5.53e-7
1.88e-6
3.88
VGG 16
1.97e-5
1.97e-5
3.93
Table 3. Mean of true logits (MTL) for 3 victim architectures;
reconstruction error (MAE) between approximate and true logits
when using mean correction (MC) and log-probabilities (LP).
6.2. Gradient Approximation
Recall that the ℓ1 loss cannot be back-propagated
through the victim since the adversary only has access to
it as a black-box. Recent work on data-free model distil-
lation [14] has claimed that the gradient information from
the teacher is ‘indispensable at the beginning of adversarial
training’ because the student alone can not provide useful
7

signal to the generator when randomly initialized. Below
we consider: (1) the quality of approximation required; and
(2) the overall impact on query complexity. In particular, we
compare two approaches for improving the training of our
DFME generator: using an increased number of queries to
compute more accurate gradient estimates or training gen-
erator for longer using poorer gradient estimates.
Number of gradient approximation steps.
When gradi-
ents used to update the generator are approximated with the
forward differences method, a larger number of random di-
rections m allows one to compute more accurate gradients.
However, in a model extraction setting, each additional gra-
dient approximation step comes at the cost of increased
query complexity. In practice, with a ﬁxed query budget
Q, changing the number of random directions directly im-
pacts the proportion of queries used to train each network.
This ratio of queries r used to train the student is given by:
r =
nS
nS + (m + 1)nG
(7)
In our setting (i.e. nG = 1, nS = 5) choosing m equal to
1 or 10 respectively results in 71% and 31% of the query
budget being used to directly train the student, while the
remainder is spent to get better gradient estimates to train
the generator. Despite using a majority of queries to train
the generator, the setting with m = 10 achieves comparable
accuracy. In Table 2, we observe how choosing lower values
of m achieves 85% test accuracy in much fewer queries.
Amortizing the cost.
We hypothesize that since early in
the training the discriminator (or student) provides only lit-
tle signal, it is beneﬁcial for the generator to initially rely
on weak signals of gradient approximation. Effectively, this
helps amortize the cost of gradient approximation over mul-
tiple epochs, and effectively pushes the expense to a later
stage when the discriminator (or student) provides stronger
signal. Figure 6 shows that relative to the query budget uti-
lization, different values of m perform similarly.
This also suggests a hybrid strategy where the adver-
sary ﬁrst extracts a (somewhat poor) student model through
distillation from a surrogate dataset. Indeed, we show in
Section 3 that surrogate datasets drawn from a different
distribution (than the victim model’s training data) enable
distillation-based model extraction—albeit to a lower accu-
racy than in-distribution surrogate datasets. This poor initial
student can then be improved by synthesizing queries with
the data-free model extraction’s generator to bring the stu-
dent model closer to the victim model’s performance.
Case m = 1.
In the extreme case where the number of
gradient approximation steps for forward differences (m) is
set to 1, the approximated gradient is colinear to the random
direction sampled for the approximation, but always points
in the direction that helps maximize the loss (i.e. its projec-
tion onto the true gradient is positive). The cosine similarity
with the true gradient is, thus, very small. To validate this
effect, we additionally experiment with m = 1 where the
approximate gradient was randomly ﬂipped to the wrong di-
rection with half probability. As hypothesized, the student
accuracy did not improve beyond 20% in our experiments
on CIFAR10. This suggests that computing gradients that
are extremely inaccurate makes it possible to train the stu-
dent as long as these gradients are in the correct direction.
6.3. Impact of Logits Correction
A model extraction attack should be applicable to the na-
ture of predictions offered by MLaaS APIs. Most APIs pro-
vide per-class probability distribution rather than the true
logits, since probabilities are more easily interpreted by the
end user. To perform model extraction successfully we thus
need to recover the logits from the victim’s prediction prob-
abilities. We show that is is possible to do so and recover
approximate logits whose Mean Average Error (MAE) with
the true logits is low, on three different victim architecture.
The MAE reported in Table 3 are negligible compared to
true logits which take values in the order of magnitude of 1.
Therefore, the adversary can use these approximate logits
in lieu of the true logits. In comparison, approximating true
logits with plain log-probabilities resulted in a MAE in the
order of magnitude of the true logits themselves. Using the
log-probabilities with such a large error makes the student
training harder—it did not yield accuracy above 75%.
This method is effective because the mean of the true
logits is nearly 0 (see Table 3). Therefore, subtracting the
mean from the log-probabilities is equivalent to subtracting
the additive constant C(x) itself.
7. Conclusions
In this paper, we demonstrate that data-free model ex-
traction is not only practical but also yields accurate copies
of the victim model. This means that model extraction at-
tacks is a credible threat to the intellectual property of mod-
els released intentionally or not to the public. We believe an
interesting direction for future work is to detect such queries
without decreasing the model’s utility to legitimate users.
Acknowledgements
We thank the reviewers and members of CleverHans Lab
for their insightful feedback. This work was supported by
a Canada CIFAR AI Chair, NSERC, a gift from Microsoft,
and sponsors of the Vector Institute.
8

References
[1] Martin Arjovsky and L´eon Bottou. Towards principled meth-
ods for training generative adversarial networks, 2017. 6
[2] Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan, 2017. 6
[3] Jimmy Ba and Rich Caruana. Do deep nets really need to
be deep? In Advances in neural information processing sys-
tems, pages 2654–2662, 2014. 2
[4] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song.
Practical black-box attacks on deep neural networks using ef-
ﬁcient query mechanisms. In European Conference on Com-
puter Vision, pages 158–174. Springer, 2018. 2, 3, 5, 12
[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners, 2020. 1
[6] V. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and
Songbai Yan. Model extraction and active learning. ArXiv,
abs/1811.02054, 2018. 1
[7] Varun Chandrasekaran, Kamalika Chaudhuri, Irene Gia-
comelli, Somesh Jha, and Songbai Yan. Exploring connec-
tions between active learning and model extraction, 2019. 1
[8] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and
Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-
box attacks to deep neural networks without training substi-
tute models. In Proceedings of the 10th ACM Workshop on
Artiﬁcial Intelligence and Security, pages 15–26, 2017. 2, 3,
12
[9] Jang Hyun Cho and Bharath Hariharan. On the efﬁcacy of
knowledge distillation, 2019. 4, 5, 11
[10] Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, and Jungwon
Lee. Data-free network quantization with adversarial knowl-
edge distillation.
In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition Work-
shops, pages 710–711, 2020. 2
[11] Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Clau-
dine Badue, Alberto F. de Souza, and Thiago Oliveira-
Santos.
Copycat cnn: Stealing knowledge by persuading
confession with random non-labeled data.
2018 Interna-
tional Joint Conference on Neural Networks (IJCNN), Jul
2018. 1
[12] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-
Fei. Imagenet: A large-scale hierarchical image database.
In 2009 IEEE Conference on Computer Vision and Pattern
Recognition, pages 248–255, 2009. 1
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale, 2020. 1
[14] Gongfan Fang, Jie Song, Chengchao Shen, Xinchao Wang,
Da Chen, and Mingli Song. Data-free adversarial distillation.
arXiv preprint arXiv:1912.11006, 2019. 2, 5, 6, 7
[15] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks, 2014. 2, 3
[16] Jianping Gou, Baosheng Yu, Stephen John Maybank, and
Dacheng Tao. Knowledge distillation: A survey, 2020. 2
[17] Alon Halevy, Peter Norvig, and Fernando Pereira. The un-
reasonable effectiveness of data. IEEE Intelligent Systems,
24:8–12, 2009. 1
[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distill-
ing the knowledge in a neural network.
arXiv preprint
arXiv:1503.02531, 2015. 2, 5, 11
[19] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex
Kurakin, and Nicolas Papernot. High accuracy and high ﬁ-
delity extraction of neural networks. In 29th {USENIX} Se-
curity Symposium ({USENIX} Security 20), 2020. 1
[20] Sanjay Kariyappa, Atul Prakash, and Moinuddin Qureshi.
Maze: Data-free model stealing attack using zeroth-order
gradient estimation, 2020. 2, 5, 6, 7
[21] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, 2009. 3, 11
[22] Xu Lan, Xiatian Zhu, and Shaogang Gong. Knowledge dis-
tillation by on-the-ﬂy native ensemble, 2018. 5, 11
[23] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
3, 11
[24] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang,
Alfred Hero, and Pramod K. Varshney. A primer on zeroth-
order optimization in signal processing and machine learn-
ing, 2020. 2
[25] Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner.
Data-free knowledge distillation for deep neural networks,
2017. 2
[26] Daniel Lowd and Christopher Meek. Adversarial learning.
In Proceedings of the Eleventh ACM SIGKDD International
Conference on Knowledge Discovery in Data Mining, KDD
’05, page 641–647, New York, NY, USA, 2005. Association
for Computing Machinery. 1
[27] Paul Micaelli and Amos J Storkey.
Zero-shot knowledge
transfer via adversarial belief matching.
In Advances in
Neural Information Processing Systems, pages 9551–9561,
2019. 2, 4, 6
[28] Smitha Milli, Ludwig Schmidt, Anca D. Dragan, and Moritz
Hardt. Model reconstruction from model explanations, 2018.
1
[29] Yurii Nesterov and Vladimir Spokoiny. Random gradient-
free minimization of convex functions. Foundations of Com-
putational Mathematics, 17(2):527–566, 2017. 2
[30] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural
images with unsupervised feature learning. In NIPS Work-
shop on Deep Learning and Unsupervised Feature Learning
2011, 2011. 3, 11
9

[31] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff nets: Stealing functionality of black-box models.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019. 1, 2
[32] Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade,
Shirish Shevade, and Vinod Ganapathy. A framework for
the extraction of deep neural networks by leveraging public
data, 2019. 1
[33] Nicolas Papernot,
Patrick McDaniel,
Ian Goodfellow,
Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Prac-
tical black-box attacks against machine learning. In Proceed-
ings of the 2017 ACM on Asia Conference on Computer and
Communications Security, ASIA CCS ’17, page 506–519,
New York, NY, USA, 2017. Association for Computing Ma-
chinery. 1
[34] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets, 2015. 2
[35] Sebastian Ruder.
Transfer Learning - Machine Learn-
ing’s Next Frontier. http://ruder.io/transfer-
learning/, 2017. 11
[36] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Advances in neural information pro-
cessing systems, pages 2234–2242, 2016. 2
[37] Ilia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Paper-
not, Robert Mullins, and Ross Anderson. Sponge examples:
Energy-latency attacks on neural networks, 2020. 1
[38] Leslie N. Smith. Cyclical learning rates for training neural
networks, 2017. 11
[39] Emma Strubell, Ananya Ganesh, and Andrew McCallum.
Energy and policy considerations for deep learning in nlp,
2019. 1
[40] Antonio Torralba, Rob Fergus, and William T. Freeman. 80
million tiny images: A large data set for nonparametric ob-
ject and scene recognition. IEEE Trans. Pattern Anal. Mach.
Intell., 30(11):1958–1970, Nov. 2008. 3, 12
[41] Florian Tram`er, Fan Zhang, Ari Juels, Michael K Reiter,
and Thomas Ristenpart. Stealing machine learning models
via prediction apis. In 25th {USENIX} Security Symposium
({USENIX} Security 16), pages 601–618, 2016. 1
[42] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan
Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng.
Autozoom: Autoencoder-based zeroth order optimization
method for attacking black-box neural networks.
In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pages 742–749, 2019. 2, 3
[43] Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti
Singh. Stochastic zeroth-order optimization in high dimen-
sions. In International Conference on Artiﬁcial Intelligence
and Statistics, pages 1356–1365, 2018. 2
[44] Andre Wibisono, Martin J Wainwright, Michael Jordan, and
John C Duchi. Finite sample convergence rates of zero-order
stochastic optimization methods. Advances in Neural Infor-
mation Processing Systems, 25:1439–1447, 2012. 3
[45] Sergey Zagoruyko and Nikos Komodakis. Paying more at-
tention to attention: Improving the performance of convolu-
tional neural networks via attention transfer, 2017. 2
[46] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chen-
glong Bao, and Kaisheng Ma. Be your own teacher: Im-
prove the performance of convolutional neural networks via
self distillation, 2019. 2
[47] Ying Zhang, Tao Xiang, Timothy M. Hospedales, and
Huchuan Lu. Deep mutual learning, 2017. 2
10

A. How Hard is it to Find a Surrogate Dataset?
To motivate the need for data-free approaches to model
extraction, we show here that an adversary relying on a sur-
rogate dataset must ensure that its distribution is close to the
one of the victim’s training set. Otherwise, model extraction
will return a poor approximation of the victim.
Consider a victim machine learning model, V, trained on
a proprietary dataset, DV. The victim model reveals its pre-
dictions through either a prediction API (as is common in
MLaaS) or through the deployment of the model on devices
accessible to adversaries. The adversary, A, attempts to
steal V by querying it with a surrogate dataset, DS. This
surrogate dataset is assumed to be publicly available or eas-
ier to access because it does not need to be labeled.
We now perform a systematic study of the features that
characterize the closeness of DS when compared to DV. Let
the private and surrogate datasets DV and DS be character-
ized by D = {X, P(X), Y, P(Y |X)} [35]. The private and
surrogate datasets can vary in three ways, which we will il-
lustrate in the following with object recognition tasks:
1. A: (XV ̸= XS). When the inputs of DV and DS belong
to different feature spaces (i.e. domain). In computer
vision, for example, this can be a scenario wherein the
input data (e.g., images, videos) for the datasets con-
tain a different number of channels or pixels.
2. B: (P(XV) ̸= P(XS)). While the input domain of
both the surrogate and private datasets is the same,
their marginal probability distribution is different. For
example, when the semantic nature is different for the
two datasets (images of animals, digits, etc).
3. C: (P(YV|XV) ̸= P(YS|XS)). We consider a setting
where the semantic distribution (P(X)) is the same,
but the class-conditional probability distributions of
the victim and surrogate training sets are different, e.g.
when the two datasets have class imbalance.
A.1. Optimization Problem
The logit distribution of the victim network can often
have a strong afﬁnity toward the true label. To address this
issue, Hinton et al. suggested scaling the logits to make the
probability distributions more informative [18].
Vi(x) =
exp (vi(x)/τ)
P
j exp (vj(x)/τ);
L = −τ 2KL (V(x), S(x))
where τ > 1 is referred to as the temperature scaling param-
eter. In the knowledge distillation literature, a combination
of both the cross entropy and knowledge distillation loss are
used to query the teacher [9]. However, model extraction
we rely only on the KL divergence loss because the queries
are made from a surrogate dataset which may not have any
semantic binding to the true class.
A.2. Experimental Setting
Hyperparameters
To search over a meaningful hyperpa-
rameter space for the temperature co-efﬁcient τ, we refer to
prior work in knowledge distillation such as [9, 18, 22] to
conﬁne the search over τ ∈{1, 3, 5, 10}. We used the SGD
optimizer, and train the CIFAR10 students for 100 epochs,
while the SVHN students were trained for 50 epochs. We
experimented with two different learning schedules: (1)
cyclic learning rate [38]; and (2) step-decay learning rate.
For step-decay, we reduced the learning rate by a factor of
0.2 at 30%, 60%, and 80% of the training process. In both
cases, the maximum learning rate was set to 0.1.
Experimental validation.
To illustrate our argument, we
next detail the relation between a task-speciﬁc surrogate
dataset and the accuracy of state-of-the-art model extraction
techniques. The victim models under attack are ResNet-18-
8x models, their accuracy is reported in Table 4. Further
details on the victim models training are provided in Sec-
tion 5). We ﬁnd that querying from the original dataset
yields the most query-efﬁcient and accurate extraction re-
sults. This is not surprising given that this setup corresponds
to the original knowledge distillation setting. Our observa-
tions are made on both CIFAR10 and SVHN:
• CIFAR10. We benchmarked model extraction on 4
surrogate datasets, each reﬂecting a different property
detailed above: CIFAR10 [21], CIFAR100 [21] (BC),
SVHN [30] (AB) and MNIST [23] (AB). To ensure a
fair comparison, we bound the maximum number of
distinct samples queried by 50,000 while performing
model extraction.
• SVHN. We evaluated model extraction by querying
from SVHN, SVHNskew (C), MNIST (A) and CI-
FAR10 (B) as surrogate datasets. Similar to the case
for CIFAR10, we cap the maximum number of distinct
samples queried to 50,000.
Finally, as a control for our experiments, we also studied
the extraction accuracy of the models when trained using
totally random queries.
Dataset adaptation.
The SVHN, CIFAR10, and CI-
FAR100 datasets contain 32 × 32 color images. To query
networks trained on CIFAR10 and SVHN with images from
the MNIST dataset, which contains grayscale images of size
28×28, we re-scaled the image and repeated the same input
across all three RGB channels. In case of random input gen-
eration, we sample input tensors from a normal distribution
with mean 0 and variance 1. Note that the teacher networks
were trained on normalized datasets in the ﬁrst place. Fi-
nally, in case of SVHNskew, we supplied images from only
11

Victim
CIFAR10
CIFAR100
SVHN
MNIST
SVHNskew
Random
CIFAR10
95.5%
95.2%
93.5%
66.6%
37.2%
-
10.0%
SVHN
96.2%
96.0%
-
96.3%
89.5%
96.1%
84.1%
Table 4. Model Extraction accuracy across various surrogate datasets. Victim models were trained on the CIFAR10 and SVHN datasets,
and the source accuracies are reported under the heading ‘Victim’
the ﬁrst 5 classes of the dataset to skew the distribution of
the modiﬁed dataset.
Results.
We present the results for accuracy of extracted
models across various surrogate datasets for CIFAR10 and
SVHN in Table 4.
Recall that both the CIFAR10 and
CIFAR100 datasets are subsets from the same TinyIm-
ages [40] dataset. We ﬁnd that the identical source dis-
tribution was extremely useful in making relevant queries
to the CIFAR10 teacher.
The accuracy of the extracted
model reached 93.5%, just below the 95.5% accuracy of
the teacher model. However, when we used the SVHN sur-
rogate dataset to query the CIFAR10 teacher, with a differ-
ent source distribution, the model extraction performance
dropped remarkably, attaining a maximum of 66.6% across
all of the hyperparameters tried. In the most extreme sce-
nario when querying the CIFAR10 teacher with MNIST–a
dataset with disjoint feature space both in terms of number
of pixels, and number of channels)—model accuracy did
not improve beyond 37.2%.
On the contrary, we notice that the victim trained on the
SVHN dataset is much easier for the adversary to extract.
Surprisingly, even when the victim is queried with com-
pletely random inputs, the extracted model attains an ac-
curacy of over 84% on the original SVHN test set. Further,
nearly all surrogate datasets are able to achieve greater than
90% accuracy on the test set. We hypothesize that this ob-
servation is linked to how the digit classiﬁcation task, at
the root of SVHN, is a simpler task for neural networks to
solve, and the underlying representations (hence, not be-
ing as complex as for CIFAR10) can be learnt even when
queried over random inputs.
Given the current understanding of model extraction, we
make two conclusions: (1) the success of model extraction
largely depends on the complexity of the task that the vic-
tim model aims to solve; and (2) similarity to source domain
is critical for extracting machine learning models that solve
complex tasks. We posit that it may be nearly as expen-
sive for the adversary to extract a CIFAR10 machine learn-
ing model with a good surrogate dataset, as is training from
scratch. A weaker or non-task speciﬁc dataset may have
lesser costs, but has high accuracy trade-offs.
B. Recovering logits from probabilities
The main difﬁculty with computing Lℓ1 is that it requires
access to V’s logits vi, but we only have access to the proba-
bilities of each class (i.e., after the softmax is applied to the
logits). In a ﬁrst approximation, the logits can be recovered
by computing the log-probabilities but the resulting approx-
imate logits are computed up to an additive constant C(x) to
which we don’t have access in a black-box setting. This ad-
ditive constant is the same for all logits but is different from
one image to another. Related works on adversarial exam-
ples [4, 8] use losses that are the difference of two logits,
effectively canceling out the additive constant. In our case,
the logits need to be used individually which makes the ℓ1
loss more difﬁcult to compute in our setting.
To overcome this issue, we propose to approximate the
true logits of each image x in two steps. First, compute the
logarithm of the probability vector V (x).
˜vi(x) = log Vi(x)= vi + C(x)
(8)
Then, compute the approximate true logits v∗
i (x) by sub-
tracting the log-probability vector with its own mean:
v∗
i (x) = ˜vi(x) −1
K
K
X
j=1
˜vj(x)
= vi(x) −1
K
K
X
j=1
vj(x) ≈vi(x)
(9)
The second equality holds because the mean of the log-
probability vector ˜vi(x) is equal to the mean of the true vic-
tim logits vi(x) plus the mean of the additive constant (i.e.
the C(x) itself). By analyzing the mean values of the true
logits from various pre-trained models—which proves to be
negligible in comparison to the logit values themselves, we
provide empirical evidence in Section 6.3 that this recovers
a highly accurate approximation of the true logits v∗
i (x).
C. Examples of Synthetic Images
Figure 7 shows 4 images from the generator towards the
end of the attack on CIFAR-10. We do not observe any sim-
ilarities with the images from the original training dataset.
12

Figure 7. Four synthetic images from the generator.
D. Hypothesis 1: Justiﬁcation
D.1. Preliminary results
Lemma 1. If S(x) ∈(0, 1)K is the softmax output of a
differentiable function (e.g. a neural network) on an input x
and s is the corresponding logits vector, then the Jacobian
matrix J = ∂S
∂s has an eigenvalue decomposition and all its
eigenvalues are in the interval [0, 1].
Proof. By deﬁnition:
∀i ∈{1 . . . K}, Si =
exp(si)
PK
k=1 exp(sk)
For some i, j ∈{1 . . . K}, if i ̸= j:
∂Si
∂sj
= −exp(sj)
exp(si)
(PK
k=1 exp(sk))
2
= −SiSj
if i = j:
∂Si
∂sj
= exp(si)(PK
k=1 exp(sk)) −exp(sj) exp(si)
(PK
k=1 exp(sk))
2
=
exp(si)
PK
k=1 exp(sk)
−
exp(si)2
(PK
k=1 exp(sk))2
= Si(1 −Si)
Therefore, ∀x,
J = ∂S
∂s =


S1(1 −S1)
−S1S2
. . .
−S1SK
−S1S2
S2(1 −S2)
. . .
−S2SK
...
...
...
−S1SK
−S2SK
. . .
SK(1 −SK)


The matrix J
is real-valued symmetric,
therefore
it has an eigen-decomposition with real eigenvalues.
∃λ1, λ2, . . . , λK ∈R, X1, X2, . . . , XK ̸= 0 such that:
∀i ∈{1 . . . K}, JXi = λiXi
Let us prove that all eigenvalues are in the interval [0, 1].
Suppose for a contradiction that one eigenvalue λ is strictly
negative. Let the associated eigenvector be:
X = [x1, x2, . . . , xK]T
The i-th component of the vector JX is:
[JX]i = Sixi −Si
K
X
k=1
xkSk
= Sixi −Si⟨X, S⟩
where ⟨·, ·⟩is the standard inner product.
Since X is an eigenvector we have,
JX = λX
So ∀i,
Sixi −Si⟨X, S⟩= λxi
xi(Si −λ) = Si⟨X, S⟩
Since X ̸= 0, ∃i0 such that xi0 ̸= 0. Furthermore, λ is
strictly negative so:
xi0(Si0 −λ) = Si0⟨X, S⟩̸= 0
Therefore, the inner product on the right hand side is
non-zero.
In addition, λ < 0 implies that Si −λ > Si > 0 so
∀i, xi and ⟨X, S⟩have the same sign. There are two cases
left.
If ⟨X, S⟩> 0, then ∀i, xi > 0 and:
xi(Si −λ) > xiSi
Si⟨X, S⟩> xiSi
By summing on all i we obtain:
K
X
i=1
Si⟨X, S⟩>
K
X
i=1
xiSi
⟨X, S⟩
K
X
i=1
Si > ⟨X, S⟩
⟨X, S⟩> ⟨X, S⟩
Which is an absurdity.
13

If ⟨X, S⟩< 0, then ∀i, xi < 0 and:
xi(Si −λ) < xiSi
Si⟨X, S⟩< xiSi
The same summation and reasoning yields an absur-
dity. We just proved that all the eigenvalues of J are non-
negative.
Lastly, the trace of the Jacobian matrix tr(J) equals the
sum of all eigenvalues. Computing the trace yields:
tr(J) =
K
X
i=1
λi =
K
X
i=1
Si(1 −Si)
=
K
X
i=1
Si −
K
X
i=1
S2
i
= 1 −
K
X
i=1
S2
i < 1
Since λi ≥0 and PK
i=1 λi < 1, all eigenvalues must be
in the interval [0, 1], which concludes the proof.
Lemma 2. In the same setting as for Lemma 1, if J is the
Jacobian matrix ∂S
∂s then for any vector Z we have:
∥JZ∥≤∥Z∥
Proof. Let λ1, λ2, . . . , λK be the eigenvalues of J and
X1, X2, . . . , XK be the associated eigenvectors. We can
decompose Z with the orthonormal eigenvector basis:
Z =
K
X
i=1
αiXi
Computing the product JZ yields:
JZ =
K
X
i=1
λiαiXi
The norm of the product is:
∥JZ∥= (JZ)T (JZ) =
K
X
i=1
λ2
i α2
i
≤
K
X
i=1
α2
i
because ∀i, |λi| ≤1 (see Lemma 1). Since the eigenvector
basis is orthonormal we have
∥JZ∥≤
K
X
i=1
α2
i = ∥Z∥
Lemma 3. Let S(x) and V(x) be the softmax output of two
differentiable functions (e.g. neural networks) on an input
x, with respective logits s(x) and v(x). When S converges
to V, then ∂S
∂s converges to ∂V
∂v .
Proof. Recall that
∂S
∂s =


S1(1 −S1)
−S1S2
. . .
−S1SK
−S1S2
S2(1 −S2)
. . .
−S2SK
...
...
...
−S1SK
−S2SK
. . .
SK(1 −SK)


If Vi(x) −Si(x) = ϵi(x), then:
Si(1 −Si) = (Vi + ϵi)(1 −Vi −ϵi)
= Vi(1 −Vi) + ϵi(1 −Vi) −ϵ2
i
= Vi(1 −Vi) + o(1)
and
−SiSj = −(Vi + ϵi)(Vj + ϵj)
= −ViVj −Viϵj −Vjϵi −ϵiϵj
= −ViVj + o(1)
Therefore, we can write:
∂S
∂s = ∂V
∂v + ¯ϵ(x)
where ¯ϵ(x) converges to the null matrix as S converges
to V. In other words we can write:
∂S
∂s
≈
S→V
∂V
∂v
D.2. Justiﬁcation of the hypothesis.
Hypothesis 1 states that for two differentiable functions
with softmax output S and V, and respective logits s and v,
the gradients of the KL divergence loss LKL with respect to
the input should be small compared to the gradients of the
ℓ1 norm loss Lℓ1 as S converges to V. ∀x ∈[−1, 1]d:
∥∇xLKL(x)∥≪
S→V ∥∇xLℓ1(x)∥
14

Proof. First, we note that:
K
X
i=1
Si = 1
implies
K
X
i=1
∂Si
∂x = 0
And the same holds for V because both are probability
distributions.
Then we compute the gradients for both loss functions:
For the ℓ1 norm loss:
∇xLℓ1(x) =
K
X
i=1
sign(vi −si)
∂vi
∂x −∂si
∂x

For the KL divergence loss:
∇xLKL(x) =
K
X
i=1
∂Vi
∂x log Vi + 1∂Vi
∂x −∂Vi
∂x log Si −∂Si
∂x
Vi
Si
=
K
X
i=1
∂Vi
∂x +
K
X
i=1
∂Vi
∂x log Vi
Si
−∂Si
∂x
Vi
Si
=
K
X
i=1
∂Vi
∂x log Vi
Si
−∂Si
∂x
Vi
Si
When S converges to V, we can write
Vi(x) = Si(x)(1 + δi(x))
where δi(x) →
S→V 0.
Since log 1 + x ≈x when x is close to 0 we can write:
∇xLKL(x) =
K
X
i=1
∂Vi
∂x log Vi
Si
−∂Si
∂x
Vi
Si
≈
K
X
i=1
∂Vi
∂x δi −∂Si
∂x (1 + δi)
≈
K
X
i=1
δi
∂Vi
∂x −∂Si
∂x

+
K
X
i=1
∂Si
∂x
≈
K
X
i=1
δi
∂Vi
∂x −∂Si
∂x

≈
K
X
i=1
δi
∂V
∂v
∂vi
∂x −∂si
∂x

(Lemma 3)
≈∂V
∂v
K
X
i=1
δi
∂vi
∂x −∂si
∂x

Using Lemma 2, the norm is upper bounded by:
∥∇xLKL(x)∥≤

K
X
i=1
δi
∂vi
∂x −∂si
∂x

(10)
For the ℓ1 norm, however, the norm is:
∥∇xLℓ1(x)∥=

K
X
i=1
sign(vi −si)
∂vi
∂x −∂si
∂x
 (11)
From equation 10, we can observe that each term is neg-
ligible compared to its counterpart in equation 11: for all i
we have:
δi
∂vi
∂x −∂si
∂x
 ≤ϵ

∂vi
∂x −∂si
∂x

And also ∀i:
sign(vi −si)
∂vi
∂x −∂si
∂x
 =

∂vi
∂x −∂si
∂x

Therefore, by summing these terms on the index i we can
expect the KL divergence gradient to be small in magnitude
compared to those of the ℓ1 norm. However, it does not
seem possible to prove this result rigorously without further
assumptions on the data distribution or the mode of conver-
gence of S.
15

