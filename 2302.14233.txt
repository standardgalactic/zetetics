Goal Driven Discovery of Distributional Differences via Language Descriptions
Ruiqi Zhong 1 Peter Zhang 1 Steve Li 2 Jinwoo Ahn 1 Dan Klein 1 Jacob Steinhardt 1
Abstract
Mining large corpora can generate useful discov-
eries but is time-consuming for humans. We for-
mulate a new task, D5, that automatically dis-
covers differences between two large corpora in
a goal-driven way. The task input is a problem
comprising a research goal (“comparing the side
effects of drug A and drug B”) and a corpus pair
(two large collections of patients’ self-reported
reactions after taking each drug). The output is
a language description (discovery) of how these
corpora differ (patients taking drug A “mention
feelings of paranoia” more often). We build a D5
system, and to quantitatively measure its perfor-
mance, we 1) contribute a meta-dataset, OPEND5,
aggregating 675 open-ended problems ranging
across business, social sciences, humanities, ma-
chine learning, and health, and 2) propose a set
of uniﬁed evaluation metrics: validity, relevance,
novelty, and signiﬁcance. With the dataset and the
uniﬁed metrics, we conﬁrm that language models
can use the goals to propose more relevant, novel,
and signiﬁcant candidate discoveries. Finally, our
system produces discoveries previously unknown
to the authors on a wide range of applications in
OPEND5, including temporal and demographic
differences in discussion topics, political stances
and stereotypes in speech, insights in commercial
reviews, and error patterns in NLP models.
1. Introduction
The processes of generating discoveries from large corpora
are ad hoc and laborious. For example, to compare the side
effects of drug A and drug B, doctors might inspect two
large corpora of patients’ self-reported reactions after taking
each drug; based on ad hoc insights, they hypothesize that
more patients taking drug A “mentions feelings of paranoia”,
and then validate this hypothesis by laboriously inspecting
text samples from the two corpora.
1University of California, Berkeley 2Harvard University. Corre-
spondence to: Ruiqi Zhong <ruiqi-zhong@berkeley.edu>.
Machine learning (ML) can potentially accelerate these dis-
covery processes. However, ML requires a uniﬁed evalu-
ation metric and input-output space, while the evaluation
of discoveries and the data that inform them vary across
applications. We need a homogeneous formulation to cast
heterogeneous discovery processes as an ML task, which
can be automated, benchmarked, learned, and analyzed.
We formalize one family of these processes as an ML task
with uniﬁed metrics and input-output space: goal driven
discovery of differences between text distributions via lan-
guage descriptions (D5). As shown in Figure 1, the input
to the D5 task is a “problem” comprising a description of
the research goal (understanding side effects) and a corpus
pair (text samples from the distributions of self-reported
reactions after taking each drug). The output is a “discov-
ery” represented as a natural language predicate (“mentions
feelings of paranoia”). We evaluate a discovery using two
categories of criteria (Section 3): (1) validity: it should de-
scribe a true difference (Zhong et al., 2022); and (2) mean-
ingfulness: it needs to be driven by the research goal and
thus relevant, novel, and signiﬁcant (McGarry, 2005).
To study the D5 task, we curate OPEND5, a meta-dataset
that aggregates 675 open-ended D5 problems ranging across
business, social sciences, humanities, health, and machine
learning (Section 2, Figure 2), comprising 4.4 million text
samples in total across problem corpora. We collected these
675 problems by surveying papers that focus on text analysis
(e.g. Nguyen et al. (2020)), brainstorming research goals,
scraping the corresponding corpora, and post-processing
them over nine months. Since we hope to build systems
that can tackle challenging research problems, we included
problems for which we do not currently know the answer,
hoping D5 systems can generate meaningful discoveries.
To tackle problems in OPEND5, we built a D5 system (Sec-
tion 4). Our system ﬁrst proposes hypotheses about how
two corpora differ, given the goal and a subset of samples
from the corpus pair. It then validates which hypotheses are
statistically different on one corpus vs. the other, and ﬁnally
outputs the valid ones as discoveries. Our system closely
mirrors that of Zhong et al. (2022), which also describes
differences in text corpora, but which does not consider the
research goal for either modeling or evaluation. We used
OPEND5 to evaluate our system and found that compared to
arXiv:2302.14233v1  [cs.CL]  28 Feb 2023

OPEND5
Research 
Goal
The original dataset includes patient’s self-reported reactions after taking a drug. The two corpora are 
generated based on what drug the patient has taken. Samples from Corpus A include self-reported 
reactions after taking drug A, while samples from Corpus B include self-reported reactions after taking 
drug B. I am a doctor. My goal is to understand the side effects of drug A. 
Corpus 
Pair
-Coughing for two months
-Felt sleepy today
-[~4K more samples omitted for brevity]
-Slowly recovering. 
-[~4K more samples omitted for brevity]
-Even little sound dries me crazy
- Feelig to worried to focus
-[~4K more samples omitted for brevity]
-My family complaints I’m too irritated
-[~4K more samples omitted for brevity]
Corpus A
Corpus B
Research 
split
Validation 
split
Output 
Discovery
Corpus A has more samples that 
“mention feelings of paranoia”
Input 
Problem
Figure 1. Each problem in OPEND5 contains 1) a corpus pair, which has ∼17K samples on average and is partitioned into two halves
called “research split” and “validation split”, and 2) a natural language description of the research goal, which also contains information
about how the corpus pair was collected. A D5 system takes the goal and the research split as inputs and generates valid and meaningful
discoveries in natural language as outputs. The underlined texts in the research goal vary across problems, while the rest are templates.
the baseline system from Zhong et al. (2022), incorporating
the goal produces relevant hypotheses 31% of the time more
often (21% for novelty and 28% for signiﬁcance).
Besides quantitative evaluation, a repository of open prob-
lems like OPEND5 allows the following list of operations:
Automate discoveries. Every time we build a better D5
system, we can apply it to a repository of open problems
and send the discoveries to researchers who posed them. We
show this paradigm is plausible by using our system to auto-
matically produce useful discoveries on OPEND5 (Section
5), including insights from commercial reviews, temporal
and demographic differences in discussion topics, politi-
cal stances and stereotypes in speeches, differences in lyric
styles, and error patterns in NLP systems. We anticipate
future systems to produce more discoveries.
Train better D5 systems. Like other machine learning
tasks, we can train a system once we have a dataset. We
describe a self-supervised learning algorithm that uses a
repository of problems (without solutions) to train LMs to
propose more valid hypotheses (Section 6). As a proof-of-
concept, we show that it can make LMs better describe the
differences between groups of text samples.
Analyze the limitations of our evaluation. Using con-
crete examples from OPEND5, we show that our current
evaluation metrics do not encourage diverse ﬁndings, do
not always produce causal conclusions, and cannot evaluate
discoveries involving heavy expert knowledge (Section 7).
These analyses inform areas for future improvement.
To conclude, by collecting OPEND5, we show that D5 can
be benchmarked, automated, analyzed, and learned, just
like any other machine learning task. Since the authors are
not domain experts in most of the open problems we have
collected, we hope future research can improve by gathering
feedback from domain experts and a more authentic meta-
dataset, potentially accelerating discoveries.1
2. OPEND5
We contribute a new meta-dataset, OPEND5, which con-
tains 675 open-ended D5 problems. We describe how the
problems are represented, how we collected them, and their
open-ended nature.
Representation. Each problem in OPEND5 is represented
by 1) a corpus pair, Corpus A and Corpus B, with on average
17K samples, and 2) a description of the research goal. In
this task, the input is a research goal and a corpus pair, while
the outputs are valid and meaningful discoveries in natural
language predicates (Figure 1). For example, Corpus A/B
can be self-reported reactions after taking drug A/B, and
the research goal is to understand the side effects of drug A;
one discovery can be that Corpus A has more samples that
“mentions feelings of paranoia”.
We use 50% of each corpus as the “research” split and
50% as the “validation” split. The system can only access
the research split, while the validation split is reserved for
1We
share
the
code
at
https://github.com/
ruiqi-zhong/D5,
and
the
dataset
at
https://doi.
org/10.5281/zenodo.7683302. The license information
is in Appendix G.

OPEND5
Domain
Example Datasets
How the Corpus Pairs are Generated
Corpus A
Corpus B
87 Business problems
Commercial 
Reviews
Airline reviews
1st-class passenger reviews
Economy passenger reviews
Product Reviews
Reviews that give 10 stars
Reviews that give 0 star
Finance
YC startups
Successful startup descriptions
Failed startup descriptions
News Headlines
Top headlines when S&P rises
Top headlines when S&P falls
278 Social Sciences problems
Politics
Administration policy
Admin policy from Trump
Admin policy from Obama
News
Reuters headlines
Headlines from 2014
Headlines from 2015
Language
Craiglist Negotiations
Dialogue from successes
Dialogue from failures
Diplomacy Dialogues
Lies
Honest statements
Sociology
Happy moments
Self-reported happy moments from females
Self-reported happy moments from males
Rate My Professor
Reviews of female lecturers
Reviews of male lecturers
169 Humanities problems
Arts
Music lyrics
Drake rap lyrics
Kanye rap lyrics
Education
Student essays
Essays that received full score
Essays with only partial credit
10 Health problems
Health
Doctor’s note
Patients diagnosed with pneumonia
Patients not diagnosed with pneumonia
131 Machine Learning problems
Machine 
Learning
NLI — distribution shift
Samples from SNLI
Samples from MNLI
QQP — spurious correlation Individual questions with label “paraphrase”
Individual questions with label “non-paraphrase”
LM’s output
Generations from one LM
Generations from another LM
inputs — error analysis
Inputs where one model is correct
Inputs where one model is wrong
WikiText — clustering
Samples from one cluster
Samples not from a cluster
Figure 2. OPEND5 contains 675 problems, and we show some examples here by row. Appendix G includes the citations.
the evaluators to validate the discovery. A validation split
prevents overﬁtting the discoveries to the given corpora and
is analogous to the train-test split in machine learning.
Collection Process. We collected 675 problems in total,
ranging across business, social sciences, humanities, health,
and machine learning; see Figure 2 for a few examples. To
build OPEND5, two of the authors performed an extensive
literature review on problems that could potentially bene-
ﬁt from our system, e.g., reading survey papers (Nguyen
et al., 2020) and courses2 on computational social sciences,
and skimming through the ACL proceedings from the past
decade3 and datasets from Kaggle4 that has an NLP tag; we
then brainstormed the research goals, scraped/generated the
corresponding corpora, and post-processed them over nine
months, resulting in 675 problems. Appendix G includes
the complete list of citations for the datasets we aggregated.
Open-Endedness. Since we hope to build systems that
can tackle challenging research problems, we did not avoid
cases where we do not know the ground truth answer. On
the contrary, we favored problems which we do not have an
answer for. This means that, for some problems, it might
be infeasible to produce any meaningful discovery. This
is different from standard benchmarking practices, where
humans can provide a reference solution to evaluate an
2http://www1.cs.columbia.edu/˜smara/
teaching/S18/
3https://aclanthology.org
4https://www.kaggle.com
AI system. However, even though we do not know the
ground truth, once a system produces a discovery, we can
still evaluate it. We present our evaluation metrics in the
next section.
3. Evaluation
For the research goal of comparing the side effects of drug
A and drug B, how do we evaluate a system-generated dis-
covery that Corpus A “mention feelings of paranoia” more
often? First, it needs to be valid, such that indeed more
samples from Corpus A satisfy this predicate, which can
be evaluated (approximately) objectively. Second, it needs
to be meaningful to the research goal of understanding
side effects, which depends on the researcher’s subjective
judgement. We deﬁne validity and meaningfulness below.
3.1. Validity
Similar to Zhong et al. (2022), we require an output discov-
ery h to be a truth predicate on a text sample. For example,
if h = “mentions about family and children”, then h is true
on the string x1 = “My daughter loves me.” and false on the
string x2 = “I’m going to school”. Deﬁne T(h, x) ∈[0, 1]
as “the certainty that h is true on x”, e.g., T(h, x1) ≈1
and T(h, x2) ≈0. We approximate T(h, x) by asking three
Turkers how certain they are and averaging their responses
(see Appendix A for more details). Let Dval
A and Dval
B denote
the validation set for Corpus A and B, respectively. Then

OPEND5
we deﬁne the validity V as
V (h) := Ex∼Dval
A [T(h, x)] −Ex∼Dval
B [T(h, x)].
(1)
In practice, we do not have the budget to compute V (h)
since it requires asking humans to read the entire validation
split just to evaluate one single discovery h; therefore, we
approximate this quantity by choosing a subset of samples
from Corpus A and Corpus B to estimate V . We compute
a p-value for the null hypothesis that V ≤0 by conducting
a t-test to compare the mean of T(h, x) on the subset from
Corpus A to that of Corpus B. A discovery should ideally
have a large V value and a small p-value.
3.2. Meaningfulness
Not every valid discovery is meaningful. For example, if
the goal is to understand the topical differences between
news from 2008 (Corpus A) and news from 2007 (Corpus
B), the discovery that Corpus A “contains news from 2008”
is completely valid by deﬁnition but meaningless, since it
provides only trivial information and is irrelevant to the goal
of understanding topical differences.
McGarry (2005) surveyed a list of desirable properties for
discovery, and we condensed them into three submetrics to
rate how meaningful a discovery is based on the research
goal: 1) relevance, 2) novelty, and 3) signiﬁcance. We
evaluate these independently of validity and assume that the
discovery is already valid. For example, the discovery that
“something can travel faster than light” is meaningful if true,
even though it is highly implausible.
We rate each submetric with 0⃝, 1⃝, or 2⃝, where higher is
better. The evaluation instructions are below.
Relevance. How relevant the discovery is to the goal. For
example, suppose we were a student comparing essays rated
as convincing vs. not convincing to ﬁgure out what writing
style is convincing. Then:
• The discovery “write in ﬁrst person” is directly related
to the writing style, so we rate it 2⃝.
• The discovery “use the word “I””, is not exactly a writ-
ing style, but can still inform the relevant underlying
principle of “write in ﬁrst person”, so we rate it 1⃝.
• The discovery “argue for abortion” does not tell us
about the underlying writing style, so we rate it 0⃝.
Novelty. The difﬁculty of generating the discovery, e.g. can
we think of the discovery in 5 minutes with the goal but
without looking at the corpora? For example, suppose we
were an airline manager trying to ﬁnd improvements to the
ﬂight experience, and we were comparing negative reviews
vs. positive reviews. Then:
• The discovery “contain more negative language” is
almost certain for negative reviews, so we rate it 0⃝.
• The discovery “complain about the crew members” is
not entirely novel, but is not tautologically true and
hence requires conﬁrmation, so we rate it 1⃝.
• The discovery “mention a language barrier with the
crew members” is speciﬁc and hard to think of without
looking at the data, so we rate it 2⃝.
Note that our evaluation is “blinded to the samples”: we
still consider a discovery novel as long as it is hard to think
of before looking at the corpora, even if it might be easy
to think of after looking at the corpora. For example, the
physical law that F = ma is easy to observe if we have
collected and plotted the data on acceleration, mass, and
force; however, it might be difﬁcult to think of before we
see any such data, so we consider it novel.
Signiﬁcance. Given the research goal, how beneﬁcial is
it to learn the discovery for the ﬁrst time? For example,
suppose we were an Amazon retailer trying to ﬁgure out
what customers like and dislike about my product based on
negative reviews and positive reviews. Then:
• The discovery “accuses the team pushing out a bad
product” is not signiﬁcant since it cannot direct the
retailer to improve the product, so we rate it 0⃝.
• The discovery “asks for a more durable product” gives
some hints about how to improve the product, but isn’t
sufﬁciently helpful on its own, so we rate it 1⃝.
• The discovery “says the wrench is missing” can lead to
concrete actions for improvement, so we rate it 2⃝.
To conclude, an ideal discovery would have a high V value
with a small p-value and achieve ratings of 2⃝across all of
relevance, novelty, and signiﬁcance. The latter three sub-
metrics are inherently subjective; however, the next section
shows that we can still use them to compare hypothesis
proposers and draw robust conclusions.
4. Method
We describe our D5 system, which maps from a corpus pair
and a research goal to a set of natural language predicates.
Our system is inspired by a two-stage model of how hu-
mans discover patterns in data: creatively brainstorming
hypotheses and then rigorously validating them on the data
(Ludwig & Mullainathan, 2022). Analogously, we ﬁrst 1)
propose hypotheses conditioned on the research goal and a
subset of samples from the corpus pair (Section 4.1), and
then 2) validate each hypothesis whether it is more often
true on one corpus than the other and outputs the valid ones
as the ﬁnal discoveries (Section 4.2); see Appendix Figure
5 for a more illustrative overview. Our system closely mir-
rors that of Zhong et al. (2022), except that we leverage
the research goal to propose more meaningful hypotheses.
Using OPEND5, we quantitatively show in Section 4.3 that
GPT-3 text-davinci-003 (abbreviated as GPT-3) can use the

OPEND5
Group A: I went with grandchildren to butterfly display at Crohn Conservatory
Group A: Watching cupcake wars with my 3 teen children
Group A:
Group B: I was selected as a Junior Engineer for a leading and…
Group B: I came in 3rd place in my Call of Duty video game.
Group B:
(some of the sentences are truncated for brevity) …
The original dataset includes self-reported happy moments and demographic 
characteristics. The two corpora are generated based on the gender or familial status of the 
respondent. Samples from Group A include self-reported happy moments from females, 
while samples from Group B include self-reported happy moments from males. I am a 
sociologist studying intimate relationships. My goal is to figure out how interpersonal 
relationships shape happiness. 
Please write a list of hypotheses  (separated by bullet points "-") of how datapoints from 
Group A differ from those from Group B. Each hypothesis should be formatted as a 
sentence fragment. Here are three examples.
- "talks about politics, such as presidential election.”
- "contains insulting language for immigrants."
- "uses double negation, i.e., using two negations in a sentence."
Based on the two sentence groups (A and B) from the above, more sentences in Group A ...
- “mentions about children and family”
- “mentions about academic relations, such as teachers or students”
- “mentions about
Samples from 
the two corpora
Research Goal
Formatting 
Instructions
Language 
Model Outputs
Check whether the TEXT satisfies a 
PROPERTY. Respond with Yes or No. When 
uncertain, output No. 
Now complete the following example -
input: PROPERTY: mentions about 
children and family
TEXT: I went with grandchildren to 
butterfly display at Crohn Conservatory
output:
Proposer prompt
Check whether the TEXT satisfies a 
PROPERTY. Respond with Yes or No. When 
uncertain, output No. 
Now complete the following example -
input: PROPERTY: mentions about children 
and family
TEXT: I came in 3rd place in my Call of Duty 
video game.
output:
Validator prompt
Pr[NextWord = “Yes”] = 90%
Pr[NextWord = “Yes”] = 5%
// a list of hypotheses not included for brevity
// 20 samples not included for brevity 
// 20 samples not included for brevity 
Figure 3. All underlined content in the prompt differs across problems, while the other content in the prompt is templated. Left: proposer
prompt. The generated hypotheses are in blue. All content with colored background is excluded from the ﬁgure for brevity. For the
baseline of not using the research goal, we removed the “research goal” block from the prompt. Right: the validator prompt.
research goal to propose more meaningful hypotheses.
As indicated in Section 2, our system only accesses the
research split of each corpus, which we denote as Dres
A /Dres
B .
4.1. Hypothesis Proposer
We prompt GPT-3 (Ouyang et al., 2022) to propose hy-
potheses. We construct the prompt by concatenating a few
random samples from Dres
A and Dres
B , the research goal, and
an instruction to output a list of hypotheses. Different from
Zhong et al. (2022), we include the research goal to elicit
meaningful hypotheses. We continue sampling hypotheses
from GPT-3 until we obtain a set of 60 distinct hypotheses,
which we call Hinit. See Figure 3 left for an example prompt,
and Appendix C for additional details.
4.2. Hypothesis Validator
Many hypotheses in Hinit are invalid: they are not more often
true on DA than on DB (i.e. V (h) < 0). To automatically
ﬁlter them out, we use a language model T ′ to simulate the
Turkers’ judgement T and hence approximate the validity
score V of a hypothesis h with V ′(h), where
V ′(h) := Ex∼Dres
A [T ′(h, x)] −Ex∼Dres
B [T ′(h, x)].
(2)
To compute T ′, we ask FLAN-T5 (Chung et al., 2022)
whether x satisﬁes h with the prompt shown in Figure 3
right. To better simulate Turker’s judgment, we collected
additional Turker annotations to ﬁne-tune FLAN-T5 (see
Appendix D for details). We then perform a t-test to com-
pare the mean value of V ′(h, x) on the research split of
Corpus A and the mean value on Corpus B, rule out the
hypotheses with p-value greater than 0.001, and output them
as a set of discoveries. Finally, we obtain additional discov-
eries by repeating the same process but asking our system
to propose and validate hypotheses about Corpus B rather
than Corpus A.
4.3. Goal Leads to More Meaningful Hypotheses
Compared to Zhong et al. (2022), we added the research
goal to our prompt when generating hypotheses. Does this
improve the quality of the proposed hypotheses? To inves-
tigate this, we sampled 100 problems from OPEND5 with
distinct research goals and randomly sampled 2 hypothe-
ses from GPT-3 with and without using research goal (see
Figure 3), resulting in 400 hypotheses to evaluate. Three
authors then rated their meaningfulness based on the three
metrics deﬁned in Section 3, while being blinded about
which hypotheses were generated with the research goal.
The results are shown in Table 1. We found that, when
prompted with the research goal, GPT-3 on average pro-
poses more relevant, novel, and signiﬁcant hypotheses;
additionally, it proposes hypotheses with ratings higher
than
0⃝31%/21%/28% more often in terms of rele-
vance/novelty/signiﬁcance. Since this is a subjective evalua-

OPEND5
with-goal
no-goal
kappa
spearmanr
p of avg
worst p of ind
Relevance
1.68
1.20
0.56
0.71
1 × 10−10
1 × 10−8
Novelty
1.24
0.97
0.37
0.50
5 × 10−6
4 × 10−2
Signiﬁcance
1.56
1.05
0.46
0.64
2 × 10−10
2 × 10−7
Table 1. Left. For each metric, we report the average rating on hypotheses generated with or without using the research goal, and ﬁnd
that the former performs better. Middle. The inter-annotator agreement rate averaged across pairs of author evaluators, measured by
Kappa and Spearman rank coefﬁcient; we ﬁnd substantial correlations between evaluators across all these subjective metrics, with
relevance > signiﬁcance > novelty. Right. We compute the p-values for the null hypothesis that “with-goal and no-goal result in the
same performance”. The p of avg column reports the p-values after we average the ratings from all evaluators, while the “worst p of ind”
column takes the max of all p-values based on ratings of individual evaluators. Overall, the conclusions are statistically signiﬁcant and
they can be robustly reproduced across individual evaluators.
tion, the Kappa inter-annotator agreement is only moderate,
ranging from 0.37 to 0.56. However, we can still robustly
conclude that the model can propose more meaningful hy-
potheses when conditioned on the goal: we calculate the
p-values for the null hypothesis that with-goal and no-goal
have equal performance, and we ﬁnd p-values to be highly
signiﬁcant and robust across evaluators, for all three sub-
metrics. We provide additional analyses in Appendix B.5
5. Application
Every time we build a better D5 system in the future, we
may use it to automatically generate useful discoveries on
an existing aggregation of open problems like OPEND5 and
send the discoveries to researchers who posed the problems.
In this section, we use our current system to automatically
generate discoveries on OPEND5.
5.1. Automatically Generating Discoveries on OPEND5
We ran our system on all problems in OPEND5, obtaining
in total 3296 discoveries across 402 problems. However, we
do not have enough budget to validate every ﬁnding, since
estimating V is expensive (Section 3.1). Therefore, from
the remaining 3296 discoveries, we manually selected 21
discoveries that 1) the authors think are meaningful enough,
2) are representative of potential use cases, 3) do not require
expert knowledge for Turkers to judge, and 4) are likely to
achieve a small p-value with fewer than 200 samples from
Dval
A and Dval
B . We then estimated their validity based on
the procedure described in Section 3.1 by using fewer than
200 samples from the validation split and calculated the p-
5The experiments in this paper were run at different iterations
of the data collection process; since they require intense manual
effort and no automatic metric is available, it is expensive to re-
run them on our ﬁnal polished version. The differences between
iterations are mainly due to 1) noticing data sharing constraints
due to licenses, 2) increasing diversity by including new problems
or removing similar problems, or 3) improving the research goal
description. For reproducibility, we include the set of research
goals for each experiment in our github repo.
values.6 Since we are testing multiple discoveries and each
of them can be statistically signiﬁcant merely due to chance,
we keep 15 discoveries with V that are signiﬁcantly non-
zero with p-value below 7%, a threshold determined by the
Benjamini Hochberg’s procedure with a false discovery rate
of 10%. In other words, fewer than 10% of the discoveries
below are false discoveries in expectation.
5.2. Example Discoveries on OPEND5
For each example discovery, we also report the estimated V
based on Turker’s rating and the AUCROC score of using
the discovery to discriminate samples from Dval
A and Dval
B .
All italicized text in quotes from this section are literal
copies of what our system generated.
Comparing lyrics from different eras.
Compared to
lyrics from the 70s, those from the 80s more often “ref-
erences violence or aggression” (V ≈0.06, AUCROC ≈
0.58).
Analyzing gender differences in self-reported happy
moments. Compared to self-reported happy moments writ-
ten by males, those by females “mentions children or family”
more often (V ≈0.08, AUCROC ≈0.56).
Analyzing errors in NLP systems. We considered the task
of perspectrum classiﬁcation (Chen et al., 2019), which has
the following instruction: “given a perspective and a claim,
classify whether the given perspective supports or under-
mines the claim. If the perspective could possibly convince
someone with different view, it is supporting, otherwise
it is undermining.” We considered two few-shot learning
systems: GPT-3 Instruct Curie (Ouyang et al., 2022) and
Tk-Instruct-11B (Wang et al., 2022). We focused on the
perspectives where the ground truth label is undermining,
and compare the following two corpora: Corpus A – the set
of perspectives where Curie correctly classiﬁes the input as
undermining but Tk-11B is wrong, and Corpus B – the set
where TK-11B is correct while Curie is wrong. We found
6We determined the number of samples s.t. V ′ can achieve a p-
value of 0.005. Estimating V for these discoveries costs ∼$1500.

OPEND5
that Corpus B more often “Uses language that is positive
or uplifting” (V ≈0.12, AUCROC ≈0.67). One possible
explanation is that Curie made many mistakes by misinter-
preting undermining as a label for negative sentiment rather
than a logical relation between the claim and the perspective.
For another example, we compared two natural language
inference models, one trained on MNLI and the other trained
on SNLI. Then we tested them on MNLI and compare two
corpora: Corpus A – the set of inputs where the model
trained with in-distribution data (MNLI) is wrong but the
model trained with out-of-distribution data is correct, and
Corpus B vice versa. We found that the latter more often
“has an informal tone, such as slang or colloquial speech”
(V ≈0.08, AUC-ROC ≈0.62). One possible explanation is
that MNLI contains more different genres and hence more
informal speeches, causing the former model to perform
better on these examples.
Understanding political stances and stereotypes in
speeches. When comparing presidential speeches on immi-
grants from Obama to those from Trump, the former “argues
for a path forward to promote the fair and just treatment of
immigrants” (V ≈0.16, AUCROC ≈0.73), while the latter
more frequently “refers to illegal immigrants as criminals”
(V ≈0.09, AUCROC ≈0.62).
Analyzing airline customer reviews. We compared the
concerns in reviews of the airline Air Canada v.s. its sub-
sidiary, Air Canada Rogue, which is considered a low-price
wing of Air Canada. The latter more often “ mentions lack
of legroom“(V ≈0.16, AUCROC ≈0.68).
Identifying temporal differences in news headlines. We
compared headlines published by ABC news across differ-
ent years. Compared to the year 2014, headlines from the
year 2010 “mention disasters and crimes, such as plane
accidents and assaults” more often (V ≈0.03, AUCROC
≈0.53). Compared to year 2019, year 2020 more often “dis-
cusses coronavirus-related topics” (V ≈0.21, AUCROC ≈
0.65).
Describing distribution shift. We compared the premises
from the SNLI dataset and MNLI dataset, and the former
“involves physical activity, such as walking, playing, climb-
ing, or biking” (V ≈0.13, AUC-ROC ≈0.64). One possible
explanation is that SNLI is based on image captions.
Comparing discussion topics between bots and human
users. We compared the topical differences between tweets
identiﬁed as written by bots vs. human users on Twitter, and
our system ﬁnds that the bots more often “contains keywords
related to business, ﬁnance or trading” (V ≈0.08, AUC-
ROC ≈0.61). One possible explanation is that bots are
frequently used to generate ﬁnance-related scams.
Describing text clusters. We present two example descrip-
tions for text clusters. One from Wikipedia: “references
pop culture, such as movies, books, and television shows.”
(V ≈0.21, AUC-ROC ≈0.73); one from PoetryFounda-
tion.com: “uses vivid imagery and metaphors to convey a
feeling” (V ≈0.09, AUC-ROC ≈0.65).
We hope future works can collect more open problems, al-
lowing D5 systems to produce more impactful discoveries.
6. Self-Supervised Learning
Since the problems in OPEND5 are open-ended, our system
could potentially produce discoveries with higher validity
scores. Therefore, we design a self-supervised learning
algorithm to improve a language model’s ability to propose
more valid hypotheses, using the principle that it is easier
to validate a discovery than to generate one.
Algorithm. Suppose we are given a set of problems for
training and an initial language model minit. Our goal is
to automatically generate a set of prompt-completion pairs
to ﬁne-tune minit so that it can propose hypotheses that are
more valid. To generate a prompt, we randomly sample a
problem and create a proposer prompt following the pro-
cedure in Section 4.1. To generate the desired completion
given a prompt, we sample multiple hypotheses from minit,
approximate their V ′ score on the samples in the proposer
prompt with the same language model minit (Section 4.2),
and select the highest scoring hypothesis. Finally, we use
the prompt-completion pairs to ﬁne-tune minit.
However, since we cannot ﬁne-tune instruction-tuned GPT-
3, we can only experiment with Flan-T5 (Chung et al., 2022),
an open-sourced instruction-tuned model that might only
work well for easier “mini-problems”. Therefore, as a proof
of concept, we test our algorithms for describing groups of
four samples, where each group comes from a text cluster.
As an overly simpliﬁed example, we will give the LM the
prompt “Group A: 1. dog 2. cat 3. pig 4. cow. Group B: 1.
phone 2. laptop 3. desk 4. cup” as an input and the LM can
output “mentions an animal” as a hypothesis of how group
A differs from group B.
Data. We created 33 corpora by merging all corpora in
OPEND5 with the same domain, and automatically gener-
ated 4503 text clusters using RoBERTa embeddings (Aha-
roni & Goldberg, 2020). We focused on clustering because
it can automatically generate a large amount of semantically
coherent groups of samples. To create a pair of four samples,
we randomly sampled a corpus, sampled two clusters within
that corpus, and took four random samples from each cluster.
To test cross-corpus generalization, we reserved 28 of the
33 corpora to create mini-problems for evaluation, using the
rest for training. We used Flan-T5 (Chung et al., 2022) as
minit and sampled hypotheses with a temperature of 0.8. For
training, we sampled 30,000 mini-problems and selected

OPEND5
the best of eight hypotheses generated by minit as the target
completion; for evaluation, we sampled 200 mini-problems
to calculate V with Turkers and 1500 mini-problems to
calculate V ′ automatically.
Results. We evaluated randomly sampled hypotheses from
the language model before and after self-supervised training.
The automated “self-evaluation” validity score V ′ improves
substantially from 0.22 to 0.37, and the “true” validity score
V according to Turker evaluation improves from 0.07 to
0.10, with a p-value of 0.02. This result provides preliminary
evidence that our algorithm (or similar variants) could be
applied to a large set of problems to improve the validity
of the hypotheses; we expect future validators to simulate
human judgments better, hence decreasing the approximated
gap of improvement between V and V ′.
7. Analysis
We use OPEND5 to analyze the limitations of our metrics,
and we discuss more limitations and future work in Ap-
pendix E.
Hypotheses about the corpora might not be appropri-
ate predicates on individual samples. When comparing
highly rated deﬁnitions from UrbanDictionary.com
to others, our system generates the hypothesis that the for-
mer “is more likely to include slang or colloquial terms.”
This is a statement about a collection of text samples, but
the validator requires the hypothesis h to be a predicate on
individual text samples x. To address this problem, we use
GPT-3 to automatically detect and remove comparatives
from the hypotheses, e.g., rewriting the hypothesis above to
“include slang or colloquial terms.”
However, some versions of this problem were harder to
remove. For example, when comparing reviews from Amer-
ican Airlines (AA) ﬂights and Delta Airlines to understand
which aspects of each airline are doing better/worse, the
proposer generated the hypothesis “mentions American Air-
lines’ staff being unfriendly and unhelpful”. Interpreted
literally, this hypothesis can only be true on the corpus of
AA reviews, since it presupposes the review to be about AA.
The correct predicate for use on individual samples should
instead be “mentions staff being unfriendly and unhelpful”
(without the words “American Airlines”’). Therefore, future
systems should explicitly convert corpus-level statements
to their corresponding correct predicates, and the metrics
should evaluate whether the validity of the predicates im-
plies the corpus-level statements.
Our metrics do not evaluate diversity. There are often
multiple valid and meaningful discoveries, and our system
should ideally generate all of them. For example, when
comparing low-rating and high-rating reviews to understand
what stands out to customers, both “mentions the hidden
fees and poor customer service at the airport” and “men-
tions the airline charging extra for carry-on items ” could
be valid discoveries. However, our current system some-
times repeats a discovery using similar paraphrases, e.g.,
“mentions the rude and unprofessional attitude of the staff”
and “mentions the staff being rude and unhelpful”. Future
evaluation metrics can take diversity into account.
Interpreting discoveries requires domain experts. We
used Turkers’ judgment when computing T(h, x) to judge
the validity of a discovery. However, many discoveries
require expert knowledge to interpret properly. For exam-
ple, it requires medical training to reliably judge whether
a self-reported drug-use experience satisﬁes “mentions
psychedelics, such as LSD and shrooms.”
Correlation ̸= causation. Our metrics currently do not
evaluate whether the discovery is causally related to how
the corpus pair was generated. For example, when compar-
ing self-reported happy moments from females and males,
even if the former corpus has more samples that “mention
children and family”, it does not necessarily imply that fam-
ily plays a more important role in inter-personal relations
for females; an alternative hypothesis is that females might
mention people in general more often than males, hence
leading to the observation that they mention family more of-
ten. Spurious correlations could also sneak into our validity
evaluation: for example, if the Turkers implicitly associate
female activities as family-related (Greenwald & Banaji,
1995), we might falsely make this discovery due to evalu-
ator biases. Future metrics should also consider plausible
alternative hypotheses to evaluate causality and control the
potential biases from the human evaluators. We should
also treat the discovery from D5 with caution to prevent
automating and amplifying societal biases.
8. Related Work
Inductive Reasoning with NLP Models. Recent works
show that language models are capable of inductive reason-
ing under restricted settings, discovering patterns from a set
of text data points and describing them with language (Hon-
ovich et al., 2022). Zhou et al. (2022) and Ye et al. (2022)
use this capability to improve zero/few-shot accuracy by in-
ferring the most likely instruction using input-output exam-
ple(s) of the target task. Zhong et al. (2022) and Singh et al.
(2022) use this capability to discover patterns in datasets,
and we improve by building a meta-dataset of open-ended
problems and require the discovery to be meaningful.
ML models can also perform inductive reasoning in other
modalities, such as vision. For example, Hernandez et al.
(2021) describes visual features that activate a neuron; Zhu
et al. (2022) describes distribution shifts between the train-
ing distribution and the test distribution for images; and

OPEND5
Eyuboglu et al. (2022) describes errors made by vision mod-
els. We hope future models can perform inductive reasoning
in additional modalities, such as sound (Aghajanyan et al.,
2023) or physical senses (Thomason et al., 2016).
Automated Discovery. It is not new to automatically dis-
cover patterns by learning from empirical data. To list a few
classical methods, linear regression analyzes the effect of
each real-valued feature by interpreting the learned weights
(Draper & Smith, 1998); n-gram models can extract discrim-
inative phrases, thus yielding insights about corpus-level
differences (Manning & Schutze, 1999); small decision
trees can extract interpretable if-then statements (Letham
et al., 2015); and an entity embedding model learned on ex-
isting relations between entities can predict unseen relations
(Socher et al., 2013). In comparison, D5 produces discov-
eries in the form of natural language predicates, which are
ﬂexible and interpretable; additionally, it is more directed
at the research goal, while machine learning classiﬁers like
linear regressions will pick up any discriminative features.
Epistemology. While the process of validating a hypoth-
esis is well-formulated, it is much less well-understood
how to automatically generate hypotheses and decide what
discoveries are meaningful (Shapere, 1964; Heckman &
Singer, 2017). Related works in this area have been sparse,
among which McGarry (2005) sketches high-level princi-
ples for evaluating knowledge discoveries and Ludwig &
Mullainathan (2022) proposes to crowd-source hypothe-
ses from MTurk workers. We concur with the perspective
of Polanyi et al. (2000) that meaningfulness of a hypoth-
esis cannot be explicitly verbalized with simple logic but
is dependent on implicit community norms; therefore, the
process of proposing hypotheses should be learned from
empirical data (e.g. pre-training) rather than deduced from
a priori analysis of concepts (Quine, 1969). We hope con-
tributions from other domains can provide more empirical
data on what discoveries are meaningful, hence guiding our
system to produce more important discoveries.
9. Conclusion
We formalized the task of D5, which discovers corpus-
level differences via language descriptions in a goal-driven
way. We deﬁned its evaluation metrics – validity, relevance,
novelty, and signiﬁcance – and collected a meta-dataset,
OPEND5, to evaluate D5 systems. We presented 10 use
cases on D5, proposed a self-supervised learning algorithm,
and analyzed the limitation of the current evaluation metrics.
To conclude, like any other traditional machine learning task,
D5 can be automated, benchmarked, learned, and analyzed.
We hope future research can improve by gathering feedback
from domain experts and a more authentic meta-dataset,
potentially accelerating future discoveries.
Acknowledgement
We thank Xiaochuang Han and Sam Bowman for their early
discussions on this project. We thank Cathy Chen, Erik
Jones, Jessy Lin, Alex Pan, Chenglei Si, Xi Ye, and Tianyi
Zhang for their helpful feedback on the paper draft. We
thank OpenAI and Anthropic for providing model access.
Individual Contributions
Ruiqi Zhong proposed the D5 task, drew the conceptual
connection to naturalistic epistemology, and proposed to
treat it as a standardized machine learning task by collecting
a dataset; co-designed the evaluation metric; collected most
of the machine learning problems in OPEND5; conducted
all the experiments; drafted the entire paper.
Peter Zhang collected all the non-machine learning prob-
lems and the text clustering problems in OPEND5; co-
designed the evaluation metrics, organized the hypotheses
evaluation, and contributed directions for future work; left
feedback on the paper draft.
Steve Li led the development of the annotation interface de-
scribed in Appendix F; provided feedback on the evaluation
metrics and participated in the evaluation; left feedback on
the paper draft.
Jinwoo Ahn provided feedback on the annotation interface;
provided feedback on the evaluation metrics and participated
in the evaluation; left feedback on the paper draft.
Dan Klein left feedback on the title, abstract, and intro.
Jacob Steinhardt provided guidance throughout the project
and left detailed feedback on the entire paper.
References
The hewlett foundation: Automated essay scoring, 2012.
URL
https://kaggle.com/competitions/
asap-aes.
The hewlett foundation:
Short answer scoring, 2013.
URL
https://kaggle.com/competitions/
asap-sas.
Ad observer. https://adobserver.org/, 2021. Accessed: 2022-
12-30.
Aghajanyan, A., Yu, L., Conneau, A., Hsu, W.-N., Ham-
bardzumyan, K., Zhang, S., Roller, S., Goyal, N.,
Levy, O., and Zettlemoyer, L.
Scaling laws for gen-
erative mixed-modal language models. arXiv preprint
arXiv:2301.03728, 2023.
Aharoni, R. and Goldberg, Y.
Unsupervised domain
clusters in pretrained language models.
In Proceed-
ings of the 58th Annual Meeting of the Association

OPEND5
for Computational Linguistics, pp. 7747–7763, Online,
July 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.692.
URL https:
//aclanthology.org/2020.acl-main.692.
Alali, M., Syed, S., Alsayed, M., Patel, S., and Bodala,
H. Justice: A benchmark dataset for supreme court’s
judgment prediction. arXiv preprint arXiv:2112.03414,
2021.
Asai, A., Evensen, S., Golshan, B., Halevy, A., Li, V.,
Lopatenko, A., Stepanov, D., Suhara, Y., Tan, W.-C.,
and Xu, Y. Happydb: A corpus of 100,000 crowdsourced
happy moments. arXiv preprint arXiv:1801.07746, 2018.
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,
et al. Training a helpful and harmless assistant with rein-
forcement learning from human feedback. arXiv preprint
arXiv:2204.05862, 2022.
Barnum, M. and Lo, J. Is the npt unraveling? evidence from
text analysis of review conference statements. Journal of
Peace Research, 57(6):740–751, 2020.
Baturo, A., Dasandi, N., and Mikhaylov, S. J. Understand-
ing state preferences with text as data: Introducing the
un general debate corpus. Research & Politics, 4(2):
2053168017712821, 2017.
Bhalotia, A. Yc company scraper. https://github.
com/akshaybhalotia/yc_company_scraper,
2022.
Bird, S., Klein, E., and Loper, E. Natural language process-
ing with Python: analyzing text with the natural language
toolkit. ” O’Reilly Media, Inc.”, 2009.
Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D.
The snli corpus. 2015.
Bramhecha,
D.
Poetry Foundation Poems,
2019.
URL
https://www.kaggle.com/datasets/
tgdivy/poetry-foundation-poems.
Card, D., Chang, S., Becker, C., Mendelsohn, J., Voigt, R.,
Boustan, L., Abramitzky, R., and Jurafsky, D. Repli-
cation code and data for “Computational analysis of
140 years of US political speeches reveals more posi-
tive but increasingly polarized framing of immigration”
[dataset]. https://github.com/dallascard/us-immigration-
speeches/, 2022.
Chalkidis, I., Androutsopoulos, I., and Aletras, N. Neu-
ral legal judgment prediction in english. arXiv preprint
arXiv:1906.02059, 2019.
Chen, S., Khashabi, D., Yin, W., Callison-Burch, C., and
Roth, D. Seeing Things from a Different Angle: Discover-
ing Diverse Perspectives about Claims. In Proc. of the An-
nual Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL), 2019.
URL http://cogcomp.org/papers/CKYCR19.
pdf.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-ﬁnetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
M., and Toutanova, K. Boolq: Exploring the surprising
difﬁculty of natural yes/no questions. arXiv preprint
arXiv:1905.10044, 2019.
Draper, N. R. and Smith, H. Applied regression analysis,
volume 326. John Wiley & Sons, 1998.
Eyuboglu, S., Varma, M., Saab, K., Delbrouck, J.-B., Lee-
Messer, C., Dunnmon, J., Zou, J., and R´e, C. Domino:
Discovering systematic errors with cross-modal embed-
dings. arXiv preprint arXiv:2203.14960, 2022.
Gao, Y., Jang, J., and Yang, D. Understanding the usage
of online media for parenting from infancy to preschool
at scale. In Proceedings of the 2021 CHI Conference on
Human Factors in Computing Systems, pp. 1–12, 2021.
Greenwald, A. G. and Banaji, M. R. Implicit social cogni-
tion: attitudes, self-esteem, and stereotypes. Psychologi-
cal review, 102(1):4, 1995.
Habernal, I. and Gurevych, I. Which argument is more
convincing? Analyzing and predicting convincingness
of Web arguments using bidirectional LSTM. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pp. 1589–1599, Berlin, Germany, 2016. As-
sociation for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P16-1150.
Hartman, K.
Advertisement Transcripts from Various
Industries, 2019.
URL https://tinyurl.com/
5w36dwdx.
He, H., Chen, D., Balakrishnan, A., and Liang, P. Decou-
pling strategy and generation in negotiation dialogues,
2018.
He,
J.
Big Data Set from RateMyProfessor.com
for Professors’ Teaching Evaluation, 2020.
URL
https://data.mendeley.com/datasets/
fvtfjyvw7d/2.

OPEND5
He, S. Goodbye world: using natural language processing to
identify suicidal posts, 2021. URL https://github.
com/hesamuel/goodbye_world.
Heckman, J. J. and Singer, B. Abducting economics. Amer-
ican Economic Review, 107(5):298–302, 2017.
Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T.,
Torralba, A., and Andreas, J. Natural language descrip-
tions of deep visual features. In International Conference
on Learning Representations, 2021.
Honovich, O., Shaham, U., Bowman, S. R., and Levy,
O.
Instruction induction:
From few examples to
natural language task descriptions.
arXiv preprint
arXiv:2205.10782, 2022.
Hossain, N., Krumm, J., and Gamon, M.
” president
vows to cut¡ taxes¿ hair”: Dataset and analysis of cre-
ative text editing for humorous headlines. arXiv preprint
arXiv:1906.00274, 2019.
Kaggle.
TMDB 5000 Movie Dataset, 2018.
URL
https://www.kaggle.com/datasets/tmdb/
tmdb-movie-metadata.
Kulkarni, R.
A Million News Headlines, 2018.
URL
https://doi.org/10.7910/DVN/SYBGZL.
Kulkarni, R. The Examiner - Spam Clickbait Catalog, 2020a.
URL
https://www.kaggle.com/datasets/
therohk/examine-the-examiner.
Kulkarni,
R.
Urban
Dictionary
Words
And
Deﬁnitions,
2020b.
URL
https://
www.kaggle.com/datasets/therohk/
urban-dictionary-words-dataset.
Kulkarni, R.
India News Headlines Dataset, 2022.
URL
https://www.kaggle.com/datasets/
therohk/india-headlines-news-dataset.
Letham, B., Rudin, C., McCormick, T. H., and Madigan, D.
Interpretable classiﬁers using rules and bayesian analysis:
Building a better stroke prediction model. The Annals of
Applied Statistics, 9(3), Sep 2015. ISSN 1932-6157. doi:
10.1214/15-aoas848.
URL http://dx.doi.org/
10.1214/15-AOAS848.
Lim, D. and Benson, A. R. Expertise and dynamics within
crowdsourced musical knowledge curation: A case study
of the genius platform. In ICWSM, pp. 373–384, 2021.
Liu, A., Swayamdipta, S., Smith, N. A., and Choi, Y.
Wanli: Worker and ai collaboration for natural lan-
guage inference dataset creation, January 2022. URL
https://arxiv.org/pdf/2201.05955.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692, 2019.
Liu,
Z.
Reuter 50 50 Data Set,
2011.
URL
https://archive.ics.uci.edu/ml/
datasets/Reuter_50_50.
Ludwig, J. and Mullainathan, S. Algorithmic behavioral sci-
ence: Machine learning as a tool for scientiﬁc discovery.
Chicago Booth Research Paper, (22-15), 2022.
Manning, C. and Schutze, H. Foundations of statistical
natural language processing. MIT press, 1999.
McGarry, K.
A survey of interestingness measures for
knowledge discovery. The knowledge engineering review,
20(1):39–61, 2005.
Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
sentinel mixture models, 2016.
Mish, N. Federal Reserve Governors Speeches 1996 - 2020,
2020. URL https://tinyurl.com/3j2e79a6.
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-
task generalization via natural language crowdsourcing
instructions. In ACL, 2022.
Misra, R. and Arora, P. Sarcasm detection using hybrid
neural network. arXiv preprint arXiv:1908.07414, 2019.
Misra, R. and Grover, J. Sculpting Data for ML: The ﬁrst act
of Machine Learning. 01 2021. ISBN 9798585463570.
Moniz, N. and Torgo, L. Multi-source social feedback of
online news feeds. CoRR, [Web Link], 2018.
Mouill´e,
M.
Kickstarter Projects,
2017.
URL
https://www.kaggle.com/datasets/
kemical/kickstarter-projects?select=
ks-projects-201612.csv.
Nguyen, D., Liakata, M., DeDeo, S., Eisenstein, J., Mimno,
D., Tromble, R., and Winters, J.
How we do things
with words: Analyzing text as social and cultural data.
Frontiers in Artiﬁcial Intelligence, 3:62, 2020.
Ni, J., Li, J., and McAuley, J. Justifying recommendations
using distantly-labeled reviews and ﬁne-grained aspects.
In Proceedings of the 2019 conference on empirical meth-
ods in natural language processing and the 9th interna-
tional joint conference on natural language processing
(EMNLP-IJCNLP), pp. 188–197, 2019.
O’Brien, E. iterative/aita dataset: Praw rescrape of entire
dataset, February 2020. URL https://doi.org/
10.5281/zenodo.3677563.

OPEND5
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al.
Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155, 2022.
P´erez-Rosas, V. and Mihalcea, R. Experiments in open
domain deception detection. In Proceedings of the 2015
conference on empirical methods in natural language
processing, pp. 1120–1125, 2015.
P´erez-Rosas, V., Abouelenien, M., Mihalcea, R., and Burzo,
M. Deception detection using real-life trial data. In Pro-
ceedings of the 2015 ACM on International Conference
on Multimodal Interaction, pp. 59–66, 2015.
P´erez-Rosas, V., Kleinberg, B., Lefevre, A., and Mihalcea,
R. Automatic detection of fake news. arXiv preprint
arXiv:1708.07104, 2017.
Peskov, D., Cheng, B., Elgohary, A., Barrow, J., Danescu-
Niculescu-Mizil, C., and Boyd-Graber, J. It takes two
to lie: One to lie and one to listen. In Association for
Computational Linguistics, 2020.
Pestian, J. P., Brew, C., Matykiewicz, P., Hovermale, D.,
Johnson, N., Cohen, K. B., and Duch, W. A shared
task involving multi-label classiﬁcation of clinical free
text. In Biological, translational, and clinical language
processing, pp. 97–104, Prague, Czech Republic, June
2007. Association for Computational Linguistics. URL
https://aclanthology.org/W07-1013.
Polanyi, M., Ziman, J., and Fuller, S.
The republic of
science: its political and economic theory minerva, i
(1)(1962), 54-73. Minerva, 38(1):1–32, 2000.
Price, I., Gifford-Moore, J., Fleming, J., Musker, S., Roich-
man, M., Sylvain, G., Thain, N., Dixon, L., and Sorensen,
J. Six attributes of unhealthy conversation. arXiv preprint
arXiv:2010.07410, 2020.
Progress, D. Statements of Administration Policy, 2022.
URL
https://github.com/unitedstates/
statements-of-administration-policy#
statements-of-administration-policy.
PromptCloud.
U.S.
Technology
Jobs
on
Dice.com,
2017.
URL
https://www.
kaggle.com/datasets/PromptCloudHQ/
us-technology-jobs-on-dicecom.
Quine, W. Naturalistic epistemology. Ontological relativity
and other essays, pp. 69–90, 1969.
Quora. Quora Question Pairs, 2017. URL https://www.
kaggle.com/c/quora-question-pairs.
Robischon, J.
Wikipedia Movie Plots, 2019.
URL
https://www.kaggle.com/datasets/
jrobischon/wikipedia-movie-plots.
Roush, A. and Balaji, A. DebateSum: A large-scale ar-
gument mining and summarization dataset. In Proceed-
ings of the 7th Workshop on Argument Mining, pp. 1–7,
Online, December 2020. Association for Computational
Linguistics. URL https://aclanthology.org/
2020.argmining-1.1.
Shapere, D. The structure of scientiﬁc revolutions. The
Philosophical Review, 73(3):383–394, 1964.
Singh, C., Morris, J. X., Aneja, J., Rush, A. M., and
Gao, J.
Explaining patterns in data with language
models via interpretable autoprompting. arXiv preprint
arXiv:2210.01848, 2022.
Socher, R., Chen, D., Manning, C. D., and Ng, A. Rea-
soning with neural tensor networks for knowledge base
completion. Advances in neural information processing
systems, 26, 2013.
Sun,
J.
Daily
News
for
Stock
Market
Predic-
tion, 2017.
URL https://www.kaggle.com/
datasets/aaron7sun/stocknews.
Thomason, J., Sinapov, J., Svetlik, M., Stone, P., and
Mooney, R. J. Learning multi-modal grounded linguistic
semantics by playing” i spy”. In IJCAI, pp. 3477–3483,
2016.
Thompson, A.
All the News 1.0, 2019.
URL
https://components.one/datasets/
all-the-news-articles-dataset.
Turcan, E. and McKeown, K. Dreaddit: A reddit dataset
for stress analysis in social media.
arXiv preprint
arXiv:1911.00133, 2019.
Udacity.
Armenian
Online
Job
Postings,
2017.
URL
https://www.kaggle.com/datasets/
udacity/armenian-online-job-postings.
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,
Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran,
A. S., Naik, A., Stap, D., et al. Super-naturalinstructions:
Generalization via declarative instructions on 1600+ nlp
tasks. URL https://arxiv. org/abs/2204.07705, 2022.
Weller, O. and Seppi, K.
The rJokes dataset: a large
scale humor collection. In Proceedings of the Twelfth
Language Resources and Evaluation Conference, pp.
6136–6141, Marseille, France, May 2020. European
Language Resources Association. ISBN 979-10-95546-
34-4. URL https://aclanthology.org/2020.
lrec-1.753.

OPEND5
Williams, A., Nangia, N., and Bowman, S. R. A broad-
coverage challenge corpus for sentence understanding
through inference.
arXiv preprint arXiv:1704.05426,
2017.
Ye, S., Kim, D., Jang, J., Shin, J., and Seo, M. Guess the
instruction! making language models stronger zero-shot
learners. arXiv preprint arXiv:2210.02969, 2022.
Zhong, R., Snell, C., Klein, D., and Steinhardt, J. Describing
differences between text distributions with natural lan-
guage. In International Conference on Machine Learning,
pp. 27099–27116. PMLR, 2022.
Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S.,
Chan, H., and Ba, J. Large language models are human-
level prompt engineers. arXiv preprint arXiv:2211.01910,
2022.
Zhu, Z., Liang, W., and Zou, J. Gsclip: A framework for
explaining distribution shifts in natural language. arXiv
preprint arXiv:2206.15007, 2022.

OPEND5
A. Computing Turker Judgement T(h, x)
Scoring. To estimate T(h, x) with Turker’s rating, where h is a truth predicate of a text sample x, the Turker needs to read
h and x and then choose among six options: “Certainly Yes”, “Likely Yes”, “Neutral”, “Likely No”, “Certainly No”, and
“Confusing/Cannot be decided.” For each (h, x) pair, we collect responses from three Turkers. To compute the average
across them, we collect a list of scores using the following rule: each “Certainly Yes” would receive a score of 1.00, “Likely
Yes” 0.75, “Neutral” 0.50, “Likely No” 0.25, “Certainly No” 0.00, and “Confusing/Cannot be decided.” receive two scores
of 0.50. We then take the average over all the scores we collected from the Turkers for one h and x. “Confusing/Cannot be
decided.” receives two scores of 0.50 because we want such a response to drag the average rating towards neutral and it has
a larger effect than choosing “Neutral”.
Payment. We adjust the payment for each HIT task based on the number of words they need to read. We pay them
approximately 0.001 cent per word, and using the conservative estimate that adults read about 200 words per minute, we pay
them around $12 per hour. We spent in total around $5K on this HIT task.
Qualiﬁcation. We only recruited Turkers who are located in the U.S. Additionally, we designed qualiﬁcation test with 8
questions; the questions are designed to be easy to answer as long as they have read our instructions below, and we only
accepted turkers who made mistakes on at most one questions.
Annotation Instruction. We show our annotation instruction below. We only show examples of choosing “Certainly Yes”,
“Certainly No”, and “Confusing” to encourage the Turkers not to choose neutral ratings. Additionally, we explicitly tried to
address Halo effect – where the text does not satisfy a predicate h but satisﬁes a predicate h′ that is highly correlated with
h. For example, for the text sample x = “Really love the ﬂight!!” does not satisfy the predicate h = “mentions that the
breakfeast is good on the plane”, even though it satisﬁes a highly correlated predicate h′ = “likes the ﬂight.”
A.1. Instructions
Below are the same instructions we have shown you during the qualiﬁcation. Thanks for visiting this page and refresh your
memory about the instruction!
Instruction: In this task, you will check whether a TEXT satisﬁes a PROPERTY
Example 1
Property: mentions a natural scene.
Text: I love the way the sun sets in the evening.
• A) Certainly Yes.
• B) Likely Yes.
• C) Neutral.
• D) Likely No.
• E) Certainly No.
• F) Confusing/Cannot be decided.
Answer. A. sun set is nature-related; if you feel a bit ambivalent, B is also acceptable.
Example 2
Property: writes in a 1st person perspective.
Text: Makima is cute.
• A) Certainly Yes.
• B) Likely Yes.
• C) Neutral.
• D) Likely No.
• E) Certainly No.
• F) Confusing/Cannot be decided.
Answer. E. This text is undoubtedly written in the 3rd person perspetive, so E.
Example 3
Property: is better than group B.

OPEND5
Text: I also need to buy a chair.
• A) Certainly Yes.
• B) Likely Yes.
• C) Neutral.
• D) Likely No.
• E) Certainly No.
• F) Confusing/Cannot be decided.
Answer. F. It is unclear what the hypothesis mean (e.g., what does group B mean?) and doesn’t seem related to the text. So
F.
Example 4
Property: mentions that the breakfast is good on the airline.
Text: The airline staff was really nice! Enjoyable ﬂight.
• A) Certainly Yes.
• B) Likely Yes.
• C) Neutral.
• D) Likely No.
• E) Certainly No.
• F) Confusing/Cannot be decided.
Answer. E. Although the text appreciates the ﬂight experience, it DOES NOT mention about the breakfast. So the answer is
E.
Example 5
Property: appreciates the writing style of the author.
Text: The paper absolutely sucks because its underlying logic is wrong. However, the presentation of the paper is clear and
the use of language is really impressive.
• A) Certainly Yes.
• B) Likely Yes.
• C) Neutral.
• D) Likely No.
• E) Certainly No.
• F) Confusing/Cannot be decided.
Answer. A. Although the text dislikes the paper, it DOES like the writing style. So the answer is A.

OPEND5
with-goal
rel
sig
nov
no-goal
rel
sig
nov
rel
1.00
0.68
0.45
rel
1.00
0.85
0.71
sig
0.68
1.00
0.56
sig
0.85
1.00
0.80
nov
0.45
0.56
1.00
nov
0.71
0.80
1.00
Table 2. For each hypothesis we take the average rating. Then we calculate the pairwise spearman rank correlation between relevance
(rel), signiﬁcance (sig), and novelty (nov) on the subset of hypotheses proposed with (left) and without (right) in the prompt.
aut3 rel
aut3 sig
aut3 nov
aut1 rel
0.60
0.49
0.43
aut1 sig
0.52
0.50
0.48
aut1 nov
0.16
0.21
0.37
Table 3. The correlation between different metrics between author 1 and author 3.
B. Correlation Between Meaningfulness Metrics
We report the pair-wise correlation between the three meaningfulness metrics in Table 2. We ﬁnd a substantial positive
correlation between these metrics both when we include or exclude the goal from the prompt. We also observe that
signiﬁcance and relevance has the highest correlation, while novelty and relevance has the least.
We then investigate the correlation between each metric across different evaluators. For example, what’s the correlation
between author 1’s rating of relevance and author 2’s rating of novelty. Suppose that all authors interpret the metrics
similarly, we should observe that author 2’s rating of relevance is more correlated with author 1’s rating of relevance than
author 1’s rating of novelty.
We report the pair-wise correlation between the author’s rating on individual metrics in Table 4, Table 3, and Table 5. We
ﬁnd that across all pairs of authors X and Y , X’s relevance rating is most predictive of Y ’s relevance rating, compared to
X’s rating of novelty and signiﬁcance. The same property also holds for signiﬁcance rating. However, author 1’s relevance
rating is more predictive for author 2 and author 3’s novelty rating than author 1’s novelty rating, which might imply that
author 1 operationalizes the evaluation of novelty differently from the other authors. We conjecture this is because author 1
has a different background from the author 2 and author 3 – author 1 is a Ph.D. student who speaks English as a second
language, while author 2 and author 3 are undergrads and native speakers.
C. Full Pipeline of the Proposer
We present the full details of how we generated the hypotheses with the language model. The process roughly contains four
stages: 1) obtaining representative samples for each corpus, 2) sampling hypotheses from GPT-3, 3) rewriting hypotheses,
and 4) optionally plugging in example hypotheses.
Obtaining representative samples. This step is the same as Zhong et al. (2022), and we borrow the related text from that
paper for the reader’s convenience. Since Dres
A and Dres
B might overlap signiﬁcantly, random samples from Dres
A and Dres
B might
not be representative and informative enough for GPT-3 to notice the differences between the two distributions. Therefore,
we choose samples that are representative of their differences. To ﬁnd those samples, we ﬁne-tune RoBERTa-Large (Liu
et al., 2019) to predict whether each sample comes from Corpus A or Corpus B and keep the top-p percentile samples with
the highest conﬁdence. Next, we take samples from the top-p percentile to prompt GPT-3.
aut2 rel
aut2 sig
aut2 nov
aut1 rel
0.61
0.51
0.45
aut1 sig
0.53
0.54
0.49
aut1 nov
0.16
0.19
0.34
Table 4. The correlation between different metrics between author 1 and author 2.

OPEND5
aut2 rel
aut2 sig
aut2 nov
aut3 rel
0.91
0.74
0.69
aut3 sig
0.68
0.88
0.67
aut3 nov
0.60
0.64
0.80
Table 5. The correlation between different metrics between author 2 and author 3.
Selecting samples to prompt GPT-3. We randomly select S =25 samples from the top-5 percentile from Corpus A and
Corpus B to prompt GPT-3 to propose the hypotheses, using the template shown in Figure 3 left. We require the length
of the prompt to be at most 3,200 GPT-3 tokens (the max window size for GPT-3 text-davinci-003 is 4096) and gradually
decrease the number of samples S in the prompt until the prompt length is less than 3,200; additionally, we truncate each
text samples to at most 256 GPT-3 tokens. Finally, to prevent GPT-3 from proposing hypotheses that reﬂect simple lexical
correlations that can be detected with unigram models, e.g., “uses the word “hey” more often.”, we incrementally construct
the subset of samples for Corpus A and Corpus B such that at any time of the construction, no single word can appear 0.25S
times more often in one corpus than the other. We repeat the same process for the top-20 and top-100 percentile until we
obtain 60 hypotheses.
Rewriting hypotheses with GPT-3. As mentioned in Section 7, the hypotheses generated by GPT-3 are frequently
statements about the corpus, while the validator requires the hypothesis to be a predicate on individual text samples. For
example, when comparing deﬁnitions that people like from UrbanDictionary.com to other deﬁnitions, the hypothesis
that the former “is more likely to include slang or colloquial terms.” is a statement about a collection of text samples, rather
than a predicate on an individual sample. T(h, x) is undeﬁned in this case, since it does not make sense to check whether a
single text sample is more likely to include slang. Ideally, we want to detect these comparison statements and automatically
remove the comparatives, e.g., rewrite it to “includes slang or colloquial terms.”.
To detect and remove the comparatives from the hypotheses, we tag the part of speech for each word in the hypotheses using
the NLTK package (Bird et al., 2009) and check whether any tag is JJR or RBR. If a hypothesis indeed contain theses tags,
we prompt GPT-3 to rewrite the hypothesis. We show an example prompt in Figure 4.
Plugging in example hypotheses (optionally). We can also add a few problem-speciﬁc example hypotheses to the prompt
to elicit more relevant hypotheses, and we do so by adding them to the “formatting instruction” part in the prompt used to
propose hypotheses Figure 3. In OPEND5, we provided example hypotheses for each problem to steer our system to generate
more meaningful discoveries; we produced the example hypotheses by prompting GPT-3 to generate a few hypotheses and
selecting the meaningful ones from them.
For the reported discoveries in Section 5, we conﬁrmed that they are unambiguously different from our provided hypotheses;
otherwise, the system might have produced the discoveries by copying the provided hypotheses. We did not use the example
hypotheses in Section 4.3 to test GPT-3’s zero-shot understanding of the goal.
D. Collecting Data to Fine-tune the Validator
Here we provide a high-level description of how the data was collected. For each problem in OPEND5, we used our proposer
to produce a list of hypotheses. We automatically judged each hypothesis on a subset of samples from the research split
using GPT-3 text-davinci-002 (Ouyang et al., 2022), Flan-T5 (Chung et al., 2022), and a model trained with RLHF from Bai
et al. (2022). We created the input distribution for training by combining and equally weighting the following 3 × 2 = 4
distributions: the subset of (h, x) pairs that GPT-3/Flan-T5/“RLHF” considers Yes or No to be the most likely answer. We
then collected averaged turker ratings for in total 3138 (h, x) pairs and used them to ﬁne-tune Flan-T5 to create the validator
(Chung et al., 2022).
To test cross problem generalization capability of our D5 system, whenever we applied our D5 system to a problem in
OPEND5 in Section 5, we used a validator that is NOT ﬁne-tuned on the (h, x) pairs from this problem. We achieved this
by keeping track of which problem each (h, x) pair comes from and split all the (h, x) pairs into three folds based on the
problems; whenever we applied our D5 system to a problem, we used the validator trained on the two folds that do not
contain this problem.

OPEND5
Figure 4. The prompt to remove comparatives from a hypotheses.
Samples from Corpus A
+
Samples from Corpus B
+
Problem Context
- hypothesis1
- hypothesis2
- hypothesis3
- hypothesis4
- hypothesis5
LM
hypothesis1
+
Sample X from Corpus A
LM
100%
hypothesis1
+
Sample Y from Corpus B
LM
0%
Judge hypothesis1 on all individual 
samples from Corpus A and Corpus B
……
Compare how often each hypothesis is 
true on Corpus A compared to Corpus B   
Propose hypotheses based on the problem context 
and  some samples from Corpus A and Corpus B
Corpus A Corpus B Diff Sound?
hypothesis1
90%
0%
90%
Yes
hypothesis2
100%
100%
0%
No
hypothesis3
10%
15%
-5%
No
[Other hypotheses not included for brevity]
Figure 5. A sketch of the baseline method. The description can be seen in Section 4 and the actual prompts can be seen in Figure 3.
E. Limitations and Future Work
We still face many challenges in building a broadly useful system. We describe technical challenges that machine learning
researchers can tackle in Appendix E.1 and organizational challenges that require domain experts in Appendix E.2.
E.1. Engineering Challenges
Beyond truth predicates. Our work requires the discovery to be a truth predicate that maps a text sample to a truth value.
However, scientiﬁc discoveries can be arbitrary natural language expressions; extending to more ﬂexible expressions requires
a signiﬁcant redesign of our system and evaluation framework. Some more feasible near-term extensions include 1) allowing
natural language expressions that map from text samples to real values, e.g., “how polite the sentence is compared to other
samples from the corpora” or 2) using additional logical forms to combine individual truth predicates; e.g., learn a shallow
and interpretable decision tree where each split point is a natural language predicate.
Beyond corpus-level differences. Our work focuses on describing corpus-level differences and validates a discovery by
comparing how often it is true on each corpus. Future work can consider other ways to validate a discovery: for example,
suppose each text sample is associated with a continuous target variable, we can validate whether a discovery is more likely
true if the target variable is large.
Clarifying a discovery. Some discoveries seem to have clear meanings on the surface, but they become ambiguous when
we judge them on individual text samples. For example, judging whether a text sample h = “mentions people” seems like an
unambiguous task a priori; however, it is unclear whether it is true on the sample x = “I woke up this morning.”, since the
“people” in h is a plural form, while x only mentions one person “I”. Future work can use a language model to automatically
clarify the meaning of a hypothesis and make it more speciﬁc, e.g., rewrite h as “mentions one or more humans.”

OPEND5
Correlation ̸= causation. Like other tools that rely on correlations to analyze patterns in data (e.g., linear regression), our
system cannot establish causal relations either. For example, when comparing self-reported happy moments from females
and males, even if the former corpus has more samples that “mention children and family”, it does not necessarily imply
family plays a more important role in inter-personal relations for females; an alternative hypothesis is that females might
mention any other people more often than males, hence leading to the observation that they mention family more often.
Future work can use language models to propose what control hypothesis to test.
Decreasing the cost of validation. As alluded to in Section 3.1, estimating V is extremely expensive as it requires a lot of
human labor. Future work can consider an importance sampling procedure that uses ˆT as a proposer to improve the sample
efﬁciency of estimating V .
Training a better proposer. We developed a self-supervised learning algorithm to propose more valid hypotheses. However,
it does not take into account the meaningfulness metric, and it is unclear how to manage its trade-offs with validity if they
exist. We look forward to future works that can train a better proposer with as minimal supervision as possible.
Combining Meaningfulness and Validity Metrics. To simplify evaluation, we assumed meaningfulness to be independent
of the magnitude validity V . Such an assumption allows us to directly evaluate hypotheses that are not necessarily valid but
is also limiting for evaluating the ﬁnal discoveries: for example, for that 2008 “discuss economy” more often than 2007, it
would be way more signiﬁcant if V = 0.99 compared to V = 0.0000001. Future works can propose better metrics that do
not assume that validity and meaningfulness are independent.
E.2. Organizational Challenges
As discussed in Polanyi et al. (2000), it requires implicit community norms rather than explicit deductive logic to decide
what counts as good research results; to guide our system to produce truly important discoveries, our system needs feedback
from researchers who work in the domain of interest. However, except for machine learning, the authors do not have research
expertise in most of the domains listed in Figure 2. We look forward to future contributions from other domains and list
concrete directions below.
What problems to solve? We generated the problems in OPEND5 by reading relevant papers and guessing what domain
experts might care about. However, our guesses can be inaccurate. Future works can directly gather problems from domain
experts to reﬂect the actual usage of our system.
How to interpret a discovery? We asked for Turker’s judgment to compute T(h, x). However, many hypotheses require
expert knowledge to interpret properly. For example, only law experts can reliably judge whether a contract x satisﬁes the
predicate h “contains a license grant that is irrevocable.” Domain experts are needed to evaluate the validity of a discovery
and supervise the validator.
What discoveries are meaningful? Our work developed the evaluation instructions to approximately evaluate what
hypotheses are meaningful. However, just as no one can become an outstanding peer reviewer simply by reading the review
guideline, we do not consider it feasible to provide a gold evaluation simply by reading our instructions. Whether a discovery
is meaningful depends heavily on implicit community norms, and we hope domain experts can provide better evaluation and
training signals for our system.
F. Annotation Interface to Collect Human-Generated Hypotheses
(This section describes an interesting research direction we did not have time to fully pursue.)
Task. To ﬁne-tune the language model to propose better hypotheses and perform validation more accurately, we also
designed an interface to collect human annotations earlier in the project. In this annotation task, the annotators see ﬁve text
samples from each of the two corpora; they then write one or many natural language predicate(s) that describe how samples
from the two groups are different and choose which text samples satisfy each predicate the annotator has written. Since it is
challenging for humans to identify systematic differences between even groups of ﬁve sentences, we made the task easier
for them by
• we chose the representative samples from each corpus to form the two groups of samples, similar to the process in
Section C, and
• we highlighted subspan of the text samples that are informative for how the two corpora differ. For example, if Corpus

OPEND5
A is sports related while Corpus B is entertainment related, we hope to highlight sports-related words like “basketball”.
To automatically identify the text spans to highlight, we ﬁne-tuned RoBERTa to classify whether a sample comes
from Corpus A and Corpus B, used the SHAP library to calculate how much each text span inﬂuences the classiﬁer’s
decision, and highlighted the text spans based on the inﬂuence.
1) Write hypotheses
2) Select most representative 
samples for a written hypothesis
3) Commit hypotheses and 
show SHAP highlights
SHAP highlight view
4) Write any additional 
hypotheses after seeing 
highlights
Figure 6. A detailed screenshot of our annotation interface.
A screenshot of the annotation interface can be seen in Figure 6.
Preliminary Results We performed initial experiments on text clusters formed on the wikitext-2 dataset (Merity et al.,
2016). We asked the authors to write hypotheses for 30-50 samples and then compare the results with GPT-3 generated
hypotheses. We found that human annotators were able to write 2-4 valid hypotheses per pair of text groups, while GPT-3

OPEND5
text-davinci-003 was able to generate 4-6. Out of the valid generated hypotheses, approximately a third were variations on
another valid hypothesis. The number of times humans were able to write a hypothesis that GPT-3 was unable to generate
was around a third of the samples, while GPT-3 was able to generate a novel hypothesis humans have not thought about
before in nearly every single text corpora. Given that GPT-3 is close to our author’s ability to write hypotheses, we estimated
that we would not be able to ﬁne-tune T5 to propose better hypotheses with human annotations, and hence gave up on this
research direction.
G. Datasets
abc-headlines. We collect headlines published by ABC news, an American news company from Kulkarni (2018). ABC
headlines are directly downloaded from Harvard Dataverse. The year is extracted from the publication date ﬁeld. Samples
are constructed from the headline text. The data is downloadable from https://doi.org/10.7910/DVN/SYBGZL
with license CC0 1.0.
ad-transcripts.
We collect ad scripts from a variety of industries from Hartman (2019).
Ad transcripts are di-
rectly downloaded from Kaggle.
The top eight industries by frequency are selected.
Newlines are replaced
with spaces.
The dataset is downloadable from https://www.kaggle.com/datasets/kevinhartman0/
advertisement-transcripts-from-various-industries with license CC0 Public Domain.
admin-statements. We collect statements of administration policy from American presidents from Progress (2022).
Administration statements are extracted from a collection hosted on GitHub. Extraneous symbols are removed and
samples are split by paragraph.
The dataset is downloadable from https://github.com/unitedstates/
statements-of-administration-policy#statements-of-administration-policy
and
origin
ﬁles have a Creative Commons Attribution 3.0 License.
ai2-natural-instruction. We collect a learning-from-instructions dataset released by the Allen Institute for AI from Mishra
et al. (2022). Natural instruction tasks are directly downloaded without modiﬁcation. The dataset is released under an
Apache-2.0 license.
airline-reviews. We collect reviews of airlines collected from the review website Skytrax. Airline reviews for airlines,
airports, and seats are downloaded from a public GitHub repository. Names of aircraft, airlines, countries, and traveler
types are standardized. Ratings of 1, 4, or 5 on a scale of 5, and 1, 5, 8, or 10 on a scale of 10 are kept. This dataset can be
downloaded via https://github.com/quankiquanki/skytrax-reviews-dataset.
aita. We collect posts on the “Am I The Asshole” Subreddit, an online forum people ask others whether they were in the
wrong from O’Brien (2020). Posts from r/AmITheAsshole are downloaded from a praw scrape of Reddit. Topic areas are
chosen based on common themes in posts and coarsely deﬁned based on manual keywords. Each post can belong to multiple
topic areas. The dataset can be downloaded at https://doi.org/10.5281/zenodo.3677563.
all-the-news. We collect news articles collected from various outlets between 2015 and 2017 from Thompson (2019). News
articles are downloaded directly from the Components website. The titles are used as text samples.The dataset can be
downloaded at https://components.one/datasets/all-the-news-articles-dataset .
amazon-reviews. We collect Amazon reviews collected from various product categories from Ni et al. (2019). Amazon
reviews are downloaded from a 2018 crawl of the website. The ﬁrst 100,000 review texts are treated as the text sample. The
dataset can be downloaded at https://nijianmo.github.io/amazon/index.html .
armenian-jobs. We collect job postings in Armenia from Udacity (2017). The Armenian job postings dataset is down-
loaded from a snapshot on GitHub. Different IT jobs are manually coded and time intervals are deﬁned in order to
balance sample availability. The dataset can be downloaded at https://www.kaggle.com/datasets/udacity/
armenian-online-job-postings .
boolq. We collect a reading comprehension dataset of yes/no questions from Clark et al. (2019). Boolean questions are down-
loaded directly as is. The dataset can be downloaded at https://github.com/google-research-datasets/
boolean-questions with license CC-SA-3.0.
clickbait-headlines. We collect headlines across time from the Examiner, a clickbait news site from Kulkarni (2020a). The
Examiner headlines are directly downloaded from Kaggle. The year is extracted from the publication date ﬁeld. Samples
are constructed from the headline text. The dataset can be downloaded at https://www.kaggle.com/datasets/

OPEND5
therohk/examine-the-examiner, with license CC0: public domain.
convincing-arguments.
We collect arguments on a variety of topics annotated for convincingness from Habernal
& Gurevych (2016).
Annotated arguments are downloaded from the GitHub repository.
Arguments are sorted
by rank.
The bottom 400 are treated as “unconvincing”, the top 200 are treated as “convincing”, and the next
200 are treated as “somewhat convincing.” The dataset can be downloaded at https://github.com/UKPLab/
acl2016-convincing-arguments, with license CC-BY 4.0.
craigslist-negotiations. We collect dialogue from Craigslist negotiations, an online seller platform from He et al. (2018).
Craigslist negotiations are downloaded from Huggingface. Sequences which contained a “quit” intention or “reject”
intention are categorized as failures; those which contained an “accept” intention are categorized as successes. The
mid-price is deﬁned as the mean price of the items sold. Within each category, the items are sorted by mid-price.
The top half is treated as high-price and the bottom half is treated as low-price. This dataset can be downloaded at
https://huggingface.co/datasets/Hellisotherpeople/DebateSum with MIT license.
debate. We collect evidence compiled for American competitive policy debate, published online by debate camps from
Roush & Balaji (2020). The train split is downloaded from Huggingface. For each sample, we use the abstract as the
text. Arguments are categorized by type, debate camp of origin, and topic/speciﬁc argument. For topics, we use domain
knowledge to list relevant keywords for each topic and include any sample with a ﬁle name that includes any keyword.
A single sample can belong to multiple topics. This dataset can be downloaded at https://huggingface.co/
datasets/Hellisotherpeople/DebateSum with MIT license.
dice-jobs. We collect American technology job postings on dice.com from PromptCloud (2017). Job postings are
downloaded from Kaggle.
Posts from the six most popular companies are categorized by company.
We remove
miscellaneous characters and blank descriptions.
We additionally apply our splitting procedure to reduce descrip-
tion length.
This dataset can be downloaded at https://www.kaggle.com/datasets/PromptCloudHQ/
us-technology-jobs-on-dicecom under CC BY-SA 4.0 .
diplomacy-deception. We collect dialogue from games of Diplomacy, which involves deception from Peskov et al. (2020).
Diplomacy dialogues are downloaded from GitHub (all splits). The data are ASCII encoded and newlines are removed. Each
message and label is treated as a sample. This dataset can be downloaded at https://huggingface.co/datasets/
diplomacy_detection under unknown license.
echr-decisions. We collect facts of cases heard before the European Court of Human Rights from Chalkidis et al. (2019).
Decisions are downloaded from a public archive. A random sample of 500 decisions is selected from the ﬁles. The samples
with any violated articles are categorized as “violation,” while the rest are categorized as “no violation.” This dataset can be
downloaded at https://paperswithcode.com/dataset/echr under unknown license.
essay-scoring. We collect essays from students from ess (2012). Essays are downloaded from a GitHub repository. Only
essays from set 5 are considered. Essays with a score of at least 3 are categorized as good essays, while essays with a
score less than 3 are bad essays. This dataset can be downloaded at https://www.kaggle.com/c/asap-aes under
unknown license.
fake-news. We collect fake and legitimate news from P´erez-Rosas et al. (2017). Fake news articles are downloaded from
the author’s website. Full articles are treated as text snippets. This dataset can be downloaded at http://web.eecs.
umich.edu/˜mihalcea/downloads.html#FakeNews under CC-BY-4.0.
fomc-speeches.
We collect Federal Open Market Committee (FOMC) speeches from 1996-2020, which describe
Federal Reserve policy from Mish (2020). Fed speeches are downloaded from Kaggle. The macro indicator data
are merged in on the year and month. Full speech text is split by paragraph and categorized by speaker, year, and
macroeconomic indicator. This dataset can be downloaded at https://www.kaggle.com/datasets/natanm/
federal-reserve-governors-speeches-1996-2020 under unknown license.
genius-lyrics. We collect lyrics collected from Genius.com before 2020 from Lim & Benson (2021). Genius lyrics are
downloaded from Google Drive. The lyrics are merged with song metadata and treated as samples. We categorize lyrics by
hand-selecting popular artists, common genres, time periods, and view counts (over 1M views is high, 500k-1M is medium).
This dataset can be downloaded at https://www.cs.cornell.edu/˜arb/data/genius-expertise/ under
unknown license.

OPEND5
happy-moments. We collect self-reported happy moments and demographic characteristics from Asai et al. (2018). The
HappyDB dataset is downloaded from the ofﬁcial GitHub repository. Demographic data is cleaned and merged into happy
moments. Happy moment descriptions are treated as samples and are categorized by type of happy moment, country of
origin, and other demographic features. This dataset can be downloaded at https://github.com/megagonlabs/
HappyDB under unknown license.
huff-post-headlines. We collect headlines from the news outlet Hufﬁngton Post from Misra & Arora (2019) and Misra &
Grover (2021). Hufﬁngton Post headlines are downloaded from Kaggle. The short description of each article is treated as a
sample and tokenized at the sentence level. This dataset can be downloaded at https://rishabhmisra.github.
io/publications/ under CC-BY-4.0.
immigration-speeches. We collect congressional and presidential speeches that mention immigration from 1880 to the
present from Card et al. (2022). Immigration speeches are downloaded from the replication package. The speech text is
preprocessed to remove extraneous spaces. We engineer features corresponding to time periods, well-known speakers, other
signiﬁcant time periods, the racial group under discussion, and the geographic area within the United States. This dataset
can be downloaded at https://github.com/dallascard/us-immigration-speeches/releases.
kickstarter. We collect names of startups on kickstarter.com from Mouill´e (2017). We download a 2018 crawl from
Kickstarter from Kaggle. The project name is treated as the text sample. This dataset can be downloaded at https://www.
kaggle.com/datasets/kemical/kickstarter-projects?select=ks-projects-201612.csv un-
der CC BY-NC-SA 4.0.
microedit-humor. We collect funny sentences generated by making one-word edits to normal statements from Hossain
et al. (2019). The Microedit dataset is downloaded from the author’s website. We make the relevant edit to each text sample
and treat the edited text sample as the data point. We bin the mean annotator grade into 4 and denote each as unfunny,
neutral, funny, and very funny, respectively. This dataset can be downloaded at https://paperswithcode.com/
dataset/humicroedit.
mnli. We collect a collection of sentence pairs annotated with textual entailment information from a range of genres from
Williams et al. (2017). The MNLI corpus is downloaded from the ofﬁcial website. We treat the premise and hypothesis
as text samples. This dataset can be downloaded from https://cims.nyu.edu/˜sbowman/multinli/, most of
which are under the OANC license.
monster-jobs. We collect American job postings on monster.com. Jobs on Monster.com are downloaded from Kaggle. Job
descriptions are treated as samples and split at the paragraph and sentence level. We keep and categorize jobs from seventeen
large cities. This dataset can be downloaded from https://www.kaggle.com/datasets/PromptCloudHQ/
us-jobs-on-monstercom under CC BY-SA 4.0 .
movie-tmdb. We collect movie plot summaries from TMDB from Kaggle (2018). TMDB movie overviews are downloaded
from Kaggle. We keep only English movies and bin popularity by deciles. The top decile is considered “hits,” the 70-80th
percentiles are considered “average,” and the 30-40th percentiles are considered “bad.” This dataset can be downloaded
from https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata21.
movie-wiki. We collect movie plot summaries collected from Wikipedia from Robischon (2019). Wikipedia movie sum-
maries are downloaded from Kaggle. This dataset can be downloaded from https://www.kaggle.com/datasets/
jrobischon/wikipedia-movie-plots under CC BY-SA 4.0.
news-popularity. We collect news headlines posted on social media platforms from Moniz & Torgo (2018). Headlines
are downloaded from a reproduction package. The headline and title text are cleaned, and the title is treated as the text
sample. The 100 most positive and negative or popular and unpopular articles on each topic are used as distributions. This
dataset can be downloaded from https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+
Multiple+Social+Media+Platforms.
nli-benchmarks. We collect training examples from various natural language inference (NLI) datasets from Liu et al.
(2022). NLI benchmarks are downloaded from a public collection on Google Drive. We examine the premise and hypothesis
separately as samples. This dataset can be downloaded from https://github.com/alisawuffles/wanli.
npt-conferences. We collect Non-Proliferation of Nuclear Weapons (NPT) conference transcripts from Barnum & Lo
(2020). NPT conference notes are extracted from the accompanying replication package. Text is split by paragraph,

OPEND5
and only paragraphs longer than 50 characters are preserved. Text is split into three time ranges: pre-2008, 2008-2012,
and post-2012. This dataset can be downloaded from https://journals.sagepub.com/doi/full/10.1177/
0022343320960523.
open-deception. We collect arbitrary lies and truths from any domain generated by crowdworkers from P´erez-Rosas
& Mihalcea (2015). Open domain lies are downloaded from the public dataset and lie texts are split into lies and
truths. This dataset can be downloaded from https://web.eecs.umich.edu/˜mihalcea/downloads.html#
OpenDeception.
open-review. We collect submissions to ICLR, a machine learning conference from 2018 to 2021. Open review abstracts
are accessed via the openreview API. We query for abstracts from the 2018-2021 ICLR blind submissions. Abstracts
are classiﬁed based on rating: >= 7 (“great”), 5-6 (“good”), and <= 4 (“bad”). This dataset can be downloaded from
https://openreview.net/.
parenting-subreddits. We collect posts from various parenting-related subreddits, which are text-based forums on the site
Reddit from Gao et al. (2021). Posts from various subreddits are downloaded from the paper’s GitHub repository. We
clean the text and split the posts according to the topic(s) each post is tagged with. This dataset can be downloaded from
https://github.com/SALT-NLP/Parenting_OnlineUsage.
poetry.
We collect poems from PoetryFoundation.com from Bramhecha (2019).
Poems are downloaded from a
2019 scrape of the PoetryFoundation website from Kaggle.
The text is cleaned and split according to subject
tags and authorship. This dataset can be downloaded from https://www.kaggle.com/datasets/tgdivy/
poetry-foundation-poems under GNU Affero General Public License.
political-ads. We collect political ads observed by Facebook users from pol (2021). Ads are downloaded from the Ad
Observer website, which maintains an aggregate of all collected ads. We extract targeting metadata from the targeting ﬁeld
and deﬁne splits according to age, gender, location, interests, time, and political lean. This dataset can be downloaded from
https://adobserver.org/ad-database/.
qqp. We collect questions from Quora.com from Quora (2017).
rate-my-prof. We collect reviews of lecturers from RateMyProfessor.com from He (2020). We download a sample of
RateMyProfessor.com reviews from an online repo. We clean the text and guess the gender of the reviewed lecturer from the
ﬁrst name using the gender-guesser package. Due to data availability, we consider only male and female names. To improve
the quality of the classiﬁcation, we remove any posts which use pronouns from the opposing sex (e.g. “him”). This dataset
can be downloaded from https://data.mendeley.com/datasets/fvtfjyvw7d/2 under CC BY 4.0 .
radiology-diagnosis. We collect impressions and medical histories of radiology patients from Pestian et al. (2007).
Radiology diagnoses are downloaded from a GitHub copy of the original task dataset. We parse the metadata to retrieve the
diagnostic code, decision type, impression, and patient history. Referencing the associated ICD codes, we convert codes to
colloquial diagnoses (e.g. 786.2 denotes cough). We treat the histories and impressions as samples and split them according
to diagnosis and level of consensus.
reddit-humor. We collect jokes posted on the Reddit forum r/Jokes, a message board for sharing jokes from Weller &
Seppi (2020). Jokes are downloaded from the dev and test splits of the dataset. We clean the text and split the dataset
according to whether they are labeled as funny. This dataset can be downloaded from https://github.com/orionw/
rJokesData under Reddit License and Terms of Service, and users must follow the Reddit User Agreement and Privacy
Policy, as well as remove any posts if asked to by the original user.
reddit-stress. We collect stress-related posts on Reddit from Turcan & McKeown (2019). We split the post text based
on which subreddit they are posted on (related to PTSD, anxiety, or stress generally). Reddit posts are downloaded
from https://github.com/gillian850413/Insight_Stress_Analysis, and we recommend following
the Reddit User Agreement and Privacy Policy, as well as remove any posts if asked to by the original user.
reuters-authorship. We collect articles from various Reuters authors from Liu (2011). The articles are split according
to the author. Reuters articles are downloaded from the UCI repository https://archive.ics.uci.edu/ml/
datasets/Reuter_50_50.
riddles. We generated several riddles. The 3000 most common English words are manually copied from a website. Words
with between 5 and 8 characters are kept. We create two popular riddles. First, we split words based on whether they have a

OPEND5
duplicate character. We exclude any words with multiple “doubles” or more than 2 of any character. Second, we split words
based on whether they have the letter T.
scotus-cases. We collect facts from cases heard by the Supreme Court of the United States (SCOTUS) from Alali et al.
(2021). Supreme Court cases are downloaded from a GitHub repository. We identify state/federal parties by manually
deﬁning keywords. We split based on the winning party, the identity of each party, and the type of decision. We then deﬁne
several time periods and relevant political eras and split decisions accordingly. Finally, we split according to the ruling’s
policy area and how it changes over time. The dataset can be downloaded from https://paperswithcode.com/
paper/justice-a-benchmark-dataset-for-supreme-court under CC-BY-SA.
short-answer-scoring. We collect short answers from students from sho (2013). Short answers are downloaded from a
GitHub mirror of the dataset. We consider only responses to essay set 1. The two scores are averaged and binned into good
(>= 2.5), medium (1.5-2.5), and bad (<1.5). The dataset can be downloaded from https://www.kaggle.com/c/
asap-sas.
snli. We collect a collection of sentence pairs annotated with textual entailment information from images from Bowman
et al. (2015). The dataset can be downloaded from https://nlp.stanford.edu/projects/snli/ under CC
BY-SA 4.0.
squad-v2. We collect reading comprehension questions crowdsourced from Wikipedia articles from ?. The dataset can be
downloaded from https://rajpurkar.github.io/SQuAD-explorer/ under CC BY-SA 4.0.
stock-news.
We collect top news headlines on Reddit, an online message board from Sun (2017).
Head-
lines are downloaded from a GitHub mirror.
We clean the text and divide the samples based on whether the
DOW rose or fell that day. The dataset can be downloaded from https://github.com/ShravanChintha/
Stock-Market-prediction-using-daily-news-headlines under Reddit License and Terms of Service,
and users must follow the Reddit User Agreement and Privacy Policy, as well as remove any posts if asked to by the original
user.
suicide-notes. We collect posts from r/SuicideWatch and r/depression, two forums on Reddit fromHe (2021). The post title
and body are combined to form the text samples. Samples are split based on whether they were posted in a suicide-related
Subreddit. The dataset can be downloaded from a github: https://github.com/hesamuel/goodbye_world,
under Reddit License and Terms of Service, and users must follow the Reddit User Agreement and Privacy Policy, as well as
remove any posts if asked to by the original user.
times-india-headlines. We collect headlines from Times of India news from Kulkarni (2022). Headlines are downloaded
from a Dataverse mirror. We use the ﬁrst 1000 headlines in each year as samples. The dataset can be downloaded from
https://www.kaggle.com/datasets/therohk/india-headlines-news-dataset under CC0 Public
Domain.
trial-deception. We collect testimonies from witnesses in real trials from P´erez-Rosas et al. (2015). Trial testimonies
are downloaded from the author’s website. The testimonies are divided based on whether they are considered truth-
ful. The dataset can be downloaded from https://web.eecs.umich.edu/˜mihalcea/downloads.html#
RealLifeDeception.
un-debates. We collect speeches from debates at the United Nations from Baturo et al. (2017). Debate transcripts are
downloaded from the Dataverse reproduction package. Samples are divided based on the country and year of the snippet.
First, we isolate samples from Russia, China, and the United States and specify 3 time periods of interest. Next, we divide
all samples by the decade. Finally, we create distributions for 19 countries of interest. The dataset can be downloaded from
https://doi.org/10.7910/DVN/0TJX8Y under CC0 1.0 .
unhealthy-conversations. We collect expert-annotated unhealthy conversations from Price et al. (2020). Conversation
transcripts are downloaded from the ofﬁcial GitHub repository. For each annotated attribute, we split the dataset based
on whether that form of unhealthy conversation is present in the sample. The dataset can be downloaded from https:
//github.com/conversationai/unhealthy-conversations under CC BY-NC-SA 4.0.
urban-dictionary. We collect deﬁnitions from UrbanDictionary.com, a crowdsourced English dictionary from Kulkarni
(2020b). Urban Dictionary entries are downloaded from Kaggle. Deﬁnitions are split into groups representing the top
1, 5, and 10 percent of deﬁnitions ranked by both upvotes and downvotes; we sample 10,000 from each and create a

OPEND5
control distribution by randomly sampling 10,000 deﬁnitions from all entries. The dataset can be downloaded from
https://www.kaggle.com/therohk/urban-dictionary-words-dataset under CC0 Public Domain.
wikitext. We collect text snippets from Wikipedia from Merity et al. (2016). The Wikipedia snippets are loaded from
HuggingFace. We remove any samples that are empty or start with ’=’ (which represent headings); samples are tokenized
at the sentence level and used for clustering. The dataset can be downloaded from https://huggingface.co/
datasets/wikitext under CC BY-SA 3.0.
yc−startups. We collect descriptions of companies that were part of the Y Combinator startup incubator from Bhalo-
tia (2022). YCombinator company descriptions are downloaded from a 2022 scrape on GitHub. Only companies with
long descriptions are preserved. Companies are split according to founder characteristics, year, “top company” designa-
tion, operating status, and location. The dataset can be downloaded from https://www.kaggle.com/datasets/
benhamner/y-combinator-companies.

