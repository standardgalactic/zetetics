Jasmin Blanchette
Laura Kovács
Dirk Pattinson (Eds.)
 123
LNAI 13385
11th International Joint Conference, IJCAR 2022
Haifa, Israel, August 8–10, 2022
Proceedings
Automated Reasoning

Lecture Notes in Artiﬁcial Intelligence
13385
Subseries of Lecture Notes in Computer Science
Series Editors
Randy Goebel
University of Alberta, Edmonton, Canada
Wolfgang Wahlster
DFKI, Berlin, Germany
Zhi-Hua Zhou
Nanjing University, Nanjing, China
Founding Editor
Jörg Siekmann
DFKI and Saarland University, Saarbrücken, Germany

More information about this subseries at https://link.springer.com/bookseries/1244

Jasmin Blanchette · Laura Kovács ·
Dirk Pattinson (Eds.)
Automated Reasoning
11th International Joint Conference, IJCAR 2022
Haifa, Israel, August 8–10, 2022
Proceedings

Editors
Jasmin Blanchette
Vrije Universiteit Amsterdam
Amsterdam, The Netherlands
Dirk Pattinson
Australian National University
Canberra, ACT, Australia
Laura Kovács
Vienna University of Technology
Wien, Austria
ISSN 0302-9743
ISSN 1611-3349 (electronic)
Lecture Notes in Artiﬁcial Intelligence
ISBN 978-3-031-10768-9
ISBN 978-3-031-10769-6 (eBook)
https://doi.org/10.1007/978-3-031-10769-6
LNCS Sublibrary: SL7 – Artiﬁcial Intelligence
© The Editor(s) (if applicable) and The Author(s) 2022. This book is an open access publication.
Open Access This book is licensed under the terms of the Creative Commons Attribution 4.0 International
License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution
and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and
the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this book are included in the book’s Creative Commons license,
unless indicated otherwise in a credit line to the material. If material is not included in the book’s Creative
Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use,
you will need to obtain permission directly from the copyright holder.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, expressed or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This volume contains the papers presented at the 11th International Joint Conference on
Automated Reasoning (IJCAR 2022) held during August 8–10, 2022, in Haifa, Israel.
IJCAR was part of the Federated Logic Conference (FLoC 2022), which took place from
July 31 to August 12, 2022, in Haifa.
IJCAR is the premier international joint conference on all aspects of automated
reasoning, including foundations, implementations, and applications, comprising several
leading conferences and workshops. IJCAR 2022 united the Conference on Automated
Deduction (CADE), the International Symposium on Frontiers of Combining Systems
(FroCoS), and the International Conference on Automated Reasoning with Analytic
Tableaux and Related Methods (TABLEAUX). Previous IJCAR conferences were held
in Siena, Italy, in 2001, Cork, Ireland, in 2004, Seattle, USA, in 2006, Sydney, Australia,
in 2008, Edinburgh, UK, in 2010, Manchester, UK, in 2012, Vienna, Austria, in 2014,
Coimbra, Portugal, in 2016, Oxford, UK, in 2018, and Paris, France, in 2020 (virtual).
There were 85 submissions. Each submission was assigned to at least three Program
Committee members and was reviewed in single-blind mode. The committee decided to
accept 41 papers: 32 regular papers and nine system descriptions.
The program also included two invited talks, by Elvira Albert and Gilles Dowek, as
well as a plenary FLoC talk by Aarti Gupta.
We acknowledge the FLoC sponsors:
• Diamond sponsors: Amazon Web Services, Meta, Intel
• Gold sponsors: Google, Nvidia, Synopsys
• Silver sponsor: Cadence
• Bronze sponsors: DLVSystem, Veridise
• Other sponsors: Technion, The Henry and Marilyn Taub Faculty of Computer Science
We also acknowledge the generous sponsorship of Springer and the Trakhtenbrot
family, as well as the invaluable support provided by the EasyChair developers. We
ﬁnally thank the FLoC 2022 organization team for assisting us with local organization
and general conference management.
May 2022
Jasmin Blanchette
Laura Kovács
Dirk Pattinson

Organization
Program Committee
Erika Abraham
RWTH Aachen University, Germany
Carlos Areces
Universidad Nacional de Córdoba, Spain
Bernhard Beckert
Karlsruhe Institute of Technology, Germany
Alexander Bentkamp
Chinese Academy of Sciences, China
Armin Biere
University of Freiburg, Germany
Nikolaj Bjørner
Microsoft, USA
Jasmin Blanchette (Co-chair)
Vrije Universiteit Amsterdam, The Netherlands
Frédéric Blanqui
Inria, France
Maria Paola Bonacina
Università degli Studi di Verona, Italy
Kaustuv Chaudhuri
Inria, France
Agata Ciabattoni
Vienna University of Technology, Austria
Stéphane Demri
CNRS, LMF, ENS Paris-Saclay, France
Clare Dixon
University of Manchester, UK
Huimin Dong
Sun Yat-sen University, China
Katalin Fazekas
Vienna University of Technology, Austria
Mathias Fleury
University of Freiburg, Austria
Pascal Fontaine
Université de Liège, Belgium
Nathan Fulton
IBM, USA
Silvio Ghilardi
Università degli Studi di Milano, Italy
Jürgen Giesl
RWTH Aachen University, Germany
Rajeev Gore
Australian National University, Australia
Marijn Heule
Carnegie Mellon University, USA
Radu Iosif
Verimag, CNRS, Université Grenoble Alpes,
France
Mikolas Janota
Czech Technical University in Prague,
Czech Republic
Moa Johansson
Chalmers University of Technology, Sweden
Cezary Kaliszyk
University of Innsbruck, Austria
Laura Kovacs (Co-chair)
Vienna University of Technology, Austria
Orna Kupferman
Hebrew University, Israel
Cláudia Nalon
University of Brasília, Brazil
Vivek Nigam
Huawei ERC, Germany
Tobias Nipkow
Technical University of Munich, Germany
Jens Otten
University of Oslo, Norway
Dirk Pattinson (Co-chair)
Australian National University, Australia
Nicolas Peltier
CNRS, LIG, France

viii
Organization
Brigitte Pientka
McGill University, Canada
Elaine Pimentel
University College London, UK
André Platzer
Carnegie Mellon University, USA
Giles Reger
Amazon Web Services, USA, and University of
Manchester, UK
Andrew Reynolds
University of Iowa, USA
Simon Robillard
Université de Montpellier, France
Albert Rubio
Universidad Complutense de Madrid, Spain
Philipp Ruemmer
Uppsala University, Sweden
Renate A. Schmidt
University of Manchester, UK
Stephan Schulz
DHBW Stuttgart, Germany
Roberto Sebastiani
University of Trento, Italy
Martina Seidl
Johannes Kepler University Linz, Austria
Viorica Sofronie-Stokkermans
University of Koblenz-Landau, Germany
Lutz Straßburger
Inria, France
Martin Suda
Czech Technical University in Prague,
Czech Republic
Tanel Tammet
Tallinn University of Technology, Estonia
Sophie Tourret
Inria, France, and Max Planck Institute for
Informatics, Germany
Uwe Waldmann
Max Planck Institute for Informatics, Germany
Christoph Weidenbach
Max Planck Institute for Informatics, Germany
Sarah Winkler
Free University of Bozen-Bolzano, Italy
Yoni Zohar
Bar-Ilan University, Israel
Additional Reviewers
László Antal
Paolo Baldi
Lionel Blatter
Brandon Bohrer
Marius Bozga
Chad Brown
Lucas Bueri
Guillaume Burel
Marcelo Coniglio
Riccardo De Masellis
Warren Del-Pinto
Zafer Esen
Michael Färber
Sicun Gao
Jacques Garrigue
Thibault Gauthier
Samir Genaim
Alessandro Gianola
Raúl Gutiérrez
Fajar Haifani
Alejandro Hernández-Cerezo
Ullrich Hustadt
Jan Jakubuv
Martin Jonas
Michael Kirsten
Gereon Kremer
Roman Kuznets
Jonathan Laurent
Chencheng Liang
Enrico Lipparini
Florin Manea
Marco Maratea

Organization
ix
Sonia Marin
Enrique Martin-Martin
Andrea Mazzullo
Stephan Merz
Antoine Miné
Sibylle Möhle
Cristian Molinaro
Markus Müller-Olm
Jasper Nalbach
Joel Ouaknine
Tobias Paxian
Wolfram Pfeifer
Andrew Pitts
Amaury Pouly
Stanisław Purgał
Michael Rawson
Giselle Reis
Clara Rodríguez-Núñez
Daniel Skurt
Giuseppe Spallitta
Sorin Stratulat
Petar Vukmirovi´c
Alexander Weigl
Richard Zach
Anna Zamansky
Michal Zawidzki

Contents
Invited Talks
Using Automated Reasoning Techniques for Enhancing the Efﬁciency
and Security of (Ethereum) Smart Contracts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Elvira Albert, Pablo Gordillo, Alejandro Hernández-Cerezo,
Clara Rodríguez-Núñez, and Albert Rubio
From the Universality of Mathematical Truth to the Interoperability
of Proof Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
Gilles Dowek
Satisﬁability, SMT Solving, and Arithmetic
Flexible Proof Production in an Industrial-Strength SMT Solver . . . . . . . . . . . . . .
15
Haniel Barbosa, Andrew Reynolds, Gereon Kremer, Hanna Lachnitt,
Aina Niemetz, Andres Nötzli, Alex Ozdemir, Mathias Preiner,
Arjun Viswanathan, Scott Viteri, Yoni Zohar, Cesare Tinelli,
and Clark Barrett
CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic . . . .
36
Paolo Felli, Marco Montali, and Sarah Winkler
SAT-Based Proof Search in Intermediate Propositional Logics . . . . . . . . . . . . . . .
57
Camillo Fiorentini and Mauro Ferrari
Clause Redundancy and Preprocessing in Maximum Satisﬁability . . . . . . . . . . . .
75
Hannes Ihalainen, Jeremias Berg, and Matti Järvisalo
Cooperating Techniques for Solving Nonlinear Real Arithmetic
in the cvc5 SMT Solver (System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
Gereon Kremer, Andrew Reynolds, Clark Barrett, and Cesare Tinelli
Preprocessing of Propagation Redundant Clauses . . . . . . . . . . . . . . . . . . . . . . . . . .
106
Joseph E. Reeves, Marijn J. H. Heule, and Randal E. Bryant
Reasoning About Vectors Using an SMT Theory of Sequences . . . . . . . . . . . . . . .
125
Ying Sheng, Andres Nötzli, Andrew Reynolds, Yoni Zohar, David Dill,
Wolfgang Grieskamp, Junkil Park, Shaz Qadeer, Clark Barrett,
and Cesare Tinelli

xii
Contents
Calculi and Orderings
An Efﬁcient Subsumption Test Pipeline for BS(LRA) Clauses . . . . . . . . . . . . . . .
147
Martin Bromberger, Lorenz Leutgeb, and Christoph Weidenbach
Ground Joinability and Connectedness in the Superposition Calculus . . . . . . . . .
169
André Duarte and Konstantin Korovin
Connection-Minimal Abduction in EL via Translation to FOL . . . . . . . . . . . . . . .
188
Fajar Haifani, Patrick Koopmann, Sophie Tourret,
and Christoph Weidenbach
Semantic Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
Fajar Haifani and Christoph Weidenbach
SCL(EQ): SCL for First-Order Logic with Equality . . . . . . . . . . . . . . . . . . . . . . . .
228
Hendrik Leidinger and Christoph Weidenbach
Term Orderings for Non-reachability of (Conditional) Rewriting . . . . . . . . . . . . .
248
Akihisa Yamada
Knowledge Representation and Justiﬁcation
Evonne: Interactive Proof Visualization for Description Logics (System
Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271
Christian Alrabbaa, Franz Baader, Stefan Borgwardt,
Raimund Dachselt, Patrick Koopmann, and Julián Méndez
Actions over Core-Closed Knowledge Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
Claudia Cauli, Magdalena Ortiz, and Nir Piterman
GK: Implementing Full First Order Default Logic for Commonsense
Reasoning (System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
Tanel Tammet, Dirk Draheim, and Priit Järv
Hypergraph-Based Inference Rules for Computing EL+-Ontology
Justiﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
Hui Yang, Yue Ma, and Nicole Bidoit
Choices, Invariance, Substitutions, and Formalizations
Sequent Calculi for Choice Logics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
Michael Bernreiter, Anela Lolic, Jan Maly, and Stefan Woltran

Contents
xiii
Lash 1.0 (System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
350
Chad E. Brown and Cezary Kaliszyk
Goéland: A Concurrent Tableau-Based Theorem Prover (System
Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
Julie Cailler, Johann Rosain, David Delahaye, Simon Robillard,
and Hinde Lilia Bouziane
Binary Codes that Do Not Preserve Primitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
369
Štˇepán Holub, Martin Raška, and Štˇepán Starosta
Formula Simpliﬁcation via Invariance Detection by Algebraically Indexed
Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
388
Takuya Matsuzaki and Tomohiro Fujita
Synthetic Tableaux: Minimal Tableau Search Heuristics . . . . . . . . . . . . . . . . . . . . .
407
Michał Socha´nski, Dorota Leszczy´nska-Jasion, Szymon Chlebowski,
Agata Tomczyk, and Marcin Jukiewicz
Modal Logics
Paraconsistent Gödel Modal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
429
Marta Bílková, Sabine Frittella, and Daniil Kozhemiachenko
Non-associative, Non-commutative Multi-modal Linear Logic . . . . . . . . . . . . . . .
449
Eben Blaisdell, Max Kanovich, Stepan L. Kuznetsov, Elaine Pimentel,
and Andre Scedrov
Effective Semantics for the Modal Logics K and KT via Non-deterministic
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
468
Ori Lahav and Yoni Zohar
Local Reductions for the Modal Cube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
486
Cláudia Nalon, Ullrich Hustadt, Fabio Papacchini, and Clare Dixon
Proof Systems and Proof Search
Cyclic Proofs, Hypersequents, and Transitive Closure Logic . . . . . . . . . . . . . . . . .
509
Anupam Das and Marianna Girlando
Equational Uniﬁcation and Matching, and Symbolic Reachability Analysis
in Maude 3.2 (System Description)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529
Francisco Durán, Steven Eker, Santiago Escobar, Narciso Martí-Oliet,
José Meseguer, Rubén Rubio, and Carolyn Talcott

xiv
Contents
Le´sniewski’s Ontology – Proof-Theoretic Characterization . . . . . . . . . . . . . . . . . .
541
Andrzej Indrzejczak
Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers . . . .
559
Chaitanya Mangla, Sean B. Holden, and Lawrence C. Paulson
A Framework for Approximate Generalization in Quantitative Theories . . . . . . .
578
Temur Kutsia and Cleo Pau
Guiding an Automated Theorem Prover with Neural Rewriting . . . . . . . . . . . . . . .
597
Jelle Piepenbrock, Tom Heskes, Mikoláš Janota, and Josef Urban
Rensets and Renaming-Based Recursion for Syntax with Bindings . . . . . . . . . . .
618
Andrei Popescu
Finite Two-Dimensional Proof Systems for Non-ﬁnitely Axiomatizable
Logics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
640
Vitor Greati and João Marcos
Vampire Getting Noisy: Will Random Bits Help Conquer Chaos? (System
Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
659
Martin Suda
Evolution, Termination, and Decision Problems
On Eventual Non-negativity and Positivity for the Weighted Sum
of Powers of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
671
S. Akshay, Supratik Chakraborty, and Debtanu Pal
Decision Problems in a Logic for Reasoning About Reconﬁgurable
Distributed Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
691
Marius Bozga, Lucas Bueri, and Radu Iosif
Proving Non-Termination and Lower Runtime Bounds with LoAT (System
Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
712
Florian Frohn and Jürgen Giesl
Implicit Deﬁnitions with Differential Equations for KeYmaera X: (System
Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
723
James Gallicchio, Yong Kiam Tan, Stefan Mitsch, and André Platzer

Contents
xv
Automatic Complexity Analysis of Integer Programs via Triangular
Weakly Non-Linear Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
734
Nils Lommen, Fabian Meyer, and Jürgen Giesl
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
755

Invited Talks

Using Automated Reasoning Techniques
for Enhancing the Eﬃciency and Security
of (Ethereum) Smart Contracts
Elvira Albert1,2(B)
, Pablo Gordillo1
, Alejandro Hern´andez-Cerezo1
,
Clara Rodr´ıguez-N´u˜nez1
, and Albert Rubio1,2
1 Complutense University of Madrid, Madrid, Spain
2 Instituto de Tecnolog´ıa del Conocimiento, Madrid, Spain
elvira@fdi.ucm.es
The use of the Ethereum blockchain platform [17] has experienced an enor-
mous growth since its very ﬁrst transaction back in 2015 and, along with it,
the veriﬁcation and optimization of the programs executed in the blockchain
(known as Ethereum smart contracts) have raised considerable interest within
the research community. As for any other kind of programs, the main properties
of smart contracts are their eﬃciency and security. However, in the context of
the blockchain, these properties acquire even more relevance. As regards eﬃ-
ciency, due to the huge volume of transactions, the cost and response time of
the Ethereum blockchain platform have increased notably: the processing capac-
ity of the transactions is limited and it is providing low transaction ratios per
minute together with increased costs per transaction. Ethereum is aware of such
limitations and it is currently working on solutions to improve scalability with
the goal of increasing its capacity. As regards security, due to the public nature
and immutability of smart contracts and the fact that their public functions can
be executed by any user at any time, programming errors can be exploited by
attackers and have a high economic impact [7,13]. Veriﬁcation is key to ensure
the security of smart contract’s execution and provide safety guarantees. This
talk will present our work on the use of automated reasoning techniques and
tools to enhance the security and eﬃciency [2–4,6] of Ethereum smart contracts
along the two directions described below.
Security. Our main focus on security will be to detect and avoid potential
reentrancy attacks, one of the best known and exploited vulnerabilities that
have caused infamous attacks in the Ethereum ecosystem due to they economic
impact [9,11,15]. Reentrancy attacks might occur on programs with callbacks,
a mechanism that allows making calls among contracts. Callbacks occur when a
method of a contract invokes a method of another contract and the latter, either
directly or indirectly, invokes one or more methods of the former before the orig-
inal method invocation returns. While this mechanism is useful and powerful
This work was funded partially by the Ethereum Foundation (Grant FY21-0372), the
Spanish MCIU, AEI and FEDER (EU) project RTI2018-094403-B-C31 and by the CM
project S2018/TCS-4314 co-funded by EIE Funds of the European Union.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 3–7, 2022.
https://doi.org/10.1007/978-3-031-10769-6_1

4
E. Albert et al.
in event-driven programming, it has been used to exploit vulnerabilities. Our
approach to detect potential reentrancy problems is to ensure that the program
meets the Eﬀectively Callback Freeness (ECF) property [10]. ECF guarantees the
modularity of a contract in the sense that executions with callbacks cannot result
in new states that are not reachable by callback free executions. This implies
that the use of callbacks will not lead to unpredicted, potentially dangerous,
states. In order to ensure the ECF property, we use commutation and projection
of fragments of code [6]. Intuitively, given a function fragment A followed by B
(denoted A.B), in case we can receive a callback to some function f between
these fragments (that is, A.f.B), we ensure safety by proving that this execu-
tion that contains callbacks is equivalent to a callback free execution: either to
A.B (projection), f.A.B (left-commutation) or A.B.f (right-commutation). The
use of automated reasoning techniques enables proving this kind of properties.
Inspired by the use of SMT solvers to prove redundancy of concurrent executions
[1,8,16], we have implemented such checks using state-of-the-art SMT solvers.
The ECF property can be generalized to allow callbacks to introduce new
behaviors as long as they are benign, as [5] does by deﬁning the notion of R-ECF.
The main diﬀerence between ECF and R-ECF is that while ECF checks that
the states reached by executions with callbacks are exactly the same as the ones
reached by executions that do not contain callbacks, R-ECF checks that they
satisfy a relation with respect to the states reached without callbacks. This way,
R-ECF is able to recognize and distinguish the benign behaviors introduced by
callbacks from the ones that are potentially dangerous, while ECF cannot. The
main application of R-ECF is that, from a particular invariant of the program, it
allows reducing the problem of verifying the invariant in the presence of callbacks,
to the callback-free setting. For example, if we consider the invariant balance ≥
0 and prove that the contract is R-ECF with respect to the relation balancecb ≥
balancecbfree (i.e., the balance reached by executions with callbacks is greater
than the one reached without callbacks), then we only need to consider callback
free executions in order to prove the preservation of the invariant.
We considered as benchmarks the top-150 contracts based on volume of
usage, and studied the modularity of their functions in terms of ECF and R-
ECF. A total of 386 of their functions were susceptible to have callbacks, from
which 62.7% were veriﬁed to be ECF. The R-ECF approach was able to increase
the accuracy of the analysis, being able to prove the correctness of an extra 2%
of functions [5,6].
Eﬃciency. The main focus on eﬃciency will be on optimizing the resource
consumption of smart contract executions. On the Ethereum blockchain, the
resource consumption is measured in terms of gas, a unit introduced in the sys-
tem to quantify the computational eﬀort and charge a fee accordingly in order
to have a transaction executed. To understand how we can optimize gas, we
need to discuss it (and do it) at the level of the Ethereum bytecode. Smart con-
tracts in Ethereum are executed using the Ethereum Virtual Machine (EVM).
The EVM is a simple stack-based architecture which uses 256-bit words and
has its own repertory of instructions (EVM opcodes). In the EVM, the mem-

Using Automated Reasoning Techniques for Enhancing the Eﬃciency
5
ory model is split into two diﬀerent structures: the storage, which is persistent
between transactions and expensive to use; and the memory, which does not
persist between transactions and is cheaper. Each opcode has a gas cost associ-
ated to its execution. Besides, an additional fee must be paid for each byte when
the smart contract is deployed. Thus, the resource to be optimized can be either
the total amount of gas in a program or its size. Even though both criteria are
usually related, there are some situations in which they do not correlate. For
instance, pushing a big number in the stack consumes a small amount of gas
and increases signiﬁcantly the bytecode size, whereas obtaining the same value
using arithmetic operations is more expensive but involves fewer bytes.
Among all possible techniques to optimize code, we have used the technique
known as superoptimization [12]. The main idea of superoptimization is auto-
matically ﬁnding an equivalent optimal sequence of instructions to another given
loop-free sequence. In order to achieve this goal, we enumerate all possible can-
didates and determine the best option among them wrt.the optimization cri-
teria. In the context of EVM, there exists several superoptimizers: EBSO [14],
SYRUP [3,4] and GASOL [2]. The techniques presented in this work correspond
to the ones implemented in GASOL, which are an improvement and extension
of the ones in SYRUP. We apply two kinds of automated reasoning techniques
to superoptimize Ethereum smart contracts, symbolic execution and Max-SMT
as described next.
– Symbolic execution is used to obtain a a representation on how the stack and
memory evolves wrt. to an initial stack. We determine the lowest size of the
stack needed to perform all the operations in a block and apply symbolic exe-
cution to an initial stack containing that number of unknown stack variables.
Opcodes representing operations that don’t manage the stack are left as unin-
terpreted functions. Then, we apply as many simpliﬁcation rules as possible
from a ﬁxed set of rules. Depending on the chosen criteria, some rules are
disabled if they lead to worse candidates. Moreover, we apply static analysis
regarding memory opcodes to determine whether there are some redundant
store or load operations inside a block that can be safely removed or replaced.
This leads to a simpliﬁed speciﬁcation of the optimal block.
– The second technique involves synthesizing the optimal block from a given
symbolic representation using a Max-SMT solver. The synthesis problem is
expressed as a ﬁrst-order formula in which every model corresponds to a
valid equivalent block. Our encoding is expressed in the simple logic QF IDL,
so that the Max-SMT solver can reason eﬀectively on EVM blocks. In this
encoding, the length of the sequence of instructions is ﬁxed by an upper
bound so that quantiﬁers are avoided. NOP operations are considered in the
encoding to allow shorter sequences. The state of the stack is represented
explicitly for each position in the sequence. Every instruction in the block and
every basic stack operation have a constraint that reﬂects the impact they
have on the stack for each possible position. Memory accesses are encoded
as a partial order relation that synthesizes the dependencies among them.
Regarding the optimization process, we express the cost (gas or bytes-size) of

6
E. Albert et al.
each instruction using soft constraints. For both criteria, the corresponding
set of soft constraints satisﬁes that an optimal model returned by the solver
corresponds to an optimal block for that criteria.
Combining both approaches, we obtain signiﬁcant savings for both criteria.
For a subset of 30 smart contracts, selected among the latest published in Ether-
scan as of June 21, 2021 and optimized using the compiler solc v0.8.9, GASOL
still manages to reduce 0.72% the amount of gas with the gas criteria enabled,
and decreases the overall size by 3.28% with the size criteria enabled.
Future work. The current directions for future work include enhancing the per-
formance of the smart contract optimizer in both accuracy and scalability of the
process while keeping the eﬃciency. For the accuracy we are currently working
on adding further reasoning on non-stack operations while staying in a quite
simple logic. This will allow us to consider a wider set of equivalent blocks and
hence increase the savings. Scalability can be threatened when we consider blocks
of code of large size. We are investigating diﬀerent approaches to scale better,
including heuristics to partition the blocks in smaller sub-blocks, more eﬃcient
SMT encodings, among others. Finally, another direction for future work is to
formally prove the correctness of the optimizer, i.e.developing a checker that
can formally prove the equivalence of the optimized and the original (Ethereum)
bytecode. For this, we are planning to use the Coq proof assistant in which
we will develop a checker that, given an original bytecode –that corresponds
a block of the control ﬂow graph– and its optimization, it can formally prove
their equivalence for any possible execution, and optionally it can generate a
soundness proof that can be used as certiﬁcate.
References
1. Albert, E., G´omez-Zamalloa, M., Isabel, M., Rubio, A.: Constrained dynamic par-
tial order reduction. In: Chockler, H., Weissenbacher, G. (eds.) CAV 2018. LNCS,
vol. 10982, pp. 392–410. Springer, Cham (2018). https://doi.org/10.1007/978-3-
319-96142-2 24
2. Albert, E., Gordillo, P., Hern´andez-Cerezo, A., Rubio, A.: A Max-SMT superopti-
mizer for EVM handling memory and storage. In: Fisman, D., Rosu, G. (eds) Tools
and Algorithms for the Construction and Analysis of Systems. TACAS 2022. LNCS,
vol. 13243. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-99524-9 11
3. Albert, E., Gordillo, P., Hern´andez-Cerezo, A., Rubio, A., Schett, M.A.: Super-
optimization of smart contracts. ACM Trans. Softw. Eng. Methodol. (2022)
4. Albert, E., Gordillo, P., Rubio, A., Schett, M.A.: Synthesis of super-optimized
smart contracts using Max-SMT. In: Lahiri, S.K., Wang, C. (eds.) CAV 2020.
LNCS, vol. 12224, pp. 177–200. Springer, Cham (2020). https://doi.org/10.1007/
978-3-030-53288-8 10
5. Albert, E., Grossman, S., Rinetzky, N., Nunez, C.R., Rubio, A., Sagiv, M.: Relaxed
eﬀective callback freedom: a parametric correctness condition for sequential mod-
ules with callbacks. IEEE Trans. Dependable Secure Comput. (2022)

Using Automated Reasoning Techniques for Enhancing the Eﬃciency
7
6. Albert, E., Grossman, S., Rinetzky, N., Rodr´ıguez-N´u˜nez, C., Rubio, A., Sagiv,
M.: Taming callbacks for smart contract modularity. In: Proceedings of the ACM
SIGPLAN Conference on Object-Oriented Programming Systems, Languages and
Applications, OOPSLA 2020, vol. 4, pp. 209:1–209:30 (2020)
7. Atzei, N., Bartoletti, M., Cimoli, T.: A survey of attacks on ethereum smart con-
tracts (SoK). In: Maﬀei, M., Ryan, M. (eds.) POST 2017. LNCS, vol. 10204, pp.
164–186. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-662-54455-
6 8
8. Bansal, K., Koskinen, E., Tripp, O.: Automatic generation of precise and useful
commutativity conditions. In: Beyer, D., Huisman, M. (eds.) TACAS 2018. LNCS,
vol. 10805, pp. 115–132. Springer, Cham (2018). https://doi.org/10.1007/978-3-
319-89960-2 7
9. Daian, P.: Analysis of the DAO exploit (2016). http://hackingdistributed.com/
2016/06/18/analysis-of-the-dao-exploit/
10. Grossman, S., et al.: Online detection of eﬀectively callback free objects with appli-
cations to smart contracts. PACMPL, 2(POPL) (2018)
11. Liu,
M.:
Urgent:
OUSD
was
hacked
and
there
has
been
a
loss
of
funds (2020). https://medium.com/originprotocol/urgent-ousd-has-hacked-and-
there-has-been-a-loss-of-funds-7b8c4a7d534c. Accessed 29 Jan 2021
12. Massalin, H.: Superoptimizer - a look at the smallest program. In: Proceedings of
the Second International Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS II), pp. 122–126 (1987)
13. Mehar, M.I., et al.: Understanding a revolutionary and ﬂawed grand experiment
in blockchain: the DAO attack. J. Cases Inf. Technol. 21(1), 19–32 (2019)
14. Nagele, J., Schett, M.A.: Blockchain superoptimizer. In: Proceedings of 29th
International Symposium on Logic-Based Program Synthesis and Transformation
(LOPSTR) (2019). https://arxiv.org/abs/2005.05912
15. Tarasov, A.: Millions lost: the top 19 DeFi cryptocurrency hacks of 2020 (2020).
https://cryptobrieﬁng.com/50-million-lost-the-top-19-deﬁ-cryptocurrency-hacks-
2020/2. Accessed 29 Jan 2021
16. Wang, C., Yang, Z., Kahlon, V., Gupta, A.: Peephole partial order reduction. In:
Ramakrishnan, C.R., Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 382–396.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-78800-3 29
17. Wood, G.: Ethereum: a secure decentralised generalised transaction ledger (2019)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

From the Universality of Mathematical
Truth to the Interoperability of Proof
Systems
Gilles Dowek(B)
Inria and ENS Paris-Saclay, Paris, France
gilles.dowek@ens-paris-saclay.fr
1
Yet Another Crisis of the Universality of Mathematical
Truth
The development of computerized proof systems, such as Coq, Matita, Agda,
Lean, HOL 4, HOL Light, Isabelle/HOL, Mizar, etc. is a major step
forward in the never ending quest of mathematical rigor. But it jeopardizes the
universality of mathematical truth [5]: we used to have proofs of Fermat’s little
theorem, we now have Coq proofs of Fermat’s little theorem, Isabelle/HOL
proofs of Fermat’s little theorem, PVS proofs of Fermat’s little theorem, etc.
Each proof system: Coq, Isabelle/HOL, PVS, etc. deﬁning its own language
for mathematical statements and its own truth conditions for these statements.
This crisis can be compared to previous ones, when mathematicians have
disagreed on the truth of some mathematical statements: the discovery of the
incommensurability of the diagonal and side of a square, the introduction of
inﬁnite series, the non-Euclidean geometries, the discovery of the independence
of the axiom of choice, and the emergence of constructivity. All these past crises
have been resolved.
2
Predicate Logic and Other Logical Frameworks
One way to resolve a crisis, such as that of non-Euclidean geometries, or that of
the axiom of choice, is to view geometry, or set theory, as an axiomatic theory.
The judgement that the statement the sum of the angles in a triangle equals
the straight angle is true evolves to that that it is a consequence of the parallel
axiom and of the other axioms of geometry. Thus, the truth conditions must
be deﬁned, not for the statements of geometry, but for arbitrary sequents: pairs
Γ ⊢A formed with a theory, a set of axioms, Γ and a statement A.
This induces a separation between the deﬁnition of the truth conditions of
a sequent: the logical framework and the deﬁnition of the various geometries
as theories in this logical framework. This logical framework, Predicate logic,
was made precise by Hilbert and Ackermann [13], in 1928, more than a century
after the beginning of the crisis of non-Euclidean geometries. The invention of
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 8–11, 2022.
https://doi.org/10.1007/978-3-031-10769-6_2

From the Universality of Mathematical Truth to the Interoperability
9
Predicate Logic was a huge step forward. But Predicate Logic also has some
limitations.
To overcome these limitation, it has been modernized in various ways in the
last decades. First, λ-Prolog [15] and Isabelle [17] have extended Predicate
logic with variable binding function symbols, such as the symbol λ in the term
λx x. Then, the λΠ-calculus [12] has permitted to explicitly represent proof-
trees, using the so-called Brouwer-Heyting-Kolmogorov algorithmic interpreta-
tion of proofs and Curry-de Bruijn-Howard correspondence. In a second stream
of research, Deduction modulo theory [4,6] has introduced a distinction between
computation and deduction, in such a way that the statement 27 × 37 = 999
computes to 999 = 999, with the algorithm of multiplication, and then to ⊤,
with the algorithm of natural number comparison. It thus has a trivial proof. A
third stream of research has extended classical Predicate logic to an Ecumeni-
cal predicate logic [3,9–11,14,18,19] with both constructive and classical logical
constants.
These streams of research have merged, to provide a logical framework, the
λΠ-calculus modulo theory [2], also called Martin-L¨of’s logical framework [16].
This framework permits function symbols to bind variables, it includes an explicit
representation for proof-trees, it distinguishes computation from deduction, and
it permits to deﬁne both constructive and classical logical constants. It is the
basis of the language Dedukti, where Simple type theory, Martin-L¨of’s type
theory, the Calculus of constructions, etc. can easily be expressed.
3
The Theory U
The expression in Dedukti of Simple type theory, Simple type theory with
polymorphism, Simple type theory with predicate subtyping, the Calculus of
constructions, etc. use symbol declarations and computation rules that play the
rˆole of axioms in Predicate logic. But, just like the various geometries or the
various set theories share a lot of axioms and distinguish by a few, these theories
share a lot of symbols and rules. This remark leads to deﬁning a large theory,
the theory U [1], that contains Simple type theory, Simple type theory with
polymorphism, Simple type theory with predicate subtyping, and the Calculus
of constructions, etc. as sub-theories.
Many proofs developed in proof processing systems can be expressed in the
theory U and depending on the symbols and rules they use they can be translated
to more common formulations of the theories implemented in these systems.
For instance, F. Thir´e has expressed a large library of arithmetic, originally
developed in Matita, in an sub-theory of the theory U, corresponding to Sim-
ple type theory with polymorphism and translated these proofs to the language
of seven proof systems [20], Y. G´eran has expressed the ﬁrst book of Euclid’s
elements originally developed in Coq, in a sub-theory of the theory U, cor-
responding to Predicate logic, and translated these proofs to the language of
many proof systems, including predicate logic ones [8], and T. Felicissimo has
shown that a large library of proofs originally developed in Matita, including

10
G. Dowek
a proof of Bertrand’s postulate, could be expressed in predicative type theory
and expressed in Agda [7].
References
1. Blanqui, F., Dowek, G., Grienenberger, ´E., Hondet, G., Thir´e, F.: Some axioms
for mathematics. In: Kobayashi, N. (ed.) Formal Structures for Computation and
Deduction, vol. 195, pp. 20:1–20:19. LIPIcs. Schloss Dagstuhl - Leibniz-Zentrum
f¨ur Informatik (2021)
2. Cousineau, D., Dowek, G.: Embedding pure type systems in the lambda-pi-calculus
modulo. In: Della Rocca, S.R. (ed.) TLCA 2007. LNCS, vol. 4583, pp. 102–117.
Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-73228-0 9
3. Dowek, G.: On the deﬁnition of the classical connectives and quantiﬁers. In:
Haeusler, E.H., de Campos Sanz, W., Lopes, B. (eds.) Why is this a Proof?
Festschrift for Luiz Carlos Pereira. College Publications (2015)
4. Dowek, G., Hardin, T., Kirchner, C.: Theorem proving modulo. J. Autom. Reason.
31, 33–72 (2003). https://doi.org/10.1023/A:1027357912519
5. Dowek, G., Thir´e, F.: The universality of mathematical truth jeopardized by the
development of computerized proof systems. In: Arana, A., Pataut, F. (eds.) Proofs,
To be published
6. Dowek, G., Werner, B.: Proof normalization modulo. J. Symb. Log. 68(4), 1289–
1316 (2003)
7. Felicissimo, T., Blanqui, F., Kumar Barnawal, A.: Predicativize: sharing proofs
with predicative systems. Manuscript (2022)
8. G´eran, Y.: Math´ematiques invers´ees de Coq. l’exemple de GeoCoq. Master thesis
(2021)
9. Gilbert, F.: Extending higher-order logic with predicate subtyping: application to
PVS. (Extension de la logique d’ordre sup´erieur avec le sous-typage par pr´edicats).
PhD thesis, Sorbonne Paris Cit´e, France (2018)
10. Girard, J.-Y.: On the unity of logic. Ann. Pure Appl. Logic 59(3), 201–217 (1993)
11. Grienenberger, ´E.: A logical system for an ecumenical formalization of mathemat-
ics. Manuscript (2020)
12. Harper, R., Honsell, F., Plotkin, G.: A framework for deﬁning logics. J. ACM
40(1), 143–184 (1993)
13. Hilbert, D., Ackermann, W.: Grundz¨uge der theoretischen Logik. Springer-Verlag
(1928)
14. Liang, C., Miller, D.: Unifying classical and intuitionistic logics for computational
control. In: 28th Symposium on Logic in Computer Science, pp. 283–292 (2013)
15. Miller, D., Nadathur, G.: Programming with Higher-Order Logic. Cambridge Uni-
versity Press (2012)
16. Nordstr¨om, B., Petersson, K., Smith, J.M.: Programming in Martin-L¨of’s type
theory. Oxford University Press (1990)
17. Paulson, L.C.: Isabelle: the next 700 theorem provers. In: Odifreddi, P. (ed.) Logic
and Computer Science, pp. 361–386. Academic Press (1990)
18. Pereira, L.C., Rodriguez, R.O.: Normalization, soundness and completeness for the
propositional fragment of Prawitz’ecumenical system. Rev. Port. Filos. 73(3–4),
1153–1168 (2017)
19. Prawitz, D.: Classical versus intuitionistic logic. In: Haeusler, E.H., de Campos
Sanz, W., Lopes, B. (eds.) Why is this a Proof? Festschrift for Luiz Carlos Pereira.
College Publications (2015)

From the Universality of Mathematical Truth to the Interoperability
11
20. Thir´e, F.: Sharing a library between proof assistants: reaching out to the HOL fam-
ily. In: Blanqui, F., Reis, G. (eds.) Proceedings of the 13th International Workshop
on Logical Frameworks and Meta-Languages, vol. 274, pp. 57–71. EPTCS (2018)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Satisﬁability, SMT Solving,
and Arithmetic

Flexible Proof Production
in an Industrial-Strength SMT Solver
Haniel Barbosa1, Andrew Reynolds2, Gereon Kremer3, Hanna Lachnitt3,
Aina Niemetz3, Andres N¨otzli3, Alex Ozdemir3, Mathias Preiner3,
Arjun Viswanathan2, Scott Viteri3, Yoni Zohar4(B), Cesare Tinelli2,
and Clark Barrett3
1 Universidade Federal de Minas Gerais, Belo Horizonte, Brazil
2 The University of Iowa, Iowa City, USA
3 Stanford University, Stanford, USA
4 Bar-Ilan University, Ramat Gan, Israel
yoni206@gmail.com
Abstract. Proof production for SMT solvers is paramount to ensure
their correctness independently from implementations, which are often
prohibitively diﬃcult to verify. Historically, however, SMT proof pro-
duction has struggled with performance and coverage issues, resulting in
the disabling of many crucial solving techniques and in coarse-grained
(and thus hard to check) proofs. We present a ﬂexible proof-production
architecture designed to handle the complexity of versatile, industrial-
strength SMT solvers and show how we leverage it to produce detailed
proofs, including for components previously unsupported by any solver.
The architecture allows proofs to be produced modularly, lazily, and with
numerous safeguards for correctness. This architecture has been imple-
mented in the state-of-the-art SMT solver cvc5. We evaluate its proofs
for SMT-LIB benchmarks and show that the new architecture produces
better coverage than previous approaches, has acceptable performance
overhead, and supports detailed proofs for most solving components.
1
Introduction
SMT solvers [9] are widely used as backbones of formal methods tools in a
variety of applications, often safety-critical ones. These tools rely on the solver’s
correctness to guarantee the validity of their results such as, for instance, that an
access policy does not inadvertently give access to sensitive data [4]. However,
SMT solvers, particularly industrial-strength ones, are often extremely complex
pieces of engineering. This makes it hard to ensure that implementation issues do
not aﬀect results. As the industrial use of SMT solvers increases, it is paramount
to be able to convince non-experts of the trustworthiness of their results.
A solution is to decouple conﬁdence from the implementation by coupling
results with machine-checkable certiﬁcates of their correctness. For SMT solvers,
This work was partially supported by the Oﬃce of Naval Research (Contract No.
68335-17-C-0558), a gift from Amazon Web Services, and by NSF-BSF grant numbers
2110397 (NSF) and 2020704 (BSF).
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 15–35, 2022.
https://doi.org/10.1007/978-3-031-10769-6_3

16
H. Barbosa et al.
this amounts to providing proofs of unsatisﬁability. The main challenges are
justifying a combination of theory-speciﬁc algorithms while keeping the solver
performant and providing enough details to allow scalable proof checking, i.e.,
checking that is fundamentally simpler than solving. Moreover, while proof pro-
duction is well understood for propositional reasoning and common theories,
that is not the case for more expressive theories, such as the theory of strings,
or for more advanced solver operations such as formula preprocessing.
We present a new, ﬂexible proof-production architecture for versatile, indus-
trial-strength SMT solvers and discuss its integration into the cvc5 solver [5]. The
architecture (Sect. 2) aims to facilitate the implementation eﬀort via modular
proof production and internal proof checking, so that more critical components
can be enabled when generating proofs. We provide some details on the core proof
calculus and how proofs are produced (Sect. 3), in particular how we support
eager and lazy proof production with built-in proof reconstruction (Sect. 3.2).
This feature is particularly important for substitution and rewriting techniques,
facilitating the instrumentation of notoriously challenging functionalities, such as
simpliﬁcation under global assumptions [6, Section 6.1] and string solving [40,46,
48], to produce detailed proofs. Finally, we describe (Sect. 5) how the architecture
is leveraged to produce detailed proofs for most of the theory reasoning, critical
preprocessing, and underlying SAT solving of cvc5. We evaluate proof production
in cvc5 (Sect. 6) by measuring the proof overhead and the proof quality over an
extensive set of benchmarks from SMT-LIB [8].
In summary, our contributions are a ﬂexible proof-producing architecture
for state-of-the-art SMT solvers, its implementation in cvc5, the production of
detailed proofs for simpliﬁcation under global assumptions and the full theory
of strings, and initial experimental evidence that proof-production overhead is
acceptable and detailed proofs can be generated for a majority of the problems.
Preliminaries. We assume the usual notions and terminology of many-sorted
ﬁrst-order logic with equality (≈) [29]. We consider signatures Σ all containing
the distinguished Boolean sort Bool. We adopt the usual deﬁnitions of well-sorted
Σ-terms, with literals and formulas as terms of sort Bool, and Σ-interpretations.
A Σ-theory is a pair T = (Σ, I) where I, the models of T, is a class of Σ-
interpretations closed under variable reassignment. A Σ-formula ϕ is T-valid
(resp., T-unsatisﬁable) if it is satisﬁed by all (resp., no) interpretations in I.
Two Σ-terms s and t of the same sort are T-equivalent if s ≈t is T-valid.
We write ⃗a to denote a tuple (a1, . . . , an) of elements, with n ≥0. Depending
on context, we will abuse this notation and also denote the set of the tuple’s
elements or, in case of formulas, their conjunction. Similarly, for term tuples ⃗s,⃗t
of the same length and sort, we will write ⃗s ≈⃗t to denote the conjunction of
equalities between their respective elements.
2
Proof-Production Architecture
Our proof-production architecture is intertwined with the CDCL(T ) architec-
ture [43], as shown in Fig. 1. Proofs are produced and stored modularly by each
solving component, which also checks they meet the expected proof structure

Flexible Proof Production in an Industrial-Strength SMT Solver
17
Pre-processor
ϕ
Propositional Engine
Clausiﬁer
SAT Solver
Post-processor
Cp
1 ... Cp
m
P : ⃗C →⊥
P : ψ1 →C1
...
P : ψm →Cm
φ1 ... φn
Theory Engine
Theory Combination
T1
T2
P : L1
L1
L2
P : L2
...
Tk
Lk P : Lk
L
P : L
Asserted Literals
SMT Proof Post-processor
P : ϕ →⊥
⊥
P : ⃗φ →⊥
P : ϕ →φ1
...
P : ϕ →φn
Fig. 1. Flexible proof-production architecture for CDCL(T )-based SMT solvers. In the
above, ψi ∈{⃗φ, ⃗L} for each i, with ψi not necessarily distinct from ψi+1.
for that component, as described below. Proofs are combined only when needed,
via post-processing. The pre-processor receives an input formula ϕ and simpliﬁes
it in a variety of ways into formulas φ1, . . . , φn. For each φi, the pre-processor
stores a proof P : ϕ →φi justifying its derivation from ϕ.
The propositional engine receives the preprocessed formulas, and its clausiﬁer
converts them into a conjunctive normal form C1 ∧· · · ∧Cl. A proof P : ψ →Ci
is stored for each clause Ci, where ψ is a preprocessed formula. Note that sev-
eral clauses may derive from each formula. Corresponding propositional clauses
Cp
1 , . . . , Cp
l , where ﬁrst-order atoms are abstracted as Boolean variables, are sent
to the SAT solver, which checks their joint satisﬁability. The propositional engine
enters a loop with the theory engine, which considers a set of literals asserted
by the SAT solver (corresponding to a model of the propositional clauses) and
veriﬁes its satisﬁability modulo a combination of theories T. If the set is T-
unsatisﬁable, a lemma L is sent to the propositional engine together with its
proof P : L. Note that since lemmas are T-valid, their proofs have no assump-
tions. The propositional engine stores these proofs and clausiﬁes the lemmas,
keeping the respective clausiﬁcation proofs in the clausiﬁer. The clausiﬁed and
abstracted lemmas are sent to the SAT solver to block the current model and
cause the assertion of a diﬀerent set of literals, if possible. If no new set is
asserted, then all the clauses C1, . . . , Cm generated until then are jointly unsat-
isﬁable, and the SAT solver yields a proof P : C1 ∧· · · ∧Cm →⊥. Note that
the proof is in terms of the ﬁrst-order clauses, as are the derivation rules that

18
H. Barbosa et al.
conclude ⊥from them. The propositional abstraction does not need to be rep-
resented in the proof.
The post-processor of the propositional engine connects the assumptions of
the SAT solver proof with the clausiﬁer proofs, building a proof P : φ1∧· · ·∧φn →
⊥. Since theory lemmas are T-valid, the resulting proof only has preprocessed
formulas as assumptions. The ﬁnal proof is built by the SMT solver’s post-
processor combining this proof with the preprocessing proofs P : ϕ →φi. The
resulting proof P : ϕ →⊥justiﬁes the T-unsatisﬁability of the input formula.
3
The Internal Proof Calculus
In this section, we specify how proofs are represented in the internal calculus of
cvc5. We also provide some low-level details on how proofs are constructed and
managed in our implementation.
The proof rules of the internal calculus are similar to rules in other calculi for
ground ﬁrst-order formulas, except that they are made a little more operational
by optionally having argument terms and side conditions. Each rule has the form
r ϕ1 · · · ϕn
ψ
or
r ϕ1 · · · ϕn | t1, . . . , tm
ψ
if C
with identiﬁer r, premises ϕ1, . . . , ϕn, arguments t1, . . . , tm, conclusion ψ, and
side condition C. The argument terms are used to construct the conclusion from
the premises and can be used in the side condition together with the premises.
3.1
Proof Checkers and Proofs
The semantics of each proof rule r is provided operationally in terms of a proof-
rule checker for r. This is a procedure that takes as input a list of argument
terms ⃗t and a list of premises ⃗ϕ for r. It returns fail if the input is malformed,
i.e., it does not match the rule’s arguments and premises or does not satisfy the
side condition. Otherwise, it returns a conclusion formula ψ expressing the result
of applying the rule. All proof rules of the internal calculus have an associated
proof-rule checker. We say that a proof rule proves a formula ψ, from given
arguments and premises, if its checker returns ψ.
cvc5 has an internal proof checker built modularly out of the individual
proof-rule checkers. This checker is meant mostly for internal debugging dur-
ing development, to help guarantee that the constructed proofs are correct. The
expectation is that users will rely instead on third-party tools to check the proof
certiﬁcates emitted by the solver.
A proof object is constructed internally using a data structure that we will
describe abstractly here and call a proof node. This is a triple (r, ⃗N, ⃗t) consisting
of a rule identiﬁer r; a sequence ⃗N of proof nodes, its children; and a sequence ⃗t
of terms, its arguments. The relationships between proof nodes and their children
induces a directed graph over proof nodes, with edges from proofs nodes to their
children. We call a single-root graph rooted at node N a proof. A proof P is

Flexible Proof Production in an Industrial-Strength SMT Solver
19
Fig. 2. Core proof rules of the internal calculus.
well-formed if it is ﬁnite, acyclic, and there is a total mapping Ψ from the
nodes of P to formulas such that, for each node N = (r, (N1, . . . , Nm), ⃗t), Ψ(N)
is the formula returned by the proof checker for rule r when given premises
Ψ(N1), . . . , Ψ(Nn) and arguments ⃗t. For a well-formed proof P with root N and
mapping Ψ, the conclusion of P is the formula Ψ(N); a subproof of P is any
proof rooted at a descendant of N in P. For convenience, we will identify a
well-formed proof with its root node from now on.
3.2
Core Proof Rules
In total, the internal calculus of cvc5 consists of 155 proof rules,1 which cover
all reasoning performed by the SMT solver, including theory-speciﬁc rules, rules
for Boolean reasoning, and others. In the remainder of this section, we describe
the core rules of the internal calculus, which are used throughout the system,
and are illustrated in Fig. 2.
Proof Rules for Equality. Many theory solvers in cvc5 perform theory-speciﬁc
reasoning on top of basic equational reasoning. The latter is captured by the
proof rules eq res, reﬂ, symm, trans, and cong. The ﬁrst rule is used to prove a
formula ψ from a formula ϕ that was proved equivalent to ψ. The rest are the
standard rules for computing the congruence closure of a set of term equalities.
Proof Rules for Rewriting, Substitution and Witness Forms. A single
coarse-grained rule, sr, is used for tracking justiﬁcations for core utilities in the
SMT solver such as rewriting and substitution. This rule, together with other
non-core rules with side conditions (omitted for brevity), allows the generation of
coarse-grained proofs that trust the correctness of complex side conditions. Those
conditions involve rewriting and substitution operations performed by cvc5 dur-
ing solving. More ﬁne-grained proofs can be constructed from coarse-grained
ones by justifying the various rewriting and substitution steps in terms of sim-
pler proof rules. This is done with the aid of the equality rules mentioned above
and the additional core rules atom rewrite and witness. To describe atom rewrite,
witness, and sr, we ﬁrst need to introduce some deﬁnitions and notations.
1 See https://cvc5.github.io/docs/cvc5-1.0.0/proofs/proof rules.html.

20
H. Barbosa et al.
A rewriter R is a function over terms that preserves equivalence in the back-
ground theory T, i.e., returns a term t↓R T-equivalent to its input t. We call
t↓R the rewritten form of t with respect to R. Currently, cvc5 uses a handful
of specialized rewriters for various purposes, such as evaluating constant terms,
preprocessing input formulas, and normalizing terms during solving. Each indi-
vidual rewrite step executed by a rewriter R is justiﬁed in ﬁne-grained proofs
by an application of the rule atom rewrite, which takes as argument both (an
identiﬁer for) R and the term s the rewrite was applied to. Note that the rule’s
soundness requires that the rewrite step be equivalence preserving.
A (term) substitution σ is a ﬁnite sequence (t1 →s1, . . . , tn →sn) of oriented
pairs of terms of the same sort. A substitution method S is a function that takes a
term r and a substitution σ and returns a new term that is the result of applying
σ to r, according to some strategy. We write S(r, σ) to denote the resulting term.
We distinguish three kinds of substitution methods for σ: simultaneous, which
returns the term obtained by simultaneously replacing every occurrence of term
ti in r with si, for i = 1, . . . , n; sequential, which splits σ into n substitutions
(t1 →s1), . . . , (tn →sn) and applies them in sequence to r using the simultane-
ous strategy above; and ﬁxed-point, which, starting with r, repeatedly applies σ
with the simultaneous strategy until no further subterm replacements are pos-
sible. For example, consider the application S(y, (x →u, y →f(z), z →g(x))).
The steps the substitution method takes in computing its result are the fol-
lowing: y ⇝f(z) if S is simultaneous; y ⇝f(z) ⇝f(g(x)) if S is sequential;
y ⇝f(z) ⇝f(g(x)) ⇝f(g(u)) if S is ﬁxed-point.
In cvc5, we use a substitution derivation method D to derive a contextual
substitution (t1 →s1, . . . , tn →sn) from a collection ⃗ϕ of derived formulas. The
substitution essentially orients a selection of term equalities ti ≈si entailed by
⃗ϕ and, as such, can be applied soundly to formulas derived from ⃗ϕ.2 We write
D(⃗ϕ) to denote the substitution computed by D from ⃗ϕ.
Finally, cvc5 often introduces fresh variables, or Skolem variables, which are
implicitly globally existentially quantiﬁed. This happens as a consequence of
Skolemization of existential variables, lifting of if-then-else terms, and some kinds
of ﬂattening. Each Skolem variable k is associated with a term k↑of the same
sort containing no Skolem variables, called its witness term. This global map
from Skolem variables to their witness term allows cvc5 to detect when two
Skolem variables can be equated, as a consequence of their respective witness
terms becoming equivalent in the current context [47]. Witness terms can also be
used to eliminate Skolem variables at proof output time. We write t↑to denote
the witness form of term t, which is obtained by replacing every Skolem variable
in t by its witness term. For example, if k1 and k2 are Skolem variables with
associated witness terms ite(x ≈z, y, z) and y −z, respectively, and ϕ is the
formula ite(x ≈k2, k1 ≈y, k1 ≈z), the witness form ϕ↑of ϕ is the formula
ite(x ≈y −z, ite(x ≈z, y, z) ≈y, ite(x ≈z, y, z) ≈z). When a Skolem variable k
2 Observe that substitutions are generated dynamically from the formulas being pro-
cessed, whereas rewrite rules are hard-coded in cvc5’s rewriters.

Flexible Proof Production in an Industrial-Strength SMT Solver
21
appears in a proof, the witness proof rule is used to explicitly constrain its value
to be the same as that of the term k↑it abstracts.3
We can now explain the sr proof rule, which is parameterized by a substitution
method S, a rewriter R, and substitution derivation method D. The rule is used
to transform the proof of a formula ϕ into one of a formula ψ provided that the
two formulas are equal up to rewriting under a substitution derived from the
premises ⃗ϕ. Note that this rule is quite general because its conclusion ψ, which
is provided as an argument, can be any formula that satisﬁes the side condition.
Proof Rules for Scoped Reasoning. Two of the core proof rules, assume
and scope, enable local reasoning. Together they achieve the eﬀect of the ⇒-
introduction rule of Natural Deduction. However, separating the local assump-
tion functionality in assume provides more ﬂexibility. That rule has no premises
and introduces a local assumption ϕ provided as an argument. The scope rule
is used to close the scope of the local assumptions ϕ1, . . . , ϕn made to prove a
formula ϕ, inferring the formula ϕ1 ∧· · · ∧ϕn ⇒ϕ.
We say that ϕ is a free assumption in proof P if P has a node (assume, (), ϕ)
that is not a subproof of a scope node with ϕ as one of its arguments. A proof
is closed if it has no free assumptions, and open otherwise.
Soundness. All proof rules other than assume are sound with respect to the
background theory T in the following sense: if a rule proves a formula ψ from
premises ⃗ϕ, every model of T that satisﬁes ⃗ϕ, and assigns the same values to
Skolem variables and their respective witness term, satisﬁes ψ as well. Based on
this and a simple structural induction argument, one can show that well-formed
closed proofs have T-valid conclusions. In contrast, open proofs have conclusions
that are T-valid only under assumptions. More precisely, in general, if ⃗ϕ are all
the free assumptions of a well-formed proof P with conclusion ψ and ⃗k are all
the Skolem variables introduced in P, then ⃗k ≈⃗k↑∧⃗ϕ ⇒ψ is T-valid.
3.3
Constructing Proof Nodes
We have implemented a library of proof generators that encapsulates common
patterns for constructing proof nodes. We assume a method getProof that takes
the proof generator g and a formula ϕ as input and returns a proof node with
conclusion ϕ based on the information in g. During solving, cvc5 uses a combina-
tion of eager and lazy proof generation. In general terms, eager proof generation
involves constructing proof nodes for inference steps at the time those steps are
taken during solving. Eager proof generation may be required if the computation
state pertinent to that inference cannot be easily recovered later. In contrast,
lazy proof generation occurs for inferred formulas associated with proof genera-
tors that can do internal bookkeeping to be able to construct proof nodes for the
formula after solving is completed. Depending on the formula, diﬀerent kinds of
proof generators are used. For brevity, we only describe in detail (see Sect. 3.2)
3 The proof rules that account for the introduction of Skolem variables in the ﬁrst
place are not part of the core set and so are not discussed here.

22
H. Barbosa et al.
Algorithm 1 . Proof generation for term-conversion generators, rewrite-once
policy. B is a lazy proof builder, R a map from terms to their converted form,
and cpre, cpost are sets of pairs of equalities and the proof generators justifying
them.
getProof(g, ϕ) where g contains cpre, cpost and ϕ is t1 ≈t2
1: B := ∅, R := ∅
2: getTermConv(t1, cpre, cpost, B, R)
3: if R[t1] ̸= t2 then fail else return getProof(B, t1 ≈R[t1])
getTermConv(s, cpre, cpost, B, R), where s = f(s1, . . . , sn)
1: if s in dom(R) then return
2: if (s ≈s′, g′) ∈cpre for some s′, g′ then
3:
R[s] := s′, addLazyStep(B, s ≈s′, g′)
4:
return
5: for 1 ≤i ≤n do getTermConv(si, cpre, cpost, B, R)
6: R[s] := r, where r = f(R[s1], . . . , R[sn])
7: if s ̸= r then addStep(B, cong, (s1 ≈R[s1], . . . , sn ≈R[sn]), f)
8: else addStep(B, rﬂ, (), s ≈s)
9: if (r ≈r′, g′) ∈cpost for some r′, g′ then
10:
R[s] := r′, addLazyStep(B, r ≈r′, g′), addStep(B, trans, (s ≈r, r ≈r′), ())
the proof generator most relevant to the core calculus, the term-conversion proof
generator, targeted for substitution and rewriting proofs.
4
Proof Reconstruction for Substitution and Rewriting
Once it determines that the input formulas ϕ1, . . . , ϕn are jointly unsatisﬁable,
the SMT solver has a reference to a proof node P that concludes ⊥from the
free assumptions ϕ1, . . . , ϕn. After the post-processor is run, the (closed) proof
(scope, P ′, (ϕ1, . . . , ϕn)) is then generated as the ﬁnal proof for the user, where
P ′ is the result of optionally expanding coarse-grained steps (in particular, appli-
cations of the rule sr) in P into ﬁne-grained ones. To do so, we require the
following algorithm for generating term-conversion proofs.
In particular, we focus on equalities t ≈s whose proof can be justiﬁed by
a set of steps that replace subterms of t until it is syntactically equal to s. We
assume these steps are provided to a term-conversion proof generator. Formally,
a term-conversion proof generator g is a pair of sets cpre and cpost. The set cpre
(resp., cpost) contains pairs of the form (t ≈s, gt,s) indicating that t should
be replaced by s in a preorder (resp., postorder) traversal of the terms that g
processes, where gt,s is a proof generator that can prove the equality t ≈s. We
require that neither cpre nor cpost contain multiple entries of the form (t ≈s1, g1)
and (t ≈s2, g2) for distinct (s1, g1) and (s2, g2).
The procedure for generating proofs from a term-conversion proof generator
g is given in Algorithm 1. When asked to prove an equality t1 ≈t2, getProof
traverses the structure of t1 and applies steps from the sets cpre and cpost from g.

Flexible Proof Production in an Industrial-Strength SMT Solver
23
The traversal is performed by the auxiliary procedure getTermConv which relies
on two data structures. The ﬁrst is a lazy proof builder B that stores the inter-
mediate steps in the overall proof of t1 ≈t2. The proof builder is given these
steps either via addStep, as a concrete triple with the proof rule, a list of premise
formulas, and a list of argument terms, or as a lazy step via addLazyStep, with a
formula and a reference to another generator that can prove that formula. The
second data structure is a mapping R from terms to terms that is updated (using
array syntax in the pseudo-code) as the converted form of terms is computed
by getTermConv. For any term s, executing getTermConv(s, cpre, cpost, B, R) will
result in R[s] containing the converted form of s according to the rewrites in cpre
and cpost, and B storing a proof step for s ≈R[s]. Thus, the procedure getProof
succeeds when, after invoking getTermConv(t1, cpre, cpost, B, R) with B and R ini-
tially empty, the mapping R contains t2 as the converted form of t1. The proof
for the equality t1 ≈R[t1] can then be constructed by calling getProof on the
lazy proof builder B, based on the (lazy) steps stored in it.
Each subterm s of t1 is traversed only once by getTermConv by checking
whether R already contains the converted form of s. When that is not the case,
s is ﬁrst preorder processed. If cpre contains an entry indicating that s rewrites
to s′, this rewrite step is added to the lazy proof builder and the converted form
R[s] of s is set to s′. Otherwise, the immediate subterms of s, if any, are traversed
and then s is postorder processed. The converted form of s is set to some term
r of the form f(R[s1], . . . , R[sn]), considering how its immediate subterms were
converted. Note that B will contain steps for ⃗s ≈R[⃗s]. Thus, the equality s ≈r
can be proven by congruence for function f with these premises if s ̸= r, and by
reﬂexivity otherwise. Furthermore, if cpost indicates that r rewrites to r′, then
this step is added to the lazy proof builder; a transitivity step is added to prove
s ≈r′ from t ≈r and r ≈r′; and the converted form R[s] is set to r′.
Example 1. Consider the equality t ≈⊥, where t = f(b)+f(a) < f(a−0)+f(b),
and suppose the conversion of t is justiﬁed by a term-conversion proof generator
g containing the sets cpre = {(f(b) + f(a) ≈f(a) + f(b), gAC), (a −0 ≈a, gArith
0
)}
and cpost = {(f(a)+f(b) < f(a)+f(b) ≈⊥, gArith
1
)}. The generator gAC provides
a proof based on associative and commutative reasoning, whereas gArith
0
and
gArith
1
provide proofs based on arithmetic reasoning. Invoking getProof(g, t ≈⊥)
initiates the traversal with getTermConv(t, cpre, cpost, ∅, ∅). Since t is not in the
conversion map, it is preorder processed. However, as it does not occur in cpre,
nothing is done and its subterms are traversed. The subterm f(b) + f(a) is
equated to f(a) + f(b) in cpre, justiﬁed by gAC. Therefore R is updated with
R[f(b) + f(a)] = f(a) + f(b) and the respective lazy step is added to B. The
subterms of f(b)+f(a) are not traversed, therefore the next term to be traversed
is f(a−0)+f(b). Since it does not occur in cpre, its subterm f(a−0) is traversed,
which analogously leads to the traversal of a−0. As a−0 does occur in cpre, both R
and B are updated accordingly and the processing of its parent f(a−0) resumes.
A congruence step added to B justiﬁes its conversion to f(a) being added to R.

24
H. Barbosa et al.
No more additions happen since f(a) does not occur in cpost. Analogously, R and
B are updated with f(b) not changing and f(a −0) + f(b) being converted into
f(a) + f(b). Finally, the processing returns to the initial term t, which has been
converted to R[f(b) + f(a)] < R[f(a + 0) + f(b)], i.e., f(a) + f(b) < f(a) + f(b).
Since this term is equated to ⊥in cpost, justiﬁed by gArith
1
, the respective lazy
step is added to B, as well as a transitivity step to connect f(b) + f(a) <
f(a −0) + f(b) ≈f(a) + f(b) < f(a) + f(b) and f(a) + f(b) < f(a) + f(b) ≈⊥.
At this point, the execution terminates with R[f(b)+f(a) < f(a+0)+f(b)] = ⊥,
as expected. A proof for t ≈⊥with the following structure can then be extracted
from B:
P0 : cong
Lazy
gAC
f(b) + f(a) ≈f(a) + f(b)
P1 |
<
f(b) + f(a) < f(a −0) + f(b) ≈f(a) + f(b) < f(a) + f(b)
P2 : reﬂ
| f(b) ≈f(b)
f(b) ≈f(b)
trans P0
Lazy
gArith
1
f(a) + f(b) < f(a) + f(b) ≈⊥
f(b) + f(a) < f(a −0) + f(b) ≈⊥
P1 : cong
cong
Lazy
gArith
0
a −0 ≈a
| f
f(a −0) ≈f(a)
P2 | +
f(a −0) + f(b) ≈f(a) + f(b)
We use several extensions to the procedures in Algorithm 1. Notice that this
procedure follows the policy that terms on the right-hand side of conversion
steps (equalities from cpre and cpost) are not traversed further. The procedure
getTermConv is used by term-conversion proof generators that have the rewrite-
once policy. A similar procedure which additionally traverses those terms is used
by term-conversion proof generators that have a rewrite-to-ﬁxpoint policy.
We now show how the term-conversion proof generator can be used for recon-
structing ﬁne-grained proofs from coarse-grained ones. In particular we focus on
proofs Pψ1 of the form (sr, (Qψ0, ⃗Q), (S, R, D, ψ)). Recall from Fig. 2 that the
proof rule sr concludes a formula ψ that can be shown equivalent to the for-
mula ψ0 proven by Qψ0 based on a substitution derived from the conclusions of
the nodes ⃗Q. A proof like Pψ1 above can be transformed to one that involves
(atomic) theory rewrites and equality rules only. We show this transformation
in two phases. In the ﬁrst phase, the proof is expanded to:
(eq res, (Qψ0, (trans, (R0, (symm, R1)))))
with Ri = (trans, ((subs, ⃗Q⃗ϕ, (S, D, ψi)), (rewrite, (), (R, S(ψi, D(⃗ϕ)))))) for i ∈
{0, 1} where ⃗ϕ are the conclusions of ⃗Q⃗ϕ, and subs and rewrite are auxiliary proof
rules used for further expansion in the second phase. We describe them next.
Substitution Steps. Let Pt≈s be the subproof (subs, ⃗Q⃗ϕ, (S, D, t)) of Ri above
proving t ≈s with s = S(ψi, D(⃗ϕ)) and D(⃗ϕ) = (t1 →s1, . . . , tn →sn). Sub-
stitution steps can be expanded to ﬁne-grained proofs using a term-conversion
proof generator. First, for each j = 1, . . . , n, we construct a proof of tj ≈sj,
which involves simple transformations on the proofs of ⃗ϕ. Suppose we store all
of these in an eager proof generator g. If S is a simultaneous or ﬁxed-point
substitution, we then build a single term-conversion proof generator C, which

Flexible Proof Production in an Industrial-Strength SMT Solver
25
recall is modeled as a pair of mappings (cpre, cpost). We add (tj ≈sj, g) to cpre
for all j. We use the rewrite-once policy for C if S is a simultaneous substi-
tution, and the rewrite-ﬁxed-point policy for C otherwise. We then replace the
proof Pt≈s by getProof(C, t ≈s), which runs the procedure in Algorithm 1.
Otherwise, if S is a sequential substitution, we construct a term-conversion
generator Cj for each j, initializing it so that its cpre set contains the single
rewrite step (tj ≈sj, g) and uses a rewrite-once policy. We then replace the
proof Pt≈s by (trans, (P1, . . . , Pn)) where, for j = 1, . . . , n: Pj is generated by
getProof(Cj, sj−1 ≈sj); s0 = t; si is the result of the substitution D(⃗ϕ) after
the ﬁrst i steps; and sn = s.
Rewrite Steps. Let P be the proof node (rewrite, (), (R, t)), which proves the
equality t ≈t↑↓R. During reconstruction, we replace P with a proof involving
only ﬁne-grained rules, depending on the rewrite method R. For example, if
R is the core rewriter, we run the rewriter again on t in proof tracking mode.
Normally, the core rewriter performs a term traversal and applies atomic rewrites
to completion. In proof tracking mode, it also return two lists, for pre- and post-
rewrites, of steps (t1 ≈s1, g), . . . , (tn ≈sn, g) where g is a proof generator that
returns (atom rewrite, (), (R, ti)) for all equalities ti ≈si. Furthermore, for each
Skolem k that is a subterm of t, we construct the rewrite steps (k ≈k↑, g′) where
g′ is a proof generator that returns (witness, (), (k)) for equalities k ≈k↑. We
add these rewrite proof steps to a term-conversion generator C with rewrite-
ﬁxed-point policy, and replace P by getProof(C, t ≈t↑↓R).
5
SMT Proofs
Here we brieﬂy describe each component shown in Sect. 2 and how it produces
proofs with the infrastructure from Sects. 3 and 3.2.
5.1
Preprocessing Proofs
The pre-processor transforms an input formula ϕ into a list of formulas to be
given to the core solver. It applies a sequence of preprocessing passes. A pass
may replace a formula ϕi with another one φi, in which case it is responsible for
providing a proof of ϕi ≈φi. It may also append a new formula φ to the list,
in which case it is responsible for providing a proof for it. We use a (lazy) proof
generator that tracks these proofs, maintaining the invariant that a proof can be
provided for all (preprocessed) formulas when requested. We have instrumented
proof production for the most common preprocessing passes, relying heavily on
the sr rule to model transformations such as expansion of function deﬁnitions
and, with witness forms, Skolemization and if-then-else elimination [6].
Simpliﬁcation Under Global Assumptions. cvc5 aggressively learns literals that
hold globally by performing Boolean constraint propagation over the input for-
mula. When a learned literal corresponds to a variable elimination (e.g., x ≈5
corresponds to x →5) or a constant propagation (e.g., P(x) corresponds to

26
H. Barbosa et al.
P(x) →⊤), we apply the corresponding (term) substitution to the input. This
application is justiﬁed via sr, while the derivation of the globally learned literals
is justiﬁed via clausiﬁcation and resolution proofs, as explained in Sect. 5.3.
The key features of our architecture that make it feasible to produce proofs
for this simpliﬁcation are the automatic reconstruction of sr steps and the abil-
ity to customize the strategy for substitution application during reconstruction,
as detailed in Sect. 3.2. When a new variable elimination x →t is learned, old
ones need to be normalized to eliminate any occurrences of x in their right-hand
sides. Computing the appropriate simultaneous substitution for all eliminations
requires quadratically many traversals over those terms. We have observed that
the size of substitutions generated by this preprocessing pass can be very large
(with thousands of entries), which makes this computation prohibitively expen-
sive. Using the ﬁxed-point strategy, however, the reconstruction for the sr steps
can apply the substitution eﬃciently and its complexity depends on how many
applications are necessary to reach a ﬁx-point, which is often low in practice.
5.2
Theory Proofs
The theory engine produces lemmas, as disjunctions of literals, from an indi-
vidual theory or a combination of them. In the ﬁrst case, the lemma’s proof is
provided directly by the corresponding theory solver. In the second case, a the-
ory solver may produce a lemma ψ containing a literal ℓderived by some other
theory solver from literals ⃗ℓ. A lemma over the combined theory is generated by
replacing ℓin ψ by ⃗ℓ. This regression process, which is similar to the computa-
tion of explanations during solving, is repeated until the lemma contains only
input literals. The proof of the ﬁnal lemma then uses rules like sr to combine the
proofs of the intermediate literals derived locally in various theories and their
replacement by input literals in the ﬁnal lemma.
Equality and Uninterpreted Function (EUF) Proofs. The EUF solver can be
easily instrumented to produce proofs [31,42] with equality rules (see Fig. 2).
In cvc5, term equivalences are also derived via rewriting in some other theory
T: when a function from T has all of its arguments inferred to be congruent to
T-values, it may be rewritten into a T-value itself, and this equivalence asserted.
Such equivalences are justiﬁed via sr steps. Since generating equality proofs
incurs minimal overhead [42] and rewriting proofs are reconstructed lazily, EUF
proofs are generated during solving and stored in an eager proof generator.
Extensional Arrays and Datatypes Proofs. While these two theories diﬀer sig-
niﬁcantly, they both combine equality reasoning with rules for handling their
particular operators. For arrays, these are rules for select, store, and array exten-
sionality (see [36, Sec. 5]). For datatypes, they are rules reﬂecting the properties
of constructors and selectors, as well as acyclicity. The justiﬁcations for lemmas
are also generated eagerly and stored in an eager proof generator.
Bit-Vector Proofs. The bit-vector solver applies bit-blasting to reduce bit-vector
problems to equisatisﬁable propositional problems. Thus, its lemmas amount

Flexible Proof Production in an Industrial-Strength SMT Solver
27
to the rewriting of the bit-vector literals into Boolean formulas, which will be
solved and proved by the propositional engine. The bit-vector lemmas are proven
lazily, analogous to sr steps, with the diﬀerence that the reconstruction uses the
bit-blaster in the bit-vector solver instead of the rewriter.
Arithmetic Proofs. The linear arithmetic solver is based on the simplex algo-
rithm [24], and each of its lemmas is the negation of an unsatisﬁable conjunction
of inequalities. Farkas’ lemma [30,49] guarantees that there exists a linear com-
bination of these inequalities equivalent to ⊥. The coeﬃcients of the combination
are computed during solving with minimal overhead [38], and the equivalence
is proven with an sr step. To allow the rewriter to prove this equivalence, the
bounds of the inequalities are scaled by constants and summed during recon-
struction. Integer reasoning is proved through rules for branching and integer
bound tightening, recorded eagerly.
Non-linear arithmetic lemmas are generated from incremental linearization
[16] or cylindrical algebraic coverings [1]. The former can be proven via propo-
sitional and basic arithmetic rules, with only a few, such as the tangent plane
lemma, needing a dedicated proof rule. The latter requires two complex rules
that are not inherently simpler than solving, albeit not as complex as those for
regular CAD-based theory solvers [2]. We point out that checking these rules
would require a signiﬁcant portion of CAD-related theory, whose proper formal-
ization is still an open, if actively researched, problem [18,25,34,41,53].
Quantiﬁer Proofs. Quantiﬁed formulas not Skolemized during pre-processing are
handled via instantiation, which produces theory lemmas of the form (∀⃗x ϕ) ⇒
ϕσ, where σ is a grounding substitution. An instantiation rule proves them
independently of how the substitution was actually derived, since any well-typed
one suﬃces for soundness.
String Proofs. The strings solver applies a layered approach, distinguishing
between core [40] and extended operators [48]. The core operators consist of
(dis)equalities between string concatenations and length constraints. Reasoning
over them is proved by a combination of equality and linear integer arithmetic
proofs, as well as speciﬁc string rules. The extended operators are reduced to core
ones via formulas with bounded quantiﬁers. The reductions are proven with rules
deﬁning each extended function’s semantics, and sr steps justifying the reduc-
tions. Finally, regular membership constraints are handled by string rules that
unfold occurrences of the Kleene star operator and split up regular expression
concatenations into diﬀerent parts. Overall, the proofs for the strings theory
solver encompass not only string-speciﬁc reasoning but also equality, linear inte-
ger arithmetic, and quantiﬁer reasoning, as well as substitution and rewriting.
Unsupported. The theory solvers for the theories of ﬂoating-point arithmetic,
sequences, sets and relations, and separation logic are currently not proof-
producing in cvc5. These are relatively new or non-standard theories in SMT
and have not been our focus, but we intend to produce proofs for them in the
future.

28
H. Barbosa et al.
Table 1. Cumulative solving times (s) on benchmarks solved by all conﬁgurations,
with the slowdown versus cvc+s in parentheses.
Logics
#
cvc+os
cvc+s
cvc+sp
cvc+spr
non-BVs 116,321
164k
166k
284k (1.7×)
299k (1.8×)
BVs
29,192
45k
57k
150k (2.6×)
224k (3.9×)
5.3
Propositional Proofs
Propositional proofs justify both the conversion of preprocessed input formulas
and theory lemmas into conjunctive normal form (CNF) and the derivation of
⊥from the resulting clauses. CNF proofs are a combination of Boolean trans-
formations and introductions of Boolean formulas representing the deﬁnition of
Tseytin variables, used to ensure that the CNF conversion is polynomial. The
clausiﬁer uses a lazy proof builder which stores the clausiﬁcation steps eagerly,
with the preprocessed input formulas as assumptions, and the theory lemmas as
lazy steps, with associated proof generators.
For Boolean reasoning, cvc5 uses a version of MiniSat [27] instrumented to
produce resolution proofs. It uses a lazy proof builder to record resolution steps
for learned clauses as they are derived (see [7, Chap 1] for more details) and to
lazily build a refutation with only the resolution steps necessary for deriving ⊥.
The resolution rule, however, is ground ﬁrst-order resolution, since the proofs are
in terms of the ﬁrst-order clauses rather than their propositional abstractions.
6
Evaluation
In this section, we discuss an initial evaluation of our implementation in cvc5 of
the proof-production architecture presented in this paper. In the following, we
denote diﬀerent conﬁgurations of cvc5 by cvc plus some suﬃxes. A conﬁguration
using variable and clause elimination in the SAT solver [26], symmetry break-
ing [23] in the EUF solver, and black-box SAT solving in the bit-vector (BV)
solver, is denoted by the suﬃx o. These techniques are currently incompatible
with the proof production architecture. Other cvc5 techniques for which we do
not yet support ﬁne-grained proofs, however, are active and have their inferences
registered in the proofs as trusted steps. A conﬁguration that includes simpli-
ﬁcation under global assumptions is denoted by s; one that includes producing
proofs by p; and one that additionally reconstructs proofs by r. The default
conﬁguration of cvc5 is cvc+os.
We split our evaluation into measuring the proof-production cost as well
as the performance impact of making key techniques proof-producing; the proof
reconstruction overhead; and the coverage of the proof production. We also com-
ment on how cvc5’s proofs compare with CVC4’s proofs. Note that the internal
proof checking described in Sect. 3, which was invaluable for a correct implemen-
tation, is disabled for evaluating performance. Experiments ran on a cluster with

Flexible Proof Production in an Industrial-Strength SMT Solver
29
100000
105000
110000
115000
120000
solved instances
100
101
102
runtime [s]
cvc+os
cvc+s
cvc
cvc+pr
cvc+spr
cvc+sp
20000 22500 25000 27500 30000 32500 35000 37500
solved instances
100
101
102
runtime [s]
cvc+os (BV)
cvc+s (BV)
cvc (BV)
cvc+pr (BV)
cvc+spr (BV)
cvc+sp (BV)
10−1
100
101
102
cvc+s [s]
10−1
100
101
102
cvc+spr [s]
10x
100x
1000x
10−1
100
101
102
cvc+s (BV) [s]
10−1
100
101
102
cvc+spr (BV) [s]
10x
100x
1000x
10−1
100
101
102
cvc+sp [s]
10−1
100
101
102
cvc+spr [s]
10x
100x
1000x
10−1
100
101
102
cvc+sp (BV) [s]
10−1
100
101
102
cvc+spr (BV) [s]
10x
100x
1000x
a
b
c
d
Fig. 3. (a) Cactus plot for non-BVs (b) Cactus plot for BVs (c) Scatter plot of overall
proof cost (d) Reconstruction cost
Intel Xeon E5-2620 v4 CPUs, with 300s and 8GB of RAM for each solver and
benchmark pair. We consider 162,060 unsatisﬁable problems from SMT-LIB [8],
across all logics except those with ﬂoating point arithmetic, as determined by
cvc5 [5, Sec. 4]. We split them into 38,732 problems with the BV theory (the
BVs set) and 123,328 problems without (the non-BVs set).
Proof Production Cost. The cost of proof production is summarized in Table 1
and Figs. 3a to 3d. The impact of running without o is negligible overall in non-
BVs, but steep for BVs, both in terms of solving time and number of problems
solved, as evidenced by the table and Fig. 3b respectively. This is expected given
the eﬀectiveness of combining bit-blasting with black-box SAT solvers. The over-
head of p is similar for both sets, although more pronounced in BVs. While the
total time is around double that of cvc+s, Fig. 3c shows a ﬁner distribution,
with most problems having a less signiﬁcant overhead. Moreover, the total num-
ber of problems solved is quite similar, as shown in Figs. 3a and 3b, particularly
for non-BVs. The diﬀerence in overhead due to p between the BVs and non-
BVs sets can be attributed to the cost of managing large proofs, which are
more common in BVs. This stems from the well-known blow-up in problem size
incurred by bit-blasting, which is reﬂected in the proofs.
The cost of generating ﬁne-grained steps for the sr rule and for the similarly
reconstructed theory-speciﬁc steps mentioned in Sect. 5, varies again between

30
H. Barbosa et al.
the two sets, but more starkly. While for non-BVs the overall solving time and
number of problems solved are very similar between cvc+sp and cvc+spr, for
the BVs set cvc+spr is signiﬁcantly slower overall. This diﬀerence again arises
mainly because of the increased proof sizes. Nevertheless, r leads to only a small
increase in unsolved problems in BVs, as shown in Fig. 3b.
The importance of being able to produce proofs for simpliﬁcation under
global assumptions is made clear by Fig. 3a: the impact of disabling s is virtu-
ally the same as that of adding p; moreover, cvc+spr signiﬁcantly outperforms
cvc+pr. In Fig. 3b the diﬀerence is less pronounced but still noticeable.
Proofs Coverage. When using techniques that are not yet fully proof-producing,
but still active, cvc5 inserts trusted steps in the proof. These are usually steps
whose checking is not inherently simpler than solving. They eﬀectively represent
holes in the proof, but are still useful for users who avail themselves of powerful
proof-checking techniques. Trusted steps are commonly used when integrating
SMT solvers into proof assistants [11,28,51].
The percentage of cvc+spr proofs without trusted steps is 92% for BVs and
80% for non-BVs. That is to say, out of 145,683 proofs, 120,473 of them are
fully ﬁne-grained proofs. The vast majority of the trusted steps in the remaining
proofs are due to theory-speciﬁc preprocessing passes that are not yet fully proof-
producing. In non-BVs, the occurrence of trusted steps is heavily dependent
on the speciﬁc SMT-LIB logic, as expected. Common oﬀenders are logics with
datatypes, with trusted steps for acyclicity checks, and quantiﬁed logics, with
trusted steps for certain α-equivalence eliminations. In non-linear real arithmetic
logics, all cylindrical algebraic coverings proofs are built with trusted steps (see
Sect. 5.2), but we note this is the state of the art for CAD-based proofs. As for
non-linear integer arithmetic logics, our proof support is still in its early stages,
so a signiﬁcant portion of their theory lemmas are trusted steps.
We stress the extent of our coverage for string proofs, which were previously
unsupported by any SMT solver. In the string logics without length constraints,
100% of the proofs are fully ﬁne-grained. This rate goes down to 80% in the
logics with length. For the remaining 20%, the overwhelming majority of the
trusted steps are for theory-speciﬁc preprocessing or some particular string or
linear arithmetic inference within the proof of a theory lemma.
Comparison with CVC4 Proofs. We compare the proof coverage of cvc5 versus
CVC4. The cvc5 proof production replaces CVC4’s [32,36], which was incom-
plete and monolithic. CVC4 did not produce proofs at all for strings, substitu-
tions, rewriting, preprocessing, quantiﬁers, datatypes, or non-linear arithmetic.
In particular, simpliﬁcation over global assumptions had to be disabled when
producing proofs. In fragments supported by both systems, CVC4’s proofs are
at most as detailed as cvc5’s. The only superior aspect of CVC4’s proof produc-
tion was to support proofs from external SAT solvers [45] used in the BV solver,
which are very signiﬁcant for solving performance, as shown above. Integrating
this feature into cvc5 is left as future work, but we note that there is no limi-
tation in the proof architecture that would prevent it. We also point out that
cvc5 produces resolution proofs for the bit-blasted BV constraints, which can

Flexible Proof Production in an Industrial-Strength SMT Solver
31
be checked in polynomial time, whereas external SAT solvers produce DRAT
proofs [33] (or reconstructions of them via other tools [19,20,37,39]), which can
take exponential time to check. So there is a signiﬁcant trade-oﬀto be considered.
7
Related Work
Two signiﬁcant proof-producing state-of-the-art SMT solvers are z3 [22] and
veriT [14]. Both can have their proofs successfully reconstructed in proof assis-
tants [3,12,13,51]. They can produce detailed proofs for the propositional and
theory reasoning in EUF and linear arithmetic, as well as for quantiﬁers. How-
ever, z3’s proofs are coarse-grained for preprocessing and rewriting, and for bit-
vector reasoning, which complicates proof checking. Moreover, to the best of our
knowledge, z3 does not produce proofs for its other theories. In contrast, veriT
can produce ﬁne-grained proofs for preprocessing and rewriting [6], which has led
to a better integration with Isabelle/HOL [51]. However, it does so eagerly, which
requires a tight integration between the preprocessing and the proof-production
code. In addition, it does not support simpliﬁcation under global assumptions
when producing proofs, which signiﬁcantly impacts its performance. Other proof-
producing SMT solvers are MathSAT5 [17] and SMTInterpol [15]. They produce
resolution proofs and theory proofs for EUF, linear arithmetic, and, in SMTIn-
terpol’s case, array theories. Their proofs are tailored towards unsatisﬁable core
and interpolant generation, rather than external certiﬁcation. Moreover, they do
not seem to provide proofs for preprocessing, clausiﬁcation or rewriting.
While cvc5 is possibly the only proof-producing solver for the full theory of
strings, CertiStr [35] is a certiﬁed solver for the fragment with concatenation
and regular expressions. It is automatically generated from Isabelle/HOL [44]
but is signiﬁcantly less performant than cvc5, although a proper comparison
would need to account for proof-checking time in cvc5’s case.
8
Conclusion and Future Work
We presented and evaluated a ﬂexible proof production architecture, showing it
is capable of producing proofs with varying levels of granularity in a scalable
manner for a state-of-the-art and industrial-strength SMT solver like cvc5.
Since currently, there is no standard proof format for SMT solvers, our archi-
tecture is designed to support multiple proof formats via a ﬁnal post-processing
transformation to convert internal proofs accordingly. We are developing back-
ends for the LFSC [52] proof checker and the proof assistants Lean 4 [21],
Isabelle/HOL [44], and Coq [10], the latter two via the Alethe proof format [50].
Since using these tools requires mechanizing the respective target proof calculi in
their languages, besides external checking, another beneﬁt is to decouple conﬁ-
dence on the soundness of the proof calculi from the internal cvc5 proof calculus.
A considerable challenge for SMT proofs is the plethora of rewrite rules used
by the solvers, which are speciﬁc for each theory and vary in complexity. In

32
H. Barbosa et al.
particular, string rewrites can be very involved [46] and hard to check. We are
also developing an SMT-LIB-based DSL for specifying rewrite rules, to be used
during proof reconstruction to decompose rewrite steps in terms of them, thus
providing more ﬁne-grained proofs for rewriting.
Finally, we plan to incorporate into the proof-production architecture the
unsupported theories and features mentioned in Sects. 5.2 and 6, particularly
those relevant for solving performance that currently either leave holes in proofs,
such as theory pre-processing or non-linear arithmetic reasoning, or that have
to be disabled, such as the use of external SAT solvers in the BV theory.
References
1. ´Abrah´am, E., Davenport, J.H., England, M., Kremer, G.: Deciding the consistency
of non-linear real arithmetic constraints with a conﬂict driven search using cylin-
drical algebraic coverings. J. Log. Algebr. Methods Program. 119, 100633 (2021)
2. Abrah´am, E., Davenport, J.H., England, M., Kremer, G.: Proving UNSAT
in SMT: the case of quantiﬁer free non-linear real arithmetic. arXiv preprint
arXiv:2108.05320 (2021)
3. Armand, M., Faure, G., Gr´egoire, B., Keller, C., Th´ery, L., Werner, B.: A modular
integration of SAT/SMT solvers to coq through proof witnesses. In: Jouannaud, J.-
P., Shao, Z. (eds.) CPP 2011. LNCS, vol. 7086, pp. 135–150. Springer, Heidelberg
(2011). https://doi.org/10.1007/978-3-642-25379-9 12
4. Backes, J., et al.: Semantic-based automated reasoning for AWS access policies
using SMT. In: Bjørner, N., Gurﬁnkel, A. (eds.) Formal Methods in Computer-
Aided Design (FMCAD), pp. 1–9. IEEE (2018)
5. Barbosa, H., et al.: cvc5: a versatile and industrial-strength SMT solver. In: Fis-
man, D., Rosu, G. (eds.) Tools and Algorithms for Construction and Analysis of
Systems (TACAS). LNCS, Springer, Cham (2022). https://doi.org/10.1007/978-
3-030-99524-9 24
6. Barbosa, H., Blanchette, J.C., Fleury, M., Fontaine, P.: Scalable ﬁne-grained proofs
for formula processing. J. Autom. Reason. 64(3), 485–510 (2020)
7. Barrett, C., de Moura, L., Fontaine, P.: Proofs in satisﬁability modulo theories.
All About Proofs Proofs All (APPA) 55(1), 23–44 (2014)
8. Barrett, C., Fontaine, P., Tinelli, C.: The SMT-LIB standard: version 2.6. Technical
report, Department of Computer Science, The University of Iowa (2017). www.
SMT-LIB.org
9. Barrett, C., Tinelli, C.: Satisﬁability Modulo Theories. In: Clarke, E., Henzinger,
T., Veith, H., Bloem, R. (eds.) Handbook of Model Checking, pp. 305–343.
Springer, Cham (2018). https://doi.org/10.1007/978-3-319-10575-8 11
10. Bertot, Y., Cast´eran, P.: Interactive Theorem Proving and Program Development -
Coq’Art: The Calculus of Inductive Constructions. Texts in Theoretical Computer
Science. An EATCS Series, Springer, Heidelberg (2004). https://doi.org/10.1007/
978-3-662-07964-5
11. Blanchette, J.C., B¨ohme, S., Paulson, L.C.: Extending sledgehammer with SMT
solvers. J. Autom. Reason. 51(1), 109–128 (2013)
12. B¨ohme, S., Fox, A.C.J., Sewell, T., Weber, T.: Reconstruction of Z3’s bit-vector
proofs in HOL4 and Isabelle/HOL. In: Jouannaud, J.-P., Shao, Z. (eds.) CPP
2011. LNCS, vol. 7086, pp. 183–198. Springer, Heidelberg (2011). https://doi.org/
10.1007/978-3-642-25379-9 15

Flexible Proof Production in an Industrial-Strength SMT Solver
33
13. B¨ohme, S., Weber, T.: Fast LCF-style proof reconstruction for Z3. In: Kaufmann,
M., Paulson, L.C. (eds.) ITP 2010. LNCS, vol. 6172, pp. 179–194. Springer, Hei-
delberg (2010). https://doi.org/10.1007/978-3-642-14052-5 14
14. Bouton, T., Caminha B. de Oliveira, D., D´eharbe, D., Fontaine, P.: veriT: an open,
trustable and eﬃcient SMT-solver. In: Schmidt, R.A. (ed.) CADE 2009. LNCS
(LNAI), vol. 5663, pp. 151–156. Springer, Heidelberg (2009). https://doi.org/10.
1007/978-3-642-02959-2 12
15. Christ, J., Hoenicke, J., Nutz, A.: SMTInterpol: an interpolating SMT solver.
In: Donaldson, A., Parker, D. (eds.) SPIN 2012. LNCS, vol. 7385, pp. 248–254.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-31759-0 19
16. Cimatti, A., Griggio, A., Irfan, A., Roveri, M., Sebastiani, R.: Satisﬁability mod-
ulo transcendental functions via incremental linearization. In: de Moura, L. (ed.)
CADE 2017. LNCS (LNAI), vol. 10395, pp. 95–113. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-63046-5 7
17. Cimatti, A., Griggio, A., Schaafsma, B.J., Sebastiani, R.: The MathSAT5 SMT
solver. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp.
93–107. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36742-7 7
18. Cohen, C.: Construction of real algebraic numbers in Coq. In: Beringer, L., Felty,
A. (eds.) ITP 2012. LNCS, vol. 7406, pp. 67–82. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-32347-8 6
19. Cruz-Filipe, L., Heule, M.J.H., Hunt, W.A., Kaufmann, M., Schneider-Kamp, P.:
Eﬃcient certiﬁed RAT veriﬁcation. In: de Moura, L. (ed.) CADE 2017. LNCS
(LNAI), vol. 10395, pp. 220–236. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-63046-5 14
20. Cruz-Filipe, L., Marques-Silva, J., Schneider-Kamp, P.: Eﬃcient certiﬁed resolu-
tion proof checking. In: Legay, A., Margaria, T. (eds.) TACAS 2017. LNCS, vol.
10205, pp. 118–135. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-
662-54577-5 7
21. de Moura, L., Ullrich, S.: The lean 4 theorem prover and programming language.
In: Platzer, A., Sutcliﬀe, G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp.
625–635. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-79876-5 37
22. de Moura, L.M., Bjørner, N.: Proofs and refutations, and Z3. In: Rudnicki, P.,
Sutcliﬀe, G., Konev, B., Schmidt, R.A., Schulz, S. (eds.) Logic for Programming,
Artiﬁcial Intelligence, and Reasoning (LPAR) Workshops. CEUR Workshop Pro-
ceedings, vol. 418. CEUR-WS.org (2008)
23. D´eharbe, D., Fontaine, P., Merz, S., Woltzenlogel Paleo, B.: Exploiting symmetry
in SMT problems. In: Bjørner, N., Sofronie-Stokkermans, V. (eds.) CADE 2011.
LNCS (LNAI), vol. 6803, pp. 222–236. Springer, Heidelberg (2011). https://doi.
org/10.1007/978-3-642-22438-6 18
24. Dutertre, B., de Moura, L.: A fast linear-arithmetic solver for DPLL(T). In: Ball,
T., Jones, R.B. (eds.) CAV 2006. LNCS, vol. 4144, pp. 81–94. Springer, Heidelberg
(2006). https://doi.org/10.1007/11817963 11
25. Eberl, M.: A decision procedure for univariate real polynomials in Isabelle/HOL.
In: Proceedings of the 2015 Conference on Certiﬁed Programs and Proofs, CPP
2015, pp. 75–83. Association for Computing Machinery, New York (2015)
26. E´en, N., Biere, A.: Eﬀective preprocessing in SAT through variable and clause
elimination. In: Bacchus, F., Walsh, T. (eds.) SAT 2005. LNCS, vol. 3569, pp.
61–75. Springer, Heidelberg (2005). https://doi.org/10.1007/11499107 5
27. E´en, N., S¨orensson, N.: An extensible SAT-solver. In: Giunchiglia, E., Tacchella,
A. (eds.) SAT 2003. LNCS, vol. 2919, pp. 502–518. Springer, Heidelberg (2004).
https://doi.org/10.1007/978-3-540-24605-3 37

34
H. Barbosa et al.
28. Ekici, B., et al.: SMTCoq: a plug-in for integrating SMT solvers into Coq. In:
Majumdar, R., Kunˇcak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 126–133.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-63390-9 7
29. Enderton, H.B.: A Mathematical Introduction to Logic, 2nd edn. Academic Press,
Cambridge (2001)
30. Farkas,
G.:
A
Fourier-f´ele
mechanikai
elv
alkamaz´asai.
Mathematikai´es
Term´eszettudom´anyi ´Ertes´ıt¨o 12, 457–472 (1894). Reference from Schrijver’s Com-
binatorial Optimization textbook (Hungarian)
31. Fontaine, P., Marion, J.-Y., Merz, S., Nieto, L.P., Tiu, A.: Expressiveness +
automation + soundness: towards combining SMT solvers and interactive proof
assistants. In: Hermanns, H., Palsberg, J. (eds.) TACAS 2006. LNCS, vol. 3920,
pp. 167–181. Springer, Heidelberg (2006). https://doi.org/10.1007/11691372 11
32. Hadarean, L., Barrett, C., Reynolds, A., Tinelli, C., Deters, M.: Fine grained SMT
proofs for the theory of ﬁxed-width bit-vectors. In: Davis, M., Fehnker, A., McIver,
A., Voronkov, A. (eds.) LPAR 2015. LNCS, vol. 9450, pp. 340–355. Springer, Hei-
delberg (2015). https://doi.org/10.1007/978-3-662-48899-7 24
33. Heule, M.J.H.: The DRAT format and drat-trim checker. CoRR, abs/1610.06229
(2016)
34. Joosten, S.J.C., Thiemann, R., Yamada, A.: A veriﬁed implementation of algebraic
numbers in Isabelle/HOL. J. Autom. Reason. 64, 363–389 (2020)
35. Kan, S., Lin, A.W., R¨ummer, P., Schrader, M.: Certistr: a certiﬁed string solver.
In: Popescu, A., Zdancewic, S. (eds.) Certiﬁed Programs and Proofs (CPP), pp.
210–224. ACM (2022)
36. Katz, G., Barrett, C., Tinelli, C., Reynolds, A., Hadarean, L.: Lazy proofs for
DPLL(T)-based SMT solvers. In: Piskac, R., Talupur, M. (eds.) Formal Methods
in Computer-Aided Design (FMCAD), pp. 93–100. IEEE (2016)
37. Kiesl, B., Rebola-Pardo, A., Heule, M.J.H.: Extended resolution simulates DRAT.
In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol.
10900, pp. 516–531. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-
94205-6 34
38. King, T.: Eﬀective algorithms for the satisﬁability of quantiﬁer-free formulas over
linear real and integer arithmetic (2014)
39. Lammich, P.: Eﬃcient veriﬁed (UN)SAT certiﬁcate checking. In: de Moura, L.
(ed.) CADE 2017. LNCS (LNAI), vol. 10395, pp. 237–254. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-63046-5 15
40. Liang, T., Reynolds, A., Tinelli, C., Barrett, C., Deters, M.: A DPLL(T) theory
solver for a theory of strings and regular expressions. In: Biere, A., Bloem, R. (eds.)
CAV 2014. LNCS, vol. 8559, pp. 646–662. Springer, Cham (2014). https://doi.org/
10.1007/978-3-319-08867-9 43
41. Mahboubi, A.: Implementing the cylindrical algebraic decomposition within the
coq system. Math. Struct. Comput. Sci. 17(1), 99–127 (2007)
42. Nieuwenhuis, R., Oliveras, A.: Proof-producing congruence closure. In: Giesl, J.
(ed.) RTA 2005. LNCS, vol. 3467, pp. 453–468. Springer, Heidelberg (2005).
https://doi.org/10.1007/978-3-540-32033-3 33
43. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories:
from an abstract Davis-Putnam-Logemann-Loveland procedure to DPLL(T). J.
ACM 53(6), 937–977 (2006)
44. Nipkow, T., Wenzel, M., Paulson, L.C. (eds.): Isabelle/HOL. LNCS, vol. 2283.
Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45949-9

Flexible Proof Production in an Industrial-Strength SMT Solver
35
45. Ozdemir, A., Niemetz, A., Preiner, M., Zohar, Y., Barrett, C.: DRAT-based bit-
vector proofs in CVC4. In: Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol.
11628, pp. 298–305. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
24258-9 21
46. Reynolds, A., N¨otzli, A., Barrett, C., Tinelli, C.: High-level abstractions for sim-
plifying extended string constraints in SMT. In: Dillig, I., Tasiran, S. (eds.) CAV
2019. LNCS, vol. 11562, pp. 23–42. Springer, Cham (2019). https://doi.org/10.
1007/978-3-030-25543-5 2
47. Reynolds, A., N¨otzli, A., Barrett, C.W., Tinelli, C.: Reductions for strings and
regular expressions revisited. In: Formal Methods in Computer-Aided Design
(FMCAD), pp. 225–235. IEEE (2020)
48. Reynolds, A., Woo, M., Barrett, C., Brumley, D., Liang, T., Tinelli, C.: Scaling
up DPLL(T) string solvers using context-dependent simpliﬁcation. In: Majumdar,
R., Kunˇcak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 453–474. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-63390-9 24
49. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, Hoboken (1998)
50. Schurr, H.-J., Fleury, M., Barbosa, H., Fontaine, P.: Alethe: towards a generic SMT
proof format (extended abstract). CoRR, abs/2107.02354 (2021)
51. Schurr, H.-J., Fleury, M., Desharnais, M.: Reliable reconstruction of ﬁne-grained
proofs in a proof assistant. In: Platzer, A., Sutcliﬀe, G. (eds.) CADE 2021. LNCS
(LNAI), vol. 12699, pp. 450–467. Springer, Cham (2021). https://doi.org/10.1007/
978-3-030-79876-5 26
52. Stump, A., Oe, D., Reynolds, A., Hadarean, L., Tinelli, C.: SMT proof checking
using a logical framework. Formal Methods Syst. Des. 42(1), 91–118 (2013)
53. Thiemann, R., Yamada, A.: Algebraic numbers in Isabelle/HOL. In: Blanchette,
J.C., Merz, S. (eds.) ITP 2016. LNCS, vol. 9807, pp. 391–408. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-43144-4 24
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

CTL∗Model Checking for Data-Aware
Dynamic Systems with Arithmetic
Paolo Felli, Marco Montali, and Sarah Winkler(B)
Free University of Bolzano-Bozen, Bolzano, Italy
{pfelli,montali,winkler}@inf.unibz.it
Abstract. The analysis of complex dynamic systems is a core research
topic in formal methods and AI, and combined modelling of systems with
data has gained increasing importance in applications such as business
process management. In addition, process mining techniques are nowa-
days used to automatically mine process models from event data, often
without correctness guarantees. Thus veriﬁcation techniques for linear
and branching time properties are needed to ensure desired behavior.
Here we consider data-aware dynamic systems with arithmetic
(DDSAs), which constitute a concise but expressive formalism of tran-
sition systems with linear arithmetic guards. We present a CTL∗model
checking procedure for DDSAs that addresses a generalization of the
classical veriﬁcation problem, namely to compute conditions on the ini-
tial state, called witness maps, under which the desired property holds.
Linear-time veriﬁcation was shown to be decidable for speciﬁc classes
of DDSAs where the constraint language or the control ﬂow are suit-
ably conﬁned. We investigate several of these restrictions for the case of
CTL∗, with both positive and negative results: witness maps can always
be found for monotonicity and integer periodicity constraint systems,
but veriﬁcation of bounded lookback systems is undecidable. To demon-
strate the feasibility of our approach, we implemented it in an SMT-based
prototype, showing that many practical business process models can be
eﬀectively analyzed.
Keywords: Veriﬁcation · CTL∗· Counter systems · Constraints ·
SMT
1
Introduction
The study of complex dynamic systems is a core research topic in AI, with a long
tradition in formal methods. It ﬁnds application in a variety of domains, such
as notably business process management (BPM), where studying the interplay
between control-ﬂow and data has gained momentum [9,10,24,46]. Processes are
increasingly mined by automatic techniques [1,3] that lack any correctness guar-
antees, making veriﬁcation even more important to ensure the desired behavior.
This work is partially supported by the UNIBZ projects DaCoMan, QUEST, SMART-
APP, VERBA, and WineId.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 36–56, 2022.
https://doi.org/10.1007/978-3-031-10769-6_4

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
37
However, the presence of data pushes veriﬁcation to the verge of undecidability
due to an inﬁnite state space. This is aggravated by the use of arithmetic, in spite
of its importance for practical applications [24]. Indeed, model checking of tran-
sition systems operating on numeric data variables with arithmetic constraints
is known to be undecidable, as it is easy to model a two-counter machine.
In this work, we focus on the concise but expressive framework of data-aware
dynamic systems with arithmetic (DDSAs) [28,38], also known as counter sys-
tems [13,20,34]. Several classes of DDSAs have been isolated where speciﬁc ver-
iﬁcation tasks are decidable, notably reachability [6,13,29,34] and linear-time
model checking [14,20,22,28,38]. Fewer results are known about the case of
branching time, except for ﬂat counter systems [21], gap-order systems where
constraints are restricted to the form x −y ≥2 [8,42], and systems with a
nice symbolic valuation abstraction [31]. However, many processes in BPM and
beyond fall into neither of these classes, as illustrated by the next example.
Example 1. The following DDSA B models a management process for road ﬁnes
by the Italian police [41]. It maintains seven so-called case data variables (i.e.,
variables local to each process instance, called “case” in the BPM literature): a
(amount), t (total amount), d (dismissal code), p (points deducted), e (expenses),
and time durations ds, dp, dj. The process starts by creating a case, upon which
the oﬀender is notiﬁed within 90 days, i.e., 2160h (send ﬁne). If the oﬀender pays
a suﬃcient amount t, the process terminates via silent actions τ1, τ2, or τ3. For
the less happy paths, the credit collection action is triggered if the payment was
insuﬃcient; while appeal to judge and appeal to prefecture reﬂect ﬁled protests
by the oﬀender, which again need to respect certain time constraints.
p1
p2
p3
p4
end
p5
p6
p7
p8
create ﬁne
aw, tw, dw, pw ≥0
payment
tw ≥0
send ﬁne
0 ≤dsw ≤2160 ∧ew ≥0
τ1
d ̸= 0 ∨(p = 0 ∧tr ≥ar)
payment
tw ≥0
insert notiﬁcation
τ2
tr ≥ar + er
payment
tw ≥0
add penalty
aw ≥0
appeal to judge
0 ≤dj w ≤1440 ∧dw ≥0
credit collection
tr < ar + er
τ3
tr ≥ar + er
τ5
dr = 0
appeal to prefecture
0 ≤dpw ≤1440
send to prefecture
dw ≥0
result prefecture
dr = 0
τ6
dr = 1
τ4
dr = 2
notify
This model was generated from real-life logs by automatic process mining tech-
niques paired with domain knowledge [41], but without any correctness guar-
antee. For instance, data-aware soundness [4,25] requires that the process can
always reach a ﬁnal state from any reachable conﬁguration, expressed by the
branching-time property A G E F end. This property is false here, as B can get
stuck in state p7 if d > 1. In addition, process-speciﬁc linear-time properties are
needed, e.g., that a send ﬁne event is always followed by a suﬃcient payment (i.e.,
⟨send ﬁne⟩⊤→F ⟨payment⟩(t ≥a), where ⟨α⟩is the next operator via action α).

38
P. Felli et al.
This example highlights how both linear-time and branching-time veriﬁca-
tion are needed. In this paper, we present a CTL∗model checking algorithm for
DDSAs, adopting a ﬁnite-trace semantics (CTL∗
f) [44] to reﬂect the nature of
processes as in Example 1. More precisely, our approach can synthesize condi-
tions on the initial variable assignment such that a given property χ holds, called
witness maps. If such a witness map can be found, it is in particular decidable
what is more commonly called the veriﬁcation problem, namely whether χ is sat-
isﬁed in a designated initial conﬁguration. We derive an abstract criterion on the
computability of witness maps, which is satisﬁed by two practical DDSA classes
that restrict the constraint language to (a) monotonicity constraints [20,25], i.e.,
variable-to-variable or variable-to-constant comparisons over Q or R, and (b)
integer periodicity constraints [18,22], i.e., variable-to-constant and restricted
variable-to-variable comparisons with modulo operators. On the other hand,
we show that the veriﬁcation problem is undecidable for bounded lookback sys-
tems [28], a control ﬂow restriction that generalizes feedback freedom [14].
In summary, we make the following contributions:
1. We present a model checking algorithm to generate a witness map for a given
DDSA and CTL∗
f property;
2. We prove an abstract termination criterion for this algorithm (Corollary 1);
3. This result is used to show that witness maps can be eﬀectively computed for
monotonicity constraint and integer periodicity constraint systems;
4. CTL∗
f veriﬁcation is shown undecidable for bounded-lookback systems;
5. We implemented our approach in the prototype ada using SMT solvers as
backends and tested it on a range of business processes from the literature.
The paper is structured as follows: The rest of this section recapitulates related
work. Section 2 compiles preliminaries about DDSAs and CTL∗
f. Section 3 is
dedicated to LTL with conﬁguration maps, which is used by our model checking
procedure in Sect. 4. Based on an abstract termination criterion, (un)decidability
results for concrete DDSA classes are given in Sect. 5. We describe our imple-
mentation in Sect. 6. Complete proofs and further examples can be found in [27].
Related work. Veriﬁcation of transition systems with arithmetic constraints, also
called counter systems, has been studied in many areas including formal meth-
ods, database theory, and BPM. Reachability was proven decidable for a variety
of classes, e.g., reversal-bounded counter machines [34], ﬁnite linear [29], ﬂat [13],
and gap-order constraint (GC) systems [6]. Considerable work has also been
dedicated to linear-time veriﬁcation: LTL model checking is decidable for mono-
tonicity constraint (MC) systems [20]. LTL veriﬁcation is also decidable for inte-
ger periodicity constraint (IPC) systems, even with past-time operators [18,22];
and feedback-free systems, for an enriched constraint language referring to a
read-only database [14]. DDSAs with MCs are also considered in [25] from the
perspective of LTL with a ﬁnite-run semantics (LTLf), giving a procedure to
compute ﬁnite, faithful abstractions. LTLf is moreover decidable for systems
with the abstract ﬁnite summary property [28], which includes MC, GC, and
systems with bounded lookback, where the latter generalizes feedback freedom.

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
39
Branching-time veriﬁcation was less studied: Decidability of CTL∗was
proven for ﬂat counter systems with Presburger-deﬁnable loop iteration [21],
even in NP [19]. Moreover, it was shown that CTL∗veriﬁcation is decidable for
pushdown systems, which can model counter systems with a single integer vari-
able [30]. For integer relational automata (IRA), i.e., systems with constraints
x ≥y or x > y and domain Z, CTL model checking is undecidable while the exis-
tential and universal fragments of CTL∗remain decidable [12]. For GC systems,
which extend IRAs to constraints of the form x−y ≥k, the existential fragment
of CTL∗is decidable while the universal one is not [8]. A similar dichotomy holds
for the EF and EG fragments of CTL [42]. A subclass of IRAs was considered
in [7,11], allowing only periodicity and monotonicity constraints. While satisﬁ-
ability of CTL∗was proven decidable, model checking is not (as already shown
in [12]), though it is decidable for CEF+ properties, an extension of the EF
fragment [7]. In contrast, rather than restricting temporal operators, we show
decidability of model checking under an abstract property of the DDSA and
the veriﬁed property, which can be guaranteed by suitably constraining the con-
straint class or the control ﬂow. More closely related is work by Gascon [31], who
shows decidability of CTL∗model checking for DDSAs that admit a nice sym-
bolic valuation abstraction, an abstract property which includes MC and IPC
systems. The relationship between our decidability criterion and the property
deﬁned by Gascon will need further investigation. Another diﬀerence is that we
here adopt a ﬁnite-path semantics for CTL∗as e.g. considered in [47], since for
the analysis of real-world processes such as business processes it is suﬃcient to
consider ﬁnite traces. On a high level, our method follows a common approach
to CTL∗: the veriﬁcation property is processed bottom-up, computing solutions
for each subproperty. These are then used to solve an equivalent linear-time
problem [2, p. 429]. For the latter, we partially rely on earlier work [28].
2
Background
We start by deﬁning the set of constraints over expressions of sort int, rat, or
real, with associated domains dom(int) = Z, dom(rat) = Q, and dom(real) = R.
Deﬁnition 1. For a given set of sorted variables V , expressions es of sort s and
atoms a are deﬁned as follows:
es:= vs | ks | es + es | es −es
a := es = es | es < es | es ≤es | eint ≡n eint
where ks ∈dom(s), vs ∈V has sort s, and ≡n denotes equality modulo some
n ∈N. A constraint is then a quantiﬁer-free boolean expression over atoms a.
The set of all constraints built from atoms over variables V is denoted by C(V ).
For instance, x ̸= 1, x < y −z, and x −y = 2 ∧y ̸= 1 are valid constraints
independent of the sort of {x, y, z}, while u ≡3 v + 1 is a constraint for integer
variables u and v. We write Var(ϕ) for the set of variables in a formula ϕ. For

40
P. Felli et al.
an assignment α with domain V that maps variables to values in their domain,
and a formula ϕ we write α |= ϕ if α satisﬁes ϕ.
We are thus in the realm of SMT with linear arithmetic, which is decidable
and admits quantiﬁer elimination [45]: if ϕ is a formula in C(X ∪{y}), thus
having free variables X ∪{y}, there is a quantiﬁer-free ϕ′ with free variables X
that is equivalent to ∃y.ϕ, i.e., ϕ′ ≡∃y.ϕ, where ≡denotes logical equivalence.
2.1
Data-Aware Dynamic Systems with Arithmetic
From now on, V is a ﬁxed, ﬁnite set of variables. We consider two disjoint,
marked copies of V , denoted V r = {vr | v ∈V } and V w = {vw | v ∈V }, called
the read and write variables. They will refer to variable values before and after a
transition, respectively. We also write V for a vector that orders V in an arbitrary
but ﬁxed way, and V
r and V
w for vectors ordering V r and V w in the same way.
Deﬁnition 2. A DDSA B = ⟨B, bI , A, T, BF , V, αI , guard⟩is a labeled transi-
tion system where (i) B is a ﬁnite set of control states, with bI ∈B the initial
one; (ii) A is a set of actions; (iii) T ⊆B×A×B is a transition relation; (iv)
BF ⊆B are ﬁnal states; (v) V is the set of process variables; (vi) αI the ini-
tial variable assignment; (vii) guard : A →C(V r ∪V w) speciﬁes executability
constraints for actions over variables V r ∪V w.
Example 2. We consider the following DDSAs B, Bbl, and Bipc, where x, y have
domain Q and u, v, s have domain Z. Initial and ﬁnal states have incoming
arrows and double borders, respectively; αI is not ﬁxed for now.
b1
b2
b3
a1 : [yw > 0]
a2 : [xw > yr]
a3 : [xr = yr]
b1
b2
b3
[sw = ur]
[sw = sr + vr]
[uw = 0 ∧vw = 0]
[uw > 0]
[vw > 0]
b1
b2
a3 : [vr = ur ∧ur > 9]
a1 : [uw ≡7 vr]
a2 : [vw ≡2 ur]
Also the system in Example 1 represents a DDSA. If state b admits a transition to
b′ via action a, namely (b, a, b′) ∈Δ, this is denoted by b a−→b′. A conﬁguration
of B is a pair (b, α) where b ∈B and α is an assignment with domain V . A
guard assignment is an assignment β with domain V r ∪V w. For an action a,
let write(a) = Var(guard(a)) ∩V w. As deﬁned next, an action a transforms a
conﬁguration (b, α) into a new conﬁguration (b′, α′) by updating the assignment
α according to the action guard, which can at the same time evaluate conditions
on the current values of variables and write new values:
Deﬁnition 3. A DDSA B = ⟨B, bI , A, T, BF , V, αI , guard⟩admits a step from
conﬁguration (b, α) to (b′, α′) via action a, denoted (b, α)
a−→(b′, α′), if b
a−→b′,
α′(v) = α(v) for all v ∈V \ write(a), and the guard assignment β given by
β(vr) = α(v) and β(vw) = α′(v) for all v ∈V , satisﬁes β |= guard(a).
For instance, for B in Example 2 and initial assignment αI (x) = αI (y) = 0, the
initial conﬁguration admits a step (b1,
 x=0
y=0

) a1
−→(b2,
 x=0
y=3

) with β(xr) = β(xw) =
β(yr) = 0 and β(yw) = 3.

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
41
A run ρ of a DDSA B of length n from conﬁguration (b, α) is a sequence of
steps ρ: (b, α) = (b0, α0) a1
−→(b1, α1) a2
−→. . . an
−−→(bn, αn). We also associate with
ρ the symbolic run σ: b0
a1
−→b1
a2
−→. . . an
−−→bn where state and action sequences
are recorded without assignments, and say that σ is the abstraction of ρ (or, σ
abstracts ρ). For some m < n, σ|m denotes the preﬁx of σ that has m steps.
2.2
History Constraints
In this section, we ﬁx a DDSA B = ⟨B, bI , A, T, BF , V, αI , guard⟩. We aim to
build an abstraction of B that covers the (potentially inﬁnite) set of conﬁgura-
tions by ﬁnitely many nodes of the form (b, ϕ), where b ∈B is a control state
and ϕ a formula that expresses conditions on the variables V . A state (b, ϕ)
thus represents all conﬁgurations (b, α) s.t. α |= ϕ. To express how such a for-
mula ϕ is modiﬁed by executing an action, let the transition formula of action
a be Δa(V
r, V
w) = guard(a) ∧
v∈V \write(a) vw = vr. This states conditions on
variables before and after executing a: guard(a) must hold and the values of all
variables that are not written are propagated by inertia. We write Δa(X, Y ) for
the formula obtained from Δa by replacing V
r by X and V
w by Y . Let a variable
vector U be a fresh copy of V if it has the same length as |V | and U ∩V = ∅.
To mimic steps on the abstract level, we deﬁne the following update function:
Deﬁnition 4. For
a
formula
ϕ
with
free
variables
V
and
action
a,
update(ϕ, a) = ∃U.ϕ(U) ∧Δa(U, V ), where U is a fresh copy of V .
Our approach will generate formulas of a special shape called history con-
straints [28], obtained by iterated update operations in combination with a
sequence of veriﬁcation constraints ϑ. Intuitively, the latter depends on the ver-
iﬁcation property. For now it suﬃces to consider ϑ an arbitrary sequence of con-
straints with free variables V . Its preﬁx of length k is denoted by ϑ|k. We need
a ﬁxed set of placeholder variables V0 disjoint from V , and assume an injective
variable renaming ν : V →V0. Let ϕν be the formula ϕν = 
v∈V v = ν(v).
Deﬁnition 5. For a symbolic run σ: b0
a1
−→b1
a2
−→. . .
an
−−→bn, and veriﬁcation
constraint sequence ϑ = ⟨ϑ0, . . . , ϑn⟩, the history constraint h(σ, ϑ) is given by
h(σ, ϑ) = ϕν ∧ϑ0 if n = 0, and h(σ, ϑ) = update(h(σ|n−1, ϑ|n−1), an)∧ϑn if n > 0.
Thus, history constraints are formulas with free variables V ∪V0. Satisfying
assignments for history constraints are closely related to assignments in runs:1
Lemma 1. For a symbolic run σ: b0
a1
−→b1
a2
−→. . . an
−−→bn and ϑ = ⟨ϑ0, . . . , ϑn⟩,
h(σ, ϑ) is satisﬁed by assignment α with domain V ∪V0 iﬀσ abstracts a run
ρ: (b0, α0) a1
−→. . . an
−−→(bn, αn) such that (i) α0(v) = α(ν(v)), and (ii) αn(v) =
α(v) for all v ∈V , and (iii) αi |= ϑi for all i, 0 ≤i ≤n.
1 Lemma 1 is a slight variation of [28, Lemma 3.5]: Deﬁnition 5 diﬀers from history
constraints in [28] in that the initial assignment is not ﬁxed. A proof can be found
in [27].

42
P. Felli et al.
2.3
CTL∗
f
For a DDSA B as above, we consider the following veriﬁcation properties:
Deﬁnition 6. CTL∗
f state formulas χ and path formulas ψ are deﬁned by the
following grammar, for constraints c ∈C(V ) and control states b ∈B:
χ := ⊤| c | b | χ ∧χ | ¬χ | E ψ
ψ := χ | ψ ∧ψ | ¬ψ | X ψ | G ψ | ψ U ψ
We use the usual abbreviations F ψ = ⊤U ψ, χ1 ∨χ2 = ¬(¬χ1 ∧¬χ2), and
A ψ = ¬E ¬ψ. To simplify the presentation, we do not explicitly treat next state
operators ⟨a⟩via a speciﬁc action a, as used in Example 1, though this would be
possible (cf. [28]). However, such an operator can be encoded by adding a fresh
data variable x to V , the conjunct xw = 1 to guard(a), and xw = 0 to all other
guards, and replacing ⟨a⟩ψ in the veriﬁcation property by X (ψ ∧x = 1).
The maximal number of nested path quantiﬁers in a formula ψ is called the
quantiﬁer depth of ψ, denoted by qd(ψ). We adopt a ﬁnite path semantics for
CTL∗[44]: For a control state b ∈B and a state assignment α, let FRuns(b, α)
be the set of ﬁnal runs ρ: (b, α) = (b0, α0) a1
−→. . . an
−−→(bn, αn) such that bn ∈F
is a ﬁnal state. The i-th conﬁguration (bi, αi) in ρ is denoted by ρi.
Deﬁnition 7. The semantics of CTL∗
f is inductively deﬁned as follows. For a
DDSA B with conﬁguration (b, α), state formulas χ, χ′, and path formulas ψ, ψ′:
(b, α) |= ⊤
(b, α) |= c
iﬀα |= c
(b, α) |= b′
iﬀb = b′
(b, α) |= χ ∧χ′ iﬀ(b, α) |= χ and (b, α) |= χ′
(b, α) |= ¬χ
iﬀ(b, α) ̸|= χ
(b, α) |= E ψ
iﬀ∃ρ ∈FRuns(b, α) such that ρ |= ψ
where ρ |= ψ iﬀρ, 0 |= ψ holds, and for a run ρ of length n and all i, 0 ≤i ≤n:
ρ, i |= χ
iﬀρi |= χ
ρ, i |= ¬ψ
iﬀρ, i ̸|= ψ
ρ, i |= ψ ∧ψ′ iﬀρ, i |= ψ and ρ, i |= ψ′
ρ, i |= X ψ
iﬀi < n and ρ, i + 1 |= ψ
ρ, i |= G ψ
iﬀfor all j, i ≤j ≤n, it holds that ρ, j |= ψ
ρ, i |= ψ U ψ′ iﬀ∃k with i + k ≤n such that ρ, i + k |= ψ′
and for all j, 0 ≤j < k, it holds that ρ, i + j |= ψ.
Instead of simply checking whether the initial conﬁguration of a DDSA B
satisﬁes a CTL∗
f property χ, we try to determine, for every state b ∈B, which
constraints on variables need to hold in order to satisfy χ. As the number of
conﬁgurations (b, α) of a DDSA B is usually inﬁnite, conﬁguration sets cannot
be enumerated explicitly. Instead, we represent a set of conﬁgurations as a con-
ﬁguration map K : B →C(V ) that associates with every control state b ∈B a
formula K(b) ∈C(V ), representing all conﬁgurations (b, α) such that α |= K(b).
We now deﬁne when a conﬁguration captures the maximal set of conﬁgura-
tions in which a formula χ holds. We call these witness maps.

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
43
Deﬁnition 8. For a DDSA B and state formula χ, a conﬁguration map K is a
witness map if it holds that (b, α) |= χ iﬀα |= K(b), for all b ∈B and all α.
For instance, for B from Example 2 and χ1 = A G (x ≥2), a witness map is given
by K = {b1 →⊥, b2 →x ≥2 ∧y ≥2, b3 →x ≥2}. For χ2 |= E X (A G (x ≥2)),
a solution is K′ = {b1 →x ≥2, b2 →y ≥2, b3 →⊥}. As b1 is the initial state,
B satisﬁes χ2 with every initial assignment that sets αI (x) ≥2.
In this paper we address the problem of ﬁnding a witness map for B and χ.
Note that a witness map in particular allows to decide what is commonly called
the veriﬁcation problem, namely to check whether (bI , αI ) |= χ holds, by testing
αI |= K(bI ). It remains to investigate whether there exist a DDSA B and χ for
which no witness map exists, as the conﬁguration set satisfying χ is not ﬁnitely
representable. Even if it exists, ﬁnding it is in general undecidable. However, in
this paper we identify DDSA classes where a witness map can always be found.
3
LTL with Conﬁguration Maps
Following a common approach to CTL∗veriﬁcation, our technique processes
the property χ bottom-up, computing solutions for each subformula E ψ, before
solving a linear-time model checking problem χ′ in which the solutions to subfor-
mulas appear as atoms. Given our representation of sets of conﬁgurations, we use
LTL formulas where atoms are conﬁguration maps, and denote this speciﬁcation
language by LTLB
f . For a given DDSA B, it is formally deﬁned as follows:
ψ := K | ψ ∧ψ | ¬ψ | X ψ | G ψ | ψ U ψ
where K ∈KB, for KB is the set of conﬁguration maps for B.
Deﬁnition 9. A run ρ of length n satisﬁes an LTLB
f formula ψ, denoted ρ |=K
ψ, iﬀρ, 0 |=K ψ holds, where for all i, 0 ≤i ≤n:
ρ, i |=K K
iﬀρi = (b, α) and α |= K(b);
ρ, i |=K ψ ∧ψ′ iﬀρ, i |=K ψ and ρ, i |=K ψ′;
ρ, i |=K ¬ψ
iﬀρ, i ̸|=K ψ;
ρ, i |=K X ψ
iﬀi < n and ρ, i+1 |=K ψ;
ρ, i |=K G ψ
iﬀρ, i |=K ψ and (i = n or ρ, i+1 |=K G ψ);
ρ, i |=K ψ U ψ′ iﬀρ, i |=K ψ′ or ( i < n and ρ, i |=K ψ and ρ, i+1 |=K ψ U ψ′).
Our approach to LTLB
f veriﬁcation proceeds along the lines of the LTLf
procedure from [28], with the diﬀerence that simple constraint atoms are replaced
by conﬁguration maps. In order to express the requirements on a run of a DDSA
B to satisfy an LTLB
f formula χ, we use a nondeterministic automaton (NFA)
Nψ = (Q, Σ, ϱ, q0, QF ), where the states Q are a set of subformulas of ψ, Σ = 2KB
is the alphabet, ϱ is the transition relation, q0 ∈Q is the initial state, and QF ⊆
Q is the set of ﬁnal states. The construction of Nψ is standard [15,28], treating
conﬁguration maps for the time being as propositions; but for completeness it is
described in [27, Appendix C]. For instance, for a conﬁguration map K, ψ = F K

44
P. Felli et al.
corresponds to the NFA
ψ
⊤
K
and ψ′ = X K to
ψ′
K
⊤
K
. (For
simplicity, edges labels {K} are shown as K, and edge labels ∅are omitted.)
For wi ∈Σ, i.e., wi is a set of conﬁguration maps, wi(b) denotes the formula

K∈w K(b). Moreover, for w = w0, . . . , wn ∈Σ∗and a symbolic run σ: b0
a1
−→
b1
a2
−→. . . an
−−→bn, let w⊗σ denote the sequence of formulas ⟨w0(b0), . . . , wn(bn)⟩,
i.e., the component-wise application of w to the control states of σ. A word
w0, . . . , wn ∈Σ∗is consistent with a run (b0, α0) a1
−→(b1, α1) a2
−→. . . an
−−→(bn, αn)
if αi |= wi(bi) for all i, 0 ≤i ≤n. The key correctness property of Nψ is the
following (cf. [28, Lemma 4.4], and see [27] for the proof adapted to LTLB
f ):
Lemma 2. Nψ accepts a word that is consistent with a run ρ iﬀρ |=K ψ.
Product Construction. As a next step in our veriﬁcation procedure, given a con-
trol state b of B, we aim to ﬁnd (a symbolic representation of) all conﬁgurations
(b, α) that satisfy an LTLB
f formula ψ. To that end, we combine Nψ with B
to a cross-product automaton N ψ
B,b. For technical reasons, when performing the
product construction, the steps in B need to be shifted by one with respect to the
steps in Nψ. Hence, given b ∈B, let Bb be the DDSA obtained from B by adding
a dummy initial state b, so that Bb has state set B′ = B ∪{b} and transition
relation T ′ = T ∪{(b, a0, b)} for a fresh action a0 with guard(a0) = ⊤.
Deﬁnition 10. The product automaton N ψ
B,b is deﬁned for an LTLB
f formula
ψ, a DDSA B, and a control state b ∈B. Let Bb = ⟨B′, b, A, T ′, BF , V, αI , guard⟩
and Nψ as above. Then N ψ
B,b = (P, R, p0, PF ) is as follows:
• P ⊆B′ × Q × C(V ∪V0), i.e., states in P are triples (b, q, ϕ) such that
• the initial state is p0 = (b, q0, ϕν);
• if b a−→b′ in T ′, q
w
−→q′ in Nψ, and update(ϕ, a) ∧w(b′) is satisﬁable, there is
a transition (b, q, ϕ) a,w
−−→(b′, q′, ϕ′) in R such that ϕ′ ≡update(ϕ, a) ∧w(b′);
• (b′, q′, ϕ′) is in the set of ﬁnal states PF ⊆P iﬀb′ ∈BF , and q′ ∈QF .
Example 3. Consider the DDSA B from Example 2, and let K = {b1 →⊥, b2 →
x ≥2 ∧y ≥2, b3 →x ≥2}. The property ψ = X K is captured by the NFA
ψ
K
⊤
K
. The product automata N ψ
B,b1 and N ψ
B,b2 are as follows:
b ψ x = x0 ∧y = y0
b1 K x = x0 ∧y = y0
b2 ⊤x = x0 ∧x ≥2 ∧y ≥2
b3 ⊤x = x0 = y ∧x0 ≥2
b2 ⊤x ≥y ∧y ≥2 ∧x0 ≥2
b3 ⊤x = y ∧y ≥2 ∧x0 ≥2
a0
K
a1
a3
a2
a3
a2
b ψ x = x0 ∧y = y0
b2 K x = x0 ∧y = y0
b3 ⊤x = x0 = y = y0 ∧y0 ≥2
b2 ⊤y = y0 ∧x ≥y ∧y ≥2
b3 ⊤x = y = y0 ∧y0 ≥2
a0
K
a3
K
a2
a3
a2
where the shaded nodes are ﬁnal. The formulas in nodes were obtained by apply-
ing quantiﬁer elimination to the formulas built using update according to Deﬁ-
nition 10. N ψ
B,b3 consists only of the dummy transition and has no ﬁnal states.

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
45
Deﬁnition 10 need not terminate if inﬁnitely many non-equivalent formulas
occur in the construction. In Sect. 4 we will identify a criterion that guarantees
termination. First, we state the key correctness property, which lifts [28, Theorem
4.7] to LTL with conﬁguration maps. Its proof is similar to the respective result
in [28], and can be found in [27].
Theorem 1. Let ψ ∈LTLB
f and b ∈B such that there is a ﬁnite product automa-
ton N ψ
B,b. Then there is a ﬁnal run ρ: (b, α0) →∗(bF , αF ) of B such that ρ |=K ψ,
iﬀN ψ
B,b has a ﬁnal state (bF , qF , ϕ) for some qF and ϕ such that ϕ is satisﬁed
by assignment γ with γ(V0) = α0(V ) and γ(V ) = αF (V ).
Thus, witnesses for ψ correspond to paths to ﬁnal states in the product
automaton: e.g., in N ψ
B,b1 in Example 3 the formula in the left ﬁnal node is satis-
ﬁed by γ(x0) = γ(x) = γ(y) = 3 and γ(y0) = 0. For α0 and α2 such that α0(V ) =
γ(V 0) = {x →3, y →0} and α2(V ) = γ(V ) = {x →3, y →3} there is a witness
run for ψ from (b1, α0) to (b1, α2), e.g., (b1,
 x=3
y=0

) a1
−→(b2,
 x=3
y=3

) a3
−→(b3,
 x=3
y=3

).
4
Model Checking Procedure
Using the results of Sect. 3, we deﬁne a model checking procedure, shown in
Fig. 1. First, we explain the tasks achieved by the three mutually recursive func-
tions:
• checkState(χ) returns a conﬁguration map representing the set of conﬁg-
urations that satisfy a state formula χ. In the base cases, it returns a function
that checks the respective condition, for boolean operators we recurse on the
arguments, and for a formula E ψ we proceed to the checkPath procedure.
• checkPath(ψ) returns a conﬁguration map K that represents all conﬁgura-
tions from which a path satisfying ψ exists. First, toLTLK is used to obtain an
equivalent LTLB
f formula ψ′ (which entails the computation of solutions for all
subproperties E η). Then solution K is constructed as follows: For every control
state b, we build the product automaton N ψ′
B,b, and collect the set ΦF of formu-
las in ﬁnal states. Every ϕ ∈ΦF encodes runs from b to a ﬁnal state of B that
satisfy ψ′. The variables V0 and V in ϕ act as placeholders for the initial and the
ﬁnal values of the runs, respectively. We rename variables in ϕ to use V at the
start and U at the end, we quantify existentially over U (as the ﬁnal valuation
is irrelevant), and take the disjunction over all ϕ ∈ΦF . The resulting formula
ϕ′ encodes all ﬁnal runs from b that satisfy ψ′, so we set K(b) := ϕ′.
• toLTLK(ψ) computes an LTLB
f formula equivalent to a path formula ψ. To
this end, it performs two kinds of replacements in ψ: (a) ⊤, b ∈B, and constraints
c are represented as conﬁguration maps; and (b) subformulas E η are replaced
by their solutions KEη, which are computed by a recursive call to checkPath.
To represent the base cases of formulas as conﬁguration maps in Fig. 1, we
deﬁne K⊤:= (λ .⊤), Kb := (λb′.b = b′ ?⊤: ⊥) for all b ∈B, and Kc := (λ .c) for
constraints c. We also write ¬K for (λb.¬K(b)) and K∧K′ for (λb.K(b)∧K′(b)).
The next example illustrates the approach.

46
P. Felli et al.
Fig. 1. Model checking procedure.
Example 4. Consider χ = E X (A G (x ≥2)) and the DDSA B in Example 2. To
get a solution K1 to checkState(χ) = checkPath(ψ1) for ψ1 = X (A G (x ≥2)), we
ﬁrst compute an equivalent LTLB
f formula ψ′
1 = X K2, where K2 is a solution to
A G (x ≥2) ≡¬E F (x < 2). To this end, we run checkPath(ψ2) for ψ2 = F (x < 2),
which is represented in LTLB
f as ψ′
2 = F (Kx<2) with NFA
ψ′
1
⊤
Kx<2
. Next,
checkPath builds N ψ′
2
B,b for all states b. For instance, for b2 we get:
b0 ψ′
2 x = x0 ∧y = y0
b2 ψ′
2 x = x0 ∧y = y0
b2 ⊤x = x0 ∧y = y0 ∧x < 2
b2 ⊤y = y0 ∧2 > x ≥y
b3 ⊤x = x0 = y0 = y ∧x0 < 2
b2 ⊤y0 = y ∧2 > y ∧x ≥y
b2 ψ′
1 x ≥y = y0
b2 ⊤y = y0 ∧x ≥y ∧x0 < 2
b3 ⊤x = y = y0 ∧y < 2
b3 ⊤x = y = y0 ∧x0 < 2
Kx<2
Kx<2
Kx<2
Kx<2
ϕ1
ϕ2
ϕ3
where dashed arrows indicate transitions to non-ﬁnal sink states. For U = ⟨ˆx, ˆy⟩,
and the formulas ϕ1, ϕ2, and ϕ3 in ﬁnal nodes, we compute
∃U. ϕ1(V , U) = ∃ˆx ˆy. ˆx = x = ˆy = y ∧x < 2 ≡x < 2
∃U. ϕ2(V , U) = ∃ˆx ˆy. ˆx = ˆy = y ∧ˆy < 2
≡y < 2
∃U. ϕ3(V , U) = ∃ˆx ˆy. ˆx = ˆy = y ∧x < 2
≡x < 2

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
47
so that K3 := checkPath(ψ2) sets K3(b2) = 3
i=1 ∃U. ϕi(V , U) ≡x < 2 ∨y < 2.
For reasons of space, the constructions for b1 and b3 are shown in [27, Appendix
B]; we obtain K3(b1) = ⊤and K3(b3) = x < 2. By negation, the solution K2
to A G (x ≥2) is K2 = ¬K3 = {b1 →⊥, b2 →x ≥2 ∧y ≥2, b3 →x ≥2}.
Now we can proceed with checkPath(ψ1). The NFA and product automata for
ψ′
1 = X K2 are as shown in Example 3 and in a similar way as above we obtain
the solution K1 for E X A G (x ≥2) as K1 = {b1 →x ≥2, b2 →y ≥2, b3 →⊥}.
Thus, B satisﬁes the property for any initial assignment αI with αI (x) ≥2.
Next we prove correctness of checkState(χ) under the condition that it is deﬁned,
i.e., all required product automata are ﬁnite. First we state our main result, but
before giving its proof we show helpful properties of toLTLK and checkPath.
Theorem 2. For every conﬁguration (b, α) of the DDSA B and every state prop-
erty χ, if checkState(χ) is deﬁned then (b, α) |= χ iﬀα |= checkState(χ)(b).
Lemma 3. Let ψ be a path formula with qd(ψ) = k. Suppose that for all conﬁ-
gurations (b, α) and path formulas ψ′ with qd(ψ′) < k, there is a ρ′ ∈FRuns(b, α)
with ρ′ |= ψ′ iﬀα |= checkPath(ψ′)(b). Then ρ |= ψ iﬀρ |=K toLTLK(ψ).
Proof (sketch). By induction on ψ. The base cases are by the deﬁnitions of K⊤,
Kb, and Kc. In the induction step, if ψ = E ψ′ then ρ |= ψ iﬀ∃ρ′ ∈FRuns(b0, α0)
with ρ′ |= ψ′, for ρ0 = (b0, α0). As qd(ψ′) < qd(ψ), this holds by assump-
tion iﬀα0 |= checkPath(ψ′)(b0). This is equivalent to ρ |=K toLTLK(ψ) =
checkPath(ψ′). All other cases are by the induction hypothesis and Deﬁnitions 7
and 9.
Lemma 4. If ψ′ = toLTLK(ψ) such that for all runs ρ it is ρ |= ψ iﬀρ |=K ψ′,
there is a run ρ ∈FRuns(b, α) with ρ |= ψ iﬀα |= checkPath(ψ)(b).
Proof. (=⇒) Suppose there is a run ρ ∈FRuns(b, α) with ρ |= ψ, so ρ is of the
form (b, α) →∗(bF , αF ) for some bF ∈BF . By assumption, this implies ρ |=K ψ′,
so that by Theorem 1, N ψ′
B,b has a ﬁnal state (bF , qF , ϕ) where ϕ is satisﬁed by
an assignment γ with domain V ∪V0 such that γ(V0) = α(V ) and γ(V ) = αF (V ).
By deﬁnition, checkPath(ψ)(b) contains a disjunct ∃U. ϕ(V , U). As γ satisﬁes
ϕ and γ(V0) = α(V ), α |= checkPath(ψ)(b). (⇐=) If α |= checkPath(ψ)(b), by
deﬁnition of checkPath there is a formula ϕ such that α |= ∃U. ϕ(V , U) and ϕ
occurs in a ﬁnal state (bF , qF , ϕ) of N ψ′
B,b. Hence there is an assignment γ with
domain V ∪V0 and γ(V0) = α(V ) such that γ |= ϕ. By Theorem 1, there is a run
ρ: (b, α) →∗(bF , αF ) such that ρ |=K ψ′. By the assumption, we have ρ |= ψ. ⊓⊔
At this point the main theorem can be proven:
Proof (of Theorem 2). We ﬁrst show (⋆): for any path formula ψ, there is a
run ρ ∈FRuns(b, α) with ρ |= ψ iﬀα |= checkPath(ψ)(b). The proof is by
induction on qd(ψ). If ψ contains no path quantiﬁers, Lemma 3 implies that
ρ |= ψ iﬀρ |=K toLTLK(ψ) for all runs ρ, so (⋆) follows from Lemma 4. In the
induction step, we conclude from Lemma 3, using the induction hypothesis of

48
P. Felli et al.
(⋆) as assumption, that ρ |= ψ iﬀρ |=K toLTLK(ψ) for all runs ρ. Again, (⋆)
follows from Lemma 4.
The theorem is then shown by induction on χ: The base cases ⊤, b′ ∈B,
c ∈C are easy to check, and for properties of the form ¬χ′ and χ1 ∧χ2 the claim
follows from the induction hypothesis and the deﬁnitions. Finally, for χ = E ψ,
(b, α) |= χ iﬀthere is a run ρ ∈FRuns(b, α) such that ρ |= ψ. By (⋆) this is the
case iﬀα |= checkPath(ψ)(b) = checkState(χ)(b).
⊓⊔
Termination. We next show that the formulas generated in our procedure all
have a particular shape, to obtain an abstract termination result. For a set of
formulas Φ ⊆C(V ) and a symbolic run σ, let a history constraint h(σ, ϑ) be over
basis Φ if ϑ = ⟨ϑ0, . . . , ϑn⟩and for all i, 1 ≤i ≤n, there is a subset Ti ⊆Φ s.t.
ϑi =  Ti. Moreover, for a set of formulas Φ, let Φ± = Φ ∪{¬ϕ | ϕ ∈Φ}.
Deﬁnition 11. For a DDSA B, a constraint set C over free variables V , and
k ≥0, the formula sets Φk are inductively deﬁned by Φ0 = C ∪{⊤, ⊥} and
Φk+1 = {

ϕ∈H ∃U. ϕ(V , U) | H ⊆Hk}
where Hk is the set of all history constraints of B with basis 
i≤k Φ±
i .
Note that formulas in Φk have free variables V , while those in Hk have free vari-
ables V0 ∪V . We next show that these sets correspond to the formulas generated
by our procedure, if all constraints in the veriﬁcation property are in C.
Lemma 5. Let E ψ have quantiﬁer depth k, ψ′ = toLTLK(ψ), and N ψ′
B,b be a
constraint graph constructed in checkPath(ψ) for some b ∈B. Then,
(1) for all nodes (b′, q, ϕ) in N ψ′
B,b there is some ϕ′ ∈Hk such that ϕ ≡ϕ′,
(2) checkPath(ψ)(b) is equivalent to a formula in Φk+1.
The statements are proven by induction on k, using the results on the product
construction ([27, Lemma 6]). From part (1) of this lemma and Theorem 2 we
thus obtain an abstract criterion for decidability that will be useful in the next
section:
Corollary 1. For a DDSA B as above and a state formula χ, if Hj(b) is ﬁnite up
to equivalence for all j < qd(χ) and b ∈B, a witness map can always be computed.
Proof. By the assumption about the sets Hj(b) for j < qd(χ), all product
automata constructions in recursive calls checkPath(ψ) of checkState(χ) termi-
nate if logical equivalence of formulas is checked eagerly. Thus checkState(χ) is
deﬁned, and by Theorem 2 the result is a witness map.
⊓⊔
The property that all sets Hj(b), j < qd(χ), are ﬁnite might not be decidable
itself. However, in the next section we will show means to guarantee this property.
Moreover, we remark that ﬁniteness of all Hj(b) implies a ﬁnite history set,
a decidability criterion identiﬁed for the linear-time case [28, Deﬁnition 3.6];
but Example 5 below illustrates that the requirement on the Hj(b)’s is strictly
stronger.

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
49
5
Decidability of DDSA Classes
We here illustrate restrictions on DDSAs, either on the control ﬂow or on the
constraint language, that render our approach a decision procedure for CTL∗
f.
Monotonicity constraints (MCs) restrict constraints (Deﬁnition 1) as follows:
MCs over variables V and domain D have the form p ⊙q where p, q ∈D ∪V
and ⊙is one of =, ̸=, ≤, <, ≥, or >. The domain D may be R or Q. We call a
boolean formula whose atoms are MCs an MC formula, a DDSA where all atoms
in guards are MCs an MC-DDSA, and a CTL∗
f property whose constraint atoms
are MCs an MC property. For instance, B in Example 2 is an MC-DDSA.
We exploit a useful quantiﬁer elimination property: If ϕ is an MC formula over
a set of constants L and variables V ∪{x}, there is some ϕ′ ≡∃x. ϕ such that ϕ′ is
a quantiﬁer-free MC formula over V and L. Such a ϕ′ can be obtained by writing
ϕ in disjunctive normal form and applying a Fourier-Motzkin procedure [36,
Sect. 5.4] to each disjunct, which guarantees that all constants in ϕ′ also occur
in ϕ.
Theorem 3. For any DDSA B and property χ over monotonicity constraints,
a witness map is computable.
Proof. Let χ be an MC property, and L the ﬁnite set of constants in constraints
in χ, α0, and guards of B. Let moreover MCL be the set of quantiﬁer-free formulas
whose atoms are MCs over V ∪V0 and L, so MCL is ﬁnite up to equivalence.
We show the following property (⋆): all history constraints h(σ, ϑ) over basis
MCL are equivalent to a formula in MCL. For a symbolic run σ: b0 →∗bn−1
a−→bn
and a sequence ϑ = ⟨ϑ0, . . . , ϑn⟩over MCL, the proof is by induction on n.
In the base case, h(σ, ϑ) = ϕν ∧ϑ0 is in MCL because ϕν is a conjunction
of equalities between V ∪V0, and ϑ0 ∈MCL by assumption. In the induc-
tion step, h(σ, ϑ) = update(h(σ|n−1, ϑ|n−1), an) ∧ϑn. By induction hypothesis,
h(σ|n−1, ϑ|n−1) ≡ϕ for some ϕ in MCL. Thus h(σ, ϑ) ≡∃U.ϕ(U)∧Δa(U, V )∧ϑn.
As B is an MC-DDSA, Δa(U, V ) is a conjunction of MCs over V ∪U and con-
stants L, and ϑn ∈MCL by assumption. By the quantiﬁer elimination property,
there exists a quantiﬁer-free MC-formula ϕ′ over variables V0 ∪V that is equiva-
lent to ∃U.ϕ(U)∧Δa(U, V )∧ϑn, and mentions only constants in L, so ϕ′ ∈MCL.
For C the set of constraints in χ, we now show that Hj ⊆MCL for all
j ≥0, by induction on j. In the base case (j = 0), the claim follows from (⋆), as
all constraints in Φ0, i.e., in χ, are in MCL. For j > 0, consider ﬁrst a formula
ϕ ∈Φj for some b ∈B. Then ϕ is of the form ϕ = 
ϕ∈H ∃U. ϕ(V , U) for
some H ⊆Hj−1. By the induction hypothesis, H ⊆MCL, so by the quantiﬁer
elimination property of MC formulas, ϕ is equivalent to an MC-formula over V
and L in MCL. As Hj s built over basis Φj, the claim follows from (⋆).
⊓⊔
Notably, the above quantiﬁer elimination property fails for MCs over integer
variables; indeed, CTL model checking is undecidable in this case [42, Theorem
4.1].

50
P. Felli et al.
Integer periodicity constraint systems conﬁne the constraint language to
variable-to-constant comparisons and restricted forms of variable-to-variable
comparisons, and are for instance used in calendar formalisms [18,22]. More
precisely, integer periodicity constraint (IPC) atoms have the form x = y, x ⊙d
for ⊙∈{=, ̸=, <, >}, x ≡k y + d, or x ≡k d, for variables x, y with domain
Z and k, d ∈N. A boolean formula whose atoms are IPCs is an IPC formula,
a DDSA whose guards are conjunctions of IPCs an IPC-DDSA, and a CTL∗
f
formula whose constraint atoms are IPCs an IPC property. For instance, Bipc in
Example 2 is an IPC-DDSA.
Using Corollary 1 and a known quantiﬁer elimination property for IPCs [18,
Theorem 2], one can show that witness maps are also computable for IPC-
DDSAs, in a proof that resembles the one of Theorem 3 (see [27, Theorem 4]).
Theorem 4. For any DDSA B and property χ over integer periodicity con-
straints, a witness map is computable.
The proofs of both Theorems 3 and 4 rely on the fact that all transition guards
and constraints in the veriﬁcation property are in a ﬁnite set of constraints C
that is closed under quantiﬁer elimination, so that for all ϕ ∈C and actions
a, update(ϕ, a) is again equivalent to a formula in C. However, this is not the
only way to ensure the requirements of Corollary 1: For a simple example, these
requirements are satisﬁed by a loop-free DDSA, where the number of runs is
ﬁnite. Interestingly, while the cases of MC and IPC systems are also captured
by the abstract decidability criterion by Gascon [31], this need not apply to loop-
free DDSAs. A clariﬁcation of the relationship between the criteria in Corollary 1
and [31, Thm 4.5] requires further investigation.
Bounded lookback [28] restricts the control ﬂow of a DDSA rather than the
constraint language, and is a generalization of the earlier feedback-freedom prop-
erty [14]. Intuitively, k-bounded lookback demands that the behavior of a DDSA
at any point in time depends only on k events from the past. We refer to [28,
Deﬁnition 5.9] for the formal deﬁnition. Systems that enjoy bounded lookback
allow for decidable linear-time veriﬁcation [28, Theorem 5.10]. However, we next
show that this result does not extend to branching time.
Example 5. We reduce control state reachability of two-counter machines (2CM)
to the veriﬁcation problem of CTL∗
f formulas in bounded lookback systems,
inspired by [42, Theorem 4.1]. 2CMs have a ﬁnite control structure and two
counters x1, and x2 that can be incremented, decremented, and tested for 0. It
is undecidable whether a 2CM will ever reach a designated control state f [43].
For a 2CM M, we build a feedback-free DDSA B = ⟨B, bI , A, T, BF , V, αI , guard⟩
and a CTL∗
f property χ such that B satisﬁes χ iﬀf is reachable in M. The set B
consists of the control states of M, together with an error state e and auxiliary
states bt for transitions t of M, and BF = {f, e}. The set V consists of x1, x2
and auxiliary variables p1, p2, m1, m2. Zero-test transitions of M are directly
modeled in B, whereas a step q →q′ that increments xi by one is modeled as:

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
51
q
bt
q′
e
xw
i ≥0 ∧pw
i = xr
i
xr
i ̸= pr
i + 1
The step q →bt writes xi, storing its previous value in pi, but if the write was not
an increment by exactly 1, a step to state e is enabled. Decrements are modeled
similarly. Intuitively, bounded lookback holds because variable dependencies are
limited: in a run of M, a variable dependency that is not an equality extends over
at most two time points. (More formally, non-equality paths in the computation
graph have at most length 1.) As increments are not exact, B overapproximates
M. However, χ = E G (¬E X e) asserts existence of a path that never allows for
a step to e (i.e., it properly simulates M) but reaches the ﬁnal state f. Thus, B
satisﬁes χ iﬀf is reachable in M.
6
Implementation
We implemented our approach in the prototype ada (arithmetic DDS analyzer)
in Python; source code, benchmarks, and a web interface are available (https://
ctlstar.adatool.dev). As input, the tool takes a CTL∗property χ together with
a DDSA in JSON format; alternatively, a given (bounded) Petri net with data
(DPN) in PNML format [5] can be transformed into a DDSA. The tool then
applies the algorithm in Fig. 1. If successful, it outputs the conﬁguration map
returned by checkState(χ), and it can visualize the product constructions. For
SMT checks and quantiﬁer elimination, ada interfaces CVC5 [23] and Z3 [17].
Besides numeric variables, ada also supports variables of type boolean and string;
for the latter, only equality comparison is supported, so diﬀerent constants can
be represented by distinct integers. In addition to the operations in Deﬁnition 6,
ada allows next operators ⟨a⟩via an action a, which are useful for veriﬁcation.
We tested ada on a set of business process models presented as Data Petri nets
(DPNs) in the literature. As these nets are bounded, they can be transformed
into DDSAs. The results are reported in the table below. We indicate whether
the system belongs to a decidable class, the veriﬁed property and whether it is
satisﬁed by the initial conﬁguration, the veriﬁcation time, the number of SMT
checks, and the number of nodes in the DDSA B and the sum of all product
constructions, respectively. We used CVC5 as SMT solver; times are without
visualization, which tends to be time-consuming for large graphs. All tests were
run on an Intel Core i7 with 4×2.60 GHz and 19 GB RAM.

52
P. Felli et al.
process
property
sat
time
checks |B| |N ψ
B,b|
(a) road ﬁnes
No deadlock
✗
7.0s
8161
9
2052
AG (p7 →E F end)
✓
7.6s
7655
1987
AG (end →total ≤amount)
✗
1m12s
111139
3622
(b) road ﬁnes
No deadlock
✓
15m27s
247563
9
4927
AG (p7 →E F end)
✓
16m7s
246813
4927
(c) road ﬁnes
No deadlock
✗
9s
9179
9
1985
AG (p7 →E F end)
✓
6.6s
6382
1597
ψc1 = EF (dS ≥2160)
✗
11.5s
17680
1280
ψc2 = EF (dP ≥1440)
✗
10.0s
15187
1280
ψc3 = EF (dJ ≥1440)
✗
10.5
16000
1280
(d) hospital billing
No deadlock
✓
20m59s 1234928
17
23147
ψd1 = EF (p16 ∧¬closed)
✓
10m20s
669379
10654
(e) sepsis
No deadlock
✓
1m36s
139 301
44939
ψe1 = AG (sink →ttr < tab)
✗
30.1s
170
22724
ψe2 = AG (sink →ttr+60 ≥tab)
✓
32s
153
22538
(f) sepsis
No deadlock
✓
7m24
4524 301 161242
ψf1 = A (¬lacticAcidG ⟨diagnostic⟩⊤)
✓
3m53s
5734
74984
(g) board: register
No deadlock
✓
1.4s
12
7
27
(h) board: transfer
No deadlock
✓
1.4s
27
7
51
(i) board: discharge
No deadlock
✓
1.5s
25
6
67
ψi1 = AG (p2 ∧o1=207 →AG o1=207)
✓
1.5s
94
91
ψi2 = A (EF ⟨tra⟩⊤∧EF ⟨his⟩⊤)
✓
1.5s
27
98
ψi3 = ¬E (F ⟨tra⟩⊤∧F ⟨his⟩⊤)
✓
1.4s
56
43
(j) credit approval
No deadlock
✓
1.7s
470
6
230
ψj1 = AG (⟨openLoan⟩⊤→ver ∧dec)
✓
13.2s
14156
645
ψj2 = A (F (ver ∧dec) →F ⟨openLoan⟩⊤)
✗
3.7s
3128
316
ψj3 = A (F (ver ∧dec) →EF ⟨openLoan⟩⊤)
✓
5.6s
4748
548
(k) package handling
No deadlock
✓
2.7ss
1025
16
693
No deadlock (τ1)
✓
2.5s
1079
398
ψk1 = EF ⟨fetch⟩⊤
✗
2.6s
850
343
ψk2 = EF ⟨τ6 ⟩⊤
✗
2.4s
875
336
(l) auction
No deadlock
✗
10.8s
1683
5
186
EF (sold ∧d > 0 ∧o ≤t)
✗
6.4s
1180
79
EF (b = 1 ∧o > t ∧F (sold ∧b > 1))
✓
26.5s
4000
263
We brieﬂy comment on the benchmarks and some properties: For all examples
we checked no deadlock, which abbreviates AG EF χf where χf is a disjunction
of all ﬁnal states. This is one of the two requirements of the crucial soundness
property (cf. Example 1). Weak soundness [4] relaxes this requirement to demand
only that if a transition is reachable, it does not lead to deadlocks; this is called
here no deadlock(a), expressed by EF (⟨a⟩⊤) →AG (⟨a⟩⊤→F χf). One can also
check whether a speciﬁc state p is deadlock-free, via AG (p →EF χf).
(a)-(c) are versions of the road ﬁne management process (cf. Example 1); (a) [40,
Fig. 12.7] and (b) [37, Fig. 13] were mined automatically from logs, while (c)
is the normative version [41, Fig. 7] shown in Example 1. While in (a) and
(c) no deadlock is violated, this issue was ﬁxed in version (b). The fact that
ψc1, ψc2, and ψc3 hold conﬁrm that the time constraints are never violated.
(d) models a billing process in a hospital [40, Fig. 15.3], which is deadlock-free.
(e) is a normative model for a sepsis triage process in a hospital [40, Fig. 13.3],
and (f) is a variation that was mined purely automatically from logs [40,
Fig. 13.6]. According to [40, Sect. 13], triage should happen before antibiotics
are administered, expressed by ψe1, which is actually not satisﬁed. However,
the desired time constraint expressed by ψe2 holds.
(g)–(i) reﬂect activities in patient logistics of a hospital, based on logs of real-
life processes [40, Fig. 14.3]. While the no deadlock property is satisﬁed by

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
53
all initial conﬁgurations, the output of ada reveals that for (h) this need not
hold for other initial assignments.
(j) is a credit approval process [16, Fig. 3]. It is deadlock-free; ψj1 and ψj2 verify
desirable conditions under which a loan is granted to a client.
(k) is a package handling routine [26, Fig. 5]. The fact that the properties ψk1
and ψk2 are not satisﬁed shows that the transitions τ6 and fetch are dead.
(l) models an auction process [28, Example 1.1], for which ada reveals a deadlock.
Results for two further properties from [28, Example 1.1] are listed as well.
Seven systems are in a decidable class wrt. the listed properties: (a), (b), (d),
(f), (h), (i), (k) are MC, while (d), (h), (i), (k) are IPC. This is due to the fact
that automatic mining techniques often produce monotonicity constraints [39].
7
Conclusion
This paper presents a technique to compute witness maps for a given DDSA and
CTL∗
f property, where a witness map speciﬁes conditions on the initial variable
assignment such that the property holds. The addressed problem is thus a slight
generalization of the common veriﬁcation problem. While our model checking
procedure need not terminate in general, we show that it does if an abstract
property on history constraints holds. Moreover, witness maps always exist for
monotonicity and integer periodicity constraint systems. However, this result
does not extend to bounded lookback systems. We implemented our approach
in the tool ada and showed its usefulness on a range of business process models.
We see various opportunities to extend this work. A richer veriﬁcation lan-
guage could support past time operators [18] and future variable values [20].
Further decidable fragments could be sought using covers [33], or aiming for
compatibility with locally ﬁnite theories [32]. Moreover, a restricted version of
the bounded lookback property could guarantee decidability of CTL∗
f, similarly
to the way feedback freedom was strengthened in [35]. The implementation could
be improved to avoid the computation of many similar formulas, thus gaining
eﬃciency. Finally, the complexity class that our approach implies for CTL∗
f in
the decidable classes is yet to be clariﬁed.
References
1. van der Aalst, W.M.P.: Process Mining: Data Science in Action. Springer (2016).
https://doi.org/10.1007/978-3-662-49851-4
2. Baier, C., Katoen, J.: Principles of Model Checking. MIT Press (2008)
3. Baral, C., De Giacomo, G.: Knowledge representation and reasoning: what’s hot.
In: Proceedings of the 29th AAAI, pp. 4316–4317 (2015)
4. Batoulis, K., Haarmann, S., Weske, M.: Various notions of soundness for decision-
aware business processes. In: Mayr, H.C., Guizzardi, G., Ma, H., Pastor, O. (eds.)
ER 2017. LNCS, vol. 10650, pp. 403–418. Springer, Cham (2017). https://doi.org/
10.1007/978-3-319-69904-2 31

54
P. Felli et al.
5. Billington, J., et al.: The petri net markup language: concepts, technology, and
tools. In: van der Aalst, W.M.P., Best, E. (eds.) ICATPN 2003. LNCS, vol. 2679,
pp. 483–505. Springer, Heidelberg (2003). https://doi.org/10.1007/3-540-44919-
1 31
6. Bozga, M., Gˆırlea, C., Iosif, R.: Iterating octagons. In: Kowalewski, S., Philippou,
A. (eds.) TACAS 2009. LNCS, vol. 5505, pp. 337–351. Springer, Heidelberg (2009).
https://doi.org/10.1007/978-3-642-00768-2 29
7. Bozzelli, L., Gascon, R.: Branching-time temporal logic extended with qualitative
Presburger constraints. In: Hermann, M., Voronkov, A. (eds.) LPAR 2006. LNCS
(LNAI), vol. 4246, pp. 197–211. Springer, Heidelberg (2006). https://doi.org/10.
1007/11916277 14
8. Bozzelli, L., Pinchinat, S.: Veriﬁcation of gap-order constraint abstractions of
counter systems. Theor. Comput. Sci. 523, 1–36 (2014). https://doi.org/10.1016/
j.tcs.2013.12.002
9. Calvanese, D., De Giacomo, G., Montali, M.: Foundations of data-aware process
analysis: a database theory perspective. In: Proceedings of the 32nd PODS, pp.
1–12 (2013). https://doi.org/10.1145/2463664.2467796
10. Calvanese, D., De Giacomo, G., Montali, M., Patrizi, F.: First-order µ-calculus over
generic transition systems and applications to the situation calculus. Inf. Comput.
259(3), 328–347 (2018). https://doi.org/10.1016/j.ic.2017.08.007
11. Carapelle, C., Kartzow, A., Lohrey, M.: Satisﬁability of ECTL∗with constraints.
J. Comput. Syst. Sci. 82(5), 826–855 (2016). https://doi.org/10.1016/j.jcss.2016.
02.002
12. ˇCer¯ans, K.: Deciding properties of integral relational automata. In: Abiteboul, S.,
Shamir, E. (eds.) ICALP 1994. LNCS, vol. 820, pp. 35–46. Springer, Heidelberg
(1994). https://doi.org/10.1007/3-540-58201-0 56
13. Comon, H., Jurski, Y.: Multiple counters automata, safety analysis and Presburger
arithmetic. In: Hu, A.J., Vardi, M.Y. (eds.) CAV 1998. LNCS, vol. 1427, pp. 268–
279. Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0028751
14. Damaggio, E., Deutsch, A., Vianu, V.: Artifact systems with data dependencies
and arithmetic. ACM Trans. Database Syst. 37(3), 22:1–22:36 (2012). https://doi.
org/10.1145/2338626.2338628
15. de Giacomo, G., De Masellis, R., Montali, M.: Reasoning on LTL on ﬁnite traces:
insensitivity to inﬁniteness. In: Proceedings of the 28th AAAI, pp. 1027–1033
(2014)
16. de Leoni, M., Mannhardt, F.: Decision discovery in business processes. In: Ency-
clopedia of Big Data Technologies, pp. 1–12. Springer (2018). https://doi.org/10.
1007/978-3-319-63962-8 96-1
17. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
18. Demri, S.: LTL over integer periodicity constraints. Theor. Comput. Sci. 360(1–3),
96–123 (2006). https://doi.org/10.1016/j.tcs.2006.02.019
19. Demri, S., Dhar, A.K., Sangnier, A.: Equivalence between model-checking ﬂat
counter systems and Presburger arithmetic. Theor. Comput. Sci. 735, 2–23 (2018).
https://doi.org/10.1016/j.tcs.2017.07.007
20. Demri, S., D’Souza, D.: An automata-theoretic approach to constraint LTL.
Inform. Comput. 205(3), 380–415 (2007). https://doi.org/10.1016/j.ic.2006.09.006
21. Demri, S., Finkel, A., Goranko, V., van Drimmelen, G.: Model-checking CTL* over
ﬂat Presburger counter systems. J. Appl. Non Class. Logics 20(4), 313–344 (2010).
https://doi.org/10.3166/jancl.20.313-344

CTL∗Model Checking for Data-Aware Dynamic Systems with Arithmetic
55
22. Demri, S., Gascon, R.: Veriﬁcation of qualitative Z constraints. Theor. Comput.
Sci. 409(1), 24–40 (2008). https://doi.org/10.1016/j.tcs.2008.07.023
23. Deters, M., Reynolds, A., King, T., Barrett, C.W., Tinelli, C.: A tour of CVC4:
how it works, and how to use it. In: Proceedings of the 14th FMCAD, p. 7 (2014).
https://doi.org/10.1109/FMCAD.2014.6987586
24. Deutsch, A., Hull, R., Li, Y., Vianu, V.: Automatic veriﬁcation of database-
centric systems. ACM SIGLOG News 5(2), 37–56 (2018). https://doi.org/10.1145/
3212019.3212025
25. Felli, P., de Leoni, M., Montali, M.: Soundness veriﬁcation of decision-aware process
models with variable-to-variable conditions. In: Proceedings of the 19th ACSD, pp.
82–91. IEEE (2019). https://doi.org/10.1109/ACSD.2019.00013
26. Felli, P., de Leoni, M., Montali, M.: Soundness veriﬁcation of data-aware process
models with variable-to-variable conditions. Fund. Inform. 182(1), 1–29 (2021).
https://doi.org/10.3233/FI-2021-2064
27. Felli, P., Montali, M., Winkler, S.: CTL∗model checking for data-aware dynamic
systems with arithmetic (extended version) (2022). https://doi.org/10.48550/
arXiv.2205.08976
28. Felli, P., Montali, M., Winkler, S.: Linear-time veriﬁcation of data-aware dynamic
systems with arithmetic. In: Proceedings of the 36th AAAI (2022). https://doi.
org/10.48550/arXiv.2203.07982
29. Finkel, A., Leroux, J.: How to compose Presburger-accelerations: applications to
broadcast protocols. In: Agrawal, M., Seth, A. (eds.) FSTTCS 2002. LNCS, vol.
2556, pp. 145–156. Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-
36206-1 14
30. Finkel, A., Willems, B., Wolper, P.: A direct symbolic approach to model checking
pushdown systems. In: Proc. 2nd INFINITY. ENTCS, vol. 9, pp. 27–37 (1997).
https://doi.org/10.1016/S1571-0661(05)80426-8
31. Gascon, R.: An automata-based approach for CTL∗with constraints. In: Proceed-
ings of the INFINITY 2006, 2007 and 2008. ENTCS, vol. 239, pp. 193–211 (2009).
https://doi.org/10.1016/j.entcs.2009.05.040
32. Ghilardi, S., Nicolini, E., Ranise, S., Zucchelli, D.: Combination methods for satis-
ﬁability and model-checking of inﬁnite-state systems. In: Pfenning, F. (ed.) CADE
2007. LNCS (LNAI), vol. 4603, pp. 362–378. Springer, Heidelberg (2007). https://
doi.org/10.1007/978-3-540-73595-3 25
33. Gulwani, S., Musuvathi, M.: Cover algorithms and their combination. In:
Drossopoulou, S. (ed.) ESOP 2008. LNCS, vol. 4960, pp. 193–207. Springer, Hei-
delberg (2008). https://doi.org/10.1007/978-3-540-78739-6 16
34. Ibarra, O.H., Su, J.: Counter machines: decision problems and applications. In:
Jewels are Forever: Contributions on Theoretical Computer Science in Honor of
Arto Salomaa, pp. 84–96 (1999)
35. Koutsos, A., Vianu, V.: Process-centric views of data-driven business artifacts. J.
Comput. Syst. Sci. 86, 82–107 (2017). https://doi.org/10.1016/j.jcss.2016.11.012
36. Kroening, D., Strichman, O.: Decision Procedures - An Algorithmic Point of View.
Second Edition. Springer (2016). https://doi.org/10.1007/978-3-662-50497-0
37. de Leoni, M., Felli, P., Montali, M.: A holistic approach for soundness veriﬁcation
of decision-aware process models. In: Trujillo, J.C., Davis, K.C., Du, X., Li, Z.,
Ling, T.W., Li, G., Lee, M.L. (eds.) ER 2018. LNCS, vol. 11157, pp. 219–235.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-00847-5 17
38. de Leoni, M., Felli, P., Montali, M.: Strategy synthesis for data-aware dynamic
systems with multiple actors. In: Proceedings of the 17th KR, pp. 315–325 (2020).
https://doi.org/10.24963/kr.2020/32

56
P. Felli et al.
39. de Leoni, M., Felli, P., Montali, M.: Integrating BPMN and DMN: modeling and
analysis. J. Data Semant. 10(1), 165–188 (2021). https://doi.org/10.1007/s13740-
021-00132-z
40. Mannhardt, F.: Multi-perspective process mining. Ph.D. thesis, Technical Univer-
sity of Eindhoven (2018)
41. Mannhardt, F., de Leoni, M., Reijers, H.A., van der Aalst, W.M.P.: Balanced multi-
perspective checking of process conformance. Computing 98(4), 407–437 (2015).
https://doi.org/10.1007/s00607-015-0441-1
42. Mayr, R., Totzke, P.: Branching-time model checking gap-order constraint sys-
tems. Fundam. Informaticae 143(3–4), 339–353 (2016). https://doi.org/10.3233/
FI-2016-1317
43. Minsky, M.: Computation: Finite and Inﬁnite Machines. Prentice-Hall (1967)
44. Murano, A., Parente, M., Rubin, S., Sorrentino, L.: Model-checking graded
computation-tree logic with ﬁnite path semantics. Theor. Comput. Sci. 806, 577–
586 (2020). https://doi.org/10.1016/j.tcs.2019.09.021
45. Presburger, M.: ¨Uber die Vollst¨andigkeit eines gewissen Systems der Arithmetik
ganzer Zahlen, in welchem die Addition als einzige Operation hervortritt. In:
Comptes Rendus du I congres de Mathem. des Pays Slaves, pp. 92–101 (1929)
46. Reichert, M.: Process and data: two sides of the same coin? In: Meersman, R.,
Panetto, H., Dillon, T., Rinderle-Ma, S., Dadam, P., Zhou, X., Pearson, S., Ferscha,
A., Bergamaschi, S., Cruz, I.F. (eds.) OTM 2012. LNCS, vol. 7565, pp. 2–19.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-33606-5 2
47. Sorrentino, L., Rubin, S., Murano, A.: Graded CTL* over ﬁnite paths. In: Pro-
ceedings of the 19th ICTCS. CEUR Workshop Proceedings, vol. 2243, pp. 152–161.
CEUR-WS.org (2018)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

SAT-Based Proof Search in Intermediate
Propositional Logics
Camillo Fiorentini1
and Mauro Ferrari2(B)
1 Department of Computer Science, Universit`a degli Studi di Milano, Milan, Italy
2 Department of Theoretical and Applied Sciences,
Universit`a degli Studi dell’Insubria, Varese, Italy
mauro.ferrari@uninsubria.it
Abstract. We present a decision procedure for intermediate logics rely-
ing on a modular extension of the SAT-based prover intuitR for IPL
(Intuitionistic Propositional Logic). Given an intermediate logic L and a
formula α, the procedure outputs either a Kripke countermodel for α or
the instances of the characteristic axioms of L that must be added to IPL
in order to prove α. The procedure exploits an incremental SAT-solver;
during the computation, new clauses are learned and added to the solver.
1
Introduction
Recently, Claessen and Ros´en have introduced intuit [4], an eﬃcient decision
procedure for Intuitionistic Propositional Logic (IPL) based on the Satisﬁability
Modulo Theories (SMT) approach. The prover language consists of (ﬂat) clauses
of the form  A1 → A2 (with Ai a set of atoms), which are fed to the SAT-
solver, and implication clauses of the form (a →b) →c (a, b, c atoms); thus,
we need an auxiliary clausiﬁcation procedure to preprocess the input formula.
The search is performed via a proper variant of the DPLL(T ) procedure [16],
by exploiting an incremental SAT-solver; during the computation, whenever a
semantic conﬂict is thrown, a new clause is learned and added to the SAT-solver.
As discussed in [9], there is a close connection between the intuit approach and
the known proof-theoretic methods. Actually, the decision procedure mimics the
standard root-ﬁrst proof search strategy for a sequent calculus strongly con-
nected with Dyckhoﬀ’s calculus LJT [5] (alias G4ip). To improve performances,
we have re-designed the prover by adding a restart operation, thus obtaining
intuitR [8] (intuit with Restart). Diﬀerently from intuit, the intuitR pro-
cedure has a simple structure, consisting of two nested loops. Given a formula
α, if α is provable in IPL the call intuitR(α) yields a derivation of α in the
sequent calculus introduced in [8], a plain calculus where derivations have a sin-
gle branch. If α is not provable in IPL, the outcome of intuitR(α) is a (typically
small) countermodel for α, namely a Kripke model falsifying α. We stress that
intuitR is highly performant: on the basis of a standard benchmarks suite, it
outperforms intuit and other state-of-the-art provers (in particular, fCube [6]
and intHistGC [12]).
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 57–74, 2022.
https://doi.org/10.1007/978-3-031-10769-6_5

58
C. Fiorentini and M. Ferrari
In this paper we present intuitRIL, an extension of intuitR to Interme-
diate Logics, namely propositional logics extending IPL and contained in CPL
(Classical Propositional Logic). Speciﬁcally, let α be a formula and L an axiom-
atizable intermediate logic having Kripke semantics; the call intuitRIL(α,L)
tries to prove the validity of α in L. To this aim, the prover searches for a set
Ψ containing instances of Ax(L), the characteristic axioms of L, such that α
can be proved in IPL from Ψ. Note that this is diﬀerent from other approaches,
where the focus is on the synthesis of speciﬁc inference rules for the logic at
hand (see, e.g., [17]). Basically, intuitRIL(α,L) searches for a countermodel K
for α, exploiting the search engine of intuitR: whenever we get K, we check
whether K is a model of L. If this is the case, we conclude that α is not valid
in L (and K is a witness to this). Otherwise, the prover selects an instance ψ
of Ax(L) falsiﬁed in K (there exists at least one); ψ is acknowledged as learned
axiom and, after clausiﬁcation, it is fed to the SAT-solver. We stress that a naive
implementation of the procedure, where at each iteration of the main loop the
computation restarts from scratch, would be highly ineﬃcient: each time the
SAT-solver should be initialized by inserting all the clauses encoding the input
problem and all the clauses learned so far. Instead, we exploit an incremental
SAT-solver, where clauses can be added but never deleted (hence, all the sim-
pliﬁcations and optimisations performed by the solver are preserved); note that
this prevents us from exploiting strategies based on standard sequent/tableaux
calculi, where backtracking is required.
If the call intuitRIL(α,L) succeeds, by tracking the computation we get a
derivation D of α in the sequent calculus CL (see Fig. 1); from D we can extract
all the axioms learned during the computation. We stress that the procedure is
quite modular: to handle a logic L, one has only to implement a speciﬁc learning
mechanism for L (namely: if K is not a model of L, pick an instance of Ax(L)
falsiﬁed in K). The main drawback is that there is no general way to bound the
learned axioms, thus termination must be investigated on a case-by-case basis.
We guarantee termination for some relevant intermediate logics, such as G¨odel-
Dummett Logic GL, the family GLn (n ≥1) of G¨odel-Dummett Logics with
depth bounded by n (GL1 coincides with Here and There Logic, well known
for its applications in Answer Set Programming [15]) and Jankov Logic (for a
presentation of such logics see [2]). As a corollary, for each of the mentioned
logic L we get a bounding function [3], namely: given α, we compute a bounded
set Ψα of instances of Ax(L) such that α is valid in L iﬀα is provable in IPL
from assumptions Ψα; in general we improve the bounds in [1,3]. The intuitRIL
Haskell implementation and other additional material (e.g., the omitted proofs)
can be downloaded at https://github.com/cﬁorentini/intuitRIL.
2
Basic Deﬁnitions
Formulas, denoted by lowercase Greek letters, are built from an enumerable set of
propositional variables V, the constant ⊥and the connectives ∧, ∨, →; moreover,
¬α stands for α →⊥and α ↔β stands for (α →β) ∧(β →α). Elements of
the set V ∪{⊥} are called atoms and are denoted by lowercase Roman letters,

SAT-Based Proof Search in Intermediate Propositional Logics
59
uppercase Greek letters denote sets of formulas. By Vα we denote the set of
propositional variables occurring in α. The notation is extended to sets: VΓ is
the union of Vα such that α ∈Γ; VΓ,Γ ′ and VΓ,α stand for VΓ ∪Γ ′ and VΓ ∪{α}
respectively. A substitution is a map from propositional variables to formulas.
By [p1 
→α1, . . . , pn 
→αn] we denote the substitution χ such that χ(p) = αi if
p = pi and χ(p) = p otherwise; the set {p1, . . . , pn} is the domain of χ, denoted
by Dom(χ); ϵ is the substitution having empty domain. The application of χ to
a formula α, denoted by χ(α), is deﬁned as usual; χ(Γ) is the set of χ(α) such
that α ∈Γ. The composition χ1 · χ2 is the substitution mapping p to χ1(χ2(p)).
A (classical) interpretation M is a subset of V, identifying the propositional
variables assigned to true. By M |= α we mean that α is true in M; M |= Γ
iﬀM |= α for every α ∈Γ. Classical Propositional Logic (CPL) is the set of
formulas true in every interpretation. We write Γ
⊢c α iﬀM |= Γ implies
M |= α, for every M. Note that α is CPL-valid (namely, α ∈CPL) iﬀ∅⊢c α.
A (rooted) Kripke model is a quadruple ⟨W, ≤, r, ϑ⟩where W is a ﬁnite
and non-empty set (the set of worlds), ≤is a reﬂexive and transitive binary
relation over W, the world r (the root of K) is the minimum of W w.r.t. ≤, and
ϑ : W 
→2V (the valuation function) is a map obeying the persistence condition:
for every pair of worlds w1 and w2 of K, w1 ≤w2 implies ϑ(w1) ⊆ϑ(w2); the
triple ⟨W, ≤, r⟩is called (Kripke) frame. The valuation ϑ is extended to a forcing
relation between worlds and formulas as follows:
w ⊩p iﬀp ∈ϑ(w), ∀p ∈V
w ⊮⊥
w ⊩α ∧β iﬀw ⊩α and w ⊩β
w ⊩α ∨β iﬀw ⊩α or w ⊩β
w ⊩α →β iﬀ∀w′ ≥w, w′ ⊩α implies w′ ⊩β.
By w ⊩Γ we mean that w ⊩α for every α ∈Γ. A formula α is valid in the
frame ⟨W, ≤, r⟩iﬀfor every valuation ϑ, r ⊩α in the model ⟨W, ≤, r, ϑ⟩. Proposi-
tional Intuitionistic Logic (IPL) is the set of formulas valid in all frames. Accord-
ingly, if there is a model K such that r ⊮α (here and below r designates the root
of K), then α is not IPL-valid; we call K a countermodel for α. We write Γ ⊢i δ
iﬀ, for every model K, r ⊩Γ implies r ⊩δ; thus, α is IPL-valid iﬀ∅⊢i α.
Let L be one of the logics IPL and CPL; then, L is closed under modus
ponens ({α, α →β} ⊆L implies β ∈L) and under substitution (for every χ,
α ∈L implies χ(α) ∈L). An intermediate logic is any set of formulas L such
that IPL ⊆L ⊆CPL, L is closed under modus ponens and under substitution. A
model K is an L-model iﬀr ⊩L; if r ⊮α, we say that K is an L-countermodel for
α. An intermediate logic L can be characterized by a set of CPL-valid formulas,
called the L-axioms and denoted by Ax(L). An L-axiom ψ of Ax(L) must be
understood as a schematic formula, representing all the formulas of the kind
χ(ψ); we call χ(ψ) an instance of ψ. Formally, IPL + Ax(L) is the intermediate
logic collecting the formulas α such that Ψ ⊢i α, where Ψ is a ﬁnite set of
instances of L-axioms from Ax(L). A bounding function for L is a map that,
given α, yields a ﬁnite set Ψα of instances of L-axioms such that Ψα ⊢i α. If L
admits a computable bounding function, we can reduce L-validity to IPL-validity
(see [3] for an in-depth discussion). Let F be a class of frames and let Log(F)
be the set of formulas valid in all frames of F; then, Log(F) is an intermediate
logic. A logic L has Kripke semantics iﬀthere exists a class of frames F such
that L = Log(F); we also say that L is characterized by F. Henceforth, when we

60
C. Fiorentini and M. Ferrari
mention a logic L, we leave understood that L is an axiomatizable intermediate
logic having Kripke semantics.
Example 1 (GL). A well-known intermediate logic is G¨odel-Dummett logic
GL [2], characterized by the class of linear frames. An axiomatization of GL
is obtained by adding the linearity axiom lin = (a →b) ∨(b →a) to IPL. Using
the terminology of [3], GL is formula-axiomatizable: a bounding function for GL
is obtained by mapping α to the set Ψα of instances of lin where a and b are
replaced with subformulas of α. In [1] it is proved that it is suﬃcient to consider
the subformulas of α of the kind p ∈Vα, ¬β, β1 →β2. In Lemma 4 we further
improve this bound tacking as bounding function the following map:
AxGL(α) = { (a →b) ∨(b →a) | a, b ∈Vα } ∪{ (a →¬a) ∨(¬a →a) | a ∈Vα }
∪{ (a →(a →b)) ∨((a →b) →a)) | a, b ∈Vα }
Thus, if Vα = {a}, the only instance of lin to consider is (a →¬a)∨(¬a →a),
independently of the size of α (the other instances are IPL-valid and can be
omitted). As pointed out in [3], GL is not variable-axiomatizable, namely: it is
not suﬃcient to consider instances of lin obtained by replacing a and b with
variables from Vα. As an example, let α = ¬a ∨¬¬a; α is GL-valid, the only
variable-replacement instance of lin is ψα = (a →a) ∨(a →a) and ψα ⊬i α. ♦
We review the main concepts about the clausiﬁcation procedure described
in [4]. Clauses ϕ and implication clauses λ are deﬁned as
ϕ :=  A1 → A2 |  A2
∅⊂Ak ⊆V ∪{⊥}, fork ∈{1, 2}
λ := (a →b) →c
a ∈V, {b, c} ⊆V ∪{⊥}
where  A1 and  A2 denote the conjunction and the disjunction of the atoms
in A1 and A2 respectively ({a} = {a} = a). Henceforth,  ∅→ A2 must
be read as  A2; R, R1, . . . denote sets of clauses, X, X1, . . . sets of implication
clauses. Given a set of implication clauses X, the closure of X, denoted by (X)⋆,
is the set of clauses b →c such that (a →b) →c ∈X.
The following lemma states some properties of clauses and closures.
Lemma 1. (i) R ⊢i g iﬀR ⊢c g, for every set of clauses R and every atom g.
(ii) X ⊢i b →c, for every b →c ∈(X)⋆.
(iii) Γ ⊢i α iﬀα ↔g, Γ ⊢i g, where g ̸∈VΓ,α.
Clausiﬁcation. We assume a procedure Clausify that, given a formula α, com-
putes sets of clauses R and X equivalent to α w.r.t. IPL. Formally, let α be a
formula and let V be a set of propositional variables such that Vα ⊆V . The
procedure Clausify(α,V ) computes a triple (R, X, χ) satisfying:
(C1) Γ, α ⊢i δ iﬀΓ, R, X ⊢i δ, for every Γ and δ such that VΓ,δ ⊆V .
(C2) Dom(χ) = VR,X \ V and Vχ(p) ⊆V for every p ∈Dom(χ).
(C3) R, X ⊢i p ↔χ(p) for every p ∈Dom(χ).

SAT-Based Proof Search in Intermediate Propositional Logics
61
Fig. 1. The sequent calculus CL.
Basically, clausiﬁcation introduces new propositional variables to represent sub-
formulas of α; as a result we obtain a substitution χ which tracks the mapping
on the new variables. Condition (C1) states that α can be replaced by R ∪X in
IPL reasoning. By (C2) the domain of χ consists of the new variables introduced
in the clausiﬁcation process. The following properties easily follow by (C1)–(C3):
(P1) R, X ⊢i α.
(P2) R, X ⊢i β ↔χ(β) for every formula β.
We exploit a Clausify procedure essentially similar to the one described
in [4], with slight modiﬁcations in order to match (C3). As discussed in [4], in IPL
we can use a weaker condition (either R, X ⊢i p →χ(p) or R, X ⊢i χ(p) →p
according to the case). It is not obvious whether the weaker condition should be
more eﬃcient; in many cases strong equivalences are more performant, maybe
because they trigger more simpliﬁcations in the SAT-solver.
Example 2. Let α = (a →b)∨(b →a) and V = {a, b}. The call Clausify(α,V )
introduces the new variables ˜p0 and ˜p1 associated with the subformulas a →b
and b →a respectively. Accordingly, the obtained sets R and X must satisfy
R, X ⊢i ˜p0 ↔(a →b) and R, X ⊢i ˜p1 ↔(b →a). We get:
R = { ˜p0 ∨˜p1, ˜p0 ∧a →b, ˜p1 ∧b →a }
χ = [ ˜p0 
→a →b, ˜p1 
→b →a ]
X = { (a →b) →˜p0, (b →a) →˜p1 }
♦
3
The Calculus CL
Let L be an intermediate logic; we introduce the sequent calculus CL to prove
L-validity. We assume that L is axiomatized by a set Ax(L) of L-axioms; by

62
C. Fiorentini and M. Ferrari
. . .
. . .
Rn−1
c g
ρn = cpl0
Rn−1, Xn−1 ⇒g
ρn−1
Rn−2, Xn−2 ⇒g
...
R1, X1 ⇒g
ρ1
R0, X0 ⇒g
ρ0 = Claus0
⇒α
∀i ∈{1, . . . , n −1}, ρi = cpl1 or ρi = Claus1
π(D) =
Ψ0 ∪· · · ∪Ψn , χ0 · . . . · χn
where Ψj, χj = π(ρj)
Fig. 2. A CL-derivation of ⇒α.
Ax(L, V ) we denote the set of instances ψ of L-axioms such that Vψ ⊆V . The
calculus relies on a clausiﬁcation procedure Clausify satisfying conditions (C1)–
(C3) and acts on sequents Γ ⇒δ such that:
– either Γ = ∅or Γ = R ∪X and (X)⋆⊆R and δ is an atom.
Rules of CL are displayed in Fig. 1. Rule cpl0 (initial rule) can only be applied
if the condition R ⊢c g holds; if this is the case, the conclusion R, X ⇒g is an
initial sequent, namely a top sequent of a derivation. The other rules depend on
parameters that are made explicit in the rule name. A bottom-up application of
cpl1 requires the choice of an implication clause λ = (a →b) →c from X, we
call the main formula, and the selection of a set of atoms A ⊆VR,X,g such that
R, A ⊢c b, where b is the middle variable in λ. As discussed in [8,9], cpl1 is a
sort of generalization of the rule L →→of the sequent calculus LJT/G4ip for
IPL [5,18]. Rules Claus0 and Claus1 exploit the clausiﬁcation procedure. Rule
Claus0 requires the clausiﬁcation of the formula α ↔g, with g a new atom
(g ̸∈Vα); in rule Claus1, the clausiﬁed formula ψ is selected from Ax(L, VR,X,g).
In both cases, the clauses returned by Clausify are stored in the premise of
the applied rule and the computed substitution χ is displayed in the rule name;
moreover, Claus0 is annotated with the new atom g and Claus1 with the chosen
L-axiom ψ. To recover the relevant information associated with the application
of a rule ρ, in Fig. 1 we deﬁne the pair π(ρ) = ⟨Ψ, χ⟩, where Ψ is a set of instances
of L-axioms and χ is a substitution. CL-trees and CL-derivations are deﬁned as
usual (see e.g. [18]); a sequent σ is provable in CL iﬀthere exists a CL-derivation
having root sequent σ. Let us consider a CL-derivation D of ⇒α (see Fig. 2).
Reading the derivation bottom-up, the ﬁrst applied rule is Claus0. After such
an application, the obtained sequents have the form σk = Rk, Xk ⇒g, where
Rk ∪Xk is non-empty, thus rule Claus0 cannot be applied any more; the rule
applied at the top is cpl0. Note that D contains a unique branch, consisting of
the sequents ⇒α, σ0, . . . , σn−1. In Fig. 2 we also deﬁne the pair π(D) = ⟨Ψ, χ⟩:
Ψ collects the (instances of) L-axioms selected by rule Claus1, χ is obtained by
composing the substitutions associated with the applied rules. The deﬁnition of
π(T ), with T a CL-tree, is similar. By T (α; R, X ⇒g) we denote a CL-tree
having root
⇒α and leaf R, X ⇒g. Given a CL-tree T , VT is the set of
variables occurring in T . We state some properties about CL-trees:

SAT-Based Proof Search in Intermediate Propositional Logics
63
Lemma 2. Let T = T (α; R, X ⇒g) and let π(T ) = ⟨Ψ, χ⟩.
(i) Vχ(p) ⊆Vα, for every p ∈VT .
(ii) R, X ⊢i β ↔χ(β), for every formula β.
(iii) If R, X, Γ ⊢i g and VΓ ⊆Vα, then Γ, χ(Ψ) ⊢i α.
Proposition 1. Let D be a CL-derivation of ⇒α and let π(D) = ⟨Ψ, χ⟩. Then,
Vχ(Ψ) ⊆Vα and χ(Ψ) ⊢i α.
Proof. Since D is a CL-derivation, D has the form
depicted on the right where T
= T (α; R, X ⇒g);
note that π(T ) = π(D) = ⟨Ψ, χ⟩. Since R ⊢c g, by
Lemma 1(i) we get R
⊢i
g, hence R, X
⊢i
g. We
can apply Lemma 2 and claim that Vχ(Ψ) ⊆Vα and
χ(Ψ) ⊢i α.
⊓⊔
D =
R ⊢c g
cpl0
R, X ⇒g
... T
⇒α
Given a CL-derivation D of
⇒α, Prop. 1 exhibits how to extract a set
of instances Ψα of the L-axioms such that Ψα ⊢i α. If D does not contain
applications of rule Claus1, Ψα is empty, and this ascertains that α is IPL-valid;
actually, D can be immediately embedded into the calculus for IPL introduced
in [8]. As an immediate consequence of Prop. 1, we get the soundness of CL: if
⇒α is provable in CL, then α is L-valid.
Even though CL-derivations have a simple structure, the design of a root-
ﬁrst proof search strategy for CL is far from being trivial. After having applied
rule Claus0 to the root sequent ⇒α, we enter a loop where at each iteration
k we search for a derivation of σk = Rk, Xk ⇒g. It is convenient to ﬁrstly
check whether Rk ⊢c g so that, by applying rule cpl0, we immediately close the
derivation at hand. To check classical provability, we exploit a SAT-solver; each
time the solver is invoked, the set Rk has increased, thus it is advantageous to use
an incremental SAT-solver. If Rk ⊬c g, we have to apply either rule cpl1 or rule
Claus1, but it is not obvious which strategy should be followed. First, we have to
select one between the two rules. If rule cpl1 is chosen, we have to guess proper λ
and A; otherwise, we have to apply Claus1, and this requires the selection of an
instance ψ of an L-axiom. In any case, if we followed a blind choice, the procedure
would be highly ineﬃcient. To guide proof search, we follow a diﬀerent approach
based on countermodel construction; to this aim, we introduce a representation
of Kripke models where worlds are classical interpretations ordered by inclusion.
Countermodels. Let W be a ﬁnite set of interpretations with minimum M0,
namely: M0 ⊆M for every M ∈W. By K(W) we denote the Kripke model
⟨W, ≤, M0, ϑ⟩where ≤coincides with the subset relation ⊆and ϑ is the identity
map, thus M ⊩p (in K(W)) iﬀp ∈M. We introduce the following realizability
relation ▷W between elements of W and implication clauses:
M ▷W (a →b) →c iﬀ(a ∈M) or (b ∈M) or (c ∈M) or
( ∃M ′ ∈W s.t. M ⊂M ′ and a ∈M ′ and b ̸∈M ′ ) .

64
C. Fiorentini and M. Ferrari
By M ▷W X we mean that M ▷W λ for every λ ∈X. We state the crucial
properties of the model K(W):
Proposition 2. Let K(W) be the model generated by W and let w ∈W. Let ϕ
be a clause and λ = (a →b) →c an implication clause.
(i) If w′ |= ϕ, for every w′ ∈W such that w ≤w′, then w ⊩ϕ.
(ii) If w′ |= b →c and w′ ▷W λ, for every w′ ∈W such that w ≤w′, then w ⊩λ.
Let K(W) be a model with root r, and assume that every interpretation w in
W is a model of R; our goal is to get r ⊩R ∪X (where (X)∗⊆R), possibly by
ﬁlling W with new worlds. To this aim, we exploit Prop. 2. By our assumption
and point (i), we claim that r ⊩R. Suppose that there is w ∈W and λ =
(a →b) →c ∈X such that w⋫W λ; is it possible to amend K(W) in order to
match (ii) and conclude r ⊩X? By deﬁnition of ▷W , none of the atoms a, b, c
belongs to w; moreover K(W) lacks a world w′ such that w ⊂w′ and a ∈w′ and
b ̸∈w′. We can try to ﬁx K(W) by inserting the missing world w′; to preserve (i),
we also need w′ |= R. Accordingly, such a w′ exists if and only if R, w, a ⊬c b.
This can be checked by querying a SAT-solver; moreover, if R, w, a ⊬c b, the
solver also computes the required w′. This completion process must be iterated
until K(W) has been saturated with all the missing worlds or we get stuck. It
is easy to check that the process eventually terminates. This is one of the key
ideas beyond the procedure intuitRIL we present in next section.
4
The Procedure intuitRIL
We present the procedure intuitRIL (intuit with Restart for Intermediate
Logics) that, given a formula α and a logic L = IPL + Ax(L), returns either a
set of L-axioms Ψα or a model K(W) with the following properties:
(Q1) If intuitRIL(α,L) returns Ψα, then Ψα ⊆Ax(L, Vα) and Ψα ⊢i α.
(Q2) If intuitRIL(α,L) returns K(W), then K(W) is an L-countermodel for α.
Thus, α is L-valid in the former case, not L-valid in the latter. If intuitRIL(α,L)
returns Ψα, by tracing the computation we can build a CL-derivation D of ⇒α
such that Ψα = χ(Ψ), where ⟨Ψ, χ⟩= π(D); this certiﬁcates that Ψα ⊢i α.
The procedure is described by the ﬂowchart in Fig. 3 and exploits a single
incremental SAT-solver s: clauses can be added to s but not removed; by R(s)
we denote the set of clauses stored in s. The SAT-solver is required to support
the following operations:
– newSolver(R) creates a new SAT-solver initialized with the clauses in R.
– addClauses(s, R) adds the clauses in R to the SAT-solver s.
– satProve(s, A, g) calls s to decide whether R(s), A ⊢c g (A is a set of
propositional variables). The solver outputs one of the following answers:
• Yes(A′): thus, A′ ⊆A and R(s), A′ ⊢c g;
• No(M): thus, A ⊆M ⊆VR(s) ∪A and M |= R(s) and g ̸∈M.
In the former case it follows that R(s), A ⊢c g, in the latter R(s), A ⊬c g.

SAT-Based Proof Search in Intermediate Propositional Logics
65
Fig. 3. Computation of intuitRIL(α, L).
The computation of intuitRIL(α,L) consists of the following steps:
(S0) The formula α ↔g, with g new propositional variable, is clausiﬁed. The
outcome (R′, X′, χ′) is used to create a new SAT-solver s and to prop-
erly initialize the global variables X (set of implication clauses), Ψ (set of
L-axiom instances), V (set of propositional variables) and χ (substitution).
(S1) A loop starts (main loop). The SAT-solver s is called to check whether
R(s)
⊢c
g. If the answer is Yes(∅), the computation stops yielding
χ(Ψ). Otherwise, the output is No(M) and the computation continues at
Step (S2).
(S2) We set r = M (the root of K(W)) and W = {r}.
(S3) A loop starts (inner loop). We have to select a pair ⟨w, λ⟩such that w ∈W,
λ ∈X and w⋫W λ. If such a pair does not exist, the inner loop ends and
next step is (S4), otherwise the inner loop continues at Step (S6).
(S4) As we show in Lemma 3, at this point K(W) is a countermodel for α. If
all the axioms in Ax(L, V ) are forced at the root r of K(W), then K(W)
is an L-countermodel for α and the computation ends returning K(W).
Otherwise, we select ψ from Ax(L, V ) such that r ⊮ψ and the computation
continues at Step (S5); we call ψ the learned axiom.

66
C. Fiorentini and M. Ferrari
(S5) We clausify ψ and we update the global variables. The computation restarts
from Step (S1) with a new iteration of the main loop (semantic restart).
(S6) Let ⟨w, (a →b) →c⟩be the pair selected at Step (S3). The SAT-solver s is
called to check whether R(s), w, a ⊢c b. If the result is No(M), the inner
loop continues at step (S7). Otherwise, the answer is Yes(A); the inner
loop ends and the computation continues at Step (S8).
(S7) The interpretation M is added to W and the computation continues at
Step (S3) with a new iteration of the inner loop.
(S8) The clause ϕ (learned basic clause) is added to the SAT-solver s and the
computation restarts from Step (S1) (basic restart).
Intuitively, intuitRIL(α,L) searches for an L-countermodel K(W) for α. In the
construction of K(W), whenever a conﬂict arises, a restart operation is triggered.
A basic restart happens when it is not possible to ﬁll the set W with a missing
world (see the discussion after Prop. 2). A semantic restart is thrown when
K(W) is a countermodel for α but it fails to be an L-model. In either case, the
construction of K(W) restarts from scratch. However, to prevent that the same
kind of conﬂict shows up again, new clauses are learned and fed to the SAT-solver
(this complies with DPLL(T ) with learning computation paradigm [16]). If the
outcome is χ(Ψ), by tracing the computation we can build a CL-derivation D
of ⇒α such that π(D) = ⟨Ψ, χ⟩. The derivation is built bottom-up. The initial
Step (S0) corresponds to the application of rule Claus0 to the root sequent ⇒α;
basic and semantic restarts bottom-up expand the derivation by applying rule
cpl1 and Claus1 respectively. We stress that the procedure is quite modular; to
treat a speciﬁc logic L one has only to provide a concrete implementation of
Step (S4). For L = IPL, Step (S4) is trivial, since the set Ax(IPL, V ) is empty.
Actually, intuitRIL applied to IPL has the same behaviour as the procedure
intuitR introduced in [8].
Example 3. Let us consider Jankov axiom wem = ¬a ∨¬¬a [2,13] (aka weak
excluded middle), which holds in all frames having a single maximal world (thus,
wem is GL-valid). The trace of the execution of intuitRIL(wem,GL) is shown
in Fig. 4. The initial clausiﬁcation yields (R0, X0, ˜g), where X0 consists of the
implication clauses λ0, λ1 in Fig. 4 and R0 contains the 7 clauses below:
˜g →˜p2,
˜p0 →˜p2,
a ∧˜p0 →⊥,
˜p1 →˜p2,
˜p0 ∧˜p1 →⊥,
˜p2 →˜g,
˜p2 →˜p0 ∨˜p1.
Each row in Fig. 4 displays the validity tests performed by the SAT-solver
and the computed answers. If the result is No(M), the last two columns show
the worlds wk in the current set W and, for each wk, the list of λ such that
w⋫W λ; the pair selected for the next step is underlined. For instance, after
call (1) we have W = {w0}, w0⋫W λ0 and w0⋫W λ1; the selected pair is ⟨w0, λ0⟩.
After call (2), the set W is updated by adding the world w1; we have w1 ▷W λ0,
w1 ▷W λ1, w0 ▷W λ0 and w0⋫W λ1. Whenever the SAT-solver outputs Yes(A),
we display the learned clause ψk. The SAT-solver is invoked 18 times and there
are 6 restarts (1 semantic, 5 basic). After (3), we get W = {w0, w1, w2} and no
pair ⟨w, λ⟩can be selected, hence the model K(W) (displayed in the ﬁgure) is

SAT-Based Proof Search in Intermediate Propositional Logics
67
a countermodel for wem. However, K(W) is not a GL-model (indeed, it is not
linear), hence we choose an instance of the linearity axiom not forced at w0,
namely ψ0, and we force a semantic restart. The clausiﬁcation of ψ0 produces 6
new clauses and the new implication clauses λ2, λ3, λ4. After each restart, the
sets Rj are:
R1 = R0 ∪{ ˜p3 →˜p4, a →˜p5, ˜p3 ∧˜p5 →a, a ∧˜p4 →˜p3, a ∧˜p3 →⊥, ˜p4 ∨˜p5 }
Rj = Rj−1 ∪{ψj−1}
for 2 ≤j ≤6 (the ψ′
js are deﬁned in Fig. 4).
The CGL-derivation of ⇒¬a ∨¬¬a extracted from the computation is:
R1, a, ˜p0 ⊢c ⊥
R2, a, ˜p0 ⊢c ⊥
R3, a, ˜p3 ⊢c ⊥
R4, ˜p0, ˜p5 ⊢c ⊥
R5, a, ˜p4 ⊢c ⊥
R6 ⊢c ˜g
cpl0
R6, X1 ⇒˜g
cpl1(λ1)
R5, X1 ⇒˜g
cpl1(λ0)
R4, X1 ⇒˜g
cpl1(λ1)
R3, X1 ⇒˜g
cpl1(λ0)
R2, X1 ⇒˜g
cpl1(λ3)
R1, X1 ⇒˜g
Claus1(ψ0, χ1)
R0, X0 ⇒˜g
Claus0(˜g, χ0)
⇒¬a ∨¬¬a
♦
Now, we discuss partial correctness and termination of intuitRIL. Let us
denote with ∼c classical equivalence (α ∼c β iﬀ
⊢c α ↔β) and with ∼i
intuitionistic equivalence (α ∼i β iﬀ⊢i α ↔β). We introduce some notation.
(†) The following terms refer to the conﬁguration at the beginning of iteration
k (k ≥0), just after the execution of Step (S2):
– Φk is the set collecting all the learned basic clauses;
– Rk is the set of clauses stored in the SAT-solver s;
– Xk, Ψk, Vk, χk, rk are the values of the corresponding global variables.
In Fig. 5 we inductively deﬁne the CL-tree Tk, having the form T (α; Rk, Xk ⇒g).
In the application of rule Claus0, g and χ′ are deﬁned as in Step (S0). In rule
cpl1, λ is the implication clause selected at iteration k −1 (of the main loop)
in the last execution of Step (S3); A is the value computed at Step (S6) of
iteration k −1. In the application of rule Claus1, ψ and χ′ are deﬁned as in the
execution of Step (S4) and (S5) of iteration k −1. One can easily check that the
applications of the rules are sound. If Step (S1) yields Yes(∅), we can turn Tk
into a CL-derivation by applying rule cpl0.
Next lemma states some relevant properties of the computations of
intuitRIL.
Lemma 3. Let us consider the execution of iteration k of the main loop (k ≥0).
(i) (Xk)⋆∪Φk ⊆Rk.
(ii) Vk = VTk and Ψk ⊆Ax(L, Vk) and π(Tk) = ⟨Ψk, χk⟩.
(iii) Vχk(p) ⊆Vα, for every p ∈Vk, and Rk, Xk ⊢i β ↔χk(β), for every β.

68
C. Fiorentini and M. Ferrari
@SAT
Answer
W
λ s.t. w
W λ
Start
(1) R0
c ˜g ?
No(w0)
w0
λ0, λ1
(2) R0, w0, ˜p0
c ⊥?
No(w1)
w1
∅
w0
λ1
(3) R0, w0, a
c ⊥?
No(w2)
w2
∅
w1
∅
w0
∅
Semantic
failure
w0 : ∅
w1 : ˜g, ˜p0, ˜p2
w2 : a, ˜g, ˜p1, ˜p2
Learned axiom:
ψ0 = (a →¬a) ∨(¬a →a)
SRest 1
(4) R1
c ˜g ?
No(w3)
w3
λ0, λ1, λ3, λ4
(5) R1, w3, ˜p0
c ⊥?
No(w4)
w4
λ3, λ4
w3
λ1, λ3, λ4
(6) R1, w4, ˜p3
c a ?
No(w5)
w5
∅
w4
λ3
w3
λ1, λ3
(7) R1, w4, a
c ⊥?
Yes( { a, ˜p0 } )
ψ1 = ˜p0 →˜p3
BRest 2
(8) R2
c ˜g ?
No(w6)
w6
λ0
(9) R2, w6, ˜p0
c ⊥?
Yes( { a, ˜p0 } )
ψ2 = a →˜p1
BRest 3
(10) R3
c ˜g ?
No(w7)
w7
λ0, λ1
(11) R3, w7, ˜p0
c ⊥?
No(w8)
w8
∅
w7
λ1
(12) R3, w7, a
c ⊥?
Yes( { a, ˜p3 } )
ψ3 = ˜p3 →˜p0
BRest 4
(13) R4
c ˜g ?
No(w9)
w9
λ0, λ1, λ2, λ3
(14) R4, w9, ˜p0
c ⊥?
Yes( { ˜p0, ˜p5 } )
ψ4 = ˜p5 →˜p1
BRest 5
(15) R5
c ˜g ?
No(w10)
w10
λ0, λ1, λ3, λ4
(16) R5, w10, ˜p0
c ⊥?
No(w11)
w11
∅
w10
λ1, λ3
(17) R5, w10, a
c ⊥?
Yes( { a, ˜p4 } )
ψ5 = ˜p4 →˜p0
BRest 6
(18) R6
c ˜g ?
Yes( ∅)
Proved
λ0 = (˜p0 →⊥) →˜p1
λ1 = (a →⊥) →˜p0
λ2 = (a →˜p3) →˜p4
λ3 = (a →⊥) →˜p3
λ4 = (˜p3 →a) →˜p5
w0 = ∅
w1 = {˜g, ˜p0, ˜p2}
w2 = {a, ˜g, ˜p1, ˜p2}
w3 = {˜p4}
w4 = {˜g, ˜p0, ˜p2, ˜p4}
w5 = {˜g, ˜p0, ˜p2, ˜p3, ˜p4}
w6 = {a, ˜p5}
w7 = {˜p3, ˜p4}
w8 = {˜g, ˜p0, ˜p2, ˜p3, ˜p4}
w9 = {˜p5}
w10 = {˜p4}
w11 = {˜g, ˜p0, ˜p2, ˜p3, ˜p4}
χ0 = [˜g
a ∨¬¬a, ˜p0
a, ˜p1
a, ˜p2
a ∨¬¬a]
χ1 = [˜p3
a, ˜p4
a →¬a, ˜p5
a →a]
Fig. 4. Computation of intuitRIL(¬a ∨¬¬a, GL).

SAT-Based Proof Search in Intermediate Propositional Logics
69
Fig. 5. Deﬁnition of Tk (k ≥0).
(iv) At every step after (S2), w |= Rk, for every w ∈W.
(v) At every step after (S2), rk is the root of K(W) and rk ⊩Rk and rk ⊮g.
(vi) At Step (S4), rk ⊩Rk ∪Xk ∪Ψk and rk ⊮g (in K(W)).
(vii) Assume that iteration k ends with a basic restart and let ϕ be the learned
basic clause. For every ϕ′ ∈Φk, ϕ ̸∼c ϕ′.
(viii) Assume that iteration k ends with a semantic restart and let ψ be the
learned axiom. For every ψ′ ∈Ψk, χk(ψ) ̸∼i χk(ψ′).
Proof. We only sketch the proof of the non-trivial points.
(iii). By Lemma 2 applied to Tk.
(v). Every interpretation M generated at Step (S6) is a superset of rk, thus
after Step (S2) rk is the minimum element of W and the root of K(W). By (iv)
and Prop. 2(i), rk ⊩Rk. Since g ̸∈rk, we get rk ⊮g.
(vi). At Step (S4), w ▷W λ for every w ∈W and λ ∈Xk. Since (Xk)⋆⊆Rk,
by Prop. 2(ii) we get rk ⊩Xk. Let ψ ∈Ψk; then, ψ has been learned at some
iteration k′ < k. Let (R′, X′, χ′) be the output of Clausify(ψ,V ) at Step (S5)
of iteration k′ . Since R′ ⊆Rk and X′ ⊆Xk, it holds that rk ⊩R′ ∪X′. By (P1)
R′, X′ ⊢i ψ, hence rk ⊩ψ, which proves rk ⊩Ψk.
(vii). Let ϕ′ ∈Φk; we show that ϕ ̸∼c ϕ′. Let ϕ = (A \ {a}) →c; then,
there are w ∈W and λ = (a →b) →c ∈Xk such that ⟨w, λ⟩has been selected
at Step (S3) and the outcome of satProve(s,w ∪{a},b) at Step (S6) is Yes(A).
Note that w⋫W λ, hence c ̸∈w; since A ⊆w ∪{a}, we get w ̸|= ϕ. On the other
hand, w |= ϕ′, since ϕ′ ∈Φk and Φk ⊆Rk. We conclude ϕ ̸∼c ϕ′.
(viii). Let ψ′ ∈Ψk and let K(W) be the model obtained at Step (S4) of
iteration k. By (iii) Rk, Xk ⊢i ψ ↔χk(ψ) and Rk, Xk ⊢i ψ′ ↔χk(ψ′). Since
rk ⊮ψ and rk ⊩ψ′ (indeed, ψ′ ∈Ψk and rk ⊩Ψk) and rk ⊩Rk ∪Xk, we get
rk ⊮χk(ψ) and rk ⊩χk(ψ′). We conclude χk(ψ) ̸∼i χk(ψ′).
⊓⊔
The following proposition proves the partial correctness of intuitRIL:
Proposition 3. intuitRIL(α,L) satisﬁes properties (Q1) and (Q2).
Proof. Let us assume that the computation ends at iteration k with output
Ψα. Then, the call to the SAT-solver at Step (S0) yields Yes(∅), meaning that
Rk ⊢c g. We can build the following CL-derivation D of ⇒α:

70
C. Fiorentini and M. Ferrari
D =
Rk ⊢c g
cpl0
Rk, Xk ⇒g
... Tk
⇒α
π(D) = π(Tk) = ⟨Ψk, χk⟩
Note that Ψα = χk(Ψk). Accordingly, by Prop. 1 we get (Q1).
Let us assume that the output is the model K(W), having root r. Then, K(W)
is an L-model (otherwise, Step (S4) should have forced a semantic restart). By
Lemma 3(vi) we get r ⊩R0 ∪X0 and r ⊮g. Since at Step (S0) we have clausiﬁed
the formula α ↔g, by (P1) we get R0, X0 ⊢i α ↔g, which implies r ⊩α ↔g.
We conclude that r ⊮α, hence (Q2) holds.
⊓⊔
It seems challenging to provide a general proof of termination, and each logic
must be treated apart. We can only state some general properties about the
termination of the inner loop and of consecutive basic restarts.
Proposition 4. (i) The inner loop is terminating.
(ii) The number of consecutive basic restarts is ﬁnite.
Proof. Let us assume, by absurd, that the inner loop is not terminating. For
every j ≥0, by Wj we denote the value of W at Step (S3) of iteration j of
the inner loop; note that the value of the variable V does not change during the
iterations. We show that Wj ⊂Wj+1, for every j ≥0. At iteration j, the outcome
of Step (S6) is No(M). Thus, there are w ∈Wj and λ = (a →b) →c ∈X such
that the pair ⟨w, λ⟩has been selected at Step (S3); accordingly, w⋫Wjλ and
w ∪{a} ⊆M and b ̸∈M. We have M ̸∈Wj, otherwise we would get w ▷Wj λ, a
contradiction. Since Wj+1 = Wj ∪{M}, this proves that Wj ⊂Wj+1. We have
shown that W0 ⊂W1 ⊂W2 . . . . This leads to a contradiction since, for every
j ≥0 and every w ∈Wj, w is a subset of V and V is ﬁnite. We conclude that
the inner loop is terminating, and this proves (i).
Let us assume, by contradiction, that there is an inﬁnite sequence of consec-
utive basic restarts. Then, there is n ≥0 such that, for every k ≥n, the iteration
k of the main loop ends with a basic restart. Let ϕk be the clause learned at
iteration k. Note that an iteration ending with a basic restart does not introduce
new atoms, thus Vϕk ⊆Vn for every k ≥n (where Vn is deﬁned as in (†)). We
get a contradiction, since Vn is ﬁnite and, by Lemma 3(vi), the clauses ϕk are
pairwise non ∼c-equivalent; this proves (ii).
⊓⊔
Lemma 3(vii) guarantees that the learned axioms are pairwise distinct, but this
is not suﬃcient to prove termination since in general we cannot set a bound on
the size and on the number of learned axioms. In next section we present some
relevant logics where the procedure is terminating.

SAT-Based Proof Search in Intermediate Propositional Logics
71
5
Termination
Let GL = IPL + lin be the G¨odel-Dummett logic presented in Ex. 1; we show
that every call intuitRIL(α,GL) is terminating. To this aim, we exploit the
bounding function AxGL(α) presented in the mentioned example.
Lemma 4. Let us consider the computation of intuitRIL(α,GL) and assume
that at iteration k of the main loop Step (S4) is executed and that the obtained
model K(W) is not linear. Then, there exists ψ ∈AxGL(α) such that rk ⊮ψ.
Proof. Let us assume that K(W) has two distinct maximal worlds w1 and w2;
note that w1 ⊆Vk and w2 ⊆Vk (with Vk deﬁned as in (†)). We show that:
(a) w1 ∩Vα ̸= w2 ∩Vα.
Suppose by contradiction w1 ∩Vα = w2 ∩Vα; let p ∈Vk and β = χk(p) (with
χk deﬁned as in (†)). By Lemma 3(iii), Rk, Xk ⊢i p ↔β; by Lemma 3(vi)
we get w1 ⊩p ↔β and w2 ⊩p ↔β. Since Vβ ⊆Vα (see Lemma 3(iii)) and
we are assuming w1 ∩Vα = w2 ∩Vα, it holds that w1 ⊩β iﬀw2 ⊩β, thus
w1 ⊩p iﬀw2 ⊩p, namely p ∈w1 iﬀp ∈w2. Since p is any element of Vk, we
get w1 = w2, a contradiction; this proves (a). By (a) there is a ∈Vα such that
either a ∈w1 \ w2 or a ∈w2 \ w1. We consider the former case (the latter one
is symmetric), corresponding to Case 1 in Fig. 6. We have w1 ⊩a and w2 ⊩¬a;
setting ψ = (a →¬a) ∨(¬a →a), we conclude rk ⊮ψ.
Assume that K(W) has only one maximal world; since it is not linear, there
are three distinct worlds w1, w2, w3 as in Case 2 in Fig. 6, namely: w1 is an
immediate successor of w2 and w3 (i.e., for j ∈{2, 3}, wj < w1 and, if wj < w,
then w1 ≤w), w2 ̸≤w3, w3 ̸≤w2. Reasoning as in (a), we get:
(b) w2 ∩Vα ̸= w3 ∩Vα.
(c) w2 ∩Vα ⊂w1 ∩Vα and w3 ∩Vα ⊂w1 ∩Vα.
By (b) there is a ∈Vα such that either a ∈w2 \ w3 or a ∈w3 \ w2. Let us
consider the former case (the latter one is symmetric). By (c), there is b ∈Vα
such that b ∈w1 \ w2. If b ∈w3 (Case 2.1 in Fig. 6), we get a ∈w2, b ̸∈w2,
a ̸∈w3, b ∈w3. Setting ψ = (a →b) ∨(b →a), we conclude rk ⊮ψ. Finally,
let us assume b ̸∈w3 (Case 2.2). We have {a, b} ⊆w1, a ∈w2, b ̸∈w2, a ̸∈w3
and b ̸∈w3. It is easy to check that w3 ⊩a →b (recall that w3 < w implies
w1 ≤w), thus w3 ⊮(a →b) →a. On the other hand w2 ⊮a →(a →b). Setting
ψ = (a →(a →b)) ∨((a →b) →a), we get rk ⊮ψ.
⊓⊔
We exploit Lemma 4 to implement Step (S4). If K(W) is linear, then K(W) is a
GL-model and we are done. Otherwise, the proof of Lemma 4 hints an eﬀective
method to select an instance ψ of lin from AxGL(α).
Proposition 5. The computation of intuitRIL(α,GL) is terminating.
Proof. Assume that intuitRIL(α,GL) is not terminating. Since the number of
iterations of the inner loop and of the consecutive basic restarts is ﬁnite (see
Prop. 4), Step (S4) must be executed inﬁnitely many times. This leads to a
contradiction, since the axioms selected at Step (S4) are pairwise distinct (see
Lemma 3(vii)) and such axioms are chosen from the ﬁnite set AxGL(α).
⊓⊔

72
C. Fiorentini and M. Ferrari
Fig. 6. Proof of Lemma 4, case analysis.
As a corollary, we get that AxGL(α) is a bounding function for GL:
Proposition 6. If α is GL-valid, there is Ψα ⊆AxGL(α) such that Ψα ⊢i α.
Other proof-search strategies for GL are discussed in [10,14]. This technique
can be extended to other notable intermediate logics. Among these, we recall
the logics GLn (G¨odel Logic of depth n), obtained by adding to GL the axioms
bdn (bounded depth) where: bd0 = a0 ∨¬a0, bdn+1 = an+1 ∨(an+1 →bdn).
Semantically, GLn is the logic characterized by linear frames having depth at
most n. We are not able to prove termination for the logics IPL + bdn, but we
can implement the following terminating strategy for GLn. Let K(W) be the
model obtained at Step (S4) of the computation of intuitRIL(α,GLn):
– If K(W) is not linear, we select the axiom ψ from AxGL(α).
– Otherwise, assume that K(W) is linear but not a GLn-model. Then, K(W)
contains a chain of worlds w0 ⊂w1 ⊂· · · ⊂wn+1. The crucial point is
that wj+1 \ wj contains at least a propositional variable from Vα, for every
0 ≤j ≤n. Thus, we can choose a proper renaming of bdn as ψ.
Another terminating logic is the Jankov Logic (see Ex. 3); actually, also in this
case the learned axiom can be chosen by renaming the wem axiom. In general,
all the logics BTWn (Bounded Top Width, at most n maximal worlds, see [2])
are terminating. An intriguing case is Scott Logic ST [2]: even though the class
of ST-frames is not ﬁrst-order deﬁnable, we can implement a learning procedure
for ST-axioms arguing as in [7] (see Sec. 2.5.2). Some of the mentioned logics
have been implemented in intuitRIL1.
One may wonder whether this method can be applied to other non-classical
logics or to fragments of predicate logics (these issues have been already raised
in the seminal paper [4]). A signiﬁcant work in this direction is [11], where the
procedure has been applied to some modal logics. However, the main diﬀerence
with the original approach is that it is not possible to use a single SAT-solver,
but one needs a supply of SAT-solvers. This is primarily due to the fact that
forcing relation of modal Kripke models is not persistent; thus worlds are loosely
related and must be handled by independent solvers.
1 Available at https://github.com/cﬁorentini/intuitRIL.

SAT-Based Proof Search in Intermediate Propositional Logics
73
References
1. Avellone, A., Moscato, U., Miglioli, P., Ornaghi, M.: Generalized tableau systems
for intermediate propositional logics. In: Galmiche, D. (ed.) TABLEAUX 1997.
LNCS, vol. 1227, pp. 43–61. Springer, Heidelberg (1997). https://doi.org/10.1007/
BFb0027404
2. Chagrov, A.V., Zakharyaschev, M.: Modal Logic, Oxford Logic Guides, vol. 35.
Oxford University Press (1997)
3. Ciabattoni, A., Lang, T., Ramanayake, R.: Bounded-analytic sequent calculi and
embeddings for hypersequent logics. J. Symb. Log. 86(2), 635–668 (2021)
4. Claessen, K., Ros´en, D.: SAT modulo intuitionistic implications. In: Davis, M.,
Fehnker, A., McIver, A., Voronkov, A. (eds.) LPAR 2015. LNCS, vol. 9450, pp.
622–637. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48899-
7 43
5. Dyckhoﬀ, R.: Contraction-free sequent calculi for intuitionistic logic. J. Symb. Log.
57(3), 795–807 (1992)
6. Ferrari, M., Fiorentini, C., Fiorino, G.: fCube: an eﬃcient prover for intuitionistic
propositional logic. In: Ferm¨uller, C.G., Voronkov, A. (eds.) LPAR 2010. LNCS,
vol. 6397, pp. 294–301. Springer, Heidelberg (2010). https://doi.org/10.1007/978-
3-642-16242-8 21
7. Fiorentini, C.: Kripke completeness for intermediate logics. Ph.D. thesis, Universit`a
degli Studi di Milano (2000)
8. Fiorentini, C.: Eﬃcient SAT-based proof search in intuitionistic propositional logic.
In: Platzer, A., Sutcliﬀe, G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 217–
233. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-79876-5 13
9. Fiorentini, C., Gor´e, R., Graham-Lengrand, S.: A proof-theoretic perspective on
SMT-solving for intuitionistic propositional logic. In: Cerrito, S., Popescu, A. (eds.)
TABLEAUX 2019. LNCS (LNAI), vol. 11714, pp. 111–129. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-29026-9 7
10. Fiorino, G.: Terminating calculi for propositional dummett logic with subformula
property. J. Autom. Reason. 52(1), 67–97 (2013). https://doi.org/10.1007/s10817-
013-9276-7
11. Gor´e, R., Kikkert, C.: CEGAR-tableaux: improved modal satisﬁability via modal
clause-learning and SAT. In: Das, A., Negri, S. (eds.) TABLEAUX 2021. LNCS
(LNAI), vol. 12842, pp. 74–91. Springer, Cham (2021). https://doi.org/10.1007/
978-3-030-86059-2 5
12. Gor´e, R., Thomson, J., Wu, J.: A history-based theorem prover for intuitionistic
propositional logic using global caching: IntHistGC system description. In: Demri,
S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562, pp.
262–268. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6 19
13. Jankov, V.: The calculus of the weak “law of excluded middle.”. Math. USSR 8,
648–650 (1968)
14. Larchey-Wendling, D.: G¨odel-dummett counter-models through matrix computa-
tion. Electron. Notes Theory Comput. Sci. 125(3), 137–148 (2005)
15. Lifschitz, V., Pearce, D., Valverde, A.: Strongly equivalent logic programs. ACM
Trans. Comput. Log. 2(4), 526–541 (2001)
16. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories:
from an abstract Davis-Putnam-Logemann-Loveland procedure to DPLL(T). J.
ACM 53(6), 937–977 (2006)

74
C. Fiorentini and M. Ferrari
17. Schmidt, R.A., Tishkovsky, D.: Automated synthesis of tableau calculi. Log. Meth-
ods Comput. Sci. 7(2) (2011)
18. Troelstra, A.S., Schwichtenberg, H.: Basic Proof Theory, Cambridge Tracts in The-
oretical Computer Science, vol. 43, 2nd edn. Cambridge University Press, Cam-
bridge (2000)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Clause Redundancy and Preprocessing
in Maximum Satisﬁability
Hannes Ihalainen, Jeremias Berg(B)
, and Matti J¨arvisalo
HIIT, Department of Computer Science, University of Helsinki, Helsinki, Finland
hannes.ihalainen@helsinki.fi, jeremias.berg@helsinki.fi,
matti.jarvisalo@helsinki.fi
Abstract. The study of clause redundancy in Boolean satisﬁability
(SAT) has proven signiﬁcant in various terms, from fundamental insights
into preprocessing and inprocessing to the development of practical proof
checkers and new types of strong proof systems. We study liftings of
the recently-proposed notion of propagation redundancy—based on a
semantic implication relationship between formulas—in the context of
maximum satisﬁability (MaxSAT), where of interest are reasoning tech-
niques that preserve optimal cost (in contrast to preserving satisﬁability
in the realm of SAT). We establish that the strongest MaxSAT-lifting of
propagation redundancy allows for changing in a controlled way the set
of minimal correction sets in MaxSAT. This ability is key in succinctly
expressing MaxSAT reasoning techniques and allows for obtaining cor-
rectness proofs in a uniform way for MaxSAT reasoning techniques very
generally. Bridging theory to practice, we also provide a new MaxSAT
preprocessor incorporating such extended techniques, and show through
experiments its wide applicability in improving the performance of mod-
ern MaxSAT solvers.
Keywords: Maximum satisﬁability · Clause redundancy ·
Propagation redundancy · Preprocessing
1
Introduction
Building heavily on the success of Boolean satisﬁability (SAT) solving [13], max-
imum satisﬁability (MaxSAT) as the optimization extension of SAT constitutes
a viable approach to solving real-world NP-hard optimization problems [6,35].
In the context of SAT, the study of fundamental aspects of clause redundancy
[20,21,23,28,29,31,32] has proven central for developing novel types of prepro-
cessing and inprocessing-style solving techniques [24,29] as well as in enabling
eﬃcient proof checkers [7,15,16,18,19,41,42] via succinct representation of most
practical SAT solving techniques. Furthermore, clause redundancy notions have
Work ﬁnancially supported by Academy of Finland under grants 322869, 328718 and
342145. The authors wish to thank the Finnish Computing Competence Infrastructure
(FCCI) for supporting this project with computational and data storage resources.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 75–94, 2022.
https://doi.org/10.1007/978-3-031-10769-6_6

76
H. Ihalainen et al.
been shown to give rise to very powerful proof systems, going far beyond res-
olution [22,23,30]. In contrast to viewing clause redundancy through the lens
of logical entailment, the redundancy criteria developed in this line of work are
based on a semantic implication relationship between formulas, making them
desirably eﬃcient to decide and at the same time are guaranteed to merely pre-
serve satisﬁability rather than logical equivalence.
The focus of this work is the study of clause redundancy in the context of
MaxSAT through lifting recently-proposed variants of the notion of propagation
redundancy [23] based on a semantic implication relationship between formulas
from the realm of SAT. The study of such liftings is motivated from several per-
spectives. Firstly, earlier it has been shown that a natural MaxSAT-lifting called
SRAT [10] of the redundancy notion of the notion of resolution asymmetric tau-
tologies (RAT) [29] allows for establishing the general correctness of MaxSAT-
liftings of typical preprocessing techniques in SAT solving [14], alleviating the
need for correctness proofs for individual preprocessing techniques [8]. However,
the need for preserving the optimal cost in MaxSAT—as a natural counterpart
for preserving satisﬁability in SAT—allows for developing MaxSAT-centric pre-
processing and solving techniques which cannot be expressed through SRAT
[2,11]. Capturing more generally such cost-aware techniques requires developing
more expressive notions of clause redundancy. Secondly, due to the fundamental
connections between solutions and so-called minimal corrections sets (MCSes) of
MaxSAT instances [8,25], analyzing the eﬀect of clauses that are redundant in
terms of expressive notions of redundancy on the MCSes of MaxSAT instances
can provide further understanding on the relationship between the diﬀerent
notions and their fundamental impact on the solutions of MaxSAT instances.
Furthermore, in analogy with SAT, more expressive redundancy notions may
prove fruitful for developing further practical preprocessing and solving tech-
niques for MaxSAT.
Our main contributions are the following. We propose natural liftings of the
three recently-proposed variants PR, LPR and SPR of propagation redundancy
in the context of SAT to MaxSAT. We provide a complete characterization of the
relative expressiveness of the lifted notions CPR, CLPR and CSPR (C standing
for cost for short) and of their impact on the set of MCSes in MaxSAT instances.
In particular, while removing or adding clauses redundant in terms of CSPR and
CLPR (the latter shown to be equivalent with SRAT) do not inﬂuence the set
of MCSes underlying MaxSAT instances, CPR can in fact have an inﬂuence on
MCSes. In terms of solutions, this result implies that CSPR or CLPR clauses
can not remove minimal (in terms of sum-of-weights of falsiﬁed soft clauses)
solutions of MaxSAT instances, while CPR clauses can.
The—theoretically greater—eﬀect that CPR clauses have on the solutions
of MaxSAT instances is key for succinctly expressing further MaxSAT reason-
ing techniques via CPR and allows for obtaining correctness proofs in a uniform
way for MaxSAT reasoning techniques very generally; we give concrete examples
of how CPR captures techniques not in the reach of SRAT. Bridging to prac-
tical preprocessing in MaxSAT, we also provide a new MaxSAT preprocessor

Clause Redundancy and Preprocessing in Maximum Satisﬁability
77
extended with such techniques. Finally, we provide large-scale empirical evidence
on the positive impact of the preprocessor on the runtimes of various modern
MaxSAT solvers, covering both complete and incomplete approaches, suggesting
that extensive preprocessing going beyond the scope of SRAT appears beneﬁcial
to integrate for speeding up modern MaxSAT solvers.
An extended version of this paper, with formal proofs missing from this
version, is available via the authors’ homepages.
2
Preliminaries
SAT. For a Boolean variable x there are two literals, the positive x and the
negative ¬x, with ¬¬l = l for a literal l. A clause C is a set (disjunction) of
literals and a CNF formula F a set (conjunction) of clauses. We assume that all
clauses are non-tautological, i.e., do not contain both a literal and its negation.
The set var(C) = {x | x ∈C or ¬x ∈C} consists of the variables of the
literals in C. The set of variables and literals, respectively, of a formula are
var(F) = 
C∈F var(C) and lit(F) = 
C∈F C, respectively. For a set L of
literals, the set ¬L = {¬l | l ∈L} consists of the negations of the literals in L.
A (truth) assignment τ is a set of literals for which x /∈τ or ¬x /∈τ for any
variable x. For a literal l we denote l ∈τ by τ(l) = 1 and ¬l ∈τ by τ(l) = 0 or
τ(¬l) = 1 as convenient, and say that τ assigns l the value 1 and 0, respectively.
The set var(τ) = {x | x ∈τ or ¬x ∈τ} is the range of τ, i.e., it consists of the
variables τ assigns a value for. For a set L of literals and an assignment τ, the
assignment τL = (τ \ ¬L) ∪L is obtained from τ by setting τL(l) = 1 for all
l ∈L and τL(l) = τ(l) for all l /∈L assigned by τ. For a literal l, τl stands for
τ{l}. An assignment τ satisﬁes a clause C (τ(C) = 1) if τ ∩C ̸= ∅or equivalently
if τ(l) = 1 for some l ∈C, and a CNF formula F (τ(F) = 1) if it satisﬁes each
clause C ∈F. A CNF formula is satisﬁable if there is an assignment that satisﬁes
it, and otherwise unsatisﬁable. The empty formula ⊤is satisﬁed by any truth
assignment and the empty clause ⊥is unsatisﬁable. The Boolean satisﬁability
problem (SAT) asks to decide whether a given CNF formula F is satisﬁable.
Given two CNF formulas F1 and F2, F1 entails F2 (F1 |= F2) if any
assignment τ that satisﬁes F1 and only assigns variables of F1 (i.e. for which
var(τ) ⊂var(F1)) can be extended into an assignment τ 2 ⊃τ that satisﬁes F2.
The formulas are equisatisﬁable if F1 is satisﬁable iﬀF2 is. An assignment τ is
complete for a CNF formula F if var(F) ⊂var(τ), and otherwise partial for F.
The restriction F

τ of F wrt a partial assignment τ is a CNF formula obtained
by (i) removing from F all clauses that are satisﬁed by τ and (ii) removing from
the remaining clauses of F literals l for which τ(l) = 0. Applying unit propaga-
tion on F refers to iteratively restricting F by τ = {l} for a unit clause (clause
with a single literal) (l) ∈F until the resulting (unique) formula, denoted by
UP(F), contains no unit clauses or some clause in F becomes empty. We say that
unit propagation on F derives a conﬂict if UP(F) contains the empty clause. The
formula F1 implies F2 under unit propagation (F1 ⊢1 F2) if, for each C ∈F2,

78
H. Ihalainen et al.
unit propagation derives a conﬂict in F1 ∧{(¬l) | l ∈C}. Note that F1 ⊢1 F2
implies F1 |= F2, but not vice versa in general.
Maximum Satisﬁability. An instance F = (FH, FS, w) of (weighted partial)
maximum satisﬁability (MaxSAT for short) consists of two CNF formulas, the
hard clauses FH and the soft clauses FS, and a weight function w: FS →N that
assigns a positive weight to each soft clause.
Without loss of generality, we assume that every soft clause C ∈FS is unit1.
The set of blocking literals B(F) = {l | (¬l) ∈FS} consists of the literals
l the negation of which occurs in FS. The weight function w is extended to
blocking literals by w(l) = w((¬l)). Without loss of generality, we also assume
that l ∈lit(FH) for all l ∈B(F)2. Instead of using the deﬁnition of MaxSAT
in terms of hard and soft clauses, we will from now on view a MaxSAT instance
F = (FH, B(F), w) as a set FH of hard clauses, a set B(F) of blocking literals
and a weight function w: B(F) →N.
Any complete assignment τ over var(FH) that satisﬁes FH is a solution to
F. The cost COST(F, τ) = 
l∈B(F) τ(l)w(l) of a solution τ is the sum of weights
of blocking literals it assigns to 13. The cost of a complete assignment τ that
does not satisfy FH is deﬁned as ∞. The cost of a partial assignment τ over
var(FH) is deﬁned as the cost of smallest-cost assignments that are extensions
of τ. A solution τ o is optimal if COST(F, τ o) ≤COST(F, τ) holds for all solutions
τ of F. The cost of the optimal solutions of a MaxSAT instance is denoted by
COST(F), with COST(F) = ∞iﬀFH is unsatisﬁable. In MaxSAT the task is to
ﬁnd an optimal solution to a given MaxSAT instance.
Example 1. Let F = (FH, B(F), w) be a MaxSAT instance with FH = {(x ∨
b1), (¬x ∨b2), (y ∨b3 ∨b4), (z ∨¬y ∨b4), (¬z)}, B(F) = {b1, b2, b3, b4} hav-
ing w(b1) = w(b4) = 1, w(b2) = 2 and w(b3) = 8. The assignment τ =
{b1, b4, ¬b2, ¬b3, ¬x, ¬z, y} is an example of an optimal solution of F and has
COST(F, τ) = COST(F) = 2.
With a slight abuse of notation, we denote by F ∧C = (FH ∪{C}, B(F ∧
C), w) the MaxSAT instance obtained by adding a clause C to an instance
F = (FH, B(F), w). Adding clauses may introduce new blocking literals but
not change the weights of already existing ones, i.e., B(F) ⊂B(F ∧C) and
wF(l) = wF∧C(l) for all l ∈B(F).
Correction Sets. For a MaxSAT instance F, a subset cs ⊂B(F) is a minimal
correction set (MCS) of F if (i) FH ∧
l∈B(F)\cs(¬l) is satisﬁable and (ii) FH ∧

l∈B(F)\css(¬l) is unsatisﬁable for every css ⊊cs. In words, cs is an MCS if it
1 A soft clause C can be replaced by the hard clause C ∨x and soft clause (¬x), where
x is a variable not in var(FH ∧FS), without aﬀecting the costs of solutions.
2 Otherwise the instance can be simpliﬁed by unit propagating ¬l without changing
the costs of solutions. As a consequence, any complete assignment for FH will be
complete for FH ∧FS as well.
3 This is equivalent to the sum of weights of soft clauses not satisﬁed by τ.

Clause Redundancy and Preprocessing in Maximum Satisﬁability
79
is a subset-minimal set of blocking literals that is included in some solution τ of
F.4 We denote the set of MCSes of F by mcs(F).
There is a tight connection between the MCSes and solutions of MaxSAT
instances. Given an optimal solution τ o of a MaxSAT instance F, the set τ o ∩
B(F) is an MCS of F. In the other direction, for any cs ∈mcs(F), there is a (not
necessary optimal) solution τ cs such that cs = B(F) ∩τ cs and COST(F, τ cs) =

l∈cs w(l).
Example 2. Consider the instance F from Example 1. The set {b1, b4} ∈mcs(F)
is an MCS of F that corresponds to the optimal solution τ described in
Example 1. The set {b2, b3} ∈mcs(F) is another example of an MCS that
instead corresponds to the solution τ2 = {b2, b3, ¬b1, ¬b4, x, ¬z, ¬y} for which
COST(F, τ) = 10.
3
Propagation Redundancy in MaxSAT
We extend recent work [23] on characterizing redundant clauses using semantic
implication in the context of SAT to MaxSAT. In particular, we provide natural
counterparts for several recently-proposed strong notions of redundancy in SAT
to the context of MaxSAT and analyze the relationships between them.
In the context of SAT, the most general notion of clause redundancy is seem-
ingly simple: a clause C is redundant for a formula F if it does not aﬀect its
satisﬁability, i.e., clause C is redundant wrt a CNF formula F if F and F ∧{C}
are equisatisﬁable [20,29]. This allows for the set of satisfying assignments to
change, and does not require preserving logical equivalence; we are only inter-
ested in satisﬁability.
A natural counterpart for this general view in MaxSAT is that the cost of
optimal solutions (rather than the set of optimal solutions) should be preserved.
Deﬁnition 1. A clause C is redundant wrt a MaxSAT instance F if COST(F) =
COST(F ∧C).
This coincides with the counterpart in SAT whenever B(F) = ∅, since then
the cost of a MaxSAT instance F is either 0 (if FH is satisﬁable) or ∞(if FH
is unsatisﬁable). Unless explicitly speciﬁed, we will use the term “redundant” to
refer to Deﬁnition 1.
Following [23], we say that a clause C blocks the assignment ¬C (and all
assignments τ for which ¬C ⊂τ). As shown in the context of SAT [23], a clause
C is redundant (in the equisatisﬁability sense) for a CNF formula F if C does not
block all of its satisfying assignments. The counterpart that arises in the context
of MaxSAT from Deﬁnition 1 is that the cost of at least one of the solutions not
blocked by C is no greater than the cost of ¬C.
Proposition 1. A clause C is redundant wrt a MaxSAT instance F if and
only if there is an assignment τ for which COST(F ∧C, τ) = COST(F, τ) ≤
COST(F, ¬C).
4 This is equivalent to a subset-minimal set of soft clauses falsiﬁed by τ.

80
H. Ihalainen et al.
The equality COST(F ∧C, τ) = COST(F, τ) of Proposition 1 is necessary, as
witnessed by the following example.
Example 3. Consider the MaxSAT instance F detailed in Example 1, the clause
C = (b5) with b5 ∈B(F ∧C) and the assignment τ = {b5}. Then 2 =
COST(F, τ) ≤COST(F, ¬C) = 2 but C is not redundant since COST(F ∧C) =
2 + wF∧C(b5) > 2 = COST(F).
Proposition 1 provides a suﬃcient condition for a clause C being redundant.
Further requirements on the assignment τ can be imposed without loss of gen-
erality.
Theorem 1. A non-empty clause C is redundant wrt a MaxSAT instance F =
(FH, B(F), w) if and only if there is an assignment τ such that
(i) τ(C) = 1,
(ii) FH

¬C |= FH

τ and
(iii) COST(F ∧C, τ) = COST(F, τ) ≤COST(F, ¬C).
As we will see later, a reason for including two additional conditions in The-
orem 1 is to allow deﬁning diﬀerent restrictions of redundancy notions, some of
which allow for eﬃciently identifying redundant clauses.
Example 4. Consider the instance F = (FH, B(F), w) detailed in Example 1, a
clause C = (¬x∨b5) for a b5 ∈B(F ∧C) and an assignment τ = {¬x, b1}. Then:
τ(C) = 1, {(b2), (y ∨b3 ∨b4), (z ∨¬y ∨b4), (¬z)} = FH

¬C |= FH

τ = {(y ∨b3 ∨
b4), (z∨¬y∨b4), (¬z)}, and 2 = COST(F∧C, τ) = COST(F, τ) ≤COST(F, ¬C) = 3.
We conclude that C is redundant.
In the context of SAT, imposing restrictions on the entailment operator and
the set of assignments has been shown to give rise to several interesting redun-
dancy notions which hold promise of practical applicability. These include three
variants (LPR, SPR, and PR) of so-called (literal/set) propagation redundancy
[23]. For completeness we restate the deﬁnitions of these three notions. A clause C
is LPR wrt a CNF formula F if there is a literal l ∈C for which F

¬C ⊢1 F

(¬C)l,
SPR if the same holds for a subset L ⊂C, and PR if there exists an assignment
τ that satisﬁes C and for which F

¬C ⊢1 F

τ. With the help of Theorem 1, we
obtain counterparts for these notions in the context of MaxSAT.
Deﬁnition 2. With respect to an instance F = (FH, B(F), w), a clause C is
– cost literal propagation redundant (CLPR) (on l) there is a literal l ∈C
for which either (i) ⊥∈UP(FH

¬C) or (ii) l /∈B(F ∧C) and FH

¬C ⊢1
FH

(¬C)l;
– cost set propagation redundant (CSPR) (on L) if there is a set L ⊂
C \ B(F ∧C) of literals for which FH

¬C ⊢1 FH

(¬C)L; and
– cost propagation redundant (CPR) if there is an assignment τ such that
(i) τ(C) = 1,
(ii) FH

¬C ⊢1 FH

τ and
(iii) COST(F ∧C, τ) = COST(F, τ) ≤COST(F, ¬C).

Clause Redundancy and Preprocessing in Maximum Satisﬁability
81
Example 5. Consider again F = (FH, B(F), w) from Example 1. The clause D =
(b1 ∨b2) is CLPR wrt F since ⊥∈UP(FH

¬D) as {(x), (¬x)} ⊂FH

¬D. As for
the redundant clause C and assignment τ detailed in Example 3, we have that
C is CPR, since FH

τ ⊂FH

¬C which implies FH

¬C ⊢1 FH

τ.
We begin the analysis of the relationship between these redundancy notions
by showing that CSPR (and by extension CLPR) clauses also satisfy the
MaxSAT-centric condition (iii) of Theorem 1. Assume that C is CSPR wrt a
instance F = (FH, B(F), w) on the set L.
Lemma 1. Let τ ⊃¬C be a solution of F. Then, COST(F, τ) ≥COST(F, τL).
The following corollary of Lemma 1 establishes that CSPR and CLPR clauses
are redundant according to Deﬁnition 1.
Corollary 1. COST(F ∧C, (¬C)L) = COST(F, (¬C)L) ≤COST(F, ¬C).
The fact that CPR clauses are redundant follows trivially from the fact that
FH

¬C ⊢1 FH

τ implies FH

¬C |= FH

τ. However, given a solution ω that does
not satisfy a CPR clause C, the next example demonstrates that the assignment
ωτ need not have a cost lower than ω. Stated in another way, the example
demonstrates that an observation similar to Lemma 1 does not hold for CPR
clauses in general.
Example 6. Consider a MaxSAT instance F = (FH, B(F), w) having FH =
{(x ∨b1), (¬x, b2)}, B(F) = {b1, b2} and w(b1) = w(b2) = 1. The clause C =
(x) is CPR wrt F, the assignment τ = {x, b2} satisﬁes the three conditions of
Deﬁnition 2. Now δ = {¬x, b1} is a solution of F that does not satisfy C for
which δτ = {x, b1, b2} and 1 = COST(F, δ) < 2 = COST(F, δτ).
Similarly as in the context of SAT, verifying that a clause is CSPR (and
by extension CLPR) can be done eﬃciently. However, in contrast to SAT, we
conjecture that verifying that a clause is CPR can not in the general case be
done eﬃciently, even if the assignment τ is given. While we will not go into
detail on the complexity of identifying CPR clauses, the following proposition
gives some support for our conjecture.
Proposition 2. Let F be an instance and k ∈N. There is another instance
FM, a clause C, and an assignment τ such that C is CPR wrt FM if and only
if COST(F) ≥k.
As deciding if COST(F) ≥k is NP-complete in the general case, Proposition 2
suggests that it may not be possible to decide in polynomial time if an assignment
τ satisﬁes the three conditions of Deﬁnition 2 unless P=NP. This is in contrast to
SAT, where verifying propagation redundancy can be done in polynomial time
if the assignment τ is given, but is NP-complete if not [24].
The following observations establish a more precise relationship between the
redundancy notions. For the following, let RED(F) denote the set of clauses that
are redundant wrt a MaxSAT instance F according to Deﬁnition 1. Analogously,
the sets CPR(F), CSPR(F) and CLPR(F) consist of the clauses that are CPR,
CSPR and CLPR wrt F, respectively.

82
H. Ihalainen et al.
Observation 1 CLPR(F) ⊂CSPR(F) ⊂CPR(F) ⊂RED(F) holds for any
MaxSAT instance F.
Observation 2 There are MaxSAT instances F1, F2 and F3 for which
CLPR(F1) ⊊CSPR(F1), CSPR(F2) ⊊CPR(F2) and CPR(F3) ⊊RED(F3).
The proofs of Observations 1 and 2 follow directly from known results in
the context of SAT [23] by noting that any CNF formula can be viewed as an
instance of MaxSAT without blocking literals.
For a MaxSAT-centric observation on the relationship between the redun-
dancy notions, we note that the concept of redundancy and CPR coincide for
any MaxSAT instance that has solutions.
Observation 3 CPR(F) = RED(F) holds for any MaxSAT instance F with
COST(F) < ∞.
We note that a result similar to Observation 3 could be formulated in the context
of SAT. The SAT-counterpart would state that the concept of redundancy (in the
equisatisﬁability sense) coincides with the concept of propagation redundancy
for SAT solving (deﬁned e.g. in [23]) for satisﬁable CNF formulas. However,
assuming that a CNF formula is satisﬁable is very restrictive in the context
of SAT. In contrast, it is natural to assume that a MaxSAT instance admits
solutions.
We end this section with a simple observation: adding a redundant clause C
to a MaxSAT instance F preserves not only optimal cost, but optimal solutions
of F ∧C are also optimal solutions of F. However, the converse need not hold;
an instance F might have optimal solutions that do not satisfy C.
Example 7. Consider an instance F = (FH, B(F), w) with FH = {(b1 ∨b2)},
B(F) = {b1, b2} and w(b1) = w(b2) = 1. The clause C = (¬b1) is CPR wrt
F. In order to see this, let τ = {¬b1, b2}. Then τ satisﬁes C (condition (i) of
Deﬁnition 2). Furthermore, τ satisﬁes FH, implying FH

¬C ⊢1 FH

τ (condition
(ii)). Finally, we have that 1 = COST(F, τ) = COST(F ∧C, τ) ≤COST(F, ¬C) = 1
(condition (iii)). The assignment δ = {b1, ¬b2} is an example of an optimal
solution of F that is not a solution of F ∧C.
4
Propagation Redundancy and MCSes
In this section, we analyze the eﬀect of adding redundant clauses on the MCSes
of MaxSAT instances. As the main result, we show that adding CSPR (and by
extension CLPR) clauses to a MaxSAT instance F preserves all MCSes while
adding CPR clauses does not in general. Stated in terms of solutions, this means
that adding CSPR clauses to F preserves not only all optimal solutions, but
all solutions τ for which (τ ∩B(F)) ∈mcs(F), while adding CPR clauses only
preserves at least one optimal solution.
Eﬀect of CLPR Clauses on MCSes. MaxSAT-liftings of four speciﬁc SAT
solving techniques (including bounded variable elimination and self-subsuming

Clause Redundancy and Preprocessing in Maximum Satisﬁability
83
resolution) were earlier proposed in [8]. Notably, the correctness of the lift-
ings was shown individually for each of the techniques by arguing individu-
ally that applying one of the liftings does not change the set of MCSes of any
MaxSAT instance. Towards a more generic understanding of optimal cost pre-
serving MaxSAT preprocessing, in [10] the notion of solution resolution asym-
metric tautologies (SRAT) was proposed as a MaxSAT-lifting of the concept of
resolution asymmetric tautologies (RAT). In short, a clause C is a SRAT clause
for a MaxSAT instance F = (FH, B(F), w) if there is a literal l ∈C \ B(F ∧C)
such that FH ⊢1 ((C ∨D) \ {¬l}) for every D ∈FH for which ¬l ∈D.
In analogy with RAT [29], SRAT was shown in [10] to allow for a general
proof of correctness for natural MaxSAT-liftings of a wide range of SAT prepro-
cessing techniques, covering among other the four techniques for which individual
correctness proofs were provided in [8]. The generality follows essentially from
the fact that the addition and removal of SRAT clauses preserves MCSes. The
same observations apply to CLPR, as CLPR and SRAT are equivalent.
Proposition 3. A clause C is CLPR wrt F iﬀit is SRAT wrt F.
The proof of Proposition 3 follows directly from corresponding results in the
context of SAT [23]. Informally speaking, a clause C is SRAT on a literal l iﬀit
is RAT [29] on l and l /∈B(F). Similarly, a clause C is CLPR on a literal l iﬀit is
LPR as deﬁned in [23] on l and l /∈B(F). Proposition 3 together with previous
results from [10] implies that the MCSes of MaxSAT instances are preserved
under removing and adding CLPR clauses.
Corollary 2. If C is CLPR wrt F, then mcs(F) = mcs(F ∧C).
Eﬀect of CPR Clauses on MCSes. We turn our attention to the eﬀect of
CPR clauses on the MCSes of MaxSAT instances. Our analysis makes use of
the previously-proposed MaxSAT-centric preprocessing rule known as subsumed
label elimination (SLE) [11,33]5.
Deﬁnition 3. (Subsumed Label Elimination [11,33])
Consider a MaxSAT
instance F = (FH, B(F), w) and a blocking literal l ∈B(F) for which ¬l /∈
lit(FH). Assume that there is another blocking literal ls ∈B(F) for which
(1) ¬ls /∈lit(FH), (2) {C ∈FH | l ∈C} ⊂{C ∈FH | ls ∈C} and (3)
w(l) ≥w(ls). The subsumed label elimination (SLE) rule allows adding (¬l) to
FH.
A speciﬁc proof of correctness of SLE was given in [11]. The following proposition
provides an alternative proof based on CPR.
Proposition 4 (Proof of correctness for SLE).
Let F be a MaxSAT
instance and assume that the blocking literals l, ls ∈B(F) satisfy the three con-
ditions of Deﬁnition 3. Then, the clause C = (¬l) is CPR wrt F.
5 Rephrased here using our notation.

84
H. Ihalainen et al.
Proof. We show that τ = {¬l, ls} satisﬁes the three conditions of Deﬁnition 2.
First τ satisﬁes C (condition (i)). Conditions (1) and (2) of Deﬁnition 3 imply
FH

τ ⊂FH

¬C which in turn implies FH

¬C ⊢1 FH

τ (condition (ii)).
As for condition (iii), the requirement COST(F ∧C, τ) = COST(F, τ) follows
from B(F ∧C) = B(F). Let δ ⊃¬C be a complete assignment of FH for which
COST(F, δ) = COST(F, ¬C). If COST(F, δ) = ∞then COST(F, τ) ≤COST(F, ¬C)
follows trivially. Otherwise δ \¬C satisﬁes FH

¬C so by FH

¬C ⊢1 FH

τ it satis-
ﬁes FH

τ as well. Thus δR = ((δ\¬C)\{¬l | l ∈τ})∪τ = (δ\{l, ¬l, ¬ls})∪{¬l, ls}
is an extension of τ that satisﬁes FH and for which COST(F, τ) ≤COST(F, δR) ≤
COST(F, δ) by condition (3) of Deﬁnition 3. Thereby τ satisﬁes the conditions of
Deﬁnition 2 so C is CPR wrt F.
⊓⊔
Example 8. The blocking literals b3, b4 ∈B(F) of the instance F detailed in
Example 1 satisfy the conditions of Deﬁnition 3. By Proposition 4 the clause (¬b3)
is CPR wrt F.
In [11] it was shown that SLE does not preserve MCSes in general. By Corol-
lary 2, this implies that SLE can not be viewed as the addition of CLPR clauses.
Furthermore, by Proposition 4 we obtain the following.
Corollary 3. There is a MaxSAT instance F and a clause C that is CPR wrt
F for which mcs(F) ̸= mcs(F ∧C).
Eﬀect of CSPR Clauses on MCSes. Having established that CLPR clauses
preserve MCSes while CPR clauses do not, we complete the analysis by demon-
strating that CSPR clauses preserve MCSes.
Theorem 2. Let F be a MaxSAT instance and C a CSPR clause of F. Then
mcs(F) = mcs(F ∧C).
Theorem 2 follows from the following lemmas and propositions. In the fol-
lowing, let C be a clause that is CSPR wrt a MaxSAT instance F on a set
L ⊂C \ B(F ∧C).
Lemma 2. Let cs ⊂B(F). If FH ∧
l∈B(F)\cs(¬l) is satisﬁable, then
(FH ∧C) ∧
l∈B(F∧C)\cs(¬l) is satisﬁable.
Lemma 2 helps in establishing one direction of Theorem 2.
Proposition 5. mcs(F) ⊂mcs(F ∧C).
Proof. Let cs ∈mcs(F). Then FH ∧
l∈B(F)\cs(¬l) is satisﬁable, which by
Lemma 2 implies that (FH ∧C) ∧
l∈B(F∧C)\cs(¬l) is satisﬁable.
To show that (FH ∧C)∧
l∈B(F∧C)\css(¬l) is unsatisﬁable for any css ⊊cs ⊂
B(F), we note that any assignment satisfying (FH∧C)∧
l∈B(F∧C)\css(¬l) would
also satisfy FH ∧
l∈B(F)\css(¬l), contradicting cs ∈mcs(F).
⊓⊔
The following lemma is useful for showing inclusion in the other direction.

Clause Redundancy and Preprocessing in Maximum Satisﬁability
85
Lemma 3. Let cs ∈mcs(F ∧C). Then cs ⊂B(F).
Lemma 3 allows for completing the proof of Theorem 2.
Proposition 6. mcs(F ∧C) ⊂mcs(F).
Proof. Let cs ∈mcs(F ∧C), which by Lemma 3 implies cs ⊂B(F). Let τ be
a solution that satisﬁes (FH ∧C) ∧
l∈B(F∧C)\cs(¬l). Then τ satisﬁes FH ∧

l∈B(F)\cs(¬l). For contradiction, assume that FH ∧
l∈B(F)\css(¬l) is satisﬁable
for some css ⊊cs. Then by Lemma 2, (FH ∧C)∧
l∈B(F∧C)\css(¬l) is satisﬁable
as well, contradicting cs ∈mcs(F ∧C). Thereby cs ∈mcs(F).
⊓⊔
Theorem 2 implies that SLE can not be viewed as the addition of CSPR
clauses. In light of this, an interesting remark is that—in contrast to CPR clauses
in general (recall Example 6)—the assignment τ used in the proof of Proposi-
tion 4 can be used to convert any assignment that does not satisfy the CPR
clause detailed in Deﬁnition 3 into one that does, without increasing its cost.
Observation 4 Let F be a MaxSAT instance and assume that the blocking lit-
erals l, ls ∈B(F) satisfy the three conditions of Deﬁnition 3. Let τ = {¬l, ls}
and consider any solution δ ⊃¬C of F that does not satisfy the CPR clause
C = (¬l). Then δτ is a solution of F ∧C for which COST(F, δτ) ≤COST(F, δ).
5
CPR-Based Preprocessing for MaxSAT
Mapping the theoretical observations into practical preprocessing, in this section
we discuss through examples how CPR clauses can be used as a uniﬁed theoret-
ical basis for capturing a wide variety of known MaxSAT reasoning rules, and
how they could potentially help in the development of novel MaxSAT reasoning
techniques.
Our ﬁrst example is the so-called hardening rule [2,8,17,26]. In terms of our
notation, given a solution τ to a MaxSAT instance F = (FH, B(F), w) and a
blocking literal l ∈B(F) for which w(l) > COST(F, τ), the hardening rule allows
adding the clause C = (¬l) to FH.
The correctness of the hardening rule can be established with CPR clauses.
More speciﬁcally, as COST(F, τ) < w(l) it follows that τ(C) = 1 (condition (i)
of Deﬁnition 2). Since τ satisﬁes F, we have that FH

τ = ⊤so FH

¬C ⊢1 FH

τ
(condition (ii)). Finally, as COST(F, δ) ≥w(l) > COST(F, τ) holds for all δ ⊃¬C
it follows that COST(F, ¬C) > COST(F, τ) = COST(F ∧C, τ). As such, (¬l) is
CPR clause wrt F. If fact, instead of assuming w(l) > COST(F, τ) it suﬃces to
assume w(l) ≥COST(F, τ) and τ(l) = 0.
The hardening rule can not be viewed as the addition of CSPR or CLPR
clauses because it does not in general preserve MCSes.
Example 9. Consider the MaxSAT instance F from Example 1 and a solution
τ = {b1, b2, b4, ¬b3, ¬z, x, y}. Since COST(F, τ) = 3 < 8 = w(b3), the clause (¬b3)
is CPR. However, mcs(F) ̸= mcs(F ∧C) since the set {b2, b3} ∈mcs(F) is not
an MCS of F ∧C as (FH ∧C) ∧
l∈B(F)\cs(¬l) = (FH ∧(¬b3)) ∧(¬b1) ∧(¬b4)
is not satisﬁable.

86
H. Ihalainen et al.
Viewing the hardening rule through the lens of CPR clauses demonstrates
novel aspects of the MaxSAT-liftings of propagation redundancy. In particular,
instantiated in the context of SAT, an argument similar to the one we made for
hardening shows that given a CNF formula F, an assignment τ satisfying F, and
a literal l for which τ(l) = 0, the clause (¬l) is redundant (wrt equisatisﬁability).
While formally correct, such a rule is not very useful for SAT solving. In contrast,
in the context of MaxSAT the hardening rule is employed in various modern
MaxSAT solvers and leads to non-trivial performance-improvements [4,5].
As another example of capturing MaxSAT-centric reasoning with CPR, con-
sider the so-called TrimMaxSAT rule [39]. Given a MaxSAT instance F =
(FH, B(F), w) and a literal l ∈B(F) for which τ(l) = 1 for all solutions of
F, the TrimMaxSAT rule allows adding the clause C = (l) to FH. In this case
the assumptions imply that all solutions of F also satisfy C, i.e., that FH

¬C is
unsatisﬁable. As such, any assignment τ that satisﬁes C and FH will also satisfy
the three conditions of Deﬁnition 2 which demonstrates that C is CPR. It is,
however, not CSPR since the only literal in C is blocking.
As a third example of capturing (new) reasoning techniques with CPR, con-
sider an extension of the central variable elimination rule that allows (to some
extent) for eliminating blocking literals.
Deﬁnition 4. Consider a MaxSAT instance F and a blocking literal l ∈B(F).
Let BBVE(F) be the instance obtained by (i) adding the clause C ∨D to F for
every pair (C ∨l), (D ∨¬l) ∈FH and (ii) removing all clauses (D ∨¬l) ∈FH.
Then COST(F) = COST(BBVE(F)) and mcs(F) = mcs(BBVE(F)).
On the Limitations of CPR. Finally, we note that while CPR clauses sig-
niﬁcantly generalize existing theory on reasoning and preprocessing rules for
MaxSAT, there are known reasoning techniques that can not (at least straight-
forwardly) be viewed through the lens of propagation redundancy. For a concrete
example, consider the so-called intrinsic atmost1 technique [26].
Deﬁnition 5. Consider a MaxSAT instance F and a set L ⊂B(F) of blocking
literals. Assume that (i) |τ ∩{¬l | l ∈L}| ≤1 holds for any solution τ of F and
(ii) w(l) = 1 for each l ∈L. Now form the instance AT-MOST-ONE(F, L) by
(i) removing each literal l ∈L from B(F), and (ii) adding the clause {(¬l) | l ∈
L} ∪{lL} to F, where lL is a fresh blocking literal with w(lL) = 1.
It has been established that any optimal solution of AT-MOST-ONE(F, L)
is an optimal solution of F [26]. However, as the next example demonstrates,
the preservation of optimal solutions is in general not due to the clauses added
being redundant, as applying the technique can aﬀect optimal cost.
Example 10. Consider the MaxSAT instance F = (FH, B(F), w) with FH =
{(li) | i = 1 . . . n}, B(F) = {l1 . . . ln} and w(l) = 1 for all l ∈B(F). Then |τ ∩
¬B(F)| = 0 ≤1 holds for all solutions τ of F so the intrinsic-at-most-one tech-
nique can be used to obtain the instance F2 = AT-MOST-ONE(F, B(F)) =
(F2
H, B(F2), w2) with F2
H = FH ∪{(¬l1 ∨. . . ∨¬ln ∨lL)}, B(F2) = {lL} and

Clause Redundancy and Preprocessing in Maximum Satisﬁability
87
w2(lL) = 1. Now δ = {l | l ∈B(F)} ∪{lL} is an optimal solution to both F2
and F for which 1 = COST(F2, δ) < COST(F, δ) = n.
Example 10 implies that the intrinsic atmost1 technique can not be viewed as
the addition or removal of redundant clauses. Generalizing CPR to cover weight
changes could lead to further insights especially due to potential connections
with core-guided MaxSAT solving [1,36–38].
6
MaxPre 2: More General Preprocesssing in Practice
Connecting to practice, we extended the MaxSAT preprocessor MaxPre [33]
version 1 with support for techniques captured by propagation redundancy. The
resulting MaxPre version 2, as outlined in the following, hence includes tech-
niques which have previously only been implemented in speciﬁc solver imple-
mentations rather than in general-purpose MaxSAT preprocessors.
First, let us mention that the earlier MaxPre [33] version 1 assumes that
any blocking literals only appear in a single polarity among the hard clauses.
Removing this assumption—supported by theory developed in Sects. 3–4—
decreases the number of auxiliary variables that need to be introduced when
a MaxSAT instance is rewritten to only include unit soft clauses. For exam-
ple, consider a MaxSAT instance F with FH = {(¬x ∨y), (¬y ∨x)} and
FS = {(x), (¬y)}. For preprocessing the instance, MaxPre 1 extends both soft
clauses with a new, auxiliary variable and runs preprocessing on the instance
F = {(¬x ∨y), (¬y ∨x), (x ∨b1), (¬y ∨b2)} with B(F) = {b1, b2}. In contrast,
MaxPre 2 detects that the clauses in FS are unit and reuses them as blocking
literals, invoking preprocessing on F = {(¬x∨y), (¬y∨x)} with B(F) = {¬x, y}.
In addition to the techniques already implemented in MaxPre 1, MaxPre
2 includes the following additional techniques: hardening [2], a variant Trim-
MaxSAT [39] that works on all literals of a MaxSAT instance, the intrinsic
atmost1 technique [26] and a MaxSAT-lifting of failed literal elimination [12]. In
short, failed literal elimination adds the clause (¬l) to the hard clauses FH of
an instance in case unit-propagation derives a conﬂict in FH ∧{(l)}. Addition-
ally, the implementation of failed literal elimination attempts to identify implied
equivalences between literals that can lead to further simpliﬁcation.
For computing the solutions required by TrimMaxSAT and detecting the car-
dinality constraints required by intrinsic-at-most-one constraints, MaxPre 2 uses
the Glucose 3.0 SAT-solver [3]. For computing solutions required by hardening,
MaxPre 2 additionally uses the SatLike incomplete MaxSAT solver [34] within
preprocessing. MaxPre 2 is available in open source at https://bitbucket.org/
coreo-group/maxpre2/.
We emphasize that, while the additional techniques implemented by MaxPre
2 have been previously implemented as heuristics in speciﬁc solver implemen-
tations, MaxPre 2 is—to the best of our understanding—the ﬁrst stand-alone
implementation supporting techniques whose correctness cannot be established
with previously-proposed MaxSAT redundancy notions (i.e., SRAT). The goal

88
H. Ihalainen et al.
of our empirical evaluation presented in the next section is to demonstrate the
potential of viewing expressive reasoning techniques not only as solver heuristics,
but as a separate step in the MaxSAT solving process whose correctness can be
established via propagation redundancy.
7
Empirical Evaluation
We report on results from an experimental evaluation of the potential of incor-
porating more general reasoning in MaxSAT preprocessing. In particular, we
evaluated both complete solvers (geared towards ﬁnding provably-optimal solu-
tions) and incomplete solvers (geared towards ﬁnding relatively good solutions
fast) on standard heterogenous benchmarks from recent MaxSAT Evaluations.
All experiments were run on 2.60-GHz Intel Xeon E5-2670 8-core machines with
64 GB memory and CentOS 7. All reported runtimes include the time used in
preprocessing (when applicable).
7.1
Impact of Preprocessing on Complete Solvers
We start by considering recent representative complete solvers covering three
central MaxSAT solving paradigms: the core-guided solver CGSS [27] (as a recent
improvement to the successful RC2 solver [26]), and the MaxSAT Evaluation
2021 versions of the implicit hitting set based solver MaxHS [17] and the solution-
improving solver Pacose [40]. For each solver S we consider the following variants.
– S: S in its default conﬁguration.
– S no preprocess: S with the solver’s own internal preprocessing turned oﬀ
(when applicable).
– S+maxpre1: S after applying MaxPre 1 using its default conﬁguration.
– S+maxpre2/none: S after applying MaxPre 2 using the default conﬁguration
of MaxPre 1.
– S + maxpre2/<TECH>: S after applying MaxPre 2 using the standard conﬁg-
uration of MaxPre 1 and additional techniques integrated into MaxPre 2 (as
detailed in Section 6) as speciﬁed by <TECH>.
More precisely, <TECH> speciﬁes which of the techniques HTVGR are applied:
H for hardening, T and V for TrimMaxSAT on blocking and non-blocking liter-
als, respectively, G for intrinsic-at-most-one-constraints and R for failed literal
elimination. It should be noted that an exhaustive evaluation of all subsets and
application orders of these techniques is infeasible in practice. Based on prelim-
inary experiments, we observed that the following choices were promising: HRT
for CGSS and MaxHS, and HTVGR for Pacose; we report results using these
individual conﬁgurations.
As benchmarks, we used the combined set of weighted instances from the
complete tracks of MaxSAT Evaluation 2020 and 2021. After removing dupli-
cates, this gave a total of 1117 instances. We enforced a per-instance time limit of

Clause Redundancy and Preprocessing in Maximum Satisﬁability
89
0 1000 2000 3000
610
620
630
640
650
660
670
680
690
Time (s)
Pacose (657)
Pacose+maxpre1 (677)
Pacose+maxpre2/none (659)
Pacose+maxpre2/HVRTG (694)
0 1000 2000 3000
740
760
780
800
820
Time (s)
CGSS (812)
CGSS+maxpre1 (799)
CGSS+maxpre2/none (805)
CGSS+maxpre2/HRT (810)
0 1000 2000 3000
740
760
780
800
820
Time (s)
Number of solved instances
MaxHS no preprocess (787)
MaxHS (826)
MaxHS+maxpre1 (824)
MaxHS+maxpre2/none (824)
MaxHS+maxpre2/HRT (829)
Fig. 1. Impact of preprocessing on complete solvers. For each solver, the number of
instances solved within a 60-min per-instance time limit in parentheses.
60 minutes and memory limit of 32 GB. Furthermore, we enforced a per-instance
120-second time limit on preprocessing.
An overview of the results is shown in Fig. 1, illustrating for each solver
the number of instances solved (x-axis) under diﬀerent per-instance time lim-
its (y-axis). We observe that for both CGSS and MaxHS, S+maxpre1 and
S+maxpre2/none leads to less instances solved compared to S. In contrast,
S+maxpre2/HRT, i.e., incorporating the stronger reasoning techniques of Max-
Pre 2, performs best of all preprocessing variants and improves on MaxHS
also in terms of the number of instances solved. For Pacose, we observe that
both Pacose+maxpre1 and Pacose+maxpre2/new (without the stronger reason-
ing techniques) already improve the performance of Pacose, leading to more
instances solved. Incorporating the stronger reasoning rules further signiﬁcantly
improves performance, with Pacose+maxpre2/HVRTG performing the best among
all of the Pacose variants.
7.2
Impact of Preprocessing on Incomplete MaxSAT Solving
As a representative incomplete MaxSAT solver we consider the MaxSAT Evalua-
tion 2021 version of Loandra [9], as the best-performing solver in the incomplete

90
H. Ihalainen et al.
Table 1. Impact of preprocessing on the incomplete solver Loandra. The wins are
organized column-wise, the cell on row X column Y contains the total number of
instances that the solver on column Y wins over the solver on row X.
#Wins
base (maxpre1) no-prepro maxpre2/ none maxpre2/ VG
base (maxpre1) —
154
135
152
no-prepro
208
—
216
218
maxpre2/none
105
143
—
77
maxpre2/VG
110
140
80
—
Score (avg):
0.852
0.840
0.863
0.870
track of MaxSAT Evaluation under a 300s per-instance time limit on weighted
instances. Loandra combines core-guided and solution-improving search towards
ﬁnding good solutions fast. We consider the following variants of Loandra.
– base (maxpre1): Loandra in its default conﬁguration which makes use of
MaxPre 1.
– no-prepro: Loandra with its internal preprocessing turned oﬀ.
– maxpre2/none: base with its internal preprocessor changed from MaxPre 1
to MaxPre 2 using the default conﬁguration of MaxPre 1.
– maxpre2/VG: maxpre2 incorporating the additional intrinsic-at-most-one con-
straints technique and the extension of TrimMaxSAT to non-blocking literals
(cf. Sect. 6), found promising in preliminary experimentation.
As benchmarks, we used the combined set of weighted instances from the
incomplete tracks of MaxSAT Evaluation 2020 and 2021. After removing dupli-
cates, this gave a total of 451 instances. When reporting results, we consider for
each instance and solver the cost of the best solution found by the solver within
300 s (including time spent preprocessing and solution reconstruction).
We compare the relative runtime performance of the solver variants using
two metrics: #wins and the average incomplete score. Assume that τx and τy
are the lowest-cost solutions computed by two solvers X and Y on a MaxSAT
instance F and that best-cost(F) is the lowest cost of a solution of F found
either in our evaluation or in the MaxSAT Evaluations. Then X wins over Y
if COST(F, τx) < COST(F, τy). The incomplete score, score(F, X), obtained by
solver X on F is the ratio between the cost of the solution found by X and
best-cost(F), i.e., score(F, X) = (best-cost(F) + 1)/(COST(F, τx) + 1). The
score of X on F is 0 if X is unable to ﬁnd any solutions within 300 s.
An overview of the results is shown in Table 1. The upper part of the table
shows a pairwise comparison on the number of wins over all benchmarks. The
wins are organized column-wise, i.e., the cell on row X column Y contains the
total number of instances that the solver on column Y wins over the solver on
row X. The last row contains the average score obtained by each solver over
all instances. We observe that any form of preprocessing improves the perfor-
mance of Loandra, as witnessed by the fact that no-prepro is clearly the worst-
performing variant. The variants that make use of MaxPre 2 outperform the

Clause Redundancy and Preprocessing in Maximum Satisﬁability
91
101
102
103
104
105
106
107
108
101 102 103 104 105 106 107 108
maxpre2/HRT
maxpre 1 - clauses
101 102 103 104 105 106 107 108
maxpre 1 - variables
101
102
103
104
105
106
107
108
101 102 103 104 105 106 107 108
maxpre2/HRT
original instance - clauses
101 102 103 104 105 106 107 108
original instance - variables
Fig. 2. Impact of preprocessing on instance size.
baseline under both metrics; both maxpre2 no new and maxpre2-w:VG obtain
a higher average score and win on more instances over base. The comparison
between maxpre2/none and maxpre2/VG is not as clear. On one hand, the score
obtained by maxpre2/VG is higher. On the other hand, maxpre2/none wins on
80 instances over maxpre2/VG and looses on 77. This suggests that the quality
of solutions computed by maxpre2/VG is on average higher, and that on the
instances on which maxpre2/none wins the diﬀerence is smaller.
7.3
Impact of Preprocessing on Instance Sizes
In addition to improved solver runtimes, we note that MaxPre 2 has a positive
eﬀect on the size of instances (both in terms of the number of variables and
clauses remaining) when compared to preprocessing with MaxPre 1; see Fig. 2
for a comparison, with maxpre2/HRT compared to maxpre1 (left) and to original
instance sizes (right).
8
Conclusions
We studied liftings of variants of propagation redundancy from SAT in the con-
text of maximum satisﬁability where—more ﬁne-grained than in SAT—of inter-
est are reasoning techniques that preserve optimal cost. We showed that CPR,
the strongest MaxSAT-lifting, allows for changing minimal corrections sets in
MaxSAT in a controlled way, thereby succinctly expressing MaxSAT reason-
ing techniques very generally. We also provided a practical MaxSAT preproces-
sor extended with techniques captured by CPR and showed empirically that
extended preprocessing has a positive overall impact on a range of MaxSAT
solvers. Interesting future work includes the development of new CPR-based
preprocessing rules for MaxSAT capable of signiﬁcantly aﬀecting the MaxSAT
solving pipeline both in theory and practice, as well as developing an under-
standing of the relationship between redundancy notions and the transforma-
tions performed by MaxSAT solving algorithms.
References
1. Ans´otegui, C., Bonet, M., Levy, J.: SAT-based MaxSAT algorithms. Artif. Intell.
196, 77–105 (2013)

92
H. Ihalainen et al.
2. Ans´otegui, C., Bonet, M.L., Gab`as, J., Levy, J.: Improving SAT-based weighted
MaxSAT solvers. In: Milano, M. (ed.) CP 2012. LNCS, pp. 86–101. Springer, Hei-
delberg (2012). https://doi.org/10.1007/978-3-642-33558-7 9
3. Audemard, G., Simon, L.: Predicting learnt clauses quality in modern SAT solvers.
In: Proceedings of the IJCAI, pp. 399–404 (2009)
4. Bacchus, F., Berg, J., J¨arvisalo, M., Martins, R. (eds.): MaxSAT Evaluation 2020:
Solver and Benchmark Descriptions, Department of Computer Science Report
Series B, vol. B-2020-2. Department of Computer Science, University of Helsinki
(2020)
5. Bacchus, F., J¨arvisalo, M., Martins, R. (eds.): MaxSAT Evaluation 2019: Solver
and Benchmark Descriptions, Department of Computer Science Report Series B,
vol. B-2019-2. Department of Computer Science, University of Helsinki (2019)
6. Bacchus, F., J¨arvisalo, M., Martins, R.: Maximum satisﬁability (chap. 24). In:
Biere, A., Heule, M., van Maaren, H., Walsh, T. (eds.) Handbook of Satisﬁability.
Frontiers in Artiﬁcial Intelligence and Applications, pp. 929–991. IOS Press (2021)
7. Baek, S., Carneiro, M., Heule, M.J.H.: A ﬂexible proof format for sat solver-
elaborator communication. In: TACAS 2021. LNCS, vol. 12651, pp. 59–75.
Springer, Cham (2021). https://doi.org/10.1007/978-3-030-72016-2 4
8. Belov, A., Morgado, A., Marques-Silva, J.: SAT-based preprocessing for MaxSAT.
In: McMillan, K., Middeldorp, A., Voronkov, A. (eds.) LPAR 2013. LNCS, vol.
8312, pp. 96–111. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-
45221-5 7
9. Berg, J., Demirovi´c, E., Stuckey, P.J.: Core-boosted linear search for incom-
plete MaxSAT. In: Rousseau, L.-M., Stergiou, K. (eds.) CPAIOR 2019. LNCS,
vol. 11494, pp. 39–56. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
19212-9 3
10. Berg, J., J¨arvisalo, M.: Unifying reasoning and core-guided search for maximum
satisﬁability. In: Calimeri, F., Leone, N., Manna, M. (eds.) JELIA 2019. LNCS
(LNAI), vol. 11468, pp. 287–303. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-19570-0 19
11. Berg, J., Saikko, P., J¨arvisalo, M.: Subsumed label elimination for maximum sat-
isﬁability. In: Proceedings of the ECAI. Frontiers in Artiﬁcial Intelligence and
Applications, vol. 285, pp. 630–638. IOS Press (2016)
12. Bhalla, A., Lynce, I., de Sousa, J.T., Marques-Silva, J.: Heuristic-based backtrack-
ing for propositional satisﬁability. In: Pires, F.M., Abreu, S. (eds.) EPIA 2003.
LNCS (LNAI), vol. 2902, pp. 116–130. Springer, Heidelberg (2003). https://doi.
org/10.1007/978-3-540-24580-3 19
13. Biere, A., Heule, M., van Maaren, H., Walsh, T.: Handbook of Satisﬁability (Second
Edition): Volume 336 Frontiers in Artiﬁcial Intelligence and Applications. IOS
Press, Amsterdam, The Netherlands (2021)
14. Biere, A., J¨arvisalo, M., Kiesl, B.: Preprocessing in SAT solving (chap. 9). In:
Biere, A., Heule, M., van Maaren, H., Walsh, T. (eds.) Handbook of Satisﬁability.
Frontiers in Artiﬁcial Intelligence and Applications, pp. 391–435. IOS Press (2021)
15. Cruz-Filipe, L., Heule, M.J.H., Hunt, W.A., Kaufmann, M., Schneider-Kamp, P.:
Eﬃcient certiﬁed RAT veriﬁcation. In: de Moura, L. (ed.) CADE 2017. LNCS
(LNAI), vol. 10395, pp. 220–236. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-63046-5 14
16. Cruz-Filipe, L., Marques-Silva, J., Schneider-Kamp, P.: Eﬃcient certiﬁed resolu-
tion proof checking. In: Legay, A., Margaria, T. (eds.) TACAS 2017. LNCS, vol.
10205, pp. 118–135. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-
662-54577-5 7

Clause Redundancy and Preprocessing in Maximum Satisﬁability
93
17. Davies, J., Bacchus, F.: Exploiting the power of mip solvers in maxsat. In:
J¨arvisalo, M., Van Gelder, A. (eds.) SAT 2013. LNCS, vol. 7962, pp. 166–181.
Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-39071-5 13
18. Heule, M., Hunt, W., Kaufmann, M., Wetzler, N.: Eﬃcient, veriﬁed checking of
propositional proofs. In: Ayala-Rinc´on, M., Mu˜noz, C.A. (eds.) ITP 2017. LNCS,
vol. 10499, pp. 269–284. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-66107-0 18
19. Heule, M., Hunt, W.A., Jr., Wetzler, N.: Bridging the gap between easy generation
and eﬃcient veriﬁcation of unsatisﬁability proofs. Softw. Test. Verif. Reliab. 24(8),
593–607 (2014)
20. Heule, M., J¨arvisalo, M., Lonsing, F., Seidl, M., Biere, A.: Clause elimination for
SAT and QSAT. J. Artif. Intell. Res. 53, 127–168 (2015)
21. Heule, M., Kiesl, B.: The potential of interference-based proof systems. In: Pro-
ceedings of the ARCADE@CADE. EPiC Series in Computing, vol. 51, pp. 51–54.
EasyChair (2017)
22. Heule, M.J.H., Kiesl, B., Biere, A.: Short proofs without new variables. In: de
Moura, L. (ed.) CADE 2017. LNCS (LNAI), vol. 10395, pp. 130–147. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-63046-5 9
23. Heule, M.J.H., Kiesl, B., Biere, A.: Strong extension-free proof systems. J. Autom.
Reason. 64(3), 533–554 (2020)
24. Heule, M.J.H., Kiesl, B., Seidl, M., Biere, A.: PRuning through satisfaction. In:
HVC 2017. LNCS, vol. 10629, pp. 179–194. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-70389-3 12
25. Hou, A.: A theory of measurement in diagnosis from ﬁrst principles. Artif. Intell.
65(2), 281–328 (1994)
26. Ignatiev, A., Morgado, A., Marques-Silva, J.: RC2: an eﬃcient MaxSAT solver. J.
Satisf. Boolean Model. Comput. 11(1), 53–64 (2019)
27. Ihalainen, H., Berg, J., J¨arvisalo, M.: Reﬁned core relaxation for core-guided
MaxSAT solving. In: Proceedings of the CP. LIPIcs, vol. 210, pp. 28:1–28:19.
Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik (2021)
28. J¨arvisalo, M., Biere, A., Heule, M.: Simulating circuit-level simpliﬁcations on CNF.
J. Autom. Reason. 49(4), 583–619 (2012)
29. J¨arvisalo, M., Heule, M.J.H., Biere, A.: Inprocessing rules. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 355–370. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 28
30. Kiesl, B., Rebola-Pardo, A., Heule, M.J.H., Biere, A.: Simulating strong practi-
cal proof systems with extended resolution. J. Autom. Reason. 64(7), 1247–1267
(2020)
31. Kiesl, B., Seidl, M., Tompits, H., Biere, A.: Super-blocked clauses. In: Olivetti,
N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 45–61. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 5
32. Kiesl, B., Seidl, M., Tompits, H., Biere, A.: Local redundancy in SAT: generaliza-
tions of blocked clauses. Log. Methods Comput. Sci. 14(4) (2018)
33. Korhonen, T., Berg, J., Saikko, P., J¨arvisalo, M.: MaxPre: an extended MaxSAT
preprocessor. In: Gaspers, S., Walsh, T. (eds.) SAT 2017. LNCS, vol. 10491, pp.
449–456. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-66263-3 28
34. Lei, Z., Cai, S.: Solving (weighted) partial MaxSAT by dynamic local search for
SAT. In: Proceedings of the IJCAI, pp. 1346–1352 (2018). ijcai.org
35. Li, C., Many`a, F.: MaxSAT, hard and soft constraints. In: Biere, A., Heule, M., van
Maaren, H., Walsh, T. (eds.) Handbook of Satisﬁability, pp. 613–631. IOS Press
(2009)

94
H. Ihalainen et al.
36. Morgado, A., Dodaro, C., Marques-Silva, J.: Core-guided MaxSAT with soft cardi-
nality constraints. In: O’Sullivan, B. (ed.) CP 2014. LNCS, vol. 8656, pp. 564–573.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10428-7 41
37. Morgado, A., Heras, F., Liﬃton, M., Planes, J., Marques-Silva, J.: Iterative and
core-guided MaxSAT solving: a survey and assessment. Constraints 18(4), 478–534
(2013)
38. Narodytska, N., Bacchus, F.: Maximum satisﬁability using core-guided MaxSAT
resolution. In: Proceedings of the AAAI, pp. 2717–2723. AAAI Press (2014)
39. Paxian, T., Raiola, P., Becker, B.: On preprocessing for weighted MaxSAT. In:
Henglein, F., Shoham, S., Vizel, Y. (eds.) VMCAI 2021. LNCS, vol. 12597, pp.
556–577. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-67067-2 25
40. Paxian, T., Reimer, S., Becker, B.: Dynamic polynomial watchdog encoding for
solving weighted MaxSAT. In: Beyersdorﬀ, O., Wintersteiger, C.M. (eds.) SAT
2018. LNCS, vol. 10929, pp. 37–53. Springer, Cham (2018). https://doi.org/10.
1007/978-3-319-94144-8 3
41. Rebola-Pardo, A., Cruz-Filipe, L.: Complete and eﬃcient DRAT proof checking.
In: Proceedings of the FMCAD, pp. 1–9. IEEE (2018)
42. Yolcu, E., Wu, X., Heule, M.J.H.: Mycielski graphs and PR proofs. In: Pulina, L.,
Seidl, M. (eds.) SAT 2020. LNCS, vol. 12178, pp. 201–217. Springer, Cham (2020).
https://doi.org/10.1007/978-3-030-51825-7 15
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Cooperating Techniques for Solving
Nonlinear Real Arithmetic in the cvc5
SMT Solver (System Description)
Gereon Kremer1
, Andrew Reynolds2(B)
, Clark Barrett1
,
and Cesare Tinelli2
1 Stanford University, Stanford, USA
2 The University of Iowa, Iowa City, USA
andrew.j.reynolds@gmail.com
Abstract. The cvc5 SMT solver solves quantiﬁer-free nonlinear real
arithmetic problems by combining the cylindrical algebraic coverings
method with incremental linearization in an abstraction-reﬁnement loop.
The result is a complete algebraic decision procedure that leverages eﬃ-
cient heuristics for reﬁning candidate models. Furthermore, it can be used
with quantiﬁers, integer variables, and in combination with other theo-
ries. We describe the overall framework, individual solving techniques,
and a number of implementation details. We demonstrate its eﬀective-
ness with an evaluation on the SMT-LIB benchmarks.
Keywords: Satisﬁability modulo theories · Nonlinear real arithmetic ·
Abstraction reﬁnement · Cylindrical algebraic coverings
1
Introduction
SMT solvers are used as back-end engines for a wide variety of academic and
industrial applications [2,19,20]. Eﬃcient reasoning in the theory of real arith-
metic is crucial for many such applications [5,8]. While modern SMT solvers
have been shown to be quite eﬀective at reasoning about linear real arithmetic
problems [21,43], nonlinear problems are typically much more diﬃcult. This is
not surprising, given that the worst-case complexity for deciding the satisﬁabil-
ity of nonlinear real arithmetic formulas is doubly-exponential in the number
of variables in the formula [15]. Nevertheless, a variety of techniques have been
proposed and implemented, each attempting to target a class of formulas for
which reasonable performance can be observed in practice.
Related Work. All complete decision procedures for nonlinear real arithmetic
(or the theory of the reals) originate in computer algebra, the most prominent
being cylindrical algebraic decomposition (CAD) [11]. While alternatives exist
[6,25,41], they have not seen much use [27], and CAD-based methods are the only
sound and complete methods in practical use today. CAD-based methods used
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 95–105, 2022.
https://doi.org/10.1007/978-3-031-10769-6_7

96
G. Kremer et al.
in modern SMT solvers include incremental CAD implementations [34,36] and
cylindrical algebraic coverings [3], both of which are integrated in the traditional
CDCL(T) framework for SMT [40].
In contrast, the NLSAT [30] calculus and the generalized MCSAT [28,39]
framework provide for a much tighter integration of a conﬂict-driven CAD-based
theory solver into a theory-aware core solver. This has been the dominant app-
roach over the last decade due to its strong performance in practice. However,
it has the signiﬁcant disadvantage of being diﬃcult to integrate with CDCL(T)-
based frameworks for theory combination.
A number of incomplete techniques are also used by various SMT solvers:
incremental linearization [9] gradually reﬁnes an abstraction of the nonlinear
formula obtained via a naive linearization by refuting spurious models of the
abstraction; interval constraint propagation [24,36,45] employs interval arith-
metic to narrow down the search space; subtropical satisﬁability [22] provides
suﬃcient linear conditions for nonlinear solutions in the exponent space of the
polynomials; and virtual substitution [12,31,46] makes use of parametric solu-
tion formulas for polynomials of bounded degree. Though all of these techniques
have limitations, each of them is useful for certain subclasses of nonlinear real
arithmetic or in combination with other techniques.
Contributions. We present an integration of cylindrical algebraic coverings and
incremental linearization, implemented in the cvc5 SMT solver. Crucial to the
success of the integration is an abstraction-reﬁnement loop used to combine the
two techniques cooperatively. The solution is eﬀective in practice, as witnessed
by the fact that cvc5 won the nonlinear real arithmetic category of SMT-COMP
2021 [44], the ﬁrst time a non-MCSAT-based technique has won since 2013. Our
integrated technique also has the advantage of being very ﬂexible: in particular, it
ﬁts into the regular CDCL(T) schema for theory solvers and theory combination,
it supports (mixed) integer problems, and it can be easily extended using further
subsolvers that support additional arithmetic operators beyond the scope of
traditional algebraic routines (e.g., transcendental functions).
2
Nonlinear Solving Techniques
The nonlinear arithmetic solver implemented in cvc5 generally follows the
abstraction-reﬁnement framework introduced by Cimatti et al. [9] and depicted
in Fig. 1. The input assertions are ﬁrst checked by the linear arithmetic solver,
where they are linearized implicitly by treating every application of multipli-
cation as if it were an arithmetic variable. For example, given input assertions
x·y > 0 ∧x > 1∧y < 0, the linear solver treats the expression x·y as a variable.
It may then ﬁnd the (spurious) model: x →2, y →−1, and x · y →1. We call
the candidate model returned by the linear arithmetic solver, where applications
of multiplication are treated as variables, a linear model. If a linear model does
not exist, i.e., the input is unsatisﬁable according to the linear solver, the linear
solver generates a conﬂict that is immediately returned to the CDCL(T) engine.

Cooperating Techniques for Solving Nonlinear Real Arithmetic in the cvc5
97
linear solver
input assertions
SAT
lemmas / conﬂicts
check model
abstraction-reﬁnement
linear model
conﬂict
repair model
Fig. 1. Structural overview of the nonlinear solver
When a linear model does exist, we check whether it already satisﬁes the
input assertions or try to repair it to do so. We only apply a few very sim-
ple heuristics for repairs such as updating the value for z in the presence of a
constraint like z = x · y based on the values of x and y.
If the model can not be repaired, we reﬁne the abstraction for the linear
solver [9]. This step constructs lemmas, or conﬂicts, based on the input asser-
tions and the linear model, to advance the solving process by blocking either
the current linear model or the current Boolean model, that is, the propositional
assignment generated by the SMT solver’s SAT engine. The Boolean model is
usually eliminated only by the coverings approach, while the incremental lin-
earization technique generates lemmas with new literals that target the linear
model, e.g., the lemma x > 0 ∧y < 0 ⇒x·y < 0 in the example above. We next
describe our implementation of cylindrical algebraic coverings and incremental
linearization, and how they are combined in cvc5.
2.1
Cylindrical Algebraic Coverings
Cylindrical algebraic coverings is a technique recently proposed by ´Abrah´am et
al. [3] and is heavily inspired by CAD. While the way the computation proceeds
is very diﬀerent from traditional CAD, and instead somewhat similar to NLSAT
[30], their mathematical underpinnings are essentially identical. The cylindri-
cal algebraic coverings subsolver in cvc5 closely follows the presentation in [3].
Below, we discuss some diﬀerences and extensions. For this discussion, we must
refer the reader to [3] for the relevant background material because of space
constraints. We note that cvc5 relies on the libpoly library [29] to provide most
of the computational infrastructure for algebraic reasoning.
Square-Free Basis. As with most CAD projection schemas, the set of projection
polynomials needs to be a square-free basis when computing the characterization
for an interval in [3, Algorithm 4]. However, the resultants computed in this
algorithm combine polynomials from diﬀerent sets, which are not necessarily
coprime. The remedy is to either make these sets of polynomials pairwise square-
free or to fully factor all projection polynomials. We adopt the former approach.
Starting Model. Although the linear model may not satisfy the nonlinear con-
straints, we may expect it to be in the vicinity of a proper model. We thus

98
G. Kremer et al.
optionally use the linear model as an initial assignment for the cylindrical alge-
braic coverings algorithm in one of two ways: either using it initially in the search
and discarding it as soon as it conﬂicts; or using it whenever possible, even if
it leads to a conﬂict in another branch of the search. Unfortunately, neither
technique has any discernible impact in our experiments.
Interval Pruning. As already noted in [3], a covering may contain two kinds
of redundant intervals: intervals fully contained in another interval, or intervals
contained in the union of other intervals. Removing the former kind of redundan-
cies is not only clearly beneﬁcial, but also required for how the characterizations
are computed. It is not clear, however, if it is worthwhile to remove redundancies
of the second kind because, while it can simplify the characterization locally, it
may also make the resulting interval smaller, slowing down the overall solving
process. Note that there may not be a unique redundant interval: e.g., if multi-
ple intervals overlap, it may be possible to remove one of two intervals, but not
both of them. We have implemented a simple heuristic to detect redundancies
of the second kind, always removing the smallest interval with respect to the
interval ordering given in [3]. Even if these redundancies occur in about 7.5%
of all QF NRA benchmarks, using this technique has only a very limited impact.
It may be that for certain kinds of benchmarks, underrepresented in SMT-LIB,
the technique is valuable. Or it may be that some variation of the technique is
more broadly helpful. These are interesting directions for future work.
Lifting and Coeﬃcient Selection with Lazard. The original cylindrical algebraic
coverings technique is based on McCallum’s projection operator [37], which is
particularly well-studied, but also (refutationally) unsound: polynomial nulliﬁ-
cation may occur when computing the real roots, possibly leading to the loss of
real roots and thus solution candidates. One then needs to check for these cases
and fall back to a more conservative, albeit more costly, projection schema such
as those due to Collins [11] or Hong [26].
Lazard’s projection schema [35], which has been proven correct only recently
[38], provides very small projection sets and is both sound and complete. This
comes at the price of a diﬀerent mathematical background and a modiﬁed lifting
procedure, which corresponds to a modiﬁed procedure for real root isolation.
Although the local projections employed in cylindrical algebraic coverings have
not been formally veriﬁed for Lazard’s projection schema yet, we expect no
signiﬁcant issues there. Adopting it seems to be a logical improvement, as already
mentioned in [3]. The modiﬁed real root isolation procedure is a signiﬁcant hurdle
in practice, as it requires additional nontrivial algorithms [32, Section 5.3.2]. We
implemented it using CoCoALib [1] in cvc5 [33], achieving soundness without
any discernible negative performance impact.
Using Lazard’s projection schema, for all its beneﬁts, may seem questionable
for the following reasons: (i) the unsoundness of McCallum’s projection operator
is virtually never witnessed in practice [32,33, Section 6.5], and (ii) the projection
sets computed by Lazard’s and McCallums’s projection operator are identical
on more than 99.5% on all of QF NRA [33]. We argue, though, that working in

Cooperating Techniques for Solving Nonlinear Real Arithmetic in the cvc5
99
the domain of formal veriﬁcation warrants the eﬀort of obtaining a (provably)
correct result, especially if it does not incur a performance overhead.
Proof Generation. Recently, generating formal proofs to certify the result of SMT
solvers has become an area of focus. In particular, there is a large and ongoing
eﬀort to produce proofs in cvc5. The incremental linearization approach can be
seen as an oracle which produces lemmas that are easy to prove individually, so
cvc5 does generate proofs for them; the complex part is ﬁnding those lemmas
and making sure they actually help the solver make progress.
The situation is very diﬀerent for cylindrical algebraic coverings: the pro-
duced lemma is the infeasible subset, and we usually have no simpler proof than
the computations relying on CAD theory. That said, cylindrical algebraic cover-
ings appear to be more amenable to automatic proof generation than traditional
CAD-based approaches [4,14]. In fact, although making these proofs detailed
enough for automated veriﬁcation is still an open problem, they are already bro-
ken into smaller parts that closely follow the tree-shaped computation of the
algorithm. This allows cvc5 to produce at least a proof skeleton in that case.
2.2
Incremental Linearization
Our theory solver for nonlinear (real) arithmetic optionally uses lemma schemas
following the incremental linearization approaches described by Cimatti et al. [9]
and Reynolds et al. [42]. These schemas incrementally reﬁne candidate models
from the linear arithmetic solver by introducing selected quantiﬁer-free lemmas
that express properties of multiplication, such as signedness (e.g., x > 0 ∧y >
0 ⇒x·y > 0) or monotonicity (e.g., |x| > |y| ⇒x·x > y·y). They are generated
as needed to refute spurious models that violate these properties.
Most lemma schemas built-in in cvc5 are crafted so as to avoid introducing
new monomial terms or coeﬃcients, since that could lead to non-termination in
the CDCL(T) search. As a notable exception, we rely on a lemma schema for
tangent planes for multiplication [9], which can be used to refute the candidate
model for any application of the multiplication operator · whose value in the
linear model is inconsistent with the standard interpretation of ·. Note that
since these lemmas depend upon the current model value chosen for arithmetic
variables, tangent plane lemmas may introduce an unbounded number of new
literals into the search. The set of lemma schemas used by the solver is user-
conﬁgurable, as described in the following section.
2.3
Strategy
The overall theory solver for nonlinear arithmetic is built from several subsolvers,
implementing the techniques described above, using a rather naive strategy,
as summarized in Algorithm 1. After a spurious linear model has been con-
structed that cannot be repaired, we ﬁrst apply a subset of the lemma schemas
that do not introduce an unbounded number of new terms (with procedure
IncLinearizationLight); then, we continue with the remaining lemma schemas

100
G. Kremer et al.
1 Function NlSolve(assertions)
2
if not LinearSolve(assertions) then return linear conﬂict
3
M = linear model for assertions
4
if RepairModel(assertions, M ) then return repaired model
5
if IncLinearizationLight(assertions, M ) then return lemmas
6
if IncLinearizationFull(assertions, M ) then return lemmas
7
return Coverings(assertions, M )
Algorithm 1: Strategy for nonlinear arithmetic solver
(with procedure IncLinearizationFull); ﬁnally, we resort to the coverings
solver which is guaranteed to ﬁnd either a conﬂict or a model. Internally, each
procedure sequentially tries its assigned lemma schemas from [9,42] until it con-
structs a lemma that can block the spurious model.
The approach is dynamically conﬁgured based on input options and the logic
of the input formula. For example, by default, we disable IncLinearizationFull
for QF NRA as it tends to diverge in cases where the coverings solver quickly
terminates.
2.4
Beyond QF NRA
The presented solver primarily targets quantiﬁer-free nonlinear real arithmetic,
but is used also in the presence of quantiﬁers and with multiple theories.
Quantiﬁed Logics. Solving quantiﬁed logics for nonlinear arithmetic requires solv-
ing quantiﬁer-free subproblems, and thus any improvement to quantiﬁer-free
solving also beneﬁts solving with quantiﬁers. In practice, however, the instanti-
ation heuristics are just as important for overall solver performance.
Multiple Theories. The theory combination framework as implemented in cvc5
requires evaluating equalities over the combined model. To support this func-
tionality, real algebraic numbers had to be properly integrated into the entire
solver; in particular, the ability to compute with these numbers could not be
local to the cylindrical algebraic coverings module or even the nonlinear solver.
3
Experimental Results
We evaluate our implementation within cvc5 (commit id 449dd7e) in comparison
with other SMT solvers on all 11552 benchmarks in the quantiﬁer-free nonlinear
real arithmetic (QF NRA) logic of SMT-LIB. We consider three conﬁgurations of
cvc5, each of which runs a subset of steps from Algorithm 1. All the conﬁgura-
tions run lines 2–4. In addition, cvc5.cov runs line 7, cvc5.inclin runs lines
5 and 6, and cvc5 runs lines 5 and 7. All experiments were conducted on Intel
Xeon E5-2637v4 CPUs with a time limit of 20 min and 8 GB memory.
We compare cvc5 with recent versions of all other SMT solvers that partici-
pated in the QF NRA logic of SMT-COMP 2021 [44]: MathSAT 5.6.6 [10], SMT-RAT
19.10.560 [13], veriT [7] (veriT+raSAT+Redlog), Yices2 2.6.4 [18] (Yices-QS for

Cooperating Techniques for Solving Nonlinear Real Arithmetic in the cvc5
101
quantiﬁed logics), and z3 4.8.14 [16]. MathSAT employs an abstraction-reﬁnement
mechanism very similar to the one described in Sect. 2.2; veriT [23] forwards non-
linear arithmetic problems to the external tools raSAT [45], which uses interval
constraint propagation, and Redlog/Reduce [17], which focuses on virtual sub-
stitution and cylindrical algebraic decomposition; SMT-RAT, Yices2, and z3 all
implement some variant of MCSAT [30]. Note that SMT-RAT also implements the
cylindrical algebraic coverings approach, but it is less eﬀective than SMT-RAT’s
adaptation of MCSAT [3].
QF_NRA
sat unsat solved
cvc5
5137 5596 10733
Yices2
4966 5450 10416
z3
5136 5207 10343
cvc5.cov
5001 5077 10078
SMT-RAT
4828 5038
9866
veriT
4522 5034
9556
MathSAT
3645 5357
9002
cvc5.inclin
3421 5376
8797
(a)
Beyond QF_NRA
sat unsat solved
NRA
Yices2
231 3817 4048
z3
236 3812 4048
cvc5.cov
236 3809
4045
cvc5
221 3809
4030
cvc5.inclin
120 3786
3906
QF_UFNRA z3
24
11
35
Yices2
23
11
34
cvc5
20
11
31
cvc5.inclin
12
11
23
cvc5.cov
2
11
13
(b)
Fig. 2. (a) Experiments for QF NRA (b) Experiments for NRA and QF UFNRA
Figure 2a shows that cvc5 signiﬁcantly outperforms all other QF NRA
solvers. Both the coverings approach (cvc5.cov) and the incremental lineariza-
tion approach (cvc5.inclin) contribute substantially to the overall perfor-
mance of the uniﬁed solver in cvc5, with coverings solving many satisﬁable
instances, and incremental linearization helping on unsatisﬁable ones. Even
though cvc5.inclin closely follows [9], it outperforms MathSAT on unsatisﬁ-
able benchmarks, those where cvc5 relies on incremental linearization the most.
Comparing cvc5 and Yices2 is particularly interesting, as the coverings app-
roach in cvc5 and the NLSAT solver in Yices2 both rely on libpoly [29], thus
using the same implementation of algebraic numbers and operations over them.
Our integration of incremental linearization and algebraic coverings is compat-
ible with the traditional CDCL(T) framework and outperforms the alternative
NLSAT approach, which is specially tailored to nonlinear real arithmetic.
Going beyond QF NRA, we also evaluate the performance of our solver in
the context of theory combination (with all 37 benchmarks from QF UFNRA) and
quantiﬁers (with all 4058 benchmarks from NRA). There, cvc5 is a close runner-up
to Yices2 and z3, thanks to the coverings subsolver which signiﬁcantly improves
cvc5’s performance. We conjecture that the remaining gap is due to components
other than the nonlinear arithmetic solver, such as the solver for equality and
uninterpreted functions, details of theory combination, or quantiﬁer instantiation

102
G. Kremer et al.
heuristics. Interestingly, the sets of unsolved instances in NRA are almost disjoint
for cvc5.cov, Yices2 and z3, indicating that each tool could solve the remaining
benchmarks with reasonable extra eﬀort.
4
Conclusion
We have presented an approach for solving quantiﬁer-free nonlinear real
arithmetic problems that combines previous approaches based on incremen-
tal linearization [9] and cylindrical algebraic coverings [3] into one coherent
abstraction-reﬁnement loop. The resulting implementation is very eﬀective, out-
performing other state-of-the-art solver implementations, and integrates seam-
lessly in the CDCL(T) framework.
The general approach also applies to integer problems, quantiﬁed formulas,
and instances with multiple theories, and can additionally be used in combina-
tion with transcendental functions [9] and bitwise conjunction for integers [47].
Further evaluations of these combinations are left to future work.
References
1. Abbott, J., Bigatti, A.M., Palezzato, E.: New in CoCoA-5.2.4 and CoCoALib-
0.99600 for SC-square. In: Satisﬁability Checking and Symbolic Computation.
CEUR Workshop Proceedings, vol. 2189, pp. 88–94. CEUR-WS.org (2018). http://
ceur-ws.org/Vol-2189/paper4.pdf
2. ´Abrah´am, E., Corzilius, F., Johnsen, E.B., Kremer, G., Mauro, J.: Zephyrus2: on
the ﬂy deployment optimization using SMT and CP technologies. In: Fr¨anzle, M.,
Kapur, D., Zhan, N. (eds.) SETTA 2016. LNCS, vol. 9984, pp. 229–245. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-47677-3 15
3. ´Abrah´am, E., Davenport, J.H., England, M., Kremer, G.: Deciding the consistency
of non-linear real arithmetic constraints with a conﬂict driven search using cylindri-
cal algebraic coverings. J. Logic. Algebr. Methods Program. 119(100633) (2021).
https://doi.org/10.1016/j.jlamp.2020.100633
4. Abrah´am, E., Davenport, J.H., England, M., Kremer, G.: Proving UNSAT
in SMT: the case of quantiﬁer free non-linear real arithmetic. arXiv preprint
arXiv:2108.05320 (2021)
5. Arnett, T.J., Cook, B., Clark, M., Rattan, K.: Fuzzy logic controller stability
analysis using a satisﬁability modulo theories approach. In: 19th AIAA Non-
Deterministic Approaches Conference, p. 1773 (2017)
6. Basu, S., Pollack, R., Roy, M.F.: On the combinatorial and algebraic complexity
of quantiﬁer elimination. J. ACM 43, 1002–1045 (1996). https://doi.org/10.1145/
235809.235813
7. Bouton, T., Caminha B. de Oliveira, D., D´eharbe, D., Fontaine, P.: veriT: an open,
trustable and eﬃcient SMT-solver. In: Schmidt, R.A. (ed.) CADE 2009. LNCS
(LNAI), vol. 5663, pp. 151–156. Springer, Heidelberg (2009). https://doi.org/10.
1007/978-3-642-02959-2 12
8. Cashmore, M., Magazzeni, D., Zehtabi, P.: Planning for hybrid systems via satisﬁ-
ability modulo theories. J. Artif. Intell. Res. 67, 235–283 (2020). https://doi.org/
10.1613/jair.1.11751

Cooperating Techniques for Solving Nonlinear Real Arithmetic in the cvc5
103
9. Cimatti, A., Griggio, A., Irfan, A., Roveri, M., Sebastiani, R.: Incremental lin-
earization for satisﬁability and veriﬁcation modulo nonlinear arithmetic and tran-
scendental functions. ACM Trans. Comput. Logic 19, 19:1–19:52 (2018). https://
doi.org/10.1145/3230639
10. Cimatti, A., Griggio, A., Schaafsma, B.J., Sebastiani, R.: The MathSAT5 SMT
solver. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp.
93–107. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36742-7 7
11. Collins, G.E.: Quantiﬁer elimination for real closed ﬁelds by cylindrical algebraic
decompostion. In: Brakhage, H. (ed.) GI-Fachtagung 1975. LNCS, vol. 33, pp.
134–183. Springer, Heidelberg (1975). https://doi.org/10.1007/3-540-07407-4 17
12. Corzilius, F., ´Abrah´am, E.: Virtual substitution for SMT-solving. In: Owe, O.,
Steﬀen, M., Telle, J.A. (eds.) FCT 2011. LNCS, vol. 6914, pp. 360–371. Springer,
Heidelberg (2011). https://doi.org/10.1007/978-3-642-22953-4 31
13. Corzilius, F., Kremer, G., Junges, S., Schupp, S., ´Abrah´am, E.: SMT-RAT: an open
source C++ toolbox for strategic and parallel SMT solving. In: Heule, M., Weaver,
S. (eds.) SAT 2015. LNCS, vol. 9340, pp. 360–368. Springer, Cham (2015). https://
doi.org/10.1007/978-3-319-24318-4 26
14. Davenport, J., England, M., Kremer, G., Tonks, Z., et al.: New opportunities for
the formal proof of computational real geometry? arXiv preprint arXiv:2004.04034
(2020)
15. Davenport, J.H., Heintz, J.: Real quantiﬁer elimination is doubly exponen-
tial. J. Symb. Comput. 5(1), 29–35 (1988). https://doi.org/10.1016/S0747-
7171(88)80004-X
16. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
17. Dolzmann, A., Sturm, T.: REDLOG: computer algebra meets computer logic. ACM
SIGSAM Bull. 31(2), 2–9 (1997). https://doi.org/10.1145/261320.261324
18. Dutertre, B.: Yices 2.2. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 737–744. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9 49
19. Ermon, S., Le Bras, R., Gomes, C.P., Selman, B., van Dover, R.B.: SMT-aided
combinatorial materials discovery. In: Cimatti, A., Sebastiani, R. (eds.) SAT 2012.
LNCS, vol. 7317, pp. 172–185. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-31612-8 14
20. Fagerberg, R., Flamm, C., Merkle, D., Peters, P.: Exploring chemistry using SMT.
In: Milano, M. (ed.) CP 2012. LNCS, pp. 900–915. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-33558-7 64
21. Faure, G., Nieuwenhuis, R., Oliveras, A., Rodr´ıguez-Carbonell, E.: SAT modulo
the theory of linear arithmetic: exact, inexact and commercial solvers. In: Kleine
B¨uning, H., Zhao, X. (eds.) SAT 2008. LNCS, vol. 4996, pp. 77–90. Springer,
Heidelberg (2008). https://doi.org/10.1007/978-3-540-79719-7 8
22. Fontaine, P., Ogawa, M., Sturm, T., Vu, X.T.: Subtropical satisﬁability. In: Dixon,
C., Finger, M. (eds.) FroCoS 2017. LNCS (LNAI), vol. 10483, pp. 189–206.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-66167-4 11
23. Fontaine, P., Ogawa, M., Sturm, T., Vu, X.T., et al.: Wrapping computer algebra is
surprisingly successful for non-linear SMT. In: SC-square 2018-Third International
Workshop on Satisﬁability Checking and Symbolic Computation (2018)
24. Gao, S., Kong, S., Clarke, E.M.: dReal: an SMT solver for nonlinear theories over
the reals. In: Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp. 208–
214. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-38574-2 14

104
G. Kremer et al.
25. Grigor’ev, D.Y., Vorobjov, N.: Solving systems of polynomial inequalities in subex-
ponential time. J. Symb. Comput. 5, 37–64 (1988). https://doi.org/10.1016/S0747-
7171(88)80005-1
26. Hong, H.: An improvement of the projection operator in cylindrical algebraic
decomposition. In: International Symposium on Symbolic and Algebraic Compu-
tation, pp. 261–264 (1990). https://doi.org/10.1145/96877.96943
27. Hong, H.: Comparison of several decision algorithms for the existential theory of
the reals. RES report, Johannes Kepler University (1991)
28. Jovanovi´c, D., Barrett, C., de Moura, L.: The design and implementation of the
model constructing satisﬁability calculus. In: Formal Methods in Computer-Aided
Design, pp. 173–180. IEEE (2013). https://doi.org/10.1109/FMCAD.2013.7027033
29. Jovanovic, D., Dutertre, B.: LibPoly: a library for reasoning about polynomials. In:
Satisﬁability Modulo Theories. CEUR Workshop Proceedings, vol. 1889. CEUR-
WS.org (2017). http://ceur-ws.org/Vol-1889/paper3.pdf
30. Jovanovi´c, D., de Moura, L.: Solving non-linear arithmetic. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 339–354. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 27
31. Koˇsta, M., Sturm, T.: A generalized framework for virtual substitution. CoRR
abs/1501.05826 (2015)
32. Kremer, G.: Cylindrical algebraic decomposition for nonlinear arithmetic problems.
Ph.D. thesis, RWTH Aachen University (2020). https://doi.org/10.18154/RWTH-
2020-05913
33. Kremer, G., Brandt, J.: Implementing arithmetic over algebraic numbers: a tutorial
for Lazard’s lifting scheme in CAD. In: Symbolic and Numeric Algorithms for Sci-
entiﬁc Computing, pp. 4–10 (2021). https://doi.org/10.1109/SYNASC54541.2021.
00013
34. Kremer, G., ´Abrah´am, E.: Fully incremental cylindrical algebraic decomposition.
J. Symb. Comput. 100, 11–37 (2020). https://doi.org/10.1016/j.jsc.2019.07.018
35. Lazard, D.: An improved projection for cylindrical algebraic decomposition. In:
Bajaj, C.L. (ed.) Algebraic Geometry and Its Applications, pp. 467–476. Springer,
New York (1994). https://doi.org/10.1007/978-1-4612-2628-4 29
36. Loup, U., Scheibler, K., Corzilius, F., ´Abrah´am, E., Becker, B.: A symbiosis of inter-
val constraint propagation and cylindrical algebraic decomposition. In: Bonacina,
M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp. 193–207. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-38574-2 13
37. McCallum, S.: An improved projection operation for cylindrical algebraic decom-
position. In: Caviness, B.F. (ed.) EUROCAL 1985. LNCS, vol. 204, pp. 277–278.
Springer, Heidelberg (1985). https://doi.org/10.1007/3-540-15984-3 277
38. McCallum, S., Parusi´nski, A., Paunescu, L.: Validity proof of Lazard’s method for
cad construction. J. Symb. Comput. 92, 52–69 (2019). https://doi.org/10.1016/j.
jsc.2017.12.002
39. de Moura, L., Jovanovi´c, D.: A model-constructing satisﬁability calculus. In: Gia-
cobazzi, R., Berdine, J., Mastroeni, I. (eds.) VMCAI 2013. LNCS, vol. 7737, pp.
1–12. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-35873-9 1
40. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories:
from an abstract Davis-Putnam-Logemann-Loveland procedure to DPLL(T). J.
ACM 53(6), 937–977 (2006). https://doi.org/10.1145/1217856.1217859
41. Renegar, J.: A faster PSPACE algorithm for deciding the existential theory of the
reals. In: Symposium on Foundations of Computer Science, pp. 291–295 (1988).
https://doi.org/10.1109/SFCS.1988.21945

Cooperating Techniques for Solving Nonlinear Real Arithmetic in the cvc5
105
42. Reynolds, A., Tinelli, C., Jovanovi´c, D., Barrett, C.: Designing theory solvers with
extensions. In: Dixon, C., Finger, M. (eds.) FroCoS 2017. LNCS (LNAI), vol. 10483,
pp. 22–40. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-66167-4 2
43. Roselli, S.F., Bengtsson, K., ˚Akesson, K.: SMT solvers for job-shop scheduling
problems: models comparison and performance evaluation. In: International Con-
ference on Automation Science and Engineering (CASE), pp. 547–552 (2018).
https://doi.org/10.1109/COASE.2018.8560344
44. SMT-COMP 2021 (2021). https://smt-comp.github.io/2021/
45. Tung, V.X., Van Khanh, T., Ogawa, M.: raSAT: an SMT solver for polynomial
constraints. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol.
9706, pp. 228–237. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
40229-1 16
46. Weispfenning, V.: Quantiﬁer elimination for real algebra-the quadratic case and
beyond. Appl. Algebra Eng. Commun. Comput. 8(2), 85–101 (1997). https://doi.
org/10.1007/s002000050055
47. Zohar, Y., et al.: Bit-precise reasoning via Int-blasting. In: Finkbeiner, B., Wies,
T. (eds.) VMCAI 2022. LNCS, vol. 13182, pp. 496–518. Springer, Cham (2022).
https://doi.org/10.1007/978-3-030-94583-1 24
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Preprocessing of Propagation Redundant Clauses
Joseph E. Reeves(B)
, Marijn J. H. Heule
, and Randal E. Bryant
Carnegie Mellon University, Pittsburgh, PA, USA
{jereeves,mheule,randy.bryant}@cs.cmu.edu
Abstract. The propagation redundant (PR) proof system generalizes the resolu-
tion and resolution asymmetric tautology proof systems used by conﬂict-driven
clause learning (CDCL) solvers. PR allows short proofs of unsatisﬁability for
some problems that are difﬁcult for CDCL solvers. Previous attempts to auto-
mate PR clause learning used hand-crafted heuristics that work well on some
highly-structured problems. For example, the solver SADICAL incorporates PR
clause learning into the CDCL loop, but it cannot compete with modern CDCL
solvers due to its fragile heuristics. We present PRELEARN, a preprocessing tech-
nique that learns short PR clauses. Adding these clauses to a formula reduces the
search space that the solver must explore. By performing PR clause learning as
a preprocessing stage, PR clauses can be found efﬁciently without sacriﬁcing
the robustness of modern CDCL solvers. On a large portion of SAT competi-
tion benchmarks we found that preprocessing with PRELEARN improves solver
performance. In addition, there were several satisﬁable and unsatisﬁable formu-
las that could only be solved after preprocessing with PRELEARN. PRELEARN
supports proof logging, giving a high level of conﬁdence in the results.
1
Introduction
Conﬂict-driven clause learning (CDCL) [27] is the standard paradigm for solving the
satisﬁability problem (SAT) in propositional logic. CDCL solvers learn clauses implied
through resolution inferences. Additionally, all competitive CDCL solvers use pre- and
in-processing techniques captured by the resolution asymmetric tautology (RAT) proof
system [21]. As examples, the well-studied pigeonhole and mutilated chessboard prob-
lems are challenging benchmarks with exponentially-sized resolution proofs [1,12]. It
is possible to construct small hand-crafted proofs for the pigeonhole problem using
extended resolution (ER) [8], a proof system that allows the introduction of new vari-
ables [32]. ER can be expressed in RAT but has proved difﬁcult to automate due to the
large search space. Even with modern inprocessing techniques, many CDCL solvers
struggle on these seemingly simple problems. The propagation redundant (PR) proof
system allows short proofs for these problems [14,15], and unlike in ER, no new vari-
ables are required. This makes PR an attractive candidate for automation.
At a high level, CDCL solvers make decisions that typically yield an unsatisﬁable
branch of a problem. The clause that prunes the unsatisﬁable branch from the search
space is learned, and the solver continues by searching another branch. PR extends this
The authors are supported by the NSF under grant CCF-2108521.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 106–124, 2022.
https://doi.org/10.1007/978-3-031-10769-6_8

Preprocessing of Propagation Redundant Clauses
107
paradigm by allowing more aggressive pruning. In the PR proof system a branch can
be pruned as long as there exists another branch that is at least as satisﬁable. As an ex-
ample, consider the mutilated chessboard. The mutilated chessboard problem involves
ﬁnding a covering of 2 × 1 dominos on an n × n chessboard with two opposite cor-
ners removed (see Section 5.4). Given two horizontally oriented dominoes covering a
2 × 2 square, two vertically oriented dominos could cover the same 2 × 2 square. For
any solution that uses the dominos in the horizontal orientation, replacing them with the
dominos in the vertical orientation would also be a solution. The second orientation is as
satisﬁable as the ﬁrst, and so the ﬁrst can be pruned from the search space. Even though
the number of possible solutions may be reduced, the pruning is satisﬁability preserv-
ing. This is a powerful form of reasoning that can efﬁciently remove many symmetries
from the mutilated chessboard, making the problem much easier to solve [15].
The satisfaction-driven clause learning (SDCL) solver SADICAL [16] incorporates
PR clause learning into the CDCL loop. SADICAL implements hand-crafted decision
heuristics that exploit the canonical structure of the pigeonhole and mutilated chess-
board problems to ﬁnd short proofs. However, SADICAL’s performance deteriorates
under slight variations to the problems including different constraint encodings [7].
The heuristics were developed from a few well-understood problems and do not gener-
alize to other problem classes. Further, the heuristics for PR clause learning are likely
ill-suited for CDCL, making the solver less robust.
In this paper, we present PRELEARN, a preprocessing technique for learning PR
clauses. PRELEARN alternates between ﬁnding and learning PR clauses. We develop
multiple heuristics for ﬁnding PR clauses and multiple conﬁgurations for learning some
subset of the found PR clauses. As PR clauses are learned we use failed literal prob-
ing [11] to ﬁnd unit clauses implied by the formula. The preprocessing is made efﬁcient
by taking advantage of the inner/outer solver framework in SADICAL. The learned PR
clauses are added to the original formula, aggressively pruning the search space in an ef-
fort to guide CDCL solvers to short proofs. With this method PR clauses can be learned
without altering the complex heuristics that make CDCL solvers robust. PRELEARN
focuses on ﬁnding short PR clauses and failed literals to effectively reduce the search
space. This is done with general heuristics that work across a wide range of problems.
Most SAT solvers support logging proofs of unsatisﬁability for independent check-
ing [17,20,33]. This has proved valuable for verifying solutions independent of a (po-
tentially buggy) solver. Modern SAT solvers log proofs in the DRAT proof system
(RAT [21] with deletions). DRAT captures all widely used pre- and in-processing tech-
niques including bounded variable elimination [10], bounded variable addition [26],
and extended learning [4,32]. DRAT can express the common symmetry-breaking tech-
niques, but it is complicated [13]. PR can compactly express some symmetry-breaking
techniques [14,15], yielding short proofs that can be checked by the proof checker
DPR-TRIM [16]. PR gives a framework for strong symmetry-breaking inferences and
also maintains the highly desirable ability to independently verify proofs.
The contributions of this paper include: (1) giving a high-level algorithm for ex-
tracting PR clauses, (2) implementing several heuristics for ﬁnding and learning PR
clauses, (3) evaluating the effectiveness of different heuristic conﬁgurations, and (4)
assessing the impact of PRELEARN on solver performance. PRELEARN improves the

108
J. E. Reeves et al.
performance of the CDCL solver KISSAT on a quarter of the satisﬁable and unsatisﬁable
competition benchmarks we considered. The improvement is signiﬁcant for a number
of instances that can only be solved by KISSAT after preprocessing. Most of them come
from hard combinatorial problems with small formulas. In addition, PRELEARN di-
rectly produces refutation proofs for the mutilated chessboard problem containing only
unit and binary PR clauses.
2
Preliminaries
We consider propositional formulas in conjunctive normal form (CNF). A CNF formula
ψ is a conjunction of clauses where each clause is a disjunction of literals. A literal l is
either a variable x (positive literal) or a negated variable x (negative literal). For a set
of literals L the formula ψ(L) is the clauses {C ∈ψ | C ∩L ̸= ∅}.
An assignment is a mapping from variables to truth values 1 (true) and 0 (false).
An assignment is total if it assigns every variable to a value, and partial if it assigns a
subset of variables to values. The set of variables occurring in a formula, assignment,
or clause is given by var(ψ), var(α), or var(C). For a literal l, var(l) is a variable.
An assignment α satisﬁes a positive (negative) literal l if α maps var(l) to true (α
maps var(l) to false, respectively), and falsiﬁes it if α maps var(l) to false (α maps
var(l) to true, respectively). We write a ﬁnite partial assignment as the set of literals it
satisﬁes. An assignment satisﬁes a clause if the clause contains a literal satisﬁed by the
assignment. An assignment satisﬁes a formula if every clause in the formula is satisﬁed
by the assignment. A formula is satisﬁable if there exists a satisfying assignment, and
unsatisﬁable otherwise. Two formula are logically equivalent if they share the same set
of satisfying assignments. Two formulas are satisﬁability equivalent if they are either
both satisﬁable or both unsatisﬁable.
If an assignment α satisﬁes a clause C we deﬁne C |α = ⊤, otherwise C |α repre-
sents the clause C with the literals falsiﬁed by α removed. The empty clause is denoted
by ⊥. The formula ψ reduced by an assignment α is given by ψ|α = {C |α | C ∈
ψ and C |α ̸= ⊤}. Given an assignment α = l1 . . . ln, C = (l1 ∨· · · ∨ln) is the clause
that blocks α. The assignment blocked by a clause is the negation of the literals in the
clause. The literals touched by an assignment is deﬁned by touchedα(C) = {l | l ∈
C and var(l) ∈var(α)} for a clause. For a formula ψ, touchedα(ψ) is the union of
touched variables for each clause in ψ. A unit is a clause containing a single literal.
The unit clause rule takes the assignment α of all units in a formula ψ and generates
ψ|α. Iteratively applying the unit clause rule until ﬁxpoint is referred to as unit propa-
gation. In cases where unit propagation yields ⊥we say it derived a conﬂict. A formula
ψ implies a formula ψ′, denoted ψ |= ψ′, if every assignment satisfying ψ satisﬁes ψ′.
By ψ ⊢1 ψ′ we denote that for every clause C ∈ψ′, applying unit propagation to the
assignment blocked by C in ψ derives a conﬂict. If unit propagation derives a conﬂict
on the formula ψ∪{{l}}, we say l is a failed literal and the unit l is logically implied by
the formula. Failed literal probing [11] is the process of successively assigning literals
to check if units are implied by the formula. In its simplest form, probing involves as-
signing a literal l and learning the unit l if unit propagation derives a conﬂict, otherwise
l is unassigned and the next literal is checked.

Preprocessing of Propagation Redundant Clauses
109
To evaluate the satisﬁability of a formula, a CDCL solver [27] iteratively performs
the following operations: First, the solver performs unit propagation, then tests for a
conﬂict. Unit propagation is made efﬁcient with two-literal watch pointers [28]. If there
is no conﬂict and all variables are assigned, the formula is satisﬁable. Otherwise, the
solver chooses an unassigned variable through a variable decision heuristic [6,25], as-
signs a truth value to it, and performs unit propagation. If, however, there is a conﬂict,
the solver performs conﬂict analysis potentially learning a short clause. In case this
clause is the empty clause, the formula is unsatisﬁable.
3
The PR Proof System
A clause C is redundant w.r.t. a formula ψ if ψ and ψ∪{C} are satisﬁability equivalent.
The clause sequence ψ, C1, C2, . . . , Cn is a clausal proof of Cn if each clause Ci (1 ≤
i ≤n) is redundant w.r.t. ψ ∪{C1, C2, . . . , Ci−1}. The proof is a refutation of ψ if Cn
is ⊥. Clausal proof systems may also allow deletion. In a refutation proof clauses can
be deleted freely because the deletion cannot make a formula less satisﬁable.
Clausal proof systems are distinguished by the kinds of redundant clauses they allow
to be added. The standard SAT solving paradigm CDCL learns clauses implied through
resolution. These clauses are logically implied by the formula, and fall under the reverse
unit propagation (RUP) proof system. The Resolution Asymmetric Tautology (RAT)
proof system generalizes RUP. All commonly used inprocessing techniques emit DRAT
proofs. The propagation redundant (PR) proof system generalizes RAT by allowing the
pruning of branches without loss of satisfaction.
Let C be a clause in the formula ψ and α the assignment blocked by C. Then C is
PR w.r.t. ψ if and only if there exists an assignment ω such that ψ|α ⊢1 ψ|ω and ω
satisﬁes C. Intuitively, this allows inferences that block a partial assignment α as long
as another assignment ω is as satisﬁable. This means every assignment containing α
that satisﬁes ψ can be transformed to an assignment containing ω that satisﬁes ψ.
Clausal proofs systems must be checkable in polynomial time to be useful in prac-
tice. RUP and RAT are efﬁciently checkable due to unit propagation. In general, deter-
mining if a clause is PR is an NP-complete problem [18]. However, a PR proof is check-
able in polynomial time if the witness assignments ω are included. A clausal proof with
witnesses will look like ψ, (C1, ω1), (C2, ω2), . . . , (Cn, ωn). The proof checker DPR-
TRIM can efﬁciently check PR proofs that include witnesses. Further, DPR-TRIM can
emit proofs in the LPR format. They can be validated by the formally-veriﬁed checker
CAKE-LPR [31], which was used to validate results in recent SAT competitions.
4
Pruning Predicates and SADICAL
Determining if a clause is PR is NP-complete and can naturally be formulated in SAT.
Given a clause C and formula ψ, a pruning predicate is a formula such that if it is
satisﬁable, the clause C is redundant w.r.t. ψ. SADICAL uses two pruning predicates
to determine if a clause is PR: positive reduct and ﬁltered positive reduct. If either
predicate is satisﬁable, the satisfying assignment serves as the witness showing the
clause is PR.

110
J. E. Reeves et al.
Given a formula ψ and assignment α, the positive reduct is the formula G ∧C
where C is the clause that blocks α and G = {touchedα(D) | D ∈ψ and D|α = ⊤}.
If the positive reduct is satisﬁable, the clause C is PR w.r.t. ψ. The positive reduct is
satisﬁable iff the clause blocked by α is a set-blocked clause [23].
Given a formula ψ and assignment α, the ﬁltered positive reduct is the formula G∧C
where C is the clause that blocks α and G = {touchedα(D) | D ∈ψ and D|α ⊬1
touchedα(D)}. If the ﬁltered positive reduct is satisﬁable, the clause C is PR w.r.t. ψ.
The ﬁltered positive reduct is a subset of the positive reduct and is satisﬁable iff the
clause blocked by α is a set-propagation redundant clause [14]. Example 1 shows a
formula for which the positive and ﬁltered positive reducts are different, and only the
ﬁltered positive reduct is satisﬁable.
Example 1. Given the formula (x1 ∨x2) ∧(x1 ∨x2), the positive reduct with α = x1
is (x1) ∧(x1), which is unsatisﬁable. The clause (x1) can be ﬁltered, giving the ﬁltered
positive reduct (x1), which is satisﬁable.
SADICAL [16] uses satisfaction-driven clause learning (SDCL) that extends CDCL
by learning PR clauses [18] based on (ﬁltered) positive reducts. SADICAL uses an in-
ner/outer solver framework. The outer solver attempts to solve the SAT problem with
SDCL. SDCL diverges from the basic CDCL loop when unit propagation after a deci-
sion does not derive a conﬂict. In this case a reduct is generated using the current as-
signment, and the inner solver attempts to solve the reduct using CDCL. If the reduct is
satisﬁable, the PR clause blocking the current assignment is learned, and the SDCL loop
continues. The PR clause can be simpliﬁed by removing all non-decision variables from
the assignment. SADICAL emits PR proofs by logging the satisfying assignment of the
reduct as the witness, and these proofs are veriﬁed with DPR-TRIM. The key to SADI-
CAL ﬁnding good PR clauses leading to short proofs is the decision heuristic, because
variable selection builds the candidate PR clauses. Hand-crafted decision heuristics en-
able SADICAL to ﬁnd short proofs on pigeonhole and mutilated chessboard problems.
However, these heuristics differ signiﬁcantly from the score-based heuristics in most
CDCL solvers. Our experiences with SaCiDaL suggest that improving the heuristics
for SDCL reduces the performance of CDCL and the other way around. This may ex-
plain why SADICAL performs worse than standard CDCL solvers on the majority of
the SAT competition benchmarks. While SADICAL integrates ﬁnding PR clauses of
arbitrary size in the main search loop, our tool focuses on learning short PR clauses as
a preprocessing step. This allows us to develop good heuristics for PR learning without
compromising the main search loop.
5
Extracting PR Clauses
The goal of PRELEARN is to ﬁnd useful PR clauses that improve the performance of
CDCL solvers on both satisﬁable and unsatisﬁable instances. Figure 1 shows how a
SAT problem is solved using PRELEARN. For some preset time limit, PR clauses are
found and then added to the original formula. Interleaved in this process is failed literal
probing to check if unit clauses can be learned. When the preprocessing stage ends,
the new formula that includes learned PR clauses is solved by a CDCL solver. If the

Preprocessing of Propagation Redundant Clauses
111
PRELEARN
CDCL
Proof Checker
CNF
PR Clauses
PR Proof
DRAT Proof
Veriﬁed
SAT
Fig. 1. Solving a formula with PRELEARN and a CDCL solver.
formula is satisﬁable, the solver will produce a satisfying assignment. If the formula is
unsatisﬁable, a refutation proof of the original formula can be computed by combining
the satisfaction preserving proof from PRELEARN and the refutation proof emitted by
the CDCL solver. The complete proof can be veriﬁed with DPR-TRIM.
PRELEARN alternates between ﬁnding PR clauses and learning PR clauses. Candi-
date PR clauses are found by iterating over each variable in the formula, and for each
variable constructing clauses that include that variable. To determine if a clause is PR,
the positive reduct generated by that clause is solved. It can be costly to generate and
solve many positive reducts, so heuristics are used to ﬁnd candidate clauses that are
more likely to be PR. It is possible to ﬁnd multiple PR clauses that conﬂict with each
other. PR clauses are conﬂicting if adding one of the PR clauses to the formula makes
the other no longer PR. Learning PR clauses involves selecting PR clauses that are non-
conﬂicting. The selection may maximize the number of PR clauses learned or optimize
for some other metric. Adding PR clauses and units derived from probing may cause
new clauses to become PR, so the entire process is iterated multiple times.
5.1
Finding PR Clauses
PR clauses are found by constructing a set of candidate clauses and solving the positive
reduct generated by each clause. In SADICAL the candidates are the clauses blocking
the partial assignment of the solver after each decision in the SDCL loop that does
not derive a conﬂict. In effect, candidates are constructed using the solver’s variable
decision heuristic. We take a more general approach, constructing sets of candidates for
each variable based on unit propagation and the partial assignment’s neighbors.
For a variable x, neighbors(x) denotes the set of variables occurring in clauses
containing literal x or x, excluding variable x. For a partial assignment α, neighbors(α)
denotes 
x∈var(α) neighbors(x) \ var(α). Candidate clauses for a literal l are generated
in the following way:
– Let α be the partial assignment found by unit propagation starting with the assign-
ment that makes l true.
– Generate the candidate PR clauses {(l ∨y), (l ∨y) | y ∈neighbors(α)}.
Example 2 shows how candidate binary clauses are constructed using both polarities
of an initial variable x. In Example 3 the depth is expanded to reach more variables and
create larger sets of candidate clauses. The depth parameter is used in Section 5.4.

112
J. E. Reeves et al.
Example 2. Consider the following formula: (x1 ∨x2) ∧(x1 ∨x3) ∧(x1 ∨x4 ∨x5) ∧
(x2 ∨x6 ∨x7) ∧(x3 ∨x7 ∨x8) ∧(x8 ∨x9),
Case 1: We start with var(x1) = 1 and perform unit propagation resulting in α =
{x1x3}. Observe that neighbors(α) = {x2, x4, x5, x7, x8}. The generated candidate
clauses are (x1 ∨x2), (x1 ∨x2), (x1 ∨x4), (x1 ∨x4), . . . , (x1 ∨x8), (x1 ∨x8).
Case 2: We start with var(x1) = 0 and perform unit propagation resulting in α =
{x1x2}. Observe that neighbors(α) = {x3, x4, x5, x6, x7}. The generated candidate
clauses are (x1 ∨x3), (x1 ∨x3), (x1 ∨x4), (x1 ∨x4), . . . , (x1 ∨x7), (x1 ∨x7).
Example 3. Take the formula from Example 2 and assignment of var(x1) = 1 as in
case 1. The set of candidate clauses can be expanded by also considering the unas-
signed neighbors of the variables in neighbors(α). For example, neighbors(x8) =
{x3, x7, x9}, of which x9 is new and unassigned. This adds (x1 ∨x9) and (x1 ∨x9) to
the set of candidate clauses. This can be iterated by including neighbors of new unas-
signed variables from the prior step.
We consider both polarities when constructing candidates for a variable. After all
candidates for a variable are constructed, the positive reduct for each candidate is gen-
erated and solved in order. Note that propagated literals appearing in the partial assign-
ment do not appear in the PR clause. The satisfying assignment is stored as the witness
and the PR clause may be learned immediately depending on the learning conﬁguration.
This process is naturally extended to ternary clauses. The binary candidates are gen-
erated, and for each candidate (x∨y), x and y are assigned to false in the ﬁrst step. The
variables z ∈neighbors(α) yield clauses (x∨y ∨z) and (x∨y ∨z). This approach can
generate many candidate ternary clauses depending on the connectivity of the formula
since each candidate binary clause is expanded. A ﬁltering operation would be useful to
avoid the blow-up in number of candidates. There are likely diminishing returns when
searching for larger PR clauses because (1) there are more possible candidates, (2) the
positive reducts are likely larger, and (3) each clause blocks less of the search space.
We consider only unit and binary candidate clauses in our main evaluation.
Ideally, we should construct candidate clauses that are likely PR to reduce the num-
ber of failed reducts generated. Note, the (ﬁltered) positive reduct can only be satisﬁable
if given the partial assignment there exists a reduced, satisﬁed clause. By focusing on
neighbors, we guarantee that such a clause exists. The reduced heuristic in SADICAL
ﬁnds variables in all reduced but unsatisﬁed clauses. The idea behind this heuristic is
to direct the assignment towards conditional autarkies that imply a satisﬁable positive
reduct [18]. The neighbors approach generalizes this to variables in all reduced clauses
whether or not they are unsatisﬁed. A comparison can be found in our repository.
5.2
Learning PR Clauses
Given multiple clauses that are PR w.r.t. the same formula, it is possible that some of
the clauses conﬂict with each other and cannot be learned simultaneously. Example 4
shows how learning one PR clause may invalidate the witness of another PR clause. It
may be that a different witness exists, but ﬁnding it requires regenerating the positive
reduct to include the learned PR clause and solving it. The simplest way to avoid con-
ﬂicting PR clause is to learn PR clauses as they are found. When a reduct is satisﬁable,

Preprocessing of Propagation Redundant Clauses
113
the PR clauses is added to the formula and logged with its witness in the proof. Then
subsequent reducts will be generated from the formula including all added PR clauses.
Therefore, a satisﬁable reduct ensures a PR clause can be learned.
Alternatively, clauses can be found in batches, then a subset of nonconﬂicting clauses
can be learned. The set of conﬂicts between PR clauses can be computed in polynomial
time. For each pair of PR clauses C and D, if the assignment that generated the pruning
predicate for D touches C and C is not satisﬁed by the witness of D, then C con-
ﬂicts with D. In some cases reordering the two PR clauses may avoid a conﬂict. In
Example 4 learning the second clause would not affect the validity of the ﬁrst clauses’
witness. Once the conﬂicts are known, clauses can be learned based on some heuristic
ordering. Batch learning conﬁgurations are discussed more in the following section.
Example 4. Assume the following clause witness pairs are valid in a formula ψ: {(x1 ∨
x2 ∨x3), x1x2x3}, and {(x1 ∨x2 ∨x4), x1x2x4}. The ﬁrst clause conﬂicts with the
second. If the ﬁrst clause is added to ψ, the clause (x1 ∨x2) would be in the positive
reduct for the second clause, but it is not satisﬁed by the witness of the second clause.
5.3
Additional Conﬁgurations
The sections above describe the PRELEARN conﬁguration used in the main evaluation,
i.e., ﬁnding candidate PR clauses with the neighbors heuristic and learning clauses in-
stantly as the positive reducts are solved. In this section we present several additional
conﬁgurations. The time-constrained reader may skip ahead to Section 5.4 for the pre-
sentation of our main results.
In batch learning a set of PR clauses are found in batches then learned. Learning as
many nonconﬂicting clauses as possible coincides with the maximum independent set
problem. This problem is NP-Hard. We approximate the solution by adding the clause
causing the fewest conﬂicts with unblocked clauses. When a clause is added, the clauses
it blocks are removed from the batch and conﬂict counts are recalculated Alternatively,
clauses can be added in a random order. Random ordering requires less computation at
the cost of potentially fewer learned PR clauses.
The neighbors heuristic for constructing candidate clauses can be modiﬁed to in-
clude a depth parameter. neighbors(i) indicates the number of iterations expanding the
variables. For example, neighbors(2) expands on the variables in neighbors(1), seen in
Example 3. We also implement the reduced heuristic, shown in Example 5. Detailed
evaluations and comparisons can be found in our repository. In general, we found that
the additional conﬁgurations did not improve on our main conﬁguration. More work
needs to be done to determine when and how to apply these additional conﬁgurations.
Example 5. Given the set of clauses (x1 ∨x2 ∨x3) ∧(x1 ∨x3 ∨x4) ∧(x3 ∨x5), and
initial assignment α = x1, only the second clause is reduced and not satisﬁed, giving
reduced(α) = {x3, x4} and candidate clauses (x1∨x3), (x1∨x4), (x1∨x3), (x1∨x4).
5.4
Implementation
PRELEARN was implemented using the inner/outer-solver framework in SADICAL.
The inner solver acts the same as in SADICAL, solving pruning predicates using CDCL.

114
J. E. Reeves et al.
The outer solver is not used for SDCL, but the SDCL data-structures are used to ﬁnd
and learn PR clauses. The outer solver is initialized with the original formula and main-
tains the list of variables, clauses, and watch pointers. By default, the outer solver has
no variables assigned other than learned units. When ﬁnding candidates, the variables
in the partial clause are assigned in the outer solver. Unit propagation makes it possible
to ﬁnd all reduced clauses in the formula with a single pass. This is necessary for con-
structing the positive reduct. After a candidate clause has been assigned and the positive
reduct solved, the variables are unassigned. This returns the outer solver to the top-level
before examining the next candidate. When a PR clause is learned, it is added to the
formula along with its watch pointers. Additionally, failed literals are found if assign-
ing a variable at the top-level causes a conﬂict through unit propagation. The negation
of a failed literal is a unit that can be added to the formula.
In a single iteration each variable in the formula is processed in a breadth-ﬁrst search
(BFS) starting from the ﬁrst variable in the numbering. When a variable is encountered
it is ﬁrst checked whether either assignment of the variable is a failed literal or a unit PR
clause. If not, binary candidates are generated based on the selected heuristic and PR
clauses are learned based on the learning conﬁguration. Variables are added to the fron-
tier of the BFS as they are encountered during candidate clause generation, but they are
not repeated. Optionally, after all variables have been encountered the BFS restarts, now
constructing ternary candidates. The repetition continues to the desired clause length.
Then another iteration begins again with binary clauses. Running PRELEARN multi-
ple times is important because adding PR clauses in one iteration may allow additional
clauses to be added in the next.
6
Mutilated Chessboard
The mutilated chessboard is an n × n grid of alternating black and white squares with
two opposite corners removed. The problem is whether or not the the board can be cov-
ered with 2 × 1 dominoes. This can be encoded in CNF by using variables to represent
Fig. 2. Occurrences of two horizontal dominoes may be replaced by two vertical dominos in a
solution. Similarly, occurrences of a horizontal domino atop two vertical dominos can be replaced
by shifting the horizontal domino down.

Preprocessing of Propagation Redundant Clauses
115
0
50
100
150
200
250
300
350
400
450
500
0
1,000
2,000
3,000
4,000
CPU time
learned clauses
Units and Binary PR Clauses Learned per Execution for N = 20
units
binary PR
execution
conﬂict found
Fig. 3. Unit and binary PR clauses learned each execution (red-dotted line) until a contradiction
was found. Markers on binary PR lines represent an iteration within an execution.
domino placements on the board. At-most-one constraints (using the pairwise encod-
ings) say only one domino can cover each square, and at-least-one constraints (using a
disjunction) say some domino must cover each square.
In recent SAT competitions, no proof-generating SAT solver could deal with in-
stances larger than N = 18. In ongoing work, we found refutation proofs that contain
only units and binary PR clauses for some boards of size N ≤30. PRELEARN can be
modiﬁed to automatically ﬁnd proofs of this type. Running iterations of PRELEARN un-
til saturation, meaning no new binary PR clauses or units can be found, yields some set
of units and binary PR clauses. Removing the binary PR clauses from the formula and
rerunning PRELEARN will yield additional units and a new set of binary PR clauses.
Repeating the process of removing binary PR clauses and keeping units will eventually
derive the empty clause for this problem. Figure 3 gives detailed values for N = 20.
Within each execution (red dotted lines) there are at most 10 iterations (red tick mark-
ers), and each iteration learns some set of binary PR clauses (red). Some executions
saturate binary PR clauses before the tenth iteration and exit early. At the end of each
execution the binary PR clauses are deleted, but the units (blue) are kept for the follow-
ing execution. A complete DPR proof (PR with deletion) can be constructed by adding
deletion information for the binary PR clauses removed between each execution when
concatenating the PRELEARN proofs. The approach works for mutilated chess because
in each execution there are many binary PR clauses that can be learned and will lead
to units, but they are mutually exclusive and cannot be learned simultaneously. Further,
adding units allows new binary PR clauses to be learned in following executions.
Table 1 shows the statistics for PRELEARN. Achieving these results required some
modiﬁcations to the conﬁguration of PRELEARN. First, notice in Figure 2 the PR
clauses that can be learned involve blocking one domino orientation that can be re-
placed by a symmetric orientation. To optimize for these types of PR clauses, we only

116
J. E. Reeves et al.
Table 1. Statistics running multiple executions of PRELEARN on the mutilated chessboard prob-
lem with the conﬁgurations described below. Total units includes failed literals and learned PR
units. The average units and average binary PR clauses learned during each execution (Exe.) are
shown as well.
N
Time (s)
# Exe. Avg. (s)
Total Units
Total Bin.
Avg. Units
Avg. Bin.
8
0.14
1
0.14
30
164
30.00
164.00
12
4.94
1
4.94
103
1,045
103.00
1,045.00
16
62.47
2
31.23
195
3,988
97.50
1,994.00
20
513.12
6
85.52
339
1,4470
56.50
2,411.67
24
4,941.38
26 190.05
512
64,038
19.69
2,463.00
constructed candidates where the ﬁrst literal was negative. The neighbors heuristic had
to be increased to a depth of 6, meaning more candidates were generated for each vari-
able. Intuitively, the proof is constructed by adding binary PR clauses in order to ﬁnd
negative units (dominos that cannot be placed) around the borders of the board. Follow-
ing iterations build more units inwards, until a point is reached where units cover almost
the entire board. This forces an impossible domino placement leading to a contradic-
tion. Complete proofs using only units and binary PR clauses were found for boards
up to size N = 24 within 5,000 seconds. We veriﬁed all proofs using DPR-TRIM. The
mutilated chessboard has a high degree of symmetry and structure, making it suitable
for this approach. For most problems it is not expected that multiple executions while
keeping learned units will ﬁnd new PR clauses.
Experiments were done with several conﬁgurations (see Section 5.3) to ﬁnd the best
results. We found that increasing the depth of neighbors was necessary for larger boards
including N = 24. Increasing the depth allows more binary PR clauses to be found, at
the cost of generating more reducts. This is necessary to ﬁnd units. The reduced heuris-
tic (a subset of neighbors) did not yield complete proofs. We also tried incrementing
the depth after each execution starting with 1 and reseting at 9. In this approach, the
execution times for depth greater than 6 were larger but did not yield more unit clauses
on average. We attempted batch learning on every 500 found clauses using either ran-
dom or the sorted heuristic. In each batch many of the 500 PR clauses blocked each
other because many conﬂicting PR clauses can be found on a small set of variables in
mutilated chess. The PR clauses that were blocked would be found again in follow-
ing iterations, leading to more reducts generated and solved. This caused much longer
execution times. Adding PR clauses instantly is a good conﬁguration for reducing exe-
cution time when there are many conﬂicting clauses. However, for some less symmetric
problems it may be worth the tradeoff to learn the clauses in batches, because learning
a few bad PR clauses may disrupt the subsequent iterations.
7
SAT Competition Benchmarks
We evaluated PRELEARN on previous SAT competition formulas. Formulas from the
’13, ’15, ’16, ’19, ’20, and ’21 SAT competitions’ main tracks were grouped by size.
0-10k contains the 323 formulas with less than 10,000 clauses and 10k-50k contains

Preprocessing of Propagation Redundant Clauses
117
Table 2. Fraction of benchmarks where PR clauses were learned, average runtime of PRELEARN,
generated positive reducts and satisﬁable positive reducts (PR clauses learned), and number of
failed literals found.
Set
Benches
Avg. (s)
Generated Reducts
Sat. Reducts
% Sat.
Failed Lits
0-10k
221/323
22.36
104,850,011
548,417
0.52%
3,416
10k-50k
237/348
71.08
163,014,068
789,281
0.48%
6,290
the 348 formulas with between 10,000 and 50,000 clauses. In general, short PR proofs
have been found for hard combinatorial problems typically having few clauses (0-10k).
These include the pigeonhole and mutilated chessboard problems, some of which ap-
pear in 0-10k benchmarks. The PR clauses that can be derived for these formulas are
intuitive and almost always beneﬁcial to solvers. Less is known about the impact of PR
clauses on larger formulas, motivating our separation of test sets by size. The repository
containing the preprocessing tool, experiment conﬁgurations, and experiment data can
be found at https://github.com/jreeves3/PReLearn.
We ran our experiments on StarExec [30]. The specs for the compute nodes can be
found online.1 The compute nodes that ran our experiments were Intel Xeon E5 cores
with 2.4 GHz, and all experiments ran with 64 GB of memory and a 5,000 second
timeout. We run PRELEARN for 50 iterations over 100 seconds, exiting early if no new
PR clauses were found in an iteration.
PRELEARN was executed as a stand-alone program, producing a derivation proof
and a modiﬁed CNF. For experiments, the CDCL solver KISSAT [5] was called once on
the original formula and once on the modiﬁed CNF. KISSAT was selected because of
its high-rankings in previous SAT competitions, but we expect the results to generalize
to other CDCL SAT solvers.
Derivation proofs from PRELEARN were veriﬁed in all solved instances using the
independent proof checker DPR-TRIM using a forward check. This can be extended to
complete proofs in the following way. In the unsatisﬁable case the proof for the learned
PR clauses is concatenated to the proof traced by KISSAT, and the complete proof is
veriﬁed against the original formula. In the satisﬁable case the partial proof for the
learned PR clauses is veriﬁed using a forward check in DPR-TRIM, and the satisfying
assignment found by KISSAT is veriﬁed by the StarExec post-processing tool. Due to
resource limitations, we veriﬁed a subset of complete proofs in DPR-TRIM. This is
more costly because it involves running KISSAT with proof logging, then running DPR-
TRIM on the complete proof.
Table 2 shows the cumulative statistics for running PRELEARN on the benchmark
sets. Note the number of satisﬁable reducts is the number of learned PR clauses, because
PR clauses are learned immediately after the reduct is solved. These include both unit
and binary PR clauses. A very small percentage of generated reducts is satisﬁable, and
subsequently learned. This is less important for small formulas when reducts can be
computed quickly and there are fewer candidates to consider. However, for the 10k-50k
formulas the average runtime more than triples but the number of generated reducts
1 https://starexec.org/starexec/public/about.jsp

118
J. E. Reeves et al.
Table 3. Number of total solved instances and exclusive solved instances running KISSAT with
and without PRELEARN. Number of improved instances running KISSAT with PRELEARN.
PRELEARN execution times were included in total execution times.
0-10k SAT
0-10k UNSAT
10k-50k SAT
10k-50k UNSAT
Total w/ PRELEARN
84
149
143
89
Total w/o PRELEARN
80
141
143
91
Exclusively w/ PRELEARN
4
10
4
1
Exclusively w/o PRELEARN
0
2
4
3
Improved w/ PRELEARN
20
44
25
13
less than doubles. PR clauses are found in about two thirds of the formulas, showing
our approach generalizes beyond the canonical problems for which we knew PR clauses
existed. Expanding the exploration and increasing the time limit did not help to ﬁnd PR
clauses in the remaining one third.
Table 3 gives a high-level picture of PRELEARN’s impact on KISSAT. PRELEARN
signiﬁcantly improves performance on 0-10k SAT and UNSAT benchmarks. These
contain the hard combinatorial problems including pigeonhole that PRELEARN was
expected to perform well on. There were 4 additional SAT formulas solved with PRE-
LEARN that KISSAT alone could not solve. This shows that PRELEARN impacts not
only hard unsatisﬁable problems but satisﬁable problems as well. On the other hand,
the addition of PR clauses makes some problems more difﬁcult. This is clear with the
10k-50k results, where 5 benchmarks are solved exclusively with PRELEARN and 7 are
solved exclusively without. Additionally, PRELEARN improved KISSAT’s performance
on 102 of 671 or approx. 15% of benchmarks. This is a large portion of benchmarks,
both SAT and UNSAT, for which PRELEARN is helpful.
Figure 4 gives a more detailed picture on the impact of PRELEARN per benchmark.
In the scatter plot the left-hand end of each line indicates the KISSAT execution time,
while the length of the line indicates the PRELEARN execution time, and so the right-
hand end gives the total time for PRELEARN plus KISSAT. Lines that cross the diagonal
indicate that the preprocessing improved KISSAT’s performance but ran for longer than
the improvement. PRELEARN improved performance for points above the diagonal.
Points on the dotted-lines (timeout) are solved by one conﬁguration and not the other.
The top plot gives the results for the 0-10k formulas, with many points on the top
timeout line as expected. These are the hard combinatorial problems that can only be
solved with PRELEARN. In general, the unsatisﬁable formulas beneﬁt more than the
satisﬁable formulas. PR clauses can reduce the number of solutions in a formula and
this may explain the negative impact on many satisﬁable formulas. However, there are
still some satisﬁable formulas that are only solved with PRELEARN.
In the bottom plot, formulas that take a long time to solve (above the diagonal in the
upper right-hand corner) are helped more by PRELEARN. In the bottom half of the plot,
many lines cross the diagonal meaning the addition of PR clauses provided a negligible
beneﬁt. For this set there are more satisﬁable formulas for which PRELEARN is helpful.

Preprocessing of Propagation Redundant Clauses
119
100
101
102
103
100
101
102
103
With PRELEARN
Without PRELEARN
SAT
UNSAT
100
101
102
103
100
101
102
103
With PRELEARN
Without PRELEARN
SAT
UNSAT
Fig. 4. Execution times w/ and w/o PRELEARN on 0-10k (top) and 10k-50k (bottom) bench-
marks. The left-hand point of each segment shows the time for the SAT solver alone; the right-
hand point indicates the combined time for preprocessing and solving.

120
J. E. Reeves et al.
Table 4. Some formulas solved by KISSAT exclusively with PRELEARN (top) and some formulas
solved exclusively without PRELEARN (bottom). (*) solved without KISSAT. Clauses include PR
clauses and failed literals learned.
Set
Value
With
Without
Clauses
Formula
Year
0-10k
UNSAT
1.26
–
2,033
ph12* 2013
0-10k
UNSAT
35.69
–
20,179
Pb-chnl15-16 c18* 2019
0-10k
UNSAT
105.01
–
46,759
Pb-chnl20-21 c18 2019
0-10k
UNSAT
59.99
–
1,633
randomG-Mix-n17-d05 2021
0-10k
UNSAT
61.08
–
1,472
randomG-n17-d05 2021
0-10k
UNSAT
407.51
–
1,640
randomG-n18-d05 2021
0-10k
UNSAT
584.95
–
1,706
randomG-Mix-n18-d05 2021
0-10k
SAT
1,082.62
–
9,650
fsf-300-354-2-2-3-2.23.opt 2013
0-10k
SAT
1,250.82
–
10,058
fsf-300-354-2-2-3-2.46.opt 2013
10k-50k
SAT
1,076.34
–
804
sp5-26-19-bin-stri-ﬂat-noid 2021
10k-50k
SAT
608.48
–
901
sp5-26-19-una-nons-tree-noid 2021
10k-50k
SAT
–
22.99
254
Ptn-7824-b13 2016
10k-50k
SAT
–
549.27
133
Ptn-7824-b09 2016
10k-50k
SAT
–
1,246.42
39
Ptn-7824-b02 2016
10k-50k
SAT
–
1,290.49
121
Ptn-7824-b08 2016
10k-50k UNSAT
–
3,650.21
31,860
rphp4 110 shufﬂed 2016
10k-50k UNSAT
–
4,273.88
31,531
rphp4 115 shufﬂed 2016
The results in Figure 4 are encouraging, with many formulas signiﬁcantly beneﬁt-
ting from PRELEARN. PRELEARN improves the performance on both SAT and UN-
SAT formulas of varying size and difﬁculty. In addition, lines that cross the diagonal
imply that improving the runtime efﬁciency of PRELEARN alone would produce more
improved instances. For future work, it would be beneﬁcial to classify formulas before
running PRELEARN. There may exist general properties of a formula that signal when
PRELEARN will be useful and when PRELEARN will be harmful to a CDCL solver.
For instance, a formula’s community structure [2] may help focus the search to parts of
the formula where PR clauses are beneﬁcial.
7.1
Benchmark Families
In this section we analyze benchmark families that PRELEARN had the greatest positive
(negative) effect on, found in Table 4. Studying the formulas PRELEARN works well
on may reveal better heuristics for ﬁnding good PR clauses.
It has been shown that PR works well for hard combinatorial problems based on
perfect matchings [14,15]. The perfect matching benchmarks (randomG) [7] are a gen-
eralization of the pigeonhole (php) and mutilated chessboard problems with varying
at-most-one encodings and edge densities. The binary PR clauses can be intuitively
understood as blocking two edges from the perfect matching if there exists two other
edges that match the same nodes. These benchmarks are relatively small but extremely
hard for CDCL solvers. Symmetry-breaking with PR clauses greatly reduces the search
space and leads KISSAT to a short proof of unsatisﬁability. PRELEARN also beneﬁts

Preprocessing of Propagation Redundant Clauses
121
other hard combinatorial problems that use pseudo-Boolean constraints. The pseudo-
Boolean (Pb-chnl) [24] benchmarks are based on at-most-one constraints (using the
pairwise encoding) and at-least-one constraints. These formulas have a similar graph-
ical structure to the perfect matching benchmarks. Binary PR clauses block two edges
when another set of edges exists that are incident to the same nodes.
For the other two benchmark families that beneﬁted from PRELEARN, the intuition
behind PR learning is less clear. The ﬁxed-shape random formulas (fsf) [29] are pa-
rameterized non-clausal random formulas built from hyper-clauses. The SAT encoding
makes use of the Plaisted-Greenbaum transformation, introducing circuit-like structure
to the problem. The superpermutation problem (sp) [22] asks whether a sequence of
digits 1–n of length l can contain every permutation of [1, n] as a subsequence, and the
optimization variant asks for the smallest such l given n. The sequence of l digits is en-
coded directly and passed through a multi-layered circuit that checks for the existence
of each individual permutation. Digits use the binary (bin) or unary (una) encoding, are
strict stri if clauses constrain digit bits to valid encodings and nonstrict nons otherwise,
and ﬂat if the circuit is a large AND or tree for preﬁx recognizing nested circuits. The
formulas given ask to ﬁnd a preﬁx of a superpermutation for n = 5 or length 26 with 19
permutations. The check for 19 permutations was encoded as cardinality constraints in
a pseudo-Boolean instance, then converted back to SAT. Each individual permutation
is checked by duplicating circuits at each possible starting position of the permutation
in l. PR clauses may be pruning certain starting positions for some permutations or
affecting the pseudo-Boolean constraints. This cannot be determined without a deeper
knowledge of the benchmark generator.
The relativized pigeonhole problem (rphp) [3] involves placing k pigeons in k −
1 holes with n nesting places. This problem has polynomial hardness for resolution,
unlike the exponential hardness of the classical pigeonhole problem. The symmetry-
breaking preprocessor BREAKID [9] generates symmetry-breaking formulas for rphp
that are easy for a CDCL solver. PRELEARN can learn many PR clauses but the formula
does not become easier. Note PRELEARN can solve the php with n = 12 in a second.
One problem is clause and variable permuting (a.k.a. shufﬂing). The mutilated
chessboard problem can still be solved by PRELEARN after permuting variables and
clauses. The pigeonhole problem can be solved after permuting clauses but not after
permuting variable names. In PRELEARN, PR candidates are sorted by variable name
independent of clause ordering, but when the variable names change the order of learned
clauses changes. In the mutilated chessboard problem there is local structure, so simi-
lar PR clauses are learned under variable renaming. In the pigeonhole problem there is
global structure, so a variable renaming can signiﬁcantly change the binary PR clauses
learned and cause earlier saturation with far fewer units.
Another problem is that the addition of PR clauses can change the existing structure
of a formula and negatively affect CDCL heuristics. The Pythagorean Triples Problem
(Ptn) [19] asks whether monochromatic solutions of the equation a2 + b2 = c2 can be
avoided. The formulas encode numbers {1, . . . , 7824}, for which a valid 2-coloring is
possible. In the namings, the N in bN denotes the number of backbone literals added
to the formula. A backbone literal is a literal assigned true in every solution. Adding
more than 20 backbone literals makes the problem easy. For each formula KISSAT can

122
J. E. Reeves et al.
ﬁnd a satisfying assignment, but timeouts with the addition of PR clauses. For one
instance, adding only 39 PR clauses will lead to a timeout. In some hard SAT and
UNSAT problems solvers require some amount of luck and adding a few clauses or
shufﬂing a formula can cause a CDCL solver’s performance to sharply decrease. The
Pythagorean Triples problem was originally solved with a local search solver, and local
search still performs well after adding PR clauses.
In a straight-forward way, one can avoid the negative effects of adding harmful PR
clauses by running two solvers in parallel: one with PRELEARN and one without. This
ﬁts with the portfolio approach for solving SAT problems.
8
Conclusion and Future Work
In this paper we presented PRELEARN, a tool built from the SADICAL framework
that learns PR clauses in a preprocessing stage. We developed several heuristics for
ﬁnding PR clauses and multiple conﬁgurations for clause learning. In the evaluation we
found that PRELEARN improves the performance of the CDCL solver KISSAT on many
benchmarks from past SAT competitions.
For future work, quantifying the usefulness of each PR clause in relation to guid-
ing the CDCL solver may lead to better learning heuristics. This is a difﬁcult task that
likely requires problem speciﬁc information. Separately, failed clause caching can im-
prove performance by remembering and avoiding candidate clauses that fail with unsat-
isﬁable reducts in multiple iterations. This would be most beneﬁcial for problems like
the mutilated chessboard that have many conﬂicting PR clauses. Lastly, incorporating
PRELEARN during in-processing may allow for more PR clauses to be learned. This
could be implemented with the inner/outer solver framework but would require a sig-
niﬁcantly narrowed search. CDCL learns many clauses during execution and it would
be infeasible to examine binary PR clauses across the entire formula.
Acknowledgements. We thank the community at StarExec for providing computational
resources.

Preprocessing of Propagation Redundant Clauses
123
References
1. Alekhnovich, M.: Mutilated chessboard problem is exponentially hard for resolution. Theo-
retical Computer Science 310(1), 513–525 (2004)
2. Ans´otegui, C., Bonet, M.L., Gir´aldez-Cru, J., Levy, J., Simon, L.: Community structure in in-
dustrial SAT instances. Journal of Artiﬁcial Intelligence Research (JAR) 66, 443–472 (2019)
3. Atserias, A., Lauria, M., Nordstr¨om, J.: Narrow proofs may be maximally long. ACM Trans-
actions on Computational Logic 17(3) (2016)
4. Audemard, G., Katsirelos, G., Simon, L.: A restriction of extended resolution for clause
learning SAT solvers. In: AAAI Conference on Artiﬁcial Intelligence. pp. 15–20. AAAI
Press (2010)
5. Biere, A., Fazekas, K., Fleury, M., Heisinger, M.: CaDiCaL, Kissat, Paracooba, Plingeling
and Treengeling entering the SAT competition 2020 (2020), unpublished
6. Biere, A., Fr¨ohlich, A.: Evaluating CDCL variable scoring schemes. In: Theory and Appli-
cations of Satisﬁability Testing (SAT). LNCS, vol. 9340, pp. 405–422 (2015)
7. Codel, C.R., Reeves, J.E., Heule, M.J.H., Bryant, R.E.: Bipartite perfect matching bench-
marks. In: Pragmatics of SAT (2021)
8. Cook, S.A.: A short proof of the pigeon hole principle using extended resolution. SIGACT
News 8(4), 28–32 (1976)
9. Devriendt, J., Bogaerts, B., Bruynooghe, M., Denecker, M.: Improved static symmetry break-
ing for SAT. In: Theory and Applications of Satisﬁability Testing (SAT). LNCS, vol. 9710,
pp. 104–122. Springer (2016)
10. E´en, N., Biere, A.: Effective preprocessing in SAT through variable and clause elimination.
In: Theory and Applications of Satisﬁability Testing (SAT). LNCS, vol. 3569, pp. 61–75.
Springer (2005)
11. Freeman, J.W.: Improvements to Propositional Satisﬁability Search Algorithms. Ph.D. thesis,
USA (1995)
12. Haken, A.: The intractability of resolution. Theoretical Computer Science 39, 297–308
(1985)
13. Heule, M.J.H., Hunt, W.A., Wetzler, N.: Expressing symmetry breaking in DRAT proofs.
In: Conference on Automated Deduction (CADE). LNCS, vol. 9195, pp. 591–606. Springer
(2015)
14. Heule, M.J.H., Kiesl, B., Biere, A.: Short proofs without new variables. In: Conference on
Automated Deduction (CADE). LNCS, vol. 10395, pp. 130–147. Springer (2017)
15. Heule, M.J.H., Kiesl, B., Biere, A.: Clausal proofs of mutilated chessboards. In: NASA For-
mal Methods. LNCS, vol. 11460, pp. 204–210 (2019)
16. Heule, M.J.H., Kiesl, B., Biere, A.: Encoding redundancy for satisfaction-driven clause
learning. In: Tools and Algorithms for the Construction and Analysis of Systems (TACAS).
LNCS, vol. 11427, pp. 41–58. Springer (2019)
17. Heule, M.J.H., Kiesl, B., Biere, A.: Strong extension free proof systems. In: Journal of Au-
tomated Reasoning. vol. 64, pp. 533–544 (2020)
18. Heule, M.J.H., Kiesl, B., Seidl, M., Biere, A.: PRuning through satisfaction. In: Haifa Veri-
ﬁcation Conference (HVC). LNCS, vol. 10629, pp. 179–194 (2017)
19. Heule, M.J.H., Kullmann, O., Marek, V.W.: Solving and verifying the boolean pythagorean
triples problem via cube-and-conquer. In: Theory and Applications of Satisﬁability Testing
(SAT). LNCS, vol. 9710, pp. 228–245. Springer (2016)
20. Heule, M.J., Hunt, W.A., Wetzler, N.: Trimming while checking clausal proofs. In: Formal
Methods in Computer-Aided Design (FMCAD). pp. 181–188 (2013)
21. J¨arvisalo, M., Heule, M.J.H., Biere, A.: Inprocessing rules. In: International Joint Conference
on Automated Reasoning (IJCAR). LNCS, vol. 7364, pp. 355–370. Springer (2012)

124
J. E. Reeves et al.
22. Johnston, N.: Non-uniqueness of minimal superpermutations. Discrete Mathematics
313(14), 1553–1557 (2013)
23. Kiesl, B., Seidl, M., Tompits, H., Biere, A.: Super-blocked clauses. In: International Joint
Conference on Automated Reasoning (IJCAR). LNCS, vol. 9706, pp. 45–61 (2016)
24. Lecoutre, C., Roussel, O.: XCSP3 competition 2018 proceedings. pp. 40–41 (2018)
25. Liang, J., Ganesh, V., Poupart, P., Czarnecki, K.: Learning rate based branching heuristic for
SAT solvers. In: Theory and Applications of Satisﬁability Testing (SAT). LNCS, vol. 9710,
pp. 123–140 (2016)
26. Manthey, N., Heule, M.J.H., Biere, A.: Automated reencoding of Boolean formulas. In: Haifa
Veriﬁcation Conference (HVC). LNCS, vol. 7857, pp. 102–117 (2013)
27. Marques-Silva, J., Lynce, I., Malik, S.: Conﬂict-driven clause learning SAT solvers. In:
Handbook of Satisﬁability, pp. 131–153. IOS Press (2009)
28. Moskewicz, M.W., Madigan, C.F., Zhao, Y., Zhang, L., Malik, S.: Chaff: Engineering an
efﬁcient sat solver. In: Proceedings of the 38th Annual Design Automation Conference. p.
530–535. ACM (2001)
29. Navarro, J.A., Voronkov, A.: Generation of hard non-clausal random satisﬁability problems.
In: AAAI Conference on Artiﬁcial Intelligence. pp. 436–442. The MIT Press (2005)
30. Stump, A., Sutcliffe, G., Tinelli, C.: StarExec: A cross-community infrastructure for logic
solving. In: International Joint Conference on Automated Reasoning (IJCAR). LNCS,
vol. 8562, pp. 367–373. Springer (2014)
31. Tan, Y.K., Heule, M.J.H., Myreen, M.O.: cake lpr: Veriﬁed propagation redundancy check-
ing in CakeML. In: Tools and Algorithms for the Construction and Analysis of Systems
(TACAS), Part II. LNCS, vol. 12652, pp. 223–241 (2021)
32. Tseitin, G.S.: On the Complexity of Derivation in Propositional Calculus, pp. 466–483.
Springer (1983)
33. Wetzler, N., Heule, M.J.H., Hunt, W.A.: DRAT-trim: Efﬁcient checking and trimming using
expressive clausal proofs. In: Theory and Applications of Satisﬁability Testing (SAT). LNCS,
vol. 8561, pp. 422–429 (2014)
Open Access This chapter is licensed under the terms of the Creative Commons Attribution
4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use,
sharing, adaptation, distribution and reproduction in any medium or format, as long as you
give appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter’s Creative
Commons license, unless indicated otherwise in a credit line to the material. If material is not
included in the chapter’s Creative Commons license and your intended use is not permitted by
statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder.

Reasoning About Vectors Using an SMT Theory
of Sequences
Ying Sheng1(B), Andres N¨otzli1, Andrew Reynolds2, Yoni Zohar3, David Dill4,
Wolfgang Grieskamp4, Junkil Park4, Shaz Qadeer4, Clark Barrett1, and Cesare Tinelli2
1 Stanford University, Stanford, USA
ying1123@stanford.edu
2 The University of Iowa, Iowa City, USA
3 Bar-Ilan University, Ramat Gan, Israel
4 Meta Novi, Menlo Park, USA
Abstract. Dynamic arrays, also referred to as vectors, are fundamental data
structures used in many programs. Modeling their semantics efﬁciently is cru-
cial when reasoning about such programs. The theory of arrays is widely sup-
ported but is not ideal, because the number of elements is ﬁxed (determined by
its index sort) and cannot be adjusted, which is a problem, given that the length
of vectors often plays an important role when reasoning about vector programs.
In this paper, we propose reasoning about vectors using a theory of sequences.
We introduce the theory, propose a basic calculus adapted from one for the the-
ory of strings, and extend it to efﬁciently handle common vector operations. We
prove that our calculus is sound and show how to construct a model when it ter-
minates with a saturated conﬁguration. Finally, we describe an implementation
of the calculus in cvc5 and demonstrate its efﬁcacy by evaluating it on veriﬁca-
tion conditions for smart contracts and benchmarks derived from existing array
benchmarks.
1
Introduction
Generic vectors are used in many programming languages. For example, in C++’s stan-
dard library, they are provided by std::vector. Automated veriﬁcation of software
systems that manipulate vectors requires an efﬁcient and automated way of reason-
ing about them. Desirable characteristics of any approach for reasoning about vec-
tors include: piq expressiveness—operations that are commonly performed on vectors
should be supported; piiq generality—vectors are always “vectors of” some type (e.g.,
vectors of integers), and so it is desirable that vector reasoning be integrated within a
more general framework; solvers for satisﬁability modulo theories (SMT) provide such
a framework and are widely used in veriﬁcation tools (see [5] for a recent survey); piiiq
efﬁciency—fast and efﬁcient reasoning is essential for usability, especially as veriﬁca-
tion tools are increasingly used by non-experts and in continuous integration.
This work was funded in part by the Stanford Center for Blockchain Research, NSF-BSF grant
numbers 2110397 (NSF) and 2020704 (BSF), and Meta Novi. Part of the work was done when
the ﬁrst author was an intern at Meta Novi.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 125–143, 2022.
https://doi.org/10.1007/978-3-031-10769-6_9

126
Y. Sheng et al.
Despite the ubiquity of vectors in software on the one hand and the effectiveness
of SMT solvers for software veriﬁcation on the other hand, there is not currently a
clean way to represent vectors using operators from the SMT-LIB standard [3]. While
the theory of arrays can be used, it is not a great ﬁt because arrays have a ﬁxed size
determined by their index type. Representing a dynamic array thus requires additional
modeling work. Moreover, to reach an acceptable level of expressivity, quantiﬁers are
needed, which often makes the reasoning engine less efﬁcient and robust. Indeed, part
of the motivation for this work was frustration with array-based modeling in the Move
Prover, a veriﬁcation framework for smart contracts [24] (see Sect. 6 for more infor-
mation about the Move Prover and its use of vectors). The current paper bridges this
gap by studying and implementing a native theory of sequences in the SMT framework,
which satisﬁes the desirable properties for vector reasoning listed above.
We present two SMT-based calculi for determining satisﬁability in the theory of
sequences. Since the decidability of even weaker theories is unknown (see, e.g., [9,15]),
we do not aim for a decision procedure. Rather, we prove model and solution soundness
(that is, when our procedure terminates, the answer is correct). Our ﬁrst calculus lever-
ages techniques for the theory of strings. We generalize these techniques, lifting rules
speciﬁc to string characters to more general rules for arbitrary element types. By itself,
this base calculus is already quite effective. However, it misses opportunities to per-
form high-level vector-based reasoning. For example, both reading from and updating
a vector are very common operations in programming, and reasoning efﬁciently about
the corresponding sequence operators is thus crucial. Our second calculus addresses
this gap by integrating reasoning methods from array solvers (which handle reads and
updates efﬁciently) into the ﬁrst procedure. Notice, however, that this integration is not
trivial, as it must handle novel combinations of operators (such as the combination of
update and read operators with concatenation) as well as out-of-bounds cases that do
not occur with ordinary arrays. We have implemented both variants of our calculus in
the cvc5 SMT solver [2] and evaluated them on benchmarks originating from the Move
prover, as well as benchmarks that were translated from SMT-LIB array benchmarks.
As is typical, both of our calculi are agnostic to the sort of the elements in the
sequence. Reasoning about sequences of elements from a particular theory can then
be done via theory combination methods such as Nelson-Oppen [18] or polite combi-
nation [16,20]. The former can be done for stably inﬁnite theories (and the theory of
sequences that we present here is stably inﬁnite), while the latter requires investigating
the politeness of the theory, which we expect to do in future work.
The rest of the paper is organized as follows. Section 2 includes basic notions from
ﬁrst-order logic. Section 3 introduces the theory of sequences and shows how it can
be used to model vectors. Section 4 presents calculi for this theory and discusses their
correctness. Section 5 describes the implementation of these calculi in cvc5. Section 6
presents an evaluation comparing several variations of the sequence solver in cvc5 and
Z3. We conclude in Sect. 7 with directions for further research.
Related Work: Our work crucially builds on a proposal by Bjørner et al. [8], but
extends it in several key ways. First, their implementation (for a logic they call
QF_BVRE) restricts the generality of the theory by allowing only bit-vector elements
(representing characters) and assuming that sequences are bounded. In contrast, our

Reasoning About Vectors Using an SMT Theory of Sequences
127
calculus maintains full generality, allowing unbounded sequences and elements of arbi-
trary types. Second, while our core calculus focuses only on a subset of the operators in
[8], our implementation supports the remaining operators by reducing them to the core
operators, and also adds native support for the update operator, which is not included
in [8].
The base calculus that we present for sequences builds on similar work for the
theory of strings [6,17]. We extend our base calculus to support array-like reasoning
based on the weak-equivalence approach [10]. Though there exists some prior work on
extending the theory of arrays with more operators and reasoning about length [1,12,
14], this work does not include support for most of the of the sequence operators we
consider here.
The SMT-solver Z3 [11] also provides a solver for sequences. However, its docu-
mentation is limited [7], it does not support update directly, and its internal algorithms
are not described in the literature. Furthermore, as we show in Sect. 6, the performance
of the Z3 implementation is generally inferior to our implementation in cvc5.
2
Preliminaries
We assume the usual notions and terminology of many-sorted ﬁrst-order logic with
equality (see, e.g., [13] for a complete presentation). We consider many-sorted sig-
natures Σ, each containing a set of sort symbols (including a Boolean sort Bool),
a family of logical symbols « for equality, with sort σ ˆ σ Ñ Bool for all sorts
σ in Σ and interpreted as the identity relation, and a set of interpreted (and sorted)
function symbols. We assume the usual deﬁnitions of well-sorted terms, literals, and
formulas as terms of sort Bool. A literal is ﬂat if it has the form K, ppx1, . . . , xnq,
␣ppx1, . . . , xnq, x « y, ␣x « y, or x « fpx1, . . . , xnq, where p and f are function
symbols and x, y, and x1, . . . , xn are variables. A Σ-interpretation M is deﬁned as
usual, satisfying MpKq “ false and assigns: a set Mpσq to every sort σ of Σ, a func-
tion Mpfq : Mpσ1q ˆ . . . ˆ Mpσnq →Mpσq to any function symbol f of Σ with
arity σ1 ˆ . . . ˆ σn →σ, and an element Mpxq P Mpσq to any variable x of sort σ.
The satisfaction relation between interpretations and formulas is deﬁned as usual and is
denoted by |“.
A theory is a pair T “ pΣ, Iq, in which Σ is a signature and I is a class of Σ-
interpretations, closed under variable reassignment. The models of T are the interpreta-
tions in I without any variable assignments. A Σ-formula ϕ is satisﬁable (resp., unsat-
isﬁable) in T if it is satisﬁed by some (resp., no) interpretation in I. Given a (set of)
terms S, we write T pSq to denote the set of all subterms of S. For a theory T “ pΣ, Iq,
a set S of Σ-formulas and a Σ-formula ϕ, we write S |“T ϕ if every interpretation
M P I that satisﬁes S also satisﬁes ϕ. By convention and unless otherwise stated, we
use letters w, x, y, z to denote variables and s, t, u, v to denote terms.
The theory TLIA “ pΣLIA, ITLIAq of linear integer arithmetic is based on the signature
ΣLIA that includes a single sort Int, all natural numbers as constant symbols, the unary
´ symbol, the binary ` symbol and the binary ď relation. When k P N, we use the
notation k · x, inductively deﬁned by 0 · x “ 0 and pm ` 1q · x “ x ` m · x. In turn,
ITLIA consists of all structures M for ΣLIA in which the domain MpIntq of Int is the set

128
Y. Sheng et al.
Fig. 1. Signature for the theory of sequences.
of integer numbers, for every constant symbol n P N, Mpnq “ n, and `, ´, and ď are
interpreted as usual. We use standard notation for integer intervals (e.g., [a, b] for the
set of integers i, where a ď i ď b and [a, bq for the set where a ď i ă b).
3
A Theory of Sequences
We deﬁne the theory TSeq of sequences. Its signature ΣSeq is given in Fig. 1. It includes
the sorts Seq, Elem, Int, and Bool, intuitively denoting sequences, elements, integers,
and Booleans, respectively. The ﬁrst four lines include symbols of ΣLIA. We write t1 ’
t2, with ’ P {ą, ă, ď}, as syntactic sugar for the equivalent literal expressed using ď
(and possibly ␣). The sequence symbols are given on the remaining lines. Their arities
are also given in Fig. 1. Notice that
`` · · · ``
is a variadic function symbol.
Interpretations M of TSeq interpret: Int as the set of integers; Elem as some set; Seq
as the set of ﬁnite sequences whose elements are from Elem; ϵ as the empty sequence;
unit as a function that takes an element from MpElemq and returns the sequence that
contains only that element; nth as a function that takes an element s from MpSeqq and
an integer i and returns the ith element of s, in case i is non-negative and is smaller than
the length of s (we take the ﬁrst element of a sequence to have index 0). Otherwise, the
function has no restrictions; update as a function that takes an element s from MpSeqq,
an integer i, and an element a from MpElemq and returns the sequence obtained from
s by replacing its ith element by a, in case i is non-negative and smaller than the length
of s. Otherwise, the returned value is s itself; extract as a function that takes a sequence
s and integers i and j, and returns the maximal sub-sequence of s that starts at index i
and has length at most j, in case both i and j are non-negative and i is smaller than the
length of s. Otherwise, the returned value is the empty sequence;1 | | as a function that
takes a sequence and returns its length; and
`` · · · ``
as a function that takes some
number of sequences (at least 2) and returns their concatenation.
1 In [8], the second argument j denotes the end index, while here it denotes the length of the
sub-sequence, in order to be consistent with the theory of strings in the SMT-LIB standard.

Reasoning About Vectors Using an SMT Theory of Sequences
129
Notice that the interpretations of Elem and nth are not completely ﬁxed by the
theory: Elem can be set arbitrarily, and nth is only deﬁned by the theory for some
values of its second argument. For the rest, it can be set arbitrarily.
3.1
Vectors as Sequences
We show the applicability of TSeq by using it for a simple veriﬁcation task. Consider the
C++ function swap at the top of Fig. 2. This function swaps two elements in a vector.
The comments above the function include a partial speciﬁcation for it: if both indexes
are in-bounds and the indexed elements are equal, then the function should not change
the vector (this is expressed by s_out==s). We now consider how to encode the ver-
iﬁcation condition induced by the code and the speciﬁcation. The function variables a,
b, i, and j can be encoded as variables of sort Int with the same names. We include two
copies of s: s for its value at the beginning, and sout for its value at the end. But what
should be the sorts of s and sout? In Fig. 2 we consider two options: one is based on
arrays and the other on sequences.
Example 1 (Arrays). The theory of arrays includes three sorts: index, element (in this
case, both are Int), and an array sort Arr, as well as two operators: x[i], interpreted as
the ith element of x; and x[i Ð a], interpreted as the array obtained from x by setting
the element at index i to a. We declare s and sout as variables of an uninterpreted sort
V and declare two functions ℓand c, which, given v of sort V , return its length (of sort
Int) and content (of sort Arr), respectively.2
Next, we introduce functions to model vector operations: «A for comparing vectors,
nthA for reading from them, and updateA for updating them. These functions need to
be axiomatized. We include two axioms (bottom of Fig. 2): Ax1 states that two vectors
are equal iff they have the same length and the same contents. Ax2 axiomatizes the
update operator; the result has the same length, and if the updated index is in bounds,
then the corresponding element is updated. These axioms are not meant to be complete,
but are rather just strong enough for the example.
The ﬁrst two lines of the swap function are encoded as equalities using nthA, and
the last two lines are combined into one nested constraint that involves updateA. The
precondition of the speciﬁcation is naturally modeled using nthA, and the post-condition
is negated, so that the unsatisﬁability of the formula entails the correctness of the func-
tion w.r.t. the speciﬁcation. Indeed, the conjunction of all formulas in this encoding is
unsatisﬁable in the combined theories of arrays, integers, and uninterpreted functions.
The above encoding has two main shortcomings: It introduces auxiliary symbols, and
it uses quantiﬁers, thus reducing clarity and efﬁciency. In the next example, we see how
using the theory of sequences allows for a much more natural and succinct encoding.
Example 2 (Sequences). In the sequences encoding, s and sout have sort Seq. No aux-
iliary sorts or functions are needed, as the theory symbols can be used directly. Further,
2 It is possible to obtain a similar encoding using the theory of datatypes; however, here we use
uninterpreted functions which are simpler and better supported by SMT solvers.

130
Y. Sheng et al.
Fig. 2. An example using TSeq.
these symbols do not need to be axiomatized as their semantics is ﬁxed by the the-
ory. The resulting formula, much shorter than in Exmaple 2 and with no quantiﬁers, is
unsatisﬁable in TSeq.
4
Calculi
After introducing some deﬁnitions and assumptions, we describe a basic calculus for
the theory of sequences, which adapts techniques from previous procedures for the
theory of strings. In particular, the basic calculus reduces the operators nth and update
by introducing concatenation terms. We then show how to extend the basic calculus by
introducing additional rules inspired by solvers for the theory of arrays; the modiﬁed
calculus can often reason about nth and update terms directly, avoiding the introduction
of concatenation terms (which are typically expensive to reason about).
Given a vector of sequence terms t “ pt1, . . . , tnq, we use t to denote the term
corresponding to the concatenation of t1, . . . , tn. If n “ 0, t denotes ϵ, and if n “ 1, t
denotes t1; otherwise (when n ą 1), t denotes a concatenation term having n children.
In our calculi, we distinguish between sequence and arithmetic constraints.
Deﬁnition 1. A ΣSeq-formula ϕ is a sequence constraint if it has the form s « t or
s ̸« t; it is an arithmetic constraint if it has the form s « t, s ≥t, s ̸« t, or s ă t where
s, t are terms of sort Int, or if it is a disjunction c1 ∨c2 of two arithmetic constraints.
Notice that sequence constraints do not have to contain sequence terms (e.g., x « y
where x, y are Elem-variables). Also, equalities and disequalities between terms of sort
Int are both sequence and arithmetic constraints. In this paper we focus on sequence

Reasoning About Vectors Using an SMT Theory of Sequences
131
Fig. 3. Rewrite rules for the reduced form t↓of a term t, obtained from t by applying these rules
to completion.
constraints and arithmetic constraints. This is justiﬁed by the following lemma. (Proofs
of this lemma and later results can be found in an extended version of this paper [23].)
Lemma 1. For every quantiﬁer-free ΣSeq-formula ϕ, there are sets S1, . . . , Sn of
sequence constraints and sets A1, . . . , An of arithmetic constraints such that ϕ is TSeq-
satisﬁable iff Si Y Ai is TSeq-satisﬁable for some i P [1, n].
Throughout the presentation of the calculi, we will make a few simplifying assumptions.
Assumption 1. Whenever we refer to a set S of sequence constraints, we assume:
1. for every non-variable term t P T pSq, there exists a variable x such that x « t P S;
2. for every Seq-variable x, there exists a variable ℓx such that ℓx « |x| P S;
3. all literals in S are ﬂat.
Whenever we refer to a set of arithmetic constraints, we assume all its literals are ﬂat.
These assumptions are without loss of generality as any set can easily be transformed
into an equisatisﬁable set satisfying the assumptions by the addition of fresh variables
and equalities. Note that some rules below introduce non-ﬂat literals. In such cases,
we assume that similar transformations are done immediately after applying the rule to
maintain the invariant that all literals in S Y A are ﬂat. Rules may also introduce fresh
variables k of sort Seq. We further assume that in such cases, a corresponding constraint
ℓk « |k| is added to S with a fresh variable ℓk.
Deﬁnition 2. Let C be a set of constraints. We write C |“ ϕ to denote that C entails
formula ϕ in the empty theory, and write ”C to denote the binary relation over T pCq
such that s ”C t iff C |“ s « t.
Lemma 2. For all set S of sequence constraints, ”S is an equivalence relation; fur-
thermore, every equivalence class of ”S contains at least one variable.
We denote the equivalence class of a term s according to ”S by [s]”S and drop the ”S
subscript when it is clear from the context.
In the presentation of the calculus, it will often be useful to normalize terms to what
will be called a reduced form.
Deﬁnition 3. Let t be a ΣSeq-term. The reduced form of t, denoted by t↓, is the term
obtained by applying the rewrite rules listed in Fig. 3 to completion.
Observe that t↓is well deﬁned because the given rewrite rules form a terminating
rewrite system. This can be seen by noting that each rule reduces the number of appli-
cations of sequence operators in the left-hand side term or keeps that number the same
but reduces the size of the term. It is not difﬁcult to show that |“TSeq t « t↓.
We now introduce some basic deﬁnitions related to concatenation terms.

132
Y. Sheng et al.
Deﬁnition 4. A concatenation term is a term of the form s1 `` · · · `` sn with n ≥2.
If each si is a variable, it is a variable concatenation term. For a set S of sequence
constraints, a variable concatenation term x1``· · ·``xn is singular in S if S ̸|“ xi « ϵ
for at most one variable xi with i P [1, n]. A sequence variable x is atomic in S if
S ⊭x « ϵ and for all variable concatenation terms s P T pSq such that S |“ x « s, s
is singular in S.
We lift the concept of atomic variables to atomic representatives of equivalence classes.
Deﬁnition 5. Let S be a set of sequence constraints. Assume a choice function α :
T pSq/”S →T pSq that chooses a variable from each equivalence class of ”S. A
sequence variable x is an atomic representative in S if it is atomic in S and x “
αp[x]”Sq.
Finally, we introduce a relation that is the foundation for reasoning about concatena-
tions.
Deﬁnition 6. Let S be a set of sequence constraints. We inductively deﬁne a relation
S |“`` x « s, where x is a sequence variable in S and s is a sequence term whose
variables are in T pSq, as follows:
1. S |“`` x « x for all sequence variables x P T pSq.
2. S |“`` x « t for all sequence variables x P T pSq and variable concatenation terms
t, where x « t P S.
3. If S |“`` x « pw `` y `` zq↓and S |“ y « t and t is ϵ or a variable concatenation
term in S that is not singular in S, then S |“`` x « pw `` t `` zq↓.
Let α be a choice function for S as deﬁned in Deﬁnition 5. We additionally deﬁne the
entailment relation S |“∗
`` x « y, where y is of length n ≥0, to hold if each element of
y is an atomic representative in S and there exists z of length n such that S |“`` x « z
and S |“ yi « zi for i P [1, n].
In other words, S |“∗
`` x « t holds when t is a concatenation of atomic representa-
tives and is entailed to be equal to x by S. In practice, t is determined by recursively
expanding concatenations using equalities in S until a ﬁxpoint is reached.
Example 3. Suppose S “ {x « y `` z, y « w `` u, u « v} (we omit the additional
constraints required by Assumption 1, part 2 for brevity). It is easy to see that u, v, w,
and z are atomic in S, but x and y are not. Furthermore, w and z (and one of u or v)
must also be atomic representatives. Clearly, S |“`` x « x and S |“ x « y `` z.
Moreover, y `` z is a variable concatenation term that is not singular in S. Hence, we
have S |“`` x « py `` zq↓, and so S |“`` x « y `` z (by using either Item 2 or
Item 3 of Deﬁntion 6, as in fact x « y `` z P S. ). Now, since S |“`` x « y `` z,
S |“ y « w `` u, and w `` u is a variable concatenation term not singular in S, we get
that S |“`` x « ppw `` uq `` zq↓, and so S |“`` x « w `` u `` z. Now, assume that
v “ αp[v]”Sq “ αp{v, u}q. Then, S |“∗
`` x « w `` v `` z.
Our calculi can be understood as modeling abstractly a cooperation between an arith-
metic subsolver and a sequence subsolver. Many of the derivation rules lift those in the
string calculus of Liang et al. [17] to sequences of elements of an arbitrary type. We
describe them similarly as rules that modify conﬁgurations.

Reasoning About Vectors Using an SMT Theory of Sequences
133
Deﬁnition 7. A conﬁguration is either the distinguished conﬁguration unsat or a pair
pS, Aq of a set S of sequence constraints and a set A of arithmetic constraints.
The rules are given in guarded assignment form, where the rule premises describe the
conditions on the current conﬁguration under which the rule can be applied, and the
conclusion is either unsat, or otherwise describes the resulting modiﬁcations to the
conﬁguration. A rule may have multiple conclusions separated by ∥. In the rules, some
of the premises have the form S |“ s « t (see Deﬁnition 2). Such entailments can be
checked with standard algorithms for congruence closure. Similarly, premises of the
form S |“LIA s « t can be checked by solvers for linear integer arithmetic.
An application of a rule is redundant if it has a conclusion where each component
in the derived conﬁguration is a subset of the corresponding component in the premise
conﬁguration. We assume that for rules that introduce fresh variables, the introduced
variables are identical whenever the premises triggering the rule are the same (i.e., we
cannot generate an inﬁnite sequence of rule applications by continuously using the same
premises to introduce fresh variables).3 A conﬁguration other than unsat is saturated
with respect to a set R of derivation rules if every possible application of a rule in R
to it is redundant. A derivation tree is a tree where each node is a conﬁguration whose
children, if any, are obtained by a non-redundant application of a rule of the calculus.
A derivation tree is closed if all of its leaves are unsat. As we show later, a closed
derivation tree with root node pS, Aq is a proof that A Y S is unsatisﬁable in TSeq. In
contrast, a derivation tree with root node pS, Aq and a saturated leaf with respect to all
the rules of the calculus is a witness that A Y S is satisﬁable in TSeq.
4.1
Basic Calculus
Deﬁnition 8. The calculus BASE consists of the derivation rules in Figs. 4 and 5.
Some of the rules are adapted from previous work on string solvers [17,22]. Compared
to that work, our presentation of the rules is noticeably simpler, due to our use of the
relation |“∗
`` from Deﬁnition 6. In particular, our conﬁgurations consist only of pairs of
sets of formulas, without any auxiliary data-structures.
Note that judgments of the form S |“∗
`` x « t are used in premises of the calculus.
It is possible to compute whether such a premise holds thanks to the following lemma.
Lemma 3. Let S be a set of sequence constraints and A a set of arithmetic constraints.
If pS, Aq is saturated w.r.t. S-Prop, L-Intro and L-Valid, the problem of determining
whether S |“∗
`` x « s for given x and s is decidable.
Lemma 3 assumes saturation with respect to certain rules. Accordingly, our proof strat-
egy, described in Sect. 5, will ensure such saturation before attempting to apply rules
relying on |“∗
``. The relation |“∗
`` induces a normal form for each equivalence class of
”S.
3 In practice, this is implemented by associating each introduced variable with a witness term as
described in [21].

134
Y. Sheng et al.
Fig. 4. Core derivation rules. The rules use k and i to denote fresh variables of sequence and
integer sort, respectively, and w1 and w2 for fresh element variables.
Lemma 4. Let S be a set of sequence constraints and A a set of arithmetic constraints.
Suppose pS, Aq is saturated w.r.t. A-Conf, S-Prop, L-Intro, L-Valid, and C-Split. Then,
for every equivalence class e of ”S whose terms are of sort Seq, there exists a unique
(possibly empty) s such that whenever S |“∗
`` x « s′ for x P e, then s′ “ s. In this
case, we call s the normal form of e (and of x).
We now turn to the description of the rules in Fig. 4, which form the core of the
calculus. For greater clarity, some of the conclusions of the rules include terms before
they are ﬂattened. First, either subsolver can report that the current set of constraints is
unsatisﬁable by using the rules A-Conf or S-Conf. For the former, the entailment |“LIA
(which abbreviates |“TLIA) can be checked by a standard procedure for linear integer
arithmetic, and the latter corresponds to a situation where congruence closure detects
a conﬂict between an equality and a disequality. The rules A-Prop, S-Prop, and S-A
correspond to a form of Nelson-Oppen-style theory combination between the two sub-
solvers. The ﬁrst two communicate equalities between the sub-solvers, while the third
guesses arrangements for shared variables of sort Int. L-Intro ensures that the length
term |s| for each sequence term s is equal to its reduced form p|s|q↓. L-Valid restricts
sequence lengths to be non-negative, splitting on whether each sequence is empty or
has a length greater than 0. The unit operator is injective, which is captured by U-Eq.
C-Eq concludes that two sequence terms are equal if they have the same normal form. If
two sequence variables have different normal forms, then C-Split takes the ﬁrst differing
components y and y′ from the two normal forms and splits on their length relationship.
Note that C-Split is the source for non-termination of the calculus (see, e.g., [17,22]).

Reasoning About Vectors Using an SMT Theory of Sequences
135
Fig. 5. Reduction rules for extract, nth, and update. The rules use k, k′, and k′′ to denote fresh
sequence variables. We write s « minpt, uq as an abbreviation for s « t ∨s « u, s ď t, s ď u.
Finally, Deq-Ext handles disequalities between sequences x and y by either asserting
that their lengths are different or by choosing an index i at which they differ.
Figure 5 includes a set of reduction rules for handling operators that are not directly
handled by the core rules. These reduction rules capture the semantics of these operators
by reduction to concatenation. R-Extract splits into two cases: Either the extraction uses
an out-of-bounds index or a non-positive length, in which case the result is the empty
sequence, or the original sequence can be described as a concatenation that includes
the extracted sub-sequence. R-Nth creates an equation between y and a concatenation
term with unitpxq as one of its components, as long as i is not out of bounds. R-Update
considers two cases. If i is out of bounds, then the update term is equal to y. Otherwise,
y is equal to a concatenation, with the middle component (k′) representing the part of y
that is updated. In the update term, k′ is replaced by unitpzq.
Example 4. Consider a conﬁguration pS, Aq, where S contains the formulas x « y``z,
z « v `` x `` w, and v « unitpuq, and A is empty. Hence, S |“ |x| « |y `` z|. By
L-Intro, we have S |“ |y `` z| « |y| ` |z|. Together with Assumption 1, we have
S |“ ℓx « ℓy ` ℓz, and then with S-Prop, we have ℓx « ℓy ` ℓz P A. Similarly, we
can derive ℓz « ℓv ` ℓx ` ℓw, ℓv « 1 P S, and so p∗qA |“LIA ℓz « 1 ` ℓy ` ℓz ` ℓw.
Notice that for any variable k of sort Seq, we can apply L-Valid, L-Intro, and S-Prop to
add to A either ℓk ą 0 or ℓk “ 0. Applying this to y, z, w, we have that A |“LIA K in
each branch thanks to p∗q, and so A-Conf applies and we get unsat.
4.2
Extended Calculus
Deﬁnition 9. The calculus EXT is comprised of the derivation rules in Figs. 4 and 6,
with the addition of rule R-Extract from Fig. 5.
Our extended calculus combines array reasoning, based on [10] and expressed by
the rules in Fig. 6, with the core rules of Fig. 4 and the R-Extract rule. Unlike in BASE,
those rules do not reduce nth and update. Instead, they reason about those operators
directly and handle their combination with concatenation. Nth-Concat identiﬁes the ith

136
Y. Sheng et al.
element of sequence y with the corresponding element selected from its normal form
(see Lemma 4). Update-Concat operates similarly, applying update to all the compo-
nents. Update-Concat-Inv operates similarly on the updated sequence rather than on the
original sequence. Nth-Unit captures the semantics of nth when applied to a unit term.
Update-Unit is similar and distinguishes an update on an out-of-bounds index (different
from 0) from an update within the bound. Nth-Intro is meant to ensure that Nth-Update
(explained below) and Nth-Unit (explained above) are applicable whenever an update
term exists in the constraints. Nth-Update captures the read-over-write axioms of arrays,
adapted to consider their lengths (see, e.g., [10]). It distinguishes three cases: In the ﬁrst,
the update index is out of bounds. In the second, it is not out of bounds, and the cor-
responding nth term accesses the same index that was updated. In the third case, the
index used in the nth term is different from the updated index. Update-Bound considers
two cases: either the update changes the sequence, or the sequence remains the same.
Finally, Nth-Split introduces a case split on the equality between two sequence variables
x and x′ whenever they appear as arguments to nth with equivalent second arguments.
This is needed to ensure that we detect all cases where the arguments of two nth terms
must be equal.
4.3
Correctness
In this section we prove the following theorem:
Theorem 1. Let X P {BASE, EXT} and pS0, A0q be a conﬁguration, and assume with-
out loss of generality that A0 contains only arithmetic constraints that are not sequence
constraints. Let T be a derivation tree obtained by applying the rules of X with pS0, A0q
as the initial conﬁguration.
1. If T is closed, then S0 Y A0 is TSeq-unsatisﬁable.
2. If T contains a saturated conﬁguration pS, Aq w.r.t. X, then pS, Aq is TSeq-satisﬁable,
and so is pS0, A0q.
The theorem states that the calculi are correct in the following sense: if a closed deriva-
tion tree is obtained for the constraints S0 Y A0 then those constraints are unsatisﬁable
in TSeq; if a tree with a saturated leaf is obtained, then they are satisﬁable. It is possible,
however, that neither kind of tree can be derived by the calculi, making them neither
refutation-complete nor terminating. This is not surprising since, as mentioned in the
introduction, the decidability of even weaker theories is still unknown.
Proving the ﬁrst claim in Theorem 1 reduces to a local soundness argument for each
of the rules. For the second claim, we sketch below how to construct a satisfying model
M from a saturated conﬁguration for the case of EXT. The case for BASE is similar
and simpler.
Model Construction Steps.
The full model construction and its correctness are
described in a longer version of this paper [23] together with a proof of the theorem
above. Here is a summary of the steps needed for the model construction.
1. Sorts: MpElemq is interpreted as some arbitrary countably inﬁnite set. MpSeqq and
MpIntq are then determined by the theory.

Reasoning About Vectors Using an SMT Theory of Sequences
137
Fig. 6. Extended derivation rules. The rules use z1, . . . , zn to denote fresh sequence variables and
e, e′ to denote fresh element variables.
2. ΣSeq-symbols: TSeq enforces the interpretation of almost all ΣSeq-symbols, except
for nth when the second input is out of bounds. We cover this case below.
3. Integer variables: based on the saturation of A-Conf, we know there is some TLIA-
model satisfying A. We set M to interpret integer variables according to this model.
4. Element variables: these are partitioned into their ”S equivalence classes. Each class
is assigned a distinct element from MpElemq, which is possible since it is inﬁnite.
5. Atomic sequence variables: these are assigned interpretations in several sub-steps:
(a) length: we ﬁrst use the assignments to variables ℓx to set the length of Mpxq,
without assigning its actual value.
(b) unit variables: for variables x with x ”S unitpzq, we set Mpxq to be [Mpzq].

138
Y. Sheng et al.
(c) non-unit variables: All other sequence variables are assigned values according
to a weak equivalence graph we construct in a manner similar to [10]. This
construction takes into account constraints that involve update and nth.
6. Non-atomic sequence variables: these are ﬁrst transformed to their unique normal
form (see Lemma 4), consisting of concatenations of atomic variables. Then, the
values assigned to these variables are concatenated.
7. nth-terms: for out-of-bounds indices in nth-terms, we rely on ”S to make sure that
the assignment is consistent.
We conclude this section with an example of the construction of M.
Example 5. Consider a signature in which Elem is Int, and a saturated conﬁguration
pS∗, A∗q w.r.t. EXT that includes the following formulas: y « y1 `` y2, x « x1 `` x2,
y2 « x2, y1 « updatepx1, i, aq, |y1| “ |x1|, |y2| “ |x2|, nthpy, iq « a, nthpy1, iq « a.
Following the above construction, a satisfying interpretation M can be built as follows:
Step 1 Set both MpIntq and MpElemq to be the set of integer numbers. MpSeqq is
ﬁxed by the theory.
Step 3, Step 4 First, ﬁnd an arithmetic model, Mpℓxq “ Mpℓyq “ 4, Mpℓy1q “
Mpℓx1q “ 2, Mpℓy2q “ Mpℓx2q “ 2, Mpiq “ 0. Further, set Mpaq “ 0.
Step 5a Start assigning values to sequences. First, set the lengths of Mpxq and Mpyq
to be 4, and the lengths of Mpx1q, Mpx2q, Mpy1q, Mpy2q to be 2.
Step 5b is skipped as there are no unit terms.
Step 5c Set the 0th element of Mpy1q to 0 to satisfy nthpy1, iq “ a (y1 is atomic, y
is not). Assign fresh values to the remaining indices of atomic variables. The result
can be, e.g., Mpy1q “ [0, 2], Mpx1q “ [1, 2], Mpy2q “ Mpx2q “ [3, 4].
Step 6 Assign non-atomic sequence variables based on equivalent concatenations:
Mpyq “ [0, 2, 3, 4], Mpxq “ [1, 2, 3, 4].
Step 7 No integer variable in the formula was assigned an out-of-bound value, and so
the interpretation of nth on out-of-bounds cases is set arbitrarily.
5
Implementation
We implemented our procedure for sequences as an extension of a previous theory
solver for strings [17,22]. This solver is integrated in cvc5, and has been generalized to
reason about both strings and sequences. In this section, we describe how the rules of
the calculus are implemented and the overall strategy for when they are applied.
Like most SMT solvers, cvc5 is based on the CDCLpTq architecture [19] which
combines several subsolvers, each specialized on a speciﬁc theory, with a solver for
propositional satisﬁability (SAT). Following that architecture, cvc5 maintains an evolv-
ing set of formulas F. When F starts with quantiﬁer-free formulas over the theory TSeq,
the case targeted by this work, the SAT solver searches for a satisfying assignment for
F, represented as the set M of literals it satisﬁes. If none exists, the problem is unsatisﬁ-
able at the propositional level and hence TSeq-unsatisﬁable. Otherwise, M is partitioned
into the arithmetic constraints A and the sequence constraints S and checked for TSeq-
satisﬁability using the rules of the EXT calculus. Many of those rules, including all

Reasoning About Vectors Using an SMT Theory of Sequences
139
those with multiple conclusions, are implemented by adding new formulas to F (fol-
lowing the splitting-on-demand approach [4]). This causes the SAT solver to try to
extend its assignment to those formulas, which results in the addition of new literals to
M (and thereby also to A and S).
In this setting, the rules of the two calculi are implemented as follows. The effect
of rule A-Conf is achieved by invoking cvc5’s theory solver for linear integer arithmetic.
Rule S-Conf is implemented by the congruence closure submodule of the theory solver
for sequences. Rules A-Prop and S-Prop are implemented by the standard mechanism
for theory combination. Note that each of these four rules may be applied eagerly, that
is, before constructing a complete satisfying assignment M for F.
The remaining rules are implemented in the theory solver for sequences. Each time
M is checked for satisﬁability, cvc5 follows a strategy to determine which rule to apply
next. If none of the rules apply and the conﬁguration is different from unsat, then it is
saturated, and the solver returns sat. The strategy for EXT prioritizes rules as follows.
Only the ﬁrst applicable rule is applied (and then control goes back to the SAT solver).
1. (Add length constraints) For each sequence term in S, apply L-Intro or L-Valid, if not
already done. We apply L-Intro for non-variables, and L-Valid for variables.
2. (Mark congruent terms) For each set of update (resp. nth) terms that are congruent
to one another in the current conﬁguration, mark all but one term and ignore the
marked terms in the subsequent steps.
3. (Reduce extract) For extractpy, i, jq in S, apply R-Extract if not already done.
4. (Construct normal forms) Apply U-Eq or C-Split. We choose how to apply the latter
rule based on constructing normal forms for equivalence classes in a bottom-up fash-
ion, where the equivalence classes of x and y are considered before the equivalence
class of x``y. We do this until we ﬁnd an equivalence class such that S |“∗
`` z « u1
and S |“∗
`` z « u2 for distinct u1, u2.
5. (Normal forms) Apply C-Eq if two equivalence classes have the same normal form.
6. (Extensionality) For each disequality in S, apply Deq-Ext, if not already done.
7. (Distribute update and nth) For each term updatepx, i, tq (resp. nthpx, jq) such that
the normal form of x is a concatenation term, apply Update-Concat and Update-
Concat-Inv (resp. Nth-Concat) if not already done. Alternatively, if the normal form
of the equivalence class of x is a unit term, apply Update-Unit (resp. Nth-Unit).
8. (Array reasoning on atomic sequences) Apply Nth-Intro and Update-Bound to
update terms. For each update term, ﬁnd the matching nth terms and apply
Nth-Update. Apply Nth-Split to pairs of nth terms with equivalent indices.
9. (Theory combination) Apply S-A for all arithmetic terms occurring in both S and A.
Whenever a rule is applied, the strategy will restart from the beginning in the next itera-
tion. The strategy is designed to apply with higher priority steps that are easy to compute
and are likely to lead to conﬂicts. Some steps are ordered based on dependencies from
other steps. For instance, Steps 5 and 7 use normal forms, which are computed in Step
4. The strategy for the BASE calculus is the same, except that Steps 7 and 8 are replaced
by one that applies R-Update and R-Nth to all update and nth terms in S.
We point out that the C-Split rule may cause non-termination of the proof strategy
described above in the presence of cyclic sequence constraints, for instance, constraints
where sequence variables appear on both sides of an equality. The solver uses methods

140
Y. Sheng et al.
for detecting some of these cycles, to restrict when C-Split is applied. In particular,
when S |“∗
`` x « pu `` s `` wq↓, S |“∗
`` x « pu `` t `` vq↓, and s occurs in v,
then C-Split is not applied. Instead, other heuristics are used, and in some cases the
solver terminates with a response of “unknown” (see e.g., [17] for details). In addition
to the version shown here, we also use another variation of the C-Split rule where the
normal forms are matched in reverse (starting from the last terms in the concatenations).
The implementation also uses fast entailment tests for length inequalities. These tests
may allow us to conclude which branch of C-Split, if any, is feasible, without having to
branch on cases explicitly.
Although not shown here, the calculus can also accommodate certain extended
sequence constraints, that is, constraints using a signature with additional functions.
For example, our implementation supports sequence containment, replacement, and
reverse. It also supports an extended variant of the update operator, in which the third
argument is a sequence that overrides the sequence being updated starting from the
index given in the second argument. Constraints involving these functions are handled
by reduction rules, similar to those shown in Fig. 5. The implementation is further opti-
mized by using context-dependent simpliﬁcations, which may eagerly infer when cer-
tain sequence terms can be simpliﬁed to constants based on the current set of assertions
[22].
6
Evaluation
We evaluate the performance of our approach, as implemented in cvc5. The evaluation
investigates: (i) whether the use of sequences is a viable option for reasoning about
vectors in programs, (ii) how our approach compares with other sequence solvers, and
(iii) what is the performance impact of our array-style extended rules. As a baseline, we
use Version 4.8.14 of the Z3 SMT solver, which supports a theory of sequences with-
out updates. For cvc5, we evaluate implementations of both the basic calculus (denoted
cvc5) and the extended array-based calculus (denoted cvc5-a). The benchmarks, solver
conﬁgurations, and logs from our runs are available for download.4 We ran all exper-
iments on a cluster equipped with Intel Xeon E5-2620 v4 CPUs. We allocated one
physical CPU core and 8 GB of RAM for each solver-benchmark pair and used a time
limit of 300 s. We use the following two sets of benchmarks:
Array Benchmarks (ARRAYS). The ﬁrst set of benchmarks is derived from the QF_AX
benchmarks in SMT-LIB [3]. To generate these benchmarks, we (i) replace declarations
of arrays with declarations of sequences of uninterpreted sorts, (ii) change the sort of
index terms to integers, and (iii) replace store with update and select with nth. The
resulting benchmarks are quantiﬁer-free and do not contain concatenations. Note that
the original and the derived benchmarks are not equisatisﬁable, because sequences take
into account out-of-bounds cases that do not occur in arrays. For the Z3 runs, we add to
the benchmarks a deﬁnition of update in terms of extraction and concatenation.
Smart Contract Veriﬁcation (DIEM). The second set of benchmarks consists of veri-
ﬁcation conditions generated by running the Move Prover [24] on smart contracts writ-
ten for the Diem framework. By default, the encoding does not use the sequence update
4 http://dx.doi.org/10.5281/zenodo.6146565.

Reasoning About Vectors Using an SMT Theory of Sequences
141
Fig. 7. Figure a lists the number of solved benchmarks and total time on commonly solved bench-
marks. The scatter plots compare the base solver (cvc5) and the extended solver (cvc5-a) on
ARRAY (Fig. b) and DIEM (Fig. c) benchmarks.
operation, and so Z3 can be used directly. However, we also modiﬁed the Move Prover
encoding to generate benchmarks that do use the update operator, and ran cvc5 on them.
In addition to using the sequence theory, the benchmarks make heavy use of quantiﬁers
and the SMT-LIB theory of datatypes.
Figure 7a summarizes the results in terms of number of solved benchmarks and total
time in seconds on commonly solved benchmarks. The conﬁguration that solves the
largest number of benchmarks is the implementation of the extended calculus (cvc5-a).
This approach also successfully solves most of the DIEM benchmarks, which suggests
that sequences are a promising option for encoding vectors in programs. The results
further show that the sequences solver of cvc5 signiﬁcantly outperforms Z3 on both the
number of solved benchmarks and the solving time on commonly solved benchmarks.
Figures 7b and 7c show scatter plots comparing cvc5 and cvc5-a on the two bench-
mark sets. We can see a clear trend towards better performance when using the extended
solver. In particular, the table shows that in addition to solving the most benchmarks,
cvc5-a is also fastest on the commonly solved instances from the DIEM benchmark set.
For the ARRAYS set, we can see that some benchmarks are slower with the extended
solver. This is also reﬂected in the table, where cvc5-a is slower on the commonly
solved instances. This is not too surprising, as the extra machinery of the extended
solver can sometimes slow down easy problems. As problems get harder, however, the
beneﬁt of the extended solver becomes clear. For example, if we drop Z3 and consider
just the commonly solved instances between cvc5 and cvc5-a (of which there are 242),
cvc5-a is about 2.47ˆ faster (426 vs 1053 s). Of course, further improving the perfor-
mance of cvc5-a is something we plan to explore in future work.
7
Conclusion
We introduced calculi for checking satisﬁability in the theory of sequences, which can
be used to model the vector data type. We described our implementation in cvc5 and
provided an evaluation, showing that the proposed theory is rich enough to naturally

142
Y. Sheng et al.
express veriﬁcation conditions without introducing quantiﬁers, and that our implemen-
tation is efﬁcient. We believe that veriﬁcation tools can beneﬁt by changing their encod-
ing of veriﬁcation conditions that involve vectors to use the proposed theory and imple-
mentation.
We plan to propose the incorporation of this theory in the SMT-LIB standard and
contribute our benchmarks to SMT-LIB. As future research, we plan to integrate other
approaches for array solving into our basic solver. We also plan to study the politeness
[16,20] and decidability of various fragments of the theory of sequences.
References
1. Alberti, F., Ghilardi, S., Pagani, E.: Cardinality constraints for arrays (decidability results and
applications). Formal Methods Syst. Des. 51(3), 545–574 (2017). https://doi.org/10.1007/
s10703-017-0279-6
2. Barbosa, H., et al.: cvc5: a versatile and industrial-strength SMT solver. In: Fisman, D., Rosu,
G. (eds.) TACAS 2022. LNCS, vol. 13243, pp. 415–442. Springer, Cham (2022). https://doi.
org/10.1007/978-3-030-99524-9 24
3. Barrett, C., Fontaine, P., Tinelli, C.: The SMT-LIB Standard: Version 2.6. Technical report,
Department of Computer Science, The University of Iowa (2017). www.SMT-LIB.org
4. Barrett, C., Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Splitting on demand in SAT modulo
theories. In: Hermann, M., Voronkov, A. (eds.) LPAR 2006. LNCS (LNAI), vol. 4246, pp.
512–526. Springer, Heidelberg (2006). https://doi.org/10.1007/11916277 35
5. Barrett, C., Tinelli, C.: Satisﬁability modulo theories. In: Clarke, E., Henzinger, T., Veith,
H., Bloem, R. (eds.) Handbook of Model Checking, pp. 305–343. Springer, Cham (2018).
https://doi.org/10.1007/978-3-319-10575-8 11
6. Berzish, M., Ganesh, V., Zheng, Y.: Z3str3: a string solver with theory-aware heuristics. In:
Stewart, D., Weissenbacher, G. (eds.) 2017 Formal Methods in Computer Aided Design,
FMCAD 2017, Vienna, Austria, 2–6 October 2017, pp. 55–59. IEEE (2017)
7. Bjørner, N., de Moura, L., Nachmanson, L., Wintersteiger, C.: Programming Z3 (2018).
https://theory.stanford.edu/∼nikolaj/programmingz3.html#sec-sequences-and-strings
8. Bjørner, N., Ganesh, V., Michel, R., Veanes, M.: An SMT-LIB format for sequences and
regular expressions. SMT 12, 76–86 (2012)
9. Bjørner, N., Tillmann, N., Voronkov, A.: Path feasibility analysis for string-manipulating
programs. In: Kowalewski, S., Philippou, A. (eds.) TACAS 2009. LNCS, vol. 5505, pp. 307–
321. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-00768-2 27
10. Christ, J., Hoenicke, J.: Weakly equivalent arrays. In: Lutz, C., Ranise, S. (eds.) FroCoS
2015. LNCS (LNAI), vol. 9322, pp. 119–134. Springer, Cham (2015). https://doi.org/10.
1007/978-3-319-24246-0 8
11. de Moura, L., Bjørner, N.: Z3: an efﬁcient SMT solver. In: Ramakrishnan, C.R., Rehof, J.
(eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg (2008). https://
doi.org/10.1007/978-3-540-78800-3 24
12. Elad, N., Rain, S., Immerman, N., Kov´acs, L., Sagiv, M.: Summing up smart transitions. In:
Silva, A., Leino, K.R.M. (eds.) CAV 2021. LNCS, vol. 12759, pp. 317–340. Springer, Cham
(2021). https://doi.org/10.1007/978-3-030-81685-8 15
13. Enderton, H.B.: A Mathematical Introduction to Logic, 2nd edn. Academic Press (2001)
14. Falke, S., Merz, F., Sinz, C.: Extending the theory of arrays: memset, memcpy, and
beyond. In: Cohen, E., Rybalchenko, A. (eds.) VSTTE 2013. LNCS, vol. 8164, pp. 108–
128. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-54108-7 6

Reasoning About Vectors Using an SMT Theory of Sequences
143
15. Ganesh, V., Minnes, M., Solar-Lezama, A., Rinard, M.: Word equations with length con-
straints: what’s decidable? In: Biere, A., Nahir, A., Vos, T. (eds.) HVC 2012. LNCS, vol.
7857, pp. 209–226. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-39611-
3 21
16. Jovanovi´c, D., Barrett, C.: Polite theories revisited. In: Ferm¨uller, C.G., Voronkov, A. (eds.)
LPAR 2010. LNCS, vol. 6397, pp. 402–416. Springer, Heidelberg (2010). https://doi.org/10.
1007/978-3-642-16242-8 29
17. Liang, T., Reynolds, A., Tinelli, C., Barrett, C., Deters, M.: A DPLL(T) theory solver for a
theory of strings and regular expressions. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS,
vol. 8559, pp. 646–662. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9 43
18. Nelson, G., Oppen, D.C.: Simpliﬁcation by cooperating decision procedures. ACM Trans.
Program. Lang. Syst. 1(2), 245–257 (1979)
19. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories: from an
abstract Davis-Putnam-Logemann-Loveland procedure to DPLL(T). J. ACM 53(6), 937–977
(2006)
20. Ranise, S., Ringeissen, C., Zarba, C.G.: Combining data structures with nonstably inﬁnite
theories using many-sorted logic. In: Gramlich, B. (ed.) FroCoS 2005. LNCS (LNAI), vol.
3717, pp. 48–64. Springer, Heidelberg (2005). https://doi.org/10.1007/11559306 3
21. Reynolds, A., N¨otzli, A., Barrett, C.W., Tinelli, C.: Reductions for strings and regular expres-
sions revisited. In: 2020 Formal Methods in Computer Aided Design, FMCAD 2020, Haifa,
Israel, 21–24 September 2020, pp. 225–235. IEEE (2020)
22. Reynolds, A., Woo, M., Barrett, C., Brumley, D., Liang, T., Tinelli, C.: Scaling up DPLL(T)
string solvers using context-dependent simpliﬁcation. In: Majumdar, R., Kunˇcak, V. (eds.)
CAV 2017. LNCS, vol. 10427, pp. 453–474. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-63390-9 24
23. Sheng, Y.,et al.: Reasoning about vectors using an SMT theory of sequences. CoRR
10.48550/ARXIV.2205.08095 (2022)
24. Zhong, J.E., et al.: The move prover. In: Lahiri, S.K., Wang, C. (eds.) CAV 2020. LNCS, vol.
12224, pp. 137–150. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-53288-8 7
Open Access This chapter is licensed under the terms of the Creative Commons Attribution
4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use,
sharing, adaptation, distribution and reproduction in any medium or format, as long as you
give appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter’s Creative
Commons license, unless indicated otherwise in a credit line to the material. If material is not
included in the chapter’s Creative Commons license and your intended use is not permitted by
statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder.

Calculi and Orderings

An Eﬃcient Subsumption Test Pipeline
for BS(LRA) Clauses
Martin Bromberger1
, Lorenz Leutgeb1,2(B)
, and Christoph Weidenbach1
1 Max Planck Institute for Informatics, Saarland Informatics Campus,
Saarbr¨ucken, Germany
{mbromber,lorenz,weidenb}@mpi-inf.mpg.de
2 Graduate School of Computer Science, Saarland Informatics Campus,
Saarbr¨ucken, Germany
Abstract. The importance of subsumption testing for redundancy elim-
ination in ﬁrst-order logic automatic reasoning is well-known. Although
the problem is already NP-complete for ﬁrst-order clauses, the mean-
while developed test pipelines eﬃciently decide subsumption in almost
all practical cases. We consider subsumption between ﬁrst-oder clauses of
the Bernays-Sch¨onﬁnkel fragment over linear real arithmetic constraints:
BS(LRA). The bottleneck in this setup is deciding implication between
the LRA constraints of two clauses. Our new sample point heuristic pre-
empts expensive implication decisions in about 94% of all cases in bench-
marks. Combined with ﬁltering techniques for the ﬁrst-order BS part
of clauses, it results again in an eﬃcient subsumption test pipeline for
BS(LRA) clauses.
Keywords: Bernays-Sch¨onﬁnkel fragment · Linear arithmetic ·
Redundancy elimination · Subsumption
1
Introduction
The elimination of redundant clauses is crucial for the eﬃcient automatic rea-
soning in ﬁrst-order logic. In a resolution [5,50] or superposition setting [4,44], a
newly inferred clause might be subsumed by a clause that is already known (for-
ward subsumption) or it might subsume a known clause (backward subsumption).
Although the SCL calculi family [1,11,21] does not require forward subsump-
tion tests, a property also inherent to the propositional CDCL (Conﬂict Driven
Clause Learning) approach [8,34,41,55,63], backward subsumption and hence
subsumption remains an important test in order to remove redundant clauses.
In this work we present advances in deciding subsumption for constrained
clauses, speciﬁcally employing the Bernays-Sch¨onﬁnkel fragment as foreground
logic, and linear real arithmetic as background theory, BS(LRA). BS(LRA) is of
particular interest because it can be used to model supervisors, i.e., components
in technical systems that control system functionality. An example for a super-
visor is the electronic control unit of a combustion engine. The logics we use
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 147–168, 2022.
https://doi.org/10.1007/978-3-031-10769-6_10

148
M. Bromberger et al.
to model supervisors and their properties are called SupERLogs—(Sup)ervisor
(E)ﬀective(R)easoning (Log)ics. SupERLogs are instances of function-free ﬁrst-
order logic extended with arithmetic [18], which means BS(LRA) is an example
of a SupERLog.
Subsumption is an important redundancy criterion in the context of hier-
archic clausal reasoning [6,11,20,35,37]. At the heart of this paper is a new
technique to speed up the treatment of linear arithmetic constraints as part of
deciding subsumption. For every clause, we store a solution of its associated con-
straints, which is used to quickly falsify implication decisions, acting as a ﬁlter,
called the sample point heuristic. In our experiments with various benchmarks,
the technique is very eﬀective: It successfully preempts expensive implication
decisions in about 94% of cases. We elaborate on these ﬁndings in Sect. 4.
For example, consider three BS clauses, none of which subsumes another:
C1 := P(a, x)
C2 := ¬P(y, z) ∨Q(y, z, b)
C3 := ¬R(b) ∨Q(a, x, b)
Let C4 be the resolvent of C1 and C2 upon the atom P(a, x), i.e., C4 := Q(a, z, b).
Now C4 backward-subsumes C3 with matcher σ := {z →x}, i.e. C4σ ⊂C3, thus
C3 is redundant and can be eliminated. Now, consider an extension of the above
clauses with some simple LRA constraints following the same reasoning:
C′
1 := x ≥1 ∥P(a, x)
C′
2 := z ≥0 ∥¬P(y, z) ∨Q(y, z, b)
C′
3 := x ≥0 ∥¬R(b) ∨Q(a, x, b)
where ∥is interpreted as an implication, i.e., clause C′
1 stands for ¬x ≥1∨P(a, x)
or simply x < 1 ∨P(a, x). The respective resolvent on the constrained clauses
is C′
4 := z ≥0, z ≥1 ∥Q(a, z, b) or after constraint simpliﬁcation C′
4 := z ≥
1 ∥Q(a, z, b) because z ≥1 implies z ≥0. For the constrained clauses, C′
4 does
no longer subsume C′
3 with matcher σ := {z →x}, because z ≥0 does not
LRA-imply z ≥1. Now, if we store the sample point x = 0 as a solution for
the constraint of clause C′
3, this sample point already reveals that z ≥0 does
not LRA-imply z ≥1. This constitutes the basic idea behind our sample point
heuristic. In general, constraints are not just simple bounds as in the above
example, and sample points are solutions to the system of linear inequalities of
the LRA constraint of a clause.
Please note that our test on LRA constraints is based on LRA theory impli-
cation and not on a syntactic notion such as subsumption on the ﬁrst-order part
of the clause. In this sense it is “stronger” than its ﬁrst-order counterpart. This
fact is stressed by the following example, taken from [26, Ex. 2], which shows
that ﬁrst-order implication does not imply subsumption. Let
C1 := ¬P(x, y) ∨¬P(y, z) ∨P(x, z)
C2 := ¬P(a, b) ∨¬P(b, c) ∨¬P(c, d) ∨P(a, d)
Then we have C1 →C2, but again, for all σ we have C1σ ̸⊆C2: Constructing
σ from left to right we obtain σ := {x →a, y →b, z →c}, but P(a, c) ̸∈C2.

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
149
Constructing σ from right to left we obtain σ := {z →d, x →a, y →c}, but
¬P(a, c) ̸∈C2.
Related Work. Treatment of questions regarding the complexity of deciding sub-
sumption of ﬁrst-order clauses [27] dates back more than thirty years. Notions
of subsumption, varying in generality, are studied in diﬀerent sub-ﬁelds of the-
orem proving, whereas we restrict our attention to ﬁrst-order theorem proving.
Modern implementations typically decide multiple thousand instances of this
problem per second: In [62, Sect. 2], Voronkov states that initial versions of
Vampire “seemed to [. . . ] deadlock” without eﬃcient implementations to decide
(forward) subsumption.
In order to reduce the number of clauses out of a set of clauses to be con-
sidered for pairwise subsumption checking, the best known practice in ﬁrst-
order theorem proving is to use (imperfect) indexing data structures as a means
for pre-ﬁltering and research concerning appropriate techniques is plentiful, see
[24,25,27–30,33,39,40,43,45–49,52–54,56,59,61] for an evaluation of these tech-
niques. Here we concentrate on the eﬃciency of a subsumption check between
two clauses and therefore do not take indexing techniques into account. Fur-
thermore, the implication test between two linear arithmetic constraints is of
a semantic nature and is not related to any syntactic features of the involved
constraints and can therefore hardly be ﬁltered by a syntactic indexing approach.
In addition to pre-ﬁltering via indexing, almost all above mentioned imple-
mentations of ﬁrst-order subsumption tests rely on additional ﬁlters on the clause
level. The idea is to generate an abstraction of clauses together with an ordering
relation such that the ordering relation is necessary to hold between two clauses
in order for one clause to subsume the other. Furthermore, the abstraction as
well as the ordering relation should be eﬃciently computable. For example, a
necessary condition for a ﬁrst-order clause C1 to subsume a ﬁrst-order clause
C2 is | vars(C1)| ≥| vars(C2)|, i.e., the number of diﬀerent variables in C1 must
be larger or equal than the number of variables in C2. Further and additional
abstractions included by various implementations rely on the size of clauses,
number of ground literals, depth of literals and terms, occurring predicate and
function symbols. For the BS(LRA) clauses considered here, the structure of the
ﬁrst-order BS part, which consists of predicates and ﬂat terms (variables and
constants) only, is not particularly rich.
The exploration of sample points has already been studied in the context of
ﬁrst-order clauses with arithmetic constraints. In [17,36] it was used to improve
the performance of iSAT [23] on testing non-linear arithmetic constraints. In
general, iSAT tests satisﬁability by interval propagation for variables. If intervals
get “too small” it typically gives up, however sometimes the explicit generation
of a sample point for a small interval can still lead to a certiﬁcate for satisﬁability.
This technique was successfully applied in [17], but was not used for deciding
subsumption of constrained clauses.

150
M. Bromberger et al.
Motivation. The main motivation for this work is the realization that comput-
ing implication decisions required to treat constraints of the background theory
presents the bottleneck of an BS(LRA) subsumption check in practice. Inspired
by the success of ﬁltering techniques in ﬁrst-order logic, we devise an exception-
ally eﬀective ﬁlter for constraints and adopt well-known ﬁrst-order ﬁlters to the
BS fragment. Our sample point heuristic for LRA could easily be generalized to
other arithmetic theories as well as full ﬁrst-order logic.
Structure. The paper is structured as follows. After a section deﬁning BS(LRA)
and common notions and notation, Sect. 2, we deﬁne redundancy notions and our
sample point heuristic in Sect. 3. Section 4 justiﬁes the success of the sample point
heuristic by numerous experiments in various application domains of BS(LRA).
The paper ends with a discussion of the obtained results, Sect. 5. Binaries, utility
scripts, benchmarking instances used as input, and the output used for evaluation
may be obtained online [13].
2
Preliminaries
We brieﬂy recall the basic logical formalisms and notations we build upon [10].
Our starting point is a standard many-sorted ﬁrst-order language for BS with
constants (denoted a, b, c), without non-constant function symbols, with vari-
ables (denoted w, x, y, z), and predicates (denoted P, Q, R) of some ﬁxed arity.
Terms (denoted t, s) are variables or constants. An atom (denoted A, B) is an
expression P(t1, . . . , tn) for a predicate P of arity n. A positive literal is an
atom A and a negative literal is a negated atom ¬A. We deﬁne comp(A) = ¬A,
comp(¬A) = A, |A| = A and |¬A| = A. Literals are usually denoted L, K, H.
Formulas are deﬁned in the usual way using quantiﬁers ∀, ∃and the boolean
connectives ¬, ∨, ∧, →, and ≡.
A clause (denoted C, D) is a universally closed disjunction of literals A1∨· · ·∨
An∨¬B1∨· · ·∨¬Bm. Clauses are identiﬁed with their respective multisets and all
standard multiset operations are extended to clauses. For instance, C ⊆D means
that all literals in C also appear in D respecting their number of occurrences. A
clause is Horn if it contains at most one positive literal, i.e. n ⩽1, and a unit
clause if it has exactly one literal, i.e. n + m = 1. We write C+ for the set of
positive literals, or conclusions of C, i.e. C+ := {A1, . . . , An} and respectively
C−for the set of negative literals, or premises of C, i.e. C−:= {¬B1, . . . , ¬Bm}.
If Y is a term, formula, or a set thereof, vars(Y ) denotes the set of all variables
in Y , and Y is ground if vars(Y ) = ∅.
The Bernays-Sch¨onﬁnkel Clause Fragment (BS) in ﬁrst-order logic consists
of ﬁrst-order clauses where all involved terms are either variables or constants.
The Horn Bernays-Sch¨onﬁnkel Clause Fragment (HBS) consists of all sets of BS
Horn clauses.
A substitution σ is a function from variables to terms with a ﬁnite domain
dom(σ) = {x | xσ ̸= x} and codomain codom(σ) = {xσ | x ∈dom(σ)}. We
denote substitutions by σ, δ, ρ. The application of substitutions is often written

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
151
postﬁx, as in xσ, and is homomorphically extended to terms, atoms, literals,
clauses, and quantiﬁer-free formulas. A substitution σ is ground if codom(σ) is
ground. Let Y denote some term, literal, clause, or clause set. A substitution σ
is a grounding for Y if Y σ is ground, and Y σ is a ground instance of Y in this
case. We denote by gnd(Y ) the set of all ground instances of Y , and by gndB(Y )
the set of all ground instances over a given set of constants B. The most general
uniﬁer mgu(Z1, Z2) of two terms/atoms/literals Z1 and Z2 is deﬁned as usual,
and we assume that it does not introduce fresh variables and is idempotent.
We assume a standard many-sorted ﬁrst-order logic model theory, and write
A ⊨φ if an interpretation A satisﬁes a ﬁrst-order formula φ. A formula ψ is a
logical consequence of φ, written φ ⊨ψ, if A ⊨ψ for all A such that A ⊨φ. Sets
of clauses are semantically treated as conjunctions of clauses with all variables
quantiﬁed universally.
2.1
Bernays-Sch¨onﬁnkel with Linear Real Arithmetic
The extension of BS with linear real arithmetic, BS(LRA), is the basis for the
formalisms studied in this paper. We consider a standard many-sorted ﬁrst-
order logic with one ﬁrst-order sort F and with the sort R for the real numbers.
Given a clause set N, the interpretations A of our sorts are ﬁxed: RA = R and
FA = F. This means that FA is a Herbrand interpretation, i.e., F is the set of
ﬁrst-order constants in N, or a single constant out of the signature if no such
constant occurs. Note that this is not a deviation from standard semantics in
our context as for the arithmetic part the canonical domain is considered and
the ﬁrst-order sort has the ﬁnite model property over the occurring constants
(note that equality is not part of BS).
Constant symbols, arithmetic function symbols, variables, and predicates are
uniquely declared together with their respective sort. The unique sort of a con-
stant symbol, variable, predicate, or term is denoted by the function sort(Y )
and we assume all terms, atoms, and formulas to be well-sorted. We assume
pure input clause sets, which means the only constants of sort R are (rational)
numbers. This means the only constants that we do allow are rational num-
bers c ∈Q and the constants deﬁning our ﬁnite ﬁrst-order sort F. Irrational
numbers are not allowed by the standard deﬁnition of the theory. The current
implementation comes with the caveat that only integer constants can be parsed.
Satisﬁability of pure BS(LRA) clause sets is semi-decidable, e.g., using hierar-
chic superposition [6] or SCL(T) [11]. Impure BS(LRA) is no longer compact
and satisﬁability becomes undecidable, but its restriction to ground clause sets
is decidable [22].
All arithmetic predicates and functions are interpreted in the usual way.
An interpretation of BS(LRA) coincides with ALRA on arithmetic predicates
and functions, and freely interprets free predicates. For pure clause sets this is
well-deﬁned [6]. Logical satisfaction and entailment is deﬁned as usual, and uses
similar notation as for BS.
Example 1. The clause y < 5 ∨x′ ̸= x + 1 ∨¬S0(x, y) ∨S1(x′, 0) is part of
a timed automaton with two clocks x and y modeled in BS(LRA). It represents

152
M. Bromberger et al.
a transition from state S0 to state S1 that can be traversed only if clock y is at
least 5 and that resets y to 0 and increases x by 1.
Arithmetic terms are constructed from a set X of variables, the set of integer
constants c ∈Z, and binary function symbols + and −(written inﬁx). Addi-
tionally, we allow multiplication · if one of the factors is an integer constant.
Multiplication only serves us as syntactic sugar to abbreviate other arithmetic
terms, e.g., x + x + x is abbreviated to 3 · x. Atoms in BS(LRA) are either
ﬁrst-order atoms (e.g., P(13, x)) or (linear) arithmetic atoms (e.g., x < 42).
Arithmetic atoms are denoted by λ and may use the predicates ≤, <, ̸=, =, >, ≥,
which are written inﬁx and have the expected ﬁxed interpretation. We use ◁as a
placeholder for any of these predicates. Predicates used in ﬁrst-order atoms are
called free. First-order literals and related notation is deﬁned as before. Arith-
metic literals coincide with arithmetic atoms, since the arithmetic predicates are
closed under negation, e.g., ¬(x ≥42) ≡x < 42.
BS(LRA) clauses are deﬁned as for BS but using BS(LRA) atoms. We often
write clauses in the form Λ ∥C where C is a clause solely built of free ﬁrst-order
literals and Λ is a multiset of LRA atoms called the constraint of the clause.
A clause of the form Λ ∥C is therefore also called a constrained clause. The
semantics of Λ ∥C is as follows:
Λ ∥C
iﬀ
 
λ∈Λ
λ

→C
iﬀ
 
λ∈Λ
¬λ

∨C
For example, the clause x > 1∨y ̸= 5∨¬Q(x)∨R(x, y) is also written x ≤1, y =
5||¬Q(x) ∨R(x, y). The negation ¬(Λ ∥C) of a constrained clause Λ ∥C where
C = A1 ∨· · · ∨An ∨¬B1 ∨· · · ∨¬Bm is thus equivalent to (
λ∈Λ λ) ∧¬A1 ∧
· · · ∧¬An ∧B1 ∧· · · ∧Bm. Note that since the neutral element of conjunction is
⊤, an empty constraint is thus valid, i.e. equivalent to true.
An assignment for a constraint Λ is a substitution (denoted β) that maps
all variables in vars(Λ) to real numbers c ∈R. An assignment is a solution
for a constraint Λ if all atoms λ ∈(Λβ) evaluate to true. A constraint Λ is
satisﬁable if there exists a solution for Λ. Otherwise it is unsatisﬁable. Note that
assignments can be extended to C by also mapping variables of the ﬁrst-order
sort accordingly.
A clause or clause set is abstracted if its ﬁrst-order literals contain only vari-
ables or ﬁrst-order constants. Every clause C is equivalent to an abstracted clause
that is obtained by replacing each non-variable arithmetic term t that occurs in
a ﬁrst-order atom by a fresh variable x while adding an arithmetic atom x ̸= t
to C. We assume abstracted clauses for theory development, but we prefer non-
abstracted clauses in examples for readability, e.g., a unit clause P(3, 5) is consid-
ered in the development of the theory as the clause x = 3, y = 5 ∥P(x, y). In the
implementation, we mostly prefer abstracted clauses except that we allow inte-
ger constants c ∈Z to appear as arguments of ﬁrst-order literals. In some cases,
this makes it easier to recognize whether two clauses can be matched or not. For
instance, we see by syntactic comparison that the two unit clauses P(3, 5) and
P(0, 1) have no substitution σ such that P(3, 5) = P(0, 1)σ. For the abstracted

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
153
versions on the other hand, x = 3, y = 5 ∥P(x, y) and u = 0, v = 1 ∥P(u, v) we
can ﬁnd a matching substitution for the ﬁrst-order part σ := {u →x, v →y}
and would have to check the constraints semantically to exclude the matching.
Hierarchic Resolution. One inference rule, foundational to most algorithms for
solving constrained ﬁrst-order clauses, is hierarchic resolution [6]:
Λ1 ∥L1 ∨C1
Λ2 ∥L2 ∨C2
σ = mgu(L1, comp(L2))

Λ1, Λ2 ∥C1 ∨C2

σ
The conclusion is called hierarchic resolvent (of the two clauses in the premise).
A refutation is the sequence of resolution steps that produces a clause Λ ∥
⊥with ALRA ⊨Λδ for some grounding δ. Hierarchic resolution is sound and
refutationally complete for the BS(LRA) clauses considered here, since every
set N of BS(LRA) clauses is suﬃciently complete [6], because all constatnts
of the arithemtic sort are numbers. Hence hierarchic resolution is sound and
refutationally complete for N [6,7]. Hierarchic unit resolution is a special case
of hierarchic resolution, that only combines two clauses in case one of them is a
unit clause. Hierarchic unit resolution is sound and complete for HBS(LRA) [6,7],
but not even refutationally complete for BS(LRA).
Most algorithms for Bernays-Schn¨onﬁnkel, ﬁrst-order logic, and beyond uti-
lize resolution. The SCL(T) calculus for HBS(LRA) uses hierarchic resolution
in order to learn from the conﬂicts it encounters during its search. The hierar-
chic superposition calculus on the other hand derives new clauses via hierarchic
resolution based on an ordering. The goal is to either derive the empty clause
or a saturation of the clause set, i.e., a state from which no new clauses can be
derived. Each of those algorithms must derive new clauses in order to progress,
but their subroutines also get progressively slower as more clauses are derived. In
order to increase eﬃciency, it is necessary to eliminate clauses that are obsolete.
One measure that determines whether a clause is useful or not is redundancy.
Redundancy. In order to deﬁne redundancy for constrained clauses, we need
an H-order, i.e., a well-founded, total, strict ordering ≺on ground literals such
that literals in the constraints (in our case arithmetic literals) are always smaller
than ﬁrst-order literals. Such an ordering can be lifted to constrained clauses and
sets thereof by its respective multiset extension. Hence, we overload any such
order ≺for literals, constrained clauses, and sets of constrained clause if the
meaning is clear from the context. We deﬁne ⪯as the reﬂexive closure of ≺and
N ⪯Λ∥C := {D | D ∈N and D ⪯Λ ∥C}. An instance of an LPO [15] with
appropriate precedence can serve as an H-order.
Deﬁnition 2 (Clause Redundancy). A ground clause Λ ∥C is redundant
with respect to a set N of ground clauses and an H-order ≺if N ⪯Λ ∥C ⊨Λ ∥C.
A clause Λ ∥C is redundant with respect to a clause set N and an H-order ≺
if for all Λ′ ∥C′ ∈gnd(Λ ∥C) the clause Λ′ ∥C′ is redundant with respect to
gnd(N).

154
M. Bromberger et al.
If a clause Λ ∥C is redundant with respect to a clause set N, then it can be
removed from N without changing its semantics. Determining clause redundancy
is an undecidable problem [11,63]. However, there are special cases of redundant
clauses that can be easily checked, e.g., tautologies and subsumed clauses. Tech-
niques for tautology deletion and subsumption deletion are the most common
elimination techniques in modern ﬁrst-order provers.
A tautology is a clause that evaluates to true independent of the predicate
interpretation or assignment. It is therefore redundant with respect to all orders
and clause sets; even the empty set.
Corollary 3 (Tautology for Constrained Clauses). A clause Λ ∥C is a
tautology if the existential closure of ¬(Λ ∥C) is unsatisﬁable.
Since ¬(Λ ∥C) is essentially ground (by existential closure and skolemiza-
tion), it can be solved with an appropriate SMT solver, i.e., an SMT solver that
supports unquantiﬁed uninterpreted functions coupled with linear real arith-
metic. In [2], it is recommended to check only the following conditions for tau-
tology deletion in hierarchic superposition:
Corollary 4 (Tautology Check).
A clause Λ ∥C is a tautology if the exis-
tential closure of Λ is unsatisﬁable or if C contains two literals L1 and L2 with
L1 = comp(L2).
The advantage is that the check on the ﬁrst-order side of the clause is still
purely syntactic and corresponds to the tautology check for pure ﬁrst-order logic.
Nonetheless, there are tautologies that are not captured by Corollary 4, e.g.,
x = y ∥P(x) ∨¬P(y). The SCL(T) calculus on the other hand requires no
tautology checks because it never learns tautologies as part of its conﬂict analysis
[1,11,21]. This property is also inherent to the propositional CDCL (Conﬂict
Driven Clause Learning) approach [8,34,41,55,63].
3
Subsumption for Constrained Clauses
A subsumed constrained clause is a clause that is redundant with respect to a
single clause in our clause set. Formally, subsumption is deﬁned as follows.
Deﬁnition 5. (Subsumption for Constrained Clauses [2]). A constrained
clause Λ1 ∥C1 subsumes another constrained clause Λ2 ∥C2 if there exists a sub-
stitution σ such that C1σ ⊆C2, vars(Λ1σ) ⊆vars(Λ2), and the universal closure
of Λ2 →(Λ1σ) holds in LRA.
Eliminating redundant clauses is crucial for the eﬃcient operation of an auto-
matic ﬁrst-order theorem prover. Although subsumption is considered one of the
easier redundancy relationships that we can check in practice, it is still a hard
problem in general:
Lemma 6. (Complexity of Subsumption in the BS Fragment). Deciding
subsumption for a pair of BS clauses is NP-complete.

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
155
Proof. Containment in NP follows from the fact that the size of subsumption
matchers is limited by the subsumed clause and set inclusion of literals can
be decided in polynomial time. For the hardness part, consider the following
polynomial-time reduction from 3-SAT. Take a propositional clause set where
all clauses have length three. Now introduce a 6-place predicate R and encode
each propositional variable P by a ﬁrst-order variable xP . Then a propositional
clause L1 ∨L2 ∨L3 can be encoded by an atom R(xP1, p1, xP2, p2, xP3, p3) where
pi is 0 if Li is negative and 1 otherwise and Pi is the predicate of Li. This way
the clause set N can be represented by a single BS clause CN. Now construct a
clause D that contains all atoms representing the way a clause of length three
can become true by ground atoms over R and constants 0, 1. For example, it
contains atoms like R(0, 0, . . .) and R(1, 1, . . .) representing that the ﬁrst literal
of a clause is true. Actually, for each such atom R(0, 0, . . .) the clause D contains
|CN| copies. Finally, CN subsumes D if and only if N is satisﬁable.
⊓⊔
In order to be eﬃcient, modern theorem provers need to decide multiple
thousand subsumption checks per second. In the pure ﬁrst-order case, this is
possible because of indexing and ﬁltering techniques that quickly decide most
subsumption checks [24,25,27–30,33,39,40,45–49,52–54,56,59,61,62].
For BS(LRA) (and FOL(LRA)), there also exists research on how to perform
the subsumption check in general [2,36], but the literature contains no dedicated
indexing or ﬁltering techniques for the constraint part of the subsumption check.
In this section and as the main contribution of this paper, we present the ﬁrst
such ﬁltering techniques for BS(LRA). But ﬁrst, we explain how to solve the
subsumption check for constrained clauses in general.
First-Order Check. The ﬁrst step of the subsumption check is exactly the
same as in ﬁrst-order logic without arithmetic. We have to ﬁnd a substitution
σ, also called a matcher, such that C1σ ⊆C2. The only diﬀerence is that it is
not enough to compute one matcher σ, but we have to compute all matchers
for C1σ ⊆C2 until we ﬁnd one that satisﬁes the implication Λ2 →(Λ1σ). For
instance, there are two matchers for the clauses C1 := x + y ≥0 ∥Q(x, y) and
C2 := x < 0, y ≥0 ∥Q(x, x) ∨Q(y, y). The matcher {x →y} satisﬁes the
implication Λ2 →(Λ1σ) and {y →x} does not. Our own algorithm for ﬁnding
matchers is in the style of Stillman except that we continue after we ﬁnd the
ﬁrst matcher [27,58].
Implication Check. The universal closure of the implication Λ2 →(Λ1σ) can
be solved by any SMT solver for the respective theory after we negate it. Note
that the resulting formula
∃x1, . . . , xn. Λ2 ∧¬(Λ1σ)
where {x1, . . . , xn} = vars(Λ2)
(1)
is already in clause normal form and that the formula can be treated as ground
since existential variables can be handled as constants. Intuitively, the universal
closure Λ2 →(Λ1σ) asserts that the set of solutions satisfying Λ2 is a subset of

156
M. Bromberger et al.
Fig. 1. Solutions of the constraints Λ1σ, Λ2, and Λ3 depicted as polytopes
the set of solutions satisfying Λ1σ. This means a solution to its negation (1) is a
solution for Λ2, but not for Λ1σ, thus a counterexample of the subset relation.
Example 7. Let us now look at an example to illustrate the role that formula (1)
plays in deciding subsumption. In our example, we have three clauses: Λ1 ∥C1,
Λ2 ∥C2, and Λ3 ∥C2, where C1 := ¬P(x, y) ∨Q(u, z), C2 := ¬P(x, y) ∨Q(2, x),
Λ1 := y ≥0 , y ≤u , y ≤x+z , y ≥x+z−2·u, Λ2 := x ≥1 , y ≤1 , y ≥x−1,
and Λ3 := x ≥2 , y ≤1 , y ≥x −2. Our goal is to test whether Λ1 ∥C1
subsumes the other two clauses. As our ﬁrst step, we try to ﬁnd a substitution
σ such that C1σ ⊆C2. The most general substitution fulﬁlling this condition is
σ := {z →x, u →2}. Next, we check whether Λ1σ is implied by Λ2 and Λ3.
Normally, we would do so by solving the formula (1) with an SMT solver, but to
help our intuitive understanding, we instead look at their solution sets depicted
in Fig. 1. Note that Λ1σ simpliﬁes to Λ1σ := y ≥0 , y ≤2 , y ≤2·x , y ≥2·x−4.
Here we see that the solution set for Λ2 is a subset of Λ1σ. Hence, Λ2 implies
Λ1σ, which means that Λ2 ∥C2 is subsumed by Λ1 ∥C1. The solution set for Λ3
is not a subset of Λ1σ. For instance, the assignment β2 := {x →3, y →1} is
a counterexample and therefore a solution to the respective instance of formula
(1). Hence, Λ1 ∥C1 does not subsume Λ3 ∥C2.
Excess Variables. Note that in general it is not suﬃcient to ﬁnd a sub-
stitution σ that matches the ﬁrst-order parts to also match the theory con-
straints: C1σ ⊆C2 does not generally imply vars(Λ1σ) ⊆vars(Λ2). In par-
ticular, if Λ1 contains variables that do not appear in the ﬁrst-order part
C1, then these must be projected to Λ2. We arrive at a variant of (1), that
is ∃x1, . . . , xn∀y1, . . . , ym. Λ2 ∧¬(Λ1σ) where {x1, . . . , xn} = vars(Λ2) and
{y1, . . . , ym} = vars(Λ1) \ vars(C1). Our solution to this problem is to normal-
ize all clauses Λ ∥C by eliminating all excess variables Y := vars(Λ) \ vars(C)
such that vars(Λ) ⊆vars(C) is guaranteed. For linear real arithmetic this is
possible with quantiﬁer elimintation techniques, e.g., Fourier-Motzkin elimina-
tion (FME). Although these techniques typically cause the size of Λ to increase
exponentially, they often behave well in practice. In fact, we get rid of almost
all excess variables in our benchmark examples with simpliﬁcation techniques
based on Gaussian elimination with execution time linear in the number of LRA
atoms. Given the precondition Y = ∅achieved by such elimination techniques,

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
157
we can compute σ as matcher for the ﬁrst-order parts and then directly use it
for testing whether the universal closure of Λ2 →(Λ1σ) holds. An alternative
solution to the issue of excess variables has been proposed: In [2], the substitu-
tion σ is decomposed as σ = δτ, where δ is the ﬁrst-order matcher and τ is a
theory matcher, i.e. dom(τ) ⊆Y and vars(codom(τ)) ⊆vars(Λ2). Then, exploit-
ing Farkas’ lemma, the computation of τ is reduced to testing the feasibility of
a linear program (restricted to matchers that are aﬃne transformations).
The reduction to solving a linear program oﬀers polynomial worst-case com-
plexity but in practice typically behaves worse than solving the variant with
quantiﬁer alternations using an SMT solver such as Z3 [36,42].
Filtering First-Order Literals. Even though deciding implication of theory
constraints is in practice more expensive than constructing a matcher and decid-
ing inclusion of ﬁrst-order literals, we still incorporate some lightweight ﬁlters
for our evaluation. Inspired by Schulz [54] we choose three features, so that every
feature f maps clauses to N0, and f(C1) ⩽f(C2) is necessary for C1σ ⊆C2.
The features are: |C+|, the number of positive ﬁrst-order literals in C, |C−|,
the number of negative ﬁrst-order literals in C, and ⌊C⌋, the number of occur-
rences of constants in C.
Sample Point Heuristic. The majority of subsumption tests fail because we
cannot ﬁnd a ﬁtting substitution for their ﬁrst-order parts. In our experiments,
between 66.5% and 99.9% of subsumption tests failed this way. This means our
tool only has to check in less than 33.5% of the cases whether one theory con-
straint implies the other. Despite this, our tool spends more time on implication
checks than on the ﬁrst-order part of the subsumption tests without ﬁltering on
the constraint implication tests. The reason is that constraint implication tests
are typically much more expensive than the ﬁrst-order part of a subsumption
test. For this reason, we developed the sample point heuristic that is much faster
to execute than a full constraint implication test, but still ﬁlters out the majority
of implications that do not hold (in our experiments between 93.8% and 100%).
The idea behind the sample point heuristic is straightforward. We store for
each clause Λ ∥C a sample solution β for its theory constraint Λ. Before we
execute a full constraint implication test, we simply evaluate whether the sample
solution β for Λ2 is also a solution for Λ1σ. If this is not the case, then β is a
solution for (1) and a counterexample for the implication. If β is a solution for
Λ1σ, then the heuristic returns unknown and we have to execute a full constraint
implication test, i.e., solve the SMT problem (1).
Often it is possible to get our sample solutions for free. Theorem provers
based on hierarchic superposition typically check for every new clause Λ ∥C
whether Λ is satisﬁable in order to eliminate tautologies. This means we can
already use this tautology check to compute and store a sample solution for
every new clause without extra cost. We only need to pick a solver for the check
that returns a solution as a certiﬁcate of satisﬁability. Although the SCL(T)
calculus never learns any tautologies, it is also possible to get a sample solution
for free as part of its conﬂict analysis [11].

158
M. Bromberger et al.
Example 8. We revisit Example 7 to illustrate the sample point heuristic. During
the tautology check for Λ2 ∥C2 and Λ3 ∥C2, we determined that β1 := {x →
2, y →1} is a sample solution for Λ2 and β2 := {x →3, y →1} a sample
solution for Λ3. Since Λ2 implies Λ1σ, all sample solutions for Λ2 automatically
satisfy Λ1σ. This is the reason why the sample point heuristic never ﬁlters out an
implication that actually holds, i.e., it returns unknown when we test whether Λ2
implies Λ1σ. The assignment β2 on the other hand does not satisfy Λ1σ. Hence,
the sample point heuristic correctly claims that Λ3 does not imply Λ1σ. Note
that we could also have chosen β1 as the sample point for Λ3. In this case, the
sample point heuristic would also return unknown for the implication Λ3 →Λ1σ
although the implication does not hold.
Trivial Cases. Subsumption tests become much easier if the constraint Λi of
one of the participating clauses is empty. We use two heuristic ﬁlters to exploit
this fact. We highlight them here because they already exclude some subsump-
tion tests before we reach the sample point heuristic in our implementation.
The empty conclusion heuristic exploits that Λ1 is valid if Λ1 is empty. In this
case, all implications Λ2 →(Λ1σ) hold because Λ1σ evaluates to true under any
assignment. So by checking whether Λ1 = ∅, we can quickly determine whether
Λ2 →(Λ1σ) holds for some pairs of clauses. Note that in contrast to the sample
point heuristic, this heuristic is used to ﬁnd valid implications.
The empty premise test exploits that Λ2 is valid if Λ2 is empty. In this case,
an implication Λ2 →(Λ1σ) may only hold if Λ1σ simpliﬁes to the empty set as
well. This is the case because any inequality in the canonical form n
i=1 aixi◁c
either simpliﬁes to true (because ai = 0 for all i = 1, . . . , n and 0◁c holds) and
can be removed from Λ1σ, or the inequality eliminates at least one assignment
as a solution for Λ1σ [51]. So if Λ2 = ∅, we check whether Λ1σ simpliﬁes to the
empty set instead of solving the SMT problem (1).
Pipeline. We call our approach a pipeline since it combines multiple procedures,
which we call stages, that vary in complexity and are independent in principle,
for the overall aim of eﬃciently testing subsumption. Pairs of clauses that “make
it through” all stages, are those for which the subsumption relation holds. The
pipeline is designed with two goals in mind: (1) To reject as many pairs of
clauses as early as possible, and (2) to move stages further towards the end of
the pipeline the more expensive they are.
The pipeline consists of six stages, all of which are mentioned above. We
divide the pipeline into two phases, the ﬁrst-order phase (FO-phase) consisting
of two stages, and the constraint phase (C-phase), consisting of four stages.
First-order ﬁltering rejects all pairs of clauses for which f(C1) > f(C2) holds.
Then, matching constructs all matchers σ such that C1σ ⊆C2. Every matcher
is individually tested in the constraint phase. Technically, this means that the
input of all following stages is not just a pair of clauses, but a triple of two clauses
and a matcher. The constraint phase then proceeds with the empty conclusion
heuristic and the empty premise test to accept (resp. reject) all trivial cases of

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
159
Algorithm 1: Saturation prover used for evaluation
Input
: A set N of clauses.
Output : ⊥or “unknown”.
1 U := {C ∈N | |C| = 1}
2 while U ̸= ∅do
3
M := ∅
4
foreach C ∈U do M := M ∪resolvents(C, N)
5
if ⊥∈M then return ⊥
6
reduce M using N
(forward subsumption)
7
if M = ∅then return “unknown”
8
reduce N using M
(backward subsumption)
9
U := {C ∈M | |C| = 1}
10
N := N ∪M
11 end
12 return “unknown”
the constraint implication test. The next stage is the sample point heuristic.
If the sample solution β2 for Λ2 is no solution for Λ1 (i.e. ⊭Λ1σβ2), then the
matcher σ is rejected. Otherwise (i.e. ⊨Λ1σβ2), the implication test Λ2 →(Λ1σ)
is performed by solving the SMT problem (1) to produce the overall result of
the pipeline and ﬁnally determine whether subsumption holds.
4
Experimentation
In order to evaluate our new approach on three benchmark instances, derived
from BS(LRA) applications, all presented techniques and their combination in
form of a pipeline were implemented in the theorem prover SPASS-SPL, a pro-
totype for BS(LRA) reasoning.
Note that SPASS-SPL contains more than one approach for BS(LRA) rea-
soning, e.g., the Datalog hammer for HBS(LRA) reasoning [10]. These vari-
ous modes of operation operate independently, and the desired mode is cho-
sen via command-line option. The reasoning approach discussed here is the
current default option. On the ﬁrst-order side, SPASS-SPL consists of a sim-
ple saturation prover based on hierarchic unit resolution, see Algorithm 1. It
resolves unit clauses with other clauses until either the empty clause is derived
or no new clauses can be derived. Note that this procedure is only complete
for Horn clauses. For arithmetic reasoning, SPASS-SPL relies on SPASS-SATT,
our sound and complete CDCL(LA) solver for quantiﬁer-free linear real and
linear mixed/integer arithmetic [12]. SPASS-SATT implements a version of the
dual simplex algorithm ﬁne-tuned towards SMT solving [16]. In order to ensure
soundness, SPASS-SATT represents all numbers with the help of the arbitrary-
precision arithmetic library FLINT [31]. This means all calculations, including
the implication test and the sample point heuristic, are always exact and thus
free of numerical errors. The most relevant part of SPASS-SPL with regards to

160
M. Bromberger et al.
Table 1. Overview of how many clause pairs advance in the pipeline (top to bottom)
lc
bakery, tad
All
All
1 244 819k
196 437k
1 441 256k
FO
Filtering
61.21%
85.03%
64.45%
f(C1) ≤f(C2)
761 905k 61.2061% 167 025k 85.0274%
928 931k 64.4540%
Matching
0.02%
39.83%
7.18%
C1σ ⊆C2
131k
0.0106%
66 531k 33.8694%
66 664k
4.6254%
C
Empty (pre./con.)
44.73%
100.00%
99.89%
⊭Λ1σ, ⊭Λ2
59k
0.0047%
66 531k 33.8694%
66 591k
4.6203%
Sample point
59.28%
0.12%
0.18%
⊨Λ1σβ2
35k
0.0028%
82k
0.0416%
117k
0.0081%
Implication
95.51%
100.00%
98.66%
Subsumes
33k
0.0027%
82k
0.0416%
115k
0.0080%
Table 2. An overview of the accuracy of non-perfect pipeline stages
Test
Speciﬁcity/Sensitivity
Pos./Neg. Predictive Value
Instances
lc
bakery, tad
All
lc
bakery, tad
All
FO Filtering
0.38797
0.14979
0.35552 0.00013
0.00049
0.00020
FO Matching
0.99996
0.60196
0.92841 0.78456
0.00123
0.00275
Empty Conclusion 0.70973
0.00000
0.00103 0.54474
0.00123
0.00173
Sample Point
0.93864
1.00000
0.99998 0.95510
1.00000
0.98653
this paper is that it performs tautology and subsumption deletion to eliminate
redundant clauses. As a preprocessing step, SPASS-SPL eliminates all tautolo-
gies from the set of input clauses. Similarly, the function resolvents(C, N) (see
Line 4 of Algorithm 1) ﬁlters out all newly derived clauses that are tautologies.
Note that we also use these tautology checks to eliminate all excess variables
and to store sample solutions for all remaining clauses. After each iteration of
the algorithm, we also check for subsumed clauses. We ﬁrst eliminate newly gen-
erated clauses by forward subsumption (see Line 6 of Algorithm 1), then use the
remaining clauses for backward subsumption (see Line 8 of Algorithm 1).
Benchmarks. Our benchmarking instances come out of three diﬀerent appli-
cations. (1.) A supervisor for an automobile lane change assistant, formulated
in the Horn fragment of BS(LRA) [9,10] (ﬁve instances, referred to as lc in
aggregate). (2.) The formalization of reachability for non-deterministic timed
automata, formulated in the non-Horn fragment of BS(LRA) [20] (one instance,
referred to as tad). (3.) Formalizations of variants of mutual exclusion proto-
cols, such as the bakery protocol [38], also formulated in the non-Horn fragment
of BS(LRA) [19] (one instance, referred to as bakery). The machine used for
benchmarking features an Intel Xeon W-1290P CPU (10 cores, 20 threads, up
to 5.2 GHz) and 64 GiB DDR4-2933 ECC main memory. Runtime was limited
to ten minutes, and memory usage was not limited.

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
161
Table 3. Evaluation of the sample point heuristic
Instances
lc
bakery, tad
All
Bottleneck
(C time ÷ FO time)
without sample point
127
2757
14867
with sample point
78
32
89
Avg. pipeline runtime in μs
without sample point
0.0315 89.9401
0.5189
with sample point
0.0311
1.4150
0.2197
Speedup
(C time with ÷ without)
1.63
˙137.88
124.16
Beneﬁt-to-cost
(C time taken ÷ saved)
6.74
˙181.72
163.72
Evaluation. In Table 1 we give an overview of how many pairs of clauses advance
how far in the pipeline (in thousands). Rows with grey background refer to a
stage of the pipeline and show which portion of pairs of clauses were kept, relative
to the previous stage. Rows with white background refer to (virtual) sets of
clauses, their absolute size, and their size relative to the number of attempted
tests, as well as the condition(s) established. The three groups of columns refer
to groups of benchmark instances. Results vary greatly between lc and the
aggregate of bakery and tad. In lc the relative number of subsumed clauses is
signiﬁcantly smaller (0.0027% compared to 0.0416%). FO Matching eliminates a
large number of pairs in lc, because the number of predicate symbols, and their
arity (lc1, . . . , lc4: 36 predicates, arities up to 5; lc5: 53 predicates, arities
up to 12) is greater than in bakery (11 predicates, all of arity 2) and tad (4
predicates, all of arity 2).
Binary Classiﬁers. To evaluate the performance of each stage of the proposed
test pipeline, we view each stage individually as a binary classiﬁer on pairs
of constrained clauses. The two classes we consider are “subsumes” (positive
outcome) and “does not subsume” (negative outcome). Each stage of the pipeline
computes a prediction on the actual result of the overall pipeline. We are thus
interested in minimizing two kinds of errors: (1) When one stage of the pipeline
predicts that the subsumption test will succeed (the prediciton is positive) but
it fails (the actual result is negative), called false positive (FP). (2) When one
stage of the pipeline predicts that the subsumption test will fail (the prediction
is negative) but it succeeds (the actual result is positive), called false negative
(FN). Dually, a correct prediction is called true positive (TP) and true negative
(TN). For each stage, at least one kind of error is excluded by design: First-
order ﬁltering and the sample point heuristic never produce false negatives. The
empty conclusion heuristic never produces false positives. The empty premise
test is perfect, i.e. it neither produces false positives nor false negatives, with the
caveat of not always being applicable. The last stage (implication test) decides
the overall result of the pipeline, and thus is also perfect. For evaluation of binary
classiﬁers, we use four diﬀerent measures (two symmetric pairs):
SPC = TN ÷ (TN + FP)
PPV = TP ÷ (TP + FP)
(2)

162
M. Bromberger et al.
The ﬁrst pair, speciﬁcity (SPC) and positive predictive value, see (2), is relevant
only in presence of false postives (the measures approach 1 as FP approaches 0).
SEN = TP ÷ (TP + FN)
NPV = TN ÷ (TN + FN)
(3)
The second pair, sensitivity (SEN) and negative predictive value (NPV), see (3),
is relevant only in presence of false negatives (the measures approach 1 as FN
approaches 0). Speciﬁcity (resp. sensitivity) might be considered the “success
rate” in our setup. They answer the question: “Given the actual result of the
pipeline is ‘subsumed’ (resp. ‘not subsumed’), in how many cases does this stage
predict correctly?” A speciﬁcity (resp. sensitivity) of 0.99 means that the clas-
siﬁer produces a false positive (resp. negative), i.e. a wrong prediction, in one
out of one hundred cases. Both measures are independent of the prevalence of
particular actual results, i.e. the measures are not biased by instances that fea-
ture many (or few) subsumed clauses. On the other hand, positive and negative
predictive value are biased by prevalence. They answer the following question:
“Given this stage of the pipeline predicts ‘subsumed’ (resp. ‘not subsumed’), how
likely is it that the actual result indeed is ‘subsumed’ (resp. ‘not subsumed’)?”
In Table 2 we present for all non-perfect stages of the pipeline speciﬁcity
(for those that produce false positives) and sensitivity (for those that produce
false negatives) as well as the (positive/negative) predictive value. Note that the
sample point heuristic has an exceptionally high speciﬁcity, still above 93% in
the benchmarks where it performed worst. For the benchmarks bakery and tad
it even performs perfectly. Combined, this gives a speciﬁcity of above 99.99%.
Considering FO Filtering, we expect limited performance, since the structure
of terms in BS is ﬂat compared to the rich structure of terms as trees in full
ﬁrst-order logic. This is evidenced by a comparatively low speciﬁcity of 35%.
However, this classiﬁer is very easy to compute, so pays for itself. FO Matching
is a much better classiﬁer, at an aggregate sensitivity of 93%. Even though this
classiﬁer is NP-complete, this is not problematic in practice.
Runtime. In Table 3 we focus on the runtime improvement achieved by the sample
point heuristic. In the ﬁrst two lines (Bottleneck), we highlight how much slower
testing implication of constraints (the C-phase) is compared to treating the ﬁrst-
order part (the FO-phase). This is equivalent to the time taken for the C-phase
per pair of clauses (that reach at least the ﬁrst C-phase) divided by the time taken
for the FO-phase per pair of clauses. We see that without the sample point heuris-
tic, we can expect the constraint implication test to take hundreds to thousands
of times longer than the FO-phase. Adding the sample point heuristic decreases
this ratio to below one hundred. In the fourth line (avg. pipeline runtime) we do
not give a ratio, but the average time it takes to compute the whole pipeline. We
achieve millions of subsumption checks per second. In the ﬁfth line (Speedup), we
take the time that all C-phases combined take per pair of clauses that reach at
least the ﬁrst C-phase, and take the ratio to the same time without applying the
sample point heuristic. In the sixth line (Beneﬁt-to-cost), we consider the time
taken to compute the sample point vs. the time it saves. The beneﬁt is about two
orders of magnitude greater than the cost.

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
163
5
Conclusion
Our next step will be the integration of the subsumption test in the backward
subsumption procedure of an SCL based reasoning procedure for BS(LRA) [11]
which is currently under development.
There are various ways to improve the sample point heuristic. One improve-
ment would be to store and check multiple sample points per clause. For instance,
whenever the sample point heuristic fails and the implication test for Λ2 →(Λ1σ)
also fails, store the solution to (1) as an additional sample point for Λ2. The new
sample point will ﬁlter out any future implication tests with Λ1σ or similar
constraints. However, testing too many sample points might lead to costs out-
weighing beneﬁts. A potential solution to this problem would be score-based
garbage collection, as done in SAT solvers [57]. Another way to store and check
multiple sample points per clause is to store a compact description of a set of
points that is easy to check against. For instance, we can store the center point
and edge length of the largest orthogonal hypercube contained in the solutions
of a constraint, which is equivalent to inﬁnitely many sample points. Computing
the largest orthogonal hypercube for an LRA constraint is not much harder than
ﬁnding a sample solution [14]. Checking whether a cube is contained in an LRA
constraint works almost the same as evaluating a sample point [14].
Although we developed our sample point technique for the BS(LRA) frag-
ment it is obvious that it will also work for the overall FOL(LRA) clause frag-
ment, because this extension does not aﬀect the LRA constraint part of clauses.
From an automated reasoning perspective, satisﬁability of the FOL(LRA) and
BS(LRA) fragments (clause sets) is undecidable in both cases. Actually, satisﬁ-
ability of a BS(LRA) clause set is already undecidable if the ﬁrst-order part is
restricted to a single monadic predicate [32]. The ﬁrst-order part of BS(LRA) is
decidable and therefore enables eﬀective guidance for an overall reasoning pro-
cedure [11]. Form an application perspective, the BS(LRA) fragment already
encompasses a number of used (sub)languages. For example, timed automata [3]
and a number of extensions thereof are contained in the BS(LRA) fragment [60].
We also believe that the sample point heuristic will speed up the constraint
implication test for FOL(LIA), ﬁrst-order clauses over linear integer arithmetic,
FOL(NRA), i.e., ﬁrst-order clauses over non-linear real arithmetic, and other
combinations of FOL with arithmetic theories. However, the non-linear case will
require a more sophisticated setup due to the nature of test points in this case,
e.g., a solution may contain root expressions.
Acknowledgments. This work was partly funded by DFG grant 389792660 as part
of TRR 248, see https://perspicuous-computing.science. We thank the anonymous
reviewers for their thorough reading and detailed constructive comments. Martin
Desharnais suggested some textual improvements.
References
1. Alagi, G., Weidenbach, C.: NRCL - a model building approach to the Bernays-
Sch¨onﬁnkel fragment. In: Lutz, C., Ranise, S. (eds.) FroCoS 2015. LNCS (LNAI),

164
M. Bromberger et al.
vol. 9322, pp. 69–84. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
24246-0 5
2. Althaus, E., Kruglov, E., Weidenbach, C.: Superposition modulo linear arithmetic
SUP(LA). In: Ghilardi, S., Sebastiani, R. (eds.) FroCoS 2009. LNCS (LNAI), vol.
5749, pp. 84–99. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-
04222-5 5
3. Alur, R., Dill, D.L.: A theory of timed automata. Theor. Comput. Sci. 126(2),
183–235 (1994). https://doi.org/10.1016/0304-3975(94)90010-8
4. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Log. Comput. 4(3), 217–247 (1994). https://doi.org/10.
1093/logcom/4.3.217
5. Bachmair, L., Ganzinger, H.: Resolution theorem proving. In: Robinson, J.A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning (in 2 volumes), pp. 19–
99. Elsevier and MIT Press, Cambridge (2001). https://doi.org/10.1016/b978-
044450813-3/50004-7
6. Bachmair, L., Ganzinger, H., Waldmann, U.: Refutational theorem proving for
hierarchic ﬁrst-order theories. Appl. Algebra Eng. Commun. Comput. 5, 193–212
(1994). https://doi.org/10.1007/BF01190829
7. Baumgartner, P., Waldmann, U.: Hierarchic superposition revisited. In: Lutz, C.,
Sattler, U., Tinelli, C., Turhan, A.-Y., Wolter, F. (eds.) Description Logic, Theory
Combination, and All That. LNCS, vol. 11560, pp. 15–56. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-22102-7 2
8. Biere, A., Heule, M., van Maaren, H., Walsh, T. (eds.): Handbook of Satisﬁability,
Frontiers in Artiﬁcial Intelligence and Applications, vol. 185. IOS Press, Amster-
dam (2009)
9. Bromberger, M., et al.: A sorted datalog hammer for supervisor veriﬁcation con-
ditions modulo simple linear arithmetic. CoRR abs/2201.09769 (2022). https://
arxiv.org/abs/2201.09769
10. Bromberger, M., Dragoste, I., Faqeh, R., Fetzer, C., Kr¨otzsch, M., Weidenbach,
C.: A datalog hammer for supervisor veriﬁcation conditions modulo simple linear
arithmetic. In: Konev, B., Reger, G. (eds.) FroCoS 2021. LNCS (LNAI), vol. 12941,
pp. 3–24. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-86205-3 1
11. Bromberger, M., Fiori, A., Weidenbach, C.: Deciding the Bernays-Schoenﬁnkel
Fragment over bounded diﬀerence constraints by simple clause learning over the-
ories. In: Henglein, F., Shoham, S., Vizel, Y. (eds.) VMCAI 2021. LNCS, vol.
12597, pp. 511–533. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-
67067-2 23
12. Bromberger, M., Fleury, M., Schwarz, S., Weidenbach, C.: SPASS-SATT. In:
Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 111–122. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-29436-6 7
13. Bromberger, M., Leutgeb, L., Weidenbach, C.: An Eﬃcient subsumption test
pipeline for BS(LRA) clauses (2022). https://doi.org/10.5281/zenodo.6544456.
Supplementary Material
14. Bromberger, M., Weidenbach, C.: Fast cube tests for LIA constraint solving. In:
Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 116–132.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 9
15. Dershowitz, N.: Orderings for term-rewriting systems. Theor. Comput. Sci. 17,
279–301 (1982). https://doi.org/10.1016/0304-3975(82)90026-3
16. Dutertre, B., de Moura, L.: A fast linear-arithmetic solver for DPLL(T). In: Ball,
T., Jones, R.B. (eds.) CAV 2006. LNCS, vol. 4144, pp. 81–94. Springer, Heidelberg
(2006). https://doi.org/10.1007/11817963 11

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
165
17. Eggers, A., Kruglov, E., Kupferschmid, S., Scheibler, K., Teige, T., Weiden-
bach, C.: Superposition Modulo Non-linear Arithmetic. In: Tinelli, C., Sofronie-
Stokkermans, V. (eds.) FroCoS 2011. LNCS (LNAI), vol. 6989, pp. 119–134.
Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-24364-6 9
18. Faqeh, R., Fetzer, C., Hermanns, H., Hoﬀmann, J., Klauck, M., K¨ohl, M.A., Stein-
metz, M., Weidenbach, C.: towards dynamic dependable systems through evidence-
based continuous certiﬁcation. In: Margaria, T., Steﬀen, B. (eds.) ISoLA 2020.
LNCS, vol. 12477, pp. 416–439. Springer, Cham (2020). https://doi.org/10.1007/
978-3-030-61470-6 25
19. Fietzke, A.: Labelled superposition. Ph.D. thesis, Universit¨at des Saarlandes
(2014). https://doi.org/10.22028/D291-26569
20. Fietzke, A., Weidenbach, C.: Superposition as a decision procedure for timed
automata. Math. Comput. Sci. 6(4), 409–425 (2012). https://doi.org/10.1007/
s11786-012-0134-5
21. Fiori, A., Weidenbach, C.: SCL clause learning from simple models. In: Fontaine, P.
(ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 233–249. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-29436-6 14
22. Fiori, A., Weidenbach, C.: SCL with theory constraints. CoRR abs/2003.04627
(2020). https://arxiv.org/abs/2003.04627
23. Fr¨anzle, M., Herde, C., Teige, T., Ratschan, S., Schubert, T.: Eﬃcient solving of
large non-linear arithmetic constraint systems with complex boolean structure. J.
Satisf. Boolean Model. Comput. 1(3–4), 209–236 (2007). https://doi.org/10.3233/
sat190012
24. Ganzinger, H., Nieuwenhuis, R., Nivela, P.: Fast term indexing with coded context
trees. J. Autom. Reason. 32(2), 103–120 (2004). https://doi.org/10.1023/B:JARS.
0000029963.64213.ac
25. Gleiss, B., Kov´acs, L., Rath, J.: Subsumption demodulation in ﬁrst-order theo-
rem proving. In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020. LNCS
(LNAI), vol. 12166, pp. 297–315. Springer, Cham (2020). https://doi.org/10.1007/
978-3-030-51074-9 17
26. Gottlob, G.: Subsumption and implication. Inf. Process. Lett. 24(2), 109–111
(1987). https://doi.org/10.1016/0020-0190(87)90103-7
27. Gottlob, G., Leitsch, A.: On the eﬃciency of subsumption algorithms. J. ACM
32(2), 280–295 (1985). https://doi.org/10.1145/3149.214118
28. Graf, P.: Extended path-indexing. In: Bundy, A. (ed.) CADE 1994. LNCS, vol. 814,
pp. 514–528. Springer, Heidelberg (1994). https://doi.org/10.1007/3-540-58156-
1 37
29. Graf, P.: Substitution tree indexing. In: Hsiang, J. (ed.) RTA 1995. LNCS, vol. 914,
pp. 117–131. Springer, Heidelberg (1995). https://doi.org/10.1007/3-540-59200-
8 52
30. Graf, P. (ed.): Term Indexing. LNCS, vol. 1053. Springer, Heidelberg (1995).
https://doi.org/10.1007/3-540-61040-5
31. Hart, W.B.: Fast library for number theory: an introduction. In: Fukuda, K.,
Hoeven, J., Joswig, M., Takayama, N. (eds.) ICMS 2010. LNCS, vol. 6327, pp.
88–91. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-15582-6 18
32. Horbach, M., Voigt, M., Weidenbach, C.: The universal fragment of pres-
burger arithmetic with unary uninterpreted predicates is undecidable. CoRR
abs/1703.01212 (2017). http://arxiv.org/abs/1703.01212
33. Purdom, P.W., Brown, C.A.: Fast many-to-one matching algorithms. In: Jouan-
naud, J.-P. (ed.) RTA 1985. LNCS, vol. 202, pp. 407–416. Springer, Heidelberg
(1985). https://doi.org/10.1007/3-540-15976-2 21

166
M. Bromberger et al.
34. Bayardo, R.J., Schrag, R.: Using CSP look-back techniques to solve exceptionally
hard SAT instances. In: Freuder, E.C. (ed.) CP 1996. LNCS, vol. 1118, pp. 46–60.
Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61551-2 65
35. Korovin, K., Voronkov, A.: Integrating Linear Arithmetic into Superposition Cal-
culus. In: Duparc, J., Henzinger, T.A. (eds.) CSL 2007. LNCS, vol. 4646, pp.
223–237. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-74915-
8 19
36. Kruglov, E.: Superposition modulo theory. Ph.D. thesis, Universit¨at des Saarlandes
(2013). https://doi.org/10.22028/D291-26547
37. Kruglov, E., Weidenbach, C.: Superposition decides the ﬁrst-order logic fragment
over ground theories. Math. Comput. Sci. 6(4), 427–456 (2012). https://doi.org/
10.1007/s11786-012-0135-4
38. Lamport, L.: A new solution of dijkstra’s concurrent programming problem. Com-
mun. ACM 17(8), 453–455 (1974). https://doi.org/10.1145/361082.361093
39. McCune, W.: Otter 2.0. In: Stickel, M.E. (ed.) CADE 1990. LNCS, vol. 449, pp.
663–664. Springer, Heidelberg (1990). https://doi.org/10.1007/3-540-52885-7 131
40. McCune, W.: Experiments with discrimination-tree indexing and path indexing for
term retrieval. J. Autom. Reason. 9(2), 147–167 (1992). https://doi.org/10.1007/
BF00245458
41. Moskewicz, M.W., Madigan, C.F., Zhao, Y., Zhang, L., Malik, S.: Chaﬀ: Engi-
neering an eﬃcient SAT solver. In: Proceedings of the 38th Design Automation
Conference, DAC 2001, Las Vegas, NV, USA, 18–22 June 2001, pp. 530–535. ACM
(2001). https://doi.org/10.1145/378239.379017
42. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
43. Nieuwenhuis, R., Hillenbrand, T., Riazanov, A., Voronkov, A.: On the evaluation
of indexing techniques for theorem proving. In: Gor´e, R., Leitsch, A., Nipkow, T.
(eds.) IJCAR 2001. LNCS, vol. 2083, pp. 257–271. Springer, Heidelberg (2001).
https://doi.org/10.1007/3-540-45744-5 19
44. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Robin-
son, J.A., Voronkov, A. (eds.) Handbook of Automated Reasoning (in 2 volumes),
pp. 371–443. Elsevier and MIT Press, Cambridge (2001). https://doi.org/10.1016/
b978-044450813-3/50009-6
45. Ohlbach, H.J.: Abstraction tree indexing for terms. In: 9th European Conference
on Artiﬁcial Intelligence, ECAI 1990, Stockholm, Sweden, pp. 479–484 (1990)
46. Overbeek, R.A., Lusk, E.L.: Data structures and control architecture for imple-
mentation of theorem-proving programs. In: Bibel, W., Kowalski, R. (eds.) CADE
1980. LNCS, vol. 87, pp. 232–249. Springer, Heidelberg (1980). https://doi.org/10.
1007/3-540-10009-1 19
47. Ramakrishnan, I.V., Sekar, R.C., Voronkov, A.: Term indexing. In: Robinson, J.A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning (in 2 volumes), pp. 1853–
1964. Elsevier and MIT Press, Cambridge (2001). https://doi.org/10.1016/b978-
044450813-3/50028-x
48. Riazanov, A., Voronkov, A.: Partially adaptive code trees. In: Ojeda-Aciego, M., de
Guzm´an, I.P., Brewka, G., Moniz Pereira, L. (eds.) JELIA 2000. LNCS (LNAI),
vol. 1919, pp. 209–223. Springer, Heidelberg (2000). https://doi.org/10.1007/3-
540-40006-0 15
49. Riazanov, A., Voronkov, A.: Eﬃcient instance retrieval with standard and rela-
tional path indexing. Inf. Comput. 199(1–2), 228–252 (2005). https://doi.org/10.
1016/j.ic.2004.10.012

An Eﬃcient Subsumption Test Pipeline for BS(LRA) Clauses
167
50. Robinson, J.A.: A machine-oriented logic based on the resolution principle. J.
ACM, 12(1), 23–41 (1965). https://doi.org/10.1145/321250.321253, http://doi.
acm.org/10.1145/321250.321253
51. Schrijver, A.: Theory of Linear and Integer Programming. Wiley-Interscience series
in discrete mathematics and optimization, Wiley, Hoboken (1999)
52. Schulz, S.: Simple and eﬃcient clause subsumption with feature vector indexing. In:
Proceedings of the IJCAR-2004 Workshop on Empirically Successful First-Order
Theorem Proving. Elsevier Science (2004)
53. Schulz, S.: Fingerprint Indexing for Paramodulation and Rewriting. In: Gramlich,
B., Miller, D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 477–
483. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 37
54. Schulz, S.: Simple and eﬃcient clause subsumption with feature vector indexing.
In: Bonacina, M.P., Stickel, M.E. (eds.) Automated Reasoning and Mathematics.
LNCS (LNAI), vol. 7788, pp. 45–67. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-36675-8 3
55. Silva, J.P.M., Sakallah, K.A.: GRASP - a new search algorithm for satisﬁability.
In: Rutenbar, R.A., Otten, R.H.J.M. (eds.) Proceedings of the 1996 IEEE/ACM
International Conference on Computer-Aided Design, ICCAD 1996, San Jose, CA,
USA, 10–14 November 1996, pp. 220–227. IEEE Computer Society/ACM (1996).
https://doi.org/10.1109/ICCAD.1996.569607
56. Socher, R.: A subsumption algorithm based on characteristic matrices. In: Lusk, E.,
Overbeek, R. (eds.) CADE 1988. LNCS, vol. 310, pp. 573–581. Springer, Heidelberg
(1988). https://doi.org/10.1007/BFb0012858
57. Soos, M., Kulkarni, R., Meel, K.S.: CrystalBall: gazing in the black box of SAT
solving. In: Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 371–387.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-24258-9 26
58. Stillman, R.B.: The concept of weak substitution in theorem-proving. J. ACM
20(4), 648–667 (1973). https://doi.org/10.1145/321784.321792
59. Tammet, T.: Towards eﬃcient subsumption. In: Kirchner, C., Kirchner, H. (eds.)
CADE 1998. LNCS, vol. 1421, pp. 427–441. Springer, Heidelberg (1998). https://
doi.org/10.1007/BFb0054276
60. Voigt, M.: Decidable ∃∗∀∗ﬁrst-order fragments of linear rational arithmetic with
uninterpreted predicates. J. Autom. Reason. 65(3), 357–423 (2020). https://doi.
org/10.1007/s10817-020-09567-8
61. Voronkov, A.: The anatomy of vampire implementing bottom-up procedures with
code trees. J. Autom. Reason. 15(2), 237–265 (1995). https://doi.org/10.1007/
BF00881918
62. Voronkov, A.: Algorithms, datastructures, and other issues in eﬃcient automated
deduction. In: Gor´e, R., Leitsch, A., Nipkow, T. (eds.) IJCAR 2001. LNCS,
vol. 2083, pp. 13–28. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-
45744-5 3
63. Weidenbach, C.: Automated reasoning building blocks. In: Meyer, R., Platzer,
A., Wehrheim, H. (eds.) Correct System Design. LNCS, vol. 9360, pp. 172–188.
Springer, Cham (2015). https://doi.org/10.1007/978-3-319-23506-6 12

168
M. Bromberger et al.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Ground Joinability and Connectedness
in the Superposition Calculus
Andr´e Duarte(B)
and Konstantin Korovin(B)
The University of Manchester, Manchester, UK
{andre.duarte,konstantin.korovin}@manchester.ac.uk
Abstract. Problems in many theories axiomatised by unit equalities
(UEQ), such as groups, loops, lattices, and other algebraic structures,
are notoriously diﬃcult for automated theorem provers to solve. Con-
sequently, there has been considerable eﬀort over decades in developing
techniques to handle these theories, notably in the context of Knuth-
Bendix completion and derivatives. The superposition calculus is a gen-
eralisation of completion to full ﬁrst-order logic; however it does not carry
over all the reﬁnements that were developed for it, and is therefore not
a strict generalisation. This means that (i) as of today, even state of the
art provers for ﬁrst-order logic based on the superposition calculus, while
more general, are outperformed in UEQ by provers based on completion,
and (ii) the sophisticated techniques developed for completion are not
available in any problem which is not in UEQ. In particular, this includes
key simpliﬁcations such as ground joinability, which have been known for
more than 30 years. In fact, all previous completeness proofs for ground
joinability rely on proof orderings and proof reductions, which are not
easily extensible to general clauses together with redundancy elimina-
tion. In this paper we address this limitation and extend superposition
with ground joinability, and show that under an adapted notion of redun-
dancy, simpliﬁcations based on ground joinability preserve completeness.
Another recently explored simpliﬁcation in completion is connectedness.
We extend this notion to “ground connectedness” and show superposi-
tion is complete with both connectedness and ground connectedness. We
implemented ground joinability and connectedness in a theorem prover,
iProver, the former using a novel algorithm which we also present in this
paper, and evaluated over the TPTP library with encouraging results.
Keywords: Superposition · Ground joinability · Connectedness ·
Closure redundancy · First-order theorem proving
1
Introduction
Automated theorem provers based on equational completion [4], such as Wald-
meister, MædMax or Twee [13,21,25], routinely outperform superposition-based
provers on unit equality problems (UEQ) in competitions such as CASC [22],
despite the fact that the superposition calculus was developed as a generalisation
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 169–187, 2022.
https://doi.org/10.1007/978-3-031-10769-6_11

170
A. Duarte and K. Korovin
of completion to full clausal ﬁrst-order logic with equality [19]. One of the main
ingredients for their good performance is the use of ground joinability criteria for
the deletion of redundant equations [1], among other techniques. However, exist-
ing proofs of refutational completeness of deduction calculi wrt. these criteria
are restricted to unit equalities and rely on proof orderings and proof reduc-
tions [1,2,4], which are not easily extensible to general clauses together with
redundancy elimination.
Since completion provers perform very poorly (or not at all) on non-UEQ
problems (relying at best on incomplete transformations to unit equality [8]), this
motivates an attempt to transfer those techniques to the superposition calculus
and prove their completeness, so as to combine the generality of the superposition
calculus with the powerful simpliﬁcation rules of completion. To our knowledge,
no prover for ﬁrst-order logic incorporates ground joinability redundancy criteria,
except for particular theories such as associativity-commutativity (AC) [20].
For instance, if f(x, y)
≈
f(y, x) is an axiom, then the equation
f(x, f(y, z)) ≈f(x, f(z, y)) is redundant, but this cannot be justiﬁed by any
simpliﬁcaton rule in the superposition calculus. On the other hand, a comple-
tion prover which implements ground joinability can easily delete the latter
equation wrt. the former. We show that ground joinability can be enabled in the
superposition calculus without compromising completeness.
As another example, the simpliﬁcation rule in completion can use f(x) ≈s
(when f(x) ≻s) to rewrite f(a) ≈t regardless of how s and t compare, while
the corresponding demodulation rule in superposition can only rewrite if s ≺t.
Our “encompassment demodulation” rule matches the former, while also being
complete in the superposition calculus.
In [11] we introduced a novel theoretical framework for proving complete-
ness of the superposition calculus, based on an extension of Bachmair-Ganzinger
model construction [5], together with a new notion of redundancy called “closure
redundancy”. We used it to prove that certain AC joinability criteria, long used
in the context of completion [1], could also be incorporated in the superposition
calculus for full ﬁrst-order logic while preserving completeness.
In this paper, we extend this framework to show the completeness of the
superposition calculus extended with: (i) a general ground joinability simpliﬁ-
cation rule, (ii) an improved encompassment demodulation simpliﬁcation rule,
(iii) a connectedness simpliﬁcation rule extending [3,21], and (iv) a new ground
connectedness simpliﬁcation rule. The proof of completeness that enables these
extensions is based on a new encompassment closure ordering. In practice, these
extensions help superposition to be competitive with completion in UEQ prob-
lems, and improves the performance on non-UEQ problems, which currently do
not beneﬁt from these techniques at all.
We also present a novel incremental algorithm to check ground joinability,
which is very eﬃcient in practice; this is important since ground joinability can
be an expensive criterion to test. Finally, we discuss some of the experimental
results we obtained after implementing these techniques in iProver [10,16].
The paper is structured as follows. In Sect. 2 we deﬁne some basic notions to
be used throughout the paper. In Sect. 3 we deﬁne the closure ordering we use to

Ground Joinability and Connectedness in the Superposition Calculus
171
prove redundancies. In Sect. 4 we present redundancy criteria for demodulation,
ground joinability, connectedness, and ground connectedness. We prove their
completeness in the superposition calculus, and discuss a concrete algorithm for
checking ground joinability, and how it may improve on the algorithms used in
e.g. Waldmeister [13] or Twee [21]. In Sect. 5 we discuss experimental results.
2
Preliminaries
We consider a signature consisting of a ﬁnite set of function symbols and the
equality predicate as the only predicate symbol. We ﬁx a countably inﬁnite set
of variables. First-order terms are deﬁned in the usual manner. Terms without
variables are called ground terms. A literal is an unordered pair of terms with
either positive or negative polarity, written s ≈t and s ̸≈t respectively (we
write s ˙≈t to mean either of the former two). A clause is a multiset of literals.
Collectively terms, literals, and clauses will be called expressions.
A substitution is a mapping from variables to terms which is the identity
for all but ﬁnitely many variables. An injective substitution onto variables is
called a renaming. If e is an expression, we denote application of a substitution
σ by eσ, replacing all variables with their image in σ. Let GSubs(e) = {σ |
eσ is ground} be the set of ground substitutions for e. Overloading this notation
for sets we write GSubs(E) = {σ | ∀e ∈E. eσ is ground}. Finally, we write e.g.
GSubs(e1, e2) instead of GSubs({e1, e2}). The identity substitution is denoted
by ϵ.
A substitution θ is more general than σ if θρ = σ for some substitution ρ
which is not a renaming. If s and t can be uniﬁed, that is, if there exists σ such
that sσ = tσ, then there also exists the most general uniﬁer, written mgu(s, t).
A term s is said to be more general than t if there exists a substitution θ that
makes sθ = t but there is no substitution σ such that tσ = s. Two terms s and t
are said to be equal modulo renaming if there exist injective θ, σ such that sθ = t
and tσ = s. The relations “less general than”, “equal modulo renaming”, and
their union are represented respectively by the symbols ⊐, ≡, and ⊒.
A more reﬁned notion of instance is that of closure [6]. Closures are pairs
e · σ that are said to represent the expression eσ while retaining information
about the original term and its instantiation. Closures where eσ is ground are
said to be ground closures. Let GClos(e) = {e · σ | eσ is ground} be the set of
ground closures of e. Overloading the notation for sets, if N is a set of clauses
then GClos(N) = 
C∈N GClos(C).
We write s[t] if t is a subterm of s. If also s ̸= t, then it is a strict subterm.
We denote these relations by s ⊵t and s ▷t respectively. We write s[t 
→t′] to
denote the term obtained from s by replacing all occurrences of t by t′.
A (strict) partial order is a binary relation which is transitive (a ≻b ≻c ⇒
a ≻c), irreﬂexive (a ⊁a), and asymmetric (a ≻b ⇒b ⊁a). A (non-strict)
partial preorder (or quasiorder) is any transitive, reﬂexive relation. A (pre)order
is total over X if ∀x, y ∈X. x ⪰y ∨y ⪰x. Whenever a non-strict (pre)order
⪰is given, the induced equivalence relation ∼is ⪰∩⪰, and the induced strict
pre(order) ≻is ⪰\∼. The transitive closure of a relation ≻, the smallest transitive

172
A. Duarte and K. Korovin
relation that contains ≻, is denoted by ≻+. A transitive reduction of a relation
≻, the smallest relation whose transitive closure is ≻, is denoted by ≻−.
For an ordering ≻over a set X, its multiset extension ≻≻over multisets of X
is given by: A ≻≻B iﬀA ̸= B and ∀x ∈B. B(x) > A(x) ∃y ∈A. y ≻x∧A(y) >
B(y), where A(x) is the number of occurrences of element x in multiset A (we
also use ≻≻≻for the the multiset extension of ≻≻). It is well known that the mutl-
tiset extension of a well-founded/total order is also a well-founded/total order,
respectively [9]. The (n-fold) lexicographic extension of ≻over X is denoted
≻lex over ordered n-tuples of X, and is given by ⟨x1, . . . , xn⟩≻lex ⟨y1, . . . , yn⟩
iﬀ∃i. x1 = y1 ∧· · · ∧xi−1 = yi−1 ∧xi ≻yi. The lexicographic extension of a
well-founded/total order is also a well-founded/total order, respectively.
A binary relation →over the set of terms is a rewrite relation if (i) l →
r ⇒lσ →rσ and (ii) l →r ⇒s[l] →s[l 
→r]. The reﬂexive-transitive closure
of a relation is the smallest reﬂexive-transitive relation which contains it. It is
denoted by
∗→. Two terms are joinable (s ↓t) if s
∗→u
∗←t.
If a rewrite relation is also a strict ordering, then it is a rewrite ordering. A
reduction ordering is a rewrite ordering which is well-founded. In this paper we
consider reduction orderings which are total on ground terms, such orderings are
also simpliﬁcation orderings i.e., satisfy s ▷t ⇒s ≻t.
3
Ordering
In [11] we presented a novel proof of completeness of the superposition calculus
based on the notion of closure redundancy, which enables the completeness of
stronger redundancy criteria to be shown, including AC normalisation, AC join-
ability, and encompassment demodulation. In this paper we use a slightly diﬀerent
closure ordering (≻cc), in order to extract better completeness conditions for the
redundancy criteria that we present in this paper (the deﬁnition of closure redun-
dant clause and closure redundant inference is parametrised by this ≻cc).
Let ≻t be a simpliﬁcation ordering which is total on ground terms. We extend
this ﬁrst to an ordering on ground term closures, then to an ordering on ground
clause closures. Let
s · σ ≻tc′ t · ρ
iﬀ
either sσ ≻t tρ
or else sσ = tρ and s ⊐t,
(1)
where sσ and tρ are ground, and let ≻tc be an (arbitrary) total well-founded
extension of ≻tc′. We extend this to an ordering on clause closures. First let
Mlc((s ≈t) · θ) = {sθ · ϵ, tθ · ϵ},
(2)
Mlc((s ̸≈t) · θ) = {sθ · ϵ, tθ · ϵ, sθ · ϵ, tθ · ϵ},
(3)
and let Mcc be deﬁned as follows, depending on whether the clause is unit or
non-unit:
Mcc(∅· θ) = ∅,
(4)
Mcc((s ≈t) · θ) = {{s · θ}, {t · θ}},
(5)

Ground Joinability and Connectedness in the Superposition Calculus
173
Mcc((s ̸≈t) · θ) = {{s · θ, t · θ, sθ · ϵ, tθ · ϵ}},
(6)
Mcc((s ˙≈t ∨· · · ) · θ) = {Mlc(L · θ) | L ∈(s ˙≈t ∨· · · )},
(7)
then ≻cc is deﬁned by
C · σ ≻cc D · ρ
iﬀ
Mcc(C · σ) ≻≻≻tc Mcc(D · ρ).
(8)
The main purpose of this deﬁnition is twofold: (i) that when sθ ≻t tθ and u
occurs in a clause D, then sθ ◁u or s ⊏sθ = u implies (s ≈t)·θρ ≺cc D ·ρ, and
(ii) that when C is a positive unit clause, D is not, s is the maximal subterm
in Cθ and t is the maximal subterm in Dσ, then s ⪰t t implies C · θ ≺cc D · σ.
These two properties enable unconditional rewrites via oriented unit equations
on positive unit clauses to succeed whenever they would also succeed in unfailing
completion [4], and rewrites on negative unit and non-unit clauses to always
succeed. This will enable us to prove the correctness of the simpliﬁcation rules
presented in the following section.
4
Redundancies
In this section we present several redundancy criteria for the superposition cal-
culus and prove their completeness. Recall the deﬁnitions in [11]: a clause C
is redundant in a set S if all its ground closures C · θ follow from closures in
GClos(S) which are smaller wrt. ≻cc; an inference C1, . . . , Cn |−D is redundant
in a set S if, for all θ ∈GSubs(C1, . . . , Cn, D) such that C1θ, . . . , Cnθ |−Dθ is
a valid inference, the closure D · θ follows from closures in GClos(S) such that
each is smaller than some C1 ·θ, . . . , Cn ·θ. These deﬁnitions (in terms of ground
closures rather than in terms of ground clauses, as in [19]) arise because they
enable us to justify stronger redundancy criteria for application in superposition
theorem provers, including the AC criteria developed in [11] and the criteria in
this section.
Theorem 1. The superposition calculus [19] is refutationally complete wrt. clo-
sure redundancy, that is, if a set of clauses is saturated up to closure redundancy
(meaning any inference with non-redundant premises in the set is redundant)
and does not contain the empty clause, then it is satisﬁable.
Proof. The proof of completeness of the superposition calculus wrt. this closure
ordering carries over from [11] with some modiﬁcations, which are presented in
a full version of this paper [12].
4.1
Encompassment Demodulation
We introduce the following deﬁnition, to be re-used throughout the paper.
Deﬁnition 1. A rewrite via l ≈r in clause C[lθ] is admissible if one of the
following conditions holds: (i) C is not a positive unit, or (let C = s[lθ] ≈t for
some θ) (ii) lθ ̸= s, or (iii) lθ ⊐l, or (iv) s ≺t t, or (v) rθ ≺t t.1
1 We note that (iv) is superﬂuous, but we include it since in practice it is easier to
check, as it is local to the clause being rewritten and therefore needs to be checked
only once, while (v) needs to be checked with each demodulation attempt.

174
A. Duarte and K. Korovin
We then have
Encompassment
Demodulation
l ≈r

C[lθ]
C[lθ 
→rθ] ,
where lθ ≻t rθ, and
rewrite via l ≈r in C is admissible.
(9)
In other words, given an equation l ≈r, if an instance lθ is a subterm in
C, then the rewrite is admissible (meaning, for example, that an unconditional
rewrite is allowed when lθ ≻t rθ) if C is not a positive unit, or if lθ occurs at
a strict subterm position, or if lθ is less general than l, or if lθ occurs outside
a maximal side, or if rθ is smaller than the other side. This restriction is much
weaker than the one given for the usual demodulation rule in superposition [17],
and equivalent to the one in equational completion when we restrict ourselves
to unit equalities [4].
Example 1. If f(x) ≻t s, we can use f(x) ≈s to rewrite f(x) ≈t when s ≺t t,
and f(a) ≈t, f(x) ̸≈t, or f(x) ≈t ∨C regardless of how s and t compare.
4.2
General Ground Joinability
In [11] we developed redundancy criteria for the theory of AC functions in the
superposition calculus. In this section we extend these techniques to develop
redundancy criteria for ground joinability in arbitrary equational theories.
Deﬁnition 2. Two terms are strongly joinable (s
t), in a clause C wrt. a set
of equations S, if either s = t, or s →s[l1σ1 
→r1σ1]
∗→t via rules li ≈ri ∈S,
where the rewrite via l1 ≈r1 is admissible in C, or s →s[l1σ1 
→r1σ1] ↓
t[l2σ2 
→r2σ2] ←t via rules li ≈ri ∈S, where the rewrites via l1 ≈r1 and
l2 ≈r2 are admissible in C. To make the ordering explicit, we may write s
≻t.
Two terms are strongly ground joinable (s
t), in a clause C wrt. a set of
equations S, if for all θ ∈GSubs(s, t) we have sθ
tθ in C wrt. S.
We then have:
Ground joinability
(((((
s ≈t ∨C
S ,
where s
t in s ≈t ∨C wrt. S,
(10a)
Ground joinability

s ̸≈t ∨C
S
C
,
where s
t in s ̸≈t ∨C wrt. S.
(10b)
Theorem 2. Ground joinability is a sound and admissible redundancy criterion
of the superposition calculus wrt. closure redundancy.
Proof. We will show the positive case ﬁrst. If s
t, then for any instance (s ≈
t ∨C) · θ we either have sθ = tθ, and therefore ∅|= (s ≈t) · θ, or we have wlog.
sθ ≻t tθ, with sθ ↓tθ. Then sθ and tθ can be rewritten to the same normal form
u by liσi →riσi where li ≈ri ∈S. Since u ≺t sθ and u ⪰t tθ, then (s ≈t∨C)·θ

Ground Joinability and Connectedness in the Superposition Calculus
175
follows from smaller (u ≈u ∨C) · θ2 (a tautology, i.e. follows from ∅) and from
the instances of clauses in S used to rewrite sθ →u ←tθ. It only remains to
show that these latter instances are also smaller than (s ≈t ∨C) · θ. Since we
have assumed sθ ≻t tθ, then at least one rewrite step must be done on sθ. Let
l1σ1 →r1σ1 be the instance of the rule used for that step, with (l1 ≈r1) · σ1 the
closure that generates it. By Deﬁnition 1 and 2, one of the following holds:
– C ̸= ∅, therefore (l1 ≈r1) · σ1 ≺cc (s ≈t ∨C) · θ, or
– l1σ1◁sθ, therefore l1σ1 ≺t sθ ⇒l1·σ1 ≺tc s·θ ⇒(l1 ≈r1)·σ1 ≺cc (s ≈t)·θ,
or
– l1σ1 = sθ and s ⊐l1, therefore l1 · σ1 ≺tc s · θ ⇒(l1 ≈r1) · σ1 ≺cc (s ≈t) · θ,
or
– l1σ1 = sθ and s ≡l1 and r1σ1 ≺t tθ, therefore r1 · σ1 ≺tc t · θ ⇒(l1 ≈
r1) · σ1 ≺cc (s ≈t) · θ.
As for the remaining steps, they are done on the smaller side tθ or on the other
side after this ﬁrst rewrite, which is smaller than sθ. Therefore all subsequent
steps done by any ljσj →rjσj will have rj · σj ≺tc lj · σj ≺tc s · θ ⇒(lj ≈
rj) · σj ≺cc (s ≈t ∨C) · θ. As such, since this holds for all ground closures
(s ≈t ∨C) · θ, then s ≈t ∨C is redundant wrt. S.
For the negative case, the proof is similar. We will conclude that (s ̸≈t∨C)·θ
follows from smaller (li ≈ri) · σi ∈GClos(S) and smaller (u ̸≈u ∨C) · θ. The
latter, of course, follows from smaller C · θ, therefore s ̸≈t ∨C is redundant wrt.
S ∪{C}.
⊓⊔
Example 2. If S = {f(x, y) ≈f(y, x)}, then f(x, f(y, z)) ≈f(x, f(z, y)) is
redundant wrt. S. Note that f(x, y) ≈f(y, x) is not orientable by any sim-
pliﬁcation ordering, therefore this cannot be justiﬁed by demodulation alone.
Testing for Ground Joinability. The general criterion presented above begs
the question of how to test, in practice, whether s
t in a clause s ˙≈t∨C. Several
such algorithms have been proposed [1,18,21]. All of these are based on the
observation that if we consider all total preorders ⪰v on Vars(s, t) and for all of
them show strong joinability with a modiﬁed ordering—which we denote ≻t[v]—
then we have shown strong ground joinability in the order ≻t [18].
Deﬁnition 3. A simpliﬁcation order on terms ≻t extended with a preorder on
variables ⪰v, denoted ⪰t[v], is a simpliﬁcation preorder (i.e. satisﬁes all the
relevant properties in Sect. 2) such that ⪰t[v] ⊇≻t ∪⪰v.
Example 3. If x ≻v y, then g(x) ≻t[v] g(y), g(x) ≻t[v] y, f(x, y) ≻t[v] f(y, x),
etc.
The simplest algorithm based on this approach would be to enumerate all
possible total preorders ⪰v over Vars(s, t), and exhaustively reduce both sides
2 Wlog. uθ = u, renaming variables in u if necessary.

176
A. Duarte and K. Korovin
via equations in S orientable by ≻t[v], checking if the terms can be reduced to the
same normal form for all total preorders. This is very ineﬃcient since there are
O(n!en) such total preorders [7], where n is the cardinality of Vars(s, t). Another
approach is to consider only a smaller number of partial preorders, based on the
obvious fact that s
≻t[v] t ⇒∀⪰′
v ⊇⪰v. s
≻t[v′] t, so that joinability under
a smaller number of partial preorders can imply joinability under all the total
preorders, necessary to prove ground joinability.
However, this poses the question of how to choose which partial preorders to
check. Intuitively, for performance, we would like that whenever the two terms
are not ground joinable, that some total preorder where they are not joinable is
found as early as possible, and that whenever the two terms are joinable, that
all total preorders are covered in as few partial preorders as possible.
Example 4. Let S = {f(x, f(y, z))≈f(y, f(x, z))}. Then f(x, f(y, f(z, f(w, u))))
≈f(x, f(y, f(w, f(z, u)))) can be shown to be ground joinable wrt. S by checking
just three cases: ⪰v ∈{z≻w , z∼w , z≺w}, even though there are 6942 possible
preorders.
Waldmeister ﬁrst tries all partial preorders relating two variables among
Vars(s, t), then three, etc. until success, failure (by trying a total order and fail-
ing to join) or reaching a predeﬁned limit of attempts [1]. Twee tries an arbitrary
total strict order, then tries to weaken it, and repeats until all total preorders are
covered [21]. We propose a novel algorithm—incremental ground joinability—
whose main improvement is guiding the process of picking which preorders to
check by ﬁnding, during the process of searching for rewrites on subterms of the
terms we are attempting to join, minimal extensions of the term order with a
variable preorder which allow the rewrite to be done in the ≻direction.
Our algorithm is summarised as follows. We start with an empty queue of
variable preorders, V , initially containing only the empty preorder. Then, while
V is not empty, we pop a preorder ⪰v from the queue, and attempt to perform
a rewrite via an equation which is newly orientable by some extension ⪰′
v of ⪰v.
That is, during the process of ﬁnding generalisations of a subterm of s or t among
left-hand sides of candidate unoriented unit equations l ≈r, when we check that
the instance lθ ≈rθ used to rewrite is oriented, we try to force this to be true
under some minimal extension ≻t[v′] of ≻t[v], if possible. If no such rewrite exists,
the two terms are not strongly joinable under ≻t[v] or any extension, and so are
not strongly ground joinable and we are done. If it exists, we exhaustively rewrite
with ≻t[v′], and check if we obtain the same normal form. If we do not obtain
it yet, we repeat the process of searching rewrites via equations orientable by
further extensions of the preorder. But if we do, then we have proven joinability
in the extended preorder; now we must add back to the queue a set of preorders
O such that all the total preorders which are ⊇⪰v (popped from the queue)
but not ⊇⪰′
v (minimal extension under which we have proven joinability) are
⊇of some ⪰′′
v ∈O (pushed back into the queue to be checked). Obtaining this
O is implemented by order diﬀ(⪰v, ⪰′
v), deﬁned below. Whenever there are no
more preorders in the queue to check, then we have checked that the terms are
strongly joinable under all possible total preorders, and we are done.

Ground Joinability and Connectedness in the Superposition Calculus
177
Together with this, some book-keeping for keeping track of completeness
conditions is necessary. We know that for completeness to be guaranteed, the
conditions in Deﬁnition 1 must hold. They automatically do if C is not a positive
unit or if the rewrite happens on a strict subterm. We also know that after a
term has been rewritten at least once, rewrites on that side are always complete
(since it was rewritten to a smaller term). Therefore we store in the queue,
together with the preorder, a ﬂag in P({L, R}) indicating on which sides does a
top rewrite need to be checked for completeness. Initially the ﬂag is {L} if s ≻t t,
{R} if s ≺t t, {L, R} if s and t are incomparable, and {} if the clause is not a
positive unit. When a rewrite at the top is attempted (say, l ≈r used to rewrite
s = lθ with t being the other side), if the ﬂag for that side is set, then we check if
lθ ⊐l or rθ ≺t. If this fails, the rewrite is rejected. Whenever a side is rewritten
(at any position), the ﬂag for that side is cleared.
The deﬁnition of order diﬀis as follows. Let the transitive reduction of ⪰be
represented by a set of links of the form x≻y / x∼y.
order diﬀ(⪰1, ⪰2) = {⪰+| ⪰∈order diﬀ′(⪰1, ⪰2
−)} ,
(11a)
order diﬀ′(⪰1, ⪰−
2) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
⪰−
2 = {x≻y} ⊎⪰−
2
′ ⇒
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x ≻1 y ⇒order diﬀ′(⪰1, ⪰−
2
′)
x ⊁1 y ⇒
{⪰1 ∪{y≻x} , ⪰1 ∪{x∼y}}
∪order diﬀ′(⪰1 ∪{x≻y}, ⪰−
2
′)
⪰−
2 = {x∼y} ⊎⪰−
2
′ ⇒
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x ∼1 y ⇒order diﬀ′(⪰1, ⪰−
2
′)
x ≁1 y ⇒
{⪰1 ∪{x≻y} , ⪰1 ∪{y≻x}}
∪order diﬀ′(⪰1 ∪{x∼y}, ⪰−
2
′)
⪰−
2= ∅
⇒∅.
(11b)
where ⪰1 ⊆⪰2. In other words, we take a transitive reduction of ⪰2, and for
all links ℓin that reduction which are not part of ⪰1, we return orders ⪰1
augmented with the reverse of ℓand recurse with ⪰1 = ⪰1 ∪ℓ.
Example 5.
⪰1
⪰2
order diﬀ(⪰1, ⪰2)
x ≻y
x ≻y ≻z ≻w x ≻y ∼z , x ≻y ≺z , x ≻y ≻z ∼w , x ≻y ≻z ≺w
y ≺x ≻z x ≻y ≻z
x ≻y ∼z , x ≻z ≻y
Theorem 3. For all total ⪰T
v ⊇⪰1, there exists one and only one ⪰i ∈{⪰2} ∪
order diﬀ(⪰1, ⪰2) such that ⪰T
v ⊇⪰i. For all ⪰T
v ⊉⪰1, there is no ⪰i ∈
{⪰2} ∪order diﬀ(⪰1, ⪰2) such that ⪰T
v ⊇⪰i.

178
A. Duarte and K. Korovin
Proof. See full version of the paper [12].
An algorithm based on searching for rewrites in minimal extensions of a
variable preorder (starting with minimal extensions of the bare term ordering,
≻t[∅]), has several advantages. The main beneﬁt of this approach is that, instead
of imposing an a priori ordering on variables and then checking joinability under
that ordering, we instead build a minimal ordering while searching for candidate
unit equations to rewrite subterms of s, t. For instance, if two terms are not
ground joinable, or not even rewritable in any ≻t[v] where it was not rewritable in
≻t, then an approach such as the one used in Avenhaus, Hillenbrand and L¨ochner
[1] cannot detect this until it has extended the preorder arbitrarily to a total
ordering, while our incremental algorithm immediately realises this. We should
note that empirically this is what happens in most cases: most of the literals we
check during a run are not ground joinable, so for practical performance it is
essential to optimise this case.
Theorem 4. Algorithm 1 returns “Success” only if s
t in C wrt. S.3
Proof. We will show that Algorithm 1 returns “Success” if and only if s
≻t[vT ] t
for all total ⪰T
v over Vars(s, t), which implies s
≻t t.
When ⟨⪰v, s, t, c⟩is popped from V , we exhaustively reduce s, t via equations
in S oriented wrt. ≻t[v], obtaining sr, tr. If sr ∼t[v] tr, then s
≻t[v] t, and so
s
≻t[vT ] t for all total ⪰T
v ⊇⪰v. If sr ≁t[v] tr, we will attempt to rewrite one
of sr, tr using some extended ≻t[v′] where ⪰′
v ⊃⪰v. If this is impossible, then
s̸
≻t[v′] t for any ⪰′
v ⊇⪰v, and therefore there exists at least one total ⪰T
v such
that s̸
⪰T
v t, and we return “Fail”.
If this is possible, then we repeat the process: we exhaustively reduce wrt.
≻t[v′], obtaining s′, t′. If s′ ≁t[v′] t′, then we start again the process from the step
where we attempt to rewrite via an extension of ⪰′
v: we either ﬁnd a rewrite with
some ≻t[v′′] with ⪰′′
v ⊃⪰′
v, and exhaustively normalise wrt. ≻t[v′′] obtaining
s′′, t′′, etc., or we fail to do so and return “Fail”.
If in any such step (after exhaustively normalising wrt. ≻t[v′]) we ﬁnd s′ ∼t[v′]
t′, then s
≻t[v′] t, and so s
≻t[vT ] t for all total ⪰T
v ⊇⪰′
v. Now at this point
we must add back to the queue a set of preorders ⪰′′
v i such that: for all total
⪰T
v ⊇⪰v, either ⪰T
v ⊇⪰′
v (proven to be
) or ⪰T
v ⊇some ⪰′′
v i (added to V
to be checked). For eﬃciency, we would also like for there to be no overlap: no
total ⪰T
v ⊇⪰v is an extension of more than one of {⪰′
v, ⪰′′
v 1, . . .}.
This is true because of Theorem 3. So we add {⟨⪰′′
v i, sr, tr, cr⟩| ⪰′′
v i ∈
order diﬀ(⪰v, ⪰′
v)} to V , where cr = c \ (if sr ̸= s then {L} else {}) \(if tr ̸=
t then {R} else {}). Note also that s
≻t[v] sr and t
≻t[v] tr, therefore also
s
≻t[vi′′] sr and t
≻t[vi′′] tr if ⪰′′
v i ⊃⪰v.
3 Note that the other direction may not always hold, there are strongly ground joinable
terms which are not detected by this method of analysing all preorders between
variables, e.g. f(x, g(y))
f(g(y), x) wrt. S = {f(x, y) ≈f(y, x)}.

Ground Joinability and Connectedness in the Superposition Calculus
179
Algorithm 1: Incremental ground joinability test
Input: literal s ˙≈t ∈C; set of unorientable equations S
Output: whether s
t in C wrt. S
begin
c ←∅if C is not pos. unit, {L} if s ≻t, {R} if s ≺t, {L, R} otherwise
V ←{⟨∅, s, t, c⟩}
while V is not empty do
⟨⪰v, s, t, c⟩←pop from V
s, t ←normalise s, t wrt. ≻t[v], with completeness ﬂag c
c ←c \ ({L} if s was changed) \ ({R} if t was changed)
if s ∼t[v] t then
continue
else
s′, t′, c′ ←s, t, c
while there exists l ≈r ∈S that can rewrite s′ or t′ wrt. some
⪰′
v ⊃⪰v, with completeness ﬂag c do
s′, t′ ←normalise s′, t′ wrt. ≻t[v′], with completeness ﬂag c
c′ ←c′ \ ({L} if s′ was changed) \ ({R} if t′ was changed)
if s′ ∼t[v′] t′ then
for ⪰′′
v in order diﬀ(⪰v, ⪰′
v) do push ⟨⪰′′
v, s, t, c⟩to V
break
end
⪰v ←⪰′
v
else
return Fail
end
end
else
return Success
end
end
where rewriting u in s, t wrt. ≻with completeness ﬂag c succeeds if
(i) u is a strict subterm of s or t,
(ii) u = s with L /∈c,
(iii) u = t with R /∈c,
(iv) instance lσ ≈rσ used to rewrite has l ⊏u,
(v) u = s with rσ ≺t,
(vi) or u = t with rσ ≺s.
end
During this whole process, any rewrites must pass a completeness test men-
tioned previously, such that the conditions in the deﬁnition of
hold. Let s0, t0
be the original terms and s, t be the ones being rewritten and c the completeness
ﬂag. If the rewrite is at a strict subterm position, it succeeds by Deﬁnition 2.
If the rewrite is at the top, then we check c. If L is unset (L /∈c), then either
s ⪰s0 ≺t0 or s ≺s0 or the clause is not a positive unit, so we allow a rewrite
at the top of s, again by Deﬁnition 2. If L is set (L ∈c), then an explicit check

180
A. Duarte and K. Korovin
must be done: we allow a rewrite at the top of s (= s0) iﬀit is done by lσ →rσ
with lσ ⊐l or rσ ≺t0. Respectively for R, with the roles of s and t swapped.
In short, we have shown that if ⟨⪰v, s′, t′, c′⟩is popped from V , then V is only
ever empty, and so the algorithm only terminates with “Success”, if s′
≻t[vT ] t′
for all total ⪰T
v ⊇⪰v. Since V is initialised with ⟨∅, s, t, c⟩, then the algorithm
only returns “Success” if s
≻t[vT ] t for all total ⪰T
v .
⊓⊔
Orienting via Extension of Variable Ordering. In order to apply the
ground joinability algorithm we need a way to check, for a given ≻t and ⪰v
and some s, t, whether there exists a ⪰′
v ⊃⪰v such that s ≻t[v′] t. Here we show
how to do this when ≻t is a Knuth-Bendix Ordering (KBO) [15].
Recall the deﬁnition of KBO. Let ≻s be a partial order on symbols, w be
an N-valued weight function on symbols and variables, with the property that
∃m ∀x ∈V. w(x) = m, w(c) ≥m for all constants c, and there may only exist
one unary symbol f with w(f) = 0 and in this case f ≻s g for all other symbols
g. For terms, their weight is w(f(s1, . . . )) = w(f) + w(s1) + · · · . Let also |s|x be
the number of occurrences of x in s. Then
f(s1, . . . ) ≻KBO g(t1, . . . )
iﬀ
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
either w(f(s1, . . . )) > w(g(t1, . . . )),
or w(f(s1, . . . )) = w(g(t1, . . . ))
and f ≻s g,
or w(f(s1, . . . )) = w(g(t1, . . . ))
and f = g,
and s1, . . . ≻KBOlex t1, . . . ;
and ∀x ∈V. |f(. . . )|x ≥|g(. . . )|x.
(12a)
f(s1, . . . ) ≻KBO x
iﬀ
|f(s1, . . . )|x ≥1 .
(12b)
x ≻KBO y
iﬀ
⊥.
(12c)
The conditions on variable occurrences ensure that s ≻KBO t ⇒∀θ. sθ ≻KBO tθ.
When we extend the order ≻KBO with a variable preorder ⪰v, the starting
point is that x ≻v y ⇒x ≻KBO[v] y and x ∼v y ⇒x ∼KBO[v] y. Then, to ensure
that all the properties of a simpliﬁcation order (included the one mentioned
above) hold, we arrive at the following deﬁnition (similar to [1]).
f(s1, . . . ) ≻KBO[v] g(t1, . . . )
iﬀ
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
either w(f(. . . )) > w(g(. . . )),
or w(f(s1, . . . )) = w(g(t1, . . . ))
and f ≻s g,
or w(f(s1, . . . )) = w(g(t1, . . . ))
and f = g,
and s1, . . . ≻KBO[v]lex t1, . . . ;
and ∀x ∈V. 
y⪰vx |f(. . . )|y
≥
y⪰vx |g(. . . )|y.
(13a)
f(s1, . . . ) ≻KBO[v] x
iﬀ
∃y ⪰v x. |f(s1, . . . )|y ≥1 .
(13b)
x ≻KBO[v] y
iﬀ
x ≻v y .
(13c)

Ground Joinability and Connectedness in the Superposition Calculus
181
To check whether there exists a ⪰′
v ⊃⪰v such that s ≻KBO[v′] t, we need
to check whether there are some x≻y or x = y relations that we can add to ⪰v
such that all the conditions above hold (and such that it still remains a valid
preorder). Let us denote “there exists a ⪰′
v ⊃⪰v such that s ≻KBO[v′] t” by
s ≻KBO[v,v′] t. Then the deﬁnition is
f(s1, . . . ) ≻KBO[v,v′] g(t1, . . . )
iﬀ
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
either w(f(. . . )) > w(g(. . . )),
or w(f(s1, . . . )) = w(g(t1, . . . ))
and f ≻s g,
or w(f(s1, . . . )) = w(g(t1, . . . ))
and f = g,
and s1, . . . ≻KBOlex t1, . . . ;
and ∃x1, y1, . . .
⪰′
v = (⪰v ∪{⟨x1, y1⟩, . . .})+ is a preorder
such that ∀x∈V. 
y⪰′vx |f(. . . )|y
≥
y⪰′vx |g(. . . )|y.
(14a)
f(s1, . . . ) ≻KBO[v,v′] x
iﬀ
∃y ⊀v x. |f(s1, . . . )|y ≥1 ,
with ⪰′
v = ⪰v ∪{x≻y}
or ⪰′
v = ⪰v ∪{x=y} .
(14b)
x ≻KBO[v,v′] y
iﬀ
	
x ⊀v y
with ⪰′
v = ⪰v ∪{x≻y} .
(14c)
This check can be used in Algorithm 1 for ﬁnding extensions of variable order-
ings that orient rewrite rules allowing required normalisations.
4.3
Connectedness
Testing for joinability (i.e. demodulating to s ≈s or s ̸≈s) and ground joinability
(presented in the previous section) require that each step in proving them is done
via an oriented instance of an equation in the set. However, we can weaken this
restriction, if we also change the notion of redundancy being used.
As criteria for redundancy of a clause, ﬁnding either joinability or ground
joinability of a literal in the clause means that the clause can be deleted or the
literal removed from the clause (in case of a positive or negative literal, resp.)
in any context, that is, we can for example add them to a set of deleted clauses,
and for any new clause, if it appears in that set, then immediately remove it
since we already saw that it is redundant. The criterion of connectedness [3,21],
however, is a criterion for redundancy of inferences. This means that a conclusion
simpliﬁed by this criterion can be deleted (or rather, not added), but in that
context only; if it ever comes up again as a conclusion of a diﬀerent inference,
then it is not necessarily also redundant. Connectedness was introduced in the
context of equational completion, here we extend it to general clauses and show
that it is a redundancy in the superposition calculus.
Deﬁnition 4. Terms s and t are connected
under clauses U
and uni-
ﬁer ρ wrt. a set of equations S if there exist terms v1, . . . , vn, equations
l1 ≈r1, . . . , ln−1 ≈rn−1, and substitutions σ1, . . . , σn−1 such that:

182
A. Duarte and K. Korovin
(i) v1 = s and vn = t,
(ii) for all i ∈1, . . . , n −1, either vi+1 = vi[liσi 
→riσi] or vi = vi+1[liσi 
→riσi],
with li ≈ri ∈S,
(iii) for all i ∈1, . . . , n −1, there exists w in 
C∈U

p ˙≈q∈C{p, q}4 such that for
ui ∈{li, ri}, either (a) uiσi ≺wρ, or (b) uiσi = wρ and either ui ⊏w or
w ∈C such that C is not a positive unit.
Theorem 5. Superposition inferences of the form
l ≈r ∨C
s[u] ≈t ∨D
(s[u 
→r] ≈t ∨C ∨D)ρ
,
where ρ = mgu(l, u),
lρ  rρ, sρ  tρ,
and u not a variable,
(15)
where s[u 
→r]ρ and tρ are connected under {l ≈r ∨C, s ≈t ∨D} and uniﬁer
ρ wrt. some set of clauses S, are redundant inferences wrt. S.
Proof. Let us denote s′ = s[u 
→r]. Let also U = {l ≈r ∨C, s ≈t ∨D} and
M = 
C∈U

p ˙≈q∈C{p, q}. We will show that if s′ρ and tρ are connected under
U and ρ, by equations in S, then every instance of that inference obeys the
condition for closure redundancy of an inference (see, Sect. 4), wrt. S.
Consider any (s′ ≈t ∨C ∨D)ρ · θ where θ ∈GSubs(Uρ). Either s′ρθ = tρθ,
and we are done (it follows from ∅), or s′ρθ ≻tρθ, or s′ρθ ≺tρθ.
Consider the case s′ρθ ≻tρθ. For all i ∈1, . . . , n−1, there exists a C′ ∈U and
a w ∈C′ such that either (iii.a) liσiθ ≺wρθ, or (iii.b) liσiθ = wρθ and li ⊏v,
or (iii.b) liσiθ = wρθ and C′ is not a positive unit. Likewise for ri. Therefore,
for all i ∈1, . . . , n −1, there exists a C′ ∈U such that (li ≈ri) · σiθ ≺C′ · ρθ.
Since (t ≈t ∨· · · )ρ · θ is also smaller than (s′ ≈t ∨· · · )ρ · θ and a tautology,
then the instance (s′ ≈t ∨· · · )ρ · θ of the conclusion follows from closures in
GClos(S) such that each is smaller than one of (l ≈r ∨C) · ρθ, (s ≈t ∨D) · ρθ.
In the case that s′ρθ ≺tρθ, the same idea applies, but now it is (s′ ≈
s′ ∨· · · )ρ · θ which is smaller than (s′ ≈t ∨· · · )ρ · θ and is a tautology.
Therefore, we have shown that for all θ ∈GSubs((l ≈r ∨C)ρ, (s ≈t∨D)ρ),
the instance (s′ ≈t∨· · · )ρ·θ of the conclusion follows from closures in GClos(S)
which are all smaller than one of (l ≈r ∨C) · ρθ, (s ≈t ∨D) · ρθ. Since
any valid superposition inference with ground clauses has to have l = u, then
any θ′ ∈GSubs(l ≈r ∨C, s ≈t ∨D, (s′ ≈t ∨C ∨D)ρ) such that the
inference (l ≈r ∨C)θ′, (s ≈t ∨D)θ′ |−(s′ ≈t ∨C ∨D)ρθ′ is valid must
have θ′ = ρθ′′, since ρ is the most general uniﬁer. Therefore, we have shown
that for all θ′ ∈GSubs(l ≈r ∨C, s ≈t ∨D, (s′ ≈t ∨C ∨D)ρ) for which
(l ≈r ∨C)θ′, (s ≈t ∨D)θ′ |−(s′ ≈t ∨C ∨D)ρθ′ is a valid superposition
inference, the instance (s′ ≈t ∨· · · )ρ · θ′ of the conclusion follows from closures
in GClos(S) which are all smaller than one of (l ≈r ∨C) · θ′, (s ≈t ∨D) · θ′, so
the inference is redundant.
⊓⊔
4 That is, in the set of top-level terms of literals of clauses in U.

Ground Joinability and Connectedness in the Superposition Calculus
183
Theorem 6. Superposition inferences of the form
l ≈r ∨C
s[u] ̸≈t ∨D
(s[u 
→r] ̸≈t ∨C ∨D)ρ
,
where ρ = mgu(l, u),
lρ  rρ, sρ  tρ,
and u not a variable,
(16)
where s[u 
→r]ρ and tρ are connected under {l ≈r ∨C, s ̸≈t ∨D} and uniﬁer
ρ wrt. some set of clauses S, are redundant inferences wrt. S ∪{(C ∨D)ρ}.
Proof. Analogously to the previous proof, we ﬁnd that for all instances of the
inference, the closure (s′ ̸≈t∨· · · )ρ·θ follows from smaller closure (t ̸≈t∨· · · )ρ·θ
or (s′ ̸≈s′ ∨· · · )ρ·θ and closures (li ≈ri)·σiθ smaller than max{(l ≈r ∨C)·θ ,
(s ̸≈t∨D)·θ , (s′ ̸≈t∨C∨D)ρ·θ}. But (t ̸≈t∨C∨D)ρ·θ and (s′ ̸≈s′∨C∨D)ρ·θ
both follow from smaller (C ∨D)ρ · θ, therefore the inference is redundant wrt.
S ∪{(C ∨D)ρ}.
⊓⊔
4.4
Ground Connectedness
Just as joinability can be generalised to ground joinability, so can connectedness
be generalised to ground connectedness. Two terms s, t are ground connected
under U and ρ wrt. S if, for all θ ∈GSubs(s, t), sθ and tθ are connected under
D and ρ wrt. S. Analogously to strong ground joinability, we have that if s and t
are connected using ≻t[v] for all total ⪰v over Vars(s, t), then s and t are ground
connected.
Theorem 7. Superposition inferences of the form
l ≈r ∨C
s[u] ≈t ∨D
(s[u 
→r] ≈t ∨C ∨D)ρ
,
where ρ = mgu(l, u),
lρ  rρ, sρ  tρ,
and u not a variable,
(17)
where s[u 
→r]ρ and tρ are ground connected under {l ≈r ∨C, s ≈t ∨D} and
uniﬁer ρ wrt. some set of clauses S, are redundant inferences wrt. S.
Theorem 8. Superposition inferences of the form
l ≈r ∨C
s[u] ̸≈t ∨D
(s[u 
→r] ̸≈t ∨C ∨D)ρ
,
where ρ = mgu(l, u),
lρ  rρ, sρ  tρ,
and u not a variable,
(18)
where s[u 
→r]ρ and tρ are ground connected under {l ≈r ∨C, s ̸≈t ∨D} and
uniﬁer ρ wrt. some set of clauses S, are redundant inferences wrt. S∪{(C∨D)ρ}.
Proof. The proof of Theorem 7 and 8 is analogous to that of Theorem 5 and 6.
The weakening of connectedness to ground connectedness only means that the
proof of connectedness (e.g. the vi, li ≈ri, σi) may be diﬀerent for diﬀerent
ground instances. For all the steps in the proof to hold we only need that for all
the instances θ ∈GSubs(l ≈r ∨C , s ˙≈t ∨D , (s[u 
→r] ˙≈t ∨C ∨D)ρ) of the
inference, θ = σθ′ with σ ∈GSubs(s[u 
→r]ρ, tρ), which is true.
⊓⊔
Discussion about the strategy for implementation of connectedness and ground
connectedness is outside the scope of this paper.

184
A. Duarte and K. Korovin
5
Evaluation
We implemented ground joinability in a theorem prover for ﬁrst-order logic,
iProver [10,16].5 iProver combines superposition, Inst-Gen, and resolution cal-
culi. For superposition, iProver implements a range of simpliﬁcations including
encompassment demodulation, AC normalisation [10], light normalisation [16],
subsumption and subsumption resolution. We run our experiments over FOF
problems of the TPTP v7.5 library [23] (17 348 problems) on a cluster of Linux
servers with 3 GHz 11 core CPUs, 128 GB memory, with each problem running on
a single core with a time limit of 300 s. We used a default strategy (which has not
yet been ﬁne-tuned after the introduction of ground joinability), with superpo-
sition enabled and the rest of the components disabled. With ground joinability
enabled, iProver solved 133 problems more which it did not solve without ground
joinability. Note that this excludes the contribution of AC ground joinability or
encompassment demodulation [11] (always enabled).
Some of the problems are not interesting for this analysis because ground
joinability is not even tried, either because they are solved before superposition
saturation begins, or because they are ground. If we exclude these, we are left
with 10 005 problems. Ground joinability is successfully used to eliminate clauses
in 3057 of them (30.6%, Fig. 1a). This indicates that ground joinability is useful
in many classes of problems, including in non-unit problems where it previously
had never been used.
Fig. 1. (a) Clauses simpliﬁed by ground joinability. (b) % of runtime spent in gr.
joinability
In terms of the performance impact of enabling ground joinability, we mea-
sure that among problems whose runtime exceeds 1 s, only in 72 out of 8574
problems does the time spent inside the ground joinability algorithm exceed 20%
of runtime, indicating that our incremental algorithm is eﬃcient and suitable for
broad application (Fig. 1b).
5 iProver is available at http://www.cs.man.ac.uk/∼korovink/iprover.

Ground Joinability and Connectedness in the Superposition Calculus
185
TPTP classiﬁes problems by rating in [0,1]. Problems with rating ≥0.9 are
considered to be very challenging. Problems with rating 1.0 have never been
solved by any automated theorem prover. iProver using ground joinability solves
3 previously unsolved rating 1.0 problems, and 7 further problems with rating in
[0.9,1.0[ (Table 1). We note that some of these latter (e.g. LAT140-1, ROB018-10,
REL045-1) were previously only solved by UEQ or SMT provers, but not by any
full ﬁrst-order prover.
Table 1. Hard or unsolved problems in TPTP, solved by iProver with ground joinabil-
ity.
Name
Rating
Name
Rating
LAT140-1 0.90
ROB018-10 0.95
REL045-1 0.90
LCL477+1
0.97
LCL557+1 0.92
LCL478+1
1.00
LCL563+1 0.92
CSR039+6
1.00
LCL474+1 0.94
CSR040+6
1.00
6
Conclusion and Further Work
In this work we extended the superposition calculus with ground joinability and
connectedness, and proved that these rules preserve completness using a modiﬁed
notion of redundancy, thus bringing for the ﬁrst time these techniques for use in
full ﬁrst-order logic problems. We have also presented an algorithm for checking
ground joinability which attempts to check as few variable preorders as possible.
Preliminary results show three things: (1) ground joinability is applicable in
a sizeable number of problems across diﬀerent domains, including in non-unit
problems (where it was never applied before), (2) our proposed algorithm for
checking ground joinability is eﬃcient, with over 3
4 of problems spending less
than 1% of runtime there, and (3) application of ground joinability in the super-
position calculus of iProver improves overall performance, including discovering
solutions to hitherto unsolved problems.
These results are promising, and further optimisations can be done. Imme-
diate next steps include ﬁne-tuning the implementation, namely adjusting the
strategies and strategy combinations to make full use of ground joinability and
connectedness. iProver uses a sophisticated heuristic system which has not yet
been tuned for ground joinability and connectedness [14].
In terms of practical implementation of connectedness and ground connect-
edness, further research is needed on the interplay between those (criteria for
redundancy of inferences) and joinability and ground joinability (criteria for
redundancy of clauses).
On the theoretical level, recent work [24] provides a general framework for
saturation theorem proving, and we will investigate how techniques developed
in this paper can be incorporated into this framework.

186
A. Duarte and K. Korovin
References
1. Avenhaus, J., Hillenbrand, T., L¨ochner, B.: On using ground joinable equations
in equational theorem proving. J. Symb. Comput. 36(1), 217–233 (2003). https://
doi.org/10.1016/S0747-7171(03)00024-5
2. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press
(1998). ISBN 978-0521779203
3. Bachmair, L., Dershowitz, N.: Critical pair criteria for completion. J. Symb. Com-
put. 6(1), 1–18 (1988). https://doi.org/10.1016/S0747-7171(88)80018-X
4. Bachmair, L., Dershowitz, N., Plaisted, D.A.: Completion without failure. In: A¨ıt-
Kaci, H., Nivat, M. (eds.) Resolution of Equations in Algebraic Structures, vol. II:
Rewriting Techniques, pp. 1–30. Academic Press (1989). https://doi.org/10.1016/
B978-0-12-046371-8.50007-9
5. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Log. Comput. 4(3), 217–247 (1994). https://doi.org/10.
1093/logcom/4.3.217
6. Bachmair, L., Ganzinger, H., Lynch, C.A., Snyder, W.: Basic paramodulation. Inf.
Comput. 121(2), 172–192 (1995). https://doi.org/10.1006/inco.1995.1131. ISSN
0890-5401
7. Barthelemy, J.P.: An asymptotic equivalent for the number of total preorders on
a ﬁnite set. Discret. Math. 29(3), 311–313 (1980). https://doi.org/10.1016/0012-
365x(80)90159-4
8. Claessen, K., Smallbone, N.: Eﬃcient encodings of ﬁrst-order horn formulas in
equational logic. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018.
LNCS (LNAI), vol. 10900, pp. 388–404. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-94205-6 26
9. Dershowitz, N., Manna, Z.: Proving termination with multiset orderings. Commun.
ACM 22(8), 465–476 (1979). https://doi.org/10.1145/359138.359142
10. Duarte, A., Korovin, K.: Implementing superposition in iProver (system descrip-
tion). In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020. LNCS (LNAI),
vol. 12167, pp. 388–397. Springer, Cham (2020). https://doi.org/10.1007/978-3-
030-51054-1 24
11. Duarte, A., Korovin, K.: AC simpliﬁcations and closure redundancies in the super-
position calculus. In: Das, A., Negri, S. (eds.) TABLEAUX 2021. LNCS (LNAI),
vol. 12842, pp. 200–217. Springer, Cham (2021). https://doi.org/10.1007/978-3-
030-86059-2 12
12. Duarte, A., Korovin, K.: Ground Joinability and Connectedness in the Superposi-
tion Calculus (2022, to appear)
13. Hillenbrand, T., Buch, A., Vogt, R., L¨ochner, B.: Waldmeister—high-performance
equational deduction. J. Autom. Reason. 18(2), 265–270 (1997). https://doi.org/
10.1023/A:1005872405899
14. Holden, E.K., Korovin, K.: Heterogeneous heuristic optimisation and scheduling for
ﬁrst-order theorem proving. In: Kamareddine, F., Sacerdoti Coen, C. (eds.) CICM
2021. LNCS (LNAI), vol. 12833, pp. 107–123. Springer, Cham (2021). https://doi.
org/10.1007/978-3-030-81097-9 8
15. Knuth, D.E., Bendix, P.: Simple word problems in universal algebras. In: Leech,
J. (ed.) Computational Problems in Abstract Algebra, pp. 263–297. Pergamon
(1970). https://doi.org/10.1016/B978-0-08-012975-4.50028-X
16. Korovin, K.: iProver—an instantiation-based theorem prover for ﬁrst-order logic
(system description). In: Armando, A., Baumgartner, P., Dowek, G. (eds.) IJCAR

Ground Joinability and Connectedness in the Superposition Calculus
187
2008. LNCS (LNAI), vol. 5195, pp. 292–298. Springer, Heidelberg (2008). https://
doi.org/10.1007/978-3-540-71070-7 24
17. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
18. Martin, U., Nipkow, T.: Ordered rewriting and conﬂuence. In: Stickel, M.E. (ed.)
CADE 1990. LNCS, vol. 449, pp. 366–380. Springer, Heidelberg (1990). https://
doi.org/10.1007/3-540-52885-7 100
19. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Robinson,
J.A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. 2, pp. 371–443.
Elsevier and MIT Press (2001). ISBN 0-444-50813-9
20. Schulz, S.: System description: E 1.8. In: McMillan, K., Middeldorp, A., Voronkov,
A. (eds.) LPAR 2013. LNCS, vol. 8312, pp. 735–743. Springer, Heidelberg (2013).
https://doi.org/10.1007/978-3-642-45221-5 49
21. Smallbone, N.: Twee: an equational theorem prover. In: Platzer, A., Sutcliﬀe, G.
(eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 602–613. Springer, Cham (2021).
https://doi.org/10.1007/978-3-030-79876-5 35
22. Sutcliﬀe, G.: The CADE ATP system competition–CASC. AI Mag. 37(2), 99–101
(2016). https://doi.org/10.1609/aimag.v37i2.2620
23. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure–from CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017). https://doi.org/
10.1007/s10817-017-9407-7
24. Waldmann, U., Tourret, S., Robillard, S., Blanchette, J.: A comprehensive frame-
work for saturation theorem proving. In: Peltier, N., Sofronie-Stokkermans, V.
(eds.) IJCAR 2020. LNCS (LNAI), vol. 12166, pp. 316–334. Springer, Cham (2020).
https://doi.org/10.1007/978-3-030-51074-9 18
25. Winkler, S., Moser, G.: MædMax: a maximal ordered completion tool. In: Galmiche,
D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp.
472–480. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 31
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Connection-Minimal Abduction in EL
via Translation to FOL
Fajar Haifani1,2(B)
, Patrick Koopmann3(B)
, Sophie Tourret1,4(B)
,
and Christoph Weidenbach1(B)
1 Max-Planck-Institut f¨ur Informatik, Saarland Informatics Campus,
Saarbr¨ucken, Germany
{f.haifani,c.weidenbach}@mpi-inf.mpg.de
2 Graduate School of Computer Science, Saarbr¨ucken, Germany
3 TU Dresden, Dresden, Germany
patrick.koopmann@tu-dresden.de
4 Universit´e de Lorraine, CNRS, Inria, LORIA, Nancy, France
sophie.tourret@inria.fr
Abstract. Abduction in description logics ﬁnds extensions of a knowl-
edge base to make it entail an observation. As such, it can be used to
explain why the observation does not follow, to repair incomplete knowl-
edge bases, and to provide possible explanations for unexpected observa-
tions. We consider TBox abduction in the lightweight description logic
EL, where the observation is a concept inclusion and the background
knowledge is a TBox, i.e., a set of concept inclusions. To avoid useless
answers, such problems usually come with further restrictions on the
solution space and/or minimality criteria that help sort the chaﬀfrom
the grain. We argue that existing minimality notions are insuﬃcient, and
introduce connection minimality. This criterion follows Occam’s razor by
rejecting hypotheses that use concept inclusions unrelated to the problem
at hand. We show how to compute a special class of connection-minimal
hypotheses in a sound and complete way. Our technique is based on a
translation to ﬁrst-order logic, and constructs hypotheses based on prime
implicates. We evaluate a prototype implementation of our approach on
ontologies from the medical domain.
1
Introduction
Ontologies are used in areas like biomedicine or the semantic web to represent
and reason about terminological knowledge. They consist normally of a set of
axioms formulated in a description logic (DL), giving deﬁnitions of concepts, or
stating relations between them. In the lightweight description logic EL [2], par-
ticularly used in the biomedical domain, we ﬁnd ontologies that contain around
a hundred thousand axioms. For instance, SNOMED CT1 contains over 350,000
axioms, and the Gene Ontology GO2 deﬁnes over 50,000 concepts. A central
1 https://www.snomed.org/.
2 http://geneontology.org/.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 188–207, 2022.
https://doi.org/10.1007/978-3-031-10769-6_12

Abduction in EL via FOL
189
reasoning task for ontologies is to determine whether one concept is subsumed
by another, a question that can be answered in polynomial time [1], and rather
eﬃciently in practice using highly optimized description logic reasoners [29]. If
the answer to this question is unexpected or hints at an error, a natural inter-
est is in an explanation for that answer—especially if the ontology is complex.
But whereas explaining entailments—i.e., explaining why a concept subsump-
tion holds—is well-researched in the DL literature and integrated into standard
ontology editors [21,22], the problem of explaining non-entailments has received
less attention, and there is no standard tool support. Classical approaches involve
counter-examples [5], or abduction.
In abduction a non-entailment T ̸|= α, for a TBox T and an observation α, is
explained by providing a “missing piece”, the hypothesis, that, when added to the
ontology, would entail α. Thus it provides possible ﬁxes in case the entailment
should hold. In the DL context, depending on the shape of the observation, one
distinguishes between concept abduction [6], ABox abduction [7–10,12,19,24,
25,30,31], TBox abduction [11,33] or knowledge base abduction [14,26]. We are
focusing here on TBox abduction, where the ontology and hypothesis are TBoxes
and the observation is a concept inclusion (CI), i.e., a single TBox axiom.
To illustrate this problem, consider the following TBox, about academia,
Ta = { ∃employment.ResearchPosition ⊓∃qualiﬁcation.Diploma ⊑Researcher,
∃writes.ResearchPaper ⊑Researcher, Doctor ⊑∃qualiﬁcation.PhD,
Professor ≡Doctor ⊓∃employment.Chair,
FundsProvider ⊑∃writes.GrantApplication }
that states, in natural language:
• “Being employed in a research position and having a qualifying diploma
implies being a researcher.”
• “Writing a research paper implies being a researcher.”
• “Being a doctor implies holding a PhD qualiﬁcation.”
• “Being a professor is being a doctor employed at a (university) chair.”
• “Being a funds provider implies writing grant applications.”
The observation αa = Professor ⊑Researcher, “Being a professor implies being
a researcher”, does not follow from Ta although it should. We can use TBox
abduction to ﬁnd diﬀerent ways of recovering this entailment.
Commonly, to avoid trivial answers, the user provides syntactic restrictions
on hypotheses, such as a set of abducible axioms to pick from [8,30], a set
of abducible predicates [25,26], or patterns on the shape of the solution [11].
But even with those restrictions in place, there may be many possible solutions
and, to ﬁnd the ones with the best explanatory potential, syntactic criteria
are usually combined with minimality criteria such as subset minimality, size
minimality, or semantic minimality [7]. Even combined, these minimality criteria
still retain a major ﬂaw. They allow for explanations that go against the principle
of parsimony, also known as Occam’s razor, in that they may contain concepts

190
F. Haifani et al.
that are completely unrelated to the problem at hands. As an illustration, let us
return to our academia example. The TBoxes
Ha1 = { Chair ⊑ResearchPosition, PhD ⊑Diploma} and
Ha2 = { Professor ⊑FundsProvider, GrantApplication ⊑ResearchPaper}
are two hypotheses solving the TBox abduction problem involving Ta and αa.
Both of them are subset-minimal, have the same size, and are incomparable w.r.t.
the entailment relation, so that traditional minimality criteria cannot distinguish
them. However, intuitively, the second hypothesis feels more arbitrary than the
ﬁrst. Looking at Ha1, Chair and ResearchPosition occur in Ta in concept inclusions
where the concepts in αa also occur, and both PhD and Diploma are similarly
related to αa but via the role qualiﬁcation. In contrast, Ha2 involves the concepts
FundsProvider and GrantApplication that are not related to αa in any way in
Ta. In fact, any random concept inclusion A ⊑∃writes.B in Ta would lead to
a hypothesis similar to Ha2 where A replaces FundsProvider and B replaces
GrantApplication. Such explanations are not parsimonious.
We introduce a new minimality criterion called connection minimality that
is parsimonious (Sect. 3), deﬁned for the lightweight description logic EL. This
criterion characterizes hypotheses for T and α that connect the left- and right-
hand sides of the observation α without introducing spurious connections. To
achieve this, every left-hand side of a CI in the hypothesis must follow from
the left-hand side of α in T , and, taken together, all the right-hand sides of the
CIs in the hypothesis must imply the right-hand side of α in T , as is the case
for Ha1. To compute connection-minimal hypotheses in practice, we present a
technique based on ﬁrst-order reasoning that proceeds in three steps (Sect. 4).
First, we translate the abduction problem into a ﬁrst-order formula Φ. We then
compute the prime implicates of Φ, that is, a set of minimal logical consequences
of Φ that subsume all other consequences of Φ. In the ﬁnal step, we construct,
based on those prime implicates, solutions to the original problem. We prove
that all hypotheses generated in this way satisfy the connection minimality cri-
terion, and that the method is complete for a relevant subclass of connection-
minimal hypotheses. We use the SPASS theorem prover [34] as a restricted SOS-
resolution [18,35] engine for the computation of prime implicates in a prototype
implementation (Sect. 5), and we present an experimental analysis of its perfor-
mances on a set of bio-medical ontologies.(Sect. 6). Our results indicate that our
method can in many cases be applied in practice to compute connection-minimal
hypotheses. A technical report companion of this paper includes all proofs as well
as a detailed example of our method as appendices [16].
There are not many techniques that can handle TBox abduction in EL or
more expressive DLs [11,26,33]. In [11], instead of a set of abducibles, a set
of justiﬁcation patterns is given, in which the solutions have to ﬁt. An arbi-
trary oracle function is used to decide whether a solution is admissible or not
(which may use abducibles, justiﬁcation patterns, or something else), and it is
shown that deciding the existence of hypotheses is tractable. However, diﬀerent
to our approach, they only consider atomic CIs in hypotheses, while we also

Abduction in EL via FOL
191
allow for hypotheses involving conjunction. The setting from [33] also considers
EL, and abduction under various minimality notions such as subset minimality
and size minimality. It presents practical algorithms, and an evaluation of an
implementation for an always-true informativeness oracle (i.e., limited to sub-
set minimality). Diﬀerent to our approach, it uses an external DL reasoner to
decide entailment relationships. In contrast, we present an approach that directly
exploits ﬁrst-order reasoning, and thus has the potential to be generalisable to
more expressive DLs.
While dedicated resolution calculi have been used before to solve abduction
in DLs [9,26], to the best of our knowledge, the only work that relies on ﬁrst-
order reasoning for DL abduction is [24]. Similar to our approach, it uses SOS-
resolution, but to perform ABox adbuction for the more expressive DL ALC.
Apart from the diﬀerent problem solved, in contrast to [24] we also provide a
semantic characterization of the hypotheses generated by our method. We believe
this characterization to be a major contribution of our paper. It provides an
intuition of what parsimony is for this problem, independently of one’s ease with
ﬁrst-order logic calculi, which should facilitate the adoption of this minimality
criterion by the DL community. Thanks to this characterization, our technique
is calculus agnostic. Any method to compute prime implicates in ﬁrst-order logic
can be a basis for our abduction technique, without additional theoretical work,
which is not the case for [24]. Thus, abduction in EL can beneﬁt from the latest
advances in prime implicates generation in ﬁrst-order logic.
2
Preliminaries
We ﬁrst recall the descripton logic EL and its translation to ﬁrst-order logic [2],
as well as TBox abduction in this logic.
Let NC and NR be pair-wise disjoint, countably inﬁnite sets of unary predi-
cates called atomic concepts and of binary predicates called roles, respectively.
Generally, we use letters A, B, E, F,... for atomic concepts, and r for roles,
possibly annotated. Letters C, D, possibly annotated, denote EL concepts, built
according to the syntax rule
C ::= ⊤| A | C ⊓C | ∃r.C .
We implicitly represent EL conjunctions as sets, that is, without order, nested
conjunctions, and multiple occurrences of a conjunct. We use {C1, . . . , Cm} to
abbreviate C1 ⊓. . . ⊓Cm, and identify the empty conjunction (m = 0) with ⊤.
An EL TBox T is a ﬁnite set of concept inclusions (CIs) of the form C ⊑D.
EL is a syntactic variant of a fragment of ﬁrst-order logic that uses NC and NR
as predicates. Speciﬁcally, TBoxes T and CIs α correspond to closed ﬁrst-order
formulas π(T ) and π(α) resp., while concepts C correspond to open formulas
π(C, x) with a free variable x. In particular, we have

192
F. Haifani et al.
π(⊤, x) := true,
π(∃r.C, x) := ∃y.(r(x, y) ∧π(C, y)),
π(A, x) := A(x),
π(C ⊑D) := ∀x.(π(C, x) →π(D, x)),
π(C ⊓D, x) := π(C, x) ∧π(D, x),
π(T ) :=

{π(α) | α ∈T }.
As common, we often omit the  in conjunctions  Φ, that is, we identify sets
of formulas with the conjunction over those. The notions of a term t; an atom
P(¯t) where ¯t is a sequence of terms; a positive literal P(¯t); a negative literal
¬P(¯t); and a clause, Horn, deﬁnite, positive or negative, are deﬁned as usual for
ﬁrst-order logic, and so are entailment and satisfaction of ﬁrst-order formulas.
We identify CIs and TBoxes with their translation into ﬁrst-order logic, and
can thus speak of the entailment between formulas, CIs and TBoxes. When
T |= C ⊑D for some T , we call C a subsumee of D and D a subsumer of C.
We adhere here to the deﬁnition of the word “subsume”: “to include or contain
something else”, although the terminology is reversed in ﬁrst-order logic. We say
two TBoxes T1, T2 are equivalent, denoted T1 ≡T2 iﬀT1 |= T2 and T2 |= T1. For
example {D ⊑C1, . . . , D ⊑Cn} ≡{D ⊑C1 ⊓. . . ⊓Cn}. It is well known that,
due to the absence of concept negation, every EL TBox is consistent.
The abduction problem we are concerned with in this paper is the following:
Deﬁnition 1. An EL TBox abduction problem (shortened to abduction prob-
lem) is a tuple ⟨T , Σ, C1 ⊑C2⟩, where T is a TBox called the background
knowledge, Σ is a set of atomic concepts called the abducible signature, and
C1 ⊑C2 is a CI called the observation, s.t. T ̸|= C1 ⊑C2. A solution to this
problem is a TBox
H ⊆{A1 ⊓· · · ⊓An ⊑B1 ⊓· · · ⊓Bm | {A1, . . . , An, B1, . . . , Bm} ⊆Σ}
where m > 0, n ≥0 and such that T ∪H |= C1 ⊑C2 and, for all CIs α ∈H,
T ̸|= α. A solution to an abduction problem is called a hypothesis.
For example, Ha1 and Ha2 are solutions for ⟨Ta, Σ, αa⟩, as long as Σ contains
all the atomic concepts that occur in them. Note that in our setting, as in [6,
33], concept inclusions in a hypothesis are ﬂat, i.e., they contain no existential
role restrictions. While this restricts the solution space for a given problem,
it is possible to bypass this limitation in a targeted way, by introducing fresh
atomic concepts equivalent to a concept of interest. We exclude the consistency
requirement T ∪H ̸|= ⊥, that is given in other deﬁnitions of DL abduction
problem [25], since EL TBoxes are always consistent. We also allow m > 1 instead
of the usual m = 1. This produces the same hypotheses modulo equivalence.
For simplicity, we assume in the following that the concepts C1 and C2 in the
abduction problem are atomic. We can always introduce fresh atomic concepts
A1 and A2 with A1 ⊑C1 and C2 ⊑A2 to solve the problem for complex concepts.
Common minimality criteria include subset minimality, size minimality and
semantic minimality, that respectively favor H over H′ if: H ⊊H′; the number
of atomic concepts in H is smaller than in H′; and if H |= H′ but H′ ̸|= H.

Abduction in EL via FOL
193
3
Connection-Minimal Abduction
To address the lack of parsimony of common minimality criteria, illustrated
in the academia example, we introduce connection minimality, Intuitively, con-
nection minimality only accepts those hypotheses that ensure that every CI in
the hypothesis is connected to both C1 and C2 in T , as is the case for Ha1
in the academia example. The deﬁnition of connection minimality is based on
the following ideas: 1) Hypotheses for the abduction problem should create a
connection between C1 and C2, which can be seen as a concept D that satisﬁes
T ∪H |= C1 ⊑D, D ⊑C2. 2) To ensure parsimony, we want this connection
to be based on concepts D1 and D2 for which we already have T |= C1 ⊑D1,
D2 ⊑C2. This prevents the introduction of unrelated concepts in the hypothe-
sis. Note however that D1 and D2 can be complex, thus the connection from C1
to D1 (resp. D2 to C2) can be established by arbitrarily long chains of concept
inclusions. 3) We additionally want to make sure that the connecting concepts
are not more complex than necessary, and that H only contains CIs that directly
connect parts of D2 to parts of D1 by closely following their structure.
To address point 1), we simply introduce connecting concepts formally.
Deﬁnition 2. Let C1 and C2 be concepts. A concept D connects C1 to C2 in T
if and only if T |= C1 ⊑D and T |= D ⊑C2.
Note that if T |= C1 ⊑C2 then both C1 and C2 are connecting concepts from
C1 to C2, and if T ̸|= C1 ⊑C2, the case of interest, neither of them are.
To address point 2), we must capture how a hypothesis creates the connec-
tion between the concepts C1 and C2. As argued above, this is established via
concepts D1 and D2 that satisfy T |= C1 ⊑D1, D2 ⊑C2. Note that having
only two concepts D1 and D2 is exactly what makes the approach parsimonious.
If there was only one concept, C1 and C2 would already be connected, and as
soon as there are more than two concepts, hypotheses start becoming more arbi-
trary: for a very simple example with unrelated concepts, assume given a TBox
that entails Lion ⊑Felidae, Mammal ⊑Animal and House ⊑Building. A possible
hypothesis to explain Lion ⊑Animal is {Felidae ⊑House, Building ⊑Mammal}
but this explanation is more arbitrary than {Felidae ⊑Mammal}—as is the case
when comparing Ha2 with Ha1 in the academia example—because of the lack of
connection of House ⊑Building with both Lion and Animal. Clearly this CI could
be replaced by any other CI entailed by T , which is what we want to avoid.
We can represent the structure of D1 and D2 in graphs by using EL descrip-
tion trees, originally from Baader et al. [3].
Deﬁnition 3. An EL description tree is a ﬁnite labeled tree T = (V, E, v0, l)
where V is a set of nodes with root v0 ∈V , the nodes v ∈V are labeled with
l(v) ⊆NC, and the (directed) edges vrw ∈E are such that v, w ∈V and are
labeled with r ∈NR.
Given a tree T = (V, E, v0, l) and v ∈V , we denote by T(v) the subtree of T that
is rooted in v. If l(v0) = {A1, . . . , Ak} and v1, . . ., vn are all the children of v0, we

194
F. Haifani et al.
∅
Chair
employment
PhD
qualiﬁcation
∅
ResearchPosition
employment
Diploma
qualiﬁcation
Fig. 1. Description trees of D1 (left) and D2 (right).
can deﬁne the concept represented by T recursively using CT = A1 ⊓. . . ⊓Ak ⊓
∃r1.CT(v1) ⊓. . . ⊓∃rl.CT(vl) where for j ∈{1, . . . , n}, v0rjvj ∈E. Conversely,
we can deﬁne TC for a concept C = A1 ⊓. . . ⊓Ak ⊓∃r1.C1 ⊓. . . ⊓∃rn.Cn
inductively based on the pairwise disjoint description trees TCi = {Vi, Ei, vi, li},
i ∈{1, . . . , n}. Speciﬁcally, TC = (VC, EC, vC, lC), where
VC = {v0} ∪n
i=1 Vi,
lC(v) = li(v) for v ∈Vi,
EC = {v0rivi | 1 ≤i ≤n} ∪n
i=1 Ei,
lC(v0) = {A1, . . . , Ak}.
If T = ∅, then subsumption between EL concepts is characterized by the
existence of a homomorphism between the corresponding description trees [3].
We generalise this notion to also take the TBox into account.
Deﬁnition 4. Let T1 = (V1, E1, v0, l1) and T2 = (V2, E2, w0, l2) be two descrip-
tion trees and T a TBox. A mapping φ : V2 →V1 is a T -homomorphism from
T2 to T1 if and only if the following conditions are satisﬁed:
1. φ(w0) = v0
2. φ(v)rφ(w) ∈E1 for all vrw ∈E2
3. for every v ∈V1 and w ∈V2 with v = φ(w), T |=  l1(v) ⊑ l2(w)
If only 1 and 2 are satisﬁed, then φ is called a weak homomorphism.
T -homomorphisms for a given TBox T capture subsumption w.r.t. T . If there
exists a T -homomorphism φ from T2 to T1, then T |= CT1 ⊑CT2. This can
be shown easily by structural induction using the deﬁnitions [16]. The weak
homomorphism is the structure on which a T -homomorphism can be built by
adding some hypothesis H to T . It is used to reveal missing links between a
subsumee D2 of C2 and a subsumer D1 of C1, that can be added using H.
Example 5. Consider the concepts
D1 = ∃employment.Chair ⊓∃qualiﬁcation.PhD
D2 = ∃employment.ResearchPosition ⊓∃qualiﬁcation.Diploma
from the academia example. Figure 1 illustrates description trees for D1 (left)
and D2 (right). The curved arrows show a weak homomorphism from TD2 to
TD1 that can be strengthened into a T -homomorphism for some TBox T that
corresponds to the set of CIs in Ha1 ∪{⊤⊑⊤}. The ﬁgure can also be used to

Abduction in EL via FOL
195
illustrate what we mean by connection minimality: in order to create a connection
between D1 and D2, we should only add the CIs from Ha1 ∪{⊤⊑⊤} unless
they are already entailed by Ta. In practice, this means the weak homomorphism
from D2 to D1 becomes a (Ta ∪Ha1)-homomorphism.
To address point 3), we deﬁne a partial order ⪯⊓on concepts, s.t. C ⪯⊓D if
we can turn D into C by removing conjuncts in subexpressions, e.g., ∃r′.B ⪯⊓
∃r.A ⊓∃r′.(B ⊓B′). Formally, this is achieved by the following deﬁnition.
Deﬁnition 6. Let C and D be arbitrary concepts. Then C ⪯⊓D if either:
• C = D,
• D = D′ ⊓D′′, and C ⪯⊓D′, or
• C = ∃r.C′, D = ∃r.D′ and C′ ⪯⊓D′.
We can ﬁnally capture our ideas on connection minimality formally.
Deﬁnition 7 (Connection-Minimal Abduction). Given an abduction prob-
lem ⟨T , Σ, C1 ⊑C2⟩, a hypothesis H is connection-minimal if there exist concepts
D1 and D2 built over Σ ∪NR and a mapping φ satisfying each of the following
conditions:
1. T |= C1 ⊑D1,
2. D2 is a ⪯⊓-minimal concept s.t. T |= D2 ⊑C2,
3. φ is a weak homomorphism from the tree TD2 = (V2, E2, w0, l2) to the tree
TD1 = (V1, E1, v0, l1), and
4. H = { l1(φ(w)) ⊑ l2(w) | w ∈V2 ∧T ̸|=  l1(φ(w)) ⊑ l2(w)}.
H is additionally called packed if the left-hand sides of the CIs in H cannot hold
more conjuncts than they do, which is formally stated as: for H, there is no H′
deﬁned from the same D2 and a D′
1 and φ′ s.t. there is a node w ∈V2 for which
l1(φ(w)) ⊊l′
1(φ′(w)) and l1(φ(w′)) = l′
1(φ′(w′)) for w′ ̸= w.
Straightforward consequences of Deﬁnition 7 include that φ is a (T ∪H)-
homomorphism from TD2 to TD1 and that D1 and D2 are connecting con-
cepts from C1 to C2 in T ∪H so that T ∪H |= C1 ⊑C2 as wanted [16].
With the help of Fig. 1 and Example 5, one easily establishes that hypothe-
sis Ha1 is connection-minimal—and even packed. Connection-minimality rejects
Ha2, as a single T ′-homomorphism for some T ′ between two concepts D1 and
D2 would be insuﬃcient: we would need two weak homomorphisms, one link-
ing Professor to FundsProvider and another linking ∃writes.GrantApplication to
∃writes.ResearchPaper.
4
Computing Connection-Minimal Hypotheses Using
Prime Implicates
To compute connection-minimal hypotheses in practice, we propose a method
based on ﬁrst-order prime implicates, that can be derived by resolution. We

196
F. Haifani et al.
C1 ⊑C2
T
translation
Φ
PI generation
Σ
PIg+
Σ (Φ)
PIg−
Σ (Φ)
recombination
S
Fig. 2. EL abduction using prime implicate generation in FOL.
assume the reader is familiar with the basics of ﬁrst-order resolution, and do not
reintroduce notions of clauses, Skolemization and resolution inferences here (for
details, see [4]). In our context, every term is built on variables, denoted x, y,
a single constant sk0 and unary Skolem functions usually denoted sk, possibly
annotated. Prime implicates are deﬁned as follows.
Deﬁnition 8 (Prime Implicate). Let Φ be a set of clauses. A clause ϕ is an
implicate of Φ if Φ |= ϕ. Moreover ϕ is prime if for any other implicate ϕ′ of Φ
s.t. ϕ′ |= ϕ, it also holds that ϕ |= ϕ′.
Let Σ ⊆NC be a set of unary predicates. Then PIg+
Σ (Φ) denotes the set of
all positive ground prime implicates of Φ that only use predicate symbols from
Σ ∪NR, while PIg−
Σ (Φ) denotes the set of all negative ground prime implicates
of Φ that only use predicates symbols from Σ ∪NR.
Example 9. Given a set of clauses Φ = {A1(sk0), ¬B1(sk0), ¬A1(x)∨r(x, sk(x)),
¬A1(x) ∨A2(sk(x)), ¬B2(x) ∨¬r(x, y) ∨¬B3(y) ∨B1(x)}, the ground prime
implicates of Φ for Σ = NC are, on the positive side, PIg+
Σ (Φ) = {A1(sk0),
A2(sk(sk0)), r(sk0, sk(sk0))} and, on the negative side, PIg−
Σ (Φ) = {¬B1(sk0),
¬B2(sk0)∨¬B3(sk(sk0))}. They are implicates because all of them are entailed
by Φ. For a ground implicate ϕ, another ground implicate ϕ′ such that ϕ′ |= ϕ
and ϕ ̸|= ϕ′ can only be obtained from ϕ by dropping literals. Such an operation
does not produce another implicate for any of the clauses presented above as
belonging to PIg+
Σ (Φ)and PIg−
Σ (Φ), thus they really are all prime.
To generate hypotheses, we translate the abduction problem into a set of ﬁrst-
order clauses, from which we can infer prime implicates that we then combine to
obtain the result as illustrated in Fig. 2. In more details: We ﬁrst translate the
problem into a set Φ of Horn clauses. Prime implicates can be computed using an
oﬀ-the-shelf tool [13,28] or, in our case, a slight extension of the resolution-based
version of the SPASS theorem prover [34] using the set-of-support strategy and
some added features described in Sect. 5. Since Φ is Horn, PIg+
Σ (Φ) contains
only unit clauses. A ﬁnal recombination step looks at the clauses in PIg−
Σ (Φ)
one after the other. These correspond to candidates for the connecting concepts
D2 of Deﬁnition 7. Recombination attempts to match each literal in one such
clause with unit clauses from PIg+
Σ (Φ). If such a match is possible, it produces a

Abduction in EL via FOL
197
suitable D1 to match D2, and allows the creation of a solution to the abduction
problem. The set S contains all the hypotheses thus obtained.
In what follows, we present our translation of abduction problems into ﬁrst-
order logic and formalize the construction of hypotheses from the prime impli-
cates of this translation. We then show how to obtain termination for the prime
implicate generation process with soundness and completeness guarantees on the
solutions computed.
Abduction Method. We assume the EL TBox in the input is in normal form
as deﬁned, e.g., by Baader et al. [2]. Thus every CI is of one of the following
forms:
A ⊑B
A1 ⊓A2 ⊑B
∃r.A ⊑B
A ⊑∃r.B
where A, A1, A2, B ∈NC ∪{⊤}.
The use of normalization is justiﬁed by the following lemma.
Lemma 10. For every EL TBox T , we can compute in polynomial time an EL
TBox T ′ in normal form such that for every other TBox H and every CI C ⊑D
that use only names occurring in T , we have T ∪H |= C ⊑D iﬀT ′∪H |= C ⊑D.
After the normalisation, we eliminate occurrences of ⊤, replacing this concept
everywhere by the fresh atomic concept A⊤. We furthermore add ∃r.A⊤⊑A⊤
and B ⊑A⊤in T for every role r and atomic concept B occurring in T . This
simulates the semantics of ⊤for A⊤, namely the implicit property that C ⊑
⊤holds for any C no matter what the TBox is. In particular, this ensures
that whenever there is a positive prime implicate B(t) or r(t, t′), A⊤(t) also
becomes a prime implicate. Note that normalisation and ⊤elimination extend
the signature, and thus potentially the solution space of the abduction problem.
This is remedied by intersecting the set of abducible predicates Σ with the
signature of the original input ontology. We assume that T is in normal form
and without ⊤in the rest of the paper.
We denote by T −the result of renaming all atomic concepts A in T using
fresh duplicate symbols A−. This renaming is done only on concepts but not on
roles, and on C2 but not on C1 in the observation. This ensures that the literals
in a clause of PIg−
Σ (Φ) all relate to the conjuncts of a ⪯⊓-minimal subsumee of
C2. Without it, some of these conjuncts would not appear in the negative impli-
cates due to the presence of their positive counterparts as atoms in PIg+
Σ (Φ).
The translation of the abduction problem ⟨T , Σ, C1 ⊑C2⟩is deﬁned as the
Skolemization of
π(T ⊎T −) ∧¬π(C1 ⊑C−
2 )
where sk0 is used as the unique fresh Skolem constant such that the Skolemiza-
tion of ¬π(C1 ⊑C−
2 ) results in {C1(sk0), ¬C−
2 (sk0)}. This translation is usually
denoted Φ and always considered in clausal normal form.
Theorem 11. Let ⟨T , Σ, C1 ⊑C2⟩be an abduction problem and Φ be its ﬁrst-
order translation. Then, a TBox H′ is a packed connection-minimal solution to
the problem if and only if an equivalent hypothesis H can be constructed from
non-empty sets A and B of atoms verifying:

198
F. Haifani et al.
• B = {B1(t1), . . . , Bm(tm)} s.t.

¬B−
1 (t1) ∨· · · ∨¬B−
m(tm)

∈PIg−
Σ (Φ),
• for all t ∈{t1, . . . , tm} there exists an A s.t. A(t) ∈PIg+
Σ (Φ),
• A = {A(t) ∈PIg+
Σ (Φ) | t is one of t1, . . . , tm}, and
• H = {CA,t ⊑CB,t | t is one of t1, . . . , tm and CB,t ̸⪯⊓CA,t}, where CA,t =

A(t)∈A A and CB,t = 
B(t)∈B B.
We call the hypotheses that are constructed as in Theorem 11 constructible. This
theorem states that every packed connection-minimal hypothesis is equivalent
to a constructible hypothesis and vice versa. A constructible hypothesis is built
from the concepts in one negative prime implicate in PIg−
Σ (Φ) and all matching
concepts from prime implicates in PIg+
Σ (Φ). The matching itself is determined by
the Skolem terms that occur in all these clauses. The subterm relation between
the terms of the clauses in PIg+
Σ (Φ) and PIg−
Σ (Φ) is the same as the ancestor
relation in the description trees of subsumers of C1 and subsumees of C2 respec-
tively. The terms matching in positive and negative prime implicates allow us
to identify where the missing entailments between a subsumer D1 of C1 and a
subsumee D2 of C2 are. These missing entailments become the constructible H.
The condition CB,t ̸⪯⊓CA,t is a way to write that CA,t ⊑CB,t is not a tautology,
which can be tested by subset inclusion.
The formal proof of this result is detailed in the technical report [16]. We
sketch it brieﬂy here. To start, we link the subsumers of C1 with PIg+
Σ (Φ). This
is done at the semantics level: We show that all Herbrand models of Φ, i.e.,
models built on the symbols in Φ, are also models of PIg+
Σ (Φ), that is itself such
a model. Then we show that C1(sk0) as well as the formulas corresponding to
the subsumers of C1 in our translation are satisﬁed by all Herbrand models. This
follows from the fact that Φ is in fact a set of Horn clauses. Next, we show, using
a similar technique, how duplicate negative ground implicates, not necessarily
prime, relate to subsumees of C2, with the restriction that there must exist a
weak homomorphism from a description tree of a subsumer of C1 to a description
tree of the considered subsumee of C2. Thus, H provides the missing CIs that
will turn the weak homomorphism into a (T ∪H)-homomorphism. Then, we
establish an equivalence between the ⪯⊓-minimality of the subsumee of C2 and
the primality of the corresponding negative implicate. Packability is the last
aspect we deal with, whose use is purely limited to the reconstruction. It holds
because A contains all A(t) ∈PIg+
Σ (Φ) for all terms t occurring in B.
Example 12. Consider the abduction problem ⟨Ta, Σ, αa⟩where Σ contains all
concepts from Ta. For the translation Φ of this problem, we have
PIg+
Σ (Φ) = { Professor(sk0), Doctor(sk0), Chair(sk1(sk0)), PhD(sk2(sk0))}
PIg−
Σ (Φ) = { ¬Researcher−(sk0),
¬ResearchPosition−(sk1(sk0)) ∨¬Diploma−(sk2(sk0))}
where sk1 is the Skolem function introduced for Professor ⊑∃employment.Chair
and sk2 is introduced for Doctor ⊑∃qualiﬁcation.PhD. This leads to two con-
structible solutions: {Professor ⊓Doctor ⊑Researcher} and Ha1, that are both

Abduction in EL via FOL
199
packed connection-minimal hypotheses if Σ = NC. Another example is presented
in full details in the technical report [16].
Termination. If T contains cycles, there can be inﬁnitely many prime impli-
cates. For example, for T = {C1 ⊑A, A ⊑∃r.A, ∃r.B ⊑B, B ⊑C2} both the
positive and negative ground prime implicates of Φ are unbounded even though
the set of constructible hypotheses is ﬁnite (as it is for any abduction problem):
PIg+
Σ (Φ) = {C1(sk0), A(sk0), A(sk(sk0)), A(sk(sk(sk0))), . . .},
PIg−
Σ (Φ) = {¬C−
2 (sk0), ¬B−(sk0), ¬B−(sk(sk0)), . . .}.
To ﬁnd all constructible hypotheses of an abduction problem, an approach that
simply computes all prime implicates of Φ, e.g., using the standard resolution
calculus, will never terminate on cyclic problems. However, if we look only
for subset-minimal constructible hypotheses, termination can be achieved for
cyclic and non-cyclic problems alike, because it is possible to construct all such
hypotheses from prime implicates that have a polynomially bounded term depth,
as shown below. To obtain this bound, we consider resolution derivations of the
ground prime implicates and we show that they can be done under some restric-
tions that imply this bound.
Before performing resolution, we compute the presaturation Φp of the set of
clauses Φ, deﬁned as
Φp = Φ ∪{¬A(x) ∨B(x) | Φ |= ¬A(x) ∨B(x)}
where A and B are either both original or both duplicate atomic concepts. The
presaturation can be eﬃciently computed before the translation, using a modern
EL reasoner such as Elk [23], which is highly optimized towards the computation
of all entailments of the form A ⊑B. While the presaturation computes nothing
a resolution procedure could not derive, it is what allows us to bind the maximal
depth of terms in inferences to that in prime implicates. If Φp is presaturated,
we do not need to perform inferences that produce Skolem terms of a higher
nesting depth than what is needed for the prime implicates.
Starting from the presaturated set Φp, we can show that all the relevant
prime implicates can be computed if we restrict all inferences to those where
R1 at least one premise contains a ground term,
R2 the resolvent contains at most one variable, and
R3 every literal in the resolvent contains Skolem terms of nesting depth at most
n×m, where n is the number of atomic concepts in Φ, and m is the number
of occurrences of existential role restrictions in T .
The ﬁrst restriction turns the derivation of PIg+
Σ (Φ) and PIg−
Σ (Φ) into an SOS-
resolution derivation [18] with set of support {C1(sk0), C−
2 (sk0)}, i.e., the only
two clauses with ground terms in Φ. This restriction is a straightforward conse-
quence of our interest in computing only ground implicates, and of the fact that
the non-ground clauses in Φ cannot entail the empty clause since every EL TBox
is consistent. The other restrictions are consequences of the following theorems,
whose proofs are available in the technical report [16].

200
F. Haifani et al.
Theorem 13. Given an abduction problem and its translation Φ, every con-
structible hypothesis can be built from prime implicates that are inferred under
restriction 4.
In fact, for PIg+
Σ (Φ) it is even possible to restrict inferences to generating only
ground resolvents, as can be seen in the proof of Theorem 13, that directly looks
at the kinds of clauses that are derivable by resolution from Φ.
Theorem 14. Given an abduction problem and its translation Φ, every subset-
minimal constructible hypothesis can be built from prime implicates that have a
nesting depth of at most n × m, where n is the number of atomic concepts in Φ,
and m is the number of occurrences of existential role restrictions in T .
The proof of Theorem 14 is based on a structure called a solution tree, which
resembles a description tree, but with multiple labeling functions. It assigns to
each node a Skolem term, a set of atomic concepts called positive label, and a
single atomic concept called negative label. The nodes correspond to matching
partners in a constructible hypothesis: The Skolem term is the term on which
we match literals. The positive label collects the atomic concepts in the positive
prime implicates containing that term. The maximal anti-chains of the tree,
i.e., the maximal subsets of nodes s.t. no node is the ancestor of another are
such that their negative labels correspond to the literals in a derivable negative
implicate. For every solution tree, the Skolem labels and negative labels of the
leaves determine a negative prime implicate, and by combining the positive and
negative labels of these leaves, we obtain a constructible hypothesis, called the
solution of the tree. We show that from every solution tree with solution H we
can obtain a solution tree with solution H′ ⊆H s.t. on no path, there are two
nodes that agree both on the head of their Skolem labeling and on the negative
label. Furthermore the number of head functions of Skolem labels is bounded by
the total number n of Skolem functions, while the number of distinct negative
labels is bounded by the number m of atomic concepts, bounding the depth of
the solution tree for H′ at n × m. This justiﬁes the bound in Theorem 14. This
bound is rather loose. For the academia example, it is equal to 22 × 6 = 132.
5
Implementation
We implemented our method to compute all subset-minimal constructible
hypotheses in the tool CAPI.3 To compute the prime implicates, we used SPASS
[34], a ﬁrst-order theorem prover that includes resolution among other calculi.
We implemented everything before and after the prime implicate computation in
Java, including the parsing of ontologies, preprocessing (detailed below), clausiﬁ-
cation of the abduction problems, translation to SPASS input, as well as the pars-
ing and processing of the output of SPASS to build the constructible hypotheses
and ﬁlter out the non-subset-minimal ones. On the Java side, we used the OWL
API for all DL-related functionalities [20], and the EL reasoner Elk for comput-
ing the presaturations [23].
3 available under https://lat.inf.tu-dresden.de/∼koopmann/CAPI.

Abduction in EL via FOL
201
Preprocessing. Since realistic TBoxes can be too large to be processed by SPASS,
we replace the background knowledge in the abduction problem by a subset of
axioms relevant to the abduction problem. Speciﬁcally, we replace the abduction
problem (T , Σ, C1 ⊑C2) by the abduction problem (M⊥
C1 ∪M⊤
C2, Σ, C1 ⊑C2),
where M⊥
C1 is the ⊥-module of T for the signature of C1, and M⊤
C2 is the ⊤-
module of T for the signature of C2 [15]. Those notions are explained in the
technical report [16]. Their relevant properties are that M⊥
C1 is a subset of T
s.t. M⊥
C1 |= C1 ⊑D iﬀT |= C1 ⊑D for all concepts D, while M⊤
C2 is a subset
of T that ensures M⊤
C2 |= D ⊑C2 iﬀT |= D ⊑C2 for all concepts D. It
immediately follows that every connection-minimal hypothesis for the original
problem (T , Σ, C1 ⊑C2) is also a connection-minimal hypothesis for (M⊥
C1 ∪
M⊤
C2, Σ, C1 ⊑C2). For the presaturation, we compute with Elk all CIs of the
form A ⊑B s.t. M⊥
C1 ∪M⊤
C2 |= A ⊑B.
Prime implicates generation. We rely on a slightly modiﬁed version of SPASS
v3.9 to compute all ground prime implicates. In particular, we added the possi-
bility to limit the number of variables allowed in the resolvents to enforce R2.
For each of the restrictions R1–R3 there is a corresponding ﬂag (or set of ﬂags)
that is passed to SPASS as an argument.
Recombination. The construction of hypotheses from the prime implicates found
in the previous stage starts with a straightforward process of matching negative
prime implicates with a set of positive ones based on their Skolem terms. It is
followed by subset minimality tests to discard non-subset-minimal hypotheses,
since, with the bound we enforce, there is no guarantee that these are valid
constructible hypotheses because the negative ground implicates they are built
upon may not be prime. If SPASS terminates due to a timeout instead of reaching
the bound, then it is possible that some subset-minimal constructible hypotheses
are not found, and thus, some non-constructible hypotheses may be kept. Note
that these are in any case solutions to the abduction problem.
6
Experiments
There is no benchmark suite dedicated to TBox abduction in EL, so we created
our own, using realistic ontologies from the bio-medical domain. For this, we used
ontologies from the 2017 snapshot of Bioportal [27]. We restricted each ontol-
ogy to its EL fragment by ﬁltering out unsupported axioms, where we replaced
domain axioms and n-ary equivalence axioms in the usual way [2]. Note that,
even if the ontology contains more expressive axioms, an EL hypothesis is still
useful if found. From the resulting set of TBoxes, we selected those contain-
ing at least 1 and at most 50,000 axioms, resulting in a set of 387 EL TBoxes.
Precisely, they contained between 2 and 46,429 axioms, for an average of 3,039
and a median of 569. Towards obtaining realistic benchmarks, we created three
diﬀerent categories of abduction problems for each ontology T , where in each
case, we used the signature of the entire ontology for Σ.

202
F. Haifani et al.
• Problems in ORIGIN use T as background knowledge, and as observation a
randomly chosen A ⊑B s.t. A and B are in the signature of T and T ̸|= A ⊑
B. This covers the basic requirements of an abduction problem, but has the
disadvantage that A and B can be completely unrelated in T .
• Problems in JUSTIF contain as observation a randomly selected CI α s.t., for
the original TBox, T |= α and α ̸∈T . The background knowledge used is a
justiﬁcation for α in T [32], that is, a minimal subset I ⊆T s.t. I ̸|= α, from
which a randomly selected axiom is removed. The TBox is thus a smaller set
of axioms extracted from a real ontology for which we know there is a way of
producing the required entailment without adding it explicitly. Justiﬁcations
were computed using functionalities of the OWL API and Elk.
• Problems in REPAIR contain as observation a randomly selected CI α s.t.
T |= α, and as background knowledge a repair for α in T , which is a maximal
subset R ⊆T s.t. R ̸|= α. Repairs were computed using a justiﬁcation-
based algorithm [32] with justiﬁcations computed as for JUSTIF. This usually
resulted in much larger TBoxes, where more axioms would be needed to
establish the entailment.
All experiments were run on Debian Linux (Intel Core i5-4590, 3.30 GHz,
23 GB Java heap size). The code and scripts used in the experiments are available
online [17]. The three phases of the method (see Fig. 2) were each assigned a
hard time limit of 90 s.
For each ontology, we attempted to create and translate 5 abduction prob-
lems of each category. This failed on some ontologies because either there was
no corresponding entailment (25/28/25 failures out of the 387 ontologies for
ORIGIN/JUSTIF/REPAIR), there was a timeout during the translation (5/5/5
failures for ORIGIN/JUSTIF/REPAIR), or because the computation of justiﬁca-
tions caused an exception (-/2/0 failures for ORIGIN/JUSTIF/REPAIR). The ﬁnal
number of abduction problems for each category is in the ﬁrst column of Table 1.
We then attempted to compute prime implicates for these benchmarks using
SPASS. In addition to the hard time limit, we gave a soft time limit of 30 s to
SPASS, after which it should stop exploring the search space and return the
implicates already found. In Table 1 we show, for each category, the percentage
of problems on which SPASS succeeded in computing a non-empty set of clauses
(Success) and the percentage of problems on which SPASS terminated within the
time limit, where all solutions are computed (Compl.). The high number of CIs
in the background knowledge explains most of the cases where SPASS reached
the soft time limit. In a lot of these cases, the bound on the term depth goes
into the billion, rendering it useless in practice. However, the “Compl.” column
shows that the bound is reached before the soft time limit in most cases.
The reconstruction never reached the hard time limit. We measured the
median, average and maximal number of solutions found (#H), size of solu-
tions in number of CIs (|H|), size of CIs from solutions in number of atomic
concepts (|α|), and SPASS runtime (time, in seconds), all reported in Table 1.
Except for the simple JUSTIF problems, the number of solutions may become
very large. At the same time, solutions always contain very few axioms (never

Abduction in EL via FOL
203
Table 1. Evaluation results.
Median / avg / max
#Probl. Success Compl. #H
|H|
|α|
Time (s.)
ORIGIN 1,925
94.7%
61.3%
1/8.51/1850
1/1.00/2 6/7.48/91 0.2/12.4/43.8
JUSTIF 1,803
100.0%
97.2%
1/1.50/5
1/1/1
2/4.21/32 0.2/1.1/34.1
REPAIR 1,805
92.9%
57.0%
43/228.05/6317 1/1.00/2 5/5.09/49 0.6/13.6/59.9
more than 3), though the axioms become large too. We also noticed that highly
nested Skolem terms rarely lead to more hypotheses being found: 8/1/15 for
ORIGIN/JUSTIF/REPAIR, and the largest nesting depth used was: 3/1/2 for
ORIGIN/JUSTIF/REPAIR. This hints at the fact that longer time limits would not
have produced more solutions, and motivates future research into redundancy
criteria to stop derivations (much) earlier.
7
Conclusion
We have introduced connection-minimal TBox abduction for EL which ﬁnds
parsimonious hypotheses, ruling out the ones that entail the observation in an
arbitrary fashion. We have established a formal link between the generation of
connection-minimal hypotheses in EL and the generation of prime implicates of
a translation Φ of the problem to ﬁrst-order logic. In addition to obtaining these
theoretical results, we developed a prototype for the computation of subset-
minimal constructible hypotheses, a subclass of connection-minimal hypotheses
that is easy to construct from the prime implicates of Φ. Our prototype uses
the SPASS theorem prover as an SOS-resolution engine to generate the needed
implicates. We tested this tool on a set of realistic medical ontologies, and the
results indicate that the cost of computing connection-minimal hypotheses is
high but not prohibitive.
We see several ways to improve our technique. The bound we computed to
ensure termination could be advantageously replaced by a redundancy criterion
discarding irrelevant implicates long before it is reached, thus greatly speeding
computation in SPASS. We believe it should also be possible to further constrain
inferences, e.g., to have them produce ground clauses only, or to generate the
prime implicates with terms of increasing depth in a controlled incremental way
instead of enforcing the soft time limit, but these two ideas remain to be proved
feasible. As an alternative to using prime implicates, one may investigate direct
method for computing connection-minimal hypotheses in EL.
The theoretical worst-case complexity of connection-minimal abduction is
another open question. Our method only gives a very high upper bound: by
bounding only the nesting dept of Skolem terms polynomially as we did with
Theorem 13, we may still permit clauses with exponentially many literals, and
thus double exponentially many clauses in the worst case, which would give us
an 2ExpTime upper bound to the problem of computing all subset-minimal con-
structible hypotheses. Using structure-sharing and guessing, it is likely possible

204
F. Haifani et al.
to get a lower bound. We have not looked yet at lower bounds for the complexity
either.
While this work focuses on abduction problems where the observation is a CI,
we believe that our technique can be generalised to knowledge that also contains
ground facts (ABoxes), and to observations that are of the form of conjunctive
queries on the ABoxes in such knowledge bases. The motivation for such an
extension is to understand why a particular query does not return any results,
and to compute a set of TBox axioms that ﬁx this problem. Since our translation
already transforms the observation into ground facts, it should be possible to
extend it to this setting. We would also like to generalize TBox abduction by
ﬁnding a reasonable way to allow role restrictions in the hypotheses, and to
extend connection-minimality to more expressive DLs such as ALC.
Acknowledgments. This work was supported by the Deutsche Forschungsgemein-
schaft (DFG), Grant 389792660 within TRR 248.
References
1. Baader, F., Brandt, S., Lutz, C.: Pushing the EL envelope. In: Kaelbling, L.P.,
Saﬃotti, A. (eds.) IJCAI-05, Proceedings of the Nineteenth International Joint
Conference on Artiﬁcial Intelligence, Edinburgh, Scotland, UK, 30 July - 5 August
2005, pp. 364–369. Professional Book Center (2005). http://ijcai.org/Proceedings/
05/Papers/0372.pdf
2. Baader, F., Horrocks, I., Lutz, C., Sattler, U.: An Introduction to Description
Logic. Cambridge University Press, Cambridge (2017). https://doi.org/10.1017/
9781139025355
3. Baader, F., K¨usters, R., Molitor, R.: Computing least common subsumers in
description logics with existential restrictions. In: Proceedings of IJCAI 1999, pp.
96–103. Morgan Kaufmann (1999)
4. Bachmair, L., Ganzinger, H.: Resolution theorem proving. In: Robinson, J.A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning (in 2 volumes), pp. 19–
99. Elsevier and MIT Press, Cambridge (2001). https://doi.org/10.1016/b978-
044450813-3/50004-7
5. Bauer, J., Sattler, U., Parsia, B.: Explaining by example: model exploration for
ontology comprehension. In: Grau, B.C., Horrocks, I., Motik, B., Sattler, U. (eds.)
Proceedings of the 22nd International Workshop on Description Logics (DL 2009),
Oxford, UK, 27–30 July 2009. CEUR Workshop Proceedings, vol. 477. CEUR-
WS.org (2009). http://ceur-ws.org/Vol-477/paper 37.pdf
6. Bienvenu, M.: Complexity of abduction in the EL family of lightweight description
logics. In: Proceedings of KR 2008, pp. 220–230. AAAI Press (2008), http://www.
aaai.org/Library/KR/2008/kr08-022.php
7. Calvanese, D., Ortiz, M., Simkus, M., Stefanoni, G.: Reasoning about explanations
for negative query answers in DL-Lite. J. Artif. Intell. Res. 48, 635–669 (2013).
https://doi.org/10.1613/jair.3870
8. Ceylan, ˙I.˙I., Lukasiewicz, T., Malizia, E., Molinaro, C., Vaicenavicius, A.: Explana-
tions for negative query answers under existential rules. In: Calvanese, D., Erdem,
E., Thielscher, M. (eds.) Proceedings of KR 2020, pp. 223–232. AAAI Press (2020).
https://doi.org/10.24963/kr.2020/23

Abduction in EL via FOL
205
9. Del-Pinto, W., Schmidt, R.A.: ABox abduction via forgetting in ALC. In: The
Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019, pp. 2768–
2775. AAAI Press (2019). https://doi.org/10.1609/aaai.v33i01.33012768
10. Du, J., Qi, G., Shen, Y., Pan, J.Z.: Towards practical ABox abduction in large
description logic ontologies. Int. J. Semantic Web Inf. Syst. 8(2), 1–33 (2012).
https://doi.org/10.4018/jswis.2012040101
11. Du, J., Wan, H., Ma, H.: Practical TBox abduction based on justiﬁcation pat-
terns. In: Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intel-
ligence, pp. 1100–1106 (2017). http://aaai.org/ocs/index.php/AAAI/AAAI17/
paper/view/14402
12. Du, J., Wang, K., Shen, Y.: A tractable approach to ABox abduction over descrip-
tion logic ontologies. In: Brodley, C.E., Stone, P. (eds.) Proceedings of the Twenty-
Eighth AAAI Conference on Artiﬁcial Intelligence, pp. 1034–1040. AAAI Press
(2014). http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8191
13. Echenim, M., Peltier, N., Sellami, Y.: A generic framework for implicate generation
modulo theories. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018.
LNCS (LNAI), vol. 10900, pp. 279–294. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-94205-6 19
14. Elsenbroich, C., Kutz, O., Sattler, U.: A case for abductive reasoning over ontolo-
gies. In: Proceedings of the OWLED’06 Workshop on OWL: Experiences and Direc-
tions (2006). http://ceur-ws.org/Vol-216/submission 25.pdf
15. Grau, B.C., Horrocks, I., Kazakov, Y., Sattler, U.: Modular reuse of ontologies:
theory and practice. J. Artif. Intell. Res. 31, 273–318 (2008). https://doi.org/10.
1613/jair.2375
16. Haifani, F., Koopmann, P., Tourret, S., Weidenbach, C.: Connection-minimal
abduction in EL via translation to FOL - technical report (2022). https://doi.
org/10.48550/ARXIV.2205.08449, https://arxiv.org/abs/2205.08449
17. Haifani, F., Koopmann, P., Tourret, S., Weidenbach, C.: Experiment data for the
paper Connection-minimal Abduction in EL via translation to FOL, May 2022.
https://doi.org/10.5281/zenodo.6563656
18. Haifani, F., Tourret, S., Weidenbach, C.: Generalized completeness for SOS reso-
lution and its application to a new notion of relevance. In: Platzer, A., Sutcliﬀe,
G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 327–343. Springer, Cham
(2021). https://doi.org/10.1007/978-3-030-79876-5 19
19. Halland, K., Britz, K.: ABox abduction in ALC using a DL tableau. In: 2012 South
African Institute of Computer Scientists and Information Technologists Confer-
ence, SAICSIT ’12, pp. 51–58 (2012). https://doi.org/10.1145/2389836.2389843
20. Horridge, M., Bechhofer, S.: The OWL API: a java API for OWL ontologies.
Semant. Web 2(1), 11–21 (2011). https://doi.org/10.3233/SW-2011-0025
21. Horridge, M., Parsia, B., Sattler, U.: Explanation of OWL entailments in protege
4. In: Bizer, C., Joshi, A. (eds.) Proceedings of the Poster and Demonstration
Session at the 7th International Semantic Web Conference (ISWC2008), Karlsruhe,
Germany, 28 October 2008. CEUR Workshop Proceedings, vol. 401. CEUR-WS.org
(2008). http://ceur-ws.org/Vol-401/iswc2008pd submission 47.pdf
22. Kazakov, Y., Klinov, P., Stupnikov, A.: Towards reusable explanation services in
protege. In: Artale, A., Glimm, B., Kontchakov, R. (eds.) Proceedings of the 30th
International Workshop on Description Logics, Montpellier, France, 18–21 July
2017. CEUR Workshop Proceedings, vol. 1879. CEUR-WS.org (2017). http://ceur-
ws.org/Vol-1879/paper31.pdf

206
F. Haifani et al.
23. Kazakov, Y., Kr¨otzsch, M., Simancik, F.: The incredible ELK - from polynomial
procedures to eﬃcient reasoning with EL ontologies. J. Autom. Reason. 53(1),
1–61 (2014). https://doi.org/10.1007/s10817-013-9296-3
24. Klarman, S., Endriss, U., Schlobach, S.: ABox abduction in the description logic
ALC. J. Autom. Reason. 46(1), 43–80 (2011). https://doi.org/10.1007/s10817-010-
9168-z
25. Koopmann, P.: Signature-based abduction with fresh individuals and complex
concepts for description logics. In: Zhou, Z. (ed.) Proceedings of the Thirti-
eth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2021, Virtual
Event/Montreal, Canada, 19–27 August 2021, pp. 1929–1935 (2021). https://doi.
org/10.24963/ijcai.2021/266
26. Koopmann, P., Del-Pinto, W., Tourret, S., Schmidt, R.A.: Signature-based abduc-
tion for expressive description logics. In: Calvanese, D., Erdem, E., Thielscher, M.
(eds.) Proceedings of the 17th International Conference on Principles of Knowl-
edge Representation and Reasoning, KR 2020, pp. 592–602. AAAI Press (2020).
https://doi.org/10.24963/kr.2020/59
27. Matentzoglu, N., Parsia, B.: Bioportal snapshot 30.03.2017 (2017). https://doi.
org/10.5281/zenodo.439510
28. Nabeshima, H., Iwanuma, K., Inoue, K., Ray, O.: SOLAR: an automated deduction
system for consequence ﬁnding. AI Commun. 23(2–3), 183–203 (2010). https://doi.
org/10.3233/AIC-2010-0465
29. Parsia, B., Matentzoglu, N., Gon¸calves, R.S., Glimm, B., Steigmiller, A.: The owl
reasoner evaluation (ORE) 2015 competition report. J. Autom. Reason. 59(4),
455–482 (2017). https://doi.org/10.1007/s10817-017-9406-8
30. Pukancov´a, J., Homola, M.: Tableau-based ABox abduction for the ALCHO
description logic. In: Proceedings of the 30th International Workshop on Descrip-
tion Logics (2017). http://ceur-ws.org/Vol-1879/paper11.pdf
31. Pukancov´a, J., Homola, M.: The AAA Abox abduction solver. KI - K¨unstliche
Intell. 34(4), 517–522 (2020). https://doi.org/10.1007/s13218-020-00685-4
32. Schlobach, S., Cornet, R.: Non-standard reasoning services for the debugging
of description logic terminologies. In: Gottlob, G., Walsh, T. (eds.) Proceed-
ings of the 18th International Joint Conference on Artiﬁcial Intelligence (IJCAI
2003), pp. 355–362. Morgan Kaufmann, Acapulco, Mexico (2003). http://ijcai.
org/Proceedings/03/Papers/053.pdf
33. Wei-Kleiner, F., Dragisic, Z., Lambrix, P.: Abduction framework for repairing
incomplete EL ontologies: complexity results and algorithms. In: Proceedings of
the Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence, pp. 1120–1127.
AAAI Press (2014). http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/
view/8239
34. Weidenbach, C., Schmidt, R.A., Hillenbrand, T., Rusev, R., Topic, D.: System
description: Spass version 3.0. In: Pfenning, F. (ed.) CADE 2007. LNCS (LNAI),
vol. 4603, pp. 514–520. Springer, Heidelberg (2007). https://doi.org/10.1007/978-
3-540-73595-3 38
35. Wos, L., Robinson, G., Carson, D.: Eﬃciency and completeness of the set of support
strategy in theorem proving. J. ACM 12(4), 536–541 (1965)

Abduction in EL via FOL
207
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Semantic Relevance
Fajar Haifani1,2
and Christoph Weidenbach1(B)
1 Max Planck Institute for Informatics, Saarland Informatics Campus,
Saarbr¨ucken, Germany
{f.haifani,weidenbach}@mpi-inf.mpg.de
2 Graduate School of Computer Science, Saarbr¨ucken, Germany
Abstract. A clause C is syntactically relevant in some clause set N,
if it occurs in every refutation of N. A clause C is syntactically semi-
relevant, if it occurs in some refutation of N. While syntactic relevance
coincides with satisﬁability (if C is syntactically relevant then N \ {C}
is satisﬁable), the semantic counterpart for syntactic semi-relevance was
not known so far. Using the new notion of a conﬂict literal we show that
for independent clause sets N a clause C is syntactically semi-relevant
in the clause set N if and only if it adds to the number of conﬂict literals
in N. A clause set is independent, if no clause out of the clause set is the
consequence of diﬀerent clauses from the clause set.
Furthermore, we relate the notion of relevance to that of a minimally
unsatisﬁable subset (MUS) of some independent clause set N. In proposi-
tional logic, a clause C is relevant if it occurs in all MUSes of some clause
set N and semi-relevant if it occurs in some MUS. For ﬁrst-order logic
the characterization needs to be reﬁned with respect to ground instances
of N and C.
1
Introduction
In our previous work [11], we introduced a notion of syntactic relevance based
on refutations while at the same time generalized the completeness result for
resolution by the set-of-support strategy (SOS) [28,33] as its test. Our notion of
syntactic relevance is useful for explaining why a set of clauses is unsatisﬁable.
In this paper, we introduce a semantic counterpart of syntactic relevance that
sheds further light on the relationship between a clause out of a clause set and
the potential refutations of this clause set. In the following Sect. 1.1, we ﬁrst
recall syntactic relevance along with an example and then proceeds to explain it
in terms of our new semantic relevance in the later Sect. 1.2.
1.1
Syntactic Relevance
Given an unsatisﬁable set of clauses N, C ∈N is syntactically relevant if it occurs
in all refutations, it is syntactically semi-relevant if it occurs in some refutation,
otherwise it is called syntactically irrelevant. The clause-based notion of relevance
is useful in relating the contribution of a clause to refutation (goal conjecture).
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 208–227, 2022.
https://doi.org/10.1007/978-3-031-10769-6_13

Semantic Relevance
209
This has in particular been shown in the context of product scenarios built out
of construction kits as they are used in the car industry [8,32].
For an illustration of our privous notions and results we now consider the
following unsatisﬁable ﬁrst-order clause set N where Fig. 1 presents a refutation
of N.
N = {(1)A(f(a)) ∨D(x3),
(2)¬D(x7),
(3)¬B(c,a) ∨B(b,f(x6)),
(4)B(x1,x2) ∨C(x1),
(5)¬C(x5),
(6)¬A(x4) ∨¬B(b,x4)}
(11) ⊥
(10) C(c)
(8) ¬B(b,f(a))
(6) ¬A(x4) ∨¬B(b,x4)
(7) A(f(a))
(1) A(f(a)) ∨D(x3)
(2) ¬D(x7)
(9) B(b,f(x6)) ∨C(c)
(4) B(x1,x2) ∨C(x1)
(3) ¬B(c,a) ∨B(b,f(x6))
(5) ¬C(x5)
{x4 →f(a)}
{x3 →x7}
{x6 →a}
{x1 →c, x2 →a}
{x5 →c}
Fig. 1. A refutation of N in tree representation
In essence, inferences in an SOS refutation always involve at least one
clause in the SOS and put the resulting clause back in it. So, this refu-
tation is not an SOS refutation from the syntactically semi-relevant clause
(3)¬B(c,a) ∨B(b,f(x6)), because only the shaded part represents an SOS
refutation starting with this clause. More speciﬁcally, there are two infer-
ences ended in (8)¬B(b,f(a)) which violates the condition for an SOS refu-
taiton. Nevertheless, it can be transformed into an SOS refutation where the
clause (3)¬B(c,a) ∨B(b,f(x6)) is in the SOS [11], Fig. 2. Please note that
N \ {(3)¬B(c, a) ∨B(b, f(x6))} is still unsatisﬁable and classical SOS complete-
ness [33] is not suﬃcient to guarantee the existence of a refutation with SOS
{(3)¬B(c,a) ∨B(b,f(x6))} [11].
In addition, N \ {(3)¬B(c, a) ∨B(b, f(x6))} is also a minimally unsatisﬁ-
able subset (MUS), where Fig. 3 presents a respective refutation. A MUS is an
unsatisﬁable clause set such that removing a clause from this set would ren-
der it satisﬁable. Consequently, a MUS-based deﬁned notion of semi-relevance
on the level of the original ﬁrst-order clauses is not suﬃcient here. The clause

210
F. Haifani and C. Weidenbach
(11) ⊥
(10) C(c)
(8’) D(x3) ∨C(c)
(1) A(f(a)) ∨D(x3)
(7’) ¬A(f(x6) ∨C(c))
(6) ¬A(x4) ∨¬B(b,x4)
(9) B(b,f(x6)) ∨C(c)
(4) B(x1,x2) ∨C(x1)
(3) ¬B(c,a) ∨B(b,f(x6))
(2) ¬D(x7)
(5) ¬C(x5)
{x3 →x7}
{x6 →a}
{x4 →f(x6)}
{x1 →c, x2 →a}
{x5 →c}
Fig. 2. Semi-relevant clause (3)¬B(c, a) ∨B(b, f(x6)) in SOS
(3)¬B(c, a) ∨B(b, f(x6)) should not be disregarded, because it leads to a dif-
ferent grounding of the clauses. For example, in the refutation of Fig. 2 clause
(5)¬C(x5) is necessarily instantiated with {x5 →c} where in the refutation of
Fig. 3 it is necessarily instantiated with {x5 →b}. Therefore, the two refutations
are diﬀerent and clause (3)¬B(c, a) ∨B(b, f(x6)) should be considered semi-
relevant. Nevertheless, in propositional logic it is suﬃcient to consider MUSes
to explain unsatisﬁability on the original clause level, Lemma 18.
(11) ⊥
(14) C(b)
(13) ¬B(b,f(a))
(12) D(x3) ∨¬B(b,f(a))
(6) ¬A(x4) ∨¬B(b,x4)
(1) A(f(a)) ∨D(x3)
(2) ¬D(x7)
(4) B(x1,x2) ∨C(x1)
(5) ¬C(x5)
{x3 →x7}
{x4 →f(a)}
{x1 →b, x2 →f(a)}
{x5 →b}
Fig. 3. A refutation of N without (3)¬B(c, a) ∨B(b, f(x6))
1.2
Semantic Relevance
We now illustrate how our new notion of relevance works on the previous exam-
ple. First, diﬀerent from the other works, we propose a way of characterizing
semantic relevance by using our novel concept of a conﬂict literal. A ground

Semantic Relevance
211
literal L is a conﬂict literal in a clause set N if there are some satisﬁable sets of
instances N1 and N2 from N s.t. N1 |= L and N2 |= comp(L). On the one hand,
explaining an unsatisﬁable clause set as the absence of a model (as it is usually
deﬁned) is not that helpful since an absence means there is nothing to discuss in
the ﬁrst place. On the other hand, the contribution of a clause to unsatisﬁability
of a clause set can only partially be explained using the concept of a MUS which
we have discussed before. A conﬂict literal provides a middle ground to explain
the contribution of a clause to unsatisﬁability between the absence of a model
and MUSes. It also better reﬂects our intuition that there is a contradiction (in
the form of two implied simple facts that cannot be both true at the same time)
in an unsatisﬁable set of clauses.
From Fig. 1, we can already see that C(c) and its complement ¬C(c) are
conﬂict literals because
N \ {¬C(x)} |= C(c)
¬C(x) |= ¬C(c)
Also, in addition to that {¬C(x)} is trivially satisﬁable, N \ {¬C(x)} is also
satisﬁable. Based on the refutation in Fig. 3, ¬C(x) is syntactically relevant due
to N \ {(3)¬B(c, a) ∨B(b, f(x6))} being a MUS. We will also show that for a
ground MUS any ground literal occurring in it is a conﬂict literal, Lemma 20.
For our ongoing example it is still possible to identify the conﬂict literals by
means of ground MUSes by looking into the refutations from Fig. 1 and Fig. 3.
This leads to the following conﬂict literals for N, see Deﬁnition 10:
conﬂict(N) = {(¬)A(f(a)),
(¬)B(b, f(a)), (¬)B(c, a),
(¬)C(b), (¬)C(c)}
∪
{(¬)D(t) | t is a ground term}
These conﬂict literals can be identiﬁed by pushing the substitutions in the refu-
tations from Fig. 1 and Fig. 3 towards the input clauses. They correspond to two
ﬁrst-order MUSes M1 and M2. All the ground literals are conﬂict literals and
all other ground conﬂict literals can be obtained by grounding the remaining
variables.
M1 = {(5)¬C(c), (2)¬D(x7),
(1)A(f(a)) ∨D(x3),
(3)¬B(c, a) ∨B(b, f(a)),
(4)B(c, a) ∨C(c),
(6)¬A(f((a))) ∨¬B(b, f(a))}
M2 = {(5)¬C(b),
(4)B(b, f(a)), (2)¬D(x7),
(1)A(f(a)) ∨D(x3),
(6)¬A(f(a)) ∨¬B(b, f(a))}

212
F. Haifani and C. Weidenbach
One can see that, despite (3)¬B(c, a) ∨B(b, f(x6)) is outside of the only MUS
on the ﬁrst-order level, an instance of it does occur in some ground MUS, take
M1 and an arbitrary grounding of x3 and x7 to the identical term t, and the con-
ﬂict literal (¬)B(c, a) depends on clause (3). Nevertheless, determining conﬂict
literals is not so obvious in the general case since we do not necessarily know
beforehand which ground terms should substitute the variables in the clauses.
Moreover, there can be an inﬁnite number of such ground MUSes of possibly
unbounded size.
Based on conﬂict literals, here we introduce a notion of relevance that is
semantic in nature, Deﬁnition 16. This will also serve as an alternative char-
acterization to our previous refutation-based syntactic relevance. As redundant
clauses, e.g., tautologies, can also be syntactically semi-relevant, we require inde-
pendent clause sets for the deﬁnition of semantic relevance. A clause set is inde-
pendent, if it does not contain clauses with instances implied by satisﬁable sets of
instances of diﬀerent clauses out of the set. Given an unsatisﬁable independent
set of clauses N, a clause C is relevant in N if N without C has no conﬂict
literals, it is semi-relevant if C is necessary to some conﬂict literals, and it is
irrelevant otherwise.
Similar to our previous work, relevant clauses are the obvious ones because
removing them would make our set satisﬁable. On the other hand, irrelevant
clauses can be freely identiﬁed once we know the semi-relevant ones. For our
running example, in fact (3)¬B(c, a) ∨B(b, f(x6)) is semi-relevant because it is
necessary for the conﬂict literals (¬)C(c) and (¬)B(c, a). More speciﬁcally, the
set of conﬂicts for N \ {¬B(c, a) ∨B(b, f(x6))} does not include (¬)C(c) and
(¬)B(c, a):
conﬂict(N \ {¬B(c, a) ∨B(b, f(x6))}) = {(¬)A(f(a)), (¬)B(b, f(a)), (¬)C(b)}⊎
{(¬)D(t)|t is a ground term}
These are conﬂict literals identiﬁable from M2: Assume that the variables
x3 and x7 in M2 are both grounded by an identical term t. Take some ground
literal, for example, A(f(a)) ∈conﬂict(N \ {¬B(c, a) ∨B(b, f(x6))), and deﬁne
N∅= {C ∈M2|A(f(a)) ̸∈C and ¬A(f(a)) ̸∈C}
= {(5)¬C(b), (4)B(b, f(a)), (2)¬D(t)}
NA(f(a)) = {C ∈M2|A(f(a)) ∈C}
= {(1)A(f(a)) ∨D(t)}
N¬A(f(a)) = {C ∈M2|¬A(f(a)) ∈C}
= {(6)¬A(f(a)) ∨¬B(b, f(a))}
N∅∪NA(f(a)) and N∅∪N¬A(f(a)) are satisﬁable because of the Herbrand model
{B(b, f(a)), A(f(a))} and {B(b, f(a))} respectively. In addition,
N∅∪NA(f(a)) |= A(f(a))
N∅∪N¬A(f(a)) |= ¬A(f(a))

Semantic Relevance
213
because A(f(a)) can be acquired using resolution between (1) and (2) for N∅∪
NA(f(a)) and ¬A(f(a)) can be acquired using resolution between (4) and (6) for
N∅∪N¬A(f(a)). In a similar manner, we can show that the other ground literals
are also conﬂict literals.
Related Work: Other works which aim to explain unsatisﬁability mostly rely on
the notion of MUSes, mainly in propositional logic [14–16,21,26]. The complexity
of determining whether a clause set is a MUS is Dp-complete for a propositional
clause set with at most three literals per clause and at most three occurrences
of each propositional variable [25]. In [14], syntactically semi-relevant clauses
for propositional logic are called a plain clause set. Using the terminology in
[16], a clause C ∈N is necessary if it occurs in all MUSes, it is potentially
necessary if it occurs in some MUS, otherwise, it is never necessary. In addition,
a clause is deﬁned to be usable if it occurs in some refutation. This is thus
similar to our syntactic notion of semi-relevance [11]: Given a clause C ∈N,
C is usable if-and-only-if C is syntactically semi-relevant. It is also argued that
a usable clause that is not potentially necessary is semantically superﬂuous. A
diﬀerent but related notion has also been applied for propositional abduction [7].
The notion of a MUS has also been used for explaining unsatisﬁability in ﬁrst-
order logic [20]. There, it has been deﬁned in a more general setting: If a set
of clauses N is divided into N = N ′ ⊎N ′′ with a non-relaxable clause set N ′
and relaxable clause set N ′′ (which must be satisﬁable), a MUS is a subset
M of N ′′ s.t. N ′ ⊎M is unsatisﬁable but removing a clause from M would
render it satisﬁable. There are also some works in satisﬁability modulo theory
(SMT) [5,6,9,35]. A deletion-based approach well-known in propositional logic
has also been used for MUS extraction in SMT [9]. In [5,6], a MUS is extracted by
combining an SMT solver with an arbitrary external propositional core extractor.
Another approach is to construct some graph representing the subformulas of
the problem instance, recursively remove clauses in a depth-ﬁrst-search manner
and additionally use some heuristics to further improve the runtime[35]. For
the function-free and equality-free ﬁrst-order fragment, there is a ”decompose-
merge” approach to compute all MUSes [19,34]. In description logic, a notion
that is related to MUS is called minimal axiom set (MinA) usually identiﬁed by
the problem of axiom pinpointing [1,4,13,30]. Its computation is usually divided
into two categories: black-box and white-box. A black-box approach picks some
inputs, executes it using some sound and complete reasoner, and then interprets
the output [13]. On the other hand, white-box approach takes some reasoner
and performs an internal modiﬁcation for it. In this case, Tableau is mostly
used [1,30]. In addition, the concept of a lean kernel has also been used to
approximate the union of such MinA’s [27]. The way relevance is deﬁned is similar
in spirit but usually used for an entailment problem instead of unsatisﬁability.
The notion of syntactic semi-relevance has also been applied to description logics
via a translation scheme to ﬁrst-order logic [10].
The paper is organized as follows. Section 2 ﬁxes the notations, deﬁnitions
and existing results in particular from [11]. Section 3 is reserved for our new

214
F. Haifani and C. Weidenbach
notion of semantic relevance. Finally, we conclude our work in Sect. 4 with a
discussion of our results.
2
Preliminaries
We assume a standard ﬁrst-order language without equality over a signature
Σ = (Ω, Π) where Ω is a non-empty set of functions symbols, Π a non-empty
set of predicate symbols both coming with their respective ﬁxed arities denoted
by the function arity. The set of terms over an inﬁnite set of variables X is
denoted by T(Σ, X). Atoms, literals, clauses, and clause sets are deﬁned as
usual, e.g., see [24]. We identify a clause with its multiset of literals. Variables
in clauses are universally quantiﬁed. Then N denotes a clause set; C, D denote
clauses; L, K denote literals; A, B denote atoms; P, Q, R, T denote predicates;
t, s terms; f, g, h functions; a, b, c, d constants; and x, y, z variables, all possibly
indexed. The complement of a literal is denoted by the function comp. Atoms,
literals, clauses, and clause sets are ground if they do not contain any variable.
An interpretation I with a nonempty domain (or universe) U assigns (i) a
total function f I : Un →U for each f ∈Ω with arity(f) = n and (ii) a relation
P ⊆Um to every predicate symbol P I ∈Π with arity(P) = m. A valuation β
is a function X →U where the assignment of some variable x can be modiﬁed
to e ∈U by β[x →e]. It is extended to terms as I(β) : T(Σ, X) →U. Seman-
tic entailment |= considers variables in clauses to be universally quantiﬁed. The
extension to atoms, literals, disjunctions, clauses and sets of clauses is as fol-
lows: I(β)(P(t1, . . . , tn)) = 1 if (I(β)(t1), . . . , I(β)(tn)) ∈P I and 0 otherwise;
I(β)(¬φ) = 1 −I(β)(φ); for a disjunction L1 ∨. . . ∨Lk, I(β)(L1 ∨. . . ∨Lk) =
max(I(β)(L1), . . . , I(β)(Lk)); for a clause C, I(β)(C) = 1 if for all valuations
β = {x1 →e1, . . . , xn →en} where the xi are the free variables in C there is
a literal L ∈C such that I(β)(L) = 1; for a set of clauses N = {C1, . . . , Ck},
I(β)({C1, . . . , Ck}) = min(I(β)(C1), . . . , I(β)(Ck)). A set of clauses N is satis-
ﬁable if there is an I of N such that I(β)(N) = 1, β arbitrary, (in this case I is
called a model of N: I |= N) otherwise N is called unsatisﬁable.
Substitutions σ, τ
are total mappings from variables to terms, where
dom(σ) := {x | xσ ̸= x} is ﬁnite and codom(σ) := {t | xσ = t, x ∈dom(σ)}.
A renaming σ is a bijective substitution. The application of substitutions is
extended to literals, clauses, and sets/sequences of such objects in the usual
way. If C′ = Cσ for some substitution σ, then C′ is an instance of C. A uniﬁer
σ for a set of terms t1, . . . , tk satisﬁes tiσ = tjσ for all 1 ≤i, j ≤k and it is called
a most general uniﬁer if for any uniﬁer σ′ of t1, . . . , tk there is a substitution τ
s.t. σ′ = στ. The function mgu denotes the most general uniﬁer of two terms,
atoms, literals if it exists. We assume that any mgu of two terms or literals does
not introduce any fresh variables and is idempotent.
The resolution calculus consists of two inference rules: Resolution and Fac-
toring [28,29]. The rules operate on a state (N, S) where the initial state for
a classical resolution refutation from a clause set N is (∅, N) and for an SOS
(Set Of Support) refutation with clause set N and initial SOS clause set S the

Semantic Relevance
215
initial state is (N, S). We describe the rules in the form of abstract rewrite rules
operating on states (N, S). As usual we assume for the resolution rule that the
involved clauses are variable disjoint. This can always be achieved by applying
renamings into fresh variables.
Resolution
(N, S ⊎{C ∨K}) ⇒RES (N, S ∪{C ∨K, (D ∨C)σ})
provided (D ∨L) ∈(N ∪S) and σ = mgu(L, comp(K))
Factoring (N, S ⊎{C ∨L ∨K}) ⇒RES (N, S ∪{C ∨L ∨K} ∪{(C ∨L)σ})
provided σ = mgu(L, K)
The clause (D∨C)σ is the result of a Resolution inference between its parents
and called a resolvent. The clause (C ∨L)σ is the result of a Factoring inference
of its parent and called a factor. A sequence of rule applications (N, S) ⇒∗
RES
(N, S′) is called a resolution derivation. It is called an SOS resolution derivation
if N ̸= ∅. In case ⊥∈S′ it is a called a (SOS) resolution refutation. If for two
clauses C, D there exists a substitution σ such that Cσ ⊆D, then we say that
C subsumes D. In this case C |= D.
Theorem 1 (Soundness and Refutational Completeness of (SOS) Res-
olution [11,28,33]). Resolution is sound and refutationally complete [28]. If for
some clause set N and initial SOS S, N is satisﬁable and N ∪S is unsatisﬁable,
then there is a (SOS) resolution derivation of ⊥from (N, S) [33]. If for some
clause set N and clause C ∈N there exists a resolution refutation from N using
C, then there is an SOS derivation of ⊥from (N \ {C}, {C}) [11].
Please note that the recent SOS completeness result of [11] generalizes the
classical SOS completeness result by [33].
Theorem 2 (Deductive Completeness of Resolution [17,22]).
Given a
set of clauses N and a clause D, if N |= D, then there is a resolution derivation
of some clause C from (∅, N) such that C subsumes D.
For deductions we require every clause to be used exactly once, so deductions
always have a tree form.
Deﬁnition 3 (Deduction [11]). A deduction πN = [C1, . . . , Cn] of a clause
Cn from some clause set N is a ﬁnite sequence of clauses such that for each Ci
the following holds:
1.1 Ci is a renamed, variable-fresh version of a clause in N, or
1.2 there is a clause Cj ∈πN, j < i s.t. Ci is the result of a Factoring inference
from Cj, or
1.3 there are clauses Cj, Ck ∈πN, j < k < i s.t. Ci is the result of a Resolution
inference from Cj and Ck,
and for each Ci ∈πN, i < n:

216
F. Haifani and C. Weidenbach
2.1 there exists exactly one factor Cj of Ci with j > i, or
2.2 there exists exactly one Cj and Ck such that Ck is a resolvent of Ci and Cj
and i, j < k.
We omit the subscript N in πN if the context is clear.
A deduction π′ of some clause C ∈π, where π, π′ are deductions from N is a
subdeduction of π if π′ ⊆π, where the subset relation is overloaded for sequences.
A deduction πN = [C1, . . . , Cn−1, ⊥] is called a refutation. While the conditions
3.1.1, 3.1.2, and 3.1.3 are suﬃcient to represent a resolution derivation, the
conditions 3.2.1 and 3.2.2 force deductions to be minimal with respect to Cn.
Note that variable renamings are only applied to clauses from N such that all
clauses from N that are introduced in the deduction are variable disjoint. Also
recall that our notion of a deduction implies a tree structure. Both assumptions
together admit the existence of overall grounding substitutions for a deduction.
Deﬁnition 4 (Overall Substitution of a Deduction [11]). Given a deduc-
tion π of a clause Cn the overall substitution τπ,i of Ci ∈π is recursively deﬁned
by
1 if Ci is a factor of Cj with j < i and mgu σ, then τπ,i = τπ,j ◦σ,
2 if Ci is a resolvent of Cj and Ck with j < k < i and mgu σ, then τπ,i =
(τπ,j ◦τπ,k) ◦σ,
3 if Ci is an initial clause, then τπ,i = ∅,
and the overall substitution of the deduction is τπ = τπ,n. We omit the subscript
π if the context is clear.
Overall substitutions are well-deﬁned because clauses introduced from N into
the deduction are variable disjoint and each clause is used exactly once in the
deduction. A grounding of an overall substitution τ of some deduction π is a
substitution τδ such that codom(τδ) only contains ground terms and dom(δ) is
exactly the variables from codom(τ).
Deﬁnition 5 (SOS Deduction [11]).
A deduction πN∪S = [C1, . . . , Cn] is
called an SOS deduction with SOS S, if the derivation (N, S0) ⇒∗
RES (N, Sm) is
an SOS derivation where C′
1, . . . , C′
m is the subsequence from [C1, . . . , Cn] with
input clauses removed, S0 = S, and Si+1 = Si ∪C′
i+1.
Oftentimes, it is of particular interest to identify the set of clauses that is
minimally unsatisﬁable, i.e., removing a clause would make it satisﬁable. The
earliest mention of such a notion is in [26] where it is introduced via a decision
problem. Minimally unsatisﬁable sets (MUS) have also gained a lot of attention
in practice.
Deﬁnition 6 (Minimal Unsatisﬁable Subset (MUS) [20]).
Given an
unsatisﬁable set of clauses N, the subset N ′ ⊆N is a minimally unsatisﬁable
subset (MUS) of N if any strict subset of N ′ is satisﬁable.

Semantic Relevance
217
In our previous work, we deﬁned a notion of relevance based on how clauses
may contribute to unsatisﬁability by means of refutations.
Deﬁnition 7 (Syntactic Relevance [11]).
Given an unsatisﬁable set of
clauses N, a clause C ∈N is syntactically relevant if for all refutations π
of N it holds that C ∈π. A clause C ∈N is syntactically semi-relevant if there
exists a refutation π of N in which C ∈π. A clause C ∈N is syntactically
irrelevant if there is no refutation π of N in which C ∈π.
Syntactic relevance can be identiﬁed by using the resolution calculus. A clause
C ∈N is syntactically semi-relevant if and only if there exists an SOS refutation
from SOS {C} and N \ {C}.
Theorem 8 (Syntactic Relevance [11]). Given an unsatisﬁable set of clauses
N, the clause C ∈N is
1. syntactically relevant if and only if N \ {C} is satisﬁable,
2. syntactically semi-relevant if and only if (N \ {C}, {C}) ⇒∗
RES (N \ {C}, S ∪
{⊥}).
An open problem from [11] is the question of a semantic counterpart to
syntactic semi-relevance. Without any further properties of the clause set N, the
notion of semi-relevance can lead to unintuitive results. For example, a tautology
could be semi-relevant. Given a refutation showing semi-relevance of some clause
C, where, in the refutation, some unary predicate P occurs, the refutation can be
immediately extended using the tautology P(x) ∨¬P(x). We may additionally
stumble upon a problem in the case where our set of clauses contains a subsumed
clause. For example, if both clauses Q(a) and Q(x) exist in a clause set, they
may be both semi-relevant, although from an intuition point of view one may
only want to consider Q(x) to be semi-relevant, or even relevant. On the other
hand, in some cases, redundant clauses are welcome as semi-relevant clauses.
Example 9 (Redundant Clauses). Given a set of clauses
N = {Q(x),
Q(a),
¬Q(a) ∨P(b),
¬P(b),
P(x) ∨¬P(x)},
all clauses are syntactically semi-relevant while ¬Q(a) ∨P(b) and ¬P(b) are
syntactically relevant. However, if we disregard the redundant clauses Q(a) and
P(x)∨¬P(x), then the clause Q(x) becomes a relevant clause. Therefore, for our
semantic notion of relevance we will only consider clause sets without clauses
implied by other, diﬀerent clauses from the clause set.
3
Semantic Relevance
Except for the trivially false clause ⊥, the simplest form of a contradiction is
two unit clauses K and L such that K and comp(L) are uniﬁable. They will
be called conﬂict literals, below. Then the idea for our semantic deﬁnition of

218
F. Haifani and C. Weidenbach
semi-relevance is to consider clauses that contribute to the number of conﬂict
literals of a clause set. Furthermore, we will show that in any MUS every literal
is a conﬂict literal.
While conﬂict literals could straightforwardly be deﬁned in propositional
logic having the above idea in mind, in ﬁrst-order logic we have always to relate
properties of literals, clauses to their respective ground instances. This is simply
due to the fact that unsatisﬁability of a ﬁrst-order clause set is given by unsat-
isﬁability of a ﬁnite set of ground instances from this set. Eventually, we will
show that for independent clause sets a clause is semi-relevant, if it contributes
to the number of conﬂict literals.
Deﬁnition 10 (Conﬂict Literal).
Given a set of clauses N over some sig-
nature Σ, a ground literal L is a conﬂict literal in a clause set N if there are
two satisﬁable clause sets N1, N2 such that
1. the clauses in N1, N2 are instances of clauses from N and
2. N1 |= L and N2 |= comp(L).
conﬂict(N) denotes the set of conﬂict literals in N.
Our notion of a conﬂict literal generalizes the respective notion in [12] deﬁned
for propositional logic.
Example 11 (Conﬂict Literal).
Given an unsatisﬁable set of clauses over the
signature Σ = ({a, b, c, d, f}, {P}):
N = {¬P(f(a, x)) ∨¬P(f(c, y)), P(f(x, d)) ∨P(f(y, b))}
Consider the following satisﬁable sets of instances from N
N1 = {¬P(f(a, d)) ∨¬P(f(c, y)), P(f(x, d)) ∨P(f(a, b))}
N2 = {¬P(f(a, b)) ∨¬P(f(c, y)), P(f(x, d)) ∨P(f(c, b))}
P(f(a, b)) is a conﬂict literal because N1 |= P(f(a, b)) and N2 |= ¬P(f(a, b)).
We can show that N1 |= P(f(a, b)) because the resolution calculus is sound.
Resolving both literals of ¬P(f(a, d)) ∨¬P(f(c, y)) with the ﬁrst literal of the
clause P(f(x, d)) ∨P(f(a, b)) results in the clause P(f(a, b)) ∨P(f(a, b)) which
can be factorized to P(f(a, b)). Moreover, N1 is satisﬁable: An interpretation I
with I(P(f(a, b))) = 1 and I(P(t)) = 0 for all terms t ̸= f(a, b) satisﬁes N1 and
P(f(a, b)). N2 |= ¬P(f(a, b)) can also be shown in the same manner.
Example 12 (Conﬂict Literal). Given
N = {¬R(z), R(c) ∨P(a, y),
Q(a), ¬Q(x) ∨P(x, b),
¬P(a, b)}

Semantic Relevance
219
its conﬂict literals are
conﬂict(N) = {P(a, b), ¬P(a, b),
R(c), ¬R(c),
Q(a), ¬Q(a)}
In addition to a refutation, the existence of a conﬂict literal is another way
to characterize unsatisﬁability of a clause set. Obviously, conﬂict literals always
come in pairs.
Lemma 13 (Minimal Unsatisﬁable Ground Clause Sets and Conﬂict
Literals). If N is a minimally unsatisﬁable set of ground clauses (MUS) then
any literal occurring in N is a conﬂict literal.
Proof Take any ground atom A such that A occurs in N. N can be split into
three disjoint clause sets:
N∅= {C ∈N|A ̸∈C and ¬A ̸∈C}
NA = {C ∈N|A ∈C}
N¬A = {C ∈N|¬A ∈C}
Since N is minimal, NA and N¬A are nonempty, because otherwise A is a pure
literal and its corresponding clauses can be removed from N preserving unsatis-
ﬁability. Obviously N∅∪NA must be satisﬁable, for otherwise the initial choice
of N was not minimal. However, N∅∪N ′
A, where N ′
A results from all NA by
deleting all A literals from the clauses of NA, must be unsatisﬁable, for oth-
erwise we can construct a satisfying interpretation for N. Thus, every model
of N∅∪NA must also be a model of A: N∅∪NA |= A. Using the same argu-
ment, N∅∪N¬A is satisﬁable and N∅∪N¬A |= ¬A. Therefore, A is a conﬂict
literal.
⊓⊔
Lemma 14 (Conﬂict Literals and Unsatisﬁability). Given a set of clauses
N, conﬂict(N) ̸= ∅if and only if N is unsatisﬁable.
Proof “⇒” Let L ∈conﬂict(N). By deﬁnition, there are two satisﬁable subsets
of instances N1, N2 from N such that N1 |= L and N2 |= comp(L). Towards
contradiction, suppose N is satisﬁable. Then, there exists an interpretation I
with I |= N and therefore it holds that I |= N1 and I |= N2. Furthermore, by
deﬁnition of a conﬂict literal, I |= L and I |= comp(L), a contradiction.
“⇐” Given an unsatisﬁable clause set N, we show that there is a conﬂict literal
in N. Since N is unsatisﬁable, by compactness of ﬁrst-order logic there is a
minimal set of ground instances N ′ from N that is also unsatisﬁable. The rest
follows from Lemma 13.
⊓⊔
Intuitively, a clause that is implied by other clauses is redundant and can be
removed from the set of clauses. However, then applying a calculus generating
new clauses, this intuitive notion of redundancy may destroy completeness [2,23].
Still, the detection and elimination of redundant clauses, compatible or incom-
patible with completeness, is an important concept to the eﬃciency of automatic

220
F. Haifani and C. Weidenbach
reasoning, e.g., in propositional logic [3,18]. It is also apparently important when
we try to deﬁne a semantic notion of relevance. For example, a syntactically rele-
vant clause would step down to be syntactically semi-relevant if it is duplicated.
So, in order to have a semantically robust notion of relevance in ﬁrst-order logic,
we need to use a strong notion of (in)dependency.
Deﬁnition 15 (Dependency). A clause C is dependent in N if there exists
a satisﬁable set of instances N ′ from N \ {C} such that N ′ |= Cσ for some σ. If
C is not dependent in N it is independent in N. A clause set N is independent
if it does not contain any dependent clauses.
A subsumed clause is obviously a dependent clause. However, there could
also be non-subsumed clauses that are dependent. For example, in the set of
clauses
N = {P(a, y), P(x, b), ¬P(a, b)}
P(x, b) is dependent because P(a, b) is an instance of P(x, b) and it is entailed
by P(a, y). Now, we are ready to deﬁne the semantic notion of relevance based
on conﬂict literals and dependency.
In some way, our notion of independence of clause sets is a strong assumption
because there might be non-redundant clauses that are considered dependent.
While this holds by design in some scenarios (e.g. the mentioned car scenario)
in others it is violated by design. In addition, one question that may arise is how
to acquire an independent clause set out of a dependent one. For example, in a
scenario where some theory is developed out of some independent axioms. Then
of course proven lemmas, theorems are dependent with respect to the axioms. In
this case one could trace out of the proofs the dependency relations between the
intermediate lemmas, theorems and the axioms and this way calculate indepen-
dent clause sets with respect to some proven conjecture. This would then lead
again to independent (sub) clause sets with respect to the proven conjecture
where our results are applicable.
Deﬁnition 16 (Semantic Relevance).
Given an unsatisﬁable set of inde-
pendent clauses N, a clause C ∈N is
1. relevant, if conﬂict(N \ {C}) = ∅
2. semi-relevant, if conﬂict(N \ {C}) ⊊conﬂict(N)
3. irrelevant, if conﬂict(N \ {C}) = conﬂict(N)
Example 17 (Dependent Clauses in Propositional Logic).
N = {P, ¬P,
¬P ∨Q, ¬R ∨P,
¬Q ∨R}
The existence of dependent clauses ¬P ∨Q and ¬R ∨P causes an independent
clause ¬Q ∨R to be a semi-relevant clause. However, ¬Q ∨R is not inside the
only MUS {P, ¬P}.

Semantic Relevance
221
Very often, concepts from propositional logic can be generalized to ﬁrst-order
logic. However, in the context of relevance this is not the case. Our notion of
(semi-)relevance can also be characterized by MUSes in propositional logic, but
not in ﬁrst-order logic without considering instances of clauses.
Lemma 18 (Propositional Clause Sets and Relevance). Given an inde-
pendent unsatisﬁable set of propositional clauses N, the relevant clauses coincide
with the intersection of all MUSes and the semi-relevant clauses coincide with
the union of all MUSes.
Proof For the case of relevance: Given C ∈N, C is relevant if and only if
conﬂict(N \ {C}) = ∅if and only if N \ {C} is satisﬁable by Lemma 14 if and
only if C is contained in all MUSes N ′ of N.
For the case of semi-relevance: Given C ∈N, we show C is semi-relevant if and
only if C is in some MUS N ′ ⊆N.
“⇒”: Towards contradiction, suppose there is a semi-relevant clause C that is
not in any MUS. By deﬁnition of semi-relevant clauses, there are satisﬁable
sets N1 and N2 and a propositional variable P such that N1 |= P, N2 |= ¬P
but the MUS M out of N1 ∪N2 does not contain C. By Theorem 2 there
exist deductions π1 and π2 of P and ¬P from N1 and N2, respectively. Since a
deduction is connected, some clauses in M and (N1 ∪N2) \ M must have some
complementary propositional literals Q and ¬Q, respectively to be eventually
resolved upon in either π1 or π2. At least one of these deductions must contain
this resolution step between a clause from M and one from (N1 ∪N2) \ M. Now
by Lemma 13 the literals Q and ¬Q are conﬂict literals in M. Thus, there are
satisﬁable subsets from M which entail Q and ¬Q, respectively. Therefore, the
clause containing Q or ¬Q in (N1 ∪N2) \ M is dependent contradicting the
assumption that N does not contain dependent clauses.
“⇐”: If C is in some MUS N ′ ⊆N, then, N ′ \ {C} is satisﬁable. So invoking
Lemma 13 any literal L ∈C is a conﬂict literal in N ′. In addition, L is not a
conﬂict literal in N \ {C} for otherwise C is dependent: Suppose L is a conﬂict
literal in N \ {C} then, by deﬁnition, there is satisﬁable subset from N \ {C}
which entails L. However, since L |= C, it means C is dependent.
⊓⊔
The next example demonstrates that the notion of a MUS cannot be carried
over straightforwardly to the level of clauses with variables to characterize semi-
relevant clauses in ﬁrst-order logic.
Example 19 (First-Order Relevant Clauses). Given a set of clauses
N = {P(a, y), ¬P(a, d) ∨Q(b, d),
¬P(x, c), ¬Q(b, d) ∨P(d, c), Q(z, e)}
over Σ = ({a, b, c, d, e}, {P, Q}). The conﬂict literals are
{(¬)P(a, c), (¬)Q(b, d), (¬)P(d, c), (¬)P(a, d)}.

222
F. Haifani and C. Weidenbach
The clause P(a, y) is relevant. The literals entailed by some satisﬁable instances
N ′ from N such that P(a, y) ̸∈N ′ are {¬Q(b, d)} ⊎{¬P(t, c), ¬Q(t, e) |
t ∈{a, b, c, d, e}} and no two of them are complementary. Thus, conﬂict(N \
{P(a, y)}) = ∅. The clause ¬P(a, d) ∨Q(b, d) is semi-relevant: Q(b, d) ̸∈
conﬂict(N \ {¬P(a, d) ∨Q(b, d)}). The clause Q(z, e) is irrelevant.
With respect to a MUS, the clause ¬P(a, d) ∨Q(b, d) from Example 19 is
irrelevant. The only MUS from N is {P(a, y), ¬P(x, c)} with grounding substi-
tution {x →a, y →c}. However, in ﬁrst-order logic we should not ignore the
clauses ¬P(a, d) ∨Q(b, d), ¬Q(b, d) ∨P(d, c), because together with the clauses
P(a, y), ¬P(x, c) they result in a diﬀerent grounding {x →d, y →d}. So, we
argue that MUS-based (semi-)relevance on the original clause set is not suﬃ-
cient to characterize the way clauses are used to derive a contradiction for full
ﬁrst-order logic. However, it does so if ground instances are considered.
Lemma 20 (Relevance and MUSes on First-Order Clauses). Given an
unsatisﬁable set of independent ﬁrst-order clauses N. Then a clause C is relevant
in N, if all MUSes of unsatisﬁable sets of ground instances from N contain a
ground instance of C. The clause C is semi-relevant in N, if there exists a
MUS of an unsatisﬁable set of ground instances from N that contains a ground
instance of C.
Proof (Relevance) Since all ground instances from N contain a ground instance
of C, then, if N \ {C} contains a ground MUS from N it means that some
ground instance of C is entailed by N \ {C}. This violates our assumption that
N contains no dependent clauses. Thus, N\{C} contains no ground MUSes. This
further means that N \ {C} is satisﬁable by the compactness theorem of ﬁrst-
order logic. By Lemma 14 it therefore has no conﬂict literals and C is relevant.
(Semi-Relevance) Take some ground MUS M containing some ground instance
C′ of C. Due to Lemma 13, any literal P ∈C′ is a conﬂict literal in M and
consequently also in N. In addition, P is not a conﬂict literal in N \ {C} for
otherwise C is dependent: Suppose P is a conﬂict literal in N \ {C}. Then,
by deﬁnition, there is some satisﬁable instances from N \ {C} which entails P.
However, since P |= C′, it means C is dependent. In conclusion, P ∈conﬂict(N)\
conﬂict(N \ {C}) and thus C is semi-relevant.
⊓⊔
In Example 19, we could identify two ground MUSes:
{P(a, c), ¬P(a, c)}
and
{P(a, d), ¬P(a, d) ∨Q(b, d), ¬P(d, c), ¬Q(b, d) ∨P(d, c)}
Our notion of relevance is thus alternatively explainable using Lemma 20: P(a, y)
is relevant because every MUS contains an instance of it (P(a, c) and P(a, d)).
The clause ¬P(a, d)∨Q(b, d) is semi-relevant as it is immediately contained in the
second MUS. The clause Q(z, e) is irrelevant since no MUS contains any instance
of Q(z, e). On the other hand, we may still encounter the case where a dependent

Semantic Relevance
223
clause is actually categorized as syntactically semi-relevant. Therefore, by using
the dependency notion while at the same time not restricting a refutation to only
use MUS as the input set, we can show that (semi-)relevance actually coincides
with the syntactic (semi-)relevance. So, the semi-decidability result also follows.
Theorem 21 (Semantic versus Syntactic Relevance).
Given an inde-
pendent, unsatisﬁable set of clauses N in ﬁrst-order logic, then (semi)-relevant
clauses coincide with syntactically (semi)-relevant clauses.
Proof We show the following: if N contains no dependent clause, C is (semi-)
relevant if and only if C is syntactically (semi-)relevant. The case for relevant
clauses is a consequence of Lemma 14. Now, we show it for semi-relevant clauses.
“⇒” Let L be a ground literal with L ∈conﬂict(N) \ conﬂict(N \ {C}). We
can construct a refutation using C. There are two satisﬁable subsets of instances
N1, N2 from N such that N1 |= L and N2 |= comp(L) where N1 ∪N2 contains at
least one instance of C, for otherwise L ̸∈conﬂict(N)\conﬂict(N \{C}). By the
deductive completeness, Theorem 2, and the fact that L and comp(L) are ground
literals, there are two variable disjoint deductions π1 and π2 of some literals
K1 and K2 such that K1σ = L and K2σ = comp(L) for some grounding σ.
Obviously, the two variable disjoint deductions can be combined to a refutation
π1.π2.⊥containing C. Thus, C is syntactically semi-relevant in N.
“⇐” Given an SOS refutation π using C, i.e., an SOS refutation π from
N \ {C} with SOS {C} and overall grounding substitution σ, we show that C is
semantically semi-relevant. Let N ′ be the variable renamed versions of clauses
from N \ {C} used in the refutation and S′ be the renamed copies of C used
in the refutation. First, we show that N ′σ is satisﬁable. Towards contradiction,
suppose N ′σ is unsatisﬁable and let Mσ ⊆N ′σ be its MUS. Since π is connected,
some clauses in Mσ and S′σ ∪(N ′σ \ Mσ) contains literals L and comp(L)
respectively. By Lemma 13, L and comp(L) are also conﬂict literals in Mσ. So,
by Deﬁnition 15, the clause containing comp(L) in S′σ∪(N ′σ\Mσ) is dependent
violating our initial assumption.
Now, since N ′σ is satisﬁable, there is a ground MUS from (N ′ ∪S′)σ con-
taining some C′σ ∈Sσ. Due to Lemma 13, any L ∈C′σ is a conﬂict literal
in N ′ (and consequently also in N). In addition, L is not a conﬂict literal in
N \ {C} for otherwise C is dependent: Suppose L is a conﬂict literal in N \ {C}.
Then, by deﬁnition, there is some satisﬁable instances from N \ {C} which
entails L. However, since L |= C′σ, it means C is dependent. In conclusion,
L ∈conﬂict(N) \ conﬂict(N \ {C}) and thus C is semi-relevant.
⊓⊔
When we have a ground MUS, identiﬁﬁcation of conﬂict literals is obvious
because all of the literals in it are. However, testing if a literal L is a conﬂict
literal is not trivial, in general. One can try enumerating all MUSes and check if
L is contained in some. This deﬁnitely works for propositional logic despite being
computationally expensive. In ﬁrst-order logic, this is problematic because there
could potentially be an inﬁnite number of MUSes and determining a MUS is not
even semi-decidable, in general. The following lemma provides a semi-decidable
test using the SOS strategy.

224
F. Haifani and C. Weidenbach
Lemma 22 Given a ground literal L and an unsatisﬁable set of clauses N with
no dependent clauses, L is a conﬂict literal if and only if there is an SOS refu-
tation from (N, {L ∨comp(L)}).
Proof “⇒” By the deductive completeness, Theorem 2, and the fact that L
and comp(L) are ground literals, there are two variable disjoint deductions π1
and π2 of some literals K1 and K2 such that K1σ = L and K2σ = comp(L)
for some grounding σ. Obviously, the two variable disjoint deductions can be
combined to a refutation π1.π2.⊥. We can then construct a refutation π1.π2.(L∨
¬L).(comp(L)).⊥where K2 is resolved with L ∨comp(L) to get comp(L) which
will be resolved with K1 from π1 to get ⊥. By Theorem 7, it means there is an
SOS refutation from (N, {L ∨¬L})
“⇐” Given an SOS refutation π using {L∨comp(L)}, i.e., an SOS refutation
π from N \{{L∨comp(L)}} with SOS {{L∨comp(L)}}, Let N ′ be the variable
renamed versions of clauses from N and overall grounding substitution σ. N ′σ
is a MUS for otherwise there is a dependent clause: Suppose N ′σ \ M is an
MUS where M is non-empty. Since π is connected, some clause D′ in M must
be resolved with some D ∈N ′σ upon some literal K. Thus, by Lemma 13, K
and comp(K) are also conﬂict literals in N ′σ \ M. So, by Deﬁnition 15, the
clause subsuming D′ in N is dependent violating our initial assumption. Finally,
because L occurs in N ′σ and N ′σ is an MUS, by Lemma 13, L is a conﬂict
literal.
⊓⊔
4
Conclusion
The main results of this paper are: (i) a semantic notion of relevance based on the
existence of conﬂict literals, Deﬁnition 10, and Deﬁnition 16, (ii) its relationship
to syntactic relevance, namely, both notions coincide for independent clause
sets, Theorem 21, and (iii) the relationship of semantic relevance to minimal
unsatisﬁable sets, MUSes, both for propositional logic, Lemma 18, and ﬁrst-
order logic, Lemma 20.
The semantic relevance notion sheds some further light on the way clauses
may contribute to a refutation beyond what can be oﬀered by the notion of
MUSes. While the syntactic notion of semi-relevance also considers redundant
clauses such as tautologies to be semi-relevant, the semantic notion rules out
redundant clauses. Here, the notions only coincide for independent clause sets.
Still, the syntactic notion is “easier” to test and there are applications where
clause sets do not contain implied clauses by construction. Hence, the syntactic-
relevance coincides with semantic relevance. For example, ﬁrst-order toolbox
formalizations have this property because every tool is formalized by its own
distinct predicate. Still a goal, refutation, can be reached by the use of diﬀerent
tools. The classic example is the toolbox for car/truck/tractor building [8,31].
Acknowledgments. This work was partly funded by DFG grant 389792660 as part of
TRR 248. We thank Christopher Lynch and David Plaisted for a number of discussions
on semantic relevance. We thank the anonymous reviewers for their constructive and
detailed comments.

Semantic Relevance
225
References
1. Baader, F., Pe˜naloza, R.: Axiom pinpointing in general tableaux. J. Log. Comput.
20(1), 5–34 (2010)
2. Bachmair, L., Ganzinger, H.: Resolution theorem proving. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, chap. 2, pp. 19–99.
Elsevier, Amsterdam (2001)
3. Boufkhad, Y., Roussel, O.: Redundancy in random SAT formulas. In: Kautz, H.A.,
Porter, B.W. (eds.) Proceedings of the Seventeenth National Conference on Artiﬁ-
cial Intelligence and Twelfth Conference on on Innovative Applications of Artiﬁcial
Intelligence, 30 July - 3 August, 2000, Austin, Texas, USA, pp. 273–278. AAAI
Press/The MIT Press (2000)
4. Bourgaux, C., Ozaki, A., Pe˜naloza, R., Predoiu, L.: Provenance for the description
logic ELHr. In: Bessiere, C. (ed.) Proceedings of the Twenty-Ninth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2020, pp. 1862–1869. ijcai.org
(2020)
5. Cimatti, A., Griggio, A., Sebastiani, R.: A simple and ﬂexible way of computing
small unsatisﬁable cores in SAT modulo theories. In: Marques-Silva, J., Sakallah,
K.A. (eds.) SAT 2007. LNCS, vol. 4501, pp. 334–339. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-72788-0 32
6. Cimatti, A., Griggio, A., Sebastiani, R.: Computing small unsatisﬁable cores in
satisﬁability modulo theories. J. Artif. Intell. Res. 40, 701–728 (2011)
7. Eiter, T., Gottlob, G.: The complexity of logic-based abduction. J. ACM 42(1),
3–42 (1995)
8. Fetzer, C., Weidenbach, C., Wischnewski, P.: Compliance, functional safety and
fault detection by formal methods. In: Margaria, T., Steﬀen, B. (eds.) ISoLA 2016.
LNCS, vol. 9953, pp. 626–632. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-47169-3 48
9. Guthmann, O., Strichman, O., Trostanetski, A.: Minimal unsatisﬁable core extrac-
tion for SMT. In: Piskac, R., Talupur, M. (eds.) 2016 Formal Methods in Computer-
Aided Design, FMCAD 2016, Mountain View, CA, USA, October 3–6, 2016. pp.
57–64. IEEE (2016)
10. Haifani, F., Koopmann, P., Tourret, S., Weidenbach, C.: On a notion of relevance.
In: Borgwardt, S., Meyer, T. (eds.) Proceedings of the 33rd International Workshop
on Description Logics (DL 2020) Co-located with the 17th International Confer-
ence on Principles of Knowledge Representation and Reasoning (KR 2020), Online
Event [Rhodes, Greece], 12th to 14th September 2020. CEUR Workshop Proceed-
ings, vol. 2663. CEUR-WS.org (2020)
11. Haifani, F., Tourret, S., Weidenbach, C.: Generalized completeness for SOS reso-
lution and its application to a new notion of relevance. In: Platzer, A., Sutcliﬀe,
G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 327–343. Springer, Cham
(2021). https://doi.org/10.1007/978-3-030-79876-5 19
12. Jabbour, S., Ma, Y., Raddaoui, B., Sais, L.: Quantifying conﬂicts in propositional
logic through prime implicates. Int. J. Approx. Reason. 89, 27–40 (2017)
13. Kalyanpur, A., Parsia, B., Horridge, M., Sirin, E.: Finding all justiﬁcations of OWL
DL entailments. In: Aberer, K. (ed.) ASWC/ISWC -2007. LNCS, vol. 4825, pp.
267–280. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-76298-
0 20
14. Kleine B¨uning, H., Kullmann, O.: Minimal unsatisﬁability and autarkies. In: Biere,
A., Heule, M., van Maaren, H., Walsh, T. (eds.) Handbook of Satisﬁability, Fron-

226
F. Haifani and C. Weidenbach
tiers in Artiﬁcial Intelligence and Applications, vol. 185, pp. 339–401. IOS Press,
Amsterdam (2009)
15. Kullmann, O.: Investigations on autark assignments. Discret. Appl. Math. 107(1–
3), 99–137 (2000)
16. Kullmann, O., Lynce, I., Marques-Silva, J.: Categorisation of clauses in conjunctive
normal forms: minimally unsatisﬁable sub-clause-sets and the lean kernel. In: Biere,
A., Gomes, C.P. (eds.) SAT 2006. LNCS, vol. 4121, pp. 22–35. Springer, Heidelberg
(2006). https://doi.org/10.1007/11814948 4
17. Lee, C.T.: A Completeness Theorem and a Computer Program for Finding Theo-
rems Derivable from Given Axioms. Ph.D. thesis, University of Berkeley, California,
Department of Electrical Engineering (1967)
18. Liberatore, P.: Redundancy in logic I: CNF propositional formulae. Artif. Intell.
163(2), 203–232 (2005)
19. Liu, S., Luo, J.: FMUS2: an eﬃcient algorithm to compute minimal unsatisﬁable
subsets. In: Fleuriot, J., Wang, D., Calmet, J. (eds.) AISC 2018. LNCS (LNAI),
vol. 11110, pp. 104–118. Springer, Cham (2018). https://doi.org/10.1007/978-3-
319-99957-9 7
20. Marques-Silva, J., Menc´ıa, C.: Reasoning about inconsistent formulas. In: Bessiere,
C. (ed.) Proceedings of the Twenty-Ninth International Joint Conference on Arti-
ﬁcial Intelligence, IJCAI 2020, pp. 4899–4906. ijcai.org (2020)
21. Menc´ıa, C., Kullmann, O., Ignatiev, A., Marques-Silva, J.: On computing the union
of MUSes. In: Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 211–
221. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-24258-9 15
22. Nienhuys-Cheng, S.-H., de Wolf, R.: The equivalence of the subsumption theorem
and the refutation-completeness for unconstrained resolution. In: Kanchanasut, K.,
L´evy, J.-J. (eds.) ACSC 1995. LNCS, vol. 1023, pp. 269–285. Springer, Heidelberg
(1995). https://doi.org/10.1007/3-540-60688-2 50
23. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Robinson,
A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, chap. 7, pp.
371–443. Elsevier, Amsterdam (2001)
24. Nonnengart, A., Weidenbach, C.: Computing small clause normal forms. In: Robin-
son, A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. 1, chap. 6,
pp. 335–367. Elsevier, Amsterdam (2001)
25. Papadimitriou, C.H., Wolfe, D.: The complexity of facets resolved. J. Comput.
Syst. Sci. 37(1), 2–13 (1988)
26. Papadimitriou, C.H., Yannakakis, M.: The complexity of facets (and some facets
of complexity). J. Comput. Syst. Sci. 28(2), 244–259 (1984)
27. Pe˜naloza, R., Menc´ıa, C., Ignatiev, A., Marques-Silva, J.: Lean kernels in descrip-
tion logics. In: Blomqvist, E., Maynard, D., Gangemi, A., Hoekstra, R., Hitzler,
P., Hartig, O. (eds.) ESWC 2017. LNCS, vol. 10249, pp. 518–533. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-58068-5 32
28. Robinson, J.A.: A machine-oriented logic based on the resolution principle. J. ACM
12(1), 23–41 (1965)
29. Robinson, J.A., Voronkov, A. (eds.): Handbook of Automated Reasoning (in 2
volumes). Elsevier and MIT Press, Cambridge (2001)
30. Schlobach, S., Cornet, R.: Non-standard reasoning services for the debugging of
description logic terminologies. In: Gottlob, G., Walsh, T. (eds.) Proceedings of
the Eighteenth International Joint Conference on Artiﬁcial Intelligence, IJCAI-03,
Acapulco, Mexico, 9–15 August 2003, pp. 355–362. Morgan Kaufmann (2003)

Semantic Relevance
227
31. Sinz, C., Kaiser, A., K¨uchlin, W.: Formal methods for the validation of automotive
product conﬁguration data. Artif. Intell. Eng. Des. Anal. Manuf. 17(1), 75–97
(2003)
32. Walter, R., Felfernig, A., K¨uchlin, W.: Constraint-based and SAT-based diagnosis
of automotive conﬁguration problems. J. Intell. Inf. Syst. 49(1), 87–118 (2016).
https://doi.org/10.1007/s10844-016-0422-7
33. Wos, L., Robinson, G., Carson, D.: Eﬃciency and completeness of the set of support
strategy in theorem proving. J. ACM 12(4), 536–541 (1965)
34. Xie, H., Luo, J.: An algorithm to compute minimal unsatisﬁable subsets for a decid-
able fragment of ﬁrst-order formulas. In: 28th IEEE International Conference on
Tools with Artiﬁcial Intelligence, ICTAI 2016, San Jose, CA, USA, 6–8 November
2016, pp. 444–451. IEEE Computer Society (2016)
35. Zhang, J., Xu, W., Zhang, J., Shen, S., Pang, Z., Li, T., Xia, J., Li, S.: Finding ﬁrst-
order minimal unsatisﬁable cores with a heuristic depth-ﬁrst-search algorithm. In:
Yin, H., Wang, W., Rayward-Smith, V. (eds.) IDEAL 2011. LNCS, vol. 6936, pp.
178–185. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-23878-
9 22
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

SCL(EQ): SCL for First-Order Logic
with Equality
Hendrik Leidinger1,2(B)
and Christoph Weidenbach1
1 Max-Planck Institute for Informatics, Saarbr¨ucken, Germany
{hleiding,weidenbach}@mpi-inf.mpg.de
2 Graduate School of Computer Science, Saarbr¨ucken, Germany
Abstract. We propose a new calculus SCL(EQ) for ﬁrst-order logic
with equality that only learns non-redundant clauses. Following the idea
of CDCL (Conﬂict Driven Clause Learning) and SCL (Clause Learning
from Simple Models) a ground literal model assumption is used to guide
inferences that are then guaranteed to be non-redundant. Redundancy
is deﬁned with respect to a dynamically changing ordering derived from
the ground literal model assumption. We prove SCL(EQ) sound and
complete and provide examples where our calculus improves on super-
position.
Keywords: First-order logic with equality · Term rewriting ·
Model-based reasoning
1
Introduction
There has been extensive research on sound and complete calculi for ﬁrst-order
logic with equality. The current prime calculus is superposition [2], where order-
ing restrictions guide paramodulation inferences and an abstract redundancy
notion enables a number of clause simpliﬁcation and deletion mechanisms, such
as rewriting or subsumption. Still this “syntactic” form of superposition infers
many redundant clauses. The completeness proof of superposition provides a
“semantic” way of generating only non-redundant clauses, however, the under-
lying ground model assumption cannot be eﬀectively computed in general [31]. It
requires an ordered enumeration of inﬁnitely many ground instances of the given
clause set, in general. Our calculus overcomes this issue by providing an eﬀective
way of generating ground model assumptions that then guarantee non-redundant
inferences on the original clauses with variables.
The underlying ordering is based on the order of ground literals in the model
assumption, hence changes during a run of the calculus. It incorporates a stan-
dard rewrite ordering. For practical redundancy criteria this means that both
rewriting and redundancy notions that are based on literal subset relations are
permitted to dynamically simplify or eliminate clauses. Newly generated clauses
are non-redundant, so redundancy tests are only needed backwards. Further-
more, the ordering is automatically generated by the structure of the clause set.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 228–247, 2022.
https://doi.org/10.1007/978-3-031-10769-6_14

SCL(EQ): SCL for First-Order Logic with Equality
229
Instead of a ﬁxed ordering as done in the superposition case, the calculus ﬁnds
and changes an ordering according to the currently easiest way to make progress,
analogous to CDCL (Conﬂict Driven Clause Learning) [11,21,25,29,34].
Typical for CDCL and SCL (Clause Learning from Simple Models) [1,14,18]
approaches to reasoning, the development of a model assumption is done by deci-
sions and propagations. A decision guesses a ground literal to be true whereas
a propagation concludes the truth of a ground literal through an otherwise false
clause. While propagations in CDCL and propositional logic are restricted to
the ﬁnite number of propositional variables, in ﬁrst-order logic there can already
be inﬁnite propagation sequences [18]. In order to overcome this issue, model
assumptions in SCL(EQ) are at any point in time restricted to a ﬁnite number
of ground literals, hence to a ﬁnite number of ground instances of the clause set
at hand. Therefore, without increasing the number of considered ground literals,
the calculus either ﬁnds a refutation or runs into a stuck state where the current
model assumption satisﬁes the ﬁnite number of ground instances. In this case
one can check whether the model assumption can be generalized to a model
assumption of the overall clause set or the information of the stuck state can
be used to appropriately increase the number of considered ground literals and
continue search for a refutation. SCL(EQ) does not require exhaustive propaga-
tion, in general, it just forbids the decision of the complement of a literal that
could otherwise be propagated.
For an example of SCL(EQ) inferring clauses, consider the three ﬁrst-order
clauses
C1 := h(x) ≈g(x) ∨c ≈d
C2 := f(x) ≈g(x) ∨a ≈b
C3 := f(x) ̸≈h(x) ∨f(x) ̸≈g(x)
with a Knuth-Bendix Ordering (KBO), unique weight 1, and precedence d ≺
c ≺b ≺a ≺g ≺h ≺f. A Superposition Left [2] inference between C2 and C3
results in
C′
4 := h(x) ̸≈g(x) ∨f(x) ̸≈g(x) ∨a ≈b.
For SCL(EQ) we start by building a partial model assumption, called a trail,
with two decisions
Γ := [h(a) ≈g(a)1:(h(x)≈g(x)∨h(x)̸≈g(x))·σ, f(a) ≈g(a)2:(f(x)≈g(x)∨f(x)̸≈g(x))·σ]
where σ := {x →a}. Decisions and propagations are always ground instances
of literals from the ﬁrst-order clauses, and are annotated with a level and a
justiﬁcation clause, in case of a decision a tautology. Now with respect to Γ clause
C3 is false with grounding σ, and rule Conﬂict is applicable; see Sect. 3.1 for
details on the inference rules. In general, clauses and justiﬁcations are considered
variable disjoint, but for simplicity of the presentation of this example, we repeat
variable names here as long as the same ground substitution is shared. The
maximal literal in C3σ is (f(x) ̸≈h(x))σ and a rewrite refutation using the
ground equations from the trail results in the justiﬁcation clause
(g(x) ̸≈g(x) ∨f(x) ̸≈g(x) ∨f(x) ̸≈g(x) ∨h(x) ̸≈g(x))· σ

230
H. Leidinger and C. Weidenbach
where for the refutation justiﬁcation clauses and all otherwise inferred clauses
we use the grounding σ for guidance, but operate on the clauses with variables.
The respective ground clause is smaller than (f(x) ̸≈h(x))σ, false with respect
to Γ and becomes our new conﬂict clause by an application of our inference rule
Explore-Refutation. It is simpliﬁed by our inference rules Equality-Resolution
and Factorize, resulting in the ﬁnally learned clause
C4 := h(x) ̸≈g(x) ∨f(x) ̸≈g(x)
which is then used to apply rule Backtrack to the trail. Observe that C4 is
strictly stronger than C′
4 the clause inferred by superposition and that C4 cannot
be inferred by superposition. Thus SCL(EQ) can infer stronger clauses than
superposition for this example.
Related Work: SCL(EQ) is based on ideas of SCL [1,14,18] but for the ﬁrst time
includes a native treatment of ﬁrst-order equality reasoning. Similar to [14] prop-
agations need not to be exhaustively applied, the trail is built out of decisions
and propagations of ground literals annotated by ﬁrst-order clauses, SCL(EQ)
only learns non-redundant clauses, but for the ﬁrst time conﬂicts resulting out
of a decision have to be considered, due to the nature of the equality relation.
There have been suggested several approaches to lift the idea of an inference
guiding model assumption from propositional to full ﬁrst-order logic [6,12,13,18].
They do not provide a native treatment of equality, e.g., via paramodulation or
rewriting.
Baumgartner et al. describe multiple calculi that handle equality by using
unit superposition style inference rules and are based on either hyper tableaux [5]
or DPLL [15,16]. Hyper tableaux ﬁx a major problem of the well-known free
variable tableaux, namely the fact that free variables within the tableau are
rigid, i.e., substitutions have to be applied to all occurrences of a free variable
within the entire tableau. Hyper tableaux with equality [7] in turn integrates
unit superposition style inference rules into the hyper tableau calculus.
Another approach that is related to ours is the model evolution calculus with
equality (MEE) by Baumgartner et al. [8,9] which lifts the DPLL calculus to
ﬁrst-order logic with equality. Similar to our approach, MEE creates a candidate
model until a clause instance contradicts this model or all instances are satisﬁed
by the model. The candidate model results from a so-called context, which con-
sists of a ﬁnite set of non-ground rewrite literals. Roughly speaking, a context
literal speciﬁes the truth value of all its ground instances unless a more speciﬁc
literal speciﬁes the complement. Initially the model satisﬁes the identity relation
over the set of all ground terms. Literals within a context may be universal or
parametric, where universal literals guarantee all its ground instances to be true.
If a clause contradicts the current model, it is repaired by a non-deterministic
split which adds a parametric literal to the current model. If the added literal
does not share any variables in the contradictory clause it is added as a universal
literal.
Another approach by Baumgartner and Waldmann [10] combined the super-
position calculus with the Model Evolution calculus with equality. In this cal-

SCL(EQ): SCL for First-Order Logic with Equality
231
culus the atoms of the clauses are labeled as “split atoms” or “superposition
atoms”. The superposition part of the calculus then generates a model for the
superposition atoms while the model evolution part generates a model for the
split atoms. Conversely, this means that if all atoms are labeled as “split atom”,
the calculus behaves similar to the model evolution calculus. If all atoms are
labeled as “superposition atom”, it behaves like the superposition calculus.
Both the hyper tableaux calculus with equality and the model evolution cal-
culus with equality allow only unit superposition applications, while SCL(EQ)
inferences are guided paramodulation inferences on clauses of arbitrary length.
The model evolution calculus with equality was revised and implemented in
2011 [8] and compares its performance with that of hyper tableaux. Model evo-
lution performed signiﬁcantly better, with more problems solved in all relevant
TPTP [30] categories, than the implementation of the hyper tableaux calculus.
Plaisted et al. [27] present the Ordered Semantic Hyper-Linking (OSHL) cal-
culus. OSHL is an instantiation based approach that repeatedly chooses ground
instances of a non-ground input clause set such that the current model does not
satisfy the current ground clause set. A further step repairs the current model
such that it satisﬁes the ground clause set again. The algorithm terminates if
the set of ground clauses contains the empty clause. OSHL supports rewriting
and narrowing, but only with unit clauses. In order to handle non-unit clauses
it makes use of other mechanisms such as Brand’s Transformation [3].
Inst-Gen [22] is an instantiation based calculus, that creates ground instances
of the input ﬁrst-order formulas which are forwarded to a SAT solver. If a ground
instance is unsatisﬁable, then the ﬁrst-order set is as well. If not then the cal-
culus creates more instances. The Inst-Gen-EQ calculus [23] creates instances
by extracting instantiations of unit superposition refutations of selected liter-
als of the ﬁrst-order clause set. The ground abstraction is then extended by the
extracted clauses and an SMT solver then checks the satisﬁability of the resulting
set of equational and non-equational ground literals.
In favor of examples and explanations we omit all proofs. They are available
in an extended version published as a research report [24]. The rest of the paper
is organized as follows. Section 2 provides basic formalisms underlying SCL(EQ).
The rules of the calculus are presented in Sect. 3. Soundness and completeness
results are provided in Sect. 4. We end with a discussion of obtained results and
future work, Sect. 5. The main contribution of this paper is the SCL(EQ) cal-
culus that only learns non-redundant clauses, permits subset based redundancy
elimination and rewriting, and its soundness and completeness.
2
Preliminaries
We assume a standard ﬁrst-order language with equality and signature Σ =
(Ω, ∅) where the only predicate symbol is equality ≈. N denotes a set of clauses,
C, D denote clauses, L, K, H denote equational literals, A, B denote equational
atoms, t, s terms from T(Ω, X) for an inﬁnite set of variables X, f, g, h function
symbols from Ω, a, b, c constants from Ω and x, y, z variables from X. The func-
tion comp denotes the complement of a literal. We write s ̸≈t as a shortcut for

232
H. Leidinger and C. Weidenbach
¬(s ≈t). The literal s # t may denote both s ≈t and s ̸≈t. The semantics of
ﬁrst-order logic and semantic entailment |= is deﬁned as usual.
By σ, τ, δ we denote substitutions, which are total mappings from variables to
terms. Let σ be a substitution, then its ﬁnite domain is deﬁned as dom(σ) := {x |
xσ ̸= x} and its codomain is deﬁned as codom(σ) = {t | xσ = t, x ∈dom(σ)}.
We extend their application to literals, clauses and sets of such objects in the
usual way. A term, literal, clause or sets of these objects is ground if it does
not contain any variable. A substitution σ is ground if codom(σ) is ground. A
substitution σ is grounding for a term t, literal L, clause C if tσ, Lσ, Cσ is
ground, respectively. By C· σ, L· σ we denote a closure consisting of a clause C,
literal L and a grounding substitution σ, respectively. The function gnd computes
the set of all ground instances of a literal, clause, or clause set. The function mgu
denotes the most general uniﬁer of terms, atoms, literals, respectively. We assume
that mgus do not introduce fresh variables and that they are idempotent.
The set of positions pos(L) of a literal (term pos(t)) is inductively deﬁned as
usual. The notion L|p denotes the subterm of a literal L (t|p for term t) at position
p ∈pos(L) (p ∈pos(t)). The replacement of a subterm of a literal L (term t)
at position p ∈pos(L) (p ∈pos(t)) by a term s is denoted by L[s]p (t[s]p). For
example, the term f(a, g(x)) has the positions {ϵ, 1, 2, 21}, f(a, g(x))|21 = x and
f(a, g(x))[b]2 denotes the term f(a, b).
Let R be a set of rewrite rules l →r, called a term rewrite system (TRS).
The rewrite relation →R⊆T(Ω, X) × T(Ω, X) is deﬁned as usual by s →R t if
there exists (l →r) ∈R, p ∈pos(s), and a matcher σ, such that s|p = lσ and
t = s[rσ]p. We write s = t↓R if s is the normal form of t in the rewrite relation
→R. We write s # t = (s′ # t′)↓R if s is the normal form of s′ and t is the normal
form of t′. A rewrite relation is terminating if there is no inﬁnite descending chain
t0 →t1 →... and conﬂuent if t ∗←s →∗t′ implies t ↔∗t′. A rewrite relation is
convergent if it is terminating and conﬂuent. A rewrite order is a irreﬂexive and
transitive rewrite relation. A TRS R is terminating, conﬂuent, convergent, if the
rewrite relation →R is terminating, conﬂuent, convergent, respectively. A term t
is called irreducible by a TRS R if no rule from R rewrites t. Otherwise it is called
reducible. A literal, clause is irreducible if all of its terms are irreducible, and
reducible otherwise. A substitution σ is called irreducible if any t ∈codom(σ) is
irreducible, and reducible otherwise.
Let ≺T denote a well-founded rewrite ordering on terms which is total on
ground terms and for all ground terms t there exist only ﬁnitely many ground
terms s ≺T t. We call ≺T a desired term ordering. We extend ≺T to equations by
assigning the multiset {s, t} to positive equations s ≈t and {s, s, t, t} to inequa-
tions s ̸≈t. Furthermore, we identify ≺T with its multiset extension comparing
multisets of literals. For a (multi)set of terms {t1, . . . , tn} and a term t, we deﬁne
{t1, . . . , tn} ≺T t if {t1, . . . , tn} ≺T {t}. For a (multi)set of Literals {L1, . . . , Ln}
and a term t, we deﬁne {L1, . . . , Ln} ≺T t if {L1, . . . , Ln} ≺T {{t}}. Given a
ground term β then gnd≺T β computes the set of all ground instances of a lit-
eral, clause, or clause set where the groundings are smaller than β according to

SCL(EQ): SCL for First-Order Logic with Equality
233
the ordering ≺T . Given a set (sequence) of ground literals Γ let conv(Γ) be a
convergent rewrite system out of the positive equations in Γ using ≺T .
Let ≺be a well-founded, total, strict ordering on ground literals, which is
lifted to clauses and clause sets by its respective multiset extension. We overload
≺for literals, clauses, clause sets if the meaning is clear from the context. The
ordering is lifted to the non-ground case via instantiation: we deﬁne C ≺D
if for all grounding substitutions σ it holds Cσ ≺Dσ. Then we deﬁne ⪯as
the reﬂexive closure of ≺and N ⪯C := {D | D ∈N and D ⪯C} and use the
standard superposition style notion of redundancy [2].
Deﬁnition 1 (Clause Redundancy). A ground clause C is redundant with
respect to a set N of ground clauses and an ordering ≺if N ⪯C |= C. A clause
C is redundant with respect to a clause set N and an ordering ≺if for all
C′ ∈gnd(C), C′ is redundant with respect to gnd(N).
3
The SCL(EQ) Calculus
We start the introduction of the calculus by deﬁning the ingredients of an
SCL(EQ) state.
Deﬁnition 2 (Trail). A trail Γ
:= [Li1:C1·σ1
1
, ..., Lin:Cn·σn
n
] is a consistent
sequence of ground equations and inequations where Lj is annotated by a level
ij with ij−1 ≤ij, and a closure Cj· σj. We omit the annotations if they are not
needed in a certain context. A ground literal L is true in Γ if Γ |= L. A ground lit-
eral L is false in Γ if Γ |= comp(L). A ground literal L is undeﬁned in Γ if Γ ̸|= L
and Γ ̸|= comp(L). Otherwise it is deﬁned. For each literal Lj in Γ it holds that
Lj is undeﬁned in [L1, ..., Lj−1] and irreducible by conv({L1, ..., Lj−1}).
The above deﬁnition of truth and undeﬁnedness is extended to clauses in the
obvious way. The notions of true, false, undeﬁned can be parameterized by a
ground term β by saying that L is β-undeﬁned in a trail Γ if β ≺T L or L is
undeﬁned. The notions of a β-true, β-false term are restrictions of the above
notions to literals smaller β, respectively. All SCL(EQ) reasoning is layered with
respect to a ground term β.
Deﬁnition 3. Let Γ be a trail and L a ground literal such that L is deﬁned in
Γ. By core(Γ; L) we denote a minimal subsequence Γ ′ ⊆Γ such that L is deﬁned
in Γ ′. By cores(Γ; L) we denote the set of all cores.
Note that core(Γ; L) is not necessarily unique. There can be multiple cores
for a given trail Γ and ground literal L.
Deﬁnition 4 (Trail Ordering). Let Γ := [L1, ..., Ln] be a trail. The (partial)
trail ordering ≺Γ is the sequence ordering given by Γ, i.e., Li ≺Γ Lj if i < j for
all 1 ≤i, j ≤n.

234
H. Leidinger and C. Weidenbach
Deﬁnition 5 (Deﬁning Core and Deﬁning Literal). For a trail Γ and
a sequence of literals Δ ⊆Γ we write max≺Γ (Δ) for the largest literal in Δ
according to the trail ordering ≺Γ . Let Γ be a trail and L a ground literal such
that L is deﬁned in Γ. Let Δ ∈cores(Γ; L) be a sequence of literals where
max≺Γ (Δ) ⪯Γ max≺Γ (Λ) for all Λ ∈cores(Γ; L), then maxΓ (L) := max≺Γ (Δ)
is called the deﬁning literal and Δ is called a deﬁning core for L in Γ. If
cores(Γ; L) contains only the empty core, then L has no deﬁning literal and
no deﬁning core.
Note that there can be multiple deﬁning cores but only one deﬁning literal for
any deﬁned literal L. For example, consider a trail Γ := [f(a) ≈f(b)1:C1·σ1, a ≈
b2:C2·σ2, b ≈c3:C3·σ3] with an ordering ≺T that orders the terms of the equations
from left to right, and a literal g(f(a)) ≈g(f(c)). Then the deﬁning cores are
Δ1 := [a ≈b, b ≈c] and Δ2 := [f(a) ≈f(b), b ≈c]. The deﬁning literal, however,
is in both cases b ≈c. Deﬁned literals that have no deﬁning core and therefore no
deﬁning literal are literals that are trivially false or true. Consider, for example,
g(f(a)) ≈g(f(a)). This literal is trivially true in Γ. Thus an empty subset of Γ
is suﬃcient to show that g(f(a)) ≈g(f(a)) is deﬁned in Γ.
Deﬁnition 6 (Literal Level). Let Γ be a trail. A ground literal L ∈Γ is of
level i if L is annotated with i in Γ. A deﬁned ground literal L ̸∈Γ is of level i
if the deﬁning literal of L is of level i. If L has no deﬁning literal, then L is of
level 0. A ground clause D is of level i if i is the maximum level of a literal in
D.
The restriction to minimal subsequences for the deﬁning literal and deﬁni-
tion of a level eventually guarantee that learned clauses are smaller in the trail
ordering. This enables completeness in combination with learning non-redundant
clauses as shown later.
Lemma 7. Let Γ1 be a trail and K a deﬁned literal that is of level i in Γ1. Then
K is of level i in a trail Γ := Γ1, Γ2.
Deﬁnition 8. Let Γ be a trail and L ∈Γ a literal. L is called a decision literal
if Γ = Γ0, Ki:C·τ, Li+1:C′·τ ′, Γ1. Otherwise L is called a propagated literal.
In our above example g(f(a)) ≈g(f(c)) is of level 3 since the deﬁning literal
b ≈c is annotated with 3. a ̸≈b on the other hand is of level 2.
We deﬁne a well-founded total strict ordering which is induced by the trail
and with which non-redundancy is proven in Sect. 4. Unlike SCL [14,18] we
use this ordering for the inference rules as well. In previous SCL calculi, conﬂict
resolution automatically chooses the greatest literal and resolves with this literal.
In SCL(EQ) this is generalized. Coming back to our running example above,
suppose we have a conﬂict clause f(b) ̸≈f(c)∨b ̸≈c. The deﬁning literal for both
inequations is b ≈c. So we could do paramodulation inferences with both literals.
The following ordering makes this non-deterministic choice deterministic.

SCL(EQ): SCL for First-Order Logic with Equality
235
Deﬁnition 9 (Trail Induced Ordering).
Let Γ := [Li1:C1·σ1
1
, ..., Lin:Cn·σn
n
]
be a trail, β a ground term such that {L1, ..., Ln} ≺T
β and Mi,j all β-
deﬁned ground literals not contained in Γ ∪comp(Γ): for a deﬁning literal
maxΓ (Mi,j) = Li and for two literals Mi,j, Mi,k we have j < k if Mi,j ≺T Mi,k.
The trail induces a total well-founded strict order ≺Γ ∗on β-deﬁned ground lit-
erals Mk,l, Mm,n, Li, Lj of level greater than zero, where
1. Mi,j ≺Γ ∗Mk,l if i < k or (i = k and j < l)
2. Li ≺Γ ∗Lj if Li ≺Γ Lj
3. comp(Li) ≺Γ ∗Lj if Li ≺Γ Lj
4. Li ≺Γ ∗comp(Lj) if Li ≺Γ Lj or i = j
5. comp(Li) ≺Γ ∗comp(Lj) if Li ≺Γ Lj
6. Li ≺Γ ∗Mk,l, comp(Li) ≺Γ ∗Mk,l if i ≤k
7. Mk,l ≺Γ ∗Li, Mk,l ≺Γ ∗comp(Li) if k < i
and for all β-deﬁned literals L of level zero:
8. ≺Γ ∗:=≺T
9. L ≺Γ ∗K if K is of level greater than zero and K is β-deﬁned
and can eventually be extended to β-undeﬁned ground literals K, H by
10. K ≺Γ ∗H if K ≺T H
11. L ≺Γ ∗H if L is β-deﬁned
The literal ordering ≺Γ ∗is extended to ground clauses by multiset extension and
identiﬁed with ≺Γ ∗as well.
Lemma 10 (Properties of ≺Γ ∗).
1. ≺Γ ∗is well-deﬁned.
2. ≺Γ ∗is a total strict order, i.e. ≺Γ ∗is irreﬂexive, transitive and total.
3. ≺Γ ∗is a well-founded ordering.
Example 11. Assume a trail Γ
:=
[a
≈
b1:C0·σ0, c
≈
d1:C1·σ1, f(a′)
̸≈
f(b′)1:C2·σ2], select KBO as the term ordering ≺T where all symbols have weight
one and a ≺a′ ≺b ≺b′ ≺c ≺d ≺f and a ground term β := f(f(a)). According
to the trail induced ordering we have that a ≈b ≺Γ ∗c ≈d ≺Γ ∗f(a′) ̸≈f(b′)
by 9.2. Furthermore we have that
a ≈b ≺Γ ∗a ̸≈b ≺Γ ∗c ≈d ≺Γ ∗c ̸≈d ≺Γ ∗f(a′) ̸≈f(b′) ≺Γ ∗f(a′) ≈f(b′)
by 9.3 and 9.4. Now for any literal L that is β-deﬁned in Γ and the deﬁning
literal is a ≈b it holds that a ̸≈b ≺Γ ∗L ≺Γ ∗c ≈d by 9.6 and 9.7. This holds
analogously for all literals that are β-deﬁned in Γ and the deﬁning literal is c ≈d
or f(a′) ̸≈f(b′). Thus we get:
L1 ≺Γ ∗... ≺Γ ∗a ≈b ≺Γ ∗a ̸≈b ≺Γ ∗f(a) ≈f(b) ≺Γ ∗f(a) ̸≈f(b) ≺Γ ∗
c ≈d ≺Γ ∗c ̸≈d ≺Γ ∗f(c) ≈f(d) ≺Γ ∗f(c) ̸≈f(d) ≺Γ ∗
f(a′) ̸≈f(b′) ≺Γ ∗f(a′) ≈f(b′) ≺Γ ∗a′ ≈b′ ≺Γ ∗a′ ̸≈b′ ≺Γ ∗K1 ≺Γ ∗. . .
where Ki are the β-undeﬁned literals and Lj are the trivially deﬁned literals.

236
H. Leidinger and C. Weidenbach
Deﬁnition 12 (Rewrite Step). A rewrite step is a ﬁve-tuple (s#t· σ, s#t ∨
C· σ, R, S, p) and inductively deﬁned as follows. The tuple (s#t· σ, s#t ∨
C· σ, ϵ, ϵ, ϵ) is a rewrite step. Given rewrite steps R, S and a position p then
(s#t· σ, s#t∨C· σ, R, S, p) is a rewrite step. The literal s#t is called the rewrite
literal. In case R, S are not ϵ, the rewrite literal of R is an equation.
Rewriting is one of the core features of our calculus. The following deﬁnition
describes a rewrite inference between two clauses. Note that unlike the superpo-
sition calculus we allow rewriting below variable level.
Deﬁnition 13 (Rewrite Inference). Let I1 := (l1 ≈r1· σ1, l1 ≈r1 ∨
C1· σ1, R1, L1, p1) and I2 := (l2#r2· σ2, l2#r2∨C2· σ2, R2, L2, p2) be two variable
disjoint rewrite steps where r1σ1 ≺T l1σ1, (l2#r2)σ2|p = l1σ1 for some position
p. We distinguish two cases:
1. if p ∈pos(l2#r2) and μ := mgu((l2#r2)|p, l1) then (((l2#r2)[r1]p)μ· σ1σ2,
((l2#r2)[r1]p)μ∨C1μ∨C2μ· σ1σ2, I1, I2, p) is the result of a rewrite inference.
2. if p ̸∈pos(l2#r2) then let (l2#r2)δ be the most general instance of l2#r2 such
that p ∈pos((l2#r2)δ), δ introduces only fresh variables and (l2#r2)δσ2ρ =
(l2#r2)σ2 for some minimal ρ. Let μ := mgu((l2#r2)δ|p, l1). Then
((l2#r2)δ[r1]pμ· σ1σ2ρ, (l2#r2)δ[r1]pμ ∨C1μ ∨C2δμ· σ1σ2ρ, I1, I2, p) is the
result of a rewrite inference.
Lemma 14. Let I1 := (l1 ≈r1· σ1, l1 ≈r1 ∨C1· σ1, R1, L1, p1) and I2 :=
(l2#r2· σ2, l2#r2 ∨C2· σ2, R2, L2, p2) be two variable disjoint rewrite steps
where r1σ1 ≺T
l1σ1, (l2#r2)σ2|p = l1σ1 for some position p. Let I3 :=
(l3#r3· σ3, l3#r3 ∨C3· σ3, I1, I2, p) be the result of a rewrite inference. Then:
1. C3σ3 = (C1 ∨C2)σ1σ2 and l3#r3σ3 = (l2#r2)σ2[r1σ1]p.
2. (l3#r3)σ3 ≺T (l2#r2)σ2
3. If N |= (l1 ≈r1 ∨C1) ∧(l2#r2 ∨C2) for some set of clauses N, then N |=
l3#r3 ∨C3
Now that we have deﬁned rewrite inferences we can use them to deﬁne a
reduction chain application and a refutation, which are sequences of rewrite
steps. Intuitively speaking, a reduction chain application reduces a literal in a
clause with literals in conv(Γ) until it is irreducible. A refutation for a literal
L that is β-false in Γ for a given β, is a sequence of rewrite steps with literals
in Γ, L such that ⊥is inferred. Refutations for the literals of the conﬂict clause
will be examined during conﬂict resolution by the rule Explore-Refutation.
Deﬁnition 15 (Reduction Chain). Let Γ be a trail. A reduction chain P
from Γ is a sequence of rewrite steps [I1, ..., Im] such that for each Ii =
(si#ti· σi, si#ti ∨Ci· σi, Ij, Ik, pi) either
1. si#tni:si#ti∨Ci·σ
i
is contained in Γ and Ij = Ik = pi = ϵ or
2. Ii is the result of a rewriting inference from rewrite steps Ij, Ik out of
[I1, ..., Im] where j, k < i.

SCL(EQ): SCL for First-Order Logic with Equality
237
Let (l # r)δo:l # r∨C·δ be an annotated ground literal. A reduction chain appli-
cation from Γ to l # r is a reduction chain [I1, ..., Im] from Γ, (l # r)δo:l # r∨C·δ
such that lδ↓conv(Γ ) = smσm and rδ↓conv(Γ ) = tmσm. We assume reduction
chain applications to be minimal, i.e., if any rewrite step is removed from the
sequence it is no longer a reduction chain application.
Deﬁnition 16 (Refutation). Let Γ be a trail and (l # r)δo:l # r∨C·δ an anno-
tated ground literal that is β-false in Γ for a given β. A refutation P from
Γ and l # r is a reduction chain [I1, ..., Im] from Γ, (l # r)δo:l # r∨C·δ such that
(sm#tm)σm = s ̸≈s for some s. We assume refutations to be minimal, i.e.,
if any rewrite step Ik, k < m is removed from the refutation, it is no longer a
refutation.
3.1
The SCL(EQ) Inference Rules
We can now deﬁne the rules of our calculus based on the previous deﬁnitions.
A state is a six-tuple (Γ; N; U; β; k; D) similar to the SCL calculus, where Γ a
sequence of annotated ground literals, N and U the sets of initial and learned
clauses, β is a ground term such that for all L ∈Γ it holds L ≺T β, k is
the decision level, and D a status that is ⊤, ⊥or a closure C · σ. Before we
propagate or decide any literal, we make sure that it is irreducible in the current
trail. Together with the design of ≺Γ ∗this eventually enables rewriting as a
simpliﬁcation rule.
Propagate
(Γ; N; U; β; k; ⊤) ⇒SCL(EQ)
(Γ, sm#tmσk:(sm#tm∨Cm)·σm
m
; N; U; β; k; ⊤)
provided there is a C ∈(N ∪U), σ grounding for C, C = C0∨C1∨L, Γ |= ¬C0σ,
C1σ = Lσ∨...∨Lσ, C1 = L1 ∨...∨Ln, μ = mgu(L1, ..., Ln, L) Lσ is β-undeﬁned
in Γ, (C0 ∨L)μσ ≺T β, σ is irreducible by conv(Γ), [I1, . . . , Im] is a reduction
chain application from Γ to Lσk:(L∨C0)μ·σ where Im = (sm#tm· σm, sm#tm ∨
Cm· σm, Ij, Ik, pm).
Note that the deﬁnition of Propagate also includes the case where Lσ is
irreducible by Γ. In this case L = sm#tm and m = 1. The rule Decide below,
is similar to Propagate, except for the subclause C0 which must be β-undeﬁned
or β-true in Γ, i.e., Propagate cannot be applied and the decision literal is
annotated by a tautology.
Decide
(Γ; N; U; β; k; ⊤) ⇒SCL(EQ)
(Γ, sm#tmσk+1:(sm#tm∨comp(sm#tm))·σm
m
; N; U;
β; k + 1; ⊤)
provided there is a C ∈(N ∪U), σ grounding for C, C = C0 ∨L, C0σ is
β-undeﬁned or β-true in Γ, Lσ is β-undeﬁned in Γ, (C0 ∨L)σ ≺T β, σ is
irreducible by conv(Γ), [I1, . . . , Im] is a reduction chain application from Γ to
Lσk+1:L∨C0·σ where Im = (sm#tm· σm, sm#tm ∨Cm· σm, Ij, Ik, pm).

238
H. Leidinger and C. Weidenbach
Conﬂict
(Γ; N; U; β; k; ⊤)⇒SCL(EQ)
(Γ; N; U; β; k; D)
provided there is a D′ ∈(N ∪U), σ grounding for D′, D′σ is β-false in Γ, σ is
irreducible by conv(Γ), D = ⊥if D′σ is of level 0 and D = D′· σ otherwise.
For the non-equational case, when a conﬂict clause is found by an SCL calcu-
lus [14,18], the complements of its ﬁrst-order ground literals are contained in the
trail. For equational literals this is not the case, in general. The proof showing
D to be β-false with respect to Γ is a rewrite proof with respect to conv(Γ).
This proof needs to be analyzed to eventually perform paramodulation steps on
D or to replace D by a ≺Γ ∗smaller β-false clause showing up in the proof.
Skip
(Γ, Kl:C·τ, Lk:C′·τ ′; N; U; β; k; D · σ)⇒SCL(EQ)
(Γ, Kl:C·τ; N; U; β; l; D · σ)
if
Dσ is β-false in Γ, Kl:C·τ.
The Explore-Refutation rule is the FOL with Equality counterpart to the
resolve rule in CDCL or SCL. While in CDCL or SCL complementary literals of
the conﬂict clause are present on the trail and can directly be used for resolution
steps, this needs a generalization for FOL with Equality. Here, in general, we need
to look at (rewriting) refutations of the conﬂict clause and pick an appropriate
clause from the refutation as the next conﬂict clause.
Explore-Refutation
(Γ, L; N; U; β; k; (D∨s # t)· σ)) ⇒SCL(EQ)
(Γ, L; N; U; β; k; (sj#tj∨Cj)· σj)
if (s # t)σ is strictly ≺Γ ∗maximal in (D ∨s # t)σ, L is the deﬁning literal of
(s # t)σ, [I1, ..., Im] is a refutation from Γ and (s # t)σ, Ij = (sj#tj· σj, (sj#tj ∨
Cj)· σj, Il, Ik, pj), 1 ≤j ≤m, (sj # tj ∨Cj)σj ≺Γ ∗(D ∨s # t)σ, (sj#tj ∨Cj)σj
is β-false in Γ.
Factorize
(Γ; N; U; β; k; (D ∨L ∨L′) · σ) ⇒SCL(EQ)
(Γ; N; U; β; k; (D ∨L)μ · σ)
provided Lσ = L′σ, and μ = mgu(L, L′).
Equality-Resolution
(Γ; N; U; β; k; (D ∨s ̸≈s′)· σ)⇒SCL(EQ)
(Γ; N; U; β; k; Dμ · σ)
provided sσ = s′σ, μ = mgu(s, s′).
Backtrack
(Γ, K, Γ ′; N; U; β; k; (D ∨L) · σ) ⇒SCL(EQ)
(Γ; N; U ∪{D ∨L}; β; j −i; ⊤)
provided Dσ is of level i′ where i′ < k, K is of level j and Γ, K the minimal trail
subsequence such that there is a grounding substitution τ with (D ∨L)τ β-false
in Γ, K but not in Γ; i = 1 if K is a decision literal and i = 0 otherwise.
Grow
(Γ; N; U; β; k; ⊤) ⇒SCL(EQ)
(ϵ; N; U; β′; 0; ⊤)
provided β ≺T β′.

SCL(EQ): SCL for First-Order Logic with Equality
239
In addition to soundness and completeness of the SCL(EQ) rules their
tractability in practice is an important property for a successful implementa-
tion. In particular, ﬁnding propagating literals or detecting a false clause under
some grounding. It turns out that these operations are NP-complete, similar to
ﬁrst-order subsumption which has been shown to be tractable in practice.
Lemma 17. Assume that all ground terms t with t ≺T β for any β are poly-
nomial in the size of β. Then testing Propagate (Conﬂict) is NP-Complete, i.e.,
the problem of checking for a given clause C whether there exists a grounding
substitution σ such that Cσ propagates (is false) is NP-Complete.
Example 18 (SCL(EQ) vs. Superposition: Saturation). Consider the following
clauses:
N := {C1 := c ≈d ∨D, C2 := a ≈b ∨c ̸≈d, C3 := f(a) ̸≈f(b) ∨g(c) ̸≈g(d)}
where again we assume a KBO with all symbols having weight one, precedence
d ≺c ≺b ≺a ≺g ≺f and β := f(f(g(a))). Suppose that we ﬁrst decide
c ≈d and then propagate a ≈b: Γ = [c ≈d1:c≈d∨c̸≈d, a ≈b1:C2]. Now we have a
conﬂict with C3. Explore-Refutation applied to the conﬂict clause C3 results in a
paramodulation inference between C3 and C2. Another application of Equality-
Resolution gives us the new conﬂict clause C4 := c ̸≈d∨g(c) ̸≈g(d). Now we can
Skip the last literal on the trail, which gives us Γ = [c ≈d1:c≈d∨c̸≈d]. Another
application of the Explore-Refutation rule to C4 using the decision justiﬁcation
clause followed by Equality-Resolution and Factorize gives us C5 := c ̸≈d. Thus
with SCL(EQ) the following clauses remain:
C′
1 = D
C5 = c ̸≈d
C3 = f(a) ̸≈f(b) ∨g(c) ̸≈g(d)
where we derived C′
1 out of C1 by subsumption resolution [33] using C5. Actually,
subsumption resolution is compatible with the general redundancy notion of
SCL(EQ), see Lemma 25. Now we consider the same example with superposition
and the very same ordering (Ni is the clause set of the previous step and N0 the
initial clause set N).
N0 ⇒Sup(C2,C3) N1 ∪{C4 := c ̸≈d ∨g(c) ̸≈g(d)}
⇒Sup(C1,C4) N2 ∪{C5 := c ̸≈d ∨D} ⇒Sup(C1,C5) N3 ∪{C6 := D}
Thus superposition ends up with the following clauses:
C2 = a ≈b ∨c ̸≈d
C3 = f(a) ̸≈f(b) ∨g(c) ̸≈g(d)
C4 = c ̸≈d ∨g(c) ̸≈g(d) C6 = D
The superposition calculus generates more and larger clauses.
Example 19 (SCL(EQ) vs. Superposition: Refutation). Suppose the following set
of clauses: N := {C1 := f(x) ̸≈a ∨f(x) ≈b, C2 := f(f(y)) ≈y, C3 := a ̸≈b}
where again we assume a KBO with all symbols having weight one, precedence

240
H. Leidinger and C. Weidenbach
b ≺a ≺f and β := f(f(f(a))). A long refutation by the superposition calculus
results in the following (Ni is the clause set of the previous step and N0 the
initial clause set N):
N0 ⇒Sup(C1,C2) N1 ∪{C4 := y ̸≈a ∨f(f(y)) ≈b}
⇒Sup(C1,C4) N2 ∪{C5 := a ̸≈b ∨f(f(y)) ≈b ∨y ̸≈a}
⇒Sup(C2,C5) N3 ∪{C6 := a ̸≈b ∨b ≈y ∨y ̸≈a}
⇒Sup(C2,C4) N4 ∪{C7 := y ≈b ∨y ̸≈a}
⇒EqRes(C7) N5 ∪{C8 := a ≈b} ⇒Sup(C3,C8) N6 ∪{⊥}
The shortest refutation by the superposition calculus is as follows:
N0 ⇒Sup(C1,C2) N1 ∪{C4 := y ̸≈a ∨f(f(y)) ≈b}
⇒Sup(C2,C4) N2 ∪{C5 := y ≈b ∨y ̸≈a}
⇒EqRes(C5) N3 ∪{C6 := a ≈b} ⇒Sup(C3,C6) N4 ∪{⊥}
In SCL(EQ) on the other hand we would always ﬁrst propagate a ̸≈b, f(f(a)) ≈
a and f(f(b)) ≈b. As soon as a ̸≈b and f(f(a)) ≈a are propagated we have a
conﬂict with C1{x →f(a)}. So suppose in the worst case we propagate:
Γ := [a ̸≈b0:a̸≈b, f(f(b)) ≈b0:(f(f(y))≈y){y→b}, f(f(a)) ≈a0:(f(f(y))≈y){y→a}]
Now we have a conﬂict with C1{x →f(a)}. Since there is no decision literal on
the trail, Conﬂict rule immediately returns ⊥and we are done.
4
Soundness and Completeness
In this section we show soundness and refutational completeness of SCL(EQ)
under the assumption of a regular run. We provide the deﬁnition of a regular run
and show that for a regular run all learned clauses are non-redundant according
to our trail induced ordering. We start with the deﬁnition of a sound state.
Deﬁnition 20. A state (Γ; N; U; β; k; D) is sound if the following conditions
hold:
1. Γ is a consistent sequence of annotated literals,
2. for each decomposition Γ = Γ1, Lσi:(C∨L)·σ, Γ2 where Lσ is a propagated lit-
eral, we have that Cσ is β-false in Γ1, Lσ is β-undeﬁned in Γ1 and irreducible
by conv(Γ1), N ∪U |= (C ∨L) and (C ∨L)σ ≺T β,
3. for each decomposition Γ = Γ1, Lσi:(L∨comp(L))·σ, Γ2 where Lσ is a decision
literal, we have that Lσ is β-undeﬁned in Γ1 and irreducible by conv(Γ1),
N ∪U |= (L ∨comp(L)) and (L ∨comp(L))σ ≺T β,
4. N |= U,
5. if D = C · σ, then Cσ is β-false in Γ, N ∪U |= C,
Lemma 21. The initial state (ϵ; N; ∅; β; 0; ⊤) is sound.
Deﬁnition 22. A run is a sequence of applications of SCL(EQ) rules starting
from the initial state.

SCL(EQ): SCL for First-Order Logic with Equality
241
Theorem 23. Assume a state (Γ; N; U; β; k; D) resulting from a run. Then
(Γ; N; U; β; k; D) is sound.
Next, we give the deﬁnition of a regular run. Intuitively speaking, in a regular
run we are always allowed to do decisions except if
1. a literal can be propagated before the ﬁrst decision and
2. the negation of a literal can be propagated.
To ensure non-redundant learning we enforce at least one application of Skip
during conﬂict resolution except for the special case of a conﬂict after a decision.
Deﬁnition 24 (Regular Run). A run is called regular if
1. the rules Conﬂict and Factorize have precedence over all other rules,
2. If k = 0 in a state (Γ; N; U; β; k; D), then Propagate has precedence over
Decide,
3. If an annotated literal Lk:C·σ could be added by an application of Propagate
on Γ in a state (Γ; N; U; β; k; D) and C ∈N ∪U, then the annotated literal
comp(L)k+1:C′·σ′ is not added by Decide on Γ,
4. during conﬂict resolution Skip is applied at least once, except if Conﬂict is
applied immediately after an application of Decide.
5. if Conﬂict is applied immediately after an application of Decide, then Back-
track is only applied in a state (Γ, L′; N; U; β; k; D· σ) if Lσ = comp(L′) for
some L ∈D.
Now we show that any learned clause in a regular run is non-redundant
according to our trail induced ordering.
Lemma 25 (Non-Redundant Clause Learning).
Let N be a clause set.
The clauses learned during a regular run in SCL(EQ) are not redundant with
respect to ≺Γ ∗and N ∪U. For the trail only non-redundant clauses need to be
considered.
The proof of Lemma 25 is based on the fact that conﬂict resolution eventually
produces a clause smaller then the original conﬂict clause with respect to ≺Γ ∗.
All simpliﬁcations, e.g., contextual rewriting, as deﬁned in [2,20,33,35–37], are
therefore compatible with Lemma 25 and may be applied to the newly learned
clause as long as they respect the induced trail ordering. In detail, let Γ be the
trail before the application of rule Backtrack. The newly learned clause can be
simpliﬁed according to the induced trail ordering ≺Γ ∗as long as the simpliﬁed
clause is smaller with respect to ≺Γ ∗.
Another important consequence of Lemma 25 is that newly learned clauses
need not to be considered for redundancy. Furthermore, the SCL(EQ) calculus
always terminates, Lemma 33, because there only ﬁnitely many non-redundant
clauses with respect to a ﬁxed β.
For dynamic redundancy, we have to consider the fact that the induced trail
ordering changes. At this level, only redundancy criteria and simpliﬁcations that

242
H. Leidinger and C. Weidenbach
are compatible with all induced trail orderings may be applied. Due to the
construction of the induced trail ordering, it is compatible with ≺T for unit
clauses.
Lemma 26 (Unit Rewriting).
Assume a state (Γ; N; U; β; k; D) resulting
from a regular run where the current level k > 0 and a unit clause l ≈r ∈N.
Now assume a clause C ∨L[l′]p ∈N such that l′ = lμ for some matcher μ. Now
assume some arbitrary grounding substitutions σ′ for C ∨L[l′]p, σ for l ≈r such
that lσ = l′σ′ and rσ ≺T lσ. Then (C ∨L[rμσσ′]p)σ′ ≺Γ ∗(C ∨L[l′]p)σ′.
In addition, any notion that is based on a literal subset relationship is also
compatible with ordering changes. The standard example is subsumption.
Lemma 27. Let C, D be two clauses. If there exists a substitution σ such that
Cσ ⊂D, then D is redundant with respect to C and any ≺Γ ∗.
The notion of redundancy, Deﬁnition 1, only supports a strict subset relation
for Lemma 27, similar to the superposition calculus. However, the newly gener-
ated clauses of SCL(EQ) are the result of paramodulation inferences [28]. In a
recent contribution to dynamic, abstract redundancy [32] it is shown that also
the non-strict subset relation in Lemma 27, i.e., Cσ ⊆D, preserves completeness.
If all stuck states, see below Deﬁnition 28, with respect to a ﬁxed β are visited
before increasing β then this provides a simple dynamic fairness strategy.
When unit reduction or any other form of supported rewriting is applied to
clauses smaller than the current β, it can be applied independently from the
current trail. If, however, unit reduction is applied to clauses larger than the
current β then the calculus must do a restart to its initial state, in particular
the trail must be emptied, as for otherwise rewriting may result generating a
conﬂict that did not exist with respect to the current trail before the rewriting.
This is analogous to a restart in CDCL once a propositional unit clause is derived
and used for simpliﬁcation. More formally, we add the following new Restart rule
to the calculus to reset the trail to its initial state after a unit reduction.
Restart
(Γ; N; U; β; k; ⊤) ⇒SCL(EQ) (ϵ; N; U; β; 0; ⊤)
Next we show refutation completeness of SCL(EQ). To achieve this we ﬁrst
give a deﬁnition of a stuck state. Then we show that stuck states only occur if
all ground literals L ≺T β are β-deﬁned in Γ and not during conﬂict resolution.
Finally we show that conﬂict resolution will always result in an application of
Backtrack. This allows us to show termination (without application of Grow)
and refutational completeness.
Deﬁnition 28 (Stuck State).
A state (Γ; N; U; β; k; D) is called stuck if
D ̸= ⊥and none of the rules of the calculus, except for Grow, is applicable.
Lemma 29 (Form of Stuck States).
If a regular run (without rule Grow)
ends in a stuck state (Γ; N; U; β; k; D), then D = ⊤and all ground literals
Lσ ≺T β, where L ∨C ∈(N ∪U) are β-deﬁned in Γ.

SCL(EQ): SCL for First-Order Logic with Equality
243
Lemma 30. Suppose a sound state (Γ; N; U; β; k; D) resulting from a regular
run where D ̸∈{⊤, ⊥}. If Backtrack is not applicable then any set of applications
of Explore-Refutation, Skip, Factorize, Equality-Resolution will ﬁnally result
in a sound state (Γ ′; N; U; β; k; D′), where D′ ≺Γ ∗D. Then Backtrack will be
ﬁnally applicable.
Corollary 31 (Satisﬁable Clause Sets).
Let N be a satisﬁable clause set.
Then any regular run without rule Grow will end in a stuck state, for any β.
Thus a stuck state can be seen as an indication for a satisﬁable clause set.
Of course, it remains to be investigated whether the clause set is actually satisﬁ-
able. Superposition is one of the strongest approaches to detect satisﬁability and
constitutes a decision procedure for many decidable ﬁrst-order fragments [4,19].
Now given a stuck state and some speciﬁc ordering such as KBO, LPO, or some
polynomial ordering [17], it is decidable whether the ordering can be instantiated
from a stuck state such that Γ coincides with the superposition model operator
on the ground terms smaller than β. In this case it can be eﬀectively checked
whether the clauses derived so far are actually saturated by the superposition
calculus with respect to this speciﬁc ordering. In this sense, SCL(EQ) has the
same power to decide satisﬁability of ﬁrst-order clause sets than superposition.
Deﬁnition 32. A regular run terminates in a state (Γ; N; U; β; k; D) if D = ⊤
and no rule is applicable, or D = ⊥.
Lemma 33. Let N be a set of clauses and β be a ground term. Then any regular
run that never uses Grow terminates.
Lemma 34. If a regular run reaches the state (Γ; N; U; β; k; ⊥) then N is unsat-
isﬁable.
Theorem 35 (Refutational Completeness).
Let N be an unsatisﬁable
clause set, and ≺T a desired term ordering. For any ground term β where
gnd≺T β(N) is unsatisﬁable, any regular SCL(EQ) run without rule Grow will
terminate by deriving ⊥.
5
Discussion
We presented SCL(EQ), a new sound and complete calculus for reasoning in ﬁrst-
order logic with equality. We will now discuss some of its aspects and present
ideas for future work beyond the scope of this paper.
The trail induced ordering, Deﬁnition 9, is the result of letting the calculus
follow the logical structure of the clause set on the literal level and at the same
time supporting rewriting at the term level. It can already be seen by examples on
ground clauses over (in)equations over constants that this combination requires
a layered approach as suggested by Deﬁnition 9, see [24].
In case the calculus runs into a stuck state, i.e., the current trail is a model
for the set of considered ground instances, then the trail information can be

244
H. Leidinger and C. Weidenbach
eﬀectively used for a guided continuation. For example, in order to use the trail
to certify a model, the trail literals can be used to guide the design of a lifted
ordering for the clauses with variables such that propagated trail literals are
maximal in respective clauses. Then it could be checked by superposition, if the
current clause is saturated by such an ordering. If this is not the case, then
there must be a superposition inference larger than the current β, thus giving
a hint on how to extend β. Another possibility is to try to extend the ﬁnite
set of ground terms considered in a stuck state to the inﬁnite set of all ground
terms by building extended equivalence classes following patterns that ensure
decidability of clause testing, similar to the ideas in [14]. If this fails, then again
this information can be used to ﬁnd an appropriate extension term β for rule
Grow.
In contrast to superposition, SCL(EQ) does also inferences below variable
level. Inferences in SCL(EQ) are guided by a false clause with respect to a
partial model assumption represented by the trail. Due to this guidance and the
diﬀerent style of reasoning this does not result in an explosion in the number of
possibly inferred clauses but also rather in the derivation of more general clauses,
see [24].
Currently, the reasoning with solely positive equations is done on and with
respect to the trail. It is well-known that also inferences from this type of rea-
soning can be used to speed up the overall reasoning process. The SCL(EQ)
calculus already provides all information for such a type of reasoning, because it
computes the justiﬁcation clauses for trail reasoning via rewriting inferences. By
an assessment of the quality of these clauses, e.g., their reduction potential with
respect to trail literals, they could also be added, independently from resolving
a conﬂict.
The trail reasoning is currently deﬁned with respect to rewriting. It could
also be performed by congruence closure [26].
Towards an implementation, the aspect of how to ﬁnd interesting ground
decision or propagation literals for the trail can be treated similar to CDCL [11,
21,25,29]. A simple heuristic may be used from the start, like counting the
number of instance relationships of some ground literal with respect to the clause
set, but later on a bonus system can focus the search towards the structure of the
clause sets. Ground literals involved in a conﬂict or the process of learning a new
clause get a bonus or preference. The regular strategy requires the propagation of
all ground unit clauses smaller than β. For an implementation a propagation of
the (explicit and implicit) unit clauses with variables to the trail will be a better
choice. This complicates the implementation of refutation proofs and rewriting
(congruence closure), but because every reasoning is layered by a ground term
β this can still be eﬃciently done.
Acknowledgments. This work was partly funded by DFG grant 389792660 as part
of TRR 248, see https://perspicuous-computing.science. We thank the anonymous
reviewers and Martin Desharnais for their thorough reading, detailed comments, and
corrections.

SCL(EQ): SCL for First-Order Logic with Equality
245
References
1. Alagi, G., Weidenbach, C.: NRCL - a model building approach to the Bernays-
Sch¨onﬁnkel fragment. In: Lutz, C., Ranise, S. (eds.) FroCoS 2015. LNCS (LNAI),
vol. 9322, pp. 69–84. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
24246-0 5
2. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Log. Comput. 4(3), 217–247 (1994)
3. Bachmair, L., Ganzinger, H., Voronkov, A.: Elimination of equality via transforma-
tion with ordering constraints. In: Kirchner, C., Kirchner, H. (eds.) CADE 1998.
LNCS, vol. 1421, pp. 175–190. Springer, Heidelberg (1998). https://doi.org/10.
1007/BFb0054259
4. Bachmair, L., Ganzinger, H., Waldmann, U.: Superposition with simpliﬁcation as
a decision procedure for the monadic class with equality. In: Gottlob, G., Leitsch,
A., Mundici, D. (eds.) KGC 1993. LNCS, vol. 713, pp. 83–96. Springer, Heidelberg
(1993). https://doi.org/10.1007/BFb0022557
5. Baumgartner, P.: Hyper tableau — the next generation. In: de Swart, H. (ed.)
TABLEAUX 1998. LNCS (LNAI), vol. 1397, pp. 60–76. Springer, Heidelberg
(1998). https://doi.org/10.1007/3-540-69778-0 14
6. Baumgartner, P., Fuchs, A., Tinelli, C.: Lemma learning in the model evolution cal-
culus. In: Hermann, M., Voronkov, A. (eds.) LPAR 2006. LNCS (LNAI), vol. 4246,
pp. 572–586. Springer, Heidelberg (2006). https://doi.org/10.1007/11916277 39
7. Baumgartner, P., Furbach, U., Pelzer, B.: Hyper tableaux with equality. In: Pfen-
ning, F. (ed.) CADE 2007. LNCS (LNAI), vol. 4603, pp. 492–507. Springer, Hei-
delberg (2007). https://doi.org/10.1007/978-3-540-73595-3 36
8. Baumgartner, P., Pelzer, B., Tinelli, C.: Model evolution with equality-revised and
implemented. J. Symb. Comput. 47(9), 1011–1045 (2012)
9. Baumgartner, P., Tinelli, C.: The model evolution calculus with equality. In:
Nieuwenhuis, R. (ed.) CADE 2005. LNCS (LNAI), vol. 3632, pp. 392–408. Springer,
Heidelberg (2005). https://doi.org/10.1007/11532231 29
10. Baumgartner, P., Waldmann, U.: Superposition and model evolution combined. In:
Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol. 5663, pp. 17–34. Springer,
Heidelberg (2009). https://doi.org/10.1007/978-3-642-02959-2 2
11. Biere, A., Heule, M., van Maaren, H., Walsh, T. (eds.): Handbook of Satisﬁability,
Frontiers in Artiﬁcial Intelligence and Applications, vol. 185. IOS Press, Amster-
dam (2009)
12. Bonacina, M.P., Furbach, U., Sofronie-Stokkermans, V.: On First-Order Model-
Based Reasoning. In: Mart´ı-Oliet, N., ¨Olveczky, P.C., Talcott, C. (eds.) Logic,
Rewriting, and Concurrency. LNCS, vol. 9200, pp. 181–204. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-23165-5 8
13. Bonacina, M.P., Plaisted, D.A.: SGGS theorem proving: an exposition. In: Schulz,
S., Moura, L.D., Konev, B. (eds.) PAAR-2014. 4th Workshop on Practical Aspects
of Automated Reasoning. EPiC Series in Computing, vol. 31, pp. 25–38. EasyChair
(2015)
14. Bromberger, M., Fiori, A., Weidenbach, C.: Deciding the Bernays-Schoenﬁnkel
fragment over bounded diﬀerence constraints by simple clause learning over the-
ories. In: Henglein, F., Shoham, S., Vizel, Y. (eds.) VMCAI 2021. LNCS, vol.
12597, pp. 511–533. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-
67067-2 23

246
H. Leidinger and C. Weidenbach
15. Davis, M., Logemann, G., Loveland, D.: A machine program for theorem-proving.
Commun. ACM 5(7), 394–397 (1962)
16. Davis, M., Putnam, H.: A computing procedure for quantiﬁcation theory. J. ACM
(JACM) 7(3), 201–215 (1960)
17. Dershowitz, N., Plaisted, D.A.: Rewriting. In: Robinson, A., Voronkov, A. (eds.)
Handbook of Automated Reasoning, vol. I, chap. 9, pp. 535–610. Elsevier (2001)
18. Fiori, A., Weidenbach, C.: SCL clause learning from simple models. In: Fontaine, P.
(ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 233–249. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-29436-6 14
19. Ganzinger, H., de Nivelle, H.: A superposition decision procedure for the guarded
fragment with equality. In: LICS, pp. 295–304 (1999)
20. Gleiss, B., Kov´acs, L., Rath, J.: Subsumption demodulation in ﬁrst-order theo-
rem proving. In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020. LNCS
(LNAI), vol. 12166, pp. 297–315. Springer, Cham (2020). https://doi.org/10.1007/
978-3-030-51074-9 17
21. Bayardo, R.J., Schrag, R.: Using CSP look-back techniques to solve exceptionally
hard SAT instances. In: Freuder, E.C. (ed.) CP 1996. LNCS, vol. 1118, pp. 46–60.
Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61551-2 65
22. Korovin, K.: Inst-Gen – a modular approach to instantiation-based automated
reasoning. In: Voronkov, A., Weidenbach, C. (eds.) Programming Logics. LNCS,
vol. 7797, pp. 239–270. Springer, Heidelberg (2013). https://doi.org/10.1007/978-
3-642-37651-1 10
23. Korovin, K., Sticksel, C.: iProver-Eq: an instantiation-based theorem prover with
equality. In: Giesl, J., H¨ahnle, R. (eds.) IJCAR 2010. LNCS (LNAI), vol. 6173, pp.
196–202. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14203-
1 17
24. Leidinger, H., Weidenbach, C.: SCL(EQ): SCL for ﬁrst-order logic with equality
(2022). arXiv: 2205.08297
25. Moskewicz, M.W., Madigan, C.F., Zhao, Y., Zhang, L., Malik, S.: Chaﬀ: engineer-
ing an eﬃcient SAT solver. In: Proceedings of the Design Automation Conference,
pp. 530–535. ACM (2001)
26. Nelson, G., Oppen, D.C.: Fast decision procedures based on congruence closure. J.
ACM 27(2), 356–364 (1980)
27. Plaisted, D.A., Zhu, Y.: Ordered semantic hyper-linking. J. Autom. Reason. 25(3),
167–217 (2000)
28. Robinson, G., Wos, L.: Paramodulation and theorem-proving in ﬁrst-order theories
with equality. In: Meltzer, B., Michie, D. (eds.) Machine Intelligence 4, pp. 135–150
(1969)
29. Silva, J.P.M., Sakallah, K.A.: GRASP - a new search algorithm for satisﬁability.
In: International Conference on Computer Aided Design, ICCAD, pp. 220–227.
IEEE Computer Society Press (1996)
30. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure - from CNF
to th0, TPTP v6.4.0. J. Autom. Reasoning 59(4), 483–502 (2017)
31. Teucke, A.: An approximation and reﬁnement approach to ﬁrst-order automated
reasoning. Doctoral thesis, Saarland University (2018)
32. Waldmann, U., Tourret, S., Robillard, S., Blanchette, J.: A comprehensive frame-
work for saturation theorem proving. In: Peltier, N., Sofronie-Stokkermans, V.
(eds.) IJCAR 2020. LNCS (LNAI), vol. 12166, pp. 316–334. Springer, Cham (2020).
https://doi.org/10.1007/978-3-030-51074-9 18

SCL(EQ): SCL for First-Order Logic with Equality
247
33. Weidenbach, C.: Combining superposition, sorts and splitting. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. 2, chap. 27, pp. 1965–
2012. Elsevier (2001)
34. Weidenbach, C.: Automated reasoning building blocks. In: Meyer, R., Platzer,
A., Wehrheim, H. (eds.) Correct System Design. LNCS, vol. 9360, pp. 172–188.
Springer, Cham (2015). https://doi.org/10.1007/978-3-319-23506-6 12
35. Weidenbach,
C.,
Wischnewski,
P.:
Contextual
rewriting
in
SPASS.
In:
PAAR/ESHOL. CEUR Workshop Proceedings, vol. 373, pp. 115–124. Australien,
Sydney (2008)
36. Weidenbach, C., Wischnewski, P.: Subterm contextual rewriting. AI Commun.
23(2–3), 97–109 (2010)
37. Wischnewski, P.: Eﬀcient Reasoning Procedures for Complex First-Order Theories.
Ph.D. thesis, Saarland University, November 2012
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Term Orderings for Non-reachability
of (Conditional) Rewriting
Akihisa Yamada(B)
National Institute of Advanced Industrial Science and Technology, Tokyo, Japan
akihisa.yamada@aist.go.jp
Abstract. We propose generalizations of reduction pairs, well-establis-
hed techniques for proving termination of term rewriting, in order to
prove unsatisﬁability of reachability (infeasibility) in plain and condi-
tional term rewriting. We adapt the weighted path order, a merger of the
Knuth–Bendix order and the lexicographic path order, into the proposed
framework. The proposed approach is implemented in the termination
prover NaTT, and the strength of our approach is demonstrated through
examples and experiments.
1
Introduction
In the research area of term rewriting, among the most well-studied topics are
termination, conﬂuence, and reachability analyses.
In termination analysis, a crucial task used to be to design reduction orders,
well-founded orderings over terms that are closed under contexts and sub-
stitutions. Well-known examples of such orderings include the Knuth–Bendix
ordering [14], polynomial interpretations [18], multiset/lexicographic path order-
ing [4,13], and matrix interpretations [5]. The dependency pair framework gen-
eralized reduction orders into reduction pairs [2,9,12], and there are a number
of implementations that automatically ﬁnd reduction pairs, e.g., AProVE [7],
TTT2 [16], MU-TERM [11], NaTT [35], competing in the International Termina-
tion Competition [8].
Traditional reachability analysis (cf. [6]) has been concerned with the pos-
sibility of rewriting a given source term s to a target t, where variables in the
terms are treated as constants. There is an increasing need for solving a more
general question: is it possible to instantiate variables so that the instance of s
rewrites to the instance of t? Let us illustrate the problem with an elementary
example.
Example 1. Consider the following TRS encoding addition of natural numbers:
Radd := { add(0, y) →y, add(s(x), y) →s(add(x, y)) }
The reachability constraint add(s(x), y) ↠y represents the possibility of rewrit-
ing from add(s(x), y) to y, where variables x and y can be arbitrary terms.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 248–267, 2022.
https://doi.org/10.1007/978-3-031-10769-6_15

Term Orderings for Non-reachability of (Conditional) Rewriting
249
This (un)satisﬁability problem of reachability, also called (in)feasibility, plays
important roles in termination [24] and conﬂuence analyses of (conditional)
rewriting [21]. A tool competition dedicated for this problem has been founded
as the infeasibility (INF) category in the International Conﬂuence Competition
(CoCo) since 2019 [25].
In this paper, we propose a new method for proving unsatisﬁability of reach-
ability, using the term ordering techniques developed for termination analysis.
Speciﬁcally, in Sect. 3, we ﬁrst generalize reduction pairs to rewrite pairs, and
show that they can be used for proving unsatisﬁability of reachability. We further
generalize the notion to co-rewrite pairs, yielding a sound and complete method.
The power of the proposed method is demonstrated by importing (relaxed)
semantic term orderings from termination analysis.
In order to import also syntactic term orderings, in Sect. 4 we identify a
condition when the weighted path order (WPO) [36] forms a rewrite pair. Since
KBO and LPO are instances of WPO, we see that these orderings can also be
used in our method. In Sect. 5 we also present how to derive co-rewrite pairs
from WPO.
In Sect. 6, we adapt the approach into conditional rewriting. Section 7 reports
on the implementation and experiments conducted on examples in the paper and
the benchmark set of CoCo 2021.
Related Work Our rewrite pairs are essentially Aoto’s discrimination pairs [1]
which are closed under substitutions. On way of disproving conﬂuence, Aoto
introduced discrimination pairs and used them in proving non-joinability. The
joinability of terms s and t is expressed as ∃u. s →∗
R u ←∗
R t, while the current
paper is concerned with ∃θ. sθ →∗
R tθ. As substitutions are not considered,
discrimination pairs do not need closure under substitutions, and Aoto’s insights
are mainly for dealing with the reverse rewriting ←∗
R.
Lucas and Guti´errez [19] proposed reducing infeasibility to the model ﬁnding
of ﬁrst-order logic. Our formulations especially in Sect. 6 are similar to theirs.
A crucial diﬀerence is that, while they encode the closure properties and order
properties into logical formulas and delegate these tasks to the background the-
ory solvers, we ensure these properties by means of reduction pairs, for which
well-established techniques exist in the literature.
Sternagel and Yamada [30] proposed a framework for analyzing reachability
by combining basic logical manipulations, and Guti´errez and Lucas [10] proposed
another framework, similar to the dependency pair framework. The present work
focuses on atomic analysis techniques, and is orthogonal to these eﬀorts of com-
bining techniques.
2
Preliminaries
We assume familiarity with term rewriting, cf. [3] or [32]. For a binary relation
denoted by a symbol like ⊐, we denote its dual relation by ⊏and the negated
relation by ̸⊐. Relation composition is denoted by ◦.

250
A. Yamada
Throughout the paper we ﬁx a set V of variable symbols. A signature is
a set F of function symbols, where each f ∈F is associated with its arity,
the number of arguments. The set of terms built from F and V is denoted by
T (F, V), where a term is either in V or of form f(s1, . . . , sn) where f ∈F is
n-ary and s1, . . . , sn ∈T (F, V). Given a term s ∈T (F, V) and a substitution
θ : V →T (F, V), sθ denotes the term obtained from s by replacing every variable
x by θ(x). A context is a term C ∈T (F, V ∪{□}) where a special variable □
occurs exactly once. Given s ∈T (F, V), we denote by C[s] the term obtained
by replacing □by s in C.
A relation ⊐over terms is closed under substitutions (resp. contexts) iﬀs ⊐t
implies sθ ⊐tθ for any substitution θ (resp. C[s] ⊐C[t] for any context C).
Relations over terms that are closed under contexts and substitutions are called
rewrite relations. Rewrite relations which are also preorders are called rewrite
preorders, and those which are strict orders are rewrite orders. Well-founded
rewrite orders are called reduction orders.
A term rewrite system (TRS) R is a (usually ﬁnite) relation over terms, where
each ⟨l, r⟩∈R is called a rewrite rule and written l →r. We do not require
the usual assumption that l /∈V and variables occurring in r must occur in l.
The rewrite step →R induced by TRS R is the least rewrite relation containing
R. Its reﬂexive transitive closure is denoted by →∗
R, which is the least rewrite
preorder containing R.
A reachability atom is a pair of terms s and t, written s ↠t. We say that
s ↠t is R-satisﬁable iﬀsθ →∗
R tθ for some θ, and R-unsatisﬁable otherwise.
3
Term Orderings for Non-reachability
Reduction pairs constitute the core ingredient in proving termination with
dependency pairs. Just as rewrite orders generalize reduction orders, we ﬁrst
introduce the notion of “rewrite pairs” by removing the well-foundedness
assumption of reduction pairs.
Deﬁnition 1 (rewrite pair). We call a pair ⟨⊒, ⊐⟩of relations an order pair
if ⊒is a preorder, ⊐is irreﬂexive, ⊐⊆⊒, and ⊒◦⊐◦⊒⊆⊐. A rewrite pair
is an order pair ⟨⊒, ⊐⟩over terms such that both ⊒and ⊐are closed under
substitutions and ⊒is closed under contexts. It is called a reduction pair if
moreover ⊐is well-founded.
Standard deﬁnitions of reduction pairs put less order-like assumptions than
the above deﬁnition, but the above (more natural) assumptions do not lose the
generality of previous deﬁnitions [34]. Due to these assumptions, our rewrite pair
satisﬁes the assumption of discrimination pairs [1].
The following statement is our ﬁrst observation: a rewrite pair can prove
non-reachability.
Theorem 1. If ⟨⊒, ⊐⟩is a rewrite pair, R ⊆⊒and s ⊏t, then s ↠t is
R-unsatisﬁable.

Term Orderings for Non-reachability of (Conditional) Rewriting
251
A similar observation has been made [20, Theorem 11], where well-
foundedness is assumed instead of irreﬂexivity. Note that irreﬂexivity is essential:
if s ⊏s for some s, then we have s ⊏s but s ↠s is R-satisﬁable.
The proof of Theorem 1 will be postponed until more general Theorem 2 will
be obtained. Instead, we start with utilizing Theorem 1 by generalizing a classical
way of deﬁning reduction pairs: the semantic approach [23].
Deﬁnition 2 (model). An F-algebra A = ⟨A, [·]⟩speciﬁes a set A called the
carrier and an interpretation [f] : An →A to each n-ary f ∈F. The evaluation
of a term s under assignment α : V →A is deﬁned as usual and denoted by [s]α.
A related/preordered F-algebra ⟨A, ⊐⟩= ⟨A, [·], ⊐⟩consists of an F-algebra
and a relation/preorder ⊐on A. Given α : V →A, we write [s ⊐t]α to mean
[s]α ⊐[t]α. We write A |= s ⊐t if [s ⊐t]α holds for every α : V →A. We
say ⟨A, ⊐⟩is a (relational) model of a TRS R if A |= l ⊐r for every l →
r ∈R. We say ⟨A, ⊐⟩is monotone if ai ⊐a′
i implies [f](a1, . . . , ai, . . . , an) ⊐
[f](a1, . . . , a′
i, . . . , an) for arbitrary a1, . . . , an, a′
i ∈A and n-ary f ∈F.
The notion of relational models is due to van Oostrom [28]. In this paper,
we simply call them models. Models in terms of equational theory are models
⟨A, =⟩in the above deﬁnition, where monotonicity is inherent. Quasi-models of
Zantema [37] are preordered (or partially ordered) monotone models. Theorem 1
can be reformulated in the semantic manner as follows:
Corollary 1. If ⟨≥, >⟩is an order pair, ⟨A, ≥⟩is a monotone model of R, and
A |= s < t, then s ↠t is R-unsatisﬁable.
Note that Corollary 1 does not demand well-foundedness on >. In particular,
one can employ models over negative numbers (or equivalently, positive numbers
with the order pair ⟨≤, <⟩).
Example 2. Consider again the TRS Radd of Example 1. The monotone ordered
F-algebra ⟨Z≤0, [·], ≥⟩deﬁned by
[add](x, y) = x + y
[s](x) = x −1
[0] = 0
is a model of Radd: Whenever x, y ∈Z≤0, we have
[add]([0], y) = y
[add]([s](x), y) = x + y −1 = [s]([add](x, y))
Now we can conclude that the reachability constraint add(s(x), y) ↠y is Radd-
unsatisﬁable by ⟨Z≤0, [·]⟩|= add(s(x), y) < y: Whenever x, y ∈Z≤0, we have
[add]([s](x), y) = x + y −1 < y
Observe that in Theorem 1, ⊐occurs only in the dual form ⊏. Hence we
now directly analyze the condition which ⊒and ⊏should satisfy to prove non-
reachability, and this gives a sound and complete method.

252
A. Yamada
Deﬁnition 3 (co-rewrite pair). We call a pair ⟨⊒, ⊏⟩of relations over terms
a co-rewrite pair, if ⊒is a rewrite preorder, ⊏is closed under substitutions, and
⊒∩⊏= ∅.
Theorem 2. s ↠t is R-unsatisﬁable if and only if there exists a co-rewrite
pair ⟨⊒, ⊏⟩such that R ⊆⊒and s ⊏t.
Proof. For the “if” direction, suppose on the contrary that sθ →∗
R tθ for some θ.
Since ⊒is a rewrite preorder containing R and →∗
R is the least of such, we must
have sθ ⊒tθ. On the other hand, since s ⊏t and ⊏is closed under substitutions,
we have sθ ⊏tθ. This is not possible since ⊒∪⊏= ∅.
For the “only if” direction, take →∗
R as ⊒and deﬁne ⊏by s ⊏t iﬀs ↠t is
R-unsatisﬁable. Then clearly ⊏is closed under substitutions, →∗
R ∩⊏= ∅, and
R ⊆→∗
R.
⊓⊔
Theorem 2 can be more concisely reformulated in the model-oriented manner,
as the greatest choice of ⊏can be speciﬁed: s ⊏t iﬀA |= s ≱t.
Corollary 2. s ↠t is R-unsatisﬁable if and only if there exists a monotone
preordered model ⟨A, ≥⟩of R such that A |= s ≱t.
Corollary 2 is useful when models over non-totally ordered carriers are con-
sidered. There are important methods (for termination) that crucially rely on
such carriers: the matrix interpretations [5], or more generally the tuple inter-
pretations [15,34].
Example 3. Consider the following TRS, where the ﬁrst rule is from [5]:
Rmat = { f(f(x)) →f(g(f(x))), f(x) →x }
The preordered {f, g}-algebra ⟨N2, [·], ≥⟩deﬁned by
[f]
x
y

=
x + y + 1
y + 1

[g]
x
y

=
x + 1
0

is a model of Rmat, where ≥is extended pointwise over N2. Indeed, the ﬁrst rule
is oriented as the following calculation demonstrates:
[f]

[f]
x
y

=
x + 2y + 3
y + 2
 ≥
≥
x + y + 3
1

= [f]

[g]

[f]
x
y

and the second rule can be easily checked. Now we prove that x ↠g(x) is Rmat-
unsatisﬁable by Corollary 2. Indeed, ⟨N2, [·]⟩|= x ≱g(x) is shown as follows:

x
y

≱
≥

x + 1
0

= [g]

x
y

for any x, y ∈N. Note also that Theorem 1 is not applicable, since ⟨N2, [·]⟩/|=
x < g(x) due to the second coordinate.

Term Orderings for Non-reachability of (Conditional) Rewriting
253
We conclude the section by proving Theorem 1 via Theorem 2.
Proof (of Theorem 1). We show that ⟨⊒, ⊏⟩form a co-rewrite pair when ⟨⊒, ⊐⟩
is a rewrite pair. It suﬃces to show that ⊒∩⊏= ∅. To this end, suppose on the
contrary that s ⊒t ⊐s. By compatibility, we have s ⊐s, which contradicts the
irreﬂexivity of ⊐.
⊓⊔
4
Weighted Path Order for Non-reachability
The previous section was concerned with the semantic approach towards obtain-
ing (co-)rewrite pairs. In this section we focus on the syntactic approach. We
choose the weighted path order (WPO), which subsumes both the lexicographic
path order (LPO) and the Knuth-Bendix order (KBO), so the result of this
section applies to these more well-known methods. The multiset path order [4]
can also be subsumed [29], but we omit this extension to keep the presentation
simple. WPO is induced by three ingredients: an F-algebra; a precedence order-
ing over function symbols; and a (partial) status, which controls the recursive
behavior of the ordering.
Deﬁnition 4 (partial status). A partial status π speciﬁes for each n-ary f ∈
F a list π(f) ∈{1, . . . , n}∗, also seen as a set, of its argument positions. We say
π is total if 1, . . . , n ∈π(f) whenever f is n-ary. When π(f) = [i1, . . . , im], we
denote [si1, . . . , sim] by πf(s1, . . . , sn).
For instance, the empty status π(f) = [ ] allows WPO to subsume weakly
monotone interpretations [36, Section 4.1]. We allow positions to be duplicated,
following [33].
Deﬁnition 5 (WPO [36]).
Let π be a partial status, A an F-algebra, and
⟨≥, >⟩and ⟨≿, ≻⟩be pairs of relations on A and F, respectively. The weighted
path order WPO(π, A, ≥, >, ≿, ≻), or WPO(A) or even WPO for short, is the
pair ⟨⊒WPO, ⊐WPO⟩of relations over terms deﬁned as follows: s ⊐WPO t iﬀ
1. A |= s > t or
2. A |= s ≥t and
(a) s = f(s1, . . . , sn), si ⊒WPO t for some i ∈π(f);
(b) s = f(s1, . . . , sn), t = g(t1, . . . , tm), s ⊐WPO tj for every j ∈π(g) and
i. f ≻g, or
ii. f ≿g and πf(s1, . . . , sn) ⊐lex
WPO πg(t1, . . . , tm).
The relation ⊒WPO is deﬁned similarly, but with ⊒lex
WPO instead of ⊐lex
WPO in (2b-ii)
and the following subcase is added in case 2:
(c) s = t ∈V.
Here ⟨⊒lex
P , ⊐lex
P ⟩denotes the lexicographic extension of a pair P = ⟨⊒P , ⊐P ⟩of
relations, deﬁned by: [s1, . . . , sn] ⊒
(
)
lex
P [t1, . . . , tm] iﬀ

254
A. Yamada
– m = 0 and n ≥
(
) 0, or
– m, n > 0 and s1 > t1 or both s1 ⊒P t1 and [s2, . . . , sn] ⊒
(
)
lex
P [t2, . . . , tm].
LPO is WPO induced by a total status π and a trivial F-algebra as A,
and is written LPO. Allowing partial statuses corresponds to applying argument
ﬁlters [2,17] (except for collapsing ones). KBO is a special case of WPO where
π is total and A is induced by an admissible weight function.
For termination analysis, a precondition for WPO to be a reduction pair is
crucial. In this work, we only need it to be a rewrite pair; that is, well-foundedness
is not necessary. Thus, for instance, it is possible to have x ⊐WPO f(x) by
[f](x) = x−1. This explains why s ∈V is permitted in case 1, which might look
useless to those who are already familiar with termination analysis.
We formulate the main claim of this section as follows.
Deﬁnition 6 (π-simplicity). We say a related F-algebra ⟨A, [·], ≥⟩is π-
simple1 for a partial status π iﬀ[f](a1, . . . , an) ≥ai for arbitrary n-ary f ∈F,
a1, . . . , an ∈A, and i ∈π(f).
Proposition 1. If ⟨≥, >⟩and ⟨≿, ≻⟩are order pairs on A and F, and ⟨A, ≥⟩
is monotone and π-simple, then ⟨⊒WPO, ⊐WPO⟩is a rewrite pair.
Under these conditions, it is known that ⊒WPO is closed under contexts and
⊐WPO is compatible with ⊒WPO [36, Lemmas 7, 10, 13]. Later in this section we
prove other properties necessary for Proposition 1, for which the claims in [36]
must be generalized for the purpose of this paper.
The beneﬁt of having syntax-aware methods can be easily observed by recall-
ing why we have them in termination analysis.
Example 4 ([13]). Consider the TRS RA consisting of the following rules:
A(0, y) →s(y)
A(s(x), 0) →A(x, s(0))
A(s(x), s(y)) →A(x, A(s(x), y))
and suppose that a monotone {A, s, 0}-algebra ⟨N, [·], ≥⟩is a model of RA. Then,
denoting the Ackermann function by A, we have
[A]([s]m(0), [s]n(0)) ≥[s]A(m,n)(0)
(1)
Now consider proving the obvious fact that x ↠s(x) is RA-unsatisﬁable. This
requires ⟨N, [·]⟩|= x < s(x), and then [s]n(0) ≥n by an inductive argument.
This is not possible if [A] is primitive recursive (e.g., a polynomial), since (1)
with [s]A(m,n)(0) ≥A(m, n) contradicts the well-known fact that the Ackermann
function has no primitive-recursive bound.
On the other hand, LPO with A ≻s satisﬁes RA ⊆⊐LPO (⊆⊒LPO) and
x ⊏LPO s(x). Thus Theorem 1 with ⟨⊒, ⊐⟩= ⟨⊒LPO, ⊐LPO⟩proves that x ↠s(x)
is RA-unsatisﬁable, thanks to Proposition 1 and Theorem 1.
1 Such a property would be called inﬂationary in the mathematics literature. In the
term rewriting, the word simple has been used (see, e.g., [32]) in accordance with
simpliﬁcation orders.

Term Orderings for Non-reachability of (Conditional) Rewriting
255
Example 5. Consider the TRS consisting of the following rules:
Rkbo := { f(g(x)) →g(f(f(x))), g(x) →x }
WPO (or KBO) induced by A = ⟨N, [·]⟩and precedence ⟨≿, ≻⟩such that
[f](x) = x
[g](x) = x + 1
f ≻g
satisﬁes Rkbo ⊆⊒WPO. Thus, for instance g(x) ↠g(f(x)) is Rkbo-unsatisﬁable
by Theorem 1. On the other hand, let ⟨A, [·], ≥⟩with A ⊆Z be a model of Rkbo.
Using the idea of [38, Proposition 11], one can show [f](x) ≤x. Hence, Corollary 2
with models over a subset of integers cannot handle the problem. LPO orients
the ﬁrst rule from right to left and hence cannot handle the problem either.
The power of WPO can also be easily veriﬁed, by considering
Rwpo := Rkbo ∪{ f(h(x)) →h(h(f(x))), f(x) →x }
By extending the above WPO with [h](x) = x and f ≻h, which does not fall
into the class of KBO anymore,2 we can prove, e.g., that f(x) ↠f(h(x)) is
R-unsatisﬁable. None of the above mentioned methods can handle this problem.
The rest of this section is dedicated for proving Proposition 1. Similar results
are present in [36], but they make implicit assumptions such as that ≥and ≿are
preorders. In this paper we need more essential assumptions as we will consider
non-transitive relations in the next section.
First we reprove the reﬂexivity of ⊒WPO. The proof also serves as a basis for
the more complicated irreﬂexivity proof.
Lemma 1. If both ≥and ≿are reﬂexive and ⟨A, ≥⟩is π-simple, then
1. i ∈π(f) implies f(s1, . . . , sn) ⊐WPO si, and
2. s ⊒WPO s, i.e., ⊒WPO is reﬂexive.
Proof. As s ⊒WPO s is trivial when s ∈V, we assume s = f(s1, . . . , sn) and
prove the two claims by induction on the structure of s. For the ﬁrst claim, by
π-simplicity, for any α we have [s]α = [f]([s1]α, . . . , [sn]α) ≥[si]α, and hence
A |= s ≥si. By the second claim of induction hypothesis we have si ⊒WPO si,
and thus s ⊐WPO si follows by (2a) of Deﬁnition 5. Next we show s ⊒WPO s
holds by (2b-ii). Indeed, A |= s ≥s follows from the reﬂexivity of ≥; s ⊐WPO
si for every i ∈π(f) as shown above; f ≿f as ≿is reﬂexive; and ﬁnally,
πf(s1, . . . , sn) ⊒lex
WPO πf(s1, . . . , sn) is due to induction hypothesis and the fact
that lexicographic extension preserves reﬂexivity.
⊓⊔
Using reﬂexivity, we can show that both ⊒WPO and ⊐WPO are closed under
substitutions. This result will be reused in Sect. 5, where it will be essential that
neither ≥nor > need be transitive.
2 When [h] is the identity. KBO requires h ≿f for any f.

256
A. Yamada
Lemma 2. If both ≥and ≿are reﬂexive and ⟨A, ≥⟩is π-simple, then both
⊒WPO and ⊐WPO are closed under substitutions.
Proof. We prove by induction on s and t that s ⊒WPO t implies sθ ⊒WPO tθ and
that s ⊐WPO t implies sθ ⊐WPO tθ. We prove the ﬁrst claim by case analysis on
how s ⊒WPO t is derived. The other claim is analogous, without case (2c) below.
1. A |= s > t: Then we have A |= sθ > tθ and thus sθ ⊐WPO tθ by case 1.
2. A |= s ≥t: Then we have A |= sθ ≥tθ. There are the following subcases.
(a) s = f(s1, . . . , sn) and si ⊒WPO t for some i ∈π(f): In this case, we
know siθ ⊒WPO tθ by induction hypothesis on s. Thus (2a) concludes
sθ ⊒WPO tθ.
(b) s = f(s1, . . . , sn), t = g(t1, . . . , tm), and s ⊐WPO tj for every j ∈π(g): By
induction hypothesis on t, we have sθ ⊐WPO tjθ. So the precondition of
(2b) for sθ ⊒WPO tθ is satisﬁed. There are the following subcases:
i. f ≻g: Then (2b-i) concludes.
ii. f ≿g and πf(s1, . . . , sn) ⊒lex
WPO πg(t1, . . . , tm): Then by induction
hypothesis we have πf(s1θ, . . . , snθ) ⊒lex
WPO πg(t1θ, . . . , tmθ), and thus
(2b-ii) concludes.
(c) s = t ∈V: Then we have sθ ⊒WPO tθ by Lemma 1.
⊓⊔
Irreﬂexivity of ⊐WPO is less obvious to have. In fact, [36] uses well-foundedness
to claim it. Here we identify more essential conditions.
Lemma 3. If ⟨≥, >⟩is an order pair on A, and ≻is irreﬂexive on F, and
⟨A, ≥⟩is π-simple, then ⊐WPO is irreﬂexive.
Proof. We show s ̸⊐WPO s for every s by induction on the structure of s. This
is clear if s ∈V, so consider s = f(s1, . . . , sn). Since > is irreﬂexive, we have
A /|= s > s, and thus s ⊐WPO s cannot be due to case 1 of Deﬁnition 5. As ≻is
irreﬂexive on F, f ⊁f and thus (2b-i) is not possible, either. Thanks to induction
hypothesis and the fact that lexicographic extension preserves irreﬂexivity, we
have πf(s1, . . . , sn) ̸⊐lex
WPO πf(s1, . . . , sn), and thus (2b-ii) is not possible either.
The remaining (2a) is more involving. To show si ̸⊒WPO f(s1, . . . , sn) for any
i ∈π(f), we prove the following more general claim: s′ ◁+
π s implies s′ ̸⊒WPO s,
where ◁π denotes the least relation such that si ◁π f(s1, . . . , sn) if i ∈π(f).
This claim is proved by induction on s′. Due to the simplicity assumption, we
have A |= s ≥s′ for every s′ ◁π s, and this generalizes for every s′ ◁+
π s by
easy induction and the transitivity of ≥. Thus we cannot have A |= s′ > s, since
A |= s ≥s′ > s contradicts the assumption that ⟨≥, >⟩is an order pair. This
tells us that s′ ⊒WPO s cannot be due to case 1. Case (2a) is not applicable
thanks to (inner) induction hypothesis on s′. Case (2b) is not possible either,
since s′ ̸⊐WPO s′ thanks to (outer) induction hypothesis on s. This concludes
s′ ̸⊒WPO s for any s′ ◁+
π s, and in particular si ̸⊒WPO s for any i ∈π(f),
refuting the last possibility for s ⊐WPO s to hold.
⊓⊔

Term Orderings for Non-reachability of (Conditional) Rewriting
257
5
Co-WPO
The preceding section demonstrated how to use WPO as a rewrite pair in The-
orem 1. In this section we show how to use WPO in combination with Theo-
rem 2, that is, when ⊒= ⊒WPO, what ⊏should be. We show that ⊏WPO, where
WPO := WPO(π, A, ≮, ≰, ⊀, ̸≾), serves the purpose.
Proposition 2. If ⟨≥, >⟩and ⟨⪰, ≻⟩are order pairs on A and F, ⟨A, ≥⟩is
π-simple and monotone, then ⟨⊒WPO, ⊏WPO⟩is a co-rewrite pair.
When ⟨A, ≥⟩is not total, Example 3 also demonstrates that using Proposi-
tion 2 with Theorem 2 is more powerful than using Proposition 1 in combination
with Theorem 1, by taking π(f) = [ ] for every f. At the time of writing, how-
ever, it is unclear to the author if the diﬀerence still exists when ⟨A, ≥⟩is totally
ordered but ⟨F, ⪰⟩is not. Nevertheless we will clearly see the merit of Proposi-
tion 2 under the setting of conditional rewriting in the next section.
The remainder of this section proves Proposition 2. Unfortunately, WPO does
not satisfy many important properties of WPO, mostly due to the fact that ⟨≮, ≰⟩
is not even an order pair. Nevertheless, Lemma 2 is applicable to ⊐WPO and gives
the following fact:
Lemma 4. If ⟨≥, >⟩is an order pair on A, ⟨A, ≥⟩is π-simple, and ≻is irreﬂex-
ive, then ⊐WPO is closed under substitutions.
Proof. We apply Lemma 2 to WPO. To this end, we need to prove the following:
– ⟨A, ≮⟩is π-simple: Suppose on the contrary one had [f](a1, . . . , an) < ai with
i ∈π(f). Due to the simplicity assumption, we have [f](a1, . . . , an) ≥ai. By
compatibility we must have ai < ai, contradicting irreﬂexivity.
– ≮and ⊀are reﬂexive: This follows from the irreﬂexivity of < and ≺.
⊓⊔
The remaining task is to show that ⊒WPO ∩⊏WPO = ∅. Due to the mutual
inductive deﬁnition of WPO, we need to simultaneously prove the property for
the other combination: ⊒WPO ∩⊏WPO = ∅.
Deﬁnition 7. We say that two pairs P = ⟨⊒P , ⊐P ⟩and Q = ⟨⊒Q, ⊐Q⟩of
relations are co-compatible iﬀ⊒P ∩⊏Q = ⊐P ∩⊑Q = ∅.
The next claim is a justiﬁcation for the word “compatible” in Deﬁnition 7.
Here the compatibility assumption of order pairs is crucial.
Proposition 3. An order pair ⟨⊒, ⊐⟩is co-compatible with itself.
Proof. Suppose on the contrary that a ⊒b and b ⊐a. Then we have a ⊐a by
compatibility, contradicting the irreﬂexivity of ⊐.
⊓⊔
Lemma 5. If P = ⟨⊒P , ⊐P ⟩and Q = ⟨⊒Q, ⊐Q⟩are co-compatible pairs of
relations, then ⟨⊒lex
P , ⊐lex
P ⟩and ⟨⊒lex
Q , ⊐lex
Q ⟩are co-compatible.

258
A. Yamada
Proof. Let us assume that both
[s1, . . . , sn] ⊒lex
P [t1, . . . , tm]
(2)
[s1, . . . , sn] ⊏lex
Q [t1, . . . , tm]
(3)
hold and derive a contradiction. The other part ⊐lex
P ∩⊑lex
Q is analogous. We
proceed by induction on the length of [s1, . . . , sn]. If n = 0, then (2) demands
m = 0 but (3) demands m > 0. Hence we have n > 0, and then (3) demands
m > 0. If s1 ⊐P t1 then by assumption we have s1 ̸⊑Q t1 but (3) demands
s1 ⊑Q t1 (or s1 ⊏Q t1). Hence (2) is due to s1 ⊒P t1 and [s2, . . . , sn] ⊒lex
P
[t2, . . . , tm]. By assumption we have s1 ̸⊏Q t1, so (3) is due to s1 ⊑Q t1 and
[s2, . . . , sn] ⊏lex
Q [t2, . . . , tm]. We derive a contradiction by induction hypothesis.
⊓⊔
We arrive at the main lemma for WPO.
Lemma 6. If ⟨≥, >⟩and ⟨≿, ≻⟩are order pairs on A and F, and ⟨A, ≥⟩is
π-simple, then WPO and WPO are co-compatible.
Proof. We show that neither s ⊒WPO t ∧s ⊏WPO t nor s ⊐WPO t ∧s ⊑WPO t hold
for any s and t, by induction on the structure of s and then t. Let us assume
s ⊒WPO t and prove s ̸⊏WPO t. The other claim is analogous. We proceed by case
analysis on the derivation of s ⊒WPO t.
1. A |= s > t: Then s ⊏WPO t cannot hold as it demands A |= s ≯t (or s ≱t).
2. A |= s ≥t: Then A |= s ≱t cannot happen and thus s ⊏WPO t must be due
to case 2 of Deﬁnition 5. There are the following subcases for s ⊒WPO t:
(a) s = f(s1, . . . , sn), si ⊒WPO t for some i ∈π(f): By induction hypothesis
on s, we have si ̸⊏WPO t, and thus s ⊏WPO t can only be due to (2a). So
t = g(t1, . . . , tm) and s ⊑WPO tj for some j ∈π(g). Then s ̸⊐WPO tj by
induction hypothesis on t. On the contrary we must have s ⊐WPO tj: By
Lemma 1–1. we have s ⊐WPO si ⊒WPO t ⊐WPO tj and hence s ⊐WPO tj as
⟨⊒WPO, ⊐WPO⟩is an order pair.
(b) s = f(s1, . . . , sn), t = g(t1, . . . , tm), and s ⊐WPO tj for every j ∈π(g):
By induction hypothesis on t, we have s ̸⊑WPO tj for any j ∈π(g). Thus
s ⊏WPO t must be due to (2b). We proceed by further considering the
following two possibilities.
i. f ≻g: As neither f ⊁g nor f ̸≿g hold, s ⊑WPO t is not possible.
ii. f ≿g and πf(s1, . . . , sn) ⊒lex
WPO πg(t1, . . . , sm): As f ̸≿g does not
hold, (2b-i) is not applicable to have s ⊏WPO t. By Lemma 5 and
induction hypothesis, we have πf(s1, . . . , sn) ̸⊏lex
WPO πg(t1, . . . , tm) and
thus (2b-ii) is also not applicable, either.
(c) s = t ∈V: Then clearly s ⊏WPO t cannot hold.
⊓⊔

Term Orderings for Non-reachability of (Conditional) Rewriting
259
6
Conditional Rewriting
Conditional term rewriting (cf. [27]) is an extension of term rewriting so that
rewrite rules can be guarded by conditions. We are interested in the “oriented”
variants, as they naturally correspond to functional programming concepts such
as where clauses of Haskell or when clauses of OCaml.
A conditional rewrite rule l →r ⇐φ consists of terms l and r, and a list
φ of pairs of terms. We may omit “⇐[ ]” and write s1 ↠t1, . . . , sn ↠tn
for [⟨s1, t1⟩, . . . , ⟨sn, tn⟩]. A conditional TRS (CTRS) R is a set of conditional
rewrite rules. A CTRS R yields the rewrite preorder →∗
R by the following deriva-
tion rules [22]:
s →∗
R s Refl
s →R t
t →∗
R u
s →∗
R u
Trans
si →R s′
i
f(s1, . . . , si, . . . , sn) →R f(s1, . . . , s′
i, . . . , sn) Mono
s1θ →∗
R t1θ
· · ·
snθ →∗
R tnθ
lθ →R rθ
Rule
if (l →r ⇐s1 ↠t1, . . . , sn ↠tn) ∈R
To approximate reachability with respect to CTRSs by means of (co-)rewrite
pairs, one needs to be careful when dealing with conditions.
Example 6. Consider the following CTRS:
Rfg := { f(x) →x, g(x) →y ⇐f(x) ↠y }
and a reachability atom g(x) ↠f(x). One might expect that a rewrite preorder
⊒such that
f(x) ⊒x
g(x) ⊒y if f(x) ⊒y
can over-approximate →∗
Rfg, but this is unfortunately false. For instance, any
LPO satisﬁes the above constraints: f(x) ⊐LPO x as LPO is a simpliﬁcation order,
and the second constraints also vacuously holds as the condition f(x) ⊒LPO y is
false. However, it is unsound to conclude that g(x) ↠f(x) is Rfg-unsatisﬁable
even if g(x) ⊏LPO f(x): by setting f ≻g one can have g(x) ⊏LPO f(x) and
g(x) ⊏LPO f(x), but g(x) →Rfg f(x).
A solution is to use co-rewrite pairs already for dealing with conditions.
Proposition 4. If ⟨⊒, ⊏⟩is a co-rewrite pair, (l →r ⇐φ) ∈R implies l ⊒r
or u ⊏v for some u ↠v ∈φ, and s ⊏t, then s ↠t is R-unsatisﬁable.
Proof. We show that s →∗
R t implies s ⊒t. This is suﬃcient, since, then sθ →∗
R
tθ implies sθ ⊒tθ, while s ⊏t demands sθ ⊏tθ, which is not possible since
⊒∩⊏= ∅. The claim is proved by induction on the derivation of s →∗
R t.
– Refl: Since ⊒is reﬂexive, we have s ⊒s.

260
A. Yamada
– Trans: We have s →R t and t →∗
R u as premises, and s ⊒t and t ⊒u by
induction hypothesis. Since ⊒is transitive we conclude s ⊒u.
– Mono: We have si
→R
s′
i as a premise and si
⊒s′
i by induction
hypothesis. Since ⊒is closed under contexts, we get f(s1, . . . , si, . . . , sn) ⊒
f(s1, . . . , s′
i, . . . , sn).
– Rule: We have (l →r ⇐s1 ↠t1, . . . , sn ↠tn) ∈R, and for every i ∈
{1, . . . , n} have siθ →∗
R tiθ as a premise and siθ ⊒tiθ by induction hypothe-
sis. Since ⊒∩⊏= ∅, we get siθ ̸⊏tiθ. Since ⊏is closed under substitutions,
we conclude si ̸⊏ti for every i ∈{1, . . . , n}. By assumption, this entails l ⊒r,
and since ⊒is closed under substitution, we conclude lθ ⊒rθ.
⊓⊔
Example 7. Consider the following singleton CTRS:
Rab := { a →b ⇐b ↠a }
Proposition 4 combined with LPO or WPO induced by a partial precedence such
that a ̸≿b and b ̸≿a proves that a ↠b is Rab-unsatisﬁable: Clearly b ⊑LPO a
and a ⊑LPO b by case (2b-i) of Deﬁnition 5. On the other hand, Proposition 4
with the term ordering induced by a totally ordered algebra ⟨A, ≥⟩cannot solve
the problem, since A |= a ≱b implies A |= b ≥a by totality, which then demands
A |= a ≥b to satisfy the assumption of Proposition 4. For the same reason, WPO
induced by a totally ordered algebra and a total precedence cannot handle the
problem either.
Note that the condition of the rule in Rab is unsatisﬁable, and this is one of
the two cases where Proposition 4 is eﬀective. The other case is when a condition
can be ignored. Proposition 4 is incomplete when conditions are essential, as in
Example 6. For dealing with essential conditional rules, the variable binding in
a rule should be taken into account. At this point, a model-oriented formulation
(a la [19]) seems more suitable.
Deﬁnition 8 (model of CTRS). We extend the notation [s ⊐t]α of Def-
inition 2 to [φ]α for an arbitrary Boolean formula φ with the single binary
predicate ⊐in the obvious manner. We say A = ⟨A, [·]⟩validates φ, written
A |= φ, iﬀ[φ]α for every α : V →A. We say a related F-algebra ⟨A, ⊐⟩is
a model of a CTRS R iﬀ3 A |= l ⊐r ∨s1 ̸⊐t1 ∨· · · ∨sn ̸⊐tn for every
(l →r ⇐s1 ↠t1, . . . , sn ↠tn) ∈R.
Besides minor simpliﬁcations (e.g., we do not need two predicates as we
are only concerned with reachability in many steps in this paper), the major
diﬀerence with [19] is that here we do not encode the monotonicity or order
axioms into logical formulas (using R of [19]). Instead, we impose these properties
as meta-level assumptions over models.
Theorem 3. For a CTRS R, s ↠t is R-unsatisﬁable if and only if there exists
a monotone preordered model ⟨A, ≥⟩of R such that A |= s ≱t.
3 Here the formula s ̸⊐t is a shorthand for ¬ s ⊐t.

Term Orderings for Non-reachability of (Conditional) Rewriting
261
Proof. We start with the “if” direction. Let ⟨A, ≥⟩be a monotone preordered
model of R. As in Proposition 4, it suﬃces to show that s →∗
R t implies A |=
s ≥t. The claim is proved by induction on the derivation of s →∗
R t.
– Refl: Since ≥is reﬂexive, we have A |= s ≥s.
– Trans: We have s →R t and t →∗
R u as premises, and A |= s ≥t and A |=
t ≥u by induction hypothesis. Since ≥is transitive we conclude A |= s ≥u.
– Mono: We have si →R s′
i as a premise and A |= si ≥s′
i by induction
hypothesis. Since ⟨A, ≥⟩is monotone, we get A |= f(s1, . . . , si, . . . , sn) ≥
f(s1, . . . , s′
i, . . . , sn).
– Rule: We have (l →r ⇐s1 ↠t1, . . . , sn ↠tn) ∈R, and for every i ∈
{1, . . . , n} have siθ →∗
R tiθ as a premise and A |= siθ ≥tiθ by induction
hypothesis. Since ⟨A, ≥⟩is a model of R, and by the fact that validity is
closed under substitutions, we have A |= lθ ≥rθ ∨s1θ ≱t1θ ∨· · ·∨snθ ≱tnθ.
Together with the induction hypotheses we conclude A |= lθ ≥rθ.
Next consider the “only if” direction. We show that ⟨T (F, V) , →∗
R⟩is a model
of R, that is, for every (l →r ⇐s1 ↠t1, . . . , sn ↠tn) ∈R, we show T (F, V) |=
l →∗
R r ∨s1 ̸→∗
R t1 ∨· · · ∨sn ̸→∗
R tn. This means lθ →∗
R rθ for every θ : V →
T (F, V) such that s1θ →∗
R t1θ, . . . , snθ →∗
R tnθ, which is immediate by Rule.
The fact that →∗
R is a preorder and closed under contexts is also immediate.
Finally, s ↠t being R-unsatisﬁable means that sθ ̸→∗
R tθ for any θ : V →
T (F, V), that is, T (F, V) |= s ̸→∗
R t.
⊓⊔
Putting implementation issues aside, it is trivial to use semantic (termina-
tion) methods in Theorem 3.
Example 8. Consider again the CTRS Rfg of Example 6. The monotone ordered
{f, g}-algebra ⟨N, [·], ≥⟩deﬁned by
[f](x) = x
[g](x) = x + 1
is a model of Rfg, since for arbitrary x, y ∈N, we have
[f](x) ≥x
[g](x) = x + 1 ≥y ∨[f](x) = x ≱y
Then, with Theorem 3 we can show that f(x) ↠g(x) is Rfg-unsatisﬁable, as
[f](x) = x ≱x + 1 = [g](x) for every x ∈N.
To use WPO(A) in combination with Theorem 3, we need to validate formulas
with predicate ⊒WPO(A) in the term algebra T (F, V). We encode these formulas
into formulas with predicates ≥and >, which are then interpreted in A.
Deﬁnition 9 (formal WPO).
Let ⟨≥, >⟩and ⟨≿, ≻⟩be pairs of relations
over some set and over F, respectively, and let π be a partial status. We deﬁne
wpo(π, ≥, >, ≿, ≻) or wpo for short to be the pair ⟨⊒wpo, ⊐wpo⟩, where for terms
s, t ∈T (F, V), s ⊒wpo t and s ⊐wpo t are Boolean formulas deﬁned as follows:
s ⊐wpo t := s > t ∨(s ≥t ∧φ)

262
A. Yamada
where φ is False if s ∈V and is 
i∈π(f) si ⊒wpo t ∨ψ if s = f(s1, . . . , sn), and
ψ is False if t ∈V and is

j∈π(g)
s ⊐wpo tj ∧

f ≻g ∨

f ≿g ∧πf(s1, . . . , sn) ⊐lex
wpo πg(t1, . . . , tm)

if t = g(t1, . . . , tm). Formula s ⊒wpo t is deﬁned analogously, except that φ is
True if s = t ∈V, and ⊐lex
wpo in formula ψ is replaced by ⊒lex
wpo.
We omit an easy proof that veriﬁes that wpo encodes WPO:
Lemma 7. s ⊒
(
)WPO(A) t iﬀA |= s ⊒
(
)wpo t.
Note carefully that s ̸⊒WPO(A) t is A /|= s ⊒wpo t but not A |= s ̸⊒wpo t. Hence we
ensure s ̸⊒WPO(A) t by A |= s ⊑wpo t, where wpo denotes wpo(π, ≮, ≰, ⊀, ̸≾).
Theorem 4. If R is a CTRS, ⟨≥, >⟩and ⟨≿, ≻⟩are order pairs on A and F,
⟨A, ≥⟩is π-simple and monotone, A |= l ⊒wpo r ∨u1 ⊏wpo v1 ∨· · · ∨un ⊏wpo vn
for every (l →r ⇐u1 ↠v1, . . . , un ↠vn) ∈R, and A |= s ⊏wpo t, then s ↠t
is R-unsatisﬁable.
Proof. We apply Theorem 3. To this end, we ﬁrst show that ⟨T (F, V) , ⊒WPO(A)⟩
is a monotone preordered model of R. Monotonicity and preorderedness are due
to Proposition 1. For being a model, let (l →r ⇐u1 ↠v1, . . . , un ↠vn) ∈R.
Due to assumption and Lemma 7, we have l ⊒WPO(A) r ∨u1 ⊏WPO(A) v1 ∨· · · ∨
un ⊏WPO(A) vn. Due to Lemmas 2 and 4, we get lθ ⊒WPO(A) rθ ∨u1θ ⊏WPO(A)
v1θ ∨· · · ∨unθ ⊏WPO(A) vnθ for every θ : V →T (F, V). With Proposition 2 we
conclude T (F, V) |= l ⊒WPO(A) r∨u1 ̸⊒WPO(A) v1 ∨· · ·∨un ̸⊒WPO(A) vn. Finally,
we need T (F, V) |= s ̸⊒WPO(A) t, i.e., sθ ̸⊒WPO(A) tθ for any θ : V →T (F, V). As
we assume s ⊏WPO(A) t, by Lemma 4 we have sθ ⊏WPO(A) tθ. By Proposition 2
we conclude sθ ̸⊒WPO(A) tθ.
⊓⊔
7
Experiments
The proposed methods are implemented in the termination prover NaTT [35],
available at https://www.trs.cm.is.nagoya-u.ac.jp/NaTT/.
Internally, NaTT reduces the problem of ﬁnding an algebra A that make
⟨A, ≥⟩a model of a TRS R (or ⊒WPO(A)⊆R) into a satisﬁability modulo theory
(SMT) problem, which is then solved by the backend SMT solver z3 [26]. The
implementation of Theorem 1 and Corollary 1 is a trivial adaptation from the
termination methods. Cororllary 2 is also trivial for totally ordered carriers, since
A |= s ≱t is equivalent to A |= s < t. Matrix/tuple interpretations are also easy,
since A |= (a1, . . . , an) ≱(b1, . . . , bn) is equivalent to A |= a1 < b1∨· · ·∨an < bn.
Theorem 2 with WPO is obtained by parametrizing WPO.

Term Orderings for Non-reachability of (Conditional) Rewriting
263
Table 1. Experimental results.
TRS
CTRS
Method
Radd Rmat RA Rkbo Rwpo COPS(15) Rab Rfg COPS(126)
Sum
6
✓
15
Sum+
6
✓
24
Sum−
✓
5
28
Mat
✓
6
✓
✓
25 (TO:88)
LPO
✓
5
✓
19
WPO(Sum)
✓
✓
6
✓
✓
15
WPO(Sum+)
✓
✓
6
✓
✓
25 (TO:29)
WPO(Sum−)
6
✓
15
infChecker
✓
✓
13
✓
✓
51+42 (TO:25)
CO3
✓
5
✓
20
NaTT 2.1
3
19
NaTT 2.2
✓
✓
✓
✓
✓
6 (TO:4)
✓
✓
31 (TO:79)
Theorem 3 needs some tricks. In the unconditional case, ﬁnding a desired
algebra A can be encoded into SMT over quantiﬁer-free linear arithmetic for
a large class of A [36]. For the conditional case, we need to ﬁnd (∃) parame-
ters that validates (∀) a disjunctive clause. Farkas’ lemma would reduce such a
problem into quantiﬁer-free SMT, but then the resulting problem is nonlinear.
Experimentally, we observe that our backend z3 performs better on quantiﬁed
linear arithmetic than quantiﬁer-free nonlinear arithmetic, and hence we choose
to leave the ∀quantiﬁers.
We conducted experiments using the examples presented in the paper and
the examples in the INF category of the standard benchmark set COPS. The
execution environment is StarExec [31] with the same settings as CoCo 2019.
Many COPS examples contain conjunctive reachability constraints of form
s1 ↠t1∧· · ·∧sn ↠tn. In this experiment we naively collapsed such a constraint
into tp(s1, . . . , sn) ↠tp(t1, . . . , tn) by introducing a fresh function symbol tp.
Two benchmarks exceed the scope of oriented CTRSs, on which NaTT immedi-
ately gives up.
As co-rewrite pairs we tested algebras Sum, Sum+, Sum−, Mat, LPO,
and WPO. The basic algebra Sum = ⟨Z, [·]⟩is given by [f](x1, . . . , xn) =
c0 + n
i=1 ci · xi, where c0 ∈Z, c1, . . . , cn ∈{0, 1}. Similarly Sum+ and Sum−
are deﬁned, where the ranges of c0, which also determine the carrier, are N and
Z≤0, respectively. The algebra Mat represents the 2D matrix interpretations.
Table 1 presents the results. For TRSs, we can observe that our proposed
methods advance the state of the art, in the sense that they prove new examples
that no tool previously participated in CoCo could handle. As there are only 15
TRS examples in the INF category of COPS 2021, we could not derive interesting
observations there. Taking CTRS examples into account, we see Sum is not as

264
A. Yamada
good as Sum+ or Sum−, while the carrier is bigger (Z versus N or Z≤0). This
phenomenon is explained as follows: For the latter two one knows variables are
bounded by 0 (from below or above), and hence one can have Sum+ |= x ≥a
or Sum−|= a ≥x by [a] = 0. Neither is possible when the carrier is unbounded.
This observation also suggests another choice of carriers that are bounded from
below and above, which is left for future work.
From the ﬁgures in CTRS examples, Sum−performs the best among our
methods. However, Mat and WPO(Sum+) solve more examples if TRS examples
are counted. It does not seem appropriate yet to judge practical signiﬁcance from
these experiments.
Finally, we implemented as the default strategy of NaTT 2.2 the sequential
application of Sum−, LPO, WPO(Sum+), and Mat after the test NaTT already
have implemented. There improvement over previous NaTT 2.1 should be clear,
although the number of timeouts (indicated by “TO:”) is signiﬁcant.
8
Conclusion
We proposed generalizations of termination techniques that can prove unsatisﬁa-
bility of reachability, both for term rewriting and for conditional term rewriting.
We implemented the approach in the termination prover NaTT, and experimen-
tally evaluated the signiﬁcance of the proposed approach.
The implementation focused on evaluating the proposed methods separately.
The only implemented way of combining their power is a naive one: apply the
tests one by one while they fail. For future work, it will be interesting to incor-
porate the proposed method into the existing frameworks [10,30].
Acknowledgments. The author would like to thank Kiraku Shintani for the technical
help with the COPS database system. I would also like to thank Nao Hirokawa, Salvador
Lucas, Naoki Nishida, and Sarah Winkler for discussions, and the anonymous reviewers
for their detailed comments that improved the presentation of the paper.
References
1. Aoto, T.: Disproving conﬂuence of term rewriting systems by interpretation and
ordering. In: Fontaine, P., Ringeissen, C., Schmidt, R.A. (eds.) FroCoS 2013. LNCS
(LNAI), vol. 8152, pp. 311–326. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-40885-4 22
2. Arts, T., Giesl, J.: Termination of term rewriting using dependency pairs.
Theor. Compt. Sci. 236(1–2), 133–178 (2000). https://doi.org/10.1016/S0304-
3975(99)00207-8
3. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press,
Cambridge (1998)
4. Dershowitz, N.: Orderings for term-rewriting systems. Theor. Compt. Sci. 17(3),
279–301 (1982). https://doi.org/10.1016/0304-3975(82)90026-3
5. Endrullis, J., Waldmann, J., Zantema, H.: Matrix interpretations for proving ter-
mination of term rewriting. J. Autom. Reason. 40(2–3), 195–220 (2008). https://
doi.org/10.1007/s10817-007-9087-9

Term Orderings for Non-reachability of (Conditional) Rewriting
265
6. Feuillade, G., Genet, T., Tong, V.V.T.: Reachability analysis over term rewrit-
ing systems. J. Autom. Reasoning 33, 341–383 (2004). https://doi.org/10.1007/
s10817-004-6246-0
7. Giesl, J., et al.: Proving termination of programs automatically with AProVE. In:
Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562,
pp. 184–191. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-
6 13
8. Giesl, J., Rubio, A., Sternagel, C., Waldmann, J., Yamada, A.: The termination and
complexity competition. In: Beyer, D., Huisman, M., Kordon, F., Steﬀen, B. (eds.)
TACAS 2019. LNCS, vol. 11429, pp. 156–166. Springer, Cham (2019). https://doi.
org/10.1007/978-3-030-17502-3 10
9. Giesl, J., Thiemann, R., Schneider-Kamp, P., Falke, S.: Mechanizing and improving
dependency pairs. J. Autom. Reason. 37(3), 155–203 (2006). https://doi.org/10.
1007/s10817-006-9057-7
10. Guti´errez, R., Lucas, S.: Automatically proving and disproving feasibility condi-
tions. In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020. LNCS (LNAI),
vol. 12167, pp. 416–435. Springer, Cham (2020). https://doi.org/10.1007/978-3-
030-51054-1 27
11. Guti´errez, R., Lucas, S.: mu-term: verify termination properties automatically
(system description). In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020.
LNCS (LNAI), vol. 12167, pp. 436–447. Springer, Cham (2020). https://doi.org/
10.1007/978-3-030-51054-1 28
12. Hirokawa, N., Middeldorp, A.: Automating the dependency pair method. Inf. Com-
put. 199(1–2), 172–199 (2005). https://doi.org/10.1016/j.ic.2004.10.004
13. Kamin, S., L´evy, J.J.: Two generalizations of the recursive path ordering (1980).
unpublished note
14. Knuth, D.E., Bendix, P.: Simple word problems in universal algebras. In: Compu-
tational Problems in Abstract Algebra, pp. 263–297. Pergamon Press, New York
(1970). https://doi.org/10.1016/B978-0-08-012975-4.50028-X
15. Kop, C., Vale, D.: Tuple interpretations for higher-order complexity. In: Kobayashi,
N. (ed.) FSCD 2021. LIPIcs, vol. 195, pp. 31:1–31:22. Schloss Dagstuhl - Leibniz-
Zentrum f¨ur Informatik (2021). https://doi.org/10.4230/LIPIcs.FSCD.2021.31
16. Korp, M., Sternagel, C., Zankl, H., Middeldorp, A.: Tyrolean termination tool 2.
In: Treinen, R. (ed.) RTA 2009. LNCS, vol. 5595, pp. 295–304. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-02348-4 21
17. Kusakari, K., Nakamura, M., Toyama, Y.: Argument ﬁltering transformation. In:
Nadathur, G. (ed.) PPDP 1999. LNCS, vol. 1702, pp. 47–61. Springer, Heidelberg
(1999). https://doi.org/10.1007/10704567 3
18. Lankford, D.: Canonical algebraic simpliﬁcation in computational logic. Technical
report ATP-25, University of Texas (1975)
19. Lucas, S., Guti´errez, R.: Use of logical models for proving infeasibility in term
rewriting. Inf. Process. Lett. 136, 90–95 (2018). https://doi.org/10.1016/j.ipl.2018.
04.002
20. Lucas, S., Meseguer, J.: 2D dependency pairs for proving operational termination of
CTRSs. In: Escobar, S. (ed.) WRLA 2014. LNCS, vol. 8663, pp. 195–212. Springer,
Cham (2014). https://doi.org/10.1007/978-3-319-12904-4 11
21. Lucas, S., Meseguer, J.: Dependency pairs for proving termination properties of
conditional term rewriting systems. J. Log. Algebraic Methods Program. 86(1),
236–268 (2017). https://doi.org/10.1016/j.jlamp.2016.03.003

266
A. Yamada
22. Lucas, S., Meseguer, J., Guti´errez, R.: The 2D dependency pair framework for
conditional rewrite systems. part I: deﬁnition and basic processors. J. Comput.
Syst. Sci. 96, 74–106 (2018). https://doi.org/10.1016/j.jcss.2018.04.002
23. Manna, Z., Ness, S.: Termination of Markov algorithms (1969). unpublished
manuscript
24. Middeldorp, A.: Approximating dependency graphs using tree automata tech-
niques. In: Gor´e, R., Leitsch, A., Nipkow, T. (eds.) IJCAR 2001. LNCS, vol. 2083,
pp. 593–610. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-45744-
5 49
25. Middeldorp, A., Nagele, J., Shintani, K.: CoCo 2019: report on the eighth con-
ﬂuence competition. Int. J. Softw. Tools Technol. Transfer 23(6), 905–916 (2021).
https://doi.org/10.1007/s10009-021-00620-4
26. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
27. Ohlebusch, E.: Advanced Topics in Term Rewriting. Springer, New York (2002).
https://doi.org/10.1007/978-1-4757-3661-8
28. Oostrom, V.: Sub-Birkhoﬀ. In: Kameyama, Y., Stuckey, P.J. (eds.) FLOPS 2004.
LNCS, vol. 2998, pp. 180–195. Springer, Heidelberg (2004). https://doi.org/10.
1007/978-3-540-24754-8 14
29. Sternagel, C., Thiemann, R., Yamada, A.: A formalization of weighted path orders
and recursive path orders. Arch. Formal Proofs (2021). https://isa-afp.org/entries/
Weighted Path Order.html, Formal proof development
30. Sternagel, C., Yamada, A.: Reachability analysis for termination and conﬂuence
of rewriting. In: Vojnar, T., Zhang, L. (eds.) TACAS 2019. LNCS, vol. 11427, pp.
262–278. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-17462-0 15
31. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec: a cross-community infrastructure
for logic solving. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014.
LNCS (LNAI), vol. 8562, pp. 367–373. Springer, Cham (2014). https://doi.org/10.
1007/978-3-319-08587-6 28
32. TeReSe: Term Rewriting Systems, Cambridge Tracts in Theoretical Computer Sci-
ence, vol. 55. Cambridge University Press, Cambridge (2003)
33. Thiemann, R., Sch¨opf, J., Sternagel, C., Yamada, A.: Certifying the weighted path
order (invited talk). In: Ariola, Z.M. (ed.) FSCD 2020. LIPIcs, vol. 167, pp. 4:1–
4:20. Schloss Dagstuhl-Leibniz-Zentrum f¨ur Informatik, Dagstuhl, Germany (2020).
https://doi.org/10.4230/LIPIcs.FSCD.2020.4
34. Yamada, A.: Multi-dimensional interpretations for termination of term rewriting.
In: Platzer, A., Sutcliﬀe, G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp.
273–290. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-79876-5 16
35. Yamada, A., Kusakari, K., Sakabe, T.: Nagoya termination tool. In: Dowek, G.
(ed.) RTA 2014. LNCS, vol. 8560, pp. 466–475. Springer, Cham (2014). https://
doi.org/10.1007/978-3-319-08918-8 32
36. Yamada, A., Kusakari, K., Sakabe, T.: A uniﬁed ordering for termination proving.
Sci. Comput. Program. 111, 110–134 (2015). https://doi.org/10.1016/j.scico.2014.
07.009
37. Zantema, H.: Termination of term rewriting by semantic labelling. Fundam. Infor-
maticae 24(1/2), 89–105 (1995). https://doi.org/10.3233/FI-1995-24124
38. Zantema, H.: The termination hierarchy for term rewriting. Appl. Algebr. Eng.
Comm. Compt. 12(1/2), 3–19 (2001). https://doi.org/10.1007/s002000100061

Term Orderings for Non-reachability of (Conditional) Rewriting
267
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Knowledge Representation
and Justiﬁcation

Evonne: Interactive Proof Visualization
for Description Logics
(System Description)
Christian Alrabbaa1(B)
, Franz Baader1(B)
, Stefan Borgwardt1(B)
,
Raimund Dachselt2(B)
, Patrick Koopmann1(B)
, and Juli´an M´endez2(B)
1 Institute of Theoretical Computer Science, TU Dresden, Dresden, Germany
{christian.alrabbaa,franz.baader,stefan.borgwardt,
patrick.koopmann}@tu-dresden.de
2 Interactive Media Lab Dresden, TU Dresden, Dresden, Germany
{raimund.dachselt,julian.mendez2}@tu-dresden.de
Abstract. Explanations for description logic (DL) entailments provide
important support for the maintenance of large ontologies. The “justiﬁca-
tions” usually employed for this purpose in ontology editors pinpoint the
parts of the ontology responsible for a given entailment. Proofs for entail-
ments make the intermediate reasoning steps explicit, and thus explain
how a consequence can actually be derived. We present an interactive
system for exploring description logic proofs, called Evonne, which visu-
alizes proofs of consequences for ontologies written in expressive DLs. We
describe the methods used for computing those proofs, together with a
feature called signature-based proof condensation. Moreover, we evaluate
the quality of generated proofs using real ontologies.
1
Introduction
Proofs generated by Automated Reasoning (AR) systems are sometimes pre-
sented to humans in textual form to convince them of the correctness of a the-
orem [9,11], but more often employed as certiﬁcates that can automatically be
checked [20]. In contrast to the AR setting, where very long proofs may be
needed to derive a deep mathematical theorem from very few axioms, DL-based
ontologies are often very large, but proofs of a single consequence are usually of
a more manageable size. For this reason, the standard method of explanation
in description logic [8] has long been to compute so-called justiﬁcations, which
point out a minimal set of source statements responsible for an entailment of
interest. For example, the ontology editor Prot´eg´e1 supports the computation of
justiﬁcations since 2008 [12], which is very useful when working with large DL
ontologies. Nevertheless, it is often not obvious why a given consequence actually
follows from such a justiﬁcation [13]. Recently, this explanation capability has
been extended towards showing full proofs with intermediate reasoning steps,
but this is restricted to ontologies written in the lightweight DLs supported by
the Elk reasoner [15,16], and the graphical presentation of proofs is very basic.
1 https://protege.stanford.edu/.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 271–280, 2022.
https://doi.org/10.1007/978-3-031-10769-6_16

272
C. Alrabbaa et al.
In this paper, we present Evonne as an interactive system, for exploring DL
proofs for description logic entailments, using the methods for computing small
proofs presented in [3,5]. Initial prototypes of Evonne were presented in [6,10],
but since then, many improvements were implemented. While Evonne does
more than just visualizing proofs, this paper focuses on the proof component
of Evonne: speciﬁcally, we give a brief overview of the interface for exploring
proofs, describe the proof generation methods implemented in the back-end,
and present an experimental evaluation of these proofs generation methods in
terms of proof size and run time. The improved back-end uses Java libraries
that extract proofs using various methods, such as from the Elk calculus, or
forgetting-based proofs [3] using the forgetting tools Lethe [17] and Fame [21] in
a black-box fashion. The new front-end is visually more appealing than the pro-
totypes presented in [6,10], and allows to inspect and explore proofs using various
interaction techniques, such as zooming and panning, collapsing and expanding,
text manipulation, and compactness adjustments. Additional features include
the minimization of the generated proofs according to various measures and the
possibility to select a known signature that is used to automatically hide parts
of the proofs that are assumed to be obvious for users with certain previous
knowledge. Our evaluation shows that proof sizes can be signiﬁcantly reduced
in this way, making the proofs more user-friendly. Evonne can be tried and
downloaded at https://imld.de/evonne. The version of Evonne described here,
as well as the data and scripts used in our experiments, can be found at [2].
2
Preliminaries
We recall some relevant notions for DLs; for a detailed introduction, see [8]. DLs
are decidable fragments of ﬁrst-order logic (FOL) with a special, variable-free syn-
tax, and that use only unary and binary predicates, called concept names and role
names, respectively. These can be used to build complex concepts, which corre-
spond to ﬁrst-order formulas with one free variable, and axioms corresponding to
ﬁrst-order sentences. Which kinds of concepts and axioms can be built depends on
the expressivity of the used DL. Here we mainly consider the light-weight DL ELH
and the more expressive ALCH. We have the usual notion of FOL entailment
O |= α of an axiom α from a ﬁnite set of axioms O, called an ontology. of special
interest are entailments of atomic CIs (concept inclusions) of the form A ⊑B,
where A and B are concept names. Following [3], we deﬁne proofs of O |= α as
ﬁnite, acyclic, directed hypergraphs, where vertices v are labeled with axioms ℓ(v)
and hyperedges are of the form (S, d), with S a set of vertices and d a vertex such
that {ℓ(v) | v ∈S} |= ℓ(d); the leaves of a proof must be labeled by elements of O
and the root by α. In this paper, all proofs are trees, i.e. no vertex can appear in
the ﬁrst component of multiple hyperedges (see Fig. 1).
3
The Graphical User Interface
The user interface of Evonne is implemented as a web application. To support
users in understanding large proofs, they are oﬀered various layout options and

Interactive Proof Visualization for DLs
273
Fig. 1. Overview of Evonne - a condensed proof in the bidirectional layout
interaction components. The proof visualization is linked to a second view show-
ing the context of the proof in a relevant subset of the ontology. In this ontology
view, interactions between axioms are visualized, so that users can understand
the context of axioms occurring in the proof. The user can also examine possible
ways to eliminate unwanted entailments in the ontology view. The focus of this
system description, however, is on the proof component: we describe how the
proofs are generated and how users can interact with the proof visualization.
For details on the ontology view, we refer the reader to the workshop paper [6],
where we also describe how Evonne supports ontology repair.
Initialization. After starting Evonne for the ﬁrst time, users create a new
project, for which they specify an ontology ﬁle. They can then select an entailed
atomic CI to be explained. The user can choose between diﬀerent proof meth-
ods, and optionally select a signature of known terms (cf. Sect. 4), which can be
generated using the term selection tool Prot´eg´e-TS [14].
Layout. Proofs are shown as graphs with two kinds of vertices: colored vertices
for axioms, gray ones for inference steps. By default, proofs are shown using a
tree layout. To take advantage of the width of the display when dealing with
long axioms, it is possible to show proofs in a vertical layout, placing axioms
linearly below each other, with inferences represented through edges on the side
(without the inference vertices). It is possible to automatically re-order vertices
to minimize the distance between conclusion and premises in each step. The third
layout option is the bidirectional layout (see Fig. 1), a tree layout where, initially,
the entire proof is collapsed into a magic vertex that links the conclusion directly
to its justiﬁcation, and from which individual inference steps can be pulled out
and pushed back from both directions.

274
C. Alrabbaa et al.
Exploration. In all views, each vertex is equipped with multiple functionalities
for exploring a proof. For proofs generated with Elk, clicking on an inference
vertex shows the inference rule used, and the particular inference with relevant
sub-elements highlighted in diﬀerent colors. Axiom vertices show diﬀerent button
( ,
,
,
) when hovered over. In the standard tree layout, users can hide sub-
proofs under an axiom
. They can also reveal the previous inference step
or the entire-sub-proof
. In the vertical layout, the button
highlights and
explains the inference of the current axiom. In the bidirectional layout, the arrow
buttons are used for pulling inference steps out of the magic vertex, as well as
pushing them back in.
Presentation. A minimap allows users to keep track of the overall structure
of the proof, thus enriching the zooming and panning functionality. Users can
adjust width and height of proofs through the options side-bar. Long axiom
labels can be shortened in two ways: either by setting a ﬁxed size to all vertices,
or by abbreviating names based on capital letters. Afterwards, it is possible to
restore the original labels individually.
4
Proof Generation
To obtain the proofs that are shown to the user, we implemented diﬀerent proof
generation techniques, some of which were initially described in [3]. For ELH
ontologies, proofs can be generated natively by the DL reasoner Elk [16]. These
proofs use rules from the calculus described in [16]. We apply the Dijkstra-like
algorithm introduced in [4,5] to compute a minimized proof from the Elk out-
put. This minimization can be done w.r.t. diﬀerent measures, such as the size,
depth, or weighted sum (where each axiom is weighted by its size), as long as
they are monotone and recursive [5]. For ontologies outside of the ELH frag-
ment, we use the forgetting-based approach originally described in [3], for which
we now implemented two alternative algorithms for computing more compact
proofs (Sect. 4.1). Finally, independently of the proof generation method, one
can specify a signature of known terms. This signature contains terminology
that the user is familiar with, so that entailments using only those terms do not
need to be explained. The condensation of proofs w.r.t. signatures is described
in Sect. 4.2.
4.1
Forgetting-Based Proofs
In a forgetting-based proof, proof steps represent inferences on concept or role
names using a forgetting operation. Given an ontology O and a predicate name x,
the result O−x of forgetting x in O does not contain any occurrences of x, while
still capturing all entailments of O that do not use x [18]. In a forgetting-based
proof, an inference takes as premises a set P of axioms and has as conclusion
some axiom α ∈P−x (where a particular forgetting operation is used to com-
pute P−x). Intuitively, α is obtained from P by performing inferences on x. To

Interactive Proof Visualization for DLs
275
compute a forgetting-based proof, we have to forget the names occuring in the
ontology one after the other, until only the names occurring in the statement
to be proved are left. For the forgetting operation, the user can select between
two implementations: Lethe [17] (using the method supporting ALCH) and
Fame [21] (using the method supporting ALCOI). Since the space of possible
inference steps is exponentially large, it is not feasible to minimize proofs after
their computation, as we do for EL entailments, which is why we rely on heuris-
tics and search algorithms to generate small proofs. Speciﬁcally, we implemented
three methods for computing forgetting-based proofs: HEUR tries to ﬁnd proofs
fast, SYMB tries to minimize the number of predicates forgotten in a proof, with
the aim of obtaining proofs of small depth, and SIZE tries to optimize the size of
the proof. The heuristic method HEUR is described in [3], and its implementation
has not been changed since then. The search methods SYMB and SIZE are new
(details can be found in the extended version [1]).
4.2
Signature-Based Proof Condensation
When inspecting a proof over a real-world ontology, diﬀerent parts of the proof
will be more or less familiar to the user, depending on their knowledge about
the involved concepts or their experience with similar inference steps in the past.
For CIs between concepts for which a user has application knowledge, they may
not need to see a proof, and consequently, sub-proofs for such axioms can be
automatically hidden. We assume that the user’s knowledge is given in the form
of a known signature Σ and that axioms that contain only symbols from Σ do
not need to be explained. The eﬀect can be seen in Fig. 1 through the “known”-
inference on the left, where Σ contains SebaceousGland and Gland. The known
signature is taken into consideration when minimizing the proofs, so that proofs
are selected for which more of the known information can be used if convenient.
This can be easily integrated into the Dijsktra approach described in [3], by
initially assigning to each axiom covered by Σ a proof with a single vertex.
5
Evaluation
For Evonne to be usable in practice, it is vital that proofs are computed eﬃ-
ciently and that they are not too large. An experimental evaluation of minimized
proofs for EL and forgetting-based proofs obtained with Fame and Lethe is pro-
vided in [3]. We here present an evaluation of additional aspects: 1) a comparison
of the three methods for computing forgetting-based proofs, and 2) an evalua-
tion on the impact of signature-based proof condensation. All experiments were
performed on Debian Linux (Intel Core i5-4590, 3.30 GHz, 23 GB Java heap size).
5.1
Minimal Forgetting-Based Proofs
To evaluate forgetting-based proofs, we extracted ALCH “proof tasks” from the
ontologies in the 2017 snapshot of BioPortal [19]. We restricted all ontologies

276
C. Alrabbaa et al.
0
10 20 30 40 50 60 70
0
10
20
30
40
50
60
70
HEUR
SYMB/SIZE
Proof size
SYMB
SIZE
102
103
104
105
106
102
103
104
105
106
HEUR
SYMB/SIZE
Run time (ms)
Fig. 2. Run times and proof sizes for diﬀerent forgetting-based proof methods. Marker
size indicates how often each pattern occurred in the BioPortal snapshot. Instances
that timed out were assigned size 0.
to ALCH and collected all entailed atomic CIs α, for each of which we computed
the union U of all their justiﬁcations. We identiﬁed pairs (α, U) that were isomor-
phic modulo renaming of predicates, and kept only those patterns (α, U) that
contained at least one axiom not expressible in ELH. This was successful in 373
of the ontologies2 and resulted in 138 distinct justiﬁcation patterns (α, U), repre-
senting 327 diﬀerent entailments in the BioPortal snapshot. We then computed
forgetting-based proofs for U |= α with our three methods using Lethe, with a
5-minute timeout. This was successful for 325/327 entailments for the heuristic
method (HEUR), 317 for the symbol-minimizing method (SYMB), and 279 for the
size-minimizing method (SIZE). In Fig. 2 we compare the resulting proof sizes
(left) and the run times (right), using HEUR as baseline (x-axis). HEUR is indeed
faster in most cases, but SIZE reduces proof size by 5% on average compared to
HEUR, which is not the case for SYMB. Regarding proof depth (not shown in the
ﬁgure), SYMB did not outperform HEUR on average, while SIZE surprisingly yielded
an average reduction of 4% compared to HEUR. Despite this good performance of
SIZE for proof size and depth, for entailments that depend on many or complex
axioms, computation times for both SYMB and SIZE become unacceptable, while
proof generation with HEUR mostly stays in the area of seconds.
5.2
Signature-Based Proof Condensation
To evaluate how much hiding proof steps in a known signature decreases proof
size in practice, we ran experiments on the large medical ontology SNOMED CT
(International Edition, July 2020) that is mostly formulated in ELH.3 As signa-
tures we used SNOMED CT Reference Sets,4 which are restricted vocabularies
2 The other ontologies could not be processed in this way within the memory limit.
3 https://www.snomed.org/.
4 https://conﬂuence.ihtsdotools.org/display/DOCRFSPG/2.3.+Reference+Set.

Interactive Proof Visualization for DLs
277
0
50 100 150 200 250 300
0
50
100
150
200
250
300
Original
Condensed
Proof Size
DEF
GPFP
GPS
IPS
0
20
40
60
80
100
0
20
40
60
80
100
Signature Coverage (%)
Ratio of Proof Size (%)
Original vs. Condensed
Fig. 3. Size of original and condensed proofs (left). Ratio of proof size depending on
the signature coverage (right).
for speciﬁc use cases. We extracted justiﬁcations similarly to the previous exper-
iment, but did not rename predicates and considered only proof tasks that use
at least 5 symbols from the signature, since otherwise no improvement can be
expected by using the signatures. For each signature, we randomly selected 500
out of 6.689.452 proof tasks (if at least 500 existed). This left the 4 reference
sets General Practitioner/Family Practitioner (GPFP), Global Patient Set (GPS),
International Patient Summary (IPS), and the one included in the SNOMED CT
distribution (DEF). For each of the resulting 2.000 proof tasks, we used Elk [16]
and our proof minimization approach to obtain (a) a proof of minimal size and
(b) a proof of minimal size after hiding the selected signature. The distribution
of proof sizes can be seen in Fig. 3. In 770/2.000 cases, a smaller proof was gener-
ated when using the signature. In 91 of these cases, the size was even be reduced
to 1, i.e. the target axiom used only the given signature and therefore nothing
else needed to be shown. In the other 679 cases with reduced size, the average
ratio of reduced size to original size was 0.68–0.93 (depending on the signature).
One can see that this ratio is correlated with the signature coverage of the origi-
nal proof (i.e. the ratio of signature symbols to total symbols in the proof), with
a weak or strong correlation depending on the signature (r between −0.26 and
−0.74). However, a substantial number of proofs with relatively high signature
coverage could still not be reduced in size at all (see the top right of the right
diagram). In summary, we can see that signature-based condensation can be
useful, but this depends on the proof task and the signature. We also conducted
experiments on the Galen ontology,5 with comparable results (see the extended
version of this paper [1]).
5 https://bioportal.bioontology.org/ontologies/GALEN.

278
C. Alrabbaa et al.
6
Conclusion
We have presented and compared the proof generation and presentation methods
used in Evonne, a visual tool for explaining entailments of DL ontologies. While
these methods produce smaller or less deep proofs, which are thus easier to
present, there is still room for improvements. Speciﬁcally, as the forgetting-based
proofs do not provide the same degree of detail as the Elk proofs, it would be
desirable to also support methods for more expressive DLs that generate proofs
with smaller inference steps. Moreover, our current evaluation focuses on proof
size and depth—to understand how well Evonne helps users to understand
DL entailments, we would also need a qualitative evaluation of the tool with
potential end-users. We are also working on explanations for non-entailments
using countermodels [7] and a plugin for the ontology editor Prot´eg´e that is
compatible with the PULi library and Proof Explanation plugin presented in [15],
which will support all proof generation methods discussed here and more.6
Acknowledgements. This work was supported by the German Research Foundation
(DFG) in Germany’s Excellence Strategy: EXC-2068, 390729961 - Cluster of Excellence
“Physics of Life” and EXC 2050/1, 390696704 - Cluster of Excellence “Centre for
Tactile Internet” (CeTI) of TU Dresden, by DFG grant 389792660 as part of TRR
248 - CPEC, by the AI competence center ScaDS.AI Dresden/Leipzig, and the DFG
Research Training Group QuantLA, GRK 1763.
References
1. Alrabbaa, C., Baader, F., Borgwardt, S., Dachselt, R., Koopmann, P., M´endez, J.:
Evonne: interactive proof visualization for description logics (system description)
- extended version (2022). https://doi.org/10.48550/ARXIV.2205.09583
2. Alrabbaa, C., Baader, F., Borgwardt, S., Dachselt, R., Koopmann, P., M´endez, J.:
Evonne: interactive proof visualization for description logics (system description)
- IJCAR22 - resources, May 2022. https://doi.org/10.5281/zenodo.6560603
3. Alrabbaa, C., Baader, F., Borgwardt, S., Koopmann, P., Kovtunova, A.: Finding
small proofs for description logic entailments: theory and practice. In: Albert, E.,
Kov´acs, L. (eds.) Proceedings of the 23rd International Conference on Logic for
Programming, Artiﬁcial Intelligence and Reasoning (LPAR 2020). EPiC Series in
Computing, vol. 73, pp. 32–67. EasyChair (2020). https://doi.org/10.29007/nhpp
4. Alrabbaa, C., Baader, F., Borgwardt, S., Koopmann, P., Kovtunova, A.: On the
complexity of ﬁnding good proofs for description logic entailments. In: Borgwardt,
S., Meyer, T. (eds.) Proceedings of the 33rd International Workshop on Description
Logics (DL 2020). CEUR Workshop Proceedings, vol. 2663. CEUR-WS.org (2020).
http://ceur-ws.org/Vol-2663/paper-1.pdf
5. Alrabbaa, C., Baader, F., Borgwardt, S., Koopmann, P., Kovtunova, A.: Finding
good proofs for description logic entailments using recursive quality measures. In:
Platzer, A., Sutcliﬀe, G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 291–308.
Springer, Cham (2021). https://doi.org/10.1007/978-3-030-79876-5 17
6 https://github.com/de-tu-dresden-inf-lat/evee.

Interactive Proof Visualization for DLs
279
6. Alrabbaa, C., Baader, F., Dachselt, R., Flemisch, T., Koopmann, P.: Visualising
proofs and the modular structure of ontologies to support ontology repair. In:
Borgwardt, S., Meyer, T. (eds.) Proceedings of the 33rd International Workshop
on Description Logics (DL 2020). CEUR Workshop Proceedings, vol. 2663. CEUR-
WS.org (2020). http://ceur-ws.org/Vol-2663/paper-2.pdf
7. Alrabbaa, C., Hieke, W., Turhan, A.: Counter model transformation for explaining
non-subsumption in EL. In: Beierle, C., Ragni, M., Stolzenburg, F., Thimm, M.
(eds.) Proceedings of the 7th Workshop on Formal and Cognitive Reasoning. CEUR
Workshop Proceedings, vol. 2961, pp. 9–22. CEUR-WS.org (2021). http://ceur-ws.
org/Vol-2961/paper 2.pdf
8. Baader, F., Horrocks, I., Lutz, C., Sattler, U.: An Introduction to Description
Logic. Cambridge University Press, Cambridge (2017). https://doi.org/10.1017/
9781139025355
9. Fiedler, A.: Natural language proof explanation. In: Hutter, D., Stephan, W. (eds.)
Mechanizing Mathematical Reasoning. LNCS (LNAI), vol. 2605, pp. 342–363.
Springer, Heidelberg (2005). https://doi.org/10.1007/978-3-540-32254-2 20
10. Flemisch, T., Langner, R., Alrabbaa, C., Dachselt, R.: Towards designing a tool
for understanding proofs in ontologies through combined node-link diagrams. In:
Ivanova, V., Lambrix, P., Pesquita, C., Wiens, V. (eds.) Proceedings of the Fifth
International Workshop on Visualization and Interaction for Ontologies and Linked
Data (VOILA 2020). CEUR Workshop Proceedings, vol. 2778, pp. 28–40. CEUR-
WS.org (2020). http://ceur-ws.org/Vol-2778/paper3.pdf
11. Horacek, H.: Presenting proofs in a human-oriented way. In: CADE 1999. LNCS
(LNAI), vol. 1632, pp. 142–156. Springer, Heidelberg (1999). https://doi.org/10.
1007/3-540-48660-7 10
12. Horridge, M., Parsia, B., Sattler, U.: Explanation of OWL entailments in Protege
4. In: Bizer, C., Joshi, A. (eds.) Proceedings of the Poster and Demonstration
Session at the 7th International Semantic Web Conference (ISWC 2008). CEUR
Workshop Proceedings, vol. 401. CEUR-WS.org (2008). http://ceur-ws.org/Vol-
401/iswc2008pd submission 47.pdf
13. Horridge, M., Parsia, B., Sattler, U.: Justiﬁcation oriented proofs in OWL. In:
Patel-Schneider, P.F., Pan, Y., Hitzler, P., Mika, P., Zhang, L., Pan, J.Z., Hor-
rocks, I., Glimm, B. (eds.) ISWC 2010. LNCS, vol. 6496, pp. 354–369. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-17746-0 23
14. Hyland, I., Schmidt, R.A.: Prot´eg´e-TS: An OWL ontology term selection tool. In:
Borgwardt, S., Meyer, T. (eds.) Proceedings of the 33rd International Workshop
on Description Logics (DL 2020). CEUR Workshop Proceedings, vol. 2663. CEUR-
WS.org (2020). http://ceur-ws.org/Vol-2663/paper-12.pdf
15. Kazakov, Y., Klinov, P., Stupnikov, A.: Towards reusable explanation services in
protege. In: Artale, A., Glimm, B., Kontchakov, R. (eds.) Proceedings of the 30th
International Workshop on Description Logics (DL 2017). CEUR Workshop Pro-
ceedings, vol. 1879. CEUR-WS.org (2017). http://ceur-ws.org/Vol-1879/paper31.
pdf
16. Kazakov, Y., Kr¨otzsch, M., Simancik, F.: The incredible ELK - from polynomial
procedures to eﬃcient reasoning with EL ontologies. J. Autom. Reason. 53(1),
1–61 (2014). https://doi.org/10.1007/s10817-013-9296-3
17. Koopmann, P.: LETHE: forgetting and uniform interpolation for expressive
description logics. K¨unstliche Intell. 34(3), 381–387 (2020). https://doi.org/10.
1007/s13218-020-00655-w

280
C. Alrabbaa et al.
18. Koopmann, P., Schmidt, R.A.: Forgetting concept and role symbols in ALCH-
ontologies. In: McMillan, K., Middeldorp, A., Voronkov, A. (eds.) LPAR 2013.
LNCS, vol. 8312, pp. 552–567. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-45221-5 37
19. Matentzoglu, N., Parsia, B.: Bioportal snapshot 30.03.2017, March 2017. https://
doi.org/10.5281/zenodo.439510
20. Reger, G., Suda, M.: Checkable proofs for ﬁrst-order theorem proving. In: Reger,
G., Traytel, D. (eds.) 1st International Workshop on Automated Reasoning: Chal-
lenges, Applications, Directions, Exemplary Achievements (ARCADE 2017). EPiC
Series in Computing, vol. 51, pp. 55–63. EasyChair (2017). https://doi.org/10.
29007/s6d1
21. Zhao, Y., Schmidt, R.A.: FAME: an automated tool for semantic forgetting in
expressive description logics. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 19–27. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-94205-6 2
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Actions over Core-Closed Knowledge
Bases
Claudia Cauli1,2(B), Magdalena Ortiz3, and Nir Piterman1
1 University of Gothenburg, Gothenburg, Sweden
claudiacauli@gmail.com
2 Amazon Web Services, Seattle, USA
3 TU Wien, Vienna, Austria
Abstract. We present new results on the application of semantic- and
knowledge-based reasoning techniques to the analysis of cloud deploy-
ments. In particular, to the security of Infrastructure as Code conﬁgura-
tion ﬁles, encoded as description logic knowledge bases. We introduce an
action language to model mutating actions; that is, actions that change
the structural conﬁguration of a given deployment by adding, modifying,
or deleting resources. We mainly focus on two problems: the problem of
determining whether the execution of an action, no matter the parame-
ters passed to it, will not cause the violation of some security requirement
(static veriﬁcation), and the problem of ﬁnding sequences of actions that
would lead the deployment to a state where (un)desirable properties are
(not) satisﬁed (plan existence and plan synthesis). For all these problems,
we provide deﬁnitions, complexity results, and decision procedures.
1
Introduction
The use of automated reasoning techniques to analyze the properties of cloud
infrastructure is gaining increasing attention [4–7,18]. Despite that, more eﬀort
needs to be put into the modeling and veriﬁcation of generic security require-
ments over cloud infrastructure pre-deployment. The availability of formal tech-
niques, providing strong security guarantees, would assist complex system-level
analyses such as threat modeling and data ﬂow, which now require considerable
time, manual intervention, and expert domain knowledge.
We continue our research on the application of semantic-based and
knowledge-based reasoning techniques to cloud deployment Infrastructure as
Code conﬁguration ﬁles. In [14], we reported on our experience using expressive
description logics to model and reason about Amazon Web Services’ proprietary
Infrastructure as Code framework (AWS CloudFormation). We used the rich
constructs of these logics to encode domain knowledge, simulate closed-world
reasoning, and express mitigations and exposures to security threats. Due to the
high complexity of basic tasks [3,26], we found reasoning in such a framework
to be not eﬃcient at cloud scale. In [15], we introduced core-closed knowledge
C. Cauli—This work was done prior to joining Amazon.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 281–299, 2022.
https://doi.org/10.1007/978-3-031-10769-6_17

282
C. Cauli et al.
bases—a lightweight description logic combining closed- and open-world rea-
soning that is tailored to model cloud infrastructure and eﬃciently query its
security properties. Core-closed knowledge bases enable partially-closed predi-
cates whose interpretation is closed over a core part of the knowledge base but
open elsewhere. To encode potential exposure to security threats, we studied the
query satisﬁability problem and (together with the usual query entailment prob-
lem) applied it to a new class of conjunctive queries that we called Must/May
queries. We were able to answer such queries over core-closed knowledge bases in
LogSpace in data complexity and NP in combined complexity, improving the
required NExptime complexity for satisﬁability over ALCOIQ (used in [14]).
Here, we enhance the quality of the analyses done over pre-deployment arti-
facts, giving users and practitioners additional precise insights on the impact
of potential changes, ﬁxes, and general improvements to their cloud projects.
We enrich core-closed knowledge bases with the notion of core-completeness,
which is needed to ensure that updates are consistent. We deﬁne the syntax and
semantics of an action language that is expressive enough to encode mutating
API calls, i.e., operations that change a cloud deployment conﬁguration by cre-
ating, modifying, or deleting existing resources. As part of our eﬀort to improve
the quality of automated analysis, we also provide relevant reasoning tools to
identify and predict the consequences of these changes. To this end, we consider
procedures that determine whether the execution of a mutating action always
preserves given properties (static veriﬁcation); determine whether there exists a
sequence of operations that would lead a deployment to a conﬁguration meet-
ing certain requirements (plan existence); and ﬁnd such sequences of operations
(plan synthesis).
The paper is organized as follows. In Sect. 2, we provide background on core-
closed knowledge bases, conjunctive queries, and Must/May queries. In Sect. 3,
we motivate and introduce the notion of core-completeness. In Sect. 4, we deﬁne
the action language. In Sect. 5, we describe the static veriﬁcation problem and
characterize its complexity. In Sect. 6, we address the planning problem and
concentrate on the synthesis of minimal plans satisfying a given requirement
expressed using Must/May queries. We discuss related works in Sect. 7 and
conclude in Sect. 8. Results and proofs that are omitted in this paper are found
in the full version [16].
2
Background
Description logics (DLs) are a family of logics for encoding knowledge in terms of
concepts, roles, and individuals; analogous to ﬁrst-order logic unary predicates,
binary predicates, and constants, respectively. Standard DL knowledge bases
(KBs) have a set of axioms, called TBox, and a set of assertions, called ABox.
The TBox contains axioms that relate to concepts and roles. The ABox contains
assertions that relate individuals to concepts and pairs of individuals to roles.
KBs are usually interpreted under the open-world assumption, meaning that the
asserted facts are not assumed to be complete.

Actions over Core-Closed Knowledge Bases
283
Core-Closed Knowledge Bases. In [15], we introduced core-closed knowledge
bases (ccKBs) as a suitable description logic formalism to encode cloud deploy-
ments. The main characteristic of ccKBs is to allow for a combination of open-
and closed-world reasoning that ensures tractability. A DL-LiteF ccKB is the
tuple K = ⟨T , A, S, M⟩built from the standard knowledge base ⟨T , A⟩and the
core system ⟨S, M⟩. The former encodes incomplete terminological and asser-
tional knowledge. The latter is, in turn, composed of two parts: S (also called
the SBox), containing axioms that encode the core structural speciﬁcations,
and M (also called the MBox), containing positive concept and role assertions
that encode the core conﬁguration. Syntactically, M is similar to an ABox but,
semantically, is assumed to be complete with respect to the speciﬁcations in S.
The ccKB K is deﬁned over the alphabets C (of concepts), R (of roles), and I
(of individuals), all partitioned into an open subset and a partially-closed subset.
That is, the set of concepts is partitioned into the open concepts CK and the
closed (speciﬁcation) concepts CS; the set of roles is partitioned into open roles
RK and closed (speciﬁcation) roles RS; and the set of individuals is partitioned
into open individuals IK and closed (model) individuals IM. We call CS and RS
core-closed predicates, or partially-closed predicates, as their extension is closed
over the core domain IM and open otherwise. In contrast, we call CK and RK
open predicates. The syntax of concept and role expressions in DL-LiteF [2,8]
is as follows:
B ::= ⊥| A | ∃p
where A denotes a concept name and p is either a role name r or its inverse r−.
The syntax of axioms provides for the three following axioms:
B1 ⊑B2,
B1 ⊑¬B2,
(funct p),
respectively called: positive inclusion axioms, negative inclusion axioms, and
functionality axioms. These axioms are contained in the sets S and T . To pre-
cisely denote the subsets of S and T having only axioms of a given type we use
the notation PIX , NIX , and FX , for X ∈{S, T }, which respectively contain only
positive inclusion axioms, negative inclusion axioms, and functionality axioms.
From now on, we denote symbols from the alphabet XX with the subscript
X, and symbols from the generic alphabet X with no subscript. In core-closed
knowledge bases, axioms and assertions fall into the scope of a diﬀerent set
depending on the predicates and individuals that they refer to, according to the
set deﬁnitions below.
M ⊆{AS(aM), RS(aM, a), RS(a, aM)}
A ⊆{AK(aK), RK(aK, bK), AS(aK), RS(aK, bK)}
S ⊆{B1
S ⊑B2
S, B1
S ⊑¬B2
S, Func(PS)}
T ⊆{B1 ⊑B2
K, B1 ⊑¬B2
K, Func(PK)}
In the above deﬁnition of the set M, role assertions link at least one individual
from the core domain IM (denoted as aM) to one individual from the general set

284
C. Cauli et al.
I (denoted as a). Node a could either be an individual from the open partition IK
or the closed partition IM. When a is an element from the set IK, we refer to it
as a “boundary node”, as it sits at the boundary between the core and the open
parts of the knowledge base. As mentioned earlier, M-assertions are assumed to
be complete and consistent with respect to the terminological knowledge given
in S; whereas the usual open-world assumption is made for A-assertions. The
semantics of a DL-LiteF core-closed KB is given in terms of interpretations I,
consisting of a non-empty domain ΔI and an interpretation function ·I. The
latter assigns to each concept A a subset AI of ΔI, to each role r a subset rI of
ΔI ×ΔI, and to each individual a a node aI in ΔI, and it is extended to concept
expressions in the usual way. An interpretation I is a model of an inclusion axiom
B1 ⊑B2 if BI
1 ⊆BI
2 . An interpretation I is a model of a membership assertion
A(a), (resp. r(a, b)) if aI ∈AI (resp. (aI, bI) ∈rI). We say that I models T ,
S, and A if it models all axioms or assertions contained therein. We say that I
models M, denoted I |=CWA M, when it models an M-assertion f if and only
if
f ∈M. Finally, I models K if it models T , S, A, and M. When K has at
least one model, we say that K is satisﬁable.
In the remainder of this paper, we will sometimes refer to the lts interpreta-
tion of M. The lts interpretation of M, denoted lts(M), is the interpretation
(Δlts(M), ·lts(M)) deﬁned only over concept and role names from the set CS and
RS, respectively, and over individual names from IK that appear in the scope
of M-assertions. The interpretation lts(M) is the unique model of M such that
lts(M) |=CWA M.
In the application presented in [14], description logic KBs are used to encode
machine-readable deployment ﬁles containing multiple resource declarations.
Every resource declaration has an underlying tree structure, whose leaves can
potentially link to the roots of other resource declarations. Let Ir ⊆IM be the
set of all resource nodes, we encode their resource declarations in M, and for-
malize the resulting forest structure by partitioning M into multiple subsets
{Mi}i∈Ir, each representing a tree of assertions rooted at a resource node i (we
generally refer to constants in M as nodes). For the purpose of this work, we
will refer to core-closed knowledge bases where M is partitioned as described;
that is, ccKBs such that K = ⟨T , A, S, {Mi}i∈Ir⟩.
Conjunctive Queries. A conjunctive query (CQ) is an existentially-quantiﬁed
formula q[⃗x] of the form ∃⃗y.conj(⃗x, ⃗y), where conj is a conjunction of positive
atoms and potentially inequalities. A union of conjunctive queries (UCQ) is a
disjunction of CQs. The variables in ⃗x are called answer variables, those in ⃗y
are the existentially-quantiﬁed query variables. A tuple ⃗c of constants appearing
in the knowledge base K is an answer to q if for all interpretations I model
of K we have I |= q[⃗c]. We call these tuples the certain answers of q over K,
denoted ans(K, q), and the problem of testing whether a tuple is a certain answer
query entailment. A tuple ⃗c of constants appearing in K satisﬁes q if there exists
an interpretation I model of K such that I |= q[⃗c]. We call these tuples the sat
answers of q over K, denoted sat−ans(K, q), and the problem of testing whether
a given tuple is a sat answer query satisﬁability.

Actions over Core-Closed Knowledge Bases
285
Must/May Queries. A Must/May query ψ [15] is a Boolean combination of
nested UCQs in the scope of a Must or a May operator as follows:
ψ ::= ¬ψ | ψ1 ∧ψ2 | ψ1 ∨ψ2 | Must ϕ | May ϕ̸≈
where ϕ and ϕ̸≈are unions of conjunctive queries potentially containing inequal-
ities. The reasoning needed for answering the nested queries can be decou-
pled from the reasoning needed to answer the higher-level formula: nested
queries Must ϕ are reduced to conjunctive query entailment, and nested queries
May ϕ̸≈are reduced to conjunctive query satisﬁability. We denote by ANS(ψ, K)
the answers of a Must/May query ψ over the core-closed knowledge base K.
3
Core-Complete Knowledge Bases
The algorithm Consistent presented in [15] computes satisﬁability of DL-LiteF
core-closed knowledge bases relying on the assumption that M is complete and
consistent with respect to S. Such an assumption eﬀectively means that the infor-
mation contained in M is explicitly present and cannot be completed by inference.
The algorithm relies on the existence of a theoretical object, the canonical inter-
pretation, in which missing assertions can always be introduced when they are
logically implied by the positive inclusion axioms. As a matter of fact, positive
inclusion axioms are not even included in the inconsistency formula built for
the satisﬁability check, as it is proven that the canonical interpretation always
satisﬁes them ([15], Lemma 3). When the assumption that M is consistent with
respect to S is dropped, the algorithm Consistent becomes insuﬃcient to check
satisﬁability. We illustrate this with an example.
Example 1 (Required Conﬁguration). Let us consider the axioms constraining
the AWS resource type S3::Bucket. In particular, the S-axiom S3::Bucket ⊑
∃loggingConﬁguration prescribing that all buckets must have a required log-
ging conﬁguration. For a set M = {S3::Bucket(b)}, according to the partially-
closed semantics of core-closed knowledge bases, the absence of an assertion
loggingConﬁguration(b, x), for some x, is interpreted as the assertion being false
in M, which is therefore not consistent with respect to S. However, the algo-
rithm Consistent will check the lts interpretation of M for an empty formula (as
there are no negative inclusion or functionality axioms) and return true.
In essence, the algorithm Consistent does not compute the full satisﬁability of the
whole core-closed knowledge base, but only of its open part. Satisﬁability of M
with respect to the positive inclusion axioms in S needs to be checked separately.
We introduce a new notion to denote when a set M is complete with respect
to S that is distinct from the notion of consistency. Let K = ⟨T , A, S, M⟩be a
DL-LiteF core-closed knowledge base; we say that K is core-complete when M
models all positive inclusion axioms in S under a closed-world assumption; we
say that K is open-consistent when M and A model all negative inclusion and
functionality axioms in K’s negative inclusion closure. Finally, we say that K is
fully satisﬁable when is both core-complete and open-consistent.

286
C. Cauli et al.
Lemma 1. In order to check full satisﬁability of a DL-LiteF core-closed KB,
one simply needs to check if K is core-complete (that is, if M models all positive
axioms in S under a closed-world assumption) and if K is open-consistent (that
is, to run the algorithm Consistent).
Proof. Dropping the assumption that M is consistent w.r.t. S causes Lemma 3
from [15] to fail. In particular, the canonical interpretation of K, can(K), would
still be a model of PIT , A, and M, but may not be a model of PIS. This is
due to the construction of the canonical model that is based on the notion of
applicable axioms. In rules c5-c8 of [15] Deﬁnition 1, axioms in PIS are deﬁned
as applicable to assertions involving open nodes aK but not to model nodes
aM in IM. As a result, if the implications of such axioms on model nodes are
not included in M itself, then they will not be included in can(K) either, and
can(K) will not be a model of PIS. On the other hand, one can easily verify
that Lemmas 1,2,4,5,6,7 and Corollary 1 would still hold as they do not rely on
the assumption. However, since it is not guaranteed anymore that M satisﬁes
all positive inclusion axioms from S, the if direction of [15] Theorem 1 does not
hold anymore: there can be an unsatisﬁable ccKB K such that db(A)∪lts(M) |=
cln(T ∪S), A, M. For instance, the knowledge base from Example 1. We also
note that the negative inclusion and functionality axioms from S will be checked
anyway by the consistency formula, both on db(A) and on lts(M).
Lemma 2. Checking whether a DL-LiteF core-closed knowledge base is core-
complete can be done in polynomial time in M. As a consequence, checking full
satisﬁability is also done in polynomial time in M.
Proof. One can write an algorithm that checks core-completeness by searching
for the existence of a positive inclusion axiom B1
S ⊑B2
S ∈PIS such that M |=
B1
S(aM) and M ̸|= B2
S(aM), where the relation |= is deﬁned over DL-LiteF
concept expressions as follows:
M |=⊥(aM)
↔
false
M |=AS(aM)
↔
AS(aM)∈M
M |=∃rS(aM)
↔
∃b. rS(aM, b)∈M
M |=∃r−
S (aM)
↔
∃b. rS(b, aM)∈M.
The knowledge base is core-complete if such a node cannot be found.
4
Actions
We now introduce a formal language to encode mutating actions. Let us remind
ourselves that, in our application of interest, the execution of a mutating action
modiﬁes the conﬁguration of a deployment by either adding new resource
instances, deleting existing ones, or modifying their settings. Here, we intro-
duce a framework for DL-LiteF core-closed knowledge base updates, triggered
by the execution of an action that enables all the above mentioned eﬀects. The

Actions over Core-Closed Knowledge Bases
287
only component of the core-closed knowledge base that is modiﬁed by the action
execution is M; while T , S, and A remain unchanged. As a consequence of
updating M, actions can introduce new individuals and delete old ones, thus
updating the set IM as well. Note that this may force changes outside IM due
to the axioms in T and S. The eﬀects of applying an action over M depend
on a set of input parameters that will be instantiated at execution time, result-
ing in diﬀerent assertions being added or removed from M. As a consequence
of assertions being added, fresh individuals might be introduced in the active
domain of M, including both model nodes from IM and boundary nodes from
IB. Diﬀerently, as a consequence of assertions being removed, individuals might
be removed from the active domain of M, including model nodes from IM but
not including boundary nodes from IB. In fact, boundary nodes are owned by
the open portion of the knowledge base and are known to exist regardless of them
being used in M. We invite the reader to review the set deﬁnitions for A- and
M-assertions (Sect. 2) to note that it is indeed possible for a generic boundary
individual a involved in an M-assertion to also be involved in an A-assertion.
4.1
Syntax
An action is deﬁned by a signature and a body. The signature consists of an
action name and a list of formal parameters, which will be replaced with actual
parameters at execution time. The body, or action eﬀect, can include conditional
statements and concatenation of atomic operations over M-assertions. For exam-
ple, let α be the action act(⃗x) = γ; that is, the action denoted by signature act(⃗x)
and body γ, with signature name act, signature parameters ⃗x, and body eﬀect γ.
Since it contains unbound parameters, or free variables, action α is ungrounded
and needs to be instantiated with actual values in order to be executed over
a set M. In the following, we assume the existence of a set Var, of variable
names, and consider a generic input parameters substitution ⃗θ : Var →I, which
replaces each variable name by an individual node. For simplicity, we will denote
an ungrounded action by its eﬀect γ, and a grounded action by the composition
of its eﬀect with an input parameter substitution γ⃗θ. Action eﬀects can either
be complex or basic. The syntax of complex action eﬀects γ and basic eﬀects β
is constrained by the following grammar.
γ ::= ϵ | β · γ | [ ϕ ⇝β ] · γ
β ::= ⊕x S | ⊖x S | ⊙xnewS | ⊖x
The complex action eﬀects γ include: the empty eﬀect ( ϵ ), the execution of
a basic eﬀect followed by a complex one ( β · γ ), and the conditional execution
of a basic eﬀect upon evaluation of a formula ϕ over the set M ( [ ϕ ⇝β ] · γ ).
The basic action eﬀects β include: the addition of a set S of M-assertions to the
subset Mx ( ⊕xS ), the removal of a set S of M-assertions from the subset Mx
(⊖xS ), the addition of a fresh subset Mxnew containing all the M-assertions in
the set S ( ⊙xnewS ), and the removal of an existing Mx subset in its entirety
( ⊖x ). The set S, the formula ϕ, and the operators ⊕/⊖might contain free

288
C. Cauli et al.
variables. These variables are of two types: (1) variables that are replaced by
the grounding of the action input parameters, and (2) variables that are the
answer variables of the formula ϕ and appear in the nested eﬀect β.
Example 2. The following is the deﬁnition of the action createBucket from the
API reference of the AWS resource type S3::Bucket. The input parameters are
two: the new bucket name “name” and the canned access control list “acl” (one
of Private, PublicRead, PublicReadWrite, AuthenticatedRead, etc.). The eﬀect of
the action is to add a fresh subset Mx for the newly introduced individual x
containing the two assertions S3::Bucket(x) and accessControl(x, y).
createBucket(x : name, y : acl) = ⊙x{S3::Bucket(x), accessControl(x, y)} · ϵ
The action needs to be instantiated by a speciﬁc parameter assignment, for
example the substitution θ = [ x ←DataBucket, y ←Private ], which binds
the variable x to the node DataBucket and the variable y to the node Private,
both taken from a pool of inactive nodes in I.
Action Query ϕ. The syntax introduced in the previous paragraph allows for
complex actions that conditionally execute a basic eﬀect β depending on the
evaluation of a formula ϕ over M. This is done via the construct [ ϕ ⇝β ] · γ.
The formula ϕ might have a set ⃗y of answer variables that appear free in its body
and are then bound to concrete tuples of nodes during evaluation. The answer
tuples are in turn used to instantiate the free variables in the nested eﬀect β.
We call ϕ the action query since we use it to select all the nodes that will be
involved in the action eﬀect. According to the grammar below, ϕ is a boolean
combination of M-assertions potentially containing free variables.
ϕ ::= AS(t) | RS(t1, t2) | ϕ1 ∧ϕ2 | ϕ2 ∨ϕ2 | ¬ϕ
In particular, AS is a symbol from the set CS of partially-closed concepts;
RS is a symbol from the set RS of partially-closed roles; and t, t1, t2 are either
individual or variable names from the set I ⊎Var, chosen in such a way that
the resulting assertion is an M-assertion. Since the formula ϕ can only refer
to M-assertions, which are interpreted under a closed semantics, its evaluation
requires looking at the content of the set M. A formula ϕ with no free variables is
a boolean formula and evaluates to either true or false. A formula ϕ with answer
variables ⃗y and arity ar(ϕ) evaluates to all the tuples ⃗t, of size equal the arity of
ϕ, that make the formula true in M. The free variables of ϕ can only appear in
the action β such that ϕ ⇝β. We denote by ANS(ϕ, M) the set of answers to
the action query ϕ over M. It is easy to see that the maximum number of tuples
that could be returned by the evaluation (that is, the size of the set ANS(ϕ, M))
is bounded by |IM ⊎IB|ar(ϕ), in turn bounded by ( 2|M| )2|ϕ|.
Example 3. The following example shows the encoding of the S3 API opera-
tion called deleteBucketEncryption, which requires as unique input parameter
the name of the bucket whose encryption conﬁguration is to be deleted. Since

Actions over Core-Closed Knowledge Bases
289
a bucket can have multiple encryption conﬁguration rules (each prescribing dif-
ferent encryption keys and algorithms to be used) we use an action query ϕ to
select all the nodes that match the assertions structure to be removed.
ϕ[y, k, z](x) = S3::Bucket(x) ∧encrRule(x, y) ∧SSEKey(y, k) ∧SSEAlgo(y, z)
The query ϕ is instantiated by the speciﬁc bucket instance (which will replace
the variable x) and returns all the triples (y, k, z) of encryption rule, key, and
algorithm, respectively, which identify the assertions corresponding to the dif-
ferent encryption conﬁgurations that the bucket has. The answer variables are
then used in the action eﬀect to instantiate the assertions to remove from Mx:
deleteBucketEncryption(x : name)
= [ϕ[y, k, z](x) ⇝⊖x{encrRule(x, y), SSEKey(y, k), SSEAlgo(y, z)}] · ϵ
4.2
Semantics
So far, we have described the syntax of our action language and provided two
examples that showcase the encoding of real-world API calls. Now, we deﬁne the
semantics of action eﬀects with respect to the changes that they induce over a
knowledge base. Let us recall that given a substitution ⃗θ for the input parameters
of an action γ, we denote by γ⃗θ the grounded action where all the input variables
are replaced according to what prescribed by ⃗θ. Let us also recall that the eﬀects
of an action apply only to assertions in M and individuals from IM, and cannot
aﬀect nodes and assertions from the open portion of the knowledge base.
The execution of a grounded action γ⃗θ over a DL-LiteF core-closed knowledge
base K = (T , A, S, M), deﬁned over the set IM of partially-closed individuals,
generates a new knowledge base Kγ⃗θ = (T , A, S, Mγ⃗θ), deﬁned over an updated
set of partially-closed individuals IMγ⃗θ. Let S be a set of M-assertions, γ a com-
plex action, ⃗θ an input parameter substitution, and ⃗ρ a generic substitution that
potentially replaces all free variables in the action γ. Let ⃗ρ1 and ⃗ρ2 be two substi-
tutions with signature Var →I such that dom(⃗ρ1)∩dom(⃗ρ2) = ∅; we denote their
composition by ⃗ρ1⃗ρ2 and deﬁne it as the new substitution such that ⃗ρ1⃗ρ2(x) = a
if ⃗ρ1(x)=a ∨⃗ρ2(x)=a, and ⃗ρ1⃗ρ2(x) = ⊥if ⃗ρ1(x)=⊥∧⃗ρ2(x)=⊥. We formalize
the application of the grounded action γ⃗θ as the transformation Tγ⃗θ that maps
the pair

M, IM
into the new pair

M′, IM′
. We sometimes use the nota-
tion Tγ⃗θ(M) or Tγ⃗θ(IM) to refer to the updated MBox or to the updated set of
model nodes, respectively. The rules for applying the transformation depend on
the structure of the action γ and are reported in Fig. 1. The transformation starts
with an initial generic substitution ⃗ρ = ⃗θ. As the transformation progresses, the
generic substitution ⃗ρ can be updated only as a result of the evaluation of an
action query ϕ over M. Precisely, all the tuples ⃗t1, ..., ⃗tn making ϕ true in M
will be considered and composed with the current substitution ⃗ρ generating n
fresh substitutions ⃗
ρt1, ..., ⃗
ρtn which are used in the subsequent application of
the nested eﬀect β. Since the core M of the knowledge base K changes at every

290
C. Cauli et al.
action execution, its domain of model nodes IM changes as well. The execution
of an action γ⃗θ over the knowledge base K = (T , A, S, M) with set of model
nodes IM could generate a new Kγ⃗θ = (T , A, S, Mγ⃗θ) with a new set of model
nodes IM′ that is not core-complete or not open-consistent (see Sect. 3 for the
corresponding deﬁnitions). We illustrate two examples next.
Fig. 1. Semantic of the action language deﬁned over the MBox M and set IM.
Example 4 (Violation of core-completeness). Consider the case where the gen-
eral speciﬁcations of the system require all objects of type bucket to have a log-
ging conﬁguration, and an action that removes the logging conﬁguration from
a bucket. Consider the core-closed knowledge base K where S = {S3::Bucket ⊑
∃loggingConﬁguration} and M = {S3::Bucket(b), loggingConﬁguration(b, c)} (con-
sistent wrt S) and the action γ deﬁned as
deleteLoggingConﬁguration(x : name)
= [(ϕ[y](x) = S3::Bucket(x) ∧loggingConﬁguration(x, y))
⇝⊖x{loggingConﬁguration(x, y)}] · ϵ
For the input parameter substitution ⃗θ = [x ←b], it is easy to see that the
transformation Tγ⃗θ applied to M results in the update Mγ⃗θ = {S3::Bucket(b)},
which is not core-complete.
Example 5 (Violation of open-consistency). Consider the case where an action
application indirectly aﬀects boundary nodes and their properties, leading to
inconsistencies in the open portion of the knowledge base. For example, when
the knowledge base prescribes that buckets used to store logs cannot be pub-
lic; however, a change in the conﬁguration of a bucket instance causes a sec-
ond bucket (initially known to be public) to also become a log store. In
particular, this happens when the knowledge base K contains the T -axiom
∃loggingDestination−⊑¬PublicBucket and the A-assertion PublicBucket(b), and

Actions over Core-Closed Knowledge Bases
291
we apply an action that introduces a new bucket storing its logs to b, deﬁned as
follows:
createBucketWithLogging(x : name, y : log)
= ⊙x{S3::Bucket(x), loggingDestination(x, y)}
For the input parameter substitution ⃗θ = [x ←newBucket, y ←b], the result
of applying the transformation Tγ⃗θ is the set M = {S3::Bucket(newBucket),
loggingDestination(newBucket, b)} which, combined with the pre-existing and
unchanged sets T and A, causes the updated Kγ⃗θ to be not open-consistent.
From a practical point of view, the examples highlight the need to re-evaluate
core-completeness and open-consistency of a core-closed knowledge base after
each action execution. Detecting a violation to core-completeness signals that we
have modeled an action that is inconsistent with respect to the systems speciﬁ-
cations, which most likely means that the action is missing something and needs
to be revised. Detecting a violation to open-consistency signals that our action,
even when consistent with respect to the speciﬁcations, introduces a change that
conﬂicts with other assumptions that we made about the system, and generally
indicates that we should either revise the assumptions or forbid the application
of the action. Both cases are important to consider in the development life cycle
of the core-closed KB and the action deﬁnitions.
5
Static Veriﬁcation
In this section, we investigate the problem of computing whether the execution of
an action, no matter the speciﬁc instantiation, always preserves given properties
of core-closed knowledge bases. We focus on properties expressed as Must/May
queries and deﬁne the static veriﬁcation problem as follows.
Deﬁnition 1 (Static Veriﬁcation). Let K be a DL-LiteF core-closed knowl-
edge base, q be a Must/ May query, and γ be an action with free variables from
the language presented above. Let ⃗θ be an assignment for the input variables of
γ that transforms γ into the grounded action γ⃗θ. Let Kγ⃗θ be the DL-LiteF core-
closed knowledge base resulting from the application of the grounded action γ⃗θ
onto K. We say that the action γ “preserves q over K” iﬀfor every grounded
instance γ⃗θ we have that ANS(q, K) = ANS(q, Kγ⃗θ). The static veriﬁcation prob-
lem is that of determining whether an action γ is q-preserving over K.
An action γ is not q-preserving over K iﬀthere exists a grounding ⃗θ for
the input variables of γ such that ANS(q, K) ̸= ANS(q, Kγ⃗θ); that is, ﬁxed
the grounding ⃗θ there exists a tuple ⃗t for q’s answer variables such that
⃗t ∈ANS(q, K) ∖ANS(q, Kγ⃗θ) or ⃗t ∈ANS(q, Kγ⃗θ) ∖ANS(q, K).
Theorem 1 (Complexity of the Static Veriﬁcation Problem). The static
veriﬁcation problem, i.e.deciding whether an action γ is q-preserving over K, can
be decided in PTime in data complexity and ExpTime in the arities of γ and q.

292
C. Cauli et al.
Proof. The proof relies on the fact that one could: enumerate all possible assign-
ments ⃗θ; compute the updated knowledge bases Kγ⃗θ; check whether these are
fully satisﬁable; enumerate all tuples ⃗t for the query q; and, ﬁnally, check whether
there exists at least one such tuple that satisﬁes q over K but not Kγ⃗θ or vice
versa. The number of assignments ⃗θ is bounded by

|IM ⊎IK|+ar(γ)
ar(γ) as it
is suﬃcient to replace each variable appearing in the action γ either by a known
object from IM ⊎IK or by a fresh one. The computation of the updated Kγ⃗θ is
done in polynomial time in M (and is exponential in the size of the action γ) as
it may require the evaluation of an internal action query ϕ and the consecutive
re-application of the transformation for a number of tuples that is bounded by a
polynomial over the size of M. As explained in Sect. 3, checking full satisﬁability
of the resulting core-closed knowledge base is also polynomial in M. The number
of tuples ⃗t is bounded by

|IM ⊎IK| + ar(γ)
ar(q) as it is enough to consider
all those tuples involving known objects plus the fresh individuals introduced
by the assignment ⃗θ. Checking whether a tuple ⃗t satisﬁes the query q over a
core-closed knowledge base is decided in LogSpace in the size of M [15] which
is, thus, also polynomial in M.
6
Planning
As discussed throughout the paper, the execution of a mutating action modi-
ﬁes the conﬁguration of a deployment and potentially changes its posture with
respect to a given set of requirements. In the previous two sections, we intro-
duced a language to encode mutating actions and we investigated the problem
of checking whether the application of an action preserves the properties of a
core-closed knowledge base. In this section, we investigate the plan existence
and synthesis problems; that is, the problem of deciding whether there exists
a sequence of grounded actions that leads the knowledge base to a state where
a certain requirement is met, and the problem of ﬁnding a set of such plans,
respectively. We start by deﬁning a notion of transition system that is gen-
erated by applying actions to a core-closed knowledge base and then use this
notion to focus on the mentioned planning problems. As in classical planning,
the plan existence problem for plans computed over unbounded domains is unde-
cidable [17,19]. The undecidability proof is done via reduction from the Word
problem. The problem of deciding whether a deterministic Turing machine M
accepts a word w ∈{0, 1}∗is reduced to the plan existence problem. Since unde-
cidability holds even for basic action eﬀects, we can show undecidability over an
unbounded domain by using the same encoding of [1].
Transition Systems. In the style of the work done in [10,21], the combination
of a DL-LiteF core-closed knowledge base and a set of actions can be viewed
as the transition system it generates. Intuitively, the states of the transition
system correspond to MBoxes and the transitions between states are labeled by
grounded actions. A DL-LiteF core-closed knowledge base K = (T , A, S, M0),
deﬁned over the possibly inﬁnite set of individuals I (and model nodes IM
0
⊆I)

Actions over Core-Closed Knowledge Bases
293
and the set Act of ungrounded actions, generates the transition system (TS) ΥK =
(I, T , A, S, Σ, M0, →) where Σ is a set of fully satisﬁable (i.e., core-complete and
open-consistent) MBoxes; M0 is the initial MBox; and →⊆Σ × LAct × Σ is a
labeled transition relation with LAct the set of all possible grounded actions.
The sets Σ and →are deﬁned by mutual induction as the smallest sets such
that: if Mi ∈Σ then for every grounded action γ⃗θ ∈LAct such that the fresh
MBox Mi+1 resulting from the transformation Tγ⃗θ is core-complete and open-
consistent, we have that Mi+1 ∈Σ and (Mi, γ⃗θ, Mi+1) ∈→.
Since we assume that actions have input parameters that are replaced during
execution by values from I, which contains both known objects from IM ⊎IK
and possibly inﬁnitely many fresh objects, the generated transition system ΥK is
generally inﬁnite. To keep the planning problem decidable, we concentrate on a
known ﬁnite subset D ⊂I containing all the fresh nodes and value assignments to
action variables that are of interest for our application. In the remainder of this
paper, we discuss the plan existence and synthesis problem for ﬁnite transition
systems ΥK = (D, T , A, S, Σ, M0, →), whose states in Σ have a domain that is
also bounded by D.
The Plan Existence Problem. A plan is a sequence of grounded actions whose
execution leads to a state satisfying a given property. Let K = (T , A, S, M0)
be a DL-LiteF core-closed knowledge base; Act be a set of ungrounded actions;
and let ΥK = (D, T , A, S, Σ, M0, →) be its generated ﬁnite TS. Let π be a ﬁnite
sequence γ1⃗θ1 · · · γn⃗θn of grounded actions taken from the set LAct. We call the
sequence π consistent iﬀthere exists a run ρ = M0
γ1⃗θ1
−−−→M1
γ2⃗θ2
−−−→· · ·
γn⃗θn
−−−→Mn
in ΥK. Let q be a Must/May query mentioning objects from adom(K) and ⃗t a
tuple from the set adom(K)ar(q). A consistent sequence π of grounded actions
is a plan from K to (⃗t, q) iﬀ⃗t ∈ANS(q, Kn = (T , A, S, Mn)) with Mn the ﬁnal
state of the run induced by π.
Deﬁnition 2 (Plan Existence). Given a DL-LiteF core-closed knowledge base
K, a tuple ⃗t, and a Must/ May query q, the plan existence problem is that of
deciding whether there exists a plan from K to (⃗t, q).
Example 6. Let us consider the transition system ΥK generated by the core-
closed knowledge base K = (T , A, S, M0) having the set of partially-closed
assertions M0 deﬁned as
{S3::Bucket(b), KMS::Key(k), bucketEncryptionRule(b, r), bucketKey(r, k),
bucketKeyEnabled(r, true), enableKeyRotation(k, false)}
and the set of action labels Act containing the actions deleteBucket, createBucket,
deleteKey, createKey, enableKeyRotation, putBucketEncryption, and deleteBucke-
tEncryption. Let us assume that we are interested in verifying the existence of a
sequence of grounded actions that when applied onto the knowledge base would
conﬁgure the bucket node b to be encrypted with a rotating key. Formally, this
is equivalent to checking the existence of a consistent plan π that when executed

294
C. Cauli et al.
on the transition system ΥK leads to a state Mn such that the tuple ⃗t = b is in
the set ANS(q, Kn = (T , A, S, Mn)) for q the query
q[x] = S3::Bucket(x) ∧Must

∃y, z. bucketSSEncryption(x, y) ∧
bucketKey(y, z) ∧enableKeyRotation(z, true)

It is easy to see that the following three sequences of grounded actions are
valid plans from K to (b, q):
π1 = enableKeyRotation(k)
π2 = createKey(k1) · enableKeyRotation(k1) · putBucketEncryption(b, k1)
π3 = deleteBucketEncryption(b, k) · createKey(k1) · enableKeyRotation(k1)·
putBucketEncryption(b, k1)
If, for example, a bucket was only allowed to have one encryption (by means
of a functional axiom in S), then π2 would not be a valid plan, as it would
generate an inconsistent run leading to a state Mi that is not open-consistent
w.r.t. S.
Lemma 3. The plan existence problem for a ﬁnite transition system ΥK gener-
ated by a DL-LiteF core-closed knowledge base K and a set of actions Act, over
a ﬁnite domain of objects D, reduces to graph reachability over a graph whose
number of states is at most exponential in the size of D.
The Plan Synthesis Problem. We now focus on the problem of ﬁnding plans
that satisfy a given condition. As discussed in the previous paragraph, we are
mostly driven by query answering; in particular, by conditions corresponding
to a tuple (of objects from our starting deployment conﬁguration) satisfying a
given requirement expressed as a Must/May query. Clearly, this problem is
meaningful in our application of interest because it corresponds to ﬁnding a set
of potential sequences of changes that would allow one to reach a conﬁguration
satisfying (resp., not satisfying) one, or more, security mitigations (resp., vul-
nerabilities). We concentrate on DL-LiteF core-closed knowledge bases and their
generated ﬁnite transition systems, where potential fresh objects are drawn from
a ﬁxed set D. We are interested in sequences of grounded actions that are min-
imal and ignore sequences that extend these. We sometimes call such minimal
sequences simple plans. A plan π from an initial core-closed knowledge base K
to a goal condition b is minimal (or simple) iﬀthere does not exist a plan π′
(from the same initial K to the same goal condition b) s.t. π = π′ · σ, for σ a
non-empty suﬃx of grounded actions.
In Algorithm 1, we present a depth-ﬁrst search algorithm that, starting from
K, searches for all simple plans that achieve a given target query membership
condition. The transition system ΥK is computed, and stored, on the ﬂy in the
Successors sub-procedure and the graph is explored in a depth-ﬁrst search traver-
sal fashion.

Actions over Core-Closed Knowledge Bases
295
Algorithm 1: FindPlans(K, D, Act,
⃗t, q

)
Inputs : A ccKB K = (T , A, S, M0), a domain D, a set of actions Act
and a pair
⃗t, q

of an answer tuple and a Must/May query
Output: A possibly empty set Π of consistent simple plans
1 def FindPlans ( K, D, Act,
⃗t, q

):
2
Π := ∅;
3
S := ⊥;
4
AllPlanSearch(M0, ϵ, ∅, K, D, Act,
⃗t, q

) ;
5
return Π;
6 def AllPlanSearch ( M, π, V, K, D, Act,
⃗t, q

):
7
if
M ∈V then
8
return;
9
if ⃗t ∈ANS(q, ⟨T , A, S, M⟩) then
10
Π := Π ∪{π};
11
return;
12
Q := ∅;
13
foreach

γ⃗θ, M′
∈Successors(M, Act, D) do
14
Q.push(

γ⃗θ, M′
);
15
V := V ∪{M};
16
while Q ̸= ∅do
17

γ⃗θ, M′
= Q.pop();
18
AllPlanSearch(M′, π · γ⃗θ, V, K, D, Act,
⃗t, q

);
19
V := V ∖{M};
20
return;
21 def Successors (M, Act, D):
22
if S[M] is deﬁned then
23
return S[M];
24
N := ∅;
25
foreach γ ∈Act, ⃗θ ∈Dar(γ) do
26
M′ := Tγ⃗θ(M);
27
if
M′is fully satisﬁable then
28
N := N ∪{

γ⃗θ, M′
}
29
S[M] := N;
30
return N;
We note that the condition ⃗t ∈ANS(q, ⟨T , A, S, M⟩) (line 9) could be
replaced by any other query satisﬁability condition and that one could easily
rewrite the algorithm to be parameterized by a more general boolean goal. For

296
C. Cauli et al.
example, the condition that a given tuple ⃗t is not an answer to a query q over
the analyzed state, with the query q representing an undesired conﬁguration,
or a boolean formula over multiple query membership assertions. We also note
that Algorithm 1 could be simpliﬁed to return only one simple plan, if a plan
exists, or NULL, if a plan does not exist, thus solving the so-called plan generation
problem. We refer the reader to the full version of this paper [16] containing the
plan generation algorithm (full version, Appendix A.1) and the proofs of Theo-
rem 2 and 3 below (full version, Appendices A.2 and A3, respectively).
Theorem 2 (Minimal Plan Synthesis Correctness). Let K be a DL-LiteF
core-closed knowledge base, D be a ﬁxed ﬁnite domain, Act be a set of ungrounded
action labels, and
⃗t, q

be a goal. Then a plan π is returned by the algorithm
FindPlans(K, D, Act,
⃗t, q

) if and only if π is a minimal plan from K to
⃗t, q

.
Theorem 3 (Minimal Plan Synthesis Complexity). The FindPlans algo-
rithm runs in polynomial time in the size of M and exponential time in size
of D.
7
Related Work
The syntax of the action language that we presented in this paper is similar to
that of [1,12,13]. Diﬀerently from their work, we disallow complex action eﬀects
to be nested inside conditional statements, and we deﬁne basic action eﬀects that
consist purely in the addition and deletion of concept and role M-assertions.
Thus, our actions are much less general than those used in their framework.
The semantics of their action language is deﬁned in terms of changes applied to
instances, and the action eﬀects are captured and encoded through a variant of
ALCHOIQ called ALCHOIQbr. In our work, instead, the execution of an action
updates a portion of the core-closed knowledge base K—the core M, which is
interpreted under a close-world assumption and can be seen as a partial assign-
ment for the interpretations that are models of K. Since we directly manipulate
M, the semantics of our actions is more similar to that of [21] and, in general, to
ABox updates [22,23]. Like the frameworks introduced in [9–11,20], our actions
are parameterized and when combined with a core-closed knowledge base gener-
ate a transition system. In [11], the authors focus on a variant of Knowledge and
Action Bases [21] called Explicit-Input KABs (eKABs); in particular, on ﬁnite
and on state-bounded eKABs, for which planning existence is decidable. Our
generated transition systems are an adaptation of the work done in Description
Logic based Dynamic Systems, KABs, and eKABs to our setting of core-closed
knowledge bases. In [24], the authors address decidability of the plan existence
problem for logics that are subset of ALCOI. Their action language is similar
to the one presented in this paper; including pre-conditions, in the form of a
set of ABox assertions, post-conditions, in the form of basic addition or removal
of assertions, concatenation, and input parameters. In [11], the plan synthesis

Actions over Core-Closed Knowledge Bases
297
problem is discussed also for lightweight description logics. Relying on the FOL-
reducibility of DL-LiteA, it is shown that plan synthesis over DL-LiteA can be
compiled into an ADL planning problem [25]. This does not seem possible in our
case, as not all necessary tests over core-closed knowledge bases are known to be
FOL-reducible. In [10] and [9], the authors concentrate on verifying and synthe-
sizing temporal properties expressed in a variant of μ-calculus over description
logic based dynamic systems, both problems are relevant in our application sce-
nario and we will consider them in future works.
8
Conclusion
We focused on the problem of analyzing cloud infrastructure encoded as descrip-
tion logic knowledge bases combining complete and incomplete information.
From a practical standpoint, we concentrated on formalizing and foreseeing the
impact of potential changes pre-deployment. We introduced an action language
to encode mutating actions, whose semantics is given in terms of changes induced
to the complete portion of the knowledge base. We deﬁned the static veriﬁca-
tion problem as the problem of deciding whether the execution of an action, no
matter the speciﬁc parameters passed, always preserves a set of properties of
the knowledge base. We characterized the complexity of the problem and pro-
vided procedural steps to solve it. We then focused on three formulations of the
classical AI planning problem: namely, plan existence, generation, and synthesis.
In our setting, the planning problem is formulated with respect to the transi-
tion system arising from the combination of a core-closed knowledge base and
a set of actions; goals are given in terms of one, or more, Must/May conjunc-
tive query membership assertion; and plans of interest are simple sequences of
parameterized actions.
Acknowledgments. This work is supported by the ERC Consolidator grant D-
SynMA (No. 772459).
References
1. Ahmetaj, S., Calvanese, D., Ortiz, M., Simkus, M.: Managing change in graph-
structured data using description logics. ACM Trans. Comput. Log. 18(4), 27:1–
27:35 (2017)
2. Artale, A., Calvanese, D., Kontchakov, R., Zakharyaschev, M.: The DL-lite family
and relations. J. Artif. Intell. Res. 36, 1–69 (2009)
3. Baader, F., Horrocks, I., Lutz, C., Sattler, U.: An Introduction to Description
Logic. Cambridge University Press, Cambridge (2017)
4. Backes, J., et al.: Reachability analysis for AWS-based networks. In: Dillig, I.,
Tasiran, S. (eds.) CAV 2019. LNCS, vol. 11562, pp. 231–241. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-25543-5 14
5. Backes, J., et al.: Stratiﬁed abstraction of access control policies. In: Lahiri, S.K.,
Wang, C. (eds.) CAV 2020. LNCS, vol. 12224, pp. 165–176. Springer, Cham (2020).
https://doi.org/10.1007/978-3-030-53288-8 9

298
C. Cauli et al.
6. Backes, J., et al.: Semantic-based automated reasoning for AWS access policies
using SMT. In: Bjørner, N., Gurﬁnkel, A. (eds.) 2018 Formal Methods in Computer
Aided Design, FMCAD 2018, Austin, TX, USA, 30 October–2 November 2018, pp.
1–9. IEEE (2018). https://doi.org/10.23919/FMCAD.2018.8602994
7. Bouchet, M., et al.: Block public access: trust safety veriﬁcation of access control
policies. In: Devanbu, P., Cohen, M.B., Zimmermann, T. (eds.) ESEC/FSE 2020:
28th ACM Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering, Virtual Event, USA, 8–13 November
2020, pp. 281–291. ACM (2020). https://doi.org/10.1145/3368089.3409728
8. Calvanese, D., Giacomo, G.D., Lembo, D., Lenzerini, M., Rosati, R.: EQL-lite:
eﬀective ﬁrst-order query processing in description logics. In: Veloso, M.M. (ed.)
Proceedings of the 20th International Joint Conference on Artiﬁcial Intelligence,
IJCAI 2007, Hyderabad, India, 6–12 January 2007, pp. 274–279 (2007). http://
ijcai.org/Proceedings/07/Papers/042.pdf
9. Calvanese, D., De Giacomo, G., Montali, M., Patrizi, F.: Veriﬁcation and synthesis
in description logic based dynamic systems. In: Faber, W., Lembo, D. (eds.) RR
2013. LNCS, vol. 7994, pp. 50–64. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-39666-3 5
10. Calvanese, D., Montali, M., Patrizi, F., Giacomo, G.D.: Description logic based
dynamic systems: modeling, veriﬁcation, and synthesis. In: Yang, Q., Wooldridge,
M.J. (eds.) Proceedings of the Twenty-Fourth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires, Argentina, 25–31 July 2015, pp.
4247–4253. AAAI Press (2015). http://ijcai.org/Abstract/15/604
11. Calvanese, D., Montali, M., Patrizi, F., Stawowy, M.: Plan synthesis for knowl-
edge and action bases. In: Kambhampati, S. (ed.) Proceedings of the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelligence, IJCAI 2016, New York,
NY, USA, 9–15 July 2016, pp. 1022–1029. IJCAI/AAAI Press (2016). http://www.
ijcai.org/Abstract/16/149
12. Calvanese, D., Ortiz, M., Simkus, M.: Evolving graph databases under description
logic constraints. In: Eiter, T., Glimm, B., Kazakov, Y., Kr¨otzsch, M. (eds.) Infor-
mal Proceedings of the 26th International Workshop on Description Logics, Ulm,
Germany, 23–26 July 2013. CEUR Workshop Proceedings, vol. 1014, pp. 120–131.
CEUR-WS.org (2013). http://ceur-ws.org/Vol-1014/paper 82.pdf
13. Calvanese, D., Ortiz, M., Simkus, M.: Veriﬁcation of evolving graph-structured
data under expressive path constraints. In: Martens, W., Zeume, T. (eds.) 19th
International Conference on Database Theory, ICDT 2016, Bordeaux, France, 15–
18 March 2016. LIPIcs, vol. 48, pp. 15:1–15:19. Schloss Dagstuhl - Leibniz-Zentrum
f¨ur Informatik (2016). https://doi.org/10.4230/LIPIcs.ICDT.2016.15
14. Cauli, C., Li, M., Piterman, N., Tkachuk, O.: Pre-deployment security assessment
for cloud services through semantic reasoning. In: Silva, A., Leino, K.R.M. (eds.)
CAV 2021. LNCS, vol. 12759, pp. 767–780. Springer, Cham (2021). https://doi.
org/10.1007/978-3-030-81685-8 36
15. Cauli, C., Ortiz, M., Piterman, N.: Closed- and open-world reasoning in dl-lite
for cloud infrastructure security. In: Proceedings of the 18th International Confer-
ence on Principles of Knowledge Representation and Reasoning, KR 2021, Hanoi,
Vietnam (2021)
16. Cauli, C., Ortiz, M., Piterman, N.: Actions over core-closed knowledge bases
(2022). https://doi.org/10.48550/ARXIV.2202.12592. https://arxiv.org/abs/2202.
12592
17. Chapman, D.: Planning for conjunctive goals. Artif. Intell. 32(3), 333–377 (1987).
https://doi.org/10.1016/0004-3702(87)90092-0

Actions over Core-Closed Knowledge Bases
299
18. Cook, B.: Formal reasoning about the security of amazon web services. In: Chock-
ler, H., Weissenbacher, G. (eds.) CAV 2018. LNCS, vol. 10981, pp. 38–47. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-96145-3 3
19. Erol, K., Nau, D.S., Subrahmanian, V.S.: Complexity, decidability and undecidabil-
ity results for domain-independent planning. Artif. Intell. 76(1–2), 75–88 (1995).
https://doi.org/10.1016/0004-3702(94)00080-K
20. Giacomo, G.D., Masellis, R.D., Rosati, R.: Veriﬁcation of conjunctive artifact-
centric services. Int. J. Cooperative Inf. Syst. 21(2), 111–140 (2012). https://doi.
org/10.1142/S0218843012500025
21. Hariri, B.B., Calvanese, D., Montali, M., Giacomo, G.D., Masellis, R.D., Felli, P.:
Description logic knowledge and action bases. J. Artif. Intell. Res. 46, 651–686
(2013)
22. Kharlamov, E., Zheleznyakov, D., Calvanese, D.: Capturing model-based ontology
evolution at the instance level: the case of dl-lite. J. Comput. Syst. Sci. 79(6),
835–872 (2013). https://doi.org/10.1016/j.jcss.2013.01.006
23. Liu, H., Lutz, C., Milicic, M., Wolter, F.: Foundations of instance level updates in
expressive description logics. Artif. Intell. 175(18), 2170–2197 (2011). https://doi.
org/10.1016/j.artint.2011.08.003
24. Milicic, M.: Planning in action formalisms based on DLS: ﬁrst results. In: Cal-
vanese, D., et al. (eds.) Proceedings of the 2007 International Workshop on Descrip-
tion Logics (DL2007), Brixen-Bressanone, near Bozen-Bolzano, Italy, 8–10 June
2007. CEUR Workshop Proceedings, vol. 250. CEUR-WS.org (2007). http://ceur-
ws.org/Vol-250/paper 59.pdf
25. Pednault, E.P.D.: ADL and the state-transition model of action. J. Logic Comput.
4(5), 467–512 (1994). https://doi.org/10.1093/logcom/4.5.467
26. Tobies, S.: A NExpTime-complete description logic strictly contained in C2. In:
Flum, J., Rodriguez-Artalejo, M. (eds.) CSL 1999. LNCS, vol. 1683, pp. 292–306.
Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48168-0 21
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

GK: Implementing Full First Order
Default Logic for Commonsense
Reasoning (System Description)
Tanel Tammet1(B)
, Dirk Draheim2
, and Priit J¨arv1
1 Applied Artiﬁcial Intelligence Group, Tallinn University of Technology,
Tallinn, Estonia
{tanel.tammet,priit.jarv1}@taltech.ee
2 Information Systems Group, Tallinn University of Technology, Tallinn, Estonia
dirk.draheim@taltech.ee
Abstract. Our goal is to develop a logic-based component for hybrid –
machine learning plus logic – commonsense question answering systems.
The paper presents an implementation GK of default logic for handling
rules with exceptions in unrestricted ﬁrst order knowledge bases. GK is
built on top of our existing automated reasoning system with conﬁdence
calculation capabilities. To overcome the problem of undecidability of
checking potential exceptions, GK performs delayed recursive checks with
diminishing time limits. These are combined with the taxonomy-based
priorities for defaults and numerical conﬁdences.
1
Introduction
The problem of handling uncertainty is one of the critical issues when considering
the use of logic for automating commonsense reasoning. Most of the facts and
rules people use in their daily lives are uncertain. There are many types of
uncertainty, like fuzziness (is a person somewhat tall or very tall), conﬁdence
(how certain does some fact seem) and exceptions (birds can typically ﬂy, but
penguins, ostriches etc., can not). Some of these uncertainties, like fuzziness
and conﬁdence, can be represented numerically, while others, like rules with
exceptions, are discrete. In [18] we present the design and implementation of
the CONFER framework for extending existing automated reasoning systems
with conﬁdence calculation capabilities. In the current paper we present the
implementation called GK for default logic [13], built by further extending the
CONFER implementation. Importantly, we design a novel practical framework
for implementing default logic for the full, undecidable ﬁrst order logic on the
basis of a conventional resolution prover.
1.1
Default Logic
Default logic was introduced in 1980 by R. Reiter [13] to model one aspect of
common-sense reasoning: rules with exceptions. It has remained one of the most
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 300–309, 2022.
https://doi.org/10.1007/978-3-031-10769-6_18

GK: Default Logic for Commonsense Reasoning
301
well-known logic-based mechanisms devoted to this goal, with the circumscrip-
tion by J. McCarthy and the autoepistemic logic being the early alternatives.
Several similar systems have been proposed later, like defeasible logic [11].
Default logic [13] extends classical logic with default rules of the form
α(x) : β1(x), ...βn(x)
γ(x)
where a precondition α(x), justiﬁcations β1(x), ...βn(x) and a consequent γ(x)
are ﬁrst order predicate calculus formulas whose free variables are among
x = x1, ..., xm. For every tuple of individuals t = t1, ..., tn, if the precondition α(t)
is derivable and none of the negated justiﬁcations ¬β(t) are derivable from a given
knowledge base KB, then the consequent γ(t) can be derived from KB. Diﬀer-
ently from classical and most other logics, default logic is non-monotonic: adding
new assumptions can make some previously derivable formulas non-derivable.
As investigated in [7], the interpretation of quantiﬁers in default rules can
lead to several versions of default logic. We follow the original interpretation
of Reiter in [13] which requires the use of Skolemization in a speciﬁc manner
over default rules. For example, a default rule: ∃xP(x) ⊢∃xP(x) should be
interpreted as : P(c) ⊢P(c), where c is a Skolem constant.
Consider a typical example for default logic: birds can normally ﬂy, but pen-
guins cannot ﬂy. The classical logic part
penguin(p) & bird(b) & ∀x.penguin(x) ⇒bird(x) & ∀x.penguin(x) ⇒¬fly(x).
is extended with the default rule bird(x) : fly(x) ⊢fly(x). From here we can
derive that an arbitrary bird b can ﬂy, but a penguin p cannot. The default
rule cannot be applied to p, since a contradiction is derivable from fly(p). This
argument cannot be easily modelled using numerical conﬁdences: the probability
of an arbitrary living bird being able to ﬂy is relatively high, while the penguins
form a speciﬁc subset of birds, for which this probability is zero.
Another well-known example – Nixon’s triangle – introduces the prob-
lem of multiple extensions and sceptical vs credulous entailment: the classical
facts republican(nixon) & quaker(nixon) extended with two mutually exclud-
ing default rules republican(x) : ¬paciﬁst(x) ⊢¬paciﬁst(x) and quaker(x) :
paciﬁst(x) ⊢paciﬁst(x). The credulous entailment allows giving diﬀerent priori-
ties to the default rules and accepts diﬀerent sets (extensions) of consequences, if
there is a way to assign priorities so that all the consequences in an extension can
be derived. The sceptical entailment requires that a consequence is present in all
extensions. GK follows the latter interpretation, but allows explicit priorities to
be assigned to the default rules.
The concept of priorities for default rules has been well investigated, with
several mechanisms proposed. G. Brewka argues in [4] that “for realistic applica-
tions involving default reasoning it is necessary to reason about the priorities of
defaults” and introduces an ordering of defaults based on speciﬁcity: default rules
for a more speciﬁc class of objects should take priority over rules for more gen-
eral classes. For example, since birds (who typically do ﬂy) are physical objects

302
T. Tammet et al.
and physical objects typically do not ﬂy, we have contradictory default rules
describing the ﬂying capability of arbitrary birds. Since birds are a subset of
physical objects, the ﬂying rule of birds should have a higher priority than the
non-ﬂying rule of physical objects.
1.2
Undecidability, Grounding and Implementations
Perhaps the most signiﬁcant problem standing in the way of automating default
logic is undecidability of the applicability of rules. Indeed, in order to apply a
default rule, we must prove that the justiﬁcations do not lead to a contradic-
tion with the rest of the knowledge base KB. For full ﬁrst order logic this is
undecidable. Hence, the standard approach for handling default logic has been
creating a large ground instance KBg of the KB, and then performing decidable
propositional reasoning on the KBg.
Almost all the existing implementations of default logic like DeReS [5], DLV2
[1] or CLINGO [8], with the noteworthy exception of s(CASP) [2], follow the
same principle. More generally, the ﬁeld of Answer Set Programming (ASP), see
[10], is devoted to this approach. As an exception, the s(CASP) system [2] solves
queries without the grounding step and is thus better suited for large domains.
It is noteworthy that the s(CASP) system has been used in [9] for automating
common sense reasoning for autonomous driving with the help of default rules.
However, s(CASP) is a logic programming system, not a universal automated
reasoner. For example, when we add a rule bird(father(X)) :- bird(X) to
the formulation of the above birds example in s(CASP), the search does not
terminate, apparently due to the inﬁnitely growing nesting of terms.
While ASP systems are very well suited for speciﬁc kinds of problems over a
small ﬁnite domain, grounding becomes infeasible for large ﬁrst order knowledge
bases (KB in the following), in particular when the domain is inﬁnite and nested
terms can be derived from the KB. The approach described in this paper accepts
the lack of logical omniscience and performs delayed recursive checking of excep-
tions with diminishing time limits directly on non-grounded clauses, combined
with the taxonomy-based priorities for defaults and numerical conﬁdences.
2
Algorithms
Our approach of implementing default rules in GK for ﬁrst order logic is to
delay justiﬁcation checking until a ﬁrst-order proof is found and then perform
recursively deepening checks with diminishing time limits. Thus, our system ﬁrst
produces a potentially large number of diﬀerent candidate proofs and then enters
a recursive checking phase. The idea of delaying justiﬁcation checking is already
present in the original paper of R. Reiter [13], where he uses linear resolution
and delayed checks as the main machinery of his proofs. The results produced by
GK thus depend on the time limits and are not stable. Showing speciﬁc ﬁxpoint
properties of the algorithm is not in the scope of our paper.

GK: Default Logic for Commonsense Reasoning
303
A practical question for implementation is the actual representation of default
rules and making the rules ﬁt the ﬁrst-order proof search machinery. To this
end we introduce blocker atoms which are similar to the justiﬁcation indexes of
Reiter.
In the following we will assume that the underlying ﬁrst order reasoner uses
the resolution method, see [3] for details. The rest of the paper assumes famil-
iarity with the basic concepts, terminology and algorithms of the resolution
method.
2.1
Background: Queries and Answers
We assume our system is presented with a question in one of two forms: (1) Is
the statement Q true? (2) Find values V for existentially bound variables in Q
so that Q is true. For simplicity’s sake we will assume that the statement Q is in
the preﬁx form, i.e., no quantiﬁers occur in the scope of other logical connectives.
In the second case, it could be that several diﬀerent value vectors can be
assigned to the variables, essentially giving diﬀerent answers. We also note that
an answer could be a disjunction, giving possible options instead of a single
deﬁnite answer.
A widely used machinery in resolution-based theorem provers for extracting
values of existentially bound variables in Q is to use a special answer predicate,
converting a question statement Q to a formula ∃X(Q(X)&¬answer(X)) for
a tuple of existentially quantiﬁed variables X in Q [6]. Whenever a clause is
derived which consists of only answer predicates, it is treated as a contradiction
(essentially, answer) and the arguments of the answer predicate are returned as
the values looked for. A common convention is to call such clauses answer clauses.
We will require that the proof search does not stop whenever an answer clause
is found, but will continue to look for new answer clauses until a predetermined
time limit is reached. See [16] for a framework of extracting multiple answers.
We also assume that queries take a general form (KB&A) ⇒Q where KB is a
commonsense knowledge base, A is an optional set of precondition statements for
this particular question and Q is a question statement. The whole general query
form is negated and converted to clauses, i.e., disjunctions of literals (positive or
negative atoms). We will call the clauses stemming from the question statement
question clauses.
2.2
Blocker Atoms and Justiﬁcation Checking
Without loss of generality we assume that the precondition and consequent for-
mulas α and γ in default rules are clauses and justiﬁcations β1, ..., βn are lit-
erals, i.e. positive or negative atoms: α : β1, ...βn ⊢γ. Complex formulas can
be encoded with a new predicate over the free variables of the formula and an
equivalence of the new atom with the formula. Recall that Reiter assumes that
the default rules are Skolemized.
We encode a default rule as a clause by concatenating into one clause the pre-
condition and consequent clauses α(x) and γ(x) and blocker atoms block(¬β1),

304
T. Tammet et al.
..., block(¬βn) where each justiﬁcation βi is either a positive or a negative atom.
The negation ¬ is used since we prefer to speak about blockers and not justiﬁ-
catons. For example, the “birds can ﬂy” default rule is represented as a clause
¬bird(X) ∨fly(X) ∨block(0, neg(fly(X)))
where X is a variable and neg(fly(X)) encodes the negated justiﬁcation. The ﬁrst
argument of the blocker (0 above) encodes priority information covered in the
next section.
A proof of a question clause is a clause containing only answer atoms and
blocker atoms. In the justiﬁcation checking phase the system attempts to prove
each decoded second blocker argument ¬βi in turn: the proof is considered
invalid if some of ¬βi can be proved and this checking-proof itself is valid. If
we pose a question fly(X) ⇒answer(X) to the system to be proved (see the
earlier example), we get two diﬀerent answers: answer(p) ∨block(neg(fly(p))
and answer(b) ∨block(neg(fly(b)). Checking the ﬁrst of these means trying to
prove ¬fly(p) which succeeds, hence the ﬁrst answer is invalid. Checking the
second answer we try to prove ¬fly(b) which fails, hence the answer is valid.
Notice that the contents ¬βi of blockers, just like answer clauses, have a role
of collecting substitutions during the proof search: this enables us to disregard
the order in which the clauses are used, i.e. both top-down, bottom-up and mixed
proof search strategies can be used.
Importantly, blockers are used during the subsumption checks similarly to
ordinary literals. A clause C1 with fewer or more general literals than C2 is
hence always preferred to C2, given that (a) the literals of C1 subsume C2,
disregarding the priority arguments of blockers, and (b) the priority arguments
of corresponding blocker literals in C1 are equal or stronger than these of C2.
When combined with the uncertainty and inconsistency handling mechanisms of
CONFER, the subsumption restrictions of the latter also apply. There are also
other diﬀerences to ordinary literals. First, we prohibit the application of equality
(demodulation or paramodulation) to the contents of blocker atoms during proof
search. Second, we discard clauses containing mutually contradictory blockers
(assuming the decoding of the second argument) like we would discard ordinary
tautologies.
2.3
Priorities, Recursion and Inﬁnite Branches
Default rule priorities are critical for the practical encoding of commonsense
knowledge. The usage of priorities in proof search is simple: when checking a
blocker with a given priority, it is not allowed to use default rules with a lower
priority. We encode priority information as a ﬁrst argument of the blocker literal,
oﬀering several ways to determine priority: either as an integer, a taxonomy class
number, a string in a taxonomy or a combination of these with an integer.
For automatically using speciﬁcity we employ taxonomy classes: a class has
a higher prirority than those above it on the taxonomy branch. We have built a
topologically sorted acyclic graph of English words using the WordNet taxonomy

GK: Default Logic for Commonsense Reasoning
305
along with an eﬃcient algorithm for quick priority checks during proof search.
Taxonomy classes are indicated with a special term like $(61598). Alternatively
one can use an actual English word like $(“bird”) which is automatically rec-
ognized to be more speciﬁc than, say, $(“object”). To enable more ﬁne-grained
priorities, an integer can be added to the term like $(“bird”, 2) generating a
lexicographic order.
The recursive check for the non-provability of blockers can go arbitrarily deep,
except for the time limits. Our algorithm allocates N seconds for the whole proof
search and spends half of N for looking for diﬀerent proofs and answers for the
query, with the other half split evenly for each answer. Again, the time allocated
for checking an answer is split evenly between the blockers in the answer. Each
such time snippet is again split between a search for the proof of the blocker, and
if found, for recursively checking the validity of this proof. Once the allocated
time is below a given threshold (currently one millisecond) the proof is assumed
to be not found.
Answers given by the system depend on the amount of time given, the search
strategy chosen etc. For example, consider the Nixon triangle presented earlier,
with two contradictory default rules. In case the priorities of these rules are equal
and we allow defaults with the same priority to be used for checking an answer
containing a blocker, the recursive check terminates only because of a time limit,
which is unpredictable. Hence, we may sometimes get one answer and sometimes
another. In order to increase both stability and eﬃciency, GK checks the blockers
in the search nodes above, and terminates with failure in cases nonterminating
loops are detected. Therefore GK always gives a sceptical result to the Nixon
triangle: neither paciﬁst(nixon) nor ¬paciﬁst(nixon) is proven.
3
Conﬁdences and Inconsistencies
GK integrates the exception-handling algorithms described in the previous
chapter with the algorithms designed for handling inconsistent KB-s and numeric
conﬁdences assigned to clauses, previously presented as a CONFER framework in
[18]. The framework is built on the resolution method. It calculates the estimates
for the conﬁdences of derived clauses, using both (a) the decreasing conﬁdence of
a conjunction of clauses as performed by the resolution and paramodulation rule,
and (b) the increasing conﬁdence of a disjunction of clauses for cumulating evi-
dence. CONFER handles inconsistent KB-s by requiring the proofs of answers to
contain the clauses stemming from the question posed. It performs searches both
for the question and its negation and returns the resulting conﬁdence calculated
as a diﬀerence of the conﬁdences found by these two searches.
The integrated algorithm is more complex than the one we previously
described. Whenever the algorithms of the previous chapter speak about “prov-
ing”, the system actually performs two independent searches – one for the pos-
itive and one for the negated goal – with the conﬁdences calculated for both
of these. A blocker is considered to be proved in case the resulting conﬁdence
is over a pre-determined conﬁgurable threshold, by default 0.5. Blocker proofs

306
T. Tammet et al.
must also contain the clause built from the blocker. Thus, the whole search tree
for a query consists of two types of interleaved layers: positive/negative conﬁ-
dence searches and blocker checking searches, the latter type potentially making
the tree arbitrarily deep up to the minimal time limit threshold.
4
Implementation and Experiments
The described algorithms are implemented by the ﬁrst author as a software
system GK available at https://logictools.org/gk/. GK is written in C on top of
our implementation of the CONFER framework [18] which is built on top of a
high-performance resolution prover GKC [17] (see https://github.com/tammet/
gkc) for conventional ﬁrst order logic. Thus GK inherits most of the capabilities
and algorithms of GKC.
A tutorial and a set of default logic example problems along with proofs
from GK are also available at http://logictools.org/gk. GK is able to quickly
solve nontrivial problems built by extending classic default logic examples. It
is also able to solve classiﬁcation problems combining exception and cumulative
evidence and problems with dynamic situations using ﬂuents, including planning
problems. We have built a very large integrated knowledge base from the Quasi-
modo [14] and ConceptNet [15] knowledge bases, converting these to default logic
plus conﬁdences. GK is able to solve simple problems using this large knowledge
base along with the Wordnet taxonomy for speciﬁcity: see the referenced web
page for examples.
The following small example illustrates the fundamental diﬀerence of GK
from the existing ASP systems for default logic. The standard penguins and
birds example presented above in the ASP syntax is
bird(b1).
penguin(p1).
bird(X) :- penguin(X).
flies(X) :- bird(X), not -flies(X).
-flies(X) :- penguin(X).
Both GK and the ASP systems clingo 5.4.0, dlv 2.1.1 and s(CASP) 0.21.10.09
give an expected answer to the queries flies(b1) and flies(p1). However,
when we add the rules
bird(father(X)) :- bird(X).
penguin(father(X)) :- penguin(X).
none of these ASP systems terminate for these queries, while GK does solve
the queries as expected. Notably, as pointed out by the author of s(CASP), this
system does terminate for the reformulation of the same problem with the two
replacement rules
flies(X) :- bird(X), not abs(X).
abs(X) :- penguin(X).

GK: Default Logic for Commonsense Reasoning
307
while clingo and dlv do not terminate. When we instead add the facts and rules
father(b1,b2).
father(p1,p2).
...
father(bN-1,bN).
father(pN-1,pN).
ancestor(X,Y):- father(X,Y).
ancestor(X,Y) :- ancestor(X,Z), ancestor(Z,Y).
for a large N, s(CASP) does not terminate and clingo and dlv become slow for
flies(b1): ca 8 s for N = 500 and ca 1 min for N = 1000 on a laptop with a
10-th generation i7 processor. GK solves the same question with N = 1000 under
half a second and with N = 100000 under three seconds: the latter problem size
is clearly out of scope of the capabilities of existing ASP systems.
We have previously shown that the conﬁdence handling mechanisms in CON-
FER may slow down proof search for certain types of problems, but do not have
a strong negative eﬀect on very large commonsense CYC [12] problems in the
TPTP problem collection. Diﬀerently from CONFER, the algorithms for default
logic described above do not substantially modify the resolution method imple-
mentation of pure ﬁrst order logic search, thus the performance of these parts
of GK are mostly the same as of GKC. The ability to give a correct answer to a
query during a given time limit depends on the performance of these components,
and not on the overall recursively branching algorithm.
5
Summary and Future Work
We have presented algorithms and an implementation of an automated reason-
ing system for default logic on the basis of unrestricted ﬁrst order logic and a
resolution method. While there are several systems able to solve default logic or
similar nonmonotonic logic problems, these are built on the basis of answer set
programming and are normally based on grounding. We are not aware of other
full ﬁrst order logic reasoning systems for default logic, and neither of systems
integrating conﬁdences and inconsistency-handling with rules with exceptions.
Future work is planned on three directions: adding features to the solver,
proving several useful properties of the algorithms and incorporating the solver
into a commonsense reasoning system able to handle nontrivial tasks posed in
natural language. The work on incorporating similarity-based reasoning into GK
and building a suitable semantic parser for natural language is currently ongoing.
We are particularly interested in exploring practical ways to integrate GK with
the machine learning techniques for natural language.
References
1. Alviano, M., et al.: The ASP system DLV2. In: Balduccini, M., Janhunen, T. (eds.)
LPNMR 2017. LNCS (LNAI), vol. 10377, pp. 215–221. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-61660-5 19

308
T. Tammet et al.
2. Arias, J., Carro, M., Salazar, E., Marple, K., Gupta, G.: Constraint answer set
programming without grounding. Theor. Pract. Logic Program. 18(3–4), 337–354
(2018)
3. Bachmair, L., Ganzinger, H.: Resolution theorem proving. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, ch. 2, pp. 19–99.
Elsevier, Amsterdam (2001)
4. Brewka, G.: Adding priorities and speciﬁcity to default logic. In: MacNish, C.,
Pearce, D., Pereira, L.M. (eds.) JELIA 1994. LNCS, vol. 838, pp. 247–260. Springer,
Heidelberg (1994). https://doi.org/10.1007/BFb0021977
5. Cholewinski, P., Marek, V.W., Truszczynski, M.: Default reasoning system deres.
KR 96, 518–528 (1996)
6. Green, C.: Theorem proving as a basis for question-answering systems. Mach. Intell.
4, 183–205 (1969)
7. Kaminski, M.: A comparative study of open default theories. Artif. Intell. 77(2),
285–319 (1995)
8. Kaminski, R., Schaub, T., Wanko, P.: A tutorial on hybrid answer set solving with
clingo. In: Ianni, G. (ed.) Reasoning Web 2017. LNCS, vol. 10370, pp. 167–203.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-61033-7 6
9. Kothawade, S., Khandelwal, V., Basu, K., Wang, H., Gupta, G.: Auto-discern:
Autonomous driving using common sense reasoning (2021). arXiv preprint.
arXiv:2110.13606
10. Lifschitz, V.: Answer Set Programming. Springer, Berlin (2019). https://doi.org/
10.1007/978-3-030-24658-7
11. Nute, D.: Defeasible Logic, vol. 3. Oxford University Press, Oxford (1994)
12. Ramachandran, D., Reagan, P., Goolsbey, K.: First-orderized researchcyc: expres-
sivity and eﬃciency in a common-sense ontology. In: AAAI Workshop on Contexts
and Ontologies: Theory, Practice and Applications, pp. 33–40 (2005)
13. Reiter, R.: A logic for default reasoning. Artif. Intell. 13(1–2), 81–132 (1980)
14. Romero, J., Razniewski, S., Pal, K., Pan, J.Z., Sakhadeo, A., Weikum, G.: Com-
monsense properties from query logs and question answering forums. In: Zhu, W.
(eds.) Proceedings of the 28th ACM International Conference on Information and
Knowledge Management, CIKM’19, pp. 1411–1420. ACM (2019)
15. Speer, R., Chin, J., Havasi, C.: ConceptNet 5.5: An open multilingual graph of
general knowledge. In: Singh, S.P., Markovitch, S. (eds.) Proceedings of the 31st
AAAI Conference on Artiﬁcial Intelligence, pp. 4444–4451. AAAI (2017)
16. Sutcliﬀe, G., Yerikalapudi, A., Trac, S.: Multiple answer extraction for question
answering with automated theorem proving systems. In: Lane, H.C., Guesgen,
H.W. (eds.) Proceedings of the 22nd International Florida Artiﬁcial Intelligence
Research Society Conference, FLAIRS’22. AAAI (2009)
17. Tammet, T.: GKC: a reasoning system for large knowledge bases. In: Fontaine, P.
(ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 538–549. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-29436-6 32
18. Tammet, T., Draheim, D., J¨arv, P.: Conﬁdences for Commonsense Reasoning. In:
Platzer, A., Sutcliﬀe, G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 507–524.
Springer, Cham (2021). https://doi.org/10.1007/978-3-030-79876-5 29

GK: Default Logic for Commonsense Reasoning
309
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Hypergraph-Based Inference Rules
for Computing EL+-Ontology
Justiﬁcations
Hui Yang(B)
, Yue Ma, and Nicole Bidoit
LISN, CNRS, Universit´e Paris-Saclay, Gif-sur-Yvette, France
{yang,ma,nicole.bidoit}@lisn.fr
Abstract. To give concise explanations for a conclusion obtained by
reasoning over ontologies, justiﬁcations have been proposed as minimal
subsets of an ontology that entail the given conclusion. Even though
computing one justiﬁcation can be done in polynomial time for tractable
Description Logics such as EL+, computing all justiﬁcations is compli-
cated and often challenging for real-world ontologies. In this paper, based
on a graph representation of EL+-ontologies, we propose a new set of
inference rules (called H-rules) and take advantage of them for provid-
ing a new method of computing all justiﬁcations for a given conclusion.
The advantage of our setting is that most of the time, it reduces the
number of inferences (generated by H-rules) required to derive a given
conclusion. This accelerates the enumeration of justiﬁcations relying on
these inferences. We validate our approach by running real-world ontol-
ogy experiments. Our graph-based approach outperforms PULi [14], the
state-of-the-art algorithm, in most of cases.
1
Introduction
Ontologies provide structured representations of domain knowledge that are suit-
able for AI reasoning. They are used in various domains, including medicine,
biology, and ﬁnance. In the domain of ontologies, one of the interesting topics is
to provide explanations of reasoning conclusions. To this end, justiﬁcations have
been proposed to oﬀer users a brief explanation for a given conclusion. Comput-
ing justiﬁcations has been widely explored for diﬀerent tasks, for instance for
debugging ontologies [1,9,11] and computing ontology modules [6]. Extracting
just one justiﬁcation can be easy for tractable ontologies, such as EL+ [17]. For
instance, we can ﬁnd one justiﬁcation by deleting unnecessary axioms one by
one. However, there may exist more than one justiﬁcation for a given conclu-
sion. Computing all such justiﬁcations is computationally complex and reveals
itself to be a challenging problem [18].
There are mainly two diﬀerent approaches [17] to compute all justiﬁcations
for a given conclusion, the black-box approach and the glass-box approach.
The black-box approach [11] relies only on a reasoner and, as such, can be
This work is funded by the BPI-France (PSPC AIDA: 2019-PSPC-09).
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 310–328, 2022.
https://doi.org/10.1007/978-3-031-10769-6_19

Hypergraph-Based Inference Rules
311
used for ontologies in any existing Description Logics. For example, a simple
(naive) black-box approach would check all the subsets of the ontology using an
existing reasoner and then ﬁlter the subset-minimal ones (i.e., justiﬁcations).
Many advanced and optimized black-box algorithms have been proposed since
2007 [10]. Meanwhile, the glass-box approaches have achieved better perfor-
mances over certain speciﬁc ontology languages (such as EL+-ontology) by going
deep into the reasoning process. Among them, the class of SAT-based methods
[1–3,14,16] performs the best. The main idea developed by SAT-based methods
is to trace, in a ﬁrst step, a complete set of inferences (complete set for short)
that contribute to the derivation of a given conclusion, and then, in a second step,
to use SAT-tools or resolution to extract all justiﬁcations from these inferences.
A detailed example is provided in Sect. 4.1.
In the real world, ontologies are always huge. For instance, the SnomedCT
ontology contains more than 300,000 axioms. Thus, the traced complete set can
be large, which could make it challenging to extract the justiﬁcations over them.
Several techniques could be applied to reduce the size of the traced complete set,
like the locality-based modules [8] and the goal-directed tracing algorithm [12].
One of their shared ideas is to identify, for a given conclusion, a particular part of
the ontology relevant for the extraction of justiﬁcations. For example, the state-
of-the-art algorithm, PULi [14], uses a goal-directed tracing algorithm. However,
even for PULi, a simple ontology O = {Ai ⊑Ai+1 | 1 ≤i ≤n −1} with the
conclusion A0 ⊑An leads to a complete set containing n −1 inferences. This set
can not be reduced further even with the previously mentioned optimizations.
From this observation, we decided to explore a new SAT-based glass-box method
to handle such situations better.
Now, let us look carefully at the ontology O above, and let us regard each
Ai as a graph node NAi. Then we are able to construct, for O, a directed graph
whose edges are of the form NAi →NAi+1. It turns out that all the justiﬁcations
for the conclusion A0 ⊑An are extracted from all the paths from NA0 to NAn,
and here we have only one such path. We can easily extend this idea on EL+-
ontology because most of the EL+-axioms can be interpreted as direct edges
except one case (i.e., A ≡B1⊓· · ·⊓Bn), for which we need a hyperedge (for more
details see Deﬁnition 3). However, for more expressive ontologies, this translation
becomes more complicated. For example, it is hard to map ALC-axioms to edges
as those axioms may contain negation or disjunction of concepts.
This example inspired us to explore a hypergraph representation of the ontol-
ogy and reformulate inferences and justiﬁcations. Roughly, our inferences are
built from elementary paths of the hypergraph and lead to particular paths
called H-paths. Then, computing all the justiﬁcations for a given conclusion
is made using such H-paths. For the previous ontology O and the conclusion
A0 ⊑An, our complete set is reduced to only two inferences (no matter the
value of n) corresponding to the unique path from NA0 to NAn. The source
of improvement provided by our method is twofold. On the one hand, it comes
from the fact that elementary paths are pre-computed while extracting the infer-
ences and that existing algorithms like depth-ﬁrst search can eﬃciently compute
such paths. On the other hand, yet as a consequence, decreasing the size of the
complete sets of inferences leads to smaller inputs for the SAT-based algorithm

312
H. Yang et al.
extracting justiﬁcations from the complete set (recall here that our method is a
SAT-based glass-box method).
The paper is organized as follows. Section 2 introduces preliminary deﬁni-
tions and notions. In Sect. 3, we associate a hypergraph representation to EL+-
ontology and introduce a new set of inference rules, called H-rules, that generate
our inferences. In Sect. 4, we develop the algorithm minH, which compute justiﬁ-
cations based on our inferences. Section 5 shows experimental results and Sect. 6
summarizes our work.
2
Preliminaries
2.1
EL+-Ontology
Given sets of atomic concepts NC
= {A, B, · · · } and atomic roles NR =
{r, s, t, · · · }, the set of EL+concepts C and axioms α are built by the follow-
ing grammar rules:
C ::= ⊤| A | C ⊓C | ∃r.C,
a ::= C ⊑C | C ≡C | r ⊑s | r1 ◦· · · ◦rn ⊑s.
A EL+-ontology O is a ﬁnite set of EL+-axioms. An interpretation I =
(△I, ·I) of O consists of a non-empty set △I and a mapping from atomic
concepts A ∈NC to a subset AI ⊆△I and from roles r ∈NR to a sub-
set rI ⊆△I × △I. For a concept C built from the grammar rules, we deﬁne
CI inductively by: (⊤)I = △I, (C ⊓D)I = CI ∩DI, (∃r.C)I = {a ∈△I |
∃b, (a, b) ∈rI, b ∈CI}, (r◦s)I = {(a, b) ∈△I ×△I | ∃c, (a, c) ∈rI, (c, b) ∈sI}.
An interpretation is a model of O if it is compatible with all axioms in
O, i.e., for all C ⊑D, C ≡D, r ⊑s, r1 ◦· · · ◦rn ⊑s ∈O, we have
CI ⊆DI, CI = DI, rI ⊆sI, (r1 ◦· · · ◦rn)I ⊆sI, respectively. We say O |= a
where α is an axiom iﬀeach model of O is compatible with α. A concept A is
subsumed by B w.r.t. O if O |= A ⊑B.
Next, we use A, B, · · · , G (possibly with subscripts) to denote atomic con-
cepts and we use X, Y, Z (possibly with subscripts) to denote atomic concepts
A, · · · , G, or complex concepts ∃r.A, · · · , ∃r.G.
We assume that ontologies are normalized. A EL+-ontology O is normalized
if all its axioms are of the form A ≡B1 ⊓· · · ⊓Bm, A ⊑B1 ⊓· · · ⊓Bm, A ≡
∃r.B, A ⊑∃r.B, r ⊑s, or r ◦s ⊑t, where A, B, Bi ∈NC, and r, s, t ∈NR. Every
EL+-ontology can be normalised in polynomial time by introducing new atomic
concepts and atomic roles.
Example 1. The following set of axioms is a EL+-ontology:
O = { a1:A ⊑D, a2:D ⊑∃r.E, a3:E ⊑F, a4:B ≡∃t.F, a5:r ⊑t, a6:G ≡
C ⊓B , a7:C ⊑A}.
It is clear that O |= A ⊑∃r.E as for all models I, we have AI ⊆DI by the
axiom a1 and DI ⊆(∃r.E)I by a2.

Hypergraph-Based Inference Rules
313
Table 1. Inference rules over EL+-ontology.
R1 : A⊑A1, · · ·, A⊑An,
A1⊓A2⊓· · · ⊓An⊑B
A⊑B
R2 : A⊑A1,
A1⊑∃r.B
A⊑∃r.B
R3 : A ⊑∃r.B1,
B1 ⊑B2,
∃r.B2⊑B
A ⊑B
R4 : A0⊑∃r1.A1,
· · ·, An−1⊑∃rn.An,
r1◦· · · ◦rn⊑r
A0⊑∃r.An
2.2
Inference, Support and Justiﬁcation
Given a EL+-ontology O, a major reasoning task over O is classiﬁcation, which
aims at ﬁnding all subsumptions O |= A ⊑B for atomic concepts A, B occurring
in O. Generally, it can be solved by applying inferences recursively over O [5].
An inference ρ is a pair ⟨ρpre, ρcon⟩whose premise set ρpre consists of EL+-
axioms and conclusion ρcon is a single EL+-axiom. As usual, a sequence of
inferences ρ1, · · · , ρn is a derivation of an axiom α from O if ρn
con = α and for
any β ∈ρi
pre, 1 ≤i ≤n, we have β ∈O or β = ρj
con for some j < i.
As usual, inference rules are used to generate inferences. For instance,
Table 1 [1,5] shows a set of inference rules for EL+-ontologies. Next, we use
O ⊢A ⊑B to denote that A ⊑B is derivable from O using inferences generated
by the rules in Table 1. The set of inference rules in Table 1 is sound and complete
for classiﬁcation [5], i.e., O |= A ⊑B iﬀO ⊢A ⊑B for any A, B ∈NC.
A support of A ⊑B over O is a sub-ontology O′ ⊆O such that O′ |= A ⊑B.
The justiﬁcations for A ⊑B are subset-minimal supports of A ⊑B. We denote
the collection of all justiﬁcations for A ⊑B w.r.t. O by JO(A ⊑B).
We say S is a complete set (of inferences) for A ⊑B if for any justiﬁcations
O′ of A ⊑B, we can derive A ⊑B from O′ using only the inferences in S.
Example 2 (Example 1 cont’d). Before applying inference rules, axioms in
O are preprocessed in order to be compatible with Table 1. For example, a4 is
replaced by B ⊑∃t.F and ∃t.F ⊑B. Then, according to the inference rules of
Table 1, we may produce the following inferences: ρ = ⟨{A ⊑D, D ⊑∃r.E}, A ⊑
∃r.E⟩, ρ′ = ⟨{A ⊑∃r.E, r ⊑t}, A ⊑∃t.E⟩and ρ′′ = ⟨{A ⊑∃t.E, E ⊑F, ∃t.F ⊑
B}, A ⊑B⟩generated by rule R2, R4 and R3 respectively. Then O ⊢A ⊑B
since A ⊑B is derivable from O by the sequence ρ, ρ′, ρ′′.
Notice that O′ = {a1, a2, a3, a4, a5} is a support for A ⊑B, and thus, any
superset O′′ of O′ is a support of A ⊑B. O′ is also one of the justiﬁcations for
A ⊑B as for any O′′′ ⊂O′, we have O′′′ ̸|= A ⊑B. Moreover, here the three
inferences ρ, ρ′, ρ′′ provide a complete set for A ⊑B.
3
Hypergraph-Based Inference Rules
3.1
H-Inferences
In general, a (directed) hypergraph G = (V, E) is deﬁned by a set of nodes V and
a set of hyperedges E [4,7]. A hyperedge is of the form e = (S1, S2), S1, S2 ⊆V.
In this paper, a hypergraph is associated to an ontology as follows:

314
H. Yang et al.
Deﬁnition 3. For a given EL+-ontology O, the associated hypergraph is GO =
(VO, EO) where (i) the set of nodes VO = {NA, Nr, N∃r.A | A ∈NC, r ∈NR} and
(ii) the set of edges EO is deﬁned by f(O) where f is the multi-valued mapping
shown in Fig. 1. Given a hyperedge e of EO, the inverse image of e, f −1(e), is
deﬁned in the obvious manner. For a set E of hyperedges, f −1(E) = ∪e∈Ef −1(e).
Fig. 1. Deﬁnition of f (left) and graphical illustrations of f(α) (right)
Notice that, the hyperedges associated with A ≡B1 ⊓· · · ⊓Bm are (i) the
hyperedge ({NB1, · · · , NBm}, {NA}) and (2) of course, the edges corresponding
to A ⊑B1 ⊓· · · ⊓Bm.
Example 4 (Example 1 cont’d).
The hypergraph GO for O is shown in
Fig. 2, where e0 = ({NC}, {NA}), e1 = ({NA}, {ND}), e2 = ({ND}, {N∃r.E}),
etc. Also, f −1(e0) = C ⊑A, f −1(e1) = A ⊑D, and f −1(e2) = D ⊑∃r.E, etc.
Fig. 2. The hypergraph associated with the ontology O.
As for graphs, a path (next called regular path) from nodes N1 to N2 in a
hypergraph is a sequence of edges:
e0 = (S0
1, S0
2), e1 = (S1
1, S1
2), · · · , en = (Sn
1 , Sn
2 )
(1)
where N1 ∈S0
1, N2 ∈Sn
2 and Si−1
2
= Si
1, 1 ≤i ≤n. Next, the existence of a
regular path from NX to NY in a hypergraph GO is denoted NX ⇝NY . Now,
we introduce hypergraph-based inferences which are based on the existence of
regular paths as follows:

Hypergraph-Based Inference Rules
315
Table 2. H-rules over GO = (VO, EO).
H0 : NX⇝NY
NX
h
⇝NY
H2 : NX
h
⇝N∃r.B1,
NB1
h
⇝NB2,
N∃r.B2⇝NY
NX
h
⇝NY
H1 : NX
h
⇝NB1, · · · , NX
h
⇝NBm,
NA⇝NY ,
e
NX
h
⇝NY
: e=({NB1, · · · , NBm}, {NA})∈EO
H3 : NX
h
⇝N∃r.A1,
NA1
h
⇝N∃s.A2,
N∃t.A2⇝NY ,
e
NX
h
⇝NY
: e=({Nr, Ns}, {Ns, Nt})∈EO
Deﬁnition 5. Given a hypergraph GO, Table 2 gives a set of inference rules
called H-rules. Inferences based on H-rules are called H-inferences. Next, we
denote by O ⊢h NX
h⇝NY (or simply NX
h⇝NY ) the fact that NX
h⇝NY can
be derived from GO using the H-inferences.
Example 6 (Example 4 cont’d). As shown in Fig. 2, we have NA ⇝N∃r.E,
NE ⇝NF , N∃r.F ⇝NB from the existence of regular paths. Then we can
derive NA
h⇝NB from GO by the H-rules H0, H0 and H2 which generate the H-
inferences ρ1, ρ2, ρ3, where ρ1 = ⟨{NA ⇝N∃r.E}, NA
h⇝N∃r.E⟩, ρ2 = ⟨{NE ⇝
NF }, NE
h⇝NF ⟩and ρ3 = ⟨{NA
h⇝N∃r.E, NE
h⇝NF , N∃r.F ⇝NB}, NA
h⇝
NB⟩, respectively.
Note that the ﬁrst rule H0, the initialization rule, makes regular paths the
elementary components of H-rules. Moreover, Proposition 7 formally states that,
in our H-inference system, we do not need to add the transitive inference rule:
NX
h⇝NZ, NZ
h⇝NY
NX
h⇝NY
.
Proposition 7. If O ⊢h NX
h⇝NZ and O ⊢h NZ
h⇝NY then O ⊢h NX
h⇝NY .
3.2
Completeness and Soundness of H-Inferences
The following result is the main result of this section. It states the equivalence
of NX
h⇝NY derivation (by Table 2) and ontology entailment for X ⊑Y , and
thus states that our H-rules are sound and complete for EL+-ontology.
Theorem 8. If O is an EL+-ontology, then O |= X ⊑Y iﬀO ⊢h NX
h⇝NY ,
where X, Y are concepts of either form A or ∃r.B.
Proof. “⇐” is obvious by induction over Table 2 and the fact that NX ⇝NY
implies O |= X ⊑Y , so we only need to prove the direction “⇒”.
Assume that O |= X ⊑Y . We consider two cases:

316
H. Yang et al.
Case 1. We assume O ⊢X ⊑Y 1. Let d(X, Y ) be the length of one shortest
derivation of X ⊑Y from O using Table 1. We prove “⇒” by induction on
d(X, Y ).
– Assume d(X, Y ) = 0. In this case O must contain axioms of the form X ≡
Y ⊓· · · or X ⊑Y ⊓· · · . Clearly we have NX ⇝NY thus O ⊢h NX
h⇝NY .
– Assuming “⇒” holds when d(X, Y ) < k, let us prove “⇒” holds for
d(X, Y ) = k. Suppose ρlast is the last inference in one shortest derivation of
X ⊑Y using Table 1. Two cases arise:
1. Assume ρlast is generated by R1(n > 1), R3 or R4(n = 2). For example,
assume ρlast = ⟨{X ⊑∃r.B1, B1 ⊑B2, ∃r.B2 ⊑Y }, X ⊑Y ⟩comes from
R3. We have d(X, ∃r.B1), d(B1, B2), d(∃r.B2, Y ) < k because their cor-
responding subsumptions can be derived without ρlast. By the assump-
tion O ⊢h NX
h⇝N∃r.B1, NB1
h⇝NB2, N∃r.B2
h⇝NY . Then we have
O ⊢h NX
h⇝N∃r.B2 by ﬁrst deriving NX
h⇝N∃r.B1, NB1
h⇝NB2, and
then applying H-inference:
ρnew = ⟨{NX
h⇝N∃r.B1, NB1
h⇝NB2, N∃r.B2 ⇝N∃r.B2}, NX
h⇝N∃r.B2⟩.
Then O
⊢h
NX
h⇝
NY
by Proposition 7 since O
⊢h
NX
h⇝
N∃r.B2, N∃r.B2
h⇝NB. The argument also holds for R1(n > 1)(or
R4(n = 2)) by applying H1 (or H3) instead of H2.
2. Assume ρlast is generated by R1(n = 1), R2 or R4(n = 1). Then, in each
case, we have ρlast has the form ⟨{X ⊑Z, Z ⊑Y }, X ⊑Y ⟩. As in
case 1, we have d(X, Z), d(Z, Y ) < k. By the assumption, O ⊢h NX
h⇝
NZ, NZ
h⇝NY , then O ⊢h NX
h⇝NY by Proposition 7.
Case 2. If O ⊢X ⊑Y does not hold, then X or Y is not atomic. In this case,
we introduce new axioms A ≡X, B ≡Y with new atomic concepts A, B and
denote the extended ontology by O′. Clearly, O′ |= A ⊑B and thus O′ ⊢A ⊑B
since Table 1 is sound and complete. Therefore, we have O′ ⊢h NA
h⇝NB by the
same arguments as above. Now, notice that GO′ is obtained from GO by adding
4 edges: ({NA}, {NX}), ({NX}, {NA}), ({NB}, {NY }) and ({NY }, {NB}), thus
we have O′ ⊢h NA
h⇝NB iﬀO ⊢h NX
h⇝NY .
3.3
Extracting Justiﬁcations from GO
Now, we formally deﬁne H-paths as a hypergraph representation of classical
derivations based on H-rules. The reader should pay attention to the fact that
H-paths are not classical hyperpaths [7]. Next, for the sake of homogeneity, we
consider a regular path from NX to NY as the set of its edges and denote it as
PX,Y .
1 The reader should recall that the equivalence (O |= X ⊑Y iﬀO ⊢X ⊑Y ) only
holds when X and Y are atomic concepts wrt. the inference system presented in
Table 1.

Hypergraph-Based Inference Rules
317
Deﬁnition 9 (H-paths). In the hypergraph GO, an H-path HX,Y from NX to
NY is a set of edges recursively generated by the following composition rules:
0. A regular path PX,Y is an H-path from NX to NY ;
1. If e = ({NB1, · · · , NBm}, {NA}) ∈VO, if HX,Bi are H-paths for i = 1..m,
and if PA,Y is a regular path, then HX,B1 ∪· · · ∪HX,Bm ∪PA,Y ∪{e} is an
H-path from NX to NY ;
2. If HX,∃r.B1, HB1,B2 are H-paths and P∃r.B2,Y is a regular path, then HX,∃r.B1∪
HB1,B2 ∪P∃r.B2,Y is an H-path from NX to NY ;
3. If e = ({Nr, Ns}, {Ns, Nt}) ∈VO, if HX,∃r.A1, HA1,∃s.A2 are H-paths and if
P∃t.A2,B is a regular path, then HX,∃r.A1 ∪HA1,∃s.A2 ∪P∃t.A2,B ∪{e} is an
H-path from NX to NY .
Fig. 3. Structure of H-paths from NX to NY
Figure 3 gives an illustration of H-paths: the blue arrows ⇝correspond to
regular paths, and the red ones
h⇝to H-paths. It is straightforward to compare
composition rules building H-paths with H-rules building derivations in Table 2.
One may also consider H-paths as deviation-trees with leaves corresponding to
the edges in GO. However, our approach provides a more direct characterization
of justiﬁcations as shown in Theorem 10.
We say that an H-path HX,Y is minimal if there is no H-path H′
X,Y such
that H′
X,Y ⊂HX,Y .
Now, we are ready to explain how H-paths and justiﬁcations are related. We
can compute justiﬁcations from minimal H-paths as stated below:
Theorem 10. Given X, Y of either form A or ∃r.B. Let
S = {f −1(HX,Y ) | HX,Y is a minimal H-path from NX to NY }.
Then JO(X ⊑Y ) = {s ∈S | s′ ̸⊂s, ∀s′ ∈S}. That is, all justiﬁcations for
X ⊑Y are the minimal subsets in S.

318
H. Yang et al.
Proof. For any justiﬁcation O′ of X ⊑Y , there exists a minimal H-path HX,Y
such that O′ = f −1(HX,Y ). The reason is that, since O′ |= X ⊑Y , there
exists an H-path HX,Y from NX to NY on GO′ by Theorem 8. Without loss of
generality, we can assume HX,Y is minimal on GO′, then it is also minimal on
GO since GO′ is a sub-graph of GO. We have O′ = f −1(HX,Y ) because otherwise
there exists O′′ ⊊O′ such that O′′ = f −1(HX,Y ), and thus O′′ |= X ⊑Y by
Theorem 8 again. Therefore, O′ is not a justiﬁcation. Contradiction.
Now, we know S
contains all justiﬁcations for X
⊑
Y . Moreover,
f −1(HX,Y ) |= X ⊑Y for any H-path HX,Y . Therefore, we have JO(X ⊑Y ) =
{s ∈S | s′ ̸⊂s, ∀s′ ∈S} by the deﬁnition of justiﬁcations.
Example 11. (Example 4 cont’d). The regular paths from NA to N∃r.E and
from NE to NF produce two H-paths HA,∃rE = {e1, e2, e3} and HE,F = {e4}.
Then, applying the third composition rule with HA,∃rE, HE,F and P∃r.F,B =
{e6}, we get HA,B = {e1, e2, e3, e4, e6}, which is the unique H-path from NA to
NB. Thus, by Theorem 10, we have {α1, α2, α3, α4, α5}, the only justiﬁcation for
A ⊑B.
4
Implementation: Computing Justiﬁcations
4.1
SAT-Based Method
In this section, we describe brieﬂy how PULi [14], the state-of-the-art glass-
box algorithm, proceeds. Given an ontology O, computing JO(X ⊑Y ) is done
through 2 steps: (1) tracing a complete set for X ⊑Y , (2) using resolution to
extract the justiﬁcations from the complete set. The following example illustrates
both steps:
Example 12 (Example 1 cont’d). Let us compute JO(G ⊑D) using PULi’s
method.
1. Using the goal-directed tracing algorithm in [12], the ﬁrst step produces a
complete set of inferences2 {ρ1, ρ2} for G ⊑D, where ρ1 = ⟨{G ⊑C, C ⊑
A}, G ⊑A⟩, ρ2 = ⟨{G ⊑A, A ⊑D}, G ⊑D⟩.
2. This step is again composed of two parts:
(a) The ﬁrst part proceeds to the translation of the inferences into clauses.
Let us denote p1:G ⊑C, p2:C ⊑A, p3:A ⊑D, p4:G ⊑A, p5:G ⊑D.
Here the literals p1, p2, p3 (with a bar) are called answer literals as they
correspond to the axioms a6, a7, a1 in O. Thus, we obtain C = {¬p1 ∨
¬p2 ∨p4, ¬p4 ∨¬p3 ∨p5} by rewriting the inferences ρ1, ρ2 as clauses.
(b) Secondly, a new clause ¬p5 is added to C, where p5 corresponds to the
conclusion G ⊑D, and resolution is applied over C. The set of all justi-
ﬁcations JO(G ⊑D) is obtained by considering (i) the clauses formed of
2 For the sake of simplicity, we use the inference rules in Table 1 although PULi uses
a slightly diﬀerent set of inference rules [13].

Hypergraph-Based Inference Rules
319
Algorithm 1: minH
input : X⊑Y
output: J: JO(X⊑Y ).
1 J ←∅;
2 U ←CompleteH(NX
h⇝NY );
3 min hpaths ←resolution(clauses(U));
4 for h ∈min hpaths do
5
if f −1(h’) ̸⊂f −1(h) for any h’ ∈min hpaths then
6
J.add(f −1(h))
7
end
8 end
answer literals only and (ii) among them keeping the minimal ones3. In
this example, after the resolution phase, the only clause that consists of
merely answer literals is ¬p1∨¬p2∨¬p3. Thus, the set of all justiﬁcations
is JO(G ⊑D) = {{a1, a6, a7}}.
Our method for computing justiﬁcations follows the same steps as PULi
although here the major diﬀerence is that the ﬁrst step computes a complete set
of H-inferences instead of a complete set of inferences wrt. Table 1.
4.2
Computing Justiﬁcation by Minimal H-Paths
In this section, given an ontology O and its associated hypergraph GO, we present
minH (Algorithm 1) that computes all justiﬁcations for X0 ⊑Y0 using the min-
imal H-paths from NX0 to NY0 over GO. The algorithm minH proceeds in two
steps described below.
Step 1. First, at Line 2, minH computes a complete set of inferences U for
NX0
h⇝NY0 using CompleteH (See Algorithm 2). Here, U is complete in the
sense that for any H-path HX,Y , we can derive NX
h⇝NY using inferences in U
from the edge set HX,Y . CompleteH computes U as follows:
– Line 3–12 of Algorithm 2: The recursive application of trace one turn
(See Algorithm 3) outputs the set of all H-inferences whose conclusion is the
given input NX1
h⇝NY1;
– Line 13–17 of Algorithm 2: Let path be the depth-ﬁrst search algorithm
that computes all regular paths from NX to NY in GO with input (NX, NY ).
Intuitively, the purpose is to shift inferences from regular paths to edges.
Step 2. Then Algorithm minH computes all justiﬁcations for X0 ⊑Y0 as follows:
3 Here a clause c is smaller than c1 if all the literals of c are in c1.

320
H. Yang et al.
Algorithm 2: CompleteH
input : NX
h⇝NY
output: U: a complete set of inferences for NX
h⇝NY .
1 U, history, Q ←∅;
// Q is a queue
2 Q.add(NX
h⇝NY );
3 while Q ̸= ∅do
4
NX1
h⇝NY1 ←Q.takeNext();
5
history.add(NX1
h⇝NY1);
6
U ←U  trace one turn(NX1
h⇝NY1);
7
for NX2
h⇝NY2 appearing in trace one turn(NX1
h⇝NY1) do
8
if NX2
h⇝NY2 ̸∈history and NX2
h⇝NY2 ̸∈Q then
9
Q.add(NX2
h⇝NY2)
10
end
11
end
12 end
13 for NX2⇝NY2 appearing in U do
14
for p={e1, e2, · · · , en} ∈path(NX2, NY2) do
15
U.add(⟨{e1, e2, · · · , en}, NX2⇝NY2⟩);
16
end
17 end
– Line 3 of Algorithm 1: It computes all minimal H-paths from NX0 to NY0
using resolution, which is developed by PULi4, over the clauses generated
from U as illustrated in Sect. 4.1. Here, a literal p is associated with each
edge e, each NX
h⇝NY , and each NX ⇝NY in U. The answer literals are
those associated with edges.
– Line 4–8 of Algorithm 1: It computes justiﬁcations by mapping back all
the minimal H-paths and select the subset-minimal sets as stated in Theorem
10.
Example 13 (Example 4 cont’d). Assume X0 = G and Y0 = D are the input
of minH. Then at line 2 of minH, we have U = {ρ1, ρ2}, where ρ1 = ⟨{NG ⇝
ND}, NG
h⇝ND⟩is H-inference obtained by CompleteH (line 3–12) and ρ2 =
⟨{e0, e1, e8}, NG ⇝ND⟩is produced from regular paths obtained by CompleteH
(line 13–17). Let us denote p0:e0, p1:e1, p2:e8 as answer literals and p3:NG ⇝
ND, p4:NG
h⇝ND. Then clauses(U) = {¬p3 ∨p4, ¬p0 ∨¬p1 ∨¬p2 ∨p3}.
By resolution over clauses(U), we obtain min hpaths = {{e0, e1, e8}} at line
3 of minH. Then the output of minH is J = {{a1, a6, a7}}, which is the set of all
justiﬁcations for G ⊑D.
4 Available at https://github.com/liveontologies/pinpointing-experiments.

Hypergraph-Based Inference Rules
321
Algorithm 3: trace one turn
input : NX
h⇝NY
output: the set result of all H-inferences whose conclusion is NX
h⇝NY .
1 result ←∅;
2 P1(X, Y ) ←{({NB1, · · ·, NBm}, {NA}) ∈EO | O|=X⊑A⊑Y };
3 for ({NB1, · · ·, NBm}, {NA}) ∈P1(X, Y ) do
4
if path(NA, NY )̸= ∅or Y =A then
5
result.add(⟨{NX
h⇝NB1, · · · , NX
h⇝NBm, NA⇝NY }, NX
h⇝NY ⟩) ;
6
end
7 end
8 P2(X, Y ) ←{(r, B1, B2) | O|=X⊑∃r.B1, B1⊑B2, ∃r.B2⊑Y };
9 for (r, B1, B2) ∈P2(X, Y ) do
10
if path(N∃r.B2, NY )̸= ∅or Y =∃r.B2 then
11
result.add(⟨{NX
h⇝N∃r.B1, NB1
h⇝NB2, N∃r.B2⇝NY }, NX
h⇝NY ⟩);
12
end
13 end
14 P3(X, Y ) ←{(r, s, t, A1, A2) | r◦s⊑t∈O, O|=X⊑∃r.A1, A1⊑∃s.A2, ∃t.A2⊑Y };
for (r, s, t, A1, A2) ∈P3(X, Y ) do
15
if path(N∃t.A2, NY )̸= ∅or Y =∃t.A2 then
16
result.add(⟨{NX
h⇝N∃r.A1, NA1
h⇝N∃s.A2, N∃t.A2⇝NY , ({Nr, Nt}, {Ns, Nt})},
{Ns, Nt})}, NX
h⇝NY ⟩);
17
end
18 end
4.3
Optimization
Below we present two optimizations that have been implemented in order to
accelerate the computation of all justiﬁcations.
1. In Algorithm 3, for the H-inference added at Line 5, we require that there
exists at least one regular path from NA to NY that does not contain an edge
ei = ({NA}, {NBi}) for some 1 ≤i ≤m. Otherwise, as shown in Fig. 4, H-
paths corresponding to this H-inference are not minimal, as they all contain
one H-path from NX to NY of the form HX,Bi ∪(PA,Y −{ei}). In the same
spirit, we require that the H-path from NX to NBi does not pass by NA.
2. If we have an H-path HA,B = HA,∃r.B1 ∪HB1,B2 ∪P∃r.B2,B where
HA,∃r.B1 = HA,∃r.C ∪HC,B1.
(2)
then HC,B2 = HC,B1∪HB1,B2 is also an H-path and HA,B = HA,∃r.C∪HC,B2∪
P∃r.B2,B. The two diﬀerent ways to decompose HA,B above are already con-
sidered in Line 8 when executing Algorithm 3 with the input NA
h⇝NB. It
means that the decomposition (2) is redundant. We can avoid such redun-
dancy by requiring ∃r.B2 ̸= Y at Line 11.

322
H. Yang et al.
Fig. 4. Illustration of Optimization 1
5
Experiments
To evaluate and validate our approach, we compare minH5 with PULi [14], the
state-of-the-art algorithm for computing justiﬁcations at this moment. Both
methods compute all justiﬁcations based on resolution but with diﬀerent infer-
ence rules generated in diﬀerent ways. PULi uses a complete set (next denoted
by elk) generated by the ELK reasoner [13], which uses inference rules slightly
diﬀerent from those in Table 1. Our method uses the complete set U generated
by Step 1 of minH, described in Sect. 4.2. To analyze the performance of our
setting, we make the following two measures: (1) we compare the size of elk with
that of U, (2) we compare the time cost of PULi with that of minH. All the
experiments were conducted on a machine with an INTEL Xeon 2.6 GHz and
128 GiB of RAM.
The experiments were processed with four diﬀerent ontologies6: go-plus,
galen7, SnomedCT (version Jan. 2015 and Jan. 2021). All the non-EL+ axioms
are deleted. Here, go-plus, galen7 are the same ontologies used in [14]. We denote
the four ontologies above by go-plus, galen7, snt2015 and snt2021. The number of
axioms, concepts, relations, and queries for each ontology are shown in Table 3.
Next a query refers to a direct subsumption7 A ⊑B. In our experiments,
for the four ontologies, the set of all justiﬁcations JO(A ⊑B) is computed for
each query A ⊑B. A query A ⊑B is called trivial iﬀall minimal H-paths from
NA to NB are regular paths, otherwise, the query is non-trivial.
Comparing Complete Sets: U vs. elk. We summarize our results in Table 4
and Fig. 5. Table 4 shows that on all four ontologies, U is much smaller than elk
on average. Especially on galen7, the diﬀerence between elk and U is even up
to 50 times. The gap is even more signiﬁcant for the median value since a large
part of the queries is trivial. However, the gap is much smaller for the maximal
number. On snt2021, the largest U in size is three times larger than that of elk.
5 A prototype is available at https://gitlab.lisn.upsaclay.fr/yang/minH.
6 Available at https://osf.io/9sj8n/, https://www.snomed.org/.
7 i.e., O |= A ⊑B and there is no other atomic concept A′ such that O |= A ⊑
A′, A′ ⊑B. Direct subsumptions can be computed by a reasoner supporting ontology
classiﬁcation.

Hypergraph-Based Inference Rules
323
Table 3. Summary of sizes of the input ontologies.
go-plus galen7 snt2015 snt2021
#axioms
105557 44475
311466
362638
#concepts 57173
28482
311480
361226
#roles
157
964
58
132
#queries
90443
91332
461854
566797
Table 4. Summary of size of elk, U.
go-plus galen7
snt2015 snt2021
elk average
166.9
3602.0
114.7
67.3
median
43.0
3648.0
10.0
31.0
max
7919.0
81501.0 2357
2226
U
average
34.2
74.6
29.4
19.4
median
4.0
5.0
1.0
3.0
max
7772
24103
2002
6452
#non-trivial query 50272
62470
195082
304321
In Fig. 5, for a given query, if the complete set elk contains fewer inference
rules than U, the corresponding blue point is below the red line. The percentage
of such cases are: 0.34% for go-plus, 0.066% for galen7, 0.79% for snt2015, and
1.01% for snt2021. It means that for most of the queries, the corresponding U is
smaller than elk.
As shown in Table 4 and in Fig. 5, sometimes minH generates bigger complete
set U than PULi. It may happen when, for example, there might be exponen-
tially many diﬀerent regular paths occurring in the computation process of minH.
Therefore, minH could produce a huge complete set. Also, U can be bigger than
elk when all the regular paths involved are simple. For example, if all regular
paths contain only one edge, then the complete set U includes many clauses
of the form ¬pe ∨pNA⇝NB, which happens because H-rules use regular paths.
Indeed, the clause ¬pe ∨pNA⇝NB is redundant since we can omit this clause by
replacing pNA⇝NB by pe. For elk, this does not happen.
Comparing Time Cost: minH vs. PULi. In the following, we only compare
the time cost on non-trivial queries. For trivial queries, all H-path are regular
paths. Thus all the justiﬁcations have already been enumerated by path in minH.
It is also easy to compute all the justiﬁcations for trivial queries for PULi.
We set a limit of 60 s for each query. The timed-out queries contribute of 60 s
to the total time cost. To compare minH with PULi, we test all three diﬀerent
strategies, threshold, top down, bottom up of the resolution algorithm proposed
in [14]. We summarize in Table 5 the total time cost (top) and the timed-out
queries (bottom). Figure 6 gives the comparisons over queries that are successful
for both minH and PULi.

324
H. Yang et al.
(a) go-plus
(b) galen7
(c) snt2015
(d) snt2021
Fig. 5. Each blue point has coordinate (log(#|U|), log(#|elk|)), where U, elk are gen-
erated from a non-trivial query, the red line is x = y. (Color ﬁgure online)
As shown in Table 5, when using the threshold strategy, minH is more time
consuming in total (+5%) on snt2021, and minH has more timed-out queries
than PULi on snt2015 and snt2021. This is in part due to the fact that U is
larger than elk for relatively many queries on snt2015 and snt2021 as shown in
Fig. 5. For the remaining 11 cases, minH performs better than PULi in terms of
total time cost and the number of timed-out queries. Especially on galen7, the
gap between the two methods is even up to ten times for the total time cost.
We can see from Table 5 that the threshold strategy performs the best for PULi
on all four ontologies. This strategy is also the best strategy for minH except for
galen7, for which the bottom up strategy is the best with minH.
For each strategy detailed in Fig. 6, the black curve (the ordered time costs
of minH on successful queries) is always below the red curve (the ordered time
costs of PULi on successful queries) for all the ontologies. This suggests that
minH spends less time over successful queries. Also, most of the green points are
below the red lines, which suggests that minH performs better than PULi most of
the time for a given query. In some cases, we can see that PULi is more eﬃcient
than minH. One of the reasons might be as follows. Note that when computing
justiﬁcations by resolution, we have to compare two diﬀerent clauses and delete
the redundant one (i.e., the non-minimal one). When regular paths are big, minH
might be time consuming because of these comparisons.

Hypergraph-Based Inference Rules
325
(a) go-plus
(b) galen7
(c) snt2015
(d) snt2021
Fig. 6. For each line, the left, middle and right charts correspond to threshold, top
down, bottom up strategies respectively. The y-axis is the log value of time(s). The red
(resp. black) curve presents the ascending ordered (log value of) time cost of PULi (resp.
minH). For a green point (x, y), ey is the time cost of minH for the query corresponding
to the red line point (x, y′). (Color ﬁgure online)

326
H. Yang et al.
Table 5. Total time cost and number of timed-out queries.
threshold
top down
bottom up
total times(s)
(PULi/minH)
go-plus
8482.7/7350.3
16352.3/8935.6
73629.1/17950.9
galen7
10796.2/3681.4 43372.9/10607.9 36300.9/3156.3
snt2015 1956.8/973.5
13650.7/1107.6
15058.3/11392.2
snt2021 2116.1/2222.6
11573.9/2361.6
19402.1/17154.9
timed-out queries
(PULi/minH/both)
go-plus
116/103 /93
202/117/114
935/223/223
galen7
48/43/43
370/123/120
228/38/38
snt2015 0/3/0
49/3/3
96/88/83
snt2021 2/8/1
39/9/9
144/133/128
6
Conclusion
In this paper, we introduce and investigate a new set of sound and complete
inference rules based on a hypergraph representation of ontologies. We design the
algorithm minH that leverages these inference rules to compute all justiﬁcations
for a given conclusion. The key of the performance of our method is that regular
paths are used as elementary components of H-paths and this leads to reducing
the size of complete sets because (1) rules are more compact than standard
ones, (2) redundant inferences are captured and eliminated by regular paths
(see Sect. 4.3). The eﬃciency of the algorithm minH has been validated by our
experiments showing that it outperforms PULi in most of the cases.
There are still many possible extensions and applications of the hypergraph
approach. For instance, to get even more compact inference rules, we could
extend the notion of regular path to a more general one that will encapsulate the
inference rule H2 in the same way as regular paths are encapsulated in H-rules.
Moreover, we will try to apply our approach for other tasks like classiﬁcation
and to compute logical diﬀerences [15].
References
1. Arif, M.F., Menc´ıa, C., Ignatiev, A., Manthey, N., Pe˜naloza, R., Marques-Silva, J.:
BEACON: an eﬃcient SAT-based tool for debugging EL+ ontologies. In: Creignou,
N., Le Berre, D. (eds.) SAT 2016. LNCS, vol. 9710, pp. 521–530. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-40970-2 32
2. Arif, M.F., Menc´ıa, C., Marques-Silva, J.: Eﬃcient axiom pinpointing with
EL2MCS. In: H¨olldobler, S., Kr¨otzsch, M., Pe˜naloza, R., Rudolph, S. (eds.) KI
2015. LNCS (LNAI), vol. 9324, pp. 225–233. Springer, Cham (2015). https://doi.
org/10.1007/978-3-319-24489-1 17
3. Arif, M.F., Menc´ıa, C., Marques-Silva, J.: Eﬃcient MUS enumeration of horn for-
mulae with applications to axiom pinpointing. In: Heule, M., Weaver, S. (eds.)
SAT 2015. LNCS, vol. 9340, pp. 324–342. Springer, Cham (2015). https://doi.org/
10.1007/978-3-319-24318-4 24
4. Ausiello, G., Laura, L.: Directed hypergraphs: introduction and fundamental
algorithms-a survey. Theor. Comput. Sci. 658, 293–306 (2017)

Hypergraph-Based Inference Rules
327
5. Baader, F., Brandt, S., Lutz, C.: Pushing the EL envelope. In: IJCAI, vol. 5, pp.
364–369 (2005)
6. Chen, J., Ludwig, M., Ma, Y., Walther, D.: Zooming in on ontologies: minimal
modules and best excerpts. In: d’Amato, C., et al. (eds.) ISWC 2017. LNCS, vol.
10587, pp. 173–189. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
68288-4 11
7. Gallo, G., Longo, G., Pallottino, S., Nguyen, S.: Directed hypergraphs and appli-
cations. Discret. Appl. Math. 42(2–3), 177–201 (1993)
8. Grau, B.C., Horrocks, I., Kazakov, Y., Sattler, U.: Modular reuse of ontologies:
theory and practice. J. Artif. Intell. Res. 31, 273–318 (2008)
9. Ignatiev, A., Marques-Silva, J., Menc´ıa, C., Pe˜naloza, R.: Debugging EL+ ontolo-
gies through horn MUS enumeration. In: Artale, A., Glimm, B., Kontchakov,
R. (eds.) Proceedings of the 30th International Workshop on Description Logics,
Montpellier, France, 18–21 July 2017. CEUR Workshop Proceedings, vol. 1879.
CEUR-WS.org (2017). http://ceur-ws.org/Vol-1879/paper54.pdf
10. Kalyanpur, A., Parsia, B., Horridge, M., Sirin, E.: Finding all justiﬁcations of
OWL DL entailments. In: Aberer, K., et al. (eds.) ASWC/ISWC -2007. LNCS,
vol. 4825, pp. 267–280. Springer, Heidelberg (2007). https://doi.org/10.1007/978-
3-540-76298-0 20
11. Kalyanpur, A., Parsia, B., Sirin, E., Hendler, J.: Debugging unsatisﬁable classes in
OWL ontologies. J. Web Semant. 3(4), 268–293 (2005)
12. Kazakov, Y., Klinov, P.: Goal-directed tracing of inferences in EL ontologies. In:
Mika, P., et al. (eds.) ISWC 2014. LNCS, vol. 8797, pp. 196–211. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-11915-1 13
13. Kazakov, Y., Kr¨otzsch, M., Simancik, F.: ELK reasoner: architecture and evalua-
tion. In: ORE (2012)
14. Kazakov, Y., Skoˇcovsk´y, P.: Enumerating justiﬁcations using resolution. In:
Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol.
10900, pp. 609–626. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-
94205-6 40
15. Ludwig, M., Walther, D.: The logical diﬀerence for ELHr-terminologies using
hypergraphs. In: Schaub, T., Friedrich, G., O’Sullivan, B. (eds.) 21st European
Conference on Artiﬁcial Intelligence, ECAI 2014, Prague, Czech Republic, 18–22
August 2014 - Including Prestigious Applications of Intelligent Systems (PAIS
2014). Frontiers in Artiﬁcial Intelligence and Applications, vol. 263, pp. 555–560.
IOS Press (2014). https://doi.org/10.3233/978-1-61499-419-0-555
16. Manthey, N., Pe˜naloza, R., Rudolph, S.: Eﬃcient axiom pinpointing in EL using
sat technology. In: Description Logics (2016)
17. Pe˜naloza, R.: Axiom pinpointing. In: Cota, G., Daquino, M., Pozzato, G.L. (eds.)
Applications and Practices in Ontology Design, Extraction, and Reasoning, Studies
on the Semantic Web, vol. 49, pp. 162–177. IOS Press (2020). https://doi.org/10.
3233/SSW200042
18. Penaloza, R., Sertkaya, B.: Understanding the complexity of axiom pinpointing in
lightweight description logics. Artif. Intell. 250, 80–104 (2017)

328
H. Yang et al.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Choices, Invariance, Substitutions,
and Formalizations

Sequent Calculi for Choice Logics
Michael Bernreiter1(B), Anela Lolic1, Jan Maly2, and Stefan Woltran1
1 Institute of Logic and Computation, TU Wien, Vienna, Austria
{mbernrei,alolic,woltran}@dbai.tuwien.ac.at
2 Institute for Logic, Language and Computation, University of Amsterdam,
Amsterdam, The Netherlands
j.f.maly@uva.nl
Abstract. Choice logics constitute a family of propositional logics and
are used for the representation of preferences, with especially qualita-
tive choice logic (QCL) being an established formalism with numerous
applications in artiﬁcial intelligence. While computational properties and
applications of choice logics have been studied in the literature, only few
results are known about the proof-theoretic aspects of their use. We pro-
pose a sound and complete sequent calculus for preferred model entail-
ment in QCL, where a formula F is entailed by a QCL-theory T if F
is true in all preferred models of T. The calculus is based on labeled
sequent and refutation calculi, and can be easily adapted for diﬀerent
purposes. For instance, using the calculus as a cornerstone, calculi for
other choice logics such as conjunctive choice logic (CCL) can be obtained
in a straightforward way.
1
Introduction
Choice logics are propositional logics for the representation of alternative options
for problem solutions [4]. These logics add new connectives to classical propo-
sitional logic that allow for the formalization of ranked options. A prominent
example is qualitative choice logic (QCL for short) [7], which adds the con-
nective ordered disjunction #»
× to classical propositional logic. Intuitively, A#»
×B
means that if possible A, but if A is not possible than at least B. The semantics
of a choice logic induce a preference ordering over the models of a formula.
As choice logics are well suited for preference handling, they have a multitude
of applications in AI such as logic programming [8], alert correlation [3], or
database querying [13]. But while computational properties and applications of
choice logics have been studied in the literature, only few results are known
about the proof-theoretic aspects of their use. In particular, there is no proof
system capable of deriving valid sentences containing choice operators. In this
paper we propose a sound and complete calculus for preferred model entailment
in QCL that can easily be generalized to other choice logics.
Entailment in choice logics is non-monotonic: conclusions that have been
drawn might not be derivable in light of new information. It is therefore not
surprising that choice logics are related to other non-monotonic formalisms. For
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 331–349, 2022.
https://doi.org/10.1007/978-3-031-10769-6_20

332
M. Bernreiter et al.
instance, it is known [7] that QCL can capture propositional circumscription
and that, if additional symbols in the language are admitted, circumscription
can be used to generate models corresponding to the inclusion-preferred QCL
models up to the additional atoms. We do not intend to use this translation of
our choice logic formulas (or sequents) in order to employ an existing calculus
for circumscription, for instance [5].
Instead, we deﬁne calculi in sequent format directly for choice logics, which
are diﬀerent from existing non-monotonic logics in the way non-monotonicity
is introduced. Speciﬁcally, the non-standard part of our logics is a new logi-
cal connective which is fully embedded in the logical language. For this reason,
calculi for choice logics also diﬀer from most other calculi for non-monotonic
logics: our calculi do not use non-standard inference rules as in default logic,
modal operators expressing consistency or belief as in autoepistemic logic, or
predicates whose extensions are minimized as in circumscription. However, one
method that can also be applied to choice logics is the use of a refutation calculus
(also known as rejection or antisequent calculus) axiomatising invalid formulas,
i.e., non-theorems. Refutation calculi for non-monotonic logics were used in [5].
Speciﬁcally, by combining a refutation calculus with an appropriate sequent cal-
culus, elegant proof systems for the central non-monotonic formalisms of default
logic [16], autoepistemic logic [15], and circumscription [14] were obtained. How-
ever, to apply this idea to choice logics, we have to take another facet of their
semantics into account.
With choice logics, we are working in a setting similar to many-valued log-
ics. Interpretations ascribe a natural number called satisfaction degree to choice
logic formulas. Preferred models of a formula are then those models with the
least degree. There are several kinds of sequent calculus systems for many-valued
logics, where the representation as a hypersequent calculus [1,10] plays a promi-
nent role. However, there are crucial diﬀerences between choice logics and many-
valued logics in the usual sense. Firstly, choice logic interpretations are classical,
i.e., they set propositional variables to either true or false. Secondly, non-classical
satisfaction degrees only arise when choice connectives, e.g. ordered disjunction
in QCL, occur in a formula. Thirdly, when applying a choice connective ◦to two
formulas A and B, the degree of A ◦B does not only depend on the degrees of
A and B, but also on the maximum degrees that A and B can possibly assume.
Therefore, techniques used in proof systems for conventional many-valued logics
can not be applied directly to choice logics.
In [11] a sequent calculus based system for reasoning with contrary-to-duty
obligations was introduced, where a non-classical connective was deﬁned to cap-
ture the notion of reparational obligation, which is in force only when a violation
of a norm occurs. This is related to the ordered disjunction in QCL, however,
based on the intended use in [11] the system was deﬁned only for the occurrence
of the new connective on the right side of the sequent sign. We aim for a proof
system for reasoning with choice logic operators, and to deduce formulas from
choice logic formulas. Thus, we need a calculus with left and right inference rules.

Sequent Calculi for Choice Logics
333
To obtain such a calculus we combine the idea of a refutation calculus with
methods developed for multi-valued logics in a novel way. First, we develop a
(monotonic) sequent calculus for reasoning about satisfaction degrees using a
labeled calculus, a method developed for (ﬁnite) many-valued logics [2,9,12].
Secondly, we deﬁne a labeled refutation calculus for reasoning about invalidity
in terms of satisfaction degrees. Finally, we join both calculi to obtain a sequent
calculus for the non-monotonic entailment of QCL. To this end, we introduce a
new, non-monotonic inference rule that has sequents of the two labeled calculi
as premises and formalizes degree minimization.
The rest of this paper is organized as follows. In the next section we present
the basic notions of choice logics and introduce the most prominent choice logics
QCL and CCL (conjunctive choice logic). In Sect. 3 we develop a labeled sequent
calculus for propositional logic extended by the QCL connective #»
×. This calculus
is shown to be sound and complete and already can be used to derive interesting
sentences containing choice operators. In Sect. 4 we extend the previously deﬁned
sequent calculus with an appropriate refutation calculus and non-monotonic rea-
soning, to capture entailment in QCL. The developed methodology for QCL can
be extended to other choice logics as well. In particular we show in Sect. 5 how
the calculi can be adapted for CCL.
2
Choice Logics
First, we formally deﬁne the notion of choice logics in accordance with the choice
logic framework of [4] before giving concrete examples in the form of QCL and
CCL. Finally, we deﬁne preferred model entailment.
Deﬁnition 1. Let U denote the alphabet of propositional variables. The set of
choice connectives CL of a choice logic L is a ﬁnite set of symbols such that
CL ∩{¬, ∧, ∨} = ∅. The set FL of formulas of L is deﬁned inductively as follows:
(i) a ∈FL for all a ∈U; (ii) if F ∈FL, then (¬F) ∈FL; (iii) if F, G ∈FL,
then (F ◦G) ∈FL for ◦∈({∧, ∨} ∪CL).
For example, CQCL = {#»
×} and ((a#»
×c) ∧(b#»
×c)) ∈FQCL. Formulas that do not
contain a choice connective are referred to as classical formulas.
The semantics of a choice logic is given by two functions, satisfaction degree
and optionality. The satisfaction degree of a formula given an interpretation
is either a natural number or ∞. The lower this degree, the more preferable
the interpretation. The optionality of a formula describes the maximum ﬁnite
satisfaction degree that this formula can be ascribed, and is used to penalize
non-satisfaction.
Deﬁnition 2. The optionality of a choice connective ◦∈CL in a choice logic L
is given by a function opt◦
L : N2 →N such that opt◦
L(k, ℓ) ≤(k + 1) · (ℓ+ 1) for
all k, ℓ∈N. The optionality of an L-formula is given via optL : FL →N with
(i) optL(a) = 1 for every a ∈U; (ii) optL(¬F) = 1; (iii) optL(F ∧G) = optL(F ∨
G) = max(optL(F), optL(G)); (iv) optL(F ◦G) = opt◦
L(optL(F), optL(G)) for
every choice connective ◦∈CL.

334
M. Bernreiter et al.
The optionality of a classical formula is always 1. Note that, for any choice
connective ◦, the optionality of F ◦G is bounded such that optL(F ◦G) ≤
(optL(F) + 1) · (optL(G) + 1). In the following, we write N for (N ∪{∞}).
Deﬁnition 3. The satisfaction degree of a choice connective ◦∈CL in a choice
logic L is given by a function deg◦
L : N2 × N
2 →N such that deg◦
L(k, ℓ, m, n) ≤
opt◦
L(k, ℓ) or deg◦
L(k, ℓ, m, n) = ∞for all k, ℓ∈N and all m, n ∈N. The satis-
faction degree of an L-formula under an interpretation I ⊆U is given via the
function degL : 2U × FL →N with
1. degL(I, a) = 1 if a ∈I, degL(I, a) = ∞otherwise for every a ∈U;
2. degL(I, ¬F) = 1 if degL(I, F) = ∞, degL(I, ¬F) = ∞otherwise;
3. degL(I, F ∧G) = max(degL(I, F), degL(I, G));
4. degL(I, F ∨G) = min(degL(I, F), degL(I, G));
5. degL(I, F ◦G) = deg◦
L(optL(F), optL(G), degL(I, F), degL(I, G)), ◦∈CL.
We also write I |=L
m F for degL(I, F) = m. If m < ∞, we say that I satisﬁes F
(to a ﬁnite degree), and if m = ∞, then I does not satisfy F. If F is a classical
formula, then I |=L
1 F ⇐⇒I |= F and I |=L
∞F ⇐⇒I ̸|= F. The symbols ⊤
and ⊥are shorthand for the formulas (a ∨¬a) and (a ∧¬a), where a can be any
variable. We have optL(⊤) = optL(⊥) = 1, degL(I, ⊤) = 1 and degL(I, ⊥) = ∞
for any interpretation I in every choice logic.
Models and preferred models of formulas are deﬁned in the following way:
Deﬁnition 4. Let L be a choice logic, I an interpretation, and F an L-
formula. I is a model of F, written as I ∈ModL(F), if degL(I, F) < ∞.
I is a preferred model of F, written as I ∈Prf L(F), if I ∈ModL(F) and
degL(I, F) ≤degL(J , F) for all other interpretations J .
Moreover, we deﬁne the notion of classical counterparts for choice connectives.
Deﬁnition 5. Let L be a choice logic. The classical counterpart of a choice
connective ◦∈CL is the classical binary connective ⊛such that, for all atoms
a and b, degL(I, a ◦b) < ∞⇐⇒I |= a ⊛b. The classical counterpart of an
L-formula F is denoted as cp(F) and is obtained by replacing all occurrences of
choice connectives in F by their classical counterparts.
A natural property of known choice logics is that choice connectives can be
replaced by their classical counterpart without aﬀecting satisﬁability, meaning
that degL(I, F) < ∞⇐⇒I |= cp(F) holds for all L-formulas F.
So far we introduced choice logics in a quite abstract way. We now introduce
two particular instantiations, namely QCL, the ﬁrst and most prominent choice
logic in the literature, and CCL, which introduces a connective #»
⊙called ordered
conjunction in place of QCL’s ordered disjunction.

Sequent Calculi for Choice Logics
335
Deﬁnition 6. QCL is the choice logic such that CQCL = {#»
×}, and, if k =
optQCL(F), ℓ= optQCL(G), m = degQCL(I, F), and n = degQCL(I, G), then
optQCL(F #»
×G) = opt
#»
×
QCL(k, ℓ) = k + ℓ, and
degQCL(I, F #»
×G) = deg
#»
×
QCL(k, ℓ, m, n) =
⎧
⎪
⎨
⎪
⎩
m
if m < ∞;
n + k
if m = ∞, n < ∞;
∞
otherwise.
In the above deﬁnition, we can see how optionality is used to penalize non-
satisfaction: given a QCL-formula F #»
×G and an interpretation I, if I satis-
ﬁes F (to some ﬁnite degree), then degQCL(I, F #»
×G) = degQCL(I, F); if I
does not satisfy F, then degQCL(I, F #»
×G) = optQCL(F) + degQCL(I, G). Since
degQCL(I, F) ≤optQCL(F), interpretations that satisfy F result in a lower
degree, i.e., are more preferable, compared to interpretations that do not sat-
isfy F. Let us take a look at a concrete example:
Example 1. Consider the QCL-formula F = (a#»
×c) ∧(b#»
×c). Note that the clas-
sical counterpart of #»
× is ∨, i.e., cp(F) = (a∨c)∧(b∨c). Thus, {c}, {a, b}, {a, c},
{b, c}, {a, b, c} ∈ModQCL(F). Of these models, {a, b} and {a, b, c} satisfy F to
a degree of 1 while {c}, {a, c}, and {b, c} satisfy F to a degree of 2. Therefore,
{a, b}, {a, b, c} ∈Prf QCL(F).
Next, we deﬁne CCL. Note that we follow the revised deﬁnition of CCL [4], which
diﬀers from the initial speciﬁcation1. Intuitively, given a CCL-formula F #»
⊙G it
is best to satisfy both F and G, but also acceptable to satisfy only F.
Deﬁnition 7. CCL is the choice logic such that CCCL = {#»
⊙}, and, if k =
optCCL(F), ℓ= optCCL(G), m = degCCL(I, F), and n = degCCL(I, G), then
optCCL(F #»
⊙G) = k + ℓ, and
degCCL(I, F #»
⊙G) =
⎧
⎪
⎨
⎪
⎩
n
if m = 1, n < ∞;
m + ℓ
if m < ∞and (m > 1 or n = ∞);
∞
otherwise.
Example 2. Consider the CCL-formula G = (a#»
⊙c) ∧(b#»
⊙c). Note that the clas-
sical counterpart of #»
⊙is the ﬁrst projection, i.e., cp(G) = a ∧b. Thus, {a, b},
{a, b, c} ∈ModCCL(G). Of these models, {a, b, c} satisﬁes G to a degree of 1
while {a, b} satisﬁes G to a degree of 2. Therefore, {a, b, c} ∈Prf CCL(G).
If L is a choice logic, then a set of L-formulas is called an L-theory. An
L-theory T entails a classical formula F, written as T |∼F, if F is true in
all preferred models of T. However, we ﬁrst need to deﬁne what the preferred
models of a choice logic theory are. There are several approaches for this. In the
original QCL paper [7], a lexicographic and an inclusion-based approach were
introduced.
1 It seems that, under the initial deﬁnition of CCL, a#»
⊙b is always ascribed a degree
of 1 or ∞, i.e., non-classical degrees can not be obtained (cf. Deﬁnition 8 in [6]).

336
M. Bernreiter et al.
Deﬁnition 8. Let L be a choice logic, I an interpretation, and T an L-theory.
I ∈ModL(T) if degL(I, F) < ∞for all F ∈T. Ik
L(T) denotes the set of formulas
in T satisﬁed to a degree of k by I, i.e., Ik
L(T) = {F ∈T | degL(I, F) = k}.
– I is a lexicographically preferred model of T, written as I ∈Prf lex
L (T), if
I ∈ModL(T) and if there is no J ∈ModL(T) such that, for some k ∈N and
all l < k, |Ik
L(T)| < |J k
L(T)| and |Il
L(T)| = |J l
L(T)| holds.
– I is an inclusion-based preferred model of T, written as I ∈Prf inc
L (T), if
I ∈ModL(T) and if there is no J ∈ModL(T) such that, for some k ∈N and
all l < k, Ik
L(T) ⊂J k
L(T) and Il
L(T) = J l
L(T) holds.
In our calculus for preferred model entailment we focus on the lexicographic
approach, but it will become clear how it can be adapted to other preferred model
semantics (see Sect. 4). We now formally deﬁne preferred model entailment:
Deﬁnition 9. Let L be a choice logic, T an L-theory, S a classical theory, and
σ ∈{lex, inc}. T |∼σ
L S if for all I ∈Prf σ
L(T) there is F ∈S such that I |= F.
Example 3. Consider the QCL-theory T = {¬(a∧b), a#»
×c, b#»
×c}. Then {c}, {a, c},
{b, c} ∈ModQCL(T). Note that, because of ¬(a∧b), a model of T can not satisfy
both a#»
×c and b#»
×c to a degree of 1. Speciﬁcally,
{a, c}1
QCL(T) = {¬(a ∧b), a#»
×c} and {a, c}2
QCL(T) = {b#»
×c},
{b, c}1
QCL(T) = {¬(a ∧b), b#»
×c} and {b, c}2
QCL(T) = {a#»
×c},
{c}1
QCL(T) = {¬(a ∧b)} and {c}2
QCL(T) = {a#»
×c, b#»
×c}.
Thus, {a, c}, {b, c} ∈Prf lex
QCL(T) but {c} ̸∈Prf lex
QCL(T). It can be concluded that
T |∼lex
QCL c ∧(a ∨b). However, T ̸|∼lex
QCLa and T ̸|∼lex
QCLb.
It is easy to see that preferred model entailment is non-monotonic. For example,
{a#»
×b} |∼lex
QCL a but {a#»
×b, ¬a} ̸|∼lex
QCLa.
3
The Sequent Calculus L[QCL]
As a ﬁrst step towards a calculus for preferred model entailment, we propose a
labeled calculus [2,12] for reasoning about the satisfaction degrees of QCL formu-
las in sequent format and prove its soundness and completeness. One advantage
of the sequent calculus format is having symmetrical left and right rules for all
connectives, in particular for the choice connectives. This is in contrast to the
representation of ordered disjunction in the calculus for deontic logic [11], in
which only right-hand side rules are considered.
As the calculus will be concerned with satisfaction degrees rather than pre-
ferred models, we need to deﬁne entailment in terms of satisfaction degrees. To
this end, the formulas occurring in the sequents of our calculus are labeled with
natural numbers, i.e., they are of the form (A)k, where A is a choice logic formula
and k ∈N. (A)k is satisﬁed by those interpretations that satisfy A to a degree of

Sequent Calculi for Choice Logics
337
k. Instead of labeling formulas with degree ∞we use the negated formula, i.e.,
instead of (A)∞we use (¬A)1. We observe that (A)k for optL(A) > k can never
have a model. We will deal with such formulas by replacing them with (⊥)1. For
classical formulas, we may write A for (A)1.
Deﬁnition 10. Let (A1)k1, . . . , (Am)km and (B1)l1, . . . , (Bn)ln be labeled QCL-
formulas. (A1)k1, . . . , (Am)km ⊢(B1)l1, . . . , (Bn)ln is a labeled QCL-sequent.
Γ ⊢Δ is valid iﬀevery interpretation that satisﬁes all labeled formulas in Γ
to the degree speciﬁed by the label also satisﬁes at least one labeled formula in Δ
to the degree speciﬁed by the label.
Note that entailment in terms of satisfaction degrees, as deﬁned above, is mon-
tonic. Frequently we will write (A)<k as shorthand for (A)1, . . . , (A)k−1 and
(A)>k for (A)k+1, . . . , (A)optQCL(A), (¬A)1. Moreover, ⟨Γ, (A)i ⊢Δ⟩i<k denotes
the sequence of sequents
Γ, (A)1 ⊢Δ . . . Γ, (A)k−1 ⊢Δ.
Analogously, ⟨Γ, (A)i ⊢Δ⟩i>k stands for the sequence of sequents Γ, (A)k+1 ⊢
Δ . . . Γ, (A)optQCL(A) ⊢Δ
Γ, (¬A)1 ⊢Δ.
We deﬁne the sequent calculus L[QCL] over labeled sequents below. In addi-
tion to introducing inference rules for #»
× we have to modify the inference rules
for conjunction and disjunction of propositional LK. The idea behind the ∨-left
rule is that a model M of (A)k is only a model of (A ∨B)k if there is no l < k
s.t. M is a model of (B)l. Therefore, every model of (A ∨B)k is a model of Δ iﬀ
– every model of (A)k is a model of Δ or of some (B)l with l < k,
– every model of (B)k is a model of Δ or of some (A)l with l < k.
Essentially the same idea works for ∧-left but with l > k. For the ∨-right rule,
in order for every model of Γ to be a model of (A ∨B)k, every model of Γ must
either be a model of (A)k or of (B)k and no model of Γ can be a model of (A)l
for l < k, i.e., Γ, (A)l ⊢⊥. Similarly for ∧-right.
Deﬁnition 11 (L[QCL]). The axioms of L[QCL] are of the form (p)1 ⊢(p)1 for
propositional variables p. The inference rules are given below. For the structural
and logical rules, whenever a labeled formula (F)k appears in the conclusion of
an inference rule it holds that k ≤optL(F).
The structural rules are:
Γ ⊢Δ
wl
Γ, (A)k ⊢Δ
Γ ⊢Δ
wr
Γ ⊢(A)k, Δ
Γ, (A)k, (A)k ⊢Δ
cl
Γ, (A)k ⊢Δ
Γ ⊢(A)k, (A)k, Δ cr
Γ ⊢(A)k, Δ
The logical rules are:
Γ ⊢(cp(A))1, Δ
¬l
Γ, (¬A)1 ⊢Δ
Γ, (cp(A))1 ⊢Δ ¬r
Γ ⊢(¬A)1, Δ
Γ, (A)k ⊢(B)<k, Δ
Γ, (B)k ⊢(A)<k, Δ
∨l
Γ, (A ∨B)k ⊢Δ
⟨Γ, (A)i ⊢Δ⟩i<k
⟨Γ, (B)i ⊢Δ⟩i<k
Γ ⊢(A)k, (B)k, Δ ∨r
Γ ⊢(A ∨B)k, Δ

338
M. Bernreiter et al.
Γ, (A)k ⊢(B)>k, Δ
Γ, (B)k ⊢(A)>k, Δ
∧l
Γ, (A ∧B)k ⊢Δ
⟨Γ, (A)i ⊢Δ⟩i>k
⟨Γ, (B)i ⊢Δ⟩i>k
Γ ⊢(A)k, (B)k, Δ ∧r
Γ ⊢(A ∧B)k, Δ
The rules for ordered disjunction, with k ≤optL(A) and l ≤optL(B), are:
Γ, (A)k ⊢Δ
#»
×l1
Γ, (A#»
×B)k ⊢Δ
Γ, (B)l, (¬A)1 ⊢Δ
#»
×l2
Γ, (A#»
×B)optQCL(A)+l ⊢Δ
Γ ⊢(A)k, Δ
#»
×r1
Γ ⊢(A#»
×B)k, Δ
Γ ⊢(¬A)1, Δ
Γ ⊢(B)l, Δ
#»
×r2
Γ ⊢(A#»
×B)optQCL(A)+l, Δ
The degree overﬂow rules2, with k ∈N, are:
Γ, ⊥⊢Δ
dol
Γ, (A)optQCL(A)+k ⊢Δ
Γ ⊢Δ
dor
Γ ⊢(A)optQCL(A)+k, Δ
Observe that the modiﬁed ∧and ∨inference rules correspond to the ∧and ∨
inference rules of propositional LK in case we are dealing only with classical
formulas. Our ∧-left rule splits the proof-tree unnecessarily for classical theories,
and the ∧-right rule adds an unnecessary third condition Γ ⊢A, B, Δ. These
additional conditions are necessary when dealing with non-classical formulas.
The intuition behind the degree overﬂow rules is that we sometimes need to
ﬁx invalid sequences, i.e., sequences in which a formula F is assigned a label k
with optQCL(F) < k < ∞.
Example 4. The following is an L[QCL]-proof of a valid sequent.3
...
b ∨c, ¬a, b ⊢a ∧b, a ∧c, b
#»
×l2
b ∨c, (a#»
×b)2 ⊢a ∧b, a ∧c, b
¬r
(a#»
×b)2 ⊢¬(b#»
×c), a ∧b, a ∧c, b
...
a ∨b, ¬b, c ⊢a ∧b, a ∧c, b
#»
×l2
a ∨b, (b#»
×c)2 ⊢a ∧b, a ∧c, b
¬r
(b#»
×c)2 ⊢¬(a#»
×b), a ∧b, a ∧c, b
∧l
((a#»
×b) ∧(b#»
×c))2 ⊢a ∧b, a ∧c, b
¬l
¬(a ∧b), ((a#»
×b) ∧(b#»
×c))2 ⊢a ∧c, b
Example 5. The following proof shows how the ∧r-rule can introduce more than
three premises. Note that we make use of the dol-rule in the leftmost branch.
...
a, b, ⊥⊢
dol
a, b, (a)2 ⊢
...
a, b, ¬a ⊢
...
a, b, c, ¬b ⊢
#»
×l2
a, b, (b#»
×c)2 ⊢
...
a, b ⊢b ∨c
¬l
a, b, ¬(b#»
×c) ⊢
...
a, b ⊢a, b
#»
×r1
a, b ⊢a, (b#»
×c)1 ∧r
a, b ⊢(a ∧(b#»
×c))1
We now show soundness and completeness of L[QCL].
2 dol/dor stands for degree overﬂow left/right.
3 Note that, once we reach sequents containing only classical formulas, we do not
continue the proof. However, it can be veriﬁed that the classical sequents on the left
and right branch are provable in this case. Moreover, given a formula (A)1 with a
label of 1, the label is often omitted for readability.

Sequent Calculi for Choice Logics
339
Proposition 1. L[QCL] is sound.
Proof. We show for all rules that they are sound.
– For (ax) and the structural rules this is clearly the case.
– (¬r) and (¬l): follows from the fact that degQCL(I, F) < ∞⇐⇒I |= cp(F)
for all QCL-formulas F.
– (∨l): Assume that the conclusion of the rule is not valid, i.e., there is a model
M of Γ and (A ∨B)k that is not a model of Δ. Then, M satisﬁes either A or
B to degree k and neither to a degree smaller than k. Assume M satisﬁes A
to a degree of k, the other case is symmetric. Then M is a model of Γ and
(A)k but, by assumption, neither of Δ nor of (B)j for any j < k. Hence at
least one of the premises is not valid. Analogously for (∧l).
– (∨r): Assume there is a model M of Γ that is not a model of Δ or of (A∨B)k.
There are two possible cases why M is not a model of (A∨B)k: (1) M satisﬁes
neither A nor B to degree k. But then the premise Γ ⊢(A)k, (B)k, Δ is not
valid as M is also not a model of Δ by assumption. (2) M satisﬁes either A or
B to a degree smaller than k. Assume that M satisﬁes A to degree j < k (the
other case is symmetric). Then the premise Γ, (A)j ⊢Δ is not valid. Indeed,
M is a model of Γ and (A)j but not of Δ. Analogously for (∧r).
– (#»
×l1) and (#»
×r1): follows from the fact that (A)k has the same models as
(A#»
×B)k for k ≤optL(A).
– (#»
×l2): Assume the conclusion of the rule is not valid and let M be the model
witnessing this. Then M is a model of (A#»
×B)optQCL(A)+l. By deﬁnition, M
satisﬁes B to degree l and is not a model of A. However, then it is also a
model of Γ, (B)l and (¬A)1, which means that the premise is not valid.
– (#»
×r2). Assume that both premises are valid, i.e., every model of Γ is either a
model of Δ or of (¬A)1 and (B)l with l ≤optL(B). Now, by deﬁnition, any
model that is not a model of A (and hence a model of (¬A)1) and of (B)l
satisﬁes A#»
×B to degree optQCL(A) + l. Therefore, every model of Γ is either
a model of Δ or of (A#»
×B)optQCL(A)+l, which means that the conclusion of
the rule is valid.
– (dol): Γ, ⊥has no models, i.e., the premise Γ, ⊥⊢Δ is valid. Crucially, the
sequent Γ, (A)optQCL(A)+k has no models as well since A cannot be satisﬁed
to a degree m with optL(A) < m < ∞. (dor) is clearly sound.
⊓⊔
Proposition 2. L[QCL] is complete.
Proof. We show this by induction over the (aggregated) formula complexity of
the non-classical formulas.
– For the base case, we observed that if all formulas are classical and labeled
with 1, then all our rules reduce to the classical sequent calculus, which is
known to be complete. Moreover, we observe that (A)1 is equivalent to A.
Hence, we can turn labeled atoms into classical atoms.
– Assume that a sequent of the form Γ, (A)optQCL(A)+k ⊢Δ with k ∈N is valid.
Since Γ, ⊥has no models, Γ, ⊥⊢Δ is valid and, by the induction hypothesis,
provable. Thus, Γ, (A)optQCL(A)+k ⊢Δ is provable using the (dol) rule.

340
M. Bernreiter et al.
– Assume that a sequent Γ ⊢(A)optQCL(A)+k, Δ is valid. (A)optQCL(A)+k can not
be satisﬁed, i.e., Γ ⊢Δ is valid and, by the induction hypothesis, provable.
Therefore, Γ ⊢(A)optQCL(A)+k, Δ is provable using the (dor) rule.
– Assume that a sequent of the form Γ ⊢(¬A)1, Δ is valid. Then every model
of Γ is either a model of (¬A)1 or of Δ. In other words, every model of Γ that
is not a model of (¬A)1 (i.e., is model of cp(A)) is a model of Δ. Therefore,
every interpretation that is a model of both Γ and cp(A) must be a model
of Δ. It follows that Γ, cp(A) ⊢Δ is valid and, by the induction hypothesis,
provable. Thus, Γ ⊢(¬A)1, Δ is provable using the (¬r) rule. Similarly for
Γ, (¬A)1 ⊢Δ.
– Assume that a sequent of the form Γ, (A∨B)k ⊢Δ is valid, with k ≤optL(A∨
B). We claim that then both Γ, (A)k ⊢(B)<k, Δ and Γ, (B)k ⊢(A)<k, Δ are
valid. Assume to the contrary that Γ, (A)k ⊢(B)<k, Δ is not valid (the other
case is symmetric). Then, there is a model M of Γ and (A)k that is neither
a model of (B)<k nor of Δ. But then M is also a model of Γ and (A ∨B)k,
but not of Δ, which contradicts the assumption that Γ, (A∨B)k ⊢Δ is valid.
Therefore, both Γ, (A)k ⊢(B)<k, Δ and Γ, (B)k ⊢(A)<k, Δ are valid and,
by the induction hypothesis, provable. This means that Γ, (A ∨B)k ⊢Δ is
provable by (∨l). Similarly for a sequent of the form Γ, (A ∧B)k ⊢Δ.
– Assume that a sequent of the form Γ ⊢(A ∨B)k, Δ is valid, with k ≤
optL(A ∨B). We claim that then for all i < k the sequents Γ, (A)i ⊢Δ and
Γ, (B)i ⊢Δ and Γ ⊢(A)k, (B)k, Δ are valid. Assume by contradiction that
there is an i < k s.t. Γ, (A)i ⊢Δ is not valid. Then, there is a model M of
Γ and (A)i that is not a model of Δ. However, then M is a model of Γ but
neither of Δ nor of (A ∨B)k (as M satisﬁes A ∨B to degree i ̸= k), which
contradicts our assumption that Γ ⊢(A∨B)k, Δ is valid. The case that there
is an i < k s.t. Γ, (B)i ⊢Δ is not valid is symmetric. Finally, we assume that
Γ ⊢(A)k, (B)k, Δ is not valid. Then, there is a model M of Γ that is not a
model of (A)k, (B)k or Δ. Then, M is model of Γ but neither of Δ nor of
(A ∨B)k, contradicting our assumption. Therefore, all sequents listed above
must be valid, and, by the induction hypothesis, Γ ⊢(A∨B)k, Δ is provable.
Similarly for a sequent of the form Γ ⊢(A ∧B)k, Δ.
– Assume that a sequent of the form Γ, (A#»
×B)k ⊢Δ with k ≤optQCL(A) is
valid. Then Γ, (A)k ⊢Δ is also valid since (A#»
×B)k and (A)k have the same
models if k ≤optQCL(A). By the induction hypothesis Γ, (A#»
×B)k ⊢Δ is
provable. Analogously for sequents of the form Γ ⊢(A#»
×B)k, Δ.
– Assume that a sequent of the form Γ, (A#»
×B)optQCL(A)+l ⊢Δ is valid, with
l ≤optL(B). We claim that the sequent Γ, (B)l, ¬A ⊢Δ is then also valid.
Indeed, if M is a model of Γ, (B)l and ¬A, then it is also a model of Γ and
(A#»
×B)optQCL(A)+l. Hence, by assumption, M must be a model of Δ. From
this, we can conclude as before that Γ, (A#»
×B)optQCL(A)+l ⊢Δ is provable.
– Assume that a sequent of the form Γ ⊢(A#»
×B)optQCL(A)+l, Δ is valid, with
l ≤optL(B). We claim that then also the sequents Γ ⊢¬A, Δ and Γ ⊢
(B)l, Δ are valid. Assume by contradiction that the ﬁrst sequent is not valid.
This means that there is a model M of Γ that is not a model of either ¬A

Sequent Calculi for Choice Logics
341
nor of Δ. However, then M is a model of A and therefore satisﬁes A#»
×B
to a degree smaller than optQCL(A). This contradicts our assumption that
Γ ⊢(A#»
×B)optQCL(A)+l, Δ is valid. Assume now that the second sequent is
not valid, i.e., that there is a model M of Γ that is neither a model of (B)l
nor of Δ. Then, M cannot be a model of (A#»
×B)optQCL(A)+l and we again
have a contradiction to our assumption. As before, it follows by the induction
hypothesis that Γ ⊢(A#»
×B)optQCL(A)+l, Δ is provable.
⊓⊔
So far we have not introduced a cut rule, and as we have shown our calculus
is complete without such a rule. However, it is easy to see that we have cut-
admissibility, i.e., L[QCL] can be extended by:
Γ ⊢(A)k, Δ
Γ ′, (A)k ⊢Δ′
cut
Γ, Γ ′ ⊢Δ, Δ′
Another aspect of our calculus that should be mentioned is that, although
L[QCL] is cut-free, we do not have the subformula property. This is especially
obvious when looking at the rules for negation, where we use the classical coun-
terpart cp(A) of QCL-formulas. For example, ¬(a#»
×b) in the conclusion of the
¬-left rule becomes cp(a#»
×b) = a ∨b in the premise.
While we believe that L[QCL] is interesting in its own right, the question
of how we can use it to obtain a calculus for preferred model entailment arises.
Essentially, we have to add a rule that allows us to go from standard to pre-
ferred model inferences. As a ﬁrst approach we consider theories Γ ∪{A} with Γ
consisting only of classical formulas and A being a QCL-formula. In this simple
case, preferred models of Γ ∪{A} are those models of Γ ∪{A} that satisfy A to
the smallest possible degree. One might add the following rule to L[QCL]:
⟨Γ, (A)i ⊢⊥⟩i<k
Γ, (A)k ⊢Δ
|∼naive
Γ, A |∼lex
QCL Δ
Intuitively, the above rule states that, if there are no interpretations that sat-
isfy Γ while also satisfying A to a degree lower than k, and if Δ follows from
all models of Γ, (A)k, then Δ is entailed by the preferred models of Γ ∪{A}.
However, the obtained calculus L[QCL] + |∼naive derives invalid sequents.
Example 6. The invalid entailment ¬a, a#»
×b |∼lex
QCL a can be derived via |∼naive.
a ⊢a
wl
¬a, a ⊢a
#»
×l1
¬a, (a#»
×b)1 ⊢a
|∼naive
¬a, a#»
×b |∼lex
QCL a
What is missing is an assertion that Γ, (A)k is satisﬁable. Unfortunately, this
can not be formulated in L[QCL]. A way of addressing this problem is to deﬁne
a refutation calculus, as has been done for other non-monotonic logics [5].

342
M. Bernreiter et al.
4
Calculus for Preferred Model Entailment
We now introduce a calculus for preferred model entailment. However, as argued
above, we ﬁrst need to introduce the refutation calculus L[QCL]−. In the liter-
ature, a rejection method for ﬁrst-order logic with equality was ﬁrst introduced
in [17] and proved complete w.r.t. ﬁnite model theory. Our refutation calculus
is based on a simpler rejection method for propositional logic deﬁned in [5].
Using the refutation calculus, we prove that (A)k is satisﬁable by deriving the
antisequent (A)k ⊬⊥.
Deﬁnition 12. A labeled QCL-antisequent is denoted by Γ ⊬Δ and it is valid
if and only if the corresponding labeled QCL-sequent Γ ⊢Δ is not valid, i.e., if
at least one model that satisﬁes all formulas in Γ to the degree speciﬁed by the
label satisﬁes no formula in Δ to the degree speciﬁed by the label.
Below we give a deﬁnition of the refutation calculus L[QCL]−. Note that most
rules coincide with their counterparts in L[QCL]. Binary rules are translated
into two rules; one inference rule per premise. (∨r) and (∧l) in L[QCL] have an
unbounded number of premises, but due to their structure they can be translated
into three inference rules. For (∧r) we need to introduce two extra rules for the
case that either A or B is not satisﬁed.
Deﬁnition 13 (L[QCL]−). The axioms of L[QCL]−are of the form Γ ⊬Δ,
where Γ and Δ are disjoint sets of atoms and ⊥̸∈Γ. The inference rules of
L[QCL]−are given below. Whenever a labeled formula (F)k appears in the con-
clusion of an inference rule it holds that k ≤optL(F).
The logical rules are:
Γ, (cp(A))1 ⊬Δ
⊬¬r
Γ ⊬(¬A)1, Δ
Γ ⊬(cp(A))1, Δ
⊬¬l
Γ, (¬A)1 ⊬Δ
Γ, (A)k ⊬(B)<k, Δ ⊬∨l1
Γ, (A ∨B)k ⊬Δ
Γ, (B)k ⊬(A)<k, Δ ⊬∨l2
Γ, (A ∨B)k ⊬Δ
Γ, (A)i ⊬Δ
⊬∨r1
Γ ⊬(A ∨B)k, Δ
Γ, (B)i ⊬Δ
⊬∨r2
Γ ⊬(A ∨B)k, Δ
Γ ⊬(A)k, (B)k, Δ ⊬∨r3
Γ ⊬(A ∨B)k, Δ
where i < k.
Γ, (A)k ⊬(B)>k, Δ ⊬∧l1
Γ, (A ∧B)k ⊬Δ
Γ, (B)k ⊬(A)>k, Δ ⊬∧l2
Γ, (A ∧B)k ⊬Δ
Γ, (A)i ⊬Δ
⊬∧r1
Γ ⊬(A ∧B)k, Δ
Γ, (¬A)1 ⊬Δ
⊬∧r2
Γ ⊬(A ∧B)k, Δ
Γ, (B)i ⊬Δ
⊬∧r3
Γ ⊬(A ∧B)k, Δ
Γ, (¬B)1 ⊬Δ
⊬∧r4
Γ ⊬(A ∧B)k, Δ
Γ ⊬(A)k, (B)k, Δ ⊬∧r5
Γ ⊬(A ∧B)k, Δ
where i > k.
The rules for ordered disjunction, with k ≤optL(A) and l ≤optL(B), are:
Γ, (A)k ⊬Δ
⊬#»
×l1
Γ, (A#»
×B)k ⊬Δ
Γ, (B)l, (¬A)1 ⊬Δ
⊬#»
×l2
Γ, (A#»
×B)optQCL(A)+l ⊬Δ
Γ ⊬(A)k, Δ
⊬#»
×r1
Γ ⊬(A#»
×B)k, Δ
Γ ⊬(¬A)1, Δ
⊬#»
×r2
Γ ⊬(A#»
×B)optQCL(A)+l, Δ
Γ ⊬(B)l, Δ
⊬#»
×r3
Γ ⊬(A#»
×B)optQCL(A)+l, Δ

Sequent Calculi for Choice Logics
343
The degree overﬂow rules, with k ∈N, are:
Γ, ⊥⊬Δ
⊬dol
Γ, (A)optQCL(A)+k ⊬Δ
Γ ⊬Δ
⊬dor
Γ ⊬(A)optQCL(A)+k, Δ
Example 7. The following is related to Example 4 and shows that the sequent
¬(a ∧b), ((a#»
×b) ∧(b#»
×c))2 is satisﬁable.
...
(a ∨b), c, ¬b ⊬a ∧b, ⊥
⊬#»
×l2
(a ∨b), (b#»
×c)2 ⊬a ∧b, ⊥
⊬¬r
(b#»
×c)2 ⊬¬(a#»
×b), a ∧b, ⊥
⊬∧l2
((a#»
×b) ∧(b#»
×c))2 ⊬a ∧b, ⊥
⊬¬l
¬(a ∧b), ((a#»
×b) ∧(b#»
×c))2 ⊬⊥
Note that the interpretation {a, c} witnesses (a ∨b), c, ¬b ⊬a ∧b, ⊥.
Proposition 3. L[QCL]−is sound.
Proof. The soundness of the negation rules is straightforward. The soundness of
the rules (#»
×l1), (#»
×l2) and (#»
×r1) follows by the same argument as for L[QCL].
For the remaining rules, it is easy to check that the same model witnessing the
validity of the premise also witnesses the validity of the conclusion.
⊓⊔
Proposition 4. L[QCL]−is complete.
Proof. We show completeness by an induction over the (aggregated) formula
complexity. Assume Γ ⊬Δ is valid, i.e. Γ ⊢Δ is not valid. Now, there must be a
rule in L[QCL] for which Γ ⊢Δ is the conclusion. By the soundness of L[QCL],
this implies that at least one of the premises Γ ∗⊢Δ∗is not valid. However, then
Γ ∗⊬Δ∗is valid and, by induction, also provable. Now, by the construction of
L[QCL]−, there is a rule that allows us to derive Γ ⊬Δ from Γ ∗⊬Δ∗.
⊓⊔
So far no cut-rule has been introduced for L[QCL]−, and indeed, a counterpart of
the cut rule would not be sound. One possibility is to introduce a contrapositive
of cut as described by Bonatti and Olivetti [5]. Again, it is easy to see that this
rule is admissible in our calculus:
Γ ⊬Δ
Γ, (A)k ⊢Δ
cut2
Γ ⊬(A)k, Δ
We are now ready to combine L[QCL] and L[QCL]−by deﬁning an inference rule
that allows us to go from labeled sequents to non-monotonic inferences. Again,
we ﬁrst consider the case where Γ is classical and A is a choice logic formula.
The preferred model inference rule is:
⟨Γ, (A)i ⊢⊥⟩i<k
Γ, (A)k ⊬⊥
Γ, (A)k ⊢Δ
|∼simple
Γ, A |∼lex
QCL Δ

344
M. Bernreiter et al.
Intuitively, the premises ⟨Γ, (A)i ⊢⊥⟩i<k along with Γ, (A)k ⊬⊥ensure that
models satisfying A to a degree of k are preferred, while the premise Γ, (A)k ⊢Δ
ensures that Δ is entailed by those preferred models.
Example 8. The valid entailment ¬(a∧b), (a#»
×b)∧(b#»
×c) |∼lex
QCL a∧c, b is provable
by choosing k = 2:
(ϕ1)
Γ, ((a #»
×b) ∧(b #»
×c))1 ⊢⊥
(ϕ2)
Γ, ((a #»
×b) ∧(b #»
×c))2 ⊬⊥
(ϕ3)
Γ, ((a #»
×b) ∧(b #»
×c))2 ⊢Δ
|∼simple
Γ, (a #»
×b) ∧(b #»
×c) |∼lex
QCL Δ
with Γ = ¬(a ∧b) and Δ = a ∧c, b. ϕ3 is the L[QCL]-proof from Example 4 and
ϕ2 is the L[QCL]−-proof from Example 7. ϕ1 is not shown explicitly, but it can
be veriﬁed that the corresponding sequent is provable.
We extend |∼simple to the more general case, where more than one non-classical
formula may be present, to obtain a calculus for preferred model entailment. An
additional rule |∼unsat is needed in case a theory is classically unsatisﬁable.
Deﬁnition 14 (L[QCL]lex
|∼). Let ≤l be the order on vectors in Nk deﬁned by
– v <l w if there is some n ∈N such that v has more entries of value n and
for all 1 ≤m < n both vectors have the same number of entries of value m.
– v =l w if, for all n ∈N, v and w have the same number of entries of value n.
L[QCL]lex
|∼consists of the axioms and rules of L[QCL] and L[QCL]−plus the
following rules, where v, w ∈Nk, Γ consists of only classical formulas, and
every Ai with 1 ≤i ≤k is a QCL-formula:
⟨Γ, (A1)w1 , . . . , (Ak)wk ⊢⊥⟩w <v
Γ, (A1)v1 , . . . , (Ak)vk ⊬⊥
⟨Γ, (A1)w1 , . . . , (Ak)wk ⊢Δ⟩w =v
|∼lex
Γ, A1, . . . , Ak |∼lex
QCL Δ
Γ, cp(A1), . . . , cp(Ak) ⊢⊥
|∼unsat
Γ, A1, . . . , Ak |∼lex
QCL Δ
We ﬁrst provide a small example and then show soundness and completeness.
Example 9. Consider the valid entailment ¬(a ∧b), (a#»
×b), (b#»
×c) |∼lex
QCL a ∧c, b
similar to Example 8, but with the information that we require (a#»
×b) and (b#»
×c)
encoded as separate formulas. It is not possible to satisfy all formulas on the left
to a degree of 1. Rather, it is optimal to either satisfy (¬(a∧b))1, (a#»
×b)1, (b#»
×c)2
or, alternatively, (¬(a ∧b))1, (a#»
×b)2, (b#»
×c)1. We choose v = (1, 1, 2), with w =
(1, 1, 1) being the only vector w s.t. w < v. Thus, we get
.
..
Γ, (a #»
×b)1, (b #»
×c)1 ⊢⊥
.
..
Γ, (a #»
×b)1, (b #»
×c)2 ⊬⊥
.
..
Γ, (a #»
×b)1, (b #»
×c)2 ⊢Δ
.
..
Γ, (a #»
×b)2, (b #»
×c)1 ⊢Δ
|∼lex
Γ, (a #»
×b), (b #»
×c) |∼lex
QCL Δ
with Γ = ¬(a ∧b) and Δ = a ∧c, b. It can be veriﬁed that indeed all branches
are provable, but we do not show this explicitly here.

Sequent Calculi for Choice Logics
345
Proposition 5. L[QCL]lex
|∼is sound.
Proof. Consider ﬁrst the |∼lex-rule and assume that all premises are derivable.
By the soundness of L[QCL] and L[QCL]−they are also valid. From the ﬁrst
set of premises ⟨Γ, (A1)w1, . . . , (Ak)wk ⊢⊥⟩w<v we can conclude that if there is
some model M of Γ that satisﬁes Ai to a degree of vi for all 1 ≤i ≤k, then
M ∈Prf lex
QCL(Γ ∪{A1, . . . , Ak}). The premise Γ, (A1)v1, . . . , (Ak)vk ⊬⊥ensures
that there is such a model M. By the last set of premises ⟨Γ, (A1)w1, . . . , (Ak)wk ⊢
Δ⟩w=v, we can conclude that all models of Γ ∪{A1, . . . , Ak} that are equally
as preferred as M, i.e., all M ′ ∈Prf lex
QCL(Γ ∪{A1, . . . , Ak}), satisfy at least one
formula in Δ. Therefore, Γ, A1, . . . , Ak |∼lex
QCL Δ is valid.
Now consider the |∼unsat-rule and assume that Γ, cp(A1), . . . , cp(Ak) ⊢⊥is
derivable and therefore valid. Thus, Γ ∪{A1, . . . , Ak} has no models and therefore
also no preferred models. Then Γ, A1, . . . , Ak |∼lex
QCL Δ is valid.
⊓⊔
Proposition 6. L[QCL]lex
|∼is complete.
Proof. Assume that Γ, A1, . . . , Ak |∼lex
QCL Δ is valid. If Γ ∪{A1, . . . , Ak} is unsat-
isﬁable then Γ, cp(A1), . . . , cp(Ak) ⊢⊥is valid, i.e., we can apply the |∼unsat-
rule. Now consider the case that Γ ∪{A1, . . . , Ak} is satisﬁable and assume that
some preferred model M of Γ ∪{A1, . . . , Ak} satisﬁes Ai to a degree of vi for
all 1 ≤i ≤k. Then, we claim that all premises of the rule are valid and, by the
completeness of L[QCL] and L[QCL]−, also derivable.
Assume by contradiction that one of the premises is not valid. First, consider
the case that Γ, (A1)w1, . . . , (Ak)wk ⊢⊥is not valid for some w < w. Then there
is a model M ′ of Γ that satisﬁes Ai to a degree of wi for all 1 ≤i ≤k. However,
this contradicts the assumption that M is a preferred model of Γ ∪{A1, . . . , Ak}.
Next, assume that Γ, (A1)v1, . . . , (Ak)vk ⊬⊥is not valid. However, M satisﬁes
Γ, (A1)v1, . . . , (Ak)vk and does not satisfy ⊥. Contradiction.
Finally, we assume that Γ, (A1)w1, . . . , (Ak)wk ⊢Δ is not valid for some
w = v. Then, there is a model M ′ of Γ that satisﬁes Ai to a degree of wi for all
1 ≤i ≤k but does not satisfy any formula in Δ. But M ′ is a preferred model of
Γ ∪{A1, . . . , Ak}, which contradicts Γ, A1, . . . , Ak |∼lex
QCL Δ being valid.
⊓⊔
In this paper, we focused on the lexicographic semantics for preferred models of
choice logic theories. However, rules for other semantics, e.g. a rule |∼inc for the
inclusion based approach (cf. Deﬁnition 8), can be obtained by simply adapting
the way in which vectors over Nk are compared (cf. Deﬁnition 14).
5
Beyond QCL
QCL was the ﬁrst choice logic to be described [7], and applications concerned
with QCL and ordered disjunction have been discussed in the literature [3,8,13].
For this reason, the main focus in this paper lies with QCL. However, as we
have seen in Sect. 2, CCL and its ordered conjunction show that interesting
logics similar to QCL exist. We will now demonstrate that L[QCL] can easily be

346
M. Bernreiter et al.
adapted for other choice logics. In particular, we introduce L[CCL] in which the
rules of L[QCL] for the classical connectives can be retained. All that is needed is
to replace the #»
×-rules by appropriate rules for the choice connective #»
⊙of CCL.
Deﬁnition 15 (L[CCL]). L[CCL] is L[QCL], except that the #»
×-rules are
replaced by the following #»
⊙-rules:
Γ, (A)1, (B)k ⊢Δ
#»
⊙l1
Γ, (A#»
⊙B)k ⊢Δ
Γ, (A)l, (¬B)1 ⊢Δ
#»
⊙l2
Γ, (A#»
⊙B)optCCL(B)+l ⊢Δ
Γ, (A)m ⊢Δ
#»
⊙l3
Γ, (A#»
⊙B)optCCL(B)+m ⊢Δ
Γ ⊢(A)1, Δ
Γ ⊢(B)k, Δ
#»
⊙r1
Γ ⊢(A#»
⊙B)k, Δ
Γ ⊢(A)l, Δ
Γ ⊢(¬B)1, Δ
#»
⊙r2
Γ ⊢(A#»
⊙B)optCCL(B)+l, Δ
Γ ⊢(A)m, Δ
#»
⊙r3
Γ ⊢(A#»
⊙B)optCCL(B)+m, Δ
where k ≤optCCL(B), l ≤optCCL(A), and 1 < m ≤optCCL(A).
Note that, given Γ, (A#»
⊙B)optCCL(B)+m ⊢Δ with 1 < m ≤optCCL(A), we need
to guess whether #»
⊙l2 or #»
⊙l3 has to be applied. We do not deﬁne L[CCL]−here,
but the necessary rules for #»
⊙can be inferred from the #»
⊙-rules of L[CCL] in a
similar way to how L[QCL]−was derived from L[QCL].
Proposition 7. L[CCL] is sound.
Proof. We consider the newly introduced rules.
– For #»
⊙l1, #»
⊙l2, and #»
⊙l3 this follows directly from the deﬁnition of CCL.
– (#»
⊙r1). Assume both premises are valid, i.e., every model of Γ is a model of Δ
or of (A)1 and (B)k with k ≤optL(B). By deﬁnition, any model that satisﬁes
(A)1 and (B)k satisﬁes A#»
⊙B to degree k. Thus, every model of Γ is a model
of Δ or of (A#»
⊙B)k, which means the conclusion of the rule is valid.
– (#»
⊙r2). Assume both premises are valid, i.e., every model of Γ is either a
model of Δ or of (A)l and (¬B)1 with l ≤optCCL(A). By deﬁnition, any
model that satisﬁes (A)l and does not satisfy B (and hence satisﬁes (¬B)1)
satisﬁes A#»
⊙B to degree optCCL(B) + l.
– (#»
⊙r3). Assume that the premise is valid, i.e., every model of Γ is either
a model of Δ or of (A)m with 1 < m ≤optCCL(A). By deﬁnition, any
model that satisﬁes (A)m, regardless of what degree this model ascribes to
B, satisﬁes A#»
⊙B to degree optCCL(B) + m.
⊓⊔
Proposition 8. L[CCL] is complete.
Proof. We adapt the induction of the proof of Proposition 2:
– Assume that a sequent of the form Γ, (A#»
⊙B)k ⊢Δ is valid, with k ≤optL(B).
All models that satisfy (A#»
⊙B)k must satisfy A to a degree of 1 and B to a
degree of k. Thus, Γ, (A)1, (B)k ⊢Δ is valid, and, by the induction hypothesis,
Γ, (A#»
⊙B)k ⊢Δ is provable. Similarly for the cases Γ, (A#»
⊙B)optCCL(B)+l ⊢Δ
with l ≤optCCL(A), and Γ, (A#»
⊙B)optCCL(B)+m ⊢Δ with 1 < m ≤
optCCL(A).

Sequent Calculi for Choice Logics
347
– Assume that a sequent of the form Γ ⊢(A#»
⊙B)k, Δ is valid, with k ≤optL(B).
We claim that then Γ ⊢(A)1, Δ and Γ ⊢(B)k, Δ are valid. Assume, for the
sake of a contradiction, that the ﬁrst sequent is not valid. This means that
there is a model M of Γ that is neither a model of (A)1 nor of Δ. However,
then M satisﬁes A#»
⊙B to a degree higher than optCCL(B). This contradicts
the assumption that Γ ⊢(A#»
⊙B)k, Δ is valid. Assume now that the second
sequent is not valid, i.e., that there is a model M of Γ that is neither a model
of (B)k nor of Δ. Then M cannot be a model of (A#»
⊙B)k, contradicting
the assumption. As before, it follows by the induction hypothesis that Γ ⊢
(A#»
⊙B)k, Δ is provable. Similarly for the cases Γ ⊢(A#»
⊙B)optCCL(B)+l, Δ with
l ≤optCCL(A), and Γ ⊢(A#»
⊙B)optCCL(B)+m, Δ with 1 < m ≤optCCL(A). ⊓⊔
We are conﬁdent that our methods can be adapted not only for QCL and CCL,
but for numerous other instantiations of the choice logic framework deﬁned in
Sect. 2. We mention here lexicographic choice logic (LCL) [4], in which A#»⋄B
expresses that it is best to satisfy A and B, second best to satisfy only A, third
best to satisfy only B, and unacceptable to satisfy neither.
Moreover, note that the inference rules |∼lex and |∼unsat (cf. Deﬁnition 14) do
not depend on any speciﬁc choice logic. Thus, once labeled calculi are developed
for a choice logic, a calculus for preferred model entailment follows immediately.
6
Conclusion
In this paper we introduce a sound and complete sequent calculus for preferred
model entailment in QCL. This non-monotonic calculus is built on two calculi:
a monotonic labeled sequent calculus and a corresponding refutation calculus.
Our systems are modular and can easily be adapted: on the one hand, calculi
for choice logics other than QCL can be obtained by introducing suitable rules for
the choice connectives of the new logic, as exempliﬁed with our calculus for CCL;
on the other hand, a non-monotonic calculus for preferred model semantics other
than the lexicographic semantics can be obtained by adapting the inference rule
|∼lex which transitions from preferred model entailment to the labeled calculi.
Our work contributes to the line of research on non-monotonic sequent calculi
that make use of refutation systems [5]. Our system is the ﬁrst proof calculus
for choice logics, which have been studied mainly from the viewpoint of their
computational properties [4] and their potential applications [3,8,13] so far.
Regarding future work, we aim to investigate the proof complexity of our
calculi, and how this complexity might depend on which choice logic or preferred
model semantics is considered. Also, calculi for other choice logics such as LCL
could be explicitly deﬁned, as was done with CCL in Sect. 5.
Acknowledgments. We thank the anonymous reviewers for their valuable feedback.
This work was funded by the Austrian Science Fund (FWF) under the grants Y698
and J4581.

348
M. Bernreiter et al.
References
1. Avron, A.: The method of hypersequents in the proof theory of propositional non-
classical logics. In: Hodges, W., Hyland, M., Steinhorn, C., Truss, J. (eds.) Logic:
From Foundations to Applications, pp. 1–32. Oxford Science Publications, Oxford
(1996)
2. Baaz, M., Lahav, O., Zamansky, A.: Finite-valued semantics for canonical labelled
calculi. J. Autom. Reason. 51(4), 401–430 (2013)
3. Benferhat, S., Sedki, K.: Alert correlation based on a logical handling of admin-
istrator preferences and knowledge. In: SECRYPT, pp. 50–56. INSTICC Press
(2008)
4. Bernreiter, M., Maly, J., Woltran, S.: Choice logics and their computational prop-
erties. In: IJCAI, pp. 1794–1800. ijcai.org (2021)
5. Bonatti, P.A., Olivetti, N.: Sequent calculi for propositional nonmonotonic logics.
ACM Trans. Comput. Log. 3(2), 226–278 (2002)
6. Boudjelida, A., Benferhat, S.: Conjunctive choice logic. In: ISAIM (2016)
7. Brewka, G., Benferhat, S., Berre, D.L.: Qualitative choice logic. Artif. Intell.
157(1–2), 203–237 (2004)
8. Brewka, G., Niemel¨a, I., Syrj¨anen, T.: Logic programs with ordered disjunction.
Comput. Intell. 20(2), 335–357 (2004)
9. Carnielli, W.A.: Systematization of ﬁnite many-valued logics through the method
of tableaux. J. Symb. Log. 52(2), 473–493 (1987)
10. Geibinger, T., Tompits, H.: Sequent-type calculi for systems of nonmonotonic para-
consistent logics. In: ICLP Technical Communications. EPTCS, vol. 325, pp. 178–
191 (2020)
11. Governatori, G., Rotolo, A.: Logic of violations: a Gentzen system for reasoning
with contrary-to-duty obligations. Australas. J. Logic 4, 193–215 (2006)
12. Kaminski, M., Francez, N.: Calculi for many-valued logics. Log. Univers. 15(2),
193–226 (2021)
13. Li´etard, L., Hadjali, A., Rocacher, D.: Towards a gradual QCL model for database
querying. In: Laurent, A., Strauss, O., Bouchon-Meunier, B., Yager, R.R. (eds.)
IPMU 2014. CCIS, vol. 444, pp. 130–139. Springer, Cham (2014). https://doi.org/
10.1007/978-3-319-08852-5 14
14. McCarthy, J.: Circumscription - a form of non-monotonic reasoning. Artif. Intell.
13(1–2), 27–39 (1980)
15. Moore, R.C.: Semantical considerations on nonmonotonic logic. Artif. Intell. 25(1),
75–94 (1985)
16. Reiter, R.: A logic for default reasoning. Artif. Intell. 13(1–2), 81–132 (1980)
17. Tiomkin, M.L.: Proving unprovability. In: LICS, pp. 22–26. IEEE Computer Soci-
ety (1988)

Sequent Calculi for Choice Logics
349
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Lash 1.0 (System Description)
Chad E. Brown1 and Cezary Kaliszyk2(B)
1 Czech Technical University in Prague, Prague, Czech Republic
2 University of Innsbruck, Innsbruck, Austria
cezary.kaliszyk@uibk.ac.at
Abstract. Lash is a higher-order automated theorem prover created as
a fork of the theorem prover Satallax. The basic underlying calculus of
Satallax is a ground tableau calculus whose rules only use shallow infor-
mation about the terms and formulas taking part in the rule. Lash uses
new, eﬃcient C representations of vital structures and operations. Most
importantly, Lash uses a C representation of (normal) terms with per-
fect sharing along with a C implementation of normalizing substitutions.
We describe the ways in which Lash diﬀers from Satallax and the perfor-
mance improvement of Lash over Satallax when used with analogous ﬂag
settings. With a 10 s timeout Lash outperforms Satallax on a collection
TH0 problems from the TPTP. We conclude with ideas for continuing
the development of Lash.
Keywords: Higher-order logic · Automated reasoning · TPTP
1
Introduction
Satallax [4,7] is an automated theorem prover for higher-order logic that was a
top competitor in the THF division of CASC [10] for most of the 2010s. The basic
calculus of Satallax is a complete ground tableau calculus [2,5,6]. In recent years
the top systems of the THF division of CASC are primarily based on resolution
and superposition [3,8,11]. At the moment it is an open question whether there
is a research and development path via which a tableau based prover could again
become competitive. As a ﬁrst step towards answering this question we have cre-
ated a fork of Satallax, called Lash, focused on giving eﬃcient C implementations
of data structures and operations needed for search in the basic calculus.
Satallax was partly competitive due to (optional) additions that went beyond
the basic calculus. Three of the most successful additions were the use of higher-
order pattern clauses during search, the use of higher-order uniﬁcation as a
heuristic to suggest instantiations at function types and the use of the ﬁrst-
order theorem prover E as a backend to try to prove the ﬁrst-order part of the
current state is already unsatisﬁable. Satallax includes ﬂags that can be used to
activate or deactivate such additions so that search only uses the basic calculus.
They are deactivated by default. Satallax has three representations of terms in
Ocaml. The basic calculus rules use the primary representation. Higher-order
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 350–358, 2022.
https://doi.org/10.1007/978-3-031-10769-6_21

Lash 1.0 (System Description)
351
uniﬁcation and pattern clauses make use of a representation that includes a
case for metavariables to be instantiated. Communication with E uses a third
representation restricted to ﬁrst-order terms and formulas. When only the basic
calculus is used, only the primary representation is needed.
Assuming only the basic calculus is used only limited information about
(normal) terms is needed during the search. Typically we only need to know the
outer structure of the principal formulas of each rule, and so the full term does
not need to be traversed. In some cases Satallax either implicitly or explicitly
traverses the term. The implicit cases are when a rule needs to know if two
terms are equal. In Satallax, Ocaml’s equality is used to test for equality of
terms, implicitly relying on a recursion over the term. The explicit cases are
quantiﬁer rules that instantiate with either a term or a fresh constant. In the
former case we may also need to normalize the result after instantiating with a
term.
In order to give an optimized implementation of the basic calculus we have
created a new theorem prover, Lash1, by forking a recent version of Satallax
(Satallax 3.4), the last version that won the THF division of CASC (in 2019).
Generally speaking, we have removed all the additional code that goes beyond
the basic calculus. In particular we do not need terms with metavariables since we
support neither pattern clauses nor higher-order uniﬁcation in Lash. Likewise we
do not need a special representation for ﬁrst-order terms and formulas since Lash
does not communicate with E. We have added eﬃcient C implementations of
(normal) terms with perfect sharing. Additionally we have added new eﬃcient C
implementations of priority queues and the association of formulas with integers
(to communicate with MiniSat). To measure the speedup given by the new parts
of the implementation we have run Satallax 3.4 using ﬂag settings that only
use the basic calculus and Lash 1.0 using the same ﬂag settings. We have also
compared Lash to Satallax 3.4 using Satallax’s default strategy with a timeout of
10 s, and have found that Lash 1.0 outperforms Satallax with this short timeout
even when Satallax is using the optional additions (including calling E). We
describe the changes and present a number of examples for which the changes
lead to a signiﬁcant speedup.
2
Preliminaries
We will presume a familiarity with simple type theory and only give a quick
description to make our use of notation clear, largely following [6]. We assume a
set of base types, one of which is the type o of propositions (also called booleans),
and the rest we refer to as sorts. We use α, β to range over sorts and σ, τ to range
over types. The only types other than base types are function types στ, which
can be thought of as the type of functions from σ to τ.
All terms have a unique type and are inductively deﬁned as (typed) variables,
(typed) constants, well-typed applications (t s) and λ-abstractions (λx.t). We
1 Lash 1.0 along with accompanying material is available at http://grid01.ciirc.cvut.
cz/∼chad/ijcar2022lash/.

352
C. E. Brown and C. Kaliszyk
also include the logical constant ⊥as a term of type o, terms (of type o) of the
form (s ⇒t) (implications) and (∀x.t) (universal quantiﬁers) where s, t have
type o and terms (of type o) of the form (s =σ t) where s, t have a common
type σ. We also include choice constants εσ of (σo)σ at each type σ. We write
¬t for t ⇒⊥and (s ̸=σ t) for (s =σ t ⇒⊥). We omit type parentheses and type
annotations except where they are needed for clarity. Terms of type o are also
called propositions. We also use ⊤, ∨, ∧, ∃with the understanding that these are
notations for equivalent propositions in the set of terms above.
We assume terms are equal if they are the same up to α-conversion of bound
variables (using de Bruijn indices in the implementation). We write [s] for the
βη-normal form of s.
The tableau calculi of [6] (without choice) and [2] (with choice) deﬁne when
a branch is refutable. A branch is a ﬁnite set of normal propositions. We let A
range over branches and write A, s for the branch A∪{s}. We will not give a full
calculus, but will instead discuss a few of the rules with surprising properties.
Before doing so we emphasize rules that are not in the calculus. There is no
cut rule stating that if A, s and A, ¬s are refutable, then A is refutable. (During
search such a rule would require synthesizing the cut formula s.) There is also no
rule stating that if the branch A, (s = t), [ps], [pt] is refutable, then A, (s = t), [ps]
is refutable (where s, t have type σ and p is a term of type σo). That is, there is
no rule for rewriting into arbitrarily deep positions using equations.
All the tableau rules only need to examine the outer structure to test if they
apply (when searching backwards for a refutation). When applying the rule,
new formulas are constructed and added to the branch (or potentially multiple
branches, each a subgoal to be refuted). An example is the confrontation rule,
the only rule involving positive equations. The confrontation rule states that if
s =α t and u ̸=α v are on a branch A (where α is a sort), then we can refute
A by refuting A, s ̸= u, t ̸= u and A, s ̸= v, t ̸= v. A similar rule is the mating
rule, which states that if ps1 . . . sn and ¬pt1 . . . tn are on a branch A (where
p is a constant of type σ1 · · · σno), then we can refute A by refuting each of
the branches A, si ̸= ti for each i ∈{1, . . . , n}. The mating rule demonstrates
how disequations can appear on a branch even if the original branch to refute
contained no reference to equality at all. One way a branch can be closed is if
s ̸= s is on the branch. In an implementation, this means an equality check is
done for s and t whenever a disequation s ̸= t is added to the branch. In Satallax
this requires Ocaml to traverse the terms. In Lash this only requires comparing
the unique integer ids the implementation assigns to the terms.
The disequations generated on a branch play an important role. Terms (of
sort α) occuring on one side of a disequation on a branch are called discrimi-
nating terms. The rule for instantiating a quantiﬁed formula ∀x.t (where x has
sort α) is restricted to instantiating with discriminating terms (or a default term
if no terms of sort α are discriminating). During search in Satallax this means
there is a ﬁnite set of permitted instantiations (at sort α) and this set grows as
disequations are produced. Note that, unlike most automated theorem provers,
the instantiations do not arise from uniﬁcation. In Satallax (and Lash) when

Lash 1.0 (System Description)
353
∀x.t is being processed it is instantiated with all previously processed instanti-
ations. When a new instantiation is produced, previously processed universally
quantiﬁed propositions are instantiated with it. When ∀x.t is instantiated with
s, then [(λx.t)s] is added to the branch. Such an instantiation is the important
case where the new formula involves term traversals: both for substitution and
normalization. In Satallax the substitution and normalization require multiple
term traversals. In Lash we have used normalizing substitutions and memorized
previous computations, minimizing the number of term traversals. The need
to instantiate arises when processing either a universally quantiﬁed proposition
(giving a new quantiﬁer to instantiate) or a disequation at a sort (giving new
discriminating terms).
We discuss a small example both Satallax and Lash can easily prove. We
brieﬂy describe what both do in order to give the ﬂavor of the procedure and
(hopefully) prevent readers from assuming the provers behave too similarly from
readers based on other calculi (e.g., resolution).
Example SEV241^5 from TPTP v7.5.0 [9] (X5201A from Tps [1]) contains a
minor amount of features going beyond ﬁrst-order logic. The statement to prove
is
∀x.U x ∧W x ⇒∀S.(S = U ∨S = W) ⇒Sx.
Here U and W are constants of type αo, x is a variable of type α and S is a
variable of type αo. The higher-order aspects of this problem are the quantiﬁer
for S (though this could be circumvented by making S a constant like U and W)
and the equations between predicates (though these could be circumvented by
replacing S = U by ∀y.Sy ⇔Uy and replacing S = W similarly). The tableau
rules eﬀectively do both during search.
Satallax never clausiﬁes. The formula above is negated and assumed. We will
informally describe tableau rules as splitting the problem into subgoals, though
this is technically mediated through MiniSat (where the set of MiniSat clauses
is unsatisﬁable when all branches are closed). Tableau rules are applied until
the problem involves a constant c (for x), a constant S′ for S and assumptions
U c, W c, S′ = U ∨S′ = W and ¬S′c on the branch. The disjunction is
internally S′ ̸= U ⇒S′ = W and the implication rule splits the problem into
two branches, one with S′ = U and one with S′ = W. Both branches are solved
in analogous ways and we only describe the S′ = U branch. Since S′ = U is an
equation at function type, the relevant rule adds ∀y.S′y = Uy to the branch.
Since there are no disequations on the branch, there is no instantiation available
for ∀y.S′y = Uy. In such a case, a default instantiation is created and used. That
is, a default constant d (of sort α) is generated and we instantiate with this d,
giving S′d =o Ud. The rule for equations at type o splits into two subgoals: one
branch with S′d and Ud and another with ¬S′d and ¬Ud. On the ﬁrst branch
we mate S′d with ¬S′c adding the disequation d ̸= c to the branch. This makes c
available as an instantiation for ∀y.S′y = Uy. After instantiating with c the rest
of the subcase is straightforward. In the other subgoal we mate U c with ¬Ud
giving the disequation c ̸= d. Again, c becomes available as an instantiation and
the rest of the subcase is straightforward.

354
C. E. Brown and C. Kaliszyk
3
Terms with Perfect Sharing
Lash represents normal terms as C structures, with a unique integer id assigned
to each term. The structure contains a tag indicating which kind of term is
represented, a number that is used to either indicate the de Bruijn index (for
a variable), the name (for a constant), or the type (for a λ-abstraction, a uni-
versal quantiﬁer, a choice operator, or an equation). Two pointers (optionally)
point to relevant subterms in each case. In addition the structure maintains
the information of which de Bruijn indices are free in the term (with de Bruijn
indices limited to a maximum of 255). Knowing the free de Bruijn indices of
terms makes recognizing potential η-redexes possible without traversing the λ-
abstraction. Likewise it is possible to determine when shifting and substitution
of de Bruijn indices would not aﬀect a term, avoiding the need to traverse the
term.
In Ocaml only the unique integer id is directly revealed and this is suﬃcient
to test for equality of terms. Hash tables are used to uniquely assign types
to integers and strings (for names) to integers and these integers are used to
interface with the C code. Various functions are used in the Ocaml-C interface to
request the construction of (normal) terms. For example, given the two Ocaml
integer ids i and j corresponding to terms s and t, the function mk norm ap
given i and j will return an integer k corresponding to the normal term [s t].
The C implementation recognizes if s is a λ-abstraction and performs all βη-
reductions to obtain a normal term. Additionally, the C implementation treats
terms as graphs with perfect sharing, and additionally caches previous operations
(including substitutions and de Bruijn shifting) to prevent recomputation.
In addition to the low-level C term reimplementation, we have also provided a
number of other low-level functionalities replacing the slower parts of the Ocaml
code. This includes low-level priority queues, as well as C code used to associate
the integers representing normal propositions with integers that are used to
communicate with MiniSat. The MiniSat integers are nonzero and satisfy the
property that minus on integers corresponds to negation of propositions.
4
Results and Examples
The ﬁrst mode in the default schedule for Satallax 3.1 is mode213. This mode
activates one feature that goes beyond the basic calculus: pattern clauses. Addi-
tionally the mode sets a ﬂag that tries to split the initial goal into several indepen-
dent subgoals before beginning the search proper. Through experimentation we
have found that setting a ﬂag (common to both Satallax and Lash) to essentially
prevent MiniSat from searching (i.e., only using MiniSat to recognize contradic-
tions that are evident without search) often improves the performance. We have
created a modiﬁed mode mode213d that deactivates these additions (and delays
the use of MiniSat) so that Satallax and Lash will have a similar (and often the
same) search space. (Sometimes the search spaces diﬀer due to diﬀerences in the
way Satallax and Lash enumerate instantiations for function types, an issue we

Lash 1.0 (System Description)
355
Table 1. Lash vs. Satallax on 2053 TH0 Problems.
Prover
Problems Solved
Lash
1501 (73%)
Satallax (with E)
1487 (72%)
Satallax (without E)
1445 (70%)
Satallax (Lash Schedule)
1412 (69%)
will not focus on here.) We have also run Lash with many variants of Satallax
modes with similar modiﬁcations. From such test runs we have created a 10 s
schedule consisting of 5 modes.
To give a general comparison of Satallax and Lash we have run both on 2053
TH0 problems from a recent release of the TPTP [9] (7.5.0). We initially selected
all problems with TPTP status of Theorem or Unsatisﬁable (so they should be
provable in principle) without polymorphism (or similar extensions of TH0). We
additionally removed a few problems that could not be parsed by Satallax 3.4
and removed a few hundred problems big enough to activate SINE in Satallax
3.4.
We ran Lash for 10 s with its default schedule over this problem set. For
comparison, we have run Satallax 3.4 for 10 s in three diﬀerent ways: using the
Lash schedule (since the ﬂag settings make sense for both systems) and using
Satallax 3.4’s default schedule both with and without access to E [12]. The
results are reported in Table 1. It is already promising that Lash has the ability
to slightly outperform Satallax even when Satallax is allowed to call E.
To get a clearer view of the improvement we discuss a few speciﬁc examples.
TPTP problem NUM638^1 (part of Theorem 3 from the Automath formal-
ization of Landau’s book) is about the natural numbers (starting from 1). The
problem assumes a successor function s is injective and that every number other
than 1 has a predecessor. An abstract notion of existence is used by having
a constant some of type (ιo)o about which no extra assumptions are made,
so the assumption is formally ∀x.x ̸= 1 ⇒some(λu.x = su). For a ﬁxed n,
n ̸= 1 is assumed and the conjecture to prove is the negation of the implication
(∀xy.n = sx ⇒n = sy ⇒x = y) ⇒¬(some(λu.n = su)). The implication is
assumed and the search must rule out the negation of the antecedent (i.e., that
n has two predecessors) and the succedent (that n has no predecessor). Satallax
and Lash both take 3911 steps to prove this example. With mode213d, Lash
completes the search in 0.4 s while Satallax requires almost 29 s.
TPTP problem SEV108^5 (SIX_THEOREM from Tps [1]) corresponds to prov-
ing the Ramsey number R(3,3) is at most 6. The problem assumes there is a
symmetric binary relation R (the edge relation of a graph with the sort as ver-
tices) and there are (at least) 6 distinct elements. The conclusion is that there
are either 3 distinct elements all of which are R-related or 3 distinct elements
none of which are R-related. Satallax and Lash can solve the problem in 14129

356
C. E. Brown and C. Kaliszyk
steps with mode mode213d. Satallax proves the theorem in 0.153 s while Lash
proves the theorem in the same number of steps but in 0.046 s.
The diﬀerence is more impressive if we consider the modiﬁed problem of
proving R(3, 4) is at most 9. That is, we assume there are (at least) 9 distinct
elements and modify the second disjunct of the conclusion to be that there are
4 distinct elements none of which are R-related. Satallax and Lash both use
186127 steps to ﬁnd the proof. For Satallax this takes 44 s while for Lash this
takes 5.5 s.
The TPTP problem SYO506^1 is about an if-then-else operator. The problem
has a constant c of type oιιι. Instead of giving axioms indicating c behaves as
an if-then-else operator, the conjecture is given as a disjunction:
(∀xy.c (x = y) x y = y) ∨¬(∀xy.c ⊤x y = x) ∨¬(∀xy.c ⊥x y = y).
After negating the conjecture and applying the ﬁrst few tableau rules the branch
will contain the propositions ∀xy.c ⊤x y
=
x, ∀xy.c ⊥x y
=
y and the
disequation c (d = e) d e ̸= e for fresh d and e of type ι. In principle the rules
for if-then-else given in [2] could be used to solve the problem without using
the universally quantiﬁed formulas (other than to justify that c is an if-then-
else operator). However, these are not implemented in Satallax or Lash. Instead
search proceeds as usual via the basic underlying procedure. Both Satallax and
Lash can prove the example using modes mode0c1 in 32704 steps. Satallax
performs the search in 9.8 s while Lash completes the search in 0.2 s.
In addition to the examples considered above, we have constructed a family of
examples intended to demonstrate the power of the shared term representation
and caching of operations. Let cons have type ιιι and nil have type ι. For each
natural number n, consider the proposition Cn given by
n (λx.cons x x) (cons nil nil) = cons (n (λx.cons x x) nil) (n (λx.cons x x) nil)
where n is the appropriately typed Church numeral. Proving the proposition
Cn does not require any search and merely requires the prover to normalize
the conjecture and note the two sides have the same normal form. However, this
normal form on both sides will be a complete binary tree of depth n+1. We have
run Lash and Satallax on Cn with n ∈{20, 21, 22, 23, 24} using mode mode213d.
Lash solves all ﬁve problems in the same amount of time, less than 0.02 s for
each. Satallax takes 4 s, 8 s, 16 s, 32 s and 64 s. As expected, since Satallax is
not using a shared representation, the computation time exponentially increases
with respect to n.
5
Conclusion and Future Work
We have used Lash as a vehicle to demonstrate that giving a more eﬃcient imple-
mentation of the underlying tableau calculus of Satallax can lead to signiﬁcant
performance improvements. An obvious possible extension of Lash would be to
implement pattern clauses, higher-order uniﬁcation and the ability to call E.

Lash 1.0 (System Description)
357
While we may do this, our current plans are to focus on directions that further
diverge from the development path followed by Satallax.
Interesting theoretical work would be to modify the underlying calculus
(while maintaining completeness). For example the rules of the calculus might
be able to be further restricted based on orderings of ground terms. On the other
hand, new rules might be added to support a variety of constants with special
properties. This was already done for constants that satisfy axioms indicating
the constant is a choice, description or if-then-else operator [2]. Suppose a con-
stant r of type ιιo is known to be reﬂexive due to a formula ∀x.r x x being
on the branch. One could avoid ever instantiating this universally quantiﬁed
formula by simply including a tableau rule that extends a branch with s ̸= t
whenever ¬r s t is on the branch. Similar rules could operationalize other spe-
cial cases of universally quantiﬁed formulas, e.g., formulas giving symmetry or
transitivity of a relation. A modiﬁcation of the usual completeness proof would
be required to prove completeness of the calculus with these additional rules
(and with the restriction disallowing instantiating the corresponding universally
quantiﬁed formulas).
Finally the C representation of terms could be extended to include precom-
puted special features. Just as the current implementation knows which de Brui-
jns are free in the term (without traversing the term), a future implementation
could know other features of the term without requiring traversal. Such features
could be used to guide the search.
Acknowledgements. The results were supported by the Ministry of Education,
Youth and Sports within the dedicated program ERC CZ under the project POSTMAN
no. LL1902 and the ERC starting grant no. 714034 SMART.
References
1. Andrews, P.B., Bishop, M., Issar, S., Nesmith, D., Pfenning, F., Xi, H.: TPS: a
theorem-proving system for classical type theory. J. Autom. Reason. 16(3), 321–
353 (1996). https://doi.org/10.1007/BF00252180
2. Backes, J., Brown, C.E.: Analytic tableaux for higher-order logic with choice.
J. Autom. Reason. 47(4), 451–479 (2011). https://doi.org/10.1007/s10817-011-
9233-2
3. Bhayat, A., Reger, G.: A combinator-based superposition calculus for higher-order
logic. In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020. LNCS (LNAI),
vol. 12166, pp. 278–296. Springer, Cham (2020). https://doi.org/10.1007/978-3-
030-51074-9 16
4. Brown, C.E.: Satallax: an automatic higher-order prover. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 111–117. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 11
5. Brown, C.E., Smolka, G.: Extended ﬁrst-order logic. In: Berghofer, S., Nipkow,
T., Urban, C., Wenzel, M. (eds.) TPHOLs 2009. LNCS, vol. 5674, pp. 164–179.
Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-03359-9 13
6. Brown, C.E., Smolka, G.: Analytic tableaux for simple type theory and its ﬁrst-
order fragment. Logical Methods Comput. Sci. 6(2) (2010). https://doi.org/10.
2168/LMCS-6(2:3)2010

358
C. E. Brown and C. Kaliszyk
7. F¨arber, M., Brown, C.: Internal guidance for Satallax. In: Olivetti, N., Tiwari, A.
(eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 349–361. Springer, Cham (2016).
https://doi.org/10.1007/978-3-319-40229-1 24
8. Steen, A., Benzm¨uller, C.: Extensional higher-order paramodulation in Leo-III.
J. Autom. Reason. 65(6), 775–807 (2021). https://doi.org/10.1007/s10817-021-
09588-x
9. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. From CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
10. Sutcliﬀe, G.: The 10th IJCAR automated theorem proving system competition
- CASC-J10. AI Commun. 34(2), 163–177 (2021). https://doi.org/10.3233/AIC-
201566
11. Vukmirovi´c, P., Bentkamp, A., Blanchette, J., Cruanes, S., Nummelin, V., Tour-
ret, S.: Making higher-order superposition work. In: Platzer, A., Sutcliﬀe, G. (eds.)
CADE 2021. LNCS (LNAI), vol. 12699, pp. 415–432. Springer, Cham (2021).
https://doi.org/10.1007/978-3-030-79876-5 24
12. Vukmirovi´c, P., Blanchette, J.C., Cruanes, S., Schulz, S.: Extending a Brainiac
prover to lambda-free higher-order logic. In: Vojnar, T., Zhang, L. (eds.) TACAS
2019. LNCS, vol. 11427, pp. 192–210. Springer, Cham (2019). https://doi.org/10.
1007/978-3-030-17462-0 11
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Go´eland: A Concurrent Tableau-Based
Theorem Prover (System Description)
Julie Cailler
, Johann Rosain
, David Delahaye
, Simon Robillard(B)
,
and Hinde Lilia Bouziane
LIRMM, Univ Montpellier, CNRS, Montpellier, France
{julie.cailler,johann.rosain,david.delahaye,simon.robillard,
hinde.bouziane}@lirmm.fr
Abstract. We describe Go´eland, an automated theorem prover for ﬁrst-
order logic that relies on a concurrent search procedure to ﬁnd tableau
proofs, with concurrent processes corresponding to individual branches of
the tableau. Since branch closure may require instantiating free variables
shared across branches, processes communicate via channels to exchange
information about substitutions used for closure. We present the proof
search procedure and its implementation, as well as experimental results
obtained on problems from the TPTP library.
Keywords: Automated Theorem Proving · Tableaux · Concurrency
1
Introduction
Although clausal proof techniques have enjoyed success in automated theo-
rem proving, some applications beneﬁt from reasoning on unaltered formulas
(rather than Skolemized clauses), while others require the production of proofs
in a sequent calculus. These roles are fulﬁlled by provers based on the tableau
method [17], as initially designed by Beth and Hintikka [2,13]. For ﬁrst-order
logic, eﬃcient handling of universal formulas is typically achieved with free vari-
ables that are instantiated only when needed to close a branch. This step is said
to be destructive because it may aﬀect open branches sharing variables. This
causes fairness (and consequently, completeness) issues, as illustrated in Fig. 1.
In this example, exploring the left branch produces a substitution that prevents
direct closure of the right branch. Reintroducing the original quantiﬁed formula
with a diﬀerent free variable is not suﬃcient to close the right branch, because an
applicable δ-rule creates a new Skolem symbol that will result in a diﬀerent but
equally problematic substitution every time a left branch is explored. Thus, sys-
tematically exploring the left branch before the right leads to non-termination of
the search. Conversely, exploring the right branch ﬁrst produces a substitution
(which instantiates the free variable X with a rather than b) that closes both
branches.
Concurrent computing oﬀers a way to implement a proof search procedure
that explores branches simultaneously. Such a procedure can compare closing
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 359–368, 2022.
https://doi.org/10.1007/978-3-031-10769-6_22

360
J. Cailler et al.
Fig. 1. Incompleteness caused by unfair selection of branches
substitutions to detect (dis)agreements between branches, and consequently
either close branches early, or restart proof attempts with limited backtrack-
ing. The simultaneous exploration of branches is handled by the concurrency
system, either by interleaving computations through scheduling, or by execut-
ing tasks in parallel if the hardware resources allow it. A concurrent procedure
naturally lends itself to parallel execution, allowing us to take advantage of
multi-core architectures for eﬃcient ﬁrst-order theorem proving. Thus, concur-
rency provides an elegant and eﬃcient solution to proof search with free variable
tableaux.
In this paper, we describe a concurrent destructive proof search procedure
for ﬁrst-order analytic tableaux (Sect. 2) and its implementation in a tool called
Go´eland, as well as its evaluation on problems from the TPTP library [19] and
comparison to other state-of-the-art provers (Sect. 3).
Related Work. A lot of research has been carried out on the parallelization of
proof search procedures [4], often focusing primarily on parallel execution and
performance. In contrast, we use concurrency not only as a way to take advan-
tage of multi-core architectures, but also as an algorithmic device that is useful
even for sequential execution (with interleaved threads). Some concurrent and
parallel approaches focus more distinctly on the exploration of the search space,
either by dividing the search space between processes (distributed search) or by
using processes with diﬀerent search plans on the same space (multi search) [3].
These approaches can be performed either by heterogeneous systems that rely on
cooperation between systems with diﬀerent inference systems [1,8,12], or homo-
geneous systems where all deductive processes use the same inference system.
According to this classiﬁcation, the technique presented here is a homogeneous
system that performs a distributed search. Concurrent tableaux provers include
the model-elimination provers CPTheo [12] and Partheo [18], and the higher-
order prover Hot [15], which notably uses concurrency to deal with fairness issues
arising from the non-terminating nature of higher-order uniﬁcation. Lastly, con-
currency has been used as the basis of a generic framework to present various
proof strategies [10] or allow distributed calculations over a network [21].

Go´eland: A Concurrent Tableau-Based Theorem Prover
361
2
Concurrent Proof Search
Free Variable Tableaux. Go´eland attempts to build a refutation proof for a ﬁrst-
order formula, i.e., a closed tableau for its negation, using a standard free-variable
tableau calculus [11]. The calculus is composed of α-, γ- and δ-rules that extend
a branch with one formula, β-rules that divide a branch by extending it with
two formulas, and a ⊙-rule that closes a branch. γ-rules deal with universally-
quantiﬁed formulas by introducing a formula with a free variable. A free variable
is not universally quantiﬁed, but is instead a placeholder for some term instanti-
ation, typically determined upon branch closure. δ-rules deal with existentially-
quantiﬁed formulas by introducing a formula with a Skolem function symbol
that takes as arguments the free variables in the branch. This ensures freshness
of the Skolem symbol independently of variable instantiation.
The branch closure rule applies to a branch carrying atomic formulas P and
Q such that, for some substitution σ, σ(P) = σ(¬Q). In that case, σ is applied
to all branches. That rule is consequently destructive: applying a substitution
to close one branch may modify another, removing the possibility to close it
immediately. A tableau is closed when all its branches are closed. Closing a
tableau can thus be seen as providing a global uniﬁer that closes all branches.
Semantics for Concurrency. Go´eland relies on a concurrent search procedure. In
order to present this procedure, we use a simple While language augmented
with instructions for concurrency, in the style of CSP [14]. Each process has its
own variable store, as well as a collection of process identiﬁers used for com-
munication: πparent denotes the identiﬁer of a process’s parent, while Πchildren
denotes the collection of identiﬁers of active children of that process. Given a
process identiﬁer π and an expression e, the command π ! e is used to send an
asynchronous message with the value e to the process identiﬁed by π. Conversely,
the command π ? x blocks the execution until the process identiﬁed by π sends a
message, which is stored in the variable x. Lastly, the instruction start creates
a new process that executes a function with some given arguments, while the
instruction kill interrupts the execution of a process according to its identiﬁer.
Proof Search Procedure. The proof search is carried out concurrently by processes
corresponding to branches of the tableau. Processes are started upon application
of a β-rule, one for each new branch. Communications between processes take
two forms: a process may send a set of closing substitutions for its branch to
its parent, or a parent may send a substitution (that closes one of its children’s
branch) to the other children. The proof search is performed by the proofSearch,
waitForParent, and waitForChildren procedures (described in Procedures 1,
2, and 3, respectively).
The proofSearch procedure initiates the proof search for a branch. It ﬁrst
attempts to apply the closure rule. A closing substitution is called local to a
process if its domain includes only free variables introduced by this process or
one of its descendants (i.e., if the variables do not occur higher in the proof tree).
If one of the closing substitutions is local to the process, it is reported and the

362
J. Cailler et al.
Procedure 1: proofSearch
Data: a tableau T
1 begin
2
var Θ ←applyClosingRule(T) ;
3
for θ ∈Θ do
4
if isLocal(θ) then
5
πparent ! {θ}
6
return
7
if Θ ̸= ∅then
8
πparent ! Θ
9
waitForParent(T, Θ)
10
else if applicableAlphaRule(T) then
11
proofSearch(applyAlphaRule(T))
12
else if applicableDeltaRule(T) then
13
proofSearch(applyDeltaRule(T))
14
else if applicableBetaRule(T) then
15
for T ′ ∈applyBetaRule(T) do
16
start proofSearch(T ′)
17
waitForChildren(T, ∅, ∅)
18
else if applicableGammaRule(T) then
19
proofSearch(applyGammaRule(T))
20
else
21
πparent ! ∅
process terminates. If only non-local closing substitutions are found, they are
reported and the process executes waitForParent. Otherwise, the procedure
applies tableau expansion rules according to the priority: α ≺δ ≺β ≺γ.
If a β-rule is applied, new processes are started, and each of them executes
proofSearch on the newly created branch, while the current process executes
waitForChildren.
The waitForParent procedure is executed by a process after it has found
closing non-local substitutions. Such substitutions may prevent closure in other
branches. In these cases, the parent will eventually send another candidate sub-
stitution. waitForParent waits until such a substitution is received, and triggers
a new step of proof search. The process may also be terminated by its parent
(via the kill instruction) during the execution of this procedure, if one of the
substitutions previously sent by the process leads to closing the parent’s branch.
The waitForChildren procedure is executed by a process after the applica-
tion of a β-rule and the creation of child processes. The set of substitutions sent
by each child is stored in a map subst (Line 2), initially undeﬁned everywhere
(f⊥). This procedure closes the branch (Line 13) if there exists a substitution
θ that agrees with one closing substitution of each child process, i.e., for each
child process, the process has reported a substitution σ such that σ(X) = θ(X)
for any variable X in the domain of σ. If no such substitution can be found

Go´eland: A Concurrent Tableau-Based Theorem Prover
363
Procedure 2: waitForParent
Data: a tableau T, a set Θsent of substitutions sent by this process to its parent
1 begin
2
πparent ? σ
3
if σ ∈Θsent then
4
πparent ! σ
5
waitForParent(T, Θsent)
6
else
7
proofSearch(σ(T))
after all the children have closed their branches, then one closing substitution
σ ∈subst is picked arbitrarily (Line 18) and sent to all the children (which are
at that point executing waitForParent) to restart their proof attempts. With
the additional constraint of the substitution σ, the new proof attempts may fail,
hence the necessity for backtracking among candidate substitutions Θbacktrack
(Line 5 and 6). At the end, if all the substitutions were tried and failed, the
process sends a failure message (symbolized by ∅) to its parent.
Thus, concurrency and backtracking are used to prevent incompleteness
resulting from unfair instantiation of free variables. Another potential source
of unfairness is the γ-rule, when applied more than once to a universal formula
(reintroduction). This may be needed to ﬁnd a refutation, but unbounded rein-
troductions would lead to unfairness. Iterative deepening [16] is used to guard
against this: a bound limits the number of reintroductions on any single branch,
and if no proof is found, the bound is increased and the proof search restarted.
Figure 2 illustrates the interactions between processes for the problem in
Fig. 1, and shows how concurrency helps ensure fairness. It describes the par-
ent process, in the top box, and below, the two children processes created upon
application of the β-rule. Dotted lines separate successive states of a process
(i.e., Procedures 1, 2 and 3 seen above), while arrows and boxes represent sub-
stitution exchanges. The number above each arrow indicates the chronology of
the interactions. After both children have returned a substitution (1), the par-
ent arbitrarily chooses one of them, starting with X →b, and sends it to the
children (2). Since this substitution prevents closure in the right branch (3), the
parent later backtracks and sends the other substitution X →a (4), allowing
both children (5) and then the parent to close successfully.
3
Implementation and Experimental Results
Implementation. The procedures presented in Sect. 2 are implemented in the
Go´eland prover1 using the Go language. Go supports concurrency and paral-
lelism, based on lightweight execution threads called goroutines [20]. Goroutines
1 Available at: https://github.com/GoelandProver/Goeland/releases/tag/v1.0.0-beta.

364
J. Cailler et al.
Procedure 3: waitForChildren
Data: a tableau T, a set Θsent of substitutions sent by this process to its
parent, a set Θbacktrack of substitutions used for backtracking
1 begin
2
var subst ←f⊥
3
while ∃π ∈Πchildren. subst[π] = ⊥do
4
π ? subst[π]
5
if subst[π] = ∅then
6
if ∃θ ∈Θbacktrack then
7
for π ∈Πchildren do π ! θ;
8
waitForChildren(T, Θsent, Θbacktrack \ {θ})
9
else
10
for π ∈Πchildren do kill π;
11
πparent ! ∅
12
return
13
if ∃θ, agreement(θ, subst) then
14
πparent ! {θ}
15
for π ∈Πchildren do kill π;
16
waitForParent(T, Θsent ∪{θ})
17
else
18
σ ←choice(subst)
19
for π ∈Πchildren do π ! σ;
20
waitForChildren(T, Θsent, Θbacktrack ∪
π subst[π] \ {σ}))
are executed according to a so-called hybrid threading (or M : N) model: M
goroutines are executed over N eﬀective threads and scheduling is managed by
both the Go runtime and the operating system. This threading model allows
the execution of a large number of goroutines with a reasonable consumption
of system resources. Goroutines use channels to exchange messages, so that the
implementation is close to the presentation of Sect. 2.
Go´eland has, for the time being, no dedicated mechanism for equality rea-
soning. However, we have implemented an extension that implements deduction
modulo theory [9], i.e., transforms axioms into rewrite rules over propositions and
terms. Deduction modulo theory has proved very useful to improve proof search
when integrated into usual automated proof techniques [5], and also produces
excellent results with manually-deﬁned rewrite rules [6,7]. In Go´eland, deduction
modulo theory selects some axioms on the basis of a simple syntactic criterion
and replaces them by rewrite rules.
Experimental Results. We evaluated Go´eland on two problems categories with
FOF theorems in the TPTP library (v7.4.0): syntactic problems without equal-
ity (SYN) and problems of set theory (SET). The former was chosen for its
elementary nature, whereas the latter was picked primarily to evaluate the per-
formance of the deduction modulo theory, as the axioms of set theory are good

Go´eland: A Concurrent Tableau-Based Theorem Prover
365
Fig. 2. Proof search and resulting proof for P(a) ∧¬P(b) ∧∀x.(P(x) ⇔∀y.P(y))
targets for rewriting. We compared the results with those of ﬁve other provers:
tableau-based provers Zenon (v0.8.5), Princess (v2021-05-10) and LeoIII (v1.6),
as well as saturation-based provers E (v2.6) and Vampire (v4.6.1). Experiments
were executed on a computer equipped with an Intel Xeon E5-2680 v4 2.4GHz
2×14-core processor and 128 GB of memory. Each proof attempt was limited to
300 s. Table 1 and Fig. 3 report the results. Table 1 shows the number of problems
solved by each prover, the cumulative time, and the number of problems solved
by a given prover but not by Go´eland (+) and conversely (−). Figure 3 presents
the cumulative time required to solve the number of problems.
As can be observed, the results of Go´eland are comparable to, or slightly
better than those of other tableau-based provers on problems from SYN, while
saturation theorem provers achieve the best results. On this category, the axioms
do not trigger deduction modulo theory rewriting rules, hence the similar results
of Go´eland and Go´eland+DMT. On SET, Go´eland+DMT obtains signiﬁcantly bet-
ter results than other tableau-based provers. This conﬁrms the previous results
on the performance of deduction modulo theory for set theory [6,7].

366
J. Cailler et al.
Table 1. Experimental results over the TPTP library
SYN (263 problems)
SET (464 problems)
Go´eland
199 (190 s)
150 (4659 s)
Go´eland+DMT 199 (196 s)
(+0, −0)
278 (1292 s) (+142, −14)
Zenon
256 (67 s)
(+60, −3)
150 (562 s)
(+75, −75)
Princess
195 (189 s)
(+1, −5)
258 (1168 s) (+141, −33)
LeoIII
195 (268 s)
(+1, −5)
177 (2925 s)
(+77, −50)
E
261 (168 s) (+62, −0) 363 (2377 s) (+223, −10)
Vampire
262 (13 s)
(+63, −0) 321 (4122 s) (+188, −17)
Fig. 3. Cumulative time per problem solved between Go´eland, Go´eland+DMT(GDMT),
Zenon, Princess, LeoIII, E, and Vampire
4
Conclusion
We have presented a concurrent proof search procedure for tableaux in ﬁrst-
order logic with the aim of ensuring a fair exploration of the search space. This
procedure has been implemented in the prover Go´eland. This tool is still in an
early stage, and (with the exception of deduction modulo theory) implements
only the most basic functionalities, yet empirical results are encouraging. We
plan on adding functionalities such as equality reasoning, arithmetic reasoning,
and support for polymorphism to Go´eland, which should increase its usability
and performance. The integration of these functionalities in the context of a
concurrent prover seems to be a promising line of research. Further investigation
is also needed to prove the fairness, and therefore completeness, of our procedure.

Go´eland: A Concurrent Tableau-Based Theorem Prover
367
References
1. Benzm¨uller, C., Kerber, M., Jamnik, M., Sorge, V.: Experiments with an agent-
oriented reasoning system. In: Baader, F., Brewka, G., Eiter, T. (eds.) KI 2001.
LNCS (LNAI), vol. 2174, pp. 409–424. Springer, Heidelberg (2001). https://doi.
org/10.1007/3-540-45422-5 29
2. Beth, E.W.: Formal Methods: An Introduction to Symbolic Logic and to the Study
of Eﬀective Operations in Arithmetic and Logic, Synthese Library, vol. 4. D. Reidel
Pub. Co. (1962)
3. Bonacina, M.P.: A taxonomy of parallel strategies for deduction. Ann. Math. Artif.
Intell. 29(1), 223–257 (2000)
4. Bonacina, M.P.: Parallel theorem proving. In: Hamadi, Y., Sais, L. (eds.) Handbook
of Parallel Constraint Reasoning, pp. 179–235. Springer, Cham (2018). https://doi.
org/10.1007/978-3-319-63516-3 6
5. Burel, G., Bury, G., Cauderlier, R., Delahaye, D., Halmagrand, P., Hermant, O.:
First-order automated reasoning with theories: when deduction modulo theory
meets practice. J. Autom. Reason. 64(6), 1001–1050 (2019). https://doi.org/10.
1007/s10817-019-09533-z
6. Bury, G., Cruanes, S., Delahaye, D., Euvrard, P.-L.: An automation-friendly set
theory for the B method. In: Butler, M., Raschke, A., Hoang, T.S., Reichl, K.
(eds.) ABZ 2018. LNCS, vol. 10817, pp. 409–414. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-91271-4 32
7. Bury, G., Delahaye, D., Doligez, D., Halmagrand, P., Hermant, O.: Automated
deduction in the B set theory using typed proof search and deduction modulo. In:
Fehnker, A., McIver, A., Sutcliﬀe, G., Voronkov, A. (eds.) Logic for Programming,
Artiﬁcial Intelligence and Reasoning (LPAR). EPiC Series in Computing, vol. 35,
pp. 42–58. EasyChair (2015)
8. Denzinger, J., Kronenburg, M., Schulz, S.: DISCOUNT - a distributed and learning
equational prover. J. Autom. Reason. 18(2), 189–198 (1997)
9. Dowek, G., Hardin, T., Kirchner, C.: Theorem proving modulo. J. Autom. Reason.
(JAR) 31(1), 33–72 (2003)
10. Fisher, M.: An open approach to concurrent theorem proving. Parallel Process.
Artif. Intell. 3, 80011 (1997)
11. Fitting, M.: First-Order Logic and Automated Theorem Proving. Springer, Hei-
delberg (1990)
12. Fuchs, M., Wolf, A.: System description: cooperation in model elimination:
CPTHEO. In: Kirchner, C., Kirchner, H. (eds.) CADE 1998. LNCS, vol. 1421,
pp. 42–46. Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0054245
13. Hintikka, J.: Two papers on symbolic logic: form and content in quantiﬁcation the-
ory and reductions in the theory of types. Societas Philosophica, Acta philosophica
Fennica 8, 7–55 (1955)
14. Hoare, C.A.R.: Communicating sequential processes. Commun. ACM 21(8), 666–
677 (1978)
15. Konrad, K.: Hot: a concurrent automated theorem prover based on higher-order
tableaux. In: Grundy, J., Newey, M. (eds.) TPHOLs 1998. LNCS, vol. 1479, pp.
245–261. Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0055140
16. Korf, R.E.: Depth-ﬁrst iterative-deepening: an optimal admissible tree search.
Artif. Intell. 27(1), 97–109 (1985)

368
J. Cailler et al.
17. Letz, R.: First-order tableau methods. In: D’Agostino, M., Gabbay, D.M., H¨ahnle,
R., Posegga, J. (eds.) Handbook of Tableau Methods, pp. 125–196. Springer, Hei-
delberg (1999). https://doi.org/10.1007/978-94-017-1754-0 3. ISBN 978-94-017-
1754-0
18. Schumann, J., Letz, R.: Partheo: a high-performance parallel theorem prover. In:
Stickel, M.E. (ed.) CADE 1990. LNCS, vol. 449, pp. 40–56. Springer, Heidelberg
(1990). https://doi.org/10.1007/3-540-52885-7 78
19. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. From CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. (JAR) 59(4), 483–502 (2017)
20. Tsoukalos, M.: Mastering Go: Create Golang Production Applications Using Net-
work Libraries, Concurrency, Machine Learning, and Advanced Data Structures,
pp. 439–463. Packt Publishing Ltd. (2019)
21. Wu, C.H.: A multi-agent framework for distributed theorem proving. Expert Syst.
Appl. 29(3), 554–565 (2005)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Binary Codes that Do Not Preserve
Primitivity
ˇStˇep´an Holub1(B)
, Martin Raˇska1
, and ˇStˇep´an Starosta2(B)
1 Faculty of Mathematics and Physics, Charles University, Prague, Czech Republic
holub@karlin.mff.cuni.cz
2 Faculty of Information Technology, Czech Technical University in Prague,
Prague, Czech Republic
stepan.starosta@fit.cvut.cz
Abstract. A code X is not primitivity preserving if there is a primitive
list w ∈lists X whose concatenation is imprimitive. We formalize a
full characterization of such codes in the binary case in the proof assis-
tant Isabelle/HOL. Part of the formalization, interesting on its own, is
a description of {x, y}-interpretations of the square xx if |y| ≤|x|. We
also provide a formalized parametric solution of the related equation
xjyk = zℓ.
1
Introduction
Consider two words abba and b. It is possible to concatenate (several copies of)
them as b·abba·b, and obtain a power of a third word, namely a square bab·bab
of bab. In this paper, we completely describe all ways how this can happen for
two words, and formalize it in Isabelle/HOL.
The corresponding theory has a long history. The question can be formulated
as solving equations in three variables of the special form W(x, y) = zℓwhere
the left hand side is a sequence of x’s and y’s, and ℓ≥2. The seminal result in
this direction is the paper by R. C. Lyndon and M.-P. Sch¨utzenberger [10] from
1962, which solves in a more general setting of free groups the equation xjyk = zℓ
with 2 ≤j, k, ℓ. It was followed, in 1967, by a partial answer to our question by
A. Lentin and M.-P. Sch¨utzenberger [9]. A complete characterization of monoids
generated by three words was provided by L. G. Budkina and Al. A. Markov
in 1973 [4]. The characterization was later, in 1976, reproved in a diﬀerent way
by Lentin’s student J.-P. Spehner in his Ph.D. thesis [14], which even explicitly
mentions the answer to the present question. See also a comparison of the two
classiﬁcations by T. Harju and D. Nowotka [7]. In 1985, the result was again
reproved by E. Barbin-Le Rest and M. Le Rest [1], this time speciﬁcally focusing
on our question. Their paper contains a characterization of binary interpretations
of a square as a crucial tool. The latter combinatorial result is interesting on its
own, but is very little known. In addition to the fact that, as far as we know,
the proof is not available in English, it has to be reconstructed from Th´eor`eme
2.1 and Lemme 3.1 in [1], it is long, technical and little structured, with many
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 369–387, 2022.
https://doi.org/10.1007/978-3-031-10769-6_23

370
ˇS. Holub et al.
intuitive steps that have to be clariﬁed. It is symptomatic, for example, that
Maˇnuch [11] cites the claim as essentially equivalent to his desired result but
nevertheless provides a diﬀerent, shorter but similarly technical proof.
The fact that several authors opted to provide their own proof of the already
known result, and that even a weaker result was republished as new shows that
the existing proof was not considered suﬃciently convincing and approachable.
This makes the topic a perfect candidate for formalization. The proof we present
here naturally contains some ideas of the proof from [1] but is signiﬁcantly dif-
ferent. Our main objective was to follow the basic methodological requirement
of a good formalization, namely to identify claims that are needed in the proof
and formulate them as separate lemmas and as generally as possible so that
they can be reused not only in the proof but also later. Moreover, the formal-
ization naturally forced us to consider carefully the overall strategy of the proof
(which is rather lost behind technical details of published works on this topic).
Under Isabelle’s pressure we eventually arrived at a hopefully clear proof struc-
ture which includes a simple, but probably innovative use of the idea of “gluing”
words. The analysis of the proof is therefore another, and we believe the most
important contribution of our formalization, in addition to the mere certainty
that there are no gaps in the proof.
In addition, we provide a complete parametric solution of the equation xkyj =
zℓfor arbitrary j, k and ℓ, a classiﬁcation which is not very diﬃcult, but maybe
too complicated to be useful in a mere unveriﬁed paper form.
The formalization presented here is an organic part of a larger project of
formalization of combinatorics of words (see an introductory description in [8]).
We are not aware of a similar formalization project in any proof assistant. The
existence of the underlying library, which in turn extends the theories of “List”
and “HOL-Library.Sublist” from the standard Isabelle distribution, critically
contributes to a smooth formalization which is getting fairly close to the way
a human paper proof would look like, outsourcing technicalities to the (reusable)
background. We accompany claims in this text with names of their formalized
counterparts.
2
Basic Facts and Notation
Let Σ be an arbitrary set. Lists (i.e. ﬁnite sequences) [x1, x2, . . . , xn] of elements
xi ∈Σ are called words over Σ. The set of all words over Σ is usually denoted
as Σ∗, using the Kleene star. A notorious ambivalence of this notation is related
to the situation when we consider a set of words X ⊂Σ∗, and are interested in
lists over X. They should be denoted as elements of X∗. However, X∗usually
means something else (in the theory of rational languages), namely the set of all
words in Σ∗generated by the set X. To avoid the confusion, we will therefore
follow the notation used in the formalization in Isabelle, and write lists X
instead, to make clear that the entries of an element of lists X are themselves
words. In order to further help to distinguish words over the basic alphabet
from lists over a set of words, we shall use boldface variables for the latter.

Binary Codes that Do Not Preserve Primitivity
371
In particular, it is important to keep in mind the diﬀerence between a letter
a and the word [a] of length one, the distinction which is usually glossed over
lightly in the literature on combinatorics on words. The set of words over Σ
generated by X is then denoted as ⟨X⟩. The (associative) binary operation of
concatenation of two words u and v is denoted by u · v. We prefer this algebraic
notation to the Isabelle’s original @. Moreover, we shall often omit the dot as
usual. If u = [x1, x2, . . . , xn] ∈lists X is a list of words, then we write concat u
for x1 · x2 · · · xn. We write ε for the empty list, and uk for the concatenation of
k copies of u (we use u@k in the formalization). We write u ≤p v, u <p v,
u ≤s v, u <s v, and u ≤f v to denote that u is a preﬁx, a strict preﬁx, suﬃx,
strict suﬃx and factor (that is, a contiguous sublist) respectively. A word is
primitive if it is nonempty and not a power of a shorter word. Otherwise, we call
it imprimitive. Each nonempty word w is a power of a unique primitive word
ρ w, its primitive root. A nonempty word r is a periodic root of a word w if
w ≤p r · w. This is equivalent to w being a preﬁx of the right inﬁnite power of r,
denoted rω. Note that we deal with ﬁnite words only, and we use the notation
rω only as a convenient shortcut for “a suﬃciently long power of r”. Two words
u and v are conjugate, we write u ∼v, if u = rq and v = qr for some words
r and q. Note that conjugation is an equivalence whose classes are also called
cyclic words. A word u is a cyclic factor of w if it is a factor of some conjugate
of w. A set of words X is a code if its elements do not satisfy any nontrivial
relation, that is, they are a basis of a free semigroup. For a two-element set
{x, y}, this is equivalent to x and y being non-commuting, i.e., xy ̸= yx, and/or
to ρ x ̸= ρ y. An important characterization of a semigroup S of words to be free
is the stability condition which is the implication u, v, uz, zv ∈S =⇒z ∈S. The
longest common preﬁx of u and v is denoted by u ∧p v. If {x, y} is a (binary)
code, then (x · w) ∧p (y · w′) = xy ∧p yx for any w, w′ ∈⟨{x, y}⟩suﬃciently
long. We explain some elementary facts from combinatorics on words used in
this article in more detail in Sect. 8.
3
Main Theorem
Let us introduce the central deﬁnition of the paper.
Deﬁnition 1. We say that a set X of words is primitivity preserving if there
is no word w ∈lists X such that
– |w| ≥2;
– w is primitive; and
– concat w is imprimitive.
Note that our deﬁnition does not take into account singletons w = [x]. In
particular, X can be primitivity preserving even if some x ∈X is imprimitive.
Nevertheless, in the binary case, we will also provide some information about
the cases when one or both elements of the code have to be primitive.
In [12], V. Mitrana formulates the primitivity of a set in terms of morphisms,
and shows that X is primitivity preserving if and only if it is the minimal set of

372
ˇS. Holub et al.
generators of a “pure monoid”, cf. [3, p. 276]. This brings about a wider concept
of morphisms preserving a given property, most classically square-freeness, see
for example a characterization of square-free morphisms over three letters by M.
Crochemore [5].
The target claim of our formalization is the following characterization of
words witnessing that a binary code is not primitivity preserving:
Theorem 1 (bin imprim code).
Let B = {x, y} be a code that is not prim-
itivity preserving. Then there are integers j ≥1 and k ≥1, with k = 1 or
j = 1, such that the following conditions are equivalent for any w ∈lists B
with |w| ≥2:
– w is primitive, and concat w is imprimitive
– w is conjugate with [x]j[y]k.
Moreover, assuming |y| ≤|x|,
– if j ≥2, then j = 2 and k = 1, and both x and y are primitive;
– if k ≥2, then j = 1 and x is primitive.
Proof. Let w be a word witnessing that B is not primitivity preserving. That is,
|w| ≥2, w is primitive, and concat w is imprimitive. Since [x]j[y]k and [y]k[x]j
are conjugate, we can suppose, without loss of generality, that |y| ≤|x|.
First, we want to show that w is conjugate with [x]j[y]k for some j, k ≥1
such that k = 1 or j = 1. Since w is primitive and of length at least two, it
contains both x and y. If it contains one of these letters exactly once, then w is
clearly conjugate with [x]j[y]k for j = 1 or k = 1. Therefore, the diﬃcult part
is to show that no primitive w with concat w imprimitive can contain both
letters at least twice. This is the main task of the rest of the paper, which is
ﬁnally accomplished by Theorem 4 claiming that words that contain at least two
occurrences of x are conjugate with [x, x, y]. To complete the proof of the ﬁrst
part of the theorem, it remains to show that j and k do not depend on w. This
follows from Lemma 1.
Note that the imprimitivity of concat w induces the equality xjyk = zℓ
for some z and ℓ≥2. The already mentioned seminal result of Lyndon and
Sch¨utzenberger shows that j and k cannot be simultaneously at least two, since
otherwise x and y commute. For the same reason, considering its primitive root,
the word y is primitive if j ≥2. Similarly, x is primitive if k ≥2. The primitivity
of x when j = 2 is a part of Theorem 4.
⊓⊔
We start by giving a complete parametric solution of the equation xjyk = zℓ
in the following theorem. This will eventually yield, after the proof of Theorem
1 is completed, a full description of not primitivity preserving binary codes.
Since the equation is mirror symmetric, we omit symmetric cases by assuming
|y| ≤|x|.
Theorem 2 (LS parametric solution). Let ℓ≥2, j, k ≥1 and |y| ≤|x|.
The equality xjyk = zℓholds if and only if one of the following cases takes
place:

Binary Codes that Do Not Preserve Primitivity
373
A. There exists a word r, and integers m, n, t ≥0 such that
mj + nk = tℓ,
and
x = rm,
y = rn,
z = rt;
B. j = k = 1 and there exist non-commuting words r and q, and integers
m, n ≥0 such that
m + n + 1 = ℓ,
and
x = (rq)mr,
y = q(rq)n,
z = rq;
C. j = ℓ= 2, k = 1 and there exist non-commuting words r and q and an
integer m ≥2 such that
x = (rq)mr,
y = qrrq,
z = (rq)mrrq;
D. j = 1 and k ≥2 and there exist non-commuting words r and q such that
x = (qrk)ℓ−1q,
y = r,
z = qrk;
E. j = 1 and k ≥2 and there are non-commuting words r and q, an integer
m ≥1 such that
x = (qr(r(qr)m)k−1)ℓ−2qr(r(qr)m)k−2rq,
y = r(qr)m,
z = qr(r(qr)m)k−1.
Proof. If x and y commute, then all three words commute, hence they are a
power of a common word. A length argument yields the solution A.
Assume now that {x, y} is a code. Then no pair of words x, y and z commutes.
We have shown in the overview of the proof of Theorem 1 that j = 1 or k = 1
by the Lyndon-Sch¨utzenberger theorem. The solution is then split into several
cases.
Case 1: j = k = 1.
Let m and r be such that zmr = x with r a strict preﬁx of z. By setting z = rq,
we obtain the solution B with n = ℓ−m −1.
Case 2: j ≥2, k = 1.
Since |y| ≤|x| and ℓ≥2, we have
2|z| ≤|zℓ| = |xj| + |y| < 2|xj|,
so z is a strict preﬁx of xj.
As xj has periodic roots both z and x, and z does not commute with x, the
Periodicity lemma implies |xj| < |z| + |x|. That is, z = xj−1u, xj = zv and
x = uv for some nonempty words u and v. As v is a preﬁx of z, it is also a preﬁx
of x. Therefore, we have
x = uv = vu′
for some word u′. This is a well known conjugation equality which implies u = rq,
u′ = qr and v = (rq)nr for some words r, q and an integer n ≥0.

374
ˇS. Holub et al.
We have
j|x| + |y| = |xjy| = |zℓ| = ℓ(j −1)|x| + ℓ|u|,
and thus |y| = (ℓj −ℓ−j)|x| + ℓ|u|. Since |y| ≤|x|, |u| > 0, j ≥2, and ℓ≥2, it
follows that ℓj −ℓ−j = 0, which implies j = l = 2. We therefore have x2y = z2
and x2 = zv, hence vy = z.
Combining u = rq, u′ = qr, and v = (rq)nr with x = vu′, z = xj−1u = xu =
vu′u, and vy = z, we obtain the solution C with m = n + 1. The assumption
|y| ≤|x| implies m ≥2.
Case 3: j = 1, k ≥2, yk≤sz.
We have z = qyk for some word q. Noticing that x = zℓ−1q yields the solution
D.
Case 4: j = 1, k ≥2, z <s yk.
This case is analogous to the second part of Case 2. Using the Periodicity lemma,
we obtain uyk−1 = z, yk = vz, and y = vu with nonempty u and v. As v is a
suﬃx of z, it is also a suﬃx of y, and we have y = vu = u′v for some u′. Plugging
the solution of the last conjugation equality, namely u′ = rq, u = qr, v = (rq)nr,
into y = u′v, z = uyk−1 and zℓ−1 = xv gives the solution E with m = n + 1.
Finally, the words r and q do not commute since x and y, which are generated
by r and q, do not commute.
The proof is completed by a direct veriﬁcation of the converse.
⊓⊔
We now show that, for a given not primitivity preserving binary code, there
is a unique pair of exponents (j, k) such that xjyk is imprimitive.
Lemma 1 (LS unique).
Let B = {x, y} be a code. Assume j, k, j′, k′ ≥1. If
both xjyk and xj′yk′ are imprimitive, then j = j′ and k = k′.
Proof. Let z1, z2 be primitive words and ℓ, ℓ′ ≥2 be such that
xjyk = zℓ
1
and
xj′yk′ = zℓ′
2 .
(1)
Since B is a code, the words x and y do not commute. We proceed by contra-
diction.
Case 1: First, assume that j = j′ and k ̸= k′.
Let, without loss of generality, k < k′. From (1) we obtain zℓ
1yk′−k = zℓ′
2 . The
case k′ −k ≥2 is impossible due to the Lyndon-Sch¨utzenberger theorem. Hence
k′ −k = 1. This is another place where the formalization triggered a sim-
ple and nice general lemma (easily provable by the Periodicity lemma) which
will turn out to be useful also in the proof of Theorem 4. Namely, the lemma
imprim_ext_suf_comm claims that if both uv, and uvv are imprimitive, then u
and v commute. We apply this lemma to u = xjyk−1 and v = y, obtaining a
contradiction with the assumption that x and y do not commute.
Case 2. The case k = k′ and j ̸= j′ is symmetric to Case 1.
Case 3. Let ﬁnally j ̸= j′ and k ̸= k′. The Lyndon-Sch¨utzenberger theorem
implies that either j or k is one, and similarly either j′ or k′ is one. We can

Binary Codes that Do Not Preserve Primitivity
375
therefore assume that k = j′ = 1 and k′, j ≥2. Moreover, we can assume that
|y| ≤|x|. Indeed, in the opposite case, we can consider the words ykxj and yk′xj′
instead, which are also both imprimitive.
Theorem 2 now allows only the case C for the equality xjy = zℓ
1. We therefore
have j = ℓ= 2 and x = (rq)mr, y = qrrq for an integer m ≥2 and some non-
commuting words r and q. Since y = qrrq is a suﬃx of zℓ
2, this implies that z2
and rq do not commute. Consider the word x · qr = (rq)mrqr, which is a preﬁx
of xy, and therefore also of zℓ
2. This means that x · qr has two periodic roots,
namely rq and z2, and the Periodicity lemma implies that |x · qr| < |rq| + |z2|.
Hence x is shorter than z2. The equality xyk′ = zℓ′
2 , with ℓ′ ≥2, now implies
on one hand that rqrq is a preﬁx of z2, and on the other hand that z2 is a
suﬃx of yk′. It follows that rqrq is a factor of (qrrq)k. Hence rqrq and qrrq are
conjugate, thus they both have a period of length |rq|, which implies qr = rq.
This is a contradiction.
⊓⊔
The rest of the paper, and therefore also of the proof of Theorem 1, is orga-
nized as follows. In Sect. 4, we introduce a general theory of interpretations,
which is behind the main idea of the proof, and apply it to the (relatively simple)
case of a binary code with words of the same length. In Sect. 5 we characterize
the unique disjoint extendable {x, y}-interpretation of the square of the longer
word x. This is a result of independent interest, and also the cornerstone of
the proof of Theorem 1 which is completed in Sect. 6 by showing that a word
containing at least two x’s witnessing that {x, y} is not primitivity preserving is
conjugate with [x, x, y].
4
Interpretations and the Main Idea
Let X be a code, let u be a factor of concat w for some w ∈lists X. The
natural question is to decide how u can be produced as a factor of words from
X, or, in other words, how it can be interpreted in terms of X. This motivates
the following deﬁnition.
Deﬁnition 2. Let X be a set of words over Σ. We say that the triple (p, s, w) ∈
Σ∗× Σ∗× lists X is an X-interpretation of a word u ∈Σ∗if
– w is nonempty;
– p · u · s = concat w;
– p <p hd w and
– s <s last w.
The deﬁnition is illustrated by the following ﬁgure, where w = [w1, w2, w3, w4]:
u
w1
w2
w3
w4
p
s
The second condition of the deﬁnition motivates the notation p u s ∼I w for the
situation when (p, s, w) is an X-interpretation of u.

376
ˇS. Holub et al.
Remark 1. For sake of historical reference, we remark that our deﬁnition of X-
interpretation diﬀers from the one used in [1]. Their formulation of the situa-
tion depicted by the above ﬁgure would be that u is interpreted by the triple
(s′, w2 · w3, p′) where p · s′ = w1 and p′ · s = w4. This is less convenient for two
reasons. First, the decomposition of w2 · w3 into [w2, w3] is only implicit here
(and even ambiguous if X is not a code). Second, while it is required that the
the words p′ and s′ are a preﬁx and a suﬃx, respectively, of an element from X,
the identity of that element is left open, and has to be speciﬁed separately.
If u is a nonempty element of ⟨X⟩and u = concat u for u ∈lists X,
then the X-interpretation ε u ε ∼I u is called trivial. Note that the trivial X-
interpretation is unique if X is a code.
As nontrivial X-interpretations of elements from ⟨X⟩are of particular inter-
est, the following two concepts are useful.
Deﬁnition 3. An X-interpretation p u s ∼I w of u = concat u is called
– disjoint if concat w′ ̸= p · concat u′ whenever w′ ≤p w and u′ ≤p u.
– extendable if p ≤s wp and s ≤p ws for some elements wp, ws ∈⟨X⟩.
Note that a disjoint X-interpretation is not trivial, and that being disjoint
is relative to a chosen factorization u of u (which is nevertheless unique if X is
a code).
The deﬁnitions above are naturally motivated by the main idea of the
characterization of sets X that do not preserve primitivity, which dates back
to Lentin and Sch¨utzenberger [9]. If w is primitive, while concat w is imprim-
itive, say concat w = zk, k ≥2, then the shift by z provides a nontrivial and
extendable X-interpretation of concat w. (In fact, k−1 such nontrivial interpre-
tations). Moreover, the following lemma, formulated in a more general setting
of two words w1 and w2, implies that the X-interpretation is disjoint if X is a
code.
Lemma 2 (shift interpret,
shift disjoint).
Let
X
be
a
code.
Let
w1, w2 ∈lists X be such that z · concat w1 = concat w2 · z where z /∈⟨X⟩.
Then z · concat v1 ̸= concat v2, whenever v1 ≤p wn
1 and v2 ≤p wn
2 , n ∈N.
In particular, concat u has a disjoint extendable X-interpretation for any
preﬁx u of w1.
The excluded possibility is illustrated by the following ﬁgure.
concat w1
concat w1
concat w2
concat w2
z
z
concat v1
concat v2

Binary Codes that Do Not Preserve Primitivity
377
Proof. First, note that z ·concat wn
1 = concat wn
2 ·z for any n. Let wn
1 = v1 ·v′
1
and wn
2 = v2 · v′
2. If z · concat v1 = concat v2, then also concat v′
2 · z =
concat v′
1. This contradicts z /∈⟨X⟩by the stability condition.
An extendable X-interpretation of u is induced by the fact that concat u is
covered by concat(w2 · w2). The interpretation is disjoint by the ﬁrst part of
the proof.
⊓⊔
In order to apply the above lemma to the imprimitive concat w = zk of a
primitive w, set w1 = w2 = w. The assumption z /∈⟨X⟩follows from the
primitivity of w: indeed, if z = concat z, with z ∈lists X, then w = zk since
B is a code.
We ﬁrst apply the main idea to a relatively simple case of nontrivial {x, y}-
interpretation of the word x · y where x and y are of the same length.
Lemma 3 (uniform square interp). Let B = {x, y} be a code with |x| = |y|.
Let p (x · y) s ∼I v be a nontrivial B-interpretation. Then v = [x, y, x] or
v = [y, x, y] and x · y is imprimitive.
Proof. From p · x · y · s = concat v, it follows, by a length argument, that |v| is
three. A straightforward way to prove the claim is to consider all eight possible
candidates. In each case, it is then a routine few line proof that shows that x = y,
unless v = [x, y, x] or v = [y, x, y], which we omit. In the latter cases, x · y is a
nontrivial factor of its square (x · y) · (x · y), which yields the imprimitivity of
x · y.
⊓⊔
The previous (sketch of the) proof nicely illustrates on a small scale the advan-
tages of formalization. It is not necessary to choose between a tedious elementary
proof for sake of completeness on one hand, and the suspicion that something
was missed on the other hand (leaving aside that the same suspicion typically
remains even after the tedious proof). A bit ironically, the most diﬃcult part
of the formalization is to show that v is indeed of length three, which needs no
further justiﬁcation in a human proof.
We have the following corollary which is a variant of Theorem 4, and also
illustrates the main idea of its proof.
Lemma 4 (bin imprim not conjug). Let B = {x, y} be a binary code with
|x| = |y|. If w ∈lists B is such that |w| ≥2, w is primitive, and concat w is
imprimitive, then x and y are not conjugate.
Proof. Since w is primitive and of length at least two, it contains both letters
x and y. Therefore, it has either [x, y] or [y, x] as a factor. The imprimitivity of
concat w yields a nontrivial B-interpretation of x · y, which implies that x · y is
not primitive by Lemma 3.
Let x and y be conjugate, and let x = r ·q and y = q ·r. Since x·y = r ·q ·q ·r
is imprimitive, also r · r · q · q is imprimitive. Then r and q commute by the
theorem of Lyndon and Sch¨utzenberger, a contradiction with x ̸= y.
⊓⊔

378
ˇS. Holub et al.
5
Binary Interpretation of a Square
Let B = {x, y} be a code such that |y| ≤|x|. In accordance with the main
idea, the core technical component of the proof is the description of the disjoint
extendable B-interpretations of the square x2. This is a very nice result which is
relatively simple to state but diﬃcult to prove, and which is valuable on its own.
As we mentioned already, it can be obtained from Th´eor`eme 2.1 and Lemme 3.1
in [1].
Theorem 3 (square interp ext.sq ext interp).
Let B = {x, y} be a code
such that |y| ≤|x|, both x and y are primitive, and x and y are not conjugate.
Let p (x · x) s ∼I w be a disjoint extendable B-interpretation. Then
w = [x, y, x],
s · p = y,
p · x = x · s.
In order to appreciate the theorem, note that the deﬁnition of interpretation
implies
p · x · x · s = x · y · x,
hence x · y · x = (p · x)2. This will turn out to be the only way how primitivity
may not be preserved if x occurs at least twice in w. Here is an example with
x = 01010 and y = 1001:
0 1 0 1 0 1 0 0 1 0 1 0 1 0
0 1 0 1 0 1 0 0 1 0 1 0 1 0
Proof. By the deﬁnition of a disjoint interpretation, we have p·x·x·s = concat w,
where p ̸= ε and s ̸= ε. A length argument implies that w has length at least
three. Since a primitive word is not a nontrivial factor of its square, we have
w = [hd w] · [y]k · [last w], with k ≥1. Since the interpretation is disjoint, we
can split the equality into p · x = hd w · ym · u and x · s = v · yℓ· last w, where
y = u · v, both u and v are nonempty, and k = ℓ+ m + 1. We want to show
hd w = last w = x and m = ℓ= 0. The situation is mirror symmetric so we
can solve cases two at a time.
If hd w = last w = y, then powers of x and y share a factor of length at
least |x| + |y|. Since they are primitive, this implies that they are conjugate, a
contradiction. The same argument applies when ℓ≥1 and hd w = y (if m ≥1
and last w = y respectively). Therefore, in order to prove hd w = last w = x,
it remains to exclude the case hd w = y, ℓ= 0 and last w = x (last w = y,
m = 0 and hd w = x respectively). This is covered by one of the technical
lemmas that we single out:
Lemma 5 (pref suf pers short). Let x ≤p v·x, x ≤s p·u·v·u and |x| > |v · u|
with p ∈⟨{u, v}⟩. Then u · v = v · u.

Binary Codes that Do Not Preserve Primitivity
379
This lemma indeed excludes the case we wanted to exclude, since the con-
clusion implies that y is not primitive. We skip the proof of the lemma here and
make instead an informal comment. Note that v is a period root of x. In other
words, x is a factor of vω. Therefore, with the stronger assumption that v ·u·v is
a factor of x, the conclusion follows easily by the familiar principle that v being
a factor of vω “synchronizes” primitive roots of v. Lemma 5 then exempliﬁes
one of the virtues of formalization, which makes it easy to generalize auxiliary
lemmas, often just by following the most natural proof and checking its minimal
necessary assumptions.
Now we have hd w = last w = x, hence p·x = x·ym·u and x·s = v·yℓ·x. The
natural way to describe this scenario is to observe that x has both the (preﬁx)
period root v · yℓ, and the suﬃx period root ym · u. Using again Lemma 5, we
exclude situations when ℓ= 0 and m ≥1 (m = 0 and ℓ≥1 resp.). It therefore
remains to deal with the case when both m and ℓare positive. We divide this
into four lemmas according to the size of the overlap the preﬁx v · yℓand the
suﬃx ym · u have in x. More exactly, the cases are:
–
v · yℓ + |ym · u| ≤|x|
– |x| <
v · yℓ + |ym · u| ≤|x| + |u|
– |x| + |u| <
v · yℓ + |ym · u| < |x| + |u · v|
– |x| + |u · v| ≤
v · yℓ + |ym · u|
and they are solved by an auxiliary lemma each. The ﬁrst three cases yield
that u and v commute, the ﬁrst one being a straightforward application of the
Periodicity lemma. The last one is also straightforward application of the “syn-
chronization” idea. It implies that x · x is a factor of yω, a contradiction with
the assumption that x and y are primitive and not conjugate. Consequently, the
technical, tedious part of the whole proof is concentrated in lemmas dealing with
the second, and the third case (see lemmas short_overlap and medium_overlap
in the theory Binary_Square_Interpretation.thy). The corresponding proofs
are further analyzed and decomposed into more elementary claims in the for-
malization, where further details can be found.
This completes the proof of w = [x, y, x]. A byproduct of the proof is the
description of words x, y, p and s. Namely, there are non-commuting words r
and t, and integers m, k and ℓsuch that
x = (rt)m+1 · r,
y = (tr)k+1 · (rt)ℓ+1,
p = (rt)k+1,
s = (tr)ℓ+1 .
The second claim of the present theorem, that is, y = s · p is then equivalent to
k = ℓ, and it is an easy consequence of the assumption that the interpretation
is extendable.
⊓⊔
6
The Witness with Two x’s
In this section, we characterize words witnessing that {x, y} is not primitivity
preserving and containing at least two x’s.

380
ˇS. Holub et al.
Theorem 4 (bin imprim longer twice). Let B = {x, y} be a code such that
|y| ≤|x|. Let w ∈lists {x, y} be a primitive word which contains x at least
twice such that concat w is imprimitive.
Then w ∼[x, x, y] and both x and y are primitive.
We divide the proof in three steps.
The Core Case. We ﬁrst prove the claim with two additional assumptions
which will be subsequently removed. Namely, the following lemma shows how
the knowledge about the B-interpretation of x · x from the previous section is
used. The additional assumptions are displayed as items.
Lemma 6 (bin imprim primitive).
Let B = {x, y} be a code with |y| ≤|x|
where
– both x and y are primitive,
and let w ∈lists B be primitive such that concat w is imprimitive, and
– [x, x] is a cyclic factor of w.
Then w ∼[x, x, y].
Proof. Choosing a suitable conjugate of w, we can suppose, without loss of
generality, that [x, x] is a preﬁx of w. Now, we want to show w = [x, x, y].
Proceed by contradiction and assume w ̸= [x, x, y]. Since w is primitive, this
implies w · [x, x, y] ̸= [x, x, y] · w.
By Lemma 4, we know that x and y are not conjugate. Let concat w = zk,
2 ≤k and z primitive. Lemma 2 yields a disjoint extendable B-interpretation of
(concat w)2. In particular, the induced disjoint extendable B-interpretation of
the preﬁx x · x is of the form p (x · x) s ∼I [x, y, x] by Theorem 3:
z
x
x
x
x
y
concat w
concat w
p
s
s p
Let p be the preﬁx of w such that concat p · p = z. Then
concat(p · [x, y]) = z · (x · p),
concat [x, x, y] = (x · p)2,
concat w = zk,
and we want to show z = xp, which will imply concat([x, x, y]·w) = concat(w·
[x, x, y]), hence w = [x, x, y] since {x, y} is a code, and both w and [x, x, y] are
primitive.
Again, proceed by contradiction, and assume z ̸= xp. Then, since both z and
x·p are primitive, they do not commute. We now have two binary codes, namely
{w, [x, x, y]} and {z, xp}. The following two equalities, (2) and (3) exploit the
fundamental property of longest common preﬁxes of elements of binary codes
mentioned in Sect. 2. In particular, we need the following lemma:

Binary Codes that Do Not Preserve Primitivity
381
Lemma 7 (bin code lcp concat). Let X = {u0, u1} be a binary code, and
let z0, z1 ∈lists X be such that concat z0 and concat z1 are not preﬁx-
comparable. Then
(concat z0) ∧p (concat z1) = concat(z0 ∧p z1) · (u0 ∧u1).
See Sect. 8 for more comments on this property. Denote αz,xp = z · xp ∧p xp · z.
Then also αz,xp = zk · (xp)2 ∧p (xp)2 · zk. Similarly, let αx,y = x · y ∧p y · x. Then
Lemma 7 yields
αz,xp = concat(w · [x, x, y]) ∧p concat([x, x, y] · w)
= concat(w · [x, x, y] ∧p [x, x, y] · w) · αx,y
(2)
and also
z · αz,xp =concat(w · p · [x, y]) ∧p concat(p · [x, y] · w)
=concat(w · p · [x, y] ∧p p · [x, y] · w) · αx,y.
(3)
Denote
v1 = w · [x, x, y] ∧p [x, x, y] · w,
v2 = w · p · [x, y] ∧p p · [x, y] · w.
From (2) and (3) we now have z · concat v1 = concat v2. Since v1 and v2 are
preﬁxes of some wn, we have a contradiction with Lemma 2.
⊓⊔
Dropping the Primitivity Assumption. We ﬁrst deal with the situation
when x and y are not primitive. A natural idea is to consider the primitive
roots of x and y instead of x and y. This means that we replace the word w
with Rw, where R is the morphism mapping [x] to [ρ x]ex and [y] to [ρ y]ey
where x = (ρ x)ex and y = (ρ y)ey. For example, if x = abab and y = aa, and
w = [x, y, x] = [abab, aa, abab], then Rw = [ab, ab, a, a, ab, ab].
Let us check which hypotheses of Lemma 6 are satisﬁed in the new setting,
that is, for the code {ρ x, ρ y} and the word Rw. The following facts are not
diﬃcult to see.
– concat w = concat(Rw);
– if [c, c], c ∈{x, y}, is a cyclic factor w, then [ρ c, ρ c] is a cyclic factor of Rw.
The next required property:
– if w is primitive, then Rw is primitive;
deserves more attention. It triggered another little theory of our formalization
which can be found in locale sings_code. Note that it ﬁts well into our context,
since the claim is that R is a primitivity preserving morphism, which implies
that its image on the singletons [x] and [y] forms a primitivity preserving set of
words, see theorem code.roots_prim_morph.
Consequently, the only missing hypothesis preventing the use of Lemma 6 is
|y| ≤|x| since it may happen that |ρ x| < |ρ y|. In order to solve this diﬃculty,
we shall ignore for a while the length diﬀerence between x and y, and obtain the
following intermediate lemma.

382
ˇS. Holub et al.
Lemma 8 (bin imprim both squares, bin imprim both squares prim). Let
B = {x, y} be a code, and let w ∈lists B be a primitive word such that
concat w is imprimitive. Then w cannot contain both [x, x] and [y, y] as cyclic
factors.
Proof. Assume that w contains both [x, x] and [y, y] as cyclic factors.
Consider the word Rw and the code {ρ x, ρ y}. Since Rw contains both
[ρ x, ρ x] and [ρ y, ρ y], Lemma 6 implies that Rw is conjugate either with the
word [ρ x, ρ x, ρ y] or with [ρ y, ρ y, ρ x], which is a contradiction with the assumed
presence of both squares.
⊓⊔
Concluding the Proof by Gluing. It remains to deal with the existence of
squares. We use an idea that is our main innovation with respect to the proof
from [1], and contributes signiﬁcantly to the reduction of length of the proof, and
hopefully also to its increased clarity. Let w be a list over a set of words X. The
idea is to choose one of the words, say u ∈X, and to concatenate (or “glue”)
blocks of u’s to words following them. For example, if w = [u, v, u, u, z, u, z],
then the resulting list is [uv, uuz, uz]. This procedure is in the general case well
deﬁned on lists whose last “letter” is not the chosen one and it leads to a new
alphabet {ui · v | v ̸= u} which is a code if and only if X is. This idea is used
in an elegant proof of the Graph lemma (see [8] and [2]). In the binary case,
which is of interest here, if w in addition does not contain a square of a letter,
say [x, x], then the new code {x · y, y} is again binary. Moreover, the resulting
glued list w′ has the same concatenation, and it is primitive if (and only if) w
is. Note that gluing is in this case closely related to the Nielsen transformation
y →x−1y known from the theory of automorphisms of free groups.
Induction on |w| now easily leads to the proof of Theorem 4.
Proof (of Theorem 4). If w contains y at most once, then we are left with the
equation xj · y = zℓ, ℓ≥2. The equality j = 2 follows from the Periodicity
lemma, see Case 2 in the proof of Theorem 2.
Assume for contradiction that y occurs at least twice in w. Lemma 8 implies
that at least one square, [x, x] or [y, y] is missing as a cyclic factor. Let {x′, y′} =
{x, y} be such that [x′, x′] is not a cyclic factor of w. We can therefore perform the
gluing operation, and obtain a new, strictly shorter word w′ ∈lists {x′ ·y′, y′}.
The longer element x′ · y′ occurs at least twice in w′, since the number of its
occurrences in w′ is the same as the number of occurrences of x′ in w, the
latter word containing both letters at least twice by assumption. Moreover, w′
is primitive, and concat w′ = concat w is imprimitive. Therefore, by induction
on |w|, we have w′ ∼[x′ · y′, x′ · y′, y′]. In order to show that this is not possible
we can successfully reuse the lemma imprim_ext_suf_comm mentioned in the
proof of Lemma 1, this time for u = x′y′x′ and v = y′. The words u and v do
not commute because x′ and y′ do not commute. Since uv is imprimitive, the
word uvv ∼concat w′ is primitive.
⊓⊔
This also completes the proof of our main target, Theorem 1.

Binary Codes that Do Not Preserve Primitivity
383
7
Additional Notes on the Formalization
The formalization is a part of an evolving combinatorics on words formalization
project. It relies on its backbone session, called CoW, a version of which is also
available in the Archive of Formal Proofs [15]. This session covers basics con-
cepts in combinatorics on words including the Periodicity lemma. An overview
is available in [8].
The evolution of the parent session CoW continued along with the pre-
sented results and its latest stable version is available at our repository [16].
The main results are part of another Isabelle session CoW Equations, which,
as the name suggests, aims at dealing with word equations. We have greatly
expanded its elementary theory Equations Basic.thy which provides auxiliary
lemmas and deﬁnitions related to word equations. Noticeably, it contains the
deﬁnition factor interpretation (Deﬁnition 2) and related facts.
Two dedicated theories were created: Binary Square Interpretation.thy
and Binary Code Imprimitive.thy. The ﬁrst contains lemmas and locales deal-
ing with {x, y}-interpretation of the square xx (for |y| ≤|x|), culminating in
Theorem 3. The latter contains Theorems 1 and 4.
Another outcome was an expansion of formalized results related to the
Lyndon-Sch¨utzenberger theorem. This result, along with many useful corollaries,
was already part of the backbone session CoW, and it was newly supplemented
with the parametric solution of the equation xjyk = zℓ, speciﬁcally Theorem 2
and Lemma 1. This formalization is now part of CoW Equations in the theory
Lyndon Schutzenberger.thy.
Similarly, the formalization of the main results triggered a substantial expan-
sion of existing support for the idea of gluing as mentioned in Sect. 6. Its reworked
version is now in a separate theory called Glued Codes.thy (which is part of the
session CoW Graph Lemma).
Let us give a few concrete highlights of the formalization. A very useful
tool, which is part of the CoW session, is the reversed attribute. The attribute
produces a symmetrical fact where the symmetry is induced by the mapping rev,
i.e., the mapping which reverses the order of elements in a list. For instance, the
fact stating that if p is a preﬁx of v, then p a preﬁx of v · w, is transformed by
the reversed attribute into the fact saying that if s is suﬃx of v, then s is a suﬃx
of w ·v. The attribute relies on ad hoc deﬁned rules which induce the symmetry.
In the example, the main reversal rule is
(rev u ≤p rev v) = u ≤s v.
The attribute is used frequently in the present formalization. For instance, Fig. 1
shows the formalization of the proof of Cases 1 and 2 of Theorem 1. Namely,
the proof of Case 2 is smoothly deduced from the lemma that deals with Case 1,
avoiding writing down the same proof again up to symmetry. See [13] for more
details on the symmetry and the attribute reversed.
To be able to use this attribute fully in the formalization of main results, it
needed to be extended to be able to deal with elements of type ′a list list,
as the constant factor_interpretation is of the function type over this exact

384
ˇS. Holub et al.
Fig. 1. Highlights from the formalization in Isabelle/HOL.
type. The new theories of the session CoW Equations contain almost 50 uses of
this attribute.
The second highlight of the formalization is the use of simple but useful
proof methods. The ﬁrst method, called primitivity_inspection, is able to
show primitivity or imprimitivity of a given word.
Another method named list_inspection is used to deal with claims that
consist of straightforward veriﬁcation of some property for a set of words given
by their length and alphabet. For instance, this method painlessly concludes
the proof of lemma
bin_imprim_both_squares_prim. The method divides the
goal into eight easy subgoals corresponding to eight possible words. All goals are
then discharged by simp_all.
The last method we want to mention is mismatch. It is designed to prove that
two words commute using the property of a binary code mentioned in Sect. 2
and explained in Sect. 8. Namely, if a product of words from {x, y} starting with
x shares a preﬁx of length at least |xy| with another product of words from
{x, y}, this time starting with y, then x and y commute. Examples of usage of
the attribute reversed and all three methods are given in Fig. 1.
8
Appendix: Background Results in Combinatorics
on Words
A periodic root r of w need not be primitive, but it is always possible to consider
the corresponding primitive root ρ r, which is also a periodic root of w. Note that
any word has inﬁnitely many periodic roots since we allow r to be longer than
w. Nevertheless, a word can have more than one period even if we consider
only periods shorter than |w|. Such a possibility is controlled by the Periodicity
lemma, often called the Theorem of Fine and Wilf (see [6]):

Binary Codes that Do Not Preserve Primitivity
385
Lemma 9 (per lemma comm).
If w has a period u and v, i.e., w ≤p uw and
w ≤p vw, with |u| + |v| −gcd(|u|, |v|) ≤|w|, then uv = vu.
Usually, the weaker test |u| + |v| ≤|w| is suﬃcient to indicate that u and v
commute.
Conjugation u ∼v is characterized as follows:
Lemma 10 (conjugation). If uz = zv for nonempty u, then there exists words
r and q and an integer k such that u = rq, v = qr and z = (rq)kr.
We have said that w has a periodic root r if it is a preﬁx of rω. If w is a factor,
not necessarily a preﬁx, of rω, then it has a periodic root which is a conjugate of
r. In particular, if |u| = |v|, then u ∼v is equivalent to u and v being mutually
factors of a power of the other word.
Commutation of two words is characterized as follows:
Lemma 11 (comm). xy = yx if and only if x = tk and y = tm for some word t
and some integers k, m ≥0.
Since every nonempty word has a (unique) primitive root, the word t can be
chosen primitive (k or m can be chosen 0 if x or y is empty).
We often use the following theorem, called “the theorem of Lyndon and
Sch¨utzenberger”:
Theorem 5 (Lyndon Schutzenberger).
If xjyk = zℓwith j ≥2, k ≥2 and
ℓ≥2, then the words x, y and z commute.
A crucial property of a primitive word t is that it cannot be a nontrivial
factor of its own square. For a general word u, the equality u · u = p · u · s with
nonempty p and s implies that all three words p, s, u commute, that is, have a
common primitive root t. This can be seen by writing u = tk, and noticing that
the presence of a nontrivial factor u inside uu can be obtained exclusively by a
shift by several t’s. This idea is often described as “synchronization”.
Let x and y be two words that do not commute. The longest common preﬁx
of xy and yx is denoted α. Let cx and cy be the letter following α in xy and
yx respectively. A crucial property of α is that it is a preﬁx of any suﬃciently
long word in ⟨{x, y}⟩. Moreover, if w = [u1, u2, . . . , un] ∈lists {x, y} is such
that concat w is longer than α, then α · [cx] is a preﬁx of concat w if u1 = x
and α · [cy] is a preﬁx of concat w if u1 = y. That is why the length of α is
sometimes called “the decoding delay” of the binary code {x, y}. Note that the
property indeed in particular implies that {x, y} is a code, that is, it does not
satisfy any nontrivial relation. It is also behind our method mismatch. Finally,
using this property, the proof of Lemma 7 is straightforward.
Acknowledgments. The authors acknowledge support by the Czech Science Foun-
dation grant GAˇCR 20-20621S.

386
ˇS. Holub et al.
References
1. Barbin-Le Rest, E., Le Rest, M.: Sur la combinatoire des codes `a deux mots. Theor.
Comput. Sci. 41, 61–80 (1985)
2. Berstel, J., Perrin, D., Perrot, J.F., Restivo, A.: Sur le th´eor`eme du d´efaut. J.
Algebra 60(1), 169–180 (1979). http://www.sciencedirect.com/science/article/pii/
0021869379901133. https://doi.org/10.1016/0021-8693(79)90113-3
3. Berstel, J., Perrin, D., Reutenauer, C.: Codes and Automata. Cambridge (2010).
https://www.ebook.de/de/product/8629820/jean berstel dominique perrin
christophe reutenauer codes and automata.html
4. Budkina, L.G., Markov, Al.A.: F-semigroups with three generators. Mat. Zametki
14, 267–277 (1973). Translated from Mat. Zametki 14(2), 267–277 (1973)
5. Crochemore, M.: Sharp characterizations of squarefree morphisms. Theor. Comput.
Sci. 18(2), 221–226 (1982). https://doi.org/10.1016/0304-3975(82)90023-8
6. Fine, N.J., Wilf, H.S.: Uniqueness theorems for periodic functions. Proc. Am. Math.
Soc. 16(1), 109–109 (1965). https://doi.org/10.1090/S0002-9939-1965-0174934-9
7. Harju, T., Nowotka, D.: On the independence of equations in three variables. The-
oret. Comput. Sci. 307(1), 139–172 (2003). https://doi-org.ezproxy.is.cuni.cz/10.
1016/S0304-3975(03)00098-7. https://doi.org/10.1016/S0304-3975(03)00098-7
8. Holub, ˇS., Starosta, ˇS.: Formalization of basic combinatorics on words. In: Cohen,
L., Kaliszyk, C. (eds.) 12th International Conference on Interactive Theorem Prov-
ing (ITP 2021). Leibniz International Proceedings in Informatics (LIPIcs), vol. 193,
pp. 22:1–22:17, Dagstuhl, Germany. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Infor-
matik (2021). https://drops.dagstuhl.de/opus/volltexte/2021/13917. https://doi.
org/10.4230/LIPIcs.ITP.2021.22
9. Lentin, A., Sch¨utzenberger, M.-P.: A combinatorial problem in the theory of free
monoids. In: Combinatorial Mathematics and its Applications (Proc. Conf., Univ.
North Carolina, Chapel Hill, N.C., 1967), pp. 128–144. University North Carolina
Press, Chapel Hill (1969)
10. Lyndon, R.C., Sch¨utzenberger, M.-P.: The equation am
=
bncp in a free
group. Michigan Math. J. 9(4), 289–298 (1962). https://doi.org/10.1307/mmj/
1028998766
11. Manuch, J.: Defect eﬀect of bi-inﬁnite words in the two-element case. Discret.
Math. Theor. Comput. Sci. 4(2), 273–290 (2001). http://dmtcs.episciences.org/
279
12. Mitrana, V.: Primitive morphisms. Inform. Process. Lett. 64(6), 277–281 (1997).
https://doi.org/10.1016/s0020-0190(97)00178-6
13. Raˇska, M., Starosta, ˇS.: Producing symmetrical facts for lists induced by the list
reversal mapping in Isabelle/HOL (2021). https://arxiv.org/abs/2104.11622
14. Spehner, J.-P.: Quelques probl`emes d’extension, de conjugaison et de presentation
des sous-mono¨ıdes d’un mono¨ıde libre. Ph.D. thesis, Universit´e Paris VII, Paris
(1976)
15. Holub, ˇS., Raˇska, M., Starosta, ˇS.: Combinatorics on words basics. Archive of
Formal Proofs, May 2021. https://isa-afp.org/entries/Combinatorics Words.html.
Formal proof development
16. Holub, ˇS., Raˇska, M., Starosta, ˇS.: Combinatorics on words formalized (release
v1.6) (2022). https://gitlab.com/formalcow/combinatorics-on-words-formalized

Binary Codes that Do Not Preserve Primitivity
387
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Formula Simpliﬁcation via Invariance
Detection by Algebraically Indexed Types
Takuya Matsuzaki(B) and Tomohiro Fujita
Tokyo University of Science, 1-3 Kagurazaka, Shinjuku-ku, Tokyo 162-8601, Japan
matuzaki@rs.tus.ac.jp, 1418097@ed.tus.ac.jp
Abstract. We describe a system that detects an invariance in a logical
formula expressing a math problem and simpliﬁes it by eliminating vari-
ables utilizing the invariance. Pre-deﬁned function and predicate symbols
in the problem representation language are associated with algebraically
indexed types, which signify their invariance property. A Hindley-Milner
style type reconstruction algorithm is derived for detecting the invariance
of a problem. In the experiment, the invariance-based formula simpliﬁ-
cation signiﬁcantly enhanced the performance of a problem solver based
on quantiﬁer-elimination for real-closed ﬁelds, especially on the problems
taken from the International Mathematical Olympiads.
1
Introduction
It is very common to ﬁnd an argument marked by the phrase “without loss of
generality” (w.l.o.g.) in a mathematical proof by human. An argument of this
kind is most often based on the symmetry or the invariance in the problem [9].
Suppose that we are going to prove, by an algebraic method, that the three
median lines of a triangle meet at a point (Fig. 1). Six real variables are needed
to represent three points on a plane. Since the concepts of ‘median lines’ and
‘meeting at a point’ are translation-invariant, we may ﬁx one of the corners at
the origin. Furthermore, because these concepts are also invariant under any
invertible linear map, we may ﬁx the other two points to, e.g., (1, 0) and (0, 1).
Thus, all six variables were eliminated and the task of proof became much easier.
W.l.o.g. arguments may thus have strong impact on the eﬃciency of inference.
It has drawn attention in several research areas including the relative strength of
proof systems (e.g., [2,3,12,20]), propositional SAT (e.g., [1,6,8,17,19]), proof
assistants [9], and algebraic methods for geometry problem solving [7,10].
Among others, Iwane and Anai [10] share exactly the same objective with
us; both aim at solving geometry problems stated in natural language, using
an algebraic method as the backend. Logical formulas resulted from mechanical
translation of problem text tend to be huge and very redundant, while the com-
putational cost of algebraic methods is generally quite sensitive to the size of
the input measured by, e.g., the number of variables. Simpliﬁcation of the input
formula is hence a mandatory part of such a problem-solving system.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 388–406, 2022.
https://doi.org/10.1007/978-3-031-10769-6_24

Simpliﬁcation via Invariance Detection
389
Fig. 1. Variable Elimination w.l.o.g. by Invariance
Iwane and Anai’s method operates on the ﬁrst-order formula of real-closed
ﬁelds (RCFs), i.e., a quantiﬁed boolean combination of equalities and inequalities
between polynomials. They proposed to detect the invariance of a problem by
testing the invariance of the polynomials under translation, scaling, and rotation.
While being conceptually simple, it amounts to discover the geometric property
of the problem solely by its algebraic representation. The detection of rotational
invariance is especially problematic because, to test that on a system of polyno-
mials, one needs to identify all the pairs (or triples) of variables that originate
from the x and y (and z) coordinates of the same points. Thus their algorithm
for 2D rotational invariance already incurs a search among a large number of
possibilities and they left the detection of 3D rotational invariance untouched.
Davenport [7] also suggests essentially the same method.
In this paper, we propose to detect the invariance in a more high-level lan-
guage than that of RCF. We use algebraically indexed types (AITs) proposed by
Atkey et al. [4] as the representation language. In AIT, each symbol in a formula
has a type with indices. An indexed-type of a function indicates that its output
undergoes the same or a related transformation as the input. The invariances
of the functions are combined via type reconstruction and an invariance in a
problem is detected.
The contribution of the current paper is summarized as follows:
1. A type reconstruction algorithm for AIT is derived. Atkey et al. [4] laid out
the formalism of AIT but did not provide a type inference/reconstruction
algorithm. We devised, for a version of AIT, a type reconstruction algorithm
that is based on semantic uniﬁcation in the theory of transformation groups.
2. A set of variable elimination rules are worked out. Type reconstruction in AIT
discerns a more ﬁne-grained notion of invariance than previous approaches.
We derived a set of elimination rules that covers all cases.
3. The practicality of the proposed method is veriﬁed; it signiﬁcantly enhanced
the performance of a problem solver based on quantiﬁer elimination for RCF,
especially on the problems from International Mathematical Olympiads.
In the rest of the paper, we ﬁrst introduce a math problem solver, on which
the proposed method was implemented, and summarize the formalism of AIT.
We then detail the type reconstruction procedure and the variable elimination
rules. We ﬁnally present the experimental results and conclude the paper.

390
T. Matsuzaki and T. Fujita
Fig. 2. Overview of Todai Robot Math Problem Solver
Fig. 3. Example of Manually Formalized Problem (IMO 2012, Problem 5)
2
Todai Robot Math Solver and Problem Library
This work is a part of the development of the Todai Robot Math Problem Solver
(henceforth ToroboMath) [13–16]. Figure 2 presents an overview of the system.
ToroboMath is targeted at solving pre-university math problems. Our long-
term goal is to develop a system that solves problems stated in natural language.
The natural language processing (NLP) module of the system accepts a prob-
lem text and derives its logical representation through syntactic analysis. Cur-
rently, it produces a correct logical form for around 50% of sentences [13], which
is not high enough to cover a wide variety of problems. Although the motiva-
tion behind the current work is to cope with the huge formulas produced by the
NLP module, we instead used a library of manually formalized problems for the
evaluation of the formula simpliﬁcation procedure.
The problem library has been developed along with the ToroboMath sys-
tem. It contains approximately one thousand math problems collected from
several sources including the International Mathematical Olympiads (IMOs).
Figure 3 presents a problem that was taken from IMO 2012.
The problems in the library are manually encoded in a polymorphic higher-
order language, which is the same language as the output of the NLP module.
Table 1 lists some of its primitive types. The language includes a large set of
predicate and function symbols that are tailored for formalizing pre-university
math problems. Currently, 1387 symbols are deﬁned using 2808 axioms. Figure 4
provides an example of the axioms that deﬁnes the predicate maximum.

Simpliﬁcation via Invariance Detection
391
Table 1. Example of Primitive Types
truth values
Bool
numbers
Z (integers), Q (rationals),
R (reals), C (complex)
vectors
2d.Vec, 3d.Vec
geometric objects
2d.Shape, 3d.Shape
angles
2d.Angle, 3d.Angle
sets and lists
SetOf(α), ListOf(α)
Fig. 4. Example of Axiom
The problem solving module of the ToroboMath accepts a formalized
problem and iteratively rewrites it using: (1) basic transformations such as
∀x.(x = α →φ(x)) ⇔φ(α) and beta-reduction, (2) simpliﬁcation of expres-
sions such as polynomial division and integration by computer algebra systems
(CASs), and (3) the axioms that deﬁne the predicate and function symbols.
Once the rewritten formula is in the language of real-closed ﬁelds (RCFs)
or Peano arithmetic, it is handed to a solver for the theory. For RCF formu-
las, we use an implementation of the quantiﬁer-elimination (QE) procedure for
RCF based on cylindrical algebraic decomposition. Finally, we solve the resulting
quantiﬁer-free formula with CASs and obtain the answer. The time complexity
of RCF-QE is quite high; it is doubly exponential in the number of variables [5].
Hence, the simpliﬁcation of the formula before RCF-QE is a crucial step.
3
Algebraically Indexed Types
This section summarizes the framework of AIT. We refrain from presenting it in
full generality and describe its application to geometry ([4, §2]) with the restric-
tion we made on it in incorporating it into the type system of ToroboMath.
In AIT, some of the primitive types have associated indices. An index rep-
resents a transformation on the object of that type. For instance, in Vec⟨B, t⟩,
the index B stands for an invertible linear transformation and t stands for a
translation. The index variables bound by universal quantiﬁers signify that a
function of that type is invariant under any transformations indicated by the
indices, e.g.,
midpoint : ∀B:GL2.∀t:T2. Vec⟨B, t⟩→Vec⟨B, t⟩→Vec⟨B, t⟩.
The type of midpoint certiﬁes that, when two points P and Q undergo an
arbitrary aﬃne transformation, the midpoint of P and Q moves accordingly.
3.1
Sort and Index Expression
The sort of an index signiﬁes the kind of transformations represented by the
index. We assume the set Sort of index sorts includes GLk (k = 1, 2, 3) (general
linear transformations), Ok (k = 2, 3) (orthogonal transformations), and Tk (k =
2, 3) (translations). In the type of midpoint, B is of sort GL2 and t is of sort T2.

392
T. Matsuzaki and T. Fujita
An index expression is composed of index variables and index operators.
In the current paper, we use the following operators: ⟨+, −, 0⟩are addition,
negation, and unit of Tk (k = 2, 3); ⟨· , −1, 1⟩are multiplication, inverse, and unit
of GLk and Ok; det is the determinant; |·| is the absolute value. An index context
Δ is a list of index variables paired with their sorts: Δ = i1:S1, i2:S2, . . . , in:Sn.
The well-sortedness of an index expression e of sort S, written Δ ⊢e : S, is
deﬁned analogously to the well-typedness in simple type theory.
3.2
Type, Term, and Typing Judgement
The set of primitive types, PrimType = {Bool, R, 2d.Vec, 3d.Vec, 2d.Shape,
. . . }, is the same as that in the language of ToroboMath. A function tyArity:
PrimType →Sort∗speciﬁes the number and sorts of indices appropriate for
the primitive types: e.g., tyArity(2d.Vec) = (GL2, T2).
A judgement Δ ⊢A type means that type A is well-formed and well-indexed
with respect to an index context Δ. Here are the derivation rules:
X ∈PrimType
tyArity(X) = (S1, . . . , Sm)
{Δ ⊢ej : Sj}1≤j≤m
Δ ⊢X⟨e1, . . . , em⟩type
TyPrim
Δ ⊢A type
Δ ⊢B type
Δ ⊢A →B type
TyArr
Δ, i:S ⊢A type
Δ ⊢∀i:S.A type TyForall
While Atkey et al.’s system is formulated in the style of System F, we allow
the quantiﬁers only at the outermost (prenex) position. The restriction permits
an eﬃcient type reconstruction algorithm analogous to Hindley-Milner’s, while
being expressive enough to capture the invariance of the pre-deﬁned functions
in ToroboMath and the invariance in the majority of math problems.
The well-typedness of a term M, written Δ; Γ ⊢M : A, is judged with respect
to an index context Δ and a typing context Γ = x1 : A1, . . . , xn : An. A typing
context is a list of variables with their types. A special context Γops consists of the
pre-deﬁned symbols and their types, e.g., + : ∀s:GL1. R⟨s⟩→R⟨s⟩→R⟨s⟩∈Γops.
We assume Γops is always available in the typing derivation and suppress it in
a judgement. The typing rules are analogous to those for lambda calculus with
rank-1 polymorphism except for TyEQ:
x : A ∈Γ
Δ; Γ ⊢x : A Var
Δ; Γ ⊢M : ∀i:S.A
Δ ⊢e:S
Δ; Γ ⊢M : A{i →e}
UnivInst
Δ; Γ, x : A ⊢M : B
Δ; Γ ⊢λx.M : A →B Abs
Δ; Γ ⊢M : A →B
Δ; Γ ⊢N : A
Δ; Γ ⊢MN : B
App
Δ; Γ ⊢M : A
Δ ⊢A ≡B
Δ; Γ ⊢M : B
TyEQ
In the Abs and App rules, the meta-variables A and B only designate a type
without quantiﬁers. In the UnivInst rule, A{i 	→e} is the result of substituting
e for i in A. The ‘polymorphism’ of the types with quantiﬁers hence takes place
only when a pre-deﬁned symbol (e.g., midpoint) enters a derivation via the Var
rule and then the bound index variable is instantiated via the UnivInst rule.

Simpliﬁcation via Invariance Detection
393
The type equivalence judgement Δ ⊢A ≡B in the TyEQ rule equates two
types involving semantically equivalent index expressions; thus, e.g., s:GL1 ⊢
R⟨s · s−1⟩≡R⟨1⟩and O:O2 ⊢R⟨| det O|⟩≡R⟨1⟩.
3.3
Index Erasure Semantics and Transformational Interpretation
The abstraction theorem for AIT [4] enables us to know the invariance of a term
by its type. The theorem relates two kinds of interpretations of types and terms:
index erasure semantics and relational interpretations. We will restate the the-
orem with what we here call transformational interpretations (t-interpretations
hereafter), instead of the relational interpretations. It suﬃces for the purpose of
justifying our algorithm and makes it easier to grasp the idea of the theorem.
The index-erasure semantics of a primitive type X⟨e1, . . . , en⟩is determined
only by X. We thus write ⌊X⟨e1, . . . , en⟩⌋= ⌊X⌋. The interpretation ⌊X⌋is
the set of mathematical objects intended for the type: e.g., ⌊2d.Vec⟨B, t⟩⌋=
⌊2d.Vec⌋= R2 and ⌊R⟨s⟩⌋= ⌊R⌋= R. The index-erasure semantics of a non-
primitive type is determined by the type structure: ⌊A →B⌋= ⌊A⌋→⌊B⌋and
⌊∀i:S. T⌋= ⌊T⌋.
The index-erasure semantics of a typing context Γ = x1:T1, . . . , xn:Tn is the
direct product of the domains of the variables: ⌊Γ⌋= ⌊T1⌋× · · · × ⌊Tn⌋. The
erasure semantics of a term Δ; Γ ⊢M : A is a function of the values assigned to
its free variables: ⌊M⌋: ⌊Γ⌋→⌊A⌋and deﬁned as usual (see, e.g., [18,21]).
The t-interpretation of a type T, denoted by T, is a function from the assign-
ments to the index variables to a transformation on ⌊T⌋. To be precise, we ﬁrst
deﬁne the semantics of index context Δ = i1:S1, . . . , in:Sn as the direct product
of the interpretation of the sorts: Δ = S1 × · · · × Sn, where S1, . . . , Sn
are the intended sets of transformations: e.g., GL2 = GL2 and T2 = T2. The
interpretation of an index expression e of sort S is a function e : Δ →S
that is determined by the structure of the expression; for ρ ∈Δ,
f(e1, . . . , en)(ρ) = f(e1(ρ), . . . , en(ρ)),
ik(ρ) = ρ(ik),
where, in the last equation, we regard ρ ∈Δ as a function from index variables
to their values. The index operations det and | · | are interpreted as intended.
The t-interpretation of a primitive type X⟨e1, . . . , en⟩is then determined by
X and the structures of the index expressions e1, . . . , en. The t-interpretation
of Vec and Shape is the aﬃne transformation of vectors and geometric objects
parametrized by ρ ∈Δ; for index expressions β:GL2 and τ:T2,
Vec⟨β, τ⟩(ρ) : R2 ∋x 	→Mβ(ρ)x + vτ(ρ) ∈R2
Shape⟨β, τ⟩(ρ) : P(R2) ∋S 	→{Mβ(ρ)x + vτ(ρ) | x ∈S} ∈P(R2),
where Mβ(ρ) and vτ(ρ) are the representation matrix and vector of β(ρ) and
t(ρ), and P(R2) denotes the power set of R2. Similarly, for the real numbers,
R⟨σ⟩(ρ) : R ∋x 	→σ(ρ)x ∈R.

394
T. Matsuzaki and T. Fujita
That is, R⟨σ⟩(ρ) is a change of scale with the scaling factor determined by the
expression σ:GL1 and the assignment ρ. For a primitive type X with no indices,
its t-interpretation is the identity map on ⌊X⌋: i.e., X(ρ) = id⌊X⌋.
The t-interpretation of a function type A →B is a higher-order function that
maps a (mathematical) function f : ⌊A⌋→⌊B⌋to another function on the same
domain and codomain such that: A →B(ρ)(f) = B(ρ) ◦f ◦(A(ρ))−1. It is
easy to check that this interpretation is compatible with currying. Equivalently,
we may say that if g = A →B(ρ)(f), then f and g are in the commutative
relation g ◦A(ρ) = B(ρ) ◦f. The typing derivation in AIT is a way to ‘pull
out’ the eﬀect of transformation A(ρ) on a free variable deep inside a term by
combining such commutative relations.
The t-interpretation of a fully-quantiﬁed type is the identity map on its era-
sure semantics: ∀i1:S1. . . . ∀in:Sn. T = id⌊T ⌋. We don’t deﬁne that of partially-
quantiﬁed types because we don’t need it to state the abstraction theorem.
3.4
Abstraction Theorem
The abstraction theorem for AIT enables us to detect the invariance of (the
erasure-semantics of) a term under a certain set of transformations on its free
variables. We ﬁrst deﬁne the t-interpretation of the typing context Γ = x1 :
T1, . . . , xn : Tn as a simultaneous transformation of η = (v1, . . . , vn) ∈⌊Γ⌋:
Γ(ρ) : ⌊Γ⌋∋η 	→Γ(ρ) ◦η = (T1(ρ) ◦v1, . . . , Tn(ρ) ◦vn) ∈⌊Γ⌋.
We now present a version of the abstraction theorem, restricted to the case of a
term of quantiﬁer-free type and restated with the t-interpretation:
Theorem 1 (Abstraction [4], restated using transformational interpretation).
If A is a quantiﬁer-free type and Δ; Γ ⊢M : A, then for all ρ ∈Δ and all
η ∈⌊Γ⌋, we have A(ρ) ◦⌊M⌋(η) = ⌊M⌋(Γ(ρ) ◦η).
Here we provide two easy corollaries of the theorem. The ﬁrst one is utilized
to eliminate variables from a formula while preserving the equivalence.
Corollary 1. If Δ; x1 : T1, . . . , xn : Tn ⊢φ(x1, . . . , xn) : Bool, then for all
ρ ∈Δ, we have φ(x1, . . . , xn) ⇔φ(T1(ρ) ◦x1, . . . , Tn(ρ) ◦xn).
This is by the abstraction theorem and the fact Bool(ρ) = id⌊Bool⌋for any ρ.
It indicates that, without loss of generality, we may ‘ﬁx’ some of the variables
to, e.g., zeros by appropriately choosing ρ.
The second corollary is for providing more intuition about the theorem.
Corollary 2. If ϵ; ϵ ⊢λx1. . . . .λxn. f(x1, . . . , xn) : ∀Δ. T1 →· · · →Tn →T0
then, for all ρ ∈Δ and all vi ∈⌊Ti⌋(i = 1, . . . , n),
T0(ρ) ◦⌊f⌋(v1, . . . , vn) = ⌊f⌋(T1(ρ) ◦v1, . . . , Tn(ρ) ◦vn).
In the statement, ∀Δ signiﬁes the universal quantiﬁcation over all index variables
in Δ. By this corollary, for instance, we can tell from the type of midpoint that,
for all x1, x2 ∈R2 and for all g ∈GL2 and t ∈T2,
⌊midpoint⌋(Mgx1 + vt, Mgx2 + vt) = Mg ⌊midpoint⌋(x1, x2) + vt.

Simpliﬁcation via Invariance Detection
395
3.5
Restriction on the Index Expressions of Sort GLk/Ok (k ≥2)
We found that the type reconstruction in AIT is far more straightforward when
we assume an index expression of sort GLk or Ok (k ≥2) includes at most
one index variable of sort GLk or Ok that is not in the determinant operator.
Assuming this, any expression e of sort GLk or Ok can be written in the form of
e =

i∈I
swi
i
·

i∈I
|si|xi ·

j∈J
det(Bj)yj ·

j∈J
| det(Bj)|zj · Bδ
0,
where {si}i∈I are of sort GL1, {B0}∪{Bj}j∈J are of sort GLk or Ok, wi, xi, yj, zj ∈
Z, and δ ∈{0, 1}. We henceforth say an expression e in the above form satisﬁes
the head variable property and call B0 the head variable of e.
Empirically, this restriction is not too restrictive; as far as we are aware of,
the invariance of all the pre-deﬁned functions and predicates in ToroboMath
is expressible with an indexed-type satisfying this.
4
Invariance Detection Through Type Reconstruction
We need type reconstruction in AIT for two purposes: to infer the invariance of
the pre-deﬁned symbols in ToroboMath and to infer the invariance in a math
problem. To this end, we only have to derive the judgement Δ; Γ ⊢φ : Bool
where φ is either a deﬁning axiom of a symbol or a formula of a problem. For
a pre-deﬁned symbol s, by a judgement Δ; s : T, · · · ⊢φ : Bool, we know s
is of type T and it has the invariance signiﬁed by T. For a problem φ, by the
judgement Δ; x1 : T1, . . . , xn : Tn ⊢φ : Bool, we know the invariance of φ under
the transformation on the free variables x1, . . . , xn according to T1, . . . , Tn.
Since all types are in prenex form, we can ﬁnd the typing derivation by
a procedure analogous to the Hindley-Milner (H-M) algorithm. It consists of
two steps: deriving equations among index expressions, and solving them. The
procedure for solving the equations in T2/T3 is essentially the same as in the
type inference for Kennedy’s unit-of-measure types [11], which is a precursor of
AIT. Further development is required to solve the equations in GL2/GL3, even
under the restriction on the form of index expressions mentioned in Sect. 3.5,
due to the existence of the index operations | · | and det.
4.1
Equation Derivation
We ﬁrst assign a type variable αi for each subterm ti in φ. Then, for a subterm ti
in the form tjtk (i.e., application of tj to tk), we have the equation αj = αk →αi.
The case for a subterm ti in the form of λx.tj is also analogous to H-M and we
omit it here. For a leaf term (i.e., a variable) ti, if it is one of the pre-deﬁned
symbols and ti : ∀i1:S1. . . . ∀in:Sn.T ∈Γops, we set αi = T{i1 	→β1, . . . , in 	→
βn}, where {i1 	→β1, . . . , in 	→βn} stands for the substitution of fresh variables
β1, . . . , βn for i1, . . . , in. By solving the equations for the type and index variables
{αi} and {βj}, we reconstruct the most general indexed-types of all the subterms.

396
T. Matsuzaki and T. Fujita
For example, consider the following axiom deﬁning perpendicular:
∀v1.∀v2.(perpendicular(v1, v2) ←→inner-prod(v1, v2) = 0),
and suppose that inner-prod is in Γops. We are going to reconstruct the type
of perpendicular. The type of inner-prod is
inner-prod : ∀s1, s2:GL1. ∀O:O2. Vec⟨s1O, 0⟩→Vec⟨s2O, 0⟩→R⟨s1 · s2⟩
and it is instantiated as inner-prod : Vec⟨s1O, 0⟩→Vec⟨s2O, 0⟩→R⟨s1 · s2⟩
where s1, s2, and O are fresh variables. Since the type of perpendicular in the
non-AIT version of our language is Vec →Vec →Bool, we set fresh variables to
all indices in the primitive types and have:
perpendicular : Vec⟨β1, τ1⟩→Vec⟨β2, τ2⟩→Bool.
Since perpendicular is applied to v1 and v2, the types of v1 and v2 are
equated to Vec⟨β1, τ1⟩and Vec⟨β2, τ2⟩. Additionally, since inner-prod is also
applied to v1 and v2, we have the following equations:
Vec⟨s1O, 0⟩= Vec⟨β1, τ1⟩,
Vec⟨s2O, 0⟩= Vec⟨β2, τ2⟩
(4.1)
If we have an equation between the same primitive type, by unifying both sides of
the equation, in turn we have one or more equations between index expressions,
i.e., if we have X⟨e1, . . . , em⟩= X⟨e′
1, . . . , e′
m⟩, then we have: e1 = e′
1, . . . , em =
e′
m. For Eq. (4.1), we hence have s1O = β1, s2O = β2, 0 = τ1, and 0 = τ2.
Thus, by recursively unifying all the equated types, we are left with a system of
equations between index expressions.
4.2
Equation Solving
To solve the derived equations between index expressions, we need to depart
from the analogy with the H-M algorithm. Namely, instead of applying syn-
tactic uniﬁcation, we need semantic uniﬁcation, i.e., we solve the equations as
simultaneous equations in the transformation groups.
We ﬁrst order the equations with respect to the sort of the equated expres-
sions. We then process them in the order T2/T3 →GL2/GL3 →GL1 as follows.1
First, since equations of sort T2/T3 are always in the form of 
i aiti =
0 (ai ∈Z), where {ti} are variables of sort Tk (k ∈{2, 3}), we can solve the
equations as is the case with a linear homogeneous system. Although the solution
may involve rational coeﬃcients as in ti = 
j
nij
mij tj (nij, mij ∈Z), we can clear
the denominators by introducing new variables t′
j such that tj = lcm{mij}i · t′
j.
Next, by the head variable property, equations of sort GL2/GL3 (henceforth
GL≥2) are always in the form of σ1B1 = σ2B2, where σ1 and σ2 are index
1 In this subsection, GL2, GL3, O2, and O3 are collectively denoted as GL2/GL3 or GL≥2.

Simpliﬁcation via Invariance Detection
397
expressions of sort GL1, and B1 and B2 are the head variables of sort GL≥2. We
decompose these equations according to Table 2, which summarizes the following
argument: Let E denote the identity transformation. Since σ1B1 = σ2B2 ⇐⇒
σ−1
1 σ2E = B1B−1
2 , there must be some s ∈GL1 such that B1B−1
2
= sE and
σ−1
1 σ2 = s. Furthermore, by the superset-subset relation between the sorts of
B1 and B2, e.g., O2 ⊂GL2 for B1 : O2 and B2 : GL2, we can express one of the
broader sort with the other as a parameter.
The algorithm for GL≥2 equations works as follows. First, we initialize the
set of solution with the empty substitution: S ←{}. For each GL≥2 equation
σ1B1 = σ2B2, we look up Table 2 and ﬁnd the GL≥2 solution Bi 	→sBj and one
or more new GL1 equations. We populate the current set of GL1 equations with
the new ones, and apply the solution Bi 	→sBj to all the remaining GL1 and
GL≥2 equations. We also compose the GL2 solution Bi 	→sBj with the current
solution set: S ←S ◦{Bi 	→sBj}.
By processing all GL≥2 equations as above, we are left with a partial solution
S and a system of GL1 equations, each of which is in the following form:

i∈I
swi
i
·

i∈I
|si|xi ·

j∈J
det(Bj)yj ·

j∈J
|det(Bj)|zj = 1
(wi, xi, yj, zj ∈Z),
where we assume about I and J that {si}i∈I are all the GL1 variables, {Bj}j∈J
are all the remaining GL≥2 variables, and I ∩J = ∅. Letting ui = si · |si|−1,
vi = |si|, uj = det(Bj) · |det(Bj)|−1, and vi = |det(Bj)|, we have si = uivi and
det(Bj) = ujvj for all i ∈I and j ∈J. By using them, we have

i
uwi
i
·

i
vwi+xi
i
·

j
uyj
j ·

j
vyj+zj
j
= 1.
Since ui, uj ∈{+1, −1} and vi, vj > 0 for all i and j, we know the above equation
is equivalent to the following two equations:

i
uwi
i
·

j
uyj
j = 1,

i
vwi+xi
i
·

j
vyj+zj
j
= 1.
We thus have two systems of equations, one in {+1, −1} and the other in R>0.
Now we temporarily rewrite the solution with ui and vi: S ←S ◦{si 	→uivi}i∈I.
First consider the system in R>0. As long as there remains an equation
involving a variable vi, which originates from a GL1 variable, we solve it for vi
and compose the solution vi 	→
i′̸=i vpi′
i′ ·
j vqj
j with S while applying it to the
remaining equations. The denominators of fractional exponents (i.e., pi′, qj ∈
Q\Z) can be cleared similarly to the case of Tk equations. If all the equations in
R>0 are solved this way, then S is the most general solution. Otherwise, there
remain one or more equations of the form 
j∈J′ | det Bj|dj = 1 for some J′ ⊂J
and {dj}j∈J′. This is the only case where we may miss some invariance of a
formula; in general, we cannot express the most general solution to this equation
only with the index variables of sort GLk and Ok. We make a compromise here
and are satisﬁed with a less general solution S ◦{Bj 	→E}j∈J′. Fortunately, this

398
T. Matsuzaki and T. Fujita
Table 2. Decomposition of GL2/GL3 equation σiBi = σjBj (s: a fresh variable)
Combination of head variables
Solution in GLk
Equations in GL1
Bi = Bj
none
σi = σj
Bi : Ok ∧Bj = E
Bi →sE
sσi = σj, |s| = 1
Bi : GLk ∧Bj = E
Bi →sE
sσi = σj
Bi : Ok ∧Bj : Ok
Bi →sBj
sσi = σj, |s| = 1
Bi : GLk ∧Bj : Ok
Bi →sBj
sσi = σj
Bi : GLk ∧Bj : GLk
Bi →sBj
sσi = σj
does not frequently happen in practice. We made this compromise only on three
out of 533 problems used in the experiment. We expect that having more sorts,
e.g., SL±
k = {M ∈GLk | | det M| = 1}, in the language of index expressions
might be of help here, but leave it as a future work.
The system in {+1, −1} is processed analogously to that in R>0. Finally, by
restoring {ui, vi}i∈I and {uj, vj}j∈J in the solution S to their original forms,
e.g., ui 	→si · |si|−1, we have a solution to the initial set of equations in terms
of the variables of sort GLk and Ok.
4.3
Type Reconstruction for Pre-deﬁned Symbols with Axioms
We incrementally determined the indexed-types of the pre-deﬁned symbols
according to the hierarchy of their deﬁnitions. We ﬁrst constructed a directed
acyclic graph wherein the nodes are the pre-deﬁned symbols and the edges repre-
sent the dependency between their deﬁnitions. We manually assigned an indexed-
type to the symbols without deﬁning axioms (e.g., + : R →R →R) and initialized
Γops with them. We then reconstructed the indexed-types of other symbols in
a topological order of the graph. After the reconstruction of the type of each
symbol, we added the symbol with its inferred type to Γops.
For some of the symbols, type reconstruction does not go as well as we hope.
For example, the following axiom deﬁnes the symbol midpoint:
∀p1, p2.(midpoint(p1, p2) = 1
2 · (p1 + p2)).
At the beginning of the type reconstruction of midpoint, the types of the symbols
in the axiom are instantiated as follows:
midpoint : Vec⟨β1, τ1⟩→Vec⟨β2, τ2⟩→Vec⟨β3, τ3⟩
· : R⟨s1⟩→Vec⟨B1, 0⟩→Vec⟨s1B1, 0⟩
+ : Vec⟨B2, t1⟩→Vec⟨B2, t2⟩→Vec⟨B2, t1 + t2⟩.
The derived equations between the index expressions are as follows:
{B2 = β1, B2 = β2, B1 = B2, β3 = s1B1, s1 = 1, t1 = τ1, t2 = τ2, 0 = t1 + t2, τ3 = 0}.

Simpliﬁcation via Invariance Detection
399
By solving these equations, we obtain the indexed-type of midpoint as follows:
midpoint : ∀B1:GL2. ∀t1:T2. Vec⟨B1, t1⟩→Vec⟨B1, −t1⟩→Vec⟨B1, 0⟩.
This type indicates that the midpoint of any two points P and Q remains the
same when we move P and Q respectively to P + t1 and Q −t1 for any t1 ∈R2.
While it is not wrong, the following type is more useful for our purpose:
midpoint : ∀B:GL2. ∀t:T2. Vec⟨B, t⟩→Vec⟨B, t⟩→Vec⟨B, t⟩.
(1)
To such symbols, we manually assigned a more appropriate type.2
In the current system, 945 symbols have a type that includes indices. We
manually assigned the types to 255 symbols that have no deﬁning axioms. For 203
symbols we manually overwrote the inferred type as in the case of midpoint. The
types of the remaining 487 symbols were derived through the type reconstruction.
5
Variable Elimination Based on Invariance
In this section, we ﬁrst provide an example of the variable elimination procedure
based on invariance. We then describe the top-level algorithm of the variable
elimination, which takes a formula as input and eliminates some of the quantiﬁed
variables in it by utilizing the invariance indicated by an index variable. We
ﬁnally list the elimination rule for each sort of index variable.
5.1
Example of Variable Elimination Based on Invariance
Let us consider again the proof of the existence of the centroid of a triangle. For
triangle ABC, the conﬁguration of the midpoints P, Q, R of the three sides and
the centroid G is described by the following formula:
ψ(A, B, C, P, Q, R, G) :=
⎛
⎝
P = midpoint(B, C) ∧on(G, segment(A, P)) ∧
Q = midpoint(C, A) ∧on(G, segment(B, Q)) ∧
R = midpoint(A, B) ∧on(G, segment(C, R))
⎞
⎠
where on(X, Y ) stands for the inclusion of point X in a geometric object Y ,
and segment(X, Y ) stands for the line segment between points X and Y . Let φ
denote the existence of the centroid (and the three midpoints):
φ(A, B, C) := ∃G. ∃P. ∃Q. ∃R. ψ(A, B, C, P, Q, R, G).
Our goal is to prove ∀A. ∀B. ∀C. φ(A, B, C).
2 The awkwardness of the type inferred for midpoint is a price for the eﬃciency of
type reconstruction; it is due to the fact that we ignore the linear space structure of
T2 (and also, we do not posit T1(≃R) as the second index of type R). Otherwise,
the type reconstruction comes closer to a search for an invariance on the algebraic
representation of the problems and the deﬁning axioms. Hence 1/2 ∗(t + t) = t is
not deduced for t : T2, which is necessary to infer the type in Eq. (1).

400
T. Matsuzaki and T. Fujita
The functions midpoint, on, and segment are invariant under translations
and general linear transformations. The reconstruction algorithm hence derives
β : GL2, τ : T2 ; A : Vec⟨β, τ⟩, B : Vec⟨β, τ⟩, C : Vec⟨β, τ⟩⊢φ(A, B, C) : Bool.
By the abstraction theorem, this judgement implies the invariance of the
proposition φ(A, B, C) under arbitrary aﬃne transformations:
∀g ∈GL2. ∀t ∈T2. ∀A, B, C. φ(A, B, C) ⇔φ(t ◦g ◦A, t ◦g ◦B, t ◦g ◦C).
First, by considering the case of g being identity, we have
∀t ∈T2. ∀A, B, C. φ(A, B, C) ⇔φ(t ◦A, t ◦B, t ◦C).
(2)
By using this, we are going to verify ∀B, C. φ(0, B, C) ⇔∀A, B, C. φ(A, B, C),
by which we know that we only have to prove ∀B, C. φ(0, B, C).
Suppose that ∀B, C. φ(0, B, C) holds. Since T2 acts transitively on R2, for
any A ∈R2, there exists t ∈T2 such that t ◦0 = A. Furthermore, for any
B, C ∈R2, by instantiating ∀B, C. φ(0, B, C) with B 	→t−1◦B and C 	→t−1◦C,
we have φ(0, t−1 ◦B, t−1 ◦C). By Eq. (2), we obtain φ(t◦0, t◦t−1 ◦B, t◦t−1 ◦C),
which is equivalent to φ(A, B, C). Since A, B, C were arbitrary, we proved
∀B, C. φ(0, B, C) ⇒∀A, B, C. φ(A, B, C).
The converse is trivial.
We thus proved ∀B, C. φ(0, B, C)
⇔
∀A, B,
C. φ(A, B, C).
The simpliﬁed formula, ∀B, C. φ(0, B, C), is still invariant under the simul-
taneous action of GL2 on B and C. Hence, by applying the type reconstruction
again, we have β : GL2 ; B : Vec⟨β, 0⟩, C : Vec⟨β, 0⟩⊢φ(0, B, C) : Bool. It
implies the following invariance: ∀g ∈GL2. ∀B, C. φ(0, B, C) ⇔φ(0, g◦B, g◦C).
We now utilize it to eliminate the remaining variables B and C. Although it
is tempting to ‘ﬁx’ B and C respectively at, e.g., e1 := (1, 0) and e2 := (0, 1), it
incurs some loss of generality. For instance, when B is at the origin, there is no
way to move B to e1 by any g ∈GL2. We consider four cases:
1. B and C are linearly independent,
2. B ̸= 0, and B and C are linearly dependent,
3. C ̸= 0, and B and C are linearly dependent, and
4. B and C are both at the origin.
For each of these cases, we can ﬁnd a suitable transformation in GL2 as follows:
1. There exists g1 ∈GL2 s.t. g1 ◦B = e1 and g1 ◦C = e2,
2. There exist g2 ∈GL2 and r ∈R s.t. g2 ◦B = e1 and g2 ◦C = re1,
3. There exist g3 ∈GL2 and r′ ∈R s.t. g3 ◦C = e1 and g3 ◦B = r′e1, and
4. We only have to know whether or not φ(0, 0, 0) holds.
By a similar argument to the one for the translation-invariance, we have
∀B, C. φ(0, B, C) ⇔φ(0, e1, e2)∧∀r. φ(0, e1, re1)∧∀r′. φ(0, r′e1, e1)∧φ(0, 0, 0).
Thus, we eliminated all four coordinate values (i.e., x and y coordinates for B
and C) in the ﬁrst and the last case and three of them in the other two cases.

Simpliﬁcation via Invariance Detection
401
5.2
Variable Elimination Algorithm
The variable elimination algorithm works as follows. We traverse the formula of
a problem in a top-down order and, for each subformula in the form of
Qx1.Qx2. · · · Qxn. φ(x1, x2, . . . , xn, y)
(Q ∈{∀, ∃})
where y = y1, . . . , ym are the free variables, we apply the type reconstruction
procedure to φ(x1, x2, . . . , xn, y) and derive a judgement Δ; Γ, x1:T1, . . . , xn:Tn ⊢
φ(x1, . . . , xn, y) : Bool. We then choose an index variable i that appears at least
once in T1, . . . , Tn but in none of the types of y. It means the transformation
signiﬁed by i acts on some of {x1, . . . , xn} but on none of y. We select from
{x1, . . . , xn} one or more variables whose types include i and are of the form R⟨σ⟩
or Vec⟨β, τ⟩. Suppose that we select x1, . . . , xl. Then we know the judgement
Δ; Γ, x1:T1, . . . , xl:Tl ⊢Qxl+1. · · · Qxn. φ(x1, . . . , xn, y) : Bool also holds. We
then eliminate (or add restriction on) the bound variables x1, . . . , xl by one of
the lemmas in Sect. 5.3 according to the sort of i. After the elimination, the
procedure is recursively applied to the resulting formula and its subformulas.
5.3
Variable Elimination Rules
We now present how to eliminate variables based on a judgement of the form
Δ; Γ, x1 : T1, . . . , xn : Tn ⊢ψ(x1, . . . , xn, y) : Bool
where T1, . . . , Tn include no other variables than i; Γ = y1:U1, . . . , ym:Um is a
typing context for y = y1, . . . , ym; and U1 . . . , Um do not include i. Note that
we can obtain a judgement of this form by the procedure in Sect. 5.2 and by
substituting the unity of appropriate sorts for all index variables other than i in
T1, . . . , Tn.
We provide the variable elimination rules as lemmas, one for each sort of i.
They state the rules for variables bound by ∀. The rules for ∃are analogous.
In stating the lemma, we suppress Δ and Γ in the judgement and y in ψ for
brevity but we still assume the above-mentioned condition hold.
Some complication arises due to the fact that if k ̸= l, then Tk and Tl may
be indexed with diﬀerent expressions of i. We thus need to consider poten-
tially diﬀerent transformations T1(i), . . . , Tn(i) applied simultaneously on
x1, . . . , xn. Please refer to supplementary material on the ﬁrst author’s web page
for a general argument behind the rules and the proofs of the lemmas (https://
researchmap.jp/mtzk/?lang=en).
Tk: The following lemma states that, as we saw in Sect. 5.1, we have only to
consider the truth of a formula ψ(x) at x = 0 if ψ(x) is translation-invariant.
Lemma 1. If x : Vec⟨1, τ(t)⟩⊢ψ(x) : Bool holds for t : Tk (t ∈{2, 3}), then
∀x. ψ(x) ⇔ψ(0).
O2: The following lemma means that we may assume x is on the x-axis if ψ(x)
is invariant under rotation and reﬂection.

402
T. Matsuzaki and T. Fujita
Lemma 2. If x : Vec⟨β(O), 0⟩⊢ψ(x) : Bool holds for O : O2, then ∀x. ψ(x) ⇔
∀r. ψ(re1).
O3: A judgement in the following form implies diﬀerent kinds of invariance
according to β1 and β2:
x1 : Vec⟨β1(O), 0⟩, x2 : Vec⟨β2(O), 0⟩⊢ψ(x1, x2) : Bool.
(3)
In any case, we may assume x1 is on the x-axis and x2 is on the xy-plane for
proving ∀x1, x2. ψ(x1, x2), as stated in the following lemma.
Lemma 3. If judgement (3) holds for O : O3, then
∀x1. ∀x2. ψ(x1, x2) ⇔∀p, q, r ∈R. ψ(pe1, qe1 + re2).
GL1: For s : GL1, a judgement x : R⟨σ(s)⟩⊢ψ(x) : Bool implies, either
– ψ(x) is invariant under change of sign, i.e., ψ(x) ⇔ψ(−x),
– ψ(x) is invariant under positive scaling, i.e., ψ(x) ⇔ψ(fx) for all f > 0, or
– ψ(x) is invariant under arbitrary scaling, i.e., ψ(x) ⇔ψ(fx) for all f ̸= 0.
The form of σ determines the type of invariance. The following lemma summa-
rizes how we can eliminate or restrict a variable for these cases.
Lemma 4. Let σ(s) = se · |s|f
(e ̸= 0 or f ̸= 0) and suppose a judgement
x : R⟨σ(s)⟩⊢ψ(s) : Bool holds for s : GL1. We have three cases:
1. if e + f = 0, then ∀x. ψ(x) ⇔∀x ≥0. ψ(x), otherwise,
2. if e is an even number, then ∀x. ψ(x) ⇔ψ(1) ∧ψ(0) ∧ψ(−1), and
3. if e is an odd number, then ∀x. ψ(x) ⇔ψ(1) ∧ψ(0).
GL2 For B : GL2, a judgement in the following form implies diﬀerent kinds of
invariance of ψ(x1, x2) depending on the form of β1 and β2:
x1 : Vec⟨β1(B), 0⟩, x2 : Vec⟨β2(B), 0⟩⊢ψ(x1, x2).
(4)
The following lemma summarizes how we eliminate the variables in each case.
Lemma 5. Let βj(B) = det(B)ej ·| det(B)|fj ·B and gj = ej +fj (j ∈{1, 2}). If
judgement (4) holds, then, letting ψ0 := ψ(0, 0) ∧∀r. ψ(re1, e1) ∧∀r. ψ(e1, re1)
and Ψ := ∀x1. ∀x2. ψ(x1, x2), the following equivalences hold:
1. If g1 + g2 + 1 = 0 and
– if e1 + e2 is an even number, then Ψ ⇔ψ0 ∧ψ(e1, e2)
– if e1 + e2 is an odd number, then Ψ ⇔ψ0 ∧ψ(e1, e2) ∧ψ(e1, −e2)
2. If g1 + g2 + 1 ̸= 0, then Ψ ⇔ψ0 ∧∀r. ψ(re1, e2).
A similar lemma holds for the invariances indicated by an index variable of sort
GL3. We refrain from presenting it for space reasons.

Simpliﬁcation via Invariance Detection
403
Table 3. Results on All RCF Problems
in ToroboMath Benchmark
Division/#Prblms
AlgIdx
Baseline
Solved Time Solved Time
IMO
116
28%
51.7s 16%
19.7s
Univ
243
69%
22.1s 62%
26.8s
Chart
174
68%
9.7s
62%
12.0s
All
533
60%
20.5s 52%
20.6s
Table 4. Results on RCF Problems with
Invariance Detected and Variable Elimi-
nated
Division/#Prblms
AlgIdx
Baseline
Speed
Solved Time Solved Time up
IMO
77
19%
91.3s 1%
3.6s
23%
Univ
49
57%
31.0s 33%
62.7s 495%
Chart 77
49%
14.3s 36%
26.0s 529%
All
203
40%
34.3s 22%
38.5s 505%
Fig. 5. Comparison of Elapsed Time with and without the Invariance Detection based
on AITs (Left: All Problems; Right: Problems Solved within 60 s)
6
Experiment
We evaluated the eﬀectiveness of the proposed method on the pre-university
math problems in the ToroboMath benchmark. We used a subset of the prob-
lems that can be naturally expressible (by human) in the language of RCF.
Most of them are either in geometry or algebra. Note that the formalization was
done in the language introduced in Sect. 2 but not directly in the language of
RCF. The problems are divided according to the source of the problems; IMO
problems were taken from past International Mathematical Olympiads, Univ
problems were from entrance exams of Japanese universities, and Chart prob-
lems were from a popular math practice book series. Please refer to another
paper [16] on the ToroboMath benchmark for the details of the problems.
The type reconstruction and formula simpliﬁcation procedures presented in
Sect. 4 and Sect. 5 were implemented as a pre-processor of the formalized prob-
lems. The time spent for the preprocessing was almost negligible (0.76 s per
problem on average) compared to that for solving the problems.
We compared the ToroboMath system with and without the pre-processor
(respectively called AlgIdx and Baseline below). The Baseline system is
equipped with Iwane and Anai’s invariance detection and simpliﬁcation algorithm
[10] that operates on the language of RCF while AlgIdx is not with it. Thus, our
evaluation shall reveal the advantage of detecting and exploiting the invariance of
the problem expressed in a language that directly encodes its geometric meaning.

404
T. Matsuzaki and T. Fujita
Table 5. Percentage of Problems from
which one or more Variables are Elimi-
nated by the Rule for each Sort
GL1 T2
O2
GL2 T3 O3 GL3 any
22.3 27.4 26.1 1.7
6.6 7.5 0.0
38.1
Table 6. Most Frequent Invariance Types
Detected and Eliminated
Table 3 presents the results on all problems. The solver was run on each
problem with a time limit of 600 s. The table lists the number of problems, the
percentages of the problems solved within the time limit, and the average wall-
clock time spent on the solved problems. The number of the solved problems is
signiﬁcantly increased in the IMO division. A modest improvement is observed
in the other two divisions. Table 4 presents the results only on the problems in
which at least one variable was eliminated by AlgIdx. The eﬀect of the proposed
method is quite clearly observed across all problem divisions and especially on
IMO. On IMO, the average elapsed time on the problems solved by AlgIdx is
longer than that by Baseline; it is because more diﬃcult problems were solved
by AlgIdx within the time limit. In fact, the average speed-up by AlgIdx (last
column in Table 4) is around 500% on Univ and Chart; i.e., on the problems
solved by both, AlgIdx output the answer ﬁve times faster than Baseline.
A curious fact is that both AlgIdx and Baseline tended to need more time
to solve the problems on which an invariance was detected and eliminated by
AlgIdx (i.e., Time in Table 4) than the average over all solved problems (Time
in Table 3). It suggests that a problem having an invariance, or equivalently a
symmetry, is harder for automatic solvers than those without it.
Figure 5 shows a comparison of the elapsed time for each problem. Each
point represents a problem, and the x and y coordinates respectively indicate
the elapsed time to solve (or to timeout) by Baseline and AlgIdx. We can
see many problems that were not solved by Baseline within 600 s were solved
within 300 s by AlgIdx. The speed-up is also observed on easier problems (those
solved in 60 s) as can be seen in the right panel of Fig. 5.
Table 5 lists the fraction of problems on which one or more variables are
eliminated based on the invariance indicated by an index variable of each sort.
Table 6 provides the distribution of the combination of the sorts of invariances
detected and eliminated by AlgIdx.
7
Conclusion
A method for automating w.l.o.g. arguments on geometry problems has been
presented. It detects an invariance in a problem through type reconstruction in
AIT and simpliﬁes the problem utilizing the invariance. It was especially eﬀective
on harder problems including past IMO problems. Our future work includes the

Simpliﬁcation via Invariance Detection
405
exploration for a more elaborate language of the index expressions that captures
various kind of invariance while keeping the type inference amenable.
References
1. Aloul, F.A., Sakallah, K.A., Markov, I.L.: Eﬃcient symmetry breaking for boolean
satisﬁability. In: Proceedings of the 18th International Joint Conference on Artiﬁ-
cial Intelligence, IJCAI 2003, pp. 271–276 (2003)
2. Arai, N.H.: Tractability of cut-free Gentzen type propositional calculus with per-
mutation inference. Theoret. Comput. Sci. 170(1), 129–144 (1996)
3. Arai, N.H., Urquhart, A.: Local symmetries in propositional logic. In: Dyckhoﬀ, R.
(ed.) TABLEAUX 2000. LNCS (LNAI), vol. 1847, pp. 40–51. Springer, Heidelberg
(2000). https://doi.org/10.1007/10722086 3
4. Atkey, R., Johann, P., Kennedy, A.: Abstraction and invariance for algebraically
indexed types. In: Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages, POPL 2013, pp. 87–100 (2013)
5. Brown, C.W., Davenport, J.H.: The complexity of quantiﬁer elimination and cylin-
drical algebraic decomposition. In: Proceedings of the 2007 International Sympo-
sium on Symbolic and Algebraic Computation, ISSAC 2007, pp. 54–60 (2007)
6. Crawford, J.M., Ginsberg, M.L., Luks, E.M., Roy, A.: Symmetry-breaking pred-
icates for search problems. In: Proceedings of the Fifth International Conference
on Principles of Knowledge Representation and Reasoning, KR 1996, pp. 148–159
(1996)
7. Davenport, J.H.: What does “without loss of generality” mean, and how do we
detect it. Math. Comput. Sci. 11(3), 297–303 (2017)
8. Devriendt, J., Bogaerts, B., Bruynooghe, M., Denecker, M.: Improved static sym-
metry breaking for SAT. In: Creignou, N., Le Berre, D. (eds.) SAT 2016. LNCS,
vol. 9710, pp. 104–122. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
40970-2 8
9. Harrison, J.: Without loss of generality. In: Berghofer, S., Nipkow, T., Urban, C.,
Wenzel, M. (eds.) TPHOLs 2009. LNCS, vol. 5674, pp. 43–59. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-03359-9 3
10. Iwane, H., Anai, H.: Formula simpliﬁcation for real quantiﬁer elimination using
geometric invariance. In: Proceedings of the 2017 ACM on International Sympo-
sium on Symbolic and Algebraic Computation, ISSAC 2017, pp. 213–220 (2017)
11. Kennedy, A.: Types for units-of-measure: theory and practice. In: Horv´ath, Z.,
Plasmeijer, R., Zs´ok, V. (eds.) CEFP 2009. LNCS, vol. 6299, pp. 268–305. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-17685-2 8
12. Krishnamurthy, B.: Short proofs for tricky formulas. Acta Inform. 22(3), 253–275
(1985)
13. Matsuzaki, T., Ito, T., Iwane, H., Anai, H., Arai, N.H.: Semantic parsing of pre-
university math problems. In: Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), ACL 2017,
pp. 2131–2141 (2017)
14. Matsuzaki, T., Iwane, H., Anai, H., Arai, N.H.: The most uncreative examinee: a
ﬁrst step toward wide coverage natural language math problem solving. In: Pro-
ceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence, AAAI
2014, pp. 1098–1104 (2014)

406
T. Matsuzaki and T. Fujita
15. Matsuzaki, T., et al.: Can an A.I. win a medal in the mathematical olympiad? -
Benchmarking mechanized mathematics on pre-university problems. AI Commu-
nications 31(3), 251–266 (2018)
16. Matsuzaki, T., et al.: Race against the teens – benchmarking mechanized math
on pre-university problems. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS
(LNAI), vol. 9706, pp. 213–227. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-40229-1 15
17. Metin, H., Baarir, S., Colange, M., Kordon, F.: CDCLSym: introducing eﬀective
symmetry breaking in SAT solving. In: Beyer, D., Huisman, M. (eds.) TACAS
2018. LNCS, vol. 10805, pp. 99–114. Springer, Cham (2018). https://doi.org/10.
1007/978-3-319-89960-2 6
18. Reynolds, J.C.: Types, abstraction and parametric polymorphism. In: Information
Processing 83, Proceedings of the IFIP 9th World Computer Congress, pp. 513–523
(1983)
19. Sabharwal, A.: SymChaﬀ: exploiting symmetry in a structure-aware satisﬁability
solver. Constraints 14(4), 478–505 (2009)
20. Szeider, S.: Homomorphisms of conjunctive normal forms. Discrete Appl. Math.
130(2), 351–365 (2003)
21. Wadler, P.: Theorems for free! In: Proceedings of the Fourth International Confer-
ence on Functional Programming Languages and Computer Architecture, FPCA
1989, pp. 347–359 (1989)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Synthetic Tableaux: Minimal Tableau
Search Heuristics
Michal Socha´nski(B)
, Dorota Leszczy´nska-Jasion(B)
,
Szymon Chlebowski
, Agata Tomczyk
, and Marcin Jukiewicz
Adam Mickiewicz University, ul. Wieniawskiego 1, 61-712 Pozna´n, Poland
{Michal.Sochanski,Dorota.Leszczynska,Szymon.Chlebowski,
Agata.Tomczyk,Marcin.Jukiewicz}@amu.edu.pl
Abstract. We discuss the results of our work on heuristics for gen-
erating minimal synthetic tableaux. We present this proof method for
classical propositional logic and its implementation in Haskell. Based on
mathematical insights and exploratory data analysis we deﬁne heuris-
tics that allows building a tableau of optimal or nearly optimal size.
The proposed heuristics has been ﬁrst tested on a data set with over
200,000 short formulas (length 12), then on 900 formulas of length 23.
We describe the results of data analysis and examine some tendencies.
We also confront our approach with the pigeonhole principle.
Keywords: Synthetic tableau · Minimal tableau · Data analysis ·
Proof-search heuristics · Haskell · Pigeonhole principle
1
Introduction
The method of synthetic tableaux (ST, for short) is a proof method based entirely
on direct reasoning but yet designed in a tableau format. The basic idea is that
all the laws of logic, and only laws of logic, can be derived directly by cases
from parts of some partition of the whole logical space. Hence an ST-proof
of a formula typically starts with a division between ‘p-cases’ and ‘¬p-cases’
and continues with further divisions, if necessary. Further process of derivation
consists in applying the so-called synthesizing rules that build complex formulas
from their parts—subformulas and/or their negations. For example, if p holds,
then every implication with p in the succedent holds, ‘q →p’ in particular; then
also ‘p →(q →p)’ holds by the same argument. If ¬p is the case, then every
implication with p in the antecedent holds, thus ‘p →(q →p)’ is settled. This
kind of reasoning proves that ‘p →(q →p)’ holds in every possible case (unless
we reject tertium non datur in the partition of the logical space). There are
no indirect assumptions, no reductio ad absurdum, no assumptions that need to
be discharged. The ST method needs no labels, no derivation of a normal form
(clausal form) is required.
This work was supported ﬁnancially by National Science Centre, Poland, grant no
2017/26/E/HS1/00127.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 407–425, 2022.
https://doi.org/10.1007/978-3-031-10769-6_25

408
M. Socha´nski et al.
In the case of Classical Propositional Logic (CPL, for short) the method may
be viewed as a formalization of the truth-tables method. The assumption that
p amounts to considering all Boolean valuations that make p true; considering
¬p exhausts the logical space. The number of cases to be considered corresponds
to the number of branches of an ST, and it clearly depends on the number of
distinct propositional variables in a formula, thus the upper bound for complexity
of an ST-search is the complexity of the truth-tables method. In the worst case
this is exponential with respect to the number of variables, but for some classes
of formulas truth-tables behave better than standard analytic tableaux (see [4–7]
for this diagnosis). However, the method of ST can perform better than truth-
tables, as shown by the example of ‘p →(q →p)’, where we do not need to
partition the space of valuations against the q/¬q cases.1 The question, obviously,
is how much better? The considerations presented in this paper aim at developing
a quasi-experimental framework for answering it.
The ST method was introduced in [19], then extended to some non-classical
logics in [20,22]. An adjustment to the ﬁrst-order level was presented in [14].
There were also interesting applications of the method in the domain of abduc-
tion: [12,13]. On the propositional level, the ST method is both a proof- and
model-checking method, which means that one can examine satisﬁability of a
formula A (equivalently, validity of ¬A) and its falsiﬁability (equivalently, incon-
sistency of ¬A) at the same time. Normally, one needs to derive a clausal form of
both A and ¬A to check the two dual semantic cases (satisﬁability and validity)
with one of the quick methods, while the ST-system is designed to examine both
of them. Wisely used, this property can contribute to limiting the increase in
complexity in veriﬁcation of semantic properties.
For the purpose of optimization of the ST method we created a heuristics that
leads to construction of a variable ordering—a task similar to the one performed
in research on Ordered Binary Decision Diagrams (OBDDs), and, generally, in
Boolean satisﬁability problem (SAT) [8,15]. In Sect. 3 we sketch a comparison
of STs to OBDDs. Let us stress at this point, however, that the aim of our anal-
ysis remains proof-theoretical—the ST method is a ‘full-blooded’ proof method
working on formulas of arbitrary representation. It was already adjusted to ﬁrst-
order and to some non-classical logics, and has a large scope of applications
beyond satisﬁability checking of clausal forms.
The optimization methods that we present are based on exploratory data
analysis performed on millions of tableaux. Some aspects of the analysis are also
discussed in the paper. The data are available on https://ddsuam.wordpress.
com/software-and-data/.
Here is a plan of what follows. The next section introduces the ST method,
Sect. 3 compares STs with analytic tableaux and with BDDs, and Sect. 4 presents
the implementation in Haskell. In Sect. 5 we introduce the mathematical concepts
1 On a side note, it is easy to show that the ST system is polynomially equivalent to
system KE introduced in [4], as both systems contain cut. What is more, there is
a strict analogy between the ST method and the inverse method (see [4,16]). The
relation between ST and KI was examined by us in detail in Sect. 2 of [14].

Synthetic Tableaux: Minimal Tableau Search Heuristics
409
needed to analyse heuristics of small tableaux generation. In Sect. 6 we describe
the analysed data, and in Sect. 7—the obtained results. Section 8 confronts our
approach with the pigeonhole principle, and Sect. 9 indicates plans for further
research.
2
The Method of Synthetic Tableaux
Language. Let LCPL stand for the language of CPL with negation, ¬, and impli-
cation, →. Var = {p, q, r, . . . , pi, . . .} is the set of propositional variables and
‘Form’ stands for the set of all formulas of the language, where the notion of
formula is understood in a standard way. A, B, C . . . will be used for formulas
of LCPL. Propositional variables and their negations are called literals. Length
of a formula A is understood as the number of occurrences of characters in A,
parentheses excluded.
Let A ∈Form. We deﬁne the notion of a component of A as follows. (i) A is a
component of A. (ii) If A is of the form ‘¬¬B’, then B is a component of A. (iii) If
A is of the form ‘B →C’, then ‘¬B’ and C are components of A. (iv) If A is of the
form ‘¬(B →C)’, then B and ‘¬C’ are components of A. (v) If C is a component
of B and B is a component of A, then C is a component of A. (vi) Nothing else
is a component of A. By ‘Comp(A)’ we mean the set of all components of A.
For example, Comp( p →(q →p) ) = {p →(q →p), ¬p, q →p, ¬q, p}. As we
can see, component of a formula is not the same as subformula of a formula; ¬q
is not a subformula of the law of antecedent, q is, but it is not its component.
Components refer to uniform notation as deﬁned by Smullyan (see [18]) which
is very convenient to use with a larger alphabet. Let us also observe that the
association of Comp(A) with a Hintikka set is quite natural, although Comp(A)
need not be consistent. In the sequel we shall also use ‘Comp±(A)’ as a short for
‘Comp(A) ∪Comp(¬A)’.
Rules. The system of ST consists of the set of rules (see Table 1) and the notion
of proof (see Deﬁnition 2). The rules can be applied in the construction of an ST
for a formula A on the proviso that (a) the premises already occur on a given
branch, (b) the conclusion (conclusions, in the case of (cut)) of a particular
application of the rule belongs (both belong) to Comp±(A). The only branching
rule, called (cut) by analogy to its famous sequent-calculus formulation, is at the
same time the only rule that needs no premises, hence every ST starts with an
application of this rule. If its application creates branches with pi and ¬pi, then
we say that the rule was applied with respect to pi.
One of the nice properties of this method is that it is easy to keep every
branch consistent: it is suﬃcient to restrict the applications of (cut), so that on
every branch (cut) is applied with respect to a given variable pi at most once.
This warrants that pi, ¬pi never occur together on the same branch.
The notion of a proof is formalized by that of a tree. If T is a labelled tree,
then by XT we mean the set of its nodes, and by rT we mean its root. Moreover,
ηT is used for a function assigning labels to the nodes in XT .

410
M. Socha´nski et al.
Table 1. Rules of the ST system for LCPL
(r1
→)
(r2
→)
(r3
→)
(r¬)
(cut)
A
pi
¬pi
¬A
B
¬B
A
A →B
A →B
¬(A →B)
¬¬A
Deﬁnition 1 (synthetic tableau). A synthetic tableau for a formula A is
a ﬁnite labelled tree T generated by the above rules, such that ηT : X\{rT } −→
Comp±(A) and each leaf is labelled with A or with ¬A.
T is called consistent if the applications of (cut) are subject to the restriction
deﬁned above: there are no two applications of (cut) on the same branch with
respect to the same variable.
T is called regular provided that literals are introduced in the same order on
each branch, otherwise T is called irregular.
Finally, T is called canonical, if, ﬁrst, it is consistent and regular, and second,
it starts with an introduction of all possible literals by (cut) and only after that
the other rules are applied on the created branches.
In the above deﬁnition we have used the notion of literals introduced in the
same order on each branch. It seems suﬃciently intuitive at the moment, so we
postpone the clariﬁcation of this notion until the end of this section.
Deﬁnition 2 (proof in ST system).
A synthetic tableau T for a formula
A is a proof of A in the ST system iﬀeach leaf of T is labelled with A.
Theorem 1. (soundness and completeness, see [21]). A formula A is valid
in CPL iﬀA has a proof in the ST-system.
Example 1. Below we present two diﬀerent STs for one formula: B = p →(q →
p). Each of them is consistent and regular. Also, each of them is a proof of the
formula in the ST system.
T1 :
T2 :
1. p
2. q →p
3. p →(q →p)
4. ¬p
5. p →(q →p)
1. q
2. p
3. q →p
4. B
5. ¬p
6. B
7. ¬q
8. q →p
9. B
In T1: 2 comes from 1 by r2
→, similarly 3 comes from 2 by r2
→. 5 comes from
4 by r1
→. In T2: nothing can be derived from 1, hence the application of (cut)
wrt p is the only possible move. The numbering of the nodes is not part of the
ST.

Synthetic Tableaux: Minimal Tableau Search Heuristics
411
There are at least two important size measures used with respect to trees: the
number of nodes and the number of branches. As witnessed by our data, there
is a very high overall correlation between the two measures, we have thus used
only one of them—the number of branches—in further analysis. Among various
STs for the same formula there can be those of smaller, and those of bigger size.
An ST of a minimal size is called optimal. In the above example, T1 is an optimal
ST for B. Let us also observe that there can be many STs for a formula of the
same size, in particular, there can be many optimal STs.
Example 2. Two possible canonical synthetic tableaux for B = p →(q →p).
Each of them is regular, consistent, but clearly not optimal (cf. T1).
T3 :
T4 :
p
q
q →p
B
¬q
q →p
B
¬p
q
B
¬q
B
q
p
q →p
B
¬p
B
¬q
p
q →p
B
¬p
B
In the case of formulas with at most two distinct variables regularity is a triv-
ial property. Here comes an example with three variables.
Example 3. T5 is an irregular ST for formula C = (p →¬q) →¬(r →p),
i.e. variables are introduced in various orders on diﬀerent branches. T6 is an
example of an inconsistent ST for C, i.e. there are two applications of (cut) on
one branch with respect to p, which results in a branch carrying both p and ¬p
(the blue one). The whole right subtree of T5, starting with ¬p, is repeated twice
in T6, where it is symbolized with letter T ∗. Let us observe that ¬¬(r →p) is
a component of ¬C due to clause (iv) deﬁning the concept of component.
T5 :
T6 :
p
r →p
¬¬(r →p)
q
¬¬q
¬(p →¬q)
C
¬q
p →¬q
¬C
¬p
p →¬q
¬¬(p →¬q)
r
¬(r →p)
C
¬r
r →p
¬¬(r →p)
¬C
p
r →p
¬¬(r →p)
p
q
¬¬q
¬(p →¬q)
C
¬q
p →¬q
¬C
¬p
T ∗
¬p
T ∗
On the level of CPL we can use only consistent STs while still having a com-
plete calculus (for details see [19,21]). An analogue of closing a branch of an
analytic tableau for formula A is, in the case of an ST, ending a branch with

412
M. Socha´nski et al.
A synthesized. And the fact that an ST for A has a consistent branch ending
with ¬A witnesses satisﬁability of ¬A. The situation concerning consistency of
branches is slightly diﬀerent, however, in the formalization of ﬁrst-order logic
presented in [14], as a restriction of the calculus to consistent STs produces an
incomplete formalization.
Finally, let us introduce some auxiliary terminology to be used in the sequel.
Suppose T is an ST for a formula A and B is a branch of T . Literals occur on B in
an order set by the applications of (cut), suppose that it is ⟨±p1, . . . , ±pn⟩, where
‘±’ is a negation sign or no sign. In this situation we call sequence o = ⟨p1, . . . , pn⟩
the order on B. It can happen that o contains all variables that occur in A, or
that some of them are missing. Suppose that q1, . . . , qm are all of (and only) the
distinct variables occurring in A. Each permutation of q1, . . . , qm will be called
an instruction for a branch of an ST for A. Further, we will say that the order
o on B complies with an instruction I iﬀeither o = I, or o constitutes a proper
initial segment of I. Finally, I is an instruction for the construction of T , if I
is a set of instructions for branches of an ST for A such that for each branch of
T , the order on the branch complies with some element of I.
Let us observe that in the case of a regular ST the set containing one instruc-
tion for a branch makes the whole instruction for the ST, as the instruction
describes all the branches. Let us turn to examples. T5 from Example 3 has
four branches with the following orders (from the left): ⟨p, q⟩, ⟨p, q⟩, ⟨p, r⟩, ⟨p, r⟩.
On the other hand, there are six permutations of p, q, r, and hence six possible
instructions for branches of an arbitrary ST for the discussed formula. Order
⟨p, q⟩complies with instruction ⟨p, q, r⟩, and order ⟨p, r⟩complies with instruc-
tion ⟨p, r, q⟩. The set {⟨p, q, r⟩, ⟨p, r, q⟩} is an instruction for the construction of
an ST for C, more speciﬁcally, it is an instruction for the construction of T5.
3
ST, Analytic Tableaux, BDDs, and SAT Solvers
The analogy between STs and analytic tableaux sketched in the last paragraph
of the previous section breaks in two points. First, let us repeat: the ST method
is both a satisﬁability checker and a validity checker at once, just like a truth
table is. Second, the analogy breaks on complexity issues. In the case of analytic
tableaux the order of decomposing compound formulas is the key to a minimal
tableau. In the case of STs, the key to an optimized use of the method is a clever
choice of variables introduced on each branch.
The main similarity between STs and Binary Decision Diagrams (BDDs, see
e.g. [8,15]) is that both methods involve branching on variables. The main diﬀer-
ences concern the representation they work on and their aims: ﬁrstly, STs con-
stitute a proof method, whereas BDDs are compact representations of Boolean
formulas, used mainly for practical aims such as design of electronic circuits
(VLSI design); secondly, ST applies to logical formulas, whereas construction
of BDDs may start with diﬀerent representations of Boolean functions, usually
circuits or Boolean formulas.
The structure of the constructed tree is also slightly diﬀerent in the two
approaches: in BDDs the inner nodes correspond to variables with outgoing

Synthetic Tableaux: Minimal Tableau Search Heuristics
413
edges labelled with 1 or 0; in STs, on the other hand, inner nodes are labelled
with literals or more complex formulas. The terminal nodes of a BDD (also called
sinks, labelled with 1 or 0) indicate the value of a Boolean function calculated
for the arguments introduced along the path from the root, whereas the leaves
of an ST carry a synthesized formula (the initial one or its negation). In addition
to that, the methods diﬀer in terms of the construction process: in case of BDDs,
tree structures are ﬁrst generated and then reduced to a more compact form using
the elimination and merging rules; the STs, in turn, are built ‘already reduced’.
However, the interpretation of the outcome of both constructions is analogous.
Firstly, for a formula A with n distinct variables p1, . . . , pn and the associated
Boolean function fA = fA(x1, . . . , xn), the following fact holds: If a branch of an
ST containing literals from a set L ends with A or ¬A synthesized (which means
that assuming that the literals from L are true is suﬃcient to calculate the value
of A), then the two mentioned reduction rules can be used in a BDD for fA,
so that the route that contains the variables occurring in L followed by edges
labelled according to the signs in L can be directed to a terminal node (sink).
For example, if A can be synthesized on a branch with literals ¬p1, p2 and ¬p3,
then fA(0, 1, 0, x4, . . . , xn) = 1 for all values of the variables y ∈{x4, . . . , xn}
and so the route in the associated BDD containing the variables x1, x2 and x3
followed by the edges labelled with 0, 1 and 0, respectively, leads directly to the
sink labelled with 1.
However, possibility of applying the reduction procedures for a BDD does
not always correspond to the possibility of reducing an ST. For example, the
reduced BDD for formula p ∨(q ∧¬q) consists of the single node labelled with
p with two edges directed straight to the sinks 1 and 0; on the other hand,
construction of an ST for the formula requires introducing q following the literal
¬p. This observation suggests that ST, in general, have greater size than the
reduced BDDs.
Strong similarity of the two methods is also illustrated by the fact that they
both allow the construction of a disjunctive normal form (DNF) of the logical
or Boolean formula to which they were applied. In the case of ST, DNF is the
disjunction of conjunctions of literals that appear on branches ﬁnished with
the formula synthesized. The smaller the ST, the smaller the DNF. Things are
analogous with BDDs.
Due to complexity issues, research on BDDs centers on ordered binary deci-
sion diagrams (OBDDs), in which diﬀerent variables appear in the same order
on all paths from the root. A number of heuristics have been proposed in order
to construct a variable ordering that will lead to the smallest OBDDs, using
characteristics of the diﬀerent types of representation of Boolean function (for
example, for circuits, topological characteristics have been used for that pur-
pose). OBDDs are clearly analogous to regular STs, the construction of which
also requires ﬁnding a good variable ordering, leading to a smaller ST. We sup-
pose that our methodology can also be used to ﬁnd orderings for OBDDs by
expressing Boolean functions as logical formulas. It is not clear to us whether
the OBDDs methodology can be used in our framework.

414
M. Socha´nski et al.
Let us move on to other comparisons, this time with a lesser degree of detail.
It is very instructive to compare the ST method to SAT-solvers, as their eﬀec-
tiveness is undeniably impressive nowadays2. The ST method does not aim at
challenging this eﬀectiveness. Let us explain, however, in what aspect the ST
method can still be viewed as a computationally attractive alternative to a SAT
solver. The latter produces an answer to question about satisﬁability, sometimes
producing also examples of satisfying valuations and/or counting the satisfy-
ing valuations. In order to obtain an answer to another question—that about
validity—one needs to ask about satisﬁability of the initial problem negated. As
we stressed above, the ST method answers the two questions at once, providing
at the same time a description of classes of valuations satisfying and not satis-
fying the initial formula. Hence one ST is worth two SAT-checks together with
a rough model counting.
Another interesting point concerns clausal forms. The method of ST does
not require derivation of clausal form, but the applications of the rules of the
system, deﬁned via α-, β-notation, reﬂects the breaking of a formula into its
components, and thus, in a way, leads to a deﬁnition of a normal form (a DNF,
as we mentioned above). But this is not to say that an ST needs a full conversion
to DNF. In this respect the ST method is rather similar to non-clausal theorem
provers (e.g. non-clausal resolution, see [9,17]).
Let us ﬁnish this section with a summary of the ST method. Formally, it
is a proof method with many applications beyond the realm of CPL. In the
area of CPL, semantically speaking, it is both satisﬁability and validity checker,
displaying semantic properties of a formula like a truth table does, but amenable
to work more eﬃciently (in terms of the number of branches) than the latter
method. The key to this eﬃciency is in the order of variables introduced in an
ST. In what follows we present a method of construction of such variable orders
and examine our approach in an experimental setting.
4
Implementation
The main functionality of the implementation described in this section is a con-
struction of an ST for a formula according to an instruction provided by the
user. If required, it can also produce all possible instructions for a given formula
and build all STs according to them. In our research we have mainly used the
second possibility.
The implemented algorithm generates non-canonical, possibly irregular STs.
Let us start with some basics. There are three main datatypes employed. Stan-
dard, recursively deﬁned formula type, For, used to represent propositional for-
mulas; Monad Maybe Formula, MF, consisting of Just Formula and Nothing—
used to express the fact that the synthesis of a given formula on a given branch
was successful (Just) or not (Nothing). To represent an ST we use type of trees
imported from Data.Tree. Thus every ST can be represented as Tree [MF]
2 See [23, p. 2021]: contemporary SAT solvers can often handle practical instances with
millions of variables and constraints.

Synthetic Tableaux: Minimal Tableau Search Heuristics
415
[Tree [MF]], that is a tree labelled by lists of MF. We employed such a gen-
eral structure having in mind possible extensions to non-classical logics (for CPL
a binary tree is suﬃcient). The algorithm generating all possible ST for a given
formula consists of the following steps:
1. We start by performing a few operations on the goal-formula A:
(a) a list of all components of A and all components of ¬A, and a separate
list of the variables occurring in A (atoms A) is generated;
(b) the ﬁrst list is sorted in such a way that all components of a given formula
in that list precede it (sort A).
2. After this initial step, based on the list atoms A, all possible instructions for
the construction of an ST for A are generated (allRules (atoms A)).
3. For each instruction from allRules (atoms A) we build an ST using the
following strategy, called ‘compulsory’:
(a) after each introduction of a literal (by (cut)) we try to synthesize (by the
other rules) as many formulas from sort A as possible;
(b) if no synthesizing rule is applicable we look into the instruction to intro-
duce an appropriate literal and we go back to (a). Let us note that T1,
T2, T5 are constructed according to this strategy.
4. Lastly, we generate a CSV ﬁle containing some basic information about each
generated tree: int.al. the number of nodes and whether the tree is a proof.
Please observe that the length of a single branch is linear in the size of a formula;
this follows from the fact that sort A contains only the components of A. On
the other hand, an ‘outburst’ of computational complexity enters on the level
of the number of STs. In general, if k is the number of distinct variables in a
formula A, then for k = 3 there are 12 diﬀerent canonical STs, for k = 4 and
k = 5 this number is, respectively, 576 and 1,688,800. In the case of k = 6 the
number of canonical STs per formula exceeds 1012 and this approach is no longer
feasible3.
The Haskell implementation together with necessary documentation is avail-
able on https://ddsuam.wordpress.com/software-and-data/.
5
dp-Measure and the Rest of Our Toolbox
As we have already observed, in order to construct an optimal ST for a given
formula one needs to make a clever choice of the literals to start with. The
following function was deﬁned to facilitate the smart choices. It assigns a rational
value from the interval ⟨0; 1⟩to each occurrence of a literal in a syntactic tree for
3 It can be shown (e.g. by mathematical induction) that for formulas with k diﬀerent
variables, the total number of canonical STs is given by the following explicit formula:
k

i=1
(k −i + 1)2i−1.
.

416
M. Socha´nski et al.
formula A (in fact, it assigns the values to all elements of Comp(A)). Intuitively,
the value reﬂects the derivative power of the literal in synthesizing A.
The ﬁrst case of the equation in Deﬁnition 3 is to make the function full
(=total) on Form × Form, it also corresponds with the intended meaning of the
deﬁned measure: if B /∈Comp(A), then B is of no use in deriving A. The second
case expresses the starting point: to calculate the values of dp(A, B) for atomic
B, one needs to assign 1 = dp(A, A); then the value is propagated down along
the branches of a formula’s syntactic tree. Dividing the value a by 2 in the
fourth line reﬂects the fact that both components of an α-formula are needed to
synthesize the formula. In order to use the measure, we need to calculate it for
both A and ¬A; this follows from the fact that we do not know whether A or
¬A will be synthesized on a given branch.
Deﬁnition 3. dp : Form × Form −→⟨0; 1⟩
dp( A, B ) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
if B ̸∈Comp(A),
1
if B = A,
a
if dp( A, ¬¬B ) = a,
a
2
if B ∈{C, ¬D} and dp( A, ¬(C →D) ) = a,
a
if B ∈{¬C, D} and dp( A, C →D ) = a.
Example 4. A visualization of calculating dp for formulas B, C from Examples
2, 3 and for D = (p →¬p) →p.
p →(q →p)1
¬p1 q →p1
¬q1 p1
¬(p →(q →p))1
p 1
2 ¬(q →p) 1
2
q 1
4 ¬p 1
4
(p →¬q) →¬(r →p)1
¬(p →¬q)1
p 1
2 ¬¬q 1
2
q 1
2
¬(r →p)1
r 1
2 ¬p 1
2
¬((p →¬q) →¬(r →p))1
p →¬q 1
2
¬p 1
2 ¬q 1
2
¬¬(r →p) 1
2
r →p 1
2
¬r 1
2 p 1
2
(p →¬p) →p1
¬(p →¬p)1
p 1
2 ¬¬p 1
2
p 1
2
p1
¬((p →¬p) →p)1
p →¬p 1
2
¬p 1
2 ¬p 1
2
¬p 1
2
As one can see from Example 4, the eﬀect of applying the dp measure to
a formula and its negation is a number of values that need to be aggregated
in order to obtain a clear instruction for an ST construction. However, some
conclusions can be drawn already from the above example. It seems clear that
the value dp( p →(q →p), p ) = 1 corresponds to the fact that p is suﬃcient to
synthesize the whole formula (as witnessed by T1, see Example 1). So is the case
with ¬p. On the other hand, even if ¬q is suﬃcient to synthesize the formula, q is
not (see T2, Example 1), hence the choice between p and q is plain. But it seems
to be the only obvious choice at the moment. In the case of the second formula,
every literal gets the same value: 0.5. What is more, in the case of longer formulas
a situation depicted by the rightmost syntactic trees is very likely to happen:

Synthetic Tableaux: Minimal Tableau Search Heuristics
417
we obtain dp(D, p) = 0.5 twice (since dp works on occurrences of literals), and
dp(¬D, ¬p) = 0.5 three times.
In the aggregation of the dp-values we use the parametrised Hamacher s-
norm, deﬁned for a, b ∈⟨0; 1⟩as follows:
a sλ b = a + b −ab −(1 −λ)ab
1 −(1 −λ)ab
for which we have taken λ = 0.1, as the value turned out to give the best results.
Hamacher s-norm can be seen as a fuzzy alternative; it is commutative and
associative, hence it is straightforward to extend its application to an arbitrary
ﬁnite number of arguments. For a = b = c = 0.5 we obtain:
a sλ b ≈0.677,
and (a sλ b) sλ c ≈0.768
The value of this norm is calculated for a formula A and a literal l by taking
the dp-values dp(A, l) for each occurrence l in the syntactic tree of A. This
value will be denoted as ‘h(A, l)’; in case there is only one value dp(A, l), we
take h(A, l) = dp(A, l). Hence, referring to the above Example 4, we have e.g.
h(B, p) = 1, h(¬B, ¬p) = 0.25, h(¬D, ¬p) ≈0.768.
Finally, function H is deﬁned for variables, not their occurrences, in formula
A as follows:
H(A, pi) = max(h(A, pi), h(¬A, pi)) + max(h(A, ¬pi), h(¬A, ¬pi))
2
The important property of this apparatus is that for a, b < 1 we have a s0.1 b >
max{a, b}, and thus h(A, l) and H(A, pi) are sensitive to the number of aggre-
gated elements. Another desirable feature of the introduced functions is that
h(A, pi) = 1 indicates that one can synthesize A on a branch starting with pi
without further applications of (cut); furthermore, H(A, pi) = 1 indicates that
both pi and ¬pi have this property.
Let us stress that the values of dp, h and H are very easy to calculate.
Given a formula A, we need to assign a dp-value to each of its components,
and the number of components is linear in the length of A. On the other hand,
the information gained by these calculations is sometimes not suﬃcient. The
assignment dp(A, pi) = 2−m says only that A can be built from pi and m other
components of A, but it gives us no clue as to which components are needed.
In Example 4, H works perfectly, as we have H(B, p) = 1 and H(B, q) = 0.625,
hence H indicates the following instruction of construction of an ST: {⟨p, q⟩}.
Unfortunately, in the case of formula C we have H(C, p) = H(C, q) = H(C, r) =
0.5, hence a more sophisticated solution is needed.
6
Data
At the very beginning of the process of data generation we faced the following
general problem: how to make any conclusive inferences about an inﬁnite pop-
ulation (all Form) on the basis of ﬁnite data? Considering the methodological

418
M. Socha´nski et al.
problems connected with applying classical statistical inference methods in this
context, we limited our analysis to descriptive statistics, exploratory analysis
and testing. To make this as informative as possible, we took a ‘big data’ app-
roach: for every formula we generated all possible STs, diﬀering in the order of
applications of (cut) on particular branches. In addition to that, where it was
feasible, we generated all possible formulas falling under some syntactical spec-
iﬁcations. The approach is aimed at testing diﬀerent optimisation methods as
well as exploring data in search for patterns and new hypotheses. The knowl-
edge gained in this way is further used on samples of longer formulas to examine
tendencies.
From now on we use l for the length of a formula, k for the number of distinct
variables occurring in a formula, and n for the number of all occurrences of
variables (leaves, if we think of formulas as trees). On the ﬁrst stage we examined
a dataset containing all possible STs for formulas with l = 12 and k ⩽4. There
are over 33 million of diﬀerent STs already for these modest values; for larger
k the data to analyse was simply too big. We generated 242,265 formulas, from
which we have later removed those with k ⩽2 and/or k = n, as the results for
them where not interesting. In the case of further datasets we also generated all
possible STs, but the formulas were longer and they were randomly generated4.
And so we considered (i) 400 formulas with l = 23, k = 3, (ii) 400 formulas with
l = 23, k = 4, (iii) 100 formulas with l = 23, k = 5. In all cases 9 ⩽n ⩽12;
this value is to be combined with the occurrences of negations in a formula—the
smaller n, the more occurrences of negation.
Having all possible STs for a formula generated, we could simply check what
is the optimal ST’ size for this formula. The idea was to look for possible rela-
tions between, on the one hand, instructions producing the small STs, and, on
the other hand, properties of formulas that are easy to calculate, like dp or
numbers of occurrences of variables. The ﬁrst dataset included only relatively
small formulas; however, with all possible formulas of a given type available, it
was possible e.g. to track various types of ‘unusual’ behaviour of formulas and
all possible problematic issues regarding the optimisation methods, which could
remain unnoticed if only random samples of formulas were generated. In case
of randomly generated formulas the ‘special’ or ‘diﬃcult’ types of formulas may
not be tracked (as the probability of drawing them may be small), but instead
we have an idea of an ‘average’ formula, or average behaviour of the optimisation
methods. By generating all the STs, in turn, we gained access to full information
not only about the regular but also irregular STs, which is the basis for indicating
the set of optimal STs and the evaluation of the optimisation methods.
7
Data Analysis and a Discussion of Results
In this section we present some results of analyses performed on our data. The
main purpose of the analyses is to test the eﬀectiveness of the function H in terms
4 The algorithm of generating random formulas is described in [11]. The author pre-
pared also the Haskell implementation of the algorithm. See https://github.com/
kiryk/random-for.

Synthetic Tableaux: Minimal Tableau Search Heuristics
419
Fig. 1. Distribution of the diﬀerence between the size of a maximal and that of a min-
imal ST for formulas with k = 4, 5.
of indicating a small ST. Moreover, we performed diﬀerent types of exploratory
analysis on the data, aiming at understanding the variation of size among all
STs for diﬀerent formulas, and how it relates to the eﬀectiveness of H.
Most results will be presented for the ﬁve combinations of the values of l and
k in our data, that is, l = 12, k ∈{3, 4} and l = 23, k ∈{3, 4, 5}; however, some
results will be presented with the values of k = 3 and k = 4 grouped together
(where the diﬀerence between them is insigniﬁcant) and the charts are presented
only for k ⩾4.
We will examine the variation of size among STs using a range statistic:
by range of the size of ST for a formula A (ST range, for short) we mean the
diﬀerence between an ST of maximal and minimal size; this value indicates the
possible room for optimization. The maximal-size ST is bounded by the size of
a canonical ST for a given formula; its size depends only on k. For k = 4 a
canonical ST has 16 branches, for k = 5 it is 32 branches.
The histograms on Fig. 1 present the distributions of ST range for formulas
with k = 4 and k = 5. The rightmost bar in the histogram for l = 23, k = 5 says
that for 5 (among 100) formulas there are STs with only two branches, where
the maximal STs for these formulas have 32 branches. We can also read from the
histograms that for formulas with k = 4 the ST range of some formulas is equal
to 0 (7.9% of formulas with l = 12 and 3.5% with l = 23), which means that
all STs have the same size. We have decided to exclude these formulas from the
results of tests of eﬃciency of H, as the formulas leave no room for optimization.
However, as can be seen on the histogram, there were no formulas of this kind
among those with k = 5. This indicates that with the increase of k the internal
diﬀerentiation of the set of STs for a formula increases as well, leading to a
smaller share of formulas with small ST range.
Two more measures relating to the distribution of the size of ST may be of
interest. Firstly, the share of formulas for which no regular ST is of optimal size—

420
M. Socha´nski et al.
Table 2. Row A: the share of formulas that do not have a regular ST of optimal size.
Row B: the share of optimal STs among all STs for a formula; this was ﬁrst calculated
for each formula, then averaged over all formulas in a given set.
k = 3
k = 4
k = 5
l = 12 l = 23 l = 12 l = 23 l = 23
A
1.5%
1.1%
4.9%
3.3%
8.0%
B 31.7% 31.8% 17.3% 17.4% 10.0%
it indicates how wrong we can be in pointing to only the regular STs. Secondly,
the percentage share of optimal STs among all STs for a given formula. The
latter gives an idea what is the chance of picking an optimal ST at random.
Table 2 presents both values for formulas depending on k and l (let us recall
that formulas with ST range equal to 0 are excluded from the analysis). In both
cases we can see clearly a tendency with growing k. As was to be expected, the
table shows that the average share of optimal STs depends on the value of k
rather than the size of the formula. This is understandable—as the number of
branches depends on k only, the length of a formula translates to the length of
branches, and the latter is linear in the former. In a way, this explains why the
results are almost identical when the size of STs is calculated in terms of nodes
rather than branches (as we mentioned above, the overall correlation between
the two measures makes the choice between them irrelevant).
We can categorise the output of the function H into three main classes. In the
ﬁrst case, the values assigned to variables by H strictly order the variables, which
results in one speciﬁc instruction of construction of a regular ST. The general
score of such unique indications was very high: 70.9% for formulas with l = 12,
92.0% for l = 23, k = 3, 4, and 72.0% for k = 5. The second possibility is when
H assigns the same value to each variable; in this case we gain no information
at all (let us recall that we have excluded the only cases that could justify such
assignments, that is, the formulas for which each ST is of the same size). The
share of such formulas in our datasets was small: 0.6% for l = 12, 0.1% for
l = 23, k = 3, 4 and 0% for k = 5, suggesting that it tends to fall with k rising.
The third possibility is that the ordering is not strict, yet some information is
gained. In this case for some, but not all, variables the value of H is the same.
The methodology used to asses eﬀectiveness of H is quite simple. We assume
that every indication must be a single regular instruction, hence we use additional
criteria in case of formulas of the second and third kind described, in order to
obtain a strict ordering. If H outputs the same value for some variables, we ﬁrst
order the variables by the number of occurrences in the formula; if the ordering
is still not strict, we give priority to variables for which the sum of depths for all
occurrences of literals in the syntactic tree is smaller; ﬁnally, where the above
criteria do not provide a strict ordering, the order is chosen at random.
We used three evaluating functions to asses the quality of indications. Each
function takes as arguments a formula and the ST for this formula indicated by

Synthetic Tableaux: Minimal Tableau Search Heuristics
421
Table 3. The third column gives the number of formulas satisfying the characteristic
presented in the ﬁrst and the second column. The further three columns display values
averaged on the sets. F1 indicates how often we indicate an optimal ST. F2 reports
the mistake of our indication calculated as the diﬀerence of sizes between the indi-
cated ST and an optimal one. Finally, POT indicates proximity to an optimal ST in a
standardized way.
k
l
no of formulas
F1
F2
POT
3 12
113,190 0.935 0.089 0.974
23
400
0.923 0.104 0.966
4 12
53,130 0.859 0.286 0.966
23
400
0.836 0.297 0.960
5 23
100
0.75
0.52
0.971
our heuristics. The ﬁrst function (F1 in Table 3) outputs 1 if the indicated ST
is of optimal size, 0 otherwise. The second function (F2 in Table 3) outputs the
diﬀerence between the size of the indicated ST and the optimal size. The third
function is called proximity to optimal tableau, POTA in symbols:
POTA(T ) = 1 −
|T | −minA
maxA −minA
where T is the ST for formula A indicated by H, |T | is the size of T , maxA is
the size of an ST for A of maximal size, and minA is the size of an optimal ST
for A. Later on we skip the relativization to A. Let us observe that the value
|T |−min
max−min represents a mistake in indication relative to the ST range of a formula,
and in this sense POTA can be considered as a standardized measure of the
quality of indication. Finally, values of each of the three evaluating functions
were calculated for sets of formulas, by taking average values over all formulas
in the set.
The results of the three functions presented in Table 3 show that optimal STs
are indicated less often for formulas with greater k; however, the POT values
seem to remain stable across all data, indicating that, on average, proximity of
the indicated ST to the optimal ones does not depend on k or l.
Further analysis showed that the factor that most inﬂuenced the eﬃciency of
our methodology was whether there is at least one value 1 among the dp-values
of literals for a formula A. We shall write ‘Max(dp) = 1’ if this is the case,
and ‘Max(dp) < 1’ otherwise (we skip the relativisation to A for simplicity).
For formulas with Max(dp) = 1, results of the evaluating functions were much
better; for example, the value of the POT function for formulas with l = 12 was
0.979 if Max(dp) = 1, and 0.814 for those with Max(dp) < 1; in case of formulas
with l = 23, k = 3, 4 those values were 0.968 and 0.869, respectively, and for
formulas with l = 23, k = 5 it was 0.974 and 0.901, respectively. This shows
that our methodology works signiﬁcantly worse if Max(dp) < 1; on the other
hand, if Max(dp) = 1, the dp measure works very well. It should also be pointed

422
M. Socha´nski et al.
Fig. 2. Distribution of the diﬀerence between indicated and optimal ST in relation
to ST-range. Every point corresponds to a formula, the points are slightly jittered in
order to improve readibility. Each chart corresponds to diﬀerent data, formulas k = 3
are excluded; additionally the colour indicates whether Max(dp) = 1 for a formula.
out that the diﬀerence between the POT values for both groups is smaller for
formulas with greater l and k. Figure 2 presents a scatter plot that gives an
idea of the whole distribution of the values of the POT function in relation to
the ST range. Each formula on the plot is represented by a point, the colours
additionally indicating whether Max(dp) < 1. The chart suggests, similarly as
Table 3, that the method works well as the values of l and k rise for formulas,
indicating STs that are on average equally close to the optimal ones.
One can point at two possible explanations of the fact that our methodology
works worse for formulas with Max(dp) < 1. Firstly, if e.g., dp(A, p) = 2−m,
we only obtain the information that, except for p, m more occurrences of com-
ponents of A are required in order to synthesize the whole formula. Secondly,
the function H neglects the complex dependencies between the various aggre-
gated occurrences of a given variable, taking into account only the number of
occurrences of literals in an aggregated group. However, considering very low
computational complexity of the method based on the dp values and the func-
tion H, the outlined framework seems to provide good heuristics for indicating
small STs. Methods that would reﬂect more aspects of the complex structure of
logical formulas would likely require much more computational resources.
On a ﬁnal note, we would like to add that exploration of the data allowed
us to study properties of formulas that went beyond the scope of the optimi-
sation of ST. The data was used in a similar way as in so called Experimental
Mathematics, where numerous instances are analysed and visualized in order to
e.g. gain insight, search for new patterns and relationships, test conjectures and
introduce new concepts (see e.g. [1]).

Synthetic Tableaux: Minimal Tableau Search Heuristics
423
Table 4. The pigeonhole principle
PHPm
the size of ST
m
k
l
indicated by H minimal canonical ST
1
2
7
3
3
4
2
6
34
15
11
64
3
12
90
99
43
4096
4
20 184
783
189
220
8
The Pigeonhole Principle
At the end we consider the propositional version of the principle introduced by
Cook and Reckhow in [3, p. 43]. In the ﬁeld of proof complexity the principle
was used to prove that resolution is intractable, that is, any resolution proof of
the propositional pigeonhole principle must be of exponential size (wrt the size
of the formula). This has been proved by Haken in [10], see also[2].
Here is PHPm in the propositional version:

0⩽i⩽m

0⩽j<m
pi,j →

0⩽i<n⩽m

0⩽j<m
(pi,j ∧pn,j)
where  and 	 stand for generalized conjunction, disjunction (respectively) with
the range indicated beneath.
The pigeonhole principle is constructed in a perfect symmetry of the roles
played by the consecutive variables. Each variable has the same number of occur-
rences in the formula, and each of them gets the same value under H, they also
have occurrences at the same depth of a syntactic tree. All this means that in our
account we can only suggest a random, regular ST. However, it is worth noticing
that, ﬁrst, H behaves consistently with the structure of the formula, and second,
the result is still attractive. In Table 4 the fourth column presents the size of the
ST indicated by our heuristics, that is, in fact, generated by random ordering of
variables. It is to be contrasted with the number 2k in the last column describing
the size of a canonical ST for the formula, which is at the same time the number
of rows in a truth table for the formula. The minimal STs for the formulas were
found with pen and paper and they are irregular.
9
Summary and Further Work
We presented a proof method of Synthetic Tableaux for CPL and explained
how the eﬃciency of tableau construction depends on the choices of variables
to apply (cut) to. We deﬁned possible algorithms to choose the variables and
experimentally tested their eﬃciency.
Our plan for the next research is well deﬁned and it is to implement heuristics
amenable to produce instructions for irregular STs. We have an algorithm, yet
untested.

424
M. Socha´nski et al.
As far as proof-theoretical aims are concerned, the next task is to extend and
adjust the framework to the ﬁrst-order level based on the already described ST
system for ﬁrst-order logic [14]. We also wish to examine the eﬃciency of our
indications on propositional non-classical logics for which the ST method exists
(see [20,22]). In the area of data analysis another possible step would be to
perform more complex statistical analysis using e.g. machine learning methods.
References
1. Borwein, J., Bailey, D.: Mathematics by Experiment: Plausible Reasoning in the
21st Century. A K Peters, Ltd., Natick (2004)
2. Buss, S.R.: Polynomial size proofs of the propositional pigeonhole principle. J.
Symb. Log. 52(4), 916–927 (1987)
3. Cook, S.A., Reckhow, R.A.: The relative eﬃciency of propositional proof systems.
J. Symb. Log. 44(1), 36–50 (1979)
4. D’Agostino, M.: Investigations into the complexity of some propositional calculi.
Technical Monograph. Oxford University Computing Laboratory, Programming
Research Group, November 1990
5. D’Agostino, M.: Are tableaux an improvement on truth-tables? Cut-free proofs
and bivalence. J. Log. Lang. Comput. 1, 235–252 (1992)
6. D’Agostino, M.: Tableau methods for classical propositional logic. In: D’Agostino,
M., Gabbay, D.M., H¨ahnle, R., Posegga, J. (eds.) Handbook of Tableau Methods,
pp. 45–123. Kluwer Academic Publishers (1999)
7. D’Agostino, M., Mondadori, M.: The taming of the cut. Classical refutations with
analytic cut. J. Log. Comput. 4(3), 285–319 (1994)
8. Ebendt, R., Fey, G., Drechsler, R.: Advanced BDD Opimization. Springer, Heidel-
berg (2005). https://doi.org/10.1007/b107399
9. Fitting, M.: First-Order Logic and Automated Theorem Proving, 2nd edn.
Springer, New York (1996). https://doi.org/10.1007/978-1-4612-2360-3
10. Haken, A.: The intractability of resolution. Theoret. Comput. Sci. 39, 297–308
(1985)
11. Kiryk, A.: A modiﬁed Korsh algorithm for random trees with various arity
(manuscript) (2022)
12. Komosinski, M., Kups, A., Leszczy´nska-Jasion, D., Urba´nski, M.: Identifying eﬃ-
cient abductive hypotheses using multi-criteria dominance relation. ACM Trans.
Comput. Log. 15(4), 1–20 (2014)
13. Komosinski, M., Kups, A., Urba´nski, M.: Multi-criteria evaluation of abductive
hypotheses: towards eﬃcient optimization in proof theory. In: Proceedings of the
18th International Conference on Soft Computing, Brno, Czech Republic, pp. 320–
325 (2012)
14. Leszczy´nska-Jasion, D., Chlebowski, S.: Synthetic tableaux with unrestricted cut
for ﬁrst-order theories. Axioms 8(4), 133 (2019)
15. Meinel, C., Theobald, T.: Algorithms and Data Structures in VLSI Design. OBDD
- Foundations and Applications, Springer, Heidelberg (1998). https://doi.org/10.
1007/978-3-642-58940-9
16. Mondadori, M.: Eﬃcient inverse tableaux. J. IGPL 3(6), 939–953 (1995)
17. Murray, N.V.: Completely non-clausal theorem proving. Artif. Intell. 18, 67–85
(1982)

Synthetic Tableaux: Minimal Tableau Search Heuristics
425
18. Smullyan, R.M.: First-Order Logic. Springer, Berlin, Heidelberg, New York (1968).
https://doi.org/10.1007/978-3-642-86718-7
19. Urba´nski, M.: Remarks on synthetic tableaux for classical propositional calculus.
Bull. Sect. Log. 30(4), 194–204 (2001)
20. Urba´nski, M.: Synthetic tableaux for Lukasiewicz’ calculus L3. Logique Anal. (N.S.)
177–178, 155–173 (2002)
21. Urba´nski, M.: Tabele syntetyczne a logika pyta´n (Synthetic Tableaux and the Logic
of Questions). Wydawnictwo UMCS, Lublin (2002)
22. Urba´nski, M.: How to synthesize a paraconsistent negation. The case of CLuN.
Logique Anal. 185–188, 319–333 (2004)
23. Vizel, Y., Weissenbacher, G., Malik, S.: Boolean satisﬁability solvers and their
applications in model checking. Proc. IEEE 103, 2021–2035 (2015)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Modal Logics

Paraconsistent G¨odel Modal Logic
Marta B´ılkov´a1
, Sabine Frittella2
, and Daniil Kozhemiachenko2(B)
1 The Czech Academy of Sciences, Institute of Computer Science,
Prague, Czech Republic
bilkova@cs.cas.cz
2 INSA Centre Val de Loire, Univ. Orl´eans, LIFO EA 4022, Bourges, France
{sabine.frittella,daniil.kozhemiachenko}@insa-cvl.fr
Abstract. We introduce a paraconsistent modal logic KG2, based on
G¨odel logic with coimplication (bi-G¨odel logic) expanded with a De Mor-
gan negation ¬. We use the logic to formalise reasoning with graded,
incomplete and inconsistent information. Semantics of KG2 is two-
dimensional: we interpret KG2 on crisp frames with two valuations v1
and v2, connected via ¬, that assign to each formula two values from
the real-valued interval [0, 1]. The ﬁrst (resp., second) valuation encodes
the positive (resp., negative) information the state gives to a statement.
We obtain that KG2 is strictly more expressive than the classical modal
logic K by proving that ﬁnitely branching frames are deﬁnable and by
establishing a faithful embedding of K into KG2. We also construct a con-
straint tableau calculus for KG2 over ﬁnitely branching frames, establish
its decidability and provide a complexity evaluation.
Keywords: Constraint tableaux · G¨odel logic · Two-dimensional
logics · Modal logics
1
Introduction
People believe in many things. Sometimes, they even have contradictory beliefs.
Sometimes, they believe in one statement more than in the other. However, if
a person has contradictory beliefs, they are not bound to believe in anything.
Likewise, believing in φ strictly more than in χ makes one believe in φ completely.
These properties of beliefs are natural, and yet hardly expressible in the classical
modal logic. In this paper, we present a two-dimensional modal logic based on
G¨odel logic that can formalise beliefs taking these traits into account.
Two-Dimensional Treatment of Uncertainty. Belnap-Dunn four-valued
logic (BD, or First Degree Entailment—FDE) [4,16,34] can be used to formalise
The research of Marta B´ılkov´a was supported by the grant 22-01137S of the Czech
Science Foundation. The research of Sabine Frittella and Daniil Kozhemiachenko was
funded by the grant ANR JCJC 2019, project PRELAP (ANR-19-CE48-0006). This
research is part of the MOSAIC project ﬁnanced by the European Union’s Marie
Sklodowska-Curie grant No. 101007627.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 429–448, 2022.
https://doi.org/10.1007/978-3-031-10769-6_26

430
M. B´ılkov´a et al.
reasoning with both incomplete and inconsistent information. In BD, formulas
are evaluated on the De Morgan algebra 4 (Fig. 1, left) where the four values
{t, f, b, n} encode the information available about the formula: true, false, both
true and false, neither true nor false. b and n thus represent inconsistent and
incomplete information, respectively. It is important to note that the values
represent the available information about the statement, not its intrinsic truth or
falsity. Furthermore, this approach essentially treats evidence for a statement (its
positive support) as being independent of evidence against it (negative support)
which allows to diﬀerentiate between ‘absence of evidence’ and the ‘evidence of
absence’. The BD negation ¬ then swaps positive and negative supports.
Fig. 1. 4 (left) and its continuous extension [0, 1] (right). (x, y) ≤[0,1] (x′, y′) iﬀ
x ≤x′ and y ≥y′.
The information regarding a statement, however, might itself be not crisp—
after all, our sources are not always completely reliable. Thus, to capture the
uncertainty, we extend 4 to the lattice [0, 1] (Fig. 1, right). [0, 1] is a twist
product (cf, [37] for deﬁnitions) of [0, 1] with itself: the order on the second
coordinate is reversed w.r.t. the order on the ﬁrst coordinate. This captures the
intuition behind the usual ‘truth’ (upwards) order: an agent is more certain in
χ than in φ when the evidence for χ is stronger than the evidence for φ while
the evidence against χ is weaker than the evidence against φ.
Note that [0, 1] is a bilattice whose left-to-right order can be interpreted as
the information order. This links the logics we consider to bilattice logics applied
to reasoning in AI in [19] and then studied further in [24,35].
Comparing Beliefs. Uncertainty is manifested not only in the non-crisp char-
acter of the information. An agent might often lack the capacity to establish the
concrete numerical value that represents their certainty in a given statement.
Indeed, ‘I am 43% certain that the wallet is Paula’s’ does not sound natural. On
the other hand, it is reasonable to assume that the agents’ beliefs can be com-
pared in most contexts: neither ‘I am more conﬁdent that the wallet is Paula’s
than that the wallet is Quentin’s’, nor ‘Alice is more certain than Britney that
Claire loves pistachio ice cream’ require us to give a concrete numerical repre-
sentation to the (un)certainty.
These considerations lead us to choosing the two-dimensional relative of the
G¨odel logic dubbed G2 as the propositional fragment of our logic. G2 was intro-

Paraconsistent G¨odel Modal Logic
431
duced in [5] and is, in fact, an extension of Moisil’s logic1 from [31] with the
prelinearity axiom (p →q) ∨(q →p). As in the original G¨odel logic G, the
validity of a formula in G2 depends not on the values of its constituent variables
but on the relative order between them. In this sense, G is a logic of comparative
truth. Thus, as we treat positive and negative supports of a given statement
independently, G2 is a logic of comparative truth and falsity. Note that while the
values of two statements may not be comparable (say, p is evaluated as (0.5, 0.3)
and q as (0, 0)), the coordinates of the values always are. We will see in Sect. 2,
how we can formalise statements comparing agents’ beliefs.
The sources available to the agents as well as the references between these
sources can be represented as states in a Kripke model and its accessibility rela-
tion, respectively. It is important to mention that we account for the possibility
that a source can give us contradictory information regarding some statement.
Still, we want our reasoning with such information to be non-trivial. This is
reﬂected by the fact that (p∧¬p) →q is not valid in G2. Thus, the logic (treated
as a set of valid formulas) lacks the explosion principle. In this sense, we call
G2 and its modal expansions ‘paraconsistent’. This links our approach to other
paraconsistent fuzzy logics such as the ones discussed in [17].
To reason with the information provided by the sources, we introduce two
interdeﬁnable modalities— and ♦—interpreted as inﬁma and suprema w.r.t.
the upwards order on [0, 1]. We mostly assume (unless stated otherwise) that
accessibility relations in models are crisp. Intuitively, it means that the sources
are either accessible or not (and, likewise, either refer to the other ones, or not).
Broader Context. This paper is a part of the project introduced in [6] and
carried on in [5] aiming to develop a modular logical framework for reasoning
based on uncertain, incomplete and inconsistent information. We model agents
who build their epistemic attitudes (like beliefs) based on information aggregated
from multiple sources.  and ♦can be then viewed as two simple aggregation
strategies: a pessimistic one (the inﬁmum of positive support and the supremum
of the negative support), and an optimistic one (the dual strategy), respectively.
They can be deﬁned via one another using ¬ in the expected manner: φ stands
for ¬♦¬φ and ♦φ for ¬¬φ. In this paper, in contrast to [15] and [6], we do
allow for modalities to nest.
The other part of our motivation comes from the work on modal G¨odel
logic (GK—in the notation of [36]) equipped with relational semantics [12,13,
36]. There, the authors develop proof and model theory of modal expansions
of G interpreted over frames with both crisp and fuzzy accessibility relations.
In particular, it was shown that the -fragment2 of GK lacks the ﬁnite model
property (FMP) w.r.t. fuzzy frames while the ♦-fragment has FMP3 only w.r.t.
fuzzy (but not crisp) frames. Furthermore, both  and ♦fragments of GK are
PSPACE-complete [28,29].
1 This logic was introduced several times: by Wansing [38] as I4C4 and then by Leit-
geb [27] as HYPE. Cf. [33] for a recent and more detailed discussion.
2 Note that  and ♦are not interdeﬁnable in GK—cf. [36, Lemma 6.1] for details.
3 There is, however, a semantics in [11] w.r.t. which bi-modal GK has FMP.

432
M. B´ılkov´a et al.
Description G¨odel logics, a notational version of modal logics, have found
their use the ﬁeld of knowledge representation [8–10], in particular, in the repre-
sentation of vague or uncertain data which is not possible in the classical ontolo-
gies. In this respect, our paper provides a further extension of representable data
types as we model not only vague reasoning but also non-trivial reasoning with
inconsistent information.
In the present paper, we are expanding the language with the G¨odel coimpli-
cation  to allow for the formalisation of statements expressing that an agent is
strictly more conﬁdent in one statement than in another one (cf. Sect. 2 for the
details). Furthermore, the presence of ¬ will allow us to simplify the frame deﬁn-
ability. Still, we will show that our logic is a conservative extension of GKc—the
modal G¨odel logic of crisp frames from [36] in the language with both  and ♦.
Logics. We are discussing many logics obtained from the propositional G¨odel
logic G. Our main interest is in the logic we denote KG2. It can be produced from
G in several ways: (1) adding De Morgan negation ¬ to obtain G2 (in which case
φ  φ′ can be deﬁned as ¬(¬φ′ →¬φ)) and then further expanding the language
with  or ♦; (2) adding  or Δ (Baaz’ delta) to G, then both  and ♦thus
acquiring KbiG4 (modal bi-G¨odel logic) which is further enriched with ¬. These
and other relations are given on Fig. 2.
Fig. 2. Logics in the article. ﬀstands for ‘permitting fuzzy frames’. Subscripts on
arrows denote language expansions. / stands for ‘or’ and comma for ‘and’.
Plan of the Paper. The remainder of the paper is structured as follows. In
Sect. 2, we deﬁne bi-G¨odel algebras and use them to present KbiG (on both
fuzzy and crisp frames) and then KG2 (on crisp frames), show how to formalise
statements where beliefs of agents are compared, and prove some semantical
properties. In Sect. 3, we show that ♦fragment of KbiGf (KbiG on fuzzy frames)
lacks ﬁnite model property. We then present a ﬁnitely branching fragment of
KG2 (KG2
fb) and argue for its use in representation of agents’ beliefs. In Sect. 4,
we design a constraint tableaux calculus for KG2
fb which we use to obtain the
complexity results. Finally, in Sect. 5 we discuss further lines of research.
4 To the best of our knowledge, the only work on bi-G¨odel (symmetric G¨odel) modal
logic is [20]. There, the authors propose an expansion of biG with  and ♦equipped
with proof-theoretic interpretation and provide its algebraic semantics.

Paraconsistent G¨odel Modal Logic
433
2
Language and Semantics
In this section, we present semantics for KbiG (modal bi-G¨odel logic) over both
fuzzy and crisp frames and the one for KG2 over crisp frames. Let Var be a count-
able set of propositional variables. The language biL¬
,♦is deﬁned via the fol-
lowing grammar.
φ := p ∈Var | ¬φ | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (φ  φ) | φ | ♦φ
Two constants, 0 and 1, can be introduced in the traditional fashion: 0 := p  p,
1 := p →p. Likewise, the G¨odel negation can be also deﬁned as expected:
∼φ := φ →0. The ¬-less fragment of biL¬
,♦is denoted with biL,♦.
To facilitate the presentation, we introduce bi-G¨odel algebras.
Deﬁnition 1. The bi-G¨odel algebra [0, 1]G
= ([0, 1], 0, 1, ∧G, ∨G, →G, G) is
deﬁned as follows: for all a, b ∈[0, 1], the standard operations are given by
a ∧G b := min(a, b), a ∨G b := max(a, b),
a →G b =

1, if a ≤b
b else,
b G a =

0, if b ≤a
b else.
Deﬁnition 2.
– A fuzzy frame is a tuple F = ⟨W, R⟩with W ̸= ∅and R : W × W →[0, 1].
– A crisp frame is a tuple F = ⟨W, R⟩with W ̸= ∅and R ⊆W × W.
Deﬁnition 3 (KbiG models). A KbiG model is a tuple M = ⟨W, R, v⟩with
⟨W, R⟩being a (crisp or fuzzy) frame, and v : Var × W →[0, 1]. v (a valuation)
is extended on complex biL,♦formulas as follows:
v(φ ◦φ′, w) = v(φ, w) ◦G v(φ′, w).
(◦∈{∧, ∨, →, })
The interpretation of modal formulas on fuzzy frames is as follows:
v(φ, w) =
inf
w′∈W{wRw′ →G v(φ, w′)},
v(♦φ, w) = sup
w′∈W
{wRw′ ∧G v(φ, w′)}.
On crisp frames, the interpretation is simpler (here, inf(∅)=1 and sup(∅)=0):
v(φ, w) = inf{v(φ, w′) : wRw′},
v(♦φ, w) = sup{v(φ, w′) : wRw′}.
We say that φ ∈biL,♦is KbiG valid on frame F (denote, F |=KbiG φ) iﬀfor
any w ∈F, it holds that v(φ, w) = 1 for any model M on F.
Note that the deﬁnitions of validity in GKc and GK coincide with those in KbiG
and KbiGf if we consider the -free fragment of biL,♦.
As we have already mentioned, on crisp frames, the accessibility relation can
be understood as availability of (trusted or reliable) sources. In fuzzy frames, it
can be thought of as the degree of trust one has in a source. Then, ♦φ represents

434
M. B´ılkov´a et al.
the search for evidence from trusted sources that supports φ: v(♦φ, t) > 0 iﬀ
there is t′ s.t. tRt′ > 0 and v(φ, t′) > 0, i.e., there must be a source t′ to
which t has positive degree of trust and that has at least some certainty in φ.
On the other hand, if no source is trusted by t (i.e., tRu = 0 for all u), then
v(♦φ, t) = 0. Likewise, χ can be construed as the search of evidence against χ
given by trusted sources: v(χ, t) < 1 iﬀthere is a source t′ that gives to χ less
certainty than t gives trust to t′. In other words, if t trusts no sources, or if all
sources have at least as high conﬁdence in χ as t has in them, then t fails to ﬁnd
a trustworthy enough counterexample.
Deﬁnition 4 (KG2 models). A KG2 model is a tuple M = ⟨W, R, v1, v2⟩with
⟨W, R⟩being a crisp frame, and v1, v2 : Var × W →[0, 1]. The valuations which
we interpret as support of truth and support of falsity, respectively, are extended
on complex formulas as expected.
v1(¬φ, w) = v2(φ, w)
v2(¬φ, w) = v1(φ, w)
v1(φ ∧φ′, w) = v1(φ, w) ∧G v1(φ′, w)
v2(φ ∧φ′, w) = v2(φ, w) ∨G v2(φ′, w)
v1(φ ∨φ′, w) = v1(φ, w) ∨G v1(φ′, w)
v2(φ ∨φ′, w) = v2(φ, w) ∧G v2(φ′, w)
v1(φ →φ′, w) = v1(φ, w)→G v1(φ′, w)
v2(φ →φ′, w) = v2(φ′, w) G v2(φ, w)
v1(φ  φ′, w) = v1(φ, w) G v1(φ′, w)
v2(φ  φ′, w) = v2(φ′, w)→G v2(φ, w)
v1(φ, w) = inf{v1(φ, w′) : wRw′}
v2(φ, w) = sup{v2(φ, w′) : wRw′}
v1(♦φ, w) = sup{v1(φ, w′) : wRw′}
v2(♦φ, w) = inf{v2(φ, w′) : wRw′}
We say that φ ∈biL¬
,♦is KG2 valid on frame F (F |=KG2 φ) iﬀfor any
w ∈F, it holds that v1(φ, w) = 1 and v2(φ, w) = 0 for any model M on F.
Convention 1. In what follows, we will denote a pair of valuations ⟨v1, v2⟩just
with v if there is no risk of confusion. Furthermore, for each frame F and each
w ∈F, we denote
R(w) = {w′ : wRw′ = 1},
(for fuzzy frames)
R(w) = {w′ : wRw′}.
(for crisp frames)
Convention 2. We will further denote with KbiG the set of all formulas KbiG-
valid on all crisp frames; KbiGf the set of all formulas KbiG-valid on all fuzzy
frames; and KG2—the set of all formulas KG2 valid on all crisp frames.
Before proceeding to establish some semantical properties, let us make two
remarks. First, neither  nor ♦are trivialised by contradictions: in contrast to
K, (p∧¬p) →q is not KG2 valid, and neither is ♦(p∧¬p) →♦q. Intuitively,
this means that one can have contradictory but non-trivial beliefs. Second, we
can formalise statements of comparative belief such as the ones we have already
given before:
wallet: I am more conﬁdent that the wallet is Paula’s than that the wallet
is Quentin’s.
ice cream: Alice is more certain than Britney that Claire loves pistachio
ice cream.

Paraconsistent G¨odel Modal Logic
435
For this, consider the following deﬁned operators.
Δτ := ∼(1  τ)
(1)
Δ¬φ := ∼(1  φ) ∧¬∼∼(1  φ)
(2)
It is clear that for any τ ∈biL,♦and φ ∈biL¬
,♦interpreted on KbiG and KG2
models, respectively, it holds that
v(Δτ, w) =

1
if v(τ, w) = 1
0
otherwise,
v(Δ¬φ, w)
=

(1, 0)
if v(φ, w) = (1, 0)
(0, 1)
otherwise.
(3)
Now we can deﬁne formulas that express order relations between values of two
formulas both for KbiG and KG2.
For KbiG they look as follows:
v(τ, w) ≤v(τ ′, w) iﬀv(Δ(τ →τ ′), w) = 1,
v(τ, w) > v(τ ′, w) iﬀv (∼Δ(τ ′ →τ), w) = 1.
In KG2, the orders are deﬁned in a more complicated way:
v(φ, w) ≤v(φ′, w) iﬀv(Δ¬(φ →φ′), w) = (1, 0),
v(φ, w) > v(φ′, w) iﬀv(Δ¬(φ′ →φ) ∧∼Δ¬(φ →φ′), w) = (1, 0).
Observe, ﬁrst, that both in KbiG and KG2 the relation ‘the value of τ (φ) is less
or equal to the value of τ ′ (φ′)’ is deﬁned as ‘τ →τ ′ (φ →φ′) has the designated
value’. In KbiG, the strict order is just a negation of the non-strict order since all
values are comparable. On the other hand, in contrast to KbiG, the strict order
in KG2 is not a simple negation of the non-strict order since KG2 is essentially
two-dimensional. We provide further details in Remark 2.
Finally, we can formalise wallet as follows. We interpret ‘I am conﬁdent’ as 
and substitute ‘the wallet is Paula’s’ with p, and ‘the wallet is Quentin’s’ with q.
Now, we just use the deﬁnition of > in biL¬
,♦to get
Δ¬(p →q) ∧∼Δ¬(q →p).
(4)
For ice cream, we need two diﬀerent modalities: a and b for Alice and Brittney,
respectively. Replacing ‘Alice loves pistachio ice cream’ with p, we get
Δ¬(ap →bp) ∧∼Δ¬(bp →ap).
(5)
Remark 1. Δ is called Baaz’ delta (cf., e.g. [3] for more details). Intuitively, Δτ
can be interpreted as ‘τ has the designated value’ and acts much like a necessity
modality: if τ is KbiG valid, then so is Δτ; moreover, Δ(p →q) →(Δp →Δq)
is valid. Furthermore, Δ and  can be deﬁned via one another in KbiG, thus the
addition of Δ to G makes it more expressive and allows to deﬁne both strict and
non-strict orders.

436
M. B´ılkov´a et al.
Remark 2. Recall that we mentioned in Sect. 1 that an agent should usually be
able to compare their beliefs in diﬀerent statements: this is reﬂected by the fact
that Δ(p →q) ∨Δ(q →p) is KbiG valid. It can be counter-intuitive if the
contents of beliefs have nothing in common, however.
This drawback is avoided if we treat support of truth and support of falsity
independently. Here is where a diﬀerence between KbiG and KG2 lies. In KG2,
we can only compare the values of formulas coordinate-wise, whence Δ¬(p →
q)∨Δ¬(q →p) is not KG2 valid. E.g., if we set v(p, w) = (0.7, 0.6) and v(q, w) =
(0.4, 0.2), v(p, w) and v(q, w) will not be comparable w.r.t. the truth (upward)
order on [0, 1].
We end this section with establishing some useful semantical properties.
Proposition 1. F |=KG2 φ iﬀfor any model M on F and any w∈F, v1(φ, w)=1.
Proof. The ‘if’ direction is evident from the deﬁnition of validity. We show the
‘only if’ part. It suﬃces to show that the following statement holds for any φ
and w ∈F:
for any v(p, w) = (x, y), let v∗(p, w) = (1−y, 1−x). Then v(φ, w) = (x, y)
iﬀv∗(φ, w) = (1 −y, 1 −x).
We proceed by induction on φ. The proof of propositional cases is identical to
the one in [5, Proposition 5]. We consider only the case of φ = ψ since  and
♦are interdeﬁnable.
Let v(ψ, w) = (x, y). Then inf{v1(ψ, w′) : wRw′} = x, and sup{v2(ψ, w′) :
wRw′} = y. Now, we apply the induction hypothesis to ψ, and thus if v(ψ, s) =
(x′, y′), then v∗(ψ, s) = (1−y′, 1−x′) for any s ∈R(w). But then inf{v∗
1(ψ, w′) :
wRw′} = 1 −y, and sup{v∗
2(ψ, w′) : wRw′} = 1 −x as required.
Now, assume that v1(φ, w) = 1 for any v1 and w. We can show that v2(φ, w)=
0 for any w and v2. Assume for contradiction that v2(φ, w)=y>0 but v1(φ, w)=
1. Then, v∗(φ)=(1−y, 1−1)=(1−y, 0). But since y>0, v∗(φ)̸=(1, 0).
Proposition 2.
1. Let φ be a formula over {0, ∧, ∨, →, , ♦}. Then, F |=GK φ iﬀF |=KbiGf φ
and F |=GKc φ iﬀF |=KbiG φ, for any F.
2. Let φ ∈biL,♦. Then, F |=KbiG φ iﬀF |=KG2 φ, for any crisp F.
Proof. 1. follows directly from the semantic conditions of Deﬁnition 3. We con-
sider 2. The ‘only if’ direction is straightforward since the semantic conditions
of v1 in KG2 models and v in KbiG models coincide. The ‘if’ direction follows
from Proposition 1: if φ is valid on F, then v(φ, w) = 1 for any w ∈F and any v
on F. But then, v1(φ, w) = 1 for any w ∈F. Hence, F |=KG2 φ.
3
Model-Theoretic Properties of KG2
In the previous section, we have seen how the addition of  allowed us to formalise
statements considering comparison of beliefs. Here, we will show that both 
and ♦fragments of KbiG, and hence KG2, are strictly more expressive than the
classical modal logic K, i.e. that they can deﬁne all classically deﬁnable classes
of crisp frames as well as some undeﬁnable ones.

Paraconsistent G¨odel Modal Logic
437
Deﬁnition 5 (Frame deﬁnability). Let Σ be a set of formulas. Σ deﬁnes
a class of frames K in a logic L iﬀit holds that F ∈K iﬀF |=L Σ.
The next statement follows from Proposition 2 since K can be faithfully embed-
ded in GKc by substituting each variable p with ∼∼p (cf. [28,29] for details).
Theorem 1. Let K be a class of frames deﬁnable in K. Then, K is deﬁnable in
KbiG and KG2.
Theorem 2. 1. Let F be crisp. Then F is ﬁnitely branching (i.e., R(w) is ﬁnite
for every w ∈F) iﬀF |=KbiG 1  ♦((p  q) ∧q).
2. Let F be fuzzy. Then F is ﬁnitely branching and sup{wRw′ : wRw′ < 1} < 1
for all w ∈F iﬀF |=KbiG 1  ♦((p  q) ∧q).
Proof. We show the case of fuzzy frames since the crisp ones can be tackled
in the same manner. Assume that F is ﬁnitely branching and that sup{wRw′ :
wRw′ <1} < 1 for all w ∈F. It suﬃces to show that v(♦((p  q) ∧q), w) < 1 for
all w ∈F. First of all, observe that there is no w′ ∈F s.t. v((p  q) ∧q, w′) = 1.
It is clear that
sup
wRw′<1
{v((p  q) ∧q, w′) ∧G wRw′} < 1 and that
sup{v((p  q) ∧q, w′) : wRw′ = 1} = max{v((p  q) ∧q, w′) : wRw′ = 1} < 1
since R(w) is ﬁnite. But then v(♦((p  q) ∧q), w) < 1 as required.
For the converse, either (1) R(w) is inﬁnite for some w, or (2) sup{wRw′ :
wRw′ < 1} = 1 for some w. For (1), set v(p, w′) = 1 for every w′ ∈R(w). Now
let W ′ ⊆R(w) and W ′ = {wi : i ∈{1, 2, . . .}}. We set v(q, wi) =
i
i+1. It is easy
to see that sup{v(q, wi) : wi ∈W ′} = 1 and that v((p  q) ∧q, wi) = v(q, wi).
Therefore, v(1  ♦((p  q) ∧q), w) = 0.
For (2), we let v(p, w′) = 1 and further, v(q, w′) = wRw′ for all w′ ∈F. Now
since sup{wRw′ : wRw′ < 1} = 1 and v(((pq)∧q), w′) = v(q, w′) for all w′ ∈F,
it follows that v(♦((p  q) ∧q), w) = 1, whence v(1  ♦((p  q) ∧q), w) = 0.
Remark 3. The obvious corollary of Theorem 2 is the lack of FMP for the ♦-
fragment of KbiGf5 since ♦((p  q) ∧q) in never true in a ﬁnite model. This
diﬀerentiates KbiGf from GK since the ♦-fragment of GK has FMP [12, Theo-
rem 7.1]. Moreover, one can deﬁne ﬁnitely branching frames in  fragments of
GK and GKc. Indeed, ∼∼(p ∨∼p) serves as such deﬁnition.
Corollary 1. KG2 and both  and ♦fragments of KbiG are strictly more
expressive than K.
Proof. From Theorems 1 and 2 since K is complete both w.r.t. all frames and
all ﬁnitely branching frames. The result for KG2 follows since it is conservative
over KbiG (Proposition 2).
5 Bi-modal KbiGf lacks have FMP since it is a conservative extension of GK.

438
M. B´ılkov´a et al.
These results show us that addition of  greatly enhances the expressive power
of our logic. Here it is instructive to remind ourselves that classical epistemic
logics are usually complete w.r.t. ﬁnitely branching frames (cf. [18] for details).
It is reasonable since for practical reasoning, agents cannot consider inﬁnitely
many alternatives. In our case, however, if we wish to use KbiG and KG2 for
knowledge representation, we need to impose ﬁnite branching explicitly.
Furthermore, allowing for inﬁnitely branching frames in KbiG or KG2 leads to
counter-intuitive consequences. In particular, it is possible that v(φ, w) = (0, 1)
even though there are no w′, w′′ ∈R(w) s.t. v1(φ, w′) = 0 or v2(φ, w′′) = 1. In
other words, there is no source that decisively falsiﬁes φ, furthermore, all sources
have some evidence for φ, and yet we somehow believe that φ is completely
false and untrue. Dually, it is possible that v(♦φ, w) = (1, 0) although there
are no w′, w′′ ∈R(w) s.t. v1(φ, w′) = 1 or v2(φ, w′′) = 0. Even though ♦is an
‘optimistic’ aggregation, it should not ignore the fact that all sources have some
evidence against φ but none supports it completely.
Of course, this situation is impossible if we consider only ﬁnitely branching
frames for inﬁma and suprema will become minima and maxima. There, all
values of modal formulas will be witnessed by some accessible states in the
following sense. For ♥∈{, ♦}, i ∈{1, 2}, if vi(♥φ, w) = x, then there is
w′ ∈R(w) s.t. vi(φ, w′) = x. Intuitively speaking, ﬁnitely branching frames
represent the situation when our degree of certainty in some statement is based
uniquely on the data given by the sources.
Convention 3. We will further use KbiGfb and KG2
fb to denote the sets of all
biL,♦and biL¬
,♦formulas valid on ﬁnitely branching crisp frames.
Observe, moreover, that  and ♦are still undeﬁnable via one another in biL,♦.
The proof is the same as that of [36, Lemma 6.1].
Proposition 3.  and ♦are not interdeﬁnable in KbiGfb.
Corollary 2.
1.  and ♦are not interdeﬁnable in KbiG, KbiGf
fb, and KbiGf.
2. Both  and ♦fragments of KbiG are more expressive than K.
In the remainder of the paper, we are going to provide a complete proof system
for KG2
fb (and hence, KbiGfb), and establish its decidability and complexity as
well as ﬁnite model property. Note, however, that the latter is not entirely for
granted. In fact, several expected ways of deﬁning ﬁltration (cf. [7,14] for more
details thereon) fail.
Let Σ ⊆biL,♦be closed under subformulas. If we want to have ﬁltration
for KbiGfb, there are three intuitive ways to deﬁne ∼Σ on the carrier of a model
that is supposed to relate states satisfying the same formulas.
1. w ∼1
Σ w′ iﬀv(φ, w) = v(φ, w′) for all φ ∈Σ.
2. w ∼2
Σ w′ iﬀv(φ, w) = 1 ⇔v(φ, w′) = 1 for all φ ∈Σ.
3. w ∼3
Σ w′ iﬀv(φ, w) ≤v(φ′, w) ⇔v(φ, w′) ≤v(φ′, w′) for all φ, φ′ ∈Σ∪{0,1}.

Paraconsistent G¨odel Modal Logic
439
Consider the model on Fig. 3 and two formulas:
φ≤:= ∼∼(p →♦p)
φ> := ∼∼(p  ♦p)
Now let Σ to be the set of all subformulas of φ≤∧φ>.
First of all, it is clear that v(φ≤∧φ>, w) = 1 for any w ∈M. Observe now
that all states in M are distinct w.r.t. ∼1
Σ. Thus, the ﬁrst way of constructing
the carrier of the new model does not give the FMP.
Fig. 3. v(p, wn) =
1
n+1
As regards to ∼2
Σ and ∼3
Σ, one can check that for any w, w′ ∈M, it holds that
w ∼2
Σ w′ and w ∼3
Σ w′. So, if we construct a ﬁltration of M using equivalence
classes of either of these two relations, the carrier of the resulting model is going
to be ﬁnite. Even more so, it is going to be a singleton.
However, we can show that there is no ﬁnite model N = ⟨U, S, e⟩s.t.
∀s ∈N : v(φ≤∧φ>, s) = 1.
Indeed, e(φ≤, t) = 1 iﬀe(p, t′) > 0 for some t′ ∈S(t), while e(φ>, t) = 1 iﬀ
v(p, t) > v(p, t′) for any t′ ∈S(t). Now, if U is ﬁnite, we have two options: either
(1) there is u ∈U s.t. R(u) = ∅, or (2) U contains a ﬁnite S-cycle.
For (1), note that v(♦p, u) = 0, and we have two options: if e(p, u) = 0, then
e(φ>, u) = 0; if, on the other hand, e(p, u) > 0, then e(φ≤, u) = 0. For (2),
assume w.l.o.g. that the S-cycle looks as follows: u0Su1Su2 . . . SunSu0.
If e(p, u0)=0, e(φ>, u0)=0, so e(p, u0)>0. Furthermore, e(p, ui)>e(p, ui+1).
Otherwise, again, e(φ>, ui) = 0. But then we have e(φ>, ui) = 0.
But this means that ∼2
Σ and ∼3
Σ do not preserve truth of formulas from w
to [w]Σ, i.e., neither of these two relations can be used to deﬁne ﬁltration. Thus,
in order to explicitly prove the ﬁnite model property and establish complexity
evaluations for KbiGfb and KG2
fb, we will provide a tableaux calculus. It will also
serve as a decision procedure for satisﬁability and validity of formulas.
4
Tableaux for KG2
fb
Usually, proof theory for modal and many-valued logics is presented in one of the
following several forms. The ﬁrst one is a Hilbert-style axiomatisation as given in
e.g. [23] for the propositional G¨odel logic and in [12,13,36] for its modal expan-
sions. Hilbert calculi are useful for establishing frame correspondence results as
well as for showing that one logic extends another one in the same language. On
the other hand, their completeness proofs might be quite complicated, and the
proof-search not at all straightforward. Second, there are non-labelled sequent
and hyper-sequent calculi (cf. [30] for the propositional proof systems and [28,29]

440
M. B´ılkov´a et al.
for the modal hypersequent calculi). With regards to modal logics, completeness
proofs of (hyper)sequent calculi often provide the answer for the decidability
problem. Furthermore, the proof search can be quite straightforwardly automa-
tised provided that the calculus is cut-free.
Finally, there are proof systems that directly incorporate semantics: in par-
ticular, tableaux (e.g., the ones for G¨odel logics [2] and tableaux for Lukasiewicz
description logic [25]) and labelled sequent calculi (cf., e.g. [32] for labelled
sequent calculi for classical modal logics). Because of the calculi’s nature, their
completeness proofs are usually simple. Besides, the calculi serve as a decision
procedure that either establishes that the given formula is valid or provides an
explicit countermodel.
Our tableaux system T

KG2
fb

is a straightforward modal expansion of con-
straint tableaux for G2 presented in [5]. It is inspired by constraint tableaux
for Lukasiewicz logics from [21,22] (but cf. [26] for an approach similar to ours)
which we modify with two-sorted labels corresponding to the support of truth
and support of falsity in the model. This idea comes from tableaux for the
Belnap—Dunn logic by D’Agostino [1]. Moreover, since KG2
fb is a conservative
extension of KbiGfb, our calculus can be used for that logic as well if we apply
only the rules that govern the support of truth of biL,♦formulas.
Deﬁnition 6 (T

KG2
fb

). We ﬁx a set of state-labels W and let ≲∈{<, ⩽} and
≳∈{>, ⩾}. Let further w ∈W, x∈{1, 2}, φ∈biL¬
,♦, and c∈{0, 1}. A structure
is either w:x:φ or c. We denote the set of structures with Str.
We deﬁne a constraint tableau as a downward branching tree whose branches
are sets containing the following types of entries:
– relational constraints of the form wRw′ with w, w′ ∈W;
– structural constraints of the form X ≲X′ with X, X′ ∈Str.
Each branch can be extended by an application of a rule6 from Fig. 4 or Fig. 5.
A tableau’s branch B is closed iﬀone of the following conditions applies:
– the transitive closure of B under ≲contains X < X;
– 0 ⩾1 ∈B, or X > 1 ∈B, or X < 0 ∈B.
A tableau is closed iﬀall its branches are closed. We say that there is a tableau
proof of φ iﬀthere is a closed tableau starting from the constraint w:1:φ < 1.
An open branch B is complete iﬀthe following condition is met.
* If all premises of a rule occur on B, then its one conclusion7 occurs on B.
Remark 4. Note that due to Proposition 1, we need to check only one valuation
of φ to verify its validity.
Convention 4 (Interpretation of constraints). The following table gives
the interpretations of structural constraints on the example of ⩽.
6 If X < 1 and X < X′ (or 0 < X′ and X < X′) occur on B, then the rules are applied
only to X < X′.
7 Note that branching rules have two conclusions.

Paraconsistent G¨odel Modal Logic
441
Fig. 4. Propositional rules of T

KG2
fb

. Bars denote branching.

442
M. B´ılkov´a et al.
entry
interpretation
w: 1:φ ⩽w′ : 2:φ′
v1(φ, w) ≤v2(φ′, w′)
w:2:φ ⩽c
v2(φ, w) ≤c with c ∈{0, 1}
As one can see from Fig. 4 and Fig. 5, the rules follow the semantical conditions
from Deﬁnition 4. Let us discuss →1⩽and 1 ≲in more details.
The premise of →1⩽is interpreted as v1(φ →φ′, w) ⩽x. To decompose the
implication, we check two options: either x = 1 (then, the value of φ →φ′ is
arbitrary) or x < 1. In the second case, we use the semantics to obtain that
v1(φ′, w) ⩽x and v1(φ, w) > v1(φ′, w).
Fig. 5. Modal rules of T

KG2
fb

. w′′ is fresh on the branch.
In order to apply 1 ≲to w:1:φ ≲X, we introduce a new state w′′ that is
seen by w. Since we work in a ﬁnite branching model, w′′ can witness the value
of φ. Thus, we add w′′ :1:φ ≲X.
We also provide an example of how our tableaux work. On Fig. 6, one can
see a successful proof on the left and a failed proof on the right.
Fig. 6. × indicates closed branches;  indicates complete open branches.

Paraconsistent G¨odel Modal Logic
443
Deﬁnition 7 (Branch realisation). We say that a model M = ⟨W, R, v1, v2⟩
with W = {w : w occurs on B} and R = {⟨w, w′⟩: wRw′ ∈B} realises a branch
B of a tree iﬀthe following conditions are met.
– vx(φ, w) ≤vx′(φ′, w′) for any w : x : φ ⩽w′ : x′ : φ′ ∈B with x, x′ ∈{1, 2}.
– vx(φ, w) ≤c for any w : x : φ ⩽c ∈B with c ∈{0, 1}.
Theorem 3 (Completeness). φ is KG2
fb valid iﬀit has a T (KG2
fb) proof.
Proof. We consider only the KG2
fb case since KbiGfb can be handled the same
way. For soundness, we check that if the premise of the rule is realised, then so is
at least one of its conclusions. We consider the cases of →1⩽and 1 ≲. Assume
that w : 1 : φ →φ′ ⩽X is realised and assume w.l.o.g. that X = u : 2 : ψ. It is
clear that either v2(ψ, u) = 1 or v2(ψ, u) < 1. In the ﬁrst case, X ⩾1 is realised.
In the second case, we have that v1(φ, w) > v1(φ′, w) and v1(φ′, w) ⩽v2(ψ, u).
Thus, X < 1, w : 1 : φ > w : 1 : φ′, and w : 1 : φ′ ⩽u : 1 : ψ are realised as well, as
required.
For 1 ≲, assume that w:1: φ⩽X is realised and assume w.l.o.g. that X =
u:2:ψ. Thus, v1(φ, w) ⩽v2(ψ, u) Then, since the model is ﬁnitely branching,
there is an accessible state w′′ s.t. v1(φ, w) ⩽v2(ψ, u). Thus, w′′ : 1 : φ ⩽X is
realised too.
As no closed branch is realisable, the result follows.
For completeness, we show that every complete open branch B is realisable.
We construct the model as follows. We let W = {w : w occurs in B}, and set
R = {⟨w, w′⟩: wRw′ ∈B}. Now, it remains to construct the suitable valuations.
For i ∈{1, 2}, if w : i : p ⩾1 ∈B, we set vi(p, w) = 1. If w : i : p ⩽0 ∈B,
we set vi(p, w) = 0. To set the values of the remaining variables q1, . . . , qn, we
proceed as follows. Denote B+ the transitive closure of B under ≲and let
[w:x:qi]=
⎧
⎨
⎩w′ :x′ :qj

w:x:qi ⩽w′ :x′ :qj ∈B+ and w:x:qi <w′ :x′ :qj /∈B+
or
w:x:qi ⩾w′ :x′ :qj ∈B+ and w:x:qi >w′ :x′ :qj /∈B+
⎫
⎬
⎭
It is clear that there are at most 2 · n · |W| [w : x : qi]’s since the only possible
loop in B+ is wi1 :x:r ⩽. . . ⩽wi1 :x:r, but in such a loop all elements belong
to [wi1 :x:r]. We put [w:x:qi] ≺[w′ :x′ :qj] iﬀthere are wk :x:r ∈[w:x:qi] and
w′
k :x′ :r′ ∈[w′ :x′ :qj] s.t. wk :x:r < w′
k :x′ :r′ ∈B+.
We now set the valuation of these variables as follows
vx(qi, w) = |{[w′ :x′ :q′] | [w′ :x′ :q′] ≺[w:x:qi]}|
2 · n · |W|
Note that if some φ contains s but B+ contains no inequality with it, the above
deﬁnition ensures that s is going to be evaluated at 0. Thus, all constraints
containing only variables are satisﬁed.
It remains to show that all other constraints are satisﬁed. For that, we prove
that if at least one conclusion of the rule is satisﬁed, then so is the premise. The
propositional cases are straightforward and can be tackled in the same manner

444
M. B´ılkov´a et al.
as in [5, Theorem 2]. We consider only the case of ♦2 ≳. Assume w.l.o.g. that
≳=⩾and X = u : 1 : ψ. Since B is complete, if w : 2 : ♦φ ⩾u : 1 : ψ ∈B, then
for any w′ s.t. wRw′ ∈B, we have w′ : 2 : φ ⩾u : 1 : ψ ∈B, and all of them are
realised by M. But then w: 2:♦φ ⩾u:1:ψ is realised too, as required.
Theorem 4.
1. Let φ ∈biL¬
,♦be not KG2
fb valid, and let |φ| denote the number of symbols in
it. Then there is a model M of the size O(|φ||φ|) and depth O(|φ|) and w ∈M
s.t. v1(φ, w) ̸= 1.
2. KG2
fb validity and satisﬁability8 are PSPACE-complete.
Proof. We begin with 1. By Theorem 3, if φ is not KG2
fb valid, we can build
a falsifying model using tableaux. It is also clear from the rules on Fig. 5 that
the depth of the constructed model is bounded from above by the maximal
number of nested modalities in φ. The width of the model is bounded by the
maximal number of modalities on the same level of nesting. The sharpness of the
bound is obtained using the embedding of K into KG2
fb since K is complete w.r.t.
ﬁnitely branching models and it is possible to force shallow trees of exponential
size in K (cf., e.g. [7, §6.7]). The embedding also entails PSPACE-hardness. It
remains to tackle membership.
First, observe from the proof of Theorem 3 that φ(p1, . . . , pn) is satisﬁable
(falsiﬁable) on M = ⟨W, R, v1, v2⟩iﬀthere are v1 and v2 that give variables values
from V =

0,
1
2·n·|W |, . . . , 2·n·|W |−1
2·n·|W | , 1

under which φ is satisﬁed (falsiﬁed).
As we mentioned, |W| is bounded from above by kk+1 with k being the
number of modalities in φ. Therefore, we replace structural constraints with
labelled formulas of the form w :i:φ=v (v ∈V) avoiding comparisons of values
of formulas in diﬀerent states. As expected, we close the branch if it contains
w:i:ψ=v and w:i:ψ=v′ for v ̸= v′.
Now we replace the rules with the new ones that work with labelled formulas
instead of structural constraints. Below, we give as an example new rules for →
and ♦9 (with |V| = m + 1):
w:1:φ →φ′ =1
w:1:φ = 0

w:1:φ=
1
m+1
w:1:φ′ =
1
m+1

w:1:φ=
1
m+1
w:1:φ′ =
2
m+1
 . . .

w:1:φ= m−1
m+1
w:1:φ′ =
m
m+1
 w:1:φ′ =1
w:1:♦φ=
r
m+1
wRw′′; w′′ :1:φ=
r
m+1
w:1:♦φ=
r
m+1; wRw′
w′ :1:φ=0 | . . . | w′ :1:φ= r−1
m+1
8 Satisﬁability and falsiﬁability (non-validity) are reducible to each other using : φ
is satisﬁable iﬀ∼∼(φ  0) is falsiﬁable; φ is falsiﬁable iﬀ∼∼(1  φ) is satisﬁable.
9 Intuitively, for a value 1 > v > 0 of ♦φ at w, we add a new state that witnesses v,
and for a state on the branch, we guess a value smaller than v. Other modal rules
can be rewritten similarly.

Paraconsistent G¨odel Modal Logic
445
We now show how to build a satisfying model for φ using polynomial space.
We begin with w0 : 1 : φ = 1 and start applying propositional rules (ﬁrst, those
that do not require branching). If we implement a branching rule, we pick one
branch and work only with it: either until the branch is closed, in which case
we pick another one; until no more rules are applicable (then, the model is
constructed); or until we need to apply a modal rule to proceed. At this stage,
we need to store only the subformulas of φ with labels denoting their value at w0.
Now we guess a modal formula (say, w0 :2:χ=
1
m+1) whose decomposition
requires an introduction of a new state (w1) and apply this rule. Then we apply
all modal rules that use w0Rw1 as a premise (again, if those require branching,
we guess only one branch) and start from the beginning with the propositional
rules. If we reach a contradiction, the branch is closed. Again, the only new
entries to store are subformulas of φ (now, with fewer modalities), their values
at w1, and a relational term w0Rw1. Since the depth of the model is O(|φ|) and
since we work with modal formulas one by one, we need to store subformulas of
φ with their values O(|φ|) times, so, we need only O(|φ|2) space.
Finally, if no rule is applicable and there is no contradiction, we mark w0 :
2 : χ =
1
m+1 as ‘safe’. Now we delete all entries of the tableau below it and
pick another unmarked modal formula that requires an introduction of a new
state. Dealing with these one by one allows us to construct the model branch by
branch. But since the length of each branch of the model is bounded by O(|φ|)
and since we delete branches of the model once they are shown to contain no
contradictions, we need only polynomial space.
We end the section with two simple observations. First, Theorems 3 and 4
are applicable both to KbiGfb and KG2
fb because the latter is conservative over
the former. Secondly, since KG2 and KbiG are conservative over GKc and since
K can be embedded in GKc, the lower bounds on complexity of a classical modal
logic of some class of frames K and G2 modal logic of K will coincide.
5
Concluding Remarks
In this paper, we developed a crisp modal expansion of the two-dimensional
G¨odel logic G2 as well as an expansion of bi-G¨odel logic with  and ♦both for
crisp and fuzzy frames. We also established their connections with modal G¨odel
logics, and gave a complexity analysis of their ﬁnitely branching fragments.
The following steps are: to study the proof theory of KG2 and KG2
fb: both
in the form of Hilbert-style and sequent calculi; establish the decidability (or
lack thereof) for the case of KG2. Moreover, two-dimensional treatment of infor-
mation invites for diﬀerent modalities, e.g. those formalising aggregation strate-
gies given in [6]—in particular, the cautious one (where the agent takes min-
ima/inﬁma of both positive and negative supports of a given statement) and
the conﬁdent one (whereby the maxima/suprema are taken). Last but not least,
while in this paper we assumed that our access to sources is crisp, one can argue
that the degree of our bias towards the given source can be formalised via fuzzy
frames. Thus, it would be instructive to construct a fuzzy version of KG2.

446
M. B´ılkov´a et al.
In a broader perspective, we plan to provide a general treatment of two-
dimensional modal logics of uncertainty. Indeed, within our project [5,6], we are
formalising reasoning with heterogeneous and possibly incomplete and inconsis-
tent information (such as crisp or fuzzy data, personal beliefs, etc.) in a modular
fashion. This modularity is required because diﬀerent contexts should be treated
with diﬀerent logics—indeed, not only the information itself can be of various
nature but the reasoning strategies of diﬀerent agents even applied to the same
data are not necessarily the same either. Thus, since we wish to account for this
diversity, we should be able to combine diﬀerent logics in our approach.
References
1. Agostino, M.D.: Investigations into the complexity of some propositional calculi.
Oxford University Computing Laboratory, Oxford (1990)
2. Avron, A., Konikowska, B.: Decomposition proof systems for G¨odel-Dummett log-
ics. Stud. Logica 69(2), 197–219 (2001)
3. Baaz, M.: Inﬁnite-valued G¨odel logics with 0-1-projections and relativizations. In:
G¨odel 1996: Logical Foundations of Mathematics, Computer Science and Physics–
Kurt G¨odel’s legacy, Brno, Czech Republic, August 1996, Proceedings, pp. 23–33.
Association for Symbolic Logic (1996)
4. Belnap, Nuel D..: How a computer should think. In: Omori, H., Wansing, H. (eds.)
New Essays on Belnap-Dunn Logic. SL, vol. 418, pp. 35–53. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-31136-0 4
5. B´ılkov´a, M., Frittella, S., Kozhemiachenko, D.: Constraint tableaux for two-
dimensional fuzzy logics. In: Das, A., Negri, S. (eds.) TABLEAUX 2021. LNCS
(LNAI), vol. 12842, pp. 20–37. Springer, Cham (2021). https://doi.org/10.1007/
978-3-030-86059-2 2
6. B´ılkov´a, M., Frittella, S., Majer, O., Nazari, S.: Belief based on inconsistent infor-
mation. In: Martins, M.A., Sedl´ar, I. (eds.) Dynamic Logic. New Trends and
Applications, pp. 68–86. Springer, Cham (2020). https://doi.org/10.1007/978-3-
030-65840-3 5
7. Blackburn, P., Rijke, M.D., Venema, Y.: Modal logic. Cambridge Tracts in Theo-
retical Computer Science, vol. 53. Cambridge University Press, 4. print. with corr.
edn. (2010)
8. Bobillo, F., Delgado, M., G´omez-Romero, J., Straccia, U.: Fuzzy description logics
under G¨odel semantics. Int. J. Approx. Reason. 50(3), 494–514 (2009)
9. Bobillo, F., Delgado, M., G´omez-Romero, J., Straccia, U.: Joining G¨odel and Zadeh
fuzzy logics in fuzzy description logics. Int. J. Uncertain. Fuzziness Knowledge-
Based Syst. 20(04), 475–508 (2012)
10. Borgwardt, S., Distel, F., Pe˜naloza, R.: Decidable G¨odel description logics without
the ﬁnitely-valued model property. In: Fourteenth International Conference on the
Principles of Knowledge Representation and Reasoning (2014)
11. Caicedo, X., Metcalfe, G., Rodr´ıguez, R., Rogger, J.: A ﬁnite model property for
G¨odel modal logics. In: Libkin, L., Kohlenbach, U., de Queiroz, R. (eds.) WoLLIC
2013. LNCS, vol. 8071, pp. 226–237. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-39992-3 20
12. Caicedo, X., Rodriguez, R.: Standard G¨odel modal logics. Stud. Logica 94(2),
189–214 (2010). https://doi.org/10.1007/s11225-010-9230-1

Paraconsistent G¨odel Modal Logic
447
13. Caicedo, X., Rodr´ıguez, R.: Bi-modal G¨odel logic over [0,1]-valued Kripke frames.
J. Log. Comput. 25(1), 37–55 (2015)
14. Chagrov, A., Zakharyaschev, M.: Modal Logic. Clarendon Press, Oxford (1997)
15. Cintula, P., Noguera, C.: Modal logics of uncertainty with two-layer syntax: a
general completeness theorem. In: Kohlenbach, U., Barcel´o, P., de Queiroz, R.
(eds.) WoLLIC 2014. LNCS, vol. 8652, pp. 124–136. Springer, Heidelberg (2014).
https://doi.org/10.1007/978-3-662-44145-9 9
16. Dunn, J.M.: Intuitive semantics for ﬁrst-degree entailments and ‘coupled trees’.
Philos. Stud. 29(3), 149–168 (1976)
17. Ertola, R., Esteva, F., Flaminio, T., Godo, L., Noguera, C.: Paraconsistency prop-
erties in degree-preserving fuzzy logics. Soft Comput. 19, 1–16 (2014). https://doi.
org/10.1007/s00500-014-1489-0
18. Fagin, R., Halpern, J.Y., Moses, Y., Vardi, M.Y.: Reasoning About Knowledge.
MIT Press, Cambridge (2003)
19. Ginsberg, M.: Multivalued logics: a uniform approach to reasoning in AI. Comput.
Intell 4, 256–316 (1988)
20. Grigolia, R., Kiseliova, T., Odisharia, V.: Free and projective bimodal symmetric
g¨odel algebras. Stud. Logica 104(1), 115–143 (2016)
21. H¨ahnle, R.: A new translation from deduction into integer programming. In: Cal-
met, J., Campbell, J.A. (eds.) AISMC 1992. LNCS, vol. 737, pp. 262–275. Springer,
Heidelberg (1993). https://doi.org/10.1007/3-540-57322-4 18
22. H¨ahnle, R.: Many-valued logic and mixed integer programming. Ann. Math. Artif.
Intell. 12(3–4), 231–263 (1994)
23. H´ajek, P.: Metamathematics of Fuzzy Logic. Trends in Logic, 4th edn. Springer,
Dordrecht (1998). https://doi.org/10.1007/978-94-011-5300-3
24. Jansana, R., Rivieccio, U.: Residuated bilattices. Soft. Comput. 16(3), 493–504
(2012)
25. Kulacka,
A.,
Pattinson,
D.,
Schr¨oder,
L.:
Syntactic
labelled
tableaux
for
Lukasiewicz fuzzy ALC. In: Twenty-Third International Joint Conference on Arti-
ﬁcial Intelligence. AAAI Press (2013)
26. Lascio, L.D., Gisolﬁ, A.: Graded tableaux for rational Pavelka logic. Int. J. Intell.
Syst. 20(12), 1273–1285 (2005)
27. Leitgeb, H.: Hype: a system of hyperintensional logic (with an application to seman-
tic paradoxes). J. Philos. Log. 48(2), 305–405 (2019)
28. Metcalfe, G., Olivetti, N.: Proof systems for a G¨odel modal logic. In: Giese,
M., Waaler, A. (eds.) TABLEAUX 2009. LNCS (LNAI), vol. 5607, pp. 265–279.
Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-02716-1 20
29. Metcalfe, G., Olivetti, N.: Towards a proof theory of G¨odel modal logics. Log.
Methods Comput. Sci. 7 (2011)
30. Metcalfe, G., Olivetti, N., Gabbay, D.: Proof Theory for Fuzzy Logics. Applied
Logic Series, vol. 36. Springer, Dordrecht (2008). https://doi.org/10.1007/978-1-
4020-9409-5
31. Moisil, G.: Logique modale. Disquisitiones mathematicae et physicae 2, 3–98 (1942)
32. Negri, S.: Proof analysis in modal logic. J. Philos. Log. 34(5–6), 507–544 (2005)
33. Odintsov, S., Wansing, H.: Routley star and hyperintensionality. J. Philos. Log.
50, 33–56 (2021)
34. Omori, H., Wansing, H.: 40 years of FDE: an introductory overview. Stud. Logica.
105(6), 1021–1049 (2017). https://doi.org/10.1007/s11225-017-9748-6
35. Rivieccio, U.: An algebraic study of bilattice-based logics. Ph.D. thesis, University
of Barcelona – University of Genoa (2010)

448
M. B´ılkov´a et al.
36. Rodriguez, R.O., Vidal, A.: Axiomatization of Crisp G¨odel Modal Logic. Stud.
Logica 109(2), 367–395 (2020). https://doi.org/10.1007/s11225-020-09910-5
37. Vakarelov, D.: Notes on N-lattices and constructive logic with strong negation.
Stud. Logica 36(1–2), 109–125 (1977)
38. Wansing, H.: Constructive negation, implication, and co-implication. J. Appl. Non-
Classical Logics 18(2–3), 341–364 (2008). https://doi.org/10.3166/jancl.18.341-
364
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Non-associative, Non-commutative Multi-modal
Linear Logic
Eben Blaisdell1, Max Kanovich2, Stepan L. Kuznetsov3,4, Elaine Pimentel2(B),
and Andre Scedrov1
1 Department of Mathematics, University of Pennsylvania, Philadelphia, USA
2 Department of Computer Science, University College London, London, UK
elaine.pimentel@gmail.com
3 Steklov Mathematical Institute of RAS, Moscow, Russia
4 Faculty of Computer Science, HSE University, Moscow, Russia
Abstract. Adding multi-modalities (called subexponentials) to linear logic
enhances its power as a logical framework, which has been extensively used in
the speciﬁcation of e.g. proof systems, programming languages and bigraphs. Ini-
tially, subexponentials allowed for classical, linear, afﬁne or relevant behaviors.
Recently, this framework was enhanced so to allow for commutativity as well.
In this work, we close the cycle by considering associativity. We show that the
resulting system (acLLΣ) admits the (multi)cut rule, and we prove two undecid-
ability results for fragments/variations of acLLΣ.
1
Introduction
Resource aware logics have been object of passionate study for quite some time now.
The motivations for this passion vary: resource consciousness are adequate for mod-
eling steps of computation; logics have interesting algebraic semantics; calculi have
nice proof theoretic properties; multi-modalities allow for the speciﬁcation of several
behaviors; there are many interesting applications in linguistics, etc.
With this variety of subjects, applications and views, it is not surprising that dif-
ferent groups developed different systems based on different principles. For example,
the Lambek calculus (L) [29] was introduced for mathematical modeling of natural lan-
guage syntax, and it extends a basic categorial grammar [3,4] by a concatenation oper-
ator. Linear logic (LL) [16], originally discovered by Girard from a semantical analysis
of the models of polymorphic λ-calculus, turned out to be a reﬁnement of classical and
intuitionistic logic, having the dualities of the former and constructive properties of the
The work of Max Kanovich was partially supported by EPSRC Programme Grant EP/R006865/1:
“Interface Reasoning for Interacting Systems (IRIS).”
The work of Stepan L. Kuznetsov was supported by the Theoretical Physics and Mathemat-
ics Advancement Foundation “BASIS” and partially performed within the framework of HSE
University Basic Research Program and within the project MK-1184.2021.1.1 “Algorithmic and
Semantic Questions of Modal Extensions of Linear Logic” funded by the Ministry of Science
and Higher Education of Russia.
Elaine Pimentel acknowledges partial support by the MOSAIC project (EU H2020-MSCA-RISE-
2020 Project 101007627).
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 449–467, 2022.
https://doi.org/10.1007/978-3-031-10769-6_27

450
E. Blaisdell et al.
latter. The key point is the presence of the modalities !, ?, called exponentials in LL. In
the intuitionistic version of LL, denoted by ILL, only the ! exponential is present.
L and LL were compared in [2], when Abrusci showed that Lambek cal-
culus coincides with a variant of the non-commutative, multiplicative version of
ILL [41]. This correspondence can be lifted for considering also the additive connec-
tives: Full (multiplicative-additive) Lambek calculus FL relates to non-commutative
multiplicative-additive version of ILL, here denoted by cLL.
In this paper we propose the sequent based system acLLΣ, a conservative extension
of cLL, where associativity is allowed only for formulas marked with a special kind of
modality, determined by a subexponential signature Σ. The notation adopted is mod-
ular, uniform and scalable, in the sense that many well known systems will appear as
fragments or special cases of acLLΣ, by only modifying the signature Σ. The core frag-
ment of acLLΣ (i.e., without the subexponentials) corresponds to the non-associative
version of full Lambek calculus, FNL [8].1
The language of acLLΣ consists of a denumerable inﬁnite set of propositional vari-
ables {p, q, r, . . .}, the unities {1, ⊤}, the binary connectives for additive conjunc-
tion and disjunction {&, ⊕}, the non-commutative multiplicative conjunction ⊗, the
non-commutative linear implications {→, ←}, and the unary subexponentials !i, with i
belonging to a pre-ordered set of labels (I, ⪯).
Roughly speaking, subexponentials [13] are substructural multi-modalities. In LL,
! A indicates that the linear formula A behaves classically, that is, it can be contracted
and weakened. Labeling ! with indices allows moving one step further: The set I can
be partitioned so that, in !iA, A can be contracted and/or weakened. This allows for
two other types of behavior (other than classical or linear): afﬁne (only weakening) or
relevant (only contraction). Pre-ordering the labels (together with an upward closeness
requirement) guarantees cut-elimination [42]. But then, why consider only weakening
and contraction? Why not also take into account other structural properties, like com-
mutativity or associativity? In [20,21] commutativity was added to the picture, so that
in !iA, A can be contracted, weakened, classical or linear, but it may also commute with
the neighbor formula. In this work we consider the last missing part: Associativity.
Smoothly extending cLL to allow consideration of the non-associative case is
non trivial. This requires a structural recasting/reframing of sequents: we pass from
sets/multisets to lists in the non-commutative case, onto trees in the case of non-
associativity [28]. As a consequence, the inference rules should act deeply over formu-
las in tree-structured sequents, which can be tricky in the presence of modalities [17].
On the other side, the multi-modal Lambek calculus introduced in [35,45] and
extended/compiled/implemented in [18,36–38]2 use different families of connectives
and contexts, distinguished by means of indices, or modes. Contexts are indexed binary
trees, with formulas built from the indexed adjoint connectives {→i, ←i} and ⊗i (e.g.
1 The multiplicative fragment of acLLΣ is the non-associative version of Lambek’s calculus, NL,
introduced by Lambek himself in [30]. Both the associative calculus L and the non-associative
calculus NL have their advantages and disadvantages for the analysis of natural language syn-
tax, as we discuss in more detail in Sect. 2.2.
2 The Grail family of theorem provers [37] works with a variety of modern type-logical frame-
works, including multimodal type-logical grammars.

Non-associative, Non-commutative Multi-modal Linear Logic
451
(A →i B, (C ⊗j D, H)k)i). Each mode has its own set of logical rules (following
the same rule scheme), and different structural features can be combined via the mode
information on the formulas. This gives to the resulting system a multi-modal ﬂavor,
but it also results in a language of binary connectives, determined by the modes. This
forces an unfortunate second level synchronization between implications and tensor,
and modalities act over whole sequents, not on single formulas.
In order to attribute particular resource management properties to individual
resources, in [27,33] explicit (classical) multi-modalities i, i were proposed. While
such unary modalities were inspired in LL exponentials, the resemblance stops there.
First of all, the logical connectives come together with structural constructors for con-
texts, which turns i, i into truncated forms of product and implication.
Second, i, i have a temporal behavior, in the sense that F ⇒F and F ⇒
F, which are not provable in LL using the “natural interpretation”  = ?,  = !.
In this paper, multi-modality is totally local, given by the subexponentials. The
signature Σ contains the pre-ordered set of labels, together with a function stating which
axioms, among weakening, contraction, exchange and associativity, are assumed for
each label. Sequents will have a nested structure, corresponding to trees of formulas.
And rules will be applied deeply in such structures. This not only gives the LL based
system a more modern presentation (based on nested systems, like e.g. in [10,15]),
but it also brings the notation closer to the one adopted by the Lambek community,
like in [25]. Finally, it also uniformly extends several LL based systems present in the
literature, as Example 8 in the next section shows.
Designing a good system serves more than simple pure proof-theoretic interests:
Well behaved, neat proof systems can be used in order to approach several impor-
tant problems, such as interpolation, complexity and decidability. And decidability of
extensions/variants/fragments of L and LL is a fascinating subject of study, since the
presence or absence of substructural properties/connectives may completely change the
outcome. Indeed, it is well known that LL is undecidable [32], but adding weakening
(afﬁne LL) turns the system decidable [24], while removing the additives (MELL –
multiplicative, exponential LL) reaches the border of knowledge: It is a long standing
open problem [50]. Non-associativity also alters decidability and complexity: L is NP-
complete [47], while NL is decidable in polynomial time [1,6]. Finally, the number of
subexponentials also plays a role in decision problems: MELL with two subexponentials
is undecidable [9].
In this work, we will present two undecidability results, all orbiting (but not encom-
passing) MELL/FNL. First, we show that acLLΣ containing the multiplicatives ⊗, →,
the additive ⊕and one classical subexponential (allowing contraction and weakening)
is undecidable. This is a reﬁnement of the unpublished result by Tanaka [51], which
states that FNL plus one fully-powered subexponential is undecidable.
In the second undecidability result, we keep two subexponentials, but with a min-
imalist conﬁguration: the implicational fragment of the logic plus two subexponen-
tials: the “main” one allowing for contraction, exchange, and associativity (weakening
is optional), and an “auxiliary” one allowing only associativity. This is a variation of
Chaudhuri’s result (in the non-associative, non-commutative case), making use of fewer
connectives (tensor is not needed) and less powerful subexponentials.

452
E. Blaisdell et al.
Table 1. Acronyms/decidability of systems mentioned in the paper.
Acronym
System
Decidable?
L
Lambek calculus
✓
LL
(propositional) linear logic
✗
ILL
intuitionistic LL
✗
MALL
multiplicative-additive LL
✓
iMALL
intuitionistic MALL
✓
FL
full (multiplicative-additive) L
✓
cLL
non-commutative iMALL
✓
acLLΣ
non-commutative, non-associative ILL with subexponentials
–
NL
non-associative L
✓
FNL
full (multiplicative-additive) NL
✓
MELL
multiplicative-exponential LL
unknown
SDML
simply dependent multimodal linear logics
–
SMALCΣ FL with subexponentials
–
The rest of the paper is organized as follows: Sect. 2 presents the system acLLΣ,
showing that it has the cut-elimination property and presenting an example in linguis-
tics; Sect. 3 shows the undecidability results; and Sect. 4 concludes the paper.
We have placed, in Table 1, the acronyms for and decidability of all considered
systems. Decidability for the cases marked with “−” depends on the signature Σ.
2
A Nested System for Non-associativity
Similar to modal connectives, the exponential ! in ILL is not canonical [13], in the sense
that if i ̸= j then !iF ̸≡!jF. Intuitively, this means that we can mark the exponential
with labels taken from a set I organized in a pre-order ⪯(i.e., reﬂexive and transitive),
obtaining (possibly inﬁnitely-many) exponentials (!ifor i ∈I). Also as in multi-modal
systems, the pre-order determines the provability relation: for a general formula F, !bF
implies !aF iff a ⪯b.
The algebraic structure of subexponentials, combined with their intrinsic structural
property allow for the proposal of rich linear logic based frameworks. This opened a
venue for proposing different multi-modal substructural logical systems, that encoun-
tered a number of different applications. Originally [42], subexponentials could assume
only weakening and contraction axioms:
C :
!iF →!iF ⊗!iF
W :
!iF →1
This allows the speciﬁcation of systems with multiple contexts, which may be repre-
sented by sets or multisets of formulas [44], as well as the speciﬁcation and veriﬁcation
of concurrent systems [43], and biological systems [46]. In [20,21], non-commutative
systems allowing commutative subexponentials were presented:
E :
(!iF) ⊗G ≡G ⊗(!iF)

Non-associative, Non-commutative Multi-modal Linear Logic
453
and this has many applications, e.g., in linguistics [21].
In this work, we will present a non-commutative, non-associative linear logic based
system, and add the possibility of assuming associativity3
A1 :
!iF ⊗(G ⊗H) ≡(!iF ⊗G) ⊗H
A2 :
(G ⊗H) ⊗!iF ≡G ⊗(H ⊗!iF)
as well as commutativity and other structural properties.
We start by presenting an adaption of simply dependent multimodal linear logics
(SDML) appearing in [31] to the non-associative/commutative case.
The language of non-commutative SDML is that of (propositional intuitionistic)
linear logic with subexponentials [21] supplied with the left residual; or similarly, that
of FL with subexponentials. Non-associative contexts will be organized via binary trees,
here called structures.
Deﬁnition 1 (Structured sequents). Structures are formulas or pairs containing
structures:
Γ, Δ := F | (Γ, Γ)
where the constructors may be empty but never a singleton.
An n-ary context Γ

1
. . .

n
is a context that contains n pairwise distinct num-
bered holes { } wherever a formula may otherwise occur. Given n contexts Γ1, . . . , Γn,
we write Γ{Γ1} · · · {Γn} for the context where the k-th hole in Γ

1
. . .

n
has been
replaced by Γk (for 1 ≤k ≤n). If Γk = ∅the hole is removed.
A structured sequent (or simply sequent) has the form Γ ⇒F where Γ is a structure
and F is a formula.
Example 2. Structures are binary trees, with formulas as leaves and commas as nodes.
The structure !iA, (B, C) represents the tree below left, while (!iA, B), C represents
the tree below right
,
!iA
,
B
C
,
,
!iA
B
C
Deﬁnition 3 (SDML). Let A be a set of axioms. A (non-associative/commutative) sim-
ply dependent multimodal logical system (SDML) is given by a triple Σ = (I, ≼, f),
where I is a set of indices, (I, ≼) is a pre-order, and f is a mapping from I to 2A.
If Σ is a SDML, then the logic described by Σ has the modality !i for every i ∈I,
with the rules of FNL depicted in Fig. 1, together with rules for the axioms f(i) and
the interaction axioms !jA →!iA for every i, j ∈I with i ≼j. Finally, every SDML
is assumed to be upwardly closed w.r.t. ⪯, that is, if i ⪯j then f(i) ⊆f(j) for all
i, j ∈I.
3 Note that the implemented rules in Fig. 2 reﬂect the left to right direction of such axioms only.

454
E. Blaisdell et al.
Figure 2 presents the structured system acLLΣ, for the logic described by the SDML
determined by Σ, with A = {C, W, A1, A2, E} where, in the subexponential rule for
S ∈A, the respective s ∈I is such that S ∈f(s) (e.g. the subexponential symbol e
indicates that E ∈f(e)). We will denote by !AxΔ the fact that the structure Δ contains
only banged formulas as leaves, each of them assuming the axiom Ax.
As an economic notation, we will write ↑i for the upset of the index i, i.e., the set
{j ∈I : i ≼j}. We extend this notation to structures in the following way. Let Γ
be a structure containing only banged formulas as leaves. If such formulas admit the
multiset partition
{!jF ∈Γ : i ≼j} ∪{!kF ∈Γ : i ̸≼k and W ∈f(k)}
then Γ ↑i is the structure obtained from Γ by easing the formulas in the second com-
ponent of the partition (equivalently, the substructure of Γ formed with all and only
formulas of the ﬁrst component of the partition). Otherwise, Γ ↑i is undeﬁned.
Example 4. Let Γ = (!iA, (!jB, !kC)) be represented below left, i ⪯j but i ̸⪯k, and
W ∈f(k). Then Γ ↑i = (!iA, !jB) is depicted below right
,
!iA
,
!jB
!kC
,
!iA
!jB
Observe that, if W /∈f(k), then Γ ↑i cannot be built. In this case, any derivation of
Γ ⇒!i(A ⊗B) cannot start with an application of the promotion rule !iR (similarly to
how promotion in ILL cannot be applied in the presence of non-classical contexts). In
this case, if A, B are atomic, this sequent would not be provable.
Example 5. The use of subexponentials to deal with associativity can be illustrated by
the preﬁxing sequent A →B ⇒(C →A) →(C →B): It is not provable for an
arbitrary formula C, but if C = !aC′, then
!aC′ ⇒!aC′ init
A ⇒A init
B ⇒B init
(A, A →B) ⇒B
→L
((!aC′, (!aC′ →A)), (A →B)) ⇒B
→L
(!aC′, ((!aC′ →A), (A →B))) ⇒B A1
((!aC′ →A), (A →B)) ⇒!aC′ →B →R
A →B ⇒(!aC′ →A) →(!aC′ →B) →R
2.1
Cut-Elimination
When it comes to the proof of cut-elimination for acLLΣ, the cut reductions for the
propositional connectives follow the standard steps for similar systems such as, e.g.,
Moot and Retor´e’s system NL in [38, Chapter 5.2.2]. The case of structural rules, on
the other hand, should be treated with care.

Non-associative, Non-commutative Multi-modal Linear Logic
455
Fig. 1. Structured system FNL for non-associative, full Lambek calculus.
Fig. 2. Structured system acLLΣ for the logic described by Σ.
Theorem 6. If the sequent Γ ⇒F is provable in acLLΣ, then it has a proof with no
instances of the rule mcut.
Proof. The most representative cases of cut reductions involving subexponentials are
detailed next. In order to simplify the notation, when possible, the mcut rule is presented
in its simple form, with an 1-ary context.
Case !a: Suppose that
π1
Δ↑a
1 ⇒F
Δ1 ⇒!aF !aR
π2
Γ{(!aF, Δ2), Δ3)} ⇒G
Γ{(!aF, (Δ2, Δ3))} ⇒G A1
Γ{(Δ1, (Δ2, Δ3))} ⇒G
mcut
Since axioms are upwardly closed w.r.t. ⪯, it must be the case that Δ↑a
1 con-
tains only formulas marked with subexponentials allowing associativity. All

456
E. Blaisdell et al.
the other formulas in Δ1 can be weakened; this is guaranteed by the applica-
tion of the rule !aR in π1. Hence the derivation above reduces to
π1
Δ↑a
1 ⇒F
Δ↑a
1 ⇒!aF !aR
π2
Γ{(!aF, Δ2), Δ3)} ⇒G
Γ

((Δ↑a
1 , Δ2), Δ3)

⇒G
mcut
Γ

(Δ↑a
1 , (Δ2, Δ3))

⇒G
A1
Γ{(Δ1, (Δ2, Δ3))} ⇒G
W
Case !c: Suppose that
π1
Δ↑c ⇒F
Δ ⇒!cF !cR
π2
Γ{!cF} . . . {!cF} . . . {!cF} ⇒G
Γ{ } . . . {!cF} . . . { } ⇒G
C
Γ{ } . . . {Δ} . . . { } ⇒G
mcut
Since Δ↑c contains only formulas marked with subexponentials allowing con-
traction, the derivation above reduces to
π1
Δ↑c ⇒F
Δ↑c ⇒!cF !cR
π2
Γ{!cF} . . . {!cF} . . . {!cF} ⇒G
Γ

Δ↑c
. . . {Δ↑c} . . . {Δ↑c} ⇒G
mcut
Γ{ } . . . {Δ↑c} . . . { } ⇒G
C
Γ{ } . . . {Δ} . . . { } ⇒G
W
Observe that here, as usual, the multicut rule is needed in order to reduce the
cut complexity.
Case !iR: Suppose that
π1
Δ↑i ⇒F
Δ ⇒!iF !iR
π2

Γ

!iF
↑j ⇒G
Γ

!iF

⇒!jG
!jR
Γ{Δ} ⇒!jG
mcut
If j ̸⪯i, then it should be the case that W ∈f(i) and

Γ

!iF
↑j =
Γ{ }↑j, since !iF will be weakened in the application of rule !jR. Hence, all
formulas in Δ can be weakened as well and the reduction is
π2
Γ{ }↑j ⇒G
Γ{ } ⇒!jG !jR
Γ{Δ} ⇒!jG W

Non-associative, Non-commutative Multi-modal Linear Logic
457
On the other hand, if j ⪯i, by transitivity all the formulas in Δ↑i also have
this property (implying that Δ↑i is a substructure of Δ↑j), and the rest of
formulas of Δ can be weakened. Hence the derivation above reduces to
π1
Δ↑i ⇒F
Δ↑j ⇒!iF !iR
π2

Γ

!iF
↑j ⇒G
(Γ{Δ})↑j ⇒G
Γ{Δ} ⇒!jG
!jR
The other cases for subexponentials are similar or simpler.
⊓⊔
The next examples illustrate what we mean by acLLΣ being a “conservative exten-
sion” of subsystems and variants. Indeed, although we remove structural properties of
the core LL, subexponentials allow them to be added back, either locally or globally.
Example 7 (Structural variants of iMALL). Adding combinations of contraction C and
/ or weakening W for arbitrary formulas to additive-multiplicative intuitionistic linear
logic (iMALL) yields, respectively, propositional intuitionistic logic ILP = iMALL +
{C, W}, and the intuitionistic versions of afﬁne linear logic aLL = iMALL + W and
relevant logic R = iMALL + C. For the sake of presentation we overload the notation
and use the connectives of linear logic also for these logics. In order to embed the
logics above into acLLΣ, let α ∈{ILP, aLL, R} and consider modalities !α with f(α) =
{E, A1, A2} ∪A where A ⊆{C, W} is the set of axioms whose corresponding rules
are in α. The translation τα preﬁxes every subformula with the modality !α. For L ∈
{ILP, aLL, R} it is then straightforward to show that a structured sequent S is cut-free
derivable in L iff its translation τα(S) is cut-free derivable in the logic described by
({α}, ≼, f) with ≼the obvious relation, and f as given above.
Example 8 (Structural variants of FNL). Following the same script as above and start-
ing from FNL:
– considering f(α) = A ⊆{E, A1, A2};
• If A = {A1, A2}, then we obtain the system FL;
• If A = {E, A1, A2} then the resulting system corresponds to iMALL.
• Adding C, W as options to A will result the afﬁne/relevant versions of the sys-
tems above.
– in a pre-order (I, ⪯), if f(i) = {A1, A2}∪Ai where Ai ⊆{E, C, W} for each i ∈I,
then the resulting system corresponds to SMALCΣ in [21] (that is, the extension of
FL with subexponentials).
2.2
An Example in Linguistics
Since its inception, Lambek calculus [29] has been applied to the modeling of natu-
ral language syntax by means of categorial grammars. In a categorial grammar, each
word is assigned one or several Lambek formulas, which serve as syntactic categories.
For a simple example, John and Mary are assigned np (“noun phrase”) and loves gets

458
E. Blaisdell et al.
(np →s) ←np. Here s stands for “sentence”, and loves is a transitive verb, which
lacks noun phrases on both sides to become a sentence. Grammatical validity of “John
loves Mary” is supported by derivability of the sequent np, (np →s) ←np, np ⇒s.
Notice that this derivability keeps valid also in the non-associative setting, if the correct
nested structure is provided: (np, ((np →s) ←np, np)) ⇒s.
The original Lambek calculus L is associative. In some cases, however, associativity
leads to over-generation, i.e., validation of grammatically incorrect sentences. Lambek
himself realized this and proposed the non-associative calculus NL in [30]. We will
illustrate this issue with the example given in [38, Sect. 4.2.2]. The syntactic category
assignment is as follows (where n stands for “noun”):
Words Types
the np ←n
Hulk n
is (np →s) ←(n ←n)
green, incredible n ←n
With this assignment, sentences “The Hulk is green” and “The Hulk is incredible”
are correctly marked as valid, by deriving the sequent
(np ←n, n), ((np →s) ←(n ←n), n ←n) ⇒s
However, in the associative setting the sequent for the phrase “The Hulk is green
incredible,” which is grammatically incorrect, also becomes derivable:
np ←n, n, (np →s) ←(n ←n), n ←n, n ←n ⇒s,
essentially due to derivability of n ←n, n ←n ⇒n ←n.
In other situations, however, associativity is useful. Standard examples include han-
dling of dependent clauses, e.g., “the girl whom John loves,” which is validated as a
noun phrase by the following derivable sequent:
np ←n, n, (n →n) ←(s ←np), np, (np →s) ←np ⇒np
Here (n →n) ←(s ←np) is the syntactic category for who.
Our subexponential extension of NL, however, handles this case using local asso-
ciativity instead of the global one. Namely, the category for whom now becomes
(n →n) ←(s ←!anp), where !a is a subexponential which allows the A2 rule,
and the following sequent is happily derivable:
np ←n, (n, ((n →n) ←(s ←!anp), (np, (np →s) ←np))) ⇒np
The necessity of this more ﬁne-grained control of associativity, instead of a global
associativity rule, is seen via a combination of these examples. Namely, we talk about
sentences like “The superhero whom Hawkeye killed was incredible” and “... was
green”. With !a, each of them is handled in the same way as the previous examples:
(np ←n, (n, ((n →n) ←(s ←!anp), (np, (np →s) ←np)))),
((np →s) ←(n ←n), n ←n) ⇒s.

Non-associative, Non-commutative Multi-modal Linear Logic
459
On one hand, without !a this sequent cannot be derived in the non-associative sys-
tem. On the other hand, if we make the system globally associative, it would validate
incorrect sentences like “The superhero whom Hawkeye killed was green incredible.”
3
Some Undecidability Results
Non-associativity makes a signiﬁcant difference in decidability and complexity matters.
For example, while L is NP-complete [47], NL is decidable in polynomial time [1,14].
For our system acLLΣ, its decidability or undecidability depends on its signature
Σ. In fact, we have a family of different systems acLLΣ, with Σ as a parameter. Recall
that the subexponential signature Σ controls not just the number of subexponentials
and the preorder among them. More importantly, it dictates, for each subexponential,
which structural rules this subexponential licenses. If for every i ∈I we have C /∈f(s),
that is, no subexponential allows contraction, then acLLΣ is clearly decidable, since the
cut-free proof search space is ﬁnite. Therefore, for undecidability it is necessary to have
at least one subexponential which allows contraction.
For a non-associative system with only one fully-powered exponential modality
s (that is, f(s) = {E, C, W, A1, A2}), undecidability was proven in a preprint by
Tanaka [51], based on Chvalovsk´y’s [11] result on undecidability of the ﬁnitary con-
sequence relation in FNL.
In this section, we prove two undecidability results. The ﬁrst one is a reﬁnement
of Tanaka’s result: We establish undecidability with at least one subexponential which
allows contraction and weakening (commutativity/associativity are optional), in a sub-
system containing only the additive connective ⊕and the multiplicatives ⊗and →.
The second undecidability result is for the minimalistic, purely multiplicative frag-
ment, which includes only →(not even ⊗). As a trade-off, however, it requires two
subexponentials: the “main” one, which allows contraction, exchange, and associativity
(weakening is optional), and an “auxiliary” one, which allows only associativity.
It should be noted that this undecidability result is orthogonal to Tanaka’s [51],
and the proof technique is essentially different. Indeed, Chvalovsk´y’s undecidability
theorem does not hold for the non-associative Lambek calculus without additives, where
the consequence relation is decidable [7].
Finally, we observe that if the intersection of these systems is decidable (which
is still an open question), then our two undecidability results are incomparable: we
have two undecidable fragments of acLLΣ, but their common part, which includes only
divisions and one exponential, would be decidable.
3.1
Undecidability with Additives and One Subexponential
We are going to derive the next theorem from undecidability of the ﬁnitary consequence
relation in FNL [11]. Recall that FNL is, in fact, the fragment of acLLΣ without subex-
ponentials (that is, with an empty I).
Theorem 9. If there exists such s ∈I that f(s) ⊇{C, W}, then the derivability prob-
lem in acLLΣ is undecidable. Moreover, this holds for the fragment with only ⊗, →,
⊕, !s.

460
E. Blaisdell et al.
In fact, using C and W, one can also derive A1, A2, E1, and E2. Therefore, if
f(s) ⊇{C, W}, then !s is actually a full-power exponential modality. (In the proof
of Theorem 9 below, we use only W and C rules, in order to avoid confusion.) How-
ever, Theorem 9 does not directly follow from undecidability of propositional linear
logic [32], because here the basic system is non-associative and non-commutative,
while linear logic is both associative and commutative. Thus, we need a different encod-
ing for undecidability.
Let Φ be a ﬁnite set of FNL sequents. By FNL(Φ) let us denote FNL extended by
adding sequents from Φ as additional (non-logical) axioms. In general, FNL(Φ) does
not enjoy cut-elimination, so mcut is kept as a rule of inference in FNL(Φ). A sequent
Γ ⇒F is called a consequence of Φ if this sequent is derivable in FNL(Φ).
Theorem 10 (Chvalovsk´y [11]). The consequence relation in FNL is undecidable, that
is, there exists no algorithm which, given Φ and Γ ⇒F, determines whether Γ ⇒F
is a consequence of Φ. Moreover, undecidability keeps valid when Φ and Γ ⇒F are
built from variables using only ⊗and ⊕.
Now, in order to prove Theorem 9, we internalize Φ into the sequent using !s,
assuming f(s) ⊇{C, W}.
First we notice that we may suppose, without loss of generality, that all sequents in
Φ are of the form ⇒A, that is, have empty antecedents. Namely, each sequent of the
form Π ⇒B can be replaced by ⇒( Π) →B, where  Π is obtained from Π by
replacing each comma with ⊗. Indeed, these sequents are derivable from one another:
from Π ⇒B to ⇒( Π) →B we apply a sequence of ⊗L followed by →R, and
for the other direction we apply a series of cuts, ﬁrst with ( Π, ( Π) →B) ⇒B,
and then with (F, G) ⇒F ⊗G several times, for the corresponding subformulas of
 Π. The following embedding lemma (“modalized deduction theorem”) holds.
Lemma 11. The sequent Γ ⇒F is a consequence of Φ = { ⇒A1, . . . , ⇒An} if
and only if the sequent

(. . . ((!sA1, !sA2), !sA3), . . . , !sAn), Γ

⇒F is derivable in
acLLΣ.
Proof. Let us denote (. . . ((!sA1, !sA2), !sA3), . . . , !sAn) by !Φ. Notice that C and W
can be applied to !Φ as a whole; this is easily proven by induction on n.
For the “only if” direction let us take the derivation of Γ ⇒F in FNL(Φ) (with
cuts) and replace each sequent of the form Δ ⇒G in it with (!Φ, Δ) ⇒G, and each
sequent of the form ⇒G with !Φ ⇒G. The translations of non-logical axioms from Φ
are derived as follows:
Ai ⇒Ai init
!sAi ⇒Ai der
!Φ ⇒Ai
W, n −1 times
Translations of axioms init and 1R are derived from the corresponding original
axioms by W, n times; ⊤R remains valid.

Non-associative, Non-commutative Multi-modal Linear Logic
461
Rules ⊗L, ⊕L, ⊕Ri, &Li, &R, and 1L remain valid. For →L, ←L, and mcut we
contract !Φ as a whole:
(!Φ, Δ) ⇒F
(!Φ, Γ{G}) ⇒H
(!Φ, Γ{((!Φ, Δ), F →G)}) ⇒H →L
(!Φ, Γ{(Δ, F →G)}) ⇒H
C
(!Φ, Δ) ⇒F
(!Φ, Γ{F} . . . {F}) ⇒C
(!Φ, Γ{(!Φ, Δ)} . . . {(!Φ, Δ)}) ⇒C
mcut
(!Φ, Γ{Δ} . . . {Δ}) ⇒C
C
For ⊗R, →R, and ←R, we combine contraction and weakening:
(!Φ, Γ1) ⇒F
(!Φ, Γ2) ⇒G
((!Φ, Γ1), (!Φ, Γ2)) ⇒F ⊗G ⊗R
(!Φ, ((!Φ, Γ1), (!Φ, Γ2))) ⇒F ⊗G W
(!Φ, (Γ1, Γ2)) ⇒F ⊗G
C
(!Φ, (F, Γ)) ⇒G
(!Φ, (F, (!Φ, Γ)) ⇒G W
(F, (!Φ, Γ)) ⇒G
C
(!Φ, Γ) ⇒F →G →R
Notice that our original derivation was in FNL(Φ), so it does not include rules
operating subexponentials.
For the “if” direction we take a cut-free proof of (!Φ, Γ) ⇒F in acLLΣ and erase
all formulas which include the subexponential. In the resulting derivation tree all rules
and axioms, except those which operate !s, remain valid. Structural rules for !s trivialize
(since the !-formula was erased). The !sR rule could not have been used, since we do
not have positive occurrences of !sF, and our proof is cut-free.
Finally, der translates into
Γ{Ai} ⇒G
Γ{} ⇒G
This is modeled by cut with one of the sequents from Φ:
⇒Ai
Γ{Ai} ⇒G
Γ{} ⇒G
mcut
Thus, we get a correct derivation in FNL(Φ).
⊓⊔
Theorem 10 and Lemma 11 immediately yield Theorem 9.
3.2
Undecidability Without Additives and with Two Subexponentials
Theorem 12. If there are a, c
∈
I such that f(a)
=
{A1, A2} and f(c)
⊇
{C, E, A1, A2}, then the derivability problem in acLLΣ is undecidable. Moreover, this
holds for the fragment with only →, !a, and !c.
Remember from Example 8 that SMALCΣ [21] denotes the extension of FL with subex-
ponentials. The undecidability theorem above is proved by encoding the one-division
fragment of SMALCΣ containing one exponential c such that f(c) ⊇{C, E}. It turns
out that that such a system is undecidable.
Theorem 13 (Kanovich et al. [22,23]). If there exists such c ∈I that f(c) ⊇{C, E},
then the derivability problem in SMALCΣ is undecidable. Moreover, this holds for the
fragment with only →and !c.

462
E. Blaisdell et al.
Observe that SMALCΣ can be obtained from acLLΣ by adding “global” associativ-
ity rules:
Γ{((Δ1, Δ2), Δ3)} ⇒G
Γ{(Δ1, (Δ2, Δ3))} ⇒G
Γ{(Δ1, (Δ2, Δ3))} ⇒G
Γ{((Δ1, Δ2), Δ3)} ⇒G
The usual formulation of SMALCΣ, of course, uses sequences of formulas instead
of nested structures as antecedents. The alternative formulation, however, would be
more convenient for us now. It will be also convenient for us to regard all subexponen-
tials in SMALCΣ to be associative, that is, f(s) ⊇{A1, A2} for each s ∈I.
In order to embed SMALCΣ into acLLΣ, we deﬁne two translations, A!−and A!+,
by mutual recursion:
z!−= !az
z!+ = z
where z is a variable,1, or ⊤
(A →B)!−= !a(A!+ →B!−)
(A →B)!+ = A!−→B!+
(B ←A)!−= !a(B!−←A!+)
(B ←A)!+ = B!+ ←A!−
(A ⊛B)!−= !a(A!−⊛B!−)
(A ⊛B)!+ = A!+ ⊛B!+
where ⊛∈{⊗, ⊕, &}
(!sA)!−= !s(A!−)
(!sA)!+ = !s(A!+)
Informally, our translation adds a !a over any formula (not only over atoms) of
negative polarity, unless this formula was already marked with a !s. Thus, all formulae
in antecedents would begin with either the new subexponential !a or one of the old
subexponentials !s, and all these subexponentials allow associativity rules A1 and A2.
Lemma 14. A sequent A1, . . . , An ⇒B is derivable in SMALCΣ if and only if its
translation (. . . (A!−
1 , A!−
2 ), . . . , A!−
n ) ⇒B!+ is derivable in acLLΣ.
Proof. For the “only if” part, let us ﬁrst note that each formula A!−
i
is of the form !sF
and A1, A2 ∈f(s). Indeed, either s is an “old” subexponential label (for which we
added A1, A2) or s = a. Thus brackets can be freely rearranged in the antecedent.
Now we take a cut-free proof of A1, . . . , An ⇒B in SMALCΣ and replace each
sequent in it with its translation. Right rules for connectives other than subexponentials,
i.e., ⊗R, ⊕Ri, &R, →R, and ←R, remain valid as they are, up to rearranging brackets
in antecedents. For !iR, we notice that the translation of a formula of the form !jF,
where j ⪯i, is also a formula of the form !jF ′. Thus, this rule also remains valid.
The same holds for the dereliction rule der, because (!iF)!−is exactly !i(F !−). Finally,
the “old” structural rules (exchange, contraction, weakening) also remain valid (up to
rearranging of brackets), since !iF gets translated into !i(F !−), which enjoys the same
structural rules.
For the other left rules, we need to derelict !a ﬁrst, and then perform the corre-
sponding rule application. Rearrangement of brackets, if needed, is performed below
dereliction or above the application of the rule in question.
The “if” part is easier. Given a derivation of (. . . (A!−
1 , A!−
2 ), . . . , A!−
n ) ⇒B!+ in
acLLΣ, we erase !a everywhere, and consider it as a derivation in SMALCΣ. Associa-
tivity rules for the erased !a (which are the only structural rules for this subexponential)
keep valid, because now associativity is global. Dereliction and right introduction for
!a trivialize. All other rules, which do not operate !a, remain as they are. Thus, we get
a derivation of A1, . . . , An ⇒B in SMALCΣ, since erasing !a makes our translations
just identical.
⊓⊔

Non-associative, Non-commutative Multi-modal Linear Logic
463
4
Related Work and Conclusion
In this paper, we have presented acLLΣ, a sequent-based system for non-associative,
non-commutative linear logic with subexponentials. Starting form FNL, we modu-
larly and uniformly added rules for exchange, associativity, weakening and contraction,
which can be applied with the subexponentials having with the respective features. This
allows for the application of structural rules locally, and it conservatively extends well
known systems in the literature, continuing the path of controlling structural properties
started by Girard himself [16].
Another approach to combining associative and non-associative behavior in
Lambek-style grammars is the framework of the Lambek calculus with brackets by
Morrill [39,40] and Moortgat [34]. The bracket approach is dual to ours: there the
base system is associative, and brackets, which are controlled by bracket modalities,
introduce local non-associativity. Both the associative Lambek calculus and the non-
associative Lambek calculus can be embedded into the Lambek calculus with brackets:
the former is just by design of the system and the latter was shown by Kurtonina [26]
by constructing a translation.
From the point of view of generative power, however, the (associative) Lambek
calculus with brackets is weaker than the non-associative system with subexponentials,
which is presented in this paper. Namely, as shown by Kanazawa [19], grammars based
on the Lambek calculus with brackets can generate only context-free languages. In
contrast, grammars based on our system with subexponentials go beyond context-free
languages, even when no subexponential allows contraction (subexponentials allowing
contraction may lead to undecidability, as shown in the last section).
As a quick example, let us consider a subexponential !ae which allows both asso-
ciativity (A1 and A2) and exchange (E). If we put this subexponential over any
(sub)formula, the system becomes associative and commutative. Using this system, one
can describe the non context-free language MIX3, which contains all non-empty words
over {a, b, c}, in which the numbers of a, b, and c are equal. Indeed, MIX3 is the per-
mutation closure of the language {(abc)n | n ≥1}. The latter is regular, therefore
context-free, and therefore deﬁnable by a Lambek grammar. The ability of our system
to go beyond context-free languages is important from the point of view of applications,
since there are known linguistic phenomena which are essentially non-context-free [49].
Regarding decidability, let us compare our results with the more well-known asso-
ciative non-commutative and associative commutative cases.
In the associative and commutative case the situation is as follows. In the pres-
ence of additives, the system is known to be undecidable with one exponential modal-
ity [32]. Without additives, we get MELL, the (un)decidability of which is a well-known
open problem [50]. However, with two subexponentials MELL again becomes undecid-
able [9]. Thus, we have the same trade-off as in our non-associative non-commutative
case: for undecidability one needs either additives, or two subexponentials.
Our results help to shed some light in the (un)decidability problem for the spectrum
of logical systems surrounding MELL/FNL, allowing for a ﬁne-grained analysis of the
problem, specially the trade-offs on connectives and subexponentials for guaranteeing
(un)decidability.

464
E. Blaisdell et al.
There is a lot to be done from now on. First of all, we would like to analyze better
the minimalist fragment of acLLΣ containing only implication and one fully-powered
subexponential, as it seems to be crucial for understanding the lower bound of unde-
cidability (or the upper bound of decidability). Second, one should deﬁnitely explore
more the use of acLLΣ in modeling natural language syntax. The examples in Sect. 2.2
show how to locally combine sentences with different grammatical characteristics, and
the MIX3 example above illustrates how that can be of importance. That is, it would
be interesting to have a formal study about acLLΣ and categorial grammars. Third, we
plan to investigate the connections between our work and Adjoint logic [48] as well as
with Display calculus [5,12]. Finally, we intend to study proof-theoretic properties of
acLLΣ, such as normalization of proofs (e.g. via focusing) and interpolation.
Acknowledgements. We are grateful for the useful suggestions from the anonymous referees.
We would like to thank L. Beklemishev, M. Moortgat, and C. Retor´e for their inspiring and
helpful comments regarding an approach based on non-associativity.
References
1. Aarts, E., Trautwein, K.: Non-associative Lambek categorial grammar in polynomial time.
Math. Log. Q. 41(4), 476–484 (1995)
2. Abrusci, V.M.: A comparison between Lambek syntactic calculus and intuitionistic linear
logic. Zeitschr. Math. Logik Grundl. Math. (Math. Logic Q.) 36, 11–15 (1990)
3. Ajdukiewicz, K.: Die syntaktische Konnexit¨at. Studia Philosophica 1, 1–27 (1935)
4. Bar-Hillel, Y.: A quasi-arithmetical notation for syntactic description. Language 29, 47–58
(1953)
5. Belnap, N.: Display logic. J. Philos. Log. 11(4), 375–417 (1982). https://doi.org/10.1007/
BF00284976
6. Bulinska, M.: On the complexity of nonassociative Lambek calculus with unit. Stud. Logica
93(1), 1–14 (2009). https://doi.org/10.1007/s11225-009-9205-2
7. Buszkowski, W.: Lambek calculus with nonlogical axioms. In: Language and Grammar,
Studies in Mathematical Linguistics and Natural Language, pp. 77–93. CSLI Publications
(2005)
8. Buszkowski, W., Farulewski, M.: Nonassociative Lambek calculus with additives and
context-free languages. In: Grumberg, O., Kaminski, M., Katz, S., Wintner, S. (eds.) Lan-
guages: From Formal to Natural. LNCS, vol. 5533, pp. 45–58. Springer, Heidelberg (2009).
https://doi.org/10.1007/978-3-642-01748-3 4
9. Chaudhuri, K.: Undecidability of multiplicative subexponential logic. In: Alves, S.,
Cervesato, I. (eds.) Proceedings Third International Workshop on Linearity, LINEARITY
2014, Vienna, Austria, 13th July 2014. EPTCS, vol. 176, pp. 1–8 (2014). https://doi.org/10.
4204/EPTCS.176.1
10. Chaudhuri, K., Marin, S., Straßburger, L.: Modular focused proof systems for intuitionistic
modal logics. In: FSCD, pp. 16:1–16:18 (2016)
11. Chvalovsk´y, K.: Undecidability of consequence relation in full non-associative Lambek cal-
culus. J. Symb. Logic 80(2), 567–586 (2015)
12. Clouston, R., Dawson, J., Gor´e, R., Tiu, A.: Annotation-free sequent calculi for full intuition-
istic linear logic. In: Rocca, S.R.D. (ed.) Computer Science Logic 2013 (CSL 2013), CSL
2013, Torino, Italy, 2–5 September 2013. LIPIcs, vol. 23, pp. 197–214. Schloss Dagstuhl -
Leibniz-Zentrum f¨ur Informatik (2013). https://doi.org/10.4230/LIPIcs.CSL.2013.197

Non-associative, Non-commutative Multi-modal Linear Logic
465
13. Danos, V., Joinet, J.-B., Schellinx, H.: The structure of exponentials: uncovering the dynam-
ics of linear logic proofs. In: Gottlob, G., Leitsch, A., Mundici, D. (eds.) KGC 1993. LNCS,
vol. 713, pp. 159–171. Springer, Heidelberg (1993). https://doi.org/10.1007/BFb0022564
14. de Groote, P., Lamarche, F.: Classical non-associative Lambek calculus. Stud. Logica 71(3),
355–388 (2002). https://doi.org/10.1023/A:1020520915016
15. Gheorghiu, A., Marin, S.: Focused proof-search in the logic of bunched implications. In:
FOSSACS 2021. LNCS, vol. 12650, pp. 247–267. Springer, Cham (2021). https://doi.org/
10.1007/978-3-030-71995-1 13
16. Girard, J.-Y.: Linear logic. Theor. Comput. Sci. 50, 1–102 (1987). https://doi.org/10.1016/
0304-3975(87)90045-4
17. Guglielmi, A., Straßburger, L.: Non-commutativity and MELL in the calculus of structures.
In: Fribourg, L. (ed.) CSL 2001. LNCS, vol. 2142, pp. 54–68. Springer, Heidelberg (2001).
https://doi.org/10.1007/3-540-44802-0 5
18. Hepple, M.: A general framework for hybrid substructural categorial logics. Technical report
94–14, IRCS (1994)
19. Kanazawa, M.: On the recognizing power of the Lambek calculus with brackets. J. Logic
Lang. Inform. 27(4), 295–312 (2018)
20. Kanovich, M., Kuznetsov, S., Nigam, V., Scedrov, A.: A logical framework with commu-
tative and non-commutative subexponentials. In: Galmiche, D., Schulz, S., Sebastiani, R.
(eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 228–245. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-94205-6 16
21. Kanovich, M., Kuznetsov, S., Nigam, V., Scedrov, A.: Subexponentials in non-commutative
linear logic. Math. Struct. Comput. Sci. 29(8), 1217–1249 (2019). https://doi.org/10.1017/
S0960129518000117
22. Kanovich, M., Kuznetsov, S., Scedrov, A.: Undecidability of the Lambek calculus with a
relevant modality. In: Foret, A., Morrill, G., Muskens, R., Osswald, R., Pogodalla, S. (eds.)
FG 2015-2016. LNCS, vol. 9804, pp. 240–256. Springer, Heidelberg (2016). https://doi.org/
10.1007/978-3-662-53042-9 14
23. Kanovich, M., Kuznetsov, S., Scedrov, A.: The multiplicative-additive Lambek calculus with
subexponential and bracket modalities. J. Log. Lang. Inform. 30, 31–88 (2021)
24. Kopylov, A.: Decidability of linear afﬁne logic. Inf. Comput. 164(1), 173–198 (2001). https://
doi.org/10.1006/inco.1999.2834
25. Kozak, M.: Cyclic involutive distributive full Lambek calculus is decidable. J. Log. Comput.
21(2), 231–252 (2011). https://doi.org/10.1093/logcom/exq021
26. Kurtonina, N.: Frames and labels. A modal analysis of categorial inference. Ph.D. thesis,
Universiteit Utrecht, ILLC (1995)
27. Kurtonina, N., Moortgat, M.: Structural control. In: Blackburn, P., de Rijke, M. (eds.) Spec-
ifying Syntactic Structures, CSLI, Stanford, pp. 75–113 (1997)
28. Lamarche, F.: On the Algebra of Structural Contexts. Mathematical Structures in Computer
Science, p. 51 (2003). Article dans revue scientiﬁque avec comit´e de lecture. https://hal.inria.
fr/inria-00099461
29. Lambek, J.: The mathematics of sentence structure. Am. Math. Monthly 65(3), 154–170
(1958)
30. Lambek, J.: On the calculus of syntactic types. In: Jakobson, R. (ed.) Structure of Language
and Its Mathematical Aspects, pp. 166–178. American Mathematical Society (1961)
31. Lellmann, B., Olarte, C., Pimentel, E.: A uniform framework for substructural logics with
modalities. In: LPAR-21, pp. 435–455 (2017)
32. Lincoln, P., Mitchell, J., Scedrov, A., Shankar, N.: Decision problems for propositional linear
logic. Ann. Pure Appl. Logic 56(1–3), 239–311 (1992)
33. Moortgat, M.: Multimodal linguistic inference. Log. J. IGPL 3(2–3), 371–401 (1995). https://
doi.org/10.1093/jigpal/3.2-3.371

466
E. Blaisdell et al.
34. Moortgat, M.: Multimodal linguistic inference. J. Logic Lang. Inform. 5(3–4), 349–385
(1996)
35. Moortgat, M., Morrill, G.: Heads and phrases: type calculus for dependency and constituent
structure. Technical report (1991)
36. Moortgat, M., Oehrle, R.: Logical parameters and linguistic variation. In: Fifth European
Summer School in Logic, Language and Information. Lecture Notes on Categorial Grammar
(1993)
37. Moot, R.: The grail theorem prover: type theory for syntax and semantics. CoRR,
abs/1602.00812 (2016). arXiv:1602.00812
38. Moot, R., Retor´e, C.: The Logic of Categorial Grammars. LNCS, vol. 6850. Springer, Hei-
delberg (2012). https://doi.org/10.1007/978-3-642-31555-8
39. Morrill, G.: Categorial formalisation of relativisation: Pied piping, islands, and extraction
sites. Technical report LSI-92-23-R, Universitat Polit`ecnica de Catalunya (1992)
40. Morrill, G.: Parsing/theorem-proving for logical grammar CatLog3. J. Log. Lang. Inf. 28(2),
183–216 (2019). https://doi.org/10.1007/s10849-018-09277-w
41. Morrill, G., Leslie, N., Hepple, M., Barry, G.: Categorial deductions and structural opera-
tions. In: Studies in Categorial Grammar, Edinburgh Working Paper in Cognitive Science,
vol. 5, pp. 1–21 (1990)
42. Nigam, V., Miller, D.: A framework for proof systems. J. Autom. Reason. 45(2), 157–188
(2010). https://doi.org/10.1007/s10817-010-9182-1
43. Nigam, V., Olarte, C., Pimentel, E.: On subexponentials, focusing and modalities in concur-
rent systems. Theor. Comput. Sci. 693, 35–58 (2017). https://doi.org/10.1016/j.tcs.2017.06.
009
44. Nigam, V., Pimentel, E., Reis, G.: An extended framework for specifying and reason-
ing about proof systems. J. Log. Comput. 26(2), 539–576 (2016). https://doi.org/10.1093/
logcom/exu029
45. Oehrle, R., Zhang, S.: Lambek calculus and preposing of embedded subjects. Coyote Papers
(1989). http://hdl.handle.net/10150/226572
46. Olarte, C., Chiarugi, D., Falaschi, M., Hermith, D.: A proof theoretic view of spatial and
temporal dependencies in biochemical systems. Theor. Comput. Sci. 641, 25–42 (2016).
https://doi.org/10.1016/j.tcs.2016.03.029
47. Pentus, M.: Lambek calculus is NP-complete. Theor. Comput. Sci. 357, 186–201 (2006)
48. Pruiksma, K., Chargin, W., Pfenning, F., Reed, J.: Adjoint logic (2018, Unpublished
manuscript)
49. Shieber, S.: Evidence against the context-freeness of natural languages. Linguist. Philos. 8,
333–343 (1985)
50. Straßburger, L.: On the decision problem for MELL. Theor. Comput. Sci. 768, 91–98 (2019).
https://doi.org/10.1016/j.tcs.2019.02.022
51. Tanaka, H.: A note on undecidability of propositional non-associative linear logics (2019).
arXiv preprint arXiv:1909.13444

Non-associative, Non-commutative Multi-modal Linear Logic
467
Open Access This chapter is licensed under the terms of the Creative Commons Attribution
4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use,
sharing, adaptation, distribution and reproduction in any medium or format, as long as you
give appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter’s Creative
Commons license, unless indicated otherwise in a credit line to the material. If material is not
included in the chapter’s Creative Commons license and your intended use is not permitted by
statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder.

Eﬀective Semantics for the Modal Logics
K and KT via Non-deterministic Matrices
Ori Lahav1 and Yoni Zohar2(B)
1 Tel Aviv University, Tel Aviv-Yafo, Israel
2 Bar Ilan University, Ramat Gan, Israel
yoni.zohar@biu.ac.il
Abstract. A four-valued semantics for the modal logic K is introduced.
Possible worlds are replaced by a hierarchy of four-valued valuations,
where the valuations of the ﬁrst level correspond to valuations that
are legal w.r.t. a basic non-deterministic matrix, and each level further
restricts its set of valuations. The semantics is proven to be eﬀective, and
to precisely capture derivations in a sequent calculus for K of a certain
form. Similar results are then obtained for the modal logic KT, by simply
deleting one of the truth values.
1
Introduction
Propositional modal logics extend classical logic with modalities, intuitively
interpreted as necessity, knowledge, or temporal operators. Such extensions have
several applications in computer science and artiﬁcial intelligence (see, e.g.,
[7,9,13]).
The most common and successful semantic framework for modal logics is the
so called possible worlds semantics, in which each world is equipped with a two-
valued valuation, and the semantic constraints regarding the modal operators
consider the valuations in accessible worlds. While this has been the gold stan-
dard for modal logic semantics for many years, alternative semantic frameworks
have been proposed. One of these approaches, initiated by Kearns [10], is based
on an inﬁnite sequence of sets of valuations in a non-deterministic many-valued
semantics. Since then, several non-deterministic many-valued semantics, with-
out possible worlds, were developed for modal logics (see, e.g., [4,8,12,14]). The
current paper is a part of that body of work. Having an alternative semantic
framework for modal logics, diﬀerent than the common possible worlds seman-
tics, has the potential of exposing new intuitions and understandings of modal
logics, and also to form the basis to new decision procedures.
Our main contribution is a four-valued semantics for the modal logic K. The
key characteristic of the semantics that we present is eﬀectiveness: when checking
We thank the anonymous reviewers for their useful feedback. This research was sup-
ported by NSF-BSF (grant number 2020704), ISF (grant numbers 619/21 and 1566/18),
and the Alon Young Faculty Fellowship.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 468–485, 2022.
https://doi.org/10.1007/978-3-031-10769-6_28

Eﬀective Semantics for the Modal Logics K and KT
469
for the entailment of a formula ϕ from a set Γ of formulas in K, it suﬃces to
only consider partial models, deﬁned over the subformulas of Γ and ϕ. To the
best of our knowledge, this is the ﬁrst eﬀective Nmatrices-based semantics for
K. Such a semantics has the potential of being subject to reductions to classical
satisﬁability [3], as it is based on ﬁnite-valued truth tables, and thus improving
the performance of solvers for modal logic by utilizing oﬀ-the-shelf SAT solvers.
Another advantage of this semantics is that it precisely captures derivations in a
sequent calculus for K that admit a certain property. Following Kearns, models
of this semantics are based on the concept of levels—valuations of level 0 are
the ordinary valuations of Nmatrices, while each level m > 0 introduces more
constraints. We show that valuations of level m correspond to derivations in
the calculus whose largest number of applications of the rule that correspond to
the axiom (K) in any branch of the derivation is at most m. Our restrictions
between the levels are more complex than the original restrictions in Kearns’
work, in order to obtain eﬀectiveness. Another precise correspondence between
the semantics and the proof system that we prove, is between the domains of
valuations and the formulas allowed to be used in derivations.
Finally, we observe that by deleting one of the truth values, a three-valued
semantics for the modal logic KT is obtained, which is similar to the one pre-
sented in [8]. Like the case of K, the resulting semantics is eﬀective, and tightly
correspond to derivations in a sequent calculus for KT.
Outline. The paper is organized as follows: Sect. 2 reviews standard notions in
non-deterministic matrices. In Sect. 3, we present our semantics for the modal
logic K, as well as the sequent calculus our investigation will be based on, which
is coupled with the notion of (K)-depth of derivations. In Sect. 4, we prove
soundness and completeness theorems between the sequent calculus and the
semantics. In Sect. 5, we prove that the semantics that we provide is eﬀective,
not only for deciding entailment, but also for producing countermodels when an
entailment does not hold. In Sect. 6 we establish similar results for the modal
logic KT. We conclude with §7, where directions for future research are outlined.
Related Work. In [10], Kearns initiated the study of modal semantics without
possible worlds. This work was recently revisited by Skurt and Omori [14], who
generalized Kearns’ work and reframed his framework within the framework of
logical Non-deterministic matrices. As indicated in [14], it was not clear how to
make this semantics eﬀective, as it requires checking truth values of inﬁnitely
many formulas when considering the validity of a given formula (see, e.g., Remark
42 of [14]). In [4], Coniglio et al. develop a similar framework for modal logics,
and some bound over the formulas that need to be considered was achieved.
However, in [5], the authors clariﬁed that it is unclear how to eﬀectively use the
resulting semantics. A semantics based on Nmatrices for the modal logics KT
and S4 was presented in [8] by Gr¨atz, that includes a method to extend a partial
model in that semantics into a total one, which results in an eﬀective semantics.
We chose here to focus on K, which is a weaker logic, forming a common basis
to all other normal modal logics. By deleting one out of four truth values, we

470
O. Lahav and Y. Zohar
obtain corresponding results for KT as well. The semantics that we present here
is similar in nature to the one presented in [8], however: (i) the truth tables
are diﬀerent, as we intentionally enforced the many-valued tables of the classical
connectives to be obtained by a straightforward duplication of truth values from
the original two-valued truth tables; and (ii) the semantic condition for levels of
valuations that we deﬁne here is inductive, where each level relies on lower levels
(thus refraining from a deﬁnition of a more cyclic nature as the one in [8], that is
better understood operationally). A variant of the semantics from [14] was also
introduced and studied in [12], but without considering the ability to perform
eﬀective automated reasoning but instead focusing on inﬁnite valuations rather
than on partial ones. A complete proof theoretic characterization in terms of
sequent calculi to the various levels of valuations was not given in any of the
above works. Also, an eﬀective semantics for K, which is the most basic modal
logic, was not given in any of the above works.
Non-deterministic matrices were introduced in [2], and have since became
a useful tool for investigating non-classical logics and proof systems (see [1]
for a survey). They generalize (deterministic) matrices [15] by allowing a non-
deterministic choice of truth values in the truth tables. Like matrices, Nmatrices
enjoy the semantic analyticity property, which allows one to extend a partial
valuation into a full one. Our semantic framework can be viewed as a further
reﬁnement of non-deterministic matrices, namely restricted non-deterministic
matrices, introduced in [6].
2
Preliminaries
In this section we provide the necessary deﬁnitions about Nmatrices following [1].
We assume a propositional language L with countably inﬁnitely many atomic
variables p1, p2, . . .. When there is no room for confusion, we identify L with its
set of well-formed formulas (e.g., when writing ϕ ∈L). We write sub(ϕ) for the
set of subformulas of a formula ϕ. This notation is extended to sets of formulas
in the natural way.
Valuations. In the context of a set V of “truth values”, a valuation is a function
v from some domain Dom(v) ⊆L to V. For a set F ⊆L, an F-valuation
is a valuation with domain F. (In particular, an L-valuation is deﬁned on all
formulas.) For X ⊆V, we write v−1[X] for the set {ϕ | v(ϕ) ∈X}. For x ∈V,
we also write v−1[x] for the set {ϕ | v(ϕ) = x}.
Deﬁnition 1. Let D ⊆V be a set of “designated truth values”. A valuation v
D-satisﬁes a formula ϕ, denoted by v |=D ϕ, if v(ϕ) ∈D. For a set Σ of formulas,
we write v |=D Σ if v |=D ϕ for every ϕ ∈Σ.
Notation 2. Let D ⊆V be a set of designated truth values and V be a set of
valuations. For sets L, R of formulas, we write L ⊢V
D R if for every v ∈V, v |=D L
implies that v |=D ϕ for some ϕ ∈R. We omit L or R in this notation when they
are empty (e.g., when writing ⊢V
D R), and set parentheses for singletons (e.g.,
when writing L ⊢V
D ϕ).

Eﬀective Semantics for the Modal Logics K and KT
471
Nmatrices. An Nmatrix M for L is a triple of the form ⟨V, D, O⟩, where V is a
set of truth values, D ⊆V is a set of designated truth values, and O is a function
assigning a truth table Vn →P(V)\ {∅} to every n-ary connective ⋄of L (which
assigns a set of possible values to each tuple of values). In the context of an
Nmatrix M = ⟨V, D, O⟩, we often denote O(⋄) by ˜⋄.
An F-valuation v is M-legal if v(ϕ) ∈pos-val(ϕ, M, v) for every formula
ϕ ∈F whose immediate subformulas are contained in F, where pos-val(ϕ, M, v)
is deﬁned by:
1. pos-val(p, M, v) = V for every atomic formula p.
2. pos-val(⋄(ψ1, . . . , ψn), M, v) = ˜⋄(v(ψ1), . . . , v(ψn)) for every non-atomic for-
mula ⋄(ψ1, . . . , ψn).
In other words, there is no restriction regarding the values assigned to atomic
formulas, whereas the values of compound formulas should respect the truth
tables.
Lemma 1 ([1]).
Let F ⊆L be a set closed under subformulas and M an
Nmatrix for L. Then every M-legal F-valuation v can be extended to an M-legal
L-valuation.
3
The Modal Logic K
In this section we introduce a novel eﬀective semantics for the model logic K.
We ﬁrst present a known proof system for this logic (Sect. 3.1), and then our
semantics (Sect. 3.2). From here on, we assume that the language L consists
of the connectives ⊃, ∧, ∨, ¬ and □with their usual arities. The standard ♦
operator can be deﬁned as a macro ♦ϕ
def= ¬□¬ϕ. Obviously, using De-Morgan
rules, fewer connectives can be used. However, we chose this set of connectives in
order to have a primitive language rich enough for the examples that we include
along the paper.
3.1
Proof System
Figure 1 presents a Gentzen-style calculus, denoted by GK, for the modal logic
K that was proven to be equivalent to the original formulation of the logic as a
Hilbert system (see, e.g., [16]). We take sequents to be pairs ⟨Γ, Δ⟩of ﬁnite sets
of formulas. For readability, we write Γ ⇒Δ instead of ⟨Γ, Δ⟩and use standard
notations such as Γ, ϕ ⇒ψ instead of (Γ ∪{ϕ}) ⇒{ψ}.
The (cut) rule is included in GK for convenience, but applications of (cut)
can be eliminated from derivations (see, e.g., [11]). Since the focus of this paper
is semantics rather than cut-elimination, we allow ourselves to use cut freely and
do not distinguish derivations that use it from derivations that do not. We write
⊢GK Γ ⇒Δ if there is a derivation of a sequent Γ ⇒Δ in the calculus GK.
In the sequel, we provide a semantic characterization of ⊢GK. It is based on
a more reﬁned notion of derivability that takes into account: (i) the set F of
formulas used in the derivation; and (ii) the (K)-depth of the derivation, as
deﬁned next.

472
O. Lahav and Y. Zohar
Fig. 1. The sequent calculus GK
Deﬁnition 3. A derivation of a sequent Γ ⇒Δ in GK is a tree in which the
nodes are labeled with sequents, the root is labeled with Γ ⇒Δ, and every node
is the result of an application of some rule of GK where the premises are the labels
of its children in the tree. A derivation is called an F-derivation if it employs
only sequents composed of formulas from F. The (K)-depth of a derivation is
the maximal number of applications of rule (K) in any of the branches of the
derivation.
Notation 4. We write ⊢F,m
GK
Γ ⇒Δ if there is a derivation of Γ ⇒Δ in GK
in which only F-sequents occur and that has (K)-depth at most m. We drop F
from this notation when F = L; and drop m to dismiss the restriction regarding
the (K)-depth.
Example 1. Let ϕ
def= □(p1 ∧p2) ⊃(□p1 ∧□p2) and F = sub(ϕ). The following
is a derivation of ⇒ϕ in GK that only uses F-formulas and has (K)-depth of 1
(though the number of applications of (K) in the derivation is 2):
p1, p2 ⇒p1
(id)
p1 ∧p2 ⇒p1
(∧⇒)
□(p1 ∧p2) ⇒□p1
(K)
p1, p2 ⇒p2
(id)
p1 ∧p2 ⇒p2
(∧⇒)
□(p1 ∧p2) ⇒□p2
(K)
□(p1 ∧p2) ⇒□p1 ∧□p2
(⇒∧)
⇒□(p1 ∧p2) ⊃□p1 ∧□p2
(⇒⊃)
3.2
Semantics
The semantics is based on a four-valued Nmatrix stratiﬁed with “levels”, where
for every m, legal valuations of level m + 1 are a subset of legal valuations of
level m. The underlying Nmatrix, denoted by MK, is obtained by duplicating
the classical truth values. Thus, the sets of truth values and of designated truth
values are given by:
V4
def= {T, t, f, F}
D
def= {T, t}

Eﬀective Semantics for the Modal Logics K and KT
473
The truth tables are as follows (we have D = {f, F}):
x˜⊃y T t F f
T
D D D D
t
D D D D
F
D D D D
f
D D D D
x˜∧y T t F f
T
D D D D
t
D D D D
F
D D D D
f
D D D D
x˜∨y T t F f
T
D D D D
t
D D D D
F
D D D D
f
D D D D
x ˜¬x
T D
t
D
F
D
f
D
x ˜□x
T
D
t
D
F
D
f
D
We employ the following notations for subsets of truth values:
TF
def= {T, F}
tf
def= {t, f}
For the classical connectives, the truth tables of MK treat t just like T, and
f just like F, and are essentially two-valued—the result is either D or D, and it
depends solely on whether the inputs are elements of D or D. Thus, for the lan-
guage without □, this Nmatrix provides a (non-economic) four-valued semantics
for classical logic.
While the output for □is also always D or D, it diﬀerentiates between T
(that results in D) and t (that results in D), and similarly between F and f. In
fact, this table is captured by the condition: ˜□(x) ∈D iﬀx ∈TF.
Example 2. Let F = sub(ϕ) where ϕ is the formula from Example 1. The fol-
lowing valuation v is an F-valuation that is MK-legal:
v(p1) = v(p2) = f
v(p1 ∧p2) = F
v(□p1) = v(□p2) = v(□p1 ∧□p2) = F
v(□(p1 ∧p2)) = T
v(□(p1 ∧p2) ⊃(□p1 ∧□p2)) = F
To show that it is MK-legal, one needs to verify that v(ψ) ∈pos-val(ψ, MK, v)
for each ψ ∈F. For example, v(p1) = f ∈V4 = pos-val(p1, MK, v). As another
example, since v(p1) = f, we have that pos-val(□p1, MK, v) = ˜□(f) = {F, f}, and
hence v(□p1) = F ∈pos-val(□p1, MK, v). Notice that v does not satisfy ϕ.
The truth table for □can be understood via “possible worlds” intuition. Our
four truth values are intuitively captured as follows, assuming a given formula
ψ and a world w:
– T: ψ holds in w and in every world accessible from w;
– t: ψ holds in w but it does not hold in some world accessible from w;
– F: ψ does not hold in w but does hold in every world accessible from w; and
– f: ψ does not hold in w and it does not hold in some world accessible from w.
In the possible worlds semantics, □ψ holds in some world w iﬀψ holds in every
world that is accessible from w, which intuitively explains the table for □. Note
that non-determinism is inherent here. For example, if ψ holds in w and in every
world accessible from w (i.e., ψ has value T), we know that □ψ holds in w, but

474
O. Lahav and Y. Zohar
we do not know whether □ψ holds in every world accessible from w (thus □ψ
has value T or t).
Now, the Nmatrix MK by itself is not adequate for the modal logic K (as
Examples 1 and 2 demonstrate). What is missing is the relation between the
choices we make to resolve non-determinism for diﬀerent formulas. Continuing
with the possible worlds intuition, we observe that if a formula ϕ follows from a
set of formulas Σ that hold in all accessible worlds (i.e., ϕ follows from formulas
whose truth value is T or F), then ϕ itself should hold in all accessible worlds
(i.e., ϕ’s truth value should be T or F). Directly encoding this condition requires
us to consider a set V of MK-legal F-valuations for which the following holds
(recall Notation 2 from Sect. 2):
∀v ∈V. ∀ϕ ∈F. (v−1[TF] ⊢V
D ϕ =⇒v(ϕ) ∈TF)
(necessitation)
In turn, to obtain completeness we take a maximal set V that satisﬁes the
necessitation condition. While it is possible to deﬁne this set of valuations as the
greatest ﬁxpoint of necessitation, following previous work, we ﬁnd it convenient
to reach this set using “levels”:
Deﬁnition 5. The set VF,m
K
is inductively deﬁned as follows:
– VF,0
K
is the set of MK-legal F-valuations.
– VF,m+1
K
def=

v ∈VF,m
K
| ∀ϕ ∈F. v−1[TF] ⊢VF,m
K
D
ϕ =⇒v(ϕ) ∈TF

We also deﬁne:
VF
K
def=

m≥0
VF,m
K
Vm
K
def= VL,m
K
VK
def=

m≥0
VL,m
K
Similarly to the idea originated by Kearns in [10], valuations are partitioned
into levels, which are inductively deﬁned. The ﬁrst level, VF,0
K
, consists solely of
the MK-legal valuations with domain F. For each m > 0, the m’th level is deﬁned
as a subset of the (m −1)’th level, with an additional constraint: a valuation v
from level m −1 remains in level m, only if every formula ϕ ∈F entailed (at
the m −1 level) from the set of formulas that were assigned a value from TF
by v, is itself assigned a value from TF by v. As we show below, in the “end”
of this process, by taking 
m≥0 VF,m
K
, one obtains the greatest set V satisfying
the necessitation condition
Remark 1. The necessitation condition is similar to the one provided in [8] to the
modal logics KT and S4. In contrast, the condition from [4,10,14] is simpler and
does not involve v−1[TF] at all, but also does not give rise to decision procedures.
Example 3. Following Example 2, while the formula ϕ is not satisﬁed by all
valuations in VF,0
K
, it is satisﬁed by all valuations in VF,m
K
for every m > 0. In
particular, the valuation v from Example 2 is not in VF,1
K
: we have p1∧p2 ⊢VF,0
K
D
p1
and v(p1 ∧p2) = F (so p1 ∧p2 ∈v−1[TF]), but v(p1) = f /∈TF.

Eﬀective Semantics for the Modal Logics K and KT
475
For each set F ⊆L and m ≥0, we obtain a consequence relation ⊢VF,m
K
D
between sets of F-formulas. Disregarding m, we also obtain the relation ⊢VF
K
D
(for every F), which we will show to be sound and complete for K. We note that
all these relations are compact. The proof of the following theorem relies on the
completeness theorems that we prove in Sect. 4.
Theorem 1 (Compactness).
1. For every m ≥0, if L ⊢VF,m
K
D
R, then Γ ⊢VF,m
K
D
Δ for some ﬁnite Γ ⊆L and
Δ ⊆R.
2. If L ⊢VF
K
D
R, then Γ ⊢VF
K
D
Δ for some ﬁnite Γ ⊆L and Δ ⊆R.
Now, to show that VF
K is indeed the largest set V of MK-legal F-valuations
that satisﬁes necessitation, we use the following two lemmas. The ﬁrst is a general
construction that relies only on the use of ﬁnite-valued valuation functions.
Lemma 2. Let v0, v1, v2, . . . be an inﬁnite sequence of valuations over a common
domain F. Then, there exists some v such that for every ﬁnite set F′ ⊆F of
formulas and m ≥0, we have v|F′ = vk|F′ for some k ≥m.
Proof (Outline). First, if F is ﬁnite, then there is only a ﬁnite number of F-
valuations, and there must exists some F-valuation vm that occurs inﬁnitely
often in the sequence v0, v1, . . .. We take v = vm, and the required property triv-
ially holds. Now, assume that F is inﬁnite, and let ϕ0, ϕ1, . . . be an enumeration
of the formulas in F. For every i ≥0, let Fi = {ϕ0, . . . , ϕi}. We construct a
sequence of inﬁnite sets A0, A1, . . . ⊆N such that:
– For every i ≥0, Ai+1 ⊆Ai.
– For every 0 ≤j ≤i, a ∈Aj, and b ∈Ai, va(ϕj) = vb(ϕj).
To do so, take some inﬁnite set A0 ⊆N such that va(ϕ0) = vb(ϕ0) for every
a, b ∈A0 (such set must exist since we have a ﬁnite number of truth values).
Then, given Ai, we let Ai+1 be some inﬁnite subset of Ai such that va(ϕi+1) =
vb(ϕi+1) for every a, b ∈Ai+1. The valuation v is deﬁned by v(ϕi) = va(ϕi) for
some a ∈Ai. The properties of the Ai’s ensure that v is well deﬁned, and it can
be shown that it also satisﬁes the required property.
⊓⊔
Using Lemma 2 and the compactness property, we can show the following:
Lemma 3. Let v0, v1, . . . be a sequence of valuations over a common domain F
such that vm ∈VF,m
K
for every m ≥0. Then, there exists some v ∈VF
K such
that for every ϕ ∈F, v(ϕ) = vm(ϕ) for some m ≥0.
Proof (Outline). By Lemma 2, there exists some v such that for every ﬁnite set
F′ of formulas, v|F′ = vm|F′ for some m ≥0. It is easy to verify that v satisﬁes
the required properties. In particular, one shows that v ∈VF,m
K
for every m ≥0
by induction on m. In that proof we use Theorem 1 to obtain a ﬁnite Γ ⊆v−1[TF]
such that Γ ⊢VF,m−1
K
D
ϕ from the assumption that v−1[TF] ⊢VF,m−1
K
D
ϕ. Then, the
above property of v is applied with F′ = Γ ∪{ϕ}.
⊓⊔

476
O. Lahav and Y. Zohar
Now, our characterization theorem easily follows:
Theorem 2. The set VF
K is the largest set V of MK-legal F-valuations that sat-
isﬁes necessitation.
Proof (Outline). To prove that VF
K satisﬁes necessitation, one needs to prove
that if v−1[TF] ⊢VF
K
D
ϕ, then also v−1[TF] ⊢VF,m
K
D
ϕ for some m ≥0. This is done
using Lemma 3. For maximality, given a set V, we assume by contradiction that
there is some m such that V ̸⊆VF,m
K
, take a minimal such m, and show that it
cannot be 0. Then, from V ⊆VF
K m −1, it follows that actually V ⊆VF,m
K
, and
thus we obtain a contradiction.
⊓⊔
Finite Domain. By deﬁnition we have VF,0
K
⊇VF,1
K
⊇VF,2
K
⊇. . . (and so,
⊢VF,0
K
D
⊆⊢VF,1
K
D
⊆⊢VF,2
K
D
⊆. . .). Next, we show that when F is ﬁnite, then this
sequence must converge.
Lemma 4. Suppose that VF,m
K
= VF,m+1
K
for some m ≥0. Then, VF
K = VF,m
K
.
Lemma 5. For a ﬁnite set F of formulas, VF
K = VF,4|F|
K
.
Proof. The left-to-right inclusion follows from our deﬁnitions. For the right-to-
left inclusion, note that by Lemma 4, VF,m
K
= VF,m+1
K
implies that VF,m
K
= VF,k
K
for every k ≥m. Thus, it suﬃces to show that VF,m
K
= VF,m+1
K
for some 0 ≤
m ≤4|F| + 1. Indeed, otherwise we have VF,0
K
⊃VF,1
K
⊃VF,2
K
⊃. . . ⊃VF,4|F|+1
K
,
but this is impossible since there are only 4|F| functions from F to V4.
⊓⊔
Optimized Tables. Starting from level 1, the condition on valuations allows us
to reﬁne the truth tables of MK, and reduce the search space for countermodels.
For instance, since ψ ⊢VF,0
K
D
ϕ ⊃ψ (for every F with {ψ, ϕ, ϕ ⊃ψ} ⊆F), at level
1 we have that if ψ ∈v−1[TF], then v(ϕ ⊃ψ) ∈TF. This allows us to remove
t and f from the ﬁrst and third columns (when y ∈TF) in the table presenting
˜⊃. The following entailments (at level 0), all with a single occurrence of some
connective, lead to similar reﬁnements, resulting in the optimized tables below
for ⊃, ∧and ∨:
ϕ, ϕ ⊃ψ ⊢VF,0
K
D
ψ
ϕ, ψ ⊢VF,0
K
D
ϕ ∧ψ
ϕ ∧ψ ⊢VF,0
K
D
ϕ
ϕ ∧ψ ⊢VF,0
K
D
ψ
ϕ ⊢VF,0
K
D
ϕ ∨ψ
ψ ⊢VF,0
K
D
ϕ ∨ψ
x˜⊃y
T
t
F
f
T
{T} {t} {F} {f}
t
{T} D {F} D
F
{T} {t} {T} {t}
f
{T} D {T} D
x˜∧y
T
t
F
f
T
{T} {t} {F} {f}
t
{t} {t} {f} {f}
F
{F} {f} {F} {f}
f
{f} {f} {f} {f}
x˜∨y
T
t
F
f
T
{T} {T} {T} {T}
t
{T} D {T} D
F
{T} {T} {F} {F}
f
{T} D
{F} D

Eﬀective Semantics for the Modal Logics K and KT
477
We note that level 1 valuations are not fully captured by these tables. For
example, they must assign T to every formula of the form ϕ ⊃ϕ, while the
table above allows also t when v(ϕ) ∈tf. A decision procedure for K can beneﬁt
from relying on these optimized tables instead of the original ones, starting from
level 1.
4
Soundness and Completeness
In this section we establish the soundness and completeness of the proposed
semantics. For that matter, we ﬁrst extend the notion of satisfaction to sequents:
Deﬁnition 6. An F-valuation v D-satisﬁes an F-sequent Γ ⇒Δ, denoted by
v |=D Γ ⇒Δ, if v ̸|=D ϕ for some ϕ ∈Γ or v |=D ϕ for some ϕ ∈Δ.
To prove soundness, we ﬁrst note that except for (K), the soundness of each
derivation rule easily follows from the Nmatrix semantics:
Lemma 6 (Local Soundness). Consider an application of a rule of GK other
than (K) deriving a sequent Γ ⇒Δ from sequents Γ1 ⇒Δ1, . . . , Γn ⇒Δn, such
that Γ ∪Γ1 ∪. . . ∪Γn ∪Δ ∪Δ1 ∪. . . ∪Δn ⊆F. Let v ∈VF,m
K
for some m ≥0.
If v |=D Γi ⇒Δi for every 1 ≤i ≤n, then v |=D Γ ⇒Δ.
For (K), we make use of the level requirement, and prove the following
lemma.
Lemma 7 (Soundness of (K)).
Suppose that Γ ∪□Γ ∪{ϕ, □ϕ} ⊆F, and
Γ ⊢VF,m−1
K
D
ϕ. Then, □Γ ⊢VF,m
K
D
□ϕ.
Proof. Let v ∈VF,m
K
such that v |=D □Γ. We prove that v |=D □ϕ. By the
truth table of □, we have that v(ψ) ∈TF for every ψ ∈Γ, and we need to
show that v(ϕ) ∈TF. Since v(ψ) ∈TF for every ψ ∈Γ, we have Γ ⊆v−1[TF].
Since Γ ⊢VF,m−1
K
D
ϕ, we have v−1[TF] ⊢VF,m−1
K
D
ϕ. Since v ∈VF,m
K
, it follows that
v(ϕ) ∈TF.
⊓⊔
The above two lemmas together establish soundness, and from soundness for
each level, we easily derive soundness for arbitrary (K)-depth.
Theorem 3 (Soundness for m). If ⊢F,m
GK
Γ ⇒Δ, then Γ ⊢VF,m
K
D
Δ.
Theorem 4 (Soundness without m). If ⊢F
GK Γ ⇒Δ, then Γ ⊢VF
K
D
Δ.
By taking F = L in Theorem 4 we get that if ⊢GK Γ ⇒Δ, then Γ ⊢VK
D Δ.
Next, we prove the following two completeness theorems:
Theorem 5 (Completeness for m).
Let F ⊆L closed under subformulas
and Γ ⇒Δ an F-sequent. If Γ ⊢VF,m
K
D
Δ, then ⊢F,m
GK
Γ ⇒Δ.

478
O. Lahav and Y. Zohar
Theorem 6 (Completeness without m).
Let F ⊆L closed under subfor-
mulas and Γ ⇒Δ an F-sequent. If Γ ⊢VF
K
D
Δ, then ⊢F
GK Γ ⇒Δ.
In fact, since F may be inﬁnite, we need to prove stronger theorems than
Theorems 5 and 6, that incorporate inﬁnite sequents.
Deﬁnition 7. An ω-sequent is a pair ⟨L, R⟩, denoted by L ⇒R, such that L and
R are (possibly inﬁnite) sets of formulas. We write ⊩F,m
GK
L ⇒R if ⊢F,m
GK
Γ ⇒Δ
for some ﬁnite Γ ⊆L and Δ ⊆R.
Other notions for sequents (e.g., being an F-sequent) are extended to ω-
sequents in the obvious way. In particular, v |=D L ⇒R if v(ψ) /∈D for some
ψ ∈L or v(ψ) ∈D for some ψ ∈R.
Theorem 7 (ω-Completeness for m). Let F ⊆L closed under subformulas
and L ⇒R an ω-F-sequent. If L ⊢VF,m
K
D
R, then ⊩F,m
GK
L ⇒R.
Theorem 8 (ω-Completeness without m). Let F ⊆L closed under subfor-
mulas and L ⇒R an ω-F-sequent. If L ⊢VF
K
D
R, then ⊩F
GK L ⇒R.
Theorem 5 is a consequence of Theorem 7. Indeed, by Theorem 7, Γ ⊢VF,m
K
D
Δ
implies that ⊢F,m
GK
Γ ′ ⇒Δ′ for some (ﬁnite) Γ ′ ⊆Γ and Δ′ ⊆Δ. Using (weak),
we obtain that ⊢F,m
GK
Γ ⇒Δ. Similarly, Theorem 6 is a consequence of Theorem
8. Also, using Lemma 3, we obtain Theorem 8 from Theorem 7. Hence in the
remainder of this section we focus on the proof of Theorem 7.
Proof of Theorem 7. We start by deﬁning maximal and consistent ω-sequents,
and proving their existence.
Deﬁnition 8 (Maximal and consistent ω-sequent). Let F ⊆L and m ≥0.
An F-ω-sequent L ⇒R is called:
1. F-maximal if F ⊆L ∪R.
2. ⟨GK, F, m⟩-consistent if ̸⊩F,m
GK
L ⇒R.
3. ⟨GK, F, m⟩-maximal-consistent (in short, ⟨GK, F, m⟩-max-con) if it is F-
maximal and ⟨GK, F, m⟩-consistent.
Lemma 8. Let F ⊆L and L ⇒R an F-ω-sequent. Suppose that ̸⊩F,m
GK
L ⇒R.
Then, there exist sets LMC(GK,F,m,L⇒R) and RMC(GK,F,m,L⇒R) such that the
following hold:
– L ⊆LMC(GK,F,m,L⇒R) and R ⊆RMC(GK,F,m,L⇒R).
– LMC(GK,F,m,L⇒R) ∪RMC(GK,F,m,L⇒R) ⊆F.
– LMC(GK,F,m,L⇒R) ⇒RMC(GK,F,m,L⇒R) is ⟨GK, F, m⟩-max-con.
Thus, given an underivable ω-sequent, we can extend it to a ⟨GK, F, m⟩-max-
con ω-sequent. This ω-sequent induces the canonical countermodel, as deﬁned
next.

Eﬀective Semantics for the Modal Logics K and KT
479
Algorithm 1. Deciding Γ ⊢VK
D ϕ.
1: F ←sub(Γ ∪{ϕ})
2: m ←4|F|
3: for v ∈VF,m
K
do
4:
if v |=D Γ and v ̸|=D ϕ then
5:
return (“NO”, v)
6: return “YES”
Notation 9. We denote the set {ψ ∈F | □ψ ∈X} by BX
F .
Deﬁnition 10. Suppose that L ⊎R = F. The canonical model w.r.t. L ⇒R,
F, and m, denoted by v(F, L ⇒R, m), is the F-valuation deﬁned as follows (in
λ notation):
For m = 0:
λϕ ∈F.
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
T
ϕ ∈L and □ϕ ∈L
t
ϕ ∈L and □ϕ /∈L
F
ϕ ∈R and □ϕ ∈L
f
ϕ ∈R and □ϕ /∈L
For m > 0:
λϕ ∈F.
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
T
ϕ ∈L and ⊩F,m−1
GK
BL
F ⇒ϕ
t
ϕ ∈L and ̸⊩F,m−1
GK
BL
F ⇒ϕ
F
ϕ ∈R and ⊩F,m−1
GK
BL
F ⇒ϕ
f
ϕ ∈R and ̸⊩F,m−1
GK
BL
F ⇒ϕ
Clearly, v(F, L ⇒R, m) ̸|=D L ⇒R. The proof of Theorem 7 is done by
induction on m, and then carries on by showing that if L ⇒R is ⟨GK, F, m⟩-
max-con, then v(F, L ⇒R, m) belongs to VF,m
K
for every m.
Concretely, let v
def= v(F, L ⇒R, m). We show that v ∈VF,k
K
for every k ≤m
by induction on k. The base case k = 0 is straightforward. For k > 0, we
have v ∈VF,k−1
K
by the induction hypothesis. Let ϕ ∈F, and suppose that
v−1[TF] ⊢VF,k−1
K
D
ϕ. To show that v(ϕ) ∈TF, we prove that ⊩F,m−1
GK
BL
F ⇒ϕ.
By the outer induction hypothesis (regarding the completeness theorem itself),
v−1[TF] ⊢VF,k−1
K
D
ϕ implies that ⊩F,k−1
GK
v−1[TF] ⇒ϕ, which implies that ⊩F,m−1
GK
v−1[TF] ⇒ϕ. Hence, there is a ﬁnite set {ϕ1, . . . , ϕn} ⊆v−1[TF] such that
⊢F,m−1
GK
{ϕ1, . . . , ϕn} ⇒ϕ. For every 1 ≤i ≤n, since ϕi ∈v−1[TF], we have
that ⊩F,m−1
GK
BL
F ⇒ϕi and hence ⊢F,m−1
GK
Γi ⇒ϕi for some Γi ⊆BL
F. Using
n applications of (cut) on these sequents and ⊢F,m−1
GK
{ϕ1, . . . , ϕn} ⇒ϕ, we
obtain that ⊢F,m−1
GK
Γ1, . . . , Γn ⇒ϕ, and so ⊩F,m−1
GK
BL
F ⇒ϕ.
5
Eﬀectiveness of the Semantics
In this section we study the eﬀectiveness of the semantics introduced in Deﬁni-
tion 5 for deciding ⊢MK . Roughly speaking, a semantic framework is said to be
eﬀective if it induces a decision procedure that decides its underlying logic.
Consider Algorithm 1. Given a ﬁnite set Γ of formulas and a formula ϕ, it
checks whether any valuations in VF,m
K
is a countermodel. The correctness of
this algorithm relies on the analyticity of GK, namely:

480
O. Lahav and Y. Zohar
Lemma 9 ([11]). If ⊢GK Γ ⇒Δ, then ⊢sub(Γ ∪{ϕ})
GK
Γ ⇒Δ.
Using Lemma 9, we show that the algorithm is correct.
Lemma 10. Algorithm 1 always terminates, and returns “YES” iﬀΓ ⊢VK
D ϕ.
Proof. Termination follows from the fact that VF,m
K
is ﬁnite. Suppose that the
result is “YES” and assume for contradiction that Γ ̸⊢VK
D ϕ. Hence, there exists
some u ∈VK such that u |=D Γ and u ̸|=D ϕ. Consider v
def= u|F. Then, v ∈
VF
K ⊆VF,m
K
, which contradicts the fact that the algorithm returns “YES”. Now,
suppose that the result is “NO”. Then, there exists some v ∈VF,m
K
such that
v |=D Γ and v ̸|=D ϕ. By Lemma 5, v ∈VF
K . Hence, Γ ̸⊢VF
K
D
ϕ. By Theorem 3,
we have ̸⊢F
GK Γ ⇒ϕ. By Lemma 9, we have ̸⊢GK Γ ⇒ϕ. By Theorem 6, we have
Γ ̸⊢VK
D ϕ.
⊓⊔
Lemma 10 shows that Algorithm 1 is a decision procedure for ⊢MK , when
ignoring the additional output provided in Line 5. However, it is typical in appli-
cations that a “YES” or “NO” answer is not enough, and often it is expected
that a “NO” result is accompanied with a countermodel. Algorithm 1 returns a
valuation v in case the answer is “NO”, but Lemma 10 does not ensure that v
is indeed a countermodel for Γ ⊢VK
D ϕ. The issue is that the valuation v from the
proof of Lemma 10 witnesses the fact that ̸⊢VK
D only in a non-constructive way.
Indeed, using the soundness and completeness theorems, we are able to deduce
that v′ |=D Γ and v′ ̸|=D ϕ for some v′ ∈VK, but the relation between v and v′ is
unclear. Most importantly, it is not clear whether v and v′ agree on F-formulas.
In the remainder of this section we prove that v′ extends v, and so the returned
countermodel of Line 5 can be trusted.
We say that a valuation v′ extends a valuation v if Dom(v) ⊆Dom(v′) and
v′(ϕ) = v(ϕ) for every ϕ ∈Dom(v) (identifying functions with sets of pairs,
this means v ⊆v′). Clearly, for a Dom(v)-formula ψ we have that v′ |=D ψ iﬀ
v |=D ψ. We ﬁrst show how to extend a given valuation v ∈VF,m
K
by a single
formula ψ such that sub(ψ)\ {ψ} ⊆F, obtaining a valuation v′ ∈VF∪{ψ},m
K
that
agrees with v on all formulas in F.
Lemma 11. Let m ≥0, F ⊆L, and v ∈VF,m
K
. Let ψ ∈L\F such that
sub(ψ)\ {ψ} ⊆F. Then, v can be extended to some v′ ∈VF∪{ψ},m
K
.
We sketch the proof of Lemma 11.
When m = 0, v′ exists from Lemma 1. For m > 0, we deﬁne v′ as follows:1
v′ def= λϕ ∈F ∪{ψ} .
⎧
⎪
⎪
⎨
⎪
⎪
⎩
v(ϕ)
ϕ ∈F
min(pos-val(ψ, MK, v) ∩TF)
ϕ = ψ ∧v−1[TF] ⊢
VF∪{ψ},m−1
K
D
ψ
min(pos-val(ψ, MK, v) ∩tf)
otherwise
1 The use of min here assumes an arbitrary order on truth values. It is used here only
to choose some element from a non-empty set of truth values.

Eﬀective Semantics for the Modal Logics K and KT
481
The proof of Lemma 11 then carries on by showing that v′ ∈VF∪{ψ},m
K
.
Next, Lemma 11 is used in order to extend partial valuations into total ones.
Lemma 12. Let v ∈VF,m
K
for some F closed under subformulas. Then, v can
be extended to some v′ ∈Vm
K .
Finally, Lemmas 3 and 12 can be used in order to extend any partial valuation
in VF
K into a total one.
Lemma 13. Let v ∈VF
K for some set F closed under subformulas. Then, v can
be extended to some v′ ∈VK.
We conclude by showing that when Algorithm 1 returns (“NO”, v), then v
is a ﬁnite representation of a true countermodel for Γ ⊢MK ϕ.
Corollary 1. If Γ ̸⊢VK
D ϕ. Then Algorithm 1 returns (“NO”, v) for some v for
which there exists v′ ∈VK such that v = v′|sub(Γ ∪{ϕ}), v′ |=D Γ, and v′ ̸|=D ϕ.
Proof. Suppose that Γ ̸⊢VK
D ϕ. Then by Lemma 10, Algorithm 1 does not return
“YES”. Therefore, it returns (“NO”, v) for some v ∈VF,m
K
such that v |=D Γ
and v ̸|=D ϕ, where F = sub(Γ ∪{ϕ}) and m = 4|F|. By Lemma 5, v ∈VKF.
By Lemma 13, v can be extended to some v′ ∈VK. Therefore, v = v′|sub(Γ ∪{ϕ}),
v′ |=D Γ, and v′ ̸|=D ϕ.
⊓⊔
Remark 2. Notice that in scenarios where model generation is not important,
m can be set to a much smaller number in Line 2 of Algorithm 1, namely, the
“modal depth” of the input.2 The reason for that is that for such m, it can be
shown that ⊢F,m
GK
Γ ⇒ϕ iﬀ⊢F
GK Γ ⇒ϕ, by reasoning about the applications of
rule (K). Using the soundness and completeness theorems, we can get Γ ⊢VF,m
K
D
ϕ
iﬀΓ ⊢VF
K
D
ϕ, and so limiting to such m is enough. Notice however, that we do not
necessarily get VF,m
K
= VF
K for such m, and so the valuation returned in Line 5
might not be an element of VF
K .
6
The Modal Logic KT
In this section we obtain similar results for the modal logic KT. First, the calculus
GKT is obtained from GK by adding the following rule (see, e.g., [16]):
(T) Γ, ϕ ⇒Δ
Γ, □ϕ ⇒Δ
Derivations are deﬁned as before. (In particular, the (K)-depth of a derivation
still depends on applications of rule (K), not of rule (T).) We write ⊢F,m
GKT
Γ ⇒Δ
2 The modal depth of an atomic formula p is 0. The modal depth of □ϕ is the modal
depth of ϕ plus 1. The modal depth of ⋄(ϕ1, . . . , ϕn) for ⋄̸= □is the maximum
among the modal depths of ϕ1, . . . , ϕn.

482
O. Lahav and Y. Zohar
if there is a derivation of Γ ⇒Δ in GKT in which only F-sequents occur and that
has (K)-depth at most m.
Next, we consider the semantics. For a valuation v ∈VK to respect rule (T),
we must have that if v |=D Γ, ϕ ⇒Δ, then v |=D Γ, □ϕ ⇒Δ. In particular,
when v ̸|=D Γ ⇒Δ, we get that if v(ϕ) /∈D, then v(□ϕ) /∈D. Now, if v(ϕ) = F,
then v(□ϕ) ∈D according to the truth table of □in MK. But, we must have
v(□ϕ) /∈D. This leads us to remove F from MK.
We thus obtain the following Nmatrix MKT: The sets of truth values and of
designated truth values are given by3
V3
def= {T, t, f}
D
def= {T, t}
and the truth tables are as follows:
x˜⊃y T t
f
T
D D {f}
t
D D {f}
f
D D D
x˜∧y
T
t
f
T
D
D {f}
t
D
D {f}
f
{f} {f} {f}
x˜∨y T t
f
T
D D D
t
D D D
f
D D {f}
x ˜¬x
T {f}
t {f}
f
D
x ˜□x
T
D
t {f}
f
{f}
Again, one may gain intuition from the possible worlds semantics. There,
the logic KT is characterized by frames with reﬂexive accessibility relation. Thus,
for instance, if ψ holds in w but not in some world accessible from w (i.e., ψ
has value t), we know that □ψ does not hold in w, and the reﬂexivity of the
accessibility relation implies that □ψ does not hold in some world accessible
from w (thus □ψ has value f).
Example 4. Let ϕ
def= □□(p1 ∧p2) ⊃□p1 and F
def= sub(ϕ). The sequent ⇒ϕ has
a derivation in GKT using only F formulas of (K)-depth of 1. However, it is not
satisﬁed by all MKT-legal F-valuations. For example, the following valuation is
an MKT-legal valuation that does not satisfy ϕ:
v(p1) = v(p2) = t
v(□p1) = f
v(p1 ∧p2) = v(□(p1 ∧p2)) = v(□□(p1 ∧p2)) = T
v(ϕ) = f
Next, we deﬁne the levels of valuations for MKT. These are obtained from
Deﬁnition 5 by removing the value F:
Deﬁnition 11. The set VF,m
KT
is recursively deﬁned as follows:
– VF,0
KT
is the set of MKT-legal F-valuations.
– VF,m+1
KT
def=

v ∈VF,m
KT
| ∀ϕ ∈F. v−1[T] ⊢VF,m
KT
D
ϕ =⇒v(ϕ) = T

We also deﬁne:
VF
KT
def=

m≥0
VF,m
KT
Vm
KT
def= VL,m
KT
VKT
def=

m≥0
VL,m
KT
3 In this section we denote the set {T} by TF.

Eﬀective Semantics for the Modal Logics K and KT
483
Example 5. Following Example 4, we note that for every v ∈VF,m
KT
with m > 0,
we have v |=D ϕ. In particular, the valuation v from Example 4 does not belong
to VF,m
KT
: □(p1 ∧p2) ∈v−1[T], □(p1 ∧p2) ⊢VF,0
KT
D
p1, but v(p1) = t.
Similarly to Theorem 2, the levels of valuations converge to a maximal set
that satisﬁes the following condition:
∀v ∈V. ∀ϕ ∈F. v−1[T] ⊢V
D ϕ =⇒v(ϕ) = T
(necessitationKT)
Theorem 9. The set VF
KT is the largest set V of MKT-legal F-valuations that
satisﬁes necessitationKT.
The proof of Theorem 9 is analogous to that of Theorem 2.
Remark 3. The necessitationKT condition is equivalent to the one given in [8],
except that the underlying truth table is diﬀerent. Theorem 9 proves that our
gradual way of deﬁning VF
KT via levels coincides with the semantic condition
from [8].
As we demonstrated for K, starting from level 1, the condition on valuations
allows us to reﬁne the truth tables of MKT, and reduce the search space. Simple
entailments (at level 0) lead to the optimized tables below for ⊃, ∧and ∨:
x˜⊃y
T
t
f
T
{T} {t} {f}
t
{T} D {f}
f
{T} D
D
x˜∧y
T
t
f
T
{T} {t} {f}
t
{t} {t} {f}
f
{f} {f} {f}
x˜∨y
T
t
f
T
{T} {T} {T}
t
{T} D
D
f
{T} D
{f}
Soundness and completeness for GKT are obtained analogously to GK, keeping
in mind that MKT is obtained from MK by deleting the value F. For soundness,
this is captured by the rule (T). For completeness, the same construction of a
countermodel is performed , while rule (T) ensures that it is three-valued.
Theorem 10 (Soundness and Completeness).
Let F ⊆L closed under
subformulas and Γ ⇒Δ an F-sequent.
1. For every m ≥0, Γ ⊢VF,m
KT
D
Δ iﬀ⊢F,m
GKT
Γ ⇒Δ.
2. Γ ⊢VF
KT
D
Δ iﬀ⊢F
GKT Γ ⇒Δ.
Eﬀectiveness is also shown similarly to K. For that matter, we use the follow-
ing main lemma, whose proof is similar to Lemma 13. The only component that
is added to that proof is making sure that the constructed model is three-valued.
Lemma 14. Let v ∈VF
KT for some set F closed under subformulas. Then, v can
be extended to some v′ ∈VKT.
Let Algorithm 2 be obtained from Algorithm 1 by setting m to 3|F| in Line
2, and taking v ∈VF,m
KT
in Line 3. Similarly to Lemma 10 and Corollary 1, we
get that Algorithm 2 is a model-producing decision procedure for ⊢MKT .

484
O. Lahav and Y. Zohar
Lemma 15. Algorithm 2 always terminates, and returns “YES” iﬀΓ ⊢VKT
D
ϕ.
Further, if Γ ̸⊢VKT
D ϕ, then it returns (“NO”, v) for some v for which there exists
v′ ∈VKT such that v = v′|sub(Γ ∪{ϕ}), v′ |=D Γ, and v′ ̸|=D ϕ.
7
Future Work
We have introduced a new semantics for the modal logic K, based on levels of
valuations in many-valued non-deterministic matrices. Our semantics is eﬀective,
and was shown to tightly correspond to derivations in a sequent calculus for K.
We also adapted these results for the modal logic KT.
There are two main directions for future work. The ﬁrst is to establish sim-
ilar semantics for other normal modal logics, such as KD, K4, S4 and S5, and to
investigate ♦as an independent modality. The second is to analyze the complex-
ity, implement and experiment with decision procedures for K and KT based on
the proposed semantics. In particular, we plan to consider SAT-based decision
procedures that would encode this semantics in SAT, directly or iteratively.
References
1. Avron, A., Zamansky, A.: Non-deterministic semantics for logical systems - a sur-
vey. In: Gabbay, D., Guenther, F. (eds.) Handbook of Philosophical Logic, vol.
16, pp. 227–304. Springer, Dordrecht (2011). https://doi.org/10.1007/978-94-007-
0479-4 4
2. Avron, A., Lev, I.: Non-deterministic multi-valued structures. J. Log. Comput. 15,
241–261 (2005). Conference version: Avron, A., Lev, I.: Canonical propositional
Gentzen-type systems. In: International Joint Conference on Automated Reason-
ing, IJCAR 2001. Proceedings, LNAI, vol. 2083, pp. 529–544. Springer (2001)
3. Biere, A., Heule, M., van Maaren, H., Walsh, T. (eds.): Handbook of Satisﬁability.
Frontiers in Artiﬁcial Intelligence and Applications, 2nd edn., vol. 336. IOS Press
(2021)
4. Coniglio, M.E., del Cerro, L.F., Peron, N.M.: Finite non-deterministic semantics
for some modal systems. J. Appl. Non Class. Log. 25(1), 20–45 (2015)
5. Coniglio, M.E., del Cerro, L.F., Peron, N.M.: Errata and addenda to ‘ﬁnite non-
deterministic semantics for some modal systems’. J. Appl. Non Class. Log. 26(4),
336–345 (2016)
6. Coniglio, M.E., Toledo, G.V.: Two decision procedures for da costa’s Cn logics
based on restricted Nmatrix semantics. Stud. Log. 110, 1–42 (2021)
7. Fagin, R., Halpern, J.Y., Moses, Y., Vardi, M.: Reasoning About Knowledge. MIT
Press, Cambridge (2004)
8. Gr¨atz, L.: Truth tables for modal logics T and S4, by using three-valued non-
deterministic level semantics. J. Log. Comput. 32(1), 129–157 (2022)
9. Halpern, J., Manna, Z., Moszkowski, B.: A hardware semantics based on temporal
intervals. In: Diaz, J. (ed.) ICALP 1983. LNCS, vol. 154, pp. 278–291. Springer,
Heidelberg (1983). https://doi.org/10.1007/BFb0036915
10. Kearns, J.T.: Modal semantics without possible worlds. J. Symb. Log. 46(1), 77–86
(1981)

Eﬀective Semantics for the Modal Logics K and KT
485
11. Lahav, O., Avron, A.: A uniﬁed semantic framework for fully structural proposi-
tional sequent systems. ACM Trans. Comput. Log. 14(4), 271–273 (2013)
12. Pawlowski, P., La Rosa, E.: Modular non-deterministic semantics for T, TB, S4,
S5 and more. J. Log. Comput. 32(1), 158–171 (2022)
13. Pratt, V.R.: Application of modal logic to programming. Stud. Log.: Int. J. Symb.
Log. 39(2/3), 257–274 (1980)
14. Skurt, D., Omori, H.: More modal semantics without possible worlds. FLAP 3(5),
815–846 (2016)
15. Urquhart, A.: Many-valued logic. In: Gabbay, D., Guenthner, F. (eds.) Handbook
of Philosophical Logic, vol. II, 2nd edn., pp. 249–295. Kluwer (2001)
16. Wansing, H.: Sequent systems for modal logics. In: Gabbay, D.M., Guenthner, F.
(eds.) Handbook of Philosophical Logic, vol. 8, 2nd edn., pp. 61–145. Springer,
Dordrecht (2002). https://doi.org/10.1007/978-94-010-0387-2 2
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Local Reductions for the Modal Cube
Cl´audia Nalon1
, Ullrich Hustadt2(B)
, Fabio Papacchini3
,
and Clare Dixon4
1 Department of Computer Science, University of Bras´ılia, Bras´ılia, Brazil
nalon@unb.br
2 Department of Computer Science, University of Liverpool, Liverpool, UK
U.Hustadt@liverpool.ac.uk
3 School of Computing and Communications, Lancaster University in Leipzig,
Leipzig, Germany
f.papacchini@lancaster.ac.uk
4 Department of Computer Science, University of Manchester, Manchester, UK
clare.dixon@manchester.ac.uk
Abstract. The modal logic K is commonly used to represent and reason
about necessity and possibility and its extensions with combinations of
additional axioms are used to represent knowledge, belief, desires and
intentions. Here we present local reductions of all propositional modal
logics in the so-called modal cube, that is, extensions of K with arbitrary
combinations of the axioms B, D, T, 4 and 5 to a normal form comprising
a formula and the set of modal levels it occurs at. Using these reductions
we can carry out reasoning for all these logics with the theorem prover
KSP. We deﬁne benchmarks for these logics and experiment with the
reduction approach as compared to an existing resolution calculus with
specialised inference rules for the various logics.
1
Introduction
Modal logics have been used to represent and reason about mental attitudes such
as knowledge, belief, desire and intention, see for example [17,20,31]. These can
be represented using extensions of the basic modal logic K with one or more
of the axioms B (symmetry), D (seriality), T (reﬂexivity), 4 (transitivity) and
5 (Euclideaness). The logic K and these extensions form the so-called modal cube,
see Fig. 1. In the diagram, a line from a logic L1 to a logic L2 to its right and/or
above means that all theorems of L1 are also theorems of L2, but not vice versa.
As indicated in Fig. 1, some of the logics have the same theorems, e.g., KB5 and
KB4. Also, all logics not explicitly listed have the same theorems as KT5 aka S5.
In total there are 15 distinct logics.
While these modal logics are well-studied and a multitude of calculi and
translations to other logics exist, see, e.g., [1,3–6,9,13,14,16,18,22,41], fully
C. Dixon was partially supported by the EPSRC funded RAI Hubs FAIR-SPACE
(EP/R026092/1) and RAIN (EP/R026084/1), and the EPSRC funded programme
Grant S4 (EP/N007565/1).
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 486–505, 2022.
https://doi.org/10.1007/978-3-031-10769-6_29

Local Reductions for the Modal Cube
487
Fig. 1. Modal Cube: Relationships between modal logics
automatic support by provers is still lacking. Early implementations covering
the full modal cube, such as Catach’s TABLEAUX system [7], are no longer
available. LoTREC 2.0 [10] supports a wide range of logics but is not intended
as an automatic theorem prover. MOIN [11] supports all the logics but the focus
is on producing human-readable proofs and countermodels for small formulae.
Other provers that go beyond just K, like MleanCoP [28] and CEGARBox [15]
only support a small subset of the 15 logics. There are also a range of transla-
tions from modal logics to ﬁrst-order and higher-order logics [13,18,19,27,33].
Regarding implementations of those, SPASS [33,43] is limited to a subset of the
15 logics, while LEO-III [13,36] supports all the logics in the modal cube, but
can only solve very few of the available benchmark formulae.
KSP [23] is a modal logic theorem prover that implements both the modal-
layered resolution (MLR) calculus [25] for the modal logic K and the global
resolution (GMR) calculus [24] for all the 15 logics considered here. It also sup-
ports several reﬁnements of resolution and a range of simpliﬁcation rules. In this
paper, we give reductions of all logics of the modal cube into a normal form for
the basic modal logic K. We then compare the performance of the combination of
these reductions with the modal-layered resolution calculus to that of the global
resolution calculus on a new benchmark collection for the modal cube.
In [29] we have presented new reductions1 of the propositional modal logics
KB, KD, KT, K4, and K5 to Separated Normal Form with Sets of Modal Levels
SNFsml. SNFsml is a generalisation of the Separated Normal Form with Modal
Level, SNFml. In the latter, labelled modal clauses are used where a natural
number label refers to a particular level within a tree Kripke structure at which a
modal clause holds. In the former, a ﬁnite or inﬁnite set of natural numbers labels
each modal clause with the intended meaning that such a modal clause is true
at every level of a tree Kripke structure contained in that set. As our prover KSP
and the modal-layered resolution calculus it implements currently only support
sets of modal clauses in SNFml, we then use a further reduction from SNFsml
1 A reduction here is a satisﬁability preserving mapping between logics.

488
C. Nalon et al.
to SNFml to obtain an automatic theorem prover for these modal logics. Where
all modal clauses are labelled with ﬁnite sets, this reduction is straightforward.
This is the case for KB, KD and KT. For K4 and K5, characterised by the axioms
2ϕ →22ϕ and 3ϕ →23ϕ, modal clauses are in general labelled with inﬁnite
sets. However, using a result by Massacci [21] for K4 and an analogous result
for K5 by ourselves, we are able to bound the maximal level occurring in those
labelling sets which in turn makes a reduction to SNFml possible.
Also in [29], we have shown experimentally that these reductions allow us
to reason eﬀectively in these logics, compared to the global modal resolution
calculus [24] and to the relational and semi-functional translation built into the
ﬁrst-order theorem prover SPASS 3.9 [33,38,42]. The reason that the comparison
only included a rather limited selection of provers is that these are the only ones
with built-in support for all six logics our reductions covered.
Unfortunately, we cannot simply combine our reductions for single axioms to
obtain satisﬁability preserving reductions for their combinations. There are two
main reasons for this. First, our calculus does not use an explicit representation
of the accessibility relationship within a Kripke structure, which would make it
possible to reﬂect modal axioms via corresponding properties of that accessibil-
ity relationship. Instead, we add labelled modal clauses based on instances of the
modal axioms for 2-formulae occurring in the modal formula we want to check
for satisﬁability. However, if we deal with multiple modal axioms, then these
axioms might interact making it necessary to add instances that are not nec-
essary for each individual axiom. For instance, consider, the converse of axiom
B, 32ϕ →ϕ, and axiom 4, 2ϕ →22ϕ. Together they imply 32ϕ →2ϕ.
Instances of this derived axiom are necessary for completeness of a reduction
from KB4 to K, but are unsound for KB and K4 separately.
Second, our reductions attempt to keep the labelling sets minimal in size in
order to decrease the number of inferences that can be performed. Again, taking
axioms B and 4 as examples, in KB, a 2-formula 2ψ true at level ml in a tree-
like Kripke structure M forces ψ to be true at level ml −1, while in K4, 2ψ
true at level ml in M forces ψ to be true all levels ml′ with ml′ > ml. This is
reﬂected in the labelling sets we use for these two logics. However, for KB4, 2ψ
true at level ml forces ψ to be true at every level in a tree-like Kripke structure
M (unless M consists only of a single world).
Since we intend to maintain these two properties of our reductions, we have to
consider each modal logic individually. As we will see, for some logics a reduction
can be obtained as the union of the existing reductions while for others we need
a logic-speciﬁc reduction to accommodate the interaction of axioms.
The structure of the paper is as follows. In Sect. 2 we recall common con-
cepts of propositional modal logic and the deﬁnition of our normal form SNFml.
Section 3 introduces our reduction for extensions of the basic modal logic K with
combinations of the axioms B, D, T, 4, and 5. Section 4 presents a transforma-
tion from SNFsml to SNFml which allows us to use the modal resolution prover
KSP to reason in all the modal logics. In Sect. 5 we compare the performance
of a combination of our reductions and the modal-layered resolution calculus
implemented in the prover KSP with resolution calculi speciﬁcally designed for
the logics under consideration as well as the prover LEO-III.

Local Reductions for the Modal Cube
489
2
Preliminaries
The language of modal logic is an extension of the language of propositional
logic with a unary modal operator 2 and its dual 3. More precisely, given a
denumerable set of propositional symbols, P = {p, p0, q, q0, t, t0, . . .} as well as
propositional constants true and false, modal formulae are inductively deﬁned
as follows: constants and propositional symbols are modal formulae. If ϕ and ψ
are modal formulae, then so are ¬ϕ, (ϕ ∧ψ), (ϕ ∨ψ), (ϕ →ψ), 2ϕ, and 3ϕ.
We also assume that ∧and ∨are associative and commutative operators and
consider, e.g., (p∨(q∨r)) and (r∨(q∨p)) to be identical formulae. We often omit
parentheses if this does not cause confusion. By var(ϕ) we denote the set of all
propositional symbols occurring in ϕ. This function straightforwardly extends
to ﬁnite sets of modal formulae. A modal axiom (schema) is a modal formula ψ
representing the set of all instances of ψ.
A literal is either a propositional symbol or its negation; the set of literals is
denoted by LP . By ¬l we denote the complement of the literal l ∈LP , that is, if
l is the propositional symbol p then ¬l denotes ¬p, and if l is the literal ¬p then
¬l denotes p. By |l| for l ∈LP we denote p if l = p or l = ¬p. A modal literal is
either 2l or 3l, where l ∈LP .
A (normal) modal logic is a set of modal formulae which includes all propo-
sitional tautologies, the axiom schema 2(ϕ →ψ) →(2ϕ →2ψ), called the
axiom K, it is closed under modus ponens (if ⊢ϕ and ⊢ϕ →ψ then ⊢ψ) and
the rule of necessitation (if ⊢ϕ then ⊢2ϕ).
K is the weakest modal logic, that is, the logic given by the smallest set of
modal formulae constituting a normal modal logic. By KΣ we denote an extension
of K by a set Σ of axioms.
The standard semantics of modal logics is the Kripke semantics or possible
world semantics. A Kripke frame F is an ordered pair ⟨W, R⟩where W is a non-
empty set of worlds and R is a binary (accessibility) relation over W. A Kripke
structure M over P is an ordered pair ⟨F, V ⟩where F is a Kripke frame and the
valuation V is a function mapping each propositional symbol in P to a subset
V (p) of W. A rooted Kripke structure is an ordered pair ⟨M, w0⟩with w0 ∈W. To
simplify notation, in the following we write ⟨W, R, V ⟩and ⟨W, R, V, w0⟩instead
of ⟨⟨W, R⟩, V ⟩and ⟨⟨⟨W, R⟩, V ⟩, w0⟩, respectively.
Satisfaction (or truth) of a formula at a world w of a Kripke structure M =
⟨W, R, V ⟩is inductively deﬁned by:
⟨M, w⟩|= true;
⟨M, w⟩̸|= false;
⟨M, w⟩|= p
iﬀw ∈V (p), where p ∈P;
⟨M, w⟩|= ¬ϕ
iﬀ⟨M, w⟩̸|= ϕ;
⟨M, w⟩|= (ϕ ∧ψ)
iﬀ⟨M, w⟩|= ϕ and ⟨M, w⟩|= ψ;
⟨M, w⟩|= (ϕ ∨ψ)
iﬀ⟨M, w⟩|= ϕ or ⟨M, w⟩|= ψ;
⟨M, w⟩|= (ϕ →ψ) iﬀ⟨M, w⟩|= ¬ϕ or ⟨M, w⟩|= ψ;
⟨M, w⟩|= 2ϕ
iﬀfor every v, w R v implies ⟨M, v⟩|= ϕ;
⟨M, w⟩|= 3ϕ
iﬀthere is v, w R v and ⟨M, v⟩|= ϕ.

490
C. Nalon et al.
Table 1. Modal axioms and relational frame properties
Name Axiom
Frame Property
D
2ϕ →3ϕ
Serial
∀v∃w.v R w
T
2ϕ →ϕ
Reﬂexive
∀w.w R w
B
ϕ →23ϕ
Symmetric ∀vw.v R w →w R v
4
2ϕ →22ϕ Transitive
∀uvw.(u R v ∧v R w) →u R w
5
3ϕ →23ϕ Euclidean
∀uvw.(u R v ∧u R w) →v R w
Table 2. Rewriting Rules for Simpliﬁcation
ϕ ∧ϕ ⇒ϕ
ϕ ∨ϕ ⇒ϕ
ϕ ∧true ⇒ϕ
ϕ ∧¬ϕ ⇒false
ϕ ∨¬ϕ ⇒true
ϕ ∧false ⇒false
2true ⇒true
3false ⇒false
ϕ ∨false ⇒ϕ
¬true ⇒false
¬false ⇒true
ϕ ∨true ⇒true
¬¬ϕ ⇒ϕ
If ⟨M, w⟩|= ϕ holds then M is a model of ϕ, ϕ is true at w in M and M
satisﬁes ϕ. A modal formula ϕ is satisﬁable iﬀthere exists a Kripke structure
M and a world w in M such that ⟨M, w⟩|= ϕ.
We are interested in extensions of K with the modal axioms shown in Table 1
and their combinations. Each of these axioms deﬁnes a class of Kripke frames
where the accessibility relation R satisﬁes the ﬁrst-order property stated in the
table. Combinations of axioms then deﬁne a class of Kripke frames where the
accessibility relation satisﬁes the combination of their corresponding properties.
Given a normal modal logic L with corresponding class of frames F, we say
a modal formula ϕ is L-satisﬁable iﬀthere exists a frame F ∈F, a valuation V
and a world w ∈F such that ⟨F, V, w⟩|= ϕ. It is L-valid or valid in L iﬀfor
every frame F ∈F, every valuation V and every world w ∈F, ⟨F, V, w⟩|= ϕ. A
normal modal logic L2 is an extension of a normal modal logic L1 iﬀall L1-valid
formulae are also L2-valid.
A rooted Kripke structure M = ⟨W, R, V, w0⟩is a rooted tree Kripke structure
iﬀR is a tree, that is, a directed acyclic connected graph where each node has at
most one predecessor, with root w0. It is a rooted tree Kripke model of a modal
formula ϕ iﬀ⟨W, R, V, w0⟩|= ϕ. In a rooted tree Kripke structure with root w0
for every world wk ∈W there is exactly one path connecting w0 and wk, the
length of that path is the modal level of wk (in M), denoted by mlM(wk).
It is well-known [17] that a modal formula ϕ is K-satisﬁable iﬀthere is a
ﬁnite rooted tree Kripke structure M = ⟨F, V, w0⟩such that ⟨M, w0⟩|= ϕ.
For the reductions presented in the next section we assume that any modal
formula ϕ has been simpliﬁed by exhaustively applying the rewrite rules in
Table 2, and it is in Negation Normal Form (NNF). That is, a formula where
only propositional symbols are allowed in the scope of negations. We say that
such a formula is in simpliﬁed NNF.
The reductions produce formulae in a clausal normal form, called Separated
Normal Form with Sets of Modal Levels SNFsml, introduced in [29]. The language

Local Reductions for the Modal Cube
491
of SNFsml extends that of the basic modal logic K with sets of modal levels as
labels. Clauses in SNFsml have one of the following forms:
S : n
i=1 li
(literal clause)
S : l′ →2l
(positive modal clause)
S : l′ →3l
(negative modal clause)
where S ⊆N and l, l′, li are propositional literals with 1 ≤i ≤n, n ∈N. We
write ⋆: ϕ instead of N : ϕ and such clauses are called global clauses. Positive
and negative modal clauses are together known as modal clauses.
Given a rooted tree Kripke structure M and a set S of natural numbers,
by M[S] we denote the set of worlds that are at a modal level in S, that is,
M[S] = {w ∈W | mlM(w) ∈S}. Then
M |= S : ϕ iﬀ⟨M, w⟩|= ϕ for every world w ∈M[S].
The motivation for using a set S to label clauses is that in our reductions
the formula ϕ may hold at several levels, possibly an inﬁnite number of levels.
It therefore makes sense to label such formulae not with just a single level, but
a set of levels. The Separated Normal Form with Modal Level, SNFml, can be
seen as the special case of SNFsml where all labelling sets are singletons.
Note that if S = ∅, then M |= S : ϕ trivially holds. Also, a Kripke structure
M can satisfy S : false if there is no world w with mlM(w) ∈S. On the other
hand, S : false with 0 ∈S is unsatisﬁable as a rooted tree Kripke structure
always has a world with modal level 0.
If M |= S : ϕ, then we say that S : ϕ holds in M or is true in M. For a set
Φ of labelled formulae, M |= Φ iﬀM |= S : ϕ for every S : ϕ in Φ, and we say Φ
is K-satisﬁable.
We introduce some notation that will be used in the following. Let S+ =
{l +1 ∈N | l ∈S}, S−= {l −1 ∈N | l ∈S}, and S≥= {n | n ≥min(S)}, where
min(S) is the least element in S. Note that the restriction of the elements being
in N implies that S−cannot contain negative numbers.
3
Extensions of K
In this section we deﬁne reductions from all the logics in the modal cube to
SNFsml. We assume that the set P of propositional symbols is partitioned into
two inﬁnite sets Q and T such that Q contains the propositional symbols of
the modal formula ϕ under consideration, and T surrogate symbols tψ for every
subformula ψ of ϕ and supplementary propositional symbols. In particular, for
every modal formula ψ we have var(ψ) ⊂Q and there exists a propositional sym-
bol tψ ∈T uniquely associated with ψ. These surrogate symbols serve the same
purpose as Tseitin variables [40] and Skolem predicates [30,39] in the transfor-
mation of propositional and ﬁrst-order formulae, respectively, to clausal form via
structural transformation.
It turns out that given a reduction ρKΣ for KΣ with {D, T} ∩Σ = ∅, there
is a uniform and straightforward way we can obtain a reduction for KDΣ and
KTΣ from ρKΣ. Also, the valid formulae of KDTΣ are the same as those of

492
C. Nalon et al.
Table 3. Categorisation of modal logics in the modal cube
‘Base logics’
K
KB
K4
K5
KB4 K45
Extensions with D KD KDB KD4 KD5
KD45
Extensions with T KT KTB KT4 KT5
KTΣ, so we do not need to consider the case of adding both axioms to KΣ.
Similarly, the logics KT45, KDB4, KTB4 and KT5 all have the same set of valid
formulae. Therefore, as shown in Table 3, we can divide the 15 modal logics into
three categories: Six ‘base logics’, ﬁve modal logics obtained by extending a ‘base
logic’ with D, and a further four modal logics obtained by extending a ‘base logic’
with T. For four of the six ‘base logics’ (namely, K, KB, K4, and K5) we have
already devised reductions in [29], so only two (i.e., KB4 and K45) remain.
Given a modal formula ϕ in simpliﬁed NNF and L = KΣ with Σ ⊆
{B, D, T, 4, 5}, we can obtain a set ΦL of clauses in SNFsml such that ϕ is
L-satisﬁable iﬀΦL is K-satisﬁable with ΦL = ρsml
L
(ϕ) = {{0} : tϕ} ∪ρL({0} :
tϕ →ϕ), where ρL is deﬁned as follows:
ρL(S : t →true) = ∅
ρL(S : t →false) = {S : ¬t}
ρL(S : t →(ψ1 ∧ψ2)) = {S : ¬t ∨η(ψ1), S : ¬t ∨η(ψ2)} ∪δL(S, ψ1) ∪δL(S, ψ2)
ρL(S : t →ψ) = {S : ¬t ∨ψ}
if ψ is a disjunction of literals
ρL(S : t →(ψ1 ∨ψ2)) = {S : ¬t ∨η(ψ1) ∨η(ψ2)} ∪δL(S, ψ1) ∪δL(S, ψ2)
if ψ1 ∨ψ2 is not a disjunction of literals
ρL(S : t →3ψ) = {S : t →3η(ψ)} ∪δL(S+, ψ)
ρL(S : t →2ψ) = PL(S : t →2ψ) ∪ΔL(S : t →2ψ)
η and δL are deﬁned as follows:
η(ψ) =

ψ,
if ψ is a literal
tψ,
otherwise
δL(S, ψ) =

∅,
if ψ is a literal
ρL(S : tψ →ψ),
otherwise
and functions PL and ΔL, are deﬁned as shown in Table 4.
We can see in Table 4 that the reduction for KB4 has an additional SNFsml
clause ⋆: t2ψ ∨t2¬t2ψ that occurs neither in the reduction for KB nor in that for
K4. It can be seen as an encoding of the derived axiom 32ψ →2ψ that follows
from the contrapositive 32ψ →ψ of B and 4 2ψ′ →22ψ′.
For K45 we see that all the SNFsml clauses in the reduction for K5 carry over.
These clauses are already suﬃcient to ensure that, semantically, if t2ψ is true at
any world at a level other than 0, then t2ψ is true at every world. Consequently,
to accommodate axiom 4, it suﬃces to add the SNFsml clause {0} : t2ψ →2t2ψ
to ensure that this also holds for the root world at level 0.

Local Reductions for the Modal Cube
493
L
PL(S : t2ψ →2ψ)
ΔL(S : t2ψ →2ψ)
K
S : t2ψ →2η(ψ)
δL(S+, ψ)
KB
S : t2ψ →2η(ψ),
S−: η(ψ) ∨t2¬t2ψ, S−: t2¬t2ψ →2¬t2ψ
δL(S−∪S+, ψ)
K4
S≥: t2ψ →2η(ψ), S≥: t2ψ →2t2ψ
δL((S+)≥, ψ)
K5
⋆: t2ψ →2η(ψ),
⋆: ¬t3t2ψ ∨t2ψ, ⋆: t3t2ψ →3t2ψ,
⋆: ¬t3t2ψ →2¬t2ψ, ⋆: t3t2ψ →2t3t2ψ
δL(⋆, ψ)
KB4
⋆: t2ψ →2η(ψ),
⋆: η(ψ) ∨t2¬t2ψ,
⋆: t2ψ ∨t2¬t2ψ,
⋆: t2¬t2ψ →2¬t2ψ, ⋆: t2ψ →2t2ψ
δL(⋆, ψ)
K45
⋆: t2ψ →2η(ψ),
{0} : t2ψ →2t2ψ iﬀ0 ∈S,
⋆: ¬t3t2ψ ∨t2ψ,
⋆: t3t2ψ →3t2ψ,
⋆: ¬t3t2ψ →2¬t2ψ, ⋆: t3t2ψ →2t3t2ψ
δL(⋆, ψ)
KDΣ {lbP
KΣ(S) : t2ψ →3η(ψ)} ∪PKΣ(S : t2ψ →2ψ) δL(lbδ
KΣ(S), ψ)
KTΣ {lbP
KΣ(S) : ¬t2ψ ∨η(ψ)} ∪PKΣ(S : t2ψ →2ψ)
δL(lbδ
KΣ(S) ∪S, ψ)
where lbP
KΣ and lbδ
KΣ are deﬁned as follows
Table 4. Reduction of 2-formulae, Σ ⊆{B, 4, 5}.
L
K
KB
K4
K5
KB4
K45
lbP
L(S)
S
S
S≥
⋆
⋆
⋆
lbδ
L(S)
S+
S−∪S+ (S+)≥
⋆
⋆
⋆
For reductions of KDΣ and KTΣ we have favoured the reuse of reductions
for KΣ, KD and KT over optimisation for speciﬁc logics. For example, take KBD.
Given that in a symmetric model, every world w except the root world w0 has
an R-successor, the axiom D only ‘enforces’ that w0 also has an R-successor. So,
instead of adding a clause S : t2ψ →3ψ for every clause S : t2ψ →2η(ψ) we
could just add {0} : t2ψ →3ψ iﬀ0 ∈S. Similarly, in KT5, because of 5, for all
worlds w except w0 we already have w Rw. So, we could again {0} : ¬t2ψ ∨η(ψ)
for every clause S : t2ψ →2η(ψ) iﬀ0 ∈S.
For the KB4-unsatisﬁable formula ψ1 = (¬p ∧332p), if we were to inde-
pendently apply the reductions for KB and K4, that is, we compute {{0} :
tψ1}∪ρKB({0} : tψ1 →ψ1)∪ρK4({0} : tψ1 →ψ1), then the result is the following
set of clauses Φ1:
(1) {0} : tψ1
(2) {0} : ¬tψ1 ∨¬p
(3) {0} : ¬tψ1 ∨t332p
(4) {0} : t332p →3t32p
(5) {1} : t32p →3t2p
(6) {2}≥: t2p →2p
(7) {2}≥: t2p →2t2p
(8) {1} : p ∨t2¬t2p
(9) {1} : t2¬t2p →2¬t2p
Clauses (1) to (5) stem from the transformation of ψ1 to SNFsml for K,
Clauses (6) and (7) stem from the reduction for 4 and Clauses (8) and (9) stem

494
C. Nalon et al.
from the reduction for B. This set of SNFsml clauses is K-satisﬁable. The clauses
imply {1} : p, but neither {1} : 2p nor {0} : p which we need to obtain a
contradiction. Part of the reason is that we would need to apply the reduction
for 4 and B recursively to newly introduced surrogates for 2-formulae which
in turn leads to the introduction of further surrogates and problems with the
termination of the reduction.
In contrast, the clause set Φ2 obtained by our reduction for KB4 is:
(10) {0} : tψ1
(11) {0} : ¬tψ1 ∨¬p
(12) {0} : ¬tψ1 ∨t332p
(13) {0} : t332p →3t32p
(14) {1} : t32p →3t2p
(15) ⋆: t2p →2p
(16) ⋆: t2p →2t2p
(17) ⋆: p ∨t2¬t2p
(18) ⋆: t2¬t2p →2¬t2p
(19) ⋆: t2p ∨t2¬t2p
(20) ⋆: t2¬t2p →2t2¬t2p
Note Clauses (19) and (20) in Φ2 for which there are no corresponding clauses
in Φ1. Also, the set of labels of Clauses (15) to (18) are strict supersets of those
of the corresponding Clauses (6) to (9). Φ2 implies both {1} : 2p and {0} : p.
The latter, together with Clauses (10) and (11), means Φ2 is K-unsatisﬁable.
Theorem 1. Let ϕ be a modal formula in simpliﬁed NNF, Σ ⊆{B, D, T, 4, 5},
and ΦKΣ = ρsml
KΣ (ϕ). Then ϕ is KΣ-satisﬁable iﬀΦKΣ is K-satisﬁable.
Proof (Sketch). For |Σ| ≤1 this follows from Theorem 5 in [29].
For K45, KB4, KDΣ′, and KTΣ′ with Σ′ ⊆{B, 4, 5} we proceed in analogy
to the proofs of Theorems 3 and 4 in [29]. Let L be one of these logics.
To show that if ϕ is L-satisﬁable then ΦL is K-satisﬁable, we show that
given a rooted L-model M of ϕ a small variation of the unravelling of M is a
rooted tree K-model ⃗ML of ΦL. The main step is to deﬁne the valuation of the
additional propositional symbols tψ so that we can prove that all clauses in ΦL
hold in ⃗ML. To show that if ΦL is K-satisﬁable then ϕ is L-satisﬁable, we take a
rooted tree K-model M = ⟨W, R, V, w0⟩of ΦL and construct a Kripke structure
ML = ⟨W, RL, V, w0⟩. The relation RL is the closure of R under the relational
properties associated with the axioms of L. The proof that ML is a model of ϕ
relies on the fact that the clauses in ΦL ensure that for subformulae 2ψ of ϕ, ψ
will be true at all worlds reachable via RL from a world where 2ψ is true.
⊓⊔
4
From SNFsml to SNFml
As KSP does not support SNFsml, in our evaluation of the eﬀectiveness of the
reductions deﬁned in Sect. 3, we have used a transformation from SNFsml to
SNFml. An alternative approach would be to reﬂect the use of SNFsml in the
calculus and re-implement the prover. Whilst we believe that redesigning the
calculus presents few problems, re-implementing KSP needs more thought in
particular how to represent inﬁnite sets. The route we adopt here allows us to
experiment with the approach in general without having to change the prover.
For extensions of K with one or more of the axioms B, D, T such a transformation

Local Reductions for the Modal Cube
495
Table 5. Bounds on the length of preﬁxes in SST tableaux
Logic L
Bound dbϕ
L
K,KD,KT, KB,KDB,KTB 1 + dϕ
m
K4,S4
2 + dϕ
3 + nϕ
3 × nϕ
2
KD4
2 + dϕ
3 + (max(1, nϕ
3) × nϕ
2)
KB4,KTB4, K5,S5,K45
2 + dϕ
3 + nϕ
3
KD5
2 + dϕ
3 + max(1, nϕ
3)
is straightforward as the sets of modal levels occurring in the normal form of
modal formulae are all ﬁnite. Thus, instead of a single SNFsml clause S : ¬tψ ∨
ηf(ψ) we can use the ﬁnite set of SNFml clauses {ml : ¬tψ ∨ηf(ψ) | ml ∈S}.
For extensions of K with at least one of the axioms 4 and 5, potentially
together with other axioms, the sets of modal levels labelling clauses are in
general inﬁnite. For each logic L it is, however, possible to deﬁne a computable
function that maps the modal formula ϕ under consideration onto a bound dbϕ
L
such that, restricting the modal levels in the normal form of ϕ by dbϕ
L, preserves
satisﬁability equivalence.
To establish the bound and prove satisﬁability equivalence, we need to intro-
duce the basic notions of Single Step Tableaux (SST) calculi for a modal logic L
[14,21], which uses sequences of natural numbers to preﬁx modal formulae in a
tableau. The SST calculus consists of a set of rules, with the (π) rule being the
only rule increasing preﬁxes’ lengths (i.e., σ : 3ϕ/σ.n : ϕ with σ.n new on the
branch). For a logic L, an L-tableau T in the SST calculus for a modal formula
ϕ is a (binary) tree where the root of T . is labelled with 1 : ϕ, and every other
node is labelled with a preﬁxed formula σ : ψ obtained by application of a rule
of the calculus. A branch B is a path from the root to a leaf. A branch B is closed
if it contains either false or a propositional contradiction at the same preﬁx. A
tableau ”T is closed if all its branches are closed. A preﬁxed formula σ : ψ is
reduced for rule (r) in B if the branch B already contains the conclusion of such
rule application. By a systematic tableau construction we mean an application
of the procedure in [14, p. 374] adapted to SST rules.
For each logic L, we establish its bound by considering an L-SST calculus,
where a modal level in an SNFsml clause corresponds to the length of a preﬁx in
an SST tableau. The bound then either follows from an already known bound
on the length of preﬁxes in an SST tableau preserving correctness of the SST
calculus, or we establish such a bound ourselves. To prove satisﬁability equiva-
lence, we show that, for a closed SST tableau with such a bound on the length
of preﬁxes in place, we can construct a resolution refutation of a set of SNFsml
or SNFml clauses with a corresponding bound on modal levels in those clauses.
For a modal formula ϕ in simpliﬁed NNF let dϕ
m be the modal depth of
ϕ, dϕ
3 be the maximal nesting of 3-operators not under the scope of any 2
operators in ϕ, nϕ
2 be the number of 2-subformulae in ϕ, and nϕ
3 be the number of

496
C. Nalon et al.
3-subformulae below 2-operators in ϕ. Our results for the bounds on the length
of preﬁxes in SST tableaux can then be summarised by the following theorem.
Theorem 2. Let L = KΣ with Σ ⊆{B, D, T, 4, 5}. A systematic tableau con-
struction of an L-tableau for a modal formula ϕ in simpliﬁed NNF under the
following Constraints (TC1) and (TC2)
(TC1) a rule (r) of the SST calculus is only applicable to a preﬁxed formula
σ : ψ in a branch B if the formula is not already reduced for (r) in B;
(TC2) rule (π) of the SST calculus is only applicable to preﬁxed formulae σ : 3ψ
with |σ| < dbϕ
L for dbϕ
L as deﬁned in Table 5
terminates in one of following states:
(1) all branches of the constructed tableau are closed and ϕ is L-unsatisﬁable or
(2) at least one branch B is not closed, no rule is still applicable to a labelled
formula in B, and ϕ is L-satisﬁable.
The proof is analogous to Massacci’s [21, Section B.2]. Note that for logics KD4
and KD5, we use max(1, nϕ
3) in the calculation of the bound. That is, if nϕ
3 ≥1
then max(1, nϕ
3) = nϕ
3 and the bound is the same as for K4 and K5. Otherwise
max(1, nϕ
3) = 1, that is, the bound is the same as for a formula with a single
3-subformula below 2-operators in ϕ.
For K, KD, KT, KB and KDB these bounds were already stated in [21, Tables
III and IV]. The bound for KTB follows straightforwardly from that for KB and
KDB. For KD4, Massacci [21, Tables III and IV] states the bound to be the
same as for K4. However, this is not correct for the case that the formula ϕ
contains no 3-formulae, where its bound would simply be 2, independent of ϕ.
For example, the formula 222false which is KD4-unsatisﬁable, does not have
a closed KD4-tableau with this bound. For the other logics the bounds are new.
As argued in [21], the bounds allow tableau decision procedures for extensions
of K with axioms 4 and 5 that do not require a loop check and are therefore of
wider interest.
Note that in KT4, 22ψ and 2ψ are equivalent and so are 2(ψ∧2ϑ) and 2(ψ∧
ϑ). So, it makes sense to further simplify KT4 formulae using such equivalences
before computing the normal form and the bound with the beneﬁt that it may not
only reduce the bound but also the size of the normal form. Similar equivalences
that can be used to reduce the number of modal operators in a formula also
exist for other logics, see, e.g., [8, Chapter 4].
To establish a relationship between closed tableaux and resolution refuta-
tions of a set of SNFml clauses, we formally deﬁne the modal layered resolution
calculus. Table 6 shows the inference rules of the calculus restricted to labels
occurring in our normal form. For GEN1 and GEN3, if the modal clauses in
the premises occur at the modal level ml, then the literal clause in the premises
occurs at modal level ml + 1.

Local Reductions for the Modal Cube
497
Let Φ be a set of SNFml clauses. A (resolution) derivation from Φ is a sequence
of sets Φ0, Φ1, . . . where Φ0 = Φ and, for each i > 0, Φi+1 = Φi ∪{D}, where
D ̸∈Φi is the resolvent obtained from Φi by an application of one of the inference
rules to premises in Φi. A (resolution) refutation of Φ is a derivation Φ0, . . . , Φk,
k ∈N, where 0 : false ∈Φk.
To map a set of SNFsml clauses to a set of SNFml clauses, using a bound
n ∈N on the modal levels, we deﬁne a function dbn on clauses and sets of
clauses in SNFsml as follows:
dbn(S : ϕ) = {ml : ϕ | ml ∈S and ml ≤n}
dbn(Φ) = 
S:ϕ∈Φ dbn(S : ϕ)
Note that preﬁxes in SST-tableaux have a minimal length of 1 while the
minimal modal level in SNFml clauses is 0. So, a preﬁx of length n in a preﬁxed
formula corresponds to a modal level n −1 in an SNFml clause.
The proof of the following theorem then takes advantage of the fact that we
have surrogates and associated clauses for each subformula of ϕ and proceeds
by induction over applications of rule (π).
Theorem 3. Let L = KΣ with Σ ⊆{B, D, T, 4, 5}, ϕ be a KΣ-unsatisﬁable
formula in simpliﬁed NNF, dbϕ
L be as deﬁned in Table 5, and ΦL = ρml
L (ϕ) =
dbdbϕ
L−1(ρsml
L
(ϕ)). Then there is a resolution refutation of ΦL.
Regarding the size of the encoding, we note that, ignoring the labelling sets,
the reduction ρsml
L
into SNFsml is linear with respect to the size of the original
formula. The size including the labelling sets would depend on the exact repre-
sentation of those sets, in particular, of inﬁnite sets. As those are not arbitrary,
there is still an overall polynomial bound on the size of the sets of SNFsml clauses
produced by ρsml
L
. When transforming clauses from SNFsml into SNFml, we may
need to add every clause to all levels within the bounds provided by Theorem 3.
The parameters for calculating those bounds, dϕ
m, dϕ
3, nϕ
3, and nϕ
2, are all them-
selves linearly bound by the size of the formula. Thus, in the worst case, which
is S4, the size of the clause set produced by ρml
L
is bounded by a polynomial of
degree 3 with respect to the size of the original formula.
It is worth pointing out that both the reduction ρsml
L
of a modal formula
to SNFsml and the reduction ρml
L
to SNFml are also reversible, that is, we can
reconstruct the original formula from the SNFsml and from the SNFml clause set
obtained by ρsml
L
or ρml
L , respectively. This reconstruction can also be performed
in polynomial time. Thus the reduction itself does not aﬀect the complexity
of the satisﬁability problem. For instance, the satisﬁability problem for S5 is
NP-complete and so is the satisﬁability problem of the subclass CS5 of SNFml
clause sets that can be obtained as the result of an application of ρml
S5 to a modal
formula. However, a generic decision procedure for K will not be a complexity-
optimal decision procedure for CS5.

498
C. Nalon et al.
Table 6. Inference rules of the MLR calculus
LRES :
ml : D ∨l
ml : D′ ∨¬l
ml : D ∨D′
MRES :
ml : l1 →2l
ml : l2 →3¬l
ml : ¬l1 ∨¬l2
GEN2 :
ml : l′
1 →2l1
ml : l′
2 →2¬l1
ml : l′
3 →3l2
ml : ¬l′
1 ∨¬l′
2 ∨¬l′
3
GEN1 :
ml : l′
1 →2¬l1
...
ml : l′
m →2¬lm
ml : l′ →3¬l
ml + 1 : l1 ∨. . . ∨lm ∨l
ml : ¬l′
1 ∨. . . ∨¬l′
m ∨¬l′
GEN3 :
ml : l′
1 →2¬l1
...
ml : l′
m →2¬lm
ml : l′ →3l
ml + 1 : l1 ∨. . . ∨lm
ml : ¬l′
1 ∨. . . ∨¬l′m ∨¬l′
5
Evaluation
An empirical evaluation of the practical usefulness of the reductions we presented
in Sects. 3 and 4 faces the challenge that there is no substantive collection of
benchmark formulae for the 15 logics of the modal cube except for basic modal
logic. Catach [7] evaluates his prover on 31 modal formulae with a maximal
length of 22 and maximal modal depth of 4. They are not suﬃciently challeng-
ing. The QMLTP Problem Library for First-Order Modal Logics [32] focuses on
quantiﬁed formulae and contains only a few formulae taken from the research
literature that are purely propositional and were not written for the basic modal
logic K. The Logics Workbench (LWB) benchmark collection [2] contains formu-
lae for K, KT and S4 but not for any of the other logics we consider. For each
of these three logics, the collection consists of 18 parameterised classes with 21
formulae each, plus scripts with which further formulae could be generated if
needed. All formulae in 9 classes are satisﬁable and all formulae in the other 9
classes are unsatisﬁable in the respective logic.
In [29] we have used the 18 classes of the LWB benchmark collection for K
to evaluate our approach for the six logics consisting of K and its extensions
with a single axiom. One drawback of using these 18 classes for other modal
logics is that formulae that are K-satisﬁable are not necessarily KΣ-satisﬁable
for non-empty sets Σ of additional axioms. For example, for K5, only 60 out of
180 K-satisﬁable formulae were K5-satisﬁable. Another drawback is that while
K-unsatisﬁable formulae are also KΣ-unsatisﬁable, a resolution refutation would
not necessarily involve any of the additional clauses introduced by our reduction
for KΣ. It may be that the additional clauses allow us to ﬁnd a shorter refutation,
but it may just be a case of ﬁnding the same refutation in a larger search space.
It is also worth recalling that simpliﬁcation alone is suﬃcient to determine that
all formulae in the class k lin p are K-unsatisﬁable while pure literal elimination
can be used to reduce all formulae in k grz p to the same simple formula [26].

Local Reductions for the Modal Cube
499
Table 7. Logic-speciﬁc modiﬁcation of unsatisﬁable benchmark formulae
Logic L ψp
l
K
false
KB
(¬qp ∧32qp)
KDB
(¬qp ∧32((2¬q′
p ∧2q′
p) ∨qp))
KTB
(¬qp ∧32((¬q′
p ∧2q′
p) ∨qp))
KD
(2¬qp ∧2qp)
KT
(¬qp ∧2qp)
K4
(2qp ∧33¬qp)
K4B
(¬qp ∧332qp)
Logic L ψp
l
KD4
(2qp ∧3323¬qp)
K5
(3¬qp ∧32qp)
KD5
((2¬qp ∧2qp) ∨(32q′
p ∧3¬q′
p)
K45
(2qp ∧32q′
p ∧33(¬qp ∨¬q′
p))
KD45
((2¬q′
p ∧2q′
p) ∧
(2qp ∧32q′
p ∧33(¬qp ∨¬q′
p))
S4
(¬q′
p ∧2(¬q′
p ∨2qp) ∧33¬qp)
S5
((¬qp ∧2qp) ∨(¬q′
p ∧3332q′
p)
Thus, some of the classes evaluate the preprocessing capabilities of a prover but
not the actual calculus and its implementation.
We therefore propose a diﬀerent approach here. The principles underlying
our approach are that (i) there should be the same number of formulae for
each logic though not necessarily the same formulae across all logics; (ii) there
should be an equal number of satisﬁable and unsatisﬁable formulae for each logic;
(iii) a formula that is L-unsatisﬁable should only be L′-unsatisﬁable for every
extension L′ of L; (iv) a formula that is L′-satisﬁable should be L-satisﬁable
for every extension L′ of L; (v) the formulae should belong to parameterised
classes of formulae of increasing diﬃculty. Note that Principles (iii) and (iv) are
intentionally not symmetric. For L-unsatisﬁable formulae it should be necessary
for a prover to use the rules or clauses speciﬁc to L instead of being able to ﬁnd
a refutation without those. For L-satisﬁable formulae we want to maximise the
search space for a model.
For unsatisﬁable formulae, we take the ﬁve LWB classes k branch p,
k path p, k ph p, k poly p, k t4p p and for each logic L in the modal cube
transform each formula in a class so that is L-unsatisﬁable, but L′-satisﬁable for
any logic L′ that is not an extension of L. The transformation proceeds by ﬁrst
converting a formula ϕ to simpliﬁed NNF. Then for each propositional literal l
it replaces all its occurrences by (l∨ψp
L) where |l| = p and ψp
L is a modal formula
uniquely associated with p and L, resulting in a formula ϕ′. Finally, for logics
KD4 and KDB we need to add a disjunct (2q ∧2¬q) to ϕ′, while for logics S4
and KTB we need to add a disjunct (q ∧2¬q), where q is a propositional symbol
not occurring in ϕ′. These disjuncts are unsatisﬁable in the respective logics but
satisﬁable in logics where D, or T, do not hold. Table 7 shows the formulae ψp
L
that we use in our evaluation. In the table, qp and q′
p are propositional variables
uniquely associated with p that do not occur in ϕ. The overall eﬀect of this
transformation is that the resulting classes of formulae satisfy Principles (iii)
and (v).
For satisﬁable formulae, we use the ﬁve classes k poly n, s4 md n, s4 ph n,
s4 path n, s4 s5 n without modiﬁcation. Although the ﬁrst of these classes was
designed to be K-satisﬁable and the other four to be S4-satisﬁable, the formulae
in those classes are satisﬁable in all the logics we consider. s4 ipc n also consists

500
C. Nalon et al.
Table 8. Benchmarking results
Logic Status Total
GMR
(cneg)
GMR
(cord)
GMR
(cplain)
R+MLR
(cneg)
R+MLR
(cord)
R+MLR
(cplain)
LEO-
III+E
K
S
100
84
85
77
100
100
100
0
KD
S
100
84
85
77
96
100
93
0
KT
S
100
70
81
50
66
68
61
0
KB
S
100
58
58
29
51
64
51
0
K4
S
100
83
85
77
56
57
50
0
K5
S
100
67
60
45
36
37
26
0
KDB
S
100
63
70
40
56
73
55
0
KTB
S
100
58
59
38
52
57
31
0
KD4
S
100
83
85
77
52
53
46
0
KD5
S
100
73
70
61
46
47
38
0
K45
S
100
45
53
34
36
37
25
0
K4B
S
100
18
19
11
23
38
15
0
KD45 S
100
67
66
56
46
47
38
0
S4
S
100
66
76
48
45
44
33
0
S5
S
100
32
28
32
32
35
24
0
All
S
1500
951
980
752
793
857
686
0
K
U
100
74
76
71
79
78
77
21
KD
U
100
74
76
71
73
75
62
13
KT
U
100
74
77
70
71
74
67
30
KB
U
100
71
78
68
71
52
55
10
K4
U
100
55
52
57
41
29
35
4
K5
U
100
74
46
75
50
30
48
8
KDB
U
100
73
77
71
73
52
56
8
KTB
U
100
72
77
69
67
50
53
9
KD4
U
100
70
59
67
40
32
39
1
KD5
U
100
75
46
77
51
40
46
3
K45
U
100
51
37
49
16
12
8
3
K4B
U
100
47
52
46
53
30
49
5
KD45 U
100
64
43
55
33
22
28
1
S4
U
100
47
68
66
45
21
23
4
S5
U
100
47
51
52
36
13
29
2
All
U
1500
968
915
964
799
610
675
122
only of S5-satisﬁable formulae but these appear to be insuﬃciently challenging
and have not been included in our benchmark set. All other classes of the LWB
benchmark classes for K and S4 are satisﬁable in some of the logics, but not
in all. The ﬁve classes satisfy Principles (iv) and (v). The benchmark collection
consisting of all ten classes together then also satisﬁes Principles (i) and (ii).

Local Reductions for the Modal Cube
501
Another challenge for an empirical evaluation is the lack of available fully
automatic theorem provers for all 15 logics that we have already discussed in
Sect. 1. This leaves us with just three diﬀerent approaches we can compare (i) the
higher-order logic prover LEO-III [12,37], with E 2.6 as external reasoner, LEO-
III+E for short, that supports a wide range of logics via semantic embedding
into higher-order logic (ii) the combination of our reductions with the modal-
layered resolution (MLR) calculus for SNFml clauses [25], R+MLR calculus for
short, implemented in the modal theorem prover KSP (iii) the global modal res-
olution (GMR) calculus, implemented in KSP, which has resolution rules for
all 15 logics [24]. For R+MLR and GMR calculi, resolution inferences between
literal clauses can either be unrestricted (cplain option), restricted by nega-
tive resolution (cneg option), or restricted by an ordering (cord option). It is
worth pointing out that negative and ordered resolution require slightly dif-
ferent transformations to the normal form that introduce additional clauses
(snf+ and snf++ options, respectively). Also, the ordering cannot be arbi-
trary [25]. For the experiments, we have used the following options: (i) input
processing: prenexing, together with simpliﬁcation and pure literal elimination
(bnfsimp, prenex, early ple); (ii) preprocessing of clauses: renaming reuses
symbols (limited reuse renaming), forward and backward subsumption (fsub,
bsub) are enabled; the usable is populated with clauses whose maximal literal is
positive (populate usable, max lit positive); pure literal elimination is set
for GMR (ple) and modal level ple is set for MLR (mlple); (iii) processing: infer-
ence rules not required for completeness are also used (unit, lhs unit,mres),
the options for preprocessing of clauses are kept and clause selection takes the
shortest clause by level (shortest).
For LEO-III we provide the prover with a modal formula in the syntax it
expects plus a logic speciﬁcation that tells the prover in which modal logic
the formula is meant to be solved, for example, $modal system S4. LEO-III
can collaborate with external reasoners during proof search and we have used
E 2.6 [34,35] as external reasoner and restricted LEO-III to one instance of E
running in parallel. LEO-III is implemented in Java and we have set the maxi-
mum heap size to 1 GB and the thread stack size to 64 MB for the JVM.
Table 8 shows our benchmarking results. The ﬁrst three columns of the table
show the logic in which we determine the satisﬁability status of each formula,
the satisﬁability status of the formulae, and their number. The next six columns
then show how many of those formulae were solved by KSP with a particular
calculus and reﬁnement. The last column shows the result for LEO-III. The
highest number or numbers are highlighted in bold. A time limit of 100 CPU
seconds was set for each formula. Benchmarking was performed on a PC with
an AMD Ryzen 5 5600X CPU @ 4.60 GHz max and 64 GB main memory using
Fedora release 34 as operating system.
While the R+MLR calculus is competitive with GMR on extensions of K
with axioms D, T and, possibly, B, the GMR calculus has better performance
on extensions with axioms 4 and 5.
On satisﬁable formulae, where for all logics we use exactly the same formulae
and both resolution calculi have to saturate the set of clauses up to redundancy,

502
C. Nalon et al.
the number of formulae solved is directly linked to the number of inferences
necessary to do so. The fact that we reduce SNFsml clauses to SNFml clauses via
the introduction of multiple copies of the same clausal formulae with diﬀerent
labels clearly leads to a corresponding multiplication of the inferences that need
to be performed. LEO-III+E does not solve any of the satisﬁable formulae. This
can be seen as an illustration of how important the use of additional techniques
is that can turn resolution into a decision procedure on embeddings of modal
logics into ﬁrst-order logic [18,33].
On unsatisﬁable formulae, where we use diﬀerent formulae for each logic,
the number of formulae solved is linked to the number of inferences it takes to
ﬁnd a refutation. For instance, on K it takes the GMR calculus on average 6.2
times the number of inferences to ﬁnd a refutation than the R+MLR calculus.
However, for all other logics the opposite is true. On the remaining 14 logics, the
R+MLR calculus on average requires 6.5 times the number of inferences to ﬁnd
a refutation than the GMR calculus. Given that the R+MLR calculus currently
uses a reduction from a modal logic to SNFsml followed by a transformation
from SNFsml to SNFml, it is diﬃcult to discern which of the two is the major
problem. It is clear that multiple copies of the same clausal formulae are also
detrimental to proof search. LEO-III+E does reasonably well on unsatisﬁable
formulae and the results clearly show the impact that additional axioms have on
its performance. It performs best for KT and K but for logics involving axioms
4 and 5 very few formulae can be solved. The external prover E ﬁnds the proof
for 121 out of the 122 modal formulae LEO-III+E can solve.
6
Conclusions
We have presented novel reductions of extensions of the modal logic K with
arbitrary combinations of the axioms B, D, T, 4, 5 to clausal normal forms
SNFsml and SNFml for K. The implementation of those reductions combined
with KSP [26], allows us to reason in all 15 logics of the modal cube in a fully
automatic way. Such support was so far extremely limited.
The transformation of sets of SNFsml to sets of SNFml relies on new results
that show that non-clausal closed tableaux in the Single Step Tableaux calculus
[14,21] can be simulated by refutations in the modal-layered resolution (MLR)
calculus for SNFml clauses [25].
We have also developed a new collection of benchmark formulae that covers
all 15 logics of the modal cube. The collection consists of classes of parameterised
and therefore scalable formulae. It contains an equal number of satisﬁable and
unsatisﬁable formulae for each logic and the satisﬁability status of each formula is
known in advance. So far extensive collections of benchmark formulae were only
available for K with smaller collections available for KT and S4. A key feature
of the approach is that it uses the systematic modiﬁcation of K-unsatisﬁable
formulae to obtain unsatisﬁable formulae in other logics. Thus, we could obtain
a more extensive collection by applying this approach to further collections of
benchmark formulae for K.

Local Reductions for the Modal Cube
503
The evaluation we presented shows that on most of the 15 modal logics the
combination of our reduction to SNFml with the MLR calculus does not per-
form as well as the global modal resolution (GMR) calculus, also implemented
in KSP. This contrasts with the evaluation in [29], where we only considered six
logics and used a diﬀerent collection of benchmarks. We believe that the new
benchmark collection more clearly indicates weaknesses in the current approach,
in particular, the reduction from SNFsml to SNFml. It is possible that the imple-
mentation of a calculus that operates directly on sets of SNFsml clauses would
perform considerably better as it avoids the repetition of clauses with diﬀerent
labels. However, it does so by using potentially inﬁnite sets of labels which makes
an implementation challenging. We intend to explore this possibility in future
work.
References
1. Areces, C., de Rijke, M., de Nivelle, H.: Resolution in modal, description and hybrid
logic. J. Log. Comput. 11(5), 717–736 (2001)
2. Balsiger, P., Heuerding, A., Schwendimann, S.: A benchmark method for the propo-
sitional modal logics K, KT, S4. J. Autom. Reasoning 24(3), 297–317 (2000).
https://doi.org/10.1023/A:1006249507577
3. Basin, D., Matthews, S., Vigano, L.: Labelled propositional modal logics: theory
and practice. J. Log. Comput. 7(6), 685–717 (1997)
4. Blackburn, P., van Benthem, J., Wolter, F. (eds.): Handbook of Modal Logic.
Elsevier (2006)
5. Blackburn, P., de Rijke, M., Venema, Y.: Modal Logic. Cambridge Tracts in The-
oretical Computer Science. Cambridge University Press (2002)
6. Br¨unnler, K.: Deep sequent systems for modal logic. Arch. Math. Log. 48(6), 551–
577 (2009). https://doi.org/10.1007/s00153-009-0137-3
7. Catach, L.: TABLEAUX: a general theorem prover for modal logics. J. Autom.
Reason. 7(4), 489–510 (1991). https://doi.org/10.1007/BF01880326
8. Chellas, B.F.: Modal Logic: An Introduction. Cambridge University Press, Cam-
bridge (1980)
9. Fitting, M.: Preﬁxed tableaus and nested sequents. Ann. Pure Appl. Log. 163(3),
291–313 (2012)
10. Gasquet, O., Herzig, A., Longin, D., Sahade, M.: LoTREC: logical tableaux
research engineering companion. In: Beckert, B. (ed.) TABLEAUX 2005. LNCS
(LNAI), vol. 3702, pp. 318–322. Springer, Heidelberg (2005). https://doi.org/10.
1007/11554554 25
11. Girlando, M., Straßburger, L.: MOIN: a nested sequent theorem prover for intu-
itionistic modal logics (system description). In: Peltier, N., Sofronie-Stokkermans,
V. (eds.) IJCAR 2020. LNCS (LNAI), vol. 12167, pp. 398–407. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-51054-1 25
12. Gleißner, T., Steen, A.: LEO-III (2022). https://github.com/leoprover/Leo-III
13. Gleißner, T., Steen, A., Benzm¨uller, C.: Theorem provers for every normal modal
logic. In: Eiter, T., Sands, D. (eds.) LPAR 2017. EPiC Series in Computing, vol.
46, pp. 14–30. EasyChair (2017). https://doi.org/10.29007/jsb9
14. Gor´e, R.: Tableau methods for modal and temporal logics. In: D’Agostino, M.,
Gabbay, D., H¨ahnle, R., Posegga, J. (eds.) Handbook of Tableau Methods, pp.

504
C. Nalon et al.
297–396. Springer, Heidelberg (1999). https://doi.org/10.1007/978-94-017-1754-
0 6
15. Gor´e, R., Kikkert, C.: CEGAR-Tableaux: improved modal satisﬁability via modal
clause-learning and SAT. In: Das, A., Negri, S. (eds.) TABLEAUX 2021. LNCS
(LNAI), vol. 12842, pp. 74–91. Springer, Cham (2021). https://doi.org/10.1007/
978-3-030-86059-2 5
16. Governatori, G.: Labelled modal tableaux. In: Areces, C., Goldblatt, R. (eds.)
AiML 2008, pp. 87–110. College Publications (2008)
17. Halpern, J.Y., Moses, Y.: A guide to completeness and complexity for modal logics
of knowledge and belief. Artif. Intell. 54(3), 319–379 (1992)
18. Horrocks, I., Hustadt, U., Sattler, U., Schmidt, R.A.: Computational modal logic.
In: Blackburn, P., van Benthem, J., Wolter, F. (eds.) Handbook of Modal Logic,
chap. 4, pp. 181–245. Elsevier (2006)
19. Hustadt, U., de Nivelle, H., Schmidt, R.A.: Resolution-based methods for modal
logics. Log. J. IGPL 8(3), 265–292 (2000)
20. van Linder, B., van der Hoek, W., Meyer, J.J.C.: Formalising abilities and oppor-
tunities of agents. Fundamenta Informaticae 34(1–2), 53–101 (1998)
21. Massacci, F.: Single step tableaux for modal logics. J. Autom. Reason. 24, 319–364
(2000). https://doi.org/10.1023/A:1006155811656
22. Mayer, M.C.: Herbrand style proof procedures for modal logics. J. Appl. Non Class.
Logics 3(2), 205–223 (1993)
23. Nalon, C.: KSP (2022). https://www.nalon.org/#software
24. Nalon, C., Dixon, C.: Clausal resolution for normal modal logics. J. Algorithms
62, 117–134 (2007)
25. Nalon, C., Dixon, C., Hustadt, U.: Modal resolution: proofs, layers, and reﬁne-
ments. ACM Trans. Comput. Log. 20(4), 23:1–23:38 (2019)
26. Nalon, C., Hustadt, U., Dixon, C.: KSP: architecture, reﬁnements, strategies and
experiments. J. Autom. Reason. 64(3), 461–484 (2020). https://doi.org/10.1007/
s10817-018-09503-x
27. Ohlbach, H.J., Nonnengart, A., de Rijke, M., Gabbay, D.M.: Encoding two-valued
nonclassical logics in classical logic. In: Robinson, J.A., Voronkov, A. (eds.) Hand-
book of Automated Reasoning, chap. 21, pp. 1403–1485. Elsevier (2001)
28. Otten, J.: MleanCoP: a connection prover for ﬁrst-order modal logic. In: Demri,
S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562, pp.
269–276. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6 20
29. Papacchini, F., Nalon, C., Hustadt, U., Dixon, C.: Eﬃcient local reductions to
basic modal logic. In: Platzer, A., Sutcliﬀe, G. (eds.) CADE 2021. LNCS (LNAI),
vol. 12699, pp. 76–92. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-
79876-5 5
30. Plaisted, D.A., Greenbaum, S.: A structure-preserving clause form translation. J.
Symb. Comput. 2(3), 293–304 (1986)
31. Rao, A.S., Georgeﬀ, M.P.: Modeling rational agents within a BDI-architecture. In:
KR 1991, pp. 473–484. Morgan Kaufmann (1991)
32. Raths, T., Otten, J.: The QMLTP problem library for ﬁrst-order modal logics.
In: Gramlich, B., Miller, D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol.
7364, pp. 454–461. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-
642-31365-3 35
33. Schmidt, R.A., Hustadt, U.: First-order resolution methods for modal logics. In:
Voronkov, A., Weidenbach, C. (eds.) Programming Logics. LNCS, vol. 7797, pp.
345–391. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-37651-
1 15

Local Reductions for the Modal Cube
505
34. Schulz,
S.:
E
2.6
(2022).
http://wwwlehre.dhbw-stuttgart.de/∼sschulz/E/
Download.html
35. Schulz, S., Cruanes, S., Vukmirovi´c, P.: Faster, higher, stronger: E 2.3. In: Fontaine,
P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 495–507. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-29436-6 29
36. Steen, A., Benzm¨uller, C.: The higher-order prover Leo-III. In: Giacomo, G.D.,
et al. (eds.) ECAI 2020. Frontiers in Artiﬁcial Intelligence and Applications, vol.
325, pp. 2937–2938. IOS Press (2020). https://doi.org/10.3233/FAIA200462
37. Steen, A., Benzm¨uller, C.: Extensional higher-order paramodulation in Leo-III.
J. Autom. Reason. 65(6), 775–807 (2021). https://doi.org/10.1007/s10817-021-
09588-x
38. The SPASS Team: SPASS 3.9 (2016). http://www.spass-prover.org/
39. Boy de la Tour, T.: An optimality result for clause form translation. J. Symb.
Comput. 14(4), 283–301 (1992)
40. Tseitin, G.S.: On the complexity of derivation in propositional calculus. In: Siek-
mann, J.H., Wrightson, G. (eds.) Automation of Reasoning: Classical Papers
on Computational Logic 1967–1970, vol. 2, pp. 466–483. Springer, Heidelberg
(1983). https://doi.org/10.1007/978-3-642-81955-1 28. Original paper (in Rus-
sian) appeared in 1968
41. Wansing, H.: Sequent calculi for normal modal proposisional logics. J. Log. Com-
put. 4(2), 125–142 (1994)
42. Weidenbach, C.: Combining superposition, sorts and splitting. In: Robinson, J.A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, pp. 1965–2013. Elsevier
and MIT Press (2001)
43. Weidenbach, C., Dimova, D., Fietzke, A., Kumar, R., Suda, M., Wischnewski,
P.: SPASS version 3.5. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol.
5663, pp. 140–145. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-02959-2 10
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Proof Systems and Proof Search

Cyclic Proofs, Hypersequents,
and Transitive Closure Logic
Anupam Das(B) and Marianna Girlando
University of Birmingham, Birmingham, UK
{a.das,m.girlando}@bham.ac.uk
Abstract. We propose a cut-free cyclic system for Transitive Closure
Logic (TCL) based on a form of hypersequents, suitable for automated
reasoning via proof search. We show that previously proposed sequent
systems are cut-free incomplete for basic validities from Kleene Algebra
(KA) and Propositional Dynamic Logic (PDL), over standard transla-
tions. On the other hand, our system faithfully simulates known cyclic
systems for KA and PDL, thereby inheriting their completeness results.
A peculiarity of our system is its richer correctness criterion, exhibiting
‘alternating traces’ and necessitating a more intricate soundness argu-
ment than for traditional cyclic proofs.
Keywords: Cyclic proofs · Transitive Closure Logic · Hypersequents ·
Propositional Dynamic Logic
1
Introduction
Transitive Closure Logic (TCL) is the extension of ﬁrst-order logic by an operator
computing the transitive closure of deﬁnable binary relations. It has been studied
by numerous authors, e.g. [15–17], and in particular has been proposed as a
foundation for the mechanisation and automation of mathematics [1].
Recently, Cohen and Rowe have proposed non-wellfounded and cyclic sys-
tems for TCL [9,11]. These systems diﬀer from usual ones by allowing proofs to
be inﬁnite (ﬁnitely branching) trees, rather than ﬁnite ones, under some appro-
priate global correctness condition (the ‘progressing criterion’). One particular
feature of the cyclic approach to proof theory is the facilitation of automation,
since complexity of inductive invariants is eﬀectively traded oﬀfor a richer proof
structure. In fact this trade oﬀhas recently been made formal, cf. [3,12], and
has led to successful applications to automated reasoning, e.g. [6,7,24,26,27].
In this work we investigate the capacity of cyclic systems to automate reason-
ing in TCL. Our starting point is the demonstration of a key shortfall of Cohen
and Rowe’s system: its cut-free fragment, here called TCG, is unable to cyclically
prove even standard theorems of relational algebra, e.g. (a ∪b)∗= a∗(ba∗)∗and
This work was supported by a UKRI Future Leaders Fellowship, ‘Structure vs Invari-
ants in Proofs’, project reference MR/S035540/1.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 509–528, 2022.
https://doi.org/10.1007/978-3-031-10769-6_30

510
A. Das and M. Girlando
(aa ∪aba)+ ≤a+((ba+)+ ∪a)) (Theorem 12). An immediate consequence of
this is that cyclic proofs of TCG do not enjoy cut-admissibility (Corollary 13).
On the other hand, these (in)equations are theorems of Kleene Algebra (KA)
[18,19], a decidable theory which admits automation-via-proof-search thanks to
the recent cyclic system of Das and Pous [14].
What is more, TCL is well-known to interpret Propositional Dynamic Logic
(PDL), a modal logic whose modalities are just terms of KA, by a natural exten-
sion of the ‘standard translation’ from (multi)modal logic to ﬁrst-order logic (see,
e.g., [4,5]). Incompleteness of cyclic-TCG for PDL over this translation is inher-
ited from its incompleteness for KA. This is in stark contrast to the situation for
modal logics without ﬁxed points: the standard translation from K (and, indeed,
all logics in the ‘modal cube’) to ﬁrst-order logic actually lifts to cut-free proofs
for a wide range of modal logic systems, cf. [21,22].
A closer inspection of the systems for KA and PDL reveals the stumbling
block to any simulation: these systems implicitly conduct a form of ‘deep infer-
ence’, by essentially reasoning underneath ∃and ∧. Inspired by this observation,
we propose a form of hypersequents for predicate logic, with extra structure
admitting the deep reasoning required. We present the cut-free system HTC and
a novel notion of cyclic proof for these hypersequents. In particular, the incorpo-
ration of some deep inference at the level of the rules necessitates an ‘alternating’
trace condition corresponding to alternation in automata theory.
Our ﬁrst main result is the Soundness Theorem (Theorem 23): non-
wellfounded proofs of HTC are sound for standard semantics. The proof is rather
more involved than usual soundness arguments in cyclic proof theory, due to the
richer structure of hypersequents and the corresponding progress criterion. Our
second main result is the Simulation Theorem (Theorem 28): HTC is complete
for PDL over the standard translation, by simulating a cut-free cyclic system
for the latter. This result can be seen as a formal interpretation of cyclic modal
proof theory within cyclic predicate proof theory, in the spirit of [21,22].
To simplify the exposition, we shall mostly focus on equality-free TCL and
‘identity-free’ PDL in this paper, though all our results hold also for the ‘reﬂexive’
extensions of both logics. We discuss these extensions in Sect. 7, and present
further insights and conclusions in Sect. 8. Full proofs and further examples not
included here (due to space constraints) can be found in [13].
2
Preliminaries
We shall work with a ﬁxed ﬁrst-order vocabulary consisting of a countable set
Pr of unary predicate symbols, written p, q, etc., and of a countable set Rel of
binary relation symbols, written a, b, etc. We shall generally reserve the word
‘predicate’ for unary and ‘relation’ for binary. We could include further relational
symbols too, of higher arity, but choose not to in order to calibrate the semantics
of both our modal and predicate settings.
We build formulas from this language diﬀerently in the modal and predicate
settings, but all our formulas may be formally evaluated within structures:

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
511
Deﬁnition 1 (Structures).
A structure M consists of a set D, called the
domain of M, which we sometimes denote by |M|; a subset pM ⊆D for each
p ∈Pr; and a subset aM ⊆D × D for each a ∈Rel.
2.1
Transitive Closure Logic
In addition to the language introduced at the beginning of this section, in the
predicate setting we further make use of a countable set of function symbols,
written f i, gj, etc. where the superscripts i, j ∈N indicate the arity of the
function symbol and may be omitted when it is not ambiguous. Nullary function
symbols (aka constant symbols), are written c, d etc. We shall also make use of
variables, written x, y, etc., typically bound by quantiﬁers. Terms, written s, t,
etc., are generated as usual from variables and function symbols by function
application. A term is closed if it has no variables.
We consider the usual syntax for ﬁrst-order logic formulas over our language,
with an additional operator for transitive closure (and its dual). Formally, TCL
formulas, written A, B, etc., are generated as follows:
A, B ::= p(t) | ¯p(t) | a(s, t) | ¯a(s, t) | (A ∧B) | (A ∨B) | ∀xA | ∃xA |
TC(λx, y.A)(s, t) | TC(λx, y.A)(s, t)
When variables x, y are clear from context, we may write TC(A(x, y))(s, t) or
TC(A)(s, t) instead of TC(λx, y.A)(s, t), as an abuse of notation, and similarly
for TC. We may write A[t/x] for the formula obtained from A by replacing every
free occurrence of the variable x by the term t. We have included both TC and
TC as primitive operators, so that we can reduce negation to atomic formulas,
shown below. This will eventually allow a one-sided formulation of proofs.
Deﬁnition 2 (Duality). For a formula A we deﬁne its complement, ¯A, by:
p(t) := ¯p(t)
a(s, t) := ¯a(s, t)
¯a(s, t) := a(s, t)
¯p(t) := p(t)
∀xA := ∃x ¯A
∃xA := ∀x ¯A
A ∧B := ¯A ∨¯B
A ∨B := ¯A ∧¯B
TC(A)(s, t) := TC( ¯A)(s, t)
TC(A)(s, t) := TC( ¯A)(s, t)
We shall employ standard logical abbreviations, e.g. A ⊃B for ¯A ∨B.
We may evaluate formulas with respect to a structure, but we need additional
data for interpreting function symbols:
Deﬁnition 3 (Interpreting function symbols). Let M be a structure with
domain D. An interpretation is a map ρ that assigns to each function symbol
f n a function Dn →D. We may extend any interpretation ρ to an action on
(closed) terms by setting recursively ρ(f(t1, . . . , tn)) := ρ(f)(ρ(t1), . . . , ρ(tn)).
We only consider standard semantics in this work: TC (and TC) is always
interpreted as the real transitive closure (and its dual) in a structure, rather
than being axiomatised by some induction (and coinduction) principle.

512
A. Das and M. Girlando
Deﬁnition 4 (Semantics). Given a structure M with domain D and an inter-
pretation ρ, the judgement M, ρ |= A is deﬁned as usual for ﬁrst-order logic with
the following additional clauses for TC and TC:1
– M, ρ |= TC(A(x, y))(s, t) if there are v0, . . . , vn+1 ∈D with ρ(s) = v0, ρ(t) =
vn+1, such that for every i ≤n we have M, ρ |= A(vi, vi+1).
– M, ρ |= TC(A(x, y))(s, t) if for all v0, . . . , vn+1 ∈D with ρ(s) = v0 and
ρ(t) = vn+1, there is some i ≤n such that M, ρ |= A(vi, vi+1).
If M, ρ |= A for all M and ρ, we simply write |= A.
Remark 5 (TC and TC as least and greatest ﬁxed points).
As expected, we
have M, ρ ̸|= TC(A)(s, t) just if M, ρ |= TC( ¯A)(s, t), and so the two operators
are semantically dual. Thus, TC and TC duly correspond to least and greatest
ﬁxed points, respectively, satisfying in any model:
TC(A)(s, t) ⇐⇒A(s, t) ∨∃x(A(s, x) ∧TC(A)(x, t))
(1)
TC(A)(s, t) ⇐⇒A(s, t) ∧∀x(A(s, x) ∨TC(A)(x, t))
(2)
Let us point out that our TC operator is not the same as Cohen and Rowe’s
transitive ‘co-closure’ operator TC op in [10], but rather the De Morgan dual
of TC. In the presence of negation, TC and TC are indeed interdeﬁnable, cf.
Deﬁnition 2.
2.2
Cohen-Rowe Cyclic System for TCL
Cohen and Rowe proposed in [9,11] a non-wellfounded system for TCL that
extends a usual sequent calculus LK= for ﬁrst-order logic with equality and
substitution by rules for TC inspired by its characterisation as a least ﬁxed
point, cf. (1).2 Note that the presence of the substitution rule is critical for the
notion of ‘regularity’ in predicate cyclic proof theory. The resulting notions of
non-wellfounded and cyclic proofs are formulated similarly to those for ﬁrst-order
logic with (ordinary) inductive deﬁnitions [8]:
Deﬁnition 6 (Sequent system). TCG is the extension of LK= by the rules:
Γ, A(s, t)
TC 0 Γ, TC(A)(s, t)
Γ, A(s, r)
Γ, TC(A)(r, t)
TC 1
Γ, TC(A)(s, t)
Γ, A(s, t)
Γ, A(s, c), TC(A)(c, t)
TC
c fresh
Γ, TC(A)(s, t)
(3)
TCG-preproofs are possibly inﬁnite trees of sequents generated by the rules of
TCG. A preproof is regular if it has only ﬁnitely many distinct sub-preproofs.
1 Note that we are including ‘parameters from the model’ in formulas here. Formally,
this means each v ∈D is construed as a constant symbol for which ρ(v) = v.
2 Cohen and Rowe’s system is originally called RTCG, rather using a ‘reﬂexive’ ver-
sion RTC of the TC operator. However this (and its rules) can be encoded (and
simulated) by deﬁning RTC(λx, y.A)(s, t) := TC(λx, y(x = y ∨A))(s, t).

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
513
The notion of ‘correct’ non-wellfounded proof is obtained by a standard pro-
gressing criterion in cyclic proof theory. We shall not go into details here, being
beyond the scope of this work, but refer the reader to those original works (as
well as [13] for our current variant). Let us write ⊢cyc for their notion of cyclic
provability using the above rules, cf. [9,11]. A standard inﬁnite descent counter-
model argument yields:
Proposition 7 (Soundness, [9,11]). If TCG ⊢cyc A then |= A.
In fact, this result is subsumed by our main soundness result for HTC (Theo-
rem 23) and its simulation of TCG (Theorem 19). In the presence of cut, a form
of converse of Proposition 7 holds: cyclic TCG proofs are ‘Henkin complete’,
i.e. complete for all models of a particular axiomatisation of TCL based on
(co)induction principles for TC (and TC) [9,11]. However, the counterexample
we present in the next section implies that cut is not eliminable (Corollary 13).
3
Interlude: Motivation from PDL and Kleene Algebra
Given the TCL sequent system proposed by Cohen and Rowe, why do we propose
a hypersequential system? Our main argument is that proof search in TCG is
rather weak, to the extent that cut-free cyclic proofs are unable to simulate a
basic (cut-free) system for modal logic PDL (regardless of proof search strategy).
At least one motivation here is to ‘lift’ the standard translation from cut-free
cyclic proofs for PDL to cut-free cyclic proofs in an adequate system for TCL.
3.1
Identity-Free PDL
Identity-free propositional dynamic logic (PDL+) is a version of the modal logic
PDL without tests or identity, thereby admitting an ‘equality-free’ standard
translation into predicate logic. Formally, PDL+ formulas, written A, B, etc.,
and programs, written α, β, etc., are generated by the following grammars:
A, B ::= p | p | (A ∧B) | (A ∨B) | [α]A | ⟨α⟩A
α, β ::= a | (α; β) | (α ∪β) | α+
We sometimes simply write αβ instead of α; β, and (α)A for a formula that is
either ⟨α⟩A or [α]A.
Deﬁnition 8 (Duality). For a formula A we deﬁne its complement, ¯A, by:
¯¯p := p
A ∧B := ¯A ∨¯B
A ∨B := ¯A ∧¯B
[α]A := ⟨α⟩¯A
⟨α⟩A := [α] ¯A
We evaluate PDL+ formulas using the traditional relational semantics of
modal logic, by associating each program with a binary relation in a structure.
Again, we only consider standard semantics, in the sense that the + operator is
interpreted as the real transitive closure within a structure.

514
A. Das and M. Girlando
Deﬁnition 9 (Semantics). For structures M with domain D, elements v ∈
D, programs α and formulas A, we deﬁne αM ⊆D × D and the judgement
M, v |= A as follows:
– (aM is already given in the speciﬁcation of M, cf. Deﬁnition 1).
– (α; β)M := {(u, v) : there is w ∈D s.t. (u, w) ∈αM and (w, v) ∈βM}.
– (α ∪β)M := {(u, v) : (u, v) ∈αM or (u, v) ∈βM}.
– (α+)M
:=
{(u, v)
:
there are w0, . . . , wn+1
∈
D s.t. u
=
w0, v
=
wn+1 and, for every i ≤n, (wi, wi+1) ∈αM}.
– M, v |= p if v ∈pM.
– M, v |= p if v /∈pM.
– M, v |= A ∧B if M, v |= A and M, v |= B.
– M, v |= A ∨B if M, v |= A or M, v |= B.
– M, v |= [α]A if ∀(v, w) ∈αM we have M, w |= A.
– M, v |= ⟨α⟩A if ∃(v, w) ∈αM with M, w |= A.
If M, v |= A for all M and v ∈|M|, then we write |= A.
Note that we are overloading the satisfaction symbol |= here, for both PDL+
and TCL. This should never cause confusion, in particular since the two notions
of satisfaction are ‘compatible’ as we shall now see.
3.2
The Standard Translation
The so-called ‘standard translation’ of modal logic into predicate logic is induced
by reading the semantics of modal logic as ﬁrst-order formulas. We now give a
natural extension of this that interprets PDL+ into TCL. At the logical level our
translation coincides with the usual one for basic modal logic; our translation of
programs, as expected, requires the TC operator to interpret the + of PDL+.
Deﬁnition 10. For PDL+ formulas A and programs α, we deﬁne the standard
translations ST(A)(x) and ST(α)(x, y) as TCL-formulas with free variables x
and x, y, resp., inductively as follows:
ST(p)(x) := p(x)
ST(a)(x, y) := a(x, y)
ST(¯p)(x) :=
¯p(x)
ST(α ∪β)(x, y) := ST(α)(x, y) ∨ST(β)(x, y)
ST(A ∨B)(x) := ST(A)(x) ∨ST(B)(x)
ST(α; β)(x, y) := ∃z(ST(α)(x, z) ∧ST(β)(z, y))
ST(A ∧B)(x) := ST(A)(x) ∧ST(B)(x)
ST(α+)(x, y) := TC(ST(α))(x, y)
ST(⟨α⟩A)(x) :=
∃y(ST(α)(x, y) ∧ST(A)(y))
ST([α]A)(x) := ∀y(ST(α)(x, y) ∨ST(A)(y))
where TC(ST(α)) is shorthand for TC(λx, y.ST(α)(x, y)).
It is routine to show that ST(A)(x) = ST( ¯A)(x), by structural induction on
A, justifying our overloading of the notation ¯A, in both TCL and PDL+. Yet
another advantage of using the same underlying language for both the modal and
predicate settings is that we can state the following (expected) result without
the need for encodings, following by a routine structural induction (see, e.g., [5]):
Theorem 11. For PDL+ formulas A, we have M, v |= A iﬀM |= ST(A)(v).

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
515
3.3
Cohen-Rowe System is not Complete for PDL+
PDL+ admits a standard cut-free cyclic proof system LPD+ (see Sect. 6.1) which
is both sound and complete (cf. Theorem 30). However, a shortfall of TCG is that
it is unable to cut-free simulate LPD+. In fact, we can say something stronger:
Theorem 12 (Incompleteness).
There exist a PDL+ formula A such that
|= A but TCG ̸⊢cyc ST(A)(x) (in the absence of cut).
This means not only that TCG is unable to locally cut-free simulate the rules
of LPD+, but also that there are some validities for which there are no cut-free
cyclic proofs at all in TCG. One example of such a formula is:
⟨(aa ∪aba)+⟩p ⊃⟨a+((ba+)+ ∪a)⟩p
(4)
A detailed proof of this is found in [13], but let us brieﬂy discuss it here. First,
the formula above is not artiﬁcial: it is derived from the well-known PDL validity
⟨(a ∪b)∗⟩p ⊃⟨a∗(ba∗)∗⟩p by identity-elimination. This in turn is essentially a
theorem of relational algebra, namely (a ∪b)∗≤a∗(ba∗)∗, which is often used
to eliminate ∪in (sums of) regular expressions. The same equation was (one of
those) used by Das and Pous in [14] to show that the sequent system LKA for
Kleene Algebra is cut-free cyclic incomplete.
The argument that TCG ̸⊢cyc ST(4)(x) is much more involved than the one
from [14], due to the fact we are working in predicate logic, but the underlying
basic idea is similar. At a very high level, the RHS of (4) (viewed as a relational
inequality) is translated to an existential formula ∃z(ST(a+)(x, z)∧ST((ba+)+ ∪
a)(z, y) that, along some branch (namely the one that always chooses aa when
decomposing the LHS of (4)) can never be instantiated while remaining valid.
This branch witnesses the non-regularity of any proof. However ST(4)(x) is cycli-
cally provable in TCG with cut, so an immediate consequence of Theorem 12 is:
Corollary 13. The
class
of
cyclic
proofs
of TCG
does
not
enjoy
cut-
admissibility.
4
Hypersequent Calculus for TCL
Let us take a moment to examine why any ‘local’ simulation of LPD+ by TCG
fails, in order to motivate the main system that we shall present. The program
rules, in particular the ⟨⟩-rules, require a form of deep inference to be correctly
simulated, over the standard translation. For instance, let us consider the action
of the standard translation on two rules we shall see later in LPD+ (cf. Sect. 6.1):
Γ, ⟨a0⟩p
⟨∪⟩0 Γ, ⟨a0 ∪a1⟩p
⇝
ST(Γ)(c), ∃x(a0(c, x) ∧p(x))
ST(Γ)(c), ∃x((a0(c, x) ∨a1(c, x)) ∧p(x))
Γ, ⟨a⟩⟨b⟩p
⟨;⟩
Γ, ⟨a; b⟩p
⇝
ST(Γ)(c), ∃y(a(c, y) ∧∃x(b(y, x) ∧p(x)))
ST(Γ)(c), ∃x(∃y(a(c, y) ∧b(y, x)) ∧p(x))

516
A. Das and M. Girlando
Fig. 1. Hypersequent calculus HTC. σ is a ‘substitution’ map from constants to terms
and a renaming of other function symbols and variables.
The ﬁrst case above suggests that any system to which the standard translation
lifts must be able to reason underneath ∃and ∧, so that the inference indicated
in blue is ‘accessible’ to the prover. The second case above suggests that the
existential-conjunctive meta-structure necessitated by the ﬁrst case should admit
basic equivalences, in particular certain prenexing. This section is devoted to the
incorporation of these ideas (and necessities) into a bona ﬁde proof system.
4.1
A System for Predicate Logic via Annotated Hypersequents
An annotated cedent, or simply cedent, written S, S′ etc., is an expression {Γ}x,
where Γ is a set of formulas and the annotation x is a set of variables. We
sometimes construe annotations as lists rather than sets when it is convenient,
e.g. when taking them as inputs to a function.
Each cedent may be intuitively read as a TCL formula, under the following
interpretation: fm({Γ}x1,...,xn) := ∃x1 . . . ∃xn
 Γ. When x = ∅then there are
no existential quantiﬁers above, and when Γ = ∅we simply identify  Γ with
⊤. We also sometimes write simply A for the annotated cedent {A}∅.
A hypersequent, written S, S′ etc., is a set of annotated cedents. Each hyper-
sequent may be intuitively read as the disjunction of its cedents. Namely we set:
fm({Γ1}x1, . . . , {Γn}xn) := fm({Γ1}x1) ∨. . . ∨fm({Γn}xn).
Deﬁnition 14 (System). The rules of HTC are given in Fig. 1. A HTC pre-
proof is a (possibly inﬁnite) derivation tree generated by the rules of HTC. A
preproof is regular if it has only ﬁnitely many distinct subproofs.
Our hypersequential system is somewhat more reﬁned than usual sequent
systems for predicate logic. E.g., the usual ∃rule is decomposed into ∃and inst,

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
517
whereas the usual ∧rule is decomposed into ∧and ∪. The rules for TC and TC
are induced directly from their characterisations as ﬁxed points in (1).
Note that the rules TC and ∀introduce, bottom-up, the fresh function sym-
bol f, which plays the role of the Herbrand function of the corresponding ∀
quantiﬁer: just as ∀x∃xA(x) is equisatisﬁable with ∀xA(f(x)), when f is fresh,
by Skolemisation, by duality ∃x∀xA(x) is equivalid with ∃xA(f(x)), when f is
fresh, by Herbrandisation. The usual ∀rule of the sequent calculus corresponds
to the case when x = ∅.
4.2
Non-wellfounded Hypersequent Proofs
Our notion of ancestry, as compared to traditional sequent systems, must account
for the richer structure of hypersequents:
Deﬁnition 15 (Ancestry).
Fix an inference step r, as typeset in Fig. 1. A
formula C in the premiss is an immediate ancestor of a formula C′ in the
conclusion if they have the same colour; if C, C′ ∈Γ then we further require
C = C′, and if C, C′ occur in S then C = C′ occur in the same cedent. A cedent
S in the premiss is an immediate ancestor of a cedent S′ in the conclusion if
some formula in S is an immediate ancestor of some formula in S′.
Immediate ancestry on both formulas and cedents is a binary relation, induc-
ing a directed graph whose paths form the basis of our correctness condition:
Deﬁnition 16 ((Hyper)traces). A hypertrace is a maximal path in the graph
of immediate ancestry on cedents. A trace is a maximal path in the graph of
immediate ancestry on formulas.
Deﬁnition 17 (Progress and proofs). Fix a preproof D. A (inﬁnite) trace
(Fi)i∈ω is progressing if there is k such that, for all i > k, Fi has the form
TC(A)(si, ti) and is inﬁnitely often principal.3 A (inﬁnite) hypertrace H is pro-
gressing if every inﬁnite trace within it is progressing. A (inﬁnite) branch is pro-
gressing if it has a progressing hypertrace. D is a proof if every inﬁnite branch
is progressing. If, furthermore, D is regular, we call it a cyclic proof.
We write HTC ⊢nwf S (or HTC ⊢cyc S) if there is a proof (or cyclic proof,
respectively) of HTC of the hypersequent S.
In usual cyclic systems, checking that a regular preproof is progressing is
decidable by straightforward reduction to the universality of nondeterministic
ω-automata, with runs ‘guessing’ a progressing trace along an inﬁnite branch.
Our notion of progress exhibits an extra quantiﬁer alternation: we must guess an
inﬁnite hypertrace in which every trace is progressing. Nonetheless, by appealing
to determinisation or alternation, we can still decide our progressing condition:
Proposition 18. Checking whether a HTC preproof is a proof is decidable by
reduction to universality of ω-regular languages.
3 In fact, by a simple well-foundedness argument, it is equivalent to say that (Fi)i<ω
is progressing if it is inﬁnitely often principal for a TC-formula.

518
A. Das and M. Girlando
As we mentioned earlier, cyclic proofs of HTC indeed are at least as expressive
as those of Cohen and Rowe’s system by a routine local simulation of rules:
Theorem 19 (Simulating Cohen-Rowe). If TCG ⊢cyc A then HTC ⊢cyc A.
4.3
Some Examples
Example 20 (Fixed point identity). The sequent {TC(a)(c, d)}∅, {TC(¯a)(c, d)}∅
is ﬁnitely derivable using rule id on TC(a)(c, d) and the init rule. However we
can also cyclically reduce it to a simpler instance of id. Due to the granularity of
the inference rules of HTC, we actually have some liberty in how we implement
such a derivation. E.g., the HTC-proof below applies TC rules below TC ones,
and delays branching until the ‘end’ of proof search, which is impossible in TCG.
The only inﬁnite branch, looping on •, is progressing by the blue hypertrace.
init
{ }∅
id
{a(c, d)}∅, {¯a(c, d)}∅
init
{ }∅
id
{a(c, e)}∅, {¯a(c, e)}∅
...
TC
•
{TC(a)(e, d)}∅, {TC(¯a)(e, d)}∅
∪
{a(c, e)}∅, {TC(a)(e, d)}∅, {¯a(c, e), TC(¯a)(e, d)}∅
2∪
{a(c, d), a(c, e)}∅, {a(c, d), TC(a)(e, d)}∅, {¯a(c, d)}∅, {¯a(c, e), TC(¯a)(e, d)}∅
inst
{a(c, d), a(c, e)}∅, {a(c, d), TC(a)(e, d)}∅, {¯a(c, d)}∅, {¯a(c, x), TC(¯a)(x, d)}x
TC
{TC(a)(c, d)}∅, {¯a(c, d)}∅, {¯a(c, x), TC(¯a)(x, d)}x
TC
•
{TC(a)(c, d)}∅, {TC(¯a)(c, d)}∅
This is an example of the more general ‘rule permutations’ available in HTC,
hinting at a more ﬂexible proof theory (we discuss this further in Sect. 8).
Example 21 (Transitivity). TC can be proved transitive by way of a cyclic proof
in TCG of the sequent TC(a)(c, d), TC(a)(d, e), TC(¯a)(c, e). As in the previous
example we may mimic that proof line by line, but we give a slightly diﬀerent
one that cannot directly be interpreted as a TCG proof:
init
{ }∅
id
a(c, d), ¯a(c, d)
Ex. 20
TC(¯a)(d, e), TC(a)(d, e)
∪
a(c, d), TC(a)(d, e), ¯a(c, d), TC(¯a)(d, e)
inst
a(c, d), TC(a)(d, e), {¯a(c, x), TC(¯a)(x, e)}x
init
{ }∅
id
a(c, c′), ¯a(c, c′)
...
TC
◦
TC(a)(c′, d), TC(a)(d, e), TC(¯a)(c′, e)
∪
a(c, c′), TC(a)(c′, d), TC(a)(d, e), ¯a(c, e), ¯a(c, c′), TC(¯a)(c′, e)
inst
a(c, c′), TC(a)(c′, d), TC(a)(d, e), ¯a(c, e), {¯a(c, x), TC(¯a)(x, e)}x
2∪
a(c, d), a(c, c′), a(c, d), TC(a)(c′, d), TC(a)(d, e), ¯a(c, e), {¯a(c, x), TC(¯a)(x, e)}x
TC
TC(a)(c, d), TC(a)(d, e), ¯a(c, e), {¯a(c, x), TC(¯a)(x, e)}x
TC
◦
TC(a)(c, d), TC(a)(d, e), TC(¯a)(c, e)
The only inﬁnite branch (except for that from Example 20), looping on ◦, is
progressing by the red hypertrace.
Finally, it is pertinent to revisit the ‘counterexample’ (4) that witnessed
incompleteness of TCG for PDL+. The following result is, in fact, already implied
by our later completeness result, Theorem 28, but we shall present it nonetheless:
Proposition 22. HTC ⊢cyc ST((aa ∪aba)+)(c, d) ⊃ST(a+((ba+)+ ∪a))(c, d).

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
519
Proof. We give the required cyclic proof in Fig. 2, using the abbreviations:
α(c, d) = ST(aa∪aba)(c, d) and β(c, d) = ST((ba+)+ ∪a)(c, d). The only inﬁnite
branch (looping on •) has progressing hypertrace is marked in blue.
Hypersequents R = {α(c, d)}∅, {α(c, d), TC(α)(e, d)}∅, {TC(a)(c, y), β(y, d)}y
and R′ = {α(c, d)}∅, {α(c, d)}∅, {TC(a)(c, y), β(y, d)}y have ﬁnitary proofs,
while P = {aba(c, e)}∅, {TC(α)(e, d)}∅, {TC(a)(c, y), β(y, d)}y has a cyclic
proof.
Fig. 2. Cyclic proof for sequent not cyclically provable by TCG.
5
Soundness of HTC
This section is devoted to the proof of the ﬁrst of our main results:
Theorem 23 (Soundness). If HTC ⊢nwf S then |= S.
The argument is quite technical due to the alternating nature of our progress
condition. In particular the treatment of traces within hypertraces requires a
more ﬁne grained argument than usual, bespoke to our hypersequential structure.
Throughout this section, we shall ﬁx a HTC preproof D of a hypersequent S.
For practical reasons we shall assume that D is substitution-free (at the cost of
regularity) and that each quantiﬁer in S binds a distinct variable.4 We further
assume some structure M× and an interpretation ρ0 such that ρ0 ̸|= S (within
M×). Since each rule is locally sound, by contraposition we can continually
choose ‘false premisses’ to construct an inﬁnite ‘false branch’:
Lemma 24 (Countermodel branch). There is a branch B× = (Si)i<ω of D
and an interpretation ρ× such that, with respect to M×:
4 Note that this convention means we can simply take y = x in the ∃rule in Fig. 1.

520
A. Das and M. Girlando
1. ρ× ̸|= Si, for all i < ω;
2. Suppose that Si concludes a TC step, as typeset in Fig. 1, and ρ× |=
TC( ¯A)(s, t) [d/x]. If n is minimal such that ρ× |= ¯A(di, di+1) for all i ≤n,
ρ×(s) = d0 and ρ×(t) = dn, and n > 1, then ρ×(f)(d) = d15 so that
ρi+1 |= ¯A(s, f(x))[d/x] and ρ× |= TC( ¯A)(f(x), t)[d/x].
Unpacking this a little, our interpretation ρ× is actually deﬁned as the limit of a
chain of ‘partial’ interpretations (ρi)i<ω, with each ρi ̸|= Si (within M×). Note
in particular that, by 2, whenever some TC-formula is principal, we choose ρi+1
to always assign to it a falsifying path of minimal length (if one exists at all),
with respect to the assignment to variables in its annotation. It is crucial at this
point that our deﬁnition of ρ× is parametrised by such assignments.
Let us now ﬁx B× and ρ× as provided by the Lemma above. Moreover, let us
henceforth assume that D is a proof, i.e. it is progressing, and ﬁx a progressing
hypertrace H = ({Γi}xi)i<ω along B×. In order to carry out an inﬁnite descent
argument, we will need to deﬁne a particular trace along this hypertrace that
‘preserves’ falsity, bottom-up. This is delicate since the truth values of formulas
in a trace depend on the assignment of elements to variables in the annotations.
A particular issue here is the instantiation rule inst, which requires us to ‘revise’
whatever assignment of y we may have deﬁned until that point. Thankfully, our
earlier convention on substitution-freeness and uniqueness of bound variables in
D facilitates the convergence of this process to a canonical such assignment:
Deﬁnition 25 (Assignment).
We deﬁne δH : 
i<ω
xi →|M×| by δH(x) :=
ρ(t) if x is instantiated by t in H; otherwise δH(x) is some arbitrary d ∈|M×|.
Note that δH is indeed well-deﬁned, thanks to the convention that each quan-
tiﬁer in S binds a distinct variable. In particular we have that each variable x is
instantiated at most once along a hypertrace. Henceforth we shall simply write
ρ, δH |= A(x) instead of ρ |= A(δH(x)). Working with such an assignment ensures
that false formulas along H always have a false immediate ancestor:
Lemma 26 (Falsity through H).
If ρ×, δH ̸|= F for some F ∈Γi, then F
has an immediate ancestor F ′ ∈Γi+1 with ρ×, δH ̸|= F ′.
In particular, regarding the inst rule of Fig. 1, note that if F ∈Γ(y) then we
can choose F ′ = F[t/y] which, by deﬁnition of δH, has the same truth value. By
repeatedly applying this Lemma we obtain:
Proposition 27 (False trace).
There exists an inﬁnite trace τ × = (Fi)i<ω
through H such that, for all i, it holds that M×, ρ×, δH ̸|= Fi.
We are now ready to prove our main soundness result.
Proof (of Theorem 23, sketch). Fix the inﬁnite trace τ × = (Fi)i<ω through H
obtained by Proposition 27. Since τ × is inﬁnite, by deﬁnition of HTC proofs, it
5 To be clear, we here choose an arbitrary such minimal ‘ ¯A-path’.

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
521
needs to be progressing, i.e., it is inﬁnitely often TC-principal and there is some
k ∈N s.t. for i > k we have that Fi = TC(A)(si, ti) for some terms si, ti.
To each Fi, for i > k, we associate the natural number ni measuring the
‘ ¯A-distance between si and ti’. Formally, ni ∈N is least such that there
are d0, . . . , dni ∈|M×| with ρ×(s) = d0, ρ×(t) = dni and, for all i < ni,
ρ×, δH |= ¯A(di, di+1). Our aim is to show that (ni)i>k has no minimal element,
contradicting wellfoundness of N. For this, we establish the following two local
properties:
Fig. 3. Rules of LPD+.
1. (ni)i>k is monotone decreasing, i.e., for all i > k, we have ni+1 ≤ni;
2. Whenever Fi is principal, we have ni+1 < ni.
So (ni)i>k is monotone decreasing, by 1, but cannot converge, by 2 and the
deﬁnition of progressing trace. Thus (ni)k<i has no minimal element, yielding
the required contradiction.
6
HTC is Complete for PDL+, Over Standard Translation
In this section we give our next main result:
Theorem 28 (Completeness for PDL+).
For a PDL+ formula A, if |= A
then HTC ⊢cyc ST(A)(c).
The proof is by a direct simulation of a cut-free cyclic system for PDL+ that is
complete. We shall brieﬂy sketch this system below.
6.1
Circular System for PDL+
The system LPD+, given in Fig. 3, is the natural extension of the usual sequent
calculus for basic multimodal logic K by rules for programs. In Fig. 3, ⟨a⟩Γ is
shorthand for {⟨a⟩B : B ∈Γ}. (Regular) preproofs for this system are deﬁned
just like for HTC or TCG. The notion of ‘immediate ancestor’ is induced by the
indicated colouring: a formula C in a premiss is an immediate ancestor of a
formula C′ in the conclusion if they have the same colour; if C, C′ ∈Γ then we
furthermore require C = C′.

522
A. Das and M. Girlando
Deﬁnition 29 (Non-wellfounded proofs). Fix a preproof D of a sequent Γ.
A thread is a maximal path in its graph of immediate ancestry. We say a thread
is progressing if it has a smallest inﬁnitely often principal formula of the form
[α+]A. D is a proof if every inﬁnite branch has a progressing thread. If D is
regular, we call it a cyclic proof and we may write LPD+ ⊢cyc Γ.
Soundness of cyclic-LPD+ is established by a standard inﬁnite descent argu-
ment, but is also implied by the soundness of cyclic-HTC (Theorem 23) and the
simulation we are about to give (Theorem 28), though this is somewhat overkill.
Completeness may be established by the game theoretic approach of Niwinsk´ı
and Walukiewicz [23], as done by Lange [20] for PDL (with identity), or by purely
proof theoretic techniques of Studer [25]. Either way, both results follow from
a standard embedding of PDL+ into the μ-calculus and its known complete-
ness results [23,25], by way of a standard ‘proof reﬂection’ argument: μ-calculus
proofs of the embedding are ‘just’ step-wise embeddings of LPD+ proofs:
Theorem 30 (Soundness and completeness, [20]). Let A be a PDL+ for-
mula. |= A iﬀLPD+ ⊢cyc A.
6.2
A ‘Local’ Simulation of LPD+ by HTC
In this subsection we show that LPD+-preproofs can be stepwise transformed
into HTC-proofs, with respect to the standard translation. In order to produce
this local simulation, we need a more reﬁned version of the standard translation
that incorporates the structural elements of hypersequents.
Fix a PDL+ formula A = [α1] . . . [αn]⟨β1⟩. . . ⟨βm⟩B, for n, m ≥0. The hyper-
sequent translation of A, written HT(A)(c), is deﬁned as:
{ST(α1)(c, d1)}∅, {ST(α2)(d1, d2)}∅, . . . , {ST(αn)(dn−1, dn)}∅,
{ST(β1)(dn, y1), ST(β2)(y2, y3), . . . , ST(βm)(ym−1, ym), ST(B)(ym)}y1,...,ym
For Γ = A1, . . . , Ak, we write HT(Γ)(c) := HT(A1)(c), . . . , HT(Ak)(c).
Deﬁnition 31 (HT-translation). Let D be a PDL+ preproof. We shall deﬁne
a HTC preproof HT(D)(c) of the hypersequent HT(A)(c) by a local translation
of inference steps. We give only a few of the important cases here, but a full
deﬁnition can be found in [13].
– A step
B1, . . . , Bk, A
ka ⟨a⟩B1, . . . , ⟨a⟩Bk, [a]A is translated to:
HT(B1)(c), . . . , HT(Bk)(c), HT(A)(c)
[d/c]
HT(B1)(d), . . . , HT(Bk)(d), HT(A)(d)
∨,∀
{CT(B1)(d)}xB1 , . . . , {CT(Bk)(d)}xBk , HT(A)(d)
wk
{CT(B1)(d)}xB1 , . . . , {CT(Bk)(d)}xBk , {ST(a)(c, d)}∅, HT(A)(d)
∪
{CT(B1)(d)}xB1 , . . . , {ST(a)(c, d), CT(Bk)(d)}xBk , {ST(a)(c, d)}∅, HT(A)(d)
inst
{ST(a)(c, y), CT(B1)(y)}xB1,y, . . . , {ST(a)(c, y), CT(Bk)(y)}xBk ,y, {ST(a)(c, d)}∅, HT(A)(d)
= ..............................................................................................................................................................................................
HT(⟨a⟩B1)(c), . . . , HT(⟨a⟩Bk)(c), HT([a]A)(c)

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
523
where (omitted) left-premisses of ∪steps are simply proved by wk, id, init. In this
and the following cases, we use the notation CT(A)(c) and xA for the appropriate
sets of formulas and variables forced by the deﬁnition of HT (again, see [13] for
further details).
– A ⟨∪⟩i step (for i = 0, 1), as typeset in Fig. 3, is translated to:
HT(Γ )(c), HT(⟨αi⟩A)(c)
= ..................................................................................
HT(Γ )(c), {ST(αi)(c, y), CT(A)(y)}xB,y
∨
HT(Γ )(c), {ST(α0)(c, y) ∨ST(α1)(c, y), CT(A)(y)}xA,y
= .................................................................................................................
HT(Γ )(c), HT(⟨α0 ∪α1⟩A)(c)
– A ⟨; ⟩step, as typeset in Fig. 3, is translated to:
HT(Γ )(c), HT(⟨α⟩⟨β⟩A)(c)
= .............................................................................................................
HT(Γ )(c), {ST(α)(c, z), ST(α)(z, y), CT(A)(y)}xA,y,z
∧
HT(Γ )(c), {ST(α)(c, z) ∧ST(α)(z, y), CT(A)(y)}xA,y,z
∃
HT(Γ )(c), {∃z(ST(α)(c, z) ∧ST(α)(z, y)), CT(A)(y)}xA,y
= ......................................................................................................................
HT(Γ )(c), HT(⟨α; β⟩A)(c)
– A [+] step, as typeset in Fig. 3, is translated to:
E
E′
HT(Γ )(c), HT([α][α+]A)(c)
= ........................................................................................................................
HT(Γ )(c), {ST(α)(c, f)}∅, {TC(ST(α))(f, d)}∅, HT(A)(d)
∪
HT(Γ )(c), {ST(α)(c, f)}∅, {ST(α)(c, d), TC(ST(α))(f, d)}∅, HT(A)(d)
∪
HT(Γ )(c), {ST(α)(c, d), ST(α)(c, f)}∅, {ST(α)(c, d), TC(ST(α))(f, d)}∅, HT(A)(d)
TC
HT(Γ )(c), {TC(ST(α))(c, d)}∅, HT(A)(d)
= ......................................................................................
HT(Γ )(c), HT([α+]A)(c)
where E and E′ derive HT(Γ)(c) and HT([α]A)(c), resp., using wk-steps.
Note that, formally speaking, the well-deﬁnedness of HT(D)(c) in the deﬁ-
nition above is guaranteed by coinduction: each rule of D is translated into a
(nonempty) derivation.
Remark 32 (Deeper inference). Observe that HTC can also simulate ‘deeper’
program rules than are available in LPD+. E.g. a rule
Γ, ⟨α⟩⟨βi⟩A
Γ, ⟨α⟩⟨β0 ∪β1⟩A may be
simulated too (similarly for [ ]). E.g. ⟨a+⟩⟨b⟩p ⊃⟨a+⟩⟨b ∪c⟩p admits a ﬁnite
proof in HTC (under ST), rather than a necessarily inﬁnite (but cyclic) one in
LPD+.
6.3
Justifying Regularity and Progress
Proposition 33. If D is regular, then so is HT(D)(c).
Proof. Notice that each rule in D is translated to a ﬁnite derivation in HT(D)(c).
Thus, if D has only ﬁnitely many distinct subproofs, then also HT(D)(c) has only
ﬁnitely many distinct subproofs.

524
A. Das and M. Girlando
Proposition 34. If D is progressing, then so is HT(D)(c).
Proof (sketch). We need to show that every inﬁnite branch of HT(D)(c) has
a progressing hypertrace. Since the HT translation is deﬁned stepwise on the
individual steps of D, we can associate to each inﬁnite branch B of HT(D)(c)
a unique inﬁnite branch B′ of D. Since D is progressing, let τ = (Fi)i<ω be a
progressing thread along B′. By inspecting the rules of LPD+ (and by deﬁni-
tion of progressing thread), for some k ∈N, each Fi for i > k has the form:
[αi,1] · · · [αi,ni][α+]A, for some ni ≥0. So, for i > k, HT(Fi)(di) has the form:
{ST(αi,1)(c, di,1)}∅, . . . , {ST(αi,ni)(di,ni−1, di,ni)}∅, {TC(ST(α))(di,ni, di)}∅, HT(A)(di)
By
inspection
of
the
HT-translation
(Deﬁnition
31)
whenever
Fi+1
is
an immediate ancestor of Fi
in B′, there is a path from the cedent
{TC(ST(α))(di+1,ni+1, di+1)}∅to the cedent {TC(ST(α))(di,ni, di)}∅in the
graph of immediate ancestry along B. Thus, since τ
=
(Fi)i<ω
is a
trace
along
B′,
we
have
a
(inﬁnite)
hypertrace
of
the
form
Hτ
:=
({Δi, TC(ST(α))(di,ni, di)}∅)i>k′ along B. By construction Δi = ∅for inﬁnitely
many i > k′, and so Hτ has just one inﬁnite trace. Moreover, by inspection of
the [+] step in Deﬁnition 31, this trace progresses in B every time τ does in B′,
and so progresses inﬁnitely often. Thus, H is a progressing hypertrace. Since the
choice of the branch B of D was arbitrary, we are done.
6.4
Putting it all Together
We can now ﬁnally conclude our main simulation theorem:
Proof (of Theorem 28, sketch). Let A be a PDL+ formula s.t. |= A. By the
completeness result for LPD+, Theorem 30, we have that LPD+ ⊢cyc A, say by
a cyclic proof D. From here we construct the HTC preproof HT(D)(c) which, by
Propositions 33 and 34, is in fact a cyclic proof of HT(A)(c). Finally, we apply
some basic ∨, ∧, ∃, ∀steps to obtain a cyclic HTC proof of ST(A)(c).
7
Extension by Equality and Simulating Full PDL
We now brieﬂy explain how our main results are extended to the ‘reﬂexive’
version of TCL. The language of HTC= allows further atomic formulas of the
form s = t and s ̸= t. The calculus HTC= extends HTC by the rules:
S, {Γ}x
=
S, {t = t, Γ}x
S, {Γ(s), Δ(s)}x
̸=
S, {Γ(s), s ̸= t}x, {Δ(t)}x
The notion of immediate ancestry is colour-coded as in Deﬁnition 15, and
the resulting notions of (pre)proof, (hyper)trace and progress are as in Def-
inition 17. The simulation of Cohen and Rowe’s system TCG extends to

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
525
their reﬂexive system, RTCG, by deﬁning their operator RTC(λx, y.A)(s, t) :=
TC(λx, y.(x = y ∨A))(s, t). Note that, while it is semantically correct to set
RTC(A)(s, t) to be s = t∨TC(A)(s, t), this encoding does not lift to the Cohen-
Rowe rules for RTC. Understanding that structures interpret = as true equality,
a modular adaptation of the soundness argument for HTC, cf. Sect. 5, yields:
Theorem 35 (Soundness of HTC=). If HTC= ⊢nwf S then |= S.
Turning to the modal setting, PDL may be deﬁned as the extension of PDL+
by including a program A? for each formula A. Semantically, we have (A?)M =
{(v, v) : M, v |= A}. From here we may deﬁne ε := ⊤? and α∗:= (ε∪α)+; again,
while it is semantically correct to set α∗= ε ∪α+, this encoding does not lift
to the standard sequent rules for ∗. The system LPD is obtained from LPD+ by
including the rules:
Γ, A
Γ, B
⟨?⟩
Γ, ⟨A?⟩B
Γ, ¯A, B
[?]
Γ, [A?]B
Again, the notion of immediate ancestry is colour-coded as for LPD+; the result-
ing notions of (pre)proof, thread and progress are as in Deﬁnition 29. Just like
for LPD+, a standard encoding of LPD into the μ-calculus yields its soundness
and completeness, thanks to known sequent systems for the latter, cf. [23,25],
but has also been established independently [20]. Again, a modular adaptation
of the simulation of LPD+ by HTC, cf. Sect. 6, yields:
Theorem 36 (Completeness for PDL).
Let A be a PDL formula. If |= A
then HTC= ⊢cyc ST(A)(c).
8
Conclusions
In this work we proposed a novel cyclic system HTC for Transitive Closure
Logic (TCL) based on a form of hypersequents. We showed a soundness theorem
for standard semantics, requiring an argument bespoke to our hypersequents.
Our system is cut-free, rendering it suitable for automated reasoning via proof
search. We showcased its expressivity by demonstrating completeness for PDL,
over the standard translation. In particular, we demonstrated formally that such
expressivity is not available in the previously proposed system TCG of Cohen and
Rowe (Theorem 12). Our system HTC locally simulates TCG too (Theorem 19).
As far as we know, HTC is the ﬁrst cyclic system employing a form of deep
inference resembling alternation in automata theory, e.g. wrt. proof checking,
cf. Proposition 18. It would be interesting to investigate the structural proof the-
ory that emerges from our notion of hypersequent. As hinted at in Examples 20
and 21, our hypersequential system exhibits more liberal rule permutations than
usual sequents, so we expect their focussing and cut-elimination behaviours to
similarly be richer, cf. [21,22]. Note however that such investigations are rather
pertinent for pure predicate logic (without TC): focussing and cut-elimination
arguments do not typically preserve regularity of non-wellfounded proofs, cf. [2].

526
A. Das and M. Girlando
Finally, our work bridges the cyclic proof theories of (identity-free) PDL and
(reﬂexive) TCL. With increasing interest in both modal and predicate cyclic
proof theory, it would be interesting to further develop such correspondences.
Acknowledgements. The authors would like to thank Sonia Marin, Jan Rooduijn
and Reuben Rowe for helpful discussions on matters surrounding this work.
References
1. Avron, A.: Transitive closure and the mechanization of mathematics. In: Kamared-
dine, F.D. (eds) Thirty Five Years of Automating Mathematics. Applied Logic
Series, vol. 28, pp. 149–171. Springer, Dordrecht (2003). https://doi.org/10.1007/
978-94-017-0253-9 7
2. Baelde, D., Doumane, A., Saurin, A.: Inﬁnitary proof theory: the multiplicative
additive case. In: Talbot, J., Regnier, L. (eds.) 25th EACSL Annual Conference
on Computer Science Logic, CSL 2016, 29 August–1 September 2016, Marseille,
France. LIPIcs, vol. 62, pp. 42:1–42:17. Schloss Dagstuhl - Leibniz-Zentrum f¨ur
Informatik (2016). https://doi.org/10.4230/LIPIcs.CSL.2016.42
3. Berardi, S., Tatsuta, M.: Classical system of martin-lof’s inductive deﬁnitions is
not equivalent to cyclic proofs. CoRR abs/1712.09603 (2017). http://arxiv.org/
abs/1712.09603
4. Blackburn, P., van Benthem, J.: Modal logic: a semantic perspective. In: Blackburn,
P., van Benthem, J.F.A.K., Wolter, F. (eds.) Handbook of Modal Logic, Studies
in Logic and Practical Reasoning, vol. 3, pp. 1–84. North-Holland (2007). https://
doi.org/10.1016/s1570-2464(07)80004-8
5. Blackburn, P., De Rijke, M., Venema, Y.: Modal Logic, vol. 53. Cambridge Uni-
versity Press (2002)
6. Brotherston, J., Distefano, D., Petersen, R.L.: Automated cyclic entailment proofs
in separation logic. In: Bjørner, N., Sofronie-Stokkermans, V. (eds.) CADE 2011.
LNCS (LNAI), vol. 6803, pp. 131–146. Springer, Heidelberg (2011). https://doi.
org/10.1007/978-3-642-22438-6 12
7. Brotherston, J., Gorogiannis, N., Petersen, R.L.: A generic cyclic theorem prover.
In: Jhala, R., Igarashi, A. (eds.) APLAS 2012. LNCS, vol. 7705, pp. 350–367.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-35182-2 25
8. Brotherston, J., Simpson, A.: Sequent calculi for induction and inﬁnite descent. J.
Log. Comput. 21(6), 1177–1216 (2011)
9. Cohen, L., Rowe, R.N.S.: Uniform inductive reasoning in transitive closure logic via
inﬁnite descent. In: Ghica, D.R., Jung, A. (eds.) 27th EACSL Annual Conference
on Computer Science Logic, CSL 2018, 4–7 September 2018, Birmingham, UK.
LIPIcs, vol. 119, pp. 17:1–17:16. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik
(2018). https://doi.org/10.4230/LIPIcs.CSL.2018.17
10. Cohen, L., Rowe, R.N.S.: Integrating induction and coinduction via closure opera-
tors and proof cycles. In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020.
LNCS (LNAI), vol. 12166, pp. 375–394. Springer, Cham (2020). https://doi.org/
10.1007/978-3-030-51074-9 21
11. Cohen, L., Rowe, R.N.: Non-well-founded proof theory of transitive closure logic.
ACM Trans. Comput. Log. 21(4), 1–31 (2020)
12. Das, A.: On the logical complexity of cyclic arithmetic. Log. Methods Comput.
Sci. 16(1) (2020). https://doi.org/10.23638/LMCS-16(1:1)2020

Cyclic Proofs, Hypersequents, and Transitive Closure Logic
527
13. Das, A., Girlando, M.: Cyclic proofs, hypersequents, and transitive closure logic
(2022). https://doi.org/10.48550/ARXIV.2205.08616
14. Das, A., Pous, D.: A cut-free cyclic proof system for Kleene algebra. In: Schmidt,
R.A., Nalon, C. (eds.) TABLEAUX 2017. LNCS (LNAI), vol. 10501, pp. 261–277.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-66902-1 16
15. Gr¨adel, E.: On transitive closure logic. In: B¨orger, E., J¨ager, G., Kleine B¨uning, H.,
Richter, M.M. (eds.) CSL 1991. LNCS, vol. 626, pp. 149–163. Springer, Heidelberg
(1992). https://doi.org/10.1007/BFb0023764
16. Gurevich, Y.: Logic and the Challenge of Computer Science, pp. 1–57. Computer
Science
Press
(1988).
https://www.microsoft.com/en-us/research/publication/
logic-challenge-computer-science/
17. Immerman, N.: Languages that capture complexity classes. SIAM J. Comput.
16(4), 760–778 (1987). https://doi.org/10.1137/0216051
18. Kozen, D.: A completeness theorem for Kleene algebras and the algebra of regular
events. In: Proceedings of the Sixth Annual Symposium on Logic in Computer
Science (LICS 1991), Amsterdam, The Netherlands, 15–18 July 1991, pp. 214–
225. IEEE Computer Society (1991). https://doi.org/10.1109/LICS.1991.151646
19. Krob, D.: Complete systems of b-rational identities. Theor. Comput. Sci. 89(2),
207–343 (1991). https://doi.org/10.1016/0304-3975(91)90395-I
20. Lange, M.: Games for modal and temporal logics. Ph.D. thesis (2003)
21. Marin, S., Miller, D., Volpe, M.: A focused framework for emulating modal proof
systems. In: Beklemishev, L.D., Demri, S., Mat´e, A. (eds.) Advances in Modal
Logic 11, Proceedings of the 11th Conference on “Advances in Modal Logic,” held
in Budapest, Hungary, 30 August–2 September 2016, pp. 469–488. College Publi-
cations (2016). http://www.aiml.net/volumes/volume11/Marin-Miller-Volpe.pdf
22. Miller, D., Volpe, M.: Focused labeled proof systems for modal logic. In: Davis, M.,
Fehnker, A., McIver, A., Voronkov, A. (eds.) LPAR 2015. LNCS, vol. 9450, pp. 266–
280. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48899-7 19
23. Niwi´nski, D., Walukiewicz, I.: Games for the mu-calculus. Theor. Comput. Sci.
163(1), 99–116 (1996). https://doi.org/10.1016/0304-3975(95)00136-0
24. Rowe, R.N.S., Brotherston, J.: Automatic cyclic termination proofs for recursive
procedures in separation logic. In: Bertot, Y., Vafeiadis, V. (eds.) Proceedings of
the 6th ACM SIGPLAN Conference on Certiﬁed Programs and Proofs, CPP 2017,
Paris, France, 16–17 January 2017, pp. 53–65. ACM (2017). https://doi.org/10.
1145/3018610.3018623
25. Studer, T.: On the proof theory of the modal mu-calculus. Stud. Logica. 89(3),
343–363 (2008)
26. Tellez, G., Brotherston, J.: Automatically verifying temporal properties of pointer
programs with cyclic proof. In: de Moura, L. (ed.) CADE 2017. LNCS (LNAI),
vol. 10395, pp. 491–508. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-63046-5 30
27. Tellez, G., Brotherston, J.: Automatically verifying temporal properties of pointer
programs with cyclic proof. J. Autom. Reason. 64(3), 555–578 (2020). https://doi.
org/10.1007/s10817-019-09532-0

528
A. Das and M. Girlando
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Equational Uniﬁcation and Matching,
and Symbolic Reachability Analysis
in Maude 3.2 (System Description)
Francisco Dur´an1
, Steven Eker2
, Santiago Escobar3(B)
,
Narciso Mart´ı-Oliet4
, Jos´e Meseguer5
, Rub´en Rubio4
,
and Carolyn Talcott2
1 Universidad de M´alaga, M´alaga, Spain
duran@lcc.uma.es
2 SRI International, Menlo Park, CA, USA
eker@csl.sri.com, clt@cs.stanford.edu
3 VRAIN, Universitat Polit`ecnica de Val`encia, Valencia, Spain
sescobar@upv.es
4 Universidad Complutense de Madrid, Madrid, Spain
{narciso,rubenrub}@ucm.es
5 University of Illinois at Urbana-Champaign, Urbana, IL, USA
meseguer@illinois.edu
Abstract. Equational uniﬁcation and matching are fundamental mech-
anisms in many automated deduction applications. Supporting them eﬃ-
ciently for as wide as possible a class of equational theories, and in a
typed manner supporting type hierarchies, beneﬁts many applications;
but this is both challenging and nontrivial. We present Maude 3.2’s eﬃ-
cient support of these features as well as of symbolic reachability analysis
of inﬁnite-state concurrent systems based on them.
1
Introduction
Uniﬁcation is a key mechanism in resolution [41] and paramodulation-based
[36] theorem proving. Since Plotkin’s work [40] on equational uniﬁcation, i.e.,
Dur´an was supported by the grant UMA18-FEDERJA-180 funded by J. Andaluc´ıa/
FEDER and the grant PGC2018-094905-B-I00 funded by MCIN/AEI/10.13039/
501100011033 and ERDF A way of making Europe. Escobar was supported by the EC
H2020-EU grant 952215, by the grant RTI2018-094403-B-C32 funded by MCIN/AEI/
10.13039/501100011033 and ERDF A way of making Europe, by the grant PROME-
TEO/2019/098 funded by Generalitat Valenciana, and by the grant PCI2020-120708-2
funded by MICIN/AEI/10.13039/501100011033 and by the European Union NextGen-
erationEU/PRTR. Mart´ı-Oliet and Rubio were supported by the grant PID2019-
108528RB-C22 funded by MCIN/AEI/10.13039/501100011033 and ERDF A way
of making Europe. Talcott was partially supported by the U. S. Oﬃce of Naval
Research under award numbers N00014-15-1-2202 and N00014-20-1-2644, and NRL
grant N0017317-1-G002.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 529–540, 2022.
https://doi.org/10.1007/978-3-031-10769-6_31

530
F. Dur´an et al.
E-uniﬁcation modulo an equational theory E, it is widely used for increased
eﬀectiveness. Since Walther’s work [47] it has been well understood that typed
E-uniﬁcation, exploiting types and subtype hierarchies, can drastically reduce a
prover’s search space. Many other automated deduction applications use typed
E-uniﬁcation as a key mechanism, including, inter alia: (i) constraint logic pro-
gramming, e.g., [12,23]; (ii) narrowing-based inﬁnite-state reachability analysis
and model checking, e.g., [6,35]; (iii) cryptographic protocol analysis modulo
algebraic properties, e.g., [8,19,28]; (iv) partial evaluation, e.g., [4,5]; and (v)
SMT solving, e.g., [32,48]. The special case of typed E-matching is also a key
component in all the above areas as well as in: (vi) E-generalization (also called
anti-uniﬁcation), e.g., [1,2]; and (vii) E-homeomorphic embedding, e.g., [3].
Maximizing the scope and eﬀectiveness of typed E-uniﬁcation and E-
matching means eﬃciently supporting as wide a class of theories E as possible.
Such eﬃciency crucially depends on both eﬃcient algorithms (and their com-
binations) and —since the number of E-uniﬁers may be large— on computing
complete minimal sets of solutions to reduce the search space. The recent Maude
3.2 release1 provides this kind of eﬃcient support for typed E-uniﬁcation and
E-matching in three, increasingly more general classes of theories E:
1. Typed B-uniﬁcation and B-matching, where B is any combination of asso-
ciativity (A) and/or commutativity (C) and/or unit element (U) axioms.
2. Typed E ∪B-uniﬁcation and matching in the user-deﬁnable inﬁnite class of
theories E ∪B with B as in (1), and E ∪B having the ﬁnite variant property
(FVP) [13,21].
3. Typed E ∪B-uniﬁcation for the inﬁnite class of user-deﬁnable theories E ∪B
with B as in (1), and E conﬂuent, terminating, and coherent modulo B.
For classes (1) and (2) the set of B- (resp. E ∪B-) uniﬁers is always complete,
minimal and ﬁnite, except for the AwoC case when B contains an A but not C
axiom for some binary symbol f.2 The typing is order-sorted [22,29] and thus
contains many-sorted and unsorted B- (resp. E∪B-) uniﬁcation as special cases.
For class (3), Maude enumerates a possibly inﬁnite complete set of E∪B-uniﬁers,
with the same AwoC exception on B. We discuss new features for classes (1)–(2),
and a new narrowing modulo E ∪B-based symbolic reachability analysis feature
for inﬁnite-state systems speciﬁed in Maude as rewrite theories (Σ, E ∪B, R)
with equations E ∪B in class (2) and concurrent transition rules R. In Sect. 5
we discuss various applications that can beneﬁt from these new features.
In comparison with previous Maude tool papers reporting on new features
—the last one was [16]— the new features reported here include: (i) computing
minimal complete sets of most general B- (resp. E ∪B-) uniﬁers for classes (1)
and (2) except for the AwoC case; (ii) a new E ∪B-matching algorithm for
class (2); and (iii) a new symbolic reachability analysis for concurrent systems
1 Publicly available at http://maude.cs.illinois.edu.
2 In the AwoC case, Maude’s algorithms are optimized to favor many commonly occur-
ring cases where typed A-uniﬁcation is ﬁnitary, and provides a ﬁnite set of solutions
and an incompleteness warning outside such cases (see [18]).

Equational Uniﬁcation and Matching, and Symbolic Reachability Analysis
531
based on narrowing with transition rules modulo equations E ∪B in class (2)
enjoying powerful state-space reduction capabilities based on the minimality and
completeness feature (i) and on “folding” less general symbolic states into more
general ones through subsumption. Section 3.1 shows the importance of the new
E ∪B-matching algorithm for eﬃcient computation of minimal E ∪B-uniﬁers.
Notation, Strict-B-Coherence, and FVP. For notation involving either term
positions, p ∈pos(t), t|p, t[t′]p, or substitutions, tθ, θμ, see [14]. Equations
(u = v) ∈E oriented as rules (u →v) ∈−→
E are strictly coherent modulo axioms
B iﬀ(t =B t′ ∧t →−
→
E ,B w) ⇒∃w′(t →−
→
E ,B w′ ∧w =B w′), where t →−
→
E ,B w
iﬀ∃(u →v) ∈−→
E , ∃θ, ∃p ∈pos(t)(uθ =B t|p ∧w = t[vθ]p). For (Σ, E ∪B) an
equational theory with −→
E conﬂuent, terminating and strictly coherent modulo
B, (1) an −→
E , B-t-variant is a pair (v, θ) s.t. v = (tθ)!−
→
E ,B ∧θ = θ!−
→
E ,B, where
u!−
→
E ,B (resp. θ!−
→
E ,B) denotes the −→
E , B-normal form of u, resp. θ; (2) for −→
E , B-t-
variants (v, θ), (u, μ), the more general relation (v, θ) ⊒B (u, μ) holds iﬀ∃γ(u =B
vγ ∧θγ =B μ); (3) (Σ, E ∪B) is FVP [13,21] iﬀany Σ-term t has a ﬁnite set
of most general −→
E , B-t-variants. Footnote 5 explains how FVP can be checked.
2
Complete and Minimal Order-Sorted B-Uniﬁers
Throughout the paper we use the following equational theory E ∪B of the
Booleans as a running example (with self-explanatory, user-deﬁnable syntax3):
fmod BOOL-FVP is protecting TRUTH-VALUE .
op _and_ : Bool Bool -> Bool [assoc comm] .
op _xor_ : Bool Bool -> Bool [assoc comm] .
op not_ : Bool -> Bool .
op _or_ : Bool Bool -> Bool .
op _<=>_ : Bool Bool -> Bool .
vars X Y Z W : Bool .
eq X and true = X [variant] .
eq X and false = false [variant] .
eq X and X = X
[variant] .
eq X and X and Y = X and Y [variant] .
*** AC extension
eq X xor false = X [variant] .
eq X xor X = false [variant] .
eq X xor X xor Y = Y [variant] .
*** AC extension
eq not X = X xor true [variant] .
eq X or Y = (X and Y) xor X xor Y [variant] .
eq X <=> Y = true xor X xor Y [variant] .
endfm
3 This module imports Maude’s TRUTH-VALUE module and the command “set include
BOOL off .” must be typed before the module to avoid default importation of BOOL.

532
F. Dur´an et al.
The axioms B are the associativity-commutativity (AC) axioms for xor and and
(speciﬁed with the assoc comm attributes). The equations E are terminating and
conﬂuent modulo B [42]. To achieve strict B-coherence [30], the needed AC-
extensions [39] are added —for example, the AC-extension of X xor X = false
is X xor X xor Y = Y. The equations E for xor and and deﬁne the theory of
Boolean rings, except for the missing4 distributivity equation X and (Y xor Z)
= (X and Y) xor (X and Z). The remaining equations in E deﬁne or, not and
<=> as deﬁnitional extensions. The variant attribute declares that the equation
will be used for folding variant narrowing [21]. The theory is FVP,5 in class (2).
In this section we will consider B-uniﬁcation (for B = AC) using this example.
E ∪B-uniﬁcation for the same example will be discussed in Sect. 3.
For B any combination of associativity and/or commutativity and/or iden-
tity axioms, Maude’s unify command computes a complete ﬁnite set of most
general B-uniﬁers, except for the AwoC case. The new irredundant unify com-
mand always returns6 a ﬁnite, complete and minimal set of B-uniﬁers, except
for the AwoC case. The output of unify for the equation below can be found
in [10, §13].
Maude> irredundant unify X and not Y and not Z =? W and Y and not X .
Decision time: 0ms cpu (0ms real)
Unifier 1
Unifier 2
X --> #1:Bool and #2:Bool
X --> #2:Bool
Z --> #1:Bool and #2:Bool
Z --> #1:Bool
Y --> #1:Bool
Y --> #2:Bool
W --> #2:Bool and not #1:Bool
W --> not #1:Bool
3
E ∪B-Uniﬁcation and Matching for FVP Theories
It is a general result from [21] that if E ∪B is FVP and B-uniﬁcation is ﬁnitary,
then E ∪B-uniﬁcation is ﬁnitary and a complete ﬁnite set of E ∪B-uniﬁers
can be computed by folding variant narrowing [21]. Furthermore, assuming that
TΣ/E,s is non-empty for each sort s, a ﬁnitary E ∪B-uniﬁcation algorithm
automatically provides a decision procedure for satisﬁability of any positive (the
∧, ∨-fragment) quantiﬁer-free formula ϕ in the initial algebra TΣ/E, since ϕ can
be put in DNF, and a conjunction of equalities Γ is satisﬁable in TΣ/E iﬀΓ is
E ∪B-uniﬁable.
Since for our running example BOOL-FVP the equations E∪B are FVP and B-
uniﬁcation (in this case B = AC) is ﬁnitary, all this has useful consequences for
4 By missing distributivity, this theory is weaker than the theory of Boolean rings.
Nevertheless, its initial algebra TΣ/E∪B is exactly the Booleans on {true,false}
with the standard truth tables for all connectives. Thus, all equations provable in
Boolean algebra hold in TΣ/E∪B, including the missing distributivity equation.
5 This can be easily checked in Maude by checking the ﬁniteness of the variants for
each f(X), resp. f(X, Y ), for each unary, resp. binary, symbol f in BOOL-FVP using
the get variants command; see [9] for a theoretical justiﬁcation of this check.
6 Fresh variables follow the form #1:Bool.

Equational Uniﬁcation and Matching, and Symbolic Reachability Analysis
533
BOOL-FVP. Indeed, TΣ/E∪B is exactly the Booleans7 on {true,false} with the
well-known truth tables for and, xor, not, or and <=>. This means that E ∪B-
uniﬁcation provides a Boolean satisﬁability decision procedure for a Boolean
expression u on such symbols, namely, u is Boolean satisﬁable iﬀthe equation
u = true is E∪B-uniﬁable. Furthermore, a ground assignment ρ to the variables
of u is a satisfying assignment for u iﬀthere exists an E∪B-uniﬁer α of u = true
and a ground substitution δ such that ρ = αδ. For the same reasons, u is a
Boolean tautology iﬀthe equation u = false has no E ∪B-uniﬁers.
A complete, ﬁnite set of E ∪B-uniﬁers can be computed with Maude’s
variant unify command whenever E ∪B is FVP, except for the AwoC case.
Instead, the new8 filtered variant unify command computes a ﬁnite, com-
plete and minimal set of E ∪B-uniﬁers, which can be considerably smaller
than that computed by variant unify. For our BOOL-FVP example, filtered
variant unify gives us a Boolean satisﬁability decision procedure plus a sym-
bolic speciﬁcation of satisfying assignments. Such a procedure is not practical: it
cannot compete with standard SAT-solvers; but that was never our purpose: our
purpose here is to illustrate with simple examples how E ∪B-uniﬁcation works
for the inﬁnite class of user-deﬁnable FVP theories E ∪B, of which BOOL-FVP
is just a simple example; dozens of other examples can be found in [32].
The diﬀerence between the variant unify and the new filtered variant
unify command is illustrated with the following example; its unﬁltered output
can be found in [10, §14]. Note that the single E ∪B-uniﬁer gives us a compact
symbolic description of this Boolean expression’s satisfying assignments.
Maude> filtered variant unify (X or Y) <=> Z =? true .
rewrites: 3224 in 12765ms cpu (14776ms real) (252 rewrites/second)
Unifier 1
X --> #1:Bool xor #2:Bool
Y --> #1:Bool
Z --> #2:Bool xor (#1:Bool and (#1:Bool xor #2:Bool))
No more unifiers.
Advisory: Filtering was complete.
The computation of a minimal set of E ∪B-uniﬁers relies on ﬁltering by E ∪B-
matching between two E ∪B-uniﬁers, as explained in the following section.
3.1
FVP E ∪B-Matching and Minimality of E ∪B-Uniﬁers
By deﬁnition, a term u E ∪B-matches another term v iﬀthere is a substitution
γ such that u =E∪B vγ. Besides the existing match command modulo axioms
7 Each connective’s truth table can be checked with Maude’s reduce command. Actu-
ally, need only check and and xor (other connectives are deﬁnitional extensions).
8 In Maude, diﬀerent command names are used to emphasize diﬀerent algorithms.
The word ‘ﬁltered’ is used instead of ‘irredundant’ because irredundancy is not
guaranteed in the AwoC case.

534
F. Dur´an et al.
B, Maude’s new variant match command computes a complete, minimal set
of E ∪B-matching substitutions for any FVP theory E ∪B in class (2), except
for the AwoC case. Such an algorithm could always be derived from an E ∪B-
uniﬁcation algorithm by replacing u by u, where all variables in u are replaced
by fresh constants in u, and computing the E ∪B-uniﬁers of u = v. But a more
eﬃcient special-purpose algorithm has been designed and implemented for this
purpose. E ∪B-matching algorithms are automatically provided by Maude for
any user-deﬁnable theory in class (2) with the variant match command.
Maude> variant match in BOOL-FVP : Z and W <=? X .
rewrites: 12 in 21ms cpu (27ms real) (545 rewrites/second)
Matcher 1
Matcher 2
Matcher 3
Z --> true
Z --> X
Z --> X
W --> X
W --> true
W --> X
This is a good moment to ask and answer a relevant question: Why is com-
puting a complete minimal set of E ∪B-uniﬁers for a uniﬁcation problem Γ,
where E ∪B is an FVP theory in class (2) except for the AwoC case, non-
trivial? We ﬁrst need to explain how minimality is achieved. Suppose that α
and β are two E ∪B-uniﬁers of a system of equations Γ with, say, typed vari-
ables x1, . . . , xn. We then say that α is more general than β modulo E ∪B,
denoted α ⊒E∪B β, iﬀthere is a substitution γ such that for each xi, 1 ≤i ≤n,
γ(α(xi)) =E∪B β(xi). But this exactly means that the vector [β(x1), . . . , β(xn)]
E ∪B-matches the vector [α(x1), . . . , α(xn)] with E ∪B-matching substitution
γ. A complete set of E ∪B-uniﬁers of Γ is by deﬁnition minimal iﬀfor any two
diﬀerent uniﬁers α and β in it we have α ̸⊒E∪B β and β ̸⊒E∪B α, i.e., the two
associated E ∪B-matching problems fail.
What is nontrivial is computing a minimal complete set of E ∪B-uniﬁers
eﬃciently. One could do so ineﬃciently by simulating E ∪B-matching with E ∪
B-uniﬁcation, and more eﬃciently by using an E∪B-matching algorithm. Maude
achieves still greater eﬃciency by directly computing the α ⊒E ∪B β relation.
The key diﬀerence between the variant unify command and the new filtered
variant unify command is that the second computes a E ∪B-minimal set of
E∪B-uniﬁers of Γ using the α ⊒E∪B β relation, whereas the ﬁrst only computes
a set of B-minimal E ∪B-uniﬁers of Γ using the cheaper α ⊒B β relation. There
are three ideas we use to make it fast in practice: (i) variant matching is faster
than variant uniﬁcation because one side is variable-free; (ii) enumerating the
variant matchers between two variant uniﬁers is far more expensive than checking
existence of a matcher; and (iii) variant uniﬁers are discarded on-the-ﬂy avoiding
further narrowing steps and computation.
4
Narrowing-Based Symbolic Reachability Analysis
In Maude, concurrent systems are speciﬁed in so-called system modules as rewrite
theories of the form: R = (Σ, G, R), where G is an equational theory either of the

Equational Uniﬁcation and Matching, and Symbolic Reachability Analysis
535
form B in class (1), or E∪B in classes (2) or (3), and R are the system transition
rules, speciﬁed as rewrite rules. When the theory R is topmost, meaning that the
rules R rewrite the entire state, narrowing with rules R modulo the equations
G is a complete symbolic reachability analysis method for inﬁnite-state systems
[35]. That is, given a term u with variables −→x , representing a typically inﬁnite
set of initial states, and another term v with variables −→y , representing a possibly
inﬁnite set of target states, narrowing can answer the question: can an instance
of u reach an instance of v? That is, does the formula ∃−→x , −→y
u →∗v hold in
R? Note that, if the complement of a system invariant I can be symbolically
described as the set of ground instances of terms in a set {v1, . . . , vn} of pattern
terms, then narrowing provides a semi-decision procedure for verifying whether
the system speciﬁed by R fails to satisfy I starting from an initial set of states
speciﬁed by u. Namely, I holds iﬀno instance of any vi can be reached from
some instance of u.
Assuming G is in class (1) or (2), Maude’s vu-narrow command implements
narrowing with R modulo G by performing G-uniﬁcation at each narrowing
step. However, the number of symbolic states that need to be explored can be
inﬁnite. This means that if no solution exists for the narrowing search, Maude
will search forever, so that only depth-bounded searches will terminate. The great
advantage of the new {fold} vu-narrow {filter,delay} command is that it
performs a powerful symbolic state space reduction by: (i) removing a newly
explored symbolic state v′ if it E ∪B-matches a previously explored state v and
replacing transition with target v′ by transitions with target v; and (ii) using
minimal sets of E ∪B-uniﬁers for each narrowing step and for checking common
instances between a newly explored state and the target term (ensured by words
filter and delay). This can make the entire search space ﬁnite and allow full
veriﬁcation of invariants for some inﬁnite-state systems. Consider the following
Maude speciﬁcation of Lamport’s bakery protocol.
mod BAKERY is
sorts Nat LNat Nat? State WProcs Procs .
subsorts Nat LNat < Nat? .
subsort WProcs < Procs .
op 0 : -> Nat .
op s : Nat -> Nat .
op [_] : Nat -> LNat .
*** number-locking operator
op < wait,_> : Nat -> WProcs .
op < crit,_> : Nat -> Procs .
op mt : -> WProcs .
*** empty multiset
op __ : Procs Procs -> Procs [assoc comm id: mt] .
*** union
op __ : WProcs WProcs -> WProcs [assoc comm id: mt] . *** union
op _|_|_ : Nat Nat? Procs -> State .
vars n m i j k : Nat . var x? : Nat? . var PS : Procs . var WPS : WProcs .
rl [new]:
m | n | PS => s(m) | n | < wait,m > PS [narrowing] .
rl [enter]: m | n | < wait,n > PS => m | [n] | < crit,n > PS [narrowing] .
rl [leave]: m | [n] | < crit,n > PS => m | s(n) | PS [narrowing] .
endm

536
F. Dur´an et al.
The states of BAKERY have the form “m | x? | PS” with m the ticket-dispensing
counter, x? the (possibly locked) counter to access the critical section, and PS a
multiset of processes either waiting or in the critical section. BAKERY is inﬁnite-
state: [new] creates new processes, and the counters can grow unboundedly.
When a waiting process enters the critical section with [enter], the second
counter n is locked as [n]; and it is unlocked and incremented when it leaves
it with [leave]. The key invariant is mutual exclusion. Note that the term
“i | x? | < crit, j > < crit, k > PS” describes all states in the comple-
ment of mutual exclusion states. Without the fold option, narrowing does not
terminate, but with the following command we can verify that BAKERY satisﬁes
mutual exclusion, not just for the initial state “0 | 0 | mt”, but for the much
more general inﬁnite set of initial states with waiting processes only “m | n |
WPS”.
Maude> {fold} vu-narrow {filter,delay}
m | n | WPS =>* i | x? | < crit, j > < crit, k > PS .
No solution.
rewrites: 4 in 1ms cpu (1ms real) (2677 rewrites/second)
The new vu-narrow {filter,delay} command can achieve dramatic state
space reductions over the previous vu-narrow command by ﬁltering E ∪B-
uniﬁers. This is illustrated by a simple cryptographic protocol example in [10,
§15] exploiting the unitary nature of uniﬁcation in the exclusive-or theory [24].
5
Applications and Conclusion
Maude can be used as a meta-tool to develop new formal tools because: (i) its
underlying equational and rewriting logics are logical —and reﬂective meta-
logical— frameworks [7,27,46]; (ii) Maude’s eﬃcient support of logical reﬂection
through its META-LEVEL module; (iii) Maude’s rewriting, search, model checking,
and strategy language features [11,15]; and (iv) Maude’s symbolic reasoning
features [15,33], the latest reported here. We refer to [11,15,31,33] for references
on various Maude-based tools. Many of them can beneﬁt from these new features.
By way of example we mention some areas ready to reap such beneﬁts: (1)
Formal Analysis of Cryptographic Protocols. The new features can yield substan-
tial improvements to tools such as Maude-NPA [19], Tamarin [28] and AKISS [8].
(2) Model Checking of Inﬁnite-State Systems. The narrowing-based LTL sym-
bolic model checker reported in [6,20], and the addition of new symbolic capa-
bilities to Real-Time Maude [37,38] can both beneﬁt from the new features. (3)
SMT Solving. In Sect. 3 we noted that FVP E∪B-uniﬁcation makes satisﬁability
of positive QF formulas in TΣ/E∪B decidable. Under mild conditions, this has
been extended in [32,44] to a procedure for satisﬁability in TΣ/E∪B of all QF
formulas which will also beneﬁt from the new features. (4) Theorem Proving.
The new Maude Inductive Theorem Prover under construction [34], as well as

Equational Uniﬁcation and Matching, and Symbolic Reachability Analysis
537
Maude’s Invariant Analyzer [43] and Reachability Logic Theorem Prover [45] all
use equational uniﬁcation and narrowing modulo equations; so all will beneﬁt
from the new features. (5) Theory Transformations based on equational uniﬁ-
cation, e.g., partial evaluation [4], ground conﬂuence methods [17] or program
termination methods [25,26] could likewise become more eﬃcient.
In conclusion, we have presented and illustrated with examples new equa-
tional uniﬁcation and matching, and symbolic reachability analysis features in
Maude 3.2. Thanks to the above-mentioned properties (i)–(iv) of Maude as a
meta-tool, we hope that this work will encourage other researchers to use Maude
and its symbolic features to develop new tools in many diﬀerent logics.
References
1. A¨ıt-Kaci, H., Sasaki, Y.: An axiomatic approach to feature term generalization.
In: De Raedt, L., Flach, P. (eds.) ECML 2001. LNCS (LNAI), vol. 2167, pp. 1–12.
Springer, Heidelberg (2003). https://doi.org/10.1007/3-540-44795-4 1
2. Alpuente, M., Ballis, D., Cuenca-Ortega, A., Escobar, S., Meseguer, J.: ACUOS2:
a high-performance system for modular ACU generalization with subtyping and
inheritance. In: Calimeri, F., Leone, N., Manna, M. (eds.) JELIA 2019. LNCS
(LNAI), vol. 11468, pp. 171–181. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-19570-0 11
3. Alpuente, M., Cuenca-Ortega, A., Escobar, S., Meseguer, J.: Order-sorted home-
omorphic embedding modulo combinations of associativity and/or commutativity
axioms. Fundam. Inform. 177(3–4), 297–329 (2020)
4. Alpuente, M., Cuenca-Ortega, A., Escobar, S., Meseguer, J.: A partial evaluation
framework for order-sorted equational programs modulo axioms. J. Log. Algebraic
Methods Program. 110, 100501 (2020)
5. Alpuente, M., Falaschi, M., Vidal, G.: Partial evaluation of functional logic pro-
grams. ACM Trans. Program. Lang. Syst. 20(4), 768–844 (1998)
6. Bae, K., Escobar, S., Meseguer, J.: Abstract logical model checking of inﬁnite-
state systems using narrowing. In: RTA 2013. LIPIcs, vol. 21, pp. 81–96. Schloss
Dagstuhl - Leibniz-Zentrum fuer Informatik (2013)
7. Basin, D., Clavel, M., Meseguer, J.: Rewriting logic as a metalogical framework.
ACM Trans. Comput. Log. 5, 528–576 (2004)
8. Chadha, R., Cheval, V., Ciobˆac˘a, S¸, Kremer, S.: Automated veriﬁcation of equiv-
alence properties of cryptographic protocols. ACM Trans. Comput. Log. 17(4),
23:1–23:32 (2016)
9. Cholewa, A., Meseguer, J., Escobar, S.: Variants of variants and the ﬁnite variant
property. Technical report, CS Dept. University of Illinois at Urbana-Champaign,
February 2014. http://hdl.handle.net/2142/47117
10. Clavel, M., et al.: Maude manual (version 3.2.1). SRI International, February 2022.
http://maude.cs.illinois.edu
11. Clavel, M., et al.: All About Maude, A High-Performance Logical Framework.
Lecture Notes in Computer Science, vol. 4350. Springer, Heidelberg (2007). https://
doi.org/10.1007/978-3-540-71999-1
12. Colmerauer, A.: An introduction to Prolog III. Commun. ACM 33(7), 69–90 (1990)
13. Comon-Lundh, H., Delaune, S.: The ﬁnite variant property: how to get rid of some
algebraic properties. In: Giesl, J. (ed.) RTA 2005. LNCS, vol. 3467, pp. 294–307.
Springer, Heidelberg (2005). https://doi.org/10.1007/978-3-540-32033-3 22

538
F. Dur´an et al.
14. Dershowitz, N., Jouannaud, J.-P.: Rewrite systems. In: van Leeuwen, J. (ed.) Hand-
book of Theoretical Computer Science, Volume B: Formal Models and Semantics,
pp. 243–320. North-Holland (1990)
15. Dur´an, F., et al.: Programming and symbolic computation in Maude. J. Log. Alge-
braic Methods Program. 110, 100497 (2020)
16. Dur´an, F., Eker, S., Escobar, S., Mart´ı-Oliet, N., Meseguer, J., Talcott, C.: Associa-
tive uniﬁcation and symbolic reasoning modulo associativity in Maude. In: Rusu, V.
(ed.) WRLA 2018. LNCS, vol. 11152, pp. 98–114. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-99840-4 6
17. Dur´an, F., Meseguer, J., Rocha, C.: Ground conﬂuence of order-sorted conditional
speciﬁcations modulo axioms. J. Log. Algebraic Methods Program. 111, 100513
(2020)
18. Eker, S.: Associative uniﬁcation in Maude. J. Log. Algebraic Methods Program.
126, 100747 (2022)
19. Escobar, S., Meadows, C., Meseguer, J.: Maude-NPA: cryptographic protocol
analysis modulo equational properties. In: Aldini, A., Barthe, G., Gorrieri, R.
(eds.) FOSAD 2007-2009. LNCS, vol. 5705, pp. 1–50. Springer, Heidelberg (2009).
https://doi.org/10.1007/978-3-642-03829-7 1
20. Escobar, S., Meseguer, J.: Symbolic model checking of inﬁnite-state systems using
narrowing. In: Baader, F. (ed.) RTA 2007. LNCS, vol. 4533, pp. 153–168. Springer,
Heidelberg (2007). https://doi.org/10.1007/978-3-540-73449-9 13
21. Escobar, S., Sasse, R., Meseguer, J.: Folding variant narrowing and optimal variant
termination. J. Algebraic Log. Program. 81, 898–928 (2012)
22. Goguen, J., Meseguer, J.: Order-sorted algebra I: equational deduction for multiple
inheritance, overloading, exceptions and partial operations. Theoret. Comput. Sci.
105, 217–273 (1992)
23. Jaﬀar, J., Maher, M.J.: Constraint logic programming: a survey. J. Log. Program.
19(20), 503–581 (1994)
24. Kapur, D., Narendran, P.: Matching, uniﬁcation and complexity. SIGSAM Bull.
21(4), 6–9 (1987)
25. Lucas, S., Meseguer, J., Guti´errez, R.: The 2D dependency pair framework for
conditional rewrite systems. Part I: deﬁnition and basic processors. J. Comput.
Syst. Sci. 96, 74–106 (2018)
26. Lucas, S., Meseguer, J., Guti´errez, R.: The 2D dependency pair framework for
conditional rewrite systems - Part II: advanced processors and implementation
techniques. J. Autom. Reason. 64(8), 1611–1662 (2020)
27. Mart´ı-Oliet, N., Meseguer, J.: Rewriting logic as a logical and semantic frame-
work. In: Gabbay, D., Guenthner, F. (eds.) Handbook of Philosophical Logic, 2nd.
Edition, pages 1–87. Kluwer Academic Publishers (2002). First published as SRI
Technical report SRI-CSL-93-05, August 1993
28. Meier, S., Schmidt, B., Cremers, C., Basin, D.: The TAMARIN prover for the
symbolic analysis of security protocols. In: Sharygina, N., Veith, H. (eds.) CAV
2013. LNCS, vol. 8044, pp. 696–701. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-39799-8 48
29. Meseguer, J.: Membership algebra as a logical framework for equational speciﬁca-
tion. In: Presicce, F.P. (ed.) WADT 1997. LNCS, vol. 1376, pp. 18–61. Springer,
Heidelberg (1998). https://doi.org/10.1007/3-540-64299-4 26
30. Meseguer, J.: Strict coherence of conditional rewriting modulo axioms. Theor.
Comput. Sci. 672, 1–35 (2017)

Equational Uniﬁcation and Matching, and Symbolic Reachability Analysis
539
31. Meseguer, J.: Symbolic reasoning methods in rewriting logic and Maude. In: Moss,
L.S., de Queiroz, R., Martinez, M. (eds.) WoLLIC 2018. LNCS, vol. 10944, pp.
25–60. Springer, Heidelberg (2018). https://doi.org/10.1007/978-3-662-57669-4 2
32. Meseguer, J.: Variant-based satisﬁability in initial algebras. Sci. Comput. Program.
154, 3–41 (2018)
33. Meseguer, J.: Symbolic computation in Maude: some tapas. In: LOPSTR 2020.
LNCS, vol. 12561, pp. 3–36. Springer, Cham (2021). https://doi.org/10.1007/978-
3-030-68446-4 1
34. Meseguer, J., Skeirik, S.: Inductive reasoning with equality predicates, contextual
rewriting and variant-based simpliﬁcation. In: Escobar, S., Mart´ı-Oliet, N. (eds.)
WRLA 2020. LNCS, vol. 12328, pp. 114–135. Springer, Cham (2020). https://doi.
org/10.1007/978-3-030-63595-4 7
35. Meseguer, J., Thati, P.: Symbolic reachability analysis using narrowing and its
application to veriﬁcation of cryptographic protocols. High.-Order Symb. Comput.
20(1–2), 123–160 (2007)
36. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Robinson,
J.A., Voronkov, A. (eds.) Handbook of Automated Reasoning (in 2 volumes), pp.
371–443. Elsevier and MIT Press (2001)
37. ¨Olveczky, P.C.: Real-time Maude and its applications. In: Escobar, S. (ed.) WRLA
2014. LNCS, vol. 8663, pp. 42–79. Springer, Cham (2014). https://doi.org/10.1007/
978-3-319-12904-4 3
38. ¨Olveczky, P.C., Meseguer, J.: Semantics and pragmatics of real-time Maude. High.-
Order Symb. Comput. 20(1–2), 161–196 (2007)
39. Peterson, G.E., Stickel, M.E.: Complete sets of reductions for some equational
theories. J. Assoc. Comput. Mach. 28(2), 233–264 (1981)
40. Plotkin, G.: Building-in equational theories. In: Meltzer, B., Michie, D. (eds.) 1971
Proceedings of the Seventh Annual Machine Intelligence Workshop on Machine
Intelligence 7, Edinburgh, pp. 73–90. Edinburgh University Press (1972)
41. Robinson, J.A.: A machine-oriented logic based on the resolution principle. J.
Assoc. Comput. Mach. 12(1), 23–41 (1965)
42. Rocha, C., Meseguer, J.: Five isomorphic Boolean theories and four equational deci-
sion procedures. Technical report UIUCDCS-R-2007-2818, CS Department, Uni-
versity of Illinois at Urbana-Champaign, February 2007. http://hdl.handle.net/
2142/11295
43. Rocha, C., Meseguer, J.: Proving safety properties of rewrite theories. In: Corra-
dini, A., Klin, B., Cˆırstea, C. (eds.) CALCO 2011. LNCS, vol. 6859, pp. 314–328.
Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-22944-2 22
44. Skeirik, S., Meseguer, J.: Metalevel algorithms for variant satisﬁability. J. Log.
Algebr. Meth. Program. 96, 81–110 (2018)
45. Skeirik, S., Stefanescu, A., Meseguer, J.: A constructor-based reachability logic for
rewrite theories. Fundam. Inform. 173(4), 315–382 (2020)
46. Stehr, M.-O., Meseguer, J.: Pure type systems in rewriting logic: specifying typed
higher-order languages in a ﬁrst-order logical framework. In: Owe, O., Krogdahl, S.,
Lyche, T. (eds.) From Object-Orientation to Formal Methods. LNCS, vol. 2635, pp.
334–375. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-39993-
3 16
47. Walther, C.: A mechanical solution of Schubert’s steamroller by many-sorted res-
olution. Artif. Intell. 26(2), 217–224 (1985)
48. Zheng, Y., et al.: Z3str2: an eﬃcient solver for strings, regular expressions, and
length constraints. Formal Methods Syst. Design 50(2–3), 249–288 (2017)

540
F. Dur´an et al.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Le´sniewski’s Ontology – Proof-Theoretic
Characterization
Andrzej Indrzejczak(B)
Department of Logic, University of Lodz, L´od´z, Poland
andrzej.indrzejczak@filhist.uni.lodz.pl
Abstract. The ontology of Le´sniewski is commonly regarded as the
most comprehensive calculus of names and the theoretical basis of mere-
ology. However, ontology was not examined by means of proof-theoretic
methods so far. In the paper we provide a characterization of elementary
ontology as a sequent calculus satisfying desiderata usually formulated
for rules in well-behaved systems in modern structural proof theory. In
particular, the cut elimination theorem is proved and the version of sub-
formula property holds for the cut-free version.
Keywords: Le´sniewski · Ontology · Calculus of Names · Sequent
Calculus · Cut Elimination
1
Introduction
The ontology of Le´sniewski is a kind of calculus of names proposed as a formal-
ization of logic alternative to Fregean paradigm. Basically, it is a theory of the
binary predicate ε understood as the formalization of the Greek ‘esti’. Informally
a formula aεb is to be read as “(the) a is (a/the) b”, so in order to be true a
must be an individual name whereas b can be individual or general name. In the
original formulation Le´sniewski’s ontology is the middle part of the hierarchical
structure involving also the protothetics and mereology (see the presentation in
Urbaniak [20]). Protothetics, a very general form of propositional logic, is the
basis of the overall construction. Its generality follows from the fact that, in addi-
tion to sentence variables, arbitrary sentence-functors (connectives) are allowed
as variables, and quantiﬁers binding all these kinds of variables are involved.
Similarly in Le´sniewski’s ontology, we have a quantiﬁcation over name variables
but also over arbitrary name-functors creating complex names. In consequence
we obtain very expressive logic which is then extended to mereology. The latter,
which is the most well-known ingredient of Le´sniewski’s construction, is a theory
of parthood relation, which provides an alternative formalization of the theory
of classes and foundations of mathematics.
Despite of the dependence of Le´sniewski’s ontology on his protothetics, we
can examine this theory, in particular its part called elementary ontology, in
isolation, as a kind of ﬁrst-order theory of ε based on classical ﬁrst-order logic
(FOL). Elementary ontology, in this sense, was investigated, among others, by
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 541–558, 2022.
https://doi.org/10.1007/978-3-031-10769-6_32

542
A. Indrzejczak
Slupecki [17] and Iwanu´s [7], and we follow this line here. The expressive power
of such an approach is strongly reduced, in particular, quantiﬁers apply only to
name variables. One should note however that, despite of the appearances, it
is not just another elementary theory in the standard sense, since the range of
variables is not limited to individual names but admits general and even empty
names. Thus, name variables may represent not only ‘Napoleon Bonaparte’ but
also ‘an emperor’ and ‘Pegasus’. This leads to several problems concerning the
interpretation of quantiﬁers in ontology, encountered in the semantical treat-
ment (see e.g. K¨ung and Canty [8] or Rickey [16]). However, for us the problems
of proper interpretation are not important here, since we develop purely syn-
tactical formulation, which is shown to be equivalent to Le´sniewski’s axiomatic
formulation.
Taking into account the importance and originality of Le´sniewski’s ontol-
ogy it is interesting, if not surprising, that so far no proof-theoretic study was
oﬀered, in particular, in terms of sequent calculus (SC). In fact, a form of natu-
ral deduction proof system was applied by many authors following the original
way of presenting proofs by Le´sniewski (see, e.g. his [9–11]). However this can
hardly be treated as a proof-theoretic study of Le´sniewski’s ontology but only
as a convenient way of simplifying presentation of axiomatic proofs. Ishimoto
and Kobayashi [6] introduced also a tableau system for part of (quantiﬁer-free)
ontology – we will say more about this system later.
In this paper we present a sequent calculus for elementary ontology and focus
on its most important properties. More speciﬁcally, in Sect. 2 we brieﬂy charac-
terise elementary ontology which will be the object of our study. In Sect. 3 we
present an adequate sequent calculus for the basic part of elementary ontology
and prove that it is equivalent with the axiomatic formulation. Then we prove
the cut elimination theorem for this calculus in Sect. 4. In the next section we
focus on the problem of extensionality and discuss some alternative formula-
tions of ontology and some of its parts, as well as the intuitionistic version of it.
Section 6 shows how the basic system can be extended with rules for new pred-
icate constants which preserve cut elimination. The problem of extension with
rules for term constants is discussed brieﬂy in Sect. 7. A summary of obtained
results and open problems closes the paper.
2
Elementary Ontology
Roughly, in this article, by Le´sniewski’s elementary ontology we mean stan-
dard FOL (in some chosen adequate formalization) with Le´sniewski’s axiom
LA added. For more detailed general presentation of Le´sniewski’s systems one
may consult Urbaniak [20] and for a detailed study of Le´sniewski’s ontology
see Iwanu´s [7] or Slupecki [17]. In the next section we will select a particular
sequent system as representing FOL and investigate several ways of possible
representation of LA in this framework.
We will consider two languages for ontology. In both we assume a denumer-
able set of name variables. Following the well-known Gentzen’s custom we apply

Le´sniewski’s Ontology - Proof-Theoretic Characterization
543
a graphical distinction between the bound variables, which will be denoted by
x, y, z, ... (possibly with subscripts), and the free variables usually called param-
eters, which will be denoted by a, b, c, .... These are the only terms we admit, and
both kinds will be called simply name variables. The basic language Lo consists
of the following vocabulary:
– connectives: ¬, ∧, ∨, →;
– ﬁrst-order quantiﬁers: ∀, ∃;
– predicate: ε.
As we can see, in addition to the standard logical vocabulary of FOL, the
only speciﬁc constant is a binary predicate ε with the formation rule: t ε t′ is
an atomic formula, for any terms t, t′. In what follows we will use a convention:
instead of t ε t′ we will write tt′. The complexity of formulae of Lo is deﬁned as
the number of occurrences of logical constants, i.e. connectives and quantiﬁers.
Hence the complexity of atomic formulae is 0.
The language Lp, considered in Sect. 6, adds to this vocabulary a number of
unary and binary predicates: D, V, S, G, U, =, ≡, ≈, ¯ε, ⊂, ⊈, A, E, I, O.
In Lo and Lp we have name variables, which range over all names (individ-
ual, general and empty), as the only terms. However Le´sniewski considered also
complex terms built with the help of speciﬁc term-forming functors. We will
discuss brieﬂy such extensions in the setting of sequent calculus in Sect. 7 and
notice important problems they generate for decent proof-theoretic treatment.
The only speciﬁc axiom of elementary ontology is Le´sniewski’s axiom LA:
∀xy(xy ↔∃z(zx) ∧∀z(zx →zy) ∧∀zv(zx ∧vx →zv))
LA→, LA←will be used to refer to the respective implications forming LA,
with dropped outer universal quantiﬁer. Note that:
Lemma 1. The following formulae are equivalent to LA:
1. ∀xy(xy ↔∃z(zx ∧zy) ∧∀zv(zx ∧vx →zv))
2. ∀xy(xy ↔∃z(zx ∧zy ∧∀v(vx →vz)))
3. ∀xy(xy ↔∃z(∀v(vx ↔vz) ∧zy))
We start with the system in the language Lo, i.e. with ε (conventionally
omitted) as the only speciﬁc predicate constant added to the standard language
of FOL.
3
Sequent Calculus
Elementary ontology will be formalised as a sequent calculus with sequents Γ ⇒
Δ which are ordered pairs of ﬁnite multisets of formulae called the antecedent
and the succedent, respectively. We will use the calculus G (after Gentzen) which
is essentially the calculus G1 of Troelstra and Schwichtenberg [19]. All necessary

544
A. Indrzejczak
(AX) ϕ ⇒ϕ
(Cut)
Γ ⇒Δ, ϕ
ϕ, Π ⇒Σ
Γ, Π ⇒Δ, Σ
(W⇒)
Γ ⇒Δ
ϕ, Γ ⇒Δ
(⇒W )
Γ ⇒Δ
Γ ⇒Δ, ϕ
(C⇒)
ϕ, ϕ, Γ ⇒Δ
ϕ, Γ ⇒Δ
(⇒C)
Γ ⇒Δ, ϕ, ϕ
Γ ⇒Δ, ϕ
(¬⇒)
Γ ⇒Δ, ϕ
¬ϕ, Γ ⇒Δ
(⇒¬)
ϕ, Γ ⇒Δ
Γ ⇒Δ, ¬ϕ
(∧⇒)
ϕ, ψ, Γ ⇒Δ
ϕ ∧ψ, Γ ⇒Δ
(⇒∧)
Γ ⇒Δ, ϕ
Γ ⇒Δ, ψ
Γ ⇒Δ, ϕ ∧ψ
(∨⇒)
ϕ, Γ ⇒Δ
ψ, Γ ⇒Δ
ϕ ∨ψ, Γ ⇒Δ
(⇒∨)
Γ ⇒Δ, ϕ, ψ
Γ ⇒Δ, ϕ ∨ψ
(→⇒)
Γ ⇒Δ, ϕ
ψ, Γ ⇒Δ
ϕ →ψ, Γ ⇒Δ
(⇒→)
ϕ, Γ ⇒Δ, ψ
Γ ⇒Δ, ϕ →ψ
(↔⇒)
Γ⇒Δ, ϕ, ψ
ϕ, ψ, Γ⇒Δ
ϕ↔ψ, Γ⇒Δ
(∀⇒)
ϕ[x/b], Γ⇒Δ
∀xϕ, Γ⇒Δ
(⇒↔)
ϕ, Γ⇒Δ, ψ
ψ, Γ ⇒Δ, ϕ
Γ⇒Δ, ϕ↔ψ
(⇒∀)
Γ⇒Δ, ϕ[x/a]
Γ⇒Δ, ∀xϕ
(∃⇒)
ϕ[x/a], Γ⇒Δ
∃xϕ, Γ⇒Δ
(⇒∃)
Γ⇒Δ, ϕ[x/b]
Γ⇒Δ, ∃xϕ
where a is a fresh parameter (eigenvariable), not present in Γ, Δ and ϕ, whereas b is
an arbitrary parameter.
Fig. 1. Calculus G
structural rules, including cut, weakening and contraction are primitive. The
calculus G consists of the rules from Fig. 1:
Let us recall that formulae displayed in the schemata are active, whereas
the remaining ones are parametric, or form a context. In particular, all active
formulae in the premisses are called side formulae, and the one in the conclusion
is the principal formula of the respective rule application. Proofs are deﬁned in
a standard way as ﬁnite trees with nodes labelled by sequents. The height of a
proof D of Γ ⇒Δ is deﬁned as the number of nodes of the longest branch in D.
⊢k Γ ⇒Δ means that Γ ⇒Δ has a proof of the height at most k.
G provides an adequate formalization of the classical pure FOL (i.e. with no
terms other than variables). However, we should remember that here terms in
quantiﬁer rules are restricted to variables ranging over arbitrary names (includ-
ing empty and general). This means, in particular, that quantiﬁers do not have
an existential import, like in standard FOL.

Le´sniewski’s Ontology - Proof-Theoretic Characterization
545
Let us call G+LA an extension of G with LA as an additional axiomatic
sequent. The following hold:
Lemma 2. The following sequents are provable in G+LA:
ab ⇒∃x(xa)
ab ⇒∀x(xa →xb)
ab ⇒∀xy(xa ∧ya →xy)
∃x(xa), ∀x(xa →xb), ∀xy(xa ∧ya →xy) ⇒ab
The proof is obvious. In fact, these sequents together allow us to derive LA
so we could use them alternatively in a characterization of elementary ontology
on the basis of G.
G+LA is certainly an adequate formalization of elementary ontology in the
sense of Slupecki and Iwanu´s. However, from the standpoint of proof theoretic
analysis it is not an interesting form of sequent calculus and it will be used only
for showing the adequacy of our main system called GO.
To obtain the basic GO we add the following four rules to G:
(R) aa, Γ ⇒Δ
ab, Γ ⇒Δ
(T)
ac, Γ ⇒Δ
ab, bc, Γ ⇒Δ
(S)
ba, Γ ⇒Δ
ab, bb, Γ ⇒Δ
(E) da, Γ ⇒Δ, dc
dc, Γ ⇒Δ, da
ab, Γ ⇒Δ
cb, Γ ⇒Δ
where d in (E) is a new parameter (eigenvariable), and a, b, c are arbitrary.
The names of rules come from reﬂexivity, transitivity, symmetry and exten-
sionality. In case of (R) and (S) it is a kind of preﬁxed reﬂexivity and symmetry
(ab →aa, bb →(ab →ba)). Why (E) comes from extensionality will be explained
later.
We can show that GO is an adequate characterization of elementary ontology.
Theorem 1. If G+LA ⊢Γ ⇒Δ, then GO ⊢Γ ⇒Δ.
Proof. It is suﬃcient to prove that the axiomatic sequent LA is provable in GO.
aa ⇒aa
(R) ab ⇒aa
(⇒∃) ab ⇒∃x(xa)
cb ⇒cb
(T)
ca, ab ⇒cb
(⇒→)
ab ⇒ca →cb
(⇒∀)
ab ⇒∀x(xa →xb) (⇒∧)
ab ⇒∃x(xa) ∧∀x(xa →xb)
(⇒∧) with:
cd ⇒cd
(T)
ca, ad ⇒cd
(S)
ca, da, aa ⇒cd (R)
ca, da, ab ⇒cd
(∧⇒)
ab, ca ∧da ⇒cd
(⇒→)
ab ⇒ca ∧da →cd
(⇒∀)
ab ⇒∀xy(xa ∧ya →xy)

546
A. Indrzejczak
yields LA→after (⇒→). A proof of the converse is more complicated (for read-
ability and space-saving we ommited all applications of weakening rules neces-
sary for the application of two- and three-premiss rules; this convention will be
applied hereafter with no comments):
ca ⇒ca
da ⇒da
ca ⇒ca
(⇒∧)
da, ca ⇒da ∧ca
dc ⇒dc
(→⇒)
da, ca, da ∧ca →dc ⇒dc
(∀⇒)
da, ca, ∀xy(xa ∧ya →xy) ⇒dc
da ⇒da
(T )
dc, ca ⇒da
ab ⇒ab
(E)
cb, ca, ∀xy(xa ∧ya →xy) ⇒ab
(→⇒)
ca, ca →cb, ∀xy(xa ∧ya →xy) ⇒ab
(∀⇒)
ca, ∀x(xa →xb), ∀xy(xa ∧ya →xy) ⇒ab
(∃⇒)
∃x(xa), ∀x(xa →xb), ∀xy(xa ∧ya →xy) ⇒ab
It is routine to prove LA.
⊓⊔
Note that to prove LA→the rules (R), (T), (S) were suﬃcient, whereas in
order to derive the converse, (E) alone is not suﬃcient - we need (T) again.
Theorem 2. If GO ⊢Γ ⇒Δ, then G+LA ⊢Γ ⇒Δ.
Proof. It is suﬃcient to prove that the four rules of GO are derivable in G+LA.
For (T):
bc ⇒∀x(xb →xc)
ab ⇒ab
ac ⇒ac (→⇒)
ab →ac, ab ⇒ac
(∀⇒)
∀x(xb →xc), ab ⇒ac (Cut)
ab, bc ⇒ac
ac, Γ ⇒Δ (Cut)
ab, bc, Γ ⇒Δ
where the leftmost leaf is provable in G+LA (Lemma 2).
For (S):
bb ⇒∀xy(xb ∧yb →xy)
bb ⇒bb
ab ⇒ab (⇒∧)
bb, ab ⇒bb ∧ab
ba ⇒ba (→⇒)
bb ∧ab →ba, bb, ab ⇒ba
(∀⇒)
∀xy(xb ∧yb →xy), bb, ab ⇒ba (Cut)
bb, bb, ab ⇒ba (C ⇒)
bb, ab ⇒ba
where the leftmost leaf is provable in G+LA (Lemma 2). By cut with the premiss
of (S) we obtain its conclusion.
For (R):
ab ⇒∀xy(xa ∧ya →xy)
ab ⇒∃x(xa)
S
(Cut)
∀xy(xa ∧ya →xy), ∀x(xa →xa), ab ⇒aa (Cut)
∀x(xa →xa), ab, ab ⇒aa (C ⇒)
∀x(xa →xa), ab ⇒aa

Le´sniewski’s Ontology - Proof-Theoretic Characterization
547
where S := ∃x(xa), ∀xy(xa ∧ya →xy), ∀x(xa →xa) ⇒aa and all leaves are
provable in G+LA (Lemma 2); in particular S is the fourth sequent with b
replaced with a. By cut with ⇒∀x(xa →xa) and the premiss of (R) we obtain
its conclusion.
Since (R), (T), (S) are all derivable in G+LA we use them in the proof of the
derivability of (E) to simplify matters. Note ﬁrst the following three proofs with
weakenings omitted:
cc ⇒cc
(R) cb ⇒cc
ca ⇒ca
(↔⇒)
ca ↔cc, cb ⇒ca
(⇒∃) ca ↔cc, cb ⇒∃x(xa)
(∀⇒) ∀x(xa ↔xc), cb ⇒∃x(xa)
da ⇒da
db ⇒db
(T)
dc, cb ⇒db
(↔⇒)
da ↔dc, cb, da ⇒db
(∀⇒) ∀x(xa ↔xc), cb, da ⇒db
(⇒→) ∀x(xa ↔xc), cb ⇒da →db
(⇒∀) ∀x(xa ↔xc), cb ⇒∀x(xa →xb)
and
da ⇒da
ea ⇒ea
de ⇒de
(T)
ce, dc ⇒de
(S)
ec, dc, cc ⇒de (R ⇒)
ec, dc, cb ⇒de (↔⇒)
dc, ea ↔ec, cb, ea ⇒de
(∀⇒)
dc, ∀x(xa ↔xc), cb, ea ⇒de
(↔⇒)
da ↔dc, ∀x(xa ↔xc), cb, da, ea ⇒de
(∀⇒) ∀x(xa ↔xc), ∀x(xa ↔xc), cb, da, ea ⇒de
(C ⇒)
∀x(xa ↔xc), cb, da, ea ⇒de
(∧⇒) ∀x(xa ↔xc), cb, da ∧ea ⇒de
(⇒→) ∀x(xa ↔xc), cb ⇒da ∧ea →de
(⇒∀) ∀x(xa ↔xc), cb ⇒∀xy(xa ∧ya →xy)
By three cuts with ∃x(xa), ∀x(xa →xb), ∀xy(xa ∧ya →xy) ⇒ab and
contractions we obtain a proof of S := ∀x(xa ↔xc), cb ⇒ab. Then we ﬁnish in
the following way:
da, Γ ⇒Δ, dc
dc, Γ ⇒Δ, da
(⇒↔)
Γ ⇒Δ, da ↔dc
(⇒∀) Γ ⇒Δ, ∀x(xa ↔xc)
S
(Cut)
cb, Γ ⇒Δ, ab
ab, Γ ⇒Δ
(Cut)
cb, Γ ⇒Δ

548
A. Indrzejczak
Note that to prove derivability of (E) we need in fact the whole LA. We
elaborate on the strength of this rule in Sect. 5.
⊓⊔
4
Cut Elimination
The possibility of representing LA by means of these four rules makes GO a
calculus with desirable proof-theoretic properties. First of all note that for G
the cut elimination theorem holds. Since the only primitive rules for ε are all
one-sided, in the sense that principal formulae occur in the antecedents only, we
can easily extend this result to GO. We follow the general strategy of cut elim-
ination proofs applied originally for hypersequent calculi in Metcalfe, Olivetti
and Gabbay [13] but which works well also in the context of standard sequent
calculi (see Indrzejczak [5]). Such a proof has a particularly simple structure and
allows us to avoid many complexities inherent in other methods of proving cut
elimination. In particular, we avoid well known problems with contraction, since
two auxiliary lemmata deal with this problem in advance. Note ﬁrst that for GO
the following result holds:
Lemma 3 (Substitution). If ⊢k Γ ⇒Δ, then ⊢k Γ[a/b] ⇒Δ[a/b].
Proof. By induction on the height of a proof. Note that (E) may require similar
relettering like (∃⇒) and (⇒∀). Note that the proof provides the height-
preserving admissibility of substitution.
⊓⊔
Let us assume that all proofs are regular in the sense that every parameter
a which is fresh by side condition on the respective rule must be fresh in the
entire proof, not only on the branch where the application of this rule takes
place. There is no loss of generality since every proof may be systematically
transformed into a regular one by the substitution lemma. The following notions
are crucial for the proof:
1. The cut-degree is the complexity of cut-formula ϕ, i.e. the number of connec-
tives and quantiﬁers occurring in ϕ; it is denoted as dϕ.
2. The proof-degree (dD) is the maximal cut-degree in D.
Remember that the complexity of atomic formulae, and consequently of cut-
and proof-degree in case of atomic cuts, is 0. The proof of the cut elimination
theorem is based on two lemmata which successively make a reduction: ﬁrst on
the height of the right, and then on the height of the left premiss of cut. ϕk, Γ k
denote k > 0 occurrences of ϕ, Γ, respectively.
Lemma 4 (Right reduction). Let D1 ⊢Γ ⇒Δ, ϕ and D2 ⊢ϕk, Π ⇒Σ with
dD1, dD2 < dϕ, and ϕ principal in Γ ⇒Δ, ϕ, then we can construct a proof D
such that D ⊢Γ k, Π ⇒Δk, Σ and dD < dϕ.

Le´sniewski’s Ontology - Proof-Theoretic Characterization
549
Proof. By induction on the height of D2. The basis is trivial, since Γ ⇒Δ, ϕ
is identical with Γ k, Π ⇒Δk, Σ. The induction step requires examination of
all cases of possible derivations of ϕk, Π ⇒Σ, and the role of the cut-formula
in the transition. In cases where all occurrences of ϕ are parametric we simply
apply the induction hypotheses to the premisses of ϕk, Π ⇒Σ and then apply
the respective rule – it is essentially due to the context independence of almost
all rules and the regularity of proofs, which together prevent violation of side
conditions on eigenvariables. If one of the occurrences of ϕ in the premiss(es) is
a side formula of the last rule we must additionally apply weakening to restore
the missing formula before the application of the relevant rule.
In cases where one occurrence of ϕ in ϕk, Π ⇒Σ is principal we make use of
the fact that ϕ in the left premiss is also principal; for the cases of contraction
and weakening it is trivial. Note that due to condition that ϕ is principal in the
left premiss it must be compound, since all rules introducing atomic formulae
as principal are working only in the antecedents. Hence all cases where one
occurrence of atomic ϕ in the right premiss would be introduced by means
of (R), (S), (T), (E) are not considered in the proof of this lemma. The only
exceptions are axiomatic sequents Γ ⇒Δ, ϕ with principal atomic ϕ, but they
do not make any harm.
⊓⊔
Lemma 5 (Left reduction). Let D1 ⊢Γ ⇒Δ, ϕk and D2 ⊢ϕ, Π ⇒Σ with
dD1, dD2 < dϕ, then we can construct a proof D such that D ⊢Γ, Πk ⇒Δ, Σk
and dD < dϕ.
Proof. By induction on the height of D1 but with some important diﬀerences.
First note that we do not require ϕ to be principal in ϕ, Π ⇒Σ so it includes
the case with ϕ atomic. In all these cases we just apply the induction hypothesis.
This guarantees that even if an atomic cut formula was introduced in the right
premiss by one of the rules (R), (S), (T), (E) the reduction of the height is done
only on the left premiss, and we always obtain the expected result. Now, in cases
where one occurrence of ϕ in Γ ⇒Δ, ϕk is principal we ﬁrst apply the induction
hypothesis to eliminate all other k −1 occurrences of ϕ in premisses and then
we apply the respective rule. Since the only new occurrence of ϕ is principal we
can make use of the right reduction lemma again and obtain the result, possibly
after some applications of structural rules.
⊓⊔
Now we are ready to prove the cut elimination theorem:
Theorem 3. Every proof in GO can be transformed into cut-free proof.
Proof. By double induction: primary on dD and subsidiary on the number of
maximal cuts (in the basis and in the inductive step of the primary induction).
We always take the topmost maximal cut and apply Lemma 5 to it. By successive
repetition of this procedure we diminish either the degree of a proof or the
number of cuts in it until we obtain a cut-free proof.
⊓⊔
As a consequence of the cut elimination theorem for GO we obtain:

550
A. Indrzejczak
Corollary 1. If ⊢Γ ⇒Δ, then it is provable in a proof which is closed under
subformulae of Γ ∪Δ and atomic formulae.
So cut-free GO satisﬁes the form of the subformula property which holds for
several elementary theories as formalised by Negri and von Plato [14].
5
Modiﬁcations
Construction of rules which are deductively equivalent to axioms may be to
some extent automatised (see e.g. Negri and von Plato [14], Bra¨uner [1], or
Marin, Miller, Pimentel and Volpe [12]). Still, even the choice of the version of
(equivalent) axiom which will be used for transformation, may have an impact
on the quality of obtained rules. Moreover, very often some additional tuning is
necessary to obtain rules, which are well-behaved from the proof-theoretic point
of view. In this section we will focus brieﬂy on this problem and sketch some
alternatives.
In our adequacy proofs we referred to the original formulation of LA, since
rules (R), (T), (S) correspond directly in a modular way to three conjuncts of
LA→. Our rule (E) however, is modelled not on LA←but rather on the suitable
implication of variant 3 of LA from Lemma 1. As a ﬁrst approximation we can
obtain the rule:
Γ⇒Δ, ∃z(∀v(va ↔vz) ∧zb)
Γ ⇒Δ, ab
which after further decomposition and quantiﬁer elimination yields:
da, Γ⇒Δ, dc
dc, Γ⇒Δ, da
Γ ⇒Δ, cb
Γ ⇒Δ, ab
(where d is a new parameter) which is very similar to (E) but with some active
atoms in the succedents. This is troublesome for proving cut elimination if ab
is a cut formula and a principal formula of (R), (S) or (T) in the right premiss
of cut. Fortunately, (E) is interderivable with this rule (it follows from the rule
generation theorem in Indrzejczak [5]) and has the principal formula in the
antecedent.
It is clear that if we focus on other variants then we can obtain diﬀerent rules
by their decomposition. In eﬀect note that instead of (E) we may equivalently
use the following rules based directly on LA, or on variants 2 and 1 respectively:
(ELA) da, Γ⇒Δ, db
da, ea, Γ⇒Δ, de
ab, Γ ⇒Δ
ca, Γ ⇒Δ
(E2) da, Γ⇒Δ, dc
da, Γ⇒Δ, cd
ab, Γ ⇒Δ
ca, cb, Γ ⇒Δ
(E1) da, ea, Γ⇒Δ, de
ab, Γ ⇒Δ
ca, cb, Γ, ⇒Δ
where d, e are new parameters (eigenvariables).
Note, that each of these rules, used instead of (E), yields a variant of GO for
which we can also prove cut elimination. However, as we will show by the end

Le´sniewski’s Ontology - Proof-Theoretic Characterization
551
of this section, (E) seems to be optimal. Perhaps, the last one is the most eco-
nomical in the sense of branching factor. However, since its left premiss directly
corresponds to the condition ∀xy(xa ∧ya →xy) it introduces two diﬀerent new
parameters to premisses which makes it more troublesome in some respects. In
fact, if we want to reduce the branching factor it is possible to replace all these
rules by the following variants:
(E′) da, Γ⇒Δ, dc
dc, Γ⇒Δ, da
cb, Γ ⇒Δ, ab
(E′
LA) da, Γ⇒Δ, db
da, ea, Γ⇒Δ, de
ca, Γ ⇒Δ, ab
(E′
2) da, Γ⇒Δ, dc
da, Γ⇒Δ, cd
ca, cb, Γ ⇒Δ, ab
(E′
1)
da, ea, Γ⇒Δ, de
ca, cb, Γ ⇒Δ, ab
with the same proviso on eigenvariables d, e. Their interderivability with the
rules stated ﬁrst is easily obtained by means of the rule generation theorem too.
These rules seem to be more convenient for proof search. However, for these
primed rules cut elimination cannot be proved in the constructive way, for the
reasons mentioned above, and it is an open problem if cut-free systems with
these rules as primitive are complete.
We ﬁnish this section with stating the last reason for choosing (E). Let us
explain why (E), the most complicated speciﬁc rule of GO, was claimed to be
connected with extensionality. Consider the following two principles:
WE ∀x(xa ↔xb) →∀x(ax ↔bx)
WExt ∀x(xa ↔xb) →∀x(ϕ(x, a) ↔ϕ(x, b))
where ϕ(x, a) denotes arbitrary formula with at least one occurrence of x (not
bound by any quantiﬁer within ϕ) and a.
Lemma 6. WE is equivalent to WExt.
Proof. That WE follows from WExt is obvious since the former is a speciﬁc
instance of the latter. The other direction is by induction on the complexity of
ϕ. In the basis there are just two cases: ϕ(x, a) is either xa or ax; the former is
trivial and the latter is just WE. The induction step goes like an ordinary proof
of the extensionality principle in FOL.
⊓⊔
Lemma 7. In G (E) is equivalent to (WE).
Proof. Note ﬁrst that in G the following sequents are provable:
– ∀x(ax ↔cx), cb ⇒ab
– ∀x(xa ↔xc), da ⇒dc
– ∀x(xa ↔xc), dc ⇒da

552
A. Indrzejczak
we will use them in the proofs to follow.
For derivability of (E):
da, Γ ⇒Δ, dc
dc, Γ ⇒Δ, da
(⇒↔)
Γ ⇒Δ, da ↔dc
(⇒∀) Γ ⇒Δ, ∀x(xa ↔xc)
D
(Cut)
Γ ⇒Δ, ∀x(ax ↔cx)
∀x(ax ↔cx), cb ⇒ab
(Cut)
cb, Γ ⇒Δ, ab
where D is a proof of ∀x(xa ↔xc) ⇒∀x(ax ↔cx) from WE and the rightmost
sequent is provable. The endsequent by cut with ab, Γ ⇒Δ yields the conclusion
of (E).
Provability of WE in G with (E):
∀x(xa ↔xc), da ⇒dc
∀x(xa ↔xc), dc ⇒da
ab ⇒ab
(E)
∀x(xa ↔xc), cb ⇒ab
In the same way we prove ∀x(xa ↔xc), ab ⇒cb which by (⇒↔), (⇒∀) and
(⇒→) yields WE.
⊓⊔
This shows that we can obtain the axiomatization of elementary ontology
by means of LA→and WE (or WExt). Also instead of LA→we can use three
axioms corresponding to our three rules (R), (S), (T). Note that if we get rid
of (E) (or WE) we obtain a weaker version of ontology investigated by Takano
[18]. If we get rid of quantiﬁer rules we obtain a quantiﬁer-free version of this
system investigated by Ishimoto and Kobayashi [6].
On the basis of the speciﬁc features of sequent calculus we can obtain here
for free also the intuitionistic version of ontology. As is well known it is suﬃcient
to restrict the rules of G to sequents having at most one formula in the succedent
(which requires small modiﬁcations like replacement of (↔⇒) and (⇒∨) with
two variants having always one side formula in the succedent) to obtain the
version adequate for the intuitionistic FOL. Since all speciﬁc rules for ε can be
restricted in a similar way, we can obtain the calculus GIO for the intuitionistic
version of elementary ontology. One can easily check that all proofs showing the
adequacy of GO and the cut elimination theorem are either intuitionistically
correct or can be easily changed into such proofs. The latter remark concerns
these proofs in which the classical version of (↔⇒) required the introduction of
the second side formula into succedent by (⇒W); the intuitionistic two versions
of (↔⇒) do not require this step.
6
Extensions
Le´sniewski and his followers were often working on ontology enriched with deﬁ-
nitions of special predicates and name-creating functors. In this section we focus

Le´sniewski’s Ontology - Proof-Theoretic Characterization
553
on a number of unary and binary predicates which are popular ontological con-
stants. Instead of adding these deﬁnitions to GO we will introduce predicates
by means of sequent rules satisfying conditions formulated for well-behaved SC
rules. Let us call Lp the language of Lo enriched with all these predicates and
GOP, the calculus with the additional rules for predicates. The deﬁnitions of the
most important unary predicates are:
Da := ∃x(xa)
V a := ¬∃x(xa)
Sa := ∃x(ax)
Ga := ∃xy(xa ∧ya ∧¬xy)
D, V, S, G are unary predicates informing that a is denoting, empty (or void),
singular or general. D and S are Le´sniewski’s ex and ob respectively. He preferred
also to apply sol(a) which we symbolize with U (for unique):
Ua := ∀xy(xa ∧ya →xy) [or simply ¬Ga]
The additional rules for these predicates are of the form:
(D ⇒)
ba, Γ⇒Δ
Da, Γ⇒Δ
(⇒D)
Γ⇒Δ, ca
Γ⇒Δ, Da
(S ⇒)
ab, Γ⇒Δ
Sa, Γ⇒Δ
(⇒S)
Γ⇒Δ, ac
Γ⇒Δ, Sa
(V ⇒)
Γ⇒Δ, ca
V a, Γ⇒Δ
(⇒V )
ba, Γ⇒Δ
Γ⇒Δ, V a
where b is new and c arbitrary in all schemata.
(G ⇒) ba, ca, Γ⇒Δ, bc
Ga, Γ⇒Δ
(⇒G) Γ⇒Δ, da
Γ⇒Δ, ea
de, Γ ⇒Δ
Γ⇒Δ, Ga
(⇒U) ba, ca, Γ⇒Δ, bc
Γ⇒Δ, Ua
(U ⇒) Γ⇒Δ, da
Γ⇒Δ, ea
de, Γ ⇒Δ
Ua, Γ⇒Δ
where b, c are new, and d, e are arbitrary parameters.
The binary predicates of identity, (weak and strong) coextensiveness, nonbe-
ing b, subsumption and antysubsumption are deﬁned in the following way:
a = b := ab ∧ba
a¯εb := aa ∧¬ab
a ≡b := ∀x(xa ↔xb) a ⊂b := ∀x(xa →xb)
a ≈b := a ≡b ∧Da
a ⊈b := ∀x(xa →¬xb)
Finally note that Aristotelian categorical sentences can be also deﬁned in
Le´sniewski’s ontology:
aAb := a ⊂b ∧Da aEb := a ⊈b ∧Da
aIb := ∃x(xa ∧xb) aOb := ∃x(xa ∧¬xb)
The rules for binary predicates:
(=⇒) ab, ba, Γ ⇒Δ
a = b, Γ ⇒Δ
(⇒=) Γ ⇒Δ, ab
Γ ⇒Δ, ba
Γ ⇒Δ, a = b
(≡⇒) Γ ⇒Δ, ca, cb
ca, cb, Γ ⇒Δ
a ≡b, Γ ⇒Δ
(⇒≡) da, Γ ⇒Δ, db
db, Γ ⇒Δ, da
Γ ⇒Δ, a ≡b
(≈⇒) da, Γ ⇒Δ, ca, cb
ca, cb, da, Γ ⇒Δ
a ≈b, Γ ⇒Δ

554
A. Indrzejczak
(⇒≈) da, Γ ⇒Δ, db
db, Γ ⇒Δ, da
Γ ⇒Δ, ca
Γ ⇒Δ, a ≈b
(¯ε ⇒) aa, Γ ⇒Δ, ab
a¯εb, Γ ⇒Δ
(⇒¯ε) Γ ⇒Δ, aa
ab, Γ ⇒Δ
Γ ⇒Δ, a¯εb
(⊂⇒) Γ ⇒Δ, ca
cb, Γ ⇒Δ
a ⊂b, Γ ⇒Δ
(⇒⊂) da, Γ ⇒Δ, db
Γ ⇒Δ, a ⊂b
(⊈⇒) Γ ⇒Δ, ca
Γ ⇒Δ, cb
a ⊈b, Γ ⇒Δ
(⇒⊈) da, db, Γ ⇒Δ
Γ ⇒Δ, a ⊈b
(A ⇒) da, Γ ⇒Δ, ca
cb, da, Γ ⇒Δ
aAb, Γ ⇒Δ
(⇒A) da, Γ ⇒Δ, db
Γ ⇒Δ, ca
Γ ⇒Δ, aAb
(E ⇒) da, Γ ⇒Δ, ca
da, Γ ⇒Δ, cb
aEb, Γ ⇒Δ
(⇒E) da, db, Γ ⇒Δ
Γ ⇒Δ, ca
Γ ⇒Δ, aEb
(I ⇒) da, db, Γ ⇒Δ
aIb, Γ ⇒Δ
(⇒I) Γ ⇒Δ, ca
Γ ⇒Δ, cb
Γ ⇒Δ, aIb
(O ⇒) da, Γ ⇒Δ, db
aOb, Γ ⇒Δ
(⇒O) Γ ⇒Δ, ca
cb, Γ ⇒Δ
Γ ⇒Δ, aOb
where d is new and c arbitrary (but c can be identical to d in rules for ≈, A, E).
Proofs of interderivability with equivalences corresponding to suitable deﬁ-
nitions are trivial in most cases. We provide only one for the sake of illustration.
The hardest case is ≈.
da, ca ⇒ca, cb
da, ca, cb ⇒cb
(≈⇒)
a ≈b, ca ⇒cb
da, ca ⇒ca, cb
da, ca, cb ⇒ca
a ≈b, cb ⇒ca
(⇒↔)
a ≈b ⇒ca ↔cb
(⇒∀) a ≈b ⇒∀x(xa ↔xb)
and
ca ⇒ca, aa, ab
(⇒∃) ca ⇒∃x(xa), aa, ab
ca, aa, ab ⇒ca
(⇒∀) ca, aa, ab ⇒∃x(xa)
(≈⇒)
a ≈b ⇒∃x(xa)
by (⇒∧) yield one part. For the second:
∀x(xa ↔xb), da ⇒db
∀x(xa ↔xb), db ⇒da
ca ⇒ca
(⇒≈)
∀x(xa ↔xb), ca ⇒a ≈b
(∃⇒) ∀x(xa ↔xb), ∃x(xa) ⇒a ≈b
(∧⇒) ∀x(xa ↔xb) ∧∃x(xa) ⇒a ≈b
where the left and the middle premiss are obviously provable by means of (∀⇒),
(↔⇒). We omit proofs of the derivability of both rules in GO enriched with the
axiom ⇒∀x(xa ↔xb) ∧∃x(xa) ↔a ≈b.
We treat all these predicates as new constants hence their complexity is ﬁxed
as 1, in contrast to atomic formulae, which are of complexity 0. Of course we
can consider ontology with an arbitrary selection of these predicates according

Le´sniewski’s Ontology - Proof-Theoretic Characterization
555
to the needs. Accordingly we can enrich GO also with arbitrary selection of
suitable rules for predicates. All the results holding for GOP are correct for any
subsystem. Let us list some important features of these rules and enriched GO:
1. All rules for predicates are explicit, separate and symmetric, which are usual
requirements for well-behaved rules in sequent calculi (see e.g. [5]). In this
respect they are similar to the rules for logical constants and diﬀer from spe-
ciﬁc rules for ε which are one-sided (in the sense of having principal formulae
always in the antecedent).
2. All these new rules satisfy the subformula property in the sense that side
formulae are only atomic.
3. The substitution lemma holds for GO with any combination of the above
rules.
4. All rules are pairwise reductive, modulo substitution of terms,
We do not prove the substitution lemma, since the proof is standard, but
we comment on the last point, since cut elimination holds due to 3 and 4. The
notion of reductivity for sequent rules was introduced by Ciabattoni [2] and it
may be roughly deﬁned as follows: A pair of introduction rules (⇒⋆), (⋆⇒) for
a constant ⋆is reductive if an application of cut on cut formulae introduced by
these rules may be replaced by the series of cuts made on less complex formulae,
in particular on their subformulae. Basically it enables the reduction of cut-
degree in the proof of cut elimination. Again we illustrate the point with respect
to the most complicated case. Let us consider the application of cut with the
cut formula a ≈b, then the left premiss of this cut was obtained by:
ca, Γ ⇒Δ, cb
cb, Γ ⇒Δ, ca
Γ ⇒Δ, da
(⇒≈)
Γ ⇒Δ, a ≈b
where c is new and d is arbitrary. And the right premiss was obtained by:
ea, Π ⇒Σ, fa, fb
ea, fa, fb, Π ⇒Σ
(≈⇒)
a ≈b, Π ⇒Σ
where e is new and f is arbitrary.
By the substitution lemma on the premisses of (⇒≈), (≈⇒) we obtain:
1. fa, Γ ⇒Δ, fb
2. fb, Γ ⇒Δ, fa
3. da, Π ⇒Σ, fa, fb
4. da, fa, fb, Π ⇒Σ
and we can derive:
Γ ⇒Δ, da
da, Π ⇒Σ, fa, fb
(Cut)
Γ, Π ⇒Δ, Σ, fa, fb
fb, Γ ⇒Δ, fa
(Cut)
Γ, Γ, Π ⇒Δ, Δ, fa, fa
(C)
Γ, Π ⇒Δ, Σ, fa
D
(Cut)
Γ, Γ, Π, Π ⇒Δ, Δ, Σ, Σ
(C)
Γ, Π ⇒Δ, Σ

556
A. Indrzejczak
where D is a similar proof of fa, Γ, Π ⇒Δ, Σ from Γ ⇒Δ, da, 4 and 1 by cuts
and contractions. All cuts are of lower degree than the original cut. It is routine
exercise to check that all rules for predicates are reductive and this is suﬃcient
for proving Lemma 4 and 5 for GOP. As a consequence we obtain:
Theorem 4. Every proof in GOP can be transformed into cut-free proof.
Since the rules are modular this holds for every subsystem based on a selec-
tion of the above rules.
7
Conclusion
Both the basic system GO and its extension GOP are cut-free and satisfy a form
of the subformula property. It shows that Le´sniewski’s ontology admits standard
proof-theoretical study and allows us to obtain reasonable results. In particular,
we can prove for GO the interpolation theorem using the Maehara strategy
(see e.g. [19]) and this implies for GO other expected results like e.g. Beth’s
deﬁnability theorem. Space restrictions forbid to present it here. On the other
hand, we restricted our study to the system with simple names only, whereas
fuller study should cover also complex names built with the help of several name-
forming functors. The typical ones are the counterparts of the well-known class
operations deﬁnable in Le´sniewski’s ontology in the following way:
a¯b := aa ∧¬ab
a(b ∩c) := ab ∧ac
a(b ∪c) := ab ∨ac
It is not a problem to provide suitable rules corresponding to these deﬁnitions:
(−⇒) aa, Γ ⇒Δ, ab
a¯b, Γ ⇒Δ
(⇒−) ab, Γ ⇒Δ
Γ ⇒Δ, aa
Γ ⇒Δ, a¯b
(∩⇒)
ab, ac, Γ ⇒Δ
a(b ∩c), Γ ⇒Δ
(⇒∩) Γ ⇒Δ, ab
Γ ⇒Δ, ac
Γ ⇒Δ, a(b ∩c)
(∪⇒) ab, Γ ⇒Δ
ac, Γ ⇒Δ
a(b ∪c), Γ ⇒Δ
(⇒∪)
Γ ⇒Δ, ab, ac
Γ ⇒Δ, a(b ∪c)
Although their structure is similar to the rules provided for predicates in the
last section, their addition raises important problems. One is of a more general
nature and well-known: deﬁnitions of term-forming operations in ontology are
creative. Although it was intended in the original architecture of Le´sniewski’s
systems, in the modern approach this is not welcome. Iwanu´s [7] has shown that
the problem can be overcome by enriching elementary ontology with two axioms
corresponding to special versions of the comprehension axiom but this opens a
problem of derivability of these axioms in GO enriched with special rules.
There is also a speciﬁc problem with cut elimination for GO with added
complex terms and suitable rules. Even if they are reductive (and the rules
stated above are reductive, as a reader can check), we run into a problem with
quantiﬁer rules. If unrestricted instantiation of terms is admitted in (⇒∃), (∀⇒)
the subformula property is lost. One can ﬁnd some solutions for this problem,
for example by using two separated measures of complexity for formula-makers

Le´sniewski’s Ontology - Proof-Theoretic Characterization
557
and term-makers (see e.g. [3]), or by restricting in some way the instantiation
of terms in respective quantiﬁer rules (see e.g. [4]). The examination of these
possibilities is left for further study.
The last open problem deserving careful study is the possibility of application
for automated proof-search and obtaining semi-decision procedures (or decision
procedures for quantiﬁer-free subsystems) on the basis of the provided sequent
calculus. In particular, due to modularity of provided rules, one could obtain in
this way decision procedures for several quantiﬁer-free subsystems investigated
by Pietruszczak [15], or by Ishimoto and Kobayashi [6].
Acknowledgements. The research is supported by the National Science Centre,
Poland (grant number: DEC-2017/25/B/HS1/01268). I am also greatly indebted to
Nils K¨urbis for his valuable comments.
References
1. Bra¨uner, T.: Hybrid Logic and its Proof-Theory. Springer, Cham (2011). https://
doi.org/10.1007/978-94-007-0002-4
2. Ciabattoni, A.: Automated generation of analytic calculi for logics with linearity.
In: Marcinkowski, J., Tarlecki, A. (eds.) CSL 2004. LNCS, vol. 3210, pp. 503–517.
Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-30124-0 38
3. Indrzejczak, A.: Fregean description theory in proof-theoretic setting. Logic Log.
Philos. 28(1), 137–155 (2019)
4. Indrzejczak, A.: Free logics are cut-free. Stud. Log. 109(4), 859–886 (2021)
5. Indrzejczak, A.: Sequents and Trees. An Introduction to the Theory and Applica-
tions of Propositional Sequent Calculi. Birkh¨auser (2021)
6. Ishimoto, A., Kobayashi, M.: A propositional fragment of Le´sniewski’s ontology
and its formulation by the tableau method. Stud. Log. 41(2–3), 181–196 (1982)
7. Iwanu´s, B.: On Le´sniewski’s elementary ontology. Stud. Log. 31(1), 73–119 (1973)
8. K¨ung, G., Canty, J.T.: Substitutional quantiﬁcation and Le´sniewskian quantiﬁers.
Theoria 36, 165–182 (1970)
9. Le´sniewski, S.: ¨Uber Funktionen, deren Felder Gruppen mit R¨ucksicht auf diese
Funktionen sind. Fundam. Math. 13, 319–332 (1929). [English translation. In:
Le´sniewski [11]]
10. Le´sniewski, S.: ¨Uber Funktionen, deren Felder Abelsche Gruppen in Bezug auf
diese Funktionen sind. Fundam. Math. 14, 242–251 (1929). [English translation.
In: Le´sniewski [11]]
11. Le´sniewski, S.: Collected Works, vol. II. Surma, S., Srzednicki, J., Barnett, D.I.
Kluwer/PWN (1992)
12. Marin, S., Miller, D., Pimentel, E., Volpe, M.: From axioms to synthetic inference
rules via focusing. Ann. Pure Appl. Log. 173(5), 103091 (2022)
13. Metcalfe, G., Olivetti, N., Gabbay, D.: Proof Theory for Fuzzy Logics. Springer,
Heidelberg (2008). https://doi.org/10.1007/978-1-4020-9409-5
14. Negri, S., von Plato, J.: Structural Proof Theory. Cambridge University Press,
Cambridge (2001)
15. Pietruszczak, A.: Quantiﬁer-free Calculus of Names. Systems and Metatheory,
Toru´n (1991). [in Polish]
16. Rickey, F.: Interpretations of Le´sniewski’s ontology. Dialectica 39(3), 181–192
(1985)

558
A. Indrzejczak
17. Slupecki, J.: S. Le´sniewski’s calculus of names. Stud. Log. 3(1), 7–72 (1955)
18. Takano, M.: A semantical investigation into Le´sniewski’s axiom and his ontology.
Stud. Log. 44(1), 71–78 (1985)
19. Troelstra, A.S., Schwichtenberg, H.: Basic Proof Theory. Oxford University Press,
Oxford (1996)
20. Urbaniak, R.: Le´sniewski’s Systems of Logic and Foundations of Mathematics.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-00482-2
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Bayesian Ranking for Strategy Scheduling
in Automated Theorem Provers
Chaitanya Mangla(B)
, Sean B. Holden
, and Lawrence C. Paulson
Computer Laboratory, University of Cambridge, Cambridge, England
{cm772,sbh11,lp15}@cl.cam.ac.uk
Abstract. A strategy schedule allocates time to proof strategies that
are used in sequence in a theorem prover. We employ Bayesian statistics
to propose alternative sequences for the strategy schedule in each proof
attempt. Tested on the TPTP problem library, our method yields a time
saving of more than 50%. By extending this method to optimize the
ﬁxed time allocations to each strategy, we obtain a notable increase in
the number of theorems proved.
Keywords: Bayesian machine learning · Strategy scheduling ·
Automated theorem proving
1
Introduction
Theorem provers have wide-ranging applications, including formal veriﬁcation
of large mathematical proofs [9] and reasoning in knowledge-bases [37]. Thus,
improvements in provers that lead to more successful proofs, and savings in the
time taken to discover proofs, are desirable.
Automated theorem provers generate proofs by utilizing inference procedures
in combination with heuristic search. A speciﬁc conﬁguration of a prover, which
may be specialized for a certain class of problems, is termed a strategy. Provers
such as E [27] can select from a portfolio of strategies to solve the goal theorem.
Furthermore, certain provers hedge their allocated proof time across a number
of proof strategies by use of a strategy schedule, which speciﬁes a time allocation
for each strategy and the sequence in which they are used until one proves the
goal theorem. This method was pioneered in the Gandalf prover [33].
Prediction of the eﬀectiveness of a strategy prior to a proof attempt is usually
intractable or undecidable [12]. A practical implementation must infer such a
prediction by tractable approximations. Therefore, machine learning methods
for strategy invention, selection and scheduling are actively researched. Machine
learning methods for strategy selection conditioned on the proof goal have shown
promising results [3]. Good results have also been reported for strategy synthesis
using machine learning [1]. Work on machine learning for algorithm portfolios—
which allocate resources to multiple solvers simultaneously—is also relevant to
strategy scheduling because of its similar goals. For this purpose, Silverthorn
and Miikkulainen propose latent class models [31] .
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 559–577, 2022.
https://doi.org/10.1007/978-3-031-10769-6_33

560
C. Mangla et al.
In this work, we present a method for generating strategy schedules using
Bayesian learning with two primary goals: to reduce proving time or to prove
more theorems. We have evaluated this method for both purposes using iLean-
CoP, an intuitionistic ﬁrst-order logic prover with a compact implementation
and good performance [18]. Intuitionistic logic is a non-standard form of ﬁrst-
order logic, of which relatively little is known with regard to automation. It is
of interest in theoretical computer science and philosophy of mathematics [7].
Among intuitionistic provers, iLeanCoP is seen as impressive and is able to prove
a suﬃcient number of theorems in our benchmarks for signiﬁcance testing. Its
core is implemented in around thirty lines of Prolog; such simplicity adds clarity
to interpretations of our results. Our method was benchmarked on the Thou-
sands of Problems for Theorem Provers (TPTP) problem library [32], in which
we are able to save more than 50% on proof time when aiming for the former
goal. Towards the latter goal, we are able to prove notably more theorems.
Our two primary, complementary, contributions presented here are: ﬁrst, a
Bayesian machine learning model for strategy scheduling; and second, engineered
features for use in that model. The text below is organized as follows. In Sect. 2,
we introduce preliminary material used subsequently to construct a machine
learning model for strategy scheduling, described in Sects. 3–7. The data used to
train and evaluate this model are described in Sect. 8, followed by experiments,
results and conclusions in Sects. 9–12.
2
Distribution of Permutations
We model a strategy schedule using a vector of strategies, and thus all schedules
are permutations of the same.
Deﬁnition 1 (Permutation). Let M ∈N. A permutation π ∈NM is a vector
of indices, with πi ∈{1, . . . , M} and ∀i ̸= j : πi ̸= πj, representing a reordering
of the components of an M-dimensional vector s to [sπ1, sπ2, . . . , sπM ]⊺.
In this text, vector-valued variables, such as π above, are in boldface, which
must change when they are indexed, like π1 for example. For probabilistic mod-
elling of schedules represented using permutations, we use the Plakett-Luce
model [14,21] to deﬁne a parametric probability distribution over permutations.
Deﬁnition 2 (Plakett-Luce distribution). The Plakett-Luce distribution
Perm(λ) with parameter λ ∈RM
>0, has support over permutations of indices
{1, . . . , M}. For permutation Π distributed as Perm(λ),
Pr(Π = π; λ) =
M

j=1
λπj
M
u=j λπu
.
In latter sections, we use the parameter λ to assign an abstract ‘score’ to
strategies when modelling distributions over schedules. This score is particularly
useful due to the following theorem.

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
561
Theorem 1. Let π∗be a mode of the distribution Perm(λ), that is
π∗= argmax
π
Pr(π; λ).
Then, λπ∗
1 ⩾λπ∗
2 ⩾λπ∗
3 ⩾. . . ⩾λπ∗
M .
Thus, assuming λ is a vector of the score of each strategy, the highest probability
permutation indexes the strategies in decreasing order of scores. Conversely, the
highest probability permutation can be obtained eﬃciently by sorting the indices
of λ with respect to their corresponding values in decreasing order. Cao et al. [4]
have presented a proof of Theorem 1, and Cheng et al. [5] have discussed some
further interesting details.
Example 1. Let λ = [1, 9]⊺, π(1) = [1, 2]⊺and π(2) = [2, 1]⊺. Then,
Pr(Π = π(1); λ) =
λπ (1)
1
λπ (1)
1
+ λπ (1)
2
·
λπ (1)
2
λπ (1)
2
=
1
1 + 9 · 9
9 = 1
10.
Similarly, Pr(Π = π(2); λ) = 9/10.
⊓⊔
Theorem 2. Perm(cλ) = Perm(λ), for any scalar constant c > 0.
In other words, the Plakett-Luce distribution is invariant to the scale of the
parameter vector.
Lemma 1. Perm(exp(λ + c)) = Perm(exp(λ)), for any scalar constant c ∈R.
Lemma 1 follows from Theorem 2, and shows the same distribution is translation
invariant if the parameter is exponentiated. Cao et al. [4] give proofs of both.
3
A Maximum Likelihood Model
We model a strategy schedule as a ranking of known strategies, where each strat-
egy is constructed by a parameter setting and time allocation. A ranking therein
is a permutation of strategies, with each strategy retaining its time allocation
irrespective of the ordering. We construct, in this section, a model for inference
of such permutations that is linear in the parameters.
Suppose we have a repository of N theorems which we test against each of
our M known strategies to build a data-set D = {(π(i), x(i))}
N
i=1, where π(i)
is a desirable ordering of strategies for theorem i and x(i) is a feature vector
representation of the theorem. In Sect. 9, we detail how we instantiated D for
our experiments, which serves as an example for any other implementation. We
assume that π(i) has Plakett-Luce distribution conditioned on x(i) such that
Pr(π; x, ω) = Perm(Λ(x, ω)),
(1)
where ω is a parameter the model must learn and Λ(·) is a vector-valued function
of range RM
>0. We use the notation Λ(·)i to index into the value of Λ(·). We

562
C. Mangla et al.
represent our prover strategies with feature vectors {d(j)}
M
j=1. To calculate the
score of strategy j using Λ(·)j, we specify
Λ(x(i), ω)j = exp (φ(x(i), d(j))
⊺ω)
(2)
to ensure that the scores are positive valued, where φ is a suitable basis expansion
function. Assuming the data is i.i.d, the likelihood of the parameter vector is
given by
L(ω) = p(D; ω) =
N

i=1
Pr(π(i); Λ(x(i), ω)).
(3)
An ˆω that maximizes this likelihood can then be used to forecast the distri-
bution over permutations for a new theorem x∗by evaluating Perm(Λ(x∗, ˆω))
for all permutations. This would incur factorial complexity; however, we are
often only interested in the most likely permutation, which can be retrieved in
polynomial time. Speciﬁcally for strategy scheduling the permutation with the
highest predicted probability should reﬂect the orderings in the data. For this
purpose, we use Theorem 1 to ﬁnd the highest probability permutation π∗by
sorting the values of {Λ(x∗, ˆω)j}M
j=1 in descending order.
Remark 1. A method named ListNet designed to rank documents for search
queries using the Plakett-Luce distribution is evaluated by Cao et al. [4]. Their
evaluation uses a linear basis expansion. We can derive a similar construction in
our model by setting
φ(x(i), d(j)) = [x(i)⊺, d(j)⊺]
⊺.
(4)
Remark 2. The likelihood in Equation (3) can be maximized by minimizing the
negative log likelihood ℓ(ω) = −log L(ω), which (as shown by Sch¨afer and
H¨ullermeier [26]) is convex and therefore can be minimized using gradient-based
methods. The minima may, however, be unidentiﬁable due to translation invari-
ance, as demonstrated by Lemma 1. This problem is eliminated in our Bayesian
model by the use of a Gaussian prior, as explained in Sect. 4.
Example 2. Let there be N = 2 theorems and M = 2 strategies. Let the the-
orems and strategies be characterized by univariate values such that x(1) = 1,
x(2) = 2, d(1) = 1 and d(2) = 2.
Suppose strategy d(1) is ideal for theorem x(1) and strategy d(2)
for x(2), as shown on the right, where a + indicates the preferred
strategy.
d(1) d(2)
x(1) +
−
x(2) −
+
This is evidently an example of a parity problem [34], and hence cannot
be modelled by a simple linear expansion using the basis function mentioned
in Remark 1. A solution in this instance is to use
φ(x(i), d(j)) = x(i) · d(j).
The parameter ω is then one-dimensional, and the required training data takes
the form D = {([1, 2]⊺, 1), ([2, 1]⊺, 2)}. We ﬁnd that L(w) is convex, with maxima
at ˆω = 0.42 as shown in Fig. 1.
⊓⊔

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
563
ˆω = 0.42
0.0
0.1
0.2
-5
-2.5
0 ˆω
2.5
5
L(ω)
Fig. 1. The likelihood function in Example 2.
4
Bayesian Inference
We place a Gaussian prior distribution on the parameter ω of the model
described in Sect. 3. This has two advantages: ﬁrst, the posterior mode is iden-
tiﬁable, as noted by Johnson et al. [11] and demonstrated in Example 3 on page
7; second, the parameter is regularized. With this prior speciﬁed as the normal
distribution
ω ∼N(m0, S0),
(5)
and assuming π is independent of D given (x, ω), the posterior predictive dis-
tribution is
p(π|x∗, D) =

p(π|x∗, ω)p(ω|D)dω,
which may be approximated by sampling from the posterior,
ωs ∼p(ω|D),
(6)
to obtain
p(π|x∗, D) ≈1
S
S

s=1
p(π|x∗, ωs).
(7)
Given a new theorem x∗, to ﬁnd the permutation of strategies with the highest
probability of success, using the approximation above would require its evalu-
ation for every permutation of π. This process incurs factorial complexity. We
instead make a Bayes point approximation [16] using the mean values of the
samples such that,
p(π|x∗, D) ≈p(π|x∗, ⟨ωs⟩)
using Eq. (7)
= Pr(π|Λ(x∗, ⟨ωs⟩)) using Eq. (1),
where ⟨·⟩denotes mean value. The mean of the Plakett-Luce parameter for
Bayesian inference has been used in prior work [8] to obtain good results. Fur-
thermore, using that, the highest probability permutation can be obtained by
using Theorem 1, thereby incurring only the cost of sorting the items. This
saving is substantial when generating a strategy schedule, because it saves on
prediction time, which is important for the following reason.

564
C. Mangla et al.
Algorithm 1. Metropolis-Hastings Algorithm
Suppose we have generated samples {ω(1), . . . , ω(i)} from the target distribution p.
Generate ω(i+1) as follows.
1: Generate candidate value ˙ω ∼q(ω(i)), where q is the proposal distribution.
2: Evaluate r ≡r(ω(i), ˙ω) where
r(x, y) = min
 p(y)
p(x)
q(x|y)
q(y|x), 1

.
3: Set
ω(i+1) =

˙ω
with probability r
ω(i)
with probability 1 −r.
Remark 3. While benchmarking and in typical use, a prover is allocated a ﬁxed
amount of time for a proof attempt, and any time taken to predict a strategy
schedule must be accounted for within this allocation. Time taken for this pre-
diction is time taken away from the prover itself which could have been invested
in the proof search. Therefore, it is essential to minimize schedule prediction
time. It is particularly wise to favour a saving in prediction time at the cost of
model optimization and training time.
Remark 4. In our implementation we set m0 = 0. This has the eﬀect of priori-
tizing smaller weights ω in the posterior. Furthermore, we set S0 = ηI, η ∈R,
where I is the identity matrix. Consequently, the hyperparameter η controls the
strength of the prior, since the entropy of the Gaussian prior scales linearly by
log |S0|.
Remark 5. A specialization of the Plakett-Luce distribution using the Thursto-
nian interpretation admits a Gamma distribution conjugate prior [8]. That, how-
ever, is unavailable to our model when parametrized as shown in Eq. (1).
5
Sampling
We use the Markov chain Monte Carlo (MCMC) Metropolis-Hastings algo-
rithm [38] to generate samples from the posterior distribution. In MCMC sam-
pling, one constructs a Markov chain whose stationary distribution matches the
target distribution p. For the Metropolis-Hastings algorithm, stated in Algo-
rithm 1, this chain is constructed using a proposal distribution y|x ∼q, where q
is set to a distribution that can be conveniently sampled from.
Note that while calculating r in Algorithm 1, the normalization constant of
the target density p cancels out. This is to our advantage; to generate samples
ωs from the posterior, which is, by Eq. (3) and Eq. (5),
p(ω|D) ∝p(D|ω)p(ω)
= L(ω)N(m0, S0),
(8)

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
565
the posterior only needs to be computed in this unnormalized form.
In this work, we choose a random walk proposal of the form
q(ω′|ω) = N(ω′|ω, Σq),
(9)
and tune Σq for eﬃcient sampling simulation. We start the simulation at a local
mode ˆω, and set N(ˆω, Σq) to approximate the local curvature of the posterior at
that point using methods by Rossi [25]. Speciﬁcally, our procedure for computing
Σq is as follows.
1. First, writing the posterior from Eq. (8) as
p(ω|D) = 1
Z e−E(ω),
where Z is the normalization constant, we have
E(ω) = −log L(ω) −log N(m0, S0).
(10)
We ﬁnd a local mode ˆω by optimizing E(ω) using a gradient-based method.
2. Then, using a Laplace approximation [2], we approximate the posterior in the
locality of this mode to
N(ˆω, H−1), where H = ∇∇E(ω)|ˆω
is the Hessian matrix of E(ω) evaluated at that local mode.
3. Finally, we set
Σq = s2 H−1
in Eq. (9), where s is used to tune all the length scales. We set this value to
s2 = 2.38 based on the results by Roberts and Rosenthal [24].
Remark 6. When calculating r in Algorithm 1 during sampling, to evaluate the
unnormalized posterior at any point ωs we compute it from Equation (10) as
exp(−E(ωs))—it is therefore the only form in which the posterior needs to be
coded in the implementation.
Example 3 (Gaussian Prior).
To demonstrate the eﬀect of using a Gaussian
prior, we build upon Example 2, with the data taking the form
D = {([1, 2]⊺, 1), ([2, 1]⊺, 2)}.
We perform basis expansion as explained in Sect. 6 with prior parameter η = 1.0,
kernel σ = 0.1 and ς = 2 centres. Thus, the model parameter is
ω = [ω1, ω2]⊺,
ω ∈R2.
The unnormalized negative log posterior E(ω1, ω2), as deﬁned in Eq. (10), is
shown in Fig. 2b; and the negative log likelihood ℓ(ω1, ω2) = −log L(ω1, ω2) as
mentioned in Remark 2, is shown in Fig. 2a. Note the contrast in the shape of the

566
C. Mangla et al.
two surfaces. The minimum is along the top-right portion in Fig. 2a, which is ﬂat
and leads to an unidentiﬁable point estimate, whereas in Fig. 2b, the minimum
is in a narrow region near the centre. The Gaussian prior, in informal terms, has
lifted the surface up, with an eﬀect that increases in proportion to the distance
from the origin.
⊓⊔
-5.0
-2.5
0.0
2.5
5.0
-5.0 -2.5
0.0
2.5
5.0
ω1
2
(·)
(0, 3]
(3, 5]
(5, 7]
(7, 15]
(15, 30]
(30, 40]
(a) Likelihood function (ω1, ω2)
-5.0
-2.5
0.0
2.5
5.0
-5.0 -2.5 0.0
2.5
5.0
ω1
ω2
E(·)
(0, 3]
(3, 5]
(5, 7]
(7, 15]
(15, 30]
(30, 40]
(40, 50]
(50, 70]
(b) Posterior function E(ω1, ω2)
Fig. 2. Comparison of the shape of the likelihood and the posterior functions.
6
Basis Expansion
Example 2 shows how the linear expansion in Remark 1 is ineﬀective even in very
simple problem instances. The maximum likelihood bilinear model presented by
Sch¨afer and H¨ullermeier [26] is related to our model deﬁned in Sect. 2 with
the basis performing the Kronecker (tensor) product φ(x, d) = x ⊗d. Their
results show such an expansion produces a competitive model, but falls behind
in comparison to their non-linear model.
To model non-linear interactions between theorems and strategies, we use a
Gaussian kernel for the basis expansion.
Deﬁnition 3 (Gaussian Kernel). A Gaussian kernel κ is deﬁned by
κ(y, z) = exp

−∥y −z∥2
2σ2

,
for σ > 0.
The Gaussian kernel κ(y, z) eﬀectively represents the inner product of y and
z in a Hilbert space whose bandwidth is controlled by σ. Smaller values of σ
correspond to a higher bandwidth, more ﬂexible, inner product space. Larger
values of σ will reduce the kernel to a constant function, as detailed in [30].
For our ranking model, we must tune σ to balance between over-ﬁtting and
under-performance.

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
567
We use the Gaussian kernel for basis expansion by setting
φ(x, d) =

κ
	
[x⊺, d⊺]⊺, c(1)
, . . . , κ
	
[x⊺, d⊺]⊺, c(C)
⊺
,
where {c(i)}
C
i=1 is a collection of centres. By choosing centres to be themselves
composed of theorems x(.) and strategies d(.), such that c(.) = [x(.)⊺, d(.)⊺]
⊺, the
basis expansion above represents each data item with a non-linear inner product
against other known items.
To ﬁnd the relevant subset of D from which centres should be formed, we
follow the method described in the steps below.
1. Initially, we set the collection of centres to every possible centre. That is, for
N theorems and M strategies, we produce a centre for every combination of
the two, thereby producing C = N · M centres.
2. Next, we use φ to expand every centre to produce the C × C matrix Γ such
that
Γi,j = φ(c(i))j = κ(c(i), c(j)).
3. Then, we generate a vector γ such that γi represents a score for centre c(i).
Since each centre is a combination of a theorem and a strategy, we set the
score to signify how well the strategy performs for that theorem, as detailed
in Remark 7 below.
4. Finally, we use Automatic Relevance Determination (ARD) [17] with Γ as
input and γ as the response variable. The result is a weight assignment to
each centre to signify its relevance. The highest absolute-weighted ς centres
are chosen, where ς is a parameter which decides the total number of centres.
This method is inspired by the procedure used in Relevance Vector Machines [35]
for a similar purpose.
Remark 7 (score). For a strategy that succeeds in proving a theorem, the score
for the pair is the fraction of the time allocation left unconsumed by the prover.
For an unsuccessful strategy-theorem combination, we set the score to a value
close to zero.
Remark 8 (ς). The parameter ς is another tunable parameter which, in similar
fashion to the parameter σ earlier in this section, controls the model complexity
introduced by the basis expansion. Both variables must be tuned together.
7
Model Selection and Time Allocations
From Remark 8, ς and σ are hyperparameters that control the complexity intro-
duced into our model through the Gaussian basis expansion; and Remark 4 intro-
duces η, the hyperparameter that controls the strength of the prior. The ﬁnal
model is selected by tuning them. Tuning must aim to avoid overﬁtting to the
training data; and to maximize, during testing, either the savings in proof-search

568
C. Mangla et al.
time or the number of theorems proved. However, we do not have a closed-form
expression relating these parameters to this aim, thus any combination of the
parameters can be judged only by testing them.
In this work we have used Bayesian optimization [29] to optimize these
hyperparameters. Bayesian optimization is a black-box parameter optimization
method that attempts to search for a global optimum within the scope of a set
resource budget. It models the optimization target as a user-speciﬁed objective
function, which maps from the parameter space to a loss metric. This model
of the objective function is constructed using Gaussian Process (GP) regres-
sion [22], using data generated by repeatedly testing the objective function.
Our speciﬁed objective function maps from the hyperparameters (ς, σ, η) to a
loss metric ξ. We use cross-validation within the training data while calculating ξ
to penalize hyperparameters that over-ﬁt. Hyperparameters are tuned at training
time only, after which they are ﬁxed for subsequent testing. The ﬁnal test set is
never used for any hyperparameter optimization.
In the method presented thus far we are only permuting strategies with ﬁxed
time allocations to build a sequence for a strategy schedule. In this setting, the
number of theorems proved cannot change, but the time taken to prove theorems
can be reduced. Therefore, with this aim, a useful metric for ξ is the total time
taken by the theorem prover to prove the theorems in the cross-validation test
set.
However, we can take further advantage of the hyperparameter tuning phase
to additionally tune the times allocated to each strategy, by treating these times
as hyperparameters. Therefore, for each strategy d(i) we create a hyperparameter
ν(i) ∈(0, 1) which sets the proportion of the proof time allocated to that strategy.
We can then optimize our model to maximize the number of theorems proved;
a count of the remaining theorems is then a viable metric for ξ. Note that once
the ν(·) are set, time allocation for d(i) is ﬁxed to ν(i), irrespective of its order
in the strategy schedule.
Remark 9. Our results include two types of experiment:
– one where the time allocations for each strategy are set to the defaults shipped
with our reference theorem prover, and so we optimize for saving proof time;
and
– another wherein we allocate time to each strategy during the hyperparam-
eter tuning phase, and so we optimize for proving the maximum number of
theorems.
8
Training Data and Feature Extraction
Our chosen theorem prover, iLeanCoP, is shipped with a ﬁxed strategy schedule
consisting of 5 strategies. It splits the allocated proof time across the ﬁrst four
strategies by 2%, 60%, 20% and 10%. However, only the ﬁrst strategy is com-
plete and therefore usually expected to take up its entire time allocation. The
remaining strategies are incomplete, and may exit early on failure. Therefore,

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
569
the ﬁfth and ﬁnal strategy, which we refer to as the fallback strategy, is allocated
all the remaining time.
Emulating iLeanCop. We have constructed a dataset by attempting to prove
every theorem in our problem library using each of these strategies individually.
With this information, the result of any proof attempt can be calculated by
emulating the behaviour of iLeanCoP. This is how we evaluate the predicted
schedules—we emulate a proof attempt by iLeanCoP using that schedule for
each theorem in the test set. For a faithful emulation of the fallback strategy, it
is always attempted last, and therefore any new schedule is only a permutation
of the ﬁrst four strategies. Our experiments allocate a time of 600 s per theorem.
The dataset is built to ensure that, within this proof time, any such strategy
permutation can be emulated. We kept a timeout of 1200 s per strategy per
theorem when building the dataset, which is more than suﬃcient for current
experiments and gives us headroom for future experiments with longer proof
times.
Strategy Features. Each strategy in iLeanCoP consists of a time allocation
and parameter settings; the parameters are described by Otten [19]. We use a
one-hot encoding feature representation for strategies based on the parameter
setting as shown in Table 1. Another feature noting the completeness of each
strategy is also shown. Another feature (not shown in the table) contains the
time allocated to each strategy. Note the fallback strategy is used in prover
emulation but not in the schedule prediction.
Table 1. Features of the four main strategies.
Strategy
Parameter
Completeness
def scut cut comp(7) conj
def,scut,cut,comp(7) 1
1
1
1
0
1
def,scut,cut
1
1
1
0
0
0
conj,scut,cut
0
1
1
0
1
0
def,conj,cut
1
0
1
0
1
0
Theorem Features. The TPTP problem library contains a large, compre-
hensive collection of theorems and is designed for testing automated theorem
provers. The problems are taken from a range of domains such as Logic Cal-
culi, Algebra, Software Veriﬁcation, Biology and Philosophy, and presented in
multiple logical forms. For iLeanCoP, we select the subset in ﬁrst-order form,
denoted there as FOF. In version 7.1.0, there are 8157 such problems covering 43
domains. Each problem consists of a set of formulae and a goal theorem. The

570
C. Mangla et al.
problems are of varying sizes. For example, the problem named HWV134+1 from
the Hardware Veriﬁcation domain contains 128975 formulae, whilst SET703+4
from the Set Theory domain contains only 12.
We have constructed a dataset containing features extracted from the ﬁrst-
order logic problems in TPTP (see Appendix A). Here, we describe how those
features were developed.
In deployment, a prover using our method to generate strategy schedules
would have to extract features from the goal theorem at the beginning of a
proof attempt. To minimize the computational overhead of feature extraction,
in keeping with our goal noted in Remark 3, we use features that can be collected
when the theorem is parsed by the prover. The collection of features developed
in this work is based on the authors’ prior experience, and later we will brieﬂy
examine the quality of each feature to discard the uninformative ones. We extract
the following features, which are all considered candidates for the subsequent
feature selection process.
Symbol Counts: A count of the logical connectives and quantiﬁers. We extract
one feature per symbol by tracking lexical symbols encountered while parsing.
Quantiﬁer Rank: The maximum depth of nesting of quantiﬁers.
Quantiﬁer Count: A count of the number of quantiﬁers.
Mean and Maximum Function Arity: Obtained by keeping track of func-
tions during parsing.
Number of Functions: A count of the number of functions.
Quantiﬁer Alternations: A count of the number of times the quantiﬁers ﬂip
between the existential and universal. When calculated by examining only the
sequence of lexical symbols, the count may be inaccurate. An accurate count
is obtained by tracking negations during parsing while collecting quantiﬁers.
We extract both as candidates.
Feature Selection and Pre-processing. We examine the degree of associa-
tion between the individual theorem features described above and the speed with
which the strategies solve each theorem; for this we use the Maximal Informa-
tion Coeﬃcient (MIC) measure [23]. For every theorem we calculate the score,
as deﬁned in Remark 7, averaged over all strategies. This score is paired with
each feature to calculate its MIC. Most lexical symbols achieve an MIC close to
zero. We selected the features with relatively high MIC for the presented work,
and these are shown in Fig. 3.
The two features based on quantiﬁer alternations are clearly correlated, but
both meet the above criterion for selection. Correlations can also be expected
between the other features. Furthermore, our features range over diﬀerent scales.
For example, the maximal function arity in TPTP averages 2, whereas the num-
ber of predicate symbols averages 2097. It is desirable to remove these correla-
tions to alleviate any burden on the subsequent modelling phase, and to stan-
dardize the features to zero mean and unit variance to create a feature space with
similar length-scales in all dimensions. The former is achieved by decorrelation,
the latter by standardization, and both together by a sphering transformation.

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
571
Symbol ∧
Mean Function Arity
Number of Quantiﬁers
Symbol ∀
Symbol =
Number of Functions
Symbol ⇒
Symbol ¬
Symbol ∨
Symbol =
Symbol ⇔
Symbol ∃
Quantiﬁer Alternations (with negations)
Quantiﬁer Alternations
Quantiﬁer Rank
Maximum Function Arity
0.00
0.05
0.10
0.15
0.20
MIC of mean score
Theorem Feature
Fig. 3. MIC between selected features and scores.
We transform our extracted features as such using Zero-phase Component Anal-
ysis (ZCA), which ensures the transformed data is as close as possible to the
original [6].
Coverage. As mentioned above, we run iLeanCoP on every ﬁrst-order theo-
rem in TPTP with each strategy allocated 1200 s. Although every theorem in
intuitionistic logic also holds for classical logic, the converse does not hold. For
that reason and because of the limitations of iLeanCoP, many theorems remain
unproved by any strategy. We exclude these theorems from our experiments,
leaving us with a data-set of 2240 theorems.
9
Experiments
We present two experiments in this work, as noted in Remark 9. In this section,
we describe our experimental apparatus in detail.
As noted in Sect. 8, our data contains:
– N = 2240 theorems that are usable in our experiments;
– ﬁve strategies, of which M = 4 are used to build strategy schedules since one
is a fallback strategy; and
– features x(i) of theorems where i ∈[1, N] and features d(j) of strategies where
j ∈[1, M].
This data needs to be presented to our model for training in the form of
D = {(π(i), x(i))}
N
i=1, as described in Sect. 3. Since the two experiments have
slightly diﬀerent goals, we specialize D according to each.
When aiming to predict schedules that minimize the time taken to prove
theorems, a natural value for π(i) is the index order that sorts strategies in
increasing amounts of time taken to prove theorem i. However, some strategies

572
C. Mangla et al.
may fail to prove theorem i within their time allocation. In that case, we consider
the failed strategies equally bad and place them last in the ordering in π(i).
Furthermore, we create additional items (π′(i), x(i)) in D, by permuting the
positions of the failed strategies in π(i) to create multiple π′(i).
When the goal is only to prove more theorems, the strategies that succeed
are all considered equally ranked above the failed strategies. In this mode, the
successful strategies are similarly permuted in the data, in addition to those that
failed.
In each experiment, a random one-third of the N theorems are separated
into a holdout test set
˙N, leaving behind a training set ¨
N. This training set
is ﬁrst used for hyperparameter tuning using BO. As explained in Sect. 7, each
hyperparameter combination is tested with ﬁve-fold cross-validation within ¨
N,
to penalize instances that overﬁt to ¨
N. This results in estimated optimum values
for the hyperparameters. These are used to set the model, which is then trained
on ¨
N and then ﬁnally evaluated on ˙N. The whole process is repeated ten times
with new random splits
˙N and
¨
N to create one set of ten results for that
experiment.
10
Results
Each experiment, repeated ten times, is conducted in two phases: ﬁrst, hyperpa-
rameter optimization; and second, model training and evaluation. The bounds
on the search space in the ﬁrst phase were always the same (see Appendix A).
The holdout test set contained 747 theorems. A proof time of 600 s was emulated.
10.1
Experiment 1: Optimizing Proof Attempt Time
The results are shown in Fig. 4. The total prediction time for all 747 theorems,
averaged across the trials, is 0.14 s.
The times across proof attempts are not normally distributed, for both the
unmodiﬁed iLeanCoP schedule and the predicted ones, as conﬁrmed by a Jarque-
Bera test. Therefore, we used the right-tailed Wilcoxon signed-rank test for a
pair-wise comparison of the times taken for each theorem by the original sched-
ule in iLeanCoP versus the predicted schedules, resulting in a p-value of less
51%
60%
56%
61%
65%
54%
64%
60%
58%
56%
1
2
3
4
5
6
7
8
9 10
Trial
Proof time saved
0
200
400
600
1
2
3
4
5
6
7
8
9 10
Trial
Proved Theorems
Proof time
Slower
Same
Faster
Fig. 4. Results of Experiment 1. Proof times are compared with precision 10−6s.

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
573
than 10−6 in each trial, conﬁrming the alternate hypothesis that the reduction
in time taken to prove each theorem comes from a distribution with median
greater than zero. This conﬁrms that the time savings are statistically signiﬁ-
cant. Furthermore, we note from Fig. 4 a saving of more than 50% in the total
proof-time in each trial.
10.2
Experiment 2: Proving More Theorems
We set our hyperparameter search to ﬁnd time allocations for strategies. The
resulting predicted schedules have gains and losses when compared to the original
schedule, as shown in the four facets of Fig. 5. However, there is a consistent gain
in the number of theorems proved and a gain of ﬁve theorems on average, evident
from the mean values in (†) and (‡).
3
6 6
2
6
3
0
7
5
12
1
2
3
4
5
6
7
8
9 10
Trial
Increase in theorems
proved by pred.
mean = 685.00
mean = 5.90
mean = 10.90
mean = 45.20
Proved by orig.
Uncertiﬁed in orig.
Proved by pred.
Uncert. in pred.
1 2 3 4 5 6 7 8 9 10
1 2 3 4 5 6 7 8 9 10
10
50
750
10
50
750
Trial
Theorems
Fig. 5. Comparison of the proof attempts by the original (orig.) and predicted (pred.)
schedules in Experiment 2. Theorems which are proved by pred. but could not be
proved by orig. are counted in †, and the vice versa in ‡.
11
Related Work
Prior work on machine learning for algorithm selection, such as that introduced
by Leyton-Brown et al. [13], is a precursor to our work. In that topic, the machine
learning methods must perform the task of selecting a good algorithm from
within a portfolio to solve the given problem instance. Typically, as was the case
in the work by Leyton-Brown et al. [13], the learning methods predict the runtime
of all algorithms, and then pick the fastest predicted one. This line of enquiry
has been extended to select algorithms for SMT solvers—a recent example is
MachSMT by Scott et al. [28]. The machine learning models in MachSMT are
trained by considering all the portfolio members in pairs for each problem in the

574
C. Mangla et al.
training set. This method is called pairwise ranking, which contrasts from our
method, called list-wise ranking, in which we consider the full list of portfolio
members all together.
In terms of the machine learning task, the work on scheduling solvers bears
greater similarity to our presented work. In MedleySolver, for example, Pim-
palkhare et al. [20] frame this task as a multi-armed bandit problem. They
predict a sequence of solvers as well as the time allocation for each to generate
schedules for the goal problems. MedleySolver is able to solve more problems
than any individual solver would on its own.
With an approach that contrasts with ours, H˚ula et al. [10] have made use of
Graph Neural Networks (GNNs) for solver scheduling. They produce a regression
model to predict, for the given problem, the runtime of all the solvers; which
is used as the key to sort the solvers in increasing order of predicted runtime
to build a schedule. This is an example of point-wise ranking. The authors use
GNNs to automatically discover features for machine learning. They combine
this feature extraction with training of the regression model. They achieve an
increase in the number of problems solved as well as a reduction in the total
proof time. Meanwhile, our use of manual feature engineering combined with
statistical methods for selection and normalization has certain advantages. For
one, we can analyse our features and derive a subjective interpretation of their
eﬃcacy. Additionally, our features eﬀectively impart our domain knowledge onto
the model. Such domain knowledge may not be available in the data itself.
Manual feature engineering such as ours can be combined with automatic feature
extraction to reap the beneﬁts of both.
12
Conclusions
We have presented a method to specialize, for the given goal theorem, the
sequence of strategies in the schedule used in each proof attempt. A Bayesian
machine learning model is trained in this method using data generated by test-
ing the prover of interest. When evaluated with the iLeanCoP prover using the
TPTP library as a benchmark, our results show a signiﬁcant reduction in the
time taken to prove theorems. For theorems that are successfully proved, the
average time saving is above 50%. The prediction time is on average low enough
to have a negligible impact on the resources subtracted from the proof search
itself.
We also extend this method to optimize time allocations to each strategy.
In this setting, our results show a notable increase in the number of theorems
proved.
This work shows, by example, that Bayesian machine learning models
designed speciﬁcally to augment heuristics in theorem provers, with detailed
consideration of the computational compromises required in this setting, can
deliver substantial improvements.
Acknowledgments. Initial investigations for this work were co-supervised by Prof.
Mateja Jamnik, Computer Laboratory, University of Cambridge, UK.

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
575
This work was supported by: the UK Engineering and Physical Sciences Research
Council (EPSRC) through a Doctoral Training studentship, award reference 1788755;
and the ERC Advanced Grant ALEXANDRIA (Project GA 742178). For the purpose
of open access, the author has applied a Creative Commons Attribution (CC-BY-4.0)
licence to any Author Accepted Manuscript version arising.
Computations for this work was performed using resources provided by the Cam-
bridge Service for Data Driven Discovery (CSD3) operated by the University of Cam-
bridge Research Computing Service, provided by Dell EMC and Intel using Tier-2
funding from the Engineering and Physical Sciences Research Council (capital grant
EP/P020259/1), and DiRAC funding from the Science and Technology Facilities Coun-
cil.
A
Implementation, Code and Data
This work is implemented primarily in Matlab [36]. All experiments can be
reproduced using the code, data and instructions available at [15]. The hyper-
parameter search space in all experiments was restricted to ς ∈[10, 300], σ ∈
[0.01, 100.0] and η ∈[1, 100].
References
1. Balunovic, M., Bielik, P., Vechev, M.T.: Learning to solve SMT formulas. In:
Annual Conference on Neural Information Processing Systems, pp. 10338–10349
(2018)
2. Barber, D.: Bayesian Reasoning and Machine Learning. Cambridge University
Press, Cambridge (2012)
3. Bridge, J.P., Holden, S.B., Paulson, L.C.: Machine learning for ﬁrst-order theorem
proving. J. Autom. Reas. 53(2), 141–172 (2014). https://doi.org/10.1007/s10817-
014-9301-5
4. Cao, Z., Qin, T., Liu, T.Y., Tsai, M.F., Li, H.: Learning to rank: from pairwise
approach to listwise approach. In: International Conference on Machine Learning,
pp. 129–136. Association for Computing Machinery (2007)
5. Cheng, W., Dembczynski, K., H¨ullermeier, E.: Label ranking methods based on
the Plackett-Luce model. In: International Conference on Machine Learning, pp.
215–222. Omnipress (2010)
6. Duboue, P.: The Art of Feature Engineering: Essentials for Machine Learning.
Cambridge University Press, Cambridge (2020)
7. Dummett, M.: Elements of Intuitionism, 2nd edn. Clarendon, Oxford (2000)
8. Guiver, J., Snelson, E.: Bayesian inference for Plackett-Luce ranking models. In:
International Conference on Machine Learning, pp. 377–384. Association for Com-
puting Machinery (2009)
9. Hales, T., et al.: A formal proof of the Kepler conjecture. In: Forum of Mathematics,
Pi, vol. 5 (2017)
10. H˚ula, J., Mojˇz´ıˇsek, D., Janota, M.: Graph neural networks for scheduling of SMT
solvers. In: International Conference on Tools with Artiﬁcial Intelligence, pp. 447–
451 (2021)
11. Johnson, S.R., Henderson, D.A., Boys, R.J.: On Bayesian inference for the
Extended Plackett-Luce model (2020). arXiv:2002.05953

576
C. Mangla et al.
12. Kaliszyk, C., Urban, J., Vyskoˇcil, J.: Machine learner for automated reasoning 0.4
and 0.5 (2014). arXiv:1402.2359
13. Leyton-Brown, K., Nudelman, E., Andrew, G., McFadden, J., Shoham, Y.: A port-
folio approach to algorithm selection. In: International Joint Conference on Artiﬁ-
cial Intelligence, pp. 1542–1543. Morgan Kaufmann Publishers Inc. (2003)
14. Luce, R.D.: Individual Choice Behavior: A Theoretical Analysis. Wiley, Hoboken
(1959)
15. Mangla, C.: BRASS (2022). https://doi.org/10.5281/zenodo.6028568
16. Murphy, K.P.: Machine Learning: A Probabilistic Perspective. MIT press, Cam-
bridge (2012)
17. Neal, R.M.: Bayesian Learning for Neural Networks. Springer, New York (1996).
https://doi.org/10.1007/978-1-4612-0745-0
18. Otten, J.: Clausal connection-based theorem proving in intuitionistic ﬁrst-order
logic. In: Beckert, B. (ed.) TABLEAUX 2005. LNCS (LNAI), vol. 3702, pp. 245–
261. Springer, Heidelberg (2005). https://doi.org/10.1007/11554554 19
19. Otten, J.: leanCoP 2.0 and ileanCoP 1.2: high performance lean theorem proving
in classical and intuitionistic logic (system descriptions). In: Armando, A., Baum-
gartner, P., Dowek, G. (eds.) IJCAR 2008. LNCS (LNAI), vol. 5195, pp. 283–291.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-71070-7 23
20. Pimpalkhare, N., Mora, F., Polgreen, E., Seshia, S.A.: MedleySolver: online SMT
algorithm selection. In: Li, C.-M., Many`a, F. (eds.) SAT 2021. LNCS, vol. 12831,
pp. 453–470. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-80223-
3 31
21. Plackett, R.L.: The analysis of permutations. J. Roy. Stat. Soc. Ser. C (Appl. Stat.)
24(2), 193–202 (1975)
22. Rasmussen, C.E., Williams, C.K.I.: Gaussian Processes for Machine Learning.
Adaptive Computation and Machine Learning, MIT Press, Cambridge (2006)
23. Reshef, D.N., et al.: Detecting novel associations in large data sets. Science
334(6062), 1518–1524 (2011)
24. Roberts, G.O., Rosenthal, J.S.: Optimal scaling for various Metropolis-Hastings
algorithms. Stat. Sci. 16(4), 351–367 (2001)
25. Rossi, P.E.: Bayesian Statistics and Marketing. Wiley, Hoboken (2006)
26. Sch¨afer, D., H¨ullermeier, E.: Dyad ranking using Plackett-Luce models based on
joint feature representations. Mach. Learn. 107(5), 903–941 (2018)
27. Schulz, S.: E - a Brainiac Theorem Prover. AI Commun. 15(23), 111–126 (2002)
28. Scott, J., Niemetz, A., Preiner, M., Nejati, S., Ganesh, V.: MachSMT: a machine
learning-based algorithm selector for SMT solvers. In: TACAS 2021. LNCS, vol.
12652, pp. 303–325. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-
72013-1 16
29. Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., De Freitas, N.: Taking the
human out of the loop: a review of Bayesian optimization. Proc. IEEE 104(1),
148–175 (2015)
30. Shawe-Taylor, J.: Kernel Methods for Pattern Analysis. Cambridge University
Press, Cambridge (2004)
31. Silverthorn, B., Miikkulainen, R.: Latent class models for algorithm portfolio meth-
ods. In: AAAI Conference on Artiﬁcial Intelligence, pp. 167–172. AAAI Press
(2010)
32. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure: from CNF
to TH0, TPTP v.6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
33. Tammet, T.: Gandalf. J. Autom. Reason. 18(2), 199–204 (1997)

Bayesian Ranking for Strategy Scheduling in Automated Theorem Provers
577
34. Thornton, C.: Parity: the problem that won’t go away. In: McCalla, G. (ed.) AI
1996. LNCS, vol. 1081, pp. 362–374. Springer, Heidelberg (1996). https://doi.org/
10.1007/3-540-61291-2 65
35. Tipping, M.E.: Sparse Bayesian learning and the relevance vector machine. J.
Mach. Learn. Res. 1, 211–244 (2001)
36. Trauth, M.H.: MATLAB R
⃝Recipes for Earth Sciences. Springer, Heidelberg
(2021). https://doi.org/10.1007/3-540-27984-9
37. Tsarkov, D., Horrocks, I.: FaCT++ description logic reasoner: system description.
In: Furbach, U., Shankar, N. (eds.) IJCAR 2006. LNCS (LNAI), vol. 4130, pp.
292–297. Springer, Heidelberg (2006). https://doi.org/10.1007/11814771 26
38. Wasserman, L.: All of Statistics: A Concise Course in Statistical Inference.
Springer, New York (2004). https://doi.org/10.1007/978-0-387-21736-9
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

A Framework for Approximate
Generalization in Quantitative Theories
Temur Kutsia(B)
and Cleo Pau
RISC, Johannes Kepler University Linz, Linz, Austria
{kutsia,ipau}@risc.jku.at
Abstract. Anti-uniﬁcation aims at computing generalizations for given
terms, retaining their common structure and abstracting diﬀerences by
variables. We study quantitative anti-uniﬁcation where the notion of the
common structure is relaxed into “proximal” up to the given degree with
respect to the given fuzzy proximity relation. Proximal symbols may
have diﬀerent names and arities. We develop a generic set of rules for
computing minimal complete sets of approximate generalizations and
study their properties. Depending on the characterizations of proximities
between symbols and the desired forms of solutions, these rules give rise
to diﬀerent versions of concrete algorithms.
Keywords: Generalization · Anti-uniﬁcation · Quantiative theories ·
Fuzzy proximity relations
1
Introduction
Generalization problems play an important role in various areas of mathematics,
computer science, and artiﬁcial intelligence. Anti-uniﬁcation [12,14] is a logic-
based method for computing generalizations. Being originally used for induc-
tive and analogical reasoning, some recent applications include recursion scheme
detection in functional programs [4], programming by examples in domain-
speciﬁc languages [13], learning bug-ﬁxing from software code repositories [3,15],
automatic program repair [7], preventing bugs and misconﬁguration in ser-
vices [11], linguistic structure learning for chatbots [6], to name just a few.
In most of the existing theories where anti-uniﬁcation is studied, the back-
ground knowledge is assumed to be precise. Therefore, those techniques are not
suitable for reasoning with incomplete, imprecise information (which is very
common in real-world communication), where the exact equality is replaced by
its (quantitative) approximation. Fuzzy proximity and similarity relations are
notable examples of such extensions. These kinds of quantitative theories have
many useful applications, some most recent ones being related to artiﬁcial intelli-
gence, program veriﬁcation, probabilistic programming, or natural language pro-
cessing. Many tasks arising in these areas require reasoning methods and compu-
tational tools that deal with quantitative information. For instance, approximate
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 578–596, 2022.
https://doi.org/10.1007/978-3-031-10769-6_34

A Framework for Approximate Generalization in Quantitative Theories
579
inductive reasoning, reasoning and programming by analogy, similarity detec-
tion in programming language statements or in natural language texts could
beneﬁt from solving approximate generalization constraints, which is a theoreti-
cally interesting and challenging task. Investigations in this direction have been
started only recently. In [1], the authors proposed an anti-uniﬁcation algorithm
for fuzzy similarity (reﬂexive, symmetric, min-transitive) relations, where mis-
matches are allowed not only in symbol names, but also in their arities (fully
fuzzy signatures). The algorithm from [9] is designed for fuzzy proximity (i.e.,
reﬂexive and symmetric) relations with mismatches only in symbol names.
In this paper, we study approximate anti-uniﬁcation from a more gen-
eral perspective. The considered relations are fuzzy proximity relations. Prox-
imal symbols may have diﬀerent names and arities. We consider four diﬀer-
ent variants of relating arguments between diﬀerent proximal symbols: unre-
stricted relations/functions, and correspondence (i.e. left- and right-total) rela-
tions/functions. A generic set of rules for computing minimal complete sets of
generalizations is introduced and its termination, soundness and completeness
properties are proved. From these rules, we obtain concrete algorithms that
deal with diﬀerent kinds of argument relations. We also show how the existing
approximate anti-uniﬁcation algorithms and their generalizations ﬁt into this
framework.
Organization: In Sect. 2 we introduce the notation and deﬁnitions. Section 3 is
devoted to a technical notion of term set consistency and to an algorithm for
computing elements of consistent sets of terms. It is used later in the main
set of anti-uniﬁcation rules, which are introduced and characterized in Sect. 4.
The concrete algorithms obtained from those rules are also described in this
section. In Sect. 5, we discuss complexity. Section 6 oﬀers a high-level picture of
the studied problems and concludes.
An extended version of this work can be found in the technical report [8].
2
Preliminaries
Proximity Relations. Given a set S, a mapping R from S ˆ S to the real
interval [0, 1] is called a binary fuzzy relation on S. By ﬁxing a number λ, 0 ď
λ ď 1, we can deﬁne the crisp (i.e., two-valued) counterpart of R, named the
λ-cut of R, as Rλ :“ {ps1, s2q | Rps1, s2q ě λ}. A fuzzy relation R on a set
S is called a proximity relation if it is reﬂexive (Rps, sq “ 1 for all s P S)
and symmetric (Rps1, s2q “ Rps2, s1q for all s1, s2 P S). A T-norm ^ is an
associative, commutative, non-decreasing binary operation on [0, 1] with 1 as
the unit element. We take minimum in the role of T-norm.
Terms and Substitutions. We consider a ﬁrst-order alphabet consisting of a
set of ﬁxed arity function symbols F and a set of variables V, which includes a spe-
cial symbol _ (the anonymous variable). The set of named (i.e., non-anonymous)
variables V\{_} is denoted by VN. When the set of variables is not explicitly

580
T. Kutsia and C. Pau
speciﬁed, we mean V. The set of terms T pF, Vq over F and V is deﬁned in the
standard way: t P T pF, Vq iﬀt is deﬁned by the grammar t :“ x | fpt1, . . . , tnq,
where x P V and f P F is an n-ary symbol with n ě 0. Terms over T pF, VNq are
deﬁned similarly except that all variables are taken from VN.
We denote arbitrary function symbols by f, g, h, constants by a, b, c, variables
by x, y, z, v, and terms by s, t, r. The head of a term is deﬁned as headpxq :“ x
and headpfpt1, . . . , tnqq :“ f. For a term t, we denote with Vptq (resp. by VNptq)
the set of all variables (resp. all named variables) appearing in t. A term is called
linear if no named variable occurs in it more than once.
The
deanonymization
operation
deanon
replaces
each
occurrence
of
the
anonymous
variable
in
a
term
by
a
fresh
variable.
For
instance,
deanonpfp_, x, gp_qqq “ fpy′, x, gpy′′qqq, where y′ and y′′ are fresh. Hence,
deanonptq P T pF, VNq is unique up to variable renaming for all t P T pF, Vq.
deanonptq is linear iﬀt is linear.
The notions of term depth, term size and a position in a term are deﬁned in
the standard way, see, e.g. [2]. By t|p we denote the subterm of t at position p
and by t[s]p a term that is obtained from t by replacing the subterm at position
p by the term s.
A substitution is a mapping from VN to T pF, VNq (i.e., without anonymous
variables), which is the identity almost everywhere. We use the Greek letters
σ, ϑ, ϕ to denote substitutions, except for the identity substitution which is writ-
ten as Id. We represent substitutions with the usual set notation. Application of
a substitution σ to a term t, denoted by tσ, is deﬁned as _σ :“ _, xσ :“ σpxq,
fpt1, . . . , tnqσ :“ fpt1σ, . . . , tnσq. Substitution composition is deﬁned as a com-
position of mappings. We write σϑ for the composition of σ with ϑ.
Argument Relations and Mappings. Given two sets N “ {1, . . . , n} and
M “ {1, . . . , m}, a binary argument relation over N ˆ M is a (possibly empty)
subset of N ˆ M. We denote argument relations by ρ. An argument relation
ρ Ď N ˆ M is (i) left-total if for all i P N there exists j P M such that pi, jq P ρ;
(ii) right-total if for all j P M there exists i P N such that pi, jq P ρ. Corres-
pondence relations are those that are both left- and right-total.
An argument mapping is an argument relation that is a partial injective
function. In other words, an argument mapping π from N “ {1, . . . , n} to M “
{1, . . . , m} is a function π : In →Im, where In Ď N, Im Ď M and |In| “ |Im|.
Note that it can be also the empty mapping: π : H →H. The inverse of an
argument mapping is again an argument mapping.
Given a proximity relation R over F, we assume that for each pair of function
symbols f and g with Rpf, gq “ α > 0, where f is n-ary and g is m-ary, there is
also given an argument relation ρ over {1, . . . , n}ˆ{1, . . . , m}. We use the nota-
tion f „ρ
R,α g. These argument relations should satisfy the following conditions:
ρ is the empty relation if f or g is a constant; ρ is the identity if f “ g; f „ρ
R,α g
iﬀg „ρ´1
R,α f, where ρ´1 is the inverse of ρ.

A Framework for Approximate Generalization in Quantitative Theories
581
Example 1. Assume that we have four diﬀerent versions of deﬁning the notion of
author (e.g., originated from four diﬀerent knowledge bases) author 1pﬁrst-name,
middle-initial, last-nameq, author 2pﬁrst-name, last-nameq, author 3plast-name,
ﬁrst-name, middle-initialq, and author 4pfull-nameq. One could deﬁne the argu-
ment relations/mappings between these function symbols e.g., as follows:
author 1 „{p1,1q,p3,2q}
R,0.7
author 2,
author 1 „{p3,1q,p1,2q,p2,3q}
R,0.9
author 3,
author 1 „{p1,1q,p3,1q}
R,0.5
author 4,
author 2 „{p1,2q,p2,1q}
R,0.7
author 3,
author 2 „{p1,1q,p2,1q}
R,0.5
author 4,
author 3 „{p1,1q,p2,1q}
R,0.5
author 4.
Proximity Relations over Terms. Each proximity relation R in this paper is
deﬁned on F Y V such that Rpf, xq “ 0 for all f P F and x P V, and Rpx, yq “ 0
for all x ‰ y, x, y P V. We assume that R is strict: for all w1, w2 P F Y V, if
Rpw1, w2q “ 1, then w1 “ w2. Yet another assumption is that for each f P F,
its pR, λq-proximity class {g | Rpf, gq ě λ} is ﬁnite for any R and λ.
We extend such an R to terms from T pF, Vq as follows:
(a) Rpt, sq :“ 0 if Rpheadpsq, headptqq “ 0;
(b) Rpt, sq :“ 1 if t “ s and t, s P V;
(c) Rpt, sq :“ Rpf, gq ^ Rpti1, sj1q ^ · · · ^ Rptik, sjkq, if t “ fpt1, . . . , tnq, s “
gps1, . . . , smq, f „ρ
R,λ g, and ρ “ {pi1, j1q, . . . , pik, jkq}.
If Rpt, sq ě λ, we write t »R,λ s. When λ “ 1, the relation »R,λ does not
depend on R due to strictness of the latter and is just the syntactic equality “.
The pR, λq-proximity class of a term t is pcR,λptq :“ {s | s »R,λ t}.
Generalizations. Given R and λ, a term r is an pR, λq-generalization of (alter-
natively, pR, λq-more general than) a term t, written as r ÀR,λ t, if there exists
a substitution σ such that deanonprqσ »R,λ deanonptq. The strict part of ÀR,λ
is denoted by ≺R,λ, i.e., r ≺R,λ t if r ÀR,λ t and not t ÀR,λ r.
Example 2. Given a proximity relation R, a cut value λ, constants a „H
R,α1 b
and b „H
R,α2 c, binary function symbols f and h, and a unary function symbol g
such that h „{p1,1q,p1,2q}
R,α3
f and h „{p1,1q}
R,α4
g with αi ě λ, 1 ď i ď 4, we have
– hpx, _q ÀR,λ hpa, xq, because hpx, x′q{x →a, x′ →x} “ hpa, xq »R,λ hpa, xq.
– hpx, _q ÀR,λ hp_, xq, because hpx, x′q{x →y′, x′ →x} “ hpy′, xq »R,λ
hpy′, xq.
– hpx, xq ̸ÀR,λ hp_, xq, because hpx, xq ̸ÀR,λ hpy′, xq.
– hpx, _q ÀR,λ fpa, cq, because hpx, x′q{x →b} “ hpb, x′q »R,λ fpa, cq.
– hpx, _q ÀR,λ gpcq, because hpx, x′q{x →c} “ hpc, x′q »R,λ gpcq.
The notion of syntactic generalization of a term is a special case of pR, λq-
generalization for λ “ 1. We write r À t to indicate that r is a syntactic gener-
alization of t. Its strict part is denoted by ≺.
Since R is strict, r À t is equivalent to deanonprqσ “ deanonptq for some σ
(note the syntactic equality here).

582
T. Kutsia and C. Pau
Theorem 1. If r À t and t ÀR,λ s, then r ÀR,λ s.
Proof. r À t implies deanonprqσ “ deanonptq for some σ, while from t ÀR,λ s we
have deanonptqϑ »R,λ deanonpsq for some ϑ. Then deanonprqσϑ »R,λ deanonpsq,
which implies r ÀR,λ s.
□
Note that r ÀR,λ t and t ÀR,λ s, in general, do not imply r ÀR,λ s due to
non-transitivity of »R,λ.
Deﬁnition 1 (Minimal complete set of pR, λq-generalizations).
Given
R, λ, t1, and t2, a set of terms T is a complete set of pR, λq-generalizations of
t1 and t2 if
(a) every r P T is an pR, λq-generalization of t1 and t2,
(b) if r′ is an pR, λq-generalization of t1 and t2, then there exists r P T such
that r′ À r (note that we use syntactic generalization here).
In addition, T is minimal, if it satisﬁes the following property:
(c) if r, r′ P T, r ‰ r′, then neither r ≺R,λ r′ nor r′ ≺R,λ r.
A minimal complete set of pR, λq-generalizations ((R, λ)-mcsg) of two terms is
unique modulo variable renaming. The elements of the pR, λq-mcsg of t1 and t2
are called least general pR, λq-generalizations ((R, λ)-lggs) of t1 and t2.
This deﬁnition directly extends to generalizations of ﬁnitely many terms.
The problem of computing an pR, λq-generalization of terms t and s is called
the pR, λq-anti-uniﬁcation problem of t and s. In anti-uniﬁcation, the goal is to
compute their least general pR, λq-generalization.
The precise formulation of the anti-uniﬁcation problem would be the follow-
ing: Given R, λ, t1, t2, ﬁnd an pR, λq-lgg r of t1 and t2, substitutions σ1, σ2, and
the approximation degrees α1, α2 such that Rprσ1, t1q “ α1 and Rprσ2, t2q “ α2.
A minimal complete algorithm to solve this problem would compute exactly the
elements of pR, λq-mcsg of t1 and t2 together with their approximation degrees.
However, as we see below, it is problematic to solve the problem in this form.
Therefore, we will consider a slightly modiﬁed variant, taking into account anony-
mous variables in generalizations and relaxing bounds on their degrees.
We assume that the terms to be generalized are ground. It is not a restriction
because we can treat variables as constants that are close only to themselves.
Recall that the proximity class of any alphabet symbol is ﬁnite. Also, the
symbols are related to each other by ﬁnitely many argument relations. One may
think that it leads to ﬁnite proximity classes of terms, but this is not the case.
Consider, e.g., R and λ, where h »{p1,1q}
R,λ
f with binary h and unary f. Then the
pR, λq-proximity class of fpaq is inﬁnite: {fpaq} Y {hpa, tq | t P T pF, Vq}. Also,
the pR, λq-mcsg for fpaq and fpbq is inﬁnite: {fpxq} Y {hpx, tq | t P T pF, Hq}.
Deﬁnition 2. Given the terms t1, . . . , tn, n ě 1, a position p in a term r is
called irrelevant for pR, λq-generalizing (resp. for pR, λq-proximity to) t1, . . . , tn
if r[s]p ÀR,λ ti (resp. r[s]p »R,λ ti) for all 1 ď i ď n and for all terms s.

A Framework for Approximate Generalization in Quantitative Theories
583
We say that r is a relevant pR, λq-generalization (resp. relevant pR, λq-pro-
ximal term) of t1, . . . , tn if r ÀR,λ ti (resp. r »R,λ ti) for all 1 ď i ď n and
r|p “ _
for all positions p in r that is irrelevant for generalizing (resp. for
proximity to) t1, . . . , tn. The pR, λq-relevant proximity class of t is
rpcR,λptq :“ {s | s is a relevant pR, λq-proximal term of t}.
In the example above, position 2 in hpx, tq is irrelevant for generalizing fpaq
and fpbq, and hpx, _q is one of their relevant generalizations. Note that fpxq
is also a relevant generalization of fpaq and fpbq, since it contains no irrelevant
positions. More general generalizations like, e.g., x, are relevant as well. Similarly,
position 2 in hpa, tq is irrelevant for proximity to fpaq and rpcR,λpfpaqq “ {fpaq,
hpa, _q}. Generally, rpcR,λptq is ﬁnite for any t due to the ﬁniteness of proximity
classes of symbols and argument relations mentioned above.
Deﬁnition 3 (Minimal complete set of relevant pR, λq-generalizations).
Given R, λ, t1, and t2, a set of terms T is a complete set of relevant pR, λq-
generalizations of t1 and t2 if
(a) every element of T is a relevant pR, λq-generalization of t1 and t2, and
(b) if r is a relevant pR, λq-generalization of t1 and t2, then there exists r′ P T
such that r À r′.
The minimality property is deﬁned as in Deﬁnition 1.
This deﬁnition directly extends to relevant generalizations of ﬁnitely many terms.
We use pR, λq-mcsrg as an abbreviation for minimal complete set of relevant
pR, λq-generalization. Like relevant proximity classes, mcsrg’s are also ﬁnite.
Lemma 1. For given R and λ, if all argument relations are correspondence
relations, then pR, λq-mcsg’s and pR, λq-proximity classes for all terms are ﬁnite.
Proof. Under correspondence relations no term contains an irrelevant position
for generalization or for proximity.
□
Hence, for correspondence relations the notions of mcsg and mcsrg coincide,
as well as the notions of proximity class and relevant proximity class.
For a term r, we deﬁne its linearized version linprq as a term obtained
from r by replacing each occurrence of a named variable in r by a fresh one.
For instance, linpfpx, _, gpy, x, aq, bqq “ fpx′, _, gpy′, x′′, aq, bq, where x′, x′′, y′
are fresh variables. Linearized versions of terms are unique modulo variable
renaming.
Deﬁnition 4 (Generalization degree upper bound). Given two terms r
and t, a proximity relation R, and a λ-cut, the pR, λq-generalization degree
upper bound of r and t, denoted by gdubR,λpr, tq, is deﬁned as follows:
Let α :“ max{Rplinprqσ, tq | σ is a substitution}. Then gdubR,λpr, tq is α if
α ě λ, and 0 otherwise.

584
T. Kutsia and C. Pau
Intuitively, gdubR,λpr, tq “ α means that no instance of r can get closer than
α to t in R. From the deﬁnition it follows that if r ÀR,λ t, then 0 < λ ⩽
gdubR,λpr, tq ≤1 and if r ̸ÀR,λ t, then gdubR,λpr, tq “ 0.
The upper bound computed by gdub is more relaxed than it would be if the
linearization function were not used, but this is what we will be able to compute
in our algorithms later.
Example 3. Let Rpa, bq “ 0.6, Rpb, cq “ 0.7, and λ “ 0.5. Then gdubR,λpfpx, bq,
fpa, cqq “ 0.7 and gdubR,λpfpx, xq, fpa, cqq “ gdubR,λpfpx, yq, fpa, cqq “ 1.
It is not diﬃcult to see that if rσ »R,λ t, then Rprσ, tq ď gdubR,λpr, tq. In
Example 3, for σ “ {x →b} we have Rpfpx, xqσ, fpa, cqq “ Rpfpb, bq, fpa, cqq “
0.6 < gdubR,λpfpx, xq, fpa, cqq “ 1.
We compute gdubR,λpr, tq as follows: If r is a variable, then gdubR,λpr, tq “ 1.
Otherwise, if headprq „ρ
R,β headptq, then gdubR,λpr, tq “ β ^ 
pi,jqPρ gdubR,λpr|i,
t|jq. Otherwise, gdubR,λpr, tq “ 0.
3
Term Set Consistency
The notion of term set consistency plays an important role in the computation
of proximal generalizations. Intuitively, a set of terms is pR, λq-consistent if all
the terms in the set have a common pR, λq-proximal term. In this section, we
discuss this notion and the corresponding algorithms.
Deﬁnition 5 (Consistent set of terms). A ﬁnite set of terms T is pR, λq-
consistent if there exists a term s such that s »R,λ t for all t P T.
pR, λq-consistency of a ﬁnite term set T is equivalent to 
tPT pcR,λptq ‰ H,
but we cannot use this property to decide consistency, since proximity classes of
terms can be inﬁnite (when the argument relations are not restricted). For this
reason, we introduce the operation [ on terms as follows: (i) t [ _ “ _ [ t “ t,
(ii) fpt1, . . . , tnq [ fps1, . . . , snq “ fpt1 [ s1, . . . , tn [ snq, n ě 0. Obviously, [ is
associative (A), commutative (C), idempotent (I), and has _ as its unit element
(U). It can be extended to sets of terms: T1 [ T2 :“ {t1 [ t2 | t1 P T1, t2 P T2}. It
is easy to see that [ on sets also satisﬁes the ACIU properties with the set {_}
playing the role of the unit element.
Lemma 2. A ﬁnite set of terms T is pR, λq-consistent iﬀŰ
tPT rpcR,λptq ‰ H.
Proof. p⇒q If s »R,λ t for all t P T, then st P rpcR,λptq, where st is obtained
from s by replacing all subterms that are irrelevant for its pR, λq-proximity to t
by _. Assume T “ {t1, . . . , tn}. Then st1 [ · · · [ stn P Ű
tPT rpcR,λptq.
p⇐q Obvious, since s »R,λ t for s P Ű
tPT rpcR,λptq and for all t P T.
□
Now we design an algorithm C that computes Ű
tPT rpcR,λptq without actu-
ally computing rpcR,λptq for each t P T. A special version of the algorithm can
be used to decide the pR, λq-consistency of T.
The algorithm is rule-based. The rules work on states, that are pairs I; s,
where s is a term and I is a ﬁnite set of expressions of the form x in T, where
T is a ﬁnite set of terms. R and λ are given. There are two rules (Z stands for
disjoint union):

A Framework for Approximate Generalization in Quantitative Theories
585
Rem: Removing the empty set
{x in H} Z I; s “⇒I; s{x →_}.
Red: Reduce a set to new sets
{x in {t1, . . . , tm}} Z I; s “⇒{y1 in T1, . . . , yn in Tn} Y I; s{x →hpy1, . . . , ynq},
where m ě 1, h is an n-ary function symbol such that h „ρk
R,γk headptkq with
γk ě λ for all 1 ď k ď m, and Ti :“ {tk|j | pi, jq P ρk, 1 ď k ď m}, 1 ď i ď n,
is the set of all those arguments of the terms t1, . . . , tm that are supposed to be
pR, λq-proximal to the i’s argument of h.
To compute Ű
tPT rpcR,λptq, C starts with {x in T}; x and applies the rules
as long as possible. Red causes branching. A state of the form H; s is called
a success state. A failure state has the form I; s, to which no rule applies and
I ‰ H. In the full derivation tree, each leaf is a either success or a failure state.
Example 4. Assume a, b, c are constants, g, f, h are function symbols with the
arities respectively 1, 2, and 3. Let λ be given and R be deﬁned so that Rpa, bq ě
λ, Rpb, cq ě λ, h „{p1,1q,p1,2q}
R,β
f, h „{p2,1q}
R,γ
g with β ě λ and γ ě λ. Then
rpcR,λpfpa, cqq “ {fpa, cq, fpb, cq, fpa, bq, fpb, bq, hpb, _, _q},
rpcR,λpgpaqq “ {gpaq, gpbq, hp_, a, _q, hp_, b, _q},
and rpcR,λpfpa, cqq[rpcR,λpgpaqq “ {hpb, a,_q, hpb, b,_q}. We show how to
compute this set with C: {x in {fpa, cq, gpaq}}; x
“⇒Red
{y1 in {a, c}, y2
:
{a}, y3 in H}; hpy1, y2, y3q “⇒Rem {y1 in {a, c}, y2 : {a}}; hpy1, y2, _q “⇒Red
{y2 in {a}}; hpb, y2, _q. Here we have two ways to apply Red to the last
state, leading to two elements of rpcR,λpfpa, cqq [ rpcR,λpgpaqq: hpb, a, _q and
hpb, b, _q.
Theorem 2. Given a ﬁnite set of terms T, the algorithm C always terminates
starting from the state {x in T}; x (where x is a fresh variable). If S is the set
of success states produced at the end, we have {s | H; s P S} “ Ű
tPT rpcR,λptq.
Proof. Termination: Associate to each state {x1 in T1, . . . xn in Tn}; s the multi-
set {d1, . . . , dn}, where di is the maximum depth of terms occurring in Ti. di “ 0
if Ti “ H. Compare these multisets by the Dershowitz-Manna ordering [5]. Each
rule strictly reduces them, which implies termination.
By the deﬁnitions of rpcR,λ and [, hps1, . . . , snq P Ű
tP{t1,...,tm} rpcR,λptq iﬀ
h „ρk
R,γk headptkq with γk ě λ for all 1 ď k ď m and si P Ű
tPTi rpcR,λptq, where
Ti “ {tk|j | pi, jq P ρk, 1 ď k ď m}, 1 ď i ď n. Therefore, in the Rem rule,
the instance of x (which is hpy1, . . . , ynq) is in Ű
tP{t1,...,tm} rpcR,λptq iﬀfor each
1 ď i ď n we can ﬁnd an instance of yi in Ű
tPTi rpcR,λptq. If Ti is empty, it
means that the i’s argument of h is irrelevant for terms in {t1, . . . , tm} and can be
replaced by _. (Rem does it in a subsequent step.) Hence, in each success branch
of the derivation tree, the algorithm C computes one element of Ű
tPT rpcR,λptq.
Branching at Red helps produce all elements of Ű
tPT rpcR,λptq.
□

586
T. Kutsia and C. Pau
It is easy to see how to use C to decide the pR, λq-consistency of T: it is
enough to ﬁnd one successful branch in the C-derivation tree for {x in T}; x.
If there is no such branch, then T is not pR, λq-consistent. In fact, during the
derivation we can even ignore the second component of the states.
4
Solving Generalization Problems
Now we can reformulate the anti-uniﬁcation problem that will be solved in the
remaining part of the paper. R is a proximity relation and λ is a cut value.
Given: R, λ, and the ground terms t1, . . . , tn, n ě 2.
Find: a set S of tuples pr, σ1, . . . , σn, α1, . . . , αnq such that
– {r | pr, . . .q P S} is an pR, λq-mcsrg of t1, . . . , tn,
– rσi
»R,λ
ti
and
αi
“
gdubR,λpr, tiq,
1
ď
i
ď
n,
for
each
pr, σ1, . . . , σn, α1, . . . , αnq P S.
(When n “ 1, this is a problem of computing a relevant proximity class of
a term.) Below we give a set of rules, from which one can obtain algorithms to
solve the anti-uniﬁcation problem for four versions of argument relations:
1. The most general (unrestricted) case; see algorithm A1 below, the computed
set of generalizations is an mcsrg;
2. Correspondence relations: using the same algorithm A1, the computed set of
generalizations is an mcsg;
3. Mappings: using a dedicated algorithm A2, the computed set of generaliza-
tions is an mcsrg;
4. Correspondence mappings (bijections): using the same algorithm A2, the com-
puted set of generalizations is an mcsg.
Each of them has also the corresponding linear variant, computing minimal
complete sets of (relevant) linear pR, λq-generalizations. They are denoted by
adding the superscript lin to the corresponding algorithm name: Alin
1 and Alin
2 .
For simplicity, we formulate the algorithms for the case n “ 2. They can be
extended for arbitrary n straightforwardly.
The main data structure in these algorithms is an anti-uniﬁcation triple
(AUT) x : T1 ﬁT2, where T1 and T2 are ﬁnite consistent sets of ground terms.
The idea is that x is a common generalization of all terms in T1 Y T2. A conﬁg-
uration is a tuple A; S; r; α1; α2, where A is a set of AUTs to be solved, S is a
set of solved AUTs (the store), r is the generalization computed so far, and the
α’s are the current approximations of generalization degree upper bounds of r
for the input terms.
Before formulating the rules, we discuss one peculiarity of approximate gen-
eralizations:
Example 5. For a given R and λ, assume Rpa, bq ě λ, Rpb, cq ě λ, h „{p1,1q,p1,2q}
R,α
f and h „{p1,1q}
R,β
g, where f is binary, g, h are unary, α ě λ and β ě λ. Then

A Framework for Approximate Generalization in Quantitative Theories
587
– hpbq is an pR, λq-generalization of fpa, cq and gpaq.
– x is the only pR, λq-generalization of fpa, dq and gpaq. One may be tempted
to have h as the head of the generalization, e.g., hpxq, but x cannot be instan-
tiated by any term that would be pR, λq-close to both a and d, since in the
given R, d is pR, λq-close only to itself. Hence, there would be no instance of
hpxq that is pR, λq-close to fpa, dq. Since there is no other alternative (except
h) for the common neighbor of f and g, the generalization should be a fresh
variable x.
This example shows that generalization algorithms should take into account not
only the heads of the terms to be generalized, but also should look deeper, to
make sure that the arguments grouped together by the given argument relation
have a common neighbor. This justiﬁes the requirement of consistency of a set
of arguments, the notion introduced in the previous section and used in the
decomposition rule below.
4.1
Anti-uniﬁcation for Unrestricted Argument Relations
Algorithms Alin
1
and A1 use the rules below to transform conﬁgurations into
conﬁgurations. Given R, λ, and the ground terms t1 and t2, we create the initial
conﬁguration {x : {t1} ﬁ{t2}}; H; x; 1; 1 and apply the rules as long as possible.
Note that the rules preserve consistency of AUTs. The process generates a ﬁnite
complete tree of derivations, whose terminal nodes have conﬁgurations with the
ﬁrst component empty. We will show how from these terminal conﬁgurations one
collects the result as required in the anti-uniﬁcation problem statement.
Tri: Trivial
{x : H ﬁH} Z A; S; r; α1; α2 “⇒A; S; r{x →_}; α1; α2.
Dec: Decomposition
{x : T1 ﬁT2} Z A; S; r; α1; α2 “⇒
{yi : Qi1 ﬁQi2 | 1 ď i ď n} Y A; S; r{x →hpy1, . . . , ynq}; α1 ^ β1; α2 ^ β2,
where T1 Y T2 ‰ H; h is n-ary with n ě 0; y1, . . . , yn are fresh; and for j “ 1, 2,
if Tj “ {tj
1, . . . , tj
mj}, then
– h „
ρj
k
R,γj
k headptj
kq with γj
k ě λ for all 1 ď k ď mj and βj “ γj
1 ^ · · · ^ γj
mj
(note that βj “ 1 if mj “ 0),
– for all 1 ď i ď n, Qij “ Ymj
k“1{tj
k|q | pi, qq P ρj
k} and is pR, λq-consistent.
Sol: Solving
{x : T1 ﬁT2} Z A; S; r; α1; α2 “⇒A; {x : T1 ﬁT2} Y S; r; α1; α2,
if Tri and Dec rules are not applicable. (It means that at least one Ti ‰ H and
either there is no h as it is required in the Dec rule, or at least one Qij from Dec
is not pR, λq-consistent.)

588
T. Kutsia and C. Pau
Let expand be an expansion operation deﬁned for sets of AUTs as
expandpSq :“ {x :
ę
tPT1
rpcR,λptq ≜
ę
tPT2
rpcR,λptq | x : T1 ≜T2 P S}.
Exhaustive application of the three rules above leads to conﬁgurations of the
form H; S; r; α1; α2, where r is a linear term. These conﬁgurations are further
postprocessed, replacing S by expandpSq. We will use the letter E for expanded
stores. Hence, terminal conﬁgurations obtained after the exhaustive rule appli-
cation and expansion have the form H; E; r; α1; α2, where r is a linear term.1
This is what Algorithm Alin
1 stops with.
To an expanded store E “ {y1 : Q11 ﬁQ12, . . . , yn : Qn1 ﬁQn2} we associate
two sets of substitutions ΣLpEq and ΣRpEq, deﬁned as follows: σ P ΣLpEq (resp.
σ P ΣRpEq) iﬀdompσq “ {y1, . . . , yn} and yiσ P Qi1 (resp. yiσ P Qi2) for each
1 ď i ď n. We call them the sets of witness substitutions.
Conﬁgurations containing expanded stores are called expanded conﬁgurations.
From each expanded conﬁguration C “ H; E; r; α1; α2, we construct the set
SpCq :“ {pr, σ1, σ2, α1, α2q | σ1 P ΣLpEq, σ2 P ΣRpEq}.
Given an anti-uniﬁcation problem R, λ, t1 and t2, the answer computed by
Algorithm Alin
1 is the set S :“ Ym
i“1SpCiq, where C1, . . . , Cm are all of the ﬁnal
expanded conﬁgurations reached by Alin
1 for R, λ, t1, and t2.2
Example 6. Assume a, b, c and d are constants with b „H
R,0.5 c, c „H
R,0.6 d, and f,
g and h are respectively binary, ternary and quaternary function symbols with
h „{p1,1q,p3,2q,p4,2q}
R,0.7
f and h „{p1,1q,p3,3q}
R,0.8
g. For the proximity relation R given in
this way and λ “ 0.5, Algorithm Alin
1 performs the following steps to anti-unify
fpa, bq and gpa, c, dq:
{x : {fpa, bq} ﬁ{gpa, c, dq}}; H; x; 1; 1 “⇒Dec
{x1 : {a} ﬁ{a}, x2 : H ﬁH, x3 : {b} ﬁ{d},
x4 : {b} ﬁH}; H; hpx1, x2, x3, x4q; 0.7; 0.8 “⇒Dec
{x2 : H ﬁH, x3 : {b} ﬁ{d}, x4 : {b} ﬁH}; H; hpa, x2, x3, x4q; 0.7; 0.8 “⇒Tri
{x3 : {b} ﬁ{d}, x4 : {b} ﬁH}; H; hpa, _, x3, x4q; 0.7; 0.8 “⇒Dec
{x4 : {b} ﬁH}; H; hpa, _, c, x4q; 0.5; 0.6.
Here Dec applies in two diﬀerent ways, with the substitutions {x4 →b}
and {x4 →c}, leading to two ﬁnal conﬁgurations: H; H; hpa, _, c, bq; 0.5; 0.6 and
H; H; hpa, _, c, cq; 0.5; 0.6. The witness substitutions are the identity substitu-
tions. We have Rphpa, _, c, bq, fpa, bqq “ 0.5, Rphpa, _, c, bq, gpa, c, dqq “ 0.6,
Rphpa, _, c, cq, fpa, bqq “ 0.5, and Rphpa, _, c, cq, gpa, c, dqq “ 0.6.
If we had h „{p1,1q,p1,2q,p4,2q}
R,0.7
f, then the algorithm would perform only the
Sol step, because in the attempt to apply Dec to the initial conﬁguration, the set
1 Note that no side of the AUTs in E in those conﬁgurations is empty due to the
condition at the Decomposition rule requiring the Qij’s to be pR, λq-consistent.
2 If we are interested only in linear generalizations without witness substitutions, there
is no need in computing expanded conﬁgurations in Alin
1 .

A Framework for Approximate Generalization in Quantitative Theories
589
Q11 “ {a, b} is inconsistent: rpcR,λpaq “ {a}, rpcR,λpbq “ {b, c}, and, hence,
rpcR,λpaq [ rpcR,λpbq “ H.
Algorithm A1 is obtained by further transforming the expanded conﬁgura-
tions produced by Alin
1 . This transformation is performed by applying the Merge
rule below as long as possible. Intuitively, its purpose is to make the linear gen-
eralization obtained by Alin
1 less general by merging some variables.
Mer: Merge
H; {x1 : R11 ﬁR12, x2 : R21 ﬁR22} Z E; r; α1; α2 “⇒
H; {y : Q1 ﬁQ2} Y E; rσ; α1; α2,
where Qi “ pR1i [ R2iq ‰ H, i “ 1, 2, y is fresh, and σ “ {x1 →y, x2 →y}.
The answer computed by A1 is deﬁned similarly to the answer computed by Alin
1 .
Example 7. Assume a, b are constants, f1, f2, g1, and g2 are unary function
symbols, p is a binary function symbol, and h1 and h2 are ternary function
symbols. Let λ be a cut value and R be deﬁned as fi „{p1,1q}
R,αi
hi and gi „{p1,2q}
R,βi
hi
with αi ě λ, βi ě λ, i “ 1, 2. To generalize ppf1paq, g1pbqq and ppf2paq, g2pbqq,
we use A1. The derivation starts as
{x : {ppf1paq, g1pbqq} ﬁ{ppf2paq, g2pbqq}}; H; x; 1; 1 “⇒Dec
{y1 : {f1paq} ﬁ{f2paq}, y2 : {g1pbq} ﬁ{g2pbq}}; H; ppy1, y2q; 1; 1 “⇒2
Sol
H; {y1 : {f1paq} ﬁ{f2paq}, y2 : {g1pbq} ﬁ{g2pbq}}; ppy1, y2q; 1; 1.
At this stage, we expand the store, obtaining
H; {y1 : {f1paq, h1pa, _, _q} ﬁ{f2paq, h2pa, _, _q},
y2 : {g1pbq, h1p_, b, _q} ﬁ{g2pbq, h2p_, b, _q}}; ppy1, y2q; 1; 1.
If we had the standard intersection X in the Mer rule, we would not be able to
merge y1 and y2, because the obtained sets in the corresponding AUTs are dis-
joint. However, Mer uses [: we have {fipaq, hipa, _, _q} [ {gipbq, hip_, b, _q} “
{hipa, b, _q}, i “ 1, 2 and, therefore, can make the step
H; {y1 : {f1paq, h1pa, _, _q} ﬁ{f2paq, h2pa, _, _q},
y2 : {g1pbq, h1p_, b, _q} ﬁ{g2pbq, h2p_, b, _q}}; ppy1, y2q; 1; 1 “⇒Mer
H; {z : {h1pa, b, _q} ﬁ{h2pa, b, _q}}; ppz, zq; 1; 1.
Indeed, if we take the witness substitutions σi “ {z →hipa, b, _q}, i “ 1, 2, and
apply them to the obtained generalization, we get
ppz, zqσ1 “ pph1pa, b, _q, h1pa, b, _qq »R,λ ppf1paq, g1pbqq,
ppz, zqσ2 “ pph2pa, b, _q, h2pa, b, _qq »R,λ ppf2paq, g2pbqq.
Theorem 3. Given R, λ, and the ground terms t1 and t2, Algorithm A1 ter-
minates for {x : {t1} ﬁ{t2}}; H; x; 1; 1 and computes an answer set S such
that

590
T. Kutsia and C. Pau
1. the set {r | pr, σ1, σ2, α1, α2q P S} is an pR, λq-mcsrg of t1 and t2,
2. for each pr, σ1, σ2, α1, α2q P S we have Rprσi, tiq ď αi “ gdubR,λpr, tiq, i “
1, 2.
Proof. Termination: Deﬁne the depth of an AUT x : {t1, . . . , tm} ﬁ{s1, . . . ,
sn} as the depth of the term fpgpt1, . . . , tmq, hps1, . . . , snqq. The rules Tri, Dec,
and Sol strictly reduce the multiset of depths of AUTs in the ﬁrst component
of the conﬁgurations. Mer strictly reduces the number of distinct variables in
generalizations. Hence, these rules cannot be applied inﬁnitely often and A1
terminates.
In order to prove (1), we need to verify three properties:
– Soundness: If pr, σ1, σ2, α1, α2q P S, then r is a relevant pR, λq-generalization
of t1 and t2.
– Completeness: If r′ is a relevant pR, λq-generalization of t1 and t2, then there
exists pr, σ1, σ2, α1, α2q P S such that r′ À r.
– Minimality: If r and r′ belong to two tuples from S such that r ‰ r′, then
neither r ≺R,λ r′ nor r′ ≺R,λ r.
Soundness: We show that each rule transforms an pR, λq-generalization into an
pR, λq-generalization. Since we start from a most general pR, λq-generalization
of t1 and t2 (a fresh variable x), at the end of the algorithm we will get an
pR, λq-generalization of t1 and t2. We also show that in this process all irrele-
vant positions are abstracted by anonymous variables, to guarantee that each
computed generalization is relevant.
Dec: The computed h is pR, λq-close to the head of each term in T1 Y T2. Qij’s
correspond to argument relations between h and those heads, and each Qij is
pR, λq-consistent, i.e., there exists a term that is pR, λq-close to each term in
Qij. It implies that xσ “ hpy1, . . . , ynq pR, λq-generalizes all the terms from
T1 YT2. Note that at this stage, hpy1, . . . , ynq might not yet be a relevant pR, λq-
generalization of T1 and T2: if there exists an irrelevant position 1 ď i ď n for
the pR, λq-generalization of T1 and T2, then in the new conﬁguration we will
have an AUT yi : H ﬁH.
Tri: When Dec generates y : H ﬁH, the Tri rule replaces y by _ in the computed
generalization, making it relevant.
Sol does not change generalizations.
Mer merges AUTs whose terms have nonempty intersection of rpc’s. Hence,
we can reuse the same variable in the corresponding positions in generalizations,
i.e., Mer transforms a generalization computed so far into a less general one.
Completeness: We prove a slightly more general statement. Given two ﬁnite
consistent sets of ground terms T1 and T2, if r′ is a relevant pR, λq-generalization
for all t1 P T1 and t2 P T2, then starting from {x : T1 ﬁT2}; H; x; 1; 1, Algorithm
A1 computes a pr, σ1, σ2, α1, α2q such that r′ À r.
We may assume w.l.o.g. that r′ is a relevant pR, λq-lgg. Due to the transitivity
of À, completeness for such an r′ will imply it for all terms more general than r′.

A Framework for Approximate Generalization in Quantitative Theories
591
We proceed by structural induction on r′. If r′ is a (named or anonymous)
variable, the statement holds. Assume r′ “ hpr′
1, . . . , r′
nq, T1 “ {u1, . . . , um},
and T2 “ {w1, . . . , wl}. Then h is such that h „ρi
R,βi headpuiq for all 1 ď i ď m
and h „μj
R,γj headpwjq for all 1 ď j ď l. Moreover, each r′
k is a relevant pR, λq-
generalization of Qk1 “ Ym
i“1{ui|q | pk, qq P ρi} and Qk2 “ Yl
j“1{wj|q | pk, qq P
μj} and, hence, Qk1 and Qk2 are pR, λq-consistent. Therefore, we can perform a
step by Dec, choosing hpy1, . . . , ykq as the generalization term and yi : Qi1 ﬁQi2
as the new AUTs. By the induction hypothesis, for each 1 ď i ď n we can
compute a relevant pR, λq-generalization ri for Qi1 and Qi2 such that r′
i À ri.
If r′ is linear, then the combination of the current Dec step with the deriva-
tions that lead to those ri’s computes a tuple pr, . . .q P S, where r “ hpr1, . . . , rnq
and, hence, r′ À r.
If r′ is non-linear, assume without loss of generality that all occurrences of a
shared variable z appear as the direct arguments of h: z “ r′
k1 “ · · · “ r′
kp for
1 ď k1 < · · · < kp ď n. Since r′ is an lgg, Qki1 and Qki2 cannot be generalized by
a non-variable term, thus, Tri and Dec are not applicable. Therefore, the AUTs
yi : Qki1 ﬁQki2 would be transformed by Sol. Since all pairs Qki1 and Qki2,
1 ď i ď p, are generalized by the same variable, we have [tPQjrpcR,λptq ‰ H,
where Qj “ Yp
i“1Qkij, j “ 1, 2. Additionally, r′
k1, . . . , r′
kp are all occurrences of z
in r′. Hence, the condition of Mer is satisﬁed and we can extend our derivation
with p ´ 1-fold application of this rule, obtaining r “ hpr1, . . . , rnq with z “
rk1 “ · · · “ rkp, implying r′ À r.
Minimality: Alternative generalizations are obtained by branching in Dec or Mer.
If the current generalization r is transformed by Dec into two generalizations
r1 and r2 on two branches, then r1 “ h1py1, . . . , ymq and r2 “ h2pz1, . . . , znq
for some h’s, and fresh y’s and z’s. It may happen that r1 ÀR,λ r2 or vice
versa (if h1 and h2 are pR, λq-close to each other), but neither r1 ≺R,λ r2 nor
r2 ≺R,λ r1 holds. Hence, the set of generalizations computed before applying
Mer is minimal. Mer groups AUTs together maximally, and diﬀerent groupings
are not comparable. Therefore, variables in generalizations are merged so that
distinct generalizations are not ≺R,λ-comparable. Hence, (1) is proven.
As for (2), for i “ 1, 2, from the construction in Dec follows Rprσi, tiq ď αi.
Mer does not change αi, thus, αi “ gdubR,λpr, tiq also holds, since the way how αi
is computed corresponds exactly to the computation of gdubR,λpr, tiq: r ÀR,λ ti
and only the decomposition changes the degree during the computation.
□
The corollary below is proved similarly to Theorem 3:
Corollary 1. Given R, λ, and the ground terms t1 and t2, Algorithm Alin
1 ter-
minates for {x : {t1} ﬁ{t2}}; H; x; 1; 1 and computes an answer set S such
that
1. the set {r | pr, σ1, σ2, α1, α2q P S} is a minimal complete set of relevant linear
pR, λq-generalizations of t1 and t2,
2. for each pr, σ1, σ2, α1, α2q P S we have Rprσi, tiq ď αi “ gdubR,λpr, tiq, i “
1, 2.

592
T. Kutsia and C. Pau
4.2
Anti-uniﬁcation with Correspondence Argument Relations
Correspondence relations make sure that for a pair of proximal symbols, no
argument is irrelevant for proximity. Left- and right-totality of those relations
guarantee that each argument of a term is close to at least one argument of its
proximal term and the inverse relation remains a correspondence relation. Con-
sequently, in the Dec rule of A1, the sets Qij never get empty. Therefore, the Tri
rule becomes obsolete and no anonymous variable appears in generalizations. As
a result, the pR, λq-mcsrg and the pR, λq-mcsg coincide, and the algorithm com-
putes a solution from which we get an pR, λq-mcsg for the given anti-uniﬁcation
problem. The linear version Alin
1 works analogously.
4.3
Anti-uniﬁcation with Argument Mappings
When the argument relations are mappings, we are able to design a more con-
structive method for computing generalizations and their degree bounds (Recall
that our mappings are partial injective functions, which guarantees that their
inverses are also mappings.) We denote this algorithm by A2. The conﬁgurations
stay the same as in before, but the AUTs in A will contain only empty or single-
ton sets of terms. In the store, we may still get (after the expansion) AUTs with
term sets containing more than one element. Only the Dec rule diﬀers from its
previous counterpart, having a simpler condition:
Dec: Decomposition
{x : T1 ﬁT2} Z A; S; r; α1; α2 “⇒
{yi : Qi1 ﬁQi2 | 1 ď i ď n} Y A; S; r{x →hpy1, . . . , ynq}; α1 ^ β1; α2 ^ β2,
where T1 Y T2 ‰ H; h is n-ary with n ě 0; y1, . . . , yn are fresh; for j “ 1, 2 and
for all 1 ď i ď n, if Tj “ {tj} then h „πj
R,βj headptjq and Qij “ {tj|πjpiq}, and if
Tj “ H then βj “ 1 and Qij “ H.
This Dec rule is equivalent to the special case of Dec for argument relations
where mj ď 1. The new Qij’s contain at most one element (due to mappings)
and, thus, are always pR, λq-consistent. Various choices of h in Dec and alterna-
tives in grouping AUTs in Mer cause branching in the same way as in A1. It is
easy to see that the counterparts of Theorem 3 hold for A2 and Alin
2 as well.
A special case of this fragment of anti-uniﬁcation is anti-uniﬁcation for sim-
ilarity relations in fully fuzzy signatures from [1]. Similarity relations are min-
transitive proximity relations. The position mappings in [1] can be modeled by
our argument mappings, requiring them to be total for symbols of the smaller
arity and to satisfy the similarity-speciﬁc consistency restrictions from [1].
4.4
Anti-uniﬁcation with Correspondence Argument Mappings
Correspondence argument mappings are bijections between arguments of func-
tion symbols of the same arity. For such mappings, if h »π
R,λ f and h is n-ary,
then f is also n-ary and π is a permutation of p1, . . . , nq. Hence, A2 combines

A Framework for Approximate Generalization in Quantitative Theories
593
in this case the properties of A1 for correspondence relations (Sect. 4.2) and of
A2 for argument mappings (Sect. 4.3): all generalizations are relevant, computed
answer gives an mcsg of the input terms, and the algorithm works with term
sets of cardinality at most 1.
5
Remarks About the Complexity
The proximity relation R can be naturally represented as an undirected graph,
where the vertices are function symbols and an edge between them indicates that
they are proximal. Graphs induced by proximity relations are usually sparse.
Therefore we can represent them by (sorted) adjacency lists. In the adjacency
lists, we can also accommodate the argument relations and proximity degrees.
In the rest of this section we use the following notation:
– n: the size of the input (number of symbols) of the corresponding algorithms,
– Δ: the maximum degree of R considered as a graph,
– a: the maximum arity of function symbols that occur in R.
– m•n: a function deﬁned on natural numbers m and n such that 1•n “ n and
m•n “ mn for m ‰ 1.
We assume that the given anti-uniﬁcation problem is represented as a com-
pletely shared directed acyclic graph (dag). Each node of the dag has a pointer
to the adjacency list (with respect to R) of the symbol in the node.
Theorem 4. Time complexities of C and the linear versions of the generaliza-
tion algorithms are as follows:
– C for argument relations and Alin
1 :
Opn · Δ · Δ•a•nq,
– C for argument mappings and Alin
2 :
Opn · Δ · Δ•nq.
Proof (Sketch). In C, in the case of argument relations, an application of the Red
rule to a state I; s replaces one element of I of size m by at most a new elements,
each of them of size m ´ 1. Hence, one branch in the search tree for C, starting
from a singleton set I of size n, will have the length at most l “ n´1
i“0 ai. At each
node on it there are at most Δ choices of applying Red with diﬀerent h’s, which
gives the total size of the search tree to be at most l´1
i“0 Δi, i.e., the number
of steps performed by C in the worst case is OpΔ•a•nq. Those diﬀerent h’s are
obtained by intersecting the proximity classes of the heads of terms {t1, . . . , tm}
in the Red rule. In our graph representation of the proximity relation, proximity
classes of symbols are exactly the adjacency lists of those symbols which we
assume are sorted. Their maximal length is Δ. Hence, the work to be done at
each node of the search tree of C is to ﬁnd the intersection of at most n sorted
lists, each containing at most Δ elements. It needs Opn · Δq time. It gives the
time complexity Opn · Δ · Δ•a•nq of C for the relation case.
In the mapping case, an application of the Red rule to a state I; s replaces
one element of I of size m by at most a new elements of the total size m ´ 1.
Therefore, the maximal length of a branch is n, the branching factor is Δ, and

594
T. Kutsia and C. Pau
the amount of work at each node, like above, is Opn · Δq. Hence, the number of
steps in the worst case is OpΔ•nq and the time complexity of C is Opn·Δ·Δ•nq.
The fact that consistency check is incorporated in the Dec rule in Alin
1 can be
used to guide the application of this rule, using the values memoized by the pre-
vious applications of Red. The very ﬁrst time, the appropriate h in Dec is chosen
arbitrarily. In any subsequent application of this rule, h is chosen according to
the result of the Red rule that has already been applied to the arguments of the
current AUT for their consistency check, as required by the condition of Dec. In
this way, the applications of Dec and Sol will correspond to the applications of
Red. There is a natural correspondence between the applications of Rem and Tri
rules. Therefore, Alin
1 will have the search tree analogous to that of C. Hence the
complexity of Alin
1 is Opn·Δ·Δ•a•nq. Alin
2 does not call the consistency check, but
does the same work as C and, hence, has the same complexity Opn · Δ · Δ•nq. □
6
Discussion and Conclusion
The diagram below illustrates the connections between diﬀerent anti-uniﬁcation
problems based on argument relations:
unrestricted relations
unrestricted mappings
correspondence relations
correspondence mappings
The arrows indicate the direction from more general problems to more spe-
ciﬁc ones. For the unrestricted cases (left column) we compute mcsrg’s. For
correspondence relations and correspondence mappings (right column), mcsg’s
are computed. (In fact, for them, the notions of mcsrg and mcsg coincide). The
algorithms for relations (upper row) are more involved than those for mappings
(lower row): Those for relations deal with AUTs containing arbitrary sets of
terms, while for mappings, those sets have cardinality at most one, thus sim-
plifying the conditions in the rules. Moreover, the two cases in the lower row
generalize the existing anti-uniﬁcation problems:
– the unrestricted mappings case generalizes the problem from [1] by extending
similarity to proximity and relaxing the smaller-side-totality restriction;
– the correspondence mappings case generalizes the problem from [9] by allow-
ing permutations between arguments of proximal function symbols.
All our algorithms can be easily turned into anti-uniﬁcation algorithms for
crisp tolerance relations3 by taking lambda-cuts and ignoring the computation of
the approximation degrees. Besides, they are modular and can be used to com-
pute only linear generalizations by just skipping the merging rule. We provided
complexity estimations for the algorithms that compute linear generalizations
(that often are of practical interest).
3 Tolerance: reﬂexive, symmetric, not necessarily transitive relation. According to
Poincaré, a fundamental notion for mathematics applied to the physical world.

A Framework for Approximate Generalization in Quantitative Theories
595
In this paper, we did not consider cases when the same pair of symbols is
related to each other by more than one argument relation. Our results can be
extended to them, that would open a way towards approximate anti-uniﬁcation
modulo background theories speciﬁed by shallow collapse-free axioms. Another
interesting direction of future work would be extending our results to quantita-
tive algebras [10] that also deal with quantitative extensions of equality.
Acknowledgments. Supported by the Austrian Science Fund, project P 35530.
References
1. Aït-Kaci, H., Pasi, G.: Fuzzy lattice operations on ﬁrst-order terms over signatures
with similar constructors: a constraint-based approach. Fuzzy Sets Syst. 391, 1–46
(2020). https://doi.org/10.1016/j.fss.2019.03.019
2. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press,
Cambridge (1998)
3. Bader, J., Scott, A., Pradel, M., Chandra, S.: Getaﬁx: learning to ﬁx bugs auto-
matically. Proc. ACM Program. Lang. 3(OOPSLA), 159:1–159:27 (2019). https://
doi.org/10.1145/3360585
4. Barwell, A.D., Brown, C., Hammond, K.: Finding parallel functional pearls: Auto-
matic parallel recursion scheme detection in Haskell functions via anti-uniﬁcation.
Future Gener. Comput. Syst. 79, 669–686 (2018). https://doi.org/10.1016/j.future.
2017.07.024
5. Dershowitz, N., Manna, Z.: Proving termination with multiset orderings. Commun.
ACM 22(8), 465–476 (1979). https://doi.org/10.1145/359138.359142
6. Galitsky, B.: Developing Enterprise Chatbots - Learning Linguistic Structures.
Springer, Heidelberg (2019). https://doi.org/10.1007/978-3-030-04299-8
7. Kirbas, S., et al.: On the introduction of automatic program repair in Bloomberg.
IEEE Softw. 38(4), 43–51 (2021). https://doi.org/10.1109/MS.2021.3071086
8. Kutsia, T., Pau, C.: A framework for approximate generalization in quantitative
theories. RISC Report Series 22-04, Research Institute for Symbolic Computation,
Johannes Kepler University Linz (2022). https://doi.org/10.35011/risc.22-04
9. Kutsia, T., Pau, C.: Matching and generalization modulo proximity and tolerance
relations. In: Özgün, A., Zinova, Y. (eds.) TbiLLC 2019. LNCS, vol. 13206, pp.
323–342. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-98479-3_16
10. Mardare, R., Panangaden, P., Plotkin, G.D.: Quantitative algebraic reasoning.
In: Grohe, M., Koskinen, E., Shankar, N. (eds.) Proceedings of the 31st Annual
ACM/IEEE Symposium on Logic in Computer Science, LICS 2016, pp. 700–709.
ACM (2016). https://doi.org/10.1145/2933575.2934518
11. Mehta, S., et al.: Rex: preventing bugs and misconﬁguration in large services using
correlated change analysis. In: Bhagwan, R., Porter, G. (eds.) 17th USENIX Sym-
posium on Networked Systems Design and Implementation, NSDI 2020, Santa
Clara, CA, USA, 25–27 February 2020, pp. 435–448. USENIX Association (2020).
https://www.usenix.org/conference/nsdi20/presentation/mehta
12. Plotkin, G.D.: A note on inductive generalization. Mach. Intell. 5(1), 153–163
(1970)
13. Raza, M., Gulwani, S., Milic-Frayling, N.: Programming by example using least
general generalizations. In: Brodley, C.E., Stone, P. (eds.) Proceedings of the
Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence, 27–31 July 2014,
Québec City, Québec, Canada, pp. 283–290. AAAI Press (2014)

596
T. Kutsia and C. Pau
14. Reynolds, J.C.: Transformational systems and the algebraic structure of atomic
formulas. Mach. Intell. 5(1), 135–151 (1970)
15. Rolim, R., Soares, G., Gheyi, R., D’Antoni, L.: Learning quick ﬁxes from code
repositories. CoRR abs/1803.03806 (2018). http://arxiv.org/abs/1803.03806
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Guiding an Automated Theorem Prover
with Neural Rewriting
Jelle Piepenbrock1,2(B)
, Tom Heskes2
, Mikoláš Janota1
,
and Josef Urban1
1 Czech Technical University in Prague, Prague, Czech Republic
Jelle.Piepenbrock@cvut.cz
2 Radboud University, Nijmegen, The Netherlands
Abstract. Automated theorem provers (ATPs) are today used to attack
open problems in several areas of mathematics. An ongoing project by
Kinyon and Veroﬀuses Prover9 to search for the proof of the Abelian
Inner Mapping (AIM) Conjecture, one of the top open conjectures in
quasigroup theory. In this work, we improve Prover9 on a benchmark
of AIM problems by neural synthesis of useful alternative formulations
of the goal. In particular, we design the 3SIL (stratiﬁed shortest solu-
tion imitation learning) method. 3SIL trains a neural predictor through
a reinforcement learning (RL) loop to propose correct rewrites of the
conjecture that guide the search.
3SIL is ﬁrst developed on a simpler, Robinson arithmetic rewriting
task for which the reward structure is similar to theorem proving. There
we show that 3SIL outperforms other RL methods. Next we train 3SIL
on the AIM benchmark and show that the ﬁnal trained network, deciding
what actions to take within the equational rewriting environment, proves
70.2% of problems, outperforming Waldmeister (65.5%). When we com-
bine the rewrites suggested by the network with Prover9, we prove 8.3%
more theorems than Prover9 in the same time, bringing the performance
of the combined system to 90%.
Keywords: Automated theorem proving · Machine learning
1
Introduction
Machine learning (ML) has recently proven its worth in a number of ﬁelds, rang-
ing from computer vision [17], to speech recognition [15], to playing games [28,40]
with reinforcement learning (RL) [45]. It is also increasingly applied in auto-
mated and interactive theorem proving. Learned predictors have been used for
premise selection [1] in hammers [6], to improve clause selection in saturation-
based theorem provers [9], to synthesize functions in higher-order logic [12], and
to guide connection-tableau provers [21] and interactive theorem provers [2,5,14].
Future growth of the knowledge base of mathematics and the complexity of
mathematical proofs will increase the need for proof checking and its better com-
puter support and automation. Simultaneously, the growing complexity of soft-
ware will increase the need for formal veriﬁcation to prevent failure modes [10].
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 597–617, 2022.
https://doi.org/10.1007/978-3-031-10769-6_35

598
J. Piepenbrock et al.
Automated theorem proving and mathematics will beneﬁt from more advanced
ML integration. One of the mathematical subﬁelds that makes substantial use
of automated theorem provers is the ﬁeld of quasigroup and loop theory [32].
1.1
Contributions
In this paper, we propose to use a neural network to suggest lemmas to the
Prover9 [25] ATP system by rewriting parts of the conjecture (Sect. 2). We test
our method on a dataset of theorems collected in the work on the Abelian Inner
Mapping (AIM) Conjecture [24] in loop theory. For this, we use the AIMLEAP
proof system [7] as a reinforcement learning environment. This setup is described
in Sect. 3. For development we used a simpler Robinson arithmetic rewriting
task (Sect. 4). With the insights derived from this and a comparison with other
methods, we describe our own 3SIL method in Sect. 5. We use a neural network to
process the state of the proving attempt, for which the architecture is described
in Sect. 6. The results on the Robinson arithmetic task are described in Sect. 7.1.
We show our results on the AIMLEAP proving task, both using our predictor
as a stand-alone prover and by suggesting lemmas to Prover9 in Sect. 7.2. Our
contributions are:
1. We propose a training method for reinforcement learning in theorem proving
settings: stratiﬁed shortest solution imitation learning (3SIL). This method
is suited to the structure of theorem proving tasks. This method and the
reasoning behind it is explained in Sect. 5.
2. We show that 3SIL outperforms other baseline RL methods on a simpler,
Robinson arithmetic rewriting task for which the reward structure is similar
to theorem proving (Sect. 7.1).
3. We show that a standalone neurally guided prover trained by the 3SIL
method outperforms the hand-engineered Waldmeister prover on the AIM-
LEAP benchmark (Sect. 7.2).
4. We show that using a neural rewriting step that suggests rephrased versions
of the conjecture to be added as lemmas improves the ATP performance on
equational problems (Sects. 2 and 7.2).
2
ATP and Suggestion of Lemmas by Neural Rewriting
Saturation-based ATPs make use of the given clause [30] algorithm, which we
brieﬂy explain as background. A problem is expressed as a conjunction of many
initial clauses (i.e., the clausiﬁed axioms and the negated goal which is always an
equation in the AIM dataset). The algorithm starts with all the initial clauses
in the unprocessed set. We then pick a clause from this set to be the given
clause and move it to the processed set and do all inferences with the clauses in
the processed set. The newly inferred clauses are added to the unprocessed set.
This concludes one iteration of the algorithm, after which we pick a new given

Guiding an Automated Theorem Prover with Neural Rewriting
599
Fig. 1. Schematic representation of the proposed guidance method. In the ﬁrst phase,
we run a reinforcement learning loop to propose actions that rewrite a conjecture. This
predictor is trained using the AIMLEAP proof environment. We collect the rewrites
of the LHS and RHS of the conjecture. In the second phase, we add the rewrites to
the ATP search input, to act as guidance. In this speciﬁc example, we only rewrote
the conjecture for 1 step, but the added guidance lemmas are in reality the product of
many steps in the RL loop.
clause and repeat [23]. Typically, this approach is designed to be refutationally
complete, i.e., the algorithm is guaranteed to eventually ﬁnd a contradiction if
the original goal follows from the axioms.
This process can produce a lot of new clauses and the search space can
become quite large. In this work, we modify the standard loop by adding useful
lemmas to the initial clause set. These lemmas are proposed by a neural network
that was trained from zero knowledge to rewrite the left- and right-hand sides of
the initial goal to make them equal by using the axioms as the available rewrite
actions. Even though the neural rewriting might not fully succeed, the rewrites
produced by this process are likely to be useful as additional lemmas when added
to the problem. This idea is schematically represented in Fig. 1.
3
AIM Conjecture and the AIMLEAP RL Environment
Automated theorem proving has been applied in the theory surrounding the
Abelian Inner Mapping Conjecture, known as the AIM Conjecture. This is one
of the top open conjectures in quasigroup theory. Work on the conjecture has
been going on for more than a decade. Automated theorem provers use hundreds
of thousands of inference steps when run on problems from this theory.
As a testbed for our machine learning and prover guidance methods we use
a previously published dataset of problems generated by the AIM conjecture [7].
The dataset comes with a simple prover called AIMLEAP that can take machine
learning advice.1 We use this system as an RL environment. AIMLEAP keeps the
state and carries out the cursor movements (the cursor determines the location
of the rewrite) and rewrites that a neural predictor chooses.
1 https://github.com/ai4reason/aimleap.

600
J. Piepenbrock et al.
The AIM conjecture concerns speciﬁc structures in loop theory [24]. A loop
is a quasigroup with an identity element. A quasigroup is a generalization of a
group that does not preserve associativity. This manifests in the presence of two
diﬀerent ‘division’ operators, one left-division (\) and one right-division (/). We
brieﬂy explain the conjecture to show the nature of the data.
For loops, three inner mapping functions (left-translation L, right-translation
R, and the mapping T) are:
L(u, x, y) := (y ∗x)\(y ∗(x ∗u))
R(u, x, y) := ((u ∗x) ∗y)/(x ∗y)
T(u, x) := x\(u ∗x)
These mappings can be seen as measures of the deviation from commutativity
and associativity. The conjecture concerns the consequences of these three inner
mapping functions forming an Abelian (commutative) group. There are two more
notions, that of the associator function a and the commutator function K:
a(x, y, z) := (x ∗(y ∗z))\((x ∗y) ∗z)
K(x, y) := (y ∗x)/(x ∗y)
From these deﬁnitions, the conjecture can be stated. There are two parts to the
conjecture. For both parts, the following equalities need to hold for all u, v, x,
y, and z:
a(a(x, y, z), u, v) = 1
a(x, a(y, z, u), v) = 1
a(x, y, a(z, u, v)) = 1
where 1 is the identity element. These are necessary, but not suﬃcient for the
two main parts of the conjecture. The ﬁrst part of the conjecture asks whether
a loop modulo its center is a group. In this context, the center is the set of all
elements that commute with all other elements. This is the case if
K(a(x, y, z), u) = 1.
The second part of the conjecture asks whether a loop modulo its nucleus is an
Abelian group. The nucleus is the set of elements that associate with all other
elements. This is the case if
a(K(x, y), z, u) = 1
a(x, K(y, z), u) = 1
a(x, y, K(z, u)) = 1
3.1
The AIMLEAP RL Environment
Currently, work in this area is done using automated theorem provers such as
Prover9 [24,25]. This has led to some promising results, but the search space
is enormous. The main strategy for proving the AIM conjecture thus far has
been to prove weaker versions of the conjecture (using additional assumptions)
and then import crucial proof steps into the stronger version of the proof. The
Prover9 theorem prover is especially suited to this approach because of its well-
established hints mechanism [48]. The AIMLEAP dataset is derived from this

Guiding an Automated Theorem Prover with Neural Rewriting
601
Prover9 approach and contains around 3468 theorems that can be proven with
the supplied deﬁnitions and lemmas [7].
There are 177 possible actions in the AIMLEAP environment [7]. We handle
the proof state as a tree, with the root node being an equality node. Three
actions are cursor movements, where the cursor can be moved to an argument
of the current position. The other actions all rewrite the current term at the
cursor position with various axioms, deﬁnitions and lemmas that hold in the
AIM context. As an example, this is one of the theorems in the dataset (\ and
= are part of the language):
T(T(T(x, T(x, y)\1), T(x, y)\1), y) = T((T(x, y)\1)\1, T(x, y)\1) .
The task of the machine learning predictor is to process the proof state and
recognize which actions are most likely to lead to a proof, meaning that the two
sides of the starting equation are equal according to the AIMLEAP system. The
only feedback that the environment gives is whether a proof has been found or
not: there is no intermediate reward (i.e. rewards are sparse). The ramiﬁcations
of this are further discussed in Sect. 5.1.
4
Rewriting in Robinson Arithmetic as an RL Task
To develop a machine learning method that can help solve equational theorem
proving problems, we considered a simpler arithmetic task, which also has a tree-
structured input and a sparse reward structure: the normalization of Robinson
arithmetic expressions. The task is to normalize a mathematical expression to
one speciﬁc form. This task has been implemented as a Python RL environment,
which we make available.2 The learning environment incorporates an existing
dataset, constructed by Gauthier for RL experiments in the interactive theorem
prover HOL4 [11]. Our RL setup for the task is also modeled after [11].
In more detail, the formalism that we use as an RL environment is Robinson
arithmetic (RA). RA is a simple arithmetic theory. Its language contains the
successor function S, addition + and multiplication * and one constant, the 0.
The theory considers only non-negative numbers and we only use four axioms
of RA. Numbers are represented by the constant 0 with the appropriate number
of successor functions applied to it. The task for the agent is to rewrite an
expression until there are only nodes of the successor or 0 types. Eﬀectively, we
are asking the agent to calculate the value of the expression. As an example,
S(S(0)) + S(0), representing 2 + 1, needs to be rewritten to S(S(S(0))).
The expressions are represented as a tree data structure. Within the environ-
ment, there are seven diﬀerent rewrite actions available to the agent. The four
axioms (equations) deﬁning these actions are x + 0 = x, x + S(y) = S(x + y),
x ∗0 = 0 and x ∗S(y) = (x ∗y) + x, where the agent can apply the equations
in either direction. There is one exception: the multiplication by 0 cannot be
applied from right to left, as this would require the agent to introduce a fresh
2 https://github.com/learningeqtp/rewriteRL.

602
J. Piepenbrock et al.
term which is out of scope for the current work. The place where the rewrite is
applied is denoted by the location of the cursor in the expression tree.
In addition to the seven rewrite actions, the agent can move the cursor to
one of the children of the current cursor node. This gives a total number of nine
actions. Moving to a child of a node with only one child counts as moving to
the left child. After a rewriting action, the cursor is reset to the root of the
expression. More details on the actions are in the RewriteRL repository.
5
Reinforcement Learning Methods
This section describes the reinforcement learning methods, while Sect. 6 then
further explains the particular neural architectures that are trained in the RL
loops. We ﬁrst brieﬂy explain here the approaches that we used as reinforcement
learning (RL) baselines, then we go into detail about the proposed 3SIL method.
5.1
Reinforcement Learning Baselines
General RL Setup. For comparison, we used implementations of four estab-
lished reinforcement learning baseline methods. In reinforcement learning, we
consider an agent that is acting within an environment. The agent can take
actions a from the action-space A to change the state s ∈S of the environment.
The agent can be rewarded for certain actions taken in a certain states, with
reward given by the reward function R : (S × A) →R. The behavior of the
environment is given by the state transition function P : (S × A) →S. The
history of the agent’s actions and the environments states and rewards at each
timestep t are collected in tuples (st, at, rt). For a given history of a certain
agent within an environment, we call the list of tuples (st, at, rt) describing this
history an episode. The policy function π : S →A allows the agent to decide
which action to take. The agent’s goal is to maximize the return R: the sum of
discounted rewards 
t≥0 γtrt, where γ is a discount factor that allows control
over how heavily rewards further in the future should be weighted. We will use
Rt when we mean R, but calculated only from rewards from timestep t on. In
the end, we are thus looking for a policy function π that maximizes the sum R
of (discounted) expected rewards [45].
In our setting, every proof attempt (in the AIM setting) or normalization
attempt (in the Robinson arithmetic setting) corresponds to an episode. The
reward structure of theorem proving is such that there is only a reward of 1 at
the end of a successful episode (i.e. a proof was found in AIM). Unsuccessful
episodes get a reward of 0 at every timestep t.
A2C. The ﬁrst method, Advantage Actor-Critic, or A2C [27] contains ideas on
which the other three RL baseline methods build, so we will go into more detail
for this method, while keeping the explanation for the other methods brief. For
details we refer to the corresponding papers.

Guiding an Automated Theorem Prover with Neural Rewriting
603
A2C attempts to ﬁnd suitable parameters for an agent by minimizing a loss
function consisting of two parts:
L = LA2C
policy + LA2C
value .
In addition to the policy function π, the agent has access to a value function
V : S →R, that predicts the sum of future rewards obtained when given a state.
In practice, both the policy and the value function are computed by a neural
network predictor. The parameters of the predictor are set by stochastic gradient
descent to minimize L. The set of parameters of the predictor that deﬁnes the
policy function π is named θ, while the parameters that deﬁne the value function
are named μ. The ﬁrst part of the loss is the policy loss, which for one time step
has the form
LA2C
policy = −log πθ(at|st)A(st, at) ,
where A(s, a) is the advantage function. The advantage function can be formu-
lated in multiple ways, but the simplest is as Rt −Vμ(st). That is to say: the
advantage of an action in a certain state is the diﬀerence between the discounted
rewards Rt after taking that action and the value estimate of the current state.
Minimizing LA2C
policy amounts to maximizing the log probability of predicting
actions that are judged by the advantage function to lead to high reward.
The value estimates Vμ(s) for computing the advantage function are supplied
by the value predictor Vμ with parameters μ, which is trained using the loss:
LA2C
value = 1
2 (Rt −Vμ(st))2 ,
which minimizes the advantage function. The logic of this is that the value
estimate at timestep t, Vμ(st), will learn to incorporate the later rewards Rt,
ensuring that when later seeing the same state, the possible future reward will
be considered. Note that the sets of parameters θ and μ are not necessarily
disjoint (see Sect. 6).
Note how the above equations are aﬀected if there is no non-zero reward rt
obtained at any timestep. In that case, the value function Vμ(st) will estimate
(correctly) that any state will get 0 reward, which means that the advantage
function A(s, a) will also be 0 everywhere. This means that LA2C
policy will be 0
in most cases, which will lead to no or little change in the parameters of the
predictor: learning will be very slow. This is the diﬃcult aspect of the structure
of theorem proving: there is only reward at the end of a successful proof, and
nowhere else. This implies a possible strategy is to imitate successful episodes,
without a value function. In this case, we would only need to train a policy
function, and no approximate value function. This an aspect we explore in the
design of our own method 3SIL, which we will explain shortly.
Compared to two-player games, such as chess and go, for which many
approaches have been tailored and successfully used [41], theorem-proving has
the property that it is hard to collect useful examples to learn from, as only

604
J. Piepenbrock et al.
successful proofs are likely to contain useful knowledge. In chess or go, however,
one player almost always wins and the other loses, which means that we can at
least learn from the diﬀerence between the two strategies used by those players.
As an example, we executed 2 million random proof attempts on the AIMLEAP
environment, which led to 300 proofs to learn from, whereas in a two-player
setting like chess, we would get 2 million games in which one player would likely
win.
ACER. The second RL baseline method we tested in our experiments is ACER,
Actor-Critic with Experience Replay [49]. This approach can make use of data
from older episodes to train the current predictor. ACER applies corrections to
the value estimates so that data from old episodes may be used to train the
current policy. It also uses trust region policy optimization [35] to limit the size
of the policy updates. This method is included as a baseline to check if using a
larger replay buﬀer to update the parameters would be advantageous.
PPO. Our third RL baseline is the widely used proximal policy optimization
(PPO) algorithm [36]. It restricts the size of the parameter update to avoid
causing a large diﬀerence between the original predictor’s behavior and the
updated version’s behavior. The method is related to the above trust region
policy optimization method. In this way, PPO addresses the training instability
of many reinforcement learning approaches. It has been used in various settings,
for example complex video games [4]. With its versatility, the PPO algorithm is
well-positioned. We use the PPO algorithm with clipped objective, as in [36].
SIL-PAAC. Our ﬁnal RL baseline uses only the transitions with positive advan-
tage to train on for a portion of the training procedure, to learn more from good
episodes. This was proposed as self-imitation learning (SIL) [29]. To avoid con-
fusion with the method that we are proposing, we extend the acronym to SIL-
PAAC, for positive advantage actor-critic. This algorithm outperformed A2C
on the sparse-reward task Montezuma’s Revenge (a puzzle game). As theorem
proving has a sparse reward structure, we included SIL-PAAC as a baseline.
More information about the implementations for the baselines can be found in
the Implementation Details section at the end of this work.
5.2
Stratiﬁed Shortest Solution Imitation Learning
We introduce stratiﬁed shortest solution imitation learning (3SIL) to tackle the
equational theorem proving domain. It learns to explicitly imitate the actions
taken during the shortest solutions found for each problem in the dataset. We do
this by minimizing the cross-entropy −log p(asolution|st) between the predictor
output and the actions taken in the shortest solution. This is in contrast to the
baseline methods, where value functions are used to judge the utility of decisions.
In our procedure this is not the case. Instead, we build upon the assumption
for data selection that shorter proofs are better in the context of theorem proving

Guiding an Automated Theorem Prover with Neural Rewriting
605
Algorithm 1. CollectEpisode
Input: problem p, policy πθ, problem history H
Generate episode by following noisy version of πθ on p
If solution, add list of tuples (s, a) to H[p]
Keep k shortest solutions in H[p]
Algorithm 2. 3SIL
Input: set of problems P, randomly initialized policy πθ, batch size B, number of
batches NB, problem history H, number of warmup episodes m, number of episodes
f, max epochs ME
Output: trained policy πθ, problem history H
for e = 0 to ME −1 do
if e = 0 then num = m else num = f
for i = 0 to num −1 do
CollectEpisode(sample(P), πθ, H) (Algorithm 1)
end for
for i = 0 to NB −1 do
Sample B tuples (s, a) with uniform probability for each problem from H
Update θ to lower −B
b=0 log πθ(ab|sb) by gradient descent
end for
end for
and expression normalization. In a sense, we value decisions from shorter proofs
more and explicitly imitate those transitions. We keep a history H for each
problem, where we store the current shortest solution (states seen and actions
taken) found for that problem in the training dataset. We can also store multiple
shortest solutions for each problem if there are multiple strategies for a proof
(the number of solutions kept is governed by the parameter k).
During training, in the case k = 1, we sample state-action pairs from each
problem’s current shortest solution at an equal probability (if a solution was
found). To be precise, we ﬁrst randomly pick a theorem for which we have a
solution, and then randomly sample one transition from the shortest encountered
solution. This directly counters one of the phenomena that we had observed: the
training examples for the baseline methods tend to be dominated by very long
episodes (as they contribute more states and actions). This stratiﬁed sampling
method ensures that problems with short proofs get represented equally in the
training process.
The 3SIL algorithm is described in more detail in Algorithm 2. Sampling from
a noisy version of policy πθ means that actions are sampled from the predictor-
deﬁned distribution and in 5% of cases a random valid action is selected. This
is also known as the ϵ-greedy policy (with ϵ at 0.05).
Related Methods. Our approach is similar to the imitation learning algorithm
DAGGER (Dataset Aggregation), which was used for several games [34] and
modiﬁed for branch-and-bound algorithms in [16]. The behavioral cloning (BC)

606
J. Piepenbrock et al.
technique used in robotics [47] also shares some elements. 3SIL signiﬁcantly
diﬀers from DAGGER and BC because it does not use an outside expert to
obtain useful data, because of the stratiﬁed sampling procedure, and because of
the selection of the shortest solutions for each problem in the training dataset.
We include as an additional baseline an implementation of behavioral cloning
(BC), where we regard proofs already encountered as coming from an expert.
We minimize cross-entropy between the actions in proofs we have found and
the predictions to train the predictor. For BC, there is no stratiﬁed sampling
or shortest solution selection, only the minimization of cross-entropy between
actions taken from recent successful solutions and the predictor’s output.
Extensions. For the AIM tasks, we introduce two other techniques, biased
sampling and episode pruning. In biased sampling, problems without a solution
in the history are sampled 5 times more during episode collection than solved
problems to accelerate progress. This was determined by testing 1, 2, 5 and 10
as sampling proportions. For episode pruning, when the agent encountered the
same state twice, we prune the episode to exclude the looping before storing the
episode. This helps the predictor learn to avoid these loops.
6
Neural Architectures
The tree-structured states representing expressions occurring during the tasks
will be processed by a neural network. The neural network takes the tree-
structured state and predicts an action to take that will bring the expression
closer to being normalized or the theorem closer to being proven.
Successor Layer
16D 0-vector
Addition Layer
Processor Network
16D 0-vector
p(action | s)
V(s)
Embedding
Fig. 2. Schematic representation of the creation of a representation of an expression (an
embedding) using diﬀerent neural network layers to represent diﬀerent operations. The
ﬁgure depicts the creation of a numerical representation for the Robinson arithmetic
expression (S(0) + 0). Note that the successor layer and the addition layer consist of
trainable parameters, for which the values are set through gradient descent.

Guiding an Automated Theorem Prover with Neural Rewriting
607
There are two main components to the neural network we use: an embed-
ding tree neural network that outputs a numerical vector representing the tree-
structured proof state and a second processor network that takes this vector
representation of the state and outputs a distribution of the actions possible in
the environment.3
Tree neural networks have been used in various settings, such as natural lan-
guage processing [20] and also in Robinson arithmetic expression embedding [13].
These networks consist of smaller neural networks, each representing one of the
possible functions that occur in the expressions. For example, there will be sep-
arate networks representing addition and multiplication. The cursor is a special
unary operation node with its own network that we insert into the tree at the
current location. For each unique constant, such as the constant 0 in RA or the
identity element 1 for the AIM task, we generate a random vector (from a stan-
dard normal distribution) that will represent this leaf. In the case of the AIM
task, these vectors are parameters that can be optimized during training.
At prediction time, the numerical representation of a tree is constructed by
starting at the leaves of the tree, for which we can look up the generated vectors.
These vectors act as input to the neural networks that represent the parent node’s
operation, yielding a new vector, which now represents the subtree of the parent
node. The process repeats until there is a single vector for the entire tree after
the root node is processed (see also Fig. 2).
The neural networks representing each operation consist of a linear transfor-
mation, a non-linearity in the form of a rectiﬁed linear unit (ReLU) and another
linear transformation. In the case of binary operations, the ﬁrst linear transfor-
mation will have an input dimension of 2n and an output dimension of n, where
n is the dimension of the vectors representing leaves of the tree (the internal rep-
resentation size). The weights representing these transformations are randomly
initialized at the beginning of training.
When we have obtained a single vector embedding representing the entire tree
data structure, this vector serves as the input to the predictor neural network,
which consists of three linear layers, with non-linearities (Sigmoid/ReLU) in
between these layers. The last layer has an output dimension equal to the number
of possible actions in the environment. We obtain a probability distribution over
the actions, e.g. by applying the softmax function to the output of this last layer.
In the cases where we also need a value prediction, there is a parallel last layer
that predicts the state’s value (usually referred to as a two-headed network [41]).
The internal representation size n for the Robinson arithmetic experiments is set
to 16, for the AIM task this is 32. The number of neurons in each layer (except
for the last one) of the predictor networks is 64.
In the AIM dataset task, an arbitrary number of variables can be introduced
during the proof. These are represented by untrainable random vectors. We add a
special neural network (with the same architecture as the networks representing
unary operations, so from size n to n) that processes these vectors before they are
3 In the reinforcement learning baselines that we use, this second processor network
has the additional task of predicting the value of a state.

608
J. Piepenbrock et al.
processed by the rest of the tree neural network embedding. The idea is that this
neural network learns to project these new variable vectors into a subspace and
that an arbitrary number of variables can be handled. The vectors are resampled
at the start of each episode, so the agent cannot learn to recognize speciﬁc
variables. This approach was partly inspired by the prime mechanism in [13], but
we use separate vectors for all variables instead of building vectors sequentially.
All our neural networks are implemented using the PyTorch library [31].
7
Experiments
We ﬁrst describe our experiments on the Robinson arithmetic task, with which
we designed the properties of our 3SIL approach with the help of comparisons
with other algorithms. We then train a predictor using 3SIL on the AIMLEAP
loop theory dataset, which we evaluate both as a standalone prover within the
RL environment and as a neural guidance mechanism for the ATP Prover9.
7.1
Robinson Arithmetic Dataset
Dataset Details. The Robinson arithmetic dataset [11] is split into three dis-
tinct sets, based on the number of steps that it takes a ﬁxed rewriting strategy
to normalize the expression. This ﬁxed strategy, LOPL, which stands for left
outermost proof length, always rewrites the leftmost possible element. If it takes
this strategy less than 90 steps to solve the problem, it is in the low diﬃculty
category. Problems with a diﬃculty between 90 and 130 are in the medium cat-
egory and a greater diﬃculty than 130 leads to the high category. The high
dataset also contains problems the LOPL strategy could not solve within the
time limit. The low dataset is split into a training and testing set. We train on
the low diﬃculty problems, but after training we also test on problems with a
higher diﬃculty. Because we have a diﬃculty measure for this dataset, we use a
curriculum setup. We start by learning to normalize the expressions that a ﬁxed
strategy can normalize in a small amount of steps. This setup is similar to [11].
Training Setup. The 400 problems with the lowest diﬃculty are the starting
point. Every time an agent reaches 95 percent success rate when evaluated on a
sample of size 400 from these problems, we add 400 more diﬃcult problems to
set of training problems P. One iteration of the collection and training phase
is called an epoch. Agents are evaluated after every epoch. The blocks of size
400 are called levels. The number of episodes m and f are set to 1000. For 3SIL
and BC, the batch size BS is 32 and the number of batches NB is 250. The
baselines are conﬁgured so that the number of episodes and training transitions
is at least as many as the 3SIL/BC approaches. Episodes that take over 100
steps are stopped. ADAM [22] is used as an optimizer.

Guiding an Automated Theorem Prover with Neural Rewriting
609
Fig. 3. The level in the curriculum reached by each method. Each method was run three
times. The bold line shows the mean performance and the shaded region shows the
minimum and maximum performance. K is the number of proofs stored per problem.
Results on RA Curriculum. In Fig. 3, we show the progression through the
training curriculum for behavioral cloning (BC), the RL methods (PPO, ACER)
and two conﬁgurations of 3SIL. Behavioral cloning simply imitates actions from
successful episodes. Of the RL baselines, PPO reaches the second level in one run,
while ACER steadily solves the ﬁrst level and in the best run solves around 80%
of the second level. Both methods do not learn enough solutions for the second
level to advance to the third. A2C and SIL-PAAC do not reach the second level,
so these are left out of the plot. However, they do learn to solve about 70–80% of
the ﬁrst 400 problems. From these results we can conclude that the RL baselines
do not perform well on this task in our experiment. We attribute this to the
diﬃculty of learning a good value function due to the sparse rewards (Sect. 5.1).
Our hypothesis is that because this value estimate inﬂuences the policy updates,
the RL methods do not learn well on this task. Note that the two methods with
a trust region update mechanism, ACER and PPO, perform better than the
methods without this mechanism. From these results, it is clear that 3SIL with
1 shortest proof stored, k = 1, is the best-performing conﬁguration. It reaches
the end of the training curriculum of about 5000 problems in 40 epochs. We
experimented with k = 3 and k = 4, but these were both worse than k = 2.
Generalization. While our approach works well on the training set, we must
check if the predictors generalize to unseen examples. Only the methods that
reached the end of the curriculum are tested. In Table 1, we show the results
of evaluating the performance of our predictors on the three diﬀerent test sets:
the unseen examples from the low dataset and the unseen examples from the
medium and high datasets. Because we expect longer solutions, the episode limits
are expanded from 100 steps to 200 and 250 for the medium and high datasets
respectively. For the low and medium datasets, the second of which contains
problems with more diﬃcult solutions than the training data, the predictors

610
J. Piepenbrock et al.
solve almost all test problems. For the high diﬃculty dataset, the performance
drops by at least 20% points. Our method outperforms the Monte Carlo Tree
Search approach used in [11] on the same datasets, which got to 0.954 on the low
dataset with 1600 iterations and 0.786 on the medium dataset (no results on the
high dataset were reported). These results indicate that this training method
might be strong enough to perform well on the AIM rewriting RL task.
Table 1. Generalization with greedy evaluation on the test set for the Robinson arith-
metic normalization tasks, shown as average success rate and standard deviation from
3 training runs. Generalization is high on the low and medium diﬃculty (training data
is similar to the low diﬃculty dataset). With high diﬃculty data, performance drops.
Low
Medium
High
3SIL (k = 1) 1.00 ± 0.01 0.98 ± 0.03 0.77 ± 0.10
3SIL (k = 2) 0.99 ± 0.00 0.96 ± 0.01 0.66 ± 0.08
BC
0.98 ± 0.01 0.98 ± 0.01 0.56 ± 0.05
7.2
AIM Conjecture Dataset
Training Setup. Finally, we train and evaluate 3SIL on the AIM Conjecture
dataset. We apply 3SIL (k = 1) to train predictors in the AIMLEAP environ-
ment. Ten percent of the AIM dataset is used as a hold-out test set, not seen
during training. As there is no estimate for the diﬃculty of the problems in terms
of the actions available to the predictor, we do not use a curriculum ordering
for these experiments. The number m of episodes collected before training is
set to 2,000,000. These random proof attempts result in about 300 proofs. The
predictor learns from these proofs and afterwards the search for new proofs is
also guided by its predictions. For the AIM experiments, episodes are stopped
after 30 steps in the AIMLEAP environment. The predictors are trained for 100
epochs. The number of collected episodes per epoch f is 10,000. The successful
proofs are stored, and the shortest proof for each theorem is kept. NB is 500 and
BS is set to 32. The number of problems with a solution in the history after each
epoch of the training run is shown in Fig. 4.
Results as a Standalone Prover. After 100 epochs, about 2500 of 3114 prob-
lems in the training dataset have a solution in their history. To test the general-
ization capability of the predictors, we inspect their performance on the holdout
test set problems. In Table 2 we compare the success rate of the trained pre-
dictors on the holdout test set with three diﬀerent automated theorem provers:
E [37,38], Waldmeister [19] and Prover9. E is currently one of the best overall
automated theorem provers [44], Waldmeister is a prover specialized in memory-
eﬃcient equational theorem proving [18] and Prover9 is the theorem prover that

Guiding an Automated Theorem Prover with Neural Rewriting
611
Fig. 4. The number of training problems for which a solution was encountered and
stored (cumulative). At the start of the training, the models rapidly collect more solu-
tions, but after 100 epochs, the process slows down and settles at about 2500 problems
with known solutions. The minimum, maximum and mean of three runs are shown.
is used for AIM conjecture research and the prover that the dataset was gener-
ated by. Waldmeister and E are the best performing solvers in competitions for
the relevant unit equality (UEQ) category [44].
Table 2. Theorem proving performance on the hold-out test set in fraction of problems
solved. Means and standard deviations are the results of evaluations of 3 diﬀerent
predictors from 3 diﬀerent training runs on the 354 unseen test set problems.
Method
Success Rate
Prover9 (60 s)
0.833
E (60 s)
0.802
Predictor + AIMLEAP(60 s) 0.702 ± 0.015
Waldmeister (60 s)
0.655
Predictor + AIMLEAP (1×)
0.586 ± 0.029
The results show that a single greedy evaluation of the predictor trying to
solve the problem in the AIMLEAP environment is not as strong as the theo-
rem proving software. However, the theorem provers got 60 s of execution time,
and the execution of the predictor, including interaction with AIMLEAP, takes
on average less than 1 s. We allowed the predictor setup to use 60 s, by run-
ning attempts in AIMLEAP until the time was up, sampling actions from the
predictor’s distribution with 5% noise, instead of using greedy execution. With
this approach, the predictor setup outperforms Waldmeister.4 Figure 5 shows the
overlap between the problems solved by each prover. The diagram shows that
each theorem prover found a few solutions that no other prover could ﬁnd within
4 After the initial experiments, we also evaluated Twee [42], which won the most recent
UEQ track: it can prove most of the test problems in 60 s, only failing for 1 problem.

612
J. Piepenbrock et al.
the time limit. Almost half of all problems from the test set that are solved are
solved by all four systems.
Fig. 5. Venn diagram of the test set problems solved by each solver with 60 s time
limit.
Results of Neural Rewriting Combined with Prover9. We also combine
the predictor with Prover9. In this setup, the predictor modiﬁes the starting
form of the goal, for a maximum of 1 s in the AIMLEAP environment. This
produces new expressions on one or both sides of the equality. We then add, as
lemmas, equalities between the left-hand side of the goal before the predictor’s
rewriting and after each rewriting (see Fig. 1). The same is done for the right-
hand side. For each problem, this procedure yields new lemmas that are added
to the problem speciﬁcation ﬁle that is given to Prover9.
Table 3. Prover9 theorem proving performance on the hold-out test set when injecting
lemmas suggested by the learned predictor. Prover9’s performance increases when
using the suggested lemmas.
Method
Success Rate
Prover9 (1 s)
0.715
Prover9 (2 s)
0.746
Prover9 (60 s)
0.833
Rewriting (1 s) + Prover9 (1 s)
0.841 ± 0.019
Rewriting (1 s) + Prover9 (59 s) 0.902 ± 0.016

Guiding an Automated Theorem Prover with Neural Rewriting
613
In Table 3, it is shown that adding lemmas suggested by the rewriting actions
of the trained predictor improves the performance of Prover9. Running Prover9
for 2 s results in better performance than running it for 1 s, as expected. The
combined (1 s + 1 s) system improved on Prover9’s 2-s performance by 12.7% (=
0.841/0.746), indicating that the predictor suggests useful lemmas. Additionally,
1 s of neural rewriting combined with 59 s of Prover9 search proves almost 8.3%
(= 0.902/0.833) more theorems than Prover9 with a 60 s time limit (Table 2).
7.3
Implementation Details
All experiments for the Robinson task were run on a 16 core Intel(R) Xeon(R)
CPU E5-2670 0 @ 2.60 GHz. The AIM experiments were run on a 72 core Intel(R)
Xeon(R) Gold 6140 CPU @ 2.30 GHz. All calculations were done on CPU. The
PPO implementation was adapted from an existing implementation [3]. The
model was updated every 2000 timesteps, the PPO clip coeﬃcient was set to
0.2. The learning rate was 0.002 and the discount factor γ was set to 0.99.
The ACER implementation was adapted from an available implementation [8].
The replay buﬀer size was 20,000. The truncation parameter was 10 and the
model was updated every 100 steps. The replay ratio was set to 4. Trust region
decay was set to 0.99 and the constraint was set to 1. The discount factor was
set to 0.99 and the learning rate to 0.001. Oﬀ-policy minibatch size was set
to 1. The A2C and SIL implementations were based on Pytorch actor-critic
example code available at the PyTorch repository [33]. For the A2C algorithm,
we experimented with two formulations of the advantage function: the 1-step
lookahead estimate (rt + γVμ(st+1)) −Vμ(st) and the Rt −Vμ(st) formulation.
However, we did not observe diﬀerent performance, so we opted in the end for
the 1-step estimate favored in the original A2C publication. For SIL-PAAC, we
implemented the SIL loss on top of the A2C implementation. There is also a
prioritized replay buﬀer with an exponent of 0.6, as in the original paper. Each
epoch, 8000 (250 batches of size 32) transitions were taken from the prioritized
replay buﬀer in the SIL step of the algorithm. The size of the prioritized replay
buﬀer was 40,000. The critic loss weight was set to 0.01 as in the original paper.
For the 3SIL and behavioral cloning implementations, we sample 8000 transitions
(250 batches of size 32) from the replay buﬀer or history. For the behavioral
cloning, we used a buﬀer of size 40,000. An example implementation of 3SIL
can be found in the RewriteRL repository. On the Robinson arithmetic task, for
3SIL and BC, the evaluation is done greedily (always take the highest probability
actions). For the other methods, we performed experiments with both greedy and
non-greedy (sample from the predictor distribution and add 5% noise) evaluation
and show the results the best-performing setting (which in most cases was the
non-greedy evaluation, except for PPO). On the AIM task, we evaluate greedily
with 3SIL.
AIMLEAP expects a distance estimate for each applicable action. This rep-
resents the estimated distance to a proof. This behavior was converted to a
reinforcement learning setup by always setting the chosen action of the model

614
J. Piepenbrock et al.
to the minimum distance and all other actions to a distance larger than the
maximum proof length. Only the chosen action is then carried out.
Versions of the automated theorem provers used: Version 2.5 of E [39], the
Nov 2017 version of Prover9 [26] and the Feb 2018 version of Waldmeister [46]
and version 2.4.1 of Twee [43].
8
Conclusion and Future Work
Our experiments show that a neural rewriter, trained with the 3SIL method
that we designed, can learn to suggest useful lemmas that assist an ATP and
improve its proving performance. With the same limit of 1 min, Prover9 managed
to prove close to 8.3% more theorems. Furthermore, our 3SIL training method
is powerful enough to train an equational prover from zero knowledge that can
compete with hand-engineered provers, such as Waldmeister. Our system on its
own proves 70.2% of the unseen test problems in 60s, while Waldmeister proved
65.5%.
In future work, we will apply our method to other equational reasoning tasks.
An especially interesting research direction concerns selecting which proofs to
learn from: some sub-proofs might be more general than other sub-proofs. The
incorporation of graph neural networks instead of tree neural networks may
improve the performance of the predictor, since in graph neural networks infor-
mation not only propagates from the leaves to the root, but also through all
other connections.
Acknowledgements. We would like to thank Chad Brown for his work with the
AIMLEAP software. In addition, we thank Thibault Gauthier and Bartosz Piotrowski
for their help with the Robinson arithmetic rewriting task and the AIM rewriting
task respectively. We also thank the referees of the IJCAR conference for their useful
comments.
This work was partially supported by the European Regional Development Fund
under the Czech project AI&Reasoning no. CZ.02.1.01/0.0/0.0/15_003/ 0000466 (JP,
JU), Amazon Research Awards (JP, JU) and by the Czech MEYS under the ERC CZ
project POSTMAN no. LL1902 (JP, MJ).
This article is part of the RICAIP project that has received funding from the Euro-
pean Union’s Horizon 2020 research and innovation programme under grant agreement
No. 857306.
References
1. Alama, J., Heskes, T., Kühlwein, D., Tsivtsivadze, E., Urban, J.: Premise selection
for mathematics by corpus analysis and kernel methods. J. Autom. Reason. 52(2),
191–213 (2013). https://doi.org/10.1007/s10817-013-9286-5
2. Bansal, K., Loos, S., Rabe, M., Szegedy, C., Wilcox, S.: HOList: an environment
for machine learning of higher order logic theorem proving. In: International Con-
ference on Machine Learning, pp. 454–463 (2019)
3. Barhate,
N.:
Implementation
of
PPO
algorithm.
https://github.com/
nikhilbarhate99

Guiding an Automated Theorem Prover with Neural Rewriting
615
4. Berner, C., et al.: DOTA 2 with large scale deep reinforcement learning. arXiv
preprint arXiv:1912.06680 (2019)
5. Blaauwbroek, L., Urban, J., Geuvers, H.: The Tactician. In: Benzmüller, C., Miller,
B. (eds.) CICM 2020. LNCS (LNAI), vol. 12236, pp. 271–277. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-53518-6_17
6. Blanchette, J.C., Kaliszyk, C., Paulson, L.C., Urban, J.: Hammering towards QED.
J. Formalized Reason. 9(1), 101–148 (2016). https://doi.org/10.6092/issn.1972-
5787/4593
7. Brown, C.E., Piotrowski, B., Urban, J.: Learning to advise an equational prover.
Artif. Intell. Theorem Proving, 1–13 (2020)
8. Chételat, D.: Implementation of ACER algorithm. https://github.com/dchetelat/
acer
9. Chvalovský, K., Jakubův, J., Suda, M., Urban, J.: ENIGMA-NG: eﬃcient neural
and gradient-boosted inference guidance for E. In: Fontaine, P. (ed.) CADE 2019.
LNCS (LNAI), vol. 11716, pp. 197–215. Springer, Cham (2019). https://doi.org/
10.1007/978-3-030-29436-6_12
10. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3_24
11. Gauthier,
T.:
Deep
reinforcement
learning
in
HOL4.
arXiv
preprint
arXiv:1910.11797v1 (2019)
12. Gauthier, T.: Deep reinforcement learning for synthesizing functions in higher-
order logic. In: International Conference on Logic for Programming, Artiﬁcial Intel-
ligence and Reasoning (2020)
13. Gauthier, T.: Tree neural networks in HOL4. In: Benzmüller, C., Miller, B. (eds.)
CICM 2020. LNCS (LNAI), vol. 12236, pp. 278–283. Springer, Cham (2020).
https://doi.org/10.1007/978-3-030-53518-6_18
14. Gauthier, T., Kaliszyk, C., Urban, J., Kumar, R., Norrish, M.: TacticToe: learning
to prove with tactics. J. Autom. Reason. 65, 1–30 (2020)
15. Graves, A., Fernández, S., Gomez, F., Schmidhuber, J.: Connectionist temporal
classiﬁcation: labelling unsegmented sequence data with recurrent neural networks.
In: Proceedings of the 23rd International Conference on Machine Learning, pp.
369–376 (2006)
16. He, H., Daume, H., III., Eisner, J.M.: Learning to search in branch and bound
algorithms. Adv. Neural Inf. Process. Syst. 27, 3293–3301 (2014)
17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
18. Hillenbrand, T., Buch, A., Vogt, R., Löchner, B.: WALDMEISTER - high-
performance equational deduction. J. Autom. Reasoning 18, 265–270 (2004)
19. Hillenbrand, T.: Citius altius fortius: lessons learned from the theorem prover Wald-
meister. ENTCS 86(1), 9–21 (2003)
20. Irsoy, O., Cardie, C.: Deep recursive neural networks for compositionality in lan-
guage. Adv. Neural Inf. Process. Syst. 27, 2096–2104 (2014)
21. Kaliszyk, C., Urban, J., Michalewski, H., Olšák, M.: Reinforcement learning of
theorem proving. Adv. Neural Inf. Process. Syst. 31, 8822–8833 (2018)
22. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
23. Kinyon,
M.:
Proof
simpliﬁcation
and
automated
theorem
proving.
CoRR
abs/1808.04251 (2018). http://arxiv.org/abs/1808.04251

616
J. Piepenbrock et al.
24. Kinyon, M., Veroﬀ, R., Vojtěchovský, P.: Loops with abelian inner mapping groups:
an application of automated deduction. In: Bonacina, M.P., Stickel, M.E. (eds.)
Automated Reasoning and Mathematics. LNCS (LNAI), vol. 7788, pp. 151–164.
Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36675-8_8
25. McCune, W.: Prover9 and Mace (2010). http://www.cs.unm.edu/~mccune/
prover9/
26. McCune, W.: Prover9. https://github.com/ai4reason/Prover9
27. Mnih, V., et al.: Asynchronous methods for deep reinforcement learning. In: Inter-
national Conference on Machine Learning, pp. 1928–1937 (2016)
28. Mnih, V., et al.: Human-level control through deep reinforcement learning. Nature
518(7540), 529–533 (2015)
29. Oh, J., Guo, Y., Singh, S., Lee, H.: Self-imitation learning. In: International Con-
ference on Machine Learning, pp. 3878–3887 (2018)
30. Overbeek, R.A.: A new class of automated theorem-proving algorithms. J. ACM
21(2), 191–200 (1974). https://doi.org/10.1145/321812.321814
31. Paszke, A., et al.: PyTorch: An imperative style, high-performance deep learning
library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E.,
Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 32, pp.
8024–8035. Curran Associates, Inc. (2019). http://papers.neurips.cc/paper/9015-
pytorch-an-imperative-style-high-performance-deep-learning-library.pdf
32. Phillips, J., Stanovský, D.: Automated theorem proving in quasigroup and loop
theory. AI Commun. 23(2–3), 267–283 (2010)
33. PyTorch:
RL
Examples.
https://github.com/pytorch/examples/tree/main/
reinforcement_learning
34. Ross, S., Gordon, G., Bagnell, D.: A reduction of imitation learning and structured
prediction to no-regret online learning. In: Proceedings of the 14th International
Conference on Artiﬁcial Intelligence and Statistics, pp. 627–635 (2011)
35. Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.: Trust region policy
optimization. In: Bach, F., Blei, D. (eds.) Proceedings of the 32nd International
Conference on Machine Learning. Proceedings of Machine Learning Research,
vol. 37, pp. 1889–1897. PMLR, Lille (2015). https://proceedings.mlr.press/v37/
schulman15.html
36. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)
37. Schulz, S.: E - a brainiac theorem prover. AI Commun. 15(2–3), 111–126 (2002)
38. Schulz, S., Cruanes, S., Vukmirović, P.: Faster, Higher, Stronger: E 2.3. In:
Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 495–507. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-29436-6_29
39. Schulz, S.: Eprover. https://wwwlehre.dhbw-stuttgart.de/~sschulz/E/E.html
40. Silver, D.: Mastering the game of go with deep neural networks and tree search.
Nature 529(7587), 484–489 (2016)
41. Silver, D., et al.: Mastering the game of go without human knowledge. Nature
550(7676), 354–359 (2017)
42. Smallbone, N.: Twee: an equational theorem prover. In: Platzer, A., Sutcliﬀe, G.
(eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 602–613. Springer, Cham (2021).
https://doi.org/10.1007/978-3-030-79876-5_35
43. Smallbone, N.: Twee 2.4.1. https://github.com/nick8325/twee/releases/download/
2.4.1/twee-2.4.1-linux-amd64
44. Sutcliﬀe, G.: The CADE-27 automated theorem proving system competition -
CASC-27. AI Commun. 32(5–6), 373–389 (2020)

Guiding an Automated Theorem Prover with Neural Rewriting
617
45. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT press,
Cambridge (2018)
46. Hillenbrand, T., Buch, A., Vogt, R., Löchner, B.: Waldmeister (2022). https://
www.mpi-inf.mpg.de/departments/automation-of-logic/software/waldmeister/
download
47. Torabi, F., Warnell, G., Stone, P.: Behavioral cloning from observation. In: Pro-
ceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, IJCAI
2018, pp. 4950–4957. AAAI Press (2018)
48. Veroﬀ, R.: Using hints to increase the eﬀectiveness of an automated reasoning
program: Case studies. J. Autom. Reason. 16(3), 223–239 (1996). https://doi.org/
10.1007/BF00252178
49. Wang, Z., et al.: Sample eﬃcient actor-critic with experience replay. In: Interna-
tional Conference on Learning Representations (2016)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Rensets and Renaming-Based Recursion
for Syntax with Bindings
Andrei Popescu(B)
Department of Computer Science, University of Sheﬃeld, Sheﬃeld, UK
a.popescu@sheffield.ac.uk
Abstract. I introduce renaming-enriched sets (rensets for short), which
are algebraic structures axiomatizing fundamental properties of renam-
ing (also known as variable-for-variable substitution) on syntax with
bindings. Rensets compare favorably in some respects with the well-
known foundation based on nominal sets. In particular, renaming is a
more fundamental operator than the nominal swapping operator and
enjoys a simpler, equationally expressed relationship with the variable-
freshness predicate. Together with some natural axioms matching proper-
ties of the syntactic constructors, rensets yield a truly minimalistic char-
acterization of λ-calculus terms as an abstract datatype – one involving
an inﬁnite set of unconditional equations, referring only to the most fun-
damental term operators: the constructors and renaming. This character-
ization yields a recursion principle, which (similarly to the case of nomi-
nal sets) can be improved by incorporating Barendregt’s variable conven-
tion. When interpreting syntax in semantic domains, my renaming-based
recursor is easier to deploy than the nominal recursor. My results have
been validated with the proof assistant Isabelle/HOL.
1
Introduction
Formal reasoning about syntax with bindings is necessary for the meta-theory
of logics, calculi and programming languages, and is notoriously error-prone.
A great deal of research has been put into formal frameworks that make the
speciﬁcation of, and the reasoning about bindings more manageable.
Researchers wishing to formalize work involving syntax with bindings must
choose a paradigm for representing and manipulating syntax—typically a vari-
ant of one of the “big three”: nameful (sometimes called “nominal” reﬂect-
ing its best known incarnation, nominal logic [23,39]), nameless (De Bruijn)
[4,13,49,51] and higher-order abstract syntax (HOAS) [19,20,28,34,35]. Each
paradigm has distinct advantages and drawbacks compared with each of the
others, some discussed at length, e.g., in [1,9] and [25, §8.5]. And there are also
hybrid approaches, which combine some of the advantages [14,18,42,47].
A signiﬁcant advantage of the nameful paradigm is that it stays close to
the way one informally deﬁnes and manipulates syntax when describing systems
in textbooks and research papers—where the binding variables are explicitly
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 618–639, 2022.
https://doi.org/10.1007/978-3-031-10769-6_36

Rensets and Renaming-Based Recursion for Syntax with Bindings
619
indicated. This can in principle ensure transparency of the formalization and
allows the formalizer to focus on the high-level ideas. However, it only works
if the technical challenge faced by the nameful paradigm is properly addressed:
enabling the seamless deﬁnition and manipulation of concepts “up to alpha-
equivalence”, i.e., in such a way that the names of the bound variables are
(present but nevertheless) inconsequential. This is particularly stringent in the
case of recursion due to the binding constructors of terms not being free, hence
not being a priori traversable recursively—in that simply writing some recursive
clauses that traverse the constructors is not a priori guaranteed to produce a
correct deﬁnition, but needs certain favorable conditions. The problem has been
addressed by researchers in the form of tailored nameful recursors [23,33,39,43,
56,57], which are theorems that identify such favorable conditions and, based
on them, guarantee the existence of functions that recurse over the non-free
constructors.
In this paper, I make a contribution to the nameful paradigm in general,
and to nameful recursion in particular. I introduce rensets, which are algebraic
structures axiomatizing the properties of renaming, also known as variable-for-
variable substitution, on terms with bindings (Sect. 3). Rensets diﬀer from nom-
inal sets (Sect. 2.2), which form the foundation of nominal logic, by their focus
on (not necessarily injective) renaming rather than swapping (or permutation).
Similarly to nominal sets, rensets are pervasive: Not only do the variables and
terms form rensets, but so do any container-type combinations of rensets.
While lacking the pleasant symmetry of swapping, my axiomatization of
renaming has its advantages. First, renaming is more fundamental than swap-
ping because, at an abstract axiomatic level, renaming can deﬁne swapping but
not vice versa (Sect. 4). The second advantage is about the ability to deﬁne
another central operator: the variable freshness predicate. While the deﬁnability
of freshness from swapping is a signature trait of nominal logic, my renaming-
based alternative fares even better: In rensets freshness has a simple, ﬁrst-
order deﬁnition (Sect. 3). This contrasts the nominal logic deﬁnition, which
involves a second-order statement about (co)ﬁniteness of a set of variables. The
third advantage is largely a consequence of the second: Rensets enriched with
constructor-like operators facilitate an equational characterization of terms with
bindings (using an inﬁnite set of unconditional equations), which does not seem
possible for swapping (Sect. 5.1). This produces a recursion principle (Sect. 5.2)
which, like the nominal recursor, caters for Barendregt’s variable convention,
and in some cases is easier to apply than the nominal recursor—for example
when interpreting syntax in semantic domains (Sect. 5.3).
In summary, I argue that my renaming-based axiomatization oﬀers some
beneﬁts that strengthen the arsenal of the nameful paradigm: a simpler repre-
sentation of freshness, a minimalistic equational characterization of terms, and
a convenient recursion principle. My results are established with high conﬁdence
thanks to having been mechanized in Isabelle/HOL [32]. The mechanization is
available [44] from Isabelle’s Archive of Formal Proofs.
Here is the structure of the rest of this paper: Sect. 2 provides background
on terms with bindings and on nominal logic. Section 3 introduces rensets and

620
A. Popescu
describes their basic properties. Section 4 establishes a formal connection to
nominal sets. Section 5 discusses substitutive-set-based recursion. Section 6 dis-
cusses related work. A technical report [45] associated to this paper includes an
appendix with more examples and results and more background on nominal sets.
2
Background
This section recalls the terms of λ-calculus and their basic operators (Sect. 2.1),
and aspects of nominal logic including nominal sets and nominal recursion
(Sect. 2.2).
2.1
Terms with Bindings
I work with the paradigmatic syntax of (untyped) λ-calculus. However, my
results generalize routinely to syntaxes speciﬁed by arbitrary binding signatures
such as the ones in [22, §2], [39,59] or [12].
Let Var be a countably inﬁnite set of variables, ranged over by x, y, z etc. The
set Trm of λ-terms (or terms for short), ranged over by t, t1, t2 etc., is deﬁned
by the grammar t ::= Vr x | Ap t1 t2 | Lm x t
with the proviso that terms are equated (identiﬁed) modulo alpha-equivalence
(also known as naming equivalence). Thus, for example, if x ̸= z ̸= y then
Lm x (Ap (Vr x) (Vr z)) and Lm y (Ap (Vr y) (Vr z)) are considered to be the
same term. I will often omit Vr when writing terms, as in, e.g., Lm x x.
What the above speciﬁcation means is (something equivalent to) the follow-
ing: One ﬁrst deﬁnes the set PTrm of pre-terms as freely generated by the gram-
mar p ::= PVr x | PAp p1 p2 | PLm x p. Then one deﬁnes the alpha-equival-
ence relation ≡: PTrm →PTrm →Bool inductively, proves that it is an equiv-
alence, and deﬁnes Trm by quotienting PTrm to alpha-equivalence, i.e., Trm =
PTrm/ ≡. Finally, one proves that the pre-term constructors are compatible
with ≡, and deﬁnes the term counterpart of these constructors: Vr : Var →Trm,
Ap : Trm →Trm →Trm and Lm : Var →Trm →Trm.
The above constructions are technical, but well-understood, and can be fully
automated for an arbitrary syntax with bindings (not just that of λ-calculus);
and tools such as the Isabelle/Nominal package [59,60] provide this automation,
hiding pre-terms completely from the end user. In formal and informal presenta-
tions alike, one usually prefers to forget about pre-terms, and work with terms
only. This has several advantages, including (1) being able to formalize concepts
at the right abstraction level (since in most applications the naming of bound
variables should be inconsequential) and (2) the renaming operator being well-
behaved. However, there are some diﬃculties that need to be overcome when
working with terms, and in this paper I focus on one of the major ones: provid-
ing recursion principles, i.e., mechanisms for deﬁning functions by recursing over
terms. This diﬃculty arises essentially because, unlike in the case of pre-term
constructors, the binding constructor for terms is not free.
The main characters of my paper will be (generalizations of) some common
operations and relations on Trm, namely:

Rensets and Renaming-Based Recursion for Syntax with Bindings
621
– the constructors Vr : Var →Trm, Ap : Trm →Trm →Trm and Lm : Var →
Trm →Trm
– (capture-avoiding) renaming, also known as (capture-avoiding) substitution
of variables for variables
[ / ] : Trm →Var →Var →Trm; e.g., we have
(Lm x (Ap x y)) [x/y] = Lm x′ (Ap x′ x)
– swapping [ ∧] : Trm →Var →Var →Trm; e.g., we have (Lm x (Ap x y)) [x∧
y] = Lm y (Ap y x)
– the free-variable operator FV : Trm →Pow(Var) (where Pow(Var) is the
powerset of Var); e.g., we have FV(Lm x (Ap y x)) = {y}
– freshness # : Var →Trm →Bool; e.g., we have x # (Lm x x); and assuming
x ̸= y, we have ¬ x # (Lm y x)
The free-variable and freshness operators are of course related: A variable x
is fresh for a term t (i.e., x # t) if and only if it is not free in t (i.e., x /∈FV(t)).
The renaming operator [ / ] : Trm →Var →Var →Trm substitutes (in terms)
variables for variables, not terms for variables. (But an algebraization of term-
for-variable substitution is discussed in [45, Appendix D].)
2.2
Background on Nominal Logic
I will employ a formulation of nominal logic [38,39,57] that does not require any
special logical foundation, e.g., axiomatic nominal set theory. For simplicity, I
prefer the swapping-based formulation [38] to the equivalent permutation-based
formulation—[45, Appendix C] gives details on these two alternatives.
A pre-nominal set is a pair A = (A, [ ∧]) where A is a set and [ ∧] :
A →Perm →A is a function called the swapping operator of A satisfying the
following properties for all a ∈A and x, x1, x2, y1, y2 ∈Var:
Identity:
a[x∧x] = a
Involution:
a[x1 ∧x2][x1 ∧x2] = a
Compositionality:
a[x1 ∧x2][y1 ∧y2] = a[y1 ∧y2][(x1[y1 ∧y2])∧(x2[y1 ∧y2])]
Given a pre-nominal set A = (A, [ ∧]), an element a ∈A and a set X ⊆Var,
one says that a is supported by X if a[x∧y] = a holds for all x, y ∈Var such that
x, y /∈X. An element a ∈A is called ﬁnitely supported if there exists a ﬁnite
set X ⊆A such that a is supported by X. A nominal set is a pre-nominal set
A = (A, [ ∧]) such that every element of a is ﬁnitely supported. If A = (A, [ ∧])
is a nominal set and a ∈A, then the smallest set X ⊆A such that a is supported
by X exists, and is denoted by suppA a and called the support of a. One calls a
variable x fresh for a, written x # a, if x /∈suppA a.
An alternative, more direct deﬁnition of freshness (which is preferred, e.g.,
by Isabelle/Nominal [59,60]) is provided by the following proposition:
Proposition 1. For any nominal set A = (A, [ ∧]) and any x ∈Var and a ∈A,
it holds that x # a if and only if the set {y | a[y ∧x] ̸= a} is ﬁnite.

622
A. Popescu
Given two pre-nominal sets A = (A, [ ∧]) and B = (B, [ ∧]), the set
F = (A →B) of functions from A to B becomes a pre-nominal set F = (F, [ ∧])
by deﬁning f[x∧y] to send each a ∈A to (f(a[x∧y]))[x∧y]. F is not a nominal
set because not all functions are ﬁnitely supported (though of course one obtains
a nominal set by restricting to ﬁnitely supported functions).
The set of terms together with their swapping operator, (Trm, [ ∧]), forms
a nominal set, where the support of a term is precisely its set of free variables.
However, the power of nominal logic resides in the fact that not only the set of
terms, but also many other sets can be organized as nominal sets—including the
target domains of many functions one may wish to deﬁne on terms. This gives
rise to a convenient mechanism for deﬁning functions recursively on terms:
Theorem 2 [39].
Let A = (A, [ ]) be a nominal set and let VrA : Var →
A, ApA : A →A →A and LmA : Var →A →A be some functions, all
supported by a ﬁnite set X of variables and with LmA satisfying the following
freshness condition for binders (FCB): There exists x ∈Var such that x /∈X
and x # LmA x a for all a ∈A.
Then there exists a unique function f : Trm →A that is supported by X
and such that the following hold for all x ∈Var and t1, t2, t ∈Trm:
(i) f (Vr x) = VrA x
(ii) f (Ap t1 t2) = ApA (f t1) (f t2)
(iii) f (Lm x t) = LmA x (f t) if x /∈X
A useful feature of nominal recursion is the support for Barendregt’s famous
variable convention [8, p. 26]: “If [the terms] t1, . . . , tn occur in a certain math-
ematical context (e.g. deﬁnition, proof), then in these terms all bound variables
are chosen to be diﬀerent from the free variables.” The above recursion princi-
ple adheres to this convention by ﬁxing a ﬁnite set X of variables meant to be
free in the deﬁnition context and guaranteeing that the bound variables in the
deﬁnitional clauses are distinct from them. Formally, the target domain opera-
tors VrA, ApA and LmA are supported by X, and the clause for λ-abstraction
is conditioned by the binding variable x being outside of X. (The Barendregt
convention is also present in nominal logic via induction principles [39,58–60].)
3
Rensets
This section introduces rensets, an alternative to nominal sets that axiomatize
renaming rather than swapping or permutation.
A renaming-enriched set (renset for short) is a pair A = (A, [ / ]) where A
is a set and [ / ] : A →Var →Var →A is an operator such that the following
hold for all x, x1, x2, x3, y, y1, y2 ∈Var and a ∈A:
Identity:
a[x/x] = a
Idempotence:
If x1 ̸= y then a[x1/y][x2/y] = a[x1/y]
Chaining:
If y ̸= x2 then a[y/x2][x2/x1][x3/x2] = a[y/x2][x3/x1]
Commutativity:
If x2 ̸= y1 ̸= x1 ̸= y2 then a[x2/x1][y2/y1] = a[y2/y1][x2/x1]

Rensets and Renaming-Based Recursion for Syntax with Bindings
623
Let us call A the carrier of A and [ / ] the renaming operator of A. Similarly
to the case of terms, we think of the elements a ∈A as some kind of variable-
bearing entities and of a[y/x] as the result of substituting x with y in a. With
this intuition, the above properties are natural: Identity says that substituting a
variable with itself has no eﬀect. Idempotence acknowledges the fact that, after
its renaming, a variable y is no longer there, so substituting it again has no
eﬀect. Chaining says that a chain of renamings x3/x2/x1 has the same eﬀect
as the end-to-end renaming x3/x1 provided there is no interference from x2,
which is ensured by initially substituting x2 with some other variable y. Finally,
Commutativity allows the reordering of any two independent renamings.
Examples. (Var, [ / ]) and (Trm, [ / ]), the sets of variables and terms with
the standard renaming operator on them, form rensets. Moreover, given any
functor F on the category of sets and a renset A = (A, [ / ]), let us deﬁne the
renset F A = (F A, [ / ]) as follows: for any k ∈F A and x, y ∈Var, k[x/y] =
F ( [x/y]) k, where the last occurrence of F refers to the action of the functor
on morphisms. This means that one can freely build new rensets from existing
ones using container types (which are particular kinds of functors)—e.g., lists,
sets, trees etc. Another way to put it: Rensets are closed under datatype and
codatatype constructions [55].
In what follows, let us ﬁx a renset A = (A, [ / ]). One can deﬁne the notion
of freshness of a variable for an element of a in the style of nominal logic. But
the next proposition shows that simpler formulations are available.
Proposition 3. The following are equivalent:
(1) The set {y ∈Var | a[y/x] ̸= a} is ﬁnite.
(2) a[y/x] = a for all y ∈Var.
(3) a[y/x] = a for some y ∈Var ∖{x}.
Let us deﬁne the predicate
# : Var →A →Bool as follows: x # a, read x
is fresh for a, if either of Proposition 3’s equivalent properties holds.
Thus, points (1)–(3) above are three alternative formulations of x # a, all
referring to the lack of eﬀect of substituting y for x, expressed as a[y/x] = a:
namely that this phenomenon aﬀects (1) all but a ﬁnite number of variables y,
(2) all variables y, or (3) some variable y ̸= x. The ﬁrst formulation is the most
complex of the three—it is the nominal deﬁnition, but using renaming instead
of swapping. The other two formulations do not have counterparts in nominal
logic, essentially because swapping is not as “eﬃcient” as renaming at exposing
freshness. In particular, (3) does not have a nominal counterpart because there is
no single-swapping litmus test for freshness. The closest we can get to property
(3) in a nominal set is the following: x is fresh for a if and only a[y∧x] = a holds
for some fresh y—but this needs freshness to explain freshness!
Examples (continued). For the rensets of variables and terms, freshness
deﬁned as above coincides with the expected operators: distinctness in the case
of variables and standard freshness in the case of terms. And applying the deﬁni-
tion of freshness to rensets obtained using ﬁnitary container types has similarly
intuitive outcomes; for example, the freshness of a variable x for a list of items
[a1, . . . , an] means that x is fresh for each item ai in the list.

624
A. Popescu
Freshness satisﬁes some intuitive properties, which can be easily proved from
its deﬁnition and the renset axioms. In particular, point (2) of the next propo-
sition is the freshness-based version of the Chaining axiom.
Proposition 4. The following hold:
(1) If x # a then a[y/x] = a
(2) x2 # a then a[x2/x1][x3/x2] = a[x3/x1]
(3) If z # a or z = x, and x # a or z ̸= y, then z # a[y/x]
4
Connection to Nominal Sets
So far I focused on consequences of the purely equational theory of rensets, with-
out making any assumption about cardinality. But after additionally postulating
a nominal-style ﬁnite support property, one can show that rensets give rise to
nominal sets—which is what I will do in this section.
Let us say that a renset A = (A, [ / ]) has the Finite Support property if,
for all a ∈A, the set {x ∈Var | ¬ x # a} is ﬁnite.
Let A = (A, [ / ]) be a renset satisfying Finite Support. Let us deﬁne the
swapping operator
[ ∧] : A →Var →Var →A as follows: a[x1 ∧x2] =
a[y/x1][x1/x2][x2/y], where y is a variable that is fresh for all the involved items,
namely y /∈{x1, x2} and y # a. Indeed, this is how one would deﬁne swapping
from renaming on terms: using a fresh auxiliary variable y, and exploiting that
such a fresh y exists and that its choice is immaterial for the end result. The
next lemma shows that this style of deﬁnition also works abstractly, i.e., all it
needs are the renset axioms plus Finite Support.
Lemma 5. The following hold for all x1, x2 ∈Var and a ∈A:
(1) There exists y ∈Var such that y /∈{x1, x2} and y # a.
(2) For all y, y′ ∈Var such that y /∈{x1, x2}, y # a, y′ /∈{x1, x2} and y′# a,
a[y/x1][x1/x2][x2/y] = a[y′/x1][x1/x2][x2/y′].
And one indeed obtains an operator satisfying the nominal axioms:
Proposition 6. If (A, [ / ]) is a renset satisfying Finite Support, then
(A, [ ∧]) is a nominal set. Moreover, (A, [ / ]) and (A, [ ∧]) have the same
notion of freshness, in that the freshness operator deﬁned from renaming coin-
cides with that deﬁned from swapping.
The above construction is functorial, as I detail next. Given two nominal
sets A = (A, [ ∧]) and B = (B, [ ∧]), a nominal morphism f : A →B
is a function f : A →B with the property that it commutes with swapping,
in that (f a)[x ∧y] = f(a[x ∧y]) for all a ∈A and x, y ∈Var. Nominal sets
and nominal morphisms form a category that I will denote by Nom. Similarly,
let us deﬁne a morphism f : A →B between two rensets A = (A, [ / ]) and
B = (B, [ ]) to be a function f : A →B that commutes with renaming, yielding
the category Sbs of rensets. Let us write FSbs for the full subcategory of Sbs
given by rensets that satisfy Finite Support. Let us deﬁne F : FSbs →Nom to be

Rensets and Renaming-Based Recursion for Syntax with Bindings
625
an operator on objects and morphisms that sends each ﬁnite-support renset to
the above described nominal set constructed from it, and sends each substitutive
morphism to itself.
Theorem 7. F is a functor between FSbs and Nom which is injective on objects
and full and faithful (i.e., bijective on morphisms).
One may ask whether it is also possible to make the trip back: from nominal
to rensets. The answer is negative, at least if one wants to retain the same
notion of freshness, i.e., have the freshness predicate deﬁned in the nominal set
be identical to the one deﬁned in the resulting renset. This is because swapping
preserves the cardinality of the support, whereas renaming must be allowed to
change it since it might perform a non-injective renaming. The following example
captures this idea:
Counterexample. Let A = (A, [ ∧]) be a nominal set such that all elements of
A have their support consisting of exactly two variables, x and y (with x ̸= y).
(For example, A can be the set of all terms with these free variables—this is
indeed a nominal subset of the term nominal set because it is closed under
swapping.) Assume for a contradiction that [ / ] is an operation on A that makes
(A, [ / ]) a renset with its induced freshness operator equal to that of A. Then,
by the deﬁnition of A, a[y/x] needs to have exactly two non-fresh variables. But
this is impossible, since by Proposition 4(3), all the variables diﬀerent from y
(including x) must be fresh for a[y/x]. In particular, A is not in the image of
the functor F : FSbs →Nom, which is therefore not surjective on objects.
Thus, at an abstract algebraic level renaming can deﬁne swapping, but not
the other way around. This is not too surprising, since swapping is fundamen-
tally bijective whereas renaming is not; but it further validates our axioms for
renaming, highlighting their ability to deﬁne a well-behaved swapping.
5
Recursion Based on Rensets
Proposition 3 shows that, in rensets, renaming can deﬁne freshness using only
equality and universal or existential quantiﬁcation over variables—without need-
ing any cardinality condition like in the case of swapping. As I am about to dis-
cuss, this forms the basis of a characterization of terms as the initial algebra of
an equational theory (Sect. 5.1) and an expressive recursion principle (Sect. 5.2)
that fares better than the nominal one for interpretations in semantic domains
(Sect. 5.3).
5.1
Equational Characterization of the Term Datatype
Rensets contain elements that are “term-like” in as much as there is a renam-
ing operator on them satisfying familiar properties of renaming on terms. This
similarity with terms can be strengthened by enriching rensets with operators
having arities that match those of the term constructors.
A constructor-enriched renset (CE renset for short) is a tuple A
=
(A, [ / ], VrA, ApA, LmA) where:

626
A. Popescu
– (A, [ / ]) is a renset
– VrA : Var →A, ApA : A →A →A and LmA : Var →A →A are functions
such that the following hold for all a, a1, a2 ∈A and x, y, z ∈Var:
(S1) (VrA x)[y/z] = VrA(x[y/z])
(S2) (ApA a1 a2)[y/z] = ApA(a1[y/z]) (a2[y/z])
(S3) if x /∈{y, z} then (LmA x a)[y/z] = LmA x (a[y/z])
(S4) (LmA x a)[y/x] = LmA x a
(S5) if z ̸= y then LmA x (a[z/y]) = LmA y (a[z/y][y/x])
Let us call VrA, ApA, LmA the constructors of A. (S1)–(S3) express the construc-
tors’ commutation with renaming (with capture-avoidance provisions in the case
of (S3)), (S4) the lack of eﬀect of substituting for a bound variable, and (S5)
the possibility to rename a bound variable without changing the abstracted item
(where the inner renaming of z ̸= y for y ensures the freshness of the “new name”
y, hence its lack of interference with the other names in the “term-like” entity
where the renaming takes place). All these are well-known to hold for terms:
Example. Terms with renaming and the constructors, namely (Trm, [ / ], Vr,
Ap, Lm), form a CE renset which will be denoted by Trm.
As it turns out, the CE renset axioms capture exactly the term structure
Trm, via initiality. The notion of CE substitutive morphism f : A →B between
two CE rensets A = (A, [ / ], VrA, ApA, LmA) and B = (B, [ / ], VrB, ApB, LmB)
is the expected one: a function f : A →B that is a substitutive morphism and
also commutes with the constructors. Let us write SbsCE for the category of CE
rensets and morphisms.
Theorem 8. Trm is the initial CE renset, i.e., initial object in SbsCE.
Proof Idea. Let A = (A, [ / ], VrA, ApA, LmA) be a CE renset. Instead of directly
going after a function f : Trm →A, one ﬁrst inductively deﬁnes a relation
R : Trm →A →Bool, with inductive clauses reﬂecting the desired properties
concerning the commutation with the constructors, e.g.,
R t a
R (Lm x t) (LmA x a). It
suﬃces to prove that R is total and functional and preserves renaming, since
that allows one to deﬁne a constructor- and renaming-preserving function (a
morphism) f by taking f t to be the unique a with R t a.
Proving that R is total is easy by standard induction on terms. Proving the
other two properties, namely functionality and preservation of renaming, is more
elaborate and requires their simultaneous proof together with a third property:
that R preserves freshness. The simultaneous three-property proof follows by a
form of “substitutive induction” on terms: Given a predicate φ : Trm →Bool,
to show ∀t ∈Trm. φ t it suﬃces to show the following: (1) ∀x ∈Var. φ (Vr x),
(2) ∀t1, t2 ∈Trm. φ t1 & φ t2 →φ (Ap t1 t2), and (3) ∀x ∈Var, t ∈Trm. (∀s ∈
Trm. Con [ / ] t s →φ s) →φ (Lm x t), where Con [ / ] t s means that t is
connected to s by a chain of renamings.
Roughly speaking, R turns out to be functional because the λ-abstraction
operator on the “term-like” inhabitants of A is, thanks to the axioms of CE

Rensets and Renaming-Based Recursion for Syntax with Bindings
627
renset, at least as non-injective as (i.e., identiﬁes at least as many items as) the
λ-abstraction operator on terms.
⊓⊔
Theorem 8 is the central result of this paper, from both practical and theo-
retical perspectives. Practically, it enables a useful form of recursion on terms (as
I will discuss in the following sections). Theoretically, this is a characterization
of terms as the initial algebra of an equational theory that only the most funda-
mental term operations, namely the constructors and renaming. The equational
theory consists of the axioms of CE rensets (i.e., those of rensets plus (S1)–(S5)),
which are an inﬁnite set of unconditional equations—for example, axiom (S5)
gives one equation for each pair of distinct variables y, z.
It is instructive to compare this characterization with the one oﬀered by
nominal logic, namely by Theorem 2. To do this, one ﬁrst needs a lemma:
Lemma 9. Let f : A →B be a function between two nominal sets A = (A, [ ∧
]) and B = (B, [ ∧]) and X a set of variables. Then f is supported by X if
and only if f(a[x∧y]) = (f a)[x∧y] for all x, y ∈Var ∖X.
Now Theorem 2 (with the variable avoidance set X taken to be ∅) can be
rephrased as an initiality statement, as I describe below.
Let us deﬁne a constructor-enriched nominal set (CE nominal set) to be
any tuple A = (A, [ ∧], VrA, ApA, LmA) where (A, [ ∧]) is a nominal set and
VrA : Var →A, ApA : A →A →A, LmA : Var →A →A are operators on A
such that the following properties hold for all a, a1, a2 ∈A and x, y, z ∈Var:
(N1) (VrA x)[y ∧z] = VrA(x[y ∧z])
(N2) (ApA a1 a2)[y ∧z] = ApA(a1[y ∧z]) (a2[y ∧z])
(N3) (LmA x a)[y ∧z] = LmA (x[y ∧z]) (a[y ∧z])
(N4) x # Lm x a, i.e., {y ∈Var | (Lm x a)[y ∧x] ̸= Lm x a} is ﬁnite.
The notion of CE nominal morphism is deﬁned as the expected extension
of that of nominal morphism: a function that commutes with swapping and the
constructors. Let NomCE be the category of CE nominal sets morphisms.
Theorem 10 ([39], rephrased). (Trm, [ ∧], Vr, Ap, Lm) is the initial CE
nominal set, i.e., the initial object in NomCE.
The above theorem indeed corresponds exactly to Theorem 2 with X = ∅:
– the conditions (N1)–(N3) in the deﬁnition of CE nominal sets correspond (via
Lemma 9) to the constructors being supported by ∅
– (N4) is the freshness condition for binders
– initiality, i.e., the existence of a unique morphism, is the same as the existence
of the unique function f : Trm →A stipulated in Theorem 2: commutation
with the constructors is the Theorem 2 conditions (i)–(iii), and commutation
with swapping means (via Lemma 9) f being supported by ∅.
Unlike the renaming-based characterization of terms (Theorem 8), the nom-
inal logic characterization (Theorem 10) is not purely equational. This is due
to a combination of two factors: (1) two of the axioms ((N4) and the Finite

628
A. Popescu
Support condition) referring to freshness and (2) the impossibility of expressing
freshness equationally from swapping. The problem seems fundamental, in that a
nominal-style characterization does not seem to be expressible purely equation-
ally. By contrast, while the freshness idea is implicit in the CE renset axioms,
the freshness predicate itself is absent from Theorem 8.
5.2
Barendregt-Enhanced Recursion Principle
While Theorem 8 already gives a recursion principle, it is possible to improve it
by incorporating Barendregt’s variable convention (in the style of Theorem 2):
Theorem 11. Let X be a ﬁnite set, (A, [ / ]) a renset and VrA : Var →A,
ApA : A →A →A and LmA : Var →A →A some functions that satisfy the
clauses (S1)–(S5) from the deﬁnition of CE renset, but only under the assumption
that x, y, z /∈X. Then there exists a unique function f : Trm →A such that th
following hold:
(i) f (Vr x) = VrA x
(ii) f (Ap t1 t2) = ApA (f t1) (f t2)
(iii) f (Lm x t) = LmA x (f t) if x /∈X (iv) f (t[y/z]) = (f t)[y/z] if y, z /∈X
Proof Idea. The constructions in the proof of Theorem 8 can be adapted
to avoid clashing with the ﬁnite set of variables X. For example, the
clause for λ-abstraction in the inductive deﬁnition of the relation R becomes
x̸∈X
R t a
R (Lm x t) (LmA x a) and preservation of renaming and freshness are also formu-
lated to avoid X. Totality is still ensured thanks to the possibility of renaming
bound variables—in terms and inhabitants of A alike (via the modiﬁed axiom
(S5)).
⊓⊔
The above theorem says that if the structure A is assumed to be “almost” a
CE set, save for additional restrictions involving the avoidance of X, then there
exists a unique “almost”-morphism—satisfying the CE substitutive morphism
conditions restricted so that the bound and renaming-participating variables
avoid X. It is the renaming-based counterpart of the nominal Theorem 2.
In regards to the relative expressiveness of these two recursion principles
(Theorems 11 and 2), it seems diﬃcult to ﬁnd an example that is deﬁnable
by one but not by the other. In particular, my principle can seamlessly deﬁne
standard nominal examples [39,40] such as the length of a term, the count-
ing of λ-abstractions or of the free-variables occurrences, and term-for-variable
substitution—[45, Appendix A] gives details. However, as I am about to discuss,
I found an important class of examples where my renaming-based principle is
signiﬁcantly easier to deploy: that of interpreting syntax in semantic domains.
5.3
Extended Example: Semantic Interpretation
Semantic interpretations, also known as denotations (or denotational seman-
tics), are pervasive in the meta-theory of logics and λ-calculi, for example when
interpretating ﬁrst-order logic (FOL) formulas in FOL models, or untyped or

Rensets and Renaming-Based Recursion for Syntax with Bindings
629
simply-typed λ-calculus or higher-order logic terms in speciﬁc models (such as
full-frame or Henkin models). In what follows, I will focus on λ-terms and Henkin
models, but the ideas discussed apply broadly to any kind of statically scoped
interpretation of terms or formulas involving binders.
Let D be a set and ap : D →D →D and lm : (D →D) →D be operators
modeling semantic notions of application and abstraction. An environment will
be a function ξ : Var →D. Given x, y ∈Var and d, e ∈D, let us write ξ⟨x := d⟩
for ξ updated with value d for x (i.e., acting like ξ on all variables except for x
where it returns d); and let us write ξ⟨x := d, y := e⟩instead of ξ⟨x := d⟩⟨y := e⟩.
Say one wants to interpret terms in the semantic domain D in the context of
environments, i.e., deﬁne the function sem : Trm →(Var →D) →D that maps
syntactic to semantic constructs; e.g., one would like to have:
– sem (Lm x (Ap x x)) ξ = lm(d →ap d d) (regardless of ξ)
– sem (Lm x (Ap x y)) ξ = lm(d →ap d (ξ y)) (assuming x ̸= y)
where I use d →. . . to describe functions in D →D, e.g., d →ap d d is the
function sending every d ∈D to ap d d.
The deﬁnition should therefore naturally go recursively by the clauses:
(1) sem (Vr x) ξ = ξ x
(2) sem (Ap t1 t2) ξ = ap (sem t1 ξ) (sem t2 ξ)
(3) sem (Lm x t) ξ = lm (d →sem t (ξ⟨x := d⟩))
Of course, since Trm is not a free datatype, these clauses do not work out of
the box, i.e., do not form a deﬁnition (yet)—this is where binding-aware recursion
principles such as Theorems 11 and 2 could step in. I will next try them both.
The three clauses above already determine constructor operations VrI, ApI
and LmI on the set of interpretations, I = (Var →D) →D, namely:
– VrI : Var →I by VrI x i ξ = ξ x
– ApI : I →I →I by ApI i1 i2 ξ = ap (i1 ξ) (i2 ξ)
– LmI : Var →I →I by LmI x i ξ = lm (d →i (ξ⟨x := d⟩))
To apply the renaming-based recursion principle from Theorem 11, one must
further deﬁne a renaming operator on I. Since the only chance to successfully
apply this principle is if sem commutes with renaming, the deﬁnition should be
inspired by the question: How can sem(t[y/x]) be determined from sem t, y and
x? The answer is (4) sem (t[y/x]) ξ = (sem t) (ξ⟨x := ξ y⟩), yielding an operator
[ / ]I : I →Var →Var →I deﬁned by i [y/x]I ξ = i (ξ⟨x := ξ y⟩).
It is not diﬃcult to verify that I = (I, [ / ]I, VrI, ApI, LmI) is a CE renset—
for example, Isabelle’s automatic methods discharge all the goals. This means
Theorem 11 (or, since here one doesn’t need Barendregt’s variable convention,
already Theorem 8) is applicable, and gives us a unique function sem that com-
mutes with the constructors, i.e., satisﬁes clauses (1)–(3) (which are instances of
the clauses (i)–(iii) from Theorem 11), and additionally commutes with renam-
ing, i.e., satisﬁes clause (4) (which is an instances of the clause (iv) from Theo-
rem 11).
On the other hand, to apply nominal recursion for deﬁning sem, one must
identify a swapping operator on I. Similarly to the case of renaming, this identiﬁ-
cation process is guided by the goal of determining sem(t[x∧y]) from sem t, x and

630
A. Popescu
y, leading to (4’) sem (t[x∧y]) ξ = sem t (ξ⟨x := ξ y, y := ξ x⟩), which yields the
deﬁnition of [ ∧]I by i [x∧y]I ξ = i (ξ⟨x := ξ y, y := ξ x⟩). However, as pointed
out by Pitts [39, §6.3] (in the slightly diﬀerent context of interpreting simply-
typed λ-calculus), the nominal recursor (Theorem 2) does not directly apply
(hence neither does my reformulation based on CE nominal sets, Theorem 10).
This is because, in my terminology, the structure I = (I, [ ∧]I, VrI, ApI, LmI)
is not a CE nominal set. The problematic condition is FCB (the freshness condi-
tion for binders), requiring that x #I (LmI x i) holds for all i ∈I. Expanding the
deﬁnition of #I (the nominal deﬁnition of freshness from swapping, recalled in
Sect. 2.2) and the deﬁnitions of [ ∧]I and LmI, one can see that x #I (LmI x i)
means the following:
lm (d →i (ξ⟨x := ξ y, y := ξ x⟩⟨x := d⟩)) = lm (d →i (ξ⟨x := d⟩)), i.e.,
lm (d →i (ξ⟨x := d, y := ξ x⟩) = lm (d →i (ξ⟨x := d⟩)), holds for all but a ﬁnite
number of variables y.
The only chance for the above to be true is if i, when applied to an envi-
ronment, ignores the value of y in that environment for all but a ﬁnite number
of variables y; in other words, i only analyzes the value of a ﬁnite number of
variables in that environment—but this is not guaranteed to hold for arbitrary
elements i ∈I. To repair this, Pitts engages in a form of induction-recursion [17],
carving out from I a smaller domain that is still large enough to interpret all
terms, then proving that both FCB and the other axioms hold for this restricted
domain. It all works out in the end, but the technicalities are quite involved.
Although FCB is not required by the renaming-based principle, note inci-
dentally that this condition would actually be true (and immediate to check) if
working with freshness deﬁned not from swapping but from renaming. Indeed,
the renaming-based version of x #I (LmI x i) says that lm (d →i (ξ⟨x :=
ξ y⟩⟨x := d⟩)) = lm (d →i (ξ⟨x := d⟩)) holds for all y (or at least for some
y ̸= x)—which is immediate since ξ⟨x := ξ y⟩⟨x := d⟩= ξ⟨x := d⟩. This further
illustrates the idea that semantic domains ‘favor’ renaming over swapping.
In conclusion, for interpreting syntax in semantic domains, my renaming-
based recursor is trivial to apply, whereas the nominal recursor requires some
fairly involved additional deﬁnitions and proofs.
6
Conclusion and Related Work
This paper introduced and studied rensets, contributing (1) theoretically, a min-
imalistic equational characterization of the datatype of terms with bindings and
(2) practically, an addition to the formal arsenal for manipulating syntax with
bindings. It is part of a longstanding line of work by myself and collabora-
tors on exploring convenient deﬁnition and reasoning principles for bindings
[25,27,43,46,47], and will be incorporated into the ongoing implementation of a
new Isabelle deﬁnitional package for binding-aware datatypes [12].
Initial Model Characterizations of the Terms Datatype. My results pro-
vide a truly elementary characterization of terms with bindings, as an “ordinary”
datatype speciﬁed by the fundamental operations only (the constructors plus

Rensets and Renaming-Based Recursion for Syntax with Bindings
631
Fiore
et al. [22]
Hofmann
[29]
Pitts
[39]
Urban
et al.
[57,56]
Norrish
[33]
Popescu
&Gunter
[46]
Gheri&
Popescu
[25]
This
paper
Paradigm
nameless
nameful
nameful
nameful
nameful
nameful
nameful
Barendregt?
n/a
yes
yes
yes
no
no
yes
Underlying
category
SetF
Set
Set
Set
Set
Set
Set
Required
operations/
relations
ctors,
rename,
free-vars
ctors,
perm
ctors,
perm
ctors,
swap,
free-vars
ctors,
term/var
subst,
fresh
ctors,
swap,
fresh
ctors,
rename
Required
properties
functori-
ality,
naturality
Horn
clauses,
fresh-def,
ﬁn-supp
Horn
clauses,
fresh-def
Horn
clauses
Horn
clauses
Horn
clauses
equations
Fig. 1. Initial model characterizations of the datatype of terms with bindings “ctors” =
“constructors”, “perm” = “permutation”, “fresh” = “the freshness predicate”, “fresh-
def” = “clause for deﬁning the freshness predicate”, “ﬁn-supp” = “Finite Support”
variable-for-variable renaming) and some equations (those deﬁning CE rensets).
As far as speciﬁcation simplicity goes, this is “the next best thing” after a com-
pletely free datatype such as those of natural numbers or lists.
Figure 1 shows previous characterizations from the literature, in which terms
with bindings are identiﬁed as an initial model (or algebra) of some kind. For
each of these, I indicate (1) the employed reasoning paradigm, (2) whether the
initiality/recursion theorem features an extension with Barendregt’s variable
convention, (3) the underlying category (from where the carriers of the models
are taken), (4) the operations and relations on terms to which the models must
provide counterparts and (5) the properties required on the models.
While some of these results enjoy elegant mathematical properties of intrinsic
value, my main interest is in the recursors they enable, speciﬁcally in the ease of
deploying these recursors. That is, I am interested in how easy it is in principle
to organize the target domain as a model of the requested type, hence obtain
the desired morphism, i.e., get the recursive deﬁnition done. By this measure,
elementary approaches relying on standard FOL-like models whose carriers are
sets rather than pre-sheaves have an advantage. Also, it seems intuitive that a
recursor is easier to apply if there are fewer operators, and fewer and structurally
simpler properties required on its models—although empirical evidence of suc-
cessfully deploying the recursor in practice should complement the simplicity
assessment, to ensure that simplicity is not sponsored by lack of expressiveness.
The ﬁrst column in Fig. 1’s table contains an inﬂuential representative of the
nameless paradigm: the result obtained independently by Fiore et al. [22] and
Hofmann [29] characterizing terms as initial in the category of algebras over the

632
A. Popescu
pre-sheaf topos SetF, where F is the category of ﬁnite ordinals and functions
between them. The operators required by algebras are the constructors, as well
as the free-variable operator (implicitly as part of the separation on levels) and
the injective renamings (as part of the functorial structure). The algebra’s carrier
is required to be a functor and the constructors to be natural transformations.
There are several variations of this approach, e.g., [5,11,29], some implemented
in proof assistants, e.g., [3,4,31].
The other columns refer to initiality results that are more closely related to
mine. They take place within the nameful paradigm, and they all rely on ele-
mentary models (with set carriers). Pitts’s already discussed nominal recursor
[39] (based on previous work by Gabbay and Pitts [23]) employs the constructors
and permutation (or swapping), and requires that its models satisfy some Horn
clauses for constructors, permutation and freshness, together with the second-
order properties that (1) deﬁne freshness from swapping and (2) express Finite
Support. Urban et al.’s version [56,57] implemented in Isabelle/Nominal is an
improvement of Pitts’s in that it removes the Finite Support requirement from
the models—which is practically signiﬁcant because it enables non-ﬁnitely sup-
ported target domains for recursion. Norrish’s result [33] is explicitly inspired by
nominal logic, but renounces the deﬁnability of the free-variable operator from
swapping—with the price of taking both swapping and free-variables as primi-
tives. My previous work with Gunter and Gheri takes as primitives either term-
for-variable substitution and freshness [46] or swapping and freshness [25], and
requires properties expressed by diﬀerent Horn clauses (and does not explore a
Barendregt dimension, like Pitts, Urban et al. and Norrish do). My previous focus
on term-for-variable substitution [46] (as opposed to renaming, i.e., variable-for-
variable substitution) impairs expressiveness—for example, the depth of a term
is not deﬁnable using a recursor based on term-for-variable substitution because
we cannot say how term-for-variable substitution aﬀects the depth of a term
based on its depth and that of the substitutee alone. My current result based
on rensets keeps freshness out of the primitive operators base (like nominal logic
does), and provides an unconditionally equational characterization using only
constructors and renaming. The key to achieving this minimality is the simple
expression of freshness from renaming in my axiomatization of rensets. In future
work, I plan a systematic formal comparison of the relative expressiveness of all
these nameful recursors.
Recursors in Other Paradigms. Figure 1 focuses on nameful recursors, while
only the Fiore et al./Hofmann recursor for the sake of a rough comparison with
the nameless approach. I should stress that such a comparison is necessarily
rough, since the nameless recursors do not give the same “payload” as the name-
ful ones. This is because of the handling of bound variables. In the nameless
paradigm, the λ-constructor does not explicitly take a variable as an input, as
in Lm x t, i.e., does not have type Var →Trm →Trm. Instead, the bindings
are indicated through nameless pointers to positions in a term. So the nameless
λ-constructor, let’s call it NLm, takes only a term, as in NLm t, i.e., has type
Trm →Trm or a scope-safe (polymorphic or dependently-typed) variation of this,

Rensets and Renaming-Based Recursion for Syntax with Bindings
633
e.g., 
n∈F Trmn →Trmn+1 [22,29] or 
α∈Type Trmα →Trmα+unit [5,11]. The λ-
constructor is of course matched by operators in the considered models, which
appears in the clauses of the functions f deﬁned recursively on terms: Instead
of a clause of the form f (Lm x t) = ⟨expression depending on x and f t⟩from
the nameful paradigm, in the nameless paradigm one gets a clause of the form
f (NLm t) = ⟨expression depending on f t⟩. A nameless recursor is usually eas-
ier to prove correct and easier to apply because the nameless constructor NLm
is free—whereas a nameful recursor must wrestle with the non-freeness of Lm,
handled by verifying certain properties of the target models. However, once the
deﬁnition is done, having nameful clauses pays oﬀby allowing “textbook-style”
proofs that stay close to the informal presentation of a calculus or logic, whereas
with the nameless deﬁnition some additional index shifting bureaucracy is nec-
essary. (See [9] for a detailed discussion, and [14] for a hybrid solution.)
A comparison of nameful recursion with HOAS recursion is also generally
diﬃcult, since major HOAS frameworks such as Abella [7], Beluga [37] or
Twelf [36] are developed within non-standard logical foundations, allowing a
λ-constructor of type (Trm →Trm) →Trm, which is not amenable to typi-
cal well-foundedness based recursion but requires some custom solutions (e.g.,
[21,50]). However, the weak HOAS variant [16,27] employs a constructor of the
form WHLm : (Var →Trm) →Trm which is recursable, and in fact yields a
free datatype, let us call it WHTrm—one generated by WHVr : Var →WHTrm,
WHAp : WHTrm →WHTrm →WHTrm and WHLm. WHTrm contains (natural
encodings of) all terms but also additional entities referred to as “exotic terms”.
Partly because of the exotic terms, this free datatype by itself is not very helpful
for recursively deﬁning useful functions on terms. But the situation is dramati-
cally improved if one employs a variant of weak HOAS called parametric HOAS
(PHOAS) [15], i.e., takes Var not as a ﬁxed type but as a type parameter (type
variable) and works with 
Var∈Type TrmVar; this enables many useful deﬁnitions
by choosing a suitable type Var (usually large enough to make the necessary dis-
tinctions) and then performing standard recursion. The functions deﬁnable in
the style of PHOAS seem to be exactly those deﬁnable via the semantic domain
interpretation pattern (Sect. 5.3): Choosing the instantiation of Var to a type
T corresponds to employing environments in Var →T. (I illustrate this at the
end of [45, Appendix A] by showing the semantic-domain version of a PHOAS
example.)
As a hybrid nameful/HOAS approach we can count Gordon and Melham’s
characterization of the datatype of terms [26], which employs the nameful con-
structors but formulates recursion treating Lm as if recursing in the weak-HOAS
datatype WHTrm. Norrish’s recursor [33] (a participant in Fig. 1) has been
inferred from Gordon and Melham’s one. Weak-HOAS recursion also has inter-
esting connections with nameless recursion: In presheaf toposes such as those
employed by Fiore et al. [22], Hofmann [29] and Ambler et al. [6], for any object
T the function space Var ⇒T is isomorphic to the De Bruijn level shifting trans-
formation applied to T; this eﬀectively equates the weak-HOAS and nameless
recursors. A ﬁnal cross-paradigm note: In themselves, nominal sets are not con-

634
A. Popescu
ﬁned to the nameful paradigm; their category is equivalent [23] to the Schanuel
topos [30], which is attractive for pursuing the nameless approach.
Axiomatizations of Renaming. In his study of name-passing process calculi,
Staton [52] considers an enrichment of nominal sets with renaming (in addition
to swapping) and axiomatizes renaming with the help of the nominal (swapping-
deﬁned) freshness predicate. He shows that the resulted category is equivalent to
the non-injective renaming counterpart of the Schanuel topos (i.e., the subcat-
egory of SetF consisting of functors that preserve pullbacks of monos). Gabbay
and Hofmann [24] provide an elementary characterization of the above category,
in terms of nominal renaming sets, which are sets equipped with a multiple-
variable-renaming action satisfying identity and composition laws, and a form
of Finite Support (FS). Nominal renaming sets seem very related to rensets
satisfying FS. Indeed, any nominal renaming set forms a FS-satisfying renset
when restricted to single-variable renaming. Conversely, I conjecture that any
FS-satisfying renset gives rise to a nominal renaming set. This correspondence
seems similar to the one between the permutation-based and swapping-based
alternative axiomatizations of nominal sets—in that the two express the same
concept up to an isomorphism of categories. In their paper, Gabbay and Hof-
mann do not study renaming-based recursion, beyond noting the availability of a
recursor stemming from the functor-category view (which, as I discussed above,
enables nameless recursion with a weak-HOAS ﬂavor). Pitts [41] introduces nom-
inal sets with 01-substitution structure, which axiomatize substitution of one of
two possible constants for variables on top of the nominal axiomatization, and
proves that they form a category that is equivalent with that of cubical sets [10],
hence relevant for the univalent foundations [54].
Other Work. Sun [53] develops universal algebra for ﬁrst-order languages with
bindings (generalizing work by Aczel [2]) and proves a completeness theorem. In
joint work with Ro¸su [48], I develop ﬁrst-order logic and prove completeness on
top of a generic syntax with axiomatized free-variables and substitution.
Renaming Versus Swapping and Nominal Logic, Final Round. I believe
that my work complements rather than competes with nominal logic. My results
do not challenge the swapping-based approach to deﬁning syntax (deﬁning the
alpha-equivalence on pre-terms and quotienting to obtain terms) recommended
by nominal logic, which is more elegant than a renaming-based alternative; but
my easier-to-apply recursor can be a useful addition even on top of the nominal
substratum. Moreover, some of my constructions are explicitly inspired by the
nominal ones. For example, I started by adapting the nominal idea of deﬁning
freshness from swapping before noticing that renaming enables a simpler formu-
lation. My formal treatment of Barendregt’s variable convention also originates
from nominal logic—as it turns out, this idea works equally well in my setting.
In fact, I came to believe that the possibility of a Barendregt enhancement is
largely orthogonal to the particularities of a binding-aware recursor. In future
work, I plan to investigate this, i.e., seek general conditions under which an
initiality principle (such as Theorems 10 and 8) is amenable to a Barendregt
enhancement (such as Theorems 2 and 11, respectively).

Rensets and Renaming-Based Recursion for Syntax with Bindings
635
Acknowledgments. I am grateful to the IJCAR reviewers for their insightful com-
ments and suggestions, and for pointing out related work.
References
1. Abel, A., et al.: Poplmark reloaded: mechanizing proofs by logical relations. J.
Funct. Program. 29, e19 (2019). https://doi.org/10.1017/S0956796819000170
2. Aczel, P.: Frege structures and notations in propositions, truth and set. In: The
Kleene Symposium, pp. 31–59. North Holland (1980)
3. Allais, G., Atkey, R., Chapman, J., McBride, C., McKinna, J.: A type and scope
safe universe of syntaxes with binding: their semantics and proofs. Proc. ACM
Program. Lang. 2(International Conference on Functional Programming (ICFP)),
90:1–90:30 (2018). https://doi.acm.org/10.1145/3236785
4. Allais, G., Chapman, J., McBride, C., McKinna, J.: Type-and-scope safe programs
and their proofs. In: Bertot, Y., Vafeiadis, V. (eds.) Proceedings of the 6th ACM
SIGPLAN Conference on Certiﬁed Programs and Proofs, CPP 2017, Paris, France,
16–17 January 2017, pp. 195–207. ACM (2017). https://doi.org/10.1145/3018610.
3018613
5. Altenkirch, T., Reus, B.: Monadic presentations of lambda terms using generalized
inductive types. In: Flum, J., Rodriguez-Artalejo, M. (eds.) CSL 1999. LNCS,
vol. 1683, pp. 453–468. Springer, Heidelberg (1999). https://doi.org/10.1007/3-
540-48168-0 32
6. Ambler, S.J., Crole, R.L., Momigliano, A.: A deﬁnitional approach to primitivexs
recursion over higher order abstract syntax. In: Eighth ACM SIGPLAN Interna-
tional Conference on Functional Programming, Workshop on Mechanized Reason-
ing About Languages with Variable Binding, MERLIN 2003, Uppsala, Sweden,
August 2003. ACM (2003). https://doi.org/10.1145/976571.976572
7. Baelde, D., et al.: Abella: a system for reasoning about relational speciﬁcations. J.
Formaliz. Reason. 7(2), 1–89 (2014). https://doi.org/10.6092/issn.1972-5787/4650
8. Barendregt, H.P.: The Lambda Calculus: Its Syntax and Semantics, Studies in
Logic, vol. 40. Elsevier (1984)
9. Berghofer, S., Urban, C.: A head-to-head comparison of de Bruijn indices and
names. Electr. Notes Theor. Comput. Sci. 174(5), 53–67 (2007). https://doi.org/
10.1016/j.entcs.2007.01.018
10. Bezem, M., Coquand, T., Huber, S.: A model of type theory in cubical sets. In:
Matthes, R., Schubert, A. (eds.) 19th International Conference on Types for Proofs
and Programs, TYPES 2013, 22–26 April 2013, Toulouse, France. LIPIcs, vol. 26,
pp. 107–128. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik (2013). https://
doi.org/10.4230/LIPIcs.TYPES.2013.107
11. Bird, R.S., Paterson, R.: De Bruijn notation as a nested datatype. J. Funct. Pro-
gram. 9(1), 77–91 (1999). https://doi.org/10.1017/S0956796899003366
12. Blanchette, J.C., Gheri, L., Popescu, A., Traytel, D.: Bindings as bounded natural
functors. Proc. ACM Program. Lang. 3(POPL), 22:1–22:34 (2019). https://doi.
org/10.1145/3290335
13. de Bruijn, N.G.: Lambda calculus notation with nameless dummies, a tool for
automatic formula manipulation, with application to the Church-Rosser theorem.
Indag. Math. 75(5), 381–392 (1972). https://doi.org/10.1016/1385-7258(72)90034-
0
14. Chargu´eraud, A.: The locally nameless representation. J. Autom. Reason. 49(3),
363–408 (2012). https://doi.org/10.1007/s10817-011-9225-2

636
A. Popescu
15. Chlipala, A.: Parametric higher-order abstract syntax for mechanized semantics.
In: Hook, J., Thiemann, P. (eds.) International Conference on Functional Program-
ming (ICFP) 2008, pp. 143–156. ACM (2008). https://doi.org/10.1145/1411204.
1411226
16. Despeyroux, J., Felty, A., Hirschowitz, A.: Higher-order abstract syntax in Coq.
In: Dezani-Ciancaglini, M., Plotkin, G. (eds.) TLCA 1995. LNCS, vol. 902, pp.
124–138. Springer, Heidelberg (1995). https://doi.org/10.1007/BFb0014049
17. Dybjer, P.: A general formulation of simultaneous inductive-recursive deﬁnitions
in type theory. J. Symb. Log. 65(2), 525–549 (2000). https://doi.org/10.2307/
2586554
18. Felty, A.P., Momigliano, A.: Hybrid: a deﬁnitional two-level approach to reason-
ing with higher-order abstract syntax. J. Autom. Reason. 48(1), 43–105 (2012).
https://doi.org/10.1007/s10817-010-9194-x
19. Felty, A.P., Momigliano, A., Pientka, B.: The next 700 challenge problems for
reasoning with higher-order abstract syntax representations - part 2 - a survey. J.
Autom. Reason. 55(4), 307–372 (2015). https://doi.org/10.1007/s10817-015-9327-
3
20. Felty, A.P., Momigliano, A., Pientka, B.: An open challenge problem repository
for systems supporting binders. In: Cervesato, I., Chaudhuri, K. (eds.) Proceed-
ings Tenth International Workshop on Logical Frameworks and Meta Languages:
Theory and Practice, LFMTP 2015, Berlin, Germany, 1 August 2015. EPTCS, vol.
185, pp. 18–32 (2015). https://doi.org/10.4204/EPTCS.185.2
21. Ferreira, F., Pientka, B.: Programs using syntax with ﬁrst-class binders. In: Yang,
H. (ed.) ESOP 2017. LNCS, vol. 10201, pp. 504–529. Springer, Heidelberg (2017).
https://doi.org/10.1007/978-3-662-54434-1 19
22. Fiore, M.P., Plotkin, G.D., Turi, D.: Abstract syntax and variable binding. In:
Logic in Computer Science (LICS) 1999, pp. 193–202. IEEE Computer Society
(1999). https://doi.org/10.1109/LICS.1999.782615
23. Gabbay, M., Pitts, A.M.: A new approach to abstract syntax involving binders.
In: Logic in Computer Science (LICS) 1999, pp. 214–224. IEEE Computer Society
(1999). https://doi.org/10.1109/LICS.1999.782617
24. Gabbay, M.J., Hofmann, M.: Nominal renaming sets. In: Cervesato, I., Veith, H.,
Voronkov, A. (eds.) LPAR 2008. LNCS (LNAI), vol. 5330, pp. 158–173. Springer,
Heidelberg (2008). https://doi.org/10.1007/978-3-540-89439-1 11
25. Gheri, L., Popescu, A.: A formalized general theory of syntax with bindings:
extended version. J. Autom. Reason. 64(4), 641–675 (2020). https://doi.org/10.
1007/s10817-019-09522-2
26. Gordon, A.D., Melham, T.: Five axioms of alpha-conversion. In: Goos, G., Hart-
manis, J., van Leeuwen, J., von Wright, J., Grundy, J., Harrison, J. (eds.) TPHOLs
1996. LNCS, vol. 1125, pp. 173–190. Springer, Heidelberg (1996). https://doi.org/
10.1007/BFb0105404
27. Gunter, E.L., Osborn, C.J., Popescu, A.: Theory support for weak higher order
abstract syntax in Isabelle/HOL. In: Cheney, J., Felty, A.P. (eds.) Logical Frame-
works and Meta-Languages: Theory and Practice (LFMTP) 2009, pp. 12–20. ACM
(2009). https://doi.org/10.1145/1577824.1577827
28. Harper, R., Honsell, F., Plotkin, G.D.: A framework for deﬁning logics. In: Logic
in Computer Science (LICS) 1987, pp. 194–204. IEEE Computer Society (1987).
https://doi.org/10.1145/138027.138060
29. Hofmann, M.: Semantical analysis of higher-order abstract syntax. In: Logic in
Computer Science (LICS) 1999, pp. 204–213. IEEE Computer Society (1999).
https://doi.org/10.1109/LICS.1999.782616

Rensets and Renaming-Based Recursion for Syntax with Bindings
637
30. Johnstone, P.T.: Quotients of decidable objects in a topos. Math. Proc. Camb.
Philos. Soc. 93, 409–419 (1983). https://doi.org/10.1017/S0305004100060734
31. Kaiser, J., Sch¨afer, S., Stark, K.: Binder aware recursion over well-scoped de bruijn
syntax. In: Andronick, J., Felty, A.P. (eds.) Proceedings of the 7th ACM SIGPLAN
International Conference on Certiﬁed Programs and Proofs, CPP 2018, Los Ange-
les, CA, USA, 8–9 January 2018, pp. 293–306. ACM (2018). https://doi.org/10.
1145/3167098
32. Nipkow, T., Wenzel, M., Paulson, L.C. (eds.): Isabelle/HOL—A Proof Assistant
for Higher-Order Logic. LNCS, vol. 2283. Springer, Heidelberg (2002). https://doi.
org/10.1007/3-540-45949-9
33. Norrish, M.: Recursive function deﬁnition for types with binders. In: Slind, K.,
Bunker, A., Gopalakrishnan, G. (eds.) TPHOLs 2004. LNCS, vol. 3223, pp. 241–
256. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-30142-4 18
34. Paulson, L.C.: The foundation of a generic theorem prover. J. Autom. Reason.
5(3), 363–397 (1989). https://doi.org/10.1007/BF00248324
35. Pfenning, F., Elliott, C.: Higher-order abstract syntax. In: Wexelblat, R.L. (ed.)
Programming Language Design and Implementation (PLDI) 1988, pp. 199–208.
ACM (1988). https://doi.org/10.1145/53990.54010
36. Pfenning, F., Sch¨urmann, C.: System description: twelf — a meta-logical frame-
work for deductive systems. In: Ganzinger, H. (ed.) CADE 1999. LNCS (LNAI),
vol. 1632, pp. 202–206. Springer, Heidelberg (1999). https://doi.org/10.1007/3-
540-48660-7 14
37. Pientka, B.: Beluga: programming with dependent types, contextual data, and
contexts. In: Blume, M., Kobayashi, N., Vidal, G. (eds.) FLOPS 2010. LNCS, vol.
6009, pp. 1–12. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-
12251-4 1
38. Pitts, A.M.: Nominal logic, a ﬁrst order theory of names and binding. Inf. Comput.
186(2), 165–193 (2003). https://doi.org/10.1016/S0890-5401(03)00138-X
39. Pitts, A.M.: Alpha-structural recursion and induction. J. ACM 53(3), 459–506
(2006). https://doi.org/10.1145/1147954.1147961
40. Pitts, A.M.: Nominal Sets: Names and Symmetry in Computer Science. Cambridge
Tracts in Theoretical Computer Science. Cambridge University Press, Cambridge
(2013)
41. Pitts, A.M.: Nominal presentation of cubical sets models of type theory. In: Herbe-
lin, H., Letouzey, P., Sozeau, M. (eds.) 20th International Conference on Types for
Proofs and Programs (TYPES 2014). Leibniz International Proceedings in Infor-
matics (LIPIcs), vol. 39, pp. 202–220. Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik, Dagstuhl, Germany (2015). http://drops.dagstuhl.de/opus/volltexte/2015/
5498
42. Pollack, R., Sato, M., Ricciotti, W.: A canonical locally named representation
of binding. J. Autom. Reason. 49(2), 185–207 (2012). https://doi.org/10.1007/
s10817-011-9229-y
43. Popescu, A.: Contributions to the theory of syntax with bindings and to process
algebra. Ph.D. thesis, University of Illinois at Urbana-Champaign (2010). https://
www.andreipopescu.uk/pdf/thesisUIUC.pdf
44. Popescu, A.: Renaming-Enriched Sets. Arch. Formal Proofs 2022 (2022). https://
www.isa-afp.org/entries/Renaming Enriched Sets.html
45. Popescu, A.: Rensets and renaming-based recursion for syntax with bindings. arXiv
(2022). https://arxiv.org/abs/2205.09233

638
A. Popescu
46. Popescu, A., Gunter, E.L.: Recursion principles for syntax with bindings and sub-
stitution. In: Chakravarty, M.M.T., Hu, Z., Danvy, O. (eds.) Proceeding of the
16th ACM SIGPLAN International Conference on Functional Programming, ICFP
2011, Tokyo, Japan, 19–21 September 2011, pp. 346–358. ACM (2011). https://
doi.org/10.1145/2034773.2034819
47. Popescu, A., Gunter, E.L., Osborn, C.J.: Strong normalization for system F by
HOAS on top of FOAS. In: Logic in Computer Science (LICS) 2010, pp. 31–40.
IEEE Computer Society (2010). https://doi.org/10.1109/LICS.2010.48
48. Popescu, A., Ro¸su, G.: Term-generic logic. Theor. Comput. Sci. 577, 1–24 (2015)
49. Sch¨afer, S., Tebbi, T., Smolka, G.: Autosubst: reasoning with de Bruijn terms and
parallel substitutions. In: Urban, C., Zhang, X. (eds.) ITP 2015. LNCS, vol. 9236,
pp. 359–374. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-22102-
1 24
50. Sch¨urmann, C., Despeyroux, J., Pfenning, F.: Primitive recursion for higher-order
abstract syntax. Theor. Comput. Sci. 266(1–2), 1–57 (2001). https://doi.org/10.
1016/S0304-3975(00)00418-7
51. Stark, K.: Mechanising syntax with binders in Coq. Ph.D. thesis, Saarland Uni-
versity, Saarbr¨ucken, Germany (2020). https://publikationen.sulb.uni-saarland.de/
handle/20.500.11880/28822
52. Staton, S.: Name-passing process calculi: operational models and structural oper-
ational semantics. Technical report, UCAM-CL-TR-688, University of Cambridge,
Computer Laboratory (2007). https://www.cl.cam.ac.uk/techreports/UCAM-CL-
TR-688.pdf
53. Sun, Y.: An algebraic generalization of Frege structures–binding algebras. Theor.
Comput. Sci. 211(1–2), 189–232 (1999)
54. The
Univalent
Foundations
Program:
Homotopy
Type
Theory.
Univalent
Foundations of Mathematics. Institute for Advanced Study (2013). https://
homotopytypetheory.org/book
55. Traytel,
D.,
Popescu,
A.,
Blanchette,
J.C.:
Foundational,
compositional
(co)datatypes for higher-order logic: category theory applied to theorem proving.
In: Logic in Computer Science (LICS) 2012, pp. 596–605. IEEE Computer Society
(2012). https://doi.org/10.1109/LICS.2012.75
56. Urban, C.: Nominal techniques in Isabelle/HOL. J. Autom. Reason. 40(4), 327–356
(2008). https://doi.org/10.1007/s10817-008-9097-2
57. Urban, C., Berghofer, S.: A recursion combinator for nominal datatypes imple-
mented in Isabelle/HOL. In: Furbach, U., Shankar, N. (eds.) IJCAR 2006. LNCS
(LNAI), vol. 4130, pp. 498–512. Springer, Heidelberg (2006). https://doi.org/10.
1007/11814771 41
58. Urban, C., Berghofer, S., Norrish, M.: Barendregt’s variable convention in rule
inductions. In: Pfenning, F. (ed.) CADE 2007. LNCS (LNAI), vol. 4603, pp. 35–
50. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-73595-3 4
59. Urban, C., Kaliszyk, C.: General bindings and alpha-equivalence in Nominal
Isabelle. Log. Methods Comput. Sci. 8(2) (2012). https://doi.org/10.2168/LMCS-
8(2:14)2012
60. Urban, C., Tasson, C.: Nominal techniques in Isabelle/HOL. In: Nieuwenhuis, R.
(ed.) CADE 2005. LNCS (LNAI), vol. 3632, pp. 38–53. Springer, Heidelberg (2005).
https://doi.org/10.1007/11532231 4

Rensets and Renaming-Based Recursion for Syntax with Bindings
639
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Finite Two-Dimensional Proof Systems
for Non-ﬁnitely Axiomatizable Logics
Vitor Greati1,2(B)
and Jo˜ao Marcos1
1 Programa de P´os-gradua¸c˜ao em Sistemas e Computa¸c˜ao & DIMAp,
Universidade Federal do Rio Grande do Norte, Natal, Brazil
vitor.greati.017@ufrn.edu.br, jmarcos@dimap.ufrn.br
2 Bernoulli Institute, University of Groningen, Groningen, The Netherlands
Abstract. The characterizing properties of a proof-theoretical presen-
tation of a given logic may hang on the choice of proof formalism, on
the shape of the logical rules and of the sequents manipulated by a given
proof system, on the underlying notion of consequence, and even on the
expressiveness of its linguistic resources and on the logical framework into
which it is embedded. Standard (one-dimensional) logics determined by
(non-deterministic) logical matrices are known to be axiomatizable by
analytic and possibly ﬁnite proof systems as soon as they turn out to
satisfy a certain constraint of suﬃcient expressiveness. In this paper we
introduce a recipe for cooking up a two-dimensional logical matrix (or
B-matrix) by the combination of two (possibly partial) non-deterministic
logical matrices. We will show that such a combination may result in B-
matrices satisfying the property of suﬃcient expressiveness, even when
the input matrices are not suﬃciently expressive in isolation, and we will
use this result to show that one-dimensional logics that are not ﬁnitely
axiomatizable may inhabit ﬁnitely axiomatizable two-dimensional logics,
becoming, thus, ﬁnitely axiomatizable by the addition of an extra dimen-
sion. We will illustrate the said construction using a well-known logic of
formal inconsistency called mCi. We will ﬁrst prove that this logic is not
ﬁnitely axiomatizable by a one-dimensional (generalized) Hilbert-style
system. Then, taking advantage of a known 5-valued non-deterministic
logical matrix for this logic, we will combine it with another one, conve-
niently chosen so as to give rise to a B-matrix that is axiomatized by a
two-dimensional Hilbert-style system that is both ﬁnite and analytic.
Keywords: Hilbert-style proof systems · ﬁnite axiomatizability ·
consequence relations · non-deterministic semantics · paraconsistency
1
Introduction
A logic is commonly deﬁned nowadays as a relation that connects collections
of formulas from a formal language and satisﬁes some closure properties. The
V. Greati acknowledges support from CAPES—Finance Code 001 and from the FWF
project P 33548. J. Marcos acknowledges support from CNPq.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 640–658, 2022.
https://doi.org/10.1007/978-3-031-10769-6_37

Finite Two-Dimensional Proof Systems
641
established connections are called consecutions and each of them has two parts,
an antecedent and a succedent, the latter often being said to ‘follow from’ (or
to be a consequence of) the former. A logic may be manufactured in a number
of ways, in particular as being induced by the set of derivations justiﬁed by
the rules of inference of a given proof system. There are diﬀerent kinds of proof
systems, the diﬀerences between them residing mainly in the shapes of their rules
of inference and on the way derivations are built. We will be interested here in
Hilbert-style proof systems (‘H-systems’, for short), whose rules of inference have
the same shape of the consecutions of the logic they canonically induce and whose
associated derivations consist in expanding a given antecedent by applications of
rules of inference until the desired succedent is produced. A remarkable property
of an H-system is that the logic induced by it is the least logic containing the
rules of inference of the system; in the words of [24], the system constitutes a
‘logical basis’ for the said logic.
Conventional H-systems, which we here dub ‘Set-Fmla H-systems’, do not
allow for more than one formula in the succedents of the consecutions that they
manipulate. Since [23], however, we have learned that the simple elimination of
this restriction on H-systems —that is, allowing for sets of formulas rather than
single formulas in the succedents— brings numerous advantages, among which
we mention: modularity (correspondence between rules of inference and proper-
ties satisﬁed by a semantical structure), analyticity (control over the resources
demanded to produce a derivation), and the automatic generation of analytic
proof systems for a wide class of logics speciﬁed by suﬃciently expressive non-
deterministics semantics, with an associated straightforward proof-search pro-
cedure [13,18]. Such generalized systems, here dubbed ‘Set-Set H-systems’,
induce logics whose consecutions involve succedents consisting in a collection of
formulas, intuitively understood as ‘alternative conclusions’.
An H-system H is said to be an axiomatization for a given logic L when the
logic induced by H coincides with L. A desirable property for an axiomatization
is ﬁniteness, namely the property of consisting on a ﬁnite collection of schematic
axioms and rules of inference. A logic having a ﬁnite axiomatization is said to
be ‘ﬁnitely based’. In the literature, one may ﬁnd examples of logics having a
quite simple, ﬁnite semantic presentation, being, in contrast, not ﬁnitely based
in terms of Set-Fmla H-systems [21]. These very logics, however, when seen
as companions of logics with multiple formulas in the succedent, turn out to be
ﬁnitely based in terms of Set-Set H-systems [18]. In other words, by updating
the underlying proof-theoretical and the logical formalisms, we are able to obtain
a ﬁnite axiomatization for logics which in a more restricted setting could not be
said to be ﬁnitely based. We may compare the above mentioned movement to
the common mathematical practice of adding dimensions in order to provide
better insight on some phenomenon. A well-known example of that is given by
the Fundamental Theorem of Algebra, which provides an elegant solution to the
problem of determining the roots of polynomials over a single variable, demand-
ing only that real coeﬃcients should be replaced by complex coeﬃcients. Another
example, from Machine Learning, is the ‘kernel trick’ employed in support vector
machines: by increasing the dimensionality of the input space, the transformed

642
V. Greati and J. Marcos
data points become more easily separable by hyperplanes, making it possible to
achieve better results in classiﬁcation tasks.
It is worth noting that there are logics that fail to be ﬁnitely based in terms
of Set-Set H-systems. An example of a logic designed with the sole purpose of
illustrating this possibility was provided in [18]. One of the goals of the present
work is to show that an important logic from the literature of logics of formal
inconsistency (LFIs) called mCi is also an example of this phenomenon. This
logic results from adding inﬁnitely-many axiom schemas to the logic mbC, a logic
that is obtained by extending positive classical logic with two axiom schemas.
Incidentally, along the proof of this result, we will show that mCi is the limit of a
strictly increasing chain of LFIs extending mbC (comparable to the case of CLim
in da Costa’s hierarchy of increasingly weaker paraconsistent calculi [16]). A nat-
ural question, then, is whether we can enrich our technology, in the same vein, in
order to provide ﬁnite axiomatizations for all these logics. We answer that in the
aﬃrmative by means of the two-dimensional frameworks developed in [11,17].
Logics, in this case, connect pairs of collections of formulas. A consecution, in
this setting, may be read as involving formulas that are accepted and those that
are not, as well as formulas that are rejected and those that are not. ‘Accep-
tance’ and ‘rejection’ are seen, thus, as two orthogonal dimensions that may
interact, making it possible, thus, to express more complex consecutions than
those expressible in one-dimensional logics. Two-dimensional H-systems, which
we call ‘Set2-Set2 H-systems’, generalize Set-Set H-systems so as to manipu-
late pairs of collections of formulas, canonically inducing two-dimensional logics
and constituting logical bases for them. Another goal of the present work is,
therefore, to show how to obtain a two-dimensional logic inhabited by a (possibly
not ﬁnitely based) one-dimensional logic of interest. More than that, the logic we
obtain will be ﬁnitely axiomatizable in terms of a Set2-Set2 analytic H-system.
The only requirements is that the one-dimensional logic of interest must have
an associated semantics in terms of a ﬁnite non-deterministic logical matrix and
that this matrix can be combined with another one through a novel procedure
that we will introduce, resulting in a two-dimensional non-deterministic matrix
(a B-matrix [9]) satisfying a certain condition of suﬃcient expressiveness [17].
An application of this approach will be provided here in order to produce the
ﬁrst ﬁnite and analytic axiomatization of mCi.
The paper is organized as follows: Sect. 2 introduces basic terminology and
deﬁnitions regarding algebras and languages. Section 3 presents the notions of
one-dimensional logics and Set-Set H-systems. Section 4 proves that mCi is
not ﬁnitely axiomatizable by one-dimensional H-systems. Section 5 introduces
two-dimensional logics and H-systems, and describes the approach to extending
a logical matrix to a B-matrix with the goal of ﬁnding a ﬁnite two-dimensional
axiomatization for the logic associated with the former. Section 6 presents a two-
dimensional ﬁnite analytic H-system for mCi. In the ﬁnal remarks, we highlight
some byproducts of our present approach and some features of the resulting
proof systems, in addition to pointing to some directions for further research.1
1 Detailed proofs of some results may be found in https://arxiv.org/abs/2205.08920.

Finite Two-Dimensional Proof Systems
643
2
Preliminaries
A propositional signature is a family Σ := {Σk}k∈ω, where each Σk is a collection
of k-ary connectives. We say that Σ is ﬁnite when its base set 
k∈ω Σk is
ﬁnite. A non-deterministic algebra over Σ, or simply Σ-nd-algebra, is a structure
A := ⟨A, ·A⟩, such that A is a non-empty collection of values called the carrier
of A, and, for each k ∈ω and c⃝∈Σk, the multifunction c⃝A : Ak →P(A)
is the interpretation of
c⃝in A. When Σ and A are ﬁnite, we say that A is
ﬁnite. When the range of all interpretations of A contains only singletons, A
is said to be a deterministic algebra over Σ, or simply a Σ-algebra, meeting
the usual deﬁnition from Universal Algebra [12]. When ∅is not in the range
of each c⃝A, A is said to be total. Given a Σ-algebra A and a c⃝∈Σ1, we
let c⃝0
A(x) := x and c⃝i+1
A (x) := c⃝A( c⃝i
A(x)). A mapping v : A →B is a
homomorphism from A to B when, for all k ∈ω, c⃝∈Σk and x1, . . . , xk ∈A, we
have f[ c⃝A(x1, . . . , xk)] ⊆c⃝B(f(x1), . . . , f(xk)). The set of all homomorphisms
from A to B is denoted by HomΣ(A, B). When B = A, we write EndΣ(A),
rather than HomΣ(A, A), for the set of endomorphisms on A.
Let P be a denumerable collection of propositional variables and Σ be a
propositional signature. The absolutely free Σ-algebra freely generated by P is
denoted by LΣ(P) and called the Σ-language generated by P. The elements of
LΣ(P) are called Σ-formulas, and those among them that are not propositional
variables are called Σ-compounds. Given Φ ⊆LΣ(P), we denote by Φc the set
LΣ(P)\Φ. The homomorphisms from LΣ(P) to A are called valuations on A,
and we denote by ValΣ(A) the collection thereof. Additionally, endomorphisms
on LΣ(P) are dubbed Σ-substitutions, and we let SubsP
Σ := EndΣ(LΣ(P)); when
there is no risk of confusion, we may omit the superscript from this notation.
Given ϕ ∈LΣ(P), let props(ϕ) be the set of propositional variables occurring
in ϕ. If props(ϕ) = {p1, . . . , pk}, we say that ϕ is k-ary (unary, for k = 1; binary,
for k = 2) and let ϕA : Ak →P(A) be the k-ary multifunction on A induced
by ϕ, where, for all x1, . . . , xk ∈A, we have ϕA(x1, . . . , xk) := {v(ϕ) | v ∈
ValΣ(A) and v(pi) = xi, for 1 ≤i ≤k}. Moreover, given ψ1, . . . , ψk ∈LΣ(P),
we write ϕ(ψ1, . . . , ψk) for the Σ-formula ϕLΣ(P )(ψ1, . . . , ψk), and, where Φ ⊆
LΣ(P) is a set of k-ary Σ-formulas, we let Φ(ψ1, . . . , ψk) := {ϕ(ψ1, . . . , ψk) | ϕ ∈
Φ}. Given ϕ ∈LΣ(P), by subf(ϕ) we refer to the set of subformulas of ϕ. Where
θ is a unary Σ-formula, we deﬁne the set subfθ(ϕ) as {σ(θ) | σ : P →subf(ϕ)}.
Given a set Θ ⊇{p} of unary Σ-formulas, we set subfΘ(ϕ) := 
θ∈Θ subfθ(ϕ).
For example, if Θ = {p, ¬p}, we will have subfΘ(¬(q ∨r)) = {q, r, q ∨r, ¬(q ∨
r)} ∪{¬q, ¬r, ¬(q ∨r), ¬¬(q ∨r)}. Such generalized notion of subformulas will
be used in the next section to provide a more generous proof-theoretical concept
of analyticity.
3
One-Dimensional Consequence Relations
A Set-Set statement (or sequent) is a pair (Φ, Ψ) ∈P(LΣ(P)) × P(LΣ(P)),
where Φ is dubbed the antecedent and Ψ the succedent. A one-dimensional con-

644
V. Greati and J. Marcos
sequence relation on LΣ(P) is a collection ▷of Set-Set statements satisfying,
for all Φ, Ψ, Φ′, Ψ ′ ⊆LΣ(P),
(O) if Φ ∩Ψ ̸= ∅, then Φ ▷Ψ
(D) if Φ ▷Ψ, then Φ ∪Φ′ ▷Ψ ∪Ψ ′
(C) if Π ∪Φ ▷Ψ ∪Πc for all Π ⊆LΣ(P), then Φ ▷Ψ
Properties (O), (D) and (C) are called overlap, dilution and cut, respectively.
The relation ▷is called substitution-invariant when it satisﬁes, for every σ ∈
SubsΣ,
(S) if Φ ▷Ψ, then σ[Φ] ▷σ[Ψ]
and it is called ﬁnitary when it satisﬁes
(F) if Φ ▷Ψ, then Φf ▷Ψ f for some ﬁnite Φf ⊆Φ and Ψ f ⊆Ψ
One-dimensional consequence relations will also be referred to as one-dimen-
sional logics. Substitution-invariant ﬁnitary one-dimensional logics will be called
standard. We will denote by ▶the complement of ▷, called the compatibility
relation associated with ▷[10].
A Set-Fmla statement is a sequent having a single formula as consequent.
When we restrict standard consequence relations to collections of Set-Fmla
statements, we deﬁne the so-called (substitution-invariant ﬁnitary) Tarskian con-
sequence relations. Every one-dimensional consequence relation ▷determines a
Tarskian consequence relation ▷⊆P(LΣ(P))×LΣ(P), dubbed the Set-Fmla
Tarskian companion of ▷, such that, for all Φ ∪{ψ} ⊆LΣ(P), Φ
▷ψ if,
and only if, Φ ▷{ψ}. It is well-known that the collection of all Tarskian con-
sequence relations over a ﬁxed language constitutes a complete lattice under
set-theoretical inclusion [25]. Given a set C of such relations, we will denote by
 C its supremum in the latter lattice.
We present in what follows two ways of obtaining one-dimensional conse-
quence relations: one semantical, via non-deterministic logical matrices [6], and
the other proof-theoretical, via Set-Set Hilbert-style systems [18,23].
A non-deterministic Σ-matrix, or simply Σ-nd-matrix, is a structure M :=
⟨A, D⟩, where A is a Σ-nd-algebra, whose carrier is the set of truth-values, and
D ⊆A is the set of designated truth-values. Such structures are also known in
the literature as ‘PNmatrices’ [7]; they generalize the so-called ‘Nmatrices’ [5],
which are Σ-nd-matrices with the restriction that A must be total. From now on,
whenever X ⊆A, we denote A\X by X. In case A is deterministic, we simply
say that M is a Σ-matrix. Also, M is said to be ﬁnite when A is ﬁnite. Every Σ-
nd-matrix M determines a substitution-invariant one-dimensional consequence
relation over Σ, denoted by ▷M, such that Φ ▷M Ψ if, and only if, for all v ∈
ValΣ(A), v[Φ] ∩D ̸= ∅or v[Ψ] ∩D ̸= ∅. It is worth noting that ▷M is ﬁnitary
whenever the carrier of A is ﬁnite (the proof runs very similar to that of the
same result for Nmatrices [5, Theorem 3.15]).
A strong homomorphism between Σ-matrices M1 := ⟨A1, D1⟩and M2 :=
⟨A2, D2⟩is a homomorphism h between A1 and A2 such that x ∈D1 if, and

Finite Two-Dimensional Proof Systems
645
only if, h(x) ∈D2. When there is a surjective strong homomorphism between
M1 and M2, we have that ▷M1 = ▷M2.
Now, to the Hilbert-style systems. A (schematic) Set-Set rule of infer-
ence Rs is the collection of all substitution instances of the Set-Set statement
s, called the schema of Rs. Each r ∈Rs is called a rule instance of Rs. A
(schematic) Set-Set H-system R is a collection of Set-Set rules of inference.
When we constrain the rule instances of R to having only singletons as succe-
dents, we obtain the conventional notion of Hilbert-style system, called here
Set-Fmla H-system.
An R-derivation in a Set-Set H-system R is a rooted directed tree t such
that every node is labelled with sets of formulas or with a discontinuation sym-
bol ∗, and in which every non-leaf node (that is, a node with child nodes) n in t
is an expansion of n by a rule instance r of R. This means that the antecedent
of r is contained in the label of n and that n has exactly one child node for
each formula ψ in the succedent of r. These child nodes are, in turn, labelled
with the same formulas as those of n plus the respective formula ψ. In case r
has an empty succedent, then n has a single child node labelled with ∗. Here we
will consider only ﬁnitary Set-Set H-systems, in which each rule instance has
ﬁnite antecedent and succedent. In such cases, we only need to consider ﬁnite
derivations. Figure 1 illustrates how derivations using only ﬁnitary rules of infer-
ence may be graphically represented. We denote by ℓt(n) the label of the node
n in the tree t. It is worth observing that, for Set-Fmla H-systems, derivations
are linear trees (as rule instances have a single formula in their succedents),
or, in other words, just sequences of formulas built by applications of the rule
instances, matching thus the conventional deﬁnition of Hilbert-style systems.
Φ
∗
Γ
∅
Φ
Φ, ψn
. . .
Φ, ψ2
Φ, ψ1
Γ
ψ1,ψ2,...,ψn
Fig. 1. Graphical representation of R-derivations, for R ﬁnitary. The dashed edges and
blank circles represent other branches that may exist in the derivation. We usually
omit the formulas inherited from the parent node, exhibiting only the ones introduced
by the applied rule of inference. In both cases, we must have Γ ⊆Φ to enable the
application of the rule.
A node n of an R-derivation t is called Δ-closed in case it is a leaf node with
ℓt(n) = ∗or ℓt(n)∩Δ ̸= ∅. A branch of t is Δ-closed when it ends in a Δ-closed
node. When every branch in t is Δ-closed, we say that R is itself Δ-closed. An
R-proof of a Set-Set statement (Φ, Ψ) is a Ψ-closed R-derivation t such that
ℓt(rt(t)) ⊆Φ.

646
V. Greati and J. Marcos
Consider the binary relation ▷R on P(LΣ(P)) such that Φ▷RΨ if, and only if,
there is an R-proof of (Φ, Ψ). This relation is the smallest substitution-invariant
one-dimensional consequence relation containing the rules of inference of R, and
it is ﬁnitary when R is ﬁnitary. Since Set-Set (and Set-Fmla) H-systems
canonically induce one-dimensional consequence relations, we may refer to them
as one-dimensional H-systems or one-dimensional axiomatizations. In case there
is a proof of (Φ, Ψ) whose nodes are labelled only with subsets of subfΘ[Φ ∪Ψ],
we write Φ ▷Θ
R Ψ . In case ▷R = ▷Θ
R , we say that R is Θ-analytic. Note that the
ordinary notion of analyticity obtains when Θ = {p}. From now on, whenever
we use the word “analytic” we will mean this extended notion of Θ-analyticity,
for some Θ implicit in the context. When the Θ happens to be important for us
or we identify any risk of confusion, we will mention it explicitly.
In [13], based on the seminal results on axiomatizability via Set-Set H-
systems by Shoesmith and Smiley [23], it was proved that any non-deterministic
logical matrix M satisfying a criterion of suﬃcient expressiveness is axiomatiz-
able by a Θ-analytic Set-Set Hilbert-style system, which is ﬁnite whenever M is
ﬁnite, where Θ is the set of separators for the pairs of truth-values of M. Accord-
ing to such criterion, an nd-matrix is suﬃciently expressive when, for every pair
(x, y) of distinct truth-values, there is a unary formula S, called a separator for
(x, y), such that SA(x) ⊆D and SA(y) ⊆D, or vice-versa; in other words, when
every pair of distinct truth-values is separable in M.
We emphasize that it is essential for the above result the adoption of Set-
Set H-systems, instead of the more restricted Set-Fmla H-systems. In fact,
while two-valued matrices may always be ﬁnitely axiomatized by Set-Fmla H-
systems [22], there are suﬃciently expressive three-valued deterministic matrices
[21] and even quite simple two-valued non-deterministic matrices [19] that fail to
be ﬁnitely axiomatized by Set-Fmla H-systems. When the nd-matrix at hand is
not suﬃciently expressive, we may observe the same phenomenon of not having
a ﬁnite axiomatization also in terms of Set-Set H-systems, even if the said nd-
matrix is ﬁnite. The ﬁrst example (and, to the best of our knowledge, the only
one in the current literature) of this fact appeared in [13], which we reproduce
here for later reference:
Example 1. Consider the signature Σ := {Σk}k∈ω such that Σ1 := {g, h} and
Σk := ∅for all k ̸= 1. Let M := ⟨A, {a}⟩be a Σ-nd-matrix, with A := {a, b, c}
and
gA(x) =

{a},
if x = c
A,
otherwise
hA(x) =

{b},
if x = b
A,
otherwise
This matrix is not suﬃciently expressive because there is no separator for the
pair (b, c), and [13] proved that it is not axiomatizable by a ﬁnite Set-Set
H-system, even though an inﬁnite Set-Set system that captures it has a quite
simple description in terms of the following inﬁnite collection of schemas:
hi(p)
p, g(p), for all i ∈ω.

Finite Two-Dimensional Proof Systems
647
In the next section, we reveal another example of this same phenomenon, this
time of the known LFI [14] called mCi. In the path of proving that this logic
is not axiomatizable by a ﬁnite Set-Set H-system, we will show that there are
inﬁnitely many LFIs between mbC and mCi, organized in a strictly increasing
chain whose limit is mCi itself.
Before continuing, it is worth emphasizing that any given non-suﬃciently
expressive nd-matrix may be conservatively extended to a suﬃciently expressive
nd-matrix provided new connectives are added to the language [18]. These new
connectives have the sole purpose of separating the pairs of truth-values for which
no separator is available in the original language. The Set-Set system produced
from this extended nd-matrix can, then, be used to reason over the original
logic, since the extension is conservative. However, these new connectives, which
a priori have no meaning, are very likely to appear in derivations of consecutions
of the original logic. This might not look like an attractive option to inferentialists
who believe that purity of the schematic rules governing a given logical constant
is essential for the meaning of the latter to be coherently ﬁxed. In the subsequent
sections, we will introduce and apply a potentially more expressive notion of logic
in order to provide a ﬁnite and analytic H-system for logics that are not ﬁnitely
axiomatizable in one dimension, while preserving their original languages.
4
The Logic mCi is Not Finitely Axiomatizable
A one-dimensional logic ▷over Σ is said to be ¬-paraconsistent when we have
p, ¬p
▶
q, for p, q ∈P. Moreover, ▷is ¬-gently explosive in case there is a
collection ⃝(p) ⊆LΣ(P) of unary formulas such that, for some ϕ ∈LΣ(P), we
have ⃝(ϕ), ϕ ▶∅; ⃝(ϕ), ¬ϕ ▶∅, and, for all ϕ ∈LΣ(P), ⃝(ϕ), ϕ, ¬ϕ ▷∅. We
say that ▷is a logic of formal inconsistency (LFI) in case it is ¬-paraconsistent
yet ¬-gently explosive. In case ⃝(p) = {◦p}, for ◦a (primitive or composite)
consistency connective, the logic is said also to be a C-system. In what follows,
let Σ◦be the propositional signature such that Σ◦
1 := {¬, ◦}, Σ◦
2 := {∧, ∨, ⊃},
and Σ◦
k := ∅for all k ̸∈{1, 2}.
One of the simplest C-systems is the logic mbC, which was ﬁrst presented in
terms of a Set-Fmla H-system over Σ◦obtained by extending any Set-Fmla
H-system for positive classical logic (CPL+) with the following pair of axiom
schemas:
(em) p ∨¬p
(bc1) ◦p ⊃(p ⊃(¬p ⊃q))
The logic mCi, in turn, is the C-system resulting from extending the H-
system for mbC with the following (inﬁnitely many) axiom schemas [20] (the
resulting Set-Fmla H-system is denoted here by HmCi):
(ci) ¬◦p ⊃(p ∧¬p)
(ci)j ◦¬j◦p (for all 0 ≤j < ω)

648
V. Greati and J. Marcos
A unary connective
c⃝is said to constitute a classical negation in a one-
dimensional logic ▷extending CPL+ in case, for all ϕ, ψ ∈LΣ(P), ∅▷ϕ∨c⃝(ϕ)
and ∅▷ϕ ⊃( c⃝(ϕ) ⊃ψ). One of the main diﬀerences between mCi and mbC is
that an inconsistency connective • may be deﬁned in the former using the para-
consistent negation, instead of a classical negation, by setting •ϕ := ¬◦ϕ [20].
Both logics above were presented in [15] in ways other than H-systems: via
tableau systems, via bivaluation semantics and via possible-translations seman-
tics. In addition, while these logics are known not to be characterizable by a
single ﬁnite deterministic matrix [20], a characteristic nd-matrix is available for
mbC [1] and a 5-valued non-deterministic logical matrix is available for mCi [2],
witnessing the importance of non-deterministic semantics in the study of non-
classical logics. Such characterizations, moreover, allow for the extraction of
sequent-style systems for these logics by the methodologies developed in [3,4].
Since mCi’s 5-valued nd-matrix will be useful for us in future sections, we recall
it below for ease of reference.
Deﬁnition 1. Let V5 := {f, F, I, T, t} and Y5 := {I, T, t}. Deﬁne the Σ◦-matrix
MmCi := ⟨A5, Y5⟩such that A5 := ⟨V5, ·A5⟩interprets the connectives of Σ◦
according to the following:
∧A5(x1, x2) :=

{f}
if either x1 ̸∈Y5 or x2 ̸∈Y5
{I, t}
otherwise
∨A5(x1, x2) :=

{I, t}
if either x1 ∈Y5 or x2 ∈Y5
{f}
if x1,x2 ̸∈Y5
⊃A5(x1, x2) :=

{I, t}
if either x1 ̸∈Y5 or x2 ∈Y5
{f}
if x1 ∈Y5 and x2 ̸∈Y5
f
F
I
T
t
¬A5 {I,t} {T} {I,t} {F} {f}
f
F
I
T
t
◦A5 {T} {T} {F} {T} {T}
One might be tempted to apply the axiomatization algorithm of [13] to the
ﬁnite non-deterministic logical matrix deﬁned above to obtain a ﬁnite and ana-
lytic Set-Set system for mCi. However, it is not obvious, at ﬁrst, whether this
matrix is suﬃciently expressive or not (we will, in fact, prove that it is not).
In what follows, we will show now mCi is actually axiomatizable neither by a
ﬁnite Set-Fmla H-system (ﬁrst part), nor by a ﬁnite Set-Set H-system (sec-
ond part); it so happens, thus, that it was not by chance that HmCi has been
originally presented with inﬁnitely many rule schemas. For the ﬁrst part, we rely
on the following general result:
Theorem 1 ([25], Theorem 2.2.8, adapted). Let
be a standard Tarskian
consequence relation. Then
is axiomatizable by a ﬁnite Set-Fmla H-system
if, and only if, there is no strictly increasing sequence
0 , 1 , . . . , n , . . . of stan-
dard Tarskian consequence relations such that
= 
i∈ω
i .

Finite Two-Dimensional Proof Systems
649
In order to apply the above theorem, we ﬁrst present a family of ﬁnite Set-Fmla
H-systems that, in the sequel, will be used to provide an increasing sequence
of standard Tarskian consequence relations whose supremum is precisely mCi.
Next, we show that this sequence is stricly increasing, by employing the matrix
methodology traditionally used for showing the independence of axioms in a
proof system.
Deﬁnition 2. For each k ∈ω, let Hk
mCi be a Set-Fmla H-system for positive
classical logic together with the schemas (em), (bc1), (ci) and (ci)j, for all 0 ≤
j ≤k.
Since Hk
mCi may be obtained from HmCi by deleting some (inﬁnitely many)
axioms, it is immediate that:
Proposition 1. For every k ∈ω,
Hk
mCi ⊆mCi .
The way we deﬁne the promised increasing sequence of consequence relations
in the next result is by taking the systems Hk
mCi with odd superscripts, namely,
we will be working with the sequence H1
mCi , H3
mCi , H5
mCi , . . . Excluding the cases
where k is even will facilitate, in particular, the proof of Lemma 3.
Lemma 1. For each 1 ≤k < ω, let
k := H2k−1
mCi . Then
1 ⊆2 ⊆. . ., and
mCi =

1≤k<ω
k .
Finally, we prove that the sequence outlined in the paragraph before Lemma 1
is strictly increasing. In order to achieve this, we deﬁne, for each 1 ≤k < ω, a
Σ◦-matrix Mk and prove that H2k−1
mCi is sound with respect to such matrix. Then,
in the second part of the proof (the “independence part”), we show that, for each
1 ≤k < ω, Mk fails to validate the rule schema (ci)j, for j = 2k, which is present
in H2(k+1)−1
mCi
. In this way, by the contrapositive of the soundness result proved
in the ﬁrst part, we will have (ci)j provable in H2(k+1)−1
mCi
while unprovable in
H2k−1
mCi . In what follows, for any k ∈ω, we use k∗to refer to the successor of k.
Deﬁnition 3. Let 1 ≤k < ω. Deﬁne the 2k∗-valued Σ◦-matrix Mk := ⟨Ak, Dk⟩
such that Dk := {k∗+ 1, . . . , 2k∗} and Ak := ⟨{1, . . . , 2k∗}, ·Ak⟩, the interpreta-
tion of Σ◦in Ak given by the following operations:
x∨Aky :=

1
if x, y ∈Dk
k∗+ 1
otherwise
x∧Aky :=

k∗+ 1
if x, y ∈Dk
1
otherwise
x ⊃Aky :=

1
if x ∈Dk and y ̸∈Dk
k∗+ 1
otherwise
◦Akx :=

1
if x = 2k∗
k∗+ 1
otherwise
¬Akx :=
⎧
⎪
⎨
⎪
⎩
k∗+ 1
if x ∈{1, 2k∗}
x + k∗
if 2 ≤x ≤k∗
x −(k∗−1)
if k∗+ 1 ≤x ≤2k∗−1

650
V. Greati and J. Marcos
Before continuing, we state results concerning this construction, which will be
used in the remainder of the current line of argumentation. In what follows, when
there is no risk of confusion, we omit the subscript ‘Ak’ from the interpretations
to simplify the notation.
Lemma 2. For all k ≥1 and 1 ≤m ≤2k,
¬m
Ak(k∗+ 1) =

(k∗+ 1) + m
2 ,
if m is even
1 + m+1
2 ,
otherwise
Lemma 3. For all 1 ≤k < ω, we have
H2k∗−1
mCi
◦¬2k◦p but ̸ H2k−1
mCi
◦¬2k◦p.
Finally, Theorem 1, Lemma 1 and Lemma 3 give us the main result:
Theorem 2. mCi is not axiomatizable by a ﬁnite Set-Fmla H-system.
For the second part —namely, that no ﬁnite Set-Set H-system axiomatizes
mCi—, we make use of the following result:
Theorem 3 ([23], Theorem 5.37, adapted).
Let ▷be a one-dimensional
consequence relation over a propositional signature containing the binary con-
nective ∨. Suppose that the Set-Fmla Tarskian companion of ▷, denoted by
▷, satisﬁes the following property:
Φ, ϕ ∨ψ
▷γ if, and only if, Φ, ϕ ▷γ and Φ, ψ
▷γ
(Disj)
If a Set-Set H-system R axiomatizes ▷, then R may be converted into a Set-
Fmla H-system for
▷that is ﬁnite whenever R is ﬁnite.
It turns out that:
Lemma 4. mCi satisﬁes (Disj).
Proof. The non-deterministic semantics of mCi gives us that, for all ϕ, ψ ∈
LΣ◦(P), ϕ ▷MmCi ϕ ∨ψ; ψ ▷MmCi ϕ ∨ψ, and ϕ ∨ψ ▷MmCi ϕ, ψ, and such facts
easily imply (Disj).
Theorem 4. mCi is not axiomatizable by a ﬁnite Set-Set H-system.
Proof. If R were a ﬁnite Set-Set H-system for mCi, then, by Lemma 4 and
Theorem 3, it could be turned into a ﬁnite Set-Fmla H-system for this very
logic. This would contradict Theorem 2.
Finding a ﬁnite one-dimensional H-system for mCi (analytic or not) over the
same language, then, proved to be impossible. The previous result also tells us
that there is no suﬃciently expressive non-deterministic matrix that character-
izes mCi (for otherwise the recipe in [13] would deliver a ﬁnite analytic Set-Set
H-system for it), and we may conclude, in particular, that:

Finite Two-Dimensional Proof Systems
651
Corollary 1. The nd-matrix MmCi is not suﬃciently expressive.
The pairs of truth-values of MmCi that seem not to be separable (at least
one of these pairs must not be, in view of the above corollary) are (t, T) and
(f, F). The insuﬃciency of expressive power to take these speciﬁc pairs of values
apart, however, would be circumvented if we had considered instead the matrix
deﬁned below, obtained from MmCi by changing its set of designated values:
Deﬁnition 4. Let Mn
mCi := ⟨A5, N5⟩, where N5 := {f, I, T}.
Note that, in Mn
mCi, we have t ̸∈N5, while T ∈N5, and we have that f ∈N5,
while F ̸∈N5. Therefore, the single propositional variable p separates in Mn
mCi
the pairs (t, T) and (f, F). On the other hand, it is not clear now whether the
pairs (t, F) and (f, T) are separable in this new matrix. Nonetheless, we will
see, in the next section, how we can take advantage of the semantics of non-
deterministic B-matrices in order to combine the expressiveness of MmCi and
Mn
mCi in a very simple and intuitive manner, preserving the language and the
algebra shared by these matrices. The notion of logic induced by the resulting
structure will not be one-dimensional, as the one presented before, but rather
two-dimensional, in a sense we shall detail in a moment. We identify two impor-
tant aspects of this combination: ﬁrst, the logics determined by the original
matrices can be fully recovered from the combined logic; and, second, since the
notions of H-systems and suﬃcient expressiveness, as well as the axiomatization
algorithm of [13], were generalized in [17], the resulting two-dimensional logic
may be algorithmically axiomatized by an analytic two-dimensional H-system
that is ﬁnite if the combining matrices are ﬁnite, provided the criterion of suﬃ-
cient expressiveness is satisﬁed after the combination. This will be the case, in
particular, when we combine MmCi and Mn
mCi. Consequently, this novel way of
combining logics provides a quite general approach for producing ﬁnite and ana-
lytic axiomatizations for logics determined by non-deterministic logical matrices
that fail to be ﬁnitely axiomatizable in one dimension; this includes the logics
from Example 1, and also mCi.
5
Two-Dimensional Logics
From now on, we will employ the symbols Y,
Y
, N and
N
to informally refer to,
respectively, the cognitive attitudes of acceptance, non-acceptance, rejection and
non-rejection, collected in the set Atts := {Y,
Y
, N,
N
}. Given a set Φ ⊆LΣ(P),
we will write Φα to intuitively mean that a given agent entertains the cognitive
attitude α ∈Atts with respect to the formulas in Φ, that is: the formulas in
ΦY will be understood as being accepted by the agent; the ones in Φ
Y
, as non-
accepted; the ones in ΦN, as rejected; and the ones in Φ
N
, as non-rejected. Where
α ∈Atts, we let ˜α be its ﬂipped version, that is, ˜Y :=
Y
, ˜
Y
:= Y, ˜N :=
N
and
˜
N
:= N.
We refer to each
as a B-statement, where
(ΦY, ΦN) is the antecedent and (Φ
Y
, Φ
N
) is the succedent. The sets in the latter

652
V. Greati and J. Marcos
pairs are called components. A B-consequence relation is a collection ·
·| ·
· of B-
statements satisfying:
(O2) if ΦY ∩Φ
Y
̸= ∅or ΦN ∩Φ
N
̸= ∅, then Φ
N
ΦY | Φ
Y
ΦN
(D2) if Ψ
N
ΨY | Ψ
Y
ΨN and Ψα ⊆Φα for every α ∈Atts, then Φ
N
ΦY | Φ
Y
ΦN
(C2) if Ωc
S
ΩS | Ωc
S
Ω
S
for all ΦY ⊆ΩS ⊆Φc
Y
and ΦN ⊆Ω
S
⊆Φc
N
, then Φ
N
ΦY | Φ
Y
ΦN
A B-consequence relation is called substitution-invariant if, in addition, Φ
N
ΦY | Φ
Y
ΦN
holds whenever, for every σ ∈SubsΣ:
(S2) Ψ
N
ΨY | Ψ
Y
ΨN and Φα = σ(Ψα) for every α ∈Atts
Moreover, a B-consequence relation is called ﬁnitary when it enjoys the property
(F2) if Φ
N
ΦY | Φ
Y
ΦN , then Φf
N
Φf
Y | Φf
Y
Φf
N , for some ﬁnite Φf
α ⊆Φα, and each α ∈Atts
In what follows, B-consequence relations will also be referred to as two-dimen-
sional logics. The complement of ·
·| ·
·, sometimes called the compatibility relation
associated with
·
·| ·
· [10], will be denoted by ·
·×| ·
·. Every B-consequence relation
C := ·
·| ·
· induces one-dimensional consequence relations ▷C
t and ▷C
f , such that
ΦY▷C
t Φ
Y
iﬀ∅
ΦY | Φ
Y
∅, and ΦN▷C
f Φ
N
iﬀΦ
N
∅| ∅
ΦN . Given a one-dimensional consequence
relation ▷, we say that it inhabits the t-aspect of C if ▷= ▷C
t , and that it
inhabits the f-aspect of C if ▷= ▷C
f . B-consequence relations actually induce
many other (even non-Tarskian) one-dimensional notions of logics; the reader is
referred to [9,11] for a thorough presentation on this topic.
As we did for one-dimensional consequence relations, we present now realiza-
tions of B-consequence relations, ﬁrst via the semantics of nd-B-matrices, then
by means of two-dimensional H-systems.
A non-deterministic B-matrix over Σ, or simply Σ-nd-B-matrix, is a struc-
ture M := ⟨A, Y, N⟩, where A is a Σ-nd-algebra, Y ⊆A is the set of designated
values and N ⊆A is the set of antidesignated values of M. For convenience, we
deﬁne
Y
:= A\Y to be the set of non-designated values, and
N
:= A\N to be
the set of non-antidesignated values of M. The elements of ValΣ(A) are dubbed
M-valuations. The B-entailment relation determined by M is a collection ·
·| ·
· M
of B-statements such that
(B-ent)
Φ
N
ΦY
|Φ
Y
ΦN
M
iff there is no M-valuation v such that
v(Φα) ⊆α for each α ∈Atts,
for every ΦY, ΦN, Φ
Y
, Φ
N
⊆LΣ(P). Whenever
Φ
N
ΦY | Φ
Y
ΦN M , we say that the B-
statement
holds in M or is valid in M. An M-valuation that bears
witness to Φ
N
ΦY×| Φ
Y
ΦN M is called a countermodel for
in M. One may eas-
ily check that ·
·| ·
· M is a substitution-invariant B-consequence relation, that is
ﬁnitary when A is ﬁnite. Taking C as ·
·| ·
· M , we deﬁne ▷M
t
:= ▷C
t and ▷M
f
:= ▷C
f .

Finite Two-Dimensional Proof Systems
653
ΦY
ΦN
∗
ΨY
ΨN
∅
∅
ΦY
ΦN
ΦY
n, ΦN
. . .
ΦY
1, ΦN
ΦY,γm
ΦN
. . .
ΦY,γ1
ΦN
ΨY
ΨN
γ1,...,γm
1,..., n
Fig. 2. Graphical representation of ﬁnite R-derivations. We emphasize that, in both
cases, we must have ΨY ⊆ΦY and ΨN ⊆ΦN to enable the application of the rule.
We move now to two-dimensional, or Set2-Set2, H-systems, ﬁrst introduced
in [17]. A (schematic) Set2-Set2 rule of inference Rs is the collection of all sub-
stitution instances of the Set2-Set2 statement s, called the schema of Rs. Each
r ∈Rs is said to be a rule instance of Rs. In a proof-theoretic context, rather
than writing the B-statement
, we shall denote the corresponding rule
by
ΦY ∥ΦN
Φ
Y
∥Φ
N
. A (schematic) Set2-Set2 H-system R is a collection of Set2-Set2
rules of inference. Set2-Set2 derivations are as in the Set-Set H-systems, but
now the nodes are labelled with pairs of sets of formulas, instead of a single set.
When applying a rule instance, each formula in the succedent produces a new
branch as before, but now the formula goes to the same component in which
it was found in the rule instance. See Fig. 2 for a general representation and
compare it with Fig. 1.
Let t be an R-derivation. A node n of t is (Ψ
Y
, Ψ
N
)-closed in case it is dis-
continued (namely, labelled with ∗) or it is a leaf node with ℓt(n) = (ΦY, ΦN)
and either ΦY ∩Ψ
Y
̸= ∅or ΦN ∩Ψ
N
̸= ∅. A branch of t is (Ψ
Y
, Ψ
N
)-closed
when it ends in a (Ψ
Y
, Ψ
N
)-closed node. An R-derivation t is said to be (Ψ
Y
, Ψ
N
)-
closed when all of its branches are (Ψ
Y
, Ψ
N
)-closed. An R-proof of
is a
(Φ
Y
, Φ
N
)-closed R-derivation t with ℓt(rt(t)) ⊆(ΦY, ΦN). The deﬁnitions of the
(ﬁnitary) substitution-invariant B-consequence relation ·
·| ·
· R induced by a (ﬁni-
tary) Set2-Set2 H-system R and Θ-analyticity are obvious generalizations of
the corresponding Set-Set deﬁnitions.
In [17], the notion of suﬃcient expressiveness was generalized to nd-B-
matrices. We reproduce here the main deﬁnitions for self-containment:
Deﬁnition 5. Let M := ⟨A, Y, N⟩be a Σ-nd-B-matrix.
– Given X, Y ⊆A and α ∈{Y, N}, we say that X and Y are α-separated,
denoted by X#αY , if X ⊆α and Y ⊆˜α, or vice-versa.
– Given distinct truth-values x, y ∈A, a unary formula S is a separator for
(x, y) whenever SA(x)#αSA(y) for some α ∈{Y, N}. If there is a separator
for each pair of distinct truth-values in A, then M is said to be suﬃciently
expressive.
In the same work [17], the axiomatization algorithm of [13] was also general-
ized, guaranteeing that every suﬃciently expressive nd-B-matrix M is axiomati-

654
V. Greati and J. Marcos
zable by a Θ-analytic Set2-Set2 H-system, which is ﬁnite whenever M is ﬁnite,
where Θ is a set of separators for the pairs of truth-values of M. Note that, in
the second bullet of the above deﬁnition, a unary formula is characterized as
a separator whenever it separates a pair of truth-values according to at least
one of the distinguished sets of values. This means that having two of such sets
may allow us to separate more pairs of truth-values than having a single set,
that is, the nd-B-matrices are, in this sense, potentially more expressive than
the (one-dimensional) logical matrices.
Example 2. Let A be the Σ-nd-algebra from Example 1, and consider the nd-
B-matrix M := ⟨A, {a}, {b}⟩. As we know, in this matrix the pair (b, c) is not
separable if we consider only the set of designated values {a}. However, as we
have now the set {b} of antidesignated truth-values, the separation becomes evi-
dent: the propositional variable p is a separator for this pair now, since b ∈{b}
and c ̸∈{b}. The recipe from [17] produces the following Set2-Set2 axiomati-
zation for M, with only three very simple schematic rules of inference:
p ∥p
∥
∥
f(p), p ∥p
∥
p
∥t(p)
By construction, the one-dimensional logic determined by the nd-matrix of
Example 1 inhabits the t-aspect of ·
·| ·
· M , thus it can be seen as being axiom-
atized by this ﬁnite and analytic two-dimensional system (contrast with the
inﬁnite Set-Set axiomatization known for this logic provided in that same
example).
We constructed above a Σ-nd-B-matrix from two Σ-nd-matrices in such a
way that the one-dimensional logics determined by latter are fully recoverable
from the former. We formalize this construction below:
Deﬁnition 6. Let M := ⟨A, D⟩and M′ := ⟨A, D′⟩be Σ-nd-matrices. The B-
product between M and M′ is the Σ-nd-B-matrix M ⊙M′ := ⟨A, D, D′⟩.
Note that Φ ▷M Ψ iff
Φ | Ψ M⊙M′ iff Φ ▷M⊙M′
t
Ψ, and Φ ▷M′ Ψ iff
Ψ | Φ M⊙M′
iff Φ ▷M⊙M′
f
Ψ. Therefore, ▷M and ▷M′ are easily recoverable from ·
·| ·
· M⊙M′ ,
since they inhabit, respectively, the t-aspect and the f-aspect of the latter. One
of the applications of this novel way of putting two distinct logics together
was illustrated in that same Example 2 to produce a two-dimensional analytic
and ﬁnite axiomatization for a one-dimensional logic characterized by a Σ-nd-
matrix. As we have shown, the latter one-dimensional logic does not need to be
ﬁnitely axiomatizable by a Set-Set H-system. We present this application of
B-products with more generality below:
Proposition 2. Let M := ⟨A, D⟩be a Σ-nd-matrix and suppose that U ⊆A×A
contains all and only the pairs of distinct truth-values that fail to be separable in
M. If, for some M′ := ⟨A, D′⟩, the pairs in U are separable in M′, then M⊙M′ is
suﬃciently expressive (thus, axiomatizable by an analytic Set2-Set2 H-system,
that is ﬁnite whenever A is ﬁnite).

Finite Two-Dimensional Proof Systems
655
6
A Finite and Analytic Proof System for mCi
In the spirit of Proposition 2, we deﬁne below a nd-B-matrix by combining
the matrices MmCi := ⟨A5, Y5⟩and Mn
mCi := ⟨A5, N5⟩introduced in Sect. 4
(Deﬁnition 1 and Deﬁnition 4):
Deﬁnition 7. Let MmCi := MmCi ⊙Mn
mCi = ⟨A5, Y5, N5⟩, with Y5 := {I, T, t}
and N5 := {f, I, T}.
When we consider now both sets Y5 and N5 of designated and antidesignated
truth-values, the separation of all truth-values of A5 becomes possible, that is,
MmCi is suﬃciently expressive, as guaranteed by Proposition 2. Furthermore,
notice that we have two alternatives for separating the pairs (I, t) and (I, T):
either using the formula ¬p or the formula ◦p. With this ﬁnite suﬃciently expres-
sive nd-B-matrix in hand, producing a ﬁnite {p, ◦p}-analytic two-dimensional H-
system for it is immediate by [17, Theorem 2]. Since mCi inhabits the t-aspect
of ·
·| ·
· MmCi , we may then conclude that:
Theorem 5. mCi is axiomatizable by a ﬁnite and analytic two-dimensional
H-system.
Our axiomatization recipe delivers an H-system with about 300 rule schemas.
When we simplify it using the streamlining procedures indicated in that paper,
we obtain a much more succinct and insightful presentation, with 28 rule
schemas, which we call RmCi. The full presentation of this system is given below:
q
∥
p ⊃q ∥⊃mCi
1
∥
p, p ⊃q ∥⊃mCi
2
p ⊃q, p ∥
q
∥⊃mCi
3
p ∥
q ∥p ⊃q ⊃mCi
4
p ⊃q, ◦(p ⊃q) ∥p ⊃q
∥
⊃mCi
5
p, q
∥
p ∧q ∥∧mCi
1
p ∧q ∥
p
∥∧mCi
2
p ∧q ∥
q
∥∧mCi
3
∥
p ∧q ∥p ∧q ∧mCi
4
p∧q, ◦(p∧q) ∥p∧q
∥
∧mCi
5
p
∥
p ∨q ∥∨mCi
1
q
∥
p ∨q ∥∨mCi
2
p ∨q ∥
p, q
∥∨mCi
3
∥
p, q ∥p ∨q ∨mCi
4
p∨q, ◦(p∨q) ∥p∨q
∥
∨mCi
5
◦p ∥
∥◦p ◦mCi
1
∥
◦◦p ∥◦mCi
2
∥◦p
◦p ∥
◦mCi
3
∥
◦p ∥p ◦mCi
4
∥
p ∥◦p ◦mCi
5
∥
∥¬p, p ¬mCi
1
¬p, ◦p, p ∥
∥¬mCi
2
¬p, p ∥
∥p ¬mCi
3
◦¬p ∥¬p, p
∥
¬mCi
4
∥¬p, p
¬p ∥
¬mCi
5
∥
¬p, ◦p ∥¬mCi
6
∥
¬p, p ∥¬mCi
7
∥
◦¬p ∥p ¬mCi
8
Note that the set of rules { c⃝mCi
i
| c⃝∈{∧, ∨, ⊃}, i ∈{1, 2, 3}} makes it
clear that the t-aspect of the induced B-consequence relation is inhabited by a
logic extending positive classical logic, while the remaining rules for these con-
nectives involve interactions between the two dimensions. Also, rule ¬mCi
2
indi-
cates that ◦satisﬁes one of the main conditions for being taken as a consistency
connective in the logic inhabiting the t-aspect. In fact, all these observations
are aligned with the fact that the logic inhabiting the t-aspect of ·
·| ·
· RmCi is
precisely mCi. See, in Fig. 3, RmCi-derivations showing that, in mCi, ¬◦p and
p∧¬p are logically equivalent and that ◦¬◦p is a theorem.

656
V. Greati and J. Marcos
p ∧¬p
p
¬p
◦p
∗
¬mCi
2
¬◦p
¬mCi
7
∧mCi
3
∧mCi
2
¬◦p
◦p
◦◦p
∗
¬mCi
2
◦mCi
2
¬p
p
◦p
◦◦p
∗
¬mCi
2
◦mCi
2
◦mCi
3
p
p ∧¬p
∧mCi
1
◦mCi
5
¬mCi
6
∅
∅
¬◦p
p
¬◦p
◦p
◦◦p
¬mCi
2
◦mCi
2
◦mCi
3
¬mCi
5
◦¬◦p
¬mCi
8
◦¬◦p
◦mCi
4
Fig. 3. RmCi-derivations showing, respectively, that
∅
p∧¬p| ¬◦p
∅
RmCi ,
∅
¬◦p| p∧¬p
∅
RmCi
and ∅
∅| ◦¬◦p
∅
RmCi . Note that, for a cleaner presentation, we omit the formulas inherited
from parent nodes.
7
Concluding Remarks
In this work, we introduced a mechanism for combining two non-deterministic
logical matrices into a non-deterministic B-matrix, creating the possibility of pro-
ducing ﬁnite and analytic two-dimensional axiomatizations for one-dimensional
logics that may fail to be ﬁnitely axiomatizable in terms of one-dimensional
Hilbert-style systems. It is worth mentioning that, as proved in [17], one may
perform proof search and countermodel search over the resulting two-dimensional
systems in time at most exponential on the size of the B-statement of interest
through a straightforward proof-search algorithm.
We illustrated the above-mentioned combination mechanism with two exam-
ples, one of them corresponding to a well-known logic of formal inconsistency
called mCi. We ended up proving not only that this logic is not ﬁnitely axiom-
atizable in one dimension, but also that it is the limit of a strictly increasing
chain of LFIs extending the logic mbC. From the perspective of the study of B-
consequence relations, these examples allow us to eliminate the suspicion that a
two-dimensional H-system R may always be converted into Set-Set H-systems
for the logics inhabiting the one-dimensional aspects of ·
·| ·
· R without losing any
desirable property (in this case, ﬁniteness of the presentation).
At ﬁrst sight, the formalism of two-dimensional H-systems may be confused
with the formalism of n-sided sequents [3,4], in which the objects manipulated
by rules of inference (the so-called n-sequents) accommodate more than two sets
of formulas in their structures. The reader interested in a comparison between
these two diﬀerent approaches is referred to the concluding remarks of [17].
We close with some observations regarding MmCi and the two-dimensional
H-system RmCi. A one-dimensional logic ▷is said to be ¬-consistent when

Finite Two-Dimensional Proof Systems
657
ϕ, ¬ϕ▷∅and ¬-determined when ∅▷ϕ, ¬ϕ for all ϕ ∈LΣ(P). A B-consequence
relation ·
·| ·
· is said to allow for gappy reasoning when ϕ×| ϕ and to allow for glutty
reasoning when ϕ×| ϕ, for some ϕ ∈LΣ(P). Notice that ¬-determinedness in the
logic inhabiting the t-aspect of a B-consequence relation by no means implies
the disallowance of gappy reasoning in the two-dimensional setting: we still have
F ∈Y5∩N5, so one may both non-accept and non-reject a formula ϕ in ·
·| ·
· RmCi ,
even though non-accepting both ϕ and its negation in mCi is not possible, in
view of rule ¬mCi
7
. Similarly, the recovery of ¬-consistency achieved via ◦in
such logic does not coincide with the gentle disallowance of glutty reasoning in
·
·| ·
· RmCi , that is, we do not have, in general, p,◦p| p RmCi or p | ◦p,p RmCi , even
though for binary compounds both are derivable in view of rules c⃝mCi
5
, for
c⃝∈{∧, ∨, ⊃}, and ◦mCi
1
. With these observations we hope to call attention to
the fact that B-consequence relations open the doors for further developments
concerning the study of paraconsistency (and, dually, of paracompleteness), as
well as the study of recovery operators [8].
References
1. Avron, A.: Non-deterministic matrices and modular semantics of rules. In: Beziau,
J.Y. (ed.) Logica Universalis, pp. 149–167. Birkh¨auser, Basel (2005). https://doi.
org/10.1007/3-7643-7304-0 9
2. Avron, A.: 5-valued non-deterministic semantics for the basic paraconsistent logic
mCi. Stud. Log. Grammar Rhetoric 127–136 (2008)
3. Avron, A., Ben-Naim, J., Konikowska, B.: Cut-free ordinary sequent calculi for
logics having generalized ﬁnite-valued semantics. Log. Univers. 1, 41–70 (2007).
https://doi.org/10.1007/s11787-006-0003-6
4. Avron, A., Konikowska, B.: Multi-valued calculi for logics based on non-
determinism. Log. J. IGPL 13(4), 365–387 (2005). https://doi.org/10.1093/jigpal/
jzi030
5. Avron, A., Lev, I.: Non-deterministic multiple-valued structures. J. Log. Comput.
15(3), 241–261 (2005). https://doi.org/10.1093/logcom/exi001
6. Avron, A., Zamansky, A.: Non-deterministic semantics for logical systems. In: Gab-
bay, D.M., Guenthner, F. (eds.) Handbook of Philosophical Logic, vol. 16, pp. 227–
304. Springer, Dordrecht (2011). https://doi.org/10.1007/978-94-007-0479-4 4
7. Baaz, M., Lahav, O., Zamansky, A.: Finite-valued semantics for canonical labelled
calculi. J. Autom. Reason. 51(4), 401–430 (2013). https://doi.org/10.1007/s10817-
013-9273-x
8. Barrio, E.A., Carnielli, W.: Volume I: recovery operators in logics of formal incon-
sistency (special issue). Log. J. IGPL 28(5), 615–623 (2019). https://doi.org/10.
1093/jigpal/jzy053
9. Blasio, C.: Revisitando a l´ogica de Dunn-Belnap. Manuscrito 40, 99–126 (2017).
https://doi.org/10.1590/0100-6045.2017.v40n2.cb
10. Blasio, C., Caleiro, C., Marcos, J.: What is a logical theory? On theories containing
assertions and denials. Synthese 198(22), 5481–5504 (2019). https://doi.org/10.
1007/s11229-019-02183-z
11. Blasio, C., Marcos, J., Wansing, H.: An inferentially many-valued two-dimensional
notion of entailment. Bull. Sect. Log. 46(3/4), 233–262 (2017). https://doi.org/10.
18778/0138-0680.46.3.4.05

658
V. Greati and J. Marcos
12. Burris, S., Sankappanavar, H.: A Course in Universal Algebra, vol. 91 (1981)
13. Caleiro, C., Marcelino, S.: Analytic calculi for monadic PNmatrices. In: Iemhoﬀ,
R., Moortgat, M., de Queiroz, R. (eds.) WoLLIC 2019. LNCS, vol. 11541, pp.
84–98. Springer, Heidelberg (2019). https://doi.org/10.1007/978-3-662-59533-6 6
14. Carnielli, W., Marcos, J.: A taxonomy of C-systems. In: Paraconsistency: The
logical way to the inconsistent. Taylor and Francis (2002). https://doi.org/10.1201/
9780203910139-3
15. Carnielli, W.A., Coniglio, M.E., Marcos, J.: Logics of formal inconsistency. In:
Gabbay, D., Guenthner, F. (eds.) Handbook of Philosophical Logic, vol. 14, 2nd
edn., pp. 1–93. Springer, Dordrecht (2007). https://doi.org/10.1007/978-1-4020-
6324-4 1
16. Carnielli, W.A., Marcos, J.: Limits for paraconsistent calculi. Notre Dame J. For-
mal Log. 40(3), 375–390 (1999). https://doi.org/10.1305/ndjﬂ/1022615617
17. Greati, V., Marcelino, S., Marcos, J.: Proof search on bilateralist judgments over
non-deterministic semantics. In: Das, A., Negri, S. (eds.) TABLEAUX 2021. LNCS
(LNAI), vol. 12842, pp. 129–146. Springer, Cham (2021). https://doi.org/10.1007/
978-3-030-86059-2 8
18. Marcelino, S., Caleiro, C.: Axiomatizing non-deterministic many-valued general-
ized consequence relations. Synthese 198(22), 5373–5390 (2019). https://doi.org/
10.1007/s11229-019-02142-8
19. Marcelino, S.: An unexpected Boolean connective. Log. Univer. (2021). https://
doi.org/10.1007/s11787-021-00280-7
20. Marcos, J.: Possible-translations semantics for some weak classically-based para-
consistent logics. J. Appl. Non-Classical Log. 18(1), 7–28 (2008). https://doi.org/
10.3166/jancl.18.7-28
21. Palasinska, K.: Deductive systems and ﬁnite axiomatization properties. Ph.D. the-
sis, Iowa State University (1994). https://doi.org/10.31274/rtd-180813-12680
22. Rautenberg, W.: 2-element matrices. Stud. Log. 40(4), 315–353 (1981)
23. Shoesmith, D.J., Smiley, T.J.: Multiple-Conclusion Logic. Cambridge University
Press, Cambridge (1978). https://doi.org/10.1017/CBO9780511565687
24. W´ojcicki, R.: Some remarks on the consequence operation in sentential logics.
Fundam. Math. 68, 269–279 (1970)
25. W´ojcicki, R.: Theory of Logical Calculi. Synthese Library, 1 edn., Springer, Dor-
drecht (1988). https://doi.org/10.1007/978-94-015-6942-2
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Vampire Getting Noisy: Will Random
Bits Help Conquer Chaos? (System
Description)
Martin Suda(B)
Czech Technical University in Prague, Prague, Czech Republic
martin.suda@cvut.cz
Abstract. Treating
a
saturation-based
automatic
theorem
prover
(ATP) as a Las Vegas randomized algorithm is a way to illuminate the
chaotic nature of proof search and make it amenable to study by prob-
abilistic tools. On a series of experiments with the ATP Vampire, the
paper showcases some implications of this perspective for prover evalua-
tion.
Keywords: Saturation-based proving · Evalutation · Randomization
1
Introduction
Saturation-based proof search is known to be fragile. Even seemingly insigniﬁcant
changes in the search procedure, such as shuﬄing the order in which input
formulas are presented to the prover, can have a huge impact on the prover’s
running time and thus on the ability to ﬁnd a proof within a given time limit.
This chaotic aspect of the prover behaviour is relatively poorly understood,
yet has obvious consequences for evaluation. A typical experimental evaluation
of a new technique T compares the number of problems solved by a baseline
run with a run enhanced by T (over an established benchmark and with a ﬁxed
timeout). While a higher number of problems solved by the run enhanced by
T indicates a beneﬁt of the new technique, it is hard to claim that a certain
problem P is getting solved thanks to T. It might be that T just helps the
prover get lucky on P by a complicated chain of cause and eﬀect not related to
the technique T—and the original idea behind it—in any reasonable sense.
We propose to expose and counter the eﬀect of chaotic behaviours by delib-
erately injecting randomness into the prover and observing the results of many
independently seeded runs. Although computationally more costly than stan-
dard evaluation, such an approach promises to bring new insights. We gain the
ability to apply the tools of probability theory and statistics to analyze the
results, assign conﬁdences, and single out those problems that robustly beneﬁt
This work was supported by the Czech Science Foundation project 20-06390Y and the
project RICAIP no. 857306 under the EU-H2020 programme.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 659–667, 2022.
https://doi.org/10.1007/978-3-031-10769-6_38

660
M. Suda
from the evaluated technique. At the same time, by observing the changes in
the corresponding runtime distributions we can even meaningfully establish the
eﬀect of the new technique on a single problem in isolation, something that is
normally inconclusive due to the threat of chaotic ﬂuctuations.
In this paper, we report on several experiments with a randomized version
of the ATP Vampire [9]. After explaining the method in more detail (Sect. 2),
we ﬁrst demonstrate the extent in which the success of a typical Vampire
proof search strategy can be ascribed to chance (Sect. 3). Next, we use the col-
lected data to highlight the speciﬁcs of comparing two strategies probabilisti-
cally (Sect. 4). Finally, we focus on a single problem to see a chaotic behaviour
smoothened into a distribution with a high variance (Sect. 5). The paper ends
with an overview of related work (Sect. 6) and a discussion (Sect. 7).
2
Randomizing Out Chaos
Any developer of a saturation-based prover will conﬁrm that the behaviour of a
speciﬁc proving strategy on a speciﬁc problem is extremely hard to predict, that
a typical experimental evaluation of a new technique (such as the one described
earlier) invariably leads to both gains and losses in terms of the solved problems,
and that a closer look at any of the “lost” problems often reveals just a com-
plicated chain of cause and eﬀect that steers the prover away from the original
path (rather than a simple opportunity to improve the technique further).
These observations bring indirect evidence that the prover’s behaviour is
chaotic: A speciﬁc prover run can be likened to a single bead falling down through
the pegs of the famous Galton board1. The bead follows a deterministic trajec-
tory, but only because the code ﬁxes every single detail of the execution, includ-
ing many which the programmer did not care about and which were left as they
are merely out of coincidence. We put forward here that any such ﬁxed detail
(which does not contribute to an oﬃcially implemented heuristic) represents a
candidate location for randomization, since a diﬀerent programmer could have
ﬁxed the detail diﬀerently and we would still call the code essentially the same.
Implementation: We implemented randomization on top of Vampire version
4.6.1; the code is available as a separate git branch2. We divided the randomiza-
tion opportunities into three groups (governed by three new Vampire options).
Shuﬄing the input (-si on) randomly reorders the input formulas and,
recursively, sub-formulas under commutative logical operations. This is done
several times throughout the preprocessing pipeline, at the end of which a ﬁn-
ished clause normal form is produced. Randomizing traversals (-rtra on) hap-
pens during saturation and consists of several randomized reorderings including:
reordering literals in a newly generated clause and in each given clause before
activation, and shuﬄing the order in which generated clauses are put into the
1 https://en.wikipedia.org/wiki/Galton board.
2 https://github.com/vprover/vampire/tree/randire.

Vampire Getting Noisy
661
Fig. 1. Blue: ﬁrst-order TPTP problems ordered by the decreasing probability of being
solved by the dis10 strategy within 50 billion instruction limit. Red: a cactus plot for
the same strategy, showing the dependence between a given instruction budget (y-axis)
and the number of problems on average solved within that budget (x-axis). (Color ﬁgure
online)
passive set. It also (partially) randomizes term ids, which are used as tiebreak-
ers in various term indexing operations and determine the default orientation of
equational literals in the term sharing structure. Finally, “randomized age-weight
ratio” (-rawr on) swaps the default, deterministic mechanism for choosing the
next queue to select the given clause from [13] for a randomized one (which only
respects the age-weight ratio probabilistically).
All the three options were active by default during our experiments.
3
Experiment 1: A Single-Strategy View
First, we set out to establish to what degree the performance of a Vampire
strategy can be aﬀected by randomization. We chose the default strategy of the
prover except for the saturation algorithm, which we set to Discount, and the
age-weight ratio, set to 1:10 ( calling the strategy dis10). We ran our experiment
on the ﬁrst-order problems from the TPTP library [15] version 7.5.03.
To collect our data, we repeatedly (with diﬀerent seeds) ran the prover on
the problems, performing full randomization. We measured the executed instruc-
tions4 needed to successfully solve a problem and used a limit of 50 billion
instructions (which roughly corresponds to 15 s of running time on our machine5)
after which a run was declared unsuccessful. We ran the prover 10 times on each
problem and additionally as many times as required to observe the instruction
count average (over both successful and unsuccessful runs) stabilize within 1%
from any of its 10 previously recorded values6.
A summary view of the experiment is given by Fig. 1. The most important to
notice is the shaded region there, which spans 965 problems that were solved by
3 Materials accompanying the experiments can be found at https://bit.ly/3JDCwea.
4 As measured via the perf event open Linux performance monitoring feature.
5 A server with Intel(R) Xeon(R) Gold 6140 CPUs @ 2.3 GHz and 500 GB RAM.
6 Utilizing all the 72 cores of our machine, such data collection took roughly 12 h.

662
M. Suda
Fig. 2. The eﬀect of turning AVATAR oﬀin the dis10 strategy (cf. Figure 1).
dis10 at least once but not by every run. In other words, these problems have
probability p of being solved between 0 < p < 1. This is a relatively large number
and can be compared to the 8720 “easy” problems solved by every run. The
collected data implies that 9319.1 problems are being solved on average (marked
by the left-most dashed line in Fig. 1) with a standard deviation σ = 11.7. The
latter should be an interesting indicator for prover developers: beating a baseline
by only 12 TPTP problems can easily be ascribed just to chance.
Figure 1 also contains the obligatory “cactus plot” (explained in the caption),
which—thanks to the collected data—can be constructed with the “on average”
qualiﬁer. By deﬁnition, the plot reaches the left-most dashed line for the full
instruction budged of 50 billion. The subsequent dashed lines mark the number
of problems we would on average expect to solve by running the prover (indepen-
dently) on each problem twice, three, four and ﬁve times. This is an information
relevant for strategy scheduling: e.g., one can expect to solve whole additional
137 problems by running randomized dis10 for a second time.
Not every strategy exhibits the same degree of variability under randomiza-
tion. Observe Fig. 2 with a plot analogous to Fig. 1, but for dis10 in which the
AVATAR [16] has been turned oﬀ. The shaded area there is now much smaller
(and only spans 448 problems). The powerful AVATAR architecture is getting
convicted of making proof search more fragile and the prover less robust7.
Remark. Randomization incurs a small but measurable computational over-
head. On a single run of dis10 over the ﬁrst-order TPTP (ﬁltering out cases
that took less than 1 s to ﬁnish, to prevent distortion by rounding errors) the
observed median relative time spent randomizing on a single problem was 0.47%,
the average 0.59%, and the worse8 13.86%. Without randomization, the dis10
strategy solved 9335 TPTP problems under the 50 billion instruction limit, i.e.,
16 problems more than the average reported above. Such is the price we pay for
turning our prover into a Las Vegas randomized algorithm.
7 Another example of a strong but fragile heuristic is the lookahead literal selection
[5], which selects literals in a clause based on the current content of the active set:
dis10 enhanced with lookahead solves 9512.4 (±13.8) TPTP problems on average,
8672 problems with p = 1 and additional 1382 (!) problems with 0 < p < 1.
8 On the hard-to-parse, trivial-to-solve HWV094-1 with 361 199 clauses.

Vampire Getting Noisy
663
Fig. 3. Scatter plots comparing probabilities of solving a TPTP problem by the baseline
dis10 strategy and 1) dis10 with AVATAR turned oﬀ(left), and 2) dis10 with blocked
clause elimination turned on (right). On problems marked red the respective technique
could not be applied (no splittable clauses derived / no blocked clauses eliminated).
4
Experiment 2: Comparing Two Strategies
Once randomized performance proﬁles of multiple strategies are collected, it is
interesting to look at two at a time. Figure 3 shows two very diﬀerent scatter
plots, each comparing our baseline dis10 to its modiﬁed version in terms of the
probabilities of solving individual problems.
On the left we see the eﬀect of turning AVATAR oﬀ. The technique aﬀects
the proving landscape quite a lot and most problems have their mark along the
edges of the plot where at least one of the two probabilities has the extreme
value of either 0 or 1. What the plot does not show well, is how many marks end
up at the extreme corners. These are: 7896 problems easy for both, 661 easy for
AVATAR and hard without, 135 hard for AVATAR and easy without.
Such “puriﬁed”, one-sided gains and losses constitute a new interesting indi-
cator of the impact of a given technique. They should be the ﬁrst to look at,
e.g., during debugging, as they represent the most extreme but robust examples
of how the new technique changes the capabilities of the prover.
The right plot is an analogous view, but now at the eﬀect of turning on blocked
clause elimination (BCE). This is a preprocessing technique coming from the
context of propositional satisﬁability [7] extended to ﬁrst-order logic [8]. We see
that here most of the visible problems show up as marks along the plot’s main
diagonal, suggesting a (mostly) negligible eﬀect of the technique. The extreme
corners hide: 8648 problems easy for both, 17 easy with BCE (11 satisﬁable and
6 unsatisﬁable), and 2 easy without BCE (1 satisﬁable and 1 unsatisﬁable).

664
M. Suda
Fig. 4. 2D-histograms for the relative frequencies (color-scale) of how often, given a
speciﬁc awr (x-axis), solving PRO017+2 required the shown number of instructions (y-
axis). The curves in pink highlight the mean y-value for every x. The performance of
dis10 (left) and the same strategy enhanced by a goal-directed heuristic (right). (Color
ﬁgure online)
5
Experiment 3: Looking at One Problem at a Time
In their paper on age/weight shapes [13, Fig. 2], Rawson and Reger plot the
number of given-clause loops required by Vampire to solve the TPTP problem
PRO017+2 as a function of age/weight ratio (awr), a ratio specifying how often
the prover selects the next clause to activate from its age-ordered and weight-
ordered queues, respectively. The curve they obtain is quite “jiggly”, indicating
a fragile (discontinuous) dependence. Randomization allows us to smoothen the
picture and reveal new, until now hidden, (probabilistic) patterns.
The 2D-histogram in Fig. 4 (left) was obtained from 100 independently seeded
runs for each of 1200 distinct values of awr from between 1:1024 = 2−10 and
4:1 = 22. We can conﬁrm Rawson and Reger’s observation of the best awr for
PRO017+2 lying at around 1:2. However, we can now also attempt to explain the
“jiggly-ness” of their curve: With a fragile proof search, even a slight change in
awr eﬀectively corresponds to an independent sample from the prover’s execution
resource9 distribution, which—although changing continuously with awr—is of
a high variance for our problem (note the log-scale of the y-axis)10.
The distribution has another interesting property: At least for certain values
of awr it is distinctly multi-modal. As if the prover can either ﬁnd a proof quickly
(after a lucky event?) or only after much harder eﬀort later and almost nothing
in between. Shedding more light on this phenomenon is left for further research.
It is also very interesting to observe the change of such a 2D-histogram
when we modify the proof search strategy. Figure 4 (right) shows the eﬀect of
turning on SInE-level split queues [3], a goal directed clause selection heuristic
9 Rawson and Reger [13] counted given-clause loops, we measure instructions.
10 Even with 100 samples for each value of awr, the mean instruction count (rendered
in pink in Fig. 4) looks jiggly towards the weight-heavy end of the plot.

Vampire Getting Noisy
665
(Vampire option -slsq on). We can see that the mean instruction count gets
worse (for every tried awr value) and also the variance of the distribution dis-
tinctly increases. A curious eﬀect of this is that we observe the shortest suc-
cessful runs with -slsq on, while we still could not recommend (in the case of
PRO017+2) this heuristic to the user. The probabilistic view makes us realize that
there are competing criteria of prover performance for which one might want to
optimize.
6
Related Work
The idea of randomizing a theorem prover is not new. Ertel [2] studied the
speedup potential of running independently seeded instances of the connection
prover SETHEO [10]. The dashed lines in our Figs. 1 and 2 capture an analogous
notion in terms of “additional problems covered” for levels of parallelism 1−5.
randoCoP [12] is a randomized version of another connection prover, leanCoP 2.0
[11]: especially in its incomplete setup, several restarts with diﬀerent seeds helped
randoCoP improve over leanCoP in terms of the number of solved problems.
Gomes et al. [4] notice that randomized complete backtracking algorithms for
propositional satisﬁability (SAT) lead to heavy-tailed runtime distributions on
satisﬁable instances. While we have not yet analyzed the runtime distributions
coming from saturation-based ﬁrst-order proof search in detail, we deﬁnitely
observed high variance also for unsatisﬁable problems. Also in the domain of
SAT, Brglez et al. [1] proposed input shuﬄing as a way of turning solver’s runtime
into a random variable and studied the corresponding distributions.
An interesting view on the trade-oﬀs between expected performance of a
randomized solver and the risk associated with waiting for an especially long
run to ﬁnish is given by Huberman et al. [6]. This is related to the last remark
of the previous section.
Finally, in the satisﬁability modulo theories (SMT) community, input shuf-
ﬂing, or scrambling, has been discussed as an obfuscation measure in competi-
tions [17], where it should prevent the solvers to simply look up a precomputed
answer upon recognising a previously seen problem. Notable is also the use of
randomization in solver debugging via fuzz testing [14,18].
7
Discussion
As we have seen, the behaviour of a state-of-the-art saturation-based theorem
prover is to a considerable degree chaotic and on many problems a mere per-
turbation of seemingly unimportant execution details decides about the success
or the failure of the corresponding run. While this may be seen as a sign of our
as-of-yet imperfect grasp of the technology, the author believes that an equally
plausible view is that some form of chaos is inherent and originates from the
complexity of the theorem proving task itself. (A higher-order logic proof search
is expected to exhibit an even higher degree of fragility.)
This paper has proposed randomization as a key ingredient to a prover eval-
uation method that takes the chaotic nature of proof search into account. The

666
M. Suda
extra cost required by the repeated runs, in itself not unreasonable to pay on con-
temporary parallel hardware, seems more than compensated by the new insights
coming from the probabilistic picture that emerges. Moreover, other uses of ran-
domization are easy to imagine, such as data augmentation for machine learning
approaches or the construction of more robust strategy schedules. It feels that
we only scratched the surface of the opened-up possibilities. More research will
be needed to fully harness the potential of this perspective.
References
1. Brglez, F., Li, X.Y., Stallmann, M.F.M.: On SAT instance classes and a method
for reliable performance experiments with SAT solvers. Ann. Math. Artif. Intell.
43(1), 1–34 (2005). https://doi.org/10.1007/s10472-005-0417-5
2. Ertel, W.: OR-parallel theorem proving with random competition. In: Voronkov,
A. (ed.) LPAR 1992. LNCS, vol. 624, pp. 226–237. Springer, Heidelberg (1992).
https://doi.org/10.1007/BFb0013064
3. Gleiss, B., Suda, M.: Layered clause selection for saturation-based theorem proving.
In: Fontaine, P., Korovin, K., Kotsireas, I.S., R¨ummer, P., Tourret, S. (eds.) PAAR
7, Paris, France, June-July 2020. CEUR Workshop Proceedings, vol. 2752, pp. 34–
52. CEUR-WS.org (2020). http://ceur-ws.org/Vol-2752/paper3.pdf
4. Gomes, C.P., Selman, B., Crato, N., Kautz, H.A.: Heavy-tailed phenomena in
satisﬁability and constraint satisfaction problems. J. Autom. Reason. 24(1/2), 67–
100 (2000). https://doi.org/10.1023/A:1006314320276
5. Hoder, K., Reger, G., Suda, M., Voronkov, A.: Selecting the selection. In: Olivetti,
N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 313–329. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 22
6. Huberman, B., Lukose, R., Hogg, T.: An economics approach to hard computa-
tional problems. Science 275, 51–4 (1997)
7. J¨arvisalo, M., Biere, A., Heule, M.: Blocked clause elimination. In: Esparza, J.,
Majumdar, R. (eds.) TACAS 2010. LNCS, vol. 6015, pp. 129–144. Springer, Hei-
delberg (2010). https://doi.org/10.1007/978-3-642-12002-2 10
8. Kiesl, B., Suda, M., Seidl, M., Tompits, H., Biere, A.: Blocked clauses in ﬁrst-order
logic. In: Eiter, T., Sands, D. (eds.) LPAR-21, Maun, Botswana, 7–12 May 2017.
EPiC Series in Computing, vol. 46, pp. 31–48. EasyChair (2017)
9. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
10. Letz, R., Schumann, J., Bayerl, S., Bibel, W.: SETHEO: a high-performance the-
orem prover. J. Autom. Reason. 8(2), 183–212 (1992). https://doi.org/10.1007/
BF00244282
11. Otten, J.: leanCoP 2.0 and ileanCoP 1.2: high performance lean theorem proving
in classical and intuitionistic logic (system descriptions). In: Armando, A., Baum-
gartner, P., Dowek, G. (eds.) IJCAR 2008. LNCS (LNAI), vol. 5195, pp. 283–291.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-71070-7 23
12. Raths, T., Otten, J.: randoCoP: randomizing the proof search order in the con-
nection calculus. In: Konev, B., Schmidt, R.A., Schulz, S. (eds.) PAAR 1, Sydney,
Australia, 10–11 August 2008. CEUR Workshop Proceedings, vol. 373. CEUR-
WS.org (2008). http://ceur-ws.org/Vol-373/paper-08.pdf

Vampire Getting Noisy
667
13. Rawson, M., Reger, G.: Old or heavy? decaying gracefully with age/weight shapes.
In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 462–476. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-29436-6 27
14. Scott, J., Sudula, T., Rehman, H., Mora, F., Ganesh, V.: BanditFuzz: fuzzing SMT
solvers with multi-agent reinforcement learning. In: Huisman, M., P˘as˘areanu, C.,
Zhan, N. (eds.) FM 2021. LNCS, vol. 13047, pp. 103–121. Springer, Cham (2021).
https://doi.org/10.1007/978-3-030-90870-6 6
15. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. From CNF
to TH0, TPTP v.6.4.0. J. Autom. Reason. 59(4), 483–502 (2017). https://doi.org/
10.1007/s10817-017-9407-7
16. Voronkov, A.: AVATAR: the architecture for ﬁrst-order theorem provers. In: Biere,
A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 696–710. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-08867-9 46
17. Weber, T.: Scrambling and descrambling SMT-LIB benchmarks. In: King, T.,
Piskac, R. (eds.) SMT@IJCAR 2016, Coimbra, Portugal, 1–2 July 2016. CEUR
Workshop Proceedings, vol. 1617, pp. 31–40. CEUR-WS.org (2016). http://ceur-
ws.org/Vol-1617/paper3.pdf
18. Winterer, D., Zhang, C., Su, Z.: Validating SMT solvers via semantic fusion. In:
Donaldson, A.F., Torlak, E. (eds.) PLDI 2020, London, UK, 15–20 June 2020, pp.
718–730. ACM (2020)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Evolution, Termination, and Decision
Problems

On Eventual Non-negativity
and Positivity for the Weighted Sum
of Powers of Matrices
S. Akshay(B)
, Supratik Chakraborty(B)
, and Debtanu Pal
Indian Institute of Technology Bombay, Mumbai 400076, India
{akshayss,supratik,debtanu}@cse.iitb.ac.in
Abstract. The long run behaviour of linear dynamical systems is often
studied by looking at eventual properties of matrices and recurrences that
underlie the system. A basic problem in this setting is as follows: given
a set of pairs of rational weights and matrices {(w1, A1), . . . , (wm, Am)},
does there exist an integer N s.t for all n ≥N, m
i=1 wi · An
i ≥0 (resp.
> 0). We study this problem, its applications and its connections to linear
recurrence sequences. Our ﬁrst result is that for m ≥2, the problem is
as hard as the ultimate positivity of linear recurrences, a long standing
open question (known to be coNP-hard). Our second result is that for any
m ≥1, the problem reduces to ultimate positivity of linear recurrences.
This yields upper bounds for several subclasses of matrices by exploiting
known results on linear recurrence sequences. Our third result is a general
reduction technique for a large class of problems (including the above)
from diagonalizable case to the case where the matrices are simple (have
non-repeated eigenvalues). This immediately gives a decision procedure
for our problem for diagonalizable matrices.
Keywords: Eventual properties of matrices · Ultimate Positivity ·
linear recurrence sequences
1
Introduction
The study of eventual or asymptotic properties of discrete-time linear dynam-
ical systems has long been of interest to both theoreticians and practitioners.
Questions pertaining to (un)-decidability and/or computational complexity of
predicting the long-term behaviour of such systems have been extensively stud-
ied over the last few decades. Despite signiﬁcant advances, however, there remain
simple-to-state questions that have eluded answers so far. In this work, we inves-
tigate one such problem, explore its signiﬁcance and links with other known
problems, and study its complexity and computability landscape.
This work was partly supported by DST/CEFIPRA/INRIA Project EQuaVE and
DST/SERB Matrices Grant MTR/2018/000744.
Author names are in alphabetical order of last names.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 671–690, 2022.
https://doi.org/10.1007/978-3-031-10769-6_39

672
S. Akshay et al.
The time-evolution of linear dynamical systems is often modeled using lin-
ear recurrence sequences, or using sequences of powers of matrices. Asymptotic
properties of powers of matrices are therefore of central interest in the study of
linear diﬀerential systems, dynamic control theory, analysis of linear loop pro-
grams etc. (see e.g. [26,32,36,37]). The literature contains a rich body of work
on the decidability and/or computational complexity of problems related to the
long-term behaviour of such systems (see, e.g. [15,19,27,29,36,37]). A question
of signiﬁcant interest in this context is whether the powers of a given matrix of
rational numbers eventually have only non-negative (resp. positive) entries. Such
matrices, also called eventually non-negative (resp. eventually positive) matri-
ces, enjoy beautiful algebraic properties ([13,16,25,38]), and have been studied
by mathematicians, control theorists and computer scientists, among others.
For example, the work of [26] investigates reachability and holdability of non-
negative states for linear diﬀerential systems – a problem in which eventually
non-negative matrices play a central role. Similarly, eventual non-negativity (or
positivity) of a matrix modeling a linear dynamical system makes it possible
to apply the elegant Perron-Frobenius theory [24,34] to analyze the long-term
behaviour of the system beyond an initial number of time steps. Another level of
complexity is added if the dynamics is controlled by a set of matrices rather than
a single one. For instance, each matrix may model a mode of the linear dynami-
cal system [23]. In a partial observation setting [22,39], we may not know which
mode the system has been started in, and hence have to reason about eventual
properties of this multi-modal system. This reduces to analyzing the sum of
powers of the per-mode matrices, as we will see.
Motivated by the above considerations, we study the problem of determining
whether a given matrix of rationals is eventually non-negative or eventually
positive and also a generalized version of this problem, wherein we ask if the
weighted sum of powers of a given set of matrices of rationals is eventually
non-negative (resp. positive). Let us formalize the general problem statement.
Given a set A = {(w1, A1), . . . (wm, Am)}, where each wi is a rational
number and each Ai is a k×k matrix of rationals, we wish to determine
if m
i=1 wi · An
i has only non-negative (resp. positive) entries for all
suﬃciently large values of n. We call this problem Eventually Non-Negative
(resp. Positive) Weighted Sum of Matrix Powers problem, or ENNSoM (resp.
EPSoM) for short. The eventual non-negativity (resp. positivity) of powers of a
single matrix is a special case of the above problem, where A = {(1, A)}. We call
this special case the Eventually Non-Negative (resp. Positive) Matrix problem,
or ENNMat (resp. EPMat) for short.
Given the simplicity of the ENNSoM and EPSoM problem statements, one may
be tempted to think that there ought to be simple algebraic characterizations
that tell us whether m
i=1 wi · An
i is eventually non-negative or positive. But
in fact, the landscape is signiﬁcantly nuanced. On one hand, a solution to the
general ENNSoM or EPSoM problem would resolve long-standing open questions
in mathematics and computer science. On the other hand, eﬃcient algorithms
can indeed be obtained under certain well-motivated conditions. This paper is a
study of both these aspects of the problem. Our primary contributions can be

On Eventual Properties for Weighted Sum of Powers of Matrices
673
summarized as follows. Below, we use A = {(w1, A1), . . . (wm, Am)} to deﬁne an
instance of ENNSoM or EPSoM.
1. If |A| ≥2, we show that both ENNSoM and EPSoM are as hard as the ultimate
non-negativity problem for linear recurrence sequences (UNNLRS, for short).
The decidability of UNNLRS is closely related to Diophantine approximations,
and remains unresolved despite extensive research (see e.g. [31]).
Since UNNLRS is coNP-hard (in fact, as hard as the decision problem for
universal theory of reals), so is ENNSoM and EPSoM, when |A| ≥2. Thus,
unless P = NP, we cannot hope for polynomial-time algorithms, and any
algorithm would also resolve long-standing open problems.
2. On the other hand, regardless of |A|, we show a reduction in the other direc-
tion from ENNSoM (resp. EPSoM) to UNNLRS (resp. UPLRS, the strict version
of UNNLRS). As a consequence, we get decidability and complexity bounds for
special cases of ENNSoM and EPSoM, by exploiting recent results on recurrence
sequences [30,31,35]. For example, if each matrix Ai in A is simple, i.e. has
all distinct eigenvalues, we obtain PSPACE algorithms.
3. Finally, we consider the case where Ai is diagonalizable (also called non-
defective or inhomogenous dilation map) for each (wi, Ai) ∈A. This is a
practically useful class of matrices and strictly subsumes simple matrices. We
present a novel reduction technique for a large family of problems (includ-
ing eventual non-negativity/positivity, everywhere non-negativity/positivity
etc.) over diagonalizable matrices to the corresponding problem over simple
matrices. This yields eﬀective decision procedures for EPSoM and ENNSoM for
diagonalizable matrices. Our reduction makes use of a novel perturbation
analysis that also has other interesting consequences.
As mentioned earlier, the eventual non-negativity and positivity problem for
single rational matrices are well-motivated in the literature, and EPMat (or EPSoM
with |A| = 1) is known to be in PTIME [25]. But for ENNMat, no decidability
results are known to the best of our knowledge. From our work, we obtain two
new results about ENNMat: (i) in general ENNMat reduces to UNNLRS and (ii) for
diagonalizable matrices, we can decide ENNMat. What is surprising (see Sect. 5)
is that the latter decidability result goes via ENNSoM, i.e. the multiple matrices
case. Thus, reasoning about sums of powers of matrices, viz. ENNSoM, is useful
even when reasoning about powers of a single matrix, viz. ENNMat.
Potential Applications of ENNSoM and EPSoM. A prime motivation for deﬁn-
ing the generalized problem statement ENNSoM is that it is useful even when
reasoning about the single matrix case ENNMat. However and unsurprisingly,
ENNSoM and EPSoM are also well-motivated independently. Indeed, for every
application involving a linear dynamical system that reduces to ENNMat/EPMat,
there is a naturally deﬁned aggregated version of the application involving multi-
ple independent linear dynamical systems that reduces to ENNSoM/EPSoM (e.g.,
the swarm of robots example in [3]).
Beyond this, ENNSoM/EPSoM arise naturally and directly when solving prob-
lems in diﬀerent practical scenarios. Due to lack of space, we detail two applica-
tions here and describe more in the longer version of the paper [3].

674
S. Akshay et al.
Partially Observable Multi-modal Systems. Our ﬁrst example comes from
the domain of cyber-physical systems in a partially observable setting. Consider
a system (e.g. a robot) with m modes of operation, where the ith mode dynamics
is given by a linear transformation encoded as a k×k matrix of rationals, say Ai.
Thus, if the system state at (discrete) time t is represented by a k-dimensional
rational (row) vector ut, the state at time t + 1, when operating in mode i, is
given by utAi. Suppose the system chooses to operate in one of its various modes
at time 0, and then sticks to this mode at all subsequent time. Further, the initial
choice of mode is not observable, and we are only given a probability distribution
over modes for the initial choice. This is natural, for instance, if our robot (multi-
modal system) knows the terrain map and can make an initial choice of which
path (mode) to take, but cannot change its path once it has chosen. If pi is a
rational number denoting the probability of choosing mode i initially, then the
expected state at time n is given by m
i=1 pi · u0An
i = u0
 m
i=1 pi · An
i

. A
safety question in this context is whether starting from a state u0 with all non-
negative (resp. positive) components, the system is expected to eventually stay
locked in states that have all non-negative (resp. positive) components. In other
words, does u0
 m
i=1 pi · An
i

have all non-negative (resp. positive) entries for
all suﬃciently large n? Clearly, a suﬃcient condition for an aﬃrmative answer
to this question is to have n
i=1 pi · An
i eventually non-negative (resp. positive),
which is an instance of ENNSoM (resp. EPSoM).
Commodity Flow Networks. Consider a ﬂow network where m diﬀerent
commodities {c1, . . . , cm} use the same ﬂow infrastructure spanning k nodes,
but have diﬀerent loss/regeneration rates along diﬀerent links. For every pair
of nodes i, j ∈{1, . . . , k} and for every commodity c ∈{c1, . . . , cm}, suppose
Ac[i, j] gives the fraction of the ﬂow of commodity c starting from i that reaches
j through the link connecting i and j (if it exists). In general, Ac[i, j] is the
product of the fraction of the ﬂow of commodity c starting at i that is sent along
the link to j, and the loss/regeneration rate of c as it ﬂows in the link from i to
j. Note that Ac[i, j] can be 0 if commodity c is never sent directly from i to j, or
the commodity is lost or destroyed in ﬂowing along the link from i to j. It can be
shown that An
c [i, j] gives the fraction of the ﬂow of c starting from i that reaches
j after n hops through the network. If commodities keep circulating through the
network ad-inﬁnitum, we wish to ﬁnd if the network gets saturated, i.e., for all
suﬃciently long enough hops through the network, there is a non-zero fraction
of some commodity that ﬂows from i to j for every pair i, j. This is equivalent
to asking if there exists N ∈N such that m
ℓ=1 An
cℓ> 0. If diﬀerent commodities
have diﬀerent weights (or costs) associated, with commodity ci having the weight
wi, the above formulation asks if m
ℓ=1 wℓ.An
cℓis eventually positive, which is
eﬀectively the EPSoM problem.
Other Related Work. Our problems of interest are diﬀerent from other well-
studied problems that arise if the system is allowed to choose its mode inde-
pendently at each time step (e.g. as in Markov decision processes [5,21]). The
crucial diﬀerence stems from the fact that we require that the mode be chosen

On Eventual Properties for Weighted Sum of Powers of Matrices
675
once initially, and subsequently, the system must follow the same mode for-
ever. Thus, our problems are prima facie diﬀerent from those related to general
probabilistic or weighted ﬁnite automata, where reachability of states and ques-
tions pertaining to long-run behaviour are either known to be undecidable or
have remained open for long ([6,12,17]). Even in the case of unary probabilis-
tic/weighted ﬁnite automata [1,4,8,11], reachability is known in general to be
as hard as the Skolem problem on linear recurrences – a long-standing open
problem, with decidability only known in very restricted cases. The diﬀerence
sometimes manifests itself in the simplicity/hardness of solutions. For example,
EPMat (or EPSoM with |A| = 1) is known to be in PTIME [25] (not so for ENNMat
however), whereas it is still open whether the reachability problem for unary
probabilistic/weighted automata is decidable. It is also worth remarking that
instead of the sum of powers of matrices, if we considered the product of their
powers, we would eﬀectively be solving problems akin to the mortality problem
[9,10] (which asks whether the all-0 matrix can be reached by multiplying with
repetition from a set of matrices) – a notoriously diﬃcult problem. The diago-
nalizable matrix restriction is a common feature in in the context of linear loop
programs (see, e.g., [7,28]), where matrices are used for updates. Finally, logics
to reason about temporal properties of linear loops have been studied, although
decidability is known only in restricted settings, e.g. when each predicate deﬁnes
a semi-algebraic set contained in some 3-dimensional subspace, or has intrinsic
dimension 1 [20].
2
Preliminaries
The symbols Q, R, A and C denote the set of rational, real, algebraic and com-
plex numbers respectively. Recall that an algebraic number is a root of a non-zero
polynomial in one variable with rational coeﬃcients. An algebraic number can
be real or complex. We use RA to denote the set of real algebraic numbers (which
includes all rationals). The sum, diﬀerence and product of two (real) algebraic
numbers is again (real) algebraic. Furthermore, every root of a polynomial equa-
tion with (real) algebraic coeﬃcients is again (real) algebraic. We call matrices
with all rational (resp. real algebraic or real) entries rational (resp. real algebraic
or real) matrices. We use A ∈Qk×l (resp. A ∈Rk×l and A ∈RAk×l) to denote
that A is a k×l rational (resp. real and real algebraic) matrix, with rows indexed
1 through k, and columns indexed 1 through l. The entry in the ith row and jth
column of a matrix A is denoted A[i, j]. If A is a column vector (i.e. l = 1),
we often use boldface letters, viz. A, to refer to it. In such cases, we use A[i]
to denote the ith component of A, i.e. A[i, 1]. The transpose of a k × l matrix
A, denoted AT, is the l × k matrix obtained by letting AT[i, j] = A[j, i] for all
i ∈{1, . . . l} and j ∈{1, . . . k}. Matrix A is said to be non-negative (resp. posi-
tive) if all entries of A are non-negative (resp. positive) real numbers. Given a set
A = {(w1, A1), . . . (wm, Am)} of (weight, matrix) pairs, where each Ai ∈Qk×k
(resp. ∈RAk×k) and each wi ∈Q, we use  An to denote the weighted matrix
sum m
i=1 wi · An
i , for every natural number n > 0. Note that  An is itself a
matrix in Qk×k (resp. RAk×k).

676
S. Akshay et al.
Deﬁnition 1. We say that A is eventually non-negative (resp. positive) iﬀthere
is a positive integer N s.t.,  An is non-negative (resp. positive) for all n ≥N.
The ENNSoM (resp. EPSoM) problem, described in Sect. 1, can now be re-phrased
as: Given a set A of pairs of rational weights and rational k × k matrices, is A
eventually non-negative (resp. positive)? As mentioned in Sect. 1, if A = {(1, A)},
the ENNSoM (resp. EPSoM) problem is also called ENNMat (resp. EPMat). We note
that the study of ENNSoM and EPSoM with |A| = 1 is eﬀectively the study of
ENNMat and EPMat i.e., wlog we can assume w = 1.
The characteristic polynomial of a matrix A ∈RAk×k is given by det(A−λI),
where I denotes the k×k identity matrix. Note that this is a degree k polynomial
in λ. The roots of the characteristic polynomial are called the eigenvalues of A.
The non-zero vector solution of the equation Ax = λix, where λi is an eigenvalue
of A, is called an eigenvector of A. Although A ∈RAk×k, in general it can
have eigenvalues λ ∈C which are all algebraic numbers. An eigenvector is said
to be positive (resp. non-negative) if each component of the eigenvector is a
positive (resp. non-negative) rational number. A matrix is called simple if all
its eigenvalues are distinct. Further, a matrix A is called diagonalizable if there
exists an invertible matrix S and diagonal matrix D such that SDS−1 = A.
The study of weighted sum of powers of matrices is intimately related to the
study of linear recurrence sequences (LRS), as we shall see. We now present some
deﬁnitions and useful properties of LRS. For more details on LRS, the reader is
referred to the work of Everest et al. [14]. A sequence of rational numbers ⟨u⟩
= ⟨un⟩∞
n=0 is called an LRS of order k (> 0) if the nth term of the sequence,
for all n ≥k, can be expressed using the recurrence: un = ak−1un−1 + . . . +
a1un−k−1 + a0un−k. Here, a0 (̸= 0), a1, . . . , ak−1 ∈Q are called the coeﬃcients
of the LRS, and u0, u1, . . . , uk−1 ∈Q are called the initial values of the LRS.
Given the coeﬃcients and initial values, an LRS is uniquely deﬁned. However,
the same LRS may be deﬁned by multiple sets of coeﬃcients and corresponding
initial values. An LRS ⟨u⟩is said to be periodic with period ρ if it can be
deﬁned by the recurrence un = un−ρ for all n ≥ρ. Given an LRS ⟨u⟩, its
characteristic polynomial is p⟨u⟩(x) = xk −k−1
i=0 aixi. We can factorize the
characteristic polynomial as p⟨u⟩(x) = d
j=1(x−λj)ρj, where λj is a root, called
a characteristic root of algebraic multiplicity ρj. An LRS is called simple if
ρj = 1 for all j, i.e. all characteristic roots are distinct. Let {λ1, λ2, . . . , λd}
be distinct roots of p⟨u⟩(x) with multiplicities ρ1, ρ2, . . . , ρd respectively. Then
the nth term of the LRS, denoted un, can be expressed as un = d
j=1 qj(n)λn
j ,
where qj(x) ∈C(x) are univariate polynomials of degree at most ρj −1 with
complex coeﬃcients such that d
j=1 ρj = k. This representation of an LRS is
known as the exponential polynomial solution representation. It is well known
that scaling an LRS by a constant gives another LRS, and the sum and product
of two LRSs is also an LRS (Theorem 4.1 in [14]). Given an LRS ⟨u⟩deﬁned
by un = ak−1un−1 + . . . + a1un−k−1 + a0un−k, we deﬁne its companion matrix
M⟨u⟩to be the k × k matrix shown in Fig. 1.

On Eventual Properties for Weighted Sum of Powers of Matrices
677
M u =
⎢⎢⎢⎢
ak−1 1 . . . 0 0
...
... ... ...
...
a2
0 . . . 1 0
a1
0 . . . 0 1
a0
0 . . . 0 0
⎥⎥⎥⎥
Fig. 1. Companion matrix
When ⟨u⟩is clear from the context, we often
omit the subscript for clarity of notation, and use
M for M⟨u⟩. Let u = (uk−1, . . . , u0) be a row vec-
tor containing the k initial values of the recurrence,
and let ek = (0, 0, . . . 1)T be a column vector of k
dimensions with the last element equal to 1 and the
rest set to 0s. It is easy to see that for all n ≥1,
uM nek gives un. Note that the eigenvalues of the
matrix M are exactly the roots of the characteristic
polynomial of the LRS ⟨u⟩.
For u = (uk−1, . . . , u0), we call the matrix G⟨u⟩=
 0
u
0T M⟨u⟩

the generator
matrix of the LRS ⟨u⟩, where 0 is a k-dimensional vector of all 0s. We omit the
subscript and use G instead of G⟨u⟩, when the LRS ⟨u⟩is clear from the context.
It is easy to show from the above that un = Gn+1[1, k + 1] for all n ≥0.
We say that an LRS ⟨u⟩is ultimately non-negative (resp. ultimately posi-
tive) iﬀthere exists N > 0, such that ∀n ≥N, un ≥0 (resp. un > 0)1. The
problem of determining whether a given LRS is ultimately non-negative (resp.
ultimately positive) is called the Ultimate Non-negativity (resp. Ultimate Posi-
tivity) problem for LRS. We use UNNLRS (resp. UPLRS) to refer to this problem.
It is known [19] that UNNLRS and UPLRS are polynomially inter-reducible, and
these problems have been widely studied in the literature (e.g., [27,31,32]). A
closely related problem is the Skolem problem, wherein we are given an LRS
⟨u⟩and we are required to determine if there exists n ≥0 such that un = 0.
The relation between the Skolem problem and UNNLRS (resp. UPLRS) has been
extensively studied in the literature (e.g., [18,19,33]).
3
Hardness of Eventual Non-negativity and Positivity
In this section, we show that UNNLRS (resp. UPLRS) polynomially reduces to
ENNSoM (resp. EPSoM) when |A| ≥2. Since UNNLRS and UPLRS are known to be
coNP-hard (in fact, as hard as the decision problem for the universal theory of
reals Theorem 5.3 [31]), we conclude that ENNSoM and EPSoM are also coNP-hard
and at least as hard as the decision problem for the universal theory of reals,
when |A| ≥2. Thus, unless P = NP, there is no hope of ﬁnding polynomial-time
solutions to these problems.
Theorem 1. UNNLRS reduces to ENNSoM with |A| ≥2 in polynomial time.
Proof. Given an LRS ⟨u⟩of order k deﬁned by the recurrence un = ak−1un−1 +
. . . + a1un−k−1 + a0un−k and initial values u0, u1, . . . , uk−1, construct two
1 Ultimately non-negative (resp. ultimately positive) LRS, as deﬁned by us, have also
been called ultimately positive (resp. strictly positive) LRS elsewhere in the literature
[31]. However, we choose to use terminology that is consistent across matrices and
LRS, to avoid notational confusion.

678
S. Akshay et al.
matrices A1 and A2 such that ⟨u⟩is ultimately non-negative iﬀ(An
1 + An
2) is
eventually non-negative. Consider A1 =
 0
u
0T M

, the generator matrix of ⟨u⟩
and A2 =
 0 0
0T P

, where P ∈Qk×k is constructed such that : P[i, j] ≥|M[i, j]|.
For example P can be constructed as: P[i, j] = M[i, j] for all j ∈[2, k] and
i ∈[1, k] and P[i, j] = max(|a0|, |a1|, . . . , |ak−1|) + 1 for j = 1. Now consider
the sequence of matrices deﬁned by An
1 + An
2, for all n ≥1. By properties of the
generator matrix, it is easily veriﬁed that An
1 =
 0 uM n−1
0T
M n

. Similarly, we get
An
2 =
 0
0
0T P n

. Therefore, An
1 + An
2 =
 0
uM n−1
0T P n + M n

, for all n ≥1. Now, we
can observe that P n + M n is always non-negative, since P[i, j] ≥|M[i, j]| ≥0
for all i, j ∈{1, . . . k} and hence P n[i, j] + M n[i, j] ≥0 for all i, j ∈{1, . . . k}
and n ≥1. Thus we conclude that A(n) = An
1 + An
2 ≥0 (n ≥1) iﬀ⟨u⟩is
ultimately non-negative, since the elements A(n)[1, 1] . . . , A(n)[1, k + 1] consists
of (un+k−2 . . . , un, un−1) and the rest of the elements are non-negative.
⊓⊔
Observe that the same reduction technique works if we are required to
use more than 2 matrices in ENNSoM. Indeed, we can construct matrices
A3, A4, . . . , Am similar to the construction of A2 in the reduction above, by
having the k × k matrix in the bottom right (see deﬁnition of A2) to have pos-
itive values greater than the maximum absolute value of every element in the
companion matrix.
A simple modiﬁcation of the above proof setting A2 =
 1 0
1T P

, where 1
denotes the k-dimensional vector of all 1’s gives us the corresponding hardness
result for EPSoM (see [3] for details).
Theorem 2. UPLRS reduces to EPSoM with |A| ≥2 in polynomial time.
We remark that for the reduction technique used in Theorems 1 and 2 to
work, we need at least two (weight, matrix) pairs in A. For explanation of why
this reduction doesn’t work when |A| = 1, we refer the reader to [3]. Having
shown the hardness of ENNSoM and EPSoM when |A| ≥2, we now proceed to
establish upper bounds on the computational complexity of these problems.
4
Upper Bounds on Eventual Non-negativity
and Positivity
In this section, we show that ENNSoM (resp. EPSoM) is polynomially reducible to
UNNLRS (resp. UPLRS), regardless of |A|.
Theorem 3. ENNSoM, reduces to UNNLRS in polynomial time.
The proof is in two parts. First, we show that for a single matrix A, we
can construct a linear recurrence ⟨a⟩such that A is eventually non-negative iﬀ

On Eventual Properties for Weighted Sum of Powers of Matrices
679
⟨a⟩is ultimately non-negative. Then, we show that starting from such a linear
recurrence for each matrix in A, we can construct a new LRS, say ⟨a⋆⟩, with
the property that the weighted sum of powers of the matrices in A is eventually
non-negative iﬀ⟨a⋆⟩is ultimately non-negative. Our proof makes crucial use of
the following property of matrices.
Lemma 1 Adapted from Lemma 1.1 of [19]). Let A ∈Qk×k be a rational
matrix with characteristic polynomial pA(λ) = det(A −λI). Suppose we deﬁne
the sequence ⟨aij⟩for every 1 ≤i, j ≤k as follows: ai,j
n
= An+1[i, j], for all
n ≥0. Then ⟨ai,j⟩is an LRS of order k with characteristic polynomial pA(x)
and initial values given by aij
0 = A1[i, j], . . . aij
k−1 = Ak[i, j].
This follows from the Cayley-Hamilton Theorem and the reader is referred to [19]
for further details. From Lemma 1, it is easy to see that the LRS ⟨ai,j⟩for
all 1 ≤i, j ≤k share the same order and characteristic polynomial (hence
the deﬁning recurrence) and diﬀer only in their initial values. For notational
convenience, we say that the LRS ⟨ai,j⟩is generated by A[i, j].
Proposition 1. A matrix A ∈Qk×k is eventually non-negative iﬀall LRS ⟨ai,j⟩
generated by A[i, j] for all 1 ≤i, j ≤k are ultimately non-negative.
The proof follows from the deﬁnition of eventually non-negative matrices and
the deﬁnition of ⟨aij⟩. Next we deﬁne the notion of interleaving of LRS.
Deﬁnition 2. Consider a set S = {⟨ui⟩: 0 ≤i < t} of t LRSes, each having
order k and the same characteristic polynomial. An LRS ⟨v⟩is said to be the
LRS-interleaving of S iﬀvtn+s = us
n for all n ∈N and 0 ≤s < t.
Observe that, the order of ⟨v⟩is tk and its initial values are given by the
interleaving of the k initial values of the LRSes ⟨ui⟩. Formally, the initial values
are vtj+i = ui
j for 0 ≤i < t and 0 ≤j < k. The characteristic polynomial p⟨v⟩(s)
is equal to p⟨ui⟩(xt).
Proposition 2. The LRS-interleaving ⟨v⟩of a set of LRSes S = {⟨ui⟩: 0 ≤i <
t} is ultimately non-negative iﬀeach LRS ⟨ui⟩in S is ultimately non-negative.
Now, from the deﬁnitions of LRSes ⟨ai,j⟩, ⟨ui⟩and ⟨v⟩, and from Proposi-
tions 1 and 2, we obtain the following crucial lemma.
Lemma 2. Given a matrix A ∈Qk×k, let S = {⟨ui⟩| ui
n = apq
n , where p =
⌊i/k⌋+ 1, q = i mod k + 1, 0 ≤i < k2} be the set of k2 LRSes mentioned in
Lemma 1. The LRS ⟨v⟩generated by LRS-interleaving of S satisﬁes the following:
1. A is eventually non-negative iﬀ⟨v⟩is ultimately non-negative.
2. p⟨v⟩(x) = k
i=1(xk2 −λi), where λ1, . . . λk are the (possibly repeated) eigen-
values of A.
3. vrk2+sk+t = usk+t
r
= as+1,t+1
r
= Ar+1[s + 1, t + 1] for all r ∈N, 0 ≤s, t < k.
We lift this argument from a single matrix to a weighted sum of matrices.

680
S. Akshay et al.
Lemma 3. Given A = {(w1, A1), . . . , (wm, Am)}, there exists a linear recur-
rence ⟨a⋆⟩, such that m
i=1 wiAn
i is eventually non-negative iﬀ⟨a⋆⟩is ultimately
non-negative.
Proof. For each matrix Ai in A, let ⟨vi⟩be the interleaved LRS as constructed
in Lemma 2. Let wi⟨vi⟩denote the scaled LRS whose nth entry is wivi
n for all
n ≥0. The LRS ⟨a⋆⟩is obtained by adding the scaled LRSes w1⟨v1⟩, w2⟨v2⟩, . . .
wm⟨vm⟩. Clearly, a⋆
n is non-negative iﬀm
i=1 wivi
n is non-negative. From the
deﬁnition of vi (see Lemma 2), we also know that for all n ≥0, vi
n = Ar+1
i
[s +
1, t + 1], where r = ⌊n/k2⌋, s = ⌊(n mod k2)/k⌋and t = n mod k. Therefore,
a⋆
n is non-negative iﬀm
i=1 wiAr+1
i
[s + 1, t + 1] is non-negative. It follows that
⟨a⋆⟩is ultimately non-negative iﬀm
i=1 wiAn
i is eventually non-negative.
⊓⊔
From Lemma 3, we can conclude the main result of this section, i.e., proof
of Theorem 3. The following corollary can be shown mutatis mutandis.
Corollary 1. EPSoM reduces to UPLRS in polynomial time.
We note that it is also possible to argue about the eventual non-negativity
(positivity) of only certain indices of the matrix using a similar argument as
above. By interleaving only the LRS’s corresponding to certain indices of the
matrices in A, we can show this problem’s equivalence with UNNLRS (UPLRS).
5
Decision Procedures for Special Cases
Since there are no known algorithms for solving UNNLRS in general, the results
of the previous section present a bleak picture for deciding ENNSoM and EPSoM.
We now show that these problems can be solved in some important special cases.
5.1
Simple Matrices and Matrices with Real Algebraic Eigenvalues
Our ﬁrst positive result follows from known results for special classes of LRSes.
Theorem 4. ENNSoM and EPSoM are decidable for A = {(w1, A1), . . . (wm, Am)}
if one of the following conditions holds for all i ∈{1, . . . m}.
1. All Ai are simple. In this case, ENNSoM and EPSoM are in PSPACE. Addition-
ally, if the rank k of all Ai is ﬁxed, ENNSoM and EPSoM are in PTIME.
2. All eigenvalues of Ai are roots of real algebraic numbers. In this case, ENNSoM
and EPSoM are in coNPPosSLP (a complexity class in the Counting Hierarchy,
contained in PSPACE).
Proof. Suppose each Ai ∈Qk×k, and let λi,1, . . . λi,k be the (possibly repeated)
eigenvalues of Ai. The characteristic polynomial of Ai is pAi(x) = k
j=1(x −
λi,j). Denote the LRS obtained from Ai by LRS interleaving as in Lemma 2
as ⟨ai⟩. By Lemma 2, we have (i) ai
rk2+sk+t = Ar+1
i
[s + 1, t + 1] for all r ∈N
and 0 ≤s, t < k, and (ii) p⟨ai⟩(x) = k
j=1

xk2 −λi,j

. We now deﬁne the

On Eventual Properties for Weighted Sum of Powers of Matrices
681
scaled LRS {⟨bi⟩, where | bi
n = wi ai
n for all n ∈N. Since scaling does not
change the characteristic polynomial of an LRS (refer [3] for a simple proof),
we have p⟨bi⟩(x) = k
j=1

xk2 −λi,j

. Once the LRSes ⟨b1⟩, . . . ⟨bm⟩are obtained
as above, we sum them to obtain the LRS ⟨b⋆⟩. Thus, for all n ∈N, we have
b⋆
n = m
i=1 bi
n = m
i=1 wi ai
n = m
i=1 wi Ar
i [s, t], where n = rk2 + sk + t, r ∈N
and 0 ≤s, t < k. Hence, ENNSoM (resp. EPSoM) for {(w1, A1), . . . (wm, Am)}
polynomially reduces to UNNLRS (resp. UPLRS) for ⟨b⋆⟩.
By [14], we know that the characteristic polynomial p⟨b⋆⟩(x) is the LCM of
the characteristic polynomials p⟨bi⟩(x) for 1 ≤i ≤m. If Ai are simple, there
are no repeated roots of p⟨bi⟩(x). If this holds for all i ∈{1, . . . m}, there are no
repeated roots of the LCM of p⟨b1⟩(x), . . . p⟨bm⟩(x) as well. Hence, p⟨b⋆⟩(x) has
no repeated roots. Similarly, if all eigenvalues of Ai are roots of real algebraic
numbers, so are all roots of p⟨bi⟩(x). It follows that all roots of the LCM of
p⟨b1⟩(x), . . . p⟨bm⟩(x), i.e. p⟨b⋆⟩(x), are also roots of real algebraic numbers.
The theorem now follows from the following two known results about LRS.
1. UNNLRS (resp. UPLRS) for simple LRS is in PSPACE. Furthermore, if the LRS
is of bounded order, UNNLRS (resp. UPLRS) is in PTIME [31].
2. UNNLRS (resp. UPLRS) for LRS in which all roots of characteristic polynomial
are roots of real algebraic numbers is in coNPPosSLP [2].
⊓⊔
Remark: The technique used in [31] to decide UNNLRS (resp. UPLRS) for simple
rational LRS also works for simple LRS with real algebraic coeﬃcients and initial
values. This allows us to generalize Theorem 4(1) to the case where all Ai’s and
wi’s are real algebraic matrices and weights respectively.
5.2
Diagonalizable Matrices
We now ask if ENNSoM and EPSoM can be decided if each matrix Ai is diagonal-
izable. Since diagonalizable matrices strictly generalize simple matrices, Theo-
rem 4(1) cannot answer this question directly, unless one perhaps looks under the
hood of the (highly non-trivial) proof of decidability of non-negativity/positivity
of simple LRSes. The main contribution of this section is a reduction that allows
us to decide ENNSoM and EPSoM for diagonalizable matrices using a black-box
decision procedure (i.e. without knowing operational details of the procedure
or details of its proof of correctness) for the corresponding problem for simple
real-algebraic matrices.
Before we proceed further, let us consider an example of a non-simple matrix
(i.e. one with repeated eigenvalues) that is diagonalizable.
A =
5
12 −6
−3 −10 6
3
12 8
Fig. 2. Diagonalizable matrix
Speciﬁcally, matrix A in Fig. 2 has eigenval-
ues 2, 2 and −1, and can be written as SDS−1,
where D is the 3 × 3 diagonal matrix with
D[1, 1] = D[2, 2] = 2 and D[3, 3] = −1, and
S is the 3 × 3 matrix with columns (−4, 1, 0)T,
(2, 0, 1)T and (−1, 1, 1)T.

682
S. Akshay et al.
Interestingly, the reduction technique we develop applies to properties much
more general than ENNSoM and EPSoM. Formally, given a sequence of matrices
Bn deﬁned by m
i=1 wiAn
i , we say that a property P of the sequence is positive
scaling invariant if it stays unchanged even if we scale all Ais by the same positive
real. Examples of such properties include ENNSoM, EPSoM, non-negativity and
positivity of Bn (i.e. is Bn[i, j] ≥0 or < 0, as the case may be, for all n ≥1 and
for all 1 ≤i, j ≤k), existence of zero (i.e. is Bn equal to the all 0-matrix for
some n ≥1), existence of a zero element (i.e. is Bn[i, j] = 0 for some n ≥1 and
some i, j ∈{1, . . . k}), variants of the r-non-negativity (resp. r-positivity and
r-zero) problem, i.e. does there exist at least/exactly/at most r non-negative
(resp. positive/zero) elements in Bn for all n ≥1, for a given r ∈[1, k]) etc. The
main result of this section is a reduction for deciding such properties, formalized
in the following theorem.
Theorem 5. The decision problem for every positive scaling invariant property
on rational diagonalizable matrices eﬀectively reduces to the decision problem for
the property on real algebraic simple matrices.
While we defer the proof of this theorem to later in the section, an immediate
consequence of Theorem 5 and Theorem 4(1) (read with the note at the end of
Sect. 5.1) is the following result.
Corollary 2. ENNSoM
and EPSoM
are decidable for A
=
{(w1, A1), . . .
(wm, Am)} if all Ais are rational diagonalizable matrices and all wis are rational.
It is important to note that Theorem 5 yields a decision procedure for checking
any positive scaling invariant property of diagonalizable matrices from a corre-
sponding decision procedure for real algebraic simple matrices without making
any assumptions about the inner working of the latter decision procedure. Given
any black-box decision procedure for checking any positive scaling property for
a set of weighted simple matrices, our reduction tells us how a corresponding
decision procedure for checking the same property for a set of weighted diago-
nalizable matrices can be constructed. Interestingly, since diagonalizable matri-
ces have an exponential form solution with constant coeﬃcients for exponential
terms, we can use an algorithm that exploits this speciﬁc property of the expo-
nential form (like Ouaknine and Worrell’s algorithm [31], originally proposed for
checking ultimate positivity of simple LRS) to deal with diagonalizable matrices.
However, our reduction technique is neither speciﬁc to this algorithm nor does
it rely on any special property the exponential form of the solution.
The proof of Theorem 5 crucially relies on the notion of perturbation of
diagonalizable matrices, which we introduce ﬁrst. Let A be a k × k real diago-
nalizable matrix. Then, there exists an invertible k × k matrix S and a diagonal
k × k matrix D such that A = SDS−1, where S and D may have complex
entries. It follows from basic linear algebra that for every i ∈{1, . . . k}, D[i, i] is
an eigenvalue of A and if α is an eigenvalue of A with algebraic multiplicity ρ,
then α appears exactly ρ times along the diagonal of D. Furthermore, for every
i ∈{1, . . . k}, the ith column of S (resp. ith row of S−1) is an eigenvector of
A (resp. of AT) corresponding to the eigenvalue D[i, i], and the columns of S

On Eventual Properties for Weighted Sum of Powers of Matrices
683
(resp. rows of S−1) form a basis of the vector space Ck. Let α1, . . . αm be the
eigenvalues of A with algebraic multiplicities ρ1, . . . ρm respectively. Wlog, we
assume that ρ1 ≥. . . ≥ρm and the diagonal of D is partitioned into segments
as follows: the ﬁrst ρ1 entries along the diagonal are α1, the next ρ2 entries are
α2, and so on. We refer to these segments as the α1-segment, α2-segment and so
on, of diagonal of D. Formally, if κi denotes i−1
j=1 ρj, the αi-segment of diagonal
of D consists of entries D[κi +1, κi +1], . . . D[κi +ρi, κi +ρi], all of which are αi.
Since A is a real matrix, its characteristic polynomial has all real coeﬃ-
cients and for every eigenvalue α of A (and hence of AT), its complex conjugate,
denoted α, is also an eigenvalue of A (and hence of AT) with the same algebraic
multiplicity. This allows us to deﬁne a bijection hD from {1, . . . , k} to {1, . . . k}
as follows. If D[i, i] is real, then hD(i) = i. Otherwise, let D[i, i] = α ∈C and let
D[i, i] be the lth element in the α-segment of the diagonal of D. Then hD(i) = j,
where D[j, j] is the lth element in the α-segment of the diagonal of D. The
matrix A being real also implies that for every real eigenvalue α of A (resp. of
AT), there exists a basis of real eigenvectors of the corresponding eigenspace.
Additionally, for every non-real eigenvalue α and for every set of eigenvectors
of A (resp. of AT) that forms a basis of the eigenspace corresponding to α, the
component-wise complex conjugates of these basis vectors serve as eigenvectors
of A (resp. of AT) and form a basis of the eigenspace corresponding to α.
Using the above notation, we choose matrix S−1 (and hence S) such that
A = SDS−1 as follows. Suppose α is an eigenvalue of A (and hence of AT) with
algebraic multiplicity ρ. Let {i + 1, . . . i + ρ} be the set of indices j for which
D[j, j] = α. If α is real (resp. complex), the i + 1st, . . . i + ρth rows of S−1 are
chosen to be real (resp. complex) eigenvectors of AT that form a basis of the
eigenspace corresponding to α. Moreover, if α is complex, the hD(i + s)th row
of S−1 is chosen to be the component-wise complex conjugate of the i + sth row
of S−1, for all s ∈{1, . . . ρ}.
Deﬁnition 3. Let A = SDS−1 be a k×k real diagonalizable matrix. We say that
E = (ε1, . . . εk) ∈Rk is a perturbation w.r.t. D if εi ̸= 0 and εi = εhD(i) for all
i ∈{1, . . . k}. Further, the E-perturbed variant of A is the matrix A′ = SD′S−1,
where D′ is the k×k diagonal matrix with D′[i, i] = εiD[i, i] for all i ∈{1, . . . k}.
In the following, we omit ”w.r.t. D” and simply say ”E is a perturbation”, when
D is clear from the context. Clearly, A′ as deﬁned above is a diagonalizable
matrix and its eigenvalues are given by the diagonal elements of D′.
Recall that the diagonal of D is partitioned into αi-segments, where each αi is
an eigenvalue of A = SDS−1 with algebraic multiplicity ρi. We now use a similar
idea to segment a perturbation E w.r.t. D. Speciﬁcally, the ﬁrst ρ1 elements of
E constitute the α1-segment of E, the next ρ2 elements of E constitute the α2-
segment of E and so on.
Deﬁnition 4. A perturbation E = (ε1, . . . εk) is said to be segmented if the jth
element (whenever present) in every segment of E has the same value, for all
1 ≤j ≤ρ1. Formally, if i = l−1
s=1 ρs + j and 1 ≤j ≤ρl ≤ρ1, then εi = εj.

684
S. Akshay et al.
Clearly, the ﬁrst ρ1 elements of a segmented perturbation E deﬁne the whole
of E. As an example, suppose (α1, α1, α1, α2, α2, α2, α2, α3) is the diagonal of D,
where α1, α2, α2 and α3 are distinct eigenvalues of A. There are four segments
of the diagonal of D (and of E) of lengths 3, 2, 2 and 1 respectively.
Example segmented perturbations in this case are (ε1, ε2, ε3, ε1, ε2, ε1, ε2, ε1)
and (ε3, ε1, ε2, ε3, ε1, ε3, ε1, ε3). If ε1 ̸= ε2 or ε2 ̸= ε3, a perturbation that is not
segmented is E = (ε1, ε2, ε3, ε2, ε3, ε2, ε3, ε1).
Deﬁnition 5. Given a segmented perturbation E = (ε1, . . . εk) w.r.t. D, a rota-
tion of E, denoted τD(E), is the segmented perturbation E′ = (ε′
1, . . . ε′
k) in which
ε′
(i mod ρ1)+1 = εi for i ∈{1, . . . ρ1}, and all other ε′
is are as in Deﬁnition 4.
Continuing with our example, if E = (ε1, ε2, ε3, ε1, ε2, ε1, ε2, ε1), then τD(E) =
(ε3, ε1, ε2, ε3, ε1, ε3, ε1, ε3), τ 2
D(E) = (ε2, ε3, ε1, ε2, ε3, ε2, ε3, ε2) and τ 3
D(E) = E.
Lemma 4. Let A = SDS−1 be a k × k real diagonalizable matrix with eigen-
values αi of algebraic multiplicity ρi. Let E = (ε1, . . . εk) be a segmented per-
turbation w.r.t. D such that all εjs have the same sign, and let Au denote
the τ u
D(E)-perturbed variant of A for 0 ≤u < ρ1, where τ 0(E) = E. Then
An =
1
 ρ1
j=1 εn
j
 ρ1−1
u=0 An
u, for all n ≥1.
Proof. Let Eu denote τ u
D(E) for 0 ≤u < ρ1, and let Eu[i] denote the ith element of
Eu for 1 ≤i ≤k. It follows from Deﬁnitions 4 and 5 that for each i, j ∈{1, . . . ρ1},
there is a unique u ∈{0, . . . ρ1 −1} such that Eu[i] = εj. Speciﬁcally, u = i −j
if i ≥j, and u = (ρ1 −j) + i if i < j. Furthermore, Deﬁnition 4 ensures that the
above property holds not only for i ∈{1, . . . ρ1}, but for all i ∈{1, . . . k}.
Let Du denote the diagonal matrix with Du[i, i] = Eu[i]D[i, i] for 0 ≤i < ρ1.
Then Dn
u is the diagonal matrix with Dn
u[i, i] =

Eu[i]D[i, i]
n for all n ≥1.
It follows from the deﬁnition of Au that An
u = S Dn
u S−1 for 0 ≤u < ρ
and n ≥1. Therefore, ρ1−1
u=0 An
u = S
 ρ1−1
u=0 Dn
u

S−1. Now, ρ1−1
u=0 Dn
u is
a diagonal matrix whose ith element along the diagonal is ρ1−1
u=0

Eu[i]D[i, i]
n
=
 ρ1−1
u=0 En
u [i]

Dn[i, i]. By virtue of the property mentioned in the previous
paragraph, ρ1−1
u=0 En
u [i] = ρ1
j=1 εn
j for 1 ≤i ≤k. Therefore, ρ1−1
u=0 Dn
u =
 ρ1
j=1 εn
j

Dn, and hence, ρ1−1
u=0 An
u =
 ρ1
j=1 εn
j

S Dn S−1 =
 ρ1
j=1 εn
j

An.
Since all εjs have the same sign and are non-zero,
 ρ1
j=1 εn
j

is non-zero for all
n ≥1. It follows that An =
1
 ρ1
j=1 εn
j
 ρ1−1
u=0 An
u.
⊓⊔
We are now in a position to present the proof of the main result of this
section, i.e. of Theorem 5. Our proof uses a variation of the idea used in the
proof of Lemma 4 above.
Proof of Theorem 5. Consider a set {(w1, A1), . . . (wi, Ai)} of (weight, matrix)
pairs, where each matrix Ai is in Qk×k and each wi ∈Q. Suppose further that
each Ai = SiDiS−1
i
, where Di is a diagonal matrix with segments along the
diagonal arranged in descending order of algebraic multiplicities of the corre-
sponding eigenvalues. Let νi be the number of distinct eigenvalues of Ai, and

On Eventual Properties for Weighted Sum of Powers of Matrices
685
let these eigenvalues be αi,1, . . . αi,νi. Let μi be the largest algebraic multiplic-
ity among those of all eigenvalues of Ai, and let μ = lcm(μ1, . . . μm). We now
choose positive rationals ε1, . . . εμ such that (i) all εjs are distinct, and (ii) for
every i ∈{1, . . . m}, for every distinct j, l ∈{1, . . . νi} and for every distinct
p, q ∈{1, . . . μ}, we have εp
εq ̸= | αi,j
αi,l |. Since Q is a dense set, such a choice of
ε1, . . . εμ can always be made once all | αi,j
αi,l |s are known, even if within ﬁnite
precision bounds.
For 1 ≤i ≤m, let ηi denote μ/μi. We now deﬁne ηi distinct and segmented
perturbations w.r.t. Di as follows, and denote these as Ei,1, . . . Ei,ηi. For 1 ≤j ≤
ηi, the ﬁrst μi elements (i.e. the ﬁrst segment) of Ei,j are ε(j−1)μi+1, . . . εjμi (as
chosen in the previous paragraph), and all other elements of Ei,j are deﬁned as in
Deﬁnition 4. For each Ei,j thus obtained, we also consider its rotations τ u
Di(Ei,j)
for 0 ≤u < μi. For 1 ≤j ≤ηi and 0 ≤u < μi, let Ai,j,u = Si Di,j,u S−1
i
denote the τ u
Di(Ei,j)-perturbed variant of Ai. It follows from Deﬁnition 3 that
if we consider the set of diagonal matrices {Di,j,u | 1 ≤j ≤ηi, 0 ≤u < μi},
then for every p ∈{1, . . . k} and for every q ∈{1, . . . μ}, there is a unique u and
j such that Di,j,u[p, p] = εq. Speciﬁcally, j = ⌊q/μi⌋. To ﬁnd u, let Ei,j[p] be
the 	pth element in a segment of Ei,j, where 1 ≤	p ≤μi, and let 	q be q mod μi.
Then, u = (	p −	q) if 	p ≥	q and u = (μi −	q) + 	p otherwise. By our choice of
εts, we also know that for all i ∈{1, . . . m}, for all j, l ∈{1, . . . νi} and for all
p, q ∈{1, . . . μ}, we have εpαi,l ̸= εqαi,j unless p = q and j = l. This ensures that
all Di,j,u matrices, and hence all Ai,j,us matrices, are simple, i.e. have distinct
eigenvalues.
Using
the
reasoning
in
Lemma
4,
we
can
now
show
that
An
i
=
1
 μ
j=1 εn
j
 ×
 ηi
j=1
μi−1
u=0 An
i,j,u

and
so,
m
i=1 wiAn
i
=
1
 μ
j=1 εn
j
 ×
 m
i=1
ηi
j=1
μi−1
u=0 wiAn
i,j,u

. Since all εjs are positive reals, μ
j=1 εn
j is a pos-
itive real for all n ≥1.
Hence, for each p, q ∈{1, . . . k}, m
i=1 wiAn
i [p, q] is > 0, < 0 or = 0 if and
only if
 m
i=1
ηi
j=1
μi−1
u=0 wiAn
i,j,u[p, q]

is > 0, < 0 or = 0, respectively. The
only remaining helper result that is now needed to complete the proof of the
theorem is that each Ai,j,u is a real algebraic matrix. This is shown in Lemma 5,
presented at the end of this section to minimally disturb the ﬂow of arguments.
⊓⊔
The reduction in proof of Theorem 5 can be easily encoded as an algorithm,
as shown in Algorithm 1. Further, in addition to Corollary 2, there are other
consequences of our reduction. One such result (with proof in [3]) is below.
Corollary 3. Given A = {(w1, A1), . . . (wm, Am)}, where each wi ∈Q and Ai ∈
Qk×k is diagonalizable, and a real value ε > 0, there exists B = {(v1, B1),
. . . (vM, BM)}, where each vi ∈Q and each Bi ∈RAk×k is simple, such that



m
i=0 wiAn
i [p, q] −M
j=0 vjBn
j [p, q]



 < εn for all p, q ∈{1, . . . k} and all n ≥1.
We end this section with the promised helper result used at the end of the proof
of Theorem 5.

686
S. Akshay et al.
Algorithm 1. Reduction procedure for diagonalizable matrices
Input: A = {(wi, Ai) : 1 ≤i ≤m, wi ∈Q, Ai ∈Qk×k and diagonalizable}
Output: B = {(vi, Bi) : 1 ≤i ≤t, vi ∈Q, Bi ∈RAk×k are simple}
s.t.
m
i=1 wiAn
i

= f(n)
t
i=1 viBn
i

, where f(n) > 0 for all n ≥0?
1: P ←{1};
▷Initialize set of forbidden ratios of various εjs
2: for i in 1 through m do
▷For each matrix Ai
3:
Ri ←{(αi,j, ρi,j) : αi,j is eigenvalue of Ai with algebraic multiplicity ρi,j};
4:
Di ←Diagonal matrix of αi,j-segments ordered in decreasing order of ρi,j;
5:
Si ←Matrix of linearly independent eigenvectors of Ai s.t. Ai = SiDiS−1
i
;
6:
P ←P ∪

|αi,j/αi,l|
: αi,j, αi,l are eigenvalues in Ri

;
μi ←maxj ρi,j
7: μ = lcm(μ1, . . . μm);
▷Count of εjs needed
8: for j in 1 through μ do
▷Generate all required εjs
9:
Choose εj ∈Q s.t. εj > 0 and εj ̸∈{πεp : 1 ≤p < j, π ∈P};
10: B ←∅;
▷Initialize set of (weight, simple matrix) pairs
11: for i in 1 through m do
▷For each matrix Ai
12:
νi ←μ/μi;
▷Count of segmented perturbations to be rotated for Ai
13:
for j in 0 through νi −1 do
▷For each segmented perturbation
14:
Ei,j
←
Seg.
perturbn.
w.r.t.
Di
with
ﬁrst
μi
elements
being
εjμi+1, . . . ε(j+1)μi;
15:
for u in 0 through μi −1 do
▷For each rotation of Ei,j
16:
Ai,j,u ←τ u
Di(Ei,j)-perturbed variant of A;
17:
B ←B ∪{(wi, Ai,j,u)};
▷Update A′
18: return B;
Lemma 5. For every real (resp. real algebraic) diagonalizable matrix A =
SDS−1 and perturbation E
∈Rk (resp. RAk), the E-perturbed variant of A
is a real (resp. real algebraic) diagonalizable matrix.
Proof. We ﬁrst consider the case of A ∈Rk×k and E ∈Rk. Given a perturbation
E w.r.t. D, we ﬁrst deﬁne k simple perturbations Ei (1 ≤i ≤k) w.r.t. D as
follows: Ei has all its components set to 1, except for the ith component, which
is set to εi. Furthermore, if D[i, i] is not real, then the hD(i)th component of Ei
is also set to εi. It is easy to see from Deﬁnition 3 that each Ei is a perturbation
w.r.t. D. Moreover, if j = hD(i), then Ej = Ei.
Let 	E = {Ei1, . . . Eiu} be the set of all unique perturbations w.r.t D among
E1, . . . Ek. It follows once again from Deﬁnition 3 that the E-perturbed variant of
A can be obtained by a sequence of Eij-perturbations, where Eij ∈	E. Speciﬁcally,
let A0, E = A and Av, E be the Eiv-perturbed variant of Av−1, E for all v ∈{1, . . . u}.
Then, the E-perturbed variant of A is identical to Au, E. This shows that it suﬃces
to prove the lemma only for simple perturbations Ei, as deﬁned above. We focus
on this special case below.
Let A′ = SD′S−1 be the Ei-perturbed variant of A, and let D[i, i] = α.
For every p ∈{1, . . . k}, let ep denote the p-dimensional unit vector whose pth
component is 1. Then, A′ep gives the pth column of A′. We prove the ﬁrst part of
the lemma by showing that A′ ep = (S D ′S−1) ep ∈Rk×1 for all p ∈{1, . . . k}.

On Eventual Properties for Weighted Sum of Powers of Matrices
687
Let T denote D′ S−1 ep. Then T is a column vector with T[r] =
D′[r, r] S−1[r, p] for all r ∈{1, . . . k}. Let U denote ST. By deﬁnition, U is
the pth column of the matrix A′. To compute U, recall that the rows of S−1
form a basis of Ck. Therefore, for every q ∈{1, . . . k}, S−1 eq can be viewed as
transforming the basis of the unit vector eq to that given by the rows of S−1
(modulo possible scaling by real scalars denoting the lengths of the row vectors of
S−1). Similarly, computation of U = ST can be viewed as applying the inverse
basis transformation to T. It follows that the components of U can be obtained
by computing the dot product of T and the transformed unit vector S−1 eq, for
each q ∈{1, . . . k}. In other words, U[q] = T · (S−1 eq). We show below that
each such U[q] is real.
By deﬁnition, U[q] = k
r=1(T[r] S−1[r, q]) = k
r=1(D′[r, r] S−1[r, p] S−1
[r, q]). We consider two cases below.
– If D[i, i] = α is real, recalling the deﬁnition of D′, the expression for U[q]
simpliﬁes to k
r=1(D[r, r] S−1[r, p] S−1[r, q]) + (εi −1) α S−1[i, p] S−1[i, q].
Note that k
r=1(D[r, r] S−1[r, p] S−1[r, q]) is the qth component of the vector
(SDS−1) ep = A ep. Since A is real, so must be the qth component of A ep.
Moreover, since α is real, by our choice of S−1, both S−1[i, p] and S−1[i, q]
are real. Since εi is also real, it follows that (εi −1) α S−1[i, p] S−1[i, q] is
real. Hence U[q] is real for all q ∈{1, . . . k}.
– If D[i, i] = α is not real, from Deﬁnition 3, we know that D′[i, i] =
εi α and D′[hD(i), hD(i)] = εi α. The expression for U[q] then simpli-
ﬁes to k
r=1

D[r, r] S−1[r, p] S−1[r, q]

+ (εi −1) (β + γ), where β =
α S−1[i, p] S−1[i, q] and γ = α S−1[hD(i), p] S−1[hD(i), q]. By our choice
of S−1, we know that S−1[hD(i), p] = S−1[i, p] and S−1[hD(i), q] = S−1[i, q].
Therefore, β = γ and hence (εi −1) (β + γ) is real. By a similar argument as
in the previous case, it follows that U[q] is real for all q ∈{1, . . . k}.
The proof when A ∈RAk×k and E ∈Qk follows from a similar reasoning as
above, and from the following facts about real algebraic matrices.
– If A is a real algebraic matrix, then every eigenvalue of A is either a real or
complex algebraic number.
– If A is diagonalizable, then for every real (resp. complex) algebraic eigenvalue
of A, there exists a set of real (resp. complex) algebraic eigenvectors that form
a basis of the corresponding eigenspace.
⊓⊔
6
Conclusion
In this paper, we investigated eventual non-negativity and positivity for matrices
and the weighted sum of powers of matrices (ENNSoM/EPSoM). First, we showed
reductions from and to speciﬁc problems on linear recurrences, which allowed us
give complexity lower and upper bounds. Second, we developed a new and generic
perturbation-based reduction technique from simple matrices to diagonalizable
matrices, which allowed us to transfer results between these settings.

688
S. Akshay et al.
Most of our results, that we showed in the rational setting, hold even with
real-algebraic matrices by adapting the complexity notions and depending on
corresponding results for ultimate positivity for linear recurrences and related
problems over reals. As future work, we would like to extend our techniques for
other problems of interest like the existence of a matrix power where all entries
are non-negative or zero. Finally, the line of work started here could lead to
eﬀective algorithms and applications in varied areas ranging from control theory
systems to cyber-physical systems, where eventual properties of matrices play a
crucial role.
References
1. Akshay, S., Antonopoulos, T., Ouaknine, J., Worrell, J.: Reachability problems for
Markov chains. Inf. Process. Lett. 115(2), 155–158 (2015)
2. Akshay, S., Balaji, N., Murhekar, A., Varma, R., Vyas, N.: Near optimal complexity
bounds for fragments of the Skolem problem. In: Paul, S., Bl¨aser, M. (eds.) 37th
International Symposium on Theoretical Aspects of Computer Science, STACS
2020, 10–13 March 2020, Montpellier, France, volume 154 of LIPIcs, pp. 37:1–
37:18. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik (2020)
3. Akshay, S., Chakraborty, S., Pal, D.: On eventual non-negativity and positivity for
the weighted sum of powers of matrices. arXiv preprint arXiv:2205.09190 (2022)
4. Akshay, S., Genest, B., Karelovic, B., Vyas, N.: On regularity of unary probabilis-
tic automata. In: 33rd Symposium on Theoretical Aspects of Computer Science,
STACS 2016, 17–20 February 2016, Orl´eans, France, volume 47 of LIPIcs, pp.
8:1–8:14. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik (2016)
5. S. Akshay, Blaise Genest, and Nikhil Vyas. Distribution based objectives for
Markov decision processes. In: 33rd Symposium on Logic in Computer Science
(LICS 2018), vol. IEEE, pp. 36–45 (2018)
6. Almagor, S., Boker, U., Kupferman, O.: What’s decidable about weighted
automata? Inf. Comput. 282, 104651 (2020)
7. Almagor, S., Karimov, T., Kelmendi, E., Ouaknine, J., Worrell, J.: Deciding ω-
regular properties on linear recurrence sequences. Proc. ACM Program. Lang.
5(POPL), 1–24 (2021)
8. Barloy, C., Fijalkow, N., Lhote, N., Mazowiecki, F.: A robust class of linear recur-
rence sequences. In: Fern´andez, M., Muscholl, A. (eds.) 28th EACSL Annual Con-
ference on Computer Science Logic, CSL 2020, 13–16 January 2020, Barcelona,
Spain, volume 152 of LIPIcs, pp. 9:1–9:16. Schloss Dagstuhl - Leibniz-Zentrum f¨ur
Informatik (2020)
9. Bell, P.C., Hirvensalo, M., Potapov, I.: Mortality for 2 × 2 matrices is NP-hard.
In: Rovan, B., Sassone, V., Widmayer, P. (eds.) MFCS 2012. LNCS, vol. 7464, pp.
148–159. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-32589-
2 16
10. Bell, P.C., Potapov, I., Semukhin, P.: On the mortality problem: from multiplica-
tive matrix equations to linear recurrence sequences and beyond. Inf. Comput.
281, 104736 (2021)
11. Bell, P.C., Semukhin, P.: Decision questions for probabilistic automata on small
alphabets. arXiv preprint arXiv:2105.10293 (2021)
12. Blondel, V.D., Canterini, V.: Undecidable problems for probabilistic automata of
ﬁxed dimension. Theory Comput. Syst. 36(3), 231–245 (2003). https://doi.org/10.
1007/s00224-003-1061-2

On Eventual Properties for Weighted Sum of Powers of Matrices
689
13. Naqvi, S.C., McDonald, J.J.: Eventually nonnegative matrices are similar to semi-
nonnegative matrices. Linear Algebra Appl. 381, 245–258 (2004)
14. Everest, G., van der Poorten, A., Shparlinski, I., Ward, T.: Recurrence Sequences.
Mathematical Surveys and Monographs, American Mathematical Society, United
States (2003)
15. Fijalkow, N., Ouaknine, J., Pouly, A., Sousa-Pinto, J., Worrell, J.: On the decid-
ability of reachability in linear time-invariant systems. In: Ozay, N., Prabhakar, P.
(eds.) Proceedings of the 22nd ACM International Conference on Hybrid Systems:
Computation and Control, HSCC 2019, Montreal, QC, Canada, 16–18 April 2019,
pages 77–86. ACM (2019)
16. Friedland, S.: On an inverse problem for nonnegative and eventually nonnegative
matrices. Isr. J. Math. 29(1), 43–60 (1978). https://doi.org/10.1007/BF02760401
17. Gimbert, H., Oualhadj, Y.: Probabilistic automata on ﬁnite words: decidable and
undecidable problems. In: Abramsky, S., Gavoille, C., Kirchner, C., Meyer auf
der Heide, F., Spirakis, P.G. (eds.) ICALP 2010. LNCS, vol. 6199, pp. 527–538.
Springer, . Probabilistic automata on ﬁnite words: Decidable and undecidable prob-
lems (2010). https://doi.org/10.1007/978-3-642-14162-1 44
18. Halava, V., Harju, T., Hirvensalo, M.: Positivity of second order linear recurrent
sequences. Discrete Appl. Math. 154(3), 447–451 (2006)
19. Halava, V., Harju, T., Hirvensalo, M., Karhum¨aki, J.: Skolem’s Problem-on the
Border Between Decidability and Undecidability. Technical report, Citeseer (2005)
20. Karimov, T., et al.: What’s decidable about linear loops? Proc. ACM Program.
Lang. 6(POPL), 1–25 (2022)
21. Korthikanti, V.A., Viswanathan, M., Agha, G., Kwon, Y.: Reasoning about MDPs
as transformers of probability distributions. In: QEST 2010, Seventh International
Conference on the Quantitative Evaluation of Systems, Williamsburg, Virginia,
USA, 15–18 September 2010, pp. 199–208. IEEE Computer Society (2010)
22. Lale, S., Azizzadenesheli, K., Hassibi, B., Anandkumar, A.: Logarithmic regret
bound in partially observable linear dynamical systems. Adv. Neural Inf. Process.
Syst. 33, 20876–20888 (2020)
23. Lebacque, J.P., Ma, T.Y., Khoshyaran, M.M.: The cross-entropy ﬁeld for multi-
modal dynamic assignment. In: Proceedings of Traﬃc and Granular Flow 2009
(2009)
24. MacCluer, C.R.: The many proofs and applications of Perron’s theorem. Siam Rev.
42(3), 487–498 (2000)
25. Noutsos, D.: On Perron-Frobenius property of matrices having some negative
entries. Linear Algebra Appl. 412(2), 132–153 (2006)
26. Noutsos, D., Tsatsomeros, M.J.: Reachability and holdability of nonnegative states.
SIAM J. Matrix Anal. Appl. 30(2), 700–712 (2008)
27. Ouaknine, J.: Decision problems for linear recurrence sequences. In: G asieniec, L.,
Wolter, F. (eds.) FCT 2013. LNCS, vol. 8070, pp. 2–2. Springer, Heidelberg (2013).
https://doi.org/10.1007/978-3-642-40164-0 2
28. Ouaknine, J., Pinto, J.S., Worrell, J.: On termination of integer linear loops. In:
Indyk, R. (ed.) Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2015, San Diego, CA, USA, 4–6 January 2015, pp.
957–969. SIAM (2015)
29. Ouaknine, J., Worrell, J.: Decision problems for linear recurrence sequences. In:
Finkel, A., Leroux, J., Potapov, I. (eds.) RP 2012. LNCS, vol. 7550, pp. 21–28.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-33512-9 3

690
S. Akshay et al.
30. Ouaknine, J., Worrell, J.: Positivity problems for low-order linear recurrence
sequences. In: Chekuri, C. (ed.) Proceedings of the Twenty-Fifth Annual ACM-
SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Oregon, USA,
5–7 January 2014, pp. 366–379. SIAM (2014)
31. Ouaknine, J., Worrell, J.: Ultimate positivity is decidable for simple linear recur-
rence sequences. In: Esparza, J., Fraigniaud, P., Husfeldt, T., Koutsoupias, E. (eds.)
ICALP 2014. LNCS, vol. 8573, pp. 330–341. Springer, Heidelberg (2014). https://
doi.org/10.1007/978-3-662-43951-7 28
32. Ouaknine, J., Worrell, J.: On linear recurrence sequences and loop termination.
ACM Siglog News 2(2), 4–13 (2015)
33. Pan, V.Y., Chen, Z.Q.: The complexity of the matrix eigenproblem. In: Proceedings
of the Thirty-First Annual ACM Symposium on Theory of Computing, STOC
2099, pp. 507–516, New York, NY, USA. Association for Computing Machinery
(1999)
34. Rump, S.M.: Perron-Frobenius theory for complex matrices. Linear Algebra Appl.
363, 251–273 (2003)
35. Akshay, S., Balaji, N., Vyas, N.: Complexity of Restricted Variants of Skolem
and Related Problems. In Larsen, K.G., Bodlaender, H.L., Raskin, J.F. (eds.)
42nd International Symposium on Mathematical Foundations of Computer Sci-
ence (MFCS 2017), volume 83 of Leibniz International Proceedings in Informatics
(LIPIcs), pp. 78:1–78:14, Dagstuhl, Germany. Schloss Dagstuhl-Leibniz-Zentrum
fuer Informatik (2017)
36. Tiwari, A.: Termination of linear programs. In: Alur, R., Peled, D.A. (eds.) CAV
2004. LNCS, vol. 3114, pp. 70–82. Springer, Heidelberg (2004). https://doi.org/10.
1007/978-3-540-27813-9 6
37. Zaslavsky, B.G.: Eventually nonnegative realization of diﬀerence control systems.
Dyn. Syst. Relat. Top. Adv. Ser. Dynam. Syst. 9, 573–602 (1991)
38. Zaslavsky, B.G., McDonald, J.J.: Characterization of Jordan canonical forms which
are similar to eventually nonnegative matrices with the properties of nonnegative
matrices. Linear Algebra Appl. 372, 253–285 (2003)
39. Zhang, A., et al.: Learning causal state representations of partially observable
environments. arXiv preprint arXiv:1906.10437 (2019)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Decision Problems in a Logic for Reasoning
About Reconﬁgurable Distributed Systems
Marius Bozga(B)
, Lucas Bueri
, and Radu Iosif
Univ. Grenoble Alpes, CNRS, Grenoble INP, VERIMAG, 38000 Saint-Martin-d’H`eres, France
marius.bozga@univ-grenoble-alpes.fr
Abstract. We consider a logic used to describe sets of conﬁgurations of dis-
tributed systems, whose network topologies can be changed at runtime, by recon-
ﬁguration programs. The logic uses inductive deﬁnitions to describe networks
with an unbounded number of components and interactions, written using a mul-
tiplicative conjunction, reminiscent of Bunched Implications [37] and Separation
Logic [39]. We study the complexity of the satisﬁability and entailment prob-
lems for the conﬁguration logic under consideration. Additionally, we consider
the robustness property of degree boundedness (is every component involved in a
bounded number of interactions?), an ingredient for decidability of entailments.
1
Introduction
Distributed systems are increasingly used as critical parts of the infrastructure of our
digital society, as in e.g., datacenters, e-banking and social networking. In order to
address maintenance (e.g., replacement of faulty and obsolete network nodes by new
ones) and data trafﬁc issues (e.g., managing the trafﬁc inside a datacenter [35]), the
distributed systems community has recently put massive effort in designing algorithms
for reconﬁgurable systems, whose network topologies change at runtime [23]. How-
ever, dynamic reconﬁguration in the form of software or network upgrades has been
recognized as one of the most important sources of cloud service outage [25].
This paper contributes to a logical framework that addresses the timely problems of
formal modeling and veriﬁcation of reconﬁgurable distributed systems. The basic build-
ing blocks of this framework are (i) a Hoare-style program proof calculus [1] used to
write formal proofs of correctness of reconﬁguration programs, and (ii) an invariant syn-
thesis method [6] that proves the safety (i.e., absence of reachable error conﬁgurations)
of the conﬁgurations deﬁned by the assertions that annotate a reconﬁguration program.
These methods are combined to prove that an initially correct distributed system cannot
reach an error state, following the execution of a given reconﬁguration sequence.
The assertions of the proof calculus are written in a logic that deﬁnes inﬁnite sets
of conﬁgurations, consisting of components (i.e., processes running on different nodes
of the network) connected by interactions (i.e., multi-party channels alongside which
messages between components are transfered). Systems that share the same architec-
tural style (e.g., pipeline, ring, star, tree, etc.) and differ by the number of components
and interactions are described using inductively deﬁned predicates. Such conﬁgurations
can be modiﬁed either by (a) adding or removing components and interactions (recon-
ﬁguration), or (b) changing the local states of components, by ﬁring interactions.
c⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 691–711, 2022.
https://doi.org/10.1007/978-3-031-10769-6_40

692
M. Bozga et al.
The assertion logic views components and interactions as resources, that can be
created or deleted, in the spirit of resource logics `a la Bunched Implications [37], or
Separation Logic [39]. The main advantage of using resource logics is their support for
local reasoning [12]: reconﬁguration actions are speciﬁed by pre- and postconditions
mentioning only the resources involved, while framing out the rest of the conﬁguration.
The price to pay for this expressive power is the difﬁculty of automating the rea-
soning in these logics. This paper makes several contributions in the direction of proof
automation, by studying the complexity of the satisﬁability and entailment problems,
for the conﬁguration logic under consideration. Additionally, we study the complexity
of a robustness property [27], namely degree boundedness (is every component involved
in a bounded number of interactions?). In particular, the latter problem is used as a
prerequisite for deﬁning a fragment with a decidable entailment problem. For space
reasons, the proofs of the technical results are given in [5].
1.1
Motivating Example
The logic studied in this paper is motivated by the need for an assertion language
that supports reasoning about dynamic reconﬁgurations in a distributed system. For
instance, consider a distributed system consisting of a ﬁnite (but unknown) number of
components (processes) placed in a ring, executing the same ﬁnite-state program and
communicating via interactions that connect the out port of a component to the in port
of its right neighbour, in a round-robin fashion, as in Fig. 1(a). The behavior of a com-
ponent is a machine with two states, T and H, denoting whether the component has a
token (T) or not (H). A component ci without a token may receive one, by executing a
transition H in
−→T, simultaneously with its left neighbour cj, that executes the transition
T out
−→H. Then, we say that the interaction (cj,out,ci,in) has ﬁred, moving a token one
position to the right in the ring. Note that there can be more than one token, moving
independently in the system, as long as no token overtakes another token.
The token ring system is formally speciﬁed by the following inductive rules:
ringh,t(x) ←∃y∃z . [x]@q∗⟨x.out, z.in⟩∗chainh′,t′(z,y)∗⟨y.out, x.in⟩
chainh,t(x, y) ←∃z. [x]@q∗⟨x.out, z.in⟩∗chainh′,t′(z,y)
chain0,1(x, x) ←[x]@T
chain1,0(x, x) ←[x]@H
chain0,0(x, x) ←[x]
where h′ def=

max(h−1,0) , if q = H
h
, if q = T and t′ def=

max(t −1,0) , if q = T
t
, if q = H
The predicate ringh,t(x) describes a ring with at least two components, such that at least
h (resp. t) components are in state H (resp. T). The ring consists of a component x in
state q, described by the formula [x]@q, an interaction from the out port of x to the
in port of another component z, described as ⟨x.out, z.in⟩, a separate chain of compo-
nents stretching from z to y (chainh′,t′(z,y)), and an interaction connecting the out port
of component y to the in port of component x (⟨y.out, x.in⟩). Inductively, a chain con-
sists of a component [x]@q, an interaction ⟨x.out, z.in⟩and a separate chainh′,t′(z,y).
Figure 1(b) depicts the unfolding of the inductive deﬁnition of the token ring, with the

Decision Problems in a Logic for Reasoning
693
c1
[x]
H
[z1]
T
⟨y.out,x.in⟩
⟨z1.out,z2.in⟩
chainh−1,t−1(z2,y)
⟨x.out,z1.in⟩
chainh−1,t (z1,y)
ringh,t (x)
(b)
···
out
out
out
(a)
in
out
in
out
T
in
out
T
in
in
out
in
out
T
cn
cn−1
in
in
c2
H
T
H
H
H
(c)
Fig. 1. Inductive Speciﬁcation and Reconﬁguration of a Token Ring
existentially quantiﬁed variables z from the above rules α-renamed to z1,z2,... to avoid
confusion.
A reconﬁguration program takes as input a mapping of program variables to com-
ponents and executes a sequence of basic operations i.e., component/interaction cre-
ation/deletion, involving the components and interactions denoted by these variables.
For instance, the reconﬁguration program in Fig. 1(c) takes as input three adjacent com-
ponents, mapped to the variables x, y and z, respectively, removes the component y
together with its left and right interactions and reconnects x directly with z. Program-
ming reconﬁgurations is error-prone, because the interleaving between reconﬁguration
actions and interactions in a distributed system may lead to bugs that are hard to trace.
For instance, if a reconﬁguration program removes the last component in state T (resp.
H) from the system, no token transfer interaction may ﬁre and the system deadlocks.
We prove absence of such errors using a Hoare-style proof system [1], based on
the logic introduced above as assertion language. For instance, the proof from Fig.
1(c) shows that the reconﬁguration sequence applied to a component y in state H (i.e.,
[y]@H) in a ring with at least h ≥2 components in state H and at least t ≥1 components
in state T leads to a ring with at least h −1 components in state H and at least t in
state T; note that the states of the components may change during the execution of the
reconﬁguration program, as tokens are moved by interactions.
The proof in Fig. 1(c) uses local axioms specifying, for each basic operation,
only those components and interactions required to avoid faulting, with a frame rule
{φ} P {ψ} ⇒{φ∗




F } P {ψ∗F}; for readability, the frame formulæ (from the pre-
conditions of the conclusion of the frame rule applications) are enclosed in boxes.
The proof also uses the consequence rule {φ} P {ψ} ⇒{φ′} P {ψ′} that applies if
φ′ is stronger than φ and ψ′ is weaker than ψ. The side conditions of the consequence
rule require checking the validity of the entailments ringh,t(y) |= ∃x∃z . ⟨x.out, y.in⟩∗
[y]@H∗⟨y.out, z.in⟩∗chainh−1,t(z,x) and chainh−1,t(z, x)∗⟨x.out, z.in⟩|= ringh−1,t(z),

694
M. Bozga et al.
for all h ≥2 and t ≥1. These side conditions can be automatically discharged using the
results on the decidability of entailments given in this paper. Additionally, checking the
satisﬁability of a precondition is used to detect trivially valid Hoare triples.
1.2
Related Work
Formal modeling coordinating architectures of component-based systems has received
lots of attention, with the development of architecture description languages (ADL),
such as BIP [3] or REO [2]. Many such ADLs have extensions that describe pro-
grammed reconﬁguration, e.g., [19,30], classiﬁed according to the underlying formal-
ism used to deﬁne their operational semantics: process algebras [13,33], graph rewrit-
ing [32,41,44], chemical reactions [43] (see the surveys [7,11]). Unfortunately, only
few ADLs support formal veriﬁcation, mainly in the ﬂavour of runtime veriﬁcation
[10,17,20,31] or ﬁnite-state model checking [14].
Parameterized veriﬁcation of unbounded networks of distributed processes uses
mostly hard-coded coordinating architectures (see [4] for a survey). A ﬁrst attempt at
specifying architectures by logic is the interaction logic of Konnov et al. [29], a combi-
nation of Presburger arithmetic with monadic uninterpreted function symbols, that can
describe cliques, stars and rings. More structured architectures (pipelines and trees) can
be described using a second-order extension [34]. However, these interaction logics are
undecidable and lack support for automated reasoning.
Specifying parameterized component-based systems by inductive deﬁnitions is not
new. Network grammars [26,32,40] use context-free grammar rules to describe sys-
tems with linear (pipeline, token-ring) architectures obtained by composition of an
unbounded number of processes. In contrast, we use predicates of unrestricted arities
to describe architectural styles that are, in general, more complex than trees. Moreover,
we write inductive deﬁnitions using a resource logic, suitable also for writing Hoare
logic proofs of reconﬁguration programs, based on local reasoning [12].
Local reasoning about concurrent programs has been traditionally the focus of Con-
current Separation Logic (CSL), based on a parallel composition rule [36], initially
with a non-interfering (race-free) semantics [8] and later combining ideas of assume-
and rely-guarantee [28,38] with local reasoning [22,42] and abstract notions of fram-
ing [15,16,21]. However, the body of work on CSL deals almost entirely with shared-
memory multithreading programs, instead of distributed systems, which is the aim of
our work. In contrast, we develop a resource logic in which the processes do not just
share and own resources, but become mutable resources themselves.
The techniques developed in this paper are inspired by existing techniques for sim-
ilar problems in the context of Separation Logic (SL) [39]. For instance, we use an
abstract domain similar to the one deﬁned by Brotherston et al. [9] for checking satis-
ﬁability of symbolic heaps in SL and reduce a fragment of the entailment problem in
our logic to SL entailment [18]. In particular, the use of existing automated reasoning
techniques for SL has pointed out several differences between the expressiveness of our
logic and that of SL. First, the conﬁguration logic describes hypergraph structures, in
which edges are ℓ-tuples for ℓ≥2, instead of directed graphs as in SL, where ℓis a
parameter of the problem: considering ℓto be a constant strictly decreases the com-
plexity of the problem. Second, the degree (number of hyperedges containing a given

Decision Problems in a Logic for Reasoning
695
vertex) is unbounded, unlike in SL, where the degree of heaps is constant. Therefore,
we dedicate an entire section (Sect. 4) to the problem of deciding the existence of a
bound (and computing a cut-off) on the degree of the models of a formula, used as a
prerequisite for the encoding of the entailment problems from the conﬁguration logic
as SL entailments.
2
Deﬁnitions
We denote by N the set of positive integers including zero. For a set A, we deﬁne A1 def= A,
Ai+1 def= Ai ×A, for all i ≥1, and A+ = 
i≥1 Ai, where × denotes the Cartesian product.
We denote by pow(A) the powerset of A and by mpow(A) the power-multiset (set of
multisets) of A. The cardinality of a ﬁnite set A is denoted as ||A||. By writing A ⊆ﬁn B
we mean that A is a ﬁnite subset of B. Given integers i and j, we write [i, j] for the
set {i,i+1,..., j}, assumed to be empty if i > j. For a tuple t = ⟨t1,...,tn⟩, we deﬁne
|t|
def= n, ⟨t⟩i
def= ti and ⟨t⟩[i, j]
def= ⟨ti,...,t j⟩. By writing x = poly(y), for given x,y ∈N, we
mean that there exists a polynomial function f : N →N, such that x ≤f(y).
2.1
Conﬁgurations
We model distributed systems as hypergraphs, whose vertices are components (i.e., the
nodes of the network) and hyperedges are interactions (i.e., describing the way the
components communicate with each other). The components are taken from a countably
inﬁnite set C, called the universe. We consider that each component executes its own
copy of the same behavior, represented as a ﬁnite-state machine B = (P,Q ,−→), where
P is a ﬁnite set of ports, Q is a ﬁnite set of states and −→⊆Q ×P ×Q is a transition
relation. Intuitively, each transition q
p−→q′ of the behavior is triggerred by a visible
event, represented by the port p. For instance, the behavior of the components of the
token ring system from Fig. 1(a) is B = ({in,out},{H,T},{H in
−→T,T out
−→H}). The
universe C and the behavior B = (P,Q ,−→) are ﬁxed in the rest of this paper.
We introduce a logic for describing inﬁnite sets of conﬁgurations of distributed
systems with unboundedly many components and interactions. A conﬁguration is a
snapshot of the system, describing the topology of the network (i.e., the set of present
components and interactions) together with the local state of each component:
Deﬁnition 1. A conﬁguration is a tuple γ = (C,I,ρ), where:
– C ⊆ﬁn C is a ﬁnite set of components, that are present in the conﬁguration,
– I ⊆ﬁn (C×P)+ is a ﬁnite set of interactions, where each interaction is a sequence
(c1, p1,...,cn, pn) ∈(C×P)n that binds together the ports p1,..., pn of the pairwise
distinct components c1,...,cn, respectively.
– ρ : C →Q is a state map associating each (possibly absent) component, a state of
the behavior B, such that the set {c ∈C | ρ(c) = q} is inﬁnite, for each q ∈Q .
The last condition requires that there is an inﬁnite pool of components in each state
q ∈Q ; since C is inﬁnite and Q is ﬁnite, this condition is feasible. For example, the con-
ﬁgurations of the token ring from Fig. 1(a) are ({c1,...,cn},{(ci,out,c(i mod n)+1,in) |

696
M. Bozga et al.
i ∈[1,n]},ρ), where ρ : C →{H,T} is a state map. The ring topology is described by
the set of components {c1,...,cn} and interactions {(ci,out,c(i mod n)+1,in) | i ∈[1,n]}.
Intuitively, an interaction (c1, p1,...,cn, pn) synchronizes transitions labeled by the
ports p1,..., pn from the behaviors (i.e., replicas of the state machine B) of c1,...,cn,
respectively. Note that the components ci are not necessary part of the conﬁguration.
The interactions are classiﬁed according to their sequence of ports, called the interac-
tion type and let Inter
def= P + be the set of interaction types; an interaction type models,
for instance, the passing of a certain kind of message (e.g., request, acknowledgement,
etc.). From an operational point of view, two interactions that differ by a permutation
of indices e.g., (c1, p1,...,cn, pn) and (ci1, pi1,...,cin, pin) such that {i1,...,in} = [1,n],
are equivalent, since the set of transitions is the same; nevertheless, we chose to distin-
guish them in the following, exclusively for reasons of simplicity.
Below we deﬁne the composition of conﬁgurations, as the union of disjoint sets of
components and interactions:
Deﬁnition 2. The composition of two conﬁgurations γi = (Ci,Ii,ρ), for i = 1,2, such
that C1 ∩C2 = /0 and I1 ∩I2 = /0, is deﬁned as γ1 • γ2
def= (C1 ∪C2,I1 ∪I2,ρ). The com-
position γ1 •γ2 is undeﬁned if C1 ∩C2 ̸= /0 or I1 ∩I2 ̸= /0.
In analogy with graphs, the degree of a conﬁguration is the maximum number of inter-
actions from the conﬁguration that involve a (possibly absent) component:
Deﬁnition 3. The degree of a conﬁguration γ = (C,I,ρ) is deﬁned as δ(γ)
def=
maxc∈C δc(γ), where δc(γ)
def= ||{(c1, p1,...,cn, pn) ∈I | c = ci, i ∈[1,n]}||.
For instance, the conﬁguration of the system from Fig. 1(a) has degree two.
2.2
Conﬁguration Logic
Let V and A be countably inﬁnite sets of variables and predicates, respectively. For
each predicate A ∈A, we denote its arity by #A. The formulæ of the Conﬁguration
Logic (CL) are described inductively by the following syntax:
φ := emp | [x] | ⟨x1.p1,...,xn.pn⟩| x@q | x = y | x ̸= y | A(x1,...,x#A) | φ∗φ | ∃x . φ
where x,y,x1,... ∈V, q ∈Q and A ∈A. A formula [x], ⟨x1.p1,...,xn.pn⟩, x@q and
A(x1,...,x#A) is called a component, interaction, state and predicate atom, respectively.
These formulæ are also referred to as atoms. The connective ∗is called the separating
conjunction. We use the shorthand [x]@q
def= [x] ∗x@q. For instance, the formula [x]@q ∗
[y]@q′ ∗⟨x.out, y.in⟩∗⟨x.in, y.out⟩describes a conﬁguration consisting of two distinct
components, denoted by the values of x and y, in states q and q′, respectively, and two
interactions binding the out port of one to the in port of the other component.
A formula is said to be pure if and only if it is a separating conjunction of state
atoms, equalities and disequalities. A formula with no occurrences of predicate atoms
(resp. existential quantiﬁers) is called predicate-free (resp. quantiﬁer-free). A variable
is free if it does not occur within the scope of an existential quantiﬁer ; we note fv(φ) the
set of free variables of φ. A sentence is a formula with no free variables. A substitution

Decision Problems in a Logic for Reasoning
697
φ[x1/y1 ...xn/yn] replaces simultaneously every free occurrence of xi by yi in φ, for all
i ∈[1,n]. Before deﬁning the semantics of CL formulæ, we introduce the set of inductive
deﬁnitions that assigns meaning to predicates:
Deﬁnition 4. A set of inductive deﬁnitions (SID) Δ consists of rules A(x1,...,x#A) ←
φ, where x1,...,x#A are pairwise distinct variables, called parameters, such that fv(φ) ⊆
{x1,...,x#A}. The rule A(x1,...,x#A) ←φ deﬁnes A and we denote by defΔ(A) the set
of rules from Δ that deﬁne A.
Note that having distinct parameters in a rule is without loss of generality, as e.g., a rule
A(x1,x1) ←φ can be equivalently written as A(x1,x2) ←x1 = x2 ∗φ. As a convention,
we shall always use the names x1,...,x#A for the parameters of a rule that deﬁnes A.
The semantics of CL formulæ is deﬁned by a satisfaction relation γ |=ν
Δ φ between
conﬁgurations and formulæ. This relation is parameterized by a store ν : V →C map-
ping the free variables of a formula into components from the universe (possibly absent
from γ) and an SID Δ. We write ν[x ←c] for the store that maps x into c and agrees with
ν on all variables other than x. The deﬁnition of the satisfaction relation is by induction
on the structure of formulæ, where γ = (C,I,ρ) is a conﬁguration (Deﬁnition 1):
γ |=ν
Δ emp
⇐⇒C = /0 and I = /0
γ |=ν
Δ [x]
⇐⇒C = {ν(x)} and I = /0
γ |=ν
Δ ⟨x1.p1,...,xn.pn⟩⇐⇒C = /0 and I = {(ν(x1), p1,...,ν(xn), pn)}
γ |=ν
Δ x@q
⇐⇒γ |=ν
Δ emp and ρ(ν(x)) = q
γ |=ν
Δ x ∼y
⇐⇒γ |=ν
Δ emp and ν(x) ∼ν(y), for all ∼∈{=,̸=}
γ |=ν
Δ A(y1,...,y#A)
⇐⇒γ |=ν
Δ φ[x1/y1,...,x#A/y#A], for some rule
A(x1,...,x#A) ←φ from Δ
γ |=ν
Δ φ1 ∗φ2
⇐⇒exist γ1,γ2, such that γ = γ1 •γ2 and γi |=ν
Δ φi, for i = 1,2
γ |=ν
Δ ∃x . φ
⇐⇒γ |=ν[x←c]
Δ
φ, for some c ∈C
If φ is a sentence, the satisfaction relation γ |=ν
Δ φ does not depend on the store, written
γ |=Δ φ, in which case we say that γ is a model of φ. If φ is a predicate-free formula, the
satisfaction relation does not depend on the SID, written γ |=ν φ. A formula φ is satisﬁ-
able if and only if the sentence ∃x1 ...∃xn . φ has a model, where fv(φ) = {x1,...,xn}.
A formula φ entails a formula ψ, written φ |=Δ ψ if and only if, for any conﬁguration γ
and store ν, we have γ |=ν
Δ φ only if γ |=ν
Δ ψ.
2.3
Separation Logic
Separation Logic (SL) [39] will be used in the following to prove several technical
results concerning the decidability and complexity of certain decision problems for
CL. For self-containment reasons, we deﬁne SL below. The syntax of SL formulæ is
described by the following grammar:
φ := emp | x0 →(x1,...,xK) | x = y | x ̸= y | A(x1,...,x#A) | φ∗φ | ∃x . φ
where x,y,x0,x1,... ∈V, A ∈A and K ≥1 is an integer constant. Formulæ of SL are
interpreted over ﬁnite partial functions h : C ⇀ﬁn CK, called heaps1, by a satisfaction
relation h ⊩ν φ, deﬁned inductively as follows:
1 We use the universe C here for simplicity, the deﬁnition works with any countably inﬁnite set.

698
M. Bozga et al.
h ⊩ν
Δ emp
⇐⇒h = /0
h ⊩ν
Δ x0 →(x1,...,xK) ⇐⇒dom(h) = {ν(x0)} and h(ν(x0)) = ⟨ν(x1),...,ν(xK)⟩
h ⊩ν φ1 ∗φ2
⇐⇒there exist h1,h2 such that dom(h1)∩dom(h2) = /0,
h = h1 ∪h2 and hi ⊩ν
Δ φi, for both i = 1,2
where dom(h)
def= {c ∈C | h(c) is deﬁned} is the domain of the heap and (dis-) equali-
ties, predicate atoms and existential quantiﬁers are deﬁned same as for CL.
2.4
Decision Problems
We deﬁne the decision problems that are the focus of the upcoming sections. As usual,
a decision problem is a class of yes/no queries that differ only in their input. In our case,
the input consists of an SID and one or two predicates, written between square brackets.
Deﬁnition 5. We consider the following problems, for a SID Δ and predicates A,B ∈A:
1. Sat[Δ,A]: is the sentence ∃x1 ...∃x#A . A(x1,...,x#A) satisﬁable for Δ?
2. Bnd[Δ,A]: is the set {δ(γ) | γ |=Δ ∃x1 ...∃x#A . A(x1,...,x#A)} ﬁnite?
3. Entl[Δ,A,B]: does A(x1,...,x#A) |=Δ ∃x#B+1 ...∃x#A . B(x1,...,x#B) hold?
The size of a formula φ is the total number of occurrences of symbols needed to write it
down, denoted by size(φ). The size of a SID Δ is size(Δ)
def= ∑A(x1,...,x#A)←φ∈Δ size(φ)+
#A+1. Other parameters of a SID Δ are:
– arity(Δ)
def= max{#A | A(x1,...,x#A) ←φ ∈Δ},
– width(Δ)
def= max{size(φ) | A(x1,...,x#A) ←φ ∈Δ},
– intersize(Δ)
def= max{n | ⟨x1.p1,...,xn.pn⟩occurs in φ,A(x1,...,x#A) ←φ ∈Δ}.
For
a
decision
problem
P[Δ,A,B],
we
consider
its
(k,ℓ)-bounded
versions
P(k,ℓ)[Δ,A,B], obtained by restricting the predicates and interaction atoms occurring
Δ to arity(Δ) ≤k and intersize(Δ) ≤ℓ, respectively, where k and ℓare either positive
integers or inﬁnity. We consider, for each P[Δ,A,B], the subproblems P(k,ℓ)[Δ,A,B] cor-
responding to the three cases (1) k < ∞and ℓ= ∞, (2) k = ∞and ℓ< ∞, and (3) k = ∞
and ℓ= ∞. As we explain next, this is because, for the decision problems considered
(Deﬁnition 5), the complexity for the case k < ∞,ℓ< ∞matches the one for the case
k < ∞,ℓ= ∞.
Satisﬁability (1) and entailment (3) arise naturally during veriﬁcation of reconﬁgu-
ration programs. For instance, Sat[Δ,φ] asks whether a speciﬁcation φ of a set conﬁgu-
rations (e.g., a pre-, post-condition, or a loop invariant) is empty or not (e.g., an empty
precondition typically denotes a vacuous veriﬁcation condition), whereas Entl[Δ,φ,ψ]
is used as a side condition for the Hoare rule of consequence, as in e.g., the proof
from Fig. 1(c). Moreover, entailments must be proved when checking inductiveness of
a user-provided loop invariant.
The Bnd[Δ,φ] problem is used to check a necessary condition for the decidability
of entailments i.e., Entl[Δ,φ,ψ]. If Bnd[Δ,φ] has a positive answer, we can reduce the
problem Entl[Δ,φ,ψ] to an entailment problem for SL, which is always interpreted over
heaps of bounded degree [18]. Otherwise, the decidability status of the entailment prob-
lem is open, for conﬁgurations of unbounded degree, such as the one described by the
example below.

Decision Problems in a Logic for Reasoning
699
Example 1. The following SID describes star topologies with a central controller con-
nected to an unbounded number of workers stations:
Controller(x) ←[x]∗Worker(x)
Worker(x) ←∃y . ⟨x.out, y.in⟩∗[y]∗Worker(x)
Worker(x) ←
emp
■
3
Satisﬁability
We show that the satisﬁability problem (Deﬁnition 5, point 1) is decidable, using a
method similar to the one pioneered by Brotherston et al. [9], for checking satisﬁability
of inductively deﬁned symbolic heaps in SL. We recall that a formula π is pure if and
only if it is a separating conjunction of equalities, disequalities and state atoms. In the
following, the order of terms in (dis-)equalities is not important i.e., we consider x = y
(resp. x ̸= y) and y = x (resp. y ̸= x) to be the same formula.
Deﬁnition 6. The closure cl(π) of a pure formula π is the limit of the sequence
π0,π1,π2,... such that π0 = π and, for each i ≥0, πi+1 is obtained by joining (with
∗) all of the following formulæ to πi:
– x = z, where x and z are the same variable, or x = y and y = z both occur in πi,
– x ̸= z, where x = y and y ̸= z both occur in πi, or
– y@q, where x@q and x = y both occur in πi.
Because only ﬁnitely many such formulæ can be added, the sequence of pure formulæ
from Deﬁnition 6 is bound to stabilize after polynomially many steps. A pure formula
is satisﬁable if and only if its closure does not contain contradictory literals i.e., x = y
and x ̸= y, or x@q and x@q′, for q ̸= q′ ∈Q . We write x ≈π y (resp. x ̸≈πy) if and only
if x = y (resp. x ̸= y) occurs in cl(π) and not(x ≈π y) (resp. not(x ̸≈πy)) whenever x ≈π y
(resp. x̸≈πy) does not hold. Note that e.g., not(x ≈π y) is not the same as x ̸≈πy.
Base tuples constitute the abstract domain used by the algorithms for checking sat-
isﬁability (point 1 of Deﬁnition 5) and boundedness (point 2 of Deﬁnition 5), deﬁned
as follows:
Deﬁnition 7. A base tuple is a triple t = (C ♯,I ♯,π), where:
– C ♯∈mpowV is a multiset of variables denoting present components,
– I ♯: Inter →mpowV+ maps each interaction type τ ∈Inter into a multiset of tuples
of variables of length |τ| each, and
– π is a pure formula.
A base tuple is called satisﬁable if and only if π is satisﬁable and the following hold:
1. for all x,y ∈C ♯, not(x ≈π y),
2. for all τ ∈Inter, ⟨x1,...,x|τ|⟩,⟨y1,...,y|τ|⟩∈I ♯(τ), there exists i ∈[1,|τ|] such that
not(xi ≈π yi),
3. for all τ ∈Inter, ⟨x1,...,x#τ⟩∈I ♯(τ) and 1 ≤i < j ≤|τ|, we have not(xi ≈π xj).
We denote by SatBase the set of satisﬁable base tuples.

700
M. Bozga et al.
Intuitively, a base tuple is an abstract representation of a conﬁguration, where compo-
nents (resp. interactions) are represented by variables (resp. tuples of variables). Note
that a base tuple (C ♯,I ♯,π) is unsatisﬁable if C ♯(I ♯) contains the same variable (tuple
of variables) twice (for the same interaction type), hence the use of multisets in the
deﬁnition of base tuples. It is easy to see that checking the satisﬁability of a given base
tuple (C ♯,I ♯,π) can be done in time poly(||C ♯||+∑τ∈Inter ||I ♯(τ)||+size(π)).
We deﬁne a partial composition operation on satisﬁable base tuples, as follows:
(C ♯
1,I ♯
1,π1)⊗(C ♯
2,I ♯
2,π2)
def= (C ♯
1 ∪C ♯
2,I ♯
1 ∪I ♯
2,π1 ∗π2)
where the union of multisets is lifted to functions Inter →mpow(V+) in the usual way.
The composition operation ⊗is undeﬁned if (C ♯
1,I ♯
1,π1)⊗(C ♯
2,I ♯
2,π2) is not satisﬁable
e.g., if C ♯
1 ∩C ♯
2 ̸= /0, I ♯
1(τ)∩I ♯
2(τ) ̸= /0, for some τ ∈Inter, or π1 ∗π2 is not satisﬁable.
Given a pure formula π and a set of variables X, the projection π↓X removes from π
all atoms α, such that fv(α) ̸⊆X. The projection of a base tuple (C ♯,I ♯,π) on a variable
set X is formally deﬁned below:
(C ♯,I ♯,π)↓X
def=

C ♯∩X,λτ . {⟨x1,...,x|τ|⟩∈I ♯(τ) | x1,...,x|τ| ∈X},cl(dist(I ♯)∗π)↓X

where dist(I ♯)
def= ∗τ∈Inter∗⟨x1,...,x|τ|⟩∈I ♯(τ)∗1≤i<j≤|τ| xi ̸= xj
The substitution operation (C ♯,I ♯,π)[x1/y1,...,xn/yn] replaces simultaneously
each xi with yi in C ♯, I ♯and π, respectively. We lift the composition, projection and
substitution operations to sets of satisﬁable base tuples, as usual.
Next, we deﬁne the base tuple corresponding to a quantiﬁer- and predicate-free
formula φ = ψ ∗π, where ψ consists of component and interaction atoms and π is pure.
Since, moreover, we are interested in those components and interactions that are visible
through a given indexed set of parameters X = {x1,...,xn}, for a variable y, we denote
by {{y}}X
π the parameter xi with the least index, such that y ≈π xi, or y itself, if no such
parameter exists. We deﬁne the following sets of formulæ:
Base(φ,X)
def=

{(C ♯,I ♯,π)} , if (C ♯,I ♯,π) is satisﬁable
/0
, otherwise
where C ♯def= {{{x}}X
π | [x] occurs in ψ}
I ♯def= λ⟨p1,..., ps⟩.

{{y1}}X
π,...,{{ys}}X
π

| ⟨y1.p1,...,ys.ps⟩occurs in ψ

We consider a tuple of variables −→X , having a variable X (A) ranging over
pow(SatBase), for each predicate A that occurs in Δ. With these deﬁnitions, each rule
of Δ:
A(x1,...,x#A) ←∃y1 ...∃ym . φ∗B1(z1
1,...,z1
#B1)∗...∗Bh(zh
1,...,zh
#Bh)
where φ is a quantiﬁer- and predicate-free formula, induces the constraint:
X (A) ⊇
	
Base(φ,{x1,...,x#A})⊗
h

ℓ=1
X (Bℓ)[x1/zℓ
1,...,x#Bℓ/zℓ
#Bℓ]

↓x1,...,x#A
(1)

Decision Problems in a Logic for Reasoning
701
input
output
1: initially
2: for
, with
quantiﬁer- and predicate-free do
3:
Base
4: while
still change do
5:
for
do
6:
if there exist
then
7:
Base
Fig. 2. Algorithm for the Computation of the Least Solution
Let Δ♯be the set of such constraints, corresponding to the rules in Δ and let µ−→X .Δ♯
be the tuple of least solutions of the constraint system generated from Δ, indexed by
the tuple of predicates that occur in Δ, such that µ−→X .Δ♯(A) denotes the entry of µ−→X .Δ♯
correponding to A. Since the composition and projection are monotonic operations,
such a least solution exists and is unique. Since SatBase is ﬁnite, the least solution can
be attained in a ﬁnite number of steps, using a Kleene iteration (see Fig. 2).
We state below the main result leading to an elementary recursive algorithm for the
satisﬁability problem (Theorem 1). The intuition is that, if µ−→X .Δ♯(A) is not empty, then
it contains only satisﬁable base tuples, from which a model of A(x1,...,x#A) can be
built.
Lemma 1. Sat[Δ,A] has a positive answer if and only if µ−→X .Δ♯(A) ̸= /0.
If the maximal arity of the predicates occurring in Δ is bound by a constant k, no
satisﬁable base tuple (C ♯,I ♯,π) can have a tuple ⟨y1,...,y|τ|⟩∈I ♯(τ), for some τ ∈
Inter, such that |τ| > k, since all variables y1,...,y|τ| are parameters denoting distinct
components (point 3 of Deﬁnition 7). Hence, the upper bound on the size of a satisﬁable
base tuple is constant, in both the k < ∞,ℓ< ∞and k < ∞,ℓ= ∞cases, which are,
moreover indistinguishable complexity-wise (i.e., both are NP-complete). In contrast,
in the cases k = ∞,ℓ< ∞and k = ∞,ℓ= ∞, the upper bound on the size of satisﬁable
base tuples is polynomial and simply exponential in size(Δ), incurring a complexity gap
of one and two exponentials, respectively. The theorem below states the main result of
this section:
Theorem 1. Sat(k,∞)[Δ,A] is NP-complete for k ≥4, Sat(∞,ℓ)[Δ,A] is EXP-complete
and Sat[Δ,A] is in 2EXP.
The upper bounds are consequences of the fact that the size of a satisﬁable base tuple is
bounded by a simple exponential in the min(arity(Δ),intersize(Δ)), hence the number
of such tuples is doubly exponential in min(arity(Δ),intersize(Δ)). The lower bounds
are by a polynomial reduction from the satisﬁability problem for SL [9].
Example 2. The doubly-exponential upper bound for the algorithm computing the least
solution of a system of constraints of the form (1) is necessary, in general, as illustrated
by the following worst-case example. Let n be a ﬁxed parameter and consider the n-arity
predicates A1,...,An deﬁned by the following SID:

702
M. Bozga et al.
Ai(x1,...,xn) ←∗n−i
j=0 Ai+1(x1,...,xi−1,[xi,...,xn]j),
for all i ∈[1,n−1]
An(x1,...,xn) ←⟨x1.p,...,xn.p⟩
An(x1,...,xn) ←emp
where, for a list of variables xi,...,xn and an integer j ≥0, we write [xi,...,xn]j for
the list rotated to the left j times (e.g., [x1,x2,x3,x4,x5]2 = x3,x4,x5,x1,x2). In this
example, when starting with A1(x1,...,xn) one eventually obtains predicate atoms
An(xi1,...,xin), for any permutation xi1,...,xin of x1,...,xn. Since An may choose to
create or not an interaction with that permutation of variables, the total number of base
tuples generated for A1 is 2n!. That is, the ﬁxpoint iteration generates 22O(nlogn) base
tuples, whereas the size of the input of Sat[Δ,A] is poly(n).
■
4
Degree Boundedness
The boundedness problem (Deﬁnition 5, point 2) asks for the existence of a bound on
the degree (Deﬁnition 3) of the models of a sentence ∃x1 ...∃x#A . A(x1,...,x#A). Intu-
itively, the Bnd[Δ,A] problem has a negative answer if and only if there are increasingly
large unfoldings (i.e., expansions of a formula by replacement of a predicate atom with
one of its deﬁnitions) of A(x1,...,x#A) repeating a rule that contains an interaction atom
involving a parameter of the rule, which is always bound to the same component. We
formalize the notion of unfolding below:
Deﬁnition 8. Given a predicate A and a sequence (r1,i1),...,(rn,in) ∈(Δ×N)+,
where r1 : A(x1,...,x#A) ←φ ∈Δ, the unfolding A(x1,...,x#A)
(r1,i1)...(rn,in)
========⇒Δ ψ is
inductively deﬁned as (1) ψ = φ if n = 1, and (2) ψ is obtained from φ by
replacing its i1-th predicate atom B(y1,...,y#B) with ψ1[x1/y1,...,x#B/y#B], where
B(x1,...,x#B)
(r2,i2)...(rn,in)
========⇒Δ ψ1 is an unfolding, if n > 1.
We show that the Bnd[Δ,A] problem can be reduced to the existence of increasingly
large unfoldings or, equivalently, a cycle in a ﬁnite directed graph, built by a variant of
the least ﬁxpoint iteration algorithm used to solve the satisﬁability problem (Fig. 3).
Deﬁnition 9. Given satisﬁable base pairs t,u ∈SatBase and a rule from Δ:
r : A(x1,...,x#A) ←∃y1 ...∃ym . φ∗B1(z1
1,...,z1
#B1)∗...∗Bh(zh
1,...,zh
#Bh)
where φ is a quantiﬁer- and predicate-free formula, we write (A,t)
(r,i)
∼∼∼∼▷(B,u) if and
only if B = Bi and there exist satisﬁable base tuples t1,...,u = ti,...,th ∈SatBase, such
that t ∈
	
Base(φ,{x1,...,x#A})⊗h
ℓ=1 tℓ[x1/zℓ
1,...,x#Bℓ/zℓ
#Bℓ]

↓x1,...,x#A. We deﬁne the
directed graph with edges labeled by pairs (r,i) ∈Δ×N:
G(Δ)
def=
	
{def(Δ)×SatBase},{⟨(A,t),(r,i),(B,u)⟩| (A,t)
(r,i)
∼∼∼∼▷(B,u)}

The graph G(Δ) is built by the algorithm in Fig. 3, a slight variation of the classical
Kleene iteration algorithm for the computation of the least solution of the constraints of
the form (1). A path (A1,t1)
(r1,i1)
∼∼∼∼▷(A2,t2)
(r2,i2)
∼∼∼∼▷...
(rn,in)
∼∼∼∼▷(An,tn) in G(Δ) induces a unique

Decision Problems in a Logic for Reasoning
703
input
output
1: initially
2: for
, with
quantiﬁer- and predicate-free do
3:
Base
4: while V or E still change do
5:
for
do
6:
if there exist
then
7:
Base
8:
9:
Fig. 3. Algorithm for the Construction of G(Δ)
unfolding A1(x1,...,x#A1)
(r1,i1)...(rn,in)
========⇒Δ φ (Deﬁnition 8). Since the vertices of G(Δ)
are pairs (A,t), where t is a satisﬁable base tuple and the edges of G(Δ) reﬂect the
construction of the base tuples from the least solution of the constraints (1), the outcome
φ of this unfolding is always a satisﬁable formula.
An elementary cycle of G(Δ) is a path from some vertex (B,u) back to itself, such
that (B,u) does not occur on the path, except at its endpoints. The cycle is, moreover,
reachable from (A,t) if and only if there exists a path (A,t)
(r1,i1)
∼∼∼∼▷...
(rn,in)
∼∼∼∼▷(B,u) in G(Δ).
We reduce the complement of the Bnd[Δ,A] problem, namely the existence of an inﬁnite
set of models of ∃x1 ...∃x#A . A(x1,...,x#A) of unbounded degree, to the existence of a
reachable elementary cycle in G(Δ′), where Δ′ is obtained from Δ, as described in the
following.
First, we consider, for each predicate B ∈def(Δ), a predicate B′, of arity #B + 1,
not in def(Δ) i.e., the set of predicates for which there exists a rule in Δ. Second, for
each rule B0(x1,...,x#B0) ←∃y1 ...∃ym . φ ∗∗h
ℓ=2Bℓ(zℓ
1,...,zℓ
#Bℓ) ∈Δ, where φ is a
quantiﬁer- and predicate-free formula and iv(φ) ⊆fv(φ) denotes the subset of variables
occurring in interaction atoms in φ, the SID Δ′ has the following rules:
B′
0(x1,...,x#B0,x#B0+1) ←∃y1 ...∃ym . φ∗∗ξ∈iv(φ)x#B0+1 ̸= ξ∗
∗h
ℓ=2B′
ℓ(zℓ
1,...,zℓ
#Bℓ,x#B0+1)
(2)
B′
0(x1,...,x#B0,x#B0+1) ←∃y1 ...∃ym . φ∗x#B0+1 = ξ∗
∗h
ℓ=2B′
ℓ(zℓ
1,...,zℓ
#Bℓ,x#B0+1)
(3)
for each variable ξ ∈iv(φ), that occurs in an interaction atom in φ.
There exists a family of models (with respect to Δ) of ∃x1 ...∃x#A . A(x1,...,x#A) of
unbounded degree if and only if these are models of ∃x1 ...∃x#A+1 . A′(x1,...,x#A+1)
(with respect to Δ′) and the last parameter of each predicate B′ ∈def(Δ′) can be mapped,
in each of the these models, to a component that occurs in unboundedly many interac-
tions. The latter condition is equivalent to the existence of an elementary cycle, con-
taining a rule of the form (3), that it, moreover, reachable from some vertex (A′,t) of
G(Δ′), for some t ∈SatBase. This reduction is formalized below:

704
M. Bozga et al.
Lemma 2. There exists an inﬁnite sequence of conﬁgurations γ1,γ2,... such that γi |=Δ
∃x1 ...∃x#A . A(x1,...,x#A) and δ(γi) < δ(γi+1), for all i ≥1 if and only if G(Δ′) has an
elementary cycle containing a rule (3), reachable from a node (A′,t), for t ∈SatBase.
The complexity result below uses a similar argument on the maximal size of (hence
the number of) base tuples as in Theorem 1, leading to similar complexity gaps:
Theorem 2. Bnd(k,∞)[Δ,A] is in co-NP, Bnd(∞,ℓ)[Δ,A] is in EXP, Bnd[Δ,A] is in 2EXP.
Moreover, the construction of G(Δ′) allows to prove the following cut-off result:
Proposition 1. Let γ be a conﬁguration and ν be a store, such that γ |=ν
Δ A(x1,...,x#A).
If Bnd(k,ℓ)[Δ,A] then (1) δ(γ) = poly(size(Δ)) if k < ∞, ℓ= ∞, (2) δ(γ) = 2poly(size(Δ)) if
k = ∞, ℓ< ∞and (3) δ(γ) = 22poly(size(Δ)) if k = ∞, ℓ= ∞.
5
Entailment
This section is concerned with the entailment problem Entl[Δ,A,B], that asks whether
γ |=ν
Δ ∃x#A+1 ...∃x#B . B(x1,...,x#B), for every conﬁguration γ and store ν, such that
γ |=ν
Δ A(x1,...,x#A). For instance, the proof from Fig. 1(c) relies on the following entail-
ments, that occur as the side conditions of the Hoare logic rule of consequence:
ringh,t(y) |=Δ ∃x∃z.[y]@H∗⟨y.out, z.in⟩∗chainh−1,t(z,x)∗⟨x.out, y.in⟩
[z]@H∗⟨z.out, x.in⟩∗chainh−1,t(x,y)∗⟨y.out, z.in⟩|=Δ ringh,t(z)
By introducing two fresh predicates A1 and A2, deﬁned by the rules:
A1(x1) ←∃y∃z.[x1]@H∗⟨x1.out, z.in⟩∗chainh−1,t(z,y)∗⟨y.out, x1.in⟩
(4)
A2(x1,x2) ←∃z.[x1]@H∗⟨x1.out, z.in⟩∗chainh−1,t(z,x2)∗⟨x2.out, x1.in⟩
(5)
the above entailments are equivalent to Entl[Δ,ringh,t,A1] and Entl[Δ,A2,ringh,t],
respectively, where Δ consists of the rules (4) and (5), together with the rules that deﬁne
the ringh,t and chainh,t predicates (Sect. 1.1).
We show that the entailment problem is undecidable, in general (Thm. 3), and
recover a decidable fragment, by means of three syntactic conditions, typically met
in our examples. These conditions use the following notion of proﬁle:
Deﬁnition 10. The proﬁle of a SID Δ is the pointwise greatest function λΔ : A →
pow(N), mapping each predicate A into a subset of [1,#A], such that, for each rule
A(x1,...,x#A) ←φ from Δ, each atom B(y1,...,y#B) from φ and each i ∈λΔ(B), there
exists j ∈λΔ(A), such that xj and yi are the same variable.
The proﬁle identiﬁes the parameters of a predicate that are always replaced by a vari-
able x1,...,x#A in each unfolding of A(x1,...,x#A), according to the rules in Δ; it is
computed by a greatest ﬁxpoint iteration, in time poly(size(Δ)).
Deﬁnition 11. A rule A(x1,...,x#A) ←∃y1 ...∃ym . φ∗∗h
ℓ=1Bℓ(zℓ
1,...,zℓ
#Bℓ), where φ
is a quantiﬁer- and predicate-free formula, is said to be:

Decision Problems in a Logic for Reasoning
705
1. progressing if and only if φ = [x1] ∗ψ, where ψ consists of interaction atoms
involving x1 and (dis-)equalities, such that h
ℓ=1{zℓ
1,...,zℓ
#Bℓ} = {x2,...,x#A} ∪
{y1,...,ym},
2. connected if and only if, for each ℓ∈[1,h] there exists an interaction atom in ψ that
contains both zℓ
1 and a variable from {x1}∪{xi | i ∈λΔ(A)},
3. equationally-restricted (e-restricted) if and only if, for every disequation x ̸= y from
φ, we have {x,y}∩{xi | i ∈λΔ(A)} ̸= /0.
A SID Δ is progressing, connected and e-restricted if and only if each rule in Δ is
progressing, connected and e-restricted, respectively.
For example, the SID consisting of the rules from Sect. 1.1, together with rules (4) and
(5) is progressing, connected and e-restricted.
We recall that defΔ(A) is the set of rules from Δ that deﬁne A and denote by def∗
Δ(A)
the least superset of defΔ(A) containing the rules that deﬁne a predicate from a rule in
def∗
Δ(A). The following result shows that the entailment problem becomes undecidable
as soon as the connectivity condition is even slightly lifted:
Theorem 3. Entl[Δ,A,B] is undecidable, even when Δ is progressing and e-restricted,
and only the rules in def∗
Δ(A) are connected (the rules in def∗
Δ(B) may be disconnected).
On the positive side, we prove that Entl[Δ,A,B] is decidable, if Δ is progressing,
connected and e-restricted, assuming further that Bnd[Δ,A] has a positive answer. In this
case, the bound on the degree of the models of A(x1,...,x#A) is effectively computable,
using the algorithm from Fig. 3 (see Proposition 1 for a cut-off result) and denote by B
this bound, throughout this section.
The proof uses a reduction of Entl[Δ,A,B] to a similar problem for SL, showed to
be decidable [18]. We recall the deﬁnition of SL, interpreted over heaps h : C ⇀ﬁn CK,
introduced in Sect. 2.3. SL rules are denoted as A(x1,...,x#(A)) ←φ, where φ is a SL
formula, such that fv(φ) ⊆{x1,...,x#(A)} and SL SIDs are denoted as Δ. The proﬁle λΔ
is deﬁned for SL same as for CL (Deﬁnition 10).
Deﬁnition 12. A SL rule A(x1,...,x#(A)) ←φ from a SID Δ is said to be:
1. progressing if and only if φ = ∃t1 ...∃tm . x1 →(y1,...,yK) ∗ψ, where ψ contains
only predicate and equality atoms,
2. connected if and only if z1 ∈{xi | i ∈λΔ(A)}∪{y1,...,yK}, for every predicate atom
B(z1,...,z#(B)) from φ.
Note that the deﬁnitions of progressing and connected rules are different for SL, com-
pared to CL (Deﬁnition 11); in the rest of this section, we rely on the context to distin-
guish progressing (connected) SL rules from progressing (connected) CL rules. More-
over, e-restricted rules are deﬁned in the same way for CL and SL (point 3 of Deﬁnition
11). A tight upper bound on the complexity of the entailment problem between SL for-
mulæ, interpreted by progressing, connected and e-restricted SIDs, is given below:
Theorem 4 ([18]). The SL entailment problem is in 22poly(width(Δ)·logsize(Δ)), for progress-
ing, connected and e-restricted SIDs.

706
M. Bozga et al.
The reduction of Entl[Δ,A,B] to SL entailments is based on the idea of viewing a conﬁg-
uration as a logical structure (hypergraph), represented by a undirected Gaifman graph,
in which every tuple from a relation (hyperedge) becomes a clique [24]. In a similar
vein, we encode a conﬁguration, of degree at most B, by a heap of degree K (Deﬁnition
13), such that K is deﬁned using the following integer function:
pos(i, j,k)
def= 1+B·
j−1
∑
ℓ=1
|τℓ|+i·|τ j|+k
where Inter
def= {τ1,...,τM} is the set of interaction types and Q
def= {q1,...,qN} is the set
of states of the behavior B = (P,Q ,−→) (Sect. 2). Here i ∈[0,B−1] denotes an interac-
tion of type j ∈[1,M] and k ∈[0,N −1] denotes a state. We use M and N throughout the
rest of this section, to denote the number of interaction types and states, respectively.
For a set I of interactions, let Tuplesj
I (c)
def= {⟨c1,...,cn⟩| (c1, p1,...,cn, pn) ∈
I, τ j = ⟨p1,..., pn⟩, c ∈{c1,...,cn}} be the tuples of components from an interac-
tion of type τj from I, that contain a given component c.
Deﬁnition 13. Given a conﬁguration γ = (C,I,ρ), such that δ(γ) ≤B, a Gaifman heap
for γ is a heap h : C ⇀ﬁn CK, where K
def= pos(0,M +1,N), dom(h) = nodes(γ) and, for
all c0 ∈dom(h), such that h(c0) = ⟨c1,...,cK⟩, the following hold:
1. c1 = c0 if and only if c0 ∈C,
2. for all j ∈[1,M], Tuplesj
I (c) = {c1,...,cs} if and only if there exist integers 0 ≤
k1 < ... < ks < B, such that ⟨h(c0)⟩inter(ki, j) = ci, for all i ∈[1,s], where inter(i, j)
def=
[pos(i−1, j,0),pos(i, j,0)] are the entries of the i-th interaction of type τj in h(c0),
3. for all k ∈[1,N], we have ⟨h(c0)⟩state(k) = c0 if and only if ρ(c0) = qk, where the
entry state(k)
def= pos(0,M +1,k −1) in h(c0) corresponds to the state qk ∈Q .
We denote by G(γ) the set of Gaifman heaps for γ.
Intuitively, if h is a Gaifman heap for γ and c0 ∈dom(h), then the ﬁrst entry of h(c0)
indicates whether c0 is present (condition 1 of Deﬁnition 13), the next B · ∑M
j=1 |τ j|
entries are used to encode the interactions of each type τj (condition 2 of Deﬁnition 13),
whereas the last N entries are used to represent the state of the component (condition
3 of Deﬁnition 13). Note that the encoding of conﬁgurations by Gaifman heaps is not
unique: two Gaifman heaps for the same conﬁguration may differ in the order of the
tuples from the encoding of an interaction type and the choice of the unconstrained
entries from h(c0), for each c0 ∈dom(h). On the other hand, if two conﬁgurations have
the same Gaifman heap encoding, they must be the same conﬁguration.
Example 3. Figure 4(b) shows a Gaifman heap for the conﬁguration in Fig. 4(a), where
each component belongs to at most 2 interactions of type ⟨out,in⟩.
■
We build a SL SID Δ that generates the Gaifman heaps of the models of the predicate
atoms occurring in a progressing CL SID Δ. The construction associates to each variable
x, that occurs free or bound in a rule from Δ, a unique K-tuple of variables η(x) ∈VK,

Decision Problems in a Logic for Reasoning
707
T
x
H
y
H
z
out in
out in
(a)
1
2
1
2
[x]
⟨out,in⟩
H T
1
2
1
2
[y]
⟨out,in⟩
H T
1
2
1
2
[z]
⟨out,in⟩
H T
x
y
z
(b)
Fig. 4. Gaifman Heap for a Chain Conﬁguration
that represents the image of the store value ν(x) in a Gaifman heap h i.e., h(ν(x)) =
ν(η(x)). Moreover, we consider, for each predicate symbol A ∈def(Δ), an annotated
predicate symbol Aι of arity #Aι = (K+1)·#A, where ι : [1,#A]×[1,M] →2[0,B−1] is
a map associating each parameter i ∈[1,#A] and each interaction type τj, for j ∈[1,M],
a set of integers ι(i, j) denoting the positions of the encodings of the interactions of type
τj, involving the value of xi, in the models of Aι(x1,...,x#A,η(x1),...,η(x#A)) (point 2
of Deﬁnition 13). Then Δ contains rules of the form:
Aι(x1,...,x#(A),η(x1),...,η(x#(A))) ←
(6)
∃y1 ...∃ym∃η(y1)...∃η(ym) . ψ∗π ∗∗h
ℓ=1 B
ℓ
ιℓ(zℓ
1,...,zℓ
#(Bℓ),η(zℓ
1),...,η(zℓ
#(Bℓ)))
for which Δ has a stem rule A(x1,...,x#(A)) ←∃y1 ...∃ym . ψ∗π∗∗h
ℓ=1Bℓ(zℓ
1,...,zℓ
#Bℓ),
where ψ∗π is a quantiﬁer- and predicate-free formula and π is the conjunction of equal-
ities and disequalities from ψ∗π. However, not all rules (6) are considered in Δ, but only
the ones meeting the following condition:
Deﬁnition 14. A rule of the form (6) is well-formed if and only if, for each i ∈[1,#A]
and each j ∈[1,M], there exists a set of integers Yi, j ⊆[0,B−1], such that:
– ||Yi, j|| = ||I j
ψ,π(xi)||, where I j
ψ,π(x) is the set of interaction atoms ⟨z1.p1,...,zn.pn⟩
from ψ of type τ j = ⟨p1,..., pn⟩, such that zs ≈π x, for some s ∈[1,n],
– Yi, j ⊆ι(i, j) and ι(i, j)\Yi, j = Zj(xi), where Zj(x)
def= h
ℓ=1
#Bℓ
k=1{ιℓ(k, j) | x ≈π zℓ
k}
is the set of positions used to encode the interactions of type τj involving the
store value of the parameter x, in the sub-conﬁguration corresponding to an atom
Bℓ(zℓ
1,...,zℓ
#(Bℓ)), for some ℓ∈[1,h].
We denote by Δ the set of well-formed rules (6), such that, moreover:
ψ
def= x1 →η(x1) ∗∗x∈fv(ψ) CompStatesψ(x) ∗∗#A
i=1 InterAtomsψ(xi), where:
CompStatesψ(x)
def=∗[x] occurs in ψ ⟨η(x)⟩1 = x ∗∗x@qk occurs in ψ ⟨η(x)⟩state(k) = x
InterAtomsψ(xi)
def=∗M
j=1∗
r j
p=1 ⟨η(xi)⟩inter( j,k j
p) = xj
p and {k j
1,...,k j
r j}
def= ι(i, j)\Zj(xi)
Here for two tuples of variables x = ⟨x1,...,xk⟩and y = ⟨y1,...,yk⟩, we denote by
x = y the formula ∗k
i=1xi = yi. Intuitively, the SL formula CompStatesψ(x) realizes
the encoding of the component and state atoms from ψ, in the sense of points (1) and
(3) from Deﬁnition 13, whereas the formula InterAtomsψ(xi) realizes the encodings of

708
M. Bozga et al.
the interactions involving a parameter xi in the stem rule (point 2 of Deﬁnition 13). In
particular, the deﬁnition of InterAtomsψ(xi) uses the fact that the rule is well-formed.
We state below the main result of this section on the complexity of the
entailment problem. The upper bounds follow from a many-one reduction of
Entl[Δ,A,B] to the SL entailment Aι(x1,...,x#A,η(x1),...,η(x#A)) ⊩Δ ∃x#B+1 ...∃x#B
∃η(x#B+1)...∃η(x#B) . Bι′(x1,...,x#B,η(x1),...,η(x#B)), in combination with the
upper bound provided by Theorem 4, for SL entailments. If k < ∞, the complexity
is tight for CL, whereas gaps occur for k = ∞,ℓ< ∞and k = ∞,ℓ= ∞, due to the cut-off
on the degree bound (Proposition 1), which impacts the size of Δ and time needed to
generate it from Δ.
Theorem 5. If Δ is progressing, connected and e-restricted and, moreover, Bnd[Δ,A]
has a positive answer, Entlk,ℓ[Δ,A,B] is in 2EXP, Entl∞,ℓ[Δ,A,B] is in 3EXP ∩2EXP-
hard, and Entl[Δ,A,B] is in 4EXP ∩2EXP-hard.
6
Conclusions and Future Work
We study the satisﬁability and entailment problems in a logic used to write proofs of
correctness for dynamically reconﬁgurable distributed systems. The logic views the
components and interactions from the network as resources and reasons also about the
local states of the components. We reuse existing techniques for Separation Logic [39],
showing that our conﬁguration logic is more expressive than SL, fact which is conﬁrmed
by a number of complexity gaps. Closing up these gaps and ﬁnding tight complexity
classes in the more general cases is considered for future work. In particular, we aim
at lifting the boundedness assumption on the degree of the conﬁgurations that must be
considered to check the validity of entailments.
References
1. Ahrens, E., Bozga, M., Iosif, R., Katoen, J.: Local reasoning about parameterized reconﬁg-
urable distributed systems. CoRR, abs/2107.05253 (2021)
2. Arbab, F.: Reo: a channel-based coordination model for component composition. Math.
Struct. Comput. Sci. 14(3), 329–366 (2004)
3. Basu, A., Bozga, M., Sifakis, J.: Modeling heterogeneous real-time components in BIP. In:
Fourth IEEE International Conference on Software Engineering and Formal Methods (SEFM
2006), pp. 3–12. IEEE Computer Society (2006)
4. Bloem, R., et al.: Decidability of Parameterized Veriﬁcation. Synthesis Lectures on Dis-
tributed Computing Theory. Morgan & Claypool Publishers (2015)
5. Bozga, M., Bueri, L., Iosif, R.: Decision problems in a logic for reasoning about reconﬁg-
urable distributed systems. CoRR, abs/2202.09637 (2022)
6. Bozga, M., Iosif, R., Sifakis, J.: Veriﬁcation of component-based systems with recursive
architectures. CoRR, abs/2112.08292 (2021)
7. Bradbury, J., Cordy, J., Dingel, J., Wermelinger, M.: A survey of self-management in
dynamic software architecture speciﬁcations. In: Proceedings of the 1st ACM SIGSOFT
workshop on Self-managed systems, pp. 28–33. ACM (2004)
8. Brookes, S., O’Hearn, P.W.: Concurrent separation logic. ACM SIGLOG News 3(3), 47–65
(2016)

Decision Problems in a Logic for Reasoning
709
9. Brotherston, J., Fuhs, C., P´erez, J.A.N., Gorogiannis, N.: A decision procedure for satisﬁ-
ability in separation logic with inductive predicates. In: CSL-LICS, pp. 25:1–25:10. ACM
(2014)
10. Bucchiarone, A., Galeotti, J.P.: Dynamic software architectures veriﬁcation using dynalloy.
Electron. Commun. Eur. Assoc. Softw. Sci. Technol. 10 (2008). https://doi.org/10.14279/tuj.
eceasst.10.145
11. Butting, A., Heim, R., Kautz, O., Ringert, J.O., Rumpe, B., Wortmann, A.: A classiﬁca-
tion of dynamic reconﬁguration in component and connector architecture description. In:
Proceedings of MODELS 2017 Satellite Event: Workshops (ModComp). CEUR Workshop
Proceedings, vol. 2019, pp. 10–16. CEUR-WS.org (2017)
12. Calcagno, C., O’Hearn, P.W., Yang, H.: Local action and abstract separation logic. In: 22nd
IEEE Symposium on Logic in Computer Science (LICS 2007), 10–12 July 2007, Wroclaw,
Poland, Proceedings, pp. 366–378. IEEE Computer Society (2007)
13. Cavalcante, E., Batista, T.V., Oquendo, F.: Supporting dynamic software architectures: from
architectural description to implementation. In: Bass, L., Lago, P., Kruchten, P. (eds.) 12th
Working IEEE/IFIP Conference on Software Architecture, WICSA 2015, pp. 31–40. IEEE
Computer Society (2015)
14. Clarke, D.: A basic logic for reasoning about connector reconﬁguration. Fundam. Inf. 82(4),
361–390 (2008)
15. Dinsdale-Young, T., Birkedal, L., Gardner, P., Parkinson, M., Yang, H.: Views: compositional
reasoning for concurrent programs. SIGPLAN Not. 48(1), 287–300 (2013)
16. Dinsdale-Young, T., Dodds, M., Gardner, P., Parkinson, M.J., Vafeiadis, V.: Concurrent
abstract predicates. In: D’Hondt, T. (ed.) ECOOP 2010. LNCS, vol. 6183, pp. 504–528.
Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14107-2 24
17. Dormoy, J., Kouchnarenko, O., Lanoix, A.: Using temporal logic for dynamic reconﬁgura-
tions of components. In: Barbosa, L.S., Lumpe, M. (eds.) FACS 2010. LNCS, vol. 6921, pp.
200–217. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-27269-1 12
18. Echenim, M., Iosif, R., Peltier, N.: Unifying decidable entailments in separation logic with
inductive deﬁnitions. In: Platzer, A., Sutcliffe, G. (eds.) CADE 2021. LNCS (LNAI), vol.
12699, pp. 183–199. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-79876-5 11
19. El-Ballouli, R., Bensalem, S., Bozga, M., Sifakis, J.: Programming dynamic reconﬁgurable
systems. Int. J. Softw. Tools Technol. Transf. 23, 701–719 (2021)
20. El-Hokayem, A., Bozga, M., Sifakis, J.: A temporal conﬁguration logic for dynamic recon-
ﬁgurable systems. In: Hung, C., Hong, J., Bechini, A., Song, E. (eds.) SAC 2021: The 36th
ACM/SIGAPP Symposium on Applied Computing, Virtual Event, Republic of Korea, 22–26
March 2021, pp. 1419–1428. ACM (2021)
21. Farka, F., Nanevski, A., Banerjee, A., Delbianco, G.A., F´abregas, I.: On algebraic abstrac-
tions for concurrent separation logics. Proc. ACM Program. Lang. 5(POPL), 1–32 (2021)
22. Feng, X., Ferreira, R., Shao, Z.: On the relationship between concurrent separation logic
and assume-guarantee reasoning. In: De Nicola, R. (ed.) ESOP 2007. LNCS, vol. 4421, pp.
173–188. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-71316-6 13
23. Foerster, K., Schmid, S.: Survey of reconﬁgurable data center networks: enablers, algorithms,
complexity. SIGACT News 50(2), 62–79 (2019)
24. Gaifman, H.: On local and non-local properties. Stud. Log. Found. Math. 107, 105–135
(1982)
25. Gunawi, H.S., et al.: Why does the cloud stop computing? Lessons from hundreds of service
outages. In: Proceedings of the Seventh ACM Symposium on Cloud Computing, SoCC 2016,
pp. 1–16. Association for Computing Machinery, New York (2016)
26. Hirsch, D., Inverardi, P., Montanari, U.: Graph grammars and constraint solving for software
architecture styles. In: Proceedings of the Third International Workshop on Software Archi-
tecture, ISAW 1998, pp. 69–72. Association for Computing Machinery, New York (1998)

710
M. Bozga et al.
27. Jansen, C., Katelaan, J., Matheja, C., Noll, T., Zuleger, F.: Uniﬁed reasoning about robust-
ness properties of symbolic-heap separation logic. In: Yang, H. (ed.) ESOP 2017. LNCS,
vol. 10201, pp. 611–638. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-662-
54434-1 23
28. Jones, C.B.: Developing methods for computer programs including a notion of interference.
Ph.D. thesis, University of Oxford, UK (1981)
29. Konnov, I.V., Kotek, T., Wang, Q., Veith, H., Bliudze, S., Sifakis, J.: Parameterized systems in
BIP: design and model checking. In: 27th International Conference on Concurrency Theory,
CONCUR 2016, volume 59 of LIPIcs, pp. 30:1–30:16. Schloss Dagstuhl - Leibniz-Zentrum
f¨ur Informatik (2016)
30. Krause, C., Maraikar, Z., Lazovik, A., Arbab, F.: Modeling dynamic reconﬁgurations in Reo
using high-level replacement systems. Sci. Comput. Program. 76, 23–36 (2011)
31. Lanoix, A., Dormoy, J., Kouchnarenko, O.: Combining proof and model-checking to validate
reconﬁgurable architectures. Electron. Notes Theor. Comput. Sci. 279(2), 43–57 (2011)
32. Le Metayer, D.: Describing software architecture styles using graph grammars. IEEE Trans.
Softw. Eng. 24(7), 521–533 (1998)
33. Magee, J., Kramer, J.: Dynamic structure in software architectures. In: ACM SIGSOFT Soft-
ware Engineering Notes, vol. 21, no. 6, pp. 3–14. ACM (1996)
34. Mavridou, A., Baranov, E., Bliudze, S., Sifakis, J.: Conﬁguration logics: modeling architec-
ture styles. J. Log. Algebr. Meth. Program. 86(1), 2–29 (2017)
35. Noormohammadpour, M., Raghavendra, C.S.: Datacenter trafﬁc control: understanding tech-
niques and tradeoffs. IEEE Commun. Surv. Tutor. 20(2), 1492–1525 (2018)
36. O’Hearn, P.W.: Resources, concurrency, and local reasoning. Theor. Comput. Sci. 375(1–3),
271–307 (2007)
37. O’Hearn, P.W., Pym, D.J.: The logic of bunched implications. Bull. Symb. Log. 5(2), 215–
244 (1999)
38. Owicki, S., Gries, D.: An axiomatic proof technique for parallel programs. In: Gries, D.
(ed.) Programming Methodology. Texts and Monographs in Computer Science, pp. 130–152.
Springer, New York (1978). https://doi.org/10.1007/978-1-4612-6315-9 12
39. Reynolds, J.C.: Separation logic: a logic for shared mutable data structures. In: Proceedings
of 17th IEEE Symposium on Logic in Computer Science (LICS 2002), 22–25 July 2002,
Copenhagen, Denmark, pp. 55–74. IEEE Computer Society (2002)
40. Shtadler, Z., Grumberg, O.: Network grammars, communication behaviors and automatic
veriﬁcation. In: Sifakis, J. (ed.) CAV 1989. LNCS, vol. 407, pp. 151–165. Springer, Heidel-
berg (1990). https://doi.org/10.1007/3-540-52148-8 13
41. Taentzer, G., Goedicke, M., Meyer, T.: Dynamic change management by distributed graph
transformation: towards conﬁgurable distributed systems. In: Ehrig, H., Engels, G., Kre-
owski, H.-J., Rozenberg, G. (eds.) TAGT 1998. LNCS, vol. 1764, pp. 179–193. Springer,
Heidelberg (2000). https://doi.org/10.1007/978-3-540-46464-8 13
42. Vafeiadis, V., Parkinson, M.: A marriage of rely/guarantee and separation logic. In: Caires,
L., Vasconcelos, V.T. (eds.) CONCUR 2007. LNCS, vol. 4703, pp. 256–271. Springer, Hei-
delberg (2007). https://doi.org/10.1007/978-3-540-74407-8 18
43. Wermelinger, M.: Towards a chemical model for software architecture reconﬁguration. IEE
Proc.-Softw. 145(5), 130–136 (1998)
44. Wermelinger, M., Fiadeiro, J.L.: A graph transformation approach to software architecture
reconﬁguration. Sci. Comput. Program. 44(2), 133–155 (2002)

Decision Problems in a Logic for Reasoning
711
Open Access This chapter is licensed under the terms of the Creative Commons Attribution
4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use,
sharing, adaptation, distribution and reproduction in any medium or format, as long as you
give appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter’s Creative
Commons license, unless indicated otherwise in a credit line to the material. If material is not
included in the chapter’s Creative Commons license and your intended use is not permitted by
statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder.

Proving Non-Termination and Lower
Runtime Bounds with LoAT (System
Description)
Florian Frohn(B)
and J¨urgen Giesl(B)
LuFG Informatik 2, RWTH Aachen University, Aachen, Germany
florian.frohn@cs.rwth-aachen.de, giesl@informatik.rwth-aachen.de
Abstract. We present the Loop Acceleration Tool (LoAT), a powerful
tool for proving non-termination and worst-case lower bounds for pro-
grams operating on integers. It is based on the novel calculus from [10,11]
for loop acceleration, i.e., transforming loops into non-deterministic
straight-line code, and for ﬁnding non-terminating conﬁgurations. To
implement it eﬃciently, LoAT uses a new approach based on unsat cores.
We evaluate LoAT’s power and performance by extensive experiments.
1
Introduction
Eﬃciency is one of the most important properties of software. Consequently,
automated complexity analysis is of high interest to the software veriﬁcation
community. Most research in this area has focused on deducing upper bounds on
the worst-case complexity of programs. In contrast, the Loop Acceleration Tool
LoAT aims to ﬁnd performance bugs by deducing lower bounds on the worst-case
complexity of programs operating on integers. Since non-termination implies the
lower bound ∞, LoAT is also equipped with non-termination techniques.
LoAT is based on loop acceleration [4,5,9–11,15], which replaces loops by
non-deterministic code: The resulting program chooses a value n, representing
the number of loop iterations in the original program. To be sound, suitable
constraints on n are synthesized to ensure that the original loop allows for at
least n iterations. Moreover, the transformed program updates the program vari-
ables to the same values as n iterations of the original loop, but it does so in
a single step. To achieve that, the loop body is transformed into a closed form,
which is parameterized in n. In this way, LoAT is able to compute symbolic
under-approximations of programs, i.e., every execution path in the resulting
transformed program corresponds to a path in the original program, but not
necessarily vice versa. In contrast to many other techniques for computing under-
approximations, the symbolic approximations of LoAT cover inﬁnitely many runs
of arbitrary length.
Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
- 235950644 (Project GI 274/6-2).
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 712–722, 2022.
https://doi.org/10.1007/978-3-031-10769-6_41

Proving Non-Termination and Lower Runtime Bounds with LoAT
713
Contributions: The main new feature of the novel version of LoAT presented
in this paper is the integration of the loop acceleration calculus from [10,11],
which combines diﬀerent loop acceleration techniques in a modular way, into
LoAT’s framework. This enables LoAT to use the loop acceleration calculus for
the analysis of full integer programs, whereas the standalone implementation of
the calculus from [10,11] was only applicable to single loops without branching
in the body. To control the application of the calculus, we use a new technique
based on unsat cores (see Sect. 5). The new version of LoAT is evaluated in
extensive experiments. See [14] for all proofs.
2
Preliminaries
Let L ⊇{main} be a ﬁnite set of locations, where main is the canonical start
location (i.e., the entry point of the program), and let ⃗x := [x1, . . . , xd] be the
vector of program variables. Furthermore, let T V be a countably inﬁnite set of
temporary variables, which are used to model non-determinism, and let sup Z :=
∞. We call an arithmetic expression e an integer expression if it evaluates to
an integer when all variables in e are instantiated by integers. LoAT analyzes
tail-recursive programs operating on integers, represented as integer transition
systems (ITSs), i.e., sets of transitions f(⃗x)
p−→g(⃗a) [ϕ] where f, g ∈L, the
update ⃗a is a vector of d integer expressions over T V ∪⃗x, the cost p is either an
arithmetic expression over T V ∪⃗x or ∞, and the guard ϕ is a conjunction of
inequations over integer expressions with variables from T V ∪⃗x.1 For example,
consider the loop on the left and the corresponding transition tloop on the right.
while x > 0 do x ←x −1
f(x)
1−→f(x −1) [x > 0]
(tloop)
Here, the cost 1 instructs LoAT to use the number of loop iterations as cost
measure. LoAT allows for arbitrary user deﬁned cost measures, since the user
can choose any polynomials over the program variables as costs. LoAT synthesizes
transitions with cost ∞to represent non-terminating runs, i.e., such transitions
are not allowed in the input.
A conﬁguration is of the form f(⃗c) with f ∈L and ⃗c ∈Zd. For any entity
s /∈L and any arithmetic expressions⃗b = [b1, . . . , bd], let s(⃗b) denote the result of
replacing each variable xi in s by bi, for all 1 ≤i ≤d. Moreover, Vars(s) denotes
the program variables and T V(s) denotes the temporary variables occurring in
s. For an integer transition system T , a conﬁguration f(⃗c) evaluates to g(⃗c ′)
with cost k ∈Z∪{∞}, written f(⃗c)
k−→T g(⃗c ′), if there exist a transition f(⃗x)
p−→
g(⃗a) [ϕ] ∈T and an instantiation of its temporary variables with integers such
that the following holds:
ϕ(⃗c) ∧⃗c ′ = ⃗a(⃗c) ∧k = p(⃗c).
1 LoAT can also analyze the complexity of certain non-tail-recursive programs, see [9].
For simplicity, we restrict ourselves to tail-recursive programs in the current paper.

714
F. Frohn and J. Giesl
As usual, we write f(⃗c) k→∗
T g(⃗c ′) if f(⃗c) evaluates to g(⃗c ′) in arbitrarily many
steps, and the sum of the costs of all steps is k. We omit the costs if they are
irrelevant. The derivation height of f(⃗c) is
dhT (f(⃗c)) := sup{k | ∃g(⃗c ′). f(⃗c) k→∗
T g(⃗c ′)}
and the runtime complexity of T is
rcT (n) := sup{dhT (main(c1, . . . , cd)) | |c1| + . . . + |cd| ≤n}.
T terminates if no conﬁguration main(⃗c) admits an inﬁnite −→T -sequence and T
is ﬁnitary if no conﬁguration main(⃗c) admits a −→T -sequence with cost ∞. Oth-
erwise, ⃗c is a witness of non-termination or a witness of inﬁnitism, respectively.
Note that termination implies ﬁnitism for ITSs where no transition has cost ∞.
However, our approach may transform non-terminating ITSs into terminating,
inﬁnitary ITSs, as it replaces non-terminating loops by transitions with cost ∞.
3
Overview of LoAT
The goal of LoAT is to compute a lower bound on rcT or even prove non-
termination of T . To this end, it repeatedly applies program simpliﬁcations,
so-called processors. When applying them with a suitable strategy (see [8,9]),
one eventually obtains simpliﬁed transitions of the form main(⃗x)
p−→f(⃗a) [ϕ]
where f ̸= main. As LoAT’s processors are sound for lower bounds (i.e., if they
transform T to T ′, then dhT ≥dhT ′), such a simpliﬁed transition gives rise to
the lower bound Iϕ·p on dhT (main(⃗x)) (where Iϕ denotes the indicator function
of ϕ, which is 1 for values where ϕ holds and 0 otherwise). This bound can be
lifted to rcT by solving a so-called limit problem, see [9].
LoAT’s processors are also sound for non-termination, as they preserve
ﬁnitism. So if p = ∞, then it suﬃces to prove satisﬁability of ϕ to prove
inﬁnitism, which implies non-termination of the original ITS, where transitions
with cost ∞are forbidden (see Sect. 2). LoAT’s most important processors are:
Loop Acceleration (Sect. 4) transforms a simple loop, i.e., a single transition
f(⃗x)
p−→f(⃗a) [ϕ], into a non-deterministic transition that can simulate several
loop iterations in one step. For example, loop acceleration transforms tloop
to
f(x)
n−→f(x −n) [x ≥n ∧n > 0] ,
(tloopn)
where n ∈T V, i.e., the value of n can be chosen non-deterministically.
Instantiation [9, Theorem 3.12] replaces temporary variables by integer expres-
sions. For example, it could instantiate n with x in tloopn, resulting in
f(x)
x−→f(0) [x > 0] .
(tloopx)

Proving Non-Termination and Lower Runtime Bounds with LoAT
715
Chaining [9, Theorem 3.18] combines two subsequent transitions into one tran-
sition. For example, chaining combines the transitions
main(x)
1−→f(x)
and tloopx to
main(x)
x+1
−−→f(0) [x > 0] .
Nonterm (Sect. 6) searches for witnesses of non-termination, characterized by
a formula ψ. So it turns, e.g.,
f(x1, x2)
1−→f(x1 −x2, x2) [x1 > 0]
(tnonterm)
into
f(x1, x2)
∞
−→sink(x1, x2) [x1 > 0 ∧x2 ≤0]
(where sink ∈L is fresh), as each ⃗c ∈Z2 with c1 > 0 ∧c2 ≤0 witnesses
non-termination of tnonterm, i.e., here ψ is x1 > 0 ∧x2 ≤0.
Intuitively, LoAT uses Chaining to transform non-simple loops into simple
loops. Instantiation resolves non-determinism heuristically and thus reduces
the number of temporary variables, which is crucial for scalability. In addition
to these processors, LoAT removes transitions after processing them, as explained
in [9]. See [8,9] for heuristics and a suitable strategy to apply LoAT’s processors.
4
Modular Loop Acceleration
For Loop Acceleration, LoAT uses conditional acceleration techniques [10].
Given two formulas ξ and qϕ, and a loop with update ⃗a, a conditional acceleration
technique yields a formula accel(ξ, qϕ,⃗a) which implies that ξ holds throughout
n loop iterations (i.e., ξ is an n-invariant), provided that qϕ is an n-invariant,
too. In the following, let ⃗a0(⃗x) := ⃗x and ⃗am+1(⃗x) := ⃗a(⃗am(⃗x)) = ⃗a[⃗x/⃗am(⃗x)].
Deﬁnition 1 (Conditional Acceleration Technique). A function accel is
a conditional acceleration technique if the following implication holds for all
formulas ξ and qϕ with variables from T V ∪⃗x, all updates ⃗a, all n > 0, and all
instantiations of the variables with integers:

accel(ξ, qϕ,⃗a) ∧∀i ∈[0, n). qϕ(⃗ai(⃗x))

=⇒∀i ∈[0, n). ξ(⃗ai(⃗x)).
The prerequisite ∀i ∈[0, n). qϕ(⃗ai(⃗x)) is ensured by previous acceleration
steps, i.e., qϕ is initially ⊤(true), and it is reﬁned by conjoining a part ξ of the
loop guard in each acceleration step. When formalizing acceleration techniques,
we only specify the result of accel for certain arguments ξ, qϕ, and ⃗a, and assume
accel(ξ, qϕ,⃗a) = ⊥(false) otherwise.

716
F. Frohn and J. Giesl
Deﬁnition 2 (LoAT’s Conditional Acceleration Techniques [10,11]).
Increase accelinc(ξ, qϕ,⃗a) := ξ
if |= ξ ∧qϕ =⇒ξ(⃗a)
Decrease acceldec(ξ, qϕ,⃗a) := ξ(⃗an−1(⃗x)) if |= ξ(⃗a) ∧qϕ =⇒ξ
Eventual Decrease accelev-dec(t > 0, qϕ,⃗a) := t > 0 ∧t(⃗an−1(⃗x)) > 0
if |= (t ≥t(⃗a) ∧qϕ) =⇒t(⃗a) ≥t(⃗a2(⃗x))
Eventual Increase accelev-inc(t > 0, qϕ,⃗a) := t > 0 ∧t ≤t(⃗a)
if |= (t ≤t(⃗a) ∧qϕ) =⇒t(⃗a) ≤t(⃗a2(⃗x))
Fixpoint
accelfp(t > 0, qϕ,⃗a) := t > 0 ∧
x∈closure⃗a(t) x = x(⃗a)
where closure⃗a(t) := 
i∈N Vars(t(⃗ai(⃗x)))
The above ﬁve techniques are taken from [10,11], where only deterministic
loops are considered (i.e., there are no temporary variables). Lifting them to
non-deterministic loops in a way that allows for exact conditional acceleration
techniques (which capture all possible program runs) is non-trivial and beyond
the scope of this paper. Thus, we sacriﬁce exactness and treat temporary vari-
ables like additional constant program variables whose update is the identity,
resulting in a sound under-approximation (that captures a subset of all possible
runs).
So essentially, Increase and Decrease handle inequations t > 0 in the loop
guard where t increases or decreases (weakly) monotonically when applying the
loop’s update. The canonical examples where Increase or Decrease applies are
f(x, . . .) →f(x+1, . . .) [x > 0 ∧. . .]
or
f(x, . . .) →f(x−1, . . .) [x > 0 ∧. . .] ,
respectively. Eventual Decrease applies if t never increases again once it
starts to decrease. The canonical example is f(x, y, . . .) →f(x + y, y −
1, . . .) [x > 0 ∧. . .]. Similarly, Eventual Increase applies if t never decreases
again once it starts to increase. Fixpoint can be used for inequations t > 0 that
do not behave (eventually) monotonically. It should only be used if accelfp(t >
0, qϕ,⃗a) is satisﬁable.
LoAT uses the acceleration calculus of [10]. It operates on acceleration prob-
lems ψ | qϕ | ϕ⃗a, where ψ (which is initially ⊤) is repeatedly reﬁned. When it
stops, ψ is used as the guard of the resulting accelerated transition. The formulas
qϕ and ϕ are the parts of the loop guard that have already or have not yet been
handled, respectively. So qϕ is initially ⊤, and ϕ and ⃗a are initialized with the
guard ϕ and the update of the loop f(⃗x)
p−→f(⃗a) [ϕ] under consideration, i.e., the
initial acceleration problem is ⊤| ⊤| ϕ⃗a. Once ϕ is ⊤, the loop is accelerated
to f(⃗x)
q−→f(⃗an(⃗x)) [ψ ∧n > 0], where the cost q and a closed form for ⃗an(⃗x) are
computed by the recurrence solver PURRS [2].
Deﬁnition 3 (Acceleration Calculus for Conjunctive Loops). The rela-
tion ⇝on acceleration problems is deﬁned as
accel(ξ, qϕ,⃗a) = ψ2
ψ1 | qϕ | ξ ∧ϕ⃗a ⇝ψ1 ∧ψ2 | qϕ ∧ξ | ϕ⃗a
accel is a conditional
acceleration technique

Proving Non-Termination and Lower Runtime Bounds with LoAT
717
So to accelerate a loop, one picks a not yet handled part ξ of the guard in
each step. When accelerating f(⃗x) −→f(⃗a) [ξ] using a conditional acceleration
technique accel, one may assume ∀i ∈[0, n). qϕ(⃗ai(⃗x)). The result of accel is
conjoined to the result ψ1 computed so far, and ξ is moved from the third to
the second component of the problem, i.e., to the already handled part of the
guard.
Example 4 (Acceleration Calculus). We show how to accelerate the loop
f(x, y)
x−→f(x −y, y) [x > 0 ∧y ≥0]
to
f(x, y)
(x+ y
2 )·n−y
2 ·n2
−−−−−−−−−−→f(x −n · y, y) [y ≥0 ∧x −(n −1) · y > 0 ∧n > 0] .
The closed form ⃗an(x) = (x −n · y, y) can be computed via recurrence solving.
Similarly, the cost (x + y
2) · n −y
2 · n2 of n loop iterations is obtained by solving
the following recurrence relation (where c(n) and x(n) denote the cost and the
value of x after n applications of the transition, respectively).
c(n) = c(n−1) + x(n−1) = c(n−1) + x −(n −1) · y
and
c(1) = x.
The guard is computed as follows:
⊤| ⊤| x > 0 ∧y ≥0⃗a ⇝y ≥0 | y ≥0 | x > 0⃗a
⇝y ≥0 ∧x −(n −1) · y > 0 | y ≥0 ∧x > 0 | ⊤⃗a .
In the 1st step, we have ξ = (y ≥0) and accelinc(y ≥0, ⊤,⃗a) = (y ≥0). In the
2nd step, we have ξ = (x > 0) and acceldec(x > 0, y ≥0,⃗a) = (x−(n−1)·y > 0).
So the inequation x −(n −1) · y > 0 ensures n-invariance of x > 0.
5
Eﬃcient Loop Acceleration Using Unsat Cores
Each attempt to apply a conditional acceleration technique other than Fix-
point requires proving an implication, which is implemented via SMT solv-
ing by proving unsatisﬁability of its negation. For Fixpoint, satisﬁability of
accelfp(t > 0, qϕ,⃗a) is checked via SMT. So even though LoAT restricts ξ to
atoms, up to Θ(m2) attempts to apply a conditional acceleration technique are
required to accelerate a loop whose guard contains m inequations using a naive
strategy (5·m attempts for the 1st ⇝-step, 5·(m−1) attempts for the 2nd step,
. . . ).
To improve eﬃciency, LoAT uses a novel encoding that requires just 5 · m
attempts. For any α ∈ATimp = {inc, dec, ev-dec, ev-inc}, let encodeα(ξ, qϕ,⃗a)
be the implication that has to be valid in order to apply accelα, whose premise is
of the form . . .∧qϕ. Instead of repeatedly reﬁning qϕ, LoAT tries to prove validity2
of encodeα,ξ := encodeα(ξ, ϕ \ {ξ},⃗a) for each α ∈ATimp and each ξ ∈ϕ, where
ϕ is the (conjunctive) guard of the transition that should be accelerated. Again,
2 Here and in the following, we unify conjunctions of atoms with sets of atoms.

718
F. Frohn and J. Giesl
proving validity of an implication is equivalent to proving unsatisﬁability of its
negation. So if validity of encodeα,ξ can be shown, then SMT solvers can also
provide an unsat core for ¬encodeα,ξ.
Deﬁnition 5 (Unsat Core). Given a conjunction ψ, we call each unsatisﬁable
subset of ψ an unsat core of ψ.
Theorem 6 shows that when handling an inequation ξ, one only has to require
n-invariance for the elements of ϕ\{ξ} that occur in an unsat core of ¬encodeα,ξ.
Thus, an unsat core of ¬encodeα,ξ can be used to determine which prerequisites
qϕ are needed for the inequation ξ. This information can then be used to ﬁnd a
suitable order for handling the inequations of the guard. Thus, in this way one
only has to check (un)satisﬁability of the 4 · m formulas ¬encodeα,ξ. If no such
order is found, then LoAT either fails to accelerate the loop under consideration,
or it resorts to using Fixpoint, as discussed below.
Theorem 6 (Unsat Core Induces ⇝-Step). Let depsα,ξ be the intersec-
tion of ϕ \ {ξ} and an unsat core of ¬encodeα,ξ. If qϕ implies depsα,ξ, then
accelα(ξ, qϕ,⃗a) = accelα(ξ, ϕ \ {ξ},⃗a).
Example 7 (Controlling Acceleration Steps via Unsat Cores). Reconsider Exam-
ple 4. Here, LoAT would try to prove, among others, the following implications:
encodedec,x>0
=
(x −y > 0 ∧y > 0) =⇒x > 0
(1)
encodeinc,y>0
=
(y > 0 ∧x > 0) =⇒y > 0
(2)
To do so, it would try to prove unsatisﬁability of ¬encodeα,ξ via SMT. For (1),
we get ¬encodedec,x>0 = (x −y > 0 ∧y > 0 ∧x ≤0), whose only unsat core is
¬encodedec,x>0, and its intersection with ϕ \ {x > 0} = {y > 0} is {y > 0}.
For (2), we get ¬encodeinc,y>0 = (y > 0∧x > 0∧y ≤0), whose minimal unsat
core is y > 0 ∧y ≤0, and its intersection with ϕ \ {y > 0} = {x > 0} is empty.
So by Theorem 6, we have accelinc(y > 0, ⊤,⃗a) = accelinc(y > 0, x > 0,⃗a).
In this way, validity of encodeα1,x>0 and encodeα2,y>0 is proven for all α1 ∈
ATimp \ {inc} and all α2 ∈ATimp. However, the premise x ≤x −y ∧y > 0
of encodeev-inc,x>0 is unsatisﬁable and thus a corresponding acceleration step
would yield a transition with unsatisﬁable guard. To prevent that, LoAT only
uses a technique α ∈ATimp for ξ if the premise of encodeα,ξ is satisﬁable.
So for each inequation ξ from ϕ, LoAT synthesizes up to 4 potential ⇝-steps
corresponding to accelα(ξ, depsα,ξ,⃗a), where α ∈ATimp. If validity of encodeα,ξ
cannot be shown for any α ∈ATimp, then LoAT tries to prove satisﬁability of
accelfp(ξ, ⊤,⃗a) to see if Fixpoint should be applied. Note that the 2nd argument
of accelfp is irrelevant, i.e., Fixpoint does not beneﬁt from previous acceleration
steps and thus ⇝-steps that use it do not have any dependencies.
It remains to ﬁnd a suitably ordered subset S of m ⇝-steps that constitutes
a successful ⇝-sequence. In the following, we deﬁne AT := ATimp ∪{fp} and we
extend the deﬁnition of depsα,ξ to the case α = fp by deﬁning depsfp,ξ := ∅.

Proving Non-Termination and Lower Runtime Bounds with LoAT
719
Lemma 8. Let C ⊆AT × ϕ be the smallest set such that (α, ξ) ∈C implies
(a) if α ∈ATimp, then encodeα,ξ is valid and its premise is satisﬁable,
(b) if α = fp, then accelfp(ξ, ⊤,⃗a) is satisﬁable, and
(c) depsα,ξ ⊆{ξ′ | (α′, ξ′) ∈C for some α′ ∈AT}.
Let S := {(α, ξ) ∈C | α ≥AT α′ for all (α′, ξ) ∈C} where >AT is the total
order inc >AT dec >AT ev-dec >AT ev-inc >AT fp. We deﬁne (α′, ξ′) ≺(α, ξ)
if ξ′ ∈depsα,ξ. Then ≺is a strict (and hence, well-founded) order on S.
The order >AT in Lemma 8 corresponds to the order proposed in [10]. Note
that the set C can be computed without further (potentially expensive) SMT
queries by a straightforward ﬁxpoint iteration, and well-foundedness of ≺follows
from minimality of C. For Example 7, we get
C = {(dec, x > 0), (ev-dec, x > 0)} ∪{(α, y > 0) | α ∈AT}
and
S = {(dec, x > 0), (inc, y > 0)} with (inc, y > 0) ≺(dec, x > 0).
Finally, we can construct a valid ⇝-sequence via the following theorem.
Theorem 9. (Finding ⇝-Sequences). Let S be deﬁned as in Lemma 8 and
assume that for each ξ ∈ϕ, there is an α ∈AT such that (α, ξ) ∈S. W.l.o.g.,
let ϕ = m
i=1 ξi where (α1, ξ1) ≺′ . . . ≺′ (αm, ξm) for some strict total order ≺′
containing ≺, and let qϕj := j
i=1 ξi. Then for all j ∈[0, m), we have:
j
i=1 accelαi(ξi, qϕi−1,⃗a)
 qϕj
 m
i=j+1 ξi

⃗a ⇝
j+1
i=1 accelαi(ξi, qϕi−1,⃗a)
 qϕj+1
 m
i=j+2 ξi

⃗a
In our example, we have ≺′ = ≺as ≺is total. Thus, we obtain a ⇝-
sequence by ﬁrst processing y > 0 with Increase and then processing x > 0
with Decrease.
6
Proving Non-Termination of Simple Loops
To prove non-termination, LoAT uses a variation of the calculus from Sect. 4,
see [11]. To adapt it for proving non-termination, further restrictions have to be
imposed on the conditional acceleration techniques, resulting in the notion of
conditional non-termination techniques, see [11, Def. 10]. We denote a ⇝-step
that uses a conditional non-termination technique with ⇝nt.
Theorem 10. (Proving Non-Termination via ⇝nt). Let f(⃗x) −→f(⃗a) [ϕ] ∈
T . If ⊤| ⊤| ϕ⃗a ⇝∗
nt ψ | ϕ | ⊤⃗a, then for every ⃗c ∈Zd where ψ(⃗c) is satisﬁ-
able, the conﬁguration f(⃗c) admits an inﬁnite −→T -sequence.
The conditional non-termination techniques used by LoAT are Increase,
Eventual Increase, and Fixpoint. So non-termination proofs can be synthe-
sized while trying to accelerate a loop with very little overhead. After successfully
accelerating a loop as explained in Sect. 5, LoAT tries to ﬁnd a second suitably
ordered ⇝-sequence, where it only considers the conditional non-termination
techniques mentioned above. If LoAT succeeds, then it has found a ⇝nt-sequence
which gives rise to a proof of non-termination via Theorem 10.

720
F. Frohn and J. Giesl
7
Implementation, Experiments, and Conclusion
Our implementation in LoAT can parse three widely used formats for ITSs (see
[13]), and it is conﬁgurable via a minimalistic set of command-line options:
--timeout to set a timeout in seconds
--proof-level to set the verbosity of the proof output
--plain to switch from colored to monochrome proof-output
--limit-strategy to choose a strategy for solving limit problems, see [9]
--mode to choose an analysis mode for LoAT (complexity or non termination)
We evaluate three versions of LoAT: LoAT ’19 uses templates to ﬁnd invari-
ants that facilitate loop acceleration for proving non-termination [8]; LoAT ’20
deduces worst-case lower bounds based on loop acceleration via metering func-
tions [9]; and LoAT ’22 applies the calculus from [10,11] as described in Sect. 5
and 6. We also include three other state-of-the-art termination tools in our eval-
uation: T2 [6], VeryMax [16], and iRankFinder [3,7]. Regarding complexity, the
only other tool for worst-case lower bounds of ITSs is LOBER [1]. However, we
do not compare with LOBER, as it only analyses (multi-path) loops instead of
full ITSs.
We use the examples from the categories Termination (1222 examples) and
Complexity of ITSs (781 examples), respectively, of the Termination Problems
Data Base [19]. All benchmarks have been performed on StarExec [18] (Intel
Xeon E5-2609, 2.40GHz, 264GB RAM [17]) with a wall clock timeout of 300 s.
By the table on the left, LoAT ’22 is the most powerful tool for non-
termination. The improvement over LoAT ’19 demonstrates that the calculus
from [10,11] is more powerful and eﬃcient than the approach from [8]. The last
three columns show the average, the median, and the standard deviation of the
wall clock runtime, including examples where the timeout was reached.
The table on the right shows the results for complexity. The diagonal cor-
responds to examples where LoAT ’20 and LoAT ’22 yield the same result. The
entries above or below the diagonal correspond to examples where LoAT ’22 or
LoAT ’20 is better, respectively. There are 8 regressions and 79 improvements,
so the calculus from [10,11] used by LoAT ’22 is also beneﬁcial for lower bounds.
LoAT is open source and its source code is available on GitHub [12]. See
[13,14] for details on our evaluation, related work, all proofs, and a pre-compiled
binary.

Proving Non-Termination and Lower Runtime Bounds with LoAT
721
References
1. Albert, E., Genaim, S., Martin-Martin, E., Merayo, A., Rubio, A.: Lower-bound
synthesis using loop specialization and Max-SMT. In: Silva, A., Leino, K.R.M.
(eds.) CAV 2021. LNCS, vol. 12760, pp. 863–886. Springer, Cham (2021). https://
doi.org/10.1007/978-3-030-81688-9 40
2. Bagnara, R., Pescetti, A., Zaccagnini, A., Zaﬀanella, E.: PURRS: towards com-
puter algebra support for fully automatic worst-case complexity analysis. CoRR
abs/cs/0512056 (2005). https://arxiv.org/abs/cs/0512056
3. Ben-Amram, A.M., Dom´enech, J.J., Genaim, S.: Multiphase-linear ranking func-
tions and their relation to recurrent sets. In: Chang, B.-Y.E. (ed.) SAS 2019. LNCS,
vol. 11822, pp. 459–480. Springer, Cham (2019). https://doi.org/10.1007/978-3-
030-32304-2 22
4. Bozga, M., Gˆırlea, C., Iosif, R.: Iterating octagons. In: Kowalewski, S., Philippou,
A. (eds.) TACAS 2009. LNCS, vol. 5505, pp. 337–351. Springer, Heidelberg (2009).
https://doi.org/10.1007/978-3-642-00768-2 29
5. Bozga, M., Iosif, R., Koneˇcn´y, F.: Fast acceleration of ultimately periodic relations.
In: Touili, T., Cook, B., Jackson, P. (eds.) CAV 2010. LNCS, vol. 6174, pp. 227–242.
Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14295-6 23
6. Brockschmidt, M., Cook, B., Ishtiaq, S., Khlaaf, H., Piterman, N.: T2: temporal
property veriﬁcation. In: Chechik, M., Raskin, J.-F. (eds.) TACAS 2016. LNCS,
vol. 9636, pp. 387–393. Springer, Heidelberg (2016). https://doi.org/10.1007/978-
3-662-49674-9 22
7. Dom´enech, J.J., Genaim, S.: iRankFinder. In: Lucas, S. (ed.) WST 2018, p. 83
(2018). http://wst2018.webs.upv.es/wst2018proceedings.pdf
8. Frohn, F., Giesl, J.: Proving non-termination via loop acceleration. In: Barrett,
C.W., Yang, J. (eds.) FMCAD 2019, pp. 221–230 (2019). https://doi.org/10.23919/
FMCAD.2019.8894271
9. Frohn, F., Naaf, M., Brockschmidt, M., Giesl, J.: Inferring lower runtime bounds
for integer programs. ACM TOPLAS 42(3), 13:1–13:50 (2020). https://doi.org/10.
1145/3410331. Revised and extended version of a paper which appeared in IJCAR
2016, pp. 550–567. LNCS, vol. 9706 (2016)
10. Frohn, F.: A calculus for modular loop acceleration. In: Biere, A., Parker, D. (eds.)
TACAS 2020. LNCS, vol. 12078, pp. 58–76. Springer, Cham (2020). https://doi.
org/10.1007/978-3-030-45190-5 4
11. Frohn, F., Fuhs, C.: A calculus for modular loop acceleration and non-termination
proofs. CoRR abs/2111.13952 (2021). https://arxiv.org/abs/2111.13952, to appear
in STTT
12. Frohn, F.: LoAT on GitHub. https://github.com/aprove-developers/LoAT
13. Frohn, F., Giesl, J.: Empirical evaluation of: proving non-termination and lower
runtime bounds with LoAT. https://ﬀrohn.github.io/loat-tool-paper-evaluation
14. Frohn, F., Giesl, J.: Proving non-termination and lower runtime bounds with LoAT
(System Description). CoRR abs/2202.04546 (2022). https://arxiv.org/abs/2202.
04546
15. Kroening, D., Lewis, M., Weissenbacher, G.: Under-approximating loops in C pro-
grams for fast counterexample detection. Formal Methods Syst. Des. 47(1), 75–92
(2015). https://doi.org/10.1007/s10703-015-0228-1
16. Larraz, D., Nimkar, K., Oliveras, A., Rodr´ıguez-Carbonell, E., Rubio, A.: Proving
non-termination using Max-SMT. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS,
vol. 8559, pp. 779–796. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
08867-9 52

722
F. Frohn and J. Giesl
17. StarExec
hardware
speciﬁcations.
https://www.starexec.org/starexec/public/
machine-specs.txt
18. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec: a cross-community infrastructure for
logic solving. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS
(LNAI), vol. 8562, pp. 367–373. Springer, Cham (2014). https://doi.org/10.1007/
978-3-319-08587-6 28
19. Termination Problems Data Base (TPDB, Git SHA 755775). https://github.com/
TermCOMP/TPDB
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Implicit Deﬁnitions with Diﬀerential
Equations for KeYmaera X
(System Description)
James Gallicchio(B)
, Yong Kiam Tan(B)
, Stefan Mitsch(B)
,
and André Platzer(B)
Computer Science Department, Carnegie Mellon University, Pittsburgh, USA
jgallicc@andrew.cmu.edu, {yongkiat,smitsch,aplatzer}@cs.cmu.edu
Abstract. Deﬁnition packages in theorem provers provide users with
means of deﬁning and organizing concepts of interest. This system
description presents a new deﬁnition package for the hybrid systems the-
orem prover KeYmaera X based on diﬀerential dynamic logic (dL). The
package adds KeYmaera X support for user-deﬁned smooth functions
whose graphs can be implicitly characterized by dL formulas. Notably,
this makes it possible to implicitly characterize functions, such as the
exponential and trigonometric functions, as solutions of diﬀerential equa-
tions and then prove properties of those functions using dL’s diﬀer-
ential equation reasoning principles. Trustworthiness of the package is
achieved by minimally extending KeYmaera X’s soundness-critical ker-
nel with a single axiom scheme that expands function occurrences with
their implicit characterization. Users are provided with a high-level inter-
face for deﬁning functions and non-soundness-critical tactics that auto-
mate low-level reasoning over implicit characterizations in hybrid system
proofs.
Keywords: Deﬁnitions · Diﬀerential dynamic logic · Veriﬁcation of
hybrid systems · Theorem proving
1
Introduction
KeYmaera X [7] is a theorem prover implementing diﬀerential dynamic logic
dL [17,19–21] for specifying and verifying properties of hybrid systems mixing
discrete dynamics and diﬀerential equations. Deﬁnitions enable users to express
complex theorem statements in concise terms, e.g., by modularizing hybrid sys-
tem models and their proofs [14]. Prior to this work, KeYmaera X had only one
mechanism for deﬁnition, namely, non-recursive abbreviations via uniform sub-
stitution [14,20]. This restriction meant that common and useful functions, e.g.,
the trigonometric and exponential functions, could not be directly used in KeY-
maera X, even though they can be uniquely characterized by dL formulas [17].
This system description introduces a new KeYmaera X deﬁnitional mecha-
nism where functions are implicitly deﬁned in dL as solutions of ordinary dif-
ferential equations (ODEs). Although deﬁnition packages are available in most
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 723–733, 2022.
https://doi.org/10.1007/978-3-031-10769-6_42

724
J. Gallicchio et al.
general-purpose proof assistants, our package is novel in tackling the question
of how best to support user-deﬁned functions in the domain-speciﬁc setting for
hybrid systems. In contrast to tools with builtin support for some ﬁxed subsets
of special functions [1,9,23]; or higher-order logics that can work with functions
via their inﬁnitary series expansions [4], e.g., exp(t) = ∞
i=0
ti
i! ; our package
strikes a balance between practicality and generality by allowing users to deﬁne
and reason about any function characterizable in dL as the solution of an ODE
(Sect. 2), e.g., exp(t) solves the ODE e′ = e with initial value e(0) = 1.
Theoretically, implicit deﬁnitions strictly expand the class of ODE invariants
amenable to dL’s complete ODE invariance proof principles [22]; such invariants
play a key role in ODE safety proofs [21] (see Proposition 3). In practice, arith-
metical identities and other speciﬁcations involving user-deﬁned functions are
proved by automatically unfolding their implicit ODE characterizations and re-
using existing KeYmaera X support for ODE reasoning (Sect. 3). The package is
designed to provide seamless integration of implicit deﬁnitions in KeYmaera X
and its usability is demonstrated on several hybrid system veriﬁcation examples
drawn from the literature that involve special functions (Sect. 4).
All proofs are in the supplement [8]. The deﬁnitions package is part of KeY-
maera X with a usage guide at: http://keymaeraX.org/keymaeraXfunc/.
2
Interpreted Functions in Diﬀerential Dynamic Logic
This section brieﬂy recalls diﬀerential dynamic logic (dL) [17,18,20,21] and
explains how its term language is extended to support implicit function deﬁ-
nitions.
Syntax. Terms e, ˜e and formulas φ, ψ in dL are generated by the following
grammar, with variable x, rational constant c, k-ary function symbols h (for any
k ∈N), comparison operator ∼∈{=, ̸=, ≥, >, ≤, <}, and hybrid program α:
e, ˜e ::= x | c | e + ˜e | e · ˜e | h(e1, . . . , ek)
(1)
φ, ψ ::= e ∼˜e | φ ∧ψ | φ ∨ψ | ¬φ | ∀xφ | ∃xφ | [α] φ | ⟨α⟩φ
(2)
The terms and formulas above extend the ﬁrst-order language of real arith-
metic (FOLR) with the box ([α] φ) and diamond (⟨α⟩φ) modality formulas which
express that all or some runs of hybrid program α satisfy postcondition φ, respec-
tively. Table 1 gives an intuitive overview of dL’s hybrid programs language for
modeling systems featuring discrete and continuous dynamics and their inter-
actions thereof. In dL’s uniform substitution calculus, function symbols h are
uninterpreted, i.e., they semantically correspond to an arbitrary (smooth) func-
tion. Such uninterpreted function symbols (along with uninterpreted predicate
and program symbols) are crucially used to give a parsimonious axiomatiza-
tion of dL based on uniform substitution [20] which, in turn, enables a trust-
worthy microkernel implementation of the logic in the theorem prover KeY-
maera X [7,16].

Implicit Deﬁnitions with Diﬀerential Equations for KeYmaera X
725
Table 1. Syntax and informal semantics of hybrid programs
Program
Behavior
?φ
Stay in the current state if φ is true, otherwise abort and discard run
x := e
Store the value of term e in variable x
x := ∗
Store an arbitrary real value in variable x
x′ = f(x) & Q Continuously follow ODE x′ = f(x) in domain Q for any duration ≥0
if(φ) α
Run program α if φ is true, otherwise skip. Deﬁnable by ?φ; α ∪?¬φ
α; β
Run program α, then run program β in any resulting state(s)
α ∪β
Nondeterministically run either program α or program β
α∗
Nondeterministically repeat program α for n iterations, for any n ∈N
{α}
For readability, braces are used to group and delimit hybrid programs
Hybrid program model (auxiliary variables s, c):
Hybrid program model (trigonometric functions):
safety speciﬁcation:
Fig. 1. Running example of a swinging pendulum driven by an external force (left), its
hybrid program models and dL safety speciﬁcation (right). Program αs uses trigono-
metric functions directly, while program ˆαs uses variables s, c to implicitly track the
values of sin(θ) and cos(θ), respectively (additions in red). The implicit characteriza-
tions φsin(s, θ), φcos(c, θ) are deﬁned in (4), (5) and are not repeated here for brevity.
(Color ﬁgure online)
Running Example. Adequate modeling of hybrid systems often requires the
use of interpreted function symbols that denote speciﬁc functions of interest.
As a running example, consider the swinging pendulum shown in Fig. 1. The
ODEs describing its continuous motion are θ′ = ω, ω′ = −g
L sin(θ) −kω, where
θ is the swing angle, ω is the angular velocity, and g, k, L are the gravita-
tional constant, coeﬃcient of friction, and length of the rigid rod suspending
the pendulum, respectively. The hybrid program αs models an external force
that repeatedly pushes the pendulum and changes its angular velocity by a

726
J. Gallicchio et al.
nondeterministically chosen value p; the guard if(. . . ) condition is designed to
ensure that the push does not cause the pendulum to swing above the horizontal
as speciﬁed by φs. Importantly, the function symbols sin, cos must denote the
usual real trigonometric functions in αs. Program ˆαs shows the same pendulum
modeled in dL without the use of interpreted symbols, but instead using aux-
iliary variables s, c. Note that ˆαs is cumbersome and subtle to get right: the
implicit characterizations φsin(s, θ), φcos(c, θ) from (4), (5) are lengthy and the
diﬀerential equations s′ = ωc, c′ = −ωs must be manually calculated and added
to ensure that s, c correctly track the trigonometric functions as θ evolves con-
tinuously [18,22].
Interpreted Functions. To enable extensible use of interpreted functions in
dL, the term grammar (1) is enriched with k-ary function symbols h that carry
an interpretation annotation [5,27], h≪φ≫, where φ ≡φ(x0, y1, . . . , yk) is a
dL formula with free variables in x0, y1, . . . , yk and no uninterpreted symbols.
Intuitively, φ is a formula that characterizes the graph of the intended interpre-
tation for h, where y1, . . . , yk are inputs to the function and x0 is the output.
Since φ depends only on the values of its free variables, its formula semantics [[φ]]
can be equivalently viewed as a subset of Euclidean space [[φ]] ⊆R × Rk [20,21].
The dL term semantics ν[[e]] [20,21] in a state ν is extended with a case for terms
h≪φ≫(e1, . . . , ek) by evaluation of the smooth C∞function characterized by [[φ]]:
ν[[h≪φ≫(e1, . . . , ek)]] =
ˆh(ν[[e1]], . . . , ν[[ek]])
if [[φ]] graph of smooth ˆh:Rk→R
0
otherwise
This semantics says that, if the relation [[φ]] ⊆R × Rk is the graph of some
smooth C∞function ˆh : Rk →R, then the annotated syntactic symbol h≪φ≫
is interpreted semantically as ˆh. Note that the graph relation uniquely deﬁnes
ˆh (if it exists). Otherwise, h≪φ≫is interpreted as the constant zero function
which ensures that the term semantics remain well-deﬁned for all terms. An
alternative is to leave the semantics of some terms (possibly) undeﬁned, but
this would require more extensive changes to the semantics of dL and extra case
distinctions during proofs [2].
Axiomatics and Diﬀerentially-Deﬁned Functions. To support reasoning
for implicit deﬁnitions, annotated interpretations are reiﬁed to characterization
axioms for expanding interpreted functions in the following lemma.
Lemma 1. (Function interpretation).
The FI axiom (below) for dL is
sound where h is a k-ary function symbol and the formula semantics [[φ]] is
the graph of a smooth C∞function ˆh : Rk →R.
FI
e0 = h≪φ≫(e1, . . . , ek) ↔φ(e0, e1, . . . , ek)
Axiom FI enables reasoning for terms h≪φ≫(e1, . . . , ek) through their
implicit interpretation φ, but Lemma 1 does not directly yield an implementation

Implicit Deﬁnitions with Diﬀerential Equations for KeYmaera X
727
because it has a soundness-critical side condition that interpretation φ charac-
terizes the graph of a smooth C∞function. It is possible to syntactically char-
acterize this side condition [2], e.g., the formula ∀y1, . . . , yk∃x0φ(x0, y1, . . . , yk)
expresses that the graph represented by φ has at least one output value x0 for
each input value y1, . . . , yk, but this burdens users with the task of proving this
side condition in dL before working with their desired function. The KeYmaera X
deﬁnition package opts for a middle ground between generality and ease-of-use by
implementing FI for univariate, diﬀerentially-deﬁned functions, i.e., the interpre-
tation φ has the following shape, where x = (x0, x1, . . . , xn) abbreviates a vector
of variables, there is one input t = y1, and X = (X0, X1, . . . , Xn), T are dL terms
that do not mention any free variables, e.g., are rational constants, which have
constant value in any dL state:
φ(x0, t) ≡

x1, . . . , xn := ∗;
 x′ = −f(x, t), t′ = −1 ∪
x′ = f(x, t), t′ = 1
 
x = X ∧
t = T

(3)
Formula (3) says from point x0, there exists a choice of the remaining coor-
dinates x1, . . . , xn such that it is possible to follow the deﬁning ODE either
forward x′ = f(x, t), t′ = 1 or backward x′ = −f(x, t), t′ = −1 in time to reach
the initial values x = X at time t = T. In other words, the implicitly deﬁned
function h≪φ(x0,t)≫is the x0-coordinate projected solution of the ODE starting
from initial values X at initial time T. For example, the trigonometric functions
used in Fig. 1 are diﬀerentially-deﬁnable as respective projections:
φsin(s, t) ≡

c := ∗;
 s′ = −c, c′ =
s, t′ = −1 ∪
s′ =
c, c′ = −s, t′ =
1
 
s = 0 ∧c = 1 ∧
t = 0

(4)
φcos(c, t) ≡

s := ∗;
 s′ = −c, c′ =
s, t′ = −1 ∪
s′ =
c, c′ = −s, t′ =
1
 
s = 0 ∧c = 1 ∧
t = 0

(5)
By Picard-Lindelöf [21, Thm. 2.2], the ODE x′ = f(x, t) has a unique solution
Φ : (a, b) →Rn+1 on an open interval (a, b) for some −∞≤a < b ≤∞.
Moreover, Φ(t) is C∞smooth in t because the ODE right-hand sides are dL terms
with smooth interpretations [20]. Therefore, the side condition for Lemma 1
reduces to showing that Φ exists globally, i.e., it is deﬁned on t ∈(−∞, ∞).
Lemma 2. (Smooth interpretation). If formula ∃x0φ(x0, t) is valid, φ(x0, t)
from (3) characterizes a smooth C∞function and axiom FI is sound for φ(x0, t).
Lemma 2 enables an implementation of axiom FI in KeYmaera X that com-
bines a syntactic check (the interpretation has the shape of formula (3)) and a
side condition check (requiring users to prove existence for their interpretations).
The addition of diﬀerentially-deﬁned functions to dL strictly increases the
deductive power of ODE invariants, a key tool in deductive ODE safety reason-
ing [21]. Intuitively, the added functions allow direct, syntactic descriptions of
invariants, e.g., the exponential or trigonometric functions, that have eﬀective
invariance proofs using dL’s complete ODE invariance reasoning principles [22].

728
J. Gallicchio et al.
Proposition 3. (Invariant expressivity). There are valid polynomial dL dif-
ferential equation safety properties which are provable using diﬀerentially-deﬁned
function invariants but are not provable using polynomial invariants.
3
KeYmaera X Implementation
The implicit deﬁnition package adds interpretation annotations and axiom FI
based on Lemma 2 in ≈170 lines of code extensions to KeYmaera X’s soundness-
critical core [7,16]. This section focuses on non-soundness-critical usability fea-
tures provided by the package that build on those core changes.
3.1
Core-Adjacent Changes
KeYmaera X has a browser-based user interface with concrete, ASCII-based
dL syntax [14]. The package extends KeYmaera X’s parsers and pretty printers
with support for interpretation annotations h«...»(...) and users can simulta-
neously deﬁne a family of functions as respective coordinate projections of the
solution of an n-dimensional ODE (given initial conditions) with sugared syntax:
implicit Real h1(Real t), ..., hn(Real t) = {{initcond};{ODE}}
For example, the implicit deﬁnitions (4), (5) can be written with the following
sugared syntax; KeYmaera X automatically inserts the associated interpretation
annotations for the trigonometric function symbols, see the supplement [8] for a
KeYmaera X snippet of formula φs from Fig. 1 using this sugared deﬁnition.
implicit Real sin(Real t), cos(Real t)
= {{sin:=0; cos:=1;}; {sin’=cos, cos’=-sin}}
In fact, the functions sin, cos, exp are so ubiquitous in hybrid system models
that the package builds their deﬁnitions in automatically without requiring users
to write them explicitly. In addition, although arithmetic involving those func-
tions is undecidable [11,24], KeYmaera X can export those functions whenever
its external arithmetic tools have partial arithmetic support for those functions.
3.2
Intermediate and User-Level Proof Automation
The package automatically proves three important lemmas about user-deﬁned
functions that can be transparently re-used in all subsequent proofs:
1. It proves the side condition of axiom FI using KeYmaera X’s automation
for proving suﬃcient duration existence of solutions for ODEs [26] which
automatically shows global existence of solutions for all aﬃne ODEs and
some univariate nonlinear ODEs. As an example of the latter, the hyperbolic
tanh function is diﬀerentially-deﬁned as the solution of ODE x′ = 1−x2 with
initial value x = 0 at t = 0 whose global existence is proved automatically.

Implicit Deﬁnitions with Diﬀerential Equations for KeYmaera X
729
2. It proves that the functions have initial values as speciﬁed by their interpre-
tation, e.g., sin(0) = 0, cos(0) = 1, and tanh(0) = 0.
3. It proves the diﬀerential axiom [20] for each function that is used to enable
syntactic derivative calculations in dL, e.g., the diﬀerential axioms for sin, cos
are (sin(e))′ = cos(e)(e)′ and (cos(e))′ = −sin(e)(e)′, respectively. Brieﬂy,
these axioms are automatically derived in a correct-by-construction manner
using dL’s syntactic version of the chain rule for diﬀerentials [20, Fig. 3], so
the rate of change of sin(e) is the rate of change of sin(·) with respect to its
argument e, multiplied by the rate of change of its argument (e)′.
These lemmas enable the use of diﬀerentially-deﬁned functions with all exist-
ing ODE automation in KeYmaera X [22,26]. In particular, since diﬀerentially-
deﬁned functions are univariate Noetherian functions, they admit complete ODE
invariance reasoning principles in dL [22] as implemented in KeYmaera X.
The package also adds specialized support for arithmetical reasoning over
diﬀerential deﬁnitions to supplement external arithmetic tools in proofs. First,
it allows users to manually prove identities and bounds using KeYmaera X’s
ODE reasoning. For example, the bound tanh(λx)2 < 1 used in the example αn
from Sect. 4 is proved by diﬀerential unfolding as follows (see supplement [8]):
⊢tanh(0)2 < 1
tanh(λv)2<1 ⊢[{v′ = 1 & v ≤x} ∪{v′ = −1 & v ≥x}] tanh(λv)2<1
⊢tanh(λx)2 < 1
This deduction step says that, to show the conclusion (below rule bar), it
suﬃces to prove the premises (above rule bar), i.e., the bound is true at v = 0
(left premise) and it is preserved as v is evolved forward v′ = 1 or backward
v′ = −1 along the real line until it reaches x (right premise). The left premise is
proved using the initial value lemma for tanh while the right premise is proved
by ODE invariance reasoning with the diﬀerential axiom for tanh [22].
Second, the package uses KeYmaera X’s uniform substitution mechanism [20]
to implement (untrusted) abstraction of functions with fresh variables when
solving arithmetic subgoals, e.g., the following arithmetic bound for example αn
is proved by abstraction after adding the bounds tanh(λx)2 < 1, tanh(λy)2 < 1.
Bound:
x(tanh(λx) −tanh(λy)) + y(tanh(λx) + tanh(λy)) ≤2

x2 + y2
Abstracted:
t2
x < 1 ∧t2
y < 1 →x(tx −ty) + y(tx + ty) ≤2

x2 + y2
4
Examples
The deﬁnition package enables users to work with diﬀerentially-deﬁned functions
in KeYmaera X, including modeling and expressing their design intuitions in
proofs. This section applies the package to verify various continuous and hybrid
system examples from the literature featuring such functions.
Discretely Driven Pendulum. The speciﬁcation φs from Fig. 1 contains a discrete
loop whose safety property is proved by a loop invariant, i.e., a formula that is
preserved by the discrete and continuous dynamics in each loop iteration [21].

730
J. Gallicchio et al.
The key invariant is Inv ≡
g
L(1 −cos θ) + 1
2ω2 <
g
L, which expresses that the
total energy of the system (sum of potential and kinetic energy on the LHS) is
less than the energy needed to cross the horizontal (RHS). The main steps are
as follows (proofs for these steps are automated by KeYmaera X):
1. Inv →

if
 1
2(ω −p)2 < g
L cos(θ)

{ω := ω −p}

Inv, which shows that the
discrete guard only allows push p if it preserves the energy invariant, and
2. Inv →

{θ′ = ω, ω′ = −g
L sin(θ) −kω}

Inv, which shows that Inv is an energy
invariant of the pendulum’s ODE.
Neuron Interaction. The ODE αn models the interaction between a pair of neu-
rons [12]; its speciﬁcation φn nests dL’s diamond and box modalities to express
that the system norm (

x2 + y2) is asymptotically bounded by 2τ.
αn ≡x′ = −x
τ + tanh(λx) −tanh(λy), y′ = −y
τ + tanh(λx) + tanh(λy)
φn ≡τ > 0 →∀ε>0⟨αn⟩[αn]

x2 + y2 ≤2τ + ε
The veriﬁcation of φn uses diﬀerentially-deﬁned functions in concert with
KeYmaera X’s symbolic ODE safety and liveness reasoning [26]. The proof uses
a decaying exponential bound

x2 + y2 ≤exp(−t
τ )

x2
0 + y2
0+2τ(1−exp(−t
τ )),
where the constants x0, y0 are symbolic initial values for x, y at initial time t = 0,
respectively. Notably, the arithmetic subgoals from this example are all proved
using abstraction and diﬀerential unfolding (Sect. 3) without relying on external
arithmetic solver support for tanh.
Longitudinal Flight Dynamics. The diﬀerential equa-
tions αa below describe the 6th order longitudinal
motion of an airplane while climbing or descend-
ing [10,25]. The airplane adjusts its pitch angle θ
with pitch rate q, which determines its axial veloc-
ity u and vertical velocity w, and, in turn, range x
and altitude z (illustrated on the right). The physical parameters are: gravity g,
mass m, aerodynamic thrust and moment M along the lateral axis, aerodynamic
and thrust forces X, Z along x and z, respectively, and the moment of inertia
Iyy, see [10, Sect. 6.2].
αa ≡u′ = X
m −g sin(θ) −qw,
w′ = Z
m + g cos(θ) + qu,
q′ = M
Iyy ,
x′ = cos(θ)u + sin(θ)w,
z′ = −sin(θ)u + cos(θ)w,
θ′ = q
The veriﬁcation of speciﬁcation J →[αa]J shows that the safety envelope
J ≡J1 ∧J2 ∧J3 is invariant along the ﬂow of αa with algebraic invariants Ji:
J1 ≡Mz
Iyy + gθ +
	X
m −qw

cos(θ) +
	 Z
m + qu

sin(θ) = 0
J2 ≡Mz
Iyy −
	 Z
m + qu

cos(θ) +
	X
m −qw

sin(θ) = 0
J3 ≡−q2 + 2Mθ
Iyy
= 0
Additional examples are available in the supplement [8], including: a bouncing
ball on a sinusoidal surface [6,13] and a robot collision avoidance model [15].

Implicit Deﬁnitions with Diﬀerential Equations for KeYmaera X
731
5
Conclusion
This work presents a convenient mechanism for extending the dL term language
with diﬀerentially-deﬁned functions, thereby furthering the class of real-world
systems amenable to modeling and formalization in KeYmaera X. Minimal
soundness-critical changes are made to the KeYmaera X kernel, which main-
tains its trustworthiness while allowing the use of newly deﬁned functions in
concert with all existing dL hybrid systems reasoning principles implemented in
KeYmaera X. Future work could formally verify these kernel changes by extend-
ing the existing formalization of dL [3]. Further integration of external arithmetic
tools [1,9,23] will also help to broaden the classes of arithmetic sub-problems
that can be solved eﬀectively in hybrid systems proofs.
Acknowledgments. We thank the anonymous reviewers for their helpful feedback
on this paper. This material is based upon work supported by the National Science
Foundation under Grant No. CNS-1739629. This research was sponsored by the AFOSR
under grant number FA9550-16-1-0288.
References
1. Akbarpour, B., Paulson, L.C.: MetiTarski: an automatic theorem prover for real-
valued special functions. J. Autom. Reason. 44(3), 175–205 (2010). https://doi.
org/10.1007/s10817-009-9149-2
2. Bohrer, B., Fernández, M., Platzer, A.: dLι: deﬁnite descriptions in diﬀerential
dynamic logic. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp.
94–110. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-29436-6_6
3. Bohrer, R., Rahli, V., Vukotic, I., Völp, M., Platzer, A.: Formally veriﬁed diﬀer-
ential dynamic logic. In: Bertot, Y., Vafeiadis, V. (eds.) CPP, pp. 208–221. ACM
(2017). https://doi.org/10.1145/3018610.3018616
4. Boldo, S., Lelay, C., Melquiond, G.: Formalization of real analysis: a survey of
proof assistants and libraries. Math. Struct. Comput. Sci. 26(7), 1196–1233 (2016).
https://doi.org/10.1017/S0960129514000437
5. Bonichon, R., Delahaye, D., Doligez, D.: Zenon: an extensible automated theorem
prover producing checkable proofs. In: Dershowitz, N., Voronkov, A. (eds.) LPAR
2007. LNCS (LNAI), vol. 4790, pp. 151–165. Springer, Heidelberg (2007). https://
doi.org/10.1007/978-3-540-75560-9_13
6. Denman, W.: Automated veriﬁcation of continuous and hybrid dynamical systems.
Ph.D. thesis, University of Cambridge, UK (2015)
7. Fulton, N., Mitsch, S., Quesel, J.-D., Völp, M., Platzer, A.: KeYmaera X: an
axiomatic tactical theorem prover for hybrid systems. In: Felty, A.P., Middeldorp,
A. (eds.) CADE 2015. LNCS (LNAI), vol. 9195, pp. 527–538. Springer, Cham
(2015). https://doi.org/10.1007/978-3-319-21401-6_36
8. Gallicchio, J., Tan, Y.K., Mitsch, S., Platzer, A.: Implicit deﬁnitions with diﬀeren-
tial equations for KeYmaera X (system description). CoRR abs/2203.01272 (2022).
http://arxiv.org/abs/2203.01272
9. Gao, S., Kong, S., Clarke, E.M.: dReal: an SMT solver for nonlinear theories over
the reals. In: Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp. 208–
214. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-38574-2_14

732
J. Gallicchio et al.
10. Ghorbal, K., Platzer, A.: Characterizing algebraic invariants by diﬀerential radical
invariants. In: Ábrahám, E., Havelund, K. (eds.) TACAS 2014. LNCS, vol. 8413, pp.
279–294. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-642-54862-
8_19
11. Gödel, K.: Über formal unentscheidbare Sätze der Principia Mathematica und
verwandter Systeme I. Monatshefte für Mathematik und Physik 38(1), 173–198
(1931). https://doi.org/10.1007/BF01700692
12. Khalil, H.K.: Nonlinear Systems. Macmillan, New York (1992)
13. Liu, J., Zhan, N., Zhao, H., Zou, L.: Abstraction of elementary hybrid systems
by variable transformation. In: Bjørner, N., de Boer, F. (eds.) FM 2015. LNCS,
vol. 9109, pp. 360–377. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
19249-9_23
14. Mitsch, S.: Implicit and explicit proof management in KeYmaera X. In: Proença,
J., Paskevich, A. (eds.) F-IDE. EPTCS, vol. 338, pp. 53–67 (2021). https://doi.
org/10.4204/EPTCS.338.8
15. Mitsch, S., Ghorbal, K., Vogelbacher, D., Platzer, A.: Formal veriﬁcation of obsta-
cle avoidance and navigation of ground robots. Int. J. Robot. Res. 36(12), 1312–
1340 (2017). https://doi.org/10.1177/0278364917733549
16. Mitsch, S., Platzer, A.: A retrospective on developing hybrid system provers in the
KeYmaera family. In: Ahrendt, W., Beckert, B., Bubel, R., Hähnle, R., Ulbrich,
M. (eds.) Deductive Software Veriﬁcation: Future Perspectives. LNCS, vol. 12345,
pp. 21–64. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-64354-6_2
17. Platzer, A.: Diﬀerential dynamic logic for hybrid systems. J. Autom. Reason. 41(2),
143–189 (2008). https://doi.org/10.1007/s10817-008-9103-8
18. Platzer,
A.:
Logical
Analysis
of
Hybrid
Systems:
Proving
Theorems
for
Complex Dynamics. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-
642-14509-4
19. Platzer, A.: The complete proof theory of hybrid systems. In: LICS, pp. 541–550.
IEEE Computer Society (2012). https://doi.org/10.1109/LICS.2012.64
20. Platzer, A.: A complete uniform substitution calculus for diﬀerential dynamic logic.
J. Autom. Reason. 59(2), 219–265 (2016). https://doi.org/10.1007/s10817-016-
9385-1
21. Platzer, A.: Logical foundations of cyber-physical systems. Springer, Cham (2018).
https://doi.org/10.1007/978-3-319-63588-0
22. Platzer, A., Tan, Y.K.: Diﬀerential equation invariance axiomatization. J. ACM
67(1) (2020). https://doi.org/10.1145/3380825
23. Ratschan, S., She, Z.: Safety veriﬁcation of hybrid systems by constraint
propagation-based abstraction reﬁnement. ACM Trans. Embed. Comput. Syst.
6(1), 8 (2007). https://doi.org/10.1145/1210268.1210276
24. Richardson, D.: Some undecidable problems involving elementary functions of
a real variable. J. Symb. Log. 33(4), 514–520 (1968). https://doi.org/10.2307/
2271358
25. Stengel, R.F.: Flight Dynamics. Princeton University Press (2004)
26. Tan, Y.K., Platzer, A.: An axiomatic approach to existence and liveness for diﬀer-
ential equations. Form. Asp. Comput. 33(4), 461–518 (2021). https://doi.org/10.
1007/s00165-020-00525-0
27. Wiedijk, F.: Stateless HOL. In: Hirschowitz, T. (ed.) TYPES. EPTCS, vol. 53, pp.
47–61 (2009). https://doi.org/10.4204/EPTCS.53.4

Implicit Deﬁnitions with Diﬀerential Equations for KeYmaera X
733
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Automatic Complexity Analysis
of Integer Programs via Triangular
Weakly Non-Linear Loops
Nils Lommen(B)
, Fabian Meyer
, and J¨urgen Giesl(B)
LuFG Informatik 2, RWTH Aachen University, Aachen, Germany
lommen@cs.rwth-aachen.de, giesl@informatik.rwth-aachen.de
Abstract. There exist several results on deciding termination and com-
puting runtime bounds for triangular weakly non-linear loops (twn-loops).
We show how to use results on such subclasses of programs where com-
plexity bounds are computable within incomplete approaches for com-
plexity analysis of full integer programs. To this end, we present a novel
modular approach which computes local runtime bounds for subpro-
grams which can be transformed into twn-loops. These local runtime
bounds are then lifted to global runtime bounds for the whole program.
The power of our approach is shown by our implementation in the tool
KoAT which analyzes complexity of programs where all other state-of-
the-art tools fail.
1
Introduction
Most approaches for automated complexity analysis of programs are based
on incomplete techniques like ranking functions (see, e.g., [1–4,6,11,12,18,
20,21,31]). However, there also exist numerous results on subclasses of pro-
grams where questions concerning termination or complexity are decidable, e.g.,
[5,14,15,19,22,24,25,32,34]. In this work we consider the subclass of triangular
weakly non-linear loops (twn-loops), where there exist complete techniques for
analyzing termination and runtime complexity (we discuss the “completeness”
and decidability of these techniques below). An example for a twn-loop is:
while (x2
1+x5
3 < x2 ∧x1 ̸= 0) do (x1, x2, x3) ←(−2·x1, 3·x2−2·x3
3, x3)
(1)
Its guard is a propositional formula over (possibly non-linear) polynomial inequa-
tions. The update is weakly non-linear, i.e., no variable xi occurs non-linear in its
own update. Furthermore, it is triangular, i.e., we can order the variables such
that the update of any xi does not depend on the variables x1, . . . , xi−1 with
smaller indices. Then, by handling one variable after the other one can compute
a closed form which corresponds to applying the loop’s update n times. Using
Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
- 235950644 (Project GI 274/6-2) and DFG Research Training Group 2236 UnRAVeL.
c
⃝The Author(s) 2022
J. Blanchette et al. (Eds.): IJCAR 2022, LNAI 13385, pp. 734–754, 2022.
https://doi.org/10.1007/978-3-031-10769-6_43

Automatic Complexity Analysis of Integer Programs
735
these closed forms, termination can be reduced to an existential formula over
Z [15] (whose validity is decidable for linear arithmetic and where SMT solvers
often also prove (in)validity in the non-linear case). In this way, one can show
that non-termination of twn-loops over Z is semi-decidable (and it is decidable
over the real numbers).
While termination of twn-loops over Z is not decidable, by using the closed
forms, [19] presented a “complete” complexity analysis technique. More precisely,
for every twn-loop over Z, it infers a polynomial which is an upper bound on
the runtime for all those inputs where the loop terminates. So for all (possibly
non-linear) terminating twn-loops over Z, the technique of [19] always computes
polynomial runtime bounds. In contrast, existing tools based on incomplete tech-
niques for complexity analysis often fail for programs with non-linear arithmetic.
In [6,18] we presented such an incomplete modular technique for complex-
ity analysis which uses individual ranking functions for diﬀerent subprograms.
Based on this, we now introduce a novel approach to automatically infer runtime
bounds for programs possibly consisting of multiple consecutive or nested loops
by handling some subprograms as twn-loops and by using ranking functions for
others. In order to compute runtime bounds, we analyze subprograms in topolog-
ical order, i.e., in case of multiple consecutive loops, we start with the ﬁrst loop
and propagate knowledge about the resulting values of variables to subsequent
loops. By inferring runtime bounds for one subprogram after the other, in the
end we obtain a bound on the runtime complexity of the whole program. We ﬁrst
try to compute runtime bounds for subprograms by so-called multiphase linear
ranking functions (MΦRFs, see [3,4,18,20]). If MΦRFs do not yield a ﬁnite run-
time bound for the respective subprogram, then we use our novel twn-technique
on the unsolved parts of the subprogram. So for the ﬁrst time, “complete” com-
plexity analysis techniques like [19] for subclasses of programs with non-linear
arithmetic are combined with incomplete techniques based on (linear) ranking
functions like [6,18]. Based on our approach, in future work one could integrate
“complete” techniques for further subclasses (e.g., for solvable loops [24,25,30,34]
which can be transformed into twn-loops by suitable automorphisms [15]).
Structure: After introducing preliminaries in Sect. 2, in Sect. 3 we show how
to lift a (local) runtime bound which is only sound for a subprogram to an
overall global runtime bound. In contrast to previous techniques [6,18], our lifting
approach works for any method of bound computation (not only for ranking
functions). In Sect. 4, we improve the existing results on complexity analysis of
twn- loops [14,15,19] such that they yield concrete polynomial bounds, we reﬁne
these bounds by considering invariants, and we show how to apply these results
to full programs which contain twn-loops as subprograms. Section 5 extends
this technique to larger subprograms which can be transformed into twn-loops.
In Sect. 6 we evaluate the implementation of our approach in the complexity
analysis tool KoAT and show that one can now also successfully analyze the
runtime of programs containing non-linear arithmetic. We refer to [26] for all
proofs.

736
N. Lommen et al.
Fig. 1. An Integer Program with a Nested Self-Loop
2
Preliminaries
This section recapitulates preliminaries for complexity analysis from [6,18].
Deﬁnition 1 (Atoms and Formulas). We ﬁx a set V of variables. The set
of atoms A(V) consists of all inequations p1 < p2 for polynomials p1, p2 ∈Z[V].
F(V) is the set of all propositional formulas built from atoms A(V), ∧, and ∨.
In addition to “<”, we also use “≥”, “=”, “̸=”, etc., and negations “¬” which
can be simulated by formulas (e.g., p1 ≥p2 is equivalent to p2 < p1 + 1 for
integers).
For integer programs, we use a formalism based on transitions, which also
allows us to represent while-programs like (1) easily. Our programs may have
non-deterministic branching, i.e., the guards of several applicable transitions
can be satisﬁed. Moreover, non-deterministic sampling is modeled by temporary
variables whose values are updated arbitrarily in each evaluation step.
Deﬁnition 2 (Integer Program). (PV, L, ℓ0, T ) is an integer program where
• PV ⊆V is a ﬁnite set of program variables, V\PV are temporary variables
• L is a ﬁnite set of locations with an initial location ℓ0 ∈L
• T is a ﬁnite set of transitions. A transition is a 4-tuple (ℓ, ϕ, η, ℓ′) with a start
location ℓ∈L, target location ℓ′ ∈L \ {ℓ0}, guard ϕ ∈F(V), and update
function η : PV →Z[V] mapping program variables to update polynomials.
Transitions (ℓ0, , , ) are called initial. Note that ℓ0 has no incoming transitions.
Example 3. Consider the program in Fig. 1 with PV = {xi | 1 ≤i ≤5}, L =
{ℓi | 0 ≤i ≤3}, and T = {ti | 0 ≤i ≤5}, where t5 has non-linear arithmetic
in its guard and update. We omitted trivial guards, i.e., ϕ = true, and identity
updates of the form η(v) = v. Thus, t5 corresponds to the while-program (1).
A state is a mapping σ : V →Z, Σ denotes the set of all states, and L × Σ
is the set of conﬁgurations. We also apply states to arithmetic expressions p or
formulas ϕ, where the number σ(p) resp. the Boolean value σ(ϕ) results from
replacing each variable v by σ(v). So for a state with σ(x1) = −8, σ(x2) = 55,
and σ(x3) = 1, the expression x2
1 + x5
3 evaluates to σ(x2
1 + x5
3) = 65 and the
formula ϕ = (x2
1 + x5
3 < x2) evaluates to σ(ϕ) = (65 < 55) = false. From now
on, we ﬁx a program (PV, L, ℓ0, T ).

Automatic Complexity Analysis of Integer Programs
737
Deﬁnition 4 (Evaluation of Programs).
For conﬁgurations (ℓ, σ), (ℓ′, σ′)
and t = (ℓt, ϕ, η, ℓ′
t) ∈T , (ℓ, σ) →t (ℓ′, σ′) is an evaluation step if ℓ= ℓt,
ℓ′ = ℓ′
t, σ(ϕ) = true, and σ(η(v)) = σ′(v) for all v ∈PV. Let →T = 
t∈T →t,
where we also write →instead of →t or →T . Let (ℓ0, σ0) →k (ℓk, σk) abbreviate
(ℓ0, σ0) →. . . →(ℓk, σk) and let (ℓ, σ) →∗(ℓ′, σ′) if (ℓ, σ) →k (ℓ′, σ′) for some
k ≥0.
So when denoting states σ as tuples (σ(x1), . . . , σ(x5)) ∈Z5 for the
program in Fig. 1, we have (ℓ0, (1, 5, 7, 1, 3))
→t0
(ℓ1, (1, 5, 7, 1, 3))
→t1
(ℓ3, (1, 1, 3, 1, 3)) →3
t5 (ℓ3, (1, −8, 55, 1, 3)) →t2 . . .. The runtime complexity
rc(σ0) of a program corresponds to the length of the longest evaluation starting
in the initial state σ0.
Deﬁnition 5 (Runtime Complexity). The runtime complexity is rc:Σ →N
with N = N ∪{ω} and rc(σ0) = sup{k ∈N | ∃(ℓ′, σ′). (ℓ0, σ0) →k (ℓ′, σ′)}.
3
Computing Global Runtime Bounds
We now introduce our general approach for computing (upper) runtime bounds.
We use weakly monotonically increasing functions as bounds, since they can
easily be “composed” (i.e., if f and g increase monotonically, then so does f ◦g).
Deﬁnition 6 (Bounds [6,18]). The set of bounds B is the smallest set with
N ⊆B, PV ⊆B, and {b1 + b2, b1 · b2, kb1} ⊆B for all k ∈N and b1, b2 ∈B.
A bound constructed from N, PV, +, and · is polynomial. So for PV = {x, y},
we have ω, x2, x + y, 2x+y ∈B. Here, x2 and x + y are polynomial bounds.
We measure the size of variables by their absolute values. For any σ ∈Σ, |σ|
is the state with |σ|(v) = |σ(v)| for all v ∈V. So if σ0 denotes the initial state,
then |σ0| maps every variable to its initial “size”, i.e., its initial absolute value.
RBglo : T →B is a global runtime bound if for each transition t and initial state
σ0 ∈Σ, RBglo(t) evaluated in the state |σ0| over-approximates the number of
evaluations of t in any run starting in the conﬁguration (ℓ0, σ0). Let →∗
T ◦→t
denote the relation where arbitrary many evaluation steps are followed by a step
with t.
Deﬁnition 7 (Global Runtime Bound [6,18]). The function RBglo : T →
B is a global runtime bound if for all t ∈T and all states σ0 ∈Σ we have
|σ0|(RBglo(t)) ≥sup{k ∈N | ∃(ℓ′, σ′). (ℓ0, σ0) (→∗
T ◦→t)k (ℓ′, σ′)}.
For the program in Fig. 1, in Example 12 we will infer RBglo(t0) = 1,
RBglo(ti) = x4 for 1 ≤i ≤4, and RBglo(t5) = 8 · x4 · x5 + 13006 · x4. By
adding the bounds for all transitions, a global runtime bound RBglo yields an
upper bound on the program’s runtime complexity. So for all σ0 ∈Σ we have
|σ0|(
t∈T RBglo(t)) ≥rc(σ0).
For local runtime bounds, we consider the entry transitions of subsets T ′ ⊆T .

738
N. Lommen et al.
Deﬁnition 8 (Entry Transitions [6,18]). Let ∅̸= T ′ ⊆T . Its entry transi-
tions are ET ′ = {t | t=(ℓ, ϕ, η, ℓ′)∈T \T ′ ∧there is a transition (ℓ′, , , )∈T ′}.
So in Fig. 1, we have ET \{t0} = {t0} and E{t5} = {t1, t4}.
In contrast to global runtime bounds, a local runtime bound RBloc : ET ′ →B
only takes a subset T ′ into account. A local run is started by an entry transition
r ∈ET ′ followed by transitions from T ′. A local runtime bound considers a subset
T ′
> ⊆T ′ and over-approximates the number of evaluations of any transition from
T ′
> in an arbitrary local run of the subprogram with the transitions T ′. More
precisely, for every t ∈T ′
>, RBloc(r) over-approximates the number of applica-
tions of t in any run of T ′, if T ′ is entered via r ∈ET ′. However, local runtime
bounds do not consider how often an entry transition from ET ′ is evaluated or
how large a variable is when we evaluate an entry transition. To illustrate that
RBloc(r) is a bound on the number of evaluations of transitions from T ′
> after
evaluating r, we often write RBloc(→r T ′
>) instead of RBloc(r).
Deﬁnition 9 (Local Runtime Bound). Let ∅̸= T ′
> ⊆T ′ ⊆T . The function
RBloc : ET ′ →B is a local runtime bound for T ′
> w.r.t. T ′ if for all t ∈T ′
>,
all r ∈ET ′ with r = (ℓ, , , ), and all σ ∈Σ we have |σ|(RBloc(→r T ′
>))
≥
sup{k ∈N | ∃σ0, (ℓ′, σ′). (ℓ0, σ0) →∗
T ◦→r (ℓ, σ) (→∗
T ′ ◦→t)k (ℓ′, σ′)}.
Our approach is modular since it computes local bounds for program parts
separately. To lift local to global runtime bounds, we use size bounds SB(t, v) to
over-approximate the size (i.e., absolute value) of the variable v after evaluating t
in any run of the program. See [6] for the automatic computation of size bounds.
Deﬁnition 10 (Size Bound
[6,18]).
The function SB : (T × PV) →B
is a size bound if for all (t, v) ∈T × PV and all states σ0 ∈Σ we have
|σ0|(SB(t, v)) ≥sup{|σ′(v)| | ∃(ℓ′, σ′). (ℓ0, σ0) (→∗◦→t) (ℓ′, σ′)}.
To compute global from local runtime bounds RBloc(→r T ′
>) and size bounds
SB(r, v), Theorem 11 generalizes the approach of [6,18]. Each local run is started
by an entry transition r. Hence, we use an already computed global runtime
bound RBglo(r) to over-approximate the number of times that such a local run
is started. To over-approximate the size of each variable v when entering the local
run, we instantiate it by the size bound SB(r, v). So size bounds on previous tran-
sitions are needed to compute runtime bounds, and similarly, runtime bounds are
needed to compute size bounds in [6]. For any bound b, “b [v/SB(r, v) | v ∈PV]”
results from b by replacing every program variable v by SB(r, v). Here, weak
monotonic increase of b ensures that the over-approximation of the variables v
in b by SB(r, v) indeed also leads to an over-approximation of b. The analysis
starts with an initial runtime bound RBglo and an initial size bound SB which
map all transitions resp. all pairs from T × PV to ω, except for the transitions t
which do not occur in cycles of T , where RBglo(t) = 1. Afterwards, RBglo and
SB are reﬁned repeatedly, where we alternate between computing runtime and
size bounds.

Automatic Complexity Analysis of Integer Programs
739
Theorem 11 (Computing Global Runtime Bounds).
Let RBglo be a
global runtime bound, SB be a size bound, and ∅̸= T ′
> ⊆T ′ ⊆T such that
T ′ contains no initial transitions. Moreover, let RBloc be a local runtime bound
for T ′
> w.r.t. T ′. Then RB′
glo is also a global runtime bound, where for all t ∈T
we deﬁne:
RB′
glo(t)=
RBglo(t),
if t∈T \T ′
>

r∈ET ′ RBglo(r) · (RBloc(→r T ′
>) [v/SB(r, v) | v∈PV]), if t∈T ′
>
Example 12. For the example in Fig. 1, we ﬁrst use T ′
> = {t2} and T ′ = T \
{t0}. With the ranking function x4 one obtains RBloc(→t0 T ′
>) = x4, since t2
decreases the value of x4 and no transition increases it. Then we can infer the
global runtime bound RBglo(t2) = RBglo(t0)·(x4 [v/SB(t0, v) | v ∈PV]) = x4 as
RBglo(t0) = 1 (since t0 is evaluated at most once) and SB(t0, x4) = x4 (since t0
does not change any variables). Similarly, we can infer RBglo(t1) = RBglo(t3) =
RBglo(t4) = x4.
For T ′
> = T ′ = {t5}, our twn-approach in Sect. 4 will infer the local runtime
bound RBloc : E{t5} →B with RBloc(→t1 {t5}) = 4 · x2 + 3 and RBloc(→t4
{t5}) = 4 · x2 + 4 · x3
3 + 4 · x5
3 + 3 in Example 30. By Theorem 11 we obtain the
global bound
RBglo(t5) = RBglo(t1) · (RBloc(→t1 {t5})[v/SB(t1, v) | v ∈PV]) +
RBglo(t4) · (RBloc(→t4 {t5})[v/SB(t4, v) | v ∈PV])
= x4 · (4 · x5 + 3) + x4 · (4 · x5 + 4 · 53 + 4 · 55 + 3)
(as SB(t1, x2) = SB(t4, x2) = x5 and SB(t4, x3) = 5)
= 8 · x4 · x5 + 13006 · x4.
Thus, rc(σ0) ∈O(n2) where n is the largest initial absolute value of all program
variables. While the approach of [6,18] was limited to local bounds resulting from
ranking functions, here we need our Theorem 11. It allows us to use both local
bounds resulting from twn-loops (for the non-linear transition t5 where tools
based on ranking functions cannot infer a bound, see Sect. 6) and local bounds
resulting from ranking functions (for t1, . . . , t4, since our twn-approach of Sect. 4
and 5 is limited to so-called simple cycles and cannot handle the full program).
In contrast to [6,18], we allow diﬀerent local bounds for diﬀerent entry tran-
sitions in Deﬁnition 9 and Theorem 11. Our example demonstrates that this can
indeed lead to a smaller asymptotic bound for the whole program: By distin-
guishing the cases where t5 is reached via t1 or t4, we end up with a quadratic
bound, because the local bound RBloc(→t1 {t5}) is linear and while x3 occurs
with degrees 5 and 3 in RBloc(→t4 {t5}), the size bound for x3 is constant after
t3 and t4.
To improve size and runtime bounds repeatedly, we treat the strongly con-
nected components (SCCs)1 of the program in topological order such that
1 As usual, a graph is strongly connected if there is a path from every node to every
other node. A strongly connected component is a maximal strongly connected sub-
graph.

740
N. Lommen et al.
improved bounds for previous transitions are already available when handling
the next SCC. We ﬁrst try to infer local runtime bounds by multiphase-linear
ranking functions (see [18] which also contains a heuristic for choosing T ′
> and
T ′ when using ranking functions). If ranking functions do not yield ﬁnite local
bounds for all transitions of the SCC, then we apply the twn-technique from
Sect. 4 and 5 on the remaining unbounded transitions (see Sect. 5 for choos-
ing T ′
> and T ′ in that case). Afterwards, the global runtime bound is updated
according to Theorem 11.
4
Local Runtime Bounds for Twn-Self-Loops
In Sect. 4.1 we recapitulate twn-loops and their termination in our setting. Then
in Sect. 4.2 we present a (complete) algorithm to infer polynomial runtime
bounds for all terminating twn-loops. Compared to [19], we increased its pre-
cision considerably by computing bounds that take the diﬀerent roles of the
variables into account and by using over-approximations to remove monomials.
Moreover, we show how our algorithm can be used to infer local runtime bounds
for twn-loops occurring in integer programs. Section 5 will show that our algo-
rithm can also be applied to infer runtime bounds for larger cycles in programs
instead of just self-loops.
4.1
Termination of Twn-Loops
Deﬁnition 13 extends the deﬁnition of twn-loops in [15,19] by an initial transition
and an update-invariant. Here, ψ is an update-invariant if |= ψ →η(ψ) where
η is the update of the transition (i.e., invariance must hold independent of the
guard).
Deﬁnition 13. (Twn-Loop). An integer program (PV, L, ℓ0, T ) is a triangu-
lar weakly non-linear loop (twn-loop) if PV = {x1, . . . , xd} for some d ≥1,
L = {ℓ0, ℓ}, and T = {t0, t} with t0 = (ℓ0, ψ, id, ℓ) and t = (ℓ, ϕ, η, ℓ) for some
ψ, ϕ ∈F(PV) with |= ψ →η(ψ), where id(v) = v for all v ∈PV, and for all
1 ≤i ≤d we have η(xi) = ci · xi + pi for some ci ∈Z and some polynomial
pi ∈Z[xi+1, . . . , xd]. We often denote the loop by (ψ, ϕ, η) and refer to ψ, ϕ, η
as its (update-) invariant, guard, and update, respectively. If ci ≥0 holds for all
1 ≤i ≤d, then the program is a non-negative triangular weakly non-linear loop
(tnn-loop).
Example 14. The program consisting of the initial transition (ℓ0, true, id, ℓ3) and
the self-loop t5 in Fig. 1 is a twn-loop (corresponding to the while-loop (1)). This
loop terminates as every iteration increases x2
1 by a factor of 4 whereas x2 is only
tripled. Thus, x2
1 + x5
3 eventually outgrows the value of x2.
To transform programs into twn- or tnn-form, one can combine subsequent
transitions by chaining. Here, similar to states σ, we also apply the update η to
polynomials and formulas by replacing each program variable v by η(v).

Automatic Complexity Analysis of Integer Programs
741
Deﬁnition 15 (Chaining). Let t1, . . . , tn be a sequence of transitions without
temporary variables where ti = (ℓi, ϕi, ηi, ℓi+1) for all 1 ≤i ≤n −1, i.e., the
target location of ti is the start location of ti+1. We may have ti = tj for i ̸= j,
i.e., a transition may occur several times in the sequence. Then the transition
t1 ⋆. . . ⋆tn = (ℓ1, ϕ, η, ℓn+1) results from chaining t1, . . . , tn where
ϕ = ϕ1 ∧η1(ϕ2) ∧η2(η1(ϕ3)) ∧. . . ∧ηn−1(. . . η1(ϕn) . . .)
η(v) = ηn(. . . η1(v) . . .) for all v ∈PV, i.e., η = ηn ◦. . . ◦η1.
Similar to [15,19], we can restrict ourselves to tnn-loops, since chaining trans-
forms any twn-loop L into a tnn-loop L ⋆L. Chaining preserves the termination
behavior, and a bound on L⋆L’s runtime can be transformed into a bound for L.
Lemma 16 (Chaining Preserves Asymptotic Runtime, see [19, Lemma
18]). For the twn-loop L = (ψ, ϕ, η) with the transitions t0 = (ℓ0, ψ, id, ℓ), t =
(ℓ, ϕ, η, ℓ), and runtime complexity rcL, the program L ⋆L with the transitions t0
and t ⋆t = (ψ, ϕ ∧η(ϕ), η ◦η) is a tnn-loop. For its runtime complexity rcL⋆L,
we have 2 · rcL⋆L(σ) ≤rcL(σ) ≤2 · rcL⋆L(σ) + 1 for all σ ∈Σ.
Example 17. The program of Example 14 is only a twn-loop and not a tnn-
loop as x1 occurs with a negative coeﬃcient −2 in its own update. Hence, we
chain the loop and consider t5 ⋆t5. The update of t5 ⋆t5 is (η ◦η)(x1) = 4 · x1,
(η ◦η)(x2) = 9 · x2 −8 · x3
3, and (η ◦η)(x3) = x3. To ease the presentation, in
this example we will keep the guard ϕ instead of using ϕ ∧η(ϕ) (ignoring η(ϕ)
in the conjunction of the guard does not decrease the runtime complexity).
Our algorithm starts with computing a closed form for the loop update,
which describes the values of the program variables after n iterations of the
loop. Formally, a tuple of arithmetic expressions cln
x = (cln
x1, . . . , cln
xd) over
the variables x = (x1, . . . , xd) and the distinguished variable n is a (normalized)
closed form for the update η with start value n0 ≥0 if for all 1 ≤i ≤d
and all σ : {x1, . . . , xd, n} →Z with σ(n) ≥n0, we have σ(cln
xi) = σ(ηn(xi)).
As shown in [14,15,19], for tnn-loops such a normalized closed form and the
start value n0 can be computed by handling one variable after the other, and
these normalized closed forms can be represented as so-called normalized poly-
exponential expressions. Here, N≥m stands for {x ∈N | x ≥m}.
Deﬁnition 18. (Normalized Poly-Exponential Expression
[14,15,19]).
Let PV = {x1, . . . , xd}. Then we deﬁne the set of all normalized poly-exponential
expressions by NPE = {ℓ
j=1 pj · naj · bn
j
 ℓ, aj ∈N, pj ∈Q[PV], bj ∈N≥1}.
Example 19. A normalized closed form (with start value n0 = 0) for the tnn-loop
in Example 17 is cln
x1 = x1 · 4n, cln
x2 = (x2 −x3
3) · 9n + x3
3, and cln
x3 = x3.
Using the normalized closed form, similar to [15] one can represent non-
termination of a tnn-loop (ψ, ϕ, η) by the formula
∃x ∈Zd, m ∈N. ∀n ∈N≥m. ψ ∧ϕ[x/cln
x].
(2)

742
N. Lommen et al.
Here, ϕ[x/cln
x] means that each variable xi in ϕ is replaced by cln
xi. Since ψ
is an update-invariant, if ψ holds, then ψ[x/cln
x] holds as well for all n ≥n0.
Hence, whenever ∀n ∈N≥m. ψ ∧ϕ[x/cln
x] holds, then clmax{n0,m}
x
witnesses
non-termination. Thus, invalidity of (2) is equivalent to termination of the loop.
Normalized poly-exponential expressions have the advantage that it is always
clear which addend determines their asymptotic growth when increasing n. So
as in [15], (2) can be transformed into an existential formula and we use an SMT
solver to prove its invalidity in order to prove termination of the loop. As shown
in [15, Theorem 42], non-termination of twn-loops over Z is semi-decidable and
deciding termination is Co-NP-complete if the loop is linear and the eigenvalues
of the update matrix are rational.
4.2
Runtime Bounds for Twn-Loops via Stabilization Thresholds
As observed in [19], since the closed forms for tnn-loops are poly-exponential
expressions that are weakly monotonic in n, every tnn-loop (ψ, ϕ, η) stabilizes
for each input e ∈Zd. So there is a number of loop iterations (a stabilization
threshold sth(ψ,ϕ,η)(e)), such that the truth value of the loop guard ϕ does not
change anymore when performing further loop iterations. Hence, the runtime of
every terminating tnn-loop is bounded by its stabilization threshold.
Deﬁnition 20 (Stabilization Threshold). Let (ψ, ϕ, η) be a tnn-loop with
PV = {x1, . . . , xd}. For each e = (e1, . . . , ed) ∈Zd, let σe ∈Σ with σe(xi) = ei
for all 1 ≤i ≤d. Let Ψ ⊆Zd such that e ∈Ψ iﬀσe(ψ) holds. Then sth(ψ,ϕ,η) :
Zd →N is the stabilization threshold of (ψ, ϕ, η) if for all e ∈Ψ, sth(ψ,ϕ,η)(e)
is the smallest number such that σe

ηn(ϕ) ↔ηsth(ψ,ϕ,η)(e)(ϕ)

holds for all
n ≥sth(ψ,ϕ,η)(e).
For the tnn-loop from Example 17, it will turn out that 2 · x2 + 2 · x3
3 + 2 · x5
3 + 1
is an upper bound on its stabilization threshold, see Example 28.
To compute such upper bounds on a tnn-loop’s stabilization threshold (i.e.,
upper bounds on its runtime if the loop is terminating), we now present a con-
struction based on monotonicity thresholds, which are computable [19, Lemma
12].
Deﬁnition 21 (Monotonicity Threshold
[19]). Let (b1, a1), (b2, a2) ∈N2
such that (b1, a1) >lex (b2, a2) (i.e., b1 > b2 or both b1 = b2 and a1 > a2). For
any k ∈N≥1, the k-monotonicity threshold of (b1, a1) and (b2, a2) is the smallest
n0 ∈N such that for all n ≥n0 we have na1 · bn
1 > k · na2 · bn
2.
For example, the 1-monotonicity threshold of (4, 0) and (3, 1) is 7 as the largest
root of f(n) = 4n −n · 3n is approximately 6.5139.
Our procedure again instantiates the variables of the loop guard ϕ by the nor-
malized closed form cln
x of the loop’s update. However, in the poly-exponential
expressions ℓ
j=1 pj · naj · bn
j resulting from ϕ[x/cln
x], the corresponding tech-
nique of [19, Lemma 21] over-approximated the polynomials pj by a polynomial

Automatic Complexity Analysis of Integer Programs
743
that did not distinguish the eﬀects of the diﬀerent variables x1, . . . , xd. Such an
over-approximation is only useful for a direct asymptotic bound on the runtime
of the twn-loop, but it is too coarse for a useful local runtime bound within the
complexity analysis of a larger program. For instance, in Example 12 it is crucial
to obtain local bounds like 4 · x2 + 4 · x3
3 + 4 · x5
3 + 3 which indicate that only
the variable x3 may inﬂuence the runtime with an exponent of 3 or 5. Thus, if
the size of x3 is bound by a constant, then the resulting global bound becomes
linear.
So we now improve precision and over-approximate the polynomials pj by
the polynomial ⊔{p1, . . . , pℓ} which contains every monomial xe1
1 · . . . · xed
d
of
{p1, . . . , pℓ}, using the absolute value of the largest coeﬃcient with which the
monomial occurs in {p1, . . . , pℓ}. Thus, ⊔{x3
3 −x5
3, x2 −x3
3} = x2 + x3
3 + x5
3. In
the following let x = (x1, . . . , xd), and for e = (e1, . . . , ed) ∈Nd, xe denotes
xe1
1 · . . . · xed
d .
Deﬁnition 22 (Over-Approximation of Polynomials).
Let p1, . . . , pℓ∈
Z[x], and for all 1 ≤j ≤ℓ, let Ij ⊆(Z\{0})×Nd be the index set of the polyno-
mial pj where pj = 
(c,e)∈Ij c·xe and there are no c ̸= c′ with (c, e), (c′, e) ∈Ij.
For all e ∈Nd we deﬁne ce ∈N with ce = max{|c| | (c, e) ∈I1 ∪. . . ∪Iℓ},
where max ∅= 0. Then the over-approximation of p1, . . . , pℓis ⊔{p1, . . . , pℓ} =

e∈Nd ce · xe.
Clearly, ⊔{p1, . . . , pℓ} indeed over-approximates the absolute value of each pj.
Corollary 23 (Soundness of
⊔{p1, . . . , pℓ}).
For all σ : {x1, . . . , xd} →Z
and all 1 ≤j ≤ℓ, we have |σ|(⊔{p1, . . . , pℓ}) ≥|σ(pj)|.
A drawback is that ⊔{p1, . . . , pℓ} considers all monomials and to obtain
weakly monotonically increasing bounds from B, it uses the absolute values of
their coeﬃcients. This can lead to polynomials of unnecessarily high degree. To
improve the precision of the resulting bounds, we now allow to over-approximate
the poly-exponential expressions ℓ
j=1 pj ·naj ·bn
j which result from instantiating
the variables of the loop guard by the closed form. For this over-approximation,
we take the invariant ψ of the tnn-loop into account. So while (2) showed that
update-invariants ψ can restrict the sets of possible witnesses for non-termination
and thus simplify the termination proofs of twn-loops, we now show that pre-
conditions ψ can also be useful to improve the bounds on twn-loops.
More precisely, Deﬁnition 24 allows us to replace addends p·na·bn by p·ni·jn
where (j, i) >lex (b, a) if the monomial p is always positive (when the precondition
ψ is fulﬁlled) and where (b, a) >lex (i, j) if p is always non-positive.
Deﬁnition 24 (Over-Approximation of Poly-Exponential Expressions).
Let ψ ∈F(PV) and let npe = 
(p,a,b)∈Λ p · na · bn ∈NPE where Λ is a set of
tuples (p, a, b) containing a monomial2 p and two numbers a, b ∈N. Here, we
2 Here, we consider monomials of the form p = c · xe1
1 · . . . · xed
d with coeﬃcients c ∈Q.

744
N. Lommen et al.
may have (p, a, b), (p′, a, b) ∈Λ for p ̸= p′. Let Δ, Γ ⊆Λ such that |= ψ →(p > 0)
holds for all (p, a, b) ∈Δ and |= ψ →(p ≤0) holds for all (p, a, b) ∈Γ.3 Then
⌈npe⌉ψ
Δ,Γ =

(p,a,b)∈Δ⊎Γ p · ni(p,a,b) · jn
(p,a,b) +

(p,a,b)∈Λ\(Δ⊎Γ ) p · na · bn
is an over-approximation of npe if i(p,a,b), j(p,a,b) ∈N are numbers such that
(j(p,a,b), i(p,a,b)) >lex (b, a) holds if (p, a, b) ∈Δ and (b, a) >lex (j(p,a,b), i(p,a,b))
holds if (p, a, b) ∈Γ. Note that i(p,a,b) or j(p,a,b) can also be 0.
Example 25. Let npe = q3 ·16n +q2 ·9n +q1 = q3 ·16n +q′
2 ·9n +q′′
2 ·9n +q′
1 +q′′
1 ,
where q3 = −x2
1, q2 = q′
2 + q′′
2, q′
2 = x2, q′′
2 = −x3
3, q1 = q′
1 + q′′
1, q′
1 = x3
3,
q′′
1 = −x5
3, and ψ = (x3 > 0). We can choose Δ = {(x3
3, 0, 1)} since |= ψ →
(x3
3 > 0) and Γ = {(−x5
3, 0, 1)} since |= ψ →(−x5
3 ≤0). Moreover, we choose
j(x3
3,0,1) = 9, i(x3
3,0,1) = 0, which is possible since (9, 0) >lex (1, 0). Similarly, we
choose j(−x5
3,0,1) = 0, i(−x5
3,0,1) = 0, since (1, 0) >lex (0, 0). Thus, we replace
x3
3 and −x5
3 by the larger addends x3
3 · 9n and 0. The motivation for the latter
is that this removes all addends with exponent 5 from npe. The motivation
for the former is that then, we have both the addends −x3
3 · 9n and x3
3 · 9n in
the expression which cancel out, i.e., this removes all addends with exponent 3.
Hence, we obtain ⌈npe⌉ψ
Δ,Γ = p2 · 16n + p1 · 9n with p2 = −x2
1 and p1 = x2. To
ﬁnd a suitable over-approximation which removes addends with high exponents,
our implementation uses a heuristic for the choice of Δ, Γ, i(p,a,b), and j(p,a,b).
The following lemma shows the soundness of the over-approximation
⌈npe⌉ψ
Δ,Γ .
Lemma 26 (Soundness of ⌈npe⌉ψ
Δ,Γ ). Let ψ, npe, Δ, Γ, i(p,a,b), j(p,a,b), and
⌈npe⌉ψ
Δ,Γ be as in Deﬁnition 24, and let D⌈npe⌉ψ
Δ,Γ =
max( {1-monotonicity threshold of (j(p,a,b), i(p,a,b)) and (b, a) | (p, a, b) ∈Δ}
∪{1-monotonicity threshold of (b, a) and (j(p,a,b), i(p,a,b)) | (p, a, b) ∈Γ}).
Then for all e ∈Ψ and all n ≥D⌈npe⌉ψ
Δ,Γ , we have σe(⌈npe⌉ψ
Δ,Γ ) ≥σe(npe).
For any terminating tnn-loop (ψ, ϕ, η), Theorem 27 now uses the new con-
cepts of Deﬁnition 22 and 24 to compute a polynomial sth⊔which is an upper
bound on the loop’s stabilization threshold (and hence, on its runtime). For any
atom α = (s1 < s2) (resp. s2 −s1 > 0) in the loop guard ϕ, let npeα ∈NPE be
a poly-exponential expression which results from multiplying (s2 −s1)[x/cln
x]
with the least common multiple of all denominators occurring in (s2−s1)[x/cln
x].
Since the loop is terminating, for some of these atoms this expression will become
non-positive for large enough n and our goal is to compute bounds on their
corresponding stabilization thresholds. First, one can replace npeα by an over-
approximation ⌈npeα⌉ψ′
Δ,Γ where ψ′ = (ψ ∧ϕ) considers both the invariant ψ
3 Δ and Γ do not have to contain all such tuples, but can be (possibly empty) subsets.

Automatic Complexity Analysis of Integer Programs
745
and the guard ϕ. Let Ψ ′ ⊆Zd such that e ∈Ψ ′ iﬀσe(ψ′) holds. By Lemma
26 (i.e., σe(⌈npeα⌉ψ′
Δ,Γ ) ≥σe(npeα) for all e ∈Ψ ′), it suﬃces to compute a
bound on the stabilization threshold of ⌈npeα⌉ψ′
Δ,Γ if it is always non-positive for
large enough n, because if ⌈npeα⌉ψ′
Δ,Γ is non-positive, then so is npeα. We say
that an over-approximation ⌈npeα⌉ψ′
Δ,Γ is eventually non-positive iﬀwhenever
⌈npeα⌉ψ′
Δ,Γ ̸= npeα, then one can show that for all e ∈Ψ ′, σe(⌈npeα⌉ψ′
Δ,Γ ) is
always non-positive for large enough n.4 Using over-approximations ⌈npeα⌉ψ′
Δ,Γ
can be advantageous because ⌈npeα⌉ψ′
Δ,Γ may contain less monomials than npeα
and thus, the construction ⊔from Deﬁnition 22 can yield a polynomial of lower
degree. So although npeα’s stabilization threshold might be smaller than the one
of ⌈npeα⌉ψ′
Δ,Γ , our technique might compute a smaller bound on the stabilization
threshold when considering ⌈npeα⌉ψ′
Δ,Γ instead of npe.
Theorem 27 (Bound on Stabilization Threshold). Let L = (ψ, ϕ, η) be a
terminating tnn-loop, let ψ′ = (ψ ∧ϕ), and let cln
x be a normalized closed form
for η with start value n0. For every atom α = (s1 < s2) in ϕ, let ⌈npeα⌉ψ′
Δ,Γ be an
eventually non-positive over-approximation of npeα and let Dα = D⌈npeα⌉ψ′
Δ,Γ .
If ⌈npeα⌉ψ′
Δ,Γ = ℓ
j=1 pj·naj ·bn
j with pj ̸= 0 for all 1 ≤j ≤ℓand (bℓ, aℓ) >lex
. . . >lex (b1, a1), then let Cα = max{1, N2, M2, . . . , Nℓ, Mℓ}, where we have:
Mj =
⎧
⎨
⎩
0,
if bj = bj−1
1-monotonicity threshold of
(bj, aj) and (bj−1, aj−1 + 1), if bj > bj−1
Nj =
⎧
⎨
⎩
1,
if j = 2
mt′,
if j = 3
max{mt, mt′}, if j > 3
Here, mt′ is the (j −2)-monotonicity threshold of (bj−1, aj−1) and (bj−2, aj−2)
and mt = max{1-monotonicity threshold of (bj−2, aj−2) and (bi, ai) | 1 ≤i ≤
j−3}. Let Polα = {p1, . . . , pℓ−1}, Pol = 
atom α occurs in ϕ Polα, C = max{Cα |
atom α occurs in ϕ}, D = max{Dα | atom α occurs in ϕ}, and sth⊔∈Z[x] with
sth⊔= 2 · ⊔Pol + max{n0, C, D}. Then for all e ∈Ψ ′, we have |σe|(sth⊔) ≥
sth(ψ,ϕ,η)(e). If the tnn-loop has the initial transition t0 and looping transition
t, then RBglo(t0) = 1 and RBglo(t) = sth⊔is a global runtime bound for L.
Example 28. The guard ϕ of the tnn-loop in Example 17 has the atoms α =
(x2
1 + x5
3 < x2), α′ = (0 < x1), and α′′ = (0 < −x1) (since x1 ̸= 0 is transformed
into α′∨α′′). When instantiating the variables by the closed forms of Example 19
with start value n0 = 0, Theorem 27 computes the bound 1 on the stabilization
thresholds for α′ and α′′. So the only interesting atom is α = (0 < s2 −s1) for
s1 = x2
1 +x5
3 and s2 = x2. We get npeα = (s2 −s1)[x/cln
x] = q3 ·16n +q2 ·9n +q1,
with qj as in Example 25.
4 This can be shown similar to the proof of (2) for (non-)termination of the loop. Thus,
we transform ∃x ∈Zd, m ∈N. ∀n ∈N≥m. ψ′ ∧⌈npeα⌉ψ′
Δ,Γ > 0 into an existential
formula as in [15] and try to prove its invalidity by an SMT solver.

746
N. Lommen et al.
In the program of Fig. 1, the corresponding self-loop t5 has two entry tran-
sitions t4 and t1 which result in two tnn-loops with the update-invariants
ψ1 = true resulting from transition t4 and ψ2 = (x3 > 0) from t1. So ψ2 is
an update-invariant of t5 which always holds when reaching t5 via transition t1.
For ψ1 = true, we choose Δ = Γ = ∅, i.e., ⌈npeα⌉ψ′
1
Δ,Γ = npeα. So we have
b3 = 16, b2 = 9, b1 = 1, and aj = 0 for all 1 ≤j ≤3. We obtain
M2 = 0, as 0 is the 1-monotonicity threshold of (9, 0) and (1, 1)
M3 = 0, as 0 is the 1-monotonicity threshold of (16, 0) and (9, 1)
N2 = 1 and N3 = 1, as 1 is the 1-monotonicity threshold of (9, 0) and (1, 0).
Hence, we get C = Cα = max{1, N2, M2, N3, M3} = 1. So we obtain the runtime
bound sth⊔
ψ1 = 2·⊔{q1, q2}+max{n0, Cα} = 2·x2 +2·x3
3 +2·x5
3 +1 for the loop
t5⋆t5 w.r.t. ψ1. By Lemma 16, this means that 2·sth⊔
ψ1+1 = 4·x2+4·x3
3+4·x5
3+3
is a runtime bound for the loop at transition t5.
For the update-invariant ψ2 = (x3 > 0), we use the over-approximation
⌈npeα⌉ψ′
2
Δ,Γ = p2 · 16n + p1 · 9n with p2 = −x2
1 and p1 = x2 from Example 25,
where ψ′
2 = (ψ2∧ϕ) implies that it is always non-positive for large enough n. Now
we obtain M2 = 0 (the 1-monotonicity threshold of (16, 0) and (9, 1)) and N2 = 1,
where C = Cα = max{1, N2, M2} = 1. Moreover, we have Dα = max{1, 0} = 1,
since
1 is the 1-monotonicity threshold of (9, 0) and (1, 0), and
0 is the 1-monotonicity threshold of (1, 0) and (0, 0).
We now get the tighter bound sth⊔
ψ2 = 2 · ⊔{p1} + max{n0, Cα, Dα} = 2 · x2 + 1
for t5 ⋆t5. So t5’s runtime bound is 2 · sth⊔
ψ2 + 1 = 4 · x2 + 3 when using invariant
ψ2.
Theorem 29 shows how the technique of Lemma 16 and Theorem 27 can
be used to compute local runtime bounds for twn-loops whenever such loops
occur within an integer program. To this end, one needs the new Theorem 11
where in contrast to [6,18] these local bounds do not have to result from ranking
functions.
To turn a self-loop t and r ∈E{t} from a larger program P into a twn-loop
(ψ, ϕ, η), we use t’s guard ϕ and update η. To obtain an update-invariant ψ, our
implementation uses the Apron library [23] for computing invariants on a version
of the full program where we remove all entry transitions E{t} except r.5 From
the invariants computed for t, we take those that are also update-invariants of t.
Theorem 29 (Local Bounds for Twn-Loops).
Let P = (PV, L, ℓ0, T ) be
an integer program with PV′ = {x1, . . . , xd} ⊆PV. Let t = (ℓ, ϕ, η, ℓ) ∈T with
ϕ ∈F(PV′), η(v) ∈Z[PV′] for all v ∈PV′, and η(v) = v for all v ∈PV\PV′.
For any entry transition r ∈E{t}, let ψ ∈F(PV′) such that |= ψ →η(ψ) and
5 Regarding invariants for the full program in the computation of local bounds for t
is possible since in contrast to [6,18] our deﬁnition of local bounds from Deﬁnition
9 is restricted to states that are reachable from an initial conﬁguration (ℓ0, σ0).

Automatic Complexity Analysis of Integer Programs
747
such that σ(ψ) holds whenever there is a σ0 ∈Σ with (ℓ0, σ0) →∗
T ◦→r (ℓ, σ).
If L = (ψ, ϕ, η) is a terminating tnn-loop, then let RBloc(→r {t}) = sth⊔, where
sth⊔is deﬁned as in Theorem 27. If L is a terminating twn-loop but no tnn-
loop, let RBloc(→r {t}) = 2 · sth⊔+ 1, where sth⊔is the bound of Theorem 27
computed for L ⋆L. Otherwise, let RBloc(→r {t}) = ω. Then RBloc is a local
runtime bound for {t} = T ′
> = T ′ in the program P.
Example 30. In Fig. 1, we consider the self-loop t5 with E{t5} = {t4, t1} and the
update-invariants ψ1 = true resp. ψ2 = (x3 > 0). For t5’s guard ϕ and update
η, both (ψi, ϕ, η) are terminating twn-loops (see Example 14), i.e., (2) is invalid.
By Theorem 29 and Example 28, RBloc with RBloc(→t4 {t5}) = 4 · x2 + 4 ·
x3
3 + 4 · x5
3 + 3 and RBloc(→t1 {t5}) = 4 · x2 + 3 is a local runtime bound for
{t5} = T ′
> = T ′ in the program of Fig. 1. As shown in Example 12, Theorem 11
then yields the global runtime bound RBglo(t5) = 8 · x4 · x5 + 13006 · x4.
5
Local Runtime Bounds for Twn-Cycles
Section 4 introduced a technique to determine local runtime bounds for twn-self-
loops in a program. To increase its applicability, we now extend it to larger
cycles. For every entry transition of the cycle, we chain the transitions of the
cycle, starting with the transition which follows the entry transition. In this way,
we obtain loops consisting of a single transition. If the chained loop is a twn-loop,
we can apply Theorem 29 to compute a local runtime bound. Any local bound
on the chained transition is also a bound on each of the original transitions.6
By Theorem 29, we obtain a bound on the number of evaluations of the
complete cycle. However, we also have to consider a partial execution which
stops before traversing the full cycle. Therefore, we increase every local runtime
bound by 1.
Note that this replacement of a cycle by a self-loop which results from chain-
ing its transitions is only sound for simple cycles. A cycle is simple if each itera-
tion through the cycle can only be done in a unique way. So the cycle must not
have any subcycles and there also must not be any indeterminisms concerning
the next transition to be taken. Formally, C = {t1, . . . , tn} ⊂T is a simple cycle
if C does not contain temporary variables and there are pairwise diﬀerent loca-
tions ℓ1, . . . , ℓn such that ti = (ℓi, , , ℓi+1) for 1 ≤i ≤n−1 and tn = (ℓn, , , ℓ1).
This ensures that if there is an evaluation with →ti ◦→∗
C\{ti} ◦→ti, then the
steps with →∗
C\{ti} have the form →ti+1 ◦. . . ◦→tn ◦→t1 ◦. . . ◦→ti−1.
Algorithm 1 describes how to compute a local runtime bound for a simple
cycle C = {t1, . . . , tn} as above. In the loop of Line 2, we iterate over all entry
transitions r of C. If r reaches the transition ti, then in Line 3 and 4 we chain
ti ⋆. . . ⋆tn ⋆t1 ⋆. . . ⋆ti−1 which corresponds to one iteration of the cycle starting
6 This is suﬃcient for our improved deﬁnition of local bounds in Deﬁnition 9 where in
contrast to [6,18] we do not require a bound on the sum but only on each transition
in the considered set T ′. Moreover, here we again beneﬁt from our extension to
compute individual local bounds for diﬀerent entry transitions.

748
N. Lommen et al.
Algorithm 1. Algorithm to Compute Local Runtime Bounds for Cycles
input
: A program (PV, L, ℓ0, T ) and a simple cycle C = {t1, . . . , tn} ⊂T
output : A local runtime bound RBloc for C = T ′
> = T ′
1 Initialize RBloc: RBloc(→r C) = ω for all r ∈EC.
2 forall r ∈EC do
3
Let i ∈{1, . . . , n} such that r’s target location is the start location ℓi of ti.
4
Let t = ti ⋆. . . ⋆tn ⋆t1 ⋆. . . ⋆ti−1.
5
if there exists a renaming π of PV such that π(t) results in a twn-loop then
6
Set RBloc(→r C) ←π−1(1 + result of Theorem 29 on π(t) and π(r)).
7 return local runtime bound RBloc.
Fig. 2. An Integer Program with a Nested Non-Self-Loop
in ti. If a suitable renaming (and thus also reordering) of the variables turns the
chained transition into a twn-loop, then we use Theorem 29 to compute a local
runtime bound RBloc(→r C) in Lines 5 and 6. If the chained transition does not
give rise to a twn-loop, then RBloc(→r C) is ω (Line 1). In practice, to use the
twn-technique for a transition t in a program, our tool KoAT searches for those
simple cycles that contain t and where the chained cycle is a twn-loop. Among
those cycles it chooses the one with the smallest runtime bounds for its entry
transitions.
Theorem 31 (Correctness of Algorithm 1). Let P = (PV, L, ℓ0, T ) be an
integer program and let C ⊂T be a simple cycle in P. Then the result RBloc :
EC →B of Algorithm 1 is a local runtime bound for C = T ′
> = T ′.
Example 32. We apply Algorithm 1 on the cycle C = {t5a, t5b} of the program
in Fig. 2. C’s entry transitions t1 and t4 both end in ℓ3. Chaining t5a and t5b
yields the transition t5 of Fig. 1, i.e., t5 = t5a ⋆t5b. Thus, Algorithm 1 essentially
transforms the program of Fig. 2 into Fig. 1. As in Example 28 and 30, we obtain
RBloc(→t4 C) = 1 + (2 · sth⊔
true + 1) = 4 · x2 + 4 · x3
3 + 4 · x5
3 + 4 and RBloc(→t1
C) = 1 + (2 · sth⊔
x3>0 + 1) = 4 · x2 + 4, resulting in the global runtime bound
RBglo(t5a) = RBglo(t5b) = 8 · x4 · x5 + 13008 · x4, which again yields rc(σ0) ∈
O(n2).
6
Conclusion and Evaluation
We showed that results on subclasses of programs with computable complexity
bounds like [19] are not only theoretically interesting, but they have an impor-

Automatic Complexity Analysis of Integer Programs
749
tant practical value. To our knowledge, our paper is the ﬁrst to integrate such
results into an incomplete approach for automated complexity analysis like [6,18].
For this integration, we developed several novel contributions which extend and
improve the previous approaches in [6,18,19] substantially:
(a) We extended the concept of local runtime bounds such that they can now
depend on entry transitions (Deﬁnition 9).
(b) We generalized the computation of global runtime bounds such that one can
now lift arbitrary local bounds to global bounds (Theorem 11). In particular,
the local bounds might be due to either ranking functions or twn-loops.
(c) We improved the technique for the computation of bounds on twn-loops such
that these bounds now take the roles of the diﬀerent variables into account
(Deﬁnition 22, Corollary 23, and Theorem 27).
(d) We extended the notion of twn-loops by update-invariants and developed
a new over-approximation of their closed forms which takes invariants into
account (Deﬁnition 13 and 24, Lemma 26, and Theorem 27).
(e) We extended the handling of twn-loops to twn-cycles (Theorem 31).
The need for these improvements is demonstrated by our leading example in
Fig. 1 (where the contributions (a)–(d) are needed to infer quadratic runtime
complexity) and by the example in Fig. 2 (which illustrates (e)). In this way, the
power of automated complexity analysis is increased substantially, because now
one can also infer runtime bounds for programs containing non-linear arithmetic.
To demonstrate the power of our approach, we evaluated the integration
of our new technique to infer local runtime bounds for twn-cycles in our re-
implementation of the tool KoAT (written in OCaml) and compare the results to
other state-of-the-art tools. To distinguish our re-implementation of KoAT from
the original version of the tool from [6], let KoAT1 refer to the tool from [6] and
let KoAT2 refer to our new re-implementation. KoAT2 applies a local control-
ﬂow reﬁnement technique [18] (using the tool iRankFinder [8]) and preprocesses
the program in the beginning, e.g., by extending the guards of transitions by
invariants inferred using the Apron library [23]. For all occurring SMT problems,
KoAT2 uses Z3 [28]. We tested the following conﬁgurations of KoAT2, which
diﬀer in the techniques used for the computation of local runtime bounds:
• KoAT2+RF only uses linear ranking functions to compute local runtime
bounds
• KoAT2+MΦRF5 uses multiphase-linear ranking functions of depth ≤5
• KoAT2+TWN only uses twn-cycles to compute local runtime bounds (Algo-
rithm 1)
• KoAT2+TWN+RF uses Algorithm 1 for twn-cycles and linear ranking func-
tions
• KoAT2+TWN+MΦRF5 uses Algorithm 1 for twn-cycles and MΦRFs of depth
≤5
Existing approaches for automated complexity analysis are already very pow-
erful on programs that only use linear arithmetic in their guards and updates.

750
N. Lommen et al.
Fig. 3. Evaluation on the Collection CINT+
The corresponding benchmarks for Complexity of Integer Transitions Systems
(CITS) and Complexity of C Integer Programs (CINT) from the Termination
Problems Data Base [33] which is used in the annual Termination and Com-
plexity Competition (TermComp) [17] contain almost only examples with linear
arithmetic. Here, the existing tools already infer ﬁnite runtimes for more than
89% of those examples in the collections CITS and CINT where this might7 be
possible.
The main beneﬁt of our new integration of the twn-technique is that in this
way one can also infer ﬁnite runtime bounds for programs that contain non-linear
guards or updates. To demonstrate this, we extended both collections CITS and
CINT by 20 examples that represent typical such programs, including several
benchmarks from the literature [3,14,15,18,20,34], as well as our programs from
Fig. 1 and 2. See [27] for a detailed list and description of these examples.
Figure 3 presents our evaluation on the collection CINT+, consisting of the 484
examples from CINT and our 20 additional examples for non-linear arithmetic.
We refer to [27] for the (similar) results on the corresponding collection CITS+.
In the C programs of CINT+, all variables are interpreted as integers over Z
(i.e., without overﬂows). For KoAT2 and KoAT1, we used Clang [7] and llvm2kittel
[10] to transform C programs into integer transitions systems as in Deﬁnition 2.
We compare KoAT2 with KoAT1 [6] and the tools CoFloCo [11,12], MaxCore [2]
with CoFloCo in the backend, and Loopus [31]. We do not compare with RaML
[21], as it does not support programs whose complexity depends on (possibly
negative) integers (see [29]). We also do not compare with PUBS [1], because as
stated in [9] by one of its authors, CoFloCo is stronger than PUBS. For the same
reason, we only consider MaxCore with the backend CoFloCo instead of PUBS.
All tools were run inside an Ubuntu Docker container on a machine with an
AMD Ryzen 7 3700X octa-core CPU and 48 GB of RAM. As in TermComp, we
applied a timeout of 5 min for every program.
In Fig. 3, the ﬁrst entry in every cell denotes the number of benchmarks from
CINT+ where the respective tool inferred the corresponding bound. The number
7 The tool LoAT [13,16] proves unbounded runtime for 217 of the 781 examples from
CITS and iRankFinder [4,8] proves non-termination for 118 of 484 programs of CINT.

Automatic Complexity Analysis of Integer Programs
751
in brackets is the corresponding number of benchmarks when only regarding
our 20 new examples for non-linear arithmetic. The runtime bounds computed
by the tools are compared asymptotically as functions which depend on the
largest initial absolute value n of all program variables. So for instance, there
are 26 + 231 = 257 programs in CINT+ (and 5 of them come from our new
examples) where KoAT2+TWN+MΦRF5 can show that rc(σ0) ∈O(n) holds for
all initial states σ0 where |σ0(v)| ≤n for all v ∈PV. For 26 of these programs,
KoAT2+TWN+MΦRF5 can even show that rc(σ0) ∈O(1), i.e., their runtime
complexity is constant. Overall, this conﬁguration succeeds on 344 examples, i.e.,
“< ∞” is the number of examples where a ﬁnite bound on the runtime complexity
could be computed by the respective tool within the time limit. “AVG+(s)” is
the average runtime of the tool on successful runs in seconds, i.e., where the tool
inferred a ﬁnite time bound before reaching the timeout, whereas “AVG(s)” is
the average runtime of the tool on all runs including timeouts.
On the original benchmarks CINT where very few examples contain non-linear
arithmetic, integrating TWN into a conﬁguration that already uses multiphase-
linear ranking functions does not increase power much: KoAT2+TWN+MΦRF5
succeeds on 344−15 = 329 such programs and KoAT2+MΦRF5 solves 328−1 =
327 examples. On the other hand, if one only has linear ranking functions, then an
improvement via our twn-technique has similar eﬀects as an improvement with
multiphase-linear ranking functions (here, the success rate of KoAT2+MΦRF5 is
similar to KoAT2+TWN+RF which solves 341 −15 = 326 such programs).
But the main beneﬁt of our technique is that it also allows to successfully han-
dle examples with non-linear arithmetic. Here, our new technique is signiﬁcantly
more powerful than previous ones. Other tools and conﬁgurations without TWN
in Fig. 3 solve at most 2 of the 20 new examples. In contrast, KoAT2+TWN+RF
and KoAT2+TWN+MΦRF5 both succeed on 15 of them.8 In particular, our run-
ning examples from Fig. 1 and 2 and even isolated twn-loops like t5 or t5 ⋆t5
from Example 14 and 17 can only be solved by KoAT2 with our twn-technique.
To summarize, our evaluations show that KoAT2 with the added twn-
technique outperforms all other conﬁgurations and tools for automated complex-
ity analysis on all considered benchmark sets (i.e., CINT+, CINT, CITS+, and
CITS) and it is the only tool which is also powerful on examples with non-linear
arithmetic.
KoAT’s source code, a binary, and a Docker image are available at https://
aprove-developers.github.io/KoAT TWN/. The website also has details on our
experiments and web interfaces to run KoAT’s conﬁgurations directly online.
Acknowledgments. We are indebted to M. Hark for many fruitful discussions about
complexity, twn-loops, and KoAT. We are grateful to S. Genaim and J. J. Dom´enech
for a suitable version of iRankFinder which we could use for control-ﬂow reﬁnement
in KoAT’s backend. Moreover, we thank A. Rubio and E. Mart´ın-Mart´ın for a static
binary of MaxCore, A. Flores-Montoya and F. Zuleger for help in running CoFloCo and
Loopus, F. Frohn for help and advice, and the reviewers for their feedback to improve
the paper.
8 One is the non-terminating leading example of [15], so at most 19 might terminate.

752
N. Lommen et al.
References
1. Albert, E., Arenas, P., Genaim, S., Puebla, G.: Automatic inference of upper
bounds for recurrence relations in cost analysis. In: Alpuente, M., Vidal, G. (eds.)
SAS 2008. LNCS, vol. 5079, pp. 221–237. Springer, Heidelberg (2008). https://doi.
org/10.1007/978-3-540-69166-2 15
2. Albert, E., Boﬁll, M., Borralleras, C., Mart´ın-Mart´ın, E., Rubio, A.: Resource
analysis driven by (conditional) termination proofs. Theory Pract. Logic Program.
19, 722–739 (2019). https://doi.org/10.1017/S1471068419000152
3. Ben-Amram, A.M., Genaim, S.: On multiphase-linear ranking functions. In:
Majumdar, R., Kunˇcak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 601–620.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-63390-9 32
4. Ben-Amram, A.M., Dom´enech, J.J., Genaim, S.: Multiphase-linear ranking func-
tions and their relation to recurrent sets. In: Chang, B.-Y.E. (ed.) SAS 2019. LNCS,
vol. 11822, pp. 459–480. Springer, Cham (2019). https://doi.org/10.1007/978-3-
030-32304-2 22
5. Braverman, M.: Termination of integer linear programs. In: Ball, T., Jones, R.B.
(eds.) CAV 2006. LNCS, vol. 4144, pp. 372–385. Springer, Heidelberg (2006).
https://doi.org/10.1007/11817963 34
6. Brockschmidt, M., Emmes, F., Falke, S., Fuhs, C., Giesl, J.: Analyzing runtime
and size complexity of integer programs. ACM Trans. Program. Lang. Syst. 38,
1–50 (2016). https://doi.org/10.1145/2866575
7. Clang Compiler. https://clang.llvm.org/
8. Dom´enech, J.J., Genaim, S.: iRankFinder. In: Lucas, S. (ed.) WST 2018, p. 83
(2018). http://wst2018.webs.upv.es/wst2018proceedings.pdf
9. Dom´enech, J.J., Gallagher, J.P., Genaim, S.: Control-ﬂow reﬁnement by partial
evaluation, and its application to termination and cost analysis. Theory Pract.
Logic Program. 19, 990–1005 (2019). https://doi.org/10.1017/S1471068419000310
10. Falke, S., Kapur, D., Sinz, C.: Termination analysis of C programs using compiler
intermediate languages. In: Schmidt-Schauß, M. (ed.) RTA 2011. LIPIcs, vol. 10,
pp. 41–50 (2011). https://doi.org/10.4230/LIPIcs.RTA.2011.41
11. Flores-Montoya, A., H¨ahnle, R.: Resource analysis of complex programs with cost
equations. In: Garrigue, J. (ed.) APLAS 2014. LNCS, vol. 8858, pp. 275–295.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-12736-1 15
12. Flores-Montoya, A.: Upper and lower amortized cost bounds of programs expressed
as cost relations. In: Fitzgerald, J., Heitmeyer, C., Gnesi, S., Philippou, A. (eds.)
FM 2016. LNCS, vol. 9995, pp. 254–273. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-48989-6 16
13. Frohn, F., Giesl, J.: Proving non-termination via loop acceleration. In: Barrett,
C.W., Yang, J. (eds.) FMCAD 2019, pp. 221–230 (2019). https://doi.org/10.23919/
FMCAD.2019.8894271
14. Frohn, F., Giesl, J.: Termination of triangular integer loops is decidable. In: Dillig,
I., Tasiran, S. (eds.) CAV 2019. LNCS, vol. 11562, pp. 426–444. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-25543-5 24
15. Frohn, F., Hark, M., Giesl, J.: Termination of polynomial loops. In: Pichardie, D.,
Sighireanu, M. (eds.) SAS 2020. LNCS, vol. 12389, pp. 89–112. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-65474-0 5, https://arxiv.org/abs/1910.
11588
16. Frohn, F., Naaf, M., Brockschmidt, M., Giesl, J.: Inferring lower runtime bounds
for integer programs. ACM Trans. Program. Lang. Syst. 42, 1–50 (2020). https://
doi.org/10.1145/3410331

Automatic Complexity Analysis of Integer Programs
753
17. Giesl, J., Rubio, A., Sternagel, C., Waldmann, J., Yamada, A.: The termination and
complexity competition. In: Beyer, D., Huisman, M., Kordon, F., Steﬀen, B. (eds.)
TACAS 2019. LNCS, vol. 11429, pp. 156–166. Springer, Cham (2019). https://doi.
org/10.1007/978-3-030-17502-3 10
18. Giesl, J., Lommen, N., Hark, M., Meyer, F.: Improving automatic complexity anal-
ysis of integer programs. In: The Logic of Software: A Tasting Menu of Formal
Methods. LNCS, vol. 13360 (to appear). Also appeared in CoRR, abs/2202.01769.
https://arxiv.org/abs/2202.01769
19. Hark, M., Frohn, F., Giesl, J.: Polynomial loops: beyond termination. In: Albert,
E., Kov´acs, L. (eds.) LPAR 2020, EPiC, vol. 73, pp. 279–297 (2020). https://doi.
org/10.29007/nxv1
20. Heizmann, M., Leike, J.: Ranking templates for linear loops. Log. Methods Comput.
Sci. 11(1), 16 (2015). https://doi.org/10.2168/LMCS-11(1:16)2015
21. Hoﬀmann, J., Das, A., Weng, S.-C.: Towards automatic resource bound analysis
for OCaml. In: Castagna, G., Gordon, A.D. (eds.) POPL 2017, pp. 359–373 (2017).
https://doi.org/10.1145/3009837.3009842
22. Hosseini, M., Ouaknine, J., Worrell, J.: Termination of linear loops over the integers.
In: Baier, C., Chatzigiannakis, I., Flocchini, P., Leonardi, S. (eds.) ICALP 2019,
LIPIcs, vol. 132 (2019). https://doi.org/10.4230/LIPIcs.ICALP.2019.118
23. Jeannet, B., Min´e, A.: Apron: a library of numerical abstract domains for static
analysis. In: Bouajjani, A., Maler, O. (eds.) CAV 2009. LNCS, vol. 5643, pp. 661–
667. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-02658-4 52
24. Kincaid, Z., Breck, J., Cyphert, J., Reps, T.W.: Closed forms for numerical
loops. Proc. ACM Program. Lang. 3(POPL), 1–29 (2019). https://doi.org/10.1145/
3290368
25. Kov´acs, L.: Reasoning algebraically about p-solvable loops. In: Ramakrishnan,
C.R., Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 249–264. Springer, Hei-
delberg (2008). https://doi.org/10.1007/978-3-540-78800-3 18
26. Lommen, N., Meyer, F., Giesl, J.: Automatic complexity analysis of integer
programs via triangular weakly non-linear loops. CoRR abs/2205.08869 (2022).
https://arxiv.org/abs/2205.08869
27. Lommen, N., Meyer, F., Giesl, J.: Empirical evaluation of: “Automatic complex-
ity analysis of integer programs via triangular weakly non-linear loops”. https://
aprove-developers.github.io/KoAT TWN/
28. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
29. RaML (Resource Aware ML). https://www.raml.co/interface/
30. Rodr´ıguez-Carbonell, E., Kapur, D.: Automatic generation of polynomial loop
invariants: algebraic foundation. In: Gutierrez, J. (ed.) ISSAC 2004, pp. 266–273
(2004). https://doi.org/10.1145/1005285.1005324
31. Sinn, M., Zuleger, F., Veith, H.: Complexity and resource bound analysis of imper-
ative programs using diﬀerence constraints. J. Autom. Reason. 59, 3–45 (2017).
https://doi.org/10.1007/s10817-016-9402-4
32. Tiwari, A.: Termination of linear programs. In: Alur, R., Peled, D.A. (eds.) CAV
2004. LNCS, vol. 3114, pp. 70–82. Springer, Heidelberg (2004). https://doi.org/10.
1007/978-3-540-27813-9 6
33. TPDB (Termination Problems Data Base). https://github.com/TermCOMP/
TPDB
34. Xu, M., Li, Z.-B.: Symbolic termination analysis of solvable loops. J. Symb. Com-
put. 50, 28–49 (2013). https://doi.org/10.1016/j.jsc.2012.05.005

754
N. Lommen et al.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Author Index
Akshay, S.
671
Albert, Elvira
3
Alrabbaa, Christian
271
Baader, Franz
271
Barbosa, Haniel
15
Barrett, Clark
15, 95, 125
Berg, Jeremias
75
Bernreiter, Michael
331
Bidoit, Nicole
310
Bílková, Marta
429
Blaisdell, Eben
449
Borgwardt, Stefan
271
Bouziane, Hinde Lilia
359
Bozga, Marius
691
Bromberger, Martin
147
Brown, Chad E.
350
Bryant, Randal E.
106
Bueri, Lucas
691
Cailler, Julie
359
Cauli, Claudia
281
Chakraborty, Supratik
671
Chlebowski, Szymon
407
Dachselt, Raimund
271
Das, Anupam
509
Delahaye, David
359
Dill, David
125
Dixon, Clare
486
Dowek, Gilles
8
Draheim, Dirk
300
Duarte, André
169
Durán, Francisco
529
Eker, Steven
529
Escobar, Santiago
529
Felli, Paolo
36
Ferrari, Mauro
57
Fiorentini, Camillo
57
Frittella, Sabine
429
Frohn, Florian
712
Fujita, Tomohiro
388
Gallicchio, James
723
Giesl, Jürgen
712, 734
Girlando, Marianna
509
Gordillo, Pablo
3
Greati, Vitor
640
Grieskamp, Wolfgang
125
Haifani, Fajar
188, 208
Hernández-Cerezo, Alejandro
3
Heskes, Tom
597
Heule, Marijn J. H.
106
Holden, Sean B.
559
Holub, Štˇepán
369
Hustadt, Ullrich
486
Ihalainen, Hannes
75
Indrzejczak, Andrzej
541
Iosif, Radu
691
Janota, Mikoláš
597
Järv, Priit
300
Järvisalo, Matti
75
Jukiewicz, Marcin
407
Kaliszyk, Cezary
350
Kanovich, Max
449
Koopmann, Patrick
188, 271
Korovin, Konstantin
169
Kozhemiachenko, Daniil
429
Kremer, Gereon
15, 95
Kutsia, Temur
578
Kuznetsov, Stepan L.
449
Lachnitt, Hanna
15
Lahav, Ori
468
Leidinger, Hendrik
228
Leszczy´nska-Jasion, Dorota
407
Leutgeb, Lorenz
147
Lolic, Anela
331
Lommen, Nils
734
Ma, Yue
310
Maly, Jan
331
Mangla, Chaitanya
559
Marcos, João
640
Martí-Oliet, Narciso
529

756
Author Index
Matsuzaki, Takuya
388
Méndez, Julián
271
Meseguer, José
529
Meyer, Fabian
734
Mitsch, Stefan
723
Montali, Marco
36
Nalon, Cláudia
486
Niemetz, Aina
15
Nötzli, Andres
15, 125
Ortiz, Magdalena
281
Ozdemir, Alex
15
Pal, Debtanu
671
Papacchini, Fabio
486
Park, Junkil
125
Pau, Cleo
578
Paulson, Lawrence C.
559
Piepenbrock, Jelle
597
Pimentel, Elaine
449
Piterman, Nir
281
Platzer, André
723
Popescu, Andrei
618
Preiner, Mathias
15
Qadeer, Shaz
125
Raška, Martin
369
Reeves, Joseph E.
106
Reynolds, Andrew
15, 95, 125
Robillard, Simon
359
Rodríguez-Núñez, Clara
3
Rosain, Johann
359
Rubio, Albert
3
Rubio, Rubén
529
Scedrov, Andre
449
Sheng, Ying
125
Socha´nski, Michał
407
Starosta, Štˇepán
369
Suda, Martin
659
Talcott, Carolyn
529
Tammet, Tanel
300
Tan, Yong Kiam
723
Tinelli, Cesare
15, 95, 125
Tomczyk, Agata
407
Tourret, Sophie
188
Urban, Josef
597
Viswanathan, Arjun
15
Viteri, Scott
15
Weidenbach, Christoph
147, 188, 208, 228
Winkler, Sarah
36
Woltran, Stefan
331
Yamada, Akihisa
248
Yang, Hui
310
Zohar, Yoni
15, 125, 468

