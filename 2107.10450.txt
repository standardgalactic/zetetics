Learning Sparse Fixed-Structure Gaussian Bayesian Networks
Arnab Bhattcharyya
National University of Singapore
arnabb@nus.edu.sg
Davin Choo
National University of Singapore
davin@u.nus.edu
Rishikesh Gajjala
Indian Institute of Science, Bangalore
rishikeshg@iisc.ac.in
Sutanu Gayen
National University of Singapore
sutanugayen@gmail.com
Yuhao Wang
National University of Singapore
yohanna.wang0924@gmail.com
Abstract
Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation models) are widely used to
model causal interactions among continuous variables. In this work, we study the problem of learning a
ﬁxed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze
the commonly used node-wise least squares regression LeastSquares and prove that it has the near-optimal
sample complexity. We also study a couple of new algorithms for the problem:
• BatchAvgLeastSquares takes the average of several batches of least squares solutions at each node,
so that one can interpolate between the batch size and the number of batches.
We show that
BatchAvgLeastSquares also has near-optimal sample complexity.
• CauchyEst takes the median of solutions to several batches of linear systems at each node. We show
that the algorithm specialized to polytrees, CauchyEstTree, has near-optimal sample complexity.
Experimentally1, we show that for uncontaminated, realizable data, the LeastSquares algorithm per-
forms best, but in the presence of contamination or DAG misspeciﬁcation, CauchyEst/CauchyEstTree and
BatchAvgLeastSquares respectively perform better.
1
Introduction
Linear structural equation models (SEMs) are widely used to model interactions among multiple variables,
and they have found applications in diverse areas, including economics, genetics, sociology, psychology and
education; see [Mue99, Mul09] for pointers to the extensive literature in this area. Gaussian Bayesian networks
are a popularly used form of SEMs where: (i) there are no hidden/unobserved variables, (ii) each variable is a
linear combination of its parents plus a Gaussian noise term, and (iii) all noise terms are independent. The
structure of a Bayesian network refers to the directed acyclic graph (DAG) that prescribes the parent nodes for
each node in the graph.
This work studies the task of learning a Gaussian Bayesian network given its structure. The problem is
obviously quite fundamental and has been subject to extensive prior work. The usual formulation of this problem
is in terms of parameter estimation, where one wants a consistent estimator that exactly recovers the parameters
of the Bayesian network in the limit, as the the number of samples approaches inﬁnity. In contrast, we consider
the problem from the viewpoint of distribution learning [KMR+94]. Analogous to Valiant’s famous PAC model
for learning Boolean functions [Val84], the goal here is to learn, with high probability, a distribution bP that is
close to the ground-truth distribution P, using an eﬃcient algorithm. In this setting, pointwise convergence of
the parameters is no longer a requirement; the aim is rather to approximately learn the induced distribution.
Indeed, this relaxed objective may be achievable when the former may not be (e.g., for ill-conditioned systems)
and can be the more relevant requirement for downstream inference tasks. Diakonikolas [Dia16] surveys the
current state-of-the-art in distribution learning from an algorithmic perspective.
1Code is available at https://github.com/YohannaWANG/CauchyEst
1
arXiv:2107.10450v1  [cs.DS]  22 Jul 2021

Consider a Gaussian Bayesian network P with n variables with parameters in the form of coeﬃcients2 ai←j’s
and variances σi’s. For each i ∈[n], the linear structural equation for variable Xi, with parent indices πi ⊆[n],
is as follows:
Xi = ηi +
X
j∈πi
ai←jXj,
ηi ∼N(0, σ2
i )
for some unknown parameters ai←j’s and σi’s. If a variable Xi has no parents, then Xi ∼N(0, σ2
i ) is simply an
independent Gaussian. Our goal is to eﬃciently learn a distribution Q such that
dTV(P, Q) ≤ε
while minimizing the number of samples we draw from P as a function of n, ε and relevant structure parameters.
Here, dTV denotes the total variation or statistical distance between two distributions:
dTV(P, Q) = 1
2
Z
Rn |P(x) −Q(x)| dx.
One way to approach the problem is to simply view P as an n-dimensional multivariate Gaussian. In this
case, it is known that the Gaussian Q ∼N(0, bΣ) deﬁned by the empirical covariance matrix bΣ, computed with
O(n2/ε2) samples from P, is ε-close in TV distance to P with constant probability [ABDH+20, Appendix
C]. This sample complexity is also necessary for learning general n-dimensional Gaussians and hence general
Gaussian Bayesian networks on n variables.
We focus therefore on the setting where the structure of the network is sparse.
Assumption 1.1. Each variable in the Bayesian network has at most d parents. i.e. |πi| ≤d, ∀i ∈[n].
Sparsity is a common and very useful assumption for statistical learning problems; see the book [HTW19]
for an overview of the role of sparsity in regression. More speciﬁcally, in our context, the assumption of bounded
in-degree is popular (e.g., see [Das97, BCD20]) and also very natural, as it reﬂects the belief that in the correct
model, each linear structural equation involves a small number of variables3.
1.1
Our contributions
1. Analysis of MLE LeastSquares and a distributed-friendly generalization BatchAvgLeastSquares
The standard algorithm for parameter estimation of Gaussian Bayesian networks is to perform node-wise
least squares regression. It is easy to see that LeastSquares is the maximum likelihood estimator.
However, to the best of our knowledge, there did not exist an explicit sample complexity bound for
this estimator. We show that the sample complexity for learning P to within TV distance ε using
LeastSquares requires only e
O(nd/ε2) samples, which is nearly optimal.
We also give a generalization dubbed BatchAvgLeastSquares which solves multiple batches of least
squares problems (on smaller systems of equations), and then returns their mean. As each batch is
independent from the others, they can be solved separately before their solutions are combined. Our
analysis will later show that we can essentially interpolate between “batch size” and “number of batches”
while maintaining a similar total sample complexity – LeastSquares is the special case of a single batch.
Notably, we do not require any additional assumptions about the coeﬃcients or variance terms in the
analyses of LeastSquares and BatchAvgLeastSquares.
2. New algorithms based on Cauchy random variables: CauchyEst and CauchyEstTree
We develop a new algorithm CauchyEst.
At each node, CauchyEst solves several small linear sys-
tems of equations and takes the component-wise median of the obtained solution to obtain an estimate of
the coeﬃcients for the corresponding structural equation. In the special case of bounded-degree polytree
structures, where the underlying undirected Bayesian network is acyclic, we specialize the algorithm to
CauchyEstTree for which we give theoretical guarantees. Polytrees are of particular interest because infer-
ence on polytree-structured Bayesian networks can be performed eﬃciently [KP83, Pea86]. On polytrees,
2We write i ←j to emphasize that Xj is the parent of Xi in the Bayesian network.
3More generally, one would want to assume a bound on the “complexity” of each equation. In our context, as each equation is
linear, the complexity is simply proportional to the in-degree.
2

we show that the sample complexity of CauchyEstTree is also e
O(nd/ε2). Somewhat surprisingly, our
analysis (as the name of the algorithm reﬂects) involves Cauchy random variables which usually do not
arise in the context of regression.
3. Hardness results
In Section 5, we show that our sample complexity upper-bound is nearly optimal in terms of the
dependence on the parameters n, d, and ϵ. In particular, we show that learning the coeﬃcients and
noises of a linear structural equation model (on n variables, with in-degree at most d) up to an ϵ-error
in dTV-distance with probability 2/3 requires Ω(ndϵ−2) samples in general. We use a packing argument
based on Fano’s inequality to achieve this result.
4. Extensive experiments on synthetic and real-world networks
We experimentally compare the algorithms studied here as well as other well-known methods for undirected
Gaussian graphical model regression, and investigate how the distance between the true and learned
distributions changes with the number of samples. We ﬁnd that the LeastSquares estimator performs
the best among all algorithms on uncontaminated datasets. However, CauchyEst, CauchyEstTree and
BatchMedLeastSquares outperform LeastSquares and BatchAvgLeastSquares by a large margin when a
fraction of the samples are contaminated. In non-realizable/agnostic learning case, BatchAvgLeastSquares,
CauchyEst, and CauchyEstTree performs better than the other algorithms.
1.2
Outline of paper
In Section 2, we relate KL divergence with TV distance and explain how to decompose the KL divergence into
n terms so that it suﬃces for us to estimate the parameters for each variable independently. We also give an
overview of our two-phased recovery approach and explain how to use recovered coeﬃcients to estimate the
variances via VarianceRecovery. For estimating coeﬃcients, Section 3 covers the estimators based on linear
least squares (LeastSquares and BatchAvgLeastSquares) while Section 4 presents our new Cauchy-based
algorithms (CauchyEst and CauchyEstTree). To complement our algorithmic results, we provide hardness
results in Section 5 and experimental evaluation in Section 6.
For a cleaner exposition, we defer some formal proofs to Appendix B.
1.3
Further related work
Bayesian networks, both in discrete and continuous settings, were formally introduced by Pearl [Pea88] in 1988
to model uncertainty in AI systems. For the continuous case, Pearl considered the case when each node is a
linear function of its parents added with an independent Gaussian noise [Pea88, Chapter 7]. The parameter
learning problem – recovering the distribution of nodes conditioned on its parents from data – is well-studied in
practice, and maximum likelihood estimators are known for various simple settings such as when the conditional
distribution is Gaussian or the variables are discrete-valued. See for example the implementation of fit in the
R package bnlearn [Scu20].
The focus of our paper is to give formal guarantees for the parameter learning in the PAC framework
introduced by Valiant [Val84] in 1984. Subsequently, Haussler [Hau18] generalized this framework for studying
parameter and density estimation problems of continuous distributions. Dasgupta [Das97] ﬁrst looked at the
problem of parameter learning for ﬁxed structure Bayesian networks in the discrete and continuous settings and
gave ﬁnite sample complexity bounds for these problems based on the VC-dimensions of the hypothesis classes.
In particular, he gave an algorithm for learning the parameters of a Bayes net on n binary variables of bounded
in-degree in dKL distance using a quadratic in n samples. Subsequently, tight (linear) sample complexity upper
and lower bounds were shown for this problem [BGMV20, BGPV20, CDKS20]. To the best of our knowledge,
a ﬁnite PAC-style bound for ﬁxed-structure Gaussian Bayesian networks was not known previously.
The question of structure learning for Gaussian Bayesian networks has been extensively studied. A number of
works [PB14, GH17, CDW19, PK20, Par20, GDA20] have proposed increasingly general conditions for ensuring
identiﬁability of the network structure from observations. Structure learning algorithms that work for high-
dimensional Gaussian Bayesian networks have also been proposed by others (e.g., see [AZ15, AGZ19, GZ20]).
3

2
Preliminaries
In this section, we discuss why we upper bound the total variational distance using KL divergence and give a
decomposition of the KL divergence into n terms, one associated with each variable in the Bayesian network.
This decomposition motivates why our algorithms and analysis focus on recovering parameters for a single
variable. We also present our general two-phased recovery approach and explain how to estimate variances
using recovered coeﬃcients in Section 2.6.
2.1
Notation
A Bayesian network (Bayes net in short) P is a joint distribution P over n variables X1, . . . , Xn deﬁned by
the underlying directed acyclic graph (DAG) G. The DAG G = (V, E) encodes the dependence between the
variables where V = {X1, . . . , Xn} and (Xj, Xi) ∈E if and only if Xj is a parent of Xi. For any variable Xi,
we use the set πi ⊆[n] to represent the indices of Xi’s parents. Under this notation, each variable Xi of P is
independent of Xi’s non-descendants conditioned on πi. Therefore, using Bayes rule in the topological order of
G, we have
P(X1, . . . , Xn) =
n
Y
i=1
Pr
P (Xi | πi)
Without loss of generality, by renaming variables, we may assume that each variable Xi only has ancestors
with indices smaller than i. We also deﬁne pi = |πi| as the number of parents of Xi and davg = 1
n
Pn
i=1 pi to be
average in-degree. Furthermore, a DAG G is a polytree if the undirected version of G is a acyclic.
We study the realizable setting where our unknown probability distribution P is Markov with respect to the
given Bayesian network. We denote the true (hidden) parameters associated with P by α∗= (α∗
1, . . . , α∗
n). Our
algorithms recover parameter estimates bα = (bα1, . . . , bαn) such that the induced probability distribution Q is
close in total variational distance to P. For each i ∈[n], α∗
i = (Ai, σi) is the set of ground truth parameters
associated with variable Xi, Ai is the coeﬃcients associated to πi, σ2
i is the variance of ηi, bα∗
i = ( bAi, bσi) is our
estimate of α∗
i .
In the course of the paper, we will often focus on the perspective a single variable of interest. This allows
us to drop a subscript for a cleaner discussion. Let us denote such a variable of interest by Y ∈V and use
the index y ∈[n]. Without loss of generality, by renaming variables, we may further assume that the parents
of Y are X1, . . . , Xp. By Assumption 1.1, we know that p ≤d. We can write Y = ηy + Pp
i=1 ay←iXi where
ηy ∼N(0, σ2
y). We use matrix M ∈Rp×p to denote the covariance matrix deﬁned by the parents of Y , where
Mi,j = E [XiXj] and M = LL⊤is the Cholesky decomposition of M. Under this notation, we see the vector
(X1, . . . , Xp) ∼N(0, M) is distributed as a multivariate Gaussian. Our goal is then to produce estimates
bay←i for each ay←i. For notational convenience, we can group the coeﬃcients into A = [ay←1, . . . , ay←p]⊤and
bA = [bay←1, . . . , bay←p]⊤. The vector ∆= ( bA −A)⊤captures the entry-wise gap between our estimates and the
ground truth.
We write [n] to mean {1, 2, . . . , n} and |S| to mean the size of a set S. For a matrix M, Mi,j denotes its
(i, j)-th entry. We use ∥·∥to both mean the operator/spectral norm for matrices and L2-norm for vectors,
which should be clear from context. We hide absolute constant multiplicative factors and multiplicative factors
poly-logarithmic in the argument using standard notations: O(·), Ω(·), and e
O(·).
2.2
Basic facts and results
We begin by stating some standard facts and results.
Fact 2.1. Suppose X ∼N(0, σ2). Then, for any t > 0, Pr (X > t) ≤exp

−t2
2σ2

.
Fact 2.2. Consider any matrix B ∈Rn×m with rows B1, . . . , Bn ∈Rm. For any i ∈[m] and any vector v ∈Rm
with i.i.d. N(0, σ2) entries, we have that (Bv)i = Biv ∼N(0, σ2 · ∥Bi∥2).
Fact 2.3 (Theorem 2.2 in [Gut09])). Let X1, . . . , Xp ∼N(0, LL⊤) be p i.i.d. n-dimensional multivariate
Gaussians with covariance LL⊤∈Rn×n (i.e. L ∈Rn×p). If X ∈Rp×n is the matrix formed by stacking
X1, . . . , Xp as rows of X, then X = GL⊤where G ∈Rp×p is a random matrix with i.i.d. N(0, 1) entries.4
4The transformation stated in [Gut09, Theorem 2.2, page 120] is for a single multivariate Gaussian vector, thus we need to take
the transpose when we stack them in rows. Note that G and G⊤are identically distributed.
4

Lemma 2.4 (Equation 2.19 in [Wai19]). Let Y = Pn
k=1 Z2
k, where each Zk ∼N(0, 1). Then, Y ∼χ2
n and for
any 0 < t < 1, we have Pr (|Y/n −1| ≥t) ≤2 exp
 −nt2/8

.
Lemma 2.5 (Consequence of Corollary 3.11 in [BVH+16]). Let G ∈Rn×m be a matrix with i.i.d. N(0, 1)
entries where n ≤m. Then, for some universal constant C, Pr (∥G∥≥2(√n + √m)) ≤√n · exp (−C · m).
Lemma 2.6. Let G ∈Rk×d be a matrix with i.i.d. N(0, 1) entries. Then, for any constant 0 < c1 < 1/2 and
k ≥d/c2
1,
Pr
 
(G⊤G)−1
op ≤
1
(1 −2c1)2 k
!
≥1 −exp

−kc2
1
2

Proof. See Appendix B.
Lemma 2.7. Let G ∈Rk×p be a matrix with i.i.d. N(0, 1) entries and η ∈Rk be a vector with i.i.d. N(0, σ2)
entries, where G and η are independent. Then, for any constant c2 > 0,
Pr
G⊤η

2 < 2σc2
p
kp

≥1 −2p exp (−2k) −p exp

−c2
2
2

Proof. See Appendix B.
The next result gives the non-asymptotic convergence of medians of Cauchy random variables. We use this
result in the analysis of CauchyEst, and it may be of independent interest.
Lemma 2.8. [Non-asymptotic convergence of Cauchy median] Consider a collection of m i.i.d. Cauchy(0, 1)
random variables X1, . . . , Xm. Given a threshold 0 < τ < 1, we have
Pr (median {X1, . . . , Xm} ̸∈[−τ, τ]) ≤2 exp

−mτ 2
8

Proof. See Appendix B.
2.3
Distance and divergence between probability distributions
Recall that we are given sample access to an unknown probability distribution P over the values of (X1, . . . , Xn) ∈
Rn and the corresponding structure of a Bayesian network G on n variables. We denote α∗as the true (hidden)
parameters, inducing the probability distribution P, which we estimate by bα. In this work, we aim to recover
parameters bα such that our induced probability distribution Q is as close as possible to P in total variational
distance.
Deﬁnition 2.9 (Total variational (TV) distance). Given two probability distributions P and Q over Rn, the
total variational distance between them is deﬁned as dTV(P, Q) = supA∈Rn |P(A) −Q(A)| = 1
2
R
Rn |P −Q| dx.
Instead of directly dealing with total variational distance, we will instead bound the Kullback–Leibler (KL)
divergence and then appeal to the Pinsker’s inequality [Tsy08, Lemma 2.5, page 88] to upper bound dTV via
dKL. We will later show algorithms that achieve dKL(P, Q) ≤ε.
Deﬁnition 2.10 (Kullback–Leibler (KL) divergence). Given two probability distributions P and Q over Rn,
the KL divergence between them is deﬁned as dKL(P, Q) =
R
A∈Rn P(A) log

P(A)
Q(A)

dA.
Fact 2.11 (Pinsker’s inequality). For distributions P and Q, dTV(P, Q) ≤
p
dKL(P, Q)/2.
Thus, if s(ε) samples are needed to learn a distribution Q such that dKL(P, Q) ≤ε, s(ε2) samples are
needed to ensure dTV(P, Q) ≤ε.
5

2.4
Decomposing the KL divergence
For a set of parameters α = (α1, . . . , αn), denote αi as the subset of parameters that are relevant to the variable
Xi. Following the approach of [Das97]5, we decompose dKL(P, Q) into n terms that can be computed by
analyzing the quality of recovered parameters for each variable Xi.
For notational convenience, we write x to mean (x1, . . . , xn), πi(x) to mean the values given to parents of
variable Xi by x, and P(x) to mean P(X1 = x1, . . . , Xn = xn). Let us deﬁne
dCP(α∗
i , bαi) =
Z
xi,πi(x)
P(xi, πi(x)) log
P(xi | πi(x))
Q(xi | πi(x))

dxidπi(x)
where each bαi and α∗
i represent the parameters that relevant to variable Xi from bα and α∗respectively. By the
Bayesian network decomposition of joint probabilities and marginalization, one can show that
dKL(P, Q) =
n
X
i=1
dCP(α∗
i , bαi)
See Appendix A for the full derivation details.
2.5
Bounding dCP for an arbitrary variable
We now analyze dCP(α∗
i , bαi) with respect to the our estimates bαi = ( bAi, bσi) and the hidden true parameters
α∗
i = (Ai, σi) for any i ∈[n]. For derivation details, see Appendix A.
With respect to variable Xi, one can derive dCP(α∗
i , bαi) = ln
bσi
σi

+ σ2
i −bσ2
i
2bσ2
i
+ ∆⊤
i Mi∆i
2bσ2
i
. Thus,
dKL(P, Q) =
n
X
i=1
dCP(α∗
i , bαi) =
n
X
i=1
ln
bσi
σi

+ σ2
i −bσ2
i
2bσ2
i
+ ∆⊤
i Mi∆i
2bσ2
i
(1)
where Mi is the covariance matrix associated with variable Xi, α∗
i = (Ai, σi) is the coeﬃcients and variance
associated with variable Xi, αi = ( bAi, bσi) are the estimates for α∗
i , and ∆i = bAi −Ai.
Proposition 2.12 (Implication of KL decomposition). Let ε ≤0.17 be a constant. Suppose bαi has the following
properties for each i ∈[n]:
∆⊤
i Mi∆i
 ≤σ2
i ·
ε · pi
n · davg
(Condition 1)

1 −
r ε · pi
n · davg

· σ2
i ≤bσ2
i ≤

1 +
r ε · pi
n · davg

· σ2
i
(Condition 2)
Then, dCP(α∗
i , bαi) ≤3 ·
ε·pi
n·davg for all i ∈[n]. Thus, dKL(P, Q) = Pn
i=1 dCP(α∗
i , bαi) ≤3ε.6
Proof. Consider an arbitrary ﬁxed i ∈[n]. Denote γ = σ2
i
bσ2
i . Observe7 that γ −1 −ln(γ) ≤(γ −1)2 for γ ≥0.3.
Under Eq. (Condition 2), γ ≥0.3 always holds since 0 ≤ϵpi ≤ndavg. Then,
ln
bσi
σi

+ σ2
i −bσ2
i
2bσ2
i
= 1
2 ·

ln
σi
bσi

+ σ2
i
bσ2
i
−1

= 1
2 · (ln (γ) + γ −1)
≤1
2 · (γ −1)2
By Condition 2
≤1
2 ·


1
1 −
q
ϵpi
ndavg
−1


2
Since

1 −
r ϵpi
ndavg

· σ2
i ≤bσ2
i
≤2ϵpi
ndavg
Holds when 0 ≤
ϵpi
ndavg
≤1
4
5[Das97] analyzes the non-realizable setting where the distribution P may not correspond to the causal structure of the given
Bayesian network. As we study the realizable setting, we have a much simpler derivation.
6For a cleaner argument, we are bounding dKL(P, Q) ≤3ε. This is qualitatively the same as showing dKL(P, Q) ≤ε since one
can repeat the entire analysis with ε′ = ε/3.
7This inequality is also used in [ABDH+20, Lemma 2.9].
6

Meanwhile,
∆⊤
i Mi∆i
2bσ2
i
≤
∆⊤
i Mi∆i

2bσ2
i
≤
piϵ
ndavg
· σ2
i
2bσ2
i
By Condition 1
≤
piϵ
2ndavg
·
1
1 −
q
ϵpi
ndavg
Since

1 −
r ϵpi
ndavg

· σ2
i ≤bσ2
i
≤
ϵpi
ndavg
Holds when 0 ≤
ϵpi
ndavg
≤1
4
Putting together, we see that dCP (α∗
i , bαi) ≤
3ϵpi
ndavg .
2.6
Two-phased recovery approach
Algorithm 1 states our two-phased recovery approach. We estimate the coeﬃcients of the Bayesian network in
the ﬁrst phase and use them to recover the variances in the second phase.
Algorithm 1 Two-phased recovery algorithm
1: Input: DAG G and sample parameters m1 and m2
2: Draw m = m1 + m2 independent samples of (X1, . . . , Xn).
3: bA1, . . . , bAn ←Run a coeﬃcient recovery algorithm using ﬁrst m1 samples.
4: bσ2
1, . . . , bσ2
n ←Run VarianceRecovery using last m2 samples and bA1, . . . , bAn
5: return bA1, . . . , bAn, bσ2
1, . . . , bσ2
n
Motivated by Proposition 2.12, we will estimate parameters for each variable in an independent fashion8.
We will provide various coeﬃcient recovery algorithms in the subsequent sections. These algorithms will recover
coeﬃcients bAi that satisfy Condition 1 for each variable Xi. We evaluate them empirically in Section 6. For
variance recovery, we use VarianceRecovery for each variable Y by computing the empirical variance9 of
Y −X bA such that the recovered variance satisﬁes Condition 2.
Algorithm 2 VarianceRecovery: Variance recovery algorithm given coeﬃcient estimates
1: Input: DAG G, coeﬃcient estimates, and m2 ∈O

ndavg
ε
log
  n
δ

samples
2: for variable Y with p parents and coeﬃcient estimate bA do
▷If p = 0, then bA = 0.
3:
Without loss of generality, by renaming variables, let X1, . . . , Xp be the parents of Y .
4:
for s = 1, . . . , m2 do
5:
Deﬁne Y (s) as the sth sample of Y .
6:
Deﬁne X(s) = [X(s)
1 , . . . , X(s)
p ] as the sth sample of X1, . . . , Xp placed in a row vector.
7:
Deﬁne Z(s) =

Y (s) −X(s) bA
2
.
8:
end for
9:
Estimate bσ2
y =
1
m2
Pm2
i=1 Z(s)
10: end for
To analyze VarianceRecovery, we ﬁrst prove guarantees for an arbitrary variable and then take union
bound over n variables.
Lemma 2.13. Consider Algorithm 2. Fix any arbitrary variable of interest Y with p parents, parameters (A, σy),
and associated covariance matrix M. Suppose we have coeﬃcient estimates bA such that
∆⊤M∆
 ≤σ2
y ·
pε
ndavg .
Suppose 0 ≤ε ≤3 −2
√
2 ≤0.17. With k = 32ndavg
εp
log
  2
δ

samples, we recover variance estimate bσy such that
Pr

1 −
r
εp
ndavg

· σ2
y ≤bσ2
y ≤

1 +
r
εp
ndavg

· σ2
y

≥1 −δ
8Given the samples, parameters related to each variable can be estimated in parallel.
9Except in our experiments with contaminated data in Section 6 where we use the classical median absolute devation (MAD)
estimator. See Appendix C for a description.
7

Proof. We ﬁrst argue that bσ2
y ∼
 σ2
y + ∆⊤M∆

· χ2
k, then apply standard concentration bounds for χ2 random
variables (see Lemma 2.4).
For any sample s ∈[k], we see that Y (s) −X(s) bA = X(s)A + η(s)
y
−X(s) bA = η(s)
y
−X(s)∆, where
∆= bA −A ∈Rp is an unknown constant vector (because we do not actually know A). For ﬁxed ∆, we see
that X(s)∆∼N(0, ∆⊤M∆). Since η(s)
y
∼N(0, σ2
y) and X(s) are independent, we have that Y (s) −X(s) bA ∼
N(0, σ2
y + ∆⊤M∆). So, for any sample s ∈[k], Z(s) = (Y (s) −X(s) bA)2 ∼
 σ2
y + ∆⊤M∆

· χ2
1. Therefore,
bσy = 1
k
Pk
s=1 Z(s) ∼(σ2
y + ∆⊤M∆)/k · χ2
k. Let us deﬁne
γ = bσ2
y
σ2y
·


1
1 + ∆⊤M∆
σ2y

∼χ2
k
k
Since p ≤ndavg, if ε ≤3 −2
√
2, then
εp
ndavg ≤3 −2
√
2 ≤3 + 2
√
2. We ﬁrst make two observations:
1. For 0 ≤
εp
ndavg ≤3 −2
√
2,

1 +
q
εp
ndavg

·
 
1
1+ ∆⊤M∆
σ2y
!
≥1 +
q
εp
4ndavg .
2. For 0 ≤
εp
ndavg ≤3 + 2
√
2,

1 −
q
εp
ndavg

·
 
1
1+ ∆⊤M∆
σ2y
!
≤1 −
q
εp
4ndavg .
Using Lemma 2.4 with the above discussion, we have
Pr
 
bσ2
y
σ2y
≥1 +
r
εp
ndavg
or
bσ2
y
σ2y
≤1 −
r
εp
ndavg
!
= Pr

γ ≥

1 +
r
εp
ndavg

·


1
1 + ∆⊤M∆
σ2y


or
γ ≤

1 −
r
εp
ndavg

·


1
1 + ∆⊤M∆
σ2y




≤Pr

γ ≥1 +
r
εp
4ndavg
or
γ ≤1 −
r
εp
4ndavg

= Pr

|γ −1| ≥
r
εp
4ndavg

≤2 exp

−
kεp
32ndavg

The claim follows by setting k = 32ndavg
εp
log
  2
δ

.
Corollary 2.14 (Guarantees of VarianceRecovery). Consider Algorithm 2. Suppose 0 ≤ε ≤3 −2
√
2 ≤
0.17 and we have coeﬃcient estimates bAi such that
∆⊤
i Mi∆i
 ≤σ2
i ·
εpi
ndavg for all i ∈[n]. With m2 ∈
O

ndavg
ε
log
  n
δ

samples, we recover variance estimate bσi such that
Pr

∀i ∈[n],

1 −
r εpi
ndavg

· σ2
i ≤bσ2
i ≤

1 +
r εpi
ndavg

· σ2
i

≥1 −δ
The total running time is O
 n2d2
avg
ε
log
  1
δ

.
Proof. For each i ∈[n], apply Lemma 2.13 with δ′ = δ/n and m2 = 32ndavg
ε
log
  2
δ

≥maxi∈[n]
32ndavg
εpi
log
  2
δ

,
then take the union bound over all n variables.
The computational complexity for a variable with p parents is O(m2 · p). Since Pn
i=1 pi = ndavg, the total
runtime is O(m2 · n · davg).
In Section 5, we show that the sample complexity is nearly optimal in terms of the dependence on n and
ε. We remark that we use one batch of samples to use for all the nodes; this is possible as we can obtain
high-probability bounds on the error events at each node.
8

3
Coeﬃcient estimators based on linear least squares
In this section, we provide algorithms LeastSquares and BatchAvgLeastSquares for recovering the coeﬃcients
in a Bayesian network using linear least squares. As discussed in Section 2.6, we will recover the coeﬃcients for
each variable such that Condition 1 is satisﬁed. To do so, we estimate the coeﬃcients associated with each
individual variable using independent samples. At each node, LeastSquares computes an estimate by solving
the linear least squares problem with respect to a collection of sample observations. In Section 3.2, we generalize
this approach via BatchAvgLeastSquares by allowing any interpolation between “batch size” and “number
of batches” – LeastSquares is a special case of a single batch. Since each solution to batch can be computed
independently before their results are combined, BatchAvgLeastSquares facilitates parallelism.
3.1
Vanilla least squares
Consider an arbitrary variable Y with p parents. Using m1 independent samples, we form matrix X ∈Rm1×p,
where the rth row consists of sample values X(r)
1 , . . . , X(r)
p , and the column vector B = [Y (1), . . . , Y (m1)]⊤∈Rm1.
Then, we deﬁne bA = (X⊤X)−1X⊤B as the solution to the least squares problem X bA = B. The pseudocode of
LeastSquares is given in Algorithm 3 and Theorem 3.1 states its guarantees.
Algorithm 3 LeastSquares: Coeﬃcient recovery algorithm for general Bayesian networks
1: Input: DAG G and m1 ∈O

ndavg
ε
· ln
  n
δ

samples
2: for variable Y with p ≥1 parents do
3:
Without loss of generality, by renaming variables, let X1, . . . , Xp be the parents of Y .
4:
Form matrix X ∈Rm1×p, where the rth row consists of sample values X(r)
1 , . . . , X(r)
p
5:
Form column vector B = [Y (1), . . . , Y (m1)]⊤∈Rm1
6:
Deﬁne bA = (X⊤X)−1X⊤B as the solution to the least squares problem X bA = B
7: end for
Theorem 3.1 (Distribution learning using LeastSquares). Let ε, δ ∈(0, 1). Suppose G is a ﬁxed directed
acyclic graph on n variables with degree at most d. Given O

ndavg
ε2
log
  n
δ

samples from an unknown Bayesian
network P over G, if we use LeastSquares for coeﬃcient recovery in Algorithm 1, then with probability at least
1 −δ, we recover a Bayesian network Q over G such that dTV(P, Q) ≤ε in O
 n2d2
avgd
ε2
log
  1
δ

time.10
Our analysis begins by proving guarantees for an arbitrary variable.
Lemma 3.2. Consider Algorithm 3. Fix an arbitrary variable Y with p parents, parameters (A, σy), and
associated covariance matrix M. With k ≥
4c2
2
(1−c1)4 · ndavg
ε
samples, for any constants 0 < c1 < 1/2 and c2 > 0,
we recover the coeﬃcients bA such that
Pr
∆⊤M∆
 ≥σ2
y ·
pε
ndavg

≤exp

−kc2
1
2

+ 2p exp (−2k) + p exp

−c2
2
2

Proof. Since
∆⊤M∆
 =
∆⊤LL⊤∆
 =
L⊤∆
2, it suﬃces to bound
L⊤∆
.
Without loss of generality, the parents of Y are X1, . . . , Xp. Deﬁne X ∈Rk×p, B ∈Rk, and bA ∈Rp as
in Algorithm 3. Let η = [η(1)
y , . . . , η(k)
y ] ∈Rk be the instantiations of Gaussian ηy in the k samples. By the
structural equations, we know that B = XA + η. So,
eA = (X⊤X)−1X⊤B = (X⊤X)−1X⊤(XA + η) = A + (X⊤X)−1X⊤η
By Fact 2.3, we can express X = GL⊤where matrix G ∈Rk×p is a random matrix with i.i.d. N(0, 1) entries.
Since ∆= bA −A, we see that ∆= (L⊤)−1(G⊤G)−1G⊤η. Rearranging, we have L⊤∆= (G⊤G)−1G⊤η and so
10In particular, this gives a dKL(P, Q) ≤ϵ guarantee for learning centered multivariate Gaussians using e
O(n2ϵ−1) samples. See
e.g. [ABDH+20] for an analogous dKL(Q, P) ≤ϵ guarantee.
9

∥L⊤∆∥≤∥(G⊤G)−1∥· ∥G⊤η∥. Combining Lemma 2.6 and Lemma 2.7, which bound ∥(G⊤G)−1∥and ∥G⊤η∥
respectively, we get
Pr
 
∥L⊤∆∥>
2σyc2√p
(1 −2c1)2 √
k
!
≤exp

−kc2
1
2

+ 2p exp (−2k) + p exp

−c2
2
2

(2)
for any constants 0 < c1 < 1/2 and c2 > 0. The claim follows by setting k =
4c2
2
(1−c1)4 · ndavg
ε
.
We can now establish Condition 1 of Proposition 2.12 for LeastSquares.
Lemma 3.3. Consider Algorithm 3.
With m1 ∈O

ndavg
ε
· ln
  n
δ

samples, we recover the coeﬃcients
bA1, . . . , bAn such that
Pr

∀i ∈[n],
∆⊤
i Mi∆i
 ≥σ2
i ·
εpi
ndavg

≤δ
The total running time is O
 n2d2
avgd
ε
ln
  1
δ

.
Proof. By setting c1 = 1/4, c2 =
p
2 ln (3n/δ), and k = 32ndavg
ε
ln
  3n
δ

≥
4c2
2
(1−c1)4 · ndavg
ε
in Lemma 3.2, we have
Pr
∆⊤
i Mi∆i
 ≥σ2
i ·
piε
ndavg

≤exp

−kc2
1
2

+ p exp (−2k) + p exp

−c2
2
2

≤δ
3n + δ
3n + δ
3n = δ
n
for any i ∈[n]. The claim holds by a union bound over all n variables.
The computational complexity for a variable with p parents is O(p2 · m1). Since maxi∈[n] pi ≤d and
Pn
i=1 pi = ndavg, the total runtime is O(m1 · n · davg · d).
Theorem 3.1 follows from combining the guarantees of LeastSquares and VarianceRecovery (given in
Lemma 3.3 and Corollary 2.14 respectively) via Proposition 2.12.
Proof of Theorem 3.1. We will show sample and time complexities before giving the proof for the dTV distance.
Let m1 ∈O

ndavg
ε
· ln
  n
δ

and m2 ∈O

ndavg
ε
log
  n
δ

. Then, the total number of samples needed is
m = m1 + m2 ∈O

ndavg
ε
log
  n
δ

. LeastSquares runs in O
 n2d2
avgd
ε
ln
  1
δ

time while VarianceRecovery
runs in O
 n2d2
avg
ε
log
  1
δ

time. Therefore, the overall running time is O
 n2d2
avgd
ε
log
  1
δ

.
By Lemma 3.3, LeastSquares recovers coeﬃcients bA1, . . . , bAn such that
Pr

∀i ∈[n],
∆⊤
i Mi∆i
 ≥σ2
i ·
εpi
ndavg

≤δ
By Corollary 2.14 and using the recovered coeﬃcients from LeastSquares, VarianceRecovery recovers
variance estimates bσ2
i such that
Pr

∀i ∈[n],

1 −
r εpi
ndavg

· σ2
i ≤bσ2
i ≤

1 +
r εpi
ndavg

· σ2
i

≥1 −δ
As our estimated parameters satisfy Condition 1 and Condition 2, Proposition 2.12 tells us that dKL(P, Q) ≤
3ε. Thus, dTV(P, Q) ≤
p
dKL(P, Q)/2 ≤
p
3ε/2. The claim follows by setting ε′ =
p
3ε/2 throughout.
3.2
Interpolating between batch size and number of batches
We now discuss a generalization of LeastSquares.
In a nutshell, for each variable with p ≥1 parents,
BatchAvgLeastSquares solves b ≥1 batches of linear systems made up of k > p samples and then uses the
mean of the recovered solutions as an estimate for the coeﬃcients. Note that one can interpolate between diﬀerent
values of k and b, as long as k ≥p (so that the batch solutions are correlated to the true parameters). The
pseudocode of BatchAvgLeastSquares is provided in Algorithm 4 and the guarantees are given in Theorem 3.1.
In Section 6, we also experimented on a variant of BatchAvgLeastSquares dubbed BatchMedLeastSquares,
where bA is deﬁned to be the coordinate-wise median of the eA(s) vectors. However, in the theoretical analysis
below, we only analyze BatchAvgLeastSquares.
10

Algorithm 4 BatchAvgLeastSquares: Coeﬃcient recovery for general Bayesian networks
1: Input: DAG G and m1 = O

ndavg
ε
 d + ln
  n
εδ

samples
▷k ∈Ω
 d + ln
  n
εδ

, b = m1
k
2: for variable Y with p ≥1 parents do
3:
Without loss of generality, by renaming variables, let X1, . . . , Xp be the parents of Y .
4:
for s = 1, . . . , b do
5:
Form matrix X ∈Rk×p, where the rth row consists of sample values X(s,r)
1
, . . . , X(s,r)
p
6:
Form column vector B = [Y (s,1), . . . , Y (s,k)]⊤∈Rk
7:
Deﬁne eA(s) = (X⊤X)−1X⊤B as the solution to the least squares problem X eA(s) = B
8:
end for
9:
Deﬁne bA = 1
b
Pb
s=1 eA(s)
10: end for
Theorem 3.4 (Distribution learning using BatchAvgLeastSquares). Let ε, δ ∈(0, 1). Suppose G is a ﬁxed
directed acyclic graph on n variables with degree at most d. Given O

ndavg
ε2
 d + ln
  n
εδ

samples from an
unknown Bayesian network P over G, if we use BatchAvgLeastSquares for coeﬃcient recovery in Algorithm 1,
then with probability at least 1 −δ, we recover a Bayesian network Q over G such that dTV(P, Q) ≤ε in
O
 n2d2
avgd
ε2
 d + ln
  n
εδ

time.
Our approach for analyzing BatchAvgLeastSquares is the same as our approach for LeastSquares: we
prove guarantees for an arbitrary variable and then take union bound over n variables. At a high-level, for
each node Y , for every ﬁxing of the randomness in generating X1, . . . , Xp, we show that each eA(s) is a gaussian.
Since the b iterations are independent, 1
b
P
s eA(s) is also a gaussian. Its variance is itself a random variable but
can be bounded with high probability using concentration inequalities.
Lemma 3.5. Consider Algorithm 4. Fix any arbitrary variable of interest Y with p parents, parameters (A, σy),
and associated covariance matrix M. With k > Ck ·
 p + ln
  n
εδ

and kb = Ckb ·

ndavg
ε
 d + ln
  n
εδ

, for some
universal constants Ck and Ckb, we recover coeﬃcients estimates bA such that
Pr
∆⊤M∆
 ≤σ2
y ·
εp
ndavg

≥1 −δ
Proof. Without loss of generality, the parents of Y are X1, . . . , Xp. For s ∈[n], deﬁne X(s) ∈Rk×p, B(s) ∈Rk,
and eA(s) ∈Rp as the quantities involved in the sth batch of Algorithm 4. Let η(s) = [η(s,1)
y
, . . . , η(s,k)
y
] ∈Rk be
the instantiations of Gaussian ηy in the k samples for the sth batch. By the structural equations, we know that
B(s) = X(s)A + η(s). So,
eA(s) = ((X(s))⊤X(s))−1(X(s))⊤B
= ((X(s))⊤X(s))−1(X(s))⊤(X(s)A + η(s))
= A + ((X(s))⊤X(s))−1(X(s))⊤η(s)
By Fact 2.3, we can express X(s) = G(s)L⊤where matrix G(s) ∈Rk×p is a random matrix with i.i.d. N(0, 1)
entries. So, we see that
L⊤∆= 1
b
b
X
s=1
((G(s))⊤G(s))−1(G(s))⊤η(s)
For any i ∈[p], Fact 2.2 tells us that
 L⊤∆

i = 1
b
b
X
s=1

((G(s))⊤G(s))−1(G(s))⊤η(s)
i ∼N
 
0, σ2
y
b2
b
X
s=1


((G(s))⊤G(s))−1(G(s))⊤
i

2
!
We can upper bound each
 ((G(s))⊤G(s))−1(G(s))⊤
i
 term as follows:


((G(s))⊤G(s))−1(G(s))⊤
i
 ≤
((G(s))⊤G(s))−1(G(s))⊤ ≤
((G(s))⊤G(s))−1 ·
G(s)
11

When k ≥4p, Lemma 2.6 tells us that Pr
 ((G(s))⊤G(s))−1 ≥4
k

≤exp
 −k
32

. Meanwhile, Lemma 2.5
tells us that Pr
G(s) ≥2(
√
k + √p)

≤√p · exp (−C · k) for some universal constant C. Let E be the event
that
 ((G(s))⊤G(s))−1(G(s))⊤
i
 < 8(
√
k+√p)
k
for any s ∈[b]. Applying union bound with the conclusions from
Lemma 2.6 and Lemma 2.5, we have
Pr
 E

= Pr
 
∃s ∈[b],


((G(s))⊤G(s))−1(G(s))⊤
i
 ≥8(
√
k + √p)
k
!
≤b · exp

−k
32

+ b · √p · exp (−C · k)
Conditioned on event E, standard Gaussian tail bounds (e.g. see Fact 2.1) give us
Pr
L⊤∆

i > σy ·
r
ε
ndavg
 E

≤exp
 
−
σ2
y ·
ε
ndavg
2 ·
σ2y
b2
Pb
s=1
 ((G(s))⊤G(s))−1(G(s))⊤
i
2
!
≤exp
 
−
ε · b · k2
128 · n · davg · (
√
k + √p)2
!
≤exp

−
ε · b · k
512 · n · davg

where the second last inequality is because of event E and the last inequality is because (
√
k+√p)2 ≤(2
√
k)2 = 4k,
since k ≥p.
Thus, applying a union bound over all p entries of L⊤∆and accounting for Pr(E), we have
Pr
L⊤∆
 > σy ·
r
εp
ndavg

≤Pr
L⊤∆
 > σy ·
r
εp
ndavg
 E

+ Pr
 E

≤p · exp

−
ε · b · k
512 · n · davg

+ b · exp

−k
32

+ b · √p · exp (−C · k)
for some universal constant C.
The claim follows by observing that
∆⊤M∆
 =
∆⊤LL⊤∆
 =
L⊤∆
2 and applying the assumptions on k
and b.
We can now establish Condition 1 of Proposition 2.12 for BatchAvgLeastSquares.
Lemma 3.6 (Coeﬃcient recovery guarantees of BatchAvgLeastSquares). Consider Algorithm 4. With m1 ∈
O

ndavg
ε
 d + ln
  n
εδ

samples, where k ∈Ω
 d + ln
  n
εδ

and b =
m1
k , we recover coeﬃcient estimates
bA1, . . . , bAn such that
Pr

∀i ∈[n],
∆⊤
i Mi∆i
 ≥σ2
i ·
εpi
ndavg

≤δ
The total running time is O(m1 · n · davg · d).
Proof. For each i ∈[n], apply Lemma 3.5 with δ′ = δ/n, then take the union bound over all n variables.
The computational complexity for a variable with p parents is O(b·p2 ·k) = O(p2 ·m1). Since maxi∈[n] pi ≤d
and Pn
i=1 pi = ndavg, the total runtime is O(m1 · n · davg · d).
Theorem 3.4 follows from combining the guarantees of BatchAvgLeastSquares and VarianceRecovery
(given in Lemma 3.6 and Corollary 2.14 respectively) via Proposition 2.12.
Proof of Theorem 3.4. We will show sample and time complexities before giving the proof for the dTV distance.
Let m1 ∈O

ndavg
ε
 d + ln
  n
εδ

and m2 ∈O

ndavg
ε
log
  n
δ

.
Then, the total number of samples
needed is m = m1 + m2 ∈O

ndavg
ε
 d + ln
  n
εδ

. BatchAvgLeastSquares runs in O(m1ndavgd) time while
VarianceRecovery runs in O
 n2d2
avg
ε
log
  1
δ

time. Therefore, the overall running time is O
 n2d2
avgd
ε
 d + ln
  n
εδ

.
12

By Lemma 3.6, BatchAvgLeastSquares recovers coeﬃcients bA1, . . . , bAn such that
Pr

∀i ∈[n],
∆⊤
i Mi∆i
 ≥σ2
i ·
εpi
ndavg

≤δ
By Corollary 2.14 and using the recovered coeﬃcients from BatchAvgLeastSquares, VarianceRecovery
recovers variance estimates bσ2
i such that
Pr

∀i ∈[n],

1 −
r εpi
ndavg

· σ2
i ≤bσ2
i ≤

1 +
r εpi
ndavg

· σ2
i

≥1 −δ
As our estimated parameters satisfy Condition 1 and Condition 2, Proposition 2.12 tells us that dKL(P, Q) ≤
3ε. Thus, dTV(P, Q) ≤
p
dKL(P, Q)/2 ≤
p
3ε/2. The claim follows by setting ε′ =
p
3ε/2 throughout.
4
Coeﬃcient recovery algorithm based on Cauchy random variables
In this section, we provide novel algorithms CauchyEst and CauchyEstTree for recovering the coeﬃcients in
polytree Bayesian networks. We will show that CauchyEstTree has near-optimal sample complexity, and later
in Section 6, we will see that both these algorithms outperform LeastSquares and BatchAvgLeastSquares on
randomly generated Bayesian networks. Of technical interest, our analysis involves Cauchy random variables,
which are somewhat of a rarity in statistical learning. As in LeastSquares and BatchAvgLeastSquares,
CauchyEst and CauchyEstTree use independent samples to recover the coeﬃcients associated to each individual
variable in an independent fashion.
Consider an arbitrary variable Y with p parents. The intuition is as follows: if ηy = 0, then one can
form a linear system of equations using p samples to solve for the coeﬃcients ay←i exactly for each i ∈π(y).
Unfortunately, ηy is non-zero in general. Instead of exactly recovering A, we partition the m1 independent
samples into k = ⌊m1/p⌋batches involving p samples and form intermediate estimates eA(1), . . . , eA(k) by solving
a system of linear equation for each batch (see Algorithm 5). Then, we “combine” these intermediate estimates
to obtain our estimate bA.
Algorithm 5 Batch coeﬃcient recovery algorithm for variable with p parents
1: Input: DAG G, a variable Y with p parents, and p samples
2: Without loss of generality, by renaming variables, let X1, . . . , Xp be the parents of Y .
3: Form matrix X ∈Rp×p where the rth row equals [X(r)
1 , . . . , X(r)
p ], corresponding to Y (r).
4: Deﬁne eA = [bay←1, . . . , bay←p]⊤as any solution to X eA =

Y (1), . . . , Y (p)⊤.
5: return eA
Consider an arbitrary copy of recovered coeﬃcients eA. Let ∆= [∆1, . . . , ∆p]⊤= eA−A be a vector measuring
the gap between these recovered coeﬃcients and the ground truth, where ∆i = eay←i −ay←i. Lemma 4.1 gives a
condition where a vector is term-wise Cauchy. Using this, Lemma 4.2 shows that each entry of the vector L⊤∆
is distributed according to σy · Cauchy(0, 1), although the entries may be correlated with each other in general.
Lemma 4.1. Consider the matrix equation AB = E where A ∈Rn×n, B ∈Rn×1, and E ∈Rn×1 such that
entries of A and E are independent Gaussians, elements in each column of A have the same variance, and all
entries in E have the same variance. That is, A·,j ∼N(0, σ2
i ) and Ei ∼N(0, σ2
n+1). Then, for all i ∈[n], we
have that Bi ∼σn+1
σi
· Cauchy(0, 1).
Proof. As the event that A is singular has measure zero, we can write B = A−1E. By Cramer’s rule,
A−1 =
1
det(A) · adj(A) =
1
det(A) · C⊤
where det(A) is the determinant of A, adj(A) is the adjugate/adjoint matrix of A, and C is the cofactor matrix
of A. Recall that the det(A) can deﬁned with respect to elements in C: For any column i ∈[n],
det(A) = A1,i · C1,i + A2,i · C2,i + . . . + An,i · Cn,i
13

So, det(A) ∼N
 0, σ2
i (C1,i + . . . + Cn,i)

. Thus, for any i ∈[n],
Bi =

1
det(A)C⊤E

i
∼N
 0, σ2
n+1 (C1,i + . . . + Cn,i)

N (0, σ2
i (C1,i + . . . + Cn,i))
= σn+1
σi
· Cauchy(0, 1)
Lemma 4.2. Consider a batch estimate eA from Algorithm 5. Then, L⊤∆is entry-wise distributed as σy ·
Cauchy(0, 1), where ∆= eA −A. Note that the entries of L⊤∆may be correlated in general.
Proof. Observe that each row of X is an independent sample drawn from a multivariate Gaussian N(0, M).
By denoting η =
h
η(1)
y , . . . , η(p)
y
i⊤
as the p samples of ηy, we can write X eA = XA + η and thus X∆= η by
rearranging terms. By Fact 2.3, we can express X = GL⊤where matrix G ∈Rp×p is a random matrix with
i.i.d. N(0, 1) entries. By substituting X = GL⊤into X∆= η, we have L⊤∆= G−1η.11
By applying Lemma 4.1 with the following parameters: A = G, B = L⊤∆, E = η, we conclude that each
entry of L⊤∆is distributed as σy · Cauchy(0, 1). However, note that these entries are generally correlated.
If we have direct access to the matrix L, then one can do the following (see Algorithm 6): for each coordinate
i ∈[p], take medians12 of

L⊤[eay←1, . . . , eay←n]⊤
i to form MEDi and then estimate [bay←1, . . . , bay←1] =
(L⊤)−1[MED1, . . . , MEDn]⊤. By the convergence of Cauchy random variables to their median, one can show that
each bay←i converges to the true coeﬃcient ay←i as before. Unfortunately, we do not have L and can only hope
to estimate it with some matrix bL using the empirical covariance matrix c
M.
Algorithm 6 CauchyEst: Coeﬃcient recovery algorithm for general Bayesian networks
1: Input: DAG G and m samples
2: for variable Y with p ≥1 parents do
▷By Assumption 1.1, p ≤d
3:
Without loss of generality, by renaming variables, let X1, . . . , Xp be the parents of Y .
4:
Let c
M be the empirical covariance matrix with respect to X1, . . . , Xp.
5:
Compute the Cholesky decomposition c
M = bLbL⊤of c
M.
6:
for s = 1, . . . , ⌊m/p⌋do
7:
Using p samples and Algorithm 5, compute a batch estimate eA(s).
8:
end for
9:
For each i ∈[n], deﬁne MEDi = median{(bL⊤eA(1))i, . . . , (bL⊤eA(⌊m/p⌋))i}.
10:
return bA = [bay←1, . . . , bay←n]⊤= ((bL⊤)−1 [MED1, . . . , MEDn]⊤)⊤.
11: end for
4.1
Special case of polytree Bayesian networks
If the Bayesian network is a polytree, then L is diagonal. In this case, we specialize CauchyEst to CauchyEstTree
and are able to give theoretical guarantees. We begin with simple corollary which tells us that the ith entry of
∆is distributed according to σy/σi · Cauchy(0, 1).
Corollary 4.3. Consider a batch estimate eA from Algorithm 5. If the Bayesian network is a polytree, then
∆i = ( eA −A)i ∼σy
σi · Cauchy(0, 1).
Proof. Observe that each row of X is an independent sample drawn from a multivariate Gaussian N(0, M).
By denoting η =
h
η(1)
y , . . . , η(p)
y
i⊤
as the p samples of ηy, we can write X eA = XA + η and thus X∆= η by
rearranging terms. Since the parents of any variable in a polytree are not correlated, each element in the ith
column of X is a N(0, σ2
i ) Gaussian random variable.
By applying Lemma 4.1 with the following parameters: A = X, B = ∆E = η, we conclude that ∆i =
( eA −A)i ∼σy
σi · Cauchy(0, 1).
11Note that event that G is singular has measure 0.
12The typical strategy of averaging independent estimates does not work here as the variance of a Cauchy variable is unbounded.
14

For each i ∈π(y), we combine the k independently copies of ea(1)
y←i, . . . , ea(k)
y←i using the median. For arbitrary
sample s ∈[k] and parent index i ∈π(y), observe that ∆(s)
i
= ea(s)
y←i −ay←i. Since ay←i is just an unknown
constant,
bay←i = medians∈[k]
n
ea(s)
y←i
o
= medians∈[k]
n
∆(s)
i
o
+ ay←i
Since each ∆(s)
i
term is i.i.d. distributed as σy · Cauchy(0, 1), the term medians∈[k]
n
∆(s)
i
o
converges to 0 with
suﬃciently large k, and thus bay←i converges to the true coeﬃcient ay←i.
The goal of this section is to prove Theorem 4.4 given CauchyEstTree, whose pseudocode we provide in
Algorithm 7.
Algorithm 7 CauchyEstTree: Coeﬃcient recovery algorithm for polytree Bayesian networks
1: Input: A polytree G and m1 ∈O

ndavgd
ε
log
  n
δ

samples
2: for variable Y with p ≥1 parents do
▷By Assumption 1.1, p ≤d
3:
Without loss of generality, by renaming variables, let X1, . . . , Xp be the parents of Y .
4:
for s = 1, . . . , ⌊m1/p⌋do
5:
Using p samples and Algorithm 5, compute a batch estimate eA(s).
6:
end for
7:
For each i ∈π(y), deﬁne estimate bay←i = median
n
ea(1)
y←i, . . . , ea(⌊m1/p⌋)
y←i
o
.
8:
return bA = [bay←1, . . . , bay←p]⊤
9: end for
Theorem 4.4 (Distribution learning using CauchyEstTree). Let ε, δ ∈(0, 1). Suppose G is a ﬁxed directed
acyclic graph on n variables with degree at most d. Given O

ndavgd
ε
log
  n
εδ

samples from an unknown Bayesian
network P over G, if we use CauchyEstTree for coeﬃcient recovery in Algorithm 1, then with probability at
least 1 −δ, we recover a Bayesian network Q over G such that dTV(P, Q) ≤ε in O
 n2d2
avgdω−1
ε
log
  n
δ

time.
Note that for polytrees, davg is just a constant. As before, we will ﬁrst prove guarantees for an arbitrary
variable and then take union bound over n variables.
Lemma 4.5. Consider Algorithm 7. Fix an arbitrary variable of interest Y with p parents, parameters (A, σy),
and associated covariance matrix M. With k = 8ndavg
ε
log
  2
δ

samples, we recover coeﬃcient estimates bA such
that
Pr
∆⊤M∆
 ≤σ2
y ·
εp
ndavg

≥1 −δ
Proof. Since M = LL⊤, it suﬃces to bound ∥L⊤∆∥. Lemma 4.2 tells us that each entry of the vector L⊤∆
is the median of k copies of Cauchy(0, 1) random variables multiplied by σy. Setting k = 8ndavg
ε
log
  2
δ

and
0 < τ =
p
ε/(ndavg) < 1 in Lemma 2.8, we see that
Pr

median of k i.i.d. Cauchy(0, 1) random variables ̸∈

−
r
ε
ndavg
,
r
ε
ndavg

≤δ
That is, each entry of L⊤∆has absolute value at most σy ·
q
ε
ndavg . By summing across all p entries of L⊤∆,
we see that
|∆⊤M∆| = |∆⊤LL⊤∆| = ∥L⊤∆∥2 ≤p · σ2
y ·
ε
ndavg
= σ2
y ·
εp
ndavg
We can now establish Condition 1 of Proposition 2.12 for CauchyEstTree.
Lemma 4.6. Consider Algorithm 7. Suppose the Bayesian network is a polytree. With m1 ∈O

ndavgd
ε
log
  n
δ

samples, we recover coeﬃcient estimates bA1, . . . , bAn such that
Pr

∀i ∈[n],
∆⊤
i Mi∆i
 ≥σ2
i ·
εpi
ndavg

≤δ
15

The total running time is O
 n2d2
avgdω−1
ε
log
  n
δ

where ω is the matrix multiplication exponent.
Proof. For each i ∈[n], apply Lemma 4.5 with δ′ = δ/n and m1 = 8ndavg
ε
log
  2n
δ

, then take the union bound
over all n variables.
The runtime of Algorithm 5 is the time to ﬁnd the inverse of a p × p matrix, which is O(pω) for some
2 < ω < 3. Therefore, the computational complexity for a variable with p parents is O(pω−1 · m1). Since
maxi∈[n] pi ≤d and Pn
i=1 pi = ndavg, the total runtime is O(m1 · n · davg · dω−2).
We are now ready to prove Theorem 4.4.
Theorem 4.4 follows from combining the guarantees of CauchyEstTree and VarianceRecovery (given in
Lemma 4.6 and Corollary 2.14 respectively) via Proposition 2.12.
Proof of Theorem 4.4. We will show sample and time complexities before giving the proof for the dTV distance.
Let m1 ∈O

ndavgd
ε
log
  n
δ

and m2 ∈O

ndavg
ε
log
  n
δ

. Then, the total number of samples needed is m =
m1 + m2 ∈O

ndavgd
ε
log
  n
εδ

. CauchyEstTree runs in O
 n2d2
avgdω−1
ε
log
  n
δ

time while VarianceRecovery
runs in O
 n2d2
avg
ε
log
  1
δ

time, where ω is the matrix multiplication exponent. Therefore, the overall running
time is O
 n2d2
avgdω−1
ε
log
  n
δ

.
By Lemma 4.6, CauchyEstTree recovers coeﬃcients bA1, . . . , bAn such that
Pr

∀i ∈[n],
∆⊤
i Mi∆i
 ≥σ2
i ·
εpi
ndavg

≤δ
By Corollary 2.14 and using the recovered coeﬃcients from CauchyEstTree, VarianceRecovery recovers
variance estimates bσ2
i such that
Pr

∀i ∈[n],

1 −
r εpi
ndavg

· σ2
i ≤bσ2
i ≤

1 +
r εpi
ndavg

· σ2
i

≥1 −δ
As our estimated parameters satisfy Condition 1 and Condition 2, Proposition 2.12 tells us that dKL(P, Q) ≤
3ε. Thus, dTV(P, Q) ≤
p
dKL(P, Q)/2 ≤
p
3ε/2. The claim follows by setting ε′ =
p
3ε/2 throughout.
5
Hardness for learning Gaussian Bayesian networks
In this section, we present our hardness results. We ﬁrst show a tight lower-bound for the simpler case of
learning Gaussian product distributions in total variational distance (Theorem 5.2). Next, we show a tight
lower-bound for learning Gaussian Bayes nets with respect to total variation distance. (Theorem 5.3). In both
cases, our hardness applies to the problems of learning the covariance matrix of a centered multivariate Gaussian,
which is equivalent to recovering the coeﬃcients and noises of the underlying linear structural equation.
We will need the following fact about the variation distance between multivariate Gaussians and a Frobenius
norm ∥·∥F between the covariance matrices.
Fact 5.1 ([DMR18]). There exists two universal constants
1
100 ≤c1 ≤c2 ≤3
2, such that for any two covariance
matrices Σ1 and Σ2,
c1 ≤dTV(N(0, Σ1), N(0, Σ2))
Σ−1
1 Σ2 −I

F
≤c2.
Theorem 5.2. Given samples from a n-fold Gaussian product distribution P, learning a bP such that in
dTV(P, bP) = O(ϵ) with success probability 2/3 needs Ω(nϵ−2) samples in general.
Proof. Let C ⊆{0, 1}n be a set with the following properties: 1) |C| = 2Θ(n) and 2) every i ̸= j ∈C have
a Hamming distance Θ(n). Existence of such a set is well-known. We create a class of Gaussian product
distributions PC based on C as follows. For each s ∈C, we deﬁne a distribution Ps ∈PC such that if the i-th
bit of s is 0, we use the distribution N(0, 1) for the i-th component of Ps; else if the i-th bit is 1, we use the
distribution N(0, 1 +
ϵ
√n). Then for any Ps ̸= Pt, dKL(Ps, Pt) = O(ϵ2). Fano’s inequality tells us that guessing
a random distribution from PC correctly with 2/3 probability needs Ω(nϵ−2) samples.
16

. . .
. . .
. . .
. . .
a1→j
ai→j
ad→j
X1
Xi
Xd
Xd+1
Xj
Xn
Figure 1: Bipartite DAG on n vertices with maximum degree d. For i ∈{1, . . . , d}, Xi = ηi where ηi ∼N(0, 1).
For j ∈{d + 1, . . . , n}, Xj = ηj + Pd
i=1 ai→jXi where ηj ∼N(0, 1). Furthermore, each Xj is associated with a
d-bit string and each coeﬃcients a1→j, . . . , ad→j is either
1
√
d(n−d) or
1+ϵ
√
d(n−d), depending on the ith bit in the
associated d-bit string.
Fact 5.1 tells us that for any Ps ̸= Pt ∈PC, dTV(Ps, Pt) ≥c3ϵ for some constant c3. Consider any algorithm
for learning a random distribution P = N(0, Σ) from PC in dTV-distance at most c4ϵ for a suﬃciently small
constant c4. Let the learnt distibution be bP = N(0, bΣ). Due to triangle inequality, Fact 5.1, and an appropriate
choice of c4, P must be the unique distribution from PC satisfying ||bΣ−1Σ −I||F ≤c5ϵ for an appropriate choice
of c5. We can ﬁnd this unique distribution by computing ||bΣ−1Σ′ −I||F for every covariance matrix Σ′ from
PC and guess the random distribution correctly. Hence, the lower-bound follows.
Now, we present the lower-bound for learning general Bayes nets.
Theorem 5.3. For any 0 < ϵ < 1 and n, d such that d ≤n/2, there exists a DAG G over [n] of in-degree d
such that learning a Gaussian Bayes net bP on G such that dTV(P, bP) ≤ϵ with success probability 2/3 needs
Ω(ndϵ−2) samples in general.
Let C ⊆{0, 1}d be a set with the following properties: 1) |C| = 2Θ(d) and 2) every i ̸= j ∈C have a
Hamming distance Θ(d). Existence of such a set is well-known. We deﬁne a class of distributions PC based on
C and the graph G shown in Fig. 1 as follows. Each vertex of each distribution in PC has a N(0, 1) noise, and
hence no learning is required for the noises. Each coeﬃcient ai→j takes one of two values

1
√
d(n−d),
1+ϵ
√
d(n−d)

corresponding to bits {0, 1} respectively.
For each s ∈C, we deﬁne As to be the vector of coeﬃcients
corresponding to the bit-pattern of s as above. We have 2Θ(d) possible bit-patterns, which we use to deﬁne
each conditional probability (Xi | X1, X2, . . . , Xd). Then, we have a class QP of |C|(n−d) distributions for the
overall Bayes net. We prune some of the distributions to get the desired subclass PC ⊆QC, such that PC is the
largest-sized subset with any pair of distributions in the subset diﬀering in at least (n −d)/2 bit-patterns (out
of (n −d) many) for the (Xi | X1, X2, . . . , Xd)’s.
Claim 5.4. |PC| ≥2Θ(d(n−d)).
Proof. Let ℓ= |C| = 2Θ(d) and m = n −d. Then, there are N = ℓm possible coeﬃcient-vectors/distributions
in QC. We create an undirectred graph G consisting of a vertex for each coeﬃcient, and edges between any
pair of vertices diﬀering in at least m/2 bit-patterns. Note that G is r-regular for r =
  m
0.5m

(ℓ−1)0.5m + · · · +
 m
0.5m+j

(ℓ−1)0.5m+j + · · · + (ℓ−1)m. Turan’s theorem says that there is a clique of size α = (1 −r
N )−1. We
deﬁne PC to be the vertices of this clique.
To show that α is as large as desired, it suﬃces to show N −r ≤N/2Θ(dm). The result follows by noting
N −r = 1 +
 m
1

(ℓ−1) + · · · +
 m
j

(ℓ−1)j + · · · +
  m
0.5m

(ℓ−1)0.5m ≤m · (4(ℓ−1))0.5m.
We also get for any two distributions Pa, Pb in this model with the coeﬃcients a, b respectively, dKL(Pa, Pb) =
1
2||a −b||2
2 from (1). Hence, any pair of PC has dKL = Θ(ϵ2). Then, Fano’s inequality tells us that given
a random distribution P ∈PC, it needs at least Ω(d(n −d)ϵ−2) samples to guess the correct one with 2/3
probability. We need the following fact about the dTV-distance among the members of PC.
Claim 5.5. Let Pa, Pb ∈PC be two distinct distributions (i.e. Pa ̸= Pb) with coeﬃcient-vectors a, b respectively.
Then, dTV(Pa, Pb) ∈Θ(ϵ).
17

Proof. By Pinsker’s inequality, we have dTV(Pa, Pb) ∈O(ϵ). Here we show dTV(Pa, Pb) ∈Ω(ϵ). Let n −d = m.
By construction, a and b diﬀer in m′ ≥m/2 conditional probabilities. Let a′ ⊆a, b′ ⊆b be the coeﬃcient-vectors
on the coordinates where they diﬀer. Let Pa′, Σa′ and Pb′, Σb′ be the corresponding marginal distributions on
(m′ + d) variables, and their covariance matrices. In the following, we show ||Σ−1
a′ Σb′ −I||F = Ω(ϵ). This proves
the claim from Fact 5.1.
Let Ma′ =
0m′×m′
0m′×d
Ad×m′
0d×d

be the adjacency matrix for Pa, where the sources appear last in the rows and
columns and in the matrix A, each Aij ∈{
1
√
md, 1+ϵ
√
md} denote the coeﬃcient from source i to sink j. Similarly,
we deﬁne Mb′ using a coeﬃcient matrix Bd×m′. Let {Ai : 1 ≤i ≤m′} and {Bi : 1 ≤i ≤m′} denote the
columns of A and B. Then for every i, Ai and Bi diﬀer in at least Θ(d) coordinates by construction.
By deﬁnition Σb′ =

•
BT
B
Id×d

and Σ−1
a′ =

Im′×m′
−AT
−A
•

, where • corresponds to certain matrices not
relevant to our discussion13. Let J = Σ−1
a′ Σb′ =

•
Xm′×d
•
•

. It can be checked that Xij = (Bi(j) −Ai(j)) for
every 1 ≤i ≤m′ and m′ + 1 ≤j ≤m′ + d. Now for every i, each of the Θ(d) places that Ai and Bi diﬀer,
Xij ∈
±ϵ
√
md. Hence, their total contribution in ||J −I||2
F = Ω(ϵ2).
Proof of Theorem 5.3. Consider any algorithm which learns a random distribution P = N(0, Σ) from PC in dTV
distance c3ϵ, for a small enough constant c3. Let the learnt distribution be bP = N(0, bΣ). Then, from Fact 5.1,
and triangle inequality of dTV, only the unique distribution P with Σ′ = Σ would satisfy ||bΣ−1Σ′ −I||F ≤c4ϵ for
every covariance matrix Σ′ from PC, for an appropriate choice of c4. This would reveal the random distribution,
hence the lower-bound follows.
6
Experiments
General Setup
For our experiments, we explored both polytree networks (generated using random Prüfer
sequences) and G(n, p) Erdős-Rényi graphs with bounded expected degrees (i.e. p = d/n for some bounded
degree parameter d) using the Python package networkx [HSSC08]. Our non-zero edge weights are uniformly
drawn from the range (−2, −1] ∪[1, 2). Once the graph is generated, the linear i.i.d. data X ∈Rm×n (with
n variables and sample size m ∈{1000, 2000, . . . , 5000}) is generated by sampling the model X = BT X + η,
where η ∼N(0, In×n) and B is a strictly upper triangular matrix.14 We report KL divergence between the
ground truth and our learned distribution using Eq. (1), averaged over 20 random repetitions. All experiments
were conducted on an Intel Core i7-9750H 2.60GHz CPU.
Algorithms
The algorithms used in our experiments are as follows: Graphical Lasso [FHT08], MLE (empirical)
estimator, CLIME [CLL11], LeastSquares, BatchAvgLeastSquares, BatchMedLeastSquares, CauchyEstTree,
and CauchyEst. Speciﬁcally, we use BatchAvg_LS+x and BatchMed_LS+x to represent the BatchAvgLeast-
Squares and BatchMedLeastSquares algorithms respectively with a batch size of p + x at each node, where p
is the number of parents of that node.
Synthetic data
Fig. 2 compares the KL divergence between the ground truth and our learned distribution
over 100 variables between the eight estimators mentioned above. The ﬁrst three estimators are for undirected
graph structure learning. For this reason, we are not using Eq. (1) but the common equation in [Duc07, page
13] for calculating the KL divergence between multivariate Gaussian distributions. Fig. 2(a) and Fig. 2(b)
shows the results on ER graphs while Fig. 2(c) shows the results for random tree graphs. The performances of
MLE and CLIME are very close to each other, thus are overlapped in Fig. 2(a). In ﬁgure Fig. 2(b), we take a
closer look at the results from Fig. 2(a) for LeastSquares, BatchMedLeastSquares, BatchAvgLeastSquares,
CauchyEstTree, and CauchyEst estimators for a clear comparison. In our experiments, we ﬁnd that the
latter ﬁve outperform the GLASSO, CLIME and MLE (empirical) estimators. With a degree 5 ER graph,
CauchyEst performs better than CauchyEstTree, while LeastSquares performs best. In our experiments for
13The missing (symmetric) submatrix of Σb′ is the identity matrix added with the entries ⟨Bi, Bj⟩. The missing (symmetric)
submatrix of Σ−1
a′
is the identity matrix added with the inner products of the rows of A.
14We do not report the results over the varied variance synthetic data, because their performance are close to the performance
of the equal variance synthetic data.
18

(a) Eight algorithms evaluated on ER graph with d = 5 (b) A closer look at some of the algorithms in the plot of
Fig. 2(a)
(c) A closer look at some of the algorithms evaluated on
random tree graphs
Figure 2: Experiment on well-conditioned uncontaminated data
our random tree graphs with in-degree 1 (see Fig. 2(c), we ﬁnd that the performances between CauchyEstTree
and CauchyEst are very close to each other and their plots overlap.
In our experiments, CauchyEst outperforms CauchyEstTree when the G(n, p) graph is generated with a
higher degree parameter d (e.g. d > 5) and the resultant graph is unlikely to yield a polytree structure.
Real-world datasets
We also evaluated our algorithms on four real Gaussian Bayesian networks from R
package bnlearn [Scu09]. The ECOLI70 graph provided by [SS05] contains 46 nodes and 70 edges. The
MAGIC-NIAB graph from [SHBM14] contains 44 nodes and 66 edges. The MAGIC-IRRI graph contains
64 nodes and 102 edges, and the ARTH150 [ORS07] graph contains 107 nodes and 150 edges. Experimental
results in Fig. 3 show that the error for LeastSquares is smaller than CauchyEst and CauchyEstTree for all
the above datasets.
Contaminated synthetic data
The contaminated synthetic data is generated in the following way: we
randomly choose 5% samples with 5 nodes to be contaminated from the well-conditioned data over n = 100
node graphs. The well-conditioned data has a N(0, 1) noise for every variable, while the small proportion of the
contaminated data has either N(1000, 1) or Cauchy(1000, 1). In our experiments in Fig. 4 and Fig. 5, CauchyEst,
CauchyEstTree, and BatchMedLeastSquares outperform BatchAvgLeastSquares and LeastSquares by a large
margin. With more than 1000 samples, BatchMedLeastSquares with a batch size of p + 20 performs similar
to CauchyEst and CauchyEstTree, but performs worse with less samples. When comparing the performance
between LeastSqures and BatchAvgLeastSquares over either a random tree or a ER graph, the experiment in
Fig. 4(a) based on a random tree graph shows that LeastSqures performs better than BatchAvgLeastSquares
when sample size is smaller than 2000, while BatchAvgLeastSquares performs relatively better with more
samples. Experiment results in Fig. 5(a) based on ER degree 5 graphs is slightly diﬀerent from Fig. 4(a). In
Fig. 5(a), BatchAvgLeastSquares performs better than LeastSqures by a large margin. Besides, we can also
observe that the performances of CauchyEst, CauchyEstTree, and BatchMedLeastSquares are better than
the above two estimators and are consistent over diﬀerent types of graphs. For all ﬁve algorithms, we use the
median absolute devation for robust variance recovery [Hub04] in the contaminated case only (see Algorithm 8
19

(a) Ecoli70 46 nodes
(b) Magic-niab 44 nodes
(c) Magic-irri 64 nodes
(d) Arth150 107 nodes
Figure 3: Experiment results over bnlearn real graph
in Appendix).
This is because both LeastSquares and BatchAvgLeastSquares use the sample covariance (of the entire
dataset or its batches) in the coeﬃcient estimators for the unknown distribution. The presence of a small
proportion of outliers in the data can have a large distorting inﬂuence on the sample covariance, making them
sensitive to atypical observations in the data. On the contrary, our CauchyEstTree and BatchMedLeastSquares
estimators are developed using the sample median and hence are resistant to outliers in the data.
Contaminated real-world datasets
To test the robustness of the real data in the contaminated condition,
we manually contaminate 5% samples of 5 nodes from observational data in ECOLI70 and ARTH150. The
results are reported in Fig. 6 and Fig. 7. In our experiments, CauchyEst and CauchyEstTree outperforms
BatchAvgLeastSquares and LeastSquares by a large margin, and therefore are stable in both contaminated
and well-conditioned case. Besides, note that diﬀerent from the well-conditioned case, here CauchyEstTree
performs slightly better than CauchyEst. This is because the Cholesky decomposition used in CauchyEst
estimator takes contaminated-data into account.
(a) all algorithms
(b) BatchMed_LS, CauchyEst, and CauchyEstTree
Figure 4: Experiment results on contaminated data (random tree with Gaussian noise)
20

(a) all algorithms
(b) BatchMed_LS, CauchyEst, and CauchyEstTree
Figure 5: Experiment results on contaminated data (ER graph with Cauchy noise)
(a) Ecoli70, 5/46 noisy nodes
(b) CauchyEst and CauchyEstTree
Figure 6: Ecoli70 under contaminated condition
(a) Arth150, 5/107 noisy nodes
(b) CauchyEst and CauchyEstTree
Figure 7: Arth150 under contaminated condition
21

(a) ER graph, d = 5
(b) Random tree
Figure 8: Experiment results on ill-conditioned data
(a) Random tree: 4 edges removed
(b) ER graph, d=5: 9 edges removed
Figure 9: Agnostic learning
Ill-conditioned synthetic data
The ill-conditioned data is generated in the following way: we classify the
node sets V into either well-conditioned or ill-conditioned nodes. The well-conditioned nodes have a N(0, 1) noise,
while ill-conditioned nodes have a N(0, 10−20) noise. In our experiments, we choose 3 ill-conditioned nodes over
100 nodes. Synthetic data is sampled from either a random tree or a Erdős Rényi (ER) model with an expected
number of neighbors d = 5. Experiments over ill-conditioned Gaussian Bayesian networks through 20 random
repetitions are presented in Fig. 8. For the ill-conditioned settings, we sometimes run into numerical issues when
computing the Cholesky decomposition of the empirical covariance matrix ˆ
M in our CauchyEst estimator. Thus,
we only show the comparision results between LeastSquares, BatchAvgLeastSquares, BatchMedLeastSquares,
and CauchyEstTree. Here also, the error for LeastSquares decreases faster than the other three estimators.
The performance of BatchMedLeastSquares is worse than BatchAvgLeastSquares but slightly better than
CauchyEstTree estimator.
Agnostic Learning
Our theoretical results treat the case that the data is realized by a distribution consistent
with the given DAG. In this section, we explore learning of non-realizable inputs, so that there is a non-zero KL
divergence between the input distribution P and any distribution consistent with the given DAG.
We conduct agnostic learning experiments by ﬁtting a random sub-graph of the ground truth graph. To do
this, we ﬁrst generate a 100-node ground truth graph G, either a random tree with 4 random edges removed or
a random ER graph with 9 random edges removed. Our algorithm will try to ﬁt the data from the original
Bayes net on G on the edited graph G∗. Fig. 9 reports the KL divergence learned over our ﬁve estimators. We
ﬁnd that BatchAvgLeastSquares estimator performs slightly better than all other estimators in both cases.
Eﬀect of changing batch size
Next, we experiment the trade oﬀbetween the batch-size (eg.
batch
size = [5, 20, 100]15) and the KL-divergence of our BatchAvgLeastSquares and BatchMedLeastSquares
estimators in detail.
As shown in Fig. 10 and Fig. 11, when batch size increases, the results are closer
15We use batch size up to 100 to ensure there are enough batch size from simulation data, so that either mean and median can
converge.
22

(a) ER graph, d = 2
(b) ER graph, d = 5
Figure 10: Eﬀect of changing batch size over Batch Average
(a) ER graph, d = 2
(b) ER graph, d = 5
Figure 11: Eﬀect of changing batch size over Batch Median (ER)
23

to the LeastSquares estimator.
In other words, LeastSquares can be seen as a special case of either
BatchAvgLeastSquares or BatchMedLeastSquares with one batch only. Thus, when batch size increases, the
performances of BatchAvgLeastSquares and BatchMedLeastSquares are closer to the LeastSquares estimator.
On the contrary, the CauchyEstTree estimator can be seen as the estimator with a batch size of p. Therefore,
with smaller batch size (eg. batch size = p + 5), BatchAvgLeastSquares and BatchMedLeastSquares performs
closer to the CauchyEstTree estimator.
Runtime comparison
We now give the amount of time spent by each algorithm to process a degree-5 ER
graph on 100 nodes with 500 samples. LeastSquares algorithm takes 0.0096 seconds, BatchAvgLeastSquares
with a batch size of p + 20 takes 0.0415 seconds, BatchMedLeastSquares with a batch size of p + 20 takes
0.0290 seconds, CauchyEstTree takes 0.6063 seconds, and CauchyEst takes 0.6307 seconds. The timings given
above are representative of the relative running times of these algorithms across diﬀerent graph sizes.
Takeaways
In summary, the LeastSquares estimator performs the best among all algorithms on uncontami-
nated datasets (both real and synthetic) generated from Gaussian Bayesian networks. This holds even when
the data is ill-conditioned. However, if a fraction of the samples are contaminated, CauchyEst, CauchyEst-
Tree and BatchMedLeastSquares outperform LeastSquares and BatchAvgLeastSquares by a large margin
under diﬀerent noise and graph types. If the data is not generated according to the input graph (i.e., the
non-realizable/agnostic learning setting), then BatchAvgLeastSquares, CauchyEst, and CauchyEstTree have
a better tradeoﬀbetween the error and sample complexity than the other algorithms, although we do not have
a formal explanation.
References
[ABDH+20] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and
Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via
compression schemes. Journal of the ACM (JACM), 67(6):1–42, 2020. 2, 6, 9
[AGZ19]
Bryon Aragam, Jiaying Gu, and Qing Zhou. Learning large-scale bayesian networks with the
sparsebn package. Journal of Statistical Software, 91(1):1–38, 2019. 3
[AZ15]
Bryon Aragam and Qing Zhou. Concave penalized estimation of sparse gaussian bayesian networks.
The Journal of Machine Learning Research, 16(1):2273–2328, 2015. 3
[BCD20]
Johannes Brustle, Yang Cai, and Constantinos Daskalakis.
Multi-item mechanisms without
item-independence: Learnability via robustness. In Proceedings of the 21st ACM Conference on
Economics and Computation, pages 715–761, 2020. 2
[BGMV20]
Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S Meel, and NV Vinodchandran. Eﬃcient distance
approximation for structured high-dimensional distributions via learning. Advances in Neural
Information Processing Systems, 33, 2020. 3
[BGPV20]
Arnab Bhattacharyya, Sutanu Gayen, Eric Price, and NV Vinodchandran. Near-optimal learning
of tree-structured distributions by chow-liu. arXiv preprint arXiv:2011.04144, 2020. ACM STOC
2021. 3
[BVH+16]
Afonso S Bandeira, Ramon Van Handel, et al. Sharp nonasymptotic bounds on the norm of
random matrices with independent entries. Annals of Probability, 44(4):2479–2506, 2016. 5
[CDKS20]
C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing bayesian networks. IEEE
Transactions on Information Theory, 66(5):3132–3170, 2020. 3
[CDW19]
Wenyu Chen, Mathias Drton, and Y Samuel Wang. On causal discovery with an equal-variance
assumption. Biometrika, 106(4):973–980, 2019. 3
[CLL11]
Tony Cai, Weidong Liu, and Xi Luo. A constrained l1 minimization approach to sparse precision
matrix estimation. Journal of the American Statistical Association, 106(494):594–607, 2011. 18
[Das97]
Sanjoy Dasgupta. The sample complexity of learning ﬁxed-structure bayesian networks. Mach.
Learn., 29(2-3):165–180, 1997. 2, 3, 6
24

[Dia16]
Ilias Diakonikolas. Learning structured distributions. Handbook of Big Data, 267, 2016. 1
[DMR18]
Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between
high-dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018. 16
[Duc07]
John Duchi. Derivations for linear algebra and optimization. Berkeley, California, 3(1):2325–5870,
2007. 18
[FHT08]
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation
with the graphical lasso. Biostatistics, 9(3):432–441, 2008. 18
[GDA20]
Ming Gao, Yi Ding, and Bryon Aragam. A polynomial-time algorithm for learning nonparametric
causal graphs. Advances in Neural Information Processing Systems, 33, 2020. 3
[GH17]
Asish Ghoshal and Jean Honorio. Learning identiﬁable Gaussian bayesian networks in polynomial
time and sample complexity. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, pages 6460–6469, 2017. 3
[Gut09]
Allan Gut. An Intermediate Course in Probability. Springer New York, 2009. 4
[GZ20]
Jiaying Gu and Qing Zhou. Learning big gaussian bayesian networks: Partition, estimation and
fusion. Journal of Machine Learning Research, 21(158):1–31, 2020. 3
[Hau18]
David Haussler. Decision theoretic generalizations of the PAC model for neural net and other
learning applications. CRC Press, 2018. 3
[HSSC08]
Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and
function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
(United States), 2008. 18
[HTW19]
Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical learning with sparsity: the
lasso and generalizations. Chapman and Hall/CRC, 2019. 2
[Hub04]
Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004. 19, 30
[JNG+19]
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan.
A short
note on concentration inequalities for random vectors with subgaussian norm. arXiv preprint
arXiv:1902.03736, 2019. 29
[KMR+94]
Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire, and Linda
Sellie. On the learnability of discrete distributions. In Proceedings of the twenty-sixth annual ACM
symposium on Theory of computing, pages 273–282, 1994. 1
[KP83]
H Kim and J Perl. ’a computational model for combined causal and diagnostic reasoning in
inference systems’, 8th ijcai, 1983. 2
[Mue99]
Ralph O Mueller. Basic principles of structural equation modeling: An introduction to LISREL
and EQS. Springer Science & Business Media, 1999. 1
[Mul09]
Stanley A Mulaik. Linear causal modeling with structural equations. CRC press, 2009. 1
[ORS07]
Rainer Opgen-Rhein and Korbinian Strimmer. From correlation to causation networks: a simple
approximate learning algorithm and its application to high-dimensional plant gene expression data.
BMC systems biology, 1(1):1–10, 2007. 19
[Par20]
Gunwoong Park. Identiﬁability of additive noise models using conditional variances. Journal of
Machine Learning Research, 21(75):1–34, 2020. 3
[PB14]
Jonas Peters and Peter Bühlmann. Identiﬁability of Gaussian structural equation models with
equal error variances. Biometrika, 101(1):219–228, 2014. 3
[Pea86]
Judea Pearl. Fusion, propagation, and structuring in belief networks. Artiﬁcial intelligence,
29(3):241–288, 1986. 2
25

[Pea88]
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan
Kaufmann Publishers, San Francisco, Calif, 2nd edition edition, 1988. 3
[PK20]
Gunwoong Park and Youngwhan Kim. Identiﬁability of Gaussian linear structural equation models
with homogeneous and heterogeneous error variances. Journal of the Korean Statistical Society,
49(1):276–292, 2020. 3
[RV09]
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 62(12):1707–1739, 2009. 28
[Scu09]
Marco Scutari.
Learning bayesian networks with the bnlearn r package.
arXiv preprint
arXiv:0908.3817, 2009. 19
[Scu20]
Marco Scutari. bnlearn, 2020. Version 4.6.1. 3
[SHBM14]
Marco Scutari, Phil Howell, David J Balding, and Ian Mackay. Multiple quantitative trait analysis
using bayesian networks. Genetics, 198(1):129–137, 2014. 19
[SS05]
Juliane Schafer and Korbinian Strimmer. A shrinkage approach to large-scale covariance matrix
estimation and implications for functional genomics. Statistical applications in genetics and
molecular biology, 4(1), 2005. 19
[Tsy08]
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008. 5
[Val84]
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.
1, 3
[Wai19]
Martin J Wainwright.
High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge University Press, 2019. 5, 28
26

A
Details on decomposition of KL divergence
In this section, we provide the full derivation of Eq. (1).
For notational convenience, we write x to mean (x1, . . . , xn), πi(x) to mean the values given to parents of
variable Xi by x, and P(x) to mean P(X1 = x1, . . . , Xn = xn). Observe that
dKL(P, Q)
=
Z
x
P(x) log
P(x)
Q(x)

dx
=
Z
x
P(x) log
Πn
i=1P(xi | πi(x))
Πn
i=1Q(xi | πi(x))

dx
(⋆)
=
n
X
i=1
Z
x
P(x) log
P(xi | πi(x))
Q(xi | πi(x))

dx
=
n
X
i=1
Z
xi,πi(x)
P(xi, πi(x)) log
P(xi | πi(x))
Q(xi | πi(x))

dxidπi(x)
Marginalization
where (⋆) is due to the Bayesian network decomposition of joint probabilities. Let us deﬁne
dCP(α∗
i , bαi) =
Z
xi,πi(x)
P(xi, πi(x)) log
P(xi | πi(x))
Q(xi | πi(x))

dxidπi(x)
where each bαi and α∗
i represent the parameters that relevant to variable Xi from bα and α∗respectively. Under
this notation, we can write dKL(P, Q) = Pn
i=1 dCP(α∗
i , bαi).
Fix a variable of interest Y with parents X1, . . . , Xp, each with coeﬃcient ci, and variance σ2. That is,
Y = ηy + Pp
i=1 ciXi for some ηy ∼N(0, σ2) that is independent of X1, . . . , Xp. By denoting X = x (i.e.
X1 = x1, . . . , Xp = xp) and c = (c1, . . . , cp), we can write the conditional distribution density of Y as
Pr (y | x, c, σ) =
1
σ
√
2π exp

−1
2σ2 ·
 
y −
p
X
i=1
ciXi
!2

We now analyze dCP(α∗
y, bαy) with respect to the our estimates bαy = ( bA, bσy) and the hidden true parameters
α∗
y = (A, σy), where bA = (bay←1, . . . , bay←p) and A = (ay←1, . . . , ay←p).
With respect to variable Y , we see that
dCP(α∗
y, bαy)
=
Z
x,y
P(x, y) ln




1
σy
√
2π exp

−1
2σ2 · (y −Pp
i=1 ay←iXi)2
1
bσy
√
2π exp

−
1
2bσ2y · (y −Pp
i=1 bay←iXi)2




dy dx
= ln
bσy
σy

−
1
2σ2y
· Ex,y
 
y −
p
X
i=1
ay←iXi
!2
+
1
2bσ2y
· Ex,y
 
y −
p
X
i=1
bay←iXi
!2
= ln
bσy
σy

−
1
2σ2y
· Ex,y
 y −A⊤X
2 +
1
2bσ2y
· Ex,y

y −bA⊤X
2
27

By deﬁning ∆= bA −A, we can see that for any instantiation of y, X1, . . . , Xp,

y −bA⊤X
2
=
 y −(∆+ A)⊤X
2
By deﬁnition of ∆
=
 (y −A⊤X) −∆⊤X
2
= (y −A⊤X)2 −2(y −A⊤X)(∆⊤X) +
 ∆⊤X
2
= (y −A⊤X)2 −2
 y∆⊤X −A⊤X∆⊤X

+
 ∆⊤X
2
= (y −A⊤X)2 −2
 yX⊤∆−A⊤XX⊤∆

+ ∆⊤XX⊤∆
Since ∆⊤X is just a number
Denote the covariance matrix with respect to X1, . . . , Xp as M ∈Rp×p, where Mi,j = E [XiXj]. Then, we
can further simplify dCP(α∗
y, bαy) as follows:
dCP(α∗
y, bαy)
= ln
bσy
σy

−
1
2σ2y
· Ex,y
 y −A⊤X
2 +
1
2bσ2y
· Ex,y

y −bA⊤X
2
From above
= ln
bσy
σy

−
1
2σ2y
· Ex,y
 y −A⊤X
2
+
1
2bσ2y
· Ex,y

(y −A⊤X)2 −2
 yX⊤∆−A⊤XX⊤∆

+ ∆⊤XX⊤∆

From above
= ln
bσy
σy

−
1
2σ2y
· Ex,yη2
y +
1
2bσ2y
· Ex,y

η2
y −2
 ηyX⊤∆

+ ∆⊤XX⊤∆

(†)
= ln
bσy
σy

−
1
2σ2y
· σ2
y +
1
2bσ2y
·

σ2
y −0 + ∆⊤M∆

(∗)
= ln
bσy
σy

−1
2 + σ2
y
2bσ2y
+ ∆⊤M∆
2bσ2y
= ln
bσy
σy

+ σ2
y −bσ2
y
2bσ2y
+ ∆⊤M∆
2bσ2y
where (†) is because y = ηy + A⊤X while (∗) is because ηy ∼N(0, σ2
y), Ex,y
 ηyX⊤∆

= Ex,yηy · Ex,yX⊤∆= 0,
and Ex,y∆⊤XX⊤∆= ∆⊤(Ex,yXX⊤)∆= ∆⊤M∆.
In conclusion, we have
dKL(P, Q) =
n
X
i=1
dCP(α∗
i , bαi) =
n
X
i=1
ln
bσi
σi

+ σ2
i −bσ2
i
2bσ2
i
+ ∆⊤
i Mi∆i
2bσ2
i
(3)
where Mi is the covariance matrix associated with variable Xi, α∗
i = (Ai, σi) is the coeﬃcients and variance
associated with variable Xi, αi = ( bAi, bσi) are the estimates for α∗
i , and ∆i = bAi −Ai.
B
Deferred proofs
This section provides the formal proofs that were deferred in favor for readability. For convenience, we will
restate the statements before proving them.
The next two lemmata Lemma 2.6 and Lemma 2.7 are used in the proof of Lemma 3.2, which is in turn
used in the proof of Theorem 3.1. We also use Lemma 2.6 in the proof of Lemma 3.5. The proof of Lemma 2.6
uses the standard result of Lemma B.1.
Lemma B.1 ([RV09]; Theorem 6.1 and Equation 6.10 in [Wai19]). Let ℓ≥d and G ∈Rℓ×d be a matrix with
i.i.d. N(0, 1) entries. Denote σmin(G) as the smallest singular value of G. Then, for any 0 < t < 1, we have
Pr

σmin(G) ≥
√
ℓ(1 −t) −
√
d

≤exp
 −ℓt2/2

.
28

Lemma 2.6. Let G ∈Rk×d be a matrix with i.i.d. N(0, 1) entries. Then, for any constant 0 < c1 < 1/2 and
k ≥d/c2
1,
Pr
 
(G⊤G)−1
op ≤
1
(1 −2c1)2 k
!
≥1 −exp

−kc2
1
2

Proof of Lemma 2.6. Observe that G⊤G is symmetric, thus (G⊤G)−1 is also symmetric and the eigenvalues of
G⊤G equal the singular values of G⊤G. Also, note that event that G⊤G is singular has measure 0.16
By deﬁnition of operation norm,
(G⊤G)−1 equals the square root of maximum eigenvalue of
((G⊤G)−1)⊤((G⊤G)−1) = ((G⊤G)−1)2,
where the equality is because (G⊤G)−1 is symmetric. Since G⊤G is invertible, we have ∥(G⊤G)−1∥= 1/∥G⊤G∥,
which is equal to the inverse of minimum eigenvalue λmin(G⊤G) of G⊤G, which is in turn equal to the square
of minimum singular value σmin(G) of G.
Therefore, the following holds with probability at least 1 −exp
 −kc2
1/2

:

 G⊤G
−1 =
1
∥G⊤G∥=
1
λmin(G⊤G) =
1
σ2
min(G) ≤
1
√
k(1 −c1) −
√
d
2 ≤
1
(1 −2c1)2 k
where the second last inequality is due to Lemma B.1 and the last inequality holds when k ≥d/c2
1.
Lemma 2.7. Let G ∈Rk×p be a matrix with i.i.d. N(0, 1) entries and η ∈Rk be a vector with i.i.d. N(0, σ2)
entries, where G and η are independent. Then, for any constant c2 > 0,
Pr
G⊤η

2 < 2σc2
p
kp

≥1 −2p exp (−2k) −p exp

−c2
2
2

Proof of Lemma 2.7. Let us denote gr ∈Rk as the rth row of G⊤. Then, we see that ∥G⊤η∥2
2 = Pp
r=1⟨gr, η⟩2.
For any row r, we see that ⟨gr, η⟩= ∥η∥2 · ⟨gr, η/∥η∥2⟩. We will bound values of ∥η∥2 and |⟨gr, η/∥η∥2⟩|
separately.
It is well-known (e.g. see [JNG+19, Lemma 2]) that the norm of a Gaussian vector concentrates around
its mean. So, Pr

∥η∥2 ≤2σ
√
k

≤2 exp (−2k). Since gr ∼N(0, Ik) and η are independent, we see that
⟨gr, η/∥η∥2⟩∼N(0, 1). By standard Gaussian bounds, we have that Pr [|⟨gr, η/∥η∥2⟩| ≥c2] ≤exp
 −c2
2/2

.
By applying a union bound over these two events, we see that ∥⟨gr, η⟩∥< 2σc2
√
k for any row with
probability 2 exp (−2k) + exp
 −c2
2/2

. The claim follows from applying a union bound over all p rows.
Lemma 2.8. [Non-asymptotic convergence of Cauchy median] Consider a collection of m i.i.d. Cauchy(0, 1)
random variables X1, . . . , Xm. Given a threshold 0 < τ < 1, we have
Pr (median {X1, . . . , Xm} ̸∈[−τ, τ]) ≤2 exp

−mτ 2
8

Proof of Lemma 2.8. Let S>τ = Pm
i=1 1Xi>τ be the number of values that are larger than τ, where E[1Xi>τ] =
Pr(X ≥τ). Similarly, let S<−τ be the number of values that are smaller than −τ. If S>τ < m/2 and
S<−τ < m/2, then we see that median {X1, . . . , Xm} ∈[−τ, τ].
For a random variable X ∼Cauchy(0, 1), we know that Pr(X ≤x) = 1/2 + arctan(x)/π. For 0 < τ < 1, we
see that Pr(X ≥τ) = 1/2 −arctan(τ)/π ≤1/2 −τ/4. By additive Chernoﬀbounds, we see that
Pr

S>τ ≥m
2

≤exp

−2m2τ 2
16m

= exp

−mτ 2
8

Similarly, we have Pr (S<−τ ≥m/2) ≤exp
 −mτ 2/8

. The claim follows from a union bound over the events
S>τ ≥m/2 and S<−τ ≥m/2.
16Consider ﬁxing all but one arbitrary entry of G. The event of this independent N(0, 1) entry making det(G⊤G) = 0 has
measure 0.
29

C
Median absolute deviation
In this section, we give a pseudo-code of the well-known Median Absolute Deviation (MAD) estimator
(see [Hub04] for example) which we use for the component-wise variance recovery in the contaminated setting.
The scale factor, 1/Φ−1(3/4) ≈1.4826 below, is needed to make the estimator unbiased.
Algorithm 8 MAD: Variance recovery in the contaminated setting
1: Input: Contaminated samples {x1, x2, . . . , xm} from a univariate Gausssian
2: bµ ←median {x1, x2, . . . , xm}.
3: bσ ←1.4826 · median {|x1 −bµ|, |x2 −bµ|, . . . , |xm −bµ|}.
4: return bσ
30

