Scaling TransNormer to 175 Billion Parameters
2Zhen Qin‚ôØ, 1,2Dong Li‚ôØ, 1,2Weigao Sun‚ôØ, 1,2Weixuan Sun‚ôØ, 1,2Xuyang Shen‚ôØ,
2Xiaodong Han, 2Yunshen Wei, 2Baohong Lv, 1Fei Yuan, 2Xiao Luo,
1Yu Qiao, 1,2Yiran Zhong‚àó
1Shanghai AI Laboratory, 2OpenNLPLab
https://github.com/OpenNLPLab/TransnormerLLM
Abstract
We present TransNormerLLM, the first linear attention-based Large Language
Model (LLM) that outperforms conventional softmax attention-based models in
terms of both accuracy and efficiency. TransNormerLLM evolves from the previous
linear attention architecture TransNormer [32] by making advanced modifications
that include positional embedding, linear attention acceleration, gating mechanism,
tensor normalization, and inference acceleration and stabilization. Specifically,
we use LRPE [34] together with an exponential decay to avoid attention dilution
issues while allowing the model to retain global interactions between tokens. Addi-
tionally, we propose Lightning Attention, a cutting-edge technique that accelerates
linear attention by more than twice in runtime and reduces memory usage by a
remarkable four times. To further enhance the performance of TransNormer, we
leverage a gating mechanism to smooth training and a new tensor normalization
scheme to accelerate the model, resulting in an impressive acceleration of over
20%. Furthermore, we have developed a robust inference algorithm that ensures
numerical stability and consistent inference speed, regardless of the sequence
length, showcasing superior efficiency during both training and inference stages.
Scalability is at the heart of our model‚Äôs design, enabling seamless deployment on
large-scale clusters and facilitating expansion to even more extensive models, all
while maintaining outstanding performance metrics. Rigorous validation of our
model design is achieved through a series of comprehensive experiments on our
self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion
tokens. To ensure data quality and relevance, we implement a new self-cleaning
strategy to filter our collected data. we plan to open-source our pre-trained models,
fostering community-driven advancements in the field and positioning our work as
a stepping-stone toward exploring efficient transformer structures in LLMs.
1
Introduction
The field of Natural Language Processing (NLP) has been revolutionized by the advent of large-scale
language models (LLMs) [43, 2, 3]. These models have demonstrated exceptional performance across
a multitude of tasks, elevating abilities to comprehend, generate, and interact with human languages
in computational frameworks. Previous language modeling development has predominantly centered
around Transformer architectures, with seminal models such as vanilla Transformer [44], GPT
series [36, 37, 3], BERT [10], and BART [24] standing as standard backbones in related fields. The
success of Transformer architectures is premised on the softmax attention mechanism, which discerns
dependencies among input tokens in a data-driven scheme and has global position awareness, offering
the model an effective way to handle the long-range dynamism of natural language.
‚àóYiran Zhong is the corresponding author. Email: zhongyiran@gmail.com. ‚ôØindicates equal contribution.
Preprint. Under review.
arXiv:2307.14995v1  [cs.CL]  27 Jul 2023

Table 1: TransNormerLLM Model Variants.
Model Size
Non-Embedding Params
Layers
Model Dim
Heads
Equivalent Models
385M
384,974,848
24
1024
8
Pythia-410M
1B
992,165,888
16
2048
16
Pythia-1B
3B
2,876,006,400
32
2560
20
Pythia-2.8B
7B
6,780,547,072
30
4096
32
LLAMA-6.7B
13B
12,620,195,840
36
5120
40
LLAMA-13B
65B
63,528,009,728
72
8192
64
LLAMA-65B
175B
173,356,498,944
88
12288
96
GPT-3
Nevertheless, conventional Transformers are not without their constraints. Primarily, their quadratic
time complexity with respect to the sequence length limits their scalability and hampers efficiency
in terms of computational resources and time during the training and inference stages. Numerous
efficient sequence modeling methods have been proposed in an attempt to reduce the quadratic time
complexity to linear [21, 5, 33, 49, 48]. However, there are two reasons that prohibit them to be
applied to LLMs: 1) their performance in language modeling is often unsatisfactory; 2) they do not
demonstrate speed advantages in real-world scenarios.
In this paper, we introduce TransNormerLLM, the first linear attention-based LLM that surpasses
conventional softmax attention in both accuracy and efficiency. The development of TransNormer-
LLM builds upon the foundations of the previous linear attention architecture, TransNormer [32],
while incorporating a series of advanced modifications to achieve superior performance. The key
enhancements in TransNormerLLM include positional embedding, linear attention acceleration,
gating mechanism, tensor normalization, and inference acceleration.
One notable improvement is the replacement of the TransNormer‚Äôs DiagAttention with Linear
Attention to enhance global interactions. To address the issue of dilution, we introduced LRPE [34]
with exponential decay. Lightning Attention, a novel technique that significantly accelerates linear
attention during training is introduced, resulting in a more than two-fold improvement, while also
reducing memory usage by four times with IO awareness. Furthermore, we simplified GLU and
Normalization, with the latter leading to a 20% overall speedup. A robust inference algorithm ensures
the stability of numerical values and constant inference speed, regardless of the sequence length,
thereby enhancing the efficiency of our model during both training and inference stages.
To validate the efficacy of TransNormerLLM, we meticulously collect a large corpus that is more
than 6TB in size and contains over 2 trillion tokens. We develop a new self-cleaning strategy to filter
the collected corpus to ensure data quality. We expand the original TransNormer model ranging from
385M to 175B parameters as illustrated in Table 1 and conduct a series of comprehensive experiments
and ablations on our corpus, demonstrating superior performance to softmax attention-based methods
as well as faster training and inference speed.
We are committed to fostering community-driven advancements in the field of LLMs. To this end,
we plan to open-source our pre-trained models, enabling researchers and practitioners to build upon
our work and explore efficient transformer structures in LLMs.
2
Related Work
2.1
Transformer-based LLMs
In recent years, the field of Large Language Models (LLMs) has experienced significant advancements.
Adhering to the scaling laws [20], various LLMs with over 100 billion parameters have been
introduced, such as GPT-3 [3], Gopher [38], PaLM [6], GLM [11] and etc.. More specialized models
like Galactica [41] have also emerged for specific domains like science. A notable development is
Chinchilla [17], an LLM model with 70 billion parameters that redefines these scaling laws, focusing
on the number of tokens rather than model weights. Furthermore, LLaMA [43] has also sparked
interest due to its promising performance and open-source availability. The discourse around LLMs
also encompasses the dynamics between open-source and closed-source models. Open-source models
such as BLOOM [45], OPT [46], LLaMA [43], Pythia [2] and Falcon [28] are rising to compete
against their closed-source counterparts, including GPT-3 [3] and Chinchilla [17]. To speed up
2

training, Sparse Attention [4, 1] was introduced, but among large models, only GPT-3 adopted
it [3, 39].
2.2
Non-Transformer-based LLMs Candidates
Despite the proliferation of Transformer-based large models in the research community, a portion
of recent work has prioritized addressing its square space-time complexity. This focus has led to
the exploration and development of a series of model architectures that diverge from the traditional
Transformer structure. Among them, four significant contenders‚Äîlinear transformers, state space
model, long convolution, and linear recurrence‚Äîhave shown promising results as substitutes for
self-attention (SA) modules when modeling long sequences. These alternatives are favored for their
superior asymptotic time complexity and competitive performances.
Linear Transformer
Linear Transformer decomposes Softmax Attention into the form of the inner
product of hidden representations, which allows it to use the "Right Product Trick," where the product
of keys and values is computed to avoid the quadratic n √ó n matrix. Different methods utilize various
hidden representations. For example, [21] uses 1+elu as an activation function, [33] uses the cos
function to approximate the properties of softmax, and [22, 48, 49] approximates softmax through
theoretical approaches. Although its theoretical complexity is O(nd2), the actual computational
efficiency of Linear Attention becomes quite low when used in causal attention due to the need
for cumsum operations [18]. On the other hand, most Linear Transformers still exhibit a certain
performance gap compared to traditional Transformers [21, 25].
State Space Model
State Space Model is based on the State Space Equation for sequence mod-
eling [15], using special initialization [13, 14], diagonalization assumptions [16], and some tech-
niques [9] to achieve performance comparable to Transformers. On the other hand, due to the
characteristics of the State Space Equation, it enables inference to be conducted within constant
complexity [15].
Long Convolution
Long convolution models [31, 12] utilize a kernel size equal to the input
sequence length, facilitating a wider context compared to traditional convolutions. Training these
models involves the efficient O(n log n) Fast Fourier Transforms (FFT) algorithm. However, long
convolutions pose certain challenges, such as the need for causal convolution inference, which
necessitates caching all historical computations similar to SA‚Äôs key-value (KV) cache. The memory
requirements for handling long sequences, coupled with the higher inference complexity compared to
RNNs, make them less ideal for processing long sequences.
Linear RNN
Linear RNNs, in contrast, stand out as more suitable replacements for SA in long-
sequence modeling. A notable example is the RWKV [29] model, a linear RNN-based LLM that has
shown competitive performance against similarly scaled GPT models.
3
TransNormerLLM
3.1
Architecture Improvement
In this section, we thoroughly investigate each module of the network and propose several improve-
ments to achieve an optimal balance between efficiency and performance. Below, we outline the key
designs of each block along with the inspiration behind each change. We conduct an ablation study
on each of these modifications in Sec. 5.
3.1.1
Improvement 1: Position encoding
In TransNormer, DiagAttention is used at the lower layers to avoid dilution issues. However, this
leads to a lack of global interaction between tokens. In TransNormerLLM, we leverage LRPE [34]
with exponential decay [30, 31] to address this issue, retaining full attention at the lower layers. The
overall expression of our position encoding is as follows:
ast = q‚ä§
s ktŒªs‚àít expiŒ∏(s‚àít) .
(1)
3

which we call it LRPE-d - Linearized Relative Positional Encoding with exponential decay. Similar
to the original LRPE, we set Œ∏ to be learnable.
Note that this position encoding is fully compatible with Linear Attention, as it can be decomposed
with respect to s and t separately. The value of Œª for the h-th head in the l-th layer (assuming there
are a total of H heads and L layers) is given by:
Œª = exp

‚àí8h
H √ó

1 ‚àíl
L

.
(2)
Here, 8h
H corresponds to the decay rate of the h-th head, while
 1 ‚àíl
L

corresponds to the decay rate
of the l-th layer. The term
 1 ‚àíl
L

ensures that the Theoretical Receptive Fields (TRF) [35] at the
lower layers is smaller compared to the higher layers, which aligns with TransNormer‚Äôs motivation. It
should be noted that the decay rate in the last layer is set to 1, allowing each token to attend to global
information. We choose Œª to be non-learnable since we empirically found that gradients become
unstable when Œª is learnable, leading to NaN values.
3.1.2
Improvement 2: Gating mechanism
Gate can enhance the performance of the model and smooth the training process. In TransNormer-
LLM, we adopted the approach from Flash [18] and used the structure of Gated Linear Attention
(GLA) in token mixing:
TokenMixer : O = Norm(QK‚ä§V) ‚äôU,
(3)
where:
Q = œï(XWq), K = œï(XWk), V = XWv, U = XWu.
(4)
We chose œï to be 1 + elu. We found that the specific choice of œï has a minimal impact on the results,
as shown in Table 8.
To further accelerate the model, we propose Simple GLU (SGLU), which removes the activation
function from the original GLU structure as the gate itself can introduce non-linearity. Therefore, our
channel mixing becomes:
ChannelMixer : O = [V ‚äôU]Wo, V = XWv, U = XWu,
(5)
We empirically find that not using an activation function does not lead to any performance loss, as
demonstrated in Table 9.
3.1.3
Improvement 3: Tensor normalization
We employ the NormAttention introduced in TransNormer [32] as follows:
O = Norm((QK‚ä§)V)
(6)
This attention mechanism eliminates the softmax and scaling operation. Moreover, it can be trans-
formed into linear attention through right multiplication:
O = Norm(Q(K‚ä§V))
(7)
This linear form allows for recurrent prediction with a complexity of O(nd2), making it efficient
during inference. Specifically, we only update K‚ä§V in a recurrent manner without computing the full
attention matrix. In TransNormerLLM, we replace the RMSNorm with a new simple normalization
function called SimpleRMSNorm, abbreviated as SRMSNorm:
SRMSNorm(x) =
x
‚à•x‚à•2/
‚àö
d
.
(8)
We empirically find that using SRMSNorm does not lead to any performance loss, as demonstrated in
the ablation study in Table. 10.
3.1.4
The overall structure
4

ùëã
SGLU
Add
SGLU
ùëà
ùëÑ
ùêæ
ùëâ
SRMSNorm
GLA
*
*
ùëà
ùëâ
GLA
SRMSNorm
Add
SRMSNorm
Figure 1: Architecture overview of the proposed
model. Each transformer block is composed of
a Simple Gated Linear Unit (SGLU) for channel
mixing and a Gated Linear Attention for token
mixing. We apply pre-norm for both modules.
The overall structure is illustrated in Figure 1.
In this structure, the input X is updated through
two consecutive steps: First, it undergoes Gated
Linear Attention (GLA) with the application of
SimpleRMSNorm (SRMSNorm) normalization.
Then, it goes through the Simple Gated Linear
Unit (SGLU) with SRMSNorm normalization
again. This overall architecture helps improve
the model‚Äôs performance based on the PreNorm
approach. The pseudo-code of the overall pro-
cess is as follows:
X = X + GLA(SRMSNorm(X)),
X = X + SGLU(SRMSNorm(X)).
(9)
3.2
Training Optimization
3.2.1
Lightning Attention
The structure of linear attention allows for ef-
ficient attention calculation with a complexity
of O(nd2) through right-multiplication. How-
ever, for causal prediction, right-multiplication
is not efficient as it necessitates cumsum compu-
tation [18], which hinders parallelism training.
As a result, during training, we continue to use the conventional left-multiplication version. To
accelerate attention calculations, we introduce the Lightning Attention algorithm inspired by [7, 8],
which makes our linear attention IO-friendly. It computes the following expression:
O = (QK‚ä§‚äôM)V.
(10)
Here, M is the attention mask which enables lower triangular causal masking and positional encoding.
In the Lightning Attention, we split the inputs Q, K, V into blocks, load them from slow HBM to
fast SRAM, then compute the attention output with respect to those blocks. Then we accumulate
the final results. The computation speed is accelerated by avoiding the operations on slow HBM.
The implementation details of Lightning Attention are shown in Algorithm 1 for forward pass and
Algorithm 2 for backward pass. Note that there is a faster implementation for gradient computation
that will be released in the future.
Algorithm 1 Lightning Attention Forward Pass
Input: Q, K, V ‚ààRn√ód, attention mask M ‚ààRn√ón, block sizes Bc, Br;
Initialize: O = 0 ‚ààRn√ód;
Divide Q into Tr =
n
Br blocks Q1, Q2, ...QTr of size Br √ó d each.
Divide K, V into Tc =
n
Bc blocks K1, K2, ...KTc, V1, V2, ...VTc of size Bc √ó d each.
Divide O into Tr =
n
Br blocks O1, O2, ...OTr of size Br √ó d each.
Divide M into Tr √ó Tc blocks M11, M12, ...MTr,Tc of size Br √ó Bc each.
for 1 ‚â§i ‚â§Tr do
Load Qi ‚ààRBr√ód from HBM to on-chip SRAM.
Initialize Oi = 0 ‚ààRBr√ód on SRAM.
for 1 ‚â§j ‚â§Tc do
Load Kj, Vj ‚ààRBc√ód from HBM to on-chip SRAM.
Load Mij ‚ààRBc√óBc from HBM to on-chip SRAM.
On chip, compute Aij = [QiK‚ä§
j ] ‚äôMij ‚ààRBr√óBc.
On chip, compute Oi = Oi + AijVj ‚ààRBr√ód.
end for
Write Oi to HBM as the i-th block of O.
end for
return O
5

Algorithm 2 Lightning Attention Backward Pass
Input: Q, K, V, dO ‚ààRn√ód, attention mask M ‚ààRn√ón, on-chip SRAM of size M, block sizes Bc, Br;
Initialize: dQ = dK = dV = 0 ‚ààRn√ód;
Divide Q into Tr =
n
Br blocks Q1, Q2, ...QTr of size Br √ó d each.
Divide K, V into Tc =
n
Bc blocks K1, K2, ...KTc, V1, V2, ...VTc of size Bc √ó d each.
Divide O, dO into Tr =
n
Br blocks O1, O2, ...OTr, dO1, dO2, ...dOTr of size Br √ó d each
Divide M into Tr √ó Tc blocks M11, M12, ...MTr,Tc of size Br √ó Bc each.
for 1 ‚â§j ‚â§Tc do
Load Kj, Vj ‚ààRBc√ód from HBM to on-chip SRAM.
Initialize dKj = dVj = 0 ‚ààRBc√ód on SRAM.
for 1 ‚â§i ‚â§Tr do
Load Qi, Oi, dOi ‚ààRBr√ód from HBM to on-chip SRAM.
Load Mij ‚ààRBc√óBc from HBM to on-chip SRAM.
Initialize dKj = dVj = 0 ‚ààRBc√ód on SRAM.
On chip, compute Aij = [QiK‚ä§
j ] ‚äôMij ‚ààRBr√óBc.
On chip, compute dVj = dVj + A‚ä§
ijdOi ‚ààRBc√ód.
On chip, compute dAij = [dOiV‚ä§
j ] ‚äôMij ‚ààRBr√óBc.
On chip, compute dKj = dkj + dA‚ä§
ijVj ‚ààRBc√ód.
Load dQi from HBM to SRAM, then on chip, compute dQi = dKi + dAijKj ‚ààRBr√ód,
write back to HBM.
end for
Write dKj, dVj to HBM as the j-th block of dK, dV.
end for
retun dQ, dK, dV
3.2.2
Model Parallelism
To effectively carry out the large-scale pre-training of TransNormerLLM, we have focused our
efforts on system optimization from a variety of angles. It is critical to emphasize that the term
"large-scale" refers to two aspects: first, the model has a large number of parameters, and second,
the computational infrastructure has a large number of GPU nodes. These factors result in a variety
of practical challenges, including GPU memory constraints, decreased computation efficiency, slow
communication speeds, etc.. Addressing these issues is critical to the successful implementation of
the large-scale pre-training process.
We use the Fully Sharded Data Parallel (FSDP) [47] approach in our study to distribute all model
parameters, gradients, and optimizer state tensors across the entire cluster. This strategic partitioning
reduces memory occupancy on each individual GPU, optimizing memory utilization. To improve
efficiency even further, we use Activation Checkpointing [40], which reduces the number of activa-
tions cached in memory during the backward pass. Instead, when calculating the gradients, these
activations are removed and recomputed. This technique aids in more efficient computations and
resource conservation. Furthermore, we use Automatic Mixed Precision (AMP) [26] to reduce GPU
memory consumption while also accelerating computation speed. It is worth noting that throughout
our experiments, we utilize BFloat16 [19] due to its observed beneficial impact on the training
stability of large TransNormerLLM models.
In addition to the aforementioned efforts, we go deeper into system engineering optimization by
performing model parallelism on linear transformers, which is heavily inspired by Nvidia‚Äôs Megatron-
LM model parallelism [40]. Each transformer layer in the conventional transformer model consists
of a self-attention block followed by a two-layer multi-layer perceptron (MLP) block. When using
Megatron-LM model parallelism, it is used independently on these two blocks. Similarly, within
the TransNormerLLM structure, which is also made up of two main blocks, SGLU and GLA, we
perform model parallelism on each of them separately. The detailed model parallelism strategies are
shown below.
Model Parallelism on SGLU
Recall the SGLU structure in Eq. 5:
O = [(XWv) ‚äô(XWu)]Wo,
(11)
6

Algorithm 3 Origin Inference Algorithm
Input: qt, kt, vt, t = 1, . . . , n;
Output: ot, t = 1, . . . , n;
Initialize: [kv]0 = 0;
for t = 1, . . . , n do
[kv]t = [kv]t‚àí1 + ktŒª‚àítv‚ä§
t ,
ot = qtŒªt[kv]t.
end for
Algorithm 4 Robust Inference Algorithm
Input: qt, kt, vt, t = 1, . . . , n;
Output: ot, t = 1, . . . , n;
Initialize: [kv]0 = 0;
for t = 1, . . . , n do
[kv]t = Œª[kv]t‚àí1 + ktv‚ä§
t ,
ot = qt[kv]t.
end for
Its model parallel version goes with:
[O‚Ä≤
1, O‚Ä≤
2] = X[W1
v, W2
v] ‚äôX[W1
u, W2
u],
(12)
= [XW1
v, XW2
v] ‚äô[XW1
u, XW2
u],
(13)
which split the weight matrices Wv and Wu along their column and obtain an output matrix split
along its column too. Then the separated output [O1, O2] is multiplied by another matrix which is
split along its row as:
O = [O‚Ä≤
1, O‚Ä≤
2]

W1
o
W2
o

= O‚Ä≤
1W1
o + O‚Ä≤
2W2
o
(14)
This whole procedure splits three GEMMs into the SGLU blocks across multiple GPUs and introduces
only a single all-reduce operation in both the forward and backward passes, respectively.
Model Parallelism on GLA
Recall the GLA block in Eqs. 3 and 4, model parallelism on GLA as
follows:
[O1, O2] = SRMSNorm(QK‚ä§V) ‚äôU,
(15)
where:
Q = œï(X[W1
q, W2
q]) = [œï(XW1
q), œï(XW2
q)],
(16)
K = œï(X[W1
k, W2
k]) = [œï(XW1
q), œï(XW2
q)],
(17)
V = X[W1
v, W2
v], U = X[W1
u, W2
u],
(18)
Note that in implementation, we use the combined QKVU projection to improve computation
efficiency. The obtained split output matrix [O1, O2] again is multiplied by a weight matrix split
along its column axis which is similar to Eq. 14.
3.3
Robust Inference
In this section, we discuss the inference problem in TransNormerLLM. It is important to note that the
formula 1 can be decomposed into the following form:
ast = (qsŒªs expiŒ∏s)‚ä§(ktŒª‚àít expiŒ∏t).
(19)
This allows TransNormerLLM to perform inference in the form of an RNN [21]. Details of the
procedure are shown in Algorithm 3. However, it is worth noting that Œª < 1, which results in:
‚à•qsŒªs expiŒ∏s ‚à•2 = ‚à•qs‚à•2Œªs ‚Üí0, ‚à•ktŒª‚àít expiŒ∏t ‚à•2 = ‚à•kt‚à•2Œª‚àít ‚Üí‚àû,
(20)
leading to numerical precision issues.
To avoid these issues, we propose a Robust Inference Algorithm in 4. Since ‚à•qs expiŒ∏s ‚à•= ‚à•qs‚à•,
‚à•kt expiŒ∏t ‚à•= ‚à•kt‚à•, for simplicity, we will omit LRPE [34] in the subsequent discussions, consider-
ing only ast = q‚ä§
s ktŒªs‚àít.
We will use induction to prove:
[kv]t = Œª‚àít[kv]t.
(21)
Thus, both the Origin Inference Algorithm and the Robust Inference Algorithm yield the same results.
7

Base Case (n = 1):
[kv]1 = ([kv]0 + k1Œª‚àí1v‚ä§
1 )
= Œª‚àí1(k1v‚ä§
1 )
= Œª‚àí1[kv]1.
(22)
Thus, the base case is true. Let us assume the statement holds for n = m ‚àí1, i.e., [kv]m‚àí1 =
Œª‚àí(m‚àí1)[kv]m‚àí1. Then, when n = m:
[kv]m = [kv]m‚àí1 + kmŒª‚àímv‚ä§
m
= Œª‚àí(m‚àí1)[kv]m‚àí1 + kmŒª‚àímv‚ä§
m
= Œª‚àím(Œª[kv]m‚àí1 + kmv‚ä§
m)
= Œª‚àím[kv]m,
(23)
the statement holds. Therefore, by induction, the statement holds for all n ‚â•1.
4
Corpus
We gather an extensive corpus of publicly accessible text from the internet, totaling over 700TB in
size. The collected data are processed by our data preprocessing procedure as shown in Figure 2,
leaving a 6TB cleaned corpus with roughly 2 trillion tokens. We categorize our data sources to
provide better transparency and understanding. The specifics of these categories are outlined in
Table 2.
4.1
Data Preprocessing
Academic
writings
Books
Code
Web
‚Ä¶
Self-Clean
Scheme
Model-based 
Filtering
Evaluation Model
Human
Evaluation
Deduplication
Rule-based
Filtering
Training data
x N
Figure 2: Data Preprocess Procedure. The collected data undergoes a process of rule-based filtering
and deduplication, followed by our self-clean data processing strategy: model-based filtering, human
evaluation, and evaluation model. After several iterations of the above cycle, we obtain high-quality
training data at around 2T tokens.
Our data preprocessing procedure consists of three steps: 1). rule-based filtering, 2). deduplication,
and 3). a self-cleaning scheme. Before being added to the training corpus, the cleaned corpus needs
to be evaluated by humans.
Rule-based filtering
The rules we used to filter our collected data are listed as follows:
‚Ä¢ Removal of HTML Tags and URLs: The initial step in our process is the elimination of
HTML tags and web URLs from the text. This is achieved through regular expression
techniques that identify these patterns and remove them, ensuring the language model
focuses on meaningful textual content.
8

Table 2: Statistics of our corpus. For each category, we list the number of epochs performed on the
subset when training on the 2 trillion tokens, as well as the number of tokens and disk sizes. We also
list the table on the right according to the language distribution.
Dataset
Epochs
Tokens
Disk size
Academic Writings
1.53
200 B
672 GB
Books
2.49
198 B
723 GB
Code
0.44
689 B
1.4 TB
Encyclopedia
1.51
5 B
18 GB
Filtered Webpages
1.00
882 B
3.1 TB
Others
0.63
52 B
154 GB
Total
-
2026 B
6 TB
Language
Tokens
Disk size
English
743 B
2.9 TB
Chiese
555 B
1.7 TB
Code
689 B
1.4 TB
Others
39 B
89 GB
Total
2026 B
6 TB
‚Ä¢ Elimination of Useless or Abnormal Strings: Subsequently, the cleaned dataset undergoes a
second layer of refinement where strings that do not provide value, such as aberrant strings
or garbled text, are identified and excised. This process relies on predefined rules that
categorize certain string patterns as non-contributing elements.
‚Ä¢ Deduplication of Punctuation Marks: We address the problem of redundant punctuation
marks in the data. Multiple consecutive punctuation marks can distort the natural flow and
structure of sentences when training the model. We employ a rule-based system that trims
these duplications down to a single instance of each punctuation mark.
‚Ä¢ Handling Special Characters: Unusual or special characters that are not commonly part of
the language‚Äôs text corpus are identified and either removed or replaced with a standardized
representation.
‚Ä¢ Number Standardization: Numerical figures may be presented in various formats across dif-
ferent texts. These numbers are standardized into a common format to maintain consistency.
‚Ä¢ Preservation of Markdown/LaTeX Formats: While removing non-textual elements, excep-
tions are made for texts in Markdown and LaTeX formats. Given their structured nature and
ubiquitous use in academia and documentation, preserving these formats can enhance the
model‚Äôs ability to understand and generate similarly formatted text.
Deduplication
To ensure the uniqueness of our data and avert the risk of overfitting, we employ an
efficient de-duplication strategy at the document or line level using MinHash and Locality-Sensitive
Hashing (LSH) algorithms. This combination of MinHash and LSH ensures a balance between
computational efficiency and accuracy in the deduplication process, providing a robust mechanism
for data deduplication and text watermark removal.
Self-cleaning scheme
Our data self-cleaning process involves an iterative loop of the following
three steps to continuously refine and enhance the quality of our dataset. An issue of using model-
based data filters is that the filtered data will have a similar distribution as the evaluation model,
which may have a significant impact on the diversity of the training data. Assuming that the majority
of the pre-processed data is of high quality, we can train an evaluation model on the entire set of
pre-processed data, and the model will automatically smooth the data manifold distribution and outlet
low-quality data while retaining the majority of the diversities.
The self-cleaning scheme unfolds as follows:
‚Ä¢ Evaluation Model: We train a 385M model on the pre-processed corpus to act as a data
quality filter.
‚Ä¢ Model-Based Data Filtering: We use the evaluation model to assess each piece of data with
perplexity. Only data achieving a score above a certain threshold is preserved for the next
step. Low-quality data are weeded out at this stage.
‚Ä¢ Human Evaluation: We sample a small portion of the filtered data and manually evaluate
the quality.
These steps are repeated in cycles, with each iteration improving the overall quality of the data and
ensuring the resulting model is trained on relevant, high-quality text. This self-cleaning process
provides a robust mechanism for maintaining data integrity, thereby enhancing the performance of
the resulting language model.
9

4.2
Tokenization
We tokenize the data with the Byte-Pair Encoding (BPE) algorithm. Notably, to enhance compatibility
with Chinese language content, a significant number of common and uncommon Chinese characters
have been incorporated into our vocabulary. In cases where vocabulary items are not present in the
dictionary, the words are broken down into their constituent UTF-8 characters. This strategy ensures
comprehensive coverage and flexibility for diverse linguistic input during model training.
5
Experiments
We use PyTorch [27] and Trition [42] to implement TransNormerLLM in Metaseq framework [46].
Our model is trained using Adam optimizer [23], and we employ FSDP to efficiently scale our model
to NVIDIA A100 80G clusters. We additionally leverage the model parallel as appropriate to optimize
performance. In ablation studies, all models are trained on a sampled corpus from our corpus with
300B tokens. In order to reduce the fluctuation of Losses and PPLs in the tables below, we compute
the average Losses and PPLs of the last 1k iterations as the final metrics.
5.1
Architecture Ablations
Transformer vs TransNormerLLM
We carried out a meticulous series of comparative tests
between our TransNormerLLM and Transformer, spanning over an array of disparate sizes. The com-
parative performance of these models is clearly illustrated in Table 3. Under identical configurations,
it becomes evident that our TransNormerLLM exhibits a superior performance profile compared to
Transformer. We observed that TransNormerLLM outperformed Transformer by a remarkable 5%
at the size of 385M. More importantly, as the size reached 1B, this superiority became even more
pronounced, with an advantage of 9% for TransNormerLLM over Transformer.
Table 3: Transformer vs TransNormerLLM. TransNormerLLM performs better than Transformer
in size of 385M and 1B under identical configurations by 5% and 9%, respectively.
Model Size
385M
1B
Method
Updates
Loss
PPL
Updates
Loss
PPL
Transformer
100K
2.362
5.16
100K
2.061
4.765
TransNormerLLM
100K
2.247
4.765
100K
1.896
3.728
Table 4: TransNormer vs TransNormerLLM.
TransNormerLLM leads the best results.
Method
Params Updates Loss
PPL
TransNormerLLM 385M
100K
2.247 4.765
TransNormer-T1
379M
100K
2.290 4.910
TransNormer-T2
379M
100K
2.274 4.858
TransNormer vs TransNormerLLM
We
compare the original TransNormer and the im-
proved TransNormerLLM and the results are
shown in Table 4. TransNormerLLM exhib-
ited an enhancement of 2% and 1% respectively,
while significantly faster.
Table 5: Positional encoding. The combination of
LRPE+LRPE-d leads the most optimal outcome.
PE Methods
Params
Updates
Loss
PPL
LRPE-d
385M
100K
2.247
4.765
APE
386M
100K
2.387
5.253
LRPE
385M
100K
2.287
4.899
Exp-Decay
385M
100K
2.267
4.834
Positional Encoding
In the positional encod-
ing experiment, we conducted a series of tests,
comparing LRPE-d, APE (absolute positional
encoding), LRPE, and Exp-Decay (exponential
decay). As evident from Table 5, our proposed
enhancement has already shown an improve-
ment over the original model. Moreover, the
final scheme demonstrated a 2% improvement
over the LRPE method.
Table 6: Ablations on decay temperature. The
results of decay temperature proved to be superior.
Temperature
Params
Updates
Loss
PPL
w/ temperature
385M
100K
2.247
4.765
w/o temperature
385M
100K
2.258
4.804
We also perform ablations on the decay tem-
perature
 1 ‚àíl
L

in Eq. 2. The perplexity of
the TransNormerLLM is reduced by adding the
decay temperature, as shown in Table 6.
10

Table 7: Ablations on gating mechanism. The
performance with the gate proved to be superior.
Gate
Params
Updates
Loss
PPL
w/ gate
385M
100K
2.247
4.765
w/o gate
379M
100K
2.263
4.820
Gating mechanism
We conduct ablation stud-
ies to examine the effect of including the gating
mechanism. As observed in Table 7, gate en-
abled the reduction of the loss value from 2.263
to 2.247.
Table 8: Ablations on GLA activation functions.
The results obtained from different activation func-
tions were virtually identical.
GLA Act
Params
Updates
Loss
PPL
1+elu
385M
100K
2.247
4.765
Swish
385M
100K
2.236
4.728
No Act
385M
100K
2.281
4.880
GLA Activation Functions
We conducted ex-
periments on the GLA (Gated Linear Units)
structure with respect to the activation function.
The outcomes from Table 8 reveal that the im-
pact of the activation function on the final results
was not substantial. Hence, taking this into ac-
count, we opted for 1 + elu in our approach.
Table 9: Ablations on GLU activation functions.
The exclusion of the activation function had no
negative impact on the results.
GLU Act
Params
Updates
Loss
PPL
No Act
385M
100K
2.247
4.765
Swish
385M
100K
2.254
4.788
GLU Activation Functions
We conduct an
experiment by removing the activation function
within the Gated Linear Units (GLU) structure.
As shown in Table 9, the results reveal that this
alteration had a negligible impact on the final
outcome. As a result, we decide to adopt the
Simple Gated Linear Units (SGLU) structure in
our final model configuration.
Table 10: Normalization functions. The devia-
tion in results among the bellowing normalization
functions is minimal.
Norm Type
Params
Updates
Loss
PPL
SRMSNorm
385M
100K
2.247
4.765
RMSNorm
385M
100K
2.247
4.766
LayerNorm
385M
100K
2.247
4.765
Normalization functions
In our study, we
conducted a series of ablation tests employing
various normalization methods including SRM-
SNorm, RMSNorm and LayerNorm. The results
indicate that there is almost no difference among
these methods when applied to TransNormer-
LLM. Nevertheless, during the course of our
testing, we revisited and re-engineered the SRM-
SNorm using Triton. As it is shown in Figure 3, empirical evidence supports that our modification
offers a significant boost in computational speed when operating with larger dimensions, compared
to the PyTorch implementation methods.
0.0
0.3
0.6
0.9
1.2
1.5
1.8
2.1
2.4
128
256
512
1024
2048
4096
8192
16384
32768
Runtime (s)
Sequence length
Forward Pass
Triton SRMSNorm
PyTorch SRMSNorm
0.0
0.6
1.2
1.8
2.4
3.0
3.6
4.2
4.8
5.4
128
256
512
1024
2048
4096
8192
16384
32768
Runtime (s)
Sequence length
Backward Pass
Triton SRMSNorm
PyTorch SRMSNorm
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
512
1024
2048
4096
8192
16384
Runtime (s)
Feature dimension
Forward Pass
Triton SRMSNorm
PyTorch SRMSNorm
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
512
1024
2048
4096
8192
16384
Runtime (s)
Feature dimension
Backward Pass
Triton SRMSNorm
PyTorch SRMSNorm
Figure 3: Performance Evaluation of SRMSNorm Implementation. The upper figures exhibit the
runtime comparison of the forward pass (left) and backward pass (right) for different sequence lengths,
with a fixed feature dimension of 3072. The lower two figures illustrate the runtime comparison for
various feature dimensions, with a fixed sequence length of 4096.
11

0
10
20
30
40
512
1024
2048
4096
8192
Memory footprint (GB)
Sequence Length
Memory Footprint
4x
0
50
100
150
200
250
512
1024
2048
4096
8192
Runtime (ms)
Sequence Length
Runtime (Fwd Pass + Bwd Pass)
2x
LightningAttn
Orignal Linear Attn
LightningAttn
Orignal Linear Attn
Figure 4: Memory and speed comparison between linear attention and lightning attention. Left:
runtime of forward + backward pass milliseconds for different sequence lengths, with a fixed feature
dimension of 2048. Right: memory footprints of forward + backward pass for different sequence
lengths, with a fixed feature dimension of 2048.
Lighning Attention
We conducted a speed and memory footprint comparison between our Light-
ning Attention compared and the baseline, which is the PyTorch implementation of the NormAt-
tention [32]. Figure 4 (left) reports the runtime in milliseconds of the forward + backward pass.
Runtime grows quadratically with respect to sequence length, but our Lightning Attention operates
significantly faster, at least 2√ó faster than the PyTorch implementation. Figure 4 (right) reports
the memory footprint of Lightning Attention compared to the baseline. The memory footprint of
Lightning Attention grows linearly with sequence length, which is up to 4√ó more memory efficient
than the baseline when the sequence length is 8192. Our proposed Lightning Attention achieves
superior efficiency.
0.0
3.0
6.0
9.0
12.0
15.0
256
512
1024
2048
4096
8192
16384 32000
Runtime (ms)
Sequence length
Inference Time
TransNormerLLM
Transformer
0.0
3.0
6.0
9.0
12.0
15.0
256
512
1024
2048
4096
8192
16384 32000
Memory (GB)
Sequence length
Inference Memory Footprint
TransNormerLLM
Transformer
Figure 5: Inference Time and Memory Footprint. Left: inference runtime of both forward and
backward passes, measured in milliseconds, across different sequence lengths. Right: memory
consumption during inference for varying sequence lengths. It is noteworthy that as the sequence
length increases, TransNormerLLM demonstrates a consistent inference time and memory footprint.
5.2
System Optimization
5.2.1
Model Parallelism
We conduct a series of experiments with a 7B TransNormerLLM model to investigate the performance
of model parallelism on TransNormerLLM in terms of speed and memory. These tests are carried
out on a single Nvidia DGX node that houses eight A100 80G GPUs linked by NVLink. In this
experiment, FSDP is enabled and Flash Attention [8] is used on the Transformer. Table 11 shows the
results for training speed and memory consumption.
It can be seen that model parallelism has a significant effect on memory conservation, as increasing
the number of partitions for the model results in lower memory consumption per GPU. Due to
NVLink constraints, we kept the dimension of model parallelism within 8 in all of our experiments.
The TransNormerLLM-7B model requires only 24.1GB of memory on a single GPU when the model
parallel size is set to 8, representing a significant memory reduction of 62.3% when compared to the
model parallel size of 1. In comparison, the Transformer-7B model consumes 28.7GB of memory
under the same configuration. While model parallelism conserves memory, it is worth noting that
training speed is only marginally reduced. TransNormerLLM consistently outperforms Transformer
by a wide margin.
12

Table 11: Model Parallelism Performance. We compare the model parallelism performance of
Transformer-7B with Flash Attention and TransNormerLLM-7B with Lightning Attention on a single
A100 node with NVLink. All experiments use a batch size of 2 and a context length of 2048.
Model
Model Parallel Size
Tokens/s
Allocated Memory/GPU
Memory Saved
Transformer-7B
1
26896.1
66.3 GB
-
2
24973.7
44.6 GB
32.7%
4
22375.8
40.2 GB
39.4%
8
19973.6
28.7 GB
56.7%
TransNormerLLM-7B
1
32048.6
64.0 GB
-
2
29750.4
41.0 GB
35.9%
4
27885.2
36.3 GB
43.3%
8
24280.0
24.1 GB
62.3%
5.2.2
Stress Tests on Model Size and Context Length
A series of stress tests are performed to assess the efficacy of the designed system optimization strategy.
The model is scaled up to 175B, which is the largest released version of the TransNormerLLM model.
However, this augmentation poses significant training challenges. We use a wide range of distributed
training techniques to effectively train such a large model, with the goal of reducing GPU memory
consumption while increasing computational and communication efficiencies. To ensure the feasibility
of training these massive TransNormerLLM models, Lightning Attention, FSDP, Model Parallelism,
AMP, and Activation Checkpointing are used. For the Transformer models, we use Flash Attention [8]
in all experiments.
Model Size
We perform training experiments on variously sized Transformer and TransNormer-
LLM models using a large-scale A100 80G GPU cluster, as shown in Table 12. To achieve the
maximum speed for various model sizes, we keep the context length constant at 2048 and increased
the batch size until we reached the GPU memory limit. TransNormerLLMs consistently outper-
form their Transformer counterparts in terms of computation speed. This observation validates the
TransNormerLLM model‚Äôs advantageous linear computational complexity, reinforcing its efficacy.
Table 12: Efficiency of training models with different sizes. For comparative purposes, we keep the
context length fixed at 2048 and increased the batch size for both transformer and TransNormerLLM
to achieve their maximum speeds without encountering out-of-memory issues.
Model
Model Size
Tokens/sec/GPU
Allocated Memory/GPU
Transformer
7B
3362.7
72.5 GB
13B
1735.6
70.6 GB
65B
318.2
73.2 GB
175B
106.2
69.5 GB
TransNormerLLM
7B
4081.0
71.9 GB
13B
2104.3
73.8 GB
65B
406.9
69.4 GB
175B
136.6
70.3 GB
Context Length
One of the strengths of TransNormerLLM lies in its utilization of linear attention
computation, which exhibits computational and storage complexities linearly correlated with the
sequence length. To validate this outstanding characteristic of TransNormerLLM, we conduct training
experiments on Transformer and TransNormerLLM models with varying parameter sizes. While
maintaining a batch size of 1, we aim to maximize the context length. All experiments run on a small
cluster with 64 A100 GPUs. The results, as presented in Table 13, demonstrate the remarkable long
context length training capability of TransNormerLLM. Under comparable computational resources,
the TransNormerLLM model exhibits the ability to train with longer context lengths compared to
conventional Transformer models and achieve higher computational speeds in the process.
13

Table 13: Maximum context length for training Transformer and TransNormerLLM. We com-
pare the maximum context lengths with different model sizes between Transformer and TransNormer-
LLM on 64 A100 80G GPUs. All experiments use a batch size of 1.
Model
Model Size
Context Length
Relative Speed
Allocated Memory/GPU
Transformer
7B
37K
1
71.1 GB
13B
24K
1
68.0 GB
65B
19K
1
73.3 GB
175B
10K
1
66.9 GB
TransNormerLLM
7B
48K
1.21
65.8 GB
13B
35K
1.23
61.0 GB
65B
23K
1.29
68.2 GB
175B
12K
1.35
63.5 GB
6
Conclusion
We introduced TransNormerLLM in this paper, an improved TransNormer that is tailored for LLMs.
Our TransNormerLLM consistently outperformed Transformers in both accuracy and efficiency and
can be effectively scaled to 175 billion parameters. Extensive ablations demonstrate the effectiveness
of our modifications and innovations in position encoding, gating mechanism, activation functions,
normalization functions, and lightning attentions. To support the training of TransNormerLLM, we
collected a large corpus that exceeds 6TB and contains over two trillion tokens. A novel self-clean
strategy was utilized to ensure data quality and relevance. Our pre-trained models will be released to
foster community advancements in efficient LLM.
References
[1] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer,
2020.
[2] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O‚ÄôBrien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff,
Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large
language models across training and scaling, 2023.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020.
[4] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers, 2019.
[5] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea
Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with
performers. In International Conference on Learning Representations, 2021.
[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker
Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,
Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,
Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei
Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling
language modeling with pathways, 2022.
[7] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
14

[8] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. FlashAttention: Fast
and memory-efficient exact attention with IO-awareness. In Advances in Neural Information
Processing Systems, 2022.
[9] Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher
R√©. Hungry hungry hippos: Towards language modeling with state space models. CoRR,
abs/2212.14052, 2022.
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171‚Äì4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics.
[11] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
Glm: General language model pretraining with autoregressive blank infilling, 2022.
[12] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri
Rudra, and Christopher R√©. Simple hardware-efficient long convolutions for sequence modeling.
CoRR, abs/2302.06646, 2023.
[13] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory
with optimal polynomial projections, 2020.
[14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R√©. On the parameterization and
initialization of diagonal state space models. In NeurIPS, 2022.
[15] Albert Gu, Karan Goel, and Christopher R√©. Efficiently modeling long sequences with structured
state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
[16] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured
state spaces. In NeurIPS, 2022.
[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia
Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent
Sifre. Training compute-optimal large language models, 2022.
[18] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time.
arXiv preprint arXiv:2202.10447, 2022.
[19] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,
Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,
et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.
[20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models, 2020.
[21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran√ßois Fleuret. Transformers
are rnns: Fast autoregressive transformers with linear attention. In International Conference on
Machine Learning, pages 5156‚Äì5165. PMLR, 2020.
[22] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training.
In International Conference on Learning Representations, 2021.
[23] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
[24] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension, 2019.
[25] Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neu-
ral architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955,
2022.
[26] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. arXiv preprint arXiv:1710.03740, 2017.
[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
15

high-performance deep learning library. In Advances in Neural Information Processing Systems
32, pages 8024‚Äì8035. Curran Associates, Inc., 2019.
[28] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb
dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.
[29] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao,
Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou,
Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna
Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind,
Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu,
and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023.
[30] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. In International Conference on Learning Representations,
2022.
[31] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng
Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh
International Conference on Learning Representations, 2023.
[32] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran
Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, pages 7025‚Äì7041, Abu Dhabi, United Arab Emirates,
Dec. 2022. Association for Computational Linguistics.
[33] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan,
Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International
Conference on Learning Representations, 2022.
[34] Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai,
Lingpeng Kong, and Yiran Zhong. Linearized relative positional encoding, 2023.
[35] Zhen Qin, Yiran Zhong, and Hui Deng. Exploring transformer extrapolation, 2023.
[36] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
Improving lan-
guage understanding by generative pre-training.
https://s3-us-west-2.amazonaws.
com/openai-assets/research-covers/language-unsupervised/language_
understanding_paper.pdf, 2018.
[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[38] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom
Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne
Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri,
Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese,
Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Suther-
land, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li,
Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazari-
dou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz,
Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de
Masson d‚ÄôAutume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark,
Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake
Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero,
Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett,
Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods,
analysis & insights from training gopher, 2022.
[39] Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari,
Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel,
Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and
Iz Beltagy. What language model to train if you have one million gpu hours?, 2022.
[40] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan
Catanzaro. Megatron-lm: Training multi-billion parameter language models using model
parallelism. arXiv preprint arXiv:1909.08053, 2019.
16

[41] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis
Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language
model for science, 2022.
[42] Philippe Tillet, Hsiang-Tsung Kung, and David D. Cox. Triton: an intermediate language
and compiler for tiled neural network computations. Proceedings of the 3rd ACM SIGPLAN
International Workshop on Machine Learning and Programming Languages, 2019.
[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023.
[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[45] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana
Ili¬¥c, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon, Matthias
Gall√©, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka
Ammanamanchi, Thomas Wang, Beno√Æt Sagot, Niklas Muennighoff, Albert Villanova del Moral,
Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu
Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren√ßon,
Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi,
Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen
Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien,
David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz√°lez Ponferrada, Efrat Levkovizh,
Ethan Kim, Eyal Bar Natan, Francesco De Toni, G√©rard Dupont, Germ√°n Kruszewski, Giada
Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac John-
son, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan
Chang, J√∂rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy,
Manan Dey, Manuel Romero Mu√±oz, Maraim Masoud, Mar√≠a Grandury, Mario ≈†a≈°ko, Max
Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Moham-
mad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier
Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo,
Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L√≥pez, Rui
Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan
Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas
Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta¬∏sar, Elizabeth Salesky,
Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud
Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari,
Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng
Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala
Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked
Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason
Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff
Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Pey-
rounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette,
Pierre Fran√ßois Lavall√©e, R√©mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith,
St√©phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia
Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur√©lie N√©v√©ol, Charles Lovering, Dan
Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli
Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova,
Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat,
Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der
Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana
Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,
Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, ZdenÀáek
Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana San-
17

tos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour,
Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu√±oz Fer-
randis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A.
Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu,
Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Ne-
jadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim
Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu,
Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy,
Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber,
Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang,
Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla,
Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi
Zhou, Chirag Jain, Chuxin Xu, Cl√©mentine Fourrier, Daniel Le√≥n Peri√±√°n, Daniel Molano, Dian
Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak,
Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas
Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa
Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P√†mies, Maria A Castillo,
Marianna Nezhurina, Mario S√§nger, Matthias Samwald, Michael Cullan, Michael Weinberg,
Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha
Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick
Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline
Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra,
Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati,
Tanmay Laud, Th√©o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh
Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye,
Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access
multilingual language model, 2023.
[46] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam
Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke
Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.
[47] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,
Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully
sharded data parallel. arXiv preprint arXiv:2304.11277, 2023.
[48] Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention
mechanism. In International Conference on Machine Learning, pages 27011‚Äì27041. PMLR,
2022.
[49] Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong. Efficient attention via control
variates. In International Conference on Learning Representations, 2023.
18

