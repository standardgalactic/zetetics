
An Introduction to Vectors, Vector Operators and Vector Analysis
Conceived as s a supplementary text and reference book for undergraduate and graduate
students of science and engineering, this book intends communicating the fundamental
concepts of vectors and their applications. It is divided into three units. The ﬁrst unit deals
with basic formulation: both conceptual and theoretical. It discusses applications of
algebraic operations, Levi-Civita notation and curvilinear coordinate systems like
spherical polar and parabolic systems. Structures and analytical geometry of curves and
surfaces is covered in detail.
The second unit discusses algebra of operators and their types. It explains the equivalence
between the algebra of vector operators and the algebra of matrices. Formulation of
eigenvectors and eigenvalues of a linear vector operator are discussed using vector algebra.
Topics including Mohr’s algorithm, Hamilton’s theorem and Euler’s theorem are discussed
in detail. The unit ends with a discussion on transformation groups, rotation group, group
of isometries and the Euclidean group, with applications to rigid displacements.
The third unit deals with vector analysis. It discusses important topics including vector
valued functions of a scalar variable, functions of vector argument (both scalar valued and
vector valued): thus covering both the scalar and vector ﬁelds and vector integration
Pramod S. Joag is presently working as CSIR Emeritus Scientist at the Savitribai Phule
University of Pune, India. For over 30 years he has been teaching classical mechanics,
quantum mechanics, electrodynamics, solid state physics, thermodynamics and statistical
mechanics at undergraduate and graduate levels. His research interests include quantum
information, and more speciﬁcally measures of quantum entanglement and quantum
discord, production of multipartite entangled states, entangled Fermion systems, models
of quantum nonlocality etc.


An Introduction to Vectors, Vector
Operators and Vector Analysis
Pramod S. Joag

4843/24, 2nd Floor, Ansari Road, Daryaganj, Delhi - 110002, India
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107154438
© Pramod S. Joag 2016
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2016
Printed in India
A catalogue record for this publication is available from the British Library
Library of Congress Cataloging-in-Publication Data
Names: Joag, Pramod S., 1951- author.
Title: An introduction to vectors, vector operators and vector analysis /
Pramod S. Joag.
Description: Daryaganj, Delhi, India : Cambridge University Press, 2016. |
Includes bibliographical references and index.
Identiﬁers: LCCN 2016019490| ISBN 9781107154438 (hardback) | ISBN 110715443X
(hardback)
Subjects: LCSH: Vector analysis. | Mathematical physics.
Classiﬁcation:LCC QC20.7.V4 J63 2016 | DDC 512/.5–dc23 LC record available at
https://lccn.loc.gov/2016019490
ISBN 978-1-107-15443-8 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication,
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

To Ela and Ninad
who made me write this document


Contents
Figures
xiii
Tables
xx
Preface
xxi
Nomenclature
xxv
I
Basic Formulation
1 Getting Concepts and Gathering Tools
3
1.1 Vectors and Scalars
3
1.2 Space and Direction
4
1.3 Representing Vectors in Space
6
1.4 Addition and its Properties
8
1.4.1 Decomposition and resolution of vectors
13
1.4.2 Examples of vector addition
16
1.5 Coordinate Systems
18
1.5.1 Right-handed (dextral) and left-handed coordinate systems
18
1.6 Linear Independence, Basis
19
1.7 Scalar and Vector Products
22
1.7.1 Scalar product
22
1.7.2 Physical applications of the scalar product
30
1.7.3 Vector product
32
1.7.4 Generalizing the geometric interpretation of the vector product
36
1.7.5 Physical applications of the vector product
38
1.8 Products of Three or More Vectors
39
1.8.1 The scalar triple product
39
1.8.2 Physical applications of the scalar triple product
43
1.8.3 The vector triple product
45
1.9 Homomorphism and Isomorphism
45

viii
Contents
1.10 Isomorphism with R3
45
1.11 A New Notation: Levi-Civita Symbols
48
1.12 Vector Identities
52
1.13 Vector Equations
54
1.14 Coordinate Systems Revisited: Curvilinear Coordinates
57
1.14.1 Spherical polar coordinates
57
1.14.2 Parabolic coordinates
60
1.15 Vector Fields
67
1.16 Orientation of a Triplet of Non-coplanar Vectors
68
1.16.1 Orientation of a plane
72
2 Vectors and Analytic Geometry
74
2.1 Straight Lines
74
2.2 Planes
83
2.3 Spheres
89
2.4 Conic Sections
90
3 Planar Vectors and Complex Numbers
94
3.1 Planar Curves on the Complex Plane
94
3.2 Comparison of Angles Between Vectors
99
3.3 Anharmonic Ratio: Parametric Equation to a Circle
100
3.4 Conformal Transforms, Inversion
101
3.5 Circle: Constant Angle and Constant Power Theorems
103
3.6 General Circle Formula
105
3.7 Circuit Impedance and Admittance
106
3.8 The Circle Transformation
107
II
Vector Operators
4 Linear Operators
115
4.1 Linear Operators on E3
115
4.1.1 Adjoint operators
117
4.1.2 Inverse of an operator
117
4.1.3 Determinant of an invertible linear operator
119
4.1.4 Non-singular operators
121
4.1.5 Examples
121
4.2 Frames and Reciprocal Frames
124
4.3 Symmetric and Skewsymmetric Operators
126
4.3.1 Vector product as a skewsymmetric operator
128

Contents
ix
4.4 Linear Operators and Matrices
129
4.5 An Equivalence Between Algebras
130
4.6 Change of Basis
132
5 Eigenvalues and Eigenvectors
134
5.1 Eigenvalues and Eigenvectors of a Linear Operator
134
5.1.1 Examples
138
5.2 Spectrum of a Symmetric Operator
141
5.3 Mohr’s Algorithm
147
5.3.1 Examples
151
5.4 Spectrum of a 2 × 2 Symmetric Matrix
155
5.5 Spectrum of Sn
156
6 Rotations and Reﬂections
158
6.1 Orthogonal Transformations: Rotations and Reﬂections
158
6.1.1 The canonical form of the orthogonal operator for reﬂection
161
6.1.2 Hamilton’s theorem
164
6.2 Canonical Form for Linear Operators
165
6.2.1 Examples
168
6.3 Rotations
170
6.3.1 Matrices representing rotations
176
6.4 Active and Passive Transformations: Symmetries
180
6.5 Euler Angles
184
6.6 Euler’s Theorem
188
7 Transformation Groups
191
7.1 Deﬁnition and Examples
191
7.2 The Rotation Group O +(3)
196
7.3 The Group of Isometries and the Euclidean Group
199
7.3.1 Chasles theorem
204
7.4 Similarities and Collineations
205
III
Vector Analysis
8 Preliminaries
215
8.1 Fundamental Notions
215
8.2 Sets and Mappings
216
8.3 Convergence of a Sequence
217
8.4 Continuous Functions
220

x
Contents
9 Vector Valued Functions of a Scalar Variable
221
9.1 Continuity and Differentiation
221
9.2 Geometry and Kinematics: Space Curves and Frenet–Seret Formulae
225
9.2.1 Normal, rectifying and osculating planes
236
9.2.2 Order of contact
238
9.2.3 The osculating circle
239
9.2.4 Natural equations of a space curve
240
9.2.5 Evolutes and involutes
243
9.3 Plane Curves
248
9.3.1 Three different parameterizations of an ellipse
248
9.3.2 Cycloids, epicycloids and trochoids
253
9.3.3 Orientation of curves
258
9.4 Chain Rule
263
9.5 Scalar Integration
263
9.6 Taylor Series
264
10 Functions with Vector Arguments
266
10.1 Need for the Directional Derivative
266
10.2 Partial Derivatives
266
10.3 Chain Rule
269
10.4 Directional Derivative and the Grad Operator
271
10.5 Taylor series
278
10.6 The Differential
279
10.7 Variation on a Curve
281
10.8 Gradient of a Potential
282
10.9 Inverse Maps and Implicit Functions
283
10.9.1 Inverse mapping theorem
284
10.9.2 Implicit function theorem
285
10.9.3 Algorithm to construct the inverse of a map
287
10.10 Differentiating Inverse Functions
291
10.11 Jacobian for the Composition of Maps
294
10.12 Surfaces
297
10.13 The Divergence and the Curl of a Vector Field
304
10.14 Differential Operators in Curvilinear Coordinates
313
11 Vector Integration
323
11.1 Line Integrals and Potential Functions
323
11.1.1 Curl of a vector ﬁeld and the line integral
341

Contents
xi
11.2 Applications of the Potential Functions
344
11.3 Area Integral
357
11.4 Multiple Integrals
360
11.4.1 Area of a planar region: Jordan measure
361
11.4.2 Double integral
363
11.4.3 Integral estimates
369
11.4.4 Triple integrals
371
11.4.5 Multiple integrals as successive single integrals
372
11.4.6 Changing variables of integration
378
11.4.7 Geometrical applications
382
11.4.8 Physical applications of multiple integrals
390
11.5 Integral Theorems of Gauss and Stokes in Two-dimensions
395
11.5.1 Integration by parts in two dimensions: Green’s theorem
400
11.6 Applications to Two-dimensional Flows
402
11.7 Orientation of a Surface
406
11.8 Surface Integrals
414
11.8.1 Divergence of a vector ﬁeld and the surface integral
419
11.9 Diveregence Theorem in Three-dimensions
421
11.10 Applications of the Gauss’s Theorem
423
11.10.1 Exercises on the divergence theorem
427
11.11 Integration by Parts and Green’s Theorem in Three-dimensions
429
11.11.1 Transformation of ∆U to spherical coordinates
430
11.12 Helmoltz Theorem
432
11.13 Stokes Theorem in Three-dimensions
434
11.13.1 Physical interpretation of Stokes theorem
436
11.13.2 Exercises on Stoke’s theorem
437
12 Odds and Ends
444
12.1 Rotational Velocity of a Rigid Body
444
12.2 3-D Harmonic Oscillator
449
12.2.1 Anisotropic oscillator
454
12.3 Projectiles and Terestrial Effects
456
12.3.1 Optimum initial conditions for netting a basket ball
456
12.3.2 Optimum angle of striking a golf ball
459
12.3.3 Effects of Coriolis force on a projectile
462
12.4 Satellites and Orbits
468
12.4.1 Geometry and dynamics: Circular motion
468
12.4.2 Hodograph of an orbit
470

xii
Contents
12.4.3 Orbit after an impulse
473
12.5 A charged Particle in Uniform Electric and Magnetic Fields
475
12.5.1 Uniform magnetic ﬁeld
475
12.5.2 Uniform electric and magnetic ﬁelds
478
12.6 Two-dimensional Steady and Irrotational Flow of an Incompressible Fluid
483
Appendices
A Matrices and Determinants
489
A.1 Matrices and Operations on them
489
A.2 Square Matrices, Inverse of a Matrix, Orthogonal Matrices
494
A.3 Linear and Multilinear Forms of Vectors
497
A.4 Alternating Multilinear Forms: Determinants
499
A.5 Principal Properties of Determinants
502
A.5.1 Determinants and systems of linear equations
505
A.5.2 Geometrical interpretation of determinants
506
B Dirac Delta Function
510
Bibliography
515
Index
517

Figures
1.1 (a) A line indicates two possible directions. A line with an arrow speciﬁes
a unique direction. (b) The angle between two directions is the amount by
which one line is to be rotated so as to coincide with the other along with
the arrows. Note the counterclockwise and clockwise rotations. (c) The angle
between two directions is measured by the arc of the unit circle swept by the
rotating direction.
5
1.2 We can choose the angle between directions ≤π by choosing which direction
is to be rotated (counterclockwise) towards which.
5
1.3 Different representations of the same vector in space
7
1.4 Shifting origin makes (a) two different vectors correspond to the same point
and (b) two different points correspond to the same vector
8
1.5 Vector addition is commutative
9
1.6 (a) Addition of two vectors (see text). (b) Vector AE equals a + b + c + d.
Draw different ﬁgures, adding a,b,c,d in different orders to check that this
vector addition is associative.
10
1.7 αa + αb = α(a + b)
10
1.8 Subtraction of vectors
11
1.9 a,b, αa + βb are in the same plane
11
1.10 An arbitrary triangle ABC formed by addition of vectors a,b; c = a+b. The
angles at the respective vertices A,B,C are denoted by the same symbols.
12
1.11 Dividing P Q in the ratio λ : (1 −λ)
13
1.12 Addition of forces to get the resultant.
17
1.13 (a) The velocity of a shell ﬁred from a moving tank relative to the ground.
(b) The southward angle θ at which the shell will ﬁre from a moving tank so
that its resulting velocity is due west.
17
1.14 (a) Left handed screw motion and (b) Left handed coordinate system.
(c) Right handed screw motion and (d) Right handed (dextral) coordinate

xiv
Figures
system. Try to construct other examples of the left and right handed
coordinate systems.
19
1.15 Scalar product is commutative. The projections of a on b and b on a give
respectively a· ˆb = |a|cosθ and b·ˆa = |b|cosθ. Multiplication on both sides
of the ﬁrst equation by |b| and the second by |a| results in the symmetrical
form a · b = |b| a · ˆb = |a| b · ˆa
23
1.16 The scalar product is distributive with respect to addition
25
1.17 Lines joining a point on a sphere with two diametrically opposite points are
perpendicular
27
1.18 Getting coordinates of a vector v (see text)
27
1.19 Euclidean distance for vectors
28
1.20 Work done on an object as it is displaced by d under the action of force F
30
1.21 Potential energy of an electric dipole p in an electric ﬁeld E
31
1.22 Torque on a current carrying coil in a magnetic ﬁeld
31
1.23 Vector product of a and b : |a×b| = |a||b|sinθ is the area of the parallelogram
as shown
32
1.24 Generalizing the geometric interpretation of vector product
36
1.25 Geometrical interpretation of coordinates of a vector product
37
1.26 Moment of a force
38
1.27 Geometric interpretation of the scalar triple product (see text)
40
1.28 The volume of a tetrahedron as the scalar triple product
41
1.29 See text
50
1.30 Spherical polar coordinates
58
1.31 Coordinate surfaces are x2 + y2 + z2 = r2 (spheres r = constant) tanθ =
(x2+y2)1/2/z (circular cones, θ = constant) tanφ = y/x (half planes φ =
constant)
60
1.32 Differential displacement corresponds to |ds| = |dr| (see text)
62
1.33 Parabolic coordinates (µ,ν,φ). Coordinate surfaces are paraboloids of
revolution (µ = constant,ν = constant) and half-planes (φ = constant)
63
1.34 Cylindrical coordinates (ρ,φ,z). Coordinate surfaces are circular cylinders
(ρ = constant), half-planes (φ = constant) intersecting on the z-axis, and
parallel planes (z = constant)
64
1.35 Prolate spheroidal coordinates (η,θ,φ). Coordinate surfaces are prolate
spheroids (η = constant), hyperboloids (θ = constant), and half-planes
(φ = constant)
65
1.36 Oblate spheroidal coordinates (η,θ,φ). Coordinate surfaces are oblate
spheroids (η = constant), hyperboloids (θ = constant), and half-planes
(φ = constant)
66

Figures
xv
1.37 (a) Positively and (b) negatively oriented triplets (a,b,c), (c) Triplet (b,a,c)
has orientation opposite to that of (a,b,c) in (a)
68
2.1 Line L with directance d = x −(ˆu · x)ˆu
76
2.2 |m| = |x × ˆu| = |d| for all x on the line L
76
2.3 See text
77
2.4 See text
78
2.5 With A and B deﬁned in Eq. (2.8) (a) |a×b| = |A|+|B| and (b) |a×b| = |B|−
|A|. These equations can be written in terms of the areas of the corresponding
triangles
80
2.6 A′ = (x −c) × (b −c) and B′ = (a −c) × (x −c) (see text)
81
2.7 Case of c parallel to x
82
2.8 See text
83
2.9 A plane positively oriented with respect to the frame (ˆi,ˆj, ˆk)
83
2.10 Every line in the plane is normal to a
85
2.11 As seen from the ﬁgure, for every point on the plane ˆk · r = constant
86
2.12 Shortest distance between two skew lines
88
2.13 A spherical triangle
89
2.14 Depicting Eq. (2.22)
91
2.15 Conics with a common focus and pericenter
92
3.1 Isomorphism between the complex plane Z and E2
95
3.2 Finding evolute of a unit circle
95
3.3 Finding
√
i
96
3.4 Finding nth roots of unity
97
3.5 z,z∗, z ± z∗
97
3.6 Depicting Eq. (3.3)
99
3.7 If D is real, z1,z2,z3,z4 lie on a circle
100
3.8 The argument ∆of ω deﬁned by Eq. (3.9)
102
3.9 Constant angle property of the circle
104
3.10 Constant power property of the circle
104
3.11 Illustrating Eq. (3.10)
105
3.12 Both impedance and admittance of this circuit are circles
106
3.13 Boucherot’s circuit
107
3.14 Four terminal network
108
3.15 Geometrical meaning of ω2
z = ω0ω∞
109
3.16 Point by point implementation of transformation Eq. (3.14)
110
3.17 An ellipse and a hyperbola
110
4.1 Inverse of a mapping. A one to one and onto map f : X 7→Y has the unique
inverse f −1 : Y 7→X
118

xvi
Figures
5.1 u ·

eiθv

=

e−iθu

· v
141
5.2 Symmetric transformation with principal values λ1 > 1 and λ2 < 1
145
5.3 An ellipsoid with semi-axes λ1,λ2,λ3
146
5.4 Parameters in Mohr’s algorithm
150
5.5 Mohr’s Circle
151
5.6 Veriﬁcation of Eq. (5.48)
152
6.1 Reﬂection of a vector in a plane
161
6.2 Reﬂection of a particle with momentum p by an unmovable plane
162
6.3 See text
164
6.4 Shear of a unit square
169
6.5 Rotation of a vector
170
6.6 Inﬁnitesimal rotation δθ of x about ˆn
171
6.7 Vectors dx and arc length ds as radius |x|sinθ is rotated through angle δθ.
As δθ 7→0 dx becomes tangent to the circle.
172
6.8 Orthonormal triad to study the action of the rotation operator
174
6.9 Equivalent rotations: One counterclockwise and the other clockwise
178
6.10 Composition of rotations. Rotations do not commute.
180
6.11 Active and passive transformations
182
6.12 Euler angles
184
6.13 Rotations corresponding to Euler angles
186
6.14 Roll, pitch and yaw
187
7.1 (a) Symmetry elements of an equilateral triangle i) Reﬂections in three planes
shown by ⊥bisectors of sides. ii) Rotations through 2π/3,4π/3 and 2π
(= identity) about the axis ⊥to the plane of the triangle passing through the
center. (b) Isomorphism with S3 (see text).
194
7.2 (a) Symmetry elements of a square (group D4) i) Reﬂections in planes
through the diagonal and bisectors of the opposite sides. ii) Rotations about
the axis through the center and ⊥to the square by angles π/2,pi,3π/2 and
2π (= identity). (b) D4 is isomorphic with a subgroup of S4 (see text).
195
7.3 Translation of a physical object by a
199
7.4 A rigid displacement is the composite of a rotation and a translation. The
translation vector a need not be in the plane of rotation.
201
7.5 Equivalence of a rotation/translation in a plane to a pure rotation
203
8.1 A converging sequence in E3
218
9.1 Geometry of the derivative
222
9.2 Parameterization by arc length
226
9.3 The Osculating circle
228
9.4 Curvature of a planar curve
229

Figures
xvii
9.5 A possible path of the satellite
235
9.6 Projections of a space curve on the coordinate planes of a moving trihedral
242
9.7 A construction for ﬁnding the equation of an involute C2 for a given evolute
C1 and vice versa
244
9.8 Construction of a evolute-involute pair
245
9.9 Finding the evolute of an involute
247
9.10 Ellipse
249
9.11 Parameters relative to foci
250
9.12 (a) Drawing ellipse with a pencil and a string (b) Semilatus rectum
(c) Polar coordinates relative to a focus
251
9.13 Cycloid
254
9.14 Epicycloid. Vectors are (i) : c, (ii) : a, (iii) : a + c, (iv) : −R(t, ˆn)c, (v) :
R(t, ˆn)(a + c), (vi) : R( a
ct, ˆn)(−R(t, ˆn)c), (vii) : x(t)
255
9.15 Cardioid
256
9.16 Hypocycloid
256
9.17 A point P on the rim of a circle rolling inside a circle of twice the radius
describes a straight line segment
257
9.18 Trochoid
258
9.19 A curve with a loop
259
9.20 Positive sense of traversing a closed curve
259
9.21 Positive and negative sides of an oriented arc
260
9.22 Orientated simple closed curve
260
9.23 Orientation of a curve with loops
261
9.24 Positive direction of the tangent and the normal
262
9.25 (a) A convex function with positive curvature, and (b) a concave function
with negative curvature
262
10.1 Sections of u = f (x,y)
267
10.2 Mapping polar to cartesian coordinates
277
10.3 The gradient vector is orthogonal to the equipotential at every point
282
10.4 Neighborhood of point (a,b) on f (x,y) = c is locally given by the implicit
function y = f (x)
286
10.5 Stereographic projection of the sphere
299
10.6 (a) Hyperboloid of one sheet and (b) Hyperboloid of two sheets
300
10.7 Creation of torus by the rotation of a circle
303
10.8 Vector ﬁelds given by (a) va (b) vb (c) vc as deﬁned in this exercise
305
10.9 Illustrating curl of a vector ﬁeld
307
10.10 Various cases of ﬁeld curling around a point
307

xviii
Figures
10.11 The Network of coordinate lines and coordinate surfaces at any arbitary
point, deﬁning a curvilinear coordinate system
314
10.12 (a) Evaluating x · da (b) Flux through the opposite faces of a volume element
318
10.13 Circulation around a loop
320
11.1 Deﬁning the line integral
323
11.2 x(t) = costˆi + sintˆj
325
11.3 A circular helix
326
11.4 In carrying a test charge from a to b the same work is done along either path
326
11.5 Line integral over a unit circle
328
11.6 Line integral around a simple closed curve as the sum of the line integrals
over its projections on the coordinate planes
330
11.7 Illustrating Eq. (11.13)
333
11.8 Each winding of the curve of integration around the z axis adds 2π to its
value
335
11.9 Illustration of a simply connected domain
337
11.10 The closed loop for integration
340
11.11 The geometry of Eq. (11.17)
342
11.12 A spherically symmetric mass distribution
347
11.13 Variables in the multipole expansion
352
11.14 Earth’s rotation affected its shape in its formative stage
354
11.15 Area integral
358
11.16 Area swept out by radius vector along a closed curve. Cross-hatched region
is swept out twice in opposite directions, so its area is zero.
359
11.17 Directed area of a self-intersecting closed plane curve.
Vertical and
horizontal lines denote areas with opposite orientation, so cross-hatched
region has zero area.
360
11.18 Interior and exterior approximations to the area of the unit disc |x| ≤1 for
n = 0,1,2 where A−
0 = 0,A−
1 = 1,A−
2 = 2,A+
2 = 4.25,A+
1 = 6,A+
0 = 12
361
11.19 Evaluation of a double integral
364
11.20 Subdivision by polar coordinate net
367
11.21 General convex region of integration
374
11.22 Non-convex region of integration
375
11.23 Circular ring as a region of integration
375
11.24 Triangle as a region of integration
376
11.25 The right triangular pyramid
378
11.26 Changing variables of integration (see text)
379
11.27 Tangent plane to the surface
385
11.28 Divergence theorem for connected regions
396

Figures
xix
11.29 ˆn deﬁnes the directional derivatives of x and y
397
11.30 Γ is the boundary of a simply connected region
400
11.31 Amount of liquid crossing segment I in time dt for uniform ﬂow of velocity v
402
11.32 (a) Flow with sink and (b) ﬂow with vortex
405
11.33 Unit vector ˆn gives the orientation of oriented surface S∗at P
408
11.34 Orientation of S with respect to u,v
411
11.35 Mobius strip
412
11.36 Illustrating Eq. (11.157)
419
11.37 Evaluation of a line integral using Stoke’s theorem
439
12.1 The rotating fan
447
12.2 Finding the instantaneous axis of rotation of a rigid body
448
12.3 Orbit of an isotropic harmonic oscillator
453
12.4 Elliptical orbit as a superposition of coplanar circular orbits
453
12.5 (a) The regions V ≤E, V1 ≤E and V2 ≤E (b) Construction of a Lissajous
ﬁgure
455
12.6 Trajectory in position space
457
12.7 Trajectory in the velocity space
457
12.8 Graphical determination of the displacement r, time of light t and ﬁnal
velocity v
458
12.9 Terrestrial Coriolis effect
462
12.10 Topocentric directional parameters
463
12.11 Net acceleration of river water
468
12.12 Eliptical orbit and Hodograph
472
12.13 Orbits after impulse
473
12.14 Earth’s atmospheric drag on a satellite circularising its orbit
474
12.15 Velocity vector precesses about ω
476
12.16 (a) Right handed helix (b) Left handed helix
477
12.17 Rotational velocity of a charge q about ω
478
12.18 Trajectory of a charged particle in uniform electric and magnetic ﬁelds
480
12.19 Directions of electric and magnetic ﬁelds for Fig. 12.18
481
12.20 Trochoids traced by a charge q when the electric and magnetic ﬁelds are
orthogonal
481
12.21 Two-dimensional ﬂow around a 90◦corner
485
12.22 Two-dimensional ﬂow around a 60◦corner
485
12.23 Two-dimensional ﬂow around a Semi-inﬁnite straight line
486
12.24 Two-dimensional ﬂow around a 2-D doublet source consisting of a source
and a sink of equal strength, at an inﬁnitesimal separation
486

Tables
2.1 Classiﬁcation of Conics and Conicoids
92
12.1 Classiﬁcation of Orbits with H , 0
471

Preface
This is a textbook on vectors at the undergraduate/advanced undergraduate level. Its
target readership is the undergraduate student of science and engineering. It may also be
used by professional scientists and engineers to brush up on various aspects of vectors and
applications of their interest. Vectors, vector operators and vector analysis form the
essential background to and the skeleton of many courses in science and engineering.
Therefore, the utility of a book which clearly builds up the theoretical structure and
applications of vectors cannot be over-emphasized. The present book is an attempt to
fulﬁll such a requirement. This book, for instance, can be used to give a course forming a
common pre-requisite for a number of science and engineering courses. In this book, I
have tried to develop the theory and applications of vectors from scratch. Although the
subject is presented in a general setting, it is developed in 3-D space using basic vector
algebra. A coordinate-free approach is taken throughout, so that all developments are free
of any particular coordinate system and apply to all coordinate systems. This approach
directly deals with vectors instead of their components or coordinates and combines these
vectors using vector algebra.
A large part of this book is inspired by the geometric algebra of multivectors that
originated in the 19th century, in the works of Grassmann and Clifford and which has had
a powerful re-incarnation with enhanced applicability in the recent works of D. Hestenes
and others [7, 10, 11]. This is one of the most general algebraic formulations of geometry
of which vectors form a special case. Keeping the multivector geometric algebra at the
backdrop makes the coordinate free approach for vectors emerge naturally. On a personal
note, the book on classical mechanics by D. Hestenes [10], which introduced me to the
multivector geometric algebra, has always been a source of joy and education for me.
I have always enjoyed solving problems from this book, many of them are included here.
In fact I have used Hestenes’ work in various places throughout the book, without using or
referring to the geometric algebra or geometric calculus.
While designing this book I was guided by two principles: A consistent development of
the subject from scratch, and also showing the beauty of the whole ediﬁce and extending
the utility of the book to the largest possible cross-section of students. The book comprises
three parts, one for each part of the title: First on the basic formulation, the second on

xxii
Preface
vector operators and the third on vector analysis. Following is the brief description of each
one of them.
The ﬁrst part gives the basic formulation, both conceptual and theoretical. The ﬁrst
chapter builds basic concepts and tools. The ﬁrst three sections are the result of my
experience with students and I have found that these matters should be explicitly dealt
with for the correct understanding of the subject. I hope that the ﬁrst three sections will
clear up the confusion and the misconceptions regarding many basic issues, in the minds
of students. I have also given the applications and examples of every algebraic operation,
starting from vector addition. Levi-Civita notation is introduced in detail and used to get
the vector identities. The metric space structure is introduced and used to understand
vectors in the context of the physical quantities they represent. Apart from the essential
structures like basis, dimension, coordinate systems and the consequences of linearity, the
curvilinear coordinate systems like spherical polar and parabolic systems are developed
systematically. Vector ﬁelds are deﬁned and their basic structure is given. The orientation
of a linearly independent triplet of vectors is then discussed, also including the orientation
of a triplet relative to a coordinate system and the related concept of the orientation of a
plane, which is later used to understand the orientation of a surface. The second chapter
deals with the analytical geometry of curves and surfaces emphasizing vector methods.
The third chapter uses complex algebra for manipulating planar vectors and for the
description and transformations of the plane curves. In this chapter I follow the treatment
by Zwikker [26] which is a complete and rigorous exposition of these issues.
The second part deals with operators on vectors. Everything about vector operators is
formulated using vector algebra (scalar and vector products) and matrices. The fourth
chapter gives the algebra of operators and various types of operators, and proves and
emphasizes the equivalence between the algebra of vector operators and the algebra of
matrices representing them. The ﬁfth chapter gives general formulation of getting
eigenvectors and eigenvalues of a linear operator on vectors using vector algebra. The
properties of the spectrum of a symmetric operator are also obtained using vector algebra.
Thus, extremely useful and general methods are accessible to the students using
elementary vector algebra. A powerful algorithm to diagonalize a positive operator acting
on a 2-D space, called Mohr’s algorithm, is then described. Mohr’s algorithm has been
routinely used by engineers via its graphical implementation, as explained in the text. The
sixth chapter develops in detail orthogonal transformations as rotations or reﬂections. The
generic forms for operators of reﬂection and rotation, as well as the matrices for the
rotation operator are obtained. The relationship between rotation and reﬂection is
established via Hamilton’s theorem. The active and passive transformations and their
connection with symmetry is discussed. The concept of broken symmetry is brieﬂy
discussed. The Euler angle construction for arbitrary rotation is then derived. The
problem of ﬁnding the axis and the angle of rotation corresponding to a given orthogonal
matrix is solved as the Euler’s theorem. The second part ends with the seventh chapter on
transformation groups and deals with the rotation group, group of isometries and the
Euclidean group, with applications to rigid displacements.

Preface
xxiii
The third part deals with vector analysis. This is a vast subject and a personal ﬂavor in
the choice of topics is inevitable. For me the guiding question was, what vector analysis a
graduating student in science and engineering must have ? Again, the variety of answers to
this question is limited only by the number of people addressing it. Thus, the third part
gives my version of the answer to this question and the resulting vector analysis. I
primarily develop the subject with geometric point of view, making as much contact with
applications as possible. My aim is to enable the student to independently read,
understand and use the literature based on vector analysis for the applications of his
interest. Whether this aim is met can only be decided by the students who learn and try to
use this material. This part is divided into ﬁve (Chapters 8–12). The eighth chapter
outlines fundamental notions and preliminary start ups, and also sets the objectives. The
ninth chapter consists of the vector valued functions of a scalar variable. Theories of space
curves and of plane curves are developed from scratch with some physical applications.
This chapter ends with the integration of such functions with respect to their scalar
argument and their Taylor series expansion. The tenth chapter deals with the functions of
vector argument, both scalar valued and vector valued, thus covering both the scalar and
vector ﬁelds. Again, everything is developed from scratch, starting with the directional
derivative, partial derivatives and continuity of such functions. A part of this development
is inspired by the geometric calculus developed by D. Hestenes and others [7, 10, 11]. To
summarize, this chapter consists of different forms of derivatives of these and inverse
functions, and their geometric/physical applications. A major omission in this chapter is
that of the systematic development of differential forms, which may not be required in an
undergraduate course. The eleventh chapter concerns vector integration. This is done in
three phases: the line, the surface and the volume integral. All the standard topics are
covered, emphasizing geometric aspects and physical applications. While writing this part,
I have made use of many books, especially the book by Courant and John [5] and that by
Lang [15], for the simple reason that I have learnt my calculus from these books, and I
have no regrets about that. In particular, my treatment of multiple integrals and matrices
and determinants in Appendix A is inspired by Courant and John’s book. I ﬁnd in their
book, the unique property of building rigorous mathematics, starting from an intuitive
geometric picture. Also, I follow Grifﬁths while presenting the divergence and the curl of
vector ﬁelds, which, I think, is possibly one of the most compact and clear treatments of
this topic. The subsections 11.1.1 and 11.8.1 and a part of section 9.2 are based on ref
[22]. The twelfth and last chapter of the book presents an assorted collection of
applications involving rotational motion of a rigid body, projectile motion, satellites and
their orbits etc, illustrating coordinate-free analysis using vector techniques. This chapter,
again, is inﬂuenced by Hestenes [10].
Appendix A develops the theory of matrices and determinants emphasizing their
connection with vectors, also proving all results involving matrices and determinants used
in the text. Appendix B gives a brief introduction to Dirac delta function.
The whole book is interspersed with exercises, which form an integral part of the text.
Most of these exercises are illustrative or they explore some real life application of the
theory. Some of them point out the subtlties involved. I recommend all students to attempt

xxiv
Preface
all exercises, without looking at the solutions beforehand. When you read a solution after
an attempt to get there, you understand it better. Also, do not be miserly about drawing
ﬁgures, a ﬁgure can show you a way which thousand words may not.
I cannot end this preface without expressing my affection towards my friend and my
deceased colleague Dr Narayan Rana, who re-kindled my interest in mechanics. Long
evenings that I spent with him discussing mechanics and physics in general, sharing and
laughing at various aspects of life from a distance, are the treasures of my life. We entered
a rewarding and fruitful collaboration of writing a book on mechanics [19]. This
collaboration and Hestenes’ book [10] motivated me to formulate mechanics in a
coordinate free way using vector methods. Apart from the book by Hestenes and his other
related work, the book by V. I. Arnold on mechanics [3] has made an indelible impact on
my understanding and my global view of mechanics, although its inﬂuence is not quite
apparent in this book. I have always enjoyed discussing mechanics and physics in general
with my colleagues Rajeev Pathak, Anil Gangal, C. V. Dharmadhikari, P. Durganandini,
and Ahmad Sayeed. The present book is produced in LATEX and I thank our students,
Dinesh Mali, Mukesh Khanore and Mihir Durve for their help in drawing ﬁgures and also
as TEXperts.

Nomenclature
α,β,γ,δ Scalars
∠(a,b) Angle between vectors a,b
a,b,x,y Vectors
θ,φ,ψ,χ Angles
R Region of 3-D space/plane
LHS Left hand side
RHS Right hand side
R3 Vector space comprising ordered triplets of real numbers
E3 3-D vector space
|a|,a Magnitude of a
||a|| Norm of a
A,B Matrices
|A|,|B| Determinants
R(z), I(z) Real and imaginary parts of a complex number
CM Center of mass
µ Magnetic moment
L Magnitude of angular momentum, A linear differential form
h Angular momentum

xxvi
Nomenclature
H Speciﬁc angular momentum : Angular momentum per unit mass
M Moment of a force, Torque
B Magnetic ﬁeld
E, E Electric ﬁeld
κ Curvature
ρ Radius of curvature
p Semilatusrectum of a conic section
e Eccentricity of a conic section
m Moment of a line
R(ˆn,θ) Operator for rotation of vector x about ˆn by angle θ
U Canonical reﬂection operator, general orthogonal operator
S Similarity transformation on E3
A Afﬁne transformation, skewsymmetric transformation
J Jacobian matrix
|J|, D Jacobian determinant
E, F, G Gaussian fundamental quantities of a surface
I Moment of Inertia operator/tensor
g(x,t) Gravitational ﬁeld of a continuous body
Q Gravitational quadrupole tensor
ω, ΩRotational velocity

Part I
Basic Formulation
Models are to be used, not believed.
H. Theil (Principles of Econometrics)


1
Getting Concepts and
Gathering Tools
1.1
Vectors and Scalars
In science and engineering we come across many quantities which require both
magnitude and direction for their complete speciﬁcation, e.g., velocity, acceleration,
momentum, force, angular momentum, torque, electrical current density, electric and
magnetic ﬁelds, pressure and temperature gradients, heat ﬂow and so on. To deal with
such quantities, we need laws to represent, combine and manipulate them. Instead of
creating these laws separately for each of these quantities, it makes good sense to create a
mathematical model to set up common laws for all quantities requiring both magnitude
and direction to be speciﬁed. This idea is neither new nor alien: right from our childhood
we deal with real numbers and integers which are the mathematical objects representing a
value of ‘something’. This ‘something’ is anything which can be quantiﬁed or measured
and whose value is speciﬁed as a single entity: length, mass, time, energy, area, volume,
curvature, cash in your pocket, the size of the memory and the speed of your computer,
bank interest rates ···. The combination and manipulation of these values is effected by
combining and manipulating the corresponding real numbers. Similarly, the values of the
quantities speciﬁed by magnitude and direction are represented by vectors. A vector is
completely speciﬁed by its magnitude and direction. Note that the magnitude of a vector is
speciﬁed by a single real number ≥0, so if we wish to change only the magnitude of a
vector, we must have the facility to multiply a vector by a real number, which we call a
scalar in this context. Henceforth, in this book, by a scalar we mean a real number. Thus,
in order to develop an algebra on the set of vectors, we need to associate with it the set of
scalars and deﬁne the laws for multiplying a vector by a scalar. If we multiply a vector by
−1 we get the vector with same magnitude but opposite in direction, which, when added
to the original vector gives the zero vector, that is, a vector with zero magnitude and no
direction. Two vectors are equal if they have equal magnitudes and the same direction.

4
An Introduction to Vectors, Vector Operators and Vector Analysis
In this book we are using boldfaced letters for vectors. A symbol which is not bold, may
represent the magnitude of the corresponding vector, or a scalar.
1.2
Space and Direction
We have not attempted to formally deﬁne ‘space’ or ‘direction’ as these are the integral
parts of our experience right from birth. By space we mean the space we live in and move
around. We experience direction by our motion as well as by observing other moving
objects. We call our space three dimensional, (3-D) because given any two different
directions, we can always choose a third direction such that going through any sequence
of displacements along any two of them, we will never move along the third and also
because given any set of four different directions we can always ﬁnd a sequence of
displacements through any three of them, which will take us along the fourth. In this
book, any n-dimensional object is denoted n-D. We also assume that space is a
continuum, that is, any region of space can be divided arbitrarily and indeﬁnitely into
smaller and smaller regions. Further, we assume that space is an inert vacuum, whose sole
purpose is to make room for different physical phenomena to occur in it. We denote this
space by a symbol R3. You may wonder about this weird symbol. However, we will
understand it in due course. For the time being we just view this symbol as a short name
for our space with the above properties.
In order to incorporate the concept of direction in our model, we note that any straight
line in space speciﬁes two directions, each by the sense in which the line is traversed. In
order to pick one of these two directions, we may put an arrow-head on the line, pointing
in the direction we want to indicate. Thus, a straight line with an arrow is our ﬁrst model
for specifying direction in space (see Fig. 1.1(a)). We will reﬁne it shortly. Note that if we
parallelly transport a line with an arrow, (that is, the transported line is always parallel to
the original one), it indicates the same direction. Thus, two different directions in space
correspond to two intersecting straight lines with arrows appropriately placed on them.
One of these directions (which we call ‘reference direction’) can be reached from the other
by rotating the other direction about the line normal to the plane containing the two
intersecting lines and passing through the point of intersection, until both, the lines and
the arrows, coincide (see Fig. 1.1(b)). The angular advance made by the rotating line is
simply the angle between the two directions. This angle can be measured by drawing a
circle of radius r in the plane of two intersecting lines with its center at the point of
intersection and measuring the length of the arc of this circle, say S, swept by the rotating
line. The angle θ swept by the rotating line is then given by
S = rθ.
Any arbitrary circle drawn in the speciﬁed plane can be used to get the value of angle θ via
the above equation (θ = S/r). In other words, the radius r is arbitrary. It is convenient
to choose a unit circle, that is, a circle with radius unity, (r = 1), so that the arc-length
and the angle swept by the rotating line are numerically equal (see Fig. 1.1(c)). Such a arc-
length measure of angle is called ‘radian measure’. Since the length of the circumference

Getting Concepts and Gathering Tools
5
of a unit circle is 2π, the angle corresponding to one complete rotation is 2π. The angle
corresponding to half the circumference is π and so on.
This procedure still leaves an ambiguity in deﬁning the angle between two directions.
We can rotate one of the directions (so as to coincide with the other direction) in two ways.
The sense of one rotation is reverse to that of the other. Each of these rotations correspond
to different angles, say θ and 2π −θ (see Fig. 1.1(b)). Which of these rotations do we
choose? We place a clock with its center at the point of intersection of the two lines so as to
view it from the top. We then choose the rotation in the sense opposite to that of the hands
of the clock. This is called counterclockwise rotation.
Fig. 1.1
(a) A line indicates two possible directions. A line with an arrow speciﬁes
a unique direction. (b) The angle between two directions is the amount
by which one line is to be rotated so as to coincide with the other
along with the arrows. Note the counterclockwise and clockwise rotations.
(c) The angle between two directions is measured by the arc of the unit
circle swept by the rotating direction.
The angle swept by a counterclockwise rotation is taken to be positive, while the angle swept
by a clockwise rotation is negative. Note that we can always choose the angle between two
directions to be ≤π by choosing which direction is to be rotated counterclockwise towards
which (see Fig. 1.2).
Fig. 1.2
We can choose the angle between directions ≤π by choosing which
direction is to be rotated (counterclockwise) towards which.

6
An Introduction to Vectors, Vector Operators and Vector Analysis
The angle between two directions is used to specify one direction relative to the other. If
you reﬂect on your experience, you will realize that the only way to specify a direction is to
specify it relative to some other reference direction which you can determine by observing
something like a magnetic needle. To appreciate this, imagine that you are on a ship sailing
in the mid-paciﬁc. Suppose that you have no device like a magnetic compass or a gyroscope
on the ship (I do not recommend this!) and that clouds block your vision of the pole star
and the other stars. Then it is impossible to tell in which direction your ship is moving.
Exercise
Consider three different non-coplanar lines1 intersecting at a point O. Take a
point P which is not on any of these three lines. Put arrows on these three lines to specify
three directions (Draw a ﬁgure). Construct a path starting at O and ending at P on which
you are moving either in or opposite to one of the three directions you have speciﬁed by
putting arrows on the three lines. Convince yourself that this is always possible. In the light
of the statements made in the ﬁrst para of this section, this exercise demonstrates that our
space is three dimensional.
□
1.3
Representing Vectors in Space
Let us now consider a physical quantity, say electric ﬁeld, whose ‘values’ are vectors. We call
such a quantity, a ‘vector quantity’. Each value is a speciﬁc vector, with given magnitude
and direction. For example, magnitude of earth’s magnetic ﬁeld can be speciﬁed as say,
0.37 gauss and the direction can be given relative to that implied by earth’s polar axis. Any
such vector can be represented in space as follows. Given the magnitude and the direction
of the vector, we draw a line in space in the direction of the vector. Then, we mark out a
segment of this line whose length is proportional to the magnitude of the vector and then
put an arrow at one of the ends of this segment to indicate the direction of the vector.
For example, to represent a vector specifying a value of the electric ﬁeld, we may choose a
length of 1 cm to correspond to the magnitude of 1 volt/meter. An electric ﬁeld vector of
magnitude x volts/meter is then represented by a segment of length x cm. Once chosen,
the same constant of proportionality must be used to represent all vectors corresponding
to the electric ﬁeld. Every vector giving a possible value of a vector quantity is completely
represented in space by the corresponding segment with an arrow at one of its ends. Of
course, the arrow can be placed anywhere on the line segment, not necessarily at one of its
ends.
The end opposite to the arrow on the vector (drawn in space) is called its base point.
Since a vector is completely speciﬁed by its magnitude and direction, it can be represented
in space at any point as its base point, because changing the base point does not change the
length or the direction of the vector. Two or more representations of the same vector based
at different points in space are to be taken as the same vector (see Fig. 1.3).
1Any number of lines all of which fall on the same plane are called coplanar. A collection of lines which are not coplanar is
called non-coplanar. A pair of intersecting lines is coplanar.

Getting Concepts and Gathering Tools
7
Fig. 1.3
Different representations of the same vector in space
Henceforth, by a vector, we will mean the representation of a value of a vector quantity in
space, which is simply proportional to the actual value of the vector quantity it represents.
This enables us to specify every vector by its length and direction, without any reference to
the physical quantity it represents. This gives us the freedom to set up the laws of combining
two or more vectors in the same sense as we set up the laws for combining real or complex
numbers without reference to the quantities they correspond to. Thus, we can develop the
theory of vectors independent of which physical quantity they represent and common to
all applications of vectors. The vectors giving the possible positions of a point particle in
space (relative to some origin) are called the position vectors. The set of all vectors is in one
to one correspondence with the set of points in space.
In some applications, a vector has to be localized in space, that is, it has to be based at
a particular point in space and cannot be parallel transported. A typical example is – the
forces applied at a given set of points on a body which is in mechanical equilibrium, so that
the net force on the body is zero, as well as the net torque about any point of the body is
zero. Here, the set of applied forces are vectors ﬁxed at the points of application. Such a
localization of vectors can be effected by assigning them to the points in space or to the
corresponding position vectors. If the number of vectors we are dealing with is ﬁnite and
small, we can assign this set of vectors to the corresponding set of position vectors by giving
an explicit table of assignment. If the vectors and the corresponding position vectors form
a continuum, then the assignment takes the form of a vector valued function of the position
vector variable, say f(x), which is called a vector ﬁeld (see section 1.15).
Apart from the vectors representing the values of vector quantities in space, we need to
draw another kind of vectors in space. These are called unit vectors whose length is always
unity. Thus, two unit vectors differ only in direction. A unit vector replaces the ‘line with an
arrow’ model to specify a direction in space. The sole purpose of a unit vector is to specify
a direction in space. In particular, the length of a unit vector does not correspond to the
magnitude of any physical quantity. We shall always denote a unit vector by a hat over it,
so that you can recognize it as a unit vector even if that is not explicitly stated. Given a
vector a, ˆa will denote the unit vector in the direction of a. Thus, every vector a , 0 can be
written as
a = |a|ˆa,
where |a| denotes the magnitude of a.

8
An Introduction to Vectors, Vector Operators and Vector Analysis
The geometric interpretation of the set of real numbers is a straight line, that is, the set
of real numbers is in one to one correspondence with the points on the line. Similarly, the
set of vectors is in one to one correspondence with the points in the three dimensional
space R3. To see this one to one correspondence, consider the set of vectors comprising all
possible values of some vector quantity. We can construct the set containing the
representatives of these vectors in space. One to one correspondence between these two
sets is obvious by construction. To transfer this correspondence to the points in R3 we
take an arbitrary point in space say O, called origin and represent every vector with O as
the base point. Since the vectors have all possible magnitudes and directions, every point
in space is at the tip of some vector based at O, representing a possible value of the vector
quantity. In this way, a unique magnitude and direction is assigned to every point in space,
establishing the one to one correspondence between the set of vectors and the set of points
in space. We could have chosen any other point, say O′ as the origin and base all vectors at
O′. This gives a new representation for each vector in the set of vectors obtained by
parallelly transporting each vector based at O to that based at O′. These two are the
representations of the same set of vectors (values of a vector quantity). However, they
generate two different one to one correspondences with the points in R3 as can be seen
from Fig. 1.4. We see that changing the origin from O to O′ makes a vector correspond to
two different points in space (or, makes a point in space correspond to two different
vectors) as we assign a vector (based at O or O′) to a point in space. Thus, changing the
origin changes the one to one correspondence between the set of vectors and the points in
space. Later, we will have a closer look at the one to one correspondence between R3 and
the set of vectors (values of a vector quantity).
Fig. 1.4
Shifting origin makes (a) two different vectors correspond to the same point
and (b) two different points correspond to the same vector
1.4
Addition and its Properties
Let us now see how to add two vectors. We will deﬁne the addition of vectors using the
representatives of the values of a vector quantity in space. This frees vector addition from
the corresponding vector quantities.
To add a and b, base the vector b at the tip of a. Then, the vector joining the base point
of a to the tip of b, in that direction, is the vector a + b. You can check that a + b = b + a
(see Fig. 1.5). Notice that the vectors a, b and a + b form a (planar) triangle and hence are
coplanar.

Getting Concepts and Gathering Tools
9
Fig. 1.5
Vector addition is commutative
The vector a + b is sometimes called the resultant of a and b. The rule of adding two or
more vectors is motivated by the net displacement of an object in space, resulting due to
many successive displacements. Thus, if we go from A to B by travelling 10km NE (vector
a) and then from B to C by travelling 6km W (vector b) the net displacement, 8km due
North from A to C (vector c), is obtained as depicted in Fig. 1.6(a), which is the same as
that given by c = a + b. Figure 1.6(b) shows the net displacement (f) after four successive
displacements (a,b,c,d) which is consistent with f = a + b + c + d.
We can now list the properties of vector addition and multiplication by a scalar.
(1) Closure
If a,b are in R3 then a + b is also in R3. That is, addition of two vectors
results in a vector.
(2) Commutativity
a + b = b + a (see Fig. 1.5).
(3) Associativity
For all vectors a,b,c in R3, a + (b + c) = (a + b) + c. Thus, while
adding three or more vectors, it does not matter which two you add ﬁrst, which two
next etc, that is, the order in which you add does not matter (see Fig. 1.6(b)).
(4) Identity
There is a unique vector 0 such that for every vector a in R3, a + 0 = a.
(5) Inverse
For every vector a , 0 in R3, there is a unique vector −a such that a +
(−a) = 0 and 0 ± 0 = 0.
To every pair α and a where α is a scalar (i.e., a real number) and a in R3 there is a vector
αa in R3. If we denote by |a| the magnitude of a, then the magnitude of αa is |α| |a|. If
α > 0, the direction of αa is the same as that of a, while if α < 0 then the direction of αa
is opposite to that of a. If α > 0, then αa is said to be the scaling of a by α. Note that α =
1/|a| produces unit vector ˆa in the direction of a. We have, for the scalar multiplication,
(1) Associativity
α(βa) = (αβ)a.
(2) Identity
1a = a.

10
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 1.6
(a) Addition of two vectors (see text). (b) Vector AE equals a+ b+ c+ d.
Draw different ﬁgures, adding a,b,c,d in different orders to check that this
vector addition is associative.
Multiplication by scalars is distributive, namely,
(3) α(a + b) = αa + αb.
(4) (α + β)a = αa + βa.
Fig. 1.7
αa + αb = α(a + b)
Note that these properties are shared by all vectors independent of the context in which
they are used and independent of which vector quantity they correspond to. As explained
in section 1.3, this is true of all the algebra of vectors and operations on vectors we develop
in this book and will not be stated explicitly again.

Getting Concepts and Gathering Tools
11
Exercise
Draw vectors a,b,c = a + b,αa,αb,C = αa + αb based at the same point A
and check using elementary geometry that αa + αb = α(a + b).
Solution
In Fig. 1.7 ∆ABC is similar to ∆ADE as two corresponding sides are parallel
and the angle at A is common. Therefore,
AE
AC = AD
AB = α|b|
|b| = α.
Substituting AC = |a + b| in the above equation, we get AE = |C| = α|a + b| = α|c|.
However, the vectors c and C are in the same direction, so that C = αc = α(a + b).
Finally, C = αa + αb giving αa + αb = α(a + b).
□
To subtract vector b from vector a we add vector −b to vector a, as shown in Fig. 1.8
a −b = a + (−b).
Fig. 1.8
Subtraction of vectors
Given any two non-zero vectors a and b, their linear combination αa + βb, (α,β scalars)
is a vector in the plane deﬁned by a and b (see Fig. 1.9). Given any set of N vectors
{x1,x2,...,xN}, their linear combination is deﬁned iteratively. The resulting vector
PN
i=1 αixi is common to the planes formed by all the pairs of vectors (PN
i=1i,k
αixi , xk), k = 1,...,N. You can verify this for N = 3.
Fig. 1.9
a,b, αa + βb are in the same plane
How are the magnitudes of non-zero vectors a, b and a + b related? We know that the
vectors a, b and a + b form a triangle. Applying the trigonometric law of cosines to this
triangle, we get (see Fig. 1.10)

12
An Introduction to Vectors, Vector Operators and Vector Analysis
|a + b|2 = |a|2 + |b|2 −2cos(∠(a,b))|a| |b|,
where ∠(a,b) is the angle between the directions of a and b. This also gives, for the angle
between a and b,
cos(∠(a,b)) = |a|2 + |b|2 −|a + b|2
2|a| |b|
.
Later, you will prove the law of cosines as an exercise. Obviously, if the vectors a and b are
perpendicular (also called orthogonal) then,
|a + b|2 = |a|2 + |b|2,
which is nothing but the statement of the Pythagorean theorem. Let us now ﬁnd the angle
made by the vector c = a + b with a say, in terms of the attributes of vectors a and b.
Here again, we make use of the fact that the triplet {a,b,c} forms a triangle. Applying the
trigonometric law of sines to this triangle, we get,
sin(∠(b,c))
a
= sin(∠(c,a))
b
= sin(∠(a,b))
c
Fig. 1.10
An arbitrary triangle ABC formed by addition of vectors a,b; c = a+b. The
angles at the respective vertices A,B,C are denoted by the same symbols.
where (a,b,c) are the magnitudes of the corresponding vectors and the angles involved are
between the directions of the vectors. Having calculated the value of c, we can use the last
equality to get,
sin(∠(c,a)) = sin(∠(a,b))b
c .
This gives ∠(c,a) as required. Again, if a and b are orthogonal, we can simplify by noting
sin(∠(c,a)) = b
c , or tan(∠(c,a)) = b
a.
Exercise
If a and b are position vectors of points P and Q, based at the origin O, then
show that the position vector x of a point X dividing P Q in the ratio λ : (1−λ) is given by
(1 −λ)a + λb.

Getting Concepts and Gathering Tools
13
For what values of λ does the position vector correspond to the point on the ray in the
direction of Q from P ?
Solution
We have, (see Fig. 1.11),
x −a = λ−−→
P Q = λ(b −a).
Fig. 1.11
Dividing P Q in the ratio λ : (1 −λ)
This gives,
x = (1 −λ)a + λb.
To answer the question, write
x = a + λ(b −a),
where b −a = −−→
P Q , to see that λ > 0.
□
Exercise
Two spheres of masses m1 and m2 are rigidly connected by a massless rod. The
system is rotating freely about its center of mass. Find the total angular momentum of the
system about CM.
Answer
Let the position vector of m1 relative to m2 be r, let the velocity of m1 relative
to CM be v and let µ =
m1m2
m1+m2 be the reduced mass of the system. Then the total angular
momentum is L = 2µr × v.
□
1.4.1
Decomposition and resolution of vectors
Just as we can add two vectors a and b to get the vector c = a + b, we can do the reverse,
namely, given a vector c we can ﬁnd two vectors a and b satisfying c = a + b. To do this,
we choose an arbitrary vector a , 0 and then get b = c −a. Thus, there are inﬁnite,

14
An Introduction to Vectors, Vector Operators and Vector Analysis
(in fact, uncountably many), pairs of vectors into which a given vector can be decomposed
or resolved. In order to resolve a given vector c into a set of N vectors we ﬁrst choose
arbitrary sets {αi , 0} and {xi , 0}, i = 1,...,N −1 of N −1 scalars and vectors
respectively and ﬁnd the vector x = PN−1
i=1 αixi. Then, we choose αN and xN to satisfy
αNxN = c −x. Thus, any vector can be resolved or decomposed in a set of N vectors in
inﬁnitely (uncountably) many ways.
Exercise
Draw ﬁgures illustrating c = αa+βb and d = αa+βb+γc for different sets
of scalars and vectors satisfying these equations.
□
Exercise
Given a vector c ﬁnd two vectors a,b of given magnitudes a,b respectively, such
that c is the resultant of a,b. When is this impossible?
Answer
Squaring both sides of b = c −a we get, for the angle between a and c, ˆc · ˆa =
cosθ = (c2 +a2 −b2)/2ca. Thus, if we draw vectors c = −−→
AC and a = −−→
AB making angle
θ = cos−1[(c2+a2−b2)/2ca] with each other at A, then the vector −−→
BC gives the required
vector b. This will fail if the vectors a,b,c cannot make a triangle, that is when a+b < c. □
Exercise
Given a vector a , 0 and N non-zero vectors xi, i = 1,...,N, no two of which
are parallel and no three of which are coplanar, show that the linear combination of {xi}’s
that equals a is unique.
Solution
We ﬁrst show it for N = 2. Let a = λx1 + µx2. Note that both the coefﬁcients
cannot be zero, otherwise a = 0. Now suppose that some other linear combination equals
a, say a = λ1x1 + µ1x2. Subtracting these two equations we get (λ −λ1)x1+ (µ −µ1)
x2 = 0. Either both of these coefﬁcients are non-zero, or both are zero, otherwise one of
the vectors x1,x2 is zero, contradicting the assumption that both are non-zero. If both
the coefﬁcients are non-zero, then the vectors x1,x2 are simply proportional to each other,
which means that they are parallel, in contradiction with the assumption that they are not.
Therefore, both the coefﬁcients (λ −λ1) and (µ −µ1) must vanish, proving that the linear
combination of x1 and x2 which equals a is unique. This also means that a given linear
combination speciﬁes a unique vector a. Now let a equal a linear combination of three
non-zero and non-coplanar2 vectors, say a = λ1x1 + λ2x2 + λ3x3. We know that the ﬁrst
two terms in this linear combination add up to a unique vector say x12 = λ1x1 + λ2x2.
Therefore, we can equivalently write this linear combination as a = x12 + λ3x3 involving
only two vectors which are not collinear because three vectors x1,x2,x3 are not coplanar,so
that we know it to be unique. This ﬁxes the coefﬁcient λ3 and hence makes the linear
combination of three vectors giving a unique. Iterating the same argument we can show
that a linear combination of non-zero, non-parallel and non-coplanar N vectors which
equals a is unique.
□
Exercise
The center of mass of the vertices of a tetrahedron P QRS (each with unit mass)
may be deﬁned as the point dividing MS in the ratio 1 : 3, where M is the center of mass
of the vertices P QR. Show that this deﬁnition is independent of the order in which the
vertices are taken and it agrees with the general deﬁnition of the center of mass.
2Note that if three vectors are non-coplanar, then no two of them can be parallel.

Getting Concepts and Gathering Tools
15
Solution
Let p,q,r,s,m be the position vectors of the points P ,Q,R,S,M respectively.
Take the origin O at the point dividing MS in the ratio 1 : 3. Thus, s = −3m. Since
m = 1/3(p + q + r), it follows that
1
4(p + q + r + s) = 0.
Thus, O is the center of mass by the general deﬁnition and clearly does not depend on the
order of the vertices.
□
Exercise
Two edges of a tetrahedron are called opposite if they have no vertex in
common. For example, the edges P Q and RS of the tetrahedron of the previous exercise
are opposite. Show that the segment joining the midpoints of opposite edges of a
tetrahedron passes through the center of mass of the vertices.
Solution
Let the edges be P Q and RS so that in the notation of the preceding solution
their midpoints have position vectors 1
2(p+q) and 1
2(r+s) respectively. From the solution
to the previous exercise 1
2(p + q) = −1
2(r + s); hence, the midpoints are collinear with
center of mass O and equidistant from it.
□
Exercise
Let a1,a2,...,an be the position vectors of n particles in space, with respect to
the origin at the center of mass G of this system, with masses m1,m2,...,mn respectively.
Show that
m1a1 + m2a2 + ··· + mnan = 0.
Solution
By deﬁnition, the left side gives the position vector of the center of mass G,
which is chosen to be zero.
□
Collinear and coplanar vectors
Two non-zero vectors are collinear if they have same or opposite directions. Two such
vectors can be made to lie on the same line because a line accommodates two opposite
directions (orientations). Obviously, two collinear vectors a and b are proportional to each
other: b = ka with |b| = |k||a| and k > 0 (k < 0) corresponds to the two vectors in the
same (opposite) direction(s). We have to differentiate between collinear vectors we have
just deﬁned and the collinear points in space which are points lying on the same line.
Three non-zero vectors are coplanar if they lie or can be made to lie in the same plane.
Three vectors are non-coplanar if they are not coplanar. Let a,b,c be three non-zero vectors
such that c can be resolved along a and b so that c is a linear combination
c = αa + βb
(1.1)
for some non-zero scalars α and β. This means, the vectors c, αa and βb form a triangle.
Since triangle is a planar ﬁgure, we conclude that vectors a,b,c are coplanar. More useful
form of Eq. (1.1) is
αa + βb + γc = 0.
(1.2)

16
An Introduction to Vectors, Vector Operators and Vector Analysis
On the other hand if a,b,c are given to be coplanar, it is possible to resolve one of them
along the other two vectors, as shown at the beginning of this subsection, so that they
satisfy Eq. (1.1) or Eq. (1.2) with α,β,γ not all zero. Thus, three non-zero vectors are
coplanar if and only if they satisfy Eq. (1.2) with two or more non-zero coefﬁcients.
It follows immediately that if three non-zero vectors satisfy Eq. (1.2) only when all the
coefﬁcients are zero, then they aught to be non-coplanar.
Exercise
Show that three points with position vectors a,b,c are collinear if and only if
there exist three non-zero scalars α,β,γ, α , ±β, such that
αa + βb + γc = 0
and
α + β + γ = 0.
Hint
From a previous exercise we can infer that if three points are collinear, the position
vector of the middle point is b = αc+γa
α+γ giving γa + αc = (α + γ)b or β + α + γ = 0.
If the given conditions are assumed, we can show that b divides the line joining a and c in
the ratio α : γ.
□
Exercise
Four points P ,Q,R,S have position vectors a,b,c,d respectively, no three of
which are collinear. Show that P ,Q,R,S are coplanar if and only if there exist four scalars
α,β,γ,δ not all zero, satisfying
αa + βb + γc + δd = 0 and α + β + γ + δ = 0.
(1.3)
Solution
Let the given P ,Q,R,S be coplanar and let the lines P Q and RS intersect at
A with position vector r, such that P A : AQ = λ/µ and RA : AS = ρ/τ. By previous
exercise this gives,
µa + λb
λ + µ
= r = τd + ρc
ρ + τ ,
or
µ
λ + µa +
λ
λ + µb −
ρ
ρ + τ c −
ρ
ρ + τ d = 0.
Replacing the coefﬁcients by α,β,γ,δ we see that conditions in Eq. (1.3) are satisﬁed. The
proof of sufﬁciency is left to you.
□
Vector methods employed to prove simple results in Euclidean geometry may be found
in [23].
1.4.2
Examples of vector addition
In this book we will draw examples from physics and engineering. This particularly suits
vectors as vectors are almost exclusively used in physics and engineering.

Getting Concepts and Gathering Tools
17
As our ﬁrst example, we calculate the acceleration of a particle of mass 0.2 kg moving
on a frictionless, horizontal and rectangular table when subjected to a force of F1 = 3N
along the breadth and F2 = 4 N along the length of the table. We know that forces are
vector quantities and the force F experienced by a particle subjected to several forces
F1,F2,...,FN is simply the sum F1 + F2 + ··· + FN. Thus, the force on the particle is
F = F1 + F2 and the magnitude of the resultant F is |F| = (F2
1 + F2
2)
1
2 = 5N and acts in a
direction making angle φ with the breadth of the table where tanφ = 4
3 (see Fig. 1.12). By
Newton’s law, F = ma, so the acceleration is in the same direction, with the magnitude
F/m = 25 m/s/s.
Fig. 1.12
Addition of forces to get the resultant.
In our second example we make use of the following principle. If an observer moving with
velocity v0 with respect to the ground sees an object moving with an apparent velocity
va, then the velocity of the object with respect to ground say vg is vg = va + v0. Thus,
consider a tank travelling due north at v0 = 10 m/s ﬁring a shell at va = 200 m/s in a
direction which appears due west to an observer on the tank. Then, the ground velocity of
the shell vg has the magnitude vg = (2002 + 102)
1
2 = 205m/s and a direction making
an angle φ north of due west where tanφ = 10/200 = 0.05 (see Fig. 1.13(a)). A more
relevant question is to ask about the direction in which the gun should be aimed so as to
hit a target due west of the tank. Here, the gun must be ﬁred in a direction θ south of due
west so that the total velocity is in the direction due west. Consulting Fig. 1.13(b) we see
that the required angle is given by sinθ = 0.05.
Fig. 1.13
(a) The velocity of a shell ﬁred from a moving tank relative to the ground.
(b) The southward angle θ at which the shell will ﬁre from a moving tank so
that its resulting velocity is due west.

18
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
A river ﬂows with a speed of 1m/s. A boy wishes to swim across the river to the
point exactly opposite to him on the other bank. He can swim relative to water at the speed
of 2m/s. At what angle θ should he aim relative to the bank?
□
Exercise
You travel from A to B with velocity 30ˆi and travel back from B to A with
velocity −70ˆi, both measured in the same units. Find your (a) average velocity (b) average
speed.
Answer
(a) 0 because the net displacement is 0. (b) Average speed = distance
travelled/time of travel = 42.
□
1.5
Coordinate Systems
Consider any three non-coplanar vectors based at a point in space. We call these vectors
ˆi, ˆj, ˆk. These vectors need not be mutually perpendicular, (orthogonal), vectors, but in
most of the applications they are taken to be so. We will take these to be unit vectors,
although this also is not necessary. Draw straight lines passing through these vectors.
These lines pass through a single common point, (the point at which three vectors are
based) and are called coordinate axes. The axes along ˆi, ˆj, ˆk are conventionally called x,y
and z axes respectively. Such a set of lines, called coordinate lines, forms a coordinate
system. The point common to all axes is the origin of the coordinate system, which is the
point corresponding to zero vector. The planes deﬁned by (ˆi, ˆj), (ˆj, ˆk) and (ˆk, ˆi) are called
the coordinate planes. Since ˆi, ˆj, ˆk are any three non-coplanar vectors, we can have
inﬁnite such triplets of vectors based at origin, each resulting in a coordinate system.
Thus, there are inﬁnite coordinate systems based at the same origin. Given a coordinate
system with its origin at some point in space, we can translate it, without rotating its axes,
to some other point in space. Similarly, we can construct a new coordinate system with its
origin displaced from that of the ﬁrst one by a vector X with some new triplet of
non-coplanar vectors based at the new origin. Note that, we can choose appropriate
translation and rotation of any one of these coordinate systems to make it coincide with
the remaining one, provided the corresponding coordinate axes can be made parallel by
rotation. This condition is obviously fulﬁlled, if the three vectors deﬁning the coordinate
systems are mutually orthogonal.
Exercise
Draw ﬁgures to illustrate everything that is said in the above paragraph.
□
1.5.1
Right-handed (dextral) and left-handed coordinate systems
Here, we restrict ourselves to coordinate systems comprising mutually orthogonal axes
which are straight lines. We call such systems rectangular Cartesian coordinate systems.
All we have said in the above paragraph still leaves a gap in the complete speciﬁcation of a
coordinate system, given the axes x, y, z. Each of the vectors ˆi, ˆj, ˆk can be in one of the
two possible directions along the corresponding axis. Which of the two possibilities we
choose for each one of them? We need a relation between ˆi, ˆj, ˆk that will ﬁx them. This is
done by relating ˆk to the sense of rotation which takes ˆi towards ˆj. Thus, the directions of

Getting Concepts and Gathering Tools
19
ˆi, ˆj, ˆk along their axes are chosen so that a rotation from ˆi to ˆj about z axis should advance
a right handed screw in the direction of ˆk along the z axis. In the last statement you can
cyclically permute ˆi −→ˆj −→ˆk −→ˆi, with the corresponding change in the axis about
which rotation takes place. The coordinate system so chosen is known as the right handed
or dextral system. As against this, we can ﬁx the ˆi, ˆj, ˆk vectors such that a rotation from ˆi
towards ˆj advances a left handed screw in the direction of ˆk. As you may know, the same
sense of rotation advances right handed and left handed screws in opposite directions.
This choice results in the left handed coordinate system. Having ﬁxed the ˆi, ˆj, ˆk vectors,
their directions are called the positive directions of the corresponding axes. All this is
depicted in Fig. 1.14.
Fig. 1.14
(a) Left handed screw motion and (b) Left handed coordinate system.
(c) Right handed screw motion and (d) Right handed (dextral) coordinate
system. Try to construct other examples of the left and right handed
coordinate systems.
1.6
Linear Independence, Basis
Why do we need the vectors ˆi, ˆj, ˆk to be non-coplanar? Because in that case, knowledge of
one or two of them cannot be used to ﬁx the remaining one(s), by combining the known
ones using vector addition and multiplication by scalars. The most general linear
combination we can prepare out of ˆi and ˆj say, is α1ˆi + α2ˆj where α1 and α2 are scalars
that we can choose. However, the vector α1ˆi+α2ˆj is always in the plane deﬁned by ˆi and ˆj

20
An Introduction to Vectors, Vector Operators and Vector Analysis
(see subsection 1.4.1, Fig. 1.9) and can never be made to coincide with the non-coplanar
vector ˆk, irrespective of the values of α1 and α2 we choose. In other words, none of the
non-coplanar vectors ˆi, ˆj, ˆk can be expressed as a linear combination of the remaining
ones. Such a set of vectors is called a set of linearly independent vectors. If a set of vectors
{v1,v2,v3}, vi , 0; i = 1,2,3 is linearly independent, then the equation
α1v1 + α2v2 + α3v3 = 0
(1.4)
is satisﬁed only when all scalars are zero. Suppose αi , 0, i ∈{1,2,3} and still satisﬁes
Eq. (1.4), (note that at least two of them have to be non-zero for this), then we can divide
Eq. (1.4) by a non-zero coefﬁcient (say α1) making the coefﬁcient of the corresponding
vector (v1) equal unity. We can then take all the other terms from LHS to RHS so that this
vector (v1) is expressed as the linear combination of the remaining ones. Thus, these two
deﬁnitions of linear independence are equivalent.
Exercise
Show that two linearly dependent vectors are parallel to each other.
□
What is most interesting is that the maximum number of linearly independent vectors we
can ﬁnd in R3 is three. In other words, any set of ≥4 vectors in R3 is linearly dependent,
that is, one or more of the vectors in this set can be expressed as a linear combination of
the remaining ones. (Compare with our discussion about the dimension of the ‘space we
live in’ in the second para of section 1.2). We identify this maximal number of linearly
independent vectors, namely three, to be the dimension of R3. (In general, the maximum
number of linearly independent vectors in an n dimensional space is n. We assume that n
is ﬁnite). Any set of three non-coplanar vectors in R3 can be used to express any vector v
in R3 as their linear combination in the following way. Consider the set {e1,e2,e3,v} of
which the ﬁrst three vectors are linearly independent, that is, non-coplanar. Since the
dimension of space is three, the above set comprising four vectors has to be linearly
dependent. Therefore, in the equation
α1e1 + α2e2 + α3e3 + α4v = 0,
not all of the scalar coefﬁcients αi, i = 1,...,4 can be zero. If α4 = 0, the equation reduces
to
α1e1 + α2e2 + α3e3 = 0
and not all these α′s can be zero. This contradicts the fact that the set {ei, i = 1,2,3} is
linearly independent. Therefore, α4 , 0 and we can write
v = −1
α4
[α1e1 + α2e2 + α3e3].
Note that we can trivially generalize this argument to any n dimensional space where the
maximum number of linearly independent vectors is n.

Getting Concepts and Gathering Tools
21
We restrict ourselves to the case where the three linearly independent vectors ˆi, ˆj, ˆk are
also mutually orthogonal, (although the following discussion in this section does
not require it) and set up the corresponding Cartesian coordinate system. Given any
vector v we want to ﬁnd three scalars vx, vy and vz such that the linear combination
vxˆi + vyˆj + vz ˆk equals v. The successive terms in this linear combination are called the
x, y, and z components of v or the components along the x,y,z axes respectively. The
scalars vx, vy, vz are the coordinates of the tip of the vector v, (or the coordinates of v for
brevity) based at the origin of the coordinate system corresponding to the mutually
orthogonal unit vectors ˆi, ˆj, ˆk we have set up. (A way to get these coordinates is given in
the next section). Given v, the scalars vx, vy, vz deﬁned by the linear combination of v in
terms of the three linearly independent vectors are unique. Suppose vx1, vy1, vz1 and vx2,
vy2, vz2 are two sets of scalars such that both the corresponding linear combinations equal
v. This means
vx1ˆi + vy1ˆj + vz1 ˆk = vx2ˆi + vy2ˆj + vz2 ˆk
or,
(vx1 −vx2)ˆi + (vy1 −vy2)ˆj + (vz1 −vz2)ˆk = 0.
Since ˆi, ˆj, ˆk are linearly independent, the last equation is satisﬁed only when
(vx1 −vx2) = 0 etc, that is, when vx1 = vx2, vy1 = vy2 and vz1 = vz2. Thus, every vector
in R3 corresponds to a unique triplet of scalars (real numbers, motivating the notation
R3) once we ﬁx the mutually orthogonal set of vectors ˆi, ˆj, ˆk. (e.g., the triplet 0,0,0
corresponds to the origin). The set of vectors {ˆi, ˆj, ˆk} has two properties: It is a maximal
set of linearly independent vectors (i.e., contains three vectors) and every vector in R3 can
be written as a unique linear combination of this set of vectors. Such a set of vectors is
called a basis. Note that we may add a vector to the set of basis vectors and express every
vector in R3 as a linear combination of this expanded set, but this linear combination can
be written as a linear combination of the basis vectors alone, because the expanded set is a
linearly dependent set of vectors. On the other hand, as we have seen above, given a
linearly independent set smaller than a basis, we can ﬁnd vectors that are not equal to any
linear combination of vectors from this smaller set. Thus, a basis (that is, a maximal set of
linearly independent vectors) is the minimal set of vectors required to span the space.
Further, there are inﬁnite possible bases as we can choose inﬁnitely many sets of three
mutually orthogonal vectors and each of them can be a basis, deﬁning the corresponding
coordinate system and the corresponding linear combinations for the vectors in R3. For
different bases (coordinate systems), the linear combinations of basis vectors which equal
a given vector are different, resulting in different coordinates for the same vector in
different coordinate systems. A basis comprising three mutually orthogonal unit vectors is
called an orthonormal basis.
Exercise
Let a = P3
k=1 αkˆik and b = P3
k=1 βkˆik with respect to the same orthonormal
basis {ˆik} k = 1,2,3. Show that a + b = P3
k=1(αk + βk)ˆik.
□

22
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
If any subset of a set of vectors is linearly dependent, then show that the whole
set is linearly dependent.
Solution
Let {xi, i = 1,...,k} out of {xi, i = 1,...,n} k < n be linearly dependent, so that
Pk
i=1 αixi = 0 such that not all αi = 0. Consider Pk
i=1 αixi + Pn
j=k+1 0xj = 0 which is
a linear combination of all the n vectors equated to zero such that not all the coefﬁcients
equal zero. Therefore, the whole set is linearly dependent.
□
From this result we conclude that every subset of a linearly independent set of vectors is
linearly independent.
Thus, any three linearly independent vectors have to be
non-coplanar, which in turn ensures that no two of them are collinear and hence no two
of them are linearly dependent.
1.7
Scalar and Vector Products
Products of vectors can be deﬁned in many ways, however, two deﬁnitions turn out to be
physically signiﬁcant. These are the so called scalar and the vector products. We learn about
them one by one. A third kind of product, called geometric product, uniﬁes scalar and
vector products, is deﬁned on a set far larger than and containing the set of vectors called
the set of multivectors and generates a beautiful algebra on this set[10, 7, 11]. However, we
do not deal with multivectors and their geometric algebra in this book, as this will cause a
long detour making us lose sight of our intended path and destinations.
1.7.1
Scalar product
We deﬁne a product of two vectors whose value is a scalar. Given two vectors a and b their
scalar or inner or dot product is denoted a · b and is given by
a · b = |a||b|cosθ = abcosθ
where a,|a| (b,|b|) is the magnitude of a (b) and θ is the angle between the directions of a
and b. (To get this angle, we have to base both the vectors at the same point). Note that
the scalar product has different signs for θ < π/2 and θ > π/2. We can always take
θ < π by choosing which direction is to be rotated counterclockwise towards which. If
one of the two vectors (say ˆb) is a unit vector, then a · ˆb is the projection of a on the
direction deﬁned by ˆb. Thus, the scalar product is the product of the projection of a on
the direction deﬁned by ˆb with the magnitude of b which is the same as the product of the
projection of b on the direction deﬁned by ˆa with the magnitude of a. This demonstrates
the obvious symmetry of the result
a · b = b · a.
This shows that the dot product is commutative (see Fig. 1.15).
The magnitude of a is also called the norm of a and denoted ||a||. Note that a·a = a2 =
||a||2 so that
||a|| = +√a · a.

Getting Concepts and Gathering Tools
23
Fig. 1.15
Scalar product is commutative. The projections of a on b and b on a
give respectively a · ˆb = |a|cosθ and b · ˆa = |b|cosθ. Multiplication on
both sides of the ﬁrst equation by |b| and the second by |a| results in the
symmetrical form a · b = |b| a · ˆb = |a| b · ˆa
If a and b are parallel or antiparallel, their scalar product evaluates to ±ab respectively. In
particular, a · a = a2.
If a and b are orthogonal,
a · b = abcos
π
2

= 0.
Thus, the scalar product of two orthogonal vectors vanishes. Conversely, a·b = 0 does not
necessarily imply either a = 0 or b = 0.
The inverse of a non-zero vector a with respect to the dot product is
a−1 = a
|a|2 ,
because a−1 · a = 1 = a · a−1. We will denote by a−1 a vector like
a
|a|2 even if it does not
occur as a factor in a dot product.
Exercise
Let a and b be two non-zero non-parallel vectors. Show that
c = a −a · b
|b|2 b = a −a · bb−1
is perpendicular to b. The vector c is called the component of a perpendicular to b.
Solution
Note that c , 0, otherwise a will be proportional to b, contradicting the
assumption that they are not parallel. Now check that b · c = 0.
□

24
An Introduction to Vectors, Vector Operators and Vector Analysis
If ˆi1,ˆi2,ˆi3 is an orthonormal basis, then
ˆik · ˆil = δkl; k,l = 1,2,3
(1.5)
where δkl is the Kronecker delta, whose value is 1 if k = l and zero if k , l.
Consider a vector v and a orthonormal basis ˆi, ˆj, ˆk so that
v = vxˆi + vyˆj + vz ˆk.
(1.6)
Dotting both sides with ˆi, ˆj, ˆk successively and using orthonormality of the basis (Eq.
(1.5)), we get,
vs = v · ˆn ; s = x,y,z ; ˆn = ˆi,ˆj, ˆk.
(1.7)
Thus, coordinates of v along x, y, z axes (namely vx,vy,vz) are given by its projections on
these axes. If we put v = 0 in Eq. (1.6), by Eq. (1.7) we get vs = 0; s = x,y,z. This means
that any orthogonal triplet of vectors is linearly independent.
Exercise
Show that n
>
1 mutually orthogonal non-zero vectors are linearly
independent.
Solution
Let {ek, k = 1,...,n} be mutually orthogonal non-zero vectors, so that
ei · ej = δij|ej|2,
where δij is the Kronecker delta. Consider the equation
α1e1 + α2e2 + ··· + αnen = 0
and take its scalar product successively with ei, i = 1,...,n to get αi = 0, i = 1,...,n.
Note that the converse is not true. We conclude that any four or more vectors in E3 cannot
be mutually perpendicular.
□
Direction cosines
Given a vector v and an orthonormal basis ˆi1,ˆi2,ˆi3 we deﬁne the quantities
ξk = v · ˆik
||v|| = vk
||v|| = ˆv · ˆik, k = 1,2,3.
If α1,α2,α3 are the angles made by the direction of v with ˆi1,ˆi2,ˆi3 respectively, then
ξk = cosαk, k = 1,2,3.
ξk are called the direction cosines of the vector v with respect to the orthonormal basis
ˆi1,ˆi2,ˆi3. Direction cosines unambiguously specify the direction of a non-zero vector. In
particular, two or more vectors having the same direction cosines with respect to some
orthonormal basis have the same directions. The only vector with all the direction cosines

Getting Concepts and Gathering Tools
25
zero and hence having no direction, is the zero vector. Note that the coordinates of a unit
vector are its direction cosines:
ˆv = ξ1ˆi1 + ξ2ˆi2 + ξ3ˆi3 ≡(ξ1,ξ2,ξ3) = (cosα1,cosα2,cosα3),
where α1,α2,α3 are the angles made by ˆv with ˆi1,ˆi2,ˆi3 respectively, or with the positive
directions of the x,y,z-axes respectively.
Distributive property
The dot product is distributive, that is,
(α1a1 + α2a2) · b = α1 a1 · b + α2 a2 · b.
This is seen from Fig. 1.16 where the projection of (α1a1 + α2a2) on b equals the sum of
the projections of α1a1 and α2a2 on b. From Fig. 1.16 we get,
α1a1 · b + α2a2 · b = (α1a1 · ˆb + α2a2 · ˆb)|b|
= ((α1a1 + α2a2) · ˆb)|b|
= (α1a1 + α2a2) · b
Note that the vectors a1,a2 need not be coplanar with b.
Fig. 1.16
The scalar product is distributive with respect to addition
We can use the distributive property of the dot product to express it in terms of the
coordinates of the factors with respect to an orthogonal Cartesian coordinate system.
Thus, let {x1,x2,x3} and {y1,y2,y3} be the coordinates of vectors a and b with respect to
an orthogonal Cartesian coordinate system. Then we have,
a · b = (x1ˆi1 + y1ˆi2 + z1ˆi3) · (x2ˆi1 + y2ˆi2 + z2ˆi3)
= x1x2 + y1y2 + z1z2

26
An Introduction to Vectors, Vector Operators and Vector Analysis
where we have used the distributive property and Eq. (1.5), that is, orthonormality of the
basis. This is the desired result. Note that for unit vectors ˆa and ˆb, we can replace the LHS
of this equation by cosθ, θ being the angle between ˆa and ˆb, and their coordinates by their
respective direction cosines, say, (λ1,µ1,ν1) and (λ2,µ2,ν2). Thus, we get
cosθ = λ1λ2 + µ1µ2 + ν1ν2.
This equation expresses the well known relation in Solid Geometry that the cosine of the
angle between two straight lines equals the sum of the products of the pairs of cosines
of the angles made by the straight lines with each of the three (mutually perpendicular)
coordinate axes.
Exercise
(law of cosines) Consider triangle ABC. We denote by A,B,C the angles
subtended at the vertices A,B,C respectively. Let a,b,c be the lengths of the sides opposite
to the vertices A,B,C respectively (see Fig. 1.10). Show that
c2 = a2 + b2 −2abcosC.
which is true for any triangle.
Hint
Let a = ⃗
BC, b = ⃗
CA, c = ⃗
BA. Then c = a+b so that c2 = c·c = (a+b)·(a+b).
Now use the distributive property and the deﬁnition of the dot product. When C = π
2 , we
recover the Pythagoras theorem.
□
Exercise
Let P and Q be diametrically opposite points and R any other point on a sphere.
Show that P R and QR are at right angles.
Solution
Take the origin at the center of the sphere and let p,q,r be the position vectors
of P ,Q,R respectively. We have,
|p|2 = |q|2 = |r|2,
each equal to the square of the radius and q = −p. Consequently, (see Fig. 1.17),
(r −p) · (r −q) = (r −p) · (r + p) = |r|2 −|p|2 = 0.
□
Polar coordinates
Let us ﬁnd a way to get the coordinates vx = v · ˆi, vy = v · ˆj, vz = v · ˆk of a vector v based
at the origin of a dextral rectangular Cartesian coordinate system. You have to refer to
Fig. 1.18 to understand whatever is said until Eq. (1.8). Let v make angle θ with the positive
direction of the z axis. this angle is called the polar angle. Take the projection of v on the
x −y plane and call the resulting vector vp. Let the angle made by vp with the positive
direction of the x axis be φ. This angle is called the azimuthal angle. The magnitude vp
of vp is v cos( π
2 −θ) = v sinθ. Project vp on x axis to get vx = vp cosφ = v sinθ cosφ.
Project vp on y axis to get vy = vp cos( π
2 −φ) = vp sinφ = v sinθ sinφ. Now project v
on z axis to get vz = v cosθ. Thus the equation,

Getting Concepts and Gathering Tools
27
Fig. 1.17
Lines joining a point on a sphere with two diametrically opposite points are
perpendicular
v = vxˆi + vyˆj + vz ˆk
can be written as
v = v sinθ cosφˆi + v sinθ sinφˆj + v cosθ ˆk.
(1.8)
If we use in Eq. (1.8) the unit vector ˆv specifying the direction of v we get
ˆv = sinθ cosφˆi + sinθ sinφˆj + cosθ ˆk.
Fig. 1.18
Getting coordinates of a vector v (see text)

28
An Introduction to Vectors, Vector Operators and Vector Analysis
since |ˆv| = 1. This equation tells us that a direction in space is completely speciﬁed ﬁxing
the values of two parameters, namely, the polar angle θ and the azimuthal angle φ.
Exercise
Show that all points on the unit sphere centered at the origin are scanned by
varying 0 ≤θ ≤π and 0 ≤φ < 2π.
Solution
Variation of φ over its range for a ﬁxed value of θ traces out a circle on the unit
sphere. As θ is varied over its range, this circle, starting from the north pole, moves over
the whole sphere to reach the south pole.
□
This exercise shows that all directions passing through a point are spanned as θ and φ vary
over their ranges.
Cauchy–Schwarz inequality
In R3 Cauchy–Schwarz inequality is almost obvious. For any two vectors x and y we have
|x · y| = ||x|| ||y|| |cosθ| ≤||x|| ||y||.
because |cosθ| ≤1. Thus,
|x · y| ≤||x|| ||y||,
which is Schwarz Inequality. Schwarz inequality is extremely useful in obtaining various
properties of vectors.
Distance between vectors
We deﬁne the (Euclidean) distance between two vectors x and y as (see Fig. 1.19),
d(x,y) = ||x −y|| = |x −y| = +
q
(x −y) · (x −y).
(1.9)
Fig. 1.19
Euclidean distance for vectors
Exercise
Check that
d2(x,y) =
3
X
k=1
(xk −yk)2,

Getting Concepts and Gathering Tools
29
where x ≡(x1,x2,x3) and y ≡(y1,y2,y3).
□
In order for d(x,y) to be called a distance (or a distance function), it should have the
following properties.
(i) d(x,y) = d(y,x).
(ii) d(x,y) ≥0 ; d(x,y) = 0 if and only if x = y.
(iii) d(x,y) ≤d(x,z) + d(z,y). This property is called triangle inequality.
(iv) d(x,y) = d(x + z,y + z).
Properties (i), (ii) and (iv) are obvious from the deﬁnition of d(x,y). We need to prove
property (iii). Here is the proof.
We observe that
||x + y||2 = (x + y) · (x + y)
= ||x||2 + 2x · y + ||y||2
≤||x||2 + 2|x · y| + ||y||2
The last expression can now be tamed using Schwarz inequality, so that
||x + y||2 ≤||x||2 + 2||x|| ||y|| + ||y||2 = (||x|| + ||y||)2.
Now replace x by x −z and y by z −y in the above inequality to get
||x −y|| ≤||x −z|| + ||z −y||
and this is the same as property (iii).
□
Exercise
Let ˆx and ˆy be two unit vectors. Show that distance between them is
√
2 −2cosθ where θ is the angle between ˆx and ˆy. If ˆx and ˆy are mutually orthogonal, the
distance between them is
√
2, consistent with the Pythagoras theorem. When x and y are
not unit vectors,
d(x,y) =
q
||x||2 + ||y||2 −2||x|| ||y||cosθ.
Hint
These results follow directly from the previous exercise in which you proved law of
cosines.
□
Exercise
Show that (a) |a + b| ≤|a| + |b| and (b) |a −b| ≥
|a| −|b|
.
Solution
Part (a) is simply the statement of triangle inequality for the triangle formed
by the vectors a,b,a + b. To get (b), we write |a| = |(a −b) + b| and apply (a) to get
|a −b| + |b| ≥|a|, or, |a −b| ≥|a| −|b|. If |a| > |b|, (b) follows. Otherwise interchange a
and b.
□

30
An Introduction to Vectors, Vector Operators and Vector Analysis
A distance function obeying conditions (i)–(iv) above is called a metric. The distance
between two vectors, as we have deﬁned via Eq. (1.9), is called the Euclidean metric. In
3-D space, it follows from its deﬁnition that the curve with minimum Euclidean distance
joining two points is a straight line. Given a smooth surface in 3-D space (see
section 10.12), the curve with ‘shortest distance’ joining two points on the surface is
constrained to lie wholly on the surface. This restriction does not allow, in general, the
curve with the shortest distance on a surface to be a straight line. However, given a smooth
surface S, we can ﬁnd a unique curve with shortest distance joining two distinct points on
the surface, called a geodesic on S. Thus, if we stretch a thread between two points on a
sphere S then this thread will lie along a great circle joining these two points and this is a
geodesic on the sphere.
1.7.2
Physical applications of the scalar product
Consider the displacement d of an object under the action of a force F. The resulting work
done by the force on the object, W , is the product of the displacement and the component
of the force in the diction of displacement or, alternatively, product of the force and the
component of the displacement in the diction of the force. From Fig. 1.20 this is
W = (F cosθ)d = Fd cosθ = F · d.
Fig. 1.20
Work done on an object as it is displaced by d under the action of force F
Exercise
A horse tows a barge along a two-path walking at 1 m/s. The tension in the rope
is 300N and the angle between the rope and the walk direction is 30◦. How much work is
done by the horse per second? (That is, ﬁnd the power produced by the horse).
□
When the work is done on the object, its energy is increased. This energy may be kinetic
(if the object accelerates) or potential (e.g., energy stored due to the change of position) or
it may be dissipated while doing work against frictional (dissipative) forces. Thus energy,
in whatever form, is a scalar quantity. In many cases the potential energy is written as the
scalar product of vector quantities. Examples are the potential energy of an electric dipole
p in an electric ﬁeld E, (see Fig. 1.21)
V = −p · E
and the potential energy of a magnetic dipole of moment µ in a magnetic ﬁeld B
V = −µ · B.

Getting Concepts and Gathering Tools
31
Fig. 1.21
Potential energy of an electric dipole p in an electric ﬁeld E
As an example, consider a square loop of wire of side L carrying a current i placed in a
magnetic ﬁeld B with the plane of the loop parallel to the ﬁeld. Two possible realizations
are depicted in Figs 1.22(a) and 1.22(b). We explicitly calculate the work done by the force
due to the magnetic ﬁeld B on the loop. We use Fig. 1.22(b). The force on a wire of length L
carrying a current i in a ﬁeld B is BiL in a direction given by Fleming’s left hand rule. Thus,
the forces acting on two of the sides of the loop give rise to the torque as in Fig. 1.22(b).
As the loop rotates, each of these sides moves a distance L/2 in the direction of the force
so that the work done is
W = 2FL/2 = BiL2
Fig. 1.22
Torque on a current carrying coil in a magnetic ﬁeld

32
An Introduction to Vectors, Vector Operators and Vector Analysis
Alternatively, we can use the expression of the potential energy involving the magnetic
moment µ. We refer to Fig. 1.22(c). The change in the potential energy V is related to the
work done on the loop by
W = −(change inV) = V(initial) −V(ﬁnal).
The magnitude of the magnetic moment µ is given by µ = iA where A is the area of the loop
and its direction is perpendicular to the loop as shown in Fig. 1.22(c). µ starts perpendicular
to B and ﬁnishes parallel to B. This gives V (initial) = 0 and we have
W = 0 −(−iAB) = BiL2
Since both the expressions for W agree, we have better conﬁdence in the formula
V = −µ · B.
1.7.3
Vector product
Given two vectors a, b, their vector or cross product is a vector (a × b) with magnitude
absinθ (a,b,θ as deﬁned while deﬁning the scalar product) and in the direction
(perpendicular to the plane of a and b) in which a right handed screw advances when a is
rotated towards b. (see Fig. 1.23). We assume that both the vectors are based at the same
point and take the angle between them to be ≤π. Thus, the magnitude of the vector
product |a × b| is the same as the area of the parallelegram with adjacent sides a and b (see
Fig. 1.23). From its deﬁnition we see that the vector product is not commutative. In fact,
b × a = −a × b,
because if we rotate a right handed screw from b to a it advances in the direction opposite
to that in which it advances when rotated from a to b.
Fig. 1.23
Vector product of a and b : |a × b| = |a||b|sinθ is the area of the
parallelogram as shown
Note that a × b = 0 whenever a and b are parallel (θ = 0) or anti-parallel (θ = π).
Both these cases are covered by requiring b = αa, α a scalar. In particular, a × a = 0. This

Getting Concepts and Gathering Tools
33
shows that a × b = 0 if and only if a , 0 and b , 0 are proportional to each other, that is,
are linearly dependent.
The vector product is not associative as can be seen from
(a × a) × b , a × (a × b)
with a , 0,b , 0, as the LHS is always zero, while RHS is never zero unless b = αa. RHS
is a vector in the plane of a, b with magnitude a2bsinθ (θ : angle between a and b).
The vector product is distributive, that is,
a × (b + c) = a × b + a × c,
and
(a + b) × c = a × c + b × c.
We will prove this result later (see subsection 1.8.1).
From its deﬁnition, it follows that multiplying one of the factors of a vector product by
a scalar amounts to multiplying the vector product itself by that scalar.
All of the above discussion leads immediately to the laws of vector multiplication:
(λa) × b = a × (λb) = λ(a × b)
a × (b + c) = a × b + a × c
(a + b) × c = a × c + b × c
b × a = −a × b.
(1.10)
Exercise
Show that (a) (a·b)2+(a×b)2 = a2b2 and (b) (a·b)2−(a×b)2 = a2b2 cos2θ
where θ is the angle between a and b. Part (a) immediately leads to Cauchy–Schwartz
inequality,
|a · b| ≤|a| |b|
with an additional piece of information that equality holds if and only if the vectors a and
b are linearly dependent.
□
Exercise
If a⊥and b⊥are the components of a and b perpendicular to a vector c then
show that (a) a × c = a⊥× c and (b) (a + b) × c = (a⊥+ b⊥) × c.
Solution
Note that c, a and a⊥are coplanar with a and a⊥on the same side of c (Draw a
ﬁgure) and a⊥×c and a×c have the same direction. Let θ be the angle between a and c and
let the angle between a and a⊥be φ. Note that θ + φ = π
2 . Therefore, for the magnitudes,
we get
a⊥= acosφ = asinθ,

34
An Introduction to Vectors, Vector Operators and Vector Analysis
leading to |a × c| = |a⊥× c| so that (a) is proved. To get (b), note that a⊥+ b⊥is the
component of a + b perpendicular to c and apply (a).
□
Consider an orthonormal basis ˆi, ˆj, ˆk forming a right handed coordinate system. From
the deﬁnitions of the vector product and a right handed coordinate system it immediately
follows that
ˆi × ˆi = ˆj × ˆj = ˆk × ˆk = 0
and
ˆi × ˆj = −ˆj × ˆi = ˆk
ˆj × ˆk = −ˆk × ˆj = ˆi
ˆk × ˆi = −ˆi × ˆk = ˆj.
(1.11)
Note that we can obtain the second and the third equation above from the ﬁrst by cyclically
permuting the vectors ˆi,ˆj, ˆk. i.e., by simultaneously changing ˆi 7→ˆj,ˆj 7→ˆk, ˆk 7→ˆi. This
useful property holds for any vector relation involving an orthonormal basis.
For a left handed coordinate system the vectors ˆi×ˆj,ˆj× ˆk, ˆk×ˆi are in opposite direction
to the basis vectors ˆk,ˆi,ˆj respectively. Therefore, Eq. (1.11) change to
ˆi × ˆj = −ˆk
ˆj × ˆk = −ˆi
ˆk × ˆi = −ˆj.
(1.12)
Equations (1.11) and (1.12) are often taken to be the deﬁnitions of the right handed and
the left handed coordinate systems respectively.
Exercise
Prove that
a × b =

a2
b2
a3
b3
 ˆσ1 −

a1
b1
a3
b3
 ˆσ2 +

a1
b1
a2
b2
 ˆσ3,
{ ˆσ1, ˆσ2, ˆσ3} is an orthonormal right handed basis and ak = a· ˆσk, bk = b· ˆσk, k = 1,2,3.
□
Exercise
Compute (a) (a + b) × (a −b); (b) (a −b) × (b −c). Give a geometrical
interpretation of these.
Hint
Think of a tetrahedron.
□

Getting Concepts and Gathering Tools
35
Exercise
If (a × b) = (c × d) and (a × c) = (b × d) then show that a −d is parallel to
b −c.
Hint
Subtract these two equations.
□
Using the distributive property of the vector product and Eq. (1.11) we can write the
vector product of two vectors in terms of their Cartesian components with respect to a
right handed coordinate system.
a × b = (axˆi + ayˆj + az ˆk) × (bxˆi + byˆj + bz ˆk)
= (aybz −byaz)ˆi + (azbx −bzax)ˆj + (axby −bxay)ˆk.
(1.13)
This expression for the vector product in component form contains no easily accessible
information about the magnitude and the direction of the vector product a × b. Also, it
depends on the coordinate system used as the components of the factors change if we use
another orthonormal basis, (that is, another coordinate system). On the other hand,
expressions involving vectors (and not their components) are invariant under the change
of coordinate system and each term in them has the same value in all coordinate systems.
Thus, if we can model a physical situation or a process using vectors and expressions
involving vectors alone, we are free of the limitation of viewing the process with reference
to a particular coordinate system and of extra baggage of transforming the expressions
from one coordinate system to the other as and when required. The most important
advantage of vectors is this coordinate-free approach they offer. In this book we will
exclusively follow this coordinate-free approach, although we will spend some time with
some of the important coordinate systems.
The components of a × b with respect to an orthonormal basis ˆi, ˆj, ˆk (and the
corresponding coordinate system) can be expressed more conveniently in the form

ˆi
ˆj
ˆk
ax
ay
az
bx
by
bz

·
Exercise
(Law of sines) Refer to the exercise where you are asked to prove law of cosines
for a triangle ABC and Fig. 1.10. Prove Eq. (1.14).
Solution
Take the vector product of c = a + b successively with vectors a,b,c to get
a × c = a × b = c × b.
Equating the magnitudes of these vectors and dividing by abc gives a relation true for any
triangle,
sinA
a
= sinB
b
= sinC
c
.
(1.14)
□

36
An Introduction to Vectors, Vector Operators and Vector Analysis
If we reﬂect a vector in the origin, it is expected to change sign. A vector which changes
sign under reﬂection in the origin (called inversion) is called a polar vector. However, this
change of sign under inversion is not carried over to the vector product of two polar vectors.
That is, if a and b are polar vectors then their vector product does not change sign under
inversion of both a and b. Due to this property a vector product of two polar vectors is
called a pseudo vector or an axial vector.
1.7.4
Generalizing the geometric interpretation of the vector product
Figure 1.23 tells us that |a × b| equals the area of the parallelogram spanned by a and b.
From the relation |a × b| = absinθ we see that the factors a,b and sinθ may be varied as
long as the product absinθ remains constant and equals |a×b|. Thus, the geometric picture
that |a × b| equals the area of a parallelogram with adjacent sides a and b, and b making
angle θ with a can be relaxed and absinθ can be taken to represent a plane area of any
shape, numerically equal to |a × b| and with its normal in the direction of a × b. To do this,
we divide the original parallelogram into a number of similar parallelograms, all copies of
one another and described in the counterclockwise sense just as the original parallelogram
as in Fig. 1.24(a). If these parallelograms are displaced in any way by sliding them in the
directions of the sides, a new ﬁgure of irregular shape is obtained such as that shown in
Fig. 1.24(b). The area of this ﬁgure is the same as that of the original parallelogram.
Fig. 1.24
Generalizing the geometric interpretation of vector product
If the number of constituent parallelograms is increased without limit, the contour of
the ﬁgure becomes a curve enclosing an area equal to that of the original parallelogram.
Note that the contours of Figs 1.24(a) and (b) are both traced in the same counterclockwise
sense. This sense is preserved however small the constituent elementary parallelograms
may be, so that in the limit an area equal to |a × b| results, with a curvilinear contour
as its boundary, traced in the counterclockwise sense. Thus, we can say that this planar
geometrical object is represented by a×b, which is a vector of scalar magnitude numerically
equal to the area of this planar ﬁgure and is at right angles to it on that side of the planar

Getting Concepts and Gathering Tools
37
ﬁgure from which the description of its contour appears counterclockwise. This marks an
important and useful generalization of the geometrical interpretation of a vector product.
Geometric interpretation of the coordinates of the vector product
Let a ≡(a1,a2,a3) , b ≡(b1,b2,b3) be two non-zero vectors with a non-zero vector
product. The individual Cartesian components of the vector product (a
×
b) ≡
(z1,z2,z3) have a geometrical interpretation related to that of (a × b) itself (see
Fig. 1.25). We have,
z3 =

a1
b1
a2
b2
 = a1b2 −a2b1.
(1.15)
Fig. 1.25
Geometrical interpretation of coordinates of a vector product
However, the right side of this equation is the magnitude of the vector product of the
vectors with Cartesian components (a1,a2,0) and (b1,b2,0), so that its absolute value
|z3| = |a1b2 −a2b1| must equal the area of the parallelogram spanned by these vectors.
The sign of z3 is determined by the direction of the corresponding vector product:
Whether it is in the positive or negative direction of the z-axis. Now the vectors (a1,a2,0)
and (b1,b2,0) are simply the projections of the vectors a and b on the xy plane. Thus, |z3|
is the area of the parallelogram obtained by projecting the parallelogram spanned by the
vectors a and b on the xy plane. (see Fig. 1.25). Similarly, |z1| and |z2| are the areas of the
projections of the parallelogram spanned by the vectors a,b on the yz and xz planes
respectively. If α1,α2,α3 are the angles made by the direction of the vector a × b with the
positive directions of the x,y,z-axes respectively, then
|zk| = |a × b||cosαk| k = 1,2,3
as shown in Fig. 1.25.

38
An Introduction to Vectors, Vector Operators and Vector Analysis
1.7.5
Physical applications of the vector product
Consider a rigid body3 which can rotate about an axis e.g., a door rotating about hinges. A
force F in a plane perpendicular to the axis acts at a point away from the axis. Then, the
moment of this force (or torque) is deﬁned as the magnitude of the force multiplied by the
perpendicular distance from the force to the axis. Referring to Fig. 1.26 we see that the
moment M has a magnitude |F|s or Fr sinθ. The direction of M is along the axis in which
a right handed screw will advance when rotated in the sense of rotation of the body (caused
by the application of F). Thus, the direction of M in Fig. 1.26 is out of the paper. All this
can be summarized in the vector equation
M = r × F.
Fig. 1.26
Moment of a force
To get the direction of M from the vector product we must base r and F at the same point
and take θ < π. In fact, the deﬁnition of torque in terms of the vector product is completely
general. The torque about any axis, not necessarily perpendicular to the plane containing r
and F is given by the component of M in the direction of the axis.
The next important physical quantity deﬁned by the vector product is angular
momentum. A particle of mass m moving with velocity v has the angular momentum L
about the origin given by
L = mr × v = r × p,
where p = mv is the linear momentum of the particle. Angular momentum is an extremely
important conserved quantity for the motion under a central force.
The force on a charge q moving with velocity v in a magnetic ﬁeld B is given by
F = qv × B.
3A rigid body is the one for which the distance between every pair of particles in it remains invariant throughout its motion.
Thus, there cannot be any relative motion between different parts of a rigid body and it cannot be deformed by applying
external forces. The motion of a rigid body is composed solely of its translation and rotation as a whole. Of course, an ideal
rigid body is a ﬁction, however, in many situations we can approximate the motion of a solid body by that of a perfect rigid
body to get the required characteristics of the actual motion.

Getting Concepts and Gathering Tools
39
The torque on a electric dipole p in an electric ﬁeld E is given by
T = p × E.
This can be easily understood by taking the dipole as two charges +q and −q separated
by a small distance 2d as in Fig. 1.21. The force on each charge has a magnitude qE. The
resulting torque is given by
M = T = 2d × (qE) = 2qd × E.
Since p = 2qd this coincides with the previous expression of T.
A similar result holds for the torque on a magnetic dipole in a magnetic ﬁeld B.
T = µ × B.
1.8
Products of Three or More Vectors
A product is a binary operation deﬁned on a set which combines two elements of a set and
returns an element of the same set. We can say that both scalar and vector products are
deﬁned on the union of the set of vectors and the set of scalars. Then, the vector and the
scalar products combine two vectors and return a vector and a scalar respectively. Any
extension of these products to more than two vectors must involve successive evaluation
of the vector and/or the scalar products of pairs of vectors drawn from a collection of
more than two vectors. Since the vector product is not associative the order in which it is
evaluated becomes important. Here, we learn about the scalar and vector triple products
involving three vectors, which yield a scalar and a vector respectively. These products
occur frequently in applications.
1.8.1
The scalar triple product
This is the scalar product of the vectors a and b × c given by a · b × c. The scalar triple
product has an elegant interpretation as the volume of the parallelepiped with edges a,b
and c based at the same origin (see Fig. 1.27). Area of the base is A = |b × c|. The volume
is V = Ah, where h is the height of the parallelepiped from the base. This height can be
expressed as h = a · ˆn, where ˆn is a unit vector normal to the base. Evidently, b × c = Aˆn
giving V = Aa · ˆn = |a · b × c|. Since this volume does not depend on which face is chosen
as the base, it follows that
a · b × c = c · a × b = b · c × a.
Thus, the scalar triple product is invariant under the cyclic permutation of its factors given
by abc ↔a →b →c →a. For example, note that
a · a × b = b · a × a = 0.
(1.16)

40
An Introduction to Vectors, Vector Operators and Vector Analysis
In fact, while keeping the cyclic order if we change the · and the × in the triple product, its
value remains the same. For example,
a · b × c = c · a × b = a × b · c,
where the last equality follows because the scalar product of two vectors is independent of
the order of the vectors. Thus, the scalar triple product depends only on the cyclic order
abc and not on the position of · and × in the product. The sign of the scalar triple product
is reversed if the cyclic order is broken by permuting two of the vectors.
Fig. 1.27
Geometric interpretation of the scalar triple product (see text)
Exercise
Show that a · b × c = 0 if and only if the vectors a,b,c are coplanar, that is, are
linearly dependent.
Answer
a·b×c = 0 if and only if the volume of the corresponding parallelepiped is zero,
if and only if a,b,c are coplanar.
□
Suppose that a,b,c are mutually orthogonal vectors forming a left handed system. Then,
the signs of a and b × c will be opposite and the value of a · b × c will be negative. The
same conclusion applies even if a,b,c are not mutually orthogonal however, b × c makes
an obtuse angle with a. In this case, the negative sign is interpreted as the negative
orientation of the volume of the parallelepiped formed by the vectors a,b,c and their
scalar triple product is said to equal the volume of their parallelepiped having negative
orientation. Thus, in general, a scalar triple product is said to equal the oriented volume of
the parallelepiped formed by its factors. The fact that the transition from a right handed to
left handed system (or vice versa) changes the sign of the scalar triple product is expressed
by saying that the scalar triple product is not a genuine scalar (whose value is invariant
under any transformation of the basis) however, a pseudo-scalar. The right handed ↔left
handed transition can be carried out by reﬂecting all the basis vectors in origin. In fact a
scalar triple product changes sign under the reﬂection of all of its factors (which form a
basis unless its value is zero) in the origin: −a · (−b × −c) = −a · b × c.
Exercise
The scalar triple product can also be geometrically interpreted as the volume
of a tetrahedron. Consider a tetrahedron OABC with one of its vertices at the origin O

Getting Concepts and Gathering Tools
41
(see Fig. 1.28 ). Show that its volume is given by 1
6[a · (b × c)] where all the vectors are as
deﬁned in Fig. 1.28.
Fig. 1.28
The volume of a tetrahedron as the scalar triple product
Solution
The required volume V is
V
= 1
3 · area of ∆OBC · AP
= 1
3 · 1
2|b × c| · |a|cosθ
= 1
6[a · (b × c)].
□
Exercise
Let a,b,c be non-coplanar. For an arbitrary non-zero vector d show that
d = [(c · d)a × b + (a · d)b × c + (b · d)c × a]/(a · b × c).
Hint
First note that the vectors a × b,b × c,c × a are non-coplanar because their scalar
triple product is not zero. Therefore, these vectors form a basis in which an arbitrary vector
d can be expanded. The coefﬁcients in this expansion are determined by taking its scalar
product successively with c,a and b.
□
Exercise
Express the scalar triple product in its component form,
a · b × c = ax(bycz −cybz) + ay(bzcx −czbx) + az(bxcy −cxby)

42
An Introduction to Vectors, Vector Operators and Vector Analysis
and write it in the determinant form
a · (b × c) =

ax
ay
az
bx
by
bz
cx
cy
cz

= det(a,b,c),
which deﬁnes det(a,b,c).4
Hint
Use Eq. (1.13) for the vector product.
□
Exercise
Let θ be the angle between the directions of vectors c and a × b. Show that
det(a,b,c) = |a × b| |c|cosθ.
□
Exercise
Show that the area of the parallelogram spanned by a,b, namely, |a×b|, can be
expressed by
|a × b|2 = (a · a)(b · b) −(a · b)(b · a) =

a · a
a · b
b · a
b · b
·
(1.17)
This determinant is called Gram determinant. Since |a × b| = 0 if and only if a,b are
dependent, we see that the gram determinant is zero if and only if a,b are dependent.
□
Exercise
Show that the determinant form of [a · (b × c)][d · (e × f)] is
[a · (b × c)][d · (e × f)] ≡

a · d
a · e
a · f
b · d
b · e
b · f
c · d
c · e
c · f

·
Hint
Treat the rows and columns forming the determinants of factors as matrices, and
ﬁnd the determinant of the product of matrix of one factor and the transpose of the matrix
of the other factor. This works because the determinant of the product of matrices is the
product of their determinants and the determinant of a matrix is invariant under transpose
of that matrix.
□
Let us now prove that the vector product is distributive. Let {a,b,c} be three arbitrary
vectors and let ˆx be an arbitrary direction. Using the fact that the scalar triple product is
invariant under the cyclic permutation of its factors, we can write
ˆx · a × (b + c) = (b + c) · (ˆx × a)
= b · (ˆx × a) + c · (ˆx × a)
= ˆx · (a × b + a × c)
4You are now advised to read the appendix on matrices and determinants, which will be used in the rest of the book.

Getting Concepts and Gathering Tools
43
Since ˆx is arbitrary, we get
a × (b + c) = a × b + a × c,
which is the desired result.
The distributive law
(a + b) × c = a × c + b × c
can be proved similarly.
A powerful notation for the scalar triple product a·(b×c) and all its cyclic permutations
is [abc]. This notation was ﬁrst used by Grassmann. However, we will use this notation very
rarely and prefer to write scalar triple product explicitly.
1.8.2
Physical applications of the scalar triple product
Reciprocal lattice of a crystal[4].
A single crystal is characterized by the periodic arrangement of atoms, ions or
molecules. This periodic arrangement is modelled by a lattice of points in space, called a
Bravais lattice. Since a crystal is a three dimensional object, we expect the lattice to have
three independent periodic arrangements in three non-coplanar directions. We can
imagine three basis vectors (called primitive vectors) say {a1,a2,a3} along three
non-coplanar directions forming the adjacent edges of a parallelepiped which is called a
primitive cell of the Bravais lattice. The Bravais lattice can be constructed by translating
this primitive cell integral number of times successively along the directions deﬁned by the
primitive vectors {a1,a2,a3}. Obviously, only those primitive cells are allowed which ﬁll in
all the space by such a translation. A vector joining the origin to any of the lattice points,
say R, is then given by
R = n1a1 + n2a2 + n3a3
where {n1,n2,n3} are integers. The whole lattice is given by the set of vectors {R}
generated by giving the triplet {n1,n2,n3} all possible integer values. Note that the volume
of a primitive cell is given by the scalar triple product a1 · a2 × a3 or any of its cyclic
permutations.
Consider a set of points {R} constituting the Bravais lattice of a crystal in which a plane
wave eik·r is excited. Here, k is a wave vector and r is an arbitrary point in the crystal. We
seek to ﬁnd the set of wave vectors {K} for which the plane wave excitation has the same
periodicity as the Bravais lattice of the crystal, that is,
eiK·(r+R) = eiK·r,
which means
eiK·R = 1
(1.18)

44
An Introduction to Vectors, Vector Operators and Vector Analysis
for all {R} in the Bravais lattice. The set of vectors {K} satisfying Eq. (1.18) is called the
reciprocal lattice of the given Bravais lattice. The corresponding Bravais lattice is called the
direct lattice.
If K1,K2 satisfy Eq. (1.18), then so do their sum and difference, which simply means
that the set of reciprocal vectors form a Bravais lattice. We show that the primitive vectors
of the reciprocal lattice are given by
b1 = 2π
a2 × a3
a1 · a2 × a3
,
b2 = 2π
a3 × a1
a1 · a2 × a3
,
b3 = 2π
a1 × a2
a1 · a2 × a3
.
(1.19)
Exercise
Show that ai · bj = 2πδij where δij = 0,1(i , j,i = j).
□
Exercise
Show that bis are not all in one plane as long as ais are not.
□
Since bis are non-coplanar, any vector k can be written as a linear combination of {bi},
k = k1b1 + k2b2 + k3b3.
If R is any direct lattice vector, then
R = n1a1 + n2a2 + n3a3
where {n1,n2,n3} are integers. Since ai · bj = 2πδij it follows that
k · R = 2π(k1n1 + k2n2 + k3n3).
We conclude from this equation that eik·R is unity for all R, only when the coefﬁcients
{k1,k2,k3} are integers (i.e., when k · R is an integral multiple of 2π). Thus, we must have,
for a reciprocal lattice vector K,
K = k1b1 + k2b2 + k3b3,
where {k1,k2,k3} are integers. Thus, reciprocal lattice is a Bravais lattice and bis can be
taken to be its primitive vectors. bis form the adjacent sides of a parallelepiped which is the
primitive cell of the reciprocal lattice.
Apart from, an enormous variety of situations in which the scalar triple product makes
its appearance, it is an important tool for the development of the theory of vector operators,
as we shall see in the next chapter. Also, the scalar triple product is the basis of a new and
powerful notation for vector algebra and calculus, namely the Levi-Civita symbols (see
section 1.11).

Getting Concepts and Gathering Tools
45
1.8.3
The vector triple product
This is the vector product of the vector a with the vector b × c, that is, a × (b × c). This
vector is in the plane containing vectors b and c because b × c is perpendicular to this
plane and a × (b × c) is perpendicular to b × c. Since the vector product is not associative
the position of the brackets in a×(b×c) is of vital importance. Generally, we do not have to
directly evaluate a vector triple product as it can be transformed into a simpler expression
via a vector identity (see section 1.12).
1.9
Homomorphism and Isomorphism
We need two algebraic concepts,
namely,
homomorphism and its special case
isomorphism between two sets, which we now deﬁne. Consider two pairs (S1,◦) and
(S2,×), where S1,S2 are sets and ◦,× are binary operations on S1 and S2 respectively. We
assume that S1,S2 are closed under the corresponding binary operations. Let ϕ : S1 7→S2
be a map from S1 to S2 such that for every a ∈S1 there is a ϕ(a) ∈S2. We say that ϕ is a
homomorphism if, for every a,b ∈S1
ϕ(a ◦b) = ϕ(a) × ϕ(b).
(1.20)
In other words, the image of the product of a and b in S1 is the product of their images
ϕ(a) and ϕ(b) in S2.
Example
Consider (Z,+) and ((1,−1),·) where Z is the set of integers and + is the
usual addition on it, while · is the usual multiplication on the two element set (1,−1).
Deﬁne a map ϕ by
ϕ(n) = (−1)n,
that is,
(−1)a+b = (−1)a · (−1)b
which is clearly true.
If the map ϕ deﬁning a homomorphism is also one to one and onto, then it is called
isomorphism.
Exercise
Show that the set Z2 of integers modulo 2 and ((1,−1),·) deﬁned above, are
isomorphic.
□
1.10
Isomorphism with R3
Until now we used R3 just as a name for the space we live in and in which all physical
phenomena occur. In this section we give exact deﬁnition of R3 and justify using it as a
name for our space.

46
An Introduction to Vectors, Vector Operators and Vector Analysis
Let us choose an orthonormal basis {ˆi,ˆj, ˆk} based at some origin O and the
corresponding Cartesian coordinate system. We now use it to assign the Cartesian
coordinates to all points in space. This procedure assigns a unique triplet of real numbers
to every point in space. In fact the set of all triplets of real numbers can be identiﬁed with
the set of all points in space. We call this set of triplets R3. Although, R3 stands for the set
of all real number triplets, its one to one correspondence with the set of points in real
space justiﬁes naming real space by R3 as we have been doing until now.
We have already established a one to one correspondence between the set of vectors
(representations of values of vector quantities in space) and R3, (for a given basis), using
the fact that every vector can be written as a unique linear combination of the basis vectors
(see sections 1.3 and 1.6). Here, we want to show something more. First, we deﬁne the
addition in R3 as
(a1,a2,a3) + (b1,b2,b3) = (a1 + b1,a2 + b2,a3 + b3).
Consider two vectors a and b with coordinates {a1,a2,a3} and {b1,b2,b3} respectively. This
means
a = a1ˆi + a2ˆj + a3 ˆk
b = b1ˆi + b2ˆj + b3 ˆk.
Using the distributive law for the multiplication of vectors by scalars and the commutativity
and the associativity of the vector addition we can write
a + b = (a1 + b1)ˆi + (a2 + b2)ˆj + (a3 + b3)ˆk.
Thus, the coordinates of a + b are simply the addition of the coordinates of a and those
of b. Then, we have the following association.
a ↔(a1,a2,a3),
b ↔(b1,b2,b3),
a + b ↔(a1 + b1,a2 + b2,a3 + b3).
(1.21)
Let us deﬁne the scalar product in R3 as the product of two 1×3 and 3×1 matrices (a row
vector and a column vector),
[a1a2a3][b1b2b3]T =
X
i
aibi
(1.22)
where the superscript T denotes the transpose of a matrix. Thus, we see that the
correspondence Eq. (1.21) preserves the scalar product of vectors.

Getting Concepts and Gathering Tools
47
Thus, we have a one to one map between the two sets: The set of vectors (whose
elements are the ‘values’ of one or more vector quantities) and R3, (whose elements are
the triplets of real numbers) which preserves the addition on individual sets in the sense of
Eq. (1.20). Thus, the one to one map deﬁned by Eq. (1.21) is an isomorphism between
these two sets. Two isomorphic sets are algebraically identical and it is enough to study
only one of them. Even the scalar and vector products can be expressed and processed in
terms of the components of vectors, which are triplets of real numbers. So you may come
up with the idea that we can just do away with the set of vectors and do everything using
the set of triplets of real numbers, namely R3. This will free us from dealing with vectors
altogether. A nice idea, but has the following problem. At the end of section 1.3 we saw
that the one to one correspondence between vectors and R3 depends on the origin and the
basis chosen. There is a different isomorphism for each possible origin and each possible
basis because a change in the basis/origin changes the coordinates of every vector (see
Fig. 1.4). Since there could be uncountably many origins and bases, there are uncountably
many isomorphisms possible between the set of vectors and R3. It is then impossible to
keep track of which isomorphism being used and to transform between these. On the
other hand, the coordinate free approach, in which we directly deal with the set of vectors,
frees us from this problem of keeping track of bases and transforming between them. It
also enables us to reach conclusions that are independent of any particular basis or the
coordinate system. Thus, coordinate free approach turns out to be more fruitful in many
applications. On the other hand, an intelligent choice of the coordinate system, basically
guided by the symmetry in the problem, can drastically reduce the algebra and can
sharpen the understanding of the physics of the situation. Therefore, a judicious choice
between these methods, depending on the problem, turns out to be rewarding.
A set V and the associated set of scalars S, with the operations of addition and scalar
multiplication deﬁned on V , which have all the properties of vector addition and scalar
multiplication as listed in section 1.4, is called a linear space. If, in addition, we deﬁne a
scalar product and the resulting metric (a distance function giving distance between every
pair of elements), then it is called a metric space. Thus, a set of vectors is a metric space
with a Euclidean metric. Let us call the 3-D space comprising all vectors (that is, all values
of one or more vector quantities) E3. Both E3 and R3 are metric spaces with Euclidean
metric (see Eq. (1.9) and the exercise following it). If a subset of a metric space is closed
under addition, that is, the addition of every pair of vectors in the subset gives a vector
in the same subset, then such a subset is a metric space in its own right and is called a
subspace of the parent metric space. A basis in a subspace can always be extended to that
of the whole space. The dimension of a subspace is always ≤that of the whole space. Thus,
for example, a set of planar vectors (a plane) and a set of vectors on a straight line (a straight
line) are the 2-D and 1-D subspaces of E3 (R3) respectively.
Since R3 and E3 are isomorphic linear spaces, they can be used interchangeably in all
contexts. However, we will basically refer to the space E3 as we intend to deal directly with
the vectors, although we will make judicious use of R3 as well (when we operate by matrices
on 3-D column vectors comprising the coordinates of vectors, see the next chapter).

48
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Any set on which addition and scalar multiplication operations (with all the
properties stated in section 1.4) are deﬁned, is a linear space. Show that (i) The set of real
numbers forms a one dimensional linear space where addition of “vectors” is ordinary
addition and multiplication by scalars is ordinary multiplication. (ii) The set of positive
real numbers forms a linear space where addition of vectors is ordinary multiplication and
scalar multiplication is appropriately deﬁned.
Solution
(ii) The zero vector is the real number 1. “Multiplication” of the vector a by
the scalar λ means raising a to power λ. Thus, if the addition is denoted by ⊕and scalar
multiplication by ⊙then
λ ⊙(a ⊕b) = (ab)λ = aλbλ = (λ ⊙a) ⊕(λ ⊙b).
□
Exercise
Verify that the complex numbers form a two dimensional linear space where
the addition is ordinary addition and scalars are real numbers.
□
1.11
A New Notation: Levi-Civita Symbols
It is our common experience that something we want to say can be expressed in many
different ways and each such expression is more or less effective depending on the context
in which it is used. Mathematical modelling of physical systems and processes, being very
much a human endeavor, is no exception. Here, this amounts to using different
mathematical notations and expressions applied to the same physical situation.
Depending on the context, that is, on the questions whose answers we are seeking,
different notations and formulations are more or less effective. Using different notations
and resulting formulations could be very effective, as this may throw light on various
aspects of the process under study, which remain hidden while using other notations and
formulations.
In this section we want to express various aspects of vectors we have learnt so far, in
a new avatar, ﬁrst used by Levi-Civita. We will ﬁnd this notation very useful to deal with
vectors in different contexts. To get to this formulation, we ﬁrst invoke a ﬁxed orthonormal
basis giving a right handed system, say {ˆ1, ˆ2, ˆ3}. Let {ˆi,ˆj, ˆk} denote the unit vectors which
are variables taking values in the set {ˆ1, ˆ2, ˆ3}. Let us now deﬁne the so called Levi-Civita
symbols by
εijk = ˆi · (ˆj × ˆk).
(1.23)
Note the one to one correspondence between the index set {i,j,k} and the unit vector
variables {ˆi,ˆj, ˆk}. Thus, different values for the index string ijk, drawn from the set
{1,2,3} uniquely decide the value of εijk by giving the corresponding values to the vector
variables ˆi,ˆj, ˆk in Eq. (1.23), drawn from the set {ˆ1, ˆ2, ˆ3}.
Exercise
Show that the number of strings of length n made up of symbols such that each
symbol in the string is drawn from a set of m symbols, is mn.

Getting Concepts and Gathering Tools
49
Solution
Each symbol can be chosen in m independent ways, so n symbols can be chosen
in mn independent ways.
□
In our case, we ask for the number of strings of length 3 made out of three symbols {123}.
By the above exercise, there are totally 33 = 27 of such strings, or, in other words, εijk
are totally 27 in number, which can be explicitly constructed by giving values from the set
{ˆ1, ˆ2, ˆ3} to the variables ˆi,ˆj, ˆk in Eq. (1.23). By Eqs (1.16) and (1.23), if any two or more
variables ˆi,ˆj, ˆk have the same value from the set {ˆ1, ˆ2, ˆ3} then εijk = 0. In other words,
εijk = 0 whenever any two or more indices ijk have the same value.
Exercise
Show that exactly 21 εijks are zero.
Hint
The number of εijk, with the indices {i,j,k} all distinct, equals the number of
permutations of (123) = 3! = 6.
□
When all of ˆi,ˆj, ˆk have different values, (ˆi , ˆj , ˆk), using Eq. (1.23), εijk = ±1 depending
on whether {ijk} is a cyclic permutation of {123} or not. This follows from Eq. (1.11) and
the fact that the scalar triple product changes sign if the cyclic order of its factors is changed
(see subsection 1.8.1). Thus, ε312 = ˆ3·(ˆ1× ˆ2) = +1 while ε132 = ˆ1·(ˆ3× ˆ2) = −1. Further,
εijk is invariant under the cyclic permutation of its indices because the scalar triple product
deﬁning it is invariant under the cyclic permutation of its factors. εijk can be viewed as a
scalar valued function of three vector variables {ˆi,ˆj, ˆk} deﬁned on the set {ˆ1, ˆ2, ˆ3}. When
we write all 27 values of εijk as a three dimensional (3 × 3 × 3) array, each element having
three indices, we call it a tensor. εijk is an antisymmetric tensor because all its non-zero
elements change sign under the exchange of two of their indices.
Incidentally, any two of the vector variables say ˆi,ˆj can be used to give an operative
deﬁnition of the Kronecker delta symbol δij as
δij = ˆi · ˆj.
This is because whenever ˆi and ˆj pick up different values from the orthonormal set {ˆ1, ˆ2, ˆ3},
ˆi · ˆj vanishes, while whenever ˆi and ˆj have the same value ˆi · ˆj is unity. Using this deﬁnition
we immediately see that
δji = δij.
Also, we have,
3
X
j=1
δijδjk = δi1δ1k + δi2δ2k + δi3δ3k = δik
(1.24)
The last equality follows because the sum in the middle is unity when i and k have the same
value out of {1,2,3}, while it vanishes if i and k have different values.

50
An Introduction to Vectors, Vector Operators and Vector Analysis
We will now prove an identity involving Levi-Civita symbols and some of the special
cases of this identity which turn out to be very useful in getting vector identities (see the
next section) and also in the development of vector calculus. This is
εijkεlmn =

δil
δim
δin
δjl
δjm
δjn
δkl
δkm
δkn

,
where the elements of the determinant on the right are the Kronecker deltas we already
know. Here, the equality means that the action of the LHS on an expression depending on
the indices {ijk} and {lmn}, (taking values in {1,2,3}), is the same as that of the
determinant expression involving Kronecker deltas on the RHS. This gives a powerful way
to simplify the expressions involving the products of Levi-Civita symbols.
To prove this identity, we ﬁrst note that the indices {ijk} and {lmn} correspond to two
sets of vector variables {ˆi,ˆj, ˆk} and {ˆl, ˆm, ˆn} respectively, both taking values in the
orthonormal basis set {ˆ1, ˆ2, ˆ3}. As shown in Fig. 1.29 we refer to another orthonormal
basis { ˆσ1, ˆσ2, ˆσ3}. By Eq. (1.23) and the determinant giving scalar triple product we can
write
εijk = ˆi · (ˆj × ˆk) =

ˆi · ˆσ1
ˆi · ˆσ2
ˆi · ˆσ3
ˆj · ˆσ1
ˆj · ˆσ2
ˆj · ˆσ3
ˆk · ˆσ1
ˆk · ˆσ2
ˆk · ˆσ3

= |A| say
and
εlmn = ˆl · ( ˆm × ˆn) =

ˆl · ˆσ1
ˆl · ˆσ2
ˆl · ˆσ3
ˆm · ˆσ1
ˆm · ˆσ2
ˆm · ˆσ3
ˆn · ˆσ1
ˆn · ˆσ2
ˆn · ˆσ3

= |B| say.
Fig. 1.29
See text

Getting Concepts and Gathering Tools
51
Here, |A| and |B| are the determinants of the corresponding matrices. Using the fact
that the determinant of a matrix is the same as that of its transpose and that the product of
the determinants of two matrices is the determinant of their product, (see Appendix A),
we get,
εijkεlmn = |A| · |B| = |A| · |BT | = |ABT | =

ˆi · ˆl
ˆi · ˆm
ˆi · ˆn
ˆj · ˆl
ˆj · ˆm
ˆj · ˆn
ˆk · ˆl
ˆk · ˆm
ˆk · ˆn

To understand the last equality, note that a typical element of ABT is (see Fig. 1.29)
(ˆi · ˆσ1)(ˆl · ˆσ1) + (ˆi · ˆσ2)(ˆl · ˆσ2) + (ˆi · ˆσ3)(ˆl · ˆσ3) = ˆixˆlx + ˆiyˆly + ˆizˆlz = ˆi · ˆl.
Since the variables {ˆi,ˆj, ˆk} and {ˆl, ˆm, ˆn} take values in the orthonormal basis set {ˆ1, ˆ2, ˆ3},
we have ˆi · ˆl = δil etc, giving us the desired identity.
Before proceeding further, we need to introduce a convention, called Einstein
summation convention, regarding the sum over a term in an expression whose terms
depend on some index set, say {i,j,k}. As per this convention, a term in which an index
say i is repeated is to be summed over that index. Thus, for example, εijkεilm =
P3
i=1 εijkεilm, a sum in which at most one term survives. Also,
δkk =
3
X
k=1
δkk = δ11 + δ22 + δ33 = 3.
Henceforth, in this book, whenever applicable, Einstein summation convention will always
be assumed to apply, unless stated otherwise. So you will have to be alert about this.
We can now obtain some special cases of the result we just proved. Thus, the
determinant for εijkεilm can be obtained from that for εijkεlmn by replacing ˆl by ˆi. Since
all the indices {i,j,k} must be different, (otherwise εijk = 0), we must have ˆi · ˆi = 1 and
ˆj · ˆi = 0 = ˆk · ˆi. Substituting these values in the determinant and evaluating it we get
εijkεilm = δjlδkm −δjmδkl.
Next, consider
εijkεijl = δjjδkl −δkjδjl = 3δkl −δkl = 2δkl.
Here, we have used δjj = 3 and δkjδjl = δkl which we proved above (see Eq. (1.24)).
Finally, we have,
εijkεijk = 2δkk = 2 · 3 = 6.

52
An Introduction to Vectors, Vector Operators and Vector Analysis
Let us try and express the vector product in terms of the Levi-Civita symbols. Using
Eqs (1.7) and (1.13) we can express the ith component of a × b as
(a × b)i = ˆi · (a × b) = ajbk −akbj = εijkajbk.
(1.25)
In the last term, a sum over indices j = 1,2,3 and k = 1,2,3 is implied, which is a sum of
nine terms. However, seven out of these nine terms vanish, because the corresponding εijk
vanish due to repeated indices, so that only two terms survive. Thus,
(a × b)1 = ε123a2b3 + ε132a3b2 = a2b3 −a3b2.
Check that other terms in the implied sum vanish.
Subsequently, we will have many occasions to use these results.
1.12
Vector Identities
In this section, we equip our toolbox by acquiring some of the most penetrating tools of
vector algebra and analysis. These are the so called vector identities. A vector identity is
an equality involving vector variables which holds good for every possible (vector) value
that these variables can take. From our school days, we are familiar with trigonometric
identities like sin2 θ +cos2 θ = 1 or cos(A+B) = cos(A)cos(B)−sin(A)sin(B) which
hold for all possible values of the angles involved. In this section, we deal with the identities
involving the vector variables. We learn about the identities involving vector operators in a
later section. All the vector identities can be proved using Levi-Civita notation.
We prove the vector identities one by one.
We ﬁrst prove
a × (b × c) = (a · c)b −(a · b)c.
(1.26)
We have,
[a × (b × c)]i = εijkaj(b × c)k
= εkijεklmajblcm
= (δilδjm −δimδjl)ajblcm
= (ajcj)bi −(ajbj)ci
= (a · c)bi −(a · b)ci.
Thus, the ith components (i = 1,2,3) of both the sides are equal, which proves the
identity. This identity tells us that the vector product of a polar and an axial vector equals

Getting Concepts and Gathering Tools
53
the difference of two polar vectors and hence is itself a polar vector. By permuting a,b,c
in cyclic order in the identity a × (b × c) = (a · c)b −(a · b)c we get two more identities,
b × (c × a) = (a · b)c −(b · c)a
c × (a × b) = (b · c)a −(c · a)b.
Adding these three identities we get
a × (b × c) + c × (a × b) + b × (c × a) = 0.
The next identity is
(a × b) · (c × d) = (a · c)(b · d) −(a · d)(b · c).
We have,
(a × b)i(c × d)i = εijkεilmajbkcldm
= (δjlδkm −δjmδkl)ajbkcldm
= (ajcj)(bkdk) −(ajdj)(bkck)
= (a · c)(b · d) −(a · d)(b · c).
Exercise
Prove the identity
(a × b) × (c × d) = (a · c × d)b −(b · c × d)a
= (a · b × d)c −(a · b × c)d.
□
Throughout the remaining text, all these identities will be used very frequently. We
recommend that you practice these identities by using them in as large variety of problems
as possible. For future convenience we list these identities once again, separately. In the
remaining part of the book we will refer to these identities by their Roman serial numbers
in this list.
(I) a × (b × c) = (a · c)b −(a · b)c.
(II) (a × b) · (c × d) = (a · c)(b · d) −(a · d)(b · c).
(III) a × (b × c) + c × (a × b) + b × (c × a) = 0.
(IV) (a × b) × (c × d) = (a · c × d)b −(b · c × d)a = (a · b × d)c −(a · b × c)d.

54
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Show that
(a × b) · (c × d) × (e × f) = [abd][cef] −[abc][def],
= [abc][fcd] −[abf][ecd],
= [cda][bef] −[cdb][aef],
where we have used Grassmann notation for the scalar triple product.
□
1.13
Vector Equations
An equation involving expressions of vectors and scalars is a (algebraic) vector equation.
A vector equation can be solved either for an unknown vector or for an unknown scalar
in it. The novel aspect of vector equations is that they can be transformed and solved
using vector algebra. Here, we give some simple results regarding vector equations. We will
have occasions to solve vector equations especially in the next chapter. In this section we
use x,y,z,... for vector variables (unknowns) and a,b,c,... for known or constant vectors
appearing in an equation. As usual, greek letters are used for scalars.
(i) A linear equation in one unknown vector may be solved similar to such a scalar
equation. Thus, the equation
λx + µa = αx + βb
can be solved by shifting terms on either side of the equation giving
x =

β
λ −α

b −

µ
λ −α

a.
(ii) The vector equation λx + µa = νb where λ , 0,µ,ν are given constant scalars and
a,b are constant vectors has a unique solution x = 1
λ(νb −µa).
The fact that this equation admits a solution can be trivially checked. We have to
subtract µa on both sides and then divide by λ on both sides to get the given solution.
Properties of vector addition and scalar multiplication allow these operations. Next
we can substitute the given solution for x in the equation and check that it satisﬁes
the equation. Thus the given solution is a solution of the given equation. To see that
this solution is unique, assume two solutions x1 and x2, substitute in the equation
and equate the two resulting expressions to show that x1 = x2.
(iii) λa+µb = c to be solved for two unknown scalars λ,µ where all the three vectors are
given constant non-zero vectors.
Taking cross product by b on both sides of the equation from right we get
λa × b = c × b.

Getting Concepts and Gathering Tools
55
Dotting both sides with c × b we get
λ =
|c × b|2
(a × b) · (c × b),
assuming that the pairs a,b and also b,c are not parallel to each other. If a and b are
parallel, we have a = νb and the equation reduces to (λν+µ)b = c. This shows that
b and c are also parallel, hence there are inﬁnite number of solutions of λ and µ. To
get µ, we take cross product by a on both sides of the equation and proceed exactly as
before. The result is
µ =
|c × a|2
(b × a) · (c × a).
(iv) The equation x · a = λ where λ is a known scalar and a is a known non-zero vector.
We rewrite the equation as
x · a = λa−1 · a, or (x −λa−1) · a = 0,
which implies that the vector x −λa−1 is orthogonal to a so that
x −λa−1 = a × b,
where b is a non-zero arbitrary vector, not parallel to a. Thus,
x = λa−1 + a × b
is the general solution we are seeking.
(v) The equation x×a = b where a , 0 and b are known vectors admits a solution if and
only if a · b = 0.
To prove the necessity, assume that the equation admits a solution x. Hence,
a · b = a · (x × a) = 0, which establishes the necessity. Now assume a · b = 0
and substitute the expansion of a vector x in terms of three non-coplanar vectors
x = λa + µb + ν(a × b)
(with λ,µ,ν scalars), in the equation. We get, after some algebra and using a · b = 0,
µ(b × a) + [ν(a · a) −1]b = 0.
Since the vectors b × a and b are linearly independent, both the coefﬁcients must
vanish separately, giving
µ = 0 and ν|a|2 −1 = 0

56
An Introduction to Vectors, Vector Operators and Vector Analysis
which means ν =
1
|a|2 and leads to
x = λa + 1
|a|2 (a × b) = λa + (a−1 × b),
which satisﬁes the given equation irrespective of the value of the scalar λ.
(vi) The equations x · a = λ and x × b = c where a,b,c are given vectors with a,b non-
orthogonal (a · b , 0,) uniquely determine vector x.
Crossing the second equation on the left by a we get
a × (x × b) = a × c,
or, using identity I,
(a · b)x −(a · x)b = a × c
or,
x =
1
a · b(λb + a × c)
which satisﬁes both the equations.
To get the uniqueness, suppose that two vectors x1 , x2 satisfy the given equations.
This leads to
(x1 −x2) · a = 0 and (x1 −x2) × b = 0.
Therefore, the vector a is perpendicular to and vector b is parallel to the vector x1−x2.
This makes vectors a and b mutually orthogonal, contradicting the assumption that
they are not. Thus, we must require
x1 −x2 = 0 or, x1 = x2.
Exercise
Show that αa−1 is a solution of
a · y = α, (α , 0).
Show that there are inﬁnitely many solutions.
□
Exercise
show that the necessary and sufﬁcient condition for the equation
a × y = b
where a and b are known and a , 0, posseses a solution is a · b = 0.

Getting Concepts and Gathering Tools
57
Solution
Since
a · b = a · (a × y) = 0,
the condition is necessary.
Now suppose a · b = 0. Then,
a × (b × a−1) = b −(a · b)a−1 = b.
Thus,
y = b × a−1
satisﬁes a × y = b. The solution is not unique since
y = b × a−1 + λa
is also a solution.
□
Exercise
Show that a vector is uniquely determined if its dot product with three non-
coplanar vectors are known.
Hint
Expand the vector in the basis comprising the given three non-coplanar vectors.
□
Exercise
The resultant of two vectors is equal in magnitude to one of them and is
perpendicular to it. Find the other vector.
Hint
Let a + b = c with |a| = |c| = λ say and let |b| = µ. Also, a · c = 0. (Draw a ﬁgure).
Take the unit vectors along a and c as the orthonormal basis. Express a,b and c in terms of
this basis and use the ﬁrst equation. Find b in terms of the angle θ it makes with c and its
magnitude λ.
Answer
θ = π
4 , µ =
√
2λ. You can get this answer just by drawing the ﬁgure.
□
1.14
Coordinate Systems Revisited: Curvilinear Coordinates
As we go along any one of the coordinate axes, say x axis, only the corresponding
coordinate changes while the other two remain unchanged. Let us deﬁne a coordinate line
to be that curve in R3 along which only one of the coordinates changes, while other two
remain the same. Now these coordinate lines need not be straight lines! The coordinate
systems for which one or more coordinate lines are curves other than straight lines are
called curvilinear coordinate systems. In fact we can set up coordinate systems in which
the coordinate lines are circles!
1.14.1
Spherical polar coordinates
One such very useful coordinate system is the so called spherical polar coordinate system
which we now set up (see Fig. 1.30). First, we ﬁx a right handed rectangular Cartesian
coordinate system at the origin O. Mark out a point P in space and draw the line joining

58
An Introduction to Vectors, Vector Operators and Vector Analysis
points O and P . This line is the r axis and distance OP is the r coordinate of P . Note that
r is always non-negative, r ≥0. Let ˆr be the unit vector based at P and pointing away
from O along the r coordinate line. Now draw the circle of radius r with center at O and
lying in the plane deﬁned by the unit vectors ˆr and ˆk. As we go along this circle, only the
polar angle θ, namely the angle between ˆr and ˆk (which deﬁnes the positive direction of
the z axis), changes, while r and the third coordinate φ (see below) do not change. This
is the θ coordinate line, which is actually a circle of radius r. Now, draw a circle in the
plane parallel to the x −y plane passing through P , with its center on the z axis and with
radius r sinθ. (see Fig. 1.30 to check that this circle passes through P ). We can measure the
angular coordinate of a point on this circle, say φ, as the angle made by the radius of this
circle passing through that point with ˆi which deﬁnes the positive direction of the x axis
(the azimuthal angle). As we go along this circle, only the coordinate φ changes, while the
other two, r and θ do not. This is the φ coordinate line, again a circle. Every point in R3
corresponds to a unique triplet of values of the (r, θ, φ) coordinates. Now draw the unit
vectors, ˆθ and ˆφ tangent (at P ) to the θ circle and φ circle respectively, so that the triplet
(ˆr, ˆθ, ˆφ) forms a right handed system. Note that different points in space have different
triplet of vectors (ˆr, ˆθ, ˆφ). We cannot express every vector as the linear combination of the
vectors from the same triplet. A vector like α ˆθ, (α a scalar) which will appear in such a
linear combination, is a vector of length α and tangent (at P ) to the θ circle. However, the
change in the θ coordinate corresponds to the angular advance of the vector r = ⃗
OP along
the θ circle and not along a vector tangent at P to this circle. The vector r = ⃗
OP equals rˆr
where ˆr belongs to the triplet (ˆr, ˆθ, ˆφ) deﬁned at P and r is the magnitude of r, or the length
of the vector ⃗
OP .
Fig. 1.30
Spherical polar coordinates

Getting Concepts and Gathering Tools
59
To ﬁnd the relation between the Cartesian (x, y, z) and spherical polar (r, θ, φ)
coordinates, replace v by ˆr (magnitude r = 1) in Eq. (1.8). We get,
ˆr = sinθ cosφˆi + sinθ sinφˆj + cosθ ˆk.
Since ⃗
OP = rˆr, we can identify the x, y, z coordinates of ⃗
OP = r to be
x = r sinθ cosφ,
y = r sinθ sinφ,
z = r cosθ.
(1.27)
Exercise
Convince yourself that the polar coordinates of all the vectors in R3 lie within
0 ≤r < ∞, 0 ≤θ ≤π and 0 ≤φ < 2π. (use Fig. 1.30)
□
Exercise
Show that
ˆθ = cosθ cosφˆi + cosθ sinφˆj −sinθ ˆk
and
ˆφ = −sinφˆi + cosφˆj.
Hint
Note that ˆθ = ˆr(θ + π
2 ,φ) and ˆφ = ˆr(θ = π
2 ,φ + π
2 ).
□
A coordinate line is a curve at all points of which two coordinates have constant values
while the remaining coordinate takes all possible values as the coordinate line is scanned.
We say that a coordinate line is parameterized by the corresponding coordinate. Thus, the
(x,y) coordinates of every point on the φ coordinate line are completely speciﬁed by the
corresponding value of φ, say φ = φ0, as x = r sinθ cosφ0, y = r sinθ sinφ0 (remember
that r and θ are constants for a φ coordinate line). Similarly, if we allow two of the three
coordinates to vary through their all possible values, keeping the third coordinate ﬁxed, we
generate a coordinate surface, labelled by the coordinate which remains constant on that
surface. For the spherical polar coordinates, the r-coordinate surface on which r remains
constant say r = R (R constant), is a sphere of radius R, For every r = constant there is a
r-surface, so we describe a family of r-surfaces as
x2 + y2 + z2 = r2 (spheres, r = constant).
From Eq. (1.27) we see that θ-coordinate surface is generated by all points whose (x,y,z)
coordinates satisfy
tanθ = (x2 + y2)1/2/z

60
An Introduction to Vectors, Vector Operators and Vector Analysis
which are circular cones θ = constant. The φ coordinate surfaces are generated by all
points whose (x,y,z) coordinates satisfy
tanφ = y/x
and z is arbitrary. These are half planes, that is, the planes which terminate at the z axis,
because the other half plane, on the other side of the z axis corresponds to π +φ. All these
coordinate surfaces are depicted in Fig. 1.31.
Given any point in space, with coordinates R,θ0,φ0, the coordinate surfaces r = R,
θ = θ0 and φ = φ0 pass through that point. The φ coordinate line is the intersection of
the r = R, θ = θ0 surfaces and lies in a plane parallel to xy plane while θ coordinate line
is the intersection of r = R and φ = φ0 surfaces and lies in a plane normal to xy plane.
Therefore, the vectors ˆθ, ˆφ tangent to these curves must be mutually perpendicular. The
plane containing these two vectors is tangent to the sphere r = R at the given point, so that
the unit vector ˆr must be normal to both ˆθ, ˆφ. Thus, the vectors ˆr, ˆθ, ˆφ form an orthonormal
basis. Such a system is called an orthogonal curvilinear coordinate system.
Fig. 1.31
Coordinate surfaces are x2+y2+z2 = r2 (spheres r = constant) tanθ =
(x2 + y2)1/2/z (circular cones, θ = constant) tanφ = y/x (half planes
φ = constant)
1.14.2
Parabolic coordinates
We shall learn about one more orthogonal curvilinear coordinate system, namely, the
parabolic coordinate system. We ﬁrst set up a right handed Cartesian coordinate system
with orthonormal basis (ˆi, ˆj, ˆk). A point in space having Cartesian coordinates (x,y,z)

Getting Concepts and Gathering Tools
61
has parabolic coordinates denoted by (µ,ν,φ), (µ,ν ≥0,0 ≤φ < 2π). These two sets of
coordinates are related by
x = µν cosφ,
y = µν sinφ,
z = 1
2

µ2 −ν2
.
(1.28)
These equations have all the information regarding the geometry of the parabolic
coordinate system. To get it, we ﬁrst identify the coordinate φ with the azimuthal angle
deﬁned above in the context of polar coordinates. Then, the ﬁrst two of Eq. (1.28) tell us
that the φ coordinate line is a circle of radius µν, passing through the given point and the
corresponding basis vector ˆφ must be tangent to this circle at the given point. The φ
coordinate plane passes through the z axis, making an angle φ with the positive direction
of the x axis.
To get the coordinate lines for µ and ν, we ﬁrst ﬁx the azimuthal angle φ = 0. This
mean we choose the xz or y = 0 plane to see the variations of µ and ν. We assume that
the given point lies in the y = 0 plane. We now give some constant value to ν, say ν = ν0.
With φ = 0 and ν = ν0 the ﬁrst of Eq. (1.28) gives µ = x/ν0 and the third of Eq. (1.28)
becomes
z = 1
2
 x2
ν2
0
−ν2
0
!
.
(1.29)
This is a parabola ﬂattened by dividing each value of x2 by the constant 2ν2
0 and shifted
downwards from the origin by 1
2ν2
0. By choosing the value of ν0 properly, we can make
this parabola pass through the given point giving ν0 as the value of its ν coordinate. This
parabola is the coordinate line for µ, because only µ varies on it, while both ν = ν0 and
φ = 0 are constants. To get the coordinate line for ν, we make µ to be a constant µ = µ0
so that the third of Eq. (1.28) becomes
z = 1
2
 
µ2
0 −x2
µ2
0
!
.
(1.30)
This is an inverted parabola ﬂattened by the division by the constant 2µ2
0 and shifted
upwards from the origin by 1
2µ2
0. By suitably choosing µ0, we can make this parabola pass
through the given point, making chosen µ0 to be the value of its µ coordinate. This
parabola is the coordinate line for ν, on which only ν varies, while µ = µ0 and φ = 0 are
constants.
Let us now show that these parabolas intersect normally at the given point. Let dr1 and
dr2 be the differential displacements along the ν = ν0 and µ = µ0 parabolas respectively.

62
An Introduction to Vectors, Vector Operators and Vector Analysis
By differential displacement we mean the displacement ds of a point along the parabola,
which is so small that the error incurred by replacing |ds| by |dr|, where dr is the difference
between the position vectors at the endpoints, is utterly negligible (see Fig. 1.32). Let dr1 =
dx1ˆi + dz1 ˆk, and dr2 = dx2ˆi + dz2 ˆk deﬁne the corresponding components. We then
have,
dr1 · dr2 = dx1dx2 + dz1dz2
= dx2
 
1 −
x2
0
µ2
0ν2
0
!
= 0
(1.31)
Fig. 1.32
Differential displacement corresponds to |ds| = |dr| (see text)
where we have taken dx1 = dx = dx2, differentiated Eqs (1.29) and (1.30) to get dz1 and
dz2 and used the fact that by the ﬁrst of Eq. (1.28) x2
0 = µ2
0ν2
0 for φ = 0. Geometrically, this
means that the tangent vectors to the two parabolas at the intersection point are orthogonal
to each other. Since the tangent vector to the φ coordinate circle at the given point is normal
to the y = 0 plane it is normal to the tangent vectors to the two parabolas. Thus, the basis
vectors for the parabolic coordinate system form an orthonormal triad ( ˆµ, ˆν, ˆφ) which are
the tangent vectors to the three coordinate lines at the given point such that they form a
right handed system (Fig. 1.33).
If we change the azimuthal angle φ from zero, the y = 0 plane rotates through the same
angle, without changing the µ and ν parabolas in any way. This completes the construction
of the parabolic coordinate system.
Note, again, that the basis triad ( ˆµ, ˆν, ˆφ) changes from point to point. Therefore, for
the same reasons as explained in the case of polar coordinates, every vector cannot be

Getting Concepts and Gathering Tools
63
expanded in terms of the same basis triad. To get the coordinate surfaces say for constant
µ (µ = µ0) we note that for an arbitrary value of φ say φ = φ0, the ﬁrst two of Eq. (1.28)
give x2 + y2 = µ2ν2 and hence the equation for the parabola with µ = µ0 in the plane
corresponding to φ = φ0 is obtained by replacing x2 in Eq. (1.29) by x2+y2. This equation
is independent of φ and hence applies to every value of φ. Thus, all the points (x,y,z)
satisfying
z = 1
2
 
µ2
0 −x2 + y2
µ2
0
!
Fig. 1.33
Parabolic coordinates (µ,ν,φ). Coordinate surfaces are paraboloids of
revolution (µ = constant, ν = constant) and half-planes (φ = constant)
or,
x2 + y2 = µ2
0

µ2
0 −2z

for constant µ = µ0 lie on the paraboloid of revolution revolving the parabola (that is,
covering all values of φ) about the z axis. On this surface µ = µ0 and (ν,φ) can take all
possible values. The surface for constant φ = φ0 is a half plane, that is, the plane
terminating at the z axis, because the half plane on the other side of the z axis corresponds
to φ = π + φ0. Thus, the families of coordinate surfaces are given by
x2 + y2 = µ2
0

µ2
0 −2z

(1.32)

64
An Introduction to Vectors, Vector Operators and Vector Analysis
(paraboloids of revolution, µ = constant),
x2 + y2 = µ2
0

µ2
0 + 2z

(1.33)
(paraboloids of revolution, ν = constant),
tanφ = y/x
(1.34)
(half planes φ = constant).
Exercise
Justify the coordinate lines and coordinate surfaces, shown in Fig. 1.34, for the
cylindrical coordinates (0 ≤ρ < ∞, 0 ≤φ < 2π, −∞< z < +∞) deﬁned by
x = ρcosφ,
y = ρsinφ,
z = z,
(1.35)
Fig. 1.34
Cylindrical coordinates (ρ,φ,z). Coordinate surfaces are circular cylinders
(ρ = constant), half-planes (φ = constant) intersecting on the z-axis, and
parallel planes (z = constant)
where the coordinate surfaces are given by
x2 + y2 = ρ2
(circular cylinders, ρ = constant),
tanφ = y/x

Getting Concepts and Gathering Tools
65
(half planes, φ = constant),
z = constant
(planes).
□
Exercise
Find the coordinate lines and the coordinate surfaces for the prolate spheroidal
coordinates (0 ≤η < ∞, 0 ≤θ ≤π, 0 ≤φ < 2π) given by (see Fig. 1.35)
x = asinhη sinθ cosφ,
y = asinhη sinθ sinφ,
z = acoshη cosθ,
(1.36)
Fig. 1.35
Prolate spheroidal coordinates (η,θ,φ). Coordinate surfaces are prolate
spheroids (η = constant), hyperboloids (θ = constant), and half-planes
(φ = constant)
where the coordinate surfaces are
x2
a2 sinh2 η
+
y2
a2 sinh2 η
+
z2
a2 cosh2 η
= 1
(prolate spheroids, η = constant),
−x2
a2 sin2 θ
−
y2
a2 sin2 θ +
z2
a2 cos2 θ = 1

66
An Introduction to Vectors, Vector Operators and Vector Analysis
(hyperboloids of two sheets, θ = constant),
tanφ = y/x
(half planes, φ = constant).
□
Exercise
Find the coordinate lines and the coordinate surfaces for the oblate spheroidal
coordinates (0 ≤η < ∞, 0 ≤θ ≤π, 0 ≤φ < 2π) given by (see Fig. 1.36)
x = acoshη sinθ cosφ,
y = acoshη sinθ sinφ,
z = asinhη cosθ,
(1.37)
Fig. 1.36
Oblate spheroidal coordinates (η,θ,φ). Coordinate surfaces are oblate
spheroids (η = constant), hyperboloids (θ = constant), and half-planes
(φ = constant)
where the coordinate surfaces are
x2
a2 cosh2 η
+
y2
a2 cosh2 η
+
z2
a2 sinh2 η
= 1
(oblate spheroids, η = constant),
x2
a2 sin2 θ +
y2
a2 sin2 θ
−
z2
a2 cos2 θ = 1

Getting Concepts and Gathering Tools
67
(hyperboloids of one sheet, θ = constant),
tanφ = y/x
(half planes, φ = constant).
□
1.15
Vector Fields
We want to understand the concept of a vector ﬁeld. The best way for us is to understand
it operatively ﬁrst, which will then lead to its mathematical meaning. To get to the physical
meaning of a ﬁeld, we must apply the principle of relativity by Einstein, however, we will
not attempt that in this book. We ﬁrst choose a point in space to be the origin O. We
then obtain the position vector of a point P in space based at the origin O. Thereafter,
we base a vector giving the value of some physical quantity at P . We now imagine that
we base some vector giving a value of this physical quantity at every point in space, or in
some region of space. This association, of the set of vectors giving values of a physical
quantity with the set of position vectors or points in space (including the origin) is called a
vector ﬁeld. To give this procedure a meaning, we must seek the rule by which the vector
values of a vector quantity are associated with the points in space. This rule can be either
a one to one or many to one correspondence between the points in space and the vector
values of a vector quantity (that is, the vectors assigned to different points in space could
be different or equal). In other words, this rule is a function taking in a position vector
and returning the vector value of a vector quantity corresponding to that position vector
or the point. Thus, the vector ﬁeld is the vector valued function of the position vectors.
A function taking in a position vector specifying a point in space and returning a scalar
is said to generate a scalar ﬁeld. A function generating a vector ﬁeld or a scalar ﬁeld can
be viewed as the function of the coordinates, that is, a function which takes in a triplet of
real numbers (components of the position vector or coordinates specifying a point in space)
and returns another triplet of real numbers (components of the vector to be assigned to that
point) or a scalar. When viewed as a function of coordinates, a function generating a ﬁeld is
required to be a ‘point function’ that is, the value of the function at any point must remain
invariant even if we switch over to another coordinate system changing the coordinates
of that point. A coordinate transformation will yield a new function of new coordinates,
which, when evaluated at the new coordinates of the same point, must give the same value
of the ﬁeld at that point. A ﬁeld value at a point cannot depend on which coordinate system
we use to refer to that point. Suppose a function of the latitude and longitude returns the
temperature at a place on earth with the given latitude and longitude. If we specify the
coordinates of the points on earth using a rotated mesh of latitude-longitude and use the
corresponding transformation to get a new function of new coordinates for this scalar ﬁeld,
then this new function, when evaluated at the new coordinates of the same place, must give
the same temperature. Temperature at a place cannot depend on which coordinate system
we choose to refer to that place.
Physically, a vector ﬁeld is produced by its sources and the problem is to relate this ﬁeld
to the characteristic properties of its sources. These relations are often expressed as

68
An Introduction to Vectors, Vector Operators and Vector Analysis
differential equations. Thus, the electromagnetic ﬁeld produced by a given source of
charges and currents is the solution of Maxwell’s equations which relate the ﬁelds with the
charge and current densities of the source. Since the Maxwell’s equations are linear, the
ﬁelds produced by the multiple sources can simply be added (superposed) to get the total
ﬁeld at a point. Another example is the velocity ﬁeld of a ﬂuid, which is the assignment of
the ﬂuid velocity vector at every point in the region of space occupied by the ﬂuid. For a
general ﬂuid, this ﬁeld has to solve the Navier–Stokes equation, whose analytical solution
still eludes us. Further, Navier–Stokes equation is non-linear and gives rise to phenomena
like turbulence, which is another unsolved problem. Solving Maxwell’s equations and
special cases of the Navier–Stokes equation in various circumstances forms the content of
Electrodynamics and Fluid Mechanics. We will not make any attempt to learn about these
differential equations as they are far away lands where we have no intentions of trading.
1.16
Orientation of a Triplet of Non-coplanar Vectors
We have seen how an ordered triplet forming an orthonormal basis can be given an
orientation when we deﬁned the right handed and left handed coordinate systems.
Generally, any ordered triple of linearly independent (non-coplanar) vectors (a,b,c)
(based at a common point O say) deﬁnes a certain sense or orientation. We may, for
example, rotate the direction of a into that of b in the (a,b) plane, by an angle between 0
and π, and try and relate a vector whose direction depends on such a rotation with that
of c. Thus, we call the triplet (a,b,c) positively oriented if the rotation of the direction of
a into that of b by an angle between 0 and π in the (a,b) plane advances a right handed
screw toward that side of the (a,b) plane to which the vector c points. The triplet (a,b,c)
is negatively oriented if the advance of the right handed screw under the above rotation is
toward the opposite side. Equivalently, the sense or orientation of the triplet (a,b,c) is
deﬁned by the sense (counterclockwise or clockwise) that the above rotation appears to
have, when viewed from that side of the (a,b) plane to which the vector c points. Thus for
example, the triplets (a,b,c) and (b,a,c) have opposite orientations (see Fig. 1.37).
Fig. 1.37
(a) Positively and (b) negatively oriented triplets (a,b,c), (c) Triplet (b,a,c)
has orientation opposite to that of (a,b,c) in (a)

Getting Concepts and Gathering Tools
69
We shall now show that the necessary and sufﬁcient condition for a triplet (a,b,c) to
be positively oriented is that c · (a × b) or any of its cyclic permutations exceeds zero.
Suppose (a,b,c) are positively oriented. Then from the deﬁnitions of the positive
orientation and the vector product we see that both (a × b) and c are on the same side of
the (a,b) plane. This implies that the angle between (a × b) and c is less than π/2 which
means c · (a × b) > 0.
Suppose c · (a × b) > 0. This means the angle between (a × b) and c is less than π/2,
or, (a × b) and c are on the same side of the (a,b) plane, or the rotation from a toward b
advances a right handed screw on the same side of the (a,b) plane to which c points. In
other words, (a,b,c) are positively oriented.
Since the scalar triple product is invariant under cyclic permutations of factors, above
proof applies to all cyclic permutations of c · (a × b). Thus, we can conclude that the
orientation of (a,b,c) is invariant under the cyclic permutation of (a,b,c).
Triplets (a,b,c) and (d,e,f) are oriented (mutually) positively (negatively) with
respect to each other if they have the same (opposite) orientations. In particular, (a,b,c)
is oriented positively (negatively) with respect to an orthonormal basis (ˆe1, ˆe2, ˆe3) or
the corresponding coordinate axes (x,y,z) if (a,b,c) and (ˆe1, ˆe2, ˆe3) have the same
(opposite) orientations. Whether a given triplet (a,b,c) is oriented positively or
negatively with respect to an orthonormal basis (ˆe1, ˆe2, ˆe3) is decided, respectively, by the
positive or negative sign of
det(a,b,c) =

a1
a2
a3
b1
b2
b3
c1
c2
c3

,
(1.38)
where each row consists of the components of the corresponding vector with respect to
the orthonormal basis (ˆe1, ˆe2, ˆe3) (see the second exercise on page 39). Exchanging the
ﬁrst two columns of this determinant amounts to exchanging x,y axes or changing over to
a coordinate system with different handedness. This changes the sign of the determinant,
so that orientation of (a,b,c) with respect to the new coordinate system becomes
opposite to that with respect to the previous one.
Thus, the sign of the determinant
comprising the components of a given triplet of vectors (a,b,c) decides the orientation of
(a,b,c) with respect to the corresponding orthonormal basis (ˆe1, ˆe2, ˆe3), or, as
sometimes said, with respect to the (x,y,z) coordinates or axes. Thus, the sign of the
determinant in Eq.
(1.38) does not have a geometrical meaning independent of a
coordinate system. However, a statement like ‘two non-coplanar ordered triplets have the
same or the opposite orientation’ has a coordinate free geometrical meaning.
Consider two ordered triplets of non-coplanar vectors a1,a2,a3 and b1,b2,b3. The two
sets have the same orientation, that is, are both positively or both negatively oriented with
respect to a common coordinate system (x1,x2,x3) if and only if the condition
det(a1,a2,a3) · det(b1,b2,b3) > 0

70
An Introduction to Vectors, Vector Operators and Vector Analysis
is satisﬁed. Using identity (A.31), we can write this condition in the form
[a1,a2,a3;b1,b2,b3] > 0
(1.39)
where the symbol on the left denotes a function of six vector variables deﬁned by
[a1,a2,a3;b1,b2,b3] =

a1 · b1
a1 · b2
a1 · b3
a2 · b1
a2 · b2
a2 · b3
a3 · b1
a3 · b2
a3 · b3

·
(1.40)
Note that for b1 = a1,b2 = a2,b3 = a3 Eq. (1.40) reduces to the deﬁnition of the
Gram determinant Γ (a1,a2,a3) (see Appendix A). Equations (1.39) and (1.40) show that,
for two ordered triplets having the same orientation (relative to a coordinate system) is a
geometric property independent of any particular Cartesian coordinate system used. We
denote this property symbolically by
Ω(a1,a2,a3) = Ω(b1,b2,b3)
(1.41)
and the property of having opposite orientation by
Ω(a1,a2,a3) = −Ω(b1,b2,b3).
(1.42)
We can combine these two equations in a single one:
Ω(a1,a2,a3) = sgn[a1,a2,a3;b1,b2,b3]Ω(b1,b2,b3).
(1.43)
The last three equations are meaningful even if we do not assign a numeric value to the
individual orientation Ω. Equation (1.43) associates a value ±1 to the ratio of two
orientations, while Eqs (1.41) and (1.42) express equality or inequality of orientations. It is
possible to specify two possible orientations of triplets of vectors completely by assigning
numerical values say Ω= ±1 to these orientations by arbitrarily choosing the standard
value +1 for the orientation of the basis vectors (ˆe1, ˆe2, ˆe3) deﬁning the coordinate
system. Such a situation arises in science and engineering in the context of every
measurable quantity. For example, equality of distances between points in space or even
the ratio of distances have meaning even if no numerical values are assigned to the
individual distances. It is of course possible to assign numerical values to individual
distances such that the ratio of distances equals the ratio of the corresponding real
numbers. This requires an arbitrary selection of a “standard distance” or a unit of distance
to which all other distances are referred. Thus Eq. (1.41) is analogus to saying that
distances between two pairs of points are equal without giving them speciﬁc values.
The triplet a1,a2,a3 is oriented positively or negatively with respect to (x1,x2,x3)
coordinates according to whether they are oriented positively or negatively with respect to
the corresponding orthonormal basis (ˆe1, ˆe2, ˆe3), that is, whether
Ω(a1,a2,a3) = Ω(ˆe1, ˆe2, ˆe3)
(1.44)

Getting Concepts and Gathering Tools
71
or
Ω(a1,a2,a3) = −Ω(ˆe1, ˆe2, ˆe3).
(1.45)
Sometimes, we denote the orientation of the coordinate system Ω(ˆe1, ˆe2, ˆe3) by
Ω(x1,x2,x3). Since the value of the determinant in Eq. (1.38) gives the signed volume of
the parallelepiped spanned by a triplet of linearly independent vectors, for two such
triplets of vectors we have,
[a1,a2,a3;b1,b2,b3] = ϵ1ϵ2V1V2
(1.46)
where V1 and V2 are, respectively the volumes of the parallelepipeds spanned by the two
triplets and the factors ϵ1,ϵ2 depend on their orientations with respect to the basis
(ˆe1, ˆe2, ˆe3) deﬁning the coordinate system:
ϵ1 = sgn[a1,a2,a3; ˆe1, ˆe2, ˆe3]
ϵ2 = sgn[b1,b2,b3; ˆe1, ˆe2, ˆe3]
(1.47)
and the relative orientation of the two triplets
ϵ1ϵ2 = sgn[a1,a2,a3;b1,b2,b3]
(1.48)
is independent of the choice of the coordinate system and has the value +1 if the
parallelepipeds have the same orientation but −1 if they have the opposite orientations. If
the two triplets refer to two different coordinate systems with the orthonormal bases
(ˆe1, ˆe2, ˆe3) and (ˆh1, ˆh2, ˆh3) then,
ϵ1 = sgn[a1,a2,a3; ˆe1, ˆe2, ˆe3]
ϵ2 = sgn[b1,b2,b3; ˆh1, ˆh2, ˆh3]
µ = sgn[e1,e2,e3; ˆh1, ˆh2, ˆh3]
(1.49)
and the relative orientation of the two triplets, independent of the coordinate systems is
given by
ϵ1ϵ2µ = sgn[a1,a2,a3;b1,b2,b3]
(1.50)
and
[a1,a2,a3;b1,b2,b3] = ϵ1ϵ2µV1V2
(1.51)
where ϵ1,ϵ2,µ equal ±1 according to whether the corresponding triplets are oriented
positively or negatively. These equations are useful while dealing with triplets of vectors
(generally based in different regions of space) which refer to different coordinate systems.

72
An Introduction to Vectors, Vector Operators and Vector Analysis
However, if it is possible to choose the two coordinate systems which are positively
oriented with respect to each other, so as to ensure µ = +1, then Eq. (1.48) applies, which
decides the relative orientation of the two triplets.
Our method of deciding the orientation of ordered sets of vectors by the sign of their
determinants can be applied to the doublets of non-collinear vectors spanning a plane. We
just have to ﬁnd out
[a1,a2;b1,b2] =

a1 · b1
a1 · b2
a2 · b1
a2 · b2
·
(1.52)
so that the equation
Ω(a1,a2) = sgn[a1,a2;b1,b2]Ω(b1,b2)
(1.53)
decides whether the two doublets (a1,a2) and (b1,b2) have the same or opposite
orientations.
Exercise
Let ˆe1, ˆe2 be an orthonormal basis in a plane. Show that the doublets ˆe1, ˆe2 and
ˆe2, ˆe1 have opposite orientations.
Solution
We have
[ˆe1, ˆe2; ˆe2, ˆe1] =

ˆe1 · ˆe2
ˆe1 · ˆe1
ˆe2 · ˆe2
ˆe2 · ˆe1
 = −1,
(1.54)
so that,
Ω(ˆe2, ˆe1) = −Ω(ˆe1, ˆe2).
□
1.16.1
Orientation of a plane
To orient a plane π we set up a 2-D coordinate system given by a pair of orthonormal
vectors ˆe1, ˆe2 and deﬁne the orientation of the oriented plane π∗by
Ω(π∗) = Ω(ˆe1, ˆe2).
(1.55)
Any two linearly independent vectors (a1,a2) in the plane are oriented positively if
Ω(a1,a2) = Ω(π∗) = Ω(ˆe1, ˆe2).
Thus, all doublets positively oriented with respect to the basis (ˆe1, ˆe2) are positively
oriented with respect to π∗.
An oriented plane π∗can be characterized by a distinguished positive sense of rotation.
If a pair of vectors a,b is oriented positively with respect to π∗, the positive sense of rotation
of π∗is the sense of rotation by an angle less than π radians that takes the direction of a
into that of b.

Getting Concepts and Gathering Tools
73
Just as we can orient a plane, we can orient a 3-D region σ by specifying an orthonormal
basis (ˆh1, ˆh2, ˆh3) and deﬁning the orientation of the oriented region σ∗by
Ω(σ ∗) = Ω(ˆh1, ˆh2, ˆh3).
All triplets which are positively oriented with respect to this basis are positively oriented
with respect to σ∗. When an oriented plane π∗lies in an oriented 3-D region σ∗, we can
deﬁne the positive and negative sides of π∗. We take two independent vectors b and c in π∗
that are positively oriented:
Ω(b,c) = Ω(π∗).
A third vector a, independent of b,c is said to point to the positive side of π∗if
Ω(a,b,c) = Ω(σ∗).
Since σ ∗is oriented positively with respect to a Cartesian coordinate system, we can replace
this condition by
det(a,b,c) > 0.
If σ ∗is oriented positively with respect to a right handed coordinate system, then the
positive side of an oriented plane π∗is the one from which the positive sense of rotation in
π∗appears counterclockwise.

2
Vectors and Analytic Geometry
Analytic geometry is the representation of curves and surfaces by algebraic equations. If
this representation is in R3, where each point in space and hence each position vector is
represented by an ordered triplet of scalars, (that is, by coordinates), the corresponding
equations representing geometrical objects involve coordinates of points on these objects.
In such a case, analytic geometry is aptly called coordinate geometry. In this section, we
try and work with E3, in a coordinate free way to obtain equations for various geometrical
curves and surfaces. Since we axiomatize that both R3 and E3 are faithful representations
of real space in which objects move, the equations we derive are supposed to represent the
paths of moving particles or the surfaces conﬁning their motions. In reality we do not deal
with point particles, therefore, the mathematical curves and surfaces described by these
equations are approximations to the actual motions.
2.1
Straight Lines
Geometry, as we practice it today, is based on straight lines and planes as the basic
elements to be used to build other forms of curves and surfaces. Therefore, we start by
ﬁnding out equations for the straight lines and planes. From the deﬁnition of the vector
product, given a ﬁxed vector u , 0, all the points with position vectors x satisfying
x × u = 0
(2.1)
lie on the straight line on which u lies. Since x = 0 satisﬁes this equation, this line passes
through the origin. Replacement of x by (x −a) in Eq. (2.1) for ﬁxed vector a, rigidly
displaces each point on the line given by Eq. (2.1) by the same vector a. The resulting line
is in the direction ˆu and passing through the point a, given by the equation
(x −a) × u = 0.
(2.2)
Each possible straight line in space is described by Eq. (2.2) for some a and u. We denote
the set of all points on the line by L , that is, L = {x}.

Vectors and Analytic Geometry
75
Exercise
From Eq. (2.2) derive the following equations for the line L in terms of
rectangular coordinates in E3:
x1 −a1
u1
= x2 −a2
u2
= x3 −a3
u3
,
where xk = x · ˆσk,ak = a · ˆσk,uk = u · ˆσk, k = 1,2,3 and ˆσ1, ˆσ2, ˆσ3 an orthonormal basis.
Hint
[(x −a) × u] · ˆσ3 = (x1 −a1)u2 −(x2 −a2)u1 etc.
□
Exercise
(a) Show that Eq. (2.2) is equivalent to the parametric equation
x = a + λˆu.
(b) Describe the solution set {x = x(t)} of the parametric equation
x = a + t2u
for all scalar values of the parameter t.
Hint
(a) Equation (2.2) implies (x −a) · ˆu ≡λ.
(b) {x} = half line with the direction u and endpoint a.
□
Dividing Eq. (2.2) by |u| and taking the constant term on the right, we get,
x × ˆu = a × ˆu.
We take the vector product on both sides from the left with ˆu and use identity I to get
x = a −(ˆu · a)ˆu + (ˆu · x)ˆu
= d + (ˆu · x)ˆu,
(2.3)
which we take to be the deﬁnition of the vector d. Noting that d · ˆu = 0 we get, for the
length of vector x,
x2 = d2 + (ˆu · x)2.
This distance is minimum for x = d or ˆu · x = 0. This minimum distance is simply the
distance of the line from the origin and is given by d = |d| (see Fig. 2.1). We call d the
directance [10](the directed distance) from the origin 0 to the line L . Its magnitude is
called the distance from the origin to the line L . Note that d can be obtained from any
point x on the line by subtracting the component of x along ˆu from x (Eq. 2.3).
The vector
m = x × ˆu

76
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 2.1
Line L with directance d = x −(ˆu · x)ˆu
is called the moment of the line L . Figure 2.2 shows that the magnitude |m|, which is the
area of the parallelogram spanned by x and ˆu, is the same for all points x on the line and
equals the distance d = |d| of the line from the origin O. Thus, any oriented line L is
uniquely determined by specifying the direction ˆu and its moment m, or by specifying a
single quantity L = ˆu + d × ˆu.
Fig. 2.2
|m| = |x × ˆu| = |d| for all x on the line L
The equation to a line can be expressed in terms of a pair of points on it, which determines
the relations between such pairs of points. In order to get such an equation, we note that
Eq. (2.2) is equivalent to the statement that the segment x −a is collinear with the vector
u. Since x and a are any two points on the line, it follows that all segments of the line are

Vectors and Analytic Geometry
77
collinear. If x, a, b are any three points on the line, the collinearity of the segments x −a
and b −a is expressed by the equation
(x −a) × (b −a) = 0.
(2.4)
This differs from Eq. (2.2) in that u is replaced by the segment b −a which is proportional
to u. Thus, Eqs (2.2) and (2.4) are equivalent provided a and b are distinct points on the
line.
Exercise
Find the directance to the line through points a and b (a) from the origin and
(b) from an arbitrary point c.
Answer
(a) ((b −a) · a)b −((b −a) · b)a
|b −a|2
.
(b) Shift the origin to c. We get, (b −a) · (a −c)(b −c) −(b −a)2(a −c)
|b −a|2
.
□
Exercise
Show that the distance from an arbitrary point A to the line BC is
|a × b + b × c + c × a|
|b −c|
where a,b,c are the position vectors of points A,B,C respectively, with respect to some
origin O, (see Fig. 2.3).
Fig. 2.3
See text
Solution
Let d be the vector from A perpendicular to b−c (see Fig. 2.3). We want to ﬁnd
|d|. We can write
|d| = |d × (c −b)|
|b −c|

78
An Introduction to Vectors, Vector Operators and Vector Analysis
as d and c −b are orthogonal. Next, check that
d = −a + c + λ(b −c), λ a scalar,
d × c = −a × c + λ(b × c),
d × b = −a × b + c × b −λ(c × b).
Thus,
d × (c −b) = c × a + a × b + b × c
and substituting in the equation for |d| above, the result follows.
□
Exercise
Let ˆu, ˆv, ˆw be the directions of three coplanar lines. The relative directions of
lines are then speciﬁed by α = ˆv · ˆw, β = ˆu · ˆw, γ = ˆu · ˆv. Show that 2αβγ = α2 + β2 +
γ2 −1.
Solution
Since ˆu, ˆv, ˆw are coplanar, they are linearly dependent, so that their Gram
determinant must vanish (see Appendix A). Thus we have,
Γ (ˆu, ˆv, ˆw) =

1
γ
β
γ
1
α
β
α
1

= 0
Expanding the determinant, we get the result.
□
Fig. 2.4
See text
Exercise
Find the parametric values λ1,λ2 for which the line x = x(λ) = a + λˆu
intersects the circle whose equation is x2 = r2 and show that λ1λ2 = a2 −r2 for every
line through a which intersects the circle.

Vectors and Analytic Geometry
79
Solution
At the points of intersection r2 = (a+λˆu)2. Thus, the corresponding values of
λ satisfy λ2+2λa· ˆu+(a2−r2) = 0, or, λ1,2 = −a· ˆu ±
p
r2 + (a · u)2 −a2. We know that
the product of the roots of the quadratic ax2 + bx + c is c/a so that the above quadratic in
λ gives λ1λ2 = a2−r2. If the line is a tangent to the circle, r2 +(a·u)2 = a2 and λ1 = λ2.
These results are valid for any line intersecting the circle.
□
Exercise
Show that the vector s (vector −−→
BC in Fig. 2.4) along the perpendicular dropped
on the line (x −a) × ˆt = 0 from the point B with position vector b (see Fig. 2.4) is given by
s = ˆt × [ˆt × (b −a)].
Solution
−s = b −a + c where c is given by the projection of b −a on the line Fig. 2.4.
Now c = −[(b −a) · ˆt]ˆt so that, using identity I we get,
−s = b −a −[(b −a) · ˆt]ˆt
= (ˆt · ˆt)(b −a) −[(b −a) · ˆt]ˆt
= ˆt × [(b −a) × ˆt],
(2.5)
which is what we wanted to prove.
□
If we expand the the vector product in Eq. (2.4) using the distributive rule and multiply
each term by 1
2 we get
1
2(a × b) = 1
2(a × x) + 1
2(x × b).
(2.6)
Now 1
2(a × b) is the directed area of a triangle with vertices a, 0, b and sides given by a,
b, b −a. The other two terms in Eq. (2.6) can be interpreted similarly. We note that any
two of these three triangles have one side in common. Thus, Eq. (2.6) expresses the area of
a triangle as the sum of the areas of two triangles into which it can be decomposed. This
is depicted in Fig. 2.5(a) when x lies between a and b and in Fig. 2.5(b) when it does not.
From Eq. (2.6)
(a × b) · x = 0
(2.7)
which means that all three vectors and the three triangles they determine are in the same
plane. We deﬁne the vectors
B ≡1
2(a × x)
A ≡1
2(x × b),
(2.8)

80
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 2.5
With A and B deﬁned in Eq.
(2.8) (a) |a × b| = |A| + |B| and (b) |a ×
b| = |B| −|A|. These equations can be written in terms of the areas of the
corresponding triangles
whose magnitudes equal the areas of the corresponding triangles. These areas are depicted
in Figs 2.5(a) and 2.5(b). Note that the orientation of A and hence, the sign of A is opposite
in the two ﬁgures.
Since the segments of a line are all collinear, we can write
a −x = λ(x −b)
(2.9)
where λ is a scalar. Taking absolute values on both sides,
|λ| = |a −x|
|x −b| or, λ = ± |a −x|
|x −b|.
Again, the vector product of Eq. (2.9) with x gives
a × x = λ(x × b).
Absolute values on both sides yield
λ = ± |a × x|
|x × b| = ±|B|
|A| = ± B
A.
We thus get,
λ = ± |a −x|
|x −b| = ±|B|
|A| = ± B
A,
(2.10)
where the positive sign applies if x is between a and b and the negative sign applies if it is
not. The point x is called the point of division for the oriented line segment [a,b] and as
per Eq. (2.10), x is said to divide [a,b] in the ratio B/A. The division ratio λ parameterizes
the segment from a to b to give
x = a + λb
1 + λ ,
(2.11)

Vectors and Analytic Geometry
81
as can be obtained by solving Eq. (2.9). Thus, the midpoint of the segment [a,b] is deﬁned
by λ = 1 and is given by 1
2(a + b).
Equation (2.11) can be written as
x = Aa + Bb
A + B .
(2.12)
The scalars A and B in Eq. (2.12) are called homogeneous (line) coordinates for the point
x. They are also called barycentric coordinates because of the similarity of Eq. (2.12) to the
formula for center of mass of a rigid body. Unlike mass, however, the scalars A and B can
be negative and can be interpreted geometrically as oriented areas.
Exercise
Prove that three points a,b,c lie on a line if and only if there are non-zero
scalars α,β,γ such that αa + βb + γc = 0 and α + β + γ = 0.
Hint
This is an immediate consequence of Eq. (2.12).
□
The parameter λ is invariant under the shift in origin from O to O′ by a vector c as depicted
in Fig. 2.6. We have, with respect to the new origin,
λ = ± |a −c −x + c|
|x −c −b + c| = ± |a −x|
|x −b|.
This means (see Fig. 2.6)
λ = B
A = B′
A′ = B ± B′
A ± A′ .
(2.13)
Fig. 2.6
A′ = (x −c) × (b −c) and B′ = (a −c) × (x −c) (see text)
For the special case when c is collinear with x, we have one of the three cases depicted
in Figs 2.7(a),(b),(c). All the three cases validate Eq. (2.13). The point x also divides the
segment [c,0] in a ratio λ′ given by
λ′ = ±|c −x|
|x|
= B
B′ = A
A′ = A ± B
A′ ± B′ .
(2.14)

82
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 2.7
Case of c parallel to x
The point x is the point of intersection of the line through points [c,0] with the line through
the points [a,b]. To get it we proceed as follows. Since c is collinear with x, we have,
λ′ = ±|c −x|
|x|
= (c −x) · ˆx
x
,
or, rearranging the terms and again using the fact that c is collinear with x,
x =
c
1 + λ′ ,
(2.15)
which gives us the point of intersection in terms of the vector c and the ratio λ′. From
Fig. 2.8 we see that
A + B = 1
2|a × b|
and
A′ + B′ = 1
2|(a −c) × (b −c)|.

Vectors and Analytic Geometry
83
These equations, when coupled with Eq. (2.14) give
λ′ =
|a × b|
|(a −c) × (b −c)|·
(2.16)
Equations (2.15) and (2.16) determine x in terms of vectors a,b,c. They determine point x
in Fig. 2.8 and by interchanging a and c, they determine the point y in the same ﬁgure.
Fig. 2.8
See text
2.2
Planes
The algebraic description of a plane is similar to that of a line.
We set up an orthonormal basis in the plane, say ˆσ1, ˆσ2. We call such a plane ˆσ1, ˆσ2
plane. Let a denote a ﬁxed point on the plane. Then, every point x on the plane must satisfy
(see Fig. 2.9)
(x −a) · ( ˆσ1 × ˆσ2) = 0.
(2.17)
Fig. 2.9
A plane positively oriented with respect to the frame (ˆi,ˆj, ˆk)

84
An Introduction to Vectors, Vector Operators and Vector Analysis
If we put ˆσ3 = ˆσ1× ˆσ2 then ˆσ1, ˆσ2, ˆσ3 form an orthonormal basis in 3-D space. ˆσ3 deﬁnes
the orientation of the plane which is positive if the triplet ˆσ1, ˆσ2, ˆσ3 is oriented positively
with respect to the orthonormal coordinate system based at the origin O (see Fig. 2.9).
Exercise
Show that the distance of the plane from the origin is given by |d| = |a· ˆσ3|. The
directance is given by the vector d along the line perpendicular to the plane and passing
through the origin, directed away from the origin.
□
Exercise
Show that three points not on a line determine a plane, by obtaining an equation
for the plane passing through three points a,b,c.
Answer
(x −a) · [(b −a) × (c −a)] = 0.
□
Exercise
Four points a,b,c,d determine a tetrahedron with directed volume V = 1
6
(b −a)× (c −a) · (d −a). Use this to determine the equation for a plane through three
distinct points a,b,c.
Answer
We make the fourth point d the variable x and require that a,b,c,x lie on
the same plane so that V = 0. The resulting equation for the plane is (b −a) ×
(c −a) · (x −a) = 0.
□
Algebraically, a plane is deﬁned as the locus of points P (x1,x2,x3) in the three dimensional
space R3 satisfying a linear equation of the form
a1x1 + a2x2 + a3x3 = c,
(2.18)
where a1,a2,a3 do not all vanish. Introducing the vector a ≡(a1,a2,a3), (a , 0) and the
position vector x = −−→
OP ≡(x1,x2,x3) of the point P , we can write Eq. (2.18) as a vector
equation:
a · x = c
(2.19)
Let y = −−−→
OQ ≡(y1,y2,y3) be the position vector of a particular point Q on the plane so
that a · y = c. Subtracting this from Eq. (2.19) we see that the points P of the plane satisfy
0 = a · (x −y) = a · −−→
P Q .
(2.20)
Thus, the vector a is perpendicular to the line joining any two points on the plane. The
plane consists of the points obtained by advancing from any one of its points Q in
all directions perpendicular to a. The direction of a is called normal to the plane
(see Fig. 2.10).
The plane described by Eq. (2.19) divides space into two open half-spaces given by a·x <
c and a · x > c. The vector a points into the half space a · x > c. Thus, a ray from a point Q
of the plane in the direction of a comprises points whose position vectors x satisfy a·x > c.
The position vectors x of points P on such a ray are given by
x = −−→
OP = −−−→
OQ + λa = y + λa

Vectors and Analytic Geometry
85
where y is the position vector of Q and λ is a positive number. Dotting this equation by a
gives,
a · x = c + λ|a|2 > c.
In general, any vector b forming an acute angle with a points into the half space a · x > c,
since a · b > 0 means
a · x = a · y + λa · b > c.
If c > 0, the half-space a · x < c contains the origin as a · 0 = 0 < c. Then the direction of a
or the direction of the normal is away from the origin.
Fig. 2.10
Every line in the plane is normal to a
Equation (2.19) describing a given plane is not unique. It can be replaced by (λa) · x =
λc, λ , 0. We can choose λ to be λ = sgn c
|a|
to cast the equation to the given plane in the
normal form
ˆa · x = d
where d > 0 is a constant and ˆa is the unit normal vector pointing away from the origin.
The constant d is the distance of the plane from the origin. To see this, note that the distance
of an arbitrary point on the plane with position vector x is |x| ≥ˆa · x = d, where equality
holds for x = dˆa. The distance d(Q) of a point Q in space with position vector y from the
plane is then |ˆa·y−d|. As an example, consider a plane wave with wave vector k propagating
in the direction ˆk. The phase of a plane wave is given by k · r where r is the position vector
of a point on the wave. For a plane wave a surface of constant phase is a plane, because the
equation to such a surface must be ˆk · r = c. Such a plane is perpendicular to ˆk as shown
in Fig. 2.11.

86
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Find the equation to the plane passing through (4,−1,2) and perpendicular to
the planes 2x −3y + z = 4 and x + 2y + 3z = 5.
Solution
The equation to the plane can be written in the form
x · ˆn = d,
where x is the position vector of a point on the plane, ˆn is unit vector normal to the plane
and pointing into the region x· ˆn > d and d is the distance of the plane from the origin. ˆn is
given to be perpendicular to the vectors 2ˆi−3ˆj+ ˆk and ˆi+2ˆj+3ˆk, so that its dot product
with these vectors must vanish, which means
2n1 −3n2 + n3 = 0,
n1 + 2n2 + 3n3 = 0
Fig. 2.11
As seen from the ﬁgure, for every point on the plane ˆk · r = constant
and ˆn being a unit vector, n2
1 + n2
2 + n2
3 = 1. Solving this system we get,
n1 = ±11
√
195
, n2 =
±5
√
195
, n3 =
∓7
√
195
.
Thus, the required equation becomes
x · ˆn =
 11x + 5y −7z
√
195
!
= d.
Since the point (4,−1,2) lies on the plane,
d = ±39 −5 −14
√
195
= ± 25
√
195
.

Vectors and Analytic Geometry
87
Regardless of which sign is used we get the required equation
11x + 5y −7z = 25.
□
Exercise
Find the equation to a plane which passes through the line of intersection of
two planes which are equidistant from the origin.
Solution
The equations to the given planes are x · ˆn1 = d = x · ˆn2 since both are
equidistant from the origin. The points lying on both these planes satisfy the linear
combination of their equations
x · (ˆn1 + µˆn2) = d + µd = (1 + µ)d
where µ is a parameter. However, each term on the LHS of this equation can be taken
to equal ±d. Choosing both to equal +d makes µ = +1 and one +d and the other −d
makes µ = −1. In the ﬁrst case the required equation becomes x · (ˆn1 + ˆn2) = 2d while
in the second case the equation is x · (ˆn1 + ˆn2) = 0 which is a plane passing through
the origin.
□
Exercise
Find an expression for the angle between two planes given by x · ˆn1 = d1 and
x · ˆn2 = d2.
Solution
The angle between two planes is the angle θ between their unit normals and is
given by cosθ = ˆn1 · ˆn2 = λ1λ2 + µ1µ2 + ν1ν2, where (λ1,µ1,ν1) and (λ2,µ2,ν2) are
the direction cosines of ˆn1 and ˆn2 respectively.
□
Exercise
Find the equation to a plane containing a line and parallel to a vector.
Solution
Let the plane contain a line x = u + λv, λ being a parameter and parallel to
a given vector ω. Thus, the plane passes through a point with position vector u and is
perpendicular to v × ω. Its equation is
(x −u) · (v × ω) = 0 or x · v × ω = u · v × ω.
□
Exercise
Find the shortest distance between two skew lines as well as the equation to the
corresponding line.
Solution
Skew lines is a pair of lines which are neither parallel nor intersecting. Let L1
and L2 be two skew lines with equations
L1 : x = u + λs and L2 : x = v + µt,
λ,µ being parameters. Thus, L1 passes through the point A with position vector u and is
parallel to vector s and L2 passes through the point B with position vector v and is parallel
to vector t (see Fig. 2.12). Let the segment P Q, joining points P and Q on the lines L1
and L2 respectively, give the shortest distance between them. Then P Q is perpendicular
to both the lines and hence, it is parallel to the cross product of the vectors s and t. The
segment P Q perpendicular to both the lines is unique, because if there was another such

88
An Introduction to Vectors, Vector Operators and Vector Analysis
segment, it would be parallel to P Q, making L1 and L2 parallel. The shortest distance is
the projection of AB speciﬁed by the vector v −u on P Q that is, on the unit vector s×t
|s×t|.
Therefore, we have,
d(P ,Q) =
(v −u) · s × t
|s × t|
.
Fig. 2.12
Shortest distance between two skew lines
Note that when d(P ,Q) = 0 we get
(v −u) · (s × t) = 0, or v · (s × t) = u · (s × t),
which is the condition of intersection of two lines.
To ﬁnd the equation to the line of shortest distance we note that the vector p joining
P Q is
p = [(v −u) · s × t]s × t
|s × t|2
.
Thus, the line passing through u and parallel to p is given by
(x −u) × ˆp = 0,
where ˆp is the unit vector along p.
□
Exercise
Find the equation to the line of intersection of the two planes (x −a) · ˆn1 = 0
and (x −b) · ˆn2 = 0 where ˆn1, ˆn2 are unit vectors normal to the respective planes.
Answer
(x −c) × (ˆn2 −ˆn1) = 0, where
c =
1
|ˆn1 × ˆn2|2 [(a · ˆn2)ˆn1 × (ˆn2 × ˆn1) + (b · ˆn1)ˆn2 × (ˆn1 × ˆn2)].
□

Vectors and Analytic Geometry
89
Exercise
Find the radius vector s of the point of intersection of three planes (x−a)·ˆn = 0,
(x−b)· ˆm = 0 and (x−c)· ˆp = 0, where ˆn, ˆm, ˆp are the unit vectors normal to the respective
planes and ˆn · ( ˆm × ˆp) , 0.
Answer
s =
1
ˆn · ( ˆm × ˆp)[(a · ˆn) ˆm × ˆp + (b · ˆm)ˆp × ˆn + (c · ˆp)ˆn × ˆm].
□
2.3
Spheres
Spheres form another instance of elementary geometrical ﬁgures. A sphere with radius r
and center c is the set of all points x ∈E3 satisfying the equation
|x −c| = r or (x −c)2 = r2.
(2.21)
The vectors {x−c} satisfying Eq. (2.21) and also the constraint ˆr·(x−c) = constant, where
ˆr is a unit vector based at the center, trace out a circle on the sphere and can be taken to be
the deﬁning equation for the circle.
As an example of applying vectors to sphere, we derive a basic result in spherical
trigonometry. For simplicity we deal with a unit sphere S with its center at the origin O
given by r2 = 1. If A,B,C are any three points on S, then we call the intersection of the
planes OAB,OAC,OBC with S a spherical triangle (see Fig. 2.13).
Fig. 2.13
A spherical triangle
The metric we adopt on S is that of the Euclidean space embedding S, so that the ‘length’
of the side AB is determined by the angle AOB = γ. In fact, these angles α,β,γ, which are
subtended by the sides BC,CA and AB at O give precisely the desired lengths if they are
expressed in radians that is, as a fraction of 2π (see section 1.2). We deﬁne the angle A at
the vertex A of the spherical triangle ABC to be that between the tangents AD and AE to
the great circles AB and AC. Note that the complementary parts of the great circles passing

90
An Introduction to Vectors, Vector Operators and Vector Analysis
through AB,BC and CA also form a spherical triangle ABC. We can specify the triangle
in Fig. 2.13 by requiring that every angle of the triangle ABC has to be less than π.
We wish to prove the identity
cosα = cosγ cosβ + sinγ sinβ cosA.
To this end we use identity II by replacing c by a and d by c. We get, remembering that all
vectors are unit vectors,
(ˆa × ˆb) · (ˆa × ˆc) = (ˆb · ˆc) −(ˆa · ˆc)(ˆb · ˆa).
The angle between (ˆa × ˆb) and (ˆa × ˆc) is the dihedral angle between the planes OAC and
OAB, that is, angle A. Further,
|ˆa × ˆb| = sinγ,
|ˆa × ˆc| = sinβ,
(ˆa × ˆb) · (ˆa × ˆc) = sinγ sinβ cosA,
ˆb · ˆc = cosα,
(ˆa · ˆb)(ˆa · ˆc) = cosγ cosβ.
which gives the required result.
Exercise
Show that, for a spherical triangle ABC, as in Fig. 2.13.,
sinA
sinα = sinB
sinβ = sinC
sinγ =
σ
sinα sinβ sinγ ,
where
σ = 2[sinssin(s −α)sin(s −α)sin(s −α)]1/2 ; s = α + β + γ.
Hint
First get |(ˆa × ˆb) × (ˆa × ˆc)| = |ˆa × ˆb||ˆa × ˆc|sinA = sinγ sinβ sinA. Then evaluate
|(ˆa× ˆb)×(ˆa×ˆc)| differently to obtain a quantity σ which is invarient under any permutation
of vectors ˆa, ˆb, ˆc.
□
2.4
Conic Sections
Next we consider an important set of planar curves called conic sections because each one
of them can be obtained as an intersection of a cone with a plane.

Vectors and Analytic Geometry
91
We prefer the following alternative deﬁnition because it leads to the generic parametric
equation which applies to all the conic sections. A conic is the set of all points in the
Euclidean plane E2 with the following property: The distance of each point from a ﬁxed
point (the focus) is in ﬁxed ratio (the eccentricity) to the distance of that point from a
ﬁxed line (the directrix). This deﬁnition can be expressed as the equation deﬁning the
conic in the following way. Denote the eccentricity by e, the directance from the focus to
the directrix by d = dˆe (ˆe2 = 1) and the directance from the focus to any point on the
conic by r (see Fig. 2.14). The deﬁning condition for the conic can then be written
e =
|r|
d −r · ˆe.
Solving this for r = |r| and introducing the eccentricity vector e = eˆe along with the so
called semi-latus rectum l = ed, we get the equation
r =
l
1 + e · ˆr.
(2.22)
Fig. 2.14
Depicting Eq. (2.22)
This expresses the distance r from the focus to a point on the conic as a function of the
direction ˆr to that point. Equation (2.22) can also be expressed as a parametric equation
for r as a function of the angle θ between e and ˆr. This equation is obtained by substituting
e · ˆr = ecosθ into Eq. (2.22). We get
r =
l
1 + ecosθ.
(2.23)
This is the standard equation for conics however, we usually prefer Eq. (2.22) as it is an
explicit function of vectors and their scalar product, so that it shows the dependence of r
on the directions ˆe and ˆr explicitly.
Equation (2.22) traces a curve when r is restricted to the directions in a plane however,
if r is allowed to range over all directions in E3 then Eq. (2.22) describes a two
dimensional surface called conicoid. Our deﬁnition of a conic can be used for a conicoid
by redeﬁning the directrix as a plane instead of a line. Different ranges of values of
eccentricity correspond to different conics or conicoid as shown in Table 2.1.

92
An Introduction to Vectors, Vector Operators and Vector Analysis
Table 2.1
Classiﬁcation of Conics and Conicoids
Eccentricity
Conic
Conicoid
e > 1
Hyperbola
hyperboloid
e = 1
parabola
paraboloid
0 < e < 1
ellipse
ellipsoid
e = 0
circle
sphere
Fig. 2.15
Conics with a common focus and pericenter
Figure 2.15 shows the 1-parameter family of conics with a common focus and pericenter.
The pericenter is the point on the conic at which r has the minimum value. For a hyperbola,
there are two pericenters, one on each branch of hyperbola. Only one of these is shown in
Fig. 2.15. If the conics in Fig. 2.15 are rotated about the axis joining the focus and the
pericenter, they “sweep out” corresponding conicoids.
Exercise
Parametric curves x = x(λ) of the second order are deﬁned by equation
x = a0 + a1λ + a2λ2
α0 + α1λ + α2λ2 .
Note that this generalizes Eq. (2.11) for a line. By the change of parameters λ →λ−
α1/2α2, this can be reduced to the form
x = a0 + a1λ + a2λ2
α + λ2
.
Show that
(a) For α = 1, the change of parameters λ = tan 1
2φ can be used to put this equation in
the form
x = acosφ + bsinφ + c

Vectors and Analytic Geometry
93
which is the general equation for an ellipse.
(b) For α = −1, λ = tanh 1
2φ gives
x = acoshφ + bsinhφ + c.
which is the general equation for a hyperbola.
Actually ultimate conclusion you can draw turns out to be true: All conics are second
order curves and conversely.
Hint
(a) mf a0 = a + c, a1 = 2b, a2 = c −a cosφ = 1−λ2
1+λ2 , sinφ =
2λ
1+λ2 .
□
Conics and conicoids can be described in many different ways which disclose a variety of
their remarkable properties. However, any discussion of these issues will take us far away
from the main theme of this book. These are discussed at length in various books on
mechanics and geometry [9, 18].

3
Planar Vectors and Complex
Numbers
The purpose of this chapter is to demonstrate how the geometry on a plane can be
effectively described using the set of complex numbers in place of planar vectors. We
choose the circle as the planar curve to be analysed for this purpose.
3.1
Planar Curves on the Complex Plane
Instead of vectors, the complex numbers and their algebra [1]1 can be used to describe
curves on a complex plane Z. Basically, we have to use the trivial isomorphism between
E2 and Z (see Fig. 3.1):
z = x + iy ↔xˆx + y ˆy = r
or,
z = reiθ; −π < θ < π, r ≥0, ↔r(eiθ ˆx) = r
where eiθ ˆx is the direction obtained by rotating vector ˆx by θ counterclockwise if θ > 0
and clockwise if θ < 0 and e±iπ ˆx = −ˆx.
Exercise
Show that the above map is both one to one and onto.
□
The required isomorphism is easily established by
z1 + z2 = (x1 + x2) + i(y1 + y2) ↔(x1 + x2)ˆx + (y1 + y2)ˆy
= (x1ˆx + y1ˆy) + (x2ˆx + y2ˆy) = r1 + r2
1We assume that the reader is familiar with the algebra of complex numbers.

Planar Vectors and Complex Numbers
95
Fig. 3.1
Isomorphism between the complex plane Z and E2
and
az ↔ar,
where a is a scalar (real number) and r and z are the images of each other under the
isomorphism.
Thus, the set of vectors on a plane can be replaced by the set of complex numbers having
richer algebraic structure, as each complex number has a multiplicative inverse and there is
a unique identity element with respect to their product (z1z2 = 1 implies z2 = 1/z1 and
z1 = 1/z2). Due to this isomorphism, we may use the same symbol z to denote a complex
number as well as a planar vector.
Fig. 3.2
Finding evolute of a unit circle
The product of two complex numbers z1 = r1eiθ1 and z2 = r2eiθ2 is z = z1
z2 = r1r2ei(θ1+θ2). Thus, the absolute value of the product is the product of the absolute
values of the factors, while the argument of the product is the sum of the arguments of the

96
An Introduction to Vectors, Vector Operators and Vector Analysis
factors. In particular, squaring a vector z doubles the argument, while taking the square
root halfs the argument. As an example we multiply the function f (z) = 1 −iu by the
function exp(iu) (u real). The graph of 1 −iu is a straight line parallel to the y-axis
passing through the point z = 1. This line is tangent to the unit circle at the point z = 1,
as depicted in Fig. 3.2. By rotating this line over the angle u it remains a tangent moving
the point A in Fig. 3.2 to the point C. Since BC equals u, the arc length of the circle, the
locus of the point C represented by the equation
z = (1 −iu)exp(iu)
is evidently the evolute of the circle (see subsection 9.2.5).
Exercise
Show that ii = exp(−π/2).
Hint
Raise the equation i = exp(iπ/2) to the ith power.
□
Exercise
Show that logi = iπ/2.
Hint
Take logarithms on both sides of ii = exp(−π/2).
□
Exercise
Show that
√
i = ± 1
√
2(1 + i) and
√
−i = ± 1
√
2(1 −i).
Hint
see Fig. 3.3.
□
Fig. 3.3
Finding
√
i
Exercise
Show that the numbers whose nth power is unity are given by z = exp

i 2πk
n

(k = 1,...,n). These are the n values of nth roots of unity,
n√
1.
Hint
Divide the circumference of the unit circle by n to ﬁnd the points whose nth
power is unity, obtained by performing one or more complete turns over the unit circle
(see Fig. 3.4).
□
The complex conjugate of a complex number z = x + iy = r exp(iθ) is given by z∗= x −
iy = r exp(−iθ). The point z∗is obtained by reﬂecting the point z in the x axis, as shown
in Fig. 3.5. We easily check that the real and imaginary parts of z are x = R(z) = 1
2(z+z∗)
and y = I(z) = −i 1
2(z −z∗).

Planar Vectors and Complex Numbers
97
Fig. 3.4
Finding nth roots of unity
Fig. 3.5
z, z∗,z ± z∗
Exercise
Find the real and imaginary parts of z = (1 −iu)exp(iu).
Answer
x = cosu + u sinu,
y = sinu −u cosu.
□
The sum of any two conjugate numbers or functions is real while their difference is
imaginary. For any complex valued function f (u), z = exp[f (u) −f ∗(u)] and
z = exp[i(f (u)+ f ∗(u))] are points on the unit circle.
Again, we easily ﬁnd that the modulus or the absolute value of a complex number z =
|z|exp(iθ) is given by |z| =
√
zz∗and its argument is obtained from exp(iθ) =
p z
z∗. Note
that any function which is the quotient of two conjugate functions must have unit modulus
:
 z
z∗
 = 1, because z
z∗z∗
z = 1.
Exercise
Show that for the function (1 −iu)exp(iu)
|z|2 = 1 + u2

98
An Introduction to Vectors, Vector Operators and Vector Analysis
and
exp(iθ) =
r
1 −iu
1 + iu exp(iu).
Note that the function
q
1−iu
1+iu has unit modulus.
□
The inverse of a complex number z = |z|exp(iθ), with respect to the product of complex
numbers, is given by 1
z =
1
|z|exp(−iθ) because their product is z( 1
z ) = 1. Thus, the
quotient of two vectors z1 and z2 is given by z1
z2 = |z1|
|z2|exp{i(θ1 −θ2)}. If the two vectors
are parallel, θ1 −θ2 = 0 and the quotient is purely real. The imaginary part vanishes in
this case, so that
z1
z2
−z∗
1
z∗
2
= 0 or z1z∗
2 −z∗
1z2 = 0.
(3.1)
This is the criterion for parallel vectors in a plane.
For a pair of orthogonal vectors, on the other hand, we have, θ1 −θ2 = ±π/2,
exp{i(θ1 −θ2)} = ±i so that z1/z2 has no real part, leading to
z1
z2
+ z∗
1
z∗
2
= 0 or z1z∗
2 + z∗
1z2 = 0
(3.2)
which is the criterion for orthogonal vectors in a plane.
These criteria are closely related to the vector and the scalar products of two planar
vectors. The magnitude of the vector product of two vectors z1 and z2 is the area of the
parallelogram formed by them. This is
A = |z1| |z2|sin(θ1 −θ2).
However,
z1z∗
2 −z∗
1z2 = 2i|z1| |z2|sin(θ1 −θ2),
so that the area of the parallelogram is
A = 1
2i (z1z∗
2 −z∗
1z2) = I(z1z∗
2).
Similarly, we get for the scalar product B:
B = |z1| |z2|cos(θ1 −θ2) = 1
2(z1z∗
2 + z∗
1z2) = R(z1z∗
2).
Thus, the scalar and the vector products turn out to be the real and imaginary parts of the
complex vector product z1z∗
2:
z1z∗
2 = B + iA.

Planar Vectors and Complex Numbers
99
Expressed in terms of x and y,
A = x1y2 −x2y1,
B = x1x2 + y1y2.
3.2
Comparison of Angles Between Vectors
Proportionality of four vectors (see Fig. 3.6)
z1
z2
= z3
z4
(3.3)
implies that moduli are proportional:
|z1|
|z2| = |z3|
|z4|
Fig. 3.6
Depicting Eq. (3.3)
and that the enclosed angles are equal: θ1 −θ2 = θ3 −θ4. The two triangles constructed
on z1,z2 and z3,z4 are similar. For equality of angles, it is enough to require
z1
z2
∝z3
z4
,
or
z1z4 ∝z2z3
with a real constant of proportionality. In the special case of z2 = z3 the two remaining
vectors make equal angles with the middle vector if
z1z2 ∝z2
2

100
An Introduction to Vectors, Vector Operators and Vector Analysis
with a real constant of proportionality. These rules are always employed to prove the
equality of angles in geometrical ﬁgures. Thus, for example, z2 =
p
f (u) bisects the angle
between z1 = f (u) and the real axis z3 = 1. Similarly, z2 =
p
if (u) bisects the angle
between z1 = f (u) and the imaginary axis z3 = i.
3.3
Anharmonic Ratio: Parametric Equation to a Circle
By the anharmonic ratio, cross ratio or double quotient D of four vectors we mean the
expression:
D = z1 −z3
z1 −z4
÷ z2 −z3
z2 −z4
.
(3.4)
D is in general complex and its argument is the difference of the arguments of z1−z3
z1−z4
and z2−z3
z2−z4 . If this difference is zero, that is, (see Fig. 3.7) if ∠z3z1z4 = ∠z3z2z4, D is real.
In this case, the four points will be situated on a circle and the criterion for the
concentric conﬁguration of four points is the reality of the cross ratio. Let three of the four
points be ﬁxed on the circle and let z4 move over it. Then, D assumes all the positive and
negative real values. The circle is then parameterized by D and the formula for the
circle passing through z1,z2,z3 is
D = z1 −z3
z1 −z ÷ z2 −z3
z2 −z .
(3.5)
Fig. 3.7
If D is real, z1,z2,z3,z4 lie on a circle
The value of the cross ratio depends on the order in which we take the four points. We
denote the sequence by writing D(1234) for the sequence chosen in the deﬁnition. We see
that interchanging 1 and 2 or 3 and 4 inverts the value. Interchanging 2 and 3 or 1 and 4
changes D into 1-D as can be checked by calculation. This leads to the rules named after
Mobius:

Planar Vectors and Complex Numbers
101
(i) D(1234) = D(3412) = D(2143) = D(4321) = δ
(ii) D(2134) = D(1243) = 1/δ
(iii) D(1324) = D(4231) = 1 −D(1234) = 1 −δ
(3.6)
and by further permutation of indices, the values 1 −1/δ,
1
1−δ and
δ
δ−1 can be obtained.
In case D is real, it represents the cross ratio of the lengths of the four vectors z1 −z3,
z1 −z4, z2 −z3 and z2 −z4.
Mobius’ third rule gives us the following famous result (see Fig. 3.7),
D(1234) = z1 −z3
z1 −z4
÷ z2 −z3
z2 −z4
= AD · BC
AC · BD = δ
D(1324) = z1 −z2
z1 −z4
÷ z3 −z2
z3 −z4
= AB · CD
AC · BD = 1 −δ.
(3.7)
Since the sum is 1 we get
AD · BC + AB · CD = AC · BD.
(3.8)
In words: The product of the diagonals of a quadrilateral inscribed in a circle equals the
sum of the products of the opposite sides.
3.4
Conformal Transforms, Inversion
A transformation ω = f (z), ω,z complex, makes one or more points of the complex ω
plane correspond to one or more points to the complex z plane. We assume that the
derivative dω/dz is a single valued function of z, that is, it is independent of the direction
of dz. In this case dω makes a constant angle with dz, this angle being the argument of
dω/dz. Two lines passing through z and making a certain angle with each other, will be
transformed into two lines passing through ω and making the same angle with each
other as the original ones. This is the deﬁning property of the so called conformal
transformations. Conformality means that inﬁnitely small polygons do not change their
shape under this transformation.
In general, dω/dz may vanish at ﬁnite number of z values (zeros of dω/dz) and may
blow up, or become inﬁnite, at some other ﬁnite number of z values (poles of dω/dz). At
neither of these two sets of points the argument of dω/dz is well deﬁned so that at these
points we may ﬁnd deviations from conformality of the transforms. Since these exceptional
points are ﬁnite in number, we call the corresponding transformation conformal. Thus, for
the transformation ω = √z, dω
dz =
1
2√z has a pole at z = 0 and a zero at z = ∞so that the
whole of z plane is transformed conformally except at z = 0 and z = ∞.

102
An Introduction to Vectors, Vector Operators and Vector Analysis
We consider here the transformation
ω = z −z0
z + z0
(3.9)
where z0 is a complex constant. This transformation plays a role in the problem of the
reﬂection of a plane wave travelling in a medium of wave-resistance z0, against a wall of
impedance z. The number ω is the complex reﬂection factor whose modulus is the ratio of
the amplitudes of the reﬂected and incident waves and the argument is the phase shift at
reﬂection.
The argument ∆of ω is constructed in Fig. 3.8.
Exercise
Show that the argument ∆of ω is constant along a circle passing through the
points −z0 and +z0 of the z plane.
Hint
Make use of the constant angle property of the circle (see below) and Fig. 3.8.
□
Fig. 3.8
The argument ∆of ω deﬁned by Eq. (3.9)
The modulus of ω is the ratio of the lengths of the vectors z −z0 and z + z0 and we know
from elementary geometry that this ratio is constant along a circle (Circle of Apollonius)
with its center on the straight line through −z0 and +z0.
As in the ω plane the lines |ω| = constant (circles around the origin) and the lines ∆=
constant (radii) are two orthogonal sets of curves, the two sets of circles in the z plane
for |ω| = constant and ∆= constant must be orthogonal by the property of conformal
transformations.
This example leads to the following two geometrical conclusions:
(a) The circle passing through the points z1 and z2 such that the chord z1z2 subtends at
any point of the arc z1z2 of the circle constant angle ∆is given by the equation:
u exp(i∆) = z −z1
z −z2
.

Planar Vectors and Complex Numbers
103
(b) The circle of Apollonius, for which the ratio of the distances of any point of the circle
to the two ﬁxed points z1 and z2 is constant say a, is given by the equation
a exp(iu) = z −z1
z −z2
.
One of the most important transformations is the inversion:
ω = 1
z∗,
which leaves the argument the same while inverting the modulus.
Exercise
Show that the inversion of the vertical straight line z = 1+iu is a circle passing
through the origin.
Solution
The real and imaginary parts of the inversion
1
1−iu are given by
x =
1
1 + u2 ; y =
u
1 + u2 ,
which are seen to satisfy the equation (x −1
2)2 + y2 = 1
4 which is the Cartesian equation
of the circle with center at (0, 1
2) and radius 1
2. We call this an O-circle.
□
Exercise
Show that all straight lines in the complex z plane (z = u + i(mu + c), u,m,c
real) can be converted to O-circles and conversely, the angle between two of these straight
lines being equal to the angle at the intersection of the two corresponding O-circles.
□
Under inversion the cross ratio of four points goes over to
D(ω) = ω1 −ω3
ω1 −ω4
÷ ω2 −ω3
ω2 −ω4
= z∗
1 −z∗
3
z∗
1 −z∗
4
÷ z∗
2 −z∗
3
z∗
2 −z∗
4
which is the conjugate value of the original D(z). The cross ratio is, in general, changed
by inversion, it will remain the same, if it is real. In other words, if the four points are on
a circle, before the transformation, they will still be on a circle after the transformation,
straight line being included as the circles of inﬁnite radius.
There are pairs of curves which are mutual inversions, e.g., parabola and cardioid,
orthogonal hyperbola and lemniscate. All the properties concerning angles between
straight lines related to one member of a pair can immediately be converted to the
properties of angles between O-circles related to the second one. Concyclical location of
four or more points will be invariant with respect to inversion.
3.5
Circle: Constant Angle and Constant Power Theorems
Let A, B and P be points on the circle z = r exp(iu); B situated on the real axis (u = 0),
A is ﬁxed (u = φ) and P is arbitrary (see Fig. 3.9).

104
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 3.9
Constant angle property of the circle
Vector −−→
AP is given by r exp(iu) −r exp(iφ). Vector −−→
BP is r exp(iu) −r. The quotient
of these two vectors contains the factor exp(iα):
(Real function)exp(iα) = r exp(iu) −r exp(iφ)
r exp(iu) −r
.
Dividing this by the conjugate equation we get exp(2iα) = exp(iφ) implying α = φ/2
which is the constant angle property of the circle.
Next consider a circle with center at origin O and choose point A on the negative real
axis at a distance A from O (see Fig. 3.10). Draw a secant through A whose formula is
z = −a + s exp(iφ).
It cuts the circle z = r exp(iu) at
−a + sexp(iφ) = r exp(iu).
Fig. 3.10
Constant power property of the circle
Multiplying this equation by its conjugate, we ﬁnd
a2 −2 ascosφ + s2 = r2.
This equation has two roots s1 and s2, the product of which equals a2 −r2, independent of
the choice of φ which proves the constant power property of the circle.

Planar Vectors and Complex Numbers
105
3.6
General Circle Formula
We prove that a curve represented by the equation
z = z1 + z2u
z3 + z4u
(3.10)
is a circle.
The curve passes through point A = z1/z3 (u = 0) and point B = z2/z4 (u = ∞) (see
Fig. 3.11). Let P be a point on the curve, represented by Eq. (3.10). The vector −−→
P A is
z1 + z2u
z3 + z4u −z1
z3
= u
z3
z2z3 −z1z4
z3 + z4u .
The vector −−→
P B is
z1 + z2u
z3 + z4u −z2
z4
= −1
z4
z2z3 −z1z4
z3 + z4u ,
so that the quotient P A
P B = −z4
z3 u. As z3 and z4 are constant and u is real, the argument of
this quotient is constant. Therefore, the angle α in Fig. 3.11 is independent of u. Therefore,
we may conclude that P describes a circle.
Fig. 3.11
Illustrating Eq. (3.10)
In order to get the radius r and the center zc of the circle represented by Eq. (3.10), we
solve this equation for u:
−u = z1 −zz3
z2 −zz4
.
As u is real, it must equal the conjugate of the right side so that the circle is represented by
the equation:
(z1 −zz3)(z∗
2 −z∗z∗
4) = (z∗
1 −z∗z∗
3)(z2 −zz4).

106
An Introduction to Vectors, Vector Operators and Vector Analysis
Comparison with the equation
(z −zc)(z∗−z∗
c) = r2
yields
zc = z1z∗
4 −z2z∗
3
z3z∗
4 −z∗
3z4
and |zc|2 −r2 = z1z∗
2 −z∗
1z2
z3z∗
4 −z∗
3z4
.
Exercise
Interpret the last expression in terms of the power of the circle (see above and
Fig. 3.11).
□
3.7
Circuit Impedance and Admittance
The impedance of an electrical circuit containing a resistance R, inductance L and capacity
C in series, with applied electromotive force of angular frequency ω is
z = iωL + R +
1
iωC .
With the new parameter
u = ωL −1
ωC ,
the impedance becomes
z = f (u) = R + iu,
which is a straight line in the complex plane. The admittance, 1
z =
1
R+iu is then a circle.
Connecting R, C and L in parallel leads to an admittance
1
z = f (u) = 1
R + iu
where u = ωC −
1
ωL and represents a straight line in the complex plane. However, the
impedance z will now be a circle.
Fig. 3.12
Both impedance and admittance of this circuit are circles

Planar Vectors and Complex Numbers
107
There are circuits for which both impedance and admittance are circles. For the circuit in
Fig. 3.12 the admittance is
1
R1
+
1
R2 + iωL = R1 + R2 + iωL
R1R2 + iωLR1
and this as well as its inversion represent a circle.
There are circuits for which the variable parameter is not the frequency however, some
other quantity pertaining to the circuit. In Boucherot’s circuit (see Fig. 3.13) we ﬁnd the
variable resistance u. The impedance is
z =
a2
i(b −a) + u
Fig. 3.13
Boucherot’s circuit
and this is again represented by a circle. We may observe that j2 is independent of u:
e = ia(j1 + j2) + v2; v2 = −iaj1; ∴e = iaj2.
As another example, the circle diagram named after Heyland is obtained by plotting the
admittance of a motor as a function of load.
The reason why circle diagrams occur so often in electrical engineering is the linear
character of the fundamental equations. As the mechanical vibrations follow similar
equations, the ﬁeld of application includes mechanics and acoustics.
3.8
The Circle Transformation
The transformation
ω = az + b
cz + d a,b,c,d complex
(3.11)
transforms the circle
z = z1 + z2u
z3 + z4u

108
An Introduction to Vectors, Vector Operators and Vector Analysis
into other circle:
ω = az1 + bz3 + (az2 + bz4)u
cz1 + dz3 + (cz2 + dz4)u .
Therefore, the transformation in Eq. (3.11) is called the circle transformation. Straight
lines are considered to be the special cases of circles, as the straight line
z = z1 + z2u
m + nu
is also transformed into a circle and can turn out to be the transform of a circle.
A prominent example of the application of the circle transformation is the four terminal
network (see Fig. 3.14). Four terminal networks can be of electrical, mechanical, acoustic
or optical character. They may be electromechanical couplings and so on. We assume a
linear relation between the input and output, that is,
v1 = av2 + bj2
j1 = cv2 + dj2.
(3.12)
Fig. 3.14
Four terminal network
Dividing these equations we get v1/j1 = a(v2/j2)+b
c(v2/j2)+d , which reduces to the transformation
in Eq. (3.11) if we identify ω = v1/j1 and z = v2/j2. Thus, if z is a circle impedance, ω
will also have circular character.
Symmetrical networks are the ones which remain invariant under the maps (v1,v2) 7→
(v2,v1) and (j1,j2) 7→(−j2,−j1). Note that applying this map twice reduces to identity.
Thus, in addition to Eq. (3.12) the equations
v2 = av1 −bj1
−j2 = cv1 + dj1
(3.13)
must hold. Eliminate v1 from the ﬁrst of Eq. (3.12) and the ﬁrst of Eq. (3.13) to get
j1 = a2 −1
b
v2 + aj2

Planar Vectors and Complex Numbers
109
and identify this with the second of Eq. (3.12). Comparing the corresponding coefﬁcients
we see that for a symmetrical network the coefﬁcients must satisfy
a = d and a2 −bc = 1.
Imposing these conditions on Eq. (3.11) we see that, for a symmetrical network, the
transformation is
ω = az + b
cz + a.
(3.14)
The characteristic value z = ∞corresponds to the open output (j2 = 0) condition, while
z = 0 corresponds to the shorted output terminals. The corresponding values of ω,
denoted ω∞and ω0 respectively, are
ω∞= a/c ; ω0 = b/a.
The case where ω = z is of importance. The corresponding value of z is called the wave
impedance. An arbitrary number of networks, put in cascade, would not change this
impedance. From Eq. (3.14) it follows that this value is
√
b/c, which we denote by ωz.
Note that ω2z = ω0ω∞, which means geometrically that (see Fig. 3.15) the triangles
ω∞Oωz and ωzOω0 are similar.
Fig. 3.15
Geometrical meaning of ω2z = ω0ω∞
The transformation in Eq. (3.14) can be written as

ω −a
c

z + a
c

= b
c −a2
c2
or,
(ω −ω∞)(z + ω∞) = ω2
z −ω2
∞
or,
ω −ω∞
ωz −ω∞
= ωz + ω∞
z + ω∞
which means the triangles ω,−ω∞,ωz and ω,ω∞,z are similar (see Fig. 3.16).

110
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 3.16
Point by point implementation of transformation Eq. (3.14)
As ω∞,−ω∞,ωz are ﬁxed points in the plane, this offers us a method to construct point ω
for any given value of z, thus performing the transformation point by point.
Exercise
Let x and y be the rectangular coordinates of a point x. Show that the equations
to an ellipse and a hyperbola, in terms of these coordinates, are
x2
a2 + y2
b2 = 1, x2
a2 −y2
b2 = 1
respectively. These parameters are related to those in Eq. (2.23) by
a =
l
|1 −e2|, b2 = al, x = r + ae.
The curves and related parameters are shown in Figs 3.17(a),(b). Use the equations in terms
of coordinates to show that an ellipse has a parametric equation x = x(φ):
x = acosφ + bsinφ,
Fig. 3.17
An ellipse and a hyperbola

Planar Vectors and Complex Numbers
111
while a hyperbola has the parametric equation:
x = acoshφ + bsinhφ,
where a2 = a2, b2 = b2, and a · b = 0.
Hint
Treat these as the curves on a complex plane and use complex algebra. Write the
equations to ellipse and hyperbola as z = acosφ + ibsinφ and z = acoshφ + ibsinhφ
respectively.
□
Theory of plane curves is a subject in itself and we recommend reference [26] for further
study.


Part II
Vector Operators
“My Lord! Please make me a cat!” prayed the mouse.
— from a Panchatantra story


4
Linear Operators
4.1
Linear Operators on E3
We have seen that the ﬁelds are functions deﬁned over the domain of position vectors or
points in space and are either vector valued or scalar valued. We now consider functions
(either vector valued or scalar valued) deﬁned over some domain of vectors (not necessarily
position vectors) with the additional requirement that the function be linear, that is,
f (αx + βy) = αf (x) + βf (y),
where α and β are scalars. Such a function is called a linear operator, or operator for
brevity. In different contexts, such a function is also called a linear transformation or a
tensor. The term ‘tensor’ is used for describing certain properties of a physical system.
Thus, the ‘inertia tensor’ is a property of a rigid body or the ‘strain tensor’ is a property of
an elastic body. These are never called an ‘inertia or strain linear transformation’. On the
other hand, the term ‘transformation’ suggests a change of state of a physical system or an
equivalence of one state with another. The term ‘linear operator’ is generally used when
the emphasis is on the mathematical structure. Finally, we note that an operator is
essentially a mapping or association between the elements of two sets or between the
elements of the same set. Henceforth, in this book, whenever we refer to an operator, we
mean it to be a linear operator, unless otherwise speciﬁed.
Two simple examples of linear operators are α(x) = αx (the scalar multiplication
operator) and f (x) = a·x. In the ﬁrst example, α is a ﬁxed scalar and the operator maps a
vector x to a vector αx. Also, here the symbol α is used as the operator as well as a scalar.
In the second example, a is a ﬁxed vector and the operator maps a vector x to the scalar
a · x. Note that if we change the ﬁxed vector a in the second operator, we get a new
operator giving a new value for every vector x. This is often expressed by saying that the
operator parametrically depends on a. Similarly, the ﬁrst operator parametrically depends
on α.

116
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Check that the operators in these examples are linear operators.
□
The set of vectors on which a given operator f acts is called its domain. The set of vectors
or scalars generated by the action of f on its domain is called its range. All the operators
we deal with act on E3. For a real life application, E3 consists of vector values of one or
more vector quantities e.g., electric and magnetic ﬁelds. When a linear operator f acts on
a vector x ∈E3, it either returns a vector y ∈E3, or a scalar α ∈R. y (or α) is called the
image of x under f . In the ﬁrst case, we denote f
: E3 7→E3 and in the second case
f : E3 7→R. We assume that the domain of an operator we deal with is whole of E3 and
its range is either a subset of E3 or a subset of R. Two operators f : E3 7→E3 or R and
g : E3 7→E3 or R are equal if they have common domain (E3) and range (a subset of
E3 or R) and if f (x) = g(x) for all x ∈E3.
The product of two linear operators, in a given order, is a linear operator in itself and is
deﬁned as an operator obtained by successively applying the two operators in the given
order. Thus, in order to get the action of the product f g on vector x we have to act ﬁrst by
the operator g on x to get the vector g(x) and then act by the operator f on the vector
g(x) to get the vector f g(x). In general, the product of two linear operators is not
commutative.1 Such a product is written in many different ways like
g(f (x)) = g(f x) = gf (x) = gf x.
Note that the product of operators f : E3 7→R and g : E3 7→E3 is deﬁned only in the order
f g : E3 7→R. The general condition for the existence of the product of the two operators,
f g, is that the range of g must be a subset of the domain of f . Note that, two commuting
operators must be deﬁned on a common set of vectors, forming the domain as well as the
range for both.
Exercise
Show that the product of two linear operators is a linear operator. Check this
for the two operators deﬁned in the above examples. Also check that two operators deﬁned
via scalar multiplication as in the ﬁrst example above, commute. In fact, check that the
operator of scalar multiplication α(x) = αx commutes with all linear operators.
□
The addition of two linear operators is deﬁned by
(f + g)(x) = f (x) + g(x)
for all x ∈E3 and is itself a linear operator (check this). The operators being added must
be either E3 7→E3 or E3 7→R. Both the product and the addition of linear operators are
associative. That is, for three linear operators f ,g and h we have
h(gf ) = hgf = (hg)f and (h + g) + f = h + (g + f ).
1Two operators f and g are said to commute if f g(x) = gf (x) for all x ∈E3.

Linear Operators
117
This follows easily from the deﬁnitions of the product and the addition of operators. Using
the linearity of operators and the deﬁnition of their product we can show that the product
of operators is distributive with respect to addition, that is,
h(g + f ) = hg + hf .
Identity operator
The identity operator I is deﬁned via
I(x) = x
for all x ∈E3. The scalar multiplication operator we saw above can also be deﬁned as
(αI)(x) = αI(x) = αx. It is trivial to check that for every operator f
If = f = f I,
that is, I commutes with every operator2.
4.1.1
Adjoint operators
To every linear operator f
:
E3 7→E3 there corresponds another linear operator
f † : E3 7→E3 uniquely deﬁned by
y · f (x) = f †(y) · x
for all vectors x and y in E3. The operator f † is called the adjoint of f . You will know its
utility after we use it in the sequel.
Exercise
Show that (f †)† = f .
□
Consider two operators f and g and their product f g. Given any two vectors x,y ∈E3 we
can write,
f gx · y = x · (f g)†y
(4.1)
and
f gx · y = gx · (f )†y = x · (g)†(f )†y.
(4.2)
Since the LHS of Eqs (4.1) and (4.2) are the same, their RHS must also be equal. Since
x,y ∈E3 are arbitrary, this leads to the operator equality
(f g)† = (g)†(f )†.
4.1.2
Inverse of an operator
Consider an operator f : E3 7→E3 or R acting on all vectors in E3. The set containing the
images of all vectors in E3 under f (the range of f , also called the image set of f ) need not
2Note that if f : E3 7→E3 then I : E3 7→E3, but if f : E3 7→R then on LHS I : E3 7→E3 while on RHS I : R 7→R.
Henceforth keep track of the mapping corresponding to operators occurring in an expression.

118
An Introduction to Vectors, Vector Operators and Vector Analysis
equal E3 (or R) however, can be a proper subset of E3 (or of R). This can happen when two
or more elements of E3 have the same image under f . However, when this image set equals
E3, (or R), that is, for every y ∈E3 (or y ∈R) there is a x ∈E3 such that f (x) = y (or
f (x) = y), we call the operator ‘onto’. If two different elements of E3 always have different
images under f then the operator f is said to make a one to one mapping (or one to one
correspondence) between E3 and its image set under f . If f is both onto and one to one,
then we can deﬁne its inverse operator f −1 : E3(or R) 7→E3 as follows. For each y ∈E3
or y ∈R we ﬁnd that unique element x ∈E3 such that f (x) = y or y (x exists and is
unique since f is onto and one to one). We then deﬁne x = f −1(y or y). This equation is
the result of solving y = f (x) (or y = f (x)) for x in just the same way as x = log(y) is
the result of solving y = ex for x. Below we give two examples to illustrate this. Figure 4.1
illustrates the concept of the inverse of a mapping.
Fig. 4.1
Inverse of a mapping. A one to one and onto map f : X 7→Y has the
unique inverse f −1 : Y 7→X
If f −1 exists, we call the operator f invertible. It follows directly from its deﬁnition
based on f being a one to one correspondence that f −1, if it exists, is unique.
Using the deﬁnition of the inverse, we can write,
(f −1f )(x) = f −1(f (x)) = f −1(y) = x
for all x ∈E3 and similarly for f f −1(y), for all y ∈E3. This gives us the operator equation
f −1f = I and f f −1 = I.
(4.3)
The identity operators in Eq. (4.3) may act on different spaces. Thus, if f : E3 7→R is
invertible with f −1 : R 7→E3 then the product f −1f : E3 7→E3 is an operator on E3 while
f f −1 : R 7→R is an operator on R. Both are identity operators on respective spaces.
We now check whether the inverse of a linear operator is linear. The answer is yes. We
have,
f −1(y1 + y2) = f −1(f (x1) + f (x2)) = f −1f (x1 + x2)
= I(x1 + x2) = x1 + x2 = f −1y1 + f −1y2

Linear Operators
119
and
f −1(αy) = f −1(αf (x)) = f −1f (αx) = αx = αf −1(y),
which proves the linearity of f −1.
For any linear operator f we show that f (0) = 0. We have
f (0) = f (x −x) = f (x) −f (x) = 0.
For an invertible operator, f (a) = 0 implies a = 0 as can be seen from
0 = f −1(0) = f −1(f (a)) = a,
where the ﬁrst equality follows because f −1 is a linear operator, so that f −1(0) = 0.
Next we show that for an invertible operator f the set {f (x),f (y),f (z)} is linearly
independent
(non-coplanar)
provided
the
set
{x,y,z}
is
linearly
independent
(non-coplanar). We see that the equation
0 = αf (x) + βf (y) + γf (z) = f (αx + βy + γz)
implies that all the coefﬁcients α,β,γ vanish, because x,y,z are linearly independent.
Here, we have used f (a) = 0 implies a = 0 for an invertible operator. The same argument
shows that if x,y,z are linearly dependent, then so are f (x),f (y),f (z).
For arbitrary x ∈E3, let f gx = y, so that x = (f g)−1y. Successively multiplying both
sides by (f )−1 and (g)−1 we get x = (g)−1(f )−1y. This leads to the operator equality
(f g)−1 = (g)−1(f )−1.
4.1.3
Determinant of an invertible linear operator
Consider an orthonormal basis { ˆσ1, ˆσ2, ˆσ3} forming a right handed system. The
parallelepiped with adjacent sides ˆσ1, ˆσ2, ˆσ3 is a cube with volume unity (unit cube). That
is, ˆσ1 · ˆσ2 × ˆσ3 = 1. Under the action of an invertible linear operator f : E3 7→E3 this
unit cube goes over to a parallelepiped with adjacent sides f ( ˆσ1),f ( ˆσ2),f ( ˆσ3) and with
volume proportional to that of the unit cube ˆσ1 · ˆσ2 × ˆσ3 = 1. We write
f ( ˆσ1) · f ( ˆσ2) × f ( ˆσ3) = det f ˆσ1 · ˆσ2 × ˆσ3 = det f .
This equation deﬁnes the proportionality factor det f which depends exclusively on the
operator f and is an important characteristic of f . det f is called the determinant of the
operator f . Note that for an invertible operator f , det f , 0 because the vectors
f ( ˆσ1),f ( ˆσ2),f ( ˆσ3) are linearly independent, that is non-coplanar. Given any set {x,y,z}
of linearly independent (non-coplanar) vectors, the number of unit cubes that can be
accommodated in the parallelepiped with adjacent sides x,y,z is given by its volume

120
An Introduction to Vectors, Vector Operators and Vector Analysis
x · y × z. Under the action of f , a unit cube is transformed to a parallelepiped with volume
det f . Therefore, the volume of the parallelepiped transformed under the action of f is
f (x) · f (y) × f (z) = det f x · y × z,
or,
det f = f (x) · f (y) × f (z)
x · y × z
.
(4.4)
The determinant det f of an invertible linear operator is invariant under the change of
orthonormal basis. We shall see later that any two triads of orthonormal unit vectors can
be made to coincide by three successive independent rotations called Euler rotations (see
section 6.5). Under these rotations the volume of the unit cube scanned by one orthonormal
triad does not change. Since the determinant of f is simply the volume of the deformed
unit cube under the action of f , we see that det f is invariant under the change of basis,
which amounts to the rotation of one orthonormal triad of vectors to the other.
If f is invertible, then we know that any non-coplanar triad (x,y,z) is mapped to
another non-coplanar triad (f (x),f (y),f (z)). This makes both the numerator and the
denominator on the RHS of Eq. (4.4) non-zero, that is, det f , 0. Thus, if f is invertible,
then det f , 0.
If f is not invertible, then there exist two vectors x,y ∈E3, x , y such that f (x) =
f (y). We can make a linearly independent triad (x,y,z) by adding a non-coplanar vector
z to the set {x,y}, x , y satisfying f (x) = f (y). Using this triad in Eq. (4.4), we see that
det f = 0. This proves that det f , 0 implies f is invertible.
The last two paragraphs together imply that a linear operator f is invertible if and only
if det f , 0.
Many simple properties of the determinant det f now follow. First, it is trivial to check
that det I = 1. Next, consider the product gf of two linear invertible operators g and f .
We have, using an orthonormal basis { ˆσ1, ˆσ2, ˆσ3},
det gf
=
gf ( ˆσ1) · gf ( ˆσ2) × gf ( ˆσ3)
=
g(f ( ˆσ1)) · g(f ( ˆσ2)) × g(f ( ˆσ3))
=
 g(f ( ˆσ1)) · g(f ( ˆσ2)) × g(f ( ˆσ3))
f ( ˆσ1) · f ( ˆσ2) × f ( ˆσ3)
!
(f ( ˆσ1) · f ( ˆσ2) × f ( ˆσ3))
=
det g · det f .
Thus, the determinant of the product is the product of determinants. This result can be
used to write
1 = det I = det f f −1 = (det f )(det f −1), or, det f −1 = (det f )−1.

Linear Operators
121
If det f < 0, the operator f not only scales the volume of the parallelepiped formed by
(x,y,z) however, also changes its orientation. That is, f changes a right handed system
formed by (x,y,z) to a left handed one or the acute angle between x and y×z to an obtuse
angle between them. (see the interpretation of the scalar triple product in subsection 1.8.1).
Further, det f is deﬁned via a scalar triple product, so that interchange of any two factors
changes its sign. This is not surprising, because interchanging any two of the three linearly
independent vectors changes them from a right handed to a left handed system and vice-
versa (see section 1.16).
4.1.4
Non-singular operators
An operator f with det f = 0 is called singular. If det f , 0 then f is called non-singular.
We can now prove that the following three statements are equivalent. We have proved some
parts of it in the last two sections however, it is worth putting everything at one place.
(a) f is non-singular.
(b) f (x) = 0 implies x = 0.
(c) f is invertible.
We ﬁrst prove (a) ⇒(b). Let { ˆσk}, k = 1,2,3 be an orthonormal basis. Assume that
f (x) = 0 for some x , 0. This means
f (x) =
X
k
xkf ( ˆσk) = 0.
Since x , 0, not all xks can be zero. Therefore, the above equation means that the vectors
f ( ˆσk) k = 1,2,3 are linearly dependent (coplanar). Therefore, det f = 0 which
contradicts the assumption that f is non-singular.
(b) ⇒(c): Suppose that f is not invertible, that is, it is not a one to one correspondence
between x and f (x) so that there are two different non-zero vectors x1 and x2 (x1 , x2)
satisfying
f (x1) = y = f (x2),
which gives
0 = f (x1) −f (x2) = f (x1 −x2)
which means that there is a non-zero vector z = x1 −x2 with f (z) = 0. This contradicts
assumption (b).
(c) ⇒(a) That f is invertible implies it is non-singular is proved in subsection 4.1.3.
4.1.5
Examples
We ﬁnd the inverses of the following linear operators.
(a) f (x) = αx + a(b · x).

122
An Introduction to Vectors, Vector Operators and Vector Analysis
(b) g(x) = αx + b × x.
(a) Let
y = f (x) = αx + a(b · x).
(4.5)
Dotting both sides with b we get,
y · b = αx · b + (a · b)(x · b),
or,
x · b =
y · b
α + a · b.
Multiply both sides by a to get
a(x · b) = a(y · b)
α + a · b.
Using Eq. (4.5) we get,
y −αx = a(y · b)
α + a · b,
or,
x = y
α −
a(y · b)
α(α + a · b) = f −1(y).
(b)
y = g(x) = αx + b × x.
(4.6)
Dot with b to get
b · y = αb · x.
(4.7)
Cross with b to get
b × y = αb × x + b × (b × x).
(4.8)
Using Eqs (4.6), (4.7) in Eq. (4.8) and identity I, we get,
b × y = α(y −αx) + (b · x)b −b2x
= αy −(α2 + b2)x + α−1(b · y)b,

Linear Operators
123
or,
x = αy + α−1(b · y)b −(b × y)
α2 + b2
= g−1(y).
Exercise
In these two examples, check that f −1f (x) = x and f f −1(y) = y. Also,
check whether both these operators are non-singular.
□
(c) We solve the vector equation
α1a1 + α2a2 + α3a3 = c
(4.9)
for αis; {ai}, i = 1,2,3 and c being given, using vector methods. We then compare
our solution with that obtained by Cramer’s rule for solving simultaneous equations.
Cross the given equation with a3 to get
α1(a3 × a1) + α2(a3 × a2) = a3 × c
Dotting with a2 and solving for α1 we get
α1 = a2 · (a3 × c)
a2 · (a3 × a1).
Similarly,
α2 = a3 · (a1 × c)
a3 · (a1 × a2)
and
α3 = a1 · (a2 × c)
a1 · (a2 × a3).
The given vector equation is equivalent to
α1a11 + α2a12 + α3a13 = c1
α1a21 + α2a22 + α3a23 = c2
α1a31 + α2a32 + α3a33 = c3
where aij is the ith component of aj and ci is the ith component of c with respect to some
orthonormal basis. By Cramer’s rule, its solution is

124
An Introduction to Vectors, Vector Operators and Vector Analysis
α1 =

c1
a12
a13
c2
a22
a23
c3
a32
a33


a11
a12
a13
a21
a22
a23
a31
a32
a33

,
where in the upper determinant the 1st column in [aij] is replaced by [c1,c2,c3]T and
similarly for α2 and α3. It is straightforward to check that the two solutions are equivalent.
If we try to apply the vector method given above to the equation with more than three
variables, (αi i = 1,...,4 say), it fails. We can make one of the four terms vanish by taking
a suitable cross product and treat the resulting equation in three unknowns by the method
given above. However, the vectors in the three term equation are all coplanar making the
scalar triple product like a1 · (a2 × a3) vanish. Thus, a generalization of our method needs
a more general kind of algebraic setting, than the vector compositions based on dot and
cross products. Geometric algebra is such an algebra in which the above method can be
generalized. We refer to references [10, 7, 11] for a comprehensive treatment of geometric
algebra.
4.2
Frames and Reciprocal Frames
In this section we deal with the problem of expressing arbitrary vectors in terms of a
non-orthonormal basis. Let {ek}, k = 1,2,3 be a basis in E3, not necessarily orthonormal.
We call it a frame and associate a pseudoscalar e = ˆe1 · (ˆe2 × ˆe3) with this frame.
e > 0(< 0) means the frame is positively (negatively) oriented. For an orthonormal frame
(basis) e = +1 or e = −1 depending on whether it is right or left handed. The reciprocal
frame {ek}, k = 1,2,3 is determined by the set of equations
ek · ej = δk
j ; j,k = 1,2,3,
where δk
j = 1 if j = k and zero otherwise. To solve for ek we note that it is a vector normal
to both the vectors ej j , k and its scalar product with ek must be +1. Such a vector is
uniquely given by the vector product of ej j , k in the cyclic order of {123}. Thus, the
unique solution to these equations are given by
e1 = e2 × e3
e
,
e2 = e3 × e1
e
,
e3 = e1 × e2
e
.

Linear Operators
125
Exercise
Check that an orthonormal frame is reciprocal to itself.
□
Any vector a can be expressed as a linear combination
a = a1e1 + a2e2 + a3e3 = akek,
(4.10)
where the summation convention is used on the right. The coefﬁcients ak are called
contravarient components of vector a (with respect to frame {ek}). We note that the
Eq. (4.10) is the same as Eq. (4.9) with α1,2,3 replaced by a1,2,3 and a1,2,3 replaced by
e1,2,3 and c replaced by a. Making these substitutions, we get the following solutions for
Eq. (4.10).
a1 =
a · (e2 × e3)
e1 · (e2 × e3),
a2 =
a · (e3 × e1)
e1 · (e2 × e3),
a3 =
a · (e1 × e2)
e1 · (e2 × e3).
(4.11)
Exercise
Show that these solutions reduce to
ak = ek · a; k = 1,2,3.
(Remember that ek, k = 1,2,3 are not mutually orthogonal!)
If we expand a vector a in terms of the ek basis, we get
a = a1e1 + a2e2 + a3e3 = akek,
where the coefﬁcients ak are called covariant components of vector a (with respect to
frame {ek}).
Exercise
Show that the contravariant components ak are given by
ak = ek · a.
□
Exercise
Let ˆi,ˆj, ˆk be an orthonormal basis and deﬁne a non-orthonormal frame by e1 =
ˆi+3ˆj, e2 = 4ˆj and e3 = ˆk. Find the corresponding reciprocal frame. Find the contravariant
and covariant components of a = 7ˆi + 2ˆj + ˆk with respect to these frames. Draw ﬁgures
to depict both the frames and the contravariant and covariant components of a.
□
Exercise
Show that primitive bases of the Bravis lattice of a crystal and its reciprocal
lattice form reciprocal frames.
□

126
An Introduction to Vectors, Vector Operators and Vector Analysis
4.3
Symmetric and Skewsymmetric Operators
We have already deﬁned the adjoint of a linear operator. We now deﬁne two important
types of linear operators, namely symmetric and skewsymmetric operators.
A linear operator S is said to be symmetric (or self adjoint) if S = S† that is, if it is
equivalent to its adjoint. Similarly, a linear operator A is said to be skewsymmetric (or
antisymmetric) if A † = −A . A very simple but important observation is the following
identity. For any operator f we can write,
f = 1
2

f + f †
+ 1
2

f −f †
= f+ + f−.
Obviously, the ﬁrst term (f+) is a symmetric operator, while the second (f−) is a
skewsymmetric operator. Thus, any operator can be written as the sum of a symmetric
and a skewsymmetric operator.
We now show that any skewsymmetric operator A can be put in a canonical (or
standard) form
A x = x × (a × b),
(4.12)
where (a × b) is a unique pseudo vector.
We ﬁrst check whether this operator is indeed a skewsymmetric operator. We have,
y · (x × (a × b)) = y · ((b · x)a −(a · x)b),
= (b · x)(a · y) −(a · x)(b · y)
= −x · (y × (a × b)),
(4.13)
giving
A † = −A .
Here, we have used identity I.
We note that it is enough to prove Eq. (4.12) for a standard basis { ˆσk}, k = 1,2,3, for
then the result is generally true by the linearity of A . We know that the vectors A ˆσk and
A † ˆσj can be written as
ak = A ˆσk =
X
j
ˆσjAjk,
where Ajk = ˆσj · A ˆσk and
A † ˆσj =
X
k
ˆσkAjk,

Linear Operators
127
where we have used the deﬁnition of the adjoint of an operator. Now we choose
a × b = 1
2
X
k
ak × ˆσk
and consider
ˆσj × (a × b) = 1
2
X
k
ˆσj × (ak × ˆσk)
= 1
2
X
k
h
ˆσj · ˆσk

ak −ˆσk

ˆσj · ak
i
= 1
2
X
k
h
δjk (A ˆσk) −ˆσkAjk
i
= 1
2

A ˆσj −A † ˆσj

= A ˆσj.
(4.14)
Here, we have used identity I and the orthonormality of the {σk} basis. As an example,
the magnetic force due to magnetic ﬁeld on a charged particle is a skewsymmetric linear
operator on the particle velocity given by F = Bv = q
c v×B acting via the psuedovector B.3
Example
We ﬁnd the adjoint as well as the symmetric and the skewsymmetric parts of the operator
f x = αx + a(b · x) + x × (c × d).
(4.15)
We ﬁnd
f †x = αx + b(a · x) + (c × d) × x,
f+x = αx + 1
2[a(b · x) + b(a · x)],
and
f−x = 1
2[a(b · x) −b(a · x)] + x × (c × d)
= 1
2x × (a × b) + x × (c × d).
(4.16)
where f+ and f−are the symmetric and skewsymmetric parts of f respectively.
3Since F and v are polar vectors, B has to be a pseudovector.

128
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Obtain these expressions for the adjoint, the symmetric and the skewsymmetric
parts of f given in Eq. (4.15).
□
4.3.1
Vector product as a skewsymmetric operator
For a ﬁxed vector a , 0, the map x 7→a × x deﬁnes an operator f on E3. This operator is
linear by virtue of the distributive property of the vector product, that is,
f (αx + βy) = a × (αx + βy) = αa × x + βa × y = αf (x) + βf (y).
This operator is skewsymmetric, because
y · f †(x) = x · f (y) = x · (a × y) = y · (x × a),
which means
f †(x) = (x × a) = −(a × x) = −f (x).
Note that this operator maps to 0 all (non-zero) vectors x which are parallel or antiparallel
to a, so that it is not invertible.
Let {a1,a2,a3} {x1,x2,x3} and {f1,f2,f3} be the components of the vectors a, x and
f (x) with respect to some orthonormal basis. Then expressing the vector product in
terms of the Levi-Civita symbols we get
fi = εijkajxk,
where we have used the summation convention. Using the values of the antisymmetric
tensor εijk we can write this equation in the matrix form


f1
f2
f3


=


0
−a3
a2
a3
0
−a1
−a2
a1
0




x1
x2
x3


·
Thus, the action of the skewsymmetric operator f (x) = a × x on a vector x is obtained by
multiplying the column vector [x1,x2,x3]T by the skewsymmetric matrix
f ≡a× ↔[εijkaj] ≡


0
−a3
a2
a3
0
−a1
−a2
a1
0


·

Linear Operators
129
4.4
Linear Operators and Matrices
Let f be a linear operator and { ˆσk} k = 1,2,3 be an orthonormal basis in E3. Using the
fact that { ˆσk} is a basis and the linearity of f , we can write
f (x) = f


X
k
( ˆσk · x) ˆσk

=
X
k
( ˆσk · x)f ( ˆσk),
where xk = ˆσk ·x k = 1,2,3 are the components of x in the basis { ˆσk}. We can expand the
vectors fk = f ( ˆσk) in the basis { ˆσk} to get
fk = f ( ˆσk) =
X
j
ˆσj( ˆσj · fk) =
X
j
ˆσjfjk.
There are three coefﬁcients fjk for each value of k (that is, each fk) so that for k = 1,2,3
there are nine coefﬁcients fjk. We arrange them in a 3×3 matrix with j running over rows
and k over columns. We have,
[f ] = [fjk] =


f11
f12
f13
f21
f22
f23
f31
f32
f33


·
The coefﬁcients
fjk = ˆσj · f ( ˆσk) = ˆσj · fk
which form a 3×3 matrix as above, are called the matrix elements of the linear operator f .
The matrix formed by fjk is called the matrix representing f in the basis { ˆσk}. If we change
over to some other orthonormal basis say {ˆek}, the matrix representing f in the basis {ˆek}
is in general different than that representing f in the basis { ˆσk}. Later in this discussion,
we shall relate these two matrix representatives of the same operator f . By [f ]{·} we denote
the matrix representing operator f using the basis {·}. Whenever the basis is ﬁxed, we shall
drop the sufﬁx {·}.
A linear operator is completely determined by its matrix in a given basis. To see this,
consider the action of f on an arbitrary vector x ∈E3. We have,
f (x) =
X
k
f ( ˆσk)xk =
X
j
X
k
ˆσjfjkxk.
Thus, the vector f (x) has the following components along ˆσj.
(f (x))j =
X
k
fjkxk.

130
An Introduction to Vectors, Vector Operators and Vector Analysis
Therefore, jth component of the vector equation
f (x) = y
is
(f (x))j =
X
k
fjkxk = yj.
There is one such equation for each value of j = 1,2,3 so that the vector equation f (x) = y
is equivalent to the set of three simultaneous equations
X
k
fjkxk = yj j = 1,2,3
completely determined by the matrix [fjk]. Written in matrix form, these equations read


f11
f12
f13
f21
f22
f23
f31
f32
f33




x1
x2
x3


=


y1
y2
y3


·
Thus, the action of a linear operator f on a vector x ∈E3 is completely determined by the
matrix of f in a given orthonormal basis. Note that the determinant of the matrix [fjk]
representing the linear operator f in the basis { ˆσk} is the same as the determinant of f ,
namely, det f = f ( ˆσ1) · f ( ˆσ2) × f ( ˆσ3) as can be seen by expressing this scalar triple
product in its determinant form (see the exercise in subsection 1.8.1). A student will do
better by explicitly working out the matrix elements fjk for different values of the indices
j,k.
4.5
An Equivalence Between Algebras
The algebra of 3 × 3 matrices is equivalent to the algebra of linear operators on E3. To see
this, we ﬁrst note that the operator sum f + g corresponds to the matrix sum
(f + g)jk = ˆσj · (f ( ˆσk) + g( ˆσk)) = ˆσj · f ( ˆσk) + ˆσj · g( ˆσk) = fjk + gjk,
where the ﬁrst equality follows from the deﬁnition of the matrix element of an operator.
Thus, the matrix element of the addition of two operators equals the addition of the
matrix elements of the operators, or,
[f + g] = [f ] + [g].
For the product of two linear operators say gf consider (work this out),

Linear Operators
131
gf ( ˆσk) =
X
j
(g( ˆσj))fjk =
X
i
ˆσi


X
j
gijfjk

.
Compare with
gf ( ˆσk) =
X
i
ˆσi( ˆσi · gf ( ˆσk))
to get
X
j
gijfjk = ˆσi · gf ( ˆσk).
The RHS of this equation is the ikth element of the matrix of the operator gf , while the
LHS is the ikth element of the product of the matrices of the operators g and f in that
order. Thus, we see that
[gf ] = [g][f ].
(4.17)
The ikth element of the identity operator is,
ˆσi · I( ˆσk) = ˆσi · ˆσk = δik
because the basis { ˆσk} is orthonormal. Thus, the matrix representing the identity operator,
(which is the identity with respect to operator multiplication), is the unit matrix I, (which
is the identity with respect to matrix multiplication).
For an invertible operator f , using Eq. (4.17), we have,
I = [f −1f ] = [f −1][f ]
(4.18)
which simply means that the matrix representing f −1 is the inverse of the matrix
representing f . Since f −1 is assumed to exist det f , 0. Since det f is the same as that of
the matrix representing f , its determinant is non-zero and Eq. (4.18) is meaningful. We
have already seen that det f is invariant under the change of orthonormal basis so that
Eq. (4.18) holds irrespective of the orthonormal basis used. In fact we shall independently
prove that the determinant of [f ] is invariant under the change of basis. In particular, the
determinant of the operator f can be alternatively deﬁned as the determinant of its matrix
in any orthonormal basis.
Thus, we have shown that the set of linear operators on E3 and the set of matrices
representing them (with respect to a ﬁxed orthonormal basis) are isomorphic under the
binary operations of addition and multiplication deﬁned on these sets. This fact is
expressed by saying that the algebra of linear operators on E3 and that of their matrix
representatives are equivalent4.
4To establish this equivalence both sets must have the algebraic structure called ring with respect to the multiplications deﬁned
on them, which is known to be true. We shall not discuss this point any further.

132
An Introduction to Vectors, Vector Operators and Vector Analysis
We establish the relation between the matrix representing an operator and that
representing its adjoint. We have,
f †
jk = ˆσj · f † ˆσk = f ˆσj · ˆσk = ˆσk · f ˆσj = fkj,
which means
[f †] = [f ]T
(4.19)
where the superscript T denotes the transpose of the matrix.
4.6
Change of Basis
We get the relation between the matrices of a linear operator f in two different orthonormal
bases. We denote these bases by { ˆσk} and {ˆek}. We write equation f (x) = y for basis
{ ˆσk} as
FX = Y
(4.20)
where F is the matrix of operator f in { ˆσk} and X and Y are the column (3 × 1) matrices
comprising coordinates of vectors x and y in the basis { ˆσk} (see the last equation in
section 4.4). We write f (x) = y for the corresponding matrices in the basis {ˆek} as
F′X′ = Y ′.
(4.21)
It is straightforward to check, by expanding the basis {ˆek} using the basis { ˆσk}, that for a
vector x ∈E3,
X′ = QX
with
Qij = ˆei · ˆσj
(4.22)
Apply Eq. (4.22) to X′ and Y ′ in Eq. (4.21) to get
F′QX = QY
or,
(Q−1F′Q)X = Y
(4.23)
Comparing Eq. (4.20) with Eq. (4.23) we have,
F = Q−1F′Q
F′ = QFQ−1

Linear Operators
133
The transformations induced by Q are called similarity transformations. Thus, the
matrices of a linear operator in different bases are related by a similarity transformation,
via Q deﬁned in Eq. (4.22).
It is now trivial to check that the determinant of a matrix of a linear operator f is
invariant under the change of basis. We have,
det(F′) = det(QFQ−1)
= det(Q)det(F)det(Q−1)
= det(QQ−1)det(F) = det(F).
Exercise
Let g, f be linear operators on E3. Prove that
det[gf ] = det[g]det[f ]
and
det[f −1] = (det[f ])−1,
where det[·] is the determinant of the matrix representative of the corresponding operator.
□

5
Eigenvalues and Eigenvectors
5.1
Eigenvalues and Eigenvectors of a Linear Operator
Suppose a non-zero vector u is transformed into a scalar multiple of itself by a linear
operator f , that is,
f u = λu,
(5.1)
where λ is a scalar. Then, we say that u is an eigenvector of f corresponding to the
eigenvalue λ. Equation (5.1) is called the eigenvalue equation for the operator
f . Equation (5.1) remains valid if we multiply it by a non-zero scalar. Therefore, any scalar
multiple of u is also an eigenvector of f corresponding to the eigenvalue λ. However,
these two eigenvectors are linearly dependent. If n ≥2 linearly independent1 eigenvectors
correspond to the same eigenvalue, we call this eigenvalue n-fold degenerate. This
happens when the eigenvalue equation is satisﬁed by n ≥2 linearly independent
eigenvectors for the same eigenvalue. We will deal with the degenerate eigenvalues later.
The problem of ﬁnding the eigenvalues and the corresponding eigenvectors of a given
linear operator is called the eigenvalue problem for that operator. If we list out all the
linearly independent eigenvectors and the corresponding eigenvalues (if there is a m-fold
degenerate eigenvalue, it will repeat m times in this list) it is called the spectrum of the
corresponding operator. This list of linearly independent eigenvectors obviously cannot
exceed the dimension of the space on which the operator acts, as the number of linearly
independent vectors cannot exceed the dimension. Thus, the maximum number of
linearly independent eigenvectors of a linear operator f acting on E3 is three.
If all the eigenvectors of an operator can form a basis of the space on which it acts, that
is, if the maximum number of linearly independent eigenvectors can be found, it is called
diagonalizable. In our case the operator f on E3 is diagonalizable if all of the three linearly
independent eigenvectors can be obtained. If all the eigenvalues of an operator are real and
1That is, two non-collinear vectors, or three non-coplanar vectors. There cannot be more than three linearly independent
vectors in E3.

Eigenvalues and Eigenvectors
135
distinct (no degeneracy) then the operator can be proved to be diagonalizable. Even if
degeneracy is present, we can ﬁnd the maximal set of linearly independent eigenvectors,
that is, the corresponding operator can be diagonalized. All the information of a
diagonalizable operator is contained in its eigenvalues and eigenvectors because its action
on any vector, (by virtue of its linearity and by the fact that its eigenvectors form a basis),
can be expressed in terms of these quantities in the simplest possible way. The differential
or integral equations, which are the principal mathematical models in physics and
engineering, are often expressed or related to the eigenvalue problem of operators on
different kinds of spaces called function spaces. These are some of the reasons why the
eigenvalue problem is of such a paramount importance in mathematical modeling of real
life processes. Here, we shall conﬁne ourselves to the case of operators on E3 with real
eigenvalues. As we shall see later, these are symmetric operators. We shall touch upon the
case of complex eigenvalues later.
The basis formed by the eigenvectors of an operator on E3 gives a coordinate frame in
E3. Its coordinate axes are called principal axes and the frame is called the principal axes
system.
Typically, the operator is given in its matrix form [fjk], that is, we are given the vectors
fk = f ( ˆσk) =
3
X
j=1
ˆσjfjk,
where { ˆσj} is a suitable orthonormal basis.
To develop a general method for solving the eigenvalue problem from this information,
we re-write the eigenvalue equation (Eq. (5.1)) as
(f −λI)u = 0.
(5.2)
Equation (5.2) tells us that the operator (f −λI) must be singular because it maps a non-
zero vector u , 0 to the zero vector, so that its determinant must vanish.
det (f −λI) = (f1 −λ ˆσ1) · [(f2 −λ ˆσ2) × (f3 −λ ˆσ3)] = 0.
(5.3)
If we expand the LHS of Eq. (5.3), successively applying the distributive law for the scalar
and the vector products, we can transform it to
λ3 −α1λ2 + α2λ −α3 = 0,
(5.4)
where,
α1 =
X
k
ˆσk · fk = f11 + f22 + f33
α2 =
ˆσ1 · (f2 × f3) + ˆσ2 · (f3 × f1) + ˆσ3 · (f1 × f2)
α3 = det f = f1 · (f2 × f3)
(5.5)

136
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Establish Eqs (5.4) and (5.5).
□
Equation (5.3) or Eq. (5.4) are commonly called the secular equation for f . It is an algebraic
equation of third degree in λ. From the fundamental theorem of algebra we know that a
third degree polynomial has exactly three roots, or, has at most three distinct roots, a pair of
which could be complex. Even if you are a junior college student, you are expected to know
this, may be without proof. These roots are the eigenvalues of f , because det(f −λI) = 0
only when λ equals one of the roots or, the eigenvalue equation (Eq. (5.2)) is satisﬁed only
when λ equals one of the roots. We assume that this cubic polynomial in λ has three real
roots, that is, all the eigenvalues of f are real. (That is, we assume the operator f to be a
symmetric operator, to be deﬁned in the next section).
Once the eigenvalues are known, the corresponding eigenvectors are found from
Eq. (5.2). We expand u in Eq. (5.2) in terms of the basis { ˆσk} and write Eq. (5.2) in the
form
g1u1 + g2u2 + g3u3 = 0,
(5.6)
where the vectors
gk = fk −λ ˆσk k = 1,2,3
(5.7)
are known for each eigenvalue λ and the scalar components uk = u · ˆσk of the eigenvector
are to be determined2 for one eigenvalue λ. We can solve Eq. (5.6) for the ratios of uk as
follows. Cross Eq. (5.6) with g3 to get
(g3 × g1)u1 + (g3 × g2)u2 = 0.
(5.8)
Dotting this with (g3 × g2)/|g3 × g2|2 we get
u2
u1
= (g3 × g1) · (g2 × g3)
|g3 × g1|2
·
(5.9)
Similarly,
u3
u1
= (g1 × g2) · (g2 × g3)
|g2 × g3|2
·
(5.10)
We have already seen that if u satisﬁes Eq. (5.2), so does any of its scalar multiples. This
means that the length or the sense (orientation) of u is not determined by the eigenvector
equation (Eq. (5.2)). Therefore, it is not a surprise that Eq. (5.6) ﬁxes only the ratios of the
components of u and we are free to ﬁx the sign and magnitude of u by assigning any
convenient value to the component u1. After u1 is assigned a value, Eq. (5.9) and
Eq. (5.10) determine u2 and u3 uniquely. Here, we have assumed that every pair of vectors
2Note that the vectors (g1,g2,g3) must be coplanar, otherwise they form a linearly independent set of vectors and Eq. (5.6)
has only the trivial solution ui = 0, i = 1,2,3.

Eigenvalues and Eigenvectors
137
formed out of (g1,g2,g3) is linearly independent, if not, all of them will be proportional
to each other3, in which case the ratios u2/u1 and u3/u1 obtained via Eq. (5.9) or
Eq. (5.10) will become indeterminate and Eqs (5.9), (5.10) do not apply. In such a case we
can proceed as follows. Since gks are proportional to each other, we can put g2 = cg1 and
g3 = dg1 in Eq. (5.6) to get
(u1 + cu2 + du3)g1 = 0,
or, since g1 , 0,
u1 + cu2 + du3 = 0.
(5.11)
Thus, we can give arbitrary values to any two of the components of u and the remaining
component is ﬁxed via Eq. (5.11). We can choose two sets of ui values in such a way that
the resulting eigenvectors (via Eq. (5.6) or Eq. (5.11)) are linearly independent. Setting
u1 = u2 = 1 in Eq. (5.6) for example, gives,
g1 + g2 + u3g3 = 0.
(5.12)
Alternatively, choose u1 = 1 and u3 = 0 so Eq. (5.6) reduces to
g1 + u2g2 = 0.
(5.13)
To get the respective eigenvectors, we have to solve Eq. (5.12) for u3 and Eq. (5.13) for u2
respectively, which is trivially done using known g1 ∝g2 ∝g3. These eigenvectors are
trivially seen to be linearly independent. Any eigenvector corresponding to a different
choice of components will be a linear combination of these two eigenvectors. Thus,
linearly dependent pairs of gks (so that they are mutually proportional) imply that the
eigenvectors belonging to the corresponding eigenvalue span a 2-D space i.e., a plane. This
is to be contrasted with the fact that when every pair of the gks is linearly independent, the
eigenvectors belonging to the corresponding eigenvalue span a 1-D space.
It turns out that if λ is a simple root of the secular equation (for a symmetric linear
operator f ), then every two of the three vectors gk = fk −λ ˆσk are necessarily linearly
independent. Thus, the eigenvectors belonging to a simple root λ span a 1-D space i.e.,
a real line in E3. If λ is a double root, every pair of gks is linearly dependent so that the
eigenvectors belonging to a double root λ span a 2-D space, i.e., a plane in E3. Note that any
two linearly independent (i.e., non-collinear) vectors in this plane can be the eigenvectors.
A multiple root of a secular equation is said to be k-fold degenerate if the root has
multiplicity k. To an eigenvalue with multiplicity k there correspond exactly k linearly
independent eigenvectors (in E3, provided f is symmetric).
Eigenvalues of a symmetric operator are real. To get a ﬂavor of the complex eigenvalues,
(i.e., complex roots of the secular equation) consider the skewsymmetric operator
f x = x × ( ˆσ1 × ˆσ2)
3This is because (g1,g2,g3) have to satisfy Eq. (5.6) with one or more ui , 0.

138
An Introduction to Vectors, Vector Operators and Vector Analysis
where { ˆσk} is some orthonormal basis. Operating on this basis we get
f ( ˆσ1) =
ˆσ1 × ( ˆσ1 × ˆσ2) = −ˆσ2 = −i ˆσ1
f ( ˆσ2) =
ˆσ2 × ( ˆσ1 × ˆσ2) = ˆσ1 = −i ˆσ2
f ( ˆσ3) = 0 = 0 ˆσ3
(5.14)
Exercise
Show that the secular equation for f is
λ(λ2 + 1) = 0
□
The root λ = 0 corresponds to the eigenvector ˆσ3 in Eq. (5.14). The eigenvalue equations
for the eigenvalue −i the ﬁrst two of Eq. (5.14). The last equalities in these equations
derive from the fact that multiplication of a vector in the complex plane by −i results in
the clockwise rotation of that vector through π/2. In general, multiplication by eiθ results
in the counterclockwise rotation through θ. Thus, we see that complex eigenvalues result
in the rotation of the eigenvectors.
5.1.1
Examples
We obtain the eigenvalues and eigenvectors of the operator f represented by the matrix
[f ] =


4
−1
−1
−1
4
−1
−1
−1
4


in an orthonormal basis { ˆσ1, ˆσ2, ˆσ3}.
Operating on the basis by f we get
f ( ˆσ1) = 4 ˆσ1 −ˆσ2 −ˆσ3 = f1
f ( ˆσ2) = −ˆσ1 + 4 ˆσ2 −ˆσ3 = f2
f ( ˆσ3) = −ˆσ1 −ˆσ2 + 4 ˆσ3 = f3
(5.15)
From these vectors we ﬁnd
f1 × f2 = 15( ˆσ1 × ˆσ2) + 5( ˆσ2 × ˆσ3) + 5( ˆσ3 × ˆσ1)
f2 × f3 = 5( ˆσ1 × ˆσ2) + 15( ˆσ2 × ˆσ3) + 5( ˆσ3 × ˆσ1)
f3 × f1 = 5( ˆσ1 × ˆσ2) + 5( ˆσ2 × ˆσ3) + 15( ˆσ3 × ˆσ1)
(5.16)

Eigenvalues and Eigenvectors
139
Using Eqs (5.15) and (5.16) we get the values of the coefﬁcients in the secular equation,
α1 = 4 + 4 + 4 = 12
α2 = 15 + 15 + 15 = 45
α3 = f1 · (f2 × f3) = 50
(5.17)
Hence, the secular equation is
λ3 −12λ2 + 45λ −50 = 0
which can be factored into
(λ −2)(λ −5)2 = 0.
So the eigenvalues are 2 (simple) and 5 (doubly degenerate).
To get the eigenvectors for λ = 2 we prepare the vectors gk deﬁned in Eq. (5.7) using
Eq. (5.15). We have,
g1 = f1 −2 ˆσ1 = 2 ˆσ1 −ˆσ2 −ˆσ3
g2 = f2 −2 ˆσ2 = −ˆσ1 + 2 ˆσ2 −ˆσ3
g3 = f3 −2 ˆσ3 = −ˆσ1 −ˆσ2 + 2 ˆσ3
(5.18)
From this we ﬁnd
g1 × g2 = 3( ˆσ1 × ˆσ2 + ˆσ2 × ˆσ3 + ˆσ3 × ˆσ1) = g2 × g3 = g3 × g1
Using this in Eqs (5.9) and (5.10) with u1 = 1 we get u2 = u3 = 1. Hence,
u1 = ˆσ1 + ˆσ2 + ˆσ3
(5.19)
is the eigenvector belonging to the eigenvalue 2.
To get the eigenvectors corresponding to λ = 5, we evaluate gk = fk −5 ˆσk to ﬁnd
g1 = g2 = g3 = −( ˆσ1 + ˆσ2 + ˆσ3)
Using this in Eq. (5.12) we ﬁnd u3 = −2 when u1 = u2 = 1 so that
u2 = ˆσ1 + ˆσ2 −2 ˆσ3
(5.20)

140
An Introduction to Vectors, Vector Operators and Vector Analysis
is an eigenvector for λ = 5. To get the other linearly independent eigenvector, we ﬁnd,
from Eq. (5.13), that u2 = −1 when u1 = 1 and u3 = 0. Hence,
u3 = ˆσ1 −ˆσ2
(5.21)
is the other eigenvector. Therefore, every vector in the plane deﬁned by u2 and u3 is an
eigenvector with eigenvalue λ = 5. (Operate by f on any linear combination of u2 and u3).
Although, our method to ﬁnd the eigenvalues and eigenvectors is sufﬁciently general, it
may cost us more work than necessary in special cases. Often, an eigenvector is known in
advance. Then, the corresponding eigenvalue is easily obtained via
(f (u) · u)/|u|2
instead of using the secular equation. More often than not, an eigenvector can be identiﬁed
easily from the symmetries in the given problem. Thus, perusal of Eq. (5.15) shows that
adding these three equations we get
f ( ˆσ1 + ˆσ2 + ˆσ3) = 2( ˆσ1 + ˆσ2 + ˆσ3),
so we know that 2 is the eigenvalue corresponding to eigenvector u = ˆσ1 + ˆσ2 + ˆσ3, in
agreement with the result of the general method, obtained after a lot of effort. In order to
get the other two eigenvectors u1 and u2 we use the fact that any two vectors in the (u1,u2)
plane will do. As we shall see in the next section, (u1,u2) plane is perpendicular to u1. So
we can write u2 = ˆσ1 + ˆσ2 + u3 ˆσ3 and choose u3 such that
u1 · u2 = ( ˆσ1 + ˆσ2 + ˆσ3) · ( ˆσ1 + ˆσ2 + u3 ˆσ3) = 2 + u3 = 0
This gives u3 = −2, so u2 = ˆσ1 + ˆσ2 −2 ˆσ3 which coincides with Eq. (5.20). From
Eq. (5.15) we now ﬁnd f (u2) = 5u2 so the eigenvalue is 5. The vector u1 × u2 =
−3( ˆσ1 −ˆσ2) is orthogonal to both u1 and u2 and is proportional to the eigenvector u3 in
Eq. (5.21).
Exercise
Obtain the eigenvectors and eigenvalues of the operators represented by the
following matrices in an orthonormal basis.
(i)
[f ] =


1
0
5
0
−2
0
5
0
1


·
(ii)
[f ] =


7
√
6
−
√
3
√
6
2
−5
√
2
−
√
3
−5
√
2
−3


·

Eigenvalues and Eigenvectors
141
(iii)
[f ] =


1
2
0
2
6
−2
0
−2
5


·
□
Exercise
Let u be an eigenvector of an invertible operator with eigenvalue λ. Show that
u is an eigenvector of f −1 with eigenvalue 1
λ.
Hint
Multiply the eigenvalue equation f u = λu by f −1 and use the deﬁnition and the
linearity of f −1.
□
5.2
Spectrum of a Symmetric Operator
We have already stated that the eigenvalues of a symmetric linear operator are real. We shall
now prove this statement. We will also score a bonus point by proving that the eigenvectors
of a symmetric operator corresponding to different eigenvalues are orthogonal.
We have seen that the eigenvalues of a linear operator f on E3 can, in general, be
complex. This makes it necessary for us to give a meaning to the multiplication of vectors
in E3 by complex numbers. Note that such a multiplication cannot be taken to be a
multiplication by a scalar, because E3 is a real linear space with a Euclidean metric, so that
scalars comprise only real numbers. A multiplication by a complex number λ = reiθ
involves multiplication by the real number r which will simply multiply the magnitude of
the vector by r. Thus, we have to worry about the interpretation of the multiplication by
eiθ. Such a multiplication can be given a meaning by noting that multiplication by a
complex number of unit magnitude (eiθ) is equivalent to the rotation of the vector
through angle θ in a suitable plane. We have already seen this in section 3.1, (where we
proved the spaces E2 and Z to be isomorphic), only difference being the space we
consider there was E2 rather than E3. To make this interpretation precise for E3, we
consider the scalar product of two linearly independent (non-collinear) vectors in E3 such
as u · (eiθv). To evaluate this scalar product, we have to rotate v, in the plane spanned by
u and v, counterclockwise through angle θ and then dot the resulting vector with u.
Equivalently, we could have rotated u clockwise through angle θ and dotted the resulting
vector with v. This is depicted in Fig. 5.1.
Fig. 5.1
u ·

eiθv

=

e−iθu

· v

142
An Introduction to Vectors, Vector Operators and Vector Analysis
However, the alternative scalar product is just (e−iθu) · v. Thus, we have the general result
u · (λv) = (λ∗u) · v
where λ = reiθ is any complex number and λ∗its complex conjugate.
Now consider a symmetric linear operator S and its eigenvectors u and v belonging to
the eigenvalues λ1 and λ2 respectively, presumably complex. We have,
(λ1u) · v = S(u) · v = u · S(v) = u · (λ2v) = (λ∗
2u) · v
(5.22)
where we have used the fact that S is symmetric. Remember that when λ is complex, the
vectors u and λu are not collinear. Equation (5.22) gives
(λ1 −λ∗
2)u · v = 0.
(5.23)
Two cases arise. In the ﬁrst case, λ1 = λ2 = λ and the scalar product in Eq. (5.23) is non-
zero. This gives λ = λ∗. This proves that the eigenvalues of a symmetric operator are real.
In the second case, λ1 , λ2 so that the scalar product of the two eigenvectors u and v must
vanish. This simply means that the eigenvectors belonging to two different eigenvalues of a
symmetric operator are orthogonal.
We now show that for every symmetric operator on E3, there exists a set of eigenvectors
which are mutually orthogonal. The axes of the resulting frame are called the principal
axes. If all the three eigenvalues of the given symmetric operator are distinct, then this
statement follows from the fact that the eigenvectors belonging to different eigenvalues of a
symmetric operator must be orthogonal. Further, in this case, the eigenvectors are unique
upto multiplication by a scalar (there is no degeneracy) so that all the principal axes are
unique.
Now suppose λ1 , λ2, λ1 , λ3 but λ2 = λ3 = λ say. Since λ1 is distinct from λ2
and λ3, the eigenvector u1 belonging to λ1 must be orthogonal to both the eigenvectors
belonging to the degenerate eigenvalue λ. Further, we know that the eigenvectors belonging
to λ are linearly independent (non-collinear) and every vector in the plane spanned by two
linearly independent eigenvectors for λ is also an eigenvector. Thus, we can take any vector
in the plane normal to u1 as one of the eigenvectors, say u2 of the eigenvalue λ and the
third eigenvector u3 can be obtained from
u3 = u1 × u2.
If all the three eigenvalues are equal to say λ, three linearly independent eigenvectors
belong to this common eigenvalue λ and every linear combination of them is also an
eigenvector. In other words, every vector in E3 is an eigenvector belonging to λ.
Obviously, any orthonormal triad of vectors (u1,u2,u3) gives a principal axes system.
The fact that a symmetric operator S has three orthogonal principal axes is expressed
by the equations
Suk = λkuk k = 1,2,3

Eigenvalues and Eigenvectors
143
and
uj · uk = 0 if j , k.
Thus, a symmetric operator is not only diagonalizable, but its eigenvectors naturally form
an orthogonal basis.
Thus, we see that the eigenvectors of a symmetric operator form an orthogonal basis
of E3. This basis is called the eigenbasis of the symmetric operator and the corresponding
eigenvectors are called principal vectors and eigenvalues are called the principal values. If
we denote this basis by (u1,u2,u3) we can write an arbitrary vector x ∈E3 as a linear
combination of the eigenvectors as
x = α1u1 + α2u2 + α3u3.
Dotting both sides by uk k = 1,2,3, using the orthogonality of the eigenvector basis and
dividing both sides by |uk|2 we get
αk = (uk · x)/|uk|2.
Thus, the result of operating by a symmetric operator S on a vector x ∈E3 can be
expressed as
Sx =
3
X
k=1
λkuk
"uk · x
|uk|2
#
=
3
X
k=1
λk ˆuk(ˆuk · x)
(5.24)
where ˆuk is a unit vector in the direction of uk. Equation (5.24) is often written in terms of
the so called projection operators. Thus,
Sx =


3
X
k=1
λkPk

x,
where the projection operator Pk which projects any vector x ∈E3 onto the kth principal
axis along uk is given by
Pkx = ˆuk(ˆuk · x).
(5.25)
In terms of the projection operators, Eq. (5.24) can be re-expressed as
Sx =


X
k
λkPk

.
(5.26)
The canonical form Eq. (5.24) or Eq. (5.26) is called the spectral decomposition (or the
spectral form) of the symmetric operator S. Note that if we use an eigenvector uk in place
of x in the spectral decomposition, the eigenvalue equation for uk emerges trivially.

144
An Introduction to Vectors, Vector Operators and Vector Analysis
As we have already seen, an eigenvector multiplied by a scalar continues to be the
eigenvector for the same eigenvalue. Thus, we can divide each of the eigenvectors by its
magnitude to get the unit vector in its direction and this unit vector continues to be the
eigenvector for the same eigenvalue. In this way, we can convert the orthogonal basis
comprising eigenvectors to the orthonormal basis comprising unit eigenvectors given by
{ˆu1, ˆu2, ˆu3}. We have used this orthonormal eigenbasis of the symmetric operator S while
obtaining Eqs (5.24), (5.26). We will always set up the matrix of a symmetric operator with
respect to its orthonormal eigenbasis.
Exercise
Using its deﬁnition, establish the following properties of projection operators.
(a) Orthogonality: PjPk = 0 if j , k
(b) Idempotence: P 2
k = Pk
(c) Completeness: P1 + P2 + P3 = I
□
Exercise
Show that the matrix for a symmetric operator in its orthonormal eigenbasis is
diagonal, with its eigenvalues appearing on the diagonal.
□
Since the determinant of a diagonal matrix is the product of its diagonal elements, the
determinant of a symmetric operator is the product of its eigenvalues. Thus, if one or more
of the eigenvalues of a symmetric operator are zero, its determinant is zero.
Such a
symmetric operator is singular, and hence non-invertible.
The matrix [S] representing a symmetric operator in an orthonormal basis { ˆσk} k =
1,2,3 is symmetric. We have, for the ijth element of such a matrix,
Sij = ˆσi · S( ˆσj) = S( ˆσi) · ˆσj = ˆσj · S( ˆσi) = Sji
where we have used the fact that S is symmetric. On the other hand, the matrix [A ]
representing a skewsymmetric operator A in an orthonormal basis is skewsymmetric. We
have, for the ijth element of the matrix [A ],
Aij = ˆσi · A ( ˆσj) = −A ( ˆσi) · ˆσj = −ˆσj · A ( ˆσi) = −Aji
where we have used the fact that A is skewsymmetric. Obviously, a matrix representing
a skewsymmetric operator has vanishing diagonal elements because they have to satisfy
Aii = −Aii. That means, the pairs ( ˆσi,A ˆσi),i = 1,2,3 are orthogonal.
As you may have noticed, all the matrices given in the exercise at the end of the last
section are symmetric.
From the spectral decomposition for a non-singular symmetric operator S, we can
write, for the inverse operator,
S−1 =
X
k
1
λk
Pk.
(5.27)

Eigenvalues and Eigenvectors
145
Exercise
Using the spectral decomposition of S (Eq. (5.26)) and that of S−1 (Eq. (5.28))
verify explicitly S−1S = I = SS−1.
□
We can show that the inverse of a symmetric operator is also symmetric. We have, for all
x ∈E3 (SS−1)†x = Ix = x implies (S−1)†Sx = x = S−1Sx which means (S−1)† = S−1
because the inverse is unique.
A symmetric operator S is called positive, if all its eigenvalues λk > 0,
k = 1,2,3
and non-negative if λk ≥0, k = 1,2,3. A positive symmetric operator S is also non-
negative, however, the converse is not true. A general linear operator f is called positive
(non-negative) if f (x) · x > 0,(≥0) for every x , 0.
Exercise
Show that a non-negative symmetric operator S has a unique square root
S1/2 =
X
k
λ1/2
k
Pk
in the sense that S1/2(S1/2x) = Sx for all x ∈E3.
□
The square root of a non-negative symmetric operator is a non-negative symmetric
operator which is obvious from its deﬁnition.
Remark
A positive symmetric operator, say S+, acting on any of its eigenvectors,
changes its length by multiplying it by a positive number, namely by its eigenvalue. Since
the eigenvectors form an orthogonal basis, the effect of the action of a positive symmetric
operator on an arbitrary vector can be completely accounted by the change in length of the
eigenvectors due to the action of the positive symmetric operator on them.
This leads to some interesting geometric consequences. Thus, S+ stretches a circle
drawn in a principal plane (containing two principal vectors) into an ellipse. In particular,
S+ stretches a unit circle drawn in a principal plane into an ellipse for which the lengths of
the semiaxes are the principal values of S+. We see from Fig. 5.2 that S+ stretches the
points on a square to points on a parallelogram.
Fig. 5.2
Symmetric transformation with principal values λ1 > 1 and λ2 < 1

146
An Introduction to Vectors, Vector Operators and Vector Analysis
A positive symmetric operator S+ on E3 transforms the unit sphere into ellipsoid. The
transformation is,
x = S+ ˆn
(5.28)
where ˆn is any unit vector. This is a parametric equation for the ellipsoid with vector
parameter ˆn. A non-parametric equation can be obtained by eliminating ˆn as follows.

S−1
+ x
2 = ˆn2 = 1.
Since S−1
+ is symmetric, we have,

S−1
+ x
2 = S−1
+ x · S−1
+ x = x ·

S−1
+
2 x = 1.
(5.29)
Now using the spectral decomposition of S−1
+
(Eq. (5.28)) and the properties of the
projection operator we can write Eq. (5.29) in the form
x2
1
λ2
1
+ x2
2
λ2
2
+ x2
3
λ2
3
= 1,
(5.30)
where xk = xk ·uk. Equation (5.30) is the standard equation for an ellipsoid with semiaxes
λ1,λ2,λ3 (see Fig. 5.3).
In some situations, eigenvalues and eigenvectors are supplied as the initial information,
so that the corresponding symmetric operator can be constructed directly from its
spectral decomposition. Some variants of the spectral form are more convenient in certain
applications. All these variants are, of course, constructed from the eigenvectors and
eigenvalues.
Fig. 5.3
An ellipsoid with semi-axes λ1,λ2,λ3
Exercise
Describe the eigenvalue spectrum of a symmetric operator S so that the
equation
x · (Sx) = 1
(5.31)

Eigenvalues and Eigenvectors
147
is equivalent to the standard coordinate forms for each of the following quadratic surfaces.
(a) Ellipsoid:
x2
1
a2 + x2
2
b2 + x2
3
c2 = 1,
(b) Hyperboloid of one sheet:
x2
1
a2 + x2
2
b2 −x2
3
c2 = 1,
(c) Hyperboloid of two sheets:
x2
1
a2 −x2
2
b2 −x2
3
c2 = 1.
Answer
(a) All positive, (b) One negative, (c) Two negative eigenvalues.
□
Exercise
Obtain the solution set {x} of the equation
[f (x −a)]2 = 1
where f is any linear operator.
Solution
The given equation is
f (x −a) · f (x −a) = 1
or,
(x −a) · f †f (x −a) = 1.
f †f is obviously a symmetric operator. Call it S. Therefore, the above equation becomes,
(x −a) · S(x −a) = 1.
(5.32)
We know from the previous exercise that Eq. (5.32) corresponds to that for an ellipsoid if
all the eigenvalues of S are positive, hyperboloid of one sheet if one eigenvalue is negative
and hyperboloid of two sheets if two of the eigenvalues are negative. Obviously, there is no
solution for all negative eigenvalues of S. Note that, for Eq. (5.32), all the quadratic surfaces
are centered at a.
□
5.3
Mohr’s Algorithm
We have developed a general method of ﬁnding the spectrum of a linear operator acting on
E3. Many a time we have to deal with the problem of ﬁnding the spectrum of a (typically
symmetric) operator acting on a plane, which is a 2-D subspace of E3. Another situation

148
An Introduction to Vectors, Vector Operators and Vector Analysis
we may face is when one of the three eigenvectors of a symmetric operator is known and
the other two are to be found. The remaining eigenvectors lie in the plane normal to the
known eigenvector, so the problem reduces to that of ﬁnding the spectrum of an operator
acting on a plane. Although, we can employ the general method to do this job, for a positive
symmetric operator S+ an efﬁcient algorithm called Mohr’s algorithm is available. We ﬁrst
state the algorithm and then justify it.4
The algorithm comprises the following.
Choose any convenient unit vector ˆb in the plane and compute the two vectors
b± = S+ ˆb ∓iS+(−i ˆb)
where multiplication by eiθ rotates a vector counterclockwise through angle θ in the plane
on which the operator S+ acts. Then for b+ × b−, 0, the vectors
u± = α(b+ ± b−)
are the principal vectors of S+ with principal values
λ± = 1
2(|b+| ± |b−|).
We will discuss the case b+×b−= 0 which we have omitted from the algorithm. Obviously,
we assume that5 λ+ , λ−. This algorithm is called Mohr’s algorithm.
To understand the algorithm, we proceed as follows.
For a positive symmetric operator S+ acting on a plane, the eigenvalue equation can be
written,
S+u± = λ±u±
where u+ and u−are the principal vectors corresponding to the principal values λ+ and
λ−respectively.
Since S+ is a given operator, the vector S+ ˆb, resulting due to its action on any given
unit vector ˆb in the plane is known. We write u = ˆu+ and decompose ˆb into components
b∥and b⊥parallel and orthogonal to u respectively. We have,
S+ ˆb = S+(b∥+ b⊥) = λ+b∥+ λ−b⊥,
Exercise
Justify the second equality in this equation.
□
Or,
S+ ˆb = λ+u(u · ˆb) + λ−(u × ˆb) × u.
(5.33)
4Mohr’s algorithm is discussed using geometric algebra in [10].
5Otherwise, if S+ has a single doubly degenerate eigenvalue, every vector in the plane is an eigenvector and the other linearly
independent eigenvector is simply the one in the same plane and normal to it.

Eigenvalues and Eigenvectors
149
Introducing the vector
ˆx = 2(u · ˆb)u −ˆb
we can re-write Eq. (5.33) (Exercise) as,
S+ ˆb = 1
2(λ+ + λ−)ˆb + 1
2(λ+ −λ−)ˆx.
(5.34)
Exercise
Show that the vector ˆx deﬁned above is a unit vector.
□
Let φ denote the angle through which ˆb has to be counterclockwise rotated to meet u. Then
u = eiφ ˆb.
(5.35)
Thus, Eq. (5.34) involves three unknowns λ+,λ−and φ (through ˆx) so we need another
equation to solve for these unknowns. We have (Exercise)
−iS+(i ˆb) = 1
2(λ+ + λ−)ˆb −1
2(λ+ −λ−)ˆx.
(5.36)
Combining Eqs (5.35) and (5.36) we get
b+ = S+ ˆb −iS+(i ˆb) = (λ+ + λ−)ˆb
(5.37)
b−= S+ ˆb + iS+(i ˆb) = (λ+ −λ−)ˆx.
(5.38)
Without losing generality, we assume λ+ ≥λ−, so Eq. (5.37) show that the principal
values are determined by the magnitudes |b±| = λ+ ± λ−of the known vectors b+ and
b−, produced by the known action of S+ on the vectors ˆb and i ˆb. Dotting the unit vector
equation ˆb−= ˆx with u we have,
u · ˆb−= u · ˆb = cosφ.
(5.39)
Equation (5.39) tells us that direction of u is half way between the directions ˆb−, (or ˆx) and
ˆb = ˆb+. Therefore,
u+ = α(ˆb+ + ˆb−)
(5.40)
is an eigenvector of S+ for any non-zero scalar α. If ˆb+ × ˆb−, 0 then
u−= α(ˆb+ −ˆb−)
(5.41)
is the other eigenvector, because u+ · u−= 0. Thus,
u± = α(ˆb+ ± ˆb−).
(5.42)

150
An Introduction to Vectors, Vector Operators and Vector Analysis
If ˆb+ × ˆb−= 0 then ˆb is parallel or antiparallel to one of the principal vectors. Then ˆx =
±α ˆb and Eq. (5.42) yields only that vector. The other eigenvector is perpendicular to the
one found.
This completes the proof of Mohr’s algorithm. Figure 5.4 depicts the parameters in
Mohr’s algorithm.
Fig. 5.4
Parameters in Mohr’s algorithm
Exercise
Show that
tan2φ = |ˆb+ × ˆb−|
ˆb+ · ˆb−
,
(5.43)
so that the principal vectors u± can be obtained via Eq. (5.35).
Mohr’s algorithm is routinely used by engineers to solve eigenvalue problem on a plane
by graphical means. Here, the key construction is the Mohr’s circle (Fig. 5.5). The
parametric equation for the Mohr’s circle can be obtained from Eqs (5.34) and (5.35) as
Z(φ) = ˆb · S+ ˆb = 1
2(λ+ + λ−) + 1
2(λ+ −λ−)cos2φ.
(5.44)
To see it as an equation to a circle, replace cos2φ by ei2φ. It is then clear that this circle
has radius 1
2(λ+ −λ−) and its center is at a distance 1
2(λ+ +λ−) from the origin along the
x axis. To solve for two unknowns (λ+,λ−), Z must be known for two values of φ. The
choice corresponding to Eq. (5.36) is
Z⊥(φ) = Z(φ + π
2 ) = 1
2(λ+ + λ−) −1
2(λ+ −λ−)cos2φ.
(5.45)
Solution of Eqs (5.44) and (5.45) is, of course, equivalent to Mohr’s algorithm.
The graphical solution to Eqs (5.44) and (5.45) is obtained as follows. First, we choose a
value of φ and choose positive direction of x axis along the vector ˆb deﬁned by Eq. (5.35).

Eigenvalues and Eigenvectors
151
This ﬁxes ˆb. Knowing the action of S+ on ˆb, we can ﬁnd the value of Z(φ) = ˆb · S+ ˆb.
We can calculate the value of Z⊥(φ) = Z(φ + π
2 ) in the same way, by noting that the
corresponding ˆb vector is orthogonal to the x axis. Now, we draw a straight line making an
angle 2φ with the positive direction of x axis, intersecting it at a point O. Then, we mark
out two points S1 and S2 on this line, which are equidistant from O and are at distances
Z(φ) and Z⊥(φ) from the origin respectively. While doing this, we may have to slide this
line parallel to itself along the x axis. Finally, we draw a circle (Mohr’s circle) with its center
at O and radius OS1,2. This circle cuts the x axis at two points at distance λ−(closer)
and λ+ (farther) from the origin, giving us the required eigenvalues. All this is depicted
in Fig. 5.5. One eigenvector is along the bisector of the angle 2φ made with the positive
direction of x axis and the second eigenvector is orthogonal to it.
Fig. 5.5
Mohr’s Circle
Exercise
Where exactly would Mohr’s algorithm fail if the operator S+ was not positive?
□
5.3.1
Examples
(a) Using Mohr’s algorithm, we solve the eigenvalue problem for the operator
S+(b) = (a × b) × a + (c × b) × c,
(5.46)
where a and c are not collinear.
Exercise
Show that S+(b) in Eq. (5.46) is both symmetric and positive.
□
The operator in Eq. (5.46) is the general form of the moment of inertia operator of a
plane lamina. Note that
S+ˆa = (c × ˆa) × c = c2ˆa −(ˆa · c)c,
(5.47)
where we have used identity I involving vector triple product and
−iS+(iˆa) = a2ˆa + (c · ˆa)c.
(5.48)
To understand Eq. (5.48), make use of Fig. 5.6 and construct the vector
−iS+(iˆa) = −i[(a × iˆa) × a + (c × iˆa) × c]
(5.49)

152
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 5.6
Veriﬁcation of Eq. (5.48)
on this ﬁgure. Convince yourself that the result holds arbitrary a and c. Otherwise,
using the standard identity for the vector triple product and noting that ˆa · iˆa = 0 we
have, from Eq. (5.49),
−iS+(iˆa) = −i
h
a2(iˆa) + c2(iˆa) −c2(iˆa · ˆc) · ˆc
i
.
(5.50)
The vector formed by the last two terms in Eq. (5.50) is orthogonal to c in the
direction −iˆc with magnitude c2 sinθ where θ is the angle between iˆa and c.
Writing sinθ = sin(φ −π/2) = −cosφ = −(ˆc· ˆa) (see Fig. 2.7) we see that the last
two terms in the bracket in Eq. (5.50) correspond to the vector −c2(ˆc · ˆa)(−iˆc). We
now multiply by −i in Eq. (5.50) to get
−iS+(iˆa) = a2ˆa + c2(ˆc · ˆa)ˆc = a2ˆa + (c · ˆa)c.
(5.51)
As the next step we obtain
a+ = S+ˆa −iS+(iˆa) = (a2 + c2)ˆa,
a+ = (c2 −a2)ˆa −2(c · ˆa)c.
(5.52)
It is trivial to check that
|a−| =

c2 −a22 + 4(c · ˆa)21/2
and |a+| = (a2 + c2) so that we get for the principal values
λ± = 1
2(|a+| + |a−|)
= 1
2

a2 + b2
± 1
2

c2 −a22 + 4(c · ˆa)21/2
.
(5.53)
By Eq. (5.42) the corresponding principal vectors are
ˆu = ˆa ±

c2 −a2
ˆa −2(c · ˆa)c
h
(c2 −a2)2 + 4(c · ˆa)2i1/2 .
(5.54)

Eigenvalues and Eigenvectors
153
We note that a free choice of the argument b in Mohr’s algorithm enabled us to
simplify the computation by taking the special structure of the operator S+ into
account.
(b) We ﬁnd the eigenvalues and eigenvectors of the operator
S+(b) = a(a · b) + c(c · b).
Exercise
Show that S+(b) is symmetric as well as positive.
□
As in the previous example, we evaluate S+ at the deﬁning unit vector ˆa. We have,
S+ˆa = a2ˆa + (c · ˆa)
and
−iS+(iˆa) = −i(c · iˆa).
These can be written, by resolving c along ˆa and iˆa as
S+ˆa = a2ˆa + (·ˆa)2ˆa + (c · ˆa)(c · iˆa)iˆa,
−iS+(iˆa) = (c · iˆa)2ˆa + (c · ˆa)(c · iˆa)iˆa.
This gives
a+ = S+ˆa −iS+(iˆa) = (1 + a2)ˆa + 2(c · ˆa)(c · iˆa)iˆa
a−= S+ˆa + iS+(iˆa) = [a2 + (c · ˆa)2 −(c · iˆa)2]ˆa
(5.55)
or,
|a−| = a2 + (c · ˆa)2 −|c × ˆa|2
|a+| = [(1 + a2)2 + 4(c · ˆa)2|c × ˆa|2]1/2.
(5.56)
Therefore,
λ± = 1
2(|a+| + |a−|)
= 1
2

1 + a22 + 4(c · ˆa)2 |c × ˆa|21/2
± 1
2
h
a2 + (c · ˆa)2 −|c × ˆa|2i
.
(5.57)

154
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Show that an eigenvector
ˆu of S+
is obtained by rotating
ˆa
counterclockwise by an angle φ satisfying
tan2φ =
 
c2
1 + a2
!
sin2θ
where θ is the angle between a and c.
□
(c) We ﬁnd the eigenvalues and the eigenvectors of the operator
S+(e) = (a × e) × a + (b × e) × b + (c × e) × c,
(5.58)
where
a + b + c = 0.
Note that the condition a+b+c = 0 makes the vectors a,b,c coplanar and the given
operator then acts on the plane containing a,b,c.
Exercise
Show that S+ in Eq. (5.58) is both, symmetric and positive.
Hint
Use identity I.
□
Thus, Mohr’s algorithm applies. We proceed on the same line as the previous
examples. We get,
S+(ˆa) = (b2 + c2)ˆa −(ˆa · b)b −(ˆa · c)c
(5.59)
−iS+(iˆa) = a2ˆa + (ˆa · b)b + (ˆa · c)c.
(5.60)
This gives,
a+ = S+(ˆa) −iS+(iˆa) = (a2 + b2 + c2)ˆa,
(5.61)
a−= S+(ˆa) −iS+(iˆa) = (b2 + c2 −a2)ˆa −2(ˆa · b)b −2(ˆa · c)c.
(5.62)
We now make use of the condition a + b + c = 0 and simplify Eq. (5.61) to
a+ = 2(a2 + b2 + a · b)ˆa
(5.63)
a−= 2(b2 −a2)ˆa −4(ˆa · b)b.
(5.64)
We have,
|a−| = 2[(b2 −a2)2 + 4(a · b)2]1/2.

Eigenvalues and Eigenvectors
155
Therefore, the principal values are
λ± = 1
2(|a+| ± |a−|) = (a2 + b2 + a · b) ± [(b2 −a2)2 + 4(a · b)2]1/2
and the corresponding eigenvectors are
u± = ˆa ±
(b2 −a2)ˆa −2(ˆa · b)b
[(b2 −a2)2 + 4(a · b)2]1/2 .
Exercise
Find the eigenvalues and the eigenvectors of the positive symmetric
operator
S+(b) = a(c · b) + c(a · b).
□
Unfortunately, no generalization of Mohr’s algorithm to the eigenvalue problem of a
positive symmetric operator acting on 3-D space is available. However, as mentioned
before, knowledge of one eigenvector enables us to use Mohr’s algorithm to ﬁnd the
remaining two eigenvectors in the plane orthogonal to the known eigenvector. For
example, any operator constructed out of two vectors a and c necessarily has a × c as
an eigenvector. Thus, for the operator in Eq. (5.46) we ﬁnd
S+(a × c) = (a2 + c2)(a × c).
5.4
Spectrum of a 2 × 2 Symmetric Matrix
We ﬁnd the eigenvalues and the eigenvectors of an operator f acting on a plane, given by a
symmetric matrix
[S] =
"S11
S12
S12
S22
#
,
with respect to an orthonormal basis ( ˆσ1, ˆσ2).
The roots of the characteristic polynomial of a 2 × 2 matrix A is easily seen to be
λ± = 1
2[T r(A) ±
q
(T r(A))2 −4 det(A)]
where T r(A) and det(A) mean the trace and the determinant of A respectively. This
immediately gives, for the eigenvalues of the symmetric matrix [S] above,
λ± = 1
2(S11 + S22) ± 1
2[(S11 −S22)2 + 4S2
12]1/2.

156
An Introduction to Vectors, Vector Operators and Vector Analysis
To get the eigenvectors, we assume the plane to be the complex plane and identify the
orthonormal basis ( ˆσ1, ˆσ2) with ( ˆσ1,i ˆσ1). In this basis, ˆσ1 has coordinates (1,0) and
ˆσ2 = i ˆσ1 has coordinates (0,1). Therefore,
S( ˆσ1) =
"S11
S12
S12
S22
#"1
0
#
=
"S11
S12
#
and
S(i ˆσ1) = S( ˆσ2) =
"S11
S12
S12
S22
#"0
1
#
=
"S12
S22
#
.
The vector −iS(i ˆσ1) is obtained by rotating S(i ˆσ1) clockwise through π/2. This will
interchange its coordinates so that
−iS(i ˆσ1) =
"S22
S12
#
and we get,
b+ = S( ˆσ1) −iS(i ˆσ1) =
"S11 + S22
2S12
#
,
and
b−= S( ˆσ1) + iS(i ˆσ1) =
"S11 −S22
0
#
·
We immediately see that b−is in the direction of ˆσ1. So the angle χ between ˆσ1 and b+ is
tanχ =
2S12
S11 + S22
.
However, we have seen before that the eigenvector u bisects the angle between b+ and b−
or ˆσ1. Denoting the angle between u and ˆσ1 to be φ, we get
tan(2φ) =
2S12
S11 + S22
.
5.5
Spectrum of Sn
We deﬁne Sn = S ◦S ◦S ···◦S (n times), where S ◦S(x) = S(S(x)). Let S be a symmetric
operator on E3 with eigenvalues {λk}. We show that Sn is symmetric with eigenvalues λn
k
and Sn has the same eigenvectors as S.

Eigenvalues and Eigenvectors
157
We prove the ﬁrst claim by induction on n. Assume that for n = l,
Sl =
X
k
(λk)l Pk.
(5.65)
is the spectral representation of Sl given by the projection operators Pk deﬁned by
Eq. (5.25). Consider,
Sl+1 =


X
k
(λk)lPk




X
j
(λj)Pj


=
X
k,j
(λk)lλjPjPk =
X
k,j
(λk)lλjδjkPk
=
X
k
(λk)l+1Pk,
(5.66)
where we have used the property PjPk = δjkPk of the projection operators. Thus, we have
shown that if Sl is a symmetric operator (because of its spectral representation in terms
of projectors) with eigenvalues (λk)l then Sl+1 is a symmetric operator with eigenvalues
(λk)l+1. However, we know that Eq. (5.65) is true for l = 1 which is simply the spectral
representation of the symmetric operator S. Therefore, by induction, Eq. (5.65) must be
true for any value l = n.
To show that Sn and S share the same set of eigenvectors, consider an eigenvector u of
S with eigenvalue λ. We have,
Sn(u) = Sn−1(Su) = Sn−1(λu) = Sn−2(λSu) = Sn−2(λ2u) = ··· = λnu.
Thus, we see that if u is an eigenvector of a symmetric operator S with an eigenvalue λ then
it is the eigenvector of the operator Sn with the eigenvalue λn. Thus, all the eigenvalues of
Sn are real (because λ are real) so that Sn is also symmetric. Note that this proof goes
through even if S were not symmetric and admitted complex eigenvalues. Therefore, the
result that S and Sn have common set of eigenvectors and eigenvalues of Sn are given by
the nth power of the eigenvalues of S is valid for a general linear operator.
Exercise
If a,b,c are mutually orthogonal and S is a symmetric operator show that the
three vectors a × S(a), b × S(b) and c × S(c) are coplanar.
Hint
If a,b,c were the principal vectors of S then each of the products a×S(a) etc, vanish.
Therefore, let {uk} denote the orthogonal principal vectors of S. We have to show
(a × S(a)) · [(b × S(b)) × (c × S(c))] = 0.
This can be done by using the spectral representation of S and the expansion of a,b,c in
the eigenbasis {uk} of S.
□

6
Rotations and Reﬂections
6.1
Orthogonal Transformations: Rotations and Reﬂections
In this section, we try and understand an extremely important class of linear operators
called orthogonal operators. These operators are frequently called transformations
because they correspond to rotations or reﬂections of points in space, which can be
physical operations of rotating (about some axis) or reﬂecting (with respect to a point or a
plane) a point particle or a system of particles. An orthogonal operator preserves the
length of a vector in E3 as well as the orthogonality of vectors, say those forming an
orthogonal basis.
An operator f is said to be orthogonal if it satisﬁes
f (x) · f (y) = x · y
(6.1)
for all vectors x,y ∈E3.
A ﬁxed point of an operator is the vector satisfying f (x) = x, that is, the vector left
invariant under the action of f . We know that the origin (the zero vector) is a ﬁxed point of
f by virtue of its linearity. However, for an orthogonal operator acting on E3, no non-zero
vector x∗, 0 can be a ﬁxed point, because for such a vector x∗the orthogonality condition
x∗· f (y) = x∗· y
(6.2)
is satisﬁed only by the vectors y whose projection along x∗equals that of f (y) along x∗
and not by all vectors in E3. All vectors y and f (y) satisfying Eq. (6.2) for any given x,
correspond to points which lie on a plane normal to x, so that an orthogonal operator
restricted to act on a plane will leave invariant all vectors on a line normal to this plane
which is a 2-D subspace of E3 and we call it E2. To see this in another way, consider an
orthogonal operator f on a plane. It has one ﬁxed point on the plane namely the origin
on the plane. As a subspace of E3, this plane can be translated parallel to itself so that the
origin traces a line normal to the plane, all points on which are invariant under the action

Rotations and Reﬂections
159
of this f . Thus, an orthogonal operator acting on E3 leaves only one vector, namely the
origin or the zero vector, invariant, while an orthogonal operator acting on a plane leaves
invariant all points on a line normal to this plane. If we club this observation with the fact
that an orthogonal operator preserves the length of a vector as well as the angle between
vectors we see that an orthogonal operator corresponds to rotation (either about a point or
an axis) or reﬂection in the origin or in a plane as we shall see below.
We now prove different properties of an orthogonal operator.
Equation (6.1) can be rewritten
f (x) · f (y) = x · f †f (y) = x · y
which implies (in order to be consistent with Eq. (6.1)) that
f † = f −1.
(6.3)
Thus, an orthogonal operator is a non-singular operator for which the inverse equals its
adjoint. The same property holds for the matrix representing an orthogonal operator. To
see this, consider the jkth element of the matrix for the operator f †f ,
(f †f )jk = ˆσj · f †f ˆσk = f ˆσj · f ˆσk = ˆσj · ˆσk = δjk,
(6.4)
which means
[f †f ] = [f †][f ] = I
(6.5)
where I is the 3 × 3 unit matrix, giving
[f †] = [f ]−1.
(6.6)
Written explicitly in elemental form, Eq. (6.5) becomes
X
i
f †
jifik = δjk,
or,
X
i
fijfik = δjk,
(1 ≤j ≤k ≤3),
(6.7)
where we have used [f †] = [f ]T (Eq. (4.19)). The matrix satisfying Eq. (6.7) consists of
columns which are mutually orthogonal and individually normalized. Such a matrix is
called orthogonal. Thus, an orthogonal operator is represented by an orthogonal matrix.
Exercise
Show that the inverse of an orthogonal operator (matrix) is an orthogonal
operator (matrix).
□

160
An Introduction to Vectors, Vector Operators and Vector Analysis
Replacing y by x in Eq. (6.1) it follows that
(f (x))2 = (x)2 = |x|2.
(6.8)
Thus, the magnitude of every vector in E3 is invariant under an orthogonal transformation.
This immediately tells us that an orthogonal operator preserves Euclidean distance between
every pair of vectors in E3, because Euclidean distance between x,y is simply the length of
the vector x −y.
We get immediately from Eq. (6.1) that
|f (x)||f (y)|cosθ2 = |x||y|cosθ1
which, when coupled with Eq. (6.8) implies,
cosθ2 = cosθ1, or, θ2 = θ1
where 0 ≤θ1,θ2 < 2π are the angles between x and y and f (x),f (y) respectively. Thus,
an orthogonal transformation preserves angle between vectors. For fk = f ˆσk, Eq. (6.1)
implies
fj · fk = ˆσj · ˆσk = δjk.
Thus, an orthogonal f preserves the orthogonality of vectors in a standard basis.
Exercise
Show that, if an operator f preserves length of all vectors, then it preserves
angle between every pair of vectors. That is, a length preserving operator is orthogonal.
Solution
We are given f x · f x = x · x for all x ∈E3. We have to show that f x · f y = x · y
for all x,y ∈E3. This follows from
x · y = 1
4(x + y) · (x + y) −1
4(x −y) · (x −y).
□
Thus, the action of an orthogonal f on a right handed orthonormal basis ˆσ1, ˆσ2, ˆσ3 results
in an orthonormal basis given by f ( ˆσ1),f ( ˆσ2),f ( ˆσ3), either right handed or left handed.
We can therefore write
( ˆσ1 · [ ˆσ2 × ˆσ3])2 = (f ( ˆσ1) · [f ( ˆσ2) × f ( ˆσ3)])2 = 1,
or,
det f = f ( ˆσ1) · [f ( ˆσ2) × f ( ˆσ3)] = ±1.
(6.9)
Condition Eq. (6.9) tells apart two kinds of orthogonal transformations. An orthogonal
transformation is said to be proper if det f = +1 and improper if det f = −1. The proper
orthogonal transformations preserve the handedness of a orthonormal basis triad, while
the improper orthogonal transformations change the handedness of an orthonormal basis

Rotations and Reﬂections
161
triad. The handedness of a basis triad is changed if all the basis vectors are reﬂected in the
origin. If we replace the basis in a given linear combination for a vector x by the basis
reﬂected in the origin, the resulting linear combination gives the vector −x obtained by
reﬂecting x in the origin. If we reﬂect one of the basis vectors in the plane normal to it, the
handedness of the basis is changed and a general vector x gets reﬂected in that plane.
Thus, we see that the improper orthogonal transformation corresponds to reﬂection either
in the origin or in a plane. In fact, inversion of a vector x in the origin is the product of
reﬂections of x in the orthogonal planes as we shall see below. Since a transformation
leaving only the origin invariant has to be either a reﬂection or a rotation, a proper
orthogonal transformation must correspond to a rotation. The fact that it preserves the
handedness of the orthonormal basis is consistent with this conclusion.
6.1.1
The canonical form of the orthogonal operator for reﬂection
Given a unit vector ˆn, we obtain an orthogonal operator U which reﬂects a vector x in the
plane normal to ˆn. We show that for a particle rebounding elastically from a ﬁxed plane
(with normal ˆn), the ﬁnal momentum p′ is related to the initial momentum p by
p′ = U (p).
(6.10)
The required operator is
U (x) = (ˆn × x) × ˆn −(x · ˆn)ˆn.
(6.11)
Note that the parenthesis in the cross product term is necessary, because cross product is
not associative. Comparing with Fig. 6.1, we see that the ﬁrst term is the projection of x in
the plane normal to ˆn (say x⊥) and the second term is the projection of x along ˆn (say x∥).
Thus we have,
U (x) = x⊥−x∥,
which is simply the vector we get by reﬂecting x in the plane normal to ˆn.
Fig. 6.1
Reﬂection of a vector in a plane
To show that U (x) is an orthogonal operator, we must test whether
U (x) · U (y) = x · y.

162
An Introduction to Vectors, Vector Operators and Vector Analysis
This can simply be done by evaluating the LHS using the deﬁnition of U (x) in Eq. (6.11).
(Hint: use identity II.)
To show that det U = −1, we take a right handed orthonormal triad { ˆσ1, ˆσ2, ˆσ3} with
ˆσ1 = ˆn. Then, it is trivial to see that
U ˆσ1 = −ˆσ1, U ˆσ2 = ˆσ2, U ˆσ3 = ˆσ3.
Therefore,
det U
= U ( ˆσ1) · (U ( ˆσ2) × U ( ˆσ3))
= −ˆσ1 · ( ˆσ2 × ˆσ3)
= −1 × (+1) = −1
(6.12)
To endorse Eq. (6.10), we make following observations. Equation (6.10), in conjunction
with Eq. (6.11), implies |p′|2 = |p|2, (Hint: use identity I) which is consistent with kinetic
energy conservation valid for an elastic scattering event. Dotting Eq. (6.10) with ˆn and
using Eq. (6.11) we get,
ˆn · p′ = −ˆn · p = (−ˆn) · p.
(6.13)
From Fig. 6.2 we see that Eq. (6.13) means θ = θ′ or the angle of reﬂection equals the angle
of incidence. Crossing Eq. (6.10) with ˆn we get
ˆn × p′ = ˆn × p,
Fig. 6.2
Reﬂection of a particle with momentum p by an unmovable plane
which simply means that p, ˆn and p′ lie in the same plane, determined by ˆn and p. Thus,
Eq. (6.10) is the full description of reﬂection, or the complete statement of the law of
reﬂection.

Rotations and Reﬂections
163
To ﬁnd the inverse of U we note that y = U (x) implies x = U (y), that is, x and
y are mutual images under reﬂection. (This establishes the operator equation U 2 = I.)
Therefore, we have
U −1 = U .
(6.14)
As a corollary, we can show that the reﬂection U is a symmetric operator. Orthogonality
of U coupled with Eq. (6.14) gives
U † = U −1 = U
(6.15)
Exercise
Prove that the product of three elementary reﬂections in orthogonal planes is
an inversion, the linear transformation that reverses the direction of every vector.
Solution
We denote by Uˆn(x) the operator for reﬂection of x in the plane normal to ˆn.
Let { ˆσ1, ˆσ2, ˆσ3} be an orthonormal triad of vectors. Note that
U ˆσj( ˆσk) = −δjk ˆσk + (1 −δjk) ˆσk
(6.16)
Now let
x = ˆσ1x1 + ˆσ2x2 + ˆσ3x3
be a vector in E3. We have, by virtue of Eq. (6.16),
U ˆσ3U ˆσ2U ˆσ1(x) = −ˆσ1x1 −ˆσ2x2 −ˆσ3x3 = −x
(6.17)
which is what we wanted to prove.
□
If we successively reﬂect a vector in two different planes in different order, we get, in
general, two different vectors. This fact is expressed by saying that reﬂections, in general,
do not commute. Thus, if U1, U2 denote the reﬂection operators for two planes then
U1U2(x) , U2U1(x).
Exercise
Show that the reﬂections deﬁned via Eq. (6.16) commute.
□
Now consider two reﬂections which commute, that is, the corresponding reﬂection
operators satisfy
U1U2(x) = U2U1(x)
for all x ∈E3. Setting U = U1U2 we get
U † = (U1U2)† = U †
2 U †
1 = U2U1 = U1U2 = U

164
An Introduction to Vectors, Vector Operators and Vector Analysis
where we have used that U1 and U2 are symmetric and that they commute. Thus, if two
reﬂections commute, their product is a symmetric operator. Physically, this means that the
effect of two successive reﬂections can be obtained via a single reﬂection.
6.1.2
Hamilton’s theorem
Hamilton’s theorem states that every rotation can be expressed as the product of two
elementary (in a single plane) reﬂections.
To prove this theorem we refer to Fig. 6.3. Let a vector x be rotated about the direction
implied by a unit vector ˆn through an angle θ to reach vector x′. Without losing generality
we assume θ < π. Let x∥and x′
∥be the projections of x and x′ on the plane normal to ˆn
and ˆu and ˆv be unit vectors along x∥and x′
∥respectively. Rotation of x to x′ is equivalent
to that of x∥to x′
∥. To show that this rotation is equivalent to two elementary reﬂections,
we ﬁrst reﬂect x∥in the plane normal to ˆu + ˆv. By construction the angle between x∥and
this plane is (π/2 −θ/2) so the reﬂected vector is along −ˆv. Now we reﬂect this vector in
the plane normal to ˆv to get x′
∥. We thus have
R(θ) = UˆvUˆu+ˆv
which proves the theorem. Here, R(θ) is the orthogonal operator for rotation about ˆn
through angle θ.
Fig. 6.3
See text
Hamilton’s theorem expresses the operation of rotation in terms of that of reﬂection. It
is trivial to see that, given any reﬂection (in a plane or in the origin) of a vector x to
produce a vector x′ it is always possible to choose a rotation (that is, the axis and the angle
of rotation) which rotates x to x′. This establishes the equivalence of rotations and
reﬂections. However, it is physically easier to implement rotations than reﬂections. Many a
time, rotation and reﬂection operations are implemented in natural systems like two
molecules which are reﬂections of each other in a plane. Most important are the structures
of physical systems that are invariant under certain rotations and reﬂections. These
operations are called the symmetry operations of the system and play a crucial role in the
dynamical and physical properties of the system. We shall say something about the
symmetries in section 6.4.

Rotations and Reﬂections
165
6.2
Canonical Form for Linear Operators
Of all the linear operators, the symmetric, positive and orthogonal operators are the most
important in modeling the physical world. The symmetric operators are diagonalizable,
have real eigenvalues and their eigenvectors can form an orthonormal basis of E3. Since
the magnitudes of all vector quantities are real, only real eigenvalues can correspond to the
values of any measurable physical quantity. Further, the action of a symmetric operator on
an arbitrary vector can be obtained via its action on its eigenvectors. This is possible only
because a symmetric operator has real eigenvalues and its eigenvectors form a basis.
A positive symmetric operator has positive eigenvalues having simple geometric
interpretation and are required to express many physical quantities that are positive, e.g.,
the kinetic energy of a rotating rigid body expressed using the moment of inertia operator.
Finally, orthogonal operators are required to incorporate elementary physical operations
like reﬂection and rotation of a system. All this motivates a question whether a given
linear operator can be expressed in terms of these operators. If this is possible, the action
of such an operator on an arbitrary vector can be completely understood and carried out.
In this section, we try and answer this question.
We start by proving that every symmetric transformation can be expressed as
the product of a symmetric orthogonal transformation and a positive symmetric
transformation. We proceed as follows.
Let S be a symmetric operator and let {ek} k = 1,2,3 be its eigenvectors forming an
orthonormal frame. Deﬁne the reﬂections
Uej(ek) = −δjkek + (1 −δjk)ek
as in Eq. (6.16). We know that Uej reverse the direction of ej and that their products are
orthogonal and symmetric. Now consider the spectral representation of S,
S =
X
k
λkPk
and deﬁne a positive symmetric operator
S+ =
X
k
|λk|Pk.
We consider four cases.
(i) All λk ≥0. We write
S = IS+.
(ii) One eigenvalue (say jth) < 0. (λj < 0). We write
S = UejS+.

166
An Introduction to Vectors, Vector Operators and Vector Analysis
(iii) Two eigenvalues (say ith and jth < 0. (λi < 0,λj < 0). We write
S = UeiUejS+.
(iv) All the eigenvalues < 0. We write
S = Ue1Ue2Ue3S+.
There are no other cases and in each of the above case we have shown that the symmetric
operator S can be written as the product of a symmetric orthogonal operator and a positive
symmetric operator.
Next, we obtain a unique rotation R for an arbitrary improper orthogonal operator I
satisfying
I = RU ,
where U is a simple reﬂection in the plane normal to any direction ˆu as expressed by its
canonical form Eq. (6.11).
We use the fact that U 2 = I to write
I = (I U )U
and deﬁne R = I U . The fact that R is a rotation follows from
det R = (det I )(det U )
= (−1)(−1) = 1.
Next, we prove the Polar Decomposition Theorem which states that every non-singular
operator f has a unique decomposition in the form
f = RS = I R,
(6.18)
where R is a rotation and S and I are positive symmetric operators given by
S = (f †f )1/2
I = (f f †)1/2.
(6.19)
A canonical form for f is therefore obtained from that for R and S.
To prove Eq. (6.18) we note that
y · (f †f x) = f (y) · f (x) = x · (f †f y),

Rotations and Reﬂections
167
so that the operator S′ deﬁned by
S′ = f †f
is symmetric. Further,
x · (f †f x) = (f x)2 > 0 if x , 0
which makes S′ positive. Therefore, the square root of S′ = f †f is well deﬁned and unique
S = (f †f )1/2.
Since S is non-singular, we solve Eq. (6.18) for the rotation R,
R = f −1 = f (f †f )−1/2.
(6.20)
We have,
detf † = det f ,
or,
det(f †f ) = (det f )2,
or,
det S−1 = det(f †f )−1/2 = (det f )−1,
or,
det R = (det f )(det S−1) = 1,
which shows that R is a rotation. The other part of Eq. (6.18) namely,
f = I R
is proved similarly.
The eigenvalues and eigenvectors of S decide the basic structural properties of f (see
below for a geometric interpretation), because the other factor is just a rotation. They are
sometimes called principal vectors and principal values of f to distinguish them from
eigenvectors and eigenvalues of f which may, in general, be complex and are not related in
a simple way with the principal values which are always real. Of course, there is no
distinction if f
itself is symmetric. Equation (6.18) clearly tells us that complex
eigenvalues correspond to rotations as we have seen before (see section 5.2).
The polar decomposition, Eq. (6.18), provides a simple geometrical interpretation for
any linear operator f . Consider the action of f on points x of a 3-D body or a geometrical

168
An Introduction to Vectors, Vector Operators and Vector Analysis
ﬁgure. According to Eq. (6.18), the body is ﬁrst stretched and/or reﬂected along the
principal directions of f . Then, the deformed body is rotated about the axis through the
angle both speciﬁed by R.
6.2.1
Examples
(a) We ﬁnd the polar decomposition of the skewsymmetric transformation
f (x) = x × (a × b).
(6.21)
Using the skew symmetry of f (f † = −f ) we can write
f †f = (a × b) × (x × (a × b))
= (a × b)2x −[(a × b) · x](a × b)
(6.22)
where we have used identity I. Note that
y · f †f x = (a × b)2x · y −[(a × b) · x][(a × b) · y]
= x · f †f y
(6.23)
which means that f †f is a symmetric operator. Further, x·f †f x > 0 for x , 0 making
f †f a positive operator. It is easily veriﬁed that the square root operator is given by
S = (f †f )1/2 = |a × b|x −(a × b) · x[
a × b
(6.24)
where [
a × b is the unit vector in the direction of a × b.
We now ﬁnd the rotation R in Eq. (6.18) with f given by Eq. (6.21). We have
already found S. We note that (a × b) · S(x) = 0 so that Sx lies in the plane normal
to a×b. Thus, we need to rotate Sx about [
a × b through π/2, so R = R[
a×b(π/2).
Taking this plane to be the complex plane, and real and imaginary axes along Sx and
f x respectively, this rotation amounts to multiplication by eiπ/2 = i.1
(b) The linear transformation
f x = x + 2α ˆσ1( ˆσ2 · x)
(6.25)
is called a shear. Figure 6.4 shows the effect of f on a unit square in the ˆσ1 ˆσ2 plane.
We ﬁnd the eigenvectors, eigenvalues, principal vectors and principal values of f in
this plane. We also ﬁnd the angle of rotation in the polar decomposition of f .
It is easily seen that the only eigenvector of f in Eq. (6.25) is ˆσ1 satisfying
f ( ˆσ1) = ˆσ1
(6.26)
1Note that f (x) is perpendicular to both, a × b and S(x).

Rotations and Reﬂections
169
Every other vector (linearly independent with ˆσ1) gets transformed to a distinct
vector (having different direction) under f . Thus, f is not diagonalizable.
Fig. 6.4
Shear of a unit square
To get the principal vectors and principal values of f we must ﬁnd the operator f †f .
Note that
f †(y) = y + 2α ˆσ2( ˆσ1 · y)
(6.27)
as can be seen from y · f x = x · f †(y) with f and f † as in Eqs (6.25) and (6.27)
respectively. We operate by f †f on the basis vectors ( ˆσ1, ˆσ2) to get
f †f ˆσ1 =
ˆσ1 + 2α ˆσ2
f †f ˆσ2 = 2α ˆσ1 +

1 + 4α2
ˆσ2.
(6.28)
Therefore, the matrix of f †f in the basis ( ˆσ1, ˆσ2) is
[f †f ] =
" 1
2α
2α
1 + 4α2
#
·
The eigenvalues of this matrix are
λ2
± = (2α2 + 1) ± 2α
p
α2 + 1,
(6.29)
whose square roots are,
λ± =
p
α2 + 1 ± α.
(6.30)
These are the required eigenvalues of the operator
S = (f †f )1/2.
Exercise
Employ Mohr’s algorithm to ﬁnd the eigenvectors of f †f , using Eq. (6.28)
with ˆσ2 = i ˆσ1.

170
An Introduction to Vectors, Vector Operators and Vector Analysis
Answer
u± = ˆσ1 ± λ± ˆσ2.
These are also the eigenvectors of the operator S.
□
To get the rotation in the polar decomposition of f we treat ˆσ1 ˆσ2 plane to be the
complex plane. Note that the vectors ˆσ1, ˆσ2 are represented by the numbers 1 and i
on the complex plane. Thus, the vectors u± are represented by the numbers 1 ± iλ±
on the complex plane and the vectors Su+ = λ+u+ and f (u+) = u+ + 2α ˆσ1
( ˆσ2 ·u+) are represented by the complex numbers λ+(1+iλ+) and (1+2αλ+)+
iλ+ respectively. We know that the operator R rotates the vector Su+ to the vector
f (u+). This gives the required rotation by (check it!)
tanθ =
−2αλ2
+
1 + 2αλ+ + λ2
+
.
6.3
Rotations
We need a canonical form of an operator which gives the vector x′ obtained as a result of
rotating a vector x about the direction implied by a unit vector ˆn. Proceeding on the lines
similar to reﬂection (subsection 6.1.1), we arrive at the following canonical form for the
rotation operator
R(x) = (x · ˆn)ˆn + eiθ(ˆn × x) × ˆn.
(6.31)
Fig. 6.5
Rotation of a vector
This operator can be understood by analyzing Fig. 6.5. First, we resolve x in its
components x∥and x⊥lying in the plane normal to ˆn and along ˆn respectively. The ﬁrst
term in the expression for R(x) is x⊥which remains invariant under rotation while the
second term corresponds to the vector obtained by rotating x∥counterclockwise through
angle θ. (x′
∥in Fig. 6.5). Here, we treat the plane normal to ˆn to be the complex plane and
multiplication by eiθ rotates a vector counterclockwise by an angle θ. Since we have

Rotations and Reﬂections
171
introduced a complex coefﬁcient in the expression for the operator, the rule for the
invariance of the scalar product has to be replaced by
f ∗x · f y = x · y.
(6.32)
where f ∗is obtained from f by complex conjugation. The operator f satisfying Eq. (6.32)
is called unitary.
Fig. 6.6
Inﬁnitesimal rotation δθ of x about ˆn
Exercise
Show that the operator R(x) in Eq. (6.31) satisﬁes Eq. (6.32). (Hint: Use
identity II).
□
Thus, the rotation operator in Eq. (6.31) preserves scalar products as it should. That the
determinant of R(x) in Eq. (6.31) is +1 can be proved along the same lines as we did for
the reﬂection operator. This establishes the operator R(x) in Eq. (6.31) as the rotation
operator. However, if we wish to carry on with the operator in Eq. (6.31) to get the
structure and properties of rotation, we need a general algebraic setting incorporating the
multiplication of a vector by a complex number as an integral part of it. Such an algebra is
the geometric algebra which can be used to model rotations in a general and elegant
manner [10, 7, 11]. Nevertheless we can develop the theory of rotations using only the
algebra of vectors we have learnt. We proceed to do that.
We ﬁrst study inﬁnitesimal rotations and then build up ﬁnite rotations as succession of
inﬁnitesimal rotations. Consider an inﬁnitesimal rotation of a vector x about the direction
implied by a unit vector ˆn, through an angle δθ (see Fig. 6.6). The tip of vector x then moves
over an inﬁnitesimal arc length ds of a circle of radius |x|sinφ giving ds = |x|sinφδθ
(Fig. 6.6). Since the circle is a smooth curve, we can choose the arc length ds generated
by the rotation to be so small that the change dx in vector x due to rotation (see Fig. 6.7)

172
An Introduction to Vectors, Vector Operators and Vector Analysis
can replace the arc length ds with a totally negligible (see discussion after Eq. (6.33)) error.
Further, when the sense of rotation is positive or counterclockwise, a right handed screw
advances in the direction of ˆn and the sense in which the rotating vector x traces the arc ds
corresponds to the direction of the vector ˆn × x. Thus, we can take
|x|sinφδθ = |ˆn × x|δθ = ds = |dx|
and
dx = δθ ˆn × x.
for every possible inﬁnitesimal rotation. In fact this equation quantitatively deﬁnes an
inﬁnitesimal rotation and the resulting inﬁnitesimal arc length ds. The quantity dx is
called the differential of x(θ) which is a vector valued function of θ. In the limit as
δθ 7→0, dx/δθ = ˆn × x becomes a vector tangent to the circle of rotation. Thus,
corresponding to an inﬁnitesimal rotation the differential dx has magnitude |dx| = ds
and direction perpendicular to the plane deﬁned by x and ˆn and tangent to the circle of
rotation as shown in Figs 6.6, 6.7. This differential has to be added to x to get the rotated
vector x′ (see sections 9.1 and 9.2). Therefore,
x′ = x + dx = x + δθ(ˆn × x).
(6.33)
Fig. 6.7
Vectors dx and arc length ds as radius |x|sinθ is rotated through angle
δθ. As δθ 7→0 dx becomes tangent to the circle.
As we shall see later, (see section 9.6), the ﬁrst equality in Eq. (6.33) becomes exact for
any angle of rotation θ if we replace its RHS by the Taylor series of the function x(θ)
whose successive terms involve successive powers of θ. Thus, the RHS of the ﬁrst equality
in Eq. (6.33) is obtained by truncating this Taylor series after the term linear in θ which
is justiﬁed if the angle of rotation is small, so that the higher powers θ2,θ3 ··· are orders
of magnitude smaller than θ and hence can be neglected. In such a case, we replace θ by
δθ to emphasize the smallness of the angle of rotation. Thus, the ﬁrst equality in Eq. (6.33)
essentially corresponds to an inﬁnitesimal rotation.

Rotations and Reﬂections
173
Let the vector x be rotated by an inﬁnitesimal angle δθ1 about the direction given by a
unit vector ˆn1 to get a vector x′. Next rotate x′ through angle δθ2 about the direction given
by unit vector ˆn2 to get the vector x′′. Using Eq. (6.33) and keeping the terms linear in δθ1
and δθ2 we get (do this algebra),
x′′ = x + δθ1(ˆn1 × x) + δθ2(ˆn2 × x).
(6.34)
Now we reverse the order of rotations: Rotate x about ˆn2 by δθ2 to get x′ and rotate x′
about ˆn1 by δθ1 to get x′′. Going through the same algebra as above, keeping terms linear
in δθ1 and δθ2, one can check that x′′ is again given by Eq. (6.34) which proves that
inﬁnitesimal rotations commute.
The fact that ﬁnite rotations do not commute will
become clear below.
Now let a vector x be rotated about a unit vector ˆn through a ﬁnite angle θ to get a
vector x′. The process is depicted in Fig. 6.5. As is shown in Fig. 6.5, we resolve x into two
components, x∥in the plane of rotation and x⊥normal to this plane, i.e., in the direction
of ˆn. Rotation affects only the component x∥while x⊥remains invariant.
We imagine that the rotation of x∥through θ is effected by N successive rotations
about ˆn, each of magnitude θ/N. We assume that N is so large (or θ/N is so small) that
Eq. (6.33) applies to each of these rotations. Denote by x1,x2,...,xN = x′
∥the successively
rotated vectors. We have,
x1 =
θ
N (ˆn × x∥) + x∥.
x2 =
θ
N (ˆn × x1) + x1
=
θ
N ˆn ×
 θ
N

ˆn × x∥+ x∥

+ θ
N ˆn × x∥+ x∥
=
" θ
N
2
ˆn × (ˆn × +
2θ
N

(ˆn × + 1
#
x∥).
Proceeding in this way, after N iterations we get
x′
∥=
"
(1 +
 N
1
! θ
N

(ˆn × +
 N
2
! θ
N
2
ˆn × (ˆn × +···
+
 N
N
! θ
N
N
ˆn × (ˆn × (···
#
x∥)
=

1 + θ
N ˆn×
N
x∥.
(6.35)

174
An Introduction to Vectors, Vector Operators and Vector Analysis
Now let the parameter N −→∞to get
x′
∥= lim
N7→∞

1 + θ
N ˆn×
N
x∥≡eθ ˆn×x∥.
(6.36)
Note that Eqs (6.35) and (6.36) deﬁne operators (1 +
θ
N ˆn×)N and eθ ˆn× respectively, on
E3. The action of eθ ˆn× on any vector x can be obtained by expanding it in powers of θ. We
have,
eθ ˆn× ≡
 
1 + θ ˆn × + θ2
2! ˆn × (ˆn × + θ3
3! ˆn × (ˆn × (ˆn × +···
!
.
(6.37)
Fig. 6.8
Orthonormal triad to study the action of the rotation operator
To understand the effect of eθ ˆn× on this space, we operate by it on a suitable basis. We
choose the basis triad to be the set of three orthogonal unit vectors ( ˆσ1, ˆσ2, ˆσ3) with
( ˆσ1, ˆσ2) lying in the plane containing x′
∥and ˆσ3 = ˆn. (see Fig. 6.8).
From Fig. 6.8 it is clear that
ˆn × ˆσ1 = ˆσ2
;
ˆn × ˆσ2 = −ˆσ1
(6.38)
To see the effect of eθ ˆn× on ˆσ1 we evaluate RHS of Eq. (6.37) acting on ˆσ1 and use
Eq. (6.38) to get
eθ ˆn× ˆσ1 = ˆσ1 + θ ˆσ2 −θ2
2! ˆσ1 −θ3
3! ˆσ2 + θ4
4! ˆσ1 + ···
Collecting the coefﬁcients of ˆσ1 and ˆσ2 we get
eθ ˆn× ˆσ1 = cosθ ˆσ1 + sinθ ˆσ2
(6.39)

Rotations and Reﬂections
175
Similarly,
eθ ˆn× ˆσ2 = −sinθ ˆσ1 + cosθ ˆσ2
(6.40)
To get the result of eθ ˆn×x∥we resolve x∥with respect to the basis ( ˆσ1, ˆσ2, ˆσ3):
x∥= a ˆσ1 + b ˆσ2.
(6.41)
Operating on the RHS of Eq. (6.41) by eθ ˆn× and using Eq. (6.39) and Eq. (6.40) we get
x′
∥= eθ ˆn×x∥= (a ˆσ1 + b ˆσ2)cosθ + (a ˆσ2 −b ˆσ1)sinθ.
By Eqs (6.38) and (6.41) this reduces to
x′
∥= eθ ˆn×x∥= x∥cosθ + (ˆn × x∥)sinθ,
or,
x′
∥= x∥+ (cosθ −1)x∥+ sinθ(ˆn × x∥).
(6.42)
Since ˆn is a unit vector perpendicular to x∥, we have
x∥= −ˆn × (ˆn × x∥).
Substituting this in the above expression for x∥we get
x′
∥= x∥+ (1 −cosθ)ˆn × (ˆn × x∥) + sinθ(ˆn × x∥).
Since x⊥and ˆn are parallel, we can add (ˆn × x⊥) = 0 = ˆn × (ˆn × x⊥) on the RHS and
x⊥(= x′⊥) on both sides of the above equation, ﬁnally giving the desired result,
x′ = x + (1 −cosθ)ˆn × (ˆn × x) + sinθ(ˆn × x).
(6.43)
Equation (6.43) is equivalent to the operator identity, deﬁning the rotation operator R
R(x) ≡eθ ˆn×x ≡[x + (1 −cosθ)ˆn × (ˆn × x) + sinθ(ˆn × x)].
(6.44)
Exercise
Show that the rotation operator R can be equivalently expressed by
R(x) = cosθx + (1 −cosθ)(ˆn · x)ˆn + sinθ(ˆn × x).
(6.45)
Hint
Use identity I.
This expression for the rotation operator was used by Josiah Willard Gibbs sometime in the
ﬁrst decade of 20th century.
□
Exercise
Show that the rotation operator as deﬁned above is orthogonal.
□

176
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Resolving x = x∥+ x⊥with x∥parallel and x⊥perpendicular to ˆn, show that
R(x) = cosθx⊥+ sinθ(ˆn × x⊥) + x∥.
□
6.3.1
Matrices representing rotations
To get the matrix elements of the rotation operator on the RHS of Eqs (6.43), (6.44) we
choose an orthonormal basis ( ˆσ1, ˆσ2, ˆσ3) (not necessarily the same as that in Fig. 6.8)
and transform each of the vectors { ˆσk},k = 1,2,3 by this operator. This will give us a new
set of orthonormal vectors {ˆek},k = 1,2,3. The matrix elements of the operator are the
coordinates of {ˆek} with respect to the basis { ˆσk}. We have,
ˆek =
3
X
j=1
ˆσj( ˆσj · ˆek) =
3
X
j=1
ˆσjejk
(6.46)
with its matrix version
[ˆek]T = [ ˆσj]T [ejk]
(6.47)
where [ ˆσj]T and [ˆek]T are the row (1 × 3) matrices with elements as the basis vectors { ˆσj}
and {ˆek} respectively. Equation (6.46) gives, ejk = ˆσj · ˆek, or,
ejk = ˆσj · [ ˆσk + (1 −cosθ) ˆn × (ˆn × ˆσk) + sinθ ˆn × ˆσk],
which reduces, via identity I to
ejk = δjk + ˆσj · [(ˆn · ˆσk)ˆn −ˆσk](1 −cosθ) + ˆσj · (ˆn × ˆσk)sinθ.
Let θk = ˆn · ˆσk denote the direction cosines of ˆn with respect to the basis { ˆσk}. We can
then write
ejk = δjk + [θjθk −δjk](1 −cosθ) −εjkmθm sinθ,
or,
ejk = δjk cosθ + θjθk(1 −cosθ) −εjkmθm sinθ.
(6.48)
where ˆn = P
m θm ˆσm and εjkm = ˆσj · ( ˆσk × ˆσm) are the Levi-Civita symbols.
Thus, if ˆn = ˆσ3 the matrix of rotation becomes


cosθ
−sinθ
0
sinθ
cosθ
0
0
0
1


.
(6.49)
Note that this matrix relates the rotated vector x′ obtained by rotating the basis vectors
{ ˆσ1,2} in the plane normal to ˆn = ˆσ3 that is, by operating the corresponding rotation

Rotations and Reﬂections
177
operator on x = P
k xk ˆσk, while its transpose relates the coordinates of the same vector
with respect to { ˆσk} and {ˆek} respectively.
Exercise
Show that the components of x given by column vectors [x′
j] and [xj] with
respect to the orthonormal bases {ˆek} and { ˆσk} respectively are related by
[x′
k] = [ejk]T [xj]
where [ejk] is the matrix deﬁned by the rotation about ˆn = ˆσ3.
Solution
Note that x = x′
1ˆe1 + x′
2ˆe2 + x′
3ˆe3 = x′
1R ˆσ1 + x′
2R ˆσ2 + x′
3R ˆσ3. This gives,
using Eq. (6.46),
x =
X
j


X
k
x′
kejk

ˆσj
or,
xj =
X
k
x′
kejk
or,
[x′
k] = [ejk]T [xj]
(6.50)
where we have used [ejk]−1 = [ejk]T since [ejk] is orthogonal.
□
We make a few observations.
It is straightforward to show (Exercise) that the rotation operator R(ˆn,θ) deﬁned in
Eqs (6.43), (6.44) is an orthogonal operator, that is,
R(ˆn,θ)x · R(ˆn,θ)y = x · y
for all x,y ∈E3. This also proves that the matrix [ejk] representing R(ˆn,θ) is orthogonal,
because we have already proved that a matrix representing an orthogonal operator is
orthogonal. If we denote by S the orthogonal matrix of the rotation operator deﬁned by
Eq. (6.48), then the orthogonality condition means,
ST S = I = SST or ST = S−1,
(6.51)
where I is the unit matrix of size 3 × 3.
The determinant of the orthogonal matrix representing a rotation is +1. First, it is
straightforward to show that the determinant of the rotation operator R(ˆn,θ) deﬁned in
Eqs (6.43), (6.44) is +1, that is,
R(ˆn,θ) ˆσ1 · (R(ˆn,θ) ˆσ2 × R(ˆn,θ) ˆσ3) = 1

178
An Introduction to Vectors, Vector Operators and Vector Analysis
by choosing ˆn = ˆσ1 as we did for the reﬂection operator. This means that the matrix [ekj] in
Eq. (6.48) representing the rotation operator has determinant +1, because, we have proved
in section 4.4 that the determinant of the matrix representing a linear operator is identical
with the determinant of the operator. There is a one to one correspondence between the
set of rotations and the set of 3 × 3 orthogonal matrices with determinant +1. To see this,
note that the equality of matrices [R1] = [R2] representing rotations R1 and R2 implies
equality of rotations R1 = R2 because the equality of matrices would mean, via Eq. (6.46),
that the action of R1 and R2 on an orthonormal basis is identical and by linearity of the
operators this implies R1x = R2x for all x ∈E3. This establishes the required one to
one correspondence. In section 4.5 we have already seen that the matrix representing a
product of operators is the product of the matrices representing the individual operators.
This means, coupled with their one to one correspondence, that the set of 3×3 orthogonal
matrices with determinant +1 is isomorphic with the set of rotations.
Note that the operator in Eqs (6.43), (6.44), which gives the counterclockwise rotation
of the vector x by an angle θ also gives the clockwise rotation of x through the angle 2π−θ
(see Fig. 6.9), because the operator remains the same if we replace in its expression θ by
−(2π −θ). This is in conformity with whatever we have said while dealing with rotation
as the means of changing direction (section 1.2). Thus, these two rotations give rise to the
same matrix representative apparently destroying the one to one correspondence between
the rotations and the set of 3 × 3 orthogonal matrices with determinant +1. However,
without losing generality we can stick only to the counterclockwise rotations alone, which
establishes the required one to one correspondence.
Fig. 6.9
Equivalent rotations: One counterclockwise and the other clockwise

Rotations and Reﬂections
179
Exercise
The sum of the diagonal matrix elements fkk of a linear transformation f is
called the trace of f and denoted T r f . Show that the trace of rotation R(ˆn,θ) is given by
T r R =
X
k
ˆσk · (R ˆσk) = 1 + 2 cosθ
(6.52)
Hint
This result follows trivially by explicitly summing the diagonal matrix elements of
R(ˆn,θ) remembering that P
j θ2
j = 1.
□
Note that the trace is independent of the basis used to set up the matrix of R. In fact this
result is quite general.
Exercise
Show that the trace of a linear operator f is independent of the basis used to
compute it.
□
We deﬁne the composition of two rotations (in the same way as the composition of two
operators) as their successive application to a vector and denote it by a ◦separating two
rotations. We have already seen that the set of rotations on E3 and that of 3×3 orthogonal
matrices with determinant +1 are in one to one correspondence. Taking, the composition
of rotations and the matrix multiplication as the respective binary operations on these
sets, we see that this one to one correspondence is actually an isomorphism. This is
because x′′ = R2x′ = R2 ◦R1x corresponds to the following equation involving the
matrix representatives and the column matrices for the vectors [x′′] = [R2][x′] =
[R2][R1][x]. It is easy to see that the product of two orthogonal matrices with
determinant +1 is an orthogonal matrix with determinant +1 (Exercise). This product
matrix must correspond to a single rotation about some axis through some angle, because
of the one to one correspondence between these two sets. Thus, the matrix representing
the result of the composition of rotations is the product of the matrices representing the
individual rotations. This establishes the required isomorphism. As a byproduct we have
found that the set of rotations is closed under their composition. Also, it is easy to see that
if R(ˆn1,θ1)x = x′ and R(ˆn2,θ2)x′ = x′′ then the single rotation corresponding to
their composition R(ˆn,θ) is the one about the unit vector ˆn normal to the plane
containing the vectors x and x′′ and through the angle given by
cosθ = x · x′′
|x||x′′|·
The fact that two ﬁnite rotations say R(ˆn1,θ1) and R(ˆn2,θ2) do not commute in general,
that is,
R(ˆn2,θ2) ◦R(ˆn1,θ1)x , R(ˆn1,θ1) ◦R(ˆn2,θ2)x
(6.53)
is amply clear from Fig. 6.10. To see this analytically, we make use of the isomorphism
between the set of rotations (with 0 ≤θ < 2π) and the set of 3 × 3 orthogonal matrices
with determinant +1 representing them. Since the multiplication of matrices is not

180
An Introduction to Vectors, Vector Operators and Vector Analysis
commutative, the matrices representing the LHS and the RHS of Eq. (6.53) are, in general,
different, corresponding to different rotations.
Fig. 6.10
Composition of rotations. Rotations do not commute.
Exercise
Show that two rotations about the same axis commute.
Hint
Just visualize it! Note that the matrices for both the rotations have the form given by
the matrix representing a rotation about ˆσz, if we take ˆσz along the axis of rotation. Show
explicitly that these matrices commute.
□
6.4
Active and Passive Transformations: Symmetries
We know that a rotation operator R(ˆn,θ) connects a vector x with the vector x′ obtained
by counterclockwise rotating x by angle θ about an axis implied by a unit vector ˆn. The
transformation x 7→x′ which involves the actual rotation of a vector x giving a new vector
x′ is called an active transformation. Physically, the active transformation involves the
actual change in the state of the system, like the rotation of an object or the change in the
vector giving a vector quantity due to some external agency like magnetic ﬁeld.
Alternatively, the rotation operator can connect the coordinates of a vector with respect to

Rotations and Reﬂections
181
a coordinate system with the coordinates of the same vector with respect to a new
coordinate system obtained by rotating the initial one about the same unit vector ˆn by the
same angle. This transformation, which does not involve an actual rotation of the vector
(so that there is no change in the state of the physical system), is called a passive
transformation. Whenever the successive application of the active and the passive
transformations amounts to the application of the identity transformation, we say that the
corresponding rotation (about the given axis through the given angle) is a symmetry or a
symmetry element for the physical system.
For example, the ﬁgure at the tip of vector F in Fig. 6.11(a), is actively rotated (about
the axis perpendicular to the xy plane and passing through the origin) with no change of
shape into a new position with position vector F′. The components of the rotated vector
are related to those of the initial vector by (see Eq. (6.49),


F′x
F′y

=


cosθ
−sinθ
sinθ
cosθ


"
Fx
Fy
#
·
In Fig. 6.11(b) the ﬁgure (and the vector F) is not rotated however, the coordinate axes
are, by the same angle and in the same sense. This is the passive transformation and the
coordinates of the vector along the new axes are (see Eq. (6.50)),


Fx′
Fy′

=


cosθ
sinθ
−sinθ
cosθ


"
Fx
Fy
#
·
Note that the transformation matrices are orthogonal and are transpose and hence inverses
of each other. We have already proved this fact generally in Eq. (6.50). Therefore, if both
transformations are successively performed, as in Fig. 6.11(c), we get
F′
x′ = Fx
F′
y′ = Fy
(6.54)
Thus, the numerical values of the new components are the same as those of the old
components. Therefore, a mere knowledge of these values does not indicate whether the
transformation was performed. This indistinguishably is due to a physical property of
plane surfaces: It is possible to rigidly rotate any plane ﬁgure. On the other hand, an
irregular surface does not allow any rigid motion. It still allows the passive coordinate
transformations which amount to mere relabeling of its points. However, there are no
corresponding active transformations, which leave the displaced body unaltered. For
example, suppose you are in a ship on the open sea and mark your position with respect to
some reference ship at a distance. If your ship and the reference ship are both rotated
about the same axis by the same angle, your position relative to the reference ship is
unaltered. This invariance is sometimes expressed by saying that the hallmark of a
symmetry is the impossibility of acquiring some physical knowledge.

182
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 6.11
Active and passive transformations
In the above analysis, we have taken the basis vector ˆσ3 (or the z axis) along the axis of
rotation. This is not necessary. Even if we take an arbitrary orthonormal basis and the
corresponding coordinate system, the matrices for the active and passive transformations
are transposes and inverses of each other, so that applying them in succession is the same as
applying the identity transformation. Thus, whether a given rotation is a symmetry element
does not depend on the orthonormal basis chosen to implement the active and passive
transformations.
Exercise
If a right hand glove is turned inside out, it becomes a left hand glove. This is
an example of an active transformation (assume that inside and outside textures and colors
are identical). What is the corresponding passive transformation?
□
Consistent with our Newtonian view of space as a continuum of points making up an inert
vacuum is the assumption that the whole space is like an ideal rigid body, that is, the

Rotations and Reﬂections
183
distance between every pair of points in space remains constant despite all events taking
place in it. Thus, when a single vector is rotated about any axis by any angle, the whole
space is rotated along with the vector. The subsequent passive transformation relabels all
the points in space to reproduce the initial situation. Therefore, for a single vector in space
(which could be the position vector of a single particle in space), every possible rotation or
reﬂection (which is equivalent to two rotations by Hamilton’s theorem), is a symmetry
transformation. If we consider a system of non-interacting particles we can apply the
symmetry transformations to each particle separately, independent of other particles, so
that the same conclusion applies to such a system. Thus, the system of non-interacting
particles such as an ideal gas possesses highest symmetry. In contrast, the symmetry
elements of a ﬁgure like an equilateral triangle or a square, or a cube or a tetrahedron act
only on the points making up the ﬁgure and not on the rest of space. However, after the
succession of the active and the passive transformations, the whole space, including the
ﬁgure, must reproduce the initial situation. This is possible, only when the active
transformation reproduces the initial conﬁguration of the ﬁgure. Only a ﬁnite set of
rotations and reﬂections meets this requirement. Thus, the symmetry elements of a solid
which leave its unit cell invariant, form a ﬁnite set. Thus, when a gas or a liquid condenses
to make a solid, the symmetry of the system is drastically reduced. This phenomenon is
called ‘symmetry breaking’. Generally, such a transition from liquid to solid phase, called a
phase transition, occurs at a particular temperature at which the symmetry breaks
spontaneously. Spontaneous symmetry breaking is responsible for the fact that the
quantities like volume, magnetization, mole numbers of chemical species etc are
macroscopically observable, that is, these variables are time independent on the atomic
scale of time and spatially homogeneous on the atomic scale of distance. On the other
hand, symmetries themselves are of far reaching signiﬁcance as they give rise to all the
conserved quantities like energy, angular momentum, linear momentum etc, which make
the understanding of the dynamics of the system possible. For example, the dynamics of a
particle driven by a central force is completely known because of the conservation of
energy and angular momentum of such a particle. Further, Kepler’s laws of planetary
motion can be easily obtained using an additional conserved quantity, namely the
Runge–Lenz vector. The underlying symmetries and symmetry breaking is crucial for the
understanding of our physical world.
Exercise
Find all the symmetry elements of a equilateral triangle and a square (see
Figs 7.1 and 7.2).
□
We now show that if the rotations R1(ˆn1,θ1) and R2(ˆn2,θ2) are symmetry elements for
a system, then so is their composition. Let R1 rotate a vector F to F′ and R2 rotate a vector
F′ to F′′. The composite rotation R12(ˆn,θ) = R2(ˆn2,θ2) ◦R1(ˆn1,θ1) must rotate F
to F′′. The matrix for the corresponding active transformation is the product [R2][R1]
and that for the passive transformation is the inverse of this product. Thus, applying the
composite active and passive transformation gives us the identity transformation. This
proves the result.

184
An Introduction to Vectors, Vector Operators and Vector Analysis
6.5
Euler Angles
We know that a rotation is completely speciﬁed by a unit vector ˆn giving the axis of
rotation and the angle of rotation χ. Thus, the set of all possible rotations can be spanned
by varying the direction (ˆn) in space and the angle of rotation χ over the range
0 ≤χ < 2π. In other words,the rotation operator is parameterized by the unit vector ˆn
and the angle of rotation. We have seen before (see subsection 1.7.1) that two independent
parameters are required to specify a direction, namely the polar and the azimuthal angles.
Thus, a rotation is parametrized by three independent angles, namely, the angle of
rotation χ and the polar and azimuthal angles (θ,φ) which specify the direction about
which the (counterclockwise) rotation takes place. A very useful way to specify a rotation
is by specifying the orthonormal basis {ˆek} k = 1,2,3 obtained by rotating the standard
basis { ˆσk} k = 1,2,3 about a given direction ˆn by the given angle χ. Thus, given the bases
{ˆek} k = 1,2,3 and { ˆσk} k = 1,2,3 corresponding to the given rotation, we need to ﬁnd
three independent rotations through three angles say φ,θ and ψ such that, when
performed successively, will rotate { ˆσk} k = 1,2,3 to {ˆek} k = 1,2,3. The angles φ,θ,ψ
are called Euler angles. The required three rotations can simply be read out from Fig. 6.12.
First, we rotate the ˆσ1 about ˆσ3 axis by an angle φ so as to make it perpendicular to the
plane deﬁned by ˆσ3 and ˆe3. The corresponding line is called the line of nodes. We denote
the corresponding rotated vector by ˆeN. Next, we rotate ˆσ3 about the line of nodes by
angle θ to make it coincide with ˆe3. Finally, we rotate the line of nodes about ˆe3 by an
angle ψ to make it coincide with ˆe1. Thus, the successive Euler rotations rotate the
orthonormal frame { ˆσk} k = 1,2,3 to {ˆek} k = 1,2,3 which was obtained by rotating
{ ˆσk} k = 1,2,3 about the direction ˆn by an angle χ.
Fig. 6.12
Euler angles

Rotations and Reﬂections
185
Thus, to every triple of Euler angles φ,θ,ψ the above construction associates a rotation
of 3-D space taking frame { ˆσk} k = 1,2,3 into the frame {ˆek} k = 1,2,3. The ranges of
the Euler angles are
0 < φ < 2π, 0 < ψ < 2π, 0 < θ < π.
Thus, by continuously varying the Euler angles, we can generate all possible rotations.
Thus, the set of all possible rotations about a point is parameterized by three Euler angles
varying in their speciﬁed ranges.
The net rotation is given by the composition of Euler rotations in the order stated above.
We have,
R(ˆn,χ) = eψˆe3×eθˆeN ×eφ ˆσ3×
(6.55)
Exercise
Show that the line of nodes has the direction
ˆeN = eθˆeN × ˆσ1 = ˆσ3 × ˆe3
| ˆσ3 × ˆe3|.
□
Let us now set up the matrix representing R(ˆn,χ) in terms of its Euler angles. To do this,
we have to expand the vectors {ˆek} k = 1,2,3 in terms of the basis { ˆσk} k = 1,2,3. To get
ˆe3 we have to ﬁrst evaluate eφ ˆσ3× ˆσ1 and then evaluate eφˆn× ˆσ3 where ˆn = eφ ˆσ3× ˆσ1 using
Eq. (6.43) or Eq. (6.45). Carrying out this calculation we get,
ˆe3 = sinθ sinφ ˆσ1 −sinθ cosφ ˆσ2 + cosθ ˆσ3.
Evaluating ˆe1 and ˆe2 in the same way, we get, for the matrix representing R(ˆn,χ) in terms
of its Euler angles,


cosψ cosφ −sinψ sinφcosθ
−sinψ cosφ −cosψ sinφcosθ
sinθ sinφ
cosψ sinφ + sinψ cosφcosθ
−sinψ sinφ + cosψ cosφcosθ
−sinθ cosφ
sinθ sinψ
sinθ cosψ
cosθ


·
If we multiply the row vector [ ˆσ1 ˆσ2 ˆσ3] on the right by this matrix, then we get the row
vector [ˆe1ˆe2ˆe3]. The Euler rotations corresponding to arbitrary rotation, deﬁned above,
are marred by the fact that their axes of rotation are not ﬁxed directions in space. We can
deﬁne Euler rotations using a construction by which every rotation R(ˆn,χ) is reduced to
a composition of rotations about ﬁxed axes of a standard basis. In this construction, an
arbitrary rotation is decomposed into Euler rotations as
R = R(ˆn,χ) = eφ ˆσ3×eθ ˆσ1×eψ ˆσ3× = RφRθRψsay
(6.56)
Thus, the ﬁrst rotation is about ˆσ3 by an angle ψ the second rotation is about ˆσ1 by an angle
θ and the third one is about ˆσ3 by an angle φ. Note that ˆek = R ˆσk = RφRθRψ ˆσk so
that it is quite easy to calculate the matrix elements of a rotation in terms of Euler angles.

186
An Introduction to Vectors, Vector Operators and Vector Analysis
Consider, for example, the rotation of ˆσ3. Rψ is a rotation about ˆσ3 and hence will leave
ˆσ3 invariant. Next, we have, using Eq. (6.43) or Eq. (6.45),
eθ ˆσ1× ˆσ3 = ˆσ3 cosθ −ˆσ2 sinθ.
Therefore,
ˆe3 = eφ ˆσ3×( ˆσ3 cosθ −ˆσ2 sinθ)
=
ˆσ3 cosθ −eφ ˆσ3× ˆσ2 sinθ
=
ˆσ3 cosθ −( ˆσ2 cosφ −ˆσ1 sinφ)sinθ.
(6.57)
From this, the matrix elements ej3 = ˆσj · ˆe3 can be read off directly. We get exactly the
same matrix representing R as before. Figure 6.13 shows the Euler rotations of a standard
basis one after the other, in the given order.
Fig. 6.13
Rotations corresponding to Euler angles

Rotations and Reﬂections
187
Note that the order of Euler rotations in Eq. (6.55) is opposite to that in Eq. (6.56).
However, both the expressions describe the same rotation R(ˆn,χ). We see that the same
set of Euler angles can be used to give two different parameterizations of the same rotation
with two different sequences of Euler rotations. The ﬁrst parameterization is preferred by
astronomers because ˆσ3 and ˆe3 can be associated with easily measured directions. On the
other hand Eq. (6.56) has the advantage of ﬁxed rotation axes for Euler rotations even when
the Euler angles change with time (R(ˆn,χ) depends on time).
To show the equivalence of Eq. (6.55) with Eq. (6.56) we note that eθˆeN × =
eφ ˆσ3×eθ ˆσ1×e−φ ˆσ3× and eψˆe3× = eφ ˆσ3×eθ ˆσ1×eψ ˆσ3×e−θ ˆσ1×e−φ ˆσ3× Substituting in Eq. (6.55)
and noting that the successive rotations by equal and opposite angles about the same axis
result in identity transformation, we get Eq. (6.56).
Exercise
In addition to Euler rotations engineers use three independent rotations called
roll, pitch and yaw, as shown in Fig. 6.14, to implement arbitrary rotation of the body via
ˆek = (yaw)(pitch)(roll) ˆσk = eφ ˆσ1×eθ ˆσ2×eψ ˆσ3× ˆσk,
Fig. 6.14
Roll, pitch and yaw
where ψ,θ,φ are the angles of rotation corresponding to roll, pitch and yaw respectively.
Show that the transformed basis is given by
ˆe1 = cosψ cosθ ˆσ1 + (cosψ sinθ sinφ + sinψ cosφ) ˆσ2
+ (sinψ sinφ −cosψ sinθ cosφ) ˆσ3

188
An Introduction to Vectors, Vector Operators and Vector Analysis
ˆe2 = −sinψ cosθ ˆσ1 + (cosψ cosφ −sinψ sinθ sinφ) ˆσ2
+ (cosψ sinφ + sinψ sinθ cosφ) ˆσ3
ˆe3 = sinθ ˆσ1 −cosθ sinφ ˆσ2 + cosθ cosφ ˆσ3
(6.58)
Write down the matrix for the corresponding rotation.
□
6.6
Euler’s Theorem
In section 6.3, we analyzed a rotation about a given axis and found the orthogonal matrix
of the corresponding rotation operator with respect to an arbitrary orthonormal basis. We
now do the reverse: Given a 3 × 3 orthogonal real matrix [R] with determinant +1, we
show that the transformation [x] 7→[R][x] ; x ∈E3 can be realized by ﬁrst choosing
a ﬁxed direction in space through the origin and then rotating x through a suitable angle
about this direction as the axis. This is known as Euler’s theorem. Henceforth in this section,
we will use the same symbol for the rotation operator and the matrix representing it and
also for the vectors and the corresponding 3×1 column vectors, because in this section, we
will be exclusively dealing with matrices.
Let λi and vi, (i = 1,2,3) denote the eigenvalues and eigenvectors of R which may
be complex, although R is real. These satisfy the equations
Rvi = λivi (i = 1,2,3).
(6.59)
Since R is an orthogonal matrix, we have (see section 6.1) ∥Rvi∥= ∥vi∥where for any
vector v, ∥v∥denotes the Euclidean length (|vx|2 + |vy|2 + |vz|2)1/2. Therefore, by
Eq. (6.59),
|λi| = 1 (i = 1,2,3).
(6.60)
The λis are the roots of the real cubic equation
det(λI −R) = 0.
(6.61)
The product of the roots is
λ1λ2λ3 = det R = 1.
(6.62)
At least one of the roots is real. To see this note that for large enough |λ|, the cubic term
dominates, so that the sign of the cubic polynomial in Eq. (6.61) is the same as that of λ.
This means that the graph of the cubic polynomial (which is a continuous function) has
to cut the λ axis at least once. If the other two eigenvalues (say λ2,λ3) are complex, then
λ3 = λ∗
2 (superﬁx ∗denotes complex conjugation) and by Eq. (6.60) λ2λ3 = 1, hence by

Rotations and Reﬂections
189
Eq. (6.62) λ1 = 1. If all the three roots are real, they can be (1,1,1) or (1,−1,−1). In any
case there is always one root, say λ1, equal to +1, hence
Rv1 = v1
(6.63)
which shows that the straight line through the origin in the direction of v1, is invariant
under the transformation x 7→Rx. Obviously, this is the axis of rotation.
Let λ1 = 1, λ2 = eiθ, λ3 = e−iθ and let v1,v2,v3 form an orthonormal set. The
eigenvectors of a orthogonal matrix can always be orthonormalized. Call
u1 = v1
u2 =
1
√
2
(v2 + v3)
u3 =
i√
2
(v2 −v3).
(6.64)
uis form a orthonormal set (check it!) and can be taken to be real, because v2 and v3
can be taken to be complex conjugates.2 From Eq. (6.64) and the values of λi i = 1,2,3
we get,
Ru1 = u1
Ru2 = cosθu2 + sinθu3
Ru3 = −sinθu2 + cosθu3.
(6.65)
We see that the transformation R is a rotation in the plane perpendicular to u1.
When the matrix R is given, the corresponding angle and axis of rotation can be
obtained as follows. Since the sum of the eigenvalues of a matrix equals its trace, the angle
θ is given by
1 + eiθ + e−iθ = R11 + R22 + R33,
or,
cosθ = 1
2 (R11 + R22 + R33 −1).
Let the axis of rotation be in the direction of the eigenvector v, that corresponds to the
eigenvalue λ = 1, so that Rv = v. Since R is orthogonal, RT R = I, hence v = RT v.
2Just take the complex conjugate of Rv2 = λ2v2 and compare with Rv3 = λ3v3 noting that λ3 = λ∗
2.

190
An Introduction to Vectors, Vector Operators and Vector Analysis
Therefore, (R −RT )v = 0 which is a homogeneous system of simultaneous linear
equations in the components v1,v2,v3 of v. Thus, the components of v are in the ratio
v1 : v2 : v3 = (R23 −R32) : (R31 −R13) : (R12 −R21).
(6.66)
Exercise
Establish Eq. (6.66).
□
In many cases the matrix of the rotation operator with respect to some basis is what is
known, so we have to carry out the procedure in this section in order to get the speciﬁc
rotation represented by the matrix. Such a speciﬁcation is required in order to get the
kinematical and dynamical description of a rotating physical system.

7
Transformation Groups
7.1
Deﬁnition and Examples
A state of a physical system at a time t is given by specifying the values of different vector
(and scalar) physical quantities pertaining to the system at that time. The values of the
vector physical quantities form a part of E3. Thus, the action of an operator or
transformation on E3 will, in general, change the state of the system. Thus, all possible
changes in the state correspond to a collection of transformations on E3. Such a set of
transformations may form an extremely important algebraic structure called a group. The
evolution of a system in time, due to its interaction with other systems, is controlled by its
Lagrangian (or Hamiltonian). The symmetry element of the system, which leaves its
Lagrangian (or Hamiltonian) invariant, gives rise to a conservation law, that is, it gives rise
to an expression involving the position and momentum vectors of the system, whose value
remains the same at all times, throughout the motion of the system. This result is called
Noether’s theorem. The set of all such symmetry elements form a group. This fact turns
out to be of great advantage in the theoretical development of mechanics, quantum
mechanics and of physics in general. In fact whole of mechanics can be developed from
the group theoretical point of view, as in the book by N. Mukunda and E. C. G. Sudarshan
[22]. Here our intention is to give elementary group theory with an emphasis on the
rotation group and the group of isometries over E3 (also called Euclidean group) with a
view to understand rigid body motion, which is a combination of the rotational and
translational motion.
A group G is any set of elements {a,b,c,...,x,y,z,...}, ﬁnite or inﬁnite, together with
the law of composition, denoted ◦, such that
(i) (Closure)
If a and b are any two elements of G, then a ◦b is an element of G.
(ii) (Associative law)
If a,b,c ∈G then
(a ◦b) ◦c = a ◦(b ◦c)
(7.1)

192
An Introduction to Vectors, Vector Operators and Vector Analysis
(iii) If a,b ∈G then there exist unique elements x,y ∈G such that
a ◦x = b and y ◦a = b
(7.2)
If the elements are numbers, vectors, matrices etc, the composition a◦b may either be
the sum or the product of a and b. In the case of mappings, transformations, rotations,
permutations. etc, the law is understood to be the usual law of composition; if a,b are
transformations, then a ◦b is the transformation which results from performing b
ﬁrst, then a.
Exercise
Show that the set of all rotations in a plane {Rφ : 0 ≤φ < 2π} forms a
group.
Hint
All the rotations are about the same axis, perpendicular to the plane, so that
Rφ1 ◦Rφ2 = Rφ1+φ2.
□
Exercise
Prove the following laws which are the consequences of axioms (i), (ii),
(iii) above.
(iv) (Law of cancellation)
If a,b,c ∈G then
a ◦b = a ◦c implies b = c
b ◦a = c ◦a implies b = c
(7.3)
Hint
Use axiom (iii) and the fact that the elements x and y deﬁned in (iii) are unique.
(v) (Identity)
There is a unique element e ∈G such that
a ◦e = a = e ◦a
for all a ∈G.
Hint
Use (iii) with b replaced by a to get
a ◦e = a e′ ◦a = a.
To show that e = e′ put a = e and use the law of cancellation.
(vi) (Inverse)
For every a ∈G, there exists a unique a−1 ∈G such that
a−1 ◦a = e = a ◦a−1.
(vii) (Extended associative law)
(a ◦(b ◦(c ◦(···)))···) ◦h = a ◦b ◦c ◦··· ◦h
so that unnecessary parentheses can be omitted.
(viii) (Extended inverse)
(a ◦b ◦c ◦··· ◦x ◦y)−1 = y−1 ◦x−1 ◦··· ◦b−1 ◦a−1.
□

Transformation Groups
193
Note that the law of composition need not be commutative, that is, in general, a◦b , b ◦a.
a,b ∈G are said to commute if a ◦b = b ◦a. If all pairs of elements of G commute, then G
is said to be commutative or Abelian.
Let a ∈G and m ≥0 be an integer. Then, am is deﬁned as follows.
a0 = e, a1 = a, a2 = a ◦a, a3 = a2 ◦a, ..., am = am−1 ◦a; a−m = (a−1)m
If all the elements an (n = 0,±1,±2,···) are distinct, then the element a is said to be of
inﬁnite order, otherwise, there is a smallest positive integer l, called the order of a, such
that al = e. Then, am = e provided l is a divisor of m and every power of a equals one
of the elements e,a,a2,...,al−1. The group comprising e,a,a2,...,al−1 is called the cyclic
group of l elements.
If a subset G′ ⊆G of a group G is a group with the same law of composition as G, it is
called a subgroup of G. For example, the rotations about a ﬁxed axis form a subgroup of
the group of rotations on E3. The distinct powers of an element a form a subgroup called a
subgroup generated by the element a. This could be the cyclic subgroup of ﬁnite or inﬁnite
order. The order of a group is the number of elements in it which can be ﬁnite or inﬁnite.
If G′ is a subgroup of G we write G′ < G. In any case, G < G and {e} < G. If G′ , G, G′ is a
proper subgroup, If G′ = {e}, G′ is a trivial subgroup.
Examples
(i) The vector space E3 is an additive Abelian group containing inﬁnite elements. This is
obvious from the properties of vector addition listed in section 1.4.
(ii) Let G denote the following set of 2 × 2 real matrices,
e =
 1
0
0
1
!
a =
 0
−1
1
0
!
b =
 −1
0
0
−1
!
c =
 0
1
−1
0
!
·
It is straightforward to check that this set forms a group under matrix multiplication.
For example, a ◦c = e and a−1 = c.
(iii) Let C4 denote the group of the rotational symmetries of a square, under the
composition of rotations, namely,
e = identity (rotation through 0)
a = counterclockwise rotation through π/2
b = counterclockwise rotation through π
a = counterclockwise rotation through 3π/2 (clockwise rotation through π/2)
Exercise
Show that the groups in examples (ii) and (iii) are simply two different
realizations of ‘cyclic group of four elements’.
□
(iv) The sets Z2 of integers modulo 2 and ((1,−1),·) are groups under the respective
binary operations and are isomorphic. We name them Z2 and C2 respectively. Both
are cyclic groups of two elements {e,a} with a2 = e. The three element group C3 is

194
An Introduction to Vectors, Vector Operators and Vector Analysis
given by {1,ω,ω2} where ω = e2πi/3. This is isomorphic with the group of three
rotations of angles 0,2π/3,4π/3 in the plane, which account for all the rotations
forming the symmetry elements of an equilateral triangle centered at the origin.
(v) We can consider the group of all symmetries of the equilateral triangle (see Fig. 7.1).
Thus, we allow reﬂections about the perpendicular bisectors as well. This is a six
element group and we denote it by S3. Labeling the vertices {1,2,3} we can link every
element in S3 with some permutation of the vertices of the triangle. Let (12) denote
the permutation which interchanges vertices 1 and 2 while leaving the vertex 3 ﬁxed.
This permutation is obtained by the reﬂection in the perpendicular bisector of the
edge joining 1 to 2. Similarly, the permutation (123), sending vertex 1 into 2, 2 into
3 and 3 into 1 is obtained by rotating the triangle through 120◦. The permutation
(132) sending vertex 1 into 3, 3 into 2 and 2 into 1 is obtained by rotating the triangle
through 240◦. Thus, we see that the group of symmetries of an equilateral triangle is
the same as the group of all permutations on three symbols.
Fig. 7.1
(a) Symmetry elements of an equilateral triangle i) Reﬂections in three
planes shown by ⊥bisectors of sides. ii) Rotations through 2π/3,4π/3
and 2π (= identity) about the axis ⊥to the plane of the triangle
passing through the center. (b) Isomorphism with S3 (see text).

Transformation Groups
195
Exercise
Show that the group of permutations of 4 symbols, S4, has 24 elements.
Generalize to n symbols to show that Sn has n! elements.
□
(vi) We now consider the group of all symmetries of a square denoted D4. This is an
eight element group, four rotations and four reﬂections, reﬂections in two diagonals
and the reﬂections in two perpendicular bisectors (see Fig. 7.2). Each element of D4
permutes the vertices 1,2,3,4 of the square. Thus, we may regard D4 as the
subgroup of S4, which has 4! = 24 elements. Similarly, the group of symmetry
elements of a regular polygon of n sides, called Dn, is the subgroup of Sn, the group
of all permutations of n symbols.
Exercise
Show that the group Dn contains 2n elements.
□
Fig. 7.2
(a) Symmetry elements of a square (group D4) i) Reﬂections in planes
through the diagonal and bisectors of the opposite sides. ii) Rotations about
the axis through the center and ⊥to the square by angles π/2,pi,3π/2
and 2π (= identity). (b) D4 is isomorphic with a subgroup of S4 (see text).
(vii) We now deal with groups with inﬁnite number of elements. Let SL(2,C) denote the
set of 2 × 2 matrices with complex entries, whose determinant equals 1. Thus, an
element of SL(2,C) is given by
A =
 a
b
c
d
!
,

196
An Introduction to Vectors, Vector Operators and Vector Analysis
where a,b,c,d are complex numbers satisfying
ad −bc = 1.
Exercise
Show that SL(2,C) forms a non-commutative group under matrix
multiplication.
Hint
Since the determinant of the product of matrices is the product of their
determinants, SL(2,C) is closed under matrix multiplication. Further, matrix
product is associative. Since det A = 1, A is invertible, and det A−1 = 1/det A = 1,
implying A−1 exists and is in SL(2,C). The identity is given by
e =
 1
0
0
1
!
·
□
(viii) SU(n) denotes the set of all n×n unitary matrices with determinant 1 and is a group
under matrix multiplication. SU(n) is closed under matrix multiplication because
given two unitary matrices U1,U2 we see that their product is also unitary,
(U1U2)† = U†
2U†
1 = U−1
2 U−1
1
= (U1U2)−1
(7.4)
and the determinant of the product of matrices is the product of their determinants.
Further, matrix product is associative. Unit n × n matrix, which is the multiplicative
identity, is unitary.
For example, the group SU(2) consists of all 2 × 2 matrices of the form
 a
b
−b∗
a∗
!
, where |a|2 + |b|2 = 1.
The superscript ∗corresponds to complex conjugation.
Given a group G, a group G′ homomorphic to G is called a representation of G. If
a representation is isomorphic to G, it is called a faithful representation.
Representation of groups by multiplicative (or additive) groups of matrices is very
useful, especially when the representation is faithful, or even otherwise, because
many properties of the original group can be obtained by studying the
corresponding group of matrices which is, generally, much easier to do.
Exercise
Show that the set of all orthogonal matrices with determinant +1 forms
a group.
□
7.2
The Rotation Group O +(3)
We have already seen that the composition of two counterclockwise rotations is a
counterclockwise rotation. Thus, the set of all rotations on E3 is closed under the

Transformation Groups
197
composition of rotations. The composition of rotations is associative because by its
deﬁnition, both the compositions (R1 ◦R2) ◦R3 and R1 ◦(R2 ◦R3) can be
implemented in only one way, namely, by applying the individual rotations in the order
R3,R2,R1 in succession. The rotation with zero angle of rotation is the identity
rotation, which does not rotate anything at all, so that its composition with any other
rotation gives back the same rotation. The inverse of a rotation R(ˆn,θ) is R(ˆn,2π −θ).
Thus, the set of all possible rotations on E3 is a group under the composition of rotations,
called O +(3). All the rotations in O +(3) together leave one point in space invariant, a
point which is common to all the axes of rotation, taken to be the origin. Physically, a
body which is only rotating has to leave at least one point in it undisplaced or stationary
because the displacement of all points in the body corresponds to the translation of the
whole body. Note that this group is not only inﬁnite but is uncountable. In fact, it is
parameterized by three Euler angles and can be scanned by continuously varying these
parameters covering their ranges. These continuous parameters scanning the group form
a region in R3 and the continuous variations in these parameters correspond to different
possible paths in this region. In particular, starting from any rotation, its three Euler
angles can be continuously reduced to zero to reach the identity. Thus, every rotation
operator is continuously connected to the identity element e0ˆn× = I corresponding to
zero rotation. Because of this property, the group of rotations, O +(3), is called a
continuous group.
We have seen that, having chosen an orthonormal basis { ˆσk} k = 1,2,3 in E3, a matrix
representing every rotation R(ˆn,θ) is given by
ejk = ˆσj · R ˆσk,
where ejk is the jkth element of the matrix. By subsection 6.3.1 we know that every such
matrix is a 3×3 orthogonal matrix with determinant +1 and that the sets of rotations and
their matrix representatives are isomorphic. The last exercise tells us that the set of all
orthogonal matrices with determinant +1 is a group under matrix multiplication which
we call SO(3). All this just means that the group of 3 × 3 matrices with determinant +1,
SO(3), is a faithful matrix representation of the rotation group O +(3). Thus, these two
groups have the same structure and properties and it is enough to study SO(3) to
understand rotations in E3. In fact each 3 × 3 real matrix A deﬁnes a linear map
f : x 7→Ax on E3, so that, by the isomorphism between SO(3) and O +(3), the group
formed by the maps x 7→Ax with A ∈SO(3) is just O +(3). Since O +(3) is a three
parameter continuous group, so must be the isomorphic group SO(3).
Exercise
Show that SO(3) is a three parameter group.
Solution
The conditions of orthogonality on a 3 × 3 matrix A = [akj] are, by Eq. (6.7),
3
X
k=1
a2
kj = 1 (j = 1,2,3)

198
An Introduction to Vectors, Vector Operators and Vector Analysis
3
X
k=1
akiakj = 0 (i,j) = (1,2),(1,3),(2,3)
(7.5)
amounting to 6 constraints to be satisﬁed by 9 elements of the 3 × 3 matrix. This leaves
only 3 independent parameters out of 9 elements of A. Actually we have not counted the
constraint det A = 1. However, it turns out that this constraint does not reduce the number
of independent parameters, but eliminates all the matrices with determinant −1 from the
parent set of orthogonal matrices with determinant ±1.
□
The set of all orthogonal transformations on E3 forms a group. We know that the set
of orthogonal transformations is partitioned into two classes,
characterized by
transformations with determinant ±1 corresponding to rotations and reﬂections
respectively. The composition of two rotations is a rotation while the composition of two
reﬂections is a rotation by Hamilton’s theorem (subsection 6.1.2). Thus, the composition
of two orthogonal transformations is an orthogonal transformation. The composition of
rotations is associative and by the same argument the composition of reﬂections is also
associative. Thus, the composition of orthogonal transformations is associative. The
inverse of an orthogonal transformation is uniquely given by its adjoint. Finally,
the identity transformation is orthogonal. This makes the set of all orthogonal
transformations on E3 a group under the composition of transformations and we call it
O (3). Obviously, O +(3) is a subgroup of O (3), however, the class of reﬂections is not,
because it is not closed under the composition of reﬂections.
Exercise
Show that the product of two orthogonal transformations (matrices) is an
orthogonal transformation (matrix).
Hint
Proceed as in Eq. (7.4) for unitary matrices.
□
In section 4.5 we proved that the matrix representing the product of two transformations
is the product of the matrices representing the factors, (in the same order). In section 6.1
we showed that the matrix representing an orthogonal operator is orthogonal. By the
above exercise, the product of two orthogonal matrices is orthogonal. If two orthogonal
matrices are equal, then the corresponding orthogonal operators are equal, just as in the
case of rotations. The set of all 3 × 3 orthogonal matrices forms a group under matrix
multiplication (Exercise) also called O (3). All this just means that the group of
orthogonal 3 × 3 matrices is a faithful representation of the group of orthogonal
transformations on E3. It is then enough to analyze the group of matrices O (3), in order
to get the structure and properties of the group of orthogonal transformations on E3. In
fact the group of orthogonal transformations is identical to the group of linear maps
x 7→Ax on E3 where A is a 3 × 3 orthogonal matrix. O (3) is also a continuous group
driven by three independent parameters as we saw for its subgroup SO(3).

Transformation Groups
199
7.3
The Group of Isometries and the Euclidean Group
We ﬁrst deﬁne the translation group. A translation τa in E3 is deﬁned by
τa(x) = x + a
(7.6)
In physical applications we apply this transformation to position vectors of particles
comprising a physical object as shown in Fig. 7.3. Notice that the translation operator is
not linear. We have,
τa(x + y) = x + y + a , τa(x) + τa(y) = x + y + 2a.
We show that the set of all translations in E3 forms an Abelian group. We have,
(i) (Closure)
(τaτb)x = τa+bx = (τbτa)x.
(ii) (Associativity)
(τaτb)τcx = τa+b+cx = τa(τbτc)x.
(iii) (Identity)
τ0(x) = x + 0 = x implies I = τ0 is the identity.
(iv) (Inverse)
τ−a is the inverse of τa.
Fig. 7.3
Translation of a physical object by a
This proves what we wanted. All these properties follow from those of vector addition in
E3. In fact the translation group is isomorphic with the group formed by E3 under vector
addition (Exercise).
An isometry of Euclidean space E3 is a bijective (one to one and onto) transformation
σ : E3 7→E3 such that d(σ(x),σ(y)) = d(x,y), where d(x,y) = +
p
(x −y) · (x −y) is
the Euclidean distance between x and y, for all x,y ∈E3.
We ﬁrst show that all the isometries {σ} form a group.
(i) (Closure)
Clearly, the composition of two isometries is an isometry, as it is the
successive application of two transformations, each preserving distance. For the
composition ησ of two isometries η and σ we have
d(ησ(x),ησ(y)) = d(σ(x),σ(y)) = d(x,y).

200
An Introduction to Vectors, Vector Operators and Vector Analysis
(ii) (Associativity)
Let σ1,σ2,σ3 be isometries. Then, both (σ1σ2)σ3 and σ1(σ2σ3)
have to be obtained by successively applying σ3,σ2,σ1 (in that order), making
them equal.
(iii) (Identity)
The identity transformation I(x) = x is an isometry.
(iv) (Inverse)
By the bijection property, every isometry σ has an inverse, σ−1 and since
σ is an isometry
d(σ−1(x),σ−1(y)) = d(σσ −1(x),σσ−1(y)) = d(x,y)
so that σ −1 is an isometry.
Items (i)–(iv) above show that the set of all isometries in E3 form a group.
We now obtain some of the basic properties of an isometry.
Consider an orthonormal basis {ˆe1, ˆe2, ˆe3} and an isometry σ which leaves the vectors
{0, ˆe1, ˆe2, ˆe3} invariant. Then, we want to show that σ is the identity. Let x,x′ ∈E3 and
σ(x) = x′. Since σ(0) = 0 we have d(x,0) = d(σ(x), σ(0)) = d(x′, 0). This gives,
x2 = (x′)2.
(7.7)
Similarly, invariance of {ˆe1, ˆe2, ˆe3} under σ gives, for example,
(x −ˆe1) · (x −ˆe1) = (x′ −ˆe1) · (x′ −ˆe1)
or,
x2 −2x · ˆe1 + 1 = (x′)2 −2x′ · ˆe1 + 1
(7.8)
From Eqs (7.7) and (7.8) we get,
x = x′
or, σ(x) = x for all x ∈E3, giving σ = I. Note that this conclusion is trivial for a linear
operator as it follows directly from linearity. However, isometry is not linear in general.
Let σ be an isometry which leaves 0 invariant, that is, σ leaves one point in E3 ﬁxed.
Then we know that σ is an orthogonal transformation. In fact from Eq. (7.7) we know that
σ(0) = 0 implies σ(x) · σ(x) = x · x, or, σ preserves the length of vectors in E3. Hence, σ
is an orthogonal transformation.
Let σ be an isometry with σ(0) = a. Then
σ(x) = Ax + a x ∈E3
(7.9)
where A is an orthogonal transformation. To see this, deﬁne a translation τa(x) = x + a.
This is an isometry with inverse τ−a(x) = x −a. Thus, τ−aσ(0) = 0 so that τ−aσ is an
isometry ﬁxing 0. Therefore, τ−aσ must be an orthogonal transformation which we denote
by A. We can then write
σ(x) = τaτ−aσ(x) = τaA(x) = A(x) + a
(7.10)

Transformation Groups
201
In fact every isometry is given by the form in Eq. (7.9), because when a , 0 (a = 0) in
σ(0) = a, it is given by Eq. (7.9) (Eq. (7.9) with a = 0) and there are no other cases.
We can now conclude that the group of isometries is a six parameter group, three
parameters are required to ﬁx the orthogonal transformation A while three more are
required to ﬁx the translation a. We are interested in the subgroup consisting of isometries
given by the product of a rotation and a translation, called Euclidean group. Each such
isometry is physically realized by a displacement of a rigid body. A rigid body is a system
of particles with ﬁxed distances from one another, so every displacement of a rigid body
must be an isometry. A ﬁnite rigid body displacement must unfold continuously, so it
must be continuously connected to the identity. In the last subsection we saw that this
property is availed by rotations which are the elements of SO(3). Thus, only the isometries
composed of a rotation and a translation have this property. An isometry of this kind is
called a rigid displacement. Thus, all rigid displacements form a continuous group of
isometries having the canonical form (see Fig. 7.4)
σ(x) = τaR(x) = R(x) + a
(7.11)
Fig. 7.4
A rigid displacement is the composite of a rotation and a translation. The
translation vector a need not be in the plane of rotation.
where R ∈SO(3) is a rotation. Note that the rotation R is about an axis through the
origin so the origin is a distinguished point in this representation of the rigid displacement.
However, the choice of origin was completely arbitrary in getting Eq. (7.11), so different
choices of the origin give different decompositions of a rigid displacement into a rotation
and a translation. Next, we show how these are related.
Let Rb denote a rotation about a point b and let R0 = R denote the same rotation
about the origin 0. The rotation about the point b can be effected via the following sequence
of operations. (i) Translate the body by −b to shift the point b to the origin. (ii) Perform
the rotation R about the origin. (iii) Translate by b to shift the origin back to the point b.
The resulting transformation is given by
Rb(x) = τbRτ−b(x) = R(x −b) + b = R(x) −R(b) + b
(7.12)

202
An Introduction to Vectors, Vector Operators and Vector Analysis
which expresses Rb in terms of R.
Next, we ﬁnd the equation to the axis of rotation through point b. The rotation axis for
Rb is the set of points invariant under Rb. This is the set of points x satisfying the
equation
Rb(x) = x
(7.13)
The points x satisfying Eq. (7.13) are the ﬁxed points of Rb. Combining Eqs (7.12) and
(7.13) we get
R(x −b) + b = x
(7.14)
As a check we ﬁnd that Rbb = b as it should. If Rb is not an identity transformation,
Eq. (7.14) determines a straight line passing through the point b. To see this, note that
Eq. (7.14) can be written R(x −b) = x −b which means that the rotation axis for the
rotation R passing through the new origin b is given by x = x′+b where x′ = x−b deﬁnes
the axis through origin. The rotations Rb and R = R0 then rotate the body through
equal angles about parallel axes passing through the points b and 0 respectively. However,
the rotations about such parallel axes do not generally commute, that is, RRb , RbR,
as seen from Eq. (7.12).
We obtain the conditions under which a rigid displacement given by Eq. (7.11) is a
rotation. That is, can we change the origin suitably so that the rigid displacement in
Eq. (7.11) is effected via the rotation Rb, deﬁned in Eq. (7.12), about the shifted origin at
b? That is, we try and ﬁnd b such that
Rb(x) = R(x) + a.
(7.15)
where Rb is a rotation about point b. The vector b can be decomposed into components
b∥and b⊥being parallel and perpendicular to the axis of rotation respectively to give
b = b∥+ b⊥
(7.16)
Putting this in Eq. (7.12) we get
Rb = R(x) + b⊥−R(b⊥)
(7.17)
Comparison between Eqs (7.15) and (7.17) tells us that the following condition must be
satisﬁed by the required vector b.
a = b⊥−R(b⊥)
The vector on the RHS of this equation lies in a plane perpendicular to the rotation
axis determined by R. We can conclude from the above condition on b that a rigid
displacement R(x) + a is a rotation if and only if the translation vector a = a⊥is

Transformation Groups
203
perpendicular to the axis of rotation. To emphasize this fact, we rewrite the condition on
b as
a⊥= b⊥−R(b⊥)
(7.18)
We note that both the axes of rotation, through the origin 0 and through b are parallel and
share the same plane of rotation perpendicular to both of them. Both vectors a⊥and b⊥
lie in the plane of rotation which we can view as a complex plane and replace the rotation
operator R in Eq. (7.18) by eiφ where φ is the angle of rotation and treat vectors a⊥and
b⊥like complex numbers. This gives
b⊥=
a⊥
1 −eiφ = 1
2a⊥
 
1 + i cot φ
2
!
or, switching over to vectors,
b⊥= 1
2
 
a⊥+ (ˆn × a⊥)cot φ
2
!
(7.19)
where ˆn is the unit vector deﬁning the axis of rotation. Note that the transformation
R(x) + a⊥leaves every plane perpendicular to the rotation axis invariant and it consists
of a rotation-translation in each such plane. Thus, we have proved that every
rotation-translation R(x) + a⊥in a plane is equivalent to the rotation centered at the
point b⊥given by Eq. (7.19) as shown in Fig. 7.5. Our proof fails if there is no rotation
(φ = 0), in which case we have pure translation. Thus, we have proved that every rigid
displacement in a plane is either a rotation or a translation.
Fig. 7.5
Equivalence of a rotation/translation in a plane to a pure rotation

204
An Introduction to Vectors, Vector Operators and Vector Analysis
It is immediate from Eq. (7.17) that b⊥= 0 implies Rb = R. Thus, the rotations differing
by the shift of origin along the rotation axis are equivalent. Indeed, no parameters deﬁning
the rotation change by a translation along the axis of rotation.
7.3.1
Chasles theorem
Given any rigid displacement σ(x) = R(x) + a, we decompose the translation a into
components a∥and a⊥, parallel and perpendicular to the rotation axis deﬁned by R,
so that
σ(x) = R(x) + a⊥+ a∥
(7.20)
Now R(x) + a⊥can be treated as a rotation Rb, so that
σ(x) = τa∥Rb(x)
(7.21)
where τa∥is the translation parallel to the rotation axis Rb. Equation (7.21) proves
Chasles theorem: Any rigid displacement can be expressed as a screw displacement. A
screw displacement consists of a product of rotation with a translation along the axis of
rotation (the screw axis). We have done more than proving Chasles theorem, we have
shown how to ﬁnd the screw axis of a given rigid displacement. Although elegant, Chasles
theorem is seldom used in practice. Equation (7.11) is usually more useful, because the
center of rotation (the origin) can be speciﬁed at will to simplify the problem at hand.
Finally, note that b = b∥(i.e., b⊥= 0 in Eq. (7.16)) gives, via Eq. (7.12),
Rb(x) = R(x)
(7.22)
as it should.
Exercise
A rigid displacement σ(x) = R(x) + a can be expressed as a product of a
translation τc and a rotation Rb centered at a speciﬁed point b. Determine the translation
vector c.
Hint
Using Eq. (7.12) R(x) + a = τcRb can be reduced to c = a −b + R(b). a,b
may be speciﬁed as column or row matrices and R ∈SO(3) as a 3 × 3 special orthogonal
matrix. Otherwise R may be given as a rotation operator.
□
Exercise
A subgroup H of group G is called an invariant subgroup if g−1hg ∈H for
every h ∈H and every g ∈G. Show that the translations T form an invariant subgroup of
the group E of isometries on E3.
Solution
Let σ ∈E and τa ∈T . Then,
(σ −1τaσ)(x) = x + σ−1(a)
which is a translation τσ−1(a) ∈T .
□

Transformation Groups
205
Exercise
Let S denote the reﬂection in the plane normal to a non-zero vector a. If τa is
the translation by a then Sa = τaSτ−a is the reﬂection S shifted to the point a. Show that
SS−a = τ2a.
Thus, a translation by a can be expressed as a product of reﬂections in parallel planes
separated by 1
2a.
Solution
Since S is a linear operator, S2 = I and S(a) = −a, we have,
S−a(x) = S(x + a) −a = S(x) + S(a) −a
= S(x) −2a
giving
SS−a = S2(x) −2S(a) = x + 2a = τ2a.
□
7.4
Similarities and Collineations
Isometries preserve lengths of vectors as well as the angles between vectors. We call a non-
empty subset of E3 a ﬁgure. Two ﬁgures S and S∗in E3 are congruent if and only if S∗=
σ(S) for some isometry σ on E3.
Exercise
Show that congruence is an equivalence relation.
Solution
This is obvious because isometries form a group.
(i) Since identity is an isometry, a ﬁgure is congruent to itself.
(ii) S∗= σ(S) implies S = σ−1S∗so that congruence is reﬂexive.
(iii) Since a composition of isometries is an isometry, if S1 is congruent to S2 and S2 is
congruent to S3 then S1 is congruent to S3. Thus, congruence is transitive.
□
Two ﬁgures are said to be similar if they have the same shape but not the same size, so that
one is congruent to an enlargement of the other. Two ﬁgures S and S∗are similar if and
only if S∗= Σ(S) where Σ, is called a similarity transformation on E3, and is given by
Σ : x 7→λA(x) + a, λ ∈R, λ , 0,A orthogonal
If λ < 0 then we take −A to be the orthogonal transformation. Similarity transformations
form a group which contains isometries as a subgroup. The similarity transformations do
not preserve distance however, they preserve ratios of distances, that is,
d(Σ(a),Σ(b))
d(Σ(c),Σ(d)) = d(a,b)
d(c,d)·

206
An Introduction to Vectors, Vector Operators and Vector Analysis
Both isometries and similarities are subgroups of a more general group of transformations
called collineations which transform lines into lines. All transformations of the form
A : x 7→A(x) + a
A invertible
are collineations and are called Afﬁne transformations. Afﬁne transformations form a
group called Afﬁne group. Note that both isometries and similarities are afﬁne
transformations.
Let G be the afﬁne group and let Ωbe the set which is either E3 or a class of ﬁgures
in E3 but not both. We deﬁne a relation ≡on Ωnamely, α ≡β if and only if there exists
σ ∈G such that σ(α) = β.
Exercise
Show that ≡is an equivalence relation.
Hint
Again, this follows from the fact that afﬁne transformations form a group.
So
proceed just the way we showed congruence to be an equivalence relation.
□
Consider a subset of Ωconsisting of all elements which are related via ≡. Such a subset is
called an equivalence class of ≡. To construct such a subset pick up an element in Ωand
collect all elements of Ωrelated to it. If the complement of this subset in Ωis not empty,
pick out an element from the complement and collect all elements related to it. Repeat this
procedure until all of Ωis exhausted. Obviously, all these subsets, or equivalence classes,
are mutually exclusive, because if any two of them have an element in common, by
transitivity property it will be related to all the elements of both the subsets, so that their
union will form a single equivalence class. Thus, Ωis partitioned by its equivalence
classes, that is, two equivalence classes have empty intersection and the union of all of
them is Ω.
When G is the afﬁne group the elements of the equivalence class of ≡on Ωvia G are
called afﬁne equivalent.
Instead of deﬁning via the afﬁne group, we can deﬁne ≡via the similarity group or the
isometry group to get the same results.
We now classify the set of all central conics, (deﬁned below), which are the orbits of
particles driven by the inverse-square law of force, using group of afﬁne transformations or
groups of isometries and similarities.
Conics are the loci of the second degree, that is, the non-empty point sets in E2, given
by Sprienger
Γ = {(x,y)|ax2 + 2hxy + by2 + 2gx + 2f y + c = 0 a , 0 or h , 0 or b , 0}
Conics for which ab , h2 are called central conics.
We intend to examine the effect of an afﬁne (or isometry or similarity) transformation
on a conic Γ . The equation of Γ , mentioned in its deﬁnition can be alternatively expressed
in the matrix form as
uAuT + 2ukT + c = 0
(7.23)

Transformation Groups
207
where u,k are 1 × 2 matrices and a is a 2 × 2 symmetric matrix
u =

x
y

A =
 a
h
h
b
!
k =

g
f

and c is a 1 × 1 matrix. Now we make an afﬁne transformation σ : u 7→u′ = [x′,y′] so
that u = u′S + w (S invertible) and obtain the matrix equation
(u′S + w)A(ST (u′)T + wT ) + 2(u′S + w)kT + c = 0
which can be simpliﬁed to
u′A′(u′)T + 2u′(k′)T + c′ = 0
(7.24)
where A′ = SAST , k′ = kST + wAST and c′ = c + 2wkT + wAwT . Equation (7.24) is
again a second degree equation so that (x′,y′) must lie on a conic
Γ ′ = {(x,y) | a′x2+2h′xy +b′y2+2g′x+2f ′y +c′ = 0 a′ , 0 or h′ , 0 or b′ , 0}
Since det A′ = (det S)2 det A (det S , 0) we have ab , h2 if and only if a′b′ , (h′)2 ; in
other words, central conics are transformed into central conics. Now choose the
transformation w = −kA−1 giving k′ = 0, thus eliminating all the ﬁrst degree terms from
Eq. (7.24). The point represented by the vector −kA−1 is called the center of the conic Γ .
Note that when ab = h2, A−1 does not exists and Γ cannot have a center. Using
w = −kA−1 we obtain c′ = c −kA−1kT which on evaluation gives
c′ =
∆
ab −h2
∆= abc + 2f gh −af 2 −bg2 −ch2
(7.25)
To ﬁnd the afﬁne equivalent class of central conics, we have to ﬁnd the criteria which
guarantee (or otherwise) the existence of an afﬁne transformation connecting the given
conics Γ and Γ ′. That is, given Γ and Γ ′, as in Eqs (7.23) and (7.24), when can one ﬁnd an
invertible matrix S transforming Γ ′ to Γ . We differ this question until we have obtained
the effect of the Euclidean transformations (isometries) on central conics and ﬁnd its
equivalence classes.
When σ : u 7→u′ is an isometry, the above analysis goes through, with the reservation
that the matrix S deﬁned by u = u′S + w must be orthogonal. We are interested in the
isometries continuously connected to identity, so we restrict to the Euclidean group and
require S to be special orthogonal (det S = +1). Since A is symmetric and S is special
orthogonal, we can choose S such that the matrix A′ = SAST is diagonal with the diagonal
elements as the eigenvalues of A. Thus, we can write A′ = diag(λ,µ) where λ,µ are the

208
An Introduction to Vectors, Vector Operators and Vector Analysis
roots of the equation t2 −(a + b)t + ab −h2 = 0. We can therefore ﬁnd an Euclidean
transformation which takes the central conic Γ into the conic Γ ′ with equation
λx2 + µy2 + ∆/(ab −h2) = 0
(7.26)
If ∆, 0 Eq. (7.26) can be rewritten as
αx2 + βy2 = 1
(7.27)
where α + β = −(a + b)(ab −h2)/∆, αβ = (ab −h2)3/∆2.
We will now show that the pair of numbers {α,β} characterizes the Euclidean
equivalence class of Γ . If Γ is Euclidean equivalent to a conic Γ ′′ with the equation
γx2 + δy2 = 1
(7.28)
then there is a transformation [x y] 7→[x y]U + c (U orthogonal) taking Γ ′ to Γ ′′. It is
easily seen that we must have c = 0 and
 γ
0
0
δ
!
= U
 α
0
0
β
!
UT
which is possible if and only if {γ,δ} = {α,β}. Thus, two central non-degenerate (i.e.,
∆, 0) conics are Euclidean equivalent if and only if they have the same values for α and β
or, equivalently, for α + β and αβ given by Eq. (7.27). In other words, the quantities
−(a + b)(ab −h2)
∆
and (ab −h2)3
∆2
(7.29)
are invariants for the central (ab , h2) and non-degenerate (∆, 0) conics under the action
of the Euclidean group.
Under the similarity group, any central non-degenerate conic again reduces to a conic
with Eq. (7.27), but this conic is equivalent to the conic in Eq. (7.28) if and only if either
γ/δ = α/β or γ/δ = β/α. The pair {α/β,β/α} or equivalently the number (α/β) +
(β/α) or equivalently the number
(α + β)2
αβ
= (a + b)2
(ab −h2)
(7.30)
is the required invariant under the similarity group.
Under the afﬁne group, the conics with Eqs (7.27) and (7.28) are equivalent if and only
if αβ and γδ have the same sign, because in this case (with U an invertible matrix not
necessarily orthogonal) the determinants of the corresponding matrices are related by
γδ = (det U)2αβ
(7.31)

Transformation Groups
209
and since the conic is central, αβ , 0. We also note that both α and β cannot be < 0
because in that case, no (x,y) can satisfy Eq. (7.27). Thus, Eq. (7.31) does imply that αβ
and γδ have the same sign. There are thus only two afﬁne equivalent classes of central
non-degenerate conics, namely those for which ab −h2 > 0 (ellipses) and those for which
ab −h2 < 0 (hyperbolae). Note that in the afﬁne geometry, any two ellipses are equivalent,
while in Euclidean geometry they are equivalent if they have the same pair of Euclidean
invariants given by Eq. (7.29), which means that the two ellipses must be of the same size.
All ellipses are afﬁne equivalent to the locus of the equation x2 +y2 = 1 that is the unit
circle. All hyperbolae are afﬁne equivalent to the locus of the equation x2 −y2 = 1. This is
a disconnected set with two components, namely,
{(x,y) | x2 −y2 = 1, x > 0} and {(x,y) | x2 −y2 = 1, x < 0}.
Finally, we note that the Euclidean equivalent ﬁgures have the following property: One
ﬁgure can be superposed on the other by rigid displacement. Thus, the group of rigid
displacements describes all possible relations of congruency. These relations underlie all
physical measurements. A ruler is a rigid body and any measurement of length involves
rigid displacements to compare a ruler with the object being measured.
Exercise
This is a small project for the students:
Discuss the Euclidean, similarity and afﬁne equivalence classes of non-singular central
quadrics in E3 i.e., the loci
n
(x,y,z) | ax2 + by2 + cz2 + 2f yz + 2gzx + 2hxy + 2ux + 2vy + 2wz + d = 0
o
with

a
h
g
h
b
f
g
f
c

, 0

a
h
g
u
h
b
f
v
g
f
c
w
u
v
w
d

, 0,
where vertical bars mean the determinants of the corresponding matrices. Show in
particular, that there are three afﬁne equivalent classes and ﬁnd simple canonical
representatives of these classes.
□


Part III
Vector Analysis


Transformation Groups
213
This may be translated as follows:1
Multiply the arc by the square of the arc and take the result of repeating that [any
number of times]. Divide [each of the above numetrators] by the squares of successive
even numbers increased by that number [lit. the root] and multiplied by the square of the
radius.
Place the arc and the successive results so obtained one below the other and
subtract each from the one above. These together give the Jiva, as collected together in the
verse beginning with “vidvan” etc.
Indian mathematics and astronomy dealt not directly with present-day sines and
cosines but with these quantities multiplied by the radius r of a standard circle. Thus, jiva
corresponds to r sinθ while sara corresponds to r(1 −cosθ).
In the present-day mathematical terminology the above passage says the following. Let
r denote the radius of the circle, s denote the arc and tn the nth expression obtained by
applying the rule cited above. The rule requires us to calculate as follows.
1. Numerator:
Multiply the arc s by its square s2, this multiplication being repeated n
times to obtain s · Πn
1s2.
2. Denominator:
Multiply the square of the radius, r2, by [(2k)2 + 2k] (“squares of
successive even numbers increased by that number”) for successive values of k,
repeating this product n times to obtain Πn
k=1r2[(2k)2 + 2k].
Thus, the nth iterate is obtained by
tn =
s2n · s
(22 + 2) · (42 + 4)···[(2n)2 + 2n] · r2n
The rule further says:
jiva = s −t1 + t2 −t3 + t4 −t5 + ···
= s −
s3
r2 · (22 + 2) +
s5
r4(22 + 2)(42 + 4) −···
1This epighraph is taken from ref.[18]

214
An Introduction to Vectors, Vector Operators and Vector Analysis
Substituting
(i) jiva = r sinθ,
(ii) s = rθ, so that s2n+1/r2n = rθ2n+1 and noticing that
(iii) [(2k)2 + 2k] = 2k · (2k + 1) so that
(iv) (22 + 2) · (42 + 4)···[(2n)2 + 2n] = (2n + 1)!,
and cancelling r from both sides, we see that the inﬁnite series for Jiva is entirely equivalent
to the well known Taylor series for sinθ :
sinθ = θ −θ3
3! + θ5
5! −θ7
7! + ···
It is now well known that calculus was developed in India starting mid-ﬁfth century
(Aryabhata in Bihar) until mid-fourteenth century (Madhava in Kerala) with a long list of
brilliant mathematicians ﬁlling in the gap. Indians invented powerful techniques to
accelerate convergence of a series and to sum a given series to the required accuracy [18].
Thus, Madhava produced a table of values of sinθ and cosθ exact upto ten decimal digits
by summing up their Taylor series (better called Madhava series!). Values to this accuracy
were required for navigation (locating ships and ﬁnding directions on open sea) and
timekeeping (yearly scheduling of agricultural activities, vis-a-vis rainy season, to
maximize production).

8
Preliminaries
8.1
Fundamental Notions
This part deals with the basic concepts and applications of differential and integral calculus
to functions involving vector variables. By a function we mean a one to one or many to one
mapping between non-empty sets say X and Y and denote it by f : X 7→Y . In general, f
maps a subset of X, called its domain and denoted D(f ), to a subset of Y called its range
or image set and denoted R(f ). If R(f ) = Y then the function is called onto. If f is
one to one and onto, it is invertible (see section 4.1). Note that the sets X and Y can be
identical, X = Y , so that the function is f : X 7→X and both the domain and the range
of f are the subsets of the same set X. If x ∈D(f ) is mapped to y ∈R(f ) under f , then
x is called the argument of f , y is called the image of x under f and is denoted f (x), that
is, y = f (x). f (x) is said to be the value of the function f at x. In general, we can say that
x is a variable taking values in D(f ) and f (x) are the corresponding values in R(f ). The
image set of a subset E ⊆D(f ) under f is denoted f (E). The equality, addition as well as
the composition of two or more functions is exactly as given in section 4.1.
In this book we are concerned with the following three classes of functions.
• Vector valued functions of a scalar variable f : R 7→E3. These functions generally
occur as a part of the kinematics and dynamics of a physical system. For example, the
velocity of a particle as a function of time v(t).
• Scalar valued functions of a vector variable, f : E3 7→R. All scalar ﬁelds φ(x) fall in
this category, as a scalar ﬁeld is a scalar valued function of position vectors or points
in space, e.g., the temperature proﬁle in a region of space.
• Vector valued functions of a vector variable, f : E3 7→E3. All linear operators on E3
fall in this category. All vector ﬁelds are also functions (of position vectors) falling in
this class.
In what follows we assume that the space E3 or (R3) and a real line R form a continuum
(see section 1.2). We also treat these as metric spaces with Euclidean metric.

216
An Introduction to Vectors, Vector Operators and Vector Analysis
In this chapter, a vector is referred to either as a vector or as a point in space. Further,
in this chapter we use the same symbol to indicate a vector or a scalar, because whatever
is said about it applies to both the cases. At any rate, its being a vector or a scalar can be
understood with reference to context. Also, by a function we mean a function in one of the
three categories described above.
8.2
Sets and Mappings
We need the following properties of sets and mappings all shared by the subsets of E3
and R.
Two sets A and B are said to be in 1 −1 correspondence if a one to one and onto map
can be found between them. Such sets are said to have the same cardinality or are said to
be equivalent and we write A ∼B. Clearly, the relation A ∼B has the following properties.
• Reﬂexivity: A ∼A.
• Symmetry: If A ∼B then B ∼A.
• Transitivity: If A ∼B and B ∼C then A ∼C.
Exercise
Prove the above properties.
Hint
The identity I : A 7→A is a 1−1 correspondence. Inverse of a 1−1 correspondence is
a 1−1 correspondence. Composition of two 1−1 correspondences is a 1−1 correspondence.
□
Let Nk denote the set {1,2,...,k} for some integer k > 0 and let N be the set {1,2,3,...} of
all integers > 0. Given a set A we say
• A is ﬁnite if A ∼Nk for some k ≥0. The empty set corresponding to k = 0 is also
considered to be ﬁnite.
• A is inﬁnite if it is not ﬁnite.
• A is countable if A ∼N.
• A is uncountable if it is neither ﬁnite nor countable.
A countable set is sometimes called enumerable or denumerable.
For two ﬁnite sets A and B we evidently have A ∼B if and only if they contain the same
number of elements. The set I of all integers is countable as can be seen from the following
1 −1 correspondence between I and N.
I : 0 1 −1 2 −2 3 −3···
N : 1 2 3 4 5 6 7···
Exercise
Find f : N 7→I generating this 1 −1 correspondence.

Preliminaries
217
Answer
f (n) =

n
2
(n even)
−n−1
2
(n odd).
□
This example shows that an inﬁnite set can be put to 1 −1 correspondence with one of its
proper subsets. This is not possible for ﬁnite sets.
Since R, R3 and E3 are continua, we expect each of them to form an uncountable set.
Also, every subset of these spaces, which forms a continuous region of space must also be
an uncountable set. We accept this to be true without supplying any proofs.
8.3
Convergence of a Sequence
All analysis, be it real, complex or vector analysis, can be constructed on the basis of a
single fundamental concept, namely, the convergence of an inﬁnite sequence of points (or
sequence for short) in the given space.
A sequence is a function deﬁned on the set of all positive integers {1,2,3,...}. We are
basically interested in sequences deﬁned by the functions f : N 7→R and f : N 7→E3
which are the sequences of scalars and vectors respectively. We denote the sequence f (n) =
xn,(n ∈N) by the symbol {xn} or by x1,x2,x3,.... The elements xn forming the sequence
are called the terms of the sequence. If A is a set and if xn ∈A for all n ∈N then {xn} is said
to be a sequence in A. Note that the terms of a sequence may be distinct or identical. The
set of all points xn, (n = 1,2,...) is the range of the sequence {xn}. The range of a sequence
may be a ﬁnite set or it may be inﬁnite. A sequence {xn} is said to be bounded if its range
is bounded (that is, the set formed by the distinct elements of a sequence is a bounded set,
see below). We are interested in sequences in R or in E3.
The concept of the convergence of a sequence in a metric space can be deﬁned without
referring to a particular metric space. Therefore, we deﬁne the the convergence of a
sequence in a metric space X which stands for both R and E3. A subset S ⊂X is said to be
bounded if there is a real M > 0 satisfying d(p,q) ≤M for all p,q ∈S. The smallest M
satisfying this condition is called the diameter of S. A r-neighborhood of a point p ∈X is
a set Nr(p) consisting of all points q such that d(p,q) < r. The number r is called the
radius of Nr(p). An open set is a subset E of X such that every point in it has a
neighborhood which is a proper subset of that set. Each such point is called an interior
point so an open set is the one whose every element is an interior point. In particular, a
r-neighborhood of any point in a metric space is an open set. A point p ∈E is its
boundary point if every neighborhood of p has a point q ∈E, q , p, but is not a subset of
E. A set containing all its interior as well as its boundary points is called a closed set. Thus,
the set of all points inside a sphere of radius R is an open set while the points on the sphere
form the set of boundary points. In general, in a metric space, given ϵ > 0, the set of points
with distance < ϵ from a given point form the ϵ-neighborhood of that point. The set of
points at a distance ϵ from the given point form the set of boundary points of this
ϵ-neighborhood.

218
An Introduction to Vectors, Vector Operators and Vector Analysis
A sequence of points in the metric space X, say, x1,x2,x3,... is said to be convergent
if for every ϵ > 0, however small, there is an open set of diameter ϵ such that all except
ﬁnitely many points of the sequence are elements of this set (see Fig. 8.1). Consider the
sequence of real numbers 0 < ϵ1 > ϵ2 > ϵ3 > ··· ϵn−1 > ϵn > ϵn+1 > ··· and the open
sets of diameters ϵ1 > ϵ2 > ϵ3 ··· each of which contains all except ﬁnitely many elements
of the converging sequence. Obviously, the set corresponding to ϵk is a proper subset of all
sets corresponding to ϵn, n < k. If the diameter of these subsets is reduced without bounds,
then these sets keep on approaching a set with singleton point, that is, the set corresponding
to ϵ = 0. This point is called the limit of the converging sequence.
Fig. 8.1
A converging sequence in E3
Exercise
Show that the limit of a converging sequence is unique.
Hint
Assume two distinct limits and arrive at a contradiction. We have to also assume
that two distinct points can have disjoint neighborhoods, a property possessed by E3
and R.
□
Exercise
If two sequences {xi} and {yi} in E3 or R converge to x∗and y∗respectively in
E3 or R, show that the sequence {xi +yi} converges to x∗+y∗in E3 or R. Further, if these
sequences are in R and converge to these limits in R, then show that the sequence {xiyi}
converges to x∗y∗in R.
Hint
We have to show that if the Euclidean distances d(xn,x∗) < ϵ and d(yn,y∗) < ϵ then
d(xn + yn,x∗+ y∗) < αϵ and d(xnyn,x∗y∗) < βϵ where α,β are constants independent
of n.
□
Exercise
A sequence {xi} in a metric space X converges to x∗in X. Show that its
isomorphic image {yi} in space Y isometrically isomorphic to X converges to the
isomorphic image y∗∈Y of the limit x∗∈X.
Hint
Two linear spaces X and Y are said to be isometrically isomorphic if the
isomorphism T satisﬁes ||T (x)|| = ||x|| for all x ∈X. Obviously, such an isomorphism
preserves distance,
d(x,y) = ||x −y|| = ||T (x −y)|| = ||T (x) −T (y)|| = d(T (x),T (y))

Preliminaries
219
from which the result follows. Thus, a sequence in E3 converging to a vector in E3 is also a
sequence in R3 converging to a point represented by the vector at the limit.
□
Uniqueness of the limit of a converging sequence enables us to re-deﬁne its convergence as
follows.
A sequence {xi} in a metric space X is a sequence converging to x∗if for every ϵ > 0
there is an integer n0 > 0 such that d(xn,x∗) < ϵ whenever n > n0. The fact that x∗is the
limit of a converging sequence {xk} is summarily expressed as limk→∞xk = x∗.
Exercise
Suppose {xn} is in R and limn→∞xn = x∗. Show that limn→∞1
xn =
1
x∗
provided xn , 0,(n = 1,2,...) and x∗, 0.
□
Exercise
(a) Suppose xn ∈R3(n = 1,2,3,...) and xn = (α1,n,α2,n,α3,n). Then xn converges to
x = (α1,α2,α3) if and only if limn→∞αj,n = αj,j = 1,2,3.
(b) Suppose {xn} {yn} are sequences in R3 and {βn} is a sequence in (R) and xn →x,
yn →y, βn →β. Then,
lim
n→∞(xn + yn) = x + y
lim
n→∞(xn · yn) = x · y
lim
n→∞βnxn = βx.
Solution
(a) If xn →x, the inequalities
|αj,n −αj| ≤|xn −x|
which follow immediately from the deﬁnition of the norm in R3 show that
limn→∞αj,n = αj,j = 1,2,3.
Conversely, if limn→∞αj,n = αj,j = 1,2,3, then to each ϵ > 0 there is an integer
N such that n ≥N implies
|αj,n −αj| < ϵ
√
3
j = 1,2,3.
Hence, n ≥N implies
|xn −x| =

3
X
j=1
|αj,n −αj|2

1
2
< ϵ,
so that xn →x, which proves (a).
(b) Hint
Use part (a).
□

220
An Introduction to Vectors, Vector Operators and Vector Analysis
8.4
Continuous Functions
Consider a converging sequence x1,x2,x3,... in the domain D(f ) of a function f with its
limit x∗∈D(f ). The function f is said to be continuous at x∗if the sequence
f (x1),f (x2),f (x3),... converges to the limit f (x∗) and this happens for all sequences in
D(f ) converging to x∗. The continuity of a function at a point can be expressed as
lim
x→x∗f (x) = f (x∗),
or,
lim
x→x∗d(f (x),f (x∗)) = 0,
or, assuming the Euclidian distance
lim
x→x∗||f (x) −f (x∗)|| = 0.
Exercise
Show that if the functions f (x) and g(x) are continuous at x∗then so is their
sum f (x) + g(x) and their product f (x)g(x).
□
In general, we say that
lim
x→x∗f (x)
exists if for every sequence {xn} converging to x∗, the corresponding sequence {f (xn)}
converges to the same limit. In terms of this deﬁnition, the result of the third exercise of
this section can be used to get
lim
x→x∗[f (x) + g(x)] = lim
x→x∗f (x) + lim
x→x∗g(x)
(8.1)
and
lim
x→x∗[f (x)g(x)] = [ lim
x→x∗f (x)][ lim
x→x∗g(x)].
(8.2)
provided the limits on the RHS of these equations exist.

9
Vector Valued Functions of
a Scalar Variable
We start with the functions in the ﬁrst of the three categories described above, namely, the
vector valued functions of a scalar variable, denoted f(t).
9.1
Continuity and Differentiation
The derivative of f(t) with respect to the scalar variable t is a new function denoted df(t)
dt
or ˙f(t) and is deﬁned by
˙f(t) = df(t)
dt
= lim
∆t→0
f(t + ∆t) −f(t)
∆t
.
(9.1)
This limit, when evaluated at a particular value t = t0, gives the value of the derivative of
f(t) at t0, that is, the value of ˙f(t0) or df
dt (t0). We say that the function f(t) is differentiable
at t0 if this limit exists at t = t0.
Note that, to be differentiable at t0, f(t) must be continuous at t0, that is,
lim
∆t→0f(t0 + ∆t) = f(t0)
Otherwise, the RHS of Eq. (9.1) will blow up as ∆t →0 because the numerator remains
ﬁnite while the denominator tends to zero.
The derivative ˙f(t) is a function of t in its own right, therefore we can differentiate it
by applying Eq. (9.1) to it, provided the corresponding limit exists. The resulting derivative
function is called the second derivative of f(t) and is denoted ¨f(t) or d2f
dt2 (t). Continuing
in this way we can deﬁne the third and higher order derivatives of f(t).
As an important application, we consider a particle moving along a path which is a
continuous and differentiable curve, that is, the curve is the graph of a continuous and

222
An Introduction to Vectors, Vector Operators and Vector Analysis
differentiable function x(t) of time t, giving the position vector of the particle at time t on
the path. The derivative ˙x = ˙x(t) is called the velocity of the particle, deﬁned by Eq. (9.1),
which we can abbrivate as
˙x = dx
dt = lim
∆t→0
∆x
∆t , which deﬁnes ∆x = x(t + ∆t) −x(t).
Fig. 9.1
Geometry of the derivative
The curve and the vectors involved in the derivative are shown in Fig. 9.1. Note that the
derivative ˙x or the velocity vector is always tangent to the curve. The derivative of the
velocity
¨x = d2x
dt2 = lim
∆t→0
∆˙x
∆t
is called the acceleration of the particle.
Using Eq. (8.2) we easily get, for functions f(t) and g(t),
d
dt (f(t) + g(t)) = df(t)
dt
+ dg(t)
dt
= ˙f(t) + ˙g(t)
(9.2)
and for two scalar valued functions of a scalar variable f (t) and g(t) we get
d
dt (f (t)g(t)) = df (t)
dt
g(t) + f (t)dg(t)
dt
= ˙f (t)g(t) + f (t) ˙g(t)
(9.3)
Using the deﬁnition of the dot product in terms of vector components and Eq. (9.3) we can
write
d
dt (f(t) · g(t)) =
d
dt (fx(t)gx(t) + fx(t)gx(t) + fx(t)gx(t))
= ˙f(t) · g(t) + f(t) · ˙g(t).
(9.4)

Vector Valued Functions of a Scalar Variable
223
In particular, for a particle with velocity v(t) and speed function v(t) = |v(t)| we get
d
dtv2(t) = d
dt (v(t) · v(t)) = 2˙v(t) · v(t).
This equation relates the rate of change of kinetic energy of a particle with its velocity and
acceleration. On the other hand, if the particle is moving along a straight line, so that its
direction ˆv is constant while its speed changes with time, (˙v(t) = ˙v(t)ˆv), then,
d
dtv2 = 2v ˙v = 2v ˙v ˆv · ˆv = 2˙v · v.
We shall now show that a vector valued function v(t) has constant magnitude if and only
if there is a vector ω satisfying
˙v = ω × v
(9.5)
To show that Eq. (9.5) implies constant magnitude for v, we just dot both sides by v to get
v · ω × v on RHS which is zero. This means 2˙v · v = d
dtv2 = 0 or |v| is constant.
To show that constant magnitude of v, that is, 2˙v · v = d
dtv2 = 0 implies the existence
of some ω satisfying Eq. (9.5), we choose ω = (ˆv× ˙v)/v. Using identity I and the fact that
˙v · v = 0 we can easily check that this ω satisﬁes Eq. (9.5).
Exercise
If ˆn is a unit vector function of the scalar variable t, then show that
ˆn × d ˆn
dt
 =

d ˆn
dt
.
Solution
We make use of the fact that the vector of constant magnitude is perpendicular
to its derivative. Thus, ˆn is perpendicular to d ˆn
dt . Therefore, we have,
ˆn × d ˆn
dt
 = |ˆn|

d ˆn
dt
sin π
2 =

d ˆn
dt

since |ˆn| = 1.
□
Exercise
Let u = u(t) be a vector valued function and write u = |u|. Show that
d
dt ( ˆu(t)) = d
dt
u
u

= (u × ˙u) × u
u3
.
Solution
By straightforward differentiation we get
d
dt
u
u

= u ˙u −˙uu
u2
.

224
An Introduction to Vectors, Vector Operators and Vector Analysis
Now consider
(u × ˙u) × u
u3
= u2 ˙u −(u · ˙u)u
u3
= u ˙u −˙uu
u2
where the last equality follows from u · ˙u = 1
2
d
dtu2 = u ˙u.
□
Exercise
Show that the conservation of angular momentum (h) of a particle driven by a
central force, ( ˙h = 0), implies that both the magnitude and the direction of h are conserved
separately. Use this to show that the orbit of the earth around the sun never changes the
direction of its circulation about the sun.
Solution
To prove the ﬁrst part consider ˙h = 0 implies h · ˙h = 0 which implies
d
dt(h · h) = d
dt(h2) = 0, where h = |h|. Thus, the magnitude of h is conserved separately.
Now, h = constant and hˆh = constant together imply ˆh = constant so that the direction
of h is separately conserved.
To get the second part, note that, for constant magnitude of h,
0 ≤|h| = h = mr2 ˙θ,
(9.6)
where r is the distance of the particle from the center of force. Equation (9.6) implies that
˙θ ≥0 always, in a dextral (that is, right handed) frame so θ = θ(t) increases
monotonically with time if h , 0. In a left handed frame ˙θ ≤0. What is important (and
physical) is that ˙θ cannot ever change its sign. This means that the orbit of the earth in the
central force ﬁeld of the sun never changes the direction of its circulation, as the angular
momentum of its orbital motion around the sun is conserved. Note that this result applies
to all central forces.
□
Let us now see the effect of differentiation on the vector product of two functions and
the product of a vector valued and the scalar valued function. Let A(t) and B(t) be two
vector valued functions of a scalar variable t and φ(t) be a scalar valued function of t.
Differentiating (A(t) × B(t))i = εijkAj(t)Bk(t) we get,
d
dt (A(t) × B(t)) = dA
dt × B + A × dB
dt .
(9.7)
Also, by differentiating the product of functions we get,
d
dt (φA) = dφ
dt A + φdA
dt .
(9.8)
We can summarily conclude
• If dA
dt · A = 0 then |A| is constant.
• If A× dA
dt = 0, A , 0, then dA
dt is parallel to A implying that A has constant direction.

Vector Valued Functions of a Scalar Variable
225
9.2
Geometry and Kinematics: Space Curves and Frenet–Seret
Formulae
Frenet–Seret formulae help us connect the geometry of the path of a particle with its
kinematics. We have seen that a path of a particle, which we assume to be a smooth curve
given by a continuous and differentiable function x(t), is parameterized by time t. That is,
evaluation of x(t) at some value of the scalar parameter t, say x(t0) at t = t0, corresponds
to a unique point on the path giving the position of the particle at time t = t0. The vector
valued function x(t) is equivalent to the triplet of scalar valued ‘coordinate functions’
(x(t),y(t),z(t)) which are the components of x(t) with respect to some orthonormal
basis.
For a curve C, the function x(t) ≡(x(t),y(t),z(t)) above, deﬁnes a one to one map of
the t-axis onto the curve, that is, a point on the t-axis is mapped to the unique point x(t) ≡
(x(t),y(t),z(t)) on the curve C. Since the function x(t) ≡(x(t),y(t),z(t)) is assumed
continuous, neighboring points on the t-axis correspond to the neighboring points on the
curve. Since the points on the t-axis are ordered, we can assign an order or the ‘sense’ to
the points of C by saying that the point x(t1) on C precedes point x(t2) on C if t1 < t2.
The parametric representation thus gives a precise meaning to the sense in which a curve
is traversed, using the order of points on a line. This still allows for the possibility x(t1) =
x(t2) on C even if t1 , t2 which just means that the particle was at the same point on the
curve at two different times t1 and t2. This is possible if path is a simple closed curve or has
a loop. A point on the curve at which dx
dt , 0 is called a regular point.
The same path can be parameterized by different parameters, given by different
monotonic functions of t. For example, a circle can be parameterized by angle θ made by
the radius vector with the positive direction of the x axis say θ = ωt where ω is the
angular or rotational velocity of the particle along the circle. Another possible
parameterization is by arc length. This parameter is given by the distance s(t) traversed by
the particle along the path, measured from a ﬁxed point on the path which corresponds to
t = 0. The path then becomes the graph of the function x(s). The value x(s0) at s = s0
simply gives the position vector of the particle at a point on the path, reached by
traversing the path of length s0 from the chosen ﬁxed point on the path. While measuring
s0 the path is traversed in the same sense in which the moving particle traverses the path,
with increasing time, as we saw in the above paragraph. All this is depicted in Fig. 9.2.
Mathematically, we change the parameter from arc length s to time t, in the range s1 ≤
s ≤s2, by means of an analytic function s = s(t) with s1 = s(t1) and s2 = s(t2) such that
ds
dt > 0 in t1 ≤t ≤t2. This ensures that the inverse function t(s) exists and is analytic in
s1 ≤s ≤s2 and that dt
ds > 0 there. This ensures 1-1 correspondence between the values of
s and t in their domains and both parameterizations traverse the curve in the same sense
as they increase through their values. As dx
dt = dx
ds
ds
dt and ds
dt , 0 a regular point for the
parameter s is also the regular point for the parameter t.

226
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 9.2
Parameterization by arc length
Exercise
A circular helix is represented by
x = acostˆi + asintˆj + bt ˆk
−∞< t < +∞,
where ˆk is along the axis of the helix. Provide the equation for the circular helix with (i) z
coordinate and (ii) arc length s as a parameter.
Solution
A circular helix is a curve which winds on a circular cylinder of radius a with its
axis along the z axis. When a point moving along the helix completes one turn, t increases
by 2π; x and y coordinates assume their original values, and z is increased by 2πb. As
dx
dt , 0, for all t, all points of the helix are regular for the parameter t.
Let z be the new parameter and b , 0. Then t = z/b and the equation to the helix
becomes
x = acos z
b
ˆi + asin z
b
ˆj + z ˆk.
Since t is an analytic function of z and dt/dz = 1/b , 0 every point of the helix is a
regular point for the new parameter z.
Now for the parameter arc length s, we know that,
ds =
s dx
dt
!2
+
 dy
dt
!2
+
 dz
dt
!2
dt.
For the circular helix this becomes
ds =
p
a2 + b2dt.

Vector Valued Functions of a Scalar Variable
227
We choose s = 0 at t = 0 and integrate to get,
s = t
p
a2 + b2.
Therefore, in terms of s we get,
x = acos
s
√
a2 + b2
ˆi + asin
s
√
a2 + b2
ˆj + b
s
√
a2 + b2
ˆk.
Since dt/ds , 0, every point on the helix is regular with respect to parameter s.
□
Consider a point x(s0) on the path corresponding to s = s0. Let x + ∆x be the position
vector of a neighboring point corresponding to the parametric value s + ∆s. Since the
curve is smooth and x(s) is differentiable, there is a small enough neighborhood of x(s0)
such that we can take |∆x| = |∆s|, that is, we can take the Euclidean distance between
x(s0) and x(s0 + ∆s) to be the same as the distance traversed along the path between
these points. In the limit,
lim
∆s→0
x(s0 + ∆s) −x(s)
∆s
=
"dx
ds
#
s=s0
then becomes a unit vector tangential to the path at the point x(s0) pointing along the
direction given by the increasing values of s. Denoting this tangential unit vector by ˆt we
can write
ˆt = dx
ds .
Since ˆt is a unit vector we have ˆt · ˆt = 1 which gives
dˆt
ds · ˆt = 0
that is, the vector dˆt
ds is orthogonal to ˆt. This vector measures the amount by which the
direction of ˆt changes as s increases i.e., as the particle moves along the path. We write
dˆt
ds =

dˆt
ds
 ˆn = κ ˆn
(9.9)
where ˆn is the unit vector in the direction of dˆt
ds and κ =
 dˆt
ds
 is the rate of change of
direction of ˆt with s. κ is called the curvature of the path at the point x(s0). ˆn is called the
principal normal unit vector. Note that ˆn is always in the direction of dˆt
ds as κ is chosen to
be non-negative.
The equation κ = 1
ρ deﬁnes the radius of curvature ρ at the corresponding point. A
straight line is a curve with zero curvature and inﬁnite radius of curvature. In this case ˆt
is along the line and ˆn can be in any direction perpendicular to ˆt. The vector X = x + ρ ˆn

228
An Introduction to Vectors, Vector Operators and Vector Analysis
determines C, the center of curvature. The circle with center at C, radius ρ and in the plane
determined by ˆn and ˆt is called the circle of curvature or the osculating circle (see Fig. 9.3).
Fig. 9.3
The Osculating circle
Exercise
Determine the curvature for the circular helix.
Referring to the previous exercise we get for the circular helix,
ˆt =
−a
√
a2 + b2 sin
s
√
a2 + b2
ˆi +
a
√
a2 + b2 cos
s
√
a2 + b2
ˆj +
b
√
a2 + b2
ˆk.
So that
dˆt
ds =
−a
a2 + b2
"
cos
s
√
a2 + b2
ˆi + sin
s
√
a2 + b2
ˆj
#
Hence,
κ =

dˆt
ds
 =
a
a2 + b2
and
ˆn = −cos
s
√
a2 + b2
ˆi −sin
s
√
a2 + b2
ˆj.
Note that the curvature is the same for all points of the helix, while ˆn changes as we go
along the helix.
□
Exercise
Obtain the parameterization of a circle of radius R by arc length. Find the
vectors ˆt and ˆn and hence the curvature and the radius of curvature at a point on the
circle. Show that these quantities are the same for the whole circle.

Vector Valued Functions of a Scalar Variable
229
Solution
The arc length parameterization of a circle of radius R is given by
x(s) ≡

Rcos s
R,Rsin s
R

.
Differentiating with respect to s we immediately get
ˆt ≡

−sin s
R,cos s
R

.
giving |ˆt| = 1. Differentiating again with respect to s gives
dˆt
ds = −1
R

cos s
R,sin s
R

so that the curvature κ = | dˆt
ds| = 1
R and the radius of curvature is R. Since these quantities
depend only on the circle radius R, they are the same for all points of the circle,
characterizing the circle as a whole.
□
Exercise
(a) For a scalar valued function of a scalar variable, y(x), which is continuous and has a
continuous ﬁrst derivative, the curvature κ is deﬁned by dα
ds where s is the arch length
parameter of the graph of y(x) verses x, α(s) is the angle made by the tangent to the
graph at s with the positive direction of the x axis (see Fig. 9.4). Show that
κ =
y′′
(1 + y′2)3/2
(9.10)
where prime denotes differentiation with respect to x.
Fig. 9.4
Curvature of a planar curve

230
An Introduction to Vectors, Vector Operators and Vector Analysis
(b) If the graph of y(x) is the path of a particle parameterized by x(t) ≡(x(t),y(t)),
show that the curvature is given by
κ =
˙x ¨y −˙y ¨x
( ˙x2 + ˙y2)3/2 ,
(9.11)
where ˙x ≡dx
dt etc. When can we have κ = ˙x ¨y −˙y ¨x?
Solution
(a) Since y(x) is continuous and differentiable, the piece of curve traversed by a small
enough increment ds can be approximated by a straight line, in which case we have
(see Fig. 9.4),
ds =
q
dx2 + dy2 = dx
q
1 + y′2.
Further, y′ = tanα or α = arctany′,
−π
2 ≤α ≤π
2 , We have,
dα
ds
= dα
dx
dx
ds
=
d
dx(arctany′)
p
1 + y′2
=
y′′
(1 + y′2)3/2 ·
(9.12)
(b) To get Eq. (9.11) just note that y′ =
˙y
˙x = sinα
cosα = tanα and transform Eq. (9.10).
Note that cosα = ±
˙x
√
˙x2+ ˙y2 and sinα = ±
˙y
√
˙x2+ ˙y2 (where the same sign must be
taken in both the formulas) are the direction cosines of the tangent vector ˙x ≡( ˙x, ˙y)
to the path at (x(t),y(t)). The claim in the next question is satisﬁed if the speed of the
particle along the path is constant and equals unity so that parameters s and t become
identical (because s = vt for constant v) and we have
dx
ds
 =
dx
dt
 =
p
˙x2 + ˙y2 = 1.
Finally, we note that we take ˙x2 + ˙y2 , 0, that is, the tangent always exists at all
points of the path. It is horizontal if ˙y = 0 and vertical if ˙x = 0.
□
Exercise
Find the curvature and the radius of curvature of a circle of radius R using
Eq. (9.11).
□
A third unit vector, orthogonal to both ˆt and ˆn is uniquely deﬁned as
ˆb = ˆt × ˆn
and is called the binormal unit vector.

Vector Valued Functions of a Scalar Variable
231
We see that the triplet {ˆt, ˆn, ˆb} forms a right handed system of orthonormal vectors at each
point of the curve. Since the triplet {ˆt, ˆn, ˆb} changes from point to point on the curve, the
corresponding coordinate system also changes and is called a moving trihedral.
Since ˆt · ˆb = 0, we have,
0 = dˆt
ds · ˆb + ˆt · d ˆb
ds = ˆt · d ˆb
ds
implying ˆt and d ˆb
ds are orthogonal. Since ˆb · ˆb = 1, ˆb · d ˆb
ds = 0. Thus, d ˆb
ds is a vector
perpendicular to both ˆt and ˆb so that the vector d ˆb
ds is along ˆn and measures the rotation
of ˆb in the plane of ˆb and ˆn perpendicular to ˆt, as the particle moves along the curve, or as
s changes. We write
d ˆb
ds = τ ˆn
(9.13)
and call τ the torsion of the curve.
Exercise
Find the binormal vector and the torsion for the circular helix.
Answer
Using the previously obtained expressions for ˆt and ˆn for the helix,
ˆb = b
c sin s
c
ˆi −b
c cos s
c
ˆj + a
c
ˆk
where c =
√
a2 + b2. Further,
τ = −b
c2 .
□
Exercise
A helix is deﬁned to be a curve with non-zero curvature, such that the tangent
at every point makes the same angle with a ﬁxed line in space called the axis. Show that
a necessary and sufﬁcient condition that a curve be a helix is that the ratio of torsion to
curvature is constant.
Solution
We ﬁrst show that the all tangents making the same angle with the axis implies
a constant ratio of κ and τ. This condition can be expressed as
ˆt · ˆe = cosθ = c,
where ˆt is a unit tangent vector to the helix, ˆe is a unit vector along the axis and θ is
the (constant) angle between the tangent and the axis. Differentiating this equation with
respect to s gives
dˆt
ds · ˆe = κ ˆn · ˆe = 0.

232
An Introduction to Vectors, Vector Operators and Vector Analysis
Since κ , 0 we must have ˆn· ˆe = 0. Hence, ˆe is in the plane spanned by ˆt and ˆb and can be
expressed as a linear combination of them. Since ˆt · ˆe = cosθ and ˆe is a unit vector,
ˆe = cosθˆt + sinθ ˆb.
Differentiating with respect to s we get, since the derivatives of ˆt and ˆb are both
proportional to ˆn,
0 = (κcosθ + τ sinθ)ˆn,
or,
κ
τ = −tanθ = constant.
We now assume that
κ
τ = −tanθ = −sinθ
cosθ = constant.
This means we can write
(κcosθ + τ sinθ)ˆn = 0.
Now, we substitute the derivatives of ˆt and ˆb for κ ˆn and τ ˆn respectively and then integrate
with respect to s to get,
cosθˆt + sinθ ˆb = ˆe
where ˆe is the constant of integration. Dotting with ˆt we get ˆt · ˆe = cosθ = constant,
that is, the angle between ˆt and ˆe is constant, or, ˆt is a tangent to an helix and ˆe is along
its axis.
□
Using the relations between the orthonormal triad (ˆt, ˆn, ˆb) and their derivatives with
respect to the arc length parameter s we can show (Exercise) that,
ˆn =
ˆb × ˆt
d ˆn
ds
= d ˆb
ds × ˆt + ˆb × dˆt
ds
= τ ˆn × ˆt + κ ˆb × ˆn
= −τ ˆb −κˆt.
(9.14)
Equations (9.13) and (9.14) constitute Frenet–Seret formulae.

Vector Valued Functions of a Scalar Variable
233
Exercise
Show that we can cast the Frenet–Seret formulae in the form
dˆt
ds = ˆd × ˆt, d ˆn
ds = ˆd × ˆn, d ˆb
ds = ˆb × ˆd
where ˆd = τˆt + κ ˆb is the Darboux vector of the curve.
□
We can express the instantaneous velocity and acceleration of the particle as it moves
along a smooth path in terms of the orthonormal basis (ˆt, ˆn, ˆb). From the deﬁnition of the
parameter s we see that the quantity ds
dt is simply the instantaneous speed v of the particle.
We then have, for the instantaneous velocity of the particle
v = dx
dt = dx
ds · ds
dt = vˆt.
(9.15)
Thus, the direction of the instantaneous velocity is always along the unit tangent vector to
the path in the direction of motion of the particle.
We get the acceleration of the particle by differentiating Eq. (9.15).
a = dv
dt
= dv
dt
ˆt + v dˆt
dt
= d2s
dt2 ˆt + v ds
dt · dˆt
ds
= d2s
dt2 ˆt + v2κ ˆn
= dv
dt
ˆt + v2κ ˆn.
(9.16)
Thus, the acceleration has two components, one given by the rate of change of
instantaneous speed along the direction of motion and the other, with magnitude v2κ,
called centripetal acceleration, along the principal normal. We have thus connected the
kinametical quantities velocity and acceleration of the particle with the local geometry of
its path given by the triad (ˆt, ˆn, ˆb).
Exercise
A kinematical quantity called jerk (denoted j) is deﬁned as the third order
derivative of the position vector with respect to time. Show that,
j ≡d3x
dt3 = −κ2ˆt + dκ
ds ˆn −κτ ˆb
(9.17)

234
An Introduction to Vectors, Vector Operators and Vector Analysis
The acceleration does not involve the torsion of the orbit, but the jerk does. Show further
that,
v · (a × j) = −κτv3
(9.18)
and
|v × a| = v3κ.
(9.19)
These equations can be used to ﬁnd the curvature κ and the torsion τ at any point of the
orbit by using the kinematical values v, a and j at that point.
□
Exercise
Find the curvature and the torsion of the spiralling path of a charged particle in
a uniform magnetic ﬁeld B.
Solution
The Newtonian equation of motion is
mdv
dt = e(v × B)
which implies
v · dv
dt = 0
so that |v| = v0 is a constant. The solution of the equation of motion is
v = v0 + e
m{(x −x0) × B}
where v0 and x0 are the constants of integration. This gives
v · B = v0 · B = v0Bcosθ
where θ is the angle between v0 and B. Taking the vector product of v on both sides of the
equation of motion, we get, using identity I,
v × dv
dt = e
m[(v · B)v −v2B].
Similarly, differentiating the equation of motion once with respect to t we get
j = d2v
dt2 =
 e
m
2
[(v · B)B −B2v].
We can now use Eqs (9.18), (9.19) to get the curvature κ and toesion τ as
κ = e
m
Bsinθ
v0

Vector Valued Functions of a Scalar Variable
235
and
τ = e
m
Bcosθ
v0
·
Exercise
A spaceship of mass m0 moves in the absence of external forces with a constant
velocity v0. To change the motion direction, a jet engine is switched on. It starts ejecting a
gas jet with velocity u which is constant relative to the spaceship and at right angle to the
spaceship motion. The engine is shut down when the mass of the spaceship decreases to m.
Through what angle θ does the direction of the motion of the spaceship deviate due to the
jet engine operation?
Solution
Figure 9.5 shows a possible path of the satellite when the jet engine is on
(the actual path will depend on v0). Since there are no external forces, the equation of
motion is
mdv
dt + u dm
dt ˆn = 0,
Fig. 9.5
A possible path of the satellite
where u = u ˆn is the velocity of the gas jet relative to the satellite and ˆn is the principal
normal. However, we know, via Eq. (9.16), that
dv
dt = v2κ ˆn + dv
dt
ˆt,
where κ is the curvature and s is the length along the path of the satellite (arc length).
Dotting the equation of motion with v and noting that v · ˆn = 0 we get v · dv
dt = dv2
dt = 0
which means that the speed of the satellite as it moves along its path is constant in time.
This follows also from the fact that there are no external forces. Thus, only the centripetal
acceleration survives giving dv
dt = v2κ ˆn. When substituted in the equation of motion it
becomes
mv2κ = −u dm
dt ,

236
An Introduction to Vectors, Vector Operators and Vector Analysis
or,
mv2
R
= −u dm
dt ,
or,
dt = −uR
v2
dm
m .
Here, we have used κ = 1
R where R is the radius of curvature. In order to get the angular
advance of the satellite we transform this equation using vdt = Rdθ (which is justiﬁed
because the path is continuous and differentiable) to get
Rdθ
v
= dt = −uR
v2
dm
m ,
or
dθ = −u
v
dm
m .
Integrating, we get the required angular advance,
θ =
Z
dθ = −u
v
Z m
m0
dm
m = u
v ln
m0
m

.
□
9.2.1
Normal, rectifying and osculating planes
We ﬁx a point on the curve by ﬁxing t at t0 or s at the corresponding value s0, that is, s0 =
s(t0). Let x(t0) = x(s0) = xp be the position vector of this point, say P . The coordinate
planes of the coordinate system given by {ˆt, ˆn, ˆb} at P are
The plane normal to ˆt, spanned by {ˆn, ˆb} called normal plane.
The plane normal to ˆn, spanned by {ˆt, ˆb} called rectifying plane and
The plane normal to ˆb, spanned by {ˆt, ˆn} called osculating plane.
These planes are tangent to the space curve at P . Note that these planes change with
the triad {ˆt, ˆn, ˆb} as the point P moves along the curve or as the parameters t or s change.
Therefore, the position vector of a point on each of these planes has to be labelled by either
t or s. So let x(t) be the position vector of an arbitrary point of each of the planes in turn.
Then, the equation of the normal plane is, suppressing the parameter,
(x −xp) · ˆt = 0,
the equation of the rectifying plane is
(x −xp) · ˆn = 0,

Vector Valued Functions of a Scalar Variable
237
and the equation of the osculating plane is
(x −xp) · ˆb = 0.
Using the deﬁnitions of ˆt and ˆn, we see that ˆb is parallel to x′p × x′′p where prime denotes
the differentiation with respect to s, and this notation will be used subsequently. Thus, the
equation to the osculating plane gets the form
(x −xp) · x′
p × x′′
p = 0.
To get to the t parameterization, note that
x′
p = ˙xp
dt
ds
and
x′′
p = ¨xp
 dt
ds
!2
+ ˙xp
d2t
ds2 .
Exercise
Show that ˙xp × ¨xp is parallel to x′p × x′′p.
□
Hence, the equation for the osculating plane, in terms of t can be written in the form
(x −xp) · ˙xp × ¨xp = 0.
If the curve is a straight line, or a point, ˙xp and ¨xp are parallel, so that equation to
the osculating plane is satisﬁed by every x in space which means that the equation does
not determine the osculating plane. For a straight line, the osculating plane is determined
by the choice of the principal normal ˆn (see the text below the place where we have
deﬁned ˆn).
In Cartesian coordinates x,y,z, the equation to the osculating plane becomes

x −xp
y −yp
z −zp
˙xp
˙yp
˙zp
¨xp
¨yp
¨zp

= 0.
Exercise
Find the equation of the osculating plane to the circular helix.
Answer

x −acost
y −asint
z −bt
−asint
acost
b
cost
sint
0

= 0,

238
An Introduction to Vectors, Vector Operators and Vector Analysis
or,
xbsint −ybcost + az = abt.
□
9.2.2
Order of contact
Consider an osculating plane tangent to a space curve x(s) at a point P with position vector
x(s0). In order to estimate how ‘close’ a space curve is to a tangent plane at a point P , we
make use of the concept of order of contact of a plane and a curve. Higher the order of
contact closer is the plane to the curve. Using this concept we show that the osculating
plane at a point on a space curve is closest to it amongst all the planes tangent to the curve
at the same point.
A plane with a common point P at x(s0) with a space curve x(s) has a contact of order n
at P if the distance of a point x(s) on the curve from the plane is a function δ(s) satisfying
δ(k)(s0) = 0, k = 0,1,...,n,
δ(n+1)(s0) , 0.
where δ(k) is the kth derivative of δ(s) with respect to s.
The distance of a point x(s) on the curve from the osculating plane is
δ(s) = ±[x(s) −x(s0)] · ˆb,
where ˆb = ˆb(s0) is the binormal. We see that
δ(1)(s0) = ±x′(s0) · ˆb = ˆt · ˆb = 0
and
δ(2)(s0) = ±x′′(s0) · ˆb = κ ˆn · ˆb = 0,
since the ﬁrst and the second derivatives of x(s) with respect to s equal ˆt and κ ˆn
respectively and ˆt, ˆn, ˆb form an orthonormal triad. Hence, the osculating plane has
contact of at least order two with the curve.
Now consider a second plane tangent to the curve at P . The distance function δ(s) for
this plane is
δ(s) = ±[x(s) −x(s0)] · ˆc
where ˆc is a unit vector normal to the plane. The ﬁrst two derivatives of δ(s) at P are
δ(1)(s0) = ±x′(s0) · ˆc = ˆt · ˆc

Vector Valued Functions of a Scalar Variable
239
and
δ(2)(s0) = ±x′′(s0) · ˆc = κ ˆn · ˆc.
Therefore, the derivatives are non-zero, unless ˆc is parallel to ˆb making two planes coincide.
Thus, the order of contact of any plane other than the osculating plane is less than two.
Exercise
Find the order of contact of the osculating plane to the circular helix.
Hint
We know that the order of contact is at least two. Using the equation of the helix with
arc length as parameter, show that δ(3) = ±x′′′(s0) · ˆb = ±ab
c4 , 0, where c =
√
a2 + b2.
Therefore, the required order of contact is two.
□
9.2.3
The osculating circle
Let P ,Q,R be three distinct points on a space curve such that the curve has a non-zero
curvature at each of them. Let x(s0),x(s1),x(s2) be the corresponding position vectors
with s0 < s1 < s2. We further assume that the points P ,Q,R also lie on a sphere (x−x0)2 =
a2, x0 being the position vector of the center. We want to ﬁnd what happens to this sphere
in the limiting case as Q and R approach P .
We start by deﬁning the function
f (s) = (x(s) −x0)2 −a2,
where s is the arc length parameter. Note that
f (s0) = f (s1) = f (s2) = 0.
Therefore, by Rolle’s theorem, we get,
f ′(ξ1) = f ′(ξ2) = 0, s0 ≤ξ1 ≤s1 ≤ξ2 ≤s2.
Applying Rolle’s theorem again to f ′(s) we get,
f ′′(ξ3) = 0, ξ1 ≤ξ3 ≤ξ2.
As Q and R approach P , s1,s2,ξ1,ξ2,ξ3 approach s0. Therefore,
f (s0) = (x(s0) −x0)2 −a2 = 0,
f ′(s0) = x′(s0) · (x(s0) −x0) = 0,
f ′′(s0) = x′′(s0) · (x(s0) −x0) + (x′(s0))2 = 0.
Since x′ = ˆt, the second of these equations shows that (x(s0) −x0) lies in the normal
plane at P . Therefore, we can express it as a linear combination of ˆn and ˆb, that is,
x(s0) −x0 = α ˆn + β ˆb.
(9.20)

240
An Introduction to Vectors, Vector Operators and Vector Analysis
Since x′′(s0) = κ ˆn and x′(s0) = ˆt, the third of the above equations gives,
ˆn · (x(s0) −x0) + ρ = 0,
(9.21)
where ρ is the radius of curvature. Dotting Eq. (9.20) with ˆn and then using Eq. (9.21) we
get α = −ρ. Squaring each side of Eq. (9.20) and using f (s0) = 0 (ﬁrst of the above three
equations) we get β = ±
p
a2 −ρ2. Using Eq. (9.20) (with the corresponding expressions
for α and β) we see that, for a > ρ, there are two limiting spheres the position vectors of
the centers of which are given by
x0 = x(s0) + ρ ˆn ±
q
a2 −ρ2 ˆb.
(9.22)
If we select a = ρ then the sphere has its center in the osculating plane. The intersection of
this sphere and the osculating plane is a circle of radius ρ and is called the osculating circle,
or the circle of curvature.
We deﬁne the order of contact between two curves in the same way as we did for a
curve and a plane. It turns out that the order of contact between the osculating circle and
the space curve is at least two.
9.2.4
Natural equations of a space curve
Two space curves are congruent if they can be made to coincide by only translating
and rotating one of them (that is, via a rigid motion). During a rigid motion, both
curvature and torsion at all points on the curve remain unaltered. Thus, the same
curvature and torsion, as functions κ(s) and τ(s) of the arc length parameter s, describe
the whole class of mutually congruent space curves. The values of κ and τ at a point
corresponding to s are given by the values of the functions κ(s) and τ(s). This fact is
expressed by the equations
κ = κ(s) and τ = τ(s),
(9.23)
which are called the natural, or intrinsic equations of a curve. We know that two congruent
curves have the same natural equations. We now show that the reverse implication is also
true: Two curves having the same natural equations are congruent.
Let the two curves be x = x1(s) and x = x2(s). By a rigid motion, we can make the
points corresponding to s = 0 coincide such that the moving trihedrals at these points
coincide. Now using Eqs (9.9), (9.13) and (9.14) it is straightforward to show that
d
ds(ˆt1 · ˆt2 + ˆn1 · ˆn2 + ˆb1 · ˆb2) = 0,
or,
ˆt1 · ˆt2 + ˆn1 · ˆn2 + ˆb1 · ˆb2 = constant.

Vector Valued Functions of a Scalar Variable
241
However, we know that at s = 0
ˆt1 = ˆt2, ˆn1 = ˆn2, ˆb1 = ˆb2.
(9.24)
Therefore,
ˆt1 · ˆt2 + ˆn1 · ˆn2 + ˆb1 · ˆb2 = 3.
(9.25)
Since ˆt, ˆn, ˆb are unit vectors, it follows from Eq. (9.25) that
ˆt1 · ˆt2 = ˆn1 · ˆn2 = ˆb1 · ˆb2 = 1
and that Eq. (9.24) applies for all s; not only at s = 0. From ˆt1 · ˆt2 we get,
x′
1 = x′
2
so that
x1 = x2 + c,
where c is the constant of integration. The initial condition x1(0) = x2(0) gives c = 0.
Therefore, for all s
x1 = x2
which means that both the curves are congruent.
It can also be shown that given two analytic functions κ(s) > 0 and τ(s) there is a curve
for which the curvature and torsion are given by Eq. (9.23). We skip the proof.
We may expand the function x(s) pertaining to the curve in Taylor series (see
section 9.6) around s = 0:
x(s) = x(0) + sx′(0) + s2
2 x′′(0) + s3
3!x′′′(0) + ··· .
Again, expressing the derivatives of x(s) in terms of curvature and torsion via Eqs (9.9),
(9.13) and (9.14), we get,
x(s) = x(0) + sˆt(0) + s2
2 κ(0)ˆn(0) + s3
3!

−κ2(0)ˆt(0) + κ′(0)ˆn(0)
+κ(0)τ(0)ˆb(0)

+ ··· .

242
An Introduction to Vectors, Vector Operators and Vector Analysis
This Taylor series is equivalent to three scalar equations in terms of the components
x1(s),x2(s),x3(s) of x(s) along triad basis ˆt, ˆn, ˆb with origin at s = 0. These are
x1(s) = s −κ2(0)s3
6
+ ···
x2(s) = κ(0)s2
2
+ κ′(0)s3
6
+ ···
x3(s) = κ(0)τ(0)s3
6
+ ···
(9.26)
We can use Eq. (9.26) to get the equations to the projections of the space curve on the
coordinate planes corresponding to the ˆt, ˆn, ˆb basis in the neighborhood of s = 0. Keeping
only the ﬁrst terms in Eq. (9.26), the projections on the osculating plane, the rectifying
plane and the normal plane respectively are given by
x2(s) = κ(0)
2
x2
1(s)
x3(s) = κ(0)τ(0)
6
x3
1(s)
x2
3(s) = 2τ2(0)
9κ(0) x3
2(s)
(9.27)
These projections are depicted in Fig. 9.6.
Fig. 9.6
Projections of a space curve on the coordinate planes of a moving trihedral
Exercise
Find the natural equations for the cycloid, parametrically given by (see the next
section on plane curves)
x = a(t −sint), y = a(1 −cost), z = 0.

Vector Valued Functions of a Scalar Variable
243
Solution
Since the curve is planar, the unit vector ˆb is a constant vector always
perpendicular to the plane of the curve. Therefore, d ˆb/ds = 0, that is, τ = 0. To get the
equation in κ, we have to ﬁnd the arc length measured from some ﬁxed point on the
curve, using the given parametric equations. We have,
ds =
q
dx2 + dy2 = 2asin t
2dt,
giving
s =
Z s
0
ds = 2a
Z t
π
sin t
2dt = −4acos t
2,
where s is measured from the top of the cycloid, that is, s = 0 at t = π. From Eq. (9.19),
the parametric equation of the curve and v = ˙x, a = ¨x we get
κ2 = |[a(1 −cost)ˆi + asintˆj] × [asintˆi + acostˆj]|2
[(a(1 −cost)ˆi + asintˆj)2]3
=
1
8a2(1 −cost)·
Further,
s2 = 16a2 cos2 t
2 = 8a2(1 + cost)
giving us the required equation,
1
κ2 + s2 = 16a2.
□
9.2.5
Evolutes and involutes
We shall now use the Frenet–Seret formulae to learn about an important genera of curves
called evolutes and involutes.
Deﬁnition:
If there is a one to one correspondence between the points of two curves C1
and C2 such that the tangent to C1 at any point on it is normal to C2 at the corresponding
point on C2 then C1 is called an evolute of C2 and C2 an involute of C1.
We denote all the quantities pertaining to the evolute curve C1 by small case letters,
while those pertaining to the involute curve C2 are denoted by the capital letters.
Suppose the equation for the evolute curve C1 is given by x = x(s). We want to ﬁnd out
the equation for its involute C2 given by X = X(S). We refer to Fig. 9.7. If the distance P1P2
is taken to be u, the position vector OP2 will be X = x + uˆt where x = x(s) and ˆt = dx
ds .
Differentiating with respect to S, the arc length parameter of the involute, we get
dX
dS = ˆT = (x′ + uˆt′ + u′ˆt) ds
dS .

244
An Introduction to Vectors, Vector Operators and Vector Analysis
Using Eqs (9.9) and (9.13) this becomes
ˆT = (ˆt + uκ ˆn + u′ˆt) ds
dS .
(9.28)
Fig. 9.7
A construction for ﬁnding the equation of an involute C2 for a given evolute C1
and vice versa
It follows from the deﬁnition of an involute that ˆt · ˆT = 0. Hence,
ˆt · (ˆt + uκ ˆn + u′ˆt) = 0,
or,
1 + u′ = 0
or, integrating,
u = c −s,
where c is the constant of integration. Hence, the equation for the involute is
X = x(s) + (c −s)ˆt
(9.29)
for any given evolute x = x(s). Actually, for each value of c there will be an involute. So for
a given evolute there exists a family of inﬁnite number of involutes. The same is true for a
given involute.
Let X1 and X2 be two points on two involutes for c = c1 and c = c2 in Eq. (9.29)
corresponding to a point P on the evolute curve x(s). Subtracting the equation of one
involute from that of the other, we get
X1 −X2 = (c1 −c2)ˆt
or,
|X1 −X2| = c1 −c2.

Vector Valued Functions of a Scalar Variable
245
Thus, the separation between two such corresponding points is constant.
The simplest realization of involutes to a given evolute is the case of winding strings on
the surface of any object. The open end of the string, if forced to remain stretched during
the process of winding, will describe an involute to the curve on the body traced by the
winding thread. The latter is an evolute as the string touches it tangentially, and the open
end of the string must move in a direction perpendicular to the string itself. Equation (9.29)
suggests that the length of the string u used up in winding is just equal to the increase in arc
length s of the evolute along which the winding takes place, that is, |X −x| + s = c, where
c is the length of the string (see Fig. 9.8). Strings of different lengths generate different
involutes. Thus, the involute of a circle is a spiral. There are families of curves such that
both the evolutes and involutes belong to the same family such as cycloids, hypocycloids
and epicycloids. Such families of self replicating evolute-involute curves are said to form
tesserals [19].
Fig. 9.8
Construction of a evolute-involute pair
Exercise
Find the equation for the involutes of a circular helix.
Solution
For the circular helix, Eq. (9.29) becomes,
X = x + cˆt −t
p
a2 + b2ˆt,
since s =
√
a2 + b2t. We get
ˆt = dx
dt
dt
ds = (−asintˆi + acostˆj + b ˆk)(a2 + b2)−1
2 .
Substituting for the equation of the involute, we get,
X =
 
acost −
ca
√
a2 + b2 sint + at sint
!
ˆi
+
 
asint +
ca
√
a2 + b2 cost −at cost
!
ˆj +
bc
√
a2 + b2
ˆk.
□

246
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Show that the curvature K of the involute of a curve is given by
K2 = κ2 + τ2
κ2(c −s)2 ·
Solution
From Frenet formula
d ˆT
dS = K ˆT.
Equation (9.21) coupled with ˆT · ˆt = 0 tells us that ˆT = ±ˆn. Then
d ˆT
dS = ±d ˆn
ds
ds
dS .
Further, u = c −s coupled with Eq. (9.21) and ˆT · ˆt = 0 gives
ˆT = κ(c −s)ˆn ds
dS ,
from which we get
ˆT · ˆT = 1 = ±κ(c −s) ds
dS .
Therefore,
d ˆT
dS = d ˆn
ds
1
κ(c −s) = −κˆt −τ ˆb
κ(c −s) = K ˆN.
and
K2 = κ2 + τ2
κ2(c −s)2 ·
□
We now solve the reversed problem: Given a space curve, C2, to ﬁnd space curves, denoted
C1, of which the given curve is an involute. We follow the same notational convention as
before: Small case letters for the evolute C1 and the capital letters for the involute C2.
From the deﬁnition of the involute, we know that C2 must be perpendicular to every
tangent to the curve C1 we are seeking. Therefore, ˆt lies in the plane of ˆN and ˆB. From
Eq. (9.29) we see that the targeted curve C1, if it exists, is given by (see Fig. 9.9)
x = X −ut.
However, ˆt is a linear combination of ˆN and ˆB. Therefore,
x = X + α ˆN + β ˆB,
(9.30)

Vector Valued Functions of a Scalar Variable
247
Fig. 9.9
Finding the evolute of an involute
where α and β are to be determined. Differentiating with respect to S we get,
dx
dS = dX
dS + αd ˆN
dS + β d ˆB
dS + ˆNdα
dS + ˆBdβ
dS .
Using Eqs (9.9), (9.13) and (9.14) we get,
dx
dS = (1 −αK) ˆT +
 dα
dS + T β
!
ˆN +
 dβ
dS −T α
!
ˆB,
(9.31)
where K and T are respectively the curvature and the torsion of the involute C2. As
dx
dS = dx
ds
ds
dS = ˆt ds
dS
and as ˆt must be a linear combination of ˆN and ˆB, it follows that the coefﬁcient of ˆT in
Eq. (9.31) must vanish, leading to
α = 1
K .
Thus, α(S) is simply the radius of curvature of the involute at S.
Next convince yourself that X −x is parallel to dx/dS. Hence the coefﬁcients of ˆN and
ˆB in Eqs (9.30) and (9.31) must be in the same ratio. We have,
α
β =
dα
dS + T β
dβ
dS −T α
.
Integration with respect to S yields
tan−1 β
α =
Z
T dS + C,

248
An Introduction to Vectors, Vector Operators and Vector Analysis
or,
β = α
"
tan
 Z
T dS + C
!#
,
where C is a constant of integration. Substituting the values of α and β in Eq. (9.30) we get
the equation of the evolute:
x = X + 1
K
ˆN + 1
K
"
tan
 Z
T dS + C
!#
ˆB.
(9.32)
Note that for a point P of the involute the corresponding points Q1(x1),Q2(x2),... on the
evolutes for different values of C C1,C2,... lie on a straight line parallel to the binormal
ˆB at P because xi −xj, i , j, i,j = 1,2,... is proportional to ˆB. Further, this line is at
a distance of 1
K (radius of curvature at P ) because x has a component along ˆN which is
normal to ˆB and has magnitude 1
K .
Exercise
Obtain the equations for the evolutes of the circular helix.
Hint
Specialize Eq. (9.32) to circular helix. All the required results are available in previous
exercises.
□
9.3
Plane Curves
A separate study of plane curves, that is, curves on a plane, is worthwhile, because many
aspects of the theory can be developed with them without losing generality and many
characteristics of geometric parameters for these curves can be deﬁned, which have no
analogue for curves in three dimensional space. Thus, for example, we can deﬁne the sign
of curvature, κ, for a plane curve, positive and negative sides of a plane curve or the
interior and the exterior regions of a closed plane curve, all of which are not meaningful
for a curve in three dimensional space.
A plane curve is parameterized by a vector valued function x(t) of parameter t where
x(t) ≡(x(t),y(t)) is a planar vector with coordinate functions (x(t),y(t)). We assume
that the functions x(t) ≡(x(t),y(t)) possess continuous derivatives with respect to t.
9.3.1
Three different parameterizations of an ellipse
We give three different ways to parameterize ellipse, ﬁrst by the so called eccentric angle
(called eccentric anomaly by the astronomers), the second by using the angle swept by the
vector based at one of the foci counterclockwise from its pericenter (called true anomaly by
the astronomers) and the third using the time taken by a planet to reach a given point on its
elliptical path around the sun assuming t = 0 at the pericenter. On the way we shall pick up
some geometrical characteristics of ellipse and also some of its physical realizations, most
prominant being planetary motion.

Vector Valued Functions of a Scalar Variable
249
We start with the non-parametric equation to the ellipse, namely,
x2
a2 + y2
b2 = 1
where (x,y) are the coordinates of a point on the ellipse with respect to the coordinate
system based at the center of the ellipse and x,y axes along its major and minor axes
respectively. a and b are the lengths of the semi-major and the semi-minor axes
respectively. Introduce the parameter u by x = acosu to get, via its non-parametric
equation above, y = bsinu. These are the parametric equations to the ellipse in terms of
the eccentric angle u. The position vector of any point P on the ellipse is given by
z = acosu + bsinu
(9.33)
where a and b are vectors along the positive x and y directions with |a| = a and |b| = b so
that a · b = 0. This is depicted in Fig. 9.10.
The above equations to the ellipse tell us that an ellipse is obtained by reducing the
ordinates (y values) of all points on the circumscribing circle (see Fig. 9.10) by the factor b
a.
Thus, the ellipse can be viewed as the projection of a circle placed in an inclined position
with respect to the x −y plane. We see that the area of the ellipse is b
a times that of a circle
with radius equal to its semi-major axis a, that is, A = πab.
Fig. 9.10
Ellipse
Suppose a point performs two harmonic motions in two mutually perpendicular
directions with the same angular velocity ω and with a phase difference of π/2 radians,
with amplitudes a and b (Lissajous motion). Then, x = acosωt and y = bsinωt and the
curve traced by the particle is an ellipse given by Eq. (9.33) with u = ωt.
For a pendulum with small oscillations, the equation of motion is
m¨r + kr = 0

250
An Introduction to Vectors, Vector Operators and Vector Analysis
where r is the position vector of the bob.
Splitting this vector equation into its components we get
m ¨x + kx = 0
;
m ¨y + ky = 0
which have particular solutions
x = acos
r
k
mt
;
y = bsin
r
k
mt
giving rise to the motion along an ellipse given by Eq. (9.33) with u =
q
k
mt.
The two foci of the ellipse are the points situated on the major axis at a distance c =
√
a2 −b2 from the center (see Fig. 9.11). The ratio e = c/a is called the eccentricity of the
ellipse. It is zero if the ellipse degenerates into a circle (a = b). In order to write down the
equation of the ellipse relative to one of the foci as origin, we add or subtract the constant
vector c =
√
a2 −b2ˆa to or from the position vector z given by Eq. (9.33). We get
(acosu ± c)ˆa + sinub =

r1
r2
·
We can easily calculate |r1| = a + ccosu and |r2| = a −ccosu giving us an important
geometric property of ellipse,
|r1| + |r2| = 2a.
(9.34)
Fig. 9.11
Parameters relative to foci
Thus, the sum of the distances from the two foci to any point of the ellipse is constant and
equals 2a. Indeed, the ellipse is popularly deﬁned to be the locus of the points for which the
sum of the distances to two ﬁxed points is constant. This property is used in the so called
gardner’s construction (Fig. 9.12(a)). Attach the ends of a chord of constant length 2a to
two ﬁxed points and draw the curve by keeping the chord stretched by a lead pencil.
For the ends of the minor axis, |r1| = a = |r2|. Figure 9.12(b) illustrates the relation
a2 = b2 + c2. For a point vertically above one of the foci, the coefﬁcient of ˆa is zero. In
this case, cosu = ±c/a = ±e. Consequently, sinu =
√
1 −e2 = b/a and the value of y

Vector Valued Functions of a Scalar Variable
251
is bsinu = b2/a. This value is denoted by p and is called the parameter or the semilatus-
rectum of the ellipse (see Fig. 9.12(b)). (2p is the latus-rectum.) The eccentricity e and the
parameter p are sufﬁcient to ﬁx the shape and size of the ellipse, just as are a and b.
Fig. 9.12
(a) Drawing ellipse with a pencil and a string (b) Semilatus rectum
(c)
Polar coordinates relative to a focus
To get the equation of the ellipse in polar coordinates we identify r = |r1| to get, as derived
before,
r = a + ccosu.
(9.35)
To get the θ coordinate (focal azimuth, see Fig. 3.9(c)) we note that
cosθ = x
r = coeﬃcient of ˆa in r1
r
= acosu + c
a + ccosu ·
(9.36)
Eliminating u from Eqs (9.35) and (9.36) we ﬁnd the polar equation to the ellipse
1
r = 1
p(1 −ecosθ).
(9.37)
Exercise
Extend the position vector r1 of a point P on the ellipse in the opposite direction
to get a chord of the ellipse. Let the chord be divided by the focus in the intercepts r1 and
r2. Show that
1
r1
+ 1
r2
= 2
p·

252
An Introduction to Vectors, Vector Operators and Vector Analysis
Hint
Use Eq. (9.37) for r1 and r2 and note that cos(θ + π) = −cosθ. Thus, each chord
passing through the focus is divided by it into two parts such that their hamonic mean is
constant and equals p.
□
Thus, we have parameterized ellipse using the eccentric angle u with origin at the center
of the ellipse and using the polar coordinate θ with origin at one of the foci. We can
parameterize ellipse by the time of travel of a particle moving on it, tracing it in the sense
of increasing t, taking t = 0 when the particle was at the pericenter. Typical realization of
this situation is the motion of planets along their elliptic orbits around the sun, which sits
at one of the foci and interacts gravitationally with the planet. Actually, we are going to
re-parameterize the elliptical path of the planet by expressing t in terms of the eccentric
anomaly u.
We start with re-writing Eq. (9.33) as
r = z −ea = a(cosu −e) + bsinu,
(9.38)
which is a parametric equation r = r(u) of the elliptic orbit. Now the task is to determine
the parameter u as a function of time u = u(t) so that Eq. (9.38) directly gives the
dependence of r on t, r = r(t). From Eq. (9.38) we get, for the speciﬁc angular
momentum (angular momentum per unit mass),
H = r × ˙r = z × ˙z −ea × ˙z,
or,
Hdt = z × dz −ea × dz.
From Eq. (9.33) we get
dz = −asinudu + bcosudu,
(9.39)
so that using Eqs (9.33) and (9.39) we have
z × dz = a × bdu.
Similarly,
a × dz = a × bcosudu.
Therefore, Hdt becomes
Hdt = a × bdu −ea × bcosudu.
(9.40)

Vector Valued Functions of a Scalar Variable
253
Now we assume that t = 0 and u = 0 at the pericenter and u to be the angular
advance of the planet from the pericenter in time t. Thus, integrating Eq. (9.40) and
remembering that the angular momentum H is a conserved quantity so that it is constant
in time, we get,
H
Z t
0
dt = a × b
Z u
0
du −ea × b
Z u
0
cosudu,
or,
Ht = a × bu −e(a × b)sinu.
(9.41)
Taking moduli on both sides of Eq. (9.41) we get
Ht = uab −esinuab.
(9.42)
We now make use of the fact that H is the arial velocity of the planet, or, HP = 2πab
where P is the period of the orbit. Substituting for H from this equation into Eq. (9.42)
and dividing out by ab, we ﬁnally get the desired equation relating u and t which can be
combined with Eq. (9.38) to get the parameterization of the elliptic orbit in time,
2πt
P
= u −esinu.
(9.43)
This equation is called Kepler’s equation and can be used to obtain the position of the
planet on its orbit at a given time. To make use of this equation, we have to solve it for u
as a function of t. Unfortunately, the equation is transcendental and the solution cannot be
expressed in terms of elementary functions. It can be solved numerically using the method
of successive approximations [2, 17].
9.3.2
Cycloids, epicycloids and trochoids
These are the curves traced out by a point marked on the circumference of a circle which
is rolling without slipping on a straight line or another circle. In the simplest case, a circle
of radius a rolls along the x axis and the path of a point P on its circumference traces out
a cycloid. We assume that at t = 0 the point P is at the origin of a cartesian coordinate
system on the plane. Let us further assume that the circle turns clockwise with unit angular
velocity so that the radius ending at P turns through an angle t in time t. Since the circle
rolls uniformly without sliding the distance traversed by its centre equals the arc length
rolled which equals at so that the coordinates of the centre of the circle at time t are (at,a).
To get the position of the point P at time t we may imagine that its position vector at t = 0
is −aˆj (see Fig. 9.13), rotate it clockwise through angle t and then translate by the vector
atˆi + aˆj. Thus, we have
x(t) = R(−t, ˆn)(−aˆj) + atˆi + aˆj,

254
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 9.13
Cycloid
where ˆn is the vector normal to the plane deﬁning the axis of rotation. Using Eq. (6.45) we
get, for the position vector of P at time t,
a(t −sint)ˆi + a(1 −cost)ˆj.
Writing x(t) = x(t)ˆi + y(t)ˆj and equating the corresponding coefﬁcients we get the
parametric equations for the cycloid,
x(t) = a(t −sint),
y(t) = a(1 −cost).
(9.44)
An epicycloid is deﬁned as the path of a point P on the circumference of a circle of radius
c as it rolls at a uniform speed without slipping, along and outside the circumference of a
second ﬁxed circle of radius a. Let the center of the ﬁxed circle be at the origin of a cartesian
coordinate system on the x−y plane. We assume that the center of the rolling circle rotates
at the uniform angular speed of unit magnitude around the origin, so that the position
vector of its center sweeps an angle t in time t (see Fig. 9.14). Let the position of P at t = 0
(x(0)) be at the point of contact given by the tip of the vector a as in Fig. 9.14. Then, the
position of P at time t (x(t)) is (see Fig. 9.14)
R
a
c t, ˆn

(−R(t, ˆn)c) + R(t, ˆn)(a + c),
where ˆn is the unit vector normal to the plane deﬁning the axis of rotation. Using Eq. (6.45)
to get the effect of rotation operators on vectors, we get, after some algebra,
x(t) = cost(a + c) −cos
a + c
c
t

c + sint ˆn × (a + c)
−sin
a + c
c
t

ˆn × c.

Vector Valued Functions of a Scalar Variable
255
Fig. 9.14
Epicycloid. Vectors are (i) : c, (ii) : a, (iii) : a + c, (iv) : −R(t, ˆn)c, (v)
: R(t, ˆn)(a + c), (vi) : R( a
ct, ˆn)(−R(t, ˆn)c), (vii) : x(t)
Resolving this into components we get the parametric equations for the epicycloid,
x(t) = (a + c)cost −ccos
a + c
c
t

y(t) = (a + c)sint −csin
a + c
c
t

.
(9.45)
When a = c the curve is called a cardioid (Fig. 9.15) and is given by the parametric
equations
x(t) = 2acost −acos(2t),
y(t) = 2asint −asin(2t).
(9.46)

256
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 9.15
Cardioid
A third kind of cycloid is the so called hypocycloid which is obtained exactly like the
epicycloid, except that the rolling circle of radius c is interior to the ﬁxed circle of radius a
(see Fig. 9.16). Assuming that the initial position of the rolling point P is at the tip of the
vector a and proceeding exactly as in the case of epicycloid,
Fig. 9.16
Hypocycloid

Vector Valued Functions of a Scalar Variable
257
we can show (Exercise) the parametric equations for the hypocycloid to be
x(t) = (a −c)cost + ccos
a −c
c
t

,
y(t) = (a −c)sint −csin
a −c
c
t

.
In the special case c = 1
2a we ﬁnd
x(t) = acost,
y(t) = 0
and the hypocycloid degenerates into the diameter of the ﬁxed circle, traced out back and
forth (see Fig. 9.17). It is interesting to note that this example provides a way to draw a
straight line merely by means of circular motions.
For the case c = a/3 the parametric equations for the hypocycloid become
x(t) = 2
3acost + 1
3acos(2t),
y(t) = 2
3asint −1
3asin(2t).
This can be converted to
x2 + y2 = 5
6a2 + 4
9a2 cos(3t),
so that the hypocycloid meets the ﬁxed circle exactly at three points and the corresponding
curve appears in Fig. 9.16.
Fig. 9.17
A point P on the rim of a circle rolling inside a circle of twice the radius
describes a straight line segment

258
An Introduction to Vectors, Vector Operators and Vector Analysis
More general curves called trochoids (epitrochoids, hypotrochoids) are obtained if
we consider the motion of a point P attached to a circle, but not necessarily on its rim,
when that circle rolls along a straight line or along the outside or inside of another circle
(see Fig. 9.18). The same type of curve arises as the path of a point moving uniformly on a
circle while the center of the circle moves uniformly along a line or a circle. For example,
Eq. (9.44) go over to
x(t) = a(t −sint),
y(t) = a(1 −cost) −ccost.
(9.47)
where a is the radius of the circle and a+c is the distance of P from its center. Note that, at
t = 0 the position of P is (0,−c), or the vector c0 in Fig. 9.18. These curves appear as the
brachistochrones and tautochrones inside a gravitating homogeneous sphere [19].
9.3.3
Orientation of curves
We are interested in connected curves in a plane, consisting of one piece (unlike e.g.,
hyperbola which has two distinct branches). A connected curve can intersect itself like the
trochoid in Fig. 9.18. A connected curve without self intersections is called simple. Within
simple curves we can distinguish closed curves, such as circles or ellipses from the curves
such as parabolas or straight line segments.
Fig. 9.18
Trochoid
Suppose a planar curve C with endpoints P0,P1 is parameterized by t 7→x(t) or
equivalently t 7→(x(t),y(t)). Such a curve is called a simple arc if t varies over a ﬁnite
interval on the real line and the mapping t 7→x(t) is one to one and onto, that is,
x(t1) = x(t2) implies t1 = t2. Further, as t increases continuously in the interval
a ≤t ≤b from a to b, suppose the vector x(t) traverses the arc continuously from P0 to
P1. In this case, we say that the traversal of the arc from P0 to P1 is the positive sense of
traversing the arc. The opposite sense of traversing the arc (from P1 to P0) is called
negative sense of traversal. If a new parameterization τ(t) is invoked such that τ increases
monotonically with t, over the interval [a,b], then the positive (or negative) sense of
traversing the arc is preserved under the new parameter τ. If τ decreases monotonically
with increasing t, then the positive sense for t becomes the negative sense for τ and vice
versa.

Vector Valued Functions of a Scalar Variable
259
As an example of a curve with a loop, consider the curve given by the parametric
equations x(t) = t2 −1, y(t) = t3 −t. As t varies from −∞to +∞the curve crosses the
origin twice for t = −1 and t = +1 while x(t) is unique for all other values of t
(Fig. 9.19). The interval −1 < t < +1 corresponds to a loop of the curve. The sense of
increasing t deﬁnes the sense of traversing the curve if we imagine the points
corresponding to t = −1 and t = +1 as distinct, one lying on top of the other.
Fig. 9.19
A curve with a loop
The whole oriented curve can be decomposed into simple arcs, for example, into the
arcs corresponding to n ≤t ≤n + 1 where n runs over all integers. The standard example
of a closed curve is a circle parameterized by x(t) = acost, y(t) = asint, which
physically describes the uniform motion of a particle on a circle of radius a with t as time.
If t varies in any half-open interval α ≤t < α + 2π the point P (x,y) traverses the circle
counterclockwise exactly once. In general, a pair of continuous functions x(t),y(t)
deﬁned in a closed interval a ≤t ≤b represents a closed curve provided x(a) = x(b) and
y(a) = y(b). The closed curve will be simple if (x(t1),y(t1)) = (x(t2),y(t2)) implies
t1 = t2 whenever a ≤t < b.
The positive sense of traversing a closed curve is deﬁned by the ordering of the points
P0P1P2 corresponding to t0 < t1 < t2 respectively (see Fig. 9.20). Note that any cyclic
permutation of the points P0P1P2 does not change the sense of traversing a closed curve.
Fig. 9.20
Positive sense of traversing a closed curve

260
An Introduction to Vectors, Vector Operators and Vector Analysis
Positive and negative sides of a curve
We can distinguish between two sides, positive (or left) side and negative (or right) side of
a oriented plane curve locally as follows.
Consider a ray issuing from a point P on the curve. Then this ray points to the positive
side of the curve if there are points Q on the curve, arbitrarily close to P and following P in
the sense given to the curve such that the angle through which the line from P to Q must
be rotated counterclockwise to reach the given ray, lies between 0 and π (Fig. 9.21). The
points on the ray lying close to P are said to lie on the positive side of the curve.
In the opposite case, the ray is said to point to the negative side of the curve and the
points on it are said to lie on the negative side of the curve.
Fig. 9.21
Positive and negative sides of an oriented arc
If the curve C is a simple closed curve, it divides all points of the plane into two classes,
those interior to C and those exterior to C. We say that C has counterclockwise orientation
if its interior lies on the positive (that is, left) side (Fig. 9.22).
Fig. 9.22
Orientated simple closed curve
If the closed curve C consists of several loops, then it is not always possible to describe C
such that all enclosed regions are on the positive side of C (see Fig. 9.23).

Vector Valued Functions of a Scalar Variable
261
Fig. 9.23
Orientation of a curve with loops
Directions of tangent and normal
The two possible choices of the direction cosines of the tangent, namely,
cosα = ±
˙x
p
˙x2 + ˙y2
and
sinα = ±
˙y
p
˙x2 + ˙y2 ,
(where the same sign must be taken in both the formulas) correspond to directions in which
the tangent can be traversed. The corresponding angles α differ by an odd multiple of π.
One of the two directions correspond to increasing t, while the other one to decreasing t.
Since y′ = ˙y
˙x = sinα
cosα the positive direction of the tangent that corresponds to increasing
values of t is the one that forms with the positive direction of x axis an angle α for which
cosα has the same sign as ˙x and sinα has the same sign as ˙y. The corresponding direction
cosines are given by
cosα =
˙x
p
˙x2 + ˙y2
and
sinα =
˙y
p
˙x2 + ˙y2 .
If ˙x > 0, then the direction of increasing t on the tangent is that of increasing x and the
angle α has a positive cosine. Similarly, the normal direction resulting due to the rotation
of the positive tangent (given by increasing t) in the counterclockwise sense by π
2 has the
unambiguous direction cosines
cos

α + π
2

=
−˙y
p
˙x2 + ˙y2 ,
sin

α + π
2

=
˙x
p
˙x2 + ˙y2 .
It is called positive normal direction and points to the ‘positive side’ of the curve (see
Fig. 9.24).
If we introduce a new parameter τ = χ(t) on the curve, then the values of cosα and
sinα remain unchanged if dτ
dt > 0 and they change sign if dτ
dt < 0 that is, if we change the
sense of the curve, then the positive sense of tangent and normal is likewise changed.

262
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 9.24
Positive direction of the tangent and the normal
Sign of curvature
We know that the curvature of a plane curve is deﬁned by the rate of change of direction
of the tangent to the curve with the arc length parameter s, measured by dα
ds where α is
the angle made by the tangent with the positive direction of the x-axis. Since the absolute
value of the difference between two values of s has a invariant geometric meaning, namely
the distance between two points of the curve measured along the curve, the absolute value
of κ namely |κ| =
dα
ds
 does not depend on the choice of a parameter. However, the sign
of the difference must always be taken to be the same as the sign of the difference of the
corresponding s values.
Since we deﬁned s to be an increasing function of t, the sign of κ depends on the sense
of the curve corresponding to increasing t. Obviously, κ > 0 if α increases with s, that is,
if the tangent to the curve turns counterclockwise as we trace the curve with increasing s
or t. This happens when the curve is convex towards the x-axis and the sense of increasing
s is from left to right, while the tangent turns clockwise, when traced in the same sense of
increasing s, if the curve is concave towards the x-axis. When κ > 0 The orientation of the
curve C is such that the positive side of C is also the inner side of C, that is, the side towards
which C curves (see Fig. 9.25).
Fig. 9.25
(a) A convex function with positive curvature, and (b) a concave function
with negative curvature

Vector Valued Functions of a Scalar Variable
263
Exercise
Find the curvature of the function y = x3 and ﬁnd its sign in the regions x < 0
and x > 0. Check how the tangent turns as x increases in these regions.
□
9.4
Chain Rule
Let f(s) be a vector valued function of a scalar variable s, which, in turn, is a scalar valued
function of another scalar variable t. By substitution, one would then have, f(s(t)) = f(t)
for the corresponding values of t and s. f(s) and f(t) are generally different functions but
their values match for the values t and s(t). This is the reason why the same symbol f
is used to denote both the functions. We assume that both f(s) and s(t) are continuous
and differentiable wherever required. This implies that as ∆t →0, both ∆f = (f(s(t +
∆t)) −f(s(t))) →0 and δs = (s(t + ∆t) −s(t)) →0. Thus, for the compound function
f(s(t)) = f(t) we have,
df
dt = lim
∆t→0
∆f(t)
∆t
= lim
∆t→0
 ∆f(s(t))
∆s(t)
∆s(t)
∆t
!
.
Note that f(s(t)) = f(s) since the corresponding values match. By Eq. (8.2) we can write
df
dt
=
 
lim
∆t→0
∆f(s(t))
∆s(t)
! 
lim
∆t→0
∆s(t)
∆t
!
=
 
lim
∆t→0
∆f(s)
∆s
! 
lim
∆t→0
∆s(t)
∆t
!
=
 df
ds
! ds
dt
!
.
(9.48)
Equation (9.48) gives us a rule for differentiating a compound function called the chain
rule.
9.5
Scalar Integration
The rules of integration of a vector valued function of a scalar variable are similar to those
for integration of a scalar valued function of a scalar variable. These are
Z b
a
f(t)dt = −
Z a
b
f(t)dt
(9.49)
and
Z b
a
[f(t) + g(t)]dt =
Z b
a
f(t)dt +
Z b
a
g(t)dt.
(9.50)

264
An Introduction to Vectors, Vector Operators and Vector Analysis
For a < c < b,
Z b
a
f(t)dt =
Z c
a
f(t)dt +
Z b
c
f(t)dt.
(9.51)
If a is a constant vector independent of t, then,
Z b
a
a · f(t)dt = a ·
"Z b
a
f(t)dt
#
,
Z b
a
a × f(t)dt = a ×
"Z b
a
f(t)dt
#
= −
"Z b
a
f(t)dt
#
× a.
(9.52)
Further, we have the “fundamental formula for the integral calculus” which evaluates the
integral of a derivative.
Z b
a
df(t)
dt dt = f(t)

b
a = f(b) −f(a).
(9.53)
Another fundamental result is the following formula for the derivative of an integral.
d
dt
Z t
a
f(s)ds = f(t),
(9.54)
where s is the dummy variable of integration.
9.6
Taylor Series
Let us assume that a function f(t) possesses derivatives of all orders in some non-empty
interval of values of the real (scalar) variable t. Let s be a scalar variable whose values
are measured from a point t in this interval. Then, the value of the function at t + s can
be evaluated by summing the inﬁnite converging series in powers of s with coefﬁcients
given by the values of derivatives of f evaluated at t. This is called Taylor series for f and is
given by
f(t + s) = f(t) + s˙f(t) + s2
2!
¨f(t) + ···
=
∞
X
k=0
sk
k!
dk
dtk f(t).
(9.55)

Vector Valued Functions of a Scalar Variable
265
Thus, if we know the values of the function and all its derivatives at t then its value at (t+s)
can be obtained as a power series in s. Such a function is said to be analytic at t. A function
analytic at all points in an interval is called analytic in that interval. A function analytic
over its entire domain is called an entire function. Taylor series is very useful in applications
because it can approximate a complicated analytic function by a polynomial obtained after
truncating its Taylor series ensuring the required accuracy. For a given analytic function,
it is always possible to ﬁnd the minimum number of terms in its Taylor series whose sum
will give the value of the function within the required accuracy.
We can use the fundamental formula, Eq. (9.53), to obtain the Taylor expansion of an
analytic function along with the remainder after k terms.
The fundamental formula gives us
I =
Z t+s
t
˙f(v)dv = f(t + s) −f(t).
This integral can be transformed to
I =
Z s
0
˙f(t + s −u)du
via the change of variables v = t +s −u. We can now integrate by parts to get, with ˙f(t) =
df
du
u=t ,
I = u˙f(t + s −u)

s
0 +
Z s
0
u¨f(t + s −u)du = s˙f(t) +
Z s
0
u¨f(t + s −u)du.
Integrating the second term by parts yields
I = s˙f(t) + s2
2!
¨f(t) +
Z s
0
u2
2!
d3
du3 f(t + s −u)du.
Thus, we have obtained
f(t + s) = f(t) + I
= f(t) + s˙f(t) + s2
2!
¨f(t) +
Z s
0
u2
2!
d3
du3 f(t + s −u)du
(9.56)
giving the ﬁrst three terms in the Taylor series, the last integral being the remainder term.
k −1 successive integrations by parts give the ﬁrst k terms of the series with the
corresponding remainder term involving kth derivative of f(t). The remainder term can
be used to estimate the truncation error incurred by truncating the series after k terms.

10
Functions with Vector Arguments
We now deal with the functions of vector arguments. These functions are either scalar
valued or vector valued, with corresponding one to one or many to one maps given by
f : E3 7→R or by f : E3 7→E3. A vector valued function of vector argument is equivalent
to a triplet of scalar valued functions of vector argument given by
f(x) = f1(x)ˆi + f2(x)ˆj + f3(x)ˆk,
(10.1)
where f1,2,3(x) are the scalar valued functions of x given by the components of f(x) with
respect to some orthonormal basis {ˆi,ˆj, ˆk}.
10.1
Need for the Directional Derivative
We have to ﬁrst address the question of differentiating such functions. For a function of
scalar argument, say f (t), the derivative is deﬁned via the difference quotient
(f (t + ∆t) −f (t))/∆t which is the difference between the function values at t and at the
incremented value t + ∆t divided by the increment ∆t. For a function with a vector
argument, say f (x), the increment in the argument, say ∆x, in different directions will, in
general, lead to different values of f (x + ∆x) −f (x). This leads to different derivatives in
different directions which we call directional derivatives. Further, in the absence of an
invertible product like the geometric product between vectors, division by the vector
increment ∆x is not possible.
10.2
Partial Derivatives
The standard way of dealing with this situation is to treat a scalar valued function of vector
argument as a function of three scalar variables, f : R3 7→R. Equation (10.1) can be
used to replace a vector valued function of a vector argument by a triplet of scalar valued
functions of vector argument, each of which can then be treated as the function of three
scalar variables. Thus, in this subsection it will be sufﬁcient to deal with the scalar valued

Functions with Vector Arguments
267
functions of three variables. A given function of three variables f (x,y,z) can be reduced
to a function of a single variable by giving constant ﬁxed values to any two of the variables,
say y and z and treat x as the only variable varying over the allowed domain of x values.
Such a function of a single variable, say x, can then be differentiated by using the standard
deﬁnition of its derivative, assuming that it is a continuous and differentiable function of x.
This derivative is called the partial derivative of f (x,y,z) with respect to x. If we ﬁx z = z0
then f (x,y,z) is reduced to the function f (x,y,z0) = f (x,y) which deﬁnes a surface
in R3. If we now ﬁx y = y0 we get the function f (x,y0,z0) = f (x) whose graph is the
curve giving the intersection of the surface f (x,y) and the plane y = y0. Geometrically,
the partial derivative of f (x,y,z) with respect to x is given by the tangent of the angle
between a parallel line to the x axis and the tangent line to the curve u = f (x,y0,z0). It is,
therefore, slope of the surface u = f (x,y,z0) in the direction of the x axis (see Fig. 10.1).
Thus, the partial derivatives of f (x,y,z) with respect to x,y,z are given by
lim
∆x→0
f (x + ∆x,y,z) −f (x,y,z)
∆x
= ∂f
∂x (x,y,z) = fx(x,y,z)
lim
∆y→0
f (x,y + ∆y,z) −f (x,y,z)
∆y
= ∂f
∂y (x,y,z) = fy(x,y,z)
lim
∆z→0
f (x,y,z + ∆z) −f (x,y,z)
∆z
= ∂f
∂z (x,y,z) = fz(x,y,z).
(10.2)
Fig. 10.1
Sections of u = f (x,y)
where the variables which are not incremented are not varied and are held constant. We
have to be careful while indicating for what values of the independent variables the
derivatives are taken. For example, the x-derivative of f (x,y) = x2 + 2xy + 4y2
evaluated at the point x = 1,y = 2 can be written as

268
An Introduction to Vectors, Vector Operators and Vector Analysis
 ∂f (x,y)
∂x
!
x=1,y=2
= fx(1,2) = (2x + 2y)x=1,y=2 = 6.
We should not write it simply as ∂f (1,2)
∂x
since f (1,2) = 21 is a constant and has 0 as its
x-derivative.
Since the partial derivatives fx,y,z(x,y,z) are the functions of three variables, they can
be again partially differentiated with respect to x,y,z. Assuming that the order of
differentiation does not matter, we get the six derivatives, namely, fxx,fxy,fxz,fyy,fyz,fzz
where fxx = ∂2f
∂x2 = ∂fx
∂x , fxy = ∂2f
∂x∂y =
∂fy
∂x etc.
Exercise
Assuming that the order of differentiation does not matter, how many partial
derivatives of order r of a function of n variables are possible?
Solution
Let r1,r2,...,rn denote the number of occurrences of the variables x1,x2,...,xn
in a rth order possible partial derivative of a function f (x1,x2,...,xn). We must have
r1 + r2 + ··· + rn = r. A general arrangement can be viewed as n stars separated by n −1
bars. For example, the eighth order partial derivative of a six variable function
∂8f
∂x3
1∂x2∂x4
6
corresponds to (r = 8 and n = 6) ∗∗∗| ∗|||| ∗∗∗∗where the string of stars ending at the kth
bar (1 ≤k ≤n−1) gives the order of differentiation with respect to the variable xk and the
string of stars starting after the n−1th bar gives the order of differentiation with respect to
the variable xn. If a pair of bars does not sandwitch any stars, or if the last string of stars
is absent, the differentiation with respect to the corresponding variable is absent. The total
number of distinct distributions is then given by the number of ways of selecting r places
out of n + r −1 places to be ﬁlled by stars and rest of the places are ﬁlled by bars. (there
are n + r −1 stars and bars together). This is given by (n+r−1
r
). Thus, there are (n+r−1
r
)
rth order partial derivatives of a function of n variables. A function of three variables has
ﬁfteen derivatives of fourth order and 21 derivatives of ﬁfth order.
□
In the last section we saw that the existence of the derivative of a function of a single scalar
variable guarantees the continuity of the function. In contrast to this, the existence of the
partial derivatives fx,y,z(x,y,z) does not imply the continuity of f (x,y,z). Thus for
example, the function u(x,y) = 2xy/(x2 + y2),
(x,y) , (0,0) ; u(0,0) = 0 is
continuous as a function of x for any ﬁxed y and is also continuous as a function of y for
any ﬁxed x, so that it has partial derivatives everywhere. However, it is discontinuous at
(0,0) as its value at all points on the line x = y is 1 except at (0,0). However, we have the
following results, which we state here without proof.
If a function f (x,y,z) has partial derivatives fx,fy and fz everywhere in an open set
R and these derivatives everywhere satisfy the inequalities
|fx(x,y,z)| < M,
|fy(x,y,z)| < M,
where M is independent of x,y,z then f (x,y,z) is continuous everywhere in R.

Functions with Vector Arguments
269
Further, if both the partial derivative of order r and the partial derivative obtained by
changing the order of differentiation in any way are continuous in a region R, then both
these derivatives are equal in R, that is, the order of differentiation is immaterial. This
makes the number of partial derivatives of rth order of f (x,y,z) decidedly smaller than
otherwise expected, as we have calculated in the previous exercise.
10.3
Chain Rule
Consider a function u(x,y,z) = u(ξ(x,y,z),η(x,y,z),...) to be a differentiable function
of n variables ξ,η,... each of which is a differentiable function of x,y,z. Let the functions
ξ,η,... have as their common domain the region R in R3. All the n functions ξ,η,...
together map a point in R to a point in the region S in Rn. The function u then maps this
point to a scalar value. Thus, u is a differentiable function of x,y,z ; u : R3 7→R. The
partial derivatives of u with respect to x,y,z are then given by
ux = ∂u
∂ξ
∂ξ
∂x + ∂u
∂η
∂η
∂x + ··· = uξξx + uηηx + ···
uy = ∂u
∂ξ
∂ξ
∂y + ∂u
∂η
∂η
∂y + ··· = uξξy + uηηy + ···
uz = ∂u
∂ξ
∂ξ
∂z + ∂u
∂η
∂η
∂z + ··· = uξξz + uηηz + ···
(10.3)
Replacing x,y,z by x1,x2,x3 and ξ,η,... by ξ1,ξ2,... we can summarize the above
equations by
uxk =
n
X
i=1
∂u
∂ξi
∂ξi
∂xk
k = 1,2,3.
(10.4)
In order to prove Eq. (10.3) all that we need to use is that all functions involved are
differentiable. We have,
ξ(x+∆x,y+∆y,z+∆z)−ξ(x,y,z) = ξ(x+∆x,y+∆y,z+∆z)−ξ(x,y+∆y,z+∆z)
+ξ(x,y + ∆y,z + ∆z) −ξ(x,y,z + ∆z) + ξ(x,y,z + ∆z) −ξ(x,y,z).
If we multiply three terms on RHS by ∆x
∆x, ∆y
∆y and ∆z
∆z respectively, we get an expression
linear in ∆x,∆y,∆z, that is,

270
An Introduction to Vectors, Vector Operators and Vector Analysis
∆ξ = ξ(x + ∆x,y + ∆y,z + ∆z) −ξ(x,y + ∆y,z + ∆z)
∆x
∆x
+ξ(x,y + ∆y,z + ∆z) −ξ(x,y,z + ∆z)
∆y
∆y
+ξ(x,y,z + ∆z) −ξ(x,y,z)
∆z
∆z.
(10.5)
By differentiability of ξ(x,y,z) we mean that replacing the difference quotients on the RHS
of this equation by the respective partial derivatives would give an error
• linear in the Euclidean distance traversed as we go from x,y,z to x +∆x,y +∆y,z +
∆z, that is, the error is given by ϵρ, where ρ =
p
∆x2 + ∆y2 + ∆z2 and
• the error goes to zero faster than ρ →0, that is, ϵ →0 faster than ρ →0.
Thus, we can write, upto ﬁrst order of smallness in ρ, (That is, neglecting the terms of
the second and higher order in ρ in the expression for error if any,)
∆ξ = ξx∆x + ξy∆y + ξz∆z.
This is exactly the same as replacing the distance traversed between two points along a path
by the Euclidean distance between these two points, a procedure we have incurred before.
Similarly, we get,
∆η = ηx∆x + ηy∆y + ηz∆z.
Since u is a differentiable function of ξ,η,... we can again write
∆u = uξ∆ξ + uη∆η + ···
Substituting, the expressions for ∆ξ, ∆η ··· we get ∆u as a result of ∆x, ∆y ∆z as
∆u = (uξξx + uηηx + ···)∆x + (uξξy + uηηy + ···)∆y + (uξξz + uηηz + ···)∆z.
However, considering u as a function of x,y,z we must have
∆u = ux∆x + uy∆y + uz∆z.
Comparing the last two equations for ∆u we get Eq. (10.3), which is called the chain rule
for differentiating a compound function of several variables.
Exercise
Find expressions for all second order derivatives of u.
□
Exercise
Find all partial derivatives of the ﬁrst and the second order with respect to x
and y for the following functions of x,y:

Functions with Vector Arguments
271
(i) u = v logw where v = x2 and w =
1
1+y .
(ii) u = evw, where v = ax and w = cosy.
(iii) u = v tan−1 w where v = xy
x−y and w = x2y + y −x.
(iv) u = g(x2 + y2,ex−y).
(v) u = tan(xtan−1 y).
□
10.4
Directional Derivative and the Grad Operator
As we have seen above, for a function of three variables f (x) ≡f (x,y,z) we can write
upto ﬁrst order in ρ,
∆f = f (x + ∆x) −f (x) = ∂f
∂x ∆x + ∂f
∂y ∆y + ∂f
∂z ∆z = ∇f · ∆x
(10.6)
Equations (10.6), (10.7) deﬁne a new operator, called ‘grad’ or ‘del’ operator which
operates on a scalar valued function f (x) and returns a vector valued function (∇f )(x)
via
(∇f )(x) = ∂f
∂x (x)ˆi + ∂f
∂y (x)ˆj + ∂f
∂z (x)ˆk.
(10.7)
where ˆi,ˆj, ˆk is the orthonormal basis in which coordinates of x are (x,y,z). Note that the
notation ∇(f (x)) is meaningless because f (x) is a number and the del operator does not
act on a number.
In order to be useful, we must show that the deﬁnition of the del operator is invariant
under the change of basis, that is, it is the same for all orthonormal bases, so that the del
operator (∇f )(x) has the same value at x irrespective of the basis used to evaluate it. We
do this by treating the del operator as a vector with coordinates u1 =
∂f
∂x1 , u2 =
∂f
∂x2 ,
u3 = ∂f
∂x3 with respect to the coordinate system corresponding to the basis ˆi,ˆj, ˆk. Let the
coordinates of a vector x in this coordinate system be x1,x2,x3. We know that a new
coordinate system is obtained from the old one by rotating and/or translating it. Hence,
the new coordinates say x′
1,x′
2,x′
3 are related to the old ones by
x′
j =
3
X
k=1
ajkxk + bj
where [ajk] is an orthogonal matrix, whose inverse equals its transpose, and b is the vector
by which the origin of the old system is translated. Due to orthogonality of the
transformation [ajk], the old coordinates can be re-expressed in terms of the new ones as
xk =
3
X
j=1
akj(x′
j −bj), k = 1,2,3.

272
An Introduction to Vectors, Vector Operators and Vector Analysis
Under this coordinate transformation, a function f (x1,x2,x3) will get transformed to
g(x′
1,x′
2,x′
3) so that the operator ∇f will get transformed to ∇f ≡

∂g
∂x′
1 , ∂g
∂x′
2 , ∂g
∂x′
3

. To
evaluate these partial derivatives we note that
g(x′
1,x′
2,x′
3) = f


3
X
k=1
ak1(x′
k −bk),
3
X
k=1
ak2(x′
k −bk),
3
X
k=1
ak3(x′
k −bk)

.
Thus, the coordinates of ∇g with respect to the new coordinate system are given by
vj = ∂g
∂x′
j
=
3
X
k=1
∂f
∂xk
∂xk
∂x′
j
=
3
X
k=1
ajkuk.
where we have used the chain rule. Thus, under the coordinate transformation the operator
∇f transforms like a vector and its components in the transformed system are given by the
partial derivatives of the transformed function with respect to the transformed coordinates.
Given a vector x the vector (∇f )(x) is the same irrespective of the coordinate system used
to evaluate it.
We are now equipped to deﬁne the directional derivative of a function of three variables.
Given a scalar valued function f (x) ≡f (x,y,z), its derivative in a direction ˆa is given by
(ˆa · ∇)f = nx
∂f
∂x + ny
∂f
∂y + nz
∂f
∂z
(10.8)
where (nx,ny,nz) are the direction cosines of ˆa with respect to the basis ˆi,ˆj, ˆk. This is
called the directional derivative of f (x) in the direction ˆa. Henceforth, we shall drop the
parentheses in the expressions for the del operator and the directional derivative, implying
their actions implicitly. Also, we may allow replacing the unit vector ˆa by a general vector a
(with magnitude different than unity) in the deﬁnition of the directional derivative. In that
case the direction cosines in Eq. (10.8) are replaced by the components of a.
Exercise
Let f (x) ≡f (x1,x2,x3) be a differentiable scalar valued function with
{x1,x2,x3} referring to the orthonormal basis {ˆi1,ˆi2,ˆi3}. Show that ∂f
∂xk , k = 1,2,3 are the
directional derivatives along {ˆi1,ˆi2,ˆi3} respectively.
Solution
Notice that ∂f
∂xk = ˆik · ∆f , k = 1,2,3.
□
Exercise
Find the directional derivative of a scalar ﬁeld f (x) along a continuous and
differentiable curve parameterized by time t.
Answer
This is simply the total time derivative of the function f (x(t)) with respect to t
as can be seen from (see section 10.7)
df
dt = ˙x · ∇f (x).

Functions with Vector Arguments
273
The RHS is just the directional derivative of f (x) in the direction of the velocity or the
tangent vector to the curve.
□
The concept of the directional derivative can be quite simply generalized to vector ﬁelds as
follows. The directional derivative of a vector ﬁeld f(x) ≡(f1(x), f2(x), f3(x)) along ˆa is
given by a vector with components (ˆa · ∇f1(x), ˆa · ∇f2(x), ˆa · ∇f3(x)) in the same basis in
which the ﬁeld f(x) is resolved. Since each component of ∇f is invarient under the change
of basis, so is ∇f itself.
Another elegant approach called geometric calculus is developed by D. Hestenes and
collaborators in the context of functions with multivector arguments. This approach can
be adapted to both the scalar as well as vector valued functions of vector arguments, vectors
being a special case of multivectors. This is a coordinate-free approach, where arguments
of functions are treated as vectors as such, without resolving them into components using
a particular basis. The increment ∆x in the vector argument x is decomposed as aτ where
the vector a gives the direction and τ is a scalar variable. The directional derivative is then
deﬁned as1
a · ∇f (x) = lim
τ→0
f (x + aτ) −f (x)
τ
.
(10.9)
Note that this deﬁnition is meaningful even if the function f is vector valued, because
the limit deﬁning it is meaningful in that case. We will show below, for a scalar valued
function, that the deﬁnitions of the directional derivative given by Eqs (10.8) and (10.9) are
equivalent. Thus, the LHS of Eq. (10.9) can be viewed as the dot product of a and ∇f (x).
In this section, unless stated otherwise, the same symbol f will represent both the scalar
and vector valued function and the corresponding result applies in both the cases.
We now obtain some basic results regarding the directional derivative. Consider
(a + b) · ∇f (x) = lim
τ→0
f (x + aτ + bτ) −f (x)
τ
= lim
τ→0
"f (x + aτ + bτ) −f (x + aτ)
τ
+ f (x + aτ) −f (x)
τ
#
= a · ∇f (x) + b · ∇f (x).
(10.10)
Similarly, for a scalar constant c,
(ca) · ∇f (x) = c lim
cτ→0
f (x + cτa) −f (x)
cτ
= c(a · ∇f (x)).
(10.11)
Exercise
Let f (x) and g(x) be two functions of vector argument x.
1See also ref [21]

274
An Introduction to Vectors, Vector Operators and Vector Analysis
(i) Show that
a · ∇(f + g) = a · ∇f + a · ∇g.
(10.12)
(ii) Assuming either f or g or both to be scalar valued, or, by replacing the product f g
by the dot product f · g if both are vector valued, show that
a · ∇(f g) = (a · ∇f )g + f (a · ∇g).
(10.13)
We refer to this as the “product rule”. It is trivial to check that a·∇cf (x) = ca·∇f (x). This
equation and Eq. (10.12) together show that the directional derivative is a linear operator.
Now let f be a scalar valued function of a scalar argument and let λ(x) be a scalar
valued function of a vector argument. Then, the directional derivative of the compound
function f (λ(x)) is
a · ∇f
= lim
τ→0
"f (λ(x + aτ)) −f (λ(x))
λ(x + aτ) −λ(x)
λ(x + aτ) −λ(x)
τ
#
=
"
lim
∆λ→0
f (λ + ∆λ) −f (λ)
∆λ
#"
lim
τ→0
λ(x + aτ) −λ(x)
τ
#
= (a · ∇λ)df
dλ.
(10.14)
This is the chain rule for the directional derivative of a compound function.
The directional derivative of the vector valued and the scalar valued constant functions
f(x) = b and f (x) = c are trivially zero, as seen from its deﬁnition. We have
a · ∇b = 0 = a · ∇c.
It follows directly from the deﬁnition of the directional derivative that the directional
derivative of the identity function I(x) = x is
a · ∇x = a.
We can use general rules Eqs (10.12) and (10.13) to ﬁnd the derivatives of more
complicated functions. The derivatives of algebraic functions of x can be obtained in this
way. For example, we note that the “magnitude function” |x| is related to x by the algebraic
equation |x|2 = x · x. Using Eq. (10.13) we can write
a · ∇(x · x) = (a · ∇x) · x + x · (a · ∇x) = a · x + x · a = 2a · x.
(10.15)
If we apply the chain rule (Eq. (10.14)) we get
a · ∇|x|2 = 2|x|a · ∇|x|.

Functions with Vector Arguments
275
Equating the RHS of both these equations we get
a · ∇|x| = a · x
|x| = a · ˆx.
(10.16)
Next, we ﬁnd the derivative of the “direction function” ˆx. We use the product rule
(Eq. (10.13)) and the chain rule (Eq. (10.14)) as follows.
a · ∇
 x
|x|

= a · ∇x
|x|
−a · ∇|x|
|x|2
x = a
|x| −(a · ˆx)x
|x|2
.
Hence,
a · ∇ˆx = a −(a · ˆx)ˆx
|x|
.
(10.17)
Exercise
Find the derivatives
(a) a · ∇(x × b) Where b is a constant vector independent of x.
Answer
a × b. Follows from the deﬁnition of the directional derivative.
(b) a · ∇(x × (x × b)).
Answer
(a · b)x + (x · b)a −2(a · x)b.
Hint
Use identity I and the product rule.
□
Exercise
Let r = r(x) = x−x′, r = |r| = |x−x′| where x′ is independent of x. Show that
(a) a · ∇r = a · ˆr.
(b) a · ∇ˆr = a −(a · ˆr)ˆr
r
.
(c) a · ∇(ˆr · a) = a2 −(a · ˆr)2
r
.
(d) a · ∇(ˆr × a) = (a · ˆr)(a × ˆr)
r
.
(e) a · ∇|ˆr × a|2 = −(ˆr · a)|ˆr × a|
r
.
(f) a · ∇ˆr
r = a −2(a · ˆr)ˆr
r2
.
(g) a · ∇1
r2 = −2a · ˆr
r3 .
(h) 1
2(a · ∇)2 1
r2 = 3(a · ˆr)2 −|ˆr × a|2
r4
.
(i) 1
6(a · ∇)3 1
r2 = 4a · ˆr(a · ˆr)2 + |ˆr × a|2
r5
.

276
An Introduction to Vectors, Vector Operators and Vector Analysis
(j) a · ∇logr = a · ˆr
r .
(k) a · ∇r2k = 2k(a · r)r2(k−1).
(l) a · ∇r2k+1 = r2k(a + 2k(a · ˆr)ˆr).
In the last two cases k , 0 is an integer and r , 0 if k < 0.
□
It is quite easy to see that the deﬁnition of the directional derivative in Eq. (10.8) follows
from that in Eq. (10.9). Choosing an orthonormal basis {ˆe1, ˆe2, ˆe3} and denoting the
components of x with respect to this basis by x1,x2,x3 we get
ˆa · ∇f (x) = (n1ˆe1 + n2ˆe2 + n3ˆe3) · ∇f (x)
= n1ˆe1 · ∇f (x) + n2ˆe2 · ∇f (x) + n3ˆe3 · ∇f (x)
= n1
∂f
∂x1
+ n2
∂f
∂x2
+ n3
∂f
∂x3
.
(10.18)
The second equality follows from Eq. (10.10) and the last equality follows because the
directional derivative (Eq. (10.9)) along the direction of one of the basis vectors reduces to
the corresponding partial derivative (Eq. (10.8)).
We note that the action of the ‘del’ operator on a scalar valued function f (x) is the
same as that of the linear operator
∇≡
3
X
j=1
ˆej
∂
∂xj
= ˆe1
∂
∂x1
+ ˆe2
∂
∂x2
+ ˆe3
∂
∂x3
.
(10.19)
Using the linearity of the directional derivative and Eq. (10.18) we can express the
directional derivative of a vector valued function in terms of the partial derivatives of its
component functions in an orthonormal basis (ˆe1, ˆe2, ˆe3). We have,
ˆa · ∇f(x) = ˆa · ∇(f1(x)ˆe1 + f2(x)ˆe2 + f3(x)ˆe3)
= ˆa · ∇f1(x)ˆe1 + ˆa · ∇f2(x)ˆe2 + ˆa · ∇f3(x)ˆe3
=
 
n1
∂f1
∂x1
+ n2
∂f1
∂x2
+ n3
∂f1
∂x3
!
ˆe1 +
 
n1
∂f2
∂x1
+ n2
∂f2
∂x2
+ n3
∂f2
∂x3
!
ˆe2
+
 
n1
∂f3
∂x1
+ n2
∂f3
∂x2
+ n3
∂f3
∂x3
!
ˆe3.
(10.20)

Functions with Vector Arguments
277
We can replace the unit vector ˆa by a vector a of arbitrary (usually small) magnitude in the
same direction, without losing generality. In this case the components of the unit vector ˆa,
namely, the direction cosines n1,n2,n3 are replaced by the components of a that is, by
a1,a2,a3. The components of the directional derivative of the vector valued function f(x),
usually called a vector ﬁeld, are completely speciﬁed by the matrix product


∂f1
∂x1
∂f1
∂x2
∂f1
∂x3
∂f2
∂x1
∂f2
∂x2
∂f2
∂x3
∂f3
∂x1
∂f3
∂x2
∂f3
∂x3




a1
a2
a3


=


a · ∇f1
a · ∇f2
a · ∇f3


·
The linear map deﬁned by the matrix
J = d(f1,f2,f3)
d(x1,x2,x3) =


∂f1
∂x1
∂f1
∂x2
∂f1
∂x3
∂f2
∂x1
∂f2
∂x2
∂f2
∂x3
∂f3
∂x1
∂f3
∂x2
∂f3
∂x3


is called the Jacobian matrix of the differentiable map x 7→f(x). When evaluated at a
particular value of x, we get a matrix whose elements are numbers, called Jacobian matrix
at x, denoted J(x). The Jacobian matrix plays the role of the derivative of the vector valued
function of a vector variable, because it gives a linear approximation to f at x just as the
derivative of a function of a single variable. Extending this analogy further, we call the
linear map deﬁned by the Jacobian J(x) to be the map tangent to f at x. The Jacobian
matrix can be generalized to a differentiable map f : Rn 7→Rm deﬁning the derivative of
such a map. When m = n (in our case m = n = 3) we can evaluate the determinant of the
Jacobian J(x) which is called the Jacobian determinant of f(x) at x, denoted |J(x)|.
Fig. 10.2
Mapping polar to cartesian coordinates

278
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Let F : R2 7→R2 be the map deﬁned by
F(r,θ) = (r cosθ,r sinθ) r ≥0
In other words, the polar coordinates map (r,θ) 7→(x,y) as
x = r cosθ, y = r sinθ
which maps a rectangle into a circular sector (see Fig. 10.2). Find the Jacobian matrix and
the Jacobian determinant of this mapping.
Find all points (r,θ) where the Jacobian
determinant vanishes.
□
10.5
Taylor Series
We can extend the Taylor series approximation to the functions of vector argument. This
enables us to approximate the arbitrary functions of vectors (mostly position vectors) by
simpler functions. Such a Taylor series involves directional derivatives, and applies to
functions for which directional derivatives (or, equivalently, partial derivatives) of all
orders (see below) exist. The basic idea is to use the Taylor series expansion of the
function with scalar argument. Given a scalar or vector valued function f (x) to be
approximated, we invoke a new function of scalar argument
G(τ) = f (x + aτ).
Using this deﬁnition of G(τ) and the deﬁnition of the directional derivative (Eq. (10.9)) it
is clear that
dG(0)
dτ
= a · ∇f (x),
d2G(0)
dτ2
= a · ∇(a · ∇f (x)) ≡(a · ∇)2f (x),
dkG(0)
dτk
= a · ∇((a · ∇)k−1f (x)) = (a · ∇)kf (x).
(10.21)
We now expand the function with scalar argument G(τ) in Taylor series about τ = 0 and
evaluate it at τ = 1. We get
G(1) = G(0) + dG(0)
dτ
+ 1
2
d2G(0)
dτ2
+ ··· =
∞
X
k=0
1
k!
dkG(0)
dτk
.

Functions with Vector Arguments
279
Using Eq. (10.21) we can express this Taylor series in terms of f (x). This gives the desired
Taylor expansion
f (x + a) = f (x) + a · ∇f (x) + (a · ∇)2
2!
f (x) + ···
=
∞
X
k=0
(a · ∇)k
k!
f (x) ≡ea·∇f (x),
(10.22)
where the last equivalence deﬁnes the operator ea·∇.
If f is a vector valued function f(x) we can resolve it in terms of its component functions
with respect to some orthonormal basis and use Eq. (10.20) iteratively to get the Taylor
expansion
f(x + a) =


f1(x)
f2(x)
f3(x)


+


a · ∇f1(x)
a · ∇f2(x)
a · ∇f3(x)


+ 1
2!


(a · ∇)2f1(x)
(a · ∇)2f2(x)
(a · ∇)2f3(x)


+ ···
Each element in the second term is simply the dot product of a and ∇fk(x), k = 1,2,3.
The second term is given by the product of the Jacobian matrix and the vector
a ↔[a1a2a3]T .
10.6
The Differential
The general form of the remainder after two terms in the Taylor series of a scalar valued
function f (x) can be found by the following argument. We say that f (x) is differentiable
at x if all its partial derivatives exist at x, or, the vector ∇f (x) is well deﬁned. Since we
cannot divide by a vector increment ∆x we can re-write the Newtonian quotient as
df = f (x + ∆x) −f (x) = ∆x · ∇f (x) + |∆x|g(∆x)
(10.23)
such that g(∆x) →0 as |∆x| →0. Note that the ﬁrst term on RHS is simply the
directional derivative in the direction of ∆x. This equation implies that, for any required
accuracy, we can choose |a| small enough so as to make the ﬁrst two terms of the Taylor
series in Eq. (10.22) give the value of f (x + a) within the required accuracy. In other
words, the remainder term in the series obtained after truncating it after the second term,
namely the last term in the equation
f (x + a) = f (x) + a · ∇f (x) + |a|g(a)
(10.24)

280
An Introduction to Vectors, Vector Operators and Vector Analysis
can be chosen as small as we please by choosing |a| small enough.2 We call the directional
derivative appearing in this equation, namely, a · ∇f (x), the differential of the function
f (x). For a scalar valued function, f (x), Eq. (10.23) is the equation for a line in 3-D in
the range x and x + dx. Thus, we see that, for any scalar valued differentiable function, the
differential provides a linear approximation to that function in a small enough range of its
argument. For a vector valued function f(x) Eq. (10.24) now becomes a vector equation,
with f replaced by
f(x + a) =


f1(x)
f2(x)
f1(x)


+


a · ∇f1(x)
a · ∇f2(x)
a · ∇f3(x)


+ |a|


g1(a)
g1(a)
g1(a)


,
where the scalar valued functions fk(x), k = 1,2,3 are the components of f(x) with
respect to some orthonormal basis. The term in the middle, involving the gradients, is
precisely equal to the product of the Jacobian matrix times a ≡[a1a2a3]T . Since
lim|a|→0 gk(a) = 0; k = 1,2,3 we can make the linear approximation to f(x) at x by the
Jacobian matrix J(x) as accurate as we please by making |a| small enough.
To appreciate the importance of the differential, (which is the same as the directional
derivative of a scalar valued or vector valued function f (x) in the direction of ﬁxed vector
(a), we view it as a function of a for ﬁxed x, say F(a). We have already shown that the
differential is a linear function of a (see Eqs (10.10),(10.11)). Expanding the Taylor series
about the point x0 and putting ∆x = x −x0 we have, for small enough |∆x|,
f (x0 + ∆x) = f (x) = f (x0) + ∆x · ∇f (x0) = (x −x0) · ∇f (x0).
(10.25)
Note that the vector x −x0 in the differential (x −x0) · ∇f (x0) plays the role of vector a in
a · ∇f (x) which is a linear function of a. Therefore, using this linearity and Eq. (10.25) we
get, to the ﬁrst order in ∆x
f (x)−f (x0) = (x−x0)·∇f (x0) = x·∇f (x0)−x0·∇f (x0) = F(x)−F(x0). (10.26)
If we couple linearity of F(x) with Eq. (10.26), we see that the differential provides a linear
approximation to any differentiable function. Since linear functions are simple enough to
be analyzed completely, Eq. (10.26) establishes the importance of the differential. Note
that Eqs (10.25) and (10.26) apply to both the scalar valued as well as the vector valued
function f (x).
2Compare with
f (a + h) = f (a) + df
dx
x=ah + |h|g(h)
where
lim
h→0g(h) = 0.

Functions with Vector Arguments
281
10.7
Variation on a Curve
We are often interested in the variation of a function f (x) along the path of a moving
particle parametrically given by x = x(t). Let us ﬁrst assume that f (x) is scalar valued.
We know that the general variation of such a function is given by
f (x + ∆x) = f (x) + ∆x · ∇f (x).
However, both x and ∆x are now not arbitrary, but x must satisfy x = x(t) and ∆x must
join the point x(t) and a neighboring point on the path given by x(t + ∆t), that is,
∆x = x(t + ∆t) −x(t). Therefore, the variation of f (x) along the curve is given by
f (x(t + ∆t)) = f (x(t)) + (x(t + ∆t) −x(t)) · ∇f (x(t)).
Now we subtract f (x(t)) from both sides, divide by ∆t on both sides and take the limit as
∆t →0 on both sides to get the desired result,
df
dt (x(t)) = ˙x(t) · ∇f (x(t)) =
3
X
i=1
˙xi(t) ∂f
∂xi
,
(10.27)
where ˙xi i = 1,2,3 and xi i = 1,2,3 are the components of ˙x(t) and x(t) respectively,
with respect to some orthonormal basis. Thus, the time rate of change of a function of the
position vector of a particle, as it moves along its path, is given by the directional derivative
of this function along the direction of the velocity vector, which is tangent to the path in
the same sense as traversed by the particle.
Exercise
The Lagrangian of a system with n degrees of freedom is a function of 2n + 1
variables,
namely,
L (q(t); ˙q(t);t) where q(t)
≡
q1(t),q2(t),...,qn(t) are the
generalized coordinates and ˙q(t) ≡˙q1(t), ˙q2(t),..., ˙qn(t) are the generalized velocities.
The motion is viewed as the path traced by a point in the conﬁguration space spanned by
the n generalized coordinates. Similarly, the Hamiltonian of such a system is given as a
function of 2n + 1 coordinates, namely, H (q(t);p(t);t) where p(t) ≡p1,p2,...,pn are
the generalized momenta. The motion is viewed as that of a point in phase space spanned
by n generalized coordinates and n generalized momenta. Find the expressions for dL
dt
and dH
dt .
Answer
dL
dt =
n
X
i=1
∂L
∂qi
˙qi +
n
X
i=1
∂L
∂˙qi
¨qi + ∂L
∂t ,
where
¨qi = d
dt ˙qi.
dH
dt =
n
X
i=1
∂H
∂qi
˙qi(p(t)) +
n
X
i=1
∂H
∂pi
˙pi + ∂H
∂t ,

282
An Introduction to Vectors, Vector Operators and Vector Analysis
where the generalized velocities are the functions of generalized momentum vector
p(t).
□
If the function f(x(t)) is vector valued, Eq. (10.27) can be expressed invoking the Jacobian.
We can write
df
dt (x(t)) =


˙x(t) · ∇f1(x(t))
˙x(t) · ∇f2(x(t))
˙x(t) · ∇f3(x(t))


,
where fi(x(t)) i = 1,2,3 are the components of the function f(x(t)). The RHS of this
equation is simply the product of the Jacobian matrix of the function f(x(t)) and the
column matrix comprising the components of the vector ˙x(t). Thus, we have found the
Chain rule for differentiating the composite function f (x(t)) or f(x(t)).
10.8
Gradient of a Potential
If a vector ﬁeld f(x) is the differential of some scalar ﬁeld, that is, a · f(x) = a · ∇φ(x) for
some scalar ﬁeld φ(x), then we say that f is the gradient of φ and write
f = ∇φ.
φ is called the potential of f. We know that the differential of a function φ(x) is its
directional derivative in the direction of a. The directional derivative is simply the rate at
which the value of φ changes in the direction of a. If we choose ˆa to be unit vector giving
the chosen direction, then ˆa · ∇φ has its maximum value when ˆa and ∇φ are in the same
direction, that is, ˆa · ∇φ = |∇φ|. Thus, the gradient ∇φ(x) speciﬁes both the direction as
well as the magnitude of the maximum change in the value of φ(x) at any point x in the
domain of φ. In general, the change in the values of φ, in any given direction ˆa, based at a
point x, is given by the scalar product of ˆa with ∇φ(x).
Fig. 10.3
The gradient vector is orthogonal to the equipotential at every point
It is interesting to ﬁnd ∇φ(x) at a point x on a surface deﬁned by the equation φ(x) =
k, that is, the surface, at each point of which φ has constant value k. Such a surface is

Functions with Vector Arguments
283
called an equipotential surface. Actually, the equation φ(x) = k deﬁnes a one-parameter
family of equipotential surfaces, one surface for each constant value of k (see Fig. 10.3).
At any point x on an equipotential surface the vector ∇φ(x) cannot have a component
tangential to the surface because a non-zero gradient tangential to the surface would mean
that φ(x) changes as x changes along the surface, contradicting the equipotential nature of
the surface. Thus, at every point x on an equipotential surface the vector ∇φ(x) must be
normal to the surface through that point. Further, it is directed towards the surfaces with
larger values of k, because the sign of the directional derivative is the same as that of the
difference in φ values and in the present case, the sign of the directional derivative is the
same as that of ∇φ(x). Figure 10.3 shows only a two dimensional cross section. In this
ﬁgure, the change in k is the same for each pair of neighboring surfaces, so the separation
provides a measure of the change in φ. The closer the surfaces, larger the gradient.
Given a scalar valued function φ(x) its gradient at x is easily found by evaluating the
corresponding directional derivative a · ∇φ(x) which is simply the scalar product of ∇φ
with an arbitrary vector a. Thus, for φ(x) = x · b where b is a constant vector, we get
a · ∇(x · b) = a · b which follows from a · ∇x = a. Hence,
∇x · b = b.
Similarly, from Eqs (10.15) and (10.16) we get
∇x2 = 2x,
∇|x| = ˆx.
(10.28)
These formulas enable us to determine the gradients of certain functions without referring
to the directional derivative at all. Thus, if f (|x|) is a function of the magnitude of x alone,
then, by using the second of Eq. (10.28) while applying the chain rule Eq. (10.14), we get
∇f = ˆx ∂f
∂|x|·
(10.29)
Later we will meet potential functions in connection with the line integrals over vector
ﬁelds derivable from a potential.
10.9
Inverse Maps and Implicit Functions
We have already seen the conditions for the map f : E3 7→E3 to be invertible
(subsection 4.1.2). Here, we are interested in a class of maps (or functions) which are
differentiable, called C1 functions, so we deﬁne the inverse of a function over the set of C1
functions. Further, by E3 we mean E3 or R3. Let U denote some open set in E3. A map
f : U 7→E3 is called a C1-map on U if ˆa · ∇f and (ˆa · ∇)2f exist at all x ∈U and for all ˆa.
This is equivalent to saying that all partial derivatives of all the component functions
f1(x),f2(x),f3(x) of f(x) exist and are continuous at all x ∈U. A C1-map f : U 7→E3 is

284
An Introduction to Vectors, Vector Operators and Vector Analysis
said to be C1-invertible if the image set f(U) is an open set V and if there exists a C1-map
g : V 7→U such that g ◦f and f ◦g are the respective identity maps on U and V . For
example, if f : E3 7→E3 is given by f(x) = x + b where b is a ﬁxed vector, then f is
C1-invertible, its inverse being the translation by −b.
Exercise
Let U be the subset of R2 consisting of all pairs (r,θ) with r > 0 and 0 < θ < π.
Let
f(rˆi + θˆj) = r cosθˆi + r sinθˆj,
(10.30)
with x = r cosθ and y = r sinθ. Show that this is a C1-map and ﬁnd the image set f(U).
Show that the inverse map is given by
g(xˆi + yˆj) =
q
x2 + y2ˆi + cos−1
x
p
x2 + y2
ˆj,
with r =
p
x2 + y2 and θ = cos−1
x
√
x2+y2 .
Answer
The image of U is the upper half plane consisting of all (x,y) such that y > 0,
and arbitrary x. Inverse can be checked explicitly.
□
In many cases a map may not be invertible over the whole space or over arbitrary subsets
of it, but can still be C1-invertible locally in the following sense. Let a point x ∈U. We say
that a map f is locally C1-invertible at x if there exists an open set U1 satisfying x ∈U1 ⊂U
such that f is C1-invertible on U1.
Exercise
Show that the map given by Eq. (10.30) is not C1-invertible on all of R2, but
given any point, it is locally invertible at that point.
Hint
If we take r < 0, the inverse map given in the previous exercise does not work.
However, we can locally invert by choosing r = −
p
x2 + y2 in the inverse map at a point
with r < 0.
□
In most cases the locally invertible map cannot be expressed in closed form. However, there
is a very important result which gives computable criterion for local invertibility of a map.
10.9.1
Inverse mapping theorem
Let U be an open set in E3 and f : U 7→E3 be a C1 map. Let x be a point in U. If the
Jacobian determinant |jf(x)| , 0 then f is locally C1-invertible at x.
We do not give a formal proof of this theorem which is quite involved. However, we
note that the Jacobian matrix corresponding to a vector valued map f(x) plays the role of
its derivative at x. The Jacobian matrix itself is a linear map a 7→a·∇f giving the directional
derivative of f along a. The determinant |Jf(x)| , 0 means that the the Jacobian matrix is
invertible at x. Thus, the inverse mapping theorem states that the map f is locally invertible
at x if the linear map deﬁning its derivative, namely, its Jacobian matrix is invertible at x.

Functions with Vector Arguments
285
Exercise
Let F : R2 7→R2 be given by F(x,y) = (ex cosy,ex siny). Show that F is
locally invertible at every point.
Answer
|JF(x,y)| = ex , 0 for all (x,y) ∈R2 with |x| < ∞and |y| < ∞.
□
Exercise
Let U be open in R2 and let f : U 7→R be a C1 function. Let (a,b) ∈U.
Assume that ∂f
∂y (a,b) , 0. Then, show that the map F : R2 7→R2 given by
(x,y) 7→F(x,y) = (x,f (x,y))
is locally invertible at (a,b).
Answer
We have to compute the Jacobian matrix and its determinant. We have
JF(x,y) =


1
0
∂f
∂x
∂f
∂y

,
so that
|JF(a,b)| = ∂f
∂y (a,b),
which, by assumption, is not zero and the inverse mapping theorem then implies what we
are asked to prove.
□
The result of this exercise can be used to discuss implicit functions. We assume that the
function f : U 7→R deﬁned in the exercise has the value c at (a,b), or, f (a,b) = c.
We wish to ﬁnd out whether there is some differentiable function y = φ(x), deﬁned near
x = a, such that φ(a) = b and
f (x,φ(x)) = c
for all x near a. If such a function φ exists, we say that y = φ(x) is the function determined
implicitly by f .
10.9.2
Implicit function theorem
Let U be open in R2 and let f : U 7→R be a C1 function. Let (a,b) ∈U and let f (a,b) = c.
Assume that ∂f
∂y (a,b) , 0. Then, there exists an implicit function y = φ(x) which is C1 in
some interval containing a with φ(a) = b.
Proof
We apply the above exercise and use its notation. Thus, we let
F(x,y) = (x,f (x,y)).
We know that F(a,b) = (a,c) and that there exists a C1-inverse G deﬁned locally near
(a,c). We can write

286
An Introduction to Vectors, Vector Operators and Vector Analysis
G(x,f (x,y)) = G(x,z) = (x,y) = (x,g(x,z))
for some function g. This equation shows that we have put z = f (x,y) and y = g(x,z).
We deﬁne
φ(x) = g(x,c).
Then on the one hand,
F(x,φ(x)) = F(x,g(x,c)) = F(G(x,c)) = (x,c)
and on the other hand,
F(x,φ(x)) = (x,f (x,φ(x))).
This proves that f (x,φ(x)) = c. Furthermore, by deﬁnition of an inverse map, G(a,c) =
(a,b) so that φ(a) = b. This proves the implicit function theorem in two dimensions.
□
Exercise
Show that the function f (x,y) = x2 + y2 implicitly deﬁnes a function y =
φ(x) near x = 1. Find this function. Take (i) (a,b) = (1,1), (ii) (a,b) = (−1,−1).
Answer
(i) c = f (1,1) = 2. ∂f
∂y (1,1) = 2 , 0, so the implicit function y = φ(x) near x = 1
exists. It can be found by explicitly solving 2 = x2 + y2 : y =
√
2 −x2.
(ii) y = −
√
2 −x2.
In general, the equation f (x,y) = c deﬁnes some curve as in Fig. 10.4(a). As indicated in
Fig. 10.4(b), we see that there is an implicit function near the point (a,b), which exists only
for points near x = a and not for all x values. It is straightforward to generalize the implicit
function theorem to higher dimensional functions f : Rn 7→R but we will not pursue it
here.
Fig. 10.4
Neighborhood of point (a,b) on f (x,y) = c is locally given by the implicit
function y = f (x)

Functions with Vector Arguments
287
10.9.3
Algorithm to construct the inverse of a map
We give an iterative algorithm [5], using the method of successive approximations, to
construct the inverse of the locally C1-invertible map f : E3 7→E3
u = f(x) = u(x)ˆi + v(x)ˆj + w(x)ˆk
with
u(x) = φ(x,y,z), v(x) = ψ(x,y,z), w(x) = χ(x,y,z).
That is, we want to solve the equation u = f(x) for x where u is a point near u0 = f(x0),
where, at x = x0 we must have, for the Jacobian determinant |Jf(x0)|,
|Jf(x0)| =

φx(x0)
φy(x0)
φz(x0)
ψx(x0)
ψy(x0)
ψz(x0)
χx(x0)
χy(x0)
χz(x0)

, 0.
The differentials dx,dy,dz and du,dv,dw satisfy the linear relations (see Eq. (10.23))
du = dφ = φxdx + φydy + φzdz
dv = dψ = ψxdx + ψydy + ψzdz
dw = dχ = χxdx + χydy + χzdz
(10.31)
or,
du = Jf(x)dx
(10.32)
where
Jf(x) =


φx(x)
φy(x)
φz(x)
ψx(x)
ψy(x)
ψz(x)
χx(x)
χy(x)
χz(x)


is the Jacobian giving the derivative of the map u = f(x).
Exercise
Find an upper bound on the Euclidean distance of the images of the points x
and x + ∆x under the map f : E3 7→E3.
Solution
The required distance is given by (see Equation following Eq. (10.24) and note
that ∆x · ∇f (x) is a vector),
q
(f(x + ∆x) −f(x)) · (f(x + ∆x) −f(x)) =
q
(∆x · ∇f (x)) · (∆x · ∇f (x))

288
An Introduction to Vectors, Vector Operators and Vector Analysis
or,
q
(∆x · ∇f (x)) · (∆x · ∇f (x))
=
q
(hφx + kφy + lφz)2 + (hψx + kψy + lψz)2 + (hχx + kχy + lχz)2
(10.33)
where φx,y,z,ψx,y,z,χx,y,z are the partial derivatives giving the row-wise elements of the
Jacobian matrix and h,k,l are the components of ∆x. Let M denote an upper bound on the
absolute values of all the elements of the Jacobian matrix taken at all points of the segment
joining x and x + ∆x. This gives
q
(hφx + kφy + lφz)2 + (hψx + kψy + lψz)2 + (hχx + kχy + lχz)2
≤
√
3M(|h| + |k| + |l|) ≤3M
p
h2 + k2 + l2
(10.34)
which is the required upper bound.
□
Thus, the distance of the image points is at most 3M times that of the original ones. Writing
y = x + ∆x we can write Eq. (10.34) as
|f(y) −f(x)| ≤3M|y −x|.
(10.35)
We now consider the mapping u = f(x) in a neighborhood
|x −x0| < δ
(10.36)
of the point x0 in the domain R of f. Let u0 = f(x0). For a ﬁxed u we write the equation
u = f(x) which is to be solved for x, in the form
x = g(x),
(10.37)
where
g(x) = x + A(u −f(x));
(10.38)
where A stands for an appropriately chosen ﬁxed non-singular operator (or matrix) with
inverse denoted by A−1. Thus, Eq. (10.37) is equivalent to A(u −f(x)) = 0, which by
multiplication with A−1 yields
A−1A(u −f(x)) = I(u −f(x)) = (u −f(x)) = 0,
where I is the identity operator represented by the unit matrix. Thus, a solution x of
Eq. (10.37), that is, a ﬁxed point of the map g, furnishes a solution of u = f(x).

Functions with Vector Arguments
289
We show that a ﬁxed point of the map g can be reached by reaching the limit of xn
deﬁned by the recursion formula
xn+1 = g(xn)
n = 0,1,2,...
(10.39)
provided the Jacobian matrix, which in this case we denote by g′(x), representing the
derivative of the vector mapping g is of sufﬁciently small size. This procedure is popularly
known as the method of successive approximations. Making the ‘small size’ requirement
more precise, we require that for all x in the neighborhood of x0 given by Eq. (10.36), the
largest element of the matrix g′ is less than 1
6 in absolute value and that
|g(x0) −x0| < 1
2δ.
The last equation is the condition on the initial value from which to start the iteration.
First, we prove by induction that, under the assumptions stated, the recursion formula
in Eq. (10.39) successively gives vectors satisfying Eq. (10.36). This assures us that xn lie in
the domain of g so that the sequence can be continued indeﬁnitely. From Eq. (10.35) with
M = 1
6 we see that,
|g(y) −g(x)| ≤1
2|y −x|
for
|x −x0| < δ, |y −x0| < δ.
(10.40)
Now the inequality in Eq. (10.36) holds trivially for x = x0. If it holds for x = xn, we ﬁnd
for the vector xn+1 deﬁned by Eq. (10.39) that
|xn+1−x0| ≤|xn+1−x1|+|x1−x0| = |g(xn)−g(x0)|+|g(x0)−x0| ≤1
2|xn−x0|+ 1
2δ.
This proves that |xn −x0| < δ for all n.
To see that the sequence {xn} converges, we observe that by Eq. (10.40),
|xn+1 −xn| = |g(xn) −g(xn−1)| ≤1
2|xn −xn−1|.
In the same way,
|xn −xn−1| ≤1
2|xn−1 −xn−2|,
|xn−1 −xn−2| ≤1
2|xn−2 −xn−3|
and so on. These inequalities together imply
|xn+1 −xn| ≤1
2n |x1 −x0| ≤
δ
2n+1 .
(10.41)

290
An Introduction to Vectors, Vector Operators and Vector Analysis
Since the distance between successive iterates decreases exponentially, the sequence {xn}
must converge to its limit say x∗. In this limit, the distance between successive iterates goes
to zero. Therefore, the substitution of this limit x∗in g(x) must return the same vector x∗.
In other words this limit x∗solves Eq. (10.37). Another way to see this is the following.
Since g(x) is continuous, if the sequence {xk} k = 1,2,... converges to x∗the sequence
g(xk) k = 1,2,... must converge to g(x∗). However, by virtue of Eq. (10.39) these two
sequences are identical, making their limits the same, that is, x∗= g(x∗).
Since the function g depends continuously on u, the xn obtained successively by
recursion formula Eq. (10.37) also depends continuously on u. Since the convergence of
the sequence {xn} does not depend on u, it follows that its limit x∗is a continuous
function of u. Also, we have |x∗−x0| ≤δ because |xn −x∗| < δ for all n. If there existed a
second solution x′ with x′ = g(x′) and |x′ −x0| ≤δ we ﬁnd from Eq. (10.40) that
|x′ −x∗| = |g(x′) −g(x∗)| ≤1
2|x′ −x∗|
which makes |x′ −x∗| = 0 and x′ = x∗.
Thus, we have established the existence, uniqueness and continuity of a solution x∗of
the equation u = f(x), for which |x∗−x0| ≤δ, provided the function g(x) deﬁned by
Eq. (10.38) has the derivative g′ with elements less than
1
6 in absolute value for
|x∗−x0| ≤δ and provided |g(x0) −x0| < 1
2δ. These requirements can be satisﬁed for all u
sufﬁciently close to u0 by a suitable choice of the matrix A. By the deﬁnition of g
(Eq. (10.38))
g′(x) = I −Af′(x),
where I is the identity. Then, for x = x0
g′(x0) = I −Af′(x0) = 0
if we choose for A the inverse of the matrix f′(x0), that is,
A = (f′(x0))−1.
The existence of this inverse is guaranteed by our basic assumption that the matrix f′(x0)
has a non-vanishing determinant, that is, the Jacobian of the mapping f does not vanish at
the point x0. The assumed continuity of the ﬁrst derivatives of the mapping f implies that
g′(x) depends continuously on x; hence the the elements of g′(x) are arbitrarily small, for
instance less than 1
6, for sufﬁciently small |x −x0|, say for |x −x0| ≤δ. Moreover, by
Eq. (10.38)
|g(x0) −x0| = |A(u −f(x0))| = |A(u −u0| < 1
2δ,

Functions with Vector Arguments
291
provided u lies in a sufﬁciently small neighborhood of u0. This completes the proof of
the local existence of a continuous inverse for a continuously differentiable mapping with
non-vanishing Jacobian.
The existence of the inverse of the Jacobian of the map u = f(x) at a point x can be
used to show the continuity and differentiability of the inverse map x = f−1(u). Since the
Jacobian deﬁnes a linear, continuous and invertible map at x it must be one-to-one and
onto on some neighborhood of x. Furthermore, every point v , u in this neighborhood,
is given by v = f(y), y , x. This means that as v →u through some sequence of points,
y →x. This enables us to invert the differential of f(x) in the following way.
f(y) −f(x) = v −u = J(x) · (y −x) + |y −x|h(y −x),
or,
y −x = J−1(x)(v −u) + |v −u|ϵ(v −u),
where
lim
v→uϵ(v −u) = 0.
This equation just says that the vector x satisfying u = f(x) is a differentiable function of
vector u and that the Jacobian matrix of x with respect to u is the inverse of the matrix
f′(x) = J(x).
10.10
Differentiating Inverse Functions
Let f(x) be a differentiable and invertible function. We assume the differentiability of the
inverse function. Let the components of f(x) be 3
u = φ(x,y,z)
v = ψ(x,y,z)
w = χ(x,y,z),
and the components of the inverse be
x = g(u,v,w)
y = h(u,v,w)
z = k(u,v,w).
We substitute the inverse functions in the given functions to get the compound functions
φ(g(u,v,w),h(u,v,w),k(u,v,w)), ψ(g(u,v,w),h(u,v,w),k(u,v,w)),
χ(g(u,v,w),h(u,v,w),k(u,v,w)),
3The transformation f(x) could be passive, that is, the one which changes the coordinates of the same vector referring to a
different basis.

292
An Introduction to Vectors, Vector Operators and Vector Analysis
which must be equal to u v and w respectively. Thus, we get the equations
u = φ(g(u,v,w),h(u,v,w),k(u,v,w))
v = ψ(g(u,v,w),h(u,v,w),k(u,v,w))
w = χ(g(u,v,w),h(u,v,w),k(u,v,w))
(10.42)
These equations are identities as they hold for all values of u,v,w. We now differentiate
each of these equations with respect to u v and w regarding them as independent
variables and apply the chain rule to differentiate the compound functions. We then
obtain the system of equations
1
=
φxgu + φyhu + φzku
0 = φxgv + φyhv + φzkv
0 = φxgw + φyhw + φzkw
0
=
ψxgu + ψyhu + ψzku
1 = ψxgv + ψyhv + ψzkv
0 = φxgw + φyhw + φzkw
0
=
χxgu + χyhu + χzku
0 = χxgv + χyhv + χzkv
1 = χxgw + χyhw + χzkw
Solving these equations for nine unknowns gu,v,w,hu,v,w,ku,v,w we get the partial
derivatives of the inverse functions x = g(u,v,w), y = h(u,v,w), z = k(u,v,w) with
respect to u,v,w expressed in terms of the derivatives of the original functions φ(x,y,z),
ψ(x,y,z),χ(x,y,z) with respect to x,y,z, namely,
gu =
1
D [ψyχz −ψzχy] gv = 1
D [χyφz −χzφy] gw
=
1
D [φyψz −φzψy]
hu =
1
D [ψzχx −ψxχz] hv = 1
D [χzφx −χxφz] hw
=
1
D [φzψx −φxψz]
ku =
1
D [ψxχy −ψyχx] kv = 1
D [χxφy −χyφx] kw
=
1
D [φxψy −φyψx]
(10.43)

Functions with Vector Arguments
293
where D stands for the Jacobian determinant
|Jf(x)| = D =

φx
φy
φz
ψx
ψy
ψz
χx
χy
χz

·
This justiﬁes calling the Jacobian the derivative of a differentiable map f : E3 7→E3. For a
2-D map Eq. (10.43) reduce to
gu =
ψy
D , gv = −
φy
D , hu = −ψx
D , hv = φx
D ,
(10.44)
where the Jacobian determinant D is given by
D =

φx
φy
ψx
ψy
·
Exercise
For polar coordinates in the plane expressed in terms of rectangular
coordinates,
u = r =
q
x2 + y2, v = θ = tan−1 y
x,
ﬁnd the partial derivatives rx,ry,θx,θy and the Jacobian determinant.
Solution
The partial derivatives are
rx =
x
p
x2 + y2 = x
r , ry =
y
p
x2 + y2 = y
r ,
θx =
−y
x2 + y2 = −y
r2 , θy =
x
x2 + y2 = x
r2 .
(10.45)
Hence, the Jacobian has the value
D = x
r
x
r2 −y
r

−y
r2

= 1
r
and the partial derivatives of the inverse functions (cartesian coordinates expressed in
terms of polar coordinates) are
xr = x
r , xθ = −y, yr = y
r , yθ = x,
as we could have found by direct differentiation of the inverse formulae x = r cosθ,y =
r sinθ.
□

294
An Introduction to Vectors, Vector Operators and Vector Analysis
From the formulae for the derivatives of the inverse functions (Eq. (10.44)) for the 2-D case,
we ﬁnd that the Jacobian determinant of the functions x = x(u,v) and y = y(u,v) (where
the coordinates themselves replace the function names g and h) with respect to u and vis
given by
d(x,y)
d(u,v) = xuyv −xvyu =
uxvy −uyvx
D2
= 1
D =
 d(u,v)
d(x,y)
!−1
.
(10.46)
Thus, the Jacobian determinant of the inverse system of functions is the reciprocal of the
Jacobian determinant of the original system.4 This is not surprising, because these
Jacobians are the inverses of each other, as we have shown above (see the last para before
the present subsection).
Exercise
Find the second order derivatives for a 2-D map xuu = ∂2x
∂u2 = guu and yuu =
∂2y
∂u2 = huu.
Hint
Differentiate the equations (with ux = φx, xu = gu etc.)
1 = uxxu + uyyu
0 = vxxu + vyyu
(10.47)
again with respect to u and use the chain rule. Then, solve the resulting system of linear
equations regarding the quantities xuu and yuu as unknowns and then replace xu and yu
by the expressions already known for them. Note that the determinant of the doubly
differentiated system is again D and hence, by hypothesis, is not zero.
Answer
xuu = −1
D3

uxxv2
y −2uxyvxvy + uyyv2
x
uy
vxxv2y −2uxyvxvy + vyyv2x
vy

and
yuu = 1
D3

uxxv2y −2uxyvxvy + uyyv2x
ux
vxxv2y −2vxyvxvy + vyyv2x
vx

·
□
10.11
Jacobian for the Composition of Maps
Let f(x) be a differentiable and 1 −1 map from open set R1 to the open set R2 and let
g(x) be a differentiable and 1 −1 map from open set R2 to the open set R3 in E3. Then,
4This is the analogue of the rule for the derivative of the inverse of a function of a single variable. See, for example, [5]
volume I.

Functions with Vector Arguments
295
we can compose these two maps to get a differentiable and 1 −1 map from open set R1 to
the open set R3 as g ◦f(x) = g(f(x)). If the components of f(x) are
ξ = φ(x,y,z), η = ψ(x,y,z), ζ = χ(x,y,z)
and the components of g(x) are
u = Φ(ξ,η,ζ), v = Ψ (ξ,η,ζ), w = Ω(ξ,η,ζ)
then the components of the composite map R1 7→R3 are
u = Φ(φ(x,y,z),ψ(x,y,z),χ(x,y,z)),v = Ψ (φ(x,y,z),ψ(x,y,z),χ(x,y,z)),
w = Ω(φ(x,y,z),ψ(x,y,z),χ(x,y,z)).
Using the chain rule to differentiate compound functions, we get
∂u
∂x
= Φξφx + Φηψx + Φζχx, ∂u
∂y = Φξφy + Φηψy + Φζχy,
∂u
∂z
= Φξφz + Φηψz + Φζχz,
∂v
∂x = Ψξφx + Ψηψx + Ψζχx, ∂v
∂y = Ψξφy + Ψηψy + Ψζχy,
∂v
∂z
= Ψξφz + Ψηψz + Ψζχz,
∂w
∂x
= Ωξφx + Ωηψx + Ωζχx, ∂w
∂y = Ωξφy + Ωηψy + Ωζχy,
∂w
∂z
= Ωξφz + Ωηψz + Ωζχz.
(10.48)
Equation (10.48) can be written in the matrix form,


∂u
∂x
∂u
∂y
∂u
∂z
∂v
∂x
∂v
∂y
∂v
∂z
∂w
∂x
∂w
∂y
∂w
∂z


=


Φξ
Φη
Φζ
Ψξ
Ψη
Ψζ
Ωξ
Ωη
Ωζ




φx
φy
φz
ψx
ψy
ψz
χx
χy
χz


·
Since the determinant of the product of matrices is the product of their determinants, we
conclude that the Jacobian determinant of the composition of two transformations is

296
An Introduction to Vectors, Vector Operators and Vector Analysis
equal to the product of the Jacobian determinants of the individual transformations.
Using the notation we have introduced for the Jacobian determinant, we have,
d(u,v,w)
d(x,y,z) = d(u,v,w)
d(ξ,η,ζ)
d(ξ,η,ζ)
d(x,y,z) .
(10.49)
Written in this form, we see that, under the composition of transformations, the Jacobians
behave in the same way as the derivatives behave under the composition of functions of a
single variable.
Exercise
Using Eq. (10.49) show that the Jacobian determinant of the differentiable
inverse of a differentiable map is the reciprocal of its Jacobian determinant.
□
Consider a continuously differentiable map R2 7→R2 mapping (x,y) plane to (ξ,η)
plane given by ξ = φ(x,y),η = ψ(x,y) which has a non-vanishing Jacobian determinant
at (x0,y0) = P0. We can then determine the mapping of directions at the point P0. A
curve passing through P0 can be described parametrically by equations x = f (t),
y = g(t) where x0 = f (t0),y0 = g(t0). The slope of the curve at P0 is given by
m = g′(t0)
f ′(t0)
(10.50)
Similarly, the slope of the image curve
ξ = φ(f (t),g(t)), η = ψ(f (t),g(t))
(10.51)
at the point corresponding to P0 is
µ = dη/dt
dξ/dt =
ψxf ′ + ψyg′
φxf ′ + φyg′ = c + dm
a + bm,
(10.52)
where a,b,c,d are the constants
a = φx(x0,y0),b = φy(x0,y0),c = ψx(x0,y0),d = ψy(x0,y0).
Since
dµ
dm =
ad −bc
(a + bm)2
we ﬁnd that µ is an increasing function of m if ad −bc > 0 and decreasing function if
ad−bc < 0. More precisely, this holds locally, excluding the directions where m or µ become
inﬁnite.
Increasing slopes correspond to increasing angles of inclination or to counterclockwise
rotation of the corresponding directions. Thus, dµ
dm > 0 implies that the counterclockwise

Functions with Vector Arguments
297
sense of rotation is preserved, while it is reversed for dµ
dm < 0 Now ad−bc is just the Jacobian
determinant
d(ξ,η)
d(x,y) =

φx
φy
ψx
ψy

evaluated at the point P0. We conclude that the mapping ξ = φ(x,y),η = ψ(x,y)
preserves or reverses orientations near the point (x0,y0) according to whether the
Jacobian determinant at that point is positive or negative.
10.12
Surfaces
As for curves, in most cases the parametric representation is found suitable for surfaces [5].
Since a surface is a two dimensional object, it requires two parameters to ﬁx a point on it, as
against one parameter required to ﬁx a point on a curve. Thus, a parametric representation
of a surface is given by parameterizing the position vector x ≡(x,y,z) of a point on the
surface,
x = Φ(u,v) ≡(x = φ(u,v),y = ψ(u,v),z = χ(u,v))
(10.53)
where we assume the surface to be smooth, that is, x = Φ(u,v) is a continuously
differentiable vector valued function or, equivalently, x = φ(u,v), y = ψ(u,v),
z = χ(u,v) are continuously differentiable scalar valued functions of two parameters
(u,v). The point (u,v) ranges over some region R in the (u,v) plane. The corresponding
point x(u,v) ≡(x = φ(u,v), y = ψ(u,v), z = χ(u,v)) ranges over a set in E3 or R3
spanning the surface. We can describe the surface in one of the three forms z = f (x,y),
y = f (z,x), x = f (y,z) by solving one of the three pairs of equations drawn out of
x = φ(u,v), y = ψ(u,v), z = χ(u,v). Solving any such pair of equations is equivalent to
inverting the corresponding R2 7→R2 map, say (u,v) 7→(x = φ(u,v), y = ψ(u,v)) to
express (u,v) as functions of (x,y) which can then be substituted in z = χ(u,v) to get
z = f (x,y). Thus, we require that not all of the three R2 7→R2 maps corresponding to
three pairs of equations be non-invertible, that is, we require that the three Jacobian
determinants

ψu
ψv
χu
χv
,

χu
χv
φu
φv
,

φu
φv
φu
φv

(10.54)
do not all vanish at once. We can summarize this condition in a single inequality
(φuψv −φvψu)2 + (ψuχv −ψvχu)2 + (χuφv −χvφu)2 > 0
(10.55)
If the inequality Eq. (10.55) is satisﬁed, in some neighbourhood of each point on the surface
given by the R2 7→R3 map in Eq. (10.53), it is certainly possible to express one of the three
coordinates in terms of the other two.

298
An Introduction to Vectors, Vector Operators and Vector Analysis
At each point on the surface with parameters u,v we can partially differentiate the
position vector to give
xu = (φu,ψu,χu)
and
xv = (φv,ψv,χv)
(10.56)
The differential of the vector x using the corresponding Jacobian, is given by
dx =


dx
dy
dz


=


φu
φv
ψu
ψv
χu
χv


 du
dv
!
= xudu + xvdv.
(10.57)
The three determinants Eq. (10.54) are just the components of the vector product xu × xv.
The expression on the left of the inequality in Eq. (10.55) is the square of the length of the
vector xu × xv so that condition Eq. (10.55) is equivalent to
xu × xv , 0
(10.58)
As an example, the spherical surface x2 + y2 + z2 = r2 of radius r is represented
parametrically by the equations
x = r cosu sinv, y = r sinu sinv z = r cosv, (0 ≤u < 2π, 0 ≤v ≤π)
(10.59)
where v = θ is the “polar inclination” or the polar angle and u = φ is the “longitude” or
the azimuthal angle made by the point on the sphere. Note that the functions relating x,y,z
to u,v are single valued and cover all the sphere. As v runs from π/2 to π the point x,y,z
spans the lower hemisphere, that is,
z = −
q
r2 −x2 −y2
while the values of v from 0 to π/2 give the upper hemisphere. Thus, for the parametric
representation it is not necessary, as it is for the representation
z = ±
q
r2 −x2 −y2,
to apply two single valued branches of the function in order to span the whole sphere.
We obtain another parametric representation of the sphere by means of stereographic
projection. In order to project the sphere x2 + y2 + z2 −r2 = 0 stereographically from the
north pole (0,0,r) on the equatorial plane z = 0, we join each point of the surface to the
north pole N by a straight line and call the intersection of this line with the equatorial plane
the stereographic image of the corresponding point of the sphere (see Fig. 10.5). We thus
obtain a 1−1 correspondence between the points of the sphere and the points of the plane,
except for the north pole N. Using elementary geometry, we ﬁnd that this correspondence
is expressed by

Functions with Vector Arguments
299
x =
2r2u
u2 + v2 + r2 , y =
2r2v
u2 + v2 + r2 , z = (u2 + v2 −r2)r
u2 + v2 + r2 ,
(10.60)
Fig. 10.5
Stereographic projection of the sphere
where (u,v) are the rectangular (cartesian) coordinates of the image point in the plane.
These equations can be regarded as the parametric representation of the sphere, the
parameters (u,v) being the rectangular coordinates in the u,v (equatorial) plane.
As a further example, we give parametric representation of surfaces
x2
a2 + y2
b2 −z2
c2 = 1
and
x2
a2 + y2
b2 −z2
c2 = −1
called the hyperboloid of one sheet and the hyperboloid of two sheets respectively
(see Fig. 10.6). The hyperboloid of one sheet is represented by
x = acosu coshv,
y = bsinu coshv,
z = csinhv
(10.61)
where 0 ≤u < 2π; −∞< v < +∞and the hyperboloid of two sheets by
x = acosu sinhv,
y = bsinu sinhv,
z = ±ccoshv
(10.62)
where 0 ≤u < 2π; 0 < v < +∞.
In general, we may regard the parametric representation of a surface as the mapping of
the region R of the (u,v) plane onto the corresponding surface. To each point of the region

300
An Introduction to Vectors, Vector Operators and Vector Analysis
R of the (u,v) plane there corresponds one point of the surface and typically the converse
is also true.5
Fig. 10.6
(a) Hyperboloid of one sheet and (b) Hyperboloid of two sheets
Just as we can parameterize a surface by mapping a region in the u,v plane via Eq. (10.53),
we can parameterize a curve on a surface by mapping an appropriate curve in the u,v plane
onto the given curve on the surface. Thus, a curve u = u(t), v = v(t) in the u,v plane
corresponds, by virtue of Eq. (10.53), to the curve
x(t) = Φ(u(t), v(t)) ≡(x(t) = φ(u(t), v(t)), y(t) = ψ(u(t), v(t)),
z(t) = χ(u(t),v(t)))
(10.63)
on the surface. Thus for example, the coordinate lines passing through a point on the sphere
have the parametric equations u = φ = constant (longitudes) and v = θ = constant
(latitudes). Corresponding curves in the u,v plane are the lines parallel to v and u axes
respectively. The net of parametric curves (the mesh of latitudes and longitudes on the
sphere) corresponds to the net of parallels to the axes in the u,v plane.
The tangent to the curve on the surface corresponding to the curve u = u(t),v = v(t)
in the u,v plane has the direction of the vector xt = dx
dt , that is,
xt = (xt,yt,zt) =
 
xu
du
dt + xv
dv
dt ,yu
du
dt + yv
dv
dt ,zu
du
dt + zv
dv
dt
!
= xu
du
dt +xv
dv
dt .
(10.64)
At a given point on the surface, the tangential vectors xt of all curves on the surface passing
through that point are linear combinations of two vectors xu,xv which respectively are
tangential to to the parametric lines v = constant and u = constant passing through that
5This is not always the case. For example, in the representation Eq. (10.59) of the sphere by spherical coordinates, the poles
of the sphere correspond to the whole line segments given by v = 0 and v = π.

Functions with Vector Arguments
301
point. (e.g., the vectors ˆφ and ˆθ for the spherical polar coordinates on a sphere.) This means
that the tangents all lie in the plane through the point spanned by the vectors xu and xv,
that is, the tangent plane to the surface at that point. The normal to the surface at that
point is perpendicular to all tangential directions, in particular to the vectors xu and xv.
Thus, the surface normal is parallel (or antiparallel) to the direction of the vector product
xu × xv = (yuzv −yvzu,zuxv −zvxu,xuyv −xvyu).
(10.65)
One of the most important keys to the understanding of the given surface is the study of
the curves that lie on it. Here, we give the expression for the arc length s of such a curve.
We start with
 ds
dt
!2
=
 dx
dt
!2
+
 dy
dt
!2
+
 dz
dt
!2
= xt · xt,
so in view of Eq. (10.64) we get
 ds
dt
!2
=
 
xu
du
dt + xv
dv
dt
!
·
 
xu
du
dt + xv
dv
dt
!
=
 
xu
du
dt + xv
dv
dt
!2
+
 
yu
du
dt + yv
dv
dt
!2
+
 
zu
du
dt + zv
dv
dt
!2
= E
 du
dt
!2
+ 2F
 du
dt
! dv
dt
!
+ G
 dv
dt
!2
.
(10.66)
Here, the coefﬁcients E,F,G, the Gaussian fundamental quantities of the surface, are
given by
E =
 ∂x
∂u
!2
+
 ∂y
∂u
!2
+
 ∂z
∂u
!2
= xu · xu
F =
∂x
∂u
∂x
∂v + ∂y
∂u
∂y
∂v + ∂z
∂u
∂z
∂v = xu · xv
G =
 ∂x
∂v
!2
+
 ∂y
∂v
!2
+
 ∂z
∂v
!2
= xv · xv
(10.67)
These depend only on xu,xv and therefore on the surface and its parametric
representation and not on the particular choice of the curve on the surface. The expression
Eq. (10.66) for the derivative of the length of arc s with respect to the parameter t usually

302
An Introduction to Vectors, Vector Operators and Vector Analysis
is written symbolically without reference to the parameter used along the curve. One says
that the line element ds is given by the quadratic differential form (“fundamental form”)
ds2 = Edu2 + 2Fdudv + Gdv2.
(10.68)
The length of the cross product xu × xv can be expressed in terms of E,F,G as
|xu × xv|2 = |xu|2|xv|2 −(xu · xv)2 = EG −F2.
(10.69)
Our original condition on the parametric representation (inequality Eq. (10.55)) can now
be formulated as the condition
EG −F2 > 0
(10.70)
for the fundamental quantities.
The direction cosines for one of the two normals to the surface are the components of
the unit vector
1
|xu × xv|xu × xv =
1
√
EG −F2 xu × xv.
It follows from Eq. (10.65) that the normal to a surface represented parametrically has the
direction cosines
cosα = yuzv −yvzu
√
EG −F2 , cosβ = zuxv −zvxu
√
EG −F2 , cosγ = xuyv −xvyu
√
EG −F2 .
(10.71)
The tangent to a curve u = u(t),v = v(t) on the surface has the direction of the vector
xt = xu
du
dt + xv
dv
dt .
If we now consider a second curve, u = u(τ),v = v(τ) on the surface referred to a
parameter τ, its tangent has the direction of the vector
xτ = xu
du
dτ + xv
dv
dτ .
If the two curves pass through the same point on the surface, the cosine of the angle of
intersection ω is the same as the cosine of the angle between xt and xτ. Hence,
cosω = xt · xτ
|xt||xτ|.

Functions with Vector Arguments
303
We have,
xt · xτ =
 
xu
du
dt + xv
dv
dt
!
·
 
xu
du
dτ + xv
dv
dτ
!
= E du
dt
du
dτ + F
 du
dt
dv
dτ + du
dτ
dv
dt
!
+ Gdv
dt
dv
dτ .
(10.72)
Consequently, the cosine of the angle between two curves on the surface is given by
cosω =
E du
dt
du
dτ + F
du
dt
dv
dτ + du
dτ
dv
dt

+ G dv
dt
dv
dτ
q
E
du
dt
2 + 2F
du
dt
dv
dt

+ G
dv
dt
2q
E
du
dτ
2 + 2F
du
dτ
 dv
dτ

+ G
 dv
dτ
2
(10.73)
We end this subsection by giving one more example of parametrization of a surface which
comes up frequently in applications. We consider torus. This is obtained by rotating a circle
about a line which lies in the plane of the circle, but does not intersect with it (see Fig. 10.7).
We take the axis of rotation as the z-axis and choose the y-axis so as to pass through the
center of the circle, whose y-coordinate we denote by a. If the radius of the circle is r < |a|,
we obtain
x = 0, y −a = r cosθ, z = r sinθ(0 ≤θ < 2π)
Fig. 10.7
Creation of torus by the rotation of a circle
as a parametric representation of the circle in the y −z plane. Now letting the circle rotate
about the z-axis, we ﬁnd that for each point on the circle x2 + y2 remains constant; that is,
x2 + y2 = (a + r cosθ)2. If φ is the angle of rotation about the z-axis, we have
x = (a + r cosθ)sinφ,

304
An Introduction to Vectors, Vector Operators and Vector Analysis
y = (a + r cosθ)cosφ,
z = r sinθ
(10.74)
with 0 ≤φ < 2π,0 ≤θ < 2π as a parametric representation of the torus in terms of the
parameters θ and φ. In this representation the torus appears as the image of the square of
side 2π in the θ,φ plane. Any pair of boundary points of this square lying on the same line
θ = constant or φ = constant corresponds to only one point on the surface and the four
corners of the square all correspond to the same point on the surface.
Equation (10.67) gives, for the line element on the torus,
ds2 = r2dθ2 + (a + r cosθ)2dφ2.
10.13
The Divergence and the Curl of a Vector Field
We have already seen that the del or the grad operator has vector like structure and it also
transforms like a vector under the rotation and translation of the coordinate system. This
enables us to formally treat the del operator like a vector with components
 ∂
∂x1 , ∂
∂x2 , ∂
∂x3

and deﬁne its scalar and vector products with vector valued functions possibly giving a
vector ﬁeld.
The divergence
The corresponding scalar product called the divergence of a ﬁeld is given by
∇· f(x) = ∂f1
∂x1
+ ∂f2
∂x2
+ ∂f3
∂x3
=
3
X
k=1
∂fk
∂xk
(10.75)
where f1,2,3(x) are the scalar valued component functions of the vector valued function
f(x) with respect to some orthonormal basis (see Eq. (10.1)). If we ﬁx a position vector
x, then we get the corresponding vector f(x) giving us the unique value of the divergence
∇·f(x). Thus, the divergence of a vector ﬁeld is itself a scalar ﬁeld and we can calculate ‘the
divergence at a point’. The value of the divergence of a vector ﬁeld at a point is a measure of
how much a vector f(x) spreads out from (or ﬂows into) the point x in question. Thus, the
vector function in Fig. 10.8(a) has large positive divergence (if the arrows pointed inward
it would be a large negative divergence), the function in Fig. 10.8(b) has zero divergence
and Fig. 10.8(c) again shows a function of positive divergence. Here is a nice possible
observation of the divergence phenomenon [9]. Imagine standing at the edge of a pond.
Sprinkle some sawdust on the surface. If the material spreads out then you have dropped
it at a point of positive divergence; if it collects together, you have dropped it at a point of
negative divergence. The vector function v in this model is the velocity of the water. This
is a two dimensional example but it helps give us a feel for the meaning of divergence. A
point of positive divergence is a sourse or ‘foucet’; a point of negative divergence is a sink
or ‘drain’.

Functions with Vector Arguments
305
Exercise
If the functions in Fig. 10.8 are va = r = xˆx + y ˆy + zˆz, vb = ˆz and vc = zˆz,
calculate the divergences.
Answer
∇· va = 3, ∇· vb = 0, ∇· vc = 1.
□
In fact the ﬁrst result can be generalized to n dimensions as ∇· x = Pn
k=1
∂xk
∂xk = n.
Fig. 10.8
Vector ﬁelds given by (a) va (b) vb (c) vc as deﬁned in this exercise
Exercise
Calculate the divergence of the following vector functions.
(a) va = x2ˆx + 3xz2ˆy −2xzˆz
(b) vb = xyˆx + 2yzˆy −3zxˆz
(c) vc = y2ˆx + (2xy + z2)ˆy + 2yzˆz
□(10.76)
Exercise
Sketch the vector function v = ˆr
r2 and compute its divergence except at r = 0.6
Hint
Write
ˆr
r2 =
x
(x2 + y2 + z2)3/2 ˆx +
y
(x2 + y2 + z2)3/2 ˆy +
z
(x2 + y2 + z2)3/2 ˆz
and evaluate ∇· v.
Answer
∇· ˆr
r2 = 0
□
The result of the above exercise can be explained as follows. The ﬂux of a vector ﬁeld across
the surface enclosing a volume is simply the integral of the corresponding vector valued
function on the surface. If we enclose the point of interest in an inﬁnitesimal cube, then,
as we will see later, this ﬂux equals ∇· vdV where v deﬁnes the ﬁeld and dV is the volume
of the inﬁnitesimal cube. For v = ˆr
r2 , looking at its expression with respect to the cartesian
system ˆx, ˆy, ˆz, it is clear that the ﬂux through the opposite faces of the cube cancel each
other so that the net ﬂux through the cube is zero. Since dV , 0 we must have ∇· ˆr
r2 = 0.
Note that the divergence of a vector ﬁeld changes even if the ﬁeld has a changing
magnitude in a single direction. Thus, for the ﬁeld given by v(x) = cos(πx)ˆx the
divergence is ∇· v = −πsin(πx)and varies sinusoidally with x. At any point the ﬁeld
6To ﬁnd what happens at r = 0 read the appendix on Dirac delta function.

306
An Introduction to Vectors, Vector Operators and Vector Analysis
ﬂows into the point along x axis if sin(πx) > 0 and out of it if sin(πx) < 0. For the ﬁeld
v =
ˆr
r2 , the ﬁeld spreads out as r2 as we go out from the origin, but its magnitude falls as
1
r2 so that its divergence is zero.
Since the operator del transforms like a vector under the rotation and translation of a
coordinate system, the divergence ∇·v of a vector ﬁeld v transforms like the scalar product
of two vectors, that is, like a scalar.
Exercise
In two dimensions, show that the divergence transforms as a scalar under
rotation.
Hint
Use the rotation (about the z-axis) matrix explicitly to transform (vx,vy) and (x,y),
then use the chain rule to show that the expression for ∇· v remains invariant.
□
The curl
The curl of a vector ﬁeld is the vector product of the del operator with the vector valued
function deﬁning the ﬁeld, say v. It can be conveniently deﬁned using Levi-Civita symbols,
(∇× v)i =
X
jk
εijk
∂vk
∂xj
i,j,k = 1,2,3.
(10.77)
Here εijk are the Levi-Civita symbols, v1,2,3(x) and x1,2,3 are the components of v(x) and
x respectively with respect to some orthonormal basis.
Exercise
Write down ∇× v explicitly in terms of its components.
Answer
∇× v =
 ∂vz
∂y −
∂vy
∂z
!
ˆx +
 ∂vx
∂z −∂vz
∂x
!
ˆy +
 ∂vy
∂x −∂vx
∂y
!
ˆz,
(10.78)
or, in terms of the determinantal deﬁnition of the cross product,
∇× v =

ˆx
ˆy
ˆz
∂
∂x
∂
∂y
∂
∂z
vx
vy
vz

·
□(10.79)
The value of ∇×v(x) at a point x is a measure of how much the vector v(x) “curls around”
the point x in question. Thus, the three functions in Fig. 10.8 all have zero curl while the
functions in Fig. 10.9 have a substantial curl, pointing in the z direction, as the rule of
ﬁxing the direction of a cross product would suggest. In anology with the illustration for
divergence, imagine that you are standing at the edge of a pond. Float a small paddle wheel
(like a cork with toothpicks pointing out radially); if it starts to rotate, you have placed it at
a point of non-zero curl. A whirlpool would be a region of large curl. To furnish intuition
further, we can read Eq. (10.78) geometrically.

Functions with Vector Arguments
307
Thus, in Fig. 10.10(a), the signs of ∂vz
∂y and
∂vy
∂z are opposite enhancing the ﬁrst term
in Eq. (10.78). In Fig. 10.10(b) these signs are the same, weakening the ﬁrst term.
Figure 10.10(c) shows that the sign of the gradient of a component along the
corresponding axis can be determined by the change in its value along that axis, thus
deciding its contribution to the curl of the ﬁeld.
Fig. 10.9
Illustrating curl of a vector ﬁeld
Exercise
Suppose the function sketched in Fig. 10.9(a) is va = y ˆx −xˆy and that in
Fig. 10.9(b) is vb = y ˆx. Calculate their curls and the divergence.
Answer
∇× va = −2ˆz and ∇× vb = −ˆz. Both have zero divergence. This is consistant
with Fig. 10.9, which shows the ﬁelds which are not spreading out, but are only curling
around.
□
Fig. 10.10
Various cases of ﬁeld curling around a point
After deﬁning the divergence and the curl, we need to obtain rules for their action on
expressions involving vector valued functions and also their combined action on such
functions. For completeness we also state here the corresponding rules for the action of
the del operator on the scalar valued functions. We have, for the scalar valued functions
f (x),g(x) and the vector valued functions A(x),B(x)

308
An Introduction to Vectors, Vector Operators and Vector Analysis
∇(f + g) = ∇f + ∇g,
∇· (A + B) = ∇· A + ∇· B
∇× (A + B) = ∇× A + ∇× B,
and
∇(kf ) = k∇f
∇· (kA) = k∇· A ∇× (kA) = k∇× A,
as can be easily checked using their deﬁnitions and the linearity of differentiation. Different
rules apply for different types of products of functions, that is, scalar valued products f g
and A · B and the vector valued products f A and A × B. This leads to six product rules,
two for gradients,
∇(f g) = f ∇g + g∇f ,
(10.80)
∇(A · B) = A × (∇× B) + B × (∇× A) + (A · ∇)B + (B · ∇)A,
(10.81)
two for divergences,
∇· (f A) = f (∇· A) + A · (∇f ),
(10.82)
∇· (A × B) = B · (∇× A) −A · (∇× B)
(10.83)
and two for curls,
∇× (f A) = f (∇× A) −A × (∇f ),
(10.84)
∇× (A × B) = (B · ∇)A −(A · ∇)B + A(∇· B) −B(∇· A).
(10.85)
Exercise
Prove Eq. (10.81).
Solution
We successively take up all terms on the RHS. We have,
[A × (∇× B)]i =
X
jklm
εkijεklmAj
∂Bm
∂xl
.
Using
εkijεklm = δilδjm −δimδjl
we get
[A × (∇× B)]i =
X
j
Aj
 ∂Bj
∂xi
−∂Bi
∂xj
!
.

Functions with Vector Arguments
309
Similarly,
[B × (∇× A)]i =
X
j
Bj
 ∂Aj
∂xi
−∂Ai
∂xj
!
.
Further, we get for the last two terms,
[(A · ∇)B]i =
X
j
Aj
∂Bi
∂xj
and
[(B · ∇)A]i =
X
j
Bj
∂Ai
∂xj
.
Putting all terms together, we get, for the ith component of the RHS,
X
j
 
Aj
∂Bj
∂xi
+ Bj
∂Aj
∂xi
!
= [∇(A · B)]i.
□
Exercise
Prove Eq. (10.83).
Solution
∇· (A × B) =
∂
∂xi
(A × B)i = ∂
∂xi
(εijkAjBk)
= εijk
∂Aj
∂xi
Bk + εijk
∂Bk
∂xi
Aj
= (∇× A)kBk −(∇× B)jAj
= B · (∇× A) −A · (∇× B).
(10.86)
Here are the rules for differentiating quotients,
∇
 f
g
!
= g∇f −f ∇g
g2
∇·
 A
g
!
= g(∇· A) −A · (∇g)
g2
∇×
 A
g
!
= g(∇× A) + A × (∇g)
g2
(10.87)

310
An Introduction to Vectors, Vector Operators and Vector Analysis
All the above rules for differentiating expressions of functions are valid for all
differentiable functions, scalar or vector valued, as the case may be. Therefore, these rules
can be treated as vector identities involving differential operators. You may try and prove
all these identities using Levi-Civita symbols.
Second derivatives
Upto now we obtained rules to ﬁnd different types of derivatives of expressions involving
various types of functions. We shall now ﬁnd rules to evaluate second derivatives obtained
by combining different types of ﬁrst derivatives, namely, the gradient, the divergence and
the curl. Since ∇f is a vector for a scalar valued function f , we can take the divergence and
the curl of it. We have,
(i) Divergence of the gradient: ∇2f ≡∇· (∇f ).
(ii) Curl of gradient: ∇× (∇f ).
The divergence ∇· v is a scalar, so we can take its gradient:
(iii) Gradient of divergence: ∇(∇· v).
The curl ∇× v is a vector, so we can take its divergence and curl:
(iv) Divergence of curl: ∇· (∇× v).
(v) Curl of a curl: ∇× (∇× v).
These are all the possibilities and we consider them one by one.
(i) The operator ∇2f deﬁned above is called the Laplacian of f . We have,
∇2f ≡∇· (∇f ) =
 
ˆx ∂
∂x + ˆy ∂
∂y + ˆz ∂
∂z
!
·
 ∂f
∂x ˆx + ∂f
∂y ˆy + ∂f
∂z ˆz
!
= ∂2f
∂x2 + ∂2f
∂y2 + ∂2f
∂z2 .
(10.88)
The Laplacian of a scalar valued function is a scalar.
Exercise
Show that the Laplacian of a scalar ﬁeld φ(x) at a point is proportional to
the difference between the value of φ at that point and the average value of φ at the
surrounding points.
Solution
Let φ0 be the value of φ at a point which we take to be the origin. Let
φ(±∆x), φ(±∆y) and φ(±∆z) be the values at points ±∆x,±∆y,±∆z respectively.
We can approximate the second order partial derivatives deﬁning the Laplacian by
the corresponding second order differences
∂2φ
∂x2
= φ(∆x) + φ(−∆x) −2φ0
∆x2

Functions with Vector Arguments
311
∂2φ
∂y2
= φ(∆y) + φ(−∆y) −2φ0
∆y2
∂2φ
∂z2
= φ(∆z) + φ(−∆z) −2φ0
∆z2
.
Taking ∆x = ∆y = ∆z = ∆and then adding these ratios we get
∇2φ
=
−6
∆2

φ0 −1
6φ(∆x) + φ(−∆x) + φ(∆y) + φ(−∆y) + φ(∆z) + φ(−∆z)

=
−6
∆2 (φ0 −φavg).
If the Laplacian at a point is negative then its value at that point exceeds average of
its values at the surrounding neighbours. Since φ is a continuous and differentiable
function, its values cannot differ drastically at nearby points. Thus, if ∇2φ < 0 at a
point, this point represents a local maximum of φ. On the other hand, if ∇2φ > 0
at a point this point must be a local minimum of φ. Since negative divergence at a
point corresponds to the inﬂow of the ﬁeld into that point, ∇2φ < 0 corresponds to
the inﬂow of the ﬁeld ∇φ towards the point of maximum φ. ∇2φ > 0 corresponds
to a point of local minimum of φ and the ﬁeld ∇φ diverges out of this point. You
can draw the maximum and a minimum (a peak and a valley) for a function of two
variables and draw ∇φ vectors perpendicular to the contours of constant φ values.
Then you can varify the above statements with reference to these pictures.
□
Exercise
Explicitly calculate ∇2φ for φ = 1
r , r , 0 and show that it vanishes.
□
We may occasionally encounter the Laplcian of a vector, ∇2v which is a vector
quantity whose x-component is the Laplacian of vx etc,7. We have,
∇2v ≡(∇2vx)ˆx + (∇2vy)ˆy + (∇2vz)ˆz.
(10.89)
(ii) The curl of a gradient is always zero. That is,
∇× (∇f ) = 0.
(10.90)
Exercise
Prove Eq. (10.90).
Solution
We have,
[∇× (∇f )]i = εijk
∂2f
∂xj∂xk
.
7For curvilinear coordinates, where the unit vectors themselves depend on position, they too must be differentiated.

312
An Introduction to Vectors, Vector Operators and Vector Analysis
In this double sum, the pairs of terms like
∂2f
∂x1∂x2 and
∂2f
∂x2∂x1 occur with opposite
signs and cancel8 and all terms can be paired this way. Hence, the sum vanishes and
we get
[∇× (∇f )]i = 0, i = 1,2,3.
□
(iii) ∇(∇· v) seldom occurs in physical applications. Note that ∇2v , ∇(∇· v).
(iv) The divergence of a curl, like the curl of a gradient, is always zero.
∇· (∇× v) = 0.
(10.91)
Exercise
Prove Eq. (10.91).
Solution
∇· (∇× v) = εijk
∂2vk
∂xi∂xj
.
In this triple sum, for a ﬁxed value of k, two terms occur with interchanged values of
indices i and j. These terms are identical but with opposite signs and hence cancel.
All terms occur in such pairs so that the sum vanishes, thus proving Eq. (10.91).
□
(v) The curl of curl operator can be decomposed into the gradient of divergence and the
vector Laplacian as follows.
∇× (∇× v) = ∇(∇· v) −∇2v.
(10.92)
Exercise
Prove Eq. (10.92).
Solution
[∇× (∇× v)]i = εkijεklm
∂2vm
∂xj∂xl
Using
εkijεklm = δilδjm −δimδjl
this becomes
∂
∂xi
 ∂vj
∂xj
!
−∂2vi
∂x2
j
= [∇(∇· v)]i −[∇2v]i.
□
Note that Eq. (10.92) can be taken to be a coordinate free deﬁnition of ∇2v in preference
to Eq. (10.89) which depends on cartesian coordinates.
8We assume, of course, that the order of differentiation does not matter.

Functions with Vector Arguments
313
Exercise
In what follows r denotes a position vector r = |r| is its magnitude, A(r) and
B(r) are vector ﬁelds, φ(r) is a scalar ﬁeld and f (r) is a function of r. All ﬁelds and
functions have continuous ﬁrst derivatives. Using Levi-Civita symbols or otherwise, prove
the following.
(i) ∇× (∇× A) = ∇(∇· A) −∇2A.
(ii) A × (∇× B) = ∇B(A · B) −(A · ∇)B where ∇B operates on B only.
(iii) Given ∇× A = 0 = ∇× B show that ∇· (A × B) = 0.
(iv) For constant a and b show that ∇× [(a × r) × b] = a × b.
(v) ∇· r = 3, ∇× r = 0, ∇(A · r) = A, (A · ∇)r = A.
(vi) ∇rn = nrn−2r, ∇· f (r)r
r
= 1
r2
d
dr (r2f ).
We will use any one or more of these results in the sequel, as and when required.
□
Exercise
A particle performs uniform circular motion on a circle of radius r and position
vector r. Show that (a) ∇× v = 2ω and (b) ∇· v = 0, where v is the linear velocity and ω
is the (constant) rotational velocity of the particle.
Solution
(a) We know that for circular motion, v = ω × r. Therefore,
∇× v = ∇× (ω × r) = ω(∇· r) −ω · ∇r.
However, ∇·r = 3 and ω·∇r = ω, which gives (a). Thus, we see that the curl operator
transforms the velocity vector into a rotational velocity vector.
(b) ∇· v = ∇· (ω × r) = r · ∇× ω −ω · ∇× r = 0, since ω is a constant vector and
∇× r = 0.
□
Thus, there are basically two types of second derivatives, the Laplacian, which is of
fundamental importance and the gradient of divergence which we seldom encounter.
Since the second derivatives sufﬁce to deal with practically all the physical applications,
going over to higher derivatives will reduce to an academic exercise without any physical
motivation.
10.14
Differential Operators in Curvilinear Coordinates
A system of curvilinear coordinates, say (u,v,w), is speciﬁed via an invertible passive
transformation R3 7→R3:
(x,y,z) 7→(u(x,y,z),v(x,y,z),w(x,y,z))
(10.93)
or via the inverse R3 7→R3 transformation
(u,v,w) 7→(x(u,v,w),y(u,v,w),z(u,v,w)).
(10.94)

314
An Introduction to Vectors, Vector Operators and Vector Analysis
A system of curvilinear coordinates is said to be orthogonal if the coordinate surfaces or
equivalently the coordinate lines or the unit vectors tangent to the coordinate lines at their
point of intersection are mutually perpendicular (see Fig. 10.11). Note that there are
different sets of coordinate surfaces, coordinate lines and the orthogonal basis vectors
ˆu, ˆv, ˆw at different points in space. The transformations in Eqs (10.93), (10.94) are
assumed to be C1, that is, having continuous partial derivatives at all points in some
region of space, as well as invertible everywhere in that region, that is, having a non-zero
Jacobian determinant at all points in that region. Such transformations between two sets
of coordinates are called admissible coordinate transformations deﬁned over a given
region of space. Just to refresh our memory, we recall the deﬁnitions of the coordinate
lines and coordinate surfaces. Given any point, one can draw a curve passing through the
point in such a way that only one of the three curvilinear coordinates changes along the
curve while the values of the other two coordinates remain constant. For three curvilinear
coordinates one can draw three such curves, all passing through the given point and
mutually intersecting at right angles. Each is called a coordinate curve or a coordinate line
and a surface passing through the given point on which a particular curvilinear coordinate
has a constant value is called a coordinate surface. The u-coordinate line, for example, is
the curve of intersection of the v and the w coordinate surfaces. All this is depicted in
Fig. 10.11.
Fig. 10.11
The Network of coordinate lines and coordinate surfaces at any arbitary
point, deﬁning a curvilinear coordinate system
Let the equations for the coordinate curves at a point (u0,v0,w0) be
x = x(u,v0,w0) x = x(u0,v,w0) x = x(u0,v0,w).
The tangents to the coordinate curves are given by the vectors
∂x(u,v0,w0)
∂u
, ∂x(u0,v,w0)
∂v
, ∂x(u0,v0,w)
∂w

Functions with Vector Arguments
315
respectively. Orthogonality of these vectors requires that
∂x
∂u · ∂x
∂v = ∂x
∂u · ∂x
∂w = ∂x
∂v · ∂x
∂w = 0.
We are interested in the differential displacement dx as we go from x(u,v,w) to x(u +
du,v + dv,w + dw). We have, in terms of the corresponding Jacobian matrix,
dx =


∂s1
∂u
∂s1
∂v
∂s1
∂w
∂s2
∂u
∂s2
∂v
∂s2
∂w
∂s3
∂u
∂s3
∂v
∂s3
∂w




du
dv
dw

= ∂x
∂u du + ∂x
∂u dv + ∂x
∂wdw = xudu + xvdv + xwdw.
(10.95)
where s1,2,3(u,v,w) = x(u,v,w)·{ˆu, ˆv, ˆw} are the components of x in the ˆu, ˆv, ˆw mutually
orthogonal directions. This deﬁnes the line element ds = |dx| via
ds2 = dx · dx = xu · xudu2 + xv · xvdv2 + xw · xwdw2
= h2
1du2 + h2
2dv2 + h2
3dw2.
(10.96)
The parameters h1,h2,h3 are the analogues of the Gaussian fundamental quantities E,F,G
of a surface, we derived before. They relate the differential displacements along the ˆu, ˆv, ˆw
directions as a result of the displacement dx to the diferentials du,dv,dw via
dx = ds1 ˆu + ds2ˆv + ds3 ˆw = h1du ˆu + h2dv ˆv + h3dw ˆw.
Thus, the volume of the rectangular parallelepiped with sides ds1,ds2,ds3 is given by
dV = ds1ds2ds3 = h1h2h3dudvdw.
The product h1h2h3 ensures that the the last term has the dimension of volume; as the
curvilinear coordinates can be dimensionless quantities like angles.
The u coordinate surface passing through the point (u0,v0,w0) is the collection of
points (x,y,z) satisfying u(x,y,z) = u0 and similarly, the v and w coordinate surfaces
are given by v(x,y,z) = v0 and w(x,y,z) = w0, where (x,y,z) are the Cartesian
coordinates with respect to some rectangular Cartesian coordinate system. We can vary
the
point
(u0,v0,w0)
over
the
region
for
which
the
curvilinear
coordinate
transformations, Eqs (10.93), (10.94), are deﬁned. Therefore, we can replace u0,v0,w0 in
the equations deﬁning the coordinate surfaces by u,v,w and say that a particular triad of
coordinate surfaces emerges when particular values of u, v and w are substituted on the
RHS of these equations. Thus, we write, for the equations deﬁning the coordinate surfaces
u = u(x,y,z) v = v(x,y,z) w = w(x,y,z).

316
An Introduction to Vectors, Vector Operators and Vector Analysis
The normals to the coordinate surfaces are given by ∇u,∇v,∇w (see section 10.8) which,
owing to orthogonality, must satisfy
0 = ∇u · ∇v = ∇u · ∇w = ∇v · ∇w.
The vectors normal to the coordinate surfaces are tangent to the corresponding coordinate
curves so that we can deﬁne the fundamental triad for the curvilinear coordinates as
ˆu = ∇u
|∇u|
ˆv = ∇v
|∇v|
ˆw = ∇w
|∇w|.
(10.97)
Let ds1 = ds1 ˆu, ds2 = ds2ˆv, ds3 = ds3 ˆw be the differential displacement along the
ˆu, ˆv, ˆw directions. Since ∇u,∇v,∇w have the same values in all the orthonormal basis
triads and since ∇u,ds1, ∇v,ds2 and ∇w,ds3 are the pairs of parallel vectors, we can
write,
du = ∇u · ds1 = |∇u||ds1|,
dv = ∇v · ds2 = |∇v||ds2|,
dw = ∇w · ds3 = |∇w||ds3|.
(10.98)
This gives,
ds2 = ds2
1 + ds2
2 + ds2
3 = du2
|∇u|2 + dv2
|∇v|2 + dw2
|∇w|2 .
(10.99)
Comparing equations Eqs (10.96) and (10.99) we get
h1 =
1
|∇u| = √xu · xu impling ˆu = h1∇u,
h2 =
1
|∇v| = √xv · xv impling ˆv = h2∇v,
h3 =
1
|∇w| = √xw · xw impling ˆw = h3∇w.
(10.100)
Example
For spherical polar coordinates we identify u = r, v = θ and w = φ, where
r =
q
x2 + y2 + z2, θ = cos−1


z
p
x2 + y2 + z2

and φ = tan−1 y
x

.

Functions with Vector Arguments
317
This gives h−1
1 = |∇r| = 1, h−1
2 = |∇θ| = r−1 and h−1
3 = |∇φ| = (rsinθ)−1. Therefore,
ds2 = h2
1dr2 + h2
2dθ2 + h2
3dφ2 = dr2 + r2dθ2 + r2 sin2 θdφ2.
Also, the fundamental triad is,
ˆu = ˆr, ˆv = ˆθ, ˆw = ˆφ.
We can also invert the transformation to get
x = r sinθ cosφ y = r sinθ sinφ z = r cosθ.
This gives (Exercise),
h1 =
s dx
dr
!2
+
 dy
dr
!2
+
 dz
dr
!2
= 1,
h2 =
s dx
dθ
!2
+
 dy
dθ
!2
+
 dz
dθ
!2
= r,
h3 =
s dx
dφ
!2
+
 dy
dφ
!2
+
 dz
dφ
!2
= r sinθ.
(10.101)
which are identical to those already obtained from forward transformations.
The gradient of a scalar valued function φ(u,v,w) with respect to the Cartesian
coordinates x,y,z can be obtained by applying chain rule:
∇φ = ∂φ
∂u ∇u + ∂φ
∂v ∇v + ∂φ
∂w∇w
=
 1
h1
∂φ
∂u
!
ˆu +
 1
h2
∂φ
∂v
!
ˆv +
 1
h3
∂φ
∂w
!
ˆw,
(10.102)
where we have used Eq. (10.100).
Our next job is to express the divergence ∇· f of a vector ﬁeld f exclusively in terms
of the derivatives with respect to the curvilinear coordinates u,v,w. We can do this, in
principle, by a systematic application of the chain rule, but that will make us go through
a lengthy algebra which only masochists can enjoy. There is a short cut, where we use the
fact that the ﬂux of a vector ﬁeld through a differential volume dV is given by ∇· fdV . In
order to get hold of this quantity, we need to know how to represent the area of a piece of
a surface by a vector. As shown in Fig. 10.12, an element of area da is represented by the
vector da given by
da = ˆnda = daxˆi + dayˆj + daz ˆk
(10.103)

318
An Introduction to Vectors, Vector Operators and Vector Analysis
where ˆn is the unit outward normal to the surface, deﬁned via its direction cosines
ˆn = cosαˆi + cosβˆj + cosγ ˆk.
(10.104)
Therefore, the components of da are given by
dax = dacosα, day = dacosβ daz = dacosγ.
In particular, x · da = |x|dacosθ where θ is the angle between ˆn and x. If θ = 0 or θ = π
then x · da = ±|x|da. This situation arises when the surface is perpendicular to x, that is,
the area of the projection of the surface element on the plane perpendicular to x is the same
as da. The ﬂux of f through an element of area da is f · da.
Fig. 10.12
(a) Evaluating x·da (b) Flux through the opposite faces of a volume element
Let us now consider a differential volume of the shape of a rectangular parallelepiped with
sides ds1,ds2,ds3 deﬁned above so that its volume is dV = ds1ds2ds3 = h1h2h3
dudvdw. Let us assume that the pairs −ˆu, ˆu be the outward normals to the front and the
back sides, −ˆv, ˆv be the outward normals to the left and the right sides and −ˆw, ˆw be the
outward normals to the bottom and the top sides of the box. Then for the front face,
da = −h2h3dvdw ˆu and f · da = −(h2h3f1)dvdw, where f1,2,3 are the components of f

Functions with Vector Arguments
319
along ˆu, ˆv, ˆw respectively and the product h2h3f1 is to be evaluated at u. On the back face,
product h2h3f1 is to be evaluated at u + du so that f · da = (h2h3f1 +
∂
∂u (h2h3f1)
du)dvdw. Therefore, the net ﬂux through the front and back pair of faces is
" ∂
∂u (h2h3f1)
#
dudvdw =
1
h1h2h3
∂
∂u (h2h3f1)dV .
In the same way, the right and the left sides give
1
h1h2h3
∂
∂v (h1h3f2)dV
and the bottom and the top sides contribute
1
h1h2h3
∂
∂w(h1h2f3)dV .
Thus, the total ﬂux through the box is given by
(∇· f)dV =
1
h1h2h3
" ∂
∂u (h2h3f1) + ∂
∂v (h1h3f2) + ∂
∂w(h1h2f3)
#
dV
This gives
∇· f =
1
h1h2h3
" ∂
∂u (h2h3f1) + ∂
∂v (h3h1f2) + ∂
∂w(h1h2f3)
#
.
(10.105)
Combining Eq. (10.105) with Eq. (10.102) we get, for the Laplacian operator,
∇2φ = ∇· ∇φ =
1
h1h2h3
" ∂
∂u
 h2h3
h1
∂φ
∂u
!
+ ∂
∂v
 h3h1
h2
∂φ
∂v
!
+ ∂
∂w
 h1h2
h3
∂φ
∂w
!#
.
(10.106)
Our last task in this subsection is to express the curl ∇× f of a vector ﬁeld f in terms of the
derivatives with respect to the curvilinear coordinates u,v,w. The principle we follow for
this is
Circulation of f around a loop enclosing an inﬁnitesimal area da = (∇×f)·da.
The sense of circulation is given by that which makes a right handed screw advance in the
direction of da. The required circulation can be explicitly calculated for an inﬁnitesimal
loop of rectangular shape. For each side of the rectangle, we have to ﬁnd the scalar
product of f with the vector along the side and in the direction consistent with the sense of
circulation. In the ﬁrst place, the surface enclosed by an inﬁnitesimal loop can be taken to
be a plane. Consider such a rectangular loop in the ˆu, ˆv plane, with ˆw normal to it

320
An Introduction to Vectors, Vector Operators and Vector Analysis
(see Fig. 10.13). From Fig. 10.13 and ˆw pointing out of the page, it is clear that the sense of
circulation which makes a right handed screw advance in ˆw direction is counterclockwise,
as shown. The vector on the side along ˆu is ds1 = h1du ˆu that on the side along ˆv is
ds2 = h2dv ˆv and area
da = h1h2dudv ˆw.
Fig. 10.13
Circulation around a loop
Along the bottom side (along ˆu) the contribution of f to circulation is
f · ds1 = h1f1du
Along the top side, the sign is reversed and h1f1 is evaluated at v + dv rather than v. Both
sides together give

−(h1f1)
v+dv + (h1f1)
v

du = −
" ∂
∂v (h1f1)
#
dudv.
Similarly, the right and the left sides yield
" ∂
∂u (h2f2)
#
dudv.
So, the total circulation is
" ∂
∂u (h2f2) −∂
∂v (h1f1)
#
dudv =
1
h1h2
" ∂
∂u (h2f2) −∂
∂v (h1f1)
#
ˆw · da.
The coefﬁcient of da serves to deﬁne the w component of the curl. Constructing the u and
v components in the same way, we get

Functions with Vector Arguments
321
∇× f =
1
h2h3
" ∂
∂v (h3f3) −∂
∂w(h2f2)
#
ˆu +
1
h1h3
" ∂
∂w(h1f1) −∂
∂u (h3f3)
#
ˆv
+
1
h1h2
" ∂
∂u (h2f2) −∂
∂v (h1f1)
#
ˆw.
(10.107)
This expression for ∇× f can be written in a compact determinantal form as
∇× f =
1
h1h2h3

h1 ˆu
h2ˆv
h3 ˆw
∂
∂u
∂
∂v
∂
∂w
h1f1
h2f2
h3f3

·
(10.108)
Exercise
Express the vector derivatives, that is, gradient, divergence, curl and Laplacian
in terms of (a) spherical polar and (b) cylindrical coordinates for a scalar ﬁeld u(x) and a
vector ﬁeld v(x).
Answer
(a) Gradient:
∇u = ∂u
∂r ˆr + 1
r
∂u
∂θ
ˆθ +
1
r sinθ
∂u
∂φ
ˆφ.
Divergence:
∇· v = 1
r2
∂
∂r (r2vr) +
1
r sinθ
∂
∂θ (sinθvθ) +
1
r sinθ
∂vφ
∂φ .
Curl:
∇× v =
1
r sinθ
" ∂
∂θ (sinθvφ) −∂vθ
∂φ
#
ˆr + 1
r
"
1
sinθ
∂vr
∂φ −∂
∂r (rvφ)
#
ˆθ + 1
r
" ∂
∂r (rvθ) −∂vr
∂θ
#
ˆφ.
Laplacian:
∇2u = 1
r2
∂
∂r
 
r2 ∂u
∂r
!
+
1
r2 sinθ
∂
∂θ
 
sinθ ∂u
∂θ
!
+
1
r2 sin2 θ
∂2u
∂φ2 .
(b) Gradient:
∇u = ∂u
∂ρ ˆρ + 1
ρ
∂u
∂φ
ˆφ + ∂u
∂z ˆz.

322
An Introduction to Vectors, Vector Operators and Vector Analysis
Divergence:
∇· v = 1
ρ
∂
∂ρ(ρvρ) + 1
ρ
∂vφ
∂φ + ∂vz
∂z .
Curl:
∇× v =
 1
ρ
∂vz
∂φ −
∂vφ
∂z
!
ˆρ +
 ∂vρ
∂z −∂vz
∂ρ
!
ˆφ + 1
ρ
 ∂
∂ρ(ρvφ) −
∂vρ
∂φ
!
ˆz.
Laplacian:
∇2u = 1
ρ
∂
∂ρ
 
ρ∂u
∂ρ
!
+ 1
ρ2
∂2u
∂φ2 + ∂2u
∂z2 .
□

11
Vector Integration
In this chapter we learn how to integrate a vector ﬁeld, or a vector valued function f(x),
over x.
Fig. 11.1
Deﬁning the line integral
We are interested in three possibilities. First, the variable of integration, x, can vary over a
continuous region R of volume V in space. Second, x is conﬁned to vary over a piece of a
smooth surface, that is, a surface parameterized by x(u,v) which has continuous partial
derivatives ∂x
∂u and ∂x
∂v. Third, x is constrained to vary over a piece of a smooth curve,
parameterized, say by x(t), which is a continuously differentiable function of t. The ﬁrst
option is called a volume or a triple integral, the second option is called a surface integral
and the last option is called a line integral. We learn about these integrals one by one,
starting with the line integral.
11.1
Line Integrals and Potential Functions
Consider a piece of smooth curve in space, joining points P0 and P1 as shown in Fig. 11.1.
We mark out points x0(≡P0),x1,x2,...,xn(≡P1) on this piece of curve and deﬁne
∆xk = xk −xk−1, k = 1,2,...,n (Fig. 11.1). Let f(xk), k = 1...,n be the values of the
ﬁeld at these points. Then, the line integral of f(x) on this curve is deﬁned as
Z P1
P0
f(x) · dx = lim
n→∞
n
X
k=1
f(xk) · ∆xk
(11.1)

324
An Introduction to Vectors, Vector Operators and Vector Analysis
In the limit as n →∞the vectors ∆xk become tangent to the curve, so we are projecting the
ﬁeld values f(xk) along the tangent at that point to the curve summing the corresponding
products along the curve. Thus, the value of the line integral is inﬂuenced by both the ﬁeld
as well as the curve along which the integral is taken. Later, we will obtain conditions under
which the value of a line integral depends only on the ﬁeld values at the end points and not
on the curve joining them.
The line integral in Eq. (11.1) can be transformed using the fact that the curve is
parameterized by a continuously differentiable function x(t). Let x0 = x(T1) and
xn = x(T2) correspond to the end points P0 and P1 respectively. We choose values
t0 = T1, t1,t2,...,tn = T2 in the closed interval [T1,T2] and let xk = x(tk). We deﬁne
∆xk = ∆x(tk) = x(tk) −x(tk−1) and ∆tk = tk −tk−1. Then the line integral in Eq. (11.1)
gets transformed to
Z T2
T1
f(x(t)) · ˙x(t)dt = lim
n→∞
n
X
k=1
f(x(tk)) ·
 ∆x(tk)
∆tk
!
∆tk
(11.2)
where ˙x(t) = dx(t)
dt
is the velocity or the tangent vector to the curve at the point x(t). If we
resolve the ﬁeld along some ﬁxed orthonormal basis then the line integral becomes
3
X
i=1
Z T2
T1
fi(x1(t),x2(t),x3(t)) ˙xi(t)dt
(11.3)
where f1,2,3 and x1,2,3 are the components of f(x) and x respectively with respect to the
ﬁxed orthonormal basis. In particular, for a scalar valued function f (x) the line integral
becomes
Z T2
T1
f (x1(t),x2(t),x3(t))dt
(11.4)
where x(t) ≡(x2(t),x2(t),x3(t)) is the parametric description of the curve.
Exercise
Let the position vectors of the points P0 and P1 be a and b respectively. Find
R P1
P0 dx.
Solution
Notice that no curve joining P0 and P1 is speciﬁed. In fact it is not necessary.
Whichever way we choose a curve joining P0 and P1 and construct the set {∆xi}, all the
vectors ∆xi add up to the vector b −a (see Fig. 11.1). The integral is
Z P1
P0
dx = b −a,
whose value depends only on the end points and not on the path connecting them.
□

Vector Integration
325
A piece of a curve of ﬁnite length, parameterized by a continuously differentiable function
x(t) is called a smooth arc. We assume that the arc is oriented such that increasing t
makes the corresponding point on the curve move from P0 towards P1. Such an arc is said
to be positively oriented. If the orientation is reverse, the arc is said to be negatively
oriented. We call such an arc a smooth oriented arc. If we denote by Γ (−Γ ) a positively
(negatively) oriented arc, then the corresponding line integrals change sign:
Z
Γ
f · dx =
Z P1
P0
f · dx = −
Z P0
P1
f · dx = −
Z
−Γ
f · dx.
The curve C over which we want to integrate a vector ﬁeld f(x) may consist of many
smooth oriented arcs C1,C2,...CN joined at their end points where their derivatives may
not match, so that the whole path can be parameterized by continuous functions with
ﬁnite jump discontinuities in the derivative at ﬁnite number of points where the smooth
arcs join. In such a case we can write
Z
C
f(x) · dx =
Z
C1
f(x) · dx +
Z
C2
f(x) · dx + ··· +
Z
CN
f(x) · dx.
(11.5)
Another possibility is that C is a closed curve. We assume that the curve is oriented
counterclockwise as the parameter t increases.
Exercise
Evaluate the integral in Eq. (11.2) for the planar ﬁeld f(x) = −yˆi −xyˆj on the
circular arc C shown in Fig. 11.2 from P0 to P1.
Solution
We parameterize C by x(t) = costˆi + sintˆj, 0 ≤t ≤π/2. Therefore,
f(x(t)) = −sintˆi −cost sintˆj.
Differentiating x(t) we get ˙x(t) = −sintˆi + costˆj. Therefore the integral becomes
Z T2
T1
f(x(t)) · ˙x(t)dt =
Z π/2
0
(sin2 t −cos2 t sint)dt = π
4 −1
3 = 0.4521.
□
Fig. 11.2
x(t) = costˆi + sintˆj

326
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Evaluate the integral in Eq. (11.2) for the ﬁeld f(x) = zˆi + xˆj + y ˆk on the helix
C shown in Fig. 11.3,
x(t) = costˆi + sintˆj + 3t ˆk 0 ≤t ≤2π
from P0 to P1.
Fig. 11.3
A circular helix
Solution
f(x(t)) · ˙x(t) = −3t sint + cos2 t + 3sint Hence the required integral is
Z 2π
0
(−3t sint + cos2 t + 3sint)dt = 7π ≈21.99.
□
Exercise
Find the work done by the electrostatic ﬁeld due to a point charge q on a test
charge as it traverses the paths shown in Fig. 11.4(a) and (b).
Fig. 11.4
In carrying a test charge from a to b the same work is done along either
path
Hint
E =
1
4πϵ0
q
r2 ˆr where r is the radial distance of the test charge from the source q. Work
done along the circular arcs is zero.

Vector Integration
327
Answer
W = −
Z b
a
E · ds =
q
4πϵ0
 1
ra
−1
rb
!
for both the paths.
□
Exercise
For the ﬁeld f(x) = xyˆi + (x2 + y2)ˆj ﬁnd
R
Γ f(x) · dx where Γ is
(i) The arc y = x2 −4 from (2,0) to (5,21) and
(ii) The x-axis from x = 2 to x = 5 and then the line x = 5 from y = 0 to y = 21.
Solution
Z
Γ
f(x) · dx =
Z
Γ
[xydx + (x2 + y2)dy].
(i) Along Γ y = x2 −4 or x2 = y + 4. We substitute for y in the ﬁrst term and for x in
the second term of the integrand to get
Z
Γ
f(x) · dx =
Z 5
2
(x3 −4x)dx +
Z 21
0
(y2 + y + 4)dy = 3501.75
(ii) Along the x axis y = 0 = dy and along the vertical line x = 5 and dx = 0. This gives
Z
Γ
f(x) · dx =
Z 21
0
(25 + y2)dy = 3612.
We see that two values do not agree, so that the integral depends on the path. Thus, as
explained below, the ﬁeld is not conservative.
□
Sometimes we may have to evaluate the line integral separately on different parts of the
given curve, as the following exercise shows.
Exercise
Evaluate
R
Γ f · dx, where f = xˆj −yˆi and Γ is the unit circle about the origin.
Solution
We note that f · dx = xdy −ydx. We can parameterize the unit circle by x as
y2 = 1 −x2, but then y is not a single valued function of x. We can circumvent this by
viewing the curve as made up of two parts (see Fig. 11.5), Γ1 and Γ2 where Γ1 is the upper
semi-circle and Γ2 the lower, arrows indicating the positive direction along Γ as shown in
Fig. 11.5.
On Γ1:
y =
√
1 −x2, dy =
−xdx
√
1 −x2
and
on Γ2:
y = −
√
1 −x2, dy =
xdx
√
1 −x2 .

328
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 11.5
Line integral over a unit circle
Therefore, the required integral is
Z
Γ
f · dx =
Z
Γ1
−x2
√
1 −x2 dx −
Z
Γ1
√
1 −x2dx +
Z
Γ2
x2
√
1 −x2 dx +
Z
Γ2
√
1 −x2dx
=
Z −1
1
 
−x2
√
1 −x2 −
√
1 −x2
!
dx +
Z 1
−1
 
x2
√
1 −x2 +
√
1 −x2
!
dx
= 2π.
□
The following three rules for evaluation of line integrals can be easily checked.
(i)
R
C(kf) · dx = k
R
C f · dx. where k is a (scalar) constant.
(ii) For two vector ﬁelds f and g
Z
C
(f + g) · dx =
Z
C
f · dx +
Z
C
g · dx.
(iii) Any two parameterizations of C giving the same orientation on C yield the same value
of the line integral Eq. (11.1).
Exercise
Prove rule (iii).
Solution
Let the curve C be parameterized by x(t),
a ≤t ≤b and also by
x∗(t∗), a∗≤t∗≤b∗and let these be related by t = φ(t∗). We are given that dt
dt∗> 0. Thus,
x(t) = x(φ(t∗)) = x∗(t∗) and dt = (dt/dt∗)dt∗. Therefore, the line integral over C can
be written
Z
C
f(x∗) · dx∗=
Z b∗
a∗f(x(φ(t∗))) · dx
dt
dt
dt∗dt∗=
Z b
a
f(x(t)) · dx
dt dt =
Z
C
f(x) · dx.

Vector Integration
329
Note that f(x(t)) and f(x(φ(t∗))) are different functions of their arguments but their
values match at t and t∗satisfying t = φ(t∗), both corresponding to the same point P on
the curve of integration.
□
We now give two results often used while evaluating line integrals. Let {ˆi,ˆj, ˆk} be an
orthonormal basis and {x,y,z} be the corresponding Cartesian coordinate system. Let a
vector ﬁeld f(x) have components f1,2,3(x) along ˆi,ˆj, ˆk respectively and let Γ be some
smooth curve in space.
(i) We can write
Z
Γ
f(x) · dx =
Z
Γ
(f1(x)ˆi + f2(x)ˆj + f3(x)ˆk) · (dxˆi + dyˆj + dz ˆk)
=
Z
Γ
f1(x)dx +
Z
Γ
f2(x)dy +
Z
Γ
f3(x)dz
(11.6)
where we have used the orthonormality of the basis. Thus, a line integral over a vector
ﬁeld along a curve Γ is the sum of the line integrals over the components of the ﬁeld
along Γ .
Exercise
Integrate the ﬁeld
f(x) = x2y2ˆi + yˆj + zy ˆk
along the curve y2 = 4x from (0,0) to (4,4).
Solution
Note that the curve is on the xy plane and z = 0 = dz along the curve.
We have,
Z
Γ
f(x) · dx =
Z
Γ
(x2y2ˆi + yˆj) · (dxˆi + dyˆj)
=
Z
Γ
x2y2dx +
Z
Γ
ydy
Along the curve, y2 = 4x so that
Z
Γ
x2y2dx =
Z 4
0
4x3dx = 256
and
Z 4
0
ydy = 8.

330
An Introduction to Vectors, Vector Operators and Vector Analysis
Therefore,
Z
Γ
f(x) · dx = 264.
□
(ii) Now let Γ be a smooth and simple closed curve oriented positively, that is,
counterclockwise. Let Γ1,Γ2,Γ3 be the projections of Γ on xy,yz and zx planes
respectively, all oriented positively. Thus, in Fig. 11.6, Γ is the oriented curve ABCA,
Γ1 is oriented as OABO, Γ2 as OBCO and Γ3 as OCAO. We have,
Z
Γ1
f · dx +
Z
Γ2
f · dx +
Z
Γ3
f · dx =
Z
AB,BO,OA
f · dx +
Z
BC,CO,OB
f · dx +
Z
CA,AO,OC
f · dx
=
Z
AB
f · dx +
Z
BC
f · dx +
Z
CA
f · dx
=
Z
Γ
f · dx
(11.7)
because all integrals except those on the arcs of Γ cancel as each of them is traversed
twice in opposite directions (see Fig. 11.6). Equation (11.7) is always valid whenever
Γ is a simple closed curve.
Fig. 11.6
Line integral around a simple closed curve as the sum of the line integrals
over its projections on the coordinate planes
These two observations come in handy while evaluating line integrals.
From Eq. (11.2) and the exercises following it, we see that after substituting the
parameterization x(t) in the integrand of a line integral, it becomes a scalar valued
function of a scalar variable t, say f (t). Let a point P on the curve of integration

Vector Integration
331
correspond to the parameter value t. We can deﬁne a function F(P ) = F(t), as a scalar
valued function of a scalar variable t, by the indeﬁnite integral
F(P ) = F(t) =
Z t
T0
f (t)dt =
Z t
T0
f(x(t)) · ˙x(t)dt,
This gives,
dF(t) = dF
dt dt = (f1(x(t))dx
dt + f2(x(t))dy
dt + f3(x(t))dz
dt )dt.
(11.8)
where f1,2,3(x(t)) are the components of f(x(t)) at the point P corresponding to t on the
curve joining P0 and P1 along which the line integral is evaluated. Thus, for any two points
P and P ′ on the curve of integration we can write, by elementary integration,
Z P ′
P
dF = F(P ′) −F(P ) = F(t′) −F(t).
(11.9)
where t′ and t are the parameter values corresponding to P ′ and P respectively. Here, we
assume that t′ > t so that the sense of traversal from P to P ′ gives the orientation of the
curve of integration.
We emphasize that the differential dF(t) is that of a scalar valued function of a single
scalar variable t. Function F(t) depends on the parameterization x(t) and hence on the
curve joining P0 and P1 along which the integration is carried out. Therefore, the value
of the integral essentially depends on the curve of integration. Equations (11.8), (11.9) are
completely general and every line integral can be expressed as in Eq. (11.9).
Taking cue from the above observations we can deﬁne what is called a Linear
Differential Form at all points in the domain of the ﬁeld f(x) (and not necessarily along
some curve) as
L = A(x)dx + B(x)dy + C(x)dz = f(x) · dx
(11.10)
where A(x),B(x),C(x) are the scalar valued functions giving components of f(x) in all
of its domain. This is called a Linear Differential Form because of its linear dependence
on the differentials dx,dy,dz while their coefﬁcients are functions of x. The advantage of
introducing the differential form L is that along a curve parameterized say by t, it naturally
reduces to the differential of a scalar valued function. Thus, every line integral along a curve
joining two points say P0 and P has the form
F(P ) =
Z P
P0
L =
Z t
T0
 
A(x(t))dx
dt + B(x(t))dy
dt + C(x(t))dz
dt
!
dt = F(t)
(11.11)
We will assume that the functions A(x),B(x),C(x) are C1, that is, they have continuous
ﬁrst derivatives throughout the domain of the ﬁeld f(x).

332
An Introduction to Vectors, Vector Operators and Vector Analysis
We are interested in ﬁnding out the class of ﬁelds, the value of whose line integral
depends only on the end points irrespective of the curve joining the end points used to
evaluate the integral. This happens when the ﬁeld is conservative, that is, the ﬁeld is the
gradient of a potential φ(x) so that
f(x) = ∇φ(x)
at all points at which f(x) is deﬁned. Then using Eq. (10.27) we can write
Z
C
f(x)·dx =
Z T2
T1
∇φ(x(t))·˙x(t)dt =
Z T2
T1
dφ
dt dt = φ(x(t))

T2
T1 = φ(P1) −φ(P0).
Thus, if the ﬁeld is the gradient of a potential, then its line integral depends only on the
values of the potential at the end points, independent of the curve joining the end points.
It turns out that the reverse implication is also true. That is, a vector ﬁeld whose line
integral over a smooth arc joining any two points in its domain depends only on its end
points then it must be conservative, that is, it must be the gradient of some scalar ﬁeld
φ(x). A vector ﬁeld being the gradient of some scalar ﬁeld is equivalent to its linear form
being a perfect differential, that is, there is a scalar valued function φ(x) satisfying
L = f · dx = ∇φ(x) · dx = ∂φ(x)
∂x
dx + ∂φ(x)
∂y
dy + ∂φ(x)
∂z
dz = dφ.
(11.12)
Note that we require this equation to be valid at every point in the domain of f and not
only at the points on some curve in the domain. The RHS of Eq. (11.12) is easily
recognized as the differential dφ of the scalar valued function φ. Now assume that the
line integral of f over some smooth oriented arc Γ depends only on the end points of Γ .
We want to show that there is a scalar function φ(x) deﬁned on the domain of f such that
dφ = L where L = A(x)dx + B(x)dy + C(x)dz is the linear differential form giving the
integrand of the line integral. Without losing generality we can assume that any two points
in the domain can be connected by a smooth oriented arc. We ﬁx a point P0 in the domain
and deﬁne the function φ(x) = φ(P ) at any point P as the value of the line integral over
any smooth oriented (from P0 to P ) curve joining P0 and P . To get the partial derivatives
of φ consider any point (x,y,z) ≡P and a smooth oriented curve, say Γ , joining P0 and P .
Since the domain is an open set, all points (x + ∆x,y,z) = P ′ are in the domain, provided
|∆x| is sufﬁciently small. Let γ be the oriented straight line segment joining P and P ′ (see
Fig. 11.7). We can arrange, without losing generality, that the curve Γ + γ is a simple
oriented polygonal arc without any knots and overlaps with initial point P0 and ﬁnal point
P ′. It follows, then, by Eq. (11.5) that
φ(x + ∆x,y,z) −φ(x,y,z) = φ(P ′) −φ(P ) =
Z
Γ +γ
L −
Z
Γ
L =
Z
γ
L
=
Z x+∆x
x
A(t,y,z)dt = A(x,y,z)∆x
(11.13)

Vector Integration
333
Dividing by ∆x and passing to the limit as ∆x →0 we ﬁnd that
∂φ
∂x = A
and similarly, ∂φ
∂y = B and ∂φ
∂z = C. This shows that dφ = L as we wanted.
Fig. 11.7
Illustrating Eq. (11.13)
Exercise
Show that a vector ﬁeld is conservative, if and only if its line integral over every
closed loop is zero.
□
We have proved that the conservative property of a vector ﬁeld and dependence of its line
integral only on the end points of the curve of integration are equivalent. However, this
result is not of much practical value unless we ﬁnd out some independent criteria to
determine whether a given vector ﬁeld is conservative or not. Equivalently, we have to ﬁnd
out whether a given differential form L is a perfect differential or not, that is, whether
there is a function φ(x) satisfying L = ∇φ · dx.
The necessary condition for a vector ﬁeld to be conservative is that its curl vanishes
everywhere in its domain. Since the ﬁeld is given to be conservative, we have,
∇× f(x) = ∇× ∇φ(x) = 0
(11.14)
for all x in the domain of f(x) because we have shown before that the curl of a gradient is
always zero (see Eq. (10.90)). It is useful to state this necessary condition in terms of the
linear differential form which, for a conservative ﬁeld, ought to be a perfect differential:
L = A(x)dx + B(x)dy + C(x)dz = ∇φ(x) · dx = ∂φ
∂x dx + ∂φ
∂y dy + ∂φ
∂z dz,
which means,
A = ∂φ
∂x , B = ∂φ
∂y C = ∂φ
∂z .

334
An Introduction to Vectors, Vector Operators and Vector Analysis
Suitably differentiating both sides of these equations and assuming that the order of
differentiation does not matter, we get the following necessary conditions for a vector ﬁeld
to be conservative.
Bz −Cy = ∂B
∂z −∂C
∂y = 0, Cx −Az = ∂C
∂x −∂A
∂z = 0, Ay −Bx = ∂A
∂y −∂B
∂x = 0.
(11.15)
Exercise
Show that Eqs (11.14) and (11.15) are equivalent.
□
Now the question is whether the condition given by Eq. (11.14) or equivalently by
Eq. (11.15) is sufﬁcient for a vector ﬁeld to be conservative. That is, given that a vector
ﬁeld satisﬁes Eq. (11.14) or Eq. (11.15), is it conservative? It turns out that unless the
domain of deﬁnition of the ﬁeld (or the corresponding differential form) is simply
connected (to be explained below) all the line integrals of the ﬁeld (or the differential
form) are not independent of path (or are not zero over every closed path) in the domain,
even if the ﬁeld satisﬁes Eq. (11.14) or Eq. (11.15) in its domain. Simple connectivity
means that a smooth curve joining any two points in the domain can be continuously
deformed within the domain to coincide with any other smooth curve with the same end
points. Equivalently, every simple closed curve in a simply connected domain can be
continuously shrunk to any one of its interior points always staying within the domain.
This is not possible if the domain has ‘holes’ in it, that is, if the ﬁeld is not deﬁned in some
region within the domain. Thus, domains with holes are not simply connected.
Before ﬁnding out in detail what is meant by a connected or a simply connected set
of points in space, we give an example to show that the conditions Eq. (11.15) are not by
themselves sufﬁcient to ensure the path independence of
R
L, that is, to ensure that
R
L
taken over every closed curve is zero. Consider the differential
L = xdy −ydx
x2 + y2
with the coefﬁcients
A =
−y
x2 + y2 , B =
x
x2 + y2 , C = 0,
which are deﬁned except for points on the z-axis (x = y = 0). Thus, the domain of
deﬁnition of this differential form, or the corresponding ﬁeld, is all space except z-axis.
We show below that this differential form satisﬁes Eq. (11.15) and is a perfect differential
but there exists a class of simple closed curves in its domain such that the integral of this
differential form around such a curve does not vanish. In order to see that this is a perfect
differential, we introduce the polar angle θ of a point P (x,y,z) by
cosθ =
x
p
x2 + y2 , sinθ =
y
p
x2 + y2

Vector Integration
335
that is, the angle formed with the x,z-plane by the plane through P and passing through
the z-axis. Then,
dθ = d tan−1 y
x = L,
so that L is represented as the total differential of the function u = θ. We get
Z
C
L =
Z 2π
0
dθ = 2π , 0
with C as any closed loop in the x,y-plane surrounding the z-axis oriented positively with
respect to θ. Thus,
R
L , 0 over a closed loop in the domain even if L is a perfect
differential there. The problem is that the inverse trigonometric functions are not single
valued: They determine the values of θ only within the integral multiples of 2π. This fact
is connected with the closed curve C of integration via
Z
C
L =
Z
C
dθ = θ + 2nπ
where n is the number of times the closed curve of integration winds around the z axis:
Each winding adds up 2π on the RHS of the above equation (see Fig. 11.8).
Fig. 11.8
Each winding of the curve of integration around the z axis adds 2π to its
value
Therefore, the value of
R P
P0 dθ taken for two different paths with end points P0,P is the same
only if going along one path from P0 to P and returning along the other path to P0 we go
zero times around the z-axis. We can avoid any path going around the z-axis by avoiding
all paths crossing the half plane y = 0, x ≤0, that is, we remove this half plane from the
region R over which the ﬁeld is deﬁned. To every point on the allowed path we can assign
a unique value of θ with −π < θ < π. Therefore, the integral
R P
P0 dθ has a unique value
θ(P ) −θ(P0), which does not depend on a particular path. Similarly, the integral over a
closed path in this region has value zero.

336
An Introduction to Vectors, Vector Operators and Vector Analysis
Simply connected sets
An open set R in space is said to be connected if every pair P0,P1 of distinct points in
it can be connected by a smooth arc wholly within R. Such an arc is parameterized by
a triplet of continuously differentiable functions (x(t),y(t),z(t)), 0 ≤t ≤1 ; the point
P (t) = (x(t),y(t),z(t)) lies in R for all t, coincides with P0 for t = 0 and with P1 for
t = 1. Obviously, in a connected set, any two points can also be joined by a path comprising
a string of smooth arcs joined at their end points.
Examples of connected sets are the convex sets R any two of whose points P ′ and P ′′
can be joined by a line segment in R. The corresponding linear paths joining P ′(x′,y′,z′)
and P ′′(x′′,y′′,z′′) are simply the triple of linear functions
x(t) = (1 −t)x′ + tx′′ y(t) = (1 −t)y′ + ty′′ z(t) = (1 −t)z′ + tz′′
for 0 ≤t ≤1.
Examples of convex sets are solid spheres or cubes. Examples of connected but not
convex sets are solid torus, a spherical shell (space between two concentric spheres) and
the outside of a sphere or cylinder. A set R which is not connected consists of connected
subsets called the components of R. Examples of disconnected sets are the set of points
not belonging to a spherical shell, or a set of points none of whose coordinates are
integers.
Now let C0 and C1 be any two paths in R, given by (x0(t),y0(t),z0(t)) and
(x1(t),y1(t),z1(t)) respectively. Let their end points P ′ and P ′′, corresponding to t = 0
and t = 1 respectively, be the same. The connected set R is simply connected, if we can
deform C0 into C1 by means of a continuous family of paths Cλ with common end points
P ′,P ′′. This means that there exist continuous functions (x(t,λ),y(t,λ),z(t,λ)) of the
two variables t,λ for 0 ≤t ≤1, 0 ≤λ ≤1 such that the point P (t,λ) = (x(t,λ),
y(t,λ),z(t,λ)) always lies in R and such that P (t,λ = 0) coincides with P (t) = (x0(t),
y0(t),z0(t)), P (t,λ = 1) coincides with P (t) = (x1(t),y1(t),z1(t)), P (t = 0,λ)
coincides with P ′ and P (t = 1,λ) coincides with P ′′. For each ﬁxed λ the functions
(x(t,λ), y(t,λ),z(t,λ)) determine a path Cλ in R that joins the end points P ′ and P ′′.
As λ varies from 0 to 1, the path Cλ changes continuously from C0 to C1. This deﬁnes the
“continuous deformation” of C0 into C1 (see Fig. 11.9).
As can be easily seen, convex sets are simply connected. The family of curves Cλ
continuously deforming C0 to C1, all curves with common end points P ′,P ′′, is given by
x(t,λ) = (1 −λ)x0(t) + λx1(t),
y(t,λ) = (1 −λ)y0(t) + λy1(t),
z(t,λ) = (1 −λ)z0(t) + λz1(t).

Vector Integration
337
Fig. 11.9
Illustration of a simply connected domain
Thus, Cλ is obtained by joining the points of C0 and C1 that belong to the same t by a line
segment and taking the point that divides the segment in the ratio
λ
1−λ. The points obtained
in this way all lie in R because of its convexity. A different type of simply connected set is
given by a spherical shell. A region R in space obtained after removing the z-axis is not
simply connected because the two semicircular paths
x = cosπt, y = sinπt, z = 0; 0 ≤t ≤1
and
x = cosπt, y = −sinπt, z = 0; 0 ≤t ≤1
have the same end points but cannot be deformed into each other without crossing the
z-axis.
We shall now prove the following theorem:
If the coefﬁcients of the differential form corresponding to the ﬁeld f given by
L = f(x) · dx = A(x)dx + B(x)dy + C(x)dz
have continuous ﬁrst derivatives in a simply connected domain R and satisfy conditions
Eq. (11.15), namely,
Bz −Cy = 0, Cx −Az = 0, Ay −Bx = 0,
then L is the total (perfect) differential of a function φ in R:
A = φx, B = φy C = φz.
It is enough to prove that
R P ′′
P ′ L over any simple polygonal arc joining P ′ and P ′′ has a value
that depends only on P ′ and P ′′. We represent two oriented arcs C0 and C1 parametrically

338
An Introduction to Vectors, Vector Operators and Vector Analysis
by (x0(t),y0(t),z0(t)) and (x1(t),y1(t),z1(t)), 0 ≤t ≤1 respectively with t = 0 yielding
P ′ and t = 1 yielding P ′′. Using the simple connectivity of R we can imbed paths C0
and C1 into a continuous family (x(t,λ),y(t,λ),z(t,λ)) reducing to (x0(t),y0(t),z0(t))
and (x1(t),y1(t),z1(t)) for λ = 0,1 respectively and to P ′,P ′′ for t = 0,1 respectively.
We have, for the integral around the loop,
Z
C1
L −
Z
C0
L =
Z 1
0
[(Axt + Byt + Czt)
λ=1 −(Axt + Byt + Czt)
λ=0]dt,
where (x,y,z) are the functions of t,λ forming the continuous family of paths. We assume
that these functions have continuous ﬁrst and mixed second derivatives with respect to t
and λ for 0 ≤t ≤1 and 0 ≤λ ≤1. Then by elementary integration,
Z
C1
L −
Z
C0
L =
Z 1
0
dt
Z 1
0
(Axt + Byt + Czt)λdλ.
Now, using the chain rule and the conditions Eq. (11.15) we get the identity
(Axt + Byt + Czt)λ = Axλt + Byλt + Czλt + Axxλxt + Ayyλxt + Azzλxt
+Bxxλyt + Byyλyt + Bzzλyt + Cxxλzt + Cyyλzt + Czzλzt
= (Axλ + Byλ + Czλ)t
(11.16)
Interchanging orders of integration we ﬁnd
Z
C1
L −
Z
C0
L =
Z 1
0
dλ
Z 1
0
(Axλ + Byλ + Czλ)tdt = 0,
since xλ,yλ,zλ vanish for t = 0,1 because the end points are independent of λ. This
completes the proof.
We see the important part played by the assumption that the region R is simply
connected: It enables us to convert the difference of the line integrals into a double integral
over some intermediate region. The above proof can be extended to the case where the
intermediate paths are continuous but may not be differentiable with respect to λ and also
to the case where C0 and C1 are only sectionally smooth, that is, polygonal arcs.
Exercise
Find out whether the ﬁeld f(x) = exyˆi + ex+yˆj is conservative.
Solution
The coefﬁcients in the linear form are A(x,y) = exy and B(x,y) = ex+y and
conditions Eq. (11.15) reduce to ∂A
∂y = ∂B
∂x . Evaluating both sides we see that they are not
equal. Hence, the ﬁeld is not conservative.
□
Exercise
Find whether the following ﬁelds are conservative.

Vector Integration
339
(i) f(x) = cosyˆi −xsinyˆj −cosz ˆk.
(ii) f(x) = xyˆi + (x2 + y2)ˆj.
(iii) f(x) = (x2 −y2)ˆi + xyˆj.
Hint
Directly evaluate ∇× f.
□
Exercise
Let the ﬁeld be f(x) = 2xyˆi + (x2 + 3y2)ˆj. Find whether the ﬁeld is
conservative and if it is, ﬁnd the potential function.
Solution
Evaluating Ay and Bx we ﬁnd that they are equal, so the potential function say
φ(x,y) may exist. To ﬁnd it, we ﬁrst evaluate the integral
R
A(x,y) dx =
R ∂φ
∂x dx =
R
2xydx keeping y constant to get x2y for the indeﬁnite integral. We must now ﬁnd a
function u(y) such that
∂
∂y (x2y + u(y)) = x2 + 3y2.
Differentiating and simplifying, this equation leads to
du
dy = 3y2.
Integrating, we ﬁnd u(y) = y3. This gives, for the potential function,
φ(x,y) = x2y + y3.
□
Exercise
Show that f(x) = (siny + z)ˆi −(xcosy −z)ˆj −(x −y)ˆk is conservative and
ﬁnd the function φ such that f(x) = ∇φ.
Solution
We check that ∇× f = 0 so that this ﬁeld is conservative.
To ﬁnd a potential φ we equate the components of f = ∇φ. We get
(i) fx = ∂φ
∂x = siny + z,
(ii) fy = ∂φ
∂y = xcosy −z,
(iii) fz = ∂φ
∂z = x −y.
Integrating fx,fy,fz with respect to x,y,z respectively, we obtain
(iv) φ = xsiny + xz + f (y,z),
(v) φ = xsiny −yz + g(x,z),
(vi) φ = xz −yz + h(x,y).

340
An Introduction to Vectors, Vector Operators and Vector Analysis
Since the derivatives are partial derivatives, the “constants” of integration are functions of
variables which are not integrated over. Note that (iv),(v),(vi) each represent φ. Therefore,
f (y,z) must occur in (v). The only possibility is to identify f (y,z) with −yz plus some
function of z but not involving x. By (vi) we see that f (y,z) must simply be −yz + C
where C is a constant. Thus,
φ = xsiny + xz −yz + C.
□
Exercise
Let f(x) = x2ˆi + xyˆj. and let the path C consist of the segment of the parabola
y = x2 between (0,0) and (1,1) (C1) and the line segment from (1,1) to (0,0) (C2) (see
Fig. 11.10). Find
R
C f(x) · dx.
Fig. 11.10
The closed loop for integration
Hint
Parameterize C1 by x(t) = tˆi + t2ˆj and C2 by x(t) = (1 −t)ˆi + (1 −t)ˆj. Evaluate
the integral separately on C1 and C2 and add.
Answer
Z
C
f(x) · dx = 1
15. Thus, the ﬁeld is not conservative.
□
Exercise
Find the potential function for the centrifugal force ﬁeld f(r) = m(ω × r) × ω
where ω is the rotational velocity of a frame rotating with respect to an inertial frame.
Solution
The required potential is 1
2m(ω × r) · (ω × r) = 1
2m|ω × r|2 which can be seen
as follows. Using identity II we get,
1
2m∇((ω × r) · (ω × r)) = 1
2m∇(ω2r2 −(ω · r)2) = m(ω2r −(ω · r)ω)

Vector Integration
341
which, using identity I reduces to
1
2m∇((ω × r) · (ω × r)) = mω × (r × ω) = m(ω × r) × ω.
□
In general, a vector ﬁeld is produced by given sources and the relation between the ﬁelds
and their sources is formulated in terms of a system of differential equations in which the
sources are given and the ﬁelds are the solutions of these differential equations. The most
celebrated examples are the Maxwell’s equations giving electric and magnetic ﬁelds
produced by the given distributions of charges and currents and the Navier–Stokes
equations giving velocity ﬁeld of an imperfect ﬂuid for given distribution of pressure,
viscosity, shear, external forces etc. For the conservative ﬁelds these differential equations
can be transformed into equations for the potential, which are easier to handle, as they
deal with scalar ﬁelds. For example, the equation for the electrostatic ﬁeld ∇· E = ρ/ϵ0
gets transformed to Poisson’s equation ∇2φ = −ρ/ϵ0 where φ is the electrostatic
potential satisfying E = −∇φ.
11.1.1
Curl of a vector ﬁeld and the line integral
We express the curl of a vector ﬁeld f(x) in terms of its line integral over a simple closed
curve Γ in a plane with unit normal ˆn. Let S be the (planar) area enclosed by Γ and P be a
point interior to or on Γ . We deﬁne a number Gn by
Gn = lim
Γ →P
1
S
Z
Γ
f(x) · dx
where the integration is taken in the positive (counterclockwise) sense. Note that, in
general, this integral depends on the direction ˆn because if we change ˆn (by rotating the
plane say) the integrand and hence the integral will change. The limit Γ →P requires that
every point of Γ approaches P . If this limit exists, then Gn is independent of Γ . As we show
below, if Γ is a planar curve and f(x) has Taylor series expansion around P , then the limit
exists and is independent of Γ .
We choose the origin at P and let a point on Γ have position vector x relative to the
origin at P . We expand f(x) around P that is, around 0. We get
f(x) = f(0) + x · ∇f(0) + R,
(11.17)
where R is the remainder containing all the second and higher order terms (x·∇)2 ··· (see
Eq. (10.22)). We set up a rectangular Cartesian coordinate system (ξ,η,ζ) with its origin
at P such that (ξ,η) plane contains Γ (see Fig. 11.11). The vector x has the components
(ξ,η,ζ) and let the components of f(x) be fξ(x),fη(x),fζ(x). This gives
x · ∇fξ(x) = ξ ∂fξ
∂ξ + η ∂fξ
∂η + ζ ∂fξ
∂ζ

342
An Introduction to Vectors, Vector Operators and Vector Analysis
and similarly for x · ∇fη(x) and x · ∇fζ(x). Then,
x · ∇f =


x · ∇fξ(x)
x · ∇fη(x)
x · ∇fζ(x)


=


ξ ∂fξ
∂ξ + η ∂fξ
∂η + ζ ∂fξ
∂ζ
ξ
∂fη
∂ξ + η
∂fη
∂η + ζ
∂fη
∂ζ
ξ ∂fζ
∂ξ + η ∂fζ
∂η + ζ ∂fζ
∂ζ


Fig. 11.11
The geometry of Eq. (11.17)
where all the partial derivatives are evaluated at the origin. Along Γ , dx ≡(dξ,dη,0) so
that
(x · ∇f) · dx =
 
ξ ∂fξ
∂ξ + η ∂fξ
∂η
!
dξ +
 
ξ
∂fη
∂ξ + η
∂fη
∂η
!
dη.
Therefore, dotting Eq. (11.17) with dx we get
f(x) · dx = f(0) · dx +
 
ξ ∂fξ
∂ξ + η ∂fξ
∂η
!
dξ +
 
ξ
∂fη
∂ξ + η
∂fη
∂η
!
dη + R · dx.
Integrating along Γ we get
Z
Γ
f(x) · dx = f(0) ·
Z
Γ
dx + ∂fξ
∂ξ
Z
Γ
ξdξ + ∂fξ
∂η
Z
Γ
ηdξ
+
∂fη
∂ξ
Z
Γ
ξdη +
∂fη
∂η
Z
Γ
ηdη +
Z
Γ
R · dx.
(11.18)
Exercise
Show that
Z
Γ
dx = 0

Vector Integration
343
Z
Γ
ξdξ = 0 = −
Z
Γ
ηdξ
and
Z
Γ
ξdη = −
Z
Γ
ηdξ = S
where S is the area enclosed by Γ .
□
Thus, Eq. (11.18) reduces to (Fig. 11.11)
1
S
Z
Γ
f(x) · dx =
 ∂fη
∂ξ −∂fξ
∂η
!
+ 1
S
Z
Γ
R · dx.
(11.19)
In the last term the integral is of the order of |x|3 as R is of the order of |x|2. Therefore, the
last term is of the order of |x| and vanishes in the limit Γ →P or |x| →0. Therefore,
Gn = lim
Γ →P
1
S
Z
Γ
f(x) · dx =
∂fη
∂ξ −∂fξ
∂η
This limit depends only on the derivatives of f evaluated at P and is independent of Γ .
Now let Γ be a planar curve in a plane deﬁned by the normal direction ˆn and let Γ1,Γ2,Γ3
be the projections of Γ on the xy,yz,zx planes of the coordinate system corresponding to
an orthonormal basis (ˆi,ˆj, ˆk). We know that (see Eq. (11.11)),
Z
Γ
f · dx =
Z
Γ1
f · dx +
Z
Γ2
f · dx +
Z
Γ3
f · dx.
The areas of projections are given by
S1 = Sˆi · ˆn S2 = Sˆj · ˆn S3 = S ˆk · ˆn
where ˆi · ˆn etc are the direction cosines of ˆn. Hence,
1
S
Z
Γ
f(x) · dx =
ˆi · ˆn
S1
Z
Γ1
f(x) · dx +
ˆj · ˆn
S2
Z
Γ2
f(x) · dx +
ˆk · ˆn
S3
Z
Γ3
f(x) · dx.
In the limit as Γ →P we get,
Gn = G′ · ˆn
where
G′ = G′
1ˆi + G′
2ˆj + G′
3 ˆk

344
An Introduction to Vectors, Vector Operators and Vector Analysis
and
G′
i = lim
Γi→P
1
Si
Z
Γi
f(x) · dx i = 1,2,3.
For the components of G′ we get
G′
1 =
lim
Γ1→P
1
S1
Z
Γ1
f(x) · dx = ∂fz
∂y −
∂fy
∂z
G′
2 =
lim
Γ2→P
1
S2
Z
Γ2
f(x) · dx = ∂fx
∂z −∂fz
∂x
G′
3 =
lim
Γ3→P
1
S3
Z
Γ3
f(x) · dx =
∂fy
∂x −∂fx
∂y .
We immediately identify G′ with curl f or ∇× f. Thus, the curl of a vector ﬁeld that can be
Taylor expanded around a point P can be approximated by its line integral around a simple
closed curve Γ surrounding the point P . The approximation gets better as the size of Γ gets
smaller but the quantitative estimate of the error will involve the ﬁeld.
11.2
Applications of the Potential Functions
In this section we deal with the potential functions appearing in real life situations.1
We obtain the gravitational ﬁeld and the gravitational potential of a continuous body of
an arbitrary shape at a given point in space. From this, we obtain the internal and external
gravitational potential of a spherically symmetric body. We get the multipole expansion of
the potential and the ﬁeld of a body with arbitrary shape upto the ﬁrst term corresponding
to the deviation from sphericity. We do the same, upto third order term for an axially
symmetric body.
Gravitational ﬁeld due to a single particle of mass m1 at x1 = x1(t) at a point x in space
is given by
g1(x,t) = −Gm1
x −x1(t)
|x −x1(t)|3 .
(11.20)
The particle at x1 = x1(t) is called the source of the ﬁeld and the mass m1 is the source
strength. The ﬁeld g1 is a map (actually a one parameter family) assigning a deﬁnite vector
g1(x,t) to every point x in space, at a given instant of time. Note that the time dependence
is solely due to the motion of the source.
1Both these applications are treated in [10] using geometric algebra.

Vector Integration
345
If a particle of mass m is placed at a point x in the gravitational ﬁeld g1, we say that the
ﬁeld exerts a force
f1 = f(x,t) = mg1(x,t).
(11.21)
Equation (11.21) is mathematically same as Newton’s force law. However, if we impart
physical reality to the ﬁeld concept, it would mean that particles interact with each other
via their force ﬁelds rather than acting directly by exerting forces on one another in
accordance with Newton’s law. The gravitational ﬁeld is regarded as a real physical entity
pervading all space surrounding its source and acting on any matter that is present. The
ﬁeld concept also has a formal mathematical advantage. It enables us to separate
gravitational interactions into two parts, namely (a) production of gravitational ﬁelds by
extended sources and (b) the effect of a given gravitational ﬁeld on given bodies. We are
concerned here with the production of ﬁelds.
The single paticle gravitational ﬁeld, Eq. (11.20), can be derived from the gravitational
potential
φ1(x,t) =
−Gm1
|x −x1(t)|,
(11.22)
by differentiation, giving
g1(x,t) = −▽xφ1(x,t),
(11.23)
where ▽x is the derivative (gradient) with respect to the ﬁeld variable x. Henceforth, we
leave sufﬁx x to be understood. The gravitational potential energy of a particle with mass
m at x is given by
V1(x,t) = mφ1(x,t) = −Gmm1
|x −x1(t)|.
(11.24)
It is important to clearly distinguish between potential and potential energy. Latter is the
shared energy of two interacting objects, while former is characteristic of a single object
namely its source.
The gravitational ﬁeld g(x,t) of a N particle system is given by the superposition of
ﬁelds
g(x,t) =
N
X
k=1
gk(x,t) = −G
N
X
k=1
mk
x −xk(t)
|x −xk(t)|3 .
(11.25)
A particle of mass m at x experiences a force
f = mg =
X
k
mgk =
X
k
fk
(11.26)

346
An Introduction to Vectors, Vector Operators and Vector Analysis
due to ﬁeld in Eq. (11.25) which is consistent with the law of superposition of forces. This
ﬁeld can be derived from a potential; thus
g(x,t) = −▽φ(x,t)
(11.27)
where
φ(x,t) =
X
k
φk(x,t) = −G
X
k
mk
|x −xk(t)|.
(11.28)
The potential energy of a particle in a ﬁeld is given by
V (x,t) = mφ(x,t).
(11.29)
Note that this does not include the potential energy of interaction between the particles
producing the ﬁeld. The internal energy can be ignored as long as we are concerned only
with the inﬂuence of the system on external objects.
The gravitational ﬁeld of a continuous body is obtained from that of a system of N
particles via the following limiting process. We divide the body into small parts which can
be regarded as particulate and in the limit of inﬁnitely small subdivision the sum in
Eq. (11.25) becomes the integral,
g(x,t) = −G
Z
dm′ x −x′(t)
|x −x′(t)|3
(11.30)
where dm′ = dm(x′,t) is the mass given by the differential of the mass distribution
m(x′,t), supposed to be known. In other words, this is the mass of a small enough
corpuscle at point x′ at time t. Similar limiting process for Eq. (11.28) gives us the
gravitational potential of a continuous body
φ(x,t) = −G
Z
dm′
|x −x′(t)|.
(11.31)
Henceforth we shall not write the time dependence explicitly.
Equation (11.27) applies with φ(x,t) given by Eq. (11.31) so that we ﬁnd the ﬁeld g by
differentiating Eq. (11.31).
For a spherically symmetric mass distribution, the integral in Eq. (11.31) can be easily
evaluated. We place the origin at body’s centre of mass and denote the position vectors with
respect to the centre of mass by r and r′ instead of x and x′ which we use in the case of an
external inertial frame (see Fig. 11.12).
A spherically symmetric mass density is a function of radial distance alone. Thus,
dm′ = ρ(r′)r′2dr′dΩ
(11.32)

Vector Integration
347
Fig. 11.12
A spherically symmetric mass distribution
where dΩ= sinθdθdφ is the element of solid angle. Thus,
φ(r) = −G
Z
dm′
|r −r′| = −G
Z
ρ(r′)r′2dr′
Z
dΩ
|r −r′|.
For r > r′, (ﬁeld point external to the body), we can easily evaluate the integral
Z
dΩ
|r −r′| = 2π
Z π
0
sinθdθ
[r2 + r′2 −2rr′ cosθ]
1
2
= 4π
r
(11.33)
and the remaining integral simply gives the total mass of the body
M =
Z
dm′ = 4π
Z R
0
ρ(r′)r′2dr′.
Therefore, the external gravitational potential of a spherically symmetric body is given by
φ(r) = −G
Z
dm′
|r −r′| = −GM
r
.
(11.34)
This is just the potential of a point particle with mass M (= mass of the body) placed at
the centre of mass of the spherically symmetric body. Obviously, the gravitational ﬁeld of
a spherically symmetric body (g = −▽φ) is also the same as the particle with mass M
placed at its centre. Since many celestial bodies are nearly spherically symmetric, this is
an excellent ﬁrst approximation to their gravitational ﬁelds. Indeed, in many cases it is
sufﬁcient to apply Eq. (11.34).
To get a more accurate description of gravitational ﬁelds produced by non-spherical
bodies, we employ perturbation methods which enable us to systematically evaluate the
effects of deviations from spherical symmetry. The basic idea is to expand the potential of a
given body in Taylor series about its centre of mass. Obviously, we need a series expansion
for the scalar valued function
1
|r−r′|. For r > r′ we have the following well known result,
which we derive at the end.
1
|r −r′| = 1
r
1 +
∞
X
n=1
 r′
r
!n
Pn(ˆr · ˆr′)
,
(11.35)

348
An Introduction to Vectors, Vector Operators and Vector Analysis
where Pn are the Legender polynomials. A ﬁrst few of these are
P1(ˆr · ˆr′) = ˆr · ˆr′
P2(ˆr · ˆr′) = 1
2(3(ˆr · ˆr′)2 −1)
P3(ˆr · ˆr′) = 1
2(5(ˆr · ˆr′)3 −3(ˆr · ˆr′)).
(11.36)
A variant of Eq. (11.35) is
1
|r −r′| = 1
r
1 +
∞
X
n=1
r−2nPn(r · r′)
,
(11.37)
where a ﬁrst few Pn(r · r′) are
P1(r · r′) = r · r′
P2(r · r′) = 1
2(3(r · r′)2 −r2r′2)
P3(r · r′) = 1
2(5(r · r′)3 −3r2r′2r · r′).
(11.38)
It is clear from Eq. (11.35) that the magnitude of the nth term in the expansion is of the
order of ( r′
r )n so the series converges rapidly at a distance r which is large compared to the
dimensions of the body. Series (11.37) gives a series for the potential
φ(r) = −G
r
(
M + 1
r2
Z
P1(r · r′)dm′ + 1
r4
Z
P2(r · r′)dm′ + ···
)
By Eq. (11.38)
Z
P1(r · r′)dm′ = r ·
"Z
r′dm′
#
= r · [0] = 0.
Here,
R
r′dm′ gives the position vector of the centre of mass which vanishes because the
centre of mass is at the origin.
It is convenient to express the next term in the expansion (involving P2(r · r′)) in terms
of the inertia operator I : R3 7−→R3 or the moment of inertia tensor of the body, deﬁned
by, (Remember that I r ∈R3 is a vector),
I r =
Z
dm′r′ × (r × r′) =
Z
dm′(r′2r −(r′ · r)r′).
(11.39)

Vector Integration
349
The trace of the inertia tensor is given by
T r I = 2
Z
dm′ r′2 = I1 + I2 + I3,
(11.40)
where I1,I2,I3 are the principal moments of inertia.
Exercise
Prove Eq. (11.40).
Solution
We set up the matrix representing the inertia operator in an orthonormal basis
{σ1,s2,s3} with elements Iij = ˆσi · I ˆσj. Then, its trace is given by the sum of its diagonal
elements. From Eq. (11.39) we get, suppressing primes,
3
X
i=1
ˆσi · I ˆσi =
Z
dm
3
X
i=1
(r2 −r2
i ) = 2
Z
dmr2 = I1 + I2 + I3,
where I1,I2,I3 are the principal moments of inertia which are the eigenvalues of the inertia
operator. The last equality follows because the trace is seen to be independent of the basis
used to compute it.
□
Therefore,
Z
P2(r · r′)dm′ =
Z
dm′ 1
2(3(r · r′)2 −r2r′2) = 1
2[r2T r I −3r · I r] = 1
2r · Qr,
(11.41)
which deﬁnes a symmetric tensor
Qr = rT r I −3I r.
(11.42)
Exercise
Show that the tensor Q is symmetric.
Solution
We ﬁrst show that the MI operator I is symmetric. We have, suppressing
primes,
Iij = σi · I σj =
Z
dm(r2δij −rirj) = σj · I σi = Iji.
This gives,
Qij = ˆσi · Q ˆσj = ˆσi · ˆσjT rI −3 ˆσi · I ˆσj = Qji.
□
Following the well known terminology from electromagnetic theory, we call Q the
gravitational quadrupole tensor. (Again, remember that LHS of Eq. (11.42) is a vector
in R3.)

350
An Introduction to Vectors, Vector Operators and Vector Analysis
Now the expanded potential is
φ(r) = −G
r
(
M + 1
2
r · Qr
r4
+ ···
)
.
(11.43)
This is called a harmonic or multipole expansion of the potential. The quadrupole term
describes the ﬁrst non-zero correction to the potential of a spherically symmetric body.
The gravitational ﬁeld (g = −▽φ) can be obtained from Eq. (11.43) with the help of
▽
1
2r · Qr

= Qr,
▽rn = nrn−1ˆr.
Thus,
g(r) = −G
r2

Mˆr −1
r2

Qˆr −5
2(ˆr · Qˆr)ˆr

+ ···

.
(11.44)
Exercise
Derive Eq. (11.44).
□
This expression for the gravitational ﬁeld holds for a body with arbitrary shape and density
distribution.
The moment of inertia tensor for an axially symmetric body can be put in the form
I r = I1r + (I3 −I1)(r · ˆu)ˆu,
(11.45)
where I1 = I2 is the moment of inertia about any axis in the plane normal to the symmetry
axis and passing through the centre of mass, called equatorial moment of inertia, I3 is the
moment of inertia about the symmetry axis, or the so called polar moment of inertia. ˆu is
the direction of the symmetry axis.
Exercise
Prove Eq. (11.45).
Solution
Let { ˆσ1, ˆσ2, ˆu} be the eigenbasis of the inertia operator of the axially symmetric
body and let r = r1 ˆσ1 + r2 ˆσ2 + r3 ˆu be a position vector. Due to symmetry about the axis
given by ˆu, the eigenvalues corresponding to { ˆσ1, ˆσ2} must be equal, giving the eigenvalues
to be I1,I1,I3. We get
I r = I1(r1 ˆσ1 + r2 ˆσ2) + I3r3 ˆu
= I1r + I3r3 ˆu −I1r3 ˆu
= I1r + (I3 −I1)(r · ˆu)ˆu.
□

Vector Integration
351
Then Eqs (11.42) and (11.40) give,
Qr = (I3 −I1)(r −3(r · ˆu)ˆu).
(11.46)
From Eq. (11.44), then, the gravitational ﬁeld of an axially symmetric body is
g(r) = −MG
r2
(
ˆr + 3
2J2
R
r
2
[1 −5(ˆr · ˆu)2 ˆr + 2ˆr · ˆu ˆu] + ···
)
.
(11.47)
where R is the equatorial radius of the body and J2 is deﬁned as
J2 = I3 −I1
MR2 .
(11.48)
The constant J2 is a dimensionless measure of the oblateness of the body and the factor
( R
r )2 in Eq. (11.47) measures the rate at which the oblateness effect (on the ﬁeld) falls off
with distance.
For an axially symmetric body, the effect of harmonics higher than the quadrupole are
rather simply found, because the series Eq. (11.35) (or Eq. (11.37)) integrates to a harmonic
expansion for the potential, giving,
φ(r) = −GM
r
1 −
∞
X
n=2
Jn
R
r
n
Pn(ˆr · ˆu)
,
(11.49)
where Jn are the dimensionless constant coefﬁcients. As stated above, J2 measures the
oblateness of the body and is related to the moment of inertia via Eq. (11.48). The
constant J3 measures the extent to which the body is “pearshaped”, (i.e., the southern
hemisphere fatter than the northern hemisphere). The advantage of Eq. (11.49) is that it
can be immediately written down once the axial symmetry is assumed and the constants
Jn can be determined empirically, in particular, by ﬁtting Eq. (11.49) to data on orbiting
satellites. For the earth,
J2 = 1.083 × 10−3, J3 = −2.5 × 10−6, J4 = −1.6 × 10−6, J5 = −0.2 × 10−6.
Clearly the quadrupole harmonic strongly dominates. The contributions of the harmonics
decrease with n because of the factor
R
r
n in Eq. (11.49). Since Jn are dimensionless,
comparison of Jn values for different planets can be used to quantitatively compare the
shapes of planets.
Using the directional derivative
ˆu · ▽ˆr = ˆu −(ˆu · ˆr)ˆr
r
,
(11.50)
we can differentiate the term (n = 3) in Eq. (11.49) to get its contribution to the
gravitational ﬁeld as

352
An Introduction to Vectors, Vector Operators and Vector Analysis
g3(r) = −GM
r3
(5
2J3
R
r
3 
−7(ˆr · ˆu)3 + 3ˆr · ˆu

r +

3(ˆr · ˆu)2 −3
5

u

,
(11.51)
where u = r ˆu. The contribution of the term with n = 2 is already obtained in Eq. (11.47).
Differentiating, in this way, term by term in Eq. (11.49), we can express the gravitational
ﬁeld of a axially symmetric body as
g(r) = −GM
r3
r +
∞
X
n=2
gn(r)
.
(11.52)
Finally, we establish Eq. (11.35). We have, using law of cosines, (see Fig. 11.13)
|r −r′|2 = r2 + r′2 −2rr′(ˆr · ˆr′)
= r2

1 +
 r′
r
!2
−2
 r′
r
!
ˆr · ˆr′


or,
|r −r′| = r
√
1 + ϵ,
where
ϵ =
 r′
r
! r′
r −2ˆr · ˆr′
!
.
(11.53)
As long as r > r′, ϵ < 1, so that we can use binomial expansion to get
1
|r −r′| = 1
r (1 + ϵ)−1
2 = 1
r

1 −1
2ϵ + 3
8ϵ2 −5
16ϵ3 + ···

.
(11.54)
Putting Eq. (11.53) in Eq. (11.54) and collecting the coefﬁcients of different powers of
r′
r

we get Eq. (11.35).
Fig. 11.13
Variables in the multipole expansion

Vector Integration
353
Exercise
Develop the multipole expansion for the electrostatic potential at r due to an
arbitrary localized charge distribution, in powers of 1
r . This is analogous to the above
development of multipole expansion for the gravitational potential of an arbitrary
localized mass distribution. Give the geometric interpretation of the of the terms
proportional to 1
r2 , 1
r3 , 1
r4 . Compare these two cases. (Consult ref [9]).
□
We shall now obtain the equation to the surface of the earth by assuming it to be an
equipotential for the effective gravitational potential
Φ(r) = V (r) −1
2(Ω× r)2,
(11.55)
where V (r) is the true gravitational potential at the earth’s surface and the last term is the
centrifugal potential. We do this by expressing Φ(r) in terms of the ellipticity parameter
for the earth given by
ϵ = a −c
c
,
a,c being the equatorial and polar radii of the earth respectively. We show that the resulting
shape of the earth is an approximate oblate spheroid. We differentiate the geopotential Φ(r)
to express the equatorial and polar gravitational accelerations, ge and gp respectively, in
terms of the ellipticity parameter ϵ. We use the observed values of ge and gp namely,
ge = 978.039 cm/sec2
gp = 983.217 cm/sec2
to estimate the ellipticity parameter ϵ.
Earth’s shape is all important for cartography and has a role in many geophysical
phenomena. It is intimately connected with the rotation of the earth as we shall see. In fact
the basic idea is that earth’s shape originated from the cooling of a spinning molten mass
to form a solid crust. Another ‘shape forming agency’ is the oscillating tides due to other
astronomical bodies like the moon and the sun. However, this effect is of higher order of
smallness to be included in more reﬁned models.
For our purpose, we model the earth as a spinning ﬂuid, held together in steady state by
the gravitational ﬁeld (see Fig. 11.14).
In a geocentric frame spinning with the earth, the ﬂuid is at rest with the effective
gravitational potential given by Eq. (11.55). The gravitational ﬁeld
g = −▽Φ
(11.56)
must be normal to the surface. If it had a tangential component, it will make the ﬂuid
ﬂow on the surface. This means that the surface of the earth is an equipotential surface
deﬁned by

354
An Introduction to Vectors, Vector Operators and Vector Analysis
Φ(r) = Φ0,
(11.57)
where Φ0 is the constant to be determined.
Fig. 11.14
Earth’s rotation affected its shape in its formative stage
Due to axial symmetry in the problem, earth’s gravitational potential V can be described by
the Legendre expansion Eq. (11.49). Therefore, to the second order, earth’s shape is given
explicitly by the equation
Φ(r) = −GM⊕
r
(
1 −1
2J2
a
r
2 h
3(ˆr · ˆu)2 −1
i)
−1
2Ω2r2 h
1 −(ˆr · ˆu)2i
= Φ0, (11.58)
where ˆu = ˆΩspeciﬁes the rotation axis, a is the equatorial radius of the earth and we
have used identity II. The surface described by this equation is called geoid. Its deviation
from the sphere is characterized by the so called ellipticity (or ﬂattening) parameter ϵ
deﬁned by
ϵ = a −c
c
,
(11.59)
with c as the earth’s polar radius. To evaluate the constant Φ0 in Eq. (11.57) we set r = c
and ˆr · ˆu = 1 in Eq. (11.58) giving
Φ0 = −GM⊕
c
(
1 −J2a2
c2
)
.
(11.60)
To express the ellipticity parameter ϵ in terms of other parameters we set r = a and ˆr· ˆu = 0
in Eq. (11.58) to get
−GM⊕
a

1 + 1
2J2

−1
2Ω2a2 = −GM⊕
a
(
1 −J2a2
c2
)
(1 + ϵ),
(11.61)

Vector Integration
355
where we have used a
c = 1 + ϵ and Eq. (11.60). Since ϵ and J2 are known to be small
quantities, it sufﬁces to solve this equation for ϵ to the ﬁrst order, so that
ϵ = 3
2J2 + 1
2β,
(11.62)
where
β = Ω2a3
GM⊕
=
Ω2a
GM⊕/a2
(11.63)
is the ratio of the centripetal to the gravitational acceleration at the equator.
The potential Φ(r) can now be expressed in terms of ϵ and β,
Φ(r) = −GM⊕
r
(
1 + (ϵ −1
2β)
a
r
3 1
3 −(ˆr · ˆu)2
+ 1
2β
r
a
3 h
1 −(ˆr · ˆu)2i)
.
(11.64)
To get the equation for the geoid, to the ﬁrst order in ϵ, we approximate
a
r

in Eq. (11.64)
by a
c = 1 + ϵ, use binomial theorem and simplify Eq. (11.64) keeping only the ﬁrst order
terms. We then equate the resulting expression to that for Φ0 obtained by expressing the
LHS of Eq. (11.61) in terms of ϵ and β, namely,
Φ0 = −GM⊕
a

1 + 1
3(ϵ + β)

.
(11.65)
This gives the equation for the geoid, to the ﬁrst order in the ellipticity parameter ϵ as
r = a(1 −ϵ(ˆr · ˆu)2).
(11.66)
That this is an equation to an approximate oblate spheroid can be seen by approximating
the equation to the oblate spheroid
1 = (r · ˆu)2
c2
+ (r × ˆu)2
b2
= r2
b2
"
1 +
 b2 −c2
c2
!
(ˆr · ˆu)2
#
(11.67)
for small b−c
c ≡ϵ′ as
r ≈
b
[1 + 2ϵ′(ˆr · ˆu)2]
1
2
≈b
h
1 −ϵ′(ˆr · ˆu)2i
.
(11.68)
To get the accelerations ge and gp, we have to differentiate Φ(r) as in Eq. (11.56). We do
it using explicit coordinate system on earth, for its geometric visualization. In terms of
coordinates (r,λ), λ being the latitude, Φ(r,λ) can be written as (see Eq. (11.58))

356
An Introduction to Vectors, Vector Operators and Vector Analysis
Φ(r,λ) = −GM⊕
r
+ GM⊕a2
2r3
J2(3sin2 λ −1) −1
2Ω2r2 cos2 λ.
(11.69)
and the magnitude of the acceleration g is given by
g = −


 ∂Φ
∂r
!2
+
 1
r
∂Φ
∂λ
!2

1
2
.
(11.70)
Due to the smallness of ϵ, g is almost normal to the spherical earth, although it is strictly
normal to the geoid. Thus, g deviates from the radial direction (which deﬁnes (λ)) only by
a small angle of the order of ϵ. Therefore,
∂Φ
∂λ

≈ϵ, making the second term in Eq. (11.70)
of the order of ϵ2 and hence negligible. Therefore,
−g = ∂Φ
∂r = GM⊕
r2
−3
2
GM⊕a2
r4
J2(3sin2 λ −1) −Ω2r(1 −sin2 λ).
(11.71)
From Eq. (11.66) we substitute the value of r on the geoid at arbitrary latitude λ
r = a

1 −ϵsin2 λ

(11.72)
in Eq. (11.71) and use the binomial expansion

1 −ϵsin2 λ
−n =

1 + nϵsin2 λ···

.
(11.73)
Neglecting the products of small quantities and higher orders in ϵ, we get
−g = GM⊕
a2

1 + 2ϵsin2 λ

−3
2
GM⊕
a2
J2

3sin2 λ −1

−Ω2a

1 −sin2 λ

.
(11.74)
Putting λ = 0 in Eq. (3.20) we get the value of the equatorial gravity
ge = −GM⊕
a2

1 + 3
2J2 −β

= GM⊕
a2

1 + ϵ −3
2β

.
(11.75)
Similarly, putting λ = π
2 in Eq. (2.20) we get the value at the poles
gp = GM⊕
a2 (1 + β).
(11.76)
Using the given experimental values of ge and gp and the known values of a and Ωwe can
solve the simultaneous Eqs (11.75) and (11.76) to get
ϵ = 0.003376;β = 0.003468
(11.77)

Vector Integration
357
We can substitute these values of ϵ and β in Eq. (11.62) to get the value of J2 which agrees
with the value of J2 mentioned above, which was obtained using satellite data, within one
percent. This gives us a check on the internal consistency of the theory.
The shape of the earth given by the geoid Eq. (11.66) agrees with measurements of
sea level to within a few meters. However, radar ranging to measure the height of the ocean
is accurate to a fraction of a meter. This shows the need to develop more reﬁned models
for the shape of the earth. The principal deviation from the geoid is an excessive bulge
around the equator. This is attributed to a retardation of the rotating earth over past million
years. For a detailed exposition of the physics of the earth, the reader may consult ref [16]
and [24].
11.3
Area Integral
We already know that the magnitude of a vector product like x1 × x2 equals the area of
the parallelogram with x1 and x2 as its adjacent sides and its direction is that in which a
right handed screw advances when rotated in the sense of x1 rotating towards x2. If ˆn is
the unit vector in the direction of x1 × x2 then the area vector representing the area of the
corresponding parallelogram is deﬁned to be
a = ±|x1 × x2|ˆn,
(11.78)
where the + sign applies if the rotation of x1 towards x2 is counterclockwise and −sign
applies if it is clockwise. This deﬁnition of the area vector suggests the following
construction of an area integral:
A = 1
2
Z b
a
x × dx = 1
2 lim
n→∞
n
X
k=1
xk × ∆xk
(11.79)
with Pn
k=1 ∆xk = b −a. Note that this is totally a vector relation in which differential area
vectors are added to give the resulting area vector in the limit as |∆x| →0. If n is large
enough, we can approximate this area integral by the sum
A ≈1
2
n
X
k=1
xk × ∆xk
= 1
2x0 × x1 + 1
2x1 × x2 + ··· + 1
2xn−1 × xn.
(11.80)
As depicted in Fig. 11.15, each term in this sum is the area vector of a triangle with one
vertex at the origin. The magnitude of the kth term approximates the area swept out by
the line segment represented by the vector variable x as its tip moves continuously along
the curve joining a and b from xk−1 to xk with its tail at the origin, while the direction of
the corresponding area vector is consistent with the sense of rotation of x from xk−1 to xk.
Thus, the sum in Eq. (11.80) approximates the area vector corresponding to the area swept

358
An Introduction to Vectors, Vector Operators and Vector Analysis
out as the variable x moves from a to b. Thus, the integral Eq. (11.79) is the area vector for
the total area swept out by the vector variable x as it moves continuously along the curve
from a to b. Thus, the value of the area integral Eq. (11.79) is not path independent as the
area swept out depends on the path from a to b.
Fig. 11.15
Area integral
If the curve is represented by the parametric equation x = x(t), with x(0) = a, then the
corresponding area vector can be obtained as a parametric function A = A(t) as
A(t) = 1
2
Z x(t)
x(0)
x × dx = 1
2
Z t
0
x × ˙xdt with x and ˙x both functions at t. (11.81)
Differentiating with respect to the upper limit of the integral, we get,
˙A = 1
2x × ˙x,
(11.82)
expressing the rate at which the area is swept out. This rate depends on the choice of the
parameterization x = x(t), although the total area swept out depends only on the curve.
If we integrate along a closed curve C in a plane, enclosing the origin, (see Fig. 11.16),
then the integral
A = 1
2
Z
C
x × dx
(11.83)
gives the area vector of the area enclosed by the curve C. This is evident by applying the
approximation of the integral by the areas of triangles as expressed by Eq. (11.80), with
x0 = xn. The sign of the area vector A is positive if the curve C has counterclockwise
orientation (as in Fig. 11.16(a)), or is negative if C has clockwise orientation. For the
situation in Fig. 11.16(a) we get,
1
2x × dx = 1
2|x × dx|ˆn

Vector Integration
359
for the kth element of the area, hence from Eq. (11.83),
|A| = 1
2
Z
C
|x × dx|.
(11.84)
We emphasize that Eq. (11.84) follows from Eq. (11.83) only when all coplanar elements of
area have the same orientation. as in Fig. 11.16(a). This condition is not met if the curve C
is self-intersecting or does not enclose the origin.
Fig. 11.16
Area swept out by radius vector along a closed curve.
Cross-hatched
region is swept out twice in opposite directions, so its area is zero.
The area integral Eq. (11.83) is independent of the origin although the values of the vector
variable x depends on the origin. To see this, displace the origin inside the curve C in
Fig. 11.16(a) to a place outside the curve as shown in Fig. 11.16(b). Choosing the points a
and b on C we separate C into two pieces C1 and C2, so the area integral becomes
A = 1
2
Z
C
x × dx = 1
2
Z
C1
x × dx + 1
2
Z
C2
x × dx.
Referring to Fig. 11.6(b) we see that the coordinate vector sweeps over the region inside
C once as it goes between a and b along C, but it sweeps over the meshed region to the
left of C2 twice, once as it traverses C2 and again as it traverses C1 and since the sweeps
over the latter region are in opposite directions their contributions to the integral have
same magnitude but opposite signs, and hence cancel. We are thus left with the area vector
corresponding to C as claimed.
For a general proof that the closed area integral is independent of the origin, we displace
the origin by a vector c by making the change of variables x →x′ = x −c. Then,
Z
C
x′ × dx′ =
Z
C
(x −c) × dx =
Z
C
x × dx −c ×
Z
C
dx.

360
An Introduction to Vectors, Vector Operators and Vector Analysis
However, the last term vanishes because
R
C dx = 0, so the independence of origin of the
area integral is proved. Note that the cancellation of the parts of the integral proving its
independence of the origin remains valid even if the origin is chosen out of the plane
containing the curve c. Thus, the value of the area integral over a closed plane curve is
invariant of the origin, even if the origin is taken out of the plane containing the curve.
The area integral of a closed planar curve can be evaluated to give the area enclosed by
an self-intersecting plane curve such as the one shown in Fig. 11.17. The sign of the area
integral for subregions are indicated in the ﬁgure, with zero for subregions which are swept
out twice with opposite signs.
The integral Eq. (11.79) or Eq. (11.83) applies to curves in space which do not lie in
plane, giving the area of the surface swept out by the vector variable x while traversing the
curve. Such integrals may ﬁnd application in Computer Aided Design, for example, applied
to the design of automobile parts.
Fig. 11.17
Directed area of a self-intersecting closed plane curve. Vertical and
horizontal lines denote areas with opposite orientation, so cross-hatched
region has zero area.
11.4
Multiple Integrals
In this section we learn about multiple integrals, speciﬁcally about double and triple
integrals. A double integral is the integral over a scalar valued function f (x) where x is a
2-D vector varying over some connected ﬁnite region in a plane. Equivalently, we integrate
a function of two scalar variables (x,y) where xˆi + yˆj span some connected region on a
plane. The coordinates (x,y) of the vector variable x range over the area of the region of
integration. A triple or a volume integral is an integral of a scalar valued function f (x),
with x spanning a ﬁnite connected region in space, or over a function of three scalar
variables say f (x,y,z) where xˆi + yˆj + z ˆk spans some ﬁnite connected region in space.
The coordinates (x,y,z) of the variable x range over the volume V of the region of
integration. The corresponding integrals on a disconnected region is the sum of the

Vector Integration
361
integrals on its connected components. We will generally express multiple integrals as
integrals over the functions of three scalar variables. In order to express multiple integrals
fully in terms of vectors and vector algebra, we have to take recourse to geometric algebra
and geometric calculus[10, 11, 7]. 2
11.4.1
Area of a planar region: Jordan measure
Our aim is to get a quantitative measure of the area of a planar region S.
Fig. 11.18
Interior and exterior approximations to the area of the unit disc |x| ≤1
for n = 0,1,2 where A−
0 = 0,A−
1 = 1,A−
2 = 2,A+
2 = 4.25,A+
1 =
6,A+
0 = 12
We divide the plane into squares by ﬁrst drawing x,y axes and then drawing the sequences
of parallel lines to x and y axis respectively at a separation of one unit of length. The
coordinates of the points of intersection of this mesh are x = 0,±1,±2,... and
y = 0,±1,±2,.... This mesh covers the whole plane by closed unit squares without a gap
or overlap. Also, the interiors of any two squares of this mesh are disjoint. Let A+
0 (S) be
the number of squares having points in common with S and A−
0(S) be the number of
squares totally contained in S. Note that A+
0 (S) and A−
0(S) also give the areas of ﬁgures
formed by these squares because the area of a single square is unity. Next, divide each
square into four equal squares of side 1
2 and area 1
4. Let A+
1 (S) be the area covered by the
number of such squares (each of area 2−1 × 2−1 = 2−2) overlapping S and A−
1(S) be the
area covered by the number of such squares contained in S. Since the area of individual
squares is now reduced by a factor of 2−2, one or more such smaller squares may get
accommodated in the interior portion of S lying between the boundary of the ﬁgure
corresponding to A−
0(S) and the boundary of S. This increases the interior area covered
2Calculus with functions of three variables is called calculus of three variables. Calculus of three variables and vector calculus
are two sides of the same coin. Former is carried out in R3 while the latter is carried out in E3.

362
An Introduction to Vectors, Vector Operators and Vector Analysis
by the smaller squares in comparison to that covered by the larger squares. On the other
hand, a larger square, overlapping S but not contained in S, when divided into four
smaller squares of equal area will generate one or more smaller squares with no overlap
with S at all, thus reducing the area of the ﬁgure corresponding to A+
0 (S). Thus, we see
that,
A−
0(S) ≤A−
1(S)
and
A+
0 (S) ≥A+
1 (S).
(11.85)
We iterate this process n times each time dividing the square in the previous iterate by 1
2
to get each square of side 2−n and area 2−2n. Reiterating exactly the same argument which
led to inequalities Eq. (11.85), we get, at nth step, (see Fig. 11.18)
A−
n−1(S) ≤A−
n(S)
and
A+
n−1(S) ≥A+
n (S).
(11.86)
It is clear that the values A+
n (S) form a monotonically decreasing and bounded sequence
converging to a value A+(S), while A−n(S) increase monotonically and converge to a
value A−(S). The value A−(S) represents the inner area, the closest we can approximate
the area of S from below by congruent squares contained in S while the outer area gives
the least upper bound obtained by covering S by congruent squares. If both these values
are the same, we say that S is Jordan measurable and call the common value
A+(S) = A−(S) = A(S) the content or the Jordan measure of S. We express the fact that
S is Jordan measurable by saying that S has an area A(S).
The difference A+
n (S) −A−n(S) gives the total area of squares after nth iteration that
overlap with S, however, are not completely contained in S. All these squares contain
boundary points of S so that
A+
n (S) −A−
n(S) ≤A+
n (∂S),
where ∂S is the boundary of S. If the boundary of S has zero area, then we ﬁnd that
A+(S) −A−(S) = lim
n→∞[A+
n (S) −A−
n(S)] = lim
n→∞A+
n (∂S) = 0,
which means A+(S) = A−(S) = A(S), that is, S has area A(S). Thus, S has an area if its
boundary ∂S has zero area. We can also show that if S has an area then A+(∂S) = 0.
The criterion A+(∂S) = 0 is sufﬁcient to show that most of the planar regions we
encounter in practice have deﬁnite area. This is certainly true if ∂S consists of a ﬁnite
number of arcs described by a function f (x) or g(y) with f or g continuous over a ﬁnite

Vector Integration
363
closed interval. The uniform continuity3 of continuous functions over bounded closed
interval immediately shows us that these arcs can be covered by a ﬁnite number of
rectangles of arbitrarily small area say ϵ2. Therefore,
A+(∂S) ≤nϵ2
with n ﬁnite and ϵ arbitrarily small we must have A+(∂S) = 0.
Given two Jordan measurable planar sets S and T with areas A(S) and A(T ) the sets
S ∪T and S ∩T are also Jordan measurable and
A(S ∪T ) = A(S) + A(T ) −A(S ∩T ).
If S and T are disjoint then
A(S ∪T ) = A(S) + A(T ).
For a ﬁnite number of disjoint Jordan measurable sets S1,...,SN,
A(∪N
i=1Si) =
N
X
i=1
A(Si).
Everything, we have said above, about the areas of the planar sets carries over immediately
to the volumes in three dimensions. In order to deﬁne the volume V (S) of a bounded set
S in 3-D space, we have to use subdivisions of space into cubes of sides 2−n. The set S has
a volume if its boundary can be covered by a ﬁnite number of these cubes with arbitrarily
small total volume. This is true for all bounded sets S whose boundary consists of a ﬁnite
number of surfaces each of which is represented by a continuous function f (x), x varying
over a closed planar set.
11.4.2
Double integral
We are now in a position to deﬁne the double integral of a function f (x) ≡f (x,y).
Let a continuous function f (x) ≡f (x,y) deﬁne the surface z = f (x,y) over its Jordan
measurable closed domain R in the x,y plane (see Fig. 11.19). For simplicity, we assume
z = f (x,y) ≥0 for all (x,y) ∈R. Consider the set S of points x ≡(x,y,z) for which
(x,y) ∈R ; 0 ≤z ≤f (x,y).
The surfaces enclosing this set are (i)z = f (x,y) (ii) R (z = 0) and (iii) (x,y) ∈∂R;
0 ≤z ≤f (x,y). We deﬁne the double integral by the volume V (S) of the set S, which can
be obtained as follows.
3Uniform continuity of f (x) means that for every ε > 0, there is a ∆> 0 such that d(x1,x2) < ∆implies d(f (x1),f (x2)) < ε
for every (x1,x2). This means that a ﬁnite arc given by f (x) can be covered by a ﬁnite, say n, number of squares of size ε2,
for every ε > 0.

364
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 11.19
Evaluation of a double integral
We divide R into non-overlapping Jordan measurable closed sets R1,...,RN. Let hi be the
minimum and Hi be the maximum of f (x,y) with (x,y) ∈Ri. The cylinder with height
hi and base Ri has the volume hiA(Ri) where A(Ri) is the area of Ri. These cylinders do
not overlap. Similarly, the cylinders with height Hi and base Ri do not overlap and have
the volume HiA(Ri). It follows that
N
X
i=1
hiA(Ri) ≤V (S) ≤
N
X
i=1
HiA(Ri).
(11.87)
The sums in this inequality are respectively called the lower sum and the upper sum.
We now make the subdivision of R ﬁner and ﬁner, such that the number of subsets the
number of subdivisions and the largest diameter of Ri ; i = 1,...,N tends to zero. The
continuous function f (x,y) is uniformly continuous in the closed and bounded set R, so
that the maximum difference Hi −hi tends to zero with the maximum diameter over the
sets Ri of the subdivision. The differences over the upper and the lower sum also tend to
zero, since,
N
X
i=1
HiA(Ri) −
N
X
i=1
hiA(Ri) =
N
X
i=1
(Hi −hi)A(Ri)
≤

max
i
(Hi −hi)
 N
X
k=1
A(Rk)
=

max
i
(Hi −hi)

A(R).
(11.88)
It follows from inequality Eq. (11.87) that the upper and lower sum both converge to the
limit V (S) as the number of subdivisions N →∞or the largest diameter tends to zero.

Vector Integration
365
We obtain the same limiting value if we take the value of the function f (xi,yi) at a point
(xi,yi) ∈Ri, instead of hi or Hi. We call the limit V (S) the double integral of f over the
set R and write
V (S) =
Z Z
R
f (x,y)dR.
(11.89)
Suppose, we now lift the restriction z = f (x,y) > 0. Due to continuity of f (x,y) the
surface (x,y) ∈R ; z = f (x,y) may cut the x,y plane in some continuous curve and the
set S deﬁned above is divided into two (or more, but we assume two) sets, one above and
the other below the x,y plane, each corresponding to two distinct parts, R+ and R−of the
domain R. These are the set S+ given by (x,y) ∈R ; z = f (x,y) > 0 and the set S−given
by (x,y) ∈R ; z = f (x,y) < 0. We deﬁne a new set S∓by (x,y) ∈R ; z = −f (x,y) > 0.
Both these are the sets of points above the x,y plane so that
Z Z
R+ f (x,y)dR = V (S+) and
Z Z
R−(−f(x,y))dR = V(S∓) = V(S−).
This means
Z Z
R
f (x,y)dR = V (S+) −V (S−).
We can summarize as follows. Consider a closed and bounded set R with area A(R) = ∆R
and a function f (x,y) that is continuous everywhere in R including its boundary. We
subdivide R into N non-overlapping Jordan measurable subsets R1,R2,...,RN with areas
∆R1,∆R2,...,∆RN. In Ri we choose an arbitrary point (xi,yi) where f (xi,yi) = fi and
form the sum
VN =
N
X
i=1
fiA(Ri).
Then, we have the theorem:
If the number N tends to inﬁnity and simultaneously the greatest of the diameters of
the subregions tends to zero, then VN tends to a limit V . This limit is independent of the
particular nature of the subdivision of R and of the choice of the point (xi,yi) in Ri. We
call the limit V the double integral of the function f (x,y) over the region R and denote
it by
Z Z
R
f (x,y)dR.
Since A(∂R) = 0 we can choose all Ri to lie entirely in the interior of R having no points
common with the boundary of R.

366
An Introduction to Vectors, Vector Operators and Vector Analysis
We consider some speciﬁc subdivisions. In the simplest case, R is a rectangle a ≤x ≤
b ; c ≤y ≤d and the subregions Ri are also rectangles obtained by dividing the x interval
into n equal parts and the y interval into m equal parts having lengths
∆x = b −a
n
and ∆y = d −c
m .
Let the points of subdivision be x0 = a,x1,x2,...,xn = b and y0 = c,y1,y2,...,yn = d.
We have N = nm. Every subregion is a rectangle with area A(Ri) = ∆Ri = ∆x∆y. For the
point (xi,yi) we take any point in the corresponding rectangle Ri and then form the sum
X
i
f (xi,yi)∆x∆y
over all the rectangles of the subdivision. If we now let both m and n simultaneously tend
to inﬁnity, the sum tends to the integral of the function f over the rectangle R.
These rectangles can also be characterized by two sufﬁxes µ and ν corresponding to the
coordinates x = a+ν∆x and y = c+µ∆y of the lower left hand corner of the rectangle in
question. Here, 0 ≤ν ≤(n−1) and 0 ≤µ ≤(m−1). With this identiﬁcation of rectangles
with sufﬁxes ν and µ we may write the sum as the double sum
n−1
X
ν=0
m−1
X
µ=0
f (xν,yµ)∆x∆y.
(11.90)
Even if R is not a rectangle, it is often convenient to subdivide it into rectangular subregions
Ri. We can superimpose a rectangular net given by
x = νh
(ν = 0,±1,±2,...),
y = µk
(µ = 0,±1,±2,...),
(11.91)
where h and k are numbers chosen conveniently. We call Ri the rectangles of the division
that lie entirely within R. Ri do not completely ﬁll the region R. However, as we have
noted above, we can calculate the integral of the function f over R by summing only over
interior rectangles and then passing to the limit. Whenever we use a rectangular grid with
lines parallel to x and y axes we replace the in the integral differential dR by dxdy. Thus,
Z Z
R
f (x,y)dR =
Z Z
R
f (x,y)dxdy.
Further, the dummy variables of integration x,y can be replaced, in the integral, by any
other pair of variables (u,v),(ξ,η) etc.

Vector Integration
367
Fig. 11.20
Subdivision by polar coordinate net
The subdivision by the polar coordinate net (see Fig. 3.20) also ﬁnds frequent application.
We subdivide the entire angle 2π into n parts ∆θ = 2π/n and also choose a quantum ∆r
for the r coordinate. We draw the lines θ = ν∆θ(ν = 0, 1,2,...,n−1) through the origin
and also the concentric circles rµ = µ∆r,(µ = 0,1,2,...). We denote by Ri the patches
formed by their intersection which lie entirely in the interior of R, and the areas of Ri by
∆Ri. Then, the integral of the function f (x,y) is given by the limit of the sum
X
f (xi,yi)∆Ri,
where (xi,yi) is a point chosen arbitrarily in Ri whose polar coordinates satisfy
xi = ri cosθi and yi = ri sinθi. By elementary geometry the area ∆Ri given by
∆Ri = 1
2(r2
µ+1 −r2
µ)∆θ = 1
2(2µ + 1)(∆r)2∆θ,
if we assume that Ri lies in the ring bounded by the circles with radia µ∆r and (µ + 1)∆r.
Therefore, the required sum can be written as
1
2
n−1
X
µ=0
n−1
X
ν=0
f (rµ cosθν,rµ sinθν)(2µ + 1)(∆r)2∆θ
and the double integral of f over R is obtained in the limit n →∞(or equivalently ∆r →0
and ∆θ →0) of this sum.
As an example, consider f (x,y) = 1 over some bounded region R in the x,y plane.
Then, the double integral of f (x,y) is given by the volume below the region R shifted
vertically to the plane z = 1. This volume is given by f (x,y) · A(R) = 1 · A(R) = A(R).
Thus, we get the result
Z Z
R
dR = A(R).

368
An Introduction to Vectors, Vector Operators and Vector Analysis
Our next example is the double integral of f (x,y) =
xy over the rectangle
a ≤x ≤b ; c ≤y ≤d, or, more generally, any function f (x,y) that can be decomposed as
a product of a function of x and a function of y in the form f (x,y) = φ(x)ψ(y). We use
the same division of the rectangle as in Eq. (11.90) and the value of the function at the
lower left hand corner of the sub-rectangle in the summand. The integral is then the
limit of the sum
∆x∆y
n−1
X
ν=0
m−1
X
µ=0
φ(ν∆x)ψ(µ∆y),
which can be written as the product of two sums as
n−1
X
ν=0
φ(ν∆x)∆x
m−1
X
µ=0
ψ(µ∆y)∆y.
From the deﬁnition of the ordinary integral, as ∆x →0 and ∆y →0 these factors tend to
the integrals of the corresponding functions over the respective intervals from a to b and
from c to d. Thus, we get a general rule that the double integral of a function satisfying
f (x,y) = φ(x)ψ(y) over a rectangle a ≤x ≤b ; c ≤y ≤d can be resolved into the
product of two integrals
Z Z
R
f (x,y)dxdy =
Z b
a
φ(x)dx ·
Z d
c
ψ(y)dy.
This rule and the summation rule (see below) yield the integral over any polynomial over
a rectangle with sides parallel to the axes.
In our last example, we use a subdivision by a polar coordinate net. Let the region R be
the unit disc centered at the origin, given by x2 + y2 ≤1 and let
f (x,y) =
q
1 −x2 −y2.
The integral of f over R is simply the volume of a hemisphere of unit radius.
We construct the polar net as above. The subregion lying between the circles rµ = µ∆r
and rµ+1 = (µ + 1)∆r and between the lines θ = ν∆θ and θ = (ν + 1)∆θ makes the
contribution
1
2
s
1 −
 rµ+1 + rµ
2
!2
(r2
µ+1 −r2
µ)∆θ = ρµ
q
1 −ρ2µ ∆r∆θ,
where we have taken the value of the function at an intermediate circle with the radius
ρµ = (rµ+1 + rµ)/2. All subregions that lie in the same ring have the same contribution
and since there are n = 2π/∆θ such regions the contribution of the whole ring is

Vector Integration
369
2πρµ
q
1 −ρ2µ ∆r.
The integral is therefore the limit of the sum
m−1
X
µ=0
2πρµ
q
1 −ρ2µ ∆r.
This sum tends to the single integral
2π
Z 1
0
r
√
1 −r2dr = −2π
3
q
(1 −r2)3

1
0 = 2π
3 .
We therefore get
Z Z
R
q
1 −x2 −y2dR = 2π
3
in agreement with the known formula for the volume of a sphere.
For double integrals, as for single integrals, the following fundamental rules apply. If c
is a constant, then
Z Z
R
cf (x,y)dR = c
Z Z
R
f (x,y)dR.
Further, the operation of integration is linear:
Z Z
R
[φ(x,y) + ψ(x,y)]dR =
Z Z
R
φ(x,y)dR +
Z Z
R
ψ(x,y)dR.
If the region R consists of two subregions R1 and R2 such that R1 ∪R2 = R and R1 ∩
R2 ⊂(∂R1 ∪∂R2), then
Z Z
R
f (x,y)dR =
Z Z
R1
f (x,y)dR1 +
Z Z
R2
f (x,y)dR2.
Thus, for the regions that are joined together, the corresponding integrals are added.
11.4.3
Integral estimates
The upper and lower bounds on the double integral can be seen quite easily under certain
conditions.
If f (x,y) ≥0 or f (x,y) ≤0 in R, then,
Z Z
R
f (x,y)dR ≥0,

370
An Introduction to Vectors, Vector Operators and Vector Analysis
or,
Z Z
R
f (x,y)dR ≤0.
From this we see that, if the inequality
f (x,y) ≥g(x,y)
holds everywhere in R, then,
Z Z
R
f (x,y)dR ≥
Z Z
R
g(x,y)dR.
From this it follows that
Z Z
R
f (x,y)dR ≤
Z Z
R
|f (x,y)|dR
and
Z Z
R
f (x,y)dR ≥−
Z Z
R
|f (x,y)|dR.
These two inequalities can be combined:

Z Z
R
f (x,y)dR
 ≤
Z Z
R
|f (x,y)|dR.
If m is the greatest lower bound and M is the least upper bound of the function f (x,y) in
R, and ∆R is the area of R, then,
m∆R ≤
Z Z
R
f (x,y)dR ≤M∆R.
The integral can then be expressed as
Z Z
R
f (x,y)dR = µ∆R
with µ lying between m and M. The precise value of µ cannot be speciﬁed more exactly.
This equation is called the mean value theorem in integral calculus. Generalizing, we can
say that for an arbitrary positive continuous function p(x,y) on R,
Z Z
R
p(x,y)f (x,y)dR = µ
Z Z
R
p(x,y)dR,

Vector Integration
371
where µ is a number between the greatest and the lowest values of f (x,y) on R that cannot
be further speciﬁed.
We close by making the following two observations. The ﬁrst is that a double integral on
R varies continuously with the function to be integrated. This means, given two functions
f and g satisfying
|f (x,y) −g(x,y)| < ϵ, (x,y) ∈R,
where ϵ > 0 is a ﬁxed number, then the integrals
R R
R f (x,y)dR and
R R
R g(x,y)dR differ
by less than ϵ∆R where ∆R is the area of R, that is, by less than a number that goes to zero
with ϵ. Similarly, we see that the integral of a function varies continuously with the region.
Suppose that the region R2 is obtained from R1 by removing portions whose total area is
less than ϵ > 0 and f (x,y) be a function continuous on both regions with |f (x,y)| < M
where M is a ﬁxed number. The two integrals
R R
R1 f (x,y)dR and
R R
R2 f (x,y)dR then
differ by less than Mϵ, that is, by a number less than that tends to zero with ϵ. Both these
observations follow from the fundamental rules stated above.
Thus, we see that an integral over a region R can be approximated as closely as we
please by evaluating it over a subregion of R whose total area differs from the area of R
by a sufﬁciently small amount. In a region R we can construct a polygon whose total area
differs from that of R by as little an amount as we please. In particular, we can construct
this polygon by piecing together rectangles whose sides are lines parallel to the axes.
11.4.4
Triple integrals
Whatever we have said about the integrals over a bounded, closed and connected region in
the x,y plane gets carried over, without further complication or introduction of new ideas,
to the integrals over a bounded, closed and connected region in the 3-D space called triple
integrals. In order to treat the integral over a 3-D region R, we need to subdivide R into
closed non-overlapping Jordan measurable subregions R1,R2,...,RN that completely ﬁll
R. If f (x) ≡f (x,y,z) is a function that is continuous in the region R and if (xi,yi,zi) is
an arbitrary point in the region Ri, we again form the sum
N
X
i=1
f (xi,yi,zi)∆Ri,
where ∆Ri is now the volume of the region Ri. The sum may be taken over all regions Ri,
or, over those Ri which are interior to R. If we now take the limit as N →∞such that the
largest of the diameters of Ri tends to zero, then the sum tends to a limiting value which is
independent of the mode of subdivision or the choice of the intermediate points. We call
this limit the integral of the function f (x,y,z) over the region R and write it as
Z Z
R
f (x,y,z)dR.

372
An Introduction to Vectors, Vector Operators and Vector Analysis
In particular, if the we subdivide into rectangular boxes with sides ∆x,∆y,∆z then the
volumes of all the inner regions Ri have the same value ∆x∆y∆z and the corresponding
integral is written as
Z Z Z
R
f (x,y,z)dxdydz.
Apart from the changes in notation all that has been said about the double integral is valid
for the triple integral.
11.4.5
Multiple integrals as successive single integrals
Evaluation of multiple integrals can be reduced to successive evaluation of single integrals.
This allows us to employ all the standard techniques available to evaluate indeﬁnite
integrals of a function of a single variable.
Integrals over a rectangle
We ﬁrst consider the case where the region of integration R is a rectangle a ≤x ≤b, c ≤
y ≤d. We want to integrate a continuous function f (x,y) over R. The procedure to do
this is given in the following theorem, which we state without proof.
To ﬁnd
R R
R f (x,y)dxdy we ﬁrst regard y as constant and integrate f (x,y) with respect
to x between the limits a and b. The resulting integral,
φ(y) =
Z b
a
f (x,y)dx
is a function of y, which we integrate between the limits c and d to obtain the double
integral. In symbols,
Z Z
R
f (x,y)dxdy =
Z d
c
φ(y)dy,
φ(y) =
Z b
a
f (x,y)dx,
or,
Z Z
R
f (x,y)dxdy =
Z d
c
dy
Z b
a
f (x,y)dx.
(11.92)
Since the roles of x and y are interchangeable, we have
Z Z
R
f (x,y)dxdy =
Z b
a
dx
Z d
c
f (x,y)dy.
(11.93)
Equations (11.92) and (11.93) together imply
Z d
c
dy
Z b
a
f (x,y)dx =
Z b
a
dx
Z d
c
f (x,y)dy.
(11.94)

Vector Integration
373
That is, in the repeated integration of a continuous function with constant limits of
integration, the order of integration can be reversed. This facility of changing the order of
integration is particularly useful in the explicit calculation of simple deﬁnite integrals for
which no indeﬁnite integral can be found.
Exercise
Evaluate I =
Z ∞
0
e−ax −e−bx
x
dx.
Solution
We can write
I = lim
T →∞
Z T
0
dx
Z b
a
e−xydy,
from which we obtain by changing the order of integration
I = lim
T →∞
Z b
a
1 −e−T y
y
dy = log b
a −lim
T →∞
Z b
a
e−T y
y
dy.
By virtue of the relation
Z b
a
e−T y
y
dy =
Z T b
T a
e−y
y dy,
the second integral tends to zero as T increases, so that,
I =
Z ∞
0
e−ax −e−bx
x
dx = log b
a.
□
Exercise
If f (t) is a C1 function of t except at countably many points for t ≥0 and if the
integral
Z ∞
1
f (t)
t
dt
exists, then show that, for positive a and b,
I =
Z ∞
0
f (ax) −f (bx)
x
dx = f (0)log b
a.
Hint
Write
I =
Z ∞
0
dx
Z a
b
f ′(xy)dy
and change the order of integration.
□

374
An Introduction to Vectors, Vector Operators and Vector Analysis
We can resolve a double integral into a succession of single integrals even if the region of
integration is not a rectangle. We ﬁrst consider a convex region R. A line parallel to x or
y axis cuts the boundary of such a region in not more than two points unless it forms a
part of the boundary (see Fig. 11.21). We can draw the so called lines of support giving the
circumscribing rectangle as shown in Fig. 11.21., at x = x0,x = x1,y = y0,y = y1. As we
move, for example, the line x = x0 towards right, it cuts the boundary of R at two points
whose y coordinates are functions of x say ψ1(x) and ψ2(x) as shown in Fig. 11.21.
Similarly, as we move the line y = y0 upwards, it cuts the boundary of R at two points
whose x coordinates are functions of y say φ1(y) and φ2(y) as shown in Fig. 11.21. Thus,
if we want to integrate f (x,y) over x for a ﬁxed value of y = yc we must integrate between
φ1(yc) and φ2(yc). Treating y as a parameter, then, the integral
Z φ2(y)
φ1(y)
f (x,y)dx
is a function of y and similarly, the integral
Z ψ2(x)
ψ1(x)
f (x,y)dy
is a function of the parameter x.
Fig. 11.21
General convex region of integration
The resolution of the double integral over R into repeated single integrals is then given by
the equations
Z Z
R
f (x,y)dR =
Z y1
y0
dy
Z φ2(y)
φ1(y)
f (x,y)dx
=
Z x1
x0
dx
Z ψ2(x)
ψ1(x)
f (x,y)dy.
(11.95)

Vector Integration
375
The generalization to the case of non-convex region R (see Fig. 11.22) is straightforward.
A line x =constant may now intersect the boundary of R in more than two points giving
rise to more than one segments over which we have to integrate f (x,y) with respect to
y. Each pair of the points of intersection of the line x =constant gives rise to a pair of
functions of x. By
R
f (x,y)dy we then mean the sum of the integrals of the function
f (x,y) for a ﬁxed x, taken over all the intervals that the line x =constant has in common
with the closed region.
It is possible to evaluate the double integral by dividing R into subregions each
corresponding to the ﬁxed number of terms in such a sum. The integral over x ranges
from x0 to x1 which are the circumscribing vertical lines for R, that is, along the whole
interval over which the region R lies.
Fig. 11.22
Non-convex region of integration
Exercise
Express the double integral of a function f (x,y) as a succession of single
integrals in (a) the unit disc deﬁned by x2 + y2 ≤1 and (b) the circular ring between the
circles x2 + y2 = 1 and x2 + y2 = 4.
Hint
See Fig. 11.23.
Fig. 11.23
Circular ring as a region of integration

376
An Introduction to Vectors, Vector Operators and Vector Analysis
Answer
(a)
Z Z
R
f (x,y)dR =
Z +1
−1
dx
Z +
√
1−x2
−
√
1−x2 f (x,y)dy.
(b)
Z Z
R
f (x,y)dR =
Z −1
−2
dx
Z +
√
4−x2
−
√
4−x2 f (x,y)dy +
Z 2
1
dx
Z +
√
4−x2
−
√
4−x2 f (x,y)dy
+
Z +1
−1
dx
Z +
√
1−x2
−
√
4−x2 f (x,y)dy +
Z +1
−1
dx
Z +
√
4−x2
−
√
1−x2 f (x,y)dy.
□(11.96)
Exercise
Express the double integral of a function f (x,y) as a succession of single
integrals over a triangle (Fig. 11.24) bounded by the lines x = y, y = 0 and x = a (a > 0).
Fig. 11.24
Triangle as a region of integration
Answer
Z Z
R
f (x,y)dR =
Z a
0
dx
Z x
0
f (x,y)dy
=
Z a
0
dy
Z a
y
f (x,y)dx.
□(11.97)
Extension to three dimensional regions
We ﬁrst consider the rectangular region R given by x0 ≤x ≤x1; y0 ≤y ≤y1; z0 ≤z ≤z1
and a function f (x,y,z) continuous in in this region. We can reduce the triple integral
V =
Z Z Z
R
f (x,y,z)dR
to a succession of single integrals or single and double integrals. For example,
Z Z Z
R
f (x,y,z)dR =
Z z1
z0
Z Z
B
f (x,y,z)dxdy,

Vector Integration
377
where
Z Z
B
f (x,y,z)dxdy
is the double integral taken over the rectangle described by x0 ≤x ≤x1; y0 ≤y ≤y1
evaluated at ﬁxed z so that the double integral is a function of the parameter z. Either of
the remaining coordinate x and y can be singled out in the same way.
The triple integral V can be evaluated as a succession of three single integrations. We
may ﬁrst consider the integration
Z z1
z0
f (x,y,z)dz,
x and y being ﬁxed and then the integration
Z y1
y0
dy
Z z1
z0
f (x,y,z)dz,
x being ﬁxed. We ﬁnally obtain
V =
Z x1
x0
dx
Z y1
y0
dy
Z z1
z0
f (x,y,z)dz.
In this repeated integral we could have carried out integration in any order, (say ﬁrst with
respect to x, then with respect to y and ﬁnally with respect to z) giving the same triple
integral. Thus, we can conclude that a repeated integral of a continuous function
throughout a closed rectangular region is independent of the order of integration.
Exercise
Express the triple integral of a function f (x,y,z) continuous on the closed
spherical region x2 + y2 + z2 ≤1 in terms of repeated single integrals.
Answer
Z Z Z
R
f (x,y,z)dxdydz =
Z +1
−1
dx
Z +
√
1−x2
−
√
1−x2 dy
Z +√
1−x2−y2
−√
1−x2−y2 f (x,y,z)dz.
□
Exercise
Find the mass of the right triangular pyramid of rectangular base sides a and
height 3a/2 with uniform density ρ (see Fig. 11.25).
Solution
Denoting the volume of the pyramid by V and its mass by M we know that
M = ρV . Thus, we have to ﬁnd the volume of the pyramid given by the triple integral
Z Z Z
P
dP,

378
An Introduction to Vectors, Vector Operators and Vector Analysis
where P is the pyramidal region of integration. To evaluate this triple integral, we convert it
to three repeated single integrals. We vary z from 0 to 3a/2. For a ﬁxed z, using similarity
of triangles AOC and ADE (see Fig. 3.55) we ﬁnd that y varies from 0 to a −2z/3.
Now ﬁxing both z and y and again using similarity of triangles which we leave for you
to ﬁnd, we see that x varies from 0 to a −2z/3 −y. Thus we get,
M = ρ
Z 3a/2
0
dz
Z a−(2z/3)
0
dy
Z a−(2z/3)−y
0
dx
= 1
4a3ρ.
□
Fig. 11.25
The right triangular pyramid
11.4.6
Changing variables of integration
Introducing new variables of integration is the principal method of transforming and
simplifying integrals. Here, we try and understand the general form of such a
transformation. Apart from facilitating the evaluation of double and triple integrals, these
transformations give us opportunity to apply the concept of integration in a wide variety
of contexts.
Consider the double integral
Z Z
R
f (x,y)dR =
Z Z
R
f (x,y)dxdy
over a region R in the x,y plane. Let
x = φ(u,v), y = ψ(u,v)

Vector Integration
379
be a 1 −1 mapping of R onto the closed region ˜R in the u,v plane. We assume that both
φ and ψ are C1 functions and their Jacobian determinant
D =

φu
φv
ψu
ψv
 = φuψv −ψuφv
is never zero in R. In other words, the functions x = φ(u,v) and y = ψ(u,v) possess a
unique inverse u = g(x,y) and v = h(x,y). Moreover, the two families of curves u =
constant and v = constant form a net over the region R. Each curve in the family u =
constant corresponds to a ﬁxed value of u and is parameterized by v. We have,
x(v) ≡(φ(u,v),ψ(u,v)) : u ﬁxed, v is the parameter
x(u) ≡(φ(u,v),ψ(u,v)) : v ﬁxed, u is the parameter.
(11.98)
We can construct the mesh of curves on the x,y plane as follows. We ﬁrst cover the u,v
plane by the rectangular mesh of straight lines u = ν∆u and v = µ∆v, ν,µ = 0,±1,
±2,... and then map each of these curves on the x,y plane by x = φ(u,v), y = ψ(u,v)
giving the mesh on the x,y plane by the curves deﬁned in Eq. (11.98). This mesh
subdivides the region of integration R into subregions Ri which are not, in general,
rectangular (see Fig. 11.16(b)). However, the subregions ˜Ri into which the region ˜R gets
divided are rectangular (see Fig. 11.16(a)). To ﬁnd the double integral, we have to ﬁnd the
area of the subregion Ri, multiply by the value of the function f at a point in Ri, sum this
product over Ri lying entirely within R and then take the limit of this sum as
∆u →0, ∆v →0.
Fig. 11.26
Changing variables of integration (see text)
The way we have constructed the subregions Ri tells us that the curves deﬁning its
boundary are separated pairwise by the parameter values ∆u and ∆v. The coordinates of
the vertices of ˜Ri are (uν,vµ),(uν + ∆u,vµ),(uν,vµ + ∆v), (uν + ∆u,vµ + ∆v) and the
x,y coordinates of the vertices of Ri are obtained by mapping these coordinates by φ and
ψ respectively. If Ri were a parallelogram joining these vertices, instead of being bounded

380
An Introduction to Vectors, Vector Operators and Vector Analysis
by curves, then the area of Ri is given by the absolute value of the determinant (or the
absolute value of the cross product of the corresponding vectors)

φ(uν + ∆u,vµ) −φ(uν,vµ)
φ(uν,vµ + ∆v) −φ(uν,vµ)
ψ(uν + ∆u,vµ) −ψ(uν,vµ)
ψ(uν,vµ + ∆v) −ψ(uν,vµ)
·
Since φ and ψ are C1, we can approximate, for example, φ(uν + ∆u,vµ) −φ(uν,vµ) by
φu(uν,vµ)∆u∆v so that the area of Ri is approximated by the absolute value of

φu(uν,vµ)
φv(uν,vµ)
ψu(uν,vµ)
ψv(uν,vµ)
∆u∆v = ∆u∆vD.
Thus, forming the required sum and passing to the limit as ∆u →0, ∆v →0, we obtain
the expression for the double integral transformed to the new variables,
Z Z
˜R
f (φ(u,v),ψ(u,v))|D|dudv.
We will not pause here to show that the area of Ri coincides with the corresponding
parallelogram in limit ∆u →0, ∆v →0 and state the ﬁnal result:
If the transformation x = φ(u,v);y = ψ(u,v) represents a continuous 1 −1 mapping
of the closed Jordan measurable region R of the x,y plane to a region ˜R of the u,v plane
and if the functions φ and ψ are C1 and their Jacobian
d(x,y)
d(u,v) = φuψv −ψuφv
is everywhere different from zero, then
Z Z
R
f (x,y)dxdy =
Z Z
˜R
f (φ(u,v),ψ(u,v))

d(x,y)
d(u,v)
dudv.
(11.99)
We may add that the transformation formula is valid even if the Jacobian determinant
vanishes without reversing its sign at a ﬁnite number of isolated points in the region of
integration. In this case we cut these points out of R by enclosing them in small circles of
radius ρ. Equation (11.99) is valid for the remaining region. If we then let ρ →0
Eq. (11.99) continues to be valid for the region R by virtue of the continuity of all
functions involved.
We can obtain the same result for the transformation of a triple integral over a three
dimensional region R which can be stated as follows.
If a closed Jordan measurable region R of x,y,z space is mapped on a region ˜R of
u,v,w space by a 1 −1 transformation
x = x(u,v,w),y = y(u,v,w),z = z(u,v,w)

Vector Integration
381
whose Jacobian determinant
d(x,y,z)
d(u,v,w)
is nowhere zero, then this transformation transforms the triple integral as
Z Z Z
R
f (x,y,z)dxdydz =
Z Z Z
˜R
F(r,θ,φ)

d(x,y,z)
d(u,v,w)
dudvdw,
(11.100)
where F(r,θ,φ) = f (x(u,v,w),y(u,v,w),z(u,v,w)).
Exercise
Find the transformed double and triple integrals respectively (a) for f (x,y)
over a closed disc of radius R in the polar and (b) for f (x,y,z) over a closed ball of radius
R in the spherical polar coordinates.
Solution
(a) We have x = r cosθ and y = r sinθ which easily gives ∂(x,y)
∂(r,θ) = r so that
Z Z
R
f (x,y)dxdy =
Z Z
˜R
f (r cosθ,r sinθ)rdrdθ.
The whole x,y plane is spanned by 0 ≤r < ∞and 0 ≤θ < 2π so that for a given
ﬁnite region, the integral on RHS can be replaced by
Z R
0
r
Z 2π
0
f (r cosθ,r sinθ)drdθ.
(b) The transformation is
x = r sinθ cosφ, y = r sinθ sinφ, z = r cosθ
with 0 ≤r < ∞, 0 ≤θ ≤π and 0 ≤φ < 2π. We obtain for the Jacobian determinant,
d(x,y,z)
d(r,θ,φ) =

sinθ cosφ
r cosθ cosφ
−r sinθ sinφ
sinθ sinφ
r cosθ sinφ
r sinθ cosφ
cosθ
−r sinθ
0

= r2 sinθ.
Thus, the required transformed integral is given by
Z Z Z
R
f (x,y,z)dxdydz =
Z R
0
r2
Z π
0
sinθ
Z 2π
0
F(r,θ,φ)drdθdφ, (11.101)
where F(r,θ,φ) = f (r sinθ cosφ,r sinθ sinφ,r cosθ).

382
An Introduction to Vectors, Vector Operators and Vector Analysis
For the spherical polar coordinates, the Jacobian determinant vanishes at r = 0 or
θ = 0,π corresponding to the origin and the whole of z-axis. However, there is no
trouble for our formula, which can be seen to be valid in the whole space in the same
way as we saw in the 2-D case, using the continuity of the functions involved.
□
Exercise
Find the transformed triple integral for f (x,y,z) over the whole space, in the
cylindrical coordinates ρ,θ,z related to cartesian coordinates by x = ρcosθ, y =
ρsinθ, z = z, where 0 ≤ρ < ∞, 0 ≤θ < 2π and −∞< z < +∞.
Solution
We easily ﬁnd that d(x,y,z)
d(ρ,θ,z) = ρ. This gives
Z Z Z
R
f (x,y,z)dxdydz =
Z ∞
0
ρ
Z 2π
0
Z +∞
−∞
F(ρ,θ,z)dρdθdz,
(11.102)
where F(ρ,θ,z) = f (ρcosθ,ρsinθ,z).
□
11.4.7
Geometrical applications
We already know that the volume of a 3-D region R is given by the integral
Z Z Z
R
dxdydz
over R. Expressing this integral as
R
dz
R R
dxdy is consistent with the fact that the volume
of a solid is known if we know the area of every planar cross section that is perpendicular
to deﬁnite line, say the z-axis. The generic triple integral given above, representing the
volume of a 3-D region, can be used to obtain closed form expressions for the volume of
a 3-D region, in terms of its geometrical characteristics. Here, we do this to calculate the
volumes of various solids.
To ﬁnd the volume of ellipsoid of revolution we write its equation
x2 + y2
a2
+ z2
b2 = 1
in the form
z = ±b
a
q
a2 −x2 −y2.
The volume of half of the ellipsoid above the x,y plane is given by the double integral
V = b
a
Z Z
R
q
a2 −x2 −y2dxdy

Vector Integration
383
over the disc R = x2 + y2 ≤a2. Transforming to polar coordinates, the double integral
becomes
Z Z
˜R
r
√
a2 −r2drdθ,
where the region ˜R is the rectangle 0 ≤ρ ≤a, 0 ≤θ ≤2π, so that resolving into single
integrals we get, for half the volume V ,
V = b
a
Z 2π
0
dθ
Z a
0
r
√
a2 −r2dr = 2πb
a
Z a
0
r
√
a2 −r2dr,
giving the required volume,
Ve = 2V = 4
3πa2b.
To ﬁnd the volume of the general ellipsoid,
x2
a2 + y2
b2 + z2
c2 = 1
we make the transformation
x = aρcosθ, y = bρsinθ, d(x,y)
d(ρ,θ) = abρ
to get, for half the volume
V = c
Z Z
R
r
1 −x2
a2 −y2
b2 dxdy =
Z Z
˜R
ρ
q
1 −ρ2dρdθ,
where the region ˜R is the rectangle 0 ≤ρ ≤1, 0 ≤θ ≤2π. Thus,
V = abc
Z 2π
0
dθ
Z 1
0
ρ
q
1 −ρ2dρ = 2
3πabc.
Therefore, the full volume Ve is
Ve = 2V = 4
3πabc.
Finally, we calculate the volume of the pyramid enclosed by the three coordinate planes
and the plane hx + ky + lz = 1 where we assume that h,k,l are positive. This volume is
given by
V = 1
l
Z Z
R
(1 −hx −ky)dxdy,

384
An Introduction to Vectors, Vector Operators and Vector Analysis
where the region of integration is the triangle 0 ≤x ≤1
h, 0 ≤y ≤(1−hx)
k
in the x,y plane.
Therefore,
V = 1
l
Z 1
h
0
Z
0
(1 −hx)
k
(1 −hx −ky)dy.
Integration with respect to y gives (1 −hx)2
2k
and we integrate again by substituting
1 −hx = t to get
V =
1
6hkl .
This agrees with the rule that the volume of a pyramid is one third of the product of its base
area with its height. Note that, in the single crystal scenario, h,k,l are the reciprocals of
the miller indices (which are positive integers with no common factors) of a crystal lattice
plane, intersecting the crystal axes, with intercepts h,k,l.
In many instances, the volume triple integral is evaluated by converting it to a
succession of single integrals over spherical polar or cylindrical coordinates. As a generic
application, we calculate the volume of a solid of revolution obtained by rotating a curve
x = φ(z) about the z-axis. We assume that the curve does not cross the z-axis and that the
solid revolution is bounded above and below by the planes z = constant. Therefore, the
inequalities deﬁning the solid are of the form a ≤z ≤b and 0 ≤x2 + y2 ≤(φ(z))2.
In terms of the cylindrical coordinates
z, ρ =
q
x2 + y2, θ = cos−1 x
ρ = sin−1 y
ρ
the volume triple integral becomes
Z Z Z
R
dxdydz =
Z b
a
dz
Z 2π
0
dθ
Z φ(z)
0
ρdρ.
This gives, after integration,
V = π
Z b
a
φ(z)2dz.
(11.103)
This integral can be interpreted as the sum of the volumes of the discs of radii φ(z) and
width ∆z stacked together to ﬁll the region of integration, in the limit ∆z →0.
Next, let the region R contain the origin O of the spherical polar coordinate system
(r,θ,φ) and let r = f (θ,φ) be the surface deﬁning the boundary of R. Then, the volume
of R is given by
V =
Z 2π
0
dφ
Z π
0
sinθdθ
Z f (θ,φ)
0
r2dr.

Vector Integration
385
Integrating with respect to r we get
V = 1
3
Z 2π
0
dφ
Z π
0
f 3(θ,φ)sinθdθ.
(11.104)
If R was a closed spherical ball of radius R, so that f (θ,φ) = R is constant, Eq. (11.104)
yields the volume 4
3πR3.
Area of a curved surface
We wish to ﬁnd an expression for the area of a curved surface by means of a double integral.
We construct a polyhedron circumscribing the given surface such that each of its polygonal
faces is tangent to the surface at one point, as follows.
We assume, that the surface is represented by a function z = f (x,y) with continuous
derivatives on a region R on the x,y plane. We subdivide R into n subregions Rν,ν = 1,
2,...,n with areas ∆Rν,ν = 1,2,...,n. In these subregions we choose points (ξν,ην),ν =
1,2,...,n. At the point on the surface xν,yν,ζν = f (xν,yν), we construct the tangent
plane to the surface and ﬁnd the area of the portion of this plane lying above the region Rν
(see Fig. 11.27). Let βν be the angle that the tangent plane
z −ζν = fx(ξν,ην)(x −ξν) + fy(ξν,ην)(y −ην)
Fig. 11.27
Tangent plane to the surface
makes with the x,y plane and let ∆τν be the area of the portion τν of the tangent plane
above Rν. Then, the region Rν is the projection of τν on the x,y plane. Therefore,
∆Rν = ∆τν cosβν.
To get cosβν note that βν is also the angle between the normals to the planes
φ1(x) = z = 0 and φ2(x) = (z −ζν) −fx(ξν,ην)(x −ξν) −fy(ξν,ην)(y −ην) = 0 or
between the gradients of φ1(x) and φ2(x). The vectors ∇φ1 and ∇φ2 are (001) and
(fx(ξν,ην)fy(ξν,ην)1) respectively. Evaluating their dot products by their components
and by their magnitudes and equating these, we get 1 = |∇φ1| |∇φ2|cosβν =
q
1 + f 2
x (ξν,ην) + f 2
y (ξν,ην)cosβν or,

386
An Introduction to Vectors, Vector Operators and Vector Analysis
cosβν =
1
q
1 + f 2
x (ξν,ην) + f 2
y (ξν,ην)
.
Therefore,
∆τν =
q
1 + f 2
x (ξν,ην) + f 2
y (ξν,ην) · ∆Rν.
We form the sum of all these areas
n
X
ν=1
∆τν
and let n →∞and simultaneously the diameter of the largest subdivision tend to zero.
This sum will then have the limit, independent of the way we subdivide R,
A =
Z Z
R
q
1 + f 2
x + f 2
y dR.
(11.105)
We use this integral to deﬁne the area of the given surface. Note that if the surface happens
to be a plane surface, for example z = f (x,y) = 0, we have
A =
Z Z
R
dR,
which agrees with our deﬁnition of the area of a planar region. Sometimes we call
dσ =
q
1 + f 2
x + f 2
y dR =
q
1 + f 2
x + f 2
y dxdy
the element of area of the surface z = f (x,y). The area integral can be written
symbolically in the form
A =
Z Z
R
dσ.
Exercise
Evaluate the area of a spherical surface of radius R.
Solution
The equation to the hemispherical surface of radius R can be written in the
z = f (x,y) form as
z =
q
R2 −x2 −y2.

Vector Integration
387
We ﬁnd
∂z
∂x = −
x
p
R2 −x2 −y2 ; ∂z
∂y = −
y
p
R2 −x2 −y2 .
The area of the full sphere is therefore given by the integral
A = 2R
Z Z
dxdy
p
R2 −x2 −y2 ,
where the region of integration is the circle of radius R lying in the x,y plane with its
origin at the center. Introducing polar coordinates and resolving into single integrals
we get
A = 2R
Z 2π
0
dθ
Z R
0
rdr
√
R2 −r2 = 4πR
Z R
0
rdr
√
R2 −r2 = 4πR2,
where the last integral on the right can be easily evaluated by substituting
R2 −r2 = u2.
□
If the equation to the surface is given in the form φ(x,y,z) = 0 then we get another
expression for its area. Assuming this equation gives implicit dependence of z on
independent variables x and y, and also that φz , 0, we get
dφ
dx = ∂φ
∂x + ∂φ
∂z
∂z
∂x = 0; or, fx = ∂z
∂x = −φx
φz
.
Similarly,
fy = ∂z
∂y = −
φy
φz
.
These two relations at once give the expression
A =
Z Z
R
q
φ2x + φ2y + φ2z

1
φz
dxdy
(11.106)
for the area where the region R is again the projection of the surface on the x,y plane.
If, instead of z = z(x,y), the surface was given by x = x(y,z) then the expression for
area would be
A =
Z Z q
1 + x2y + x2zdydz =
Z Z q
φ2x + φ2y + φ2z

1
φx
dydz,
(11.107)

388
An Introduction to Vectors, Vector Operators and Vector Analysis
or, if the surface was given by y = y(z,x) then the expression for area would be
A =
Z Z q
1 + y2x + y2z dzdx =
Z Z q
φ2x + φ2y + φ2z

1
φy
dzdx.
(11.108)
Equations (11.106), (11.107), (11.108) deﬁne the same area. To see this, apply the
transformation
x = x(y,z), y = y,
where x = x(y,z) is obtained by solving the equation φ(x,y,z) = 0, to the integral in
Eq. (11.106). The Jacobian determinant is
d(x,y)
d(y,z) = φz
φx
,
so that
Z Z
R
q
φ2x + φ2y + φ2z

1
φz
dxdy =
Z Z
˜R
q
φ2x + φ2y + φ2z

1
φx
dydz,
where ˜R is the projection of the surface on the y,z plane.
We can get rid of any special assumption about the relation of the surface and the
coordinate system, by representing the surface in the parametric form
x = φ(u,v), y = ψ(u,v), z = χ(u,v)
and expressing the area of the surface as an integral over the appropriate parameter domain.
Then, a deﬁnite region in the (u,v) plane corresponds to the surface. Without going into
the details we simply state the expression for the area of a surface in terms of its parametric
description.
A =
Z Z
R
q
(φuψv −ψuφv)2 + (ψuχv −χuψv)2 + (χuφv −φuχv)2dudv. (11.109)
Exercise
For a given surface parameterized by u,v show that
A =
Z Z √
EG −F2dudv
(11.110)
and that the element of area
dσ =
√
EG −F2dudv,
where E,F,G are the coefﬁcients of the line element given by Eqs (10.66), (10.67), (10.68).

Vector Integration
389
Hint
A simple calculation shows
EG −F2 = (φuψv −ψuφv)2 + (ψuχv −χuψv)2 + (χuφv −φuχv)2.
□
Exercise
Using the parametric representation of a sphere of radius R via the spherical
polar coordinates θ,φ, 0 ≤θ ≤π, 0 ≤φ ≤2π, show that
dσ = R2 sinθdθdφ
and hence show that the area of this sphere is 4πR2.
□
We can apply Eq. (11.110) to the surface of revolution formed by rotating the curve z =
φ(x), cut off by the two planes z = z0 and z = z1, about the z-axis. Referring the surface
to polar coordinates r,θ in the x,y plane as parameters, we get
x = r cosθ, y = r sinθ, z = φ(
q
x2 + y2) = φ(r).
Here, r coordinate of a point on the curve is the radius of the circle it traces out as it rotates
about the z-axis. This gives
E = 1 + φ′2(r), F = 0, G = r2
so that the area is given by
Z 2π
0
dθ
Z r1
r0
r
q
1 + φ′2(r)dr = 2π
Z r1
r0
r
q
1 + φ′2(r)dr.
Recognizing
q
1 + φ′2(r)dr = ds where s is the arc length parameter of the curve z =
φ(r) we can express the area of the surface of revolution in the form
2π
Z s1
s0
rds,
where r is the distance from the z-axis to the point on the rotating curve corresponding
to s.
Exercise
Use the above integral with respect to the arc length parameter to calculate
the surface area of the torus obtained by rotating the circle (x −a)2 + z2 = r2 about the
z-axis.
Solution
We introduce arc length as parameter, so that the distance u of a point on the
circle from the z-axis is given by u = a + r cos(s/r). The area is, therefore,
2π
Z 2πr
0
uds = 2π
Z 2πr
0

a + r cos s
r

ds = 2πa · 2πr.

390
An Introduction to Vectors, Vector Operators and Vector Analysis
The area of the torus is therefore equal to the product of the circumference of the
generating circle and the length of the path traced out by the center of the circle.
□
11.4.8
Physical applications of multiple integrals
Applications of multiple integrals to science and engineering are ubiquitous and are found
in all parts of it and in all kinds of situations. An exposure to it can really be obtained
through a wide verity of books and literature on various subjects and also while practising
different professions. Here, we give a brief account of some applications to mechanics.
Consider a distribution of n particles with respect to a cartesian coordinate system,
whose masses and positions are given by mν,(xν,yν,zν), ν = 1,2,...,n. Then, the
moments of such a mass distribution with respect to x,y y,z and z,x planes are deﬁned to
be Tz = Pn
ν=1 mνzν, Tx = Pn
ν=1 mνxν and Ty = Pn
ν=1 mνyν respectively. When we deal
with a continuous distribution of mass with density µ(x,y,z) in a region R in space or a
surface S or a curve γ, going through the same limiting process as we did while deﬁning
multiple integrals, The corresponding moments go over to
Tx =
Z Z Z
R
µxdxdydz, Ty =
Z Z Z
R
µydxdydz, Tz =
Z Z Z
R
µzdxdydz,
(11.111)
and we call these the moments of a volume distribution.
If the mass is continuously distributed over a surface S given by x = φ(u,v),
y = ψ(u,v), z = χ(u,v) with density µ(u,v), we deﬁne the moments of the surface
distribution by expressions
Tx =
Z Z
S
µxdσ =
Z Z
R
µx
√
EG −F2dudv,
Ty =
Z Z
S
µydσ =
Z Z
R
µy
√
EG −F2dudv,
Tz =
Z Z
S
µzdσ =
Z Z
R
µz
√
EG −F2dudv.
(11.112)
Finally, the moments of a curve x(s),y(s),z(s) in space with mass density µ(s) are
deﬁned by
Tx =
Z s1
s0
µxds, Ty =
Z s1
s0
µyds, Tz =
Z s1
s0
µzds,
where s denotes the arc length.
The center of mass of a continuous mass distribution over a region R, with total mass
M, is deﬁned as the point with coordinates
ξ = Tx
M , η =
Ty
M , ζ = Tz
M .
(11.113)

Vector Integration
391
That is, the center of mass has the coordinates, {ξ,η,ζ} = 1
M
R R R
R µ{x,y,z}dxdydz where
M =
R R R
R µdxdydz.
If the mass distribution is homogeneous, that is, µ = constant the center of mass of
the region is called its centroid. The centroid is clearly independent of the choice of the
constant positive value of the mass density. Thus, the centroid becomes a geometrical
concept associated only with the shape of the region R, independent of the mass
distribution.
Exercise
Find the center of mass of a homogeneous hemispherical region H with mass
density 1.
Solution
The region is given by x2 + y2 + z2 ≤1 ; z ≥0. The two moments Tx and Ty
are zero as the respective integrations with respect to x and y vanish. For
Tz =
Z Z Z
H
zdxdydz,
we introduce cylindrical coordinates (r,θ,z) via the equations
z = z, x = r cosθ, y = r sinθ
to get
Tz =
Z 1
0
zdz
Z √
1−z2
0
rdr
Z 2π
0
dθ = 2π
Z 1
0
1 −z2
2
zdz = π
4 .
Since the total mass is 2π
3 , the coordinates of the center of mass are (0,0, 3
8).
□
Exercise
Find the center of mass of a hemispherical surface of unit radius over which a
mass of unit density is uniformly distributed.
Solution
For the parametric representation
x = sinθ cosφ, y = sinθ sinφ z = cosθ
we get, for the surface element,
dσ =
√
EG −F2dθdφ = sinθdθdφ.
This leads to Tx = 0 = Ty because these involve integrating cosφ and sinφ over a single
period and
Tz =
Z π/2
0
sinθ cosθdθ
Z 2π
0
dφ = π.
Since the total mass is 2π we see that the coordinates of the center of mass are (0,0, 1
2). □

392
An Introduction to Vectors, Vector Operators and Vector Analysis
Moment of inertia
The moment of inertia plays the role of mass for the rotational motion of a rigid body. The
kinetic energy of a body rotating uniformly about an axis equals the product of the square
of the rotational velocity and the moment of inertia. The moment of inertia of a continuous
mass distribution with density µ(x) = µ(x,y,z) over a region R with respect to the x-axis
is given by
Ix =
Z Z Z
R
µ(y2 + z2)dxdydz.
This is simply integrating over the distance of every point (x,y,z) in R from the x-axis
multiplied by the mass density µ(x,y,z). The moments of inertia about the other two axes
are deﬁned similarly. The moment of inertia about a point, say the origin is deﬁned to be
Z Z Z
R
µ(x2 + y2 + z2)dxdydz
and the moment of inertia with respect to a plane say the y,z plane is
Z Z Z
R
µx2dxdydz.
A complete description of the arbitrary rotational motion of a rigid body requires the so
called products of inertia
Ixy = −
Z Z Z
R
µxydxdydz = Iyx,
Iyz = −
Z Z Z
R
µyzdxdydz = Izy,
Izx = −
Z Z Z
R
µzxdxdydz = Ixz.
(11.114)
The three quantities Ix,Iy,Iz and the six products of inertia are sufﬁcient to describe
arbitrary rotational motion of a rigid body. These nine quantities, written as a symmetric
matrix, are collectively called the moment of inertia tensor. The mutually perpendicular
axes with respect to which the moment of inertia tensor becomes diagonal are called the
principal axes. Generally, these are determined by the symmetry elements of the rigid body.
The moment of inertia with respect to an axis parallel to x-axis and passing through the
point (ξ,η,ζ), is given by the expression
Z Z Z
R
µ[(y −η)2 + (z −ζ)2]dxdydz,

Vector Integration
393
obtained by shifting the origin to (ξ,η,ζ).
If we let (ξ,η,ζ) be the coordinates of the center of mass and use Eq. (11.113) for the
coordinates of the center of mass, we immediately get
Z Z Z
R
µ(y2 + z2)dxdydz
=
Z Z Z
R
µ[(y −η)2 + (z −ζ)2]dxdydz + (η2 + ζ2)
Z Z Z
R
µdxdydz.
Since any arbitrary axis of rotation can be chosen to be the x-axis, the result we have got
can be expressed as follows.
The moment of inertia of a rigid body with respect to an arbitrary axis of rotation is
equal to the moment of inertia of the body about a parallel axis through its center of mass
plus the product of the total mass and the square of the distance between the center of mass
and the axis of rotation.
Finally, the moment of inertia of a surface distribution, with respect to the x-axis, is
given by
Z Z
S
µ(y2 + z2)dσ,
where µ(u,v) is the continuous function of two parameters u and v.
Exercise
Find the moment of inertia of a sphere of unit radius and unit density and
occupying region V , about any axis through its center at the origin.
Solution
By symmetry the moment of inertia about any axis through the origin is
I
=
Z Z Z
V
(y2 + z2)dxdydz,
=
Z Z Z
V
(x2 + z2)dxdydz,
=
Z Z Z
V
(x2 + y2)dxdydz.
(11.115)
Adding these three integrals we obtain,
3I = 2
Z Z Z
V
(x2 + y2 + z2)dxdydz.

394
An Introduction to Vectors, Vector Operators and Vector Analysis
In spherical polar coordinates
I = 2
3
Z 1
0
r4dr
Z π
0
sinθdθ
Z 2π
0
dφ = 8π
15 .
□
Exercise
For a beam with edges a,b,c parallel to x-axis, y-axis, z-axis respectively, with
unit density and the center of mass at the origin, ﬁnd the moment of inertia about the x,y
plane.
Solution
Z a/2
−a/2
dx
Z b/2
−b/2
dy
Z c/2
−c/2
z2dz = ab c3
12.
□
Exercise
Find the moment of inertia tensor of a right triangular pyramid with constant
density ρ shown in Fig. 11.25 about the origin O. Diagonalize this matrix using the
technique developed in Chapter 2 to ﬁnd its eigenvalues and eigenvectors, which deﬁne
the principal values and principal axes of this moment of inertia tensor.
Solution
With i,j = x,y,z we can write, for the elements of the moment of inertia
tensor,
Iij = ρ
Z 3a/2
0
dz
Z a−(2z/3)
0
dy
Z a−(2z/3)−y
0
dx


y2 + z2
−xy
−zx
−xy
z2 + x2
−yz
−zx
−yz
x2 + y2


·
(11.116)
We have already found that the total mass of the pyramid is M = 1
4a3ρ, or, ρ = 4M
a3 .
Carrying out the integrations in the above equation and using the expression for ρ in terms
of the total mass M we get
Iij = Ma2
40


13
−2
−3
−2
13
−3
−3
−3
8


·
(11.117)
In order to obtain the principal moments of inertia about the origin O and the principal
axes of inertia, we diagonalize the inertia tensor using the methods of section 5.1. The result
is
I (p)
ij
= Ma2
40


15
0
0
0
5
0
0
0
14


,
(11.118)
with the eigenvectors for the principal axes
ˆip =
1
√
2
(ˆi −ˆj)

Vector Integration
395
ˆjp =
1
√
6
(ˆi + ˆj + 2ˆk)
ˆkp = −
√
2
3 (ˆi + ˆj) + 1
√
3
ˆk,
(11.119)
where ˆi,ˆj, ˆk are the unit vectors along the x,y,z-axes shown in Fig. 11.25.
□
11.5
Integral Theorems of Gauss and Stokes in Two-dimensions
For a function f (x) of a single variable, the fundamental connection between
differentiation and integration is given by the equation
Z x1
x0
f ′(x)dx = f (x1) −f (x0).
where the integral is expressed in terms of the values of f (x) at the boundary points.
The corresponding result in two dimensions is called Gauss’s theorem, or the divergence
theorem. This theorem connects the integral of the divergence of a 2-D vector ﬁeld f(x) =
f (x)ˆi + g(x)ˆj in a 2-D region R with the line integral of the normal component of this
ﬁeld along the boundary curve of R taken in the positive sense, which we denote by C+. In
two dimensions, this theorem, stated in the form involving functions of several variables,
is called Gauss’s theorem, or the divergence theorem. When stated in the vector form, the
same result is called Stokes theorem. The divergence theorem in 2-D is thus stated as
Z Z
R
[fx(x,y) + gy(x,y)]dxdy =
Z
C+
[f (x,y)dy −g(x,y)dx],
(11.120)
where the boundary C of the region R is regarded as an oriented curve C+ choosing as
positive sense on C the one for which the region R remains on the left side as we traverse
it. As a special case, we note that f (x,y) = x, g(x,y) = 0 in Eq. (11.120) gives the area of
R in terms of a line integral on its oriented boundary C+ :
A =
Z Z
R
dxdy =
Z
C+
xdy.
Similarly, for f (x,y) = 0, g(x,y) = y we obtain
A =
Z Z
R
dxdy = −
Z
C+
ydx.
The divergence theorem holds good for any 2-D open set R bounded by one or more closed
curves, each consisting of a ﬁnite number of smooth arcs and for functions f and g which
are C1 throughout R and on C (see Fig. 11.28). We do not give the proof of this theorem

396
An Introduction to Vectors, Vector Operators and Vector Analysis
in detail, sufﬁce it to say that the proof is based on the method of expressing the double
integral as successive single integrals, which we have seen in detail before. We will explore
some of the applications of this all-important theorem.
Fig. 11.28
Divergence theorem for connected regions
Stokes theorem
As we have already noted, Stokes theorem in 2-D is obtained by casting the divergence
theorem in the vector form. Thus, let the functions f (x,y) and g(x,y) be the components
of a 2-D vector ﬁeld f. Thus, the integrand of the double integral in Eq. (11.120) simply
becomes the divergence of f. In order to get a vector expression for line integral in
Eq. (11.120), we parameterize the oriented boundary curve C+ by the arc length s. Here,
the sense of increasing s corresponds to the positive orientation of the boundary curve C.
The RHS of Eq. (11.143) then becomes
Z
C
[f (x,y) ˙y −g(x,y) ˙x]ds,
where we have put dx/ds = ˙x and dy/ds = ˙y.
We have seen earlier, (see section 9.2), that the vector ˆt with components ˙x and ˙y has
unit length and is in the direction of the tangent in the sense of increasing s and hence in
the direction corresponding to the orientation of C. The vector ˆn with components ξ = ˙y
and η = −˙x has unit length, and is orthogonal to the tangent. Moreover, ˆn has the the same
position relative to the vector ˆt as the positive x-axis has relative to the positive y-axis. This
can be seen from the continuity considerations. Suppose that the tangent to the curve is
continuously rotated to make it coincide with the y-axis such that ˆt points in the direction
of increasing y. Then, the components of ˆt are (0,1), so that ˆn becomes (1,0) and hence in
the positive direction of x-axis. Thus, if a π/2 clockwise rotation takes positive y-axis into
positive x-axis, the vector ˆn is obtained by a π/2 clockwise rotation from the tangent vector

Vector Integration
397
ˆt. Thus, ˆn is the normal pointing to the right side of the oriented curve C (see subsection
9.2.5). Since in this case C+ is oriented so as to have region R on the left side of C+, we see
that ˆn is the outward normal to the region R (see Fig. 11.29). The components ξ,η of the
unit vector ˆn are the direction cosines of the outward normal:
ξ = cosθ ; η = sinθ
Fig. 11.29
ˆn deﬁnes the directional derivatives of x and y
if ˆn subtends an angle θ with the positive x-axis. The components of ˆn are the directional
derivatives of x and y in the direction of ˆn as can be seen from ˆn·∇x = cosθ and ˆn·∇y =
sinθ. Denoting these directional derivatives by dx
dn and dy
dn respectively, we can write the
divergence theorem in the form:
Z Z
R
∇· fdxdy =
Z
C
 
f dx
dn + g dy
dn
!
ds.
(11.121)
Here, the integrand on the right is the scalar product f · ˆn of the vector f with components
f ,g and the vector ˆn with components dx
dn, dy
dn. Since ˆn is a unit vector, the scalar product
f· ˆn represents the component fn of the vector f in the direction of ˆn. Thus, the divergence
theorem takes the form
Z Z
R
∇· fdxdy =
Z
C
f · ˆnds =
Z
C
fnds.
(11.122)
In words, the double integral of the divergence of a plane vector ﬁeld over a set R is equal
to the line integral, along the boundary C of R, of the component of the vector ﬁeld in the
direction of the outward normal.
There is another form of Stokes theorem in the plane offering an entirely different vector
interpretation. To get to it, we put
a(x,y) = −g(x,y),
b(x,y) = f (x,y).

398
An Introduction to Vectors, Vector Operators and Vector Analysis
Then, by Eq. (11.120),
Z Z
R
(bx −ay)dxdy =
Z
C
(a ˙x + b ˙y)ds =
Z
C+
(adx + bdy).
(11.123)
We take the two functions a and b to be the components of a vector ﬁeld g, where g is
obtained at each point from vector f by its counterclockwise π/2 rotation. We see that
a ˙x + b ˙y = g · ˆt = gt,
where gt is the tangential component of the vector g. The integrand of the double integral
in Eq. (11.123) gives the z component of (∇× g)z of curlg provided we assume the ﬁeld
g to continue in the whole of 3-D space coinciding with g ≡(a(x,y),b(x,y)) on the x,y
plane. The Stokes theorem now takes the form,
Z Z
R
(∇× g)zdxdy =
Z
C
gtds.
Since any plane in space can be taken to be the x,y plane of a suitable coordinate system,
we arrive at the following general formulation of Stokes theorem.
Z Z
R
(∇× g)ndA =
Z
C
gtds,
(11.124)
where R is any plane region in space, bounded by the curve C and (∇×g)n is the component
of the vector ∇×g or curlg in the direction of the normal ˆn to the plane containing R. Here
C has to be oriented in such a way that the tangent vector ˆt points in the counterclockwise
direction as seen from the side of the plane toward which ˆn points. In other words, the
corresponding rotation of a right handed screw should advance it in the direction of ˆn.
If the complete boundary C of R consists of several closed curves (see Fig. 11.28), these
results remain valid provided we extend the line integral over each of these curves oriented
properly, so as to leave R on its left side. If the functions a and b satisfy the condition
ay = bx,
the expression adx+bdy becomes a perfect differential. Since ay = bx, the double integral
over R in Eq. (11.123) vanishes so that,
Z
C
(adx + bdy) = 0
(11.125)
whenever C denotes the complete boundary of a region R in which ay = bx holds. Further,
under these conditions, the integral
Z
C
(adx + bdy)

Vector Integration
399
over a path joining two end points P0 and P1 has the same value for all paths in R joining
the end points P0 and P1, provided R is simply connected (see section 11.1).
Exercise
Use the divergence theorem in the plane to evaluate the line integral
Z
C
f du + gdv
for the following functions and paths taken in the counterclockwise sense about the given
region.
(a) f = au + bv, g = 0, u ≥0 v ≥0 α2u + β2v ≤1. Ans: −
b
α2β2 .
(b) f = u2 −v2, g = 2uv, |u| < 1, |v| < 1. Ans: 0.
(c) f = vn, g = un, u2 + v2 ≤r2. Ans: 0.
□
Exercise
Obtain the formula for the divergence theorem in polar coordinates:
Z
C+
f (r,θ)dr + g(r,θ)dθ =
Z Z
R
1
r
"∂g
∂r −∂f
∂θ
#
dS.
□
Exercise
Assuming the conditions for the divergence theorem hold, derive the following
expressions in polar coordinates for the area of a region R with boundary C:
1
2
Z
C+
r2dθ,
−
Z
C+
rθdr,
where in the second formula we assume that R does not contain the origin.
Hint
Note that A = 1
2
R
C+(xdy −ydx).
□
Exercise
Apply Stokes theorem in the x,y plane to show that
Z Z
R
d(u,v)
d(x,y) dS =
Z
C+
u(∇v) · ˆtds,
where x = x(u,v) is a continuously differentiable 1 −1 transformation and ˆt is the
positively oriented unit tangent vector for C.
Hint
Write d(u,v)/d(x,y) = (uvy)x −(uvx)y = ∇× (u∇v).
□
Exercise
Let C1,C2,...,Cn be non-overlapping simple closed curves in the xy plane and
let C be a simple closed curve enclosing all Ci, i = 1,2,...,n. Let a(x,y) and b(x,y) be
continuously differentiable functions such that ay = bx outside Ci, i = 1,2,...,n. If
Z
Ci
(adx + bdy) = mi, i, i = 1,2,...,n,

400
An Introduction to Vectors, Vector Operators and Vector Analysis
then show that
Z
C
(adx + bdy) =
n
X
i=1
mi.
(11.126)
Solution
We ﬁrst make the region interior to C simply connected by means of cuts as
shown in Fig. 11.30. Let Γ be the boundary of the simply connected region so formed, say
R, oriented positively, that is, by traversing it keeping region R on the left. The positive
sense on Γ is indicated by the arrows in Fig. 11.30. We have,
Z
Γ
(adx + bdy) =
Z
C
(adx + bdy) −
Z
C1
(adx + bdy) −··· −
Z
Cn
(adx + bdy)
=
Z
C
(adx + bdy) −
n
X
i=1
mi.
(11.127)
Fig. 11.30
Γ is the boundary of a simply connected region
The integrals along the cuts cancel as they are traversed in opposite directions in going
around Γ . The negative signs in Eq. (11.127) occur as the positive sense of traversing
individual Cis is opposite to the positive sense of traversing Γ . Since Γ is the boundary of a
simply connected region R and ay = bx, in R, Eq. (11.125) holds with C replaced by Γ
and in conjunction with Eq. (11.127) establishes Eq. (11.126).
□
11.5.1
Integration by parts in two dimensions: Green’s theorem
The divergence theorem, as stated in Eq. (11.121), namely,
Z Z
R
(fx + gy)dxdy =
Z
C
 
f dx
dn + g dy
dn
!
ds
(11.128)

Vector Integration
401
combined with the rule for differentiating a product immediately gives a prescription for
integrating by parts that is basic to the theory of partial differential equations. We substitute
for both f and g a product of functions, namely, f (x,y) = a(x,y)u(x,y) and g(x,y) =
b(x,y)v(x,y), where the functions a,b,u,v are C1 in R as well as on C. Since
fx + gy = (aux + bvy) + (axu + byv),
we can write Eq. (11.128) as
Z Z
R
(aux + bvy)dxdy =
Z
C
 
au dx
dn + bv dy
dn
!
ds −
Z Z
R
(axu + byv)dxdy.
(11.129)
To get Green’s ﬁrst theorem, we impose u = v, a = ωx and b = ωy for some ω(x,y). We
assume that u is C1 while ω has continuous second derivatives in the closure of R. This
transforms Eq. (11.129) into
Z Z
R
(uxωx+uyωy)dxdy =
Z
C
u
 
ωx
dx
dn + ωy
dy
dn
!
ds−
Z Z
R
u(ωxx+ωyy)dxdy.
Recognizing
ωxx + ωyy = ∇2ω ≡∆ω
and that dx
dn and dy
dn are the direction cosines of the outward normal to the boundary C of
R so that
ωx
dx
dn + ωy
dy
dn = dω
dn
is the directional derivative ofω in the direction of the outward normal to C, we obtain, for
Green’s ﬁrst theorem,
Z Z
R
(uxωx + uyωy)dxdy =
Z
C
u dω
dn ds −
Z Z
R
u∆ωdxdy.
(11.130)
If in addition u has continuous second derivatives, we get from Eq. (11.130), by
interchanging the roles of u and ω the equation
Z Z
R
(ωxux + ωyuy)dxdy =
Z
C
ωdu
dnds −
Z Z
R
ω∆udxdy.
Subtracting the two equations gives an equation symmetric in u and ω known as Green’s
second theorem:
Z Z
R
(ω∆u −u∆ω)dxdy =
Z
C
 
u dω
dn −ωdu
dn
!
ds.
(11.131)

402
An Introduction to Vectors, Vector Operators and Vector Analysis
These two theorems of Green are basic in solving the partial differential equation (Laplace
equation) ∇2u = uxx + uyy = 0 (see books on Electrodynamics like [9, 13]).
11.6
Applications to Two-dimensional Flows
Electrodynamics and ﬂuid mechanics are the two areas of knowledge where the integral
theorems we have discussed, (or their 3-D generalizations we will encounter subsequently),
ﬁnd their most natural applications. Here, we deal with some applications to ﬂuid dynamics
and leave the electrodynamic version to standard books on the subject [9, 13]. In particular,
in this subsection we try and understand the fundamental role of the 2-D integral theorem
like the divergence or the Stokes theorem in modelling the motion of a liquid moving in the
x,y plane (remember that any plane can be taken to be the x,y plane). We use the velocity
ﬁeld, which is the assignment of the vector v(x,t) ≡(v1(x,t),v2(x,t)) to the position
vector x in the plane at time t, to describe the motion of the liquid on the x,y plane.
Let us ﬁrst assume that the velocity of the liquid is independent of (x,t). Then, the
amount of liquid crossing a line segment I of length s in the time interval [t,t + dt] ﬁlls at
t + dt a parallelogram of area (v · ˆn)sdt where ˆn is the unit normal vector to I pointing
to the side of I to which the liquid crosses (angle between ˆn and v is less than π/2), as
depicted in Fig. 11.31.
Exercise
Check that the parallelogram is formed by the points ( ˜x, ˜y) for which the
segment with end points ( ˜x, ˜y) and (x,y) = ( ˜x −v1dt, ˜y −v2dt) has points common
with I.
□
Fig. 11.31
Amount of liquid crossing segment I in time dt for uniform ﬂow of velocity v
We take this area of the parallelogram swept by the liquid crossing the segment I in time
dt (∠(ˆn,v) < π/2) to be positive while the corresponding area for the unit vector ˆn such
that ∠(ˆn,v) > π/2 is taken to be negative. If ρ is the density of the liquid, then (v· ˆn)ρsdt
is the mass of the liquid that crosses I toward the side to which ˆn points.
Now let C be the curve in the x,y plane. We select one of the two possible unit normals
along C and call it ˆn. In a ﬂow with velocity and density depending on x,t the integral
Z
C
(v · ˆn)ρds
(11.132)

Vector Integration
403
represents mass of the liquid crossing C in unit time toward the side of C to which ˆn points.
This follows by approximating C by a polygon and the ﬂow for which the velocity is constant
across each side of the polygon.
If C is the boundary of a region R and if ˆn is the outward normal, the integral
represents the mass of the liquid leaving R in unit time. Applying the divergence theorem
as in Eq. (11.122)we can express the ﬂow through C as a double integral
Z
C
(v · ˆn)ρds =
Z
C
(ρv) · ˆnds =
Z Z
R
∇· (ρv)dxdy.
(11.133)
We can compare this ﬂow of mass through C out of R with the change in mass contained
in R. The total mass of the liquid contained in the region R at time t is
Z Z
R
ρ(x,t)dxdy.
Thus, in unit time, the loss of mass from R is given by
−d
dt
Z Z
R
ρ(x,t)dxdy = −
Z Z
R
∂ρ
∂t (x,t)dxdy.
If we assume that the mass is conserved, then mass can only be lost from R by passing
through the boundary C. Hence, by Eq. (11.133) we have
Z Z
R
(∇· (ρv) + ∂ρ
∂t )dxdy = 0.
(11.134)
Since this identity holds for arbitrary R, if we progressively reduce the area of R the integral
will have the value given by the product of the integrand evaluated at some arbitrary point
in R and the area of R. Since area of R > 0, the integrand must vanish at all points at
which the velocity ﬁeld is deﬁned. Stated more rigorously, if we divide Eq. (11.134) by the
area of R then in the limit as area of R tends to zero, we get
∇· (ρv) + ∂ρ
∂t = 0.
(11.135)
This differential equation expresses the law of conservation of mass in the ﬂow. In terms of
the components (v1,v2) of the velocity vector we can write Eq. (11.135) as
∂ρ
∂t + v1
∂ρ
∂x + v2
∂ρ
∂y + ρ
 ∂v1
∂x + ∂v2
∂y
!
= 0.
(11.136)
An important special case is that of an incompressible homogeneous liquid in which the
density ρ has a constant value independent of location and time. In this case, Eqs (11.135),
(11.136) reduce to an equation involving the velocity vector alone:

404
An Introduction to Vectors, Vector Operators and Vector Analysis
∇· v = ∂v1
∂x + ∂v2
∂y = 0.
(11.137)
Combining Eqs (11.133) and (11.137) we see that the total amount of an incompressible
liquid crossing a closed curve C is zero:
Z
C
(v · ˆn)ds = 0.
(11.138)
Stokes theorem, in the form of Eq. (11.124) applied to the vector ﬁeld v has also
interesting consequences for the liquid ﬂow. The integral over a closed oriented curve C
namely,
Z
C
v · ˆtds
where ˆt is the unit tangent vector corresponding to the orientation of C, is called the
circulation of the liquid around C. By stokes theorem, this circulation is equal to the
double integral
Z Z
R
(∇× v)zdxdy
over the enclosed region R. Hence, the quantity
(∇× v)z = ∂v2
∂x −∂v1
∂y ,
(11.139)
called the vorticity of the motion, measures the density of circulation at the point
x ≡(x,y) in the sense that the area integral of the vorticity gives the circulation around
the boundary. A ﬂow is called irrotational if the vorticity vanishes everywhere, that is, if
(∇× v)z = ∂v2
∂x −∂v1
∂y = 0.
(11.140)
By stokes theorem, the circulation around a closed curve C vanishes if C is the boundary of
a region where the motion is irrotational. Since Eq. (11.140) is the condition for
v1dx + v2dy to be a perfect differential, there exists for an irrotational ﬂow in every
simply connected region a scalar valued function φ(x,t) such that
v(x) = −∇φ(x).
(11.141)
The scalar φ, which is determined within a constant, is called a velocity potential.
The irrotational motion of an incompressible homogeneous liquid satisﬁes both,
Eqs (11.137) and (11.140). Combining these, we ﬁnd that the velocity potential is a
solution of Laplace’s equation:
∆φ = φxx + φyy = 0.

Vector Integration
405
As an example, we consider the ﬂow corresponding to the solution
φ = alogr = alog
q
x2 + y2
of the Laplace equation. By Eq. (11.141) the velocity potential has the components
v1 = −ax
r2
v2 = −ay
r2
and is singular at the origin (see Fig. 11.32(a)). All velocity vectors point towards the origin
for a > 0, away from the origin for a < 0. The velocity of the liquid at a given location does
not change with time, although the velocities at different points are different. Such a ﬂow is
said to be a steady ﬂow. The circulation around any closed curve not passing through the
origin vanishes, since vorticity is zero as can be easily checked so that
Z
C
v · ˆtds =
Z
C
v1dx + v2dy = −
Z
C
dφ = 0.
The amount of liquid passing outward through a simple closed curve C in unit time is
ρ
Z
C
v · ˆnds = ρ
Z
C
 
v1
dy
ds −v2
dx
ds
!
= ρ
Z
C
v1dy −v2dx = −aρ
Z
C
xdy −ydx
x2 + y2 ,
where θ is the polar angle from origin.
Fig. 11.32
(a) Flow with sink and (b) Flow with vortex
Exercise
Show that
Z
C
xdy −ydx
x2 + y2
=
Z
C
dθ.
Hint
Put x = cosθ and y = sinθ.
□
We assume that C does not pass through the origin. If C encloses the origin and is oriented
counterclockwise,
R
C dθ =
R 2π
0
dθ = 2π while if C does not enclose the origin then the

406
An Introduction to Vectors, Vector Operators and Vector Analysis
starting and ﬁnishing values of θ are the same as we trace the simple closed curve C once,
making the limits of the integral the same, so that the value of the integral is zero. Therefore,
ρ
Z
C
v · ˆnds =

0
if C does not enclose the origin
−2πaρ
if C encloses the origin.
Thus, the amount of mass ﬂowing through every simple closed curve C enclosing the origin
in unit time is the same. For a > 0 the origin acts as a sink where mass disappears at the
rate of 2πaρ units in unit time. For a < 0 there is a source of mass at the origin, giving out
mass at the same rate.
Let us now consider a steady ﬂow given by the velocity potential
φ = cθ = ctan−1 y
x.
Despite φ being multiple valued, the corresponding velocity ﬁeld is single valued:
v1 = cy
r2
v2 = −cx
r2 .
The vector ﬁeld v is everywhere normal to the radii from the origin (see Fig. 11.32(b)).
Again, the velocity ﬁeld is singular at the origin.
The circulation around a closed curve C has the value
Z
C
v1dx + v2dy = −
Z
C
dφ = −c
Z
C
dθ.
Thus, the circulation is zero for a simple closed curve not enclosing the origin. For a simple
closed curve encircling the origin in the counterclockwise sense we ﬁnd the value −2πc for
the circulation. This corresponds to a vortex of strength −2πc concentrated at the origin.
On the other hand, the ﬂow of mass in unit time through any closed curve C not passing
through the origin is zero, since here
ρ
Z
C
v · ˆnds = cρ
Z
C
xdx + ydy
x2 + y2
= cρ
Z
C
dr
r = 0.
Thus, the origin is not a source or sink of mass.
11.7
Orientation of a Surface
While learning about the line integral of a vector or a scalar valued function along a curve
C in a plane or in space, we found that the curve C cannot be treated just as a collection of
points in space, but needs to be assigned some sense or orientation. Similarly, the surface
integral, which we will study next, requires an orientation to be assigned to the surface over
which the integral is carried out. Thus, we need the deﬁnition as well as the understanding
of just how to assign an orientation to a surface.

Vector Integration
407
We consider a 2-D surface in the 3-D space which is piecewise smooth, that is, every
point P0(x0) of the surface has a neighborhood S which can be represented by a vector
valued function of two parameters x(u,v) having continuous partial derivatives with
respect to u and v in S. All points in S are covered by varying x(u,v) as the parameters
u,v vary over an open set γ in the u,v plane such that different (u,v) correspond to
different points on S. Further, we want the function x(u,v) to have the derivatives
xu(u,v) and xv(u,v) with respect to u,v in γ that are continuous and linearly
independent. We call such a representation a regular local representation of the surface.
We have seen that, the equations
x = x(u,v), y = y(u,v), z = z(u,v),
equivalent to x = x(u,v), represent a surface provided
xu × xv , 0 or |xu × xv|2 > 0.
Using identity II, this condition can be converted to
|xu × xv|2 = (xu × xv) · (xu × xv) =

xu · xu
xu · xv
xv · xu
xv · xv
 > 0,
where the determinant in this equation, denoted Γ (xu,xv), is called the Gram determinant.
Γ (xu,xv) = 0 implies that xu × xv = 0, that is, xu,xv are collinear and hence linearly
dependent. Conversely, if |xu × xv|2 = Γ (xu,xv) > 0, then xu × xv , 0 so that xu,xv
are not collinear and hence are linearly independent. Therefore, the fact that x(u,v) is a
regular local parameterization of the surface implies that Γ (xu,xv) > 0, so that xu,xv are
linearly independent.
The vectors xu(u,v) and xv(u,v) at a point P = x(u,v) of S are tangential to S at P
and span the tangent plane π(P ) of S at P . Thus, every point of the tangent plane has the
position vector xT (u,v) = x(u,v) + λxu(u,v) + µxv(u,v) with suitable coefﬁcients λ
and µ.
In order to orient the surface S, we ﬁrst assign an orientation to the tangent plane π(P ).
Orienting a plane means specifying one of the two sides of it. This can be done by specifying
one of the two unit normals to the plane. In order to specify one of the two unit normals
to the tangent plane π(P ) and make it the oriented tangent plane π∗(P ), we specify an
ordered pair of linearly independent vectors ξ(P ) and η(P ) in π(P ). The order of these
vectors, ξ(P ),η(P ) or η(P ),ξ(P ) decides which of the two possible directions along the
line normal to the plane π(P ) at P is the direction of the corresponding vector product
ξ(P ) × η(P ) or η(P ) × ξ(P ) (see Fig. 11.33).

408
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 11.33
Unit vector ˆn gives the orientation of oriented surface S∗at P
Thus, the orientation of π∗(P ) is speciﬁed in terms of the direction of the vector product of
ξ(P ),η(P ), with the order of factors being the same as that of the ordered pair of ξ(P ) and
η(P ) chosen to specify the orientation of π∗(P ). Thus, the oriented tangent plane π∗(P )
can be speciﬁed by the pair (π(P ), ˆn) or (π(P ),−ˆn) where
ˆn = ξ(P ) × η(P )
|ξ(P ) × η(P )|
(11.142)
or
−ˆn = η(P ) × ξ(P )
|η(P ) × ξ(P )|
(11.143)
is the unit vector giving the direction of the vector product of ξ(P ) and η(P ) in the
chosen order. Since there are only two possible orientations of π(P ), they can be speciﬁed
via a dicotomic function Ω(π∗(P )) = Ω(ξ(P ),η(P )) = ±1 where each of the two values
corresponds to one of the two possible orientations, but which value corresponds to which
orientation is arbitrary (see subsection 1.16.1). Any other ordered pair of independent
tangential vectors ξ′(P ),η′(P ) at P determines the same orientation if the angle between
the corresponding vector products is less than π/2, that is,
[ξ(P ),η(P );ξ′(P ),η′(P )] = (ξ(P )×η(P ))·(ξ′(P )×η′(P )) =

ξ · ξ′
ξ · η′
η · ξ′
η · η′
 > 0.
where we have used identity II. Generally, we can say that
Ω(ξ(P ),η(P )) = sgn[ξ(P ),η(P );ξ′(P ),η′(P )]Ω(ξ′(P ),η′(P ))
(11.144)

Vector Integration
409
where sgn(x) equals −1 for x < 0 and +1 for x ≥0 respectively. Equivalently, the the
ordered pairs of tangential vectors ξ,η and ξ′,η′ give the same orientation to π if
ˆn · ˆn′ > 0
where ˆn and ˆn′ are the unit vectors specifying the directions of ξ × η and ξ′ × η′
respectively. Since ξ(P ),η(P ) and ξ′(P ),η′(P ) belong to the same plane, there are only
two possibilities: ˆn · ˆn′ = +1 (ˆn′ = ˆn) or ˆn · ˆn′ = −1 (ˆn′ = −ˆn).
We now use the orientation of the tangent plane to the surface S at a point P on S to
deﬁne the orientation of the surface S in the following way. We say that the unit normals
deﬁning the orientations of the tangent planes π∗(P ) depend continuously on P , when
these normals to the planes π∗(P ) at the points close to each other (in the Euclidean
sense) are themselves close to each other (in the Euclidean sense). That is, given ϵ > 0
however small, there exists δ > 0 such that,
p
(u −u1)2 + (v −v1)2 < δ implies
|ˆn(x(u,v)) −ˆn(x(u1,v1))| < ϵ. This is expressed by saying that the the orientation
Ω(π∗(P )) of tangent plane at P varies continuously as P varies on S. An oriented surface
S∗is deﬁned as a surface S with continuously oriented tangent planes π∗(P ).
It is possible to ﬁnd another criterion to ascertain the unit vector deciding the
orientation of a tangent plane. If {ξ,η} stands for one of the two ordered pairs drawn out
of ξ(P ) and η(P ), then the corresponding unit normal ˆn deciding the orientation of
π∗(P ) is the one which makes the triplet {ξ,η}, ˆn positively oriented (see section 1.16).
This is equivalent to the following inequality,
ˆn · {ξ × η} = det(ˆn,{ξ,η}) = |{ξ × η}| > 0.
(11.145)
Thus, out of the two possible unit vectors perpendicular to π(P ) at P , the unit vector
satisfying inequality Eq. (11.145) decides the orientation of π(P ). As we show in the next
para, this vector also speciﬁes the orientation of a connected surface S. Let ˆe1, ˆe2, ˆe3 be
the orthonormal basis to which all vectors are referred. Then, if the triplets (ξ,η, ˆn) and
ˆe1, ˆe2, ˆe3, have the same orientation, we can write (see section 1.16),
Ω(ξ,η, ˆn) = Ω(ˆe1, ˆe2, ˆe3)
and we call this vector ˆn, deﬁning the orientation of S∗, the unit normal vector pointing to
the positive side of the oriented surface S∗or the positive unit normal to S∗.
We can now understand how to assign an orientation to a connected surface S. We
choose a point P on S, and the pair (ξ,η) in the tangent plane π(P ), which decides, via
Eq. (11.142), one of the two possible unit vectors ˆn and ˆn′ specifying the orientation of
S at P . This unit vector actually speciﬁes the orientation of the whole surface S, as the
following argument shows. At P we have ˆn′ = ϵ ˆn, where ϵ = ϵ(P ) = ±1. Since the unit
vectors ˆn, ˆn′ are assumed to vary continuously with P , the same is true for ϵ(P ) = ˆn · ˆn′.
Thus, ϵ is a continuous function on S having only the values +1 or −1. If ϵ(P ) , ϵ(Q)
for any two distinct points P and Q on S, it follows from the continuity of ϵ that ϵ = 0
at some point along a curve on S joining P and Q, contradicting the deﬁnition of ϵ. As a

410
An Introduction to Vectors, Vector Operators and Vector Analysis
result, ϵ has same value at all points on S. Thus, any orientation of S is given by either the
unit normal ˆn(P ) or ˆn′(P ) = −ˆn(P ). If the positive unit normal corresponding to S∗is ˆn,
the other possible orientation corresponding to −ˆn as its positive unit normal is called −S∗.
From Eq. (11.144) we see that
Ω(−S∗) = −Ω(S∗),
where Ω(S∗) = Ω(ξ(P ),η(P )) for some tangent plane π(P ) on S. Thus, the orientation
of the positive normal ˆn to a connected surface S at a single point P uniquely determines
the positive normal at any other point Q and hence determines the orientation of S. All
that we need to do is to continuously carry the positive unit normal at P to Q along a curve
on S joining P and Q, so that it coincides with the positive unit normal at Q to S. There
are connected surfaces on which a positive unit normal at a point cannot be transported
along a curve on the surface, to coincide with the positive unit normal at some other point
on the surface. Such a surface cannot be assigned any orientation and is not orientable. The
Mobius strip is the most celebrated example of a connected surface that is not orientable.
Orientation of a surface S becomes quite simple if it forms the boundary of a region R
in space. Such a surface can be oriented even if it is not connected, as, for example, the
surface forming the boundary of a spherical shell. At each point P on S we can distinguish
an interior normal pointing into R from an exterior normal pointing away from R. Both
these normals vary continuously with P . We can take the exterior normal as the positive
normal to deﬁne an orientation of S. We call the resulting oriented surface S∗Oriented
positively with respect to R. Thus, for example, for a spherical shell
a ≤|x| ≤b
the positive oriented boundary S∗of R has the positive unit normal
ˆn = −x/a for |x| = a and ˆn = x/b for |x| = b.
Let a portion of a oriented surface S∗have a regular parametric representation x = x(u,v)
with (u,v) varying over an open set γ of the u,v plane. Then,
ˆz = xu × xv
|xu × xv|
(11.146)
deﬁnes a unit normal vector for (u,v) in γ. If ˆn is the positive unit normal to S∗we have
ˆn = ϵˆz
with ϵ = ϵ(u,v) = ±1. By continuity of ˆn and ˆz ϵ is continuous which, when coupled with
the fact that ϵ = ±1, would mean that ϵ is constant on every connected component of γ.
For ϵ = 1, that is, for
Ω(S∗) = Ω(xu,xv),

Vector Integration
411
we say that S∗is oriented positively with respect to parameters u,v and write
Ω(S∗) = Ω(u,v).
If the same part of S∗has another regular parametric representation in terms of parameters
¯u, ¯v varying over the region ¯γ, we have, by Eq. (10.65),
xu × xv =
 d(y,z)
d(u,v), d(z,x)
d(u,v), d(x,y)
d(u,v)
!
,
or,
xu × xv = d( ¯u, ¯v)
d(u,v)(¯xu × ¯xv).
Thus, the unit normals ˆz and ˆ¯z for the two parametric representations are related by
ˆz = sgn
 d( ¯u, ¯v)
d(u,v)
!
ˆ¯z.
Thus, S∗is oriented positively with respect to both the parameterizations provided
d( ¯u, ¯v)
d(u,v) > 0.
As an illustration, we consider the unit sphere S∗with center at the origin, oriented
positively with respect to its interior. With u = x and v = y as parameters for z , 0, we
have,
x = (u,v,ϵ
√
1 −u2 −v2), where ϵ = sgn z.
The corresponding unit normal vector ˆz given by Eq. (11.146) becomes
ˆz = (ϵx,ϵy,ϵz) = ϵ ˆn
where ˆn is the exterior unit normal. Hence, S∗is oriented positively with respect to the
parameters x,y for z > 0 and negatively for z < 0 (see Fig. 11.34).
Fig. 11.34
Orientation of S with respect to u,v

412
An Introduction to Vectors, Vector Operators and Vector Analysis
We end this subsection by demonstrating the non-orientability of the Mobius strip. We
can easily produce Mobius strip by fastening the ends of a rectangular strip of paper after
rotating one of the ends by 180◦(see Fig. 11.35). Starting with the initial rectangle 0 < u <
2π, −a < v < a (where 0 < a < 1) in the u,v plane, we rigidly move each segment u =
constant so that its center moves to the point (cosu,sinu,0) of the unit circle in the x,y
plane, the segment is perpendicular to the tangent of the circle at that point and makes the
angle u/2 with the positive direction of the z-axis, to get the Mobius strip.
Fig. 11.35
Mobius strip
The assumption a < 1 keeps the surface from intersecting itself. The resulting strip has the
parametric representation
x =

1 + v sin u
2

cosu,

1 + v sin u
2

sinu,v cos u
2

with v restricted to −a < v < a. The points (u,v),(u + 4π,v),(u + 2π,−v) in the u,v
plane correspond to the same point on the surface. Making a deﬁnite choice of parameters
u0,v0 for an arbitrary point P on the surface, Eq. (11.147) gives a regular local parametric
representation of S for (u,v) ∈γ given by
u0 −π < u < u0 + π,
−a < v < a.
Along the center line v = 0 on the surface, Eq. (11.146) deﬁnes a unit normal vector
ˆz =

cosu cos u
2,sinu cos u
2,−sin u
2

that varies continuously with u. Starting out with the unit normal ˆz = (1,0,0) at the point
(1,0,0) of S corresponding to u = 0 and letting u increase from 0 to 2π, we complete a
circuit along the center line of the surface, returning to the same point but with the opposite
unit normal ˆz = (−1,0,0). We ﬁnd similarly that carrying a small oriented tangential curve
along the circuit, we return to the same point with its orientation reversed. Thus, it is not
possible to choose a continuously varying unit normal, or a side of S in a consistent way.
In other words, the Mobius strip is not orientable.

Vector Integration
413
Exercise
Let S be the mobius strip with the parametric representation given by
Eq. (11.147). (a) Show that the line v = a/2 divides S into an orientable and a
non-orientable set. (b) Show that the line v = 0 does not divide S, that is, the set S1
obtained by removing all points with v = 0 from S is still connected. (c) Show that S1 is
orientable.
Solution
(a) The line v = a/2 divides S into a part S′ a/2 < v < a (or, equivalently, −a < v <
−a/2) and oriented by ξ = xu, η = xv and a part S′′ given by −a/2 < v < a/2
which is just another Mobius strip.
(b) S1 is representable by Ω(ξ(P ),η(P )) with v restricted to the interval 0 < v < a,
where P can be varied continuously over S1. Obviously, any two points on S1 can be
joined by a curve on S1 which is the image of the corresponding points (u,v) in the
parameter plane.
(c) S1 is oriented by ξ = xu, η = xv.
□
Exercise
Let ξ, η be independent vectors in a plane π. Put a = |ξ|2, b = ξ · η, c = |η|2
form for any θ the vector
X(θ) =
 
cosθ −
b
√
ac −b2 sinθ
!
ξ + asinθ
√
ac −b2 η.
Prove that X(θ) is obtained by rotating the vector ξ in the plane π by an angle θ in the
sense given by the orientation Ω(ξ,η).
Solution
We can easily check that X(θ) has length |ξ| and is linearly dependent on ξ, η
so that X(θ) lies in π. Moreover, X(θ) · ξ/|ξ|2 = cosθ. The vector X(θ) coincides with
ξ for θ = 0 and has the direction of η for a certain θ between 0 and π, or, for that θ
determined by the relations
cosθ =
b
√ac, sinθ =
r
1 −b2
ac .
□
Summary
We now summarize the relevant points covered in this section. First, an orientable surface
S has two possible orientations which are given by the two possible normals to the surface
at some point P on it. These are obtained by choosing two non-collinear (linearly
independent) vectors ξ and η based at P , spanning the tangent plane to S at P . We form
two possible ordered pairs (ξ,η) and (η,ξ) to deﬁne two possible unit normals to the
tangent plane or to the surface S at P :
ˆn = ξ × η
|ξ × η| ; −ˆn = η × ξ
|η × ξ|.

414
An Introduction to Vectors, Vector Operators and Vector Analysis
We denote the corresponding oriented surface by S∗. From the above equations it is clear
that the triplet (ξ,η, ˆn) is positively oriented, while the triplet (ξ,η,−ˆn) is negatively
oriented. In order to decide which of these unit normals deﬁne the positive orientation of
S∗, we ﬁrst orient the 3-D space containing S∗as follows. We choose a coordinate system
deﬁned by the orthonormal basis (ˆe1, ˆe2, ˆe3) to resolve the vectors in some region of space
R and say that R is positively oriented if this coordinate system is right handed and
negatively oriented if it is left handed. If the coordinate system is right handed, it has the
same orientation as the triplet (ξ,η, ˆn) and we say that ˆn deﬁnes the positive orientation
of S∗and −ˆn deﬁnes its negative orientation. On the other hand, if the coordinate system
is left handed, it has the same orientation as (ξ,η,−ˆn) and we say that −ˆn deﬁnes the
positive orientation of S∗and ˆn deﬁnes its negative orientation. In general, a unit vector ˆn
deﬁning the positive orientation of S∗is said to be on the positive side of S∗. For a closed
surface S at the boundary of a region R we choose the coordinate system such that the
unit normal deﬁning the positive orientation of S∗is its outward normal.
If a surface S is parameterized by a C1 function x = x(u,v), we can replace the pair
(ξ,η) by the pair of tangent vectors (xu,xv) to deﬁne the unit vector, via their vector
product, giving the orientation of S∗. If this unit vector ˆz deﬁnes the positive orientation of
S∗, we say that S∗is positively oriented with respect to the parameters u,v.
Now, consider an oriented surface S∗with an oriented and closed boundary curve C∗.
Let the unit normal vector ˆn at point P on S decide the orientation of S∗. We drop a
perpendicular from P to the plane containing the curve C∗to meet this plane at point O.
Let P1 and P2 be the points on C∗such that traversing C∗from P1 toward P2 is in the same
sense deﬁning the orientation of C∗. Then C∗is positively oriented with respect to S∗if the
triplet (−−→
OP 1, −−→
OP 2, ˆn) is positively oriented. Further, we say that S∗is positively oriented
with respect to the x,y axes if the triplet (ˆe1, ˆe2, ˆn) is positively oriented.
11.8
Surface Integrals
The orientation of the region over which an integral is carried out is fundamentally
connected to its value, although the Riemannian sums involved are deﬁned in terms of
quantities like length, area and volume, which are inherently positive quantities. Thus, if
we want the additivity rule
Z b
a
f (x)dx +
Z c
b
f (x)dx =
Z c
a
f (x)dx
to hold without restricting the relative positions of a,b,c, we have to deﬁne
R b
a f (x)dx both
for a ≤b as well as a ≥b by the relation
Z b
a
f (x)dx = −
Z a
b
f (x)dx.
(11.147)

Vector Integration
415
Geometrically, the ordered pair of numbers a,b determines an oriented interval I∗on the
x-axis with initial point a and the ﬁnal point b. The value of
Z b
a
f (x)dx =
Z
I∗f (x)dx
is the one given by the limit of the Riemann sum (positive for positive f ) when the
orientation of I∗corresponds to the sense of increasing x, that is, for a < b. Interchanging
the end points of I∗converts I∗into the interval −I∗, with opposite orientation, so that
Eq. (11.147) can also be written as
Z
−I∗f (x)dx = −
Z
I∗f (x)dx.
(11.148)
A similar situation prevails regarding the integral over an oriented region R∗in the x,y
plane. When R∗is oriented positively with respect to the ˆe1, ˆe2 basis deﬁning the
coordinate system, Ω(R∗) = Ω(ˆe1, ˆe2), the differential area dxdy is positive and the
double integral
Z Z
R∗f (x,y)dxdy
is the limit of the Riemann sums obtained from the subdivisions of the plane into squares
of area 2−2n. The integral has a non-negative value for a non-negative f . In case Ω(R∗) =
−Ω(ˆe1, ˆe2) = Ω(ˆe2, ˆe1) resulting in a negative value for the differential area dydx we get
Z Z
R∗f dxdy = −
Z Z
R∗f dydx,
where the integral on the right has the usual meaning as the limit of sums. Thus, we have
the rule that
Z Z
−R∗f dxdy = −
Z Z
R∗f dxdy,
where −R∗is obtained by changing the orientation of R∗. The substitution formula given
by Eq. (11.99) becomes, for the oriented region R∗,
Z Z
R∗f (x,y)dxdy =
Z Z
T ∗f (x(u,v),y(u,v)) d(x,y)
d(u,v)dudv,
for smooth 1 −1 mappings
x = x(u,v), y = y(u,v)

416
An Introduction to Vectors, Vector Operators and Vector Analysis
of T ∗onto R∗as long as the Jacobian determinant d(x,y)/d(u,v) has the same sign
throughout T ∗. The sign given by the orientation of R∗or that of T ∗to the corresponding
integrals is determined as follows. The rule is that the orientation of R∗attributes a
positive sign to dxdy if the x,y coordinate system has the orientation of R∗and negative
one otherwise. The sign attributed by the orientation of T ∗to dudv is then the one that
agrees with the relation
dxdy = d(x,y)
d(u,v)dudv.
Once the proper sign is attached to the differential area dS = dxdy or dT = dudv, the
rest of the integration amounts to the evaluation of the corresponding double integral.
While learning about line integrals, we came across linear differential forms, also
called ﬁrst order differential forms, which are expressions linear in the differentials
dx,dy,dz. A second order differential form is an expression quadratic in the differentials
dx,dy,dz and has the form
ω = a(x)dxdy + b(x)dydz + c(x)dzdx
where a,b,c are C1 functions over their domain. Here, we obtain a general form of the
surface integral of the second order differential form over an oriented surface S∗in terms
of the surface integral of functions over the unoriented surface S. We already know that if
S has the parametric representation
x = x(u,v), y = y(u,v), z = z(u,v)
and if ξ,η,ζ denote the components of the normal vector
ξ = d(y,z)
d(u,v), η = d(z,x)
d(u,v), ζ = d(x,y)
d(u,v),
(11.149)
the area of S is given by
A =
Z Z
R
q
ξ2 + η2 + ζ2dudv.
Here, the integral is over the region R in the u,v plane corresponding to S The integral is
understood in the sense of a double integral with the surface element
dS =
q
ξ2 + η2 + ζ2dudv
being treated as a positive quantity or, equivalently, R is given the positive orientation with
respect to the u,v system. Orientability of S is not essential for the deﬁnition of A.
Exercise
Express the total area of the Mobius strip as an integral, using its parametric
representation given by Eq. (11.147).
□

Vector Integration
417
More generally, for a function f (x) deﬁned on the surface S, we can form the integral of f
over the surface:
Z Z
S
f dS =
Z Z
R
f
q
ξ2 + η2 + ζ2dudv.
(11.150)
The value of this integral is independent of the particular parametric representation used
for S and does not involve any orientation of S. It is positive for positive f .
In order to relate the integral of a second order differential form over an oriented
surface S∗to the surface integrals of functions over the unoriented surface S as deﬁned by
Eq. (11.150), we introduce the direction cosines of the positive normal to S∗
cosα =
ϵξ
p
ξ2 + η2 + ζ2 ,cosβ =
ϵη
p
ξ2 + η2 + ζ2 ,cosγ =
ϵζ
p
ξ2 + η2 + ζ2 ,
where ξ,η,ζ are as deﬁned in Eq. (11.149), ϵ = ±1 and Ω(S∗) = ϵΩ(xu,xv) (see
subsection 1.16.1). We can write ω in the form
ω = Kdudv
where
K =
ω
dudv = a d(y,z)
d(u,v) + b d(z,x)
d(u,v) + c d(x,y)
d(u,v)
(11.151)
so that
Z Z
S∗ω =
Z Z
R∗Kdudv
=
Z Z
R∗
 
a d(y,z)
d(u,v) + b d(z,x)
d(u,v) + c d(x,y)
d(u,v)
!
dudv.
(11.152)
Exercise
Show that the value of this integral of ω over the oriented surface S∗is
independent of the particular parametric representation for S∗.
□
From Eqs (11.151) and (11.152) we can write
K =
ω
dudv = ϵ(acosα + bcosβ + ccosγ)
q
ξ2 + η2 + ζ2.
By Eq. (11.152)
Z Z
S∗ω =
Z Z
R∗Kdudv = ϵ
Z Z
R
Kdudv.

418
An Introduction to Vectors, Vector Operators and Vector Analysis
Therefore, Eq. (11.150) yields the identity
Z Z
S∗ω =
Z Z
S∗adydz + bdzdx + cdxdy
=
Z Z
S
(acosα + bcosβ + ccosγ)dS
=
Z Z
R
(acosα + bcosβ + ccosγ)
q
ξ2 + η2 + ζ2dudv,
(11.153)
which expresses the integral of the differential form ω over the oriented surface S∗as an
integral over the unoriented surface S or over the unoriented region R in the parameter
plane. Note, however, that here the integrand depends on the orientation of S∗, since it
involves the direction cosines of the normal ˆn to S∗pointing to its positive side. If the
oriented surface S∗comprises many parts S∗
i each having a parametric representation x =
x(u,v) we apply identity Eq. (11.153) to each part and add over different parts to get the
same identity for the integral of ω over the whole surface S∗.
The direction cosines of the normal ˆn pointing to the positive side of S∗can be identiﬁed
with the derivatives of x,y,z in the direction of ˆn, so that4
Z Z
S∗ω =
Z Z
S
 
adx
dn + bdy
dn + c dz
dn
!
dS
(11.154)
or, in vector notation
Z Z
S∗ω =
Z Z
S
v · ˆndS,
(11.155)
where ˆn ≡(cosα,cosβ,cosγ) is the unit normal vector on the positive side of S∗and v(x)
is the vector ﬁeld with components (a(x),b(x),c(x)).
The concept of a surface integral can be interpreted in terms of the 3-D ﬂow of an
incompressible ﬂuid of unit density. Let the vector ﬁeld v(x) be the velocity ﬁeld of this
ﬂow. Then at each point of the surface S∗the product v · ˆn gives the component of the
velocity of the ﬂow in the direction of the normal ˆn to the surface. The expression v · ˆndS
can then be identiﬁed with the amount of ﬂuid that ﬂows across the element of surface dS
from the negative side of S∗to the positive side in unit time. Note that this quantity may be
negative. The surface integral in Eq. (11.155) therefore represents the total amount of ﬂuid
ﬂowing across the surface S∗from the negative to the positive side in unit time. Note the
fundamental part played by the orientation (distinction between the positive and negative
sides) of S∗in the description of the motion of the ﬂuid.
We may also consider the ﬁeld deﬁned by the integrand of Eq. (11.155) as the ﬁeld of
force F(x). The direction of the vector F then gives the direction of the lines of force and its
4We have, dx
dn = ˆn · ∇x = [cosα cosβ cosγ][1 0 0]T = cosα etc.

Vector Integration
419
magnitude gives the magnitude of the force. The integral in Eq. (11.155) is then interpreted
as the total ﬂux of force across the surface from the negative to the positive side.
11.8.1
Divergence of a vector ﬁeld and the surface integral
We wish to express the divergence of a vector ﬁeld f(x) at a point P in terms of a surface
integral, that is,
∇· f
P = lim
S→P
1
V
Z
S
f · ˆnds,
(11.156)
where S is a closed surface enclosing volume V . The point P is interior to or on the surface
S. The limit S →P means every point on S approaches P . If this limit exists, the integral
in Eq. (11.156) is independent of S and deﬁnes the divergence of f at P . We show that the
limit exists if f can be expanded in Taylor series in the neighborhood of P .
We construct a Cartesian coordinate system (ξ,η,ζ) with its origin at P . As in
subsection 11.1.1 we expand f(x) with x on the surface S in Taylor series around the
origin 0 at P . We have,
f(x) = f(0) + x · ∇f(0) + R,
where R is of the order of |x|2 and all the derivatives are evaluated at the origin, that is, at
point P . Therefore, integrating over the surface S we get
Z
S
f(x) · ds = f(0) ·
Z
S
ds +
Z
S
(x · ∇)f(x) · ds +
Z
S
R · ds.
We ﬁrst resolve the vector ds along the basis (ˆi,ˆj, ˆk), (see Fig. 11.36)
ds = ˆidsξ + ˆjdsη + ˆkdsζ
(11.157)
Fig. 11.36
Illustrating Eq. (11.157)

420
An Introduction to Vectors, Vector Operators and Vector Analysis
where the components of ds are the projections of ds on yz,zx and xy planes respectively.
As in subsection 11.1.1 we express (x · ∇)f(x) · ds in terms of the derivatives with respect
to (ξ,η,ζ) to get
Z
S
f · ds = f(0) ·
Z
S
ds + ∂fξ
∂ξ
Z
Sξ
ξdsξ + ∂fξ
∂η
Z
Sξ
ηdsξ + ∂fξ
∂ζ
Z
Sξ
ζdsξ
∂fη
∂ξ
Z
Sη
ξdsη +
∂fη
∂η
Z
Sη
ηdsη +
∂fη
∂ζ
Z
Sη
ζdsη
∂fζ
∂ξ
Z
Sζ
ξdsζ + ∂fζ
∂η
Z
Sζ
ηdsζ + ∂fζ
∂ζ
Z
Sζ
ζdsζ +
Z
S
R · ds,
where (Sξ,Sη,Sζ) are the projections of S on the coordinate planes and the last integral
goes as |x|4. We shall show later in an exercise that
Z
S
ds = 0.
Further,
Z
Sξ
ξdsξ = V ,
since
R
Sξ ξdsξ gives the volume under the upper part minus that under the lower part (see
subsection 11.4.2). Similarly,
Z
Sη
ηdsη = V =
Z
Sζ
ζdsζ.
Moreover, the integrals of the form
R
Sξ ηdsξ vanish. Everything put together we get
Z
S
f · ds =
 ∂fξ
∂ξ +
∂fη
∂η + ∂fζ
∂ζ
!
V + O(|x|4).
Divide both sides by V , (so that the last term is O(|x|) and goes to zero as |x| →0), and
take the limit as |x| →0 to get
lim
S→P
1
V
Z
S
f · ds =
 ∂fξ
∂ξ +
∂fη
∂η + ∂fζ
∂ζ
!
= ∇· f.
Thus, the limit exists and does not depend on S. It depends only on the derivatives of f at
point P .

Vector Integration
421
Exercise
Let S be a closed surface and let P be an interior point of S or a point on S. For
a scalar ﬁeld f and a vector ﬁeld F show that
∇f = lim
S→P
1
V
Z
S
f ds
and
∇× F = lim
S→P
1
V
Z
S
ds × F.
□
11.9
Diveregence Theorem in Three-dimensions
This is the extension of the Gauss’s theorem in two dimensions we proved before. There an
integral over a plane region is reduced to a line integral taken around the boundary of the
region. In its 3-D version, we consider a closed bounded region R in space bounded by
a surface S. To start with we assume that S is intersected by every straight line parallel to
x,y,z axes only at two points, or does not intersect at all. We will remove this assumption
later. Let the functions a(x),b(x),c(x) be C1 in R. Consider the integral
Z Z Z
R
∂c
∂zdxdydz
over the region R, oriented positively with respect to x,y,z coordinate system. Due to the
assumption made regarding the mesh of straight lines parallel to the axes and the region R,
such a region R can be described by the inequalities
z0(x,y) ≤z ≤z1(x,y)
where (x,y) varies over the projection B of R on the x,y plane. We assume that B has an
area and that the functions z0(x,y) and z1(x,y) are C1 in B. We can express the volume
integral over R as the succession of integrals
Z Z Z
R
f dxdydz =
Z Z
B
dxdy
Z z1
z0
f dz.
Here, f = ∂c/∂z so that the integral over z can be carried out, giving
Z z1
z0
∂c
∂z = c(x,y,z1) −c(x,y,z0) = c1 −c0,
so that,
Z Z Z
R
∂c
∂zdxdydz =
Z Z
B
c1dxdy −
Z Z
B
c0dxdy.

422
An Introduction to Vectors, Vector Operators and Vector Analysis
If we assume that the boundary surface S is positively oriented with respect to the region
R, then the part of the oriented boundary surface S∗comprising points of entry
z = z0(x,y) has a negative orientation with respect to x,y coordinates when projected on
the x,y plane. On the other hand the part z = z1(x,y) consisting of points of exit has a
positive orientation. To understand this, note that the triplets (ˆe1, ˆe2, ˆn), one with ˆn at the
entry point has negative orientation and the one with ˆn at the exit point has positive
orientation (see the summary in section 11.7). Hence, the last two integrals combine to
form the integral
Z Z
S∗c(x,y,z)dxdy
taken over the whole surface S∗. We thus have,
Z Z Z
R
∂c
∂zdxdydz =
Z Z
S∗c(x,y,z)dxdy.
If a part S′∗of S∗is a cylinder perpendicular to the x,y plane, the normal deﬁning its
orientation lies parallel to the x,y plane and has no contribution to the integral on the
right.
Exercise
Prove this statement.
Hint
Take the parametric representation x = u, y = φ(u), z = v for S′∗and then
evaluate
R R
S∗c(x,y,z)dxdy after transforming it to the integral over u,v.
□
We get the corresponding equations for the components a(x) and b(x) of the vector ﬁeld
with components a,b,c. Adding all the three equations we get the desired result:
Z Z Z
R
"∂a(x)
∂x
+ ∂b(x)
∂y
+ ∂c(x)
∂z
#
dxdydz =
Z Z
S∗[a(x)dydz+b(x)dzdx+c(x)dxdy].
(11.158)
which is known as Gauss’s theorem, or divergence theorem. Using Eq. (11.153) we can write
this in the form
Z Z Z
R
[ax + by + cz]dxdydz =
Z Z
S
(acosα + bcosβ + ccosγ)dS
=
Z Z
S
 
adx
dn + bdy
dn + c dz
dn
!
dS,
(11.159)
where, α,β,γ are the angles made by the outward normal ˆn with the positive coordinate
axes, corresponding to the positive orientation of S∗with respect to R.

Vector Integration
423
We can lift the restriction stated at the beginning, that is the region R can be covered
by a mesh of straight lines with each line intersecting the boundary surface exactly at two
points, if the region R can be divided onto subregions separately satisfying this restriction
and each subregion is bounded by an orientable surface. Then Gauss’s theorem separately
holds for each subregion. Upon adding, on the left we get a triple integral over the whole
region R and on the right some of the surface integrals combine to form the integral over
the oriented surface S, while the others making extra surfaces required to cover each
subregion cancel one another. Assuming that we get the same integral independent of the
way we divide the region R into subregions, this procedure generalizes Gauss’s theorem to
more general regions in space.
Exercise
Use Gauss’s theorem to get the volume of a region R bounded by the surface S∗
oriented positively with respect to R.
Answer
V =
Z Z Z
R
dxdydz =
Z Z
S∗xdydz =
Z Z
S∗zdxdy =
Z Z
S∗ydzdx.
Hint
To get the ﬁrst equality, for example, put a = 0, b = 0, c = z in Eq. (11.158).
□
To get the vector form of the divergence theorem, let v be the vector ﬁeld with component
functions a(x),b(x),c(x). Then, the integrand on the left of Eq. (11.159) is simply the
divergence of this ﬁeld and the integrand on the right is its component along the outward
normal, so that
Z Z Z
R
∇· vdV =
Z Z
S∗v · ˆndS,
(11.160)
where dV = dxdydz is the differential volume.
Exercise
Show that
Z
V
f (∇· A)dτ = −
Z
V
A · (∇f )dτ +
Z
S
f A · da
(11.161)
where f and A are scalar and vector valued functions respectively, da = daˆn is the vector
differential area and the surface S encloses volume V .
Hint
Use ∇· (f A) = f (∇· A) + A · (∇f ) and the divergence theorem.
□
11.10
Applications of the Gauss’s Theorem
(a) Application to ﬂuid ﬂow
We generalize to three dimensions the results about two dimensional ﬂow of a ﬂuid we
obtained before. We deal with two ﬁelds, the velocity ﬁeld v(x,t) and the momentum
vector (per unit volume) ﬁeld A(x,t) = ρ(x,t)v(x,t). If R is a ﬁxed region in space

424
An Introduction to Vectors, Vector Operators and Vector Analysis
bounded by the surface S then the total mass of ﬂuid that ﬂows across a small area ∆S of
S from interior to exterior of R in unit time is approximately ρv · ˆn∆S where v · ˆn is the
component of the velocity v in the direction of the outward normal ˆn at a point on the
surface element deﬁned by ∆S.
Thus, the total amount of ﬂuid ﬂowing across the
boundary S of R from inside to outside in unit time is given by the integral
Z Z
S
ρv · ˆndS
over the whole boundary S. By Gauss’s theorem, the amount of ﬂuid leaving R in unit time
through the boundary is
Z Z Z
R
∇· (ρv)dxdydz.
The total mass of ﬂuid contained in R at any instant of time is given by
Z Z Z
R
ρ(x,t)dxdydz
and the decease in unit time in the mass content of R is
−d
dt
Z Z Z
R
ρ(x,t)dxdydz = −
Z Z Z
R
∂ρ
∂t dxdydz.
By the law of conservation of mass, in the absence of sources or sinks of mass in R, the
amount of mass of ﬂuid leaving R through surface S must be exactly equal to the loss of
mass of ﬂuid contained in R. We must then have,
Z Z Z
R
∇· (ρv)dxdydz = −
Z Z Z
R
∂ρ
∂t dxdydz
at any time t for any region R. Dividing both sides of this identity by the volume of R and
taking the limit as the size of R goes to zero, (as we did in the 2-D case), we get the three
dimensional continuity equation:
∇· (ρv) + ∂ρ
∂t = 0,
or,
∂ρ
∂t + ∂(ρu)
∂x
+ ∂(ρv)
∂y
+ ∂(ρw)
∂z
= 0
where u(x),v(x),w(x) are the components of v(x). The continuity equation expresses the
law of conservation of mass for the motion of ﬂuids.

Vector Integration
425
If the law of conservation of mass is not invoked, the expression
∇· (ρv) + ∂ρ
∂t
measures the amount of mass created (or annihilated if negative) in unit time per unit
volume.
Of particular interest is the case of a homogeneous and incompressible ﬂuid, for which
the density is constant both in space and time. For such a constant ρ, we deduce from the
continuity equation that,
∇· v = ∂(ρu)
∂x
+ ∂(ρv)
∂y
+ ∂(ρw)
∂z
= 0
if mass is to be preserved. From Gauss’s theorem it then follows that
Z Z
S
ρv · ˆndS = 0
whenever surface S bounds a region R. Consider, in particular, two surfaces S1 and S2
bounded by the same oriented curve C∗in space, and together forming the boundary S of
a three dimensional region R. We ﬁnd that
0 =
Z Z
S
ρv · ˆndS =
Z Z
S1
ρv · ˆndS +
Z Z
S2
ρv · ˆndS
where, on both S1 and S2, ˆn denotes the normal pointing away from R. We can make both
S1 and S2 into oriented surfaces S∗
1 and S∗
2 in such a way that the orientation of C∗is positive
with respect to both S∗
1 and S∗
2. On both these surfaces, let ˆn∗be the unit normal pointing
to the positive side. For a right handed orientation of space, this implies that ˆn∗points to
that side of the surface from which the orientation of C∗appears counterclockwise. Then,
necessarily, ˆn∗= ˆn on one of the surfaces S1, S2 and ˆn∗= −ˆn on the other. It follows from
the last equation that
Z Z
S1
ρv · ˆn∗dS =
Z Z
S2
ρv · ˆn∗dS.
In words, if the ﬂuid is incompressible and homogeneous and mass is conserved, then the
same amount of ﬂuid ﬂows across any two surfaces with the same boundary curve C∗that
together bound a three dimensional region in space. This amount of ﬂuid does not
depend on the precise form of the surfaces, it is plausible that it must be determined by the
boundary curve C∗alone. We will answer this question in the next subsection by means of
stokes theorem.

426
An Introduction to Vectors, Vector Operators and Vector Analysis
Application to surface forces and space (body) forces
The forces acting in the continuous media are classiﬁed as space or body forces (e.g.,
gravitational force, electrostatic force) or as surface forces (pressures, tractions). This is
not a fundamental distinction and the effect of a force can be expressed in both these
forms. The connection between these points of view is given by Gauss’s theorem.
The continuous medium we consider is a ﬂuid of density ρ(x), in which there is a
pressure p(x) which in general depends on the position (x) in the ﬂuid. This means that
the force acting on a portion R of the ﬂuid exerted by the remaining part of the ﬂuid can
be considered as a force acting on each point of the surface S of R in the direction of the
inward drawn normal and of magnitude p per unit surface area. Denoting by dx/dn,
dy/dn, dz/dn the direction cosines of the outward normal at a point of a surface S of R,
the components of the force per unit area are given by
−pdx
dn, −pdy
dn, −p dz
dn.
Thus, the resultant of the surface forces acting on R is a force with components
Fx = −
Z Z
S
pdx
dndS, Fy = −
Z Z
S
pdy
dndS, Fz = −
Z Z
S
p dz
dndS.
By Gauss’s theorem (Eq. (11.159)), we can write these components as volume integrals
Fx = −
Z Z Z
R
pxdxdydz, Fy = −
Z Z Z
R
pydxdydz, Fz = −
Z Z Z
R
pzdxdydz.
The resultant force F (a vector) is given by
F = −
Z Z Z
R
∇pdxdydz.
(11.162)
We can express this by saying that the forces in a ﬂuid due to a pressure p(x) may, on the
one hand, be regarded as surface forces (pressure) that act with density p(x) perpendicular
to each surface element through the point (x) and on the other hand, as space forces, that
is, the forces that act on every element of volume with volume density −∇p.
Consider a ﬂuid in equilibrium under the joint action of forces due to pressure and
gravity. Then, the force F due to pressure must balance the total attractive force G on the
ﬂuid contained in R:
F + G = 0.
If the gravitational force acting on a unit mass at the point x is given by the vector g(x), we
have,
G =
Z Z Z
R
g(x)ρ(x)dxdydz.

Vector Integration
427
From equation F + G = 0, valid for any portion R of the ﬂuid, we conclude, as we did
previously while deriving continuity equations, that the corresponding relation holds for
the integrands, that is, that at each point of the ﬂuid the equation
−∇p + ρg = 0
(11.163)
applies. Since the gradient of a scalar φ is perpendicular to the level surfaces of the scalar
(given by φ = constant), we conclude that for a ﬂuid in equilibrium under pressure and
gravity, the gravitational force at each point of a surface of constant pressure p (isobaric
surface) is perpendicular to the surface. If we costumerily assume that the force of
gravitational force per unit mass near the surface of the earth is given by g = (0,0,−g)
where g is the (constant) gravitational acceleration, we ﬁnd from Eq. (11.163) that
px = 0, py = 0, pz = −gρ.
(11.164)
In particular, for a homogeneous liquid of constant density ρ bounded by a free surface of
pressure zero, Eq. (11.164) tells us that along this free surface,
0 = dp = pxdx + pydy + pzdz = −gρdz,
implying dz = 0 or that a free surface of a liquid has to be a plane z = constant = z0. For
any point x in the liquid, by Eq. (11.164), the value of the pressure is
p(x,y,z) = −
Z z0
z
pz(x,y,ζ)dζ = gρ(z0 −z).
Therefore, at the depth z0 −z = h the pressure has the value gρh. For a solid partly or
wholly immersed in the liquid, let R denote the portion of the solid lying below the free
surface z = z0. We ﬁnd from Eqs (11.162) and (11.164) that the resultant of the pressure
forces acting on the solid equals the buoyancy force with components
Fx = 0, Fy = 0, Fz =
Z Z Z
R
gρdxdydz.
This force is directed vertically upward and its magnitude equals the weight of the displaced
liquid (Archimedes’ principle).
11.10.1
Exercises on the divergence theorem
In what follows we denote a position vector by x and its magnitude |x| by r. F(x) and
f (x) denote a vector and a scalar ﬁeld respectively. We assume that both the ﬁelds have
continuous derivatives of any required order, at all points in their domains. Γ denotes a
simple closed curve and S is either a surface with Γ as its boundary or a closed surface
enclosing the interior with volume V . ˆn is the outward normal to S.

428
An Introduction to Vectors, Vector Operators and Vector Analysis
(1) Show that
R
S ds = 0 over a closed surface.
Solution
Let a be an arbitrary constant vector. Then, by the divergence theorem,
a ·
Z
S
ds =
Z
S
a · ds =
Z
V
∇· adτ.
Since a is constant, ∇· a = 0, so that a ·
R
S ds = 0. Since a is arbitrary, it follows that
R
S ds = 0.
□
(2) Show that the volume enclosed by a closed surface is
V = 1
6
Z
S
∇(r2) · ds,
where r = |x| and x is the position vector of a point of ds.
Solution
1
6
Z
S
∇(r2) · ds = 1
6
Z
S
∇(x · x) · ds = 1
6
Z
S
2((x · ∇)x)) · ds
= 1
3
Z
S
x · ds = 1
3
Z
V
∇· xdτ =
Z
V
dτ = V .
where we have used (x · ∇)x = x, ∇· x = 3 and the divergence theorem.
□
(3) Show that
Z
S
f ˆnds =
Z
V
∇f dτ.
(11.165)
Solution
Let a be an arbitrary constant vector. We apply the divergence theorem to
the vector f a. We get, since f is a scalar,
a ·
Z
S
f ˆnds =
Z
S
ˆn · f ads =
Z
V
∇· f adτ.
Further,
∇· f a = f ∇· a + a · ∇f .
The ﬁrst term on RHS is zero as a is a constant vector. Therefore, integrating we get
Z
V
∇· f adτ = a ·
Z
V
∇f dτ.

Vector Integration
429
Thus,
a ·
"Z
S
f ˆnds −
Z
V
∇f dτ
#
= 0.
Since a is arbitrary, the second factor in the dot product must vanish, proving
Eq. (11.165).
□
(4) Show that
Z
S
ˆn × Fds =
Z
V
(∇× F)dτ.
(11.166)
Solution
Let a be an arbitrary constant vector and apply the divergence theorem to
the vector F × a. We get,
Z
S
ˆn · F × ads =
Z
V
∇· (F × a)dτ,
or,
Z
S
a · ˆn × Fds =
Z
V
(a · ∇× F −F · ∇× a)dτ =
Z
V
a · ∇× Fdτ.
Taking a· out of these integrals and collecting all the terms on one side we get the
result.
□
(5) Show that
Z
S
f F · ds =
Z
V
(f ∇· F + F · ∇f )dτ.
□
11.11
Integration by Parts and Green’s Theorem in
Three-dimensions
Here, we obtain the the generalization of the corresponding result in two dimensions.
Applying Gauss’s theorem (Eq. (11.159)) to the products of functions au,bv,cw leads to a
prescription for integration by parts:
Z Z Z
R
(aux + bvy + cwz)dxdydz =
Z Z
S
 
au dx
dn + bv dy
dn + cw dz
dn
!
dS
−
Z Z Z
R
(axu + byv + czw)dxdydz.
(11.167)

430
An Introduction to Vectors, Vector Operators and Vector Analysis
If u = v = w = U and if a,b,c are of the form a = Vx,b = Vy,c = Vz where U(x) and
V (x) are scalar valued functions, we obtain Green’s ﬁrst theorem
Z Z Z
R
∇U · ∇V dxdydz =
Z Z
S
U dV
dn dS −
Z Z Z
R
U∆V dxdydz,
(11.168)
where ∆is the Laplace operator deﬁned by
∆V = ∇2V = Vxx + Vyy + Vzz
and dV /dn is the derivative of V in the direction of the outward normal:
dV
dn = Vx
dx
dn + Vy
dy
dn + Vz
dz
dn.
Interchanging U and V in Eq. (11.168) and subtracting the resulting equation from it, we
get Green’s second theorem
Z Z Z
R
(U∆V −V ∆U)dxdydz =
Z Z
S
 
U dV
dn −V dU
dn
!
dS.
(11.169)
11.11.1
Transformation of ∆U to spherical coordinates
We can use Green’s theorem to express ∆U in terms of spherical polar coordinates. We set
V = 1 in Green’s theorem, (Eq. (11.169)), to get
Z Z Z
R
∆Udxdydz =
Z Z
S
dU
dn dS =
Z Z
S
∇U · ˆndS.
(11.170)
The spherical polar coordinate system is deﬁned by
x = r sinθ cosφ, y = r sinθ sinφ, z = r cosθ.
We apply Eq. (11.170) to a wedge shaped region ˜R described by inequalities of the form
r1 < r < r2, θ1 < θ < θ2, φ1 < φ < φ2.
The boundary S of ˜R consists of six faces along each of which one of the coordinates r,θ,φ
has constant value. Applying the formula for transformation of triple integrals we write the
left side of Eq. (11.170) as
Z Z Z
R
∆Udxdydz =
Z Z Z
˜R
∆U d(x,y,z)
d(r,θ,φ drdθdφ
=
Z Z Z
˜R
∆Ur2 sinθdrdθdφ.
(11.171)

Vector Integration
431
In order to transform the surface integral in Eq. (11.170) we introduce the position vector
x ≡(x,y,z) = (r sinθ cosφ,r sinθ sinφ,r cosθ)
and ﬁnd that its ﬁrst derivatives satisfy the relations
xr · xθ = 0, xθ · xφ = 0, xφ · xr = 0
xr · xr = 1, xθ · xθ = r2, xφ · xφ = r2 sin2 θ.
(11.172)
Thus, at each point the vector xr is normal to the coordinate surface r = constant passing
through that point, the vector xθ normal to the surface θ = constant and the vector xφ
normal to the surface φ = constant. (In other words, the unit vectors in the direction
of these vectors form the ˆr, ˆθ, ˆφ basis at that point). More precisely, on one of the faces
r = constant = rk,k = 1,2 of the region ˜R deﬁned above, the outward normal unit
vector ˆn is given by (−1)kxr. Hence, on these faces
∇U · ˆn = (−1)k∇U · xr = (−1)k ∂U
∂r .
Using θ,φ as parameters on the face r = rk, we get, for the element of area (see
section 10.12)
dS =
√
EG −F2dθdφ =
q
(xθ · xθ)(xφ · xφ) −(xθ · xφ)dθdφ = r2 sinθdθdφ.
Thus, the contribution of the two faces r = r1 and r = r2 to the integral of dU/dn over S
is represented by the expression
Z Z
r=r2
r2 sinθ∂U
∂r dθdφ −
Z Z
r=r1
r2 sinθ∂U
∂r dθdφ,
where integration is over the rectangle
θ1 < θ < θ2, φ1 < φ < φ2.
We can write this difference of integrals as the triple integral
Z Z Z
˜R
∂
∂r
 
r2 sinθ∂U
∂r
!
drdθdφ.
Similarly, we ﬁnd that on a face θ = constant = θk,k = 1,2
ˆn = (−1)k 1
r xθ, dS = r sinθdφdr, dU
dn = (−1)k
r
∂U
∂θ

432
An Introduction to Vectors, Vector Operators and Vector Analysis
and on a face
φ = constant = φk
ˆn = (−1)k
1
r sinθxφ, dS = rdrdθ, dU
dn = (−1)k
r sinθ
∂U
∂φ.
Combining the contributions of the opposite faces θ = constant or φ = constant, as we
did for r = constant, we ﬁnd the total surface integral to be
Z Z
S
dU
dn dS =
Z Z Z
˜R
" ∂
∂r
 
r2 sinθ∂U
∂r
!
+ ∂
∂θ
 
sinθ∂U
∂θ
!
+ ∂
∂φ
 
1
sinθ
∂U
∂φ
!#
drdθdφ.
Comparing with Eq. (11.171) dividing with the volume of the wedge ˜R and taking the limit
as this volume tends to zero, we can equate the corresponding integrands to get the desired
expression for the Laplace operator in the spherical coordinates:
∆U =
1
r2 sinθ
" ∂
∂r
 
r2 sinθ∂U
∂r
!
+ ∂
∂θ
 
sinθ∂U
∂θ
!
+ ∂
∂φ
 
1
sinθ
∂U
∂φ
!#
.
(11.173)
11.12
Helmoltz Theorem
In this subsection, we make use of the Dirac delta function, so we assume that you have
read the appendix on the Dirac delta function. In this subsection we use r,r′ to denote
position vectors and deﬁne r = |r| and r′ = |r′|. Now consider a vector ﬁeld f(r) satisfying
the relations
∇· f(r) = d(r),
∇× f(r) = c(r).
(11.174)
Since the divergence of curl is always zero, the second of the above equations gives
∇· c = 0.
(11.175)
The question we are interested in is this: knowing the functions d(r) and c(r), can we use
Eqs (11.174) and (11.175) to uniquely specify the ﬁeld f(r)? The answer is yes, provided
d(r) and c(r) tend to zero faster than 1/r2 as r →∞. It turns out that
f = −∇u + ∇× w,
(11.176)
where
u(r) = 1
4π
Z d(r′)
γ
dτ′,
(11.177)

Vector Integration
433
and
w(r) = 1
4π
Z
c(r′)
γ dτ′,
(11.178)
where the integrals are over all space, dτ′ is the differential volume element and
γ = |r −r′|. If f is given by Eq. (11.176), then its divergence is given by, (since divergence
of curl is zero), (see Appendix),
∇· f = −∇2u = −1
4π
Z
d∇2
 1
γ
!
dτ′ =
Z
d(r′)δ3(r −r′)dτ′ = d(r).
Regarding the curl of the ﬁeld, we have, since the curl of a gradient is zero,
∇× f = ∇× (∇× w) = −∇2w + ∇(∇· w).
(11.179)
where we have used Eq. (10.92). The last term yields
−∇2w = −1
4π
Z
c∇2
 1
γ
!
dτ′ =
Z
c(r′)δ3(r −r′)dτ′ = c(r).
Thus, we need to show that the ∇(∇·w) vanishes. Using integration by parts, Eq. (11.161),
and noting that the derivatives of γ with respect to primed coordinates differ by a sign from
those with respect to unprimed coordinates, we get
4π∇· w =
Z
c · ∇
 1
γ
!
dτ′ = −
Z
c · ∇′
 1
γ
!
dτ′
=
Z 1
γ ∇′ · cdτ −
Z 1
γ c · da.
(11.180)
However, the divergence of c is zero, by Eq. (11.175) and the surface integral vanishes as
γ →∞as long as c(r) goes to zero sufﬁciently rapidly. The rate of divergence of d(r)
and c(r) as r →∞is important for the convergence of the integrals in Eqs (11.177) and
(11.178). In the large r′ limit, where γ ≈r′, the integrals are of the form
Z ∞X(r′)
r′
r′2dr′ =
Z ∞
r′X(r′)dr′,
where X stands for d or c as the case may be. If X ∼1/r′ the integrand is constant so that
the integral blows up or if X ∼1/r′2 the integral is a logarithm and blows up. Evidently,
the divergence and the curl of f must vanish more rapidly than 1/r′2 as r′ →∞for the
above proof to hold.

434
An Introduction to Vectors, Vector Operators and Vector Analysis
Assuming that the required conditions on d(r) and c(r) are satisﬁed, is the solution
(11.176) unique? Not in general, because we can add to f any vector function with
vanishing divergence and curl to get the same solution. However, it turns out that there is
no function with vanishing divergence and curl everywhere and goes to zero at inﬁnity.
So, if we include the requirement that f(r) →0 as r →∞then solution (11.176) is
unique. For example, generally we do expect the electromagnetic ﬁelds to go to zero far
away from the charge and current distributions which produce them.
We can thus state the all-important Helmoltz theorem rigorously as follows.
If the divergence d(r) and the curl c(r) of a vector ﬁeld f(r) are speciﬁed and if they
both go to zero faster than 1/r2 as r →∞, and if f(r) goes to zero as r →∞, then f is
given uniquely by Eq. (11.176).
From Helmoltz theorem it follows that a vector ﬁeld with vanishing curl is derivable
from a scalar potential, while a ﬁeld with vanishing divergence can be expressed as the
curl of some other vector ﬁeld. For example, in electrostatics, ∇· E = ρ/ϵ0 where ρ is the
given charge distribution and ∇× E = 0, so
E(r) = −∇V (r),
where V is the scalar electrostatic potential. While in magnetostatics ∇· B = 0 and
∇× B = µ0J where J is the given current distribution, so that
B(r) = ∇× A,
where A is the vector potential.
11.13
Stokes Theorem in Three-dimensions
We generalize this all important theorem to three dimensions. In three dimensions, this
theorem connects the integral of the normal component of the curl of a vector ﬁeld over
a curved surface with the integral of the tangential component of the vector ﬁeld over the
boundary curve.
Consider an oriented surface S∗in 3-D space bounded by a closed curve C∗oriented
positively with respect to S∗. We choose a right handed coordinate system so that space is
oriented positively with respect to x,y,z-axes. Let ˆn denote the unit normal vector at each
point of S∗pointing to its positive side, that is, ˆn deﬁnes the positive orientation of S∗. Let
ˆt be the unit tangent vector to C∗pointing in the direction corresponding to the
orientation of C∗. Let v(x) ≡(a(x),b(x),c(x)) be a vector ﬁeld deﬁned in a region of
space R containing the surface S. Stokes theorem asserts that
Z Z
S
(∇× v(x)) · ˆndS =
Z
C
v · ˆtds.
(11.181)
where on the right we integrate along C, in the direction deﬁned by its orientation, dictated
by the choice of ˆt, over the arclength ds. The orientations of S and C is imposed on this
integration via the choice of the unit vectors ˆn and ˆt.

Vector Integration
435
In terms of the components of vectors ˆn and ˆt, we can write
Z Z
S
"
cy −bz
 dx
dn + (az −cx) dy
dn +

bx −ay
 dz
dn
#
dS =
Z
C
 
adx
ds + bdy
ds + cdz
ds
!
ds.
(11.182)
or, using Eq. (11.154),
Z Z
S∗(cy−bz)dydz+(az−cx)dzdx+(bx−ay)dxdy =
Z
C∗adx+bdy+cdz. (11.183)
Stokes theorem can be made plausible by using the fact that it is true for plane surfaces.
If S is a polyhedral surface composed of plane polygonal surfaces, so that the boundary
curve C is a polygon, we can apply Stokes theorem to each of the plane surfaces and add
the corresponding contributions. Then, the line integrals along all the interior edges of the
polyhedron cancel and we obtain the stokes theorem for the polyhedral surface. In order
to prove the general statement of the Stokes theorem, we only have to pass to the limit,
leading from approximate polyhedra to arbitrary surfaces S bounded by arbitrary curves C.
The regorous validation of this passage to the limit could be cumbersome so the proof is
generally carried out by transforming the whole surface S into a plane surface and proving
that the theorem is preserved under such transformations. We omit the details of this proof
and assume the theorem.
We can now settle the question asked in the discussion regarding the incompressible
and homogeneous ﬂuid in section 11.10. Since the ﬂuid is incompressible, its divergence is
everywhere zero so by Helmoltz theorem it must be given by the curl of some vector ﬁeld.
Now by applying Stokes theorem we can write
Z Z
S
v · ˆndS =
Z Z
S
(∇× A) · ˆndS =
Z
C
A · ˆtds.
Thus, the total amount of ﬂuid passing through any two surfaces with the same boundary
curve C is determined by the curve C alone.
Exercise
Show that the arguments leading to Eqs (10.105) and (10.107) can be extended
to prove the divergence theorem and Stokes theorem respectively (see Grifﬁths [9]).
Compare with our proofs of these theorems.
□
The following two exercises give two fundamental results based on the Helmoltz theorem
and the stokes theorem.
Exercise
Curl-less or irrotational ﬁelds. Let F be a vector ﬁeld. Show that the following
conditions are equivalent.
(a) ∇× F = 0 everywhere.
(b) F is the gradient of some scalar, F = −∇V (x).

436
An Introduction to Vectors, Vector Operators and Vector Analysis
(c)
R b
a F·dx is independent of path for any given end points and depend only on the end
points in a simply connected region.
(d)
H
F · dx = 0 for any closed loop.
Solution
(a) ⇒(b) : By Helmoltz theorem. (b) ⇒(c) is proved in section 11.1. (c) ⇒(d):
Take any two distinct points P1 and P2 on the closed loop. Then,
I
F · dx =
Z P2
P1
F · dx +
Z P1
P2
F · dx =
Z P2
P1
F · dx −
Z P2
P1
F · dx = 0.
(d) ⇒(a) by Stokes theorem.
□
Exercise
Divergence-less or solenoidal ﬁelds. Show that the following conditions are
equivalent.
(a) ∇· F = 0 everywhere.
(b) F = ∇× A for some vector ﬁeld A.
(c)
R R
S F · ˆndS is independent of surface, for any given boundary curve, being equal to
the integral of A along the boundary curve in the positive sense with respect to the
surface.
(d)
R R
S F · ˆndS = 0 for any closed surface.
Solution
(a) ⇒(b) : By Helmoltz theorem. By Eq. (11.177), u(r) = 0 in Eq. (11.176).
(b) ⇒(c) : By Stokes theorem the integral over the surface reduces to that over the
boundary curve.
(c) ⇒(d): View the closed surface as two surfaces with common
boundary curve. The integral in (c) reduces to the integrals over the boundary curve in
the opposite sense for the two surfaces, because the positive orientation of the boundary
curve with respect to two surfaces are opposite, so that these integrals cancel each other.
(d) ⇒(a): By divergence theorem, condition (d) means the volume integral of ∇· F over
the region R enclosed by the surface vanishes. Since this is true for any closed surface
enclosing any region R, we can divide by the volume of R and take the limit as this
volume tends to zero to yield condition (a).
□
11.13.1
Physical interpretation of Stokes theorem
This is similar to that we have seen in the two dimensional case. We interpret the vector
ﬁeld v(x) ≡(v1(x),v2(x),v3(x)) as the velocity ﬁeld of the ﬂow of a ﬂuid. We call the
integral
Z
C
v · ˆtds =
Z
C∗v · dx

Vector Integration
437
taken over an oriented closed curve C∗the circulation of the ﬂow along this curve. Stokes
theorem states that the circulation along C∗equals the integral
Z Z
S
(∇× v) · ˆndS,
where S is any orientable surface bounded by C and ˆn is the unit normal to S making it the
oriented surface S∗such that the curve C∗is oriented positively with respect to S∗. Suppose
we divide the circulation around C by the area of the surface S bounded by C and pass to
the limit by making C shrink to a point while remaining the boundary of the surface. For
the surface integral of the normal component of curl v divided by the area, this limit gives
the value (∇× v) · ˆn at the limit point. Thus, we can regard the component of curl v in the
direction of the surface normal ˆn as the circulation density of the ﬂow across the surface at
the corresponding point.
The vector curl v is called the vorticity of the ﬂuid motion. Therefore, the circulation
around a curve C equals the integral of the normal component of the vorticity over a surface
bounded by C. The motion is called irrotational if the vorticity vector vanishes at every
point occupied by the ﬂuid, that is, if the vorticity vector satisﬁes the relations
v3y −v2z = 0, v1z −v3x = 0, v2x −v1y = 0.
As a result of Stokes theorem, the circulation in an irrotational motion vanishes along any
curve C that bounds a surface contained in the region ﬁlled with the ﬂuid.
By the above exercise we know that an irrotational vector ﬁeld is also conservative.
That is,
∇× v = 0 implies v = ∇φ.
Thus, the velocity ﬁeld of an irrotational ﬂuid ﬂow in a simply connected region implies the
existance of a velocity potential φ(x) satisfying
v(x) = ∇φ(x).
If, in addition the ﬂuid is homogeneous and incompressible we have
∇· v = 0.
Thus, the velocity potential satisﬁes the Laplace equation
0 = ∇· ∇φ = ∇2φ = φxx + φyy + φzz.
11.13.2
Exercises on Stoke’s theorem
In what follows we denote a position vector by x and its magnitude |x| by r. F(x) and
f (x) denote a vector and a scalar ﬁeld respectively. We assume that both the ﬁelds have
continuous derivatives of any required order, at all points in their domains. Γ denotes a

438
An Introduction to Vectors, Vector Operators and Vector Analysis
simple closed curve and S is either a surface with Γ as its boundary or a closed surface
enclosing the interior with volume V . ˆn is the outward normal to S.
(1) Verify Stoke’s theorem for the ﬁeld
F = zˆi + xˆj + y ˆk
where Γ is the unit circle in the xy plane bounding the hemisphere z =
p
1 −x2 −y2.
Solution
Z
Γ
F(x) · dx =
Z
Γ
zdx +
Z
Γ
xdy +
Z
Γ
ydz.
On Γ z = 0 = dz so that
Z
Γ
F(x) · dx =
Z
Γ
xdy = π.
Now
∇× F = ˆi + ˆj + ˆk
so that
Z
S
(∇× F) · ds =
Z
S
ˆi · ds +
Z
S
ˆj · ds +
Z
S
ˆk · ds = π
because
Z
S
ˆi · ds = 0 =
Z
S
ˆj · ds
and
Z
S
ˆk · ds = π,
as the integrals represent the projected areas of the hemisphere on the coordinate
planes. Alternatively, we can express F and ∇× F in terms of spherical polar
coordinates and integrate over sinθdθdφ, 0 ≤θ ≤π/2, 0 ≤φ < 2π. This
establishes Stoke’s theorem for the given ﬁeld.
□
(2) Evaluate
R
Γ F · dx for F = (x2 −y2)ˆi + xyˆj and Γ is the arc of y = x3 from (0,0) to
(2,8).
Solution
First check that the given ﬁeld is not conservative, so that the integral
depends on the given curve. However, we can evaluate the given integral over the

Vector Integration
439
closed loop enclosing area A
C = C1 + C2 −Γ
as shown in Fig. 11.37, by using Stoke’s theorem,
Z
C
F · dx =
Z
S
ˆn · ∇× Fds.
Since, as a part of C, Γ is negatively oriented, (−Γ ), we have
Z
C
F · dx =
Z (2,0)
(0,0)
F · dx +
Z (2,8)
(2,0)
F · dx −
Z
Γ
F · dx.
Or,
Z
Γ
F · dx = −
Z
S
ˆn · ∇× Fds +
Z (2,0)
(0,0)
F · dx +
Z (2,8)
(2,0)
F · dx.
Or, since ˆn = ˆk,
Z
Γ
F · dx = −
Z
S
3yds +
Z 2
0
x2dx +
Z 8
0
2ydy
= −
Z 2
0
Z x3
0
3ydydx +
Z 2
0
x2dx +
Z 8
0
2ydy = 824
21 ,
where we have evaluated the ﬁrst integral as repeated single integrals.
□
Fig. 11.37
Evaluation of a line integral using Stoke’s theorem
(3) Let F(x) = 0 at every point on a surface S. Show that ∇× F is tangent to S at every
point on it.

440
An Introduction to Vectors, Vector Operators and Vector Analysis
Solution
Suppose ∇× F is not tangent to S at a point P on S. Then, by continuity,
there is some neighborhood of P on S, say S′, in which the component of ∇×F along
the normal ˆn has the same sign, at every point in S′. Applying Stoke’s theorem to S′,
Z
Γ
F · dx =
Z
S′ ˆn · ∇× Fds,
where Γ is the boundary of S′. Since F = 0 on S′,
R
Γ F · dx = 0 on S′. However,
R
S′ ˆn · ∇× Fds , 0 on S′, since the integrand is not zero by assumption and has the
same sign throughout S′. This contradicts Stoke’s theorem, so that ˆn · ∇× F = 0
or ∇× F must be perpendicular to ˆn, that is, tangent to S at P . Since point P was
arbitrary, ∇× F is tangent to S at all points on it.
□
(4) Show that
Z
Γ
f (x)dx =
Z
S
ˆn × ∇f ds,
(11.184)
where Γ is the boundary of S.
Solution
Let a be an arbitrary constant vector. Then, f (x)a is a vector. By Stoke’s
theorem,
Z
Γ
f (x)a · dx =
Z
S
ˆn · ∇× f (x)ads
=
Z
S
ˆn · (f ∇× a + ∇f × a)ds
=
Z
S
ˆn · (∇f × a)ds =
Z
S
a · (ˆn × ∇f )ds
as ∇× a = 0. This gives
a ·
"Z
Γ
f (x)dx −
Z
S
(ˆn × ∇f )ds
#
= 0.
Since a is arbitrary, the second factor in the dot product must vanish, proving
Eq. (11.184).
□
(5) Show that
Z
Γ
dx × F =
Z
S
(ˆn × ∇) × Fds
(11.185)
where Γ is the boundary of the surface S.

Vector Integration
441
Solution
Let a be an arbitrary constant vector and consider
Z
Γ
a × F · dx = a ·
Z
Γ
F × dx.
By Stoke’s theorem,
Z
Γ
a × F · dx =
Z
S
ˆn · ∇× (a × F)ds
=
Z
S
ˆn · [a(∇· F) −(a · ∇)F]ds
= a ·
Z
S
[(∇· F)ˆn −∇(F · ˆn)]ds
= −a ·
Z
S
(ˆn × ∇) × Fds.
All these steps can be proved using Levi-Civita symbols and noting that ∇does not
operate on ˆn. Last two equations give
a ·
"Z
Γ
dx × F −
Z
S
(ˆn × ∇) × Fds
#
= 0.
Since a is arbitrary, the second factor of the dot product must vanish, proving
Eq. (11.185).
□
(6) Show that

R
Γ dx × x
, where Γ is a closed curve in the xy plane, is twice the enclosed
area A.
Solution
In Eq. (11.185) we replace ˆn by ˆk which is the vector normal to xy plane
and F by x. We get,
Z
Γ
dx × x =
Z
S
(ˆk × ∇) × xds
=
Z
S
[∇(ˆk · x) −ˆk(∇· x)]ds
=
Z
S
(ˆk −3ˆk)ds = −2ˆk
Z
S
ds = −2ˆkA,

442
An Introduction to Vectors, Vector Operators and Vector Analysis
where the second equality can be proved using Levi-Civita symbols and noting that
ˆk is a constant vector. Thus,

Z
Γ
dx × x
 = | −2ˆkA| = 2A.
□
(7) For two scalar ﬁelds f (x) and g(x) show that
Z
Γ
f (x)∇g(x) · dx +
Z
Γ
g(x)∇f (x) · dx = 0.
(11.186)
where Γ is a simple closed curve.
Solution
Let F = f ∇g and S be a surface with Γ as its boundary. Applying Stoke’s
theorem we get
Z
Γ
f ∇g · dx =
Z
S
ˆn · ∇× (f ∇g)ds
=
Z
S
ˆn · (∇f × ∇g)ds +
Z
S
f ˆn · (∇× ∇g)ds
=
Z
S
ˆn · (∇f × ∇g)ds,
because ∇× ∇g = 0. Similarly,
Z
Γ
g∇f · dx = −
Z
S
ˆn · (∇f × ∇g)ds.
Last two equations prove Eq. (11.186).
□
(8) Prove that ∇· ∇× F = 0.
Solution
We will use both, the divergence theorem and the Stoke’s theorem. By
divergence theorem and referring to Fig. 11.37 we see that
Z
V
∇· ∇× Fdτ =
Z
S1
ˆn · ∇× Fds1 +
Z
S2
ˆn · ∇× Fds2.
Now applying Stoke’s theorem to each term on the RHS we get,
Z
S1
ˆn · ∇× Fds1 =
Z
Γ
F · dx

Vector Integration
443
and
Z
S2
ˆn · ∇× Fds2 = −
Z
Γ
F · dx.
The sign is reversed while transforming the second integral because the positive
directions around the boundaries of the two surfaces are opposite. Hence,
Z
V
∇· ∇× Fdτ = 0.
Since this equation holds for all volume elements it follows that ∇· ∇× F = 0. □
(9) For a closed surface S, show that
(i)
Z
S
ˆn · (∇× F)ds = 0
and
(ii)
Z
S
ˆn × ∇f ds = 0.
Hints
(i) Divide S into two parts, S1 and S2, with the common boundary curve Γ
(see Fig. 11.37) and write
Z
S
ˆn · (∇× F)ds =
Z
S1
ˆn · (∇× F)ds1 +
Z
S2
ˆn · (∇× F)ds2.
Now, apply Stoke’s theorem to both the terms on RHS keeping in mind that the
positive sense of traversing Γ as the boundary of S1 is opposite to that traversing Γ as
the boundary of S2. This makes the two terms on RHS cancel after applying Stoke’s
theorem and the result follows.
(ii) Following the hint for part (i) we can write
Z
S
ˆn × ∇f ds =
Z
S1
ˆn × ∇f ds1 +
Z
S2
ˆn × ∇f ds2.
Now use Eq. (11.184) for the two terms on RHS and then follow rest of the hint for
part (i).
□
(10) Show that (with Γ as the boundary of S)
Z
Γ
f F · dx =
Z
S
ˆn · (∇f × F + f ∇× F)ds.
□
(11) If F is continuous and F × dx = 0 for every closed curve, show that F is constant.
□

12
Odds and Ends
In this chapter we present an assorted collection of situations to demonstrate how they
can be analyzed using vectors. Here, we do not attempt a systematic development of any
particular topic. The basic idea is to illustrate how a large variety of problems can be tackled
using vectors in a coordinate free way.
12.1
Rotational Velocity of a Rigid Body
We ﬁnd the rotational velocity of an arbitrarily rotating rigid body. This section is a
continuation of our development of rotation operator and its matrix representation. In
particular, we freely use the symbols introduced there, without re-deﬁning them here.
Since the rotational velocity of a rigid body is common to all points of the body, it is
enough to ﬁnd the rotational velocity of the position vector x(t) of a point in the body. Let
x0 denote the value of x(t) at t = 0. We seek a relation of the form
˙x = [ΩS]x0 = Ωx.
(12.1)
and solve for Ω. Note that the time dependence must reside in the operator ΩS. Obviously,
Ω= ˙SST = ˙SS−1
(12.2)
solves Eq. (12.1) giving ˙x = ˙Sx0 where ˙S is the time derivative of the operator S(t). Note
that we are using the same symbol for the operator and its matrix. This is justiﬁed because
they are isomorphic. Differentiating SST = I we get
˙SST + S ˙ST = 0.
(12.3)
Here, we have used the fact that the operations of transpose and differentiation commute.
Equations (12.2) and (12.3) lead to
Ω= ˙SST = −S ˙ST = −ΩT
(12.4)

Odds and Ends
445
Thus, the operator Ωis skewsymmetric. We know that every skewsymmetric operator on
a 3-D Euclidean space is the operator of vector multiplication by a ﬁxed vector. We denote
the corresponding vector for the skewsymmetric operator Ωby ω, so that
Ωx = ω × x ∀x ∈R3.
To appreciate this result, consider the matrix of Ωin some basis


0
−ω3
ω2
ω3
0
−ω1
−ω2
ω1
0


.
This gives vector ω ↔[ω1 ω2 ω3]T as an eigenvector with eigenvalue 0. By applying the
matrix Ωto a vector x ↔[x1 x2 x3]T we get the vector ω × x ↔[ω2x3 −ω3x2 ω3x1 −
ω1x3 ω1x2 −ω2x1]T .
In order to get the expression for ω we proceed as follows. Put θ ˆn × x = Ax deﬁning
operator A. Then, the equation connecting x0 at t = 0 with x(t) at a later time t, namely,
x = eθ ˆn×x0
can be written, in terms of A as
x = eAx0
(12.5)
whence S = eA. It then follows that
Ω= d
dt

eA
e−A
(12.6)
which can be expanded, giving
Ω= ˙A + 1
2![A, ˙A] + 1
3![A,[A, ˙A]] + ···
(12.7)
Here, [A,B] = AB −BA is the commutator of A and B. To prove Eq. (12.7) we deﬁne
Ω(λ) = ∂
∂t

eλA
e−λA
where λ is a parameter independent of t. We have Ω(0) = 0 and
∂Ω(λ)
∂λ
= ˙A + [A,Ω(λ)].

446
An Introduction to Vectors, Vector Operators and Vector Analysis
The higher derivatives can be evaluated iteratively e.g.,
∂2Ω(λ)
∂λ2
= [A, ∂Ω(λ)
∂λ
] = [A, ˙A] + [A,Ω(λ)],
∂3Ω(λ)
∂λ3
= [A,[A, ˙A]] + [A,Ω(λ)]
and proceeding iteratively, we get, for the nth derivative
∂nΩ(λ)
∂λn
= [A,[A,[A,[···[A,
|              {z              }
(n−1)factors
˙A]···] + [A,Ω(λ)].
Expanding Ω(λ) in Taylor series in λ about λ = 0 and using the above derivatives we get
Ω(λ) = λ ˙A + λ2
2! [A, ˙A] + λ3
3! [A,[A, ˙A]] + ···
and Eq. (12.7) follows with λ = 1.
We must now evaluate the commutator of two skewsymmetric matrices. This is also a
skewsymmetric matrix. Further, given a vector x ∈R3 and A, B skewsymmetric operators,
Ax = a×x and Bx = b×x implies [A,B]x = (a×b)×x, as you can check. Thus, Eq. (12.7)
can be written as (remember A = θ ˆn×)
Ωx = ω × x =
∞
X
m=0
[(θ×)m × ˙θ]
(m + 1)! θx =
∞
X
m=0
[θm+1(ˆn×)m × ˙ˆn]
(m + 1)!
× x + ˙θ ˆn × x
(12.8)
Here, we deﬁne (θ×)m by θ × (θ × (θ × (···
|               {z               }
m factors
and similarly for (ˆn×)m. Also, we have
˙θ = ˙θ ˆn + θ ˙ˆn. To obtain the inﬁnite sum in Eq. (12.8) we note that ˙ˆn lies in the plane
perpendicular to ˆn. Therefore, as we have seen before,
ˆn × (ˆn × ˙ˆn) = −˙ˆn
Using this equation in Eq. (12.8) and collecting the coefﬁcients of ˆn × ˙ˆn and ˆnθ(ˆnθ ˙ˆn)
we get
ω = ˙θ ˆn + (1 −cosθ)ˆn × ˙ˆn −sinθ[ ˆn × (ˆn × ˙ˆn)],
or,
ω = ˙θ ˆn + (1 −cosθ)ˆn × ˙ˆn + sinθ ˙ˆn.
(12.9)
We see that ω , ˙θ unless ˙ˆn = 0. Thus, it is more appropriate to call ω ‘rotational velocity’
rather than ‘angular velocity’ of the body.

Odds and Ends
447
Exercise
A
fan
operates
as
shown
in
Fig.
12.1.
A
horizontal
shaft
rotates
counterclockwise with constant rotational velocity ω1 about a vertical stand and the fan
blades rotate counterclockwise with constant rotational velocity ω2 about this shaft. Find
the rotational velocity of the fan blade relative to the origin O of the stationary frame as
shown in the ﬁgure.
Solution
Applying Eq. (12.9) to the ﬁrst rotation about ˆn3 we get, since ˙ˆn3 = 0,
ω1 = ˙ˆi1 = ω1 ˆn3.
(12.10)
Fig. 12.1
The rotating fan
Now apply Eq. (12.9) to get the rotational velocity of the blade,
ω = ω2ˆi1 + (1 −cosω2t)ˆi1 × ˙ˆi1 + sinω2t˙ˆi1
= ω2 cosω1t ˆn1 + ω2 sinω1t ˆn2
+ω1(1 −cosω2t)(cosω1t ˆn1 × ˆn3 + sinω1t ˆn2 × ˆn3) + ω1 sinω2t ˆn3
= ω2(cos(ω1t)ˆn1 + sin(ω1t)ˆn2)
+ω1(1 −cos(ω2t))(sin(ω1t)ˆn1 −cos(ω1t)ˆn2) + ω1 sin(ω2t)ˆn3
= (ω2 cos(ω1t) + ω1 sin(ω1t)(1 −cos(ω2t)))ˆn1
+(ω2 sin(ω1t) −ω1 cos(ω1t)(1 −cos(ω2t)))ˆn2 + ω1 sin(ω2t)ˆn3.

448
An Introduction to Vectors, Vector Operators and Vector Analysis
where we have put ˆi1 = cosω1t ˆn1 + sinω1t ˆn2, used Eq. (12.10) and the fact that
ˆn1, ˆn2, ˆn3 form a right handed system.
□
Exercise
Suppose a rigid body is rotating in space and you know its instantaneous
rotational velocity ω. This does not mean that you know the instantaneous axis of
rotation, because ω speciﬁes only its direction, which corresponds to a continuum of
parallel lines in space. Obtain the equation of the instantaneous axis of rotation in terms
of ω and the instantaneous (inertial) position and velocity vectors of a particle in the rigid
body, which is not on the instantaneous axis of rotation.
Solution
[20] In what follows we refer to Fig. 12.2 and use symbols and the quantities
speciﬁed in this ﬁgure, without deﬁning them in the text, as they are self explanatory. Thus,
you have to read the solution jointly with Fig. 12.2.
Fig. 12.2
Finding the instantaneous axis of rotation of a rigid body
Let r and v denote the position and velocity of the particle as speciﬁed in the problem. As
all of the velocity v is taken to be rotational, it follows that
ω × Rc = v.
Crossing both sides of the above equation with ω we get
ω × (ω × Rc) = ω × v.
This equation can be transformed to
ω(ω · Rc) −Rc(ω · ω) = ω × v.

Odds and Ends
449
Now ω · Rc = 0 and ω · ω = |ω|2, so that
Rc = −(ω × v)
|ω|2
.
Now
r′ = Rc + λω
|ω| = λω
|ω| −(ω × v)
|ω|2
,
where λ is a scalar parameter with the dimensions of length. Hence, we have, with R =
r −r′,
R = r −λω
|ω| + (ω × v)
|ω|2
.
As λ varies, the locus of the tip of this vector generates the line of the instantaneous axis of
rotation for a point moving with velocity v and position vector r in the rigid body. Since the
instantaneous rotational velocity ω is common to the whole rigid body, the instantaneous
axis of rotation we have found is also common to all points in the body.
12.2
3-D Harmonic Oscillator
Oscillatory motion is performed by systems close to their stable equilibrium conﬁgurations
under speciﬁed force ﬁelds. Here, we deal with a particle moving under the action of a
force ﬁeld f(x). We are concerned with an equilibrium point for the ﬁeld, that is, the
point x0 such that f(x0) = 0. Thus, the particle does not experience any force at x0. We
assume that x0 is an isolated equilibrium point, that is, it has a neighborhood devoid of any
equilibrium point other than itself. Shifting the origin to x0, the condition for equilibrium
can be stated as
f(0) = 0.
An equilibrium point is said to be stable, if it has a neighborhood at every point of which
the particle is accelerated towards the interior of the neighborhood. A famous theorem due
to Lagrange states that an equilibrium point is stable if and only if it is a point of local
minimum for the potential corresponding to the ﬁeld.
Thus, we are interested in the bounded motion of a system near a stable equilibrium
point. To get this motion, we must ﬁrst get an approximate form of the force ﬁeld in a
neighborhood of the equilibrium point. This can be achieved if the ﬁeld f(x) has a Taylor
expansion in some neighborhood of the stable equilibrium point which we have taken to
be the origin. We can then write
f(x) = f(0) + r · ∇f(0) + 1
2(r · ∇)2f(0) + ··· .

450
An Introduction to Vectors, Vector Operators and Vector Analysis
The ﬁrst term on RHS vanishes because origin is an equilibrium point, while the second
term is linear in r, that is,
(α1r1 + α2r2) · ∇f(0) = α1r1 · ∇f(0) + α2r2 · ∇f(0).
Third and further terms are of higher order of smallness and can be neglected. Stability of
the equilibrium point is ensured if we impose r · ¨r < 0, or, equivalently, we use the
following stability condition,
r · ∇f(0) ≤0,
with equality only when r = 0.
If we keep only the second term in the Taylor expansion of the force and neglect all
further higher order terms, the corresponding equation of motion is the following second
order linear differential equation:
m¨r = r · ∇f(0).
Since this is a second order equation, it has two linearly independent solutions (that is,
they are not proportional to each other) say r1 = r1(t) and r2 = r2(t). Since it is linear,
any linear combination of these two linearly independent solutions say rn = αnr1(t)+
βnr2(t) is also a solution. This system, governed by a linear force obeying the stability
condition, is called a harmonic oscillator. This superposition principle makes the analysis
of harmonic oscillator manageable. As we will see below, if the force satisﬁes one
additional requirement of being isotropic, the harmonic oscillator equation can be
integrated to get exact solutions. On the other hand, if we add the third term in the Taylor
series to the equation of motion, the resulting differential equation ceases to be linear. The
mathematical analysis of this so called anharmonic oscillator becomes very difﬁcult and is
generally analysed using perturbation techniques, where the anharmonic term is treated
as a small perturbation to the harmonic one.
Let us now specialize to the case where the force f is not only linear, but also isotropic
or central, that is, it is only a function of the magnitude of r and not of its direction. Thus,
f(r) ≡(f (r),0,0), expressed in the ˆr, ˆθ, ˆφ basis. It is straightforward to check that in this
case
r · ∇f(0) = −krˆr = −kr, where
−k = df(r)
dr
r=0.
k is called the force constant and gives the strength of the isotropic binding force. Note that,
if V (r) is the potential function for the isotropic force, df (r)
dr
r=0 = −d2V (r)
dr2
r=0 = −k. By
Lagrange’s theorem potential V (r) has a local minimum at r = 0 so that d2V (r)
dr2
r=0 > 0,
making k > 0. This makes −k < 0 and satisﬁes the stability condition. The force −kr,(k >
0), is commonly called Hooke’s law force after Robert Hooke, who invented it to explain
the elastic force causing oscillations of a spring. However, one has to remember that the

Odds and Ends
451
general form of the Hooke’s law force is given by the second term in the Taylor expansion of
any force ﬁeld near a stable equilibrium position, thus giving an universal approximation
to any force ﬁeld having Taylor expansion near a stable equilibrium point. This explains
why Hooke’s law is so ubiquitous in physics and engineering applications. By the same
argument, Hooke’s law is not a fundamental force law, but only a very useful approximation.
Thus, we have to solve the equation
¨r + ω2
0r = 0 where ω2
0 = k
m.
(12.11)
It turns out that bounded orbits of the attractive central force −kr are closed [3, 10, 19], so
that the motion in the vicinity of a stable equilibrium point under such a force is periodic.
Further, note that the torque exerted by −kr on the particle is −kr × r = 0. Therefore, the
angular momentum of the particle must be conserved. This ﬁxes the angular momentum
vector mr×v in space conﬁning the position vector r and the velocity vector v of the particle
to a plane perpendicular to the angular momentum vector. Thus, the motion under such a
central force is planar.
In order to get two linearly independent solutions of Eq. (12.11) let us choose one of
them as the circular orbit obtained by rotating a vector a+ counterclockwise in the plane
of the orbit through the angle ω0t in time t about the unit vector ˆn perpendicular to the
plane of the orbit. Using Eq. (6.45) we get,
r+(t) = Rˆn,ω0t(a+) = cosω0ta+ + sinω0t(ˆn × a+).
(12.12)
To construct the other linearly independent solution we take a vector a−in the plane of the
orbit and rotate it clockwise by the angle ω0t in time t. This amounts to replacing ˆn by −ˆn
and a+ by a−in Eq. (12.12). We get,
r−(t) = cosω0ta−−sinω0t(ˆn × a−).
(12.13)
To get a general solution we add these two linearly independent solutions. We get,
r(t) = a0 cosω0t + b0 sinω0t,
(12.14)
where
a0 = a+ + a−
and
b0 = (a+ −a−) × ˆn.
(12.15)
The two constant vectors a0 and b0 can have any values. Therefore, Eq. (12.14) is the
general solution of Eq. (12.11). If either a0 = 0 or b0 = 0, the motion becomes one
dimensional with oscillations along the line of the surviving vector. Thus, the motion
ceases to be planar and the unit vector ˆn is not uniquely deﬁned, but any unit vector
normal to a± will do.

452
An Introduction to Vectors, Vector Operators and Vector Analysis
The vector coefﬁcients a0 and b0 can be expressed in terms of initial conditions, that
is, the values of the position and velocity vectors at t = 0. Putting t = 0 in the expressions
for r(t) as in Eq. (12.14) and ˙r(t) obtained by differentiating Eq. (12.14) with respect to t,
we get
r0 = r(0) = a0
v0 = ˙r(0) = ω0b0.
(12.16)
An inspection of Eq. (12.14) tells us that it represents a superposition of independent simple
harmonic motions along the lines determined by a0 and b0 or, via Eq. (12.16) along r0
and v0. The resultant motion is elliptical and reduces to one-dimensional simple harmonic
motion if r0 = 0 or v0 = 0 or, generally, when r0×v0 = 0. To see that the orbit is an ellipse
we recast Eq. (12.14) as
r = acosφ + bsinφ,
(12.17)
where a2 ≥b2, a·b = 0 and φ = φ(t) = ω0t +φ0. Vectors a and b deﬁne the major axis
and the minor axis of the ellipse respectively.
Exercise
For an oscillator with orbit
r(t) = acos(ω0t + φ0) + bsin(ω0t + φ0),
ﬁnd the major axis a and the minor axis b from the initial conditions r0 = r(0) and v0 =
˙r(0). Show that, for 0 < φ0 < π
2 ,
a = r0 cosφ0 −v0
ω0
sinφ0
b = r0 sinφ0 + v0
ω0
cosφ0
and that φ0 is given by
tan2φ0 = 2ω0r0 · v0
v2
0 −ω2
0r2
0
.
Hint
Expand the trigonometric functions in the expression for r(t) and compare with
Eq. (12.14) to get expressions for a0 and b0 in terms of a and b and invert these equations
to get the result. To get the equation for φ0 use a · b = 0.
□
We may eliminate φ0 by taking φ0 = ω0t0 and then shifting the origin in time to t0. You
may recognize Eq. (12.17) to be the equation to an ellipse parameterized by φ which we
have encountered before (see section 2.4). a and b respectively give the major axis and the

Odds and Ends
453
minor axis of this ellipse (see Fig. 12.3). As we have mentioned above, we can now see that
the elliptic orbit of an isotropic harmonic oscillator is periodic in space that is, the particle
aquires the same position vector r after a ﬁxed period of time T . However, something more
is true. Both the state variables r and ˙r have exactly the same values at any two times
separated by a ﬁxed time interval T called the period of the motion. We express this by
saying that the motion of the isotropic harmonic oscillator is periodic. For the elliptical
motion, the period T is related to the natural frequency of the oscillator ω0 by
ω0T = 2π.
The motion over a single period is called an oscillation. The constant φ0 is called phase
of an oscillation beginning at t = 0. The maximum displacement from the equilibrium
point during an oscillation is called its amplitude. For the elliptical motion, the amplitude
is A = |a|.
Fig. 12.3
Orbit of an isotropic harmonic oscillator
As we have seen, Eq. (12.14) represents the elliptical motion as a superposition of two
uniform circular motions with opposite senses. This is illustrated in Fig. 12.4. As we can
see from the ﬁgure, this relation provides a practical way to construct an ellipse from two
circles.
Fig. 12.4
Elliptical orbit as a superposition of coplanar circular orbits

454
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Show that the total energy of the oscillator E =
1
2m˙r2 + 1
2kr2
is constant in
time, and hence a constant of the motion. Show further, that E = 1
2k(a2 + b2). In fact
energy is an additive constant of motion, that is, the energy of n > 1 oscillators is the
sum of the energies of individual oscillators. Such additive constants of motion are called
conserved quantities.
□
Exercise
Learn about the damped harmonic oscillator (an oscillator oscillating in a
resistive medium) from a suitable book and try to formulate and solve it using vector
methods. Differentiate between three cases: Light damping, heavy damping and critical
damping.
□
12.2.1
Anisotropic oscillator
In this case we continue to terminate the Taylor series for the force ﬁeld near the stable
equilibrium point after the term linear in r, so that the force is linear in r. However, we lift
the requirement that the force be isotropic. Thus, the anisotropic force is a linear operator
on E3 and all its eigenvalues must be real, because they have to be measurable. Hence, it
must be a symmetric operator. Further, all its eigenvalues must be distinct, because, if two
of them are equal, then the operator will not be anisotropic on the plane spanned by the
corresponding eigenvectors.
The three eigenvectors of an anisotropic force are called
principal vectors and form an orthonormal basis in E3. The corresponding eigenvalue
equations are
f(e1) = −k1e1
f(e2) = −k2e2
f(e3) = −k3e3
where k1,2,3 are the positive force constants giving the strength of the binding force along
the three principal directions. Now, the superposition principle tells us that we can resolve
the general motion along the three principal directions. If ri is the component of
displacement along ei, i = 1,2,3, then we can write for the equation of motion,
m¨r = m¨r1 + m¨r2 + m¨r3 = −k1r1 −k2r2 −k3r3 = −
X
i
ki ˆei.
Since ri are orthogonal and ˆei, i = 1,2,3 do not change with time, each component must
independently satisfy
m¨ri = −kiri, i = 1,2,3,
whose solutions must be of the same form as those for the isotropic oscillator restricted to
one dimensional motion. Thus, the general solution to the anisotropic oscillator is
r = A1e1 cos(ω1t + φ1) + A2e2 cos(ω2t + φ2) + A3e3 cos(ω3t + φ3),
(12.18)

Odds and Ends
455
where, Ak, k = 1,2,3 are the amplitudes of oscillation along ˆek, k = 1,2,3 respectively
and the three natural frequencies are given by
ωi =
r
ki
m.
The orbit corresponding to Eq. (12.18) will be closed and the corresponding motion be
periodic only when the ratios ω1/ω2 and ω2/ω3 are rational numbers. In general, the
orbit will not lie in a plane. Since the individual 1-D oscillations are harmonic, energy of
each of them is conserved so that the total energy of an anisotropic oscillator is conserved.
The corresponding conserved energy is given, with m = 1 by
1
2 ˙r2
i + 1
2ω2
i r2
i = Ei(0) = 1
2A2
i , i = 1,2,3,
where Ei(0) is the value of the energy at t = 0 and Ai is the amplitude of ith oscillation
along ith principal axis. The conservation of energy for the anisotropic oscillator becomes
1
2
3
X
i=1
(˙r2
i + 1
2ω2
i r2
i ) =
3
X
i=1
Ei(0) = E.
It then follows that the orbit of the anisotropic oscillator will be conﬁned to an ellipsoidal
region given by V (r1,r2,r3) = P3
i=1 Vi(ri) = 1
2
P3
i=1 ω2
i r2
i ≤E with principal axes
e1,e2,e3 and centered at the equilibrium point.
If k3 = 0 = ω3, the orbit will be
restricted to e1,e2 plane and is commonly known as a Lissajous ﬁgure.
Exercise
Write down the energy conservation equations for a 2-D anisotropic oscillator.
Show that the rectangle formed by the sides 2A1 and 2A2 is inscribed in the ellipse (see
Fig. 12.5(a)).
□
Fig. 12.5
(a) The regions V ≤E, V1 ≤E and V2 ≤E (b) Construction of a Lissajous
ﬁgure

456
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
Justify, the following procedure to construct an orbit of the anisotropic
oscillator in the e1,e2 plane, with axes labelled x1,x2, ω1 = 1, ω2 = ω, that is,the
corresponding Lissajous ﬁgure. Consider a cylinder with base 2A1 and a strip of width
2A2. We draw on the strip a sine wave with period 2πA1/ω and amplitude A2 and wind
the strip onto the cylinder (see Fig. 12.5(b)). The orthogonal projection (x1,x2,x3) 7→
(x1,x2,0) of the sinusoid wound around the cylinder onto x1,x2 plane gives the desired
orbit, or the Lissajous ﬁgure.
□
Lissajous ﬁgures can be conveniently seen on an oscilloscope which displays independent
harmonic oscillations along the horizontal and vertical axes.
12.3
Projectiles and Terestrial Effects
In this section we deal with projectile motion and the effect of earth’s rotation on it.1
12.3.1
Optimum initial conditions for netting a basket ball
We will now ﬁnd the optimum speed v0 and the angle θ0 with the horizontal for netting a
basket ball at height h and distance L. We show that θ0 is greater than π/4 by an amount
arctan(h/L).
We analyze the above projectile motion in the velocity space (as explained below). This
is conceptually simple, it clearly brings out the basic mechanics and geometry of the
situation and saves algebra.
We assume that the ball is to be thrown from the origin at a horizontal distance L from
the pole on which the basket mounted at a height h from the horizontal xy plane passing
through the origin. Let the initial velocity of the ball be v0. It will experience a constant
force due to gravity, inducing a constant acceleration g in it, directed vertically
downwards. The resulting equation of motion is,
˙r = v = gt + v0,
(12.19)
which integrates to
r = 1
2gt2 + v0t.
(12.20)
The last equation is the parametric equation for the displacement of the ball as a function
of time. The trajectory is a segment of a parabola as shown in Fig. 12.6. To go over to the
velocity space, we look for a curve traced by the vector v = v(t), just as in the position
space we look for the curve traced by r = r(t). This curve is called a hodograph.
According to Eq. (12.19), the hodograph of the ball, which is subject to a constant force, is
a straight line (see Fig. 12.7). To represent the location of the ball on this hodograph, we
need a velocity vector proportional to r(t). Such a vector is obtained by dividing
Eq. (12.20) by t:
⟨v⟩(t) = r
t = 1
2gt + v0.
(12.21)
1Applications in this and the next section are treated in [10] using geometric algebra.

Odds and Ends
457
Fig. 12.6
Trajectory in position space
The vector ⟨v⟩(t) is the average velocity of the ball. Note that ⟨v⟩(t) and r(t) have the
same direction. Comparing Eqs (12.19) and (12.21), we get a simple relation between the
actual velocity and the average velocity:
v(t) = ⟨v⟩(t) + 1
2gt = r
t + 1
2gt.
(12.22)
Figure 12.6 depicts the hodograph given by Eq. (12.21) and also displays Eq. (12.22). We
see that the increment in the velocity of the ball in equal intervals of time is equal.
Fig. 12.7 contains all the information about the projectile motion, so all questions
regarding the motion can be answered by dealing with the triangles in the ﬁgure
graphically or algebraically.
Fig. 12.7
Trajectory in the velocity space
First, consider the question of determining the range r of a target sighted in a direction ˆr
(not necessarily along the horizontal) which is hit by a projectile, launched with velocity
v0. This can be done graphically by using the properties of Fig. 12.7. Having laid out v0
on a graph paper (by choosing appropriate units and scale!) as indicated in Fig. 12.7, one
extends a line from the base of v0, in the direction ˆr, to its intersection with the vertical line
extending from the tip of v0. The length of the two sides of the triangle thus constructed
are then measured, say v1 and v2, to get the magnitude of 1
2gt and r
t respectively. This gives
the time of ﬂight t = 2v1/g and the range r = 2(v1v2)/g. How to get the ﬁnal velocity is
also evident from Fig. 12.7.

458
An Introduction to Vectors, Vector Operators and Vector Analysis
To get to our problem, we ﬁnd the range r algebraically. Crossing Eq. (12.21) with r
we get,
1
2t(g × r) = r × v0,
giving
t = 2v0
g
|ˆr × ˆv0|
|ˆg × ˆr| (Time of ﬂight).
(12.23)
Again, crossing Eq. (12.21) with (−gt), after some simpliﬁcation, using Eq. (12.23) for t,
we get
r = 2v2
0
g
(−ˆg × ˆv0) · (ˆv0 × ˆr)
| −ˆg × ˆr|2
.
(12.24)
Using identity II we get,
(−ˆg × ˆv0) · (ˆv0 × ˆr) = −ˆg · ˆr + (ˆv0 · ˆr)(−ˆg · ˆv0)
= cos
π
2 −φ

+ cos(θ0 −φ)cos
π
2 −θ0

,
(12.25)
where θ0 and φ are the angles respectively made by ˆv0 and ˆr with the horizontal, as shown
in Fig. 12.7.
Fig. 12.8
Graphical determination of the displacement r, time of light t and ﬁnal
velocity v
Now we complete the job in the following two steps. First, for a given v0 and ˆr, we ﬁnd ˆv0
which maximizes the range r in the direction ˆr and also ﬁnd this maximum range, say rmax.
Using this ˆv0 and (r, ˆr) as given, we solve for v0 with rmax = r. Note that r =
√
h2 + L2
and ˆr is speciﬁed by tan(φ) = h/L.

Odds and Ends
459
To ﬁnd the direction ˆv0 which maximizes the range r along ˆr, we note that r is
maximum when the RHS of Eq. (12.25) is maximum. Since ˆr and −ˆg are ﬁxed directions,
we have to maximize the second term on the RHS of Eq. (12.25). This is maximum when
π
2 −θ0 = θ0 −φ which implies θ0 = π
4 + φ
2 . Thus, ˆv0 is directed along the line bisecting
the angle between ˆr and −ˆg (see Fig. 12.8).
Thus,
ˆv0 = ˆr −ˆg
|ˆr −ˆg|.
(12.26)
Substituting Eq. (12.26) in Eq. (12.24) we get,
rmax = 2v2
0
g
1
|ˆr −ˆg|2 = v2
0
g
1
1 + sin(φ).
(12.27)
We leave the last equality for you to check. Solving Eq. (12.27) for v0 with rmax =
√
h2 + L2 = r0 say, (note that sin(φ) = h/r0), we get
v0 =
q
g(r0 + h).
Using θ0 = π
4 + φ
2 and φ = arctan(h/L) we get
θ0 = π
4 + 1
2 arctan
 h
L
!
.
12.3.2
Optimum angle of striking a golf ball
For non-spinning high speed golf balls the force of air drag is roughly linear with velocity
(FD = Cv). Assume that C/m = 0.25 s−1, m = 0.046 kg and that the maximum
horizontal range of 152 m is obtained with an initial speed of 61 m/sec. We show that the
angle of striking has to be 32 degrees with the horizontal, whereas in the absence of any
air drag it would have been 45 degrees.
We have to set up and solve the equation of motion for a ball projected with the initial
velocity v0 from origin under the force of linear drag and constant gravity. Let the drag
force be given by
FD = Cv = −mγv.
which deﬁnes γ. Then the equation of motion is
˙v = g −γv,
(12.28)
or,
(˙v + γv) = g.

460
An Introduction to Vectors, Vector Operators and Vector Analysis
Noting that eγt is the integrating factor, we get,
eγt(˙v + γv) = d
dt (eγtv) = eγtg.
Integrating, we get,
eγtv(t) −v0 = g
Z t
0
eγt′dt′ = g
 eγt −1
γ
!
.
Solving for v(t), we get,
v(t) = g
 1 −e−γt
γ
!
+ v0e−γt.
(12.29)
The constant γ−1 is called relaxation time which is the measure of the time it takes for the
retarding force to make the particle forget its initial conditions. If t ≫γ−1, then e−γt << 1
so that the ﬁrst term on the RHS of Eq. (12.29) dominates all others, irrespective of the
value of v0, giving
v = v∞= γ−1g.
The value v∞is called the terminal velocity, which can also be obtained by putting ˙v = 0
in the equation of motion.
The displacement r of the ball from the origin is found by directly integrating
Eq. (12.29). This gives
r = g
 e−γt + γt −1
γ2
!
+ v0
 1 −e−γt
γ
!
.
(12.30)
Let the plane of motion of the ball be the x−y plane with x axis horizontal. Equation (12.30)
gives rise to the equations
x = v0x
 1 −e−γt
γ
!
(12.31)
y = g
 e−γt + γt −1
γ2
!
+ v0y
 1 −e−γt
γ
!
.
(12.32)
At the end of its range, the ball touches the ground, so y = 0, making the RHS of
Eq. (12.32) equal to zero. This gives a transcendental equation for the time of ﬂight t
which does not have a closed form solution. Assuming t to be sufﬁciently large so as to
make e−γt small enough, we expand e−γt in powers of t and retain terms only up to
second order so that contribution due to gravity is properly included. We now ﬁnd the
positive root of the resulting quadratic in t and substitute in Eq. (12.31). Putting

Odds and Ends
461
v0x = v0 cosθ0 and v0y = v0 sinθ0 where θ0 is the angle at which the ball is projected
and v0 = |v0|, we ﬁnd that we have now got an equation expressing the range x as a
function of θ0. To ﬁnd θ0 for the maximum range, we solve
dx
dθ0 = 0. Using the given
data, we get θmax
0
= 320.
Equation (12.28) is useful in the analysis of microscopic motions also. For example,
consider an electron (with mass m and charge e) moving in a conductor under the
inﬂuence of a constant electric ﬁeld E. The electron’s motion is retarded by the collisions
with the lattice. We may represent the retardation by the resistive force proportional to the
velocity of the electron. If the resistance is independent of the direction in which the
electron moves, we say that the conductor is an isotropic medium. We can then write the
resistive force in the form −µv, where µ is a scalar constant. We are thus led to the
equation, (compare with Eq. (12.28)),
˙v = eE −µv.
(12.33)
For times large compared to the relaxation time τ = m/µ, the electron reaches the
terminal velocity
v =
" e
µ
#
E
(12.34)
and the result is a steady current in the conductor. The electric current density J is given by
J = Nev,
(12.35)
where N is the number density of electrons. Substituting Eq. (12.34) in Eq. (12.35) we get
Ohm’s law
J = σE,
(12.36)
where the conductor’s d-c conductivity σ is given by
σ = Ne2/µ.
(12.37)
Ohm’s law holds remarkably well for many conductors over a wide range of currents. The
conductivity σ and the electron density N can be measured, so µ can be calculated from
Eq. (12.37). Then, the relaxation time can also be calculated and compared with the
measured values. These are in general agreement with the extremely short relaxation
times observed in metals. Thus, Eq. (12.33) is vindicated to some degree. However, we
note that the velocity v in Eq. (12.33) cannot be regarded as the velocity of an individual
electron, whose trajectory must be very irregular as it collides repeatedly with the massive
atoms in the lattice. Thus, v in Eq. (12.33) must be a kind of average electron velocity.
Thus, our classical analysis can describe, (if at all), only the average motion in the
microscopic domain. Derivation and explanation of equations like Eq. (12.33), pertaining
to the electron’s motion in a metal, requires statistical mechanics and the basic equations
of quantum mechanics.

462
An Introduction to Vectors, Vector Operators and Vector Analysis
12.3.3
Effects of Coriolis force on a projectile
A projectile is ﬁred due east from a point on the surface of the earth at a geographical
latitude λ with speed v0 and at an angle of elevation α above the horizontal. We ﬁnd the
lateral deﬂection of the projectile when it strikes the earth. We also ﬁnd the change in the
range of the projectile due to the rotation of the earth.
We use a rotating frame of reference ﬁxed to the surface of the earth topocentric frame
to analyse this motion (see Fig. 12.9). We have to account for the inertial forces namely the
centrifugal force m(ω × r) × ω and Coriolis force 2m(v × ω) where ω is the rotational
velocity of the earth and (r,v) are the instantaneous position and velocity of the projectile,
as measured in the rotating (topocentric) frame. We add the gravitational and centrifugal
accelerations to get,
geﬀ= g + (ω × r) × ω.
Since the earth’s surface is a geoid, geﬀis normal to it.
Thus, the equation of motion becomes
˙v = ¨r = geﬀ+ 2(v × ω).
(12.38)
Henceforth, we replace geﬀby g so that whenever we write g we actually mean geﬀ. Also
we neglect the resistance due to air.
Fig. 12.9
Terrestrial Coriolis effect
From Eq. (12.38) we can compute the effect of Coriolis force on the projectile motion,
treating g to be a constant. The principal source of variation in g is the deviation of earth’s
ﬁgure from sphericity and the non-uniformity of its mass distribution (density). Another
reason is the possible fall from great heights (multiples of earth’s radius) which is unrealistic
for a surface to surface projectile. Anyway, here we shall treat g to be a constant. Actually,

Odds and Ends
463
in the approximation of constant g and ω Eq. (12.38) can be exactly solved. In our case,
however, for typical velocities we have 2|(v×ω)| << g because of the relatively small value
for the angular speed of the earth (ω = 7.29 × 10−5 radians sec−1).
Thus, a perturbation solution is more useful here and we proceed to get it in the
following way.
We regard the Coriolis term in Eq. (12.38) as a small perturbing force. Then
Eq. (12.38) can be solved by the method of successive approximations. We write velocity v
as an expansion of successive orders in ω,
v = v1 + v2 + v3 + ···
(12.39)
The zeroth order term v1 is required to satisfy the unperturbed equation ˙v1 = g, which
integrates to
v1 = gt + v0,
(12.40)
where v0 is the initial velocity. Inserting v to the ﬁrst order in Eq. (12.38) we get,
˙v = ˙v1 + ˙v2 = g + 2(v1 + v2) × ω.
Neglecting the second order term 2v2 × ω this reduces to an equation for v2 when v1 is
replaced by the RHS of Eq. (12.40),
˙v2 = 2v1 × ω = 2(gt + v0) × ω.
This integrates to
v2 = (gt2 + 2v0t) × ω.
(12.41)
We can determine the higher order corrections v3,v4 ··· in a similar way.
Fig. 12.10
Topocentric directional parameters

464
An Introduction to Vectors, Vector Operators and Vector Analysis
Substituting Eqs (12.40) and (12.41) in Eq. (12.39) we get the velocity to the ﬁrst order in
ω as
v = v0 + gt + (gt2 + 2v0t) × ω
(12.42)
Integrating this, we get a parametric equation for the displacement
r = 1
2gt2 + v0t + ∆r
(12.43)
where the deviation ∆r from a parabolic trajectory (due to Coriolis force) is given to the
ﬁrst order by
∆r = (v0 + 1
3gt) × ωt2 + ···
(12.44)
To estimate the magnitude of the correction ∆r, we observe from Eqs (12.43) and (12.44)
that
|∆r|
|r| ≈ωt
(12.45)
For the correction to be one percent we must have ωt ≥0.01 and from the value of ω we
ﬁnd that the time of ﬂight must be at least two minutes, which is more than the time of ﬂight
in a typical projectile problem. Hence, we need not consider the corrections of order higher
than the ﬁrst. Indeed, before considering the higher order corrections, the assumption that
g is a constant should be examined.
The expression in Eq. (12.44) for the Coriolis deﬂection ∆r is not in its most convenient
form as it is not given as a function of target location r. To circumvent this, we use the zeroth
order approximation
r ≈1
2gt2 + v0t
(12.46)
to eliminate v0 in Eq. (12.44), with the result,
∆r = −tω × (r −1
6gt2).
(12.47)
This shows the directional dependence of ∆r on r. To eliminate t from Eq. (12.47) in favour
of r we cross both sides of Eq. (12.47) with g to get,
(r × g) = (v0 × g)t,
or,
(r × g) · (v0 × g) = |v0 × g|2t,

Odds and Ends
465
or,
t = (r × g) · (v0 × g)
|v0 × g|2
.
(12.48)
Similarly, again from Eq. (12.46) we have,
1
2t2 = (r × v0) · (g × v0)
|g × v0|2
.
(12.49)
Note that
r −1
6gt2 = r
 
ˆr −1
3
"(ˆr × ˆv0) · (ˆg × ˆv0)
|ˆg × ˆv0|2
#
ˆg
!
.
(12.50)
This shows that the two terms in Eq. (12.47) are of the same order of magnitude.
To ﬁnd the change in range due to the Coriolis force we have to ﬁnd the component of
∆r in the direction ˆr, which is easily obtained from Eq. (12.47) as
ˆr · ∆r = t3
6 ˆr · (ω × g).
(12.51)
Similarly, the vertical deﬂection is given by
ˆg · ∆r = tr · (ω × ˆg)
(12.52)
The vector ω × ˆg is directed west, except at poles, so both Eqs (12.51) and (12.52) vanish
for the trajectories to the north or south. They have maximum values for the trajectories to
the west. This is due to rotation of the earth in opposite direction while the projectile is in
ﬂight.
In most circumstances, resistive forces have a greater effect on the range and vertical
deﬂection than the Coriolis force. The lateral Coriolis deﬂection is more signiﬁcant as it
will not be masked by resistive forces, that is, the observed lateral deﬂection is solely due to
Coriolis force, as the resistive forces do not have any component in the lateral direction. Of
course, resistive forces will change ∆r (and also its lateral component) via their inﬂuence
on the velocity which in turn governs the Coriolis force.
For a target on a horizontal plane, g · r = 0 and ˆg × ˆr is a rightward unit vector. From
Eq. (12.47), then, the rightward deﬂection ∆R is given by
∆R = (ˆg × ˆr) · ∆r
= −t(ˆg × ˆr) ·
 
ω ×
 
r −t2
6 g
!!
= −t
"
(ˆg · ω)
 
ˆr ·
 
r −t2
6 g
!!
−
 
ˆg ·
 
r −t2
6 g
!!
(ˆr · ω)
#

466
An Introduction to Vectors, Vector Operators and Vector Analysis
= t
"
−r(ˆg · ω) −t2
6 g(ˆr · ω)
#
= −rtω ·
 
ˆg + t2g
6r ˆr
!
= −rtω ·
 
ˆg + 1
3
(ˆr × ˆv0) · (ˆg × ˆv0)
(ˆg × ˆv0) · (ˆg × ˆv0) ˆr
!
.
Here, we have used Eq. (12.49). We now use the identity II and Fig. 12.10 to get
∆R = rtωcosλ(tanλ −1
3 tanα cosφ).
(12.53)
For nearly horizontal trajectories (α ≈0), the second term in Eq. (12.53) can be neglected,
giving ∆R = rtωsinλ which is positive in the northern hemisphere and negative in the
southern hemisphere. As a general rule, therefore, the Coriolis force tends to deﬂect
particles to the right in the northern hemisphere and to the left in the southern
hemisphere. However, this rule is violated by highly arched trajectories and Eq. (12.53)
tells us that for a trajectory satisfying
tanα0 = 3tanλ
cosφ ,
(12.54)
the Coriolis deﬂection ∆R vanishes. In the northern hemisphere, deﬂection will be to the
left for α > α0 and to the right for α < α0. In the southern hemisphere, these inequalities
reverse.
From Eq. (12.48), the time of ﬂight for a target on the horizontal plane is
t =
r
v0 cosα.
(12.55)
Since the projectile is ﬁred due east, φ = π
2 , so from Eq. (12.53) we get,
∆R = rtωsinλ.
(12.56)
We eliminate t from Eq. (12.56) using Eq. (12.55) to get,
∆R =
 r2
v0
!
ωsecα sinλ
(12.57)
with obvious dependence on the hemisphere.
To get the change in range for a projectile ﬁred due east, we note that the angle between
ˆr and ˆω × ˆg is π. So (ω ×g)· ˆr = −ωg cosλ. Substituting this result and the expression for
the time of ﬂight from Eq. (12.55) into Eq. (12.51) we get,

Odds and Ends
467
Change in range = −1
6
 r
v0
!3
ωg sec3 α cosλ
which does not depend on the hemisphere.
Exercise
River Brahamaputra ﬂows southwards near Guwahati. Find the difference in
water levels at its right and left banks if its width is 5000 meters, the latitude of Guwahati
is 26◦11′ and its speed is 10 km/hr.
Solution
It is observed that the Coriolis effect denudes the right banks of large rivers in
the northern hemisphere ﬂowing over long stretches more than their left banks. On the
rivers in the southern hemisphere, the effect is opposite. The following solution to this
exercise will help you understand this.
We set up the topocentric coordinate system with its x-axis along the ﬂow
(southwards), y axis along the transverse horizontal direction to the left of the ﬂow
(eastward) and the local vertical along the z axis. Let the direction of the ﬂow make an
angle φ (in the anticlockwise sense) with respect to the geographical north direction.
Since the river is ﬂowing southwards, φ = π. (Now draw a ﬁgure). In this frame, the
rotational velocity of the earth and the velocity of the river v can be resolved as
ω = ω(sinλ ˆk + cosλcosφ ˆi −cosλsinφ ˆj)
and
v = vˆi.
Here, ˆi,ˆj, ˆk are the unit vectors along x,y,z axes respectively. With φ = π the Coriolis
acceleration ac becomes
ac = 2v × ω = −2vωsinλ ˆj,
(12.58)
which is towards the right of the ﬂow (westward). So the total acceleration of the water is
ac + g (see Fig. 12.11) with ac given by Eq. (12.58). From Fig. 12.11 we see that the angle
made by the resultant ac + g with g (angle α in Fig. 12.11) is given by
tanα =
 ac
g
!
.
(12.59)
Now the water surface must be normal to the vector ac + g, so it makes angle α with the
horizontal. If the level difference is h and width of the river is W we have from Eq. (12.59),
 h
W
!
=
 ac
g
!

468
An Introduction to Vectors, Vector Operators and Vector Analysis
or,
h =
 ac
g
!
W
Putting numerical values of all the quantities involved we get the result.
Fig. 12.11
Net acceleration of river water
12.4
Satellites and Orbits
12.4.1
Geometry and dynamics: Circular motion
We show that motion on a circular orbit, conserving angular momentum, corresponds to
the force f = −mv2
r ˆr, where r is the radius of the circle. Note that this is an attractive central
force. Further, if Kepler’s third law is satisﬁed, we show that the force must vary inversely
as the square of the distance r from the center.
We ﬁrst make only one assumption, that the angular momentum is conserved. To get
the velocity, we differentiate r = rˆr with respect to time to get,
˙r = ˙rˆr + r˙ˆr.
(12.60)
Cross Eq. (12.60) with r to get,
H = r × ˙r = rr × ˙ˆr = r2ˆr × ˙ˆr,
(12.61)
where H is the speciﬁc angular momentum (angular momentum per unit mass) which is
conserved. Cross Eq. (12.61) by ˆr on the right so that
H × ˆr
r2
= (ˆr × ˙ˆr) × ˆr = ˙ˆr,
(12.62)

Odds and Ends
469
where we have used the identity I and the fact that ˆr · ˙ˆr = 0. We substitute Eq. (12.62) in
Eq. (12.60) to get
˙r = ˙rˆr + H × ˆr
r
.
(12.63)
To get the acceleration we differentiate Eq. (12.63) with respect to t and again use
Eq. (12.62) and identity I. We have,
¨r =
 
¨r −H2
r3
!
ˆr.
(12.64)
Now, we make use of the assumption that the motion is circular. This means H = rv,
where v is the constant speed of the particle on the circle and also ˙r = 0 = ¨r. Therefore,
the acceleration is
¨r = −v2
r ˆr
and the force is
f = m¨r = −mv2
r
ˆr.
(12.65)
Let us now assume that Kepler’s third law is valid i.e., r3
P 2 is a constant say C, where P is
the period of the orbit. For circular motion the period P is related to v by v = 2πr
P
or,
v2 = 4π2r2
P 2
.
Putting 1
P 2 = C
r3 in this equation we get
v2 = 4π2 C
r .
(12.66)
Put Eq. (12.66) in Eq. (12.65) to get
f = −4π2Cm
r2
ˆr.
Thus, the conservation of angular momentum and Kepler’s third law mean that, for circular
motion, the force exerted on a moving particle is central, attractive and varies inversely as
the square of the radius of the circle.

470
An Introduction to Vectors, Vector Operators and Vector Analysis
Exercise
The turning points of a satellite orbit are deﬁned by the condition v · r = 0.
Show that, for a turning point, the conservation of Runge–Lenz vector gives the relation
r = K
2E′ (e −ˆr),
(12.67)
where E′ is the speciﬁc energy (energy per unit mass) and K is the constant in the
gravitational force law.
The conservation of the Runge–Lenz (or the eccentricity) vector e is given by
v × H = K(e + ˆr),
(12.68)
where H is the angular momentum per unit mass (speciﬁc angular momentum). Put
H = r × v and use the identity I to get
v2r −(r · v) = K(e + ˆr).
(12.69)
At the turning point r·v = 0, so the second term on the LHS vanishes. Further, v2 is related
to E′ by [19]
v2 = 2

E′ + K
r

.
(12.70)
Substitute this expression for v2 in Eq. (12.69) to get
2

E′ + K
r

r = K(e + ˆr),
which easily simpliﬁes to Eq. (12.67). It is instructive to sketch this relation on an elliptic
or hyperbolic orbit. Note that Eq. (12.67) speciﬁes the turning points only in terms of the
conserved quantities.
□
12.4.2
Hodograph of an orbit
We ﬁnd the hodograph for the Keplerian orbit of a satellite/spacecraft, that is, a curve over
which the tip of the velocity vector moves as the satellite moves on its orbit.
We know that a Keplerian orbit is a consequence of the conservation of the eccentricity
vector given by
v × H = K(e + ˆr),
(12.71)
so it is no surprise that the hodograph, (which is the orbit in the velocity space), follows
directly from it. Take the vector product with H on both sides of Eq. (12.71) to get
H × (v × H) = KH × (e + ˆr).
(12.72)

Odds and Ends
471
Using identity I on the LHS of Eq. (12.72) we get,
H2v −(H · v)H = KH × (e + ˆr).
Since H · v = 0, we get,
v = K
H ( ˆH × e + ˆH × ˆr).
(12.73)
Since H × e is a constant vector, let us put
u = K
H ( ˆH × e),
(12.74)
so that
v −u = K
H ( ˆH × ˆr)
(12.75)
or, squaring both sides,
(v −u)2 = K2
H2 .
(12.76)
This equation describes a circle of radius (K/H) centered at point u given by Eq. (12.74).
Since the centre of the circle is determined by the eccentricity vector as in Eq. (12.74),
the distance u = |u| of the centre from the origin is used to classify the orbits as shown in
the following table. In the fourth column, we use |K| to make room for both attractive (K >
0) and repulsive (K < 0) inverse square law force, (for example, Coulomb force between
two like charges, where K = −q1q2 < 0), although here we have assumed attractive inverse
square law (Newtonian gravity), as we are dealing with spacecrafts and satellites.
Table 12.1
Classiﬁcation of Orbits with H , 0
Conic section
Eccentricity
Energy
Hodograph centre
Hyperbola
e > 1
E′ > 0
u > |K|
H
parabola
e = 1
E′ = 0
u = |K|
H
Ellipse
0 < e < 1
E′ < 0
u < |K|
H
Cirle
e = 0
E′ = −K2
2H2
u = 0
Thus, the orbit is an ellipse if the origin is inside the circle, or an hyperbola if the origin is
outside the circle. For an elliptical orbit the hodograph described by Eq. (12.73) is a single
complete circle, as shown in Fig. 12.12 You may check the consistancy of Fig. 12.12 with
Eq. (12.73). Notice how, by parallelly moving any velocity vector v on the hodograph, we
can determine the corresponding position r on the orbit.

472
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 12.12
Eliptical orbit and Hodograph
As an application, we ﬁnd the orbital distance of a satellite as a function of its velocity. First,
I leave it for you to show, using Eq. (12.73), Eq. (12.74), the fact that H · e = 0 and using
(twice!) identity II that
u · v = K2
H2 (e2 + e · ˆr).
Now, we know that the eccentricity is related to the speciﬁc energy, that is, energy per unit
(reduced) mass by
e2 = 1 + 2E′H2
K2
Therefore, after a bit of rearrangement we get,
u · v −2E′ = K2
H2 (1 + e · ˆr).
Using the equation to the orbit (in the real space!)
(1 + e · ˆr) = H2
K
1
r
we ﬁnally get
r = r(v) =
−K
2E′ −u · v
as the orbital distance of a satellite as a function of its velocity. Note that both u and E′
are conserved quantities. Thus, knowledge of u and E′ for a particular orbit enables us to
determine the orbital distance of the satellite if we know its velocity.

Odds and Ends
473
12.4.3
Orbit after an impulse
An impulsive force such as ﬁring of a rocket will produce a change ∆v in the velocity of a
satellite without a signiﬁcant change in its position during a short time interval for which
the impulse acts. We show that, to the ﬁrst order, the resulting change in the eccentricity
vector of satellite’s orbit is given by
K∆e = v × ∆H + ∆v × H,
(12.77)
where ∆H = r × ∆v. We use this to determine qualitatively the effect of a radial and a
tangential impulse on a circular orbit. We also get the effect of an impulse perpendicular to
the orbital plane.
As pointed out, the impulsive force will change the velocity from v to v + ∆v
instantaneously, without any corresponding change in r. Therefore, after the impulse the
eccentricity vector will go over to the new (conserved) value given by
Kenew = (v + ∆v) × (r × (v + ∆v)) −Kˆr.
So, the change in the eccentricity vector ∆e is given by
K∆e = (v + ∆v) × (r × (v + ∆v)) −v × (r × v).
Using the distributive property of the cross product and neglecting terms of higher order
in ∆v, the above expression goes over to
K∆e = v × (r × ∆v) + ∆v × (r × v)
(12.78)
= v × ∆H + ∆v × H
(12.79)
For a circular orbit e = 0, so after the impulse, if ∆e , 0, then a circular orbit will go
over to an orbit with eccentricity ∆e. For a radial impulse to a circular orbit, as shown in
Fig. 12.13(a), ∆H = r×∆v = 0, so K∆e = ∆v×H which is a vector pointing towards east
if the direction of ∆v is north. The resulting elliptical orbit is shown in Fig. 12.13(b).
Fig. 12.13
Orbits after impulse

474
An Introduction to Vectors, Vector Operators and Vector Analysis
For a tangential impulse towards west, as shown in Fig. 12.13(c), both the terms in
Eq. (12.77) point towards north, pushing the force centre towards north. The resulting
elliptical orbit is shown in Fig. 12.13(d).
I leave it for you to show that ∆e = 0 for an impulse perpendicular to the plane of the
orbit. So this impulse does not change the shape of the orbit.
Exercise
Atmospheric drag tends to reduce the orbit of a satellite to a circle. For a rough
estimate of this effect, suppose that the net effect of the atmosphere is a small impulse at
the perigee which reduces the satellite speed by a factor α (see Fig. 12.14 ). Show that the
resulting change in the eccentricity is
∆e = −2α(e + 1)ˆe.
(12.80)
For e = 0.9 and α = 0.01 estimate the number of orbits required to get to a circular orbit.
Show that the speed at perigee actually increases with each orbit.
Solution
We have to obtain the change in the eccentricity due to impulse at perigee. The
general expression for the change in eccentricity due to an impulse ∆v is given by
Eq. (12.77) with the corresponding deﬁnition of ∆H.
In this problem the relevant
quantities are,
∆v = −αv+ ˆv ; r = r+ˆr = a(1 −e)ˆr ; v = v+ ˆv.
Here, r+ denotes the distance of perigee from the origin (a focus) and v+ denotes the speed
at perigee. Putting these expressions in Eq. (12.77) and simplifying, we get,
K∆e = −2αv2
+a(1 −e)ˆe.
(12.81)
To get rid of v2
+, note that for ˆr = ˆe, the conservation law for the eccentricity vector
becomes,
v × H = v2
+a(1 −e)ˆe = K(e + 1)ˆe.
(12.82)
Fig. 12.14
Earth’s atmospheric drag on a satellite circularising its orbit

Odds and Ends
475
Substitute for v2
+a(1 −e) from Eq. (12.82) into Eq. (12.81) to get Eq. (12.80). The number
of orbits required to get to a circular orbit that is, to reduce the eccentricity to zero, with
the given values of α and e is

e
∆e

=
0.9
0.038  24.
I leave it for you to check the last sentence in the exercise.
□
12.5
A Charged Particle in Uniform Electric and Magnetic Fields
12.5.1
Uniform magnetic ﬁeld
A uniform magnetic ﬁeld is constant in space and time within the region in which the
charged particle moves. The classical equation of motion of a particle with charge q, mass
m and velocity v in a constant magnetic ﬁeld B is
m˙v = q
c v × B.
(12.83)
We club the constants together by writing
ω ≡−q
mcB,
(12.84)
so that Eq. (12.83) becomes
˙v = ω × v.
(12.85)
Dotting both sides of Eq. (12.85) with v we see that d
dt(v · v) = 0 which means that the
magnitude of the velocity of a charged particle moving in constant magnetic ﬁeld is
invariant in time. Thus, we expect vector v to perform pure rotational motion about the
constant magnetic ﬁeld B or ω. This is expressed by saying that vector v precesses around
magnetic ﬁeld B (see Fig. 12.15).
Taking cue from this observation, we resolve v into components parallel and
perpendicular to ω or B as
v = v∥+ v⊥.
(12.86)
We substitute Eq. (12.86) in Eq. (12.85) to get two equations, one for each of v∥and v⊥
˙v⊥= ω × v⊥and
˙v∥= 0.
(12.87)
The second of these equations can be integrated immediately, giving
v∥(t) = v0∥,
(12.88)

476
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 12.15
Velocity vector precesses about ω
where v0 = v0∥+ v0⊥and v0 = v(0) is the value of v(t) at t = 0.
We have to deal with the ﬁrst of Eq. (12.87) separately. We know that v⊥rotates about
ω without any change in its magnitude. We expect a solution of the form
v⊥(t) = eωt ˆω×v0⊥= cosωtv0⊥+ sinωt( ˆω × v0⊥),
(12.89)
where ω = |ω|.
Exercise
Show that v · (ω × eωt ˆω×v0⊥) = 0.
Hint
Show ﬁrst that ω ×eωt ˆω×v0⊥= cosωt(ω ×v0⊥) −ωsinωtv0⊥. Both terms cancel
after dotting with v⊥because v⊥·v0⊥= |v⊥|2 cosωt and v⊥·(ω ×v0⊥) = ω|v⊥|2 sinωt.
From this exercise we ﬁnd that the vector eωt ˆω×v0⊥is normal to both v and ω. Therefore,
it must be proportional to ˙v. The proportionality constant is not of any physical
consequence and can be taken to be unity. Thus, the solution to Eq. (12.85) is
v(t) = eωt ˆω×v0⊥+ v0∥
= cosωtv0⊥+ sinωt( ˆω × v0⊥) + v0∥.
(12.90)
To get the trajectory of the particle we have to integrate v(t) with respect to time. We get,
r(t) = x(t) −x0 = sin(ωt)
ω
v0⊥+ cos(ωt)
ω
(v0⊥× ˆω) + v0∥t or,
r(ωt) =
"eωt ˆω×(v0 × ω)
ω2
#
+
v0 · ω
ω2

ωt,
(12.91)

Odds and Ends
477
where x0 is the constant of integration, so that the state of the particle at t = 0 is given
by (x0,v0). We have also used (ω × v0∥) = 0, and v0∥= v0 · ˆω ˆω. Equation (12.91) is a
coordinate free equation of an helix (see Fig. 12.16) with radius
a ≡(v0 × ω)
ω2
and pitch
b ≡v0 · ω
ω2 .
We can make Eq. (12.91) look like a helix by expressing it in terms of
θ = θ ˆω where θ = ωt.
(12.92)
Fig. 12.16
(a) Right handed helix (b) Left handed helix
In terms of these variables, Eq. (12.91) takes the form
r(θ) = eθ ˆθ×a + bθ
(12.93)
where a · θ = 0. The helix is said to be right handed if b > 0 and left handed if b < 0 (see
Fig. 12.16).
Equation (12.91) gives a circular trajectory if v0∥= 0. The radius vector r rotates with
an angular speed ω = |qB|/mc called the cyclotron frequency. Equation (12.84) tells us
that ω has the same (opposite) direction as the magnetic ﬁeld B when the charge q is
negative (positive). As shown in Fig. 12.17, the circular motion of a negative (positive)
charge is right handed (left handed).

478
An Introduction to Vectors, Vector Operators and Vector Analysis
Fig. 12.17
Rotational velocity of a charge q about ω
12.5.2
Uniform electric and magnetic ﬁelds
Here, we consider the motion of a point charge q, driven by the simultaneously present
uniform electric and magnetic ﬁelds. The equation of motion of a charged particle with
charge q and mass m moving under the simultaneous action of constant electric ﬁeld E and
constant magnetic ﬁeld B is obtained via the Lorentz force as
m˙v = q

E + v
c × B

.
(12.94)
We can supress all constants by writing
g = q
mE and ω = −qB
mc,
(12.95)
so that the equation of motion becomes
˙v = g + ω × v.
(12.96)
As in the case of uniform magnetic ﬁeld, we resolve each vector in this equation into its
components parallel and perpendicular to ω so that,
v = v∥+ v⊥,
g = g∥+ g⊥.
(12.97)
This generates two equations, since ω × v∥= 0,
˙v∥= g∥,
˙v⊥= g⊥+ ω × v⊥.
(12.98)
Let the velocity at t = 0 be v(0) = v0 which is also resolved parallel and perpendicular
to ω:
v(0) = v0 = v0∥+ v0⊥.
(12.99)

Odds and Ends
479
The ﬁrst of Eq. (12.98) with initial condition Eq. (12.99) can be readily integrated to give,
v∥(t) = g∥t + v0∥
= (g · ω)ω−1t + v0∥
=
q
mE∥t + v0∥= bt + v0∥say,
(12.100)
where ω−1 = ω/|ω|2 (see subsection 1.7.1).
To integrate the second of Eq. (12.98) with initial condition Eq. (12.99), we re-write it,
using identity I and the fact that g⊥· ω = 0, as follows.
˙v⊥= ω × [(g⊥× ω−1) + v⊥].
(12.101)
Equation (12.101) is the same as the ﬁrst of Eq. (12.87) with v⊥replaced by the expression
in the square bracket, which is given by adding a constant vector to v⊥. Therefore, it can
be solved in a similar way and is given by
v⊥(t) = eωt ˆω×a + c
= cosωta + sinωt( ˆω × a) + c,
(12.102)
with
a = (g⊥× ω−1) + v0⊥= (g × ω−1) + v0⊥= v0⊥−d E × B−1
(d : a scalar constant) and c is the constant of integration. Since
v⊥(0) = v0⊥= (g⊥× ω−1) + v0⊥+ c,
we must have
c = −(g⊥× ω−1) = −(g × ω−1).
(12.103)
Noting that (g⊥× ω−1) = (g × ω−1) and combining Eqs (12.100), (12.102) and (12.103),
we can write the solution of Eq. (12.96) as
v(t) = eωt ˆω×a + bt + c,
(12.104)
where the vectors a and b are deﬁned above and the vector c is re-deﬁned as
c = v0∥−g × ω−1 = v0∥+ d E × B−1.

480
An Introduction to Vectors, Vector Operators and Vector Analysis
Integrating Eq. (12.104) with respect to time, we get the equation to the path of the charge
q (Exercise) as
r(t) = x(t) −x0 = eωt ˆω×(a × ω−1) + 1
2bt2 + ct,
(12.105)
where x0 is the constant of integration, giving the initial position of the particle to be
x(0) = x0 + a×ω−1. If we take the origin at x(0), then x0 = ω−1 ×a. With this choice of
the origin, r(0) = a × ω−1, so the vector r at t = 0 lies on the circle of radius a/ω with its
center at the origin and the particle trajectory passes through this point. Note that the
vectors a and a × ω−1 lie in the plane perpendicular to ω, while b is parallel to ω.
It is instructive to write
r(t) = r1(t) + r2(t),
(12.106)
where
r1(t) = 1
2bt2 + ct,
(12.107)
which is an equation to a parabola parameterized by t and
r2(t) = eωt ˆω×(a × ω−1),
(12.108)
which generates a uniform circular motion along a circle of radius |a × ω−1| = |a|/|ω| =
a/ω.
Fig. 12.18
Trajectory of a charged particle in uniform electric and magnetic ﬁelds
Thus, we see that the motion of a charged particle under the combined inﬂuence of
uniform electric and magnetic ﬁelds is the composite of two motions, a parabolic motion
of the guiding center described by Eq. (12.107) and the uniform circular motion around
the guiding center along a circle with radius a/ω, in a plane normal to ω, given by
Eq. (12.108). The composite motion corresponding to Eq. (12.106) can be viewed as the

Odds and Ends
481
motion of a point on a spinning disc whose axis is aligned with the vertical and whose
center is traversing a parabola. This is depicted in Fig. 12.18 and the corresponding
directions of the electric and magnetic ﬁelds are shown in Fig. 12.19.
Fig. 12.19
Directions of electric and magnetic ﬁelds for Fig. 12.18
Fig. 12.20
Trochoids traced by a charge q when the electric and magnetic ﬁelds are
orthogonal

482
An Introduction to Vectors, Vector Operators and Vector Analysis
The position vector of the particle relative to the guiding center repeats itself after a period
of 2π/ω = 2πmc/|qB|. Thus, after every such period, the net change in r(t) can be
viewed as a result of only the motion of the guiding center along the parabola. This fact is
expressed by saying that the motion about the guiding center averages to zero over a
period of 2π/ω. So motion of the guiding center can be regarded as an average motion of
the particle. Accordingly, the velocity of the guiding center is called the drift velocity of
the particle.
Case of orthogonal ﬁelds
The special case of motion in orthogonal electric and magnetic ﬁelds has important
applications. In this case, g · ω = 0 = E · B making b = 0. Thus, Eq. (12.107) becomes
r1(t) = ct and the parabolic trajectory of the guiding center reduces to a straight line
parallel to c. If the initial velocity is orthogonal to the magnetic ﬁeld,
v0∥= 0 so that
˙r1 = c = ω−1 × g = d E × B−1.
Thus, the drift velocity is perpendicular to both the electric and the magnetic ﬁeld. The
particle trajectory is the composition of the drift motion of the center of a circle and the
uniform circular motion of a point on this circle. The resulting path of the particle is the
curve traced out by a point on a disc at a distance a/ω from the center, rolling without
slipping with its center drifting along vector c with drift speed |c| = |ω−1 × g| = d |E×
B−1| = d |E|/|B| and angular speed ω = −q|B|/mc. This curve is, in general, a trochoid
we described in subsection 9.3.2. Now if r2 is the position vector of the dot on the rolling
disc which traces the path of the charged particle, then its linear velocity must match with
that of the particle, namely c. Thus, we require that
|ω × r2| = |c|.
In terms of magnitudes of individual vectors, this condition means r2 = c/ω. Since r2
depends on ﬁxed quantities c and ω, it has ﬁxed value provided we assume that the
initial velocity does not have a component parallel to the magnetic ﬁeld. Comparison with
the radius of the disc a/ω, which depends on the initial velocity, generates three
possibilities, namely, r2 = a/ω, r2 < a/ω and r2 > a/ω. These conditions characterize
three classes of trochoids, the ﬁrst of which is the cycloid. These trochoids are illustrated
in Fig. 12.20(a,b,c).
Equation (12.105) tells us that the particle motion coincides with that of the guiding
center if a = 0, which is satisﬁed if
v0 = ω−1 × g = d E × B−1.
(12.109)
The trajectory is a straight line if E · B = 0. This suggests an effective way to construct a
velocity ﬁlter for charged particles. Only a particle with initial velocity satisfying condition

Odds and Ends
483
Eq. (12.109) will continue moving in its original staight line without any deﬂection. E and
B ﬁelds can be adjusted to select a large range of velocities. The selection is independent of
the sign of the charge or the mass of the particle.
12.6
Two-dimensional Steady and Irrotational Flow of an
Incompressible Fluid
By irrotational ﬂow, we mean its velocity ﬁeld satisﬁes
∇× q = 0.
It follows that the velocity ﬁeld q is derivable from a scalar potential φ(x),
q = −∇φ(x).
Since the ﬂow is steady and the ﬂuid incompressible, its net ﬂow through any closed
volume is zero, giving
∇· q = 0.
This implies
∇2φ = 0,
or, the potential φ(x) satisﬁes the Laplace equation in two dimensions
∂2φ
∂x2 + ∂2φ
∂y2 = 0.
A function ψ(x) which forms a pair of harmonic functions with φ(x) also satisﬁes
∇2φ = 0
for such a ﬂow.
Since the ﬂow is 2-D, we can use the isomorphism between the planar vectors and
complex numbers and express the ﬂow via the function
f (z) = φ(x,y) + ψ(x,y).
Now consider the integral of f (z) along a curve C in the complex plane
Z
C
f (z)dz =
Z
C
(φ + iψ)(dx + idy)
=
Z
C
(φdx −ψdy) + i
Z
C
(ψdx −φdy).

484
An Introduction to Vectors, Vector Operators and Vector Analysis
For an irrotational ﬂow derivable from a potential, we expect this integral to be independent
of the chosen curve C and be a function only of the end point coordinates. This is possible
if and only if φ(x,y) and ψ(x,y) satisfy
∂φ
∂x = ∂ψ
∂y , and ∂ψ
∂x = −∂φ
∂y ,
which are the Cauchy–Riemann conditions, necessary and sufﬁcient for the function f (z)
to be analytic. We can turn around and say that the real and imaginary parts of an analytic
function represent a 2-D irrotational steady ﬂow of an incompressible ﬂuid, as all analytic
functions satisfy the Cauchy–Riemann conditions.
It is easy to see that at all points
∇φ · ∇ψ =
 
ˆi∂φ
∂x + ˆj∂φ
∂y
!
·
 
ˆi∂ψ
∂x + ˆj∂ψ
∂y
!
= ∂φ
∂x
∂ψ
∂x + ∂φ
∂y
∂ψ
∂y
= 0
by virtue of the Cauchy–Riemann conditions. Thus, the equipotential surfaces for φ and ψ
at each point are perpendicular to each other. If φ(x,y) is taken to be the velocity potential,
then the velocity q = −∇φ must be along the line of constant ψ. Such a curve, with its
tangent given by ∇φ, is called the stream line. By Bernoulli’s theorem (see for example,
[19]), the stream function is constant along all stream lines. So ψ can be treated as the
stream function of the problem.
We will now pick up some analytic functions and see what type of ﬂow patterns they
represent.
(i) f (z) = z2 = (x2 −y2) + i2xy.
Thus,
φ(x,y) = x2 −y2 and ψ(x,y) = 2xy.
The ﬂow pattern is depicted in Fig. 12.21. This is the ﬂow pattern expected around a
rectangular corner. (Combine half x axis and half y axis to form a rectangle.)
(ii) f (z) = zn, n > 2.
Here,
f (z) = (reiθ)n = rneinθ = rn cos(nθ) + irn sin(nθ) = φ + iψ.

Odds and Ends
485
Fig. 12.21
Two-dimensional ﬂow around a 90◦corner
This corresponds to a ﬂow pattern around an angle α = π/n. The case with n = 3
is shown in Fig. 12.22.
Fig. 12.22
Two-dimensional ﬂow around a 60◦corner
(iii) f (z) = A√z,
A being a real constant. Here,
φ(x,y) = A
√
r cos(θ/2) and ψ(x,y) = A
√
rsin(θ/2).
This gives
2φ2
A2 = 2r cos2(θ/2) = r(1 + cosθ) = r + x
and
2ψ2
A2 = 2r sin2(θ/2) = r(1 −cosθ) = r −x.

486
An Introduction to Vectors, Vector Operators and Vector Analysis
Hence, φ = constant and ψ = constant are the confocal and coaxial parabolas
respectively (see Fig. 12.23). This corresponds to a ﬂow turning around the edge of a
semi-inﬁnite plane sheet.
Fig. 12.23
Two-dimensional ﬂow around a Semi-inﬁnite straight line
(iv) f (z) = −M
2πz,
M being a real constant. This gives,
φ = −M cosθ
2πr
and ψ = Msinθ
2πr
.
Fig. 12.24
Two-dimensional ﬂow around a 2-D doublet source consisting of a source
and a sink of equal strength, at an inﬁnitesimal separation
The resulting ﬂow pattern is shown in Fig. 12.24. This ﬂow represents a doublet source
with a source and sink sitting at the origin. The streamlines are like that of some dipole
ﬁeld lines. The source strength M is like the dipole moment of the source.
f (z) = q0z. This gives the uniform stream with stream velocity q0 in the direction of
the negative x axis.

Appendices


A
Matrices and Determinants
In this appendix we develop the theory of matrices and determinants, as required by this
book, emphasizing their connection with vectors. This approach is not coordinate-free: We
have to represent vectors by their coordinates with respect to some basis. This approach has
the advantage of being easily generalizable to higher dimensional spaces. Our interest in
matrices and determinants stems from their role in understanding of and computations
with linear operators and their connection with the orientations of triplets of vectors and
of surfaces. In the course of this appendix we may re-derive some of the results we have
obtained in the text. Of course, this appendix can be used to explain all instances where
we have used matrices and/or determinants. Theory of matrices is an independent, fully
developed branch of mathematics worthy of an independent, rewarding and fruitful study.
We recommend [12] for such a study.
A.1
Matrices and Operations on them
A matrix is the arrangement of m × n real or complex numbers in m rows and n columns.
In this book, we deal with real matrices with m,n ≤3, although in this appendix we deal
with a general m × n real matrix. The pair (m,n) deﬁnes the size of a matrix. We use
capital letters to denote a matrix, thus a matrix with m rows and n columns is denoted
Am×n or just A if the sufﬁx m × n can be left understood. An element in the ith row and
jth column in A is denoted aij and the matrix is written
A = [aij] i = 1,...,m ; j = 1,...,n.
On most occasions the ranges of the subscripts i and j are left understood.
By ﬁxing an orthonormal basis in En we have the isomorphism
x ∈En ↔


x1
x2...
xn


∈Mn×1,
(A.1)

490
Appendices
where Mn×1 is the space of n×1 matrices called column vectors. For an orthonormal basis
ˆek k = 1,...,n in En we have the correspondence
ˆek ↔


0
0
...
1
...
0


; k = 1,...,n
(A.2)
where for ˆek, 1 occurs in the kth row. The transpose of a vector x is deﬁned by xT =
(x1 x2 ... xn). The transpose of a column vector is the corresponding row vector. Both the
column vectors representing {ˆek}, k = 1,...,n and the row vectors representing {ˆeT
k }, k =
1,...,n are called “coordinate vectors”.
Exercise
Show that the set of all m × n real matrices forms a linear space of dimension
mn.
Hint
Show that this set is isomorphic with the space of all mn-tuples, namely Rmn.
□
The rows of a m×n matrix A can be identiﬁed with the vectors a1,a2,...,am as the vectors
in Rn,
ak = (ak1,ak2,...,akn) ; k = 1,2,...,m.
The matrix A can be written as
A =


a1
a2
...
am


.
(A.3)
Given an n dimensional vector x and a m dimensional vector y,
x ↔


x1
...
xn


and y ↔


y1
...
ym


,
(A.4)
the equation
Ax = y
(A.5)

Appendices
491
stands for a system of equations
a11x1 + a12x2 + ··· + a1nxn
=
y1
a21x1 + a22x2 + ··· + a2nxn
=
y2
...
am1x1 + am2x2 + ··· + amnxn
=
ym
(A.6)
The system of simultaneous equations, (Eq. (A.6)), can be written as
x1


a11
a21
...
am1


+ x2


a12
a22
...
am2


+ ··· + xn


a1n
a2n
...
amn


=


y1
y2
...
ym


.
(A.7)
Viewed as the system of simultaneous equations, Eq. (A.5) connects the components
(x1,...,xn) of the vector x with respect to the basis of vectors deﬁned in the last equation
in an n-dimensional subspace to the components of the same vector (y1,...,ym) with
respect to the basis ˆek ; k = 1,...,m. Thus, in this case Eq. (A.5) becomes a passive
transformation transforming the components of the same vector from one basis to the
other.
We can also view Eq. (A.5) as an active transformation or as a map or a linear operator
A : En 7→Em mapping vectors x ∈En to vectors y ∈Em. If we shift the origin by a constant
vector b then Eq. (A.5) becomes
y = Ax + b
(A.8)
Equation (A.8) deﬁnes an afﬁne transformation. This is the most general result of the action
of a matrix on a vector.
As an example, the matrix
A =


2
3
−1
3
−1
3
2
3
−1
3
−1
3


(A.9)
can be actively interpreted as a mapping of vectors x = (x1x2) in the (x1x2) plane onto
the vectors y = (y1,y2,y3) in the plane deﬁned by
y1 + y2 + y3 = 0

492
Appendices
which is perpendicular to the vector N = (1,1,1) and which we call π. Geometrically, the
point (y1,y2,y3) is obtained by projecting the point (x1x2,0) perpendicularly to the plane
π. Alternatively, the corresponding system of equations
y1 = 2
3x1 −1
3x2; y2 = −1
3x1 + 2
3x2; y3 = −1
3x1 −1
3x2
can be interpreted passively as a parametric representation of the plane π, with x1,x2 as
parameters.
Given a scalar λ we have,
λA = [λaij] ; i = 1,...,m ; j = 1,...,n.
Two matrices of the same size can be added. The ijth element of the matrix obtained by
adding A and B is the addition of the ijth elements of the matrices A and B :
A + B = [aij + bij]
C = A + B implies cij = aij + bij. Thus, we can construct a linear combination P
k λkAk
where Ak k = 1,... are the matrices of the same size say m×n and λk are scalars. Addition
of matrices is associative, (A+B)+C = A+(B+C) and commutative, A+B = B+A. It
is distributive with respect to the multiplication by a scalar. That is, λ(A + B) = λA + λB
and (α + β)A = αA + βA, α,β,λ being scalars.
Two matrices can be multiplied provided the number of columns of the left multiplier
equals the number of rows of the right multiplier. Then the ijth element of the product is
cij =
X
k
aikbkj.
That is, the ith row of A is is elementwise multiplied with the jth column of B and the
corresponding products are summed over, to get the ijth element of the product C = AB.
Note that, in general, AB , BA, that is, matrix product is not commutative. In fact only
one of the products AB or BA may be deﬁned while the other may not.
Product of matrices can be understood via the composition of mappings. If y = Ax
is the map A : Em 7→En deﬁned by the matrix An×m = [aji] then by linearity, as shown
above, its explicit form is
yj =
m
X
i=1
ajixi.
Now suppose Bp×n = [bkj] deﬁnes a map z = By, En 7→Ep, then the vector z is given by
zk =
n
X
j=1
bkjyj =
n
X
j=1
m
X
i=1
bkjajixi =
m
X
i=1
ckixi,

Appendices
493
where
cki =
n
X
j=1
bkjaji; k = 1,...,p; i = 1,...,m.
Thus, z = Cx where C = BA = [cki] is the matrix with p rows and m columns deﬁned by
the last equation. Accordingly, we take the matrix C deﬁned above to be the product BA of
matrices A and B in that order.
The matrix product is associative and distributive with respect to matrix addition. Thus,
for three matrices A,B,C with appropriate sizes,
(AB)C = A(BC)
and
A(B + C) = AB + AC.
Note that, in the last equation, matrices B and C must be of the same size, so if the product
AB is deﬁned, so is AC. The last equation is valid with multiplication in the reverse order.
For the mappings of vectors determined by matrices, we can write
(A + B)x = Ax + Bx; (λA)x = λ(Ax); A(B + C)x = ABx + ACx.
From the deﬁnition of the scalar product of two vectors in terms of their coordinates, we
see that x·y = xT y where x and y are the column vectors (n×1 matrices) representing the
vectors x and y. For an orthonormal basis {ˆek}, k = 1,...,n we have
ˆeT
i · ˆek = [0 0 ··· 1 ··· 0]


0
0
...
1
...
0


=

0
for i , k,
1
for i = k.
(A.10)
where 1 is at ith place in the left multiplier and at kth place in the right multiplier. Thus,
coordinate vectors are orthonormal, as they should be. In general, for any two orthogonal
vectors, we have,
x · y = xT y = 0.
We end this subsection by deﬁning the transpose of a matrix. The transpose of a m × n
matrix A is the n × m matrix AT obtained by interchanging the rows and columns of A.
Thus, the ijth element of AT , denoted aT
ij is the same as the jith element of A giving us the
deﬁning equation

494
Appendices
aT
ij = aji.
The transpose AT of a n × n square matrix A is also a n × n square matrix.
A.2
Square Matrices, Inverse of a Matrix, Orthogonal Matrices
Square matrices are those having equal number of rows and columns and are extremely
important in applications. The order of a square matrix is the number of rows or columns.
Any two square matrices of the same order n can be added or multiplied. We can form
powers of such a matrix
A2AA, A3 = AAA,··· .
The zero matrix O of order n is the matrix all of whose elements are zero. All the rows
(columns) of zero matrix are zero vectors 0 = (0,0,...,0)T of n dimensional space. It has
the obvious properties
A + O = A = O + A, AO = OA = O
for all nth order matrices A and
Ox = O for all x ∈En.
The unit matrix of order n, denoted I is the matrix representing the identity mapping
Ix = x for all x ∈En.
In particular, for any orthonormal basis in En we must have
I ˆek = ˆek k = 1,2,...,n,
from which we can conclude that the column (row) vectors in I are given by the coordinate
vectors as in Eq. (A.2).
I = (ˆe1, ˆe2,··· , ˆen) =


1
0
0
···
0
0
1
0
···
0
...
...
...
...
0
0
0
···
1


·
(A.11)
The nth order unit matrix I is the multiplicative identity for matrix multiplication. That is,
IA = AI = A
for all nth order matrices A.

Appendices
495
Given a nth order matrix A, the matrix A−1 satisfying
A−1A = I = AA−1
is called the inverse of A. A nth order matrix A for which A−1 exists is called invertible. We
state and prove the following properties of a nth order invertible matrix.
(i) The inverse of a nth order invertible matrix A is unique.
Proof
If possible, let B and C be two distinct inverses of A satisfying AB = BA =
I = AC. Then we have,
B −C = BA(B −C) = B(AB −AC) = BO = O
so that B = C.
□
(ii) A nth order matrix A is invertible if and only if Ax = 0 implies x = 0, or, if and only
if x , 0 implies Ax , 0.
Proof
(if part). We are given that Ax = 0 implies x = 0. We show that the
corresponding map A : En 7→En is both one to one and onto and hence invertible. If
possible, let x1 , x2 with Ax1 = Ax2. This means, by linearity of A that
A(x1 −x2) = 0 so that A maps a non-zero vector x1 −x2 to the zero vector,
contradicting the axiom. Therefore, Ax1 = Ax2 implies x1 = x2 or, in other words,
A is one to one. Since the images of two distinct vectors in En under the map A are
distinct, and since the map A is deﬁned for all vectors in En, the image set of A
coincides with its domain En or, in other words, A is onto. Therefore, the inverse of
the map A exists and the corresponding matrix is the inverse of the matrix A.
(only if part). We are given that A is invertible. Then Ax = 0
=⇒
A−1Ax =
0
=⇒
x = 0. A matrix mapping a non-zero vector to the zero vector is called
singular. Thus, a matrix is invertible if and only if it is non-singular
□
(iii) A nth order matrix A is invertible if and only if its determinant is not zero.
Proof
(if part) The determinant of a square matrix is the product of its eigenvalues.
If the determinant is zero, then at least one of the eigenvalues of A is zero. Since the
eigenvector is non-zero, the corresponding eigenvalue equation reads Ax = 0x = 0,
so that A maps a non-zero vector to the zero vector and hence must not be
invertible. Alternatively, if det(A) , 0, the system AX = Y has unique solution
BY = X. Substituting, these two equations into each other we get AB = I = BA
which means B = A−1.
(only if part) We are given that A is invertible. Therefore, A−1A = I so that
det(A−1A) = det(A−1)det(A) = det(I) = 1 which means det(A) , 0.
□
(iv) A nth order matrix A is invertible if and only if it maps every basis to some basis.
Proof
(if part) We are given that A maps a linearly independent set x1,x2,...,xn to
the linearly independent set Ax1,Ax2,...,Axn. Consider x = Pn
k=1 akxk such that

496
Appendices
Ax = Pn
k=1 akAxk = 0. Since {Axk}; k = 1,...,n are linearly independent, this
equation is satisﬁed only when all aks are zero, in which case x = Pn
k=1 akxk = 0.
Thus, Ax = 0 implies x = 0 or A is invertible.
(only if part) We are given that A is invertible, so that Ax = Pn
k=1 akAxk = 0
implies x = Pn
k=1 akxk = 0. Since {xk} is a basis, the last equation makes all aks
zero, which means, via the previous equation, that the set Ax1,Ax2,...,Axn is
linearly independent.
□
(v) A nth order matrix A is invertible if and only if the column vectors of A are linearly
independent.
Proof
From Eq. (A.7) it is clear that Ax = 0 for x , 0 if and only if the column
vectors of A are linearly dependent.
□
Exercise
Show that a matrix is singular if and only if its determinant vanishes.
□
We have deﬁned and used orthogonal matrices in connection with the rotation of a vector
about a direction in space. The orthogonal matrices correspond to linear operators or
transformations that preserve length or distance between points in space. If two points
P ,Q in space with coordinates (xi,yi), i = 1,...,n go over to points P ′,Q′, with
coordinates (x′
i,y′
i), i = 1,...,n under an orthogonal transformation deﬁned by the
orthogonal matrix R = [aij], then we require that
d2(P ,Q) =
n
X
i=1
(xi −yi)2 =
n
X
i=1
(x′
i −y′
i)2 = d2(P ′,Q′).
(A.12)
Putting x′
i = P
j aijxj and y′
i = P
k aikxk in Eq. (A.12) you can check that Eq. (A.12) is
satisﬁed provided
n
X
i=1
aijaik = δjk,
(A.13)
where δjk is the Kronaker delta, which is zero when j , k and is 1 if j = k, or,
aj · ak = δjk.
(A.14)
That is, the jth and the kth column vectors of R are orthonornal. Since a set of orthogonal
vectors is essentially linearly independent, the n column vectors of R form an orthonormal
basis of the n dimensional space. Thus, every orthogonal matrix is invertible, by virtue of
(v) above. In fact Eq. (A.13) can be written as
n
X
i=1
aT
jiaik = δik,

Appendices
497
or,
RT R = I = RRT .
(A.15)
Thus, the transpose of an orthogonal matrix equals its inverse.
More generally, the orthogonal transformation preserves the scalar product:
Rx · Ry = x · y.
(A.16)
Exercise
Show that an orthogonal matrix R must have det(R) = ±1.
Solution
We have, det(RT R) = det(RT )det(R) = (det(R))2 = det(I) = 1 which
gives det(R) = ±1.
□
The set of orthogonal 3×3 matrices with det(R) = +1 represents all possible rotations in
3-D Euclidean space. This result is due to Euler (see section 6.6). In fact, the
correspondence between the the rotations and orthogonal matrices with det(R) = +1 is
an isomorphism:
R1 ◦R2 = R =⇒[R1] [R2] = [R],
where [R1],[R2] and [R] represent the corresponding rotations.
The passive and active interpretations of the orthogonal transformations are described
in the text (see section 6.4).
A.3
Linear and Multilinear Forms of Vectors
Our next task in this appendix is to deﬁne determinants and formulate their principal
properties. We need some general albraic notions to do this job.
A function f (x) of vector argument x is called a linear form in x if
f (λx + µy) = λf (x) + µf (y)
for any vectors x,y and scalars λ,µ. Thus, for example, f (x) = f (x1,x2,x3) = ax1 −
bx2 + cx3 is a linear form, while f (x) = |x| =
q
x2
1 + ··· + x2n is not. More generally, a
linear form is the one satisfying
f (λ1x1 + ··· + λmxm) = λ1f (x1) + ··· + λmf (xm)
valid for any m vectors x1,...,xm and scalars λ1 ...,λm. In fact we can write any vector a
as a normal form involving a basis ˆe1,··· , ˆen :
a = a1ˆe1 + ··· + anˆen ≡(a1,a2,...,an).

498
Appendices
Thus, f (a) has the form
f (a) = a1f (ˆe1) + ··· + anf (ˆen) = c1a1 + c2a2 + ··· + cnan,
where ci are the constant values ci = f (ˆei). We deﬁne the vector c ≡(c1,c2,...,cn) to get
f (a) = c · a.
Thus, the most general linear form in a vector a is the scalar product of a with with a
suitable constant vector c.
A function f (x,y) of two vectors x ≡(x1,...,xn), y ≡(y1,...,yn) is called a bilinear
form in x,y if f is a linear form in x for ﬁxed y and a linear form in y for ﬁxed x. Thus, we
require that
f (λx + µy,z) = λf (x,z) + µf (y,z)
f (x,λy + µz) = λf (x,y) + µf (x,z)
(A.17)
for any vectors x,y,z and scalars λ,µ. The simplest example of a bilinear form is the vector
product
f (a,b) = a · b.
Here, the rules Eq. (A.17) reduce to the associative and distributive laws for the scalar
product. More generally, we ﬁnd,
f (αa + βb,γc + δd)
=
αf (a,γc + δd) + βf (b,γc + δd)
=
αγf (a,c) + αδf (a,d) + βγf (b,c) + βδf (b,d).(A.18)
Thus, we can deal with the binary forms as we deal with ordinary products in multiplying
out expressions. Using the decomposition of a vector in terms of a basis ˆe1,··· , ˆen, we get,
for the most general bilinear form in a,b,
f (a,b) =
n
X
j,k=1
ajbkf (ˆej, ˆek) =
n
X
j,k=1
cjkajbk
(A.19)
with constant coefﬁcients
cjk = f (ˆej, ˆek).
For b = a, the bilinear form f goes over to the quadratic form
f (a,a) =
n
X
j,k=1
cjkajak.

Appendices
499
It is now straightforward to generalize to the multilinear forms in m vectors a1,a2,...,am
along with their components
a1 ≡(a11,a21,...,an1); a2 ≡(a12,a22,...,an2); ...am ≡(a1m,a2m,...,anm).
The function f is a multilinear form f (a1,a2,...,am) in a1,a2,...,am if it is a linear form
in each vector when the others are held ﬁxed. We can also consider f as a function of a
n × m matrix
A = [a1,a2,...,am] = [ajk],
where a1,a2,...,am are its column vectors. Generalizing the bilinear case, the most general
multilinear form in a1,a2,...,am is given by
f (a1,a2,...,am) =
X
j1,j2,...,jm=1,...,n
cj1j2···jmaj11aj22 ···ajmm
(A.20)
where
cj1j2···jm = f (ˆej1, ˆej2,..., ˆejm).
Exercise
Write explicitly Eq. (A.20) for m = 3,4,5 and n = 3. Construct explicitly the
n × m matrix in each case.
□
A.4
Alternating Multilinear Forms: Determinants
A function of several arguments, which could be vectors or scalars, is called alternating
if it just changes its sign as a result of interchanging any two of its arguments. Examples
of alternating functions of scalar arguments are φ(x,y) = y −x, φ(x,y,z) = (z −y)
(z −x)(y −x). A function f of two n-dimensional vectors a1,a2 is alternating if
f (a1,a2) = −f (a2,a1)
for all a1,a2. This implies that
f (a,a) = 0.
Consider a 2-dimensional space and an alternating function f (a1,a2) with a1 =
(a11,a21), a2 = (a12,a22). Then,
f (ˆe1, ˆe1) = f (ˆe2, ˆe2) = 0, f (ˆe2, ˆe1) = −f (ˆe1, ˆe2).
It then follows from Eq. (A.19) that
f (a1,a2) = f (a11ˆe1 + a21ˆe2,a12ˆe1 + a22ˆe2)

500
Appendices
and using the fact that f is alternating, the right side of this equation can be written
(a11a22 −a12a21)f (ˆe1, ˆe2) = c

a11
a12
a21
a22
 = c det(a1,a2),
(A.21)
where c = f (ˆe1, ˆe2) and we take the last equality as the deﬁnition of the determinant of
the second order of the matrix whose columns comprise the components of vectors a1,a2.
Thus, every bilinear alternating form of two vectors a1,a2 in two-dimensional space differs
from the determinant of the matrix with columns a1,a2 by a constant factor c.
More generally, an alternating bilinear form of two vectors in n-dimensional space can
be written
f (a1,a2) =
n
X
j,k=1
cjkaj1ak2,
where
cjk = −ckj, cjj = 0.
Combining the terms with subscripts which differ only by a permutation, we can express f
as the linear combination of second order determinants.
f (a1,a2) =
n
X
j,k=1
j<k
cjk

aj1
ak1
aj2
ak2
·
(A.22)
The alternating function of three vectors, f (a1,a2,a3) changes sign whenever any two
of its arguments are exchanged. More generally, its sign is changed when the number of
exchanges of the pairs of its arguments is odd, and its sign does not change if the number
of corresponding exchanges are even. f vanishes if two of its arguments are equal.
Exercise
Construct all possible permutations of the arguments a1,a2,a3 of an
alternating form which change its sign and which do not change its sign.
□
Let
a1 ≡(a11,a21,a31), a2 ≡(a12,a22,a32), a3 ≡(a13,a23,a33)
be three 3-D vectors. The general alternating trilinear form f in a1,a2,a3 is
f (a1,a2,a3) =
3
X
j,k,r=1
cjkraj1ak2ar3,

Appendices
501
where, using the conditions under which an alternating form changes or does not change
sign and the conditions under which it vanishes, we have,
cjkr = f (ˆej, ˆek, ˆer) = εjkrf (ˆe1, ˆe2, ˆe3),
where εjkr are simply the Levi-Civita symbols which by now we know so well.
Exercise
Show that εjkr = sign(φ(j,k,r)) where φ(j,k,r) = (r −k)(r −j)(k −j).
□
We can now write the expression for f (a1,a2,a3) explicitly using the deﬁnition of {cjkr}.
We have,
f (a1,a2,a3)
=
(a11a22a33 + a12a23a31 + a13a21a32
−a13a22a31 −a11a23a32 −a12a21a33)f (ˆe1, ˆe2, ˆe3)
(A.23)
or,
f (a1,a2,a3) = c

a11
a12
a13
a21
a22
a23
a31
a32
a33

,
(A.24)
where c = f (ˆe1, ˆe2, ˆe3) is a constant. Therefore, the most general trilinear alternating form
in three 3-dimensional vectors a1,a2,a3 differs from the determinant of the matrix with
columns a1,a2,a3 by a constant factor c. Note that
f (ˆe1, ˆe2, ˆe3) = det(ˆe1, ˆe2, ˆe3)f (ˆe1, ˆe2, ˆe3)
so that
det(ˆe1, ˆe2, ˆe3) = 1
as it should be.
Generalization to higher order matrices is now straightforward. Consider a n×n matrix
A =


a11
a12
···
a1n
a21
a22
···
a2n
...
...
...
an1
an2
···
ann


,
(A.25)
with column vectors a1,a2,...,an. Let f be a multilinear alternating form in a1,a2,...,an
as given by Eq. (A.20) where the coefﬁcients cj1j2···jn have the form cj1j2···jn =
f (ˆej1, ˆej2,..., ˆejn). Since f is an alternating form, these coefﬁcients are given by
cj1j2···jn = f (ˆej1, ˆej2,..., ˆejn) = εj1j2···jnf (ˆe1, ˆe2,..., ˆen),

502
Appendices
where εj1j2···jn = −1 whenever j1j2 ···jn is obtained from 1,2,...,n by odd number of
pairwise exchanges (odd permutation of 1,2,...,n), εj1j2···jn = +1 whenever j1j2 ···jn
is obtained from 1,2,...,n by even number of pairwise exchanges (even permutation of
1,2,...,n) and εj1j2···jn = 0 if any two of j1j2 ···jn are equal. Thus, εj1j2···jn are the set of
nn symbols each with n subscripts which can be deﬁned to be the Levi-Civita symbols with
n subscripts.
Exercise
Find the values of ε321, ε2143, ε4231, ε54321.
□
Exercise
Show that εj1j2···jn
=
sign(φ(j1,j2,...,jn)) where φ(j1,j2,...,jn)
=
Πj,k=1,...,n
j<k
(xk −xj).
□
We deﬁne the determinant of the matrix A in Eq. (A.25) as
det(A) =

a11
a12
···
a1n
a21
a22
···
a2n
...
...
...
an1
an2
···
ann

=
X
j1...jn
εj1j2···jnaj11aj22 ...ajnn.
(A.26)
where j1 ...jn runs over the set of permutations of 1,2,...,n (see the following exercise).
Exercise
Show that there are n! terms in the expansion of an nth order determinant given
by Eq. (A.26).
Solution
We have to show that there are n! non-zero values of εj1j2···jn. Since no two
values of the subscripts can be the same, we have n choices for j1, n−1 choices for j2 ... n−k
choices for jk ... so that the total number of distinct εj1j2···jn are n(n−1)(n−2)···(n−k)···1
or n! which is the same as the number of terms in the required expansion. This makes the
nth order determinant a nth degree form in the ajk consisting of n! terms.
□
Exercise
Show that determinant is linear in each of its columns separately.
□
A.5
Principal Properties of Determinants
Equation (A.26) gives the explicit formula for the determinant of a n×n matrix, or the nth
order determinant, in terms of its n2 elements ajk. As shown above, this determinant is
an nth degree form having n! terms. Apart from the Levi-Civita symbols, each term is a
product of n elements one from each column and each row. Although, the expaqnsion in
Eq. (A.26) is explicitly computable, it has too many terms to keep track of (5! = 120 for a
ﬁfth order determinant and 10! = 36,28,800 for a tenth order determinant) to be useful
for numerical computations and more efﬁcient ways of evaluating determinants have been
devised.
From the fact that any nth order determinant is proprtional to a n degree alternating
multilinear form in n vectors a1,a2,...,an in an n-dimensional space, we infer that for the

Appendices
503
corresponding matrix A with these vectors as its column vectors, the determinant changes
sign if we interchange any two of its columns. Thus, the determinant of a square matrix A
changes sign if we interchange any two columns of A; in particular, the determinant of a
square matrix A with two identical columns vanishes. Using the linearity of the determinant
in each of its columns separately, we ﬁnd that multiplying one column of the matrix A by a
factor λ has the effect of multiplying the determinant of A by λ. For example,
det(λa1,a2,...,an) = λdet(a1,a2,...,an).
In particular, for λ = 0 and arbitrary a1 we ﬁnd
det(0,a2,...,an) = 0,
with the same result for any other column so that the determinant of a matrix A vanishes
if any column of A is the zero vector. Multiplying all elements of A by λ amounts to
multiplying every column of A by λ so that
det(λA) = λn det(A).
From the multilinearity of determinants, we conclude more generally that
det(a1+λa2,a2,...,an) = det(a1,a2,...,an)+λdet(a2,a2,...,an) = det(a1,a2,...,an)
since the matrix (a2,a2,...,an) has two identical columns. Generally, the value of the
determinant of the matrix A does not change if we add a multiple of one column to a
different column. However, if we multiply a column by λ and add it to the same column,
then the value of the determinant changes by the factor 1 + λ.
We now show that the determinant of the product of two nth order matrices A and B is
the product of their determinants. To see this, note that if C = AB the resulting matrix C
is given by
C =


a1 · b1
a1 · b2
···a1 · bn
a2 · b1
a2 · b2
···a2 · bn
...
...
...
an · b1
an · b2
···an · bn


,
(A.27)
where a1,a2,...,an are the row vectors of A while b1,b2,...,bn are the column vectors
of B. From Eq. (A.27) we see that, keeping A ﬁxed, det(C) is a linear form in column
vectors {bk} of B. Further, this is an alternating form because interchanging two columns
of B corresponds exactly to interchanging the corresponding columns of C. Hence, det(C)
is an alternating multilinear form in the column vectors of the matrix B. Consequently,
det(C) = γ det(B),

504
Appendices
where γ is the value of det(C) when bk = ˆek k = 1,...,n or when B is the unit matrix I.
Now, if B = I, then C = AB = AI = A so that γ = det(A). Thus we get,
det(AB) = det(A)det(B).
(A.28)
Exercise
Show that det(A−1) = 1/(det(A)).
□
We shall now show that a square matrix A and its transpose AT have the same determinant:
det(A) = det(AT ).
(A.29)
To see this, note that in the expansion of the determinant (Eq. (A.26)) we can rearrange the
factors in each term according to the ﬁrst subscripts (e.g., a31a12a23 = a12a23a31) so that,
aj11aj22 ...ajnn = a1k1a2k2 ...ankn.
(A.30)
where k1,k2,...,kn is again a perpmutation of 1,2,...,n.
Exercise
Show that εj1j2···jn = εk1k2···kn.
Solution
We have to show that the permutations j1j2 ···jn and k1k2 ···kn of 1,2,...,n are
either both even or both odd. This follows from the observation that these permutations
are inverses of each other.
□
Equation (A.30) and the above exercise immediately lead to
det(A) =
X
k1k2...kn
εk1k2···kna1k1a2k2 ...ankn = det(AT ).
An immediate consequence of Eq. (A.29) is that a determinant can be considered to be an
alternating multilinear form of its row vectors. In particular, determinant changes its sign if
we interchange any two rows. Another consequence is that if det(A) , 0 , det(AT ) then
the matrix AT is invertible, so that the column vectors of AT or the row vectors of A also
form a linearly independent set.
Combining Eqs (A.28) and (A.29) we get
det(A)det(B) = det(AT )det(B) = det(AT B).
Combining this result with Eq. (A.27) we get, for the matrices A,B deﬁned via their column
vectors, A = (a1,a2,...,an) and B = (b1,b2,...,bn),
det(A)det(B) = det(AT B) =

a1 · b1
a1 · b2
···a1 · bn
a2 · b1
a2 · b2
···a2 · bn
...
...
...
an · b1
an · b2
···an · bn

·
(A.31)

Appendices
505
A.5.1
Determinants and systems of linear equations
Determinants can be used to ﬁnd whether a set of n vectors a1,a2,...,an in
n-dimensional space are depemdent, or, equivalently, when a square matrix A with
column vectors a1,a2,...,an is singular. We show that a square matrix A is singular if and
only if its determinant is zero.
If A is singular, then its column vectors a1,a2,...,an are linearly dependent. Thus, one
of the column vectors, say a1 can be expressed in terms of the others:
a1 = λ2a2 + λ3a3 + ··· + λnan.
It then follows from the multilinearity of determinants that
det(A)
=
det(λ2a2 + λ3a3 + ··· + λnan,a2,...,an)
=
λ2 det(a2,a2,...,an) + λ3 det(a3,a2,a3,...,an) + ···
+λndet(an,a2,a3,...,an)
=
0,
(A.32)
since each of the matrices has a repeated column.
Conversely, if A is non-singular, it is invertible and we have
det(AA−1) = det(A)det(A−1) = det(I) = 1
so that det(A) , 0, which completes the proof.
Now consider the system of equations
AX = Y
where X and Y are n × 1 column vectors and A is an n × n matrix with column vectors
a1,a2,...,an. This system of equations can be re-expressed as
x1a1 + x2a2 + ... + xnan = y.
Then, it is straightforward to show (Exercise) that
det(a1,...,ak−1,y,ak+1,...,an) = xk det(a1,a2,...,an), k = 1,2,...,n.
If the matrix A is non-singular, we can divide by its determinant and get the solution
x1,x2,...,xn expressed in terms of determinants:
x1 = det(y,a2,...,an)
det(a1,a2,...,an), x2 = det(a1,y,...,an)
det(a1,a2,...,an),
...,xn = det(a1,a2,...,y)
det(a1,a2,...,an).
This is Crammer’s rule for the solution of n linear equations in n unknowns.

506
Appendices
A.5.2
Geometrical interpretation of determinants
We start by showing how various properties of the vector product are related to
determinants. We start with the deﬁnition
det(a,b,c) =

a1
b1
c1
a2
b2
c2
a3
b3
c3

=

a1
a2
a3
b1
b2
b3
c1
c2
c3

·
(A.33)
Written out as an alternating linear form in vector c we have, (see Eq. (A.23)),
det(a,b,c) = (a2b3 −a3b2)c1 + (a3b1 −a1b3)c2 + (a1b2 −a2b1)c3 = z · c,
where z ≡(z1,z2,z3) is the vector with components
z1 = a2b3 −a3b2 =

a2
b2
a3
b3
,
z2 = a3b1 −a1b3 =

a3
b3
a1
b1
,
z3 = a1b2 −a2b1 =

a1
b1
a2
b2
·
From the components of z it is clear that z = a × b. Therefore,
det(a,b,c) = c · (a × b).
If we cyclically permute the factors on the right side, we have to interchange the columns
(or rows) of the determinant on the left twice, leaving the determinant invarient. Thus,
det(a,b,c) = a · (b × c) = c · (a × b) = b · (c × a).
(A.34)
The components zi of the vector z = a × b are themselves second order determinants and
hence are bilinear alternating forms of vectors a,b. This immediately leads to the laws of
vector multiplication stated in the text (see Eq. (1.10)).
The property a×a = 0 follows from a×b = −b×a. More generally, the vector product
two vectors a×b vanishes if a and b are linearly dependent, as we have seen in the text. To
prove this using determinants we note that by Eq. (A.34) a × b = 0 implies
det(a,b,c) = 0 for all vectors c,
which just means that a,b,c are dependent for all c. Since we can always choose c which is
linearly independent of a,b, we conclude that a × b = 0 implies that a and b are linearly
dependent or are proportional to each other.

Appendices
507
From the equations (a×b)·a = det(a,b,a) = 0 and (a×b)·b = det(a,b,b) = 0 we
see that a × b is perpendicular to both a and b.
Exercise
Show that
|a × b|2 = |a|2 + |b|2 −(a · b)2,
Hint
Write left side in terms of the components of a × b.
□
Using the above exercise we get
|a × b| =
p
|a|2|b|2 −|a|2|b|2 cosθ = |a| |b|sinθ,
where θ is the angle between a and b and equals the area of the parallelogram spanned by
a and b. Using the above exercise the square of the area A2 of the parallelogram spanned
by vectors a,b can be written elegently in terms of a determinant as
A2 = (a · a)(b · b) −(a · b)(b · a) =

a · a
a · b
b · a
b · b
·
(A.35)
The determinant appearing in this equation is called the Gram determinant of vectors a,b
and denoted Γ (a,b). It is clear from the derivation that
Γ (a,b) ≥0
for all vectors a,b and that equality holds only if a and b are linearly dependent.
We can derive a similar expression for the square of the volume V of a parallelopiped
spanned by three vectors a,b,c. This volume V is the product of the area A of one of its
faces multiplied by the corresponding altitude h. Choosing for A the area of the
parallelogram spanned by the vectors a and b, we get
V 2 = h2A2 = h2Γ (a,b) = h2

a · a
a · b
b · a
b · b
·
(A.36)
Let the vectors a,b,c be the position vectors of the points P1,P2,P3 respectively and let P
denote the foot of the perpendicular to the a,b plane dropped from P3. Then h in Eq. (A.36)
is the length of the vector d = −−→
P P 3 The position vector of the point P , say p, lies in the
a,b plane so that
p = λa + µb.
Hence, the vector d can be expressed as
d = c −p = c −λa −µb
(A.37)

508
Appendices
with suitable constants λ,µ. Since d is perpendicular to a,b plane, it must satisfy
a · d = 0 = b · d.
This leads to a system of linear equations for λ and µ:
λa · a + µa · b = a · c, λb · a + µb · b = b · c.
(A.38)
The determinant of these equations is just the Gram determinant Γ (a,b). Assuming a and
b to be independent vectors, (otherwise V = 0), we have Γ (a,b) , 0. There is, then, a
unique solution λ,µ to Eq. (A.38) and hence a unique vector d perpendicular to a,b plane
with initial point in that plane. The length of that vector is the required distance h so that,
by Eq. (A.37) and using orthogonality of d with vectors a and b, we have,
h2 = c · c −λc · a −µc · b.
This gives the volume V of the parallelopiped spanned by vectors a,b,c in terms of vectors
a,b,c as
V 2 = (c · c −λa · c −µb · c)Γ (a,b).
(A.39)
This expression can be written more elegently as the Gram determinant formed from the
vectors a,b,c:
V 2 =

a · a
a · b
a · c
b · a
b · b
b · c
c · a
c · b
c · c

= Γ (a,b,c).
(A.40)
We show the identity of Eqs (A.39) and (A.40) for V 2, using the fact that the value of the
determinant Γ (a,b,c) is unultered if we subtract from the last column λ times the ﬁrst
column and µ times the second column. Doing this and using Eq. (A.38) we get,
1Γ (a,b,c) =

a · a
a · b
0
b · a
b · b
0
c · a
c · b
c · c −λc · a −µc · b

·
(A.41)
Expanding this determinant in terms of the last column leads immediately to the expansion
in Eq. (A.39).
Equation (A.40) shows that the volume V of the parallelopiped spanned by the vectors
a,b,c does not depend on the choice of the face and of the corresponding altitude used in
the computation, because the value of Γ (a,b,c) does not change when we permute a,b,c.
For example, Γ (a,b,c) is invarient under the exchange of ﬁrst two rows and the ﬁrst two
columns.

Appendices
509
Equation (A.39) can be written as
Γ (a,b,c) = |d|2Γ (a,b).
It follows that
Γ (a,b,c) ≥0
for any vectors a,b,c. The equality sign can only hold if either Γ (a,b) = 0 or d = 0. The
ﬁrst of these equations implies that a and b are dependent. The second of these equations
would mean c = λa + µb so that c depends on a and b. Hence, the Gram determinant
vanishes if and only if the vectors a,b,c are dependent.
Our derivation of the expression for V 2 (Eq. (A.40)) is valid for any n-dimensional
space (n ﬁnite). If we restrict to 3-dimensional space, Eq. (A.40) follows immediately from
Eq. (A.31)
V 2 = det(a,b,c)det(a,b,c) = Γ (a,b,c).

B
Dirac Delta Function
Consider the vector valued function
f(r) = ˆr
r2
which blows up at the origin. We know that this function is proportional to the
electrostatic ﬁeld produced by a point charge at the origin. It is easy to see that at any
r , 0, the divergence of f, ∇· f, is zero:
∇· f = 1
r2
∂
∂r

r2 1
r2

= 1
r2
∂
∂r (1) = 0.
However, at r = 0 1
r2 blows up and

r2 1
r2

becomes indeterminate. Further, the surface
integral of f(r) over a sphere of radius R, centered at the origin, is
Z
f(r) · ds =
Z  ˆr
R2

· (R2 sinθdθdφˆr)
=
 Z π
0
sinθdθ
! Z 2π
0
dφ
!
= 4π.
Thus, the surface integral remains ﬁnite despite the singularity at the origin. Now, we
require on physical grounds that the electrostatic ﬁeld due to a point charge must obey the
divergence theorem. Hence, we must have
Z
∇· fdV = 4π
for any volume containing the origin. Since ∇· f = 0 everywhere except at r = 0, all the
contribution to this integral must come from ∇· f at the origin. Thus, ∇· f has the bizarre
property that it vanishes everywhere except at one point, the origin, and yet its integral over

Appendices
511
any volume containing that point is 4π. Such a behavior is not expected of any ordinary
function. The object required to salvage the situation can be constructed as follows. We
require the linear space D of inﬁnitely differentiable (C∞) and square integrable functions
φ : E3 7→R with compact support.1 Then the required object is the functional δ3(r) :
D 7→R deﬁned via
Z
V
φ(r)δ3(r)dV = φ(0) ∈R,
(B.1)
or, shifting the origin to a,
Z
V
φ(r)δ3(r −a)dV = φ(a) ∈R,
(B.2)
where we have assumed that the point 0 ∈V in the ﬁrst case, while the point a ∈V in the
second, failing which the corresponding integrals vanish. Taking φ(r) = 1 in Eq. (B.1)
we get,
Z
V
δ3(r)dV = 1.
(B.3)
Of course, all of the above three equations hold unconditionally, if all the integrals are over
all space. The functional δ3(r) deﬁned via the above three equations is an instance of a
mathematical structure called distributions, but is given the name ‘Dirac delta function’
after its inventor, P.A.M Dirac, although it is not a function in its usual sense.
Thus, the apperent paradox regarding the application of the divergence theorem to the
electrostatic ﬁeld due to a point charge at the origin is resolved if we recognize
∇·
 ˆr
r2

= 4πδ3(r),
(B.4)
so that
Z
∇·
 ˆr
r2

dV = 4π
Z
δ3(r)dV = 4π.
More generally,
∇·
 [
r −r′
|r −r′|2
!
= 4πδ3(r −r′),
(B.5)
where, the differentiation is with respect to r while r′ is held constant. Since
∇

1
|r −r′|

= −
 [
r −r′
|r −r′|2
!
,
(B.6)
1The support of a function is the set of points in its domain at which its value is different from zero and a set is said to be
compact if it is close and bounded.

512
Appendices
it follows that
∇2 
1
|r −r′|

= −4πδ3(r −r′).
(B.7)
In order to construct the delta function for one dimensional physical phenomena, we need
the linear space D of functions of a single variable which are continuously differentiable
at all orders, and have compact support. Then, the Dirac delta function is the functional
δ(x) : D 7→R deﬁned via
Z ∞
−∞
φ(x)δ(x)dx = φ(0),
(B.8)
and
Z ∞
−∞
φ(x)δ(x −a)dx = φ(a),
(B.9)
or, with φ(x) = 1,
Z ∞
−∞
δ(x)dx = 1.
(B.10)
The 3-D delta function δ3(r) and 1-D delta function δ(x) can be connected by evaluating
the integral over volume by successive evaluation of three single integrals.
Z
all space
δ3(r)dV =
Z ∞
−∞
Z ∞
−∞
Z ∞
−∞
δ(x)δ(y)δ(z)dxdydz = 1.
Thus, we can write
δ3(r) = δ(x)δ(y)δ(z).
(B.11)
Exercise!
Show that
δ(kx) = 1
|k|δ(x),
where k is any non-zero constant. (In particular, δ(−x) = δ(x).)
Solution
For φ(x) ∈D consider
Z ∞
−∞
φ(x)δ(kx)dx.

Appendices
513
We change the variables to y = kx giving x = (1/k)y and dx = dy/k. With this change
of variables we get
Z ∞
−∞
φ(x)δ(kx)dx = ±1
k
Z ∞
−∞
φ(y/k)δ(y)dy = 1
|k|φ(0),
where ± corresponds to k > 0 and k < 0 respectively, so that ±1
k can be replaced by 1
|k|. This
means
Z ∞
−∞
φ(x)δ(kx)dx =
Z ∞
−∞
φ(x)
 1
|k|δ(x)

dx.
This is the required result.
□
We can deﬁne the derivative of the delta function, denoted δ′(x), in the following way. For
φ(x) ∈D we write, integrating by parts,
Z ∞
−∞
φ(x)δ′(x)dx = φ(x)δ(x)

∞
−∞−
Z ∞
−∞
φ′(x)δ(x) = φ′(0),
as the ﬁrst term on the right vanishes because φ(x) has compact support and the prime
denotes diffrentiation with respect to x. Thus we get,
Z ∞
−∞
φ(x)δ′(x)dx = −φ′(0).
(B.12)
Exercise!
Consider the Heaviside function on R
H(x) =

1,
x ≥0
0,
x < 0
(B.13)
which deﬁnes the functional (distribution) on D by
TH(φ) =
Z ∞
−∞
H(x)φ(x)dx =
Z ∞
0
φ(x)dx.
Show that the delta function is the derivative of TH.
Solution
We again integrate by parts to get
T ′
H(φ) = −TH(φ′) = −
Z ∞
0
φ′(x)dx = φ(0) = δ(φ).
Note that φ(∞) = 0 because φ has compact support.
□
Exercise!
Prove the following properties of the delta function.
(i) δ′(x) = −δ′(−x).
(ii) xδ(x) = 0.

514
Appendices
(iii) xδ′(x) = −δ(x).
(iv) δ(x2 −a2) = (2a)−1[δ(x −a) + δ(x + a)], a > 0.
(v)
R
δ(a −x)δ(x −b)dx = δ(a −b).
(vi) f (x)δ(x −a) = f (a)δ(x −a).
Here, a prime denotes differentiation with respect to the argument.
□
There are various expressions involving limits and integrals which mimic delta function
and are called various representations of delta function. We do not deal with them because
we have not used them in this book. However these are very useful in many branches of
physics and can be found in standard text books on quantum mechanics (see e.g., [6]). The
standard reference on distributions is the book by Kesavan [14].

Bibliography
1. Ahlfors, L. V. 1979. Complex Analysis. New York: Tata McGraw-Hill.
2. Antia, H. M. 1991. Numerical Methods for Scientists and Engineers. New Delhi: Tata
McGraw-Hill Publishing Company.
3. Arnold, V. I. 1989. Mathematical Methods of Classical Mechanics. New York:
Sprienger-Verlag.
4. Ashcroft, N. W., and Mermin, N. D. 1976. Solid State Physics. Fort Worth: Harcourt
Brace College Publishers.
5. Courant, R., and John, F. 1974. Introduction to Calculus and Analysis. Vol. I & II.
New York: John Wiley and Sons.
6. Cohen-Tannoudji, C., Diu, B., and Laloe, F. 1991. Quantum Mechanics. Vol. I & II.
Wiley-VCH.
7. Doran, C. J. L., and Lasenby, A. N. 2003. Geometric Algebra for Physicists.
Cambridge: Cambridge University Press.
8. Fleisch D. 2011. A Student’s Guide to Vectors and Tensors. Cambridge: Cambridge
University Press.
9. Grifﬁths, D. J. 1999. Introduction to Electrodynamics. New Delhi: Prentice-Hall of
India Pvt. Ltd.
10. Hestenes, D. 1986. New Foundations for Classical Mechanics. Dordrecht: Kluwer
Academic Publishers.
11. Hestenes, D., Sobczyk, G. 1987. Clifford Algebra to Geometric Calculus: A Uniﬁed
Language for Mathematics and Physics. (Fundamental Theories of Physics).
Dordrecht: Springer.
12. Horn, R. A., and Johnson, C. R. 1985. Matrix Analysis. Vol. I & II. Cambridge:
Cambridge University Press.
13. Jackson, J. D. 1999. Classical Electrodynamics. New York: John Wiley and Sons.
14. Kesavan, S. 1989. Topics in Functional Analysis and Applications. New Delhi: Wiley.

516
Bibliography
15. Lang,
S.
1973.
Calculus
of
Several
Variables.
Reading,
Massachusetts:
Addison-Wesley.
16. Munk, W. H., and Macdonald, G. J. F. 1960. The Rotation of the Earth. Cambridge:
Cambridge University Press.
17. Rajaraman, V. 2009. Computer Oriented Numerical Methods. New Delhi: Prentice-
Hall of India.
18. Raju, C. K. 2007. Cultural Foundations of Mathematics: The Nature of Mathematical
Proof and the Transmission of Calculus from India to Europe in the 16th c. CE. Delhi:
Pearson Longman
19. Rana, N. C., and Joag, P. S. 1991. Classical Mechanics. New Delhi: Tata McGraw-Hill
Publishing Company Limited.
20. Rosenberg, C.B. Private communication.
21. Schey, H. M. 2005. Div, Grad, Curl, and all that: an informal text on Vector Calculus.
4th Ed. New York: W. W. Norton
22. Schwartz, M., Green, S. and Rutledge, W. A. 1960. Vector Analysis with Applications
to Geometry and Physics. New York: Harper & Brothers.
23. Shorter, L. R. 2014. Problems and Worked Solutions in Vector Analysis. Mineola,
New York: Dover Publications, Inc.
24. Stacey, F. D. 1969. Physics of the Earth. New York: John Wiley & Sons Inc.
25. Sudarshan, E. C. G., and N. Mukunda. 1974. Classical Dynamics: A Modern
Perspective. New York: Wiley.
26. Zwikker, C. 1950. Advanced Plane Geometry. Amsterdam: North Holland Publishing
Company.

Index
Acceleration 17
Active transformation 180
Addition of vectors 8
Adjoint of an operator 117
Admittance of an electrical circuit 106
Afﬁne equivalent 206
Afﬁne group 206
Afﬁne transformations 206
Analytic function 265
Angle 4
Angle between vectors 12
Angular momentum 38
Conservation of 468
Arc length parameterization 225
Axial vector 36
Baricentric coordinates 81
see also (Homogeneous coordinates) 81
Basis 21
Binormal 230
Boundary point 217
Boundary point of a set 217
Brachistochrone 258
Bravais lattice 43
Primitive cell of 43
C1-invertible 284
Cardinality of a set 216
Cardioid 255
Cauchy–Riemann conditions 484
Cauchy–Schwarz inequality 28, 33
Center of mass 14
Central conics 206
Central quadrics 209
Centripetal acceleration 233
Chain rule 263, 274
Change of basis 132
Chasles theorem 204
Circle of Apollonius 102
Circle of curvature 228
Circle transformation 108
Circular orbit 468
Closed set 217
Colinear 15
Collineations 206
Commuting operators 116
Components of a vector 21
Composition of rotations 179
Composition of symmetry elements 183
Conformal transformations 101
Congruence 205
Conic sections 90
Conicoid 91
Conics 206
Continuous functions 220
Continuous groups 197
Convergence of a sequence 217
Convergent sequence 218
Coordinate axes 18
Coordinate functions 225
Coordinate line 57
Coordinate lines 18
Coordinate planes 18
Coordinate surface 59
Coordinate system 18
Dextral 19
Left handed 19
Right handed 19
Coordinates of a vector 21

518
Index
Coplanar 15
Lines 6
Countable set 216
Cramer’s rule 123
Criterion for orthogonal vectors in a plane 98
Criterion for parallel vectors in a plane 98
Cross product
see also (Vector product) 32
Cross ratio 100
Curl in spherical polar coordinates
in cylindrical coordinates 321
Curl of a vector ﬁeld 306
Curvature 227
Cycloid 253
Cyclotron frequency 477
Darboux vector 233
Decomposition of vectors
see also (Resolution of vectors) 13
Determinant of an operator 119
Dextral coordinate system
see also (Right handed coordinate system) 18
Diagonalizable operator 134
Diameter of a set 217
Differential of a function 280
Dimension 4, 20
Direct lattice 44
Directance 75
Direction 4
Direction cosines 24
Directional derivative 266
Directions of tangent and normal 261
Directrix 91
Distance between vectors 28
Distributive property
Multiplication by scalars 10
Divergence in spherical polar coordinates
in cylindrical coordinates 321
divergence of a vector ﬁeld 304
Division ratio 80
Domain 215
Domain of an operator 116
Dot product 22
Drift velocity 482
Eccentric anomaly 248
Eccentricity 91
Eigenvalue 134
Eigenvector 134
Ellipsoid 147
Entire function 265
Epicycloid 254
Epitrochoids 258
Equipotential surface 283
Equivalence relation 205, 206
Euclidean group 199, 201
Euclidean metric 30
Euler angles 184
Euler’s theorem 188
Evolutes and involutes 243
Extended associative law 192
Extended inverse 192
Faithful representation 196
Finite set 216
Focus 91
Force 17
Frenet-Seret formulae 232
Gaussian fundamental quantities 301
General (complex) equation to a circle 105
Geodesic 30
Geometric calculus 273
Gradient in spherical polar coordinates
in cylindrical coordinates 321
Gradient of a potential 282
Gram determinant 42, 407
Group (deﬁnition) 191
Group of isometries 199
Harmonic oscillator 450
Hodograph 456
Homogeneous coordinates
see also (Baricentric coordinates) 81
Homomorphism 45
Hyperboloid of one sheet 147
Hyperboloid of two sheets 147
Hypocycloid 256
Hypotrochoids 258
Identity operator 117
Image set 117, 215
Impedance of an electrical circuit 106
Implicit function theorem 285
Implicit functions 283, 285
Inﬁnite set 216
Inﬁnitesimal rotations 171
Inner product 22
Interior point 217
Interior point of a set 217

Index
519
Intrinsic equation of a space curve 240
Inverse
Multiplicative 23
Inverse mapping theorem 284
Inverse maps 283
Inverse of a matrix 495
Inverse of a vector
Additive 9
Inverse operator 118
Inversion 103
Invertible operator 118
Isomorphism 45
Jacobian determinant 277, 285, 287, 293
Jacobian matrix 277, 279, 285
Jerk 233
Kepler’s equation 253
Kepler’s third law 468
Kronecker delta 24
Laplacian in spherical polar coordinates
in cylindrical coordinates 321
Law of cosines 11, 26
Law of sines 12, 35
left handed coordinate system 18
Levi-Civita symbols 44, 48
Limit of a converging sequence 218
Line element 302
Line element on the torus 304
Linear combination of vectors 11
Linear form 497
Linear momentum 38
Linear operator 115
Linear space 47
Linear transformation 115
Linearly dependent vectors 20
Linearly independent vectors 20
Lissajous ﬁgure 455
Lissajous motion 249
Matrices 129
Matrices representing rotations 176
Matrix representing an operator 129
Method of successive approximations 253
Method of successive approximations 289
Metric space 47
Mobius rules 100
Mohr’s algorithm
Graphical implementation 150
Mohr’s algorithm 147
Mohr’s circle 150
Moment of a line 76
Moment of force 38
Moving trihedral 231
Natural equation of a space curve 240
Negatively oriented triplet 68
Non-coplanar
Vectors 15
Non-coplanar lines 6
Non-singular operators 121
Norm of a vector 22
Normal plane 236
One to one correspondence 118
One to one map or function 118, 215
Onto map or function 118, 215
Open set 217
Order of contact 238
Orientation of a plane 72
Orientation relative to a coordinate system 69
Oriented volume 40
Orthogonal operator 158
Orthogonal transformations 158
Orthogonal vectors 12
Orthonormal basis 21
Osculating circle 228, 239
Osculating plane 236
Parabolic coordinates 61
Parameterization of a curve 225
Parametric representation of a sphere 298
Partial derivative 267
Passive transformation 181
Plane
Equation in normal form 85
Plane wave 85
Planes 83
Point function 67
point of division 80
Polar coordinates 26
Polar Decomposition Theorem 166
Polar vector 36
Position vectors 7
Positive and negative sides of a curve 260
Positively oriented triplet 68
Potential of a vector ﬁeld 282
Principal axes 135
Principal axes system 135

520
Index
Principal normal 227
principal values 167
principal vectors 167
Product of two linear operators 116
Product rule 274
Projectile motion 456
Pseudo-scalar 40
Pseudo-vector 36
Quadratic differential form 302
r-neighborhood of a point 217
Radian measure 4
Radians 89
Radius of a set 217
Radius of curvature 227
Range 215
Range of an operator 116
Reciprocal frames 124
Reciprocal lattice 44
Reciprocal lattice of a crystal 43
Rectifying plane 236
Reﬂection
Orthogonal operator for 161
Reﬂection operator
Canonical form of 161
Regular point 225
Representation of a group 196
Resolution of vectors
see also (Decomposition of vectors) 13
Right handed coordinate system
see also (Dextral coordinate system) 18
Rigid body 38
Rotation
as an orthogonal transformation 158
Counterclockwise 5
Rotation group 196
Scalar 3
Scalar integration 263
Scalar product 22
Distributive property 25
Scalar product as potential energy 30
Scalar triple product 39
Sequence 217
Shear 168
Sign of curvature 262
Similar ﬁgures 205
Similarities 206
Similarity transformation 205
Similarity transformations 133
Skewsymmetric operator 126
Space 4
Spectral decomposition 143
Spectral form 143
Spheres 89
Spherical triangle 89
Stereographic image 298
Stereographic projection 298
Straight lines 74
Stram line 484
Surfaces, parametric representation of 297
Symmetric matrix 155
Symmetric operator 126, 141
Symmetry 181
Symmetry breaking 183
Symmetry element 181
Tautochrone 258
Taylor series 264, 278
Tensor 49, 115
Tesserals 245
Topocentric frame 462
Torous, parametric representation of 303
Torque 38
Torsion 231
Trochoids 258
True anomaly 248
Uncountable set 216
Variation on a curve 281
Vector ﬁelds 67
Vector identities 52
Vector product
see also (Cross product) 32
Vector triple product 45
Velocity space 456

