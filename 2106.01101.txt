Learning a Single Neuron with Bias Using Gradient Descent
Gal Vardi* , Gilad Yehudai* , and Ohad Shamir
Weizmann Institute of Science
{gal.vardi,gilad.yehudai,ohad.shamir}@weizmann.ac.il
Abstract
We theoretically study the fundamental problem of learning a single neuron with a bias term (x 7→
σ(⟨w, x⟩+ b)) in the realizable setting with the ReLU activation, using gradient descent. Perhaps sur-
prisingly, we show that this is a signiﬁcantly different and more challenging problem than the bias-less
case (which was the focus of previous works on single neurons), both in terms of the optimization ge-
ometry as well as the ability of gradient methods to succeed in some scenarios. We provide a detailed
study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and
providing positive convergence guarantees under different sets of assumptions. To prove our results,
we develop some tools which may be of independent interest, and improve previous results on learning
single neurons.
1
Introduction
Learning a single ReLU neuron with gradient descent is a fundamental primitive in the theory of deep
learning, and has been extensively studied in recent years. Indeed, in order to understand the success of
gradient descent on complicated neural networks, it seems reasonable to expect a satisfying analysis of
convergence on a single neuron. Although many previous works studied the problem of learning a single
neuron with gradient descent, none of them considered this problem with an explicit bias term.
In this work, we study the common setting of learning a single neuron with respect to the squared loss,
using gradient descent. We focus on the realizable setting, where the inputs are drawn from a distribution D
on Rd+1, and are labeled by a single target neuron of the form x 7→σ(⟨v, x⟩), where σ : R →R is some
non-linear activation function. To capture the bias term, we assume that the distribution D is such that its
ﬁrst d components are drawn from some distribution ˜D on Rd, and the last component is a constant 1. Thus,
the input x can be decomposed as (˜x, 1) with ˜x ∼˜D, the vector v can be decomposed as (˜v, bv), where
˜v ∈Rd and bv ∈R, and the target neuron computes a function of the form x 7→σ(⟨˜v, ˜x⟩+ bv). Similarly,
we can deﬁne the learned neuron as x 7→σ(⟨˜w, ˜x⟩+ bw), where w = ( ˜w, bw). Overall, we can write the
objective function we wish to optimize as follows:
F(w) :=
E
x∼D
1
2

σ(w⊤x) −σ(v⊤x)
2
(1)
=
E
˜x∼˜D
1
2

σ( ˜w⊤˜x + bw) −σ(˜v⊤˜x + bv)
2
.
(2)
*equal contribution
1
arXiv:2106.01101v1  [cs.LG]  2 Jun 2021

Throughout the paper we consider the commonly used ReLU activation function: σ(x) = max{0, x}.
Although the problem of learning a single neuron is well studied (e.g. [16, 21, 5, 4, 9, 17, 12, 13]), none
of the previous works considered the problem with an additional bias term. Moreover, previous works on
learning a single neuron with gradient methods have certain assumptions on the input distribution D, which
do not apply when dealing with a bias term (for example, a certain ”spread” in all directions, which does not
apply when D is supported on {1} in the last coordinate).
Since neural networks with bias terms are the common practice, it is natural to ask how adding a bias
term affects the optimization landscape and the convergence of gradient descent. Although one might con-
jecture that this is just a small modiﬁcation to the problem, we in fact show that the effect of adding a bias
term is very signiﬁcant, both in terms of the optimization landscape and in terms of which gradient descent
strategies can or cannot work. Our main contributions are as follows:
• We start in Section 3 with some negative results, which demonstrate how adding a bias term makes the
problem more difﬁcult. In particular, we show that with a bias term, gradient descent or gradient ﬂow1
can sometimes fail with probability close to half over the initialization, even when the input distribution
is uniform over a ball. In contrast, [21] show that without a bias term, for the same input distribution,
gradient ﬂow converges to the global minimum with probability 1.
• In Section 4 we give a full characterization of the critical points of the loss function. We show that adding
a bias term changes the optimization landscape signiﬁcantly: In previous works (cf. [21]) it has been
shown that under mild assumptions on the input distribution, the only critical points are w = v (i.e., the
global minimum) and w = 0. We prove that when we have a bias term, the set of critical points has a
positive measure, and that there is a cone of local minima where the loss function is ﬂat.
• In Sections 5 and 6 we show that gradient descent converges to the global minimum at a linear rate, under
some assumptions on the input distribution and on the initialization. We give two positive convergence
results, where each result is under different assumptions, and thus the results complement each other.
We also use different techniques for proving each of the results: The analysis in Section 6 follows from
some geometric arguments and extends the technique from [21, 5]. The analysis in Section 5 introduces a
novel technique, not used in previous works on learning a single neuron, and has a more algebraic nature.
Moreover, that analysis implies that under mild assumptions, gradient descent with random initialization
converges to the global minimum with probability 1 −eΩ(d).
• The best known result for learning a single neuron without bias using gradient descent for an input distri-
bution that is not spherically symmetric, establishes convergence to the global minimum with probability
close to 1
2 over the random initialization [21, 5]. With our novel proof technique presented in Section 5
this result can be improved to probability at least 1 −eΩ(d) (see Remark 5.7).
Related work
Although there are no previous works on learning a single neuron with an explicit bias term, there are many
works that consider the problem of a single neuron under different settings and assumptions.
Several papers showed that the problem of learning a single neuron can be solved under minimal assump-
tions using algorithms which are not gradient-based (such as gradient descent or SGD). These algorithms
1I.e., gradient descent with inﬁnitesimal step size.
2

include the Isotron proposed by [8] and the GLMtron proposed by [7]. The GLMtron algorithm is also ana-
lyzed in [3]. These algorithms allow learning a single neuron with bias. We note that these are non-standard
algorithms, whereas we focus on standard gradient based methods.
In [12] the authors study the empirical risk of the single neuron problem. However, their analysis does
not include the ReLU activation, or adding a bias term. A related analysis is also given in [13], where the
ReLU activation is not considered.
Several papers showed convergence guarantees for the single neuron problem with ReLU activation
under certain distributional assumptions, although none of these assumptions allows for a bias term. Notably,
[18, 16, 9, 1] showed convergence guarantees for gradient methods when the inputs have a standard Gaussian
distribution, without a bias term. [4] showed that under a certain subspace eigenvalue assumption a single
neuron can be learned with SGD, although this assumption does not allow adding a bias term. [21, 5] use an
assumption about the input distribution being sufﬁciently ”spread” in all directions, which does not allow for
a bias term (since that requires an input distribution supported on {1} in the last coordinate). [21] showed a
convergence result under the realizable setting, while [5] considered the agnostic and noisy settings. In [17]
convergence guarantees are given for the absolute value activation, and a speciﬁc distribution which does
not allow a bias term.
Less directly related, [19] studied the problem of implicit regularization in the single neuron setting.
In [20, 10] it is shown that approximating a single neuron using random features (or kernel methods) is
not tractable in high dimensions. We note that these results explicitly require that the single neuron which
is being approximated will have a bias term. Thus, our work complements these works by showing that
the problem of learning a single neuron with bias is also learnable using gradient descent (under certain
assumptions). Agnostically learning a single neuron with non gradient-based algorithms was studied in
[3, 6].
2
Preliminaries
Notations.
We use bold-faced letters to denote vectors, e.g., x = (x1, . . . , xd). For u ∈Rd we denote
by ∥u∥the Euclidean norm. We denote ¯u =
u
∥u∥, namely, the unit vector in the direction of u. For
1 ≤i ≤j ≤d we denote ui:j = (ui, . . . , uj) ∈Rj−i+1. We denote by 1(·) the indicator function, for
example 1(t ≥5) equals 1 if t ≥5 and 0 otherwise. We denote by U([−r, r]) the uniform distribution over
the interval [−r, r] in R, and by N(0, Σ) the multivariate normal distribution with mean 0 and covariance
matrix Σ. Given two vectors w, v we let θ(w, v) = arccos

⟨w,v⟩
∥w∥∥v∥

= arccos(⟨¯w, ¯v⟩) ∈[0, π]. For a
vector u ∈Rd+1 we often denote by ˜u ∈Rd the ﬁrst d components of u, and denote by bu ∈R its last
component.
Gradient methods.
In this paper we focus on the following two standard gradient methods for optimizing
our objective F(w) from Eq. (2):
• Gradient descent: We initialize at some w0 ∈Rd+1, and set a ﬁxed learning rate η > 0. At each iteration
t ≥0 we have: wt+1 = wt −η∇F(wt).
• Gradient Flow: We initialize at some w(0) ∈Rd+1, and for every time t ≥0, we set w(t) to be the
solution of the differential equation ˙w = −∇F(w(t)). This can be thought of as a continuous form of
gradient descent, where the learning rate is inﬁnitesimally small.
3

The gradient of the objective in Eq. (1) is:
∇F(w) = E
x∼D
h
σ(w⊤x) −σ(v⊤x)

· σ′(w⊤x)x
i
.
(3)
Since σ is the ReLU function, it is differentiable everywhere except for 0. Practical implementations of
gradient methods deﬁne σ′(0) to be some constant in [0, 1]. Following this convention, the gradient used by
these methods still correspond to Eq. (3). We note that the exact value of σ′(0) has no effect on our results.
3
Negative results
In this section we demonstrate that adding bias to the problem of learning a single neuron with gradient
descent can make the problem signiﬁcantly harder.
First, on an intuitive level, previous results (e.g., [21, 5, 16, 4, 17]) considered assumptions on the input
distribution, which require enough ”spread” in all directions (for example, a strictly positive density in some
neighborhood around the origin). Adding a bias term, even if the ﬁrst d coordinates of the distribution satisfy
a ”spread” assumption, will give rise to a direction without ”spread”, since in this direction the distribution
is concentrated on 1, hence the previous results do not apply.
Next, we show two negative results where the input distribution is uniform on a ball around the origin.
We note that due to Theorem 6.4 from [21], we know that gradient ﬂow on a single neuron without bias will
converge to the global minimum with probability 1 over the random initialization. The only case where it
will fail to converge is when w0 is initialized in the exact direction −v, which happens with probability 0
with standard random initializations.
3.1
Initialization in a ﬂat region
If we initialize the bias term in the same manner as the other coordinates, then we can show that gradient
descent will fail with probability close to half, even if the input distribution is uniform over a (certain)
origin-centered ball:
Theorem 3.1. Suppose we initialize each coordinate of w0 (including the bias) according to U([−1, 1]).
Let ϵ > 0 and let ˜D be the uniform distribution supported on a ball around the origin in Rd of radius ϵ.
Then, w.p > 1/2 −ϵ
√
d, gradient descent on the objective in Eq. (2) satisﬁes wt = w0 for all t (namely, it
gets stuck at its initial point w0).
Note that by Theorem 6.4 in [21], if there is no bias term in the objective, then gradient descent will
converge to the global minimum w.p 1 using this random initialization scheme and this input distribution.
The intuition for the proof is that with constant probability over the initialization, bw is small enough so that
σ( ˜w⊤˜x+bw) = 0 almost surely. If this happens, then the gradient will be 0 and gradient descent will never
move. The full proof can be found in Appendix B.1.
3.2
Targets with negative bias
Theorem 3.1 shows a difference between learning with and without the bias term. A main caveat of this
example is the requirement that the bias is initialized in the same manner as the other parameters. Standard
deep learning libraries (e.g. Pytorch [14]) often initialize the bias term to zero by default, while using
random initialization schemes for the other parameters.
4

Alas, we now show that even if we initialize the bias term to be exactly zero, and the input distribution is
uniform over an arbitrary origin-centered ball, we might fail to converge to the global minimum for certain
target neurons:
Theorem 3.2. Let ˜D be the uniform distribution on B = {˜x ∈Rd : ∥˜x∥≤r} for some r > 0. Let
v ∈Rd+1 such that ˜v = (1, 0, . . . , 0)⊤and bv = −
 r −
r
2d2

. Let w0 ∈Rd+1 such that bw0 = 0 and ˜w0
is drawn from the uniform distribution on a sphere of radius ρ > 0. Then, with probability at least 1
2 −od(1)
over the choice of w0, gradient ﬂow does not converge to the global minimum.
We prove the theorem in Appendix B.2. The intuition behind the proof is the following: The target
neuron has a large negative bias, so that only a small (but positive) measure of input points are labelled
as non-zero. By randomly initializing ˜w, with probability close to 1
2 there are no inputs that both v and
w label positively. Since the gradient is affected only by inputs that w labels positively, then during the
optimization process the gradient will be independent of the direction of v, and w will not converge to the
global minimum.
Remark 3.3. Theorem 3.2 shows that gradient ﬂow is not guaranteed to converge to a global minimum
when bv is negative, instead it converges to a local minimum with a loss of F(0). However, the loss F(0) is
determined by the input distribution. Take v =
 1, 0, . . . , 0, −
 r −
r
2d2
⊤considered in the theorem. On
one hand, for a uniform distribution on a ball of radius r as in the theorem we have:
F(0) = 1
2 · E
x

σ(v⊤x)
2
= 1
2 · E
x

1(v⊤x ≥0)

v⊤x
2
= 1
2 · E
x

1

x1 ≥r −
r
2d2
 
x1 −

r −
r
2d2
2
≤1
2 · r2
4d4 · Pr
x

x1 ≥r

1 −
1
2d2

≤r2e−Ω(d) .
Thus, for any reasonable r, a local minimum with loss F(0) is almost as good as the global minimum. On
the other hand, take a distribution ˜D with a support bounded in a ball of radius r, such that half of its mass
is uniformly distributed in A :=
˜x ∈Rd : x1 > r −
r
4d2
	
, and the other half is uniformly distributed in
B \ A. In this case, it is not hard to see that the same proof as in Theorem 3.2 works, and gradient ﬂow will
converge to a local minimum with loss F(0) = Ω
  r
d2

, which is arbitrarily large if r is large enough.
Although in the example given in Theorem 3.2 the objective at w = 0 is almost as good as the objective
at w = v, we emphasize that w.p almost 1
2 gradient ﬂow cannot reach the global minimum even asymp-
totically. On the other hand, in the bias-less case by Theorem 6.4 in [21] gradient ﬂow on the same input
distribution will reach the global minimum w.p 1. Also note that the scale of the initialization of w0 has no
effect on the result.
4
Characterization of the critical points
In the previous section we have shown two examples where gradient methods on the problem of a single
neuron with bias will either get stuck in a ﬂat region, or converge to a local minimum. In this section we
delve deeper into the examples presented in the previous section, and give a full characterization of the
critical points of the objective. We will use the following assumption on the input distribution:
5

Assumption 4.1. The distribution ˜D on Rd has a density function p(˜x), and there are β, c > 0, such that ˜D
is supported on {˜x : ∥˜x∥≤c}, and for every ˜x in the support we have p(˜x) ≥β.
The assumption essentially states that the distribution over the ﬁrst d coordinates (without the bias term)
has enough ”spread” in all directions, and covers standard distributions such as uniform over a ball of radius
c. Other similar assumptions are made in previous works (e.g. [21, 5]). We note that in [21] it is shown
that without any assumption on the distribution, it is impossible to ensure convergence, hence we must have
some kind of assumption for this problem to be learnable with gradient methods. Under this assumption we
can characterize the critical points of the objective.
Theorem 4.2. Consider the objective in Eq. (2) with v ̸= 0, and assume that the distribution ˜D on the ﬁrst
d coordinates satisﬁes Assumption 4.1. Then w ̸= 0 is a critical point of F (i.e., is a root of Eq. (3)) if and
only if it satisﬁes one of the following:
• w = v, in which case w is a global minimum.
• w = ( ˜w, bw) where ˜w = 0 and bw < 0.
• ˜w ̸= 0 and −bw
∥˜w∥≥c.
In the latter two cases, F(w) = F(0). Hence, if 0 is not a global minimum, then w is not a global minimum.
We note that F(0) = 1
2 Ex[σ(v⊤x)2], so 0 is a global minimum only if the target neuron returns 0 with
probability 1.
Remark 4.3 (The case w = 0). We intentionally avoided characterizing the point w = 0, since the
objective is not differentiable there (this is the only point of non-differentiability), and the gradient there is
determined by the value of the ReLU activation at 0. For σ′(0) = 0 the gradient at w = 0 is zero, and this
is a non-differentiable saddle point. For σ′(0) = 1 (or any other positive value), the gradient at w = 0 is
non-zero, and it will point at a direction which depends on the distribution. We note that in [16] the authors
deﬁne σ′(0) = 1, and use a symmetric distribution, in which case the gradient at w = 0 points exactly at
the direction of the target v. This is a crucial part of their convergence analysis.
We emphasize that with a bias term, there is a non-zero measure manifold of critical points (correspond-
ing to the third bullet in the theorem). On the other hand, without a bias term the only critical point besides
the global minimum (under mild assumptions on the input distribution) is at the origin w = 0 (cf. [21]).
The full proof is in Appendix C.
The assumption on the support of ˜D is made for simplicity. It can be relaxed to having a distribution with
exponentially bounded tail, e.g. standard Gaussian. In this case, some of the critical points will instead have
a non-zero gradient which is exponentially small. We emphasize that when running optimization algorithms
on ﬁnite-precision machines, which are used in practice, these ”almost” critical points behave essentially
like critical points since the gradient is extremely small.
Revisiting the negative examples from Section 3, the ﬁrst example (Theorem 3.1) shows that if we do
not initialize the bias of w to zero, then there is a positive probability to initialize at a critical point which
is not the global minimum. The second example (Theorem 3.2) shows that even if we initialize the bias
of w to be zero, there is still a positive probability to converge to a critical point which is not the global
minimum. Hence, in order to guarantee convergence we need to have more assumptions on either the input
distribution, the target v or the initialization. In the next section, we show that adding such assumptions are
indeed sufﬁcient to get positive convergence guarantees.
6

5
Convergence for initialization with loss slightly better than trivial
In this section, we show that under some assumptions on the input distribution, if gradient descent is initial-
ized such that F(w0) < F(0) then it is guaranteed to converge to the global minimum. In Subsection 5.2,
we study under what conditions this is likely to occur with standard random initialization.
5.1
Convergence if F(w0) < F(0)
To state our results, we need the following assumption:
Assumption 5.1.
1. The distribution D is supported on {x ∈Rd+1 : ∥x∥≤c} for some c ≥1.
2. The distribution ˜D over the ﬁrst d coordinates is bounded in all directions: there is c′ > 0 such that for
every ˜u with ∥˜u∥= 1 and every a ∈R and b ≥0, we have Pr˜x∼˜D
˜u⊤˜x ∈[a, a + b]

≤b · c′.
3. We assume w.l.o.g. that ∥v∥= 1 and c′ ≥1.
Assumption (3) helps simplifying some expressions in our convergence result, and is not necessary.
Assumption (2) requires that the distribution is not too concentrated in a short interval. For example, if ˜D is
spherically symmetric then the marginal density of the ﬁrst (or any other) coordinate is bounded by c′. Note
that we do not assume that ˜D is spherically symmetric.
Theorem 5.2. Under Assumption 5.1 we have the following. Let δ > 0 and let w0 ∈Rd+1 such that
F(w0) ≤F(0) −δ. Let γ =
δ3
3·122(∥w0∥+2)3c8c′2 . Assume that gradient descent runs starting from w0 with
step size η ≤γ
c4 . Then, for every t we have
∥wt −v∥2 ≤∥w0 −v∥2 (1 −γη)t .
The formal proof appears in Appendix D, but we provide the main ideas below. First, note that
∥wt+1 −v∥2 = ∥wt −η∇F(wt) −v∥2
= ∥wt −v∥2 −2η⟨∇F(wt), wt −v⟩+ η2∥∇F(wt)∥2 .
Hence, in order to show that ∥wt+1 −v∥2 ≤∥wt −v∥2 (1 −γη) we need to obtain an upper bound for
∥∇F(wt)∥and a lower bound for ⟨∇F(wt), wt −v⟩. Achieving the lower bound for ⟨∇F(wt), wt −v⟩is
the challenging part, and we show that in order to establish such a bound it sufﬁces to obtain a lower bound
for Prx

w⊤
t x ≥0, v⊤x ≥0

. We prove that if F(wt) ≤F(0) −δ then Prx

w⊤
t x ≥0, v⊤x ≥0

≥
δ
c2∥wt∥. Hence, if F(wt) remains at most F(0) −δ for every t, then a lower bound for ⟨∇F(wt), wt −v⟩
can be achieved, which completes the proof. However, it is not obvious that F(wt) remains at most F(0)−δ
throughout the training process. When running gradient descent on a smooth loss function we can choose
a sufﬁciently small step size such that the loss decreases in each step, but here the function F(w) is highly
non-smooth around w = 0. That is, the Lipschitz constant of ∇F(w) is unbounded. We show that if
F(wt) ≤F(0) −δ then wt is sufﬁciently far from 0, and hence the smoothness of F around wt can be
bounded, which allows us to choose a small step size that ensures that F(wt+1) ≤F(wt) ≤F(0) −δ.
Hence, it follows that F(wt) remains at most F(0) −δ for every t.
As an aside, recall that in Section 4 we showed that other than w = v all critical points of F(w) are in
a ﬂat region where F(w) = F(0). Hence, the fact that F(wt) remains at most F(0) −δ for every t implies
7

that wt does not reach the region of bad critical points, which explains the asymptotic convergence to the
global minimum.
We also note that although we assume that the distribution has a bounded support, this assumption is
mainly made for simplicity, and can be relaxed to have sub-Gaussian distributions with bounded moments.
These distributions include, e.g. Gaussian distributions.
5.2
Convergence for Random Initialization
In Theorem 5.2 we showed that if F(w0) < F(0) then gradient descent converges to the global minimum.
We now show that under mild assumptions on the input distribution, a random initialization of w0 near zero
satisﬁes this requirement. We will need the following assumption, also used in [21, 5]:
Assumption 5.3. There are α, β > 0 s.t the distribution ˜D satisﬁes the following: For any vector ˜w ̸= ˜v,
let ˜D ˜w,˜v denote the marginal distribution of ˜D on the subspace spanned by ˜w, ˜v (as a distribution over R2).
Then any such distribution has a density function p ˜w,˜v(ˆx) over R2 such that inf ˆx:∥ˆx∥≤α p ˜w,˜v(ˆx) ≥β.
The main technical tool for proving convergence under random initialization is the following:
Theorem 5.4. Assume that the input distribution D is supported on {x ∈Rd+1 : ∥x∥≤c} for some
c ≥1, and Assumption 5.3 holds. Let v ∈Rd+1 such that ∥v∥= 1 and −bv
∥˜v∥≤α ·
sin( π
8 )
4
. Let
M =
α4β sin3( π
8 )
256c
. Let w ∈Rd+1 such that bw = 0, θ( ˜w, ˜v) ≤3π
4 and ∥w∥< 2M
c2 . Then, F(w) ≤
F(0) + ∥w∥2 · c2
2 −∥w∥· M < F(0).
We prove the theorem in Appendix F. The main idea is that since
F(w) = E
x
1
2

σ(w⊤x) −σ(v⊤x)
2
= F(0) + 1
2 E
x

σ(w⊤x)
2
−E
x
h
σ(w⊤x)σ(v⊤x)
i
≤F(0) + ∥w∥2 · c2
2 −∥w∥· E
x
h
σ( ¯w⊤x)σ(v⊤x)
i
,
then it sufﬁces to obtain a lower bound for Ex

σ( ¯w⊤x)σ(v⊤x)

. In the proof we show that such a bound
can be achieved if the conditions of the theorem hold.
Suppose that w0 is such that ˜w0 is drawn from a spherically symmetric distribution and bw0 = 0. By
standard concentration of measure arguments, it holds w.p. at least 1 −eΩ(d) that θ( ˜w0, ˜v) ≤3π
4 (where
the notation Ω(d) hides only numerical constants, namely, it does not depend on other parameters of the
problem). Therefore, if ˜w0 is drawn from the uniform distribution on a sphere of radius ρ < 2M
c2 , then
the theorem implies that w.h.p. we have F(w0) < F(0). For such initialization Theorem 5.2 implies
that gradient descent converges to the global minimum. For example, for ρ =
M
c2 we have w.h.p. that
F(w0) ≤F(0) + ρ2c2
2
−ρM = F(0) −M2
2c2 , and thus Theorem 5.2 applies with δ = M2
2c2 . Thus, we have
the following corollary:
Corollary 5.5. Under Assumption 5.1 and Assumption 5.3 we have the following. Let M =
α4β sin3( π
8 )
256c
, let
ρ = M
c2 , let δ = M2
2c2 , and let γ =
δ3
3·122(ρ+2)3c8c′2 . Suppose that −bv
∥˜v∥≤α ·
sin( π
8 )
4
, and w0 is such that
bw0 = 0 and ˜w0 is drawn from the uniform distribution on a sphere of radius ρ. Consider gradient descent
8

with step size η ≤γ
c4 . Then, with probability at least 1 −eΩ(d) over the choice of w0 we have for every t:
∥wt −v∥2 ≤∥w0 −v∥2 (1 −γη)t .
We note that a similar result holds also if ˜w0 is drawn from a normal distribution N(0, ρ2
d I).
Remark 5.6 (The assumption on bv). The assumption −bv
∥˜v∥≤α ·
sin( π
8 )
4
implies that the bias term bv may
be either positive or negative, but in case it is negative then it cannot be too large. This assumption is indeed
crucial for the proof, but for ”well-behaved” distributions, if this assumption is not satisﬁed (for a large
enough α), then the loss at F(0) is already good enough. For example, for a standard Gaussian distribution
and for every ϵ > 0, we can choose α large enough such that for any bias term (positive or negative) we
either: (1) converge to the global minimum with a loss of zero, or; (2) converge to a local minimum with a
loss of F(0), which is smaller then ϵ. Moreover, we can show that by choosing α appropriately, and using
the example in Theorem 3.2, if −bv
∥˜v∥≥2α then gradient ﬂow will converge to a non-global minimum with
loss of F(0). This means that our bound on α is tight up to a constant factor. For a further discussion on
the assumption on bv, and how to choose α see Appendix G.
Previous papers have shown separation between random features (or kernel) methods and neural net-
works in terms of their approximation power (see [20, 10], and the discussion in [11]). These works show
that under a standard Gaussian distribution, random features cannot even approximate a single ReLU neu-
ron, unless the number of features is exponential in the input dimension. That analysis crucially relies on the
single neuron having a non-zero bias term. In this work we complete the picture by showing that gradient
descent can indeed ﬁnd a near-optimal neuron with non-zero bias. Thus, we see there is indeed essentially
a separation between what can be learned using random features and using gradient descent over neural
networks.
Remark 5.7 (Learning a neuron without bias). [21] studied the problem of learning a single ReLU neuron
without bias using gradient descent on a single neuron without bias. For input distributions that are not
spherically symmetric they showed that gradient descent with random initialization near zero converges to
the global minimum w.p. at least 1
2 −od(1). Their result is also under Assumption 5.3. An immediate
corollary from the discussion above is that if we learn a single neuron without bias using gradient descent
with random initialization on a single neuron with bias, then the algorithm converges to the global minimum
w.p. at least 1−eΩ(d). Moreover, our proof technique can be easily adapted to the setting of learning a single
neuron without bias using gradient descent on a single neuron without bias, namely, the setting studied in
[21]. It can be shown that in this setting gradient descent converges w.h.p to the global minimum. Thus, our
technique allows us to improve the result of [21] from probability 1
2 −od(1) to probability 1 −eΩ(d).
6
Convergence for spread and symmetric distributions
In this section we show that under a certain set of assumptions, different from the assumptions in Section 5,
it is possible to show linear convergence of gradient descent to the global minimum. The assumptions we
make for this theorem are as follows:
Assumption 6.1.
1. The target vector v satisﬁes that bv ≥0 and ∥˜v∥= 1.
2. The distribution ˜D over the ﬁrst d coordinates is spherically symmetric.
9

3. Assumption 5.3 holds, and denoting by τ := E˜x∼˜
D[|˜x1˜x2|]
E˜x∼˜
D[˜x2
1] , then α ≥2.5
√
2 · max
n
1,
1
√τ
o
where α is
from Assumption 5.3.
4. Denote by c := E˜x∼˜D

∥˜x∥4
, then c < ∞.
Under these assumptions, we prove the following theorem:
Theorem 6.2. Assume we initialize w0 such that ∥w0 −v∥2 < 1, bw0 ≥0 and that Assumption 6.1
holds. Then, there is a universal constant C, such that using gradient descent on F(w) with step size
η < C ·
β
cα2 min{1, τ} yields that for every t we have ∥wt −v∥2 ≤(1 −ηλ)t∥w0 −v∥2 , for λ = C ·
β
cα2 .
This result has several advantages and disadvantages compared to those of the previous section. The
main disadvantage is that the assumptions are generally more stringent: We focus only on positive target
biases (bv ≥0) and spherically symmetric distributions ˜D. Also we require a certain technical assumption
on the the distribution, as speciﬁed by τ, which are satisﬁed for standard spherically symmetric distributions,
but is a bit non-trivial2. Finally, the assumption on the initialization (∥w0 −v∥2 < 1 and bw0 ≥0) is much
more restrictive (although see Remark 6.3 below). In contrast, the initialization assumption in the previous
section holds with probability close to 1 with random initialization. On the positive side, the convergence
rate does not depend on the initialization, i.e., here by initializing with any w0 such that ∥w0 −v∥2 < 1
and bw0 ≥0, we get a convergence rate that only depends on the input distribution. On the other hand, in
Theorem 5.2, the convergence rate depends on the parameter δ which depends on the initialization. Also,
the distribution is not necessarily bounded – we only require its fourth moment to be bounded.
Remark 6.3 (Random initialization). For bv = 0 the initialization assumption (∥w0 −v∥2 < 1) is satisﬁed
with probability close to 1/2 with standard initializations, see Lemma 5.1 from [21]). For bv > 0, a similar
argument applies if bw is initialized close enough to bv.
The proof of the theorem is quite different from the proofs in Section 5, and is more geometrical in
nature, extending previously used techniques from [21, 5]. It contains two major parts: The ﬁrst part is an
extension of the methods from [21] to the case of adding a bias term. Speciﬁcally, we show a lower bound
on ⟨∇F(w), w −v⟩, which depends on both the angle between ˜w and ˜v, and the bias terms bw and bv (see
Theorem A.2). This result implies that for suitable values of w, gradient descent will decrease the distance
from v. The second part of the proof is showing that throughout the optimization process, w will stay in an
area where we can apply the result above. Speciﬁcally, the intricate part is showing that the term −bw
∥˜w∥does
not get too large. Note that due to Theorem 4.2, we know that keeping this term small means that w stays
away from the cone of bad critical points which are not the global minimum. The full proof can be found in
Appendix E.
7
Discussion
In this work we studied the problem of learning a single neuron with a bias term using gradient descent.
We showed several negative results, indicating that adding a bias term makes the problem more difﬁcult
than without a bias term. Next, we gave a characterization of the critical points of the problem under
some assumptions on the input distribution, showing that there is a manifold of critical points which are
not the global minimum. We proved two convergence results using different techniques and under different
2For example, for standard Gaussian distribution, we have that τ =
2
π ≈0.63, hence we can take α = 4.5, and β = O(1).
Since the distribution ˜D is symmetric, we present the assumption w.l.o.g with respect to the ﬁrst 2 coordinates.
10

assumptions. Finally, we showed that under mild assumptions on the input distribution, reaching the global
minimum can be achieved by standard random initialization.
We emphasize that previous works studying the problem of a single neuron either considered non-
standard algorithms (e.g. Isotron), or required assumptions on the input distribution which do not allow a
bias term. Hence, this is the ﬁrst work we are aware of which gives positive and negative results on the
problem of learning a single neuron with a bias term using gradient methods.
In this work we focused on the gradient descent algorithm. We believe that our results can also be ex-
tended to the commonly used SGD algorithm, using similar techniques to [21, 15], and leave it for future
work. Another interesting future direction is analyzing other previously studied settings, but with the ad-
dition of a bias term. These settings can include convolutional networks, two layers neural networks, and
agnostic learning of a single neuron.
References
[1] A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,
2017.
[2] S. Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980, 2014.
[3] I. Diakonikolas, S. Goel, S. Karmalkar, A. R. Klivans, and M. Soltanolkotabi. Approximation schemes
for relu regression. In Conference on Learning Theory, pages 1452–1485. PMLR, 2020.
[4] S. S. Du, J. D. Lee, and Y. Tian.
When is a convolutional ﬁlter easy to learn?
arXiv preprint
arXiv:1709.06129, 2017.
[5] S. Frei, Y. Cao, and Q. Gu. Agnostic learning of a single neuron with gradient descent. arXiv preprint
arXiv:2005.14426, 2020.
[6] S. Goel, S. Karmalkar, and A. Klivans. Time/accuracy tradeoffs for learning a relu with respect to
gaussian marginals. arXiv preprint arXiv:1911.01462, 2019.
[7] S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efﬁcient learning of generalized linear and single
index models with isotonic regression. In Advances in Neural Information Processing Systems, pages
927–935, 2011.
[8] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT.
Citeseer, 2009.
[9] S. M. M. Kalan, M. Soltanolkotabi, and A. S. Avestimehr. Fitting relus via sgd and quantized sgd. In
2019 IEEE International Symposium on Information Theory (ISIT), pages 2469–2473. IEEE, 2019.
[10] P. Kamath, O. Montasser, and N. Srebro.
Approximate is good enough: Probabilistic variants of
dimensional and margin complexity. In Conference on Learning Theory, pages 2236–2262. PMLR,
2020.
[11] E. Malach, P. Kamath, E. Abbe, and N. Srebro. Quantifying the beneﬁt of using differentiable learning
over tangent kernels. arXiv preprint arXiv:2103.01210, 2021.
11

[12] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for non-convex losses. arXiv
preprint arXiv:1607.06534, 2016.
[13] S. Oymak and M. Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent takes the
shortest path? arXiv preprint arXiv:1812.10004, 2018.
[14] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint
arXiv:1912.01703, 2019.
[15] O. Shamir. A stochastic pca and svd algorithm with an exponential convergence rate. In International
Conference on Machine Learning, pages 144–152, 2015.
[16] M. Soltanolkotabi. Learning relus via gradient descent. In Advances in Neural Information Processing
Systems, pages 2007–2017, 2017.
[17] Y. S. Tan and R. Vershynin. Online stochastic gradient descent with arbitrary initialization solves
non-smooth, non-convex phase retrieval. arXiv preprint arXiv:1910.12837, 2019.
[18] Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications
in convergence and critical point analysis. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 3404–3413. JMLR. org, 2017.
[19] G. Vardi and O. Shamir. Implicit regularization in relu networks with the square loss. arXiv preprint
arXiv:2012.05156, 2020.
[20] G. Yehudai and O. Shamir. On the power and limitations of random features for understanding neural
networks. In Advances in Neural Information Processing Systems, 2019.
[21] G. Yehudai and O. Shamir.
Learning a single neuron with gradient methods.
arXiv preprint
arXiv:2001.05205, 2020.
Appendices
A
Auxiliary Results
In this appendix we extend several key results from [21] for the case of adding a bias term. Speciﬁcally, we
extend Theorem 4.2 from [21] which shows that under mild assumptions on the distribution, the gradient of
the loss points in a good direction which depends on the angle between the learned vector w and the target
v. We also bound the volume of a certain set in R2, which can be seen as an extension of Lemma B.1 from
[21].
Lemma A.1. Let P = {y ∈R2 : w⊤y > b, v⊤y > b, ∥y∥≤α} for b ∈R and w, v ∈R2 with
∥w∥, ∥v∥= 1 and θ(w, v) ≤π −δ for δ ∈[0, π]. If b < α sin
  δ
2

then Vol(P) ≥(α sin( δ
2)−b)
2
4 sin( δ
2)
.
12

Proof. The volume of P is smallest when the angle is exactly π −δ, thus we can lower bound the volume
by assuming that θ(w, v) = π −δ. Next, we can rotate to coordinates to consider without loss of generality
the volume of the set
P ′ =

(y1, y2) ∈R2 : θ((y1, y2 −b′), e2) ≤δ/2, ∥(y1, y2)∥≤α
	
,
where b′ =
b
sin(δ/2) and e2 = (0, 1). Let P ′′ = {(x, y) ∈R2 : x2 + (y −b′)2 ≤(α −b′)2} be the
disc of radius α −b′ around the point (0, b′). It is enough to bound the volume of P ′ ∩P ′′. We deﬁne the
rectangular sets:
P1 =
(α −b′)
2
sin
δ
4

, (α −b′) sin
δ
4

×

b′ + (α −b′)
2
cos
δ
4

, b′ + (α −b′) cos
δ
4

P2 =

−(α −b′) sin
δ
4

, −(α −b′)
2
sin
δ
4

×

b′ + (α −b′)
2
cos
δ
4

, b′ + (α −b′) cos
δ
4

See Figure 1 for an illustration. We have that P1, P2 ⊆P ′ ∩P ′′. We will show it for P1, the same
argument also works for P2. First, P1 ⊆P ′′ is immediate by the deﬁnition of the two sets. For P ′, the
straight line in the boundary of P ′ is deﬁned by y2 = b′ + y1 ·
cos( δ
2)
sin( δ
2) . It can be seen that each vertex of the
rectangle P1, is above this line. Moreover, the norm of each vertex of P1 is at most α. Hence all the vertices
are inside P ′, which means that P1 ⊆P ′. In total we get:
Vol(P) ≥Vol(P ′ ∩P ′′) ≥Vol(P1 ∪P2)
= (α −b′)2
2
sin
δ
4

cos
δ
4

=
 α sin
  δ
2

−b
2
4 sin
  δ
2

Theorem A.2. Let w, v ∈Rd+1 , denote by ˜w, ˜v their ﬁrst d coordinates and by bw, bv their last coordinate.
Assume that θ( ˜w, ˜v) ≤π −δ for some δ ∈[0, π), and that the distribution D is such that its ﬁrst d
coordinates satisfy Assumption 4.1 (1) from [21], and that its last coordinate is a constant 1. Denote
b′ = max{−bw/∥˜w∥, −bv/∥˜v∥, 0} ·
1
sin( δ
2), and assume that b′ < α, then:
⟨∇F(w), w −v⟩≥(α −b′)4 sin
  δ
4
3 β
84
· min

1, 1
α2

∥w −v∥2
Proof. Let ˜x be the ﬁrst d coordinates of x. We have that:
⟨∇F(w), w −v⟩= Ex∼D
h
σ′(w⊤x)(σ(w⊤x) −σ(v⊤x))(w⊤x −v⊤x)
i
≥Ex∼D
h
1(w⊤x > 0, v⊤x > 0)(w⊤x −v⊤x)2i
= ∥w −v∥2 · Ex∼D
h
1( ˜w⊤˜x > −bw, ˜v⊤˜x > −bv)((w −v)⊤x)2i
≥∥w −v∥2 ·
inf
u∈span{w,v},∥u∥=1 Ex∼D
h
1( ˜w⊤˜x > −bw, ˜v⊤˜x > −bv)(u⊤x)2i
13

Figure 1: An illustration of the set P ′ (in red), the circle P ′′ (in blue) and the two rectangles P1, P2 (in
black), for the case of δ = π/2, α = 1 and b = 0.3. For b = 0, P ′ would be a pie slice, and the blue circle
P ′′ will coincide with the red circle.
Let b = max{−bw/∥˜w∥, −bv/∥˜v∥, 0}, then we can bound the above equation by:
∥w −v∥2 ·
inf
u∈span{w,v},∥u∥=1 Ex∼D
h
1( ˜w
⊤˜x > b, ˜v
⊤˜x > b)(u⊤x)2i
≥∥w −v∥2 ·
inf
u∈span{w,v},∥u∥=1 E˜x∼˜D
h
1( ˜w
⊤˜x > b, ˜v
⊤˜x > b, ∥˜x∥≤α)(˜u⊤˜x + bu)2i
(4)
Here bu is the bias term of u, ˜u are the ﬁrst d coordinates of u and ˜D is the marginal distribution of x
on its ﬁrst d coordinates. Note that since the last coordinate represents the bias term, then the distribution
on the last coordinate of x is a constant 1. The condition that ∥u∥= 1 (equivalently ∥u∥2 = 1) translates to
∥˜u∥2 + b2
u = 1.
Our goal is to bound the term inside the inﬁmum. Note that the expression inside the distribution depends
just on inner products of ˜x with ˜w or ˜v, hence we can consider the marginal distribution D ˜w,˜v of ˜x on the
2-dimensional subspace spanned by ˜w and ˜v (with density function p ˜w,˜v). Let ˆw and ˆv be the projections
of ˜w and ˜v on that subspace. Let P = {y ∈R2 : ˆw
⊤y > b, ˆv
⊤y > b, ∥y∥≤α}, then we can bound
Eq. (4) with:
14

∥w −v∥2 ·
inf
u∈R2,bu∈R:∥u∥2+b2u=1 Ey∼D ˜
w,˜v
h
1(y ∈P) · (u⊤y + bu)2i
= ∥w −v∥2 ·
inf
u∈R2,bu∈R:∥u∥2+b2u=1
Z
y∈R2 1(y ∈P) · (u⊤y + bu)2p ˜w,˜v(y)dy
≥β∥w −v∥2 ·
inf
u∈R2,bu∈R:∥u∥2+b2u=1
Z
y∈P
(u⊤y + bu)2dy
Combining with Proposition A.3 ﬁnishes the proof
Proposition A.3. Let P = {y ∈R2 : ˆw
⊤y > b, ˆv
⊤y > b, ∥y∥≤α} for b ∈R and ˆw, ˆv ∈R2 with
θ( ˆw, ˆv) ≤π −δ for δ ∈[0, π]. Then
inf
u∈R2,bu∈R:∥u∥2+b2u=1
Z
y∈P
(u⊤y + bu)2dy ≥(α −b′)4 sin
  δ
4
3
84
· min

1, 1
α2

for b′ =
b
sin( δ
2).
Proof. As in the proof of Lemma A.1, we consider the rectangular sets:
P1 =
(α −b′)
2
sin
δ
4

, (α −b′) sin
δ
4

×

b′ + (α −b′)
2
cos
δ
4

, b′ + (α −b′) cos
δ
4

P2 =

−(α −b′) sin
δ
4

, −(α −b′)
2
sin
δ
4

×

b′ + (α −b′)
2
cos
δ
4

, b′ + (α −b′) cos
δ
4

with b′ =
b
sin(δ/2). Since we have P1 ∪P2 ⊆P, and the function inside the integral is positive, we can lower
bound the target integral by integrating only over P1 ∪P2. Now we have:
inf
u∈R2,bu∈R:∥u∥2+b2u=1
Z
y∈P
(u⊤y + bu)2dy
≥
inf
u1,u2,bu∈R:u2
1+u2
2+b2u=1
Z
y∈P1∪P2
(u1y1 + u2y2 + bu)2dy
=
inf
u1,u2,bu∈R:u2
1+u2
2+b2u=1
Z
y∈P1∪P2
(u1y1)2dy +
Z
y∈P1∪P2
(u2y2 + bu)2dy +
Z
y∈P1∪P2
2u1y1(u2y2 + bu)dy
=
inf
u1,u2,bu∈R:u2
1+u2
2+b2u=1
Z
y∈P1∪P2
(u1y1)2dy +
Z
y∈P1∪P2
(u2y2 + bu)2dy
where in the last equality we used that P1 ∪P2 are symmetric around the y2 axis, i.e. (y1, y2) ∈P1 ∪P2 iff
(−y1, y2) ∈P1 ∪P2. By the condition that u2
1 + u2
2 + b2
u = 1 we know that either u2
1 ≥1
2 or u2
2 + b2
u ≥1
2.
Using that both integrals above are positive, we can lower bound:
inf
u1,u2,bu∈R:u2
1+u2
2+b2u=1
Z
y∈P1∪P2
(u1y1)2dy +
Z
y∈P1∪P2
(u2y2 + bu)2dy
≥min
(
1
2
Z
y∈P1∪P2
y2
1dy,
inf
u2,u3∈R:u2
2+u2
3= 1
2
Z
y∈P1∪P2
(u2y2 + u3)2dy
)
.
15

We will now lower bound both terms in the above equation. For the ﬁrst term, note that for every
y ∈P1 ∪P2 we have that |y1| ≥(α−b′)
2
sin
  δ
4

. Hence we have:
1
2
Z
y∈P1∪P2
y2
1dy ≥
≥1
2
Z
y∈P1∪P2
(α −b′)2
4
sin
δ
4
2
dy
=(α −b′)2
8
sin
δ
4
2
· (α −b′)2
2
sin
δ
4

cos
δ
4

≥(α −b′)4
16
√
2
sin
δ
4
3
(5)
where in the last inequality we used that δ ∈[0, π], hence δ/4 ∈[0, π/4].
For the second term we have:
inf
u2,u3∈R:u2
2+u2
3= 1
2
Z
y∈P1∪P2
(u2y2 + u3)2dy
=
inf
u∈
h
−1
√
2 , 1
√
2
i
Z
y∈P1∪P2
 
uy2 +
r
1
2 −u2
!2
dy
=(α −b′) sin
δ
4

inf
u∈
h
−1
√
2 , 1
√
2
i
Z
y2∈C
 
uy2 +
r
1
2 −u2
!2
dy2 .
(6)
The last equality is given by changing the order of integration into integral over y2 and then over y1, denoting
the interval C =
h
b′ + (α−b′)
2
cos
  δ
4

, b′ + (α −b′) cos
  δ
4
i
, and noting that the term inside the integral
does not depend on y1.
Fix some u ∈
h
−1
√
2,
1
√
2
i
. If u = 0, then we can bound Eq. (6) by (α−b′)2
4
sin
  δ
4

cos
  δ
4

. Assume
u ̸= 0, we split into cases and bound the term inside the integral:
Case I:

q
1
2 −u2
u
 ≥b′ + 3
4 · (α −b′) cos
  δ
4

. In this case, solving the inequality for u we have
|u| ≤
r
1
2+2(b′+ 3
4 (α−b′) cos( δ
4))
2 . Hence, we can also bound:
r
1
2 −u2 ≥
s
1
2 −
1
2 + 2
 b′ + 3
4(α −b′) cos
  δ
4
2 =
v
u
u
t
 b′ + 3
4(α −b′) cos
  δ
4
2
2 + 2
 b′ + 3
4(α −b′) cos
  δ
4
2
16

In particular, for every y2 ∈
h
b′ + (α−b′)
2
cos
  δ
4

, b′ + 5(α−b′)
8
cos
  δ
4
i
we get that:
uy2 +
r
1
2 −u2

≥

v
u
u
t
 b′ + 3
4(α −b′) cos
  δ
4
2
2 + 2
 b′ + 3
4(α −b′) cos
  δ
4
2 −
v
u
u
t
 b′ + 5
8(α −b′) cos
  δ
4
2
2 + 2
 b′ + 3
4(α −b′) cos
  δ
4
2

≥
(α −b′) cos
  δ
4

8
q
2 + 2
 b′ + (α −b′) cos
  δ
4
2
Case II:

q
1
2 −u2
u
 < b′ + 3
4 · (α −b′) cos
  δ
4

. Using the same reasoning as above, we get for every
y2 ∈
h
b′ + 7(α−b′)
8
cos
  δ
4

, b′ + (α −b′) cos
  δ
4
i
that:
uy2 +
r
1
2 −u2
 ≥
(α −b′) cos
  δ
4

8
q
2 + 2
 b′ + (α −b′) cos
  δ
4
2
Combining the above cases with Eq. (6) we get that:
inf
u2,u3∈R:u2
2+u2
3= 1
2
Z
y∈P1∪P2
(u2y2 + u3)2dy
≥(α −b′) sin
δ
4
 Z
y2∈C
(α −b′)2 cos
  δ
4
2
82(2 + 2
 b′ + (α −b′) cos
  δ
4
2)
dy2
≥
(α −b′)4 cos
  δ
4
3 sin
  δ
4

2 · 82  2 + 2
 b′ + (α −b′) cos
  δ
4
2
≥
(α −b′)4 sin
  δ
4
3
2 · 82√
2
 2 + 2
 b′ + (α −b′) cos
  δ
4
2
≥(α −b′)4 sin
  δ
4
3
84
· min

1, 1
α2

(7)
where in the second inequality we used that for δ ∈[0, π] we have sin
  δ
4

≤cos
  δ
4

, and in the last
inequality we used that b′ ≤α, and (α−b′) cos
  δ
4

≤α. Combining Eq. (5) with Eq. (7) ﬁnishes the proof.
B
Proofs from Section 3
B.1
Proof of Theorem 3.1
Let ϵ > 0, for the input distribution, we consider the uniform distribution on the ball of radius ϵ. Let bw be
the last coordinate of w, and denote by ˜w, ˜x the ﬁrst d coordinates of w and x. Using the assumption on
17

the initialization of w0 and on the boundness of the distribution ˜D we have:
|⟨˜w0, ˜x⟩| ≤∥˜w0∥∥˜x∥≤ϵ
√
d.
Since bw0 is also initialized with U([−1, 1]), w.p > 1/2 −ϵ
√
d we have that bw0 < −ϵ
√
d. If this event
happens, since the activation is ReLU we get that σ′(⟨w0, x⟩) = 1(⟨˜w0, ˜x⟩+ bw0 > 0) = 0 for every ˜x in
the support of the distribution. Using Eq. (3) we get that ∇F(w0) = 0, hence gradient ﬂow will get stuck
at its initial value.
B.2
Proof of Theorem 3.2
Lemma B.1. Let w ∈Rd+1 such that bw = 0, ˜w1 < −4
√
d, and ∥˜w2:d∥≤2
√
d. Then,
Pr
x∼D
h
w⊤x ≥0, v⊤x ≥0
i
= 0 .
Proof. If v⊤x ≥0 then x1 ≥r −
r
2d2 and hence x2
1 ≥r2 −r2
d2 . Since we also have ∥˜x∥≤r then
∥˜x2:d∥2 = ∥˜x∥2 −x2
1 ≤r2 −

r2 −r2
d2

= r2
d2 .
Hence,
Pr
x∼D
h
w⊤x ≥0, v⊤x ≥0
i
≤Pr
x∼D
h
w⊤x ≥0, x1 ≥r −
r
2d2 , ∥˜x2:d∥≤r
d
i
.
Since bw = 0, ∥˜w2:d∥≤2
√
d and ˜w1 < −4
√
d, then for every ˜x ∈B such that x1 ≥r −
r
2d2 ≥r
2 and
∥˜x2:d∥≤r
d we have
w⊤x = ˜w⊤˜x = ˜w1˜x1 + ⟨˜w2:d, ˜x2:d⟩< −4
√
d
· r
2 + 2
√
d · r
d = 0 .
Therefore, Prx∼D

w⊤x ≥0, v⊤x ≥0

= 0.
Lemma B.2. With probability 1
2 −od(1) over the choice of w0, we have
Pr
x∼D
h
w⊤
0 x ≥0, v⊤x ≥0
i
= 0 .
Proof. Let w ∈Rd+1 such that bw = 0 and ˜w ∼N(0, Id). Since ˜w1 has a standard normal distribution,
then we have ˜w1 < −4
√
d with probability 1
2 −od(1). Moreover, note that ∥˜w2:d∥2 has a chi-square distri-
bution and the probability of ∥˜w2:d∥2 ≤4d is 1 −od(1). Hence, by Lemma B.1, with probability 1
2 −od(1)
over the choice of w, we have
Pr
x∼D
h
w⊤x ≥0, v⊤x ≥0
i
= 0 .
Therefore,
Pr
x∼D

ρ w⊤
∥w∥x ≥0, v⊤x ≥0

= Pr
x∼D
h
w⊤x ≥0, v⊤x ≥0
i
= 0 .
Since ρ w⊤
∥w∥has the distribution of w0, the lemma follows.
18

Lemma B.3. Assume that w0 satisﬁes Prx∼D

w⊤
0 x ≥0, v⊤x ≥0

= 0. Let γ > 0 and let w ∈Rd+1
such that ˜w = γ ˜w0, and bw ≤0. Then, Prx∼D

w⊤x ≥0, v⊤x ≥0

= 0. Moreover, we have
• If −bw
∥˜w∥< r, then d ˜w
dt = −s ˜w for some s > 0, and dbw
dt < 0.
• If −bw
∥˜w∥≥r, then d ˜w
dt = 0 and dbw
dt = 0.
Proof. For every x we have: If w⊤x = γ ˜w⊤
0 ˜x+bw ≥0 then γ ˜w⊤
0 ˜x ≥0, and therefore w⊤
0 x = ˜w⊤
0 ˜x ≥0.
Thus
Pr
x∼D
h
w⊤x ≥0, v⊤x ≥0
i
≤Pr
x∼D
h
w⊤
0 x ≥0, v⊤x ≥0
i
= 0 .
(8)
We have
−d ˜w
dt = ∇˜wF(w) = E
x

σ(w⊤x) −σ(v⊤x)

σ′(w⊤x)˜x
= E
x

σ(w⊤x) −σ(v⊤x)

1(w⊤x ≥0)˜x
= E
x

σ(w⊤x) −σ(v⊤x)

1(w⊤x ≥0, v⊤x < 0)˜x
+ E
x

σ(w⊤x) −σ(v⊤x)

1(w⊤x ≥0, v⊤x ≥0)˜x
(Eq. (8))
=
E
x

σ(w⊤x) −σ(v⊤x)

1(w⊤x ≥0, v⊤x < 0)˜x
= E
x

σ(w⊤x)

˜x
= E
˜x 1( ˜w⊤˜x > −bw)( ˜w⊤˜x + bw)˜x .
If −bw
∥˜w∥≥r then for every ˜x ∈B we have ˜w⊤˜x ≤∥˜w∥r ≤−bw and hence d ˜w
dt = 0. Note that if
−bw
∥˜w∥< r, i.e., ∥˜w∥r > −bw, then Pr˜x
 ˜w⊤˜x > −bw

> 0. Since ˜D is spherically symmetric, then we
obtain d ˜w
dt = −s ˜w for some s > 0.
Next, we have
−dbw
dt = ∇bwF(w) = E
x

σ(w⊤x) −σ(v⊤x)

σ′(w⊤x) · 1
= E
x

σ(w⊤x) −σ(v⊤x)

1(w⊤x ≥0)
(Eq. (8))
=
E
x

σ(w⊤x) −σ(v⊤x)

1(w⊤x ≥0, v⊤x < 0)
= E
x

σ(w⊤x)

= E
˜x 1( ˜w⊤˜x > −bw)( ˜w⊤˜x + bw) .
If −bw
∥˜w∥≥r then for every ˜x ∈B we have ˜w⊤˜x ≤∥˜w∥r ≤−bw and hence dbw
dt = 0. Otherwise, we have
dbw
dt < 0.
Proof of Theorem 3.2. By Lemma B.2 w0 satisﬁes Prx∼D

w⊤
0 x ≥0, v⊤x ≥0

= 0 w.p. at least 1
2 −
od(1). Then, by Lemma B.3 we have for every t > 0 that ˜wt = γt ˜w0 for some γt > 0, bwt < 0, and
19

−bwt
∥˜wt∥≤r. Moreover, we have Prx∼D

w⊤
t x ≥0, v⊤x ≥0

= 0. Hence, for every t we have
F(wt) = 1
2 · E
x

σ(w⊤
t x) −σ(v⊤x)
2
= 1
2 · E
x

σ(w⊤
t x)
2
+ 1
2 · E
x

σ(v⊤x)
2
−E
x

σ(w⊤
t x)σ(v⊤x)

= 1
2 · E
x

σ(w⊤
t x)
2
+ 1
2 · E
x

σ(v⊤x)
2
≥1
2 · E
x

σ(v⊤x)
2
= F(0) .
Thus, gradient ﬂow does not converge to the global minimum F(v) = 0 < F(0).
C
Proofs from Section 4
Proof of Theorem 4.2. The gradient of the objective is:
∇F(w) = E
x∼D
h
σ(w⊤x) −σ(v⊤x)

· σ′(w⊤x)x
i
.
We can rewrite it using that σ is the ReLU activation, and separating the bias terms:
∇F(w) = E
˜x∼˜D
h
σ( ˜w⊤˜x + bw) −σ(˜v⊤˜x + bv)

· 1( ˜w⊤˜x + bw > 0)x
i
.
First, notice that if ˜w = 0 and bw < 0 then 1( ˜w⊤˜x + bw > 0) = 0 for all ˜x, hence ∇F(w) = 0. Second,
using Cauchy-Schwartz we have that |⟨˜w, ˜x⟩| ≤c · ∥˜w∥. Hence, for w with ˜w ̸= 0 and −bw
∥˜w∥≥c we have
that 1( ˜w⊤˜x + bw > 0) = 0 for all ˜x in the support of the distribution, hence ∇F(w) = 0. Lastly, it is
clear that for w = v we have that ∇F(w) = 0. This shows that the points described in the statement of the
proposition are indeed critical points. Next we will show that these are the only critical points.
Let w ∈Rd+1 which is not a critical point deﬁned above - i.e. either ˜w = 0 and bw > 0, or ˜w ̸= 0 and
−bw
∥˜w∥< c. Then we have:
⟨∇F(w), w −v⟩= Ex∼D
h
σ′(w⊤x)(σ(w⊤x) −σ(v⊤x))(w⊤x −v⊤x)
i
= Ex∼D
h
1(w⊤x > 0, v⊤x > 0)(σ(w⊤x) −σ(v⊤x))(w⊤x −v⊤x)
i
+
+ Ex∼D
h
1(w⊤x > 0, v⊤x ≤0)σ(w⊤x)(w⊤x −v⊤x)
i
≥Ex∼D
h
1(w⊤x > 0, v⊤x > 0)(w⊤x −v⊤x)2i
+
+ Ex∼D
h
1(w⊤x > 0, v⊤x ≤0)(w⊤x)2i
.
= Ex∼D
h
1( ˜w⊤˜x > −bw, ˜v⊤˜x > −bv)(w⊤x −v⊤x)2i
+
+ Ex∼D
h
1( ˜w⊤˜x > −bw, ˜v⊤˜x ≤−bv)(w⊤x)2i
.
(9)
Denote:
A1 := {˜x ∈Rd : ˜w⊤˜x > −bw, ˜v⊤˜x > −bv, ∥˜x∥< c}
A2 := {˜x ∈Rd : ˜w⊤˜x > −bw, ˜v⊤˜x ≤−bv, ∥˜x∥< c}
20

Since w is not a critical point as deﬁned above, we know that the set {˜x ∈Rd : ˜w⊤˜x > −bw, ∥˜x∥< c} has
a positive measure, hence either A1 or A2 have a positive measure. Assume w.l.o.g that A1 have a positive
measure, the other case is similar. Since both terms inside the expectations of Eq. (9) are positive, we can
lower bound it with:
Ex∼D
h
1( ˜w⊤˜x > −bw, ˜v⊤˜x > −bv)(w⊤x −v⊤x)2i
= ∥w −v∥2Ex∼D
h
1(˜x ∈A1)((w −v)⊤x)2i
(10)
Denote u := w −v, and note that w ̸= v, hence ∥u∥= 1. Denote by p(˜x) the pdf of ˜D, then we can
rewrite Eq. (10) as:
∥w −v∥2 ·
Z
˜x∈Rd 1(˜x ∈A1) · (˜u⊤˜x + bu)2p(˜x)d˜x
= ∥w −v∥2 ·
Z
˜x∈A1
(˜u⊤˜x + bu)2p(˜x)d˜x
(11)
Since the set A1 has a positive measure, and the set {˜x : ˜u⊤˜x + bu = 0} is of zero measure, there is a
point ˜x0 such that ˜u⊤˜x + bu ̸= 0. By continuity, there is a small enough neighborhood A of ˜x0, such that
˜u⊤˜x + bu ̸= 0 for every ˜x ∈A. Using Assumption 4.1 we can lower bound Eq. (11) by:
∥w −v∥2 · β
Z
˜x∈A
(˜u⊤˜x + bu)2d˜x
where this integral is positive. This shows that ⟨∇F(w), w−v⟩> 0, which shows that ∇F(w) ̸= 0, hence
w is not a critical point.
D
Proofs from Section 5
The following lemmas are required in order to prove Theorem 5.2. First, we show that if F(w) ≤F(0) −δ
then we can lower bound ∥w∥and Prx

w⊤x ≥0, v⊤x ≥0

.
Lemma D.1. Let δ > 0 and let w ∈Rd+1 such that F(w) ≤F(0) −δ. Then
∥w∥≥δ
c2 ,
and
Pr
x
h
w⊤x ≥0, v⊤x ≥0
i
≥
δ
c2∥w∥.
Proof. We have
F(0) −δ ≥F(w) = 1
2 E
x(σ(w⊤x) −σ(v⊤x))2
= 1
2 E
x(σ(w⊤x))2 + 1
2 E
x(σ(v⊤x))2 −E
x(σ(w⊤x)σ(v⊤x))
≥F(0) −E
x(σ(w⊤x)σ(v⊤x)) .
21

Hence
δ ≤E
x σ(w⊤x)σ(v⊤x) = E
x 1(w⊤x ≥0, v⊤x ≥0) · w⊤x · v⊤x
≤∥w∥c2 · Pr
x
h
w⊤x ≥0, v⊤x ≥0
i
.
Thus,
∥w∥≥
δ
c2 · Prx [w⊤x ≥0, v⊤x ≥0] ≥δ
c2 ,
and
Pr
x
h
w⊤x ≥0, v⊤x ≥0
i
≥
δ
c2∥w∥.
Using the above lemma, we now show that if F(w) ≤F(0) −δ then ∥w −v∥decreases.
Lemma D.2. Let δ > 0 and let B > 1. Let w ∈Rd+1 such that F(w) ≤F(0) −δ and ∥w −v∥≤B −1.
Let γ =
δ3
3·122B3c8c′2 and let 0 < η ≤γ
c4 . Let w′ = w −η∇F(w). Then,
∥w′ −v∥2 ≤∥w −v∥2 · (1 −γη) ≤(B −1)2 .
Proof. We have
∥w′ −v∥2 = ∥w −η∇F(w) −v∥2
= ∥w −v∥2 −2η⟨∇F(w), w −v⟩+ η2∥∇F(w)∥2 .
(12)
We ﬁrst bound ∥∇F(w)∥2. By Jensen’s inequality and since σ is 1-Lipschitz, we have:
∥∇F(w)∥2 ≤E
x

σ(w⊤x) −σ(v⊤x)
2
σ′(w⊤x)∥x∥2

≤c2 E
x

σ(w⊤x) −σ(v⊤x)
2
≤c2 E
x

w⊤x −v⊤x
2
= c2 E
x

(w −v)⊤x
2
≤c4∥w −v∥2 .
(13)
Next, we bound ⟨∇F(w), w −v⟩. Let u = w −v. We have
⟨∇F(w), w −v⟩= E
x

σ(w⊤x) −σ(v⊤x)

σ′(w⊤x)(w⊤x −v⊤x)
= E
x

w⊤x −v⊤x
2
1(w⊤x ≥0, v⊤x ≥0)+
E
x w⊤x · (w⊤x −v⊤x)1(w⊤x ≥0, v⊤x < 0)
≥∥w −v∥2 · E
x 1(w⊤x ≥0, v⊤x ≥0)(u⊤x)2 .
22

Let ξ =
δ
12Bc3c′ . The above is at least
∥w −v∥2 · ξ2 · Pr
x
h
w⊤x ≥0, v⊤x ≥0, (u⊤x)2 ≥ξ2i
= ∥w −v∥2 · ξ2 ·

Pr
x
h
w⊤x ≥0, v⊤x ≥0
i
−Pr
x
h
w⊤x ≥0, v⊤x ≥0, (u⊤x)2 < ξ2i
.
By Lemma D.1, and since ∥w∥≤∥w −v∥+ ∥v∥≤B −1 + 1 = B, the above is at least
∥w −v∥2 · ξ2 ·

δ
c2∥w∥−Pr
x
h
w⊤x ≥0, v⊤x ≥0, |u⊤x| < ξ
i
≥∥w −v∥2 · ξ2 ·
 δ
c2B −Pr
x
h
|u⊤x| ≤ξ
i
= ∥w −v∥2 · ξ2 ·
 δ
c2B −Pr
x
h
|˜u⊤˜x + bu| ≤ξ
i
.
(14)
We now bound Prx

|˜u⊤˜x + bu| ≤ξ

. We denote a = ∥˜u∥. If a ≤
1
4c, then since ∥u∥= 1 we have
bu ≥
q
1 −
1
16c2 ≥
q
1 −1
16 =
√
15
4 . Hence, for every x with ∥x∥≤c we have
|˜u⊤˜x + bu| ≥|bu| −|˜u⊤˜x| ≥
√
15
4
−ac ≥
√
15
4
−1
4 > 1
2 .
Note that
ξ =
δ
12Bc3c′ ≤
F(0)
12Bc3c′ =
1
12Bc3c′ · 1
2 E
x(σ(v⊤x))2 ≤
1
12Bc3c′ · 1
2c2 =
1
24Bcc′ ≤1
24 ,
where the last inequality is since B, c, c′ ≥1. Therefore, |˜u⊤˜x + bu| > ξ. Thus,
Pr
x
h
|˜u⊤˜x + bu| ≤ξ
i
= 0 .
Assume now that a ≥1
4c. We have
Pr
x
h
|˜u⊤˜x + bu| ≤ξ
i
= Pr
x
h
˜u⊤˜x ∈[−ξ −bu, ξ −bu]
i
= Pr
x

¯˜u⊤˜x ∈[−ξ
a −bu
a , ξ
a −bu
a ]

≤c′ · 2 · ξ
a
≤8cc′ξ .
Combining the above with Eq. (14), we obtain
⟨∇F(w), w −v⟩≥∥w −v∥2 · ξ2 ·
 δ
c2B −8cc′ξ

= ∥w −v∥2
δ2
122B2c6c′2 ·
 δ
c2B −8cc′ ·
δ
12Bc3c′

= ∥w −v∥2
δ2
122B2c6c′2 ·

δ
3c2B

= ∥w −v∥2
δ3
3 · 122B3c8c′2 .
(15)
23

Combining Eq. (12), (13) and (15), and using γ =
δ3
3·122B3c8c′2 , we have
∥w′ −v∥2 ≤∥w −v∥2 −2η∥w −v∥2 · γ + η2c4∥w −v∥2
= ∥w −v∥2 ·
 1 −2ηγ + η2c4
.
Since η ≤γ
c4 , we obtain
∥w′ −v∥2 ≤∥w −v∥2 ·

1 −2ηγ + ηc4 · γ
c4

= ∥w −v∥2 · (1 −γη) ≤∥w −v∥2 ≤(B −1)2 .
Next, we show that F(w) remains smaller than F(0) −δ during the training. In the following two
lemmas we obtain a bound for the smoothness of F in the relevant region, and in the two lemmas that follow
we use this bound to show that F(w) indeed remains small.
Lemma D.3. Let w ∈Rd+1 such that F(w) ≤F(0). Then, ∥∇F(w)∥≤c
p
2F(0).
Proof. By Jensen’s inequality, we have
∥∇F(w)∥2 ≤E
x

σ(w⊤x) −σ(v⊤x)
2
σ′(w⊤x)∥x∥2
≤c2 E
x

σ(w⊤x) −σ(v⊤x)
2
≤c22F(w) ≤2c2F(0) .
Lemma D.4. Let M, B > 0 and let w, w′ ∈Rd+1 be such that for every s ∈[0, 1] we have M ≤
∥w + s(w′ −w)∥≤B. Then,
∥∇F(w) −∇F(w′)∥≤∥w −w′∥· c2

1 + 8(B + 1)c′c2
M

.
Proof. We assume w.l.o.g. that ∥w −w′∥≤M
2c . Indeed, let 0 = s0 < . . . < sk = 1 for some integer k, let
wi = w + si(w′ −w), and assume that ∥wi −wi+1∥≤M
2c for every i. If the claim holds for every pair
wi, wi+1, then we have
∥∇F(w) −∇F(w′)∥= ∥
k−1
X
i=0
∇F(wi) −∇F(wi+1)∥
≤
k−1
X
i=0
∥∇F(wi) −∇F(wi+1)∥
≤
k−1
X
i=0
∥wi −wi+1∥· c2

1 + 8(B + 1)c′c2
M

= c2

1 + 8(B + 1)c′c2
M

∥w −w′∥.
24

We have
∥∇F(w) −∇F(w′)∥
= ∥E
x(σ(w⊤x) −σ(v⊤x))σ′(w⊤x)x −(σ(w′⊤x) −σ(v⊤x))σ′(w′⊤x)x∥
≤∥E
x 1(w⊤x ≥0, w′⊤x ≥0)

w⊤x −σ(v⊤x) −w′⊤x + σ(v⊤x)

x∥+
∥E
x 1(w⊤x ≥0, w′⊤x < 0)

w⊤x −σ(v⊤x)

x∥+
∥E
x 1(w⊤x < 0, w′⊤x ≥0)

w′⊤x −σ(v⊤x)

x∥.
By Jensen’s inequality and Cauchy-Shwartz, the above is at most
E
x 1(w⊤x ≥0, w′⊤x ≥0)∥w −w′∥· ∥x∥· ∥x∥+
E
x 1(w⊤x ≥0, w′⊤x < 0) (∥w∥· ∥x∥+ ∥v∥· ∥x∥) · ∥x∥+
E
x 1(w⊤x < 0, w′⊤x ≥0)
 ∥w′∥· ∥x∥+ ∥v∥· ∥x∥

· ∥x∥.
By our assumption we have ∥x∥≤c and ∥w∥, ∥w′∥≤B. Hence, the above is at most
∥w −w′∥c2 + Pr
x
h
w⊤x ≥0, w′⊤x < 0
i
· c2 · (B + 1)
+ Pr
x
h
w⊤x < 0, w′⊤x ≥0
i
· c2 · (B + 1) .
(16)
Now, we bound Prx

w⊤x ≥0, w′⊤x < 0

. If w⊤x ≥0 and w′⊤x < 0 then
w⊤x = w′⊤x + (w −w′)⊤x < 0 + ∥w −w′∥· ∥x∥≤c · ∥w −w′∥.
Hence, we only need to bound
Pr
x
h
w⊤x ∈[0, c · ∥w −w′∥]
i
= Pr
x
h
˜w⊤˜x + bw ∈[0, c · ∥w −w′∥]
i
.
We denote a = ∥˜w∥. If a ≤M
4c , then since ∥w∥≥M we have |bw| ≥
q
M2 −
  M
4c
2 = M
p
1 −1/(16c2).
Hence for every x we have
| ˜w⊤˜x + bw| ≥|bw| −| ˜w⊤˜x| ≥|bw| −ac ≥M
p
1 −1/(16c2) −M
4 ≥M
p
1 −1/16 −M
4
= M ·
√
15 −1
4
> M/2 ≥c∥w −w′∥.
Thus, Prx
 ˜w⊤˜x + bw ∈[0, c · ∥w −w′∥]

= 0.
Assume now that a ≥M
4c . Hence, c
a ≤4c2
M . Therefore, we have
Pr
x
h
˜w⊤˜x + bw ∈[0, c · ∥w −w′∥]
i
= Pr
x

¯˜w⊤˜x ∈[−bw
a , −bw
a + c
a · ∥w −w′∥]

≤Pr
x

¯˜w⊤˜x ∈[−bw
a , −bw
a + 4c2
M · ∥w −w′∥]

≤c′ · 4c2
M · ∥w −w′∥.
25

Hence, Prx

w⊤x ≥0, w′⊤x < 0

≤c′ · 4c2
M · ∥w −w′∥. By similar arguments, this inequality holds
also for Prx

w⊤x < 0, w′⊤x ≥0

. Plugging it into Eq. (16), we have
∥∇F(w) −∇F(w′)∥≤∥w −w′∥

c2 + 2 · c2 · (B + 1) · c′ · 4c2
M

= ∥w −w′∥· c2

1 + 8(B + 1)c′c2
M

.
Lemma D.5. Let f : Rd →R and let L > 0. Let x, y ∈Rd be such that for every s ∈[0, 1] we have
∥∇f(x + s(y −x)) −∇f(x)∥≤Ls∥y −x∥. Then,
f(y) −f(x) ≤∇f(x)⊤(y −x) + L
2 ∥y −x∥2 .
Proof. The proof follows a standard technique (cf. [2]). We represent f(y) −f(x) as an integral, apply
Cauchy-Schwarz and then use the L-smoothness.
f(y) −f(x) −∇f(x)⊤(y −x) =
Z 1
0
∇f(x + s(y −x))⊤(y −x)ds −∇f(x)⊤(y −x)
≤
Z 1
0
∥∇f(x + s(y −x)) −∇f(x)∥· ∥y −x∥ds
≤
Z 1
0
Ls∥y −x∥2ds
= L
2 ∥y −x∥2 .
Hence, we have
f(y) −f(x) ≤∇f(x)⊤(y −x) + L
2 ∥y −x∥2 .
Lemma D.6. Let B, δ > 0 and let L = c2 
1 + 16(B+1)c′c4
δ

. Let w ∈Rd+1 such that F(w) ≤F(0) −δ
and let w′ = w −η · ∇F(w), where η ≤min

δ
2c3√
2F(0), 1
L

= min

δ
2c3√
2F(0),
δ
δc2+16(B+1)c′c6

.
Assume that ∥w∥, ∥w′∥≤B. Then, we have F(w′) −F(w) ≤−η
 1 −L
2 η

∥∇F(w)∥2, and F(w′) ≤
F(w) ≤F(0) −δ.
Proof. Let M =
δ
2c2 . By Lemmas D.1 and D.3, we have ∥w∥≥δ
c2 and ∥∇F(w)∥≤c
p
2F(0). Hence for
every λ ∈[0, 1] we have
∥w −λη∇F(w)∥≥δ
c2 −η · c
p
2F(0) ≥δ
c2 −
δ
2c3p
2F(0)
· c
p
2F(0) =
δ
2c2 = M .
Since ∥w∥, ∥w′∥≤B, we also have ∥w −λη∇F(w)∥≤B. By Lemma D.4, we have for every λ ∈[0, 1]
that
∥∇F(w) −∇F(w −λη∇F(w))∥≤λη∥∇F(w)∥· c2

1 + 8(B + 1)c′c2
M

.
26

We have L = c2 
1 + 16(B+1)c′c4
δ

= c2 
1 + 8(B+1)c′c2
M

. By Lemma D.5 we have
F(w −η∇F(w)) −F(w) ≤−η∥∇F(w)∥2 + L
2 η2∥∇F(w)∥2 .
Since η ≤1
L, we also have F(w −η∇F(w)) ≤F(w) ≤F(0) −δ.
We are now ready to prove the theorem:
Proof of Theorem 5.2. Let B = ∥w0∥+ 2. Assume that η ≤min

δ
2c3√
2F(0),
δ
δc2+16(B+1)c′c6 , γ
c4

. We
have ∥w0 −v∥≤∥w0∥+ ∥v∥= ∥w0∥+ 1 ≤B −1. By Lemmas D.2 and D.6, for every t we have
∥wt −v∥≤B −1 (thus, ∥wt∥≤B) and F(wt) ≤F(0)−δ. Moreover, by Lemma D.2, we have for every
t that ∥wt+1 −v∥2 ≤∥wt −v∥2 · (1 −γη). Therefore, ∥wt −v∥2 ≤∥w0 −v∥2 (1 −γη)t.
It remains to show that
min
(
δ
2c3p
2F(0)
,
δ
δc2 + 16(B + 1)c′c6 , γ
c4
)
= γ
c4 .
Note that we have δ ≤F(0) = 1
2 Ex(σ(v⊤x))2 ≤1
2 · c2. Thus
γ
c4 =
δ3
3 · 122B3c12c′2 ≤
δ
3 · 122B3c12c′2 · c4
4 =
δ
123B3c8c′2 .
We have
δ
2c3p
2F(0)
≥
δ
2c4 ≥γ
c4 ,
where the last inequality is since B, c, c′ ≥1. Finally,
δ
δc2 + 16(B + 1)c′c6 ≥
δ
c4
2 + 16(B + 1)c′c6 ≥
δ
17(B + 1)c′c6 ≥
δ
34Bc′c6 ≥γ
c4 .
E
Proofs from Section 6
Before proving Theorem 6.2, we ﬁrst proof two auxiliary propositions which bounds certain areas for which
the vector w cannot reach during the optimization process. The ﬁrst proposition shows that if the norm of
˜w is small, and its bias is close to zero, then the bias must get larger. The second proposition shows that if
the norm of ˜w is small, and the bias is negative, then the norm of ˜w must get larger.
Proposition E.1. Assume that ∥˜w −˜v∥2 ≤1, and that Assumption 6.1 holds. If ∥˜w∥≤0.4 and bw ∈
h
0, α3β
640
i
then (∇F(w))d+1 ≤−α3β
640 .
27

Proof. The d + 1 coordinate of the distribution D is a constant 1. We denote by ˜D the ﬁrst d coordinates of
the distribution D. Hence, we can write:
(∇F(w))d+1 = Ex∼D
h
(σ(w⊤x) −σ(v⊤x))1(w⊤x > 0)
i
= E˜x∼˜D
h
(σ( ˜w⊤˜x + bw) −σ(˜v⊤˜x + bv))1( ˜w⊤˜x > −bw)
i
= E˜x∼˜D
h
( ˜w⊤˜x + bw) · 1( ˜w⊤˜x > −bw)
i
−
−E˜x∼˜D
h
(˜v⊤˜x + bv) · 1( ˜w⊤˜x > −bw, ˜v⊤˜x > −bv)
i
(17)
We will bound each term in Eq. (17) separately. Using the assumption that ˜D is spherically symmetric,
we can assume w.l.o.g that ˜w = ∥˜w∥e1, the ﬁrst unit vector. Hence we have that :
E˜x∼˜D
h
( ˜w⊤˜x + bw) · 1( ˜w⊤˜x > −bw)
i
=E˜x∼˜D

(∥˜w∥x1 + bw) · 1

x1 > −bw
∥˜w∥

=∥˜w∥E˜x∼˜D

x11

x1 > −bw
∥˜w∥

+ bwE˜x∼˜D

1

x1 > −bw
∥˜w∥

(a)
≤0.4E˜x∼˜D

x11

x1 > −bw
∥˜w∥

+ bw
(b)
≤0.4E˜x∼˜D [x11(x1 > 0)] + bw .
(18)
Here, (a) is since ∥˜w∥≤0.4, and E˜x∼˜D
h
1

x1 > −bw
∥˜w∥
i
≤1, (b) is since bw ≥0, hence
E˜x∼˜D

x11

0 > x1 > −bw
∥˜w∥

≤0 .
For the second term of Eq. (17), we assumed that ∥˜w−˜v∥2 ≤1, which shows that θ( ˜w, ˜v) ≤π
2 , and the
term is largest when this angle is largest. Hence, to lower bound this term we can assume that θ( ˜w, ˜v) = π
2 ,
and since the distribution is spherically symmetric we can also assume w.l.o.g that ˜v = e2, the second unit
vector. Now we can bound:
E˜x∼˜D
h
(˜v⊤˜x + bv) · 1( ˜w⊤˜x > −bw, ˜v⊤˜x > −bv)
i
≥E˜x∼˜D

(x2 + bv) · 1

x1 > −bw
∥˜w∥, x2 > −bv

≥1
2E˜x∼˜D [(x2 + bv) · 1 (x2 > −bv)]
≥1
2E˜x∼˜D [x2 · 1 (x2 > 0)] + 1
2E˜x∼˜D [(x2 + bv) · 1 (0 > x2 > −bv)]
≥1
2E˜x∼˜D [x2 · 1 (x2 > 0)] = 1
2E˜x∼˜D [x1 · 1 (x1 > 0)] ,
(19)
where we used the assumption bv ≥0 and the symmetry of the distribution. Combining Eq. (18), Eq. (19)
with Eq. (17) we get:
28

(∇F(w))d+1 ≤bw −0.1E˜x∼˜D [x1 · 1(x1 > 0)] .
Let ˆD be the marginal distribution of ˜D on the plane spanned by e1 and e2, and denote by ˆx the projection
of ˜x on this plane. By Assumption 6.1(3) we have that the pdf of this distribution is at least β in a ball or
radius α around the origin. This way we can bound:
E˜x∼˜D [x1 · 1(x1 > 0)] = Eˆx∼ˆD [x1 · 1(x1 > 0)]
≥αβ
2 P(α/2 < ∥ˆx∥< α, x1 > α/2)
≥αβ
2 P(x1 ∈[α/2, 3α/4], x2 ∈[−α/4, α/4]) = α3β
32 .
Combining the above, and using the assumption on bw we get that:
(∇F(w))d+1 ≤bw −α3β
320 ≤−α3β
640
Proposition E.2. Assume that ∥˜w −˜v∥< 1, and Assumption 6.1 holds. Denote by τ =
E˜x∼˜
D[|x1x2|]
E˜x∼˜
D[x2
1]
where ˜D is the projection of the distribution D on its ﬁrst d coordinates. If ∥˜w∥≤τ
2 and bw ≤0 then
⟨∇F(w)1:d, ˜w⟩≤0.
Proof. Denote by ˜D the projection of the distribution D on its ﬁrst d coordinates, we have that:
⟨∇F(w)1:d, ˜w⟩=Ex∼D
h
σ(w⊤x) −σ(v⊤x)

1(w⊤x > 0) ˜w⊤˜x
i
=E˜x∼˜D
h
σ( ˜w⊤˜x + bw) −σ(˜v⊤˜x + bv)

1( ˜w⊤˜x > −bw) ˜w⊤˜x
i
≤E˜x∼˜D
h
˜w⊤˜x + bw −σ(˜v⊤˜x)

· 1( ˜w⊤˜x > −bw) ˜w⊤˜x
i
.
(20)
The inequality above is since bv ≥0. Recall that our goal is to prove that the above term is negative,
hence we will divide it by ∥˜w∥. Also, since the distribution ˜D is symmetric we can assume w.l.o.g that
˜w = ∥˜w∥e1. Hence, it is enough to prove that the following term is non-positive:
∥˜w∥E˜x∼˜D

∥˜w∥x1 + bw −σ(˜v⊤˜x)

· 1

x1 > −bw
∥˜w∥

x1

=∥˜w∥E˜x∼˜D
 ∥˜w∥x2
1 + bwx1

· 1

x1 > −bw
∥˜w∥

−∥˜w∥E˜x∼˜D

x1˜v⊤˜x · 1

x1 > −bw
∥˜w∥, ˜v⊤˜x > −bv

≤∥˜w∥E˜x∼˜D
 ∥˜w∥x2
1 + bwx1

· 1

x1 > −bw
∥˜w∥

−∥˜w∥E˜x∼˜D

x1˜v⊤˜x · 1

x1 > −bw
∥˜w∥, ˜v⊤˜x > 0

.
(21)
We will ﬁrst bound the second term above. Since the term only depend on inner products between ˜w, ˜v
with ˜x, we can consider the marginal distribution ˆD, of ˜D on the plane spanned by ˜w and ˜v. Since ˜D is
symmetric we can assume w.l.o.g that ˆD is spanned by the ﬁrst two coordinates x1 and x2. Let ˆ˜v be the
29

projection of ˜v on this plane, then we can write ˆ˜v = (v1, v2) where v2
1 + v2
2 = 1. Note that since the
distribution ˆD is symmetric, we have that E[x2
1] = E[x2
2]. By Cauchy-Schwarz we have:
|cov ˆD(x1, x2)| ≤
q
var ˆD(x1) · var ˆD(x2) = var ˆD(x1)
Again, by symmetry of ˆD we have that E[x1] = E[x2]. Opening up the above terms we get that E[x1 · x2] ≤
E[x2
1]. Also, we assumed that ∥˜w −˜v∥< 1, then θ(˜v, ˜w) ≤π
2 which means that v1 ≥0. Hence, the second
term of Eq. (21) is smallest when ˜v = e2. In total, we can bound Eq. (21) by:
∥˜w∥E˜x∼˜D
 ∥˜w∥x2
1 + bwx1

· 1

x1 > −bw
∥˜w∥

−∥˜w∥E˜x∼˜D

x1x2 · 1

x1 > −bw
∥˜w∥, x2 > 0

≤∥˜w∥E˜x∼˜D

∥˜w∥x2
1 + bwx1 −1
2x1|x2|

· 1

x1 > −bw
∥˜w∥

=∥˜w∥E˜x∼˜D

∥˜w∥x1 + bw −1
2|x2|

· x11

x1 > −bw
∥˜w∥

(22)
By our assumption, bw ≤0. Both terms inside the expectation in Eq. (22) are largest when bw = 0.
Hence, we can bound Eq. (22) by:
∥˜w∥E˜x∼˜D

∥˜w∥x1 −1
2|x2|

· x11 (x1 > 0)

=∥˜w∥2
2
E˜x∼˜D

x2
1

−∥˜w∥
4 E˜x∼˜D [|x1x2|]
≤∥˜w∥2
2
E˜x∼˜D

x2
1

−∥˜w∥τ
4
E˜x∼˜D

x2
1

= c1
∥˜w∥2
2
−∥˜w∥τ
4

.
(23)
In particular, for ∥˜w∥≤τ
2, Eq. (23) non-positive.
We are now ready to prove the main theorem:
Proof of Theorem 6.2. Denote bt = max{0, −bwt
∥˜wt∥}. We will show by induction on the iterations of gradi-
ent descent that throughout the optimization process bt < 2.4 · max
n
1,
1
√τ
o
and θ( ˜wt, ˜v) ≤π
2 for every
t ≥0.
By the assumption on the initialization we have that ∥˜w0 −˜v∥2 ≤∥w0 −v∥2 < 1, and also ∥˜v∥= 1,
hence θ( ˜w0, ˜v) ≤π
2 . We also have that bw0 ≥0, hence b0 = 0 this proves the case of t = 0. Assume this
is true for t. We will bound the norm of the gradient of the objective using Jensen’s inequality:
∥∇F(w)∥2 ≤Ex∼D
h
(σ(w⊤x) −σ(v⊤x))21(w⊤x > 0)x⊤x
i
≤Ex∼D
h
(w⊤x −v⊤x)2x⊤x
i
≤∥w −v∥2Ex∼D

∥x∥4
= ∥w −v∥2c .
(24)
For the (t + 1)-th iteration of gradient descent we have that:
∥wt+1 −v∥2 =∥wt −η∇F(wt) −v∥2
=∥wt −v∥2 −2η⟨∇F(wt), wt −v⟩+ η2∥∇F(wt)∥2
≤∥wt −v∥2 −2η⟨∇F(wt), wt −v⟩+ η2c∥wt −v∥2 .
(25)
30

By Theorem A.2, and the induction assumption on θ( ˜wt, ˜v) we get that there is a universal constant c0,
such that ⟨∇F(wt), wt −v⟩≥c0β(α−
√
2bt)
α2
∥wt −v∥2. Using the induction assumption that bt < 2.4 ·
max
n
1,
1
√τ
o
and Assumption 6.1(3) we can bound (α −
√
2bt) ≥0.1. In total we get that ⟨∇F(wt), wt −
v⟩≥
c0β
10α2 ∥wt −v∥2. By taking η ≤
c0β
10cα2 and combining with Eq. (25) we have that:
∥wt+1 −v∥2 < ∥wt −v∥2 .
In particular, ∥˜wt+1 −˜v∥2 ≤∥wt+1 −v∥2 < ∥wt −v∥2 < 1, which shows that θ( ˜wt+1, ˜v) ≤π
2 , and
concludes the ﬁrst part of the induction.
The bound for bt is more intricate, for an illustration see Figure 2. Let t′ be the ﬁrst iteration for which
∥˜wt′∥≥0.4. First assume that t ≤t′, we will show that in this case bt = 0. Assume otherwise, and let t0
be the ﬁrst iteration for which bt0 > 0, this means that bwt0 < 0 and bwt0−1 ≥0. We have that:
bwt0 = bwt0−1 −η∇F(wt0)d+1 .
If bwt0−1 ≤
α3β
640 , then by Proposition E.1 the last coordinate of the gradient is negative, hence bwt0 >
bwt0−1 ≥0. Otherwise, assume that bwt0−1 > α3β
640 . By Eq. (24): |∇F(wt0)d+1| ≤∥∇F(w)∥≤√c.
Hence, by taking η <
β
640√c ≤
α3β
640√c, we get that bwt0 ≥0, which is a contradiction (note that by
Assumption 6.1(3), we have α ≥1). We proved that if t ≤t′ then bwt ≥0, which means that bt = 0.
Assume now that t > t′. We will need the following calculation: Assume that ∥˜wt∥= δ, Then
∥˜wt −˜v∥2 ≥(1 −δ)2, and the minimum is achieved at ˜w = δ˜v. Since we have:
∥˜wt −˜v∥2 + (bwt −bv)2 = ∥wt −v∥2 ≤1 ,
we get that (bwt−bv)2 ≤1−(1−δ)2 ≤2δ. If we further assume that bwt ≤0, then b2
wt ≤(bwt−bv)2 ≤2δ.
Combining all the above, we get that if ∥˜wt∥= δ then:
bt = max

0, −bwt
∥˜wt∥

≤
r
2
δ .
(26)
To show the bound on bt we split into cases, depending on the norm of ˜wt:
Case I: 2τ
5 < ∥˜wt∥≤τ
2 and bwt ≤0. In this case we have:
∥˜wt+1∥2 = ∥˜wt −η∇F(wt)1:d∥2
= ∥˜wt∥2 −2η⟨˜wt, ∇F(wt)1:d⟩+ η2∥∇F(wt)1:d∥2
≥∥˜wt∥2 −2η⟨˜wt, ∇F(wt)1:d⟩.
We can use Proposition E.2 to get that ⟨˜wt, ∇F(wt)1:d⟩≤0, hence ∥˜wt+1∥2 ≥∥˜wt∥2. By Eq. (26) we
get that bt+1 ≤
q
5
τ ≤2.4
√τ .
Case II: ∥˜wt∥≥min

0.4, τ
2
	
. In this case, by choosing a step size η <
1
40c min{1, τ}we can bound
∥˜wt+1∥≥∥˜wt∥2 −2η⟨˜wt, ∇F(wt)1:d⟩
≥∥˜wt∥2 −2η∥˜wt∥∥∇F(wt)1:d∥
≥∥˜wt∥2 −2η∥˜wt∥∥∇F(wt)∥
≥∥˜wt∥2 −2η · 2c ≥min

0.39, 2τ
5

.
31

Figure 2: A 2-d illustration of the optimization landscape. The x axis represents ∥˜w∥, and the y-axis
represents bw. In the ﬁgure, for simplicity, we assume that bv = 0, and τ = 0.1 which means that 2τ
5 = 0.4.
The red circle represents the area with ∥w −v∥≤1, throughout the optimization process wt stays in this
circle. The black region represents the area where bt = −bw
∥˜w∥can be potentially large, our goal is to show
that wt stays out of this region. Case I shows that wt cannot cross the blue region. Case II shows that if wt
is to the right of the black region, then bt is upper bounded. Case III shows that wt cannot cross the orange
region (sub-cases (a) and (b)), and cannot cross from the green region directly to the black region (sub-case
(c)).
Again, by Eq. (26) we get that bt+1 ≤max
n√
5.2, 2.4
√τ
o
≤2.4·max
n
1,
1
√τ
o
. This concludes the induction.
Case III: ∥˜wt∥≤min

0.4, 2τ
5
	
. We split into sub-cases depending on the previous iteration: (a) If
bwt−1 ≤0, then by Case I the norm of ˜w cannot get below 2τ
5 , hence this sub-case is not possible; (b) If
bwt−1 ≥0 and ∥˜wt−1∥≤min

0.4, 2τ
5
	
, then by the same reasoning in the case of t < t′, bwt cannot get
smaller than zero. Hence, we must have that bwt+1 ≥0; (c) If bwt−1 ≥0 and ∥˜wt−1∥≥min

0.4, 2τ
5
	
then the bound depend on whether ∥˜wt−1∥is larger than 0.4 or not. If ∥
˜
wt−1∥≤0.4, then using the
same reasoning as the case of t′ < t twice (both for the t −1 and t iterations) we get that bt+1 ≥0. If
∥˜wt−1∥> 0.4 and bwt ≥0, then again this is the same case as in the case of t′ < t (since ∥˜wt∥≤0.4. The
last case is when ∥˜wt−1∥> 0.4 and bwt < 0, here using the same calculation as in Case II, we have that
∥˜wt∥≥0.39. Since ∥˜wt∥≤min

0.4, 2τ
5
	
, using Proposition E.2, the norm of ˜wt can only grow, hence
by the same reasoning as in Case I we can also bound bt+1 < 2.4 max
n
1,
1
√τ
o
.
Until now we have proven that throughout the entire optimization process we have that θ( ˜wt, ˜v) ≤π
2
32

and bt ≤2.4 · max
n
1,
1
√τ
o
. Let δ = π −θ( ˜wt, ˜v), we now use Theorem A.2 and Eq. (24) to get that:
∥wt+1 −v∥2 =∥wt −η∇F(wt) −v∥2
=∥wt −v∥2 −2η⟨∇F(wt), wt −v⟩+ η2∥∇F(wt)∥2
≤∥wt −v∥2 −2η

α −
bt
sin( δ
2)
4
β
84α2
sin
δ
4
3
∥wt −v∥2 + η2c∥wt −v∥2
≤∥wt −v∥2 −η
 α −
√
2bt
4 β
84α2
sin
δ
4
3
∥wt −v∥2 + η2c∥wt −v∥2
≤∥wt −v∥2 −η ˜Cβ
α2 ∥wt −v∥2 + η2c∥wt −v∥2
(27)
where ˜C is some universal constant, and we used the bounds from the induction above that δ ∈
π
2 , π

,
bt ≤2.4 · max
n
1,
1
√τ
o
, and by the assumption that α ≥2.5
√
2 max
n
1,
1
√τ
o
. By choosing η ≤
˜Cβ
2cα2 , and
setting λ =
˜Cβ
2cα2 we get that:
∥wt −v∥2 −η ˜Cβ min

1, 1
α2

∥wt −v∥2 + η2c∥wt −v∥2
≤(1 −λη)∥wt −v∥2 ≤· · · ≤(1 −ηλ)t∥w0 −v∥2 ,
which ﬁnished the proof.
F
Proofs from Subsection 5.2
F.1
Proof of Theorem 5.4
We have
F(w) = 1
2 E
x

σ(w⊤x) −σ(v⊤x)
2
= F(0) + 1
2 E
x

σ(w⊤x)
2
−E
x σ(w⊤x)σ(v⊤x)
≤F(0) + ∥w∥2c2
2
−∥w∥E
x σ( ¯w⊤x)σ(v⊤x) .
(28)
Let ξ =
α
4√c sin
  π
8

. We have
E
x σ( ¯w⊤x)σ(v⊤x) ≥ξ2 · Pr
x
h
σ( ¯w⊤x)σ(v⊤x) ≥ξ2i
≥ξ2 · Pr
x

¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c

.
(29)
In the following two lemmas we bound Prx
h
¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c
i
.
33

Lemma F.1. If bv ≥0 then
Pr
x

¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c

≥β
 α sin
  π
8

−2√cξ
2
4 sin
  π
8

.
Proof. If ∥˜v∥≥1
4c, then we have
Pr
x

¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c

≥Pr
x

¯w⊤x ≥2√cξ, ˜v⊤˜x ≥
ξ
2√c

= Pr
x

˜¯w⊤˜x ≥2√cξ, ¯˜v⊤˜x ≥
ξ
2√c∥˜v∥

≥Pr
x
h
˜¯w⊤˜x ≥2√cξ, ¯˜v⊤˜x ≥2√cξ
i
≥β
 α sin
  π
8

−2√cξ
2
4 sin
  π
8

,
where the last inequality is due to Lemma A.1, since θ( ˜w, ˜v) ≤3π
4 .
If ∥˜v∥≤1
4c, then
bv ≥
r
1 −
1
16c2 ≥
r
1 −1
16 =
√
15
4
> 3
4 ,
and hence
v⊤x = ˜v⊤˜x + bv > −1
4c · c + 3
4 = 1
2 ≥ξ ≥
ξ
2√c .
Therefore,
Pr
x

¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c

= Pr
x
h
¯w⊤x ≥2√cξ
i
= Pr
x
h
˜¯w⊤˜x ≥2√cξ
i
.
For ˜u ∈Rd such that ∥˜u∥= 1 and θ( ˜w, ˜u) = 3π
4 , Lemma A.1 implies that the above is at least
Pr
x
h
˜¯w⊤˜x ≥2√cξ, ˜u⊤˜x ≥2√cξ
i
≥β
 α sin
  π
8

−2√cξ
2
4 sin
  π
8

.
Lemma F.2. If bv < 0 and −bv
∥˜v∥≤α ·
sin( π
8 )
4
, then
Pr
x

¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c

≥β
 α sin
  π
8

−2√cξ
2
4 sin
  π
8

.
Proof.
Pr
x

¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c

= Pr
x

¯w⊤x ≥2√cξ, ˜v⊤˜x ≥
ξ
2√c −bv

= Pr
x

˜¯w⊤˜x ≥2√cξ, ¯˜v⊤˜x ≥
ξ
2√c∥˜v∥−bv
∥˜v∥

.
(30)
34

Moreover, we have
 
α · sin
  π
8

4
!2
≥
 bv
∥˜v∥
2
= 1 −∥˜v∥2
∥˜v∥2
=
1
∥˜v∥2 −1 ,
and hence
∥˜v∥2 ≥
16
α2 sin2   π
8

+ 16 ≥
16
 α sin
  π
8

+ 4
2 ≥
16
(c · 1 + 4c)2 ,
where in the last inequality we used c ≥α and c ≥1. Thus,
∥˜v∥≥4
5c ≥1
2c .
Combining the above with Eq. (30), and using −bv
∥˜v∥≤α ·
sin( π
8 )
4
, we have
Pr
x

¯w⊤x ≥2√cξ, v⊤x ≥
ξ
2√c

≥Pr
x
"
˜¯w⊤˜x ≥2√cξ, ¯˜v⊤˜x ≥√cξ + α · sin
  π
8

4
#
= Pr
x
h
˜¯w⊤˜x ≥2√cξ, ¯˜v⊤˜x ≥2√cξ
i
≥β
 α sin
  π
8

−2√cξ
2
4 sin
  π
8

,
where the last inequality is due to Lemma A.1, since θ( ˜w, ˜v) ≤3π
4 .
Combining Eq. (29) with Lemmas F.1 and F.2, we have
E
x σ( ¯w⊤x)σ(v⊤x) ≥ξ2 · β
 α sin
  π
8

−2√cξ
2
4 sin
  π
8

= α2 sin2   π
8

16c
· β
  α
2 sin
  π
8
2
4 sin
  π
8

= α4β sin3   π
8

256c
= M .
Plugging the above into Eq. (28) we have
F(w) ≤F(0) + ∥w∥2c2
2
−∥w∥· M .
The above expression is smaller than F(0) if ∥w∥< 2M
c2 .
G
Discussion on the Assumption on bv
In Corollary 5.5 we had an assumption that −bv
∥˜v∥≤α ·
sin( π
8 )
4
. This implies that either the bias term bv is
positive, or it is negative but not too large. Here we discuss why this assumption is crucial for the proof of
the theorem, and what can we still say when this assumption does not hold.
35

In Theorem 3.2 we showed an example with bv < 0 where gradient descent with random initialization
does not converge w.h.p. to a global minimum even asymptotically3. In the example from Theorem 3.2
we have −bv
∥˜v∥= r
 1 −
1
2d2

, and the input distribution is uniform over a ball of radius r. In this case, we
must choose α from Assumption 5.3 to be smaller than r (otherwise β = 0) and hence −bv
∥˜v∥> α
 1 −
1
2d2

.
Therefore it does not satisfy the assumption −bv
∥˜v∥≤α·
sin( π
8 )
4
(already for d > 1). If we choose, e.g., α = r
2,
then the example from Theorem 3.2 satisﬁes −bv
∥˜v∥= α
 2 −1
d2

≤2α. It implies that our assumption on
−bv
∥˜v∥is tight up to a constant factor, and is also crucial for the proof, since already for −bv
∥˜v∥= 2α we have
an example of convergence to a non-global minimum.
On the other hand, if −bv
∥˜v∥> α ·
sin( π
8 )
4
(i.e. the assumption does not hold) we can calculate the loss at
zero:
F(0) = 1
2 · E
x

σ(v⊤x)
2
= 1
2 · E
x

1(˜v⊤˜x + bv ≥0)

˜v⊤˜x + bv
2
= 1
2 · E
x
"
1

¯˜v⊤˜x ≥−bv
∥˜v∥

∥˜v∥2

¯˜v⊤˜x + bv
∥˜v∥
2#
≤∥˜v∥2
2
· E
x
"
1
 
¯˜v⊤˜x ≥α · sin
  π
8

4
! 
¯˜v⊤˜x
2
#
.
Let ϵ > 0 be a small constant. Suppose that the distribution ˜D is spherically symmetric, and that α is large,
such that the above expectation is smaller than ϵ. For such α, we either have −bv
∥˜v∥≤α ·
sin( π
8 )
4
, in which
case gradient descent converges w.h.p. to the global minimum, or −bv
∥˜v∥> α ·
sin( π
8 )
4
, in which case the
loss at w = 0 is already almost as good as the global minimum. For standard Gaussian distribution, we can
choose α to be a large enough constant that depends only on ϵ (independent of the input dimension), hence
β will also be independent of d. This means that for standard Gaussian distribution, for every constant ϵ > 0
we can ensure either convergence to a global minimum, or the loss at 0 is already ϵ-optimal.
Note that in Remark 3.3 we have shown another distribution which is non-symmetric and depends on
the target v, such that the loss F(0) is highly sub-optimal, but gradient ﬂow converges to such a point with
probability close to 1
2.
3In Theorem 3.2 we have ∥v∦= 1, but it still holds if we normalize v, namely, replace v with
v
∥v∥.
36

