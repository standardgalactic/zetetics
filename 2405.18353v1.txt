Simulating Infinite-dimensional Nonlinear Diffusion
Bridges
Gefan Yang1
Elizabeth Louise Baker1
Michael L. Severinsen2
Christy Anna Hipsley3
Stefan Sommer1
1Department of Computer Science, University of Copenhagen
2Globe Institute, University of Copenhagen
3Department of Biology, University of Copenhagen
1{gy,elba,sommer}@di.ku.dk
2michael.baand@sund.ku.dk
3christy.hipsley@bio.ku.dk
Abstract
The diffusion bridge is a type of diffusion process that conditions on hitting a
specific state within a finite time period. It has broad applications in fields such
as Bayesian inference, financial mathematics, control theory, and shape analysis.
However, simulating the diffusion bridge for natural data can be challenging
due to both the intractability of the drift term and continuous representations of
the data. Although several methods are available to simulate finite-dimensional
diffusion bridges, infinite-dimensional cases remain unresolved. In the paper, we
present a solution to this problem by merging score-matching techniques with
operator learning, enabling a direct approach to score-matching for the infinite-
dimensional bridge. We construct the score to be discretization invariant, which is
natural given the underlying spatially continuous process. We conduct a series of
experiments, ranging from synthetic examples with closed-form solutions to the
stochastic nonlinear evolution of real-world biological shape data, and our method
demonstrates high efficacy, particularly due to its ability to adapt to any resolution
without extra training.
1
Introduction
Diffusion processes are commonly utilized in diverse scientific fields, including mathematics, physics,
evolutionary biology, and finance, to model stochastic dynamics. Updating the posterior of the
model through conditioning on existing observed data is a crucial element of the process, and there
are several methods to achieve this. For example, the Gaussian process regression can be used for
finite-dimensional or infinite-dimensional linear models [Shi and Choi, 2011], and finite-dimensional
Doob’s h-transform is designed for finite nonlinear processes [Rogers and Williams, 2000, Chapter
6]. On the other hand, many data types, such as images and sound signals, are naturally continuous
and described by functions, and the stochasticity is also nonlinear. One approach is to discretize and
represent them by vectors using finite-dimensional models. However, a more natural way is to work
directly in the infinite-dimensional function space, performing only finite-dimensional projections
during inference. This functional formulation offers resolution-invariant features, which are more
memory-efficient and demonstrate greater generalization capacities. Simulating infinite-dimensional
linear processes has been studied in some research [Franzese et al., 2024, Lim et al., 2023, Pidstrigach
et al., 2023], while the extension to the conditioning of nonlinear processes remains an open question.
Interest in the study of linear diffusion processes has surged recently, largely driven by the ad-
vancements in diffusion generative models. In diffusion generative models, the data undergoes
Preprint. Under review.
arXiv:2405.18353v1  [cs.LG]  28 May 2024

perturbation through a forward unconditional linear diffusion process, resulting in noise that follows
a simple distribution and is easy to sample. The sampled noise is then transformed back to the
clean data through a reverse process, which is also linear and can be learned by the score-matching
techniques [Hyvärinen and Dayan, 2005, Vincent, 2011]. Recently, the study of diffusion models
using stochastic differential equations (SDEs) and their mathematical interpretations has gained
significant attention [Song et al., 2020, Huang et al., 2021]. Based on the rich research on the
time-reversed diffusion processes and the design of neural network approximation, we show that
similar techniques can be applied to simulate the conditioning of nonlinear diffusion processes.
0.0
0.2
0.4
0.6
0.8
1.0
x
0.0
0.2
0.4
0.6
0.8
1.0
y
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
Figure 1: The nonlinear stochastic
bridge between two continuous but-
terfly shapes (blue and red) is char-
acterized by discrete landmarks.
The nonlinearity of the bridge pre-
vents the shape from overlapping
and the landmarks from colliding,
which could happen with a linear
process.
In order to condition an infinite-dimensional nonlinear diffusion
process on function-valued observations, we use the theory of
infinite-dimensional Doob’s h-transform, as outlined in Baker
et al. [2024]. This transformation converts an unconditional
SDE into a conditional one, which is commonly referred to as
the diffusion bridge [Delyon and Hu, 2006, Schauer et al., 2017,
Heng et al., 2021]. Theoretically, the existence of such con-
ditional processes has been established, and our research has
focused on developing a numerically efficient method for their
simulation. This involved reversing the time of the diffusion
bridge, which adheres to an SDE characterized by a learnable
drift. We also propose a tractable loss function for learning
such drift, which is inspired by the denoising score-matching
object [Vincent, 2011]. By directly tackling the learning of the
infinite-dimensional process, we employ a time-dependent neu-
ral operator as the approximator. Given the temporal symmetry
between the diffusion bridge and its reversal, we simulate the
reversed bridge backward in time, effectively recreating paths
of the original bridge and thereby resolving the issue.
We aim to incorporate the score-matching techniques into the diffusion bridge simulation methodology,
especially to handle infinite-dimensional nonlinear bridges. Our contributions are outlined as follows:
• Our study introduces a novel approach for simulating infinite-dimensional nonlinear diffu-
sion bridges utilizing a score-matching technique. Expanding upon the research of [Heng
et al., 2021] that concentrated on simulating bridges in finite dimensions, we further develop
this technique to encompass infinite dimensions. By directly approximating the additional
drift introduced by the infinite-dimensional Doob’s h-transform, we are able to reverse the
diffusion bridge and simulate it backward using the learned drift operator. This framework
holds the potential for addressing simulating general infinite-dimensional nonlinear diffusion
bridges, especially those that involve continuous data within infinite-dimensional function
spaces.
• In order to approximate the drift operator in the reversed bridge, we have devised a continu-
ous time-dependent neural operator architecture that draws inspiration from the Fourier time
modulation [Park et al., 2023] and the U-shaped neural operator [Ashiqur Rahman et al.,
2022]. This structure is able to represent continuous time-dependent nonlinear operators that
are shown to be discretization invariant. Furthermore, by splitting the large nonlinear transi-
tion step into small tractable Gaussian steps, we design the loss function on the Gaussian
steps and, therefore, holds a computable form.
• We illustrate our proposed methodology using continuous function-data-valued stochastic
processes. This will involve a demonstration of the Brownian bridge with a closed-form
drift, as well as the simulation of stochastic nonlinear biological shape evolution. To our
best knowledge, this is the first time the nonlinear diffusion shape bridges can be simulated
using a discretization-invariant framework.
2
Related work
Bridge simulations: The linear diffusion process always has a Gaussian transition density, which
gives the additional drift term in the corresponding conditional process a closed form. The challenge
2

lies in the nonlinear case, where the transition is no longer Gaussian. Several methods have been
developed [Delyon and Hu, 2006, Van der Meulen and Schauer, 2020, Corstanje et al., 2024] for
simulating bridges in different scenarios. The essential idea is to approximate the unknown drift
with a proposed known one. The bridge, including such an approximate drift, is called the proposed
process. By sampling from the proposed process and computing the likelihood ratio between the
proposed and the true ones, the MCMC is used to correct the sampling. Such methods suffer from
expensive matrix inversion in high-dimensional cases and time-costly MCMC updating iterations.
Another approach is to directly approximate the drift, which is proposed by [Heng et al., 2021], where
they use denoising score matching to learn the score for the finite-dimensional nonlinear bridge.
Based on [Heng et al., 2021]’s work, we further generalize it to the infinite-dimensional settings.
Infinite-dimensional diffusion models: (Score-based) diffusion generative models (SGMs) are
initially developed to generative samples in Euclidean spaces [Sohl-Dickstein et al., 2015, Song
and Ermon, 2019, Song et al., 2021]. Various studies have been done to generalize SGMs into
infinite-dimensional space with either Hilbert-space-defined score functions [Pidstrigach et al., 2023,
Baldassari et al., 2024, Lim et al., 2023] or finite-dimensional score projections [Franzese et al., 2024,
Hagemann et al., 2023]. However, due to the generative purpose, their methods are all limited to the
linear SDEs (often referred to as variance preserving/exploring SDEs), i.e., with a diffusion term
independent of the value of the process, and such SDEs are unconditional to the end states. While
we consider a more general conditional nonlinear case, where the drift and diffusion terms are more
complex to derive a closed-form solution.
Diffusion Schrödinger bridge: Another diffusion bridge related model is the diffusion Schrödinger
bridge [De Bortoli et al., 2021, Shi et al., 2024, Tang et al., 2024, Thornton et al., 2022], where the
transition between two distributions are treated as the optimal transport (OT) problem. Among them,
the work in [Shi et al., 2024] is closely related to ours, as they also minimize the KL divergence
to match the unknown bridge. However, since their focus is on generative purposes, a simpler
finite-dimensional linear process and the bridge points are considered. We are instead interested in
the imputation between the points.
3
Infinite-dimensional nonlinear bridges
3.1
Conditioning via infinite-dimensional Doob’s h-transform
The infinite-dimensional diffusion bridge can be achieved by applying the infinite-dimensional
Doob’s h-transform [Baker et al., 2024] on an unconditional diffusion process. Let (H, ⟨·, ·⟩) denote
a separable Hilbert space with a chosen countable orthonormal base {ei}∞
i=1, and let (Ω, F, P) be
a probability space with natural filtration {Ft}, let B(H) the Borel algebra, P(H) be the set of
probability measures on (H, B(H)) and µ0, µT be two measures in P(H) that we want to bridge
between. A H-valued diffusion process X = (X(t))t∈[0,T ] is defined as:
dX(t) = f(t, X(t)) dt +g(t, X(t)) dW(t),
X0 ∼µ0
(1)
where W(t) is a Wiener process in another Hilbert space U (where U can equal H) with a trace-class
covariance operator C (i.e., Tr(C) < ∞), f : [0, T] × H →H, g : [0, T] × H →HS(C1/2(U), H),
where HS(C1/2(U), H) denotes the Hilbert-Schmidt operator from C1/2(U) to H. We can then
condition (1) on hitting X(T) ∼µT within a finite time T by applying infinite-dimensional Doob’s
h-transform, such transformed diffusion process Xc = (Xc(t))t∈[0,T ] follows the SDE:
dXc(t) = {f(t, Xc(t)) + a(t, Xc(t))∇log h(t, Xc(t))} dt +g(t, Xc(t)) dW(t)
(2)
with Xc(0) = x0 ∼µ0, Xc(T) = xT ∼µT , where a := gg∗: [0, T] × H →HS(C1/2(U), H),
∇log h : [0, T] × H →C1/2(U) is another Hilbert-Schmidt operator, we shall call it “score
operator” for short in the following text. h : [0, T] × H →H is the Doob’s h function, defined as
h(t, x) := P(X(T) ∼µT | X(t) = x). The composition a(·, ·)∇log h(·, ·) : [0, T] × H →H is
then an operator that maps within H and, therefore, can be treated as the additional drift term.
In finite dimension, the h : [0, T] × Rd →(0, ∞) can be chosen as a function satisfying h(t, x) =
R
h(t + s, y)p(t+s)|t(y | x) dy [Rogers and Williams, 2000, Chapter 6], where p(t+s)|t(y | x) is the
conditional probability density with respect to the Lebesgue measure dy in Rd. A common choice
of h is h(t, x) := pT |t(y | x)/pT |0(y | x0), and therefore the score becomes ∇log pT |0(y | x0).
However, since the Lebesgue measure dy and the probability density p are no longer well-defined in
3

H, such a choice is unavailable for infinite dimension. The definition of ∇log h as an operator, as
described above, can be used to address the well-posedness of the score. However, simulating (2) is
still nontrivial even with the finite-dimensional projection as used in [Franzese et al., 2024] because
of the intractable conditional probability, which only has a closed form when g is X-independent. To
simulate a more general nonlinear bridge, inspired by [Heng et al., 2021], we apply the generalized
time reversal theorem on the bridge process, and it turns out that in the time-reversed bridge process,
the additional drift term can be approximated by sampling from the unconditional process (1). In the
following sections, we shall detail the infinite-dimensional time reversal and a learnable object.
3.2
Time-reversed bridges
Under specific conditions, the finite-dimensional version of (1) Xd(t) ∈Rd has a time-reversed
process Y d(t) ∈Rd that satisfies [Haussmann and Pardoux, 1986]:
forward:
dXd(t) = f d(t, Xd(t)) dt +gd(t, Xd(t)) dW d(t),
Xd(t) ∈Rl,
(3)
reversed:
dY d(t) = ¯f d(t, Y d(t)) dt +gd(T −t, Y d(t)) dBd(t),
Y d(t) ∈Rl,
(4)
with f d : [0, T] × Rd →Rd, gd : [0, T] × Rd →Rl as finite-dimensional drift and diffusion terms,
W l(t), Bl(t) are Wiener processes in Rl, ¯f d(t, x) = −f d(T−t, x)+
1
pT −t(x)∇[ad(T−t, x)pT −t(x)],
and pt(x) as the probability density in Rd. Furthermore, [Millet et al., 1989] shows that such a time
reversal exists for infinite-dimensional SDEs. Suppose the Hilbert space U, where the Wiener process
W(t) lives, has a countable orthonormal basis {kj}∞
j=1, with the basis {ei}∞
i=1 of H, we can write
(1) in terms of these bases:
dXi(t) = fi(t, X(t)) dt +
∞
X
j
gij(t, X(t)) dWj(t),
i = 1, . . . , ∞,
(5)
where fi : [0, T] × H →R is defined as ⟨f(t, x), ei⟩and gi,j : [0, T] × H →R is defined by
⟨g(t, x)(ej), ei⟩.
This has a family of time-reversed processes:
dYi(t) = ¯fi(T −t, Y (t)) dt +
∞
X
j
gij(T −t, Y (t)) dBj(t),
i = 1, . . . , ∞,
(6)
where ¯fi(t, x) = −fi(t, x) +
1
pT −t(xi|ξi)
P
j∈I(i) ∇j[aij(T −t, x)pT −t(xi | ξi)], where pt(xi | ξi)
is the conditional probability density of Xi(t) = (Xj(t), j ∈I(i)) given (Xj(t), j /∈I(i)) = ξi, and
I(i) is a finite set of indices characterized by i. Such a density is assumed to exist with respect to the
Lebesgue measure in RI (See [Millet et al., 1989, Section 5]), which is a finite subspace of H. We
can apply this time reversal upon the infinite-dimensional bridge (2) derived in the previous section
and obtain the infinite-dimensional time-reversed bridge. We shall see that within this reversed bridge,
the additional term does not require the information of µT , which makes it possible to learn from the
process that starts from µ0 without hitting µT .
Theorem 3.1. Let f, g, and a be as defined before, and let B(t) be a Wiener process in U. The
conditional process Xc(t) has a time-reversal Y c = (Y c(t))t∈[0,T ] := Xc(T −t) following the
SDE:
dY c(t) = ¯f(t, Y c(t)) dt +g(T −t, Y c(t)) dB(t)
(7)
where ¯f(t, x) = −f(T −t, x)+a(T −t, x)∇log ¯h(T −t, x)+∇·a(t, x), and ¯h(t, x) = P(X(t) ∈
dx | X(0) = x0) =: P(0, x0, t, dx), which is the law of X(t) given X(0) = x0, and ∇· a(t, x) :=
P
j ∇jaij(t, x).
The proof of Theorem 3.1 can be found in A.1. Note that (7) does not require µT . If the process
starts from µ0, it is impossible to sample from µT conditioned on the intermediate distribution µt
without constructing the bridge forehead. Therefore the h in (2) is actually inaccessible. In contrast,
sampling from µt while conditioning on µ0 is feasible, which can be achieved by sampling the
unconditional process that starts from µ0. This insight makes learning ¯h possible instead. However,
the general transition from 0 to t is still intractable unless the original process has an X-independent
diffusion term, and then the transition shall be Gaussian, but we can address this issue by using the
Markovian property of the process, which enable us to split the large transition into small steps with
Gaussian-like transitions.
4

4
Learning the bridge
4.1
Learning the nonlinear drift
The idea of learning (7) is to minimize the distance between the estimated bridge and the true bridge
in the sense of KL divergence, which is sometimes referred to as “bridge matching” [Shi et al.,
2024, Liu et al., 2023a, Delbracio and Milanfar, 2023]. We can write the object function as the KL
divergence between the estimated and the true bridge. However, the true bridge still has no closed
form, and the object function is thereby uncomputable. Fortunately, the bridge is Markovian, which
means any single transition step is independent of the history, or we can say the whole information of
the process is “compressed” into a single small step. This property enables us to learn with only small
local steps and average it over the whole trajectory to gain global information on the whole process.
Let Px0 denote the path measure induced by (7) starting from Y c(T) = x0, and Qx0
θ be the path
measure induced by another time-reversed process Y = (Y (t))t∈[0,T ] follows:
dY (t) = fθ(t, Y (t)) dt +g(T −t, Y (t)) dBt
(8)
where fθ(t, x) = −f(T −t, x)+Gθ(T −t, x)+∇·a(T −t, x), Gθ : [0, T]×H →H is a parameterized
operator by θ. Let dPx0 / dQx0
θ denote the Radon-Nikodym derivative of Px0 with respect to Qx0
θ ,
then by minimizing the KL divergence DKL(Px0 | Qx0
θ ) = EX(t)[log dPx0 / dQx0
θ (X(t))], we shall
approximate the bridge (7) using Gθ.
Theorem 4.1. For any time partition (ti)n
i=1 of the interval [0, T], the loss function which is defined
as:
ˆL(θ) := 1
2
n
X
i=1
Z ti
ti−1
EX(t)

∥At(x){Gθ(t, x) −b(t, x, ti−1, xti−1)}∥U0

dt
(9)
is equivalent to DKL(Px0
| Qx0
θ ) up to a θ-independent C, where b(t, x, ti−1, xti−1) :=
a(t, xt)∇log Px0(X(t) ∈dx | X(ti−1) = xti−1), At(x) = C−1/2g−1(t, x) : [0, T] × H →U is a
θ-independent operator, ∥· ∥U is the norm of U.
Furthermore, At(x) is positive-definite because g and C are assumed to be positive, so ker(At(x)) =
{0}, and we can drop it when using gradient descent as suggested in [Lim et al., 2023]. Finally, we
train with the following object function:
L(θ) := 1
2
n
X
i=1
Z ti
ti−1
EX(t)

∥Gθ(t, x) −b(t, x, ti−1, xti−1)∥2
U
	
dt .
(10)
4.2
Time-dependent neural operators
As described in the previous section, Gθ : [0, T] × H →H is a time-dependent operator. We use a
time-dependent neural operator to approximate it. Neural operators are initially developed to solve
partial differential equations where the mapping from initial conditions to solutions is treated as a
nonlinear smooth operator [Li et al., 2020a,b, Lu et al., 2021, Kovachki et al., 2023]. Such a learned
operator is shown to be independent of the mesh or discretization used during the training [Kovachki
et al., 2023], thereby enabling them to operate on the continuous domain. We choose the Fourier
neural operator (FNO) proposed in [Li et al., 2020a] for two main reasons: 1) FNO is efficient due
to the Fast Fourier Transform (FFT) and easy to implement; 2) Within the FNO, the functions are
decomposed by the Fourier basis, which can represent the continuous periodic signals. It is especially
suitable for our use since we model closed shapes that can be treated as periodic.
Since our target operator Gθ is time-dependent, we incorporate the time information by using the
continuous-in-time FNO (CTFNO) proposed by [Park et al., 2023], which modulates the time
embedding into the Fourier modes. Besides the time step modulation, we also refer to the design
of UNO [Ashiqur Rahman et al., 2022], which is shown to outperform the classical FNO structure
used in [Li et al., 2020a] with less memory and higher accuracy. By combining both, we propose
the structure of U-shaped CTFNO, as shown in Figure 2. In practice, we first specify the discrete
spatial-temporal grid on which the functions are evaluated. Normally, the grid resolution for training
is lower than that for inference. We assume for both input and output functions, the dimension of
the domains is 1, and the codomains are du and dv, respectively. Then, the function evaluated at
5

Figure 2: Continuous-time U-shaped FNO architecture, acting as a parameterized operator Gθ :
(u, t) 7→v, where both u and v are evaluated on a discretized spatial-temporal grid (x, t) with
x ∈Rm, t ∈Rn
m-discretized domains can be represented with the m × du matrix. In fact, in the context of bridge
approximation, du = dv, we keep the notation for generality. After discretizing the spatial-temporal
grid as an Rm × Rn space, we evaluate u and obtain a finite-dimensional vector u(x, t) ∈Rdv×m×n.
Then u(x, t) and t are fed into the neural operator. Within the operator, P and Q are lifting and
projecting maps individually; the former increases the dimension of the codomain, and the latter
decreases. Both these two projections are implemented as convolutional layers with 1-size kernels.
At the same time, the time t is transformed into two embeddings through φ and ψ. We implemented
both as sinusoidal embeddings [Vaswani et al., 2017] as suggested in [Park et al., 2023]. The lifted
v0(x, t) and the embeddings φ(t), ψ(t) are fed into one of the continuous-time Fourier layers Gl.
Within the block, the input v0 is transformed into the Fourier modes through the FFT; Meanwhile,
the embedding φ(t) is transformed by a learnable linear transformation Aθ and will be used with a
learnable linear transformation Rθ on the Fourier modes through Aθφ(t)RθF(v0). Conversely, ψ(t)
and v0 shall go through a similar process, except all the actions are executed on the physical domain
without Fourier transformation. Note that the output spatial dimension does not have to be equal to
the input, i.e., xout ∈Rmout, xin ∈Rmin, mout ̸= min; that is, a spatial subspace is focused, which
is similar to the classical U-net. Finally, the transformed Fourier component F−1[Aθφ(t)RθF[v0]]
and the residual component Bθψ(t)Wθv0 are added and then activated by a point-wise nonlinear
activation function σ. Minic the traditional U-net, our neural operator is equipped with a decreasing
hierarchy from top to bottom in two aspects: 1) the output spatial dimension is decreasing; 2) the
truncated Fourier modes in FFT are decreasing. Both the designs are for extracting multi-level
Fourier features [Ashiqur Rahman et al., 2022]. The skip connections between the downsampling
and upsampling paths are kept as well. Compared to a normal FNO, our modified neural operator can
achieve better performance with fewer parameters and fast convergence speed, thereby being suitable
for the operator approximator in our case.
4.3
Numerical implementations
As we represent Gθ as a neural operator, we implicitly project the infinite-dimensional optimization
object 10 into finite dimension. Such projection P d : H →Rdcan be achieved by, for example,
taking the first d components {ei}d
i=1 ⊂{ei}∞
i=1, and projecting the element h ∈H by hd =
Pd
i=1⟨h, ei⟩H · ei. In the subspace Rd ⊂H, the conditional probability P(X(t) | X(0)) has a
density function p(Xd(t) | Xd(0)) given the defined Lebesgue measure. Xd(t) ∈Rd is a finite-
dimensional vector obtained by evaluating X(t) on a finite-dimensional grid. To sample Xd(t), we
need to solve the finite-dimensional version of (1):
dXd(t) = f d(t, Xd(t)) dt +gd(t, Xd(t)) dW k(t),
Xd(0) ∼µd
0,
(11)
6

where f d : [0, T] × Rd →Rd, gd : [0, T] × Rd →Rd×k, W k is a standard Wiener process lives
in Rk, µd
0 is the push-forwards of µ0 by the projection map P d : H →RdPidstrigach et al. [2023],
defined by:
µd := (P d)∗µ,
where
(P d)∗µ(A) = µ((P d)−1(A)), A ⊂H.
(12)
Solving (11) can be done using many finite-dimensional SDE numerical solving approaches. We
choose the Euler-Maruyama solver (See, for example, [Bayram et al., 2018]). In the Euler-Maruyama
scheme, the small-step transition is modeled as a Gaussian density, and the time integral from ti−1 to
ti can be approximated by the discrete sum. The single-step update of Euler-Maruyama is:
Xd(ti) = Xd(ti−1) + f d(ti−1, Xd(ti−1))δt + gd(ti−1, Xd(ti−1))(W k(ti) −W k(ti−1)),
(13)
where W k(ti) −W k(ti−1) ∼N(0, δtIk) is the independent Brownian increment. Then the approxi-
mate loss is:
Ld(θ) := 1
2δt
n
X
i=1
Ex0{∥Gθ(ti, Xd(ti)) −bd(ti, Xd(ti), ti−1, Xd(ti−1)∥2}}.
(14)
As δt →0, Ld(θ) →L(θ) by the property of Euler-Maruyama solver. In practice, we choose the
time steps as 0.01 within the total time interval [0, 1], which we consider as a proper choice of
balance between accuracy and efficiency. bd(ti, Xd(ti), ti−1, Xd(ti−1)) = Σd(ti−1, Xd(ti−1)) ·
∇log p(Xd(ti) | Xd(ti−1)).Since p is Gaussian, bd(ti, Xd(ti), ti−1, Xd(ti−1)) can be easily com-
puted as:
bd(ti, Xd(ti), ti−1, Xd(ti−1)) = −1
δt

Xd(ti) −Xd(ti−1) −δtf d(ti−1, Xd(ti−1))
	
.
(15)
Note that bd can be computed simultaneously as the SDE-solving step iterates without additional
computations afterward. So far, the parameterized operator Gθ and the computable loss function
Ld(θ) have been introduced. We summarize the algorithms for training and inference as Algorithm 1
and 2 in the Appendix.
5
Experiments
5.1
Functional Brownian bridges
Quadratic functions: We first study the function-valued Brownian process, whose score has a closed
form when projected into the finite dimension. For the object functions, we choose the quadratic
function f(x) = ax2 + ε, where a = {1, −1} and ε ∼N(0, 10−4) is used to equip the set with
non-zero measure, as used in [Phillips et al., 2022]. We choose the function-valued Brownian process
as the unconditional process since its derived bridge has a closed form. We choose the bounded grid
of [−1, 1] to evaluate the function, and the grid is uniformly discretized into a finite number of points,
as shown in Figure 3a. Since the SDE is defined in H, such discretization stands for choosing a finite
base and projecting it into the finite dimension. However, the projected process should be consistent
under arbitrary discretizations. We show it by training the neural operator under a low-resolution
scheme (8 points distributed within [−1, 1]) and evaluate it in a high-resolution case (128 points).
Note that our learned bridge (Figure 3b) shows high consistency with the ground true Brownian bridge
(Figure 3c), even in a much finer grid. We also compared the MSE between the simulated trajectories
by the true score and the estimated one with the same random seed initialization. The results indicate
that the learned process is discretization-invariant, i.e., it represents the infinite-dimensional process.
We further study the model’s performance under different discretization levels, as shown in Figure
3d; we observe that the model shows great consistency under discretizations in terms of the MSE
metrics. More samples from the estimated bridge can be found in Figure 7
Circles: We further test our method by simulating the Brownian bridge between 2D circles, which
acts as the simplest 2D stochastic shape bridge and holds a known form that we can compare. We
shall see in the next experiment the true shape bridge is inaccessible. The Circle can be treated as a
function from R to R2, and can be characterized by finite points along the outlines. We are trying to
bridge two circles with different radii. Figure 4a gives a visualization of the forward unconditional
process. Then we train the model with only 16 points evenly sampled from the circle and evaluate it
in higher resolutions (See Figure 4b and 8). Also, Figure 4c shows the ground true bridge simulated
7

1.0
0.5
0.0
0.5
1.0
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
(a) Quadratic functions
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
0.0
0.2
0.4
0.6
0.8
1.0
t
(b) Learned bridge
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
0.0
0.2
0.4
0.6
0.8
1.0
t
(c) Ground truth
816 32
64
128
256
Number of discretized points
2.25
2.50
2.75
3.00
3.25
MSE
1e
4
(d) MSE
Figure 3: Qualitative results for Brownian bridges between two quadratic functions; (a) Two quadratic
functions are chosen as the starting (f(x) = x2 + ε, blue) and the target (f(x) = −x2 + ε, red).
Both are evaluated at 8 points evenly distributed within [−1, 1] for training (marked in cyan and
orange respectively); (b) One sample from the learned reversed Brownian bridge, evaluated at 128
evenly distributed points; (c) One sample from the true reversed Brownian bridge, simulated with the
same random seed as (b) for comparison; (d) MSE between the estimated and the true bridges under
different resolutions.
with the same random seed, and we could expect consistency between these two simulations. Besides
the visual inspection, we show the score output at different stages of the process since we have access
to the ground true score that we can compare. The results are shown in Figure 5. Since, in the
Brownian case, there is no drift term in the unconditional process, the additional drift introduced by
the transformation is the only force that drives the functions toward the target, which is reflected in the
visualization that the score arrows should always point toward the center, despite of the discretization,
which can be observed in the Figure 5 as well.
1
0
1
x
1
0
1
y
0.0
0.2
0.4
0.6
0.8
1.0
(a) Unconditional process
1
0
1
x
1
0
1
y
0.0
0.2
0.4
0.6
0.8
1.0
t
(b) Learned bridge
1
0
1
x
1
0
1
y
0.0
0.2
0.4
0.6
0.8
1.0
t
(c) Ground truth
Figure 4: Qualitative results for Brownian bridges between two circles; (a) The unconditional forward
process starting from the red circle discretized into 32 points, the training uses 16 points marked in
orange crosses; (b) One sample from the learned reversed Brownian bridge, evaluated at 128 evenly
distributed points; (c) One sample from the true reversed Brownian bridge, simulated with the same
random seed as (b) for comparison.
1
0
1
x
1.5
1.0
0.5
0.0
0.5
1.0
1.5
y
t=0.20
Truth
Estimation
1
0
1
x
y
t=0.40
Truth
Estimation
1
0
1
x
y
t=0.60
Truth
Estimation
1
0
1
x
y
t=0.80
Truth
Estimation
Figure 5: The inspections of the estimated score and the true score at different time steps of the bridge
process.
5.2
Biological shape evolution
We finally evaluate our method on real data. Specifically, we are interested in modeling the stochastic
change in butterfly morphometry over time. In the phylogenetic analysis, such bridge processes
model the transitions along the edge between nodes in a phylogenetic tree, where the nodes store the
8

observed data that comes from specimens. Developing the fast bridge simulation approach facilitates
large-scale phylogenetic tree simulation and biological inferences. However, when modeling the
shape, the linear process can not be directly applied since the topology of the shape must be preserved.
We detail our nonlinear setting for the shape process in B.4. By applying the proposed method,
we can now bridge between two butterfly shapes. It’s worth noticing that the bridge is constructed
in infinite rather than finite dimensions. To prove this, we only train the score approximator with
32 points, corresponding to a finite discretization of the shape function. During the inference, we
evaluate the bridge on different levels of discretization, and it shows that the approximator can give
reasonably enough interpolations between low-discretized points, as shown in Figure 6. We point out
that our method does not constrain a specific shape since the deformation of the shape is modeled
instead of the shape itself. Also, since the bridge is constructed between two sets with non-zero
measures, in practice, it allows the observation to contain noise. Both these features make our method
suitable for modeling stochastic biological shape evolution, where 1) Multiple similar shapes are
considered, and 2) The observations often contain noise. We test our model on more butterfly species
which are close on the phylogenetic tree, the results are shown in Figure 9 and 10. This marks the
first instance where the simulation of nonlinear diffusion shape bridges has been achieved using a
discretization-invariant method, to the best of our knowledge.
0.0
0.2
0.4
0.6
0.8
1.0
x
0.0
0.2
0.4
0.6
0.8
1.0
y
0.0
0.2
0.4
0.6
0.8
1.0
t
(a) Evaluate on 64 landmarks
0.0
0.2
0.4
0.6
0.8
1.0
x
0.0
0.2
0.4
0.6
0.8
1.0
y
0.0
0.2
0.4
0.6
0.8
1.0
t
(b) Evaluate on 128 landmarks
0.0
0.2
0.4
0.6
0.8
1.0
x
0.0
0.2
0.4
0.6
0.8
1.0
y
0.0
0.2
0.4
0.6
0.8
1.0
t
(c) Evaluate on 256 landmarks
Figure 6: The stochastic shape bridge between two butterfly shapes (Papilio polytes in red, Parnassius
honrathi in blue). The shape is represented by landmarks of different numbers. Our method only
needs to train on a few sample points (marked in cyan and orange crosses)and can generalize to
arbitrary resolutions.
6
Conclusion
We introduce a novel approach for simulating nonlinear diffusion bridges in infinite-dimensional
spaces. This method utilizes Doob’s h-transform applied to an unconditional process, transforming
it into a conditional diffusion bridge. Subsequently, we reverse this bridge in time and simulate
the reversed sequence using a drift learned from the original, unconditional process. Utilizing a
neural operator, we establish that this learned drift retains continuity across discretizations, affirming
its role as a discretization-invariant representation of the underlying infinite-dimensional dynamics.
We validate our approach through several functional diffusion bridge test cases, especially the
phylogenetic shape analysis problem, where our method showcases its effectiveness of high-resolution
zero-shot training and generalization to various shapes, together with robustness under different levels
of discretizations.
However, simulating the time-reversed bridge poses computational challenges, particularly the
necessity of calculating the Jacobian of a. As outlined in Heng et al. [2021], learning the forward
bridge typically involves repeated sampling from the reverse bridge, which significantly escalates
computational costs. To address these issues, we aim to develop a method that directly learns the
forward bridge, thereby bypassing the need for reversal. We also anticipate applying this approach to
additional applications, such as stochastic image alignment and other contexts involving continuous
data, to further validate its versatility and effectiveness.
9

References
Jian Qing Shi and Taeryon Choi. Gaussian process regression analysis for functional data. CRC
press, 2011.
Leonard CG Rogers and David Williams. Diffusions, markov processes, and martingales: Volume 1,
foundations, volume 1. Cambridge university press, 2000.
Giulio Franzese, Giulio Corallo, Simone Rossi, Markus Heinonen, Maurizio Filippone, and Pietro
Michiardi. Continuous-time functional diffusion processes. Advances in Neural Information
Processing Systems, 36, 2024.
Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli,
Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based diffusion
models in function space. arXiv preprint arXiv:2302.07400, 2023.
Jakiw Pidstrigach, Youssef Marzouk, Sebastian Reich, and Sven Wang. Infinite-dimensional diffusion
models for function spaces. arXiv e-prints, pages arXiv–2302, 2023.
Aapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.
Journal of Machine Learning Research, 6(4), 2005.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-
tion, 23(7):1661–1674, 2011.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. International
Conference on Learning Representations, 2020.
Chin-Wei Huang, Jae Hyun Lim, and Aaron C Courville. A variational perspective on diffusion-based
generative models and score matching. Advances in Neural Information Processing Systems, 34:
22863–22876, 2021.
Elizabeth Louise Baker, Gefan Yang, Michael L Severinsen, Christy Anna Hipsley, and Stefan
Sommer. Conditioning non-linear and infinite-dimensional diffusion processes. arXiv preprint
arXiv:2402.01434, 2024.
Bernard Delyon and Ying Hu. Simulation of conditioned diffusion and application to parameter
estimation. Stochastic Processes and their Applications, 116(11):1660–1675, 2006.
Moritz Schauer, Frank van der Meulen, and Harry van Zanten. Guided proposals for simulating
multi-dimensional diffusion bridges. Bernoulli, 23(4A), November 2017. ISSN 1350-7265. doi:
10.3150/16-bej833. URL http://dx.doi.org/10.3150/16-BEJ833.
Jeremy Heng, Valentin De Bortoli, Arnaud Doucet, and James Thornton. Simulating diffusion bridges
with score matching. arXiv preprint arXiv:2111.07243, 2021.
Yesom Park, Jaemoo Choi, Changyeon Yoon, Myungjoo Kang, et al. Learning pde solution operator
for continuous modeling of time-series. arXiv preprint arXiv:2302.00854, 2023.
Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural
operators. arXiv e-prints, pages arXiv–2204, 2022.
Frank Van der Meulen and Moritz Schauer. Automatic backward filtering forward guiding for markov
processes and graphical models. arXiv preprint arXiv:2010.03509, 2020.
Marc Corstanje, Frank van der Meulen, Moritz Schauer, and Stefan Sommer. Simulating conditioned
diffusions on manifolds. arXiv preprint arXiv:2403.05409, 2024.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International conference on machine learning,
pages 2256–2265. PMLR, 2015.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in neural information processing systems, 32, 2019.
10

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
PxTIG12RRHS.
Lorenzo Baldassari, Ali Siahkoohi, Josselin Garnier, Knut Solna, and Maarten V de Hoop. Conditional
score-based diffusion models for bayesian inference in infinite dimensions. Advances in Neural
Information Processing Systems, 36, 2024.
Paul Hagemann, Sophie Mildenberger, Lars Ruthotto, Gabriele Steidl, and Nicole Tianjiao Yang.
Multilevel diffusion: Infinite dimensional score-based diffusion models for image generation.
arXiv preprint arXiv:2303.04772, 2023.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger
bridge with applications to score-based generative modeling. Advances in Neural Information
Processing Systems, 34:17695–17709, 2021.
Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrödinger
bridge matching. Advances in Neural Information Processing Systems, 36, 2024.
Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, and Baining Guo. Simplified diffusion
schrödinger bridge. arXiv preprint arXiv:2403.14623, 2024.
James Thornton, Michael Hutchinson, Emile Mathieu, Valentin De Bortoli, Yee Whye Teh, and
Arnaud Doucet. Riemannian diffusion schrödinger bridge. arXiv preprint arXiv:2207.03024, 2022.
Ulrich G Haussmann and Etienne Pardoux. Time reversal of diffusions. The Annals of Probability,
pages 1188–1205, 1986.
Annie Millet, David Nualart, and Marta Sanz. Time reversal for infinite-dimensional diffusions.
Probability theory and related fields, 82(3):315–347, 1989.
Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima
Anandkumar. I2sb: Image-to-image schrödinger bridge. arXiv preprint arXiv:2302.05872, 2023a.
Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising
diffusion for image restoration. arXiv preprint arXiv:2303.11435, 2023.
Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
Stuart, Anima Anandkumar, et al. Fourier neural operator for parametric partial differential
equations. In International Conference on Learning Representations, 2020a.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential
equations. arXiv preprint arXiv:2003.03485, 2020b.
Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning
nonlinear operators via deeponet based on the universal approximation theorem of operators.
Nature machine intelligence, 3(3):218–229, 2021.
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with
applications to pdes. Journal of Machine Learning Research, 24(89):1–97, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems, 30, 2017.
Mustafa Bayram, Tugcem Partal, and Gulsen Orucova Buyukoz. Numerical methods for simulation
of stochastic differential equations. Advances in Difference Equations, 2018:1–10, 2018.
Angus Phillips, Thomas Seror, Michael Hutchinson, Valentin De Bortoli, Arnaud Doucet, and Emile
Mathieu. Spectral diffusion processes. arXiv preprint arXiv:2209.14125, 2022.
11

Giuseppe Da Prato and Jerzy Zabczyk. Stochastic equations in infinite dimensions. Cambridge
university press, 2014.
Wilfried Loges. Girsanov’s theorem in hilbert space and an application to the statistics of hilbert
space-valued stochastic differential equations. Stochastic processes and their applications, 17(2):
243–263, 1984.
Laurent Younes. Shapes and diffeomorphisms, volume 171. Springer, 2010.
Martin Bauer, Martins Bruveris, and Peter W Michor. Overview of the geometries of shape spaces
and diffeomorphism groups. Journal of Mathematical Imaging and Vision, 50:60–97, 2014.
Hiroshi Kunita and Hiroshi Kunita. Stochastic flows and stochastic differential equations, volume 24.
Cambridge university press, 1990.
GBIF.Org User.
Occurrence download, 2024.
URL https://www.gbif.org/occurrence/
download/0075323-231120084113126.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 4015–4026, 2023.
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei
Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for
open-set object detection. arXiv preprint arXiv:2303.05499, 2023b.
Dean C Adams and Erik Otárola-Castillo. geomorph: an r package for the collection and analysis of
geometric morphometric shape data. Methods in ecology and evolution, 4(4):393–399, 2013.
12

A
Proofs
In this section, we provide the proofs for two theorems in the paper.
A.1
Proof of Theorem 3.1
Proof. We follow the setup in Millet et al. [1989]. Let I(i) ⊂Z, ∀i ∈Z be a finite set of indices,
{ei}∞
i=1 be an orthonormal basis for H ∋X(t), and {kj}∞
j=1 be an orthonormal basis for U ∋W(t).
Then we split X(t) into an infinite number of finite vectors; specifically, we denote Xi(t) =
(Xj(t), j ∈I(i)) ∈RI to be the vector consisting of all the components of X indexed by the entries
in I(i), with Xj(t) = ⟨X(t), ej⟩H. Similarly, let ˆXi(t) = (Xj(t), j /∈I(i)) be the complement of
Xi(t). Then, by assumption, the conditional law of Xi(t) given its complement ˆXi(t) = ξi has a
density pt(xi | ξi).
Consider the conditional bridge process (2), written in terms of the bases {ei} and {kj}:
dXc
i (t) = [f(t, Xc(t)) + a(t, Xc(t))∇log h(t, Xc(t))]i dt +
∞
X
j=1
gij(t, Xc(t))dWj(t).
(16)
We apply the time reversal theorem [Millet et al., 1989], to obtain the reversed bridge Y c(t):
dY c
i (t) = −[f(T −t, Y c(t))]idt −[a(T −t, Y c(t))∇log h(T −t, Y c(t))]idt
(17a)
+


1
pt(Y c,i(t) | ˆY c,i(t))
X
j∈I(i)
∇j[aij(T −t, Y c(t))pt(Y c,i(t) | ˆY c,i(t))]


i
dt
(17b)
+
∞
X
j=1
gij(T −t, Y c(t))dBj(t).
(17c)
In order to write this in the form appearing in the theorem, we focus on the second term. By properties
of the Fréchet derivative, it holds that
1
pt(Y c,i(t) | ˆY c,i(t))
X
j∈I(i)
∇j[aij(T −t, Y (t))pt(Y c,i(t) | ˆY c,i(t))]
(18a)
=
X
j∈I(i)
∇j[aij(T −t, Y (t))]
(18b)
+
1
pt(Y c,i(t) | ˆY c,i(t))
X
j∈I(i)
aij(T −t, Y (t))∇j[pt(Y c,i(t) | ˆY c,i(t))]
(18c)
=
X
j∈I(i)
∇j[aij(T −t, Y (t))] +
X
j∈I(i)
aij(T −t, Y (t))∇j log[pt(Y c,i(t) | ˆY c,i(t))].
(18d)
We now compare this to the second term by expanding that. Given h(T −t, Y (t)) = P(Y (T) | Y (t)),
we expand it using conditional densities:
P(Y c(T) | Y c(t)) =
∞
X
i
pT |t(Y c,i(T) | Y c,i(t), ˆY c,i(t), ˆY c,i(T))
(19)
Therefore,
[a(T −t, Y c(t))∇log h(T −t, Y c(t))]i
=
X
j∈I(i)
aij(T −t, Y c(t))∇j log pT |t(Y c,i(T) | Y c,i(t), ˆY c,i(t), ˆY c,i(T))
(20)
13

Note that pt(Y c,i(t) | ˆY c,i(t)) is a marginal density over 0, T, thereby can be written as:
pt(Y c,i(t) | ˆY c,i(t))
(21a)
= pt(Y c,i(t) | ˆY c,i(t), Y c,i(0), ˆY c,i(0), Y c,i(T), ˆY c,i(T))
(21b)
= pt|0(Y c,i(t) | ˆY c,i(t), Y c,i(0) | ˆY c,i(0))pT |t(Y c,i(T) | ˆY c,i(T), Y c,i(t) | ˆY c,i(t))
pT |0(Y c,i(T) | ˆY c,i(T), Y c,i(0) | ˆY c,i(0))
(21c)
Taking the logarithm and the gradient with respect to Y c,i(t), we obtain:
∇Y c,i(t) log pt(Y c,i(t) | ˆY c,i(t))
(22a)
= ∇Y c,i(t) log pt|0(Y c,i(t) | ˆY c,i(t), Y c,i(0), ˆY c,i(0))
(22b)
+ ∇Y c,i(t) log pT |t(Y c,i(T) | ˆY c,i(T), Y c,i(t), ˆY c,i(t))
(22c)
Therefore, the reversed bridge can be written with respect to the bases:
dY c
i (t)
(23a)
=
"
−f(T −t, Y c(t)) +
X
j∈I(i)
∇Y c,i(t)aij(T −t, Y c(t))
(23b)
−
X
j∈I(i)
aij(T −t, Y c(t))∇Y c,i(t) log pt|0(Y c,i(t) | ˆY c,i(t), Y c,i(0), ˆY c,i(t))
#
i
dt
(23c)
+
∞
X
j=1
gij(T −t, Y c(t))dBj(t)
(23d)
By summing over indices i, we obtain the infinite-dimensional reversed bridge as shown in Theorem
3.1.
A.2
Proof of Theorem 4.1
Proof. We first compute the Radon-Nikodym derivative of Px0 with respect to Qx0
θ by Girsanov’s
theorem (See Da Prato and Zabczyk [2014], Theorem 10.14, Lemma 10.15, also see Loges [1984]
Theorem 2), which states that there exists a U-valued stochastic process ψ(t, Y (t)) that satisfies
g(t, Y (t))C1/2ψ(t, Y (t)) = ˜f(t, Y (t)) −fθ(t, Y (t)),
(24)
such that,
dPx0
dQx0
θ
(Y (t)) = exp
n Z T
0
∥ψ(t, Y (t))∥U dB(t) −1
2
Z T
0
∥ψ(t, Y (t))∥2
U dt
o
Thus, the KL divergence is:
DKL(Px0 | Qx0
θ )
(25a)
= EY (t)
(Z T
0
∥ψ(t, Y (t)∥U dB(t)
)
+ EY (t)
(
−1
2
Z T
0
∥ψ(t, Y (t))∥2
U dt
)
(25b)
= EX(t)
(
1
2
Z T
0
∥ψ(t, X(t))∥2
U dt
)
(25c)
= EX(t)
(
1
2
Z T
0
C−1/2g−1(t, X(t))
n
˜f(t, X(t)) −fθ(t, X(t))
o
2
U dt
)
(25d)
= EX(t)
(
1
2
Z T
0
C−1/2g−1(t, X(t))
n
a(t, X(t))∇log ˜h(t, X(t)) −Gθ(t, X(t))
o
2
U dt
)
(25e)
14

In the second equation, the first term vanishes because of the martingale property of the Itô integral,
while the second term flips the sign because of the change of process from Y (t) to X(t) with the
reversed time variable. Denote C−1/2g−1(t, x) by At(x) for short, expand the squared norm in (25):
DKL(Px0 | Qx0
θ ) = EX(t)
(
1
2
Z T
0
At(X(t))
n
a(t, X(t))∇log ˜h(t, X(t)) −Gθ(t, X(t))
o
2
U dt
)
(26a)
= 1
2
Z T
0
Z
H
At(x)
n
a(t, x)∇log ˜h(t, x) −Gθ(t, x)
o
2
U dµt(dx) dt
(26b)
= 1
2
Z T
0
Z
H
∥At(x)Gθ(t, x)∥2
U dµt(dx) dt
|
{z
}
C1
(26c)
+ 1
2
Z T
0
Z
H
At(x)a(t, x)∇log ˜h(t, x)

2
U dµt(dx) dt
|
{z
}
C2
(26d)
−
Z T
0
Z
H
⟨At(x)Gθ(t, x), At(x)a(t, x)∇log ˜h(t, x)⟩U dµt(dx) dt
|
{z
}
C3
(26e)
where dµt(dx) denotes Px0(X(t) ∈dx | X(0)) = P(0, x0, t, dx). Under the given partition (ti)I
i=1
of [0, T], we shall split the time integral in C3 by finite parts:
C3 =
I
X
i=1
Z ti
ti−1
Z
H
⟨At(x)Gθ(t, x), At(x)a(t, x)∇log ˜h(t, x)⟩U dµt(dx) dt
(27a)
=
I
X
i=1
Z ti
ti−1
Z
H
⟨At(x)Gθ(t, x), At(x)a(t, x)∇log P(0, x0, t, dx)⟩UP(0, x0, t, dx) dt
(27b)
=
I
X
i=1
Z ti
ti−1
Z
H
⟨At(x)Gθ(t, x), At(x)a(t, x)∇P(0, x0, t, dx)⟩U dt
(27c)
Using the Chapman-Kolmogorov equation [Da Prato and Zabczyk, 2014, Corollary 9.15], for any
0 ≤ti−1 < t ≤T, we show:
∇P(0, x0, t, dx) = ∇X(t)
Z
H
P(0, x0, ti−1, dxti−1)P(ti−1, xti−1, t, dx)
(28a)
=
Z
H
∇X(t)P(ti−1, xti−1, t, dx)P(0, x0, ti−1, dxti−1)
(28b)
=
Z
H
∇X(t) log P(ti−1, xti−1, t, dx)P(ti−1, xti−1, t, dx)P(0, x0, ti−1, dxti−1)
(28c)
15

Therefore, substitute (28) back into (27)
C3 =
I
X
i=1
Z ti
ti−1
dt
Z
H
P(ti−1, xti−1, t, dx)P(0, x0, ti−1, dxti−1)
(29a)
Z
H
⟨At(x)Gθ(t, x), At(x)a(t, x)∇log P(ti−1, xti−1, t, dx)⟩U
(29b)
=
I
X
i=1
Z ti
ti−1
dt P(0, x0, t, dx)
Z
H
⟨At(x)Gθ(t, x), At(x)a(t, x)∇log P(ti−1, xti−1, t, dx)⟩U
(29c)
=
I
X
i=1
Z ti
ti−1
Z
H
⟨At(x)Gθ(t, xt), At(x)a(t, x)∇log P(ti−1, xti−1, t, dx)⟩U dµt(dx) dt (29d)
We then claim that:
L(θ) := 1
2
I
X
i=1
Z ti
ti−1
EX(t)

∥At(x){Gθ(t, x) −b(t, x, ti−1, xti−1)}∥2
U
	
dt = C1 + C4 −C3
where
C4 = 1
2
I
X
i=1
Z ti
ti−1
Z
H
At(x)a(t, x)∇log P(ti−1, xti−1, t, dx)
2
U dµt(dx) dt
To show that, we expand the squared norm and get:
L(θ) = 1
2
I
X
i=1
Z ti
ti−1
Z
H
∥At(x)Gθ(t, x)∥2
U dµt(dx) dt
|
{z
}
C1
(30a)
+ 1
2
I
X
i=1
Z ti
ti−1
Z
H
∥At(x)a(t, xt)∇log P(ti−1, xti−1, t, dx)∥2
U dµt(dx) dt
|
{z
}
C4
(30b)
−
I
X
i=1
Z ti
ti−1
Z
H
⟨At(x)Gθ(t, x), At(x)a(t, x)∇log P(ti−1, xti−1, t, dx)⟩H dµt(dx) dt
|
{z
}
C3
(30c)
Finally, by comparing with (26) and taking C = C4 −C2, we finish the proof.
B
Experiment Details
In this section, we provide the general algorithms for training and evaluating the model, also more
details on the three experiments. All the experiments are done on the platform with one NVIDIA RTX
4090 GPU and Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz with 128GB memory and Ubuntu
22.04.4 LTS OS. The codes are available on https://github.com/bookdiver/scoreoperator
B.1
Algorithms
B.2
Quadratic functions
We model the stochastic dynamics in the function space as the Brownian motion:
dX(t) = σ dW(t)
(31)
with σ = 0.1, which has a closed-form time-reversed bridge:
dY (t) = σ2 yT −Y (t)
T −t
dt +σ ddB(t)
(32)
We designed our operator to have 6 Fourier layers. The details are shown in Table 1. We train with
10,000 i.i.d. batched samples and with a batch size of 16. We chose the Adam optimizer with an
initial learning rate of 0.001 and cosine decay to 1e-5 as 80% of the training is finished.
16

Algorithm 1 Training
Require: Bridging object measure µd
0
Require: Total time length T, time step δt
Require: Batch size B
Require: Initialized Gθ
Require: Drift term f d, diffusion term gd
1: Form discrete time grid {t0 = 0, t1, . . . , tn−1, tn = T}, n = ⌊T/δt⌋
2: while Not converged do
3:
Sample {Xd
j (0)}B
j=1 ∼µd
0, i.i.d.
4:
for doi ←1, . . . , n
5:
Sample {εj}B
j=1 ∼N(0, Ik), i.i.d
6:
for doj ←1, . . . , B
7:
Xd
j (ti) ←Xd
j (ti−1) + f d(ti−1, Xd
j (ti−1))δt + gd(ti−1, Xd
j (ti−1))εj
8:
bd
j(ti, Xd
j (ti), ti−1, Xd
j (ti−1)) ←−gd(ti−1, Xd
j (ti−1))εj/δt
9:
end for
10:
end for
11:
Ld(θ) =
1
2B δt PB
j=1
Pn
i=1 ∥Gθ(ti, Xd
j (ti)) −bd
j(ti, Xd
j (ti), ti−1, Xd
j (ti−1)∥2}
12:
Perform gradient descent step on Ld(θ)
13: end while
Algorithm 2 Inference
Require: Bridging object measures µd
T
Require: Total time length T, time step δt
Require: Pretrained Gθ
Require: Drift term f d, diffusion term gd
1: Form discrete time grid {t0 = 0, t1, . . . , tn−1, tn = T}, n = ⌊T/δt⌋
2: Sample X(c)d(T) ∼µd
T , i.i.d.
3: for doi ←n, . . . , 1
4:
Sample ε ∼N(0, Ik), i.i.d
5:
X(c)d(ti−1)
←
X(c)d(ti)
+
{f d(ti, X(c)d(ti))
+
Gθ(ti, X(c)d(ti))
+
∇
·
(gd(gd)T )(ti, X(c)d(ti))}δt + gd(ti, X(c)d(ti))ε
6: end for
B.3
Circles
The SDE of circles follows the same as the one in the quadratic function experiment. The bigger
circle has a radius of 1.5, and the smaller circle has a radius of 1.0. For the neural operator design, it
is shown in Table 2. We train with 10,000 i.i.d. batched samples and with a batch size of 16. We
chose the Adam optimizer with an initial learning rate of 0.001 and cosine decay to 1e-5 as 80% of
the training is finished.
B.4
Butterflies
We shall first briefly review the stochastic shape analysis. A shape s : Rd →Rd is usually modeled
as an embedding in Rd [Younes, 2010]. Matching two shapes s0, sT along time is achieved by
finding a diffeomorphism f : [0, T] × Rd →Rd, such that f(0, s0(ξ)) = s0(ξ) and f(T, s0(ξ)) =
sT (ξ) for any ξ ∈Rn. There are normally infinitely many numbers of such fs that suffice, while
only the one with the lowest energy is considered, corresponding to the “simplest” deterministic
transformation between the shapes. Finding such a diffeomorphism is achieved by optimizing a
given energy functional [Bauer et al., 2014]. However, we are more interested in the stochastic
transformation between the shapes, that is, f follows a function-valued SDE that conditions on
hitting close enough to f(T) : s0(ξ) 7→sT (ξ) with the starting point as f0 = id. One way of
defining f is by the displacement of the shape, that is, f(t, s0(ξ)) = X(t, ξ) + s0(ξ), x ∈Rd,
where X ∈L2([0, T] × Rd, Rd) is a random variable in Hilbert space. It is easy to show that
17

Layer
Input
Output
Grid size
Fourier modes
Activation
Lifting
u : R →R
v0 : R →R16
8
-
-
Down1
v0 : R →R16
v1 : R →R16
8
6
GeLU
Down2
v1 : R →R16
v2 : R →R32
4
4
GeLU
Down3
v2 : R →R32
v3 : R →R64
2
2
GeLU
Up1
v3 : R →R64
v4 : R →R32
2
2
GeLU
Up2
v4 : R →R32
v5 : R →R16
4
4
GeLU
Up3
v5 : R →R16
v6 : R →R16
8
6
GeLU
Projection
v6 : R →R16
v : R →R
8
-
-
Table 1: Neural operator structure for quadratic function experiments
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
-1
-0.5
0
0.5
1
x
1.0
0.5
0.0
0.5
1.0
f(x)=x2
Figure 7: More samples of estimated Brownian bridges between two quadratic functions
X(0, ·) = 0, X(T, ·) = sT −s0. Furthermore, we define the unconditional SDE of X as:
dX(t) =
Z t
0
˜Q(X(t)) dW(t),
X(0) = 0
(33)
Where for each shape s ∈H = L2(Rd, Rd), ˜Q(s) : H →H is a Hilbert-Schmidt operator, defined
by some smooth kernel k ∈L2(Rd × Rd, Rd) as:
˜Q(s)(f(t, ξ)) =
Z
Rd k(s(ξ) + ξ, η)f(t, η) dη,
η ∈Rd
(34)
The SDE 33 is first defined in [Kunita and Kunita, 1990]. This formulation ensures that the described
process is diffeomorphic, which is required for shape evolution since the shape change must be
smooth and invertible. In our experiment, we choose the butterfly shapes live in R2, and choose the
kernel k to be 2D-Gaussian, i.e., k(x, y) = σ exp(−∥x −y∥2/κ). Specifically, we set the domain
in R2 to be finite as −0.5, 1.5]2 and discretize it with the resolution of 50 × 50, we then choose the
kernel with σ = 0.04 and κ = 0.02, which allows only the correlations within small areas. We then
design the operator as Table 3. We train with 20,000 i.i.d. batched samples and with a batch size of
16. We chose the Adam optimizer with an initial learning rate of 0.001 and cosine decay to 1e-5 as
80% of the training is finished.
The butterfly images are obtained from gbif.org [GBIF.Org User, 2024], and are segmented with the
Python package Segment Anything [Kirillov et al., 2023] and Gounding Dino [Liu et al., 2023b]. The
assigned landmarks are interpolated to be evenly distributed along the outlines, then aligned by the R
package Germorph v.4.06 [Adams and Otárola-Castillo, 2013] and normalized into [0, 1] intervals.
18

Layer
Input
Output
Grid size
Fourier modes
Activation
Lifting
u : R →R2
v0 : R →R16
16
-
-
Down1
v0 : R →R16
v1 : R →R16
16
8
GeLU
Down2
v1 : R →R16
v2 : R →R32
8
6
GeLU
Down3
v2 : R →R32
v3 : R →R64
4
4
GeLU
Up1
v3 : R →R64
v4 : R →R32
4
4
GeLU
Up2
v4 : R →R32
v5 : R →R16
8
6
GeLU
Up3
v5 : R →R16
v6 : R →R16
16
8
GeLU
Projection
v6 : R →R16
v : R →R2
16
-
-
Table 2: Neural operator structure for circle experiments
1
0
1
32 Points
1
0
1
64 Points
1
0
1
1
0
1
128 Points
1
0
1
1
0
1
1
0
1
x
y
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
0.0
0.2
0.4
0.6
0.8
1.0
t
Figure 8: More samples of estimated Brownian bridges between two circles, evaluated under different
levels of discretization
Layer
Input
Output
Grid size
Fourier modes
Activation
Lifting
u : R →R2
v0 : R →R16
32
-
-
Down1
v0 : R →R16
v1 : R →R16
32
16
GeLU
Down2
v1 : R →R16
v2 : R →R32
16
8
GeLU
Down3
v2 : R →R32
v3 : R →R64
8
6
GeLU
Down4
v3 : R →R64
v4 : R →R64
8
6
GeLU
Up1
v4 : R →R64
v5 : R →R64
8
6
GeLU
Up2
v5 : R →R64
v6 : R →R32
8
6
GeLU
Up3
v6 : R →R32
v7 : R →R16
16
8
GeLU
Up4
v7 : R →R16
v8 : R →R16
32
16
GeLU
Projection
v8 : R →R16
v : R →R2
32
-
-
Table 3: Neural operator structure for butterfly experiments
19

Allancastria cerisyi
Battus polydamas
Archon apollinus
Archon apollinus
Figure 9: The additional bridge simulations between different species, the bridges are constructed
between Archon apollinus (blue) and Allancastria cerisyi/Battus polydamas (red).
20

Cressida cressida
Graphium agamemon
Archon apollinus
Archon apollinus
Figure 10: The additional bridge simulations between different species, the bridges are constructed
between Archon apollinus (blue) and Cressida cressida/Graphium agamemon (red).
21

