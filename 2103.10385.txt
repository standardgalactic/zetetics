GPT Understands, Too
Xiao Liu * 1 2 Yanan Zheng * 1 2 Zhengxiao Du 1 2 Ming Ding 1 2 Yujie Qian 3 Zhilin Yang 4 2 Jie Tang 1 2
Abstract
While GPTs with traditional ﬁne-tuning fail to
achieve strong results on natural language under-
standing (NLU), we show that GPTs can be bet-
ter than or comparable to similar-sized BERTs
on NLU tasks with a novel method P-tuning—
which employs trainable continuous prompt em-
beddings. On the knowledge probing (LAMA)
benchmark, the best GPT recovers 64% (P@1)
of world knowledge without any additional text
provided during test time, which substantially
improves the previous best by 20+ percentage
points.
On the SuperGlue benchmark, GPTs
achieve comparable and sometimes better per-
formance to similar-sized BERTs in supervised
learning. Importantly, we ﬁnd that P-tuning also
improves BERTs’ performance in both few-shot
and supervised settings while largely reducing the
need for prompt engineering. Consequently, P-
tuning outperforms the state-of-the-art approaches
on the few-shot SuperGlue benchmark.
1. Introduction
Language model pre-training has been a successful approach
for many natural language processing tasks (Brown et al.,
2020). Evidences suggest that during the pre-training, not
only do language models learn contextualized text represen-
tations, but also grammar (Vig, 2019; Clark et al., 2019b),
syntactic (Hewitt & Manning, 2019), commonsense (Davi-
son et al., 2019) and even world knowledge (Petroni et al.,
2019; Wang et al., 2020).
According to the training objectives, pre-trained language
models can be divided into three categories: unidirectional
language models (e.g., GPT (Radford et al., 2019)) for nat-
ural language generation (NLG), bidirectional language
models (e.g., BERT (Devlin et al., 2018)) for natural lan-
*Equal contribution
1Tsinghua University, Beijing, China
2Beijing Academy of Artiﬁcial Intelligence, Beijing, China
3Massachusetts Institute of Technology, Cambridge, U.S.A.
4Recurrent AI, Ltd..
Correspondence to:
Zhilin Yang
<kimi yang@rcrai.com>, Jie Tang <jietang@tsinghua.edu.cn>.
Figure 1. Average scores on 7 dev datasets of SuperGlue. GPTs
can be better than similar-sized BERTs on NLU with P-tuning.
guage understanding (NLU) and hybrid language models
(e.g., XLNet (Yang et al., 2019), UniLM (Dong et al., 2019))
for combining the ﬁrst two paradigms. For long, researchers
have observed that GPT-style models perform poorly for
NLU tasks with ﬁne-tuning, and thus assumed that they are
not suitable for language understanding in nature.
The emerging GPT-3 (Brown et al., 2020) and its particular
performance on few-shot and zero-shot learning with hand-
crafted prompts has swept the machine learning community.
Its success suggests that giant unidirectional language mod-
els together with appropriate manual prompt may work for
natural language understanding. However, handcrafting a
best-performing prompt is like ﬁnding a needle in a haystack,
which often requires impractically large validation sets. In
many cases, prompt engineering effectively means overﬁt-
ting the test set. Besides, it is easy to create adversarial
prompts that result in a substantial performance decrease.
In light of these problems, recent works have focused on au-
tomatically searching discrete prompts (Jiang et al., 2020b;
Shin et al., 2020; Reynolds & McDonell, 2021; Gao et al.,
2020) and demonstrated their effectiveness. However, since
neural networks are inherently continuous, discrete prompts
can be sub-optimal.
In this work, we propose a novel method– P-tuning– to
automatically search prompts in the continuous space to
bridge the gap between GPTs and NLU applications.1 P-
tuning leverages few continuous free parameters to serve as
prompts fed as the input to the pre-trained language models.
We then optimize the continuous prompts using gradient
descent as an alternative to discrete prompt searching.
The simple P-tuning method brings substantial improve-
ments to GPTs. We examine the P-tuning based GPTs on
1Codes
will
be
at
https://github.com/THUDM/
P-tuning
arXiv:2103.10385v1  [cs.CL]  18 Mar 2021

GPT Understands, Too
two NLU benchmarks: the LAMA (Petroni et al., 2019)
knowledge probing and SuperGLUE (Wang et al., 2019b).
In LAMA knowledge probing where model parameters are
ﬁxed, compared to original handcraft prompts, GPTs based
on P-tuning show absolute gains of 26.2%-41.1% in Pre-
cision@1. The best one achieves 64.2% in LAMA, which
signiﬁcantly surpasses the state-of-the-art 45.2% prompt
searching approach. In another NLU benchmark, Super-
Glue, we jointly apply the P-tuning and ﬁne-tuning in both
few-shot and fully supervised scenarios. As a result, GPTs
present a competitive performance to BERT models with the
same scales, and for some datasets, GPTs even outperform
BERTs. Further experiments demonstrate that BERT-style
models can also beneﬁt from P-tuning to some extent. We
show that ALBERT with P-tuning substantially outperforms
previous approaches and achieves new state-of-the-art re-
sults on the few-shot SuperGLUE benchmark.
Our discovery breaks the stereotype that GPTs can only gen-
erate but do not understand. It also suggests that language
models contain much more world knowledge and prior task
knowledge than we previously assumed. P-tuning also
serves as a general method to tune pre-trained language
models for the best downstream task performance. To sum
up, we make the following contributions:
• We show that GPTs can be as competitive as BERTs
in natural language understanding (sometimes even
better) with P-tuning, which can boost pre-trained lan-
guage models’ performance. This reveals that the po-
tential of GPT-style architectures for natural language
understanding has been under-estimated.
• We show that P-tuning is a general method to im-
prove GPTs and BERTs in both few-shot and fully-
supervised settings.
Particularly, with P-tuning,
our method outperforms state-of-the-art methods on
LAMA knowledge probing and few-shot SuperGlue,
which indicates that language models have grasped
more world knowledge and prior-task knowledge dur-
ing pre-training than we previously thought.
2. Motivation
The miracle of GPT-3 (Brown et al., 2020) and DALL-
E (Ramesh et al., 2021) seem to suggest that giant models
are always nothing short of a panacea for boosting machine
intelligence. However, behind the prosperity, there are unig-
norable challenges.
A fatal one is that giant models suffer from poor transfer-
ability. Fine-tuning on downstream tasks hardly works for
those trillion-scale models. Even for the many-shot ﬁne-
tuning setting, these models are still too large to memorize
the ﬁne-tuning samples (Yue et al., 2020) quickly.
As a substitution, GPT-3 and DALL-E have been reported to
leverage handcrafted prompts to steer the model for down-
stream applications. However, handcraft prompt searching
heavily relies on impractically large validations sets, but
its performance is also volatile. We show a similar case in
LAMA (Petroni et al., 2019) knowledge probing (Table 1),
where a single-word’s change can cause a drastic difference.
Prompt
P@1
[X] is located in [Y]. (original)
31.29
[X] is located in which country or state? [Y].
19.78
[X] is located in which country? [Y].
31.40
[X] is located in which country? In [Y].
51.08
Table 1. Case study on LAMA-TREx P17 with bert-base-cased. A
single-word change in prompts could yield a drastic difference.
In light of the challenge, while some recent works have
concentrated on automating the search of discrete prompts
by mining training corpus (Jiang et al., 2020b), gradient
searching (Shin et al., 2020) and using separate model (Gao
et al., 2020), we delve into the problem of ﬁnding continuous
prompts that can be differentially optimized.
3. Method: P-tuning
In this section, we present the implementation of P-tuning.
Similar to discrete prompts, the P-tuning only applies non-
invasive modiﬁcation to the input. Nevertheless, the P-
tuning replaces the input embeddings of pre-trained lan-
guage models with its differential output embeddings.
3.1. Architecture
Given a pre-trained language model M, a sequence of dis-
crete input tokens x1:n = {x0, x1, ..., xn} will be mapped
to input embeddings {e(x0), e(x1), ..., e(xn)} by the pre-
trained embedding layer e ∈M. In a speciﬁc scenario,
condition on the context x, we often use the output embed-
dings of a set of target tokens y for downstream processing.
For instance, in the pre-training, x refers to the unmasked
tokens while y refers to the [MASK] ones; and in the sen-
tence classiﬁcation, x refers to the sentence tokens while y
often refers to the [CLS].
The function of a prompt p is to organize context x, target
y and itself into a template T. For example, in the task
of predicting a country’s capital (LAMA-TREx P36), a
template may be “The capital of Britain is [MASK].” (see
Figure 2), in which “The capital of ... is ... .” is prompt,
“Britain” is the context and “[MASK]” is the target. Prompts
can be so ﬂexible that we may even insert them into the
context or target.
Let V refers to the vocabulary of a language model M and
[Pi] refers to the ith prompt token in a template T. For sim-

GPT Understands, Too
Pre-trained Language Model
(GPT, BERT, …
Prompt Encoder
[P0]
[Pi]
[Pi+1]
[Pm]
…
…
h0
hi
…
hi+1
hm
…
Pseudo Prompts
Input embedding
Back 
Propagation
Pre-trained Language Model
(GPT, BERT, …
Prompt Generator
Input embedding
Discrete rewards
Britain
[MASK]
The
capital
of
is
e(Britain)
e(The) e(capital) e(of)
e(is)
e([MASK])
Britain
[MASK]
e(Britain)
e([MASK])
(a) Discrete Prompt Search
(b) P-tuning
capital
e(capital)
Figure 2. An example of prompt search for “The capital of Britain is [MASK]”. Given the context (blue zone, “Britain”) and target (red
zone, “[MASK]”), the orange zone refer to the prompt tokens. In (a), the prompt generator only receives discrete rewards; on the contrary,
in (b) the pseudo prompts and prompt encoder can be optimized in a differentiable way. Sometimes, adding few task-related anchor tokens
(such as “capital” in (b)) will bring further improvement.
plicity, given a template T = {[P0:i], x, [Pi+1:m], y}, com-
pared to traditional discrete prompts which satisfy [Pi] ∈V
and map the T into
{e([P0:i]), e(x), e([Pi+1:m]), e(y)}
(1)
P-tuning instead regards the [Pi] as pseudo tokens and map
the template to
{h0, ..., hi, e(x), hi+1, ..., hm, e(y)}
(2)
where hi(0 ≤i < m) are trainable embedding tensors. This
enables us to ﬁnd a better continuous prompts beyond the
original vocabulary V of M could express. Finally, with the
downstream loss function L, we can differentially optimize
the continuous prompt hi(0 ≤i < m) by
ˆh0:m = arg min
h
L(M(x, y))
(3)
3.2. Optimization
Although the idea of training continuous prompts is straight-
forward, in practice, it faces two optimization challenges:
1) Discreteness: the original word embedding e of M has
already become highly discrete after pre-training. If h is
initialized with random distribution and then optimized with
stochastic gradient descent (SGD), which has been proved to
only change the parameters in a small neighborhood (Allen-
Zhu et al., 2019), the optimizer would easily fall into local
minima. 2) Association: another concern would be, in-
tuitively, we believe the values of prompt embeddings hi
should be dependent on each other rather than independent.
We need some mechanism to associate prompt embeddings
with each other.
In light of the challenges, in the P-tuning we propose to also
model the hi as a sequence using a prompt encoder consists
of a very lite neural network that can solve the discreteness
and association problems. And in practice, we choose a bidi-
rectional long-short term memory networks (LSTM), with a
ReLU activated two-layer multilayer perceptron (MLP) to
encourage discreteness. Formally speaking, the real input
embeddings h′
i to the language model M is derived from
hi = MLP([−→
hi : ←−
hi])
= MLP([LSTM(h0:i) : LSTM(hi:m)])
(4)
Though the LSTM head’s use indeed adds some parame-
ters to the training of continuous prompts, the LSTM head
is several magnitude orders smaller than the pre-trained
model. Moreover, in the inference, we only need the output
embedding h and can discard the LSTM head.
Besides, we also ﬁnd that adding few anchor tokens helps
some NLU tasks in the SuperGLUE benchmark. For in-
stance, for RTE task, the token “?” within prompt template
“[PRE][prompt tokens][HYP]?[prompt tokens][MASK]” is
specially added as an anchor token and affects the perfor-
mance a lot. Usually such anchor words characterize each
component, where in this case “?” indicate that “[HYP]”
acts as an interrogation part.
4. Experiments
In this section, we conduct extensive experiments on
two widely acknowledged natural language understanding
benchmarks: LAMA (Petroni et al., 2019) knowledge prob-
ing and SuperGlue (Wang et al., 2019b). The encouraging
results show that P-tuning can substantially boost GPTs’
performance on natural language understanding, and BERT-
style models can also be improved with a smaller gain.
4.1. Knowledge Probing
Knowledge probing, or referred to as fact retrieval, evalu-
ates how much real-world knowledge has language models
gained from pre-training. The LAMA (Petroni et al., 2019)
dataset evaluates it with cloze tests created from triples
selected in the knowledge bases. For example, we will

GPT Understands, Too
Prompt type
Model
P@1
Original
(MP)
BERT-base
31.1
BERT-large
32.3
E-BERT
36.2
Discrete
LPAQA (BERT-base)
34.1
LPAQA (BERT-large)
39.4
AutoPrompt (BERT-base)
43.3
P-tuning
BERT-base
48.3
BERT-large
50.6
Model
MP
FT
MP+FT
P-tuning
BERT-base (109M)
31.7
51.6
52.1
52.3 (+20.6)
-AutoPrompt (Shin et al., 2020)
-
-
-
45.2
BERT-large (335M)
33.5
54.0
55.0
54.6 (+21.1)
RoBERTa-base (125M)
18.4
49.2
50.0
49.3 (+30.9)
-AutoPrompt (Shin et al., 2020)
-
-
-
40.0
RoBERTa-large (355M)
22.1
52.3
52.4
53.5 (+31.4)
GPT2-medium (345M)
20.3
41.9
38.2
46.5 (+26.2)
GPT2-xl (1.5B)
22.8
44.9
46.5
54.4 (+31.6)
MegatronLM (11B)
23.1
OOM∗
OOM∗
64.2 (+41.1)
* MegatronLM (11B) is too large for effective ﬁne-tuning.
Table 2. Knowledge probing Precision@1 on LAMA-34k (left) and LAMA-29k (right). P-tuning outperforms all the discrete prompt
searching baselines. And interestingly, despite ﬁxed pre-trained model parameters, P-tuning overwhelms the ﬁne-tuning GPTs in
LAMA-29k. (MP: Manual prompt; FT: Fine-tuning; MP+FT: Manual prompt augmented ﬁne-tuning; PT: P-tuning ).
transform the triple (Dante, born-in, Florence) into a cloze
sentence with the handcraft prompt “Dante was born in
[MASK].”, and then we ask language models to inference
the target. Because we want to evaluate knowledge gained
from pre-training, pre-trained language models’ parameters
are ﬁxed (i.e., not ﬁne-tuned).
4.1.1. DATASETS AND FORMULATION
Datasets. LAMA enforces all answers in single-token for-
mat. We ﬁrst adopt the original LAMA-TREx dataset, con-
sisting of 41 Wikidata relations and altogether 34,039 test-
ing triples (namely LAMA-34k, which covers all BERT
vocabularies). Because GPT and BERT’s vocabularies are
different, we set up another version of LAMA, which covers
the intersection of GPT and BERT’s vocabulary. This subset
adds up to about 29,000 testing triples, and we name it the
LAMA-29k.
As for training, all prompt searching approaches need some
additional data to train or ﬁnd the prompts. We follow the
setting in AutoPrompt (Shin et al., 2020), where the authors
construct a training set from the original TRE-x dataset.
This training set is similar to the test set but with a slightly
different answer distribution.
Evaluation. Originally, LAMA has provided a handcraft
prompt for each relation such as Table 1, which are effec-
tive but sub-optimal. For bidirectional masked language
models, we only need to replace the “[X]” with the subject
entity and “[Y]” with the [MASK] token; for unidirectional
language models such as GPT, following LAMA’s original
setting on Transformer-XL (Dai et al., 2019), we use the
network output just before the target position. In terms of
conducting P-tuning , we use a (3, sub,3, obj,3) template for
bidirectional models and (3, sub,3, obj) for unidirectional
models, where the number indicates the number of prompt
tokens. In this knowledge probing task, we do not use any
anchor tokens. During the training, we set the learning rate
to 1e-5 and use the Adam optimizer.
4.1.2. RESULTS
General performance. The results are presented in Ta-
ble 2. The P-tuning signiﬁcantly pushes the boundary of
knowledge probing from 43.3% to 50.6% in LAMA-34k
and 45.2% to a maximum of 64.2% in LAMA-29k. This
result strongly suggests that language models capture far
more knowledge than people previously believed by merely
ﬁnding a better prompt and without ﬁne-tuning. When P-
tuning is compared with previous discrete prompt searching
approaches such as AutoPrompt (Shin et al., 2020) and
LPAQA (Jiang et al., 2020b) on the same-size models, P-
tuning still outperforms them.
P-tuning v.s. Fine-tuning. It is not allowed to change the
pre-trained model’s parameters by ﬁne-tuning in traditional
knowledge probing. We seek to evaluate how much knowl-
edge has language models learned during pre-training. How-
ever, this work’s essential aspect is to compare P-tuning and
ﬁne-tuning, particularly on unidirectional language models
like GPT. We are especially interested in the following ques-
tion: Are unidirectional and bidirectional language models
gaining similar improvement from P-tuning ?
To make a comprehensive review on existing tuning meth-
ods, we include the following approaches: 1) Manual
Prompt (MP): use original handcraft prompts from LAMA.
2) Fine-tuning (FT): only to present the subject and ﬁne-
tune the model to predict the object. 3) Manual Prompt
with Fine-tuning (MP+FT): ﬁne-tuning the language model
with the handcraft prompts. 4) P-tuning : use continuous
prompts (while ﬁxing language models’ parameters).
We implement the four strategies in the LAMA-29k (see
Table 2, right), and we ﬁnd that P-tuning is comparable to or
better than ﬁne-tuning-based methods, which is surprising
but reasonable. The surprising thing is that ﬁne-tuning
should have been more potent since it tunes all language

GPT Understands, Too
models’ parameters, while P-tuning not. However, it is
also reasonable because, in terms of knowledge probing,
many facts can only be hard-coded rather than inferenced by
language models. The ﬁne-tuning of parameters might result
in catastrophic forgetting. On the contrary, P-tuning does
not change the pre-trained models’ parameters but evoke the
stored knowledge by ﬁnding a better continuous prompt.
Besides, it is interesting to see a clear gap between BERT
and GPT’s improvement to the P-tuning . Fine-tuning with
high-quality manual prompts (MP+FT) (Schick & Sch¨utze,
2020; Gao et al., 2020) has been proved to be quite effective,
which is also observed in our experiments. However, it is
surprising that GPTs do not beneﬁt from MP+FT as much as
from P-tuning as BERTs do. In other words, P-tuning shows
a better afﬁnity with unidirectional language models. In
terms of much larger models such as MegatronLM2 with
11 billion parameters, while ﬁne-tuning hardly works, P-
tuning is still applicable and achieve the state-of-the-art on
LAMA.
4.2. SuperGLUE
To evaluate P-tuning , we perform experiments on the Super-
GLUE (Wang et al., 2019a) benchmark. There are 8 natural
language understanding (NLU) tasks in total, and we focus
on 7 of them as (Schick & Sch¨utze, 2020), since the other
ReCoRD (Zhang et al., 2018) adopts no prompts, thus no
P-tuning . Tasks include question answering (BoolQ (Clark
et al., 2019a) & MultiRC (Khashabi et al., 2018)), textual en-
tailment (CB (De Marneffe et al., 2019) & RTE (Dagan et al.,
2005)), co-reference resolution (WiC (Pilehvar & Camacho-
Collados, 2018)), causal reasoning (COPA (Roemmele et al.,
2011)), and word sense disambiguation (WSC (Levesque
et al., 2012)).
For experimental settings, we consider both a fully-
supervised setting and a few-shot setting.
In the fully-
supervised setting, we use the entire training sets (Dtrain)
and use development sets (Ddev) for model selection
and hyper-parameter tuning.
For the few-shot setting,
the few-shot version of SuperGLUE (also known as
FewGlue) (Schick & Sch¨utze, 2020) is adopted. FewGLUE
is a subset of SuperGLUE, each task consisting of 32 train
data (Dtrain32) and an additional unlabeled set (Dunlabeled)
of different size ranging from 400 to 20000. Different from
previous work (Schick & Sch¨utze, 2020) which assumes
no development sets and adopts ﬁxed hyper-parameters ac-
cording to empirical selection (which essentially overﬁts
the test sets), we construct appropriate few-shot develop-
ment sets (denoted as Ddev32). Since it has been proven
that larger development sets confer additional signiﬁcant
advantages (Gao et al., 2020), Ddev32 is built by randomly
2Provided in fairseq: https://github.com/pytorch/
fairseq/tree/master/examples/megatron_11b
selecting samples from unused training data and is strictly
restricted to be no larger than the size of few-shot train sets.
We adopt the same evaluation metrics as (Schick & Sch¨utze,
2020).
We reformulate NLU tasks into blank ﬁlling tasks. Un-
like (Schick & Sch¨utze, 2020) that use patterns with human
hand-crafted prompts, P-tuning puts initial prompt embed-
dings in different positions within patterns and then ﬁne-
tunes the prompt embeddings together with the pretrained
models. For fully-supervised settings, we use the AdamW
optimizer with a linearly decayed learning rate. We perform
grid search of hyper-parameters and take the best combi-
nation on Ddev or Ddev32. Speciﬁcally, we take learning
rates from 1e-5, 2e-5, 3e-5 and batch sizes from 16, 32.
For small datasets (COPA, WSC, CB, RTE), we ﬁne-tune
pretrained models for 20 epochs. For larger datasets (WiC,
BoolQ, MultiRC), we reduce the number of training epochs
to be 10 as the model converges earlier. We evaluate the
performance of every epoch. We use early stopping to avoid
overﬁtting to the training data. For few-shot learning, we
use the same hyperparameters as (Schick & Sch¨utze, 2020)
except extending ﬁne-tuning steps to be 3500 (instead of
250 as (Schick & Sch¨utze, 2020)), since the ﬁne-tuning of
prompt embeddings requires even more steps.
P-tuning can be used on all unidirectional and bidirectional
models. We choose models with similar scales of total com-
putes for a fair comparison, where we choose to compare
BERT-base 3 with GPT2-base and compare BERT-large
with GPT2-medium. Models like RoBERTa have a simi-
lar model size but was trained with much larger compute,
which should be compared with larger-scale GPTs. This is
left to future work. Besides, for few-shot learning, we also
experiment with albert-xxlarge-v2, which is proved to be the
best-performed pretrained models for the few-shot setting
in (Schick & Sch¨utze, 2020). For each pretrained model, we
report the performance of standard ﬁnetuning (i.e., classiﬁ-
cation using [CLS] embeddings), PET ﬁnetuning (Schick &
Sch¨utze, 2020), PET zero-shot, and P-tuning .
4.2.1. FULLY-SUPERVISED LEARNING
The main results are shown in Table 3 and Table 4. Firstly,
for both bert-base-cased and bert-large-cased models, P-
tuning outperforms all the other bert-based models on 5
out of 7 tasks. Exceptions are WiC and MultiRC, where
P-tuning performs a little bit worse than standard ﬁne-tune.
Since both WiC and MultiRC have relatively large train sets,
we conjecture this can be attributed to the fact that standard
ﬁne-tune could take even more advantages from a larger
dataset than P-tuning . On the contrary, P-tuning appears to
be more beneﬁcial in low-resource settings. Similar obser-
3We use the cased version of BERT for comparison since the
vocabulary of GPT2 is also cased.

GPT Understands, Too
Method
BoolQ
CB
WiC
RTE
MultiRC
WSC
COPA
Avg.
(Acc.)
(Acc.)
(F1)
(Acc.)
(Acc.)
(EM)
(F1a)
(Acc.)
(Acc.)
BERT-base-cased (109M)
Fine-tuning
72.9
85.1
73.9
71.1
68.4
16.2
66.3
63.5
67.0
66.2
MP zero-shot
59.1
41.1
19.4
49.8
54.5
0.4
0.9
62.5
65.0
46.0
MP ﬁne-tuning
73.7
87.5
90.8
67.9
70.4
13.7
62.5
60.6
70.0
67.1
P-tuning
73.9
89.2
92.1
68.8
71.1
14.8
63.3
63.5
72.0
68.4
GPT2-base (117M)
Fine-tune
71.2
78.6
55.8
65.5
67.8
17.4
65.8
63.0
64.4
63.0
MP zero-shot
61.3
44.6
33.3
54.1
49.5
2.2
23.8
62.5
58.0
48.2
MP ﬁne-tuning
74.8
87.5
88.1
68.0
70.0
23.5
69.7
66.3
78.0
70.2
P-tuning
75.0
(+1.1)
91.1
(+1.9)
93.2
(+1.1)
68.3
(-2.8)
70.8
(-0.3)
23.5
(+7.3)
69.8
(+3.5)
63.5
(+0.0)
76.0
(+4.0)
70.4
(+2.0)
Table 3. Fully-supervised learning on SuperGLUE dev with base-scale models. MP refers to manual prompt. For a fair comparison, MP
zero-shot and MP ﬁne-tuning report results of a single pattern, while anchors for P-tuning are selected from the same prompt. Subscript in
red represents advantages of GPT with P-tuning over the best results of BERT.
Method
BoolQ
CB
WiC
RTE
MultiRC
WSC
COPA
Avg.
(Acc.)
(F1)
(Acc.)
(Acc.)
(Acc.)
(EM)
(F1a)
(Acc.)
(Acc.)
BERT-large-cased (335M)
Fine-tune∗
77.7
94.6
93.7
74.9
75.8
24.7
70.5
68.3
69.0
72.5
MP zero-shot
49.7
50.0
34.2
50.0
49.9
0.6
6.5
61.5
58.0
45.0
MP ﬁne-tuning
77.2
91.1
93.5
70.5
73.6
17.7
67.0
80.8
75.0
73.1
P-tuning
77.8
96.4
97.4
72.7
75.5
17.1
65.6
81.7
76.0
74.6
GPT2-medium (345M)
Fine-tune
71.0
73.2
51.2
65.2
72.2
19.2
65.8
62.5
66.0
63.1
MP zero-shot
56.3
44.6
26.6
54.1
51.3
2.2
32.5
63.5
53.0
47.3
MP ﬁne-tuning
78.3
96.4
97.4
70.4
72.6
32.1
74.4
73.0
80.0
74.9
P-tuning
78.9
(+1.1)
98.2
(+1.8)
98.7
(+1.3)
69.4
(-5.5)
75.5
(-0.3)
29.3
(+4.6)
74.2
(+3.7)
74.0
(-7.7)
81.0
(+5.0)
75.6
(+1.0)
* We report the same results taken from SuperGLUE (Wang et al., 2019b).
Table 4. Fully-supervised learning on SuperGLUE dev with large-scale models. MP refers to manual prompt. For fair comparison, MP
zero-shot and MP ﬁne-tuning report results of a single pattern, while anchors for P-tuning are selected from the same prompt. Subscripts
in red represents improvements of GPT with P-tuning over the best results of BERT.
vations are also shown in Section 4.2.2. Secondly, for both
gpt2-base and gpt2-medium models, P-tuning achieves the
most promising results among all gpt2-base models. Above
all, we can conclude that P-tuning can effectively boost the
NLU performance of both bert-based and gpt-based models.
Besides, under the scale of base models, gpt2-base with P-
tuning outperforms the best results of BERT-based models
on 6 out of 7 tasks while achieving comparable results on
WiC. Comparing with BERT-large models, GPT2-medium
with P-tuning shows advantages on 4 out of 7 tasks while
being comparable on RTE and WSC tasks. The only ex-
ception is the WiC task. It is noticed that on the WiC task,
standard ﬁne-tune shows the best results on different mod-
els with different settings. We speculate that it is because
the word sense disambiguation task is not appropriate for
prompt-based MLM prediction. Above all, we conclude
that with P-tuning , GPT2 achieves comparable and even
better performance as BERT-based models. The discovery
subverts our common belief that bidirectional models (such
as BERT) are always better at NLU tasks than unidirectional
models (such as GPT2).
4.2.2. FEW-SHOT LEARNING
Sub-optimal and Sensitive Manual Prompts. Initially,
PET/iPET (Schick & Sch¨utze, 2020) has achieved the state-
of-the-arts on SuperGLUE few-shot learning tasks with
several manually-written prompts, which are effective, but
sub-optimal and labor-intensive. For a comprehensive un-
derstanding of manual prompts, comparative experiments
are ﬁrst conducted. Table 6 shows the results of using differ-
ent manual prompts and P-tuning . First, results show that
the few-shot performance has no obvious correlations with
prompts’ semantics, format, grammar. Prompts that humans
consider reasonable is not necessarily effective for language
models. Second, minor changes in manual prompts would
cause substantial performance differences. Pretrained lan-
guage models are pretty sensitive to the choice of prompts.
We can conclude that manual handwriting prompts are more

GPT Understands, Too
Dev size
Method
BoolQ
CB
WiC
RTE
MultiRC
WSC
COPA
(Acc.)
(Acc.)
(F1)
(Acc.)
(Acc.)
(EM)
(F1a)
(Acc.)
(Acc.)
32
PET∗
73.2±3.1
82.9±4.3
74.8±9.2
51.8±2.7
62.1±5.3
33.6±3.2
74.5±1.2
79.8±3.5
85.3±5.1
PET best†
75.1
86.9
83.5
52.6
65.7
35.2
75.0
80.4
83.3
P-tuning
77.8
(+4.6)
92.9
(+10.0)
92.3
(+17.5)
56.3
(+4.5)
76.5
(+14.4)
36.1
(+2.5)
75.0
(+0.5)
84.6
(+4.8)
87.0
(+1.7)
Full
GPT-3
77.5
82.1
57.2
55.3
72.9
32.5
74.8
75.0
92.0
PET‡
79.4
85.1
59.4
52.4
69.8
37.9
77.3
80.1
95.0
iPET§
80.6
92.9
92.4
52.2
74.0
33.0
74.0
-
-
* We report the average and standard deviation of each candidate prompt’s average performance.
† We report the best performed prompt selected on full dev dataset among all candidate prompts.
‡ With additional ensemble and distillation.
§ With additional data augmentation, ensemble, distillation and self-training.
Table 5. Few-shot learning (32 train samples) on SuperGLUE dev. Previous few-shot learning approaches use the original full dev set
(Ddev) for validation, which does not make sense. We construct a new dev set (Ddev32) with 32 unused samples from original training
set. Under fair comparison, P-tuning signiﬁcantly outperforms PET (Ddev32) and PET best (Ddev32) on all tasks. More interestingly,
P-tuning even outperforms GPT-3, PET (Ddev) and iPET (Ddev) on 4 out of 7 tasks. Subscripts in red represents the improvements of
P-tuning over PET(Ddev32).
complicated than we thought. Moreover, Table 6 also proves
that it is impossible to ﬁnd the best-performed manual
prompts using Ddev32. This indicates it is also challenging
to pick the best manual prompts out in the few-shot setting.
In contrast, P-tuning appears promising in automatically
searching better prompts with far fewer hand-crafts.
Updated SOTA for SuperGLUE Few-shot Learning. Ta-
ble 5 presents the latest state-of-the-art results for Super-
GLUE few-shot learning, achieved by P-tuning . We com-
pared it with several baselines, including PET (Schick &
Sch¨utze, 2020) and GPT-3 (Brown et al., 2020), which
achieves previous SuperGLUE Few-shot SOTA.
It is worth noting that, aside from manual prompt ﬁne-
tuning, the original PET (Schick & Sch¨utze, 2020) adopts
multiple additional technologies, including data augmenta-
tion, ensemble, and distillation, to boost the performance.
Besides, it performs model selection and hyper-parameter
tuning by over-ﬁtting test sets. To ensure a fair comparison,
PET is re-experimented under our Ddev32 setting, remov-
ing all auxiliary technologies (data augmentation, ensem-
ble, and distillation). Considering PET provides multiple
manual prompts, both averaged performance, and the best-
performed-prompt performance are reported.
Table 5 illustrates that P-tuning consistently outperforms
PET (Ddev32) and PET-best (Ddev32) with manual prompts
on all tasks.
The improvements of solution over PET
(Ddev32) are larger than the standard deviations over multi-
ple patterns on 5 out of 7 tasks, proving that P-tuning can
search far better prompts than manual ones and signiﬁ-
cantly improve few-shot task performance. On tasks in-
cluding CB,WiC,RTE and WSC, P-tuning even outperforms
PET/iPET (Ddev), which adopt auxiliary technologies (data
augmentation, ensemble and distillation). Compared with
GPT-3, with much larger scale than P-tuning (albert-xxlarge-
v2), P-tuning outperforms on 6 out of 7 tasks. Results
demonstrate the advantages of P-tuning in few-shot NLU
tasks.
4.2.3. FINETUNING V.S. MP FINETUNING V.S.
P-TUNING
Table 3 and Table 4 present results of three tuning-based
paradigms for improving NLU performance. We are par-
ticularly interested in how these tuning-based paradigms
perform differently. Overall, P-tuning outperforms ﬁne-
tuning and MP ﬁne-tuning on average by around 2 points
on BERT-based models and more than 5 points on GPT2-
based models. Speciﬁcally, though P-tuning achieves the
best results on most of the tasks, ﬁne-tuning can outper-
form on tasks (e.g., WiC) that are hard to formulate as
cloze questions. Comparing P-tuning and MP ﬁne-tuning,
P-tuning generally shows more advantages than MP ﬁne-
tuning on average, since it is tricky for MP ﬁne-tuning to
ﬁnd good manual prompts. In contrast, P-tuning can always
automatically search for better prompts.
As a new paradigm of tuning pretrained models, P-
tuning could search over a vast prompt space while tun-
ing pretrained models’ parameters. Results demonstrate its
competitive potential in prompting larger-scale pre-trained
models that are hard to ﬁne-tune.
5. Related work
5.1. Pre-trained Language Models
The recent breakthrough in self-supervised (Liu et al., 2020)
pre-trained language models has boosted the development
of natural language processing. GPT (Radford et al., 2019)
ﬁrst leverages the transformer architecture to pre-train on

GPT Understands, Too
Prompt
Ddev Acc.
Ddev32 Acc.
Does [PRE] agree with [HYP]? [MASK].
57.16
53.12
Does [HYP] agree with [PRE]? [MASK].
51.38
50.00
Premise: [PRE] Hypothesis: [HYP] Answer: [MASK].
68.59
55.20
[PRE] question: [HYP]. true or false? answer: [MASK].
70.15
53.12
P-tuning
76.45
56.25
Table 6. Few-shot performance comparison of different manual prompts and tuned prompts on RTE tasks using albert-xxlarge-v2.
Experiments use Ddev32 for model selection and hyper-parameter tuning and evaluate on Ddev. There’s no obvious correlations between
manual prompts and performance. Besides, Ddev32 is not able to select the best manual prompts.
large-scale web texts. BERT (Devlin et al., 2018) proposes
the masked language modeling and creates the pre-train/ﬁne-
tuning paradigm. Later on, various kinds of language mod-
els grown up, including XLNet (Yang et al., 2019) which in-
novates the permutation language modeling. RoBERTa (Liu
et al., 2019) conducts detailed experiments to demonstrate
useful techniques related to pre-training. BART (Lewis
et al., 2019), T5 (Raffel et al., 2019) and UniLM (Dong
et al., 2019) which try to unify the language understanding
and generation.
5.2. Language Models as Knowledge Base
Since the birth of language models, researchers have ob-
served that they not only learn contextualized text represen-
tations but also various types and amounts of knowledge,
including linguistic and world knowledge. (Hewitt & Man-
ning, 2019) demonstrates that contextualized representation
produced by language models can form a parsing tree in
the embedding space. (Vig, 2019; Clark et al., 2019b) look
into the multi-head attention internal transformers and dis-
cover that certain attention heads may correspond to some
grammatical functions, including co-reference and noun
modiﬁers.
Another important stream is about how much world knowl-
edge or factual knowledge has language models learned.
LAMA (Petroni et al., 2019; 2020) propose to leverage
cloze test transformed from fact triples in knowledge bases
to examine language model’s ability in memorizing facts
with answers in the single-token format. In (Wang et al.,
2020), the authors investigate the attention matrices to ﬁnd
that the attentions would also indicate knowledge triples con-
tained in the context and thus develop an open knowledge
graph construction framework. (Jiang et al., 2020a) based
on LAMA develops a multi-token fact retrieval dataset.
5.3. Language Model Prompting
The birth of GPT-3 (Brown et al., 2020) has blown peo-
ple’s minds with its outstanding performance in multi-task
and few-shot learning. However, GPT-3 is not designed
for ﬁne-tuning, and it heavily relies on handcraft prompts
(or the in-context learning (Liu et al., 2021; Zhao et al.,
2021)) to transfer to downstream tasks. To better apply
large language models to natural language understanding
(NLU), recent works have concentrated on automating the
search of discrete prompts by mining training corpus (Jiang
et al., 2020b), token-based gradient searching (Shin et al.,
2020) and using separate model (Gao et al., 2020) such as
T5 to generate prompts. However, the search over discrete
space is challenging to optimize and sub-optimal due to the
continuous nature of neural networks.
Recently, (Li & Liang, 2021) propose preﬁx-tuning for natu-
ral language generation (NLG) tasks, which adopts a similar
strategy to our P-tuning to train continuous prompt. Nev-
ertheless, they are different in several aspects. First, preﬁx-
tuning is designed for NLG and GPTs, while P-tuning tar-
gets NLU and all types of language models. Second, preﬁx-
tuning only allows adding prompt tokens at the beginning
of the input sequence, while P-tuning can insert the tokens
anywhere. Third, preﬁx-tuning invasively concatenates con-
tinuous prompt tokens in every layer of the transformer
because the authors ﬁnd mere prompting in the input does
not take effect; on the contrary, P-tuning non-invasively
adds continuous prompts only in the input to work well.
Finally, P-tuning also introduces how to use anchor prompts
for further improvement. Despite the differences, we believe
that both our P-tuning and preﬁx-tuning point out that learn-
ing continuous prompts is useful and superior to discrete
prompt searching.
6. Conclusion
In this paper, we present–P-tuning–which augments pre-
trained model’s ability in natural language understanding
by automatically searching better prompts in the continuous
space. Our P-tuning method relies less on a large validation
set, suffers less from adversarial prompts, and alleviates
over-ﬁtting. We show that our P-tuning method can recover
64% (P@1) of world knowledge from a pre-trained lan-
guage model without any additional text provided during
test time. On the SuperGLUE benchmark, P-tuning en-
dows GPT-style models to show competitive performance
with similar-size BERTs in natural language understanding,
which is assumed impossible in the past. P-tuning also helps
on bidirectional models and consequently outperforms state-
of-the-art methods in the few-shot SuperGlue benchmark. It

GPT Understands, Too
also proves that language models effectively capture more
world knowledge and prior-task knowledge than we thought
during pre-training.
References
Allen-Zhu, Z., Li, Y., and Song, Z. A convergence theory for
deep learning via over-parameterization. In International
Conference on Machine Learning, pp. 242–252. PMLR,
2019.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020.
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
M., and Toutanova, K. Boolq: Exploring the surprising
difﬁculty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and Short
Papers), pp. 2924–2936, 2019a.
Clark, K., Khandelwal, U., Levy, O., and Manning, C. D.
What does bert look at? an analysis of bert’s attention.
arXiv preprint arXiv:1906.04341, 2019b.
Dagan, I., Glickman, O., and Magnini, B. The pascal recog-
nising textual entailment challenge. In Machine Learning
Challenges Workshop, pp. 177–190. Springer, 2005.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and
Salakhutdinov, R. Transformer-xl: Attentive language
models beyond a ﬁxed-length context. arXiv preprint
arXiv:1901.02860, 2019.
Davison, J., Feldman, J., and Rush, A. M. Commonsense
knowledge mining from pretrained models. In Proceed-
ings of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 1173–1178, 2019.
De Marneffe, M.-C., Simons, M., and Tonhauser, J. The
commitmentbank: Investigating projection in naturally
occurring discourse. In proceedings of Sinn und Bedeu-
tung, volume 23, pp. 107–124, 2019.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805,
2018.
Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y.,
Gao, J., Zhou, M., and Hon, H.-W. Uniﬁed language
model pre-training for natural language understanding
and generation. arXiv preprint arXiv:1905.03197, 2019.
Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-
guage models better few-shot learners. arXiv preprint
arXiv:2012.15723, 2020.
Hewitt, J. and Manning, C. D. A structural probe for ﬁnd-
ing syntax in word representations. In North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies (NAACL). Association
for Computational Linguistics, 2019.
Jiang, Z., Anastasopoulos, A., Araki, J., Ding, H., and Neu-
big, G. X-factr: Multilingual factual knowledge retrieval
from pretrained language models. In Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pp. 5943–5959, 2020a.
Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we
know what language models know? Transactions of the
Association for Computational Linguistics, 8:423–438,
2020b.
Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., and
Roth, D. Looking beyond the surface: A challenge set for
reading comprehension over multiple sentences. In Pro-
ceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers),
pp. 252–262, 2018.
Langley, P. Crafting papers on machine learning. In Langley,
P. (ed.), Proceedings of the 17th International Conference
on Machine Learning (ICML 2000), pp. 1207–1216, Stan-
ford, CA, 2000. Morgan Kaufmann.
Levesque, H., Davis, E., and Morgenstern, L. The winograd
schema challenge. In Thirteenth International Confer-
ence on the Principles of Knowledge Representation and
Reasoning. Citeseer, 2012.
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.
Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.
Li, X. L. and Liang, P. Preﬁx-tuning: Optimizing continuous
prompts for generation. arXiv preprint arXiv:2101.00190,
2021.
Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen,
W. What makes good in-context examples for gpt-3?
arXiv preprint arXiv:2101.06804, 2021.
Liu, X., Zhang, F., Hou, Z., Wang, Z., Mian, L., Zhang,
J., and Tang, J. Self-supervised learning: Generative or
contrastive. arXiv preprint arXiv:2006.08218, 1(2), 2020.

GPT Understands, Too
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692, 2019.
Petroni, F., Rockt¨aschel, T., Lewis, P., Bakhtin, A., Wu,
Y., Miller, A. H., and Riedel, S. Language models as
knowledge bases?
arXiv preprint arXiv:1909.01066,
2019.
Petroni, F., Lewis, P., Piktus, A., Rockt¨aschel, T., Wu,
Y., Miller, A. H., and Riedel, S. How context affects
language models’ factual predictions.
arXiv preprint
arXiv:2005.04611, 2020.
Pilehvar, M. T. and Camacho-Collados, J. Wic: 10, 000
example pairs for evaluating context-sensitive representa-
tions. CoRR, abs/1808.09121, 2018.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. OpenAI blog, 1(8):9, 2019.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad-
ford, A., Chen, M., and Sutskever, I. Zero-shot text-
to-image generation. arXiv preprint arXiv:2102.12092,
2021.
Reynolds, L. and McDonell, K. Prompt programming for
large language models: Beyond the few-shot paradigm.
arXiv preprint arXiv:2102.07350, 2021.
Roemmele, M., Bejan, C. A., and Gordon, A. S. Choice
of plausible alternatives: An evaluation of commonsense
causal reasoning. In AAAI Spring Symposium: Logical
Formalizations of Commonsense Reasoning, pp. 90–95,
2011.
Schick, T. and Sch¨utze, H. It’s not just size that matters:
Small language models are also few-shot learners. Com-
puting Research Repository, arXiv:2009.07118, 2020.
Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and
Singh, S. Autoprompt: Eliciting knowledge from lan-
guage models with automatically generated prompts.
arXiv preprint arXiv:2010.15980, 2020.
Vig, J. A multiscale visualization of attention in the trans-
former model. arXiv preprint arXiv:1906.05714, 2019.
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,
Michael, J., Hill, F., Levy, O., and Bowman, S. R. Su-
perGLUE: A Stickier Benchmark for General-Purpose
Language Understanding Systems. In NeurIPS 2019, pp.
3261–3275, 2019a.
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A.,
Michael, J., Hill, F., Levy, O., and Bowman, S. R. Super-
glue: A stickier benchmark for general-purpose language
understanding systems. arXiv preprint arXiv:1905.00537,
2019b.
Wang, C., Liu, X., and Song, D. Language models are open
knowledge graphs. arXiv preprint arXiv:2010.11967,
2020.
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov,
R., and Le, Q. V. Xlnet: Generalized autoregressive
pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.
Yue, Z., Zhang, H., Sun, Q., and Hua, X.-S. Interventional
few-shot learning.
arXiv preprint arXiv:2009.13000,
2020.
Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., and Van Durme,
B. Record: Bridging the gap between human and machine
commonsense reading comprehension. arXiv preprint
arXiv:1810.12885, 2018.
Zhao, T. Z., Wallace, E., Feng, S., Klein, D., and Singh, S.
Calibrate before use: Improving few-shot performance
of language models. arXiv preprint arXiv:2102.09690,
2021.

