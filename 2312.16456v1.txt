Procedia Computer Science 00 (2023) 1–35
Procedia
Computer
Science
www.elsevier.com/locate/procedia
Adaptive trajectory-constrained exploration strategy for deep
reinforcement learning
Guojian Wanga,b,c, Faguo Wub,c,d,e,∗, Xiao Zhanga,b,c,e,∗∗, Ning Guoa,b,c, Zhiming
Zhengb,c,d,e
aSchool of Mathematical Sciences, Beihang University, Beijing 100191, China
bKey Laboratory of Mathematics, Informatics and Behavioral Semantics, Ministry of Education, Beijing 100191, China
cPeng Cheng Laboratory, Shenzhen 518055, Guangdong, China
dInstitute of Artificial Intelligence, Beihang University, Beijing 100191, China
eZhongguancun Laboratory, Beijing 100194, China
Abstract
Deep reinforcement learning (DRL) faces significant challenges in addressing the hard-exploration problems in tasks
with sparse or deceptive rewards and large state spaces. These challenges severely limit the practical application of DRL.
Most previous exploration methods relied on complex architectures to estimate state novelty or introduced sensitive
hyperparameters, resulting in instability. To mitigate these issues, we propose an efficient adaptive trajectory-constrained
exploration strategy for DRL. The proposed method guides the policy of the agent away from suboptimal solutions by
leveraging incomplete offline demonstrations as references. This approach gradually expands the exploration scope of
the agent and strives for optimality in a constrained optimization manner. Additionally, we introduce a novel policy-
gradient-based optimization algorithm that utilizes adaptively clipped trajectory-distance rewards for both single- and
multi-agent reinforcement learning. We provide a theoretical analysis of our method, including a deduction of the worst-
case approximation error bounds, highlighting the validity of our approach for enhancing exploration. To evaluate the
effectiveness of the proposed method, we conducted experiments on two large 2D grid world mazes and several MuJoCo
tasks. The extensive experimental results demonstrate the significant advantages of our method in achieving temporally
extended exploration and avoiding myopic and suboptimal behaviors in both single- and multi-agent settings. Notably,
the specific metrics and quantifiable results further support these findings. The code used in the study is available at
https://github.com/buaawgj/TACE.
© 2023 Published by Elsevier Ltd.
Keywords: deep reinforcement learning, hard-exploration problem, policy gradient, offline suboptimal demonstrations
1. Introduction
Deep reinforcement learning has achieved considerable success in various fields over the past few years,
for example, playing Atari games with raw pixel inputs [1, 2], mastering the game of Go [3], and acquiring
∗Corresponding author at: Institute of Artificial Intelligence, Beihang University, Beijing 100191, China.
E-mail address:
faguo@buaa.edu.cn
∗∗Corresponding author at: School of Mathematical Sciences, Beihang University, Beijing 100191, China.
E-mail address:
xiao.zh@buaa.edu.cn
arXiv:2312.16456v1  [cs.LG]  27 Dec 2023

2
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
complex robotic manipulation and locomotion skills from raw sensory data [4, 5, 6]. Despite these success
stories, these DRL algorithms may suffer from poor performance in tasks with sparse and deceptive rewards,
and large state spaces[7, 8]. We refer to this as a hard-exploration problem, which has received increasing
attention [9, 10]. Such hard-exploration tasks are common in the real world. For example, in a navigation
task, a reward is only received after the agent collects certain items or reaches terminal points. Our proposed
method can be applied to such tasks and help agents explore their environment more systematically.
The hard-exploration problem of reinforcement learning (RL) can be formally defined as an exploration
in environments where rewards are sparse and even deceptive [11, 12]. For both single- and multi-agent RL
methods, the difficulty lies primarily in the fact that random exploration rarely results in terminated states or
meaningful feedback collection. The sparsity of rewards makes the training of neural networks extremely
inefficient [8, 13], because there are no sufficient and immediate rewards as supervisory signals to guide the
training of neural networks. The challenge is trickier when troublesome deceptive rewards exist in these
tasks because they can lead to myopic behaviors; hence, the agent may often lose the chance to obtain a
higher score [11]. Efficient exploration is the key to solving these problems by encouraging the agent to
visit underexplored states.
Generally, sample efficiency and premature convergence interfere with the exploration of DRL algo-
rithms in environments with sparse and deceptive rewards and large state spaces [8, 11, 14, 15]. First, when
we use DRL algorithms to solve these hard-exploration tasks in single- and multi-agent settings, such as
ϵ-greedy [1, 16, 17] or uniform Gaussian exploration noise [6, 18], may cause an exponential difference in
the sampling complexity for different goals [19], which places a high demand on computing power. Second,
when an agent frequently collects trajectories for goals with deceptive rewards, it tends to adopt myopic
behaviors and learn suboptimal policies. Under the current reinforcement learning paradigm, the agent
further limits its exploration to small regions of the state space around suboptimal goals because of the
myopic behaviors learned from previous experiences [14]. Therefore, the agent will permanently lose the
opportunity to achieve a higher score and become stuck in local optima [11].
In this study, we developed a novel TrAjectory-Constrained Exploration (TACE) method to overcome
these challenges. Our method exploits offline suboptimal demonstration data for faster and more efficient
exploration without incurring high computational costs and ensuring stability. Our approach orients its
policy away from suboptimal policies in the perspective of constrained optimization by considering offline
data as a reference. We developed three practical policy-gradient-based algorithms, TCPPO, TCHRL, and
TCMAE, with clipped novelty distance rewards. These algorithms can search for a new policy whose state-
action visitation distribution is different from the policies represented by offline data. Furthermore, the
scale of novelty for a state-action pair is determined based on the maximum mean discrepancy, and this
definition of novelty is comparable. Therefore, a distance normalization method was introduced to enable
the agent to gradually expand the scope of exploration centered on past trajectories. Thus, the proposed
method adaptively adjusts constrained boundaries. To further improve algorithm performance, an adaptive
scaling method was developed to ensure that the agent remained inside the feasible region. We then provide
a theoretical analysis of our algorithm, deduce the worst-case approximation error bound and theoretically
determine the range of hyperparameter values. Finally, sufficient experimental results demonstrated the
effectiveness of TACE compared with other state-of-the-art baseline methods for various benchmarking RL
tasks.
In summary, our contributions are summarized as follows:
1. This paper investigated a trajectory-constrained exploration strategy that promotes sample efficiency
and avoids premature convergence for the hard-exploration challenge of single- and multi-agent tasks.
2. We show a feasible instance where offline suboptimal demonstrations are used as a reference to pro-
vide dense and sustainable exploration guidance and enhance the sample efficiency of RL methods.
3. No additional neural networks are required to model the novelty. Our proposed algorithm is simple
in form and explicit in physical meaning, which helps adjust constrained boundaries and expand the
exploration scope adaptively.

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
3
4. A theoretical analysis of our method is provided, explaining the validity of TACE in achieving diverse
exploration and the rationality of the TACE design.
5. The proposed methods were evaluated for various benchmarking tasks, including two large 2D grid
world mazes and two MuJoCo mazes. Our method outperforms other advanced baseline methods in
terms of exploration efficiency and average returns.
The remainder of this paper is organized as follows: Section 2 describes the progress of the related work.
Section 3 briefly describes the preliminary knowledge of the article. Section 4 introduces the proposed
trajectory-constrained exploration strategy. The experimental results are presented in Section 7. Finally, the
conclusions are presented in Section 8.
2. Related work
Several methods have been proposed in previous works to encourage sufficient exploration of the agent.
Some studies suggest adding noise sampled from a stochastic distribution, such as the Gaussian distribu-
tion [6, 18] or the Ornstein-Unlenbeck process [4], to actions generated by the policy network, which can
motivate the agent to access underexplored areas. Maximum entropy RL methods [20, 21] allow the agent
to explore the environment by encouraging high-entropy distributions over action spaces, given the state in-
puts. However, such methods are unlikely to achieve satisfactory performance and may result in suboptimal
behavior in tasks with sparse and deceptive rewards, long horizons, and large state spaces [22].
At the same time, some methods use intrinsic rewards [23], such as surprise [24] and curiosity [25], to
encourage the agent to visit unfamiliar states. For example, curiosity in [26] was formulated as an error in an
agent’s ability to predict the consequences of its actions in a visual feature space. Although these methods
can alleviate the sparse reward problem, they usually rely on auxiliary models to calculate intrinsic rewards,
and therefore increase the complexity of the entire model and may incur high computational costs. Another
way to cope with sparse or delayed rewards is to define a reward function as a combination of shaping
and terminal rewards [27]. However, these methods need to introduce extra hyperparameters to balance the
weight of importance between the RL task rewards and intrinsic rewards, which might incur instability.
Diversity-regularized exploration expands an agent’s exploration space efficiently, and there have been
several recent studies in this area [9, 14, 10]. For instance, collaborative exploration [14] employs a team of
heterogeneous agents to explore an environment and utilizes a special regularization mechanism to maintain
team diversity. Diversity-driven exploration [9] proposes the addition of a Kullback-Leibler (KL) divergence
regularization term to encourage the DRL agent to attempt policies that differ from previously learned
policies. Other studies have enabled RL agents to perform exploration more consistently without incurring
additional computational costs by adding random noise to the network parameters [28, 29]. However, the
diversity term in these methods only considers the divergence in the action space.
DIPG [10] uses a maximum mean discrepancy (MMD) regularization term to encourage the agent to
learn a novel policy that induces a different distribution over the trajectory space. Specifically, the MMD
distance was introduced between the trajectory distribution of the current policy and that of the previously
learned policies. Moreover, DIPG considers the diversity gradient during training because it adds an MMD
regularization term to its objective function. Therefore, its objective function incurs sensitive hyperparam-
eters that cause instability and even lead to failure to solve the specified task. There are many differences
between our proposed method and DIPG, including the definition of the distance measure, formulation of
the optimization problem, and design of the optimization solution method. These significant differences are
described in detail in Section 4.
Reinforcement Learning from Demonstrations (RLfD) has been proven to be an effective approach for
solving problems that require sample efficiency and involve difficult exploration. Demonstrations of RLfD
were generated by either the expert or the agent. For example, self-imitation learning [30] demonstrates that
exploiting previous good experiences can indirectly drive deep exploration. Deep q-learning from demon-
strations (DQfD) [31] leverages even very small amounts of demonstration data to accelerate learning and
enhance the exploration of the agent. Recurrent Replay Distributed DQN from Demonstrations (R2D3) [32]

4
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
extracts information from expert demonstrations in a manner that guides an agent’s autonomous exploration
in the environment. The Learning Online with Guidance Offline (LOGO) algorithm [33] orients the up-
date of its policy by obtaining guidance from offline data, which can significantly reduce the number of
exploration actions in sparse reward settings. These methods train the output of the current policies to
be close to that of expert policies represented by the demonstration data. However, the cost of obtaining
expert demonstration data may be high, and it is often hopeless for the agent to generate highly rewarded
trajectories in some hard-exploration tasks.
HRL has long been recognized as a promising approach to overcoming sparse reward and long-horizon
problems [34, 35, 36, 37]. Recent studies proposed a range of HRL methods to efficiently learn policies
in long-horizon tasks with sparse rewards [38, 39, 8, 40]. Under this paradigm, the state-action search
space for the agent is exponentially reduced through several modules of abstraction at different levels, and
some subsets of these modules may be suitable for reuse [8]. Currently, HRL methods are divided into
two main categories. The first category is subgoal-based HRL methods, in which the high-level policy sets
a subgoal for the low-level policy to achieve. Distance measurement is required to measure the internal
rewards of low-level policies according to current states and subgoals. Some algorithms, such as HAC [38]
and HIRO [39], simply use the Euclidean distance, while FeUdal Networks (FuNs) [41] adopt the cosine
distance. However, these measurements of state spaces do not necessarily reflect the ”true” distance between
two states; therefore, these algorithms are sensitive to state-space representation [42].
The second category of HRL methods allows a high-level policy to select a pre-trained low-level skill
over several time steps. Therefore, they typically require training in high- and low-level policies for different
tasks. Moreover, low-level skills are pre-trained by maximizing diversity objectives [43], proxy rewards [8],
or specialized simple tasks [40]. When solving downstream tasks, pre-trained skills are often frozen and
only high-level policies are trained, which may lead to significant suboptimality in future tasks [44]. The
Option-Critic algorithm [45] makes an effort to train high-level and low-level policies jointly. However,
joint training may lead to semantic loss of high-level policies [41] and the collapse of low-level skills [46].
Although the single-agent exploration problem is extensively studied and has achieved considerable
success, few exploration strategies have been developed for multi-agent reinforcement learning (MARL).
MAVEN [47] encodes a shared latent variable with a hierarchical policy and learns several separate state-
action value functions for each agent. EITI [48] uses mutual information (MI) to capture the influence
of one agent’s behavior on expected returns of the multi-agent team. Previous work [15] demonstrates that
exploiting structural information on the reward function in MARL tasks can promote exploration. EMC [49]
introduces a curiosity-driven exploration for episodic MARL by utilizing the results in the episodic memory
to regularize the loss function.
3. Preliminaries
3.1. Reinforcement Learning
We consider an infinite-horizon discounted Markov decision process (MDP) defined by a tuple M =
(S, A, P, Re, ρ0, γ), where S is a state space, A is a (discrete or continuous) action space, P : S×A×S →R+
is the transition probability distribution, Re : S × A →[Rmin, Rmax] is the reward function, ρ0 : S →R+ is
the distribution of the initial state s0, and γ ∈[0, 1] is a discount factor. A stochastic policy πθ : S →P(A)
parametrized by θ, maps the state space S to a set of probability distributions over the action space A. The
standard state-action value function Qe, the value function Ve and the advantage function Ae are defined as
follows:
Qe(st, at) = Est+1,at+1,...

∞
X
l=0
γlRe(st+l, at+l)
,
Ve(st) = Eat,st+1,at+1,...

∞
X
l=0
γlRe(st+l, at+l)
,
Ae(s, a) = Qe(s, a) −Ve(s),

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
5
where st ∼π(at|st), st+1 ∼P(st+1|st, at), ∀t ≥0.
Generally, the objective of the RL algorithm is to determine the optimal policy πθ that maximizes the
expected discounted return.
J(πθ) = Eτ

∞
X
t=0
γtRe(st, at)
,
here we use τ = (s0, a0, s1, a1, . . . ) to denote the entire history of state-action pairs in an episode, and
s0 ∼ρ0(s0), at ∼πθ(at|st), and st+1 ∼P(st+1|st, at).
State visitation distribution is also of interest. When γ < 1, the discounted state visitation distribution
dπ is defined as dπ(s) = (1 −γ) P∞
t=0 γtP(st = s|π, ρ0), where P(st = s|π, ρ0) denotes the probability of st = s
concerning the randomness induced by π, P and ρ0.
3.2. Multi-Agent Reinforcement Learning
A cooperative multi-agent system can be modeled as a multi-agent Markov decision process.
An
n−agent MDP is defined by a tuple ( ¯S, ¯
A, ¯P, I, ¯Re, γ), where I = {1, 2, . . . , n} is the finite sets of agents,
¯S = ×i∈ISi is the joint state space and Si is the state space of i−th agent. At each time step, each agent
selects an action ai ∈Ai at state s ∈
¯S to form a joint a ∈
¯
A, a shared extrinsic reward ¯Re(s, a) can
be received for each agent, and the next state s′ is generated according to the transition function ¯P(·|s, a).
The objective of the cooperative multi-agent task is that each agent learns a policy πi(ai|si) to jointly team
performance. In this study, different from this original multi-agent optimization objective, we focus on
multi-agent exploration (MAE) and design a novel approach to encourage the agent team to cooperatively
explore the environment.
3.3. Maximum Mean Discrepancy
Maximum Mean Discrepancy is an integral probability metric that measures the difference (or similarity)
between two different probability distributions [50, 51, 52, 10, 53]. Let the probability distributions p and
q be defined in a nonempty compact metric space X. Let x and y be observations taken independently of p
and q, respectively. Subsequently, the MMD metric was defined as
MMD(p, q, F ) = sup
f∈F

Ex∼p
 f(x) −Ey∼q
 f(y)
,
(1)
where F is the class of the functions in X. If F satisfies the condition p = q, if and only if Ex∼p[ f(x)] =
Ey∼q[ f(y)], ∀f ∈F , then MMD is a metric measuring the discrepancy between p and q [54].
What type of function class makes up the MMD metrics? According to the literature [50], the space
of bounded continuous functions on X satisfies the condition; however, it is difficult to compute the MMD
distance between p and q with finite samples in such a function class because of its uncountability. If F is
the Reproducing Kernel Hilbert Space (RKHS) H, f in Eq. (1) can be replaced by a kernel function k ∈H,
such as the Gaussian or Laplace kernel functions. Gretton et al. [51] show that
MMD2(p, q, H) = E[k(x, x′)] −2E[k(x, y)] + E[k(y, y′)],
(2)
where x, x′ i.i.d. ∼p and y, y′ i.i.d. ∼q.
3.4. Skill-Based HRL Algorithms
Consider a skill-based HRL algorithm with a 2-level hierarchy, whose policy is composed of a high-
level policy πθh and a low-level policy πθl. In this framework, the skills of a low-level policy are usually
pre-trained and distinguished using different latent codes z. For example, a single stochastic neural network
(SNN) [55] can encode many low-level skills simultaneously [8, 44]. The high-level policy πθh(z|s) does
not take actual actions to interact with the external environment directly. Instead, it samples latent codes on
a slower timescale than the low-level policy, such as observing the environment once every p time steps.
The inputs of low-level policy πθl(a|s, z) are not only environment state vectors but also latent codes from

6
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
Fig. 1. In a two-layer structure, the high-level policy πθh(zt|st) samples a latent-codes zt, and decides which low-policy performs over
next p time steps. The low-level policy πθl(at|st, zt) outputs actions at straight interacting with a external environment. Note that
zt = zkp, from t = kp to (k + 1)p −1. After p time steps, the high-level policy takes a new high-level action once again.
the high-level policy. That is, the high-level policy determines the action output of the low-level policy for
the next p time steps [45, 8, 56]. Fig. 1 illustrates the framework of an HRL algorithm with a two-level
hierarchy.
4. Proposed Approach
In this section, we propose a trajectory-constrained exploration strategy for efficient exploration, which
solves the hard-exploration problem from the perspective of constrained optimization. The main objective of
the proposed trajectory-constrained exploration strategy is to encourage an RL agent to visit underexplored
regions of the state space and prevent it from adopting suboptimal and myopic behaviors.
4.1. Trajectory-Constrained Exploration Strategy
Assuming that there is a collection M of offline suboptimal trajectory data that might lead to goals with
deceptive rewards, we aim to drive the agent to systematically explore the state space and generate new
trajectories by visiting novel regions of the state space. One method to achieve this goal is to maximize the
difference between the current and previous trajectories. We used the MMD metric defined in Eq. (1) to
measure the disparity between the different trajectories.
The agent collects a certain number of trajectories and stores them in on-policy trajectory buffer B
at each epoch. Specifically, every trajectory τ in buffer B and offline replay memory M is treated as a
deterministic policy. Then we calculate their corresponding state-action visitation distributions ρτ. Finally,
instead of computing the MMD diversity measurement of the distribution on the trajectory space induced by
previous policies [10], in our study, the MMD distance is calculated between different state-action visitation
distributions that belong to the offline demonstration trajectory in M and the current trajectory in B.
Let K(·, ·) denote a kernel of the reproducing kernel Hilbert space H, such as a Gaussian kernel, then an
unbiased empirical estimate of MMD(τ, υ, H) is given by:
MMD2(τ, υ, H) =
E
x,x′∼ρτ
k  x, x′ −2 E
x∼ρτ
y∼ρυ
k(x, y) +
E
y,y′∼ρυ
k(y, y′) ,
where τ ∈B, υ ∈M, and ρτ and ρυ are the corresponding state-action visitation distributions of τ and υ,
respectively. The function k(·, ·) is given by:
k(x, y) = K (g(x), g(y)) .

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
7
Function g provides the flexibility to adjust the focus of the MMD distance metric for different aspects, such
as state visits, action choices, or both. In our experiments, we measure the MMD distance only concerning
a relevant subset of the information contained in each state-action pair and choose this subset to be the
coordinate c of the center of mass (CoM), i.e., the function g maps a state-action pair (s, a) to c. Moreover,
we believe it should also make sense to let g(s, a) = (c, a), although it may require us to design a new kernel
function K(·, ·). Finally, we define the distance D(x, M) of the state-action pair x = (s, a) to replay memory
M as follows:
D(x, M) = E
τ∈Bx
h
MMD2(τ, M, H)
i
,
where Bx = {τ|x ∈τ, τ ∈B}, and MMD2(τ, M, H) is defined by:
MMD2(τ, M, H) = min
υ∈M MMD2(τ, υ, H).
To emphasize that the distance is defined based on the MMD, we add the subscript MMD to the symbol
D in Eq. (4.1). The stochastic optimization problem with MMD distance constraints is defined as follows:
max
θ
J(θ),
s.t. DMMD(x, M) ≥δ,
∀x ∈B,
(3)
where J is an ordinary reinforcement learning objective, and δ is a constant MMD boundary constraint.
Remark 1. Replay memory M did not change during any epochs of the training process. Instead, replay
memory M is updated only with trajectories generated by a suboptimal policy learned after a previous
training process. Furthermore, the offline trajectories in M can be collected from human players. The key
insight of our study is that it orients its policy away from suboptimal policies by considering incomplete
offline demonstrations as references. In contrast to RLfD methods, our method does not require perfect and
sufficient demonstrations, which is more realistic in practice.
Remark 2. DIPG [10] uses the MMD distance between probability distributions over trajectory spaces
induced by the current policy and the previously learned suboptimal policy as a diversity regularization
term. However, DIPG simply stacks the states and actions from the first N steps of a trajectory into a single
vector, which is not sufficient for measuring the distance between trajectory distributions. Instead, we regard
each past suboptimal trajectory as a deterministic policy and use the MMD distance metric to calculate
the distance between the state-action visitation distributions induced by the current and past suboptimal
trajectories. Consequently, our method can reduce the sampling and computational complexities compared
with DIPG.
The constrained optimization problem (3) can be solved using the Lagrange multiplier method, and its
corresponding unconstrained form is as follows:
L(θ, σ) = J(θ) + σ
X
x∈B
min
n
DMMD(x, ρµ) −δ, 0
o
.
(4)
The unconstrained form in Eq. (4) is intractable because its second term is numerically unstable, and its
gradient concerning the policy parameters is difficult to calculate directly. To address these challenges, we
provide a specialized approach to achieve the goal of introducing policy parameters into the second term of
the unconstrained optimization problem and balancing the contributions of the RL objective and constraints.
According to statistical theory, frequency is an unbiased estimate of probability when the number of samples
is sufficiently large. Therefore, when the sample number N of the on-policy buffer B is sufficiently large,
the following formula holds:
lim
N→∞
1
N
X
x∈B
min
n
DMMD(x, ρµ) −δ, 0
o
= E
x∼ρπ
h
min
n
DMMD(x, ρµ) −δ, 0
oi
.

8
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
Furthermore, the optimization problem (3) is converted into an unconstrained form:
L(θ, σ) = J(θ) + σ E
x∼ρπ
h
min
n
DMMD(x, ρµ) −δ, 0
oi
,
(5)
where σ > 0 is the Lagrange multiplier. Since N is a constant, it is absorbed by choosing the appropriate
coefficient σ.
Second, we estimate the gradient of the unconstrained optimization problem concerning the policy pa-
rameters. The first term of the unconstrained problem is an ordinary RL objective, and hence, the gradient
of this term can be calculated easily [18, 2, 1]. We then derive the gradient of the MMD term in Eq. (5)
for policy parameters θ, which enables us to efficiently optimize the policy. The result is described in the
following lemma.
Lemma 1 (Gradient Derivation of the MMD term). Let ρπ(s, a) be the state-action visitation distribution
induced by the current policy π. Let D(x, M) be the MMD distance between the state-action pair x and
replay memory M. Then, if the policy π is parameterized by θ, the gradient of the MMD term of Eq. (5) for
parameters θ is derived as follows:
∇θDMMD =
E
ρπ(s,a)
∇θ log πθ(a|s)Qi(s, a) ,
(6)
where
Qi(st, at) =
E
ρπ(s,a)

T−t
X
l=0
γlRi(s, a)
,
(7)
and
Ri(s, a) = min {DMMD(x, M) −δ, 0} .
(8)
Appendix B presents the derivation of the MMD term gradient gradient.
Remark 3. In Algorithm 1, we showed that this trajectory-constrained exploration strategy can be readily
applied to on-policy algorithms, such as PPO [18]. Whereas the PPO algorithm does not maintain replay
memory, our method maintains n trajectories of each past suboptimal policy in replay memory M to compute
the DMMD distance measure. Generally, n = 5 is sufficient to achieve satisfactory performance. The batch
of on-policy data is stored in buffer B. Moreover, because the MMD distance term is treated as a diversity
reward in Lemma 1, we believe that it can also be integrated into bootstrapped Q-learning by adding the
MMD diversity bonus to both the immediate reward and the next Q value, such as OB2I [57], which deserves
further investigation.
Remark 4. DIPG [10] considers the diversity gradient during the training process because it adds an
MMD regularization term to its objective function. Therefore, its objective function incurs sensitive hy-
perparameters that cause instability and even lead to failure to solve the specified task. To resolve the
contradiction between the RL and diversity objectives, we reformulate the hard-exploration task as a con-
strained optimization problem, where the optimization goal is formulated by the naive RL objective, and the
MMD constraints bind the agent’s exploration region away from the offline demonstrations above a certain
threshold. Using this formulation, the offline trajectories regulate the policy updating only when the current
trajectories are outside the constraint region, which ensures stability and enables the agent to eliminate the
local optima.
4.2. Fast Adaptation Methods
4.2.1. Adaptive Constraint Boundary Adjustment Method
Generally, the constraint boundary δ in Eq. (3) should be environmentally dependent for different tasks,
which remains a great challenge for us to determine the size of parameter δ in various tasks. Moreover,
the expected MMD distance of each epoch gradually increases as the training process continues because
of the gradient of the MMD term derived in Lemma 1. Consequently, it is not conducive for the agent to

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
9
achieve temporally extended exploration when adopting a constant distance constraint boundary δ during
the entire training process. To resolve the problem of an increased MMD distance, we propose an adaptive
constraint distance normalization method, inspired by the batch normalization method [58]. After the agent
generates a batch of N trajectories {τi}n
i and stores them in B, the distance d(x, M) = DMMD(x, M) of each
state-action pair x in the on-policy buffer B is calculated in each epoch. We then compute the normalized
distance ˆd(x, M) according to:
ˆd(x, M) = d(x, M) −E [D(B)]
√Var [D(B)]
,
(9)
where the expectation E[D(B)] and variance Var[D(B)] are computed over the distance set D(B) =
{d(xi, M)|xi ∈B)} of all state-action pairs in B in every epoch.
Intuitively, our proposed policy gradient updates the policy parameters to drive the agent to visit the
underexplored state-action pairs while increasing the return value of the agent. Hence the expected MMD
distance of state-action pairs in B for each epoch increases gradually during the training process. Distance
normalization enables us to adjust distance constraint boundaries dynamically because true distance bound-
aries represented by the normalized parameter δ adaptively change for each epoch. With distance normal-
ization, the problem of determining parameter δ becomes relaxed and environmentally independent. In our
experiments, we usually choose δ = 0.5 as the distance constraint boundary. We compare the performance
of our algorithm with different parameter choices in Section 7.3. The experimental results demonstrate that
this method can stabilize the training process and enable the agent to achieve better temporally extended
exploration.
4.2.2. Adaptive Scaling Method
Although the value of the Lagrange multiplier σ can be updated by gradient ascent, we find that this
method is less than ideal in some cases, as demonstrated by the experimental results presented in Section 7.
To solve this problem, similar to [9] and [28], we propose an adaptive scaling method (ASM) based on the
MMD distance to adjust the contributions of the ordinary RL objective and the MMD term to the gradient.
We associate σ with the MMD distance metric DMMD and adaptively increase or decrease the value of
σ depending on whether the MMD distance DMMD between current trajectories and previous suboptimal
trajectories are below or above a certain distance threshold ϵ. Different values of the threshold ϵ are adopted
by different methods in our experiments. The simple approach employed to update σ for each training
iteration [9, 28] is given by:
σ =

1.05σ,
if ∃υ ∈B s.t. MMD(υ, M) ≤ϵ,
0.98σ,
if ∀υ ∈B s.t. MMD(υ, M) ≥2ϵ,
(10)
where MMD(υ, M) := minτ∈M MMD(υ, τ). In addition, if trajectories generated by the current policy lead
to the same sparse reward as previously stored trajectories, then we adjust the value of σ to 1.2 times the
current value, i.e. σ = 1.2σ. It is worth noting that the values of 1.05, 0.995, and 1.2 are selected empirically.
However, they have a certain universality, and we use these values in all the experiments of this study.
5. Theoretical Analysis of TACE Performance Bounds
In this section, we present the theoretical foundation of TACE and demonstrate how it improves the
exploratory performance of the agent. We derive a novel bound for the difference in returns between two
arbitrary policies under the proposed MMD constraints. This result can be viewed as an extension of the
previous work [59, 60, 2, 61] on the new constrained policy search problem in Eq. (3). Against this back-
ground, this section provides some guarantees of performance improvement and helps us better understand
our proposed algorithms at a theoretical level.
The DMMD gradient estimation in Eq. (6) in Lemma 1 is similar to policy gradients introduced
in [18], except that environmental rewards Re(s, a) are replaced by MMD-based diversity constraints

10
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
min {DMMD(x, M) −δ, 0}. Thus, TACE considers the long-term effects of constraints while first guaran-
teeing that the constraints are valid. When the MMD gradient of Lemma 1 is integrated with the gradient of
J(θ) to update the policy parameters, the final gradient gθ for the parameter update is expressed as
gθ = Eρπ(s,a)
∇θ log πθ(a|s) (Qe(s, a) + σQi(s, a)) .
(11)
Due to the similarity between the forms of the DMMD gradient and the RL gradient of J(θ), MMD-
based diversity constraints min {DMMD(x, M) −δ, 0} can be viewed as an intrinsic reward Ri(s, a) for each
state-action pair and be integrated with environmental rewards as follows:
Qe(st, at) + σQi(st, at) = Est+1,at+1,...

∞
X
l=0
γl (Re(st+l, at+l) + σRi(st+l, at+l))
,
where Re(st+l, at+l)+σRi(st+l, at+l) can be viewed as the total reward R(s, a). The following theorem connects
the difference in returns between two arbitrary policies to their average variational divergence.
Theorem 1. [Performance bound of the trajectory-constrained exploration strategy] For the proposed con-
strained optimization problem of Eq. (3). Subsequently, for any policies π and π′, define δ f (s, a, s′) .=
Re(s, a) + σRi(st, at) + γ f(s′) −f(s),
ϵπ′
f
.= max
s
Ea∼π′[δf (s, a, s′)]
 ,
Tπ, f (π′) .= E
s∼dπ
a∼π
" π′(a|s)
π(a|s) −1
!
δ f (s, a, s′)
#
, and
D±
π,f (π′) .= Tπ, f (π′)
1 −γ
±
2γϵπ′
f
(1 −γ)2 E
s∼dπ
DTV(π′||π)[s] ,
where s′ ∼P(·|s, a), Ri(s, a) is the MMD-based distance reward min {DMMD(x, M) −δ, 0} and γ is the
discount factor. DTV(π′||π)[s] = 1
2
P
a |π′(a|s) −π(a|s)| is used to represent the total variational divergence
between two action distributions of π and π′ when the state is s. The following bounds hold:
D+
π, f (π′) ≥L(θ′, σ) −L(θ, σ) ≥D−
π,f (π′),
(12)
Furthermore, the bounds are tight (when π′ = π, all three expressions are identically zero). Here, L(·, ·) is
defined in Eq. (4), σ is the Lagrange multiplier.
Before proceeding, it is worth noting that Theorem 1 is similar to Theorem 1 of [61]. When choosing
σ = 0, the result of this theorem degenerates into Theorem 1 of [61]. According to Lemma 1, our approach
transforms the constrained optimization problem into an unconstrained policy search task. In this manner,
it considers the long-term effects of constraints on returns. Moreover, different from Theorem 1 in [61], our
method derives a new performance bound D±
π, f (π′) based on δ f (s, a, s′) .= Re(s, a)+σRi(st, at)+γ f(s′)−f(s).
Hence, this theorem can be used to analyze the effectiveness of our approach in improving exploration. By
bounding the expectation of the total variational divergence Es∼dπ [DTV(π′||π)[s]] with maxs [DTV(π′||π)[s]],
picking f(s) to be the value function V(s) computed with the total reward R(s, a), the following corollary
holds:
Corollary 1. For any policies π and π′, with ϵπ′ .= maxs |Ea∼π′[A(s, a)]|, in which A(s, a) is the advantage
function calculated with the total reward R(s, a) .= Re(s, a) + σRi(st, at), then the following bound holds:
L(θ′, σ) −L(θ, σ) ≥
1
1 −γ E
s∼dπ
a∼π′
"
A(s, a) −2γϵπ′
1 −γ DTV(π′||π)[s]
#
.
(13)
Here, L(·, ·) is defined in Eq. (4), γ is the discount factor, and σ is the Lagrange multiplier.

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
11
The bound in Corollary 1 can be regarded as the worst-case approximation error. The TV-divergence
and KL-divergence are related by DTV(p||q) ≤
p
DKL(p||q)/2 [62]. Combining this inequality with Jensen’s
inequality, we obtain:
E
s∼dπ
DTV(π′||π)[s] ≤E
s∼dπ

r
1
2DKL(π′||π)[s]
≤
r
1
2 E
s∼dπ [DKL(π′||π)[s]].
(14)
It is worth mentioning that the advantage A(s, a) can be decomposed as the sum of the environmental
advantage Ae(s, a) and the MMD-based advantage Ai(s, a), which is expressed as:
A(s, a) = Ae(s, a) + Ai(s, a).
(15)
Substituting Eq. (14) and (15) into Eq. (13), we obtain the following corollary about determining the
value of σ, such that the worst-case approximation error in Eq. (13) is greater than the threshold ∆:
Corollary 2. Suppose a performance improvement threshold ∆for any policies π and π′, and π and π′ satisfy
Es∼dπ [DKL(π′||π)[s]] ≤η and E [Ai(s, a)] ≥β > 0. When (1 −γ)∆−E [Ae(s, a)] −
p
2ηγϵπ′(1 −γ)−1 > 0,
then if
σ ≥E
s∼dπ
a∼π′
−1 [Ai(s, a)]
(1 −γ)∆−E
s∼dπ
a∼π′
[Ae(s, a)] −
p
2ηγϵπ′
1 −γ
,
(16)
we have
L(θ′, σ) −L(θ, σ) ≥∆.
(17)
Here, L(·, ·) is defined in Eq. (4), γ is the discount factor, and σ is the Lagrange multiplier.
This corollary illustrates the feasibility of the adaptive scaling method in Section 4.2.2. According to
Corollary 2, by choosing suitable parameters σ and δ, we can make L(θ′, σ) −L(θ, σ) ≥∆hold. Note
that when E [Ae(s, a)] decreases, for example, if the agent adopts a single behavioral pattern and learns a
suboptimal policy, a larger lower bound of σ is calculated. In this manner, our approach helps the agent to
be exempt from myopic behaviors and drives it to stay in the feasible region. In practice, even if we use
the minimal value of σ recommended by the corollary above, σ is still very large to obtain the performance
improvement of ∆. Moreover, σ is only computed when the parameters of the policy have been updated.
Hence, determining the value of σ before the update of the policy parameters in each iteration is difficult.
One way to take a smaller value of σ robustly is to use the heuristic adaptive scaling method.
6. Experimental Setup
6.1. Environments
Gridworld. We evaluated the performance of the TCPPO (PPO with TACE) algorithm in tasks with discrete
state and action spaces, as shown in Fig. 2(a). In this experimental setting, the agent started from the bottom-
left corner of the map, and the optimal goal with the highest reward of 6 is located in the top-right corner.
Moreover, there is a suboptimal goal with a relatively small reward of 1 on the right side of the initial
position that can be accessed by the agent more easily. The deceptive reward provided by a suboptimal goal
can easily distract the agent from finding the goal with the highest reward in the top-right corner. At each
time step, the agent observes its coordinates relative to the starting point and chooses from four possible
actions: moving east, south, west, and north. An episode terminates immediately once the agent reaches
either of the two goals or the maximum number of steps for an episode is exceeded.
Deceptive Reacher. To test the proposed TCPPO method in continuous robotic settings, we used a variant
of the classic 3D-Reacher environment with formulable obstacles and misleading rewards [22]. In this
problem, as shown in Fig. 2(b), a two-joint robot arm [63] attempts to move its end effector (fingertip)
close to the red target position to obtain an optimal reward of 60. Instead, the end effector obtains a small
deceptive reward of 10 more easily by entering the box. At the start of a new episode, the robot arm is

12
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
(a)
(b)
Fig. 2. (a) Gridworld; (b) Deceptive Reacher.
spawned at a random position sampled from a specific range. The agent’s observation space consisted of the
angles and angular velocities of the two arms and the coordinates of the reacher’s fingertips. Furthermore,
the actions performed by the agent are sampled from a two-dimensional continuous action space [64].
Hierarchical Control Tasks. Two MuJoCo mazes with continuous state-action spaces were adapted from
the benchmarking hierarchical tasks used in [8] and [38], which were used to test TCHRL (SNN4HRL with
TACE). The observation space of the agent in these tasks is composed of the internal information S a of the
agent, such as the agent’s joint angles, and the task-specific characteristics S e, for example, walls, goals,
and other objects seen through a range sensor. These robots were described in [65]. In these tasks, the
agent is rewarded for reaching a specified position in the maze, as shown in Fig. 3. The problem of sparse
rewards and long horizons continues to pose significant challenges to RL because an agent rarely obtains
nonzero rewards; therefore, the gradient-based optimization of RL for parameterized policies is incremental
and slow.
(a)
(b)
Fig. 3.
(a) Maze 0 with two different goals. The agent is rewarded 60 for reaching the red goal and 50 for reaching the blue goal.
(b) Maze 1 with three different goals. The agent is rewarded 90 for reaching the red goal, 60 for reaching the green goal, and 30 for
reaching the blue goal.
As shown in Fig. 3(a), the structure of Maze 0 is the same as that introduced in [38], except that our
maze is larger in scale and has two goals placed in the upper-left and upper-right rooms. The agent can
receive rewards of 60 for reaching the goal in the top-left room and 50 for reaching another goal. The agent
was initially positioned in the bottom-right room of this maze.
Compared to Maze 0, Maze 1 has a different structure and more goals. Maze 1, shown in Fig. 3(b), has
three different goals, located at the top-left corner, bottom-left corner, and right side of the maze. The agent
obtains rewards of 90, 60, and 30 to reach each goal, and its initial position is near the bottom-left corner.
The agent can be more easily distracted from finding the optimal goal using two suboptimal goals.
Multi-Agent Control Tasks. TCMAE (Multi-Agent Exploration with TACE) was evaluated on two chal-

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
13
lenging environments: (1) a discrete version of the multiple-particle environment (MPE) [66, 48]; and (2)
a modified MuJoCo continuous control task - SparseAnt Maze. In both environments, sparse rewards are
only collected when agents reach the specified locations. Moreover, agents can receive deceptive suboptimal
rewards from targets that are closer to them.
(a)
(b)
Fig. 4. (a) Discrete multiple-particle environment (b) SparseAnt Maze
As shown in Fig. 4(a), two agents operate within the room of a 70 × 70 grid in the MPE environment.
There is a suboptimal goal of a deceptive reward near the initial position, and the optimal goal is located in
the up-right corner of the grid. The agents in the team can receive reward signals only when they reach any
goal in the environment. The action space of each agent consists of four discrete actions: moving east, south,
west, and north, similar to the grid-world environment. The agent team hopes to get rid of the suboptimal
goal and cooperatively collect the optimal reward by sharing the trajectory information.
In Fig. 4(b), two MuJoCo Ant agents are rewarded for reaching the specified position in a maze, and other
than that, they cannot receive any reward signal. The ant agents start at the same location and cooperatively
explore the environment similar to the agents in the MPE task. The observation space of the ant agent is
composed of the internal information S a of the agent and the task-specific characteristics S e as described in
the hierarchical control tasks.
6.2. Neural Architectures and Hyperparameters
For grid world tasks, all policies were trained with a learning rate of 0.000018 and a discount factor
of 0.99. All neural networks of the policies were represented with fully connected networks that have two
layers of 64 hidden units. The batch size and maximum episode length in the 50 × 50 grid world maze were
8 episodes and 160, respectively. Meanwhile, The batch size was also 8 episodes in the 70 × 70 grid world
maze, while the maximum episode length was 220. The initial value of σ is set to 0.5 for these two tasks.
For the deceptive reacher task, all policies were trained with a learning rate of 0.000006 and a discount
factor of 0.99. We used fully connected networks that have two layers of 64 hidden units to implement the
agent policy. The batch size and maximum episode length were 8 episodes and 150 steps, respectively. The
initial value of σ is set to 0.5 for TCPPO.
For the MuJoCo maze tasks, we used an SNN to learn pre-trained skills that were trained by TRPO. The
number of pre-trained skills (i.e., the dimensions of the latent code fed into the SNN) was six. A high-level
policy is implemented using a fully connected neural network trained by the PPO. All the neural networks
(SNN and fully connected networks) had two layers of 64 hidden units. The other settings were identical to
those in [8]. The initial value of the parameter σ is set to 0.4.
For the discrete MPE environment, all neural networks of the policies were implemented with fully
connected networks that have two layers of 64 hidden units. The size of the learning rate was 0.000018, and
the discount factor was 0.99. The batch size and maximum episode length were 8 episodes and 240 steps.
For the SparseAnt maze task, the network structure is the same as that of the discrete MPE task. The size of

14
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
the learning rate is 0.001, and the discount factor is 0.99. The batch size and maximum episode length were
30 episodes and 500 steps.
6.3. Baseline Methods
The baseline methods used for performance comparisons varied for different tasks. For discrete control
tasks, we compared TCPPO with the following baseline methods: (1) DIPG [10], (2) vanilla PPO [18], (3)
Noisy-A2C: the noisy network [28] variant of A2C [67], (4) Div-A2C: A2C with diversity-driven explo-
ration strategy [9], (5) RIDE [68], and (6) NovelD [69]. DIPG and Div-A2C promote agent exploration
by adding a diversity regularization term to the original RL objective function. PPO is an on-policy RL
method derived from TRPO, which is a practical algorithm that addresses continuous state-action spaces.
Noisy-A2C is a variant of A2C that adopts noisy networks to increase the randomness of the action outputs.
RIDE and NovelD design novel intrinsic reward functions for driving deep exploration in the sparse setting.
For hierarchical continuous control tasks, we compared TCHRL with the following baseline methods:
(1) PPO, (2) SNN4HRL [8], (3) DIPG-HRL, and (4) HAC [38]. SNN4HRL is a state-of-the-art sub-goal-
based HRL method. DIPG-HRL is a combination of DIPG [10] and SNN4HRL, where we use the DIPG
objective function to train a high-level policy based on pre-trained skills. Similar to our method, SNN4HRL,
and DIPG-HRL share the same set of pre-trained low-level skills as TCHRL. However, pre-trained skills
are unadaptable in the training processes of SNN4HRL and DIPG-HRL. Instead, TCHRL sets auxiliary
MMD distance rewards for low-level skill training to enable efficient and simultaneous learning of high-
level policies and low-level skills without using task-specific knowledge. HAC is a standard end-to-end
subgoal-based HRL baseline without delicate techniques of subgoal discovery or quantization.
In multi-agent control tasks, TCMAE is compared with the following baseline methods: (1) SAC [21],
(2) QMIX [17], (3) EMC [49], (4) MAPPO [70] and (5) IPPO [70]. QMIX is a novel value-based method
that trains decentralized policies of multiagent in a centralized end-to-end learning fashion. EMC is a
curiosity-driven exploration method for deep cooperative multi-agent reinforcement learning (MARL).
MAPPO is a PPO-based algorithm with centralized value function inputs for MARL. IPPO represents
the independent PPO algorithm where each agent has local inputs for both the policy and value function
networks. Recently, MAPPO and IPPO were revisited by [70] and demonstrated that they could achieve
competitive or superior results in various challenging tasks.
Table 1. Performance comparison with different methods
50 × 50
70 × 70
Average reward
Success rate
Average reward
Success rate
TCPPO
6.00
1.00
6.00
1.00
Div-A2C
4.18
0.64
3.00
0.40
TCPPO w/o ASM
3.92
0.58
3.18
0.36
Noisy-A2C
3.08
0.42
3.00
0.40
DIPG
3.08
0.42
1.61
0.20
PPO
2.36
0.27
2.64
0.34
RIDE
6.00
1.00
5.22
0.87
NovelD
4.50
0.75
2.25
0.38
7. Evaluation of Results
We evaluate the proposed method using several discrete and continuous control tasks. Discrete control
tasks comprise 2D grid world environments of different sizes. The continuous control tasks consisted of sim-
ulated robotic environments developed by MuJoCo [63]. Furthermore, we directly compared the proposed
approach with other state-of-the-art algorithms.

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
15
7.1. Grid-world Task
7.1.1. Performance Comparisons with Baseline Methods
In this experiment, we combined our trajectory-constrained exploration strategy with the PPO algorithm
[18] to obtain the TCPPO algorithm. We evaluated the trajectory-constrained exploration strategy using
2D grid worlds of two different sizes: 50 × 50, and 70 × 70. The performances of the baseline methods
and TCPPO are reported based on their average returns in Table 1. Some results of the 70 × 70 maze are
presented in Fig. 5. All curves in Fig. 5 were obtained by averaging over eleven different random seeds,
and for clarity, the shaded error bars represent 0.35 standard errors. Furthermore, we plotted the state-
visitation count graphs of each method (Fig. 6) in a 70 × 70 grid world, which illustrates the differences in
the exploration behaviors for different agents.
(a)
(b)
Fig. 5.
Learning curves of average return and success rate for different methods in Maze 0 when the replay memory stores previous
suboptimal trajectories leading to the local optimal goal. The success rate is used to illustrate the frequency at which the agent reaches
the global optimal goal during training processes.
As shown in Table 1 and Fig. 6, the results of TCPPO outperform those of the other methods in these
two 2D grid worlds of different sizes. Specifically, TCPPO learns faster and achieves higher average returns.
The average return of RIDE dramatically increases, and the agent quickly learns to reach the optimal goal.
However, the convergent values of both average return and success rate are inferior to those of TCPPO.
These results verify that the TCPPO prevents the agent from adopting myopic behaviors and drives deep
exploration. From the state-visitation count graph (see Fig. 6), the four baseline approaches visited only
a smaller part of the state spaces and hardly collected higher rewards. However, Fig. 6(g) shows that the
TCPPO method enhances the exploring behavior of the agent and promotes the agent to explore a wider
region of the 2D grid world. Consequently, our method helps the agent escape from the region in which
the deceptive reward is located and successfully reach the optimal goal with a higher score. Furthermore,
we compare the changing trends in the MMD distances of the different methods in Fig. 7(a). According
to Fig. 7(a), TCPPO increases the MMD distance between the old and current policies during the training
process. Note that DIPG does not learn the optimal policy, but also results in a larger MMD distance, similar
to TCPPO.
7.1.2. Analysis of Adaptive Scaling Method
The adaptive scaling method for the Lagrange multiplier σ serves as a critical component of our pro-
posed trajectory-constrained exploration strategy. This ensures that the agent remains within the feasible

16
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
(a) PPO
(b) Div-A2C
(c) DIPG
(d) RIDE
(e) Noisy-PPO
(f) NovelD
(g) TCPPO
Fig. 6.
State-visitation counts of different algorithms in the 70 × 70 grid world: (a) PPO, (b) Div-A2C, (c) DIPG, (d) RIDE, (e)
Noisy-A2C, (f) NovelD, and (g) TCPPO.
region by increasing the Lagrange multiplier if there are state-action pairs outside the feasible region. As
shown in Eq. (10), we also adopt a naive linear decay method to stabilize training processes because it
hinders the learning process if the Lagrange multiplier σ remains a large constant throughout the whole
training process. Table 1 lists the influence of the adaptive scaling method on the agent performance. The
experimental results illustrate that the adaptive scaling method enables the agent to gain a higher average
return and outperform other methods during training.
Fig. 7(b) shows the changing trend of the Lagrange multiplier σ during the training process. At the
beginning of the training phase, the agent occasionally encounters a suboptimal goal, which is the same as
the goal demonstration trajectories lead to. According to our design in Section 4.2.2, our method drastically
increases the value of parameter σ to force the agent away from the local maximum. After several episodes,
the agent gets rid of the suboptimal goal and gradually explores more diverse regions of the state-action
space, wandering between areas of radius ϵ and 2ϵ. After about 200 episodes, the value of the parameter σ
drops exponentially and slowly, which illustrates that the agent always stays outside the area with a radius of
2ϵ. Therefore, our adaptive scaling method significantly improves the efficiency of exploration and protects
the agent from falling into a local maximum and adopting myopic behaviors.
7.2. Deceptive Reacher Task
A variant of the classic two-jointed robot arm environment was used to test the proposed method on
a continuous robotic control problem. In this deceptive reacher task, we compared our proposed TCPPO
algorithm with the same baseline methods used in the grid world task. In this task, to fulfill our design,
the replay memory was maintained to store the suboptimal trajectories. Furthermore, we found that it is
sufficient to store no more than five trajectories in the replay memory. We report the learning curves of all
the methods in Fig. 8 in terms of the average return and success rate. All learning curves were obtained by
averaging the results generated with different random seeds, and the shaded error bars represent the standard
errors.
Compared with other baseline methods, our approach succeeded in moving out of the box with deceptive
rewards and did not adopt myopic behaviors. Therefore, TCPPO can learn faster and achieve higher return
values at the end of the training. RIDE fully explored the environment and quickly reached the optimal
goal during training. In contrast, the other baseline methods rarely encountered the optimal goal and always
fell into the local optimum by collecting deceptive rewards from the box. The PPO is not aimed at long-
horizon sparse-reward problems. Consequently, the success rate of the PPO in this continuous control

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
17
(a)
(b)
Fig. 7. (a) Changing trends in MMD distances; (b) The changing trend of the parameter σ.
(a)
(b)
Fig. 8. Learning curves of average return and success rate for different methods in the deceptive reacher task.
task was always zero. Noisy-A2C and Div-A2C are designed for efficient exploration and occasionally
generate trajectories with the optimal rewards. However, our results indicate that these two algorithms do
not eliminate the myopic behaviors and cannot learn the optimal policy after training.
7.3. Performance in Four-Room Maze
7.3.1. Results and Comparisons
In this task, we compared the TCHRL algorithm with the state-of-the-art skill-based method
SNN4HRL [8]. In addition, we conducted comparative experiments using a DIPG [10] variant that is com-

18
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
bined with SNN4HRL (denoted DIPG-HRL). In this algorithm, DIPG is only used to train the high-level
policy and not to adapt pre-trained skills along with the high-level policy during the training process. We
maintain empty replay memories M for DIPG-HRL and TCHRL at the beginning of the training. When M
is empty, TCHRL and DIPG-HRL degenerate into SNN4HRL. The number of prior suboptimal trajectories
is at most n, which are generated by the same previous policy. In our experiments, n = 5 was sufficient
to produce satisfactory performance. In the worst case, an agent learns the optimal policy only after it
learns all the suboptimal policies and stores all the corresponding trajectories in M. Consequently, the
agent may need to sequentially train g different policies in a complete training session, where g represents
the number of goals in the maze. For fairness, we trained the PPO agent the same number of times. We
plotted the statistical results of different methods based on different goals. All the curves were obtained by
averaging over different random seeds, and the shaded error bars represent the confidence intervals. Note
that the learning curves shown in Fig. 9 are drawn when the replay memory stores the previous suboptimal
trajectories, leading to a suboptimal goal.
(a)
(b)
(c)
Fig. 9. (a) Learning curves of average return, (b) Learning curves of success rate. They are obtained when the replay memory stores
previous suboptimal trajectories leading to the local optimal goal. The success rate is used to illustrate the frequency at which agents
reach the globally optimal goal during the training process. (c) Performance comparison when parameters δ take different values.
In Fig. 9, we compare the different methods from two aspects of average return and success rate. Our
method is superior to the other methods in both aspects. Specifically, the PPO is not designed for long-
horizon tasks with sparse rewards; hence, the success rate of the PPO in achieving the optimal goal is
zero in this maze. Although HAC is a subgoal-based HRL algorithm, HAC cannot find goals and obtain
any sparse reward according to Fig. 9, which may be caused by the sensitivity of goal space design [42].
SNN4HRL learned only the myopic policy leading to the suboptimal goal and was rewarded with 50 during
training, which indicated that the agent was trapped in the local optimum. DIPG-HRL reached the optimal
goal at the top-left corner with a high percentage. Moreover, it can be seen from Fig. 9(b) that its success
rate in reaching the global optimal goal is lower than that of TCHRL, and it learns slower. Compared
with SNN4HRL and DIPG-HRL, the experimental results in Fig 9 demonstrate that TCHRL drives deep
exploration and avoids suboptimal and misguided behaviors, thereby attaining a higher score and learning
rate.
7.3.2. Effect of the Distance Normalization
In this section, we examine the influence of the distance normalization method on the performance of
agents in continuous control tasks. We chose different values of the parameter δ = 0, 0.5, and0.75, as the
boundary constraints for comparative experiments. The TCHRL algorithm without distance normalization
is viewed as a basic comparison baseline, in which the value of the parameter δ needs to be chosen manually
according to different experimental settings. We use “none” to represent this method in Fig. 9(c). The
experimental results reported in Fig. 9(c) demonstrate that by simply setting δ = 0.5, TCHRL achieved

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
19
the best results among the four different parameter settings. Hence, the distance normalization method can
reduce the dependence of parameter δ on the environment and stabilize the learning process.
7.4. Multiple-Goal Maze Task
7.4.1. Performance Comparisons
We compared our algorithm with state-of-the-art skill-based HRL methods SNN4HRL [8] and DIPG-
HRL in a multiple-goal maze task. When the replay memory M maintains trajectories leading to suboptimal
goals, TCHRL encourages the agent to generate new trajectories that visit novel regions of the state-action
space, gradually expanding the exploration range. As shown in Fig. 10, the TCHRL agent collects trajec-
tories ending with the optimal goal and learns the policy to collect optimal rewards. In the early training
phase, the average return of the TCHRL algorithm was smaller than that of SNN4HRL because our approach
preferred to explore the environment more systematically at the beginning of the training time. Moreover,
TCHRL gradually adapted its pre-training skills during the training process. Hence, TCHRL does not im-
mediately adopt myopic behaviors to obtain deceptive rewards more easily. All the curves are obtained by
averaging over different random seeds, and the shaded error bars represent the confidence intervals.
In Fig. 10, the PPO is not an algorithm specifically designed for long-horizon tasks with sparse or
deceptive rewards. Therefore, the success rate of the PPO in this maze was zero, as it was in Maze 0. HAC
did not receive any reward and learn meaningful policies, which is consistent with previous results in Maze
0. SNN4HRL was rewarded with 30 from the suboptimal blue goal, and DIPG-HRL reached the suboptimal
green goal and obtained a reward of 60. Neither received the reward with the highest score from the red goal.
Therefore, this result verifies that SNN4HRL and DIPG-HRL cannot fully explore the environment, always
adopt myopic behaviors, and learn suboptimal policies. In contrast, our TCHRL can escape suboptimal
behaviors and reach the global optimal goal. The MMD distances between the different trajectories that
(a)
(b)
Fig. 10. Learning curves of average return and success rate in Maze 1 when prior trajectory buffers store trajectories leading to the
local optimal goals. The success rate is used to indicate the frequency at which agents reach the globally optimal target during the
training process.
reached the two suboptimal goals were larger than those of the other two trajectories. Because DIPG uses
a regularization term based on trajectory distributions, it is difficult for DIPG to adjust the contributions of
the RL objective and regularization term. Therefore, the DIPG-HRL agent tends to learn to reach another
suboptimal goal if it has learned a suboptimal policy. Notably, these trajectory distributions were induced
by the corresponding policies, and additional trajectory data were required to achieve a good estimation of

20
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
these distributions. Moreover, DIPG-HRL does not adapt the parameters of the pre-training skills along
with a high-level policy, which further degrades the performance of the algorithm. This concept is discussed
in detail in the following section. In this manner, we illustrate the inability of the DIPG-HRL agent to learn
the optimal policy when the replay memory M maintains trajectories that lead to two suboptimal goals.
7.4.2. Adaptation of Pretrained Skills
To further explain why TCHRL achieves such excellent performance compared with other baseline
methods, we conducted a more in-depth study of the experimental results. TCHRL is a skill-based HRL
method. To train the TCHRL agent, we first use stochastic neural networks to learn diverse low-level
skills [8]. In Fig. 11, we compare the low-level skills before and after training in the Swimmer Maze task.
The swimmer agent was always initialized at the center of the maze and used a single skill to travel for a
fixed number of timesteps, where the colors indicate the latent code sampled at the beginning of the rollout.
Each latent code generates a particular interpretable behavior. Given that the initialization orientation of the
agent is always the same, different skills are truly distinct ways of moving: forwards, backward, or sideways.
(a)
(b)
Fig. 11. (a) Visitation graph of the pre-training skills of the agent; (b) Span of skills after training with TCHRL
Many other existing HRL algorithms either use pre-trained skills that are unadaptable or require domain-
specific information to define the low-level rewards. In contrast, TCHRL simultaneously adapts low-level
skills to downstream tasks while maintaining the generality of low-level reward design by setting auxiliary
rewards for low-level skill training, based on the MMD distance. Comparing Fig. 11(b) with Fig. 11(a),
we note that the swimmer agent learns to turn right (skill in baby blue) and left (skill in navy blue). It
is favorable to perform these two skills in the maze task in Fig. 3(b) when attempting to reach the green
and blue goals. Therefore, the adaptation of pre-trained skills distinctly leads to more diverse skills and
effectively drives the agent to explore a wider range of state spaces, which is beneficial for the downstream
tasks based on the experimental results.
7.5. Results of Multi-Agent Tasks
7.5.1. Performance Comparisons of Discrete MPE Environment
In this discrete MPE environment, TCMAE was used to encourage the multi-agent team to fully explore
the grid world, and its experimental results were compared with those of the baseline methods. In this task,
to fulfill our design, the replay memory of each agent was maintained to store the suboptimal trajectories.
The agents of the team shared trajectory information during each epoch. We report the learning curves of all
the methods in Fig. 12. All learning curves were obtained by averaging the results generated with different
random seeds, and the shaded error bar represents the standard error. Since TCMAE focuses on enhancing
the exploration of the multi-agent team, we choose to report the results of agents that generate the highest
return values and learning rates.

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
21
(a)
(b)
Fig. 12. Learning curves of average return and success rate for different methods in the discrete MPE environment.
As shown in Fig. 12, TCMAE achieves competitive and superior performance in terms of average return
and success rate compared to the other baseline methods. The large performance gap can be observed from
Figs. 12(a) and 12(b). Due to exploratory learning behavior, the learning rate of EMC is significantly slower
than TCMAE, and the learning process of EMC is more instability than TCMAE. QMIX and IPPO can
occasionally find the optimal goal with a simple heuristic exploration strategy, which can lead to inefficient
exploration, especially for IPPO. The experimental results demonstrate the great capability of TCMAE to
help the multi-agent team explore the environment and reach the optimal goal.
7.5.2. Experimental results of SparseAnt Maze Task
To evaluate TCMAE in environments with continuous state-action space, we designed a SparseAnt maze
task with sparse and deceptive rewards and compared the performance of TCMAE with several baseline
methods. The agent of the team shared the good trajectory information during the training process, which
can be used to compute the MMD-based intrinsic reward. We report the learning curves of all the methods
in Fig. 13. All learning curves were obtained by averaging the results generated with different random seeds,
and the shaded error bar represents the 95% confidence interval. The available code of QMIX and EMC
released by the authors can only be applied to the environments with discrete action spaces; hence they
cannot be used as baseline methods in the SparseAnt maze task.
In Fig. 13, the experimental results are reported in terms of average return and success rate. TCMAE
achieves a remarkable performance level, and the results of TCMAE considerably outperform baseline
methods. ISAC did not obtain significant policy performance improvement after training, and cannot find a
policy to collect the optimal rewards. Noticeably, MAPPO can achieve competitive results in both perfor-
mance metrics, however, its final results are inferior to those of TCMAE and its learning rate is slower than
TCMAE. We also compared TCMAE with the multi-agent RL algorithm IPPO to further demonstrate the
effectiveness of TCMAE in facilitating multi-agent exploration.
8. Conclusion
In this study, we present a trajectory-constrained exploration strategy for long-horizon tasks with large
state spaces and sparse or deceptive rewards. We propose to promote the agent’s exploration by treat-
ing incomplete offline demonstration data as references and demonstrate that this goal can be achieved

22
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
(a)
(b)
Fig. 13. Learning curves of average return and success rate for different methods in the SparseAnt maze task.
by introducing an effective distance metric to measure the disparity between different trajectories. We
reformulated the policy optimization for RL as a constrained optimization problem, which enhanced the
agent’s exploration behavior and avoided sensitive hyperparameters. Subsequently, we developed a novel
policy-gradient-based algorithm with adaptive clipped trajectory-based distance rewards. Our method can
be effectively combined with non-hierarchical and hierarchical RL methods and can continuously adapt pre-
trained skills along with high-level policies when the agent employs a hierarchical policy. Furthermore, we
introduced an adaptive scaling method and a distance normalization strategy to achieve better performance.
The proposed trajectory-constrained strategy is evaluated in large 2D grid worlds and MuJoCo maze en-
vironments, and the experimental results show that our method outperformed other baseline algorithms in
terms of improving exploration efficiency in large state spaces and avoiding local optima.
Our method encourages agents to visit underexplored regions by considering imperfect offline demon-
strations as references. However, when offline imperfect trajectories are on the way to the optimal goal, our
method may ignore the fact that exploiting such experiences can indirectly drive deep exploration. Further
research may focus on considering the diverse exploration problem in a teamwork setting and exploiting
imperfect demonstrations to indirectly accelerate learning and drive deep exploration.
Appendix A. Reproducing Kernel Hilbert Spaces
Appendix A.1. Reproducing Kernel Hilbert Spaces
Definition 1. An inner product ⟨µ, υ⟩can be
1. a dot product: ⟨µ, υ⟩= υ′µ = P
i υiµi;
2. a kernel product: ⟨µ, υ⟩= k(υ, µ) = ψ(υ)′ψ(µ) (where ψ(µ) may have infinite dimensions).
Obviously, an inner product ⟨·, ·⟩must satisfy the following conditions:
1. Symmetry
⟨µ, υ⟩= ⟨υ, µ⟩∀µ, υ ∈X
0This section mainly refers to [71].

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
23
2. Bilinearity
⟨αµ + βυ, ω⟩= α⟨µ, ω⟩+ β⟨υ, ω⟩∀µ, υ, ω ∈X, ∀α, β ∈R
3. Positive definiteness
⟨µ, µ⟩≥0, ∀µ ∈X
⟨µ, µ⟩= 0 ⇐⇒µ = 0
Definition 2. A Hilbert Space is an inner product space that is complete and separable with respect to the
norm defined by the inner product.
The vector space Rn with the vector dot product ⟨a, b⟩= b′a ∀a, b ∈Rn is an example of Hilbert space.
Definition 3. k : X × X →R is a kernel if
1. k is symmetric: k(x, y) = k(y, x);
2. k is positive semi-definite, i.e., ∀x1, x2, . . . , xn ∈X, the Gram Matrix K defined as Ki j = k(xi, x j) is
positive semi-definite.
Definition 4. k(·, ·) is a reproducing kernel of a Hilbert space H if ∀f ∈H, f(x) = ⟨k(x, ·), f(·)⟩.
Definition 5. A Reproducing Kernel Hilbert Space (RKHS) is a Hilbert space H with a reproducing kernel
whose span is dense in H.
Therefore, an RKHS is a Hilbert space of functions with all evaluation functionals bounded and linear.
Appendix A.2. Build a Reproducing Kernel Hilbert Space
Given a kernel k, we can define a reproducing kernel feature map Φ : X →RX as:
Φ(x) = k(·, x).
(A.1)
Consider the vector space:
span ({Φ(x : x ∈X)}) =

n
X
i=1
αik(·, xi) : n ∈N, xi ∈X, αi ∈R
.
(A.2)
For f = P
i αik(·, µi) and g = P
i βik(·, υi), define ⟨f, g⟩= P
i, j αiβjk(µi, υj). Note that:
⟨f, k(·, x)⟩=
X
i
αik(x, µi) = f(x),
(A.3)
i.e., k has the reproducing property.
We show that ⟨f, g⟩is an inner product on the vector space defined by Eq. (A.2) by checking the follow-
ing conditions:
1. Symmetry: ⟨f, g⟩= P
i, j αiβjk(µi, υj) = P
i, j βjαik(υj, µi) = ⟨g, f⟩;
2. Bilinearity: ⟨f, g⟩= P
i αig(υi) = P
j βj f(µj);
3. Positive definiteness: ⟨f, f⟩= α′Kα ≥0 with equality if f = 0.
Then we can define a new Hilbert space by completing the inner product space ⟨·, ·⟩:
Definition 6. For a (compact) X ⊆Rd, and a Hilbert space H of functions f : X →R, we say H is a
Reproducing Kernel Hilbert Space if ∃k : X →R, s.t.
1. k has the reproducing property, i.e., f(x) = ⟨f(·), k(·, x)⟩;
2. k spans H = span{k(·, x) : x ∈X}.

24
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
Appendix B. Proof of Lemma 1
Lemma 1 (Gradient Derivation of the MMD term). Let ρπ(s, a) be the state-action visitation distribution
induced by the current policy π. Let D(x, M) be the MMD distance between the state-action pair x and
replay memory M. Then, if the policy π is parameterized by θ, the gradient of the MMD term of Eq. (5) for
parameters θ is derived as follows:
∇θDMMD =
E
ρπ(s,a)
∇θ log πθ(a|s)Qi(s, a) ,
(6)
where
Qi(st, at) =
E
ρπ(s,a)

T−t
X
l=0
γlRi(s, a)
,
(7)
and
Ri(s, a) = min {DMMD(x, M) −δ, 0} .
(8)
Proof. Let x = (s, a) denote a state-action pair. Let Ri(s, a) be the intrinsic reward function derived from the
maximum mean discrepancy:
Ri(s, a) = min {DMMD(x, M) −δ, 0} ,
and Qi(·, ·) be the Q-function calculated using Ri(s, a) as the reward:
Qi(st, at) = E

T−t
X
l=0
γlRi(st+l, at+l)
.
We can easily derive the formula from the policy gradient theorem [72]:
∇θDMMD = Eρπ(s,a)
∇θ log πθ(a|s)Qi(s, a) .
Appendix C. Proof of Theorem 1 and Two Relevant Corollaries
The following lemmas are proved in [61], and we have excerpted them here. The detailed proof process
can be found in the appendix of [61].
Lemma 2. For any function f : S →R and any policy π,
(1 −γ) E
s∼ρ0
f(s) + E
s∼dπ
a∼π
s′∼P
γ f(s′) −E
s∼dπ
 f(s) = 0,
(C.1)
where γ is the discount factor, ρ0 is the starting state distribution, and P is the transition probability function.
Lemma 3. For any function f : S →R and any policies π′ and π, define
Tπ, f (π′) .= E
s∼dπ
a∼π
s′∼P
" π′(a|s)
π(a|s) −1
!  R(s, a) + γ f(s′) −f(s)#
,
(C.2)
and ϵπ′
f
.= maxs
Ea∼π′,s′∼P[R(s, a) + γ f(s′) −f(s)]
. Consider the standard RL objective function J(πθ) =

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
25
Eτ
hP∞
t=0 γtR(st, at)
i
, the following bounds hold:
J(π′) −J(π) ≥
1
1 −γ

Tπ, f (π′) −2ϵπ′
f DTV(dπ′||dπ)

,
(C.3)
J(π′) −J(π) ≤
1
1 −γ

Tπ, f (π′) + 2ϵπ′
f DTV(dπ′||dπ)

,
(C.4)
where DTV is the total variational divergence. Furthermore, the bounds are tight (when π′ = π, the LHS and
RHS are identically zero). Here, γ is the discount factor and dπ is the discounted future state distribution.
Lemma 4. The divergence between discounted future state visitation distributions, ∥dπ′ −dπ∥1, is bounded
by an average divergence of the policies π′ and π:
∥dπ′ −dπ∥1 ≤
2γ
1 −γ E
s∼dπ
DTV(π′||π)[s] ,
(C.5)
where DTV(π′||π)[s] = (1/2) P
a |π′(a|s) −π(a|s)| is the total variational divergence at s.
Theorem 1. [Performance bound of the trajectory-constrained exploration strategy] For the proposed
constrained optimization problem of Eq. (3). Subsequently, for any policies π and π′, define δf (s, a, s′) .=
Re(s, a) + σRi(st, at) + γ f(s′) −f(s),
ϵπ′
f
.= max
s
Ea∼π′[δf (s, a, s′)]
 ,
Tπ, f (π′) .= E
s∼dπ
a∼π
" π′(a|s)
π(a|s) −1
!
δ f (s, a, s′)
#
, and
D±
π,f (π′) .= Tπ, f (π′)
1 −γ
±
2γϵπ′
f
(1 −γ)2 E
s∼dπ
DTV(π′||π)[s] ,
where s′ ∼P(·|s, a), Ri(s, a) is the MMD-based distance reward min {DMMD(x, M) −δ, 0} and γ is the
discount factor. DTV(π′||π)[s] = 1
2
P
a |π′(a|s) −π(a|s)| is used to represent the total variational divergence
between two action distributions of π and π′ when the state is s. The following bounds hold:
D+
π, f (π′) ≥L(θ′, σ) −L(θ, σ) ≥D−
π,f (π′),
(12)
Furthermore, the bounds are tight (when π′ = π, all three expressions are identically zero). Here, L(·, ·) is
defined in Eq. (4), σ is the Lagrange multiplier.
Proof. When the MMD gradient of Lemma 1 is integrated with the gradient of J(θ) to update the parameters
of the policy, the final gradient gθ for the parameter update of Eq. (5) can be expressed as:
gθ = Eρπ(s,a)
∇θ log πθ(a|s) (Qe(s, a) + σQi(s, a)) .
(C.6)
Due to the similarities of the forms between the DMMD gradient and the RL gradient of J(θ), MMD-based
constraints min {DMMD(x, M) −δ, 0} can be viewed as an intrinsic reward r(s, a) = min {DMMD(x, M) −δ, 0}
for each state-action pair and be integrated with environmental rewards as follows:
Qe(st, at) + σQi(st, at) = Est+1,at+1,...

∞
X
l=0
γl (Re(st+l, at+l) + σRi(s, a))

Then, with the bounds from Lemma 3 and bound the divergence DTV(dπ′||dπ) by Lemma 4, we can easily
come to the conclusion.
Theorem 1 is similar to Theorem 1 of [61]. Different the Theorem 1 in [61], our approach transforms

26
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
constrained optimization problems into unconstrained problems and considers the long-term effects of con-
straints for returns. Our proposed theorem can be used to analyze the effectiveness of our approach in
improving exploration.
Corollary 1. For any policies π and π′, with ϵπ′ .= maxs |Ea∼π′[A(s, a)]|, in which A(s, a) is the advantage
function calculated with the total reward R(s, a) .= Re(s, a) + σRi(st, at), then the following bound holds:
L(θ′, σ) −L(θ, σ) ≥
1
1 −γ E
s∼dπ
a∼π′
"
A(s, a) −2γϵπ′
1 −γ DTV(π′||π)[s]
#
.
(13)
Here, L(·, ·) is defined in Eq. (4), γ is the discount factor, and σ is the Lagrange multiplier.
Proof. Let f = ˜V in Theorem 1, and ˜V is the value function computed with environmental and MMD-based
rewards. Then, we can derive this corollary.
Corollary 2. Suppose a performance improvement threshold ∆for any policies π and π′, and π and π′ satisfy
Es∼dπ [DKL(π′||π)[s]] ≤η and E [Ai(s, a)] ≥β > 0. When (1 −γ)∆−E [Ae(s, a)] −
p
2ηγϵπ′(1 −γ)−1 > 0,
then if
σ ≥E
s∼dπ
a∼π′
−1 [Ai(s, a)]
(1 −γ)∆−E
s∼dπ
a∼π′
[Ae(s, a)] −
p
2ηγϵπ′
1 −γ
,
(16)
we have
L(θ′, σ) −L(θ, σ) ≥∆.
(17)
Here, L(·, ·) is defined in Eq. (4), γ is the discount factor, and σ is the Lagrange multiplier.
Proof. the TV-divergence and KL-divergence are related by DTV(p||q) ≤
p
DKL(p||q)/2 [62]. Combining
this inequality with Jensen’s inequality, we obtain:
E
s∼dπ
DTV(π′||π)[s] ≤E
s∼dπ

r
1
2DKL(π′||π)[s]
≤
r
1
2 E
s∼dπ [DKL(π′||π)[s]].
(C.7)
Further, notice that the advantage A(s, a) can be decomposed as the sum of the environmental advantage
Ae(s, a) and the MMD-based advantage Ai(s, a), which is expressed by:
Aπ(s, a) = Ae(s, a) + Ai(s, a),
(C.8)
Substituting Eq. (C.7) and (C.8) into the right-hand side of Eq. (13), and suppose that π and π′ satisfy
E
s∼dπ [DKL(π′||π)[s]] ≤η, we can obtain the following equation:
1
1 −γ E
s∼dπ
a∼π′
"
A(s, a) −2γϵπ′
1 −γ DTV(π′||π)[s]
#
≥
1
1 −γ
E
s∼dπ
a∼π′
[Ae(s, a)] + σ E
s∼dπ
a∼π′
[Ai(s, a)] −
√
2γϵπ′
1 −γ
q
E
s∼dπ [DKL(π′||π)[s]]

≥
1
1 −γ
E
s∼dπ
a∼π′
[Ae(s, a)] + σ E
s∼dπ
a∼π′
[Ai(s, a)] −
p
2ηγϵπ′
1 −γ
.
(C.9)
Let the last term of Eq. (C.9) be greater than ∆, i.e.
1
1 −γ
E
s∼dπ
a∼π′
[Ae(s, a)] + σ E
s∼dπ
a∼π′
[Ai(s, a)] −
p
2ηγϵπ′
1 −γ
≥∆.
(C.10)

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
27
After sorting out Eq. (C.10), we get:
σ ≥E
s∼dπ
a∼π′
−1 [Ai(s, a)]
(1 −γ)∆−E
s∼dπ
a∼π′
[Ae(s, a)] +
p
2ηγϵπ′
1 −γ
.
(C.11)
That is to say, L(θ′, σ) −L(θ, σ) ≥∆when σ satisfies Eq. (C.11).
Appendix D. An Overview of Hyperparameter Configurations & Search Spaces
Appendix D.1. Stable Baselines Default Configurations
Table Appendix
D.1 shows the default hyperparameters we used in the experiments of Section 7.
TCHRL is based on TRPO in the hierarchical navigation tasks, and hence some hyperparameters are not
necessary for it, and we use ”-” to indicate this situation.
Table D.2. Hyper-parameter choices of LOPE for different tasks.
Hyper-parameter
Grid World
Hierarchical
Discrete MPE
SparseAnt
Value Function Learning Rate
0.00012
−
0.00012
0.001
Policy Learning Rate
0.000018
0.01
0.000018
0.00009
Action Regularization
None
True
None
True
Policy Optimizer
Adam
Adam
Adam
Adam
Value Function Optimizer
Adam
Adam
Adam
Adam
Max Length per Episode
240
1e4
240
500
Episodes per iteration
8
16
8
30
Epoches per iteration
65
−
65
30
Discount Factor
0.99
0.99
0.99
0.99
Gradient Clipping
False
False
False
False
Clipped Epsilon
0.20
-
0.20
0.24
Initial Sigma
0.42
0.60
0.42
0.70
Delta
0.7
0.7
0.7
0.7
Appendix D.2. Sweep Values of Hyperparameters
To determine the optimal hyperparameter value, we first swept over the different hyperparameter values
for the same dimension in a relatively large range. For some hyperparameters, we further refined the scope
of the search.
For the learning rate, the first search scope was {1e −2, 5e −3, 1e −3, 5e −4, 1e −4, 5e −5, 1e −5, 5e −
6, 1e −6, 5e −7}. Then, we narrowed down the search and reduced the search step size, and the second
search scope was {5e−5, 4e−5, 3e−5, 2e−5, 1e−5, 9e−6, 8e−6, 7e−6, 6e−6, 5e−6}. After two rounds of
searching, we find that the highest performance of TACE was obtained when learn rate = 2e−5. Finally,
we fine-tuned its value around 2e −5 and finally obtained the learning rate of 1.8e −5.
For the clip range hyperparameter of PPO, which was the basis of our algorithm TCPPO, we selected
its proper value by scanning the values in {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. We selected the initial
Lagrange multiplier σ from the search space {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. The suitable value
of ϵ is determined by scanning over the set {0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4}.
Because we adopted the heuristic adaptive scaling method, the value of δ is environment-independent,
and hence the optimal value of δ can be selected for different environments by sweeping over the set
{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}.

28
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
Appendix E. Pseudocode of TCPPO Algorithms
Algorithm 1 describes our method TCPPO in detail.
Notations:
θ = Policy parameters
σ = Lagrange multiplier
α = Learning rate
N = Size of on-policy buffer
G = Number of goals in the environment
Algorithm 1: Trajectory-Constrained PPO
1: B ←on-policy buffer
2: M ←empty replay memory
3: θ, σ ∼initial parameters
4: for i = 0 to G do
5:
for j = 0 to N do
6:
Generate batch of N trajectories {τi}n
i and store them in B
7:
Calculate the MMD distance DMMD(x, M) for each state-action pair x
8:
Normalize the MMD distance DMMD(x, M) according to Eq. (9)
9:
Estimate the MMD gradient ∇θDMMD using M and B
10:
Estimate the policy gradient ∇θJ based on B
11:
Calculate the final gradient ∇θL = ∇θJ + σ∇θDMMD
12:
θ ←θ + α∇θL
13:
Update σ according to Eq. (10)
14:
end for
15: end for
Appendix F. Training Process of TCHRL Algorithms
Algorithm 2 describes our method TCHRL in detail. At each time step, the algorithm is executed
according to the framework shown in Fig. G.14. State-action pairs generated by the current policy are
stored in the on-policy set B. Then we use these experiences to estimate the policy gradient ∇θL according
to Eq. (5) and calculate the diversity measurement DMMD between different policies. Finally, we update
parameters of πθ with the gradient ascent algorithm and adapt the penalty factor according to Eq. (10).
Notations:
θ := {θh, θl} = Policy parameters
σ = Lagrange multiplier
α = Learning rate
N = Size of on-policy buffer
G = Number of goals in the environment

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
29
Algorithm 2: Trajectory-Constrained HRL
1: Pre-train low-level skills πθl
2: Initialize the on-policy buffer B
3: Initialize the replay memory M
4: Initialize high-level policy network parameters θh
5: Initialize the parameter σ
6: for i = 0 to M do
7:
for j = 0 to N do
8:
Generate batch of N trajectories {τi}n
i and store them in B
9:
Calculate the normalized MMD distance Dh
MMD and Dl
MMD for the high-level policy and the
low-level policy respectively
10:
Normalize the MMD distance Dh
MMD and Dl
MMD separately according to Eq. (9)
11:
Estimate the MMD gradient ∇θDMMD according to (G.4) using the estimations of Dh
MMD and
Dl
MMD
12:
Estimate the policy gradient ∇θJ based on B
13:
Calculate the final gradient ∇θL = ∇θJ + σ∇θDMMD
14:
θ ←θ + α∇θL
15:
Update σ according to Eq. (10)
16:
end for
17: end for
Appendix G. TCHRL Learning Frameworks
In this section, we introduce the implementation of our exploration strategy using a hierarchical policy.
Our implementation was based on the state-of-the-art skill-based hierarchical reinforcement learning algo-
rithm SNN4HRL [8]. Instead of freezing the low-level skills during the training phase of the downstream
task, our proposed diversity incentive adapts the pre-training skills along with the high-level policy training.
Fig. G.14 shows the execution process of our trajectory-constrained hierarchical reinforcement learning
algorithm. In this HRL algorithm with a 2-level hierarchy, the agent takes a high-level action (or a latent
code) zt every p timesteps after receiving a new observation st, i.e., zt = zkp if kp ≤t ≤(k + 1)p −1. The
low-level policy πθl is another neural network that treats the current observation st and high-level action zt
as inputs, and its outputs are low-level actions at used to interact with the environment directly. The skills
selected by the high-level policy are executed by the low-level policy for the next p time steps. In our
framework. Different skills of the low-level policy are distinguished by different latent codes z, and a single
stochastic neural network [55] is employed to encode all the pre-trained skills [8, 44].
Under our framework, a trajectory τ can be expressed as:
τ = (s0, a0, s1, a1, . . . , sT, aT),
(G.1)
and the probability of generating this trajectory can be expressed as [56]:
p(τ) =

T/p
Y
k=0

m
X
j=1
πθh(z j|skp)
(k+1)p−1
Y
t=kp
πθl(at|st, z j)

·
ρ(s0)
T
Y
t=1
P(st+1|st, at)
,
(G.2)
where m represents the number of different low-level skills, s0 ∼ρ0(s0), zkp ∼πθh(zkp|skp), at ∼πθl(at|st, zt),
and st+1 ∼P(st+1|st, at). Hence, we define the sequence of a high-level action zk followed by p low-level
actions (akp, . . . , a(k+1)p−1) as a macro action ˜a, then the probability of a macro action can be written as
π(˜a|skp) = πθh(zj|skp)
(k+1)p−1
Y
t=kp
πθl(at|st, z j).
(G.3)

30
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
Fig. G.14.
Graphical description of the algorithm execution process. When the TCHRL algorithm first starts, the high-level policy
outputs a latent code z0, then the low-level policy inputs state along with this latent code to the neural network. After the low-level
policy takes p time steps, the high-level policy selects another latent code z1.
TCHRL allows a high-level policy to select a pre-trained skill to perform low-level actions over several
time steps. Unlike general sub-goal-based HRL methods [38, 39, 73], these pre-trained skills do not need
to reach the sub-goals set by a high-level policy. Moreover, these low-level skills can be rewarded by
environmental and MMD distance rewards, which can be used to compute the Q-functions ˜Qh(skp, zkp)
and ˜Ql(st, zkp, at) for low-level and high-level policies, respectively. By Lemma 1, combining Eqs. (G.2)
and (G.3), the following hierarchical MMD gradient formula holds:
∇θDMMD = Eρπ(s,˜a)
"
∇θh log πθh(zkp|skp) ˜Qh(skp, zkp) +
(k+1)p−1
X
t=kp
∇θl log πθl(at|st, zkp) ˜Ql(st, zkp, at)
#
,
(G.4)
where ˜Q(skp, zkp) is calculated with high-level trajectories based on Eq. (7), and ˜Q(st, zkp, at) is calculated
with low-level trajectories similar to ˜Q(skp, zkp).
In this framework, replay memory M is maintained to store the suboptimal trajectories generated by
past suboptimal policies learned during the past training process. Instead, state-action pairs collected by
the current policy are stored in the on-policy buffer B. Our method uses these trajectories in M and B to
compute the MMD distance.
Appendix H. Training Process of TCMAE Algorithms
Algorithm 3 describes our method TCMAE in detail.
Notations:
I := {1, 2, . . . , n} = the finite sets of agents
θ := {θh, θl} = Policy parameters
σ = Lagrange multiplier
α = Learning rate
N = Size of on-policy buffer
G = Number of goals in the environment

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
31
Algorithm 3: Trajectory-Constrained MAE
1: Initialize the on-policy buffer B
2: Initialize the replay memory Mk, k ∈I
3: Initialize the parameters θk and σk, k ∈I
4: for i = 0 to M do
5:
for j = 0 to N do
6:
Generate batch of N trajectories {τi}n
i and store them in Bk for each agent
7:
for k ∈I do
8:
Update Mk using the trajectories of local observations and actions generated by the k−th agent
9:
Calculate the normalized MMD distance Dk
MMD for the k−th agent using the episodic
state-action visitation distributions of Ml, l ∈I, l , k
10:
Normalize the MMD distance Dk
MMD separately according to Eq. (9)
11:
Estimate the MMD gradient ∇θDk
MMD according to Eq. (6) with the estimations of Dk
MMD
12:
end for
13:
Estimate the policy gradient ∇θJ based on B
14:
Calculate the final gradient ∇θL = ∇θJ + P
k∈I σk∇θDk
MMD
15:
θ ←θ + α∇θL
16:
Update each σk according to Eq. (10)
17:
end for
18: end for
Appendix I. Additional Experimental Results
In this section, we further evaluate the performance of TCPPO in tasks with discrete state-action spaces,
as shown in Fig. 2(a). Compared with the maze in Fig. 2(a), the only difference in Fig. I.15 is the addition of
a new deception goal with a reward of 2. In addition, the settings for both grid world mazes were identical.
The size of the 2D grid world maze was 70 × 70 steps. All curves are obtained by averaging over different
random seeds, and for clarity, the shaded error bars represent 0.35 standard errors.
Fig. I.15. Gridworld with multiple goals.
As shown in Figs. I.16, the TCPPO algorithm significantly outperformed the other baseline methods
in this 2D grid world maze. Similar to the results in Section 7.1.1, TCPPO learned faster and achieved
higher average returns. Meanwhile, with the help of TCPPO, the agent avoided adopting myopic behaviors
and performed deep exploration. In particular, the performance gap between our approach and the baseline
methods was even greater than that described in Section 7.1.1. Hence, the proposed method has obvious
advantages for this more difficult task.
Furthermore, we compared the changing trends in the MMD distances of the different methods, as
shown in Fig. I.17. The results illustrate that TCPPO increases the MMD distance between the old and

32
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
(a)
(b)
Fig. I.16. Learning curves of average return and success rate for different methods when the replay memory stores previous suboptimal
trajectories leading to the two local optimal goals. The success rate is used to illustrate the frequency at which the agent reaches the
global optimal goal during training processes.
current policies during the training process. By contrast, the four baseline methods could not achieve such
a large MMD distance.
Fig. I.17. Changing trends of MMD distances.
Acknowledgment
This work was supported by the National Key R&D Program of China (2022ZD0116401)

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
33
Conflict of interest
The authors declare that they have no known competing financial interests or personal relationships that
could have appeared to influence the work reported in this paper
Authors’ Contributions
Guojian Wang: Conceptualization, Methodology, Software, Formal analysis, Writing–Original Draft.
Faguo Wu: Writing–Review and Editing, Supervision. Xiao Zhang: Validation, Supervision, Funding
acquisition. Ning Guo: Formal analysis, Visualization. Zhiming Zheng: Supervision, Validation, Funding
acquisition.
Availability of data and materials
The datasets generated during and/or analysed during the current study are available from the cor-
responding author on reasonable request.
Code for this paper is available at https://github.com/
buaawgj/TACE.
References
[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland,
G. Ostrovski, et al., Human-level control through deep reinforcement learning, Nature 518 (7540) (2015) 529–533.
[2] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region policy optimization, in: International Conference on
Machine Learning, PMLR, 2015, pp. 1889–1897.
[3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. V. D. Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel,
D. Hassabis, Mastering the game of go with deep neural networks and tree search, Nature 529 (2016) 484–489.
[4] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, Continuous control with deep reinforce-
ment learning, arXiv preprint arXiv:1509.02971.
[5] J. Schulman, P. Moritz, S. Levine, M. Jordan, P. Abbeel, High-dimensional continuous control using generalized advantage
estimation, arXiv preprint arXiv:1506.02438.
[6] S. Fujimoto, H. Hoof, D. Meger, Addressing function approximation error in actor-critic methods, in: International Conference
on Machine Learning, PMLR, 2018, pp. 1587–1596.
[7] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, P. Abbeel, Vime: Variational information maximizing exploration,
in: Advances in Neural Information Processing Systems, 2016.
[8] C. Florensa, Y. Duan, P. Abbeel, Stochastic neural networks for hierarchical reinforcement learning, arXiv preprint
arXiv:1704.03012.
[9] Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, C.-Y. Lee, Diversity-driven exploration strategy for deep reinforcement
learning, in: Advances in Neural Information Processing Systems, 2018.
[10] M. A. Masood, F. Doshi-Velez, Diversity-inducing policy gradient: Using maximum mean discrepancy to find a set of diverse
policies, in: International Joint Conference on Artificial Intelligence, 2019.
[11] Y. Guo, J. Choi, M. Moczulski, S. Feng, S. Bengio, M. Norouzi, H. Lee, Memory based trajectory-conditioned policies for
learning from sparse rewards, in: Advances in Neural Information Processing Systems, 2020.
[12] C. Gulcehre, T. Le Paine, B. Shahriari, M. Denil, M. Hoffman, H. Soyer, R. Tanburn, S. Kapturowski, N. Rabinowitz, D. Williams,
et al., Making efficient use of demonstrations to solve hard exploration problems, in: International Conference on Learning
Representations, 2019.
[13] T. Gangwani, Q. Liu, J. Peng, Learning self-imitating diverse policies, in: 7th International Conference on Learning Representa-
tions, 2019.
[14] Z. Peng, H. Sun, B. Zhou, Non-local policy optimization via diversity-regularized collaborative exploration, arXiv preprint
arXiv:2006.07781.
[15] I.-J. Liu, U. Jain, R. A. Yeh, A. Schwing, Cooperative exploration for multi-agent deep reinforcement learning, in: International
Conference on Machine Learning, PMLR, 2021, pp. 6826–6836.
[16] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, IEEE Transactions on Neural Networks (2005) 285–286.
[17] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, S. Whiteson, Monotonic value function factorisation for deep
multi-agent reinforcement learning, The Journal of Machine Learning Research 21 (1) (2020) 7234–7284.
[18] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy optimization algorithms, arXiv preprint
arXiv:1707.06347.
[19] I. Osband, B. Van Roy, Z. Wen, Generalization and exploration via randomized value functions, in: International Conference on
Machine Learning, PMLR, 2016, pp. 2377–2386.

34
G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
[20] T. Haarnoja, H. Tang, P. Abbeel, S. Levine, Reinforcement learning with deep energy-based policies, in: International Conference
on Machine Learning, PMLR, 2017, pp. 1352–1361.
[21] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor, in: International Conference on Machine Learning, PMLR, 2018, pp. 1861–1870.
[22] Y. Zhang, W. Yu, G. Turk, Learning novel policies for tasks, in: International Conference on Machine Learning, PMLR, 2019,
pp. 7483–7492.
[23] Y. Burda, H. Edwards, A. Storkey, O. Klimov, Exploration by random network distillation, in: International Conference on
Learning Representations, 2018.
[24] J. Achiam, S. Sastry, Surprise-based intrinsic motivation for deep reinforcement learning, arXiv preprint arXiv:1703.01732.
[25] B. C. Stadie, S. Levine, P. Abbeel, Incentivizing exploration in reinforcement learning with deep predictive models, arXiv preprint
arXiv:1507.00814.
[26] D. Pathak, P. Agrawal, A. A. Efros, T. Darrell, Curiosity-driven exploration by self-supervised prediction, in: International
Conference on Machine Learning, PMLR, 2017, pp. 2778–2787.
[27] M. Tipaldi, R. Iervolino, P. R. Massenio, Reinforcement learning in spacecraft control applications: Advances, prospects, and
challenges, Annual Reviews in Control.
[28] M. Plappert, R. Houthooft, P. Dhariwal, S. Sidor, R. Y. Chen, X. Chen, T. Asfour, P. Abbeel, M. Andrychowicz, Parameter space
noise for exploration, in: International Conference on Learning Representations, 2018.
[29] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin, et al., Noisy
networks for exploration, arXiv preprint arXiv:1706.10295.
[30] J. Oh, Y. Guo, S. Singh, H. Lee, Self-imitation learning, in: International Conference on Machine Learning, PMLR, 2018, pp.
3878–3887.
[31] T. Hester, M. Vecer´ık, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, G. Dulac-Arnold,
J. P. Agapiou, J. Z. Leibo, A. Gruslys, Deep q-learning from demonstrations, in: AAAI Conference on Artificial Intelligence,
2018.
[32] C. Gulcehre, T. Le Paine, B. Shahriari, M. Denil, M. Hoffman, H. Soyer, R. Tanburn, S. Kapturowski, N. Rabinowitz, D. Williams,
et al., Making efficient use of demonstrations to solve hard exploration problems, in: International Conference on Learning
Representations, 2019.
[33] D. Rengarajan, G. Vaidya, A. Sarvesh, D. Kalathil, S. Shakkottai, Reinforcement learning with sparse rewards using guidance
from offline demonstration, in: International Conference on Learning Representations, 2021.
[34] P. Dayan, G. E. Hinton, Feudal reinforcement learning, in: Advances in Neural Information Processing Systems, 1992.
[35] R. S. Sutton, D. Precup, S. Singh, Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement
learning, Artificial Intelligence (1999) 181–211.
[36] T. G. Dietterich, Hierarchical reinforcement learning with the maxq value function decomposition, Journal of artificial intelli-
gence research 13 (2000) 227–303.
[37] N. Chentanez, A. Barto, S. Singh, Intrinsically motivated reinforcement learning, in: Advances in Neural Information Processing
Systems, 2004.
[38] A. Levy, G. Konidaris, R. Platt, K. Saenko, Learning multi-level hierarchies with hindsight, in: Proceedings of International
Conference on Learning Representations, 2019.
[39] O. Nachum, S. S. Gu, H. Lee, S. Levine, Data-efficient hierarchical reinforcement learning, in: Advances in Neural Information
Processing Systems, 2018.
[40] N. Heess, G. Wayne, Y. Tassa, T. Lillicrap, M. Riedmiller, D. Silver, Learning and transfer of modulated locomotor controllers,
arXiv preprint arXiv:1610.05182.
[41] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, K. Kavukcuoglu, Feudal networks for hierarchical
reinforcement learning, in: International Conference on Machine Learning, PMLR, 2017, pp. 3540–3549.
[42] Z. Dwiel, M. Candadai, M. Phielipp, A. K. Bansal, Hierarchical policy learning is sensitive to goal space design, arXiv preprint
arXiv:1905.01537.
[43] B. Eysenbach, A. Gupta, J. Ibarz, S. Levine, Diversity is all you need: Learning skills without a reward function, in: International
Conference on Learning Representations, 2018.
[44] S. Li, R. Wang, M. Tang, C. Zhang, Hierarchical reinforcement learning with advantage-based auxiliary rewards, in: Advances
in Neural Information Processing Systems, 2019.
[45] P.-L. Bacon, J. Harb, D. Precup, The option-critic architecture, in: Proceedings of the Thirty-First AAAI Conference on Artificial
Intelligence, 2017, pp. 1726–1734.
[46] J. Harb, P.-L. Bacon, M. Klissarov, D. Precup, When waiting is not an option: Learning options with a deliberation cost, in:
Proceedings of the AAAI Conference on Artificial Intelligence, no. 1, 2018.
[47] A. Mahajan, T. Rashid, M. Samvelyan, S. Whiteson, Maven: Multi-agent variational exploration, Advances in neural information
processing systems 32.
[48] T. Wang, J. Wang, Y. Wu, C. Zhang, Influence-based multi-agent exploration, in: International Conference on Learning Repre-
sentations, 2020.
[49] L. Zheng, J. Chen, J. Wang, J. He, Y. Hu, Y. Chen, C. Fan, Y. Gao, C. Zhang, Episodic multi-agent reinforcement learning with
curiosity-driven exploration, Advances in Neural Information Processing Systems 34 (2021) 3757–3769.
[50] A. Gretton, K. Borgwardt, M. Rasch, B. Sch¨olkopf, A. Smola, A kernel method for the two-sample-problem, in: Advances in
Neural Information Processing Systems, 2006, pp. 513–520.
[51] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, B. K. Sriperumbudur, Optimal kernel choice
for large-scale two-sample tests, in: Advances in neural information processing systems, Citeseer, 2012, pp. 1205–1213.
[52] P. Thomas, B. C. Silva, C. Dann, E. Brunskill, Energetic natural gradient descent, in: International Conference on Machine

G. Wang et al. / Procedia Computer Science 00 (2023) 1–35
35
Learning, PMLR, 2016, pp. 2887–2895.
[53] G. K. Dziugaite, D. M. Roy, Z. Ghahramani, Training generative neural networks via maximum mean discrepancy optimization,
in: Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, 2015, pp. 258–267.
[54] R. Fortet, E. Mourier, Convergence de la r´epartition empirique vers la r´epartition th´eorique, in: Annales scientifiques de l’´Ecole
Normale Sup´erieure, 1953, pp. 267–285.
[55] C. Tang, R. R. Salakhutdinov, Learning stochastic feedforward neural networks, in: Advances in Neural Information Processing
Systems, 2013, pp. 530–538.
[56] A. Li, C. Florensa, I. Clavera, P. Abbeel, Sub-policy adaptation for hierarchical reinforcement learning, in: International Confer-
ence on Learning Representations, 2019.
[57] C. Bai, L. Wang, L. Han, J. Hao, A. Garg, P. Liu, Z. Wang, Principled exploration via optimistic bootstrapping and backward
induction, in: International Conference on Machine Learning, PMLR, 2021, pp. 577–587.
[58] S. Ioffe, C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: Interna-
tional Conference on Machine Learning, PMLR, 2015, pp. 448–456.
[59] S. Kakade, J. Langford, Approximately optimal approximate reinforcement learning, in: In Proceedings 19th International Con-
ference on Machine Learning, Citeseer, 2002.
[60] M. Pirotta, M. Restelli, A. Pecorino, D. Calandriello, Safe policy iteration, in: International Conference on Machine Learning,
PMLR, 2013, pp. 307–315.
[61] J. Achiam, D. Held, A. Tamar, P. Abbeel, Constrained policy optimization, in: International conference on machine learning,
PMLR, 2017, pp. 22–31.
[62] I. Csisz´ar, J. K¨orner, Information theory: coding theorems for discrete memoryless systems, Cambridge University Press, 2011.
[63] E. Todorov, T. Erez, Y. Tassa, Mujoco: A physics engine for model-based control, in: IEEE/RSJ International Conference on
Intelligent Robots and Systems, IEEE, 2012, pp. 5026–5033.
[64] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, Openai gym (2016). arXiv:arXiv:
1606.01540.
[65] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, Benchmarking deep reinforcement learning for continuous control, in:
International Conference on Machine Learning, PMLR, 2016, pp. 1329–1338.
[66] R. Lowe, A. Tamar, J. Harb, O. Pieter Abbeel, I. Mordatch, Multi-agent actor-critic for mixed cooperative-competitive environ-
ments, Advances in neural information processing systems 30.
[67] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu, Asynchronous methods for deep
reinforcement learning, in: International Conference on Machine Learning, PMLR, 2016, pp. 1928–1937.
[68] R. Raileanu, T. Rockt¨aschel, Ride: Rewarding impact-driven exploration for procedurally-generated environments, arXiv preprint
arXiv:2002.12292.
[69] T. Zhang, H. Xu, X. Wang, Y. Wu, K. Keutzer, J. E. Gonzalez, Y. Tian, Noveld: A simple yet effective exploration criterion,
Advances in Neural Information Processing Systems 34 (2021) 25217–25230.
[70] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, Y. Wu, The surprising effectiveness of ppo in cooperative multi-agent
games, Advances in Neural Information Processing Systems 35 (2022) 24611–24624.
[71] P. Bartlett, Lecture of reproducing kernel hilbert spaces, in: CS281B/Stat241B (Spring 2008) Statistical Learning Theory, 2008,
pp. 1–4.
[72] R. S. Sutton, D. McAllester, S. Singh, Y. Mansour, Policy gradient methods for reinforcement learning with function approxima-
tion, in: Advances in Neural Information Processing Systems, 1999.
[73] O. Nachum, S. Gu, H. Lee, S. Levine, Near-optimal representation learning for hierarchical reinforcement learning, in: Interna-
tional Conference on Learning Representations, 2018.

