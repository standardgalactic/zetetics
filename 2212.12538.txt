Mathematical Foundations for a
Compositional Account of the
Bayesian Brain
Toby St Clere Smithe
St Edmund Hall
University of Oxford
A thesis submitted for the degree of
Doctor of Philosophy
Hilary 2023
arXiv:2212.12538v1  [q-bio.NC]  23 Dec 2022

Acknowledgements
This thesis would not exist in anything like this form without the marvellous Applied
Category Theory community, a more welcoming and thoughtful group of researchers
one could not wish to ﬁnd. This community makes a serious and thoroughgoing eﬀort
to be inclusive and outward-looking, and it was in this spirit that they set up the Applied
Category Theory Adjoint School, which I attended in 2019, and which I recommend any
category-theory-curious thinker to attend. Without that experience, and the group of
friends I made there, none of this would have been possible.
Before I attended the Adjoint School, I was trying to understand too much about
the brain, and seeking a mathematically coherent unifying framework with which
I could organize my thoughts. In Oxford, I was a member of the Department of
Experimental Psychology, but had become aware of the work being done on cognition
and linguistics in the Quantum Group, in the Department of Computer Science, and
so I began attending lectures and hanging around there. It was there that I attended
the Open Games workshop in 2018, at which I realized that predictive coding and
open games had the same abstract structure; a fact that took me longer than it should
have to formalize, but about which I started telling anyone who listened. The ﬁrst
individuals who took me seriously were Jules Hedges and Brendan Fong, and I thank
them heartily for their encouragement and assistance: it was after discussion with Jules
(and Bruno Gavranović) at the Sixth Symposium on Compositional Structures (SYCO
6, in Leicester) that I proved abstractly that “Bayesian updates compose optically”;
and it was Brendan Fong who let me know about the Adjoint School, at which we
(Brendan, Bruno, David Spivak, David Jaz Myers, and Sophie Libkind, as well as others
occasionally, including Jules, Eliana Lorch, and davidad) discussed autopoiesis from a
categorical perspective.
After these meetings, and through my Quantum Group interactions, I acquired some
funding from the Foundational Questions Institute to concentrate on the category
theory of predictive coding and approximate inference, which was distributed through
the Topos Institute. I thank everyone who made these interactions possible and
delightful, including (in no particular order) the following individuals that I have not

yet named: Samson Abramsky; Bob Coecke; Johannes Kleiner; Tim Hosgood; Valeria
de Paiva; Evan Patterson; Sam Staton; Juliet Szatko; Tish Tanski; Sean Tull; and Vincent
Wang.
Outside of Oxford, I have been fortunate to be part of some wonderful interactions
through the Active Inference and Strathclyde MSP (Mathematically Structured Program-
ming) communities. I ﬁrst spoke about categorical active inference to Karl Friston’s
group in March 2020, shortly after my ﬁrst visit to Glasgow at the end of 2019; and I
found Glasgow so appealing that I now ﬁnd myself living there. For these interactions,
besides those named above, I must recognize: Dylan Braithwaite; Matteo Capucci;
Lance da Costa; Neil Ghani; Maxwell Ramstead; Riu Rodríguez Sakamoto; and Dalton
Sakthivadivel.
I would not have had the opportunity to pursue this research at all had I not been
granted a position in the Oxford Experimental Psychology department, where I have
been a member of the Oxford Centre for Theoretical Neuroscience and Artiﬁcial
Intelligence (OCTNAI), under the direction of Simon Stringer. I thank Simon for his
patience and latitude, particularly when my plans were not quite as he would have
expected, and I thank my Oxford co-supervisor, Mark Buckley, and director of graduate
studies, Brian Parkinson, for their always excellent advice. Thanks also to the other
student members of OCTNAI (particularly Dan, Hannah, Harry, James, Nas, and Niels)
for being so welcoming to an oddball such as myself. And at this point, it would be
remiss not to thank also the administrative staﬀof the Department, and my college, St
Edmund Hall, who are always helpful and wise; in particular, Rebecca Cardus, who
has guided me through much of Oxford’s strange bureaucracy.
Finally, and most of all, I thank my family and my beloved wife, Linda, who in particular
has suﬀered through this long journey with me with beyond-inﬁnite patience, love, and
understanding (so much patience, in fact, that she humoured the category-theoretic
content of my wedding speech!). Thank you, to you all.
3

Abstract
This dissertation reports some ﬁrst steps towards a compositional account of active
inference and the Bayesian brain. Speciﬁcally, we use the tools of contemporary applied
category theory to supply functorial semantics for approximate inference. To do so, we
deﬁne on the ‘syntactic’ side the new notion of Bayesian lens and show that Bayesian
updating composes according to the compositional lens pattern. Using Bayesian lenses,
and inspired by compositional game theory, we deﬁne categories of statistical games
and use them to classify various problems of statistical inference. On the ‘semantic’
side, we present a new formalization of general open dynamical systems (particularly:
deterministic, stochastic, and random; and discrete- and continuous-time) as certain
coalgebras of polynomial functors, which we show collect into monoidal opindexed
categories (or, alternatively, into algebras for multicategories of generalized polynomial
functors). We use these opindexed categories to deﬁne monoidal bicategories of
cilia: dynamical systems which control lenses, and which supply the target for
our functorial semantics. Accordingly, we construct functors which explain the
bidirectional compositional structure of predictive coding neural circuits under the
free energy principle, thereby giving a formal mathematical underpinning to the
bidirectionality observed in the cortex. Along the way, we explain how to compose
rate-coded neural circuits using an algebra for a multicategory of linear circuit diagrams,
showing subsequently that this is subsumed by lenses and polynomial functors. Because
category theory is unfamiliar to many computational neuroscientists and cognitive
scientists, we have made a particular eﬀort to give clear, detailed, and approachable
expositions of all the category-theoretic structures and results of which we make use.
We hope that this dissertation will prove helpful in establishing a new “well-typed”
science of life and mind, and in facilitating interdisciplinary communication.

Contents
1. Introduction
1
1.1.
Overview of the dissertation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.
Contributions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2. Basic category theory for computational and cognitive (neuro)scientists
9
2.1.
Categories, graphs, and networks
. . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1.1.
Three examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.1.1.1.
Neural circuits: dynamical networks of neurons . . . . . . . . .
10
2.1.1.2.
Bayesian networks: belief and dependence . . . . . . . . . . . .
10
2.1.1.3.
Computations: sets and functions . . . . . . . . . . . . . . . . .
11
2.1.2.
From graphs to categories . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.1.2.1.
Diagrams in a category, functorially . . . . . . . . . . . . . . . .
16
2.2.
Connecting the connections
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2.1.
Enriched categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2.2.
2-categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.2.3.
On functorial semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.2.4.
Adjunction and equivalence . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.3.
Universal constructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.3.1.
The universality of common patterns . . . . . . . . . . . . . . . . . . . . .
37
2.3.1.1.
Disjunctions, or coproducts
. . . . . . . . . . . . . . . . . . . .
37
2.3.1.2.
Conjunctions, products, and sections . . . . . . . . . . . . . . .
39
2.3.1.3.
Subobjects and equalizers . . . . . . . . . . . . . . . . . . . . . .
41
2.3.1.4.
Coequalizers and quotients . . . . . . . . . . . . . . . . . . . . .
42
2.3.2.
The pattern of universality . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.3.3.
Limits and colimits: mapping in to and out of diagrams . . . . . . . . . . .
46
2.3.3.1.
Functoriality of taking limits . . . . . . . . . . . . . . . . . . . .
48
2.3.3.2.
(Co)limits as adjoints . . . . . . . . . . . . . . . . . . . . . . . .
49
2.3.3.3.
Hom preserves limits . . . . . . . . . . . . . . . . . . . . . . . .
50
2.3.4.
Closed categories and exponential objects . . . . . . . . . . . . . . . . . .
53
2.3.4.1.
Dependent products . . . . . . . . . . . . . . . . . . . . . . . . .
56
i

2.4.
The Yoneda Lemma: a human perspective . . . . . . . . . . . . . . . . . . . . . . .
57
2.4.1.
Formalizing categorical reasoning via the Yoneda embedding . . . . . . .
58
2.4.2.
Knowing a thing by its relationships . . . . . . . . . . . . . . . . . . . . .
59
3. Algebraic connectomics
67
3.1.
Categories and calculi for process theories . . . . . . . . . . . . . . . . . . . . . .
68
3.1.1.
String diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.1.2.
Monoidal categories
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.1.3.
Closed monoidal categories . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.1.4.
Bicategories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.2.
Parameterized systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.2.1.
Internal parameterization
. . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.2.2.
External parameterization . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.3.
Systems from circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.3.1.
Multicategorical algebra for hierarchical systems . . . . . . . . . . . . . .
84
3.3.2.
Linear circuit diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.3.3.
An algebra of rate-coded neural circuits . . . . . . . . . . . . . . . . . . .
88
3.4.
From monoids to monads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
3.4.1.
Comonoids
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
3.5.
Polynomial functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
4. The compositional structure of Bayesian inference
110
4.1.
Compositional probability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
4.1.1.
Discrete probability, algebraically . . . . . . . . . . . . . . . . . . . . . . .
112
4.1.1.1.
Stochastic matrices . . . . . . . . . . . . . . . . . . . . . . . . .
117
4.1.1.2.
Monoidal structure . . . . . . . . . . . . . . . . . . . . . . . . .
117
4.1.1.3.
Copy-discard structure . . . . . . . . . . . . . . . . . . . . . . .
118
4.1.1.4.
Bayesian inversion
. . . . . . . . . . . . . . . . . . . . . . . . .
119
4.1.2.
Abstract Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . .
120
4.1.3.
Density functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.1.4.
S-ﬁnite kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
4.2.
Dependent data and bidirectional processes . . . . . . . . . . . . . . . . . . . . . .
129
4.2.1.
Indexed categories and the Grothendieck construction . . . . . . . . . . .
130
4.2.1.1.
The monoidal Grothendieck construction . . . . . . . . . . . . .
135
4.2.2.
Grothendieck lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
4.2.2.1.
Monoidal categories of lenses
. . . . . . . . . . . . . . . . . . .
142
ii

4.3.
The bidirectional structure of Bayesian updating . . . . . . . . . . . . . . . . . . .
143
4.3.1.
State-dependent channels . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
4.3.2.
Bayesian lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
4.3.3.
Bayesian updates compose optically
. . . . . . . . . . . . . . . . . . . . .
147
4.3.4.
Lawfulness of Bayesian lenses . . . . . . . . . . . . . . . . . . . . . . . . .
149
5. Open dynamical systems, coalgebraically
152
5.1.
Categorical background on dynamics and coalgebra . . . . . . . . . . . . . . . . .
152
5.1.1.
Dynamical systems and Markov chains . . . . . . . . . . . . . . . . . . . .
152
5.1.2.
Coalgebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
5.2.
Open dynamical systems on polynomial interfaces . . . . . . . . . . . . . . . . . .
156
5.2.1.
Deterministic systems in general time
. . . . . . . . . . . . . . . . . . . .
156
5.2.2.
Polynomials with ‘eﬀectful’ feedback, and open Markov processes
. . . .
162
5.2.3.
Open random dynamical systems . . . . . . . . . . . . . . . . . . . . . . .
165
5.3.
Cilia: monoidal bicategories of cybernetic systems . . . . . . . . . . . . . . . . . .
169
5.3.1.
Hierarchical bidirectional dynamical systems . . . . . . . . . . . . . . . .
169
5.3.2.
Diﬀerential systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6. Bayesian brains are open cybernetic systems
180
6.1.
Contexts for Bayesian lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
6.1.1.
Simple contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
6.1.2.
Interlude on profunctors and coends . . . . . . . . . . . . . . . . . . . . .
182
6.1.3.
Complex contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
6.2.
Statistical games: formalizing approximate inference problems . . . . . . . . . . .
188
6.2.1.
Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
6.2.1.1.
Maximum likelihood estimation . . . . . . . . . . . . . . . . . .
190
6.2.1.2.
Approximate Bayesian inference . . . . . . . . . . . . . . . . . .
191
6.2.1.3.
Autoencoder games and the free energy
. . . . . . . . . . . . .
191
6.2.1.4.
Validity games . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
6.2.2.
Parameterized statistical games . . . . . . . . . . . . . . . . . . . . . . . .
195
6.2.2.1.
Internally parameterized statistical games
. . . . . . . . . . . .
196
6.2.2.2.
Externally parameterized statistical games . . . . . . . . . . . .
198
6.3.
Approximate inference doctrines . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
6.3.1.
Channels with Gaussian noise . . . . . . . . . . . . . . . . . . . . . . . . .
199
6.3.2.
Predictive coding circuits and the Laplace doctrine . . . . . . . . . . . . .
203
6.3.3.
Synaptic plasticity with the Hebb-Laplace doctrine . . . . . . . . . . . . .
213
iii

7. Future directions
221
7.1.
Structured worlds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
7.1.1.
Bayesian sensor fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
7.1.2.
Learning structure and structured learning . . . . . . . . . . . . . . . . . .
223
7.1.3.
Compositional cognitive cartography . . . . . . . . . . . . . . . . . . . . .
224
7.2.
Societies of systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
7.2.1.
Active inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
7.2.2.
What is the type of a plan?
. . . . . . . . . . . . . . . . . . . . . . . . . .
227
7.2.3.
Reinforcement learning, open games, and ecosystems
. . . . . . . . . . .
228
7.3.
The mathematics of life . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
7.3.1.
Bayesian mechanics and the free energy principle . . . . . . . . . . . . . .
230
7.3.2.
Biosemiotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
7.4.
Fundamental theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
7.4.1.
Geometric methods for (structured) belief updating . . . . . . . . . . . . .
232
7.4.2.
Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
7.4.3.
Computation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
A. Auxiliary material
235
A.1. From monads to operads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
A.2. 2-local contexts, graphically
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
B. Bibliography
241
iv

1. Introduction
The work of which this dissertation is a report began as a project to understand the brain’s “cognitive
map”, its internal representation of the structure of the world. Little of that work is reported here,
for it rapidly became clear at the outset that there was no coherent framework in which such a
project should most proﬁtably be undertaken. This is not to say that no progress on understanding
the cognitive map can be made, a claim which would be easily contradicted by the evidence. Rather,
each research group has its own language and its own research questions, and it is not always
evident how to translate concepts from one group, or even one moment in time, faithfully to
another; what translation is done is performed at best highly informally.
If the aim of science1 is to tell just-so stories, or if the aim is only to answer one’s own research
questions in isolation, then this state of aﬀairs may be perfectly satisfactory. But the brain and the
behaviours that brains produce are so marvellous and so complex, and the implications of a ﬁner
understanding so monumental, that one cannot but hope that science could do better. Of course, of
late, science has not been doing better, with disciplines as socially important as psychology [1]
and medicine [2–4] and machine learning [5, 6] struck by crises of reproducibility. At the same
time, as broadband internet has spread across the globe, the sheer amount of output produced by
scientists and other researchers has ballooned, contributing to the impossibility of veriﬁcation
and the aforementioned translational diﬃculties, at least if one desires to do other than simply
following the herd. In some sense, although scientists all now speak English, science still lacks a
lingua franca, or at least a suﬃciently precise one.
As luck would have it, while mainstream science has been suﬀering from this loss of faith,
the ﬁrst phrases of a potentially adequate precise new language have begun to spread, with the
coalescence of a new community of researchers in applied category theory2. One part of the present
diﬃculty of scientiﬁc translation is that each research group has not only its own language, but
also its own perspective; and another part of the diﬃculty is that these languages and perspectives
1Or indeed, “if the aim of scientists”, as science itself may not have volition of its own.
2The ﬁrst major interdisciplinary meeting of applied category theorists (or at least the ﬁrst meeting suﬃciently conﬁdent
to take Applied Category Theory as its name) was held in 2018 in Leiden, although categorical methods have for some
time been used in computer science [7] and physics [8], and especially at their nexus [9–11]. More sporadically,
category theory had shown up elsewhere, such as in biology [12, 13], network theory [14–16], game theory [17–19],
cognitive science [20–24] and linguistics [25–27], and in 2014 a workshop was held at Dagstuhl bringing together
some of these researchers [28], in what was to be a precursor to the Applied Category Theory meetings; many of
those researchers still work in this new interdisciplinary ﬁeld.
1

are not well connected, with the English language a very lossy medium through which to make
these connections. Fortunately, the language of category theory—being a mathematical rather than
a natural language—resolves both of these diﬃculties.
Category theory is the mathematics of pattern, composition, connection, and interaction; its
concepts are as crisp and clear as the water of a mountain pool; its simplicity lends it great power.
Categories describe how objects can be constructed from parts, and such compositional descriptions
extend to categories themselves: as a result, the language of category theory is ‘homoiconic’, and
can be used to translate constructions between contexts. One is able to abstract away from irrelevant
details, and show precisely how structures give rise to phenomena; and by choosing the abstractions
carefully, it becomes possible to see that, sometimes, important constructions are ‘universal’, able
to be performed in any relevant context. As a result, category theory resolves both problems of
scientiﬁc translation indicated above: concepts expressed categorically are inevitably expressed in
context, and not in isolation; and these contexts are naturally interconnected as if by a categorical
web (with the connections also expressed categorically). Moreover, not being English, categorical
deﬁnitions tend to be extremely concise and information-dense; and since the basic concepts of
category theory are themselves simple, concepts so expressed are not biased by geography or
geopolitics.
From the middle of the 20th century, the concepts of category theory began to revolutionize much
of mathematics3, and applied category theorists such as the present author believe that the time is
nigh for this revolution to spread throughout the sciences and alleviate some of their struggles.
Just as the internet constitutes physical infrastructure that fundamentally accelerates human
communications, we expect category theory to constitute conceptual infrastructure of similar
catalytic consequence. This thesis is a contribution to building this infrastructure, in the speciﬁc
domain of computational neuroscience and the general domain of (what was once, and will be again,
called) cybernetics4. In particular, we show that a prominent theory of brain function—predictive
coding—has a clear compositional structure, that explains the bidirectional circuitry observed in the
brain [35], and that renders precise connections to the structure of statistical and machine learning
3The basic concepts of category theory were originally written down by Eilenberg and Mac Lane in order to formalize
processes of translation, and so clarify structures in the ways indicated in the main text above, in the ﬁeld of algebraic
topology. This occurred at the end of the ﬁrst half of the 20th century, in 1945 [29]. The ideas soon spread beyond
algebraic topology, gathering momentum rapidly from the 1950s, in which Cartan deﬁned the concept of sheaf [30,
31] and Grothendieck reconceived the foundations of algebraic geometry [32]. By the mid-1960s, and especially
through the work of Lawvere on logic [33] and set theory [34], it was clear that category theory would be able to
supply supple but sturdy new foundations for all of mathematics.
4Owing to its aﬃnity for pattern and abstraction, it is hard to do interesting domain-speciﬁc work in category theory
without there being at least some more general results to be found, and indeed this is the case here: what began as
a project in theoretical neuroscience swiftly became a study of adaptive and cybernetic systems more broadly, of
which the brain is of course the prime exemplar.
2

systems [36–38], as well as to the structure of much larger scale adaptive systems traditionally
modelled by economic game theory [17].
Predictive coding models were originally developed in the neuroscience of vision to explain
observations that neural activity might decrease as signals became less surprising [39] (rather
than increase as signals became more ‘preferred’), as well as to explain the robustness of sensory
processing to noise [40] and a source of metabolic eﬃciency [41]5. The typical form of these
models involves a neuron or neural ensemble representing the system’s current prediction of (or
expectation about) its input, alongside another neuron or ensemble representing the diﬀerence
between this prediction and the actual input (i.e., representing the prediction error). We can think
of the former ensemble as directed from within the brain towards the sensory interface (such as
the retina), and the latter ensemble as carrying information from the world into the brain: this is
the aforementioned bidirectionality.
Another important observation about visual processing in the brain is that its circuitry seems
to be roughly hierarchical [42], with regions of cortex further from the retina being involved in
increasingly abstract representation [43]. Given a model of predictive coding at the level of a single
circuit, accompanied by models of how sensory circuits are coupled (and their representations
transformed), a natural next step is to construct hierarchical predictive coding models, in an attempt
to extend the beneﬁts of the single circuit to a whole system; and indeed such hierarchical circuits
were prominently proposed in the literature [39, 44].
This hierarchical structure is a hint of compositionality, and thus a sign that a categorical
approach may be helpful and enlightening. This impression is strengthened when one considers a
particularly inﬂuential class of predictive coding models, obtained in the context of the “free energy
principle” [44–46], where the underlying equations themselves exhibit a form of compositionality
which is (more or less explicitly) used to obtain the hierarchical models6. Despite this hint of
compositionality, the equations of motion for these hierarchical systems are typically derived from
scratch each time [35, 47–52], a redundant eﬀort that would not be required had a compositional
formalism such as category theory been used from the start. This thesis supplies such a categorical
formalism and exempliﬁes it with hierarchical predictive coding under the free energy principle.
The “free energy” framework not only underpins a modern understanding of predictive coding,
but has more broadly been proposed as a uniﬁed theory of brain function [45], and latterly of all
adaptive or living systems [53–56]. In the neuroscientiﬁc context, it constitutes a theory of the
Bayesian brain, by which most or all brain function can be understood as implementing approximate
5If the prediction is good, then communicating the diﬀerence between prediction and actuality can be done much more
eﬃciently than transmitting the whole incoming signal, which would contain much redundant information. This is
the principle underlying most data compression algorithms.
6That is to say, the dynamics of each level of hierarchy i are governed by a quantity Fi, and the dynamics of two
adjacent levels i and i ` 1 are governed by Fi ` Fi`1; see Buckley et al. [47, Eq. 72].
3

Bayesian inference [57]; in the more broadly biological (or even metaphysical) contexts, this claim
is generalized to state that all life can be understood in this way. However, despite these claims to
universality, these proposals have to date been quite informally speciﬁed, leading to confusion [58,
59] and charges of unfalsiﬁability [56, 60, 61]. As we will see, category theory has a rich formal
vocabulary for precisely describing universal constructions, and so not only does a categorical
formulation of the free energy framework promise to clarify the current confusions, but it may be
expected also to shed light on its potential universality. In particular, as we describe in Chapter 7,
we will be able to make precise the questions of whether any dynamical system of the appropriate
type can universally be seen as performing approximate inference (in our language, “playing a
statistical game”), and of whether any cybernetic system (such as an economic game player) can be
expressed as an active inference system.
The notion of active inference is closely related to the free energy framework: an active inference
model of a system describes both the processes by which it updates its internal states on the
basis of incoming signals, and the processes by which it chooses how to act, using approximate
Bayesian inference. In this thesis, we do not get as far as a completely general formulation of active
inference, but we hope that our development of statistical games and their “dynamical semantics”
in approximate inference doctrines will provide a useful starting point for such a formulation, and
in our ﬁnal chapter (7) we sketch how we might expect this formulation to go. Because active
inference models, and the free energy framework more broadly, are descriptions of systems that
are ‘open’ to an environment, interacting with it, and therefore situated “in context”, they are
particularly suited to a category-theoretic reformulation. Likewise, Bayesianism and the free energy
framework lend themselves to a subjectivist metaphysics [53, 62, 63], which is itself in alignment
with the unavoidable perspective-taking of categorical models, and which is not dissimilar from
the emerging ‘biosemiotic’ reconceptualization of biological information-processing [64]. As we
have indicated, categorical tools help us to draw connections between concepts, and we see our
eﬀorts as a contribution to this endeavour.
It is through these connections that we hope eventually to make contact again with the cognitive
map. As noted above, the state of the art is fragmented, but there exist current models that are
expressed in the language of approximate (variational) inference [65], models expressed in the
language of reinforcement learning [66], and models that attempt to combine the two [67]. We
will see throughout the thesis that reinforcement learning (and its cousin, game theory) is closely
related to approximate inference, and so we expect that the foundations developed here, along
with the extensions proposed in §7.1.3, will help us unify these accounts. The key observation that
we expect to drive such a development is that learning a cognitive map (alternatively, learning
a “world model”) means internalizing a representation of the structure of the environment; and
comparing and translating structures is category theory’s forte.
4

Of course, even if the theory that we develop is suﬃcient to unify these computational-
phenomenological models, this is not to say it will satisfy all neuroscientists, many of which
may be expected to desire more biologically detailed models. In the contemporary undergraduate
neuroscience curriculum, one is taught informally to relate models at a high ‘computational’ level
to lower level models concerned with biological ‘implementation’, following Marr’s “three levels
of explanation” [42]. As we discuss in §2.2.3, this story is a shadow of the categorical notion of
functorial semantics, by which structures are translated precisely between contexts formalized
as categories. Although we concentrate on the more abstract computational level in this thesis,
our discussion of functorial semantics foreshadows the introduction of formal algebraic tools for
building biologically plausible neural circuit models (§3.3).
Our treatment of cognitive and neural systems is not the ﬁrst to adopt categorical methods,
but we do believe that it is the ﬁrst to do so in a comprehensively integrated and wide-ranging
way, taking functorial semantics seriously. Categorical concepts have been variously proposed
in biology as early as 1958 [12], and in cognitive science (with one eye toward the brain) since at
least 1987 [21, 68]; more recently, category theory has been used to study classic cognitive-science
concepts such as systematicity [20]. While inspirational, these studies do not make the most of the
translational power of categories, using only some concepts or methods in isolation. Moreover,
by working almost purely categorically, these works were invariably rather abstract, and did not
make direct contact with the tools and concepts of mainstream mathematical science. As a result,
they did not have the unifying impact or adoption that we hope the new wave of applied category
theoretical developments to have.
Our primary motivation in writing this thesis is to lay the groundwork for well-typed cognitive
science and computational neuroscience. ‘Types’ are what render categorical concepts so precise,
and what allow categorical models to be so cleanly compositional: two systems can only “plug
together” if their interface types match. Because every concept in category theory has a type (i.e.,
every object is an object of some category), categorical thinking is forced to be very clear. As we
will sketch in §2.3.4, the “type theories” (or “internal languages”) of categories can be very richly
structured, but still the requirement to express concepts with types is necessarily burdensome. But
this burden is only the burden of thinking clearly: if one is not able to supply a detailed type, one
can resort to abstraction. And, to avoid the violence of declaring some object to be identiﬁed as of
some type7, it is necessary to understand the relationships between types; fortunately, as we will
soon make clear, and as we have attempted to emphasize, category theory is fundamentally the
mathematics of relationship.
Contemporary science is unavoidably computational, and the notion of ‘type’ that we invoke
here is closely related to (though not identical with) the informal notion of type that is used in
7A perspective for which we must thank Brendan Fong.
5

computer programming. Just as one of the strategies adopted to overcome the crises of modern
science that we invoked at the opening of this introduction is the making available of the code and
data that underlie scientiﬁc studies, we can envisage a near future in which accompanying these is
a formal speciﬁcation of the types of the concepts that each study is about8. Some work along these
lines has already begun, particularly with the development of tha Algebraic Julia ecosystem [69].
The free energy framework, like the structurally adjacent framework of compositional game
theory, has a strong ﬂavour of teleology (that follows directly from its mathematics): systems act in
order to make their predictions come true. We therefore hope that, although we do not quite get as
far as a full compositional theory of active inference, the contributions reported in this dissertation
may in some small way help to make this particular prediction (of a well-typed science) come
true, and thereby help to overcome some of the aforenoted crises of scientiﬁc faith—as well as to
shed light not only on the form and function of ‘Bayesian’ brains, but also other complex adaptive
systems, such as the whole scientiﬁc community itself.
1.1. Overview of the dissertation
Category theory being quite alien to most researchers in computational neuroscience (and the
cognitive sciences more broadly), we begin the work of this dissertation in Chapter 2 with
a comprehensive review of the concepts and results needed to understand our mathematical
contributions. Using three hopefully familiar examples, we introduce categories as contrapuntal
to graphs, which are more familiar to scientists, but which lack important features of categories
such as composition and (somehow) dynamism. We then explain how enriched categories allow us
to “connect the connections” of categories, and attach extra data to them, and we exemplify these
concepts with the 2-category of categories, functors, and natural transformations—as well as a more
formal discussion of functorial ‘translation’ and semantics. The remainder of Chapter 2 is dedicated
to introducing the remaining key concepts of basic category theory: universal constructions, and
the Yoneda Lemma (categories’ fundamental theorem).
In Chapter 3, we begin to reapproach neural modelling, and more generally the ‘algebraic’
modelling of the structure of interacting systems. We explain how ‘monoidal’ categories allow us to
consider processes “in parallel” (as well as just sequentially), and how this gives us a formal account
of the concept of ‘parameterized’ system. We then change the perspective a little, and introduce
our ﬁrst piece of original work: an account of how to connect neural circuits into larger-scale
systems, using ‘multicategorical’ algebra. The remainder of the chapter is dedicated to developing
8One might think of this speciﬁcation as akin to a scientiﬁcally elaborated version of the notion of header ﬁle in
programming languages such as C or C++: these ﬁles specify the types of functions and data structures, typically
without instantiating these types with detailed implementations. We can thus think of category theory as a very rich
metaprogramming language for the mathematical sciences (and this analogy goes quite far, as categorical proofs are
typically ‘constructive’ and hence correspond to computable functions, as we also sketch in §2.3.4).
6

the theory of such algebra to the point needed later in the thesis, ending with the introduction
of polynomial functors which will supply a rich syntax for the interaction of systems, as well as a
language in which to express their dynamical semantics.
Chapter 4 presents our ﬁrst main result, that Bayesian updating composes according to the
categorical ‘lens’ pattern. This result is abstractly stated, and so applies to whichever compositional
model of probability one might be interested in—but because we are later interested in concrete
models, we spend much of the chapter developing compositional probability theory using the tools
introduced in Chapters 2 and 3 and instantiating it in discrete and continuous settings. We also
introduce and contextualize the lens pattern, in order to deﬁne the notion of Bayesian lens, which
provides a mathematical formalization of the bidirectionality of predictive coding circuits.
Our main aim in this thesis is to formalize predictive coding through functorial semantics, and
Bayesian lenses will provide an important part of the ‘syntax’ of statistical models that we need. In
Chapter 5, we introduce the corresponding semantical categories, with a new abstract formalization
of the concept of open dynamical system. We make much use here of the language of polynomial
functors: they will represent both the interfaces of interacting systems, with the dynamical systems
themselves being deﬁned as particular classes of morphisms of polynomials. We then synthesize
these developments with the algebraic structures of Chapter 3, to deﬁne monoidal bicategories of
cybernetic systems.
Chapter 6 presents our functorial formalization of predictive coding, using a new notion of
approximate inference doctrine, by which statistical models are translated into dynamical systems.
This formalizes the process by which research in active inference turns the abstract speciﬁcation of
a “generative model” (Deﬁnition 6.2.12) into a dynamical system that can be simulated and whose
behaviours can then be compared with experimentally observed data. This formalization proceeds
via a classiﬁcation of various statistical problems into statistical games.
Finally, Chapter 7 reviews the prospects for future work, from the mathematics of the cognitive
map (a programme that we call compositional cognitive cartography), to the composition of multi-
agent systems and ecosystems and the connections with compositional game theory (and other
categorical approaches to cybernetics). We close with some speculation on a new mathematics of
life, along with associated developments of fundamental theory.
1.2. Contributions
The main individual contribution of this thesis is the formalization of models of predictive coding
circuits as functorial semantics, and the associated development and exempliﬁcation of categories of
statistical games, as well as the introduction of Bayesian lenses and the proof that Bayesian updates
compose optically. We believe our presentation of general open dynamical systems as certain
7

polynomial coalgebras also to be novel, along with the concept of cilia and their associated monoidal
bicategories. The categories of statistical games (and of Bayesian lenses) supply the syntax, and
the monoidal bicategories of cilia the semantics, for our functorial treatment of predictive coding,
and hence the basis for our compositional active inference framework. Each of these structures is
to our knowledge new, although of course inspired by much work that has gone before, and by
interactions with the beneﬁcent community of researchers of which this author ﬁnds himself a
member.
Each of these strands of work has in some way been exhibited through publication, principally as
refereed presentations at the conference on Applied Category Theory (ACT) in 2020 [70], 2021 [71],
and 2022 [72]; but also in preliminary form at NeurIPS 2019 [73], through a number of more informal
invited talks (e.g. [74]), as one main theme of a full-day workshop at the 2022 Cognitive Science
Society conference [75], and our ongoing series of preprints on compositional active inference [76,
77]. We are presently preparing for journal publication an account of our compositional framework
for predictive coding aimed explicitly at computational neuroscientists.
Besides these speciﬁc novel contributions, we hope that this dissertation contributes to a
renaissance of cognitive and computational (neuro)science through the adoption of categorical
methods; it is for this reason that we have been so diligent in our exposition of the basic theory.
We hope that this exposition proves itself a useful contribution for interested researchers, and that
its cognitive-neuroscientiﬁc framing is suﬃciently novel to be interesting.
Some work performed during the author’s DPhil studies is not included in this dissertation.
In particular, there has unfortunately not been the scope to include our simulation results on
a fragment of the circuitry underlying the cognitive map—a study on the development of place
and head-direction cells, which was published as [78]—although this did motivate our algebra
of rate-coded neural circuits (§3.3), which is to the best of our knowledge novel (though much
inspired by earlier work on wiring-diagram algebras [79, 80]). We have also not exhibited our work
on Bayesian optics (as an alternative to Bayesian lenses) [81], as this would require a digression
through some unnecessarily complicated theory; and we have not presented in detail the examples
of “polynomial life” presented at ACT 2021 [71].
8

2. Basic category theory for computational
and cognitive (neuro)scientists
This chapter constitutes a comprehensive review of the concepts and results from basic category
theory that scaﬀold the rest of the thesis, written for the computational neuroscientist or cognitive
scientist who has noticed the ‘network’ structure of complex systems like the brain and who wonders
how this structure relates to the systems’ function. Category theory gives us a mathematical
framework in which precise answers to such questions can be formulated, and reveals the
interconnectedness of scientiﬁc ideas. After introducing the notions of category and diagram
(§2.1), we swiftly introduce the notions of enriched category, functor, and adjunction (§2.2), with
which we can translate and compare mathematical concepts. We then explain how category theory
formalizes pattern as well as translation, using the concept of universal construction (§2.3), which
we exemplify with many common and important patterns. Finally, we introduce the fundamental
theorem of category theory, the Yoneda Lemma, which tells us that to understand a thing is to see
it from all perspectives (§2.4).
Category theory is well established in the foundations of mathematics, but not yet explicitly in
the foundations of science. As a result, although the only slightly original part of this chapter is its
presentation, we have given proofs of most results and plentiful examples, in order to familiarize
the reader with thinking categorically.
2.1. Categories, graphs, and networks
We begin by motivating the use of category theory by considering what is missing from a purely
graph-theoretic understanding of complex computational systems. Later in the thesis, we will see
how each of the diagrams depicted below can be formalized categorically, incorporating all the
scientiﬁcally salient information into coherent mathematical objects.
9

2.1.1. Three examples
2.1.1.1. Neural circuits: dynamical networks of neurons
In computational and theoretical neuroscience, it is not unusual to encounter diagrams depicting
proposed architectures for neural circuits, such as on the left or right below:
E
I
On the left, we have depicted a standard “excitatory-inhibitory circuit” motif, in which one neuron
or ensemble of neurons E receives input from an external source as well as from a counterposed
inhibitory circuit I which itself is driven solely by E. On the right, we have reproduced a ﬁgure
depicting a “predictive coding” circuit from Bogacz [82], and we see that the E-I circuit is indeed
motivic, being recapitulated twice: we could say that the predictive coding circuit is composed
from interconnected E-I motifs, in a sense similarly to the composition of the E-I circuit from the
subnetworks E and I of neurons.
Both circuits have evident graphical structure — the nodes are the white circles, and the edges
the black wires between them — but of course there is more to neural circuits than these graphs:
not only do graphs so deﬁned omit the decorations on the wires (indicating whether a connection
is excitatory or inhibitory), but they miss perhaps the more important detail, that these are circuits
of dynamical systems, which have their own rich structure and behaviours. Moreover, mere graphs
miss the aforementioned compositionality of neural circuits: we can ﬁll in the white circles with
neurons or ensembles or other circuits and we can wire circuits together, and at the end of doing
so we have another ‘composite’ neural circuit.
Working only with graphs means we have to treat the decorations, the dynamics, and the
compositionality informally, or at least in some other data structure, thereby increasing the overhead
of this accounting.
2.1.1.2. Bayesian networks: belief and dependence
In computational statistics, one often begins by constructing a model of the causal dependence
between events, which can then be interrogated for the purposes of inference or belief-updating.
Such models are typically graphical, with representations as shown below; the nodes are again the
circles, and the dashed edge implies the repetition of the depicted motif:
10

On the left, the graph represents a model of an event with two possible antecedents; on the right, a
set of events (or an event, repeated) with a recurrent cause. Although these graphical models —
otherwise known as Bayesian networks — may encode useful information about causal structure,
in themselves they do not encode the information about how events are caused; this is data that
must be accounted for separately. And once again, mere graphs are unlike causality in that they
are non-compositional: the structure does not explain how, given the causal dependence of B on A
and A1 and of C on B, one might model the dependence of C on A.
2.1.1.3. Computations: sets and functions
In a similar way, pure computations — in the sense of transformations between sets of data — are
often depicted graphically:
Wf
Wo
Wi
Wc
Uf
Uo
Ui
Ui
`
`
`
`
d
d
`
d
σ
σ
σ
σ
ct´1
xt
ht´1
ct
ht
Here, we have depicted a single ‘cell’ from a long short-term memory network [83]: a function
that ingests three variables (ct´1, an internal state; xt, an external input; and ht´1, an internal
‘memory’), and emits two (ct, a new internal state; and ht, an updated memory). This function is
itself composed from other functions, depicted above as boxes. (One typically takes the variables
ct, xt, ht as vectors of given dimension for all t, so that the domain and codomain of the function
are products of vector spaces; the boxes Wi and Ui represent matrices which act on these vectors;
the boxes ` and d denote elementwise sum and product; the box σ represents the elementwise
application of a logisitic function; and the splitting of wires represents the copying of data.) The
nodes of the graph in this instance are the functions (boxes), and the edges encode the ﬂow of
information. Once more, however, a purely graphical model does not account for the compositional
structure of the computation: we could ﬁll in the boxes with other graphs (representing elaborations
of the computations implied), and we could adjoin another such diagram beside and connect the
wires where the types match. To account for this compositionality — here and in the examples
above — we will need to add something to the structure: we need to move from graphs to categories.
11

2.1.2. From graphs to categories
A category is a directed graph in which edges can be composed: whenever the target of an edge f
is the source of another edge g, then there must be a composite edge denoted g ˝ f whose source is
the source of f and whose target is the target of g, as in the following diagram.
‚
‚
‚
f
g
g˝f
This composition rule incorporates into the structure a way to allow systems with compatible
interfaces to connect to each other, and for the resulting composite system also to be a system
of the same ‘type’; but as we will see, it has some other important consequences. Firstly, every
(‘small’) category has an underlying directed graph: but because of the composition rule, this
underlying graph typically has more edges than the graphs of the examples above, in order to
account for the existence of composites. Secondly, it is the edges, which in a categorical context we
will call morphisms, that compose: the nodes, which we will call objects, represent something like
the ‘interfaces’ at which composition is possible. This means that we cannot just interpret a circuit
diagram “as a category”, whose objects are ensembles of neurons and whose morphisms are their
axons: as we will see in §3.3, we need to do something a bit more sophisticated.
Before we get to that, however, we must ﬁrst deﬁne categories precisely. We will take a graphical
approach, with a view to interpreting the above examples categorically, starting with the diagram
demonstrating the composition of g ˝ f: how should we interpret this in a category? To answer
this question, we ﬁrst need to specify exactly what we mean by ‘graph’.
Deﬁnition 2.1.1. A directed graph G is a set G0 of nodes along with a set Gpa, bq of edges from
a to b for each pair a, b : G0 of nodes. We will sometimes write G1 to denote the disjoint union
of the sets of edges, G1 :“ ř
a,b Gpa, bq. If e : Gpa, bq is an edge from a to b, we will write this as
e : a Ñ b and call a its source or domain and b its target or codomain. This assignment of domain
and codomain induces a pair of functions, dom, cod : G1 Ñ G0 respectively, such that for e : a Ñ b
we have dompeq “ a and codpeq “ b.
A category is a graph whose edges can be ‘associatively’ composed together, and where every
node has a special edge from itself to itself called its ‘identity’.
Deﬁnition 2.1.2. A (small) category C is a directed graph whose nodes are each assigned a
corresponding identity edge and whose edges are equipped with a composition operation ˝ that is
12

associative and unital with respect to the identities. In the context of categories, we call the nodes
C0 the objects or 0-cells, and the edges C1 the morphisms or 1-cells.
Identities are assigned by a function id : C0 Ñ C1 satisfying dompidaq “ a “ codpidaq for every
object a. The composition operation is a family of functions ˝a,b,c : Cpb, cq ˆ Cpa, bq Ñ Cpa, cq for
each triple of objects a, b, c. The notation Cpa, bq indicates the set of all morphisms a Ñ b, for each
pair of objects a and b; we call this set the hom set from a to b.
Given morphisms f : a Ñ b and g : b Ñ c, their composite a
fÝÑ b
gÝÑ c is written g ˝ f, which
we can read as “g after f”.
Associativity means that h ˝ pg ˝ fq “ ph ˝ gq ˝ f, and so we can omit the parentheses to
write h ˝ g ˝ f without ambiguity. Unitality means that, for every morphism f : a Ñ b, we have
idb ˝f “ f “ f ˝ ida.
Remark 2.1.3. We say small category to mean that both the collection of objects C0 and the
collection of morphisms C1 is a true set, rather than a proper class. We will say a category is locally
small if, for every pair a, b of objects in C, the hom set Cpa, bq is a set (rather than a proper class);
this allows for the collection of objects still to be a proper class, while letting us avoid “size issues”
such as Russell’s paradox in the course of normal reasoning.
More precisely, we can ﬁx a ‘universe’ of sets, of size at most a given cardinality ℵi. Then we
say that a category is locally small with respect to ℵi if every hom set is within this universe, or
small if both C0 and C1 are. We say that a category is large if it is not small, but note that the ‘set’
of objects or morphisms of a large category may still be a ‘set’, just in a larger universe: a universe
whose sets are of cardinality at most ℵi`1 ą ℵi.
In the remainder of this thesis, we will typically assume categories to be locally small with
respect to a given (but unspeciﬁed) universe.
Our ﬁrst example of a category is in some sense the foundation of basic category theory, and
supplies a classic illustration of size issues.
Example 2.1.4. The category Set has sets as objects and functions as morphisms. The identity on
a set A is the identity function idA : A Ñ A : a ÞÑ a. Composition of morphisms in Set is function
composition: given f : A Ñ B and g : B Ñ C, their composite is the function g ˝ f : A Ñ C
deﬁned for each a : A by pg ˝ fqpaq “ gpfpaqq; it is easy to check that function composition is
associative.
Note that Set is a large category: the set Set0 of all sets of at most size ℵi must live in a larger
universe.
Not all categories are large, of course. Some are quite small, as the following examples
demonstrate.
13

Example 2.1.5. There is a category with only two objects 0 and 1 and four morphisms: the
identities id0 : 0 Ñ 0 and id1 : 1 Ñ 1, and two non-identity morphisms s, t : 0 Ñ 1, as in the
following diagram:
0
1
s
t
When depicting categories graphically, we often omit identity morphisms as they are implied by
the objects.
Example 2.1.6. There is a category, denoted 1, with a single object ˚ and a single morphism, its
identity.
Example 2.1.7. The natural numbers N form the morphisms of another category with a single
object ˚: here, composition is addition and the identity morphism id˚ : ˚ Ñ ˚ is the number 0.
Since addition is associative and unital with respect to 0, this is a well-deﬁned category.
Since a category is a directed graph equipped with a composition operation, we can ‘forget’ the
latter to recover the underlying graph on its own.
Proposition 2.1.8. Given a category C, we can obtain a directed graph pC0, C1q by keeping the
objects C0 and morphisms C1 and forgetting the composition and identity functions.
Proof. Take the objects to be the nodes and the morphisms to be the edges.
However, in the absence of other data, obtaining a category from a given graph is a little
more laborious, as we must ensure the existence of well-deﬁned composite edges. The following
proposition tells us how we can do this.
Proposition 2.1.9. Given a directed graph G, we can construct the free category generated by G,
denoted FG, as follows. The objects of FG are the nodes G0 of G. The morphisms FGpa, bq from
a to b are the paths in G from a to b: ﬁnite lists pe, f, gq of edges in which the domain of the ﬁrst
edge is a, the codomain of any edge equals the domain of its successor (if any), and the codomain
of the last edge is b. Composition is by concatenation of lists, and the identity morphism for any
node is the empty list pq.
Proof. Let f :“ pf1, . . . , flq : a Ñ b, g :“ pg1, . . . , gmq : b Ñ c, and h :“ ph1, . . . , hnq : c Ñ d be
paths. Then
h ˝ pg ˝ fq “ ph1, . . . , hnq ˝ pf1, . . . , fl, g1, . . . , gmq
“ pf1, . . . , fl, g1, . . . , gm, h1, . . . , hnq
“ pg1, . . . , gm, h1, . . . , hnq ˝ pf1, . . . , flq “ ph ˝ gq ˝ f
14

so concatenation of lists is associative. Concatenation is trivially unital on both right and left:
pq ˝ pf1, . . . , flq “ pf1, . . . , flq “ pf1, . . . , flq ˝ pq. So the free category as deﬁned is a well-deﬁned
category.
Remark 2.1.10. Observe that the underlying graph of FG is not in general the same as the original
graph G: because the edges of G have no composition information (even if, given a pair of edges
a Ñ b and b Ñ c, there is an edge a Ñ c), we needed a canonical method to generate such
information, without any extra data. Since there is a notion of path in any graph, and since paths
are naturally composable, this gives us the canonical method we seek.
We begin to see some important diﬀerences between categories and graphs, as foreshadowed
above. Categories are somehow more ‘dynamical’ objects, more concerned with movement and
change than graphs; later in Chapter 5, we will even see how a general deﬁnition of dynamical
system emerges simply from some of the examples we have already seen.
At this point, to emphasize that categories allow us to study not just individual structures
themselves but also the relationships and transformations between structures, we note that directed
graphs themselves form a category.
Example 2.1.11. Directed graphs pG0, G1, domG, codGq are the objects of a category, denoted
Graph. Given directed graphs G :“ pG0, G1, domG, codGq and H :“ pH0, H1, domH, codHq, a
morphism f : G Ñ H is a graph homomorphism from G to H: a pair of functions f0 : G0 Ñ G0
and f1 : G1 Ñ H1 that preserve the graphical structure in the sense that for every edge e in G,
f0pdomGpeqq “ domHpf1peqq and f0pcodGpeqq “ codHpf1peqq. Since graph homomorphisms are
pairs of functions, they compose as functions, and the identity morphism on a graph G is the pair
pidG0, idG1q of identity functions on its sets of nodes and edges.
In large part, the power of category theory derives from its elevation of relationship and
transformation to mathematical prominence: objects are represented and studied in context, and
one we gain the ability to compare patterns of relationships across contexts. By expressing these
patterns categorically, we are able to abstract away irrelevant detail, and focus on the fundamental
structures that drive phenomena of interest; and since these patterns and abstract structures
are again expressed in the same language, we can continue to apply these techniques, to study
phenomena from diverse perspectives. Indeed, as we will soon see, category theory is ‘homoiconic’,
able to speak in its language about itself.
Accordingly, it is often helpful to apply graphical or diagrammatic methods to reason about
categories: for example, to say that two (or more) morphisms are actually equal. We can illustrate
this using the category Graph: the deﬁnition of graph homomorphism requires two equalities to
be satisﬁed. These equalities say that two (composite) pairs of functions are equal; since functions
15

are morphisms in Set, this is the same as saying that they are equal as morphisms there. Using the
fact that Set has an underlying graph, we can represent these morphisms graphically, as in the
following two diagrams:
G1
H1
G0
H0
f1
f0
domG
domH
G1
H1
G0
H0
f1
f0
codG
codH
(2.1)
Then to say that f0 ˝ domG “ domH ˝f1 and f0 ˝ codG “ codH ˝f1 is to say that these diagrams
commute.
Deﬁnition 2.1.12. We say that two paths in a graph are parallel if they have the same start and
end nodes. We say that a diagram in a category C commutes when every pair of parallel paths in
the diagram corresponds to a pair of morphisms in C that are equal.
To clarify this deﬁnition, we can use category theory to formalize the concept of diagram, which
will have the useful side-eﬀect of simultaneously rendering it more general and more precise.
2.1.2.1. Diagrams in a category, functorially
The richness of categorical structure is reﬂected in the variety of diagrammatic practice, and in this
thesis we will encounter a number of formal diagram types. Nonetheless, there is one type that is
perhaps more basic than the rest, which we have already begun to call diagrams in a category: these
are the categorical analogue of equations in algebra. Often in category theory, we will be interested
in the relationships between more than two morphisms at once, and expressing such relationships
by equations quickly becomes cumbersome; instead, one typically starts with a directed graph and
interprets its nodes as objects and its edges as morphisms in one’s category of interest.
Formally, this interpretation is performed by taking the category generated by the graph and
mapping it ‘functorially’ into the category of interest. However, in order to account for relationships
such as equality between the morphisms represented in the graph, the domain of this mapping
cannot be as ‘free’ as in Proposition 2.1.9, as it needs to encode these relationships. To do this, we
can quotient the free category by the given relationships, as we now show.
Proposition 2.1.13 (Mac Lane [84, Prop. II.8.1]). Let G be a directed graph, and suppose we are
given a relation „a,b on each set FGpa, bq of paths a Ñ b; write „ for the whole family of relations,
and call it a relation on the category C. Then there is a category FG{„, the quotient of the free
category FG by „, which we call the category generated by G with relations „ or simply generated
by pG, „q.
16

The objects of FG{„ are again the nodes G0. The morphisms are equivalence classes of paths
according to „: suppose p „a,b p1; then they both belong to the same equivalence class rps, and
correspond to the same morphism rps : a Ñ b in FG{„.
Before we can prove this proposition, and thus establish that composition in FG{„ does what
we hope, we need the concept of a congruence.
Deﬁnition 2.1.14. Suppose „ is a relation on the category C. We call „ a congruence when its
constituent relations „a,b are equivalence relations compatible with the compositional structure of
C. This means that
1. if any parallel morphisms f, f1 : a Ñ b are related by „a,b, i.e. f „a,b f1, then, for all
g : a1 Ñ a and h : b Ñ b1, we must have h ˝ f ˝ g „a1,b1 h ˝ f1 ˝ g; and
2. for each pair of objects a, b : C, „a,b is a symmetric, reﬂexive, transitive relation.
The notion of congruence is what allows us to extend the family of relations „ to composites of
morphism and thus ensure that it is compatible with the categorical structure; constructing the
most parsimonious congruence from „ is the key to the following proof.
Proof of Proposition 2.1.13. First of all, we need to extend „ to a congruence; we choose the smallest
congruence containing „, and denote it by –. Concretely, we deﬁne – as the symmetric, reﬂexive,
transitive closure of an intermediate relation, denoted ». We deﬁne » such that if φ »a,d φ1 for
parallel morphisms φ, φ1 : a Ñ d, then there exist objects b, c : FG as well as morphisms f : a Ñ b,
and g, g1 : b Ñ c, and h : c Ñ d, such that φ “ h ˝ g ˝ f and φ1 “ h ˝ g1 ˝ f and g „b,c g1. We
can write » compositionally as follows. Suppose f : a Ñ b and h : c Ñ d are morphisms, and
write FGf,h : FGpb, cq Ñ FGpa, dq for the function that takes g : b Ñ c and acts by composition
to return h ˝ g ˝ f. Note that each of the relations „b,c can be written as a subset of the product
set FGpb, cq ˆ FGpb, cq, and write FGf,h2 : FGpb, cq ˆ FGpb, cq Ñ FGpa, dq ˆ FGpa, dq for the
product of FGf,h with itself (i.e., the function that applies FGf,h to each argument). We then deﬁne
»a,d as the following union of sets
ď
b,c:FG
ď
f:aÑb,
h:cÑd
FGf,h2
˚ „b,c
where FGf,h2
˚ „b,c
denotes the image of
„b,c
under FGf,h2. Finally, we deﬁne – as the
symmetric, reﬂexive, transitive closure of ». Explicitly, if φ, φ are both morphisms a Ñ d, then
φ –a,d φ1 whenever φ »a,d φ1, or φ1 »a,d φ (symmetry), or φ “ φ1 (reﬂexivity), or there exists some
γ : a Ñ d such that φ »a,d γ and γ »a,d φ1 (transitivity). To see that – is the least congruence on
FG, observe that every congruence must contain it by deﬁnition.
17

Having constructed the congruence –, we can form the quotient of FG by it, which we denote
by FG{„ in reference to the generating relation „. As in the statement of the proposition, the
objects of FG{„ are the nodes of G and the morphisms are equivalence classes of paths, according
to –; since – is by deﬁnition an equivalence relation, these equivalence classes are well-deﬁned.
Moreover, the composite of two equivalence classes of morphisms rfs : a Ñ b and rgs : b Ñ c
coincides with the equivalence class rg ˝ fs.
Example 2.1.15. To exemplify the notion of category generated with relations, let J denote the
following directed graph
G1
H1
G0
H0
ϕ1
ϕ0
δG
δH
and let „ be the relation ϕ0 ˝ δG „ δH ˝ ϕ1. Then the category FJ {„ generated by pJ , „q has
four objects (G1, G0, H1, H0) and nine morphisms: an identity for each of the four objects; the
morphisms ϕ0 : G0 Ñ H0, ϕ1 : G1 Ñ H1, δG : G1 Ñ G0, and δH : H1 Ñ H0; and a single
morphism G1 Ñ H0, the equivalence class consisting of ϕ0 ˝ δG and δH ˝ ϕ1.
The category FJ {„ generated in this example expresses the commutativity of one of the
diagrams deﬁning graph homomorphisms, but as things stand, it is simply a category standing
alone: to say that any particular pair of functions pf0, f1q satisﬁes the property requires us to
interpret the morphisms ϕ0 and ϕ1 accordingly as those functions. That is, to interpret the diagram,
we need to translate it, by mapping FJ {„ into Set. Such a mapping of categories is known as a
functor.
Deﬁnition 2.1.16. A functor F : C Ñ D from the category C to the category D is a pair of
functions F0 : C0 Ñ D0 and F1 : C1 Ñ D1 between the sets of objects and morphisms that preserve
domains, codomains, identities and composition, meaning that F0pdomCpfqq “ domDpF1pfqq
and F0pcodCpfqq “ codDpF1pfqq for all morphisms f, F1pidaq “ idFpaq for all objects a, and
F1pg ˝ fq “ F1pgq ˝ F1pfq for all composites g ˝ f in C.
Remark 2.1.17. Note that we could equivalently say that a functor C Ñ D is a homomorphism
from the underlying graph of C to that of D that is additionally functorial, meaning that it preserves
identities and composites.
Notation 2.1.18. Although a functor F consists of a pair of functions pF0, F1q, we will typically
write just F whether it is applied to an object or a morphism, since the distinction will usually be
clear from the context. Since function composition (and hence application) is associative, we will
also often omit brackets, writing Fa for Fpaq, except where it is helpful to leave them in.
18

For each object c in a category C, there are two very important functors, the hom functors, which
exhibit C in Set “from the perspective” of c by returning the hom sets out of and into c.
Deﬁnition 2.1.19. Given an object c : C, its covariant hom functor Cpc, ´q : C Ñ Set is deﬁned
on objects x by returning the hom sets Cpc, xq and on morphisms g : x Ñ y by returning the
postcomposition function Cpc, gq : Cpc, xq Ñ Cpc, yq deﬁned by mapping morphisms f : c Ñ x
in the set Cpc, xq to the composites g ˝ f : c Ñ y in Cpc, yq. To emphasize the action of Cpc, gq
by postcomposition, we will sometimes write it simply as g ˝ p´q. (That Cpc, ´q is a well-deﬁned
functor follows immediately from the unitality and associativity of composition in C.)
The covariant hom functor Cpc, ´q “looks forward” along morphisms emanating out of c, in
the direction that these morphisms point, and therefore in the direction of composition in C: it is
for this reason that we say it is covariant. Dually, it is of course possible to “look backward” at
morphisms pointing into c. Since this means looking contrary to the direction of composition in C,
we say that the resulting backwards-looking hom functor is contravariant. To deﬁne it as a functor
in the sense of Deﬁnition 2.1.16, we perform the trick of swapping the direction of composition in
C around and then deﬁning a covariant functor accordingly.
Deﬁnition 2.1.20. For any category C there is a corresponding opposite category C op with the
same objects as C and where the hom set C oppa, bq is deﬁned to be the ‘opposite’ hom set in C,
namely Cpb, aq. Identity morphisms are the same in C op as in C, but composition is also reversed. If
we write ˝ for composition in C and ˝op for composition in C op, then, given morphisms g : c Ñ b
and f : b Ñ a in C op corresponding to morphisms g : b Ñ c and f : a Ñ b in C, their composite
f ˝op g : c Ñ a in C op is the morphism g ˝ f : a Ñ c in C. (Observe that this makes C op a
well-deﬁned category whenever C is.)
Remark 2.1.21. Because we can always form opposite categories in this way, categorical
constructions often come in two forms: one in C, and a ‘dual’ one in C op. Typically, we use
the preﬁx co- to indicate such a dual construction: so if we have a construction in C, then its dual
in C op would be called a coconstruction.
The dual of the covariant hom functor Cpc, ´q : C Ñ Set is the contravariant hom functor.
Deﬁnition 2.1.22. Given an object c : C, its contravariant hom functor Cp´, cq : C op Ñ Set is
deﬁned on objects x by returning the hom sets Cpx, cq. Given a morphism f : x Ñ y in C, we
deﬁne the precomposition function Cpf, cq : Cpy, cq Ñ Cpx, cq by mapping morphisms g : y Ñ c
in the set Cpy, cq to the composites g ˝ f : x Ñ c in Cpx, cq. To emphasize the action of Cpf, cq
by precomposition, we will sometimes write it simply as p´q ˝ f. (That Cp´, cq is a well-deﬁned
functor again follows from the unitality and associativity of composition in C and hence in C op.)
19

Remark 2.1.23. A contravariant functor on C is a (covariant) functor on C op.
Notation 2.1.24. In line with other mathematical literature, we will also occasionally write the
precomposition function p´q ˝ f as f˚; dually, we can write the postcomposition function g ˝ p´q
as g˚. In these forms, the former action f˚ is also known as pullback along f, as it “pulls back”
morphisms along f, and the latter action g˚ is also known as pushforward along g, as it “pushes
forward” morphisms along g. There is a close relationship between the pulling-back described here
and the universal construction also known as pullback (Example 2.3.43): f˚p´q deﬁnes a functor
which acts by the universal construction on objects and by precomposition on morphisms, which
we spell out in Deﬁnition 4.2.24.
Functors are the homomorphisms of categories, and just as graphs and their homomorphisms
form a category, so do categories and functors.
Example 2.1.25. The category Cat has categories for objects and functors for morphisms. The
identity functor idC on a category C is the pair pidC0, idC1q of identity functions on the sets of
objects and morphisms. Since functors are pairs of functions, functor composition is by function
composition, which is immediately associative and unital with respect to the identity functors so
deﬁned. Note that, without a restriction on size, Cat is a large category, like Set.
As an example, we observe that the construction of the category FG{„ generated by pG, „q
from the free category FG is functorial.
Example 2.1.26. There is a ‘projection’ functor r¨s : FG Ñ FG{„. It maps every object to
itself, and every morphism to the corresponding equivalence class. The proof of Proposition 2.1.13
demonstrated the functoriality: identities are preserved by deﬁnition, and we have rg˝fs “ rgs˝rfs
by construction.
With the notion of functor to hand, we can formalize the concept of diagram simply as follows.
Deﬁnition 2.1.27. A J-shaped diagram in a category C is a functor D : J Ñ C. Typically, J is a
small category generated from a graph with some given relations, and the functor D interprets J
in C.
Example 2.1.28. The diagrams expressing the commutativity conditions for a graph homomor-
phism (2.1) are therefore witnessed by a pair of functors FJ {„ Ñ Set from the category FJ {„
generated in Example 2.1.15 into Set: each functor interprets ϕ0 and ϕ1 as f0 and f1 respectively,
while one functor interprets δG as domG and δH as domH and the other interprets δG as codG and
δH as codH. The fact that there is only a single morphism G1 Ñ H0 in FJ {„ (even though there
are two in FJ ) encodes the requirements that f0 ˝ domG “ domH ˝f1 and f0 ˝ codG “ codH ˝f1.
20

Throughout this thesis, we will see the utility of diagrams as in Deﬁnition 2.1.27: not only will
they be useful in reasoning explicitly about categorical constructions, but in §2.3.3 they will also be
used to formalize ‘universal constructions’, another concept which exhibits the power of category
theory.
Despite this, ‘mere’ categories and their diagrams are in some ways not expressive enough:
often we will want to encode looser relationships than strict equality, or to compose diagrams
together by ‘pasting’ them along common edges; we may even want to consider morphisms between
morphisms! For this we will need to ‘enrich’ our notion of category accordingly.
2.2. Connecting the connections
As we have indicated, basic category theory is not suﬃcient if we want to encode information
about the relationships between morphisms into the formal structure. In this section, we will see
how to enrich the notion of category by letting the morphisms collect into more than just sets,
and how this leads naturally to higher category theory, where we have morphisms between the
morphisms, and from there to the notion of adjunction, with which we can translate concepts
faithfully back and forth between contexts. Amidst the development, we discuss the concept of
“functorial semantics” from a scientiﬁc perspective, considering how categorical tools let us supply
rich semantics for structured models of complex systems such as the brain.
2.2.1. Enriched categories
We can think of the condition that a diagram commutes — or equivalently the speciﬁcation of an
equivalence relation on its paths — as a ‘ﬁlling-in’ of the diagram with some extra data. For example,
we can ‘ﬁll’ the diagram depicting the graph homomorphism condition f0 ˝ domG “ domH ˝f1
with some annotation or data witnessing this relation, as follows:
G1
H1
G0
H0
f1
f0
domG
domH
If we have a composite graph homomorphism g ˝ f : G Ñ I, we should be able to paste the
commuting diagrams of the factors together and ﬁll them in accordingly:
G1
H1
I1
G0
H0
I0
domG
domH
domI
f1
g1
f0
g0
21

and we should be able to ‘compose’ the ﬁller equalities to obtain the diagram for the composite:
G1
H1
I1
G0
H0
I0
domG
domI
f1
g1
f0
g0
.
The extra data with which we have ﬁlled these diagrams sits ‘between’ the morphisms, and so
if we wish to incorporate it into the categorical structure, we must move beyond mere sets, for
sets are just collections of elements, with nothing “in between”. What we will do is allow the hom
sets of a category to be no longer sets, but objects of another ‘enriching’ category. Now, observe
that, in pasting the two diagrams above together, we had to place them side by side: this means
that any suitable enriching category must come equipped with an operation that allows us to place
its objects side by side; in the basic case, where our categories just have hom sets, the enriching
category is Set, and this side-by-side operation is the product of sets.
Deﬁnition 2.2.1. Given sets A and B, their product is the set A ˆ B whose elements are pairs
pa, bq of an element a : A with an element b : B.
We have already made use of the product of sets above, when we deﬁned the composition
operation for (small) categories in Deﬁnition 2.1.2. In general, however, we don’t need precisely a
product; only something weaker, which we call tensor.
Deﬁnition 2.2.2. We will say that a category C has a tensor product if it is equipped with a functor
b : C ˆ C Ñ C along with an object I : C called the tensor unit and three families of isomorphisms:
1. associator isomorphisms αa,b,c : pa b bq b c „
ÝÑ a b pb b cq for each triple of objects a, b, c ;
2. left unitor isomorphisms λa : I b a „
ÝÑ a for each object a; and
3. right unitor isomorphisms ρa : a b I
„
ÝÑ a for each object a.
Remark 2.2.3. The notion of tensor product forms part of the deﬁnition of monoidal category,
which we will come to in §3.1.2.
Unsurprisingly, the product of sets gives us our ﬁrst example of a tensor product structure.
Example 2.2.4. The product of sets gives us a tensor product ˆ : Set ˆ Set Ñ Set. To see
that it is functorial, observe that, given a product of sets A ˆ B and a function f : A Ñ A1, we
naturally obtain a function f ˆ B : A ˆ B Ñ A ˆ A1 by applying f only to the A-components of
the elements of the product A ˆ B; likewise given a function g : B Ñ B1. The unit of the tensor
product structure is the set 1 with a single element ˚. The associator and unitors are almost trivial:
for associativity, map ppa, bq, cq to pa, pb, cqq.
22

Using the tensor product to put morphisms side by side, we can deﬁne the notion of enriched
category.
Deﬁnition 2.2.5. Suppose pE, b, I, α, λ, ρq is a category equipped with a tensor product. An
E-category C, or category C enriched in E, constitutes
1. a set C0 of objects;
2. for each pair pa, bq of C-objects, an E-object Cpa, bq of morphisms from a to b;
3. for each object a in C, an E-morphism ida : I Ñ Cpa, aq witnessing identity; and
4. for each triple pa, b, cq of C-objects, an E-morphism ˝a,b,c : Cpb, cq b Cpa, bq Ñ Cpa, cq
witnessing composition;
such that composition is unital, i.e. for all a, b : C
Cpa, bq b I
Cpa, bq b Cpa, aq
Cpa, bq
ρCpa,bq
Cpa,bqbida
˝a,a,b
and
Cpa, bq b Cpa, aq
I b Cpa, bq
Cpa, bq
λCpa,bq
idbbCpa,bq
˝a,b,b
,
and associative, i.e. for all a, b, c, d : C
`
Cpc, dq b Cpb, cq
˘
b Cpa, bq
Cpc, dq b
`
Cpb, cq b Cpa, bq
˘
Cpb, dq b Cpa, bq
Cpc, dq b Cpa, cq
Cpa, dq
αa,b,c,d
˝b,c,dbCpa,bq
Cpc,dqb˝a,b,c
˝a,b,d
˝a,c,d
.
Our ﬁrst example of enriched categories validates the deﬁnition.
Example 2.2.6. A locally small category is a category enriched in pSet, ˆ, 1q.
Remark 2.2.7. In Set, morphisms 1 Ñ A out of the unit set 1 correspond to elements of A: each
such morphism is a function mapping the unique element ˚ : 1 to its corresponding element of A.
This is why identities in enriched category theory are given by morphisms I Ñ Cpa, aq, and it is
also why we will call morphisms out of a tensor unit generalized elements. (Even more generally,
we might say that morphisms X Ñ A are generalized elements of shape X, reﬂecting our use of
the word ‘shape’ to describe the domain of a diagram.)
23

To incorporate nontrivial ﬁllers into our diagrams, we move instead to enrichment in partially
ordered sets.
Example 2.2.8. A preordered set or proset is a category where there is at most one morphism
between any two objects. The objects of such a ‘thin’ category are the points of the proset, and the
morphisms encode the (partial) ordering of the points; as a result, they are often written a ď a1.
Functors between prosets are functions that preserve the ordering, and the restriction of Cat to
prosets produces a category that we denote by Pro. The product of sets extends to prosets as
follows: if A and B are prosets, then their product is the proset A ˆ B whose points are the points
of the product set A ˆ B and a morphism pa, bq ď pa1, b1q whenever there are morphisms a ď a1
and b ď b1 in A and B respectively.
A category enriched in Pos is therefore a category whose hom sets are (pre)ordered and whose
composition operation preserves this ordering, which we can illustrate as follows:
A
B
C
f
g
f1
g1
ď
ď
˝ÞÝÑ
A
C
g˝f
g1˝f1
ď
We can see how enrichment in Pro generalizes the situation with which we introduced this
section, where we considered ﬁlling diagrams with data witnessing the equality of morphisms:
here we have inequality data, and it is not hard to see how enriched composition encompasses the
pasting-and-composing discussed there (just replace the cells here by the squares above).
In order to make these ﬁlled diagrams precise, we need to extend the notion of functor to the
enriched setting; and so we make the following deﬁnition.
Deﬁnition 2.2.9. Suppose C and D are E-categories. Then an E-functor F constitutes
1. a function F0 : C0 Ñ D0 between the sets of objects; and
2. for each pair pa, bq : C0 ˆ C0 of objects in C, an E-morphism Fa,b : Cpa, bq Ñ DpF0a, F0bq
which preserve identities
I
Cpa, aq
DpF0a, F0aq
ida
idF0a
Fa,a
24

and composition
Cpb, cq b Cpa, bq
Cpa, cq
DpF0b, F0cq b DpF0a, F0bq
DpF0a, F0cq
Fb,cbFa,b
˝a,b,c
Fa,c
˝F0a,F0b,F0c
.
A diagram in an E-enriched category C is therefore a choice of E-enriched category J (the
diagram’s shape) and an E-functor J Ñ C. J encodes the objects, morphisms and relationships
of the diagram, and the functor interprets it in C. In this enriched setting, we need not quotient
parallel paths in the shape of a diagram (which destroys their individuality); instead, we have extra
data (the ﬁllers) encoding their relationships.
2.2.2. 2-categories
We have seen that ﬁlling the cells of a diagram with inequalities pushes us to consider enrichment
in Pro. Since Pro is the category of categories with at most one morphism (i.e., the inequality)
between each pair of objects, a natural generalization is to allow a broader choice of ﬁller: that is,
to allow there to be morphisms between morphisms. This means moving from enrichment in Pro
to enrichment in Cat, and hence to the notion of 2-category. We therefore make the following
deﬁnition.
Deﬁnition 2.2.10. A strict 2-category is a category enriched in the 1-category Cat. This means
that, instead of hom sets, a 2-category has hom categories: the objects of these hom categories are
the 1-cells of the 2-category, and the morphisms of the hom categories are the 2-cells; the 0-cells
of the 2-category are its objects. To distinguish between the composition deﬁned by the enriched
category structure from the composition within the hom categories, we will sometimes call the
former horizontal and the latter vertical composition.
Remark 2.2.11. We say 1-category above to refer to the ‘1-dimensional’ notion of category deﬁned
in Deﬁnition 2.1.2.
Remark 2.2.12. We say strict to mean that the associativity and unitality of composition hold up
to equality; later, it will be helpful to weaken this so that associativity and unitality only hold up to
“coherent isomorphism”, meaning that instead of asking the diagrams in Deﬁnition 2.2.5 simply to
commute (and thus be ﬁlled by equalities), we ask for them to be ﬁlled with ‘coherently’ deﬁned
isomorphism. Weakening 2-categorical composition in this way leads to the notion of bicategory
(§3.1.4).
In order to give a well-deﬁned notion of enrichment in Cat, we need to equip it with a suitable
tensor product structure; for this, we can extend the product of sets to categories, as follows.
25

Proposition 2.2.13. Given categories C and D, we can form the product category C ˆ D. Its set
of objects pC ˆ Dq0 is the product set C0 ˆ D0. Similarly, a morphism pc, dq Ñ pc1, d1q is a pair
pf, gq of a morphism f : c Ñ c1 in C with a morphism g : d Ñ d1 in D; hence pC ˆ Dq1 “ C1 ˆ D1.
Composition is given by composing pairwise in C and D: pf1, g1q ˝ pf, gq :“ pf1 ˝ f, g1 ˝ gq.
Proof. That composition is associative and unital in CˆD follows immediately from those properties
in the underlying categories C and D.
Remark 2.2.14. Using the product of categories, we can gather the co- and contravariant families
of hom functors Cpc, ´q and Cp´, cq into a single hom functor Cp´, “q : C op ˆ C Ñ Set, mapping
px, yq : C op ˆ C to Cpx, yq.
Proposition 2.2.15. The product of categories extends to a functor ˆ : Cat ˆ Cat Ñ Cat.
Given functors F : C Ñ C1 and G : D Ñ D1, we obtain a functor F ˆ G by applying F to the left
factor of the product C ˆ D and G to the right.
Proof. Suﬃciently obvious that we omit it.
The archetypal 2-category is Cat itself, as we will now see: morphisms between functors are
called natural transformation, and they will play an important role throughout this thesis.
Deﬁnition 2.2.16. Suppose F and G are functors C Ñ D. A natural transformation α : F ñ G
is a family of morphisms αc : Fpcq Ñ Gpcq in D and indexed by objects c of C, such that for any
morphism f : c Ñ c1 in C, the following diagram commutes
Fc
Gc
Fc1
Gc1
αc
αc1
Ff
Gf .
We call this diagram a naturality square for α.
Example 2.2.17. Every morphism f
: a Ñ b in a category C induces a (contravariant)
natural transformation Cpf, ´q : Cpb, ´q ñ Cpa, ´q between covariant hom functors, acting
by precomposition. Dually, every morphism h : c Ñ d induces a (covariant) natural transformation
Cp´, hq : Cp´, cq ñ Cp´, dq between contravariant hom functors, acting by postcomposition. To
see that these two families are natural, observe that the square below left must commute for all
objects a, b, c : C and morphisms f : a Ñ b and h : c Ñ d, by the associativity of composition in C
26

(as illustrated on the right)
Cpb, cq
Cpa, cq
Cpb, dq
Cpa, dq
Cpf,cq
Cpb,hq
Cpf,dq
Cpa,hq
g
g ˝ f
h ˝ g
h ˝ g ˝ f
and that it therefore constitutes a naturality square for both Cpf, ´q and Cp´, hq. Note also that we
can take either path through this square as a deﬁnition of the function Cpf, hq : Cpb, cq Ñ Cpa, dq
which thus acts by mapping g : b Ñ c to h ˝ g ˝ f : a Ñ d.
Remark 2.2.18. We will see in §3.1.2 that the families of structure morphisms for a tensor product
(and hence used in the deﬁnition of enriched category) are more properly required to be natural
transformations.
The existence of morphisms between functors implies that the collection of functors between
any pair of categories itself forms a category, which we now deﬁne.
Proposition 2.2.19. The functors between two categories C and D constitute the objects of a
category, called the functor category and denoted by CatpC, Dq or DC, whose morphisms are the
natural transformations between those functors. The identity natural transformation on a functor
is the natural transformation whose components are all identity morphisms.
Proof. First, observe that the identity natural transformation is well-deﬁned, as the following
diagram commutes for any morphism f : c Ñ c1:
Fc
Fc
Fc1
Fc1
idF c
idF c1
Ff
Ff
(Note that in general, we will depict an identity morphism in a diagram as an elongated equality
symbol, as above.) Given two natural transformations α : F ñ G and β : G ñ H, their composite
is the natural transformation deﬁned by composing the component functions: pβ ˝ αqc :“ βc ˝ αc.
We can see that this gives a well-deﬁned natural transformation by pasting the component naturality
squares:
Fc
Gc
Hc
Fc1
Gc1
Hc1
αc
αc1
Ff
Gf
βc
βc1
Hf
27

Since the two inner squares commute, so must the outer square. And since the composition
of natural transformations reduces to the composition of functions, and the identity natural
transformation has identity function components, the composition of natural transformations
inherits strict associativity and unitality from composition in Set.
This gives us our a ﬁrst nontrivial example of a 2-category.
Example 2.2.20. Functor categories constitute the hom categories of the strict 2-category Cat,
and henceforth we will write Cat1 to denote the 1-category of categories and functors; we can
therefore say that Cat is enriched in Cat1. The 0-cells of Cat are categories, the 1-cells are
functors, and the 2-cells are natural transformations. If α is a natural transformation F ñ G, with
F and G functors C Ñ D, then we can depict it as ﬁlling the cell between the functors:
C
D
F
G
α
(More generally, we will depict 2-cells in this way, interpreting such depictions as diagrams of
enriched categories in the sense discussed above.)
Since Cat is a 2-category, it has both vertical composition (composition within hom-categories)
and horizontal (composition between them). In Proposition 2.2.19, we introduced the vertical
composition, so let us now consider the horizontal, which we will denote by ˛ to avoid ambiguity.
The horizontal composition of 1-cells is the composition of functors (as morphisms in Cat1),
but by the deﬁnition of enriched category, it must also extend to the 2-cells (here, the natural
transformations). Suppose then that we have natural transformations ϕ and γ as in the following
diagram:
B
C
D
F
G
F 1
G1
ϕ
γ
The horizontal composite γ ˛ ϕ is the natural transformation GF ñ G1F 1 with components
GFb
Gϕb
ÝÝÑ GF 1b
γF 1b
ÝÝÑ G1F 1b .
Notation 2.2.21 (Whiskering). It is often useful to consider the horizontal composite of a natural
transformation α : F ñ G with (the identity natural transformation on) a functor, as in the
following diagrams, with precomposition on the left and postcomposition on the right:
D
C
C1
L
F
G
L
α
idL
C
C1
D1
F
G
R
R
α
idR
28

We will often write the left composite α ˛ L : FL ñ GL as αL, since its components are
αLd : FLd Ñ GLd for all d : D; and we will often write the right composite R ˛ α : RF ñ RG
as Rα, since its components are Rαc : RFc Ñ RGc for all c : C. This use of notation is called
whiskering.
2.2.3. On functorial semantics
At this point, we pause to consider category theory from the general perspective of our motivating
examples, to reﬂect on how category theory might surprise us: as we indicated in §2.1.2, categories
are more ‘dynamical’ than graphs, more preoccupied with change, and so behave diﬀerently; in fact,
they have a much richer variety of behaviours, and just as categories can often be very well-behaved,
they can also be quite unruly. Through its homoiconicity—its ability to describe itself—the use
of category theory impels us to consider not only how individual systems are constructed, nor
only how systems of a given type can be compared, but also how to compare diﬀerent classes of
system. In this way, category theory rapidly concerns itself with notions not only of connection
and composition, but also of pattern and translation.
Scientiﬁcally, this is very useful: in the computational, cognitive, or otherwise cybernetic sciences,
we are often concerned with questions about when and how natural systems ‘compute’. Such
questions amount to questions of translation, between the abstract realm of computation to the
more concrete realms inhabited by the systems of interest and the data that they generate; one often
asks how natural structures might correspond to ‘algorithmic’ details, or whether the behaviours of
systems correspond to computational processes. It is for this reason that we chose our motivating
examples, which exhibited (abstract) natural structure as well as two kinds of informational
or computational structure: a central question in contemporary neuroscience is the extent to
which neural circuits can be understood as performing computation (particularly of the form now
established in machine learning). This question is in some way at the heart of this thesis, which
aims to establish a compositional framework in which the theories of predictive coding and active
inference may be studied.
The dynamism of categories is a hint that it is possible to describe both the structure of systems
and their function categorically, with a ‘syntax’ for systems on the one hand and ‘semantics’ on
the other. This is the notion of functorial semantics [33], by which we translate syntactic structures
in one category to another category which supplies semantics: the use of functors means that
this translation preserves basic compositional structure, and we often ask for these functors to
preserve other structures, too; a typical choice, that we will adopt in Chapter 3 is to use lax monoidal
functors, which preserve composition in two dimensions, allowing us to place systems “side by
side” as well as “end to end”.
29

Of course, the particular choices of syntactic and semantic category will depend upon the subject
at hand—in this thesis we will be particularly interested in supplying dynamical semantics for
approximate inference problems—but typically the syntactic category will have some ‘nice’ algebraic
structure that is then preserved and interpreted by the functorial semantics. This is, for instance,
how functorial semantics lets us understand processes that “happen on graphs”, and as a simple
example, we can consider diagrams in Set: the shape of the diagram tells us how to compose the
parts of a system together, while the diagram functor gives us, for each abstract part, a set of possible
components that have a compatible interface, as well as functions realizing their interconnection.
In categorical ‘process’ theory, and the more general categorical theory of systems, one therefore
often considers the objects of the ‘syntactic’ category as representing the shapes or interfaces of
systems and the morphisms as representing how the diﬀerent shapes can plug together. This is
an algebraic approach to systems design: mathematically, the syntactic structure is encoded as
a monad, and the functorial semantics corresponds to a monad algebra, as we explain in 3; and
the desire for composition richer than merely end-to-end is another motivation for venturing into
higher category theory. In 5, we will ‘unfold’ a combination of these ideas, to construct a bicategory
whose objects represent interfaces, whose 1-cells are processes ‘between’ the interfaces that can be
composed both sequentially and in parallel, and whose 2-cells are homomorphisms of processes.
This bicategory will then in 6 supply the semantics for models of predictive coding.
In science, there is rarely only one way to study a phenomenon, and our collective understanding
of phenomena is therefore a patchwork of perspectives. At the end of this chapter, we will discuss
the Yoneda Lemma, which formalizes this observation that to understand a thing is to see it from all
perspectives, and it is for this reason that we expect category theory to supply a lingua franca for the
mathematical sciences. In computational neuroscience speciﬁcally, an inﬂuential methodological
theory is David Marr’s “three levels of explanation” [42], in which complex cognitive systems are
proﬁtably studied at the levels of ‘computation’, ‘algorithm’, and ‘implementation’. These levels
are only very informally deﬁned, and the relationships between them not at all clear. We hope
that functorial semantics and other categorical approaches can replace such methodologies so that
instead of a simplistic hierarchical understanding of systems, we can progressively and clearly
expose the web of relationships between models.
2.2.4. Adjunction and equivalence
We discussed above the use of functors to translate between mathematical contexts. Often, we
are interested not only in translation in one direction, but also in translating back again. When
we have a pair of functors—or 1-cells more generally—in opposite directions and when the two
translations are somehow reversible, we often ﬁnd ourselves with an adjunction; for example, the
functorial mappings of graphs to categories and back are adjoint (Example 2.2.25 below), and we
30

conjecture in Chapter 7 that the mapping of “statistical games” to dynamical systems forms part
of an adjunction, too. Adjunctions are particularly well-behaved ‘dual’ translations, and they will
therefore be of much use throughout this thesis. For its conceptual elegance, we begin with an
abstract deﬁnition, which exhibits the fundamental essence.
Deﬁnition 2.2.22. Suppose L : C Ñ D and R : D Ñ C are 1-cells of a 2-category. We say that
they are adjoint or form an adjunction, denoted L % R, if there are 2-cells η : idC ñ RL and
ϵ : LR ñ idD, called respectively the unit and counit of the adjunction, which satisfy the triangle
equalities ϵL ˝ Lη “ idL and Rϵ ˝ ηR “ idR, so called owing to their diagrammatic depictions:
L
LRL
L
Lη
ϵL
and
R
RLR
R
ηR
Rϵ
The unit and counit of the adjunction measure ‘how far’ the round-trip composite functors
LR : C Ñ C and RL : D Ñ D leave us from our starting place, as indicated in the following
diagrams:
D
C
C
L
R
idC
η
and
C
D
D
R
L
idD
ϵ
The triangle identities then ensure that the round-trips have an isomorphic ‘core’, so that it is
possible to translate morphisms on one side to the other losslessly (which we will exemplify in
Proposition 2.2.26), and that the adjunction has a natural ‘algebraic’ interpretation (which we will
encounter in Proposition 3.4.13). To make sense of the former property, we need the notion of
isomorphism.
Deﬁnition 2.2.23. A morphism l : c Ñ d in a 1-category is an isomorphism if there is a morphism
r : d Ñ c such that l ˝ r “ idd and idc “ r ˝ l. We say that l and r are mutually inverse. When the
component 1-cells of a natural transformation α are all isomorphisms, then we call α a natural
isomorphism.
In the speciﬁc case of the 2-category Cat, we can make the following alternative characterization
of adjunctions. Here we see that the “isomorphic core” of the adjunction can be characterized by
saying that morphisms into objects in C that come from D via R are in bijection with morphisms
out of objects in D that come from C via L.
Deﬁnition 2.2.24. Suppose L : C Ñ D and R : D Ñ C are functors between categories C and
D. We say that they are adjoint functors when there is an isomorphism between the hom-sets
DpLc, dq – Cpc, Rdq that is natural in c : C and d : D.
31

Given a morphism f : Lc Ñ d in D, we denote its (right) adjunct in C by f7 : c Ñ Rd. Inversely,
given a morphism g : c Ñ Rd in C, we denote its (left) adjunct in D by g5 : Lc Ñ d. The existence
of the isomorphism means that f75 “ f and g “ g57.
Example 2.2.25. The functor F : Graph Ñ Cat mapping a graph to the corresponding free
category (Proposition 2.1.9) is left adjoint to the forgetful functor U : Cat Ñ Graph mapping
a category to its underlying graph (Proposition 2.1.8). To see this, we need to ﬁnd a natural
isomorphism CatpFG, Cq – GraphpG, UCq. A graph homomorphism G Ñ UC is a mapping
of the nodes of G to the objects of C and of the edges of G to the morphisms of C that preserves
sources (domains) and targets (codomains). A functor FG Ñ C is a mapping of the nodes of G to
the objects of C along with a mapping of paths in G to morphisms in C that preserves domains,
codomains, identities and composites. A path in G is a list of ‘composable’ edges, with the identity
path being the empty list, so such a mapping of paths is entirely determined by a mapping of
edges to morphisms that preserves domains and codomains. That is to say, a functor FG Ñ C
is determined by, and determines, a graph homomorphism G Ñ UC, and so the two sets are
isomorphic: in some sense, functors between free categories are graph homomorphisms. To see that
the isomorphism is natural, observe that it doesn’t matter if we precompose a graph homomorphism
G1 Ñ G (treated as a functor between free categories) or postcompose a functor C Ñ C1 (treated as
a graph homomorphism): because graph homomorphisms compose preserving the graph structure,
we would still have an isomorphism CatpFG1, C1q – GraphpG1, UC1q.
Before we can properly say that adjoint functors form an adjunction, we need to prove it. As the
following proof shows, the mappings p´q7 and p´q5 deﬁne and are deﬁned by the unit and counit
of the adjunction.
Proposition 2.2.26. Functors that form an adjunction in Cat are exactly adjoint functors.
Proof. We need to show that functors that form an adjunction are adjoint, and that adjoint functors
form an adjunction; that is, we need to show that any pair of functors L : C Ñ D and R : D Ñ C
satisfying the deﬁnition of adjunction in Deﬁnition 2.2.22 necessarily constitute adjoint functors
according to Deﬁnition 2.2.24, and that if L and R are adjoint according to Deﬁnition 2.2.24 then
they form an adjunction according to Deﬁnition 2.2.22: i.e., the two deﬁnitions are equivalent.
We begin by showing that if L % R, then L and R are adjoint functors. This means we need
to exhibit a natural isomorphism DpLc, dq – Cpc, Rdq. We deﬁne a function p´q7 : DpLc, dq Ñ
Cpc, Rdq by setting
f7 :“ c
ηc
ÝÑ RLc
Rf
ÝÝÑ Rd
and a function p´q5 : Cpc, Rdq Ñ DpLc, dq by setting
g5 :“ Lc
Lg
ÝÑ LRd
ϵd
ÝÑ d .
32

We then use naturality and the triangle equalities to show that f75 “ f and g57 “ g:
f75 “ Lc
Lf7
ÝÝÑ LRd
ϵd
ÝÑ d
“ Lc
Lηc
ÝÝÑ LRLc
LRf
ÝÝÝÑ LRd
ϵd
ÝÑ d
“ Lc
Lηc
ÝÝÑ LRLc ϵLc
ÝÝÑ Lc
fÝÑ d
“ Lc
fÝÑ d
g57 “ c
ηc
ÝÑ RLc
Rg5
ÝÝÑ Rd
“ c
ηc
ÝÑ RLc RLc
ÝÝÑ RLRd
Rϵd
ÝÝÑ Rd
“ c
gÝÑ Rd
ηRd
ÝÝÑ RLRd
Rϵd
ÝÝÑ Rd
“ c
gÝÑ Rd
In each case the ﬁrst two lines follow by deﬁnition, the third by naturality, and the fourth by the
triangle equality; hence we have an isomorphism DpLc, dq – Cpc, Rdq. The naturality of this
isomorphism follows from the naturality of η and ϵ. We ﬁrst check that the isomorphisms p´q7 are
natural in c, which means that the following squares commute for all φ : c1 Ñ c in C:
DpLc, dq
Cpc, Rdq
DpLc1, dq
Cpc1, Rdq
DpLφ,dq
p´q7
c1,d
Cpφ,Rdq
p´q7
c,d
This requires in turn that pf ˝ Lφq7 “ f7 ˝ φ, which we can check as follows:
pf ˝ Lφq7 “ c1 ηc1
ÝÝÑ RLc1 RLφ
ÝÝÝÑ RLc
Rf
ÝÝÑ Rd
“ c1 φÝÑ c
ηc
ÝÑ RLc
Rf
ÝÝÑ Rd
“ c1 φÝÑ c
f7
ÝÑ Rd
where the second equality holds by the naturality of η. The naturality of p´q7 in d requires that
pφ1 ˝ fq7 “ Rφ1 ˝ f7 for all φ1 : d Ñ d1, which can be checked almost immediately:
pφ1 ˝ fq7 “ c
ηc
ÝÑ RLc
Rf
ÝÝÑ Rd
Rφ1
ÝÝÑ Rd1
“ c
f7
ÝÑ Rd
Rφ1
ÝÝÑ Rd1
Dually, the naturality of p´q5 : Cpc, Rdq Ñ DpLc, dq in d requires that pRφ1 ˝ gq5 “ φ1 ˝ g5 for all
φ1 : d Ñ d1, which obtains by the naturality of ϵ:
pRφ1 ˝ gq5 “ Lc
Lg
ÝÑ LRd
LRφ1
ÝÝÝÑ LRd1 ϵd1
ÝÑ d1
“ Lc
Lg
ÝÑ LRd
ϵd
ÝÑ d
φ1
ÝÑ d1
“ Lc
g5
ÝÑ d
φ1
ÝÑ d1
The naturality of p´q5 in c, which requires that pg ˝ φq5 “ g5 ˝ Lφ, obtains similarly immediately:
pg ˝ φq5 “ Lc1 Lφ
ÝÝÑ Lc
Lg
ÝÑ LRd
ϵd
ÝÑ d
“ Lc1 Lφ
ÝÝÑ Lc
g5
ÝÑ d
33

Thus p´q7 and p´q5 are both natural in c and d, and hence L and R are adjoint functors.
To show the converse, that if L : C Ñ D and R : D Ñ C are adjoint functors then L % R,
we need to establish natural transformations η : idC ñ RL and ϵ : LR ñ idD from the natural
isomorphisms p´q7 and p´q5, such that the triangle equalities ϵL ˝Lη “ idL and Rϵ˝ηR “ idR are
satisﬁed. We ﬁrst deﬁne η componentwise, by observing that ηc must have the type c Ñ RLc, and
that the image of idLc : Lc Ñ Lc under p´q7 is of this type, and therefore deﬁning ηc :“ pidLcq7.
Dually, we deﬁne ϵ by observing that ϵd must have the type LRd Ñ d, and that the image of idRd
under p´q5 has this type. We therefore deﬁne ϵd :“ pidRdq5. To see that these deﬁnitions constitute
natural transformations, observe that they are themselves composed from natural transformations.
Explicitly, the naturality of η means that for any f : c Ñ c1, we must have RLf ˝ ηc “ ηc1 ˝ f, and
the naturality of ϵ means that for any g : d Ñ d1, we must have g ˝ ϵd “ ϵd1 ˝ LRg. These obtain
as follows:
RLf ˝ ηc “ c
pidLcq7
ÝÝÝÝÑ RLc
RLf
ÝÝÝÑ RLc1
“ c
pLf˝idLcq7
ÝÝÝÝÝÝÑ RLc1
“ c
pidLc1 ˝Lfq7
ÝÝÝÝÝÝÝÑ RLc1
“ c
fÝÑ c1 pidLc1q7
ÝÝÝÝÑ RLc1
“ ηc1 ˝ f
g ˝ ϵd “ LRd
pidRdq5
ÝÝÝÝÑ d
gÝÑ d1
“ LRd
pRg˝idRdq5
ÝÝÝÝÝÝÝÑ d1
“ LRd
pidRd1 ˝Rgq5
ÝÝÝÝÝÝÝÑ d1
“ LRd
LRg
ÝÝÝÑ LRd1 pidRd1q5
ÝÝÝÝÝÑ d1
“ ϵd1 ˝ LRg
In each case, the ﬁrst equality holds by deﬁnition, the second by naturality of p´q7 and p´q5 (left
and right, respectively) in d, the third by naturality of id, the fourth by naturality in c, and the last
by deﬁnition. It remains to check that η and ϵ so deﬁned satisfy the triangle equalities. Expressed
componentwise, we demonstrate that ϵLc ˝ Lηc “ idLc and that Rϵd ˝ ηRd “ idRd as follows:
ϵLc ˝ Lηc “ Lc
LpidLcq7
ÝÝÝÝÝÑ LRLc
pidRLcq5
ÝÝÝÝÝÑ Lc
“ Lc
pidRLc ˝pidLcq7q5
ÝÝÝÝÝÝÝÝÝÝÑ Lc
“ Lc
pidLcq75
ÝÝÝÝÝÑ Lc
“ Lc idLc
ÝÝÑ Lc
Rϵd ˝ ηRd “ Rd
pidLRdq7
ÝÝÝÝÝÑ RLRd
RpidRdq5
ÝÝÝÝÝÑ Rd
“ Rd
ppidRdq5˝idLRdq7
ÝÝÝÝÝÝÝÝÝÝÑ Rd
“ Rd
pidRdq57
ÝÝÝÝÝÑ Rd
“ Rd
idRd
ÝÝÑ Rd
The ﬁrst equality (on each side) holds by deﬁnition, the second (on the left) by naturality of p´q5 in
c and (on the right) by naturality of p´q7 in d, the third by unitality of composition, and the fourth
by the 7/5 isomorphism. This establishes that L % R, and hence the result.
Sometimes, the ‘distances’ measured by the unit and counit are so small that the categories C
and D are actually ‘equivalent’: this happens when the unit and counit are natural isomorphisms,
meaning that the isomorphic core of the adjunction extends to the whole of C and D. This gives us
the following deﬁnition.
34

Deﬁnition 2.2.27. Suppose L % R in a 2-category. When the unit and counit of the adjunction
are additionally isomorphisms, we say that L and R form an adjoint equivalence.
Remark 2.2.28. More generally, an equivalence of categories is a pair of functors connected by
natural isomorphisms of the form of the unit and counit of an adjunction, but which may not
necessarily satisfy the triangle identities; however, given any such equivalence, it is possible to
modify the unit or counit so as to upgrade it to an adjoint equivalence. Henceforth, we will have
no need to distinguish equivalences from adjoint equivalences, so we will say simply ‘equivalence’
for both. If there is an equivalence between a pair of categories, then we will say that the two
categories are equivalent.
Note that the notion of equivalence of categories can be generalized to equivalence in a 2-category,
by replacing the categories by 0-cells, the functors by 1-cells, and the natural isomorphisms by
invertible 2-cells.
The structure of an equivalence of categories can alternatively be speciﬁed as properties of the
functors concerned, which in some situations can be easier to verify.
Deﬁnition 2.2.29. We say that a functor F : C Ñ D is
1. full when it is surjective on hom sets, in the sense that the functions Fa,b : Cpa, bq Ñ
DpFa, Fbq are surjections;
2. faithful when it is injective on hom sets, in the sense that the functions Fa,b are injections;
3. fully faithful when it is both full and faithful (i.e., isomorphic on hom sets); and
4. essentially surjective when it is surjective on objects up to isomorphism, in the sense that for
every object d : D there is an object c : C such that Fc – d.
Proposition 2.2.30. Categories C and D are equivalent if and only if there is a functor F : C Ñ D
that is fully faithful and essentially surjective.
Proof [85, Lemma 9.4.5]. First, we show that if F % G : D Ñ C is an equivalence of categories,
then F : C Ñ D is fully faithful and essentially surjective. For the latter, observe that G gives
us, for any d : D, an object Gd : C and ϵd is by deﬁnition an isomorphism FGd
„
ÝÑ d; hence
F is essentially surjective. To show that F is fully faithful means showing that each function
Fa,b : Cpa, bq Ñ DpFa, Fbq is an isomorphism; we can deﬁne the inverse F ´1
a,b as the following
composite:
DpFa, Fbq
GF a,F b
ÝÝÝÝÑ
CpGFa, GFbq
Cpηa,η´1
b
q
ÝÝÝÝÝÝÑ
Cpa, bq
g
ÞÑ
Gg
ÞÑ
`
a
ηa
ÝÑ GFa
Gg
ÝÝÑ GFb
η´1
b
ÝÝÑ b
˘
35

Here, the function Cpηa, η´1
b q is the function f ÞÑ η´1
b
˝ f ˝ ηa obtained from the hom functor
(Remark 2.2.14). Hence F ´1
a,b pgq :“ η´1
b
˝ Gg ˝ ηa. To see that this is indeed the desired inverse,
consider applying the functor F to the morphism F ´1
a,b pgq; we have the following equalities:
Fa
Fηa
ÝÝÑ FGFa
FGg
ÝÝÝÑ FGFb
Fη´1
b
ÝÝÝÑ Fb
“ Fa
gÝÑ Fb
Fηb
ÝÝÑ FGFb
Fη´1
b
ÝÝÝÑ Fb
“ Fa
gÝÑ Fb
where the ﬁrst equality holds by the naturality of η and the second equality holds since ηb is an
isomorphism. Since F is therefore isomorphic on hom sets, it is fully faithful.
Next, we show that if F : C Ñ D is fully faithful and essentially surjective, then there is a
functor G : D Ñ C and natural isomorphisms η : idC ñ GF and ϵ : FG ñ idD. On objects d : D,
we can deﬁne Gd : C as any choice of object such that FGd „
ÝÑ d: such an object must exist since
F is essentially surjective. We then deﬁne ϵd to be the associated isomorphism FGd Ñ d; it is
easy to verify that ϵ so deﬁned is natural. On morphisms, let the functions Gd,e be deﬁned as the
composite functions
Dpd, eq
Dpϵd,ϵ´1
e q
ÝÝÝÝÝÝÑ
DpFGd, FGeq
F ´1
Gd,Ge
ÝÝÝÝÑ
CpGd, Geq
g
ÞÑ
`
FGd
ϵd
ÝÑ d
gÝÑ e ϵ´1
e
ÝÝÑ FGe
˘
ÞÑ
F ´1
Gd,Ge
`
ϵ´1
e
˝ g ˝ ϵd
˘ .
Since F is a fully faithful functor and ϵ is a natural isomorphism, this makes G a well-deﬁned
functor. Finally, we deﬁne η as having the components ηc :“ F ´1
c,GFc
`
ϵ´1
Fc
˘
; since ϵ is a natural
isomorphism, so is ϵ´1, which is thus preserved as such by the inverse action of F in deﬁning η.
This establishes all the data of the equivalence.
(Note that we can actually prove a little more: it is not hard to verify additionally that the two
constructions are inverse, so that equivalences are themselves equivalent to fully faithful essentially
surjective functors.)
Remark 2.2.31. In the above proof, we assumed the axiom of choice, deﬁning Gd as a choice
of object such that FGd
„
ÝÑ d. It is possible to avoid making this assumption, by asking for
the surjection on objects F0 : C0 Ñ D0 to be ‘split’ in the sense that it comes with a function
s : D0 Ñ C0 such that F0pspdqq – d in D for every object d : D; then we just set Gd :“ spdq.
2.3. Universal constructions
In the preceding sections, we have used diagrams to represent some patterns in a categorical
context, and we have discussed how functors allow us to translate patterns and structures between
contexts; indeed, we used functors to formalize diagrams themselves. But an important facet of
36

the notion of pattern is replication across contexts, and in many important situations, we will
encounter patterns that apply to all objects in a category. We call such patterns universal, and much
of science is a search for such universal patterns: for example, much of physics, and by extension
much of the theory of the Bayesian brain, is a study of the universal principle of stationary action.
In this section, we introduce the formal characterization of universality and exemplify it with some
examples that will be particularly important later on — as well as some examples that we have
encountered already.
2.3.1. The universality of common paterns
We begin with some basic examples of universal patterns.
2.3.1.1. Disjunctions, or coproducts
Our ﬁrst example of a universal pattern is the coproduct, which captures the essence of the following
examples — situations like disjunction, where there is an element of choice between alternatives.
Example 2.3.1. Given two propositions, such as P1 :“ “ ´ is ﬂat” and P2 :“ “ ´ is sharp”, we
can form their disjunction P1 _ P2 (meaning “ ´ is ﬂat or sharp”). Similarly, given two subsets
P1, P2 Ď X, we can form their join or union, P1 Y P2: an element x is an element of P1 Y P2 if
(and only if) it is an element of P1 or an element of P2.
Example 2.3.2. Given two numbers, we can form their sum; for instance, 1 ` 2 “ 3. More
generally, given two sets A and B, we can form their disjoint union, denoted A ` B. The elements
of A ` B are pairs pi, xq where x is an element of A or of B and i indicates which set x is drawn
from (this ensures that if an element x of A is the same as an element of B, it is added twice to the
disjoint union). Therefore, if A has 1 element and B has 2, then A ` B has 3 elements.
Remark 2.3.3. The preceding example illustrates how we can think of numbers equivalently as
sets of the indicated cardinality. Many operations on sets are generalizations of familiar operations
on numbers in this way.
Example 2.3.4. Given two graphs, G and G1, we can form the sum graph G ` G1, whose set of
nodes is G0 ` G1
0 and whose set of edges is G1 ` G1
1.
Example 2.3.5. Given two vector spaces V and V 1, we can form their direct sum V ‘ V 1, whose
vectors are linear combinations of vectors either in V or in V 1.
Each of these is an example of a coproduct, which we now deﬁne.
37

Deﬁnition 2.3.6. Given objects A and B in a category C, their coproduct (if it exists) is an object,
canonically denoted A`B, equipped with two morphisms injA : A Ñ A`B and injB : B Ñ A`B
such that, for any object Q equipped with morphisms f : A Ñ Q and g : B Ñ Q, there is a unique
morphism u : A ` B Ñ Q such that f “ u ˝ injA and g “ u ˝ injB. The morphisms injA and injB
are called injections, and the morphism u is called the copairing and often denoted by rf, gs.
Example 2.3.7. Morphisms of subsets are inclusions, so given subsets P1, P2 Ď X, there are
evident inclusions P1 Ď P1 Y P2 and P2 Ď P1 Y P2. Moreover, given a subset Q such that P1 Ď Q
and P2 Ď Q, it is clearly the case that P1 Ď P1 ` P2 Ď Q and P2 Ď P1 ` P2 Ď Q.
Similarly, morphisms of propositions are implications, so given P1 and P2 such that P1 Ñ Q and
P2 Ñ Q, then it is necessarily the case that P1 Ñ P1 _ P2 Ñ Q and P2 Ñ P1 _ P2 Ñ Q: clearly,
both P1 and P2 imply P1 _ P2 by deﬁnition, and if both P1 and P2 imply Q, then so does P1 _ P2.
Example 2.3.8. Given sets A and B, the injections injA : A Ñ A ` B and injB : B Ñ A ` B
are the corresponding inclusions: injA maps a to p1, aq and injB maps b to p2, bq, where 1 tags
an element as coming from A, and 2 tags an element as coming from B. Given f : A Ñ Q and
g : B Ñ Q the copairing rf, gs : A ` B Ñ Q is the function that takes an element pi, xq and
returns fpxq if i “ 1 or gpxq if i “ 2; it is from this that the ‘choice’ interpretation arises for the
coproduct.
Example 2.3.9. Morphisms of vectors spaces are linear maps, and if the spaces are ﬁnite-
dimensional, then we can represent these maps as matrices: if V is n-dimensional and W is
m-dimensional, then a morphism V Ñ W is a matrix of shape pm, nq; writing the elements of V
and W as column vectors, such a matrix has m rows and n columns. Moreover, in this case, the
direct sum V ‘ W is pn ` mq-dimensional.
Therefore suppose that V , V 1 and W have respective dimensions n, n1 and m, and suppose we
have linear maps f : V Ñ W and g : V 1 Ñ W. The injection V Ñ V ‘ V 1 is the block matrix
˜
1n
0n1
¸
where 1n is the n-by-n identity matrix and 0n1 is the n1-by-n1 zero matrix; similarly, the
injection V 1 Ñ V ‘ V 1 is the block matrix
˜
0n
1n1
¸
. And the copairing rf, gs : V ‘ V 1 Ñ W is the
block matrix
´
f
g
¯
.
Remark 2.3.10. The coproducts we have considered so far have all been binary, being coproducts
of only two objects. More generally, we can often consider coproducts of more objects, by repeating
the binary coproduct operation; typically, there is an isomorphism pA ` Bq ` C – A ` pB ` Cq.
We can extend this further to ﬁnite (and, often, inﬁnite) collections of objects. Suppose then that
tAiu is a collection of objects indexed by i : I, where I is a set, and form the iterated coproduct
ř
i:I Ai; we will call this object a dependent sum, because the summands Ai depend on i : I. In the
38

case where the objects Ai are all sets, the dependent sum ř
i Ai is again a set, whose elements are
pairs pi, xq where i is an element of I and x is an element of Ai. In other categories C, typically the
name dependent sum is reserved for the case when all of the objects Ai and the object I are objects
of C. But when I remains a set, we may still be able to form the I-indexed coproduct ř
i Ai in C.
2.3.1.2. Conjunctions, products, and sections
Our next example of a universal pattern is the product, which captures situations like conjunction,
in which things come along in separable pairs of individuals.
Example 2.3.11. Given two propositions, such as P1 :“ “´ is small” and P2 :“ “´ is connected”,
we can form their conjunction P1 ^ P2 (meaning “ ´ is small and connected”). Similarly, given
two subsets P1, P2 Ď X, we can form their meet or intersection, P1 X P2: an element x is an
element of P1 X P2 if (and only if) it is an element of P1 and an element of P2.
Example 2.3.12. Given two numbers, we can form their product; for instance, 2 ˆ 3 “ 6. More
generally, as we saw in Deﬁnition 2.2.1, we can form the product of any two sets A and B, denoted
A ˆ B. The elements of A ˆ B are pairs pa, bq where a is an element of A and b is an element of
B. Therefore, if A has 2 elements and B has 3, then A ˆ B has 6 elements.
Remark 2.3.13. When all the summands of a dependent sum are the same set or object A regardless
of their associated index i : I, then the object ř
i:I A is isomorphic to the product I ˆ A: this is
simply a categoriﬁcation of the fact that “multiplication is repeated addition”.
Example 2.3.14. Given vector spaces V and V 1 (of respective dimensions n and n1), their product
is again the direct sum V ‘ V 1. Since the direct sum of vector spaces is both a product and a
coproduct, it is also said to be a biproduct.
Categorically, the product is the dual of the coproduct.
Deﬁnition 2.3.15. Given objects A and B in a category C, their product (if it exists) is an object,
canonically denoted A ˆ B, equipped with two morphisms projA : A ˆ B Ñ A and projB :
A ˆ B Ñ B such that, for any object Q equipped with morphisms f : Q Ñ A and g : Q Ñ B,
there is a unique morphism u : Q Ñ A ˆ B such that f “ projA ˝ u and g “ projB ˝ u. The
morphisms projA and projB are called projections, and the morphism u is called the pairing and
often denoted by pf, gq.
Example 2.3.16. Given subjects P1, P2 Ď X, there are evident inclusions P1 X P2 Ď P1 and
P1 X P2 Ď P2. Moreover, given a subset Q such that Q Ď P1 and Q Ď P2, it is clearly then the
case that Q Ď P1 X P2 Ď P1 and Q Ď P1 X P2 Ď P2.
Similarly, given propositions P1 and P2 such that Q Ñ P1 and Q Ñ P2, it is (by the deﬁnition
of “and”) the case that Q Ñ P1 ^ P2 Ñ P1 and Q Ñ P1 ^ P2 Ñ P2.
39

Example 2.3.17. Given sets A and B, the projections projA : AˆB Ñ A and projB : AˆB Ñ B
are the functions pa, bq ÞÑ a and pa, bq ÞÑ b respectively. Given f : Q Ñ A and g : Q Ñ B, the
pairing pf, gq : Q Ñ A ˆ B is the function x ÞÑ
`
fpxq, gpxq
˘
; note that this involves ‘copying’ x,
which will be relevant when we come to consider copy-discard categories in §3.1.
Remark 2.3.18. Above, we observed that a coproduct with constant summands A is equivalently
a product I ˆ A of the indexing object I with A; we therefore have a projection projI : I ˆ A Ñ I.
More generally, for any dependent sum ř
i:I Ai, there is a projection ř
i:I Ai Ñ I; in the case of
dependent sums in Set, this is unsurprisingly the function pi, xq ÞÑ x.
Example 2.3.19. Suppose we have vector spaces V , V 1 and W of respective dimensions n, n1 and
m. The projection V ‘ V 1 Ñ V is the block matrix
´
1n
0n1
¯
, and the projection V ‘ V 1 Ñ V 1
is the block matrix
´
0n
1n1
¯
. Given linear maps f : W Ñ V and g : W Ñ V 1, the pairing
pf, gq : W Ñ V ‘V 1 is the block matrix
˜
f
g
¸
. Note that, in a sign of the duality between products
and coproducts, the projections and the pairing are respectively the injections and the copairing
transposed.
Remark 2.3.20. Just as in the case of coproducts, we can also consider products of more than two
objects, by repeating the product operation; there is again typically an isomorphism pAˆBqˆC –
A ˆ pB ˆ Cq. If tAiu is a collection of objects indexed by i : I (with I again a set), we can form
the dependent product1 ś
i:I Ai. In the case where I is ﬁnite and the objects Ai are all sets, the
dependent product ś
i:I Ai is again a set, whose elements can equivalently be seen as lists pai, . . . q
indexed by i with each element ai drawn from the corresponding set Ai or as functions s with
domain I and codomain the dependent sum ř
i:I Ai such that each spiq is tagged by i. This situation
is summarized by the commutativity of the diagram
I
ř
i:I Ai
I
s
π
where π is the projection and which therefore requires that π ˝ s “ idI. Such a function s is known
as a section of p, and we can think of sections therefore as dependent functions, since the types of
their output values (i.e., Ai) may depend on the input values i.
The notion of section is important enough to warrant a general deﬁnition.
1This set-indexed product is also known as an indexed product, to emphasize that the factors are indexed by the set I;
since I has elements, we can properly think of these as indices, which may not be true for other kinds of object. We
will see in Deﬁnition 2.3.63 how to use categorical structure to abstract away the requirement that I be a set.
40

Deﬁnition 2.3.21. Suppose p : E Ñ B is a morphism. A section of p is a morphism s : B Ñ E
such that p ˝ s “ idB.
2.3.1.3. Subobjects and equalizers
Our next examples of universal patterns do not involve pairing or grouping objects together to make
new ones. For instance, in many situations, it is of interest to restrict our attention to ‘subobjects’
(of a single given object) that satisfy a certain property, which may not extend to the whole object
at hand.
Example 2.3.22. In examples above, we saw that subsets and propositions behave similarly with
respect to disjunctions and conjunctions. More broadly, there is a correspondence between subsets
and propositions, if we think of propositions on a set X as functions X Ñ 2, where 2 is the
2-element set tK, Ju of truth values (where we interpret K as ‘false’ and J as ‘true’). Every subset
A Ď X has an associated injection, A ãÑ X, and there is a correspondence between such injections
and propositions PA : X Ñ 2, where PApxq is true whenever x is an element of A. This situation
can be summarized by the commutativity of the diagram
A
1
X
2
!
J
PA
where 1 is the singleton set t˚u, ! is the unique function sending every element of A to ˚, and
J is the function ˚ ÞÑ J picking the truth value J. If, in a category C, there is an object such
that, for any subobject A ãÑ X, there is a unique morphism X Ñ 2 such that the above diagram
commutes, then we say that the object 2 is a subobject classiﬁer in C; in this case, we interpret 1 as
the ‘terminal’ object in C (introduced below, in Example 2.3.41).
A pattern that will be particularly common is that in which we care about a subset of elements
of a set that make two functions equal. This can be generalized to arbitrary categories using the
following notion.
Deﬁnition 2.3.23. Suppose f and g are both morphisms X Ñ Y . Their equalizer is an object E
equipped with a function e : E Ñ X such that f ˝ e “ g ˝ e (so that e is said to equalize f and g)
as in the commuting diagram
E
X
Y
f
g
e
41

and such that, for any d : D Ñ X equalizing f and g, there is a unique morphism u : D Ñ E such
that d “ e ˝ u, as in the diagram
D
E
X
Y
f
g
e
u
d
.
Example 2.3.24. Via the correspondence between subsets and propositions, we can express the
conjunction of propositions as an equalizer. Suppose have have two propositions PA : X Ñ 2
and PB : X Ñ 2, corresponding to subsets A ãÑ X and B ãÑ X, whose inclusions we denote by
ιA and ιB respectively. The equalizer of A ˆ B
X
ιA˝projA
ιB˝projB
is the subset of A ˆ B whose
elements are pairs pa, bq in which a “ b in X. This subset is isomorphic to the meet A X B, which
corresponds as a proposition to the conjunction PA ^ PB.
2.3.1.4. Coequalizers and quotients
We can also make objects ‘smaller’ by dividing them into equivalence classes, as we did when
quotienting free categories by given relations (cf. Proposition 2.1.13). In general, this pattern is
captured by the notion of coequalizer, which is dual to the notion of equalizer in the same way that
coproducts are dual to products.
Deﬁnition 2.3.25. Suppose f and g are both morphisms X Ñ Y . Their coequalizer is an object
P equipped with a function p : Y Ñ P such that p ˝ f “ p ˝ g (with p said to coequalize f and g)
as in the commuting diagram
X
Y
P
f
g
p
and such that, for any q : Y Ñ Q coequalizing f and g, there is a unique morphism u : P Ñ Q
such that q “ u ˝ p, as in the diagram
X
Y
P
Q
f
g
p
u
q
.
Example 2.3.26. A relation „ on a set X is a proposition on X ˆ X, and thus equivalently a
subset R ãÑ X ˆ X; let ι denote the inclusion. The coequalizer of R
X
proj1˝ι
proj2˝ι
is the set
of equivalence classes of X according to „, which is precisely the quotient X{„.
42

2.3.2. The patern of universality
There is a common pattern to the common patterns above: in each case, we described an object
U equipped with some morphisms, such that, given any object X with morphisms of a similar
shape, there was a unique morphism u relating X and U. The existence of such a unique morphism
for any comparable X makes the object U a universal representative of the situation at hand and
has a number of powerful consequences: in particular, it entirely characterizes the object U up to
isomorphism. Much of the power of category theory comes from the use of universal properties to
classify, compare, and reason about situations of interest — for the general notion of universality
itself can be characterized categorically.
Deﬁnition 2.3.27. Suppose F : C Ñ D is a functor and X : D an object. We deﬁne two dual
universal constructions. A universal morphism from X to F is a morphism u : X Ñ FU for a
corresponding universal object U : C such that for any f : X Ñ FV in D there exists a unique
e : U Ñ V such that f “ X
uÝÑ FU
Fe
ÝÝÑ FV .
Dually, a universal morphism from F to X is a morphism u : FU Ñ X for a given U : C such
that for any f : FV Ñ X in D there exists a unique e : V Ñ U such that f “ FV
Fe
ÝÝÑ FU
uÝÑ X.
We can now formalize the universal properties of the examples we met above, beginning with
the coproduct.
Example 2.3.28. Let ∆: C Ñ C ˆ C denote the functor X ÞÑ pX, Xq. A coproduct of X and
Y in C is a universal morphism from the object pX, Y q in C ˆ C to ∆: that is, an object X ` Y
in C and a morphism pinjX, injY q : pX, Y q Ñ pX ` Y, X ` Y q in C ˆ C such that, for any
pf, gq : pX, Y q Ñ pQ, Qq in C ˆ C, the copairing rf, gs : X ` Y Ñ Q uniquely satisﬁes the
equation pf, gq “ pX, Y q
pinjX,injY q
ÝÝÝÝÝÝÑ pX ` Y, X ` Y q
prf,gs,rf,gsq
ÝÝÝÝÝÝÝÑ pQ, Qq.
Example 2.3.29. Again let ∆: C Ñ C ˆ C denote the functor X ÞÑ pX, Xq. A product of X and
Y in C is a universal morphism from the object pX, Y q : C ˆ C to ∆: that is, an object X ˆ Y
in C and a morphism pprojX, projY q : pX ˆ Y, X ˆ Y q Ñ pX, Y q in C ˆ C such that, for any
pf, gq : pQ, Qq Ñ pX, Y q in C ˆ C, the pairing pf, gq : Q Ñ X ˆ Y uniquely satisﬁes the equation
pf, gq “ pQ, Qq
ppf,gq,pf,gqq
ÝÝÝÝÝÝÝÑ pX ˆ Y, X ˆ Y q
pprojX,projY q
ÝÝÝÝÝÝÝÝÑ pX, Y q.
Remark 2.3.30. If we let 2 denote the two-object discrete category t‚ ‚u, then there is an
equivalence C ˆ C – C2 and so a pair of morphisms in C is equivalently a natural transformation
in C2. (This is a categoriﬁcation of the familiar fact that “exponentiation is repeated multiplication”,
which we will explore in §2.3.4.)
Consequently, the functor ∆from the preceding examples can equivalently be deﬁned as a
functor C Ñ C2. Letting the exponent take a more general shape, we obtain a family of constant
functors.
43

Proposition 2.3.31. Suppose C and D are categories, and d : D is an object. Then there is a
constant functor on d, denoted ∆d : C Ñ D, which takes each object c : C to d : D and each
morphism f : c Ñ c1 to idd; note that Fc “ d “ Fc1. The assignment d ÞÑ ∆d is itself trivially
functorial, giving a functor ∆: D Ñ DC which we call the constant functor functor.
Example 2.3.32. Let J be the category with two objects, 1 and 2, and two non-identity morphisms
α, β : 1 Ñ 2, as in the diagram 1
2
α
β
, and let ∆be the constant functor functor C Ñ CJ .
Now suppose f and g are two morphisms X Ñ Y in C. To construct their equalizer as a universal
morphism, let D be the diagram J Ñ C mapping α ÞÑ f and β ÞÑ g. Then an equalizer of
f and g is a universal morphism from ∆to D (with D being an object of the functor category
CJ ): that is, an object E : C equipped with a natural transformation ϵ : ∆E ñ D satisfying the
universal property that, for any ϕ : ∆F ñ D there exists a unique morphism u : F Ñ E such
that ϕ “ ϵ ˝ ∆u.
Unraveling this deﬁnition, we ﬁnd that such a natural transformation ϵ consists of a pair of
morphisms ϵ1 : E Ñ X and ϵ2 : E Ñ Y making the following naturality squares commute:
E
X
E
Y
ϵ1
ϵ2
f
E
X
E
Y
ϵ1
ϵ2
g
We can therefore set ϵ1 “ e, where e is the equalizing morphism E Ñ X. The commutativity of
the naturality squares enforces that f ˝ e “ ϵ2 “ g ˝ e and hence that f ˝ e “ g ˝ e, which is the
ﬁrst condition deﬁning the equalizer. Unwinding the universal property as expressed here shows
that the morphisms ϕ1 and u correspond exactly to the morphisms d and u of Deﬁnition 2.3.23.
Example 2.3.33. The case of a coequalizer is precisely dual to that of an equalizer. Therefore, let
J , ∆, and D be deﬁned as above. A coequalizer of f, g : X Ñ Y is then a universal morphism
from D to ∆.
Example 2.3.34. In Proposition 2.1.13, we constructed a category generated with relations FG{„
as a quotient of a free category on a graph FG. Since this category FG{„ is a quotient and
quotients are coequalizers (by Example 2.3.26), the projection functor FG Ñ FG{„ (Example
2.1.26) constitutes the associated universal morphism, in the sense dual to the morphism ϵ1 of
Example 2.3.32.
Example 2.3.35. The free category construction itself (Proposition 2.1.9) exhibits a universal
property, as a consequence of the free-forgetful adjunction (Example 2.2.25): given a graph G and a
category C, any functor FG Ñ C is uniquely determined by a graph homomorphism G Ñ UC from
G to the underlying graph of C. More precisely, there is a universal morphism from the singleton set
44

1 to the functor CatpF´, Cq for every category C. This means that, for any graph G, every functor
f : FG Ñ C factors as FG Fh
ÝÝÑ FUC uÝÑ C where u is the universal morphism and h is the unique
graph homomorphism. This universal property follows abstractly from facts that we will soon
encounter: that adjoint functors are ‘representable’ (Proposition 2.4.16); and that representable
functors are universal (Proposition 2.4.23). We hinted at this property in Example 2.2.25, where we
observed that functors between free categories ‘are’ graph homomorphisms: the homomorphism h
here is the graph homomorphism corresponding to the functor f, and u renders it as a functor into
C.
When an object satisﬁes a universal property, then this property characterizes the object uniquely:
as a result, universal properties are powerful constructions, telling us that for certain questions,
there can be only one possible answer. Characterizing an object by a universal property abstracts
away from contextually irrelevant details (for example, the particular elements making up a set),
and crystallizes its essence.
The uniqueness of universal morphisms is formalized by the following proposition.
Proposition 2.3.36 (Universal constructions are unique up to unique isomorphism). Suppose
u : X Ñ FU and u1 : X Ñ FU1 are both universal morphisms from X : C to F : C Ñ D. Then
there is a unique isomorphism i : U Ñ U1.
To prove this, we need to know that functors preserve isomorphisms.
Proposition 2.3.37. If F : C Ñ D is a functor and f : x Ñ y is an isomorphism in C, then
Ff : Fx Ñ Fy is an isomorphism in D.
Proof. For f to be an isomorphism, there must be a morphism f´1 : y Ñ x such that f´1 ˝f “ idx
and f ˝ f´1 “ idy. We have idFx “ Fpidxq “ Fpf´1 ˝ fq “ Ff´1 ˝ Ff, where the ﬁrst and third
equations hold by the functoriality of F and the second equation holds ex hypothesi. Similarly,
idFy “ Fpidyq “ Fpf ˝ f´1q “ Ff ˝ Ff´1. Therefore Ff´1 is both a right and left inverse for
Ff, and so Ff is an isomorphism.
Proof of Proposition 2.3.36. Since u1 is a morphism from X to F, the universal property of u says
that there exists a unique morphism i : U Ñ U1 such that u1 “ Fi ˝ u. Similarly, the universal
property of u1 stipulates that there exists a unique morphism i1 : U1 Ñ U such that u “ Fi1 ˝ u1.
We can substitute the latter into the former and the former into the latter:
u1 “ X
u1
ÝÑ FU1 i1
ÝÑ FU
Fi
ÝÑ FU1
“ X
u1
ÝÑ FU1 Fpi˝i1q
ÝÝÝÝÑ FU1
“ X
u1
ÝÑ FU1 F idU1
ÝÝÝÝÑ FU1
u “ X
uÝÑ FU
Fi
ÝÑ FU1 Fi1
ÝÝÑ FU
“ X
uÝÑ FU
Fpi1˝iq
ÝÝÝÝÑ FU
“ X
uÝÑ FU
F idU
ÝÝÝÑ FU
and since functors preserve isomorphism, we have i ˝ i1 “ idU1 and i1 ˝ i “ idU. Therefore, i is an
isomorphism which is unique by deﬁnition.
45

2.3.3. Limits and colimits: mapping in to and out of diagrams
Many of the universal constructions above2 fall into their own general pattern, in which a diagram
of objects and morphisms is speciﬁed, and a universal morphism is produced which encodes the
data of mapping into or out of that diagram, in a suﬃciently parsimonious way that any other
way of mapping into or out of the diagram factors through it. In the case of the (co)product, the
diagram is simple: simply a pair of objects, with no morphisms between them. In the case of the
(co)equalizer, the diagram is a little more complex, being a ‘fork’ of the form 1
2
α
β
. We
can generalize these examples further, to consider the most parsimonious ways of mapping into
or out of diagrams of arbitrary shape: these universal constructions are called colimits and limits
respectively, and to formalize them, we need to deﬁne what it means to map into or out of a diagram.
For this purpose, we use the following notion of cone over a diagram.
Deﬁnition 2.3.38. A cone over the J-shaped diagram D in C is a natural transformation ∆c ñ D
for a given object c : C which we call its apex. Dually, a cocone under D with apex c is a natural
transformation D ñ ∆c. We say that J is the shape of the cone.
With this deﬁnition to hand, the notions of limit and colimit are easy to deﬁne.
Deﬁnition 2.3.39. A limit is a universal cone, and a colimit is a universal cocone. More explicitly,
if D is a J-shaped diagram in C, then the limit of D is a universal morphism from the constant
diagram functor functor ∆: C Ñ CJ to D (considered as an object of the functor category), and
the colimit of D is a universal morphism from D to ∆; alternatively, a colimit in C is a limit in C op.
In both cases, the apex of the cone is the universal object of the construction, which in the case of
the limit of D we denote by lim D, and in the case of the colimit, colim D.
Note that we will often say ‘(co)limit’ to refer to the apex of the universal (co)cone, even though
the (co)limit is properly the whole universal construction. We are entitled to say “the (co)limit”
thanks to the uniqueness of universal constructions.
We will often denote a universal cone by proj and call its component morphisms projections;
dually, we will often denote a universal cocone by inj and call its morphisms injections.
Example 2.3.40. We can now exemplify the pattern of the limiting examples above. We will draw
diagrams to depict the shape categories, with each symbol ‚ indicating a distinct object and each
arrow Ñ indicating a distinct non-identity morphism.
1. A coproduct is a colimit of shape
␣
‚
‚
(
;
2. a product is a limit of shape
␣
‚
‚
(
;
2Products, coproducts, equalizers, and coequalizers.
46

3. an equalizer is a limit of shape
␣
‚
‚
(
; and
4. a coequalizer is a colimit of shape
␣
‚
‚
(
.
Of course, these are not the only possible shapes of limits and colimits. Some others will be
particularly important, too.
Example 2.3.41. Let 0 denote the category with no objects or morphisms. A limit of shape 0 is
known as a terminal object. This is an object 1 such that, for every object X, there is a unique
morphism ! : X Ñ 1. The terminal object in Set is a singleton set t˚u.
Dually, a colimit of shape 0 is known as an initial object: an object 0 such that, for every object
X, there is a unique morphism
!
: 0 Ñ X. The initial object in Set is the empty set.
Remark 2.3.42. In Remark 2.2.7, we noted that morphisms 1 Ñ A in Set correspond to elements
of A. In general categories C with a terminal object, one sometimes calls morphisms out of the
terminal object global elements. The word ‘global’ emphasizes the special position of the terminal
object in a category, which has a unique view of every object.
Example 2.3.43. A pullback is a limit of shape
␣
‚
‚
‚
(
. That is, given morphisms
f : A Ñ X and g : B Ñ X, their pullback is an object P and morphisms projA : P Ñ A and
projB : P Ñ B making the following diagram commute
P
B
A
X
g
f
projB
projA
{
in the universal sense that, for any object Q and morphisms πA : Q Ñ A and πB : Q Ñ B such
that f ˝ πA “ g ˝ πB, then there is a unique morphism u : Q Ñ P such that πA “ projA ˝ u and
πB “ projB ˝ u. We indicate a pullback square using the symbol { as in the diagram above, and will
variously denote the limiting object P by A ˆX B, f˚B, or g˚A, depending on the context.
The interpretation of the pullback is something like a generalized equation: in the category Set,
the pullback A ˆX B is the subset of the product A ˆ B consisting of elements pa, bq for which
fpaq “ gpbq. Alternatively, it can be understood as a kind of generalized intersection: given two
objects A and B and “ways of assigning them properties in X” f and g, the pullback A ˆX B is
the generalized intersection of A and B according to these X-properties. In fact, we already saw
this latter interpretation in Example 2.3.24, where we exhibited an intersection as an equalizer;
now we can see that that equalizer was ‘secretly’ a pullback.
Remark 2.3.44. Dually, a colimit of shape
␣
‚
‚
‚
(
is known as a pushout. Whereas
a pullback has an interpretation as a subobject of a product, a pushout has an interpration as a
quotient of a coproduct. In this work, we will make far more use of pullbacks than pushouts.
47

The observation that pullbacks can be interpreted as subobjects of products (and dually that
pushouts can be interpreted as quotients of coproducts) is a consequence of the more general result
that all limits can be expressed using products and equalizers (and hence dually that colimits can
be expressed using coproducts and coequalizers).
Proposition 2.3.45. Let D : J Ñ C be a diagram in C, and suppose the products ś
j:J0 Dpjq and
ś
f:J1 Dpcod fq exist. Then, if it exists, the equalizer of
ś
j:J0 Dpjq
ś
f:J1 Dpcod fq
ś
f:J1pDf ˝ projdom fq
ś
f:J1 projcod f
is the limit of D.
Proof sketch. Observe that the equalizer of the diagram above is an object L such that, for every
morphism f : j Ñ j1 in J, the diagram
L
Dj
Dj1
projj
projj1
Df
commutes, and such that any cone over D factors through it. This is precisely the universal property
of the limit of D, and so by Proposition 2.3.36, pL, projq is the limit.
Remark 2.3.46. As we indicated above, a dual result holds expressing colimits using coequalizers
and coproducts. Because results obtained for limits in C will hold for colimits in C op, we will
henceforth not always give explicit dualizations.
2.3.3.1. Functoriality of taking limits
In the statement of Proposition 2.3.45, we used the fact that taking products extends to morphisms,
too: a fact that was exempliﬁed concretely in Example 2.2.4, and which follows from the fact that a
pair of morphisms in C is equivalently a morphism in C ˆ C. We then saw in Remark 2.3.30 that
C ˆ C – C2. By letting the exponent again vary, the functoriality of taking products generalizes to
the functoriality of taking limits, as long as C has all limits of the relevant shape.
Proposition 2.3.47 (Taking limits is functorial). Suppose C has all limits of shape J (i.e., for any
diagram D : J Ñ C, the limit lim D exists in C). Then lim deﬁnes a functor CatpJ, Cq Ñ C.
48

Proof. We only need to check the assignment is well-deﬁned on morphisms and functorial. Suppose
D and D1 are two diagrams J Ñ C with corresponding limiting cones u : ∆lim D ñ D and
u1 : ∆lim D1 ñ D1, and suppose δ : D ñ D1 is a natural transformation. Observe that the
composite natural transformation ∆lim D
uùñ D
δùñ D1 is a cone on D1, and that cones on D1 are in
bijection with morphisms in C into the apex object lim D1. Therefore, by the universal property of
the limit, there is a unique morphism d : lim D Ñ lim D1 such that δ ˝ u “ u1 ˝ ∆d. This situation
is summarized by the commutativity of the following diagram, where the dashed arrow indicates
the uniqueness of ∆d:
∆lim D
∆lim D1
D
D1
∆d
δ
u
u1
We deﬁne the action of the functor lim : CatpJ, Cq Ñ C on the natural transformation δ by this
unique morphism d, setting lim δ :“ d : lim D Ñ lim D1.
It therefore only remains to check that this assignment is functorial (i.e., that it preserves identities
and composites). To see that lim preserves identities, just take δ “ idD in the situation above;
clearly, by the uniqueness of d, we must have lim idD “ idlim D. Now suppose δ1 : D1 Ñ D2 is
another natural transformation. To see that limpδ1 ˝ δq “ lim δ1 ˝ lim δ, consider the pasting of the
associated diagrams:
∆lim D
∆lim D1
∆lim D2
D
D1
D2
∆d
δ
u
u1
∆d1
u2
δ1
∆d1d
We have limpδ1 ˝ δq “ d1d, which is unique by deﬁnition. Therefore we must have d1d “ d1 ˝ d “
limpδ1q ˝ limpδq, and hence limpδ1 ˝ δq “ limpδ1q ˝ limpδq as required.
2.3.3.2. (Co)limits as adjoints
Since taking limits is functorial, it makes sense to ask if the functor lim has an adjoint, and indeed
it does, in a familiar form.
Proposition 2.3.48. The functor lim : CJ Ñ C is right adjoint to the constant diagram functor
functor ∆: C Ñ CJ, i.e. ∆% lim.
Proof. We need to show that CJp∆c, Dq – Cpc, lim Dq naturally in c : C and D : J Ñ C. It is
suﬃcient to demonstrate naturality in each argument separately, by the universal property of
49

the product in Cat. We have already established naturality in c : C in Lemma 2.3.51 and shown
that taking limits is functorial (Proposition 2.3.47). So it only remains to show that this extends
to naturality in D : J Ñ C, which requires the commutativity of the following diagram for any
δ : D Ñ D1, where we write αD for the isomorphism Cpc, lim Dq „
ÝÑ CJp∆c, Dq:
Cpc, lim Dq
CJp∆c, Dq
Cpc, lim D1q
CJp∆c, D1q
αD
αD1
CJp∆c,δq
Cpc,lim δq
Chasing a morphism β : c Ñ lim D around this diagram, we ﬁnd that its commutativity amounts
to the commutativity of the following diagram of cones for all ϕ : i Ñ j in J, where by deﬁnition
αDpβqi “ πi ˝ β and αD1plim δ ˝ βqi “ π1
i ˝ lim δ ˝ β:
Di
D1i
c
lim D
lim D1
Dj
D1j
Dϕ
δi
D1ϕ
δj
πi
πj
π1
i
π1
j
lim δ
β
πi˝β
πj˝β
This diagram commutes by deﬁnition, so the isomorphism is natural in D, which therefore
establishes the desired adjunction.
Remark 2.3.49. Dually, if all colimits of shape J exist in C, then colim is left adjoint to ∆.
Later, we will see that every adjoint functor exhibits a universal property (Propositions 2.4.16
and 2.4.23, results that we’ve already seen exempliﬁed in Example 2.3.35), and this therefore gives
us another perspective on the universality of limits.
2.3.3.3. Hom preserves limits
We end this section with a useful result on the interaction between the covariant hom functors
Cpc, ´q : C Ñ Set and taking limits.
Proposition 2.3.50 (Hom functor preserves limits). Suppose D : J Ñ C is a diagram in the
category C. There is an isomorphism Cpc, lim Dq – lim Cpc, Dp´qq which is natural in c : C.
To prove this proposition, it helps to have the following lemma, which establishes a natural
isomorphism between the set of morphisms into a limit and the set of cones on the corresponding
diagram.
Lemma 2.3.51. Cpc, lim Dq – CJp∆c, Dq, naturally in c : C.
50

Proof. For a given c : C, the isomorphism Cpc, lim Dq – CJp∆c, Dq follows directly from the
universal property of the limit: morphisms from c into the limiting object lim D are in bijection
with cones ∆c ñ D. So it only remains to show that this isomorphism is natural in c : C. Writing
α : Cp´, lim Dq ñ CJp∆p´q, Dq for the natural transformation that takes each morphism into the
limit to the corresponding cone on D, naturality amounts to the commutativity of the following
square for each f : c1 Ñ c in C:
Cpc, lim Dq
CJp∆c, Dq
Cpc1, lim Dq
CJp∆c1, Dq
Cpf,lim Dq
CJp∆f,Dq
αc
αc1
Commutativity of this naturality square witnesses the fact that, given a morphism g : c Ñ lim D,
you can either take the corresponding cone αcpgq and pull it back along ∆f (at its apex) to obtain
the cone αcpgq ˝ ∆f, or you can form the composite morphism g ˝ f and take its cone αc1pg ˝ fq,
and you’ll have the same cone: αcpgq ˝ ∆f “ αc1pg ˝ fq. This is illustrated by the commutativity
of the following diagram, which shows fragments of the limiting cone denoted π, the cone αcpgq,
and the cone αc1pg ˝ fq, for a morphism ϕ : i Ñ j in J:
Di
c1
c
lim D
Dj
Dϕ
πi
πj
g
f
αcpgqi
αc1pg˝fqi
αcpgqj
αc1pg˝fqj
By the universal property of the limit, we must have αc1pg ˝ fqi “ αcpgqi ˝ f “ πi ˝ g ˝ f naturally
in i, and hence αc1pg ˝ fq “ αcpgq ˝ ∆f.
Proof of Proposition 2.3.50. By Lemma 2.3.51, we have a natural isomorphism CJp∆c, Dq –
lim Cpc, Dp´qq, so it suﬃces to establish a natural isomorphism CJp∆c, Dq – lim Cpc, Dp´qq.
This says that cones on D with apex c are isomorphic to the limit of Cpc, Dp´qq : J Ñ Set,
naturally in c. First, note that this limiting cone in Set is constituted by a family of functions
tpi : lim Cpc, Dp´qq Ñ Cpc, Diqui:J, as in the following commuting diagram:
Cpc, Diq
lim Cpc, Dp´qq
Cpc, Djq
Cpc,Dϕq
pi
pj
51

Next, note there is a bijection between cones ∆c ñ D on D in C with apex c, as in the commuting
diagram below-left, and cones ∆1 ñ Cpc, Dp´qq in Set, as in the commuting diagram below-right.
Di
c
Dj
βi
Dϕ
βj
Cpc, Diq
1
Cpc, Djq
Cpc,Dϕq
βi
βj
By the univeral property of the limit, any cone tβiu as on the right factors uniquely through
lim Cpc, Dp´qq, as in the following commuting diagram. Similarly, any element β of lim Cpc, Dp´qq
induces a corresponding cone tpipβqu, by composition with the limiting cone p. To see that this
correspondence is an isomorphism, observe that the element of the set lim Cpc, Dp´qq assigned to
the cone tpipβqu must be exactly β, since the universal property of lim Cpc, Dp´qq ensures that β
is uniquely determined.
Cpc, Diq
˚
lim Cpc, Dp´qq
Cpc, Djq
Cpc,Dϕq
pi
pj
β
βi
βj
It only remains to check that this correspondence is natural in c, so suppose f is any morphism
c1 Ñ c in C. If we write p´ : lim Cpc, Dp´qq Ñ CJp∆c, Dq to denote the function β ÞÑ tpipβqu,
and p1
´ to denote the corresponding function for c1, naturality requires the following square to
commute:
lim Cpc, Dp´qq
CJp∆c, Dq
lim Cpc1, Dp´qq
CJp∆c1, Dq
p´
p1
´
CJp∆f,Dq
lim Cpf,Dp´qq
The commutativity of this square in turn corresponds to the commutativity of the following diagram
52

in Set, for any cone β:
Cpc, Diq
Cpc1, Diq
1
lim Cpc, Dp´qq
lim Cpc1, Dp´qq
Cpc, Djq
Cpc1, Djq
Cpf,Diq
Cpf,Djq
Cpc1,Dϕq
p1
i
p1
j
Cpc,Dϕq
lim Cpf,Dp´qq
pi
pj
β
By the correspondence between cones ∆c ñ D in C and cones ∆1 ñ Cpc, Dp´qq in Set, this
diagram commutes if and only if the following diagram commutes:
Di
c1
c
Dj
βi
Dϕ
βj
f
βi˝f
βj˝f
This diagram commutes by the deﬁnition of β and of the composites tβi ˝ fu, thereby establishing
the naturality of the isomorphism lim Cpc, Dp´qq – CJp∆c, Dq. Since we also have a natural
isomorphism CJp∆c, Dq – Cpc, lim Dq, we have established the result.
The preceding proof established more than just the hom functor’s preservation of limits: it gave
us another useful natural isomorphism, this time betwen the set of cones ∆c ñ D in C and the set
of cones ∆1 ñ Cpc, Dq on the diagram Cpc, Dq : J Ñ Set with apex the terminal set 1.
Corollary 2.3.52. There is an isomorphism CJp∆c, Dq – SetJp∆1, Cpc, Dqq, natural in c : C.
Remark 2.3.53. Since limits in C op are colimits in C, Proposition 2.3.50 implies that the
contravariant hom functors Cp´, cq turn limits into colimits; i.e. Cplim D, cq – colim CpDp´q, cq.
2.3.4. Closed categories and exponential objects
A distinguishing feature of adaptive systems such as the brain is that they contain processes
which themselves control other processes, and so it is useful to be able to formalize this situation
compositionally. When a category contains objects which themselves represent the morphisms of
the category, we say that the category is closed: in such categories, we may have processes whose
outputs are again processes, and we may think of the latter as controlled by the former.
A basic instance of this mathematical situation is found amidst the natural numbers, where
repeated multiplication coincides with exponentiation, as in 2 ˆ 2 ˆ 2 “ 23. If we think of numbers
53

as sets of the corresponding size, and let 23 denote the set of functions 3 Ñ 2, then it is not hard
to see that there are 8 such distinct functions. If we generalize this situation from numbers to
arbitrary objects, and from functions to morphisms, we obtain the following general deﬁnition of
exponentiation.
Deﬁnition 2.3.54. Let ˆ denote the product in a category C. When there is an object e : C
such that Cpx, eq – Cpx ˆ y, zq naturally in x, we say that e is an exponential object and denote
it by zy. The image of idzy under the isomorphism is called the evaluation map and is written
evy,z : zy ˆ y Ñ z.
Example 2.3.55. In Set, given sets A and B, the exponential object BA is the set of functions
A Ñ B. Given a function f : A Ñ B, the evaluation map evB,A acts by applying f to elements of
A: i.e., evB,Apf, aq “ fpaq.
Typically, we are most interested in situations where every pair of objects is naturally
exponentiable, which induces the following adjunction, formalizing the idea that exponentiation is
repeated multiplication.
Proposition 2.3.56. When the isomorphism Cpxˆy, zq – Cpx, zyq is additionally natural in z, we
obtain an adjunction p´q ˆ y % p´qy called the product-exponential adjunction, and this uniquely
determines a functor C op ˆ C Ñ C : py, zq ÞÑ zy that we call the internal hom for C.
Proof. That the natural isomorphism induces an adjunction is immediate from Proposition 2.2.26;
the counit of this adjunction is the family of evaluation maps ev : p´qy ˆ y ñ idC. The uniqueness
of the internal hom follows from the uniqueness of adjoint functors (which we will establish in
Corollary 2.4.18).
Deﬁnition 2.3.57. A category C in which every pair of objects has a product is called Cartesian.
A Cartesian category C with a corresponding internal hom is called Cartesian closed.
Example 2.3.58. We’ve already seen that Set is Cartesian closed. So is Cat: the internal hom CB
is the category of functors B Ñ C.
Example 2.3.59 (A non-example). The category Meas of measurable spaces with measurable
functions between them is Cartesian but not Cartesian closed: the evaluation function is not always
measurable.
It is not hard to prove the following result, which says that Cartesian closed categories can
“reason about themselves”.
Proposition 2.3.60. A Cartesian closed category is enriched in itself.
54

This ‘internalization’ is witnessed by the hom functors, which in the case of a Cartesian closed
enriching category E become E-functors.
Proposition 2.3.61. Suppose C is an E-category where E is Cartesian closed. Then the hom
functor Cp´, “q is an E-functor C op ˆC Ñ E. On objects pc, dq, the hom functor returns the object
Cpc, dq in E of morphisms c Ñ d. Then, for each quadruple pb, c, a, dq of objects in C, we deﬁne an
E-morphism C oppb, aq ˆ Cpc, dq Ñ E
`
Cpb, cq, Cpa, dq
˘
as the image of the composite
`
Cpa, bq ˆ Cpc, dq
˘
ˆ Cpb, cq αÝÑ Cpa, bq ˆ
`
Cpc, dq ˆ Cpb, cq
˘
¨ ¨ ¨
¨ ¨ ¨
Cpa,bqˆ˝b,c,d
ÝÝÝÝÝÝÝÝÑ Cpa, bq ˆ Cpb, dq σÝÑ Cpb, dq ˆ Cpa, bq
˝a,b,d
ÝÝÝÑ Cpa, dq
under the product-exponential isomorphism
E
`
Cpa, bq ˆ Cpc, dq, Cpa, dqCpb,cq˘
– E
´`
Cpa, bq ˆ Cpc, dq
˘
ˆ Cpb, cq, Cpa, dq
¯
where α is the associativty of the product and σ is its symmetry X ˆ Y – Y ˆ X, and where we
have used that C oppb, aq “ Cpa, bq.
Remark 2.3.62. The role of the symmetry here is testament to the fact that we can read a composite
morphism g ˝ f as either “g after f” or “f before g”.
Proof sketch. To give an E-functor (Deﬁnition 2.2.9) is to give a function on objects and a family
of E-morphisms (corresponding to the hom objects of C) such that identities and composites are
preserved. We have given such a function and such a family in the statement of the proposition,
and so it remains to check the axioms: these follow by the unitality and associativity of composition
in an E-category (Deﬁnition 2.2.5).
When E is Cartesian closed, then as a corollary its hom functor Ep´, “q is an E-functor.
When a diagram commutes, every parallel path is equal when interpreted as a morphism. If a
diagram commutes up to some 2-cell or 2-cells, then parallel paths can be transformed into each
other using the 2-cell(s). Much categorical reasoning therefore consists in using morphisms in the
base of enrichment to translate between diﬀerent hom objects; the simplest such of course being
pre- and post-composition. In the next section, we will see many explicit examples of this kind of
reasoning when we prove the Yoneda Lemma—which says that the hom objects contain all the data
of the category—but we have already seen examples of it above, when we considered adjunctions:
after all, adjunctions are families of isomorphisms between hom objects.
When a category is Cartesian closed, it is its own base of enrichment, and so one does not have
to move to an external perspective to reason categorically about it: one can do so using its ‘internal
language’. We have already seen a correspondence between the language of logic and that of sets,
55

in which we can think of elements of sets as witnesses to the proof of propositions represented
by those sets, and where logical operations such as conjunction and disjunction correspond to
operations on sets. This correspondence extends to Cartesian closed categories generally: universal
constructions such as those we have introduced above can be interpreted as encoding the logic of
the internal language.
More precisely, Cartesian closed categories are said to provide the semantics for intuitionistic type
theory: a higher-order logic in which propositions are generalized by ‘types’3. One can construct
a ‘syntactic’ category representing the logic of the type theory, and then interpret it functorially
in a Cartesian closed category. This correspondence is known as the Curry-Howard-Lambek
correspondence, which says that logical proofs correspond to morphisms in a Cartesian closed
category, and that such morphisms can equally be seen as representing the functions computed
by deterministic computer programs. (In general, the correspondence is an adjoint one: dually,
one can construct from a given category a ‘syntactic’ category encoding the logic of its internal
language.)
When a category moreover has (internal) dependent sums and products, then it can be interpreted
as a model of dependent type theory, in which types themselves may depend on values; for instance,
one might expect that the type of a weather forecast should depend on whether one is on land or at
sea. We will not say much more about dependent type theory, although we will make implicit use
of some of its ideas later in the thesis. Therefore, before moving on to the Yoneda Lemma, we will
say just enough to deﬁne the notion of dependent product ‘universally’, without reference to sets.
2.3.4.1. Dependent products
In Remark 2.3.20, we discussed products where the factors were indexed by an arbitrary set and
explained how they correspond to sets of generalized ‘dependent’ functions, where the codomain
type may vary with the input. In that case, we were restricted to considering products indexed by
sets, but with the machinery of limits at hand, we can ‘internalize’ the deﬁnition to other Cartesian
closed categories.
Deﬁnition 2.3.63. Suppose C is Cartesian closed and has all limits, and suppose p : E Ñ B is a
morphism in C. The dependent product of p along B is the pullback object ś
B p as in the diagram
ś
B p
EB
1
BB
idB
pB
{
3A type is something like a proposition in which we’re ‘allowed’ to distinguish between its witnesses, which we call
values of the given type.
56

where 1 is the terminal object, idB is the element picking the identity morphism B Ñ B, and pB is
the postcomposition morphism induced by the functoriality of exponentiation.
Remark 2.3.64. When p is the projection ř
b:B Pb Ñ B out of a dependent sum, we will write its
dependent product as ś
b:B Pb. Since a product B ˆ C is isomorphic to the dependent sum ř
b:B C,
note that this means we can alternatively write the exponential object CB as ś
b:B C.
To understand how Deﬁnition 2.3.63 generalizes Remark 2.3.20, we can interpret the former in
Set and see that the two constructions coincide. The set EB is the set of functions s : B Ñ E,
and pB acts by s ÞÑ p ˝ s. The indicated pullback therefore selects the subset of EB such that
p ˝ s “ idB. This is precisely the set of sections of p, which is in turn the dependent product of p in
Set.
Remark 2.3.65. Deﬁnition 2.3.63 is entirely internal to C: it depends only on structure that is
available within C itself, and not on ‘external’ structures (such as indexing sets) or knowledge
(such as knowledge of the make-up of the objects of C). It is epistemically parismonious: a purely
categorical deﬁnition, stated entirely in terms of universal constructions.
Remark 2.3.66. Under the Curry-Howard-Lambek correspondence, exponential objects represent
the propositions that one proposition implies another; in type theory, they represent the type
of functions from one type to another. As dependent exponential objects, dependent products
could therefore be seen as representing ‘dependent’ implications; as we have already seen, they do
represent the type of dependent functions. However, dependent products and sums have another
kind of logical interpretation: as quantiﬁers. That is, the logical proposition represented by ś
b:B Pb
is @b : B.Ppbq: an element of ś
b:B Pb is a proof that, for all b : B, the proposition Ppbq is satisﬁed.
Dually, the proposition represented by ř
b:B Pb is Db : B.Ppbq: an element of ř
b:B Pb is a pair
pb, xq of a witness to B and a witness x of the satisfaction of Ppbq.
2.4. The Yoneda Lemma: a human perspective
We end this chapter by introducing the fundamental theorem of category theory, the Yoneda
Lemma, which expresses mathematically the idea that to know how a thing is related to other
things is to know the identity of the thing itself. The notion of relational identity is recognized
throughout human endeavour. In linguistics, it underlies the observation of Firth [86] that “you
shall know a word by the company it keeps!”, which in turn is the foundation of distributional
semantics and thus much of contemporary natural language processing in machine learning. In
culture, it is illustrated by the ancient parable of the blind men and the elephant, in which the
identity of the creature is only known by stitching together evidence from many perspectives.
In society, it is reﬂected in the South African philosophy of ubuntu (meaning “I am because we
57

are”) and the M¯aori notion of whanaungatanga (in which personal identity is developed through
kinship), and the observation that “actions speak louder than words”. Finally, the Yoneda Lemma is
manifest in science, where our understanding of phenomena derives from the accumulation across
contexts of results and their interpretation and translation: no single individual understands the
totality of any subject, and no subject or phenomenon is understood in isolation.
2.4.1. Formalizing categorical reasoning via the Yoneda embedding
In §2.3.4, we saw how Cartesian closed categories allow us to internalize categorical reasoning. The
category Set is the archetypal Cartesian closed category, and constitutes the base of enrichment
for all locally small categories. The Yoneda embedding allows us to move from reasoning about
the objects in any given category C to reasoning about the morphisms between its hom sets: the
natural transformations between hom functors. In this context, the hom functors constitute special
examples of functors into the base of enrichment, which we call ‘presheaves’ (contravariantly) and
‘copresheaves’ (covariantly), and which can be thought of as C-shaped diagrams in Set.
Deﬁnition 2.4.1. Let C be a category. A presheaf on C is a functor C op Ñ Set. Dually, a copresheaf
is a functor C Ñ Set. The corresponding functor categories are the categories of (co)presheaves on
C.
Remark 2.4.2. In the enriched setting, when C is enriched in E, an E-presheaf is an E-functor
C op Ñ E and an E-copresheaf is an E-functor C Ñ E.
As a ﬁrst example of a presheaf, we have an alternative deﬁnition of the notion of directed graph.
Example 2.4.3. Let G denote the category of Example 2.1.5 containing two objects 0 and 1 and
two morphisms s, t : 0 Ñ 1. Then a directed graph is a presheaf on G.
This deﬁnition is justiﬁed by the following proposition.
Proposition 2.4.4. There is an equivalence of categories Graph – SetG op, where Graph is the
category of directed graphs introduced in Example 2.1.11.
Proof. To each graph G we can associate a presheaf G : G op Ñ Set by deﬁning Gp0q :“ G0,
Gp1q :“ G1, Gpsq :“ domG and Gptq :“ codG; and to each presheaf we can likewise associate a
graph, so that we have deﬁned a bijection on objects. It therefore only remains to show that there
is a bijection between graph homomorphisms and natural transformations accordingly: but this is
easy to see once we have observed that the graph homomorphism axioms are precisely the law of
naturality, as illustrated diagrammatically in (2.1).
58

Taking again a general perspective, the Yoneda embedding is the embedding of a category C into
its presheaf category, obtained by mapping c : C to the presheaf Cp´, cq; and there is of course a
dual ‘coYoneda’ embedding.
Remark 2.4.5. We say ‘embedding’ to mean a functor that is injective on objects and faithful
(injective on hom sets). The Yoneda embedding will turn out to be fully faithful, as a consequence
of the Yoneda lemma.
Owing to its importance, we make a formal deﬁnition of the Yoneda embedding.
Deﬁnition 2.4.6. Let C be a category. By applying the product-exponential adjunction in Cat to
the hom functor Cp´, “q : C op ˆ C Ñ Set, we obtain a functor よ: C Ñ SetC op : c ÞÑ Cp´, cq
of C into its presheaf category, and dually a functor
よ
: C op Ñ SetC : c ÞÑ Cpc, “q into the
copresheaf category. We call the former functor the Yoneda embedding and the latter the coYoneda
embedding. When C is an E-category and E is Cartesian closed, then the Yoneda embedding is
instead an E-functor C Ñ EC op (and likewise for the coYoneda embedding).
Remark 2.4.7. This abstract deﬁnition does not make explicit how よacts on morphisms. However,
we have already seen this action, when we ﬁrst exempliﬁed natural transformations in Example
2.2.17.
As we discussed in §2.3.4, much categorical reasoning corresponds to following morphisms
between hom objects, and often the reasoning is agnostic either to where one starts, or to where
one ends up. The Yoneda embedding witnesses such proofs as morphisms in the (co)presheaf
categories. As an example, consider the proof of Proposition 2.4.20 below: each step corresponds to
the application of a natural transformation.
Remark 2.4.8. It also so happens that every (co)presheaf category is very richly structured,
inheriting its structure from the base of enrichment. For example, this means that the presheaf
category SetC op has all limits, is Cartesian closed, has a subobject classiﬁer, and dependent sums
and products, even when C has none of these. (Interestingly, this means that the category of directed
graphs is accordingly richly structured, being a presheaf category by Proposition 2.4.4.) As a result,
(co)presheaf categories are very powerful places to do categorical reasoning.
2.4.2. Knowing a thing by its relationships
The Yoneda lemma says that every (co)presheaf on C is determined by “how it looks from C”. Since
under the (co)Yoneda embedding every object gives rise to a (co)presheaf, a corollary of the Yoneda
lemma is that every object can be identiﬁed by its relationships.
59

Remark 2.4.9. If the base of enrichment of a category is Cartesian closed, then one can prove an
analogous enriched version of the Yoneda lemma. We will only prove the standard Set-enriched
case here.
We will also only prove the Yoneda lemma for presheaves; there is of course a dual coYoneda
lemma for copresheaves, which follows simply by swapping C and C op.
Proposition 2.4.10 (Yoneda lemma). Let F : C op Ñ Set be a presheaf on C. Then for each c : C,
there is an isomorphism Fc – SetC oppCp´, cq, Fq. Moreover, this isomorphism is natural in both
F : C op Ñ Set and c : C.
Proof. We ﬁrst deﬁne a mapping γ : Fc Ñ SetC oppCp´, cq, Fq as follows. Given h : Fc, we
deﬁne the natural transformation γphq : Cp´, cq ñ F to have components γphqb : Cpb, cq Ñ
Fb : f ÞÑ Ffphq; note that since h : Fc and f : b Ñ c, we have Ff : Fc Ñ Fb and hence
Ffphq : Fb. To check that this deﬁnition makes γphq into a natural transformation, suppose
g : a Ñ b. We need to check Fg ˝ γphqb “ γphqa ˝ Cpg, cq. Since Cpg, cqpfq “ f ˝ g, this means
verifying Fg ˝ Ffphq “ Fpf ˝ gqphq. But F is a contravariant functor, so Fpf ˝ gq “ Fg ˝ Ff,
thereby establishing naturality.
Conversely, we deﬁne a mapping γ1 : SetC oppCp´, cq, Fq Ñ Fc as follows. Suppose α is a
natural transformation Cp´, cq ñ F, so that its component at c is the function αc : Cpc, cq Ñ Fc.
We deﬁne γ1pαq :“ αcpidcq.
Next, we need to establish that γ and γ1 are mutually inverse. First, we check that γ1 ˝ γ “ idFc.
Given h : Fc, we have
γ1pγphqq “ γphqcpidcq “ Fpidcqphq “ idFcphq “ h
as required. We now check that γ ˝ γ1 “ idSetC oppCp´,cq,Fq. Given α : Cp´, cq ñ F, we have
γ1pαq “ αcpidcq by deﬁnition. Hence γpγ1pαqq : Cp´, cq ñ F has components γpγ1pαqqb :
Cpb, cq Ñ Fb which act by f ÞÑ Ffpαcpidcqq. So we need to show that Ffpαcpidcqq “ αbpfq.
This follows directly from the naturality of α. The commutativity of the naturality square on the
left in particular holds at idc : Cpc, cq as on the right:
Cpc, cq
Fc
Cpb, cq
Fb
αc
Ff
Cpf,cq
αb
idc
αcpidcq
f
αbpfq “ Ffpαcpidcqq
Note that Cpf, cqpidcq “ idc ˝f “ f. This establishes that γ ˝ γ1 “ idSetC oppCp´,cq,Fq, and since
γ1 ˝ γ “ idFc, we have Fc – SetC oppCp´, cq, Fq.
60

It remains to verify that this isomorphism is natural in F and c. Suppose ϕ : F ñ F 1 is a natural
transformation, and write γ1
Fc for the function γ1 deﬁned above, and γ1
F 1c for the corresponding
function for F 1. Naturality in F means that the diagram on the left below commutes, which we
can see by chasing the natural transformation α as on the right:
SetC oppCp´, cq, Fq
Fc
SetC oppCp´, cq, F 1q
F 1c
SetC oppCp´,cq,ϕq
γ1
F c
γ1
F 1c
ϕc
α
γ1
Fcpαq
ϕ ˝ α
γ1
F 1cpϕ ˝ αq “ ϕc ˝ γ1
Fcpαq
Since γ1
Fcpαqc :“ αcpidcq and γ1
F 1cpϕ˝αqc :“ ϕc ˝αcpidcq, the equation γ1
F 1cpϕ˝αq “ ϕc ˝γ1
Fcpαq
holds by deﬁnition, thereby establishing naturality in F. Finally, suppose f : b Ñ c in C, and write
γFc for the function γ deﬁned above and γFb for the corresponding function for b : C. Naturality
in c means the commutativity of the following diagram:
Fc
SetC oppCp´, cq, Fq
Fb
SetC oppCp´, bq, Fq
SetC oppCp´,fq,Fq
γF c
γF b
Ff
Suppose h : Fc. The component of γFcphq at a : C is the function γFcphqa : Cpa, cq Ñ Fa
deﬁned by g ÞÑ Fgphq. The component of SetC oppCp´, fq, Fq ˝ γFcphq at a : C is thus the
function γcphqa ˝ Cpa, fq : Cpa, bq Ñ Fa taking g : a Ñ b to Fpf ˝ gqphq. On the other hand,
the component of γFbpFfphqq at a : C is the function γFbpFfphqqa : Cpa, bq Ñ Fa taking g
to FgpFfphqq. Since F is a contravariant functor, we have Fpf ˝ gqphq “ FgpFfphqq. This
establishes the commutativity of the naturality square, and thus naturality in c as well as F.
The identiﬁcation of an object with its collection of hom sets is formalized by the following
corollary.
Corollary 2.4.11 (Representables are unique up to isomorphism). Suppose there is an isomorphism
of presheaves Cp´, aq – Cp´, bq. Then a – b in C.
This corollary follows from the next one, which expresses that the image of the Yoneda embedding
is isomorphic with C itself.
Corollary 2.4.12. The Yoneda embedding is fully faithful.
61

Proof. The Yoneda embedding deﬁnes a family of functions on the hom sets of C:
よb,c : Cpb, cq Ñ SetC oppCp´, bq, Cp´, cqq
f ÞÑ Cp´, fq
By the Yoneda lemma, we immediately have SetC oppCp´, bq, Cp´, cqq – Cpb, cq, which is the
required isomorphism of hom sets.
Next, we have the following fact, that fully faithful functors transport isomorphisms in their
codomain to their domain (they ‘reﬂect’ them).
Proposition 2.4.13 (Fully faithful functors reﬂect isomorphisms). Suppose F : C Ñ D is a fully
faithful functor. If f : a Ñ b is a morphism in C such that Ff is an isomorphism in D, then f is an
isomorphism in C.
Proof. Ff : Fa Ñ Fb being an isomorphism means that there is a morphism g1 : Fb Ñ Fa in D
such that g1 ˝ Ff “ idFa and Ff ˝ g1 “ idFb. By the functoriality of F, we have idFa “ F ida and
idFb “ F idb. Hence g1 ˝Ff “ F ida and Ff ˝g1 “ F idb. Since F is isomorphic on hom sets, there
is a unique g : b Ñ a such that g1 “ Fg. Hence Fg ˝ Ff “ F ida and Ff ˝ Fg “ F idb. By the
functoriality of F, we have Fg ˝Ff “ Fpg ˝fq and Ff ˝Fg “ Fpf ˝gq. Hence Fpg ˝fq “ F ida
and Fpf ˝ gq “ F idb. Finally, since F is isomorphic on hom sets, we must have g ˝ f “ ida and
f ˝ g “ idb, and hence f is an isomorphism in C.
And this gives us the proof we seek:
Proof of Corollary 2.4.11. Since the Yoneda embedding is fully faithful (Corollary 2.4.12), it reﬂects
isomorphisms by Proposition 2.4.13.
Presheaves in the image of the Yoneda embedding consequently play a special role in category
theory: to show that an arbitrary presheaf F is isomorphic to Cp´, cq is to identify it with the object
c itself, and in this case, we can say that F is represented by c. We therefore make the following
deﬁnition.
Deﬁnition 2.4.14. Suppose F is a presheaf on C. We say that it is representable if there is a natural
isomorphism F – Cp´, cq for some object c : C which we call its representing object; we call the
natural isomorphism Cp´, cq ñ F its representation. Dually, if F is instead a copresheaf, we call
it corepresentable if there is a natural isomorphism F – Cpc, “q, with c being the corepresenting
object; we call the natural isomorphism Cpc, “q ñ F its corepresentation.
Remark 2.4.15. Corepresentable copresheaves will play an important role later in this thesis: their
coproducts are called polynomial functors (§3.5), and these will be used to formalize the interfaces
of interacting adaptive systems.
62

Via the uniqueness of representables, the Yoneda lemma underlies universal constructions, since
knowing the morphisms into or out of an object is enough to identify that object. The deﬁnition of
a limit, notably, is the statement that morphisms into it correspond to morphisms into a diagram;
and this in turn is equivalently the statement that lim is right adjoint to ∆. Indeed, adjointness
is itself a certain kind of representability: the deﬁnition of adjoint functor (2.2.24) is precisely a
natural characterization of morphisms into and out of objects, as related by the adjunction!
Proposition 2.4.16 (Adjoints are representable). Suppose R : D Ñ C is right adjoint to L. Then
for every d : D, the presheaf DpL´, dq : C op Ñ Set is represented by the object Rd : C. Dually,
the copresheaf Cpc, R´q : D Ñ Set is corepresented by the object Lc : D.
Proof. Since L $ R, we have an isomorphism DpLc, dq – Cpc, Rdq natural in c and d. Therefore
in particular we have a natural isomorphism of presheaves Cp´, Rdq ñ DpL´, dq and a natural
isomorphism of copresheaves DpLc, ´q ñ Cpc, R´q; the former is a representation and the latter
a corepresentation.
From this, we can formalize the representability of limits and colimits.
Corollary 2.4.17 (Limits are representations). Suppose D : J Ñ C is a diagram in C. A limit
of D is a representation of CJp∆p´q, Dq : C op Ñ Set, or equivalently of SetJp∆1, Cp´, Dqq.
Dually, a colimit of D is a corepresentation of CJpD, ∆p´qq : C Ñ Set, or equivalently of
SetJp∆1, CpD, ´qq.
Proof. If C has all limits of shape J, then this follows directly from the facts that lim is right adjoint
to ∆(Proposition 2.3.48) and that adjoints are representable (Proposition 2.4.16); the dual result
follows similarly from the fact that colim is left adjoint to ∆.
Otherwise, the limit case follows immediately from Lemma 2.3.51 (or equivalently Corollary
2.3.52) and the deﬁnition of representation (2.4.14); the colimit case is formally dual.
Accordingly, we recover the uniqueness of universal constructions.
Corollary 2.4.18. Adjoint functors are unique up to unique isomorphism.
Corollary 2.4.19. Limits and colimits are unique up to unique isomorphism.
Using these ideas, we obtain the following useful result relating limits and adjoint functors.
Proposition 2.4.20 (Right adjoints preserve limits). Suppose D : J Ñ D is a diagram in D and
L $ R : D Ñ C is an adjunction. Then R lim D – lim RD in C.
63

Proof. We have the following chain of natural isomorphisms:
Cpc, R lim Dq – DpLc, lim Dq
since R is right adjoint to L
– lim DpLc, Dq
since hom preserves limits
– lim Cpc, RDq
since R is right adjoint to L
– Cpc, lim RDq
since hom preserves limits
Since representables are unique up to isomorphism and we have established an isomorphism of
presheaves Cp´, R lim Dq – Cp´, lim RDq, we must have R lim D – lim RD in C.
Remark 2.4.21. There is of course a dual result that left adjoints preserve colimits.
Remark 2.4.22. One might speculate about the converse: is it the case that the preservation of
limits by a functor is enough to guarantee the existence of its left adjoint? The answer to this
question is, “under certain conditions” on the size and structure of the categories and functors
involved, and a positive answer is called an adjoint functor theorem. The “certain conditions” hold
quite generally, and so it is often suﬃcient just to check whether a functor preserves limits (or
colimits) to see that it is a right (or left) adjoint.
We end this chapter by closing the loop between universality and representability.
Proposition 2.4.23 (Universality of representability). Representable presheaves F : C op Ñ Set
correspond bijectively to universal morphisms from 1 : Set to F.
Proof. A representation of F is a choice of object c : C and a natural isomorphism υ : Cp´, cq ñ F.
We construct a bijection between the set of representations of F and the set of universal morphisms
from 1 to F. Therefore suppose given a representation υ : Cp´, cq ñ F of F; its component at c : C
is the isomorphism υc : Cpc, cq Ñ Fc. The Yoneda lemma assigns to υ an element γ1pυq : 1 Ñ Fc
satisfying γ1pυq “ υcpidcq. We now show that this element υcpidcq satisﬁes the universal property
that for all f : 1 Ñ Fb there exists a unique morphism h : b Ñ c in C such that f “ Fh ˝ υcpidcq.
Therefore let f be any such element 1 Ñ Fb. Since υ is a natural isomorphism, it has an inverse
component at b : C, denoted υ1
b : Fb Ñ Cpb, cq, and so we obtain by composition an element
h :“ 1
fÝÑ Fb
υ1
b
ÝÑ Cpb, cq of Cpb, cq. Such an element is precisely a morphism h : b Ñ c in C.
Consider now the following diagram:
1
Cpc, cq
Cpb, cq
Fc
Fb
idc
Cph,cq
υb
υc
Fh
υcpidcq
64

The triangle on the left commutes by deﬁnition and the square on the right commutes by the
naturality of υ, so that the whole diagram commutes. The composite morphism Cph, cq ˝ idc along
the top of the diagram picks out the element idc ˝h of Cpb, cq. By the unitality of composition, this
element is equal to h itself, so we can rewrite the diagram as follows:
1
Cpb, cq
Fc
Fb
υb
Fh
υcpidcq
h
Next, we can substitute the deﬁnition h :“ υ1
b ˝ f, and observe that υb ˝ υ1
b “ idFb (since υb is an
isomorphism with υ1
b its inverse):
1
Fb
Cpb, cq
Fc
Fb
υb
Fh
υcpidcq
f
υ1
b
The commutativity of this diagram means that f “ Fh˝υcpidcq. Moreover, since h “ υ1
b ˝f and υ1
b
is an isomorphism, h is unique for a given f. Therefore υcpidcq : 1 Ñ Fc is a universal morphism
from 1 to F.
Next, suppose given a universal morphism u : 1 Ñ Fc. The Yoneda lemmea associates to this
element a natural transformation γpuq whose component at b is the function γpuqb : Cpb, cq Ñ Fb
which acts by f ÞÑ Ffpuq. We need to show that this function is an isomorphism for every b : C, so
that γpuq : Cp´, cq ñ F is a natural isomorphism and hence F is represented by c. We therefore
need to deﬁne an inverse function ϕb : Fb Ñ Cpb, cq, which we do using the universal property
of u: for each element f : 1 Ñ Fb, we have a unique morphism h : b Ñ c such that f “ Fhpuq.
This unique h is an element of Cpb, cq, and so we can simply deﬁne ϕbpfq :“ h. The uniqueness
of h ensures that ϕb is an inverse of γpuqb: observe that γpuqb ˝ ϕb acts by f ÞÑ h ÞÑ Fhpuq and
f “ Fhpuq by deﬁnition; in the opposite direction, we necessarily have f ÞÑ Ffpuq ÞÑ f.
We have constructed mappings between the set of representations of F and universal morphisms
from 1 to F, so it remains to show that these mappings are mutually inverse. This again follows
directly from the Yoneda lemma: the mapping of representations to universal morphisms takes a
representation υ to the element γ1pυq induced by the Yoneda lemma; and the mapping of universal
morphisms to representations takes a universal morphism u to the natural transformation γpuq
induced by the Yoneda lemma. Since the functions γ and γ1 are mutually inverse, so must these
mappings be: γ ˝ γ1pυq “ υ and γ1 ˝ γpuq “ u.
65

Using the universality of representability, and the uniqueness of universal morphisms, and the
representability of limits and adjoints, we therefore obtain alternative proofs of the uniqueness of
those universal constructions.
66

3. Algebraic connectomics
In Chapter 2, we motivated applied category theory in the context of complex systems like brains
by its abilities to relate structure and function, to translate between models and frameworks, and
to distil phenomena to their essences. However, the focus in that chapter was on ‘one-dimensional’
morphisms, which can be understood as connecting one interface to another, with the composition
of 1-cells representing something like the ‘end-to-end’ composition of processes; although we
considered some higher-dimensional category theory, this was largely restricted to weakening
equalities and thus comparing morphisms.
Because systems can be placed ‘side-by-side’ as well as end-to-end, and because two systems
placed side by side may be nontrivially wired together, in this chapter we extend the higher-
dimensional categorical language accordingly, with a particular focus once more on the graphical
and diagrammatic representation of systems and processes. In line with the distinction made in
§2.2.3 between syntax and semantics, our treatment here of the syntax of wiring—of connectomics—is
largely ‘algebraic’. Later, in Chapter 5, we will see how our semantic focus will be ‘coalgebraic’.
We will begin therefore by introducing the graphical calculus of monoidal categories, which
allow us to depict and reason about sequential and parallel composition simultaneously. We follow
this with the formal underpinnings of the structure—to use the term from Chapter §2, a monoidal
structure is a ‘well-behaved’ tensor product—before explaining how monoidal categories relate
to the higher category theory of Chapter 2 using the notion of bicategory. We then make use
of the extra freedom aﬀorded by bicategories to consider parameterized systems via the Para
construction, with which we can model systems that not only act but also learn.
By this point, we will ﬁnd ourselves ready to apply our new toolkit, and so in §3.3, we use
functorial semantics to deﬁne a graphical algebra for neural circuits, revisiting our ﬁrst example
from Chapter 2. This involves a change of perspective from the graphical calculus with which we
begin the chapter: instead of using the composition of morphisms to encode the plugging-together
of systems at the same ‘scale’ or “level of hierarchy”, we use composition to encode the wiring of
circuits at one level into systems at a higher level. Although formally closely related to monoidal
categories, this ‘hierarchical’ perspective is strictly speaking multicategorical and allows morphisms’
domains to take very general shapes.
After this extended example, we return to algebra, explaining what makes monoidal categories
monoidal, and using the related concept of monad to explain how we think of them as algebraic;
67

monads will later prove to be of importance in categorical approaches to probability theory. Finally,
we end the chapter by introducing the richly structured category of polynomial functors Set Ñ Set,
which we will use in Chapter 5 both to formalize a wide variety of open dynamical systems as well
as to specify the shapes of those systems’ interfaces.
Excepting the extended example of §3.3, the content of this chapter is well known to category-
theoreticians. However, since it is not well known to mathematical scientists, we have again
endeavoured to supply detailed motivations for the concepts and results that we introduce.
3.1. Categories and calculi for process theories
In this section, we introduce an alternative way of depicting morphisms and their composites in
categories equipped with notions of both sequential and parallel composition. Such categories
are useful for representing processes in which information ﬂows: we formalize the processes
as morphisms, and consider the ﬂow as from domain to codomain, even when the categories
themselves are quite abstract and lack a notion of time with which to make sense of ‘ﬂow’. In
such contexts, the categories are often not only monoidal, but also copy-discard categories, since a
distinctive feature of classical information is that it can be copied and deleted. Monoidal categories
will therefore be important not only in depicting composite computations (as indicated in §2.1.1.3),
but also in depicting and manipulating the factorization of probabilistic models (as indicated in
§2.1.1.2).
3.1.1. String diagrams
Rather than beginning with the formal deﬁnition of “monoidal category”, we start with the
associated graphical calculus of string diagrams and its intuition.
Sequential and parallel composition
Diagrams in the graphical calculus depict morphisms
as boxes on strings: the strings are labelled with objects, and a string without a box on it can
be interpreted as an identity morphism. Sequential composition is represented by connecting
strings together, and parallel composition by placing diagrams adjacent to one another; sequential
composition distributes over parallel, and so we can of course compose parallel boxes in sequence.
Because monoidal structures are “well-behaved tensor products”, we will typically denote them
using the same symbols that we adopted in Chapter 2, with sequential composition denoted by ˝ and
parallel composition (tensor) denoted by b. Diagrams will be read in the direction of information
ﬂow, which will be either bottom-to-top or left-to-right; we will adopt the former convention in
this section.
68

In this way, c : X Ñ Y , idX : X Ñ X, d ˝ c : X
cÝÑ Y
dÝÑ Z, and f b g : X b Y Ñ A b B are
depicted respectively as:
c
X
Y
X
X
d
Z
c
X
f
X
A
g
Y
B
A monoidal structure comes with a monoidal unit, which we will also continue to call a tensor unit,
and which will be not be depicted in diagrams, but rather left implicit. (Alternatively, it is depicted as
the “empty diagram”.) This is justiﬁed, as we will see, by the requirement that I bX – X – X bI
naturally in X.
States and costates
In Remark 2.2.7, we called a morphism I Ñ X out of the tensor unit a
generalized element, but owing to the many roles they play, such morphisms go by many names.
When we think of X as representing a system, we will also call morphisms such morphisms states
of X. Dually, morphisms X Ñ I can be called costates, or sometimes eﬀects. When the unit object
is the terminal object (such as when the monoidal structure is given by the categorical product),
then these costates are trivial. In other categories, costates may be more eﬀectful, and so carry
more information: for example, in a category of vector spaces, states are vectors, costates are linear
functionals, and so the composite of a state with a costate is an inner product.
Graphically, states σ : I Ñ X and costates η : X Ñ I will be represented respectively as
follows:
σ
X
η
X
Discarding, marginalization, and causality
In a category with only trivial eﬀects, we can
think of these as witnessing the ‘discarding’ of information: in electronics terms, they “send the
signal to ground”. For this reason, we will denote such trivial eﬀects by the symbol
, writing
X : X Ñ I for each object X.
We can use discarding to depict marginalization. Given a ‘joint’ state (a state of a tensor product)
ω : I Ñ X b Y , we can discard either Y or X to obtain ‘marginal’ states ω1 of X and ω2 of Y
respectively, as in the following depiction:
ω
“
ω1
X
X
and
ω
“
ω2
Y
Y
.
69

We will see in Chapter 4 how this corresponds to the marginalization familiar from probability
theory.
To make the notion of discarding more mathematically precise, we can use it to encode a causality
condition: physically realistic processes should not be able to aﬀect the past.
Deﬁnition 3.1.1. Whenever a morphism c satisﬁes the equation
“
c
we will say that c is causal: the equation says that, if you do c and throw away the result, the eﬀect
is of not having done c at all—and so c could not have had an anti-causal eﬀect on its input.
Remark 3.1.2. If in a category every morphism is causal, then this is equivalently a statement of
the naturality of family of discarding morphisms
X : X Ñ I, which implies that there is only one
such morphism X Ñ I for every object X, and which therefore means that I must be a terminal
object.
Some categories of interest will have nontrivial costates, yet we will still need notions of
discarding and marginalization. In these categories, it suﬃces to ask for each object X to be
equipped with a ‘comonoid’ structure (to be elaborated in §3.4.1), of which one part is a ‘counit’
morphism X Ñ I which can play a discarding role, and which we will therefore also denote by
X.
Copying
The other part of a comonoid structure on X is a ‘copying’ map
X : X Ñ X b X,
which has an intuitive graphical representation. As we will see in §3.4.1, the comonoid laws say
that copying must interact nicely with the discarding maps:
“
“
and
“
These equations say that making a copy and throwing it away is the same as not making a copy (left,
counitality; and that in copying a copy, it doesn’t matter which copy you copy (right, coassociativity).
A category with a comonoid structure p
X,
Xq for every object X is called a copy-discard
category [87].
70

Symmetry
In all our applications, the tensor product structure will be symmetric, meaning that
X b Y can reversibly be turned into Y b X simply by swapping terms around. In the graphical
calculus, we depict this by the swapping of wires, which we ask to satisfy the following equations:
“
and
“
The equations say that swapping is self-inverse (on the left), and that copying is invariant under the
symmetry (on the right). (Strictly speaking, the right equation is an axiom called cocommutativity
that we additionally ask the comonoid structure to satisfy in the presence of a symmetric tensor.)
3.1.2. Monoidal categories
It being important to use tools appropriate for the jobs at hand, we will not always work just with
the graphical calculus: we will need to translate between string diagrams and the symbolic algebra
of Chapter 2. In the ﬁrst instance, this means making mathematical sense of the graphical calculus
itself, for which the key deﬁnition is that of the monoidal category.
Deﬁnition 3.1.3. We will call a category C monoidal if it is equipped with a functor b : C ˆC Ñ C
called the tensor or monoidal product along with an object I : C called the monoidal unit and three
natural isomorphisms
1. an associator α : pp´q b p´qq b p´q ñ p´q b pp´q b p´qq;
2. a left unitor λ : I b p´q ñ p´q; and
3. a right unitor ρ : p´q b I ñ p´q
such that the unitors are compatible with the associator, i.e. for all a, b : C the diagram
pa b Iq b b
a b pI b bq
a b b
ρabidb
αa,I,b
ida bλb
71

commutes, and such that the associativity is ‘order-independent’, i.e. for all a, b, c, d : C the diagram
pa b pb b cqq b d
a b ppb b cq b dq
ppa b bq b cq b d
a b pb b pc b dqq
pa b bq b pc b dq
αabb,c,d
αa,b,cbd
αa,b,cbidd
αa,bbc,d
ida bαb,c,d
commutes.
We call C strict monoidal if the associator and unitors are equalities rather than isomorphisms;
in this case, the diagrams above commute by deﬁnition.
Example 3.1.4. Any category equipped with a tensor product in the sense of Deﬁnition 2.2.2
where the structure isomorphisms are additionally natural and satisfy the axioms of compatibility
and order-independence is a monoidal category.
Example 3.1.5. If pC, b, Iq is a monoidal category, then so is pC op, bop, Iq, where bop is the
induced opposite functor C op ˆ C op Ñ C op.
The associativity of the tensor is what allows us to depict string diagrams “without brackets”
indicating the order of tensoring, and the unitality is what allows us to omit the monoidal unit
from the diagrams. Note that the functoriality of the tensor means that b distributes over ˝ as in
pf1 ˝ fq b pg1 ˝ gq “ pf1 b g1q ˝ pf b gq, both of which expressions are therefore depicted as
f1
f
g1
g
.
The symmetry of a monoidal structure is formalized as follows.
Deﬁnition 3.1.6. A symmetric monoidal category is a monoidal category pC, b, I, α, λ, ρq that
is additionally equipped with a natural isomorphism σ : p´q b p“q ñ p“q b p´q, called the
symmetry, such that σb,a ˝ σa,b “ idabb for all a, b : C, and whose compatibility with the associator
is witnessed by the commutativity of the following diagram:
pa b bq b c
a b pb b cq
pb b cq b a
pb b aq b c
b b pa b cq
b b pc b aq
αa,b,c
σa,bbc
αb,c,a
σa,bbidc
αb,a,c
idb bσa,c
72

Here is a familiar family of examples of symmetric, but not strict, monoidal categories.
Example 3.1.7. Any category with in which every pair of objects has a product is said to have
ﬁnite products, and any category with ﬁnite products and a terminal object is a monoidal category.
This includes the Cartesian products of sets (Deﬁnition 2.2.1 and Example 2.2.4) and of categories
(Propositions 2.2.13 and 2.2.15).
To see that the Cartesian product of sets is not strictly associative, observe that the elements
of A ˆ pB ˆ Cq are tuples pa, pb, cqq whereas the elements of pA ˆ Bq ˆ C are tuples ppa, bq, cq;
evidently, these two sets are isomorphic, but not equal, and the same holds for the product of
categories.
And here is a family of examples of strict, but not symmetric, monoidal categories.
Example 3.1.8. If C is any category, then the category CC of endofunctors C Ñ C is a strict
monoidal category, where the monoidal product is given by composition ˝ of endofunctors and the
monoidal unit is the identity functor idC on C. That the monoidal structure here is strict follows
from the fact that composition in a category is strictly associative and unital.
In practice, we will tend to encounter strict monoidal categories only when the monoidal structure
derives from the composition operator of a category, as in the preceding example. However, when
we work with the graphical calculus, we are often implicitly working with strict monoidal structure,
as a result of the following important theorem.
Theorem 3.1.9 (Mac Lane [84, Theorem XI.3.1]). Every monoidal category is strong monoidally
equivalent to a strict monoidal one.
As a consequence of this coherence theorem, any two string diagrams where one can be
transformed into the other by a purely topological transformation are equal, as in the following
example (read from left to right):
“
This follows because the coherence theorem renders parallel morphisms entirely constructed from
identities, associators and unitors (and the symmetry, as long as it is strictly self-inverse) equal “on
the nose”1.
To make sense of the notion of strong monoidal equivalence, we need a notion of functor that
preserves monoidal structure; we deﬁne the ‘weak’ case ﬁrst.
1This process of turning natural isomorphisms into equalities is called strictiﬁcation.
73

Deﬁnition 3.1.10. Suppose pC, bC, ICq and pD, bD, IDq are monoidal categories. A lax monoidal
functor pC, bC, ICq Ñ pD, bD, IDq is a triple of
1. a functor F : C Ñ D;
2. a state ϵ : ID Ñ FpICq called the unit; and
3. a natural transformation, the laxator, µ : Fp´q bD Fp“q ñ Fpp´q bC p“qq
satisfying the axioms of
(a) associativity, in that the following diagram commutes
pFpaq bD Fpbqq bD Fpcq
Fpaq bD pFpbq bD Fpcqq
Fpa bC bq bD Fpcq
Fpaq bD Fpb bC cq
Fppa bC bq bC cq
Fpa bC pb bC cqq
αD
F paq,F pbq,F pcq
FpaqbDµb,c
µa,bbDFpcq
µabCb,c
FpαC
a,b,cq
µF paq,bbCc
where αC and αD are the associators of the respective monoidal structures on C and D; and
(b) (left and right) unitality, in that the following diagrams commute
ID bD Fpaq
FpICq bD Fpaq
Fpaq
FpIC bC aq
λD
F paq
µIC,a
ϵbDFpaq
FpλC
aq
and
Fpaq bD ID
Fpaq bD FpICq
Fpaq
Fpa bC ICq
ρD
F paq
µa,IC
FpaqbDϵ
FpρC
aq
where λC and λD are the left, and ρC and ρD the right, unitors of the respective monoidal
structures on C and D.
A strong monoidal functor is a lax monoidal functor for which the unit and laxator are isomorphisms.
A strong monoidal equivalence is therefore an equivalence of categories in which the two functors
are strong monoidal.
Remark 3.1.11. Laxness can be read as a sign of an “emergent property”: if F is lax monoidal,
then this means there are systems of type FpX b Y q that do not arise simply by placing a system
of type FpXq beside a system of type FpY q using b; whereas if F is strong monoidal, then there
are no such ‘emergent’ systems. More generally, we can think of emergence as an indication of
74

higher-dimensional structure that is hidden when one restricts oneself to lower dimensions (and
hence can appear mysterious). In this example, the higher-dimensional structure is the 2-cell of the
laxator.
There is of course a notion of monoidal natural transformation, making monoidal categories, lax
monoidal functors, and monoidal natural transformations into the constituents of a 2-category.
Deﬁnition 3.1.12. If pF, µ, ϵq and pF 1, µ1, ϵ1q are lax monoidal functors pC, bC, ICq Ñ pD, bD, IDq,
then a monoidal natural transformation α : pF, µ, ϵq ñ pF 1, µ1, ϵ1q is a natural transformation
α : F ñ F 1 that is compatible with the unitors
ID
FpICq
F 1pICq
ϵ
ϵ1
αIC
and the laxators
Fa bD Fb
F 1a bC F 1b
Fpa bC bq
F 1pa bC bq
αabDαb
αabCb
µa,b
µ1
a,b
for all a, b : C.
Proposition 3.1.13. Monoidal categories, lax monoidal functors, and monoidal natural transfor-
mations form the 0-cells, 1-cells, and 2-cells of a 2-category, denoted MonCat.
Proof. Given composable lax monoidal functors pF, ϵ, µq : pC, bC, ICq Ñ pD, bD, IDq and
pF 1, ϵ1, µ1q : pD, bD, IDq Ñ pE, bE, IEq, form their horizontal composite as follows. The functors
compose as functors, G ˝ F. The composite state is given by IE
ϵ1
ÝÑ F 1pIDq F 1ϵ
ÝÝÑ F 1FpICq. The
laxator is given by
F 1Fp´q bE F 1Fp“q
µ1
F p´q,F p“q
ùùùùùùùñ F 1pFp´q bD Fp“qq
F 1µa,b
ùùùùñ F 1Fpp´q bC p“qq .
The identity lax monoidal functor on C is given by pidC, idIC, idp´qbCp“qq. Unitality and associativity
of composition of lax monoidal functors follow straightforwardly from unitality and associativity
of composition of morphisms, functors, and natural transformations. Monoidal natural transforma-
tions compose vertically as natural transformations, and it is easy to see that the composites satisfy
the compatibility conditions by pasting the relevant diagrams.
75

3.1.3. Closed monoidal categories
Since one source of monoidal structures is the generalization of the categorical product, it is no
surprise that there is a corresponding generalization of exponentials: a ‘tensor-hom’ adjunction
that induces a concept of closed monoidal category. Such categories will be important later in the
thesis when we consider learning and adaptive systems: our compositional model of predictive
coding, for example, will be built on a certain generalized exponential (see Remark 5.3.2).
Deﬁnition 3.1.14. Let pC, b, Iq be a monoidal category. When there is an object e : C such that
Cpx, eq – Cpx b y, zq naturally in x, we say that e is an internal hom object and denote it by
ry, zs. The image of idry,zs under the isomorphism is called the evaluation map and is written
evy,z : ry, zs b y Ñ z.
Proposition 3.1.15. When the isomorphism Cpx b y, zq – Cpx, ry, zsq is additionally natural in
z, we obtain an adjunction p´q b y % ry, ´s called the tensor-hom adjunction, which uniquely
determines a functor C op ˆ C Ñ C : py, zq ÞÑ ry, zs that we call the internal hom for C.
Proof. A direct generalization of the Cartesian case (Proposition 2.3.56).
Deﬁnition 3.1.16. A monoidal category C with a corresponding internal hom is called monoidal
closed.
Example 3.1.17. The category of ﬁnite-dimensional real vector spaces and linear maps between
them is monoidal closed with respect to the tensor product of vector spaces, as each space of linear
maps is again a vector space and the tensor is necessarily bilinear.
As in the Cartesian case, monoidal closed categories can reason about themselves.
Proposition 3.1.18. A monoidal closed category is enriched in itself.
And when a category is enriched in a symmetric monoidal category, then its hom functor is
likewise enriched.
Proposition 3.1.19. Suppose C is an E-category where E is symmetric monoidal closed. Then the
hom functor Cp´, “q is an E-functor.
Proof. A direct generalization of Proposition 2.3.61.
Remark 3.1.20. Since Cartesian closed categories have a rich internal logic, via the Curry-Howard-
Lambek correspondence, one might wonder if there is an analogous situation for monoidal closed
categories. To a certain intricate extent there is: the internal logic of monoidal closed categories
is generally known as linear logic, and its corresponding language linear type theory. These are
‘reﬁnements’ of intuitionistic logic and type theory which of course coincide in the Cartesian case,
76

but which more generally clarify certain logical interactions; we shall say no more in this thesis,
except that such logics ﬁnd application in quantum mechanics, owing to the monoidal closed
structure of vector spaces, where the linear structure constrains the use of resources (in relation,
for example, to the famous quantum ‘no-cloning’ and ‘no-deleting’ theorems).
With respect to dependent types, the situation is a little more vexed, as the existence of well-
behaved dependent sums and products classically depends on the existence of pullbacks and their
coherence with products (and, for example, the tensor product of vector spaces is not a categorical
product); this means that classical dependent data is somehow not resource-sensitive. Nonetheless,
various proposals have been made to unify linear logic with dependent type theory[88–92]: the
simplest of these proceed by requiring dependence to be somehow Cartesian, which is the approach
we will take in Chapter 5 when we face a similar quandary in the context of deﬁning a category of
polynomial functors with non-deterministic feedback. (We will see in Chapter 4 that the property
of Cartesianness is equally closely related to determinism.)
3.1.4. Bicategories
Monoidal categories are not the ﬁrst two-dimensional categorical structures we have so far
encountered, the other primary example being 2-categories. These two classes of examples are
closely related: a strict monoidal category is a 2-category with one object; and so just as a monoidal
category is a correspondingly weakened version, a bicategory is a ‘weak 2-category’.
Deﬁnition 3.1.21. A bicategory B constitutes
1. a set B0 of objects or 0-cells;
2. for each pair pA, Bq of B-objects, a category BpA, Bq called the hom category, the objects of
which are the morphisms or 1-cells from A to B, and the morphisms of which are the 2-cells
between those 1-cells;
3. for each 0-cell A, a 1-cell ida : BpA, Aq witnessing identity; and
4. for each triple pA, B, Cq of 0-cells, a functor ˛A,B,C : BpB, Cq ˆ BpA, Bq Ñ BpA, Cq
witnessing horizontal composition (with vertical composition referring to composition within
each hom category);
5. for each pair pA, Bq of 0-cells, natural isomorphisms ρA,B (the right unitor) and λA,B (the
77

left unitor) witnessing the unitality of horizontal composition, as in the diagrams
BpA, Bq ˆ 1
BpA, Bq ˆ BpA, Aq
BpA, Bq
PBpA,Bq
BpA,BqˆidA
˛A,A,B
ρA,B
and
BpA, Aq ˆ BpA, Bq
1 ˆ BpA, Bq
BpA, Bq
ΛBpA,Bq
idBˆBpA,Bq
˛A,B,B
λA,B
where Λ : 1 ˆ p´q ñ p´q and P : p´q ˆ 1 ñ p´q are the (almost trivial) left and right
unitors of the product ˆ on Cat; and
6. for each quadruple pA, B, C, Dq of 0-cells, a natural isomorphism αA,B,C,D witnessing the
associativity of horizontal composition, as in the diagram
`
BpC, Dq ˆ BpB, Cq
˘
ˆ BpA, Bq
BpC, Dq ˆ
`
BpB, Cq ˆ BpA, Bq
˘
BpB, Dq ˆ BpA, Bq
BpC, Dq ˆ BpA, Cq
BpA, Dq
ABpC,Dq,BpB,Cq,BpA,Bq
˛B,C,DˆBpA,Bq
BpC,Dqˆ˛A,B,C
˛A,B,D
˛A,C,D
αA,B,C,D
where A : pp´q ˆ p´qq ˆ p´q ñ p´q ˆ pp´q ˆ p´qq is the (almost trivial) associator of the
product ˆ on Cat;
such that the unitors are compatible with the associator, i.e. for all 1-cells a : BpA, Bq and
b : BpB, Cq the diagram
pb ˛ idbq ˛ a
b ˛ pidb ˛aq
b ˛ a
ρb˛ida
αb,idb,a
idb ˛λb
commutes (where we have omitted the subscripts indexing the 0-cells on α, ρ, and λ); and such that
the associativity is ‘order-independent’, i.e. for all 1-cells a : BpA, Bq, b : BpB, Cq, c : BpC, Dq,
78

and d : BpD, Eq the diagram
pa ˛ pb ˛ cqq ˛ d
a ˛ ppb ˛ cq ˛ dq
ppa ˛ bq ˛ cq ˛ d
a ˛ pb ˛ pc ˛ dqq
pa ˛ bq ˛ pc ˛ dq
αa˛b,c,d
αa,b,c˛d
αa,b,c˛idd
αa,b˛c,d
ida ˛αb,c,d
commutes (where we have again omitted the subscripts indexing the 0-cells on α).
Remark 3.1.22. Just as a 2-category is a category enriched in Cat, a bicategory is a category
weakly enriched in Cat. This is easy to see by comparing Deﬁnition 3.1.21 with Deﬁnition 2.2.5: the
former is obtained from the latter by taking E to be Cat and ﬁlling the unitality and associativity
diagrams with nontrivial ﬁllers which are required to satisfy coherence laws generalizing those of
the monoidal category structure (Deﬁnition 3.1.3). Conceptually, we can see this weakening in the
context of our brief discussion of emergence above (Remark 3.1.11): we recognize the property of
axiom-satisfaction as a shadow of a higher-dimensional structure (the ﬁllers), which we categorify
accordingly.
Bicategories will appear later in this thesis when we construct categories of dynamical hierarchical
inference systems: the construction proceeds by using polynomial functors to “wire together”
categories of dynamical systems, and the composition of polynomials distributes weakly but
naturally over the categories of systems, thereby producing a category weakly enriched in Cat.
Before then, we will encounter bicategories in the abstract context of general parameterized
morphisms, where the 2-cells witness changes of parameter.
For now, our ﬁrst examples of bicategories are induced by monoidal categories, which are
equivalently single-object bicategories.
Proposition 3.1.23. Suppose pC, b, Iq is a monoidal category. Then there is a bicategory BC with
a single 0-cell, ˚, and whose category of 1-cells BCp˚, ˚q is C. The identity 1-cell is I, and horizontal
composition is given by the monoidal product C; vertical composition is just the composition
of morphisms in C. The unitors and associator of the bicategory structure are the unitors and
associator of the monoidal structure. We call BC the delooping of C.
Proof. The bicategory axioms are satisﬁed immediately, because the structure morphisms satisfy
the (in this case identical) monoidal category axioms.
In the opposite direction, the equivalence is witnessed by the following proposition.
79

Proposition 3.1.24. Suppose B is a bicategory with a single 0-cell, ˚, and whose horizontal
composition is denoted ˛. Then
`
Bp˚, ˚q, ˛, id˚
˘
is a monoidal category.
Remark 3.1.25. It is possible to deﬁne a notion of monoidal bicategory, as something like a
monoidal category weakly enriched in Cat, or as a one-object ‘tricategory’, and in many cases the
bicategories considered below are likely to have such structure. We will say a little more about this
in Remark 3.4.7 below, but will not deﬁne or make formal use of this higher structure in this thesis.
More generally, there are analogues of the other structures and results of basic category theory
introduced both in this chapter and in Chapter 2 that are applicable to higher-dimensional categories
such as bicategories, but they too will not play an important role in this thesis.
3.2. Parameterized systems
A category does not have to be monoidal closed for us to be able to talk about “controlled processes”
in it: its being monoidal is suﬃcient, for we can consider morphisms of the form P b X Ñ Y and
treat the object P as an object of adjustable parameters. Parameterized morphisms of this form
can easily be made to compose: given another morphism Q b Y Ñ Z, we can straightforwardly
obtain a composite parameterized morphism pQ b Pq b X Ñ Z, as we elaborate in §3.2.1 below.
Categories of such parameterized morphisms play a central role in the compositional modelling
of cybernetic systems[70, 93], where we typically see the parameter as controlling the choice of
process, and understand learning as a ‘higher-order’ process by which the choice of parameter is
adjusted. More concretely, consider the synaptic strengths or weights of a neural network, which
change as the system learns about the world, aﬀecting the predictions it makes and actions it takes;
or consider the process of Bayesian inference, where the posterior is dependent on a parameter
that is typically called the ‘prior’.
In this section, we introduce two related formal notions of parameterization: ‘internal’, where the
parameter object constitutes a part of the domain of morphisms in a category; and ‘external’, where
the parameters remain outside of the category being parameterized and the choice of morphism
is implemented as a morphism in the base of enrichment. We will make use of both kinds of
parameterization in this thesis.
3.2.1. Internal parameterization
Internal parameterization generalizes the case with which we opened this section, of morphisms
P b X Ñ Y , to a situation in which the parameterization may have diﬀerent structure to the
processes at hand, so that the parameterizing objects live in a diﬀerent category. For this reason, we
describe the ‘actegorical’ situation in which a category of parameters M acts on on a category of
processes C to generate a category of parameterized processes. Nonetheless, even in this case, the
80

parameter ends up constituting part of the domain of the morphism representing the parameterized
process.
The ﬁrst concept we need is that of an ‘actegory’, which categoriﬁes the better known
mathematical notion of monoid action2.
Deﬁnition 3.2.1 (M-actegory). Suppose M is a monoidal category with tensor b and unit object
I. We say that C is a left M-actegory when C is equipped with a functor d : M ˆ C Ñ C
called the action along with natural unitor and associator isomorphisms λd
X : I d X
„
ÝÑ X and
ad
M,N,X : pM b Nq d X
„
ÝÑ M d pN d Xq compatible with the monoidal structure of pM, b, Iq.
Given an actegory, we can deﬁne a category of correspondingly parameterized morphisms.
Proposition 3.2.2 (Capucci et al. [93]). Let pC, d, λd, adq be an pM, b, Iq-actegory. Then there is
a bicategory of M-parameterized morphisms in C, denoted Parapdq. Its objects are those of C. For
each pair of objects X, Y , the set of 1-cells is deﬁned as ParapdqpX, Y q :“ ř
M:M CpM d X, Y q;
we denote an element pM, fq of this set by f : X
M
ÝÑ Y . Given 1-cells f : X
M
ÝÑ Y and g : Y
N
ÝÑ Z,
their composite g ˝ f : X
NbM
ÝÝÝÝÑ Z is the following morphism in C:
pN b Mq d X
ad
N,M,X
ÝÝÝÝÝÑ N d pM d Xq
idN df
ÝÝÝÝÑ N d Y
gÝÑ Z
Given 1-cells f : X
M
ÝÑ Y and f1 : X
M1
ÝÝÑ Y , a 2-cell α : f ñ f1 is a morphism α : M Ñ M1 in
M such that f “ f1 ˝ pα d idXq in C; identities and composition of 2-cells are as in C.
And in many cases, these parameterized categories inherit a monoidal structure.
Proposition 3.2.3. When C is equipped with both a symmetric monoidal structure pb, Iq and an
pM, b, Iq-actegory structure, and there is a natural isomorphism µr
M,X,Y : M d pX b Y q
„
ÝÑ
XbpM dY q called the (right) costrength, the symmetric monoidal structure pb, Iq lifts to Parapdq.
First, from the symmetry of b, one obtains a (left) costrength, µl
M,X,Y : M d pX b Y q
„
ÝÑ
pM d Xq b Y . Second, using the two costrengths and the associator of the actegory structure, one
obtains a natural isomorphism ιM,N,X,Y : pM b Nq d pX b Y q „
ÝÑ pM d Xq b pN d Y q called
the interchanger. The tensor of objects in Parapdq is then deﬁned as the tensor of objects in C,
and the tensor of morphisms (1-cells) f : X
M
ÝÑ Y and g : A N
ÝÑ B is given by the composite
f bg : X bA MbN
ÝÝÝÝÑ Y bB :“ pM bNqdpX bAq
ιM,N,X,A
ÝÝÝÝÝÝÑ pM dAqbpN dAq
fbg
ÝÝÑ Y bB .
We can see a monoidal product b : C ˆ C Ñ C as an action of C on itself, and this induces the
self-parameterization of C.
2For a comprehensive reference on actegory theory, see Capucci and Gavranović [94].
81

Proposition 3.2.4. If pC, b, Iq is a monoidal category, then it induces a parameterization Parapbq
on itself. For each M, X, Y : C, the morphisms X
M
ÝÑ Y of Parapbq are the morphisms M bX Ñ
Y in C.
Notation 3.2.5. When considering the self-paramterization induced by a monoidal category
pC, b, Iq, we will often write ParapCq instead of Parapbq.
It will frequently be the case that we do not in fact need the whole bicategory structure. The
following proposition tells us that we can also just work 1-categorically, as long as we work with
equivalence classes of isomorphically-parameterized maps, in order that composition is suﬃently
strictly associative.
Proposition 3.2.6. Each bicategory Parapdq induces a 1-category Parapdq1 by forgetting the
bicategorical structure. The hom sets Parapdq1pX, Y q are given by UParapdqpX, Y q{ „ where
U is the forgetful functor U : Cat Ñ Set and f „ g if and only if there is some 2-cell α : f ñ g
that is an isomorphism. We call Parapdq1 the 1-categorical truncation of Parapdq. When Parapdq
is monoidal, so is Parapdq1.
3.2.2. External parameterization
In a monoidal closed category, morphisms P b X Ñ Y correspond bijectively to morphisms
P Ñ rX, Y s. The fact that monoidal closed categories are enriched in themselves presents an
opportunity for generalization in a diﬀerent direction to the actegorical approach taken above:
that is, given a category of processes C enriched in E, we can think of an externally parameterized
process from X to Y as a morphism P Ñ CpX, Y q in E.
This notion of external parameterization can be operationally more faithful to the structure of
systems of interest, even though in the case of monoidal closed categories it is equivalent. For
example, the improvement of the performance of a system of inference due to learning is often
treated ‘externally’ to the inference process itself: the learning process might proceed by observing
(but not interfering with) the inference process, and updating the parameters accordingly; and, if
treated dynamically, the two processes might be assumed to exhibit a separation of timescales such
that the parameters are stationary on the timescale of inference. We will make such assumptions
when we formalize learning in Chapter 6, and so we will make use of external parameterization.
The deﬁnition of external parameterization is simpliﬁed by using the ‘slice’ construction.
Deﬁnition 3.2.7. Suppose X is an object of a category E. We deﬁne the slice of E over X, denoted
E{X, as the category of ‘bundles’ over X in E: its objects are morphisms p : A Ñ X into X for
82

any A : E, which we call bundles over X and write as pA, pq. The morphisms f : pA, pq Ñ pB, qq
in E{X are morphisms f : A Ñ B in E such that q ˝ f “ p, as in the diagram
A
B
X
f
p
q
.
We therefore deﬁne external parameterization using slices over hom objects.
Deﬁnition 3.2.8. Given a category C enriched in pE, ˆ, 1q, we deﬁne the external parameterization
PC of C in E as the following bicategory. 0-cells are the objects of C, and each hom-category
PCpA, Bq is given by the slice category E{CpA, Bq. The composition of 1-cells is by composing
in C after taking the product of parameters: given f : Θ Ñ CpA, Bq and g : ΩÑ CpB, Cq, their
composite g ˝ f is
g ˝ f :“ Ωˆ Θ
gˆf
ÝÝÑ CpB, Cq ˆ CpA, Bq ‚ÝÑ CpA, Cq
where ‚ is the composition map for C in E. The identity 1-cells are the points on the identity
morphisms in C. For instance, the identity 1-cell on A is the corresponding point idA : 1 Ñ CpA, Aq.
We will denote 1-cells using our earlier notation for parameterized morphisms: for instance,
f : A
Θ
ÝÑ B and idA : A
1ÝÑ A. The horizontal composition of 2-cells is given by taking their
product.
Remark 3.2.9. In prior work, this external parameterization has been called ‘proxying’ [95].
We prefer the more explicit name ‘external parameterization’, reserving ‘proxying’ for a slightly
diﬀerent double-categorical construction to appear in future work by the present author.
3.3. Systems from circuits
The dominant motivation for the use of monoidal categories so far has been in modelling the
compositional structure of processes, on the basis of the observation that processes may generally
be composed both sequentially and in parallel, and so 1-dimensional category theory alone is
insuﬃcient. The processes for which this kind of structure is most suited are those that exhibit a
ﬂow of information. For example, if we take the morphisms of the category Set as computable
functions, then we see that the corresponding “process theory” is adequate for interpreting diagrams
of the form of §2.1.1.3; and we will encounter in Chapter 4 a process-theoretic framework formalizing
probabilistic graphical models of the kind discussed in §2.1.1.2.
In these monoidal categories, processes are represented by morphisms, with composition used
to connect processes together: the composite of two processes is again a process. However, some
morphisms are purely ‘structural’, implementing the plumbing of information ﬂow—such as copying,
83

discarding, and swapping—and so these categories somewhat blur the boundary between syntax
and semantics. At the same time, it is strange to think of something like a neural circuit as a
‘process’: although it might reify some process in its behaviour, it is rather a system.
To sharpen the syntax-semantics boundary, one can show that every monoidal category arises
as an algebra for a certain monad. We will make these notions precise in §3.4 below, and here
it will suﬃce to provide some intuition: the monad deﬁnes the syntax, and the algebra supplies
a compatible semantics. Algebra in this sense is a vast generalization of the abstract algebra of
familiar mathematics, and typically involves deﬁning symbolic operations and rules by which they
can be combined, substituted, compared, and reduced.
In this section, although we do not explicitly make use of the technology of monads, we exemplify
this approach with an example of compositional connectomics: on the syntactic side, we will
introduce a ‘multicategory’ of linear circuit diagrams which govern patterns of neural connectivity;
while on the semantic side, we will equip this multicategory with a functorial algebra of rate-coded
neural circuits3. We will ﬁnd that this more explicitly algebraic approach resolves the dilemma
observed above between the compositional structure of processes and that of systems: algebraic
syntax is in some sense about substitution, and so circuit diagrams will have ‘holes’ into which can
be substituted other circuit diagrams. That is to say, a circuit diagram is a morphism which takes a
given pattern of holes and connects them together into a single circuit, as in the following diagram,
which brings us back to our ﬁrst motivating example from §2.1.1.1 and which we formalize below.
E
I
ÞÑ
EI
We will use a similar approach when we supply dynamical semantics for approximate inference,
although there, for our general syntax of systems, we will use categories of polynomial functors,
which we introduce in §3.5 at the end of this chapter. In any case, it will turn out that linear
circuit diagrams embed naturally into polynomials, and so the circuits below can be understood as
providing a sample of what is to come.
3.3.1. Multicategorical algebra for hierarchical systems
A multicategory is like a category, but where morphisms may have a ‘complex’ domain, such as
a list of objects [96]. A morphism whose domain is an n-length list is called ‘n-ary’, and we can
abstractly think of such morphisms as ‘n-ary operations’: for example, we will use them to model
3In the Appendix (§A.1), we sketch the connection between this multicategorical story and the monadic one.
84

connecting n circuits together into a single system. Because these morphisms eﬀect a kind of
‘zooming-out’, we can use them to construct hierarchical or ‘nested’ systems-of-systems.
Deﬁnition 3.3.1. A multicategory O consists of
1. a set O0 of objects;
2. a set O1 of morphisms, equipped with
a) a codomain function cod : O1 Ñ O0, and
b) a domain function dom : O1 Ñ ListpO0q, where ListpO0q is the set of ﬁnite lists of
objects po1, . . . , onq,
so that each n-ary morphism f has a list of n objects as its domain and a single object as its
codomain, written f : po1, . . . , onq Ñ p;
3. an identity function id : O0 Ñ O1 such that codpidoq “ o and dompidoq “ poq, so that the
identity on o is written ido : o Ñ o;
4. a family of composition functions
˝p,poiq,poj
i q : Opo1, . . . , on; pq ˆ Opo1
1, . . . , ok1
1 ; o1q ˆ ¨ ¨ ¨ ˆ Opo1
n, . . . , okn
n ; onq
Ñ Opo1
1, . . . , ok1
1 , . . . , o1
n, . . . , okn
n ; pq
written as
pf, f1, . . . , fnq ÞÑ f ˝ pf1, . . . , fnq
for each object p, n-ary list objects po1, . . . , onq, and n ki-ary lists of objects po1
i , . . . , oki
i q;
satisfying the equations of associativity
f ˝
`
f1 ˝ pf1
1 , . . . , fk1
1 q, . . . , fn ˝ pf1
n, . . . , fkn
n q
˘
“
`
f ˝ pf1, . . . , fnq
˘
˝ pf1
1 , . . . , fk1
1 , . . . , f1
n, . . . , fkn
n q
whenever such composites make sense, and unitality
f ˝ pido1, . . . , idonq “ f “ idp ˝f
for every f : po1, . . . , onq Ñ p.
For our purposes, the order of objects in the lists will not matter, which we formalize with the
notion of symmetric multicategory—analogous to the symmetric monoidal categories of §3.1.2.
85

Deﬁnition 3.3.2. Let Sn be the symmetric group on n elements. A symmetric multicategory O is
a multicategory O which is additionally equipped, for each n : N, with an action σn of Sn on the
set On
1 of n-ary morphisms
σn : Sn ˆ On
1 Ñ On
1
such that composition ˝ preserves this action.
Remark 3.3.3. In other applied-category-theoretical contexts, multicategories of this kind are
sometimes called operads (cf. e.g. [79, 80, 97–106]). Traditionally, an operad is the same as a
multicategory with one object[96]; sometimes therefore, multicategories are called coloured or
typed operads[96, 98, 107, 108]. In order to avoid confusion, we will stick with ‘multicategory’.
Although the multicategorical intuition—of hierarchically constructing complex systems—is
valuable, the following fact means that there is a close connection between multicategories and
monoidal categories, for in a monoidal category, we can interpret an n-ary tensor x1 b ¨ ¨ ¨ b xn as
an n-ary list of objects.
Proposition 3.3.4. Any monoidal category pC, b, Iq induces a corresponding multicategory OC.
The objects OC0 are the objects C0 of C. The n-ary morphisms pc1, . . . , cnq Ñ d are the morphisms
c1 b ¨ ¨ ¨ b cn Ñ d; i.e., OCpc1, . . . , cn; dq :“ Cpc1 b ¨ ¨ ¨ b cn, dq. Identities are as in C, and
composition is deﬁned by pf, f1, . . . , fnq ÞÑ f ˝ pf1 b ¨ ¨ ¨ b fnq. When C is symmetric, so is OC.
Example 3.3.5. An example that will soon become important is the operad Sets of sets and n-ary
functions, which is obtained from the symmetric monoidal category Set by Sets :“ O Set.
As we discussed above, we will consider multicategories as supplying a syntax for the composition
of systems, and so actually to compose systems requires the extra data of what those systems are
and how they can be composed according to the syntax. This extra semantic data is called an
‘algebra’ for the multicategory.
Deﬁnition 3.3.6. An algebra for a multicategory M is a multifunctor M Ñ Sets.
Multifunctors are the multicategorical analogues of functors; but fortunately (even though the
deﬁnition is not a hard one), we will not need to deﬁne them, owing to the following result, which
relates multifunctors and lax monoidal functors.
Proposition 3.3.7 (Leinster [96, Example 4.3.3, Deﬁnition 2.1.12]). If the multicategory M arises
from a monoidal category pC, b, Iq as M “ OC, then an algebra for M is determined by a lax
monoidal functor pC, b, Iq Ñ pSet, ˆ, 1q.
86

Remark 3.3.8. In §3.4, we will encounter the concept of “algebra for a monad”, which is perhaps
the more familiar concept in mathematics and computer science. One might therefore wonder
what the relationship between the two notions of ‘algebra’ is: why do they both have this name?
The answer is provided by Leinster [96]: every ‘shape’ of multicategory corresponds to a certain
monad; and every multicategory algebra corresponds to an algebra for a monad derived (in the
context of the particular multicategory at hand) from this multicategory-shape monad. For the
interested reader, we review these results in the Appendix (§A.1). In §3.3.2, we will exemplify the
notion of monad algebra with the more basic result that every small category corresponds to an
algebra for a certain monad. Monad algebras will also prove useful later in the thesis in the context
of compositional probability theory.
3.3.2. Linear circuit diagrams
Let us now exhibit the multicategory formalizing circuit diagrams of the type with which we
opened this section. Although our motivation is multicategorical, for simplicity we will proceed by
deﬁning a symmetric monoidal category. Its objects will represent the ‘output-input’ dimensions of
a circuit, written as pairs of numbers pno, niq, and its morphisms pno, niq Ñ pmo, miq encode how
to wire a circuit with no outputs and ni inputs together to produce a circuit of mo outputs and mi
inputs: this may involve connecting some of the no outputs to the mo outputs; or connecting some
of the mi inputs, or (to allow recurrence) the no outputs, to the ni inputs. The deﬁnition may seem
somewhat mysterious at ﬁrst, but its form is owed to a more abstract structure (lenses) that we will
deﬁne later, in §4.2.
Example 3.3.9. We deﬁne a symmetric monoidal category
`
LinCirc, `, p0, 0q
˘
of linear circuit
diagrams and consider the induced multicategory OLinCirc. The objects of LinCirc are pairs
pno, niq of natural numbers. A morphism pno, niq Ñ pmo, miq is a pair of real-valued matrices
pA, Bq with A of shape pmo, noq and semi-orthogonal (i.e., such that AAT “ 1mo) and B of shape
pni, no ` miq; equivalently, A is a semi-orthogonal linear map Rno Ñ Rmo and B is a linear map
Rno`mi Ñ Rni. The identity morphism idpno,niq on pno, niq is the pair of matrices p1no, 01noq
where 01no is the block matrix
´
0no
1no
¯
. Given morphisms pA, Bq : pno, niq Ñ pmo, miq and
pA1, B1q : pmo, miq Ñ pko, kiq, their composite is the pair pA1A, BB1
Aq where A1A is the usual
matrix product and BB1
A is deﬁned as the following block matrix multiplication:
BB1
A :“ B
˜
1no
0
0
B1
¸ ¨
˚
˝
1no
0
A
0
0
1ki
˛
‹‚
Unitality and associativity of composition follow from those properties of matrix multiplication,
and AA1 is easily seen to be semi-orthogonal (by AA1pAA1qT “ AA1A1T AT “ AAT “ 1mo), so
LinCirc is a well-deﬁned category.
87

We now turn to the monoidal structure. The monoidal unit is the pair p0, 0q; note that Ro – 1.
The monoidal product ` is deﬁned on objects as the pointwise sum: pno, niq ` pmo, miq :“ pno `
mo, ni ` miq; note that Rno`mo – Rno ˆ Rmo. Given morphisms pA, Bq : pno, niq Ñ pmo, miq
and pA1, B1q : pn1
o, n1
iq Ñ pm1
o, m1
iq, their monoidal product pA, Bq ` pA1, B1q is deﬁned as the
pair pA ‘ A1, B ‘ B1q : pno ` n1
o, ni ` n1
iq Ñ pmo ` m1
o, mi ` m1
iq with
A ‘ A1 :“
˜
A
0
0
A1
¸
and
B ‘ B1 :“
˜
B
0
0
B1
¸
¨
˚
˚
˚
˝
1no
0
0
0
0
0
1mi
0
0
1n1o
0
0
0
0
0
1m1
i
˛
‹‹‹‚ .
For each pair of objects pno, niq and pmo, miq, the symmetry σpno,niq,pmo,miq : pno, niq `
pmo, miq Ñ pmo, miq ` pno, niq is deﬁned as the pair of matrices pσo
n,m, σi
n,mq,
σo
n,m :“
˜
0
1mo
1no
0
¸
and
σi
n,m :“
˜
0
0
0
1ni
0
0
1mi
0
¸
.
That this deﬁnition produces a well-deﬁned symmetric monoidal structure follows from more
abstract considerations that we explain in Remark 4.2.29 and Corollary 4.2.32: LinCirc is a
subcategory of Cartesian lenses, with the monoidal structure inherited accordingly.
The category of linear circuit diagrams is a syntactic category: on its own, it does not do anything.
We need to equip it with semantics.
3.3.3. An algebra of rate-coded neural circuits
We begin by deﬁning a notion of ‘rate-coded’ neural circuit.
Deﬁnition 3.3.10. An no-dimensional rate-coded neural circuit with ni-dimensional input is an
ordinary diﬀerential equation
9x “ ´λ d x ` h
`
Wpx ‘ iq; α, β, γ
˘
where x, λ, α, β, γ are real vectors of dimension no, i a real vector of dimension ni, W a real
matrix of shape pno, no ` niq, d elementwise multiplication, ‘ the direct sum (so that x ‘ i is the
concatenation
˜
x
i
¸
), and h the logistic function
hpx; α, β, γq “
γ
1 ` exp
`
´βpx ´ αq
˘
applied elementwise. We summarize the data of such a circuit as the tuple pλ, α, β, γ, Wq.
88

Remark 3.3.11. Rate-coded neural circuits are a coarse phenomenological model of neural
dynamics. The state variable x represents the ﬁring rates of an ensemble of neurons, either
averaged over time or over subpopulations. Neural activity is of course not so simple: neurons
communicate by the transmission of discrete ‘action potentials’ along their axons. The emission of
an action potential is governed by the electrical potential of its cellular membrane: if this potential
crosses a threshold, then the neuron ‘ﬁres’ an action potential down its axon. The axon crosses the
dendrites of other neurons at junctions called synapses, which modulate and transmit the activity
accordingly: it is these aﬀerent signals which in large part determine the neurons’ membrane
potentials.
There are of course detailed physiological models of this process (cf. e.g. [109–112]), as well
as many models which aim to capture its statistics and phenomenology in a more explicitly
computational setting (cf. e.g. [113–120]), but in some situations, one can simply model neural
ﬁring as an inhomogeneous Poisson process: in this case the variable x encodes the rate parameters
of the processes. We expect there to be functorial connections between the diﬀerent classes of
models: in particular, we expect adjoint functors between certain spike-emission models and ﬁring
rate models of the class deﬁned above; and in the speciﬁc case of ‘eﬃcient balanced’ networks[41,
118], the relationships are expected to be quite simple. Nonetheless, we leave the exploration of
such functors to future work.
The parameters of a rate-coded neural circuit—the terms λ, α, β, γ, W— have a neurological
interpretation, even though this dynamical model is not physiologically faithful. The term λ
represents the ‘leak’ of voltage from the neuron’s membrane, which has the eﬀect of determining
the timescale of its memory or signal-sensitivity (eﬀectively, the voltage leak entails a process of
ﬁltering). The term α represents an abstraction of the neuron’s ﬁring threshold, and the term β its
sensitivity (i.e., how much its ﬁring rate increases with incoming signals); the term γ determines
the maximum ﬁring rate of the neuron (and is typically normalized to 1). Finally, the matrix W
records the strengths of the synaptic connections within the circuit: positive coeﬃcients represent
excitatory connections, while negative coeﬃcients represent inhibitory connections.
Rate-coded neural circuits can be organized into complex ‘hierarchical’ systems using linear
circuit diagrams: the linear connectivity of the diagrams is used to deﬁne the synaptic connection
matrix of the complex, algebraically. The proof that the following construction does actually
constitute an algebra ensures that composing systems from circuits using diagrams is predictably
well-behaved, as we will subsequently exemplify.
Proposition 3.3.12 (Algebra of rate-coded neural circuits). There is a LinCirc-algebra pR, µ, ϵq :
`
LinCirc, `, p0, 0q
˘
Ñ pSet, ˆ, 1q of rate-coded neural circuits. On objects pno, niq, deﬁne
89

Rpno, niq to be the set of no-dimensional rate-coded neural circuits with ni-dimensional input.
Then, given a linear circuit diagram pA, Bq : pno, niq Ñ pmo, miq, deﬁne a function
RpA, Bq : Rpno, niq Ñ Rpmo, miq
pλ, α, β, γ, Wq ÞÑ pAλ, Aα, Aβ, Aγ, WABq
where WAB is the following block matrix product:
WAB :“ AW
˜
1no
0
0
B
¸ ¨
˚
˝
1no
0
1no
0
0
1mi
˛
‹‚
˜
AT
0
0
1mi
¸
.
The laxator µ is deﬁned componentwise as the family of functions
µpno,niq,pmo,miq : Rpno, niq ˆ Rpmo, miq Ñ R
`
pno, niq ` pmo, miq
˘
taking a pair of circuits pλ, α, β, γ, Wq : Rpno, niq and pλ1, α1, β1, γ1, W 1q : Rpmo, miq to the
circuit pλ ‘ λ1, α ‘ α1, β ‘ β1, γ ‘ γ1, WW 1q where x ‘ y is again the direct sum
˜
x
y
¸
and where
the matrix WW 1 is deﬁned as
WW 1 :“
˜
W
0
0
W 1
¸
¨
˚
˚
˚
˝
1no
0
0
0
0
0
1ni
0
0
1mo
0
0
0
0
0
1mi
˛
‹‹‹‚ .
The unitor ϵ is the isomorphism ϵ : 1 „
ÝÑ Rp0, 0q.
Proof. We need to check that R is a lax monoidal functor, and begin by verifying functoriality.
So suppose pA1, B1q is a linear circuit diagram pmo, miq Ñ pko, kiq. On the terms λ, α, β, γ, the
functoriality of R is immediate from matrix multiplication, so we concentrate on the action of
R on W. We need to show that R
`
pA1, B1q ˝ pA, Bq
˘
pWq “ RpA1, B1q ˝ RpA, BqpWq, where
RpA, BqpWq “ WAB as deﬁned above. Note that we can alternatively write WAB as the following
composite linear map
mo ` mi
AT `mi
ÝÝÝÝÝÑ no ` mi
`mi
ÝÝÝÝÑ no ` no ` mi
no`B
ÝÝÝÝÑ no ` ni
W
ÝÑ no
A
ÝÑ mo .
We can therefore write RpA1, B1qpWABq as
ko ` ki
A1T `ki
ÝÝÝÝÝÑ mo ` ki
AT `ki
ÝÝÝÝÑ no ` ki
`ki
ÝÝÝÝÑ no ` no ` ki ¨ ¨ ¨
¨ ¨ ¨
no`
`ki
ÝÝÝÝÝÝÑ no ` no ` no ` ki
no`no`A`ki
ÝÝÝÝÝÝÝÝÑ no ` no ` mo ` ki ¨ ¨ ¨
¨ ¨ ¨
no`no`B1
ÝÝÝÝÝÝÝÑ no ` no ` mi
no`B
ÝÝÝÝÑ no ` ni
W
ÝÑ no
A
ÝÑ mo
A1
ÝÑ ko
90

and R
`
pA1, B1q ˝ pA, Bq
˘
pWq as
ko ` ki
A1T `ki
ÝÝÝÝÝÑ mo ` ki
`ki
ÝÝÝÝÑ mo ` mo ` ki
mo`B1
ÝÝÝÝÑ mo ` mi
AT `mi
ÝÝÝÝÝÑ ¨ ¨ ¨
¨ ¨ ¨ no ` mi
`mi
ÝÝÝÝÑ no ` no ` mi
no`B
ÝÝÝÝÑ no ` ni
W
ÝÑ no
A
ÝÑ mo
A1
ÝÑ ko
so it suﬃces to check that
mo ` ki
AT `ki
ÝÝÝÝÑ no ` ki
`ki
ÝÝÝÝÑ no ` no ` ki
no`
`ki
ÝÝÝÝÝÝÑ no ` no ` no ` ki ¨ ¨ ¨
¨ ¨ ¨
no`no`A`ki
ÝÝÝÝÝÝÝÝÑ no ` no ` mo ` ki
no`no`B1
ÝÝÝÝÝÝÝÑ no ` no ` mi
“
mo ` ki
`ki
ÝÝÝÝÑ mo ` mo ` ki
mo`B1
ÝÝÝÝÑ mo ` mi
AT `mi
ÝÝÝÝÝÑ no ` mi
`mi
ÝÝÝÝÑ no ` no ` mi
which we can do using the graphical calculus:
AT
A
B1
mo
ki
no
no
mi
(1)“
AT
A
B1
AT
mo
ki
no
no
mi
¨ ¨ ¨
¨ ¨ ¨
(2)“
AT
A
B1
AT
mo
ki
no
no
mi
AT
(3)“
AT
B1
AT
mo
ki
no
no
mi
¨ ¨ ¨
¨ ¨ ¨
(4)“
AT
B1
AT
mo
ki
no
no
mi
(5)“
B1
AT
mo
ki
no
no
mi
where the equality (1) holds because AT is a comonoid morphism (Deﬁnition 3.4.23)4, (2) likewise,
(3) because A is semi-orthogonal, (4) by the coassociativity of copying, and (5) again because AT is
a comonoid morphism. Finally, we observe that the last string diagram depicts the linear map
mo ` ki
`ki
ÝÝÝÝÑ mo ` mo ` ki
AT `B1
ÝÝÝÝÑ no ` mi
`mi
ÝÝÝÝÑ no ` no ` mi
which equals the required map
mo ` ki
`ki
ÝÝÝÝÑ mo ` mo ` ki
mo`B1
ÝÝÝÝÑ mo ` mi
AT `mi
ÝÝÝÝÝÑ no ` mi
`mi
ÝÝÝÝÑ no ` no ` mi
4This in turn because ‘ is the Cartesian product, and so every morphism is a ‘-comonoid morphism.
91

by the unitality of composition. This establishes that R preserves composites; it remains to check
that it preserves identities. Once again, this follows immediately on the terms λ, α, β, γ, so we
concentrate on the action on W. We have
Rp1no, 01noqpWq “ 1noW
˜
1no
0
0
0
0
1no
¸ ¨
˚
˝
1no
0
1no
0
0
1mi
˛
‹‚
˜
1no
0
0
1mi
¸
which is easily seen to be equal to W itself. Therefore R deﬁnes a functor.
We now need to verify that the unitor and laxator satisfy the unitality and associativity
axioms of a lax monoidal functor. We begin by checking associativity, so suppose that we
are given three circuits: pλ, α, β, γ, Wq : Rpno, niq, and pλ1, α1, β1, γ1, W 1q : Rpmo, miq, and
pλ2, α2, β2, γ2, W 2q : Rpko, kiq. Associativity on all the terms but W, W 1, W 2 follows from
the associativity of the direct sum ‘, and so we just need to check that µpW, µpW 1, W 2qq “
µpµpW, W 1q, W 2q where µpW, W 1q “ WW 1 and µpW 1, W 2q “ W 1W 2, according to the deﬁnition
above. Once more, we use the graphical calculus. Observe that we can depict WW 1 and W 1W 2 as
W 1
W
no
mo
mi
ni
mo
no
and
W 2
W 1 mo
ko
ki
mi
ko
mo
respectively. Hence µpW, µpW 1, W 2qq satisﬁes the equality
µpW, µpW 1, W 2qq
“
W
no
W 1W 2
mo
ko
ki
mi
ko
mo
no
ni
“
W
no
ki
mi
ko
mo
no
ni
W 2
W 1 mo
ko
and likewise µpµpW, W 1q, W 2q satisﬁes
µpµpW, W 1q, W 2q
“
WW 1
no
W 2
mo
ko
ki
mi
ko
mo
no
ni
“
W
no
ki
mi
ko
mo
no
ni
W 2
W 1 mo
ko
.
92

The two diagrams on the right hand side are equal up to a topological deformation, and so the
depicted morphisms are equal by the coherence theorem for monoidal categories. This establishes
the associativity of the laxator. It remains to establish unitality: but this follows immediately,
because Rp0, 0q – R0 and the 0-dimensional space is the unit for the direct sum ‘. Hence pR, µ, ϵq
is a lax monoidal functor, and hence an algebra for
`
LinCirc, `, p0, 0q
˘
.
Remark 3.3.13. At points in the preceding proof, we used the fact that a linear map is a comonoid
morphism, which implies that it commutes with copying. We will deﬁne the notion of comonoid
morphism in §3.4.1 below; meanwhile, the fact that AT is one follows from the fact that ‘ is the
categorical product of vector spaces, and so every linear map is a ‘-comonoid morphism.
Remark 3.3.14. Let us return brieﬂy to the distinction made at the beginning of this section
between processes and systems, and their corresponding categorical incarnations. One might be
tempted to try constructing a symmetric monoidal category of neural circuits using this algebra
whose objects would be natural numbers and whose morphisms i Ñ o would be circuits in
Rpo, iq, treated thus as ‘processes’. But this won’t work, because there is no neural circuit that
will function as an identity morphism! Later in the thesis (§5.3), we will see one way around
this problem, building monoidal categories of hierarchical dynamical systems that are in some
sense analogous to these circuits (while being more general): there, we will use the rich structure
of polynomial functors to deﬁne both the syntax of composition as well as the hom categories
(for our construction will be bicategorical) of dynamical systems, and the extra generality will
mean we will have identity systems (that compose appropriately unitally). Until then, we note
that the moral of this observation might be that it aﬃrms that the composition of neural circuits is
multicategory-algebraic (formalizing a notion of hierarchy), rather than merely categorical.
The weight matrices resulting from the linear circuit algebra encode the pattern of connectivity
speciﬁed by the diagram, as we now exemplify.
Example 3.3.15. Let us consider the circuit example from the beginning of this section, the wiring
of an inhibitory circuit to an excitatory circuit, as in the diagram
E
I
ÞÑ
EI
which depicts a linear circuit diagram E ` I Ñ EI. In such a diagram, the input dimension of
an object (such as E) must have dimension equalling the sum of the dimensions of the incoming
wires. Dually, the dimension along a wire emanating from an object must have dimension equal to
93

the output dimension of that object. To distinguish the source and target of a wire, we decorate the
target ends: a ﬁlled circle denotes an inhibitory connection, interpreted in the linear circuit as the
negative identity matrix ´1 of the appropriate dimension; and an inverted arrowhead denotes an
excitatory connection, interpreted as the positive identity 1 of the appropriate dimension. We will
write the dimensions of the object E as poE, iEq, of I as poI, iIq, and of EI as poEI, iEIq. Therefore,
in this example, the following equalities must hold: iE “ oI ` iEI; iI “ oE; and oEI “ oE ` oI.
The last equation holds because the circuit EI is formed from the sum of the circuits E and I.
To give a circuit diagram pA, Bq : poE, iEq ` poI, iIq Ñ poEI, iEIq is to give a semi-orthogonal
real matrix A of shape poEI, oE ` oIq and a real matrix B of shape piE ` iI, oE ` oI ` iEIq. Using
the preceding equalities, these are equivalently shaped as poE ` oI, oE ` oIq and poI ` iEI `
oE, oE ` oI ` iEIq, and we just choose the identity matrix 1oE`oI for A. To deﬁne B, we read it
oﬀfrom the diagram as
B :“
¨
˚
˝
0
´1oI
0
0
0
1iEI
1oE
0
0
˛
‹‚ .
Now suppose pλE, α,E , βE, γE, WEq and pλI, α,I , βI, γI, WIq are two rate-coded neural circuits,
the former of type RpoE, iEq and the latter of type RpoI, iIq. How does RpA, Bq act upon them to
give our composite circuit?
On all the parameters but the weight matrices, RpA, Bq acts trivially (since A is just the identity
matrix), and so we will concentrate on the action on WE, WI. Firstly, we need to form the monoidal
product of the weight matrices, µpWE, WIq, which is deﬁned by
µpWE, WIq “
˜
WE
0
0
WI
¸
¨
˚
˚
˚
˝
1oE
0
0
0
0
0
1iE
0
0
1oI
0
0
0
0
0
1iI
˛
‹‹‹‚
“
˜
WE
0
0
WI
¸
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
0
0
0
1oI
0
0
0
0
0
1iEI
0
0
1oI
0
0
0
0
0
0
0
1oE
˛
‹‹‹‹‹‚
where the second equality holds by applying the equalities between the dimensions deﬁned above.
The weight matrix RpA, BqpµpWE, WIqq is then deﬁned as
AµpWE, WIq
˜
1oE`oI
0
0
B
¸ ¨
˚
˝
1oE`oI
0
1oE`oI
0
0
1iEI
˛
‹‚
˜
AT
0
0
1iEI
¸
.
94

Since A “ 1oE`oI, and by substituting the deﬁnition of µpWE, WIq, this reduces to
˜
WE
0
0
WI
¸
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
0
0
0
1oI
0
0
0
0
0
1iEI
0
0
1oI
0
0
0
0
0
0
0
1oE
˛
‹‹‹‹‹‚
˜
1oE`oI
0
0
B
¸ ¨
˚
˝
1oE`oI
0
1oE`oI
0
0
1iEI
˛
‹‚ .
Then, by substitution and matrix multiplication, we have the following equalities:
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
0
0
0
1oI
0
0
0
0
0
1iEI
0
0
1oI
0
0
0
0
0
0
0
1oE
˛
‹‹‹‹‹‚
˜
1oE`oI
0
0
B
¸ ¨
˚
˝
1oE`oI
0
1oE`oI
0
0
1iEI
˛
‹‚
“
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
0
0
0
1oI
0
0
0
0
0
1iEI
0
0
1oI
0
0
0
0
0
0
0
1oE
˛
‹‹‹‹‹‚
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
0
0
1oI
0
0
0
0
0
0
´1oI
0
0
0
0
0
1iEI
0
0
1oE
0
0
˛
‹‹‹‹‹‚
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
1oI
0
1oE
0
0
0
1oI
0
0
0
1iEI
˛
‹‹‹‹‹‚
“
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
0
0
0
1oI
0
0
0
0
0
1iEI
0
0
1oI
0
0
0
0
0
0
0
1oE
˛
‹‹‹‹‹‚
¨
˚
˚
˚
˚
˚
˝
1oI
0
0
0
1oI
0
0
´1oI
0
0
0
1iEI
1oE
0
0
˛
‹‹‹‹‹‚
“
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
´1oI
0
0
0
1iEI
0
1oI
0
1oE
0
0
˛
‹‹‹‹‹‚
so that the resulting weight matrix RpA, BqpµpWE, WIqq is
˜
WE
0
0
WI
¸
¨
˚
˚
˚
˚
˚
˝
1oE
0
0
0
´1oI
0
0
0
1iEI
0
1oI
0
1oE
0
0
˛
‹‹‹‹‹‚
.
Reading oﬀthis weight matrix, we see that the E neurons receive external input plus recurrent
excitatory input from themselves as well as inhibitory input from I, and that the I neurons receive
only recurrent excitatory input plus excitatory input from E. This is exactly as it should be,
given the diagram: by formalizing these computations, we render them mechanical (and hence
computer-implementable). In particular, we can treat the resulting EI circuit as a “black box” and
substitute it into other diagrams to construct still larger-scale systems.
95

Since linear circuit diagrams allow for any linear pattern of connectivity, we can of course
generalize the picture above to allow for more subtle interconnections.
Example 3.3.16. Suppose that instead of incorporating only excitatory or inhibitory connections,
we sought something a little more complex, as in the following circuit diagram:
E
I
D
C
ÞÑ
EI
Now, we have decorated the wires with ﬂeches, to indicate the ﬂow of activity; and besides the
circular boxes (representing circuits), we have incorporated square boxes (representing linear
patterns of interconnection). Using the same notation for the dimensions of the circuits E,I and
EI as in Example 3.3.15, this means that the boxes square boxes represent matrices C of shape
piE ` iI, n ` iEIq and D of shape pn, oE ` oIq, where n is the dimension of the D-C wire. If we
again write pA, Bq for the implied circuit diagram, and we can again set A to be the identity matrix,
and read B from the diagram as the composite matrix
B :“ oE ` oI ` iEI
D‘1iEI
ÝÝÝÝÝÑ n ` iEI
CÝÑ iE ` iI .
The rest of the calculation follows mechanically, just as before.
One feature missing from the construction in this section is synaptic plasticity: although we have
shown how to compose circuits into systems, it is only the neural ﬁring rates that are dynamical;
the connection matrices remain ﬁxed. In the preceding section, we motivated the introduction of
parameterized categories by their application to learning problems, and indeed one could factorize
the linear circuit algebra above by extracting the connection matrices into parameters; if one
wanted to retain a choice of initial weight matrix, this could also be incorporated into a ‘pointed’
version of the structure.
This parameterized construction would be bicategorical, and so a faithful semantics for it
would no longer land in Set, but rather in Cat: we would have categories of circuits related by
reparameterizations of the weight matrices, and with the dynamics also incorporating plasticity5.
With a suﬃciently sophisticated algebra, it would even be possible to allow the circuit diagrams
themselves to be dynamical and subject to learning. We will not pursue this line of enquiry further
5An even more faithful dynamical semantics would land in “bundle dynamical systems”, of the form that we introduce
in Chapter 5: two two levels of the bundle would witness the dynamics of the ﬁring activity and the plasticity, and
the bundles themselves would witness the timescale separation.
96

here, but we will return to it when we introduce plasticity into approximate inference doctrines:
there, our structures will be suﬃciently supple to incorporate all of the concepts sketched here.
3.4. From monoids to monads
In order to reach the level of suppleness required by plastic dynamical approximate inference, it will
help to understand the structures underlying the deﬁnitions and constructions introduced so far in
this chapter—in particular, we will need a ﬁrm grasp of the concepts of monad and comonoid—and
so at this point we return to an expository mode.
The fundamental concept underlying many of the structures we have seen so far is the monoid:
an object equipped with two operations, one binary and one ‘nullary’, with the latter acting as a
‘unit’ for the former, and although the major operation is only binary, it can be chained in order to
form n-ary operations. For this reason, monoids are fundamental to abstract algebra: categories
themselves are “monoids with many objects” (in the same way that a multicategory is an operad
with many objects). Both monads and comonoids can be deﬁned using monoids.
Even though monoids are fundamental and intimately familiar to mathematicians and computer
scientists, they remain underappreciated in computational and cognitive neuroscience. For this
reason, we once again take a fairly pedagogical approach in this section.
Deﬁnition 3.4.1. Suppose pC, b, Iq is a monoidal category. A monoid object in C is an object m
equipped with a multiplication morphism µ : m b m Ñ m and a unit morphism η : I Ñ m,
satisfying the axioms of (left and right) unitality
“
“
η
η
µ
µ
m
and associativity
µ
µ
µ
µ
“
.
97

If C is symmetric monoidal then we say that the monoid pm, µ, ηq is commutative if µ commutes
with the symmetry as in
“
µ
µ
.
Since we are doing category theory, it is important to understand morphisms of monoids.
Deﬁnition 3.4.2. Suppose pm, µ, ηq and pm1, µ1, η1q are monoids in pC, b, Iq. A monoid morphism
pm, µ, ηq Ñ pm1, µ1, η1q is a morphism f : m Ñ m1 in C that is compatible with the monoidal
structures, i.e. by satisfying the axioms
η
f
η1
“
and
µ
f
µ1
f
f
“
.
Monoids and their morphisms in C constitute a category MonpCq; composition and identities are
as in C, and it is easy to check that the composite of two monoid morphisms is again a monoid
morphism.
If C is symmetric monoidal, then there is a subcategory CMonpCq ãÑ MonpCq of commutative
monoids and their morphisms.
In the names MonpCq and CMonpCq, we leave the monoidal structure implicit; should it be
necessary to be explicit, we write MonbpCq and CMonbpCq.
Let us consider some ﬁrst examples of monoids in monoidal categories.
Example 3.4.3. The natural numbers N equipped with addition ` : N ˆ N Ñ N and zero 0
constitute a monoid in Set. (In fact, pN, `, 0q is the free monoid generated by a single element.)
Example 3.4.4. If A is a set, then there is a monoid
`
ListpAq, ˝, pq
˘
of lists of elements of A: the
elements of the set ListpAq are ﬁnite lists pa, b, . . . q of elements of A; the multiplication ˝ : ListpAqˆ
ListpAq Ñ ListpAq is given by concatenation of lists pb1, . . . q ˝ pa1, . . . q “ pa1, . . . , b1, . . . q; and
the unit 1 Ñ ListpAq is given by then empty list pq. We saw in the proof of Proposition 2.1.9 that
list concatenation is associative and unital.
98

Example 3.4.5. A monoid pm, ˝, ˚q in Set is a category with a single object, denoted ˚. We
already saw an example of this, in Example 2.1.7: the monoid pN, `, 0q, treated as a category. More
generally, a monoid in a monoidal category pC, b, Iq is a C-enriched category with a single object.
Example 3.4.6. A monoid pC, b, Iq in the monoidal category pCat, ˆ, 1q of categories and
functors is a strict monoidal category: the tensor is the monoid multiplication, and its unit is
the monoid unit. In fact, this explains the name “monoidal category”: a (strict) monoidal category
is a monoid object in Cat.
Remark 3.4.7. Non-strict monoidal categories are ‘weak’ in the same sense that bicategories
are weak 2-categories; after all, a monoidal category is a one-object bicategory. In this way, we
can also weaken the notion of monoid object in a bicategory, so that the axioms of unitality and
associativity only hold up to ‘coherent isomorphism’: that is, up to isomorphisms that cohere with
the weak unitality and associativity of the ambient bicategory. Such weak monoid objects are
called pseudomonoids6, and when interpreted in the monoidal 2-category pCat, ˆ, 1q their formal
deﬁnition[121, §3] yields exactly the non-strict monoidal categories.
But note that to make sense in general of the notion of pseudomonoid, we ﬁrst need to have a
notion of monoidal bicategory. Abstractly, such a thing should be a one-object tricategory, but this
often doesn’t help: in those cases, we need something more concrete. Informally, then, a monoidal
bicategory is a bicategory equipped with a monoidal structure that is coherent with the 2-cells, but
as we have begun to see here, to specify all this coherence data quickly becomes quite verbose,
and to prove their satisfaction by any given structure quite arduous, so we will only make use
informally in this thesis of the notions of monoidal bicategory and pseudomonoid — and when we
do, it will be by reference to the familiar structures on and in Cat: its Cartesian monoidal structure;
and (non-strict) monoidal categories.
Finally, we note that the general phenomenon, of which we observe an instance here, wherein
algebraic structures (such as monoids) may be deﬁned internally to categories equipped with
higher-dimensional analogues of that same structure is known as the microcosm principle[107].
In Example 3.1.8, we saw that categories of endofunctors are strict monoidal categories. Following
Example 3.4.6, this means that endofunctor categories are equivalently monoid objects. In fact,
since categories are monoids with many objects7, this means we can consider any object of
endomorphisms as an appropriately typed monoid object.
Example 3.4.8. If c : C is any object in any category C, then the hom-set Cpc, cq is a monoid
`
Cpc, cq, ˝, idc
˘
in Set. More generally, if C is enriched in E, then
`
Cpc, cq, ˝, idc
˘
is a monoid in E.
In each case, we call the monoid the endomorphism monoid on c.
6One often uses the preﬁx ‘pseudo-’ in category theory to denote a weak structure.
7This pattern—of extending structures to “many objects”—is sometimes called horizontal categoriﬁcation, to distinguish
it from the ‘vertical’ categoriﬁcation of adding an extra dimension of morphism.
99

In the case when the endomorphism objects are categories, as in the case of Example 3.1.8, the
monoidal structure makes them into monoidal categories, and so we can consider monoids objects
deﬁned internally to them. More generally, we can do this inside any bicategory, and the resulting
monoids will play an important role subsequently.
Remark 3.4.9. Just as a monoidal category is a bicategory with a single object, the hom-category
Bpb, bq for any 0-cell b in a bicategory B is a monoidal category: the objects are the 1-cells b Ñ b, the
morphisms are the 2-cells between them, composed vertically; the tensor is horizontal composition
of 1-cells, and its unit is the identity 1-cell idb. We can therefore deﬁne a monoid in a bicategory B
to be a monoid in Bpb, bq for some 0-cell b : B, using this induced monoidal structure.
Since Cat is morally a 2-category (and a fortiori a bicategory), and thus to avoid confusion with
monoid objects in pCat, ˆ, 1q (i.e. strict monoidal categories), we will introduce a new term for
monoids in the bicategory Cat.
Deﬁnition 3.4.10. A monad on the category C is a monoid object in the strict monoidal category
pCC, ˝, idCq.
Monads are often deﬁned in a more explicit way, by expressing the monoid structures and axioms
directly and diagrammatically.
Proposition 3.4.11. A monad on C is equivalently a triple pT, µ, ηq of
1. a functor T : C Ñ C;
2. a natural transformation µ : TT ñ T called the multiplication; and
3. a natural transformation η : idC ñ T called the unit;
such that, for all c : C, the following diagrams commute:
TTTc
TTc
TTc
Tc
µT c
Tµc
µc
µc
and
Tc
TTc
Tc
Tc
µc
ηT c
Tηc
A monad is like a monoidal structure for composition: instead of taking two objects and
constructing a single object representing their conjunction (like the tensor of a monoidal category),
a monad takes two levels of nesting and composes them into a single level; this is the source of the
connection between multicategory algebras and monad algebras.
100

Example 3.4.12. Recall the list monoid from Example 3.4.4. The mapping A ÞÑ ListpAq deﬁnes
the functor part of a monad List : Set Ñ Set; given a function f : A Ñ B, Listpfq : ListpAq Ñ
ListpBq is deﬁned by applying f to each element of the lists: pa1, a2, . . . q ÞÑ pfpa1q, fpa2q, . . . q.
The monad multiplication µ : List2 ñ List is given by “removing inner brackets” from lists of lists:
µA
`
pa1
1, a2
1, . . . q, pa1
2, . . . q, . . .
˘
“ pa1
1, a2
1, . . . , a1
2, . . . , . . . q; equivalently, form the perspective of
Example 3.4.4, this is the concatenation of the ‘inner’ lists into a single list. The monad unit
η : idSet ñ List is deﬁned by returning ‘singleton’ lists: ηA : A Ñ ListpAq : a ÞÑ paq.
There is a close connection between monads and adjunctions: every adjunction induces a monad.
Proposition 3.4.13. Suppose L % R : D Ñ C is an adjunction, with unit η : idC ñ RL and
counit ϵ : LR ñ idD. Then pRL, RϵL, ηq is a monad.
Proof. To see that the associativity axiom is satisﬁed, observe that RϵLRL “ RLϵRL “ RLRϵL
by naturality. Right unitality follows by the triangle identity ϵL ˝ Lη “ idL, which entails the
required equation RϵL ˝ RLη “ idRL; and left unitality follows from right unitality by naturality,
as ηRL “ RLη.
It is also true that every monad arises from an adjunction: in fact, there are typically multiple
adjunctions inducing the same monad, and we will exhibit one extremal case in §4.1.
Remark 3.4.14. This dual correspondence is itself an example of an adjunction—in the quite
general bicategorical sense, following the deﬁnition of monad as a monoid in a bicategory—though
we leave the demonstration of this to the reader.
Before we show in generality how every monad arises from an adjunction, we can exhibit the
list monad as a classic special case.
Example 3.4.15 (Lists are free monoids). There is a forgetful functor U : MonpSetq Ñ Set,
taking each monoid pM, ˝, ˚q (or monoid morphism f) and forgetting the monoid structure to
return just the set M (or the morphism f). This functor has a left adjoint F : Set Ñ MonpSetq,
which takes each set A to the free monoid on A; this free monoid FpAq is precisely the monoid
`
ListpAq, ˝, pq
˘
of lists in A, equipped with concatenation as multiplication and the empty list as
unit, as described in Example 3.4.4. The induced monad pList, µ, ηq, described in Example 3.4.12, is
then precisely the monad induced by this adjunction, with List “ UF.
At this point, with an example of a monad to hand, we can start to explore their connection to
algebra.
101

Deﬁnition 3.4.16. Suppose pT, µ, ηq is a monad on C. A T-algebra is a choice of object A : C and
a morphism a : TA Ñ A such that the following diagrams commute:
A
TA
A
ηA
a
and
TTA
TA
TA
A
a
a
µA
Ta
Once again, this being category theory, we are interested less in individual T-algebras than in
their category.
Deﬁnition 3.4.17. A morphism of T-algebras pA, aq Ñ pB, bq is a morphism f : A Ñ B that
preserves the T-algebra structures, in the sense that the following diagram commutes:
TA
TB
A
B
f
b
a
Tf
T-algebras and their morphisms constitute a category, denoted AlgpTq and called the category of
T-algebras or the Eilenberg-Moore category for T. (Algebra morphisms compose by the composition
of morphisms; a composite morphism of T-algebras is again a morphism of T-algebras by pasting.
Identities are the usual identity morphisms in C.)
We now demonstrate the ‘algebra’ of monad algebras using two familiar examples.
Example 3.4.18. The category of monoids in pSet, ˆ, 1q is equivalent to the category of List-
algebras. A List-algebra is a pair of a set A and a function a : ListpAq Ñ A satisfying the algebra
axioms, which mean that a must map singleton lists to their corresponding elements, and that a
must respect the ordering of elements in the list (so that it doesn’t matter whether you apply a to
the lists in a lists of lists, or to the collapsed list resulting from the monad multiplication). To obtain
a monoid, we can simply take the set A. The monoid multiplication is given by the action of a on
2-element lists; and the monoid unit is given by the action of a on the empty list. Since a satisﬁes
the monad algebra laws, the resulting multiplication and unit satisfy the monoid axioms: the monad
laws are a categoriﬁcation of the monoid axioms, and the algebra laws ensure compatibility with
them.
Dually, given a monoid pA, m, eq, we can construct a List algebra a by induction: on empty lists,
return e; on singleton lists, return their elements; on 2-element lists, apply m; on lists of length
n, apply m to the ﬁrst two elements to obtain a list of length n ´ 1 repeatedly until reaching the
2-element case. The monoid laws then ensure that the monad axioms are satisﬁed.
102

Example 3.4.19. Recall from Proposition 2.1.8 that one can obtain from any category a directed
graph by forgetting the compositional structure and retaining only the objects and morphisms as
nodes and edges. Recall also from Proposition 2.1.9 that one can obtain from any directed graph G
a category FG, the free category on G whose objects are the nodes of G and whose morphisms are
paths in G. These two constructions form a free-forgetful adjunction, F % U : Cat Ñ Graph,
and the induced monad UF : Graph Ñ Graph is called the path monad: on objects, it takes a
graph G and returns a graph with the same nodes but whose edges are the paths in G. The category
AlgpUFq of algebras of UF is equivalent to the category Cat of (small) categories.
To see this, note that a UF-algebra is a graph homomorphism UFG Ñ G, for some graph G:
a mapping of nodes in UFG to nodes in G, and a mapping of edges in UFG to edges in G that
preserves domains and codomains. Since UFG and G have the same nodes, the simplest choice is
to map each node to itself: we will consider the nodes as the objects of the resulting category. The
mapping of paths to edges induces a composition operation on the edges of G, which we henceforth
think of as morphisms. The reasoning proceeds inductively, much like the List-algebra case: we
take paths of length 0 to be identity morphisms; paths of length 1 are taken to their constituent
morphisms; paths of length 2 are taken to their composites; and one obtains the composites of
longer paths by induction. Associativity and unitality then follow easily from the monad algebra
laws.
Remark 3.4.20. Both the preceding examples suggest a connection between monad algebras and
inductive reasoning, and indeed one can formalize inductive reasoning (as inductive types) in terms
of algebras. Dually, there is a close connection between ‘coalgebras’ and ‘coinduction’, which can
be used to formalize the behaviours of systems that can be iterated, such as dynamical systems. As
an informal example, the coinductive type corresponding to List is the type of “streams”: possibly
inﬁnite lists of the states or outputs of transition systems. In Chapter 5, we use coalgebra to
formalize the compositional structure of ‘open’ (i.e., interacting) dynamical systems quite generally.
In the Appendix (§A.1), we pursue the monad algebra story a little further, to demonstrate the
connection with multicategory algebra. However, since that connection is not strictly germane to
the rest of the thesis, and with the suggested notion of coalgebra to whet our appetite, we now
turn to monoids’ duals, comonoids.
3.4.1. Comonoids
We introduced comonoids graphically at the beginning of §3.1.1, as a structural manifestation of
copying and discarding, but in the fullest of generality, comonoids are simply monoids in opposite
categories.
103

Deﬁnition 3.4.21. A comonoid in pC, b, Iq is a monoid in C op, when C op is equipped with the
opposite monoidal structure induced by pb, Iq. Explicitly, this means an object c : C equipped with
a comultiplication δ : c Ñ c b c and counit ϵ : c Ñ I, satisfying counitality and coassociativity laws
formally dual to the corresponding unitality and associativity laws of monoids: read the diagrams
of Deﬁnition 3.4.1 top-to-bottom, rather than bottom-to-top. Likewise, if C is symmetric monoidal,
we say that a comonoid in C is cocommutative if its comultiplication commutes with the symmetry.
Example 3.4.22. Every object in a category with ﬁnite products ˆ and a terminal object 1 is a
comonoid with respect to the monoidal structure pˆ, 1q. The comultiplications δX : X Ñ X ˆ X
are deﬁned by the pairing pidX, idXq (recall Deﬁnition 2.3.15) and the counits ϵX : X Ñ 1 are
(necessarily) the unique morphisms into the terminal object.
Coassociativity follows because pidX, pidX, idXqq “ αX,X,X ˝ ppidX, idXq, idXq, where α is the
associator of the product. Counitality follows by the naturality of pairing, pidX ˆ !q ˝ pidX, idXq “
pidX, !q, and because projX ˝ pidX, !q “ idX by the universal property of the product; note that
projX is the X component of the right unitor of the monoidal structure, and pidX, !q is its inverse.
Instantiating this example in Set, we see that the comultiplication is given by copying, i.e.,
x ÞÑ px, xq; and the counit is the unique map x ÞÑ ˚ into the singleton set. This justiﬁes our
writing of the comonoid structure in copy-discard style as p
X,
Xq.
In general, when a comonoid structure is to be interpreted as a copy-discard structure, we will
therefore write the struture morphisms as p
,
q and depict them accordingly in the graphical
calculus, rather than using the boxed forms of Deﬁnition 3.4.1. However, copy-discard structures
are not the only important comonoids that we will encounter. In the next section, we introduce
the category of polynomial functors Set Ñ Set, and since these are endofunctors, their category
inherits a monoidal structure given by functor composition. Comonoids for this monoidal structure
in Poly give us another deﬁnition for a now multifariously familiar concept: they are again small
categories, although their morphisms are not functors but rather cofunctors.
Of course, a morphism of comonoids is much like a morphism of monoids.
Deﬁnition 3.4.23. A comonoid morphism f : pc, δ, ϵq Ñ pc1, δ1, ϵ1q in pC, b, Iq is a morphism
f : c Ñ c1 that is compatible with the comonoid structures, in the sense of satisfying axioms dual
to those of Deﬁnition 3.4.2. There is thus a category ComonpCq of comonoids in C and their
morphisms, as well as a subcategory CComonpCq ãÑ ComonpCq of commutatitve comonoids.
In the more familiar copy-discard setting, comonoid morphisms also play an important role. In
the next chapter, we will see that, in the context of stochastic maps, comonoid morphisms (with
respect to the tensor product) correspond to the deterministic functions. This result is closely
related to the following fact.
104

Proposition 3.4.24. If every morphism in the monoidal category pC, b, Iq is a comonoid morphism,
then a b b satisﬁes the universal property of the product for every a, b : C, and hence b is the
categorical product and I the terminal object in C (up to isomorphism).
Proof. If every morphism is a comonoid morphism, then every object a : C carries a comonoid
structure; assume a choice of comonoid structure p
a : a Ñ a b a,
a : a Ñ Iq for every a : C.
The universal property of the product says that every morphism f : x Ñ a b b factors as
f
a
b
x
“
a
b
x
fa
fb
where fa : x Ñ a and fb : x Ñ b are uniquely deﬁned as
fa :“
f
a
x
and
fb :“
f
b
x
.
Since f is ex hypothesi a comonoid morphism, we have
f
a
b
x
“
f
x
a
b
“
f
a
f
b
x
“
a
b
x
fa
fb
where the ﬁrst equality holds by counitality, the second since f commutes with
x ex hypothesi,
and the third by deﬁnition. This establishes that a b b satisﬁes the universal property, and hence
that b is the categorical product.
To see that I is the terminal object up to isomorphism, suppose that 1 is the terminal object. Since
b is the categorical product, there is an isomorphism a „
ÝÑ a b 1 for any a : C, by the universal
property. In particular, there is an isomorphism I
„
ÝÑ I b 1. But since I is the monoidal unit for b,
the component of the left unitor at 1 is an isomorphism I b 1 „
ÝÑ 1. Hence we have a composite
isomorphism I
„
ÝÑ I b 1 „
ÝÑ 1, and so I – 1.
Of course, there is also a notion of comonad, dual to monad: a comonad is quite generally a
comonoid in a bicategory, in the sense of Remark 3.4.9, or, less generally, a comonoid with respect to
the composition product in a category of endofunctors. This means that the polynomial comonoids
we discussed above are by deﬁnition comonads.
In Remark 3.4.20, we introduced the notion of ‘coalgebra’, and indeed there is a notion of comonad
coalgebra that is dual to the notion of monad algebra; and indeed we will use coalgebras later
to formalize dynamical systems. But although these coalgebras will be morphisms of the form
105

FX Ñ X, for F an endofunctor and X an object, the endofunctor F will not necessarily be a
comonad, and so the coalgebras will be more general than the algebras we considered above: there
will be no comonad compatibility axioms to satisfy.
In many cases, the endofunctor F will be a polynomial functor, so let us now introduce these.
3.5. Polynomial functors
In order to be considered adaptive, a system must have something to adapt to. This ‘something’ is
often what we call the system’s environment, and we say that the system is open to its environment.
The interface or boundary separating the system from its environment can be thought of as
‘inhabited’ by the system: the system is embodied by its interface of interaction; the interface
is animated by the system. In this way, the system can aﬀect the environment, by changing the
shape or conﬁguration of its interface8; through the coupling, these changes are propagated to the
environment. In turn, the environment may impinge on the interface: its own changes, mediated
by the coupling, arrive at the interface as immanent signals; and the type of signals to which the
system is alive may depend on the system’s conﬁguration (as when an eye can only perceive if its
lid is open). Thus, information ﬂows across the interface.
The mathematical language capturing this kind of inhabited interaction is that of polynomial
functors, which we adopt following Spivak and Niu [122]. We will see that this language—or rather,
its category—is suﬃciently richly structured to provide both a satisfactory syntax for the patterns
of interaction of adaptive systems, generalizing the circuit diagrams of §3.3.2, as well as a home for
the dynamical semantics that we will seek.
Polynomial functors are so named because they are a categoriﬁcation of polynomial functions:
functions built from sums, products, and exponentials, of the form y ÞÑ ř
i:I bi yai. To categorify
a function of this kind, we can simply interpret the coeﬃcients and exponents and the variable
y as standing for sets rather than mere numbers. In this way, we reinterpret the term yai as
the representable copresheaf Setpai, ´q, so that we can substitute in any set X and obtain the
exponential Xai (just as in the classical case). To categorify the sums and products, we can simply
use the universal constructions available in the copresheaf category SetSet: these are still available
in the subcategory Poly, since Poly is by deﬁnition the subcategory of the copresheaf category
on sums of representables (and as we have seen, products are equivalently iterated coproducts).
Remark 3.5.1. Limits and colimits in (co)presheaf categories are computed ‘pointwise’. Therefore,
if F and G are two copresheaves C Ñ Set, then their sum F ` G is the copresheaf deﬁned by
x ÞÑ Fpxq ` Gpxq and their product is the copresheaf deﬁned by x ÞÑ Fpxq ˆ Gpxq.
8Such changes can be very general: consider for instance the changes involved in producing sound (e.g., rapid vibration
of tissue) or light (e.g., connecting a luminescent circuit, or the molecular interactions involved therein).
106

We will adopt the standard notation for polynomial functors of Spivak and Niu [122], so that
if p is a polynomial, we will expand it as ř
i:pp1q ypris. When treating p as encoding the type of a
system’s interface, we will interpret pp1q as encoding the set of possible conﬁgurations (or ‘shapes’)
that the system may adopt, and for each conﬁguration i : pp1q, the set pris is the set of possible
immanent signals (‘inputs’) that may arrive on the interface in conﬁguration i.
Deﬁnition 3.5.2. First, if A be any set, we will denote by yA its representable copresheaf yA :“
SetpA, ´q : Set Ñ Set. A polynomial functor p : Set Ñ Set is then an indexed coproduct of
such representable copresheaves, written p :“ ř
i:pp1q ypi, where pp1q : E denotes the indexing
object and pris the representing object for each i. The category of polynomial functors is the full
subcategory Poly ãÑ SetSet of the copresheaf category spanned by coproducts of representables.
A morphism of polynomials is thus a natural transformation.
Remark 3.5.3. Note that, given a polynomial functor p : Set Ñ Set, the indexing set pp1q is
indeed obtained by applying p to the terminal set 1.
We will make much use of the following ‘bundle’ representation of polynomial functors and
their morphisms.
Proposition 3.5.4. Every polynomial functor ř
i:pp1q ypi corresponds to a bundle (a function)
p : ř
i:pp1q pi Ñ pp1q, where the set ř
i:pp1q pi is the pp1q-indexed coproduct of the representing
objects pi and p is the projection out of the coproduct onto the indexing set pp1q.
Every morphism of polynomials f : p Ñ q corresponds to a pair pf1, f7q of a function f1 :
pp1q Ñ qp1q and a pp1q-indexed family of functions f7
i : qrf1piqs Ñ pris making the diagram
below commute. We adopt the notation pris :“ pi, and write f7 to denote the coproduct ř
i f7
i .
ř
i:pp1q pris
ř
i:pp1q qrf1piqs
ř
j:qp1q qrjs
pp1q
pp1q
qp1q
f7
q
p
f1
{
Given f : p Ñ q and g : q Ñ r, their composite g ˝ f : p Ñ r is as marked in the diagram
ř
i:pp1q pris
ř
i:pp1q rrg1 ˝ f1piqs
ř
k:rp1q rrks
pp1q
pp1q
rp1q
pgfq7
r
p
g1˝f1
{
where pgfq7 is the coproduct of the pp1q-indexed family of composite maps
rrg1pf1piqqs
f˚g7
ÝÝÝÑ qrf1piqs
f7
ÝÑ pris .
The identity morphism on a polynomial p is pidpp1q, idq.
107

Proof. We just need to show that every natural transformation between polynomial functors
corresponds to a pair of maps pf1, f7q as deﬁned above.
The set of natural transforma-
tions ř
i:pp1q ypris ñ ř
j:qp1q yqrjs is the hom-set SetSet`ř
i:pp1q ypris, ř
j:qp1q yqrjs˘
. Since the
contravariant hom functor takes colimits to limits (Remark 2.3.53), this hom-set is isomor-
phic to ś
i:pp1q SetSetpypris, ř
j:qp1q yqrjsq. By the Yoneda lemma, this is in turn isomorphic
to ś
i:pp1q
ř
j:qp1q prisqrjs.
And since products distribute over sums, we can rewrite this as
ř
f1:pp1qÑqp1q
ś
i:pp1q prisqrf1piqs. The elements of this set are precisely pairs of a function f1 :
pp1q Ñ qp1q along with a family of functions qrf1piqs Ñ pris indexed by i : pp1q, such that the
diagram above commutes.
We now recall a handful of useful facts about polynomials and their morphisms, each of which
is explained in Spivak and Niu [122] and summarized in Spivak [123].
We will consider the unit polynomial y to represent a ‘closed’ system, since it has no nontrivial
conﬁgurations and no possibility of external input. For this reason, morphisms p Ñ y will represent
ways to make an open system closed, and in this context the following fact explains why: such
morphisms correspond to a choice of possible input for each p-conﬁguration; that is, they encode
“how the environment might respond to p”.
Proposition 3.5.5. Polynomial morphisms p Ñ y correspond to sections pp1q Ñ ř
i pris of the
corresponding function p : ř
i pris Ñ pp1q.
The following embedding of Set into Poly will be useful both in constructing ‘hierarchical’
dynamical systems.
Proposition 3.5.6. There is an embedding of Set into Poly given by taking sets X to the linear
polynomials Xy : Poly and functions f : X Ñ Y to morphisms pf, idXq : Xy Ñ Y y.
There are many monoidal structures on Poly, but two will be particularly important for us. The
ﬁrst represents the parallel composition of systems.
Proposition 3.5.7. There is a symmetric monoidal structure pb, yq on Poly that we call tensor,
and which is given on objects by pbq :“ ř
i:pp1q
ř
j:qp1q yprisˆqrjs and on morphisms f :“ pf1, f7q :
p Ñ p1 and g :“ pg1, g7q : q Ñ q1 by f b g :“
`
f1 ˆ g1, Σpf7, g7q
˘
, where Σpf7, g7q is the family
of functions
Σpf7, g7qi,j :“ p1rf1piqs ˆ q1rg1pjqs
f7
i ˆg7
j
ÝÝÝÝÑ pris ˆ qrjs
indexed by pi, jq : pp1q ˆ qp1q. This is to say that the ‘forwards’ component of f b g is the product
of the forwards components of f and g, while the ‘backwards’ component is the pointwise product
of the respective backwards components.
108

Proposition 3.5.8. pPoly, b, yq is symmetric monoidal closed, with internal hom denoted r´, “s.
Explicitly, we have rp, qs “ ř
f:pÑq y
ř
i:pp1q qrf1piqs. Given an set A, we have rAy, ys – yA.
The second important monoidal structure is that inherited from the composition of endofunctors.
To avoid confusion with other composition operators, we will in this context denote the operation
by ◁.
Proposition 3.5.9. The composition of polynomial functors q˝p : E Ñ E Ñ E induces a monoidal
structure on PolyE, which we denote ◁, and call ‘composition’ or ‘substitution’. Its unit is again y.
Proposition 3.5.10 ([124, §3.2]). Comononids in pPoly, ◁, yq correspond to categories, and their
comonoid morphisms are ‘cofunctors’.
The following ◁-comonoids will play a particularly prominent role in the development of
dynamical systems.
Proposition 3.5.11. If T is a monoid in pSet, ˆ, 1q, then the comonoid structure on yT witnesses
it as the category BT.
Proposition 3.5.12. Monomials of the form SyS can be equipped with a canonical comonoid
structure witnessing the codiscrete groupoid on S.
109

4. The compositional structure of Bayesian
inference
This chapter introduces the fundamental concepts and structures needed for the development of
Bayesian lenses and statistical games in Chapter 6, and proves the motivating result, that Bayesian
updating composes according to the ‘lens’ pattern. To make sense of this statement, we ﬁrst
introduce compositional probability (§4.1), motivating it as a resolution of some imprecision that
arises when one works informally with probability and statistics, particularly in the context of
‘hierarchical’ models. We exhibit categorical probability theory both abstractly (§4.1.2 and §4.1.3)
and concretely (using discrete probability in §4.1.1 and ‘continuous’ probability in §4.1.4). We then
move on to construct categories of bidirectional processes in §4.2, by ﬁrst categorifying our earlier
discussion of dependent data using the Grothendieck construction §4.2.1 and then using this to
introduce the lens pattern §4.2.2. Finally, in §4.3, we present our new results. First, we introduce
the indexed category of “state-dependent channels” in §4.3.1. These formalize the type of Bayesian
inversions, and so in §4.3.2 we deﬁne the associated notion of Bayesian lens, and show in §4.3.3
that Bayesian updating composes according to the lens pattern. We end with a brief discussion of
the ‘lawfulness’ of Bayesian lenses.
4.1. Compositional probability
In informal scientiﬁc literature, Bayes’ rule is often written in the following form:
PpA|Bq “ PpB|Aq ¨ PpAq
PpBq
where PpAq is the probability of the event A, and PpA|Bq is the probability of the event A given
that the event B occurred; and vice versa swapping A and B. Unfortunately, this notation obscures
that there is in general no unique assignment of probabilities to events: diﬀerent observers can hold
diﬀerent beliefs. Moreover, we are usually less interested in the probability of particular events
than in the process of assigning probabilities to arbitrarily chosen beliefs; and what should be done
if PpBq “ 0 for some B? The aim in this section is to exhibit a general, precise, and compositional,
form of Bayes’ rule; we begin, as before, by introducing the intuition.
In the categorical setting, the assignment of probabilities or beliefs to events will formally be the
task of a state (in the sense of §3.1.1) on the space from which the events are drawn; we should
110

think of states as generalizing distributions or measures. With this notion to hand, we can write
PπpAq to denote the probability of A according to the state π.
The formalization of conditional probability will be achieved by morphisms that we will call
channels, meaning that we can write PcpB|Aq to denote the probability of B given A according to
the channel c. We can think of the channel c as taking events A as inputs and emitting states cpAq
as outputs. This means that we can alternatively write PcpB|Aq “ PcpAqpBq.
If the input events are drawn from the space X and the output states encode beliefs about Y ,
then the channel c will be a morphism XÑ
‚ Y . Given a channel c : XÑ
‚ Y and a channel d : Y Ñ
‚ Z,
we will understand their composite d ‚ c : XÑ
‚ Z as marginalizing (averaging) over the possible
outcomes in Y . We will see precisely how this works in various settings below.
Notation 4.1.1. In a stochastic context, we will denote channels by the arrow Ñ
‚ , and write
their composition operator as ‚. We do this to distinguish stochastic channels from deterministic
functions, which we will continue to write as Ñ with composition ˝; in a number of situations, it
will be desirable to work with both kinds of morphism and composition.
Given two spaces X and Y of events, we can form beliefs about them jointly, represented by
states on the product space denoted X b Y . The numerator in Bayes’ rule represents such a joint
state, by the law of conditional probability or ‘product rule’:
PωpA, Bq “ PcpB|Aq ¨ PπpAq
(4.1)
where ¨ is multiplication of probabilities, π is a state on X, and ω denotes the joint state on X b Y .
By composing c and π to form a state c ‚ π on Y , we can write
Pω1pB, Aq “ Pc:
πpA|Bq ¨ Pc‚πpBq
where c:
π will denote the Bayesian inversion of c with respect to π.
Joint states in classical probability theory are symmetric—and so the tensor b is symmetric—
meaning that there is a family of isomorphisms swap : X b Y „
ÝÑ
‚ Y b X, as in §3.1.1, and which
will satisfy the symmetric monoidal category axioms (Deﬁnition 3.1.3). Consequently, we have
PωpA, Bq “ Pω1pB, Aq where ω1 “ swap ‚ ω, and thus
PcpB|Aq ¨ PπpAq “ Pc:
πpA|Bq ¨ Pc‚πpBq
(4.2)
where both left- and right-hand sides are called disintegrations of the joint state ω [87]. From this
equality, we can write down the usual form of Bayes’ theorem, now with the sources of belief
indicated:
Pc:
πpA|Bq “ PcpB|Aq ¨ PπpAq
Pc‚πpBq
.
(4.3)
111

As long as Pc‚πpBq ‰ 0, this equality deﬁnes the inverse channel c:
π. If the division is undeﬁned,
or if we cannot guarantee Pc‚πpBq ‰ 0, then c:
π can be any channel satisfying (4.2).
There is therefore generally no unique Bayesian inversion c: : Y Ñ
‚ X for a given channel
c : XÑ
‚ Y : rather, we have an inverse c:
π : Y Ñ
‚ X for each prior state π on X. Moreover, c:
π is not
a “posterior distribution” (as written in some literature), but a channel which emits a posterior
distribution, given an observation in Y . If we denote our category of stochastic channels by C, then,
by allowing π to vary, we obtain a map of the form c:
p¨q : PX Ñ CpY, Xq, where PX denotes a
space of states on X. Note that here we are not assuming the object PX to be an object of C itself
(though it often will be), but rather an object in its base of enrichment, so that here we can think of
c:
p¨q as a kind of externally parameterized channel (in the sense of §3.2.2). Making the type of this
‘state-dependent’ channel c:
p¨q precise is the task of §4.2.1.
Remark 4.1.2. There are two easily confused pieces of terminology here. We will call the channel
c:
π the Bayesian inversion of the channel c with respect to π. Then, given some y P Y , the state
c:
πpyq is a new ‘posterior’ distribution on X. We will call c:
πpyq the Bayesian update of π along c
given y.
In the remainder of this section, we instantiate the ideas above in categories of stochastic channels
of various levels of generality, beginning with the familiar case of discrete (ﬁnitely supported, or
‘categorical’) probability.
4.1.1. Discrete probability, algebraically
Interpreting the informal Bayes’ rule (4.3) is simplest in the case of discrete or ﬁnitely-supported
probability. Here, every event is a set, generated as the disjoint union of so many atomic (singleton)
events, which one can therefore take as the elements of the set. A ﬁnitely-suported probability
distribution is then simply an assignment of nonzero probabilities to ﬁnitely many elements, such
that the sum of all the assignments is 1. This condition is a convexity condition, and so in this
subsection we will introduce discrete compositional probability theory from a geometric perspective,
using the algebraic tools of the previous chapter.
Deﬁnition 4.1.3. Suppose X is a set. A function c : X Ñ r0, 1s such that cpxq ą 0 for only
ﬁnitely many x : X and ř
x:X cpxq “ 1 will be called a discrete or ﬁnitely-supported distribution on
X. We write DX to denote the set of discrete distributions on X. A (real-valued) convex set is a set
X equipped with a function EX : DX Ñ X called its expectation.
Convex sets X are sets in which we can form convex combinations of elements. Algebraically,
we can model these convex combinations as distributions on X, and the expectations realize the
convex combinations (distributions) as elements again of X: geometrically, the expectation function
returns the barycentre of the distribution.
112

In light of Chapter 3, this situation may seem familiar. Indeed, the assignment X Ñ DX is
the functor part of a monad on Set, whose algebras are convex sets. This monad arises from a
free-forgetful adjunction between the category of convex sets (the category of algebras of the
monad) and the category Set. Later, we will ﬁnd that the category of ﬁnitely-supported conditional
probability distributions—the category of discrete stochastic channels—is equivalent to the category
of free convex sets and their morphisms: a free convex set on X is equivalently a distribution on X.
Let us ﬁrst formalize the functor D.
Proposition 4.1.4. The mapping of sets X ÞÑ DX is functorial. Given a function f : X Ñ Y , we
obtain a function Df : DX Ñ DY mapping c : DX to the distribution Dfpcq : DY ,
Dfpcq : Y Ñ r0, 1s
y ÞÑ
ÿ
x:fpxq“y
cpxq .
Proof. Given f : X Ñ Y and g : Y Ñ Z, we have
DgpDfpcqq : Z Ñ r0, 1s
z ÞÑ
ÿ
y:gpyq“z
ÿ
x:fpxq“y
cpxq
“
ÿ
x:g˝fpxq“z
cpxq
hence Dg ˝ Df “ Dpg ˝ fq. We also have
DpidXqpcq : X Ñ r0, 1s
x ÞÑ
ÿ
x1:idXpx1q“x
cpx1q
“ cpxq
and hence Dpidq “ idD.
To obtain the monad structure of D, we will exhibit the free-forgetful adjunction. We start by
deﬁning the category of convex sets, and the speciﬁc case of a free convex set.
Deﬁnition 4.1.5. The category of (real-valued) convex sets Conv has convex sets pX, EXq as
objects. Its morphisms pX, EXq Ñ pY, EY q are functions f : X Ñ Y that preserve the convex
structure, in the sense that the following square commutes:
DX
DY
X
Y
Df
f
EX
EY
113

Deﬁnition 4.1.6. If X is any set, then the free convex set on X is the set DX equipped with the
expectation µX : DDX Ñ DX which maps α : DDX to the distribution µXpαq : DX,
µXpαq : X Ñ r0, 1s
x ÞÑ
ÿ
c:DX
αpcq ¨ cpxq .
Notation 4.1.7. To emphasize the algebraic nature of ﬁnitely-supported distributions π : DX,
instead of writing them as functions x ÞÑ πpxq, we can write them as formal sums or formal convex
combinations ř
x:X πpxq |xy, with each element x : X corresponding to a formal “basis vector” |xy
with the coeﬃcient πpxq. If X is a convex set, then the expectation realizes this formal sum as an
actual element (‘vector’) in X.
We are now in a position to exhibit the adjunction: the reasoning behind the following proposition
follows the lines of Example 3.4.19 and Proposition 2.2.25 (on the free-forgetful adjunction between
graphs and categories).
Proposition 4.1.8. The mapping of X : Set to the free convex set pDX, µXq deﬁnes a functor
F : Set Ñ Conv which takes functions f : X Ñ Y to morphisms Ff : pDX, µXq Ñ pDY, µY q
deﬁned by Df : DX Ñ DY . This functor F is left adjoint to the forgetful functor U : Conv Ñ
Set which acts by pX, EXq ÞÑ X.
Using Proposition 3.4.13, the adjunction gives us a monad.
Corollary 4.1.9. The functor D : Set Ñ Set is equivalently the functor part of the monad
induced by the free-forgetful adjunction on convex sets. It therefore acquires a monad structure
pµ, ηq where the components of the multiplication µ are the free expectations µX : DDX Ñ DX,
and the unit η has components ηX : X Ñ DX which return the ‘Dirac’ distributions, as in
ηXpxq : X Ñ r0, 1s
x1 ÞÑ
#
1
iﬀx “ x1
0
otherwise.
And Conv is the category of algebras for this monad.
Corollary 4.1.10. Conv – AlgpDq.
Using Corollary 4.1.10, we can actually exhibit the relationship between the monad D and
its deﬁning adjunction tautologously: every monad T on a category C induces an free-forgetful
adjunction between its category of algebras AlgpTq and C itself, such that the monad generated
by this adjunction is again T. This is precisely the situation here.
114

Proposition 4.1.11. Suppose pT, µ, ηq is a monad on the category C. There is a forgetful functor
U : AlgpTq Ñ C which has a left adjoint F : C Ñ AlgpTq taking each object X : C to the free
T-algebra pTX, µXq on X, where µX : TTX Ñ TX is the component of the monad multiplication
µ at X. The unit of the adjunction is the monadic unit η, the counit ϵ is deﬁned by ϵpX,EXq :“ EX,
and the monad induced by the adjunction is pT, µ, ηq.
Proof sketch. The proof that F is left adjoint to U is standard (see Borceux [125, Prop. 4.1.4]), and
that the adjunction generates the monad follows almost immediately from Proposition 3.4.13.
Remark 4.1.12. It must be emphasized that, although every monad arises from such a free-forgetful
adjunction, not every adjunction does! (Consider for example the adjunction ∆% lim of Proposition
2.3.48: ∆does not assign to each c : C the “free J-shaped diagram on c”, and lim does not simply
forget diagrammatic structure.) Those adjunctions which do arise from monads in this way are
called monadic.
There is a special name for subcategories of free algebras.
Deﬁnition 4.1.13. Suppose pT, µ, ηq is a monad on C. The subcategory of AlgpTq on the free
T-algebras pTX, µXq is called the Kleisli category for T, and denoted KℓpTq.
The following proposition gives us an alternative presentation of KℓpTq which, when applied to
the monad D, will yield a computationally meaningful category of ﬁnitely-supported stochastic
channels.
Proposition 4.1.14. The objects of KℓpTq are the objects of C. The morphisms XÑ
‚ Y of KℓpTq
are the morphisms X Ñ TY of C. Identity morphisms idX : XÑ
‚ X are given by the monadic unit
ηX : X Ñ TX. Composition is deﬁned by Kleisli extension: given g : Y Ñ
‚ Z, we form its Kleisli
extension g▷: TY Ñ
‚ Z as the composite TY
Tg
ÝÝÑ TTZ
µZ
ÝÝÑ TZ in C. Then, given f : XÑ
‚ Y , we
form the composite g ‚ f : XÑ
‚ Z as g▷˝ f: X
fÝÑ TY
Tg
ÝÝÑ TTZ
µZ
ÝÝÑ TZ.
Proof. Observe that there is a bijection between the objects X of C and the free T-algebras pTX, µXq.
We therefore only need to establish a bijection between the hom-sets AlgpTq
`
pTX, µXq, pTY, µY q
˘
and KℓpTqpX, Y q, with the latter deﬁned as in the statement of the proposition.
First, we demonstrate that Kleisli extension deﬁnes a surjection
KℓpTqpX, Y q Ñ AlgpTq
`
pTX, µXq, pTY, µY q
˘
.
115

Suppose φ is any algebra morphism pTX, µXq Ñ pTY, µY q; we show that it is equal to the Kleisli
extension of the Kleisli morphism X
ηX
ÝÝÑ TX
φÝÑ TY :
TX
pφ˝ηXq▷
ÝÝÝÝÝÑ TY “ TX
TηX
ÝÝÝÑ TTX
Tφ
ÝÝÑ TTY
µT Y
ÝÝÝÑ TY
“ TX
TηX
ÝÝÝÑ TTX
µT X
ÝÝÝÑ TX
φÝÑ TY
“ TX
idT X
ùùùùù TX
φÝÑ TY
“ TX
φÝÑ TY
where the ﬁrst equality holds by deﬁnition, the second line by naturality of µ, and the third by
the unitality of the monad pT, µ, ηq. Hence every free algebra morphism is in the image of Kleisli
extension, and so Kleisli extension deﬁnes a surjection.
Next, we show that this surjection is additionally injective. Suppose f, g are two Kleisli morphisms
X Ñ TY such that their Kleisli extensions are equal
TX
Tf
ÝÝÑ TTY
µT Y
ÝÝÝÑ TY “ TX
Tg
ÝÝÑ TTY
µT Y
ÝÝÝÑ TY
and recall that the identity in KℓpTq is η. We therefore have the following equalities:
X
ηX
ÝÝÑ TX
Tf
ÝÝÑ TTY
µT Y
ÝÝÝÑ TY “ X
ηX
ÝÝÑ TX
Tg
ÝÝÑ TTY
µT Y
ÝÝÝÑ TY
“ X
fÝÑ TY
Tηy
ÝÝÑ TTY
µT Y
ÝÝÝÑ TY “ X
gÝÑ TY
Tηy
ÝÝÑ TTY
µT Y
ÝÝÝÑ TY
“ X
fÝÑ Y
“ X
gÝÑ Y .
where the equality in the ﬁrst line holds ex hypothesi, the second by naturality, and the third
by monadic unitality. Since f “ g when their Kleisli extensions are equal, Kleisli extension
is injective. Since it is also surjective, we have an isomorphism between KℓpTqpX, Y q and
AlgpTq
`
pTX, µXq, pTY, µY q
˘
. Hence KℓpTq is the subcategory of AlgpTq on the free alge-
bras.
If T is a monad on C, there is a canonical embedding of C into KℓpTq. In the case of KℓpDq, this
will yield the subcategory of deterministic channels: those which do not add any uncertainty.
Proposition 4.1.15. Suppose T is a monad on C. Then there is an identity-on-objects embedding
C ãÑ KℓpTq given on morphisms by mapping f : X Ñ Y in C to the Kleisli morphism X
ηX
ÝÝÑ
TX
Tf
ÝÝÑ TY .
Proof sketch. Functoriality follows from the unitality of η in the monad structure, since Kleisli
composition involves post-composing the monad multiplication, and µT ˝ Tη “ id.
116

4.1.1.1. Stochastic matrices
At this point, let us exhibit KℓpDq a little more concretely, by instantiating Proposition 4.1.14. Since
a distribution π on the set X is a function X Ñ r0, 1s, and following the “formal sum” intuition, we
can alternatively think of π as a vector, whose coeﬃcients are indexed by elements of X (the basis
vectors |xy). Morphisms XÑ
‚ Y in KℓpDq are functions X Ñ DY , and so we can similarly think of
these as stochastic matrices, by the Cartesian closure of Set: a function X Ñ DY is equivalently
a function X Ñ r0, 1sY , which in turn corresponds to a function X ˆ Y Ñ r0, 1s, which we can
read as a (left) stochastic matrix, with only ﬁnitely many nonzero coeﬃcients and each of whose
columns must sum to 1. We will adopt ‘conditional probability’ notation for the coeﬃcients of
these matrices: given p : XÑ
‚ Y , x P X and y P Y , we write ppy|xq :“ ppxqpyq P r0, 1s for “the
probabilty of y given x, according to p”.
Composition in KℓpDq is then matrix multiplication: given p : X Ñ DY and q : Y Ñ DZ, we
compute their composite q ‚ p : X Ñ DZ by ‘averaging over’ or ‘marginalizing out’ Y via the
Chapman-Kolmogorov equation:
q ‚ p : X Ñ DZ :“ x ÞÑ
ÿ
z:Z
ÿ
y:Y
qpz|yq ¨ ppy|xq |zy .
Here we have again used the formal sum notation, drawing a box to indicate the coeﬃcients (i.e.,
the probabilities returned by the conditional distribution q ‚ ppxq for each atomic event z in Z).
Via the monadic unit, identity morphisms idX : XÑ
‚ X in KℓpDq take points to ‘Dirac delta’
distributions: idX :“ x ÞÑ 1 |xy. The embedding Set ãÑ KℓpDq makes any function f : Y Ñ X
into a (deterministic) channel f “ ηX ˝ f : Y Ñ DX by post-composing with ηX.
4.1.1.2. Monoidal structure
We will want to equip KℓpDq with a copy-discard category structure, in order to represent joint
states (joint distributions) and their marginalization, as well as the copying of information. The ﬁrst
ingredient making a copy-discard category, after the category itself, is a monoidal structure. Once
again, in the case of KℓpDq, this can be obtained abstractly from a more fundamental structure—the
categorical product pˆ, 1q on Set—as a consequence of D being a ‘monoidal’ monad. We will write
the induced tensor product on KℓpDq as b; its monoidal unit remains the object 1.
Deﬁnition 4.1.16. A monoidal monad is a monad in MonCat. This means that it is a monad
pT, µ, ηq in Cat whose functor T : C Ñ C is additionally equipped with a lax monoidal structure
pα, ϵq such that the monad multiplication µ and unit η are monoidal natural transformations
accordingly.
With this extra structure, it is not hard to verify that the following proposition makes KℓpTq
into a well-deﬁned monoidal category.
117

Proposition 4.1.17. The Kleisli category KℓpTq of a monoidal monad pT, α, ϵ, µ, ηq is a monoidal
category. The monoidal product is given on objects by the monoidal product b of the base category
C. On Kleisli morphisms f : XÑ
‚ Y and f1 : X1Ñ
‚ Y 1, their tensor f b g is given by the following
composite in C:
X b X1 fbf1
ÝÝÝÑ TX b TX1 αX,X1
ÝÝÝÝÑ TpX b X1q
The monoidal unit is the monoidal unit I in C. The associator and unitor of the monoidal category
structure are inherited from C under the embedding C ãÑ KℓpTq. When pC, b, Iq is symmetric
monoidal, then so is pKℓpTq, b, Iq.
In the speciﬁc case of KℓpDq, the tensor product b is given on objects by the product of sets and
on stochastic channels f : X Ñ DA and g : Y Ñ DB as
X ˆ Y
fˆg
ÝÝÑ DA ˆ DB
αA,B
ÝÝÝÑ DpA ˆ Bq .
Note that because not all joint states have independent marginals, the monoidal product b is not
Cartesian: that is, given an arbitrary ω : DpX b Y q, we do not necessarily have ω “ pρ, σq for
some ρ : DX and σ : DY . The laxator takes a pair of distributions pρ, σq in DX ˆ DY to the joint
distribution on X ˆ Y given by px, yq ÞÑ ρpxq ¨ σpyq; ρ and σ are then the (independent) marginals
of this joint distribution. (Of course, the joint distribution pρ, σq is not the only joint distribution
with those marginals: other joint states may have these marginals but also correlations between
them, and this is what it means for not all joint states to have independent marginals.)
Since pSet, ˆ, 1q is symmetric monoidal, KℓpDq is too, with swap isomorphisms swapX,Y :
X b Y „
ÝÑ
‚ Y b X similarly inherited form those of ˆ.
4.1.1.3. Copy-discard structure
The copy-discard structure in KℓpDq is inherited from Set through its embedding: since every
object in KℓpDq is an object in Set, and every object in Set is a comonoid (Example 3.4.22), and
since functors preserve equalities, these comonoid structures are preserved under the embedding.
More explicitly, the discarding channels
X are given by x ÞÑ 1 |˚y, and the copiers
X by
x ÞÑ 1 |x, xy. Note that the copiers are not natural in KℓpDq: in general,
‚ f ‰ f b f ‚
, as a
result of the possibility of correlations.
Since the projections projX : X ˆ Y Ñ X in Set satisfy projX “ ρX ˝ pidX ˆ
Y q where
ρX : X ˆ 1 Ñ X is component of the right unitor, we can see how discarding and projection
give us marginalization, thereby explaining the string diagrams of §3.1.1. Given some joint state
ω : 1Ñ
‚ X b Y , its X-marginal ωX : 1Ñ
‚ X is given by projX ‚ ω, which in KℓpDq is given by the
formal sum formula ř
x:X
ř
y:Y ωpx, yq |xy, where we have again drawn a box to distinguish the
probability assigned to |xy, which we note coincides with the classical rule for marginal discrete
probability. (The Y -marginal is of course symmetric.)
118

Remark 4.1.18. A semicartesian category is a monoidal category in which the monoidal unit is
terminal. In a semicartesian monoidal category, every tensor product X b Y is equipped with a
natural family of projections projX : X b Y Ñ X and projY : X b Y Ñ Y given by ‘discarding’
one of the factors and using the unitor; the existence of such projections is not otherwise implied
by a monoidal structure (though of course it does follow when the tensor is the product).
A related notion is that of an aﬃne functor, which is one that preserves the terminal object, and
of which D is an example. As a result, and following the discussion above, we can see that KℓpDq
is an example of a semicartesian category.
Semicartesian copy-discard categories are also known as Markov categories, following Fritz [126].
Remark 4.1.19. Since 1 is therefore terminal in KℓpDq, Proposition 3.4.24 tells us that those
channels f that do commute with copying (i.e., for which
is natural), and which are therefore
comonoid morphisms, are precisely the deterministic channels: those in the image of the embedding
of Set (and which therefore emit Dirac delta distributions).
As a result, we can think of
ComonpKℓpDqq as the subcategory of deterministic channels, and write ComonpKℓpDqq – Set.
(Intuitively, this follows almost by deﬁnition: a deterministic process is one that has no informational
side-eﬀects; that is to say, whether we copy a state before performing the process on each copy, or
perform the process and then copy the resulting state, or whether we perform the process and then
marginalize, or just marginalize, makes no diﬀerence to the resulting state.)
4.1.1.4. Bayesian inversion
We can now instantiate Bayesian inversion in KℓpDq, formalizing Equation (4.3). Given a channel
p : X Ñ DY satisfying the condition in Remark 4.1.20 below, its Bayesian inversion is given by
the function
p: : DX ˆ Y Ñ DX :“ pπ, yq ÞÑ
ÿ
x:X
ppy|xq ¨ πpxq
ř
x1:X ppy|x1q ¨ πpx1q |xy “
ÿ
x:X
ppy|xq ¨ πpxq
pp ‚ πqpyq
|xy
(4.4)
so that the Bayesian update of p along π is the conditional distribution deﬁned by
p:
πpx|yq “ ppy|xq ¨ πpxq
pp ‚ πqpyq
.
Note that here we have used the Cartesian closure of Set, writing the type of p: as DX ˆY Ñ DX
rather than DX Ñ KℓpDqpY, Xq, where KℓpDqpY, Xq “ pDXqY .
Remark 4.1.20. In the form given above, p: is only well-deﬁned when the support of p ‚ π is the
whole of Y , so that, for all y, pp ‚ πqpyq ą 0; otherwise, the division is ill-deﬁned. Henceforth,
in the context of Bayesian inversion, we will therefore assume that p ‚ π has full support (see
Deﬁnition 4.1.21).
119

To avoid this (rather ugly) condition, one can replace it by the assumption that the notion of
‘support’ is well-deﬁned, and modify the type of p: accordingly: this is the reﬁnement made by
Braithwaite and Hedges [127], and were it not for the presently-uncertain nature of support objects
in general, it would now be this author’s preferred approach. This leads to writing the type of
the inversion p: as ř
π:DX supppp ‚ πq Ñ DX, where supppp ‚ πq is the subobject of Y on which
p ‚ π is supported: with this type, p:
π is always a well-deﬁned channel. One can then proceed with
the deﬁnition of ‘dependent’ Bayesian lenses accordingly; for the details, we refer the reader to
Braithwaite and Hedges [127]. In this thesis, for simplicity of exposition and faithfulness to this
author’s earlier work, we will proceed under the full-support assumption.
4.1.2. Abstract Bayesian inversion
Beyond the concerns of Remark 4.1.20, in a more general setting it is not always possible to deﬁned
Bayesian inversion using an equation like Equation (4.4) or Equation (4.3): the expression ppy|xq
might not be well-deﬁned, or there might not be a well-deﬁned notion of division. Instead being
guided by Equation (4.3) in deﬁning Bayesian inversion, we can use Equation (4.2). Therefore,
supposing a channel c : XÑ
‚ Y and a state π : IÑ
‚ X in an ambient copy-discard category C, we
can ask for the Bayesian inversion c:
π to be any channel satisfying the graphical equality [87, eq. 5]:
c
π
X
Y
“
c:
π
π
c
X
Y
(4.5)
This diagram can be interpreted as follows. Given a prior π : IÑ
‚ X and a channel c : XÑ
‚ Y , we
form the joint distribution ω :“ pidX b cq ‚
X ‚ π : IÑ
‚ X b Y shown on the left hand side: this
formalizes the product rule, PωpA, Bq “ PcpB|Aq¨PπpAq, and π is the corresponding X-marginal.
As in the concrete case of KℓpDq, we seek an inverse channel Y Ñ
‚ X witnessing the ‘dual’ form of
the rule, PωpA, Bq “ PpA|Bq ¨ PpBq; this is depicted on the right hand side. By discarding X, we
see that c ‚ π : IÑ
‚ Y is the Y -marginal witnessing PpBq. So any channel c:
π : Y Ñ
‚ X witnessing
PpA|Bq and satisfying the equality above is a Bayesian inverse of c with respect to π.
In light of Remark 4.1.20, we therefore make the following deﬁnition.
120

Deﬁnition 4.1.21. We say that a channel c : XÑ
‚ Y admits Bayesian inversion with respect to
π : IÑ
‚ X if there exists a channel c:
π : Y Ñ
‚ X satisfying equation (4.5). We say that c admits
Bayesian inversion tout court if c admits Bayesian inversion with respect to all states π : IÑ
‚ X
such that c ‚ π has full support.
4.1.3. Density functions
Abstract Bayesian inversion (4.5) generalizes the product rule form of Bayes’ theorem (4.2) but
in most applications, we are interested in a speciﬁc channel witnessing PpA|Bq “ PpB|Aq ¨
PpAq{ PpBq. In the typical measure-theoretic setting, this is often written informally as
ppx|yq “ ppy|xq ¨ ppxq
ppyq
“
ppy|xq ¨ ppxq
ş
x1:X ppy|x1q ¨ ppx1q dx1
(4.6)
but the formal semantics of such an expression are not trivial: for instance, what is the object
ppy|xq, and how does it relate to a channel c : XÑ
‚ Y ?
We can interpret ppy|xq as a density function for a channel, abstractly witnessed by an eﬀect
X b Y Ñ
‚ I in our ambient category C. Consequently, C cannot be semicartesian—as this would
trivialize all density functions—though it must still supply comonoids. We can think of this as
expanding the collection of channels in the category to include acausal or ‘partial’ maps and
unnormalized distributions or states.
Example 4.1.22. An example of such a category is KℓpDď1q, whose objects are sets and whose
morphisms XÑ
‚ Y are functions X Ñ DpY ` 1q. Then a stochastic map is partial if it sends any
probability to the added element ˚, and the subcategory of total (equivalently, causal) maps is
KℓpDq (see [128] for more details).
A morphism XÑ
‚ 1 in KℓpDď1q is therefore a function X Ñ Dp1 ` 1q. Now, a distribution π on
1 ` 1 is the same as a number ¯π in r0, 1s: note that 1 ` 1 has two points, and so π assigns ¯π to one
of them and 1 ´ ¯π to the other. Therefore an eﬀect XÑ
‚ 1 is equivalently a function X Ñ r0, 1s,
which is precisely the type we expect for a density function.
We therefore make the following abstract deﬁnition.
Deﬁnition 4.1.23 (Density functions). A channel c : XÑ
‚ Y is said to be represented by an eﬀect
p : X b Y Ñ
‚ I with respect to µ : IÑ
‚ Y if
c
X
Y
µ
p
X
Y
“
.
121

In this case, we call p a density function for c.
We will also need the concepts of almost-equality and almost-invertibility.
Deﬁnition 4.1.24 (Almost-equality, almost-invertibility). Given a state π : IÑ
‚ X, we say that two
channels c : XÑ
‚ Y and d : XÑ
‚ Y are π-almost-equal, denoted c π„ d, if
c
π
X
Y
“
d
π
X
Y
and we say that an eﬀect p : XÑ
‚ I is π-almost-invertible with π-almost-inverse q : XÑ
‚ I if
π„
q
p
X
X
.
The following basic results about almost-equality will prove helpful.
Proposition 4.1.25 (Composition preserves almost-equality). If c π„ d, then f ‚ c π„ f ‚ d.
Proof. Immediate from the deﬁnition of almost-equality.
Proposition 4.1.26 (Almost-inverses are almost-equal). Suppose q : XÑ
‚ I and r : XÑ
‚ I are both
π-almost-inverses for the eﬀect p : XÑ
‚ I. Then q π„ r.
Proof. By assumption, we have
π„
q
p
π„
r
p
.
122

Then, by the deﬁnition of almost-equality (Deﬁnition 4.1.24):
q
p
π
π
π
r
p
π
“
“
“
.
(4.7)
We seek to show that
“
q
π
r
π
.
(4.8)
Substituting the right-hand-side of (4.7) for π in the left-hand-side of (4.8), we have that
q
r
p
π
q
π
“
r
q
p
π
r
π
“
“
which establishes the result. The second equality follows by the coassociativity of
and the third
by its counitality.
123

With these notions, we can characterise Bayesian inversion via density functions. The result is
due to Cho and Jacobs [87], but we include the graphical proof for expository completeness.
Proposition 4.1.27 (Bayesian inversion via density functions; Cho and Jacobs [87]). Suppose
c : XÑ
‚ Y is represented by the eﬀect p with respect to µ. The Bayesian inverse c:
π : Y Ñ
‚ X of c
with respect to π : IÑ
‚ X is given by
p
π
p´1
X
Y
where p´1 : Y Ñ
‚ I is a µ-almost-inverse for the eﬀect
p
π
Y
Proof. We seek to establish the relation (4.5) characterizing Bayesian inversion. By substituting the
density function representations for c and c:
π into the right-hand-side of (4.5), we have
c:
π
π
c
“
µ
p
p
π
p´1
π
124

“
µ
p
p
π
p´1
π
“
µ
p
π
c
π
“
as required. The second equality holds by the coassociativity of
, the third since p´1 is an
almost-inverse ex hypothesi, and the fourth by the counitality of p
,
q and the density function
representation of c.
The following proposition is an immediate consequence of the deﬁnition of almost-equality and
of the abstract characterisation of Bayesian inversion (4.5). We omit the proof.
Proposition 4.1.28 (Bayesian inverses are almost-equal). Suppose α : Y Ñ
‚ X and β : Y Ñ
‚ X are
both Bayesian inversions of the channel c : XÑ
‚ Y with respect to π : IÑ
‚ X. Then α c‚π
„ β.
We will also need the following two technical results about almost-equality.
Lemma 4.1.29. Suppose the channels α and β satisfy the following relations for some f, q, r:
α
q
f
“
and
β
r
f
“
Suppose q
µ„ r. Then α
µ„ β.
125

Proof. By assumption, we have
α
q
f
“
and
β
r
f
“
.
Consequently,
α
“
µ
q
f
µ
“
f
q
µ
β
“
µ
r
f
µ
“
f
r
µ
“
and so α
µ„ β. The ﬁrst equality holds by the deﬁnition of α, the second by the coassociativity of
, the third since q
µ„ r, the fourth by coassociativity, and the ﬁfth by the deﬁnition of β.
Lemma 4.1.30. If the channel d is represented by an eﬀect with respect to the state ν, and if f ν„ g,
then f
d‚ρ
„ g for any state ρ on the domain of d.
Proof. We start from the left-hand-side of the relation deﬁning almost-equality (Deﬁnition
4.1.24), substituting the density function representation for d. This gives the following chain
126

of isomorphisms:
ρ
d
ν
q
ρ
ν
q
ρ
f
f
f
“
“
ν
q
ρ
g
ν
q
ρ
g
“
ρ
d
g
“
“
The second equality holds by the coassociativity of
; the third since f
ν„ g; the fourth by
coassociativity; and the ﬁfth by the density function representation for d. This establishes the
required relation.
4.1.4. S-finite kernels
To represent channels by concrete density functions, we can work in the category sfKrn of
measurable spaces and s-ﬁnite kernels. We will only sketch the structure of this category, and refer
the reader to Cho and Jacobs [87] and Staton [129] for elaboration.
Objects in sfKrn are measurable spaces pX, ΣXq; often we will just write X, and leave the
σ-algebra ΣX implicit. Morphisms pX, ΣXqÑ
‚ pY, ΣY q are s-ﬁnite kernels. A kernel k from X to
Y is a function k : X ˆ ΣY Ñ r0, 8s satisfying the following conditions:
• for all x P X, kpx, ´q : ΣY Ñ r0, 8s is a measure; and
• for all B P ΣY , kp´, Bq : X Ñ r0, 8s is measurable.
A kernel k : X ˆ ΣY Ñ r0, 8s is ﬁnite if there exists some r P r0, 8q such that, for all x P X,
kpx, Y q ď r. And k is s-ﬁnite if it is the sum of at most countably many ﬁnite kernels kn,
k “ ř
n:N kn.
127

Identity morphisms idX : XÑ
‚ X are Dirac kernels δX : X ˆ ΣX Ñ r0, 8s :“ x ˆ A ÞÑ 1 iﬀ
x P A and 0 otherwise. Composition is given by a Chapman-Kolmogorov equation, analogously to
composition in KℓpDq. Suppose c : XÑ
‚ Y and d : Y Ñ
‚ Z. Then
d ‚ c : X ˆ ΣZ Ñ r0, 8s :“ x ˆ C ÞÑ
ż
y:Y
dpC|yq cpdy|xq
where we have again used the ‘conditional probability’ notation dpC|yq :“ d ˝ py ˆ Cq. Reading
dpC|yq from left to right, we can think of this notation as akin to reading the string diagrams from
top to bottom, i.e. from output(s) to input(s).
Monoidal structure on sfKrn
There is a monoidal structure on sfKrn analogous to that on
KℓpDq. On objects, X b Y is the Cartesian product X ˆ Y of measurable spaces. On morphisms,
f b g : X b Y Ñ
‚ A b B is given by
f b g : pX ˆ Y q ˆ ΣAˆB :“ px ˆ yq ˆ E ÞÑ
ż
a:A
ż
b:B
δAbBpE|x, yq fpda|xq gpdb|yq
where, as above, δAbBpE|a, bq “ 1 iﬀpa, bq P E and 0 otherwise. Note that pf b gqpE|x, yq “
pg bfqpE|y, xq for all s-ﬁnite kernels (and all E, x and y), by the Fubini-Tonelli theorem for s-ﬁnite
measures [87, 129], and so b is symmetric on sfKrn.
The monoidal unit in sfKrn is again I “ 1, the singleton set. Unlike in KℓpDq, however, we do
have nontrivial eﬀects p : XÑ
‚ I, given by kernels p : pX ˆ Σ1q – X Ñ r0, 8s, with which we
will represent density functions.
Comonoids in sfKrn
Every object in sfKrn is a comonoids, analogously to KℓpDq. Discarding
is given by the family of eﬀects
X : X Ñ r0, 8s :“ x ÞÑ 1, and copying is again Dirac-like:
X : X ˆ ΣXˆX :“ x ˆ E ÞÑ 1 iﬀpx, xq P E and 0 otherwise. Because we have nontrivial
eﬀects, discarding is only natural for causal or ‘total’ channels: if c satisﬁes
‚ c “
, then cp´|xq
is a probability measure for all x in the domain1. And, once again, copying is natural (that is,
‚ c “ pc b cq ‚
) if and only if the channel is deterministic.
Channels represented by eﬀects
We can interpret the string diagrams of §4.1.3 in sfKrn, and
we will do so by following the intuition of the conditional probability notation and reading the string
diagrams from outputs to inputs. Hence, if c : XÑ
‚ Y is represented by the eﬀect p : X b Y Ñ
‚ I
with respect to the measure µ : IÑ
‚ Y , then
c : X ˆ ΣY Ñ r0, 8s :“ x ˆ B ÞÑ
ż
y:B
µpdyq ppy|xq.
1This means that the subcategory of total maps in sfKrn is equivalent to the Kleisli category KℓpGq of the Giry monad
G taking each measurable space X to the space GX of measures over X.
128

Note that we also use conditional probability notation for density functions, and so ppy|xq :“
p ˝ px ˆ yq.
Suppose that c : XÑ
‚ Y is indeed represented by p with respect to µ, and that d : Y Ñ
‚ Z is
represented by q : Y b ZÑ
‚ I with respect to ν : IÑ
‚ Z. Then in sfKrn, d ‚ c : XÑ
‚ Z is given by
d ‚ c : X ˆ ΣZ :“ x ˆ C ÞÑ
ż
z:C
νpdzq
ż
y:Y
qpz|yq µpdyq ppy|xq
Alternatively, by deﬁning the eﬀect ppµqq : X b ZÑ
‚ I as
ppµqq : X ˆ Z Ñ r0, 8s :“ x ˆ z ÞÑ
ż
y:Y
qpz|yq µpdyq ppy|xq,
we can write d ‚ c as
d ‚ c : X ˆ ΣZ :“ x ˆ C ÞÑ
ż
z:C
νpdzq ppµqqpz|xq.
Bayesian inversion via density functions
Once again writing π : IÑ
‚ X for a prior on X, and
interpreting the string diagram of Proposition 4.1.27 for c:
π : Y Ñ
‚ X in sfKrn, we have
c:
π : Y ˆ ΣX Ñ r0, 8s :“ y ˆ A ÞÑ
ˆż
x:A
πpdxq ppy|xq
˙
p´1pyq
“ p´1pyq
ż
x:A
ppy|xq πpdxq,
(4.9)
where p´1 : Y Ñ
‚ I is a µ-almost-inverse for eﬀect p‚pπbidY q, and is given up to µ-almost-equality
by
p´1 : Y Ñ r0, 8s :“ y ÞÑ
ˆż
x:X
ppy|xq πpdxq
˙´1
.
Note that from this we recover the informal form of Bayes’ rule for measurable spaces (4.6). Suppose
π is itself represented by a density function pπ with respect to the Lebesgue measure dx. Then
c:
πpA|yq “
ż
x:A
ppy|xq pπpxq
ş
x1:X ppy|x1q pπpx1q dx1 dx.
4.2. Dependent data and bidirectional processes
Two properties of Bayesian inversion are particularly notable. Firstly, given a channel XÑ
‚ Y ,
its inversion is a channel in the opposite direction, Y Ñ
‚ X. Secondly, this inverse channel does
not exist in isolation, but rather depends on a supplied ‘prior’ distribution. In Chapter 6 we will
want to assign functorially to stochastic channels dynamical systems that invert them, and to do
this requires understanding how inversions compose. The general pattern for the composition
of dependent bidirectional processes is called the lens pattern, and this section is dedicated to
introducing it. The more fundamental aspect is that of dependence, which we began to explore
in the context of dependent sums and products in Chapter 2: we therefore begin this section by
introducing the Grothendieck construction, a ‘ﬁbrational’ framework for composing dependent
processes.
129

4.2.1. Indexed categories and the Grothendieck construction
At various point above, we have encountered ‘dependent’ objects and morphisms: indexed and
dependent sums (Remark 2.3.10); indexed products (Remark 2.3.20); dependent products (§2.3.4.1);
hints at dependent type theory (end of §2.3.4); parameterized morphisms (§3.2.2); circuit algebras
(§3.3); and, of course, Bayesian inversions. The Grothendieck construction classiﬁes each of these
as examples of a common pattern, allowing us to translate between ‘indexed’ and ‘ﬁbrational’
perspectives: from the indexed perspective, we consider functors from an indexing object into a
category (think of diagrams); from the ﬁbrational perspective, we consider bundles as projection
maps. The correspondence is then, roughly speaking, between “the object indexed by i” and “the
subobject that projects to i”, which is called the ‘ﬁbre’ of the bundle over i.
For this reason, categories of bundles are an important part of the story, from which much else
is generalized. Recall from Deﬁnition 3.2.7 that these categories of bundles are slice categories: the
category of bundles over B in C is the slice C{B, whose objects are pairs pE, pq of an object E and
a morphism p : E Ñ B; and whose morphisms pE, pq Ñ pE1, p1q are morphisms α : E Ñ E1 of C
such that p “ p1 ˝ α. We call this the category of bundles over B as a generalization of the notion
of “ﬁbre bundle”, from which we inherit the notion of ‘ﬁbre’.
Deﬁnition 4.2.1. Suppose C is a category with ﬁnite limits. Given a bundle p : E Ñ B in C, its
ﬁbre over b : B is the subobject Eb of E such that ppeq “ b for all e : Eb. The ﬁbre Eb can be
characterized as the following pullback object, where 1 is the terminal object in C:
Eb
E
1
B
b
p
{
In the case where C “ Set, there is an equivalence between the slice Set {B and a certain
presheaf category: the category of B-diagrams in Set, which we can equivalently think of as the
category of B-indexed sets.
Deﬁnition 4.2.2. Suppose B is a set. The discrete category on X is the category whose objects are
the elements of B and whose only morphisms are identity morphisms idb : b Ñ b for each element
b : B. We will denote the discrete category on B simply by B.
Proposition 4.2.3. For each set B, there is an equivalence Set {B – SetB.
Proof. In the direction Set {B Ñ SetB, let p : E Ñ B be a bundle over B. We construct a functor
P : B Ñ Set by deﬁning Ppbq :“ Eb, where Eb is the ﬁbre of p over b; there are no nontrivial
morphisms in B, so we are done. Now suppose f : pE, pq Ñ pF, qq is a morphism of bundles. A
130

natural transformation ϕ : P ñ Q in SetB is just a family of functions ϕb : Pb Ñ Qb indexed by
b. Hence, given f, we deﬁne ϕb as the restriction of f to Eb for each b : B.
In the direction SetB Ñ Set {B, let P : B Ñ Set be a functor. We deﬁne E as the coproduct
ř
b:B Ppbq, and the bundle p : E Ñ B as the projection pb, xq ÞÑ b for every pb, xq in ř
b:B Ppbq.
Now suppose ϕ : P ñ Q is a natural transformation in SetB. We deﬁne the function f : pE, pq Ñ
pF, qq by the coproduct of the functions ϕb, as f :“ ř
b:B ϕb.
These two constructions are easily veriﬁed as mutually inverse.
If the B in SetB is not just a set, but rather a category, then there is a correspondingly categoriﬁed
notion of the category of bundles.
Deﬁnition 4.2.4. Suppose F : C Ñ Set is a copresheaf on C. Its category of elements C{F has for
objects pairs pX, xq of an object X : C and an element x : FX. A morphism pX, xq Ñ pY, yq is
a morphism f : X Ñ Y in C such that Ffpxq “ y, as in the following diagram, where the top
triangle commutes in Set:
1
FX
FY
X
Y
x
Ff
y
f
.
Identities are given by identity morphisms in C, and composition is composition of the underlying
morphisms in C. There is an evident forgetful functor πF : C{F Ñ C, which acts on objects as
pX, xq ÞÑ X and on morphisms as f ÞÑ f.
To validate that the category of elements construction is a good generalization of the slice
category, we have the following example.
Example 4.2.5. The category of elements of a representable copresheaf CpC, ´q is equivalent to
the slice category C{C, from which we derive the similar notation.
Remark 4.2.6. Another way to look at the morphisms pX, xq Ñ pY, yq in C{F is as pairs pf, ιq,
where f is a morphism X Ñ Y in C and ι is an identiﬁcation Ffpxq “ y. Then composition
in
ş
F is not just composition of morphisms in C, but also composition of identiﬁcations: given
pf, ιq : pX, xq Ñ pY, yq and pg, κq : pY, yq Ñ pZ, zq, the composite pg, κq˝pf, ιq is pg˝f, κ˝Fgpιqq,
where κ ˝ Fgpιq is the composite identiﬁcation Fpg ˝ fqpxq
Fgpιq
ùùùùù Fgpyq
κ
ùù z. We can think of
these identiﬁcations as witnesses to the required equalities. This perspective on C{F is analogous
to the process of categoriﬁcation we considered in Chapter 2, where we added witnesses (ﬁllers) to
equations and diagrams.
131

A better way to validate the category of elements construction is to generalize the Grothendieck
correspondence, Proposition 4.2.3, which means we need something to correspond to SetB: a
category of categories of elements. These generalized categories of elements are called “discrete
opﬁbrations”, and constitute our ﬁrst examples of categoriﬁed bundles.
Deﬁnition 4.2.7. A discrete opﬁbration is a functor F : C Ñ D such that, for every object C in
C and morphism g : FC Ñ B in D, there exists a unique morphism h : C Ñ C1 in C such that
Fh “ g (called the lift of g):
C
C1
FC
D
h
g
Write DOpﬁbpDq to denote the full subcategory of Cat{D on those objects which are discrete
opﬁbrations.
Example 4.2.8. The forgetful functor πF : C{F Ñ C out of the category of elements of a copresheaf
F is a discrete opﬁbration: for any object pX, xq in C{F and morphism g : X Ñ Y in C, there is a
unique morphism g : pX, xq Ñ pY, yq, namely where y “ Ffpxq.
And thus we obtain a Grothendieck correspondence at the next level of categoriﬁcation.
Proposition 4.2.9. For any category B, there is an equivalence DOpﬁbpBq – SetB.
Proof sketch. We only sketch the bijection on objects; the correspondence on morphisms subse-
quently follows quite mechanically.
Given a discrete opﬁbration p : E Ñ B, we can form the ﬁbre of p over b : B as the subcategory
Eb of E whose objects are mapped by p to b and whose morphisms are mapped to idb; it is easy to
check that Eb is a discrete category and hence a set. Given a morphism f : b Ñ c in B, we deﬁne a
function ϕ : Eb Ñ Ec by mapping each e : Eb to the codomain of the unique lift h. This deﬁnes a
functor B Ñ Set; functoriality follows from the uniqueness of lifts.
In the inverse direction, given a copresheaf F : B Ñ Set, take the forgetful functor πF : B{F Ñ
B out of its category of elements, which is a discrete opﬁbration by the example above. Given a
natural transformation σ : F ñ G, deﬁne a functor S : B{F Ñ B{G on objects as SpX, xq “
pX, σXpxqq and on morphisms f : pX, xq Ñ pY, yq as Sf “ pX, σXpxqq
fÝÑ pY, σY pyqq; this is
well-deﬁned by the naturality of σ and the deﬁnition of f, since Gf ˝ σXpxq “ σY ˝ Ffpxq and
Ffpxq “ y.
The veriﬁcation that these two constructions are mutually inverse is straightforward.
132

In many cases, the dependent data of interest will have more structure than that of mere sets. For
example, in §3.3 we introduced a rate-coded circuit diagrams as an indexing of sets of rate-coded
circuits by a category of circuit diagrams; later, we will see that dynamical systems have a canonical
notion of morphism, and so our dynamical semantics will take the form of an indexed collection of
categories, and this requires us to categorify not only the domain of indexing (as we have done
above), but also the codomain of values (as we do now). As with monoidal categories—and as in
the case of circuit algebras—in this higher-dimensional setting, it becomes necessary to work with
weak composition, and the relevant notion of weak functor is the ‘pseudofunctor’.
Deﬁnition 4.2.10. Suppose C is a category and B is a bicategory. A pseudofunctor or weak functor
F : C Ñ B constitutes
1. a function F0 : C0 Ñ B0 on objects;
2. for each pair of objects a, b : C, a function Fa,b : Cpa, bq Ñ Bpa, bq0 on morphisms;
3. for each object c : C, a 2-isomorphism Fidc : idF0c ñ Fc,cpidcq; and
4. for each composable pair of morphisms f : a Ñ b and g : b Ñ c in C, a 2-isomorphism
Fg,f : Fb,cpgq ˛ Fa,bpfq ñ Fa,cpg ˝ fq, where we have written composition in C as ˝ and
horizontal composition in B as ˛;
satisfying conditions of weak functoriality:
(a) coherence with left and right unitality of horizontal composition, so that the respective
diagrams of 2-cells commute:
idF0b ˛Fa,bpfq
Fa,bpfq
Fb,bpidbq ˛ Fa,bpfq
Fa,bpidb ˝fq
λFa,bpfq
Fidb˛Fa,bpfq
Fidb,f
Fa,bpfq ˛ idF0a
Fa,bpfq
Fa,bpfq ˛ Fa,apidaq
Fa,bpf ˝ idaq
ρFa,bpfq
Fa,bpfq˛Fida
Ff,ida
(b) coherence with associativity of horizontal composition, so that the following diagram of
2-cells commutes:
pFc,dphq ˛ Fb,cpgqq ˛ Fa,bpfq
Fc,dphq ˛ pFb,cpgq ˛ Fa,bpfqq
Fb,dph ˝ gq ˛ Fa,bpfq
Fc,dphq ˛ Fa,cpg ˝ fq
Fa,dpph ˝ gq ˝ fq
Fa,dph ˝ pg ˝ fqq
αh,g,f
Fc,dphq˛Fg,f
Fh,g˝f
Fh,g˛Fa,bpfq
Fh˝g,f
.
133

Remark 4.2.11. If C is in fact a nontrivial bicategory, then the deﬁnition of pseudofunctor is
weakened accordingly: the functions Fa,b are replaced by functors between the corresponding
hom-categories, and the equalities in the functoriality conditions (a) and (b) are replaced by the
relevant unitor or associator isomorphism. In this thesis, we will consider only pseudofunctors for
which the domain C is merely a category.
With pseudofunctors, we gain a notion of indexed category.
Deﬁnition 4.2.12. An indexed category is a pseudofunctor F : C op Ñ Cat, for some indexing
category C. An opindexed category is a pseudofunctor F : C Ñ Cat.
And the Grothendieck construction tells us how to translate between (op)indexed categories and
(op)ﬁbrations: in some situations, it will be easier to work with the one, and in others the other.
In particular, categories of lenses (and polynomial functors) will be seen to arise as Grothendieck
constructions.
Deﬁnition 4.2.13. Suppose F : C op Ñ Cat is a pseudofunctor. Its (contravariant) Grothendieck
construction is the category
ş
F deﬁned as follows. The objects of
ş
F are pairs pX, xq of an object
X : C and an object x : FX. A morphism pX, xq Ñ pY, yq is a pair pf, ϕq of a morphism
f : X Ñ Y in C and a morphism ϕ : x Ñ Ffpyq in FX, as in the following diagram, where the
upper triangle is interpreted in Cat (note the contravariance of Ff):
1
FX
FY
X
Y
x
Ff
y
f
ϕ
We can thus write the hom set
ş
F
`
pX, xq, pY, yq
˘
as the dependent sum ř
f : CpX,Y q FX
`
x, Ffpyq
˘
.
The identity morphism on pX, xq is pidX, idxq, and composition is deﬁned as follows. Given
pf, ϕq : pX, xq Ñ pY, yq and pg, γq : pY, yq Ñ pZ, zq, their composite pg, γq ˝ pf, ϕq is the pair
pg ˝ f, Ffpγq ˝ ϕq .
There is a forgetful functor
ş
F Ñ C mapping pX, xq ÞÑ X and pf, ϕq ÞÑ f.
Remark 4.2.14. Dually, there is a covariant Grothendieck construction, for opindexed categories
F : C Ñ Cat. The objects of
ş
F are again pairs pX : C, x : FXq, but now the morphisms
pX, xq Ñ pY, yq are pairs pf, ϕq with f : X Ñ Y in C as before and now ϕ : Ffpxq Ñ y; all
that we have done is swapped the direction of the arrow Ff in the diagram in Deﬁnition 4.2.13
134

(compare the identiﬁcations in the category of elements of a copresheaf, in Deﬁnition 4.2.4). As a
result, we can write the hom set
ş
F
`
pX, xq, pY, yq
˘
in this case as ř
f : CpX,Y q FX
`
Ffpxq, y
˘
.
Remark 4.2.15. The Grothendieck construction induces an analogue of Proposition 4.2.9 between
the bicategory of pseudofunctors B op Ñ Cat and the bicategory of Grothendieck ﬁbrations on B.
Indeed there are analogues of Propositions 4.2.9 and 4.2.3 in any categorical dimension. Because
ﬁbrations are the higher-dimensional analogues of bundles, they have a base category (the codomain)
and a ‘total’ category (the domain), which is a kind of colimit of the ﬁbres (constructed by the
Grothendieck construction): strictly speaking, what we have called the Grothendieck construction
above is total category of the full ﬁbrational construction; the ﬁbration itself is the corresponding
forgetful (projection) functor. For a highly readable exposition of Grothendieck constructions, we
refer the reader to Loregian and Riehl [130].
4.2.1.1. The monoidal Grothendieck construction
When C is a monoidal category with which F is appropriately compatible, then we can ‘upgrade’ the
notions of indexed category and Grothendieck construction accordingly. As noted in Remark 4.2.11,
we will only encounter pseudofunctors for which the domain category C is trivially a bicategory,
and so, although Moeller and Vasilakopoulou [131] work out the structure in full bicategorical
detail, we will restrict ourselves to ‘1-categorical’ monoidal indexed categories.
Deﬁnition 4.2.16 (After Moeller and Vasilakopoulou [131, §3.2]). Suppose pC, b, Iq is a monoidal
category. We say that F is a monoidal indexed category when F is a weak lax monoidal functor
pF, µ, ηq : pC op, b op, Iq Ñ pCat, ˆ, 1q. This means that the laxator µ is given by a natural family
of functors µA,B : FA ˆ FB ñ FpA b Bq along with, for any morphisms f : A Ñ A1 and
g : B Ñ B1 in C, a natural isomorphism µf,g : µA1,B1 ˝pFf ˆ Fgq ñ Fpf bgq˝µA,B. The laxator
and the unitor η : 1 Ñ FI together satisfy axioms of associativity and unitality that constitute
indexed versions of the associators and unitors of a monoidal category (Deﬁnition 3.1.3).
Explicitly, this means that there must be three families of natural isomorphisms, indexed by
objects A, B, C : C,
1. an associator family αA,B,C : µAbB,CpµA,Bp´, ´q, ´q ñ µA,BbCp´, µB,Cp´, ´qq;
2. a left unitor λA : µI,Apη, ´q ñ idFA; and
3. a right unitor ρA : µA,Ip´, ηq ñ idFA
135

such that the unitors are compatible with the associator, i.e. for all A, B : C the diagram
µAbI,BpµA,Ip´, ηq, ´q
µA,IbBp´, µI,Bpη, ´qq
µA,Bp´, ´q
µPA,BpρA,´q
αA,I,Bp´,η,´q
µA,ΛB p´,λBq
commutes (where P and Λ are the right and left associators of the monoidal structure pb, Iq on
C), and such that the associativity is ‘order-independent’, i.e. for all A, B, C, D : C, the diagram
µAbpBbCq,DpµA,BbCp´, µB,Cp´, ´qq, ´q
µA,pBbCqbDp´, µBbC,DpµB,Cp´, ´q, ´qq
µpAbBqbC,DpµAbB,CpµA,Bp´, ´q, ´q, ´q
µA,BbpCbDqp´, µB,CbDp´, µC,Dp´, ´qqq
µAbB,CbDpµA,Bp´, ´q, µC,Dp´, ´qq
αA,BbC,D
µAA,B,C ,DpαA,B,C,´q
αAbB,C,D
αA,B,CbD
µA,AB,C,Dp´,αB,C,Dq
commutes (where A is the associator of the monoidal structure on C).
We say that F is a strong monoidal indexed category if the natural isomorphisms µf,g are in fact
equalities, and strict if the associators and unitors are equalities as well.
The following proposition exhibits the monoidal structure carried by the Grothendieck construc-
tion when the indexed category is monoidal.
Proposition 4.2.17 (Moeller and Vasilakopoulou [131, §6.1]). Suppose pF, µ, ηq : pC op, b op, Iq Ñ
pCat, ˆ, 1q is a strong monoidal indexed category. Then the total category of the Grothendieck
construction
ş
F obtains a monoidal structure pbµ, Iµq. On objects, deﬁne
pC, Xq bµ pD, Y q :“
`
C b D, µCDpX, Y q
˘
where µCD : FC ˆ FD Ñ FpC b Dq is the component of µ at pC, Dq. On morphisms pf, f:q :
pC, Xq ÞÑ pC1, X1q and pg, g:q : pD, Y q ÞÑ pD1, Y 1q, deﬁne
pf, f:q bµ pg, g:q :“
`
f b g, µCDpf:, g:q
˘
.
The monoidal unit Iµ is deﬁned to be the object Iµ :“
`
I, ηp˚q
˘
. Writing λ : I b p´q ñ p´q
and ρ : p´q b I ñ p´q for the left and right unitors of the monoidal structure on C, the left and
right unitors in
ş
F are given by pλ, idq and pρ, idq respectively. Writing α for the associator of the
monoidal structure on C, the associator in
ş
F is given by pα, idq.
136

4.2.2. Grothendieck lenses
Lenses formalize bidirectional processes in which the ‘backward’ process depends on data in the
domain of the ‘forward’ process. The name originates in database theory [132, 133], where the
forward process gives a zoomed-in ‘view’ onto a database record, and the backward process is used
to update it. Following an observation of Myers and Spivak [134], lenses of this general shape can
be given a concise deﬁnition using the Grothendieck construction. In order to obtain the backward
directionality of the dependent part, we use the “pointwise opposite” of a pseudofunctor.
Deﬁnition 4.2.18. Suppose F : C op Ñ Cat is a pseudofunctor. We deﬁne its pointwise opposite
F p : C op Ñ Cat to be the pseudofunctor c ÞÑ Fc op returning the opposite of each category Fc;
given f : c Ñ c1, F pf : Fc op Ñ Fc1 op is deﬁned as pFfq op : Fc op Ñ Fc1 op.
Categories of Grothendieck lenses are then obtained via the Grothendieck construction of
pointwise opposites of pseudofunctors.
Deﬁnition 4.2.19 (Grothendieck lenses [134]). We deﬁne the category LensF of Grothendieck
lenses for a pseudofunctor F : C op Ñ Cat to be the total category of the Grothendieck construction
for the pointwise opposite F p of F. Explicitly, its objects pLensF q0 are pairs pC, Xq of objects C
in C and X in FpCq, and its hom-sets LensF
`
pC, Xq, pC1, X1q
˘
are the dependent sums
LensF
`
pC, Xq, pC1, X1q
˘
“
ÿ
f : CpC,C1q
FpCq
`
FpfqpX1q, X
˘
(4.10)
so that a morphism pC, Xq
ÞÑ
pC1, X1q is a pair pf, f:q of f
:
CpC, C1q and f:
:
FpCq
`
FpfqpX1q, X
˘
. We call such pairs Grothendieck lenses for F or F-lenses. We say that the
morphism f is the forward component of the lens, and the morphism f: the backward component.
The identity Grothendieck lens on pC, Xq is idpC,Xq “ pidC, idXq. Sequential composition is
as follows. Given pf, f:q : pC, Xq ÞÑ pC1, X1q and pg, g:q : pC1, X1q ÞÑ pD, Y q, their composite
pg, g:q  pf, f:q is deﬁned to be the lens
`
g ‚ f, f: ˝ Fpfqpg:q
˘
: pC, Xq ÞÑ pD, Y q.
Notation 4.2.20. In the context of lenses, we will often write the backward map as f: or f7, with
the former particularly used for Bayesian lenses. We will also use ÞÑ to denote a lens, and  for
lens composition. Above, we additionally used ‚ for composition in the base category and ˝ for
composition in the ﬁbres.
Since lenses are bidirectional processes and English is read horizontally, when it comes to string
diagrams for lenses, we will depict these horizontally, with the forwards direction read from left to
right.
Whenever C is a monoidal category, it gives rise to a canonical category of lenses, in which the
forwards morphisms are comonoid morphisms in C and the backwards morphisms are (internally)
137

parameterized by the domains of the forwards ones. Comonoids and their morphisms are necessary
to copy parameters during composition. The resulting ‘monoidal’ lenses are a natural generalization
of the ‘Cartesian’ lenses used in the database setting, and we will see that Bayesian lenses are
similarly constructed using an indexed category of (externally) parameterized morphisms.
Example 4.2.21. Suppose pC, b, Iq is a monoidal category and let ComonpCq be its subcategory
of comonoids and comonoid morphisms. A monoidal lens pX, Aq ÞÑ pY, Bq is a pair pf, f7q of a
comonoid morphism f : X Ñ Y in ComonpCq and a morphism f7 : X b B Ñ A in C. Such
lenses can be characterized as Grothendieck lenses, following Spivak [134, §3.2].
First, deﬁne a pseudofunctor P : ComonpCq op Ñ Cat as follows. On objects X : ComonpCq,
deﬁne PX as the category with the same objects as C and with hom-sets given by PXpA, Bq :“
CpX bA, Bq; denote a morphism f from A to B in PX by f : A X
ÝÑ B. The identity idA : A X
ÝÑ A
is deﬁned as the projection projA : X b A
XbidA
ÝÝÝÝÝÝÑ I b A
λA
ÝÝÑ A. Given f : A
X
ÝÑ B and
g : B X
ÝÑ C, their composite g ˝ f : A X
ÝÑ C is given by the following string diagram in C:
f
g
X
A
B
.
Given h : X Ñ Y in ComonpCq, the functor Ph : PY Ñ PX acts by precomposition on
morphisms, taking f : A YÝÑ B to the morphism Phpfq : A X
ÝÑ B given by
X b A hbidA
ÝÝÝÝÑ Y b A
fÝÑ B .
(An alternative way to obtain PX is as the ‘coKleisli’ category of the comonad X b p´q.)
The category of monoidal lenses is then deﬁned to be the category of Grothendieck P-lenses.
The objects of LensP are pairs pX, Aq of a comonoid X and an object A in C, and the morphisms
are monoidal lenses. Given lenses pf, f7q : pX, Aq Ñ pY, Bq and pg, g7q : pY, Bq Ñ pZ, Cq, the
composite lens has forward component given by g ˝ f : X Ñ Z and backward component given
by f7 ˝ Pfpg7q : C
X
ÝÑ A.
We can depict monoidal lenses string-diagrammatically, with the forward and backward
components oriented in opposite directions. To exemplify this, note that, because the forwards
components are comonoid morphisms, the following equality holds for all composite monoidal
138

lenses pg, g7q ˝ pf, f7q:
f
f7
X
A
B
Y
X
g
g7
C
Z
Y
“
f
f7
X
A
g
g7
C
Z
f
Here, we have decorated the strings with ﬂetches to indicate the direction of information-ﬂow
and disambiguate the bidirectionality, and drawn boxes around the pairs that constitute each lens.
Note however that the parameterizing input to the backwards component of the ﬁrst lens is not
constrained to be a copy of the input to the forwards component; it is only for compositional
convenience that we depict lenses this way.
Deﬁnition 4.2.22. When C is Cartesian monoidal, so that its monoidal structure pˆ, 1q is the
categorical product, we will call monoidal lenses in C Cartesian lenses.
Remark 4.2.23. The string-diagrammatic depictions of lenses above were not strictly formal, or
at least we have not explain how they might be; we have not exhibited a coherence theorem such
as 3.1.9. In this case, the diagrams above are depictions in the graphical calculus of Boisseau [135].
An alternative graphical language for a generalization of lenses called optics[136, 137] has been
described by Román [138], and we make use of this in the Appendix (§A.2).
Monoidal lenses ﬁnd uses not only in database theory, but in many other situations, too: the
general pattern is “interacting systems where information ﬂows bidirectionally”. In economics
(speciﬁcally, compositional game theory), lenses are used to model the pattern of interaction of
economic games: the forward maps encode how players act in light of observations, and the
backward maps encode how utility is passed “backwards in time” from outcomes, in order to
assign credit[17]. In non-probabilistic machine learning, lenses can be used to formalize reverse
diﬀerentiation and hence the backpropagation of error (another kind of credit assignment): the
forwards maps represent diﬀerentiable processes (such as neural networks), and the backward maps
are the reverse-derivatives used to pass error back (e.g., between neural network layers)[139, 140].
Generalizations of lenses known as optics[136, 137] have also been used both to model economic
games with uncertainty (‘mixed’ strategies)[141] and to model the process of dynamic programming
(Bellman iteration) used in the related ﬁeld of reinforcement learning[142], as well as to model
client-server interactions in computing[143].
139

In systems theory, lenses can be used to formalize various kinds of dynamical system: the forward
maps encode their ‘outputs’ or ‘actions’, and the backward maps encode how states and inputs
give rise to transitions[144]. This latter application will be a particular inspiration to us, and is
closely related to Example 4.2.26, which expresses polynomial functors as lenses (thereby explaining
Proposition 3.5.4), and for which we need the following canonical family of indexed categories.
Deﬁnition 4.2.24. When a category C has pullbacks, its slice categories C{C collect into an indexed
category C{p´q : C op Ñ Cat called the self-indexing of C, and deﬁned as follows. On objects
C : C, the self-indexing unsurprisingly returns the corresponding slice categories C{C.
Given a morphism f : A Ñ B, the functor C{f : C{B Ñ C{A is deﬁned by pullback. On objects
pE, pq : C{B, we deﬁne pC{fqpE, pq :“ pf˚E, f˚pq, where f˚E is the pullback object A ˆB E
and f˚E is the associated projection to A. On morphisms ϕ : pE, pq Ñ pE1, p1q in C{B, we deﬁne
pC{fqpϕq as the morphism f˚ϕ : pf˚E, f˚pq Ñ pf˚E1, f˚p1q induced by the universal property
of the pullback f˚E1, as in the commuting diagram
f˚E
E
f˚E1
E1
A
B
ϕ
f˚ϕ
{
p1
p
f
f˚p1
{
f˚p
.
Remark 4.2.25. The functors C{f : C{B Ñ C{A are also known as base-change functors, as they
change the ‘base’ of the slice category.
Example 4.2.26. The category Poly of polynomial functors (§3.5) is equivalent to the category
of Grothendieck lenses for the self-indexing of Set: that is, Poly – LensSet {p´q. To see this,
observe that the objects of LensSet {p´q are bundles p : E Ñ B of sets. If we deﬁne the set pris to
be the ﬁbre Ei of p for each i : B, we have an isomorphism E – ř
i:B pris. We can then deﬁne
a polynomial functor P :“ ř
i:B ypris, and then ﬁnd that Pp1q “ B, which justiﬁes writing the
original bundle as p : ř
i:pp1q pris Ñ pp1q. We saw in Proposition 3.5.4 how to associate to any
polynomial functor P a bundle p, and it is easy to check that applying this construction to the P
deﬁned here returns our original bundle p. This shows that the objects of Poly are in bijection
with the objects of LensSet {p´q. What about the morphisms?
A morphism p Ñ q in LensSet {p´q, for p : X Ñ A and q : Y Ñ B is a pair of functions
140

f1 : A Ñ B and f7 : f˚
1 Y Ñ X such that f˚
1 q “ p ˝ f7, as in the following diagram:
X
f˚
1 Y
Y
A
A
B
f7
q
f˚
1 q
p
f1
{
Replacing the bundles p and q by their polynomial representations p : ř
i:pp1q pris Ñ pp1q and
q : ř
j:qp1q qrjs Ñ qp1q, we see that the pair pf1, f7q is precisely a morphism of polynomials of the
form established in Proposition 3.5.4, and that every morphism of polynomials corresponds to such
a lens. This establishes an isomorphism of hom-sets, and hence Poly – LensSet {p´q.
Lenses are also closely related to wiring diagrams[80, 145] and our linear circuit diagrams (§3.3.2).
Example 4.2.27. Let FVect denote the category of ﬁnite-dimensional real vector spaces and
linear maps between them; write n for the object Rn. FVect has a Cartesian monoidal product
p`, 0q given by the direct sum of vectors (n ` m “ Rn ‘ Rm “ Rn`m), and whose unit object
is 0. The category of monoidal lenses in pFVect, `, 0q is the category of linear circuit diagrams
(Example 3.3.9).
Cartesian lenses pX, Aq ÞÑ pY, Bq are in some sense ‘non-dependent’ lenses, because the domain
of the backwards map is a simple product X ˆ B, in which the object B does not depend on x : X.
We can see polynomial functors are a dependent generalization of Cartesian lenses in Set.
Proposition 4.2.28. The category of monoidal lenses in pSet, ˆ, 1q is equivalently the full
subcategory of Poly on the monomials XyA.
Proof sketch. A morphism of monomials pf1, f7q : XyA Ñ Y yB is a pair of functions f1 : X Ñ Y
and f7 : X ˆ B Ñ A; this is a Cartesian lens pX, Aq Ñ pY, Bq. There is clearly a bijection of
objects XyA Ø pX, Aq.
In particular, this situation encompasses linear circuit diagrams, which embed into Poly
accordingly.
Remark 4.2.29. There is a forgetful functor from vector spaces to sets, U : FVect Ñ Set. If we
write LenspCq to denote the category of monoidal lenses in C (with the relevant monoidal structure
left implicit), this forgetful functor induces a ‘change of base’ LenspUq : LenspFVectq Ñ
LenspSetq, since the Grothendieck construction is functorial by Remark 4.2.15, and hence so is
the Lens construction. There is therefore a canonical embedding of linear circuit diagrams into
Poly, LenspFVectq
LenspUq
ÝÝÝÝÝÑ LenspSetq ãÑ Poly.
141

Our dynamical semantics for approximate inference (Chapter 6) can, if one squints a little, be
therefore seen as a kind of probabilistic generalization of our algebra for rate-coded neural circuits:
it will be an algebra for (a stochastic analogue of) the multicategory OPoly with semantics in
categories of (stochastic) dynamical systems. One can see a morphism of polynomials therefore
as a kind of ‘dependent’ circuit diagram, with the forwards component transporting ‘outgoing’
information from inside a (‘nested’) system to its boundary (its external interface), and the backward
component transporting ‘incoming’ information (“immanent signals”) from the boundary internally,
depending on the conﬁguration of the boundary.
Of course, to give an OPoly-algebra is to give a lax monoidal functor, which means knowing
the relevant monoidal structure. While we saw this in the case of polynomial functors of sets in
Proposition 3.5.7, it will be helpful when it comes to generalizing Poly to see how this structure is
obtained. Moreover, we will want a monoidal structure on Bayesian lenses, in order to deﬁne joint
approximate inference systems. For these reasons, we now turn to monoidal categories of lenses.
4.2.2.1. Monoidal categories of lenses
The monoidal structures on categories of Grothendieck lenses—at least those of interest here—are a
direct corollary of the monoidal Grothendieck construction, Proposition 4.2.17.
Corollary 4.2.30. When F : C op Ñ Cat is equipped with a strong monoidal indexed category
structure pµ, ηq, its category of lenses LensF becomes a monoidal category pLensF , b1
µ, Iµq. On
objects b1
µ is deﬁned as bµ in Proposition 4.2.17, as is Iµ. On morphisms pf, f:q : pC, Xq ÞÑ
pC1, X1q and pg, g:q : pD, Y q ÞÑ pD1, Y 1q, deﬁne
pf, f:q b1
µ pg, g:q :“
`
f b g, µop
CDpf:, g:q
˘
where µop
CD : FpCq op ˆFpDq op Ñ FpC bDq op is the pointwise opposite of µCD. The associator
and unitors are deﬁned as in Proposition 4.2.17.
As an example, this gives us the tensor product on Poly, which is inherited by the category of
Cartesian lenses in Set.
Example 4.2.31. The tensor product structure pb, yq on Poly is induced by a strong monoidal
indexed category structure pµ, ηq on the self-indexing of pSet, ˆ, 1q. To deﬁne the unitor η, ﬁrst
note that Set {1 – Set, so that η equivalently has the type 1 Ñ Set; we thus make the natural
choice for η, the terminal element ˚ ÞÑ 1. The laxator µ is deﬁned for each B, C : Set by the
functor
µB,C
:
Set {B
ˆ
Set {C
Ñ
Set {pB ˆ Cq
`
p :
ÿ
i:B
pris Ñ B, q :
ÿ
j:C
qrjs Ñ C
˘
ÞÑ
ÿ
pi,jq:BˆC
pris ˆ qrjs
142

the naturality and functoriality of which follow from the functoriality of ˆ. Applying Corollary
4.2.30 to this structure, we obtain precisely the tensor product of polynomials introduced in
Proposition 3.5.7.
Corollary 4.2.32. Since the category of Cartesian lenses in Set is the monomial subcategory
of Poly, to which the tensor structure pb, yq restricts, the latter induces a symmetric monoidal
structure on the former, the unit of which is the object p1, 1q. Given objects pX, Aq and pX1, A1q,
their tensor pX, Aq b pX1, A1q is pX ˆ X1, A ˆ A1q. Given lenses pf, f7q : pX, Aq Ñ pY, Bq and
pf1, f17q : pX1, A1q Ñ pY 1, B1q, their tensor has forward component f ˆ f1 : X ˆ X1 Ñ Y ˆ Y 1
and backward component
X ˆ X1 ˆ B ˆ B1 idX ˆσX1,BˆidB1
ÝÝÝÝÝÝÝÝÝÝÑ X ˆ B ˆ X1 ˆ B1 f7ˆf17
ÝÝÝÝÑ A ˆ A1
where σ is the symmetry of the product ˆ.
We will see that the monoidal structure on Bayesian lenses is deﬁned similarly. First of all, we
need to deﬁne Bayesian lenses themselves.
4.3. The bidirectional structure of Bayesian updating
In this section, we deﬁne a collection of indexed categories, each denoted Stat, whose morphisms can
be seen as generalized Bayesian inversions. Following Deﬁnition 4.2.19, these induce corresponding
categories of lenses which we call Bayesian lenses. In §4.3.3, we show abstractly that, for the
subcategories of exact Bayesian lenses whose backward channels correspond to ‘exact’ Bayesian
inversions, the Bayesian inversion of a composite of forward channels is given (up to almost-
equality) by the lens composite of the corresponding backward channels. This justiﬁes calling these
lenses ‘Bayesian’, and provides the foundation for the study of approximate (non-exact) Bayesian
inversion in Chapter 6.
4.3.1. State-dependent channels
As we saw in §4.1, a channel c : XÑ
‚ Y admitting a Bayesian inversion induces a family of inverse
channels c:
π : Y Ñ
‚ X, indexed by ‘prior’ states π : 1Ñ
‚ X. Making the state-dependence explicit, in
typical cases where c is a probability kernel we obtain a function c: : GX ˆ Y Ñ GX, under the
assumption that c‚π is fully supported for all π : GX (see Remark 4.1.20 for our justiﬁcation of this
simplifying assumption). In more general situations, and in light of the full-support assumption,
we obtain a morphism c: : CpI, Xq Ñ CpY, Xq in the base of enrichment of the monoidal category
pC, b, Iq of c, which for simplicity we take to be Set2. We call morphisms of this general type
state-dependent channels, and structure the indexing as an indexed category.
2The construction still succeeds for an arbitrary Cartesian closed base of enrichment with all pullbacks.
143

Deﬁnition 4.3.1. Let pC, b, Iq be a monoidal category. Deﬁne the C-state-indexed category Stat :
C op Ñ Cat as follows.
Stat
:
C op Ñ Cat
X ÞÑ StatpXq :“
¨
˚
˚
˚
˝
StatpXq0
:“
C0
StatpXqpA, Bq
:“
Set
`
CpI, Xq, CpA, Bq
˘
idA :
StatpXqpA, Aq
:“
#
idA : CpI, Xq Ñ CpA, Aq
ρ
ÞÑ
idA
˛
‹‹‹‚
(4.11)
f : CpY, Xq ÞÑ
¨
˚
˚
˚
˚
˚
˝
Statpfq :
StatpXq
Ñ
StatpY q
StatpXq0
“
StatpY q0
SetpCpI, Xq, CpA, Bqq
Ñ
Set
`
CpI, Y q, CpA, Bq
˘
α
ÞÑ
f˚α :
`
σ : CpI, Y q
˘
ÞÑ
`
αpf ‚ σq : CpA, Bq
˘
˛
‹‹‹‹‹‚
Composition in each ﬁbre StatpXq is as in C. Explicitly, indicating morphisms CpI, Xq Ñ CpA, Bq
in StatpXq by A X
ÝÑ
‚ B, and given α : A X
ÝÑ
‚ B and β : B X
ÝÑ
‚ C, their composite is β ˝ α : A X
ÝÑ
‚ C :“
ρ ÞÑ βpρq ‚ αpρq, where here we indicate composition in C by ‚ and composition in the ﬁbres
StatpXq by ˝. Given f : Y Ñ
‚ X in C, the induced functor Statpfq : StatpXq Ñ StatpY q acts by
pullback (compare Deﬁnition 4.2.24 of the functorial action of the self-indexing).
Remark 4.3.2. If we do not wish to make the full-support assumption, and instead we known that
the category C has a well-deﬁned notion of support object[126, 127, 146], then for a given general
channel c : XÑ
‚ Y , we can write the type of its Bayesian inversion c: as ř
π:CpI,Xq C
`
supppc‚πq, Y
˘
.
As Braithwaite and Hedges [127] shows, this corresponds to a morphism in a certain ﬁbration, and
gives rise to a category of dependent Bayesian lenses.
Notation 4.3.3. Just as we wrote X
M
ÝÑ Y for an internally M-parameterized morphism in
CpM d X, Y q (see Proposition 3.2.2) and A Θ
ÝÑ B for an externally Θ-parameterized morphism
in E
`
Θ, CpA, Bq
˘
(see Deﬁnition 3.2.8), we write A X
ÝÑ
‚ B for an X-state-dependent morphism
in Set
`
CpI, Xq, CpA, Bq
˘
. Given a state ρ in CpI, Xq and an X-state-dependent morphism f :
A X
ÝÑ
‚ B, we write fρ for the resulting morphism in CpA, Bq.
Remark 4.3.4. The similarities between state-dependent channels and externally parameterized
functions are no coincidence: the indexed category Stat is closely related to an indexed category
underlying external parameterization in Set, which in previous work, reported by Capucci et al.
[95], we called Prox (for ‘proxies’).
When C is a Kleisli category KℓpTq, it is of course possible to deﬁne a variant of Stat on the
other side of the product-exponential adjunction, with state-dependent morphisms A X
ÝÑ
‚ B having
144

the types TX ˆ A Ñ TB. This avoids the technical diﬃculties sketched in the preceding example
at the cost of requiring a monad T. However, the exponential form makes for better exegesis, and
so we will stick to that.
We will want to place inference systems side-by-side, which means we want a monoidal category
structure for Bayesian lenses. Following Corollary 4.2.30, this means Stat needs to be a monoidal
indexed category.
Proposition 4.3.5. Stat is a strong monoidal indexed category, in the sense of Deﬁnition 4.2.16.
The components µXY : StatpXq ˆ StatpY q Ñ StatpX b Y q of the laxator are deﬁned on objects
by µXY pA, A1q :“ A b A1 and on morphisms f : A X
ÝÑ
‚ B and f1 : A1 YÝÑ
‚ B1 as the X b Y -state-
dependent morphism denoted f b f1 and given by the function
µXY pf, f1q : CpI, X b Y q Ñ CpA b A1, B b B1q
ω ÞÑ fωX b f1
ωY
.
Here, ωX and ωY are the X and Y marginals of ω, given by ωX :“ projX ‚ ω and ωY :“ projY ‚ ω.
The unit η : 1 Ñ StatpIq of the lax monoidal structure is the functor mapping the unique object
˚ : 1 to the unit object I : StatpIq.
At this point, we can turn to Bayesian lenses themselves.
4.3.2. Bayesian lenses
We deﬁne the category of Bayesian lenses in C to be the category of Grothendieck Stat-lenses.
Deﬁnition 4.3.6. The category BayesLensC of Bayesian lenses in C is the category LensStat of
Grothendieck lenses for the functor Stat. A Bayesian lens is a morphism in BayesLensC. Where
the category C is evident from the context, we will just write BayesLens.
Unpacking this deﬁnition, we ﬁnd that the objects of BayesLensC are pairs pX, Aq of objects
of C. Morphisms (that is, Bayesian lenses) pX, Aq ÞÑ pY, Bq are pairs pc, c:q of a channel c : XÑ
‚ Y
and a generalized Bayesian inversion c: : B X
ÝÑ
‚ A; that is, elements of the hom objects
BayesLensC
`
pX, Aq, pY, Bq
˘
: “ LensStat
`
pX, Aq, pY, Bq
˘
– CpX, Y q ˆ V
`
CpI, Xq, CpB, Aq
˘
.
The identity Bayesian lens on pX, Aq is pidX, idAq, where by abuse of notation idA : CpI, Y q Ñ
CpA, Aq is the constant map idA deﬁned in Equation (4.11) that takes any state on Y to the identity
on A.
145

The sequential composite pd, d:q  pc, c:q of pc, c:q : pX, Aq ÞÑ pY, Bq and pd, d:q : pY, Bq ÞÑ
pZ, Cq is the Bayesian lens
`
pd ‚ cq, pc: ˝ c˚d:q
˘
: pX, Aq ÞÑ pZ, Cq where pc: ˝ c˚d:q : C X
ÝÑ
‚ A
takes a state π : IÑ
‚ X to the channel c:
π ‚ d:
c‚π : CÑ
‚ A.
To emphasize the structural similarity between Bayesian and monoidal lenses, and visualize the
channel c:
π ‚ d:
c‚π, note that following Example 4.2.21, we can depict Bayesian lens composition
using the graphical calculus of Boisseau [135] as
c
c:
X
A
B
Y
X
d
d:
C
Z
Y
“
c
c:
X
A
d
d:
C
Z
c
.
Remark 4.3.7. Strictly speaking, these depictions are diagrams in Boisseau [135]’s calculus of
string diagrams for optics, which means that they are not direct depictions of the Bayesian lenses
themselves; rather they are depictions of the corresponding optics, which we deﬁne and elaborate in
St. Clere Smithe [81]. Brieﬂy, these optics are obtained by embedding the categories of forrwards and
backwards channels into their corresponding (co)presheaf categories and coupling them together
along the ‘residual’ category C; in the depictions, the string diagrams in the forwards and backwards
directions are thus interpreted in these diﬀerent categories. This explains why we are allowed to
‘copy’ the channel c in the depiction above, producing the right-hand side by pushing c through the
copier as if it were a comonoid morphism: it is because the comonoids in question are CpI, Xq and
CpI, Y q, and the function CpI, cq is indeed a comonoid morphism, even though c is in general not!
Remark 4.3.8. Note that the deﬁnition of Stat and hence the deﬁnition of BayesLensC do not
require C to be a copy-delete category, even though our motivating categories of stochastic channels
are; all that is required for the deﬁnition is that C is monoidal. On the other hand, as we can deﬁne
Bayesian lenses in any copy-delete category, we can deﬁne them in Set, where Setp1, Xq – X
for every set X: in this case, Bayesian lenses coincide with Cartesian lenses.
Of course, since Stat is a monoidal indexed category, BayesLensC is a monoidal category.
Proposition 4.3.9. BayesLensC is a monoidal category, with structure
`
pb, pI, Iq
˘
inherited
from C. On objects, deﬁne pA, A1q b pB, B1q :“ pA b A1, B b B1q. On morphisms pf, f:q :
pX, Aq ÞÑ pY, Bq and pg, g:q : pX1, A1q ÞÑ pY 1, B1q, deﬁne pf, f:q b pg, g:q :“ pf b g, f: b g:q,
146

where f: b g: : B b B1 XbX1
ÝÝÝÝÑ
‚
A b A1 acts on states ω : IÑ
‚ X b X1 to return the channel
f:
ωX b g:
ωX1, following the deﬁnition of the laxator µ in Proposition 4.3.5. The monoidal unit in
BayesLensC is the pair pI, Iq duplicating the unit in C. When C is moreover symmetric monoidal,
so is BayesLensC.
Proof sketch. The main result is immediate from Proposition 4.3.5 and Corollary 4.2.30. When b is
symmetric in C, the symmetry lifts to the ﬁbres of Stat and hence to BayesLensC.
But BayesLensC is not in general a copy-discard category.
Remark 4.3.10. Although BayesLensC is a monoidal category, it does not inherit a copy-discard
structure from C, owing to the bidirectionality of its component morphisms. To see this, we can
consider morphisms into the monoidal unit pI, Iq, and ﬁnd that there is generally no canonical
discarding map. For instance, a morphism pX, Aq ÞÑ pI, Iq consists in a pair of a channel XÑ
‚ I
(which may indeed be a discarding map) and a state-dependent channel I X
ÝÑ
‚ A, for which there is
generally no suitable choice satisfying the comonoid laws. Note, however, that a lens of the type
pX, Iq ÞÑ pI, Bq might indeed act by discarding, since we can choose the constant state-dependent
channel B X
ÝÑ
‚ I on the discarding map
: BÑ
‚ I. By contrast, the Grothendieck category
ş
Stat
is a copy-delete category, as the morphisms pX, Aq Ñ pI, Iq in
ş
Stat are pairs XÑ
‚ I and A X
ÝÑ
‚ I,
and so for both components we can choose morphisms witnessing the comonoid structure.
4.3.3. Bayesian updates compose optically
In this section we prove the fundamental result that justiﬁes the development of statistical games as
hierarchical inference systems in Chapter 6: that the Bayesian inversion of a composite channel is
given up to almost-equality by the lens composite of the backwards components of the associated
‘exact’ Bayesian lenses.
Deﬁnition 4.3.11. Let pc, c:q : pX, Xq ÞÑ pY, Y q be a Bayesian lens. We say that pc, c:q is exact
if c admits Bayesian inversion and, for each π : IÑ
‚ X such that c ‚ π has full support, c and c:
π
together satisfy equation (4.5). Bayesian lenses that are not exact are said to be approximate.
Theorem 4.3.12. Let pc, c:q and pd, d:q be sequentially composable exact Bayesian lenses. Then
the contravariant component of the composite lens pd, d:q  pc, c:q “ pd ‚ c, c: ˝ c˚d:q is, up to
d ‚ c ‚ π-almost-equality, the Bayesian inversion of d ‚ c with respect to any state π on the domain
of c such that c ‚ π has non-empty support. That is to say, Bayesian updates compose optically:
pd ‚ cq:
π
d‚c‚π
„
c:
π ‚ d:
c‚π.
147

Proof. Suppose c:
π : Y Ñ
‚ X is the Bayesian inverse of c : XÑ
‚ Y with respect to π : IÑ
‚ X. Suppose
also that d:
c‚π : ZÑ
‚ Y is the Bayesian inverse of d : Y Ñ
‚ X with respect to c ‚ π : IÑ
‚ Y , and that
pd ‚ cq:
π : ZÑ
‚ X is the Bayesian inverse of d ‚ c : XÑ
‚ Z with respect to π : IÑ
‚ X:
d
π
c
Y
Z
“
d:
c‚π
π
c
d
Y
Z
and
c
π
d
X
Z
“
pd ‚ cq:
π
π
c
d
X
Z
The lens composite of these Bayesian inverses has the form c:
π ‚ d:
c‚π : ZÑ
‚ X, so to establish
the result it suﬃces to show that
d:
c‚π
π
c
d
c:
π
X
Z
“
c
π
d
X
Z
.
We have
d:
c‚π
π
c
d
c:
π
X
Z
“
d
π
c
c:
π
X
Z
“
c
π
d
X
Z
where the ﬁrst obtains because d:
c‚π is the Bayesian inverse of d with respect to c ‚ π, and the
second because c:
π is the Bayesian inverse of c with respect to π. Hence, c:
π ‚ d:
c‚π and pd ‚ cq:
π are
both Bayesian inversions of d ‚ c with respect to π. Since Bayesian inversions are almost-equal
(Proposition 4.1.28), we have c:
π ‚ d:
c‚π
d‚c‚π
„
pd ‚ cq:
π, as required.
148

Remark 4.3.13. Note that, in the context of ﬁnitely-supported probability (i.e., in KℓpDq), almost-
equality coincides with simple equality, and so Bayesian inversions are then just equal.
Historically, lenses have often been associated with ‘lens laws’: additional axioms guaranteeing
their well-behavedness. These laws originate in the context of database systems, and we now
investigate how well they are satisﬁed by Bayesian lenses, where one might see an inference system
as a kind of uncertain database. We will ﬁnd that Bayesian lenses are not lawful in this traditional
sense, because they ‘mix’ information.
4.3.4. Lawfulness of Bayesian lenses
The study of Cartesian lenses substantially originates in the context of bidirectional transformations
of data in the computer science and database community [132, 133], where we can think of the
view (or get) function as returning part of a database record, and the update (or put) function as
‘putting’ a part into a record and returning the updated record. In this setting, axioms known as
lens laws can be imposed on lenses to ensure that they are ‘well-behaved’ with respect to database
behaviour: for example, that updating a record with some data is idempotent (the ‘put-put’ law).
We might hope that well-behaved or “very well-behaved” lenses in the database context should
roughly correspond to our notion of exact Bayesian lens: with the view that an inference system,
formalized by a Bayesian lens, is something like a probabilistic database. However, as we will see,
even exact Bayesian lenses are only weakly lawful in the database sense: Bayesian updating mixes
information in the prior state (the ‘record’) with the observation (the ‘data’), rather than replacing
the prior information outright.
We will concentrate on the three lens laws that have attracted recent study [135, 137]: GetPut,
PutGet, and PutPut. A Cartesian lens satisfying the former two is well-behaved while a lens
satisfying all three is very well-behaved, in the terminology of Foster et al. [133]. Informally,
GetPut says that getting part of a record and putting it straight back returns an unchanged
record; PutGet says that putting a part into a record and then getting it returns the same part
that we started with; and PutPut says that putting one part and then putting a second part has
the same eﬀect on a record as just putting the second part (that is, update completely overwrites
the part in the record). We will express these laws graphically, and consider them each brieﬂy in
turn.
Note ﬁrst that we can lift any channel c in the base category C into any state-dependent ﬁbre
StatpAq using the constant (identity-on-objects) functor taking c to the constant-valued state-
indexed channel ρ ÞÑ c that maps any state ρ to c. We can lift string diagrams in C into the ﬁbres
accordingly.
149

GetPut
Deﬁnition 4.3.14. A lens pc, c:q is said to satisfy the GetPut law if it satisﬁes the left equality
in (4.12) below. Equivalently, because the copier induced by the Cartesian product is natural (i.e.,
˝ f “ pf ˆ fq ˝
), for any state π, we say that pc, c:q satisﬁes GetPut with respect to π if it
satisﬁes the right equality in (4.12) below.
c
c:
“
ùñ
π
c
c:
π
π
“
(4.12)
(Note that here we have written the copying map as
, since we are assuming an ambient Cartesian
monoidal structure; hence for a Bayesian lens we interpret the left diagram above in the image of
the Yoneda embedding.)
Proposition 4.3.15. When c is causal, the exact Bayesian lens pc, c:q satisﬁes the GetPut law
with respect to any state π for which c admits Bayesian inversion.
Proof. Starting from the right-hand-side of (4.12), we have the following chain of equalities
π
c:
π
π
c
c
π
“
“
π
“
π
c
c:
π
“
where the ﬁrst holds by the counitality of
, the second by the causality of c, the third since c
admits Bayesian inversion (4.5) with respect to π, and the fourth again by counitality.
Note that by Bayes’ law, exact Bayesian lenses only satisfy GetPut with respect to states. This
result means that, if we think of c as generating a prediction c ‚ π from a prior belief π, then if our
observation exactly matches the prediction, updating the prior π according to Bayes’ rule results in
no change.
150

PutGet
The PutGet law is characterized for a lens pv, uq by the following equality:
u
v
“
In general, PutGet does not hold for exact Bayesian lenses pc, c:q. However, because GetPut
holds with respect to states π, we do have c ‚ c:
π ‚ c ‚ π “ c ‚ π; that is, PutGet holds for exact
Bayesian lenses pc, c:q for the prior π and ‘input’ c ‚ π.
The reason PutGet fails to hold in general is that Bayesian updating mixes information from
the prior and the observation, according to the strength of belief. Consequently, updating a belief
according to an observed state and then producing a new prediction need not result in the same
state as observed; unless, of course, the prediction already matches the observation.
PutPut
Finally, the PutPut law for a lens pv, uq is characterized by the following equality:
u
u
u
“
PutPut fails to hold for exact Bayesian lenses for the same reason that PutGet fails to hold in
general: updates mix old and new beliefs, rather than entirely replace the old with the new.
Comment
In the original context of computer databases, there is assumed to be no uncertainty,
so a ‘belief’ is either true or false. Consequently, there can be no ‘mixing’ of beliefs; and in database
applications, such mixing may be highly undesirable. Bayesian lenses, on the other hand, live in a
fuzzier world: our present interest in Bayesian lenses originates in their application to describing
cognitive and cybernetic processes such as perception and action, and here the ability to mix beliefs
according to uncertainty is desirable.
Possibly it would be of interest to give analogous information-theoretic lens laws that characterize
exact and approximate Bayesian lenses and their generalizations; and we might then expect the
‘Boolean’ lens laws to emerge in the extremal case where there is no uncertainty and only Dirac
states. We leave such an endeavour for future work: Bayes’ law (4.5) is suﬃciently concise and
productive for our purposes here.
151

5. Open dynamical systems, coalgebraically
In Chapter 3, we saw how to compose neural circuits together using an algebraic approach to
connectomics together. These neural circuits are dynamical systems, formalized as sets of ordinary
diﬀerential equations. However, simply specifying these sets obscures the general compositional
structure of dynamical systems themselves, the revelation of which supports a subtler intertwining
of syntax and semantics, form and function—or, as it happens, algebra and coalgebra. In this
chapter we begin by introducing categorical language for describing general dynamical systems
‘behaviourally’. These systems will be ‘closed’ (non-interacting), and so we then explain how the
language of coalgebra, and speciﬁcally polynomial coalgebras, can be used to open them up.
However, traditional coalgebraic methods are restricted to discrete-time dynamical systems,
whereas we are interested in the continuous-time systems that are commonly used in science, such
as our earlier neural circuits. This motivates the development of a class of generalized polynomial
coalgebras that model open systems governed by a general time monoid, and which therefore
encompass systems of dynamically interacting ordinary diﬀerential equations. In order to account
for stochastic dynamics, we generalize the situation still further, by redeﬁning the category of
polynomial functors so that it can be instantiated in a nondeterministic setting. This will show us
how to deﬁne open Markov processes coalgebraically, and we also demonstrate related categories
of open random dynamical systems.
Finally, we use the polynomial setting to package these systems into monoidal bicategories of
‘hierarchical’ cybernetic systems, of which some are usefully generated diﬀerentially. In the next
chapter, these bicategories will provide the setting in which we cast the dynamical semantics of
approximate inference.
5.1. Categorical background on dynamics and coalgebra
In this section, we introduce the background material needed for the development of our
development of open dynamical systems as polynomial coalgebras.
5.1.1. Dynamical systems and Markov chains
We begin by recalling a ‘behavioural’ approach to dynamical systems popularized by Lawvere
and Schnauel [147] (who give a pedagogical account). These systems are ‘closed’ in the sense that
152

they do not require environmental interaction for their evolution. Later, when we consider open
systems, their ‘closures’ (induced by interaction with an environment) will constitute dynamical
systems of this form.
The evolution of dynamics is measured by time, and we will take time to be represented by
an arbitrary monoid pT, `, 0q. This allows us to consider time-evolution that is not necessarily
reversible, such as governed by N or R`, as well as reversible evolution that is properly governed
by groups such as Z or R. With this in mind, we give a classic deﬁnition of dynamical system, as a
T-action.
Deﬁnition 5.1.1. Let pT, `, 0q be a monoid, representing time. Let X : E be some space, called
the state space. Then a closed dynamical system ϑ with state space X and time T is an action of
T on X. When T is also an object of E, then this amounts to a morphism ϑ : T ˆ X Ñ X (or
equivalently, a time-indexed family of X-endomorphisms, ϑptq : X Ñ X), such that ϑp0q “ idX
and ϑps ` tq “ ϑpsq ˝ ϑptq. In this dynamical context, we will refer to the action axioms as the
ﬂow conditions, as they ensure that the dynamics can ‘ﬂow’.
Note that, in discrete time, this deﬁnition implies that a dynamical system is governed by a single
transition map.
Proposition 5.1.2. When time is discrete, as in the case T “ N, any dynamical system ϑ is entirely
determined by its action at 1 : T. That is, letting the state space be X, we have ϑptq “ ϑp1q˝t
where ϑp1q˝t means “compose ϑp1q : X Ñ X with itself t times”.
Proof. The proof is by (co)induction on t : T. We must have ϑp0q “ idX and ϑpt`sq “ ϑptq˝ϑpsq.
So for any t, we must have ϑpt`1q “ ϑptq˝ϑp1q. The result follows immediately; note for example
that ϑp2q “ ϑp1 ` 1q “ ϑp1q ˝ ϑp1q.
An ordinary diﬀerential equation 9x “ fpxq deﬁnes a vector ﬁeld x ÞÑ px, fpxqq on its state
space X, and its solutions xptq for t : R deﬁne in turn a closed dynamical system, as the following
example sketches.
Example 5.1.3. Let T denote the tangent bundle functor E Ñ E on the ambient category of spaces
E. Suppose X : U Ñ TU is a vector ﬁeld on U, with a corresponding solution (integral curve)
χx : R Ñ U for all x : U; that is, χ1ptq “ Xpχxptqq and χxp0q “ x. Then letting the point x vary,
we obtain a map χ : R ˆ U Ñ U. This χ is a closed dynamical system with state space U and time
R.
So far, we have left the ambient category implicit, and abstained from using much categorical
language. But these closed dynamical systems have a simple categorical representation.
153

Proposition 5.1.4. Closed dynamical systems with state spaces in E and time T are the objects
of the functor category CatpBT, Eq, where BT is the delooping of the monoid T. Morphisms of
dynamical systems are therefore natural transformations.
Proof. The category BT has a single object ˚ and morphisms t : ˚ Ñ ˚ for each point t : T; the
identity is the monoidal unit 0 : T and composition is given by `. A functor ϑ : BT Ñ E therefore
picks out an object ϑp˚q : E, and, for each t : T, a morphism ϑptq : ϑp˚q Ñ ϑp˚q, such that
the functoriality condition is satisﬁed. Functoriality requires that identities map to identities and
composition is preserved, so we require that ϑp0q “ idϑp˚q and that ϑps ` tq “ ϑpsq ˝ ϑptq. Hence
the data for a functor ϑ : BT Ñ E amount to the data for a closed dynamical system in E with
time T, and the functoriality condition amounts precisely to the ﬂow condition. A morphism of
closed dynamical systems f : ϑ Ñ ψ is a map on the state spaces f : ϑp˚q Ñ ψp˚q that commutes
with the ﬂow, meaning that f satisﬁes f ˝ ϑptq “ ψptq ˝ f for all times t : T; this is precisely the
deﬁnition of a natural transformation f : ϑ Ñ ψ between the corresponding functors.
By changing the state space category E, this simple framework can represent diﬀerent kinds of
dynamics. For example, by choosing E to be a category of stochastic channels, such as KℓpDq or
sfKrn, we obtain categories of closed Markov processes.
Example 5.1.5 (Closed Markov chains and Markov processes). A closed Markov chain is given
by a stochastic transition map XÑ
‚ X, typically interpreted as a Kleisli morphism X Ñ PX for
some probability monad P : E Ñ E. Following the discussion above, a closed Markov chain is
therefore an object in Cat
`
BN, KℓpPq
˘
. With more general time T, one obtains closed Markov
processes: objects in Cat
`
BT, KℓpPq
˘
. More explicitly, a closed Markov process is a time-indexed
family of Markov kernels; that is, a morphism ϑ : T ˆ X Ñ PX such that, for all times s, t : T,
ϑs`t “ ϑs ‚ ϑt as a morphism in KℓpPq. Note that composition ‚ in KℓpPq is typically given by
the Chapman-Kolmogorov equation, so this means that
ϑs`tpy|xq “
ż
x1:X
ϑspy|x1q ϑtpdx1|xq .
5.1.2. Coalgebra
We saw above that a closed discrete-time deterministic dynamical system is a function X Ñ X,
and that a closed discrete-time Markov chain is a function X Ñ PX. This suggests a general
pattern for discrete-time dynamical systems, as morphisms X Ñ FX for some endofunctor F:
such a morphism is called a coalgebra for the endofunctor F.
Deﬁnition 5.1.6. Let F : E Ñ E be an endofunctor. A coalgebra for F, or F-coalgebra, is a pair
pX, cq of an object X : E and a morphism c : X Ñ FX.
154

A morphism of F-coalgebras or coalgebra morphism pX, cq Ñ pX1, c1q is a morphism f : X Ñ
X1 that commutes with the coalgebra structures; i.e., that makes the following diagram commute:
X
X1
FX
FX1
f
c
c1
Ff
F-coalgebras and their morphisms constitute a category, denoted CoalgpFq.
The identity
morphism on pX, cq is simply the identity morphism idX : X Ñ X.
Remark 5.1.7. In §3.4.1, we brieﬂy discussed the notion of coalgebra for a comonad, which is a
coalgebra in the sense of the preceding deﬁnition that additionally satisﬁes axioms dual to those
deﬁning algebras for a monad (Deﬁnition 3.4.16). In our dynamical applications, the endofunctors
not in general be comonads, and so it does not make sense to demand such axioms.
Remark 5.1.8. At the same time, the duality of algebra and coalgebra underlies the subtle powers
of the ﬁeld of coalgebraic logic, in which the algebraic structure of logical syntax is used to deﬁne
constraints on or propositions about the behaviours of dynamical systems[148–152]. These tools
are particularly useful in setting of formal veriﬁcation, where it is desirable to prove that systems
behave according to a speciﬁcation (for instance, for safety reasons).
With the notion of F-coalgebra to hand, we immediately obtain categories of closed discrete-time
deterministic systems and Markov chains:
Example 5.1.9. The category of closed discrete-time deterministic dynamical systems in E is the
category Coalgpidq of coalgebras for the identity endofunctor idE : E Ñ E.
Example 5.1.10. Let P : E Ñ E be a probability monad on E. The category of Markov chains is
the category CoalgpPq of P-coalgebras.
Of course, polynomial functors are endofunctors Set Ñ Set, so they come with a notion of
coalgebra, and we may ask how such objects behave.
Example 5.1.11. Suppose p : Set Ñ Set is a polynomial functor. A coalgebra for p is a function
c : X Ñ pX for some set X. By Deﬁnition 3.5.2, we can write p as ř
i : pp1qypris, and hence
the p-coalgebra c has the form c : X Ñ ř
i : pp1qXpris. Such a function corresponds to a
choice, for each x : X, of an element of pp1q which we denote copxq and an associated function
cu
x : prc1pxqs Ñ X. We can therefore write c equivalently as a pair pco, cuq where cu is the
coproduct ř
x cu
x : ř
x prcopxqs Ñ X. We think of p as deﬁning the interface of the dynamical
system represented by c, with pp1q encoding the set of possible ‘outputs’ or ‘conﬁgurations’ of the
system, each pris the set of possing ‘inputs’ for the system when it is in conﬁguration i : pp1q, and
155

X as the dynamical state space. The coalgebra c can then be understood as an open discrete-time
dynamical system: the map cu takes a state x : X and a corresponding input in prcopxqs and returns
the next state; and the map co takes a state x : X and returns the system’s corresponding output or
conﬁguration copxq.
A pair of functions co : X Ñ pp1q and cu : ř
x prcopxqs Ñ X is precisely a morphism
c : XyX Ñ p of polynomials, and so we have established a mapping from p-coalgebras pX, cq to
morphisms XyX Ñ p. In fact, we have a stronger result.
Proposition 5.1.12. There is an isomorphism of hom-sets PolypAyB, pq – SetpA, pBq natural
in A, B, p, and hence adjunctions p´qyB % p´q ˝ B : Poly Ñ Set and Ayp´q % p ˝ p´q :
Poly op Ñ Set.
Proof sketch. In Example 5.1.11, we established a mapping SetpA, pBq Ñ PolypAyB, pq for the
case where A “ B; the general case is analogous. The inverse mapping follows directly from
Proposition 3.5.4. Naturality in A and B follows from naturality of pre-composition; naturality in
p follows from naturality of post-composition.
Polynomial coalgebras therefore constitute a type of open discrete-time dynamical systems. But
what if we want open continuous-time dynamical systems: do these ﬁt into the coalgebra formalism?
In a diﬀerent direction, what if we want open Markov chains? In discrete time, we should be able to
consider coalgebras for composite endofunctors pP, but what if we want to do this in general time?
Let us turn now to answering these questions.
5.2. Open dynamical systems on polynomial interfaces
In this section, we begin by incorporating dynamical systems in general time into the coalgebraic
framework, before generalizing the notion of polynomial functor to incorporate ‘side-eﬀects’ such
as randomness. The resulting framework will allow us to deﬁne types of system of interest, such as
open Markov processes, quite generally using coalgebraic methods, and in the subsequent sections
we will make much use of the newly available compositionality.
5.2.1. Deterministic systems in general time
In this section, let us suppose for simplicity that the ambient category if Set. We will begin by
stating our general deﬁnition, before explaining the structures and intuitions that justify it.
Deﬁnition 5.2.1. A deterministic open dynamical system with interface p, state space S and time
T is a morphism β : SyS Ñ rTy, ps of polynomials, such that, for any section σ : p Ñ y of p, the
156

induced morphism
SyS
βÝÑ rTy, ps
rTy,σs
ÝÝÝÝÑ rTy, ys „
ÝÑ yT
is a ◁-comonoid homomorphism.
To see how such a morphism β is like an ‘open’ version of the closed dynamical systems
of §5.1.1, note that by the tensor-hom adjunction, β can equivalently be written with the type
TybSyS Ñ p. In turn, such a morphism corresponds to a pair pβo, βuq, where βo is the component
‘on conﬁgurations’ with the type T ˆ S Ñ pp1q, and βu is the component ‘on inputs with the
type ř
t:T
ř
s:S prβopt, sqs Ñ S. We will call the map βo the output map, as it chooses an output
conﬁguration for each state and moment in time; and we will call the map βu the update map, as it
takes a state s : S, a quantity of time t : T, and an input in prβopt, sqs, and returns a new state. We
might imagine the new state as being given by evolving the system from s for time t, and the input
as supplied while the system is in the conﬁguration corresponding to ps, tq.
It is, however, not suﬃcient to consider merely such pairs β “ pβo, βuq to be our open dynamical
systems, for we need them to be like ‘open’ monoid actions: evolving for time t then for time s
must be equivalent to evolving for time t ` s, given the same inputs. It is fairly easy to prove the
following proposition, whose proof we defer until after establishing the categories CoalgTppq,
when we prove it in an alternate form as Proposition 5.2.4.
Proposition 5.2.2. Comonoid homomorphisms SyS Ñ yT correspond bijectively with closed
dynamical systems with state space S, in the sense given by functors BT Ñ Set.
This establishes that seeking such a comonoid homomorphism will give us the monoid action
property that we seek, and so it remains to show that a composite comonoid homomorphism of the
form rTy, σs ˝ β is a closed dynamical system with the “right inputs”. Unwinding this composite,
we ﬁnd that the condition that it be a comonoid homomorphism corresponds to the requirement
that, for any t : T, the closure βσ : T ˆ S Ñ S of β by σ given by
βσptq :“ S
βoptq˚σ
ÝÝÝÝÝÑ
ÿ
s:S
prβopt, sqs
βu
ÝÑ S
constitutes a closed dynamical system on S. The idea here is that σ gives the ‘context’ in which we
can make an open system closed, thereby formalizing the “given the same inputs” requirement
above.
With this conceptual framework in mind, we are in a position to render open dynamical systems
on p with time T into a category, which we will denote by CoalgTppq. Its objects will be pairs
pS, βq with S and β an open dynamical on p with state space S; we will often write these pairs
equivalently as triples pS, βo, βuq, making explicit the output and update maps. Morphisms will be
maps of state spaces that commute with the dynamics:
157

Proposition 5.2.3. Open dynamical systems over p with time T form a category, denoted
CoalgTppq. Its morphisms are deﬁned as follows. Let ϑ :“ pX, ϑo, ϑuq and ψ :“ pY, ψo, ψuq be
two dynamical systems over p. A morphism f : ϑ Ñ ψ consists in a morphism f : X Ñ Y such
that, for any time t : T and section σ : pp1q Ñ
ř
i:pp1q
pris of p, the following naturality squares
commute:
X
ř
x:X
prϑopt, xqs
X
Y
ř
y:Y
prψopt, yqs
Y
ϑoptq˚σ
ϑuptq
f
f
ψoptq˚σ
ψuptq
The identity morphism idϑ on the dynamical system ϑ is given by the identity morphism idX on
its state space X. Composition of morphisms of dynamical systems is given by composition of the
morphisms of the state spaces.
Proof. We need to check unitality and associativity of composition. This amounts to checking that
the composite naturality squares commute. But this follows immediately, since the composite of
two commutative diagrams along a common edge is again a commutative diagram.
We can alternatively state Proposition 5.2.2 as follows, noting that the polynomial y represents
the trivial interface, exposing no conﬁguration to any environment nor receiving any signals from
it:
Proposition 5.2.4. CoalgT
idpyq is equivalent to the classical category CatpBT, Setq of closed
dynamical systems in Set with time T.
Proof. The trivial interface y corresponds to the trivial bundle id1 : 1 Ñ 1. Therefore, a dynamical
system over y consists of a choice of state space S along with a trivial output map ϑo “
:
T ˆ S Ñ 1 and a time-indexed update map ϑu : T ˆ S Ñ S. This therefore has the form of a
classical closed dynamical system, so it remains to check the monoid action. There is only one
section of id1, which is again id1. Pulling this back along the unique map ϑoptq : S Ñ 1 gives
ϑoptq˚ id1 “ idS. Therefore the requirement that, given any section σ of y, the maps ϑu ˝ ϑoptq˚σ
form an action means in turn that so does ϑu : TˆS Ñ S. Since the pullback of the unique section
id1 along the trivial output map ϑoptq “
: S Ñ 1 of any dynamical system in CoalgT
idpyq is the
identity of the corresponding state space idS, a morphism f : pϑp˚q, ϑu,
q Ñ pψp˚q, ψu,
q in
CoalgT
idpyq amounts precisely to a map f : ϑp˚q Ñ ψp˚q on the state spaces in Set such that the
naturality condition f ˝ ϑuptq “ ψuptq ˝ f of Proposition 5.1.4 is satisﬁed, and every morphism in
CatpBT, Setq corresponds to a morphism in CoalgT
idpyq in this way.
158

Now that we know that our concept of open dynamical system subsumes closed systems, let us
consider some more examples.
Example 5.2.5. Consider a dynamical system pS, ϑo, ϑuq with outputs but no inputs. Such a
system has a ‘linear’ interface p :“ Iy for some I; alternatively, we can write its interface p as the
‘bundle’ idI : I Ñ I. A section of this bundle must again be idI, and so ϑoptq˚ idI “ idS. Once
again, the update maps collect into to a closed dynamical system in CatpBT, Setq; just now we
have outputs ϑo : T ˆ S Ñ pp1q “ I exposed to the environment.
Proposition 5.2.6. When time is discrete, as with T “ N, any open dynamical system pX, ϑo, ϑuq
over p is entirely determined by its components at 1 : T. That is, we have ϑoptq “ ϑop1q : X Ñ pp1q
and ϑuptq “ ϑup1q : ř
x:X prϑopxqs Ñ X. A discrete-time open dynamical system is therefore a
triple pX, ϑo, ϑuq, where the two maps have types ϑo : X Ñ pp1q and ϑu : ř
x:X prϑopxqs Ñ X.
Proof. Suppose σ is a section of p. We require each closure ϑσ to satisfy the ﬂow conditions, that
ϑσp0q “ idX and ϑσpt`sq “ ϑσptq˝ϑσpsq. In particular, we must have ϑσpt`1q “ ϑσptq˝ϑσp1q.
By induction, this means that we must have ϑσptq “ ϑσp1q˝t (compare Proposition 5.1.2). Therefore
we must in general have ϑoptq “ ϑop1q and ϑuptq “ ϑup1q.
Example 5.2.7. We can express ‘open’ vector ﬁelds in this framework. Suppose therefore that X is
a diﬀerentiable manifold (and write X equally for its underlying set of points), and let 9x “ fpx, aq
and b “ gpxq, with f : X ˆ A Ñ TX and g : X Ñ B. Then, as for the ‘closed’ vector ﬁelds
of Example 5.1.3, this induces an open dynamical system pX,
ş
f, gq : CoalgRpByAq, where
ş
f : R ˆ X ˆ A Ñ X returns the pX, Aq-indexed solutions of f.
Example 5.2.8. The preceding example is easily extended to the case of a general polynomial
interface. Suppose similarly that 9x “ fpx, axq and b “ gpxq, now with f : ř
x:X prgpxqs Ñ TX
and g : X Ñ pp1q. Then we obtain an open dynamical system pX,
ş
f, gq : CoalgR
idppq, where
now
ş
f : R ˆ ř
x:X prgpxqs Ñ X is the ‘update’ and g : X Ñ pp1q the ‘output’ map.
By letting the polynomial p vary, it is quite straightforward to extend CoalgTppq to an opindexed
category CoalgT.
Proposition 5.2.9. CoalgT extends to an opindexed category CoalgT : PolyE Ñ Cat. On
objects (polynomials), it returns the categories above. On morphisms of polynomials, we simply
post-compose: given ϕ : p Ñ q and β : SyS Ñ rTy, ps, obtain SyS Ñ rTy, ps Ñ rTy, qs in the
obvious way.
When we introduced Poly in §3.5, it was as a “syntax for interacting adaptive systems”, and
we know that we can understand Poly multicategorically, as it has a monoidal structure pb, yq
159

allowing us to place systems’ interfaces side-by-side (and which therefore gives us a multicategory,
OPoly by Proposition 3.3.4). We motivated our development of coalgebraic dynamical systems as
a compositional extension of the sets of ordinary diﬀerential equations that we used to formalize
rate-coded neural circuits (Deﬁnition 3.3.10), and we have seen that linear circuit diagrams embed
into Poly (Remark 4.2.29).
One may wonder, therefore, whether the opindexed categories CoalgT might supply the general
“semantics for interacting adaptive systems” that we seek: more precisely, is CoalgT a Poly-
algebra? This question can be answered aﬃrmatively, as CoalgT is lax monoidal: more precisely,
it is a strong monoidal opindexed category.
Proposition 5.2.10. CoalgT is a strong monoidal opindexed category pPoly, b, yq
Ñ
pCat, ˆ, 1q.
Proof. We need to deﬁne a natural family of functors µp,q : CoalgTppq ˆ CoalgTpqq Ñ
CoalgTpp b qq constituting the laxator, and a unit η : 1 Ñ CoalgTpyq, along with associators α
and left and right unitors λ and ρ satisfying the pseudomonoid axioms of Deﬁnition 4.2.16.
The unit η : 1 Ñ CoalgTpyq is given by the trivial system p1, !, !q with the trivial state space
and the trivial interface: the output map is the unique map 1 Ñ 1 (the identity); likewise, the
update map is the unique map 1 ˆ 1 Ñ 1. Note that 1 ˆ 1 – 1.
The laxator µp,q is given on objects pX, ϑq : CoalgTppq and pY, ϕq : CoalgTpqq by the
µp,qpϑ, ϕq :“
`
XY, pϑϕq
˘
where the state space XY “ X ˆ Y and pϑϕq is the system given by
the right adjunct of
XY yXY b Ty
„b
y
ÝÝÝÝÑ XyX b Y yY b Ty b Ty
XyXbswapbTy
ÝÝÝÝÝÝÝÝÝÝÑ XyX b Ty b Y yY b Ty
ϑ5bϕ5
ÝÝÝÝÑ p b q
under the tensor-hom adjunction in Poly, where ϑ5 and ϕ5 are the corresponding left adjuncts of
ϑ and ϕ, and where „ is the isomorphism XY yXY
„
ÝÑ XyX b Y yY . On morphisms f : pX, ϑq Ñ
pX1, ϑ1q and g : pY, ϕq Ñ pY 1, ϕq, µp,q acts as µp,qpf, gq :“ f ˆg; functoriality hence follows from
that of ˆ.
Next, we need to deﬁne µ on morphisms ζ : p Ñ p1 and ξ : q Ñ q1 of polynomials, giving
natural isomorphisms µζ,ξ : µp1,q1 ˝
`
CoalgTpζq ˆ CoalgTpξq
˘
ñ CoalgTpζ b ξq ˝ µp,q. But it is
easy to see in fact that µp1,q1 ˝
`
CoalgTpζq ˆ CoalgTpξq
˘
“ CoalgTpζ b ξq ˝ µp,q, as both sides
act by post-composing ζ b ξ. Hence CoalgT will be strong monoidal.
The associator is deﬁned componentwise on objects as
αp,q,r :
´
pXY qZypXY qZ
pϑbϕqbψ
ÝÝÝÝÝÝÑ rppbqqbr, Tys
¯
ÞÑ
´
XpY ZqyXpY Zq ϑbpϕbψq
ÝÝÝÝÝÝÑ rpbpqbrq, Tys
¯
and on morphisms as αp,q,r : pf ˆ gq ˆ h ÞÑ f ˆ pg ˆ hq, implicitly using the associators of b on
Poly and ˆ on Set.
160

Likewise, the left unitor is deﬁned by
λp :
´
1Xy1X
µy,ppη,ϑq
ÝÝÝÝÝÑ ry b p, Tys
¯
ÞÑ
´
XyX
ϑÝÑ rp, Tys
¯
implicitly using the left unitors of b on Poly and ˆ on Set; and the right unitor is deﬁned dually,
using the corresponding right unitors on Poly and Set.
That the associators and unitors satisfy the indexed monoidal category axioms follows from
the satisfaction of the monoidal category axioms by pPoly, b, yq and pSet, ˆ, 1q. (But it is easy,
though laborious, to verify this manually.)
Remark 5.2.11. Note that, although a strong monoidal indexed category, the functor CoalgT
itself really is lax monoidal—the laxators are not equivalences—since not all systems over the
parallel interface p b q factor into a system over p alongside a system over q.
With this indexed monoidal structure, we can show that, as we might hope from a general
semantics for interacting dynamical systems, Coalg subsumes our earlier linear circuit algebra of
rate-coded neural circuits.
Proposition 5.2.12. There is an inclusion ι of monoidal indexed categories as in the diagram
`
LinCirc, `, p0, 0q
˘
pSet, ˆ, 1q
pPoly, b, yq
pCat, ˆ, 1q
R
CoalgR
ι
where R is the algebra from Proposition 3.3.12.
Proof sketch. ι is deﬁned by a family of functors ιpno,niq : Rpno, niq Ñ CoalgRpRnoyRniq, where
each set Rpno, niq is treated as the corresponding discrete category; this means that ιpno,niq is
trivially functorial, and needs only be deﬁned on objects (rate-coded neural circuits). Each such
circuit pλ, α, β, γ, Wq deﬁnes an ‘open’ ordinary diﬀerential equation by Deﬁnition 3.3.10 with
inputs i : Rni. ιpno,niq is then deﬁned by taking this ordinary diﬀerential equation to a corresponding
open dynamical system following Example 5.2.7, where the output space is the same as the state
space Rno and the output map is idRno.
We then need to check that this deﬁnition of ι is natural, meaning that the following diagram
commutes for each linear circuit diagram pA, Bq : pno, niq Ñ pmo, miq, where CoalgRpA, Bq is
deﬁned by treating pA, Bq as a lens and hence a morphism of monomials of the type indicated.
Rpno, niq
Rpmo, miq
CoalgRpRnoyRniq
CoalgRpRmoyRmiq
RpA,Bq
CoalgRpA,Bq
ιpno,niq
ιpmo,miq
161

To see that this diagram commutes, observe that we can write a rate-coded neural circuit κ as
a morphism RnoyTRno Ñ RnoyRni of polynomials, where T is the tangent bundle functor; and
observe that the action of RpA, Bq is to post-compose the lens pA, Bq after κ, as in RnoyTRno
κÝÑ
RnoyRni
pA,Bq
ÝÝÝÑ RmoyRmi. Now, ιpno,niq acts by taking κ to a system RnoyRno Ñ rTy, RnoyRnis,
and CoalgRpA, Bq post-composes rTy, pA, Bqs, so we obtain the system
RnoyRno
ιpno,niqpκq
ÝÝÝÝÝÝÑ rTy, RnoyRnis
rTy,pA,Bqs
ÝÝÝÝÝÝÑ rTy, RmoyRmis .
This is precisely the system obtained by applying ιpmo,miq to pA, Bq ˝ κ, and hence ι is natural.
Finally, it is easy to check that ι is a monoidal natural transformation (Deﬁnition 3.1.12), which
by Moeller and Vasilakopoulou [131, Proposition 3.6] entails that ι is a morphism of monoidal
indexed categories. That it is additionally an inclusion follows from the evident fact that each
functor ιpno,niq is an embedding.
At some point during the preceding exposition, the reader may have wondered in what sense
these open dynamical systems are coalgebras. To answer this, recall from Proposition 5.1.12 that a
polynomial morphism SyS Ñ q is equivalently a function S Ñ qS and hence by Example 5.1.11
a q-coalgebra. Then, by setting q “ rTy, ps, we see the connection immediately: the objects of
CoalgTppq are rTy, ps-coalgebras that satisfy the ◁-comonoid condition, and the morphisms of
CoalgTppq are coalgebra morphisms.
In the following subsection, we generalize the constructions above to allow for non-deterministic
(‘eﬀectful’) feedback, using a generalization of the category Poly.
5.2.2. Polynomials with ‘eﬀectful’ feedback, and open Markov processes
The category Poly of polynomial functors Set Ñ Set can be considered as a category of
‘deterministic’ polynomial interaction; notably, morphisms of such polynomials, which we take to
encode the coupling of systems’ interfaces, do not explicitly incorporate any kind of randomness
or uncertainty. Even if the universe is deterministic, however, the ﬁniteness of systems and their
general inability to perceive the totality of their environments make it a convenient modelling
choice to suppose that systems’ interactions may be uncertain; this will be useful not only in
allowing for stochastic interactions between systems, but also to deﬁne stochastic dynamical
systems ‘internally’ to a category of polynomials.
To reach the desired generalization, we begin by recalling that Poly is equivalent to the category
of Grothendieck lenses for the self-indexing of Set (Example 4.2.26). We deﬁne our categories of
generalized polynomials from this perspective, by considering Kleisli categories indexed by their
“deterministic subcategories”. This allows us to deﬁne categories of Grothendieck lenses which
behave like Poly when restricted to the deterministic case, but also admit uncertain inputs. In
order to apply the Grothendieck construction, we begin by deﬁning an indexed category.
162

Deﬁnition 5.2.13. Suppose E is a category with all limits, and suppose M : E Ñ E is a monad
on E. Deﬁne the indexed category EM{´ : E op Ñ Cat as follows. On objects B : E, we deﬁne
EM{B to be the full subcategory of KℓpMq{B on those objects ιp : EÑ
‚ B which correspond
to maps E
pÝÑ B
ηB
ÝÝÑ MB in the image of ι. Now suppose f : C Ñ B is a map in E. We
deﬁne EM{f : EM{B Ñ EM{C as follows. The functor EM{f takes objects ιp : EÑ
‚ B to
ιpf˚pq : f˚EÑ
‚ C where f˚p is the pullback of p along f in E, included into KℓpMq by ι.
To deﬁne the action of EM{f on morphisms α : pE, ιp : EÑ
‚ Bq Ñ pF, ιq : FÑ
‚ Bq,
note that since we must have ιq ‚ α “ ιp, we can alternatively write α as the B-dependent
sum ř b : Bαb : ř
b:B prbs Ñ ř
b:B Mqrbs. Then we can deﬁne pEM{fqpαq accordingly as
pEM{fqpαq :“ ř
c:C αfpcq : ř
c:C prfpcqs Ñ ř
c:C Mqrfpcqs.
Deﬁnition 5.2.14. We deﬁne PolyM to be the category of Grothendieck lenses for EM{´. That
is, PolyM :“
ş
EM{´ op, where the opposite is again taken pointwise.
Example 5.2.15. When E “ Set and M “ idSet, Deﬁnition 5.2.13 recovers our earlier deﬁnition
of Poly.
Example 5.2.16. When M is a monad on Set, we ﬁnd that the objects of PolyM are the same
polynomial functors as constitute the objects of Poly. The morphisms f : p Ñ q are pairs pf1, f#q,
where f1 : B Ñ C is a function in Set and f# is a family of morphisms qrf1pxqsÑ
‚ prxs in KℓpMq,
making the following diagram commute:
ř
x:B Mprxs
ř
b:B qrf1pxqs
ř
y:C qrys
B
B
C
f#
q
ηB˚p
f1
{
Remark 5.2.17. Consequently, we can think of PolyM as a dependent version of the category of
M-monadic lenses, in the sense of Clarke et al. [153, §3.1.3].
Remark 5.2.18. Since E is assumed to have all limits, it must have a product structure pˆ, 1q.
When M is additionally a monoidal monad (Deﬁnition 4.1.16), then PolyM acquires a tensor akin
to that deﬁned for Poly in Proposition 3.5.7, and which we also denote by pb, Iq: the deﬁnition
only diﬀers by substituting the structure pb, Iq on KℓpMq for the product pˆ, 1q on Set. This
monoidal structure follows as before from the monoidal Grothendieck construction: EM{´ is lax
monoidal, with laxator taking p : EM{B and q : EM{C to p b q : EM{pB b Cq.
On the other hand, for PolyM also to have an internal hom rq, rs requires each ﬁbre of EM{´
to be closed with respect to the monoidal structure. In cases of particular interest, E will be
locally Cartesian closed, and restricting EM{´ to the self-indexing E{´ gives ﬁbres which are thus
Cartesian closed. In these cases, we can think of the broader ﬁbres of EM{´, and thus PolyM itself,
163

as being ‘deterministically’ closed. This means, for the stochastic example PolyP for P a probability
monad, we get an internal hom satisfying the adjunction PolyPpp b q, rq – PolyPpp, rq, rsq only
when the backwards components of morphisms p b q Ñ r are ‘uncorrelated’ between p and q.
Remark 5.2.19. For PolyM to behave faithfully like the category Poly of polynomial functors
of sets and their morphisms, we should want the substitution functors EM{f : EM{C Ñ EM{B to
have left and right adjoints (corresponding respectively to dependent sum and product). Although
we do not spell it out here, it is quite straightforward to exhibit concretely the left adjoints. On
the other hand, writing f˚ as shorthand for EM{f, we can see that a right adjoint only obtains
in restricted circumstances. Denote the putative right adjoint by Πf : EM{B Ñ EM{C, and for
ιp : EÑ
‚ B suppose that pΠfEqrys is given by the set of ‘partial sections’ σ : f´1tyu Ñ TE of p
over f´1tyu as in the commutative diagram:
f´1tyu
tyu
TE
B
C
f
{
ηB˚p
σ
Then we would need to exhibit a natural isomorphism EM{Bpf˚D, Eq – EM{CpD, ΠfEq. But
this will only obtain when the ‘backwards’ components h#
y : Drys Ñ MpΠfEqrys are in the image
of ι—otherwise, it is not generally possible to pull f´1tyu out of M.
Despite these restrictions, we do have enough structure at hand to instantiate CoalgT in PolyM.
The only piece remaining is the composition product ◁, but for our purposes it suﬃces to deﬁne
its action on objects, which is identical to its action on objects in Poly1, and then to consider
◁-comonoids in PolyM. The comonoid laws force the structure maps to be deterministic (i.e., in
the image of ι), and so ◁-comonoids in PolyM are just ◁-comonoids in PolyE.
Finally, we note that, even if the internal hom r´, ´s is not available in general, we can deﬁne
morphisms β : SyS Ñ rTy, ps: these again just correspond to morphisms Ty b SyS Ñ p, and the
condition that the backwards maps be uncorrelated between Ty and p is incontrovertibly satisﬁed
because Ty has a trivial exponent. Unwinding such a β according to the deﬁnition of PolyM indeed
gives precisely a pair pβo, βuq of the requisite types; and a comonoid homomorphism SyS Ñ yT in
PolyM is precisely a functor BT Ñ KℓpMq, thereby establishing equivalence between the objects
of CoalgTppq established in PolyM and the objects of CoalgT
Cppq.
Henceforth, therefore, we will write CoalgT
M to denote the instantiation of CoalgT in PolyM.
We will call the objects of CoalgT
Mppq pM-coalgebras with time T, and to get a sense of how, in
the case where M is a probability monad, they provide a notion of open Markov process, we can
read oﬀthe deﬁnition a little more explictly.
1We leave the full exposition of ◁in PolyM to future work.
164

Proposition 5.2.20. A pM-coalgebra with time T consists in a triple ϑ :“ pS, ϑo, ϑuq of a state
space S : E and two morphisms ϑo : T ˆ S Ñ pp1q and ϑu : ř
t:T
ř
s:S prϑopt, sqs Ñ MS, such
that, for any section σ : pp1q Ñ ř
i:pp1q pris of p in E, the maps ϑσ : T ˆ S Ñ MS given by
ÿ
t:T
S
ϑoptq˚σ
ÝÝÝÝÝÑ
ÿ
t:T
ÿ
s:S
prϑopt, sqs ϑu
ÝÑ MS
constitute an object in the functor category Cat
`
BT, KℓpMq
˘
, where BT is the delooping of T
and KℓpMq is the Kleisli category of M. (Once more, we call the closed system ϑσ, induced by a
section σ of p, the closure of ϑ by σ.)
Following Example 5.1.5 and the intuition of Example 5.2.8, we can see how this produces an
open version of a Markov process.
Since stochastic dynamical systems are often alternatively presented as random dynamical
systems, we now brieﬂy consider how these can be incorporated into the coalgebraic framework.
5.2.3. Open random dynamical systems
In the analysis of stochastic systems, it is often fruitful to consider two perspectives: on one side,
one considers explicitly the evolution of the distribution of the states of the system, by following
(for instance) a Markov process, or Fokker-Planck equation. On the other side, one considers the
system as if it were a deterministic system, perturbed by noisy inputs, giving rise to the frameworks
of stochastic diﬀerential equations and associated random dynamical systems.
Whereas a (closed) Markov process is typically given by the action of a time monoid on an
object in a Kleisli category of a probability monad, a (closed) random dynamical system is given
by a bundle of closed dynamical systems, where the base system is equipped with a probability
measure which it preserves: the idea being that a random dynamical system can be thought of as
a ‘random’ choice of dynamical system on the total space at each moment in time, with the base
measure-preserving system being the source of the randomness [154].
This idea corresponds in non-dynamical settings to the notion of randomness pushback [126,
Def. 11.19], by which a stochastic map f : A Ñ PB can be presented as a deterministic map
f5 : Ωˆ A Ñ B where pΩ, ωq is a probability space such that, for any a : A, pushing ω forward
through f5p-, aq gives the state fpaq; that is, ω induces a random choice of map f5pω, -q : A Ñ B.
Similarly, under nice conditions, random dynamical systems and Markov processes do coincide,
although they have diﬀerent suitability in applications.
In this section, we sketch how the generalized-coalgebraic structures developed above extend also
to random dynamical systems.We begin by deﬁning the concept of measure-preserving dynamical
system, which itself requires the notion of probability space (in order that measure can be preserved).
165

Deﬁnition 5.2.21. Suppose E is a category equipped with a probability monad P : E Ñ E and
a terminal object 1 : E. A probability space in E is an object of the slice 1{KℓpPq of the Kleisli
category of the probability monad under 1.
Proposition 5.2.22. There is a forgetful functor 1{KℓpPq Ñ E taking probability spaces pB, βq
to the underlying spaces B and their morphisms f : pA, αq Ñ pB, βq to the underlying maps
f : A Ñ PB. We will write B to refer to the space in E underlying a probability space pB, βq, in
the image of this forgetful functor.
Deﬁnition 5.2.23. Let pB, βq be a probability space in E. A closed metric or measure-preserving
dynamical system pϑ, βq on pB, βq with time T is a closed dynamical system ϑ with state space
B : E such that, for all t : T, Pϑptq ˝ β “ β; that is, each ϑptq is a pB, βq-endomorphism in
1{KℓpPq.
Proposition 5.2.24. Closed measure-preserving dynamical systems in E with time T form the
objects of a category CatpBT, EqP whose morphisms f : pϑ, αq Ñ pψ, βq are maps f : ϑp˚q Ñ
ψp˚q in E between the state spaces that preserve both ﬂow and measure, as in the following
commutative diagram, which also indicates their composition:
Pϑp˚q
Pϑp˚q
1
Pψp˚q
Pψp˚q
1
Pλp˚q
Pλp˚q
α
β
γ
α
β
γ
Pϑptq
Pψptq
Pλptq
Pf
Pf
Pg
Pg
Proof. The identity morphism on a closed measure-preserving dynamical system is the identity
map on its state space. It is easy to check that composition as in the diagram above is thus both
associative and unital with respect to these identities.
As we indicated in the introduction to this section, closed random dynamical systems are bundles
of deterministic over metric systems.
Deﬁnition 5.2.25. Let pϑ, βq be a closed measure-preserving dynamical system. A closed random
dynamical system over pϑ, βq is an object of the slice category CatpBT, Eq{ϑ; it is therefore a
bundle of the corresponding functors.
166

Example 5.2.26. The solutions Xpt, ω; x0q : R` ˆ Ωˆ M Ñ M to a stochastic diﬀerential
equation dXt “ fpt, Xtqdt ` σpt, XtqdWt, where W : R` ˆ ΩÑ M is a Wiener process in M,
deﬁne a random dynamical system R` ˆ Ωˆ M Ñ M : pt, ω, xq ÞÑ Xpt, ω; x0q over the Wiener
base ﬂow θ : R` ˆ ΩÑ Ω: pt, ωq ÞÑ Wps ` t, ωq ´ Wpt, ωq for any s : R`.
We can use the same trick, of opening up closed systems along a polynomial interface, to deﬁne
a notion of open random dynamical system — although at this point we do not have an elegant
concise deﬁnition.
Deﬁnition 5.2.27. Let pθ, βq be a closed measure-preserving dynamical system in E with time
T, and let p : PolyE be a polynomial in E. Write Ω:“ θp˚q for the state space of θ, and let
π : S Ñ Ωbe an object (bundle) in E{Ω. An open random dynamical system over pθ, βq on the
interface p with state space π : S Ñ Ωand time T consists in a pair of morphisms ϑo : TˆS Ñ pp1q
and ϑu : ř
t:T
ř
s:S
prϑopt, sqs Ñ S, such that, for any section σ : pp1q Ñ
ř
i:pp1q
pris of p, the maps
ϑσ : T ˆ S Ñ S deﬁned as
ÿ
t:T
S
ϑop´q˚σ
ÝÝÝÝÝÑ
ÿ
t:T
ÿ
s:S
prϑop´, sqs ϑu
ÝÑ S
form a closed random dynamical system in CatpBT, Eq{θ, in the sense that, for all t : T and
sections σ, the following diagram commutes:
S
ř
s:S
prϑopt, sqs
S
Ω
Ω
π
π
θptq
ϑoptq˚σ
ϑuptq
Proposition 5.2.28. Let pθ, βq be a closed measure-preserving dynamical system in E with time T,
and let p : PolyE be a polynomial in E. Open random dynamical systems over pθ, βq on the interface
p form the objects of a category RDynTpp, θq. Writing ϑ :“ pπX, ϑo, ϑuq and ψ :“ pπY , ψo, ψuq,
a morphism f : ϑ Ñ ψ is a map f : X Ñ Y in E making the following diagram commute for all
times t : T and sections σ of p:
X
ř
x:X
prϑopt, xqs
X
Ω
Ω
Y
ř
y:Y
prψopt, yqs
Y
πX
πX
θptq
ϑoptq˚σ
ϑuptq
ψoptq˚σ
ψuptq
πY
πY
f
f
167

Identities are given by the identity maps on state-spaces. Composition is given by pasting of
diagrams.
Proposition 5.2.29. The categories RDynTpp, θq collect into a doubly-indexed category of the
form RDynT : PolyE ˆ CatpBT, EqP Ñ Cat. By the universal property of the product ˆ in
Cat, it suﬃces to deﬁne the actions of RDynT separately on morphisms of polynomials and on
morphisms of closed measure-preserving systems.
Suppose therefore that ϕ : p Ñ q is a morphism of polynomials. Then, for each measure-
preserving system pθ, βq : CatpBT, EqP, we deﬁne the functor RDynTpϕ, θq : RDynTpp, θq Ñ
RDynTpq, θq as follows. Let ϑ :“ pπX : X Ñ Ω, ϑo, ϑuq : RDynTpp, θq be an object (open
random dynamical system) in RDynTpp, θq. Then RDynTpϕ, θqpϑq is deﬁned as the triple
pπX, ϕ1 ˝ ϑo, ϑu ˝ ϕo˚ϕ#q : RDynTpq, θq, where the two maps are explicitly the following
composites:
T ˆ X
ϑo
ÝÑ pp1q
ϕ1
ÝÑ qp1q ,
ÿ
t:T
ÿ
x:X
qrϕ1 ˝ ϑopt, xqs
ϑo˚ϕ#
ÝÝÝÝÑ
ÿ
t:T
ÿ
x:X
prϑopt, xqs ϑu
ÝÑ X .
On morphisms f : pπX : X Ñ Ω, ϑo, ϑuq Ñ pπY : Y Ñ Ω, ψo, ψuq, the image RDynTpϕ, θqpfq :
RDynTpϕ, θqpπX, ϑo, ϑuq Ñ RDynTpϕ, θqpπY , ψo, ψuq is given by the same underlying map
f : X Ñ Y of state spaces.
Next, suppose that φ : pθ, βq Ñ pθ1, β1q is a morphism of closed measure-preserving dynamical
systems, and let Ω1 :“ θ1p˚q be the state space of the system θ1. By Proposition 5.2.24, the morphism
φ corresponds to a map φ : ΩÑ Ω1 on the state spaces that preserves both ﬂow and measure.
Therefore, for each polynomial p : PolyE, we deﬁne the functor RDynTpp, φq : RDynTpp, θq Ñ
RDynTpp, θ1q by post-composition. That is, suppose given open random dynamical systems and
morphisms over pp, θq as in the diagram of Proposition 5.2.28. Then RDynTpp, φq returns the
following diagram:
X
ř
x:X
prϑopt, xqs
X
Ω1
Ω1
Y
ř
y:Y
prψopt, yqs
Y
θ1ptq
ϑoptq˚σ
ϑuptq
ψoptq˚σ
ψuptq
f
f
φ˝πY
φ˝πX
φ˝πY
φ˝πX
That is, RDynTpp, φqpϑq :“ pφ ˝ πX, ϑo, ϑuq and RDynTpp, φqpfq is given by the same
underlying map f : X Ñ Y on state spaces.
168

5.3. Cilia: monoidal bicategories of cybernetic systems
Whereas it is the morphisms (1-cells) of process-theoretic categories—such as categories of lenses,
or the categories of statistical games to be deﬁned in Chapter 6—that represent open systems, it is
the objects (0-cells) of the opindexed categories CoalgT
M
2 that play this role; in fact, the objects of
CoalgT
M each represent both an open system and its (polynomial) interface. In order to supply
dynamical semantics for statistical games—functors from categories of statistical games to categories
of dynamical systems—we need to cleave the dynamical systems from their interfaces, making the
interfaces into 0-cells and systems into 1-cells between them, thereby letting the systems’ types
and composition match those of the games. Doing this is the job of this section, which we ﬁrst
perform in the case of the general categories CoalgT
M, followed by the speciﬁc case of systems
generated diﬀerentially, as in the vector-ﬁeld Examples 5.2.7 and 5.2.8.
5.3.1. Hierarchical bidirectional dynamical systems
To construct “hierarchical bidirectional systems”, we will associate to each pair of objects pA, Sq
and pB, Tq of a category of (for our purposes, Bayesian) lenses a polynomial vAyS, ByT w whose
conﬁgurations correspond to lenses and whose inputs correspond to the lenses’ inputs. The
categories CoalgT
P
`
vAyS, ByT w
˘
will then form the hom-categories of bicategories of hierarchical
inference systems called cilia3, and it is in these bicategories that we will ﬁnd our dynamical
semantics.
Throughout this subsection, we will ﬁx a category C of stochastic channels, deﬁned by C :“
KℓpPq as the Kleisli category of a probability monad P : E Ñ E, which we will also take to deﬁne
a category PolyP of polynomials with stochastic feedback. We will assume P to be a monoidal
monad, and we will write the monoidal structure on C as pb, Iq. Finally, we will assume that C is
enriched in E.
Deﬁnition 5.3.1. Let BayesLens be the category of Bayesian lenses in C. Then for any pair of
objects pA, Sq and pB, Tq in BayesLens, we deﬁne a polynomial vAyS, ByT w in PolyP by
vAyS, ByT w :“
ÿ
l:BayesLens
`
pA,Sq,pB,Tq
˘ yCpI,AqˆT .
Remark 5.3.2. We can think of vAyS, ByT w as an ‘external hom’ polynomial for BayesLens,
playing a role analogous to the internal hom rp, qs in PolyP. Its ‘bipartite’ structure—with domain
and codomain parts—is what enables cleaving systems from their interfaces, which are given by
these parts. The deﬁnition, and the following construction of the monoidal bicategory, are inspired
by the operad Org introduced by Spivak [155] and generalized by St Clere Smithe [72].
2or, more precisely, their corresponding opﬁbrations
ş
CoalgT
M
3‘Cilia’, because they “control optics”, like the ciliary muscles of the eye.
169

Remark 5.3.3. Note that vAyS, ByT w is strictly speaking a monomial, since it can be written
in the form IyJ for I “ BayesLens
`
pA, Sq, pB, Tq
˘
and J “ CpI, Aq ˆ T. However, we have
written it in polynomial form with the view to extending it in future work to dependent lenses and
dependent optics [156, 157] and these generalized external homs will in fact be true polynomials.
Proposition 5.3.4. Deﬁnition 5.3.1 deﬁnes a functor BayesLens op ˆ BayesLens Ñ PolyP.
Suppose c :“ pc1, c#q : pZ, Rq ÞÑ pA, Sq and d :“ pd1, d#q : pB, Tq ÞÑ pC, Uq are Bayesian
lenses. We obtain a morphism of polynomials vc, dw : vAyS, ByT w Ñ vZyR, CyUw as follows.
Since the conﬁgurations of vAyS, ByT w are lenses pA, Sq ÞÑ pB, Tq, the forwards map acts by pre-
and post-composition:
vc, dw1 :“ d  p´q  c : BayesLens
`
pA, Sq, pB, Tq
˘
Ñ BayesLens
`
pZ, Rq, pC, Uq
˘
l ÞÑ d  l  c
For each such l, the backwards map vc, dw#
l has type CpI, Zq b U Ñ CpI, Aq b T in C, and is
obtained by analogy with the backwards composition rule for Bayesian lenses. We deﬁne
vc, dw#
l :“ CpI, Zq b U
c1˚bU
ÝÝÝÝÑ CpI, Aq b U
bU
ÝÝÝÑ CpI, Aq b CpI, Aq b U ¨ ¨ ¨
¨ ¨ ¨
CpI,Aqbl1˚bU
ÝÝÝÝÝÝÝÝÝÑ CpI, Aq b CpI, Bq b U
CpI,Aqbd#bU
ÝÝÝÝÝÝÝÝÝÑ CpI, Aq b CpU, Tq b U ¨ ¨ ¨
¨ ¨ ¨
CpI,AqbevU,T
ÝÝÝÝÝÝÝÝÑ CpI, Aq b T
where l1 is the forwards part of the lens l : pA, Sq ÞÑ pB, Tq, and c1˚ :“ CpI, c1q and l1˚ :“ CpI, l1q
are the push-forwards along c1 and l1, and evU,T is the evaluation map induced by the enrichment
of C in E.
Less abstractly, noting that C “ KℓpPq, we can write vc, dw#
l as the following map in E, depicted
as a string diagram:
vc, dw#
l
“
c1˚
l1˚
d5
PZ
U
PT
PA
str
Here, we have assumed that KℓpPqpI, Aq “ PA, and deﬁne d5 : PB ˆ U Ñ PT to be the image
of d# : PB Ñ KℓpPqpU, Tq under the Cartesian closure of E, and str : PA ˆ PT Ñ P
`
PA ˆ Tq
the (right) strength of the strong monad P.
170

Proof. We need to check that the mappings deﬁned above respect identities and composition. It is
easy to see that the deﬁnition preserves identities: in the forwards direction, this follows from the
unitality of composition in BayesLens; in the backwards direction, because pushing forwards
along the identity is again the identity, and because the backwards component of the identity
Bayesian lens is the constant state-dependent morphism on the identity in C.
To check that the mapping preserves composition, we consider the contravariant and covariant
parts separately. Suppose b :“ pb1, b#q : pY, Qq ÞÑ pZ, Rq and e :“ pe1, e#q : pC, Uq ÞÑ pD, V q
are Bayesian lenses. We consider the contravariant case ﬁrst: we check that vc  b, ByT w “
vb, ByT w ˝ vc, ByT w. The forwards direction holds by pre-composition of lenses. In the backwards
direction, we note from the deﬁnition that only the forwards channel c1 plays a role in vc, ByT w#
l ,
and that role is again pre-composition. We therefore only need to check that pc1 ‚ b1q˚ “ c1˚ ˝ b1˚,
and this follows immediately from the functoriality of CpI, ´q.
We now consider the covariant case, that vAyS, e  dw “ vAyS, ew ˝ vAyS, dw. Once again, the
forwards direction holds by composition of lenses. For simplicity of exposition, we consider the
backwards direction (with C “ KℓpPq) and reason graphically. In this case, the backwards map on
the right-hand side is given, for a lens l : pA, Sq ÞÑ pB, Tq by the following string diagram:
l1˚
e5
PA
V
d5˚
d1˚
str
PU
PA
It is easy to verify that the composition of backwards channels here is precisely the backwards
channel given by e  d—see Theorem 4.3.12—which establishes the result.
Now that we have an ‘external hom’, we might expect also to have a corresponding ‘external
composition’, represented by a family of morphisms of polynomials; we establish such a family
now, and it will be important in our bicategorical construction.
Deﬁnition 5.3.5. We deﬁne an ‘external composition’ natural transformation c, with components
vAyS, ByT w b vByT , CyUw Ñ vAyS, CyUw
given in the forwards direction by composition of Bayesian lenses. In the backwards direction, for
each pair of lenses c : pA, Sq ÞÑ pB, Tq and d : pB, Tq ÞÑ pC, Uq, we need a map
c#
c,d : CpI, Aq b U Ñ CpI, Aq b T b CpI, Bq b U
˘
171

which we deﬁne as follows:
c#
c,d :“ CpI, Aq b U
b
ÝÝÝÝÑ CpI, Aq b CpI, Aq b U b U ¨ ¨ ¨
¨ ¨ ¨
CpI,Aqbc1˚bUbU
ÝÝÝÝÝÝÝÝÝÝÝÑ CpI, Aq b CpI, Bq b U b U ¨ ¨ ¨
¨ ¨ ¨
CpI,Aqb
bCpI,BqbUbU
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ CpI, Aq b CpI, Bq b CpI, Bq b U b U
¨ ¨ ¨
CpI,AqbCpI,Bqbd#bUbU
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ CpI, Aq b CpI, Bq b CpU, Tq b Y b U
¨ ¨ ¨
CpI,AqbCpI,BqevU,T bU
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ CpI, Aq b CpI, Bq b T b U
¨ ¨ ¨
CpI,AqbswapbU
ÝÝÝÝÝÝÝÝÝÝÑ CpI, Aq b T b CpI, Bq b U
where c1˚ and evU,T are as in 5.3.4.
Since C “ KℓpPq, we can equivalently (and more legibly) deﬁne c#
c,d by the following string
diagram:
c#
c,d
:“
d5
c1˚
str
PA
PT
PB
U
PA
U
where d5 and str are also as in Proposition 5.3.4.
We leave to the reader the detailed proof that this deﬁnition produces a well-deﬁned natural
transformation, noting only that the argument is analogous to that of Proposition 5.3.4: one observes
that, in the forwards direction, the deﬁnition is simply composition of Bayesian lenses (which is
immediately natural); in the backwards direction, one observes that the deﬁnition again mirrors
that of the backwards composition of Bayesian lenses.
Next, we establish the structure needed to make our bicategory monoidal.
Deﬁnition 5.3.6. We deﬁne a distributive law d of v´, “w over b, a natural transformation with
components
vAyS, ByT w b vA1yS1, B1yT 1w Ñ vAyS b A1yS1, ByT b B1yT 1w ,
noting that AyS b A1yS1 “ pA b A1qypSbS1q and ByT b B1yT 1 “ pB b B1qypTbT 1q. The forwards
component is given simply by taking the tensor of the corresponding Bayesian lenses, using the
172

monoidal product (also denoted b) in BayesLens. Backwards, for each pair of lenses c : pA, Sq ÞÑ
pB, Tq and c1 : pA1, S1q ÞÑ pB1, T 1q, we need a map
d#
c,c1 : CpI, A b A1q b T b T 1 Ñ CpI, Aq ˆ T ˆ CpI, A1q ˆ T 1
for which we choose
CpI, A b A1q b T b T 1
bTbT 1
ÝÝÝÝÝÝÑ CpI, A b A1q b CpI, A b A1q b T b T 1 ¨ ¨ ¨
¨ ¨ ¨
CpI,projAqbCpI,projA1qbTbT 1
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ CpI, Aq b CpI, A1q b T b T 1 ¨ ¨ ¨
¨ ¨ ¨
CpI,AqbswapbT 1
ÝÝÝÝÝÝÝÝÝÝÑ CpI, Aq b T b CpI, A1q b T 1
where swap is the symmetry of the tensor b in C. Note that d#
c,c1 so deﬁned does not in fact depend
on either c or c1.
We now have everything we need to construct a monoidal bicategory CiliaT
C of dynamical
hierarchical inference systems in C, following the intuition outlined at the beginning of this section.
We call systems over such external hom polynomials cilia, as they “control optics”, akin to the
ciliary muscles of the eye. In future work, we will study the general structure of these categories
and their relationship to categorical systems theory [144, 158] and related work in categorical
cybernetics [159].
Deﬁnition 5.3.7. Let CiliaT
C denote the monoidal bicategory whose 0-cells are objects pA, Sq in
BayesLens, and whose hom-categories CiliaT
C
`
pA, Sq, pB, Tq
˘
are given by CoalgT
P
`
vAyS, ByT w
˘
.
The identity 1-cell idpA,Sq : pA, Sq Ñ pA, Sq on pA, Sq is given by the system with trivial state
space 1, trivial update map, and output map that constantly emits the identity Bayesian lens
pA, Sq ÞÑ pA, Sq. The composition of a system pA, Sq Ñ pB, Tq then a system pB, Tq Ñ pC, Uq
is deﬁned by the functor
CiliaT
C
`
pA, Sq, pB, Tq
˘
ˆ CiliaT
C
`
pB, Tq, pC, Uq
˘
“ CoalgT
P
`
vAyS, ByT w
˘
ˆ CoalgT
P
`
vByT , CyUw
˘
λÝÑ CoalgT
P
`
vAyS, ByT w b vByT , CyUw
˘
CoalgT
Ppcq
ÝÝÝÝÝÝÝÑ CoalgT
P
`
vAyS, CyUw
˘
“ CiliaT
C
`
pA, Sq, pC, Uq
˘
where λ is the laxator and c is the external composition morphism of Deﬁnition 5.3.5.
The monoidal structure pb, yq on CiliaT
C derives from the structures on PolyP and BayesLens,
justifying our overloaded notation. On 0-cells, pA, Sq b pA1, S1q :“ pA b A1, S b S1q. On 1-cells
173

pA, Sq Ñ pB, Tq and pA1, S1q Ñ pB1, T 1q, the tensor is given by
CiliaT
C
`
pA, Sq, pB, Tq
˘
ˆ CiliaT
C
`
pA1, S1q, pB1, T 1q
˘
“ CoalgT
P
`
vAyS, ByT w
˘
ˆ CoalgT
P
`
vA1yS1, B1yT 1w
˘
λÝÑ CoalgT
C
`
vAyS, ByT w b vA1yS1, B1yT 1w
˘
CoalgT
Ppdq
ÝÝÝÝÝÝÝÑ CoalgT
P
`
vAyS b A1yS1, ByT b B1yT 1w
˘
“ CiliaT
C
`
pA, Sq b pA1, S1q, pB, Tq b pB1, T 1q
˘
where d is the distributive law of Deﬁnition 5.3.6. The same functors
CiliaT
C
`
pA, Sq, pB, Tq
˘
ˆCiliaT
C
`
pA1, S1q, pB1, T 1q
˘
Ñ CiliaT
C
`
pA, SqbpA1, S1q, pB, TqbpB1, T 1q
˘
induce the tensor of 2-cells; concretely, this is given on morphisms of dynamical systems by taking
the product of the corresponding morphisms between state spaces.
We do not give here a proof that this makes CiliaT
C into a well-deﬁned monoidal bicategory;
brieﬂy, the result follows from the facts that the external composition c and the tensor b are
appropriately associative and unital, that CoalgT
P is lax monoidal, that v´, “w is functorial in both
positions, and that v´, “w distributes naturally over b.
Before we move on to considering doctrines of approximate inference, it will be useful to spell
out concretely the elements of a morphism pA, Sq Ñ pB, Tq in CiliaT
KℓpPq.
Proposition 5.3.8. Suppose P is a monad on a Cartesian closed category E. Then a 1-cell ϑ :
pA, Sq Ñ pB, Tq in CiliaT
KℓpPq is given by a tuple ϑ :“ pX, ϑo
1, ϑo
2, ϑuq of
• a choice of state space X,
• a forwards output map ϑo
1 : T ˆ X ˆ A Ñ PB in E,
• a backwards output map ϑo
2 : T ˆ X ˆ PA ˆ T Ñ PS in E, and
• an update map ϑu : T ˆ X ˆ PA ˆ T Ñ PX in E,
satisfying the ‘ﬂow’ condition of Proposition 5.2.20.
Proof. The result follows immediately upon unpacking the deﬁnitions, using the Cartesian closure
of E.
174

5.3.2. Diﬀerential systems
Approximate inference doctrines describe how systems play statistical games, and are particularly
of interest when one asks how systems’ performance may improve during such game-playing.
One prominent method of performance improvement involves descending the gradient of the
statistical game’s loss function, and we will see below that this method is adopted by both the
Laplace and the Hebb-Laplace doctrines. The appearance of gradient descent prompts questions
about the connections between such statistical systems and other ‘cybernetic’ systems such as deep
learners or players of economic games, both of which may also involve gradient descent [140, 159];
indeed, it has been proposed [93] that parameterized gradient descent should form the basis of a
compositional account of cybernetic systems in general4.
In order to incorporate gradient descent explicitly into our own compositional framework,
we follow the recipes above to deﬁne here ﬁrst a category of diﬀerential systems opindexed by
polynomial interfaces and then a monoidal bicategory of diﬀerential hierarchical inference systems.
We then show how we can obtain dynamical from diﬀerential systems by integration, and sketch
how this induces a “change of base” from dynamical to diﬀerential hierarchical inference systems.
Again we will assume a category C enriched in E, with C :“ KℓpPq for a monoidal monad P on
E. We will also assume some knowledge of basic diﬀerential geometry in this subsection.
Notation 5.3.9. Write DiﬀC for the subcategory of compact smooth manifold objects in E and
diﬀerentiable morphisms between them. Write T : DiﬀC Ñ VectpDiﬀCq for the corresponding
tangent bundle functor, where VectpDiﬀCq is (the total category of) the ﬁbration of vector bundles
over DiﬀC and their homomorphisms. Write U : VectpDiﬀCq Ñ DiﬀC for the functor that
forgets the bundle structure. Write T :“ UT : DiﬀC Ñ DiﬀC for the induced endofunctor.
Recall that morphisms AyB Ñ p in PolyP correspond to morphisms AÑ
‚ pB in C.
Deﬁnition 5.3.10. For each p : PolyP, deﬁne the category DiﬀSysCppq as follows. Its objects
are objects M : DiﬀC, each equipped with a morphism m : MyTM Ñ p of polynomials in
PolyP, such that for any section σ : p Ñ y of p, the composite morphism σ ˝ m : MyTM Ñ y
corresponds to a section mσ : M Ñ TM of the tangent bundle TM Ñ M. A morphism
α : pM, mq Ñ pM1, m1q in DiﬀSysCppq is a map α : M Ñ M1 in DiﬀC such that the following
4Our own view on cybernetics is somewhat more general, since not all systems that may be seen as cybernetic are
explicitly structured as gradient-descenders, and nor even is explicit diﬀerential structure always apparent. In earlier
work, we suggested that statistical inference was perhaps more inherent to cybernetics [70], although today we
believe that a better, though more informal, deﬁnition of cybernetic system is perhaps “an intentionally-controlled
open dynamical system”. Nonetheless, we acknowledge that this notion of “intentional control” may generally be
reducible to a stationary action principle, again indicating the importance of diﬀerential structure. We leave the
statement and proof of this general principle to future work.
175

diagram commutes:
M
pTM
M1
pTM1
m
α
m1
pTα
We obtain a monoidal opindexed category from this data in much the same way as we did for
CoalgT.
Proposition 5.3.11. DiﬀSysC deﬁnes an opindexed category PolyP Ñ Cat. Given a morphism
ϕ : p Ñ q of polynomials, DiﬀSysCpϕq : DiﬀSysCppq Ñ DiﬀSysCpqq acts on objects by
postcomposition and trivially on morphisms.
Proposition 5.3.12. The functor DiﬀSysC is lax monoidal pPolyC, b, yq Ñ pCat, ˆ, 1q.
Proof sketch. Note that T is strong monoidal, with Tp1q – 1 and TpMq b TpNq – TpM b Nq.
The unitor 1 Ñ DiﬀSysCpyq is given by the isomorphism 1yT1 – 1y1 – y induced by the strong
monoidal structure of T. The laxator λp,q : DiﬀSysCppq ˆ DiﬀSysCpqq Ñ DiﬀSysCpp b qq
is similarly determined: given objects m : MyTM Ñ p and n : NyTN Ñ q, take their tensor
m b n : pM b NqyTMbTN and precompose with the induced morphism pM b NqyTpMbNq Ñ
pM b NqyTMbTN; proceed similarly on morphisms of diﬀerential systems. The satisfaction of the
unitality and associativity laws follows from the monoidality of T.
We now deﬁne a monoidal bicategory DiﬀCiliaC of diﬀerential hierarchical inference systems,
following the deﬁnition of CiliaT
C above.
Deﬁnition 5.3.13. Let DiﬀCiliaC denote the monoidal bicategory whose 0-cells are again the
objects pA, Sq of BayesLens and whose hom-categories DiﬀCiliaC
`
pA, Sq, pB, Tq
˘
are given
by DiﬀSysC
`
vAyS, ByT w
˘
. The identity 1-cell idpA,Sq : pA, Sq Ñ pA, Sq on pA, Sq is given by
the diﬀerential system y Ñ vAyS, ByT w with state space 1, trivial backwards component, and
forwards component that picks the identity Bayesian lens on pA, Sq. The composition of diﬀerential
systems pA, Sq Ñ pB, Tq then pB, Tq Ñ pC, Uq is deﬁned by the functor
DiﬀCiliaC
`
pA, Sq, pB, Tq
˘
ˆ DiﬀCiliaC
`
pB, Tq, pC, Uq
˘
“ DiﬀSysC
`
vAyS, ByT w
˘
ˆ DiﬀSysC
`
vByT , CyUw
˘
λÝÑ DiﬀSysC
`
vAyS, ByT w b vByT , CyUw
˘
DiﬀSysCpcq
ÝÝÝÝÝÝÝÑ DiﬀSysC
`
vAyS, CyUw
˘
“ DiﬀCiliaC
`
pA, Sq, pC, Uq
˘
where λ is the laxator of Proposition 5.3.12 and c is the external composition morphism of Deﬁnition
5.3.5.
176

The monoidal structure pb, yq on DiﬀCiliaC is similarly deﬁned following that of CiliaT
C. On
0-cells, pA, SqbpA1, S1q :“ pAbA1, S bS1q. On 1-cells pA, Sq Ñ pB, Tq and pA1, S1q Ñ pB1, T 1q
(and their 2-cells), the tensor is given by the functors
DiﬀCiliaC
`
pA, Sq, pB, Tq
˘
ˆ DiﬀCiliaC
`
pA1, S1q, pB1, T 1q
˘
“ DiﬀSysC
`
vAyS, ByT w
˘
ˆ DiﬀSysC
`
vA1yS1, B1yT 1w
˘
λÝÑ DiﬀSysC
`
vAyS, ByT w b vA1yS1, B1yT 1w
˘
CoalgT
Cpdq
ÝÝÝÝÝÝÑ DiﬀSysC
`
vAyS b A1yS1, ByT b B1yT 1w
˘
“ DiﬀCiliaC
`
pA, Sq b pA1, S1q, pB, Tq b pB1, T 1q
˘
where d is the distributive law of Deﬁnition 5.3.6.
Following Prop. 5.3.8, we have the following characterization of a diﬀerential hierarchical
inference system pA, Sq Ñ pB, Tq in KℓpPq, for P : E Ñ E.
Proposition 5.3.14. A 1-cell δ : pA, Sq Ñ pB, Tq in DiﬀCiliaKℓpPq is given by a tuple δ :“
pX, δo
1, δo
2, δ#q of
• a choice of ‘state space’ X : DiﬀE;
• a forwards output map δo
1 : X ˆ A Ñ PB in E,
• a backwards output map δo
2 : X ˆ PA ˆ T Ñ PS in E,
• a stochastic vector ﬁeld δ# : X ˆ PA ˆ T Ñ PTX in E.
We can obtain continuous-time dynamical systems from diﬀerential systems by integration, and
consider how to discretize these ﬂows to give discrete-time dynamical systems.
Proposition 5.3.15. Integration induces an indexed functor Flow : DiﬀSysC Ñ CoalgR
P.
Proof. Suppose pM, mq is an object in DiﬀSysCppq. The morphism m : MyTM Ñ p consists of a
map m1 : M Ñ pp1q in ComonpCq along with a morphism m# : ř
x:M prm1pxqs Ñ TM in C.
Since, for any section σ : p Ñ y, the induced map mσ : M Ñ TM is a vector ﬁeld on a compact
manifold, it generates a unique global ﬂow Flowppqpmqσ : R ˆ M Ñ M [160, Thm.s 12.9, 12.12],
which factors as
ÿ
t:R
M
m˚
1 σ
ÝÝÝÑ
ÿ
t:R
ÿ
x:M
prm1pxqs
Flowppqpmqu
ÝÝÝÝÝÝÝÑ M .
We therefore deﬁne the system Flowppqpmq to have state space M, output map m1 (for all t : R),
and update map Flowppqpmqu. Since Flowppqpmqσ is a ﬂow for any section σ, it immediately
satisﬁes the monoid action condition. On morphisms α : m Ñ m1, we deﬁne Flowppqpαq by
177

the same underlying map on state spaces; this is again well-deﬁned by the condition that α is
compatible with the tangent structure. Given a morphism ϕ : p Ñ q of polynomials, both the
reindexing DiﬀSysCpϕq and CoalgR
C pϕq act by postcomposition, and so it is easy to see that
CoalgR
C pϕq ˝ Flowppq – Flowpqq ˝ DiﬀSysCpϕq naturally.
Not only can we integrate a diﬀerential system to obtain a continuous-time dynamical system,
we can also variously discretize the continuous-time system to obtain a discrete-time one.
Proposition 5.3.16. Any map f : T1 Ñ T of monoids induces an indexed functor (a natural
transformation) CoalgT
P Ñ CoalgT1
P .
Proof. We ﬁrst consider the induced functor CoalgT
Pppq Ñ CoalgT1
P ppq, which we denote by
∆p
f. Note that we have a morphism rfy, ps : rTy, ps Ñ rT1y, ps of polynomials by substitution
(precomposition). A system β in CoalgT
P is a morphism SyS Ñ rTy, ps for some S, and so we
deﬁne ∆p
fpβq to be rf, ps ˝ β : SyS Ñ rTy, ps Ñ rT1y, ps. To see that this satisﬁes the monoid
action axiom, consider that the closure ∆p
fpβqσ for any section σ : p Ñ y is given by
ÿ
t:T1
S
βopfptqq˚σ
ÝÝÝÝÝÝÝÑ
ÿ
t:T1
ÿ
s:S
prβopfptq, sqs
βu
ÝÑ S
which is an object in the functor category CatpBT1, Cq since f is a monoid homomorphism. On
morphisms of systems, the functor ∆p
f acts trivially.
To see that ∆f collects into an indexed functor, consider that it is deﬁned on each polynomial p
by the contravariant action rf, ps of the internal hom r´, “s, and that the reindexing CoalgTpϕq
for any morphism ϕ of polynomials is similarly deﬁned by the covariant action rTy, ϕs. By
the bifunctoriality of r´, “s, we have rT1y, ϕs ˝ rfy, ps “ rfy, ϕs “ rfy, qs ˝ rTy, ϕs, and so
CoalgT1
P pϕq ˝ ∆p
f “ ∆q
f ˝ CoalgT
P.
Corollary 5.3.17. For each k : R, the canonical inclusion ιk : N ãÑ R : i ÞÑ ki induces a
corresponding ‘discretization’ indexed functor Disck :“ ∆ι : CoalgR
P Ñ CoalgN
P.
Remark 5.3.18. From Proposition 5.3.15 and Corollary 5.3.17 we obtain a family of composite
indexed functors DiﬀSysC
Flow
ÝÝÝÑ CoalgR
P
Disck
ÝÝÝÑ CoalgN
P taking each diﬀerential system to a
discrete-time dynamical system in C. Below, we will deﬁne approximate inference doctrines in
discrete time that arise from processes of (stochastic) gradient descent, and which therefore factor
through diﬀerential systems, but the form in which these are given—and in which they are found
in the informal literature (e.g., Bogacz [82])—is not obtained via the composite Disck ˝ Flow for any
k, even though there is a free parameter k that plays the same role (intuitively, a ‘learning rate’).
Instead, one typically adopts the following ‘naïve’ discretization scheme.
178

Let CartDiﬀSysC denote the sub-indexed category of DiﬀSysC spanned by those systems
with Cartesian state spaces Rn. Naive discretization induces a family of indexed functors Naivek :
CartDiﬀSysC Ñ CoalgN
P, for k : R, which we illustrate for a single system pRn, mq over a ﬁxed
polynomial p, with m : RnyRnˆRn Ñ p (since TRn – Rn ˆ Rn). This system is determined by a
pair of morphisms m1 : Rn Ñ pp1q and m# : ř
x:Rn prm1pxqs Ñ Rn ˆ Rn, and we can write the
action of m# as px, yq ÞÑ px, vxpyqq.
Using these, we deﬁne a discrete-time dynamical system β over p with state space Rn. This β
is given by an output map βo, which we deﬁne to be equal to m1, βo :“ m1, and an update map
βu : ř
x:Rn prβopxqs Ñ Rn, which we deﬁne by px, yq ÞÑ x ` k vxpyq. Together, these deﬁne a
system in CoalgN
Pppq, and the collection of these systems β produces an indexed functor by the
deﬁnition Naivekppqpmq :“ β.
By contrast, the discrete-time system obtained via Disck ˝Flow involves integrating a continuous-
time one for k units of real time for each unit of discrete time: although this in general produces
a more accurate simulation of the trajectories implied by the vector ﬁeld, it is computationally
more arduous; to trade oﬀsimulation accuracy against computational feasibility, one may choose a
more sophisticated discretization scheme than that sketched above, or at least choose a “suﬃciently
small” timescale k.
Finally, we can use the foregoing ideas to translate diﬀerential hierarchical inference systems to
dynamical hierarchical inference systems.
Corollary 5.3.19. Let CartDiﬀCiliaC denote the restriction of DiﬀCiliaC to hom-categories in
CartDiﬀSysC. The indexed functors Disck : CoalgR
P Ñ CoalgN
P, Flow : DiﬀSysC Ñ CoalgR
P,
and Naivek : CartDiﬀSysC Ñ CoalgN
P induce functors (respectively) HDisck : CiliaR
C Ñ
CiliaN
C , HFlow : DiﬀCiliaC Ñ CiliaR
C and HNaivek : CartDiﬀCiliaC Ñ CiliaN
C by change of
base of enrichment.
179

6. Bayesian brains are open cybernetic
systems
The Bayesian lenses that satisfy Theorem 4.3.12 are exact, but most physically realistic cybernetic
systems—brains included—do not compute exact inversions: the inversion of a channel c with
respect to a prior π involves evaluating the composite c ‚ π, which is unfeasibly computationally
costly; therefore, if they instantiate any process of Bayesian inference, realistic systems must
instantiate approximate Bayesian lenses. Fortunately, as a consequence of Theorem 4.3.12, an
approximate inversion of a composite channel will be approximately equal to this lens composite;
conversely, the lens composite of approximate inversions will be approximately equal to the exact
inversion of the corresponding composite.
In this way, we can approximate the inversion of a composite channel by the lens composite
of approximations to the inversions of the components. But what do we mean by “approximate”?
There is often substantial freedom in the choice of approximation scheme—often manifest as some
form of parameterization—and in typical situations, the ‘ﬁtness’ of a particular scheme will be
context-dependent. We think of Bayesian lenses as representing ‘open’ statistical systems, in
interaction with some environment or context, and so the ﬁtness of a particular lens then depends
not only on the conﬁguration of the system (i.e., the choice of lens), but also on the suitability of its
conﬁguration for the environment.
In order to quantify this context-dependent ﬁtness, we ﬁrst need to formalize the notion of
context, which we deﬁne in §6.1 following compositional game theory [17, 141]: a context is
“everything required to make an open system closed”. We use this in §6.2 to deﬁne the notion of
statistical game: a Bayesian lens paired with a contextual ﬁtness function. The ﬁtness function
measures how well the lens performs in each context, and the “aim of the game” is then to choose
the lens or context (depending on your perspective) that somehow optimizes the ﬁtness. We then
exemplify statistical games by formalizing a number of classic and not-so-classic problems in
statistics.
In the ﬁnal section of this chapter (§6.3), we turn statistical games into the dynamical systems
which play them: a ﬁrst step to a formal account of active inference, the notion of approximate
inference doctrine, which we show to encompass the theories of predictive coding that are at the
180

heart of the idea of the “Bayesian brain”. Approximate inference doctrines are dynamical semantics
for predictive systems: algebras for a process theory of stochastic channels.
6.1. Contexts for Bayesian lenses
6.1.1. Simple contexts
Our ﬁrst step is to show how to “close oﬀ” a lens with respect to sequential composition , using
the notion of simple context. This will allow us to deﬁne categories of statistical games, but they
will not be satisfactorily monoidal: for that, we need the complex contexts of the next subsection.
Deﬁnition 6.1.1. A simple context for a Bayesian lens over C is an element of the functor
BayesLensC deﬁned by
BayesLensC
ˆ
BayesLens op
C
Ñ
Set
´
ˆ
“
ÞÑ
BayesLensCppI, Iq, ´q ˆ BayesLensCp“, pI, Iqq .
If the lens is pA, Sq ÞÑ pB, Tq, its set of simple contexts is
BayesLensC
`
pI, Iq, pA, Sq
˘
ˆ BayesLensC
`
pB, Tq, pI, Iq
˘
.
If we denote the lens by f, then we can denote this set by
Ctxpfq :“ BayesLensC
`
pA, Sq, pB, Tq
˘
.
Functors C ˆ D op Ñ Set are known as profunctors or, as they are a categoriﬁcation of relations,
relators. The calculus of profunctors will prove important for us in deﬁning complex contexts and
their composition below, and so to acknowledge this importance we make a formal deﬁnition.
Deﬁnition 6.1.2. A profunctor (sometimes also known as a relator or a bimodule of categories)
from a category D to a category C is a functor C ˆ D op Ñ Set.
When the monoidal unit I is terminal in C, or equivalently when C is semicartesian, the object
of simple contexts acquires a simpliﬁed (and intuitive) form.
Proposition 6.1.3. When I is terminal in C, we have
BayesLensC
`
pX, Aq, pY, Bq
˘
– CpI, Xq ˆ V
`
CpI, Bq, CpI, Y q
˘
.
Proof. A straightforward calculation which we omit: use causality (Deﬁnition 3.1.1).
Remark 6.1.4. Proposition 6.1.3 means that, when C is a Markov category (a semicartesian
copy-discard category—cf. Remark 4.1.18—such as KℓpDq), a context for a Bayesian lens consists
of a ‘prior’ on the domain of the forwards channel and a ‘continuation’: a function which takes
the output of the forwards channel (possibly a ‘prediction’) and returns an observation for the
update map. We think of the continuation as encoding the response of the environment given the
prediction.
181

In order to deﬁne the sequential composition of statitistical games, we will need to construct,
from the context for a composite lens, contexts for each factor of the composite. The functoriality
of BayesLensC guarantees the existence of such local contexts.
Deﬁnition 6.1.5. Given another lens g : pB, Tq ÞÑ pC, Uq and a simple context for their composite
g  f : pA, Sq ÞÑ pC, Y q, then we can obtain a 1-local context for f by the action of the profunctor
BayesLensC
`
pA, Sq, g
˘
: BayesLensC
`
pA, Sq, pC, Uq
˘
Ñ BayesLensC
`
pA, Sq, pB, Tq
˘
and we can obtain a simple context for g similarly:
BayesLensC
`
f, pC, Uq
˘
: BayesLensC
`
pA, Sq, pC, Uq
˘
Ñ BayesLensC
`
pB, Tq, pC, Uq
˘
.
Note that we can write g˚ : Ctxpg  fq Ñ Ctxpfq for the former action and f˚ : Ctxpg  fq Ñ
Ctxpgq for the latter, since g˚ acts by precomposition (pullback) and f˚ acts by postcomposition
(pushforwards). Note also that, given hgf, we have h˚ ˝f˚ “ f˚ ˝h˚ : Ctxphgfq Ñ Ctxpgq
by the associativity of composition.
Remark 6.1.6. The local contexts of the preceding deﬁnition are local with respect to sequential
composition of lenses. If we view the monoidal category BayesLensC as a one-object bicategory,
then sequential composition is composition of 1-cells, which explains their formal naming as 1-local
contexts.
To lift the monoidal structure of BayesLensC to our categories of statistical games, we will
similarly need ‘2-local’ contexts: from the one-object bicategory perspective, these are local contexts
with respect to 2-cell composition. By analogy with the 1-local case, this means exhibiting maps of
the form Ctxpf b f1q Ñ Ctxpfq and Ctxpf b f1q Ñ Ctxpf1q which give the ‘left’ and ‘right’ local
contexts for a parallel pair of lenses.
6.1.2. Interlude on profunctors and coends
As indicated by their alternative name ‘relators’ (due to Loregian [161]), profunctors are categoriﬁed
relations, and they compose analogously. What does this mean? By the correspondence between
logical propositions (treated as functions into the set of truth values 2) and subsets (cf. Example
2.3.22), we can understand a relation R between two sets A and B as either a subset of the product
R ãÑ A ˆ B or as a function χR : A ˆ B Ñ 2 (cf. Example 2.3.26).
We discussed in §2.3.4 how sets can therefore be seen as “witnesses to truth”, and so following
this intuition, we can categorify propositions by considering not functions P : A Ñ 2 but rather
functors P : A Ñ Set, where the set A has been upgraded to a category A; we can think of Ppaq
as ‘true’ as long as it is not the empty set, and its elements as witnesses to its truth. Since a relation
182

is a proposition on a product of sets, this leads to the notion of relator as a categoriﬁed proposition
on a product of categories1: that is to say, as a profunctor.
Following this line of thought, to understand how profunctors compose, it helps to see how
relations compose; we will see that it is like a Boolean form of matrix multiplication2.
Proposition 6.1.7. There is a category Rel whose objects are sets and whose morphisms A ù B
are relations A ˆ B Ñ 2. The identity relation on a set A is the constant function on J. Given
relations R : A ù B and S : B ù C, the composite relation SR : A ù C is given by the
following logical formula:
a SR c ðñ SRpa, cq ðñ Db : B . aRb ^ bSc .
That is, a : A is related to c : C by the composite relation SR if and only if there is some b : B
such that Rpa, bq “ J and Spb, cq “ J. (Note that we will occasionally switch between such inﬁx
and preﬁx notation.)
Remark 6.1.8. There is a close relationship between the category Rel and the bicategory SpanSet,
that we will not explore in detail (but see e.g. [162, 163]), except to note that every span A
fÐÝ S
gÝÑ B
induces a relation impf, gq ãÑ A ˆ B, and every relation R ãÑ A ˆ B induces a span A Ð R Ñ B
by projecting onto A and B, and that the composition of spans is like a ‘weak’ version of the
composition of relations (being deﬁned only up to isomorphism by pullback).
The composition rule for profunctors will be a categoriﬁcation of the rule for relations. Since
profunctors are deﬁned on categories and not merely sets, this composition rule needs to account for
their morphisms. The Yoneda Lemma tells us that, given a categoriﬁed proposition P : A Ñ Set,
the witnesses in Ppaq correspond to natural transformations Apa, ´q ñ P. In this way, given a
profunctor Q : C op ˆ D Ñ Set, we can think of Qpc, dq as the set of ways that c : C is related to
d : D3; more concisely, we can think of Qpc, dq as a set of ‘paths’ from c to d.
The intuition for profunctor composition will then be that “there is a composite path from a
to c if and only if there is some b such that there are paths a Ñ b and b Ñ c”. Alternatively,
the set of paths from a to c can be constructed by ranging over objects b, collecting the paths
a Ñ b and b Ñ c, and gluing them along compatible choices of b. We can formalize this kind of
“ranging-and-gluing” using the universal notion of coend, which is itself a kind of colimit.
If P : C op ˆ C Ñ Set is any ‘endoprofunctor’, then the coend
şC P is the greatest quotient
of ř
X:C PpX, Xq that coequalizes the left and right actions of P on morphisms in C; here,
1We also need to take the opposite on one side, to account for the directedness of morphisms and their composition.
2It is for a similar reason that we can think of stochastic channels as “stochastic relations”.
3Note that here we have chosen to put the op on the left factor of the domain of the profunctor, rather than the right
as we did in §6.1.1. Since C and C op are formally dual, or since ˆ is symmetric in Cat, it does not matter which
convention we choose, and we can switch between them as best suits the exposition.
183

ř
X:C PpX, Xq is the coproduct (disjoint union) of all the sets PpX, Xq. Then the elements
of the coend
şC P are equivalence classes such that two elements u : PpW, Wq and v : PpX, Xq
are related if there exists some f : PpX, Wq satisfying u ˝ f “ f ˝ v; that is, u and v are related by
the coend if we can ‘slide’ some f between them, witnessing the ‘compatibility’ of the preceding
paragraph. (This ‘sliding’ intuition will be rendered clearer in §A.2 of the Appendix, where we
exhibit the graphical calculus associated to profunctors.) The universal property of the coend is
then that it is the largest such quotient, so that any morphism out of any such quotient factors
through the coend.
More formally, let f : X Ñ Y be any morphism in C. We can then formally deﬁne the coend
şC P as the following coequalizer.
Deﬁnition 6.1.9. Deﬁne witnesses to the left and right actions of P as follows.
λ˚ :“
ÿ
W:C, X:C,
f : CpW, Xq
ιdompfq ˝ P
`
f, dompfq
˘
and
ρ˚ :“
ÿ
W:C, X:C,
f : CpW, Xq
ιcodpfq ˝ P
`
codpfq, f
˘
where, given an object Y : C0, we denote by ιY : PpY, Y q Ñ ř
X PpX, Xq the inclusion of PpY, Y q
into the coproduct ř
X PpX, Xq. Then the coend
şC P is given by the following coequalizer:
ÿ
W:C, X:C,
f : CpW, Xq
P
`
codpfq, dompfq
˘
ÿ
X
PpX, Xq
ż C
P
λ˚
ρ˚
To make the ‘variable of integration’ explicit, we often denote the coend
şC P by
ż X: C
PpX, Xq
Remark 6.1.10. Note that, just as we can think of profunctors as generalized relations, we can
think of coends as generalized existential quantiﬁers, and this is how we will use them to categorify
the composition rule for relations (Proposition 6.1.7). To clarify this, note that we can generalize
the notion of profunctor above to the enriched-categorical setting, so that an E-profunctor is then
an E-functor C op b D Ñ E. In the case of a 2-enriched category, the profunctor P is then simply
a relation, and PpX, Y q encodes the relationship between X and Y according to P. Then we can
read
şX:C PpX, Xq as “there exists some X : C such that PpX, Xq is true”; that is, the coend
şX:C
reduces precisely to an existential quanitifer DX : C. (In the rest of this thesis, we will remain in
the classic Set-enriched world, but it is helpful to be aware of enriched profunctors.)
Let us now use these ideas to deﬁne a composition rule for profunctors, and so obtain a category.
184

Proposition 6.1.11. There is a category Prof whose objects are categories and whose morphisms
C ù D are profunctors. The identity profunctor idC : C ù C on a category C is given by the
hom profunctor idC :“ Cp´, “q. Given profunctors P : C ù D and Q : D ù E, their composite
Q ˝ P : C ù E is given by the following coend formula:
pQ ˝ Pqpc, eq :“
ż d:D
Ppc, dq b Qpd, eq
We’ve already seen that this reduces to the existential formula for relations in the case that
the base of enrichment is not Set but 2. In the Set-enriched case, we can think of Ppc, dq as
representing the set of ways that c : C is related to d : D according to the profunctor P. Then the
coend formula for pQ ˝ Pqpc, eq says that the set of ways that c : C is related to e : E according to
the composite profunctor Q ˝ P is obtained by ranging over objects in D, collecting the elements
of Ppc, dq and Qpd1, eq for each such pair of objects d, d1, and considering them ‘related’ whenever
there is some morphism d Ñ d1. And this, in turn, is just the intuition we started with (following
Proposition 6.1.7).
Remark 6.1.12. It may seem strange that the identity profunctor is the hom functor, but consider
again the case of relations: here, the identity morphism on X is the function px, xq ÞÑ J. Now,
compare this with the profunctorial case when the base of enrichment is 2: we know that any
object in a category, and a fortiori in any 2-category C, must be related to itself, since there is
always an identity morphism. And this is precisely what the hom 2-profunctor says in this case,
since Cpc, cq “ J for any c : C! With Set-profunctors, we can think of the coend as performing a
kind of generalized convolution, and so the identity profunctor must be something which leaves
other profunctors unchanged after convolving either on the left or the right. Iin this setting, we can
therefore think of the hom profunctor as like a generalized Dirac ‘delta’ distribution. And, indeed,
we can think of profunctors as generalized distributions, and the coend composition formula as a
kind of generalized matrix multiplication or Chapman-Kolmogorov equation.
At this point, we are well equipped to deﬁne complex contexts for Bayesian lenses. For much
more information on (co)ends and their wonderful calculus, we refer the reader gladly to Loregian
[161].
6.1.3. Complex contexts
In order to provide contexts for the factors of a tensor of lenses without requiring extra structure
from C or BayesLensC
4, we need to pass from simple contexts to complex ones: a ‘2-local’ context
should allow for information to pass alongside a lens, through a process that is parallel to it.
4An alternative strategy might be to ask for BayesLensC to come equipped with a natural family of ‘discarding’
morphisms pA, Sq Ñ pI, Iq. This in turn means asking for canonical channels AÑ
‚ S, or equivalently canonical
sections of pA, Sq, which in general we do not have: if we are working with stochastic channels, we could obtain
185

Formally, we adjoin an object to the domain and codomain of the context, and quotient by the
rule that, if there is any process that ‘ﬁlls the hole’ represented by the adjoined objects, then we
consider the objects equivalent: this allows us to forget about the contextually parallel processes,
and keep track only of the type of information that ﬂows. Such adjoining-and-quotienting gives us
the following coend formula deﬁning complex contexts.
Deﬁnition 6.1.13. We deﬁne a complex context to be an element of the profunctor
Č
BayesLensC
`
pA, Sq, pB, Tq
˘
:“
ż pM,Nq:BayesLensC
BayesLensC
`
pM, NqbpA, Sq, pM, NqbpB, Tq
˘
.
If f : pA, Sq ÞÑ pB, Tq is a lens, then we will write Ctxpfq :“
Č
BayesLensC
`
pA, Sq, pB, Tq
˘
.
As in the case of simple contexts, we have ‘projection’ maps g˚ : Ctxpg  fq Ñ Ctxpfq and
f˚ : Ctxpg  fq Ñ Ctxpgq; these are deﬁned similarly. We will call the adjoined object, here
denoted pM, Nq, the residual. There is a canonical inclusion Ctxpfq ãÑ Ctxpfq given by adjoining
the trivial residual pI, Iq to each simple context.
We immediately have the following corollary of Proposition 6.1.3:
Corollary 6.1.14. When I is terminal in C,
Č
BayesLensC
`
pA, Sq, pB, Tq
˘
–
ż pM,Nq
CpI, M b Aq ˆ Set
`
CpI, M b Bq, CpI, N b Tq
˘
.
Remark 6.1.15. Since the coend denotes a quotient, its elements are equivalence classes. The
preceding corollary says that, when I is terminal, an equivalence class of contexts is represented by
a choice of residual pM, Nq, a prior on M bA in C, and a continuation CpI, M bBq Ñ CpI, N bTq
in Set.
Of course, when I is not terminal, then the deﬁnition says that a general complex context for
a lens pA, Sq ÞÑ pB, Tq is an equivalence class represented by: as before, a choice of residual
pM, Nq, a prior on M b A, and a continuation CpI, M b Bq Ñ CpI, N b Tq; as well as an ‘eﬀect’
M b BÑ
‚ I in C, and what we might call a ‘vector’ CpI, Iq Ñ CpN b S, Iq in Set. The eﬀect
and vector measure the environment’s ‘internal response’ to the lens’ outputs (as opposed to
representing the environment’s feedback to the lens).
Using complex contexts, it is easy to deﬁne the ‘projections’ that give 2-local contexts.
such canonical states by allowing the channels to emit subdistributions, and letting the canonical states be given by
those which assign 0 density everywhere. But even though we can make the types check this way, the semantics are
not quite what we want: rather, we seek to allow information to “pass in parallel”.
186

Deﬁnition 6.1.16. Given lenses f : Φ ÞÑ Ψ and f1 : Φ1 ÞÑ Ψ1 and a complex context for their
tensor f b f1, the left 2-local context is the complex context for f given by
πf : Ctxpf b f1q “
Č
BayesLensC
`
Φ b Φ1, Ψ b Ψ1˘
“
ż Θ:BayesLensC
BayesLensC
`
Θ b Φ b Φ1, Θ b Ψ b Ψ1˘
„
ÝÑ
ż Θ:BayesLensC
BayesLensC
`
Θ b Φ1 b Φ, Θ b Ψ1 b Ψ
˘
“
ż Θ:BayesLensC
BayesLensC
`
pI, Iq, Θ b Φ1 b Φ
˘
ˆ BayesLensC
`
Θ b Ψ1 b Ψ, pI, Iq
˘
f1
ÝÑ
ż Θ:BayesLensC
BayesLensC
`
pI, Iq, Θ b Ψ1 b Φ
˘
ˆ BayesLensC
`
Θ b Ψ1 b Ψ, pI, Iq
˘
“
ż Θ:BayesLensC
BayesLensC
`
Θ b Ψ1 b Φ, Θ b Ψ1 b Ψ
˘
ãÑ
ż Θ1:BayesLensC
BayesLensC
`
Θ1 b Φ, Θ1 b Ψ
˘
“ Ctxpfq .
The equalities here are just given by expanding and contracting deﬁnitions; the isomorphism uses
the symmetry of b in C; the arrow marked f1 is given by composing idΘ bf1 b idΦ after the ‘prior’
part of the context; and the inclusion is given by collecting the tensor of Θ and Ψ1 together into
the residual. These steps formalize the idea of ﬁlling the right-hand hole of the complex context
with f1 to obtain a local context for f.
The right 2-local context is the complex context for f1 obtained similarly:
πf1 : Ctxpf b f1q “
Č
BayesLensC
`
Φ b Φ1, Ψ b Ψ1˘
“
ż Θ:BayesLensC
BayesLensC
`
Θ b Φ b Φ1, Θ b Ψ b Ψ1˘
“
ż Θ:BayesLensC
BayesLensC
`
pI, Iq, Θ b Φ b Φ1˘
ˆ BayesLensC
`
Θ b Ψ b Ψ1, pI, Iq
˘
fÝÑ
ż Θ:BayesLensC
BayesLensC
`
pI, Iq, Θ b Ψ b Φ1˘
ˆ BayesLensC
`
Θ b Ψ b Ψ1, pI, Iq
˘
“
ż Θ:BayesLensC
BayesLensC
`
Θ b Ψ b Φ1, Θ b Ψ b Ψ1˘
ãÑ
ż Θ1:BayesLensC
BayesLensC
`
Θ1 b Φ1, Θ1 b Ψ1˘
“ Ctxpf1q
Note here that we do not need to use the symmetry, as both the residual and the ‘hole’ (ﬁlled with
f) are on the left of the tensor. (Strictly speaking, we do not require symmetry of b for πf either,
only a braiding.)
Remark 6.1.17. It is possible to make the intuition of “ﬁlling the left and right holes” more
immediately precise, at the cost of introducing another language, by rendering the 2-local context
187

functions in the graphical calculus of the monoidal bicategory of Set-profunctors. We demonstrate
how this works, making the hole-ﬁlling explicit, in §A.2 of the Appendix.
Henceforth, when we say ‘context’, we will mean ‘complex context’.
6.2. Statistical games: formalizing approximate inference
problems
We are now in a position to deﬁne monoidal categories of statistical games over C. In typical
examples, the ﬁtness functions will be valued in the real numbers, but this is not necessary for the
categorical deﬁnition; instead, we allow the ﬁtness functions to take values in an arbitrary monoid.
Proposition 6.2.1. Let C be a category admitting Bayesian inversion. There is a category SGameC
whose objects are the objects of BayesLensC and whose morphisms pX, Aq Ñ pY, Bq are
statistical games: pairs pf, φq of a lens f : BayesLensC
`
pX, Aq, pY, Bq
˘
and a ﬁtness function
φ : Ctxpfq Ñ R.
Proof. Suppose given statistical games pf, φq : pX, Aq Ñ pY, Bq and pg, ψq : pY, Bq Ñ pZ, Cq.
We seek a composite game pg, ψq ˝ pf, φq :“ pgf, ψφq : pX, Aq Ñ pZ, Cq. We have gf “ g  f by
lens composition. The composite ﬁtness function ψφ is obtained using the 1-local contexts by
ψφ :“ Ctxpg  fq ÝÑ Ctxpg  fq ˆ Ctxpg  fq
pg˚,f˚q
ÝÝÝÝÑ Ctxpfq ˆ Ctxpgq
pφ,ψq
ÝÝÝÑ R ˆ R `
ÝÑ R
The identity game pX, Aq Ñ pX, Aq is given by pid, 0q, the pairing of the identity lens on pX, Aq
with the unit 0 of the monoid R. Associativity and unitality follow from those properties of lens
composition, the coassociativity of copying
in Set, and the monoid laws of pR, `, 0q.
Deﬁnition 6.2.2. A simple Bayesian lens is a Bayesian lens pX, Xq ÞÑ pY, Y q for any X and Y in
C. We will write SimpSGameC ãÑ SGameC for the full subcategory of SGameC deﬁned on
simple Bayesian lenses. For both simple lenses and simple games we will eschew redundancy by
writing the objects pX, Xq simply as X.
Proposition 6.2.3. SGameC inherits a monoidal structure
`
b, pI, Iq
˘
from
`
BayesLensC, b, pI, Iq
˘
.
When pC, b, Iq is furthermore symmetric monoidal, then the monoidal structure on SGameC is
symmetric.
Proof. The structure on objects and on the lens components of games is deﬁned as in Proposition
4.3.9. On ﬁtness functions, we use the monoidal structure of R: given games pf, φq : pX, Aq Ñ
188

pY, Bq and pf1, φ1q : pX1, A1q Ñ pY 1, B1q, we deﬁne the ﬁtness function φ b φ1 of their tensor
pf, φq b pf1, φ1q as the composite
φbφ1 :“ Ctxpfbf1q ÝÑ Ctxpfbf1qˆCtxpfbf1q
pπf,πf1q
ÝÝÝÝÝÑ CtxpfqˆCtxpf1q
pφ,φ1q
ÝÝÝÑ RˆR `
ÝÑ R
That is, we form the left and right 2-local contexts, compute the local ﬁtnesses, and compose them
using the monoidal operation ` (addition) on R. Unitality and associativity follow from that of
pb, Iq on BayesLensC and p`, 0q on R.
Remark 6.2.4 (Multi-player games). In future work, we will consider how to compose the statistical
games of multiple interacting agents, so that the game-playing metaphor becomes more visceral: the
observations predicted by each system will then be generated by other systems, with each playing a
game of optimal prediction; some preliminary investigations in this direction were discussed in our
2021 paper[71]. In this thesis, however, we usually think of each statistical game as representing a
single system’s model of its environment (its context), even where the games at hand are themselves
sequentially or parallelly composite. That is to say, our games here are fundamentally ‘two-player’,
with the two players being the system and the context.
Remark 6.2.5. Both in the case of sequential of parallel composition of statistical games, the local
ﬁtnesses are computed independently and then summed. If a ﬁtnesss function depends somehow
on the residual, this might lead to ‘double-counting’ the ﬁtness of any overlapping factors of the
residual. For our purposes, this assumption of ‘independent ﬁtness’ will suﬃce, and so we leave
the question of gluing together correlated ﬁtness functions for future work.
6.2.1. Examples
In this subsection, we describe how a number of common concepts in statistics and particularly
statistical inference ﬁt into the framework of statistical games. We begin with the simple example of
maximum likelihood estimation and progressively generalize to include ‘variational’ [164] methods
such as the variational autoencoder [165] and generalized variational inference [166]. Along the
way, we introduce the concepts of free energy and evidence upper bound. We do not yet consider
the algorithms by which statistical games may be played or optimized—that is a matter for the next
section of this thesis (§6.3)—although we do consider parameterized statistical games, which will
be a useful tool in the construction of dynamical semantics. Instead, we see statistical games as
providing a syntax for the compositional construction of inference problems.
Remark 6.2.6 (The role of ﬁtness functions). Before we introduce our ﬁrst example, we note that
the games here are classiﬁed by their ﬁtness functions, with the choice of lens being somewhat
189

incidental to the classiﬁcation5. We note furthermore that our ﬁtness functions will tend to be of
the form Ek‚c‚πrfs, where pπ, kq is a context for a lens, c is a channel, and f is an appropriately
typed eﬀect6. This form hints at the existence of a compositional treatment of ﬁtness functions,
which seem roughly to be something like “lens functionals”. We leave such a treatment, and its
connection to Remark 6.2.5, to future work.
6.2.1.1. Maximum likelihood estimation
We ﬁrst study the classic problem of maximum likelihood estimation, beginning by establishing an
auxiliary result about contexts.
Proposition 6.2.7. Let I denote the monoidal unit pI, Iq in BayesLensC, and let l : I ÞÑ Ψ be a
lens. Then
Ctxplq “
ż Θ:BayesLensC
BayesLensCpI, Θ b Iq ˆ BayesLensCpΘ b Ψ, Iq
–
ż Θ:BayesLensC
BayesLensCpI, Θq ˆ BayesLensCpΘ b Ψ, Iq
– BayesLensCpI b Ψ, Iq
– BayesLensCpΨ, Iq .
Suppose Ψ “ pA, Sq. Then Ctxplq “ CpA, Iq ˆ Set
`
CpI, Aq, CpI, Sq
˘
by the deﬁnition of
BayesLensC.
Proof. The ﬁrst and third isomorphisms hold by unitality of b; the second holds by the Yoneda
lemma (see Loregian [161, Prop. 2.2.1] for the argument).
Example 6.2.8 (Maximum likelihood). When C is semicartesian, a Bayesian lens of the form
pI, Iq ÞÑ pX, Xq is determined by its forwards channel, which is simply a state π : IÑ
‚ X. Following
Proposition 6.2.7, and using that I is terminal, a context for such a lens is given simply by a
continuation k : CpI, Xq Ñ CpI, Xq taking states on X to states on X. A maximum likelihood game
is then any statistical game π of the type pI, Iq Ñ pX, Xq with ﬁtness function φ : Ctxpπq Ñ R
given by φpkq “ Ekpπq rpπs, where pπ is a density function for π. More generally, we might consider
maximum f-likelihood games for monotone functions f : R Ñ R, in which the ﬁtness function is
given by φpkq “ Ekpπq rf ˝ pπs. A typical choice here is f :“ log.
5This incidentality is lessened when we consider examples of parameterized games, but even here the parameterization
only induces something of a ‘sub’-classiﬁcation; the main classiﬁcation remains due to the ﬁtness functions.
6The resulting ‘optimization-centric’ perspective is in line with the aesthetic preference of [166], though we do not yet
know what this alignment might signify; we are interested to ﬁnd examples of a diﬀerent ﬂavour.
190

Remark 6.2.9. Recalling that we can think of probability density as a measure of the likelihood of
an observation, we have the intuition that an “optimal strategy” (i.e., an optimal choice of lens) for
a maximum likeihood game is one that maximizes the likelihood of the state obtained from the
context, or in other words provides the “best explanation” of the data generated by the continuation.
6.2.1.2. Approximate Bayesian inference
Considering parameterized maximum likelihood games, which are equipped with parameter-update
maps, leads one to wonder how to optimize this ‘inferential’ backwards part of the game, and not
just the ‘predictive’ forwards part. Such backwards optimization is approximate Bayesian inference.
Example 6.2.10 (Bayesian inference). Let D : CpI, Xq ˆ CpI, Xq Ñ R be a measure of
divergence between states on X. Then a (simple) D-Bayesian inference game is a statistical
game pc, φq : pX, Xq Ñ pY, Y q with ﬁtness function φ : Ctxpcq Ñ R given by φpπ, kq “
Ey„L π | c | k M
”
D
´
c1
πpyq, c:
πpyq
¯ı
, where c “ pc1, c1q constitutes the lens part of the game and c:
π is
the exact inversion of c1 with respect to π.
Note that we say that D is a “measure of divergence between states on X”. By this we mean any
function of the given type with the semantical interpretation that it acts like a distance measure
between states. But this is not to say that D is a metric or even pseudometric. One usually requires
that Dpπ, π1q “ 0 ðñ π “ π1, but typical choices do not also satisfy symmetry nor subadditivity.
An important such typical choice is the relative entropy or Kullback-Leibler divergence, denoted
DKL.
Deﬁnition 6.2.11. The Kullback-Leibler divergence DKL : CpI, Xq ˆ CpI, Xq Ñ R is deﬁned by
DKLpα, βq :“ E
x„αrlog ppxqs ´ E
x„αrlog qpxqs
where p and q are density functions corresponding to the states α and β.
6.2.1.3. Autoencoder games and the free energy
In many situations, computing the exact inversion c:
πpxq is costly, and so is computing the divergence
D
´
c1
πpxq, c:
πpxq
¯
. Consequently, approximate inversion schemes typically either approximate the
divergence (as in Monte Carlo methods), or they optimize an upper bound on it (as in variational
methods). In this section, we are interested in diﬀerent choices of ﬁtness function, rather than the
algorithms by which the functions are exactly or approximately evaluated; hence we here consider
the latter ‘variational’ choice, leaving the former for future work.
One widespread choice is to construct an upper bound on the divergence called the free energy
or the evidence upper bound; this is the choice that underlies the canonical free-energy framework
for predictive coding in the brain[39, 82]. In this setting, it is helpful to have a name for the pairing
191

of a stochastic channel (thought of as a conditional ‘predictive’ model) with a state on its domain
(understood as a ‘prior’ belief about the ‘latent’ causes of the predicted observations).
Deﬁnition 6.2.12. A generative model is a pair pπ, cq of a state π : IÑ
‚ X with a channel c : XÑ
‚ Y
for some objects X and Y .
With this idea to hand, we deﬁne the notion of D-free energy.
Deﬁnition 6.2.13 (D-free energy). Let pπ, cq be a generative model with c : XÑ
‚ Y . Let pc :
Y ˆX Ñ R` and pπ : X Ñ R` be density functions corresponding to c and π. Let pc‚π : Y Ñ R`
be a density function for the composite c ‚ π. Let c1
π be a channel Y Ñ
‚ X that we take to be an
approximation of the Bayesian inversion of c with respect to πand that admits a density function
q : X ˆ Y Ñ R`. Finally, let D : CpI, Xq ˆ CpI, Xq Ñ R be a measure of divergence between
states on X. Then the D-free energy of c1
π with respect to the generative model pπ, cq given an
observation y : Y is the quantity
FDpc1
π, c, π, yq :“
E
x„c1πpyq r´ log pcpy|xqs ` D
`
c1
πpyq, π
˘
.
(6.1)
We will elide the dependence on the model when it is clear from the context, writing only FDpyq.
The D-free energy is an upper bound on D when D is the relative entropy DKL, as we now
show.
Proposition 6.2.14 (Evidence upper bound). The DKL-free energy satisﬁes the following equality:
FDKLpyq “ DKL
“
c1
πpyq, c:
πpyq
‰
´ log pc‚πpyq “
E
x„c1πpyq
„
log
qpx|yq
pcpy|xq ¨ pπpxq

Since log pc‚πpyq is always negative, the free energy is an upper bound on DKL
”
c1
πpyq, c:
πpyq
ı
,
where c:
π is the exact Bayesian inversion of the channel c with respect to the prior π. Similarly, the
free energy is an upper bound on the negative log-likelihood ´ log pc‚πpyq. Thinking of this latter
quantity as a measure of the “model evidence” gives us the alternative name evidence upper bound
for the DKL-free energy.
Proof. Let pω : Y ˆ X Ñ R` be the density function pωpy, xq :“ pcpy|xq ¨ pπpxq corresponding
to the joint distribution of the generative model pπ, cq. We have the following equalities:
´ log pc‚πpyq “
E
x„c1πpyq r´ log pc‚πpyqs
“
E
x„c1πpyq
«
´ log pωpy, xq
pc:
πpx|yq
ﬀ
(by Bayes’ rule)
“
E
x„c1πpyq
«
´ log pωpy, xq
qpx|yq
qpx|yq
pc:
πpx|yq
ﬀ
“ ´
E
x„c1πpyq
„
log pωpy, xq
qpx|yq

´ DKL
“
c1
πpyq, c:
πpyq
‰
192

Deﬁnition 6.2.15. We will call FDKL the variational free energy, or simply free energy, and denote
it by F where this will not cause confusion. We will take the result of Proposition 6.2.14 as a
deﬁnition of the variational free energy, writing
Fpyq “
E
x„c1πpyq
„
log
qpx|yq
pcpy|xq ¨ pπpxq

where each term is deﬁned as in Deﬁnitions 6.2.13 and 6.2.11.
Remark 6.2.16. The name free energy is due to an analogy with the Helmholtz free energy in
thermodynamics, as, when D “ DKL, we can write it as the diﬀerence between an (expected)
energy and an entropy term:
Fpyq “
E
x„c1πpyq
„
log
qpx|yq
pcpy|xq ¨ pπpxq

“
E
x„c1πpyq r´ log pcpy|xq ´ log pπpxqs ´ SX
“
c1
πpyq
‰
“
E
x„c1πpyq
“
Epπ,cqpx, yq
‰
´ SX
“
c1
πpyq
‰
“ U ´ TS
where we call Epπ,cq : X ˆ Y Ñ R` the energy of the generative model pπ, cq, and where
SX : CpI, Xq Ñ R` is the Shannon entropy on X. The last equality makes the thermodynamic
analogy: U is the internal energy of the system; T “ 1 is the temperature; and S is again the
entropy.
Having now deﬁned a more tractable ﬁtness function, we can construct statistical games
accordingly. Since the free energy is an upper bound on relative entropy, optimizing the former
can have the side eﬀect of optimizing the latter7. We call the resulting games autoencoder games,
for reasons that will soon be clear.
Example 6.2.17 (Autoencoder). Let D : CpI, Xq ˆ CpI, Xq Ñ R be a measure of divergence be-
tween states on X. Then a simple D-autoencoder game is a simple statistical game pc, φq : pX, Xq Ñ
pY, Y q with ﬁtness function φ : Ctxpcq Ñ R given by φpπ, kq “ Ey„L π | c | k M rFD pc1
π, c, π, yqs
where c “ pc, c1q : pX, Xq ÞÑ pY, Y q constitutes the lens part of the game.
Remark 6.2.18 (Meaning of ‘autoencoder’). Why do we call autoencoder games thus? The name
originates in machine learning, where one thinks of the forwards channel as ‘decoding’ some latent
state into a prediction of some generated data, and the backwards channel as ‘encoding’ a latent
7Strictly speaking, one can have a decrease in free energy along with an increase in relative entropy, as long as the
former remains greater than the latter. Therefore, optimizing the free energy does not necessarily optimize the
relative entropy. However, as elaborated in Remark 6.2.18, the diﬀerence between the variational free energy and the
relative entropy is the log-likelihood, so optimizing the free energy corresponds to simultaneous maximum-likelihood
estimation and Bayesian inference.
193

state given an observation of the data; typically, the latent state space is thought to have lower
dimensionality than the observed data space, justifying the use of this ‘compression’ terminology.
A slightly more precise way to see this is to consider an autoencoder game where the context and
forwards channel are ﬁxed. The only free variable available for optimization in the ﬁtness function
is then the backwards channel, and the optimum is obtained when the backwards channel equals
the exact inversion of the forwards channel (given the prior in the context, and for all elements of
the support of the state obtained from the continuation). Conversely, allowing only the forwards
channel to vary, it is easy to see that the autoencoder ﬁtness function is then equal to the ﬁtness
function of a maximum log-likelihood game (up to a constant). Consequently, optimizing the ﬁtness
of an autoencoder game in general corresponds to performing approximate Bayesian inference
and maximum likelihood estimation simultaneously. The optimal strategy (lens or parameter) can
then be considered as representing an ‘optimal’ model of the process by which observations are
generated, along with a recipe for inverting that model (and hence ‘encoding’ the causes of the
data). The preﬁx auto- indicates that this model is learnt in an unsupervised manner, without
requiring input about the ‘true’ causes of the observations.
Some authors (in particular, Knoblauch et al. [166]) take a variant of the D-autoencoder ﬁtness
function to deﬁne a generalization of Bayesian inference: in an echo of Remark 6.2.18, the intuition
here is that Bayesian inference simply is maximum likelihood estimation, except ‘regularized’ by
the uncertainty encoded in the prior, which stops the optimum strategy being trivially given by a
Dirac delta distribution. By allowing both the choice of likelihood function and divergence measure
to vary, one obtains a family of generalized inference methods. Moreover, when one retains the
standard choices of log-density as likelihood and relative entropy as divergence, the resulting
generalized Bayesian inference games coincide with variational autoencoder games; then, when
the forwards channel (or its parameter) is ﬁxed, both types of game coincide with the Bayesian
inference games of Example 6.2.10 above.
Example 6.2.19 (Generalized Bayesian inference [166]). Let D : CpI, Xq ˆ CpI, Xq Ñ R be a
measure of divergence between states on X, and let l : Y b XÑ
‚ I be any eﬀect on Y b X. Then a
simple generalized pl, Dq-Bayesian inference game is a simple statistical game pc, φq : pX, Xq Ñ
pY, Y q with ﬁtness function φ : Ctxpcq Ñ R given by
φpπ, kq “
E
y„L π | c | k M
„
E
x„c1πpyq rlpy, xqs ` Dpc1
πpyq, πq

where pc, c1q : pX, Xq ÞÑ pY, Y q constitutes the lens part of the game.
Proposition 6.2.20. Generalized Bayesian inversion and autoencoder games coincide when D “
DKL and l “ ´ log pc, where pc is a density function for the forwards channel c.
194

Proof. Consider the DKL-free energy. We have
FDKL
`
c1
π, c, π, y
˘
“
E
x„c1πpyq r´ log pcpy|xq ´ log pπpxqs ´ SX
“
c1
πpyq
‰
by Remark 6.2.16
“
E
x„c1πpyq r´ log pcpy|xqs `
E
x„c1πpyq rlog qpx|yq ´ log pπpxqs
“
E
x„c1πpyq r´ log pcpy|xqs ` DKL
`
c1
πpyq, π
˘
“
E
x„c1πpyq rlpy, xqs ` D
`
c1
πpyq, π
˘
where q is a density function for c1
π.
6.2.1.4. Validity games
In the case where C has nontrivial eﬀects—such as when C “ sfKrn, where I is not terminal and
morphisms into I correspond to functions into r0, 8s—composing a Bayesian lens and its context
gives a lens pI, Iq ÞÑ pI, Iq: both the forwards and backwards parts of this ‘I-endolens’ return
positive reals (which in this context Jacobs and colleagues call validities [128, 167]), and which we
can think of as “the environment’s measurements of its compatibility with the lens”.
In this case, we can therefore deﬁne validity games, where the ﬁtness function is simply given by
computing the backwards validity8. Since such a ﬁtness function measures the interaction of the
lens with its environment, the corresponding statistical games may be of relevance in modelling
multi-agent or otherwise interacting statistical systems—for instance, in modelling evolutionary
dynamics. We leave the exploration of this for future work, although we note that this concept
seems to be closely related to the lens treatment of learning by gradient descent [93, 140], in which
lens costates supply the error that is to be backpropagated.
6.2.2. Parameterized statistical games
A statistical game as deﬁned above a simply a pairing of a Bayesian lens with a ﬁtness function.
Because, for a given statistical game, the lens is therefore ﬁxed, the only freedom to improve at
playing the game comes from the openness to the environment—but this seems like a strange
model of adaptive or cybernetic systems, which should also be free to change themselves in order
to improve their performance. Indeed, this changing-oneself is at the heart of the construction of
approximate inference doctrines, and in order to incorporate it into the structure, there must be
some more freedom in the model: the freedom to choose the lens.
This freedom is aﬀorded by the use of parameterized statistical games. In this subsection, we
consider both internally and externally parameterized games, although to simplify our presentation
8Note that the backwards eﬀect, as an I-state-dependent eﬀect (or ‘vector’), already depends upon the forwards validity,
so we do not need to include the forwards validity directly in the ﬁtness computation.
195

of approximate inference doctrines (and to be more faithful to what seems to be the natural structure
of our systems of interest), these will be deﬁned using only external parameterization.
6.2.2.1. Internally parameterized statistical games
Since each category SGameC is monoidal (Proposition 6.2.3), we obtain categories ParapSGameCq
of parameterized statistical games via Proposition 3.2.4 which are themselves monoidal.
A
parameterized statistical game of type pX, Aq
pΩ,Θq
ÝÝÝÑ pY, Bq in ParapSGameCq is a statistical
game
´
ΩbX
ΘbA
¯
Ñ
´
Y
B
¯
in SGameC; that is, a pair of a Bayesian lens
´
ΩbX
ΘbA
¯
ÞÑ
´
Y
B
¯
and a ﬁtness
function
Č
BayesLensC
´´
ΩbX
ΘbA
¯
,
´
Y
B
¯¯
Ñ R. If we ﬁx a choice of parameter ω : Θ and discard
the updated parameters in Θ—that is, if we reparameterize along the 2-cell induced by the lens
pω,
q : pI, Iq ÞÑ pΩ, Θq—then we obtain an unparameterized statistical game pX, Aq Ñ pY, Bq.
In this way, we can think of the parameters of a parameterized statistical game as the strategies by
which the game is to be played: each parameter ω : Ωpicks out a Bayesian lens, whose forwards
channel we think of as a model by which the system predicts its observations and whose backwards
channel describes how the system updates its beliefs. And, if we don’t just discard them, then these
updated beliefs may include updated parameters (of a possibly diﬀerent type Θ): this can be used to
implement learning in a fully Bayesian fashion. A successful strategy (a good choice of parameter)
for a statistical game is then one which optimizes the ﬁtness function in the contexts of relevance
to the system: we can think of these “relevant contexts” as something like the system’s ecological
niche.
Accordingly, we can consider parameterized versions of the example games above; but ﬁrst, we
deﬁne some simplifying notation, in order to represent how an environment responds to a system.
Notation 6.2.21 (Environmental feedback). Let C be semicartesian and consider a Bayesian lens
l “ pl1, l1q : pA, Sq
ÞÑ pB, Tq, with a context represented by: a residual pM, Nq; a prior π :
IÑ
‚ M b A; and a continuation k : CpI, M b Bq Ñ CpI, N b Tq. Write L π | l | k M to denote
k
`
pidM b l1q ‚ π
˘
T where pidM b l1q ‚ π is the map
I πÝÑ
‚ M b A idM b l1
ÝÝÝÝÝÑ
‚
M b B
and where p´qT denotes the projection (marginalization) onto T; here, by the channel N b TÑ
‚ T.
Note that L π | l | k M therefore has the type IÑ
‚ T in C: it encodes the environment’s feedback in
T to the lens, given its output in B and the context.
Example 6.2.22 (Parameterized maximum likelihood). A parameterized Bayesian lens with the
type pI, Iq
pΩ,Θq
ÝÝÝÑ
|
pX, Xq is equivalently a Bayesian lens pΩ, Θq ÞÑ pX, Xq, and hence given by
a pair of a channel (or “parameter-dependent state”; something like a random variable) ΩÑ
‚ X
and a parameter-update X ΩÝÑ
‚ Θ. When I is terminal in C, a context for the lens is represented by
196

π : IÑ
‚ M b Ωand k : CpI, M b Xq Ñ CpI, N b Xq. We then deﬁne a parameterized maximum
f-likelihood game to be a parameterized statistical game of the form l “ pl1, l1q : pI, Iq
pΩ,Θq
ÝÝÝÑ
|
pX, Xq
with ﬁtness function φ : Ctxpπq Ñ R given by φpπ, kq “ EL π | l | k M rf ˝ pl‚πΩs .
Here, pl‚πΩ: X Ñ r0, 8s is a density function for the composite channel l ‚ πΩ. In applications
one often ﬁxes a single choice of parameter o, with the marginal state πΩthen being a Dirac
delta distribution on that choice. One then writes the density function pl‚πΩp´q as plp´|oq or
pl p´|Ω“ oq.
One also of course has parameterized versions of the autoencoder games.
Example 6.2.23 (Simply parameterized autoencoder). A simply parameterized D-autoencoder game
is a simple parameterized statistical game pc, πq : pX, Xq
pΩ,Ωq
ÝÝÝÑ pY, Y q with the D-autoencoder
ﬁtness function φ : Ctxpcq Ñ R given by φpπ, kq “ Ey„L π | c | k M rFD pc1
π, c, π, yqs. That is, a
simply parameterized D-autoencoder game is just a simple D-autoencoder game with tensor
product domain type.
In applications, one may not want to use the same backwards channel to update both the “belief
about the causes” in X and the parameters in Ωsimultaneously. Instead, the backwards channel may
update only the beliefs over X, and any updating of the parameters is left to another ‘higher-order’
process. The X-update channel may nonetheless still itself be parameterized in Ω: for instance, if
it represents an approximate inference algorithm, then one may want to be able to improve the
approximation, and such improvement amounts to a change of parameters. The ‘higher-order’
process that performs the parameter updating can then be represented as a reparameterization: a
2-cell in the bicategory ParapSGameCq9. The next example tells the ﬁrst part of this story.
Example 6.2.24 (Parameterized autoencoder). A (simple) parameterized D-autoencoder game is a
parameterized statistical game pc, φq : pX, Xq
pΩ,Θq
ÝÝÝÑ pY, Y q with ﬁtness function φ : Ctxpcq Ñ R
given by
φpπ, kq “
E
y„L π | c | k M
“
FD
`
pc1
πqX, c|πΩ, πX, y
˘‰
.
As before, the notation p´qX indicates taking the X marginal, along the projection Θ b XÑ
‚ X.
We also deﬁne c|πΩ:“ c ‚ pπΩb idXq, indicating “c given the parameter state πΩ”. Written out in
full, the ﬁtness function is therefore given by
φpπ, kq “
E
y„L π | c | k M
“
FD
`
projX ‚ c1
π, c ‚ pπΩb idXq, projX ‚ π, y
˘‰
.
Note that the pair
`
c|πΩ, pc1
πΩqX
˘
deﬁnes an unparameterized simple Bayesian lens pX, Xq ÞÑ
pY, Y q, with c|πΩ: XÑ
‚ Y and pc1
πΩqX : Y X
ÝÑ
‚ X.
9This is not quite the approach that we take in §6.3; there, we explicitly construct dynamical systems which perform
the updating, rather than leaving this ‘external’ to the compositional story: in this example, the implementation of
the process of reparameterization is left unspeciﬁed
197

Unsurprisingly, as in the autoencoder case, there are parameterized and simply parameterized
variants of generalized Bayesian inference games.
6.2.2.2. Externally parameterized statistical games
In advance of our use of externally parameterization in the construction of approximate inference
doctrines, we brieﬂy consider externally parameterized statistical games, which will form an
intermediate part of the construction.
First, we observe that an internally parameterized statistical game—and hence an internally
parameterized Bayesian lens—may demand more data than is required in applications: if pγ, ρq :
pA, Sq
pΘ,Ωq
ÝÝÝÑ
|
pB, Tq is such a lens, then its forward channel γ has the unparameterized type Θ b
AÑ
‚ B, and the backwards channel ρ has the type CpI, Θ b Aq Ñ CpT, Ωb Sq. This means that
the inversion ρ depends on a joint prior over Θ b A, and produces an updated state over Ωb S,
even though one is often interested only in a family of inversions of the type CpI, Aq Ñ CpT, Sq
parameterized by Ω, with the updating of the parameters taking place in an external process that
‘observes’ the performance of the statistical game. This will be the case in the next section, and we
externalize the learning (parameter-updating) process using external parameterization.
Recalling that we write the external parameterization of a category C in its base of enrichment E
as PC, let us exhibit the category of externally parameterized statistical games.
Example 6.2.25. The category PSGameC of externally parameterized statistical games in C has
as 0-cells pairs of objects in C (as in the case of Bayesian lenses or plain statistical games). Its 1-cells
pA, Sq Θ
ÝÑ pB, Tq are externally parameterized games, consisting in a choice of parameter space
Θ, an externally parameterized lens f : Θ Ñ BayesLensCppA, Sq, pB, Tqq, and an externally
parameterized loss function φ : ř
ϑ:Θ Ctxpfϑq Ñ R. The identity on pA, Sq is given by the
trivially parameterized element idpA,Sq : 1 Ñ BayesLensCppA, Sq, pA, Sqq, equipped with the
zero loss function, as in the case of unparameterized statistical games. Given parameterized games
pf, φq : pA, Sq Θ
ÝÑ pB, Tq and pg, ψq : pB, Tq Θ1
ÝÑ pC, Uq, we form their composite as follows. The
composite parameterized lens is given by taking the product of the parameter spaces:
ΘˆΘ1 fˆg
ÝÝÑ BayesLensC
`
pA, Sq, pB, Tq
˘
ˆBayesLensC
`
pB, Tq, pC, Uq
˘ ÝÑ BayesLensC
`
pA, Sq, pC, Uq
˘
The composite ﬁtness function is given accordingly:
ÿ
ϑ:Θ,ϑ1:Θ1
Ctxpgϑ1fϑq ÝÑ
ÿ
ϑ,ϑ1
Ctxpgϑ1fϑq2 pgϑ1 ˚,fϑ˚q
ÝÝÝÝÝÝÑ
ÿ
ϑ,ϑ1
CtxpfϑqˆCtxpgϑ1q
pφϑ,ψϑ1q
ÝÝÝÝÝÑ RˆR `
ÝÑ R
For concision, when we say parameterized statistical game or parameterized lens in the absence
of further qualiﬁcation, we will henceforth mean the externally (as opposed to internally)
parameterized versions.
198

Remark 6.2.26. Before moving on to examples of approximate inference doctrines, let us note
the similarity of the notions of external parameterization, diﬀerential system, and dynamical
system: both of the latter can be considered as externally parameterized systems with extra
structure, where the extra structure is a morphism or family of morphisms back into (an algebra
of) the parameterizing object: in the case of diﬀerential systems, this ‘algebra’ is the tangent
bundle; for dynamical systems, it is trivial; and forgetting this extra structure returns a mere
external parameterization. Approximate inference doctrines are thus functorial ways of equipping
morphisms with this extra structure, and in this respect they are close to the current understanding
of general compositional game theory [93, 159].
6.3. Approximate inference doctrines
We are now in a position to build the bridge between abstract statistical models and the dynamical
systems that play them, with the categories of hierarchical dynamical systems developed in the
previous section supplying the semantics. These bridges will be lax monoidal functors, which we
call approximate inference doctrines. In general, they will be “dynamical algebras” for categories of
parameterized stochastic channels (considered as statistical models), which take the parameters as
part of the dynamical state space in order to implement something like learning. Moreover, we
are often particularly interested in only a particular class of statistical models, typically forming
a subcategory of a broader category of stochastic channels. We therefore make the following
deﬁnition.
Deﬁnition 6.3.1. Let D be a subcategory of PC. An approximate inference doctrine for D in time
T is a lax monoidal functor D Ñ CiliaT
C.
6.3.1. Channels with Gaussian noise
Our motivating examples from the computational neuroscience literature are deﬁned over a
subcategory of channels between Cartesian spaces with additive Gaussian noise [47, 82, 168];
typically one writes x ÞÑ fpxq ` ω for a deterministic map f : X Ñ Y and ω sampled from
a Gaussian distribution over Y . This choice is made, as we will see, because it permits some
simplifying assumptions which mean the resulting dynamical systems resemble known neural
circuits. In this section, we develop the categorical language in which we can express such Gaussian
channels. We begin by introducing the category of probability spaces and measure-preserving
maps (compare Deﬁnition 5.2.23 of measure-preserving dynamical systems), which we then use
to deﬁne channels of the general form x ÞÑ fpxq ` ω, before restricting to the ﬁnite-dimensional
Gaussian case.
199

Deﬁnition 6.3.2. Let P-Spc be the category Comon
`
1{KℓpPq
˘
of probability spaces pM, µq
with µ : 1Ñ
‚ M in KℓpPq (i.e., 1 Ñ PM in E), and whose morphisms f : pM, µq Ñ pN, νq are
measure-preserving maps f : M Ñ N (i.e., such that f ‚ µ “ ν in KℓpPq).
We can think of x ÞÑ fpxq ` ω as a map parameterized by a noise source, and so to construct a
category of such channels, we can use the Para construction, following Proposition 3.2.2. The
ﬁrst step is to spell out the actegory structure.
Proposition 6.3.3. Let P : E Ñ E be a probability monad on a symmetric Cartesian monoidal
category pE, ˆ, 1q. Then there is a P-Spc-actegory structure ˚ : P-Spc Ñ CatpE, Eq on E as
follows. For each pM, µq : P-Spc, deﬁne pM, µq ˚ p´q : E Ñ E by pM, µq ˚ X :“ M ˆ X. For
each morphism f : pM, µq Ñ pM1, µ1q in P-Spc, deﬁne f ˚ X :“ f ˆ idX.
Proof sketch. The action on morphisms is well-deﬁned because each morphism f : MÑ
‚ N in
Comon
`
1{KℓpPq
˘
corresponds to a map f : M Ñ N in E; it is clearly functorial. The unitor and
associator are inherited from the Cartesian monoidal structure pˆ, 1q on E.
The resulting Para bicategory, Parap˚q, can be thought of as a bicategory of maps each of
which is equipped with an independent noise source; the composition of maps takes the product
of the noise sources, and 2-cells are noise-source reparameterizations. The actegory structure
˚ is symmetric monoidal, and the 1-categorical truncation Parap˚q1 (cf. Proposition 3.2.6) is a
copy-delete category, as we now sketch.
Proposition 6.3.4. Parap˚q1 is a copy-delete category.
Proof sketch. The monoidal structure is deﬁned following Proposition 3.2.3. We need to deﬁne a
right costrength ρ with components pN, νq ˚ pX ˆ Y q „
ÝÑ X ˆ ppN, νq ˚ Y q. Since ˚ is deﬁned by
forgetting the probability structure and taking the product, the costrength is given by the associator
and symmetry in E:
pN, νq˚pXˆY q “ NˆpXˆY q „
ÝÑ NˆpY ˆXq „
ÝÑ pNˆY qˆX
„
ÝÑ XˆpNˆY q “ XˆppN, νq˚Y q
It is clear that this deﬁnition gives a natural isomorphism; the rest of the monoidal structure follows
from that of the product on E.
We now need to deﬁne a symmetry natural isomorphism βX,Y : X ˆ Y
„
ÝÑ Y ˆ X in Parap˚q.
This is given by the symmetry of the product in E, under the embedding of E in Parap˚q that takes
every map to its parameterization by the terminal probability space.
The rest of the copy-delete structure is inherited similarly from E.
If we think of KℓpPq as a canonical category of stochastic channels, for Parap˚q1 to be considered
as a subcategory of Gaussian channels, we need the following result.
200

Proposition 6.3.5. There is an identity-on-objects strict monoidal embedding of Parap˚q1 into
KℓpPq. Given a morphism f : X
pΩ,µq
ÝÝÝÑ Y in Parap˚q1, form the composite f ‚ pµ, idXq : XÑ
‚ Y
in KℓpPq.
Proof sketch. First, the given mapping preserves identities: the identity in Parap˚q is trivially
parameterized, and is therefore taken to the identity in KℓpPq. The mapping also preserves
composites, by the naturality of the unitors of the symmetric monoidal structure on KℓpPq. That
is, given f : X
pΩ,µq
ÝÝÝÑ Y and g : Y
pΘ,νq
ÝÝÝÑ Z, their composite g ˝ f : X
pΘbΩ,νbµq
ÝÝÝÝÝÝÝÑ Z is taken to
X „
ÝÑ
‚ 1 b 1 b X νbνbidX
ÝÝÝÝÝÝÑ
‚
Θ b Ωb X
g˝f
ÝÝÑ
‚
Z
where here g ˝ f is treated as a morphism in KℓpPq. Composing the images of g and f under the
given mapping gives
X „
ÝÑ
‚ 1 b X
µbidX
ÝÝÝÝÑ
‚
Ωb X
fÝÑ
‚ Y „
ÝÑ
‚ 1 b Y νbY
ÝÝÝÑ
‚
Θ b Y
gÝÑ
‚ Z
which is equal to
X „
ÝÑ
‚ 1 b 1 b X
νbµbidX
ÝÝÝÝÝÝÑ
‚
Θ b Ωb X
idΘ bf
ÝÝÝÝÑ
‚
Θ b Y
gÝÑ
‚ Z
which in turn is equal to the image of the composite above.
The given mapping is therefore functorial. To show that it is an embedding is to show that it is
faithful and injective on objects. Since Parap˚q and KℓpPq have the same objects, the embedding
is trivially identity-on-objects (and hence injective); it is similarly easy to see that it is faithful, as
distinct morphisms in Parap˚q are mapped to distinct morphisms in KℓpPq.
Finally, since the embedding is identity-on-objects and the monoidal structure on Parap˚q is
inherited from that on KℓpPq (producing identical objects), the embedding is strict monoidal.
We now restrict our attention to Gaussian maps.
Deﬁnition 6.3.6. We say that f : XÑ
‚ Y in KℓpPq is Gaussian if, for any x : X, the state fpxq : PY
is Gaussian10. Similarly, we say that f : X
pΩ,µq
ÝÝÝÑ Y in Parap˚q is Gaussian if its image under
the embedding Parap˚q1 ãÑ KℓpPq is Gaussian. Given a category of stochastic channels C, write
GausspCq for the subcategory generated by Gaussian morphisms and their composites in C. Given
a separable Banach space X, write GausspXq for the space of Gaussian states on X.
Example 6.3.7. A class of examples of Gaussian morphisms in Parap˚q that will be of interest
to us in section 6.3.3 is of the form x ÞÑ fpxq ` ω for some map f : X Ñ Y and ω distributed
according to a Gaussian distribution over Y . Writing Erωs for the mean of this distribution, the
resulting channel in KℓpPq emits for each x : X a Gaussian distribution with mean fpxq ` Erωs
and variance the same as that of ω.
10We admit Dirac delta distributions, and therefore deterministic channels, as Gaussian, since delta distributions can be
seen as Gaussians with inﬁnite precision.
201

Remark 6.3.8. In general, Gaussian morphisms are not closed under composition: pushing a
Gaussian distribution forward along a nonlinear transformation will not generally result in another
Gaussian. For instance, consider the Gaussian morphisms x ÞÑ fpxq ` ω and y ÞÑ gpyq ` ω1.
Their composite in Parap˚q is the morphism x ÞÑ g
`
fpxq ` ωq
˘
` ω1; even if g
`
fpxq ` ωq
˘
is
Gaussian-distributed, the sum of two Gaussians is in general not Gaussian, and so g
`
fpxq`ωq
˘
`ω1
will not be Gaussian. This non-closure underlies the power of statistical models such as the
variational autoencoder, which are often constructed by pushing a Gaussian forward along a
learnt nonlinear transformation [169], in order to approximate an unknown distribution; since
sampling from Gaussians is relatively straightforward, this method of approximation can be
computationally tractable. The Gauss construction here is an abstraction of the Gaussian-
preserving transformations invoked by Shiebler [170], and is to be distinguished from the category
Gauss introduced by Fritz [126], whose morphisms are aﬃne transformations (which do preserve
Gaussianness) and which are therefore closed under composition; there is nonetheless an embedding
of Fritz’s Gauss into our Gauss
`
KℓpPq
˘
.
For our approximate inference doctrines, we will be interested only in Gaussian channels between
ﬁnite-dimensional Cartesian spaces, and so we now turn to deﬁning the corresponding subcategory.
Proposition 6.3.9. Let FdCartSpcpEq denote the full subcategory of E spanned by ﬁnite-
dimensional Cartesian spaces Rn, where n : N. Let P-FdCartSpc denote the corresponding
subcategory of P-Spc. Let ‹ : P-FdCartSpc Ñ Cat
`
FdCartSpcpEq, FdCartSpcpEq
˘
be
the corresponding restriction of the monoidal action ˚ : P-Spc Ñ CatpE, Eq from Proposition
6.3.3. Then Parap‹q is a monoidal subbicategory of Parap˚q.
We will write PFd : FdCartSpcpEq Ñ FdCartSpcpEq to denote the restriction of the
probability monad P : E Ñ E to FdCartSpcpEq.
Finally, we give the density function representation of Gaussian channels in KℓpPFdq.
Proposition 6.3.10. Every Gaussian channel c : XÑ
‚ Y in KℓpPFdq admits a density function
pc : Y ˆX Ñ r0, 1s with respect to the Lebesgue measure on Y . Moreover, since Y “ Rn for some
n : N, this density function is determined by two maps: the mean µc : X Ñ Rn, and the covariance
Σc : X Ñ Rnˆn in E. We call the pair pµu, Σcq : X Ñ Rn ˆ Rnˆn the statistical parameters for c
(to be distinguished from any external parameterization).
Proof. The density function pc : Y ˆ X Ñ r0, 1s satisﬁes
log pcpy|xq “ 1
2
A
ϵc, Σcpxq´1ϵc
E
´ log
a
p2πqn det Σcpxq
where ϵc : Y ˆ X Ñ Y : py, xq ÞÑ y ´ µcpxq.
202

6.3.2. Predictive coding circuits and the Laplace doctrine
Our ﬁrst example of a doctrine—and the primary motivation for the work presented in this thesis—
arises in the computational neuroscience literature, which has sought to explain the apparently
‘predictive’ nature of sensory cortical circuits using ideas from the theory of approximate inference
[35]; the general name for this neuroscientiﬁc theory is predictive coding, and the task of a predictive
coding model is to deﬁne a dynamical system whose structures and behaviours mimic those observed
in neural circuits in vivo. One way to satisfy this constraint is to describe a procedure that turns a
statistical problem into a dynamical system of a form known to be simulable by a neural circuit:
that is to say, there are certain classes of dynamical systems which are known to reproduce the
phenomenology of neural circuits and which are built out of parts that correspond to known
biological structures, and so a “biologically plausible” model of predictive coding should produce
an instance of such a class, given a statistical problem.
This procedure pushes the ‘plausibility’ constraint back to the level of the statistical problem
(since there are presently no known neural circuit models that can solve any inference problem in
general), and one restriction that is usefully made is that all noise sources in the model are Gaussian.
This restriction allows us to make an approximation, known as the Laplace approximation, to the
loss function of an autoencoder game which in turn entails that performing stochastic gradient
descent on this loss function (with respect to the mean of the posterior distribution) generates a
dynamical system that is biologically plausible (up to some level of biological plausibility) [35, 82].
In this subsection, we begin by deﬁning the Laplace approximation and the resulting dynamical
system, and go on to show both how it arises and how the procedure is functorial: that is, we show
that it constitutes an approximate inference doctrine, and describe how this presentation clariﬁes
the role of what has been called the “mean ﬁeld” assumption in earlier literature [168]. (We leave
the study of the biological plausibility of compositional dynamical systems for future work.)
Lemma 6.3.11 (Laplace approximation). Suppose:
1. pγ, ρ, φq : pX, Xq Ñ pY, Y q is a simple DKL-autoencoder game with Gaussian channels
between ﬁnite-dimensional Cartesian spaces;
2. for all priors π : GausspXq, the statistical parameters of ρπ : Y Ñ PX are denoted
pµρπ, Σρπq : Y Ñ R|X| ˆ R|X|ˆ|X|, where |X| is the dimension of X; and
3. for all y : Y , the eigenvalues of Σρπpyq are small.
Then the loss function φ : Ctxpγ, ρq Ñ R can be approximated by
φpπ, kq “
E
y„L π | γ | k M
“
Fpyq
‰
«
E
y„L π | γ | k M
“
FLpyq
‰
203

where
FLpyq “ Epπ,γq pµρπpyq, yq ´ SX rρπpyqs
(6.2)
“ ´ log pγpy|µρπpyqq ´ log pπpµρπpyqq ´ SX rρπpyqs
where Sxrρπpyqs “ Ex„ρπpyqr´ log pρπpx|yqs is the Shannon entropy of ρπpyq, and pγ : Y ˆ X Ñ
r0, 1s, pπ : X Ñ r0, 1s, and pρπ : X ˆY Ñ r0, 1s are density functions for γ, π, and ρπ respectively.
The approximation is valid when Σρπ satisﬁes
Σρπpyq “
`
B2
xEpπ,γq
˘
pµρπpyq, yq´1 .
(6.3)
We call FL the Laplacian free energy and Epπ,γq the corresponding Laplacian energy.
Proof. Following Proposition 6.3.10, we can write the density functions as:
log pγpy|xq “ 1
2
@
ϵγ, Σγ´1ϵγ
D
´ log
b
p2πq|Y | det Σγ
log pρπpx|yq “ 1
2
@
ϵρπ, Σρπ
´1ϵρπ
D
´ log
b
p2πq|X| det Σρπ
(6.4)
log pπpxq “ 1
2
@
ϵπ, Σπ´1ϵπ
D
´ log
b
p2πq|X| det Σπ
where for clarity we have omitted the dependence of Σγ on x and Σρπ on y, and where
ϵγ : Y ˆ X Ñ Y : py, xq ÞÑ y ´ µγpxq ,
ϵρπ : X ˆ Y Ñ X : px, yq ÞÑ x ´ µρπpyq ,
(6.5)
ϵπ : X ˆ 1 Ñ X : px, ˚q ÞÑ x ´ µπ .
Then, recall from [76, Remark 5.12] that we can write the free energy Fpyq as the diﬀerence between
expected energy and entropy:
Fpyq “
E
x„ρπpyq
„
log
pρπpx|yq
pγpy|xq ¨ pπpxq

“
E
x„ρπpyq r´ log pγpy|xq ´ log pπpxqs ´ SX rρπpyqs
“
E
x„ρπpyq
“
Epπ,γqpx, yq
‰
´ SX rρπpyqs
Next, since the eigenvalues of Σρπpyq are small for all y : Y , we can approximate the expected
energy by its second-order Taylor expansion around the mean µρπpyq:
Fpyq « Epπ,γqpµρπpyq, yq ` 1
2
@
ϵρπ pµρπpyq, yq ,
`
B2
xEpπ,γq
˘
pµρπpyq, yq ¨ ϵρπ pµρπpyq, yq
D
´ SX
“
ρπpyq
‰
.
where
`
B2
xEpπ,γq
˘
pµρπpyq, yq is the Hessian of Epπ,γq with respect to x evaluated at pµρπpyq, yq.
204

Note that
@
ϵρπ pµρπpyq, yq ,
`
B2
xEpπ,γq
˘
pµρπpyq, yq ¨ ϵρπ pµρπpyq, yq
D
“ tr
“`
B2
xEpπ,γq
˘
pµρπpyq, yq Σρπpyq
‰
,
(6.6)
that the entropy of a Gaussian measure depends only on its covariance,
SX
“
ρπpyq
‰
“ 1
2 log det p2π e Σρπpyqq ,
and that the energy Epπ,γqpµρπpyq, yq does not depend on Σρπpyq. We can therefore write down
directly the covariance Σ˚
ρπpyq minimizing Fpyq as a function of y. We have
BΣρπ Fpyq « 1
2
`
B2
xEpπ,γq
˘
pµρπpyq, yq ` 1
2Σρπ
´1 .
Setting BΣρπ Fpyq “ 0, we ﬁnd the optimum as expressed by equation (6.3)
Σ˚
ρπpyq “
`
B2
xEpπ,γq
˘
pµρπpyq, yq´1 .
Finally, on substituting Σ˚
ρπpyq in equation (6.6), we obtain the desired expression of equation (6.2)
Fpyq « Epπ,γq pµρπpyq, yq ´ SX rρπpyqs “: FLpyq .
Remark 6.3.12. The terms ϵγ : Y ˆ X Ñ Y (&c.) of eq. (6.5) are known as error functions, since
they encode the diﬀerence between y : Y and the expected element µγpxq : Y given x : X. One
can think of these errors as prediction errors, interpreting µγ as the system’s prediction of the
expected state of Y .
One can then deﬁne the precision-weighted errors
ηγpy, xq :“ Σγpxq´1ϵγpy, xq : Y ˆ X Ñ Y ,
(6.7)
noting that the inverse covariance matrix Σγpxq´1 can be interpreted as encoding the ‘precision’ of
a belief: roughly speaking, low variance (or ‘diﬀusivity’) means high precision11. The log-densities
of eq. (6.3.11) are then understood as measuring the precision-weighted length of the error vectors.
Using the Laplace approximation, we can associate to each Gaussian channel a dynamical system
that (approximately) inverts it12.
11Consider the one-dimensional case: as the variance σ of a normal distribution tends to 0, the distribution approaches
a Dirac delta distribution, which is “inﬁntely precise”.
12We work with discrete-time dynamics for simplicity of exposition, and for ease of comparison with Bogacz [82],
though we expect a continuous-time presentation to be not much more eﬀort.
205

Deﬁnition 6.3.13. Suppose γ : XÑ
‚ Y is a Gaussian channel in KℓpPq. Then the discrete-time
Laplace doctrine deﬁnes a system Lpγq : pX, Xq Ñ pY, Y q in CiliaN
GausspKℓpPFdqq as follows (using
the representation of Proposition 5.3.8).
• The state space is X;
• the forwards output map Lpγqo
1 : X ˆ X Ñ GausspY q is given by γ:
Lpγqo
1 :“ X ˆ X
proj2
ÝÝÝÑ X
γÝÑ GausspY q
• the backwards output map Lpγqo
2 : X ˆ GausspXq ˆ Y Ñ GausspXq is given by:
Lpγqo
2 : X ˆ GausspXq ˆ Y Ñ R|X| ˆ R|X|ˆ|X| ãÑ GausspXq
px, π, yq ÞÑ
`
x, Σρpx, π, yq
˘
(6.8)
where the inclusion picks the Gaussian state with the given statistical parameters, whose
covariance Σρpx, π, yq :“
`
B2
xEpπ,γq
˘
px, yq´1 is deﬁned following equation (6.3) (Lemma
6.3.11);
• the update map Lpγqu : X ˆ GausspXq ˆ Y Ñ GausspXq returns a point distribution on
the updated mean
Lpγqu : X ˆ GausspXq ˆ Y Ñ GausspXq
px, π, yq ÞÑ ηP
X
`
µρpx, π, yq
˘
where ηP
X : X Ñ GausspXq denotes the unit of the monad P and µρ is deﬁned by
µρpx, π, yq :“ x ` λ BxµγpxqT ηγpy, xq ´ λ ηπpxq .
Here, the precision-weighted error terms η are as in equation (6.7) (Remark 6.3.12), and
λ : R` is some choice of ‘learning rate’.
Remark 6.3.14. Note that the update map Lpgqu as deﬁned here is actually deterministic, in
the sense that it is deﬁned as a deterministic map followed by the unit of the probability monad.
However, the general stochastic setting is necessary, because the composition of system depends
on the composition of Bayesian lenses, which is necessarily stochastic.
The Laplace doctrine will be constructed factoring through the following class of statistical
games.
Deﬁnition 6.3.15. A Laplacian statistical game is a parameterized statistical game pγ, ρ, φq :
pX, Xq X
ÝÑ pY, Y q satisfying the following conditions:
206

1. X and Y are ﬁnite-dimensional Cartesian spaces;
2. the forward channel γ is an unparameterized Gaussian channel;
3. the backward channel ρ is parameterized by X and deﬁned as the backwards output map of
the Laplace doctrine (equation (6.8) of Deﬁnition 6.3.13); that is,
ρ : X ˆ GausspXq ˆ Y Ñ R|X| ˆ R|X|ˆ|X| ãÑ GausspXq
px, π, yq ÞÑ
`
x, Σρpx, π, yq
˘
where the inclusion picks the Gaussian with mean x and Σρpx, π, yq “
`
B2
xEpπ,γq
˘
px, yq´1;
4. the loss function φ : ř
x:X Ctx
`
γ, ρx
˘
Ñ R is given for each x : X by φxpπ, kq “
Ey„L π | γ | k M
“
FLpyq
‰
, where FL is the Laplacian free energy
FLpyq “ Epπ,γq px, yq ´ SX
“
ρpx, π, yq
‰
“ ´ log pγpy|xq ´ log pπpxq ´ SX
“
ρpx, π, yq
‰
as deﬁned in equation (6.2) of Lemma 6.3.11.
(By “unparameterized channel”, we mean a channel parameterized by the trivial space 1; the pair
pγ, ρq constitutes a parameterized Bayesian lens with parameter space X, where the choice of γ
simply forgets the parameter, discarding it along the universal map X Ñ 1.)
In particular, the systems in the image of L will be obtained by stochastic gradient descent.
Proposition 6.3.16. Given a Laplacian statistical game pγ, ρ, φq : pX, Xq Ñ pY, Y q, Lpγq is
obtained by stochastic gradient descent of the loss function φ with respect to the mean x of the
posterior ρpx, π, yq.
Proof. We have φxpπ, kq “ Ey„L π | γ | k M
“
FLpyq
‰
, where
FLpyq “ ´ log pγpy|xq ´ log pπpxq ´ SX
“
ρpx, π, yq
‰
.
Since the entropy SX rρπpyqs depends only on the variance Σρpx, π, yq, to optimize the mean x
it suﬃces to consider only the energy Epπ,γqpx, yq. We have
Epπ,γqpx, yq “ ´ log pγpy|xq ´ log pπpxq
“ ´1
2
A
ϵγpy, xq, Σγpxq´1ϵγpy, xq
E
´ 1
2
@
ϵπpxq, Σπ´1ϵπpxq
D
` log
b
p2πq|Y | det Σγpxq ` log
b
p2πq|X| det Σπ
207

and a straightforward computation shows that
BxEpπ,γqpx, yq “ ´BxµγpxqT Σγpxq´1ϵγpy, xq ` Σπ´1ϵπpxq .
We can therefore rewrite the mean parameter µρpx, π, yq emitted by the update map Lpγqu as
µρpx, π, yq “ x ` λ BxµγpxqT ηγpy, xq ´ λ ηπpxq
“ x ´ λ BxEpπ,γqpx, yq
“ x ´ λ BxFLpyq
where the last equality holds because the entropy does not depend on x. This shows that Lpγqu
descends the gradient of the Laplacian energy with respect to x.
To see then that Lpγqu performs stochastic gradient descent of φ, note that in the dynamical
semantics, the input y : Y is supplied by the context. In CiliaN
GausspKℓpPFdqq, the dynamics in
the context are stochastic, meaning that each y : Y is in general sampled from a random variable
valued in Y . If we ﬁx the context to sample y from L π | γ | k M then, for a given x : X, the expected
trajectory of µρ is given by
E
y„L π | γ | k M
“
µρpx, π, yq
‰
“
E
y„L π | γ | k M
“
x ´ λ BxFLpyq
‰
“ x ´ λ Bx
E
y„L π | γ | k M
“
FLpyq
‰
by linearity of expectation
“ x ´ λ Bxφxpπ, kq .
Since L π | γ | k M is just a placeholder for the random variable from which y is sampled, this
establishes the result.
Using the preceding proposition, we can construct the functor deﬁning the Laplace doctrine.
Theorem 6.3.17. Let G denote the subcategory of PSGameKℓpPFdq generated by Laplacian
statistical games pγ, ρ, φq : pX, Xq
X
ÝÑ pY, Y q and by the structure morphisms of a monoidal
category.
Then L extends to a strict monoidal functor GausspKℓpPFdqq ãÑ G Ñ CiliaN
GausspKℓpPFdqq,
where the ﬁrst factor is the embedding taking any such γ to the corresponding Laplacian game,
and the second factor performs stochastic gradient descent of loss functions with respect to their
external parameterization.
It helps to separate the proof of the theorem from the proof of the following lemma.
Lemma 6.3.18. There is an identity-on-objects strict monoidal embedding of GausspKℓpPFdqq
into G.
208

Proof. The structure morphisms of GausspKℓpPFdqq are mapped to the (trivially parameterized)
structure morphisms of G, and any Gaussian channel γ : XÑ
‚ Y is mapped to the unique Laplacian
statistical game with γ as the (unparameterized) forward channel, and the (parameterized) backward
channel and loss function determined by the deﬁnition of Laplacian statistical game. It is clear that
this deﬁnition gives a faithful functor, and thus an embedding. Since it preserves explicitly the
monoidal structure, it is also strict monoidal.
Proof of Theorem 6.3.17. Thanks to Lemma 6.3.18, we now turn to the functor G Ñ CiliaN
GausspKℓpPFdqq,
which we will also denote by L; the composite functor is obtained by pulling this functor
G Ñ CiliaN
GausspKℓpPFdqq back along the embedding GausspKℓpPFdqq ãÑ G.
Suppose then that g :“ pγ, ρ, φq : pX, Xq X
ÝÑ pY, Y q is a Laplacian statistical game. Proposition
6.3.16 tells us that Lpgq is obtained by stochastic gradient descent of the loss function φ with respect
to the mean parameter of the backwards channel ρ. By deﬁnition of ρ, this mean parameter is
given precisely by the external parameterization, and so we have that Lpgq is obtained by stochastic
gradient descent of φ with respect to this parameterization.
To extend L to a functor accordingly, we need to check that performing stochastic gradient
descent with respect to the external parameterization preserves identities and composition. First
we note that, following Deﬁnition 6.3.13, the dynamical systems in the image of L emit lenses by
ﬁlling in the parameterization with the dynamical state, and by the preceding remarks, update
the state by stochastic gradient descent. Next, note that identity parameterized lenses are trivially
parameterized, so there is no parameter to ‘ﬁll in’, and no state to update; similarly, the loss function
of an identity game is the constant function on 0, and therefore has zero gradient. On identity games
pX, Xq 1ÝÑ pX, Xq, therefore, L returns the system with trivial state space 1 that constantly outputs
the identity lens pX, Xq ÞÑ pX, Xq: but this is just the identity on pX, Xq in CiliaN
GausspKℓpPFdqq,
so L preserves identities.
We now consider composites. Suppose h :“ pδ, σ, ψq : pY, Y q YÝÑ pZ, Zq is another Laplacian
game satisfying the hypotheses of the theorem. Since CiliaN
GausspKℓpPFdqq is a bicategory, we need
to show that Lphq ˝ Lpgq – Lph ˝ gq. In fact, we will show the stronger result that Lphq ˝ Lpgq “
Lph ˝ gq, which means demonstrating equalities between the state spaces, output maps, and update
maps of the systems on the left- and right-hand sides.
On state spaces, the equality obtains since the composition of externally parameterized games
(Example 6.2.25) returns a game whose parameter space is the product of the parameter spaces of
the factors. Similarly, composition of systems in CiliaN
GausspKℓpPFdqq (after Deﬁnition 5.3.7) returns
a system whose state space is the product of the state spaces of the factors. Finally, L acts by taking
parameter spaces to state spaces, and we have X ˆ Y “ X ˆ Y .
Next, we note that the output of a composite system in CiliaN
GausspKℓpPFdqq is given by composing
the outputs of the factors. This is the same as the output returned by L on a composite game, since
209

outputs in the image of L just ﬁll in the external parameter using the dynamical state. Therefore
`
Lphq ˝ Lpgq
˘o “ Lph ˝ gqo.
We now consider the update maps, beginning by computing Lph˝gqu. The state space is XˆY and
h˝g has type pX, Xq XˆY
ÝÝÝÑ pZ, Zq, so Lph˝gqu has type X ˆY ˆGausspXqˆZ Ñ GausspX ˆ
Y q. Following Example 6.2.25, the composite loss function pψφq : ř
µρ:X,µσ:Y Ctxphµσ  gµρq Ñ R
is given by:
pψφqpµρ, µσ, π, kq “
E
y„σpµσqγ‚πX ‚L π | γ | δ˚k M
“
FL`
ρpµρqπX, γ; πX, y
˘‰
`
E
z„L pMbγq‚π | δ | k M
“
FL`
σpµσqγ‚πX, δ; γ ‚ πX, z
˘‰
Here, µρ and µσ are the parameters in X and Y , respectively, and we write gµρ and hµσ to indicate
the corresponding lenses with those parameters. The context is pπ, kq, with π : 1Ñ
‚ M b X in
GausspKℓpPqq and πX denoting its X marginal, and with continuation k : GausspKℓpPqqp1, Mb
Zq Ñ GausspKℓpPqqp1, N b Zq, for some choices of residual objects M and N. The backwards
channels ρ and σ are externally parameterized and state-dependent, so that ρpµρqπX : Y Ñ
‚ X is
returned by ρpµρq at πX. Explicitly, ρ has the type X Ñ E
`
GausspXq, GausspKℓpPqqpY, Xq
˘
,
and σ has the type Y Ñ E
`
GausspY q, GausspKℓpPqqpZ, Y q
˘
. Finally, δ˚k is the function
GausspKℓpPqqp1, MbY q
GausspKℓpPqqp1,Mbδq
ÝÝÝÝÝÝÝÝÝÝÝÝÝÝÑ GausspKℓpPqqp1, MbZq kÝÑ GausspKℓpPqqp1, NbZq
obtained by pulling back k along δ.
We therefore have L π | γ | δ˚k M “ L pM b γq ‚ π | δ | k M, meaning that we can rewrite the loss
function as
E
z„L π | γ | δ˚k M
«
FL`
σpµσqγ‚πX, δ; γ ‚ πX, z
˘
`
E
y„σpµσqγ‚πX pzq
“
FL`
ρpµρqπX, γ; πX, y
˘‰
ﬀ
.
In the dynamical semantics for stochastic gradient descent, z and πX are supplied by the inputs to
the dynamical system: the inputs replace the context for the game. Rewriting the loss accordingly
gives a function
f : pz, πX, µρ, µσq ÞÑ FL`
σpµσqγ‚πX, δ; γ ‚ πX, z
˘
`
E
y„σpµσqγ‚πX pzq
“
FL`
ρpµρqπX, γ; πX, y
˘‰
.
Next, we compute Bpµρ,µσqfpz, πXq. We obtain
Bpµρ,µσqfpz, πXq “
˜
Bµρ
E
y„σpµσqγ‚πX pzq
“
FL`
ρpµρqπX, γ; πX, y
˘‰
, Bµσ FL`
σpµσqγ‚πX, δ; γ ‚ πX, z
˘
¸
“
˜
E
y„σpµσqγ‚πX pzq
“
BµρFL`
ρpµρqπX, γ; πX, y
˘‰
, Bµσ FL`
σpµσqγ‚πX, δ; γ ‚ πX, z
˘
¸
.
210

Now, Lph ˝ gqu is deﬁned as returning the point distribution on pµρ, µσq ´ λ Bpµρ,µσqfpz, πXq:
pµρ, µσq ´ λ Bpµρ,µσqfpz, πXq
“
˜
E
y„σpµσqγ‚πX pzq
“
µρ ´ λ BµρFL`
ρpµρqπX, γ; πX, y
˘‰
, µσ ´ λ Bµσ FL`
σpµσqγ‚πX, δ; γ ‚ πX, z
˘
¸
.
We can simplify this expression by making some auxiliary deﬁnitions
ρupa, π, yq :“ a ´ λ BaFL`
ρpaqπ, γ; π, y
˘
σupb, π1, zq :“ b ´ λ Bb FL`
σpbqπ1, δ; π1, z
˘
so that
pµρ, µσq ´ λ Bpµρ,µσqfpz, πXq “
˜
E
y„σpµσqγ‚πX pzq rρupµρ, πX, yqs , σupµσ, γ ‚ πX, zq
¸
.
(6.9)
By currying ρupµρ, πX, yq into a function ρupµρ, πXq : Y Ñ PX, we can simplify this still further,
since
E
y„σpµσqγ‚πX pzq rρupµρ, πX, yqs “ ρupµρ, πXq ‚ σpµσqγ‚πXpzq .
Since equation (6.9) deﬁnes Lph ˝ gqu, we have
Lph ˝ gqupµρ, µσ, π, zq “ ηP
XˆY
`
ρupµρ, πq ‚ σpµσqγ‚πpzq, σupµσ, γ ‚ π, zq
˘
(6.10)
where ηP
XˆY : X ˆ Y Ñ GausspX ˆ Y q is the component of the unit of the monad P at X ˆ Y ,
which takes values in Dirac delta distributions and is therefore Gaussian.
Next, we compute the update map of the system Lphq ˝ Lpgq, using Deﬁnitions 5.3.5 and 5.3.7
(which deﬁne composition in CiliaN
GausspKℓpPFdqq). This update map is given by composing the
‘double strength’13 dst : GausspXq ˆ GausspY q Ñ GausspX ˆ Y q after the following string
13The double strength is also known as the ‘commutativity’ of the monad P with the product ˆ. It says that a pair of
distributions π on X and χ on Y can also be thought of as a joint distribution pπ, χq on X ˆ Y . It is Gaussian on
Gaussians, as the product of two Gaussians is again Gaussian.
211

diagram:
γ˚
σ5
Lpgqu
Lphqu
GausspXq
Y
X
Z
GausspY q
GausspXq
(6.11)
Here, σ5 denotes the uncurrying of the parameterized state-dependent channel σ : Y
Ñ
StatpY qpZ, Y q: we can equivalently write the type of σ as Y Ñ E
`
GausspY q, GausspKℓpPqqpZ, Y q
˘
,
which we can uncurry twice to give the type Y ˆ GausspY q ˆ Z Ñ GausspY q.
Observe now that we can write Lpgqu and Lphqu as
Lpgqupa, π, yq “ ηP
X
`
ρupa, π, yq
˘
Lphqupb, π1, zq “ ηP
Y
`
σupb, π1, zq
˘
and that ηP
XˆY “ dstpηP
X, ηP
Y q. Reading the string diagram and applying this equality, we ﬁnd that
it represents
`
Lphq ˝ Lpgq
˘upµρ, µσ, π, zq as
ηP
XˆY
`
ρupµρ, πq ‚ σpµσqγ‚πpzq, σupµσ, γ ‚ π, zq
˘
which is precisely the same as the deﬁnition of Lph ˝ gqu in equation (6.10).
Therefore, as required,
`
Lphq ˝ Lpgq
˘u “ Lph ˝ gqu.
Finally, because the functor L is identity-on-objects, the unit and multiplication of its monoidal
structure are easily seen to be given by identity morphisms, and so L is strict monoidal: L maps
the structure morphisms to constant dynamical systems emitting the structure morphisms of
CiliaN
GausspKℓpPFdqq, and so the associativity and unitality conditions are satisﬁed.
Remark 6.3.19. From the diagram (6.11), we can reﬁne our understanding of what is known in the
literature as the mean ﬁeld approximation [168, around eq.39], in which the posterior over X b Y
212

is assumed at each instant of time to have independent marginals. We note that, even though the
backwards output maps emit posterior distributions with means determined entirely by their local
parameterization, and even though these parameters are updated by the tensor Lpgqu b Lphqu, the
resulting dynamical states are correlated across time by the composition rule: this is made very
clear by the wiring of diagram (6.11), since both factors Lpgqu and Lphqu have common inputs. We
also note that, even if the means of the emitted posteriors are entirely parameter-determined, this
is not true of their covariances, which are functions of both the prior and the observation. The
operational result of these observations is that the functorial (and pictorial) approach advocated
here (as opposed to writing down a complete, and complex, joint distribution for each model
of interest and proceeding from there) helps us understand the structural properties of complex
systems—where it is otherwise easy to get lost in the weeds.
Remark 6.3.20. Above we exhibited the Laplace doctrine directly as a functor
GausspKℓpPFdqq ãÑ G Ñ CiliaN
GausspKℓpPFdqq .
In fact, Proposition 6.3.16 implies that it factors further, as
GausspKℓpPFdqq ãÑ G ∇
ÝÑ DiﬀCiliaGausspKℓpPFdqq
HNaivek
ÝÝÝÝÝÑ CiliaN
GausspKℓpPFdqq
where ∇: G Ñ DiﬀCiliaGausspKℓpPFdqq takes an externally parameterized statistical game and
returns a diﬀerential system that performs gradient descent on its loss function with respect to its
parameterization. We leave the precise exhibition of this factorisation for future work.
6.3.3. Synaptic plasticity with the Hebb-Laplace doctrine
The Laplace doctrine constructs dynamical systems that produce progressively better posterior
approximations given a ﬁxed forwards channel, but natural adaptive systems—brains in particular—
do more than this: they also reﬁne the forwards channels themselves, in order to produce better
predictions. In doing so, these systems better realize the abstract nature of autoencoder games, for
which improving performance means improving both prediction as well as inversion. To be able to
improve the forwards channel requires allowing some freedom in its choice, which means giving it
a nontrivial parameterization.
The Hebb-Laplace doctrine that we introduce in this section therefore modiﬁes the Laplace
doctrine by ﬁxing a class of parameterized forwards channels and performing stochastic gradient
descent with respect to both these parameters as well as the posterior means; we call it the Hebb-
Laplace doctrine as the particular choice of forwards channels results in their parameter-updates
resembling the ‘local’ Hebbian plasticity known from neuroscience, in which the strength of the
connection between two neurons is adjusted according to their correlation[111, 171–174]. (Here,
we could think of the ‘neurons’ as encoding the level of activity along a basis vector.)
213

We begin by deﬁning the category of these parameterized forwards channels, after which we
introduce Hebbian-Laplacian games and the resulting Hebb-Laplace doctrine, which is derived
similarly to the Laplace doctrine above.
Deﬁnition 6.3.21. Let H denote the subcategory of PGausspParap‹qq generated by the structure
morphisms of the symmetric monoidal category GausspParap‹qq (trivially parameterized), and
by morphisms X Ñ Y of the form (written in E)
ΘX Ñ GausspParap‹qqpX, Y q
θ
ÞÑ
´
x ÞÑ θ hpxq ` ω
¯
where h is a diﬀerentiable map X Ñ Y , ΘX is the vector space of square matrices on X, and ω is
sampled from a Gaussian distribution on Y .
Note that there is a canonical embedding of PGausspParap‹qq into PKℓpPFdq, obtained in
the image of Proposition 6.3.5 under the external parameterization P.
Deﬁnition 6.3.22. A Hebbian-Laplacian statistical game is a parameterized statistical game
pγ, ρ, φq : pX, Xq ΘXˆX
ÝÝÝÝÑ pY, Y q satisfying the following conditions:
1. X and Y are ﬁnite-dimensional Cartesian spaces;
2. the forward channel γ is a morphism in H (i.e., of the form x ÞÑ θ hpxq ` ω);
3. the backward channel is as for a Laplacian statistical game (Deﬁnition 6.3.15);
4. the loss function is as for a Laplacian statistical game, with the substitution γ ÞÑ γpθq for
parameter θ : ΘX.
We will write GH to denote the subcategory of PSGame generated by Hebbian-Laplacian statistical
games and by the structure morphisms of a monoidal category.
Deﬁnition 6.3.23. Suppose γ : X Ñ Y is a morphism in H. Then the discrete-time Hebb-Laplace
doctrine deﬁnes a system Hpγq : pX, Xq Ñ pY, Y q in CiliaN
GausspKℓpPFdqq as follows (using the
representation of Proposition 5.3.8).
• The state space is ΘX ˆ X (where ΘX is again the vector space of square matrices on X);
• the forwards output map Hpγqo
1 : ΘX ˆ X ˆ X Ñ GausspY q is given by γ:
Hpγqo
1 :“ ΘX ˆ X ˆ X
proj1,3
ÝÝÝÝÑ ΘX ˆ X
γ5
ÝÑ GausspY q
where γ5 is the uncurried form of the morphism γ : ΘX Ñ GausspParap‹qqpX, Y q in the
image of the embedding of H in PKℓpPq;
214

• the backwards output map Hpγqo
2 : ΘX ˆ X ˆ GausspXq ˆ Y Ñ GausspXq is given by:
Hpγqo
2 : ΘX ˆ X ˆ GausspXq ˆ Y Ñ R|X| ˆ R|X|ˆ|X| ãÑ GausspXq
pθ, x, π, yq ÞÑ
`
x, Σρpθ, x, π, yq
˘
where the inclusion picks the Gaussian state with the given statistical parameters, whose
covariance Σρpθ, x, π, yq :“
`
B2
xEpπ,γpθqq
˘
px, yq´1 is deﬁned following equation (6.3)
(Lemma 6.3.11);
• the update map Hpγqu : ΘX ˆ X ˆ GausspXq ˆ Y Ñ GausspΘX ˆ Xq optimizes the
parameter for γ as well as the mean of the posterior (as in the Laplace doctrine):
Hpγqu : ΘX ˆ X ˆ PX ˆ Y Ñ PpΘX ˆ Xq
pθ, x, π, yq ÞÑ ηP
ΘXˆX
`
θupθ, x, yq, µρpθ, x, π, yq
˘
where ηP denotes the unit of the monad P, and θu and µρ are deﬁned by
θupθ, x, yq :“ θ ´ λθ ηγpθqpy, xq hpxqT
µρpθ, x, π, yq :“ x ` λρ BxhpxqT θT ηγpθqpy, xq ´ λρ ηπpxq .
Here, λθ, λρ : R` are chosen learning rates, and the precision-weighted error terms η are
again as in equation (6.7) (Remark 6.3.12).
Remark 6.3.24. The ‘Hebbian’ part of the Hebb-Laplace doctrine enters in the forwards-parameter
update map, θupθ, x, yq “ θ ´ λθ ηγpθqpy, xq hpxqT , since the change in parameters is proportional
to something resembling the correlation between ‘pre-synaptic’ and ‘post-synaptic’ activity. Here,
the post-synaptic activity is represented by the term hpxq: we may think of the components of the
vector x as each representing the “internal activity” of a single neuron, and the “activation function”
h as returning the corresponding ﬁring rates (analogously to the rate-coded neural circuits of
Deﬁnition 3.3.10); these quantities are ‘post-synaptic’ as the ﬁring is emitted down a neuron’s axon,
which occurs computationally after the neuron’s synaptic inputs. The synaptic inputs (generating
the pre-synaptic activity) are then thought to be represented by the error term ηγpθqpy, xq, so that
expected trajectory of the outer product ηγpθqpy, xq hpxqT computes the correlation between pre-
and post-synaptic acivity.
Note that this means that we assume that λθ ă λρ, because the neural activity x itself must
change on a faster timescale than the synaptic weights θ, in order for θ to learn these correlations.
Given the foregoing deﬁnition, we obtain the following theorem.
Theorem 6.3.25. The Hebb-Laplace doctrine H deﬁnes an identity-on-objects strict monoidal
functor H ãÑ GH Ñ CiliaN
GausspKℓpPFdqq.
215

This theorem follows in the same way as the corresponding result for the Laplace doctrine; and
so we begin with a small lemma, and subsequently show that the doctrine arises by stochastic
gradient descent, before putting the pieces together to prove the theorem itself.
Lemma 6.3.26. There is an identity-on-objects strict monoidal embedding H ãÑ GH.
Proof sketch. The proof proceeds much as the proof of Lemma 6.3.18, except that the forwards
channels of games in the image of the embedding are given by the parameterized morphisms of
H.
Proposition 6.3.27. Given a Hebbian-Laplacian statistical game pγ, ρ, φq : pX, Xq ΘXˆX
ÝÝÝÝÑ pY, Y q,
Hpγq is obtained by stochastic gradient descent of the loss function φ with respect to the weight
matrix θ : ΘX of the channel γ and the mean x : X of the posterior ρ.
Proof. The proof proceeds much as the proof of Proposition 6.3.16, except now the forwards channel
γ is parameterized: this gives us another factor against which to perform gradient descent, and
furthermore means that γpθq must be substituted for γ in expressions in the derivation of µρ.
The ﬁrst such expression is the deﬁnition of the loss function φ : ř
pθ,xq:ΘXˆX Ctx
`
γpθq, ρpxq
˘
Ñ
R; we will write φpθ,xq for the component of φ at pθ, xq with the corresponding type
Ctx
`
γpθq, ρpxq
˘
Ñ R. We have φpθ,xqpπ, kq “ Ey„L π | γpθq | k M
“
FLpyq
‰
, where now
FLpyq “ ´ log pγpθqpy|xq ´ log pπpxq ´ SX
“
ρpx, π, yq
‰
.
We ﬁnd
BxFLpyq “ BxEpπ,γpθqq
“ ´ BxµγpθqpxqT Σγpθqpxq´1ϵγpθqpy, xq ` Σπ´1ϵπpxq
“ ´ BxhpxqT θT ηγpθqpy, xq ` ηπpxq
and
BθFLpyq “ BθEpπ,γpθqq
“ ´ Bθ
2
A
ϵγpθqpy, xq, Σγpθqpxq´1ϵγpθqpy, xq
E
“ ´ Bθ
2
A
y ´ θhpxq, Σγpθqpxq´1`
y ´ θhpxq
˘E
“ Σγpθqpxq´1`
y ´ θhpxq
˘
hpxqT
“ Σγpθqpxq´1ϵγpθqpy, xq hpxqT
“ ηγpθqpy, xq hpxqT .
216

Consequently, we have
µρpθ, x, π, yq “ x ` λρ BxhpxqT θT ηγpθqpy, xq ´ λρ ηπpxq
“ x ´ λρ BxFLpyq
and
θupθ, x, yq “ θ ´ λθ ηγpθqpy, xq hpxqT
“ θ ´ λθ BθFLpyq ,
and this means that we can write
Hpγqupθ, x, π, yq “ ηP
ΘXˆX ˝
´
pθ, xq ´ pλθ, λρq Bpθ,xqFLpyq
¯
“ ηP
ΘXˆX ˝
´
p ´ λ BpFLpyq
¯
where p :“ pθ, xq and λ :“ pλθ, λρq, which establishes that Hpγqu descends the gradient of the
free energy with respect to the parameterization p.
Finally, with y sampled from a ﬁxed context, we can see that the expected trajectory of Hpγq
follows
E
y„L π | γpθq | k M
´
p ´ λ Bp FLpyq
¯
“
´
p ´ λ Bp
E
y„L π | γpθq | k M
“
FLpyq
‰¯
“
´
p ´ λ Bp φppπ, kq
¯
which demonstrates that Hpγq performs stochastic gradient descent of the loss function.
Proof of Theorem 6.3.25. Lemma 6.3.26 gives us the ﬁrst factor H ãÑ GH, so we only need to
establish that the Hebb-Laplace doctrine obtains by pulling a functor GH Ñ CiliaN
GausspKℓpPFdqq
back along this inclusion. We now turn to establishing that stochastic gradient descent returns the
desired identity-on-objects functor GH Ñ CiliaN
GausspKℓpPFdqq. Proposition 6.3.27 shows that H is
obtained by applying stochastic gradient descent to morphisms in GH, so we need to show that the
resulting mapping is functorial.
As in the case of Theorem 6.3.17, the structure morphisms are preserved trivially: they have trivial
parameterization, and so stochastic gradient descent returns the trivial systems constantly emitting
the corresponding lenses; in particular, this means that stochastic gradient descent preserves
identities.
We now show that, for composable games h and g, Hphq ˝ Hpgq “ Hph ˝ gq. This means
demonstrating equalities between state spaces, output maps, and update maps. As for Theorem
217

6.3.17, the state spaces are given by the external parameterization, and the parameterization of the
composite game h ˝ g and the state space of the composite system Hphq ˝ Hpgq are both given by
taking the product of the factors, and so the state spaces on the left- and right-hand sides of the
desired equation are equal.
The proof that the equality holds for output maps is also as in the proof of Theorem 6.3.17: the
output of a composite system is given by composing the output lenses of the factors, which is
the same as the output returned by H on a composite game, since outputs in the image of H are
obtained by ﬁlling in the external parameter.
We now turn to the update maps, for which we need to show that
`
Hphq ˝ Hpgq
˘u “ Hph ˝ gqu.
Suppose g :“ pγ, ρ, φq : pX, Xq Ñ pY, Y q and h :“ pσ, δ, ψq : pY, Y q Ñ pZ, Zq are Hebbian-
Laplacian statistical games; we will denote the corresponding parameters by pθγ, µρq and pθδ, µσq
respectively. Following the proof of Theorem 6.3.17, we can write the loss function of the composite
game pσ, δ, ψq ˝ pγ, ρ, φq as
E
z„L π | γpθγq | δpθδq˚k M
”
FL`
σpµσqγpθγq‚πX, δpθδq; γpθγq ‚ πX, z
˘
`
E
y„σpµσqγpθγq‚πX pzq
“
FL`
ρpµρqπX, γpθγq; πX, y
˘‰ ı
.
(This expression is obtained by making the substitutions γ ÞÑ γpθγq and δ ÞÑ δpθδq in the
corresponding expression in the proof of Theorem 6.3.17.)
As before, z and πX are supplied by the inputs to the dynamical system, and so we obtain a
function
f : pz, πX, θγ, µρ, θδ, µσq ÞÑ FL`
σpµσqγpθγq‚πX, δpθδq; γpθγq ‚ πX, z
˘
`
E
y„σpµσqγpθγq‚πX pzq
“
FL`
ρpµρqπX, γpθγq; πX, y
˘‰
.
If we write p :“ pθγ, µρq and q :“ pθδ, µσq, then pp, qq denotes the parameter for h ˝ g. Since H
performs stochastic gradient descent with respect to the parameterization, Hph ˝ gqu is therefore
deﬁned as returning the point distribution on pp, qq ´ λ Bpp,qqfpz, πXq, where λ :“ pλp, λqq, and
λp “ pλγ, λρq and λq “ pλδ, λσq.
We have Bpp,qqf “
`
Bpf, Bqfq and so
pp, qq ´ λ Bpp,qqfpz, πXq “
`
p ´ λp Bpfpz, πXq, q ´ λq Bqfpz, πXq
˘
.
We make some auxiliary deﬁnitions
gupθγ, µρ, π, yq :“ pθγ, µρq ´ λp Bpθγ,µρqFL`
ρpµρqπ, γpθγq; π, y
˘
hupθδ, µσ, π1, zq :“ pθδ, µσq ´ λq Bpθδ,µσqFL`
σpµσqπ1, δpθδq; π1, z
˘
218

and ﬁnd that
pp, qq ´ λ Bpp,qqfpz, πXq
“ pθγ, µρ, θδ, µσq ´ λ Bpθγ,µρ,θδ,µσqfpz, πXq
“
`
pθγ, µρq ´ λp Bpθγ,µρqfpz, πXq, pθδ, µσq ´ λq Bpθδ,µσqfpz, πXq
˘
“
˜
E
y„σpµσqγpθγq‚πX pzq
“
gupθγ, µρ, πX, yq
‰
, hu`
θδ, µσ, γpθγq ‚ πX, z
˘
¸
“
´
gupθγ, µρ, πXq ‚ σpµσqγpθγq‚πXpzq, hu`
θδ, µσ, γpθγq ‚ πX, z
˘¯
.
Writing PQ to denote the composite parameter space ΘX ˆXˆΘY ˆY , the foregoing computation
deﬁnes Hph ˝ gqu : PQ ˆ GausspXq ˆ Z Ñ GausspPQq as
Hph˝gqupθγ, µρ, θδ, µσ, π, zq “ ηP
PQ
´
gupθγ, µρ, πXq‚σpµσqγpθγq‚πXpzq, hu`
θδ, µσ, γpθγq‚πX, z
˘¯
.
(6.12)
The update map of the composite system
`
Hphq ˝ Hpgq
˘u is given by composing the double
strength dst : GausspPq ˆ GausspQq Ñ GausspP ˆ Qq after the string diagram
γ5
˚
σ5
Hpgqu
Hphqu
GausspXq
Y
X
Z
GausspQq
GausspPq
ΘX
ΘY
219

where γ5
˚ indicates the uncurrying of the pushforwards of the parameterized forwards channel γ:
γ : ΘX Ñ Gauss
`
Parap‹q
˘
pX, Y q
embeds
ÞÝÝÝÝÑ
ΘX Ñ GausspKℓpPqqpX, Y q
p´q˚
ÞÝÝÝÑ
ΘX Ñ E
`
GausspKℓpPqqp1, Xq, GausspKℓpPqqp1, Y q
˘
„
ÞÝÑ
ΘX Ñ EpGausspXq, GausspY qq
p´q5
ÞÝÝÝÑ
γ5
˚ : ΘX ˆ GausspXq Ñ GausspY q .
Next, note that we can write Hpgqu and Hphqu as
Hpgqupθγ, µρ, π, yq “ ηP
P
`
gupθγ, µρ, π, yq
˘
Hphqupθδ, µσ, π1, zq “ ηP
Q
`
hu`
θδ, µσ, π1, z
˘˘
where P :“ ΘX ˆ X and Q :“ ΘY ˆ Y , and that ηP
PQ “ dstpηP
P , ηP
Qq. Reading the string diagram
and comparing with equation (6.12), we therefore ﬁnd that
`
Hphq ˝ Hpgq
˘u “ Hph ˝ gqu.
Finally, the proof that H is strict monoidal is precisely analogous to the proof that L is strict
monoidal: H is identity-on-objects and maps structure morphisms to structure morphisms, so that
the associativity and unitality conditions are immediately satisﬁed.
220

7. Future directions
A powerful motivation propelling the development of this thesis was the belief that science, and
particularly the cognitive sciences, will beneﬁt from being supplied with well-typed compositional
foundations. In this ﬁnal chapter, we survey a number of new vistas that we have glimpsed from
the vantage point of our results, and indicate routes that we might climb in order to see them better.
One important beneﬁt of the categorical framework is that it helps us express ideas at a useful
level of abstraction, and thereby compare patterns across systems and phenomena of interest. As a
result, although our primary system of interest is the brain, we are aware that much of our work is
more diversely applicable, and so our survey here is similarly not restricted to neural systems. At
the same time, as neural systems are are ﬁnest examples of natural intelligence, we attempt to stay
grounded in current neuroscience.
Beyond the evident shortcomings of the work that we have presented—which we review
momentarily—we ﬁrst consider how to use the categorical language of structure to incorporate
structure better into our models themselves (§7.1), with a particular focus on the brain’s “cognitive
maps” (§7.1.3). We will see that the compositional consideration of the structure of open systems
naturally leads us to consider societies of systems (§7.2), and hence the relationships between
compositional active inference and single- and multi-agent reinforcement learning and economic
game theory (§7.2.3), although our ﬁrst priority in this section is the incorporation of action (§7.2.1)
and planning (§7.2.2) into the framework of statistical games. From our abstract vantage point, there
is little diﬀerence between societies of agents and collective natural systems such as ecosystems1,
and so we then consider the prospects for a compositional mathematics of life (§7.3). Finally, we
close with some thoughts on matters of fundamental theory (§7.4).
Before we get into the thick of it, let us note three prominent examples of the aforementioned
evident shortcomings. Firstly, it is clear that the notion of approximate inference doctrine should
be tested with further exempliﬁcation, and that this is likely to demand a satisfactory resolution
to the (potential) problem of double-counting (Remark 6.2.5). Secondly, the generative models
that we have considered are somehow ‘static’, despite our interest in dynamical systems, and this
warrants a satisfactory exploration of dynamical generative model. Thirdly, although we considered
“lower level” neural circuit models in §3.3, we did not explicitly connect our approximate inference
1After all, a single multicellular organism is itself a kind of society of agents.
221

doctrines to these more ‘biological’ models. A satisfactory account of the Bayesian brain would of
course span from abstract principles to detailed biology, a relationship the elaboration of which we
sadly we leave to future work.
Fortunately, although these three shortcomings may be pressing, we expect that the pursual of a
research programme akin to that sketched below would result in overcoming them.
7.1. Structured worlds
7.1.1. Bayesian sensor fusion
A situation that is common in natural embodied systems but which is not yet well treated by
current statistical and machine learning methods2, particularly those that are most popular in
computational neuroscience, is that of sensor fusion. In this situation, one has a number of sensors
(such as cameras or retinal ganglion cells) which report spatially situated data, and where the sensor
ﬁelds overlap in the space; the problem is then how to combine these “local views” of the space
into a coherent global picture. Mathematically, fusing ‘local’ data into a ‘global’ representation is
the job of sheaves: a sheaf is a “spatially coherent data type”—something like a bundle for which
“local sections” can always be uniquely glued together into a global section—and sheaf theory and
the related ﬁelds of applied topology and cohomology allow us to judge when it is possible to
form a consensus, and quantify the obstructions to the formation of consensus; recent work has
also begun to suggest algorithms and dynamics by which we can construct consensus-forming
distributed sensor systems [177].
Sheaves therefore allow us to construct and to measure spatially structured data types, but missing
from the current sheaf-theoretic understanding of sensor fusion is a thorough treatment of belief
and uncertainty, especially from a Bayesian perspective. Since biological systems contain many
distributed sensor types, and each of these systems is constituted by many cells, the mathematics of
neural representations may be expected to be sheaf-theoretic. A ﬁrst possible extension of the work
presented here, therefore, is to extend statistical games and approximate inference doctrines (and
hence the classes of model that they encompass) to structured data types such as sheaves. Because
statistical games and approximate inference doctrines are deﬁned using lenses over an abstract
category of stochastic channels, we expect that the ﬁrst step will be to consider categories of
channels between sheaves; recently, there has been work on categorifying lenses [178, 179], and we
expect that this may prove relevant here. We also expect that at this point it will become particularly
pressing to resolve the concerns of Remark 6.2.5 relating to the possible double-counting of ﬁtness
as a result of correlations: the use of sheaves will demand that overlaps are correctly accounted for.
2This is beginning to change: recently, the use of sheaf-theoretic and other applied-topological devices has started to
penetrate machine learning [175, 176].
222

In this context, we may also encounter connections to sheaf-theoretic approaches to ‘contextual-
ity’, in which answers to questions depend on (the topology of) how they are asked, and which
seems to lie at the heart of quantum nonlocality. It is notable that lenses originated in database
theory [132, 133] and that contextuality can also be observed in database systems [180, 181], and so
at this point, it may be possible to uncover the mathematical origins of ‘quantum-like’ psychological
eﬀects [182–184], and relate them formally to other kinds of perceptual bistability that have been
interpreted in a Bayesian context [185, 186]. Sheaves come with a cohomology theory that permits
the quantiﬁcation of the ‘disagreements’ that underlie such paradoxes [187–189], and dynamical
systems can be designed accordingly to minimize disagreements and thus seek consensus [177, 190,
191]. We hope that these tools will supply new and mathematically enlightening models of these
psychological phenomena, while at the same time also suggesting new connections to work on
quantum-theoretic formulations of the free-energy framework itself [192, 193].
The adoption of a sheaf-theoretic framework in this way may furthermore illuminate connections
between computational neuroscience and machine learning. Graph neural networks [194–196],
and their generalization in ‘geometric’ deep learning [197], are increasingly used to apply the
techniques of deep learning to arbitrarily structured domains, and, as indicated above, recent work
has found sheaves to supply a useful language for their study [175]. In a similar fashion, we expect
connections here to the structure of message passing algorithms [196, 198–201] (also hinted at by
Sergeant-Perthuis [202]) and less conventional structured machine learning architectures such as
capsule networks [203]. Finally, each category of sheaves is naturally a topos [204], and hence
comes with its own rich internal language, modelling dependent type theory (cf. §2.3.4).
7.1.2. Learning structure and structured learning
Having considered the incorporation of structured data into the process of inference, we can
consider the incorporation of structure into the process of learning, and here we make an important
distinction between structured learning and learning structure. By the former, we mean extending
the process of learning to a structured setting (such as the sheaf-theoretic one of the preceding
section), whereas by the latter, we mean learning the underlying structures themselves. This
latter process is also known in the literature as structure learning [205–207], but in order to avoid
ambiguity, we swap the order of the two words.
The observation at the end of the preceding section, that each category of sheaves forms a topos,
is pertinent here, as dependent type theory formalizes a notion of logical ‘context’. These contexts
are diﬀerent from those of 6.1: they contain the “axioms that are valid in the present situation”,
and determine which (non-tautological) statements can be derived. In the categorical semantics of
dependent type theory, the context is witnessed by the object over which a slice category is deﬁned,
and so in some sense it deﬁnes the “shape of the universe”. By the Grothendieck construction, there
223

is a correspondence between sheaves and certain bundles (objects of slice categories), and so (very
roughly speaking) we can think of structured inference and learning as taking place in appropriate
slice categories.
In the same way that we motivated random dynamical systems (qua bundles, §5.2.3) through “pa-
rameterization by a noise source”, we can think of bundle morphisms as generalized parameterized
maps. The problem of learning structure then becomes a problem of generalized parameter-learning,
and much like this can be formalized by a ‘reparameterization’ in the Para construction (cf. §6.2.2.1),
in this more general setting it is formalized by the “generalized reparameterization” of base-change
between topoi (cf. Remark 4.2.25). Base-change synthesizes notions of parallel transport, allowing
us to translate spatially-deﬁned data coherently between spaces—and, in particular, along changes
of structure. In this setting therefore, we expect that work on functorial lenses [178], as well as
work on functorial data migration [208, 209], may prove relevant.
At the same time, we expect this line of enquiry to clarify the relationships between our formalism
of approximate inference and other related work on the categorical foundations of cybernetics [93,
159], which have typically been studied in a diﬀerential rather than probabilistic setting [140]. We
expect the connection to be made via information geometry [210–212], where Bayesian inference
can be understood both using gradient descent [213] and as a kind of parallel transport [214].
7.1.3. Compositional cognitive cartography
In Remark 6.1.4, we explained that the continuation of a statistical game supplies the data to which
Bayesian updating is a system’s response, and that in computer inference systems, the resulting
state (that is the input to the backwards part of the lens) encodes the statistics of a ‘batch’ of
data. But learning, and particularly structure-learning, need not proceed in such a ‘batched’ way:
for example, natural systems such as animals learn the structure of their environments as they
explore them. We will come below (§7.2.1) to the question of how to incorporate action—and hence
exploration—into the compositional framework that we have developed here, but meanwhile we
note that the topos-theoretic developments sketched above may provide a suitable setting in which
to understand the neural basis for navigation, and help explain how ostensibly ‘spatial’ navigation
processes and circuits are invariably involved in more abstract problem solving [215–219].
There are two key observations underlying this proposal. Firstly, a topos is not only a richly
structured category of spaces (or spatial types), but it can also be understood as a categoriﬁed space
itself [220]: in this context, we can call each categoriﬁed space a ‘little’ topos, and the category of
spaces itself is the corresponding ‘big’ topos; changes in spatial structure—witnessed by base-change
between little topoi—thus correspond to trajectories within the space represented by the big topos.
Secondly, under the free energy principle, there is a close relationship between beliefs about the
geometry of an environment and beliefs about expected future trajectories in that environment
224

[50]: fundamentally, this is also the idea underlying the “successsor representation” [221] of the
cognitive map, which says roughly that the brain’s representation of where it is is equivalently a
representation of where it soon expects to be [66, 222, 223]. Although there have been studies in the
informal scientiﬁc literature attempting to connect free-energy models of navigation, exploration,
and the cognitive map with the successor representation [67], and to place both of these in less
overtly spatial contexts [223, 224], there has not yet been a comprehensive mathematical treatment
explaining the structures that underlie this nexus.
By placing such a mathematical treatment in a topos-theoretic context, it may be possible to
make sense of the “logic of space” of topoi to explain why animals’ abstract problem-solving makes
use of their abilities for spatial navigation: in particular, proving a proposition is mathematically
analogous to ﬁnding a path from premise to conclusion. Moreover, in a spatial topos, the “truth
values” are no longer simply binary, but encode where a proposition is (believed to be) true; the
(sub)object classiﬁer of a spatial topos encodes something like the “internal universe” of that topos,
or “the universe according to the system”.
To be successful, this mathematical treatment should be attentive to the results and proposals of
computational and theoretical neuroscience, and so we now turn to our second key observation: the
relationship between (believed) geometry and (expected) dynamics. This will require an extension
of statistical games and approximate inference to dynamical generative models; until this point,
our treatment has merely supplied inference (or ‘recognition’ [47]) dynamics to static models.
Through this extension, we should expect a connection to other work on dynamical inference, such
as ﬁltering [225, 226] and particularly its emerging compositional treatment [227, 228].
Under the free-energy principle, and similarly under the successor representation, the expected
dynamics is a geodesic ﬂow, which is by geodesy determined by beliefs about the spatial geometry.
But these beliefs in turn are not static: they depend on what the agent believes will happen [48,
229], and this has been suggested as an explanation for the ‘predictive’ nature of the cognitive
map [50]. The cognitive map has its central locus in the hippocampus [230–232], which we
may therefore understand as representing the base space over which the big topos is sliced; and
since changes-of-plan seem therefore to induce changes-of-base, we might see the ‘functional’
connectivity of the brain [233] as witnessing this mathematical structure.
Because the internal universe of the topos represented by the cognitive map is inherently
context-dependent, it seems to ﬁt naturally with the subjectivist metaphysics implied by the free
energy framework—that the universe as experienced by an agent is a construction of that agent’s
internal model, as updated by approximate inference—and thus provide a natural setting for the
mathematical study of phenomenology. Moreover, as categories of sheaves, agents’ internal topoi
encode the consensus formed by the distributed circuits and sensors that constitute their beliefs,
and this points a way towards understanding how societies of agents are able to inhabit shared
225

spaces about which they form a consensus between themselves: the mathematics of this shared
universe should be little diﬀerent from the mathematics of a single agent’s internal universe.
Such multi-agent adaptive systems have been studied in the context of reinforcement learning
(of which more below), but this potential for the formation of ‘nested’ systems (perhaps
multicategorical) with shared universes implied by consensus is not the only connection between
cognitive cartography and reinforcement learning, as it is in reinforcement learning that the
successor representation originates. We therefore hope that this line of enquiry may illuminate the
relationship between reinforcement learning and compositional active inference, to the basis of
which we now turn.
7.2. Societies of systems
Adaptive agents being necessarily in interaction with an external environment, we saw in the
previous section how consideration of the compositional structure of agents’ internal maps of their
worlds easily leads to the consideration of societies of agents. However, in order for us to study
these, we ﬁrst need to make the more basic step of incorporating action into the compositional
framework: a collection of purely passive agents is no society.
7.2.1. Active inference
The doctrines of approximate inference introduced in this thesis are inherently perceptual. As we
described in §6.1, the forwards channel of a statistical game points “towards the environment”,
predicting the expected incoming sense-data, whereas the backwards channel points from the
environment into the agent, terminating in the agent’s most causally abstract beliefs. In other
contemporary work on categorical cybernetics, the orientation appears diﬀerent: the forwards
channel of an open (economic) game, for instance, points along the direction of interaction in the
environment, in the direction of causality, from observations to actions [17, 141]; there is no room
for prediction and its inversion, and the two kinds of game seem somehow perpendicular.
In resolution of this apparent disagreement, we can observe that an open economic game does
have a perpendicular direction: a second3 dimension inhabited by the strategies. That is to say, an
open economic game is a lens externally parameterized by strategies, a function from the latter
to the former, and therefore formally much like our cilia (§5.3). This resemblence becomes even
closer when one considers the recent ‘diegetic’ formulation of open games, in which strategies
themselves can be updated using a backwards map from the arena of the game back into strategies
(or rather, strategy updates).
3Or third, if one remembers the monoidal structure.
226

This suggests one way in which we can incorporate action and thereby shape the framework
of this thesis into a framework for active inference: the forwards channel should predict not only
sense-data incoming from the environment, but also the actions to be taken by the agent. Indeed
this matches the usual informal presentation of active inference, which adopts a channel of the
form XÑ
‚ S b A, where S is the space of sense-data, A the space of possible actions, and X the
‘latent’ space.
Yet at this point the formal similarity between compositional active inference and compositional
game theory again begins to recede, as a channel XÑ
‚ S b A is more like a “stochastic span” than
an open economic game’s player model Σ Ñ rS, As. Moreover, we expect our active inference
systems to have a richer variety of patterns of interaction, being embodied in a world—in part,
this motivated our adoption of polynomial functors for structuring interaction. We therefore
expect the compositional theory of active inference to have forwards channels rather of the form
XÑ
‚ ř
a:A Sras, so that an agent’s sensorium depends on the conﬁguration (or ‘action’) that it has
chosen.
This was the approach we sketched in our earlier work-in-progress on Polynomial Life [71],
where we suggested that polynomial functors supply a formalization of the notion of “Markov
blanket” used in the informal active inference literature to characterize the interaction boundaries
of adaptive systems [53, 234, 235]. In this way, we believe that a fruitful direction in which to
pursue a compositional theory of active inference is, like our theory of open dynamical systems, as
a Poly-algebra of statistical games. Fortunately, although this types prove somewhat diﬀerent, the
structural resemblence between active inference and economic games is maintained: in both cases,
one has lenses into the arena of interaction, in philosophical (and thus we expect also mathematical)
concordance with the categorical systems theory proposed by Myers [158].
Once again, this line of enquiry naturally leads on to the consideration of multi-agent systems.
But before we come to that, there remain important questions about single-agent systems, and the
connection between single-agent active inference and the cousin of economic games, reinforcement
learning.
7.2.2. What is the type of a plan?
Each active inference system has an internal ‘latent’ state space equipped (by its generative model)
with a ‘prior’ distribution, which represents the systems’s initial beliefs about the likelihood of
those states. As we have seen, the system can ‘perceive’, changing that distribution better to match
incoming sense data. And as we hope to see, it should also be able to ‘act’, aﬀecting its environment
so that future states better match its initial beliefs. Perception and action are thus in general the
two dual ways in which a system can minimize its free energy, akin to the two degrees of freedom
available in an autoencoder game (§6.2.1.3).
227

But a system that acts must necessarily be motivated towards some goal, even if that goal is
simply “stay alive” or “perform action a”, and even though this goal may be adjusted by the system’s
perceptions. In order to realize its goal, whatever it may be, the system must enact a plan, however
trivial—and the informal literature on active inference encodes the plan into the system’s latent
prior. When it comes to static models, the prior may be simply a (static) distribution over the state
space itself; but in the dynamical case, it is typically a distribution over trajectories of states.
Such a distribution is often [48, 50] taken to encode likelihoods of hypothetical courses of action,
which one might call a policy4: the system then perceives and acts in order to implement its policy.
But the construction of this policy may involve a lot of data, such as the speciﬁcation of goal states
and the accumulation of the “expected free energy” of trajectories in the context of those goals,
and so it seems unnecessarily crude to hide all of this data inside a single undiﬀerentiated choice of
prior distribution.
This prompts us to ask, what is the form of this data, and how can we incorporate it into the
compositional framework? In other words, what is the type of a plan? These seem to us to be key
questions for future work.
7.2.3. Reinforcement learning, open games, and ecosystems
There is known to be a close relationship between active inference in Markov decision problems
(MDPs) and reinforcement learning [236], and it is through this relationship that one sees
particularly clearly the strangeness of encoding all the data of an agent’s policy in a single ‘prior’
state. This relationship is seemingly not superﬁcial, as there are hints of a deep structural connection.
First, recall that the standard algorithm for obtaining a Bellman-optimal policy for an MDP is
backward induction (otherwise known as dynamic programming) [237, 238]5. It is now known that
backward induction is structured according a similar bidirectional pattern (the optic pattern) to
that of both Bayesian inference and reverse diﬀerentiation [142], and that MDPs themselves ﬁt into
the associated general framework of open games [141] (which are governed by the same pattern).
Second, in the informal active inference approach to MDPs, the system in question counterfactually
evaluates policies using a backward-induction-like process, accumulating free energies in order to
score them [236]. It is this process that results in the prior discussed above, which is then updated
by the agent’s inference process. Future work will need to untangle this knot of interrelated
bidirectional processes; and as usual in categorical modelling, this means ﬁrst writing them all
down precisely. We hope that, having done so, we will see how the whole picture emerges, and
how it relates to the developing geometric (or ‘diegetic’) framework in categorical cybernetics
[159] (possibly involving the further development of our notion of ‘cilia’ from §5.3). In particular,
4In the language of polynomial functors, this seems to be something like a distribution over the cofree comonad on the
system’s polynomial interface.
5Also see [19, 142, 239–241] for other presentations.
228

since the free energy principle underlying active inference asserts a certain informal universality
(on which more in §7.3.1), we might also hope that the satisfactory development of compositional
active inference might exhibit a universal property: that any other doctrine of cybernetic systems
factors uniquely through it.
The story of these connections will initially be told from the perspective of a single agent,
as backward induction only considers how to ﬁnd a single policy for a single MDP; although
this policy may involve multiple agents, the implied global search entails a common controller:
the procedure doesn’t consider the factorisation of the agents. But casting this account into the
emerging framework of compositional active inference will point towards a bridge to multi-agent
reinforcement learning. For example, multi-agent RL often studies the emergence of collaboration,
and we might expect to see this represented in the formal structure, thereby understanding how to
incorporate the factorisation of agents into the compositional framework for backward induction
(which in turn may be helpful for designing collaborative ‘edge’ AI systems).
The resulting general account of multi-agent intelligence will encompass both reinforcement
learning and active inference, allowing us to understand their relative strengths and diﬀerences.
One seeming diﬀerence (at this early stage, and following our thinking above) is that compositional
active inference envisages the latent state spaces of agents as their “internal universes”, which
come along with suﬃcient structure that we might consider them as Umwelten (i.e., their subjective
worlds, in the sense of biosemiotics; see §7.3.2 below). Consequently, we should be able to study how
agents might come to consensus, thereby resolving their disagreements. And because agents are
embodied in a shared world within which they act, this process might involve planning cooperation,
at which point the teleological structure of compositional game theory may become important, as
cooperating agents will have to bet on spatiotemporally distributed actions. We hope therefore that
one distal outcome of this work will be a new and beneﬁcial understanding of corporate activity.
Below, in §7.3, we will discuss how active inference and the free energy principle aim not only
to be theories of brains or other prominent intelligent systems, but rather universal theories of all
adaptive things. Consequently, their compositional treatment should extend in the ‘multi-agent’ case
not just to corporate activity, but to ecosystems more broadly. And, following the multicategorical
algebra latent throughout this thesis, it will undoubtedly prove natural, once we have considered a
single level of nesting of systems into ecosystems, to let the hierarchy continue to inﬁnity, producing
a fractal-like structure. At this point, we should expect once more to make contact with topics
such as higher categories and type theory, particularly in operadic or opetopic forms; synthetic
approaches to mathematical physics; and iterated parameterization in categorical systems theory.
It almost goes without saying that we should expect any framework resulting from this work to
capture existing models of collective active inference, such as recent work on spin glasses [242].
229

7.3. The mathematics of life
We move on to consider the relationships between compositional active inference and the
contemporary mathematics of life. We hope that compositional active inference may supply
part of the story of a modern theory of autopoiesis, the ability for life to recreate itself [243].
7.3.1. Bayesian mechanics and the free energy principle
Recently, it has been suggested in various venues [53, 54] that the free energy framework provides
a ‘universal’ way to understand the behaviour of adaptive systems, in the sense that, given a
random dynamical system, it may be possible to write down a generative model such that the
dynamics of the system can be modeled as performing inference on this model. In the language
of the conjectured compositional framework for active inference, we may be able to describe a
canonical statistical game that each given random dynamical system can be seen as playing.
If this is true, we should be able to express this canonicity precisely: in particular, it should
correspond to a universal property. Since approximate inference doctrines already gives us functorial
ways to turn statistical games into dynamical systems, this suggests we should seek functors that
associate to each random dynamical system a statistical game; and we should expect these functors
to be adjoint (as morphisms of indexed categories). The desired universal property would then be
expressed by the adjunction. (Notably, adjunctions are at the heart of recent synthetic approaches to
mathematical physics [244].) This would constitute an important mathematical step to establishing
the universality of the free energy principle, or to establishing the conditions that must be satisﬁed
by any satisfactory successor.
Bayesian mechanics promises to build upon the nascent understanding of random dynamics via
inference [245] to supply a new theory of mechanics for statistical systems [246]. The present
formulation of Bayesian mechanics is constructed using mathematical tools from physics, but not
(yet) the kinds of compositional tool promoted in this thesis and further described above. We expect
that developments along the lines sketched here will unify the on-going development of Bayesian
mechanics (and the resulting understanding of non-equilibirum systems) with the new synthetic
understanding of mathematical physics. By casting all dynamics as abstract inference, we should
also expect this line of enquiry to begin to quantify the persistence of things and imbue much of
physics with an élan vital.
7.3.2. Biosemiotics
It is increasingly acknowledged that biological systems are characterized not only by information-
processing, but by communication [64]: an often overlooked fact about ‘information’ in the strict
mathematical sense is that it is only meaningful in context. In the original Nyquist-Hartley-Shannon
230

conception of information, this context is the communication of a predeﬁned message over a noisy
channel [247–249]; but more generally, we might think of this context6 as simply “a question”, in
which case it is easy to see that information answering one question may not be useful in answering
another; or, in a more computational setting, we can see that the bits of an encrypted signal are
only useful in revealing the message if one has the decryption key.
Still, one often encounters descriptions of signals as containing n bits of information, without a
clear speciﬁcation of about what. Mathematically, the confusion arises because information theory
is framed by classical probability, and the assumed context is always the problem of trying to
communicate a probability distribution over a pre-deﬁned space X; and once the space is ﬁxed, the
only question that can be asked is “what is the distribution?” (Mathematically, this is to say that in
the Markov category of classical stochastic channels, there are no non-trivial eﬀects or costates.)
Yet, in the shared universe that we inhabit, there are more questions than this: in quantum
theory, for instance, one can ask many questions of the state of a system, by projecting the state
onto the subspace representing one’s question. (These projections are the non-trivial eﬀects or
costates of quantum probability.) This act of projection is an act of interpretation of the message
encoded by the state at hand.
The emerging ‘biosemiotic’ reconceptualization of life explicitly acknowledges the importance
and universality of communication in context [64], proposing that in any such situation the
interpreting system necessarily has an internal representation of the external world (its Umwelt)
which is updated by interpreting incoming signals. We can in principle reconstruct the external
world by understanding it as “that which a collection of systems agrees about”: perhaps, then,
the shared universe (as determined topos-theoretically) of a fusion of active inference agents
is a good model of this ‘semiosphere’. It seems therefore that the mathematics resulting from
our work on internal universes and their interactions — and, more broadly, many of the formal
ingredients of compositional active inference — is well aligned with the informal structures of
biosemiotics, and so it may be desirable to re-express biosemiotics accordingly. In doing so, perhaps
the mathematics for a modern Bayesian subjectivist metaphysics will be found7: for instance,
by expressing communication and its phenomenology as a geometric morphism (a generalized
base-change) between agents’ internal universes. More pragmatically, perhaps we will be able to
say precisely when some object may act as a symbol, and how systems may (learn to) manipulate
such symbols.
6This notion of context is more closely related to the notion of context of §6.1 than the topos-theoretic or type-theoretic
notion of context introduced above.
7Perhaps getting to the structure heart of the theory known as QBism [250, 251].
231

7.4. Fundamental theory
Future work connected to this thesis need not only be in applications; a number of purely theoretical
questions raise themselves, too.
7.4.1. Geometric methods for (structured) belief updating
The mathematics of ‘belief’ is in large part about replacing deﬁnite points with ‘fuzzier’ distributions
over them. In dependent type theory, we replace points with ‘terms’ (non-dependent terms are
exactly points): so a type theory with belief should somehow encompass “fuzzy terms”. Just as we
can replace points with distributions, we can replace dependent points with dependent distributions.
However, the standard replacement (moving from a category of functions to a category of stochastic
channels) obscures some of the ‘universal’ categorical structure that underpins the rules of type
theory. This standard replacement also misses something else: while it does allow for fuzzy terms,
it omits a model of fuzzy types; and we might well want to express beliefs about things whose
identity we are not quite sure. (This omission also seems to be related to the loss of universal
structure.)
There seem to be a couple of related resolutions to this puzzle. The ﬁrst is to notice that replacing
points by distributions yields another space: the space of distributions over the original space; this
is akin to the move in dynamics from working with the random motion of states to working with
the deterministic motion of the distribution over states. This space of distributions has a particular
geometry (its information geometry), and hence we should expect corresponding ﬂavours of topos
and type theory. As we have indicated above, there is a move in fundamental mathematical physics
(cf. Schreiber [244]) to work ‘synthetically’, expressing concepts using the universal structures
of higher topoi. This has proven particularly fruitful in the context of diﬀerential systems, but
it is interesting that stochastic and diﬀerential structures bear a number of similarities8: what
are we to make of this? Does Bayesian inversion induce a canonical geometric morphism, by
which structured models may be coherently updated? We have already indicated above signs of a
relationship between inference and parallel transport.
The second resolution is to work with topoi as we work with random dynamical systems, by
noticing that randomness is often like “an uncertain parameterization”. By parameterizing a topos
with a category of noise sources, we may obtain a notion of “stochastic topos” in which the standard
8Both conditional probability and diﬀerential calculus exhibit “chain rules” of similar types, which give rise to backwards
actions that compose via the lens rule: in the former case, Bayesian inversion; in the latter, reverse diﬀerentiation.
Categories that admit a diﬀerentiation operation have begun to be axiomatized (as diﬀerential categories [252]
and reverse-derivative categories [253]), and categories whose morphisms behave like stochastic channels are also
presently being axiomatized (in the framework of Markov categories [126]), but the connections between these
various formalisms are not yet clear. The similar structures indicate that the two families of axiomatisation may
have a common generalization.
232

operations of dependent type theory are available, but where each type and term may depend on
the realization of the noise source, thereby giving us notions of fuzzy term and fuzzy type. The
mathematics of such uncertainly parameterized topoi is as yet undeveloped, although we expect
that they should bear a relationship to the “topoi of beliefs” of the foregoing ﬁrst resolution similar
to the relationship of Fokker-Planck to random dynamical systems.
Finally, we note that higher topoi behave abstractly somewhat like vector spaces (with sheaves like
categoriﬁed functionals). Since distributions are themselves like vectors, perhaps this observation
is a ﬁrst step towards relating the resolutions.
7.4.2. Dynamics
Chapter 5 has supplied the beginnings of a compositional coalgebraic theory for open stochastic
and random dynamical systems in general time, and we hope that this theory could provide a
home for a modern account of non-equilibrium systems, with the category of polynomial functors
supplying a satisfactory account of these systems’ interfaces (i.e., the boundaries across which
information ﬂows, along which they compose, and through which they interact).
In this context, and in parallel to the abstract questions above, there are similar questions to be
asked speciﬁcally of dynamical systems. For instance, what is the precise relationship between
the category of Markov processes on an interface, and the category of random dynamical systems
on that interface? We know that categories of deterministic discrete-time polynomial coalgebras
are topoi [155], so does the same hold in general time? To what extent is the logic of our systems
related to coalgebraic logics [148–150, 152, 254]?
Besides these ‘parallel’ questions, there are a number of more technical ones. For instance, our
current deﬁnition of “Markov process on a polynomial interface” is somewhat inelegant, and we
seek to simplify it. Similarly, we believe that there is a better deﬁnition of “random dynamical
system on a polynomial interface” that may be obtained by a (diﬀerent) generalization of the
category of polynomial functors, using random variables. And we know that a topology for the
cofree comonoid on an interface can be generated by the corresponding free monoid, which may
be relevant for understanding the topological structure of open systems. An important set of open
questions about open random dynamical systems in this framework comes from attempting to
import notions about random systems from the classical ‘closed’ setting: fundamentally, we ask,
does this framework indeed supply a satisfactory setting in which to understand stochastic systems
away from equilibrium?
7.4.3. Computation
The early 21st century understanding of biological systems as information-processing involves
treating them as computational, but remarkably lacks a precise concept of what it means for a
233

system to compute, other than in the context of artiﬁcial machines. To us, it seems that a crisper
understanding of computation in general might begin with the slogan that “computation is dynamics
plus semantics”, which is philosophically aligned with the semiotic understanding of biological
information-processing sketched above: for example, we know that attractor networks in the brain
can informally be understood as computational [255], but these are ‘continuous’ systems for which
we do not yet have a good corresponding concept of algorithm (and it is upon algorithms that our
current understanding is built). But what more is an algorithm than a description of a discrete-time
open dynamical system? The quality that makes an algorithm computational is that its states
or its outputs correspond to some quantity of interest, and that it reaches a ﬁxed point (it halts)
at the target quantity when the computation is complete. If this intuition is correct, then a new
understanding of computation may follow the semiotic understanding of information-processing
that we propose above: perhaps we could say more precisely that computation is the dynamics of
semiosis? The time is right for such a reconceptualization, as human-made systems increasingly
move away from von Neumann architectures towards more biosimilar ones (such as memristors,
optical processors, neuromorphic technology, graph processors, or even many-core and mesh-based
evolutions of classical processors).
234

A. Auxiliary material
A.1. From monads to operads
The assignment of domain and codomain to the morphisms of a small category C constitutes a
pair of functions C1 Ñ C0, which we can write as a span, C0
cod
ÐÝÝ C1
dom
ÝÝÑ C0. Similarly, the
assignment of domain and codomain to the morphisms of a multicategory M constitutes a span
M0
cod
ÐÝÝ M1
dom
ÝÝÑ ListpM0q. This observation was used by Leinster [96] to construct a general
framework for constructing multicategories, replacing List with an arbitrary ‘Cartesian’ monad T,
which opens the way to a connection between monad algebras and multicategory algebras. In this
section, we explore this connection, starting by deﬁning categories of spans.
Deﬁnition A.1.1. Suppose A and B are two objects in a category C. We will write a span from A
to B as pX, xq : A
xA
ÐÝÝ X
xB
ÝÝÑ B, and call X the apex of the span and xA, xB its legs or projections.
The category of spans from A to B, denoted SpanpA, Bq has spans pX, xq as objects, and the
morphisms f : pX, xq Ñ pX1, x1q are morphisms f : X Ñ X1 in C that commute with the spans,
as in the following diagram:
X
A
B
X1
xA
x1
A
xB
x1
B
f
We can treat the categories SpanpA, Bq as the hom categories of a bicategory.
Deﬁnition A.1.2. Suppose C is a category with all pullbacks. The bicategory of spans in C, denoted
Span, has for objects the objects of C, and for hom-categories the categories SpanpA, Bq of spans
from A to B. Given spans pX, xq : A Ñ B and pY, yq : B Ñ C, their horizontal composite
pY, yq ˝ pX, xq : A Ñ C is the pullback span deﬁned by
X ˆB Y
X
Y
A
B
C
projX
xA
projY
yC
xB
yB
{
.
235

If pX1, x1q : A Ñ B and pY 1, y1q : B Ñ C are also spans with f : pX, xq ñ pX1, x1q and
g : pY, yq ñ pY 1, y1q vertical morphisms, the horizontal composite of f and g is also deﬁned by
pullback as f ˆB y : pY, yq ˝ pX, xq ñ pY 1, y1q ˝ pX1, x1q. The identity span on an object A is
pA, idq : A ù A ù A.
If the ambient category C is not clear from the context, we will write SpanC to denote the
bicategory of spans in C.
Remark A.1.3. Note that Span really is a bicategory rather than a 2-category: since the horizontal
composition of spans is deﬁned by pullback, it is only deﬁned up to isomorphism. Consequently,
the composition of spans can in general only be associative and unital up to isomorphism, rather
than the strict equality required by a 2-category.
Now, recall that ‘monad’ is another name for “monoid in a bicategory”, where the bicategory has
so far been taken to be Cat: but it need not be.
Remark A.1.4. Since CC is the endomorphism monoid on C in the bicategory Cat, we can
generalize the preceding deﬁnition of monad to any bicategory B: a monad in a bicategory B is
simply a monoid in B, as deﬁned in Remark 3.4.9. That is, a monad in B is a monoid object in
the monoidal category
`
Bpb, bq, ˝, idb
˘
for some choice of 0-cell b : B, where ˝ denotes horizontal
composition. Explicitly, a monad pt, µ, ηq in B is a 1-cell t : b Ñ b, a multiplication 2-cell µ : t˝t ñ t,
and a unit 2-cell η : idb ñ t, such that the associativity and unitality diagrams commute in Bpb, bq:
ttt
tt
tt
t
µt
tµ
µ
µ
and
t
tt
t
t
µ
ηt
tη
With this more general notion of monad, we obtain another monadic deﬁnition of “small category”,
to add to the explicit Deﬁnition 2.1.2 and the monad-algebraic Example 3.4.19.
Proposition A.1.5. Small categories are monads in SpanSet.
Proof. A monad in SpanSet is a choice of object C0 and monoid in SpanSetpC0, C0q. Such a
monoid is a span of sets C : C0
cod
ÐÝÝ C1
dom
ÝÝÑ C0 along with functions ‚ : C1 ˆC0 C1 Ñ C1 and
id : C0 Ñ C1. The set C1 ˆC0 C1 is the apex of the pullback span C ˝ C as in
C1 ˆC0 C1
C1
C1
C0
C0
C0
cod
dom
dom
cod
{
236

so that ‚ and id make the following diagrams commute:
C1 ˆC0 C1
C1
C1
C0
C1
C0
cod
dom
cod
dom
‚
and
C0
C0
C1
C0
id
cod
dom
This means that codpg ‚ fq “ codpgq and dompg ‚ fq “ dompfq, and codpidxq “ dompidxq “ x.
It is easy to check that pC, ‚, idq therefore constitutes the data of a small category; moreover, the
functions ‚ and id must satisfy the monoid axioms of associativity and (right and left) unitality,
which correspond directly to the categorical axioms of associativity and unitality.
As we indicated at the opening of this section, by generalizing to a category of ‘spans’ of the
form A Ð X Ñ TB, we can use the preceding result to produce generalized multicategories
whose morphisms have domains “in the shape of T”. Since the horizontal composition of spans is
by pullback, we need an extra condition on the monad T to ensure that pullbacks of T-spans are
well-deﬁned. This condition is known as ‘Cartesianness’.
Deﬁnition A.1.6. A Cartesian natural transformation between functors F and G is a natural
transformation α : F ñ G for which every naturality square is a pullback:
Fa
Ga
Fb
Gb
αa
Gf
Ff
αb
{
A Cartesian monad is a monad pT : C Ñ C, µ, ηq such that C has all pullbacks, T preserves
these pullbacks (sending pullback squares to pullback squares), and µ and η are Cartesian natural
transformations.
Deﬁnition A.1.7. Suppose T is a monad on C. A T-span from A to B is a span from A to TB in
C. The category of T-spans from A to B, denoted SpanT pA, Bq has T-spans as objects, and the
morphisms f : pX, xq Ñ pX1, x1q are morphisms f : X Ñ X1 in C that commute with the spans,
as in the diagram
X
A
TB
X1
xA
x1
A
xB
x1
B
f
.
237

Deﬁnition A.1.8. Suppose pT, µ, ηq is a Cartesian monad on C. The bicategory of T-spans
in C, denoted SpanT , has for objects the objects of C, and for hom-categories the categories
SpanT pA, Bq of T-spans from A to B. Given T-spans pX, xq : A Ñ B and pY, yq : B Ñ C, their
horizontal composite pY, yq ˝ pX, xq : A Ñ C is the outer T-span in the diagram
X ˆTB TY
X
TY
A
TB
TTC
TC
projX
xA
projT Y
TyC
xB
TyB
{
µC
.
If pX1, x1q : A Ñ B and pY 1, y1q : B Ñ C are also T-spans with f : pX, xq ñ pX1, x1q and
g : pY, yq ñ pY 1, y1q vertical morphisms, the horizontal composite of f and g is deﬁned as
f ˆTB Tg accordingly. The identity span on an object A is A
idA
ÐÝÝ A
ηA
ÝÑ TA.
With these notions to hand, the general concept of T-multicategory is easy to deﬁne.
Deﬁnition A.1.9 (Leinster [96, Def. 4.2.2]). Suppose T is a Cartesian monad on C. A T-
multicategory is a monad in the bicategory SpanT of T-spans.
And of course we can recover our earlier examples of category shapes accordingly.
Example A.1.10. The identity monad on a category with all pullbacks is trivially a Cartesian
monad. Therefore, taking T “ idSet to be the identity monad on Set, we immediately see that an
idSet-multicategory is a small category.
Example A.1.11 (Leinster [96, Examples 4.1.4 and 4.2.7]). The free monoid monad List : Set Ñ
Set is Cartesian. Unpacking the deﬁnitions, we ﬁnd that a List-multicategory is precisely a
multicategory as in Deﬁnition 3.3.1.
At this point, we can sketch how multicategory algebras correspond to monad algebras, referring
the reader to Leinster [96, §4.3] for the details. The basic picture is that, if T : C Ñ C is a Cartesian
monad and M is a T-multicategory, then one can obtain functorially a monad TM on the slice
C{M0 of C over the object M0 of M-objects. The algebras α : TMpX, pq Ñ pX, pq of this monad
are morphisms α : TMX Ñ X as in the commuting diagram
TMX
X
TX
M1
TM0
M0
p
cod
dom
Tp
{
α
238

where TMpX, pq is deﬁned as the bundle TMX Ñ M0 on the right leg of the pullback square.
To get a sense for how this works, consider the case where T “ idSet: a T-multicategory is then
simply a small category C, and as Leinster [96, Example 4.3.2] shows, its algebras are functors
C Ñ Set.
A.2. 2-local contexts, graphically
To clarify the idea that the 2-local contexts for the factors of a tensor product game (or morphism
more generally) are obtained by “ﬁlling the hole” on the left or right of the tensor, one can work in
the monoidal bicategory of V-profunctors and use the associated graphical calculus [138] to depict
the ‘hole’ in the context and its ﬁller. In this section, we work with a general monoidal category C,
which may or may not be a category of lenses or games. Nonetheless, the basic idea is the same:
a (complex) context for a morphism X Ñ Y is given by a triple of a residual denoted Θ, a state
I Ñ Θ b X and a ‘continuation’ (or ‘eﬀect’) Θ b Y Ñ I, coupled according to the coend quotient
rule.
Below, we show how to obtain the object of right local 2-contexts for a tensor product morphism
f b f1 : X b X1 Ñ Y b Y 1, using the graphical calculus of V-Prof. At each stage, we depict on
the left the object named on the right. We start with a complex context for the tensor along with a
‘ﬁller’ object of morphisms X1 Ñ Y 1, which is shown “ﬁlling the (left-hand) hole”. In the ﬁrst step,
we use the composition rule of C to connect the matching ‘ports’ on the domain X. We then couple
the matching Y port using the coend and gather Θ and Y together into a single residual. Note
that these steps correspond directly to factors in the deﬁnition of πf1 : Ctxpf b f1q Ñ Ctxpf1q
(Deﬁnition 6.1.16).
239

I
X
X1
Y
X
Y
Y 1
I
b
b
ż Θ:C
CpI, Θ b X b X1q ˆ CpX, Y q ˆ CpΘ b Y b Y 1, Iq
ÝÑ
I
X1
Y
Y
Y 1
I
b
b
ż Θ:C
CpI, Θ b Y b X1q ˆ CpΘ b Y b Y 1, Iq
ãÝÑ
I
X1
Y 1
I
b
b
ż Θ1:C
CpI, Θ1 b X1q ˆ CpΘ1 b Y 1, Iq
We ﬁnd this graphical representation to be a useful aid in comprehension, and often simpliﬁes
the symbolic ‘book-keeping’ that can complicate expressions such as those in Deﬁnition 6.1.16.
The cost of this expressivity is the introduction of another categorical structure, and its associated
cognitive load. In previous work [70], we have made more use of this representation: there, we
worked with the ‘optical’ deﬁnition of Bayesian lenses described in St. Clere Smithe [81]; and
we note that an earlier informal version of this graphical language was originally used to deﬁne
local contexts for tensor product games in the compositional game theory literature [141]. Since
the V-profunctorial setting [153] is required in order to deﬁne optics, we had already paid this
extra cognitive cost. In this paper, however, we have preferred to stick with the simpler ﬁbrational
deﬁnition of Bayesian lenses.
Remark A.2.1. A diﬀerent but related graphical calculus for optics is described by Boisseau [135].
However, this alternative calculus is somewhat less general than that of Román [138], and its
adoption here would not eliminate the extra cognitive cost; we do nonetheless make use of it in
[81].
240

B. Bibliography
[1]
Open Science Collaboration. “Estimating the Reproducibility of Psychological Science”.
In: Science 349.6251 (08/28/2015), aac4716. issn: 0036-8075, 1095-9203. doi: 10.1126/
science.aac4716. url: https://www.science.org/doi/10.1126/
science.aac4716 (visited on 12/23/2022).
[2]
John P. A. Ioannidis. “Contradicted and Initially Stronger Eﬀects in Highly Cited Clinical
Research”. In: JAMA 294.2 (07/13/2005), p. 218. issn: 0098-7484. doi: 10.1001/jama.
294.2.218. url: http://jama.jamanetwork.com/article.aspx?
doi=10.1001/jama.294.2.218 (visited on 12/23/2022).
[3]
C. Glenn Begley and Lee M. Ellis. “Raise Standards for Preclinical Cancer Research”. In:
Nature 483.7391 (03/2012), pp. 531–533. doi: 10.1038/483531a.
[4]
Aaron Mobley et al. “A Survey on Data Reproducibility in Cancer Research Provides
Insights into Our Limited Ability to Translate Findings from the Laboratory to the Clinic”.
In: PLoS ONE 8.5 (05/15/2013). Ed. by Hirofumi Arakawa, e63221. issn: 1932-6203. doi:
10.1371/journal.pone.0063221. url: https://dx.plos.org/10.
1371/journal.pone.0063221 (visited on 12/23/2022).
[5]
Matthew Hutson. “Artiﬁcial Intelligence Faces Reproducibility Crisis”. In: Science (New
York, N.Y.) 359.6377 (02/2018), pp. 725–726. issn: 0036-8075, 1095-9203. doi: 10.1126/
science.359.6377.725. url: https://www.science.org/doi/10.
1126/science.359.6377.725.
[6]
Sayash Kapoor and Arvind Narayanan. Leakage and the Reproducibility Crisis in ML-based
Science. 07/14/2022. arXiv: 2207.07048 [cs, stat]. url: http://arxiv.org/
abs/2207.07048 (visited on 12/23/2022).
[7]
Benjamin C. Pierce. Basic Category Theory for Computer Scientists. MIT Press, 1991.
[8]
John C. Baez and James Dolan. “Higher-dimensional Algebra and Topological Quantum
Field Theory”. In: Journal of Mathematical Physics 36.11 (11/1995), pp. 6073–6105. issn:
0022-2488, 1089-7658. doi: 10.1063/1.531236. url: http://aip.scitation.
org/doi/10.1063/1.531236 (visited on 12/23/2022).
241

[9]
Samson Abramsky and Bob Coecke. “A Categorical Semantics of Quantum Protocols”. In:
Logic in Computer Science, 2004. Proceedings of the 19th Annual IEEE Symposium On. IEEE.
2004, pp. 415–425.
[10]
Bob Coecke and Aleks Kissinger. “Categorical Quantum Mechanics I: Causal Quantum
Processes”. 10/19/2015. arXiv: 1510.05468v3 [quant-ph].
[11]
Bob Coecke and Aleks Kissinger. “Categorical Quantum Mechanics II: Classical-Quantum
Interaction”. 05/27/2016. arXiv: 1605.08617v1 [quant-ph].
[12]
Robert Rosen. “The Representation of Biological Systems from the Standpoint of the Theory
of Categories”. In: The Bulletin of Mathematical Biophysics 20.4 (12/1958), pp. 317–341.
issn: 0007-4985, 1522-9602. doi: 10.1007/BF02477890. url: http://link.
springer.com/10.1007/BF02477890 (visited on 12/23/2022).
[13]
Andrée C. Ehresmann and Jean-Paul Vanbremeersch. “The Memory Evolutive Systems as
a Model of Rosen’s Organisms – (Metabolic, Replication) Systems”. In: Axiomathes 16.1-2
(03/2006), pp. 137–154. issn: 1572-8390. doi: 10.1007/s10516-005-6001-0. url:
http://link.springer.com/10.1007/s10516-005-6001-0.
[14]
Brendan Fong. “Causal Theories: A Categorical Perspective on Bayesian Networks”.
University of Oxford, 2013-01-26, 2013. arXiv: 1301.6201v1 [math.PR].
[15]
Brendan Fong. “Decorated Cospans”. In: Theory and Applications of Categories 30.33 (2015),
pp. 1096–1120.
[16]
Brendan Fong. “The Algebra of Open and Interconnected Systems”. University of Oxford,
2016.
[17]
Neil Ghani et al. “Compositional Game Theory”. In: Proceedings of Logic in Computer Science
(LiCS) 2018 (2016). arXiv: 1603.04641 [cs.GT].
[18]
Samson Abramsky and Viktor Winschel. “Coalgebraic Analysis of Subgame-perfect
Equilibria in Inﬁnite Games without Discounting”. 10/16/2012. doi: 10 . 1017 /
S0960129515000365. arXiv: 1210.4537 [cs.GT].
[19]
Martín Escardó and Paulo Oliva. “Selection Functions, Bar Recursion and Backward
Induction”. In: Mathematical Structures in Computer Science 20.2 (03/2010), pp. 127–168. doi:
10.1017/s0960129509990351.
[20]
Steven Phillips and William H. Wilson. “Categorial Compositionality: A Category Theory
Explanation for the Systematicity of Human Cognition”. In: PLoS Computational Biology 6.7
(07/2010). Ed. by Karl J. Friston, e1000858. doi: 10.1371/journal.pcbi.1000858.
242

[21]
A.C. Ehresmann and J.P. Vanbremeersch. Memory Evolutive Systems; Hierarchy, Emergence,
Cognition. Studies in Multidisciplinarity. Elsevier Science, 2007. isbn: 978-0-08-055541-6.
url: https://books.google.co.uk/books?id=OqcYQbY79GMC.
[22]
Yaared Al-Mehairi, Bob Coecke, and Martha Lewis. “Compositional Distributional Cogni-
tion”. In: International Symposium on Quantum Interaction. Springer, 08/12/2016, pp. 122–134.
doi: 10.1007/978-3-319-52289-0_10. arXiv: 1608.03785 [cs.AI].
[23]
Yaared Al-Mehairi, Bob Coecke, and Martha Lewis. “Categorical Compositional Cognition”.
In: Lecture Notes in Computer Science (2017), pp. 122–134. issn: 1611-3349. doi: 10.1007/
978-3-319-52289-0_10.
[24]
Joe Bolt et al. “Interacting Conceptual Spaces I: Grammatical Composition of Concepts”. In:
Springer, 2019, 2017, pp. 151–181. arXiv: 1703.08314.
[25]
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. “Mathematical Foundations for a
Compositional Distributional Model of Meaning”. In: Lambek Festschirft, special issue of
Linguistic Analysis, 2010. (03/2010). eprint: 1003.4394. url: https://arxiv.org/
abs/1003.4394.
[26]
Chris Heunen, Mehrnoosh Sadrzadeh, and Edward Grefenstette, eds. Quantum Physics and
Linguistics: A Compositional, Diagrammatic Discourse. Oxford University Press, 02/2013.
isbn: 978-0-19-964629-6. url: http://ukcatalogue.oup.com/product/
9780199646296.do#.UPAFH4mLLqp.
[27]
Bob Coecke. “The Mathematics of Text Structure”. 2019. arXiv: 1904.03478.
[28]
Samson Abramsky et al. “Categorical Methods at the Crossroads (Dagstuhl Perspectives
Workshop 14182)”. In: (2014). In collab. with Marc Herbstritt, 15 pages. doi: 10.4230/
DAGREP.4.4.49. url: http://drops.dagstuhl.de/opus/volltexte/
2014/4618/ (visited on 12/23/2022).
[29]
Samuel Eilenberg and Saunders MacLane. “General Theory of Natural Equivalences”. In:
Transactions of the American Mathematical Society 58.0 (1945), pp. 231–294. issn: 0002-9947,
1088-6850. doi: 10 . 1090 / S0002 - 9947 - 1945 - 0013131 - 6. url: https :
//www.ams.org/tran/1945-058-00/S0002-9947-1945-0013131-
6/ (visited on 12/23/2022).
[30]
Henri Cartan. “Variétés Analytiques Complexes et Cohomologie”. In: Colloque Sur Les
Fonctions de Plusieurs Variables Tenu a Bruxelles. 1953. url: https://www.inp.
nsk.su/~silagadz/Cartan.pdf.
[31]
Henri Paul Cartan and Samuel Eilenberg. Homological Algebra. Princeton Landmarks in
Mathematics and Physics. Princeton, N.J: Princeton University Press, 1956. 390 pp. isbn:
978-0-691-04991-5.
243

[32]
Alexander Grothendieck. “Sur Quelques Points d’algèbre Homologique”. In: Tohoku
Mathematical Journal 9.2 (01/01/1957). issn: 0040-8735. doi: 10 . 2748 / tmj /
1178244839. url: https://projecteuclid.org/journals/tohoku-
mathematical - journal / volume - 9 / issue - 2 / Sur - quelques -
points - dalg % c3 % a8bre - homologique - I / 10 . 2748 / tmj /
1178244839.full (visited on 12/23/2022).
[33]
F. William Lawvere. “Functorial Semantics of Algebraic Theories”. In: Proceedings of the
National Academy of Sciences 50.5 (11/1963), pp. 869–872. issn: 0027-8424, 1091-6490. doi:
10.1073/pnas.50.5.869. url: https://pnas.org/doi/full/10.
1073/pnas.50.5.869 (visited on 12/23/2022).
[34]
F. William Lawvere. “An Elementary Theory of the Category of Sets”. In: Proceedings of
the National Academy of Sciences 52.6 (12/1964), pp. 1506–1511. issn: 0027-8424, 1091-6490.
doi: 10.1073/pnas.52.6.1506. url: https://pnas.org/doi/full/10.
1073/pnas.52.6.1506 (visited on 12/23/2022).
[35]
A. M. Bastos et al. “Canonical Microcircuits for Predictive Coding”. In: Neuron 76.4 (11/2012),
pp. 695–711. doi: 10.1016/j.neuron.2012.10.038. pmid: 23177956.
[36]
James C. R. Whittington and Rafal Bogacz. “An Approximation of the Error Backpropagation
Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity”. In:
Neural Computation 29.5 (05/01/2017), pp. 1229–1262. issn: 0899-7667, 1530-888X. doi:
10 . 1162 / NECO _ a _ 00949. url: https : / / direct . mit . edu / neco /
article/29/5/1229/8261/An- Approximation- of- the- Error-
Backpropagation (visited on 12/23/2022).
[37]
Beren Millidge et al. Predictive Coding: Towards a Future of Deep Learning beyond Backprop-
agation? 02/18/2022. arXiv: 2202.09467 [cs]. url: http://arxiv.org/abs/
2202.09467 (visited on 12/23/2022).
[38]
Robert Rosenbaum. “On the Relationship between Predictive Coding and Backpropagation”.
In: PLOS ONE 17.3 (03/31/2022). Ed. by Gennady S. Cymbalyuk, e0266102. issn: 1932-6203.
doi: 10.1371/journal.pone.0266102. url: https://dx.plos.org/10.
1371/journal.pone.0266102 (visited on 12/23/2022).
[39]
R. P. Rao and D. H. Ballard. “Predictive Coding in the Visual Cortex: A Functional
Interpretation of Some Extra-Classical Receptive-Field Eﬀects”. In: Nature Neuroscience 2.1
(01/1999), pp. 79–87. doi: 10.1038/4580. pmid: 10195184.
[40]
Mandyam Veerambudi Srinivasan, Simon Barry Laughlin, and A Dubs. “Predictive Coding:
A Fresh View of Inhibition in the Retina”. In: Proceedings of the Royal Society of London. Series
B. Biological Sciences 216.1205 (11/22/1982), pp. 427–459. issn: 0080-4649, 2053-9193. doi:
10.1098/rspb.1982.0085. url: https://royalsocietypublishing.
org/doi/10.1098/rspb.1982.0085 (visited on 12/23/2022).
244

[41]
M Boerlin et al. “Predictive Coding of Dynamical Variables in Balanced Spiking Networks”.
In: PLoS computational biology 9.11 (2013), e1003258. url: http://journals.plos.
org/ploscompbiol/article?id=10.1371/journal.pcbi.1003258.
[42]
David Marr. Vision: A Computational Investigation into the Human Representation and
Processing of Visual Information. Henry Holt and Co, 1982.
[43]
Tomaso Poggio and Thomas Serre. “Models of Visual Cortex”. In: Scholarpedia 8.4 (2013),
p. 3516. issn: 1941-6016. doi: 10.4249/scholarpedia.3516. url: http://
www.scholarpedia.org/article/Models_of_visual_cortex (visited
on 12/23/2022).
[44]
Karl Friston and Stefan Kiebel. “Predictive Coding under the Free-Energy Principle”.
In: Philosophical Transactions of the Royal Society B: Biological Sciences 364.1521 (2009),
pp. 1211–1221. doi: 10 . 1098 / rstb . 2008 . 0300. url: http : / / m . rstb .
royalsocietypublishing.org/content/364/1521/1211.
[45]
K. Friston. “A Theory of Cortical Responses”. In: Philos. Trans. R. Soc. Lond., B, Biol.
Sci. 360.1456 (04/2005), pp. 815–836. doi: 10 . 1098 / rstb . 2005 . 1622. pmid:
15937014.
[46]
Karl J. Friston and Klaas E. Stephan. “Free-Energy and the Brain”. In: Synthese. An
International Journal for Epistemology, Methodology and Philosophy of Science 159.3 (09/2007),
pp. 417–458. doi: 10.1007/s11229-007-9237-y. url: http://dx.doi.
org/10.1007/s11229-007-9237-y.
[47]
Christopher L Buckley et al. “The Free Energy Principle for Action and Perception: A
Mathematical Review”. In: Journal of Mathematical Psychology 81 (05/24/2017), pp. 55–79.
arXiv: 1705.09156v1 [q-bio.NC].
[48]
Lancelot Da Costa et al. “Active Inference on Discrete State-Spaces: A Synthesis”. 01/20/2020.
arXiv: 2001.07203 [q-bio.NC].
[49]
Alexander Tschantz, Anil K. Seth, and Christopher L. Buckley. “Learning Action-Oriented
Models through Active Inference”. In: (09/2019). doi: 10.1101/764969.
[50]
Raphael Kaplan and Karl J. Friston. “Planning and Navigation as Active Inference”. In:
Biological Cybernetics 112.4 (03/2018), pp. 323–343. doi: 10.1007/s00422- 018-
0753-2.
[51]
Kai Ueltzhöﬀer. “Deep Active Inference”. In: Biological Cybernetics 112.6 (2017-09-07, 2018-
10), pp. 547–573. doi: 10.1007/s00422-018-0785-7. arXiv: 1709.02341
[q-bio.NC].
245

[52]
Karl J. Friston et al. “Action and Behavior: A Free-Energy Formulation”. In: Biological
Cybernetics 102.3 (02/2010), pp. 227–260. doi: 10.1007/s00422-010-0364-z.
[53]
Karl Friston. “A Free Energy Principle for a Particular Physics”. 06/24/2019. arXiv: 1906.
10184v1 [q-bio.NC].
[54]
Thomas Parr, Lancelot Da Costa, and Karl Friston. “Markov Blankets, Information Geometry
and Stochastic Thermodynamics”. In: Philosophical Transactions of the Royal Society A:
Mathematical, Physical and Engineering Sciences 378.2164 (12/2019), p. 20190159. doi: 10.
1098/rsta.2019.0159.
[55]
Michael D. Kirchhoﬀ. “Autopoiesis, Free Energy, and the Life–Mind Continuity Thesis”. In:
Synthese 195.6 (2018), pp. 2519–2540. issn: 0039-7857, 1573-0964. doi: 10.1007/s11229-
016- 1100- 6. url: http://link.springer.com/10.1007/s11229-
016-1100-6 (visited on 12/23/2022).
[56]
Evert A. Boonstra and Heleen A. Slagter. “The Dialectics of Free Energy Minimization”. In:
Frontiers in Systems Neuroscience 13 (09/10/2019), p. 42. issn: 1662-5137. doi: 10.3389/
fnsys.2019.00042. url: https://www.frontiersin.org/article/
10.3389/fnsys.2019.00042/full (visited on 12/23/2022).
[57]
David C Knill and Alexandre Pouget. “The Bayesian Brain: The Role of Uncertainty in
Neural Coding and Computation”. In: TRENDS in Neurosciences 27.12 (2004), pp. 712–719.
doi: 10.1016/j.tins.2004.10.007. url: http://www.sciencedirect.
com/science/article/pii/S0166223604003352.
[58]
Martin Biehl, Felix A. Pollock, and Ryota Kanai. “A Technical Critique of the Free Energy
Principle as Presented in "Life as We Know It" and Related Works”. 01/12/2020. arXiv:
2001.06408v2 [q-bio.NC].
[59]
Karl Friston, Lancelot Da Costa, and Thomas Parr. “Some Interesting Observations on the
Free Energy Principle”. 02/05/2020. arXiv: 2002.04501v1 [q-bio.NC].
[60]
Matteo Colombo and Cory Wright. “First Principles in the Life Sciences: The Free-Energy
Principle, Organicism, and Mechanism”. In: Synthese 198.S14 (06/2021), pp. 3463–3488.
issn: 0039-7857, 1573-0964. doi: 10.1007/s11229-018-01932-w. url: https:
//link.springer.com/10.1007/s11229- 018- 01932- w (visited on
12/23/2022).
[61]
Daniel Williams. “Is the Brain an Organ for Free Energy Minimisation?” In: Philosophical
Studies 179.5 (05/2022), pp. 1693–1714. issn: 0031-8116, 1573-0883. doi: 10 . 1007 /
s11098-021-01722-0. url: https://link.springer.com/10.1007/
s11098-021-01722-0 (visited on 12/23/2022).
246

[62]
Christopher A. Fuchs and Ruediger Schack. “A Quantum-Bayesian Route to Quantum-State
Space”. 12/2009. doi: 10 . 1007 / s10701 - 009 - 9404 - 8. arXiv: 0912 . 4252
[quant-ph].
[63]
Maria Carla Galavotti. “Subjectivism, Objectivism and Objectivity in Bruno de Finetti’s
Bayesianism”. In: Foundations of Bayesianism. Ed. by David Corﬁeld and Jon Williamson.
Red. by Dov M. Gabbay and Jon Barwise. Vol. 24. Applied Logic Series. Dordrecht: Springer
Netherlands, 2001, pp. 161–174. isbn: 978-90-481-5920-8 978-94-017-1586-7. doi: 10 .
1007/978- 94- 017- 1586- 7_7. url: http://link.springer.com/
10.1007/978-94-017-1586-7_7 (visited on 12/23/2022).
[64]
Marcello Barbieri, ed. Introduction to Biosemiotics The New Biological Synthesis. The New
Biological Synthesis. Springer, 2007. isbn: 978-1-4020-4814-2.
[65]
James C. R. Whittington et al. “The Tolman-Eichenbaum Machine: Unifying Space and
Relational Memory through Generalisation in the Hippocampal Formation”. In: (09/2019).
doi: 10.1101/770495. eprint: https://www.biorxiv.org/content/
biorxiv/early/2019/09/24/770495.full-text.pdf.
[66]
Kimberly Lauren Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. “The
Hippocampus as a Predictive Map”. In: (12/2016). doi: 10.1101/097170. url: http:
//dx.doi.org/10.1101/097170.
[67]
Beren Millidge and Christopher L. Buckley. Successor Representation Active Inference.
07/20/2022. arXiv: 2207.09897 [cs]. url: http://arxiv.org/abs/2207.
09897 (visited on 12/23/2022).
[68]
A Ehresmann and J Vanbremeersch. “Hierarchical Evolutive Systems: A Mathematical
Model for Complex Systems”. In: Bulletin of Mathematical Biology 49.1 (1987), pp. 13–50.
issn: 00928240. doi: 10.1016/S0092-8240(87)80033-2. url: http://link.
springer.com/10.1016/S0092-8240(87)80033-2 (visited on 12/23/2022).
[69]
Micah Halter et al. Compositional Scientiﬁc Computing with Catlab and SemanticModels.
06/29/2020. arXiv: 2005.04831 [cs, math]. url: http://arxiv.org/abs/
2005.04831 (visited on 12/23/2022).
[70]
Toby St. Clere Smithe. “Cyber Kittens, or Some First Steps Towards Categorical Cybernetics”.
In: Proceedings 3rd Annual International Applied Category Theory Conference 2020 (ACT 2020).
2020.
[71]
Toby St. Clere Smithe. “Polynomial Life: The Structure of Adaptive Systems”. In: Fourth
International Conference on Applied Category Theory (ACT 2021). Ed. by K. Kishida.
Vol. EPTCS 370. 2021, pp. 133–147. doi: 10.4204/EPTCS.370.28.
247

[72]
Toby St Clere Smithe. “Open Dynamical Systems as Coalgebras for Polynomial Func-
tors, with Application to Predictive Processing”. 06/08/2022. arXiv: 2206 . 03868
[math.CT].
[73]
Toby B. St. Clere Smithe. “Radically Compositional Cognitive Concepts”. 11/14/2019. arXiv:
1911.06602 [q-bio.NC].
[74]
Toby St Clere Smithe. “Compositional Active Inference”. In: Finding the Right Abstractions.
Topos Institute, 05/12/2021.
[75]
Britt Anderson et al. “Category Theory for Cognitive Science”. In: Proceedings of the Annual
Meeting of the Cognitive Science Society. Vol. 44. 2022.
[76]
Toby St. Clere Smithe. “Compositional Active Inference I: Bayesian Lenses. Statistical
Games”. 09/09/2021. arXiv: 2109.04461 [math.ST].
[77]
Toby St. Clere Smithe. “Compositional Active Inference II: Polynomial Dynamics. Approxi-
mate Inference Doctrines”. 08/25/2022. arXiv: 2208.12173 [nlin.AO].
[78]
Toby St Clere Smithe and Simon M Stringer. “The Role of Idiothetic Signals, Landmarks, and
Conjunctive Representations in the Development of Place and Head-Direction Cells: A Self-
Organizing Neural Network Model”. In: Cerebral Cortex Communications 3.1 (01/01/2022),
tgab052. issn: 2632-7376. doi: 10 . 1093 / texcom / tgab052. url: https : / /
academic.oup.com/cercorcomms/article/doi/10.1093/texcom/
tgab052/6358621 (visited on 12/23/2022).
[79]
David I. Spivak. The Operad of Wiring Diagrams: Formalizing a Graphical Language for
Databases, Recursion, and Plug-and-Play Circuits. 05/01/2013. arXiv: 1305.0297 [cs,
math]. url: http://arxiv.org/abs/1305.0297 (visited on 12/23/2022).
[80]
Donald Yau. Operads of Wiring Diagrams. Vol. 2192. 2018. doi: 10.1007/978- 3-
319-95001-3. arXiv: 1512.01602 [math]. url: http://arxiv.org/abs/
1512.01602 (visited on 11/16/2022).
[81]
Toby St. Clere Smithe. “Bayesian Updates Compose Optically”. 05/31/2020. arXiv: 2006.
01631v1 [math.CT].
[82]
Rafal Bogacz. “A Tutorial on the Free-Energy Framework for Modelling Perception and
Learning”. In: Journal of Mathematical Psychology 76 (02/2017), pp. 198–211. doi: 10.
1016/j.jmp.2015.11.003.
[83]
Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In: Neural Compu-
tation 9.8 (11/01/1997), pp. 1735–1780. issn: 0899-7667, 1530-888X. doi: 10.1162/neco.
1997.9.8.1735. url: https://direct.mit.edu/neco/article/9/8/
1735-1780/6109 (visited on 12/17/2022).
248

[84]
Saunders Mac Lane. Categories for the Working Mathematician. 2nd ed. Graduate Texts in
Mathematics 5. New York: Springer-Verlag, 1998. isbn: 0-387-98403-8.
[85]
The Univalent Foundations Program. Homotopy Type Theory: Univalent Foundations of
Mathematics. Institute for Advanced Study: https://homotopytypetheory.org/book, 2013.
eprint: http://saunders.phil.cmu.edu/book/hott- a4.pdf. url:
https://homotopytypetheory.org/book/.
[86]
John Rupert Firth. “A Synopsis of Linguistic Theory, 1930-1955”. In: Studies in Linguistic
Analysis. Oxford: Basil Blackwell, 1957, pp. 1–32.
[87]
Kenta Cho and Bart Jacobs. “Disintegration and Bayesian Inversion via String Dia-
grams”. In: Math. Struct. Comp. Sci. 29 (2019) 938-971 (08/29/2017). doi: 10 . 1017 /
S0960129518000488. arXiv: 1709.00322v3 [cs.AI].
[88]
Matthijs Vákár. Syntax and Semantics of Linear Dependent Types. 01/16/2015. arXiv: 1405.
0033 [cs, math]. url: http://arxiv.org/abs/1405.0033 (visited on
12/17/2022).
[89]
Conor McBride. “I Got Plenty o’ Nuttin’”. In: A List of Successes That Can Change the World.
Springer International Publishing, 2016, pp. 207–233. doi: 10.1007/978-3-319-
30936-1_12.
[90]
Martin Lundfall. “A Diagram Model of Linear Dependent Type Theory”. 06/25/2018. arXiv:
1806.09593 [math.LO].
[91]
Robert Atkey. “Syntax and Semantics of Quantitative Type Theory”. In: Proceedings of
the 33rd Annual ACM/IEEE Symposium on Logic in Computer Science. ACM, 07/2018. doi:
10.1145/3209108.3209189.
[92]
Peng Fu, Kohei Kishida, and Peter Selinger. “Linear Dependent Type Theory for Quantum
Programming Languages”. 04/28/2020. arXiv: 2004.13472 [cs.PL].
[93]
Matteo Capucci et al. “Towards Foundations of Categorical Cybernetics”. 05/13/2021. arXiv:
2105.06332 [math.CT].
[94]
Matteo Capucci and Bruno Gavranović. “Actegories for the Working Amthematician”.
03/30/2022. arXiv: 2203.16351 [math.CT].
[95]
Matteo Capucci, Bruno Gavranović, and Toby St. Clere Smithe. “Parameterized Categories
and Categories by Proxy”. In: Category Theory 2021. 2021.
[96]
Tom Leinster. Higher Operads, Higher Categories. London Mathematical Society Lecture
Note Series 298. Cambridge University Press, Cambridge, 2004. isbn: 0-521-53215-9. doi:
10.1017/CBO9780511525896.
249

[97]
Brendan Fong and David I. Spivak. Seven Sketches in Compositionality: An Invitation to
Applied Category Theory. 2018. arXiv: 1803.05316v3 [math.CT].
[98]
Brendan Fong and David I. Spivak. “Hypergraph Categories”. In: Journal of Pure and Applied
Algebra 223.11 (11/2019), pp. 4746–4777. issn: 00224049. doi: 10.1016/j.jpaa.
2019.02.014. url: https://linkinghub.elsevier.com/retrieve/
pii/S0022404919300489 (visited on 12/17/2022).
[99]
Evan Patterson, David I. Spivak, and Dmitry Vagner. “Wiring Diagrams as Normal Forms
for Computing in Symmetric Monoidal Categories”. In: Electronic Proceedings in Theoretical
Computer Science 333 (02/08/2021), pp. 49–64. issn: 2075-2180. doi: 10.4204/EPTCS.
333.4. arXiv: 2101.12046 [cs]. url: http://arxiv.org/abs/2101.
12046 (visited on 10/27/2022).
[100]
Brandon Shapiro and David I. Spivak. “Dynamic Categories, Dynamic Operads: From Deep
Learning to Prediction Markets”. 05/08/2022. arXiv: 2205.03906 [math.CT].
[101]
John C. Baez et al. “Network Models”. In: Theory and Applications of Categories, Vol. 35, 2020,
No. 20, pp 700-744 (10/31/2017). arXiv: 1711.00037 [math.CT].
[102]
Dmitry Vagner, David I. Spivak, and Eugene Lerman. “Algebras of Open Dynamical Systems
on the Operad of Wiring Diagrams”. In: Theory and Applications of Categories 30 (2015),
Paper No. 51, 1793–1822. issn: 1201-561X.
[103]
David I. Spivak, Patrick Schultz, and Dylan Rupel. “String Diagrams for Traced and Compact
Categories Are Oriented 1-Cobordisms”. In: J. Pure Appl. Algebra 221 (2017), no. 8, pp. 2064-
2110 (08/05/2015). doi: 10.1016/j.jpaa.2016.10.009. arXiv: 1508.01069
[math.CT].
[104]
Eugene Lerman and David I. Spivak. “An Algebra of Open Continuous Time Dynamical
Systems and Networks”. 02/02/2016. arXiv: 1602.01017v2 [math.DS].
[105]
David I. Spivak and Joshua Tan. “Nesting of Dynamic Systems and Mode-Dependent
Networks”. 02/25/2015. arXiv: 1502.07380.
[106]
Dylan Rupel and David I. Spivak. The Operad of Temporal Wiring Diagrams: Formalizing a
Graphical Language for Discrete-Time Processes. 2013. eprint: arXiv:1307.6894.
[107]
John C. Baez and James Dolan. “Higher-Dimensional Algebra III: N-Categories and the
Algebra of Opetopes”. In: Adv. Math. 135 (1998), 145-206. (02/10/1997). arXiv: q-alg/
9702014.
[108]
Eugenia Cheng. “Weak N-Categories: Opetopic and Multitopic Foundations”. 04/21/2003.
arXiv: math/0304277.
250

[109]
A. L. Hodgkin and A. F. Huxley. “A Quantitative Description of Membrane Current and
Its Application to Conduction and Excitation in Nerve”. In: The Journal of Physiology
117.4 (08/1952), pp. 500–544. issn: 0022-3751. doi: 10 . 1113 / jphysiol . 1952 .
sp004764. url: http : / / dx . doi . org / 10 . 1113 / jphysiol . 1952 .
sp004764.
[110]
Michael V Mascagni, Arthur S Sherman, et al. “Numerical Methods for Neuronal
Modeling”. In: Methods in neuronal modeling 2 (1989). url: http : / / cox . iwr .
uni-heidelberg.de/teaching/numsimneuro_ss2011/mascagni_
sherman.pdf.
[111]
Peter Dayan and Laurence F Abbott. Theoretical Neuroscience. Vol. 806. Cambridge, MA:
MIT Press, 2001.
[112]
Ausra Saudargiene, Bernd Porr, and Florentin Wörgötter. “How the Shape of Pre- and
Postsynaptic Signals Can Inﬂuence STDP: A Biophysical Model.” In: Neural computation 16.3
(03/2004), pp. 595–625. doi: 10.1162/089976604772744929. pmid: 15006093.
url: http://dx.doi.org/10.1162/089976604772744929.
[113]
Wulfram Gerstner. “Spike-Response Model”. In: Scholarpedia 3.12 (2008), p. 1343.
[114]
Renaud Jolivet, Timothy J., and Wulfram Gerstner. “The Spike Response Model: A
Framework to Predict Neuronal Spike Trains”. In: Lecture Notes in Computer Science (2003),
pp. 846–853. issn: 0302-9743. doi: 10.1007/3-540-44989-2_101. url: http:
//dx.doi.org/10.1007/3-540-44989-2_101.
[115]
Eugene M Izhikevich. “Neural Excitability, Spiking and Bursting”. In: International Journal
of Bifurcation and Chaos 10.06 (06/2000), pp. 1171–1266. issn: 1793-6551. doi: 10 .
1142 / s0218127400000840. url: http : / / dx . doi . org / 10 . 1142 /
S0218127400000840.
[116]
Srdjan Ostojic and Nicolas Brunel. “From Spiking Neuron Models to Linear-Nonlinear
Models”. In: PLoS computational biology 7.1 (01/2011). Ed. by Peter E. Latham, e1001056.
doi: 10.1371/journal.pcbi.1001056. url: http://dx.doi.org/10.
1371/journal.pcbi.1001056.
[117]
Sophie Deneve. “Making Decisions with Unknown Sensory Reliability”. In: Frontiers in
neuroscience 6 (2012). url: http://www.ncbi.nlm.nih.gov/pmc/articles/
PMC3367295/.
[118]
Sophie Denève and Christian K Machens. “Eﬃcient Codes and Balanced Networks”. In:
Nature neuroscience 19.3 (02/2016), pp. 375–382. issn: 1546-1726. doi: 10.1038/nn.
4243.
251

[119]
Evan S. Schaﬀer, Srdjan Ostojic, and L. F. Abbott. “A Complex-Valued Firing-Rate Model
That Approximates the Dynamics of Spiking Networks”. In: PLoS computational biology
9.10 (10/2013). Ed. by Bard Ermentrout, e1003301. doi: 10.1371/journal.pcbi.
1003301. url: http://dx.doi.org/10.1371/journal.pcbi.1003301.
[120]
Carl van Vreeswijk and Haim Sompolinsky. “Chaos in Neuronal Networks with Balanced
Excitatory and Inhibitory Activity”. In: Science (New York, N.Y.) 274.5293 (1996), pp. 1724–
1726. doi: 10.1126/science.274.5293.1724. pmid: 8939866.
[121]
Brian Day and Ross Street. “Monoidal Bicategories and Hopf Algebroids”. In: Advances
in Mathematics 129.1 (07/1997), pp. 99–157. issn: 00018708. doi: 10 . 1006 / aima .
1997.1649. url: https://linkinghub.elsevier.com/retrieve/
pii/S0001870897916492 (visited on 12/17/2022).
[122]
David I Spivak and Nelson Niu. Polynomial Functors: A General Theory of Interaction. 2021.
url: https://raw.githubusercontent.com/ToposInstitute/poly/
main/Book-Poly.pdf.
[123]
David I. Spivak. “A Reference for Categorical Structures on Poly”. 02/01/2022. arXiv: 2202.
00534 [math.CT].
[124]
Danel Ahman and Tarmo Uustalu. “Directed Containers as Categories”. In: EPTCS 207,
2016, pp. 89-98 (04/05/2016). doi: 10.4204/EPTCS.207.5. arXiv: 1604.01187
[cs.LO].
[125]
Francis Borceux. Handbook of Categorical Algebra 2. Categories and Structures. Vol. 51.
Encyclopedia of Mathematics and Its Applications. Cambridge University Press, Cambridge,
1994. isbn: 978-0-521-44179-7.
[126]
Tobias Fritz. “A Synthetic Approach to Markov Kernels, Conditional Independence and
Theorems on Suﬃcient Statistics”. 08/19/2019. arXiv: 1908.07021v3 [math.ST].
[127]
Dylan Braithwaite and Jules Hedges. Dependent Bayesian Lenses: Categories of Bidirectional
Markov Kernels with Canonical Bayesian Inversion. 09/29/2022. arXiv: 2209.14728 [cs,
math, stat]. url: http : / / arxiv . org / abs / 2209 . 14728 (visited on
12/17/2022).
[128]
Kenta Cho et al. “An Introduction to Eﬀectus Theory”. 2015. arXiv: 1512.05813.
[129]
Sam Staton. “Commutative Semantics for Probabilistic Programming”. In: Programming
Languages and Systems. Springer Berlin Heidelberg, 2017, pp. 855–879. doi: 10.1007/
978-3-662-54434-1_32.
[130]
Fosco Loregian and Emily Riehl. “Categorical Notions of Fibration”. 06/15/2018. arXiv:
1806.06129v2 [math.CT].
252

[131]
Joe Moeller and Christina Vasilakopoulou. “Monoidal Grothendieck Construction”.
09/03/2018. arXiv: 1809.00727v2 [math.CT].
[132]
Aaron Bohannon, Benjamin C Pierce, and Jeﬀrey A Vaughan. “Relational Lenses: A Language
for Updatable Views”. In: Proceedings of the Twenty-Fifth ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems. ACM. 2006, pp. 338–347.
[133]
J. Nathan Foster et al. “Combinators for Bidirectional Tree Transformations”. In: ACM
Transactions on Programming Languages and Systems 29.3 (05/2007), p. 17. doi: 10.1145/
1232420.1232424.
[134]
David I. Spivak. “Generalized Lens Categories via Functors Cop →Cat”. 08/06/2019. arXiv:
1908.02202v2 [math.CT].
[135]
Guillaume Boisseau. “String Diagrams for Optics”. 02/11/2020. arXiv: 2002.11480v1
[math.CT].
[136]
Bartosz Milewski. Profunctor Optics: The Categorical View. 2017. url: https : / /
bartoszmilewski . com / 2017 / 07 / 07 / profunctor - optics - the -
categorical-view/.
[137]
Mitchell Riley. “Categories of Optics”. 09/03/2018. arXiv: 1809.00738v2 [math.CT].
[138]
Mario Román. “Open Diagrams via Coend Calculus”. 04/09/2020. arXiv: 2004.04526v2
[math.CT].
[139]
Brendan Fong and Michael Johnson. “Lenses and Learners”. In: In: J. Cheney, H-S. Ko
(eds.): Proceedings of the Eighth International Workshop on Bidirectional Transformations (Bx
2019), Philadelphia, PA, USA, June 4, 2019, published at http://ceur-ws.org (03/05/2019). arXiv:
1903.03671v2 [cs.LG].
[140]
Geoﬀrey S. H. Cruttwell et al. “Categorical Foundations of Gradient-Based Learning”. In:
Programming Languages and Systems. Springer International Publishing, 2022, pp. 1–28.
doi: 10.1007/978-3-030-99336-8_1.
[141]
Joe Bolt, Jules Hedges, and Philipp Zahn. “Bayesian Open Games”. 10/08/2019. arXiv:
1910.03656v1 [cs.GT].
[142]
Jules Hedges and Riu Rodríguez Sakamoto. “Value Iteration Is Optic Composition”.
06/09/2022. arXiv: 2206.04547 [math.CT].
[143]
Andre Videla and Matteo Capucci. Lenses for Composable Servers. 03/29/2022. arXiv: 2203.
15633 [cs]. url: http : / / arxiv . org / abs / 2203 . 15633 (visited on
12/17/2022).
253

[144]
David Jaz Myers. “Double Categories of Open Dynamical Systems (Extended Abstract)”. In:
Electronic Proceedings in Theoretical Computer Science 333 (02/08/2021), pp. 154–167. issn:
2075-2180. doi: 10.4204/EPTCS.333.11. arXiv: 2005.05956 [math]. url:
http://arxiv.org/abs/2005.05956 (visited on 11/16/2022).
[145]
David I. Spivak. “Poly: An Abundant Categorical Setting for Mode-Dependent Dynamics”.
05/05/2020. arXiv: 2005.01894 [math.CT].
[146]
Dario Maxmilian Stein. “Structural Foundations for Probabilistic Programming Languages”.
University of Oxford, 2021. url: https://dario-stein.de/notes/thesis.
pdf.
[147]
F. W. Lawvere and S. H. Schnauel. Conceptual Mathematics : A First Introduction to Categories.
Cambridge, UK New York: Cambridge University Press, 2009. isbn: 978-0-511-80419-9. doi:
10.1017/CBO9780511804199.
[148]
Alexander Kurz. Logic Column 15: Coalgebras and Their Logics. 05/28/2006. arXiv: cs/
0605128. url: http://arxiv.org/abs/cs/0605128 (visited on 12/17/2022).
[149]
Dusko Pavlovic, Michael Mislove, and James B Worrell. “Testing Semantics: Connecting
Processes and Process Logics”. In: International Conference on Algebraic Methodology and
Software Technology. Springer. Springer Berlin Heidelberg, 2006, pp. 308–322. doi: 10.
1007/11784180_24.
[150]
Bart Jacobs. Introduction to Coalgebra. Vol. 59. Cambridge University Press, 2017.
[151]
Corina Cirstea. “An Algebra-Coalgebra Framework for System Speciﬁcation”. In: Electronic
Notes in Theoretical Computer Science 33 (2000), pp. 80–110.
[152]
David Corﬁeld. Coalgebraic Modal Logic. 2009. url: https://golem.ph.utexas.
edu/category/2009/09/coalgebraic_modal_logic.html.
[153]
Bryce Clarke et al. “Profunctor Optics, a Categorical Update”. 01/21/2020. arXiv: 2001.
07488v1 [cs.PL].
[154]
Ludwig Arnold. Random Dynamical Systems. Springer Berlin Heidelberg, 1998. 612 pp.
isbn: 3-540-63758-3. url: https://www.ebook.de/de/product/1248025/
ludwig_arnold_random_dynamical_systems.html.
[155]
David I. Spivak. “Learners’ Languages”. 03/01/2021. arXiv: 2103.01189 [math.CT].
[156]
Pietro Vertechi. “Dependent Optics”. 04/20/2022. arXiv: 2204.09547 [math.CT].
[157]
Dylan Braithwaite et al. “Fibre Optics”. 12/21/2021. arXiv: 2112.11145 [math.CT].
254

[158]
David Jaz Myers. Categorical Systems Theory (Draft). 2022. url: http://davidjaz.
com/Papers/DynamicalBook.pdf.
[159]
Matteo Capucci. “Diegetic Representation of Feedback in Open Games”. 06/24/2022. arXiv:
2206.12338 [cs.GT].
[160]
John M. Lee. “Smooth Manifolds”. In: Introduction to Smooth Manifolds. New York, NY:
Springer New York, 2012, pp. 1–31. isbn: 978-1-4419-9982-5. doi: 10.1007/978-1-
4419-9982-5_1. url: https://doi.org/10.1007/978-1-4419-9982-
5_1.
[161]
Fosco Loregian. (Co)End Calculus. London Mathematical Society Lecture Note Se-
ries. Cambridge: Cambridge University Press, 2015-01-11, 2021. doi: 10 . 1017 /
9781108778657. arXiv: 1501.02503v6 [math.CT].
[162]
Aurelio Carboni, Stefano Kasangian, and Ross Street. “Bicategories of Spans and Relations”.
In: Journal of Pure and Applied Algebra 33.3 (09/1984), pp. 259–267. issn: 00224049. doi: 10.
1016/0022-4049(84)90061-6. url: https://linkinghub.elsevier.
com/retrieve/pii/0022404984900616 (visited on 12/17/2022).
[163]
Finn Lawler. Fibrations of Predicates and Bicategories of Relations. 02/27/2015. arXiv: 1502.
08017 [math]. url: http://arxiv.org/abs/1502.08017 (visited on
12/17/2022).
[164]
David M. Blei, Alp Kucukelbir, and Jon D. McAuliﬀe. “Variational Inference: A Review
for Statisticians”. In: Journal of the American Statistical Association, Vol. 112 , Iss. 518, 2017
(01/04/2016). doi: 10.1080/01621459.2017.1285773. arXiv: 1601.00670v9
[stat.CO].
[165]
Diederik P Kingma and Max Welling. “Auto-Encoding Variational Bayes”. 12/20/2013. arXiv:
1312.6114v10 [stat.ML].
[166]
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. “Generalized Variational
Inference”. 12/13/2019. arXiv: 1904.02063 [stat.ML].
[167]
Bart Jacobs. Structured Probabilitistic Reasoning (Forthcoming). 2019.
[168]
K. Friston et al. “Variational Free Energy and the Laplace Approximation”. In: Neuroimage
34.1 (01/2007), pp. 220–234. doi: 10.1016/j.neuroimage.2006.08.035. pmid:
17055746.
[169]
Diederik P. Kingma. “Variational Inference & Deep Learning. A New Synthesis”. PhD thesis.
University of Amsterdam, 2017. url: https://hdl.handle.net/11245.1/
8e55e07f-e4be-458f-a929-2f9bc2d169e8.
255

[170]
Dan Shiebler. “Categorical Stochastic Processes and Likelihood”. In: Compositionality 3,
1 (2021) (05/10/2020). doi: 10.32408/compositionality-3-1. arXiv: 2005.
04735 [cs.AI].
[171]
Donald Olding Hebb. The Organization of Behavior: A Neuropsychological Approach. John
Wiley & Sons, 1949.
[172]
Wulfram Gerstner and Werner M. Kistler. “Mathematical Formulations of Hebbian Learning”.
In: Biological Cybernetics 87.5-6 (12/2002), pp. 404–415. issn: 0340-1200. doi: 10.1007/
s00422-002-0353-y. url: http://dx.doi.org/10.1007/s00422-
002-0353-y.
[173]
Harel Shouval. “Models of Synaptic Plasticity”. In: Scholarpedia 2.7 (2007), p. 1605. issn:
1941-6016. doi: 10.4249/scholarpedia.1605. url: http://dx.doi.org/
10.4249/scholarpedia.1605.
[174]
Edmund T. Rolls and Alessandro Treves. Neural Networks and Brain Function. 1st ed. Oxford
University Press, USA, 01/15/1998. isbn: 0-19-852432-3.
[175]
Cristian Bodnar et al. Neural Sheaf Diﬀusion: A Topological Perspective on Heterophily and
Oversmoothing in GNNs. 10/21/2022. arXiv: 2202.04579 [cs, math]. url: http:
//arxiv.org/abs/2202.04579 (visited on 12/23/2022).
[176]
Dániel Unyi et al. Utility of Equivariant Message Passing in Cortical Mesh Segmentation.
06/15/2022. arXiv: 2206.03164 [cs]. url: http://arxiv.org/abs/2206.
03164 (visited on 12/23/2022).
[177]
Jakob Hansen. “Laplacians of Cellular Sheaves: Theory and Applications”. PhD thesis.
University of Pennsylvania, 2020.
[178]
Bryce Clarke. “Internal Lenses as Functors and Cofunctors”. In: Electronic Proceedings
in Theoretical Computer Science 323 (09/15/2020), pp. 183–195. issn: 2075-2180. doi: 10.
4204/EPTCS.323.13. url: http://arxiv.org/abs/2009.06835v1
(visited on 12/23/2022).
[179]
Bryce Clarke and Matthew Di Meglio. An Introduction to Enriched Cofunctors. 09/02/2022.
arXiv: 2209.01144 [math]. url: http://arxiv.org/abs/2209.01144
(visited on 12/23/2022).
[180]
Samson Abramsky and Giovanni Carù. “Non-Locality, Contextuality and Valuation Algebras:
A General Theory of Disagreement”. In: Philosophical Transactions of the Royal Society
A: Mathematical, Physical and Engineering Sciences 377.2157 (09/2019), p. 20190036. doi:
10.1098/rsta.2019.0036.
256

[181]
Giovanni Carù. “Logical and Topological Contextuality in Quantum Mechanics and Beyond”.
University of Oxford / University of Oxford / University of Oxford, 2019. url: https:
/ / ora . ox . ac . uk / objects / uuid : 9bc2335a - b627 - 463b - 9526 -
f4b881b0fbbf.
[182]
Jerome R Busemeyer and Peter D Bruza. Quantum Models of Cognition and Decision.
Cambridge University Press, 2012.
[183]
Andrei Khrennikov et al. “Quantum Models for Psychological Measurements: An Unsolved
Problem”. In: PLoS ONE 9.10 (10/2014). Ed. by Zhong-LinEditor Lu, e110909. issn: 1932-6203.
doi: 10.1371/journal.pone.0110909. url: http://dx.doi.org/10.
1371/journal.pone.0110909.
[184]
Diederik Aerts et al. “Quantum Cognition Beyond Hilbert Space: Fundamentals and
Applications”. In: Lecture Notes in Computer Science (2017), pp. 81–98. issn: 1611-3349.
doi: 10.1007/978-3-319-52289-0_7.
[185]
Renaud Jardri and Sophie Deneve. “Computational Models of Hallucinations”. In: The
Neuroscience of Hallucinations. Springer, 2013, pp. 289–313.
[186]
Pantelis Leptourgos et al. “Circular Inference in Bistable Perception”. In: Journal of Vision 20.4
(04/21/2020), p. 12. issn: 1534-7362. doi: 10.1167/jov.20.4.12. url: https://
jov.arvojournals.org/article.aspx?articleid=2765046 (visited
on 12/23/2022).
[187]
Samson Abramsky et al. “Contextuality, Cohomology and Paradox”. In: 24th EACSL Annual
Conference on Computer Science Logic (CSL 2015), Leibniz International Proceedings in
Informatics (LIPIcs), 41: 211-228, 2015 (02/10/2015). doi: 10 . 4230 / LIPIcs . CSL .
2015.211. arXiv: 1502.03097 [quant-ph].
[188]
Justin Curry. “Sheaves, Cosheaves and Applications”. 03/13/2013. arXiv: 1303.3255v2
[math.AT].
[189]
Kenneth S. Brown. “Abstract Homotopy Theory and Generalized Sheaf Cohomology”. In:
Transactions of the American Mathematical Society 186 (1973), pp. 419–419. doi: 10.1090/
s0002-9947-1973-0341469-9. JSTOR: 1996573.
[190]
Jakob Hansen and Robert Ghrist. “Learning Sheaf Laplacians from Smooth Signals”. In:
ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 05/2019. doi: 10.1109/icassp.2019.8683709.
[191]
Jakob Hansen and Robert Ghrist. “Opinion Dynamics on Discourse Sheaves”. 05/26/2020.
arXiv: 2005.12798 [math.DS].
257

[192]
Chris Fields et al. “A Free Energy Principle for Generic Quantum Systems”. In: Progress
in Biophysics and Molecular Biology 173 (09/2022), pp. 36–59. doi: 10 . 1016 / j .
pbiomolbio.2022.05.006.
[193]
Chris Fields, James F. Glazebrook, and Antonino Marciano. “The Physical Meaning of
the Holographic Principle”. In: Quanta 11.1 (11/21/2022), pp. 72–96. issn: 1314-7374.
doi: 10 . 12743 / quanta . v11i1 . 206. arXiv: 2210 . 16021 [gr-qc,
physics:hep-th, physics:quant-ph]. url: http : / / arxiv . org /
abs/2210.16021 (visited on 12/23/2022).
[194]
Thomas N. Kipf and Max Welling. Variational Graph Auto-Encoders. 11/21/2016. arXiv:
1611.07308 [cs, stat]. url: http://arxiv.org/abs/1611.07308
(visited on 12/23/2022).
[195]
Thomas N. Kipf and Max Welling. Semi-Supervised Classiﬁcation with Graph Convolutional
Networks. 02/22/2017. arXiv: 1609.02907 [cs, stat]. url: http://arxiv.
org/abs/1609.02907 (visited on 12/23/2022).
[196]
Jie Zhou et al. “Graph Neural Networks: A Review of Methods and Applications”. 12/20/2018.
arXiv: 1812.08434v4 [cs.LG].
[197]
Michael M. Bronstein et al. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics,
and Gauges. 05/02/2021. arXiv: 2104.13478 [cs, stat]. url: http://arxiv.
org/abs/2104.13478 (visited on 12/23/2022).
[198]
Judea Pearl. “Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach”.
In: Probabilistic and Causal Inference. Ed. by Hector Geﬀner, Rina Dechter, and Joseph Y.
Halpern. 1st ed. New York, NY, USA: ACM, 1982, pp. 129–138. isbn: 978-1-4503-9586-1.
doi: 10.1145/3501714.3501727. url: https://dl.acm.org/doi/10.
1145/3501714.3501727 (visited on 12/23/2022).
[199]
J.S. Yedidia, W.T. Freeman, and Y. Weiss. “Constructing Free-Energy Approximations and
Generalized Belief Propagation Algorithms”. In: IEEE Transactions on Information Theory
51.7 (07/2005), pp. 2282–2312. issn: 0018-9448. doi: 10.1109/TIT.2005.850085.
url: http://ieeexplore.ieee.org/document/1459044/ (visited on
12/23/2022).
[200]
Jason Morton. “Belief Propagation in Monoidal Categories”. In: Proceedings of the 11th
Workshop on Quantum Physics and Logic. Vol. 172. EPTCS, 12/28/2014, pp. 262–269. doi:
10.4204/EPTCS.172.18. arXiv: 1405.2618 [math]. url: http://arxiv.
org/abs/1405.2618 (visited on 12/23/2022).
[201]
Bert de Vries and Karl J. Friston. “A Factor Graph Description of Deep Temporal Active
Inference”. In: Frontiers in Computational Neuroscience 11 (10/2017). doi: 10.3389/
fncom.2017.00095.
258

[202]
Grégoire Sergeant-Perthuis. Regionalized Optimization. 05/19/2022. arXiv: 2201.11876
[cs, math]. url: http : / / arxiv . org / abs / 2201 . 11876 (visited on
12/23/2022).
[203]
Sara Sabour, Nicholas Frosst, and Geoﬀrey E Hinton. “Dynamic Routing Between Capsules”.
In: Advances in Neural Information Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran
Associates, Inc., 2017. url: https://proceedings.neurips.cc/paper/
2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf.
[204]
Saunders MacLane and Ieke Moerdijk. Sheaves in Geometry and Logic: A First Introduction
to Topos Theory. Springer, 1992. isbn: 0-387-97710-4.
[205]
D Gowanlock R Tervo, Joshua B Tenenbaum, and Samuel J Gershman. “Toward the Neural
Implementation of Structure Learning”. In: Current Opinion in Neurobiology 37 (04/2016),
pp. 99–105. issn: 0959-4388. doi: 10.1016/j.conb.2016.01.014. url: http:
//dx.doi.org/10.1016/j.conb.2016.01.014.
[206]
Christopher Summerﬁeld, Fabrice Luyckx, and Hannah Sheahan. “Structure Learning and
the Posterior Parietal Cortex”. In: Progress in Neurobiology (10/2019), p. 101717. doi: 10.
1016/j.pneurobio.2019.101717.
[207]
Amirhossein Jafarian et al. “Structure Learning in Coupled Dynamical Systems and Dynamic
Causal Modelling”. In: Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences 377.2160 (10/2019), p. 20190048. doi: 10.1098/rsta.
2019.0048.
[208]
David I. Spivak. “Functorial Data Migration”. In: Information and Computation 217 (2012),
pp. 31–51. issn: 0890-5401. doi: 10.1016/j.ic.2012.05.001. url: https:
//doi.org/10.1016/j.ic.2012.05.001.
[209]
David I. Spivak. Functorial Aggregation. 01/31/2022. arXiv: 2111.10968 [cs, math].
url: http://arxiv.org/abs/2111.10968 (visited on 12/23/2022).
[210]
Shun-ichi Amari. “Information Geometry”. In: Contemporary Mathematics. Ed. by Hanna
Nencka and Jean-Pierre Bourguignon. Vol. 203. Providence, Rhode Island: American
Mathematical Society, 1997, pp. 81–95. isbn: 978-0-8218-0607-4 978-0-8218-7794-4. doi:
10.1090/conm/203/02554. url: http://www.ams.org/conm/203/
(visited on 12/23/2022).
[211]
Shun’ichi Amari. Information Geometry and Its Applications. Applied Mathematical Sciences
volume 194. Japan: Springer, 2016. 374 pp. isbn: 978-4-431-55977-1.
[212]
Frank Nielsen. “An Elementary Introduction to Information Geometry”. 08/17/2018. arXiv:
1808.08271 [cs.LG].
259

[213]
Yann Ollivier. “The Extended Kalman Filter Is a Natural Gradient Descent in Trajectory
Space”. 01/03/2019. arXiv: 1901.00696v1 [math.OC].
[214]
Dalton A. R. Sakthivadivel. A Constraint Geometry for Inference and Integration. 04/18/2022.
arXiv: 2203 . 08119 [cond-mat, physics:math-ph]. url: http : / /
arxiv.org/abs/2203.08119 (visited on 12/23/2022).
[215]
Mona M Garvert, Raymond J Dolan, and Timothy EJ Behrens. “A Map of Abstract Relational
Knowledge in the Human Hippocampal–Entorhinal Cortex”. In: eLife 6 (04/2017). issn:
2050-084X. doi: 10.7554/elife.17086.
[216]
Timothy EJ Behrens et al. “What Is a Cognitive Map? Organizing Knowledge for Flexible
Behavior”. In: Neuron 100.2 (2018), pp. 490–509.
[217]
Shirley Mark et al. “Transferring Structural Knowledge across Cognitive Maps in Humans
and Models”. In: Nature Communications 11.1 (09/2020). doi: 10.1038/s41467-020-
18254-6.
[218]
Silvia Bernardi et al. “The Geometry of Abstraction in Hippocampus and Pre-Frontal Cortex”.
In: (09/2018). doi: 10.1101/408633.
[219]
Jacob LS Bellmund et al. “Navigating Cognition: Spatial Codes for Human Thinking”. In:
Science (New York, N.Y.) 362.6415 (2018, 2018-11), eaat6766. doi: 10.1126/science.
aat6766.
[220]
Michael Shulman. “Homotopy Type Theory: The Logic of Space”. In: New Spaces for
Mathematics and Physics. Ed. by M. Anel and G. Catren. Cambridge University Press,
2017-03-08, 2017. arXiv: 1703.03007 [math.CT]. url: http://arxiv.org/
abs/1703.03007.
[221]
Peter Dayan. “Improving Generalization for Temporal Diﬀerence Learning: The Successor
Representation”. In: Neural Computation 5.4 (07/1993), pp. 613–624. issn: 1530-888X. doi:
10.1162/neco.1993.5.4.613.
[222]
Kimberly L Stachenfeld, Matthew Botvinick, and Samuel J Gershman. “Design Principles of
the Hippocampal Cognitive Map”. In: Advances in Neural Information Processing Systems
27. Ed. by Z. Ghahramani et al. Curran Associates, Inc., 2014, pp. 2528–2536. url: http:
//papers.nips.cc/paper/5340- design- principles- of- the-
hippocampal-cognitive-map.pdf.
[223]
Iva K. Brunec and Ida Momennejad. “Predictive Representations in Hippocampal and
Prefrontal Hierarchies”. In: bioRxiv : the preprint server for biology (2019). doi: 10.1101/
786434. eprint: https://www.biorxiv.org/content/early/2019/
09/30/786434.full.pdf. url: https://www.biorxiv.org/content/
early/2019/09/30/786434.
260

[224]
Thomas Parr, Giovanni Pezzulo, and Karl J. Friston. Active Inference. The Free Energy Principle
in Mind, Brain, and Behavior. MIT Press, 2022, p. 288. isbn: 978-0-262-04535-3.
[225]
Karl Friston et al. “Generalised Filtering”. In: Mathematical Problems in Engineering 2010
(2010), pp. 1–34. doi: 10.1155/2010/621670.
[226]
R. E. Kalman. “A New Approach to Linear Filtering and Prediction Problems”. In: Journal of
Basic Engineering 82.1 (1960), p. 35. doi: 10.1115/1.3662552. url: http://dx.
doi.org/10.1115/1.3662552.
[227]
Frank van der Meulen and Moritz Schauer. Automatic Backward Filtering Forward Guiding
for Markov Processes and Graphical Models. 10/31/2022. arXiv: 2010.03509 [stat].
url: http://arxiv.org/abs/2010.03509 (visited on 12/23/2022).
[228]
Frank van der Meulen. Introduction to Automatic Backward Filtering Forward Guiding.
10/31/2022. arXiv: 2203.04155 [math, stat]. url: http://arxiv.org/
abs/2203.04155 (visited on 12/23/2022).
[229]
Beren Millidge, Alexander Tschantz, and Christopher L Buckley. “Whence the Expected
Free Energy?” 04/17/2020. arXiv: 2004.08128 [cs.AI].
[230]
Russell A Epstein et al. “The Cognitive Map in Humans: Spatial Navigation and Beyond”.
In: Nature Neuroscience 20.11 (10/2017), pp. 1504–1513. doi: 10.1038/nn.4656.
[231]
John O’Keefe. “Place Units in the Hippocampus of the Freely Moving Rat”. In: Experimental
neurology 51.1 (1976), pp. 78–109. url: http://www.sciencedirect.com/
science/article/pii/0014488676900558.
[232]
Lynn Nadel and Lloyd MacDonald. “Hippocampus: Cognitive Map or Working Memory?”
In: Behavioral and Neural Biology 29.3 (1980), pp. 405–409. issn: 0163-1047. doi: 10.1016/
S0163- 1047(80)90430- 6. url: http://www.sciencedirect.com/
science/article/pii/S0163104780904306.
[233]
Olaf Sporns. “Brain Connectivity”. In: Scholarpedia 2.10 (2007), p. 4695. issn: 1941-6016. doi:
10.4249/scholarpedia.4695. url: http://dx.doi.org/10.4249/
scholarpedia.4695.
[234]
Ensor Rafael Palacios et al. “Biological Self-organisation and Markov Blankets”. In: (11/2017).
doi: 10.1101/227181.
[235]
Michael Kirchhoﬀet al. “The Markov Blankets of Life: Autonomy, Active Inference and the
Free Energy Principle”. In: Journal of The Royal Society Interface 15.138 (01/2018), p. 20170792.
doi: 10.1098/rsif.2017.0792.
261

[236]
Lancelot Da Costa et al. “Reward Maximisation through Discrete Active Inference”.
07/11/2022. arXiv: 2009.08111 [cs.AI].
[237]
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
1st ed. Wiley Series in Probability and Statistics. Wiley, 04/15/1994. isbn: 978-0-471-
61977-2 978-0-470-31688-7. doi: 10 . 1002 / 9780470316887. url: https : / /
onlinelibrary.wiley.com/doi/book/10.1002/9780470316887
(visited on 12/23/2022).
[238]
Ernst Zermelo. “Über Eine Anwendung Der Mengenlehre Auf Die Theorie Des Schachspiels”.
In: Proceedings of the Fifth International Congress of Mathematicians. Vol. 2. Cambridge
University Press, 1913, pp. 501–504.
[239]
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. 2nd ed.
Vol. 1. 1. MIT press Cambridge, 2018.
[240]
Achim Blumensath and Viktor Winschel. “A Compositional Coalgebraic Semantics of
Strategic Games”. 12/22/2017. arXiv: 1712.08381v1 [cs.GT].
[241]
Karl J. Friston, Jean Daunizeau, and Stefan J. Kiebel. “Reinforcement Learning or Active
Inference?” In: PLoS ONE 4.7 (07/2009). Ed. by Olaf Sporns, e6421. doi: 10 . 1371 /
journal.pone.0006421.
[242]
Conor Heins et al. “Spin Glass Systems as Collective Active Inference”. 07/14/2022. arXiv:
2207.06970 [cond-mat.dis-nn].
[243]
F. G. Varela, H. R. Maturana, and R. Uribe. “Autopoiesis: The Organization of Living Systems,
Its Characterization and a Model”. In: George J. Klir. Facets of Systems Science. Boston, MA:
Springer US, 1991, pp. 559–569. isbn: 978-1-4899-0720-2 978-1-4899-0718-9. doi: 10.1007/
978-1-4899-0718-9_40. url: http://link.springer.com/10.1007/
978-1-4899-0718-9_40 (visited on 12/23/2022).
[244]
Urs Schreiber. “Diﬀerential Cohomology in a Cohesive Inﬁnity-Topos”. 10/29/2013. arXiv:
1310.7930 [math-ph].
[245]
Dalton A R Sakthivadivel. “Towards a Geometry and Analysis for Bayesian Mechanics”.
04/25/2022. arXiv: 2204.11900 [math-ph].
[246]
Maxwell J D Ramstead et al. “On Bayesian Mechanics: A Physics of and by Beliefs”.
05/23/2022. arXiv: 2205.11543 [cond-mat.stat-mech].
[247]
H. Nyquist. “Certain Topics in Telegraph Transmission Theory”. In: Transactions of the
American Institute of Electrical Engineers 47.2 (04/1928), pp. 617–644. issn: 0096-3860. doi:
10.1109/T- AIEE.1928.5055024. url: http://ieeexplore.ieee.
org/document/5055024/ (visited on 12/23/2022).
262

[248]
R. V. L. Hartley. “Transmission of Information 1”. In: Bell System Technical Journal 7.3
(07/1928), pp. 535–563. issn: 00058580. doi: 10 . 1002 / j . 1538 - 7305 . 1928 .
tb01236.x. url: https://ieeexplore.ieee.org/document/6769394
(visited on 12/23/2022).
[249]
C. E. Shannon. “A Mathematical Theory of Communication”. In: Bell System Technical
Journal 27.3 (07/1948), pp. 379–423. issn: 00058580. doi: 10.1002/j.1538-7305.
1948.tb01338.x. url: https://ieeexplore.ieee.org/document/
6773024 (visited on 12/23/2022).
[250]
Christopher A. Fuchs, N. David Mermin, and Ruediger Schack. “An Introduction to QBism
with an Application to the Locality of Quantum Mechanics”. In: Am. J. Phys., Vol. 82,
(11/2013), No.8, August2014, 749–754. eprint: 1311.5253.
[251]
Christopher A. Fuchs. “Notwithstanding Bohr, the Reasons for QBism”. In: Mind and Matter
15(2), 245-300 (2017) (05/09/2017). arXiv: 1705.03483v2 [quant-ph].
[252]
R. F. Blute, J. R. B. Cockett, and R. A. G. Seely. “Diﬀerential Categories”. In: Math-
ematical Structures in Computer Science 16.06 (11/2006), p. 1049. doi: 10 . 1017 /
s0960129506005676.
[253]
Robin Cockett et al. “Reverse Derivative Categories”. In: CSL 2020. 28th International
Conference on Computer Science Logic. 2019-10-15, 2020. arXiv: 1910 . 07065v1
[cs.LO].
[254]
Yde Venema. “Automata and Fixed Point Logic: A Coalgebraic Perspective”. In: Information
and Computation 204.4 (04/2006), pp. 637–678. doi: 10.1016/j.ic.2005.06.003.
[255]
Daniel J Amit. Modeling Brain Function: The World of Attractor Neural Networks. Cambridge
University Press, 1992.
263

