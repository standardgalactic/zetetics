Language Models can Solve Computer Tasks
Geunwoo Kim
University of California, Irvine
kgw@uci.edu
Pierre Baldi
University of California, Irvine
pfbaldi@ics.uci.edu
Stephen McAleer∗
Carnegie Mellon University
smcaleer@cs.cmu.edu
Abstract
Agents capable of carrying out general tasks on a computer can improve efﬁciency
and productivity by automating repetitive tasks and assisting in complex problem-
solving. Ideally, such agents should be able to solve new computer tasks presented
to them through natural language commands. However, previous approaches to this
problem require large amounts of expert demonstrations and task-speciﬁc reward
functions, both of which are impractical for new tasks. In this work, we show
that a pre-trained large language model (LLM) agent can execute computer tasks
guided by natural language using a simple prompting scheme where the agent
Recursively Criticizes and Improves its output (RCI). The RCI approach signif-
icantly outperforms existing LLM methods for automating computer tasks and
surpasses supervised learning (SL) and reinforcement learning (RL) approaches on
the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL
method, using only a handful of demonstrations per task rather than tens of thou-
sands, and without a task-speciﬁc reward function. Furthermore, we demonstrate
RCI prompting’s effectiveness in enhancing LLMs’ reasoning abilities on a suite of
natural language reasoning tasks, outperforming chain of thought (CoT) prompting.
We ﬁnd that RCI combined with CoT performs better than either separately.
1
Introduction
A long-standing goal in artiﬁcial intelligence has been to create generally-intelligent agents that can
accomplish cognitive tasks as well as humans. Such agents should be able to solve any computer task
a human can by communicating via natural language. By automating repetitive tasks and providing
assistance in complex problem-solving, generally-intelligent virtual agents may radically increase
productivity.
Recently, large language models (LLMs) have shown remarkable in-context learning capabilities
across a variety of domains and tasks [10, 53, 4, 14, 20, 49, 7, 35, 5]. Although LLMs can impressively
manipulate text and can use high-level API tools [44, 37, 30], previous approaches to using LLMs
that directly take keyboard and mouse actions on computers have had difﬁculty compared to imitation
learning and reinforcement learning approaches [18]. LLMs that take keyboard and mouse actions on
computers face a number of obstacles, such as ensuring that generated actions are task-appropriate
(task grounding), feasible in the agent’s current state (state grounding), and admissible to be executed
(agent grounding).
The previous best-performing approaches for taking actions on computers have not used LLMs.
Instead, they have trained networks from scratch to predict actions given prompts and screenshots or
∗Corresponding author.
arXiv:2303.17491v1  [cs.CL]  30 Mar 2023

Figure 1: MiniWoB++ environment. Every task contains a natural language prompt in yellow. The
agent then uses keyboard strokes and mouse clicks to accomplish the task.
DOM information, either via supervised learning (SL) from expert demonstrations, reinforcement
learning (RL) on a handcrafted reward signal, or both (SL+RL) [23]. Although SL+RL works well
on a number of individual computer tasks, since it requires expert data and a reward function for
every task, it has not been shown to generalize to novel tasks in a few-shot setting.
In this work, we show that a pre-trained LLM agent can successfully execute computer tasks
guided by natural language. Our method employs a simple prompting scheme, which we call
Recursive Criticism and Improvement (RCI), that signiﬁcantly outperforms existing LLM methods
for automating computer tasks. RCI works by ﬁrst having the LLM generate an output based on
zero-shot prompting. Then, RCI prompts the LLM to identify problems with the given output. After
the LLM has identiﬁed problems with the output, RCI prompts the LLM to generate an updated
output.
When applying RCI to computer tasks, we improve task grounding, state grounding, and agent
grounding sequentially. Firstly, task grounding prompts the Language Model (LLM) with the task
text, instructing it to generate a high-level plan. Secondly, state grounding connects high-level
concepts derived from the task grounding step with actual HTML elements present in the current
state, subsequently outputting the appropriate action. Finally, agent grounding ensures the correct
formatting of the action output obtained from the state grounding step. RCI is applied to each of
these three steps; however, we ﬁnd that critiquing the state-grounding step is only necessary once.
We evaluate the RCI approach on the MiniWoB++ benchmark [46], and show it surpasses existing
SL, RL, and LLM approaches. Furthermore, it proves itself to be competitive with the state-of-
the-art SL+RL method, using only a small number of demonstrations per task instead of tens of
thousands, and without relying on a task-speciﬁc reward function. This signiﬁcant reduction in
required demonstrations and the elimination of task-speciﬁc reward functions make our method more
practical and accessible for new tasks. Furthermore, as the capabilities of LLMs continue to improve,
one can expect the performance of our method to improve as well.
In addition to its success in automating computer tasks, we also showcase the effectiveness of RCI
prompting in enhancing the reasoning abilities of LLMs on a suite of natural language reasoning
tasks. Our method achieves a signiﬁcant performance increase over zero-shot prompting and slightly
improves upon chain-of-thought [57] (CoT) prompting. Interestingly, RCI and CoT have a synergistic
effect, and their combination outperforms all other methods.
In summary, our work presents a new powerful and practical approach to enabling LLM agents to
execute computer tasks guided by natural language. The RCI prompting scheme not only outperforms
2

previous methods in computer tasks, but also improves reasoning abilities for LLMs more broadly,
making it a signiﬁcant contribution in the development of intelligent agents.
2
Related Works
2.1
Automated computer tasks
The automation of computer tasks is an important topic for both information retrieval and natural
language processing [32, 39, 38, 24, 50]. Recent efforts have focused on the development of
reinforcement learning agents that interact with websites using raw mouse and keyboard actions [46].
MiniWoB, a benchmark proposed in [46], has been extended in MiniWoB++ [27], which has become
a widely-used platform for studying models for computer tasks. Reinforcement Learning and
Imitation Learning have been employed in several studies to tackle MiniWoB++ tasks [27, 19, 25, 17].
However, achieving human-level performance requires a signiﬁcant amount of expert demonstration
data (6,300 hours), as demonstrated in [23]. Recent work [18] has suggested the use of large language
models (LLMs) to comprehend HTML code, with a few-shot in-context approach showing promising
results without extensive RL exploration. Nevertheless, signiﬁcant amounts of expert demonstration
data are still required to ﬁnetune LLMs. On the contrary, the agent we suggest needs less than
two demonstrations per task on average and doesn’t necessitate any ﬁnetuning. WebGPT [31] and
WebShop [60] show that LLMs can automate some web-based tasks by introducing a handful of
custom commands such as Search <query> and Next Page. As a result, these methods are limited
in scope and do not work on general computer tasks which require keyboard strokes and mouse clicks.
In contrast, our approach can tackle open-domain tasks at scale.
2.2
LLMs with actions
In recent years, there have been signiﬁcant advancements in large language models (LLMs), leading
to new possibilities for utilizing natural language for decision-making tasks. One approach involves
augmenting LLMs with executable actions [30]. Huang et al. [21] showed that LLMs can be used
to plan and achieve simple household tasks, utilizing a method for grounding the actions generated
by LLMs by comparing their embeddings with a predeﬁned list of admissible actions. However,
their work did not consider state grounding. Another study by Ahn et al. [1] proposed SayCan,
which grounded the actions by multiplying each candidate action’s probability under FLAN [56]
with the action’s value function, serving as an indicator for the suitability of actions. Huang et
al. [22] proposed an extension to the SayCan model called Inner Monologue, which incorporates a
feedback loop for state grounding. However, Inner Monologue still requires a pre-trained language-
conditioned robot policy with underlying reasoning capabilities that are not free-formed and ﬂexible,
thereby hindering generalization to diverse task domains. Similarly, Zeng et al.[63] employed a
combination of LLMs with a visual-language model (VLM) and a pre-trained language-conditioned
robot policy [48] to perform open vocabulary pick-and-place robotic tasks. Meanwhile, Dasgupta
et al.[11] used Chinchilla[20] as a planner for an agent in the PycoLab environment, but their actor
module requires pre-training with reinforcement learning (RL) to follow natural language instructions.
In a related line of research, Carta et al. [6] employed online RL ﬁne-tuning to achieve functional
grounding of LLMs in the BabyAI-Text environment. In contrast to these previous approaches, our
method does not rely on additional model components beyond LLMs for grounding actions. Instead,
we propose the use of RCI prompting, which enables LLMs to update their actions to be grounded
autonomously. As a result, our approach can scale to a wider range of action spaces, including
keyboard and mouse actions. Furthermore, prior approaches have been limited by the need for
ﬁne-tuning. In contrast, our RCI prompting method is a zero-shot approach that overcomes these
limitations. More recently, an approach to improve the efﬁcacy of LLMs involves their integration
with APIs, allowing them to use external tools such as information retrieval systems, code interpreters,
and web browsers [44, 52]. Notably, these external tools necessitate manual engineering and may be
constrained in their functionality. In contrast, our agent is equipped with a general computer interface,
enabling it to access a wide range of functionalities offered by computers.
2.3
LLMs with reasoning
Recent research has also demonstrated that large language models (LLMs) exhibit enhanced per-
formance in a diverse range of tasks when they produce traces of the underlying reasoning process
3

Q: Valerie earns $5000 per month, 1/2 of what her brother earns. If their mother earns twice their combined salary, what's the total amount of money they all have together?
A:
If Valerie earns 1/2 of what her brother earns, then her brother earns 2*$5000 = $<<2*5000=10000>>10000 per month. Together, Valerie and her brother earn $5000+$10000 
= $<<5000+10000=15000>>15000 per month. If their mother earns twice their combined salary, then she earns 2*$15000 = $<<2*15000=30000>>30000 per month. 
Therefore, the total amount of money they all have together is $30000.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.
Q: Valerie earns $5000 per month, 1/2 of what her brother earns. If their mother earns twice their combined salary, what's the total amount of money they all have together?
A:
Valerie earns $5000 per month, which is 1/2 of what her brother earns. This means her brother earns 2*$5000 = $10000 per month. Together, Valerie and her brother earn 
$5000+$10000 = $15000 per month. Their mother earns twice their combined salary, which is 2*$15000 = $30000 per month. Therefore, the total amount of money they all 
have together is $30000.
Q: Valerie earns $5000 per month, 1/2 of what her brother earns. If their mother earns twice their combined salary, what's the total amount of money they all have together?
A: Let's think step by step. First, we need to figure out how much Valerie's brother earns. We know that Valerie earns 1/2 of what her brother earns, so we can set up an 
equation:
Next, we need to find out how much their mother earns. We know that their mother earns twice their combined salary, so we can add up Valerie and Brother's earnings and 
multiply by 2: Total earnings = 2 * (Valerie's earnings + Brother's earnings) Total earnings = 2 * ($5000 + $10,000) Total earnings = $30,000. Therefore, the total amount of 
money they all have together is $30,000.
If Valerie earns 1/2 of what her brother earns, then her brother earns 2*$5000 = $<<2*5000=10000>>10000 per month. 
Therefore, the total amount of money they all have together is $30000.
Review your previous answer and find problems with your answer.
All individuals, including Valerie, her brother, and mother, should be taken into account when calculating the overall sum of money.
Based on the problems you found, improve your answer.
I apologize for the mistake in my previous answer. Here's the correct solution: If Valerie earns 1/2 of what her brother earns, then her brother earns 2*$5000 = 
$<<2*5000=10000>>10000 per month. Together, Valerie and her brother earn $5000+$10000 = $<<5000+10000=15000>>15000 per month. If their mother earns twice their 
combined salary, then she earns 2*$15000 = $<<2*15000=30000>>30000 per month. Therefore, the total amount of money they all have together is $5000+$10000+$30000 
= $<<5000+10000+30000=45000>>45000.
Standard
Few-Shot-CoT
Zero-Shot-CoT
RCI
Figure 2: Examples of explicit RCI prompting and baseline prompting approaches on the GSM8K
dataset. RCI prompting effectively addresses logical errors that arise in the baseline prompting
approaches. Prompts are highlighted in green.
along with the ﬁnal answer, as evidenced by studies such as [57, 34, 26]. This discovery has led
to the emergence of a new line of research where reasoning capabilities are used to address tasks
beyond reasoning [61, 22], or enhance reasoning proﬁciency [26, 28, 54, 29, 62, 40, 58, 51, 12].
Furthermore, various reasoning architectures have been proposed, expanding from naive prompting,
such as Selection-Inference [9], Least-to-Most [65], and Faithful reasoning [8]. In the existing
literature, a work closely related to our research is ReAct [61] which interleaves reasoning and action
for resolving the issue of hallucination and error propagation as well as helping the model induce,
track, and update actions plans. An alternative method, Reﬂexion [47], extends ReAct by improving
its performance by allowing LLMs to consider previous trial and error experiences. Nevertheless,
due to the necessity of multiple rounds of explicit task-speciﬁc success feedback from trial and error,
this approach may not scale as effortlessly as ours because it requires task-speciﬁc success feedback.
RCI pertains to an extended reasoning architecture where LLMs are instructed to ﬁnd errors in their
outputs and improve them accordingly, which can further be used to ground actions generated from
LLMs in decision-making problems. Saunders et al. [43] used a similar approach to ours by utilizing
the self-critiquing ability of LLMs to generate critical feedback on summaries produced by LLMs.
The aim is to accelerate the human evaluation process by uncovering possible errors in the generated
summaries. Likewise, Ganguli et al. [16], employed LLMs to morally self-correct their outputs to
prevent the generation of harmful content. However, our work is, to the best of our knowledge, the
ﬁrst to demonstrate the self-critiquing capability of LLMs in enhancing reasoning proﬁciency.
3
Methods
3.1
RCI Prompting
The self-critiquing ability of LLMs has demonstrated that LLMs can ﬁnd errors in their own output
by themselves [43, 16]. In light of this, we introduce a simple reasoning architecture called RCI
4

Implicit RCI
Explicit RCI
Here is a plan to solve the task 
using the instructions specified: 
1. Type "rm [filename]" command 
to delete the file ending with ".rb".
2. Press "enter" key
Find problems with this plan.
This plan does not specify the 
filename of the file ending with the 
extension .rb. Without knowing the 
filename, the agent cannot delete 
the file
Based on this, the improved plan for the 
agent to complete the task are as follows.
1. Type "ls" command to list all files in the 
terminal.
2. Type "rm [filename]" command to delete 
the file ending with ".rb".
3. Press "enter" key after typing each 
command to execute the command.
Plan
Critique
Improvement
According to the current 
plan, the next proper 
instruction should be `type 
"rm [filename]"`.
Therefore, considering the output 
on the webpage, the specific 
instruction for solving the task 
should be `type` instruction with 
"rm code.rb" command.
The task is to use the terminal below to delete a file ending with the extension .rb.
Task
Without explanation, the single 
instruction that matches one of the 
regular expressions is `'type "rm 
code.rb"`
3rd step
2nd step
1st step
Initial state
Current state
Task grounding
State grounding
Agent grounding
Figure 3: Execution trace of the agent for terminal tasks with RCI prompting. The language model
generates a step-by-step plan for the high-level task described in natural language, which in this case
involves using the terminal to delete a ﬁle ending with ".rb". We then run an explicit RCI on this
plan, where we sample an improved plan based on the critique and the previous plan, resulting in
an improvement in the task-grounding of the plan. For each step, we ﬁrst sample the task-grounded
action that follows the improved plan, and then the implicit RCI updates the task-grounded actions
sequentially to provide state-grounding and agent-grounding. Finally, the agent-grounded action is
executed by the instruction-following agent on the environment. The prompts are highlighted, and
the remaining text shows the outputs generated by the language model.
prompting, where we prompt LLMs to ﬁnd problems in their output and improve the output based
on what they ﬁnd. This architecture is designed to further enhance the reasoning ability of LLMs
by inserting a critique step before generating the ﬁnal answer. Figure 2 compares example traces of
RCI prompting and baseline prompting methods on GSM8K dataset where language models should
answer grade school math problems. While baselines elicit answers with a single step of prompting,
RCI consists of two steps: criticize the previous answer (e.g., "Review your previous answer and
ﬁnd problems with your answer") and improve the answer based on the critique (e.g., "Based on
the problems you found, improve your answer"). In this way, RCI prompting ﬁnds errors (e.g., the
overall sum of money only considered Valerie and her brother) in the previous answer and generates
an improved answer (e.g., money from Valerie’s mother is included in the total) conditioned on
the critique. The iterative process of RCI can be continued until speciﬁc conditions are satisﬁed,
which could include receiving feedback from the environment, reaching the maximum predetermined
number of iterations, or adhering to certain heuristics. We deﬁne two approaches for achieving RCI:
explicit RCI and implicit RCI. Explicit RCI includes the critique in the prompt to generate improved
output and implicit RCI updates the previous output directly without sampling a critique explicitly.
Examples of explicit RCI and implicit RCI applied to computer tasks are illustrated in Figure 3 where
the action plan is improved based on an explicit critique while actions are updated without an explicit
critique. This process is described in more detail in the following section.
3.2
RCI for Computer Tasks
In this section we describe the application of RCI to computer tasks via a decomposition of action
selection into three reasoning steps: task grounding, state grounding, and agent grounding. The ﬁrst
step, task grounding, involves generating a plan for task-solving and conditioning actions on this plan,
with RCI being used to improve the plan’s success rate. The state grounding subsection discusses the
importance of grounding actions in the environment for language-based agents and how implicit RCI
is used to reﬁne task-grounded actions to be feasible in the current state. Lastly, the agent grounding
step focuses on ensuring that actions are admissible for the computer agent by employing implicit
5

RCI and conditioning agent-grounded actions on the current state, task, and other grounded actions,
with a loop count set to optimize performance.
3.2.1
Problem Setting
We assume that we are given an instruction-following computer agent that can execute a set of
admissible actions given some natural language instructions. An instruction that is not part of the
admissible actions will be ignored. At every step, we receive a high-level natural language task
prompt and a state of the environment. Given the current state and task, we sample the most probable
action from LLMs. The generated natural language action is then fed into the computer agent.
Sampling the actions in a fully generative manner presents a challenge, as the actions must consider
the given task, feasibility in the current state, and admissibility for the computer agent simultaneously.
Therefore, we propose decomposing this action sampling into three reasoning steps each of which
considers task grounding, state grounding, and agent grounding. Task grounding improves actions to
be more effective in solving the given task, state grounding ensures the feasibility of actions in the
current state, and agent grounding considers the executability of actions given the speciﬁcation of
the computer agent. We ﬁrst sample a step-by-step to solve the given task which improves the task
grounding. Next, the task-grounded action is sampled conditioned on the current state, task, and the
generated plan. The state-grounded actions is generated conditioned on the task-grounded action.
If the task-grounded action is not executable by the computer agent, the agent-grounded action is
sampled. For each sampling of grounded action, we use RCI prompting to make LLM consider some
speciﬁc information for grounding.
3.2.2
Grounding Language Model in Computer Tasks
Task grounding.
In the action sampling process, the ﬁrst step involves generating a plan of
actionable steps for task solving from LLMs. Subsequently, actions are sampled from the same
LLMs, taking into account the present state, task, and generated plan. The beneﬁts of conditioning on
the plan for improved grounding of actions are twofold. First, it enables LLMs to identify the stage
of task solving at which the agent is located, serving as a memory module. Second, we can perform
explicit RCI on the generated plan to further improve the plan’s success rate. Although the number of
explicit RCI loops can be arbitrary, we observe that a single pass of explicit RCI sufﬁces for most of
MiniWoB++ tasks.
State grounding.
In language-based agents, grounding actions in the environment is a crucial step
to enable real-world task performance. The aim of this phase is to enhance the task-grounded actions
to be feasible in the current state. Although the actions generated in the preceding phase may align
with the task, they may lack the speciﬁcity required to be executed in the current context. For example,
if the assigned task is to forward an email from Bob to Alice and the action obtained from the task
grounding phase is to click on an email from Bob in the email inbox, it is necessary to establish a
connection between the abstract concept of "email from Bob" and the concrete element, such as the
email heading, in the current webpage state represented by HTML. To achieve this goal, we perform
the implicit RCI and prompt the LLMs to consider the current state, which subsequently outputs
reﬁned state-grounded actions. Moreover, the state-grounded action is additionally conditioned on
the task-grounded action. We avoid repeating the implicit RCI cycle more than once as it does not
impact the success rate based on our observations.
Agent grounding.
To ensure the successful integration of language-based methodologies in
decision-making processes, it is imperative to establish a scalable framework that guarantees the
admissibility of actions derived from the language model. While the preceding steps of sampling
produce a state-grounded action that is both feasible and grounded in the task, it may not be executable
by the agent due to issues such as improper formatting. To address this, Implicit RCI is employed,
whereby an agent-grounded action is sampled conditioned on the current state, task, task-grounded
action, and state-grounded action. The LLMs are prompted to consider speciﬁcations of the computer
agent. The implicit RCI is repeatedly run until the resulting action is executable, with a maximum
loop count set to limit the number of iterations. Empirical analysis on MiniWoB++ tasks suggests
that setting the loop count to 3 yields optimal performance.
6

4
Evaluation
Arithmetic
Common Sense
GSM8K
MultiArith
AddSub
SVAMP
SingleEq
AQuA
CommonSenseQA
StrategyQA
Zero-Shot
78.35
96.06
85.83
78.35
91.34
55.91
53.15
51.57
Zero-Shot + RCI
85.43
97.64
89.76
84.65
94.49
67.32
68.11
61.81
Table 1: RCI prompting increases the reasoning capability of LLMs on all of eight reasoning
benchmarks.
4.1
Reasoning tasks
In our grounding enhancement process, RCI prompts the LLM to criticize its prior output, considering
the given context (e.g., current task, state, and agent), which ultimately leads to improved output. We
ﬁrst demonstrate the effectiveness of RCI prompts in augmenting the reasoning capabilities of LLMs
across a range of reasoning benchmarks. We compare RCI to Chain-of-Thought (CoT) prompting, a
state-of-the-art method recognized for its effectiveness in reasoning tasks.
Speciﬁcally, we compare our approach with Few-Shot-CoT [57] where a few chain-of-thought
demonstrations are given as examples in prompting, and Zero-Shot-CoT [26] that elicit multiple
reasoning steps by simply adding "Let’s think step by step" to the prompt. Following Kojima et
al. [26], our evaluation is conducted with 8 datasets from two categories of reasoning: arithmetic and
commonsense. We use the same experimental setting with their answer extraction method except
that we use InstructGPT-3 + RLHF as the underlying language model. We use the same prompts
that CoT uses and we also use the answer cleansing approach used in CoT, but we only used answer
extraction prompting in zero-shot CoT experiments. We also use the same few-shot examples that
were introduced in [57] to evaluate Few-Shot CoT’s performance on ﬁve arithmetic reasoning tasks.
Comparison with Zero-Shot.
RCI prompting is better at solving reasoning tasks compared to zero-
shot prompting. Table 1 summarizes the accuracy of our approach (Zero-Shot + RCI) and standard
zero-shot prompting for each reasoning benchmark. Zero-Shot + RCI substantially outperforms the
standard prompting in all benchmarks including arithmetic (GSM8K, MultiArith, AddSub, AQUA,
SVAMP, SingleEq) and common sense (CommonSenseQA, StrategyQA) tasks. RCI prompting
even achieves score gains from two arithmetic reasoning tasks (SingleEq and AddSub), which
do not require multi-step reasoning. This distinguishes our RCI prompting from the previous
CoT prompting methods [57, 26] that are not useful in simple reasoning tasks. It is also worth
noting that RCI prompting achieves a signiﬁcant performance gain in commonsense reasoning tasks
(CommonSenseQA and StrategyQA). While Wei et al. [57] reported that only a substantially large
PaLM (540B) model can beneﬁt from Few-Shot-CoT, RCI prompting can provide performance gain
even with a smaller InstructGPT-3 + RLHF (175B) model.
GSM8K
MultiArith
AddSub
SVAMP
SingleEq
Zero-Shot
78.35
96.06
85.83
78.35
91.34
Zero-Shot + RCI
85.43
97.64
89.76
84.65
94.49
Zero-Shot CoT
82.28
96.85
83.86
79.92
89.37
Zero-Shot CoT + RCI
86.22
97.64
89.76
85.83
90.94
Few-Shot CoT
80.31
98.82
89.37
83.46
91.73
Few-Shot CoT + RCI
84.25
99.21
90.55
87.40
93.70
Table 2: Chain-of-Thought prompting exhibits a synergistic effect when coupled with RCI prompting
in arithmetic reasoning tasks.
Comparison with Chain-of-Thought.
The performance results of RCI and CoT baselines on
arithmetic reasoning tasks are summarized in Table 2. Notably, Zero-Shot + RCI outperforms
Zero-Shot CoT and Few-Shot CoT without any CoT prompting in four tasks except MultiArith.
7

CC-Net
WebN-T5-3B
(no few-shot examples)
WebN-T5-3B
 CC-Net
Others
CC-Net
(no dictionary actions)
Ours
  CC-Net
SL SotA
RL SotA
SL + RL SotA
Model
0.0
0.2
0.4
0.6
0.8
Average Success Rate
0.32
0.45
0.50
0.57
0.76
0.85
0.91
0.94
0.57
0.67
0.94
MiniWoB++ Average Performance
Few-Shot In-Context Learning (Ours)
RL + SL
SL
RL
(a)
10
1
10
2
10
3
10
4
Amount of Demos per Task
0.0
0.2
0.4
0.6
0.8
1.0
Average Success Rate
CC-Net (SL)
WebN-T5-3B
WebN-T5-3B
(no few-shot examples)
CC-Net (SL + RL)
World of bits
(SL + RL)
Workflow guided exploration
(SL + RL)
Ours
MiniWoB++ Average Performance
 vs. Amount of Data
(b)
Figure 4: (a) Average performance comparison with baselines. The tasks that were included in the
averaging process are visually indicated in gray color within Table 21. The performance data for
CC-Net with no dictionary-based action is obtained from the ablation study section in their paper.
The state-of-the-art (SotA) in supervised learning (SL) is represented by the works of [23, 18] while
the SotA in reinforcement learning (RL) includes the studies of [23, 19, 25]. Furthermore, the SotA
in the combined application of SL and RL consists of the contributions of [23, 46, 27]. The group of
models proposed before the publication of [23] are collectively referred to as Others, and include
[46, 27, 19, 25]. (b) Relationship between performance and amount of expert training data. Our
agent displays comparable performance to the current state-of-the-art scores on the MiniWoB++
benchmark, despite using the least amount of data.
In MultiArith tasks, where most of the standard prompting’s answers are correct (96.06%), RCI
prompting does not yield signiﬁcant performance gains. RCI prompting has a synergistic collaborative
impact on the two CoT baselines. Namely, Zero-Shot CoT + RCI and Few-Shot CoT + RCI attain the
highest scores on four out of the ﬁve tasks. These ﬁndings suggest a promising avenue for future
research: combining RCI with other prompting methods for CoT, such as self-consistency [43].
4.2
Computer tasks
4.2.1
Setup
MiniWoB++ benchmark suite.
The miniwob++ task suite is selected as the main benchmark to
evaluate our computer agent. MiniWoB++ [27], an extension of MiniWoB [46], is a web-based
simulation environment that offers a diverse range of computer tasks, from simple button-clicking to
complex compositional tasks requiring advanced reasoning, such as solving math problems. Its shared
action space, including keyboard and mouse, and a common state space centered around HTML code
enables our proposed agent to be thoroughly evaluated in ample tasks. Additionally, the varying levels
of complexity between tasks enable a systematic evaluation of our work. The action space consists
of two operations each of wich controls the keyboard and mouse. The ﬁrst action enables typing of
arbitrary characters or special keys such as Backspace and Enter. The second action involves moving
and clicking the mouse, allowing the agent to interact with visible HTML elements on a webpage.
All actions can be executed through natural language instructions deﬁned by regular expressions that
are presented within the initial prompts provided to the LLMs. The regular expressions employed in
our evaluation are presented in Appendix B. Our action space deﬁnition is similar to previous works,
such as [19, 25, 27], in which clicking actions directly interact with HTML elements. However, for
typing actions, we extend beyond simple form-ﬁlling by using keyboard-based actions and excluding
dictionary-based typing actions [23]. Our approach, therefore, has a better generalization capability
for diverse computer tasks. The state space of our agent consists solely of HTML code.
Model choices.
For the purpose of evaluating the effectiveness of RCI prompting, multiple language
models were used in our experiments. Speciﬁcally, we employed three models, namely, GPT-3
(davinci) [4], InstructGPT-3 (text-davinci-002) [36, 56, 42], and InstructGPT-3 + RLHF (text-davinci-
003, gpt-3.5-turbo) [36]. Unless otherwise speciﬁed, we primarily evaluated our computer agent with
8

identify-shape
multi-layouts
navigate-tree
login-user-popup
click-widget
count-shape
click-button
click-button-sequence
click-dialog
click-link
click-test
click-test-2
focus-text
focus-text-2
enter-text-dynamic
enter-password
click-checkboxes
tic-tac-toe
enter-text
multi-orderings
click-option
click-checkboxes-soft
login-user
guess-number
click-tab
use-autocomplete
click-checkboxes-transfer
email-inbox-forward-nl
click-shape
grid-coordinate
click-tab-2
email-inbox
email-inbox-forward-nl-turk
click-collapsible-2
click-menu
click-tab-2-hard
search-engine
click-checkboxes-large
click-color
choose-list
email-inbox-nl-turk
click-dialog-2
social-media
use-spinner
social-media-some
enter-date
click-collapsible
click-scroll-list
click-shades
enter-time
social-media-all
1
0
1
Residual
Comparison with LLM SotA
identify-shape
navigate-tree
multi-layouts
login-user-popup
count-shape
click-widget
guess-number
click-button
click-button-sequence
click-dialog
click-link
click-tab
click-test
click-test-2
focus-text
focus-text-2
enter-text
enter-text-dynamic
enter-password
click-checkboxes
tic-tac-toe
click-dialog-2
multi-orderings
click-option
click-checkboxes-soft
click-color
login-user
click-collapsible
grid-coordinate
use-autocomplete
click-checkboxes-transfer
email-inbox-forward-nl
use-spinner
click-shape
click-collapsible-2
click-tab-2
click-tab-2-hard
email-inbox
email-inbox-forward-nl-turk
text-transform
click-menu
search-engine
click-checkboxes-large
email-inbox-nl-turk
choose-list
social-media
social-media-some
enter-date
click-shades
enter-time
simple-algebra
click-scroll-list
social-media-all
terminal
1
0
1
Residual
Comparison with SL SotA
use-autocomplete
count-shape
click-collapsible-2
click-tab-2
identify-shape
tic-tac-toe
navigate-tree
click-tab-2-hard
enter-date
click-widget
social-media
click-button
click-button-sequence
click-checkboxes
click-collapsible
click-color
click-dialog
click-dialog-2
click-link
click-option
click-tab
click-test
click-test-2
enter-password
enter-text
enter-text-dynamic
focus-text
focus-text-2
login-user
search-engine
social-media-all
choose-list
enter-time
guess-number
email-inbox
click-scroll-list
click-checkboxes-transfer
click-shape
click-menu
email-inbox-nl-turk
login-user-popup
click-checkboxes-soft
text-transform
multi-layouts
use-spinner
social-media-some
click-checkboxes-large
email-inbox-forward-nl-turk
click-shades
grid-coordinate
email-inbox-forward-nl
multi-orderings
simple-algebra
terminal
1
0
1
Residual
Comparison with RL SotA
guess-number
count-shape
use-autocomplete
click-collapsible-2
login-user-popup
multi-layouts
tic-tac-toe
click-tab-2
identify-shape
click-checkboxes-soft
click-tab-2-hard
navigate-tree
use-spinner
email-inbox-forward-nl-turk
enter-date
click-widget
email-inbox
email-inbox-nl-turk
click-button
click-button-sequence
click-collapsible
click-color
click-dialog
click-dialog-2
click-link
click-option
click-shades
click-tab
click-test
click-test-2
email-inbox-forward-nl
enter-password
enter-text
enter-text-dynamic
focus-text
focus-text-2
grid-coordinate
login-user
multi-orderings
search-engine
choose-list
click-checkboxes-transfer
click-checkboxes
click-shape
enter-time
social-media-some
click-menu
social-media
text-transform
click-checkboxes-large
simple-algebra
social-media-all
click-scroll-list
terminal
1
0
1
Residual
Comparison with SL + RL SotA
Figure 5: The task-level performance comparison with the state-of-the-art (SotA) baselines. The
y-axis represents the residual values, which are obtained by subtracting the performance of SotA
from our agent’s performance.
the InstructGPT + RLHF model. Additionally, we used GPT-3 and InstructGPT-3 models for ablation
studies. Furthermore, the InstructGPT + RLHF model (gpt-3.5-turbo) was employed to carry out
diverse reasoning tasks. All the models were obtained through the OpenAI API, and further details
can be found in AppendixA.1.
Evaluated tasks.
We employ a set of 55 tasks to enable fair comparisons with baselines, as previous
works are only evaluated on a subset of tasks consistently. Furthermore, to assess the performance of
models on challenging tasks, we have selected tasks that involve free-form language typing actions,
which have been reported to have an almost-zero success rate in previous works (e.g., terminal).
Notably, certain commonly evaluated tasks in prior works are excluded due to the excessive length of
HTML code for some UI components, which are described in Appendix A.2.
Metrics
Consistent with prior studies, our main evaluation criterion is the success rate, which
measures the ability of our agent to actually complete the assigned task. This rate is calculated as the
proportion of successful episodes, which are deﬁned as those in which the agent receives a positive
reward. We identiﬁed two modes of failure: the production of unexecutable actions and task failure.
When the agent generates an unexecutable action following the implicit RCI step, it fails immediately.
Moreover, an episode is considered unsuccessful when the agent, despite effectively executing the
plan generated, is unable to accomplish the task and thus receives no reward.
9

4.2.2
Outperforming baselines on MiniWoB++ task suite
We present Figure 4a which summarizes the average success rate of our agent and baseline models
over the MiniWoB++ benchmark. The results demonstrate signiﬁcant outperformance of our approach
over supervised learning models. Speciﬁcally, we observe a 41% higher score than the WebN-T5-
3B, which employs a ﬁnetuned large language model with 12K expert demonstration data. Our
approach also outperforms reinforcement learning approaches that require an order of magnitude
more interactions with the environment. Among all the baselines, our approach achieves the second
highest score. The sole model that surpasses our agent is the CC-Net, which involves co-training
of reinforcement learning and imitation learning. However, a direct comparison with CC-Net is
not possible since it uses dictionary-based typing actions. In other words, CC-Net selects text from
a predeﬁned list for typing actions in some tasks, while our approach is fully generative. Thus,
CC-Net (without dictionary-based action) in Figure 4a serves as our appropriate comparison and we
outperform it by a small margin (6%).
Another comparative analysis is performed to evaluate the performance of our agent in contrast to
the state-of-the-art agents in three categories, namely supervised learning, reinforcement learning,
and a combination of both. To facilitate a fair comparison, we speciﬁcally isolate LLM-based
state-of-the-art approaches, which share similarities with our approach to solving computer tasks.
The best performance achieved by each category is then aggregated, and the outcomes are presented
as SotA in Figure 4a. Our ﬁndings reveal that our agent surpasses SotA in supervised learning by 34%
and in reinforcement learning by 24%. Notably, our proposed RCI prompting method outperforms
the SotA LLM approach, even when the latter employs both ﬁnetuning and few-shot examples in
prompts. This outcome highlights the effectiveness of our approach in extracting vital knowledge for
computer tasks from language models. However, our agent underperforms in comparison to SotA in
the combination of supervised and reinforcement learning, which employs signiﬁcantly more expert
data and online interactions. We also provide task-level performance comparisons in Figure 5, where
tasks are arranged in ascending order based on the difference between our agent’s performance and
the baseline. We observed three main failure modes of our agent: (i) underperformance in tasks
that require long-horizon planning (e.g., guess-number, search-engine, use-spinner), (ii) difﬁculty
in selecting appropriate actions for tasks that require multi-step reasoning (e.g., tic-tac-toe, use-
autocomplete), and (iii) lower scores in tasks that rely on visual rendering of HTML code to solve the
task (e.g., count-shape). These failures are explained in more detail in Appendix D.
4.2.3
Lowest sample complexity
Figure 4b provides a comparative analysis of the total number of samples used in several models and
their mean performance. We begin by discussing CC-Net [23] model, which employs 2.4 million
expert demonstrations (equivalent to 6,300 hours) collected from 77 human participants across 104
tasks for behavior cloning. This amounts to an average of 23,076 demonstrations per task. In contrast,
the WebN-T5-3B [18] model uses 12,000 expert demonstrations to ﬁne-tune its pre-trained T5 model.
Rather than directly updating model parameters with demonstration data, our approach involves
integrating two to three demonstrations into the prompt for in-context learning, which biases the
model output without any parameter updates. This approach allows our agent to generalize to unseen
tasks with only a handful of demonstrations. Our results show that our agent achieved a higher success
rate than the baselines except CC-Net (SL + RL), requiring 120x fewer samples than WebN-T5-3B
and 11,000x fewer samples than CC-Net. Given the challenges of obtaining expert demonstrations for
computer tasks, our ﬁndings demonstrate the practicality of our approach in automating such tasks.
4.2.4
Ablating the groundings
This section examines the impact of grounding improvement on task success rates. We conduct
ablations to isolate the contributions of task, state, and agent grounding improvements by eliminating
RCI prompting at each stage. We categorize tasks by three different difﬁculty levels to provide a
more detailed understanding of the effects of grounding improvements across a diverse range of tasks.
We conducted a task grounding ablation by eliminating the plan sampling stage. This modiﬁcation
entails generating actions directly from the state, without the need for conditioning on a step-by-step
plan. State grounding is evaluated by directly applying the agent-grounding update to task-grounded
actions. Lastly, we ablate the implicit RCI of the agent grounding by letting the state-grounded
action be the ﬁnal output of the agent. Figure 6 illustrates the performance degradation resulting
10

from each ablation of grounding. Our results indicate that each grounding contribution is essential
to solving computer tasks, with each contributing almost equally to the overall success rate. The
reason for this is partially due to the fact that the three methods of improving grounding are not
mutually exclusive, but rather complementary, with one enhancement in grounding contributing to
multiple action groundings. Examples of cross-grounding improvement are provided in Appendix C.
Moreover, it has been observed that state grounding plays a crucial role in enabling an agent to use
relevant information during episodes, particularly in scenarios where the initial state does not offer
sufﬁcient information to accomplish the task, such as terminal task. Interestingly, task grounding
signiﬁcantly improves the success rate when a task requires a long-horizon action plan, such as
the click checkboxes large task. We also observe that agent grounding signiﬁcantly enhances the
feasibility of actions. Notably, in simpler tasks, the success rate decreases by 60% in contrast to the
baseline without the agent grounding. This ﬁnding is of particular signiﬁcance as it distinguishes
our work from prior investigations [1, 21], which employ additional trained model components. In
contrast, our study solely relies on the reasoning ability of language models.
Baseline
Without
task
grounding
Without
state
grounding
Without
agent
grounding
0.0
0.2
0.4
0.6
0.8
1.0
Success rate
Easy
Baseline
Without
task
grounding
Without
state
grounding
Without
agent
grounding
0.0
0.2
0.4
0.6
0.8
1.0
Success rate
Medium
Baseline
Without
task
grounding
Without
state
grounding
Without
agent
grounding
0.0
0.2
0.4
0.6
0.8
1.0
Success rate
Hard
Figure 6: Ablation analysis on the different types of grounding across tasks with varying degrees of
difﬁculty. The experimental design employs the use of InstructGPT-3 + RLHF model.
4.2.5
Ablating the language model
The performance of our agent is contingent on the quality of the underlying pre-trained language
models used, so enhancing language models can lead to an improvement in the agent’s performance.
In this section, we present a comparison of the agent’s performance using three distinct language
models: GPT-3, InstructGPT-3, and InstructGPT-3 + RLHF. Our objective is to investigate the
relationship between LLMs’ capability and their ability to solve MiniWoB++ tasks. The experimental
setting employed in Section 4.2.4 is replicated in this study. Figure 7 depicts the average success rate
of three language models on tasks of varying difﬁculty levels. Our results reveal that LLMs struggle
to effectively complete tasks without instruction ﬁne-tuning. This may be attributed to the absence
of intricate prompt engineering, as our observations have indicated that GPT-3 displays sufﬁcient
competence in comprehending HTML code, regular expressions, and engaging in reasoning.
InstructGPT-3
+RLHF
(Baseline)
InstructGPT-3
GPT-3
0.00
0.25
0.50
0.75
1.00
Success rate
Easy
InstructGPT-3
+RLHF
(Baseline)
InstructGPT-3
GPT-3
0.00
0.25
0.50
0.75
1.00
Success rate
Medium
InstructGPT-3
+RLHF
(Baseline)
InstructGPT-3
GPT-3
0.00
0.25
0.50
0.75
1.00
Success rate
Hard
Figure 7: Ablation study on different language models across tasks of varying degrees of difﬁculty.
5
Discussion
This work is part of a growing literature showing that LLMs might be all you need for hard decision-
making problems [59]. In contrast to imitation learning and reinforcement learning approaches,
LLMs can solve novel tasks in a zero-shot or few-shot manner, and don’t require task-dependent
11

expert data or a reward function. Furthermore, we expect that as the capabilities of LLMs and
foundation models increase, our method will naturally improve as well. However, we ﬁnd that
current capabilities of LLMs aren’t as powerful as task-dependent SL+RL approaches on some
computer tasks. Also, RCI is more expensive to run compared to approaches that just sample once
from the LLM. There are many avenues for future research in increasing the capacity of LLMs in
decision-making tasks. First, our experiments use LLMs on HTML code, but ideally methods based
on multimodal foundation models [13, 41, 2, 35] will be able to take actions based on text, images,
audio, and video as input [3, 15, 33, 55]. Second, the results presented in this paper all use pre-trained
LLMs. We expect the performance of our method to increase when using LLMs ﬁne-tuned to solve
computer tasks.
Importantly, current LLMs are poor at reasoning tasks, such as playing tic-tac-toe, because they do
not think ahead. Although RCI improves reasoning capabilities in LLMs, there exist much work
to be done on increasing the reasoning capabilities in LLMs. This will be crucial to accomplish
hard cognitive tasks on computers that require thinking ahead. Similar to other prompting-based
approaches for reasoning in LLMs, RCI can be viewed as using the LLM’s output to write to an
external memory, which is later retrieved to choose an action. LLMs with memory have been
demonstrated to be computationally universal [45], meaning that in principle all that is needed to
run arbitrary programs is the right prompt. Since RCI represents a basic version of this powerful
framework, we anticipate the development of more advanced RCI variations in the future. There
is a vast array of potential methods that repeatedly feed the output of particular prompts into the
LLM. For example, multiple different LLMs can simulate the information exchange between team
members in an organization. This would enable the merging of diverse perspectives to tackle complex
problems. In such a context, incorporating game theory and multi-agent systems research could
signiﬁcantly enhance the overall performance. Reinforcement learning could be used to discover
effective structures involving loops and prompts [64], either through human feedback or a given
reward function. This optimization process can be further reﬁned by exploring the space of potential
loop and prompt structures, identifying those that yield the best results, and ﬁne-tuning the model
accordingly [58].
6
Broader Impacts
Although the results presented in this paper are only on a research benchmark, if we extrapolate
forward the capabilities of these models and methods, we anticipate vast broader impacts that have
the potential to revolutionize numerous industries. By allowing LLMs to execute tasks on computers,
our approach can enhance the capabilities of AI assistants and automation tools. This could lead to
increased efﬁciency, reduced labor costs, and improved user experiences across any sector which
uses computers to do work. We are most excited about gains in productivity in science and education,
including AI research, which will lead to even faster development of new beneﬁcial technologies and
treatments.
However, there are many potential misuses and unintended consequences associated with allowing
these models to take actions in the world. Malicious actors may leverage LLMs to automate cyber-
attacks, manipulate information, or propagate disinformation on a large scale. Additionally, the
potential loss of jobs due to widespread automation could lead to economic disruption and increased
income inequality. There are also obvious security risks of running LLMs on computers (or even
virtual machines) such as prompt injection attacks. Perhaps most alarming, future LLMs taking
actions on computers may lead to catastrophic runaway chains of events, especially if LLMs are
integrated widely in the economy.
To mitigate these risks, it is crucial for researchers, policymakers, and industry leaders to work
together to establish regulations and ethical guidelines that govern the development and deployment
of such technologies. Ensuring transparency, accountability, and fairness in AI systems will be vital
in harnessing the beneﬁts while minimizing potential harm. We also believe that the time has come
where we as a research community must discuss possible ways to coordinate to slow down the pace
of developing highly-disruptive technology, if necessary.
12

Acknowledgement
This material is based upon work supported by the National Science Foundation under Grant #2127309
to the Computing Research Association for the CIFellows 2021 Project.
References
[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,
Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not
as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. Advances in Neural Information Processing Systems,
35:23716–23736, 2022.
[3] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon
Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching
unlabeled online videos. Advances in Neural Information Processing Systems, 35:24639–24654,
2022.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020.
[5] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece
Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artiﬁcial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[6] Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-
Yves Oudeyer. Grounding large language models in interactive environments with online
reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.
[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[8] Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv
preprint arXiv:2208.14271, 2022.
[9] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large
language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.
[10] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can GPT learn
in-context? Language models secretly perform gradient descent as meta optimizers. arXiv
preprint arXiv:2212.10559, 2022.
[11] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix
Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. In Second
Workshop on Language and Reinforcement Learning, 2022.
[12] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo
Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-Dickstein, et al. Language
model cascades. arXiv preprint arXiv:2207.10342, 2022.
[13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied
multimodal language model. arXiv preprint arXiv:2303.03378, 2023.
[14] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efﬁcient scaling of
language models with mixture-of-experts. In International Conference on Machine Learning,
pages 5547–5569. PMLR, 2022.
[15] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew
Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended em-
bodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track, 2022.
13

[16] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil˙e Lukoši¯ut˙e, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for
moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.
[17] Izzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook Choi, Manoj Tiwari, Honglak Lee, and
Aleksandra Faust. Environment generation for zero-shot compositional reinforcement learning.
Advances in Neural Information Processing Systems, 34:4157–4169, 2021.
[18] Izzeddin Gur, Oﬁr Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowd-
hery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding HTML with large
language models. arXiv preprint arXiv:2210.03945, 2022.
[19] Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to navigate
the web. In International Conference on Learning Representations, 2019.
[20] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
An empirical analysis of compute-optimal large language model training. Advances in Neural
Information Processing Systems, 35:30016–30030, 2022.
[21] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as
zero-shot planners: Extracting actionable knowledge for embodied agents. In International
Conference on Machine Learning, pages 9118–9147. PMLR, 2022.
[22] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,
Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied
reasoning through planning with language models. In 6th Annual Conference on Robot Learning,
2022.
[23] Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia,
Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. A
data-driven approach for learning to control computers. In International Conference on Machine
Learning, pages 9466–9482. PMLR, 2022.
[24] Taichi Iki and Akiko Aizawa. Do BERTs learn to use browser user interface? Exploring
multi-step tasks with uniﬁed vision-and-language berts. arXiv preprint arXiv:2203.07828, 2022.
[25] Sheng Jia, Jamie Ryan Kiros, and Jimmy Ba. DOM-Q-NET: Grounded RL on structured
language. In International Conference on Learning Representations, 2019.
[26] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
Large language models are zero-shot reasoners. In Advances in Neural Information Processing
Systems, 2022.
[27] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement
learning on web interfaces using workﬂow-guided exploration. In International Conference on
Learning Representations, 2018.
[28] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny
Zhou, and Andrew M Dai. Mind’s eye: Grounded language model reasoning through simulation.
In International Conference on Learning Representations, 2023.
[29] Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it
takes two to tango. arXiv preprint arXiv:2209.07686, 2022.
[30] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru,
Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al.
Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.
[31] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-
pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
[32] Rodrigo Nogueira and Kyunghyun Cho. End-to-end goal-driven web navigation. Advances in
Neural Information Processing Systems, 29, 2016.
[33] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi,
Sameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep?: Embodied
decision making using language guided world modelling. arXiv preprint arXiv:2301.12050,
2023.
14

[34] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your
work: Scratchpads for intermediate computation with language models. In Deep Learning for
Code Workshop ICLR 2022, 2022.
[35] OpenAI. Gpt-4 technical report, 2023.
[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to
follow instructions with human feedback. Advances in Neural Information Processing Systems,
35:27730–27744, 2022.
[37] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,
and Marco Tulio Ribeiro. ART: Automatic multi-step reasoning and tool-use for large language
models. arXiv preprint arXiv:2303.09014, 2023.
[38] Panupong Pasupat, Tian-Shun Jiang, Evan Liu, Kelvin Guu, and Percy Liang. Mapping natural
language commands to web elements. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 4970–4976, 2018.
[39] Panupong Pasupat and Percy Liang. Zero-shot entity extraction from web pages. In Proceedings
of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 391–401, 2014.
[40] Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.
Measuring and narrowing the compositionality gap in language models.
arXiv preprint
arXiv:2210.03350, 2022.
[41] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov,
Gabriel Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.
A generalist agent. Transactions on Machine Learning Research, 2022.
[42] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training
enables zero-shot task generalization. In International Conference on Learning Representations,
2022.
[43] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan
Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802,
2022.
[44] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach
themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.
[45] Dale Schuurmans. Memory augmented large language models are computationally universal.
arXiv preprint arXiv:2301.04589, 2023.
[46] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of
bits: An open-domain platform for web-based agents. In International Conference on Machine
Learning, pages 3135–3144. PMLR, 2017.
[47] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reﬂexion: an autonomous agent with
dynamic memory and self-reﬂection. arXiv preprint arXiv:2303.11366, 2023.
[48] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. CLIPort: What and where pathways for
robotic manipulation. In Conference on Robot Learning, pages 894–906. PMLR, 2022.
[49] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari,
Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using
deepspeed and megatron to train megatron-turing NLG 530b, a large-scale generative language
model. arXiv preprint arXiv:2201.11990, 2022.
[50] Shashank Srivastava, Oleksandr Polozov, Nebojsa Jojic, and Christopher Meek. Learning web-
based procedures by reasoning over explanations and demonstrations in context. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7652–7662,
2020.
[51] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented
language models. In International Conference on Learning Representations, 2023.
15

[52] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for
dialog applications. arXiv preprint arXiv:2201.08239, 2022.
[53] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mord-
vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
descent. arXiv preprint arXiv:2212.07677, 2022.
[54] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.
Self-
consistency improves chain of thought reasoning in language models. In International Confer-
ence on Learning Representations, 2023.
[55] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and
select: Interactive planning with large language models enables open-world multi-task agents.
arXiv preprint arXiv:2302.01560, 2023.
[56] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In
International Conference on Learning Representations, 2022.
[57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In
Advances in Neural Information Processing Systems, 2022.
[58] Mengjiao Sherry Yang, Dale Schuurmans, Pieter Abbeel, and Oﬁr Nachum. Chain of thought im-
itation with procedure cloning. Advances in Neural Information Processing Systems, 35:36366–
36381, 2022.
[59] Sherry Yang, Oﬁr Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foun-
dation models for decision making: Problems, methods, and opportunities. arXiv preprint
arXiv:2303.04129, 2023.
[60] Shunyu Yao, Howard Chen, John Yang, and Karthik R Narasimhan. Webshop: Towards scalable
real-world web interaction with grounded language agents. In Advances in Neural Information
Processing Systems, 2022.
[61] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan
Cao. React: Synergizing reasoning and acting in language models. In International Conference
on Learning Representations, 2023.
[62] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STaR: Bootstrapping reasoning
with reasoning. Advances in Neural Information Processing Systems, 35:15476–15488, 2022.
[63] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek
Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic
models: Composing zero-shot multimodal reasoning with language. In International Conference
on Learning Representations, 2023.
[64] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E. Gonzalez. TEM-
PERA: Test-time prompt editing via reinforcement learning. In The Eleventh International
Conference on Learning Representations, 2023.
[65] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting
enables complex reasoning in large language models. In The Eleventh International Conference
on Learning Representations, 2023.
16

Appendix
A
Experimental setup
A.1
Language models
In our evaluation, various pre-trained language models were used. RCI prompting on reasoning tasks
is evaluated using gpt-3.5-turbo, which is presented in Table 1 and Table 2. Our primary evaluation
on MiniWoB++ tasks is conducted using text-davinci-003, as shown in Figure 4a and Figure 5. We
also used davinci, text-davinci-002, and gpt-3.5-turbo for our ablation study on MiniWoB++ tasks.
For all model usage, a maximum token length of 128 and a temperature value of 0, indicating greedy
decoding, are used. All models are accessed through the OpenAI API between January 2023 and
March 2023.
Language model
# of parameters
Max. tokens
API provider
API name
GPT-3
175 B(∗1)
2,049
OpenAI API
davinci
InstructGPT-3
−(∗2)
4,097
OpenAI API
text-davinci-002
InstructGPT-3 + RLHF
−(∗2)
4,097
OpenAI API
text-davinci-003
InstructGPT-3 + RLHF
−(∗2)
4,096
OpenAI API
gpt-3.5-turbo
Table 3: Description of language models. (∗1) We identify the model size of GPT-3 by referring to the
ofﬁcial document that OpenAI provides (https://beta.openai.com/docs/model-index-for-researchers).
(∗2) The size of InstructGPT-based models remains undisclosed by its provider.
A.2
MiniWoB++ task selection
In order to ensure a fair and comprehensive evaluation, a subset of MiniWoB++ tasks we use in the
evaluation is selected from WebN-T5-3B [18], the most recent work on MiniWoB++ tasks, which
employs LLMs. However, certain tasks such as book-ﬂight, choose-date-easy, choose-date-medium,
choose-date, and click-pie have been excluded from our evaluation due to their HTML code exceeding
the maximum context length of language models. On the other hand, some of the challenging tasks
such as terminal and simple-algebra have been included in the evaluation. The choice of these tasks
is determined by the suboptimal performance of CC-Net [23], which currently represents the state-of-
the-art model in the ﬁeld. The purpose of this inclusion is to showcase the potential of leveraging
LLMs in computer tasks, in contrast to the conventional approaches of Supervised Learning (SL)
and Reinforcement Learning (RL). While our agent has not been evaluated on tasks that necessitate
additional actions, such as drag and copy & paste, we posit that their inclusion can be readily achieved
through the expansion of the actions space speciﬁcation within the prompts.
A.3
MiniWoB++ task selection for ablation studies
In ablation studies, the classiﬁcation of tasks is based on the success rate achieved by our agent in
performing them. We employ a subset of tasks at three levels of difﬁculty, as depicted in Table 4.
easy (1, 0.9]
click-shape
0.98
click-widget
0.98
enter-date
0.96
medium (0.9, 0.6]
click-checkboxes-soft
0.72
click-collapsible-2
0.62
click-tab-2
0.74
hard (0.6, 0]
click-tab-2-hard
0.56
count-shape
0.4
guess-number
0.2
Table 4: The tasks used in the ablation study are classiﬁed according to their level of difﬁculty.
17

A.4
Modiﬁcations on MiniWoB++ tasks
In Table 5, we outline several modiﬁcations that were incorporated into the MiniWoB++ benchmark
for the purpose of our evaluation with language models that have a limited context length.
Tasks
Modiﬁcations
social-media-all
We constrain the quantity of media components ranging from three to six.
social-media
social-media-some
email-inbox-forward-nl-turk
The quantity of randomly generated emails has been restricted to a range of three to six.
email-inbox-forward-nl
email-inbox-nl-turk
Table 5: Modiﬁcations on MiniWoB++ tasks.
B
Prompts for MiniWoB++ tasks
We have an autonomous computer control agent that can perform atomic instructions speciﬁed
by natural language to control computers. There are two types of instructions it can execute.
First, given the instruction that matches the regular expression, <type regex>, it can type a list of
characters via the keyboard. This instruction should specify the target keyboard input for the
agent to type. Before this typing instruction, you should ﬁrst locate the cursor by clicking the
input box with the click instruction.
Second, given the instruction that matches the regular expression, <press regex>, it can press a
speciﬁc key on the keyboard.
Third, given the instruction that matches the regular expression, <clickoption regex>, it can click
an option HTML element in a list with an XPath that is visible on the webpage. The target of
this instruction should be a valid XPath.
Fourth, given the instruction that matches the regular expression, <movemouse regex>, it can
move the mouse cursor on an HTML element with an XPath that is visible on the webpage.
Lastly, given the instruction that matches the regular expression, <clickxpath regex>, it can click
an HTML element with an XPath that is visible on the webpage. The target of this instruction
should be a valid XPath.
Table 6: Prompt to specify agent.
type regex = "^type\s.{1,}$"
press regex = "^press\s(enter|arrowleft|arrowright|arrowup|arrowdown|
backspace)$"
clickxpath regex = "^clickxpath\s.{1,}$"
clickoption regex = "^clickoption\s.{1,}$"
movemouse regex = "^movemouse\s.{1,}$"
Listing 1: Regular expressions used to specify admissible actions.
Based on the current plan and task, the next proper instruction should be `
Table 7: Prompt for task grounding.
Based on the current plan and task, the ﬁrst instruction should be `
Table 8: Prompt for the ﬁrst action task grounding.
18

Therefore, considering the output on the webpage, this instruction should be `
Table 9: Prompt for state grounding.
This action does not match the regular expressions. The updated instruction that matches one of
the regular expressions is `
Table 10: Prompt for agent grounding.
C
Cross grounding trajectories
task: click on the "ok" button on the webpage.
plan:
1. type "sit arcu nullam:"
2. clickxpath //button[@data-wob_ref="6"]
We have a history of instructions that have been already executed by the autonomous agent so
far.
No instruction has been executed yet.
Based on the current plan and the task, the ﬁrst instruction should be `type "sit arcu nullam:"`.
Therefore, considering the output on the webpage, this instruction for solving the task should be
`clickxpath //button[@data-wob_ref="6"]`.
Table 11: The state grounding optimizes the action plan.
task: Find the email by Clarinda and reply to them with the text "Vel id at.".
plan:
1. clickxpath //div[@class=’email-sender’ and text()=’Clarinda’]
2. clickxpath //span[@class=’email-reply’]
3. clickxpath //*[@id="reply-text"]
4. type Vel id at.
5. clickxpath //*[@id="send-reply"]
We have a history of instructions that have been already executed by the autonomous agent so
far.
0: clickxpath //div[@class=’email-sender’ and text()=’Clarinda’]
1: clickxpath //span[@class=’email-reply’]
2: clickxpath //*[@id="reply-text"]
Based on the current plan and the task, the next proper instruction after the history should be
`type Vel id at`. Therefore, considering the output on the webpage, this instruction should be
`type Vel id at.`
Table 12: The state grounding corrects actions to be grounded in the task. Although the target
of typing action in the action plan is "Vel id at," the generated task-grounded action lacked the
last punctuation mark. Nonetheless, the state grounding update was successful in remedying this
discrepancy, resulting in the state-grounded action being "Vel id at." Consequently, the state-grounded
action now matches the typing action speciﬁed in the action plan.
19

(a) click-checkboxes-soft
(b) click-collapsible-2
(c) click-dialog-2
Figure 8: Screenshots.
D
Failure modes on MiniWoB++ tasks
D.1
click-checkboxes-soft
The purpose of this task is to evaluate the agent if it effectively recognizes and selects English words
that exhibit semantic similarity to a given set of words. The task presents a signiﬁcant challenge due
to the inherent ambiguity of synonyms, which makes it difﬁcult for Language Model models (LLMs)
to accurately distinguish the appropriate list of words.
task: select words similar to scarlet, response, quiet, tragic from the list of words provided and
click the Submit button.
plan:
1. clickxpath //label[text() = ’answer’]/input
2. clickxpath //label[text() = ’quiet’]/input
3. clickxpath //label[text() = ’unhappy’]/input
4. clickxpath //label[text() = ’ﬂames’]/input
5. clickxpath //label[text() = ’vermillion’]/input
6. clickxpath //button[@id=’subbtn’]
Table 13: Example trajectory on click-checkboxes-soft. The agent attempts to select the word "ﬂame"
which is not a suitable word with similar meaning.
D.2
click-collapsible-2
A predominant reason for the failure in this task can be attributed to the erroneous selection of the
appropriate collapsible tab containing the target hyperlink. This could possibly arise from the limited
capacity of LLMs in comprehending HTML code.
task: ﬁnd and click on the link "eleifend" on the webpage.
plan:
1. clickxpath //*[@id="ui-id-1"]
2. clickxpath //span[@class=’alink’ and contains(text(), ’eleifend’)]
Table 14: Example trajectory on click-collapsible-2. The agent chooses a different tab that the link
"eleifend" is not located.
20

(a) click-menu
(b) click-tab-2
(c) count-shape
Figure 9: Screenshots.
D.3
click-dialog-2
The state-grounding process is occasionally inadequate for maintaining task-grounding of actions.
Speciﬁcally, in the click-dialog-2 task, grounding actions in the current state results in task-irrelevant
actions. Consequently, we opt to exclude the state grounding step for this particular task.
task: click the button in the dialog box labeled "x".
plan:
1. Clickxpath //div[@class=’ui-dialog-titlebar ui-corner-all ui-widget-header ui-helper-clearﬁx
ui-draggable-handle’]/button[@title=’Close’]
We have a history of instructions that have been already executed by the autonomous agent so
far.
No instruction has been executed yet.
Based on the current plan and task,
the ﬁrst instruction should be `clickxpath
//div[@class=’ui-dialog-titlebar ui-corner-all ui-widget-header ui-helper-clearﬁx ui-draggable-
handle’]/button[@title=’Close’]`. Therefore, considering the output on the webpage, this
instruction should be ` clickxpath //div[@class=’ui-dialog-buttonset’]/button[@title=’OK’]` to
click the button in the dialog box labeled "x".
Table 15: Example trajectory on click-dialog-2. The state grounding updates the task-grounded
actions to be irrelevant to the task.
D.4
click-menu
We ﬁnd that including explanatory trajectories in the few-shot examples can signiﬁcantly reduce
the number of required few-shot examples. Speciﬁcally, in the click-menu task, the selection of the
target component necessitates the mouse hovering over the expandable menu. Therefore, we provided
explanations regarding the mouse hovering technique in the few-shot examples.
D.5
click-tab-2-hard, click-tab-2
Our agent also encountered challenges in locating the appropriate tab containing the desired link.
This issue could possibly stem from LLM’s inability to understand HTML code, considering that our
agent can address click-tabs task where the direct indication of the relevant tab number is provided in
the utterance.
21

task: select Leonelle > Jane > Amabel from the HTML code provided.
plan:
1. movemouse //*[text()="Leonelle"] // we need to hover the mouse to expand the menu
2. movemouse //*[text()="Jane"] // we need to hover the mouse to expand the menu
3. clickxpath //*[text()="Amabel"]
task: select the option "Joye > Phylis" from the HTML code provided.
plan:
1. movemouse //*[text()="Joye"] // we need to hover the mouse to expand the menu
2. clickxpath //*[text()="Phylis"]
Table 16: Example trajectory on click-menu. Providing explanatory trajectory enhances the sample-
efﬁciency of few-shot examples.
task: switch between the tabs to ﬁnd and click on the link "nisl".
plan:
1. Clickxpath //*[@id="ui-id-2"] 2. Clickxpath //*[@class="alink" and contains(text(), "nisl")]
Table 17: Example trajectory on click-tab-2-hard. The agent cannot ﬁnd a tab that includes the target
link.
(a) guess-number
(b) identify-shapes
(c) login-pop-up
Figure 10: Screenshots.
D.6
count-shape
This task necessitates an agent to visually perceive rendered images on a canvas element and
enumerate the relevant shapes in accordance with the accompanying verbal expression. Unfortunately,
our agent encounters difﬁculties in tackling such tasks which entail the need for perceiving visual
representations, as it solely depends on HTML observation.
task: click the button with the number "2" on the webpage in order to answer the question "How
many large items are there?".
plan:
1. Clickxpath //button[text()="2"]
Table 18: Example trajectory on count-shape. The agent struggle to solve tasks that requires visual
rendering of HTML.
22

D.7
guess-number
When considering decision-making that involves a long-term perspective, our agent encounters
difﬁculties in generating actions that are grounded in the current task. For instance, in this guess-
number task, the plan generated by our agent involves the sequential typing of numbers from 0 to 9
in order to solve the task. However, due to the high number of steps required (over 50), our agent
occasionally produces actions that deviate from the original plan, ultimately resulting in irreparable
failure.
task: Guess the number between 0-9 and press Submit. Use the feedback below to ﬁnd the right
number.
plan:
1. clickxpath //*[@id="tt"]
2. type 0
3. clickxpath //*[@id="subbtn"]
4. clickxpath //*[@id="tt"]
5. press backspace
6. type 1
7. clickxpath //*[@id="subbtn"]
8. clickxpath //*
9. press backspace
10. type 2
11. clickxpath //*[@id="subbtn"]
12. clickxpath //*[@id="tt"]
13. press backspace
14. type 3
15. clickxpath //*[@id="subbtn"]
16. clickxpath //*[@id="tt"]
17. press backspace
18. type 4
19. clickxpath //*[@id="subbtn"]
20. clickxpath //*[@id="tt"]
21. press backspace
22. type 5
23. clickxpath //*[@id="subbtn"]
24. clickxpath //*[@id="tt"]
25. press backspace
26. ...
We have a history of instructions that have been already executed by the autonomous agent so
far.
0: clickxpath //*[@id="tt"]
1: type 0
2: clickxpath //*[@id="subbtn"]
3: press backspace
4: type 1
5: clickxpath //*[@id="subbtn"]
Based on the current plan and the task, the next proper instruction after the history should be
`type 2`.
Table 19: Example trajectory on guess-number. The agent fails to generate task-grounded action in
long-horizon decision making is required.
23

(a) multi-layouts
(b) tic-tac-toe
(c) use-autocomplete
Figure 11: Screenshots.
D.8
identify-shape
Our agent also encounters difﬁculties in producing accurate answers in the identify-shape task when
presented with numbers and characters. We ﬁnd that our agent fails to discern between letters and
numbers, which may be attributed to their shared initialization using the <text> HTML tag.
D.9
login-user-popup
This task involves the identiﬁcation of an optimal strategy for an agent to handle an unpredictable
pop-up window that emerges during a task. Due to the unexpected nature of the pop-up window,
pre-planning the closure of the window is not feasible. Our agent is designed to adapt the agent’s
actions to the current state, so it should generate an appropriate instruction to close the pop-up window
in the state-grounding step. Nevertheless, there are instances where it is unsuccessful in executing the
pop-up window closure, leading to task failure.
D.10
multi-layouts
The present task necessitates the completion of three input boxes with the corresponding inputs.
However, it has been observed that LaSC occasionally fails to differentiate the appropriate input box
for each input. This failure can be attributed to the LLM’s limited comprehension of HTML code,
which impedes its ability to associate the appropriate text label with the corresponding input box.
D.11
tic-tac-toe
We also examine the causes of the poor success rate in playing tic-tac-toe. Two distinct factors are
identiﬁed as responsible for its failure. The ﬁrst factor is that our agent is unable to adopt a defensive
strategy when its opponent is only one move away from victory and there is no immediate opportunity
to win the game. The second factor relates to the inability to consider the possibility of its attack
being blocked by the opponent. A case in point is the bottom-left illustration in Figure 12, where our
agent’s fourth move is ineffective since its diagonal direction is already obstructed by the opponent.
D.12
use-autocomplete
In use-autocomplete task, our agent demonstrated an ability to select words beginning with speciﬁc
characters. However, it struggles when it comes to selecting words that ended with particular
characters. This difﬁculty arose from the need to identify the correct word from the autocomplete list,
a process that required the agent to press the down arrow button the exact number of times needed to
locate the desired word within the list.
24

1
3
5
2
4
2
5
1
3
4
6
2
4
1
3
5
4
6
2
5
3
7
1
2
6
1
4
5
3
3
2
1
6
5
4
LASP
Opponent
1
6
7
2
5
3
4
Figure 12: Failure examples of tic-tac-toe task.
task: Enter an item that starts with "Se" and ends with "ia".
plan:
1. click the input ﬁeld (e.g., clickxpath //*[@id="tags"])
2. type the starting word (e.g., type Se)
3. press the down arrow key to select the word ends with "ia" (e.g., press arrowdown)
5. select the word (e.g., press enter)
6. click the submit button (e.g., clickxpath //*[@id="subbtn"])
Table 20: Example trajectory on use-autocomplete. The agent fails to identify how many times it has
to press the down-arrow key.
E
Additional results
TASK
Ours
WebN-
T5-3B
WebN-T5-3B
(no few-shot
examples)
CC-
Net
(SL +
RL)
CC-
Net
(RL)
CC-
Net
(SL)
Others
(SL +
RL)
SotA
(SL)
SotA
(RL)
SotA
(SL +
RL)
bisect-angle
n/a
n/a
n/a
0.97
1.00
0.29
0.80
0.29
1.00
0.97
book-ﬂight
n/a
0.00
0.00
0.87
0.00
0.00
1.00
0.00
1.00
0.87
chase-circle
n/a
n/a
n/a
0.93
0.93
0.80
1.00
0.80
0.93
1.00
choose-date-easy
n/a
0.03
0.05
0.99
0.99
0.42
n/a
0.42
0.99
0.99
choose-date-medium
n/a
0.00
0.00
0.99
0.02
0.26
n/a
0.26
0.02
0.99
choose-date
n/a
0.00
0.00
0.97
0.01
0.12
1.00
0.12
1.00
0.97
choose-list
1.00
0.26
0.14
0.99
0.99
0.19
0.26
0.26
0.99
0.99
circle-center
n/a
n/a
n/a
0.97
1.00
0.36
0.98
0.36
1.00
0.98
click-button-sequence
1.00
1.00
1.00
1.00
1.00
0.47
1.00
1.00
1.00
1.00
click-button
1.00
1.00
0.96
1.00
0.80
0.78
1.00
1.00
1.00
1.00
click-checkboxes-large
0.94
0.22
0.00
0.71
0.00
0.00
0.84
0.22
0.00
0.71
click-checkboxes-soft
0.72
0.54
0.43
0.95
0.12
0.04
0.94
0.54
0.12
0.95
click-checkboxes-transfer
1.00
0.63
0.34
0.99
0.55
0.36
0.64
0.63
0.55
0.99
click-checkboxes
1.00
0.96
0.84
0.98
0.45
0.32
1.00
0.96
1.00
0.98
click-collapsible-2
0.62
0.00
0.01
0.98
0.88
0.17
0.99
0.17
0.88
0.98
click-collapsible
1.00
0.00
0.01
1.00
1.00
0.81
1.00
0.81
1.00
1.00
click-color
1.00
0.27
0.23
1.00
1.00
0.82
1.00
0.82
1.00
1.00
click-dialog-2
1.00
0.24
0.35
1.00
1.00
0.88
1.00
0.88
1.00
1.00
click-dialog
1.00
1.00
1.00
1.00
1.00
0.95
1.00
1.00
1.00
1.00
click-link
1.00
1.00
0.96
0.99
0.94
0.59
1.00
1.00
1.00
1.00
click-menu-2
n/a
n/a
n/a
0.83
0.96
0.52
0.16
0.52
0.96
0.83
click-menu
1.00
0.37
0.38
0.94
0.48
0.22
0.13
0.38
0.48
0.94
click-option
1.00
0.87
0.78
0.99
0.78
0.21
1.00
0.87
1.00
1.00
click-pie
n/a
0.51
0.14
0.97
0.92
0.15
1.00
0.51
1.00
0.97
click-scroll-list
1.00
0.00
0.00
0.60
0.59
0.01
0.07
0.01
0.59
0.60
click-shades
1.00
0.00
0.00
1.00
0.02
0.04
0.99
0.04
0.02
1.00
click-shape
0.98
0.53
0.54
0.95
0.50
0.11
0.64
0.54
0.50
0.95
click-tab-2-easy
n/a
n/a
n/a
0.99
0.94
0.61
n/a
0.61
0.94
0.99
click-tab-2-hard
0.76
0.12
0.13
0.98
0.87
0.19
n/a
0.19
0.87
0.98
click-tab-2-medium
n/a
n/a
n/a
0.99
0.96
0.54
n/a
0.54
0.96
0.99
click-tab-2
0.74
0.18
0.09
0.98
0.91
0.27
1.00
0.27
1.00
0.98
25

click-tab
1.00
0.74
1.00
1.00
1.00
0.95
1.00
1.00
1.00
1.00
click-test-2
1.00
1.00
1.00
1.00
1.00
0.95
1.00
1.00
1.00
1.00
click-test-transfer
n/a
n/a
n/a
1.00
1.00
0.94
n/a
0.94
1.00
1.00
click-test
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
click-widget
0.98
1.00
0.97
1.00
1.00
0.56
1.00
1.00
1.00
1.00
copy-paste-2
n/a
n/a
n/a
0.63
0.00
0.01
0.00
0.01
0.00
0.63
copy-paste
n/a
n/a
n/a
0.79
0.00
0.04
0.00
0.04
0.00
0.79
count-shape
0.40
0.41
0.43
0.85
0.70
0.21
0.76
0.43
0.70
0.85
count-sides
n/a
n/a
n/a
1.00
1.00
0.74
0.30
0.74
1.00
1.00
drag-box
n/a
n/a
n/a
1.00
0.19
0.61
0.31
0.61
0.19
1.00
drag-cube
n/a
n/a
n/a
0.79
0.95
0.23
0.18
0.23
0.95
0.79
drag-item
n/a
n/a
n/a
1.00
0.00
0.61
n/a
0.61
0.00
1.00
drag-items-grid
n/a
n/a
n/a
0.98
0.00
0.05
0.01
0.05
0.00
0.98
drag-items
n/a
n/a
n/a
0.99
0.00
0.13
0.41
0.13
0.00
0.99
drag-shapes
n/a
n/a
n/a
0.99
0.23
0.26
0.92
0.26
0.23
0.99
drag-sort-numbers
n/a
n/a
n/a
0.97
0.00
0.11
0.66
0.11
0.00
0.97
email-inbox-delete
n/a
n/a
n/a
1.00
1.00
0.22
1.00
0.22
1.00
1.00
email-inbox-forward-nl-turk
0.94
0.33
0.09
1.00
0.00
0.00
n/a
0.33
0.00
1.00
email-inbox-forward-nl
1.00
0.60
0.09
1.00
0.00
0.00
n/a
0.60
0.00
1.00
email-inbox-forward
n/a
n/a
n/a
1.00
0.00
0.01
n/a
0.01
0.00
1.00
email-inbox-important
n/a
n/a
n/a
1.00
1.00
0.30
n/a
0.30
1.00
1.00
email-inbox-nl-turk
0.98
0.23
0.26
1.00
0.46
0.05
0.93
0.26
0.46
1.00
email-inbox-noscroll
n/a
n/a
n/a
1.00
0.48
0.13
n/a
0.13
0.48
1.00
email-inbox-reply
n/a
n/a
n/a
1.00
0.00
0.00
n/a
0.00
0.00
1.00
email-inbox-star-reply
n/a
n/a
n/a
1.00
0.47
0.11
n/a
0.11
0.47
1.00
email-inbox
0.98
0.38
0.21
1.00
0.58
0.09
0.99
0.38
0.58
1.00
enter-date
0.96
0.00
0.00
1.00
1.00
0.02
1.00
0.02
1.00
1.00
enter-password
1.00
0.97
0.92
1.00
0.01
0.02
1.00
0.97
1.00
1.00
enter-text-2
n/a
n/a
n/a
0.98
0.00
0.04
0.00
0.04
0.00
0.98
enter-text-dynamic
1.00
0.98
0.92
1.00
1.00
0.39
1.00
0.98
1.00
1.00
enter-text
1.00
0.89
0.99
1.00
1.00
0.35
1.00
0.99
1.00
1.00
enter-time
1.00
0.00
0.01
0.97
0.89
0.04
0.90
0.04
0.89
0.97
ﬁnd-midpoint
n/a
n/a
n/a
0.97
0.97
0.35
0.31
0.35
0.97
0.97
ﬁnd-word
n/a
n/a
n/a
0.88
0.00
0.05
0.00
0.05
0.00
0.88
focus-text-2
1.00
1.00
1.00
1.00
1.00
0.96
1.00
1.00
1.00
1.00
focus-text
1.00
1.00
1.00
1.00
1.00
0.99
1.00
1.00
1.00
1.00
grid-coordinate
1.00
0.49
0.42
1.00
0.02
0.66
1.00
0.66
0.02
1.00
guess-number
0.20
0.00
0.00
1.00
0.21
0.20
0.21
0.00
1.00
highlight-text-2
n/a
n/a
n/a
1.00
0.34
0.40
0.13
0.40
0.34
1.00
highlight-text
n/a
n/a
n/a
1.00
1.00
0.51
0.90
0.51
1.00
1.00
identify-shape
0.76
0.88
0.89
1.00
1.00
0.68
1.00
0.89
1.00
1.00
login-user-popup
0.68
0.72
0.40
1.00
0.10
0.02
n/a
0.72
0.10
1.00
login-user
1.00
0.82
0.64
1.00
0.00
0.00
1.00
0.82
1.00
1.00
moving-items
n/a
n/a
n/a
0.88
0.69
0.13
0.78
0.13
0.69
0.88
multi-layouts
0.72
0.83
0.48
1.00
0.00
0.00
1.00
0.83
0.00
1.00
multi-orderings
1.00
0.88
0.64
1.00
0.00
0.00
1.00
0.88
0.00
1.00
navigate-tree
0.86
0.91
0.99
0.99
0.94
0.32
1.00
0.99
1.00
0.99
number-checkboxes
n/a
n/a
n/a
0.99
0.00
0.00
0.16
0.00
0.00
0.99
read-table-2
n/a
n/a
n/a
0.94
0.00
0.00
0.00
0.00
0.00
0.94
read-table
n/a
n/a
n/a
0.97
0.00
0.01
0.00
0.01
0.00
0.97
resize-textarea
n/a
n/a
n/a
1.00
0.68
0.27
0.11
0.27
0.68
1.00
right-angle
n/a
n/a
n/a
0.98
0.98
0.26
0.38
0.26
0.98
0.98
scroll-text-2
n/a
n/a
n/a
1.00
1.00
0.88
0.96
0.88
1.00
1.00
scroll-text
n/a
n/a
n/a
0.96
0.00
0.04
0.00
0.04
0.00
0.96
search-engine
1.00
0.34
0.34
1.00
0.01
0.15
1.00
0.34
1.00
1.00
simon-says
n/a
n/a
n/a
0.00
0.00
0.02
0.28
0.02
0.00
0.28
simple-algebra
1.00
n/a
n/a
0.75
0.00
0.03
0.04
0.03
0.00
0.75
simple-arithmetic
n/a
n/a
n/a
0.86
0.00
0.38
0.07
0.38
0.00
0.86
social-media-all
1.00
0.00
0.00
0.75
0.00
0.00
1.00
0.00
1.00
0.75
social-media-some
0.90
0.02
0.00
0.85
0.02
0.01
0.42
0.02
0.02
0.85
social-media
0.98
0.21
0.24
0.90
0.02
0.03
1.00
0.24
1.00
0.90
terminal
1.00
n/a
n/a
0.00
0.00
0.00
0.00
0.00
0.00
0.00
text-editor
n/a
n/a
n/a
0.98
0.00
0.11
0.01
0.11
0.00
0.98
text-transform
0.80
n/a
n/a
0.60
0.10
0.19
0.00
0.19
0.10
0.60
tic-tac-toe
0.56
0.48
0.40
0.83
0.76
0.32
0.47
0.48
0.76
0.83
unicode-test
n/a
n/a
n/a
1.00
1.00
0.86
n/a
0.86
1.00
1.00
use-autocomplete
0.58
0.22
0.15
1.00
1.00
0.07
0.98
0.22
1.00
1.00
use-colorwheel-2
n/a
n/a
n/a
0.95
0.85
0.38
1.00
0.38
0.85
1.00
use-colorwheel
n/a
n/a
n/a
0.98
0.82
0.68
1.00
0.68
0.82
1.00
use-slider-2
n/a
n/a
n/a
0.95
0.00
0.03
0.15
0.03
0.00
0.95
use-slider
n/a
n/a
n/a
0.91
0.47
0.18
0.51
0.18
0.47
0.91
use-spinner
0.88
0.07
0.05
1.00
0.02
0.47
0.17
0.47
0.02
1.00
visual-addition
n/a
n/a
n/a
0.99
0.00
0.36
0.01
0.36
0.00
0.99
Table 21: Comprehensive task-level success rate evaluation of baseline models in MiniWoB++ tasks.
The performance of baseline models has been sourced from prior studies [23, 18]. The average
success rates of the tasks highlighted with gray color are shown in Figure 3. Combined result of
models proposed prior to CC-Net [23] is denoted as Others, which include [46, 27, 19, 25]. This
corresponds to Aggregated SotA (Augmented) baseline in previous works. We generously estimate
the performance of CC-Net (RL) based on their ﬁgures.
26

