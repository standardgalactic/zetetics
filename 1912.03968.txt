arXiv:1912.03968v1  [stat.ME]  9 Dec 2019
Estimating an Extreme Bayesian Network via Scalings
Claudia Kl¨uppelberg∗
Mario Krali∗
December 10, 2019
Abstract
Recursive max-linear vectors model causal dependence between its components by ex-
pressing each node variable as a max-linear function of its parental nodes in a directed
acyclic graph and some exogenous innovation. Motivated by extreme value theory, innova-
tions are assumed to have regularly varying distribution tails. We propose a scaling technique
in order to determine a causal order of the node variables. All dependence parameters are
then estimated from the estimated scalings. Furthermore, we prove asymptotic normality of
the estimated scalings and dependence parameters based on asymptotic normality of the em-
pirical spectral measure. Finally, we apply our structure learning and estimation algorithm
to ﬁnancial data and food dietary interview data.
AMS 2010 Subject Classiﬁcations: primary:
60G70;
62-09;
62G32; secondary:
65S05
Keywords: causal order, directed acyclic graph, extreme value statistics, graphical model, recur-
sive max-linear model, regular variation, structural equation model, structure learning.
1
Introduction
Human society is continuously faced with challenges arising from factors of both uncontrollable
and/or synthetic nature. The former is manifested through events such as natural disasters,
in particular climate extremes like heavy rainfall or storms, unusually high/low temperatures,
or river ﬂooding. Similarly, synthetic factors correspond to those catastrophes inﬂuenced by
human intervention, for instance industry ﬁre, terrorist attacks, or a ﬁnancial market crash.
Such events occur rarely in isolation, but are rather interconnected, and occur simultaneously
across certain instances; for example, ﬂoods disseminate through a river network, or extreme
losses occur across several ﬁnancial sectors. Such events make it necessary to not only understand
dependencies between rare events, but also their causal structure.
When modeling rare events, one faces by deﬁnition a limited amount of data. While extremes
in a univariate setting are well studied, multivariate extremes still remain a focus of present
research. This is partly due to the augmented dimensionality problem, which aﬀects crucially
non-parametric methods (see de Haan and Ferreira (2006), Chapter 7), but also by the lack of a
parametric family to characterize interdependencies (see Beirlant et al. (2004), Chapters 8, 9).
Recently, there has been interest in graphical models for modeling dependencies between ex-
treme risks, which brings not only a potential complexity reduction, but also allows for modelling
cause and eﬀect in the context of extreme risk analysis. The model we consider in the present
∗Center for Mathematical Sciences, Technische Universit¨at M¨unchen, 85748 Garching, Boltzmannstrasse 3,
Germany, e-mail: cklu@tum.de, mario.krali@tum.de
1

paper originates from Gissibl and Kl¨uppelberg (2018), where max-linear structural equation
models have been proposed and investigated. The underlying graphical structure of the model is
a directed acyclic graph (DAG), also called a Bayesian network. Identiﬁability and estimation of
recursive max-linar models are investigated in Gissibl et al. (2018). We refer to Lauritzen (1996)
and Diestel (2010) for details on graphical modeling and graph theory, respectively.
Some other methods for combining graphical modeling with extremes have been proposed
recently. In Segers (2019), Markov trees with regularly varying node variables are investigated
using the so-called tail chains. In Engelke and Hitz (2018), a new approach using conditional
independence relations between node variables is introduced, when considering undirected graph-
ical models for extremes. This work is based on the assumption of a decomposable graph as well
as the existence of density, which then leads to a Hammersley-Cliﬀord type factorization of the
latter into a lower dimensional setting. The method is applied to the estimation of ﬂood in the
Danube river network. A recursive max-linear model has been ﬁtted to data from the EURO
STOXX 50 Index in Einmahl et al. (2018), where the structure of the DAG is assumed to be
known.
High dimensions are a serious challenge of dependence modeling of extreme events, and as
a consequence most of the applications so far have focused on a lower dimensional setting. An
exception is Cooley and Thibaud (2019), who present a new approach to extract the dependence
structure from a regularly varying random vector. The authors propose the use of a dependence
summary similar to the extreme dependence measure from Larsson and Resnick (2012), which
can be considered an analogue to the covariance. Aiming at reducing the complexity, the authors
propose a decomposition technique alike that of the Principal Component Decomposition for
normal distributions. Other attempts aiming at dimension reduction of extremes involve Chautru
(2015), and Janssen and Wan (2019), who present clustering approaches, or Haug et al. (2015),
who propose a factor analysis for extremes.
In the present paper we develop a new structure learning and estimation algorithm for the
recursive max-linear model in Gissibl and Kl¨uppelberg (2018). Our approach is motivated by
Cooley and Thibaud (2019) and applies to regularly varying node variables, which is a com-
mon assumption for extreme risk modelling. Multivariate node distributions have heavy-tailed
marginals and are eponymous to those which lie in the domain of attraction of multivariate
Fr´echet distributions; see Resnick (1987), Section 5.4.2 (Proposition 5.15). We refer to Resnick
(1987, 2007) for further details on regular variation.
Our multivariate regular variation setting is similar to that in Gissibl et al. (2018), which
investigates the use of the tail dependence coeﬃcients matrix towards the recovery of a causal
order and identiﬁability of the max-linear coeﬃcient matrix. Their method has the severe draw-
back that the initial nodes have to be known. For instance, in a DAG with two nodes and one
edge there can only be one initial node, which can not be determined by the tail dependence
coeﬃcient as it is symmetric. This problem is to be encountered also in a DAG of larger size
with several initial nodes, thus being a serious disadvantage to ﬁnd a complete causal order.
In contrast, other than regular variation itself, our methodology is free of assumptions. More
recently, in a heavy-tailed setting, Gnecco et al. (2019) propose a method for identifying a causal
order from the estimated conditional means of the integral transforms of pairs of nodes.
For arbitrary recursive max-linear models, a diﬀerent identiﬁcation and estimation method
based on a generalized MLE can be found in Gissibl et al. (2018) and Kl¨uppelberg and Lauritzen
(2020). An extension of this method to models with observational noise has been investigated
2

in Buck and Kl¨uppelberg (2019). In these papers all innovations have to be independent and
identically distributed, which is stronger than the tail assumptions imposed by regular variation.
We develop a new non-parametric methodology aimed at applying recursive max-linear mod-
els to extreme phenomena in a multivariate regular variation setting. First, targeting the problem
of recovering a causal structure as a graphical model on a DAG, we propose a new technique via
the scaling parameters of multivariate marginal distributions. This scaling technique allows for
the manipulation of the dependence structure between extremes by simple scalar multiplication.
These manipulations then uncover speciﬁc parts of the spectral measure, which fully character-
ize the dependence structure of interest to pave the way for estimating the causal dependence
structure of the model. Second, we estimate the spectral measure empirically, where we focus
on the relevant parts for the estimation of the required scaling. Asymptotic properties of the
empirical spectral measure proven as an extension of a result of Larsson and Resnick (2012) lead
to consistent and asymptotically normal estimates of all dependence parameters.
The application of the proposed methodology to ﬁnancial data and to food dietary data
shows that the recursive max-linear model can model multivariate extremes from real-life data
with the goal of inferring causality for high risks.
Our paper is structured as follows. Preliminaries on graph theoretical terminology and reg-
ular variation, including the scaling parameter, are introduced in Section 2. Section 3 provides
relevant properties of recursive max-linear models with regularly varying node variables. In Sec-
tion 4 we show how the dependence structure of a recursive max-linear model can be identiﬁed
from the scaling parameters of the model. Section 5 prepares for the causal inference by apply-
ing the scaling technique to ﬁnd initial nodes as well as to reorder all other components into
generations. Section 6 deals with statistical inference of the model. We propose non-parametric
estimators of the relevant scalings, which also yield estimators of the dependence parameters.
This allows us to estimate a partial order of the nodes and, in particular, a well-ordered graphical
model on a DAG. We also show the asymptotic normality of the model dependence parameters.
Finally, Section 7 is dedicated to two applications, namely to a real world ﬁnancial data set of
industry portfolio returns, as well as food dietary interview data.
2
Preliminaries
2.1
Some graphical notation
Let D = (V,E) be a directed acyclic graph (DAG) with nodes V = {1,... ,d} and edges E =
{(j,i) ∶i ∈V and j ∈pa(i)}, where pa(i) are the parents of node i. Each node of D is associated
with a random variable, and dependence between two random variables can be represented via
an edge connecting the corresponding nodes; for background see Lauritzen (1996).
Throughout we use the following notation. A path pji ∶= [l0 = j →l1 →⋯→lm = i] from node
j to i has length ∣pji∣= m, and we summarize all paths from j to i in the set Pji.
For a node i with parents pa(i) we set Pa(i) = pa(i) ∪{i}, likewise, we denote by an(i)
the ancestors of i and set An(i) = an(i) ∪{i}. The ancestral set of some subset C ⊂V of
nodes is denoted by an(C) or An(C) = an(C) ∪C. We also work with the following two notions
throughout.
Deﬁnition 1. (i)
We call i ∈V an initial node, if pa(i) = ∅, and denote by V0 the set of all
initial nodes.
3

(ii)
In a DAG D, a generation of nodes is the set of all nodes that have a longest path of same
length from any initial node. Let G0 = V0, then the i-th generation of nodes is deﬁned by:
Gi = {k ∈V ∖∪
l<iGl ∶
max
pjk∈Pjk∶j∈V0∣pjk∣= i}.
◻
The following two auxiliary results provide some properties of this concept.
Lemma 1. In a DAG D there is no path between two nodes of the same generation.
Proof. Suppose that there exists a path pij in some generation Gk ⊂V , k ≥1 for nodes i,j ∈Gk
on D. A longest path from V0 to i would be of length k, say pti for some t ∈V0. Extend now the
same path along pij to get ptj = [t →⋯→i →⋯→j]. Clearly ptj is longer than pti, giving a
contradiction to j ∈Gk.
The next result proves useful; its proof is not diﬃcult and can be found in Krali (2018),
Lemma 3.3.
Lemma 2. Consider a DAG D = (V,E) with ∣V ∣= d, and the set V0 of initial nodes. Suppose
that D has l generations. Then for i ∈{1,... ,l}, 1 ≤l ≤d and k ∉Ge for e < i, we have k ∈Gi if
and only if for all j ∈∪
m≥iGm it holds that j ∉an(k).
Deﬁnition 2. A directed graph D = (V,E) is well-ordered, if for all i ∈V we have i < j for all
j ∈pa(i). We refer to such an order as a causal order.
◻
Note that we employ a reverse ordering than in Gissibl and Kl¨uppelberg (2018).
2.2
Multivariate Regular Variation
Considering max-linear models from an extreme risk perspective, we focus on node variables,
which are multivariate regularly varying. Throughout all random objects are deﬁned on a prob-
ability space (Ω,A,P).
Multivariate regular variation can be deﬁned in various ways, and we shall work with the
following two equivalent deﬁnitions (cf. Resnick (2007), Theorem 6.1).
Deﬁnition 3. [Multivariate regular variation]
(a) A random vector X ∈Rd
+ is multivariate regularly varying if there exists a sequence bn →∞
as n →∞such that
nP(X/bn ∈⋅)
v→νX(⋅),
n →∞,
where
v→denotes vague convergence in M+(Rd
+ ∖{0}), the set of non-negative Radon measures
on Rd
+ ∖{0}. The measure νX is called exponent measure of X.
(b) A random vector X ∈Rd
+ is multivariate regularly varying if for any choice of the norm ∥⋅∥
there exists a ﬁnite measure HX on the positive unit sphere Θd−1
+
= {ω ∈Rd
+ ∶∥ω∥= 1} and a
sequence bn →∞as n →∞such that for the polar representation (R,ω) ∶= (∥X∥,X/∥X∥) of
X,
nP[(R/bn,ω) ∈⋅)]
v→να × HX(⋅),
n →∞,
4

in M+((0,∞] × Θd−1
+
), dνα(x) = αx−α−1dx for some α > 0, and for Borel subsets C ⊆Θd−1
+
,
HX(C) ∶= νX({y ∈Rd
+ ∖{0} ∶∥y∥≥1,y/∥y∥∈C}).
The measure HX is called the spectral measure.
(c) If X satisﬁes the above deﬁnition, we write X ∈RV d
+ (α), and α is called the index of regular
variation.
◻
As explained in Theorem 6.5 of Resnick (2007), starting with an arbitrary vector X with
positive components, we can always standardize all marginals to X ∈RV d
+ (2) with normalizing
sequence as in (b) chosen as bn = √n. This implies that all scaling information is pushed into
HX.
Fix now α = 2, and the Euclidean norm ∥⋅∥2, such that the positive unit sphere is Θd−1
+
=
{ω ∈Rd
+ ∶∥ω∥2 = 1}. The following scaling parameters have been used in Cooley and Thibaud
(2019) for a dependence summary statistics.
Deﬁnition 4. Let X ∈RV d
+ (2) and consider its polar representation (R,ω) as in Deﬁnition 3(b)
such that ωi = Xi
R for i = 1,... ,d. For every 1 ≤i,j ≤d deﬁne
σ2
ij = σ2
Xij ∶= ∫Θd−1
+
ωiωjdHX(ω),
ω = (ω1,... ,ωd) ∈Θd−1
+
.
We abbreviate σi = σXi = σXii and call it the scaling or scaling parameter of Xi.
◻
The following auxiliary results are well-known and simple consequences of the deﬁnitions of
regular variation. For the sake of completeness, we provide short proofs.
Lemma 3. Let X ∈RV d
+ (2) and choose bn = √n.
(a)
Then limn→∞nP(Xi/√n > z) = z−2σ2
i .
(b)
Let HX be the spectral measure of X, then HX(Θd−1
+
) = ∑d
i=1 σ2
i .
Proof. (a) From the homogeneity of the exponent measure and its polar representation in Deﬁ-
nition 3(b) we obtain
lim
n→∞nP(Xi/√n > z) = νX({x ∈Rd
+ ∶
x
∥x∥2
∈Θd−1
+
,xi > z})
= ∫{ω∈Θd−1
+
} ∫{r>z/ωi} 2r−3drdHX(ω) = z−2σ2
i .
(b) We simply compute the total mass of the d-dimensional unit simplex Θd−1
+
:
HX(Θd−1
+
) = ∫Θd−1
+
dHX(ω) = ∫Θd−1
+
d
∑
i=1
ω2
i dHX(ω) =
d
∑
i=1∫Θd−1
+
ω2
i dHX(ω) =
d
∑
i=1
σ2
i .
Immediately from Deﬁnition 4 and Lemma 3 above we ﬁnd for i ∈{1,... ,d} that, if Xi has
scaling σi, then cXi has scaling cσi for every c > 0.
Remark 1. (i)
As HX is a ﬁnite measure, it can be normalised to a probability measure by
deﬁning
˜HX(⋅) ∶=
HX(⋅)
HX(Θd−1
+
).
5

(ii)
Deﬁne ω ∶= (ω1,... ,ωd) = (X1/R,... ,Xd/R). Let f∶Θd−1
+
→R+ be a continuous function.
Since f is compactly supported (on Θd−1
+
), by vague convergence we have
E ˜HX[f(ω)] ∶= lim
x→∞E[f(ω) ∣R > x] = ∫Θd−1
+
f(ω)d ˜HX(ω).
(2.1)
◻
For a simple assessment of the dependence structure of the components of a random vector,
various summary measures have been introduced; see e.g. Sections 8.2.7 and 9.5.1 of Beirlant et al.
(2004). We note that Deﬁnition 4 is a non-normalized version of the extreme dependence measure
(EDM), which is a bivariate dependence measure on the positive unit sphere Θd−1
+
and measures
the limit of conditional cross-moments in the radial components of two random variables. The
EDM has been introduced in Section 3 of Resnick (2004). A more reﬁned version can be found
in Propositions 3 and 4 in Larsson and Resnick (2012), where also more details on the EDM are
given.
Deﬁnition 5. [Extreme dependence measure (EDM)] Let X ∈RV d
+ (α). Then for any two
components Xi,Xj of X, setting (ωi,ωj) ∶= (Xi
R , Xj
R ), the EDM is given by
EDM(Xi,Xj) = lim
x→∞E[Xi
R
Xj
R ∣R > x] = E ˜HX[ωiωj] = ∫Θd−1
+
ωiωjd ˜HX(ω).
◻
3
Recursive Max-linear Models
Recursive max-linear models were introduced in Gissibl and Kl¨uppelberg (2018) and estimated
with diﬀerent methods in Gissibl et al. (2018); Gissibl et al. (2018); Kl¨uppelberg and Lauritzen
(2020). We summarize notation and results needed in the present paper.
A max-linear structural equation model X on a DAG D is deﬁned as
Xi ∶=
⋁
k∈pa(i)
cikXk ∨ciiZi,
i = 1,... ,d,
(3.1)
for independent random variables Z1,... ,Zd, which have support R+ and are atomfree, and edge
weights cik which are positive for all i ∈V and k ∈pa(i) ∪{i}. We call Z = (Z1,... ,Zd) with
these properties an innovations vector.
Deﬁne now the operator ×max between two matrices C ∈Rd×q
+
and D ∈Rq×l
+
by
(C ×max D)ij =
q
⋁
k=1
cikdkj,
i = 1,... ,d, j = 1,... ,l.
From Theorem 2.2 of Gissibl and Kl¨uppelberg (2018) we know that a max-linear structural
equation model X from (3.1) has a solution in terms of its innovations Z, which can be found
by a path analysis. For each path pji = [j →k1 →⋯→kl = i] of length l ≥1 from j to i deﬁne
the path weights d(pji) ∶= cjjck1j⋯cikl−1. Furthermore, deﬁne for i = 1,... ,d,
aij =
⋁
pji∈Pji
d(pji) for j ∈An(i),
aij = 0 for j ∈V ∖An(i),
aii = cii.
Then X can be written as the recursive max-linear (ML) vector:
Xi =
⋁
j∈An(i)
aijZj,
i = 1,... ,d.
(3.2)
The matrix A = (aij)i,j=1,...,d is called the ML coeﬃcient matrix. Furthermore, a path pji from j
to i such that aij = d(pji) is called max-weighted.
6

3.1
Regular Variation of a Recursive Max-Linear Vector
Let Z ∈Rd
+ be an innovations vector and A ∈Rd×d
+
a ML coeﬃcient matrix. Throughout this
paper we let the innovations vector Z ∈RV d
+ (α) have independent and standardized components;
i.e., nP(n−1/αZi > x) →x−α as n →∞for all i = 1,... ,d. According to Resnick (2007), p. 193f,
this is equivalent to the spectral measure of Z being a discrete measure on the basis vectors ei
for i = 1,... ,d with unit mass on each of the ei. Reformulating (3.2), the recursive ML random
vector has representation
X = A ×max Z.
(3.3)
If the innovations vector Z ∈RV d
+(α), then by simple calculations given e.g. in Proposition A.2
of Gissibl et al. (2018), see also Proposition 4.1 of Krali (2018), X ∈RV d
+(α) with discrete
spectral measure
HX(⋅) =
d
∑
k=1
∥ak∥αδ{ ak
∥ak∥}(⋅),
(3.4)
where ak = (a1k,... ,adk)⊺is the k-th column of A. Obviously, the entries of A are the dependence
parameters of X.
Using the representation of HX in (3.4) together with Remark 1 (ii) we obtain the following
lemma.
Lemma 4. Let X be a recursive ML vector as in (3.3). Let f ∶Θd−1
+
→R+ be continuous, and
deﬁne the radial components of X as ω = (ω1,... ,ωd) = (X1/R,... ,Xd/R). Then
E ˜HX[f(ω)] =
1
∑d
i=1∥ai∥α
d
∑
k=1
∥ak∥αf( a1k
∥ak∥,... , adk
∥ak∥).
For α = 2 and the Euclidean norm, the scalings of the recursive ML random vector (3.3) can
be expressed by the matrix A as follows.
Proposition 1. Let X = A ×max Z, where Z ∈RV d
+ (2) is an innovations vector, and A ∈Rd×d
+
.
Then σ2
ij = (AAT )ij. Moreover, every component Xk of X has scaling σ2
k = ∑d
i=1 a2
ki for k =
1,... ,d.
Proof. From Deﬁnition 4 and (3.4) we ﬁnd for i ≠j
σ2
ij = ∫Θd−1
+
ωiωjdHX(ω) =
d
∑
k=1
∥ak∥2
2
aik
∥ak∥2
ajk
∥ak∥2
=
d
∑
k=1
aikajk = (AAT )ij.
The calculation of squared scalings σ2
i is analogous.
Finally, we consider the standardized recursive ML random vector X from (3.3) by stan-
dardizing the ML coeﬃcient matrix.
Deﬁnition 6. [Standardized ML coeﬃcient matrix]
Deﬁne
¯A = (¯aij)d×d ∶= (
aα
ij
∑k∈An(i) aα
ik
)
1/α
d×d
= (
aα
ij
∑d
k=1 aα
ik
)
1/α
d×d
.
(3.5)
Then ¯A is referred to as standardized ML coeﬃcient matrix.
◻
7

Remark 2. Since the innovations vector Z is standardized, all scaling information is in ¯A:
Proposition 1 entails that the recursive ML vector X = ¯A ×max Z has components with squared
scalings σ2
i = ( ¯A ¯AT )ii = 1 for i = 1,... ,d.
◻
The following result has been proven in Lemma 2.1 of Gissibl et al. (2018).
Lemma 5. Assume that the DAG corresponding to the recursive ML vector X is well-ordered.
Then
¯ajj > ¯aij
for all
i ∈V
and
j ∈an(i).
We summarize all model assumptions used throughout the rest of the paper.
Assumptions:
(A1) The innovations vector Z ∈RV d
+ (2) has independent and standardized components.
(A2) We work with the Euclidean norm ∥⋅∥2.
(A3) The ML coeﬃcient matrix A is standardized as in eq. (3.5), such that the components of
X are standardized.
4
Identiﬁcation of the ML Coeﬃcient Matrix From Scalings
In this section we consider a recursive ML vector X = A×maxZ such that (A1)-(A3) are satisﬁed.
We show how to identify A from X, when X is a recursive ML vector on a well-ordered DAG;
i.e.,
X =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣
X1
X2
⋮
Xd
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦
= A ×max Z =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣
a11
a12
⋯
a1d
0
a22
⋯
a2d
⋮
⋮
⋱
⋮
0
0
⋯
add
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦
×max Z.
(4.1)
We identify A from the squared scalings of maxima over combinations of components of X. For
a set h ⊆{1,... ,d} we deﬁne
Mh ∶= max
i∈h (Xi).
(4.2)
We ﬁrst compute the relevant squared scalings.
Lemma 6. The random variable Mh is again max-linear, in particular Mh ∈RV 1
+ (2) with
squared scalings as follows:
(a) Let h ⊆{1,... ,d}, then
σ2
Mh
=
d
∑
k=1
(⋁
i∈h
a2
ik).
(4.3)
(b) If h = {1,... ,d}, then σ2
Mh = ∑d
k=1 a2
kk.
Proof. (a) Starting with Xi =
⋁
j=1,...,d
aijZj for i = 1,... ,d, we calculate:
Mh = max
i∈h (Xi) = ⋁
i∈h
⋁
j=1,...,d
aijZj =
⋁
j=1,...,d
(⋁
i∈h
aij)Zj.
8

By eq. (3.4) Mh is regularly varying. In order to compute the squared scaling σ2
Mh, we use the
same arguments as in Proposition 1 which yields (4.3).
(b) This follows directly from (a) in combination with Lemma 5.
We now illustrate the identiﬁcation of the ML coeﬃcient matrix by the following example.
Example 1. Let X be a recursive ML vector on a well-ordered DAG satisfying (A1)-(A3) such
that
X = A ×max Z =
⎡⎢⎢⎢⎢⎢⎢⎣
a11
a12
a13
0
a22
a23
0
0
a33
⎤⎥⎥⎥⎥⎥⎥⎦
×max Z.
Note that by standardization every row must have norm 1.
We ﬁrst compute the diagonal entries. By standardization, a2
33 = σ2
3 = 1. By Lemma 5 we
know that aii > aki for k < i. Let Mij for 1 ≤i,j ≤3 and M123 be deﬁned as in (4.2). From
Lemma 6 we obtain
σ2
M123
=
a2
11 + a2
22 + a2
33
=
a2
11 + a2
22 + 1,
σ2
M23
=
a2
21 ∨a2
31 + a2
22 + a2
33
=
0 + a2
22 + 1.
From this we ﬁrst ﬁnd a2
11 = σ2
M123 −σ2
M23. Similarly a2
22 = σ2
M23 −σ2
3.
The next step is to ﬁnd the remaining entries in the ﬁrst row of A, namely a12 and a13.
Proceeding with a12 we ﬁnd from Lemma 6 for M13 ﬁrst σ2
M13 = a2
11 + a2
12 + a2
33 = a2
11 + a2
12 + σ2
3,
which yields
a2
12 = σ2
M13 −σ2
3 −a2
11 = σ2
M13 + σ2
M23 −σ2
M123 −σ2
3.
Finally, we ﬁnd a13,a23, since the rows of A have norm 1.
◻
We now proceed by proving the correctness of the above recursion, which gives rise to Algo-
rithm 1 below.
Proposition 2. Let X be a recursive ML vector on a well-ordered DAG satisfying (A1)-(A3).
Then the following recursion yields the standardized ML coeﬃcient matrix A:
a2
dd = σ2
d = 1 and a2
ii = σ2
Mi,...,d −σ2
Mi+1,...,d
i = 1,... ,d −1,
(4.4)
a2
ij = σ2
Mi,j+1,j+2,...,d −σ2
Mj+1,j+2,...,d −
j−1
∑
k=i
a2
ik
i = 1,... ,d −2,j = i + 1,... ,d −1.
(4.5)
a2
id = σ2
i −
d−1
∑
k=i
a2
ik = 1 −
d−1
∑
k=i
a2
ik
i = 1,... ,d −1.
(4.6)
Proof. We ﬁrst show (4.4). From Lemma 6 we ﬁnd for i = 1,... ,d −1:
σ2
Mi,i+1,...,d =
d
∑
k=i
a2
kk
and
σ2
Mi+1,i+2,...,d =
d
∑
k=i+1
a2
kk,
which implies that a2
ii = σ2
Mi,...,d−σ2
Mi+1,...,d. For i = p, by standardization of A we have a2
dd = σ2
d = 1.
In order to prove (4.5) we compute ﬁrst:
σ2
Mi,j+1...,d =
j
∑
k=i
a2
ik +
d
∑
k=j+1
a2
kk
and
σ2
Mj+1,...,d =
d
∑
k=j+1
a2
kk,
9

which implies
σ2
Mi,j+1...,d −σ2
Mj+1,...,d =
j
∑
k=i
a2
ik.
(4.7)
Fix now i ∈{1,... ,d−1}. We proceed by induction over j. We start with the initial index j = i+1.
By (4.4) we know all aii for i = 1,... ,d, and by (4.7),
σ2
Mi,i+2...,d −σ2
Mi+2,...,d −a2
ii =
i+1
∑
k=i
a2
ik −a2
ii = a2
i,i+1.
By the induction hypothesis, suppose that we have found aij for all j ∈{i + 1,...,l −1}, where
i + 2 < l < d. Let j = l. Then, it is straightforward to see that
σ2
Mi,j+1...,d −σ2
Mj+1,...,d −
j−1
∑
k=i
a2
ik =
j
∑
k=i
a2
ik −
j−1
∑
k=i
a2
ik = a2
ij.
Equation (4.6) follows from the fact that A is standardized, hence, all rows have norm 1 (by
Remark 2, σ2
i = ∑d
k=i a2
ik = 1.)
The Algorithm corresponding to Proposition 2 reads as follows.
Algorithm 1 Computation of the ML Coeﬃcient Matrix A
1: procedure
2:
Set A = (0)d×d
3:
for i = 1,... ,d −1 do
4:
Compute σ2
Mi,i+1,...,d;σ2
Mi+1,...,d
5:
Set a2
ii = σ2
Mi,i+1,...,d −σ2
Mi+1,...,d
6:
a2
dd = σ2
d
7:
if i ∈{1,... ,d −2} do
8:
for j = i + 1,... ,d −1 do
9:
Compute σ2
Mi,j+1,j+2,...,d;σ2
Mj+1,j+2,...,d
10:
Set a2
ij = σ2
Mi,j+1,j+2,...,d −σ2
Mj+1,j+2,...,d −∑j−1
k=i a2
ik
11:
end for
12:
Set a2
id = σ2
i −∑d−1
k=i a2
ik
13:
end if
14:
Set a2
d−1,d = σ2
d−1 −a2
d−1,d−1
15:
end for.
In Proposition 2 we have shown that we can compute the diagonal entries of A from the
squared scalings σ2
M1,2,...,d,σ2
M2,3,...,d,... , σ2
Md−1,d,σ2
d by a recursion algorithm. Furthermore, we
have identiﬁed the non-diagonal entries of the i-th row of A from
(σ2
Mi,i+1,...,d,σ2
Mi,i+2,...,d,... ,σ2
Mi,d,σ2
i ),
i = 1,... ,d.
We summarize all these quantities into one column vector SM ∈Rd(d+1)/2
+
; i.e.,
SM ∶= (σ2
M1,2,...,d,σ2
M1,3,...,d,... ,σ2
M1,d,σ2
1,σ2
M2,3,...,d,σ2
M2,4,...,d,... ,σ2
M2,d,σ2
2,... ,σ2
Md−1,d,σ2
d−1,σ2
d)⊺. (4.8)
10

Consider the row-wise vectorized version of the squared entries of the upper triangular matrix
A, where we use A2 for the matrix with squared entries of A and its vectorized version
A2 ∶= (a2
11,... ,a2
1d,a2
22,... ,a2
2d,... ..,a2
d−1,d−1,a2
d−1,d,a2
dd)⊺.
(4.9)
Note that both vectors A2 and SM show a similar structure, built from row vectors with d,d −
1,... ,1 components, respectively; so both have d(d+1)/2 components. By means of Proposition 2
we show that A2 can be written as a linear transformation of SM.
Theorem 1. Let SM and A2 be as in (4.8) and (4.9), respectively. Then
A2
=
T SM,
(4.10)
where T ∶= (tuv)k×k ∈Rk×k for k = d(d + 1)/2 has non-zero entries in the rows corresponding to
the non-zero components a2
ij in the vector (4.9) given by
a2
ii ∶
tlii,lii = 1, tlii,li+1,i+1 = −1 for i = 1,... ,d −1;
a2
dd ∶
tlii,lii = 1 for i = d;
a2
ij ∶
tlij,lij = 1,tlij,lj+1,j+1 = −1,tlij,li,j−1 = −1,tlij,ljj = 1 for i < j ≤d −1;
a2
id ∶
tlid,lid = 1,tlid,li,d−1 = −1,tlid,ldd = 1 for i = 1,... ,d −1,
where lij = (j −d) + ∑i−1
k=0(d −k) for i = 1,...,d and j ≥i. All other entries of T are equal to zero.
Proof. (i)
From (4.4) we know that
a2
ii = σ2
Mi,...,d −σ2
Mi+1,...,d, i = 1,... ,d −1, and a2
dd = σ2
d = 1
(ii)
Starting from (4.5) we show by induction that for i = 1,... ,d −2 and j = i + 1,... ,d −1,
a2
ij = (σ2
Mi,j+1,j+2,...,d −σ2
Mj+1,j+2,...,d) −(σ2
Mi,j,...,d −σ2
Mj,...,d).
(4.11)
For j = i+1 we clearly have that a2
i,i+1 = (σ2
Mi,i+2,...,d −σ2
Mi+2,...,d)−(σ2
Mi,i+1,...,d −σ2
Mi+1,...,d). Suppose
now that this holds for all i < j < d −2. We show now that it holds for j + 1. More speciﬁcally,
a2
i,j+1 = (σ2
Mi,j+2,j+3,...,d −σ2
Mj+2,j+3,...,d) −
j
∑
k=i
a2
ik
= (σ2
Mi,j+2,j+3,...,d −σ2
Mj+2,j+3,...,d) −
j
∑
k=i
[(σ2
Mi,k+1,k+2,...,d −σ2
Mk+1,k+2,...,d) −(σ2
Mi,k,...,d −σ2
Mk,...,d)]
= (σ2
Mi,j+2,j+3,...,d −σ2
Mj+2,j+3,...,d) −(σ2
Mi,j+1,...,d −σ2
Mj+1,...,d),
where the last equality is due to the telescoping sum after noting that σ2
Mi,i,...,d = σ2
Mi,...,d.
(iii) Similar to (ii), for aid with i < d,
a2
id = σ2
i −(σ2
Mi,d −σ2
d),
(4.12)
while for i = d we obtain again a2
dd = σ2
d.
(iv)
The results in (i)-(iii) show already the linearity between the vectors A2 and SM. It
remains to construct the matrix T = (tuv)k×k for k = d(d + 1)/2 such that A2 = TSM. We start
11

by renumbering the vector components in A2 and replacing the double indices ij for i = 1,... ,d
and j ≥i by
lij = (j −d) +
i−1
∑
k=0
(d −k),
j ≥i.
(4.13)
Then the vector in (4.9) becomes (a2
1,a2
2,... ,a2
d(d+1)/2). Moreover, (4.13) maps ii into lii and
li,i+k = lii + k for i = 1,... ,d and 1 ≤i + k ≤d.
Also notice that for all i = 1,... ,d, by the structure of SM, its lij-th component is Slij =
σ2
Mi,j+1,...,d for i ≤j < d, and Slid = σ2
i .
(v)
We construct now T, where by (i)-(iii) T contains many zeros, and we focus on the
non-zero entries.
Since a2
ii becomes a2
lii for i = 1,... ,d and, by the structure of SM, the l11,... ,ldd-th com-
ponents of SM are σ2
M1,2,...,d,σ2
M2,3,...,d, ... ,σ2
Md−1,d,σ2
Md = σ2
d, respectively, in each lii-th row of
T there must be a 1 on the diagonal; i.e. tlii,lii = 1. Furthermore, tlii,li+1,i+1 = −1, and all other
entries in this row are 0.
Similarly, we ﬁnd the other non-zero entries by representation (4.11) and (4.12).
Example 2. We illustrate the linear transformation (4.10) for d = 4, which clariﬁes the structure
also for higher dimensions. For a recursive ML vector with 4 nodes, by (4.11) and (4.12) the
identity A2 = TSM becomes
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a2
11
a2
12
a2
13
a2
14
a2
22
a2
23
a2
24
a2
33
a2
34
a2
44
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0
0
−1
0
0
0
0
0
−1
1
0
0
1
0
0
−1
0
0
0
−1
1
0
0
0
0
1
0
−1
0
0
−1
1
0
0
0
0
0
1
0
0
0
0
1
0
0
−1
0
0
0
0
0
0
−1
1
0
1
0
−1
0
0
0
0
0
−1
1
0
0
1
0
0
0
0
0
0
0
1
0
−1
0
0
0
0
0
0
0
−1
1
1
0
0
0
0
0
0
0
0
0
1
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
×
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
σ2
M1,2,3,4
σ2
M1,3,4
σ2
M1,4
σ2
1
σ2
M2,3,4
σ2
M2,4
σ2
2
σ2
M3,4
σ2
3
σ2
4
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
◻
5
Reordering the Vector Components
In Section 4 we have assumed that the DAG underlying the recursive ML vector X is well-
ordered. In a real life situation this will rarely be the case, and the components of X have to be
reordered. In this section we use again the scalings for ﬁnding a causal order of the components
of X. This is achieved by ﬁrst identifying the initial nodes, which can be ordered arbitrarily
within all initial nodes. The same applies for every following generation: within each generation
the order is arbitrary. All such obtained partial orders correspond to equivalent well-ordered
DAGs and we construct one representative DAG by the method as follows.
We start with an auxiliary result which ensures that the recursive ML vector X = A ×max Z
is invariant with respect to column permutations of the ML coeﬃcient matrix A.
12

Lemma 7. Let X ∈Rd
+ be a recursive ML vector with ML coeﬃcient matrix A ∈Rd×d
+
and
innovations vector Z ∈Rd
+. Let π be a permutation of the columns of A. Then Xπ = X.
Proof. Denote by π ∶{1,... ,d} →{1,... ,d} an arbitrary permutation of the columns of A, and
notice that an arbitrary component i ∈{1,... ,d} of X is given by
Xi =
⋁
k=1,...,d
aikZk =
⋁
π(k)=1,...,d
aiπ(k)Zπ(k) =
⋁
k′=1,...,d
aik′Zk′ =∶Xπ
i
and, therefore, Xπ = X.
Since by Lemma 7 the distribution of X is invariant with respect to column permutations, we
can assume that an arbitrarily ordered recursive ML vector X∗= (X1∗,... ,Xd∗) = A∗×max Z,
needs only row permutations in A, denoted by ν ∶(1∗,... ,d∗) →(1,... ,d), to become well-
ordered:
ν ∶A∗=
⎡⎢⎢⎢⎢⎢⎢⎣
a1∗1
⋯
a1∗d
⋮
⋱
⋮
ad∗1
⋯
ad∗d
⎤⎥⎥⎥⎥⎥⎥⎦
→Aν = A =
⎡⎢⎢⎢⎢⎢⎢⎣
a11
⋯
a1d
⋮
⋱
⋮
0
⋯
add
⎤⎥⎥⎥⎥⎥⎥⎦
.
We refer to entries of the matrix A∗as ai∗k and to entries from the row-permuted matrix Aν
as aik ∶= aν(i∗)k, corresponding to a reordered vector Xν = X in distribution on a well-ordered
DAG.
5.1
Reordering the Vector Components: Finding the Initial Nodes
In order to ﬁnd an initial node, we ﬁx one node which we want to investigate and extend the
notation from (4.2) to maxima over a d-tuple of partly scaled random variables: for a > 0 we
deﬁne for m ∈{1,... ,d},
M−m,am ∶= max(X1,... ,Xm−1,aXm,Xm+1,... ,Xd).
(5.1)
By Lemma 6, also M−m,am ∈RV+(2). The following theorem provides necessary and suﬃcient
conditions for the identiﬁcation of initial nodes.
Theorem 2. Let X∗= (X1∗,... ,Xd∗) be an arbitrarily ordered recursive ML vector with ML
coeﬃcient matrix A∗satisfying (A1)-(A3). Then the following holds.
(a) If m∗∈{1∗,... ,d∗} is an initial node of the recursive ML vector Xν in a well ordered DAG,
then for all scalars a > 1 it holds that
σ2
M−m∗,am∗−σ2
M1,...,d = a2 −1.
(5.2)
(b) If there exists a scalar a > 1, such that for m∗∈{1∗,... ,d∗} eq. (5.2) holds, then m∗is an
initial node of the recursive ML vector Xν in a well ordered DAG.
Proof. (a) Let Xm∗be the component of X∗such that m∗is an initial node. W.l.o.g. we may
set Xν(m∗) = Xd. By the representation (4.1), and given the standardized scalings, we know that
aν(m∗),1 = ⋅⋅⋅= aν(m∗),d−1 = 0, and aν(m∗),d = 1. By Lemma 6(b), for some a > 1, we compute
σ2
M1,...,d = a2
11 + ⋅⋅⋅+ a2
d−1,d−1 + 1
and
σ2
M−m∗,am∗= a2
11 + ⋅⋅⋅+ a2
d−1,d−1 + a2.
Taking the diﬀerence yields (5.2).
(b)
We prove this by contradiction. Let ν be a row permutation that transforms X∗into a
13

recursive ML vector Xν on a well-ordered DAG, and suppose that for some non-initial node,
say k∗∈{1∗,... ,d∗}, there exists some a > 1 such that
σ2
M−k∗,ak∗= σ2
M1,...,d + a2 −1.
(5.3)
Given that a recursive ML vector can have more than one initial node, w.l.o.g. assume that
there are 0 < l < d initial nodes. Since k∗is not an initial node, we know by Deﬁnition 2 that
ν(k∗) ≤d −l. Furthermore, as k∗must have an ancestor, there exists some node j > ν(k∗)
for j ∈{1,...,d} ∖{ν(k∗)}, such that j ∈an(ν(k∗)) and, hence, aν(k∗)j > 0. Since ν is a row
permutation, which permutes X∗into a recursive ML vector on a well ordered DAG, we know
that there exists some j∗∈{1∗,...,d∗} ∖{k∗}, such that ν(j∗) = j. By Lemma 5, and since
j > ν(k∗), and j ∈an(ν(k∗)), it follows that ajj > aν(k∗)j > 0. This implies that
σ2
M−ν(k∗),aν(k∗) =
j=ν(k∗)−1
∑
j=1
a2
jj + a2a2
ν(k∗),ν(k∗) +
d
∑
j=ν(k∗)+1
(a2a2
ν(k∗)j) ∨a2
jj
σ2
M1,...,d =
j=ν(k∗)−1
∑
j=1
a2
jj + a2
ν(k∗),ν(k∗) + ⋅⋅⋅+ a2
dd.
The diﬀerence gives
σ2
M−ν(k∗),aν(k∗) −σ2
M1,...,d = (a2 −1)a2
ν(k∗),ν(k∗) +
d
∑
j=ν(k∗)+1
((a2a2
ν(k∗)j) ∨a2
jj −a2
jj).
(5.4)
Next, for the summands in the sum on the right-hand side, following Lemma 5, we obtain
(a2a2
ν(k∗)j) ∨a2
jj −a2
jj =
⎧⎪⎪⎨⎪⎪⎩
a2a2
ν(k∗)j −a2
jj < (a2 −1)a2
ν(k∗)j,
if a2a2
ν(k∗)j > a2
jj
0,
else.
(5.5)
This implies
d
∑
j=ν(k∗)+1
(a2a2
ν(k∗)j ∨a2
jj −a2
jj) <
d
∑
j=ν(k∗)+1
(a2 −1)a2
ν(k∗)j,
which, when combined with eq. (5.4), yields the inequality
σ2
M−ν(k∗),aν(k∗) −σ2
M1,...,d < (a2 −1)(a2
ν(k∗),ν(k∗) +
d
∑
j=ν(k∗)+1
a2
ν(k∗)j) = a2 −1.
However, this is a contradiction to eq. (5.3).
5.2
Reordering the Vector Components: Finding the Descendants
Once we have identiﬁed the initial nodes of the recursive ML vector X, we provide a necessary
and suﬃcient criterion for identifying the causal order of the descendants.
We proceed iteratively by identifying every new generation in the DAG. Suppose we have
found all nodes which belong to a certain number of generations, and that there are h ≤d −1
such nodes which we have ordered as d,d −1,... ,d −h + 1. Let Xν−1(d),... ,Xν−1(d−h+1) be the
corresponding components in the arbitrarily ordered recursive ML vector X∗.
The next logical step is to investigate, whether m∗∈{1∗,... ,d∗}∖{ν−1(d),... , ν−1(d−h+1)},
belongs to the next generation of nodes in the causal order. Deﬁne h ∶= {ν−1(d),... ,ν−1(d−h+1)}
14

and let hc contain all other components. Then we take the maximum over a d-tuple of partly
scaled random variables: for a > 0 deﬁne
Mha,m∗a,{h∪{m∗}}c ∶= max(aXν−1(d),... ,aXν−1(d−h+1),aXm∗,
max
k∗∉{h∪{m∗}}Xk∗).
(5.6)
Theorem 3. Let X∗= (X1∗,... ,Xd∗) be an arbitrarily ordered recursive ML vector with ML
coeﬃcient matrix A∗satisfying (A1)-(A3). Let h = {ν−1(d),... ,ν−1(d −h + 1)} be the ﬁrst h
nodes of the recursive ML vector Xν of a well-ordered DAG, which have already been ordered.
Then the following holds:
(a) If m∗∉h has no ancestors in hc, then for all scalars a > 1 it holds that
σ2
Mha,m∗a,{h∪{m∗}}c −σ2
M1,...,d = (a2 −1)σ2
Mh,m∗.
(5.7)
(b) If there exists a scalar a > 1 such that (5.7) holds, then we identify m∗∉h as the (h + 1)-th
node.
Proof. (a) W.l.o.g. let m∗be a node such that ν(m∗) = d −h. Consider the squared scaling of
Mha,m∗a,{h∪{m∗}}c as in (5.6), and M1,...,d. By representation (4.1) and Lemma 6, and following
similar steps as in the proof of Theorem 2(i), we ﬁnd
σ2
Mha,m∗a,{h∪{m∗}}c −σ2
M1,...,d = a2(
d
∑
i=d−h+1
a2
ii + a2
ν(m∗),ν(m∗)) +
d−h−1
∑
j=1
a2
jj −(
d
∑
i=1
a2
ii)
= (a2 −1)(
d
∑
i=d−h
a2
ii) = (a2 −1)σ2
Mh,m∗.
Therefore, (5.7) is satisﬁed.
(b) Suppose now that for Xm∗(m∗∉h) there exists some a > 1 such that
σ2
Mha,m∗a,{h∪{m∗}}c −σ2
M1,...,d = (a2 −1)σ2
Mh,m∗.
(5.8)
We have the following system of equalities:
σ2
M1,...,d =
d
∑
i=1
a2
ii;
σ2
Mh,m∗=
d
∑
i=d−h+1
a2
ii +
d−h
∑
j=ν(m∗)
a2
ν(m∗)j;
σ2
Mha,m∗a,{h∪{m∗}}c = a2(
d
∑
i=d−h+1
a2
ii + a2
ν(m∗),ν(m∗)) +
d−h
∑
j=ν(m∗)+1
((a2a2
ν(m∗)j) ∨a2
jj) +
ν(m∗)−1
∑
j=1
a2
jj;
σ2
Mha,m∗a,{h∪{m∗}}c −σ2
M1,...,d = (a2 −1)(
d
∑
i=d−h+1
a2
ii + a2
ν(m∗),ν(m∗)) +
d−h
∑
j=ν(m∗)+1
((a2a2
ν(m∗)j) ∨a2
jj −a2
jj).
The summands in the last summation are non-negative.
For ν(m∗) = d −h we can take m∗as the (h + 1)-th node. Then the diﬀerence is equal to
(a2 −1)σ2
Mh,m∗. Similarly, if aν(m∗)j = 0 for ν(m∗) + 1 ≤j ≤d −h, then this implies that ν−1(j) ∉
an(m∗) and, thus, that m∗can be chosen as the (h + 1)-th node.
Suppose now that ν(m∗) < d −h, and aν(m∗)j > 0 for some ν(m∗) + 1 ≤j ≤d −h. Then, by
Lemma 5 and since a > 1, a bound similar to that in (5.5) gives
d−h
∑
j=ν(m∗)+1
(a2a2
ν(m∗)j ∨a2
jj −a2
jj) <
d−h
∑
j=ν(m∗)+1
(a2 −1)a2
ν(m∗)j
15

which contradicts (5.8), since
σ2
Mha,m∗a,{h∪{m∗}}c −σ2
M1,...,d < (a2 −1)σ2
Mh,m∗.
Therefore we have that either ν(m∗) = d −h, or aν(m∗)j = 0 for ν(m∗) + 1 ≤j ≤d −h. In both
cases m∗can be chosen as the (h + 1)-th node.
One of the consequences of the proof of Theorem 3 provides a criterion, when two or more
components of X are neither descendants nor ancestors of one another in a DAG.
Corollary 1. Let X be as in Theorem 3 and suppose that we have found the ﬁrst h nodes.
If σ2
Mha,m∗a,{h∪{m∗}}c −σ2
M1,...,d = (a2 −1)σ2
Mh,m∗for m∗∈{i∗,j∗} ∩hc and some a > 1, then
ai∗j∗= aj∗i∗= 0.
As another direct consequence of Theorem 3 we obtain the following corollary.
Corollary 2. Let X be as in Theorem 3 and suppose that we have found the ﬁrst h nodes. Let
Xi∗,Xj∗be such that Xi∗is the (h + 1)-th node, while Xj∗belongs to a diﬀerent generation.
Deﬁne for m∗∈{1∗,... ,d∗} and a > 1
∆m∗∶= σ2
Mha,m∗a,{h∪{m∗}}c −σ2
M1,...,d −(a2 −1)σ2
Mh,m∗.
Then ∆i∗> ∆j∗.
Example 3. [The reordering algorithms for a 10 nodes model]
We assess the performance of the reordering procedure based on Theorems 2 and 3. We assume
that the recursive ML vectors have standard Fr´echet(2) components, which allows us to estimate
the scalings by the standard MLE given for Mh by (see Krali (2018), Section 5.4 for details)
ˆσ2
Mh = ( 1
n
n
∑
ℓ=1
1
m2
hℓ
)
−1
,
where mhℓis the empirical maximum ∨i∈hXℓi for the ℓ-th observation. We consider the 10-
nodes DAG depicted in Figure 1. All diagonal entries of the edge weight matrix C10 ∈R10×10
+
(see (3.1)) are set to cii = 1 and the squares of the non-diagonal non-zero entries of the upper-
triangular matrix are drawn from a discrete uniform distribution over {2/1,2/2,... ,2/8}. We
have chosen edge weights cij larger than 1, equal to 1, and smaller than 1 to capture amplifying
and downsizing risk in the network.
10
8
9
7
6
5
4
3
2
1
Figure 1: DAG with 10 nodes.
16

To perform the reordering we turn both Theorem 2 and Theorem 3 into Algorithm 2 and
Algorithm 3, respectively. For every node, say i∗, Algorithm 2 checks the criterion in Theorem 2
(see Line 4 below). The diﬀerence between σ2
M−ν(i∗),aν(i∗) −σ2
M1,...,d and a2 −1 is entered into the
i∗-th- component of the vector ∆. Finally, the vector N is ﬁlled with non-zero components only
for those nodes, which satisfy the criterion of Theorem 2. Algorithm 3 follows a similar logic.
When estimating the scalings, then the ˆ∆i∗-s in Algorithms 2 and 3 are a.s. diﬀerent from
0. Hence, both algorithms are adapted to allow for small bounds to both quantities, which have
to be chosen appropriately. We are interested in checking if the ﬁnal order identiﬁes the nodes
in accordance with their respective generations. To this end, we choose a small bound ǫ3, which
enables Algorithm 3 to return more than one node per iteration step; namely the generations.
Based on simulation experience, we chose a =
√
2 and ε1 = 0.1,ε2 = 0.05,ε3 = 0.1. All simulations
and data analysis are done using R, R Core Team (2016).
Algorithm 2 Identifying the initial nodes in X10
1: procedure
2:
Set ˆ∆= (0)1×d;N = (0)1×d;a > 1;ε1 > 0;ε2 > 0
3:
for i∗= d,... ,1 do
4:
ˆ∆i∗= ˆσ2
M−i∗,ai∗−ˆσ2
M1,...,d −a2 + 1
5:
if ( ˆ∆i∗≥0 and ˆ∆i∗≤ε1), or ( ˆ∆i∗≤0 and ˆ∆i∗≥−ε2), then
6:
Ni∗= i∗
7:
else Ni∗= 0
8:
end for.
Algorithm 3 Identifying the (h + 1)-th node of X10
1: procedure
2:
Set ˆ∆= (0)1×d;N = (0)1×d;a > 1;ǫ3 > 0;h = {ν−1(d),... ,ν−1(d −h + 1)}
3:
for i∗∉h do
4:
ˆ∆i∗= ˆσ2
Mha,i∗a,{h∪{i∗}}c −ˆσ2
M1,...,d −(a2 −1)ˆσ2
Mh,i∗
5:
if ∣ˆ∆i∗∣≤ǫ3, then
6:
Ni∗= i∗
7:
else Ni∗= 0
8:
end for.
As the initial order is irrelevant, we set w.l.o.g. X∗
10 = (X1,... ,X10). We perform 100 simu-
lation runs for each of the sample sizes n ∈{2000,3000,5000, 10000}.
The reorderings are obtained by ﬁrst applying Algorithm 2 and then Algorithm 3. For some
simulations it has happened that the bounds in Line 5 in Algorithms 2 and 3 are not satisﬁed by
any of the components. We indicate this in Table 5.1, where the column “Valid Runs” corresponds
to the number of simulation runs (out of 100) for which the conditions of the bounds in the
algorithms are satisﬁed each time the “if” loop is entered.
The column “Correctly Reordered” gives the number of runs for which Algorithms 2 and 3
return the generations V0 = {10},G1 = {8,9},G2 = {5,6,7},G3 = {1,2,3,4}. The column “Success
Ratio” presents the ratio of “Correctly Reordered” over “Valid Runs”.
◻
17

Sample size
Simulation Runs
Valid Runs
Correctly Reordered
Success Ratio
2000
100
81
65
80.24%
3000
100
91
80
87.91%
5000
100
96
94
97.92%
10000
100
99
99
100%
Table 5.1: Results of the reordering procedure for the 10-node DAG in Figure 1. Correct orders are only
those with initial node V0 = {10}, and generations G1 = {8,9},G2 = {5,6,7}, G3 = {1,2,3,4} with
arbitrary order within each generation.
6
Statistical Theory for Regularly Varying Innovations
The discrete spectral measure of the ML model poses serious challenges towards the objective
of estimation. Einmahl et al. (2016), Einmahl et al. (2012), and Einmahl et al. (2018) develop
estimation procedures for the stable tail dependence function. In both, Einmahl et al. (2012)
and Einmahl et al. (2018), the methods are also applied to models with discrete spectral mea-
sure, whose dependence parameters can be obtained from the stable tail dependence function.
Janssen and Wan (2019) provide a new way for estimating the atoms of the spectral measure
on the unit sphere by using a clustering approach. For our purposes we resort to the empirical
spectral measure.
Let X1,... ,Xn be an i.i.d. sample of X ∈RV d
+ (2). For ℓ= 1,... ,n deﬁne
Rℓ∶= ∥Xℓ∥2
and
ωℓ= (ωℓ1,...,ωℓd) ∶= Xℓ
Rℓ
,
(6.1)
to obtain their respective polar representation {(Rℓ,ωℓ) ∶ℓ= 1,... ,n}. A consistent estimator
for the standardized spectral measure ˜HX as in Remark 1 is based on the limit relation (2.1)
and given e.g. in eq. (9.32) in Chapter 9.2 of Resnick (2007) as (⌊s⌋denotes the integer part of
s ∈R )
˜HX,⌊n/k⌋(⋅) =
∑n
ℓ=1 1{(Rℓ/b⌊n
k ⌋,ωℓ) ∈[1,∞] × ⋅}
∑n
ℓ=1 1{Rℓ/b⌊n
k ⌋≥1}
d→˜HX(⋅),
as n →∞, k →∞, k/n →0. Since R(k)/b⌊n
k ⌋
P→1 (cf. below eq. (9.32) of Resnick (2007)), where
R(k) is the k-th largest among R1,...,Rn. Hence, setting b⌊n
k ⌋= R(k), the denominator becomes
∑n
ℓ=1 1{Rℓ≥R(k)} = k and the above estimator reads
˜HX,⌊n/k⌋(⋅) = 1
k
n
∑
ℓ=1
1{Rℓ≥R(k),ωℓ∈⋅}.
Then an estimator for E ˜HX[f(ω)] is given by
ˆE ˜HX[f(ω)] = 1
k
n
∑
ℓ=1
f(ωℓ)1{Rℓ≥R(k)}.
(6.2)
Our goal is to estimate the squared scalings σ2
Mh of Mh for h ⊆{1,... ,d} as in (4.2). To this
end we choose f ∶Θd−1
+
→R+ deﬁned for ℓ= 1,... ,n via f(ωℓ) = d( ⋁
k∈h
ω2
ℓk), which is a continuous
function.
18

Deﬁnition 7. [Non-parametric scaling estimators] Let X ∈RV d
+ (2) and X1,... ,Xn be an
i.i.d. sample of X with respective polar representations (Rℓ,ωℓ) for ℓ= 1,... ,n as in (6.1). For
1 ≤k ≤n estimate σ2
i for i = 1,... ,d and σ2
Mh for h ⊆{1,... ,d} by (6.2) as
ˆσ2
i = d
k
n
∑
ℓ=1
ω2
ℓi1{Rℓ≥R(k)}
and
ˆσ2
Mh = d
k
n
∑
ℓ=1
⋁
j∈h
ω2
ℓj1{Rℓ≥R(k)}.
(6.3)
◻
Finally, we estimate A by the linear transformation as given in Theorem 1.
6.1
Asymptotic Normality of the Non-paramatric Estimators
Larsson and Resnick (2012) have proven in their Theorem 1 a CLT for the EDM as in Deﬁni-
tion (5) based on the fact that the function f ∶Θ+ →R+ given by f(ω) = ω1ω2 is continuous.
This result can be generalised to any continuous functions f ∶Θd−1
+
→R and any dimension d.
The following CLT holds for every X ∈RV d
+ (2). Again we use the polar representation (6.1).
Let the radial component R of X have distribution function F.
Theorem 4 (Central Limit Theorem). Let X ∈RV d
+ (2) and X1,... ,Xn be i.i.d. copies of X.
Choose k such that k = o(n) and k →∞as n →∞. Let f∶Θd−1
+
→R+ be a continuous function.
Assume that
lim
n→∞
√
k(n
k E[f(ω1)1{R1 ≥b⌊n
k ⌋t−1/α}] −E ˜HX[f(ω1)]n
k
¯F(b⌊n
k ⌋t−1/α)) = 0
(6.4)
holds locally uniformly for t ∈[0,∞), and Var ˜
HX(f(ω)) > 0. Then
√
k(ˆE ˜
HX[f(ω)] −E ˜
HX[f(ω)])
D→N(0,σ2),
n →∞.
Proof. The proof follows closely Theorem 1 of Larsson and Resnick (2012), which only covers
the case of f(ω1,ω2) = ω1ω2. Going through this proof line by line we ﬁnd that it applies to
every continuous function f ∶Θd−1
+
→R+, where the asymptotic variance σ2 has to be adapted
to the chosen function f.
Assumption (6.4) requires the dependence between ωℓand Rℓfor Rℓ> b⌊n
k ⌋to decay suﬃ-
ciently fast as n →∞.
6.2
Asymptotic Normality of the Scalings of Maxima
From Theorem 1 we know that we can identify A by a known linear transformation T from the
vector SM ∈Rd(d+1)/2
+
of squared scalings deﬁned in (4.8). Each of these squared scalings we
estimate by ˆσ2
i and ˆσ2
Mh as in eq. (6.3) and denote the resulting estimation vector by
ˆSM ∶= (ˆσ2
M1,2,...,d, ˆσ2
M1,3,...,d,... , ˆσ2
M1,d, ˆσ2
1, ˆσ2
M2,3,...,d, ˆσ2
M2,4,...,d,... , ˆσ2
M2,d, ˆσ2
2,... , ˆσ2
Md−1,d, ˆσ2
d−1, ˆσ2
d).
(6.5)
We show asymptotic multivariate normality of the vector ˆSM. To this end we use the Cram´er-
Wold device and a properly chosen continuous function f on Θd−1
+
to which we then apply
Theorem 4.
19

Theorem 5. Let X = A×maxZ be a recursive ML vector satisfying (A1)-(A3), and let X1,... ,Xn
be i.i.d. copies of X. Choose k such that k = o(n) and k →∞as n →∞. Furthermore, assume
that (6.4) holds for f(ω) = d( ⋁
i∈hi
ω2
i ) and that Var ˜
HX(d( ⋁
i∈hi
ω2
i )) > 0 for all hi ⊂{1,... ,d} such
that SMhi is a component of SM. Then
√
k( ˆSM −SM)
D→N(0,WM),
n →∞,
where the entries of the covariance matrix WM are given by the right-hand sides of the following
two limits. The diagonal entries for h ⊂{1,... ,d} satisfy
lim
n→∞kVar( ˆSMh) = d2Var ˜
HX(⋁
i∈h
ω2
i )),
and the non-diagonal entries for two diﬀerent sets hi ≠hj ⊂{1,... ,d} are given by
lim
n→∞k Cov( ˆSMhi, ˆSMhj ) = d2
2 (Var ˜
HX( ⋁
k∈hi
ω2
k + ⋁
l∈hj
ω2
l ) −Var ˜HX( ⋁
k∈hi
ω2
k) −Var ˜
HX( ⋁
l∈hj
ω2
l )).
Moreover, the covariance matrix WM is singular.
Proof. By the Cram´er-Wold device we have to show asymptotic normality of
√
ktT ( ˆSM −SM)
for every t ∈Rd(d+1)/2. We ﬁrst index the entries of SM according to hi for i ∈{1,... ,d(d+1)/2}.
Then we re-write
tT SM =
d(d+1)/2
∑
i=1
tiSMhi =
d(d+1)/2
∑
i=1
dtiE ˜
HX[ ⋁
j∈hi
ω2
j ] = E ˜
HX[d
d(d+1)/2
∑
i=1
ti( ⋁
j∈hi
ω2
j )].
Now choose f ∶Θd−1
+
→R+ as
f(ω) = d
d(d+1)/2
∑
i=1
ti( ⋁
j∈hi
ω2
j),
which is—as a linear function of continuous functions—itself continuous on Θd−1
+
. The empirical
estimator for f is by (6.3) given as
ˆE ˜HX[f(ω)] = d
k
n
∑
ℓ=1
d(d+1)/2
∑
i=1
ti( ⋁
j∈hi
ω2
ℓj)1{Rℓ≥R(k)}.
(6.6)
Applying Theorem 4 for the given choice of f it follows that
√
ktT ( ˆSM −SM)
D→N(0,wM),
n →∞,
where wM = d2Var ˜
HX(∑
d(d+1)/2
i=1
ti( ⋁
j∈hi
ω2
j )). By the Cram´er-Wold device this implies that
√
k( ˆSM −SM)
D→N(0,WM),
n →∞.
To show that WM is singular, let ti = 1 for hi such that ∣hi∣= 1, and set the remaining components
of the vector t to zero. Summarize all these hi into the set H1 ∶= {i ∈{1,... ,d(d + 1)/2} ∶∣hi∣=
1}, and note that ∣H1∣= d since there are exactly d such entries in SM, namely σ2
1,σ2
2,...,σ2
d
corresponding to the dimension of X. Then we obtain
tT ˆSM = d
k
n
∑
ℓ=1
∑
i∈H1
ti( ⋁
j∈hi
ω2
ℓj)1{Rℓ≥R(k)}
20

= d
k
n
∑
ℓ=1
d
∑
m=1
tmω2
ℓm1{Rℓ≥R(k)}
= d
k
n
∑
ℓ=1
d
∑
m=1
ω2
ℓm1{Rℓ≥R(k)}
= ˆE ˜
HX[d
d
∑
m=1
ω2
m] = d,
where the last line is due to (6.6) for the particularly chosen f(ω) = d∑d
m=1 ω2
m = d, and the last
equality follows from d
k ∑n
ℓ=1 1{Rℓ≥R(k)} = d. Since this is non-random, the limiting multivariate
normal distribution is degenerate.
We proceed now by computing the entries of the covariance matrix.
First, for i = 1,... ,d(d + 1)/2 we compute the i-th diagonal element of WM, corresponding to
the asymptotic variance Var( ˆSMh) = Var(ˆσ2
Mh). We ﬁnd this from Theorem 4 as Var ˜
HX(f(ω))
for f(ω) = d( ⋁
i∈h
ω2
i ):
lim
n→∞k Var( ˆSMhi) = d2Var ˜HX(⋁
i∈h
ω2
i ) = d2(E ˜
HX[⋁
i∈h
ω4
i ] −(E ˜
HX[⋁
i∈h
ω2
i ])2).
(6.7)
Next, when computing the covariance we simply use the identity 2Cov(X,Y ) = Var(X + Y ) −
Var(X)−Var(Y ). Let hi ≠hj ⊆{1,... ,d}. Consider f(ω) = d( ⋁
k∈hi
ω2
k + ⋁
l∈hj
ω2
l ). Then using again
Theorem 4 we get
lim
n→∞2k Cov( ˆSMhi, ˆSMhj ) = d2(Var ˜
HX( ⋁
k∈hi
ω2
k + ⋁
l∈hj
ω2
l ) −Var ˜
HX( ⋁
k∈hi
ω2
k) −Var ˜HX( ⋁
l∈hj
ω2
l )). (6.8)
The asymptotic covariance matrix Wm can also be expressed in terms of the squared entries
of the matrix A.
Corollary 3. Let the assumptions of Theorem 5 hold. Then the entries of WM are given by the
right-hand sides of the following two limits: On the diagonal we obtain
lim
n→∞k Var( ˆSMh) = d(
d
∑
j=1
⋁
i∈h
a4
ij
∥aj∥2
2
) −(
d
∑
j=1
(⋁
i∈h
a2
ij))2 > 0,
where (∑d
j=1( ⋁
i∈h
a2
ij))2 = σ4
Mh. For the non-diagonal entries we obtain
lim
n→∞k Cov( ˆSMhi, ˆSMhj ) = d
d
∑
k=1
(
1
∥ak∥2
2
( ⋁
m∈hi
a2
mk)( ⋁
l∈hj
a2
lk)) −((
d
∑
k=1
⋁
m∈hi
a2
mk)(
d
∑
k=1
⋁
l∈hj
a2
lk))2,
where ∑d
k=1 ⋁
m∈hi
a2
mk = σ2
Mhi and ∑d
k=1 ⋁
l∈hj
a2
lk = σ2
Mhj .
Proof. With the explicit form of the spectral measure (3.4), expression (6.7) becomes:
d2Var ˜
HX(⋁
i∈h
ω2
i ) = d(
d
∑
j=1
⋁
i∈h
a4
ij
∥aj∥2
2
) −(
d
∑
j=1
(⋁
i∈h
a2
ij))2.
21

Similarly, for the covariance, from (6.8) we obtain:
d2Var ˜
HX( ⋁
m∈hi
ω2
m + ⋁
l∈hj
ω2
l )
= d
d
∑
k=1
( ⋁
m∈hi
a4
mk
∥ak∥2
2
+ ⋁
l∈hj
a4
lk
∥ak∥2
2
+
2
∥ak∥2
2
( ⋁
m∈hi
a2
mk)( ⋁
l∈hj
a2
lk)) −((
d
∑
k=1
⋁
m∈hi
a2
mk) + (
d
∑
k=1
⋁
l∈hj
a2
lk))2
Summing the variance terms together we obtain
2kCov ˜HX(SMhi,SMhj ) = 2d
d
∑
k=1
(
1
∥ak∥2
2
( ⋁
m∈hi
a2
mk)( ⋁
l∈hj
a2
lk)) −2(
d
∑
k=1
⋁
m∈hi
a2
mk)(
d
∑
k=1
⋁
l∈hj
a2
lk).
The identities giving σ2
Mh, σ2
Mhi, and σ2
Mhj follow from (4.3).
Finally we prove asymptotic normality of the estimated ML coeﬃcient matrix A computed
via Theorem 1 as A2 = TSM. As T is a deterministic matrix, we obtain ˆA2 as T ˆSM. Then
Theorem 5 and Corollary 3 gives the asymptotic normality of the estimated ML coeﬃcient
matrix A.
Theorem 6. Let X = A×maxZ be a recursive ML vector satisfying (A1)-(A3), and let X1,... ,Xn
be i.i.d. copies of X. Let the assumptions of Theorem 5 hold and assume that the ML coeﬃcient
matrix A = (aij)d×d satisﬁes
d(
d
∑
j=1
⋁
i∈h
a4
ij
∥aj∥2
2
) −(
d
∑
j=1
(⋁
i∈h
a2
ij))2 > 0.
Estimate ˆA2 = T ˆSM with T as in Theorem 1 and ˆSM as in (6.5). Then
√
k( ˆA2 −A2)
D→N(0,TWMT T ),
n →∞,
where WM is the covariance matrix in Corollary 3.
7
Data Applications
7.1
Structure Learning and Estimation
For estimating a causal order as well as for the estimation of the ML coeﬃcient matrix A we need
estimates for the scalings in Algorithms 1, 2, and 3, respectively. Notice that in Proposition 2 we
estimate the ML coeﬃcient matrix A for a well-ordered DAG, so that we ﬁrst estimate the order
of the nodes and then A. The structure learning is based on the scalings of Mha,m∗a,{h∪{m∗}}c
as deﬁned in (5.6), and the estimation of A as in Proposition 2 is based on scalings of Mh as
in (4.2). In contrast to Example 3 we make no distributional assumptions on X, but use the
non-parametric estimation method developed in Section 6.
For the non-parametric estimation of all scalings needed in the algorithms, as in Section 6
we denote by k the number of upper order statistics corresponding to the radial threshold used
for the estimation of the spectral measure. We recall that it has to be chosen as k = o(n) and
we choose k ≈√n. We observe that we may have rather few components exceeding the radii
computed as in (6.1) based on all d components. If some components of an observation are very
large, then other components may not exceed the corresponding threshold. As the choice of the
k upper order statistics is based on these radii, there may be rather few exceedances in some
component. Hence, we resort to lower dimensional vectors Xq = (Xi ∶i ∈q) for appropriate sets
q ⊆{1,...,d} for the estimation of the various scalings.
22

7.1.1
Structure Learning
We want to apply Algorithms 2 and 3 for structure learning by replacing the theoretical scalings
by their estimated counterparts. However, we have to modify both algorithms to account for
estimation errors.
As the limited number of exceedances of radii in some components is particularly critical for
identiﬁcation of the initial nodes, we modify the estimation procedure, which has been presented
in (5.1), and estimate for every m ∈{1,... ,d} the initial nodes only based on max(Xi,aXm) for
a > 1 (corresponding to q = {i,m}), but then for all i ∈{1,... ,d} ∖{m}. This means that the
identiﬁcation of the initial nodes is carried out by applying a pairwise version of Theorem 2 for
all pairs. The following pairwise version of Algorithm 2 identiﬁes the intial nodes of the DAG.
It is based on Theorem 5.6 and Algorithm 3 of Krali (2018), adapted for possible estimation
errors. The positive bounds ε1,ε2 have to be chosen appropriately to ensure that the estimates
ˆ∆i∗m∗are close to zero. The scalar a > 1 has to be chosen in accordance with Theorem 2.
Algorithm 4 Identifying the initial nodes of X
1: procedure
2:
Set ˆ∆= (0)d×d;N = (0)1×d;a > 1;ε1 > 0;ε2 > 0
3:
for m∗∈{d∗,...,1∗} do
4:
for i∗∈{d∗,...,1∗} do
5:
ˆ∆i∗,m∗= ˆσ2
Mam∗,i∗−ˆσ2
Mi∗,m∗−a2 + 1
6:
end for
7:
Set ˆ∆m∗= ( ˆ∆1∗,m∗,..., ˆ∆d∗,m∗)
8:
end for
9:
Set I1 = {m∗∈{d∗,...,1∗} ∶{
max
i∗∈{d∗,...,1∗}
ˆ∆i∗,m∗≤ε1} ∩{
min
i∗∈{d∗,...,1∗}
ˆ∆i∗,m∗≥−ε2}}
10:
for m∗∈{d∗,...,1∗} do
11:
if m∗∈I1, then
12:
Nm∗= m∗
13:
else Nm∗= 0
14:
end for.
Once the initial nodes are identiﬁed, we proceed ﬁnding the descendants. When searching
for the (h + 1)-th node, we apply the following modiﬁcation of Algorithm 3, which has again
been adapted for possible estimation errors by an application of Corollary 2.
Algorithm 5 Identifying the (h + 1)-th node of X
1: procedure
2:
Set ˆ∆= (0)1×d;N = (0)1×d;a > 1;h = {ν−1(d),... ,ν−1(d −h + 1)}
3:
for i∗∉h do
4:
ˆ∆i∗= ˆσ2
Mha,i∗a,{h∪{i∗}}c −ˆσ2
Mh,i∗,{h∪{i∗}}c −(a2 −1)ˆσ2
Mh,i∗
5:
end for
6:
for i∗∉h do
7:
if ˆ∆i∗= max
i∗∉h
ˆ∆i∗, then
8:
Ni∗= i∗
9:
else Ni∗= 0
10:
end for.
23

In contrast to Example 3, where our goal was to identify the generations of the graph, here
we are only interested in a causal order of the nodes, and thus modify Algorithm 3 based on
Corollary 2 so that it returns a unique node at each step of the if-loop.
7.1.2
Estimation of the Scalings
According to Line 4 of Algorithm 5 three squared scalings need to be estimated:
-σ2
Mh,i∗,{h∪{i∗}}c : By (5.6), this estimate involves all d components of X. Thus, we set q ∶=
{1,...,d} and proceed as in step (i) below;
-σ2
Mh,i∗
: We estimate the spectral measure based on q ∶= h ∪{i∗} and proceed as in
step (i) below; such scalings we also need to estimate in Line 5 of Algorithm 4;
-σ2
Mha,i∗a,{h∪{i∗}}c: By (5.6), this is the estimated scaling of the rescaled vector X and we follow
step (ii) below.
For Algorithm 1 we have to estimate the following squared scalings for i,j ∈{1,...,d} and i ≤j+1:
-σ2
Mi,j,j+1,...,d: We estimate the spectral measure based on q ∶= {i,j,j + 1,...,d}.
-σ2
Mj,j+1,...,d : Here we set q ∶= {j,j + 1,...,d}.
The following two steps modify the setting of Section 6 and summarize the estimation of the
scalings in both Algorithms 4, 5, and Algorithm 1.
(i) For the estimation of the squared scalings σ2
Mq of Mq for some subset of components
q ⊆{1,...,d} as speciﬁed above, we take Xq = (Xi ∶i ∈q), and compute for each observation
ℓ∈{1,... ,n}, the (reduced) polar representation
Rℓ∶= ∥Xℓq∥2 = (∑
i∈q
X2
ℓi)1/2
and
ωℓ= (ωℓi ∶i ∈q) ∶= Xℓq
Rℓ
,
ℓ= 1,...,n.
(7.1)
Then for 1 ≤k ≤n we estimate σ2
Mq as
ˆσ2
Mq = ∣q∣
k
n
∑
ℓ=1
⋁
j∈q
ω2
ℓk1{Rℓ≥R(k)}.
When q = {i}, corresponding to the scaling of a single component, then by (7.1), ω = 1, and
plugging this in the estimator in (2), we obtain ˆσ2
i = 1, which is the true scaling parameter σi = 1.
(ii) For the estimation of the scaling of Mha,m∗a,{h∪{m∗}}c, we replace Xq above by the rescaled
version (aXh,aXm∗,X{h∪{m∗}}c), and re-apply the same procedure as in (i) to obtain new polar
representations, say (Raℓ,ωaℓ), and then estimate:
ˆσ2
Mha,m∗a,{h∪{m∗}}c = (a2 −1)(∣h∣+ 1) + d
k
n
∑
ℓ=1
d
⋁
j=1
ω2
aℓj1{Raℓ≥R(k)
a }.
The numerator, (a2 −1)(∣h∣+ 1) + d corresponds to the new mass of the spectral measure as a
consequence of the scaling by a of the components involved in Xh and Xm∗, see for instance
Lemma 3(b).
7.1.3
Estimating the ML Coeﬃcient Matrix
After having estimated also the scalings needed for Algorithm 1, we have to take care of es-
timation errors. Indeed, it can happen that entries of A2 are estimated as being negative. For
the two data examples to follow we simply set ˆA =
√
max( ˆA2,0), with the square root taken
entrywise, and keeping all positive estimates. For larger networks it may be advisable to choose
a thresholding or lasso procedure to obtain a sparse graph.
24

7.2
Industry Portfolio Data
The data consist of seven time series of value-averaged daily percentage returns, each assigned to
one of seven industry portfolios as part of the 30-Industry-Portfolio in the Kenneth French Data
Library available at https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/datalibrary.html.
All 30 portfolios have been analysed in Cooley and Thibaud (2019); Janssen and Wan (2019),
where in Cooley and Thibaud (2019) it is suggested that the tails of the data are regularly
varying. We refer to the introduction for more details on the objectives of these papers.
The seven portfolios we consider are: Chemicals (Ch), Fabricated Products (FP), Electrical
Equipment (EE), Healthcare (H), Smoke (S), Utilities (U), and Others (O), which includes
products which are not speciﬁc to any of the other listed industries. A precise description of the
data, in particular of each industry sector can be found on the website above.
The data has been collected over the years 1950-2015. Since the time series over this time
period is non-stationary and, in particular, since also the dependence structure changes over
this long period, we have selected the time window from 01.06.1989 to 15.06.1998 containing
2285 observations, which show marginal stationarity. To each of the 7 time series we have ﬁtted
moving average processes of order 3, with the exception of Others where we have ﬁtted a moving
average process of order 4, and performed a Ljung-Box test with 8 lags on the residuals. The
test did not reject the independence hypothesis of the residuals, supporting the assumption that
the time series are stationary. Figure 2 depicts the time series plots in %-returns for the seven
industries.
As we assess dependence in extreme negative returns, we transform the vector of the 7
portfolio time series to X∗= max(−X,0). Our aim is to ﬁt a recursive ML model model to X∗.
We transform the data by the empirical integral transform to standardize them to Fr´echet(2)
margins (see for instance, p. 381 in Beirlant et al. (2004), or Cooley and Thibaud (2019)). We
map (S,H,EE,U,O,FP,Ch) ↦(1,2,3,4,5,6,7) and deﬁne for i = 1,... ,7
Xℓi ∶= ( −log {
1
n + 1
n
∑
j=1
1{X∗
ji≤X∗
ℓi}})
−1/2
,
ℓ= 1,... ,n = 2285.
(7.2)
We also collect the 7-dimensional data into a vector time series x∗
ℓ= (x∗
ℓ,1,x∗
ℓ,2,x∗
ℓ,3,x∗
ℓ,4,x∗
ℓ,5,x∗
ℓ,6,x∗
ℓ,7),
ℓ= 1,... ,2285.
Running maxima. In order to provide some insight in the data structure, we start our analy-
sis with a time-line of the high risk events of the seven industry portfolios and their association
with shocks entering the industry network from the relevant component of the innovation vector
Z. The horizontal axes of Figure 3 shows every time point, when the maximum over all seven
standardized industry returns happens and indicates on the vertical axes the respective innova-
tion component causing this maximum. Since the innovations Zi for i = 1,... ,d are atomfree and
by Assumption (1) independent, representation (3.2) ensures that each recursive ML component
Xi realises its maximum in exactly one innovation. If two components of X are realised by the
same innovation, still the realised values of the two components are diﬀerent by Lemma 5, which
implies that max{X1,...,Xd} is unique. These innovations are indicated in Figure 3.
25

1990
1992
1994
1996
1998
−10
5
Smoke
1990
1992
1994
1996
1998
−6
0
4
Healthcare
1990
1992
1994
1996
1998
−6
0
4
Electrical Equipment
1990
1992
1994
1996
1998
−3
0
2
Utilities
1990
1992
1994
1996
1998
−6
0
4
Others
1990
1992
1994
1996
1998
−6
0
Fabricated Products
1990
1992
1994
1996
1998
−6
0
4
Chemicals
Figure 2: Time-series of %-returns for the seven industries from 01.06.1989 until 15.06.1998.
1990
1992
1994
1996
1998
1
4
7
Figure 3: Dates of the maximal occurrences associated to the components i ∈{1,...,7}.
We ﬁnd that most of the shocks originate from the initial node Chemicals (7) during the time
period 1990-1991 which coincides with the Gulf War, caused by the invasion of Kuwait by Iraq.
During the war it was feared that Iraq made use of chemical warfare. Regarding Fabricated
Products (6), the larger losses occur close to the end of 1997, which is associated with the
26

slowdown in the Asian economies and which had spillover eﬀects on the U.S economy, eventually
leading also to the October 27, 1997 Mini-Crash. The Utilities (4) experience large losses in the
years 1994 and 1996 associated with deregulation of the electric energy supply in the US, which
was initiated in 1992. The Healthcare sector (2) experiences losses in the period 1992-1993
associated with the Clinton Health Care reform.
Our ﬁnal goal is to approximate the causal dependence structure via a recursive ML model by
means of the learning algorithm presented in the previous sections of this paper. This algorithm
is based on all returns above a high threshold, not only on the maximum value.
Bivariate extremes. In a second exploratory analysis we plot the bivariate extremes (real
data and simulated ones) in Figure 6 of Appendix A. We also simulate a 7-dimensional ran-
dom vector X ∈RV 7
+ (2) from an innovation vector Z ∈RV 7
+ (2) with independent standard
Fr´echet(2) components of dimension n = 2285 via X = ˆA ×max Z, where the estimated matrix ˆA
is given in (7.3). Two columns always belong together, the left one gives the empirical bivariate
extremes of two of the seven components, respectively, which have also been the basis for the
estimation procedure of Section 7.1. The right one presents a simulation of the estimated model.
We plot only those bivariate observations with the 50 largest radii. Left and right (real and
simulated data) look very much alike, indicating that the estimated bivariate models are valid
approximations to the biviariate empirical distribution in the tails.
We give an interpretation of Utilities versus Electrical Equipment (line 6, columns 3 and 4):
Here we notice that large losses in Utilities do not necessarily occur together with high risk for
Electrical Equipment. This can be veriﬁed in the plot of realised bivariate extremes (column 3),
where large losses for Utilities occurring close to the vertical axis correspond only to negligible
losses for Electrical Equipment. This suggests that the common large losses between Utilities
and Electrical Equipment are rather caused by their common ancestors. This may be due to
idiosyncratic risks associated with one but not the other, which in this case might correspond
to the innovation terms Z3 (ˆa43 = 0) and Z4 (ˆa34 = 0).
Fitting a recursive ML model. Finally, we approximate the extreme dependence structure of
X∗by a recursive ML model. To this end, according to Section 6, we have to choose a threshold
value k = o(n) of the radial components. We choose k ≈√n and set k = 50. For identiﬁcation of
the initial nodes we employ Algorithm 4 with a = 1.01 and ε1 = 0.0045,ε2 = 0.0045, and then in
order to reorder the remaining nodes Algorithm 5. As output we obtain the ordered vector
X = (XS,XH,XEE,XU,XO,XF P ,XCh).
Finally, we estimate the ML coeﬃcient matrix by the estimation version of Algorithm 1 and
setting ˆA =
√
max( ˆ
A2,0). The estimated standardised ML coeﬃcient matrix is given by
ˆA =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0.708
0
0.462
0
0.142
0.333
0.476
0
0.566
0.459
0
0.076
0.147
0.687
0
0
0.649
0
0.344
0.136
0.686
0
0
0
0.709
0.333
0.188
0.593
0
0
0
0
0.682
0.250
0.688
0
0
0
0
0
0.674
0.739
0
0
0
0
0
0
1
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(7.3)
The estimated squared scaling parameters of the components of X are obtained by summing the
squared entries of the respective row. We ﬁnd the estimated vector of scalings (1.036,1.01,1.01, 1,1, 1, 1)
27

and recall that the theoretical ones are all equal to 1. Deviations from scalings of 1 stem from
the fact that we set ˆA =
√
max( ˆA2,0). In doing so, once ˆA2 has been computed, we ignore its
entries that are close to zero but negative, for instance ˆa2
12, ˆa2
14, ˆa2
24, ˆa2
34. Consequently this can
make the sums of the square entries of the respective row be slightly greater than one.
The DAG corresponding to ˆA is given in Figure 4. We recall that aij = 0 implies no edge
from j to i.
Ch
FP
O
U
H
S
EE
Figure 4: Learned DAG structure for the seven considered industries. Note that Chemicals is the only
initial node of the DAG.
The estimated DAG should provide insight into the causality structure of the seven industries.
We associate the estimated scalings with the standardised risk of each component. Then the
estimated entries in ˆA indicate the proportions of risk inferred from the causal dependence.
The only initial node is Chemicals, whose high losses impact risk on all other industries as it
has out-degree 6. For instance, the line of productions in Fabricated Products, Healthcare (which
includes pharmaceutical industry), Electrical Equipment, Utilities (which includes electricity
services and supply), and Smoke are highly dependent on the supply of chemical products and
chemical processing.
Fabricated Products has out-degree 5 with its high risk aﬀecting Utilities, Others, Smoke,
Healthcare, and Electrical Equipment. A reason for this may lie in the industrial fabrication of
many products in the aﬀected industries. In particular the impact on Smoke, whose production
line depends heavily on machinery, is stronger than that on Utilities, Electrical Equipment, and
Others, since a16 > max(a26,... ,a56).
Others, whose components include also Cogeneration Power Producers, has out-degree 4 with
its high risk aﬀecting Utilities, Smoke and Electrical Equipment, and Healthcare to a lesser ex-
tent. On the other hand, Others has in-degree 2, so high risk in Chemicals or Fabricated Products
aﬀects Others, which can be seen from a57 and a56 with higher inﬂuence from Chemicals.
Electrical Equipment has out-degree 2 and in-degree 3, so its high risk is caused by Others,
Chemicals, and Fabricated Products, where the inﬂuence of Chemicals is about twice as large as
Others, and the inﬂuence of high risk in Fabricated Products is much lower. On the other hand,
high risk in Electrical Equipment impacts on Healthcare and Smoke in about equal proportions.
Utilities, Healthcare, and Smoke have out-degree 0, so these portfolios are aﬀected by high
risk of other portfolios, but their high risks do not spread elsewhere. This is seen from columns
1,2, and 4 of ˆA, where the quantities on the diagonal correspond to the idiosyncratic risk. The
28

quantities to the right measure the high risk inﬂuencing these three portfolios.
7.3
Dietary Supplement Data
The data is taken from a dietary interview from the NHANES report for the year 2015-2016,
which is available at https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DR1TOT_I.XPT; here
also more details about the 168 data components can be found. The objective is that of es-
timating the total intake of calories, nutrients and non-nutrient food components from foods
and beverages consumed a day prior to the interview. From the above data 38 components have
been investigated in Janssen and Wan (2019) using the clustering approach mentioned already
in the introduction.
We focus on four of the components, Vitamin A (DR1TVARA), Beta-Carotene (DR1TBCAR),
Lutein+Zeaxanthin (DR1TLZ) and Alpha-Carotene (DR1TACAR). We abreviate them as VA,
BC, LZ, AC, respectively. For each component there are n = 9544 observations, each correspond-
ing to a diﬀerent individual and generated from survey interviews, thus the data sample can be
treated as an i.i.d. sample. A Hill plot (see e.g. Embrechts et al. (1997), Section 6.4) suggests
that all data components are regularly varying with some positive index. We map (VA, BC,
LZ, AC) ↦(1,2,3,4) and apply the empirical integral transform to standardize the data to
Fr´echet(2) margins as in (7.2). As in Section 7.2 we choose k ≈√n as radial threshold (see Sec-
tion 6), taking k = 100 upper order statistics. The bivariate extremes (real data and simulated
ones) are plotted in Figure 7 of Appendix B with interpretations as in Section 7.2.
We apply Algorithms 4 and 5 to reorder the nodes and estimate a recursive ML model. In
the two algorithms we set a = 1.01,ε1 = 0.002,ε2 = 0.001. This results in the causal order (VA,
BC, LZ, AC). Following the procedure in Section 7.2, we obtain the ML coeﬃcient matrix
ˆA =
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣
0.680
0.406
0.303
0.531
0
0.651
0.500
0.571
0
0
0.960
0.281
0
0
0
1.000
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(7.4)
The estimated scaling parameters of the components of X equals up to three digits (1,1,1,1).
The DAG corresponding to ˆA is presented in Figure 5. We interpret dependence in high amounts
of the four given food components. The estimated entries in ˆA indicate the proportion of high
intake from food consumption.
AC
LZ
BC
V A
Figure 5: DAG structure between the four food components.
From the DAG in Figure 5 we observe that the only initial node is Alpha-Carotene. Having
out-degree 3 its high intake aﬀects the intake of the other three food components. Alpha-Carotene
aﬀects in particular Vitamin A, and Beta-Carotene, where in both cases it behaves as the main
29

contributor of high values amongst the respective ancestral nodes. This can be seen from the
relative magnitude of the entries, namely a14 > max(a12,a13) and a24 > a23. To a lesser degree
Alpha-Carotene also aﬀects high intake of Lutein+Zeaxanthin as is seen from a34 = 0.281.
On the other hand, high proportions of Lutein+Zeaxanthin, which has in-degree 1 and out-
degree 2, lead to large intakes of Beta-Carotene, and aﬀect those of Vitamin A in a similar
fashion, but to a lesser proportion. From the estimated matrix ˆA in (7.4), we can infer that
Lutein+Zeaxanthin, along with Alpha-Carotene is one of the main causes of high Beta-Carotene,
with approximately equal contributions, judging by the relative sizes of a23,a24. Finally Beta-
Carotene, with in-degree 2 and out-degree 1 is the second largest contributor to high intake
of Vitamin A, since a12 > a13. Among all four components, Vitamin A has in-degree 3, and
out-degree 0, showing that high intake of VA does not inﬂuence any of BC, LZ, or AC.
8
Conclusions
We have developed a new structure learning and estimation algorithm for the recursive ML
model (3.2). The proposed methodology is designed for estimating recursive max-linear models
for extreme events in a multivariate regular variation setting. The technique is non-parametric
based on the empirically estimated spectral measure. The parametric estimation step focuses
on the scalings and reﬂects the changes caused by simple scalar multiplications of the observed
data variables on the scaling parameters. In addition, based on the very same scalings, we
have shown how to estimate all extreme dependence parameters. The latter are shown to be
asymptotically normal. Finally, the application of the new estimation method to ﬁnancial and
food dietary intake data shows that the recursive max-linear model can be ﬁtted for capturing
causal dependence structures in the extremes arising from real-life data.
References
Beirlant, J., Goegebeur, Y., Segers, J. and Teugels, J. (2004). Statistics of Extremes: Theory
and Applications. Wiley. Chichester.
Buck, J. and Kl¨uppelberg, C. (2019). Recursive max-linear models with propagating noise. In
preparation.
Chautru, E. (2015). Dimension reduction in multivariate extreme value analysis. Electronic Jour-
nal of Statistics, 9, 383–418.
Cooley, D. and Thibaud, E. (2019). Decompositions of dependence for high-dimensional ex-
tremes. Biometrika, 106(3), 587–604.
de Haan, L. and Ferreira, A. (2006). Extreme Value Theory: An Introduction. Springer. New
York.
Diestel, R. (2010). Graph Theory. 4th edn. Springer. Heidelberg.
Einmahl, J. H. J., Kiriliouk, A., Krajina, A. and Segers, J. (2016). An M-estimator of spatial
tail dependence. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
78(1), 275–298.
30

Einmahl, J. H. J., Kiriliouk, A. and Segers, J. (2018). A continuous updating weighted least
squares estimator of tail dependence in high dimensions. Extremes, 21(2), 205–233.
Einmahl, J. H. J., Krajina, A. and Segers, J. (2012). An M-estimator for tail dependence in
arbitrary dimensions. The Annals of Statistics, 40(3), 1764–1793.
Embrechts, P., Kl¨uppelberg, C. and Mikosch, T. (1997). Modelling Extremal Events for Insurance
and Finance. Springer. Heidelberg.
Engelke, S. and Hitz, A. S. (2018). Graphical models for extremes. arXiv:1812.01734.
Gissibl, N. and Kl¨uppelberg, C. (2018). Max-linear models on directed acyclic graphs. Bernoulli,
24(4A), 2693–2720.
Gissibl, N., Kl¨uppelberg, C. and Lauritzen, S. L. (2018). Identiﬁability and estimation of recur-
sive max-linear models. arXiv:1901.03556.
Gissibl, N., Kl¨uppelberg, C. and Otto, M. (2018). Tail dependence of recursive max-linear models
with regularly varying noise variables. Econometrics and Statistics, 6, 149 – 167.
Gnecco, N., Meinshausen, N., Peters, J. and Engelke, S. (2019). Causal discovery in heavy-tailed
models. arXiv:1908.05097.
Haug, S., Kl¨uppelberg, C. and Kuhn, G. (2015). Copula structure analysis based on extreme
dependence. Statistics and Its Interface, 8(1), 93–107.
Janssen, A. and Wan, P. (2019). k-means clustering of extremes. arXiv:1904.02970.
Kl¨uppelberg, C. and Lauritzen, S. (2020). Bayesian networks for max-linear models. in F. Biagini,
G. Kauermann and T. Meyer-Brandis, eds, ‘Network Science - An Aerial View from Diﬀerent
Perspectives’. Springer.
Krali,
M.
(2018).
Causality
and
Estimation
of
Multivariate
Extremes
on
Directed
Acyclic
Graphs. Master’s
thesis.
Technical
University
of
Munich.
(Available
from:
https://mediatum.ub.tum.de/doc/1447163/1447163.pdf)
Larsson, M. and Resnick, S. I. (2012). Extremal dependence measure and extremogram: the
regularly varying case. Extremes, 15, 231–256.
Lauritzen, S. L. (1996). Graphical Models. Clarendon Press. Oxford, United Kingdom.
R Core Team (2016). R: A Language and Environment for Statistical Computing. R Foundation
for Statistical Computing. Vienna, Austria. (Available from: https://www.R-project.org)
Resnick, S. I. (1987). Extreme Values, Regular Variation, and Point Processes. Springer. New
York.
Resnick, S. I. (2004). The extremal dependence measure and asymptotic independence. Stoch.
Models, 20(2), 205–227.
Resnick, S. I. (2007). Heavy-Tail Phenomena: Probabilistic and Statistical Modeling. Springer.
New York.
Segers, J. (2019). One- versus multi-component regular variation and extremes of Markov trees.
arXiv:1902.02226.
31

32

A
Figures: Portfolio Data
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Smoke
Returns for Healthcare
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Smoke
Simulated Returns for Healthcare
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Smoke
Returns for Elc. Eq.
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Smoke
Simulated Returns for Elc. Eq.
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Smoke
Returns for Utilities
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Smoke
Simulated Returns for Utilities
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Smoke
Returns for Others
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Smoke
Simulated Returns for Others
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Smoke
Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Smoke
Simulated Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Smoke
Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Smoke
Simulated Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Healthcare
Returns for Elc. Eq.
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Healthcare
Simulated Returns for Elc. Eq.
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Healthcare
Returns for Utilities
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Healthcare
Simulated Returns for Utilities
33

0
10
20
30
40
50
0
10
20
30
40
50
Returns for Healthcare
Returns for Others
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Healthcare
Simulated Returns for Others
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Healthcare
Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Healthcare
Simulated Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Healthcare
Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Healthcare
Simulated Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Elc. Eq.
Returns for Utilities
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Elc. Eq.
Simulated Returns for Utilities
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Elc. Eq.
Returns for Others
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Elc. Eq.
Simulated Returns for Others
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Elc. Eq.
Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Elc. Eq.
Simulated Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Elc. Eq.
Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Elc. Eq.
Simulated Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Utilities
Returns for Others
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Utilities
Simulated Returns for Others
34

0
10
20
30
40
50
0
10
20
30
40
50
Returns for Utilities
Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Utilities
Simulated Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Utilities
Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Utilities
Simulated Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Others
Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Others
Simulated Returns for Fab. Pr.
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Others
Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Others
Simulated Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Returns for Fab. Pr.
Returns for Chemicals
0
10
20
30
40
50
0
10
20
30
40
50
Simulated Returns for Fab. Pr.
Simulated Returns for Chemicals
Figure 6: Bivariate extremes above a high radial threshold for the DAG in Figure 4: The left plots
contain the tails from the real data, the right ones contain simulated realizations.
35

B
Figures: Food Components
0
20
40
60
80
0
20
40
60
80
DR1TVARA
DR1TBCAR
0
20
40
60
80
0
20
40
60
80
Simulated DR1TVARA
Simulated DR1TBCAR
0
20
40
60
80
0
20
40
60
80
DR1TVARA
DR1TLZ
0
20
40
60
80
0
20
40
60
80
Simulated DR1TVARA
Simulated DR1TLZ
0
20
40
60
80
0
20
40
60
80
DR1TVARA
DR1TACAR
0
20
40
60
80
0
20
40
60
80
Simulated DR1TVARA
Simulated DR1TACAR
0
20
40
60
80
0
20
40
60
80
DR1TBCAR
DR1TLZ
0
20
40
60
80
0
20
40
60
80
Simulated DR1TBCAR
Simulated DR1TLZ
0
20
40
60
80
0
20
40
60
80
DR1TBCAR
DR1TACAR
0
20
40
60
80
0
20
40
60
80
Simulated DR1TBCAR
Simulated DR1TACAR
0
20
40
60
80
0
20
40
60
80
DR1TLZ
DR1TACAR
0
20
40
60
80
0
20
40
60
80
Simulated DR1TLZ
Simulated DR1TACAR
Figure 7: Bivariate extremes above a high radial threshold for the DAG in Figure 5: The left plots
contain the tails from the real data, the right ones contain simulated realizations.
36

