PyTorchPipe: a framework for rapid prototyping of
pipelines combining language and vision
Tomasz Kornuta
IBM Research AI
Almaden Research Center
San Jose, CA 15120
tkornut@us.ibm.com
Abstract
Access to vast amounts of data along with affordable computational power stim-
ulated the reincarnation of neural networks. The progress could not be achieved
without adequate software tools, lowering the entry bar for the next generations of
researchers and developers. The paper introduces PyTorchPipe (PTP), a framework
built on top of PyTorch. Answering the recent needs and trends in machine learning,
PTP facilitates building and training of complex, multi-modal models combining
language and vision (but is not limited to those two modalities). At its core, PTP
employs a component-oriented approach and relies on the concept of a pipeline,
deﬁned as a directed acyclic graph of loosely coupled components. A user deﬁnes
a pipeline using yaml-based (thus human-readable) conﬁguration ﬁles, whereas
PTP provides generic workers for their loading, training, and testing using all the
computational power (CPUs and GPUs) that is available to the user. The paper
covers the main concepts of PyTorchPipe, discusses its key features and brieﬂy
presents the currently implemented tasks, models and components.
1
Introduction
Deep learning [LeCun et al., 2015] enabled impressive improvement in performance across many
problem domains, and neural network based models became dominating solutions, achieving state of
the art performances, e.g., in speech recognition [Graves et al., 2013], image classiﬁcation [Krizhevsky
et al., 2012], object detection [Redmon et al., 2016], instance segmentation [He et al., 2017], question
answering [Weston et al., 2014] or machine translation [Bahdanau et al., 2014]. This would not
be possible without (1) access to large-scale datasets, (2) harnessing the computational power and
parallel processing of GPUs and (3) the appropriate software. Starting from low-level libraries
such as CUDA [Sanders and Kandrot, 2010], the AI community has been developing tools and
libraries offering more and more levels of abstractions above the multiply–accumulate operations
and elementary tensor algebra, e.g. Torch [Collobert et al., 2002], Theano [Bastien et al., 2012],
Chainer [Tokui et al., 2015] or TensorFlow [Abadi et al., 2016]. Those API based tools facilitate
building and training of deeper and more complex neural models and signiﬁcantly lower the entry bar
for new users and researchers. As the community started to shift towards multi-problem suites such
as MS COCO [Lin et al., 2014], being a large-scale object detection, segmentation, and captioning
dataset, or ParlAI [Miller et al., 2017] and AllenNLP [Gardner et al., 2017], offering wide varieties
of linguistic tasks, researchers started to feel the need for more specialized APIs that will enable
comparison of different models under exactly the same settings and target the so-called reproducibility
crisis [Hutson, 2018], with examples being object detection APIs such as Detectron [Girshick et al.,
2018] for PyTorch [Paszke et al., 2017] or object detection API in TensorFlow [Huang et al., 2017].
The need for more abstraction from both problem domain (dataset) and model specialized for a given
domain led to development of the next generation of tools, such as Tensor2Tensor [Vaswani et al.,
SysML 2019 workshop at 33rd Conference on Neural Information Processing Systems (NeurIPS 2019).
arXiv:1910.08654v1  [cs.LG]  18 Oct 2019

2018], MI-Prometheus [Kornuta et al., 2018], Pytia [Singh et al., 2018], OpenSeq2Seq [Kuchaiev
et al., 2018] or Ludwig [Molino, 2019]. Aside of the mentioned modularity and abstraction, all those
tools offer powerful scripts, agnostic to dataset and model, that standardize training (and testing)
procedures, once again lowering the entry bar and enabling the user to focus more on the model
rather than on the training loop, validation procedures, utilization of GPUs etc.
Figure 1: Exemplary multi-modal ﬂow diagram. In PTP each block is implemented as a separate
component, whereas framework handles passing data between them and controls the execution order.
As the individual problem domains matured and saturated with diverse solutions, a new strong trend
called Multi-Modal Machine Learning [Baltrušaitis et al., 2018] emerged, with tasks combining vision
and language [Mogadala et al., 2019] such as Image Captioning [Karpathy and Fei-Fei, 2015] and
Image/Visual Question Answering (VQA) [Malinowski and Fritz, 2014, Antol et al., 2015] becoming
slightly more popular than the others. A clearly different nature of those two modalities (spatial vs
sequential) implies diverse transformation that the inputs need to undergo, e.g. convolutional layers
in the case of images and tokenization, indexing, word embeddings etc. in the case of questions.
As a result, the most general VQA architecture consists of four major modules: two encoders
responsible for encoding of raw image and question into more useful representations, a reasoning
module that combines them and decoder that produces the answer. In early VQA systems, reasoning
modules implemented diverse multi-modal fusion mechanisms [Malinowski and Doersch, 2018],
from concatenation to pooling, e.g. MCB [Fukui et al., 2016] and MLB [Kim et al., 2017], to
diverse (co-)attention mechanisms, e.g. question-driven attention over image features [Kazemi and
Elqursh, 2017]. More recently, researchers have focused on reasoning mechanisms such as Relational
Networks [Santoro et al., 2017, Desta et al., 2018] and Memory, Attention and Composition (MAC)
networks [Hudson and Manning, 2018, Marois et al., 2018]. Still, they all can ﬁt into the architectural
pattern deﬁned above. Similarly, one can assume adequate architectural patterns for Image Captioning
or any other task, e.g. Visual Dialog [Das et al., 2017] or Visual Image Generation [Mogadala et al.,
2019]. Such assumptions enable development of software facilitating their creation, for example,
OpenSeq2Seq assumes model to follow the Encoder(s)-Decoder architecture, Ludwig imposes it
to be composed of Encoder(s)-(Combiner)-Decoder, whereas Pytia assumes that user will create a
monolithic model, but instead standardizes interfaces by enforcing that batches fetched by all datasets
(Tasks) would have exactly the same signature consisting of (image, context, text, output).
However, in practice, some additional computations that need to be done. When looking at an
exemplary VQA system presented in Fig. 1, one might notice that answers also undergo some
transformations (from words to indices/one-hot encoding) and aside of loss there are some additional
modules calculating statistics. Besides, for the test split the answers are typically not provided.
Hence some of the modules should not be executed at all when working with that split. Finally,
2

it is a common practice [Jiang et al., 2018, Teney et al., 2018] to use transfer learning [Pan and
Yang, 2009] and incorporate into the model image encoders and/or word embeddings pre-trained a
priori on external datasets. Moreover, typical approach during training is to freeze the encoder(s)
weights at ﬁrst (which enables faster convergence), but at the end to unfreeze them and ﬁne-tune the
whole model jointly (which gives a slight accuracy boost). Those observations lead to relaxation of
assumptions regarding utilization of ﬁxed architectural patterns and formulated the requirement for
ﬂexible decomposition of models into smaller modules that laid the core foundation for PyTorchPipe.
2
PyTorchPipe
PyTorchPipe (PTP)1, being the main contribution of this paper, is a software framework build
on top of PyTorch [Paszke et al., 2017]. PTP is a component-oriented framework that facilitates
development and rapid prototyping of computational multi-modal pipelines and comparison of
diverse neural network-based models. The most important feature of PTP is the decomposition
of complex "monolithic" models into graphs with many inter-connected computational nodes,
that increases their reusability while still enabling their joint training. As a consequence, PyTorchPipe
enables:
• Plugging in/out “modules” (components) at run-time2,
• Importing the pre-trained models (or their “parts”) and saving them during/after training,
• Freezing/unfreezing of the models (or their “parts”) on demand (at run-time),
• Run-time parametrization of all “modules”,
• Pipeline-agnostic, generic scripts for training and testing, enabling run-time parametrization
(hyper-parameters) and utilization of many CPUs/GPUs on-demand.
Additionally, PTP offers several other functionalities useful for rapid prototyping and training of
new models, such as: different optimizers, sampling methods, automated logging and statistics
collection facilities, the export of statistics to ﬁles (and TensorBoard) and automated saving of the
best performing pipelines/models during training. Moreover, PTP provides out-of-a-box library of
diverse, parameterizable components that can be used for rapid creation of various prototypes.
Figure 2: Architecture of the PyTorchPipe framework.
From the architectural point of view, PTP can be seen as a stack of layers, as presented in Fig. 2. The
two bottom layers are external libraries that PTP and its components depend on. The Framework
Core layer contains all the tools and utilities realizing core functionalities, i.e. loading conﬁguration
ﬁles into registry, managing registry and global parameters, building the pipeline based on loaded
conﬁguration, component factories, loggers, statistics collectors and aggregators. Workers later use
those for building and deploying the actual pipelines and execution of experiments. The Components
1https://github.com/ibm/pytorchpipe
2Run-time is deﬁned here as the moment of execution of the experiment.
3

layer contains the implementations of components along with their default conﬁgurations that are
loaded and managed by the workers. Finally, the Experiments layer contains ﬁles associated with a
given experiment that the user wants to perform, staring from the conﬁguration ﬁle(s), all the logs and
statistic collected and saved to ﬁles during experiment execution, ﬁles associated with a given dataset,
loaded and saved checkpoint(s) with weights of the models incorporated in the pipeline etc. Please
note that the layer architecture reﬂects three types of user roles classiﬁed by their expertise, from top:
• Experiment runners might have little to no knowledge of the underlying models, training
procedures, methods (backpropagation) etc. but are able to deﬁne experiment by writing
conﬁguration ﬁle(s) incorporating the existing components and relying on existing workers3.
• Component developers must understand how a given model works, however, do not need to
understand every aspect of the training/testing procedures nor how PTP operates internally.
• Worker developers must understand both how models and training/testing work, as well as
be familiar with internal operation of PTP core, how pipelines are assembled etc.
2.1
Pipelines
What the previously mentioned tools typically deﬁne as a Model, in PTP is framed as a Pipeline:
a directed acyclic graph (DAG) consisting of many inter-connected components. The components
are loosely coupled and care only about the Input Streams they retrieve from and Output Streams
they publish to. There are three special classes of components: Task, Model and Loss. Tasks
are components feeding batches of samples into pipelines, and due to the training methodologies
that interlace training and validation batches there are treated is a slightly different way, thus are
in fact outside of pipeline. Models are components containing trainable weights, whereas Losses
are components calculating values of loss function for a given batch (and from which gradients are
back-propagated). Once the components are implemented, a user deﬁnes a pipeline by writing a
conﬁguration ﬁle (Fig. 3). Components are executed following the priorities deﬁned by the user.
A pipeline can consist of any number of components of a given type, including many Models
and many Losses (what enables e.g. Multi-Task Learning [Caruana, 1997]) and other components
providing required transformations and computations. The user connects the components by explicitly
indicating the data streams connections; Naming Facilities play an important role here, enabling
one to rename any stream at the run time, without the need to change the component code. The
framework offers full ﬂexibility and it is up to the programmer to choose the granularity of his/her
components/models/pipelines.
training:
task:
type: task_type1
streams:
output1: data_stream_x
output2: data_stream_y
pipeline:
component_b:
priority: 2
type: model_type2
streams:
input: data_stream_y
output: data_stream_z
component_c:
priority: 3
type: loss_type3
streams:
input1: data_stream_x
input2: data_stream_z
Figure 3: Exemplary pipeline (left) with its YAML deﬁnition (right).
3Also note that all PTP components and workers come with default conﬁguration ﬁles, so one does not have
to look at the actual implementation when deﬁning its own experiment.
4

2.2
Component
All components have two basic operation modes: Initialization and Execution (Fig. 4), that also
reﬂect two main modes of PTP workers. During Initialization workers load conﬁguration ﬁle(s) and
create all instances of the components in the pipeline. Each component receives its subsection of
conﬁguration ﬁle and uses it to set its internal variables, rename stream/global names, get/set global
variables, and perform other component-speciﬁc operations (load dataset/create nn module etc.). As
soon as all components are initialized the worker performs handshaking, i.e. makes sure that all
streams are correctly connected by checking compatibility of their output-input deﬁnitions. Initializa-
tion happens only once and when ﬁnished, workers switch to Execution mode. During Execution
each component processes data from input streams and publishes results to output streams, logs its
operation, collects statistics (automatically exported to CSV ﬁles and, optionally, to Tensorboard) etc.
Figure 4: Component operation modes
2.3
Workers
PTP workers are python scripts that are agnostic to tasks/models/components/pipelines that they are
supposed to work with. Currently our framework offers three types of workers:
• ptp-ofﬂine-trainer: a trainer relying on classical methodology interlacing training and
validation at the end of every epoch; creates separate instances of training and validation
tasks and trains the models by feeding the created pipeline with batches of data.
• ptp-online-trainer: a ﬂexible trainer creating separate instances of training and validation
tasks; validation is performed using a single batch every user-deﬁned interval; relying on
the notion of an episode rather than epoch.
• ptp-processor: performing a single pass over all the samples (batches) returned by a given
task; useful for collecting scores on test sets, answers for submissions for competitions etc.
In its core, to accelerate the computations on their own, PTP relies on PyTorch and all workers
extensively use its mechanisms for distribution of computations on CPUs/GPUs, including multi-
process data loaders and multi-GPU data parallelism. The Tasks, Models and other components
are agnostic to those operations and the user indicates whether to use the former (data loaders) in
conﬁguration ﬁles or the latter (GPUs) by passing an adequate argument (–gpu) at run-time.
2.4
Task/Model/Component Zoo
PTP is generally agnostic to operations performed by the components and data passed in data streams.
However, the currently provided tasks, models and components focus mostly on applications that can
be roughly classiﬁed as belonging to: (a) computer vision domain, (b) Natural Language Processing
domain and (c) domain combining vision and language. Fig. 5 reﬂect this. Aside of selection
5

of tasks covering various aspects from those three domains, PTP offers several models that are
specialized for language or vision, but also includes several general usage components, such as
Feed Forward Network (with variable number of Fully Connected layers with activation functions
and dropout between them) or Recurrent Neural Network (with several cell types available; with
activation functions and dropout; the component can also work as encoder or decoder). PTP also
provides several ready to use non-trainable (but still parametrizable) components. That list includes
components useful when working with language, vision or other types of streams (e.g., tensor
transformations). There are also several general-purpose components, such as components calculating
losses and statistics, and publishers and viewers that enable the user to digest and analyze contents of
data streams. The diversity of those components illustrates the ﬂexibility of the framework.
Figure 5: Tasks, models and other components currently available in PTP
3
Use case: the Image-CLEF Med-VQA 2019 challenge
The VQA-Med 2019 Ben Abacha et al. [2019] is an open challenge organized as part of the Image-
CLEF 2019 initiative Ionescu et al. [2019]. The associated dataset belongs to the Visual Question
Answering (VQA) problem domain. with a focus on radiology images. Despite its small size (a
training set of 3,200 images with 12,792 question-answer pairs, divided into four C1–C4 categories)
the dataset is scattered, noisy and heavily biased, thus we decided to use it as a test bed for PTP, as
we could encounter all the challenges that one must deal with in practical scenarios.
Figure 6: Multi-stage training of the Supporting Facts Network
Summarizing our efforts, within a month we have build more than 50 diverse pipelines, developed
dozens of components and made 4 entries to the leaderboard by submitting the ﬁles generated straight
6

in PTP. Our ﬁnal model, called Supporting Facts Network (SFN) [Kornuta et al., 2019], shared
knowledge between upstream and downstream tasks through the use of a coarse-grained categorizer
combined with ﬁve task-speciﬁc, ﬁne-grained classiﬁers. Training of the ﬁnal SFN architecture
(the whole ﬁnal pipeline had 53 components!) was a complex procedure, with several transfer
learning steps (represented as red arrows in Fig. 6): (0) using pre-trained "Word Embeddings"
(GloVe.6B.50d) and "Feature Map Extractor" (ResNet-50), (1) pre-training of ”Question Categorizer”
on C1,C2,C3 and C4 categories, (2) pre-training “Input Fusion” on C1,C2 and C3 categories, (3)
loading and freezing the ”Question Categorizer”, loading and ﬁne-tuning the "Input Fusion" and (4)
training jointly the ﬁnal SFN architecture using 5 losses (one for each of the ﬁne-grained classiﬁer).
Despite we haven’t won the challenge (ranked 7), we found PTP extremely useful, supporting all the
abovementioned operations and enabling us to rapidly prototype new solutions and ideas.
References
M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard, et al. TensorFlow: a system for large-scale machine learning. In OSDI, volume 16, pages
265–283, 2016.
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh. Vqa: Visual
question answering. In Proceedings of the IEEE international conference on computer vision,
pages 2425–2433, 2015.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473, 2014.
T. Baltrušaitis, C. Ahuja, and L.-P. Morency. Multimodal machine learning: A survey and taxonomy.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):423–443, 2018.
F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-
Farley, and Y. Bengio.
Theano: new features and speed improvements.
arXiv preprint
arXiv:1211.5590, 2012.
A. Ben Abacha, S. A. Hasan, V. V. Datla, J. Liu, D. Demner-Fushman, and H. Müller. VQA-Med:
Overview of the medical visual question answering task at imageclef 2019. In CLEF 2019 Working
Notes, CEUR Workshop Proceedings, Lugano, Switzerland, September 09-12 2019. CEUR-WS.org
< http : //ceur −ws.org/ >.
R. Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.
R. Collobert, S. Bengio, and J. Mariéthoz. Torch: a modular machine learning software library.
Technical report, Idiap, 2002.
A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and D. Batra. Visual
dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 326–335, 2017.
M. T. Desta, L. Chen, and T. Kornuta.
Object-based reasoning in VQA.
arXiv preprint
arXiv:1801.09718, 2018.
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach. Multimodal compact
bilinear pooling for visual question answering and visual grounding. In EMNLP, 2016.
M. Gardner, J. Grus, M. Neumann, O. Tafjord, P. Dasigi, N. F. Liu, M. Peters, M. Schmitz, and L. S.
Zettlemoyer. Allennlp: A deep semantic natural language processing platform. 2017.
R. Girshick, I. Radosavovic, G. Gkioxari, P. Dollár, and K. He. Detectron. https://github.com/
facebookresearch/detectron, 2018.
A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In
2013 IEEE international conference on acoustics, speech and signal processing, pages 6645–6649.
IEEE, 2013.
K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision, pages 2961–2969, 2017.
7

J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song,
S. Guadarrama, and Kevin. Speed/accuracy trade-offs for modern convolutional object detectors.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7310–
7311, 2017.
D. A. Hudson and C. D. Manning. Compositional attention networks for machine reasoning. In
International Conference on Learning Representations (ICLR), 2018.
M. Hutson. Artiﬁcial intelligence faces reproducibility crisis. Science, 359(6377):725–726, 2018.
B. Ionescu, H. Müller, R. Péteri, Y. D. Cid, V. Liauchuk, V. Kovalev, D. Klimuk, A. Tarasau,
A. Ben Abacha, S. A. Hasan, V. Datla, J. Liu, D. Demner-Fushman, D.-T. Dang-Nguyen, L. Piras,
M. Riegler, M.-T. Tran, M. Lux, C. Gurrin, O. Pelka, C. M. Friedrich, A. G. S. de Herrera,
N. Garcia, E. Kavallieratou, C. R. del Blanco, C. C. Rodríguez, N. Vasillopoulos, K. Karampidis,
J. Chamberlain, A. Clark, and A. Campello. ImageCLEF 2019: Multimedia retrieval in medicine,
lifelogging, security and nature. In Experimental IR Meets Multilinguality, Multimodality, and
Interaction, Proceedings of the 10th International Conference of the CLEF Association (CLEF
2019), Lugano, Switzerland, September 9-12 2019. LNCS Lecture Notes in Computer Science,
Springer.
Y. Jiang, V. Natarajan, X. Chen, M. Rohrbach, D. Batra, and D. Parikh. Pythia v0. 1: the winning
entry to the vqa challenge 2018. arXiv preprint arXiv:1807.09956, 2018.
A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128–3137,
2015.
V. Kazemi and A. Elqursh. Show, ask, attend, and answer: A strong baseline for visual question
answering. arXiv preprint arXiv:1704.03162, 2017.
J.-H. Kim, K.-W. On, W. Lim, J. Kim, J.-W. Ha, and B.-T. Zhang. Hadamard product for low-rank
bilinear pooling. In ICLR, 2017.
T. Kornuta, V. Marois, R. L. McAvoy, Y. Bouhadjar, A. Asseman, V. Albouy, T. Jayram, and A. S.
Ozcan. Accelerating machine learning research with mi-prometheus. In NeurIPS Workshop on
Machine Learning Open Source Software (MLOSS), volume 2018, 2018.
T. Kornuta, D. Rajan, C. Shivade, A. Asseman, and A. S. Ozcan. Leveraging medical visual question
answering with supporting facts. arXiv preprint arXiv:1905.12008, 2019.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097–1105, 2012.
O. Kuchaiev, B. Ginsburg, I. Gitman, V. Lavrukhin, J. Li, H. Nguyen, C. Case, and P. Micikevi-
cius. Mixed-precision training for nlp and speech recognition with openseq2seq. arXiv preprint
arXiv:1805.10387, 2018.
Y. LeCun, Y. Bengio, and G. Hinton. Deep Learning. Nature, 521(7553):436, 2015.
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick.
Microsoft coco: Common objects in context. In European conference on computer vision, pages
740–755. Springer, 2014.
M. Malinowski and C. Doersch. The visual qa devil in the details: The impact of early fusion and
batch norm on clevr. arXiv preprint arXiv:1809.04482, 2018.
M. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes
based on uncertain input. In Advances in neural information processing systems, pages 1682–1690,
2014.
V. Marois, T. Jayram, V. Albouy, T. Kornuta, Y. Bouhadjar, and A. S. Ozcan. On transfer learning
using a MAC model variant. In NeurIPS’18 Visually-Grounded Interaction and Language (ViGIL)
Workshop, 2018.
8

A. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bordes, D. Parikh, and J. Weston. Parlai: A
dialog research software platform. arXiv preprint arXiv:1705.06476, 2017.
A. Mogadala, M. Kalimuthu, and D. Klakow. Trends in integration of vision and language research:
A survey of tasks, datasets, and methods. arXiv preprint arXiv:1907.09358, 2019.
P. Molino. Ludwig: a type-based declarative deep learning toolbox. To appear, 2019.
S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data
engineering, 22(10):1345–1359, 2009.
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic differentiation in PyTorch. 2017.
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Uniﬁed, real-time object
detection. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 779–788, 2016.
J. Sanders and E. Kandrot. CUDA by example: an introduction to general-purpose GPU programming.
Addison-Wesley Professional, 2010.
A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap.
A simple neural network module for relational reasoning. In Advances in neural information
processing systems, pages 4967–4976, 2017.
A. Singh, V. Natarajan, Y. Jiang, X. Chen, M. Shah, M. Rohrbach, D. Batra, and D. Parikh. Pythia-a
platform for vision & language research. In SysML Workshop, NeurIPS, volume 2018, 2018.
D. Teney, P. Anderson, X. He, and A. van den Hengel. Tips and tricks for visual question answering:
Learnings from the 2017 challenge. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4223–4232, 2018.
S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a next-generation open source framework for
deep learning. In Proceedings of workshop on machine learning systems (LearningSys) in the
twenty-ninth annual conference on neural information processing systems (NIPS), volume 5, pages
1–6, 2015.
A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws, L. Jones, Ł. Kaiser,
N. Kalchbrenner, N. Parmar, et al. Tensor2tensor for neural machine translation. arXiv preprint
arXiv:1803.07416, 2018.
J. Weston, S. Chopra, and A. Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.
9

