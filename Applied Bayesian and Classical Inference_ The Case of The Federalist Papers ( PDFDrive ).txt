Springer Series in Statistics 
Advisors: 
D. Brillinger, S. Fienberg, J. Gani, 
J. Hartigan, K. Krickeberg 

Springer Series in Statistics 
L. A. Goodman and W. H. Kruskal, Measures of Association for Cross Classifications. 
x, 146 pag!ils, 1979. 
J. O. Berger, $tatistical Decision Theory: Foundations, Concepts, and Methods. xiv, 
4~5 pages, 1980. 
R. G. Miller, Jr., Simultaneous Statistical Inference, 2nd edition. xvi, 299 pages, 1981. 
P. Bremaud, Point Processes and Queues: Martingale Dynamics. xviii, 354 pages, 
1981. 
. 
E. Seneta, Non-Negative Matrices and Markov Chains. xv, 279 pages, 1981. 
F. J. Anscombe, Computing in Statistical Science through APL. xvi, 426 pages, 1981. 
J. W. Pratt and J. D. Gibbons, Concepts of Nonparametric Theory. xvi, 462 pages, 
1981. 
V. Vapnik, Estimation of Dependences based on Empirical Data. xvi, 399 pages, 1982. 
H. Heyer, Theory of Statistical Experiments. x, 289 pages, 1982. 
L. Sachs, Applied Statistics: A Handbook of Techniques, 2nd edition. xxviii, 707 
pages, 1984. 
M. R. Leadbetter, G. Lindgren and H. Rootzen, Extremes and Related Properties of 
Random Sequences and Processes. xii, 336 pages, 1983. 
H. Kres, Statistical Tables for Multivariate Analysis. xxii, 504 pages, 1983. 
J. A. Hartigan, Bayes Theory. xii, 145 pages, 1983. 
F. Mosteller, O.S. Wallace, Applied Bayesian and Classical Inference: The Case of 
The Federalist Papers. xxxv, 301 pages, 1984. 
D. Pollard, Convergence of Stochastic Processes. xiv, 215 pages, 1984. 

Frederick Mosteller 
David L. Wallace 
Applied Bayesian and 
Classical Inference 
The Case of The Federalist Papers 
2nd Edition of Inference and Disputed Authorship: 
The Federalist 
Springer-Verlag 
New York 'Berlin Heidelberg Tokyo 

Frederick Mosteller 
Department of Statistics 
Harvard University 
Cambridge, MA 02138 
U.S.A. 
AMS Classification: 6201, 62F15 
Library of Congress Cataloging in Publication Data 
Mosteller, Frederick 
Applied Bayesian and classical inference. 
(Springer series in statistics) 
Earlier ed.published in 1964. 
Bibliography: p. 
Includes index. 
David L. Wallace 
Department of Statistics 
University of Chicago 
Chicago, IL 60637 
U.S.A. 
1. Federalist. 2. English language-Word frequency-
Case studies. 
I. Wallace, David L.(David Lee), 
1928-
II. Mosteller, Frederick, 1916-
Inference and disputed authorship, The Federalist. 
III. Title. IV. Series. 
JK155.M59 1984 
342.73'029 
84-5489 
With 11 illustrations. 
The first edition of this book,Irlference and Disputed Authorship: The Federalist, 
was previously published by Addison-Wesley Publishing Company, Inc., Reading, 
MA, in 1964. 
© 1964, 1984 by Springer-Verlag New York Inc. 
Softcover reprint of the hardcover 2nd edition 1984 
All rights reserved. No part of this book may be translated or reproduced in any 
form without written permission from Springer-Verlag, 175 Fifth Avenue, New 
York, New York 10010, U.S.A. 
ISBN-13: 978-1-4612-9759-8 
e-ISBN-13: 978-1-4612-5256-6 
DOl: 10.1007/978-1-4612-5256-6 

To the memory of 
Edwin Glenn Olds 
and 
Samuel Stanley Wilks 

Preface to the Second Edition 
The new version has two additions. First, at the suggestion of Stephen 
Stigler I we have replaced the Table of Contents by what he calls an Analytic 
Table of Contents. Following the title of each section or subsection is a 
description of the content of the section. This material helps the reader in 
several ways, for example: by giving a synopsis of the book, by explaining 
where the various data tables are and what they deal with, by telling what 
theory is described where. We did several distinct full studies for the 
Federalist papers as well as many minor side studies. Some or all may offer 
information both to the applied and the theoretical reader. We therefore 
try to give in this Contents more than the few cryptic words in a section 
heading to ~peed readers in finding what they want. 
Seconq, we have prepared an extra chapter dealing with authorship work 
published from. about 1969 to 1983. Although a chapter cannot compre-
hensively Gover a field where many books now appear, it can mention most 
ofthe book-length works and the main thread of authorship' studies published 
in English. We founq biblical authorship studies so extensive and com-
plicated that we thought it worthwhile to indicate some papers that would 
bring out the controversies that are taking place. We hope we have given the 
flavor of developments over the 15 years mentioned. 
We have also corrected a few typographical errors. 
As usual, many have helped us. Erich Lehmann and Walter Kaufmann-
Biihler suggested that we prepare this new edition. Many have advised us 
about the writing or contributed material-Persi Diaconis, Bradley Efron, 
Alvar Ellegard, John D. Emerson, Irene Fairley, Katherine Godfrey, 
Katherine Taylor Halvorsen, David C. Hoaglip., Peter J. Kempthorne, 
Erich L. Lehmapn, Colin Mallows, Lincoln E. Moses, Marjorie Olson, 
Stephen L. Portnoy, Stephen M. Stigler, and Wing H. Wong. Augustine 
Kong carried out an important computer search of the literature. Cleo Youtz 
edited and re-edited the manuscript of the new material and checked it for 
accuracy. The work was partly facilitated by National Science Foundation 
Grant SES 8023644 to Harvard University. 
' 
December 24, 1983 
F. M. and D. L. W. 
vii 

Preface to the First Edition 
We apply a 200-year-old mathematical theorem to a 175-year-old historical 
problem, more to advance statistics than history. Though problems of disputed 
authorship are common in history, literature, and politics, scholars regard their 
solutions as minor advances. 
For us the question of whether Hamilton or 
Madison wrote the disputed Federalist papers has served as a laboratory and 
demonstration problem for developing and comparing statistical methods. 
While we help solve this historical problem, our practical application of Bayes' 
theorem to a large analysis of data is 3: step in testing the feasibility of a method 
of inference that has been heavily criticized in the past, but which is currently 
being explored with fresh attitudes and fresh mathematics. Furthermore, large 
practical applications have until now been few, and our work helps fill that gap. 
Historians will find that our results strengthen their general trend of opinion 
in favor of Madison's authorship of the disputed papers by providing a different 
sort of evidence from that they have ordinarily used. They can add these 
results to evidence of other kinds. 
Methods like O\lrs can be used for other authorship studies, and we anticipate 
that the cost will become relatively cheap in the future. Preparing text for a 
high-speed computer is currently responsible for a major part of the cost. 
Savings should come when an electronic reader becomes available that can read 
text directly from printed material into the computer. 
Some may feel that we should make more definite- pr!:mouncements about the 
comparative value of different methods of inference-especially Bayesian versus 
classical. Obviously we are favorably disposed toward the Bayesian approach, 
but in the field of data analysis at least, the techniques are in their infancy. As 
these techniques evolve, their merits and demerits may be easier to judge. 
Even though individual statisticians may claim generally to follow the 
Bayesian school or the classical school, no one has an adequate rule for deciding 
what school is being represented at a given moment. When we have thought we 
were at our most Bayesian, classicists have told us that we were utterly classical; 
and when we have thought ourselves to be giving a classical treatment, Bayesians 
have told us that the ideas are not 'in the classical lexicon. So we cannot pretend 
to speak for anyone but ourselves, and we do this m~hl.ly in Chapter 9 by point-
ing out some developments that we think statisticalinference needs. 
ix 

X 
PREFACE TO THE FIRST EDITION 
After an introduction to the historical problem and the data to be used 
(Chapters 1 and 2), we launch four parallel studies. They are the main Bayesian 
study (Chapters 3 and 4), a standard approach to discrimination (Chapter 5), 
and two other simplified approaches: one Bayesian (Chapter 6) and one using 
Bayesian ideas but not a Bayesian interpretation (Chapter 7). Chapter 8 pre-
sents ancillary studies and describes a simplified approach to an authorship 
problem. Finally, Chapter 9 summarizes our conclusions. A reader who wants 
to know quickly our main results could read Chapters 1, 2, 3, 8, and 9. He 
would skip the parallel developments of Chapters 5,6, and 7, and the extensive 
and mathematical Chapter 4. Though a reading of Chapter 4 might well come 
last and then only by the mathematically inclined, statisticians may find the 
most professional meat there and in Chapter 9. 
The main study gives results in terms of odds. These odds tend to be very 
large for most of the papers, and they are not to be taken at face value. To 
appreciate them or, perhaps better, to depreciate them properly, the reader 
should study Section 3.7F with care. 
A word about our exposition of mathematics outside of Chapter 4 may h!'llp 
bring order out of chaos. When we found we could in a reasonable space ex-
plain a mathematical development in detail for those without much mathe-
matical preparation, we have tried to do this. But where the needed mathemati-
cal preparation was substantial, we have not tried to simplify the exposition. 
We felt that the unprepared reader would have to skip through these more 
difficult developments in any case and that he would have no trouble in recog-
nizing these spots. We have, of course, tried to make the principal results 
understandable for all. 
In discussing our work on The Federalist at the Minnesota meetings reported 
below, Jerzy Neyman suggested that categorizing statistical methods as Bayesian 
or non-Bayesian is less revealing than categorizing them as inferential or be-
havioristic, in either of which Bayes' theorem may often be used. The be-
havioristic approach in our problem calls for establishing a rule for deciding who 
wrote any disputed paper and evaluating or bounding the frequencies of in-
correct classifications if the rule is followed. In the inferential approach, one 
tries to provide odds or other measures of confidence for (or against) Madison's 
authorship of any paper. Under these definitions, our methods of Chapters 3 
and 6 are clearly inferential, though we could, if necessary, immediately specify 
a decision rule, for example, by deciding for Hamilton if the log odds are posi-
tive, for Madison if they are negative. The methods of Chapter 5 and 7 fall 
more nearly in the behavioristic approach, though they become more inferential 
when in Section 5.5 we try to assess the strength of the evidence by computing 
tail probabilities and by estimating confidence limits on the likelihood ratio. 
Neyman also suggested that nonparametric discrimination methods along lines 
developed by Fix and Hodgl3s (1951, 1952) would be of considerable interest in 
our problem. Although we have not followed up this &uggestion, we hope others 
may, and have added the papers to our reference list. 

PREFACE TO THE FIRST EDITION 
xi 
Our references are gathered at the end of the book. 
We are aware of a considerable body of writing both on problems of discrimi-
nation generally and on the analysis of style for purposes of deciding authorship. 
To review these works would require a monograph comparable in size to the 
present one, and so we have not made the attempt. Hodges' (1950) review of 
work in discrimination fills part of this void, though a comparable new review 
would be welcome. 
Acl{nowledgments 
We acknowledge with thanks the many helpful discussions and suggestions 
received from colleagues: Douglass Adair, F. J. Anscombe, Carl Christ, William 
G. Cochran, Arthur P. Dempster, Martin Diamond, John Gilbert, Morris Halle, 
William T. Hutchinson, William Kruskal, David Levin, P. J. McCarthy, Ann 
Mitchell, John W. Pratt, Howard Raiffa, L. J. Savage, Robert Schlaifer, 
Maurice M. Tatsuoka, George B. Thomas, Jr., and John W. Tukey. 
Miles Davis has handled the programming of the high-speed calculations. 
Wayne Wiitanen, C. Harvey Willson, and Robert A. Hoodes, under the direc-
tion of Albert E. Beaton, programmed the word counts. Roger Carlson, Robert 
M. Elashoff, Ivor Francis, Robert Kleyle, Charles Odoroff, P. S. R. S. Rao, and 
Marie Yeager have assisted with many parts of this work. Mrs. Cleo Youtz 
has supervised a number of the studies. Mrs. Virginia Mosteller cooperated 
~n the screening study. 
We appreciate the careful calculations and other work of Linda Alger, Virgil 
Archer, Judithe Bailey, Eleanor Beissel, Barbara Block, Mary Blyman, Alf 
Brandin, John Burnham, Margaret Byron-Scott, Helen Canning, Philip 
Chartrand, Adelle Crowne, Roy D'Andrade, Abraham Davidson, Roy Dooley, 
Sara Dustin, Gerald Fowler, Miriam Gallaher, Yoel Haitovsky, Jane Hallowell, 
Joanna Handlin, Joan Hastings, Elizabeth Ann Hecht, Mervyn Hecht, Ann 
Hilken, Theodore Ingalls, Helen V. Jensen, Kathryn Karrasik, Vincent Kruskal, 
Frederick Loo, Christine Lyman, Nancy McCarthy, William Mosteller, Joseph 
Naus, Loneta Newburgh, Mary Nye, Eva Pahnke, Mitchell Robbins, Susan 
Rogers, Eleanor Rosenberg, Astrid Salvesen, Epifanio San Juan, Jr., Mary 
Shillman, Lucy Steig, Ralph A. Stewart, Jr., Ruth'M. Stouffer, Victoria Sullivan, 
George Telford, Elizabeth Thorndike, Henry Ti~erYi Martha Van Genderen, 
Bruce B. Venrick, C. Kristine Wallstrom, Dtuscilla Wendoloski, Richard 
Wendoloski, Herbert Winokur, and Charles Zimmer. 
For work on the manuscripts leading to this work, we thank Mrs. Jane Adams, 
Mrs. Irene Bickenbach, Mrs. Rita Chartrand, Joan Gobby, Mrs. Vendolyn 
Harris, Mrs. Angela Klein, Mrs. Mary McQuilli~, Janet Mendell, Marguerite 
O'Leary, Mrs. Helena Smith, Mrs. Cleo Youtz, and Phyllis Zamatha. 

xii 
PREFACE 
Several publishers and individuals have kindly given us permission to quote 
from their copyrighted works: 
The quotations in Chapter 1 from the editor's introduction to The Federalist, 
edited by Jacob E. Cooke, copyright © 1961 by Wesleyan University are re-
printed by permission of Wesleyan University Press. 
The quotation early in Chapter 3 from Julian Lowell Coolidge's book, I ntro-
duction to mathematical probability, is reprinted here by permission of the 
copyright owners, the Clarendon Press, Oxford. 
The quotation early in Chapter 3 from Egon Pearson's 1925 article in Bio-
metrika, "Bayes' theorem, examined in the light of experimental sampling," is 
quoted with the permission of the author and the Editor of Biometrika. 
The quotation in Chapter 3 from Joseph Berkson's 1930 article in The Annals 
of Mathematical Statistics, entitled "Bayes' theorem," is given with the per-
mission of the author. 
We reported publicly on this research at a session of Special Papers Invited 
by the Presidents of the American Statistical Association, The Biometric 
Society (ENAR), and The Institute of Mathematical Statistics at the statistical 
meetings in Minneapolis, Minnesota, September 9, 1962. The prepared dis-
cussants were Douglass Adair, F. J. Anscombe, and Jerzy Neyman, with Leo 
Goodman in the chair. We presented the material in Mosteller and Wallace 
(1963). 
This work has been facilitated by grants from The Ford Foundation, The 
Rockefeller Foundation, and from the National Science Foundation NSF 
G-13040 and G-10368, contracts with the Office of Naval Research Nonr 
1866(37), 2121(09), and by the Laboratory of Social Relations, Harvard Uni-
versity. 
The work was done in part at the M.LT. Computation Center, 
Cambridge, Massachusetts, and at the Center for Advanced Study in the 
Behavioral Sciences, Stanford, California. Permission is granted for reproduc-
tion in whole or in part for purposes of the United States Government. 
May, 1964 
F. M. and D. L. W. 

Analytic Table of Contents 
Chapter 1. The Federalist Papers As a Case Study . 
1 
1.1. Purpose . 
1 
To study how Bayesian inference works in a large-scale data analysis, 
we chose to try to resolve the problem of the authorship of the disputed 
Federalist papers. 
1.2. The Federalist papers 
2 
The Federali8t papers were written by Hamilton, Madison, and Jay. 
Jay's papers are known. Of the 77 papers originally published in news-
papers, 12 are in dispute between Hamilton and Madison, and 3 may 
regarded as joint by them. Historians have varied in their attributions. 
1.3. Early work . 
6 
Frederick Williams and Frederick Mosteller found that sentence length 
and its variability within papers did not discriminate. Tables 1.3-1, 
2, 3, 4 show that they found some discriminating power in percentage of 
nouns, of adjectives, of one- and two-letter words, and of the's. Together 
these variables could have decided whether Hamilton or Madison wrote 
all the disputed papers, if that were the problem, but the problem is to 
make an effective assignment for each paper. 
1.4. Recent work-pilot study . 
10 
We call marker words those which one author often uses and the other 
rarely uses. Douglass Adair found while (Hamilton) versus whilst 
(Madison). We found enough (Hamilton) and upon (Hamilton); see 
Tables 1.4-1, 2 for incidence and rates. Tables 1.4-3,4,5 give an over-
view of marker words for Federali8t and non-Federali8t writings. Alone, 
they would not settle the dispute compellingly. 
1.5. Plots and honesty 
. 
14 
Some say that the dispute is not a matter of honesty but a matter of 
memory. Hamilton was hurried in his annotation by an impending duel, 
but Madison had plenty of time. Editing may bd a hazard. We want to 
use many words as discriminating variables. 
1.6. The plan of the book 
15 
xiii 

xiv 
ANALYTIC TABLE OF CONTENTS 
Chapter 2. Words and Their Distributions 
16 
2.1. Why words? 
16 
Hamilton and Madison use the same words at different rates, and so 
their rates offer a vehicle for discrimination. Some words like by and to 
vary relatively little in their rates as context changes, others like war 
vary a lot, as the empirical distributions in the four tables show. 
Generally, less meaningful words offer more stability. 
2.2. Variation with time 
19 
In Table 2.2-2, a separate study illustrated by Madison's rates for 11 
function words over a 26-year period examines the stability of rates 
through time. We desire stability because we need additional text of 
known authorship to choose words and their rates for discriminating 
between authors. Among function words, some pronouns and auxiliary 
verbs seem unstable. 
2.3. How frequency of use varies . 
22 
For establishing a mathematical model, we need to find out empirically 
how rates of use by an author vary from one chunk of writing to another. 
2.3A. Independence of words from one block of text to another . 
23 
A special study of extensive empirical data tests the independence of 
the occurrences of the same word (for 51 words) in four successive blocks 
of approximately 200 words of Hamilton text. Table 2.3-1 compares the 
observed counts with the binomial distributions for the 39 sets of four 
blocks for each word. Some words give evidence oflack ofindependence, 
especially his, one, only, and than. 
2.3B. Frequency of occurrence. 
28 
For 51 words we show in Table 2.3-3 the frequency distribution of 
occurrences in about 250 blocks of 200. The Poisson distribution does not 
fit all the empirical distributions of the number of occurrences of high-
frequency words, but the negative binomial distribution comes close to 
doing so. For 10 of these words Poisson and negative binomials are fitted 
and displayed in Table 2.3-4 for Hamilton and for Madison. The nega-
tive binomial distribution allows for higher tails than does the Poisson. 
2.4. Correlations between rates for different words 
. 
35 
Theoretical study shows that the correlation between the rates of 
occurrence for different words should ordinarily be small but negative. 
An empirical study whose results appear in Table 2.4-1 shows that these 
correlations are ordinarily negligible for our work. 
2.5. Pools of words . 
37 
Three pools of words produced potential discriminators. 
2.5A. The function words . 
39 
From a list of 363 function words prepared by Miller, Newman, and 
Friedman, we selected the 70 highest-frequency and a random 20 low-
frequency words without regard to their ability to discriminate author-
ship. They appear in Tables 2.5-2 and 2.5-3. 

ANALYTIC TABLE OF CONTENTS 
XV 
2.SB. Initial screening study ' .  
39 
We used some of the papers of known authorship to cut 3000 candidate 
words to the 28 listed in Table 2.5-4, based on ability to discriminate. 
2.SC. Word index with frequencies 
42 
From 6700 different words, 103 nort-contextual words were chosen from 
240 that looked promising as discriminators on papers of known author-
ship. Of these words, the 48 in Table 2.5-6 were new. 
2.6. Word counts and their accuracies 
43 
Some word counts were carried out by hand using slips of paper, one 
word per slip. Others were done by a high-speed computer which con-
structed a concordance. 
2.7. Concluding remarks 
. . 
45 
Although words offer only one set of discriminators, one needs a large 
enough pool of potential discriminators to offer a good chance of success. 
We need to avoid selection and regression effects. Ideally we want 
enough data to get a grip on the distribution theory for the variables 
to be used. 
Chapter 3. The Main Study 
46 
In the main study, we use Bayes' theorem to determine odd;> of author-
ship for each disputed paper by weighting the evidence from words. 
Bayesian methods enter centrally in estimating the word rates and 
choosing the words to use as discriminators. We use not one but an 
empirically based range of prior distributions. We present the results 
for the disputed papers and examine the sensitivity of the results to 
various aspects of the analysis. 
After a brief guide to the chapter, we describe some views of prob-
ability as a degree of belief and we discuss the need and the difficulties 
of such an interpretation. 
3.1. Introduction to Bayes' theorem and its applications 
49 
We give an overview, abstracted from technical detail, of the ideas and 
methods of the main study, and we describe the principal sources of 
difficulties and how we go about meeting them. 
3.1A. An example applying Bayes' theorem with . both initial odds and 
parameters known . 5 2  
A simple probability calculation gives the probability of authorship 
from evidence on one word. Casting the resul~, the classical Bayes' 
theorem, into odds form is helpful and gives: 
Final odds = initial odds X likelihood ratio. 
3.1B. Selecting words and weighting their evidence 
54 
Applying Bayes' theorem to several words, and taking logarithms gives 
the final log odds as the sum of initial log odds and the log likelihood 
ratios for the separate words. The difference between the expected log 
likelihood ratio for the two authors is a measure of importance of the 

xvi 
ANALYTIC TABLE OF CONTENTS 
word as a discriminator. We discard words with small importances. No 
bias arises from selection when rates are known. 
3.1C. Initial odds 
Initial odds of authorship reflect the investigator's assessment of the 
historical evidence. The final odds is a product of the initial odds and the 
likelihood ratio, and a large likelihood ratio can overwhelm most vari-
ation in initial odds. We concentrate on the likelihood ratio. Our serious 
use of Bayes' theorem lies elsewhere, in our handling of unknown 
parameters. 
56 
3.1D. Unknown parameters 
57 
Even if data distributions were Poisson, we would not know the mean 
rates. From the known Hamilton and Madison texts, we can estimate the 
rates, but with important uncertainties: the simple use of Bayes' theorem 
is not quite right, and the selection effects in choosing the words are not 
negligible. We treat the rates as random quantities and use the con-
tinuous form of Bayes' theorem to determine the posterior distribution 
to represent their uncertainty. Figure 3.1-1 shows the logical structure 
of the two different uses of Bayes' theorem. The factor from initial odds 
to final odds is. no longer a simple likelihood ratio, but a ratio of two 
averaged probabilities, averaged over the posterior distributions of the 
word rates. The factor can often be approximated by a likelihood ratio 
for an appropriately estimated set of rates. 
3.2. Handling unknown parameters of data distributions 
60 
We begin to set out the components of our Bayesian analysis. 
3.2A. Choosing prior distributions 
61 
We expect both authors to have nearly the same rates for most words, 
we shift to parameters measuring the combined rate and a differential 
rate. For any word, let 0' be the sum of the rates for the two authors and 
let T be the ratio of Hamilton's rate to the sum 0'. Empirical evidence 
on 90 unselected words illustrated in the Figure 3.2-1 plot of estimated 
parameters guides the choice of families of prior distributions for 0' and T. 
3.2B. The interpretation of the prior distributions . 
63 
We work with a parametric family of prior distributions, and call its 
parameters underlying constants. By 1984, hyper parameters has become 
the accepted term for them. 
3.2C. Effect of varying the prior . 
63 
We do not determine a single choice of the underlying constants, but 
study the sensitivity of the assessments of authorship to changes in the 
prior distributions reflecting changes in the underlying constants. 
3.2D. The posterior distribution of (a, 't') 
64 
For any choice of the underlying constants, the joint posterior density 
of (0', T) follows directly from Bayes' theorem. The mode of the posterior 
density can be located by numerical methods and gives the modal 
estimates of parameters used for determining the odds of authorship. 

ANALYTIC TABLE OF CONTENTS 
xvii 
3.2E. Negative binomial 
65 
The negative binomial data distribution underlies our best analysis of 
authorship. The parametrizations and the assumed families of prior 
distributions are set forth. The priors are parametrized by five under-
lying constants. Posterior modal estimates were obtained for all words 
under each of 21 sets of underlying constants. For one typical set, 
Table 3.2-3 presents the modal estimates ofthe negative binomial para-
meters for the final 30 words used to assess the disputed papers. 
3.2F. Final choices of underlying constants . 
67 
Analyses (to be described in Section 4.5) of a pool of90 unselected words 
provide plausible ranges for the underlying constants. Table 3.2-2 shows 
six choices in that range. We interpret the effect of the five underlying 
constants and describe an approximate data-equivalence for the prior 
distributions they specify. 
3.3. Selection of words . 
67 
The prior distributions are the route for allowing and protecting against 
selection effects in choice of words. We use an unselected pool of90 words 
for estimating the underlying constants of the priors, and we assunle the 
priors apply to the populations of words from which we developed our 
pool of 165 words. We then selectively reduce that pool to the final 30 
words. We describe a stratification of words into word groups and our 
deletion of two groups because of contextuality. 
"~~ 
~ 
We compute the logarithm of the odds factor that changes initial odds 
to final odds and call it simply log oddB. The computations use the 
posterior modal estimates as if they were exact and are made under the 
various choices of underlying constants and using both negative binomial 
or Poisson models .. 
3.4A. Checking the method 
69 
Table 3.4-1 shows the total log odds over the 30 final words when each 
of the 98 papers of known authorship is treated as if unknown. It shows 
the results for six choices of prior for the negative binomial, four for the 
Poisson. For almost all papers with known author, the log odds strongly 
favor the actual author. Choice of prior makes about 10 per cent 
difference in the log odds. Choice of data distribution has far larger 
effects. Paper length matters, and paper-to-paper variation is huge. 
3.4B. '.Ole disputed papers 
. . 
75 
For each. disputed paper, Table 3.4-2 shows the log odds factors, totaled 
for the 30 final words, for ten choices of priors, six for the negative 
binomial and four for a Poisson model. The evidence strongly favors 
Madison, with paper 55 weakest with an odds factor of 240 to 1. 
3.5 Log odds hy words and word groups . 
77 
3.5A. Word groups 
77 
Table 3.5-1 breaks the log odds into contributions by the five word 
groups for the disputed, joint, and some papers of known authorship. 
The general consistency of evidence is examined. 

xviii 
ANALYTIC TABLE OF CONTENTS 
3.SB. Single words . 
77 
Tables 3.5...,.2A, B, C show the contributions to the log odds from single 
words: 9 high-frequency words, 11 Hamilton markers, 9 Madison 
markers. The gross difference between behavior of Poisson and nega-
tive binomial models for extreme usages of rare words is illustrated. 
3iSC. Contributions of marker and high-frequency words . 
81 
Table 3.5-3 shows how papers with words at the Madison. mean rate, at 
the Hamilton mean rate, and at the average would be assessed; also how 
papers with all or none of the Hamilton or of the Madison markers 
would fare. The comparisons support the fairness of the final 30 words. 
3.6. Late Hamilton papers . 
. . 
83 
We assess the log odds for four of the late Federalist papers, written by 
Hamilton after the newspaper articles appeared and not used in any of 
our other analyses. The log odds all favor Hamilton, very strongly for 
all but the shortest paper. 
3.7. Adjustments to the log odds 
84 
Through special studies, we estimate the magnitude of effects on the log 
odds of various approximations and imperfect assumptions underlying 
the main computations and results presented in Section 3.4. Percentage 
reductions. ill log odds are a good way to extrapolate from the special 
studies to the main study. 
3.7 A. Correlation 
84 
The study of correlations among words suggests that log odds based on 
independence should be reduced by an amount between 6 per cent and 
12 per cent. 
3.7B. Effects of varying the underlying constants that determine the prior 
distributions 
84 
The choice of prior distribution used in most of the presented results is 
in the middle of the estimated range of the underlying constants. Other 
choices might raise or lower the log odds, but not likely by more than 
±12 per cent. 
3.7C. Accuracy of the approximate log odds calculation 
85 
A study of the approximation for five of the most important words 
suggests that the modal approximation tends to overstate the log odds 
and that a 15 per cent reduction is indicated. 
3.7D. Changes in word countS 
86 
Some word changes between the original newspaper editions and the 
McLean edition we used for making our word counts require adjust-
ment. Two changes involving upon and whilst reduce the log odds for 
Madison in two disputed papers. Other errors, including counting errors, 
are smaller and nearly balanced in direction. 
3.7E. Approximate adjusted log odds for the disputed papers . 
88 
Table 3.7-2 . shows the log odds for the disputed papers after making 
the specific adjustments for· the major word changes, and with three 
levels of a composite adjustment for other effects. Even the extreme 

ANALYTIC TABLE OF CONTENTS 
XIX 
adjustment leaves all but two papers with odds of over 2500 to 1 favoring 
Madison, and the two weakest at 33: 1 (paper *55 with log odds -3.5) 
and 180: 1 (paper*56 with log odds -5.2). 
3.7F. Can the odds be believed? 
. 
The odds, even after adjustment, are often over a million to one, and on 
average about 60,000 to 1. We note that all forms of statistical inference 
have the equivalents of such strong evidence, but in different forms from 
the Bayesian odds calculations. We discuss the believability of such odds 
from the standpoint of statistical models, and then from a broader view-
point external to the model, allowing for what we call outrageous 
events. We examine how one can ever justify strong evidence for dis-
crimination, and how independent evidence can be built up. We see how 
the evidence from upon is reasonable and more defensible for a pro-
Madison finding than it would have been in a pro-Hamilton finding. 
We note some potential failings such as computational and other 
blunders, fraud and serious errors, which can never be absolutely ruled 
out. We offer evidence for the implausibility of Madison's having edited 
Hamilton's papers to look like his own writings in the way we assess his 
style. A probability calculation shows how a small probability of an out-
rageous event has little impact on weak evidence from a statistical 
analysis, but does put a bound on strong evidence. 
Chapter 4. Theoretical Basis of the Main Stndy 
This chapter is a sequence of technical sections supporting the methods 
and results of the main study presented in Chapter 3. We set out the 
distributional assumptions, our methods of determining final odds of 
authorship, and the logical basis of the inference. We explain our 
methods for choosing prior distributions. We develop theory and 
approximate methods to explore the adequacy of the assumptions and 
to support the methods and the findings. 
88 
92 
4.1. The negative binomial distribution 
93 
We review and develop properties of the negative binomial family of 
distributions. 
4.1A. Standard properties . 
93 
For the negative binomial, we set out the frequency function, the 
cumulant generating function, the first four cumulants, the repre-
sentation as a gamma mixture of Poisson distributions, and the limiting 
relationship to the Poisson family. 
4.1B. Distributions of word freqnency 
96 
The mixture representation motivates the negative binomial as a 
distribution of word frequency. 
4.1C. Parametrization . 
96 
Several parametrizations of the negative binomial are compared by 
criteria of interpretability in several modelings, asymptotic ortho-
gonality, and stability of value across applications to different words. 

xx 
ANALYTIC TABLE OF CONTENTS 
We choose the mean and a measure of deviation from the Poisson that 
is not the usual choice. 
4.1D. Estimation 
Parameter estimation by maximum likelihood has no closed forms 
(exceptfor the mean when all paper lengths are the same). The method of 
moments gives initial estimates for use directly or as starting values for 
iteration. Explicit method-of-moments estimates and approximate 
standard errors are given. 
97 
4.2. Analysis of the papers of known authorship 
99 
We treat the choice of prior distributions, the determination of the 
posterior distribution, and the computational problem in finding 
posterior modes. 
4.2A. The data: uotations and distributional assumptions . 
99 
Notation and formal distributional assumptions are set out for all 
words and all papers of known authorship for negative binomial and 
Poisson models. 
4.2B. Object of the analysis 
100 
The odds factor for authorship of any unknown paper is a ratio of 
posterior expectations, taken over the distribution of parameters 
posterior to the data on papers of known authorship. A modal ap-
proximation is natural and leads to the determination of posterior 
modal estimates as a principal intermediate goal of the analysis. 
4.2C. Prior distributions: assumptions 
100 
For each word, two negative binomial parameters describe Hamilton's 
usage, and two describe Madison's usage. These four are reparametrized 
to a form in which a sampling model for a pool of words is in accord with 
empirical support from studies of method-of-moments estimates. Table 
4.2-1 presents the method-of-moments estimates for 22 function words. 
A parametric family of prior distributions is assumed with five hyper-
parameters that we call underlying constants. The 21 sets of underlying 
constants used in sensitivity studies are listed in Table 4.2-2. 
4.2D. The posterior distribution 
103 
For any choice of underlying constants, the posterior distributions are 
independent across words. For each word, the posterior is a four-
dimensional density known up to its normalizing constant. The posterior 
mode and the Hessian matrix of second derivatives of the logarithmic 
density are determined by a Newton-Raphson iterative algorithm. 
4.2E. The modal estimates 
106 
The posterior modal estimates are the main output of the empirical 
Bayesian analysis of the papers of known authorship and the main 
input for assessing the evidence of authorship on any unknown paper. 
The Hessian matrices are important for exploring the adequacy of 
approximations. The modal estimates for the 30 final words and one 
choice of prior were set out in Table 4.2-3. 

ANALYTIC TABLE OF CONTENTS 
xxi 
4.2F. An alternative choice of modes 
106 
Modes of asymmetric densities are not ideal for approximating posterior 
expectations. Some inexpensive improvements come from using modes 
of densities relative to a measure element other than Lebesgue measure. 
For the gamma- and beta-like prior densities used here, these relative 
modes are equivalent to a change in the underlying constants. 
4.2G. Choice of initial estimate 
108 
Iterative procedures require starting values; method-of-moment esti-
ma tes are natural candidates but are inadequate for low -frequency words 
where the shrinking effect of the prior density is strong. An approximate 
data equivalent of the prior leads to weighted initial estimates of good 
quality. Combining tight-tailed priors with long -tailed data distributions 
gives rise to special needs that must be faced in the absence of sufficiency 
or conjugacy. 
4.3. Abstract structure of the main study . 
111 
We describe an abstract structure for our problem; we derive the appro-
priate formulas for our application of Bayes' theorem and give a formal 
basis for the method of bracketing the prior distribution. The treatment 
is abstracted both from the notation of words and their distributions and 
from numerical evaluations. 
4.3A. Notation and assumptions . 
111 
Four initial assumptions model the probabilistic relations among the 
observables (the data on the disputed papers and the data on the 
known-author papers) and the non-observables (the parameters of the 
data distributions and the authorship of the disputed papers). The 
authorship is the goal of the analysis of The Federalist. The basic appli-
cation of Bayes' theorem represents the final odds of authorship as the 
product of the initial odds of authorship and an odds factor that involves 
the data on the known papers. 
4.3B. Stages of analysis 
112 
The factorization in Section 4.3A divides the analysis into three stages: 
choosing data distributions and estimating their parameters, evaluating 
the odds factors for the disputed papers, and combining the odds factors 
with initial odds of authorship. The first two are heavily statistical. 
4.3C. Derivation of the odds formula 
112 
The fundamental factorization result of Section 4.3A is derived from 
four assumptions. 
4.3D. Historical information . 
113 
Historical evidence bears on authorship and can be treated as logically 
prior to the analysis of the linguistic data. A fifth assumption sets out 
what is needed for the statistical evidence that determines our odds 
factors to be independent of and acceptable to historians, regardless of 
how they assess the historical evidence. This subjective element is 
isolated to the assessment of the initial odds. 
4.3E. Odds for single papers. 
114 
Odds factors for authorship of a single paper are interesting and 
important. 

xxii 
ANALYTIC TABLE OF CONTENTS 
4.3F. Prior distributions for many nuisance parameters 
114 
Our data consist of word frequencies for more than a hundred words. 
Modeling each as distributed independently as a negative binomial 
leads to four parameters per word. Estimating hundreds of parameters 
with the available data cannot be done safely using a flat prior, or with 
any non-Bayesian equivalent such as maximum likelihood. Here, we 
consider the abstract notion of modeling the behavior of the word-
frequency parameters as sampled from a hyperpopulation. The hyper-
population is modeled as a parametric family of low dimension with 
parameters we call underlying constants but for which hyper parameter 
has come into common use by 1984. In lieu of an infeasible full Bayesian 
analysis, we propose to carry out the main analysis conditional on 
assumed known values of the hyperparameters. The hyperparameters 
are estimated in a separate analysis and the sensitivity of the main 
results to the assumed hyperparameters is explored. The method is 
empirical, and the Bayesian logic is examined. Some similarities and 
some distinctions from Robbins' "empirical Bayes procedures" are 
noted. 
4.3G. Summary 
117 
4.4 Odds factors for the negative binomial model 
117 
We develop properties of the Poisson and negative binomial families of 
distributions. The discussion of appropriate shapes for the likelihood 
ratio function may suggest new ways to choose the form of distributions. 
4.4A. Odds factors for an unknown paper . 
117 
The odds factor for an unknown paper is the product, over words, of 
a ratio of expectations of two negative binomial probabilities, the 
numerator expectation with respect to the posterior distribution of the 
Hamilton parameters, the denominator with respect to the posterior 
distribution of the Madison parameters for the word. 
4.4B. Integration difficulties in evaluation of A • 
119 
For any word, the posterior distribution for the four parameters is 
determined up to a normalizing constant. To get the marginal distri-
butions of the two Hamilton or of the two Madison parameters would 
require quadrature or other approximation. The calculations of the 
exact odds factor A for any word and unknown paper then is a ratio of 
two four-dimensional integrals, a formidable calculation that we bypass 
by the modal approximation. 
4.4C. Behavior of likelihood ratios. . 
120 
With known parameters and a single word, the odds factor is a simple 
likelihood ratio depending on the frequency of the word in the unknown 
paper. Likelihood ratios whose logarithms are monotone or even linear 
are popular in statistical theory, and arise for Poisson and other expon-
ential family models. For representing intuitive assessment of evidence, 
shapes that redescend toward zero for very high (and suspect) fre-
quencies are appealing. The behavior for the negative binomial is 
examined. It is not linear, but is unbounded, and to prevent any word 

ANALYTIC TABLE OF OONTENTS 
xxiii 
contributing excessively, truncation rules were set up to prevent any 
word from contributihg more strongly than the extreme observed in 
the 98 papers of known authorship. 
4.4D. Summary 
124 
Further work is needed to develop asymptotic expansions and well-
designed quadrature and Monte Carlo methods to evaluate the integrals 
that arise in Bayesian analyses. Also needed is a greater range of appro-
priate shapes for log likelihood ratios. 
4.5. Choosing the prior distributions 
124 
We give methods for choosing sets of underlying constants to bracket 
the prior distributions and we explore the effects of varying the prior 
on the log odds. Choices are based in part on empirical analysis bu t also On 
heuristic considerations of reasonableness, analogy, and tractability. 
4.5A. Estimation of PI and P2: first analysis 
125 
The first two hyperparameters fil and fi2 measure the spread of the 
prior distribution of the differential word rate. A variance components 
analysis ofthe observed mean word rates in 90 function words stratified 
according to total frequency of use leads to estimates of fil and fi2 for the 
pool offunction words. The stratification can be collapsed to give a better 
estimate of fil' We apply the jackknife procedure with eight random 
subgroups to produce a standard error for the estimated fil' 
4.5B. Estimation of PI and P2: second analysis 
128 
If posterior modal estimates of the word rate parameters were used as if 
exact to estimate hyperparameters, those measuring variation would be 
too small because ofthe shrinking effect ofthe Bayesian estimation. For 
an analogous binomial problem, the extent of underestimation is deter-
mined and used as an informal guide to the actual situation. 
4.5C. Estimation of P3 
130 
The hyperparameter f33' measuring the spread of differential non-
Poissonness is assessed informally from the frequency distribution of 
method-of-moments estimates and from the posterior modal estimates 
of differential non-Poissonness. These tend respectively to show too 
much and too little variation and bracket fi3' A weakness from an 
assumed symmetry is considered. 
4.5D. Estimation of P4 and P5 
131 
These two hyperparameters that measure the mean and variance of the 
composite non-Poissonness are assessed by informal analyses. 
4.5E. Effect of varying the set of underlying constants 
132 
The sensitivity of the final log odds factors to the choice of underlying 
constants or hyperparameters is examined by selective comparisons 
among the 21 sets chosen to bracket the priors. An appropriate response 
measure is a proportional change to the log odds, and a 12 per cent 
change up or down from the primary choice is judged an adequate 
allowance. The main effect of changing each hyperparameter is exam-
ined as are some interactions. 

xxiv 
ANALYTIC TABLE OF CONTENTS 
4.SF. Upon: a case study. 
135 
The effect of choice of prior on the estimated rates and non-Poissonness 
parameters for the highly discriminating word upon illustrates some 
possible strange effects of tail behavior of a prior interacting with a four-
dimensional likelihood surface. Use of priors conjugate to the likelihood 
hold few surprises, even when the priors and likelihood are quite in-
consistent, because prior and likelihood effectively represent equivalent 
and exchangeable data. Our gamma- and beta-like priors have tight 
tails, and in extreme situations can dominate the broader tails of the 
negative binomial likelihoods, and strongly shift the parameters from 
the observed rates. This behavior stands in contrast to analyses with 
flat priors and tight-tailed data distributions. 
4.SG. Summary 
138 
Sensitivity to choice of priors is modest relative to other source of 
variation. The study of Section 4.5F suggests a point likely important 
throughout Bayesian inference: the effect of small tails of the prior is 
very different from the effect of small tails of the data distribution. 
4.6. Magnitudes of adjustments required by the modal approximation to the 
odds factor . 
138 
We study, by example, the effect of using the posterior mode as if it were 
exact. To make the assessment we develop some general asymptotic 
theory of posterior densities. 
4.6A. Ways of studying the approximation . 
138 
The odds factor is a ratio of two expectations, usually with respect to a 
concentrated posterior distribution. An expectation can be approximated 
by the integrand evaluated at the mean or, to a higher order, by the 
next delta-method adjustment using covariances. We have only the 
posterior modes and the Hessian matrix of second derivatives at 
the mode, and want to use that information to assess the modal 
approximation. 
4.6B. Normal theory for adjusting the negative binomial modal approxi-
Dlation 
140 
We further transform the four parameters for each word to a form in which 
a normal posterior is a plausible approximation. We use the mode and 
inverse of the Hessian matrix in the new parametrization as if they were 
the exact mean and variance matrix. We apply the first two terms of the 
delta method approximations to the required expectations, and study the 
changes in log odds for five words, including the three strongest discri-
minators and two of the strongest rare words. The modal approximation 
gives log odds that are too large (in magnitude), and a 15 per cent re-
duction in total log odds is a rough bound for the effect. 
4.6C. ApproxiDlations to expectations 
146 
The delta method is based on means and covariances. The Laplace 
integral expansion for a posterior density gives the equivalent approxi-
mation in terms of the posterior mode, and second and third derivatives 
at the mode. Using modes relative to specially chosen density elements 

ANALYTIC TABLE OF CONTENTS 
XXV 
can reduce the role of the third derivatives. Normal, beta, and gamma 
distributions motivate choices of density elements. Multivariate 
extensions are set forth. 
4.6D. Notes on asymptotic methods . 
152 
The asymptotic basis of the approximations developed and used in the 
preceding sections is explored for a general posterior density and related 
to the Laplace integral expansion. The sampling distribution of the 
posterior density makes plausible the appropriate asymptotic form, but 
in any application the shape and behavior of the actual density should 
be examined. 
4.7. Correlations . 
155 
We study the magnitudes of effects of erroneous assumptions: the 
effects of correlations between rates for different words. 
4.7 A. Independence and odds . 
155 
Odds calculated assuming independence of word usage are likely to be 
too high. We seek to assess how much the log odds based on independence 
would differ from a log odds based on a model incorporating dependence. 
4.7B. Adjustment for a pair of words 
155 
If dependence is modeled by bivariate normality, the needed additive 
adjustment follows from standard discrimination theory. The adjust-
ment has a normal distribution with mean and standard deviation that 
allow qualitative and quantitative assessment of effects. 
4.7C. Example. The words upon and on 
157 
The pairwise adjustment model is applied to the two strongest discrimi-
nators, upon and on, whose use is somewhat complementary. Word rates 
are transformed to a Freeman-Tukey square root scale to stabilize 
variance and to improve the normality assumption. The observed 
correlation is mildly negative, and the adjustment reduces the strong 
discrimination. 
4.7D. Study of 15 word pairs . 
159 
The pairwise model is applied to 15 pairs among the 11 words of highest 
frequency for which the observed correlations exceeds .15, and the 
mean and standard deviation of the adjustment is shown in Table 
4.7-2. The expected adjustments would increase discrimination in 
nearly half of the pairs but the total of the adjustments would reduce 
the composite discrimination modestly. 
4.7E. Several words 
159 
The adjustment of Section 4.7B extends directly under multivariate 
normality. Table 4.7-3 gives the correlation matrix for 11 words. The 
mean and standard deviation of the needed adjustment are obtained 
assuming multivariate normality. The observed correlation matrix is 
used directly, and also after two shrinkage adjustments that allow for 
the sampling errors in the estimated correlations. The composite results 
are comparable to the pairwise analysis. 

xxvi 
ANALYTIC TABLE OF CONTENTS 
4. 7F. Further theory 
162 
The matrix inversion needed for the Section 4.7E analysis was ac-
complished by power expansion. Although the computational methods 
are superseded by readily available facilities for matrix operations, the 
expansion allows examination of the relation of the full multivariate 
and the pairwise methods. 
4.7G. Summary. 
163 
The adjustments for correlations modestly reduce the discrimination 
indicated from independence. Open questions on methodology are 
raised. 
4.8. Studies of regression effects 
163 
To study the adequacy of assumptions, we compare the performance 
of the log odds for the disputed papers with theoretical expectations. 
4.8A. Introduction. 
163 
If all assumptions were correct, and the posterior modal estimates of 
parameters were exact, we could compute the mean and standard devi-
ation of the log odds that would be obtained for a fresh paper by 
Hamilton or for one by Madison. Table 4.8-l displays these expected 
log odds for the 30 final words for a 2000-word paper. In total, the 
expected log odds under the negative binomial model is about 14 for a 
Hamilton paper, about -14 for a Madison paper. 
4.8B. The study of word r~tes 
165 
For each of the 11 words of highest frequency, we show in Table 4.8-3 
the frequency distributions of observed rates in the 48 Hamilton papers, 
the 50 Madison papers, and the 12 disputed papers. In Table 4.8-2, we 
show the mean rates in the three groups of papers, as well as their 
expectations under the assumed Poisson and negative binomial models. 
Only the disputed papers are independent of the fitted models, and for 
them, the standard deviation of the rates are also shown, and the lack 
of regression effects explored. 
4.8C. Total log odds for the final 30 words . 
172 
The log odds calculation is applied to each of the known and disputed 
papers, and an adjustment is made to estimate what the log odds would 
have been had each paper been 2000 words in length. The mean log odds 
for the 48 Hamilton papers is compared with the expected log odds fOl' 
a fresh Hamilton paper as discussed in Section 4.8A. Table 4.8-5 shows 
the great variability in the observed log odds. 
4.8D. Log odds by word group 
175 
The comparison of observed and expected log odds is made for each of 
six groups of words, and applied to the known papers used in the esti-
mation and modeling, to the disputed papers, and to four late Hamilton 
Federalist papers not used in any preceding analysis. In three of the five 
groups there is regression for the known papers to the fresh papers. 
There is little regression from the expected log odds, for which the prior 
distributions has made allowance for the selection effects. 

ANALYTIC TABLE OF CONTENTS 
xxvii 
4.8E. Theory for the Poisson model . 
177 
Under a Poisson model, log odds are linear in the observed rate and 
expectations are easily calculated. As preparation for the harder 
negative binomial model, we note that the expectation can be obtained 
exactly as a weighted sum of the log odds for any two observed rates. 
The expected log odds is proportional to paper length and a propor-
tional adjustment to a standard paper length is appropriate. 
4.8F. Theory for the negative binomial model . 
178 
Calculating the expected log odds for the negative binomial is not easy, 
and we use a two-point weighted sum like that for the Poisson. We pro-
pose and use an ad hoc adjustment to adjust a log odds to what would 
have been obtained for a standard length paper. 
4.8G. Two-point formulas for expectations of negative binomial log odds 
179 
The basis for the two-point approximations are explored for the negative 
binomial. The specific uses made would be obviated by modern com-
putational capabilities. 
4.9. A logarithmic penalty study 
180 
We explore a way of assessing the validity of methods for making strong 
probabilistic predictions when only limited test materials are available. 
We set up a scoring scheme that penalizes probabilistic predictions 
(quantitative statements about the probability of a future event) on 
the basis of outcomes of the events. The approach is not closely tied to 
authorship and is generally applicable to the evaluation of proba-
bilistic forecasts. 
4.9A. Probability predictions . 
181 
A method for making probability predictions is a rule for taking data 
on a trial, for example, word rates for one unknown paper, and producing 
a probability distribution over a discrete range of outcomes, for example, 
authorship. Methods may be Bayesian or not, but we restrict attention 
to methods that predict separately for each unknown and allow no feed-
back from results of preceding trials. 
4.9B. The Federalist application: procedure 
181 
The study is carried out separately for predictions based on each of three 
words: by, on, to. As test trials, we use the papers of known authorship, 
specifically, 48 Hamilton and 48 Madison papers. The methods of 
prediction to be studied allow choice of Poisson or negative binomial 
models, choice of the underlying constants, choice of initial odds, and 
choice of an adjustment factor. The latter two are mostly kept at neutral 
values of 1. The role of reusing parameters estimated from all the data 
including the test case is discussed. 
4.9C. The Federalist application: the penalty function 
183 
For any test paper and method, penalize the probability forecast by 
an amount equal to the negative logarithm of the probability predicted 
for the actual author. Better prediction method:;; should get smaller 
total penalties. The penalty score is like a log likelihood function over 
the space of "methods". As one calibration of the penality score, we table 

xxviii 
ANALYTIC TABLE OF CONTENTS 
how a prediction that was right with constant probability would fare. 
A (lorrect method should score as well as its data allow, and also should 
predict how incorrect methods would score. We compare the observed 
score for one method against tha expected score assuming the correct-
ness of a second method, both in units of penality score and in standard 
deviation units. 
4.9D. The Federalist application: numerical results 
185 
For predictions based on rates of high-frequency words, Tables 4.9-2 
and 4.9-4 show the observed scores for several Poisson and negative 
binomial methods and also the expected scores if the respective models 
were correct. For most words, negative binomial methods get lower 
(better) scores than do Poisson methods. The negative binomial observed 
scores are close to what would be expected if their method were correct. 
The scores for Poisson methods are much worse than what would be ex-
pected were their methods correct, showing again that Poisson log odds 
are severely inflated. More detailed study of by develops further support 
for the correctness of the negative binomial predictions and some sup-
port for our choices of the underlying constants. The effect of initial 
odds is examined briefly. 
We comment on the minimization of the penalty score as a criterion 
for estimation of parameters, and some of its disturbing properties. 
That method has come into wide use as a conditional maximum likeli-
hood fitting of logistic models (linear for Poisson, but not for nega-
tive binomial). Computational capabilities have made the method 
possible, but the inability of the method usefully to handle very strong 
discrimination remains a severe limitation. 
4.9E. The Federalist application: adjusted log odds 
189 
Would a further multiplicative adjustment factor on the predicted log 
odds improve performance beyond the Bayesian modeling or whatever 
was built into the predictive method 1 For each of three words studied, 
the factor that minimizes the observed penalty when the predictions are 
applied to the known papers is very close to 1 for a negative binomial 
method, and close to .5 for a Poisson method. The discounted Poisson 
odds are very close to the negative binomial odds, but that does not 
hold for rare words like whilst. 
4.9F. The choice of penalty function 
190 
A proper scoring rule encourages predictions with the correct prob-
abilities when these are known. We show that for predictions of three or 
more possible outcomes, the logarithmic penalty is the only proper 
scoring penalty that depends only on the probability predicted for the 
outcome that obtains. For two outcomes, the logarithmic choice is not 
unique, but is related to Shannon information, and to likelihood inter-
pretations. Our approach to scoring rules was influenced by work ofL. J. 
Savage that has been published as "Elicitation of Personal Probabilities 
and Expectations" (J. Amer. Statist. A88oc. 66 (1971), 783-801). 

ANALYTIC TABLE OF CONTENTS 
xxix 
4.9G. An approximate likelihood interpretation 
192 
We develop an approximate representation ofthe total penalty score as 
a conditional likelihood for assessing models and methods, conditional 
on the observed data on the papers. 
4.10. Techniques in the final choice of words 
195 
This section provides details of a special difficulty, and its possible 
general value lies in illustrating how to investigate the effects of a split 
into two populations of what was thought to be a single population. 
4.10A. Systematic variation in Madison's writing . 
195 
A ~ajor part of the known Madison papers comes from a long essay 
Neutral Trade that differs in time and form from his other writings and 
from the disputed papers. To avoid distortion of the discrimination on 
the disputed papers, we want to eliminate words for which Madison's 
patterns of usage change importantly between the two sources of 
writings. Table 4.10-1 shows for 27 semi-final words the mean log odds 
for Hamilton versus Madison, and the expected log odds for discrimi-
nating between "composite" Madison and "early" or "Federalist" 
Madison. 
4.10B. Theory . 
198 
To discriminate between Early and Late Madison writings is a problem 
equivalent to the main discrimination problem. To get the numbers 
needed without recourse to major computation, we used a variety of 
approximations to simplify the problem: linearization of the negative 
binomial log likelihood ratio, simplified parameter estimates, and 
Bayes' adjustments. The results are used only to identify major 
offending words and great precision is not needed. 
Chapter 5. Weight-Rate Analysis 
200 
5.1. The study, its strengths and weaknesses 
200 
Using a screening set of papers, we choose words and weights to use in 
a linear discriminant function for distinguishing authors. We use a 
calibrating set to allow for selection and regression effects. A stronger 
study would use the covariance structure of the rates for different 
words in choosing the weights; we merely allow for it through the 
calibrating set. The zero-rate words also weaken the st\ldy because we 
have not allowed for length of paper as we have done in the main study 
and in a robust one reported later. 
5.2. Materials and techniques . 
201 
Using the pool of words described in Chapter 2, we develop a linear 
discriminant function fi = 
~ WiXi, where Wi is the weight assigned to 
the ith word and Xi is the rate for that word. The Wi are chosen so that 
fi tends to be high if Hamilton is the author, low if Madison is. Ideally 
the weights are proportional to the difference between the authors' 
rates and inversely proportional to the sum of the variances. By a 

XXX 
ANALYTIC TABLE OF CONTENTS 
simplified and robust calculation, an index of importance of a word was 
created. We use it to cut the number of words used to 20. 
5.3. Results for the screeuing and calibrating sets 
203 
The 20 words, their weights, and estimated importances are displayed in 
Table 5.3-1, upon being outstanding by a factor of 4. Table 5.3-2 shows 
the results of applying the weights to the screening set of papers. 
Hamilton's 23 average .87 and all exceed .40, while Madison's 25 average 
-.41 and all are below -.19. For the calibrating set Hamilton averages 
.92 and Madison -.38. The smallest Hamilton score is .31, and the 
largest Madison is .15 (zero plays no special role here). 
5.4. Regression effects . 
208 
As a rough measure of separation, we use the number of standard 
deviations between the Hamilton and Madison means. For the whole set 
of 20 words, the separation regresses from 6.9 standard deviations in the 
screening set to 4.5 in the calibrating set. In Section 5.3, we see almost 
no change from screening to calibration set in the average separations; 
the loss comes from increased standard deviations. In a general way, as 
the groups of words become more contextual the regression effect is 
larger. Group 1, the word upon, actually gains strength from screening 
to calibration set. 
5.5. Results for the disputed papers 
210 
After displaying the numerical outcome of the weight-rate discriminant 
function for the disputed papers in Table 5.5-1, we carry out two types 
of analyses, one based on significance tests and one based on likeli-
hood ratios. In Table 5.5-2 we show two t-statistics and corresponding 
P-values for each paper, first for testing that the paper is a Hamilton 
paper, and second for testing that the paper is a Madison paper. We 
compute 
y-fh 
tj = -s j-..;-'ft1:=+=(C=1 j=:=n=j) , 
where j = Hamilton or Madison, y is the value for the disputed paper 
from Table 5.5-1, Sj is the standard deviation for author j for the cali-
brating set, and nj = 25, the number of papers in each calibrating set. 
Except for paper 55, the P-values for the Hamilton hypotheses are all 
very small (less than .004); the P-values for the Madison hypotheses are 
large, the smallest being .087. Paper 55 is further from Madison than 
from Hamilton but both P-values are significant. 
Table 5.5-3 gives log likelihood ratios for the joint and disputed 
papers, assuming normal distributions and using the means and 
variances in the calibrating set. To allow for the uncertainty in esti-
mating the means and variances, conservative 90 per cent confidence 
limits are shown for the log likelihood ratio, and a Bayesian log odds is 
calculated using the t-distribution. Except for paper 55, which goes 
slightly in Hamilton's favor, the odds favor Madison for the disputed 
papers. 

ANALYTIC TABLE OF CONTENTS 
xxxi 
Chapter 6. A Robust Hand-Calculated Bayesian Analysis. 
215 
6.1. Why a robust study? 
215 
Because the main study leans on parametric assumptions and heavy 
calculations, we want a study to check ourselves that depends less on 
distributional assumptions and that has calculations that a human 
being can check. This robust approach, based on Bayes' theorem, 
naturally sacrifices information. It dichotomizes the observed frequency 
distributions of occurrences of words. For choosing and weighting words, 
it llses both a screening set and a validating set of papers. 
6.2. Papers and words . 
216 
Using a screening set of 46 papers of length about 2000 words, we 
selected the words shown in Table 6.2-2 for the robust Bayes study. 
6.3. Log odds for high-frequency words 
217 
For each of the 64 high-frequency words, we divide the rates of the 
46 papers in the 2000-word set into two equal parts, highs and lows. For 
each word, we form a 2 X 2 table for the high and for the low rates. To 
estimate the odds (Hamilton to Madison) to be assigned to a word, we 
first add 1.25 to the count in each of the four cells of the word's 2 x2 
table. We explain the theoretical framework for this adjustment which 
is based on a beta prior distribution. We use the adjusted counts to 
estirr lte the odds for the high and for the low rate for that word. 
6.4. Low-frequency words . 
220 
For low-frequency -Words, we use the probability of zero occurrence and 
must adjust the Hamilton-Madison odds according to the length of 
paper. 
6.5. The procedure for low-frequency words 
220 
Following the theory of Section 6.6, this section explains the arithmetic 
leading to log odds for each word appropriate to the length of the paper. 
Ultimately we sum the log odds. 
6.6. Bayesian discussion for low-frequency words 
222 
Theoretical development required for the procedure given in Section 6.5. 
6.7. Log odds for 2000-word set and validating set • 
225 
For each of the five groups of words in Table 6.2-2 and in total, Table 
6.7-1 shows the log odds for each paper in the 2000.word set used to 
choose the words and create the odds. All Hamilton papers have 
positive log odds (averaging 14.0) and all Madison papers have negative 
log odds (averaging -14.2). Table 6.7-2 gives a more relevant assess-
ment: the same information for the validating set of papers not used to 
develop the odds. The corresponding averages are 10.2 for 13 Hamilton 
papers and -8.2 for 18 Madison papers. One Hamilton paper has log 
odds of 0 or equivalently even odds of 1 : 1. 

xxxii 
ANALYTIC TABLE OF CONTENTS 
6.8. Disputed papers 
228 
Table 6.8-1 gives the detailed data parallel to the previous tables for 
the unknown papers. Only paper 55 is not ascribed to Madison. The 
strength of attribution is, of course, much weaker than in the main 
study. 
Chapter 7. Three-Category Analysis. 
229 
7.1. The general plan 
229 
By categorizing rates into three categories-low, middle, and high-
and estimating log odds for each category, we can get a score for each 
unknown paper. This study defends against outlying results and 
failures of assumptions though it does a crude job of handling zero 
frequencies. 
7.2. Details of method 
229 
For a given word, the rates in 48 papers (23 Hamilton and 25 Madison) 
were ranked with the lowest 18 papers giving the cutoff for "low" 
and the highest 18 papers the cutoff for "high". Table 7 .2-2 gives the 
cut-points so determined and the log odds for 63 words. To get a score 
for a paper, sum the log odds. Some special rules killed some words and 
pooled categories in others. 
7 .3. Groups of words 
234 
After applying the rules of Section 7.2, we had 63 words left, grouped as 
before by perceived degrees of contextuality. 
7.4. Results for the screening and calibrating sets 
235 
The scoring system was applied to the screening set of papers. As shown 
in Table 7.4-1, all Hamilton papers scored positive averaging 20.54, all 
Madison negative averaging -31.24. To see the regression effect, the 
same scheme was applied to a calibrating set as shown in Table 7.4-2 
with average log odds for Hamilton of 8.54 and for Madison of 
-19.30. 
7.5. Regression effects 
239 
7 .SA. Word group 
239 
For each word group we show in Table 7.5-1 the regression effect from 
screening to calibrating set. Generally speaking, the more the group is 
perceived as contextual, the greater its regression effect. The word 
upon improved from screening to calibrating set. 
7.SB. The regression effect by single words 
241 
Table 7.5-2 gives the numerical results. 
7.6. Results for the joint and disputed papers 
241 
As in the analysis of Chapter 6, all disputed papers but paper 55 lean 
strongly toward Madison, and that paper falls on the fence. 

ANALYTIC TABLE OF CONTENTS 
XXXlll 
Chapter 8. Other Studies . 
243 
8.1. How word rates vary from one text to another 
243 
For 165 words we give rates in Table 8.1-1 from six sources: Hamilton, 
Madison, Jay, Miller-Newman-Friedman, Joyce's Ulysses, and the Bible. 
8.2. Making simplified studies of authorship . 
249 
To begin an authorship study we advise: Edit for quotations and special 
usage; make counts for separate pieces, using a list of words of moderate 
length; obtain the rates; assess variation and discard words; get 
statistical help if the problem is delicate; use natural groupings; use a 
high-speed computer; see Chapter 10 for some new variables. 
8.3. The Caesar letters . 
251 
As a little example, we explore the possibility that Hamilton, as opposed 
to someone else,wrote the Caesar letters. Table 8.3-1 shows the rates for 
23 high-frequency words for the Caesar letters, and for Hamilton, for 
Madison, and for Jay. For 13 of the words, the Caesar rate differs from 
the Hamilton rate by two or more standard deviations under Poisson 
theory. If we apply the log odds computation of Chapter 3 for 
Hamilton versus Madison to the Caesar letters, we get -4.2, instead of 
positive log odds in the teens or twenties as we would expect if Hamilton 
were the author. The results are strongly against Hamilton, though not 
in favor of Madison, but of some unknown author. 
8.4. Further analysis of Paper No. 20 
252 
Among the three papers we classified as having joint Hamilton-Madison 
authorship, paper No. 20 is most nearly on the fence. We hunted for 
Hamilton's contribution. Some Hamilton markers could be traced not to 
him but to the writing of Sir William Temple, from which Madison drew 
extensively for this paper. We abandoned the analysis. 
8.5. How words are used 
253 
Joanna F. Handlin made an elaborate study of the various dictionary 
meanings of 22 marker words and probably. In 15 appearances of upon, 
Madison had 3 usages that Hamilton never used in 216 appearances. 
Table 8.5-1 gives detailed data for the occurrences of 13 meanings of of 
in several papers for each author-a study carried out by Miriam 
Gallaher. 
8.6. Scattered investigations 
256 
We hunted for useful pairs of words like toward-towards with little 
success. Use of comparatives and superlatives showed great variation. 
Words with emotional tone gave no discrimination. How Hamilton and 
Madison handled enumerations led nowhere. A study of conditional 
clauses failed because of unreliability in classification. 
Relating 
strength of discrimination to proportion of origina,l material, although 
suggestive, was not useful. Length of papers offered some discrimination, 
but we feared it because of contextuality and because of newspaper 
constraints. 

xxxiv 
ANALYTIC TABLE OF CONTENTS 
8.7. Distributions of word-Ieugth . 
259 
The earliest discrimination analyses by Mendenhall used word length as 
a discriminator. Robert M. Kleyle and Marie Yeager display the distri-
bution of word length for eight Hamilton and seven Madison papers in 
Table 8.7-1 and in three figuies. The chi-squared statistic for goodness 
of fit in Table 8. 7-~ shows so much variation that we cannot use it for 
discrimination. The Hamilton papers fit the Madison averages as well 
as do the Madison papers. 
Chapter 9. Summary of Results and Conclusions 
263 
9.1. Results ou the authorship of the disputed Federalist papers 
263 
Except for paper No. 55, the odds are strong for Madison in the main 
study. For No. 55 they are about 90 to 1 for Madison. 
The choice of data distribution mattered a great deal, the Poisson log 
odds were about twice those of the negative binomial, but several studies 
discredit the Poisson results while supporting the negative binomial. 
Variations in prior distributions mattered less than other sources of 
variation. 
9.2. Authorship problems 
265 
Function words offer a fertile source of discriminators. Oontextuality 
must be investigated. See also Ohapter 10 for further variables. 
9.3. Discrimination problems 
265 
A large pool of variables systematically explored may payoff when 
obvious important variables are not available. Oontextual effects have 
counterparts in other situations. Selection effects must be allowed for. 
9.4. Remarks ou Bayesian studies . 
266 
We recommend sensitivity studies made by varying the priors. We like 
priors that have an empirical orientation. Data distributions matter. 
We need simple routine Bayesian methods. 
9.5. Summing up 
267 
We tracked the problems of Bayesian analysis to their lair and solved 
the problem of the disputed Federalist papers. 
Chapter 10. The State of Statistical Authorship Studies in 1984 
268 
10.1. Scope . 
268 
We treat the time period since 1969, emphasizing prose disputes almost 
exclusively. This chapter discusses both technological advances and 
empirical studies. 
10.2. Computers, concordances, texts, and monographs . 
268 
The computer and its software leading to easy compilation of con-
cordances have been the major technological advance. Scholars have 
produced several monographs but few statistical texts in stylistics. 

ANALYTIC TABLE OF CONTENTS 
xxxv 
10.3. General empirical work 
269 
Morton studies sentence length further, and like Ellegard, uses pro-
portional pairs of words (the fraction that the occurrences of word U 
makeup the total occurrences of word U and word V). Mortonintroduces 
collocation variables to expand the number of potential discriminators. 
(A collocation consists of a keyword like in and has associated words 
that precede or succeed it). The ratio of the number oftimes the asso-
ciate word occurs with the keyword to the number of times the keyword 
appears is the measure of collocation. Position of a word in a sentence 
(especially first or last) offers additional discriminators. 
10.4. Poetry versus prose 
270 
To examine a possible systematic difference between poetry and prose, 
Williams looks at the Shakespeare-Bacon controversy. Hetakessamples 
of Shakespeare (who wrote only poetry), Bacon (who wrote only 
prose), and as a control samples of both poetry and prose from Sir Philip 
Sidney. Williams uses words oflength 3 and 4 as discriminators. Table 
10.4-1 shows the comparisons. He concludes that poetry and prose pro-
duce differing distributions of word lengths, and that the difference 
between Shakespeare and Bacon could be regarded as a poetry -to-prose 
effect rather than an authorship effect. 
10.5. Authorship studies similar to the Junius or Federalist studies 
271 
10.5A. And Quiet Flows the Don. 
272 
We review the dispute about the authorship of the Russian novel And 
Quiet Flows the Don. An anonymous critic, D*, in a book with preface 
by Solzhenitsyn, regards Mikhail Sholokhov, the reputed author, as 
having plagiarized much of the work of the anti-Bolshevik author 
Fyodor Kryukov, who died before publishing his work on the Don 
Cossacks. Roy A. Medvedev reviews the issues, concluding that 
Sholokhov probably had access to some Don Cossack writings. 
The comparisons Kjetsaa gives for The Quiet Don, for Sholokhov's 
other writings, and for Kryukov's support Sholokhov more than 
Kryukov. 
10.5B. Kesari . 
273 
In discriminating between two possible authors of certain editorials 
published in the Indian newspaper Kesari, Gore, Gokhale, and Joshi use 
the variables word length, sentence length, and the rate of use of com-
mas as discriminators. They reject the hypothesis that word length 
follows the log normal distribution. They find sentence length to be ap-
proximately log normal, but unfortunately unstable for material from 
the same author, and so not helpful. Their new variable, rate of use of 
commas, offers· some discrimination. 
10.5e. Die Nachtwachen 
The author of this pseudonymous romantic German novel has been 
hotly sought since its publication in 1804. Wickmann uses transition 
frequencies from one part of speech to another as discriminators and 
274 

xxxvi 
ANALYTIC TABLE OF CONTENTS 
concludes that among several candidates only Hoffmann is a reasonable 
possibility. 
10.5D. Economic history 
274 
O'Brien and Darnell tackle six authorship puzzles from the field of 
economics. They use the collocation method and the first words of 
sentences to decide authorship in a book-length sequence of studies. 
10.6. Homogeneity problems 
275 
In the simplest homogeneity problem, we have two pieces of text and 
we ask whether they were produced by the same author. 
10.6A. Aristotle and Ethics 
276 
Kenny analyzes two versions of a book on ethics reputed to be by 
Aristotle, using their common material as a standard to decide which 
version was more similar to the common material. He uses many special 
studies in his book and concludes that the version which at one time was 
regarded by scholars as not the more mature version is closer statistically 
to the common material. 
10.6B. The Bible . 
The studies of The Bible, both Old and New Testament, have become 
so extensive that they cannot readily be discussed here. We indicate 
studies that try to settle whether each of Isaiah, Zechariah, and Genesis 
was written entirely by a single author. The latter two studies have 
led to controversies, and we cite some papers that deal instructively 
with the issues. Students of authorship studies will find them helpful. 
276 
10.7. Anonymous translation 
277 
Michael and Jill Farringdon deal with the most unusual authorship 
problem we found in our literature search. Did Henry Fielding, the 
English novelist, translate the military history of Charles XII from 
French into English? By using as discriminators the rates of words that 
critics had used to parody Fielding and by looking at pooled rates of a 
variety of other authors, they conclude that Fielding did the translation. 
They use especially word pairs like whilst versus while. We think this 
problem deserves further study because of its challenge. 
10.8. Forensic disputes 
278 
10.8A. Morton . 
278 
Morton writes about his troubling experiences in giving authorship 
testimony in court. 
10.8B. Bailey 
278 
Bailey gives three requirements before a legal authorship dispute can be 
decided. His attempts to introduce authorship stylometrics into the 
Patricia Hearst case were denied by the judge. 
IO.8e. Howland will 
279 
Although this dispute concerns the authorship of the signature of a will, 
it brings out many of the issues that arise in other authorship problems. 

ANALYTIC TABLE OF CONTENTS 
XXXVll 
The main actors are famous: mathematician Benjamin Peirce, his 
scientist son Charles Sanders Peirce, and the woman who latter became 
a multimillionaire when a million was real money, Hetty Green. Meier 
and Zabell's treatment is most instructive. 
10.9. Concluding remarks 
280 
Although some new variables for use as discriminators have been intro-
duced, the level of statistical analysis in authorship studies has not 
generally advanced. We suggest that some additional empirical studies 
might help in future English authorship disputes. We especially need 
more data on variability within and between authors. 
Appendix 
283 
References 
285 
Index 
291 

~ 
--,~, --......... ~....-. .------
MISC8LLANr. 
... \kl. 'IriIt c!tftft~ ,lit people 111 A_cah.r.,.. 
of Ihe daa,en iocident to lelrtr republia. ",ill ex· 
JIOfe them 10 Ihe iDq,nYCllial!!1 0( remaioiag for a 
lo.ger}iIDC; under the iaaudc:e 4If thole .ir"flre. 
fenlaliona,wllid, the c-'llll'l'd inliJR'l'_d'intmftcd 
TIM: F ru DE R ALl S t,.;., No.6,. 
.encoay fllCCftClc,.Rn}utingennr .... em ••• 
.... tIN ~ 
• • 1',. Slbit, .1 h.r..... 
,Ir~. no filial" 'Wei,,, til aB-thel't cOilSdera-
• ,ov~ "'Iio 
•• , to reco~ tha~ ,·hilo.,. illlor_ 111. no 
A FIFTH defadtratulD iIIuftratinr the ultlit,. 
long li,cd reJlllblic which Irad not a Senate. Spart~ 
l Sell:fte, i. the wtat oh due kufe of Da-
Rowe and CartMre are In faa th~ onl)' Stata to 
tional elmaaer. W/thollt a f:le.!t and ftahle mem· 
whOia that' cltarader can be ·oppbtd. 1ft eKh ul 
bel' .. f the BnVC'rn .... ~nt"t"e eftf'tm of fu~~gn pow-
tlae IWO firft there "'" a Semte tor life. The Con-
'f\'S will not. Mly be toddled by aft nnenltghttntel -
ftilutiOIl of the Senate in the 11ft, i. lef, kIIowII. 
" .... able 1'01tc)·, proceeding fr"m the ca~res alread, 
'Circiamftantial "ideDce mue.'it probable that it 
IIlmtioned ,·but ihe oational couorill Will 1Iot pof. 
WII IIoC dill'crml in Ihi. particular from tile two I!' 
(..... th. klolibilit)' to the opioio., of tile _I~, then. It i. at leaa tertaill that it had fome quala-
wloirld. perla.puaoe lef.1ICCc~ry illordd'toaaml, t, or other which rendered it aa anchor a,aillit 
aban it it tn obtain, it. refpcc!l and CODIidCft,!' 
• 
popular luauatioat i &ad dIat a rmaller council 
An' att~ntiOll t. the jud,ment of otbcr mllona II 
draw. out of the Scnate w"appoipttel not ollly for 
important to eYCrJ governmeat for two reaf_ : 
life; but filltd up VAelnda ilrdf. Thefe exam,leI, 
TIle one it, th. iockpcndtntly of tbe menu of a~, thoush II ullfit for tile, imiladop, Il' they :ll'e re •. 
~lIticulat pl.1I or IScafure, ill. c!efircllllie GO ,,~-
JlIIlnant to the genilla of America, are ootwitb-
0111 accoullt.. thaI it !hould appear to other aatl_ 
Amdin" WhCD compartel with Ihe flllilive Ind t:lr' 
at tJic oII'.ptiJII ol~ wile and 1I0llOrable policy: TIle 
bulellt caiftencc of OdlCl' antieDt republiu. "ery 
(ecODcl i., thlt ill doubtful cafel, particularly where 
inAnaaive proof. of tke neccllilY of Come inftitll' 
the utiOfta' c_ :II ma, be warped bJ fome ftrOlll 
tion that will blend ftabilil)' Willa liberty. I am 
palloft, or momentary intereft, the prefulDCcl or 
not uaa,,'arc • the. circumftancrs whkh diftiRguilb ,I 
know. opinion of the impartial world, ma, be the 
the Alilcrican from other popular governmenta, II 
bellguidc tllat un'be followed. What.b" not .Il-
",clllPtient I. modern; and which render cxtmDl 
laeric" loft by htr want of tharaaer wlt~ fore!Jlll 
circumfpel'lion necelfary ia reaConing from the one 
.ati00l1 And h .. " man, C'rrorl 1M folllCl :",ould 
cafe 10 tbe otllcr. But after allowing due weight 
Gae aot hlye ayoic/tel, it tbe jufticc IIKI prop"!ty t 
teL chi. conlideration. il may Ail! be maintaincd that 
ller mrafurtt had ita e"tI')' InRanee llecn prevlOU )' I 
daere arc milly poin .. of 6mililudc wbich readcr 
tried hy the light in wbich they 'Woulc! probably ap" 
thefe exampl" not unworthy of our aUfDtion. Ma. 
~ 
to lhe unbiall'cd part of mankind.. 
n)' oftbe defeda al we Jian fecn, which can only 
Yet however requilit.a fenle of national ~hlra~- I be fuppli~ by a renatorial iqllitulion. are commo .. 
ter may be. it it evident t!!at it can IIc .. er be (uflicl· 
to a lIumeroui alTembl, frrquently elcded b)' the 
ently'potretl'td by a numerciul and changeable body. 
people, and 10 the people themfdvCl. There IIrc 
It ClR ollly ~ found in • number fo linaJl, that.a 
otbert peculiar to the I.,rmer, which n:quire the I 
fenfihle degree of the praife .lId blame of p"blae 
controul of fuch an inllilulion. The pt'ople "an 
IDcaful'C!' may be thc ponilln oC each individual; or 
ac:yer wilfully betray tbear own inlerdts: But they 
iu an a4'cmbly f., durabl, invefted with public trull, 
may poffibly be betrayed by the rcprefcDtiltivcs uf ' 
that the pride and confequence qf it. mem~ ma, 
the people; aad the danger will be evidently gre"t'l 
be fcnlibly incorporaltd .. ·ilh the rtputauoa and 
er where the whQle Icgi~tive truft i. lodged in Ihe 
profpcrity of the community. The half· yearly re-
huds of one body of m<:II, than where the concur. 
prefclltallye. nf Rhode· Uland, would probably hut 
rence of feparatc and diRimilar bedia u required ill , 
11« .. lillie lIf1'eaed in their deliberation. on tbe ini-
e,·e.,. public act. 
i 
.uit.qtl. IlleJure. oft"al Slate, b)' argumellU dra.. 
Th, difl'crencc moft J'l!litel on bet"'ccn the Ame- I 
from the li,h! in wbick fucb meafuret would be 
rican and other tepublia, conGlla in thc prill.::cl~ 
.. iew~ b, foreign natinn.. or even by the filter 
of reprefcntatioD, which i. the nI .. .,. -
St·t ... I whilft it can fcarcely be doubttd, lhat if the 
former moye. an4 whie" . 
"' • ... d RabIe body had bfcn 
unknown tl'lh.' 
Detail from The Federalist No. 63 (numbered 62 in the original 
numbering system) as it appeared in The Ne~-York Packet, 
March 4, 1788. This essay, one of twelve Federahst pap~rs whose 
authorship has been in dispute between Alexander HamIlton and 
James Madison, also appeared in The Independent Journal, 
March 1, 1788. 

CHAPTER 1 
The Federalist Papers 
As a Case Study 
1.1. PURPOSE 
When two statisticians, both flanks unguarded, blunder into an historical and 
literary controversy, merciless slaughter is imminent. Our persistence needs 
explanation. 
From the point of view of statistical methods, authorship problems fall into a 
generlll area called discrimination or classification problems. In these problems 
the task is to assign a category to an object or individual whose true category is 
uncertain. In our authorship problem the objects are essays written by either 
Hamilton or Madison. We reduce our uncertainty about the authorship of an 
"unl}nown" essay by comparing its properties with information obtained from 
essays whose authorship is known. Classifying plants in biology, skulls in anthro-
pology, candidates for"parole in criminology, and subjects according to person-
ality in psychology are related operations that sometimes employ similar 
methods, even though the properties that aid the classification vary drastically 
from one area to another. The methods used to study one problem in discrimin-
ation can sometimes be extended to other areas of research. Weare concerned 
with the methodology of discrimination studies, and we especially wish to 
compare a number of methods of discrimination all based on much the same data. 
As explained later, for accidental personal reasons we began to study the 
authorship of the disputed Federalist papers. Because standard methods of 
historical research have not firmly settled this authorship problem, we felt 
justified in pursuing it with statistical methods. As the work progressed, we 
became dissatisfied with our rather catch-as-catch-can methods, although they 
may have been adequate for the immediate purpose of deciding authorship, 
and we realized that this problem presented an opportunity for a systematic 
comparison of two general methods of attack. One of these is the classical method 
of discrimination as devised by R. A. Fisher (1936). The other flows from the 
work of Thomas Bayes (1763) on statistical inference. Critics of the Fisherian 
approach complain that the method does not incorporate some important 
1 

2 
THE FEDERALIST PAPERS AS A CASE STUDY 
[1.2 
practical information; critics of the Bayesian plan usually agree that the in-
clusion of the information would be an asset, but they regard the proper assess-
ment of this information as a hopeless task. 
Harold Jeffreys (1939) had 
attempted to popularize the Bayesian approach. But it seems fair to say that 
it was not until about 1955 that statisticians began to think seriously of using 
Bayesian analysis directly in many practical problems, and even then it was 
mainly thinking. EVen by 1963, very few life-sized problems have employed 
Bayesian methods for their solution, and far fewer involve substantial analyses 
of data. 
To us, this lack of experience with the new method is unfortunate. So often in 
science and engineering, one finds that the difficulties anticipated from the arm-
chair scarcely overlap those that confound one in the field or laboratory. With 
all this in mind, we decided to enlarge our effort with The Federalist papers to 
produce a case study of the use of Bayesian and other methods of discrimination. 
The main value of our work does not depend much upon one's view of the 
authorship question used as a vehicle for this case study, although we do add 
information there. Rather the value resides in the illustrative use of the various 
techniques and in the generalizations that emerge from their study. In retro-
spect, the methodological analysis could have been restricted to sets of papers 
whose authors are known. Still, the responsibility of making judgments about 
authorship in disputed cases adds a little hard realism and promotes additional 
care that might otherwise have been omitted. 
Later, we explain the ideas and controversies about the methodology, but now 
let us turn to the subject of our case study-the authorship of the disputed 
Federalist papers. 
1.2. 
THE FEDERALIST PAPERS* 
The Federalist papers were written in 1787-1788 by Alexander Hamilton, 
John Jay, and James Madison to persuade the citizens of the State Of New York 
to ratify the Constitution. As was common in those days, these short essays, 
about 900 to 3500 words in length, appeared in newspapers signed with a 
pseudonym, in this instance, "Publius." They covered nearly every phase of the 
proposed Constitution. Seventy-seven essays first appeared in several different 
newspapers, and then Hamilton wrote an additional eight essays designed to 
complete the job. All the papers then were published in book form and have 
been republished repeatedly both here and abroad. 
* We are not historians, and we would be chagrined but not astonished to find some 
of the description in this section mildly in error, but it should be adequate to 19.y the 
setting for this report. A reader who wants more information and references will find 
the Adair (1944a, 1944b) articles both useful and fascinating. As for ourselves, we 
abbreviate Adair's description drastically and shamelessly. An additional good source 
is the preface to the Cooke (1961) edition. 

1.2] 
THE FEDERALIST PAPERS 
3 
Of the first 77 papers, it is generally agreed that Jay wrote five, and these are 
identified; drafts of Nos. 3, 5, and 64 exist, although that for No. 64 was mislaid 
for a while and then rediscovered. * Hamilton and Madison, as well as historians, 
seem agreed upon the authorship of an additional 57 papers, 43 by Hamilton 
and 14 by Madison. The authorship of another twelve is in flat dispute be-
tween Hamilton and Madison, and these are referred to below as "the disputed 
papers." An additional three are usually referred to as "Hamilton and Madison." 
In a nutshell, Hamilton said they were joint papers, and Madison said that 
Hamilton's material "was left with Mr. M on its appearing that the latter was 
engaged in it, with larger materials, and with a view to a more precise delineation; 
and from the pen of the latter, the several papers went to the Press." This 
gives us a definite "maybe" as far as the inclusion of Hamilton's material is 
concerned. 
Why is authorship hard to settle? For one thing, after Hamilton and Madison 
had written the papers they each took political positions on important issues 
that differed from some of the views presented in their own papers. Old writings 
can be embarrassing. The papers, after all, were meant as propaganda, so there 
is no guarantee that the views expressed were invariably held by an author even 
at the time he wrote them. Definitely some positions expressed were not held 
at later times. 
For concreteness, we mention two matters. Hamilton actually worked for a 
strong central government and against strong state governments, but he wrote 
in paper 28 that "the State governments will, in all possible contingencies, .af-
ford complete security against invasions of the public liberty by the national 
authority." Furthermore the party opposing Hamilton wanted strong state 
governments. As a similar difficulty, Madison in paper 44 took the position that 
once an end was sought by the law, the means were authorized. Later he adopt-
ed a "strict construction" theory of the Constitution. Hamilton used the means-
end idea to support the creation of the National Bank, and Jefferson and 
Madison the "strict construction" to try to strike it down. 
While it was generally known who had written The Federalist, no public 
assignment of specific papers to authors occurred until 1807, three years after 
Hamilton's death, when a Philadelphia publication printed a list, in a lettert to 
the editor signed M., claiming it to be based on Hamilton's own writing in his 
own copy. The letter announced that the executors of Hamilton's will deposited 
the copy in the Publick Library of New York. That copy has been missing for 
over 150 years. The list coincides with another list, the "Benson list," not made 
* The New York Times, p. 1, November 19,1959, reports the rediscovery as a major 
historical event. Number 64 has a special significance, as we see in the discussion of 
the Benson list. 
t The Port Folio, November 14, 1807, p. 318. On p. 316 a small notice says "The 
information from M, relative to the respective shares which Gen. Hamilton, Mr. Jay, 
and Mr. Madison had in the composition of that i'Pperishable collection of political 
essays, 'THE FEDERALIST,' is curious and valuable." 

4 
THE FEDERALIST PAPERS AS A CASE STUDY 
[1.2 
public until 1817, but said * to have been left with Egbert Benson shortly be-
fore Hamilton's fatal duel with Aaron Burr over remarks attributed to Hamilton 
concerning Burr's fitness for public office during a New York gubernatorial 
campaign. The Benson list reads "Nos. 2, 3, 4,5,54 by J. Nos. 10, 14,37 to 48 
inclusive, M. Nos. 18, 19,20, M. & H. jointly. All the others by H." (In The 
Port Folio the names are spelled out, and it cioses "all the rest by Mr. Hamilton.") 
Number 64 rather than 54 was actually written by Jay, so 54 is apparently an 
error. If 48 were similarly an error for 58, all but two of the papers in the 
controversy would be accounted for. Translating from Romari numerals helps 
promote such errors, especially when the original numerals have· misprints. 
In the 1788 McLean edition, essay 48 is numbered LXVIII, essay 49 is LXIX, 
and essay 70 is numbered LXXX at the head of the essays. 
Madison did not reply to the 1807 newspaper list. Of course, he may not 
have been aware of the article, and even if he were, he may not have been eager 
to enter his claim. Not until his retirement from the presidency did Madison's 
claim come forward. He claimed full authorship of numbers.49 through 58 and 
62 and 63, and, based on Madison's corrected copy of The Federali8t, the 1818 
Gideon edition of The Federali8t so appeared. Apparently, his count was accepted 
for fiE"arly half a century. 
Before we go further, the reader may as well know that there are more than 
two lists, and that they are not all alike. 
The Kent (Chancellor James Kent) list is well described by Cooket who says 
that 
"Because of differences in the ink and pen he used, Kent's statement may be 
divided into three parts, each of which was written at a different time. In 
the following copy of Kent's notes the three parts are indicated by Roman 
numerals: 
I. I am assured that Numbers 2. 3. 4. 5. & 54 [the number "6" was later 
written over the number "5"] were written by Jayt Jq,y. Numbers 10. 
14. 37 to 48 [the number "9" was later written over the number "8"] 
both inclusive & 53 by Jame8 Madi80n Jun. Numbers 18. 19.20. by 
Messrs Madi80n & Hamilton jointly-all the rest by Mr. Hamilton. 
II. (Mr. Hamilton told me that Mr. Miuli80n wrote No. 68 [the number 
"4" was later written over the number "6"] & 69 {the number "4" was 
later written over the number "6"] or from pa. 101 to 112 of Vol 2d) 
III. NB. I showed the above memo to General Hamilton in my office in 
Albany & he said it was correct saving the correction above made-See 
Hall's Law Journal Vol 6 p 461. 
* Cooke (1961, p. xxiii) points out that the Benson list is suspect because no one 
authenticating it claims to have seen it. The list disappeared in 1818. 
t Cooke (1961, pp. xxiv and xxvi) reproduced here with permission. See Preface. 
t Misprint for John. 

1.2] 
THE FEDERALIST PAPERS 
5 
Ii ••• on the page opposite the memorandum quoted above he [Kent] pasted a 
copy of the article from the City of Washington Gazette which stated that 
Madison had written Numbers 10, 14, 17, 18, 19,21,37-58,62-63, and that 
Jay was the author of Numbers 2,3,4,5,64. Underneath this clipping Kent 
wrote: 
I have no doubt that Mr. Jay wrote No 64 on the Treaty Power-He 
made a speech on that subject in the NY Convention, & I am told he says 
he wrote it. I suspect therefore from internal Evidence the above to be 
the correct List & not the one on the opposite page." 
The article Kent clipped out was said to have been "furnished by Madison 
himself." A letter signed Corrector in the Daily National I ntelligencer in the 
spring of 1817 gives to Madison 10, 14, 18-20, 37-58, 62-64, to Jay 2-5, the 
rest to Hamilton. The author says these numbers were copied from a "memo-
randum in the hand of Madison." 
Richard Rush's copy of The Federalist 
papers contains a list said to be in Madison's own handwriting: to Madison 10, 
14, 18-20, 37-58, 62, 63, to Jay 2-5, and 64, and the rest to Hamilton. Two 
lists come from Jefferson! In one, said to be in Jefferson's handwriting: to 
Madison 10, 14, 17-19, 21, 37-58, 62 and 63, to Jay 2-5, 64, the rest to 
Hamilton. The other list is said to be in the handwriting of Gideon Granger, a 
member of Jefferson's Cabinet, who says that the "information derived from 
Jefferson": to Madison 10, 14, 37-48, joint 18-20, to Jay 2-5, 54, the rest to 
Hamilton. While there may be other lists, the flavor of the situation is well 
contained in these. 
John Church Hamilton (1864), son of Alexander Hamilton, compared the 
disputed papers with other writings by Alexander Hamilton and decided that 
the latter wrote all the disputed papers. Cooke (1961, p. xxviii) feels Hamilton 
"produces some evidence" for Nos. 55-58. Kevertheless, Cooke gives more 
credence to Madison's claim. 
In 1888, the historian and, later, senator, Henry Cabot Lodge reanalyzed the 
position and decided largely on the basis of a credibility analysis to return in 
the main to Benson's list. 
About 1896, the Yale historian, E. G. Bourne (1901), gradually was sucked 
into the whirlpool of the Federalists while studying the use of history by framers 
of the Constitution. 
Bourne's approach was to match in parallel columns 
many lines from Madison's extensive notes of the Constitutional Convention 
and other writings with lines in the disputed papers. Madison's Convention notes 
seem to have been written up after The Federalist. Bourne felt that papers 51, 
53, 62, 63 were definitely Madison's and that 49 and 50 were very likely Madi-
son's. Similarly he felt the "joint" papers 18, 19, 20 could be fairly assigned to 
Madison. On balance, he concluded Madison's Gideon list to be correct. 
Paul Leicester Ford (1898) took up the fight and also analyzed the disputed 
papers, giving 49-51 to Madison, and 52-58 and 62 and 63 to Hamilton. He 
also awarded 47 and 48 to Hamilton, clouding matters a bit more. 

6 
THE FEDERALIST PAPERS AS A CASE STUDY 
[1.3 
Adair, cited earlier, decided on the basis of all the historical evidence he could 
find, including his own original researches, that Madison wrote the disputed 12 
papers, and was largely responsible for the joint papers 18, 19, and 20. 
Nevertheless, he pursued the matter further and has encouraged us to do so. 
Besides Cooke's edition, two other unabridged American editions (Rossiter, 
1961, and Wright, 1961) have appeared since we began our study of the author-
ship problem. Among the 12 disputed papers, Cooke assigns to Madison Nos. 49 
and 53, and the rest have "been attributed to Madison, but to indicate Ham-
ilton's claim his name has been placed in brackets underneath that of Madison." 
Rossiter assigns 49-58 to Madison, "and probably 62, 63." He owes his con-
fidence in this attribution " ... chiefly to the scholarly labors of Professor 
Douglass Adair ... " (p. xi) In Wright's edition, Adair's "assignment of author-
ship is followed, though question marks have been inserted after Madison's 
name at the head of Numbers 62 and 63, where there appears to be reason 
for doubt." (p. 10) 
Recently Irving Brant (1961) reviewed again the data on 62 and 63 and 
presses the Madison claim to these largely on the basis of the Kent list. Broadus 
Mitchell (1957, p. 419) in a Hamilton biography gives all but Nos. 62 and 63 to 
Madison. 
By and large the available historical evidence today is much the same as it 
was when Lodge attacked Madison's claim. Adair has noted that the preference 
shown for each man's claim has, over the years, swung with the popularity of 
the man's views, and with the evidence of the kind it is, if Hamilton's star 
should wax and Madison's wane, perhaps the recent thoughtful decisions can be 
reversed. Our own view isthat the historical evidence is modest enough that a 
reasonable but stubborn skeptic could retain the Scotch verdict "Not proven," 
and that others with special doubts or beliefs could sensibly maintain the op-
posite of the current viewpoint favorable to Madison. 
Later we develop our evidence, evidence of a different sort from that so far 
discussed. Using data internal to The Federalist, but not depending on its in-
tellectual content, we provide new evidence that adds to the historical evidence 
and permits a more nearly definitive assignment. The point, of course, is not 
just to make an assignment of authorship, but to provide solid communicable 
evidence about the value of one's assignment, and that we are able to supply in 
plenty. 
1.3. EARLY WORK 
Frederick Williams introduced Frederick Mosteller to this authorship problem 
in 1941 when the latter was a graduate student at Princeton University. In-
fluenced by Yule's (1938) and C. B. Williams' (1939) work on word counts and 
sentence length, Williams and Mosteller independently counted the number of 

1.3] 
EARLY WORK 
7 
words in all the sentences of The Federalist, * and their first frustration was the 
discovery of an important empirical principle-people cannot count, at least 
not very high (Miller, 1956; Wundt, 1912). But they created checks to get 
accurate results. The second difficulty, common to nearly every investigation, 
is that special problems must be settled by judgment. How should quotations 
from other authors be handled? How should numbers written out or in numerals 
be counted in words, and so on? These matters took a great deal of time, but 
finally, the task was done. When Williams and Mosteller emerged from their 
bout with the desk calculators, their second frustration was: 
Average sentence length: Hamilton-34.55 (words), 
Madison-34.59 (words). 
Still all was not lost: perhaps the variability in sentence length was the key to 
the style. They computed the standard deviation (a measure of variability) 
of the sentence lengths for each paper, then found the average of these for the 
two men: 
Average standard deviation: Hamilton-19.2 (words), 
Madison-20.3 (words). 
Needless to say, these measures would be hopeless for discriminating between 
the two authors. You will observe that the sentence length is long, on the aver-
age, and that the large standard deviation means that some sentences were very 
long. Both Hamilton and Madison had developed a style of writing much ad-
mired in their period, rather in imitation of the Spectator papers. To a modern 
reader the style is oratorical and somewhat overwhelming. It sounds important 
and convincing when read aloud, but the listener may find it difficult to recall 
the ideas in a passage, though they are plentiful. The style is formal and com-
plicated, the words are long, and the sentences are crowded with qualifications. 
* The text used by Williams and Mosteller was: The Federalist, Sesquicentennial 
Edition, National Home Library Foundation, Washington, D.C., 1937. 
From the "Bibliographical Note" (p. xxii): 
"The text here given follows closely the original McLean edition of 1788, which 
is generally accepted as authentic. But as Hamilton's table of contents is a mere 
skeleton, we have used, with the permission of the publishers, G. P. Putnam's Sons, 
the more inclusive table of contents from the edition of Henry Cabot Lodge (1886)." 
In preparing text for the high-speed computers, we used copies of The Modern 
Library edition of The Federalist, Random House, New York. This text appears to be 
identical with that of the Sesquicentennial Edition; indeed on p. iv, we read: 
"The publishers are indebted to the National Home Library Foundation for making 
this edition .possible and wish to thank them for the courtesy extended in the use of 
their plates." 
The Modern Library edition has an additional four pages (619-622) that give The 
Declaration of Independence of the United States. 

8 
THE FEDERALIST PAPERS AS A CASE STUDY 
TABLE 1.3-1 
TABLE 1.3-2 
NOUNS 
ADJECTIVES 
Per cent 
H 
M 
Per cent 
H 
21 
13 
6 
9 
22 
13 
7 
10 
15 
23 
10 
1 
11 
20 
24 
5 
12 
4 
25 
1 
13 
2 
Totals 42* 
14 
14 
1 
15 
Totals 42* 
TABLE 1.3-3 
TABLE 1.3-4 
ONE- AND Two-LETTER WORDS 
the's 
Per cent 
H 
M 
Per cent 
H 
21 
2 
7 
2 
22 
2 
3 
8 
12 
23 
1 
6 
9 
12 
24 
11 
1 
10 
10 
25 
12 
1 
11 
6 
26 
9 
1 
12 
27 
6 
13 
28 
1 
Totals 42* 
Totals 42* 
14 
TABLE 1.3-5 
DISCRIMINANT FUNCTION ANALYSIS 
Class intervals 
.55-.60 
.60-.65 
.65-.70 
.70-.75 
.75-.80 
.80-.85 
.85-.90 
Hamilton 
1 
5 
23 
12 
1 
42* 
Madison 
1 
2 
4 
5 
2 
14 
Disputed 
3 
6 
3 
12 
M 
1 
1 
6 
5 
1 
14 
M 
2 
6 
1 
2 
2 
1 
14 
* Two papers were pooled for this analysis; thus the total is 42 instead of 43. 
[1.3 

1.3] 
EARLY WORK 
9 
Cooke (1961, p.xxviii) says of the remarkably similar prose styles of Hamilton 
and Madison:* 
"This was no unique phenomenon, for most educated Americans of the 
late eighteenth century-a few particularly gifted writers, like Jefferson, per-
haps excepted-employed the same stylistic devices, the same standard 
phrases, and remarkably similar sentence structure." 
An additional way to use the word counts was to make counts of additional 
variables. Williams and Mosteller computed for each paper the per cents of 
nouns, of adjectives, of one- and two-letter words, and of the's. 
N one of the short words caused much trouble, but the workers were appalled 
by the number of special cases that seemed to arise in classifying words as nouns 
and as adjectives. Although they had been taught in school that these classes 
were clearly definable, they accumulated a great deal of evidence that would 
support the convictions of modern linguists that the categories of Latin grammar 
are often ill fitted to describe English (Roberts, 1958, pp. 131-150). Before 
finIshing, they assembled quite a little book of decisions (not all of which look 
wise in retrospect, although they did serve to create consistency in the counting). 
On the basis of theseclata, they constructed a statistic that was intended to 
separate Hamilton's writings from those of Madison by giving high scores to 
Hamilton's writings and low to Madison's. The statistic, which produces a 
weighted sum of the rates of occurrence of the four variables for each piece of 
writing, is calied a linear discriminant function. 
The frequency distributions for the percentages are shown in Tables 1.3-1, 
1.3-2, 1.3-3, and 1.3-4, where H stands for Hamilton and M for Madison. The 
entries are the numbers of papers of known authorship falling between p -
.5 
and p + .5, where p is a whole per cent. 
The frequency table for the discriminant function is shown in Table 1.3-5, 
grouped by class intervals of .05. The rightmost column gives the results for 
the 12 disputed papers. 
Some casual calculations in this table are suggestive, but not definitive. 
Ratios of proportions or of probabilities sometimes give odds. For example, 
in the interval .65-.70 Hamilton has the fraction -i2 of his known papers, while 
Madison has the fraction 1~. The ratio of these two fractions would give odds 
of about 12 to 1 for Madison. Similarly, in the interval.70-.75 the odds are 3 to 1 
(15 to 5) for Madison, and in the interval .75-.80 about 4 to 1 (23 to 6) for 
Hamilton. These calculations are rough in two ways. First, they have con-
siderable uncertainty because, for this purpose, the number of papers we have 
available by either author is small; second, the use of these odds implies that 
each author has an equal chance of having written a disputed paper, an 
assumption the reader may not wish to accept and one whose consideration 
and revision we defer until Chapter 3. 
* Quoted with permission. See Preface. 

10 
THE FEDERALIST PAPERS AS A CASE STUDY 
[1.4 
As often happens in discriminant analysis, the items needing to be sorted lie 
balefully in the middle, between the two criterion distributions. Had the prob-
lem been to assign the 12 papers in a block to one man, this evidence would 
support Madison. But that is not the problem. Each paper is to be settled sep-
arately. True odds factors of three or four to one are not compelling in decisions 
of this sort, and estimated odds of this magnitude are even less so. 
Events drew Mosteller and Williams apart, and they never succeeded in con-
tinuing their work together. But a few years ago Mosteller had a number of 
inquiries about these calculations, and these inquiries led to the present inves-
tigation. The original decisions on The Federalist as to what material to include 
and what to omit (for example, quotations) are retained in the new studies. 
1.4. RECENT WORK-PILOT STUDY 
About 1959, after previous correspondence, Douglass Adair informed Mos-
teller that he had found a pair of words (which we call marker words) that 
distinguished Hamilton and Madison quite well when the words occurred. Ham-
ilton uses while and Madison in a corresponding situation uses whilst. Adair 
has pursued his investigation of these words, including verification in the 
original newspapers. He has also found an earlier writer, Bailey (1916), who 
noticed the while-whilst distinction. On the other hand, both words have low 
rates per thousand words of text, so that not all papers can be discriminated by 
them. Furthermore, since there are a few exceptional reversals of use in writings 
of known authorship, a skeptic might well feel that one occurrence of such a 
marker word would not be enough to justify classification of the whole paper. 
For example, he might attribute an occurrence to a change by a typesetter. 
Indeed Cooke (1961, pp. xxviii-xxix) says* 
"To attempt to find in any of the disputed essays words which either man 
used and which the other never employed is futile, if only because the enor-
mous amount which each wrote allows the assiduous searcher to discover 
almost any word in the earlier or subsequent writings of both. " 
And Cooke goes on (p.606) to explain the further difficulties of using marker 
words. He notes that whilst in No. 51 was put there by Hamilton in the McLean 
edition as a change from and in the original newspapers. 
We do not attempt to use single words on an all-or-none basis to assist in the 
identification-rather we try to get so many words and clues that the total 
evidence is overwhelming, although no one clue is. And to this end, we use 
rates and other methods for weighting evidence. 
Adair's findings encouraged us to look for additional marker words and to 
begin a large study. We knew we wanted the counts done on high-speed com-
* Quoted with permission. See Preface. 

1.4] 
RECENT WORK-PILOT STUDY 
11 
TABLE 1.4-1 
INCIDENCE: NUMBERS OF PAPERS IN WHICH WORD OCCURRED 
AT LEAST ONCE 
enough 
while 
whilst 
Number of 
upon 
papers examined 
Hamilton 
14 
10 
0 
23 
23 
Madison 
0 
0 
13 
4 
19 
Disputed 
0 
0 
5 
1 
12 
Joint 
1 
0 
2 
2 
3 
TABLE 1.4-2 
RATES PER 1000 WORDS 
enough 
while 
whilst 
Total words in 
upon 
1000's 
Hamilton 
0.59 
0.26 
0 
2.93 
45.7 
Madison 
0 
0 
0.47 
0.16 
51.0 
Disputed 
0 
0 
0.34 
0.08 
23.9 
Joint 
0.18 
0 
0.36 
0.36 
5.5 
126.1 
puters, but while the programmers worked, we decided to have word counts 
made on a few papers by hand, as described in Section 2.6. 
We then made a rough screening plan that involved looking at incidences 
of each word in 6 Hamilton and 5 Madison papers (we call this set wave A); this 
screening led to the elimination or choice of marker word candidates. Straight-
forward objective rules were used, except that contextual words (see Section 2.1) 
were discarded freely. We then tried the words that looked promising on a set 
of papers on similar topics (5 by Hamilton and 5 by Madison), labeled the 
Exterior set, not in The Federalist (there is danger in the screening process of 
wasting the known Madison Federalist papers). This led to further elimination 
of marker word candidates. By now, Adair's while-whilst pair had emerged as 
important candidates (as we already knew), thereby suggesting that the screen-
ing method was not hopeless. Another word, upon, emerged as a Hamilton 
marker. These three words were tried on a second set of 10 Federalist papers 
(6 Hamilton and 4 Madison), wave B, and upon by itself separated the 10 
papers correctly and emphatically because Hamilton always had a rate of use 
higher than a prechosen number, about 1.5 per 1000 words, and Madison always 
a lower rate, usually zero. We decided that upon was definitely a marker word. 
Later the computer programming was completed and a few more Federalist 
papers became available (6 Hamilton and 5 Madison), wave C. The word enough 
emerged as another Hamilton marker. Summary data on the four strong marker 
words are given in Tables 1.4-1 and 1.4-2. 

12 
THE FEDERALIST PAPERS AS A CASE STUDY 
TABLE 1.4-3 
HAMILTON KNOWN PAPERS 
Paper 
Wave 
Federalist 
1 
6 
7 
8 
9 
11 
12 
24 
26 
36 
59 
65 
66 
70 
71 
72 
74 
77 
A 
A 
A 
A 
A 
B 
B 
B 
C 
B 
C 
B 
A 
B 
C 
C 
C 
C 
Total 
words 
1,570 
1,893 
2,245 
1,986 
1,629 
2,495 
2,140 
1,805 
2,379 
2,721 
1,847 
2,013 
2,215 
3,029 
1,702 
2,032 
906 
1,970 
36,577 
enough 
1 
1 
2 
1 
1 
7 
1 
1 
3 
3 
2 
1 
Exterior (identified in Section 2.2) 
Cont. 1 
1,188 
Cont.2 
1,380 
2 
Pac. 1 
2,945 
Pac. 2 
2,470 
1 
Pac. 3 
1,178 
9,161 
Total 
45,738 
Rate per 1000 words: 
27 
.59 
Absolute counts 
while 
whilst 
1 
1 
1 
1 
2 
1 
2 
1 
1 
1 
12 
.26 
o o 
upon 
6 
4 
11 
3 
4 
6 
7 
7 
6 
6 
3 
10 
11 
6 
3 
5 
2 
10 
2 
5 
6 
10 
i 
134 
2.93 
[1.4 
Rate per 
1000 words 
upon 
3.82 
2.11 
4.90 
1.51 
2.46 
2.40 
3.27 
3.88 
2.52 
2.21 
1.62 
4.97 
4.97 
1.98 
1.76 
2.46 
2.21 
5.08 
1.68 
3.62 
2.04 
4.05 
.85 
Looking at Table 1.4-2 for the disputed papers, note the absence of enough 
and while, the low rate for upon and the presence of whilst. The pattern of 
the whole line gives strong evidence for Madison's authorship. 
These data show rather clearly that the disputed papers as a whole are Mad-
isonian, but in this form they cannot settle the papers singly. Note that enough 
occurs among the joint papers, which otherwise look Madisonian. What is 
wanted from these and other data is, for each paper, a good measure of the 
weight of evidence toward Madison or Hamilton. 

1.41 
RECENT WORK-PILOT STUDY 
13 
Paper 
Wave 
Federalist 
10 
14 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
A 
A 
A 
A 
C 
B 
B 
C 
C 
B 
C 
B 
A 
C 
Total 
words 
2,987 
2,149 
2,709 
3,314 
2,585 
2,718 
3,506 
2,711 
3,029 
2,561 
2,117 
2,601 
2,547 
1,561 
37,095 
TABLE 1.4-4 
MADISON KNOWN PAPERS 
enough 
Absolute counts 
while 
whilst 
1 
1 
2 
1 
1 
2 
2 
2 
Exterior (identified in Section 2.2) 
Relv. 1 
2,945 
Relv. 2 
2,704 
Relv.3 
2,458 
N. Am. 1 
3,048 
N. Am. 2 
2,798 
13,953 
Total 
51,048 
Rate per 1000 words: 
0 
o 
1 
3 
1 
3 
4 
24 
.47 
upon 
1 
4 
2 
1 
8 
.16 
Rate per 
1000 words 
upon 
.37 
1.21 
.74 
.37 
By breaking down the data of this preliminary study, we can give the reader 
some idea of the state of the evidence at this point, before we turn to more 
systematic approaches. Tables 1.4-3, 1.4-4, and 1.4-5 show the data for Hamil-
ton, Madison, and the Disputed and Joint papers. They show all four words as 
strong discriminators, though upon is far and away the best. 
On the basis of Table 1.4-5 the evidence seems to be that all 12 disputed 
papers are by Madison and that most of the material in Nos. 18-20 is too. 
For us the problem is to extend this evidence and to measure its strength. 
We are not faced with a black-or-white situation, and we are not going to pro-
vide an absolutely conclusive settlement. Outside those parts of logic and mathe-
matics that are divorced from the empirical world, strong confidence in conclu-
sions is the most an investigation can be expected to offer. 

14 
THE FEDERALIST PAPERS AS A CASE STUDY 
[1.5 
TABLE 1.4-5 
J OINT AND DISPUTED PAPERS 
Total 
Absolute counts 
Rate per 
Paper 
words 
1000 words 
enough 
while 
whilst 
upon 
upon 
Joint 
18 
2,084 
1 
1 
1 
.48 
19 
2,019 
1 
20 
1,438 
1 
.70 
Total 
5,541 
1 
2 
2 
Rate per 1000 words: 
.18 
° 
.36 
.36 
Disputed 
49 
1,594 
1 
50 
1,103 
51 
1,911 
2 
52 
1,841 
53 
2,160 
1 
54 
1,996 
2 
1.00 
55 
2,034 
56 
1,560 
1 
57 
2,200 
3 
58 
2,082 
62 
2,380 
63 
·3,020 
Total 
23,881 
° 
° 
8 
2 
Rate per 1000 words: 
° 
0 
.34 
08 
1.5. PLOTS AND HONESTY 
Most writers on this disputed authorship problem tumble over themselves to 
assure the reader that there is no question of honesty or integrity on the part of 
either Hamilton or Madison, that the whole matter is one of innocent mistakes, 
that there has been no attempt at deceit, that it is a question of memory, 
not veracity. Madison said much the same thing, attributing Hamilton's dis-
agreement, essentially, to carelessness. From the point of view of the historical 
evidence, the assumption of honesty is fairly important. Current historical 
attribution leans on Madison's mature consideration for his claim and on the 
Gideon edition which agrees with and is that claim. Further, it soft-pedals the 
Benson list because the authenticators did not say they saw it. The Kent list, 
with its contradictory notes written over a period of years, corrected in who 
knows whose handwriting, is one basis for allowing Hamilton to yield some of 
the disputed papers to Madison. If Hamilton ever had a claim, it seems to pass 

1.6] 
THE PLAN OF THE BOOK 
15 
into limbo largely by default because of the vanishing of lists and of his own 
copy of The Federalist, and the general doubt surrounding the authenticity 
of all claims attributed to him. Except then for Nos. 62 and 63, current 
historical evidence largely relies on Madison's attribution in the Gideon edition. 
We are not trying to slight the excellent historical work; we are trying to get to 
the critical assumptions underlying the arguments. 
Thus, we feel that the historical evidence is strong, but not enormously 
strong, because it leans so heavily upon the integrity and memory of one man. 
Our own evidence has a different weakness. It depends upon the extent of 
the editing done by one or the other man. Of course, since the papers were writ-
ten quickly, we suppose they were not heavily edited except by their authors, 
but we cannot guarantee that. If Hamilton wrote and Madison edited Nos. 49 
through 58, say, but not other Hamilton papers, then Madison might remove 
upon's and enough's, change while's to whilst's, and so on, to confuse our methods, 
but there is a lot against this, as we see later. Although we cannot overcome this 
criticism completely, we can mitigate it by producing a great deal more evi-
dence, based on many marker words, and even more, on high-frequency words, 
such as by, oj, and to. With the frequently used words, the differences between 
these authors'. rates of use are relatively slight, and the chance much greater 
that neither author would be aware of the differences. 
As the reader will 
see later, we are encouraged to find considerable consistency among the papers 
generally agreed to be Hamilton's and among those generally agreed to be 
Madison's. If the editing had been heavy, we might hope that it would 
partially reveal itself through inconsistency in papers other than the disputed 
ones. Because this trouble does not arise, we have some evidence for the lack 
of cross-editing, and some for the accuracy of the Gideon edition, but we find the 
amount hard to evaluate. We pursue this further in Section 3.7F, where we 
can give the reader a better notion of the large quantity and cunning quality 
of editing required to match the facts. 
1.6. THE PLAN OF THE BOOK 
In all we give four parallel studies, each with somewhat different methodology: 
first, in Chapter 3, the main study based on Bayes' theorem, with Chapter 4 as 
its technical appendix; second, in Chapter 5, a classical linear discrimination 
study; third, in Chapter 6, a "robust" Bayes analysis that is cheaper and less 
subject to troubles from distributional assumptions than the main study, but 
also less sensitive; fourth, in Chapter 7, a simplified rate study along some-
what classical lines. Later chapters attend to a variety of matters. 
Chapter 2 lays the foundation for the word counts on which all these studies 
are based. 

CHAPTER 2 
Words and Their 
Distributions 
2.1. WHY WORDS? 
When we leave general style as a poor bet and pay attention to words, we 
find that Hamilton and Madison use certain words at quite different rates. 
Douglass Adair brought this spectacularly to our attention by pointing out 
their uses of while and whilst. In our work, we have used individual words as the 
principal basis for measuring likelihood of authorship. 
Early investigations 
convinced us that most single variables, carefully selected or not, have little 
discriminating value, and that a large pool of variables provides the greatest 
hope of success. Sentence length is a good example of a stylistic variable which 
had even been used effectively elsewhere, yet failed miserably here. Since the 
rate for each word can be regarded as a variable, words supply a pool of thou-
sands of variables. Furthermore, words are easily recognized and effective for 
discrimination. 
Our "word" is a composite of all words of the same spelling, capitalization 
neglected. To put our worst foot forward at once, we do not distinguish abuse as 
noun from abuse as verb, the great of great plan from that of Great Britain, nor the 
personal pronoun I from the Roman numeral I. Distinguishing between words of 
the same spelling is rejected almost solely because it cannot be done routinely, and 
certainly not yet by a high-speed computer (except in experimental programs). 
Actually, the words used as discriminators in most of our work rarely admit 
these ambiguities. 
Variables involving grammatical concepts have attractions. One fine class 
consists of single words, each split into its different uses (see Section 8.5). 
Such variables seem more likely to discriminate between authors, and indeed to 
underlie the discriminatory ability of words as we use them. The difficulties of 
this proposed improvement are huge. Even if we assume that one could list 
the (word, use) pairs to be considered, the task of classification looks unfeasible. 
That it could be made acceptably objective is doubtful, and the difficult decisions, 
if nature showed its usual malice for the investigator, would occur not frequently 
but just where discrimination exists. 
16 

2.1] 
WHY WORDS? 
17 
TABLE 2.1-1 
FREQUENCY DISTRIBUTION OF RATE PER THOUSAND WORDS 
FOR THE 48 HAMILTON AND 50 MADISON PAPERS 
FOR by, from, AND to. 
by 
from 
to 
Rate per 
H 
M 
Rate per 
H 
M 
Rate per 
H 
M 
1000 words 
1000 words 
1000 words 
1-3 
2 
1-3 
3 
3 
20-25 
3 
3-5 
7 
3-5 
15 
19 
25-30 
2 
5 
5-7 
12 
5 
5-7 
21 
17 
30-35 
6 
19 
7-9 
18 
7 
7-9 
9 
6 
35-40 
14 
12 
9-11 
4 
8 
9-11 
1 
40-45 
15 
9 
11-13 
5 
16 
11-13 
3 
45-50 
8 
2 
13-15 
6 
13-15 
1 
50-55 
2 
15-17 
5 
Totals 
48 
50 
55-60 
1 
17-19 
3 
Totals 
48 
50 
Totals 
48 
50 
Let us examine a few words for their ability to discriminate and for their 
consistency of rate. For this purpose, we discuss some results on 98 items of 
writing: 48 by Hamilton, 50 by Madison. We call each item a "paper" (the 
precise sources of the writings are described in Section 2.2). One class of words 
that we use has been called function words-the filler words of the language, 
such as a, an, by, to, and that. Generally they include prepositions, conjunctions, 
pronouns, and certain adverbs, adjectives, and auxiliary verbs. 
Table 2.1-1 gives frequency distributions for the rates of use per thousand 
words of text for the function words by, from, and to. We employ rate here 
rather than frequency because the papers vary in length-from 906 words to 
3551. A length of 2000 is somewhat typical. 
Casual inspection of Table 2.1-1 suggests that low rates for by are favorable 
to Hamilton's authorship, and high rates to Madison's. Rates for to are in the 
opposite direction. Very high rates for from suggest Madison, but low rates 
give practically no information. It appears that by discriminates better than to, 
which in turn is superior to from. 
We like the function words rather well because many of them are not much 
influenced by the context of the writing, but other sorts of more meaningful 
words also seem relatively free from context, for example, commonly, innovation, 
fortune, vigor, and voice. In Table 2.1-2, we give the rates for commonly and 
innovation. 
In contrast with by, from, and to, which are high-frequency words, commonly 
and innovation have low frequencies. Zero is the most frequent rate for both 
words. Our longest paper has 3551 words, so that the lowest possible nonzero 

18 
WORDS AND THEIR DISTRIBUTIONS 
[2.1 
TABLE 2.1-2 
FREQUENCY DISTRIBUTIONS FOR commonly AND innovation 
commonly 
innovation 
Rate per 
H 
M 
Rate per 
H 
M 
1000 words 
1000 words 
o (exactly) 
31 
49 
o (exactly) 
47 
34 
OL .2 
(cannot occur) 
0+- .2 
(cannot occur) 
.2- .4 
3 
1 
.2- .4 
6 
.4- .6 
6 
.4- .6 
1 
6 
.6- .8 
3 
.6- .8 
1 
.8-1.0 
2 
.8-1.0 
2 
1.0-1.2 
2 
1.0-1.2 
1 
1.2-1.4 
1 
Totals 
48 
50 
Totals 
48 
50 
rate is 1000 X 1/3551 ~ .28. * Generally speaking, occurrences of commonly 
are favorable to Hamilton's authorship, of innovation to Madison's. Nonoccur-
rence of either word says relatively little about the authorship. 
Words such as law, executive, liberty, money, trade, war, and states vary greatly 
in their rate with the context of a paper. Since The Federalist papers deal with 
specific topics in the proposed Constitution, variation is to be expected. Suppose 
that an investigator allowed such words among his potential discriminators 
and found one, say, trade, to discriminate well among the known papers. It is 
still quite possible that a peculiar assignment of tasks between Hamilton and 
Madison might find the one who had ordinarily not discussed trade writing on 
that topic in a disputed paper and thus throwing the analysis off. The assump-
tions underlying our later analyses seem inappropriate for contextual words 
because of this dependence on external information, such as who would be likely 
to write about trade. While such words provide evidence, we cannot evaluate 
it to our own satisfaction, let alone to that of one who believes that, say, Hamil-
ton was so expert on trade that he would never have agreed to let Madison 
write on it. Consequently, we decided to eliminate these meaningful, contextual 
words. 
Unfortunately, recognition of words that should be so discarded is 
difficult. In some of our studies, contextual words were eliminated on an ad hoc, 
intuitive basis; in others the problem was met by constructing lists of non-
contextual words to be used as candidates for discriminators. 
To give an example of a contextual word, we display the distribution of rates 
for war in Table 2.1-3. 
For both authors, the rates vary from 0 to 15 per thousand. While occur-
rences of the word look somewhat favorable to Madison's authorship, Hamilton 
* We use the symbol ,,~" for "is approximately equal to" or "approximately equals." 

2.2] 
VARIATION WITH TIME 
19 
TABLE 2.1-3 
TABLE 2.1-4 
FREQUENCY DISTRIBUTION FOR war 
FREQUENCY DISTRIBUTION FOR upon 
Rate per 
H 
M 
Rate per 
H 
M 
1000 words 
1000 words 
o (exactly) 
23 
15 
o (exactly) 
41 
0+-2 
16 
13 
0+-1 
1 
7 
2-4 
4 
5 
1-2 
10 
2 
4-f\ 
2 
4 
2-3 
11 
6-8 
1 
3 
3-4 
11 
8-10 
1 
3 
4-5 
10 
10-12 
3 
5-6 
3 
12-14 
2 
6-7 
1 
14-16 
1 
2 
7-8 
1 
Totals 
48 
50 
Totals 
48 
50 
occasionally has a startlingly high rate. Incidentally, it is amusing to find that 
Madison uses it more, while Hamilton is spoken of by some biographers as 
having strong military ambitions. 
Our best single word for discrimination is upon, and its distribution is shown 
in Table 2.1-4. 
Low rates for upon go with Madison, high rates with Hamilton. The spread 
or variability of the distributions seems appropriate for both authors: Hamilton's 
consistent with his 3/1000 rate and Madison's consistent with his .18/1000. 
To ilU~marize, our discussion of the distributions in Tables 2.1-1 through 
2.1-4 gives the reader some notion of the sorts of variables used in this study 
and al~rts him to the special worries we have about contextual words. 
2.2. VARIATION WITH TIME 
In our work, we find it necessary to use papers not included in The Federalist 
as well as those that are. Since Madison had but 14 known papers in The 
Federalist, we felt forced to enlarge our sample of his writings from other material. 
This means in turn that we have analyzed parts of his writings from the years 
1780 through 1806, and thus arose the opportunity to see how his rates varied 
through the years. Since we rarely went outside The Federalist for Hamilton's 
writings, we could not make a corresponding study. 
Before proceeding, the reader may wish to review the list of writings of the 
two authors, extedor to The Federalist, that we employ. In the Appendix a 
special list of such references is given together with the code numbers we use to 
identify the materials. 

20 
WORDS AND THEIR DISTRIBUTIONS 
The Madison papers that we studied fall into time periods as follows: 
(a) 1 paper (about 2900 words) 
An essay on money in Freneau' s Magazine, not published until 
1791. 
(b) 2 papers (about 5800 words total) 
The North American No. I and The North American No. II, 
anonymous political writings. 
(c) 14 papers (about 37,000 words total) 
The 14 known papers from The Federalist. 
(d) 1 paper (about 3600 words) 
(Oct.) 
Observations on a draft Constitution for Virginia. 
(e) 7 papers (about 11,000 words total) 
Further Freneau essays. 
(f) 5 papers (about 13,000 words total) 
The five Helvidius papers, written in reply to a series by Hamil-
ton (signed Pacificus) on powers of the Executive. Helvidius's 
remarks on war were in accord with his questioning of the powers 
of the Executive branch. 
(g) 20 papers (about 40,000 words total) 
The long Neutral Trade paper, which we have broken into twenty 
pieces of about 2000 words each. 
[2.2 
Year 
1780 
1783 
1787-8 
1788 
1791-2 
1793 
1806 
Since groups (a) and (d) contain only one paper eaph, we combined group (a) 
with group (b) and group (d) with (c). The five rellulting groups are listed 
in Table 2.2-1, together with the code designation of each paper and the word 
count for each group. 
At the outset of the study, groups II and V were noted to have particular 
interest because of the time lapse between the papers, the contextual change, 
and the large number of words in each. Group II represents the Federalist 
group, and the papers of group V are primarily concerned with questions of 
neutral trade in time of war. These two groups are especially convenient to 
compare since the total number of words is about the same in each. Within 
each period, we have pooled all the papers, and Table 2.2-2 shows the rates 
for the five periods for every seventh function word on one of our basic lists 
(see Section 2.5A), plus one function word, her, that we thought would display 
conte:lttuality: 
In studying these rates, one has to face squarely the impossibility of clearly 
separating temporal effects from contextual ones. Generally, the absence of a 
positive reason for attributing a change to contextuality leaves us with time 
as a residual cause. We supposed that in the Neutral Trade papers, group V, 
discussions of countries WOlfld lead to the frequent use of her, and so it did. 
In Table 2.2-2, by, from, must, one, some, and were appear fairly stable. 
The only word suggesting a trend with time is in (and possibly from), and the 

2.2] 
VARIATION WITH TIME 
21 
TABLE 2.2-1 
MADISON'S PAPERS, GROUPED BY DATE OF WRITING 
Group 
Number of 
Date 
Code 
Description 
Length 
papers 
number 
I 
3 
1780 
302 
M-4 
8,725 words 
1783 
121, 122 
NA-1, NA-2 
II 
15 
1787-88 
10,14, 
14 known 
40,646 
37,38,39, 
Federalists, 
40,41,42, 
N9 
43,44,45, 
46,47,48, 
141 
III 
7 
1791-92 
301,311, 
M-l to M-3, 
10,889 
312,313, 
M-5 to M-9, 
314, 315, 
N-1 to N-8 
316 
IV 
5 
1793 
131, 132, 
Helv. 1-5 
12,779 
133, 134, 
135 
V 
20 
1806 
201 through Neutral 
40,561 
220 
Trade 
TABLE 2.2-2 
MADISON'S RATES PER 1000 WORDS OVER A 25-YEAR PERIOD 
Group 
I 
II 
III 
IV 
V 
Total 
Length in 
thousands of words 
8.725 
40.646 
10.889 
12.779 
40.561 
113.600 
Years 
1780-83 1787-88 1791-92 
1793 
1806 
Word 
any 
1.38 
1.92 
.83 
3.36 
3.23 
2.40 
by 
7.45 
11.91 
12.95 
10.80 
11.54 
11.41 
from 
4.47 
4.99 
5.14 
5.32 
6.80 
5.65 
her 
2.06 
.66 
.28 
1.10 
5.46 
2.49 
~n 
15.47 
21.03 
21.39 
24.49 
26.89 
23.11 
must 
2.18 
2.36 
2.48 
2.43 
1.46 
2.04 
one 
3.90 
3.08 
4.22 
2.19 
2.20 
2.83 
some 
.57 
1.53 
.72 
1.10 
1.19 
1.21 
there 
1.26 
.84 
1.84 
3.13 
1.14 
1.33 
were 
1.95 
1.97 
1.56 
1.09 
2.27 
1.93 
would 
5.39 
4.43 
2.85 
5.24 
1.95 
3.56 

22 
WORDS AND THEIR DISTRIBUTIONS 
[2.3 
words any, her, there, and would vary considerably from one group to another. 
Our general conclusions based on this table and on similar studies with other 
words are that pronouns and auxiliary-verb forms are potentially contextual 
and offer risky discrimination. Furthermore, we concluded that for the main 
study special pains should be taken to examine all candidates for the final 
list of discriminators for evidence of large variability between groups of writings. 
Essentially we had three choices: to revise our model for word distributions 
in such a way as to handle excessive variability between groups of writings 
within the model, or to restrict our source of words to The Federalist papers 
alone in hopes that rates are homogenous there, or, finally, to try to choose words 
whose rates are not much affected by the group of writings. We chose the 
third of these alternatives, partly for expediency, because we regard the first 
as an ideal choice that we could not afford, but in definite preference to the 
second. Since 10 of the disputed papers come in a clump, the group as a whole 
might differ substantially in its rates from those of its author or authors in other 
Federalist writings. Therefore the second choice could easily lay the study open 
to the danger we wish to avoid. Quite aside from these considerations, persons 
interested in such studies will prefer methods that spot authorship over a period 
of years and a variety of topics to those that are tightly restricted. 
2.3. HOW FREQUENCY OF USE VARIES 
A mathematical model is a description of a process in mathematical terms. 
For example, the parabolas that describe the behavior of bodies moving through 
a vacuum near the surface of the earth, together with the rules for generating 
them, form a mathematical model for bodies in flight. Similarly, the mathe-
matical descriptions of the behavior of light in systems of mirrors and lenses are 
mathematical models of the real thing. One advantage of the mathematical 
model is that it can be manipulated in the absence of the structure it represents; 
another is that consequences can sometimes be derived from the mathematics 
that are not obvious from inspection of apparatus or data. The models for 
moving bodies and light do not ordinarily have uncertainty or variability built 
into them. We need a model to describe the changing rates with which words are 
used. One reason is that we do not have infinite amounts of data associated 
with each paper, and we need to be able to appraise our uncertainty. We need 
to be able to assemble information from different words to strengthen our in-
ference, and without a model one is hard put to know how to do this. 
Even the finest mathematical models do not represent a physical process 
perfectly or completely, but if we can represent a process fairly closely, we expect 
the consequences from the model to be fairly close to the truth. This section 
and the next are devoted to considering what would be an adequate model for 
describing the variation in the use of words. 
Setting up a complete statistical model for the frequency of use for even a 
single word is a verydifficuIt taskl and fortunately, for our purposes, it is un-

2.3] 
HOW FREQUENCY OF USE VARIES 
23 
necessary. 
We examine word frequencies only in good-sized blocks of text, 
mostly of 1000 words or more, and of at least 100 words in all intended studies. 
Over pieces longer than 100 words, details of local dependence introduced by 
grammatical structure and style, such as avoidance of repetitions, ought not to 
be especially important when we study total counts. In what follows, we talk 
in terms of models without any allowance for local dependence, with the intent 
that the models be judged by and used for their behavior in pieces of text of 
100 or more words. 
How does frequency of use vary among and within chunks of writing? Two 
counter-pressures that we all recognize are those for splendid isolation and for 
clubbiness. In writing, we avoid using the same word over and over, and this 
isolation tends to space occurrences and make the rate quite constant. But for 
emphasis, parallelism, or clarity, we may repeat a word several times in a brief 
passage. One concrete way to raise the distributional problem is this: if a word 
has just been used, is it more or less likely to be used in the next, say, 200 words 
than if it has not been? 
2.3A. Independence of words from one block of text to another. One 
could scarcely hope that the chance of occurring soon does not depend on whether 
or not the word just occurred. The absence of such dependence implies statistical 
independence. We often use such an assumption as statistical independence as a 
base line to measure departures against. Let us discuss the idea first for pairs of 
adjacent blocks of 200 words. Suppose that in the long run t the blocks contain 
the word at least once and that we do have independence. Then among a great 
many pairs of adjacent blocks! of the pairs would not contain the word at all, 
t would contain it in exactly one block, and! would contain it in both blocks. 
An empirical test of independence can show deviations from this simple theory. 
Perhaps many more than t the pairs have occurrences in just one of the two 
adjacent blocks-just as in baseball, for example, too many double-headers are 
split for independence to hold. 
The classical theory of the binomial distribution (see, for example, Mosteller, 
Rourke, and Thomas, 1961, Chapter 7) tells how the counts of occurrences 
should behave for probabilities of incidence more general than t, and for more 
than two blocks. In what follows we examine results for four adjacent blocks. 
We use four instead of two blocks to magnify the effects of dependence. We 
now present the special theory that assumes independence for four blocks. 
THEORY. Let p be the fraction of blocks in which a word occurs; then the 
formula for the desired binomial probabilities can be shown to be as follows: 
Number of blocks 
with word present 
Probability of 
that number 
o 
1 
2 
3 
4 

24 
WORDS AND THEIR DISTRIBUTIONS 
[2.3 
We can estimate p as the observed fraction of all blocks in which the word 
occurs. Then the estimated probabilities in the above array can be multiplied 
by the number of sets of four blocks, 39 in our problem, to make a direct com-
parison of frequencies. Tables of the binomial distribution (National Bureau of 
Standards, 1949) facilitate the calculation. 
Application. To get appropriate data, we broke a sequence of Hamilton 
papers (Federalist 13,16,17,23,25-35,59-61,68,69,71-77) into 247 blocks of 
about 200 words each. Then we formed 39 sets of four adjacent blocks, each set 
contained entirely in a single paper. For any set, a given word can occur in 0, 1, 
2, 3, or 4 blocks. Table 2.3-1 shows the observed frequencies for 51 words, 
selected as discussed later, and beneath them frequencies computed on the basis 
of the binomial distribution, where p is again estimated as the fraction of blocks 
in which the word occurred. 
To the eye, most of the observed and fitted distributions agree rather well. 
Some exceptions are his, one, only, than. We thought it wise to compute for 
each word the binomial dispersion index (Hoel, 1954, pp. 175-177): 
2 
L(X -
X)2 
X = 
, 
x[1 - (x/4)] 
where the sum is over the 39 sets, the 4 is the number of blocks in a set, x is the 
count fora single set of four blocks, and x is the average number of blocks per set 
with occurrences. 
The purpose of the index is to measure the agreement between observed 
set-to-set variation and the theoretical variability supplied by the binomial dis-
tribution. The denominator provides the theoretical estimate 
2 
(x)(n-x) 
(f = npq "'" n n -n-' 
and the numerator provides the sum of squares that makes the distribution of 
the index, as an approximation, belong to a standard family of distributions 
much used in the statistical literature, the family of x2-distributions. 
The 
particular member of that family which is appropriate for this problem is the 
one designated by the expression "38 degrees of freedom, " one degree of freedom 
less than the number of squares, 39, that are added. Fortunately, tables of the 
percentage points of these distributions are widely available, and we make use 
of them in what follows. 
Instead of computing one x2 for one word, we examine the observed x2-values 
for the 51 words and compare the resulting distribution with the theoretical 
distribution for x2 which is broken into tenths in Table 2.3-2. 
The theoretical distribution therefore has n = 
5.1 values expected per inter-
val. The observed distribution is reasonably consistent with this uniformity 
except for the upper tail, where quite a few words yield extremely high values of 
x2 . Possibly, serious nonbinomiality-block-to-block dependence-is restricted 

2.3] 
HOW FREQUENCY OF USE VARIES 
25 
TABLE 2.3-1 
HAMILTON'S INCIDENCE DISTRIBUTION FOR SETS OF 4 BLOCKS 
Blocks 
Word 
0 
1 
2 
3 
4 
all 
observed 
4 
8 
15 
lO 
2 
binomial 
2.6 
lO.1 
14.6 
9.4 
2.3 
also 
observed 
31 
6 
2 
0 
0 
binomial 
29.9 
8.2 
.8 
.0 
.0 
an 
observed 
I 
2 
13 
13 
lO 
binomial 
.4 
3.2 
lO.7 
15.9 
8.8 
any 
observed 
2 
9 
17 
9 
2 
binomial 
2.4 
9.8 
14.6 
9.8 
2.4 
at 
observed 
5 
7 
16 
6 
5 
binomial 
2.6 
10.1 
14.6 
9.4 
2.2 
been 
observed 
5 
15 
15 
4 
0 
binomial 
6.5 
14.7 
12.4 
4.7 
.7 
can 
observed 
9 
12 
5 
12 
1 
binomial 
5.0 
13.5 
13.5 
6.0 
1.0 
do 
observed 
27 
12 
0 
0 
0 
binomial 
28.4 
9.3 
1.2 
.1 
.0 
down 
observed 
35 
4 
0 
0 
0 
binomial 
35.0 
3.7 
.2 
.0 
.0 
even 
observed 
19 
14 
5 
1 
0 
binomial 
18.5 
15.2 
4.7 
.6 
.0 
every 
observed 
lO 
18 
8 
2 
1 
binomial 
lO.5 
16.3 
9.5 
2.5 
.2 
from 
observed 
0 
3 
14 
17 
5 
binomial 
.6 
4.4 
12.1 
15.0 
7.0 
had 
observed 
21 
15 
2 
1 
0 
binomial 
21.3 
13.9 
3.4 
.4 
.0 
has 
observed 
9 
14 
11 
5 
0 
binomial 
7.9 
15.5 
11.4 
3.8 
.5 
her 
observed 
35 
3 
1 
0 
0 
binomial 
34.2 
4.5 
.2 
.0 
.0 
his 
observed 
19 
11 
5 
1 
3 
binomial 
13.7 
16.4 
7.3 
1.5 
.1 
(cont.) 

26 
WORDS AND THEIR DISTRIBUTIONS 
[2.3 
TABLE 2.3-1 (cont.) 
Blocks 
Word 
0 
1 
2 
3 
4 
if 
observed 
1 
13 
13 
7 
5 
binomial 
2.2 
9.4 
14.6 
10.1 
2.6 
into 
observed 
13 
13 
12 
1 
0 
binomial 
11.7 
16.4 
8.7 
2.0 
.2 
its 
observed 
5 
7 
12 
9 
6 
binomial 
1.9 
8.6 
14.5 
10.9 
3.1 
may 
observed 
2 
10 
13 
12 
2 
binomial 
2.2 
9.4 
14.6 
10.1 
2.6 
more 
observed 
8 
11 
11 
8 
1 
binomial 
5.4 
13.8 
13.2 
5.6 
.9 
must 
observed 
9 
16 
9 
5 
0 
binomial 
8.8 
15.9 
10.7 
3.2 
.4 
my 
observed 
37 
2 
0 
0 
0 
binomial 
37.8 
1.2 
.0 
.0 
.0 
no 
observed 
11 
14 
9 
5 
0 
binomial 
9.9 
16.2 
9.9 
2.7 
.3 
now 
observed 
32 
5 
2 
0 
0 
binomial 
30.4 
7.8 
.7 
.0 
.0 
on 
observed 
3 
12 
13 
9 
2 
binomial 
4.7 
13.1 
13.7 
6.3 
1.1 
one 
observed 
1 
22 
10 
5 
1 
binomial 
5.4 
13.8 
13.2 
5.6 
.9 
only 
observed 
18 
10 
8 
3 
0 
binomial 
14.4 
16.3 
6.9 
1.3 
.1 
or 
observed 
0 
4 
15 
17 
3 
binomial 
.8 
5.3 
13.0 
14.1 
5.8 
our 
observed 
22 
12 
4 
1 
0 
binomial 
20.4 
14.4 
3.8 
.4 
.0 
should 
observed 
7 
14 
13 
4 
1 
binomial 
6.5 
14.7 
12.4 
4.7 
.6 
80 
observed 
9 
16 
9 
5 
0 
binomial 
8.8 
15.9 
10.7 
3.2 
.4 
some 
observed 
18 
13 
8 
0 
0 
binomial 
16.8 
15.8 
5.5 
.9 
.0 

2.3) 
HOW FREQUENCY OF USE VARIES 
27 
TABLE 2.3-1 (cont.) 
Blocks 
Word 
0 
1 
2 
3 
4 
such 
observed 
7 
17 
14 
1 
0 
binomial 
8.8 
15.9 
10.7 
3.2 
.4 
than 
observed 
7 
14 
5 
10 
3 
binomial 
4.4 
12.8 
13.9 
6.7 
1.2 
their 
observed 
0 
9 
19 
8 
3 
binomial 
1.9 
8.6 
14.5 
10.9 
3.1 
then 
observed 
31 
6 
2 
0 
0 
binomial 
29.9 
8.2 
.8 
.0 
.0 
there 
observed 
7 
12 
9 
10 
1 
binomial 
4.7 
13.1 
13.7 
6.3 
1.1 
things 
observed 
34 
5 
0 
0 
0 
binomial 
34.2 
4.5 
.2 
.0 
.0 
this 
observed 
1 
1 
5 
15 
17 
binomial 
.1 
1.1 
6.4 
16.2 
15.2 
unto 
observed 
39 
0 
0 
0 
0 
binomial 
39 
0 
0 
0 
0 
up 
observed 
31 
6 
2 
0 
0 
binomial 
29.9 
8.2 
.8 
.0 
.0 
upon 
observed 
3 
11 
14 
11 
0 
binomial 
3.3 
11.3 
14.4 
8.2 
1.8 
was 
observed 
18 
14 
6 
0 
1 
binomial 
16.8 
15.8 
5.5 
.9 
.0 
were 
observed 
19 
14 
5 
1 
0 
binomial 
18.5 
15.2 
4.7 
.6 
.0 
what 
observed 
15 
16 
6 
2 
0 
binomial 
14.4 
16.3 
6.9 
1.3 
.1 
when 
observed 
14 
18 
6 
1 
0 
binomial 
15.2 
16.2 
6.4 
1.1 
.1 
who 
observed 
10 
14 
9 
6 
0 
binomial 
8.3 
15.7 
11.1 
3.5 
.4 
will 
observed 
2 
7 
12 
10 
8 
binomial 
1.9 
8.6 
14.5 
10.9 
3.1 
would 
observed 
2 
4 
10 
12 
11 
binomial 
.5 
3.8 
11.4 
15.5 
7.9 
your 
observed 
39 
0 
0 
0 
0 
binomial 
39 
0 
0 
0 
0 

28 
WORDS AND THEIR DISTRIBUTIONS 
TABLE 2.3-2 
x2-DISTRIBUTION IN SETS OF FOUR BLOCKS 
o -27.32 
27.32-30.51 
30.51-33.00 
33.00-35.19 
35.19-37.34 
37.34-39.57 
39.57-42.03 
42.03-45.12 
45.12-49.56 
49.56-00 
Expected 
frequency 
5.1 
5.1 
5.1 
5.1 
5.1 
5.1 
5.1 
5.1 
5.1 
5.1 
Observed 
frequency 
2 
5 o 
4 
5 
3 
5 
6 
7 
12 
49 (Two words had 
no occurrences) 
[2.3 
to a modest fraction of the words, perhaps 20 per cent. To make a more serious 
assessment would require building a model for nonbinomiality, an unnecessary 
step at this point. Here it is enough to see that for most words the binomial 
distribution gives a fair picture, but that it is not entirely adequate. There is 
some dependence, at least for some of the words. Results for Madison are similar, 
and we do not present them. 
2.3B. Frequency of occurrence. A.lthough the study of distributions for 
blocks is instructive, our main studies depend upon the counts of occurrences of 
words. By treating all blocks in all papers alike, we can get some notion of the 
appropriate family of distributions. It is pleasllnt to work with these blocks 
which are approximately equal in length, becausethp varied length of the papers 
has been a nagging backache in most of our analyses. 
For our study of distributions of words we have chosen function words from 
the Miller-Newman-Friedman list (see Section 2.5A); we take the words with 
total frequencies between 45 and 180 in the 35,000 words of text that they 
counted. 
The distribution of the number of occurrences for the 51 words chosen is 
shown in Table 2.3-3. For example, in the Hamilton text there are 45 blocks in 
which this did not occur, 80 in which this occurred exactly once, and so on. 
The order of display in the table was determined by the count in the cell for zero 
occurrences. With the notable exceptions of her and his, the frequency dis-
tributions appear orderly to the eye because the counts in the cells for high 
numbers of occurrences shrink as the count in the zero cell increases. For more 
detailed study, we select a few representative and unrepresentative words from 
this list: an, from, any, may, upon, can, every, his, do, my. These 10 words 
exemplify for Hamilton all the distributions, and inch.lde an exceptional word his. 
Just as the binomial distribution set a base line in the study of incidence, the 
Poisson distribution corresponds to independence for counts of occurrences. 

2.3] 
HOW FREQUENCY OF USE VARIES 
29 
TABLE 2.3-3 
DISTRIBUTION OF OCCURRENCES FOn FUNCTION WORDS OF 
CLASSES 4 AND 5 
Hamilton 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
14 
this 
45 
80 
71 
39 
8 
4 
an* 
77 
89 
46 
21 
9 
4 
1 
or 
86 
69 
49 
21 
9 
6 
6 
1 
would 
90 
47 
29 
28 
22 
14 
5 
7 
4 
1 
from* 
93 
82 
51 
13 
5 
2 
1 
will 
105 
60 
31 
26 
12 
8 
3 
2 
its 
116 
82 
29 
11 
4 
3 
2 
their 
118 
66 
34 
16 
9 
2 
1 
1 
if 
118 
87 
31 
7 
3 
1 
any* 
125 
88 
26 
7 
1 
may* 
128 
67 
32 
14 
4 
1 
1 
upon* 
129 
83 
20 
9 
5 
1 
at 
129 
70 
42 
4 
2 
all 
132 
67 
32 
13 
2 
1 
there 
138 
75 
23 
8 
2 
1 
been 
138 
65 
24 
14 
3 
3 
than 
143 
66 
29 
6 
2 
1 
on 
145 
67 
27 
7 
1 
one 
149 
77 
16 
3 
2 
more 
152 
70 
15 
7 
2 
1 
can* 
157 
60 
20 
5 
2 
2 
1 
has 
157 
57 
20 
11 
2 
should 
161 
58 
26 
2 
who 
163 
53 
25 
3 
2 
1 
no 
167 
60 
11 
8 
1 
so 
170 
55 
19 
2 
1 
such 
173 
56 
13 
5 
must 
173 
49 
14 
9 
1 
1 
into 
183 
50 
12 
2 
only 
185 
54 
7 
1 
every* 
186 
46 
14 
1 
what 
188 
44 
11 
3 
1 
was 
192 
42 
7 
2 
3 
1 
were 
194 
44 
7 
1 
1 
when 
195 
40 
9 
3 
had 
200 
35 
8 
4 
some 
200 
38 
9 
even 
204 
39 
4 
his* 
192 
18 
17 
7 
3 
2 
4 
1 
2 
1 
our 
212 
23 
9 
1 
1 
1 
do* 
228 
16 
2 
1 
then 
230 
17 
up 
231 
14 
2 
also 
232 
15 
now 
234 
13 
things 
236 
11 
down 
240 
6 
1 
my* 
241 
6 
her 
241 
3 
1 
1 
1 
unto 
247 
your 
247 
(cont.) 

30 
WORDS AND THEIR DISTRIBUTIONS 
[2.3 
TABLE 2.3-3 (cont.) 
Madison 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
14 
on 
63 
80 
55 
32 
20 
8 
4 
this 
80 
83 
71 
21 
6 
1 
Jrom* 
90 
93 
42 
17 
8 
9 
3 
or 
103 
74 
51 
16 
12 
3 
2 
1 
an* 
122 
77 
40 
14 
8 
1 
was 
129 
55 
32 
16 
20 
5 
3 
2 
been 
132 
74 
35 
12 
4 
5 
at 
133 
86 
31 
8 
1 
3 
any * 
145 
90 
19 
8 
all 
146 
68 
28 
16 
4 
may* 
156 
63 
29 
8 
4 
1 
1 
than 
157 
84 
19 
2 
its 
158 
66 
29 
7 
2 
more 
158 
68 
30 
4 
2 
their 
159 
71 
17 
12 
2 
1 
no 
165 
71 
16 
7 
2 
1 
has 
167 
64 
19 
9 
1 
1 
1 
would 
167 
49 
31 
9 
1 
3 
2 
such 
170 
66 
22 
3 
1 
if 
171 
60 
22 
6 
3 
one 
172 
66 
15 
5 
3 
1 
will 
172 
55 
19 
8 
3 
3 
2 
into 
178 
71 
11 
2 
were 
179 
58 
18 
5 
1 
1 
only 
182 
65 
11 
4 
so 
182 
59 
17 
3 
1 
had 
185 
52 
16 
6 
2 
1 
her 
200 
26 
10 
9 
4 
4 
3 
2 
1 
2 
1 
must 
202 
47 
10 
2 
1 
some 
205 
53 
4 
there 
208 
47 
4 
2 
1 
evert 
209 
43 
4 
6 
can 
211 
44 
6 
1 
his* 
213 
21 
9 
11 
2 
2 
1 
2 
1 
should 
218 
32 
12 
then 
219 
34 
8 
1 
what 
220 
33 
8 
1 
even 
221 
38 
3 
also 
222 
36 
4 
who 
227 
28 
5 
2 
now 
236 
24 
2 
when 
236 
23 
2 
1 
do* 
244 
15 
2 
1 
our 
245 
13 
4 
down 
246 
15 
1 
up 
254 
8 
upon* 
254 
7 
1 
thin,gs 
254 
7 
1 
my* 
259 
3 
unto 
262 
your 
262 
Note: Words marked * are studied further in Table 2.3-4. 

2.3] 
HOW FREQUENCY OF USE VARIES 
31 
This distribution is a fair approximation for many kinds of counts, such as the 
number of radioactive particles striking a Geiger counter in a short interval, 
accidents at a given place for comparable times, wire worms per plot, blood 
corpuscles per square, the number of men on base when a home run is hit, and 
the number of vacancies in the United States Supreme Court in a given year. 
Usually, an indefinitely large number of occurrences is possible when the Poisson 
distribution is applied. But when the average number of occurrences is small, 
this limitation may not matter, as in the last two examples above. 
If we know the average number of occurrences, we know all about the Poisson 
distribution. For example, suppose that a word occurs, on the average, twice per 
block; then the Poisson distribution gives the following probability table of 
occurrences. 
Number of 
0 
1 
2 
3 
4 
5 
6 
occurrences 
or more 
Probability 
.135 
.271 
.271 
.180 
.090 
.036 
.017 
More generally, if A is the average number of occurrences, then the prob-
ability of exactly x occurrences for a given block is 
x = 0,1,2, ... , 
where xl = x(x -
1) .. ·2· 1 and e( =2.718 ... ) is the base of the naturalloga-
rithms. We can estimate A from the average number of occurrences per block for 
a given word, then we can compute the probabilities from tables (for example, 
Molina, 1942), and multiply the results by the number of blocks to make direct 
comparisons of frequencies. 
In Table 2.3-4 we carry out this program for 10 words which typify the 
distributions shown in Table 2.3-3. (The third distribution is the fitted negative 
binomial discussed later.) 
While the fitted Poisson and the observed distribution agree fairly well for 
some of the words, for example, an, from, any, upon, can, every, do, my, even the 
most motherly eye can scarcely make twins of the distributions for mayor 
those for his. Thus, we are forced to revise our notions of the form of the dis-
tribution. The Poisson family is not rich enough to describe the variation in 
the frequencies. 
Like Ulysses, statisticians are never at a loss, and naturally we have several 
other distributions as candidates. Some of these, called contagious distributions, 
are designed to produce clumping of occurrences. While we have explored some 
of these, we found few that are mathematically manageable. 
We call the parts of a frequency distribution far from the mean "the tails of 
the distribution." In the Poisson distribution, the right-hand tail is long but 

32 
WORDS AND THEIR DISTRIBUTIONS 
[2.3 
TABLE 2.3-4 
OBSERVED AND FITTED POISSON AND N EGA'l'IVE BINOMIAL 
DISTRIBUTIONS· FOR SELECTED W ORns 
Hamilton 
Occurrences 
0 
1 
2 
3 
4 
5 
6 
7 or 
more 
observed 
77 
89 
46 
21 
9 
4 
1 
an 
Poisson 
71.6 
88.6 
54.9 
22.7 
7.0 
1.7 
.4 
.1 
*N.B. (6.41) 
81.0 
82.7 
49.2 
22.0 
8.2 
2.7 
1.0 
.2 
observed 
93 
82 
51 
13 
5 
2 
1 
from Poisson 
90.5 
90.9 
45.6 
15.3 
3.8 
.8 
.1 
N.B. (7) 
93.2 
84.7 
44.2 
17.2 
5.6 
1.6 
.4 
.1 
observed 
125 
88 
26 
7 
0 
1 
any Poisson 
126.3 
84.6 
28.5 
6.4 
1.1 
.2 
N.B.(oo) 
same as Poisson 
observed 
128 
67 
32 
14 
4 
1 
1 
may Poisson 
109.9 
88.9 
36.0 
9.7 
2.0 
.3 
.1 
N.B. (1.64) 128.2 
69.4 
30.1 
12.1 
4.6 
1.7 
.6 
.3 
observed 
129 
83 
20 
9 
5 
1 
upon Poisson 
121.6 
86.1 
30.6 
7.3 
1.3 
.2 
N.B. (4.20) 131.1 
77.1 
27.9 
8.2 
2.1 
.5 
.1 
observed 
157 
60 
20 
5 
2 
2 
1 
can Poisson 
141.0 
78.9 
22.3 
4.2 
.6 
.1 
.0 
N.B. (1.06) 157.3 
57.6 
20.7 
7.3 
2.6 
.9 
.3 
.3 
observed 
186 
46 
14 
1 
every Poisson 
180.9 
56.2 
8.8 
.9 
.1 
N.B. (1.58) 185.5 
48.1 
10.4 
2.2 
.3 
.5t 
observed 
192 
18 
17 
7 
3 
2 
4 
4 
his 
Poisson 
131.7 
82.7 
26.2 
5.5 
.9 
.1 
.0 
.0 
N.B. (.154) 192.2 
23.8 
11.0 
6.4 
4.0 
2.7 
1.9 
5.0 
observed 
228 
16 
2 
1 
do 
Poisson 
225.7 
20.3 
.9 
.0 
N.B. (.21) 
228.7 
14.8 
2.7 
.7 
observed 
241 
6 
my 
Poisson 
241.1 
5.8 
.1 
N.B.(oo) 
same as Poisson 
Note: Estimated values of K follow N.B. in parentheses; an * means K was estimated 
from the variance. A t means sum of remaining frequencies. 

2.3] 
HOW FREQUENCY OF USE VARIES 
33 
TABLE 2.3-4 (cont.) 
OBSERVED AND FITTED POISSON AND NEGATIVE BINOMIAL 
DISTRIBUTIONS FOR SELECTED WORDS 
Madison 
Occurrences 
0 
1 
2 
3 
4 
5 
6 
7 or 
more 
observed 
122 
77 
40 
14 
8 
0 
1 
an 
Poisson 
106.1 
95.9 
43.4 
13.1 
2.9 
.5 
.1 
*N.B. (2.45) 121.8 
80.2 
37.2 
14.9 
5.4 
1.9 
.6 
observed 
90 
93 
42 
17 
8 
9 
3 
from Poisson 
76.6 
94.3 
57.9 
23.7 
7.3 
1.8 
.4 
N.B. (2.62) 
95.9 
79.9 
45.8 
22.5 
10.0 
4.2 
1.7 
2.0 
observed 
145 
90 
19 
8 
any Poisson 
146.8 
84.9 
24.7 
4.8 
.7 
.1 
N.B. (00) 
same as PoisSOIl 
observed 
156 
63 
29 
8 
4 
1 
1 
may Poisson 
136.1 
88.9 
29.2 
6.4 
1.1 
.2 
.0 
N.B. (1.15) 157.7 
63.9 
25.2 
9.4 
3.7 
1.3 
.5 
.3 
observed 
254 
7 
1 
upon Poisson 
253.2 
8.6 
.2 
N.B. (.11) 
254.4 
6.6 
.9 
.1 
observed 
211 
44 
6 
1 
can Poisson 
209.2 
47.1 
5.3 
.4 
N.B. (2.6) 
210.9 
43.9 
6.6 
.6 
observed 
209 
43 
4 
6 
every Poisson 
201.4 
53.0 
7.0 
.6 
N.B. (,695) 209.6 
39.8 
9.2 
2.3 
l.lt 
observed 
213 
21 
9 
11 
2 
2 
1 
3 
his 
Poisson 
167.3 
74.9 
17.0 
2.6 
.3 
.0 
.0 
.0 
N.B. (.149) 222.7 
14.7 
8.4 
5.2 
3.4 
2.4 
1.3 
3.9 
observed 
244 
15 
2 
1 
do 
Poisson 
240.9 
20.2 
.9 
.0 
N.B. (.16) 
245.0 
13.5 
2.7 
.7 
observed 
259 
3 
my 
Poisson 
259.0 
3.0 
N.B. (00) 
same as Poisson 
Note: Estimated values of K follow N.B. in parentheses; an * means K was estimated 
from the variance. A t means sum of remaining frequencies. 

34 
WORDS AND THEIR DISTRIBUTIONS 
[2.3 
TABLE 2.3-5 
A COMPARISON OF NEGATIVE BINOMIAL AND 
POISSON DISTRIBUTIONS 
Negative 
Poisson 
Count 
binomial 
x 
probability 
probability 
~ = K = 2 
~ = 2 
0 
.250 
.135 
1 
.250 
.271 
2 
.188 
.271 
3 
.125 
.180 
4 
.078 
.090 
5 
.047 
.036 
6 
.027 
.012 
7 
.016 
.003 
8 
.009 
.001 
9 and beyond 
.010 
.000+ 
thin. What we need to describe words like may and his is a family that offers a 
fatter tail than the Poisson. One such family is the negative binomial. A variety 
of mechanisms can lead to the negative binomial, but one which we present 
informally here (and formally in Section 4.1) is a two-stage sampling operation. 
Suppose that, before writing a block, the author chooses a rate for that block, 
and then behaves according to the Poisson theory for that rate and block. For 
the next block he chooses another rate and so on. If he chooses his rates random-
ly in a way that we will explain later, he winds up with the negative binomial. 
Without being quite so fanciful, we can say that the negative binomial offers one 
way to introduce a greater block-to-block variation of rates than the Poisson 
can indicate. 
THEORY. As in the Poisson distribution, let ~ be the mean frequency of 
use for blocks of fixed size, and let K be another number (K > 0); then for 
the negative binomial the probability of exactly x occurrences is 
P(x) = K(K + 1}",: .. (K + x-I) (~K_)X ( 
1 
)K, 
1 . 2· .. x 
1 + X!K 
1 + ~/K 
x = 0, 1,2, .... 
When x = 0, the coefficient on the right is to be taken as unity. 
To illustrate the relation with the Poisson distribution, let K = 2, ~ = 2; 
then the formula reduces to 
P(x) = (x + 1)/2"'+2, 
x = 0, 1,2, ... , 
and we display the numerical result in Table 2.3-5. 

2.4] 
CORRELATIONS BETWEEN RATES FOR DIFFERENT WORDS 
35 
The main point of Table 2.3-5 is that for large values of x (beyond 4), the 
negative binomial has substantially larger probabilities than the Poisson with 
the same mean. It can be shown that if K is very large, the negative binomial 
closely approximates the Poisson and that when K is small, it does not. We 
could take I/K as a measure of non-Poissonness, but we actually use AlK for 
reasons we give in Chapter 4. 
We estimate A from the average of the observed distribution, and Kby more 
grisly devices (from mode or variance) and then fit the distributions. Some are 
displayed in Table 2.3-4. We find it heartwarming to see the improvement in 
agreement, especially for may and his for Hamilton, and an, from, may, and his 
for Madison when the Poisson is replaced by the negative binomial. 
2.4. 
CORRELATIONS BETWEEN RATES 
FOR DIFFERENT WORDS 
Although we said earlier that we would set aside the fine structure of diction, 
we must attend to the gross relations among words. In our later work we largely 
behave as if words were independent, and then inquire what steps to take to 
correct for the failure of that assumption. How nearly independent or uncorrel-
ated are rates of use of words of the kind we ultimately employ for discrim-
inations? 
If we make repeated counts of the makes of 100 successive automobiles seen 
on the road, we will find that when the count of Ford's is high, that of Chevrolet's 
is lower on the average, and vice versa. The restraint that the total counts be 
of fixed size creates a negative correlation between the counts in different 
categories. This correlation * reaches its extreme for two categories, where, for 
example, the correlation between the number of sons and the number of daugh-
ters in families of size five is exactly -1. That is, if you know the number of 
sons you can state exactly the number of daughters. This negative multinomial 
correlation is somewhat artificial and arises even if the category that occurs 
on one trial is utterly independent of that on another. 
Given such independence of trials, we can compute the theoretical corre-
lation coefficient from the true proportions belonging to the various categories. 
Let Pi (= 1 -
qi), i = 1, ... , k be the proportion associated with the ith 
category. Then the correlation between the observed counts in categories i andj 
is 
* A coefficient of correlation is a way of measuring the degree of relation between 
two variables, or more properly, the degree of linear relation. When the coefficient is 
+ 1, the relation between the two is perfect-indeed a straight-line graph relates them. 
The same is true when the coefficient is -1, except th~t then when one variable is high, 
the other is low. Independence implies zero correlation, and often small correlations 
imply lack of predictability of the value of one variable from the other. 

36 
WORDS AND THEIR DISTRIBUTIONS 
[2.4 
TABLE 2.4-1 
OBSERVED DISTRIBUTION OF CORRELATION COEFFICIENTS 
Midpoint of 
Frequency for 
Frequency for 
class interval 
Hamilton 
Madison 
-.425 
1 
1 
-.375 
3 
1 
-.325 
3 
2 
-.275 
7 
9 
-.225 
12 
12 
-.175 
32 
31 
-.125 
54 
70 
-.075 
56 
72 
-.025 
66 
56 
.025 
39 
53 
.075 
34 
46 
.125 
34 
30 
.175 
22 
11 
.225 
19 
20 
.275 
9 
8 
.325 
3 
5 
.375 
7 
1 
.425 
3 
3 
.475 
1 
3 
.525 
0 
1 
.575 
1 
0 
Total 
406 
435 
l' 
-.0026 
-.0120 
S.D. 
.1547 
.1450 
When Pi = Pi = qi = qj = !, Pij = -1 as mentioned earlier. Among the 
words we use, of and to have the largest proportions, about .06 and .04, which 
would create a correlation of about - .05 between them. Other correlations 
from this multinomial source are considerably smaller. Since correlations from 
the multinomial source seem negligible, the correlations that we attend to are 
those created by actudlaffiliations or substitutabilities between words. To assess 
the magnitUdes of these correlations, the Pearson correlation coefficient was 
computed for rates of al1 pairs of the 30 words used in the main study of Chapter 
3 for each author, using 48 Hamilton papers and 50 Madison papers as the basis 
for the rates. Miles Davis made the calculations and Ivor Francis provided an 
analysis of the results. 
The frequency distributions for 406 Hamilton and 435 Madison correlations 
are displayed in Table 2.4-1 (29 correlations are missing from Hamilton because 
of his failure ever to use one of the words in this text). The mean correlation, 

2.5] 
POOLS OF WORDS 
37 
'f, for both authors is slightly negative (about as expected), and for each the 
observed standard deviation is about .15, which is close to l/vn ::::< -+ ::::< .14, 
the standard deviation that a theoretical correlation of zero would imply. 
Thus most of the true correlations are probably slightly negative but near zero, 
and the deviations that we observe are mainly sampling variation. For very 
low-frequency words, the correlations are likely not very meaningful. For words 
that have an average rate of over lloo for each author, the average correlations 
for the two authors that exceed .15 in absolute value (about 1.5 standard de-
viations from the average) are: 
an-there, 
.20, 
an-this, 
.19, 
by-this, 
.24, 
of-to, 
-.39, 
on-to, -.19. 
While some interpretations suggest themselves, the main point is that these 
numbers are quite modest. Later, in Sections 3.7 and 4.7, we use correlations 
to adjust our appraisals. 
2.5. POOLS OF WORDS 
The reader can abandon the notion that our study is a unified whole, conceived 
carefully in advance, and executed as planned. Instead, we have made many 
studies, some contributing to all the others, some utter failures, abandoned 
early on. Most dealt with the search for words that especially help us to dis-
criminate. Here we summarize those studies that contributed the words finally 
used. 
Three pools of words produced four sets of possible discriminators, and Table 
2.5-1 may help the reader prepare for their discussion. 
TABLE 2.5-1 
POOLS OF WORDS AND THEIR OUTPUTS 
Miller-N ewman-Friedman 
list of 363 function words 
produced 
70 "unselected" 
high-frequency words and 
20 random low-frequency 
words 
Screening study based 
on all different words 
(about 3000) in 
11 Federalist papers 
produced 
28 selected words 
Index with frequencies 
based on some Hamilton 
Federalist papers outside 
the screening study and 
some Madison 
non-Federalist writings 
produced 
103 selected words 
~--------------------------~~--------------------------
which total to 165 different words 

38 
WORDS AND THEIR DISTRIBUTIONS 
[2.5 
TABLE 2.5-2 
FUNCTION WORDS AND THEIR CODE NUMBERS 
1 a 
15 do 
29 is 
43 or 
57 this 
2 all 
16 down 
30 it 
44 our 
58 to 
3 also 
17 even 
31 its 
45 shall 
59 up 
4 an 
18 every 
32 may 
46 should 
60 upon 
5 and 
19 for 
33 more 
47 so 
61 
was 
6 any 
20 from 
34 must 
48 some 
62 were 
7 are 
21 had 
35 my 
49 such 
63 what 
8 as 
22 has 
36 no 
50 than 
64 when 
9 at 
23 have 
37 not 
51 that 
65 which 
10 
be 
24 her 
38 now 
52 the 
66 who 
11 
been 
25 his 
39 of 
53 their 
67 will 
12 but 
26 if 
40 on 
54 then 
68 with 
13 by 
27 in 
41 
one 
55 there 
69 would 
14 can 
28 into 
42 only 
56 things 
70 your 
TABLE 2.5-3 
ADDITIONAL WORDS AND THEIR CODE NUMBERS 
*71 affect + ed 
*95 join + ed 
*72 again 
*96 language 
*73 although 
97 most 
74 among 
98 nor 
75 another 
*99 offensive 
76. because 
100 often 
77 between 
*101 
pass + es + ed + ing 
78 both 
102 perhaps 
*79 city + cities 
*103 rapid 
*80 commonly 
104 same 
*81 
consequently 
105 second 
*82 considerable + ly 
106 still 
. *83 contribute 
107 those 
*84 defensive 
*108 throughout 
*85 destruction 
109 under 
86 did 
*110 vigor + ous 
*87 direction 
*111 violate + s + d + ing 
*88 disgracing 
*112 violence 
89 either 
*113 voice 
*90 enough (and in sample of 20) 
114 where 
*91 fortune + s 
115 whether 
*92 function + s 
*116 while 
93 himself 
*117 whilst 
*94 innovation + s 
Note: An * means the word emerged from the screening study; the rest 
came from a random sample of low-frequency function words. 

2.5] 
POOLS OF WORDS 
39 
2.5A. The function words. We obtained a pool of words by using a list of 
"function" words, made up earlier, for a different purpose, by Miller, Newman, 
and Friedman (1958). Their list of 363 words is an expansion of a much smaller 
list compiled by Fries (1952). Their list gives frequency counts based upon 
35,000 words of text taken from the King James Bible, William James, and 
The Atlantic (1957). Though not directly relevant to the Federalist period, the 
counts have been helpful, but many words are so rare that they are of no value 
for our study. Function words, as opposed to "content" words, include, in ad-
dition to the filler words already mentioned, ordinals, cardinals, and some others. 
The list is objective with respect to the Federalist problem; thus it relieves us of a 
large onus of choice and plays a central role in the present work. 
From the Miller-Newman-Friedman list, we eliminated some biblical words, 
cardinals larger than one, and personal pronouns except in the possessive form. 
The main point is that the selection of the remaining words has nothing to do 
with their ability to decide authorship in our problem. In the present work, the 
70 most frequent words in the Miller-Newman-Friedman text have been used. 
This is the first set of Table 2.5-1. The list, shown in Table 2.5-2, is not totally 
satisfactory because certain types of function words are potentially dangerous. 
Personal pronouns and auxiliary verbs, especially with respect to mood and 
tense, are likely to be related to external details, and inference from them is 
difficult. 
We added 20 words for purposes of estimation: a random sample of 10 from 
words with counts 11 to 21 and another 10 from words with counts 22 to 44 in 
the Miller-Newman-Friedman list. These form a set of low-frequency func-
tion words unselected for ability to decide authorship. They are shown in 
Table 2.5-3. 
2.5B. Initial screening study. The study has been described sketchily in 
Chapter 1, and some results of it are given there. Here we give the rules followed 
and the words selected. During this screening, we thought that contexts were 
important, and we eliminated words by the hundreds for their potential depend-
ence on context. Still, our attitude toward this danger was not as serious and 
quantitative as it later became. Consequently, we believe that our studies, 
using the words selected from this pool, still suffer more from contexts than they 
would have with more care. 
Our plan was to explore low-frequency words separately from middle- and high-
frequency ones (definitions were vague). The present description applies to 
that low-frequency study. 
General plan. Papers were studied in waves. A wave consisted of about 10 
papers, nearly equally divided between the authors. Words were scored on a wave 
according to the number of Hamilton and Madison papers in which the word 
occurred. Thus, (3,2) means that the word occurred in three Hamilton and two 
Madison papers in the set. If a word scored (5,0) or (0,5) we would be most 

40 
WORDS AND THEIR DISTRIBUTIONS 
[2.5 
TABLE 2.5-4 
SURVIVORS OF THE Low-FREQUENCY SCREENING STUDY 
Hamilton markers 
Madison markers 
(H, M) 
(H, M) 
(14, 0) 
enough 
(0, 13) whilst 
(10, 0) 
while 
(2, 13) consequentl y 
(8, 0) destruction 
(0, 8) 
although 
(8, 0) 
offensive 
(1, 9) violate + s + d + ing 
(10,1) affect + ed 
(3, 12) 
pass + es + ed + ing 
(9, 1) commonly 
(1, 8) 
voice 
(9, 1) vigor + ous 
(1, 8) 
throughout 
(6, 0) city + cities 
(2, 10) language 
(6, 0) 
contribute 
(0, 5) fortune + s 
(6, 0) 
defensive 
(0, 5) join + ed 
(8, 1) direction 
(0, 5) violence 
(5, 0) 
disgracing 
(1, 7) 
again 
(5, 0) rapid 
(1, 7) function + s 
(13, 4) 
considerable + ly 
(1, 7) innovation + s 
Note: Incidences in Hamilton and Madison papers (H, M). 
encouraged, but (3,2) or (3,4) would be discouraging. Strong marker words like 
while and whilst were desired. 
At the end of a wave, a word was retained. or discarded on the basis of the 
cumulative score to date. In all, there were four waves (including the Exterior 
set) as described in Chapter l. 
Details of study. More specifically, we began with 6 Hamilton and 5 Madison 
Federalists, and we retained words that scored (2,0), (0,2), (3,0), (0,3), (4,0), 
(0,4), (5,0), (0,5), (6,0), (4,1), (1,4), (6,1), (5,1), (1,5), (6,2), (2,5), and discarded 
all others. About 3000 different words were in the original pool of 11 papers, 
and 305 words survived the first screening (wave A). 
Because Madison had so few known Federalists, we feared to use them up 
early, and so we obtained as our next wave an Exterior set of 10 papers. The 
305 words that survived the wave-A screening were scored in the same manner 
in this Exterior set, and the cumulative score based on these first 21 papers was 
obtained for each word. For example, argument, pooled with its plural, scored 
(5,1) on wave A, and (1,3) on the Exterior set for a total of (6,4), but upon 
scored (6,2) and (5,1) for an (11,3) total. The latter word was earmarked for 
special study because of its high Hamilton rate. We did not eliminate any words 
at this point, but proceeded to wave B. 
Wave B had 10 papers (6 Hamilton and 4 Madison). The cumulative score 
for each word described above was further augmented by adding on its score 
based on these ten papers. The cumulative score is now a number pair (x,y), 
where x can vary from ° through 17, and y from 0 through 14. 

2.5] 
POOLS OF WORDS 
41 
TABLE 2.5-5 
WORDS THAT SURVIVED UNTIL WAVE C, BUT FAILED THERE 
Score before 
Wave C 
Final 
Hamilton markers 
wave C 
score 
score 
criterion 
(4, 0) 
(0, 1) 
(4, 1) 
finance + s 
(4,0) 
(1, 1) 
(5, 1) 
hostility 
(7, 1) 
(0, 1) 
(7, 2) 
intimate + d 
(4,0) 
(1, 1) 
(5, 1) 
occasional 
(4, 0) 
(0, 0) 
(4,0) 
pervert 
(4,0) 
(0, 1) 
(4, 1) 
probability 
(6, 1) 
(4, 2) 
(10, 3) 
utility 
(7, 1) 
(0, 1) 
(7, 2) 
utmost 
(4, 0) 
(0, 0) 
(4,0) 
wide 
(4, 0) 
(0,0) 
(4,0) 
Madison markers 
commensurate 
(0,4) 
(0,0) 
(0,4) 
death 
(0,4) 
(0, 0) 
(0, 4) 
ensues + ing 
(0,4) 
(0,0) 
(0,4) 
expression 
(1, 6) 
(2, 1) 
(3, 7) 
face 
(0,4) 
(1, 1) 
(1, 5) 
fundamental 
(0,5) 
(2,0) 
(2, 5) 
probably 
(2, 8) 
(2, 3) 
(4, 11) 
work + s 
(1, 8) 
(1, 0) 
(2, 8) 
At this point, the index z = (x -
y)2/(x + y) was introduced as a measure 
of the discrimination achieved by a word. We decided to retain only those 
words for which z ~ 3.6. For Hamilton markers, this means that (x,O) words 
are kept if x is 4 or mote, (x,l) are kept if x is 6 or more, (x,2) are kept if x is 8 
or more, (x,3) are kept if x is 10 or more. Similar cutoffs hold for y in Madison 
markers, with scores (O,y), (l,y), (2,y), and (3,y). 
These machinations left us 46 words to test on wave C (6 Hamilton and 5 
Madison papers). We retained those words with final z-values of at least 4.5. 
This is the third set of words mentioned in Table 2.5-1. These 28 words are shown 
in Table 2.5-3 together with their code numbers, and also in Table 2.5-4, ordered 
according to their final z-values. That there are equal numbers of Hamilton and 
Madison markers is quite accidental. 
Words that lost out in the final screening ·are shown in Table 2.5-5, together 
with their scores. For us, it was a sad personal blow that probability and probably 
failed this final test. 
Someone may well ask at this point whether the whole enterprise is a boon-
doggle. Is it not reasonable that none of the words has predictive power and 
that we are merely keeping a chance selection of words? Possibly all these words 

42 
WORDS AND THEIR DISTRIBUTIONS 
[2.5 
are used with approximately equal incidences by the two authors, and we have 
chosen only those extreme words that would occur by chance in any such large 
collection. To answer such an inquiry, suppose a word has an equal chance for 
incidence for both authors. There are 6 Hamilton and 5 Madison papers in 
wave C. Then in the (x,y) pairs for these 11 papers of wave C, the quantities 
(xI6) -
(yI5), or equivalently 5x -
6y, would be distributed about zero with 
no dependence on the forecasting power of the information in the previous waves. 
To test this, we made two frequency distributions for 5x -
6y, one for 24 
Hamilton markers, the other for 22 Madison markers entering wave C, with 
the following results. 
Hamilton 
Madison 
Center of class interval: 5x -
6y 
-25 -20 -15 -10 -5 
1 
2 
3 
2 
4 
5 
o 
5 
6 
5 
10 
15 
20 
25 
6 
2 
3 
1 
4 
1 
1 
Total 
24 
22 
Visual inspection shows that the markers are discriminating. We conclude then 
that the discriminators have some power. 
2.5C. Word index with frequencies. Late in our work we were able to 
construct an index of Hamilton and Madison words covering 18 Hamilton 
Federalists and 19 Madison papers. (Hamilton: 16, 23, 25, 27-35, 60, 61, 68, 69, 
75,76; Madison: Helvidius 4,5, Ml through M9, Nl through N8, Neutral Trade 
201-209.) In the index, five counts were given for each word: total frequency 
for the two authors, frequency for each author, and incidence in papers for each. 
To obtain this large list of words became feasible only when the alphabetical 
lists and counts from many papers could be merged into a single listing. This 
merging was accomplished on papers totaling about 70,000 words, divided 
nearly equally between Hamilton and Madison. The merging could be done 
conveniently only for those papers whose initial alphabetization was available on 
a tape output. The only Madison papers included are external to the Federalist, 
while the Hamilton papers were Federalist papers, but not the wave-A, -B, or -C 
papers used in the screening study. 
What was available, then, was an index restricted to words actually used by 
Hamilton and Madison in a substantial body of writings. The listings contained 
about 6700 different words, where varying forms (for example, plurals, verb 
conjugates) account for much of the total. 
We analyzed the index to find out whether there were any outstanding words 
that we had missed in the screening study. Naturally the screening study can 
easily miss a fine discriminator that does not do well on the starting set (wave A). 
On binomial probability paper (Mosteller and Tukey, 1949) we set three stan-
dard-deviation confidence limits on the Hamilton-Madison split in frequencies. 

2.6] 
WORD COUNTS AND THEIR ACCURACIES 
43 
TABLE 2.5-6 
NEW WORDS FROM THE INDEX STUDY 
AND THEIH CODE NUMBERS 
118 about 
142 intrust + s + ed + ing 
119 according 
143 kind 
120 adversaries 
144 large 
121 
after 
145 likely 
122 aid 
146 matter + s 
123 always 
147 moreover 
124 apt 
148 necessary 
125 asserted 
149 necessity + ies 
126 before 
150 others 
127 being 
151 
particularly 
128 better 
152 principle 
129 care 
153 probability 
130 choice 
154 proper 
131 
common 
155 propriety 
132 danger 
156 provision + s 
133 decide + s + d + ing 
157 requisite 
134 degree 
158 substance 
135 during 
159 they 
136 expence + s 
160 though 
137 expense + s 
161 
truth + s 
138 
~xtent 
162 us 
139 follow + s + ed + ing 
163 usage + s 
140 
I 
164 we 
141 imagine + s + d + ing 
165 work + s 
If a word fell outside these limits, it was noted as a possible discriminator. The 
technique ignores the negative binomiality of words. The resulting list had about 
240 words, and a great many of them were quite contextual. After eliminating 
words that we regarded as contextual, we were left with 103 words. Forty-eight 
were new, and these are listed in Table 2.5-6, together with their code numbers. 
These new words are used in the main Bayesian study (Chapter 3) and in the 
robust Bayesian study (Chapter 6). 
2.6. 
WORD COUNTS AND THEIR ACCURACIES 
Our counts have been done partly on high-speed computers (machine counts) 
and partly by hand. In this section we describe that work. 
Machine counts. Certain Federalist papers were typed on punched cards by 
personnel at the Littauer Statistical Center of Harvard University. A program 
for the counting of frequencies of single words was prepared by Wayne Wiitanen, 
C. Harvey Willson, and Robert. A. Hoodes, under the direction of Albert E. Bea-

44 
WORDS AND THEIR DISTRIBUTIONS 
[2.6 
THE SUPPOSITION THAT EACH CONFEDERACY INTO WHICH THE STATES WOULD 
13 
037 
BE LIKELY TO BE. DIVIDED WOULD REQUIRE A GOVERNMENT NOT LESS 
13 
038 
COMPREHENSIVE THAN THE ONE PROPOSED, WILL BE STRENGTHENED BY ANOTHER 
13 
039 
SUPPOSITION, MORE PROBABLE THAN THAT WHICH PRESENTS US WITH THREE 
13 
040 
CONFEDERACIES AS THE ALTERNATIVE TO A GENERAL UNION. 
IF WE ATTEND 
13 
041 
LESS 
LIBERTY 
LIGHT 
LIKE 
5 (3, 60, 63), (19, 43, 46), (38, 56, 59), (71, 22, 25), (84, 5, 8), 
1 
(85, 14, 20), 
1 
(74, 59, 63), 
1 
(31, 26, 29), 
LIKELIHOOD 
1 
(67, 62, 71), 
LIKELY 
1 
(38, 4, 
9), 
LINKS 
1 
(46, 26, 30), 
FIG. 2.6-1. Portion of machine output for Hamilton's Federalist No. 13. 
ton. The first run was proofed and corrected. Another run on each paper was 
made and a second proofing done. This corrected run is discussed here. 
For each separate paper in the final run, the total frequency of every word 
was tabulated separately, but a count for the entire paper was summed on hand 
machines. 
Figure 2.6-1 illustrates parts of a page of machine output for Hamilton's 
paper No. 13. Each line at the top corresponds to a single card input. The text 
also illustrates the style of writing. 
Following each line of text are the paper number and the line number (essen-
tially punched card number). After the words are alphabetized they are counted, 
and then each appearance is listed as a number triple (x, y, z), where x is the line 
number and y and z are the position of first and last letters of the word on that 
line. For example, likely appears in line 038 from characters 4 through 9. 
Hand counts. Certain Federalist papers were typed on roll paper (adding-
machine paper, one word to a line) and proofed. At the same time, the page 
number and line number were written opposite each word (for some but not 
all papers). The words on the roll paper were then cut and sorted into alpha-
betical order. (During this operation a deep breath created a storm of confetti 
and a permanent enemy.) The count of each word was tabulated by hand and 
typed or written on a master sheet. A complete count for the total number of 
words in the particular paper was also tabulated and recorded. This total count 
was later checked and rechecked (but only a few words were recounted). 
The disputed papers were counted both by hand and by machine. This gave 
an opportunity to compare the results. A reconciliation of the hand counts and 
of the machine counts for the disputed papers was carried out by Theodore S. 
Ingalls, Ralph A. Stewart, Jr., and Cleo Youtz. It has been reported in Mosteller 
and Wallace (1962). 

2.7] 
CONCLUDING REMARKS 
45 
2.7. CONCLUDING REMARKS 
We close with some remarks about other sorts of variables and considerations 
for similar studies. 
It would be ridiculous to claim that words are the only or even the best 
choice for a set of variables. But for hope of success in a difficult discrimination 
problem, requirements like the following are needed, though no rules can make 
useless variables discriminate. The pool of potential discriminators should be 
large enough, say, 50 to 1000, to offer a good chance of success. For some kinds 
of analysis, the pool needs to be delimited by systematic rules so that proper 
allowance can be made for selection. For when many variables are involved, 
the ones that appear to be best in a sample may not actually be the best; they 
usually do not work so well later as they appeared to initially. These deterior-
ations are called selection and regression effects. Enough data are needed to allow 
a strong grip on the distribution theory for the variables used. Distributional 
assumptions on each variable should be acceptable to those of divergent views 
on the question of authorship. Finally, measurement of the variables on the 
known and disputed papers should be objective and routine, and, preferably, 
mechanizable. 
Phrases are attractive, and, were it possible to choose a moderately large list, 
they could be handled much as words. The measurement problem is more 
difficult to carry out, and the available data would be much less adequate. 
Choosing a pool of phrases is not easy. Possibly, the index could be used to help 
build a small study of phrases, though the problems are substantial. 
This discussion is not meant to discourage others from using, in such prob-
lems, variables that they feel have deeper or more appropriate significance than 
ours. Some problems of authorship can take advantage of facts about authors, 
or of specialized knowledge. Indeed, we are disappointed not to have found a 
way to use the facts of Hamilton's and Madison's differing childhoods. For 
example, Hamilton spoke French fluently as a youth, but we are not clear about 
Madison. A scholar who is well equipped in the study of the regional languages 
of colonial times might know how to look for, and might find, some striking 
variables. Similarly, a psychiatrist, clinical psychologist, or poet might discover 
discrimination in the imagery of the language. 

CHAPTER 3 
The Main Study 
In this chapter, we present the methods and results of the main study. To 
simplify the exposition, we describe the methods only for the simpler model 
based on Poisson distributions of word frequencies but we give the numerical 
results for both the Poisson and the negative binomial. The technical develop-
ment for the full model based on negative binomial distributions is postponed 
to Chapter 4, along with detailed mathematical treatments of special problems. 
Section 3.1 begins with an overview of the methods of the main study, and 
describes the principal sources of difficulties and how we go about meeting them. 
Starting in Section 3.lA, we develop the procedures for assessing the evidence 
on authorship of a disputed paper; first, we derive Bayes' theorem in its simplest 
form for assessing the evidence of a single word, then use it to combine evi-
dence from several words and to guide in the selection of' words. Second, we 
treat the relation of the statistical evidence and prior evidence on authorship. 
With the completion of this much of the development in Section 3.IC, the 
reader without interest in the statistical methodology can turn to the results 
for The Federalist in Section 3.4, although a glance at the final selection of words 
described in Section 3.3 would be desirable. 
With Section 3.l!), we begin to treat one of the central methodological issues 
-unknown parameters. Here we indicate the final method, leaving most of the 
intermediate steps for Section 3.2. We end Section 3.1 with a formal outline of 
the logic of our two kinds of uses of Bayes' theorem. 
Section 3.2 treats the problem of getting posterior distributions for the 
parameters, with the major effort spent on the nature and source of the prior 
distribution. We introduce here the notion of underlying constants that specify 
which prior distribution is being used. We complete the presentation of the 
method in Section 3.3 with the choice of the 30 words that are used in the final 
analysis of The Federalist. 
We present the results, in the form of log odds of authorship for the combi-
nation of all words, in Section 3.4, first for the papers of known authorship to 
provide a check on the method, then for the disputed Federalist papers. In 
Section 3.5, the log odds are broken down into the contributions from single 
words, and from groups of words. Section 3.6 presents an additional check, 
resorting to some papers by Hamilton not previously used in the study. 
46 

THE MAIN STUDY 
47 
Section 3.7 serves several important functions. It describes the general mag-
nitudes of adjustments in the final log odds needed because of imperfect as sump-
tionsand approximations. The range of adjusted log odds is given in Section 
3.7E. In Section 3.7F, we discuss the question, "Can the odds be believed?," 
both within the mathematical model and with respect to difficulties external to 
the model. 
Probability has several interpretations. We must mention two: probability 
as degree of belief, and probability as relative frequency. In the relative fre-
quency interpretation, probability may be applied only to events that can be 
repeated over and over under much the same conditions. The degree-of-belief 
interpretation is more widely applicable, but specifying the probabilities is often 
difficult in the absence of conditions of symmetry or long-run relative frequencies. 
When the two interpretations are simultaneously applicable, the same numer-
ical values would normally be assigned in each interpretation. 
Much of what we do in the main study concerns propositions that would not 
ordinarily be assigned frequency probabilities; for example, the proposition 
"Hamilton wrote paper No. 52." To be able to use a concept of probability for 
such a proposition greatly facilitates the analysis of data and the interpretation 
of evidence, because we can use techniques derived from Bayes' theorem, which 
is discussed below. To achieve a notion of probability that can also be given a 
frequency interpretation is highly desirable, for it strengthens the interpretation 
and widens the acceptability of the results. In ways that are explained much 
later, our work comes close to merging the two interpretations. 
Right here, we need to discuss these ideas a bit further. In the course of this 
discussion, Bayes' theorem is often mentioned. One need only know that it is 
a mathematical device that combines evidence from data with prior information. 
The reservations that people have had about using Bayes' theorem concern the 
meaningfulness and source of some of the probabilities needed for its application. 
Its correctness has never been in dispute. However, we warn readers who are 
well informed in probability that our approach to a Bayesian analysis may 
differ considerably from their preconceptions. 
Coolidge (1925) illustrates conditions required to use Bayes' theorem and have 
probabilities that are relative frequencies with the example of repeated draw-
ings from an urn containing white and black balls in an unknown ratio. He 
points out that Bayes' theorem was originally derived on the assumption that 
initially all compositions of urns were equally likely. * 
"Imagine an immense number of urns containing black and white balls in 
varying proportions, but with a fixed number of urns with each mixture. 
Then if an urn be drawn at random and n drawings, with replacement, be 
made therefrom, showing just r white balls, the probability that the next ball 
will be white is accurately given by [a formula derived from Bayes' theorem]. 
* Quoted with permission. See Preface. 

48 
THE MAIN STUDY 
It is only when we can give a really precise statement of this sort that Bayes' 
principle can be used with perfect confidence, and the cases are rare. 
"Why not, then, reject the formula outright? Because, defective as it is, 
Bayes' formula is the only thing we have to answer certain important ques-
tions which do arise in the calculus of probability. The question as to the 
likelihood that a coin which showed a given succession of heads and tails 
should be bad is real and insistent. To say what might reasonably have been 
expected from a good coin under the circumstances does not, by any means, 
cover the case. Therefore we use Bayes' formula with a sigh, as the only 
thing available under. the circumstances .... " (p. 100) 
In striving for a match between theory and fact, one need not suppose that 
all compositions of urns are equally likely, and Bayes' theorem has too often 
been cast aside because that has not been understood. One approach to the 
problem of choosing distributions over "compositions of urns" might be to in-
vestigate a large class of events with a view to finding what distributions occur. 
Although this old idea has rarely been tried, one large investigation was carried 
out by Egon Pearson (1925) "with a view to making more clear the use and the 
limitations of Bayes' Theorem in the field of practical statistics." (p. 388) 
He made counts for many variables that might be expected to obey a binomial 
distribution with a view to finding the prior distribution of the probability p of 
success. Examples: fraction of men seen smoking a pipe; fraction of vehicles 
drawn by horses (p's do change with time!); fraction of chestnut colts born of 
bay mares; fraction of verbs in Carlyle. Pearson found that a uniform prior 
distribution was inconsistent with his data, but that a U- or V-shaped density 
produced a better fit. 
Berkson (1930), apparently commenting on this study, says:* 
"There are writers, however, who, admitting that the assumption is to be 
questioned, believe it may be subjected to experimental test, and have 
essayed to actually sample at random the probabilities that characterize the 
universes of our experience. It would be impertinent to assert that an experi-
mental investigation is bound to be futile, but the utility of this sort of 
procedure seems to us exceedingly dubious. We doubt indeed that any clear 
meaning can be assigned to the concept of 'the universes of our experience,' 
of which random samples are to be obtained. But granting the existence of 
such a distribution of a priori probabilities we doubt the relevancy of its 
estimation to any practical problem. In any actual investigation, we deal 
with a definite slice of possible experience; an anthropologist is not concerned 
with the universes dealt with in the investigation of an economist or an 
epidemiologist. If a priori probabilities are of interest to him, they are those 
that obtain in his peculiar world of observation. It appears to us quite as 
* Quoted with permission. See Preface. 

3.1] 
BAYES' THEOREM AND ITS APPLICATIONS 
49 
wide of the mark aimed at, to call in a formula which obtains its a priori 
probability from experience in general, as to obtain it from the unique ex-
perience at hand, and indeed it may be argued that, as between the two, the 
latter is the more reasonable." (pp. 54-55) 
While we find Pearson's study most educational, we admit that Berkson's 
criticism is a telling one. Why, indeed, should a man studying the authorship 
of The Federalist papers use stud books as a basis for prior distributions? 
Pearson concluded, * in part (although we do not know that these remarks 
would now represent his views): 
"The exact form of the distribution. . . seems to be something which each 
statistician can only determine for hiinself a posteriori by an examination of 
his own statistical experience, and Bayes' Theorem [with a uniform prior] can 
only be accepted as providing a valuable working rule for prediction, on the 
assumption that among the problems with which most statisticians are con-
fronted there is in fact a distribution ... whose difference from [uniformity] 
is of the same order as that observed in the experiments ... in this paper." 
(p.433) 
Coolidge implies: lacking better, we do the best we can. We do have better. 
We can meet Berkson's criticism head on by getting data appropriate to the 
problem, and this is what we do. Berkson and Pearson are both agreed that the 
most we can hope for is a good approximation. What is good enough for all the 
rest of applied mathematics is going to have to be good enough for statistical 
inference. The sort of approximation we have in mind is the one that the 
statistician ordinarily makes when he chooses a data distribution and proceeds 
to use it freely without further question, even though he knows that it is not, 
and cannot be, exactly right. We do not suggest carel~ss acceptance of either 
prior or data distributions, and in the main study, we pay more attention than 
usual to the effects of distributions. 
3.1. INTRODUCTION TO BAYES' THEOREM AND ITS 
APPLICATIONS 
How shall we use new observations to change our beliefs? In the realm of 
uncertain inference, Bayes' theorem offers one answer. Our problem is to assess 
our beliefs about the authorship of the disputed Federalist papers in the light 
of the evidence provided by the word frequencies. In the main study, we express 
our beliefs in terms of probabilities of authorship, and we use the standard form 
of inference based on Bayes' theorem. The object, then, is to determine the 
posterior, or conditional, or final probabilities of authorship, so named because 
they are conditional on or posterior to the evidence. 
* Quoted with permission. See Preface. 

50 
THE MAIN STUDY 
[3.1 
Neither using Bayes' theorem in practical problems nor formulating proba-
bilities as degree of belief would have won many popularity prizes over the last 
couple of centuries, so the reader may properly expect difficulties. We want to 
see what some of these difficulties are in a large analysis of data. 
In this main study, we use probabilities freely to express uncertainty, not only 
about authorship, but also about imperfectly known quantities (parameters) 
that describe how the frequencies of words vary. We think these uses of proba-
bility set the main study off a bit from the objective or relative-frequency ap-
proach to statistics in the direction of what is sometimes called "Bayesian" 
inference. Just how large the offset is, and just how practical the methods are, 
the reader will judge for himself, although he may have to wade upstream 
through our personal views to do it. 
Before proceeding to the technical development of the main study, we want 
to give a general description of our approach-foretell the sources of difficulties, 
and state roughly how we deal with each. 
One essential feature of our approach is the use of a range of values rather 
than a single value for any final probability, say, that Hamilton wrote paper 
No. 52. We can thus handle some forms of uncertainty without having to make 
the specific critical assumptions and computations needed to lead to a single 
"right" value. Of course, a range of values will be satisfactory only if the same 
general conclusion about authorship is appropriate to all values in the range. 
Consider briefly the basic components of Bayesian inference as applied to a 
single disputed paper. Suppose a set of word frequencies has been observed for 
the paper. To determine the final odds of authorship, we need to know the 
initial odds, as well as the respective sampling distributions of the set if 
Hamilton or if Madison wrote the paper. 
Given this information, Bayes' 
theorem is remarkably simple, but to get each of these pieces of information 
requires work, and even their meaning is troublesome. An additional problem-
the choice of words to be used in the inference-is intertwined with the other 
problems. 
Consider first the initial odds of authorship. Here, as in many applications 
of Bayesian inference, the initial or prior probabilities are present because they 
are required for Bayes' theorem, and not because of any desire to make some 
profit from prior information. Initial odds are a major source of uncertainty. 
They vary from person to person according to what prior evidence-here, mainly 
historical evidence-is available, and how the available evidence is assessed. 
There is no "correct" assessment, and we have no desire nor any special com-
petence to attempt one. 
Fortunately, the analysis of the statistical evidence can be carried out sepa-
rately from the determination of initial odds and the two combined at the very 
end. We concentrate on the former, and allow each reader to put in the initial 
odds as he chooses. We hope to produce such strong statistical evidence as to 
overwhelm any moderate assessment of initial odds. 

3.1] 
BAYES' THEOREM AND ITS APPLICATIONS 
51 
The first of several restrictions on words arises in the separation of the assess-
ment of the statistical evidence from the choice of initial probabilities, because 
the statistical evidence must be independent of the historical evidence on which 
the initial probabilities are based. To accomplish this, we eliminate most mean-
ingful words, i.e., what we call contextual words. The contextual words un-
doubtedly provide evidence, but the assessment of this evidence and of the 
fraction of it that is new is more a historical than a statistical problem. We hope 
to produce enough evidence without the contextual words. 
In Chapter 2, we studied the form of distributions of word frequencies in 
pieces of writing by a single author, and we found that for most noncontextual 
words, the negative binomial family of distributions fits well. In the main study, 
we assume that word frequencies are independently distributed according to 
negative binomial distributions with parameters that depend on the word, 
author, and length of paper. In Bayesian inference, the exact form of the 
distribution plays an important role, and as L. J. Savage says, the probability 
model that is chosen should be big enough to "fit an elephant." In our judgment, 
the negative binomial is adequate for The Federalist. As we proceed through the 
main study, we shall describe several more studies that support this judgment. 
The Poisson family of distributions performed distinctly less well in the 
studies in Chapter 2. Nevertheless, we have carried both families through the 
main study. in part to see where the choice of distributions is or is not important, 
and in part to allow motivation and exposition of our procedures for the simpler 
Poisson model. 
To specify a sampling distribution requires choosing not only the form but 
also the values of the parameters. Our knowledge of the parameters is im-
perfect, and more seriously so because we select words to be used from a large 
pool. The unknown parameters and the associated problem of selectivity cause 
the chief conceptual and computational difficulties in the main study. In facing 
these difficulties we make a second, more serious use of Bayes' theorem. 
If we knew the parameters, we could then calculate the probability of x oc-
currences of a word in a disputed paper if Hamilton wrote the paper, and the 
corresponding probability if Madison wrote the paper, the two necessary numbers 
for assessing the evidence from this word. The source of information about the 
parameters is the papers of known authorship; From these, the parameters can 
be estimated within a range of uncertainty. The probability of x occurrences 
in a disputed paper if Hamilton is the author will vary according to what values 
of the parameters are chosen within this range. What is required is an average 
of ~hese prob~bilities. 
We represent our state of uncertainty about these 
parameters by a probability distribution and use the weighted average with 
respect to this probability distribution. 
We form a conceptual pool of all the words that might be used. The general 
rate of use of a word is a source of recognizable inhomogeneity in the pool. To 
take account of this stratification is easy and of no import here. Suppose that, 

52 
THE MAIN STUDY 
[3.1 
apart from this, the pool can be regarded as homogeneous. Suppose also, for 
the moment, that the distribution of the parameter!:! over the words in the 
pool be known. This distribution is then the correct prior distribution for use 
in Bayes' theorem in combination with the observed word rates in the Papers 
of known authorship. Then, for any word chosen from the pool, we obtain a 
posterior probability distribution for its parameters. That distribution is then 
used ~o carry out the averaging to elhninate the unknown parameters from the 
sampJing distributions for the unknown paper. 
. 
The allowance for selectivity in this process is automatic, and works roughly 
as follows. Consider some measure of discriminating ability of words, say, a 
relative difference between mean rates. The distribution of this measure in the 
pool will be concentrateq about "no discriminating ability." The posterior dis-
tribution for any word will reflect this prior concentration at "no ability" and 
the evidence for apparent discrimi~ation in the papers of known authorship. 
For rare words, the evidence in the known papers is relatively slight, and the 
apparent discriminating ability will be heavily discounted. For high-frequency 
words, the balance reverses, and the ·evidence in the known papers will be dis:-
counted only slightly. 
We do not know the distribution over the pool of words, but we can estimate 
it sufficiently well for our ne~s. Using again the feature of providing a range 
of final answers, we carry through the entire analysis for each of about a dozen 
distributions in the pool. Actually, we have here a family of distributions, and 
are choosing several members of the farnily by choosing parameters of the family. 
We call these parameters "underlying constants" to avoid confusion with the 
word-rate parameters .. Our final' results, log odds of authorship, are given for 
each of several sets of unqerlying constants. Each set completely specifies the 
distribution in the pool. 
How, finally, do we get any information on. the distribution in the pool of 
words? Quite apart from questions of authorship, what we have now is a 
straightforward, but messy, estimation problem, akin to the estimation of the 
"between" variance component in Model II analysis of variance. Using the 
word frequencies for a sample of word!:! that are not preselected for discrimi-
nating ability, we can estimate the parameters for each word separately; then 
combini~g all words, we can estimate the parameters of the distribution of the 
word-rate parameters. There is nothing especially Bayesian in the way we 
handle this final problem. 
We turn now to the technical development of this program. A sequence of 
examples introduces both the ideas and problems involved. 
3.IA. An example applying B~yes' theorem with both initial odds 
and parameters known. Suppose Hamilton's and Madison's use of the word 
also is well represented by Poisson distributions whose parameters are the ex-
pected numbers of occurrences W,uH and W,uM, where w is paper length in thou-
sands of words and the ,u's are the rates per thousand. Suppose further that the 

3.1) 
BAYES' THEOREM AND ITS APPLICATIONS 
53 
TABLE 3.1-1 
POISSON PROBABILITIES WITH EXPECTED COUNTS 
WIl-H = .62, WIl-M = 1.34 
Frequency 
Hamilton 
Madison 
0 
.538 
.262 
1 
.334 
.351 
2 
.103 
.235 
3 
.0214 
.105 
4 
.00331 
.0352 
5 
.000411 
.00943 
6 
.0000424 
.00211 
rates are known to be Il-H = .31 and Il-M = .67. Then for an unknown paper of 
length W = 2 (2000 words), the probabilities for 0 to 6 usages by each author 
are shown to three significant figures in Table 3.1-1. 
Suppose also is used four times in a 2000-word paper written either by Hamil-
ton or by Madison; what are the odds that Hamilton wrote the paper? Natu-
rally, the answer depends on our uncertainty. If initially we thought the 
authorship was a tossup, the comparison of the probabilities .00331 and .0352 
would give us new odds of about 10 to 1 (.0352/.00331) in favor of Madison. 
But if we were nearly certain before the observation that Hamilton wrote the 
paper, even though the evidence would reduce the odds in favor of Hamilton by 
a factor of 10, we might still be left with a probability nearly 1 that Hamilton 
wrote it. Now we develop these ideas more formally. 
Let PI and P2 = 1 -
PI be the probabilities before the observation that 
Hypotheses 1 and 2, respectively, are true. For example, Hypothesis 1 might 
be that Hamilton wrote the paper, Hypothesis 2 that Madison wrote it. Let 
fi(X), i = 1, 2, be the conditional probabilities of observing the result x, given 
that Hypothesis i is true. For simplicity, assume that x is one of a discrete set of 
possible observations. 
The probability that the specific result x occurs is pdI(X) + P2.f2(X). (The 
probability is obtained by adding its two similar components; the first is the 
probability that Hypothesis 1 is true and that x occurs, or PI.ft(X).) The condi-
tional probability that Hypothesis 1 is true, given observation x, is 
. I 
PIft(X) 
P (HypotheSIS 1 x) = 
f ( ) + f ( ) 
PI I X 
P2 2 X 
This result is a special case of Bayes' theorem. 
Both computational and intuitive advantages accrue if we use odds instead 
of probabilities. The odds for Hypothesis 1 relative to Hypothesis 2 are given 

54 
THE MAIN STUDY 
by the ratio of their probabilities. Thus the odds are defined to be 
Odds (1 21 x) = P (Hypothesis 1 1 x) = Plh(x) = (Pl) (t!SoX)) 
, 
. 
P (Hypothesis 2 1 x) 
P2!z(x) 
pz 
fz(x) 
(1) 
[3.1 
= (initial odds) 
X 
(likelihood ratio) = final odds. 
Thus pt/pz is the initial odds determined by our beliefs prior to the execution 
of the experiment leading to the result x, and fl(X)/fz(x) is the likelihood ratio 
determined by the data of the experiment itself. 
To return to our example, suppose that initially we are very sure that Hamilton 
wrote the new paper, say Pl = .999 and pz = .001; then the initial odds for 
Hamilton are 999/1 or 999. The observed count of four usages in the paper 
gives the likelihood ratio fl(4)/fz(4) ~ .00331/.0352 ~ .1. Thus the final odds 
are 
Odds (Hamilton, Madison 1 x = 4) ~ 999(0.1) = 99.9, 
or about 100 to 1 for Hamilton. 
3.1B. Selecting words and weighting their evidence. Bayes' theorem, 
as described in Eq. (1), provides one rationale for selecting the words that we 
use to determine authorship, and it provides automatically for weighting the 
evidence from each word, as we now illustrate. 
TABLE 3.1-2 
RATES PER THOUSAND WORDS FOR also, an, AND because, 
AND A MEASUHE OF IMPOHTANCE FOR DISCRIMINATION 
Word 
Hamilton 
Madison 
Importance 
rate 
rate 
(w = 2) 
also 
.31 
.67 
.55 
an 
6.00 
4.50 
.86 
because 
.45 
.50 
.01 
Let us extend our previous example and suppose that the counts for also, an, 
and because are independent Poisson variables, with the known rates given in 
Table 3.1-2. Suppose that a 2000-word paper by one of our two authors con-
tains 4 also's, 7 an's, and 0 because's. If Hamilton is the author, the probability 
of the triple (4,7,0) is the product of three Poisson probabilities 
fp(41 .62)fp(7112)fp(0 1·9), 
and if Madison is the author, the probability is 
fp(411.34)fp(719)fp(0 11). 

3.1] 
BAYES' THEOREM AND ITS APPLICATIONS 
55 
Finally, the likelihood ratio for the three words is the product of the likelihood 
ratios for the separate words, and the weighting problem is solved. 
The numerical values of the Poisson probabilities instruct us further. They 
give the likelihood ratio as 
.~g::; X '~:13; X ::~~ ~ .0940 X .374 X 1.11 ~ .039. 
The contribution of because, the third factor, 1.11, is near unity since the two 
authors use the word with nearly identical rates. 
The logarithmic form of Bayes' theorem can be obtained by taking the loga-
rithm of both sides of Eq. (1) to get 
Final log odds = Initial log odds + log likelihood ratio, 
and then evidence from independent measurements is additive instead of 
multiplicative. 
For a single observation x drawn from a Poisson distribution, the likelihood 
ratio, Hamilton to Madison, is 
K = (/J-H//J-Mte-W(!J.H-!J.M), 
and the log likelihood ratio is 
For one observation on each of n independent words, the log likelihood ratio is 
where the subscript i stands for the ith word and the other subscripts have their 
obvious meanings. 
In choosing independent words as discriminators, the criterion is whether 
their contribution is worth the cost of including them. When rates are known, 
no bias arises from selection. (Selection effects arise because chosen variables 
are not as good at discriminating as they appear to be.) In the example, because 
cannot contribute much to the log odds for any plausible observation; indeed 
its likelihood ratios for x = 0, 1,2,3 are, successively, 1.11, 1.00, .90, .81. If 
the cost of observing the frequency of because is high, or in more complicated 
problems, if the cost of computing is large, then we may prefer to discard the 
word. Of course, that. decision should not be preceded by a peek at the fre-
quencies of because in the unknown material, for medical and psychological ex-
periments are not the only ones that require controls to guard against the 
frailty of an investigator's objectivity. Objective decisions come easily when one 
has no knowledge of the unknowns, but once one has the information, trying to 
ignore it puts one in the position of the small boy who is trying not to think of 
a rhinoceros that is standing on top of a flagpole. 

56 
THE MAIN STUDY 
[3.1 
One useful measure for the importance of a word is the difference between 
expected log likelihood ratio given Hamilton's authorship and that given Madi-
son's. For a 2000-word paper (w = 2.0), this measure is 
In Table 3.1-2, we list importances for also, an, and because, and the value for 
the latter is negligible compared with the others. For the Poisson, this measure 
of importance of a word can be described as the difference between the log odds 
for a paper whose observed rate for this word is at the Hamilton mean and the 
log odds for a paper whose observed rate for this word is at the Madison mean. 
An importance of zero implies identical means, and an importance of, say, 
log 2 implies that a paper of unknown authorship with observed rate at the 
Hamilton mean would be assigned odds for Hamilton against Madison twice 
as large as the odds that would be assigned if the observed rate were at the Madi-
son mean. This discussion concerns only the contribution of this one word. 
Among words whose summed mean rate for Hamilton and Madison exceeds 1.0, 
we have discarded those words for which this swing in log odds between the 
mean rates for a 2000-word paper was less than .34 = log 1.4. For low-frequency 
words, a less stringent criterion seems reasonable. We computed the swing in 
log odds between a paper with no occurrences and one with one occurrence, and 
discarded the word if this swing in log odds was less than .48 = log 1.62. (The 
actual rules were made on the basis of the negative binomial distribution and a 
3000-word length of paper, but the essentials of the decisions are contained in 
the above remarks.) 
3.le. Initial odds. All this seems perfectly straightforward. Wherein lie 
the difficulties? The first place is in the choice of the initial odds-here, the values 
of Pl and P2. Your final odds-posterior odds-will differ from mine, if we 
choose differing values for Pl. In some problems, the choice of Pl might be 
made on the basis of objective frequencies, but in others, personal degree of 
belief may be involved. In our own problem, you might regard the initial odds 
as 1 for an unknown paper, thus setting a 50-50 chance as appropriate to your 
degree of ignorance. Or you might notice that Hamilton wrote 43 of the known 
papers to Madison's 14, and then assign prior odds of 43/14 ~ 3. Or if you are 
an historian with knowledge of the problem, you may have quite strong beliefs 
that lead to the assignment of large odds in one or the other direction. 
Nevertheless, an analysis using more and better words than the three in the 
example may provide a likelihood ratio that overwhelms most of the variation 
in initial odds. Then the final odds; though still variable from person to person, 
would be very large or very small: 
For example, a likelihood ratio of 10-6 would convert strong initial odds of 103 
(1000 to 1 for Hamilton) to final odds of 10-3 (1000 to 1 for Madison). In 
terms of probabilities, the same likelihood ratio converts an initial probability 

3.1] 
BAYES' THEOREM AND ITS APPLICATIONS 
57 
of .999 to a final probability of .001, and any initial probability less than .999 
to a final probability of less than .001. Naturally, for any strength of data, 
opinions can be so extreme that the direction of odds cannot be changed. 
In summary, by the factorization of final odds into the product of initial odds 
and likelihood ratio, the difficulties in assessing the initial odds-what might 
be called the evaluatio!1 of the historical evidence-have been separated from 
the statistical analysis needed to determine the likelihood ratio. Further, if 
the likelihood ratio is large enough, or small enough, the final probabilities 
(not odds) are changed only slightly by wide changes in the initial odds. The 
statistical analysis in our study is devoted largely to the likelihood ratio. What 
we call "odds" in the bulk of this monograph are these likelihood ratios or, in 
more complex situations, the analogous factors for converting initial odds to 
final odds. Thus our odds apply without adjustment only for even initial odds, 
while a simple adjustment is needed for any other initial odds. 
By this factorization and the resulting factorization of the problem, we have 
largely avoided having to specify the initial probabilities, often considered the 
major obstacle to using Bayes' theorem. Consequently, our approach coincides 
with much of the work on classification problems (cf. Anderson, 1958, 
Chapter 6; or Rao, 1952, Chapter $). In our handling of nuisance parameters, 
we make more critical use of Bayes' theorem, and to that problem we now 
turn. 
3.ID. Unknown parameters. A second and more serious difficulty arises 
from uncertainty in the data distribution. We do not know that it is Poisson; 
indeed we have seen evidence in favor of the negative binomial. But even if 
the form were known exactly, we would not know the parameters of the distri-
bution exactly, nor could we be confident that the parameters remain constant 
from one sort of text to another. We mention the la,tter problem briefly in 
Section 3.3 and treat it in detail in Section 4.10. 
Let us look at the difficulty of the unknown parameter values. For purposes 
of exposition, assume that Poisson distributions adequately represent the dis-
tribution of the word frequencies. Then for each word the unknown parameters 
are the rates per 1000 words of text in Hamilton's and in Madison's writings. 
Denote these rates by IlH and IlM. 
The usual way to get information on parameter values is to observe large 
amounts of data known to be sampled from each desired distribution. If these 
data provided precise point estimates of the parameters, we would, in the 
tradition of large-sample statistics, use the estimates in place of the known 
values to evaluate the likelihood ratios. 
In the actual problem, we have 94,000 words of text known to be written by 
Hamilton and 114,000 words by Madison, seemingly vast amounts, yet 
surprisingly little for handling any but words of the highest frequency. 
For example, if the word also had the rates .31 and .67 in Hamilton's and 
Madison's writings, we would expect only 29 and 76 occurrences, respectively, 

58 
THE MAIN STUDY 
[3.1 
and the standard errors of the observed mean rates would be V.31/94 :::::: .057 
and V.67/114 :::::: .077. These standard errors are substantial, and selection 
effects are serious. The word also was chosen as one of the better words for 
discrimination on the basis of the difference between observed rates, and so the 
true rates are likely to be closer together than the observed rates. 
For any but the highest-frequency words, we must make allowance for im-
perfect knowledge of the rate parameters-the nuisance parameters in this 
discrimination problem. One allowance uses Bayes' theorem, this time applied 
to many continuous parameters, rather than to two hypotheses. It is this 
second use of Bayes' theorem that is the core of our development. 
We assume that the state of knowledge about the rates, the fJ-'S, for each word 
and author can be represented by probability distributions. In other words, 
we treat the fJ-'S as if they were random variables. We need the distribution of 
the fJ-'S posterior to, or conditional on, the data in the papers whose authorship 
is known. Section 3.2 treats the problems of choosing a distribution to represent 
prior information about comparative and average rates of word use and of 
applying Bayes' theorem to determine the posterior distributions given the data 
in the papers of known authorship. 
Some discussion of the relation of the two uses of Bayes' theorem and of the 
several prior and posterior distributions and bodies of data involved may be 
helpful. Figure 3.1-1 shows a block diagram of the two stages of analysis. Our 
exposition heretofore has been concerned with the second stage. In Stage I, 
which has both logical and computational precedence, we analyze the papers 
whose authorship is known and convert the prior information about word rates 
into posterior distributions for these parameters. This analysis has no direct 
effect on the probabilities of authorship for the disputed papers. 
In Stage II, both prior odds of authorship and probability distributions for 
the rates are required for analyzing the disputed papers. The distributions of 
the parameters prior to Stage II are the posterior distributions from the Stage I 
analysis. The prior odds of authorship are what we have called initial odds. 
The outputs of the Stage II analysis are posterior odds of authorship, which 
we call final odds. We could also get the posterior (II) distributions of the 
parameters, based now on all the data, but we do not need them. We study 
distributions of the parameters only insofar as needed to get the final odds of 
authorship, and the distributions posterior to Stage I but prior to Stage II are 
all that are needed. (However, in some classification problems, for example, 
where the classes are not clearly specified in advance, the Stage II posterior 
distributions would be more important.) 
To avoid confusion, we adopt the adjectives "prior" and "posterior" to de-
scribe the distributions of parameters before and after the Stage I analysis, 
and "initial" and "final" to describe the odds and probabilities of authorship 
before and after the Stage II analysis. 
Suppose the Stage I analysis has been carried out; let us return to the prob-
lem of assessing the evidence from four occurrences of also in an unknown paper. 

3.1] 
Stage I 
Stage II 
BAYES' THEOREM AND ITS APPLICATIONS 
/ 
/ 
Prior (I) 
.' 
Posterior (1) 
distributions of 
I. An~lysls of 
distributions of 
parameters (word I-_~ data In papers 
~ 
parameters. "The 
rates, etc.). "The 
of. known author-
posterior distri-
prior distribu-
ship 
butions" 
tions" 
" 
/ 
/ 
'\ 
Prior (II) = 
Posterior (1) 
distributions 
of parameters 
\. 
./ 
Prior odds of 
authorship. "The 
initial odds" 
II. Analysis of 
data in papers 
of unknown 
authorship 
Posterior odds of 
authorship. "The 
final odds" 
\.------.---.-/ 
I 
,.-------- .... 
I 
' 
1 
\ 
1 Posterior (II) 
I 
1 distributions 
I 
----, 
1 
1 of parameters 
1 
I (not needed) 
I 
\ 
I 
' .. _-----_ ... ' 
FIG. 3.1-1. Block diagram showing two stages of analysis. 
59 
What we need as the likelihood ratio factor for changing initial odds to final 
odds is the ratio of the probability of observing four occurrences of also if 
Hamilton is the author to the probability of four occurrences if Madison is the 
author. Our Poisson model does not specify these probabilities; it specifies only 
such conditional probabilities as that of four occurrences given that Hamilton 
is the author and given that his rate is ,uH: in symbols, P{x = 4 1 H, ,uH}. 
Denote by P(,uH, ,uM) the joint posterior density of the rates ,uH and ,uM given 
by the Stage I analysis, and by P(,uH) and P(,uM) the marginal posterior densities. 
The desired unconditional probability of four occurrences if Hamilton is the 
author can be found by averaging the conditional probabilities over the pos-
terior distribution of the rate ,uH. The probability can be written as an expecta-
tion or as an integral, 
P{x = 41 H} = f P{x = 41 H, ,uH}p(,uH) d,uH = E(P{x = 41 H, fiH}), 
where the - over ,uH indicates that the expectation is taken over the distribu-
tion of fiH. 

60 
THE MAIN STUDY 
[3.2 
Similarly, 
P{x = 4/ M} = f P{x = 4/ M, IlM}P(IlM) dllM = E(P{x = 4/ M, ,aM}), 
so the unconditional likelihood ratio is the ratio of two expectations or integrals. 
If the posterior densities of the rates are known for each word and each 
author, we need now, in principle, only carry out the integrations and obtain 
the two probabilities required for each word and thence the likelihood ratio. 
But even for the Poisson family of distributions, carrying out the integration 
requires bivariate numerical integrations. The integrals look univariate, but the 
marginal density of IlH appearing in the integrand must itself be obtained by 
an integration from the joint density P(IlH,IlM) that Bayes' theorem yields. 
(The rates cannot be treated independently, because they are a priori depend-
ent.) In the extension to the negative binomial family, the integrals become 
four-dimensional. 
In the presence of a substantial amount of data, the posterior distributions of 
the rates are rltther sharply concentrated and a natural approximlttion for the 
expectation of !l,ny function, say g, over this distribution is the function evalu-
ated at some central value of the rate that we label p,: 
E(g(,a)) = f g(ll)p(ll) dll ~ g(fJ,). 
The mean might be the preferred choice for p" but it can be determined only 
by integrations of the type being avoided. The mode, the position of the highest 
point of the density, is a more feasible choice and one well articulated with the 
use of Bayes' theorem. We do use the modes of the posterior distributions in 
this way, discussing necessary adjustments later. The unadjusted result of 
using this approximlttion is identical with that obtained if the modal estimated 
rates were used as the known rates. Except for very high-frequency words like 
the or of, the modal estimated rates are not the sa~e as the observed mean rates 
in the Hamilton and Madison papers, because the prior distributions have the 
effect of moving the estimated rates closer together, thereby making allowances 
for selection of the apparently best words. 
3.2. HANDLING UNKNOWN PARAMETERS OF DATA 
DISTRIBUTIONS 
When parameters of a data distribution are unknown, Bayes' theorem re-
quires both data sampled from the data distributions and a prior distribution 
for the parameters, treated now as random variables rather than as constants. 
Bayes' theorem then yields a posterior distribution for the parameters. 
For the Poisson family, the rate parameters J.tH and J.tM for each word must 
be assigned a prior distribution. III our problem, Bayes' theorem uses the data 
on papers of known authorship to obtain a posterior distribution for the word 

3.2] 
UNKNOWN PARAMETERS OF DATA DISTRIBUTIONS 
61 
rates. Finally, we complete the classification of unknown papers as described 
in Section 3.1D. 
How to choose prior distributions is the main topic of this section and our 
approach introduces two essential features of our study: we use more than one 
choice of prior distribution, and we base our choices of priors on data, even if 
feebly. 
3.2A. Choosing prior distributions. The data in the papers of known 
authorship are sufficient to dominate any prior information on the rate of use 
of a word averaged over the two authors, but not large enough to dominate 
prior information on the comparative rates for the same word. We expect both 
authors to have nearly identical rates for almost any word. We choose the prior 
distributions of rates to represent this prior expectation. The effect of these 
prior distributions is to reduce the apparent quality of discriminators, and 
thereby we automatically allow for effects of selection of apparently good dis-
criminators from large pools of words. 
To separate average rate of use from a comparison of the rates for the two 
authors, we introduce a pair of parameters for each word: 
(J' = IJ-H + IJ-M, 
T = 
IJ-H 
IJ-H + IJ-M 
(Our (J' has nothing to do with standard deviation.) 
Clearly,!(J' measures 
average frequency, and T measures the ability to discriminate. For fixed T(:;;z£ !), 
the bigger(J', the better the discrimination. When T = !, the authors have equal 
rates. 
Prior information about (J' arises from a few studies of word rates in texts 
written more than one hundred years before or after The Federalist. The net 
effect of this information is almost negligible in comparison to the observed rates 
in the 94,000 words of Hamilton text and the 114,000 words of Madison text. 
We use a flat prior for (J' for each word. 
For authors writing together on the same topic at the same period, we suppose 
that the prior distribution of T for any word would be very nearly symmetric 
and unimodal with much probability near!. The spread may depend on the 
combined rate (J' of the word, but otherwise the same distribution might reason-
ably be expected to apply, independently, to almost any word free from severe 
contextuality. The distribution's concentration around! is critical, yet hard 
to specify without reference to any data. Our plan is to get rough estimates of 
the distribution of T over a group of words, unselected for ability to discriminate, 
and representative of the pools of words from which all words were selected; 
in particular, we use the 90 function words from the Miller-Newman-Friedman 
list (see Section 2.SA). We cannot expect to determine the distribution of T 
exactly, but we can hope to estimate it within a range adequate for our uses. 
A graphical treatment is instructive. 
Let mH and mM be Hamilton's and 
Madison's observed rates for a given word. Then 8 = mH + mM estimates (J' 

62 
THE MAIN STUDY 
[3.2 
1.0 
• 
• 
. 9 
.8 
• 
• 
• 
.7~~~----------------------~----------
• 
. 
6~----~~~~~=-----~.~------------~--
• 
• 
-- ---.- _--e- -.-----.-.--. -:.---- -e- - -- - -
- -.- - - -r 
.5r---~~·~~---r--~~~~-------~·----~·~6 
• C 
• 
• 
.4L-____ ~~::~~;~~-:D~-:-:-::-:-:-:r:-::-:~~-==-=-=-==-:-::-
.3~~~~~----~----~----~--------
• 
• 
8n 
FIG. 3.2-1. Sample estimates (Sn, tn) of the parameters (Un, Tn) for 90 function words. 
Curves Band C show two-standard-error bands for tn if Tn = .5. Curve A shows a 
two-standard-error band above Tn = .55. Curve D shows a two-standard-error band 
below Tn = .45. 
and t = mH/s estimates T. For our material, 8 has a small standard error, and 
so, for practical purposes, we take 8 ~ 0'. The standard error of t depends upon 
both T and 0'. 
In Fig. 3.2-1 the (8, t) pairs are plotted for the 90 unselected function words. 
(The square-root scale on the horizontal axis is used to stabilize the variance of 8. 
The circled points have been moved to the left to bring them into the figure.) 
Curves Band C in th~ figure give two-standard-error bands around the null 
value T = .5. Curve A is located two standard errors above the value T = .55, 
and curve D the same distance below T = .45. 
The variation vertically in Fig. 3.2-1 is much larger than can be expected 
with T = .5 for every word and Poisson variation in the rates. But if we want 
to estimate the fraction of words with 7' outside the interval .3 to .7, the estimate 
should be less than the observed fraction .12 of t's outside, because sampling 
/ 
/ 

3.2] 
UNKNOWN PARAMETERS OF DATA DISTRIBUTIONS 
TABLE 3.2-1 
PAIRS OF VALUES OF (31, (32, CHOSEN FOR THE 
INITIAL CALCULATIONS 
5 
2 
10 
20 
5 
2 
20 
1 
111 
5 
10 
10 
63 
variation alone could put some t's outside even if no T'S are. Similarly the true 
proportion outside .4 to .6 is almost surely less than the observed .28. 
If the distribution of T values were a beta distribution with probability density 
function T'Y-l(1 -
T)'Y- 1/B('Y, 'Y) with equal arguments 'Y, we could investigate 
how the probabilities vary with 'Y. We chose the form 'Y = (31 + (32(1 to allow 
decreased variability for T with increased (1. After reviewing the effects of chang-
ing 'Y on the tails of the distribution, we decided that a range of 'Y between 5 
and 20 was plausible. Therefore we selected the pairs of values for (31 and (32, 
shown in Table 3.2-1, that would more than handle the range. 
The analysis of Fig. 3.2-1 for the Poisson model is an oversimplification 
because part of the variation, in excess of Poisson variation, is due to non-
Poisson variation in the counts for each author and not to variation in rates 
between the authors. Our final choices of (3's for bracketing the true prior 
distribution are carried out for the negative binomial distribution. The detailed 
discussion of the choices of parametrization and of families of distributions and 
initial choices of (3's is given here in a relatively simple situation, not only for 
exposition of the method but also because our initial choices for the negative 
binomial model are based in large part on direct or analogous use of this Poisson 
development. 
3.2B. The interpretation of the prior distributions. More formally, we 
suppose there are underlying linguistic quantities (31, (32 that determine the gen-
eral similarity of word occurrences by Hamilton and Madison for a large pool 
of words-including the pools of words from which we selected our 165. We 
further assume that given (31, f32, the prior distribution of the T for each word 
in the pool, given its (1, is adequately represented by a symmetric beta distri-
bution, both of whose arguments are 'Y = (31 + (32(1. We assume that the dis-
tributions of the T'S are independent across words. 
3.2C. Effect of varying the prior. Logically, the (3's are parameters of the 
distribution of the differential-rate parameter T, but because this terminology 
is intolerable, we call them "underlying constants." To try next to introduce 
a prior distribution for (31 and (32 is to invite an infinite regression. Instead, we 
repeatedly carry out the analysis assuming known (31, (32, for several sets shown 
in Table 3.2-1. Naturally, after the initial evaluations, further refinements 

64 
THE MAIN STUDY 
[3.2 
can be introduced. 
The essential feature is the many analyses, with their 
fluctuating log odds. Nevertheless, the variation may not be enough to matter 
in the final assessments of authorship. 
3.2D. The posterior distribution of (a, 1'). For any pair of underlying 
constants (31, (32, the posterior density of the parameters (u, r), given the vectors 
of data XH, XM on the papers of known authorship, is, by Bayes' theorem: 
p(u, r I XH, XM) = C(X)p(u, r)p(XH' XM I u, r), 
where C(X) is a constant, p(u, r) is the prior density of the parameters (u, f), 
and P(XH, XM I u, r) is the density for the vectors of data, given the parameters. 
Return to the example of also. For the Poisson, the likelihood of observing 
26 counts in 94,000 words of Hamilton, and 80 in 114,000 of Madison, with 
rates JLH = ur, JLM = u(I -
r), has'logarithm: 
log p(XH, XM I u, r) :- -94ur + 26Iog[94ur] -
log 26! 
-114u(I -
r) + 80Iog[114u(I -
r)] 
-log 80!. 
The prior density with (31 = 10, (32 = ° (our preferred choice) has logarithm 
log p(u, r) = const + (10 -
1) log[r(I -
r)], 
where the constant includes log B(IO, 10) and the constant prior assigned to u. 
Then by Bayes' theorem the posterior density of (u, r) for also has logarithm 
94 + 114 
log p(u, r I XH, XM) = const -
2 
u + (80 + 26) log u 
+ (114 -
94)u(r - !) 
+ (26 + 10 -
1) log r 
+ (80 + 10 -
1) log(I -
r). 
Approximate methods tell us that the mode of the posterior density is near 
U = .99, f = .316. The effect of this prior is to give an estimate f = .316 
instead of the estimate t = .282 based on the observed rates. The allowance 
for selection effects is thus moderate for also, but stronger for low-frequency 
words, as seems appropriate. 
Solve for iJ.H = .31, iJ.M = .67. At last we are ready to estimate odds for this 
set of underlying constants. If a paper of 2000 words has four also's, we mUltiply 
bo$ rates by 2.000 and use the Poisson tables to get 
fp(41·62) 
fp(411.34) ~ .1, 
described at the start of Section 3.1. 

3.2] 
UNKNOWN PARAMETERS OF DATA DISTRIBUTIONS 
65 
TABLE 3.2-2 
FINAL CHOICES OF SETS OF UNDERLYING CONSTANTS 
Set 
{31 
{32 
{33 
{34 
(35 
22 
10 
0 
12 
1.25 
2.0 
31 
10 
0 
12 
.83 
1.2 
33 
15 
0 
12 
.83 
1.2 
38 
5 
5 
6 
.83 
1.2 
21 
5 
6 
1.25 
2.0 
11 
5 
1 
1.5 
1.25 
2.0 
3.2E. Negative binomial. The entire study was carried out in parallel for 
the negative binomial and Poisson data distributions. The negative binomial 
introduces many complications that strongly influence our allocation of effort, 
but few new ideas. The following brief statement of the treatment of prior 
distributions is given for completeness. 
For each word, four parameters are needed: the mean rate,u and non-Poisson-
ness 0 = ,uIK for each author. The mean rates were handled exactly as for the 
Poisson. A study of estimates based on moments, in the spirit of Fig. 3.2-1, 
suggested that non-Poissonness 0 was nearly independent of the rate ,u, and this 
independence was the reason for usillg the measure o. A tail-reducing transforma-
tion from 0 to r = log(1 + 0) was made for each author, and then transformed 
to ~ = rH + rM, 1/ = 
rH/~, so that ~ and 1/ are analogous to (J' and T. 
We introduced five underlying constants {31, {32, {33, {34, {35, and assumed that, 
given the {3's, ((J', T, ~, 1/) are independent across words; ((J', T), ~, 1/ are independ-
ent of each other for each word; (J' has a distribution that can be adequately 
approximated by a constant density; conditional on (J', the parameter T has the 
symmetric beta density: 
[T(l -
T)tl+~20"-1 
. 
B({31 + {32(J', (31 + (32(J') , 
1/ has the symmetric beta density: 
[1/(1 -
1/)1~3-1 
B({33, (33) 
, 
~ has the gamma density with mean {34, argument {35: 
({3 51 (34)~5~~5-1e-~5~1 {34 
r({35) 
Each quintuple of {3's is called a set of underlying constants and is assigned a 
code number. Table 3.2-2 shows the values of the underlying constants for the 
six sets used in the displayed log odds in Section 3.4. Altogether, 21 different 
sets were used in some phase of the study. 

66 
THE MAIN STUDY 
[3.2 
TABLE 3.2-3 
FINAL WORDS AND WORD GROUPS 
ESTIMATED NEGATIVE BINOMIAL PARAMETERS 
Set 31 
Set 22 
Code 
Word 
1·1.1 
jJ.2 
(J 
T 
151 
152 
151 
152 
B3A 
60 
upon 
3.24 
.23 
3.47 
.932 
.25 
.39 
.29 
.48 
B3B 
3 
also 
.32 
.67 
.99 
.327 
.09 
.lO 
.13 
.14 
4 
an 
5.95 
4.58 
lO.53 
.565 
.02 
.02 
.07 
.07 
13 
by 
7.32 
11.43 
18.75 
.390 
.35 
.40 
.37 
.43 
39 
of 
64.51 
57.89 
122.40 
.527 
.24 
.25 
.26 
.28 
40 
on 
3.38 
7.75 
11.12 
.304 
.34 
.42 
.37 
044 
55 
there 
3.20 
1.33 
4.53 
.706 
.23 
.24 
.25 
.27 
57 
this 
7.77 
6.00 
13.77 
.564 
.21 
.21 
.25 
.23 
58 
to 
40.79 
35.21 
76.00 
.537 
.39 
.45 
Al 
.48 
B3G 
73 
although 
.06 
.17 
.23 
.267 
.11 
.11 
.21 
.18 
78 
both 
.52 
1.04 
1.56 
.334 
.12 
.14 
.15 
.18 
90 
enough 
.25 
.lO 
.35 
.727 
.47 
.52 
.54 
.64 
116 
while 
.21 
.07 
.28 
.744 
.23 
.25 
.29 
.35 
117 
whilst 
.08 
042 
.50 
.153 
.15 
.13 
.27 
.20 
123 
always 
.58 
.20 
.78 
.742 
.07 
.07 
.13 
.13 
160 
though 
.91 
.51 
1.42 
.639 
.08 
.08 
.11 
.12 
B3E 
80 
commonly 
.17 
.05 
.23 
.763 
.05 
.05 
.14 
.16 
81 
consequently 
.lO 
.42 
.52 
.189 
.16 
.14 
.25 
.20 
82 
considerable (ly) 
.37 
.17 
.54 
.684 
.07 
.08 
.13 
.15 
119 
according 
.17 
.54 
.71 
.238 
.30 
.30 
.36 
.34 
124 
apt 
.27 
.08 
.35 
.770 
.06 
.07 
.13 
.16 
B3Z 
87 
direction 
.17 
.08 
.25 
.693 
.31 
.32 
.39 
.43 
94 
innovation(s) 
.06 
.15 
.20 
.278 
.06 
.06 
.15 
.13 
96 
language 
.08 
.18 
.26 
.316 
.05 
.05 
.11 
.lO 
110 
vigor (ous) 
.18 
.08 
.26 
.680 
.02 
.02 
.08 
.09 
143 
kind 
.69 
.17 
.86 
.799 
.25 
.22 
.29 
.27 
146 
matter(s) 
.36 
.09 
.45 
.790 
.05 
.05 
.12 
.13 
151 
particularly 
.15 
.37 
.51 
.282 
.14 
.16 
.20 
.21 
153 
probability 
.27 
.09 
.36 
.757 
.02 
.02 
.07 
.08 
165 
work(s) 
.13 
.27 
040 
.326 
.46 
042 
.55 
047 
Note: Estimates are based on set 31 of underlying constants. Estimates of 151 and 
02 for set 22 are also shown in final columns. 

3.3] 
SELECTION OF WORDS 
67 
For each set of underlying constants, the mode of the posterior distribution 
of (u, T, ~, T/) was determined for each word. * The corresponding values of the 
parameters JJ-1, 01 and JJ-2, 02t were used in the calculation of likelihood ratios 
for the unknown papers as if they were the known parameters. Table 3.2-3 
shows the parameters estimated in this way from set 31 of underlying constants 
for each of the 30 words finally chosen for the study. The final columns give the 
non-Poissonness parameters estimated from set 22. The estimated rates from 
set 22 differ from those from set 31 by .01 or less. Set 22 is our preferred choice, 
but many side studies use the estimates from set 31. 
3.2F. Final choices of underlying constants. For both the Poisson and 
negative binomial families, a variety of sets of (3's were used, each giving a dif-
ferent prior distribution. The pool of 90 unselected words provides evidence on 
plausible ranges for the (3's as indicated in Section 3.2A. The methods for esti-
mating the (3's are presented in Section 4.5. The 6 sets shown in Table 3.2-2 
and used in the displayed log odds in Section 3.4 are spread over the estimated 
range. The one set that most closely represents the available evidence is set 22. 
For the Poisson, (33, (34, (35 are irrelevant. 
How (31 and (32 influence the posterior mode of T can most easily be seen by 
means of an approximate sampling equivalent of the prior information. The 
posterior mode of T, using (31 and (32, is approximately equal to the sample esti-
mate t from a modified sample in which the number (31 + (328' of extra occur-
rences hat:! . been added to the occurrences' of the word in both the Hamilton and 
the Madison texts. Here, 8' is the estimated combined rate, estimated from the 
original data. The net discriminating power of the word decreases as (31 + (32U 
increases; for low-frequency words, (31 is important; for high-frequency words, 
(32 is important. 
The final three {3's concern the non-Poissonness parameters 81, 82 of the nega-
tive binomial. Large values of (33 require 81, 82 to be nearly equal; small values 
of (33 allow them to be nearly independently determined. The final two, (34 and 
(35, influence the determination of the average non-Poissonness in a way that is 
neither easily described nor very important. 
3.3. 
SELECTION OF WORDS 
The pools of words and the 165 words chosen for inclusion in the study were 
described in Chapter 2. The analysis of the disputed papers in the main study is 
based on 30 words: "the final 30 words." Here we describe how these words 
were chosen. 
An essential feature of the main study is that allowance for selection and 
regression effects is made entirely through the prior distributions. We assume 
* The computation was programmed for digital computer by Miles Davis. 
t Subscripts 1 and 2 refer to Hamilton and Madison, respectively. 

68 
THE MAIN STUDY 
[3.3 
that the prior distributions apply to any and every word chosen from some large 
pool of words. Then, according to the model, the prior distribution will reduce 
the apparent discriminating ability to the appropriate extent. 
With this selection feature, we may choose words for inclusion by whatever 
methods are convenient, so long as they are independent of the unknown papers. 
Thus the words obtained from the screening and the index studies may be 
included. What matters more is that we may continue to eliminate words 
without changing the treatment or validity of the remaining words. 
Two restrictions are important. First, to esthnh.te the underlying constants 
of the prior distribution, we must not select the words on the basis of discrimi-
nating ability. For this purpose we use the 70 words with highest frequency in 
the Miller-Newman-Friedman list and the random sample of 20 words of low 
frequency from that list. Second, we assume that the variation in parameters 
measuring word rates for these 90 miselected words is representative of that in 
the entire pool from which all 165 words were chosen. 
We grouped words according to categories of use and source that we felt 
might bear on contextuality. The groups were ~de up of the i65 words in 
the study, though the definitions were made to apply to pools from which the 
165 were chosen. Groups A, B, G (fdr gaIlllIla), and D contain wotds occurring 
on the Miller-Newman-Friedman list: D, the pronouns arid auxiliary verbs; 
A, upon; B, the remaining words among the 70 high-frequency fUnction words; 
G, the remaining low-frequency function words. Our 90 unselected words are 
directly representative of the pools corresponding to A, B, G, D. Group E 
consists of words that occur on the list of "well-liked" words (mostly adjectives 
and adverbs) made up, withOut regard to discriminating ability, from the index. 
Group Z (for zeta.) consists of words on a larger, incompletely defined list, made 
up from the index and including mostly abstract nouns. We regarded the remain-
ing words, group H (for eta) as severely contextual and did not use them further 
in the main study. Of the groups we retained, D and Z seemed to us inost likely 
to have strong effects of contextuality. This division of the 165 words gave 
group sizes: 
A,l; 
B,46; 
G,30; 
D,31; 
E,9; 
Z,31; 
H,17. 
Most of the 165 words show no appreciable discriminating abilitY', and we 
kept only those words that had sufficient "importance" as described in SectIon 
3.1. After this aSsessment, groups A, B, G, D, E, Z had 1, 11, 9, 5, 6, 13 
words, respectively. IIi this step we eliminated a handful of words for which 
the computation of the posterior mode was not successful. 
, 
We evaluated log odds for the known papers and examined them for troubles. 
A major trouble waS a systematic deviation between the Madison word rates 
in The Federalist and those in the long Neutral Trade paper. By methods that 
we present in Section 4.10, we retained only those words that are ~omogeneous 
across the Madison papers. In addition; we eliminated all words in the group 
containing personal pronouns and auxiliary verbs because we feared that they 

3.4] 
LOG ODDS 
69 
were contextual. The net was 30 words in five groups, called B3A, B3B, B3G, 
B3E, B3Z, and displayed in Table 3.2-3 with the estimated parameters for the 
negative binomial analysis for set 31 of underlying constants. We have decided 
to retain the cumbersome code for groups of words, rather than risk introducing 
errors. 
3.4. LOG ODDS 
In this section and the next, we give the results of our log odds computations, 
reserving adjustments for later sections. What we present is a sprinkling of the 
log odds from among the 113 X 302 ~ 105 numbers for 113 papers, 30 words, 
and 30 sets of underlying constants. * First we examine the log odds in total 
for the final 30 words: then in Section 3.5, we examine log odds for separate word 
groups and individual words. 
As explained in Section 3.1C, our log odds are logarithms of the final odds on 
the authorship of a paper when the initial odds are 1 to 1. For any other initial 
odds, our log odds are the logarithm of the factor that changes initial odds to 
final odds. 
3.4A. Checking the method. In Table 3.4-1 we present the total log odds 
from the 30 words for each of the 98 papers of known authorship, obtained by 
treating each paper as if it were a disputed paper. Each entry is the approximate 
log odds obtained by using, for each word, the estimated parameters based on 
the indicated set of underlying constants as if the estimates were the true param-
eter values. 
Looking first at the negative binomial model for set 22 of underlying con-
stants, we see that every Hamilton paper has positive log odds; every Madison 
paper, negative log odds. The weakest result among the Hamilton papers is 
for No. 113 with log odds of 3.0; the weakest result among the Madison papers 
is for No. 134 with log odds of -.8. Paper 134, for all four sets of Poisson under-
lying constants, has log odds pointing mildly in the wrong direction. As a whole, 
though, the log odds for all sets of underlying constants and both models point 
consistently and forcibly in the right direction. Just how strong these odds 
are can be better appreciated by consulting the following brief table of antilogs. 
Log odds 
Odds 
Log odds 
Odds 
0 
1 to 1 
4 
55 to 
.1 
1.1 to 
5 
150 to 
,5 
1.6 to 
10 
22,000 to 1 
1 
e ~ 2.7 to 
15 
3.3 X 106 to 1 
2 
7 to 1 
20 
480 X 106 to 1 
3 
20 to 1 
25 
7.1 X 109 to 1 
* The computation of log odds was programmed for digital computer by Miles 
Davis. 

Hamilton 
Paper 
number 
1 
6 
7 
8 
9 
11 
12 
13 
15 
16 
17 
21 
22 
23 
24 
25 
26 
27 
28 
29 
TABLE 3.4-1 
-:t 
o 
TOTAL NATURAL LOG ODDS FOR 98 PAPERS OF KNOWN AUTHORSHIP 
TOTAL FOR 30 FINAL VVORDS 
6 SETS OF UNDERLYING CONSTANTS, 2 DISTRIBUTIONS 
MODAL ApPROXIMATION 
Negative binomial 
Poisson 
Paper length 
Set of underlying constants 
Set of underlying constants 
(thousands of words) 
22 
33 
38 
21 
11 
31 
22,31 
33 
38 
11,21 
1.6 
13.9 
11.9 
13.5 
15.7 
13.7 
14.8 
22.9 
20.5 
22.4 
25.7 
1.9 
16.8 
15.7 
16.4 
17.7 
17.4 
17.5 
27.4 
25.5 
26.1 
29.2 
2.2 
16.6 
14.3 
13.8 
17.5 
14.4 
18.0 
36.6 
33.7 
33.7 
39.0 
2.0 
14.0 
13.0 
16.5 
16.5 
16.3 
15.1 
20.7 
18.3 
21.4 
23.8 
1.6 
11.6 
10.2 
11.2 
12.6 
12.5 
11.8 
16.7 
15.3 
15.8 
18.0 
2.5 
16.3 
15.4 
15.6 
17.2 
16.6 
17.4 
30.7 
28.7 
28.4 
32.2 
2.1 
13.0 
11.3 
12.0 
14.1 
13.4 
13.8 
25.2 
23.0 
23.7 
27.3 
1.0 
7.8 
7.3 
7.4 
8.0 
7.7 
8.0 
12.0 
11.4 
11.1 
12.4 
3.1 
20.3 
18.5 
18.4 
21.5 
22.1 
21.7 
48.9 
45.4 
45.2 
51.4 
1.9 
24.9 
23.1 
25.2 
27.5 
28.2 
26.8 
46.6 
42.5 
45.6 
51.0 
1.6 
19.8 
17.9 
19.1 
21.5 
19.8 
21.2 
32.0 
29.1 
31.1 
35.1 
2.0 
15.6 
14.0 
15.4 
17.3 
18.1 
16.9 
35.6 
32.2 
34.5 
39.0 
3.5 
30.2 
27.6 
29.1 
33.0 
32.8 
32.5 
68.4 
63.2 
64.4 
72.8 
1.8 
16.2 
15.1 
15.0 
16.9 
14.1 
18.0 
35.0 
32.1 
33.6 
38.0 
1.8 
15.0 
13.1 
13.7 
16.3 
14.9 
16.0 
27.1 
24.7 
25.6 
29.3 
2.0 
11.6 
10.3 
15.1 
14.4 
15.2 
12.5 
16.7 
14.2 
18.0 
20.0 
2.4 
16.5 
15.5 
16.8 
17.9 
16.4 
18.0 
36.1 
32.5 
35.7 
40.2 
1.4 
17.2 
15.9 
18.3 
19.4 
18.1 
18.7 
25.6 
23.1 
25.4 
28.5 
1.6 
13.3 
12.8 
12.2 
13.2 
12.6 
13.7 
19.4 
18.6 
17.4 
19.6 
2.2 
27.8 
25.6 
24.1 
28.7 
26.0 
29.5 
56.9 
53.3 
53.2 
59.9 

30 
2.0 
16.7 
13.9 
11.7 
17.2 
31 
1.7 
24.8 
21.6 
18.6 
25.4 
32 
1.4 
3;3 
3.4 
3.9 
3.2 
33 
1.6 
9.7 
8.3 
5.7 
9.1 
34 
2.2 
17.1 
15.6 
13.0 
16.8 
35 
2.2 
14.1 
13.2 
12.5 
14.4 
36 
2.7 
22.1 
20.9 
21.7 
23.6 
59 
1.8 
14.0 
13.7 
14.5 
14.7 
60 
2.2 
14.9 
13.4 
12.8 
15.3 
61 
1.5 
10.0 
9.3 
10.1 
10.7 
65 
2.0 
16.5 
14.3 
12.7 
17.0 
66 
2.2 
18.8 
16.5 
15.1 
19.5 
67 
1.4 
11.7 
10.9 
7.8 
10.6 
68 
1.5 
9.7 
8.9 
10.3 
10.6 
69 
2.6 
23.3 
21.1 
19.6 
24.0 
70 
3.0 
16.6 
14.9 
18.0 
19.2 
71 
1.7 
12.0 
10.6 
15.1 
14.6 
72 
2.0 
17.1 
15.6 
17.5 
18.9 
73 
2.3 
14.9 
12.6 
10.9 
15.2 
74 
0.9 
12.4 
11.4 
12.8 
13.6 
75 
1.9 
16.5 
15.3 
15.9 
17.4 
76 
1.8 
18.7 
16.5 
16.3 
19.9 
77 
2.0 
18.9 
16.8 
15.1 
19.4 
101 
1.2 
6.5 
5.9 
7.0 
7.3 
102 
1.4 
15.7 
13.9 
15.4 
17.5 
111 
2.9 
11.9 
12.0 
9.9 
10.9 
112 
2.5 
9.3 
7.5 
7.6 
10.0 
113 
1.2 
3.0 
2.2 
4.6 
4.0 
12.9 
18.2 
43.4 
20.4 
26.5 
53.6 
2.7 
3.7 
4.8 
6.3 
10.4 
22.5 
14.7 
18.2 
38.1 
12.5 
16.0 
33.0 
24.0 
23.5 
45.3 
14.8 
15.0 
18.9 
14.1 
15.9 
29.4 
10.5 
10.5 
15.1 
15.2 
17.4 
35.5 
17.2 
20.0 
41.5 
8.5 
12.3 
22.8 
10.9 
9.9 
12.7 
21.4 
24.9 
52.0 
19.5 
17.6 
32.8 
13.8 
13.2 
19.8 
17.9 
18.2 
30.3 
10.4 
16.5 
40.2 
13.9 
12.8 
17.7 
14.5 
17.5 
31.3 
18.4 
19.8 
33.6 
17.1 
20.1 
38.1 
7.5 
6.7 
8.9 
16.0 
16.7 
26.0 
9.7 
12.6 
27.0 
8.8 
10.2 
23.4 
4.7 
2.9 
2.8 
40.1 
39.4 
49.9 
49.1 
4.5 
3.9 
21.1 
19.4 
36.1 
34.1 
30.7 
30.4 
42.5 
42.6 
17.9 
17.5 
27.3 
26.6 
14.0 
14.1 
33.0 
32.2 
38.6 
37.9 
22.0 
19.6 
11.7 
12.2 
48.7 
47.6 
29.5 
32.7 
16.7 
21.5 
27.5 
29.8 
36.9 
36.5 
16.2 
17.6 
28.6 
30.5 
31.0 
31.3 
35.5 
34.7 
8.1 
8.4 
23.3 
25.4 
26.6 
23.4 
21.1 
21.2 
2.0 
3.2 
45.8 
56.2 
4.7 
22.9 
39.1 
34.9 
47.6 
19.7 
30.7 
16.0 
37.2 
43.6 
22.6 
13.7 
54.3 
36.8 
24.0 
33.5 
42.6 
'19.6 
34.2 
35.8 
39.9 
9.6 
28.8 
26.5 
25.3 
3.8 
(cont.) 
-l 
...... 

Madison 
Paper 
number 
10 
14 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
121 
122 
131 
132 
133 
TABLE 3.4-1 (cant.) 
--l 
tv 
TOTAL NATURAL LOG ODDS FOR 98 PAPERS OF KNOWN AUTHORSHIP 
TOTAL FOR 30 FINAL WORDS 
G SETS OF UNDERLYING CONSTANTS, 2 DISTRIBUTIONS 
MODAL ApPROXIMATION 
Negative binomial 
Poisson 
Paper length 
Set of underlying constants 
Set of underlying constants 
(thousands of words) 
22 
33 
38 
21 
11 
31 
22,31 
33 
38 
11,21 
3.0 
-17.5 
-17.2 
-16.6 
~18.2 
-19.8 
-18.3 
-30.5 
-29.5 
-28.5 
-31.0 
2.1 
-20.0 
-18.5 
-20.6 
-22.6 
-24.2 
-20.6 
-28.7 
-26.5 
-28.8 
-31.5 
2.7 
-20.2 
-18.9 
-19.8 
-23.4 
-25.5 
-21.2 
-32.7 
-30.4 
-33.0 
-35.8 
3.3 
-16.5 
-15.3 
-17.8 
-19.6 
-21.8 
-17.5 
-25.4 
-22.9 
-27.5 
-29.2 
2.6 
-24.6 
-23.6 
-24.5 
-26.6 
-28.9 
-25.8 
-45.1 
-42.5 
-43.8 
-47.7 
2.7 
-19.2 
-18.5 
-19.7 
-20.9 
-21.9 
-20.3 
-30.1 
-28.6 
-29.3 
-31.7 
3.5 
-15.6 
-15.1 
-15.4 
-17.5 
-18.9 
-16.5 
-'-27.6 
-26.5 
-26.3 
-28.7 
2.7 
-11.9 
-11.1 
-11.0 
-13.4 
-15.2 
-12.4 
-21.1 
-20.0 
-21.2 
-22.6 
3.0 
-28.5 
-27.2 
-28.2 
-31.2 
-35.6 
-29.6 
-48.8 
-46.7 
-46.7 
-50.8 
2.6 
-20.8 
-19.7 
-21.0 
-23.0 
-26.5 
-21.8 
-34.3 
-32.3 
-33.4 
-36.6 
2.1 
-14.2 
-12.9 
-15.7 
-16.5 
-17.6 
-15.1 
-19.6 
-17.3 
-20.5 
-22.8 
2.6 
-27.0 
-25.7 
-27.0 
-29.5 
-32.3 
-28.2 
-44.4 
-42.1 
-43.0 
~47.0 
2.5 
-21.9 
-21.2 
-22.0 
-23.9 
-27.6 
-22.9 
-36.9 
-35.2 
-35.3 
-38.3 
1.6 
-13.1 
-12.5 
-12.6 
-13.9 
-15.1 
-13.4 
-18.3 
-17.6 
-17.5 
-19.0 
3.0 
-14.9 
-14.2 
-15.1 
-16.1 
-18.4 
-15.8 
-25.5 
-23.9 
-24.9 
-27.5 
2.8 
-24.7 
-23.8 
-25.9 
-27.2 
-29.8 
-26.7 
-45.2 
-41.9 
-45.1 
-49.7 
2.9 
-16.6 
-14.9 
-18.7 
-19.8 
-22.7 
-17.6 
-26.7 
-23.6 
-28.2 
-31.0 
2.7 
-20.3 
-19.3 
-20.5 
-22.9 
-26.7 
-21.7 
-31.9 
-29.3 
-32.1 
-35.1 
2.5 
-13.3 
-11.7 
-14.0 
-15.6 
-16.9 
-13.6 
-20.4 
-18.7 
-20.6 
-22.7 

134 
1.7 
-0.8 
-0.1 
-1.7 
-1.9 
135 
3.0-
-20.7 
-19.5 
-22.0 
-24.1 
141 
3.6 
-18.5 
-17.4 
-19.9 
-20.9 
201 
2.0 
-21.1 
-20.5 
-19.8 
-21.9 
20'2 
2.0 
-13.1 
-12.6 
-11.5 
-14,1 
203 
2.0 
-17.2 
-15.9 
-17.8 
-19.4 
204 
2.0 
-22.8 
-21.3 
-24.2 
-25.4 
205 
1.9 
-14.5 
-14.1 
-14.6 
-15.3 
206 
1.9 
-18.3 
-17.5 
-17.7 
-20.1 
207 
1.9 
-19.0 
-18.0 
-20.3 
-21.3 
208 
2.0 
-21.4 
-20.7 
-21.4 
-23.1 
209 
2.1 
-13.7 
-13.0 
-14.1 
-16.0' 
210 
2.1 
-26.4 
-25.2 
-26.2 
-28.7 
211 
2.0 
-21.7 
-20.5 
-22.2 
-23.9 
212 
2.2 
-14.7 
-14.0 
-14.9 
-17.0 
213 
2.0 
-16.9 
--16.0 
-16.4 
-18.3 
214 
2.0 
-12.7 
-11.2 
-13.8 
-14.9 
215 
2.0 
-19.3 
-17.9 
-20.3 
-22.0 
216 
2.0 
-5.7 
-5.2 
-6.2 
-6.5 
217 
2.2 
-20.6 
-19.9 
-19.7 
-22.0 
218 
2.0 
-18.9 
-17.6 
-19.7 
-21.4 
219 
2.1 
-16.8 
-15.6 
-17.7 
-19.1 
220 
2.0 
-15.3 
-14.6 
-14.5 
-16.3 
301 
2.0 
-14.1 
-13.3 
-14.1 
-15.5 
302 
2.9 
-17.6 
-16.6 
-17.4 
-19.5 
311 
1.5 
-14.8 
-13.9 
-14.8 
-16.1 
312 
1.4 
-9.9 
-9.4 
-9,3 
-10 .. 2 
313 
1.5 
-4.0 
-3.8 
-3.6 
-4.0 
314 
1.5 
-15.1 
-14.2 
-15.2 
-16.6 
315 
1.5 
-15.3 
-14.2 
-15.S 
-17.2 
316 
1.5 
-9.S 
-9.2 
-9~6 
-11.2 
-2.4 
-0.8 
0.9 
-27.0 
-22.0 
-34.7 
-23.9 
-19.7 
-28.5 
-23.6 
-21.7 
-33.7 
-15.3 
-13.7 
-24.6 
-21.2 
-17.9 
-26.6 
-26.8 
-24.4 
-36.9 
-14.0 
-15.4 
-21.0 
-18.9 
-19.5 
-32.6 
-23.7 
-20.3 
-28.0 
-26.3 
-22.4 
-34.6 
-14.8 
-14.7 
-25.1 
-31.5 
-27.3 
-40.8 
-25.5 
-22.8 
-30.8 
-17.0 
-15.4 
-25.7 
-21.1 
-17.3 
-24.6 
-16.0 
-13.0 
-13.0 
-25.4 
-20.0 
-27.1 
-7.4 
-6.3 
-9.6 
-24.2 
-21.3 
-35.8 
-23.3 
-19.8 
-34.2 
-19.8 
-17.6 
-23.5 
-16.8 
-15.7 
-26.4 
-17.3 
~14.5 
-19.9 
-21.4 
-18.3 
-32.9 
-17.5 
-15.3 
-19.6 
-10.4 
-10.1 
-14.0 
-4.0 
-4.1 
-5.3 
-17.1 
-15.5 
-20.3 
-IS.7 
-15.7 
-20.0 
-12.6 
-10.6 
-15.9 
1.4 
0.3 
-32.2 
-34.7 
-26.3 
-28.5 
-32.5 
-31.7 
-23.5 
-23.8 
-24.5 
-26.6 
-33.3 
-37.6 
-19.6 
-20.5 
-30.4 
-32.7 
-25.8 
-28.4 
-33.0 
-33.4 
-23.1 
-26.3 
-38.9 
-39.4 
-28.6 
-3004 
-24.0 
-26.4 
-;23.6 
-23.5 
-11.4 
-13.6 
-25.1 
-27.2 
-8.5 
-9.8 
-34.6 
-33.8 
-31.4 
-34.2 
-21.5 
-23.8 
-25.1 
-25.2 
-18.8 
-19.2 
-31.1 
-31.6 
-18.3 
-19.3 
-13.4 
-13.2 
-5.3 
-4.S 
-19.0 
-20.0 
-IS.5 
-19.9 
-14.3 
-16.5 
0.1 
-38.0 
-31.1 
-34.5 
-25.7 
-28.9 
-41.6 
-22.5 
-35.5 
-31.3 
-36.2 
-28.4 
-42.9 
-33.4 
-28.2 
-25.6 
-15.0 
-29.6 
-11.1 
-36.7 
-37.5· 
-26.1 
-27.5 
-21.1 
-34.7 
-21.3 
-14.4 
-5.2 
-21.S 
-21.S 
-IS.2 
--l 
W 

Paper 
number 
Joint 
18 
19 
20 
Disputed 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
62 
63 
TABLE 3.4-2 
~ 
~ 
TOTAL NATURAL LOG ODDS FOR THE PAPERS OF JOINT AND 
DISPUTED AUTHORSHIP 
TOTAL FOR THE 30 FINAL WORDS 
6 SETS OF UNDERLYING CONSTANTS, 2 DISTRIBUTIONS 
MODAL APPROXIMATION 
Negative binomial 
Poisson 
Paper length 
Set of underlying constants 
Set of underlying constants 
(thousands of words) 
22 
33 
38 
21 
11 
31 
22,31 
33 
38 
11,21 
2.1 
-11.0 
-10.8 
-9.0 
-11.4 
-11.2 
-11.4 
-20.1 
-19.5 
-18.9 
-20.5 
2.0 
-12.1 
-12.0 
-10.8 
-12.2 
-12.9 
-12.5 
-18.6 
-18.4 
-16.7 
-18.3 
1.4 
-4.6 
-5.0 
-1.9 
-3.6 
-3.3 
-4.6 
-7.0 
-7.6 
-5.8 
-6.0 
1.6 
-13.2 
-12.2 
-12.9 
-14.6 
-15.8 
-13.4 
-18.1 
-17.1 
-17.6 
-19.3 
1.1 
-14.3 
-13.7 
-13.7 
-15.1 
-15.9 
-14.5 
-18.2 
-17.5 
-17.4 
-18.9 
1.9 
-21.9 
-20.9 
-22.1 
-24.0 
-25.4 
-23.0 
-33.4 
-31.3 
-32.7 
-35.9 
1.8 
-16.0 
-15.7 
-15.0 
-16.5 
-17.1 
-16.6 
-23.1 
-22.5 
-21.6 
-23.4 
2.2 
-15.8 
-15.0 
-16.2 
-17.4 
-18.5 
-16.4 
-22.0 
-20.7 
-21.7 
-23.6 
2.0 
-14.3 
-13.6 
-13.2 
-15.7 
-16.1 
-14.8 
-22.9 
-21.7 
-22.7 
-24.3 
2.0 
-5.8 
-5.5 
-5.9 
-6.2 
-6.4 
-6.1 
-7.1 
-6.6 
-6.9 
-7.6 
1.6 
-8.7 
-8.2 
-8.8 
-9.6 
-9.9 
-9.0 
-10.6 
-10.0 
-10.4 
-11.4 
2.2 
-16.7 
-15.7 
-17.2 
-18.4 
-20.8 
-17.6 
-26.1 
-24.2 
-25.9 
-28.6 
2.1 
-18.0 
-p.1 
-17.6 
-19.4 
-21.5 
-18.5 
-26.3 
-25.1 
-25.2 
-27.4 
2.4 
-16.5 
-16.0 
-16.0 
-17.3 
-17.5 
-17.3 
-26.9 
-25.6 
-25.6 
-28.0 
3.0 
-18.5 
-17.7 
-17.7 
-19.6 
-21.1 
-19.1 
-32.2 
-31.2 
-30.2 
-32.9 

3.4] 
LOG ODDS 
75 
Since we know who wrote each of these papers, the log odds in Table 3.4-1 
offer a check on the method. We have used these papers in estimating parame-
ters and have used the log odds for words in the choice of final words, and so 
we might expect the method to work a bit better here than elsewhere; and we 
discuss how much better in Section 4.8. But the results here seem very strong 
and satisfactory. 
We can examine the effect of varying the underlying constants. The values 
of the {3's for each of these sets are shown in Table 3.2-2. As a rough rule, the 
changes run to 10 per cent of the log odds. The changes for these and other 
sets of underlying constants are studied in Section 4.5E, and summarized in 
Section 3.7B. 
Visual inspection will assure the reader that the variation in log odds from 
one set of underlying constants to another is modest compared to the variation 
from one paper to another. Different paper lengths explain only part of this 
variation; the rest is primarily natural random variation. Examine, for example, 
the log odds for the Madison papers Nos. 201-220, all of almost equal length. 
The changes in log odds from the negative binomial distribution to the 
Poisson are huge, running between 50 and 150 per cent increases! This shows 
at once that the choice of data distribution does matter immensely. For the 
moment, the reader may wish to be conservative and attend to the negative 
binomial odds-if median odds of three million to one can ever be called con-
servative. 
Readers of Damon Runyon may remember that Ii ••• nothing between human 
beings is 1 to 3 .... " * How can anyone so believe his model as to accept as 
remotely reasonable odds of millions to one, when only 48 papers by Hamilton 
and 50 papers by Madison are in hand? The question is indeed appropriate, 
and the Poisson log odds are not defensible. But we believe that the negative 
binomial log odds are defensible, at least after we make the adjustments pre-
sented in Section 3.7. The basis is primarily the moderate contributions from 
each of many nearly independent words. We postpone further discussion to 
Sections 3.5B, 3.7E, and 3.7F where we treat possibilities outside the framework 
of the mathematical model. 
3.4B. The disputed papers. Kext, the piece de resistance, Table 3.4-2, 
presents total log odds for the joint and disputed papers. Attending to the 
12 disputed papers, we see that every set of underlying constants gives odds for 
all papers strongly in favor of Madison. The weakest of these are for papers 
Nos. 55 and 56, and the lowest odds for No. 55 are 240 to 1 (e 5. 5) in favor of 
Madison-not absolutely overwhelming, in the language of Section 3.4A. 
Essentially, No. 55 does not have its share of marker words, no matter who 
wrote the paper, and the high-frequency words produce no information. 
* We are indebted to Frank Anscombe, who recalled that the passage occurs in the 
story "A Nice Price" in the book Money From Home. 

TABLE 3.5-1 
'" 
LOG ODDS BY WORD GROUPS FOR SET 22 OF UNDERLYING CONSTANTS 
0) 
Negative binomial 
Poisson 
103 
II: 
Paper-
Word group 
tol 
Authorship 
number 
B3A 
B3B 
B3G 
B3E 
B3Z 
B3A 
B3B 
B3G 
B3E 
B3Z 
.a:: 
:>-
Hamilton 
1 
4.7 
2.0· 
1.9 
3.1 
2.3 
11.6 
2.8 
2.2 
4.0 
2.3 
.... 
Z 
6 
2.5 
9.2 
1.7 
.3 
3,2 
5.2 
14.3 
2.1 
.7 
5.2 
Ul 
7 
6.4 
4.6 
-.2 
2.6 
3.3 
23.3 
6.9 
-.3 
3.0 
3.8 
103 
8 
1.2 
3.0 
4.9 
2.2 
2.8 
2.1 
5.1 
7.4 
2.4 
3.6 
q 
t::I 
9 
2.9 
3.5 
2.6 
1.0 
1.6 
6.0 
5.4 
3.0 
1.2 
1.2 
>-< 
11 
3.2 
8.2 
-.2 
1.5 
3.6 
8.8 
14.2 
-.3 
2.2 
5.9 
12 
4.5 
3.4 
.2 
1.9 
3.1 
12.6 
5.8 
-.1 
2.7 
4.2 
111 
2.6 
11.9 
1.9 
-3.0 
-1.4 
7.4 
22.7 
2.3 
-3.4 
-1.8 
112 
5.6 
-1.3 
2.2 
-.7 
3.4 
19.8 
-3.6 
2.5 
-1.2 
5.9 
113 
-.3 
-1.2 
.8 
.9 
2.8 
-.9 
-1.4 
.8 
1.1' 
3.2 
Madison 
10 
-6.5 
-6.8 
-.9· 
-2.1 
-1.2 
-9,1 
-14.1 
-1.9 
-3.5 
-1.8 
14 
-5.0 
-7.9 
-1.4 
-1.9 
-3.8 
-6.6 
--12.3 
-1.5 
-2.5 
-5.8 
37 
-3.2 
-9:2 
-3.1 
-1.3 
-3.4 
-5.5 
-16.1 
-3.4 
-1.2 
-6.5 
38 
.3 
-4.6 
-6.4 
-3.0 
-2.9 
.8 
-8.4 
-10.1 
-3.4 
-4.4· 
39 
-5.8 
-B.7 
-.8 
-2.7 
-3.6 
-7.9 
-21.1 
-1.4 
-10.0 
-4.7 
40 
-6.0· 
-8.5 
-.3 
-.9 
-3.5 
-8.3 
-16.0 
-.5 
-.7 
-4.7 
41 
-1.3 
-6.9 
-1.3 
1.6 
-1.7 
-10.7 
-14.3 
-1.9 
2.4· 
-3.0 
132 
-3.2 
-7.0 
-5.4 
-2.4 
-2:a 
-5.5 
-10.0 
-10.8 
-2.2 
-3.5 
133 
-5.6 
-1.4 
-1.4 
-1.9 
-2.9 
-7.5 
-4.8 
-1.6 
-2.4 
-4.1 
1'34 
-4.2 
5:0 
-.3 
.2 
-1.1 
-5.2 
7.8 
-.1 
.4 
-2.0 
Joint 
18 
-2.1 
-'-8.1 
1.3 
-1.0 
-1.1 
-3.6 
-14.4 
1.9 
-2.7. 
-1.3 
19 
-4.8 
-7.6 
-.9 
1.4 
-.2 
-6.2 
-13.0 
-.9 
1.7 
-.2 
20 
-.9 
-7.6 
.8 
1.0 
2.0 
-1.6 
-9.9 
.7 
1.1 
2.6 
Disputed 
49 
-4.0 
-5.5 
-.8 
-1.3 
-1.6 
-4.9 
-9.4 
-.9 
-1.1 
-1.9 
50 
-2.9 
-9.0 
-1.1. 
.2 
-1.5 
-3.4 
-12.2 
-1.2 
.3 
-1.8 
51 
-4.6 
-9.3 
-3.8 
-1.9 
-2.4 
-5.8 
-16.4 
-5.4 
-2.6 
-3.3 
52 
-4.4 
-lO.2 
.2 
.2 
-1.8 
-5.6 
-15.9 
.1 
.4 
-2.2 
53 
-5.1 
-6.4 
-4.6 
1.4 
-1.2 
-6.6 
-10.1 
-5.4 
1.7 
-1.7 
54 
-.2 
-8.6 
-1.3 
-2.3 
-1.9 
-.6 
-15.2 
-1.7 
-3.1 
-2.3 
55 
-4.8 
-.1 
.8 
-.7 
-1.0 
-6.2 
1.1 
.6 
-1.4 
-1.3 
56 
-3.9 
-2.4 
-3.1 
1.0 
-.4 
-4.8 
-3.1 
-3.5 
1.2 
-.4 
C;; 
57 
-5.1 
~5.9 
-2.6 
-.9 
-2.1 
-6.7 
-10.9 
-5;4 
-.8 
-2.4 
58 
-4.9 
-8.6 
-1.3 
-1.3 
-2.0 
-6.4 
-15.1 
-1.5 
-1.0 
-2.4 
en 
62 
-5.5 
-8.1 
-.2 
-1.5 
-1.2 
-7.3 
-14.1 
-.9 
-3.2 
-1.5 
63 
-6.6 
-8.4 
-1.5 
.2 
-2.3 
-9.2 
-19.1 
-1.6 
.6 
-2.9 

3.5] 
LOG ODDS BY WORDS AND WORD GROUPS 
77 
Among the joint papers, No. 20 looks especially mixed, but the small log odds 
are confounded with the brevity of the paper. The matter is further compli-
cated because Madison's notes suggest that he borrowed much of it from Felice 
and Sir William Temple. We discuss this further in Section S.4. 
3.5. LOG ODDS BY WORDS AND WORD GROUPS 
3.5A. Word groups. The total log Qdds makes a strong prediction in the 
right direction for almost all of the papers of known authorship, and strong 
predictions for Madison for each disputed paper. To show how consistently the 
different word groups behave and to what extent each contributes to the total, 
we present, in Table 3.5-1, the log odds for each of the five word groups, for 
set 22 of underlying constants, and both distributions. All disputed and joint 
papers are included, but only a selection from the papers of known authorship. 
For each author, we give his first 7 papers from The Federalist, and, iIi addition, 
for Hamilton, Pacificus I, II, and III (code numbers 111, 112, 113); for 
Madison, Helvidius II, III, and IV (code numbers 132, 133, 134). Some ex-
terior papers are included to illustrate the consistency of behavior of word groups 
over changes in source of writing. The papers include numbers 113 and 134, 
the papers of Hamilton and Madison most poorly identified in total log odds. 
All groups look remarkably consistent, considering their different strengths, 
for a weak set should point in the wrong direction occasionally. This general 
consistency is a further sign in support of the method. 
The set B3B is stronger than B3A (upon), which in turn looks nearly as strong 
as the other three groups put together. Recall that B3B contains the high-
frequency function words: to, this, there, on, oj, by, an, also. So in the end, the 
high-frequency words outshone all the marker words. Although this does not 
prove that cleverness in selecting variables fails to pay, it does show that routine 
can pay. 
3.5B. Single words. The contribution of each separate word to the log odds 
is shown in Table 3.5-2 for the negative binomial distribution having set 22 as 
underlying constants for all disputed and joint papers and for some of the known 
papers that were used in Section 3.5A. The log odds for upon have been given 
already in Table 3.5-1 as word group B3A and are not repeated. For a few 
words, the Poisson log odds for set 22 are shown alongside those for the negative 
binomial. 
The words are divided into three groups: in Table 3.5-2A the 9 words of 
highest frequency, in Table 3.5-2B the 11 Hamilton marker words, and in 
Table 3.5-2C the 9 Madison marker words. For the Hamilton markers, every 
negative log odds means that the word did not occur. For the Madison markers, 
every positive log odds corresponds to a nonoccurrence of the word. 

TABLE 3.5-2A 
-1 
00 
LOG ODDS FOR SINGLE WORDS: 9 HIGH-FREQUENCY WORDS 
>'3 
Word: 
an 
of 
there 
this 
to 
both 
though 
by 
by* 
on 
on* 
~ 
Authorship 
trl 
Number: 
4 
39 
55 
57 
58 
78 
160 
13 
13 
40 
40 
=:: 
Paper number 
po 
..... 
Z 
Hamilton 
1 
.6 
.5 
-.8 
.6 
.9 
.6 
-.5 
.0 
.2 
-.4 
-.6 
U2 
6 
.2 
1.7 
2.1 
-.3 
-1.5 
.1 
.7 
1.9 
3.2 
4.3 
6.7 
>'3 
c:1 
7 
.7 
1.7 
2.1 
1.0 
-.3 
.9 
-.8 
-1.2 
-2.2 
-.2 
-.1 
t:I 
8 
.6 
.7 
-1:3 
.4 
.1 
.8 
.7 
1.8 
3.2 
.5 
1.2 
t< 
9 
.3 
1.9 
-.1 
-.3 
-.4 
.7 
.0 
1.6 
2.6 
.7 
1.3 
n 
.4 
1.5 
1.4 
1.0 
-.8 
1.0 
-.8 
.6 
1.3 
3.7 
6.8 
Madison 
10 
-.3 
-1.7 
.0 
-1.5 
-.8 
-1.2 
-1.0 
-2.2 
-4.9 
-.9 
-1.9 
14 
-.5 
-.7 
-3.2 
-.2 
-.7 
.9 
-.1 
.1 
.3 
-2.4 
-4.8 
37 
-.9 
-.7 
-2.1 
-.2 
-1.3 
.4 
-.3 
-1.3 
-2.6 
-1.5 
-3.2 
38 
.1 
-.9 
-2.1 
.2 
-.6 
-1.4 
-.5 
-1.2 
-2.7 
.7 
2.0 
39 
-1.7 
-.1 
-3.7 
.5 
-.5 
-.5 
.1 
-1.9 
-3.9 
-4.1 
-9.7 
40 
-.7 
-1.0 
-2.1 
.0 
.1 
.4 
-.3 
-4.5 
-10.4 
.8 
1.9 
Joint 
18 
-1.3 
.3 
-.7 
.3 
-2.3 
.2 
1.1 
-3.1 
-6.0 
-1.8 
-3.4 
19 
-.3 
.0 
-2.1 
-.2 
-1.6 
.8 
.3 
-.8 
-1.4 
-2.7 
-5.4 
20 
-.5 
-1.0 
2.2 
-.5 
-1.4 
.0 
.0 
-1.0 
-1.6 
.1 
.4 
Disputed 
49 
.6 
-.2 
-.8 
-1.1 
-.2 
.7 
-.5 
.1 
.3 
-3.5 
-6.5 
50 
-.6 
-.7 
-1.8 
,0 
-1.6 
.5 
-.4 
-.3 
-.3 
-3.3 
-5.2 
51 
-1.1 
-.3 
.0 
.0 
-2.1 
-.3 
-.6 
-1.3 
-2.3 
-4.1 
-8.5 
52 
-1.3 
-.9 
-2.8 
.4 
.1 
.1 
-.6 
-1.3 
-2.1 
-3.6 
-7.1 
53 
-1.2 
-.3 
-1.5 
.0 
-.7 
-.7 
-.7 
-2.5 
-4.8 
1.4 
2.8 
54 
-.3 
-.5 
-2.1 
1.2 
-1.3 
-.3 
.3 
-1.8 
-3.3 
-3.6 
-7.2 
55 
-1.8 
-.2 
.5 
-.3 
.0 
.2 
-.1 
1.1 
2.1 
.6 
1.4 
56 
-1.2 
1.0 
.0 
.4 
-2.1 
-.5 
-.5 
1.2 
1.9 
-1.4 
-2.4 
57 
-1.2 
.9 
-.2 
-.3 
-.7 
.2 
-.7 
-1.1 
-2.0 
-3.0 
-6.3 
58 
-.4 
-.9 
-1.4 
.0 
-1.5 
.2 
-.1 
-.7 
-1.2 
-3.3 
-6.8 
62 
.1 
-.2 
-3.4 
.3 
-.8 
-.6 
-.2 
-1.4 
-2.6 
-2.6 
-5.5 
63 
.4 
-.5 
.8 
-.5 
-2.0 
-1.2 
-.4 
-4.1 
-10.1 
-1.6 
-3.5 
C;; 
Note: Set 22 of underlying constants; columns marked * give Poisson log odds; all others are negative binomial. 
Cl1 

TABLE 3.5-2B 
~ 
LOG ODDS FOR SINGLE WORDS: 11 HAMILTON MARKERS 
~ 
Authorship 
Word: enough 
while 
always 
commonly considerable 
apt 
direction 
vigor 
kind 
matter 
probability 
Number: 
90 
116 
123 
80 
82 
124 
87 
110 
143 
146 
153 
Paper number 
Hamilton 
1 
.8 
-.1 
.5 
.9 
-.2 
1.4 
-1 
.5 
-.6 
.9 
.8 
6 
.8 
-.2 
-.6 
-.2 
.9 
-.3 
.8 
-.1 
1.3 
-.4 
.7 
7 
-.2 
-.2 
-.7 
.9 
-.3 
.8 
-.1 
-.1 
.4 
.7 
1.4 
8 
.9 
-.2 
1.7 
.9 
.4 
-.3 
.6 
.9 
-.8 
-.4 
1.4 
9 
.8 
.9 
-.5 
-.1 
.4 
-.2 
.7 
-.1 
.6 
.8 
-.2 
11 
-.2 
-.2 
-.8 
-.2 
.8 
-.4 
-.1 
.9 
1.6 
-.5 
.6 
Madison 
10 
-.2 
-.3 
.8 
-.3 
-.5 
.7 
-.1 
-.2 
-1.0 
-.6 
.6 
t"' 
14 
-.2 
-.2 
-.7 
-.2 
-.3 
-.3 
-.1 
-.1 
-.8 
-.4 
-.3 
0 
37 
-.2 
-.2 
-.8 
-.2 
-.4 
.3 
-.1 
-.2 
-1.0 
.6 
-.4 
0 
38 
-.3 
-.3 
-1.0 
-.3 
-.5 
-.5 
-.2 
-.2 
-1.1 
.5 
-.5 
0 tj 
39 
-.2 
-.2 
-.8 
-.2 
-.4 
-.4 
-.1 
-.2 
-.9 
-.5 
-.4 
tj 
40 
-.2 
-.2 
-.8 
-.2 
-.4 
-.4 
-.1 
-.2 
-1.0 
-.6 
-.4 
Ul 
t:d 
~ 
Joint 
18 
.8 
-.2 
.3 
-.2 
.4 
-.3 
-.1 
-.1 
-.8 
-.4 
-.3 
::;J 
19 
-.2 
-.2 
-.6 
.9 
-.3 
-.3 
.6 
-.1 
-.8 
-.4 
-.3 
0 
20 
-.1 
-.1 
.5 
-.1 
.5 
-.2 
-.1 
.5 
-.6 
1.6 
-.2 
!;li 
tj 
Ul 
Disputed 
49 
-.1 
-.1 
.4 
-.1 
-.2 
-.2 
-.1 
-.1 
-.6 
-.3 
-.2 
>-
50 
-.1 
-.1 
-.3 
-.1 
-.2 
-.1 
-.1 
-.1 
-.4 
-.2 
-.1 
Z 
tj 
51 
-.2 
-.2 
-.6 
-.2 
-.3 
-.3 
-.1 
-.1 
-.7 
-.4 
-.3 
52 
-.2 
-.2 
.4 
-.1 
-.3 
-.3 
-.1 
-.1 
-.7 
-.4 
-.3 
::;J 
53 
-.2 
-.2 
-.7 
-.2 
-.3 
.8 
-.1 
-.1 
-.8 
1.5 
-.3 
0 
!;li 
54 
-.2 
-.2 
-.6 
-.2 
.4 
-.3 
-.1 
-.1 
-.8 
-.4 
-.3 
tj 
55 
-.2 
-.2 
.3 
-.2 
.4 
-.3 
-.1 
-.1 
-.8 
-.4 
-.3 
0 
56 
-.1 
-.1 
-.5 
-.1 
.5 
-.2 
-.1 
-.1 
.6 
-.3 
-.2 
!;li 
0 
57 
-.2 
-.2 
.3 
-.2 
-.3 
-.3 
-.1 
-.1 
-.8 
-.5 
-.3 
q 
58 
-.2 
-.2 
-.6 
-.2 
-.3 
-.3 
-.1 
-.1 
-.8 
-.4 
-.3 
'"t:I 
Ul 
62 
-.2 
-.2 
.2 
-.2 
.3 
-.3 
-.1 
-.2 
-.9 
-.5 
-.3 
63 
-.2 
-.3 
-.9 
.8 
-.5 
-.4 
-.2 
-.2 
-1.1 
-.6 
-.4 
-l 
Note: Set 22 of underlying constants; all entries are negative binomial log odds. 
<:0 

TABLE 3.5-2C 
00 
0 
LOG ODDS FOR SINGLE WORDS: 9 MADISON MARKERS 
come-
in1lOlla-
partic-
accord-
accord-
8 
Word: 
al80 
although 
quently 
tion 
language 
marly 
work 
whilst 
whilst* 
ing 
ing* 
~ 
Authorship 
trJ 
Number: 
3 
73 
81 
94 
96 
151 
165 
ll7 
ll7 
ll9 
119 
~ 
Paper number 
1-1 
Z 
Hamilton 
1 
.4 
.1 
.4 
.1 
.1 
.3 
.1 
.4 
.5 
.4 
.6 
Ul 
6 
.5 
.1 
.5 
.1 
.1 
.3 
.1 
.5 
.6 
-.6 
-.5 
8 
7 
.6 
.2 
.6 
.1 
.1 
.4 
.2 
.6 
.8 
.6 
.9 
q 
t:l 
8 
-.1 
.1 
.5 
.1 
.1 
.3 
.2 
.5 
.7 
.5 
.8 
~ 
9 
-.1 
.1 
.4 
.1 
.1 
.3 
-.7 
.4 
.5 
.4 
.6 
11 
.0 
.2 
.6 
.1 
.2 
.4 
.2 
.7 
.8 
.6 
1.0 
Madison 
10 
.8 
.2 
-.7 
.2 
.2 
-.3 
.2 
.8 
1.0 
-1.2 
-2.7 
14 
.0 
.2 
-1.5 
-.7 
-.5 
.3 
-.8 
-l.l 
-1.0 
.6 
.8 
37 
-.9 
-.7 
-1.4 
-.7 
-l.l 
.4 
-.7 
-1.0 
-.8 
-.4 
-.1 
38 
-.7 
-1.0 
-.6 
.2 
-.4 
-.3 
-.6 
-1.5 
-2.4 
-.8 
-1.2 
39 
.0 
.2 
.6 
-.7 
.2 
-.9 
.2 
.7 
.9 
-2.2 
-9.5 
40 
-.9 
.2 
.7 
-1.0 
.2 
-.4 
.2 
.7 
.9 
-.4 
-.1 
Joint 
18 
.6 
.1 
.5 
.1 
.1 
.3 
.2 
-'-1.1 
-1.0 
-1.4 
-3.1 
19 
.5 
.1 
.5 
.1 
.1 
.3 
.2 
-l.l 
-1.0 
.5 
.8 
20 
-.8 
.1 
.4 
.1 
.1 
.2 
.1 
.4 
.5 
.4 
.6 
Disputed 
49 
-.2 
.1 
-1.0 
.1 
.1 
.3 
-.6 
-1.2 
-1.2 
.4 
.6 
50 
-.3 
-.9 
.3 
.1 
.1 
-.6 
.1 
.3 
.3 
.3 
.4 
51 
-.1 
.1 
-1.6 
.1 
.1 
-1.0 
.1 
-1.8 
-2.9 
.5 
.8 
52 
-.6 
.1 
.5 
.1 
.1 
-.5 
.1 
.5 
.6 
.5 
.7 
53 
-1.4 
-.8 
.5 
-1.0 
.1 
-.4 
.2 
-l.l 
-1.0 
.6 
.9 
54 
.0 
-.8 
-1.5 
.1 
.1 
-.5 
.2 
.5 
.7 
-.5 
-.4 
55 
.0 
.1 
.5 
.1 
.1 
.3 
.2 
.5 
.7 
-l.l 
-1.7 
56 
-.2 
.1 
.4 
.1 
.1 
-.5 
.1 
-1.2 
-1.2 
.4 
.6 
57 
.0 
.2 
.6 
-.7 
.1 
.3 
.2 
-2.1 
-4.6 
-.5 
-.4 
58 
.0 
-.8 
-.9 
.1 
.1 
-.5 
.2 
.6 
.7 
.5 
.8 
62 
.0 
.2 
-1.9 
.1 
.1 
.4 
.2 
.6 
.8 
.6 
.9 
63 
-.8 
.2 
.7 
.2 
.2 
-.3 
.2 
-.9 
-.7 
-.3 
-.1 
~ 
en 
Note: Set 22 of underlying constants; columns marked! give Poisson log odds; all others are negative binomial. 

3.5] 
LOG ODDS BY WORDS AND WORD GROUPS 
81 
From the log odds for single words we see that, with the exception of the two 
words upon and on, the total log odds are composed of moderate contributions 
frorn many words. Taking mean log odds over the 12 disputed papers, upon 
gives -4.3, on gives -2.3, the total for 30 words gives -15.0. Thus the total 
log odds has as its components about 30 per cent from upon, 15 per cent from on, 
and 55 per cent from the remaining 28 words. In Section 3.7E, the magnitude of 
these contributions is examined in more detail. If on and upon are excepted, only 
three instances of odds higher than 20 to 1 remain for single words in the dis-
puted and joint papers. Ail in all, this slight dependence on huge contributions 
by single words makes the final odds much more acceptable. 
The contribution from low-frequency words comes mostly from an occurrence 
of the word. "No occurrence" usually has a small effect, rising for the strongest 
low-frequency marker whilst to between .4 and .8, depending on paper length. 
Most failures of low-frequency word groups to discriminate correctly are caused 
by the occurrence of too few words. 
The log odds for Madison paper No. 39 based on according illustrates how the 
negative binomial stamps down the odds compared to the Poisson, when a low-
frequency marker word has an unusually high rate. In paper 39, 9 occurrences 
of according give a Poisson log odds of -9.5 which is reduced to -2.2 for the 
negative binomial! It comes to this: the negative binomial provides automatic 
damping fot low-frequency words and thus prevents words from getting badly 
out of hand. We eliminated most words with severe outliers, but we still need 
something like this negative binomial effect to protect us against the ever-
present possibilities of an unexpectedly high rate. 
3.5C. Contributions of marker and high-frequency words. What log 
odds would be assigned to a paper containing each Hamilton marker once, no 
Madison markers, and each high-frequency word at the Hamilton mean rate? 
Table 3.5-3 presents the answer to this and similar questions. Our purpose is 
to see whether either author is being treated unfairly by our words, or put 
another way, whether there are delicate matters that need our attention. 
We divided the 30 final words into 10 high-frequency words, 11 Hamilton 
markers, and 9 Madison markers corresponding to sections A, B, C of Table 
3.5-2, with upon added to the high-frequency words. Then, we obtained log 
odds for twelve artificial 2000-word papers which are formed to give the 12 
(2 X 2 X 3) combinations of 1 Or 0 occurrences of every Hamilton marker, 
1 or 0 occurrences of every M:idison marker, and high-frequency words at rates 
nearest to the Hamilton mean rate, the Madison mean rate, or the average mean 
rate. Table 3.5-3 shows for each combination the log odds under the negative 
binomial having set 31 as underlying constants, and it shows the contribution 
of each of the three component groups of words. 
The log odds in such artificial papers show no gross weighting toward one 
author or the other, and what asymmetry there is seems to favor Hamilton. 
Thus, in total, the paper with Hamilton usage in all three categories has log 

82 
THE MAIN STUDY 
TABLE 3.5-3 
CONTRIBUTIONS OF MARKER AND HIGH-FHEQUENCY WORDS, 
2000-W ORD PAPERS 
SET 31 OF UNDERLYING CONSTANTS, NEGATIVE BINOMIAL 
[3.5 
11 Hamilton markers (word numbers 80, 82, 87, 90, 110, 116, 123, 124, 143, 146, 153) 
Expected number of occurrences for Hamilton: 7.0; for Madison: 2.4 
o occurrences of each Hamilton marker give log odds -4.4 
1 occurrence of each Hamilton marker gives log odds 
7.6 
9 Madison markers (word numbers 3, 73, 81, 94, 96,117,119,151,165) 
Expected number of occurrences for Hamilton: 2.3; for Madison: 6.4 
o occurrences of each Madison marker give log odds 
3.5 
1 occurrence of each Madison marker gives log odds 
-6.0 
10 high-frequency words (numbers 4, 13, 39, 40, 55, 57, 58, 60, 78, 160) 
Occurrences of each at the Hamilton mean rate give log odds 
12.3 
Occurrences of each at the Madison mean rate give log odds 
-11.1 
Occurrences of each at the average mean rate give log odds 
1.8 
Total log odds for 30 words 
Hamilton markers 
None 
1 each 
None 
Madison markers 
None 
None 
1 each 
j 
Homilton 
mean 
11.4 
23.4 
2.0 
High-frequency 
Madison 
word rate 
mean 
-11.9 
.1 
-21.4 
l 
Average 
1.0 
13.0 
-8.5 
1 each 
1 each 
14.0 
-9.4 
3.5 
odds 23.4, the corresponding Madison paper, -21.4, and the "in-between" paper 
with no markers, 1.0. 
Among the high-frequency words, the excess of 12.3 over -11.1 is due mostly 
to upon. The 11 Hamilton markers give log odds 7.6, stronger than the -6.0 
obtained from 9 Madison markers. In partial compensation, the expected num-
ber (7.0) of Hamilton markers in a Hamilton paper is not quite as large a fraction 
(7.0/11) of the 11 Hamilton markers as the expected number (6.4) of Madison 
markers in a Madison paper is of the 9 Madison markers. Expected log odds are 
studied in more detail in Section 4.8. 
In the main study, we assess the evidence of each word without regard to 
possible imbalance of Hamilton and Madison markers or of the potential log 
odds that can be attained. Nevertheless, we are pleased that possible log odds 
seem well in balance, as one protection against ill effects of violations of assump-
tions. Further, the slight weighting in favor of Hamilton means that any im-

3.6] 
LATE HAMILTON PAPERS 
83 
balance has not tended to increase the observed evidence for Madison in the 
disputed papers. Not that we have anything against Madison, but as the data 
have turned out, their evidence is just a bit easier to defend than equivalent 
results in the direction of Hamilton would be. 
3.6. LATE HAMILTON PAPERS 
As a separate validation of the main study, we checked the method on four 
late Hamilton Federalist papers, numbers 79, 80, 82, and 85, which Hamilton 
wrote especially for the McLean edition. From the point of view of the main 
study these are fresh untouched papers that did not contribute to the determi-
nation of the constants. We chose the four shortest papers among the papers 
numbered 78 through 85. 
Considering their lengths, all four papers give strong log odds for Hamilton, 
as they should. Table 3.6-1 shows the total log odds and its breakdown by 
word groups, and at the top, the lengths of the papers. 
TABLE 3.6-1 
LOG ODDS FOR FOUR LATE HAMILTON Federalist PAPERS 
NEGATIVE BINOMIAL DISTRIBUTION 
SET 22 OF UNDERLYING CONSTANTS 
Paper number: 
79 
80 
82 
85 
Paper length 
(thousands of words): 1.0 
2.3 
1.5 
2.5 
Word group 
B3A 
1.8 
3.5 
3.2 
6.5 
B3B 
1.9 
5.7 
9.0 
6.1 
B3G 
-.7 
1.9 
.3 
2.6 
B3E 
.9 
.2 
.2 
-1.0 
B3Z 
.2 
2.2 
-.3 
1.2 
Total (30 words)* 
4.0 
13.5 
12.4 
15.4 
* Total log odds may disagree with sum of log odds 
by word groups because of rounding. 
The short paper No. 79 has the weakest log odds, while the other three are 
satisfactorily high. 
The log odds may be compared with expectations if the model and prior 
distributions used were correct, as is done for the papers of disputed and of 
known authorship in Section 4.8. The log odds for papers 82 and 85 are slightly 
above expectation; for paper 80, slightly below; for paper 79, about I! standard 
errors below expectation. Results for individual words indicate that too few 
Hamilton and too many Madison marker words occurred. The high-frequency 
words and upon behaved according to expectations. 

84 
THE MAIN STUDY 
(3.7 
3. 7. 
ADJUSTME~TS TO THE LOG ODDS 
When we presented the log odds in earlier sections, we put off an examination 
of the many underlying assumptions and approximations. In this section, we 
plug up this hole through studies that estimate corrections to the log odds for 
the disputed papers. Although most of these studies are specialized, they suggest 
that per cent reduction in log odds gives a good way to extrapolate to our 
problem. Consequently, the adjustments reduce extreme odds most heavily. 
The reader must understand our intent. The assumptions and approximations 
were introduced to make the entire study feasible. We merely try to establish 
the general order of magnitude of adjustments needed to allow for the errors 
introduced. To get accurate corrections would, we believe, be as difficult as 
carrying out the analysis without the assumptions and approximations. 
3.7 A. Correlation. Treating words as independent in the calcll-Iation of log 
odds may either inflate or deflate the estimated odds. In Section ~.7, an in-
vestigation of the effect of correlation on 11 high-frequency words suggests that 
the log odds based on these words should, on the average, be adjll-sted toward 
zero by an amount variously estimated between .6 and 1.2. The adjustment 
amounts to 6 per cent to 12 per cent of the average log odds for these 11 words, 
and we apply the same range of per cent reduction to the total log odds. 
An alternative way of viewing the correlation adjustment is instructive. 
Among the final 30 words, on and upon form the one pair where a correlation 
would be clearly expected. Since upon and on are the two stropgest words, the 
need to adjust for their correlation was evident before we undertook any general 
exploration of correlations among words. We expected that the log odds based 
on independence would need to be deflated, and found for this pair of words 
that the log odds should be shifted toward zero by about .9, or by about 
13 per cent of the combined log odds for these two words. This.9 reduction in 
log odds is in the middle of the range appropriate as the total adjustment for 
the 11 high-frequency words, so that the correlations beyond that between 
upon and on produce very nearly no net adjustment. The result of the study of 
correlations among the 11 high-frequency words can thert be given the alternative 
summarization: the log odds from on and upon should be reduced by about 
13 per cent; the net effect of other correlations tends neither to reduce nor to 
increase the log odds. 
3.7B. Effects of varying the underlying constants that deterllline the 
prior distributions. Set 22 of underlying constants, 
{Jl = 10, {J2 = 0, {J3 = 12, {J4 = 1.25, {J5 = 2, 
determines the prior distribution that seems best by the estimates made on 
pools of words. These underlying constants are not precisely determined, and 
over a range of values that seem very likely to bracket the true prior, the log 
odds might vary up to about 12 per cent above or below the log odds for set 22. 

3.7] 
ADJUSTMEN'rs TO THE LOG ODDS 
85 
Most of the allowance is due to the choice of i31, i32' An increase of one unit 
in i31 decreases log odds by about two per cent; an increase of one unit in i32 is 
nearly equivalent to an increase of 1.5 in i31' The range 5 ~ i31 + 1.5i32 ~ 15 
is an adequate bracket. The effect of changes in i33, i34, i35 is much less, and the 
suggested total allowance of ±12 per cent is generous. 
3.7C. Accuracy of the approximate log odds calculation. How accurate 
is the modal approximation to log odds? For five of the final 30 words, we here 
describe some answers obtained by the approximate method described in 
Section 4.6. The five words 40 (on), 58 (to), 60 (upon), 90 (enough), 117 (whilst), 
include the two strongest words (60, 40), the strongest rare word (117), the word 
with the highest non-Poissonness (90), and a word (58) with very high rate and 
considerable non-Poissonness. 
Th~ modal estimates iiI, ii2, 0-, r, 5r, 52 are given in Table 3.2-3 for all 30 
words. For these five words, the modes 0-, r, 51, 52 and approximate standard 
deviations of the posterior distributions of these four parameters are shown in 
Table 3.7-1. If the posterior distribution of 0-, r, 51, 52 could be regarded as 
approximately normal, the correction to the modal approximation would need 
take account only of the variances and covariances and the second derivatives 
of the negative binomial probabilities for the unknown papers. 
TABLE 3.7-1 
MODES AND ApPROXIMATE STANDARD DEVIATIONS OF POSTERIOR 
DISTRIBUTIONS OF (J, T, 01, 02 
IJ 
T 
01 
02 
40 on 
Mode 
11.13 
.304 
.37 
.44 
S.D. 
.45 
.018 
.14 
.15 
58 to 
Mode 
76.01 
.537 
.41 
.48 
S.D. 
1.20 
.008 
.15 
.16 
60 upon 
Mode 
3.49 
.930 
.29 
.48 
S.D. 
.25 
.018 
.13 
.32 
90 enough 
Mode 
.36 
.721 
.54 
.64 
S.D. 
.09 
.073 
.26 
.46 
117 whilst 
Mode 
.51 
.161 
.27 
.20 
S.D. 
.08 
.051 
.26 
.13 
Note: Set 22 of underlying constants. 

86 
THE MAIN STUDY 
[3.7 
But the non-Poissonness rates are non-negative quantities, and such large 
standard deviations relative to the modes, as exhibited in Table 3.7-1, indicate 
that the distributions must be severely skewed with long tails to the right. 
Most of the main study has been based, not on 01 and 02, but on transformed 
parameters ~ and TI, chosen (in Section 3.2E) to reduce the long tails of the 
distributions of the o's. To study the accuracy of the modal approximation, we 
transformed again, from ~ to log ~, to reduce still further the asymmetry and 
provide better approximations. 
After transforming, we estimated the adjustments required in the log odds 
for the range of occurrences actually found. Generally, but not always, the 
calculations implied a reduction in log odds toward zero. For four of the five 
words investigated, the changes, word by word, averaged roughly from 5 
per cent to 15 per cent of the estimated log odds. The largest adjustment is for 
the word upon-for which reductions of 25 per cent and 15 per cent seem ap-
propriate for Hamilton and Madison, respectively. As an overall net correction, 
a reduction of 15 per cent from this source might be applied to the negative 
binomial log odds, perhaps 18 per cent for Hamilton and 12 per cent for Madison, 
because upon is such an important word and its correction has a larger impact 
on Hamilton than on Madison. 
3.7D. 
Changes in word counts. The recent Cooke edition (1961) of 
The Federalist calls attention to many changes between the original newspaper 
edition and various book editions. This valuable list came too late to affect our 
calculations, but two changes are worth noting here. 
Cooke says that one of the two whilst's in paper No. 51 was added by Hamilton 
in preparing the McLean edition, on which our work is largely based. If, as some 
have argued, Hamilton made use of newspaper text with Madison's own changes 
in preparing papers by Madison for the McLean edition, then Hamilton's anoma-
lous use of whilst would be explained, and we might even be justified in using 
the McLean text. We are not especially competent to decide this issue, and so 
we rely on the original newspaper edition as the standard. This path of least 
resistance is the easier because the paper itself is very strongly decided without 
the further whilst. 
Removing the extra whilst is straightforward, and it reduces the log odds for 
paper No. 51 based on whilst from -1.9 to -1.2, and the unadjusted total log 
odds from -21.9 to -21.2, all based on set 22 of underlying constants. 
In paper No. 58, an upon in the newspaper text was deleted from the McLean 
edition. Putting it back reduces the log odds given by upon from -4.9 to -2.1, 
and the unadjusted total log odds from -18.0 to -15.2. 
These two changes show that shifts of one marker word can have a substantial 
effect on the log odds. Nevertheless, the log odds that remain are still enormous, 
and we emphasize that no other changes of single words could have caused so 
large a change in the log odds as did either of these. 
The counts for individual words are subject to error, notwithstanding the 
checks we have built into the counting and recording processes. A final check 

3.7] 
ADJUSTMENTS TO THE LOG ODDS 
87 
on the word counts for the 30 words used in the calculation of log odds showed 
some minor errors, and we here indicate the nature of the changes in log odds 
that are induced. The log odds presented do not incorporate these changes; 
rather, we treat the counting errors as an additional source of error in the 
analysis and make a rough allowance for its effect on the log odds. 
From nine errors among the joint and disputed papers, the two largest changes 
are one of .5 in favor of Hamilton in the log odds for paper No. 56 from an 
extra occurrence of considerable, and a change of .4 in favor of Madison in 
paper No. 51 from an extra occurrence of particularly. No other change exceeds 
.25 in log odds, and the changes are nearly balanced in direction. 
Some larger errors occurred among the papers of known authorship where 
fewer checks were earlier employed. The effect of these errors on the estimation 
of parameters would be negligible. The effect on the log odds assigned each paper 
when treated as an unknown would be larger, but these log odds were only il-
lustrative and no essential changes occur. 
The direction of changes induced by the errors in counts appears unrelated 
to author, and has no consistent strengthening or weakening effect on the log 
odds. An allowance of plus or minus three per cent in the log odds seems ade-
quate for this source of variation. 
TABLE 3.7-2 
ApPROXIMATE ADJUSTED TOTAL NATURAL LOG ODDS 
TOTAL FOR THE FINAL 30 WORDS 
Adjusted for 
Adjusted log odds 
Paper 
combined estimated reductions 
number word changes 
Median 
Maximum 
Minimum 
only 
(23%) 
(40%) 
(3%) 
Joint 
18 
-11.0 
-8.5 
-6.6 
-10.7 
19 
-12.1 
-9.3 
-7.3 
-11.7 
20 
-4.6 
-3.5 
-2.8 
-4.5 
Disputed 
49 
-13.2 
-10.2 
-7.9 
-12.8 
50 
-14.3 
-11.0 
-8.6 
-13.9 
51 
-21.2 
-16.3 
-12.7 
-20.6 
52 
-16.0 
-12.3 
-9.6 
-15.5 
53 
-15.8 
-12.2 
-9.5 
-15.3 
54 
-14.3 
-11.0 
-8.6 
-13.9 
55 
-5.8 
-4.5 
-3.5 
-5.6 
56 
-8.7 
-6.7 
-5.2 
-8.4 
57 
-16.7 
-12.9 
-10.0 
-16.2 
58 
-15.2 
-11.7 
-9.1 
-14.7 
62 
-16.5 
-12.7 
-9.9 
-16.0 
63 
-18.5 
-14.2 
-11.1 
-17.9 

88 
THE MAIN STUDY 
[3.7 
3.7E. Approximate adjusted log odds for the disputed papers. The 
adjustments discussed in the preceding sections may be combined to give, besides 
the specific adjustments for word changes given in Section 3.7D, an estimated 
23 per cent reduction of the total log odds for each paper, based on the log odds 
for set 22 of underlying constants. Taking the extremes of each adjustment 
range leads to a range of reductions between 3 per cent and 40 per cent. 
These values arise from independently applying the midpoints or the ends of 
the ranges of each reduction: 9 ±3 per cent for correlation, 0 ±12 per cent for 
choice of prior distribution, 15 ±5 per cent for modal approximation, 0 ±3 
per cent for errors in word count. 
Table 3.7-2 shows, for each disputed paper and joint paper, the log odds 
adjusted for specific word changes, and the final log odds based on the median 
estimated reduction, and on each of the two extremes of estimated reductions. 
3.7F. Can the odds be believed? Even after adjustment, some of the odds 
in the preceding section exceed a million to one. The average of the log odds 
for the "median" adjustments comes to about -11 or odds of about 60,000 to 1. 
Such odds seem large, indeed scary, and in common sense one shies from them. 
To understand them in their proper perspective requires both a fair apprais~l 
of the model's strength and a sensible attitude toward what we shall call out-
rageous events. We feel that the calculated odds after adjustment are believable 
in the absence of outrageous events, but the possibility of such events might 
require serious reduction of the more extreme odds. 
Strength of the model. To justify the odds is to justify the choice of data distri-
bution. The general study of such choices and assessments of the reliance to be 
placed on the choices is a vastly important, yet too little investigated, subject. 
In our own problem, we assume that the allowances for selection effects, for 
length of paper, and for estimation of parameters are roughly satisfactory; thus 
we assume that the reliability of data distributions is the critical issue. For 
simplicity, assume that we have for each author observations on 50 papers of 
equal lengths. 
Consider first the odds achievable from a single dichotomous variable, say the 
presence or absence of some marker word. 
Suppose the word is present in 
49 Hamilton papers and in 1 Madison paper. With just this information, an 
odds factor more extreme than 49 or i9 is unjustifiable, and one less extreme is 
appropriate when one remembers errors in estimation. Results from several 
such marker words cannot be compounded to improve the odds without outside 
evidence for their independence. 
On the other hand, for markers of lower 
quality than 49 to 1, odds may be built up to a total near 49 to 1 by near inde-
pendence that can be checked by the available data. A look at the log odds for 
the disputed papers by single words in Table 3.5-2 shows that, except for 
upon, on, by, and there, single words never contribute above 12 to 1, and marker 
words rarely above 5 to 1 odds, and then only on the basis of double occurrences. 
vVe feel, on balance, that the odds from marker words are not in serious excess, 

3.7) 
ADJUSTMENTS TO THE LOG ODDS 
89 
even before the adjustments for correlation and errors in the modal approxi-
mations. 
For words of moderate to high frequency, larger odds can be achieved, but 
the distributional problems are harder. 
For these words, we have internal 
evidence for modest dependence, and we have studied the amount of adjustment 
needed to be applied to the odds compounded under independence. For each 
single word, we have used a negative binomial distribution. Studies of the dis-
tributions for large numbers of words make the negative binomial appear satis-
factory for all but a few words, and these have long since been eliminated. 
To illustrate the distributional problem, consider upon, the one word that 
gives a huge contribution to the odds. Our discussion is based on the frequency 
distribution for upon in the 48 Hamilton and 5Q Madison papers as given in 
Table 2.1-4 or Table 4.8-3. Let us neglect variation of paper length. 
Consider an observed rate of 4.5 in an unknown paper. The likelihood ratio 
is the quotient of a small Hamilton probability, fairly well determined, and a 
tiny Madison probability. The determination of the latter depends critically 
on the distribution chosen to extrapolate the Madison data to rates far above 
those observed, and consequently the likelihood ratio may be very large but is 
also sensitive to the choice. For observed rates above zero and below two, the 
likelihood ratio is more firmly determined, and moderate odds are justified. 
Finally, consider zero occurrences. For Madison the probability is well esti-
mated as about .8, while for Hamilton, the estimate is small for any reasonable 
family of distributions we might choose, say io or less, which implies a likelihood 
ratio of 40 or more. The support for a reasonable family is that the observed 
distribution for upon is quite regular, and well fit by a negative binomial. So 
too are the distributions for many other words. There is not, among these words, 
evidence of frequent aberrant observations. In fact, after the 15 per cent re-
duction for modal approximation, the odds contributed py no occurrence of 
upon in a 2000-word paper are just about 60 to 1, strong but not much beyond 
what sense easily allows. Fortunately, none of the disputed papers show high 
rates of upon. The believability of the huge odds based on upon for papers with 
Hamilton-like rates would be much more suspect than the more modest ones 
for papers with Madison-like rates. 
For other high-frequency words, much the same analysis is I1Ppropriate, 
except that the authors' distributions overlap more, so that the observed results 
never fall so far into the tail of either distribution. Only on gives consistently 
strong odds, but from the alternative explanation of the correlation effect, we 
may allocate the bulk of the correlation adjustment to the on-upon correlation, 
and tl1ke the adjustment away from the odds for on. The effect is roughly to 
halve the log odds for on, and then the odds rarely exceed 10 to 1. 
As one final indication of acceptability of the log odds based on the negative 
binomial distribution, the study of Section 4.9 shows that probability predic-
tion::; under the negative binomial model are not excessive when judged by a 
criterion that strongly penalizes strong incorrect predictions by single words. 

90 
THE MAIN STUDY 
[3.7 
The unadjusted log odds for the negative binomial (words: by, on, and to) 
perform remarkably well without deflation. In contrast, the Poisson log odds 
for these words are best halved. 
So much for justification of the odds given by our model. 
Outrageous events. One could say: "Without judging the matter, let us suppose 
that you have handled these technical details with surpassing skill; are you not, 
thus far, neglecting some earthy possibilities?" We must answer "Yes," and try 
to indicate a few kinds of things that might seriously deflate the model's odds. 
Although the chances of a blunder in large-scale calculations are fairly high, 
we do have parallel work in the other three studies that is largely independent, 
and that offers some checks on the work-not precise checks, but checks on 
reasonableness that reduce the chances of grave blunders. Other sorts of poor 
workmanship may recommend themselves to the reader. 
The reader has to ask himself whether we have somehow falsified the evidence. 
Deliberate fraud or hoax is not impossible in scientific work; at a guess, the 
chance is well over one in a million. In this problem, such activities could be 
expensive to detect if we managed to suppress words that were favorable to 
Hamilton in the disputed papers, but left good discrimination elsewhere. 
Certainly every reader wonders whether Hamilton wrote the disputed papers 
and Madison edited them so fiercely that for the variables we use Hamilton's 
traces have been washed away. Historical evidence would be important here, 
but we think it unlikely that these authors would mark up one another's manu-
scripts in that manner. If one or two marker words were at issue, we could 
imagine the possibility of such changes' or even of slight printer's errors' mat-
tering, but with so many small words the possibility seems remote. 
How could Madison change a Hamilton paper of 2000 words, so that our 30 
final words appear with average Madison rates instead of average Hamilton 
rates? He would have had to remove about 50 occurrences of Hamilton words 
among the final 30, and to add about 20 occurrences of Madison words. 
If instead of thinking in terms of a single paper written by Hamilton and 
heavily revised by Madison, we were to think of this possibility for all 12 dis-
puted papers, the results of the regression study of Section 4.8 would be relevant. 
That study shows that word rates in the 12 disputed papers are very near to 
the Madison rates, and the log odds are close to predictions from theory, given 
Madison as author. Thus, judged by our 30 words, the disputed papers are much 
like typical Madison papers .. For Madison to have edited more than a few 
Hamilton papers and not have some of the resulting papers appear like a 
Hamilton-Madison mixture or a Madison parody instead of a fresh Madison 
paper would be remarkable indeed. 
Still the chance of this does not seem so remote that odds of millions to one 
are appropriate. If you thought the odds were 1 to 1000 that Hamilton wrote 
a disputed paper but that it was thoroughly edited by Madison, then 1000-to-l 
odds are the most that Madison can have for the paper. 
More generally, let P(M) be the probability that Madison wrote the paper-, 
let 1 -
a be the probability that no outrageous event occurred, let 1 -
b be 

3.7] 
ADJUSTMENTS TO THE LOG ODDS 
91 
the probability that Madison wrote the paper, given that no outrageous event 
occurred, and let A be the probability that Madison wrote the paper, given that 
an outrageous event did occur. Then 
P(M) = (1 -
a)(l -
b) + aA, 
and we suppose that a, b are small. Note that P(M) is least when A = 0, so 
that the minimum odds in favor of Madison are 
(1 -
a)(l -
b)/I1 -
(1 -
a)(l -
b)]. 
Roughly this is l/(a + b). If a is much smaller than b, then the odds are roughly 
l/b, which is the point we have been making. But going to the other extreme, 
if a is large and b small, then the odds are roughly l/a. So, for example, for 
a ~ 10-3, b ~ 10-6, the odds go down from 106 to 103 but for a ~ 10-3, 
b ~ 10-2, the odds only go down from 100 to 1 to about 90 to l. 
Clearly, frauds, blunders, poor workmanship, and coincidences snip away at 
very high odds, and the reader has to supply the adjustment for these. The 
chance of being brought to what we call "roguish ruin" by one or another sort 
of outrageous event is sufficiently large that final odds of millions to one cannot 
be supported, but these long odds can be understood in their own place within 
the mathematical model. Whenever strong discrimination or precise estimation 
exists, something akin to these odds of millions to one arises, no matter what 
approach to statistics is being used. For example, a difference between means of 
10 standard deviations may be reported. When we see such a report, we all go 
through the same process of having reservations, of wanting to verify the cor-
rectness or incorrectness of the results, and of continuing to hold reservations if 
verification isnot feasible. (We ask: Has someone multiplied by an extra vn? 
Isn't the wrong measure of variance being used?) But when the results are the 
usual one to three standard deviations, we are more relaxed. A good many 
people on first hearing about odds of millions to one are shocked and feel that 
the methods are automatically discredited. But outrageous events have an 
impact on all approaches to inference, not just on the Bayesian approach. We 
have tried to explain both the sense in which long odds or huge differences can 
occur and the need for reservations in making final appraisals. We do not, 
however, wish to be whipped by the backlash of the question, "Don't you trust 
yourselves?" Yes, we do, but not millions to one. 
Finally, we do not regard the need for discussion of outrageous events as a 
shortcoming of our analysis. Rather it shows its strength. Had we obtained 
modest odds of say five to one, the chances of outrageous events would scarcely 
modify the results, and the emphasis of the discussion would have been on the 
uncertainty of the attribution, and whether five to one is of much help to 
historians. Strong results force the discussion of outrageous events. 

CHAPTER 4 
Theoretical Basis 
of the Main Study 
This chapter is a sequence of technical appendices to Chapter 3. These ap-
pendices indicate solutions to some methodological problems that arise in 
applying Bayes' theorem to the analysis of data. In a textbook, Chapter 4 
would be starred; the chapters that follow are not dependent on the material in 
this chapter, and most readers would do well to skip it on first reading. Unlike 
the other chapters, this chapter is written only for those seriously interested in 
statistical theory. We hope that, beyond documentation, the solutions presented 
may stimulate others to make improvements, generalizations, or to create analo-
gous solutions for new problems. 
Rarely could we adapt any available development for our use. For example, 
the problems in using Bayes' theorem to eliminate large numbers of nuisance 
parameters have only recently begun to be faced. Then, too, in the main study 
we work with observations whose different distributions have no sufficient 
statistics, a situation for which knowledge is, not so justifiably, slight. But 
this inability to find adaptable solutions plagues us, even in a non-Bayesian 
analysis of identically, normally distributed observations. How do you set 
confidence limits on a likelihood ratio at a given point between two normal 
distributions whose parameters are estimated in the usual way? (Section 5.5.) 
We feel obliged to provide treatments of techniques that are not new with us 
and are known to many practicing statisticians, yet, as far as we know, are not 
available in the literature. The development of the asymptotic expansions in 
Section 4 6 is a prime example. The delta method is old and much used, mostly 
for finding standard errors. So, too, the Laplace integral expansion and the 
asymptotic behavior of the log-likelihood function are common knowledge, 
although their applications in statistics are usually limited to the leading term. 
Straightforward extensions and interrelations of these methods apparently are 
not available. Plugging these gaps in the statistical literature lengthened the 
course of our research and swelled this report. 
Ou):' developments are often lengthy and at times quite rough. This arises, in 
part, out of a need to introduce approximations at almost every stage in order to 
92 

4.1] 
THE NEGATIVE BINOMIAL DISTRIBUTION 
93 
make any progress. We see a desperate need for better approximate procedures-
better in representing the problem more faithfully and in being of more general 
applicability, yet still feasible computationally. 
The remainder of this introduction guides the reader to the contents and the 
interrelations of the sections of this chapter. Sections 4.1 and 4.4 develop proper-
ties of the Poisson and negative binomial families of distributions. The discussion 
in Section 4.4 of appropriate shapes for the likelihood··ratio function may 
suggest new ways to choose the form of distributions. 
In Section 4.3, we describe an abstract structure for our problem; we derive 
the appropriate formulas for our applications of Bayes' theorem and give a formal 
basis for the method of bracketing the prior distribution. 
Sections 4.2,4.5, and 4.10 deal with the central problem of the main study, i.e., 
getting the log odds of authorship. In Section 4.2, we treat the choice of prior 
distributions, the determination of the posterior distribution, and the computa-
tional problems in finding posterior modes. In Section 4.5, we give methods for 
choosing sets of underlying constants to bracket the prior distributions, and we 
explore the effects of varying the prior on the log odds. Section 4.10 provides 
details of a special difficulty, and its possible general value lies in illustrating 
how to investigate the effects of a split into two populations of what was thought 
to be a single population. 
In Sections 4.6 and 4.7, we study the magnitudes of effects of erroneous 
assumptions. Section 4.6 assesses the effect of using the posterior mode of the 
parameters as if it were exact. To make the assessment, we had to develop a 
general asymptotic theory of posterior densities for moderate sample sizes. 
Section 4.7 studies the effects of correlations between rates for different words, 
and we believe it suggests how similar studies could be made in other problems. 
Sections 4.8 and 4.9 present studies of the adequacy of assumptions. Section 
4.8 compares the performance of the log odds for the disputed papers with 
theoretical expectations. Section 4.9 measures the adequacy of distributional 
assumptions by setting up a scoring scheme that penalizes probabilistic pre-
dictions (quantitative statements about the probability of a future event) on the 
basis of the outcomes of the events. The methods of Section 4.9 should be 
generally applicable to the evaluation of probabilistic forecasts. 
4.1. THE NEGATIVE BINOMIAL DISTRIBUTION 
4.IA. Standard properties. For us the more important of the two data 
distributions used in the main study is the negative binomial. 
Given this 
distribution and a paper of length w, the probability of x occurrences of a word is 
( ) 
( ) _ 
(-I 
). - rex + K) ()X( 
)-(X+K). 
1 
p x -
jnb X Wp" K, WO -
X!r(K) 
Wo 
1 + Wo 
, 
x = 0, 1, 2, ... , 
W > 0, 
0 > 0, 
p, > 0, 
K > 0, 
KO = p,. 
The subscript "nb" is an abbreviation for "negative binomial." Since /(0 = Il, 

94 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.1 
any two of the quantities Wj.l, K, and wo determine the distribution, and we find 
it convenient to be able to shift from one set of parameters to another. The three 
parameters have the interpretations: j.l, the mean rate (per unit length); 0, the 
non-Poisson ness rate (per unit length); K, the exponent. * If the paper length 
W equals 1, one of the standard forms of the distribution results. 
The negative binomial is a two-parameter family of distributions that arises 
in inverse binomial sampling, in "contagious" generalizations of the Poisson 
distribution, and in many other ways. We use it as a data distribution because, 
with its second parameter, it can have greater variability relative to its mean 
than has the Poisson distribution, and because it fits fairly well. Essentially, 
the contagious interpretation is the one we use. 
By direct calculation, or by use of the mixture representation below, the first 
four cumulants ofa random variable x with the negative binomial density 
fnb(X I Wj.l, K, wo) are: 
E(x-) = Wp" 
(2) 
Vex) = Wj.l(1 + wo), 
K3(X) = wp,(1 + wo)(1 + 2wo), 
K4(X) = wj.l(1 + wo)(l + 6wo + 6W202). 
The cumulant-generating function is 
10g(E(etX)) = -Klog[l + wo(1 -
et)]. 
The convolution of two negative binomial distributions with the same non-
Poissonness, WO, is again a negative binomial. 
The Poisson distribution with mean Wj.l is the limiting distribution of the 
negative binomial when K ~ 00 (0 ~ 0) for fixed j.l. Thus 
(3) 
lim fnb(X I Wj.l, K, wo) = f p(x [ wp,). 
0->0 
K->OO 
(Recall that fp refers to the Poisson density.) For some parameter values, for 
example, small values of WO, one expects the Poisson distribution to give a good 
approximation to the negative binomial. In addition, correction terms to the 
Poisson approximation are available. For example, for x and j.l fixed, 
1 j.l2 a2fp(x I Wj.l) 
2 
f nb(X I Wj.l, K, WO) = f p(X I Wj.l) + 2" -
a 2 
+ 0(0 ) 
(4) 
K 
j.l 
= fp(x I Wj.l) {I + wo [(x -
Wj.l)2 -
~]} + 0(02). 
2 
Wj.l 
wp, 
This approximation is of little value in the Federalist application because, as with 
* Corresponds to the fixed number of successes required before stopping the sequence 
of trials in inverse binomial sampling. 

4.1] 
THE NEGATIVE BINOMIAL DISTRIBUTION 
95 
most such approximations, the percent error is high in the tails of the distri-
bution. Note, however, that for x within a few standard deviations of the mean 
Wp" the quantity in square brackets is a modest number not depending much 
on p, or 0, so that the error in the Poisson approximation depends primarily on 
wo (and, of course, on how many standard deviations x is from the mean). 
For our study, an important representation of a negative binomial probability 
is as a mixture of Poisson probabilities with a gamma distribution for the Poisson 
mean: 
(5) 
fnb(X I Wp" K, wo) = 10'" fp(x I wu)f'Y (u I K, ~) duo 
Here 
x = 0, 1,2, ... , u > 0, 
is the Poisson density at x with mean wu and 
( !) _ (1/ote-u/ouK- 1 
f'Y u I K, 0 -
r(K) 
, 
u > 0, 
0 > 0, 
K > 0, 
is the density function at u of a gamma distribution with mean KO = p" argument 
K (degrees of freedom 2K), and variance p,o = p,2/K. 
Equivalently, if X, 11 are random variables such that 11 has density 
u> 0, 
and X, given 11 = u, has conditional density 
fp(x I wu), 
x = 0, 1,2, ... , 
then x has unconditional density 
(6) 
fnb(X I wp" K, wo) = E{fp(x I w11)}, 
which is just an alternative way of writing Eq. (5). 
The mixture representation is useful in derivations of most of the preceding 
facts; for example, the variance: 
Vex) = E(V(x I 11» + V(E(x I 11» 
= E(w11) + V(w11) 
= wp, + w2p,0 
= wp,(l + wo). 
If fp(x I wu) as a function of u is expanded in a Taylor's series about u = p" 
term-by-term application of the expectation operator in Eq. (6) yields an expan-
sion of which Eq. (4) gives the first two terms. The Poisson limit of Eq. (3) 
follows immediately from Eq. (4). 

96 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.1 
4.1B. Distributions of word frequency. The mixture representation of 
Section 4.1A provides one possible rationale for the negative binomial as the 
distribution of word frequencies. If the use of a word within any paper follows 
a Poisson rate u, which in turn varies from paper to paper according to a gamma 
distribution, then the counts for papers of the same length vary according to 
the negative binomial. 
Within papers, quite apart from the model, we suppose that blocks of text of 
length 100 to 200 words do not all have the same Poisson rate as the total paper, 
but that the variation in rate within papers is not large compared to the variation 
between papers. On the other hand, we have not made a systematic study of 
this matter, and our only study of blocks was discussed in Section 2.3. 
We take all these models with a grain of salt, as we would take other arguments 
leading to other contagious distributions. The negative binomial has the general 
sort of variation that the data need, and is more tractable, vicious as it is, than 
other distributions we have considered. 
4.1C. Parametrization. We use the mean rate p, and the non-Poisson ness 
o as the fundamental parameters of the negative binomial family. Since this 
choice differs from the common use of p,and the index K, some explanation 
is in order. 
The parametrization of a family of distributions is largely a matter of con-
venience. Questions of approximation of distributions, comparisons of several 
distributions of the family, sampling theory, assessment of prior distributions, 
evaluations of probabilitie~, can lead to different choices of parameters. 
The mean p, is a natural choice for one parameter. For the negative binomial 
in the authorship problem, 0 has relatively stable behavior and simple interpre-
tations. 
We use the negative binomial distribution as a generalization of the Poisson 
distribution, and we expect words that discriminate well to have distributions 
near the Poisson limit. 
The parameter ° is a measure of non-Poissonness, with value zero for the 
Poisson limit. As noted following Eq. (4), the first-order correction to the 
Poisson approximation (within a few standard errors of the mean) depends 
essentially on 0, and not on p, or K,or 11K. Then, too, 0 measures the proportional 
increase per unit length of paper in the negative binomial variance wp,(1 + wo) 
over the Poisson variance wp,. 
Finally, on the basis of observations of estimated parameters for the dis-
tributions of many words, we find that non-Poissonness as measured by 0 seems 
not to depend on the mean rate p, for the word. For the Hamilton papers and 
for the Madison papers, Table 4.2-1, in the next section, shows method-of-
moments estimates of p" 0, and 11K for each of 22 function words. For these 
words, the estimates of 11K, unlike those of 0, vary (inversely) with p,. Un-
fortunately, Hamilton and Madison show differing non-Poissonness for the same 
word whether 0 or 11K is our measure. 

4.1] 
THE NEGATIVE BINOMIAL DISTRIBUTION 
97 
The most common parametrization for the negative binomial family in its 
interpretation as a contagious distribution is in terms of the mean rate }J- and 
index or exponent K. In the inverse binomial sampling interpretation, K is a 
known constant, but the unknown parameter is ordinarily 1/(1 + wo) rather 
than}J-. The pair (}J-, K) has a technical advantage in estimation because for them 
the Fisher information matrix is diagonal; thus the maximum likelihood esti-
mates of}J- and K are asymptotically independent. If several negative binomial 
populations are being sampled, estimation and inference are greatly simplified 
when Kl = K2. Then a beautiful property follows: the likelihood ratio, given 
observed x, is not only monotone in x but is even logarithmically linear in x. 
This advantage arises from the common K and not from the parametrization 
itself. 
The negative binomial density has its simplest form in terms of the param-
eters (0, K), and this pair is convenient for numerical computation and differen-
tiation. Almost equivalently, ((1 + wo)-l, K), the inverse binomial sampling 
parametrization would be equally convenient except for its dependence on the 
varying paper lengths in our problem. 
In the general use of the negative 
binomial as it two-parameter data distribution, the parametrization (0, K) has no 
evident virtues other than the simplicity of the density. 
4.1D. Estimation. Estimation of the parameters of the negative binomial 
distribution is messy at best, and if, as happens in this work, each observation 
comes from a paper of different length, the problem is aggntvated. Suppose 
frequencies XlJ ... , XJ in J papers of length WlJ ... , WJ, respectively, are 
distributed according to negative binomial distributions, with common rate 
parameters}J-, O. The likelihood function is 
P(Xl' ... , XJ I }J-, 0) 
J II !nb(Xj I Wj}J-, K, Wjo) 
j=l 
and there are no nontrivial sufficient statistics to simplify it. Even if the paper 
lengths were constant, the only simplification possible would be to the propor-
tions of observations at each frequency x, which are helpful, even then, only 
for rare words. 
Maximum likelihood estimates cannot be obtained explicitly, except that the 
sample mean rate x/w is the maximum likelihood estimate of}J- when all papers 
have the same length w. Most work on various estimators and their properties 
is of little direct use here, because the proposed methods require equal w's and 
estimate only K. An exception is our use of Anscombe's (1950) work on estima-
tion by the method of moments. 

98 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.1 
We make auxiliary use of moments: weighted moments, with weights chosen 
to be optimal at the Poisson limit, 
(7) 
_ 
LXj 
m-~, 
L.JWj 
1 
(X. 
)2 
V = -- LWj 2.. -
m 
. 
J -
1 
Wj 
The expectations of m and v are 
(8) 
E[mlp"c5] = p" 
E[ii I p" c5] = p,(1 + rc5), 
with 
1 
( 
LW2.) 
r = --
LWj -
__ 
3 
• 
J -
1 
LWj 
Equating m and v to their expectations gives us the usual estimates by the 
method of moments: 
* 
p, 
= m, 
(9) 
* 
{v-m} 
c5 = d = max 0, --:;n;:- . 
We have replaced any negative estimate of c5 by O. 
Asymptotic sampling 
variances of m and d suffice for our uses of these estimates. These asymptotic 
sampling variances are determined under the simplifying assumption that all 
paper lengths are equal to the mean iii of the actual lengths. Then 
(10) 
V(d) <::: 2(1 + Wc5)2 [1 + ~_ (2 + 3Wc5)]. 
Jw 2 
2p,] + wc5 
These approximations can be obtained, under the Wj = W assumption, by the 
usual delta-method computations from the results in Anscombe (1950}. 
The asymptotic efficiency of the estimate d is low for rare words. An improved 
estimate based on the proportion of 0 observations instead of on the second 
moment is not evidently available when the length of paper varies. For rare 
words we used the d's only as preliminary estimates in an iterative process, and 
so we have not hunted for improved estimates. 
In our application to the distribution of word occurrences, we have a Hamilton 
sample with 
J = 48, 
Jw = 94.0, 
w = 1.96, 
and a Madison sample with 
J =50, 
Jw = 113.6, 
w = 2.27, 
~W2 /~w = 2.40. 

4.2] 
ANALYSIS OF THE PAPERS OF KNOWN AUTHORSHIP 
99 
4.2. ANALYSIS OF THE PAPERS OF KNOWN AUTHORSHIP 
4.2A. 
The data: notations and distributional assumptions. 
We 
suppose the data consist of frequencies of each of N words in each of J 1 papers 
known to be written by Hamilton (author 1) and J 2 papers known to be written 
by Madison (author 2). Index words by n: n = 1,2, ... , N; index papers by 
(i,j): j. = 1, ... , Ji, i = 1,2. Denote by Xnij the frequency of word n in paper 
(i,j) , i.e. the jth paper by author i; denote by Wij the length (in thousands of 
words) of paper (i,j). Introduce J.Lni, Oni as parameters for word n and author i 
and let Kni = J.Lnd Oni. 
Use the notation X to denote the entire set {Xnij} of data; use Xn to denote 
the data for the nth word. Similarly, use the notation J.L to denote the entire 
set {J.Lni, Oni} of parameters; use J.Ln to denote the parameters of the nth word. 
Assume that word frequencies are independently distributed across words 
and papers according to negative binomial distributions. 
Formally, assume 
N 
2 
Ji 
(1) 
p( {Xnij} I {J.Lni, Oni}) = IT IT IT !nb(Xnij I WijJ.Lni, Kni, WijOni), 
n=l i=l j=l 
in which !nb(X I WJ.L, K, wo) is the negative binomial density with mean rate J.L, 
index K and non-Poissonness rate 0, as given in Eq. (1) of Section 4.1. Considered 
as a function of the parameters, p(X I J.L), given in Eq. (1), is the likelihood 
function of the data. 
We sometimes wish to work with the simpler Poisson model. We then assume: 
N 
2 
Ji 
(2) 
p( {Xnij} I {J.Lni}) = IT IT IT !P(Xnij I WijJ.Lni), 
n=l i=l j=l 
with fp(x I WJ.L) the Poisson density with mean WJ.L. Equation (2) simplifies to 
if we introduce 
N 
2 
p( {Xnij} I {J.Ln;}) = II II fP(Xni+ I Wi+J.Lni), 
n=l i=l 
Xni+ = L: Xnij, 
j 
Wi+ = L: Wij· 
j 
Whether the general notation p(X I J.L) refers to Eq. (1) or Eq. (2) will be clear 
from the context; otherwise a model parameter cp, with values cp = P (for Poisson) 
or cp = nb (for negative binomial) will be introduced; for example, 
p(X I /J, cp = P) or p(X I/J,.cp = nb). 

100 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.2 
4.2B. Object of the analysis. From the data X. dn the papers of known 
authorship, we want to determine p( {Jln, 5n} I X), which is the posterior dis-
tribution of the parameters, for use in exact or approximate evaluation of odds 
factors, 
(3) 
E[Ol (il) I Xl J 
01 (Jl)p(Jl I X) dJl 
E[02(il) I Xl = J 
02 (Jl)p(Jl I X) dJl ' 
that involve two expectations with respect to this posterior density. 
The 
functions Oi are of the form 
Oi(Jl) = lIfnb(Xno I WOJlni, Kni, W05ni). 
n 
The integrals in the numerator and demoninator of Eq. (3) cannot be carried 
out explicitly and we need to use quadrature or approximations. Difficulties 
and possibilities of quadrature are discussed in Section 4.4. The bulk of the 
study is based on the simplest approximations of the form 
where p, is some central value of the posterior distribution of il given X. If the 
posterior distribution of il is tightly concentrated about p" relative to the smooth-
ness of the function (/1, the approximation should be good. On grounds of 
feasibility, we are led naturally to use a mode of the posterior density of il or of 
some reparametrization of il. The resulting approximate evaluation of the odds 
factor of Eq. (3) is called the modal approximation. Accuracy of the approxi-
ma.tion is explored in Section 4.6. 
The principal goal of Section 4.2 is the determination of a point estimate p,: 
some modal central value of the posterior density of jL. 
4.2C. Prior distributions: assumptions. In Section 3.2, we have il-
lustrated, for the Poisson model, the use of pools of words to give a sampling 
basis to the prior distributions for parameters and the use of a variety of sets 
of underlying constants to bracket the true prior. The extension to the negative 
binomial parameters was based to a large extent on analogy with the Poisson 
procedure, but partly on examination of method-of-moments estimates for a 
small pool of words. Here we briefly indicate the motivation of our assumptions, 
with technical treatment reserved for Section 4.5. 
in Table 4.2-1 we show the method-of-moments estimates (mI, m2, dl; d2) 
of parameters (Jl1, Jl2, 51, 52) for 22 high-frequency function words among the 
words 1 through 70. 
The table also shows the moments estimates of l1K1 
and I/K2 and of parameters ~, '1/ to be introduced shortly. The words are arranged 
in descending order of combined mean rate 8 = ml + m2. The sampling error 
in the estimates db d2 increases rapidly as 8 decreases. The method of selecting 
the words in this list was somewhat dependent on the value of t = md 8. 

4.2] 
ANALYSIS OF THE PAPERS OF KNOWN AUTHORSHIP 
101 
TABLE 4.2-1 
METHOD-OF-MoMENTS ESTIMATES OF NEGATIVE BINOMIAL PARAMETERS 
FOR 22 FUNCTION WORDS 
Parameter 
JJ.l 
JJ.2 
81 
82 
llKl 
l/K2 
~ 
TJ 
Word 
Estimate 
ml 
m2 
dl 
d2 
ddml 
d21m2 
number Word 
52 
the 
91.27 
93.65 
.69 
1.35 
.0076 
.014 
1.38 
.38 
39 
of 
64.65 
57.80 
.26 
.26 
.0039 
.0045 
.46 
.50 
58 
to 
40.71 
35.25 
.37 
.54 
.0091 
.015 
.74 
.42 
5 
and 
24.50 
27.55 
.43 
1.00 
.018 
.036 
1.05 
.34 
1 
a 
22.85 
20.22 
.26 
.89 
.012 
.044 
.87 
.27 
30 
it 
13.82 
13.34 
.64 
.56 
.046 
.042 
.94 
.53 
13 
by 
7.34 
11.44 
.30 
.48 
.042 
.042 
.65 
.41 
69 
would 
8.63 
3.56 
2.91 
1.77 
.34 
.50 
2.38 
.57 
40 
on 
3.28 
7.83 
.34 
.53 
.10 
.067 
.72 
,41 
26 
if 
3.59 
2.72 
.32 
.28 
.088 
.10 
.52 
.53 
33 
more 
2.71 
3.23 
.51 
.72 
.19 
.22 
.96 
,43 
61 
was 
1.63 
3.79 
.99 
3.58 
.61 
.95 
2.21 
.31 
55 
there 
3.28 
1.29 
.26 
.30 
.079 
.23 
,49 
.47 
60 
upon 
3.35 
.14 
.12 
.36 
.035 
2.6 
,42 
.27 
44 
our 
2.27 
1.11 
3.65 
2.29 
1.6 
2.1 
2.73 
.56 
24 
her 
.65 
2,49 
4.84 
7.09 
7.4 
2.8 
3.85 
,46 
64 
when 
1.29 
.74 
.74 
.21 
.57 
.29 
.75 
.74 
54 
then 
.33 
.93 
.045 
.27 
.14 
.29 
.28 
.16 
38 
now 
.37 
.63 
.00 
.63 
.00 
1.00 
,49 
.00 
3 
also 
.28 
.70 
.003 
.19 
.011 
.27 
.18 
.02 
59 
up 
.29 
.17 
.62 
.009 2.1 
.053 
,49 
.98 
35 
my 
.21 
.06 
1.71 
.16 
8.1 
2.7 
1.15 
.87 
As explained in Section 4.1C, the choice of the non-Poissonness parameter 5 
was based in part on its stability across words, as shown by this table. 
Study of the estimates suggests that 5 is decently stable and roughly in-
dependent of Jl across words and authors. Quite large deviations exist, so some 
tail-reducing transformation seems essential if parameters are to have a simple 
prior distribution. The choice r = 10g(1 + a5), with a = 1 for length mea-
sured in units of 103 words, appeared helpful and manageable and has been used. 
For most words, both authors have similar values i of r (or 5), but there are 
some deviations not explainable by the estimation errors, great as they are. 
Thus 81 = 52 would be satisfactory for most words, but for some it would be seri-
ously wrong. Differential non-Poissonness is potentially discriminating, but the 
motivation is to avoid upsetting the analysis of differential mean usage, rather 
than to achieve a new source of discrimination. In p~ssing we note that posses-
sive pronouns appear to be an identifiable source of Huge non-Poissonness, and 
early elimination of this class of words would have eased much of the analysis. 

102 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.2 
TABLE 4.2-2 
SETS OF UNDERLYING CONSTANTS 
VALUES OF (31, (32, (33, (34, (35 
Set 
(31 
(32 
(33 
(34 
(35 
1 (and 21) 
5 
1 
6 
1.25 
2 
2 
2 
1 
6 
1.25 
2 
3 
10 
1 
6 
1.25 
2 
4 
20 
1 
6 
1.25 
2 
5 
5 
5 
6 
1.25 
2 
6 
2 
10 
6 
1.25 
2 
7 
20 
10 
6 
1.25 
2 
8 
5 
1 
6 
.91 
2 
9 
5 
1 
6 
1.54 
2 
10 
5 
1 
18 
1.25 
2 
11 
5 
1 
1.5 
1.25 
2 
22 
10 
0 
12 
1.25 
2 
23 
10 
0 
6 
1.25 
2 
31 
10 
0 
12 
.83 
1.2 
32 
10 
0 
6 
.83 
1.2 
33 
15 
0 
12 
.83 
1.2 
34 
10 
0 
18 
.83 
1.2 
35 
10 
0 
12 
.72 
1.2 
36 
10 
0 
30 
.83 
1.2 
37 
5 
0 
12 
.83 
1.2 
38 
5 
5 
6 
.83 
1.2 
A further reparametrization is needed when, for a word, the parameters of 
both authors are considered together. The means J.l.b J.l.2 are transformed exactly 
as with the Poisson means to u = J.l.l + J.l.2 and T = J.l.IiUi analogously, rl, r2 
are transformed to ~ = rl + r2 and '11 = 
rd'~, measures, respectively, of 
combined and differential non-Poissonness. Estimates of ~, '11 for the 22 words 
used·in the preliminary study are shown in Table 4.2-1. No strong dependence 
among ~, '11, and the pair (u, T) is evident from examination of these estimates. 
Formally, for the nth word, we introduce the parameters 
Un = J.l.nl + J.l.n2, 
Tn = 
J.l.nl 
J.l.nl + J.l.n2 , 
(4) 
rni = log(1 + ~ni), 
i = 1,2, 
~n = rnl + rn2, 
'1In = 
rnl 
. 
rnl + rn2 

ANALYSIS OF THE PAPERS OF KNOWN AUTHORSHIP 
103 
For each set of values of underlying constants, fJ = (fJ1, (32, fJ3, fJ4, fJ5), we 
assume that for all words in a pool from which the N words were selected: 
(5) 
the quadruples (un, Tn, ~n, 'In) are independent across words, 
(6) 
~n, 'In, and the pair (un, Tn) are independent of each other for each n, 
(7) 
Un has a density xn(un) which can be approximated by a constant, 
(8) 
conditional on Un, the parameter Tn has a symmetric beta density: 
(9) 
'In has a symmetric beta density: 
(10) 
~n has a gamma density with mean (34, argument fJ5: 
Each choice of ((31, ... , (35) is called a set of underlying constants and is assigned 
a number. The sets used in the analysis are shown in Table 4.2-2. The choice 
among these sets is discussed in Section 4.5. The values of (34 and fJ5 should be 
considered in conjunction with the alternative treatments presented in Section 
4.2F. 
We use several different parametrizations; for two of these we introduce 
special notations: 
(11) 
4.20. The posterior distribution. Under the distributional assumptions 
of Eq. (1) for the data and of Eqs. (5) through (10) for the parameters, the poste-
rior density of the parameters is given, up to a multiplicative constant, by a 
direct application of Bayes' theorem. A first huge simplification results from the 
assumptions. The parameters are assumed independent a priori across words: 
n 
the likelihood function factors by independence of the data: 
n 

104 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.2 
hence the parameters are independent a posteriori: 
(12) 
n 
For the remainder of this section, we work with one word at a time, and drop the 
subscript n, indexing words. 
Introduce the functions h, hI, h2: 
(13) 
h1('Y) = -log p("I), 
h2("I) = -log p(X I "I), 
h("I) = -log p("1 I X). 
Then Bayes' theorem determines h("I) up to an additive constant as 
The constant c is determined in principle by the condition 
1 = I e-h(r) d"l = IIII e-h(<T."'.~.~) du dT d~ d1J, 
but for us this integration requires a four-dimensional quadrature or some other 
approximate evaluation. In detail 
(14) 
hl("I) = c + log B(,81 + ,82U, ,81 + ,82U) 
-
(,81 + ,82U -
1) 10g[T(1 -
T)] 
-
(,83 ~ 1) 10g[1J(1 -
1J)] + ~ (~:) -
(,85 -
1) log ~. 
(15) 
h2("I) = c + 
t t{-IOg[r(K~ti)Xii)J -
xijlog Oi + (Xij + Ki) 10g(1 + WiiOi)} 
in which Ki = Miloi. 
Explicit evaluation of the constant is hopeless, so that the available facts 
about the posterior density are those determined directly by the kernel of the 
density: notably, modes and logarithmic derivatives at the mode. We find the 
mode and second logarithmic derivative of the posterior density at the mode. 
The mode of a density is not invariant under change of variables, here param-
eters. Even for fixed parametrization, the mode of the density relative to a mea-
sure other than Lebesgue measure might be used. Our choice of the mode of the 
ordinary posterior density of "I = (u, T, ~, 1J) is simple but not clearly the best. 
We now feel that using the mode of the posterior density of (u, T, ~, 1J) relative 
to the measure element 
du 
dT 
d~ 
d1J 
(i T(1 -
T) T 1J(l -
1J) 

4.2] 
ANALYSIS OF THE PAPERS OF KNOWN AUTHORSHIP 
105 
would be preferable; that is, we expect that it would lead to more accurate 
integral evaluations in forming the odds (see Section 4.2B). Some of the early 
development was carried out using the mode of the ordinary posterior density 
of a = (U,7, 01, 02). 
Except for the factor 1iu, whose effect is unimportant, we can interpret the 
modes that we obtain as relative modes based on a modified set of underlying 
constants. The required changes in $1, (32, (33 are small in comparison to the 
ranges used for those constants. For the less important constants (34 and (35, 
the changes are larger. This alternative choice of modes is discussed in Section 
4.2F. 
We make one further simplification, in line with neglecting prior information 
on u. Use of (31 + (32U as the argument of the beta density of 7 was made to 
introduce a slight tightening of the prior density of 7 for higher-frequency words. 
For the choice (32 = 0 that now seems nearly best, the dependence onu vanishes. 
We assume that in the argument (31 + (32U, U may be replaced by an estimated 
value. We might have used an external estimate of word rate (times 2), Or we 
might have used the estimate s, but we chose to use the modal estimate of u. 
The change in the posterior density is slight, for the same reason that we can 
approximate the density of u by a constant: because u is estimated precisely by 
the data. 
The immediate computational problem· is to locate the maximum of the 
function h(u, 7, ~, 'Y/) for each word and for each set of values of underlying 
constants. No solution in closed form is remotely possible, 'and an iterative 
computation is required. The choice of the initial estimate for the iteration is 
of statistical interest and is described in Section 4.2G. 
The maximum was located by a N ewton-Raphson iteration. At each step, 
the function h was approximated by its quadratic Taylor expansion about the 
estimate, and the maximum of the quadratic used as the next estimate. For 
reasons no longer relevant, the process was carried out in the variables of the 
a-parametrization instead of in those of the I'-parametrization. 
Formally, let if;(a) = h(I'(a». Denote by V and K, respectively, the vector 
of first derivatives and the matrix of second derivatives of if; with respect to the 
components (u, 7, 01, 02) of a, and evaluated at a vt;tlue ao. If a, ao, V are 
interpreted as column vectors, and if transposes are denoted by', then h(I') may 
be approximated for a near ao or I' = I'(a) near 1'0 = I'(ao) by 
(16) 
h(I') = if;(a) ~ h(I'o) + (a -
ao)' V + !(a -
ao)' K(a -
ao). 
The maximum of the quadratic approximation to h(I') is located at: 
(17) 
We Used an iterative procedure: we chose ao, evaluated V, K, K- 1, then used 
the value of a determined from Eq. (17) as the next value of ao. 
This computation was programmed and implemented by Miles Davis. 

106 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.2 
4.2E. The modal estimates. The computation described in Section 4.2D 
was carried out, with some exceptions, for words 1-117 for all sets of underlying 
constants in Table 4.2-2; for words 118-165 for all sets except 2-4 and 6-10. 
Exceptions were caused for a handful of words by a combination of human errors, 
machine errors, and some misbehaving distributions that gave indefinite matrices 
K. 
The final values of the parameters obtained in the computation are called the 
modal estimates and these parameter values, or any functions of them, are 
denoted by a superposed A; thus 1 = (ft, r, ~, ~), a = (ft, r, 51, 52), it1, it2, 
/(1, /(2, etc. We emphasize that these are the parameter values at the mode of 
the posterior density of 1', not at the mode of a or of some other parametrization. 
Although I' is appropriate for assessing the prior and posterior distributions, 
most of the interpretations and the computations of log odds are based on the 
untransformed parameters and use it1, it2, 51, 52, K1, K2. 
Needless to say, we do not display the entire output. Illustrative pieces are 
tabled throughout Chapters 3 and 4. Table 3.2-3 displays the results for the 30 
words in the final study for set 31 of underlying constants: the modal estimates 
of 0', T, 01, 02, and the corresponding estimates of ILl and IL2, as well as the 
estimates of 01 and 02 for set 22. Table 4.5-5 shows estimated parameters for 
upon for a variety of sets of underlying constants. 
The matrix K of second derivatives and the inverse matrix K- 1 are also 
available from the computation, and are used in Section 4.6 to explore the 
adequacy of the modal approximation. 
With Section 4.2E, we conclude the presentation of the essential technical 
basis for the main application of Bayes' theorem. Armed with the modal esti-
mates of the parameters and the modal approximation, we are ready to determine 
odds factors for the authorship of a disputed paper. Sections 4.2F and 4.2G 
treat technically important details in the choice of modes and estimates. 
4.2F. An alternative choice of modes. Since the computations for the 
main study were completed, we have come to feel that the use of the mode of the 
posterior density of I' in the modal approximation is not as wise as would be the 
use of certain relative modes that are as readily obtained. By a modification 
of the underlying constants of the prior distributions, we can reinterpret some 
of our results as if they were based on a preferred choice of modes. In this 
section, we explore briefly this alternative interpretation in order to show that 
effects on the final odds are small. 
Our study of the accuracy of the modal approximation (Section 4.6) suggests 
that the ordinary mode of a unimodal but asymmetric density is distinctly 
inferior to the mean for a point approximation to expectations. The mean is 
not feasible for our use, but a mode of a transformed parameter relative to a 
weighted measure element is feasible, and may be nearly as good as the mean. 
The transformation from (01, 02) to (~, 'Ij) greatly reduces the asymmetry in 
the distribution of (01, 02), so that the mode and mean of the density of I' = 

4.2] 
ANALYSIS OF THE PAPERS OF KNOWN AUTHORSHIP 
107 
(cr, T, ~, fJ) should be much closer than the mode and mean of the density of 
a = (cr, T, (h, 152). 
The prior distribution of T is a beta distribution and the posterior distribution 
is nearly a beta distribution, The mode of the density of T relative to the measure 
element dT /[T(I -
T)] coincides with the mean if the distribution is exactly a 
beta, and will be very close to the mean if the distribution is nearly beta. The 
same argument applies to fJ. The prior distribution of ~ is a gamma distribution 
for which the mode relative to the measure element d~/ ~ coincides with the mean. 
For want of something better, we would use the product of these measure 
elements. 
We proceed to explore the use of the mode of the posterior density, say «p, 
of 'Y relative to the measure element 
dcr 
dT 
d~ 
dfJ 
T(I -
T) 
~ fJ(1 -
fJ) 
instead of the mode of the ordinary posterior density, h, of 'Y. The additional 
factor l/cr might be preferable. Its effect would be small. The ratio of the 
posterior densities «P and h is 
But this is just the ratio of the prior density based on underlying constants (3* 
to the prior density based on (3, where 
* 
f31 = 
f31 -
1, 
* 
(32 = (32, 
.<1* _ 
(3 
(35 -
1 
fJ4 -
4 
• 
(35 
* 
(33 = (33 -
1, 
* 
(35 = (35 -
1, 
Thus the relative posterior density cp based on f3* is identical to the ordinary 
posterior density h based on (3. The mode Ii based on f3 is then also the relative 
mode based on f3*. 
This particular change of densities and modes is exactly equivalent to a change 
of underlying constants. The reductions by unity of f31 and (33 are small compared 
to the ranges used in the final log odds in Table 3;4-2: 5 to 15 for (31, and 1.5 to 12 
for (33. However, for the choice f33 = 1.5 used in set 11, we find f3~ = .5, so 
that in this alternative interpretation, set 11 corresponds to a prior density for fJ 
proportional to [fJ(I -
fJ)]-1/2, with modes at 0 and 1 and a minimum at .5, 
and hence of considerably different shape. 
The effect on (34 and f35 is larger. The two pairs of ((34, (35) used most of ten-
(1.25,2.0) and (.~3, 1.2)-are changed to (.62, 1.0) a~d (.14, .2) for ((34, (3t). 
Of these pairs, all but (.14, .2) represent prior distributions for ~ that are roughly 
in accord with the'. evidence similar to that in Table 4 .. 2-1. We prefer set 22 to 
set 31 because 22, with ((34, (35) = (1.25,2.0) and ((34, M) = (.62,1.0), approxi-
mates the desired prior distribution for ~ in either interpretation. The ((34, (3~) 
interpretation is used again in Sections 4.5D and 4.5E .. 

108 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.2 
4.2C. Choice of initial estimate. If the prior density of «(J', 7', ~, 17) were 
constant, the determination of the posterior mode would reduce to the determi-
nation of the maximum likelihood estimates of the parameters. The prior density 
is far from constant, and the maximum likelihood problem is not solved, but the 
similarity between posterior modes and maximum likelihood estimates suggests 
means of obtaining approximations useful in themselves or as starting points for 
iterative solutions. The procedure illustrates how results from conventional 
theory of estimation may be useful in Bayesian analysis. 
The prior density can be regarded as having arisen from some (fictitious) 
previous experiment; hence it can be factored into a fictitious likelihood func-
tion and a "pre-prior" density. Choose the representation so that the pre-prior 
density is nearly flat over the relevant range. Then the fictitious likelihood 
function is nearly determined, although the fitltitious experiment and the out-
come are ill-determined. 
Choice. of semirealistic fictitious experiments may 
often be possible, and more often, dependence can be restricted to the likelihood 
only. Using the representation, the problem of finding the posterior mode is 
then very close to the problem of finding the maximum likelihood estimates 
from the combination of two experiments. Approximation to maximum likeli-
hood estimates is quite common in conventional statistics, though other esti-
mates serve other functions, too. With the two experiments, approximations 
to the maximum likelihood estimates might be obtained in each separate ex-
periment, and their approximate variances determined. Then, combination by 
weighting proportional to the inverse variances is quite natural, at least for 
large sample likelihoods. In the present problem, method-of-moments estimates 
are conveniently available for the negative binomial likelihoods, while the 
fictitious likelihoods are sufficiently simple that they can be used directly. The 
procedure will be illustrated only for the parameters u and 7'. 
For the sample estimates, method-of-moments estimates are used, and for 
our data, these are fairly efficient. Denote by ml, m2, dl , d2, the method-of-
moments estimates of ILl, IL2, th, 02, and let s = 1n1 + m2 and t = mds be 
the corresponding estimates of (J' and 7'. The estimates m and d for a single 
sample and their approximate asymptotic variances have been given in Section 
4.1D. 
For the weighting of estimates, something akin to sampling variances is 
needed, but they need not be determined precisely. From the usual delta-
method manipulations, we find: 
V(s) = V(nh) + V(nh) , 
Vel) R: [(1 -
7')2V(11h) + 7'2v(rn2)1. 
(J'2 
Use Vern) of Eq. (10), Section 4.1, in which the appropriate subscript 1 or 2 is 
appended to all quantities including number of papers J and mean paper 
length w. 

4.2] 
ANALYSIS OF THE PAPERS OF KNOWN AUTHORSHIP 
109 
The variances V(s) and V(l) must be estimated, but the replacement of 
parameters by method-of-moments estimates may be inadequate for V(l). The 
variance V (l) can be written 
The bracketed term depends only slightly on T and is bounded away from 0, 
and (f is estimated sufficiently accurately by 8. But the factor T(1 -
T) can 
cause trouble whenever t is small. (For whil8t, we have t = 0.) We evaluate the 
factor in braces, but keep T(1 -
T) unevaluated. 
Consider now the prior information. As treated here, no prior estimate of (f is 
used, or, alternatively, it is weighted 0, so that the preliminary estimate (f(0) = 8 
is used. The prior likelihood in T has a maximum at T = .5 and a second log-
arithmic derivative of 8(131 + 1328 -
1). Alternatively, the prior density can 
be regarded as a likelihood of a binomial experiment in which (131 + 1328 -
1) 
heads and (131 + 1328 -
1) tails were obtained, with probability T per toss. 
Then the natural estimate of T would be .5 and the "sampling" variance would be 
T(1 -
T) 
The second logarithmic derivative obtained above is just the inverse variance 
evaluated at the estimate .5. Here, such evaluation causes virtually no trouble 
since T(1 -
T) is fiat near .5. However, in weighting the two estimates t and .5, 
it would seem sensible to eliminate the common factor T(1 -
T) in getting the 
relative weights instead of evaluating T(1 -
T) at two widely different values 
of T. Thus we use the preliminary estimate: 
We might have absorbed the -
1 of 131 + 1328 -
1 into the "pre-prior" density. 
What we have done amounts to giving zero weight to .5 when the prior density 
is fiat, and of course, that requires 131 + 1328 ~ 1 if the result is to be at all 
sensible. For 131 + 1328 < 1, the estimate .5 is then the least likely, a priori. 
To give some idea of the adequacy of the preliminary estimate of T, we show 
in Table 4.2-3, for each of the final 30 words, the method-of-moments estimate t, 
the preliminary estimate T(O)' and the final estimate f. The latter two estimates 
are for set 31 of underlying constants. The procedure works unusually well for T, 
in part because, if the Poisson model held and if equal lengths of text by Hamil-

110 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.2 
TABLE 4.2-3 
ESTIMATES OF T FOR SET 31 OF UNDERLYING CONSTANTS 
Method-of-
Word 
moments, 
Preliminary, 
Final modal, 
number 
Word 
t 
7(0) 
T 
B3A 
60 
upon 
.960 
.925 
.932 
B3B 
3 
also 
.282 
.319 
.327 
4 
an 
.566 
.565 
.565 
13 
by 
.391 
.393 
.390 
39 
of 
.528 
.528 
.527 
40 
on 
.295 
.301 
.304 
55 
there 
.718 
.706 
.706 
57 
this 
.565 
.564 
.564 
58 
to 
.536 
.536 
.537 
B3G 
73 
although 
.048 
.293 
.267 
78 
both 
.302 
.325 
.334 
90 
enough 
.928 
.766 
.727 
116 
while 
.940 
.770 
.744 
117 
whilst 
.000 
.142 
.153 
123 
always 
.798 
.744 
.742 
160 
though 
.655 
.634 
.639 
B3E 
80 
commonly 
.962 
.774 
.763 
81 
consequently 
.063 
.201 
.189 
82 
considerable (l y ) 
.742 
.676 
.684 
119 
according 
.134 
.226 
.238 
124 
apt 
.901 
.756 
.770 
B3Z 
87 
direction 
.894 
.716 
.693 
94 
innovation( s) 
.054 
.289 
.278 
96 
language 
.168 
.308 
.316 
110 
vigor(ous) 
.801 
.683 
.680 
143 
kind 
.890 
.825 
.799 
146 
matter(s) 
.904 
.798 
.790 
151 
particularly 
.168 
.261 
.282 
153 
probability 
.879 
.762 
.757 
165 
work(s) 
.190 
.326 
.326 

4.3] 
ABSTRACT STRUCTURE OF THE MAIN STUDY 
III 
ton and Madison were observed, the procedure would be exact, and in part 
because the sample data nearly dominate the prior evidence. Use of the prelim-
inary estimates of T without any of the iterative modal computation would 
appear adequate. 
Similar procedures were used to form preliminary estimates of ~ and 1/. The 
required sampling variances are more complex, but there is no trouble, as 
with V(l), with vanishing estimated variances. The results in our application 
were not very good, because of the severe skewness of the prior density of ~. 
In summary, we foresee the need for methods to get good estimates of param-
eters, estimates that are readily obtained and that utilize both the sample 
and prior evidence. These estimates may serve as preliminary estimates for 
iterative calculations of posterior modes, etc., or they may be used as inexpensive 
approximations. 
Some "inefficient" estimates from the data or from data 
equivalents of the prior density may prove very useful in setting up these meth-
ods. 
4.3. ABSTRACT STRUCTURE OF THE MAIN STUDY 
4.3A. Notation and assumptions. The ultimate goal of the analysis is to 
determine posterior probabilities of authorship of L unknown papers, given data 
on each, data on papers of known authorship, and other relevant information. 
Our contribution is almost solely that of finding and assessing statistical evi-
dence, but we mention assumptions on the relation between statistical and 
historical evidence that would allow their separate assessment and combination. 
We need first a considerable body of notation and assumptions. Four assump-
tions, (A1) through (A4), are made in Section 4.3A, one, (A5), in Section 4.3D, 
and four more, (A6) through (A9), in Section 4.3F. 
Each unknown paper is assumed to be written by Hamilton (author 1) or 
Madison (author 2). Assign the lth unknown paper a classification parameter Ol 
whose value is 1 if Hamilton, 2 if Madison, wrote it. For the moment, reserve 
the historical evidence. 
Represent the statistical data on the lth unknown 
paper as the observed value Y z of a random variable Yl ; represent the statistical 
data on the papers of known authorship as the observed value X of a random 
variable X. Introduce the unobservable random variable j1 to represent the 
unknown parameters of the distributions of the statistical data. 
N ext introduce the usual sorts of independence assumptions about separate 
pieces of data. Assume that conditional on 01 = 01, •.• , h = OL, j1 = f.L, the 
random variables Yb ... , YL, X are independent, the distribution of Yl depends 
only on Oz and f.L and l, and the distribution of X depends only on f.L. Assume 
that j1 and (Oil' .. , OL) are independent (a priori). 
The assumptions can be written more formally as shown below, where we 
have adopted the convention for a function with thelabel p that p(ulv) is the 
density of the random variable it, given that v = v. The p-notation allows us 

112 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.3 
to mention many distributions without elaborate notation or alphabets. The 
densities p(x) and p(y) are rarely identical. 
(AI) 
p(X, Y1, ... , YL I01, ... ,OL,P.) 
= p(X 101, ... , OL, p.) . IIp(Y I I 01, ... , OL, p.), 
I 
(A2) 
p(YI I 01, ... , OL, p.) = h(YI 101, p.), 
(A3) 
p(X I 01, ... , OL, p.) = g(X I p.), 
(A4) 
p(p., 01, ... , OL) = h(p.) . 7r(01, ... , OL). 
With these assumptions, the fundamental application of Bayes' theorem can 
be made: 
p(iJ1 = 01, ... , lh = 0 L I X, Y 1, ... , Y L) 
P(81 = 01, ... , 8L = 0i I X, Y 1, ... , Y L) 
(1) 
P(81 = 01, ... , 8L = OL) E{IIfl(Y I 1 01, j1) I X} 
= P(81 = OL ... , h = 0L) . E{IIfz(Y I I of, ji) I X} 
In words, the posterior odds between two assignments of authorship is the 
product of the prior odds and a factor that is called the odds factor. The odds 
factor is the ratio of the expectations of the likelihoods under the two assign-
ments of the observed data; the expectations are taken with respect to the 
conditional distribution of ji, given the data X. 
4.3B. Stages of analysis. Equation (1) permits a useful division of the 
analysis into three stages: choosing the data distribution and estimating its 
parameters, evaluating odds factors, and combining these with prior information. 
In the first, the analysis of papers of known authorship, we get the conditional 
distribution h(p.1 X) of the nuisance parameter p., given the data X on the papers 
of known authorship. This distribution is often called the posterior distribution 
of ji, though it is posterior only to X, prior (logically) to Y 1, ... ,Yl . The 
choice of data is handled in the first stage, which comprises the bulk of our work. 
The second stage is the evaluation of the odds factors for the assignment of 
authorship to the unknown papers. Each evaluation requires the calculation of 
two expectations with respect to the distribution h(p. I X), which was deter-
mined in the first stage. Approximations are needed to reduce this high-dimen-
sional integration to a feasible piece of arithmetic. 
In the third stage, prior odds and odds factor are combined to obtain posterior 
odds. Probabilities of authorship depend heavily on the highly subjective prior 
odds of authorship. Our consideration of this stage is brief. 
4.3C. Derivation of the odds forlllula. A derivation of the fundamental 
formula for odds, Eq. (1), follows. Here, we denote by an unsubscripted 0 or 
Y the vector of the L corresponding quantities, and CX,Y is a normalizing 
constant. 

4.3] 
ABSTRACT STRUCTURE OF THE MAIN STUDY 
113 
(2) 
p(e = 0 I X, Y) = CX,yp(e = 0 I X)p(Y I X, 0) 
(3) 
= CX,yp(e = 0 I X)E{p(Y I X, 0, p,) I X, O} 
(4) 
= cx,Yp(e = 0 I X)E {llfl(YI I 01, j1) I X, O} 
(5) 
= CX,yp(e = O)E {llfl(YI I 01, j1) I X, O} 
(6) 
= CX,yP(O = O)E {llfl(YI I 01, j1) I X} . 
Equation (2) is a direct application of Bayes' theorem, entirely conditional on X. 
Equation (3) uses the usual fundamental property of conditional probability 
densities and expectations. Equation (4) uses the independence, given 0 and fl, 
of X, i\, ... , V L that follows from assumption (AI) and the specific de-
pendence of VI on 8 and fl which was assumed in (A2). Equations (5) and (6) use, 
respectively, the independence of X from 8 and the independence, given X, of fl 
and 8, each following from the independence of 8 and (X, fl) which in turn 
follows from assumptions (A3) and (A4). Finally, Eq. (1) follows from the 
ratio of two applications of Eq. (6), one with 8 = 0, and the other with 8 = 8'. 
As a postscript, we give an integral representation of the expectation in the 
odds factor. The conditional distribution of j1, given X, has density 
h(jJ-1 X) = Cxh(jJ-)g(X I jJ-) 
relative to some measure element djJ-. The odds factor for a single unknown 
paper, dropping subscript l, is then 
E{j(Y /8, fl) / X} 
E {f( Y / 8', fl) / X} 
fI(Y /8, jJ-)h(p,/ X) dp, 
[fey 18', jJ-)h(jJ-1 X) djJ-
f I( Y / 0, p,)g(X / p,)h(p,) dp, 
f f( Y I 0', jJ-)g(X I jJ-)h(jJ-) djJ-
4.3D. Historical inforlTIation. If possible, we wish to keep the probabi-
listic contribution of the historians' views separated from the statistical evidence 
that we present until the close of the inference. To this end, regard the historical 
evidence denoted by Z as logically prior to the preceding analysis. Clearly the 
prior probabilities of authorship, 
depend strongly on the assessment of the historical evidence. If nothing else 
depends on Z, the posterior odds is expressed by Eq. (1) as the product of prior 
odds determined by the historical evidence and the odds factor determined by 
the statistical evidence (and by certain nonhistorical and perhaps nonstatistical 
prior information on fl). 

114 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.3 
The important assumption needed for this separation of historical and statis-
tical evidence can be expressed as follows: 
Conditional on (01, •.. , OL), the random variables X, Y I, ••• , Y L, and fl. are 
jointly independent of the historical evidence, or formally, 
(A5) 
p(X, YI, ... , YL,f,L 101, ••• , OL,Z) = p(X, Y l , ... , YL,f,L 1 OI, ... , OL). 
Informally, what is required is that assumptions (AI) through (A4) except for 
the determination of 7r( OI, ••• , OL) should be equally acceptable to those who 
assess the historical evidence differently. Specifically, the choices of sampling 
distributions, the independence of the data from paper to paper, and the choice 
of prior distribution of the parameter f,L of the sampling distributions should be 
acceptable to all, acceptable, at least, in the sense, "as far as it goes it's satis-
factory, although there could be better analyses." 
4.3E. Odds for single papers. Thus far, the analysis has been given in 
terms of simultaneous assignments for L unknown papers. Odds for assignment 
of single papers are simple and depend only on the data for that unknown paper 
and, of COUl'se, on the data X from known papers. 
(7) 
P(OI = 1 1 X, Y) 
P(OI,= 1) E[lz(YI 1 fl., 01 = 1) 1 Xl 
P(OI = 21 X, Y) = PCel -.: 2) . E[fI(YI 1 fl., (Jz = 2) 1 Xl . 
The classification parameters 01, ••• , OL are not independent a posteriori, and so 
the separate odds are not sufficient for the simultaneous odds. Prior dependence 
arises, for example, because much of the historical evidence makes it likely that 
one or the other author wrote all or almost all of the papers. 
But even the odds factor in Eq. (1) is not the product of the odds factors from 
Eq. (7) for each separate paper because the same incomplete knowledge about fl. 
represented by h(f,L 1 X) enters the odds factor for each paper. Were f,L known, 
or were a single estimated value P- used as if known (a practice we follow), the 
joint odds factor would be a product of separate odds factors. We do not use 
the joint odds in any of the studies. 
The joint classification problem becomes essential in modified problems, for 
instance, when no papers of known authorship are available. 
(Suppose that 
Madison, Hamilton, and Jay had died without ever identifying the author of 
any of the Federalist papers.) Classification problems in taxonomy and psy-
chology often are more nearly of this type. 
4.3F. 
Prior distributions for many nuisance parameters. 
Under 
assumptions (AI) through (AS), the subjective element in the assessment of 
historical evidence has been isolated in the prior odds. If the sampling dis-
tributions of the data in the known and unknown pape~s are objectively accept-
able, the potentially subjective element remaining is the choice of prior dis-

4.3] 
ABSTRACT STRUCTURE OF THE MAIN STUDY 
115 
tribution of the parameter fJ-. The use of evidence from data across words to 
remove much of this subjectivity is formalized here. 
Assume that X and Y consist of observations on each of N words, X = 
(Xl, ... ,XN), Y = (Yl, ... ,YN), that the distribution of the data on the nth 
word depends only on the nth component iln of the parameter il = (ill, ... , ilN), 
and that given the parameters, the word-frequency data are independent across 
words. Specifically, assume: 
(A6) 
n 
(A7) 
n 
Restricting attention to the first stage, the analysis of the known papers, we find 
that we need the posterior distribution 
In our applications, N is about 100, and each component parameter is either 
two-dimensional (Poisson distributions) or four-dimensional (negative binomial 
distributions). The amount of data X is by no means enough to overwhelm 
assumptions about h(fJ-), so the choice does matter. An assumption of a flat prior 
in several hundred dimensions is not reasonable. An assumption of independence 
of ill, ... ,ilN would make the analysis simple, for then the posterior distribution 
of ill, ... , ilN, given X, would again be independent, and the odds factor for an 
unknown paper would be a product of the odds factors obtained separately for 
each word. This desirable state is achieved as an approximation. 
One type of special assumption that seems often both reasonable and feasible 
is a sampling assumption on the parameters. The appropriateness of this device 
for use with parameters of word-frequency distributions is discussed in Sections 
3.2, 4.2, and 4.5. This approach is very similar to some uses of model II analysis 
of variance, and the idea is almost exactly that behind "empirical Bayes 
procedures" (Robbins, 1951, 1956), although the spirit and goals of its use differ. 
One main difference is the interest here in the joint posterior distribution of the 
parameters rather than the marginal posterior distributions. 
The independence of ill, ... , ilN may be unacceptable because of some under-
lying structure among the parameters. Were this structure known, the remain-
ing fluctuations in ill, ... , ilN might be independent. The idea is, of course, 
just that typically used in specifications for observed random variables. The 
following sampling model permits a large variety of dependence. 
Introduce a quantity ~ and assume that the {iln} are conditionally indepen-
dently sampled from (possibly different) distributions with parameter~. We call 
~ the underlying constant to avoid a confusing double use of "parameter," and 
because, in the bulk of our work, ~ is treated as a constant. A low dimension-

116 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.3 
ality for ~ is intended. In symbols we have 
(A8) 
n 
The acceptability of assumption (A8) depends strongly on the forms of the func-
tions Qn(J.1n I (3). To complete a specification of the problem, denote the prior 
density of ~ by r((3) : 
(A9) 
p((3) = r((3). 
One hopes that the choice of r is unimportant. 
If the true value of ~ were known, that is, if ~ were in some way ob:servable, 
then analyses conditional on (3 would be possible and appropriate, and the 
resulting posterior distribution of ill, ... , ilN given X and (3 would be as if 
ill, ... , ilN were independent a priori-and hence a posteriori .. 
Specifically under assumptions (AI) through (A8), and conditional on ~ = (3, 
p(J.1l, ... , J.1N I Xl, ... , XN, (3) = ITP(J.1n I X n, (3), 
n 
(8) 
P(J.1n I X n, (3) -
Cp,Xng(Xn I J.1n) . q(J.1n I (3). 
In practice ~ is not observable but the conditional analysis may still be usable. 
Suppose we determine from the posterior distribution of~, given X, a set of values 
of ~ of high posterior probability. (Alternatively, we might use a confidence set 
for ~.) With high probability the true (3 is contained in the set, and if the analysis 
conditional on (3 is carried out for each value of (3 in the set, the correct condi-
tional analysis will likely be included. If the conclusions, say the probabilities 
of authorship, do not vary much as (3 varies in the set, then the common con-
clusion is justified. If the conclusions or decisions do vary seriously, a more 
careful analysis is required. This same procedure is used for handling a variety 
of parameters of models that are too complex for straightforward treatment. 
Problems of inference are more amenable to this approach than clear-cut decision 
problems. 
We are also led to use the conditional analysis with an estimated (3 as an 
approximation to an exact analysis. Under assumptions (AI) through (A9) the 
posterior density of ~, given X, is 
(9) 
r((3 I Xl, ... , XN) = Cxr((3) IT f g(Xn I J.1n)q(J.1n I (3) dJ.1n, 
n 
and the posterior density of il given X, but not conditional on (3, is: 
(10) 
P(J.11, ... , J.1N I Xl, ... , XN) = C'x f IT {g(Xn I J.1n)q(J.1n I (3)}r((3) d(3 
n 

4.4] 
ODDS FACTORS FOR THE NEGATIVE BINOMIAL MODEL 
117 
Equations (9) and (10) are the natural forms for studying the exact marginal 
distributions of ~ and of p. Equation (11) is preferable for suggesting approxi-
mations. Its appearance is deceptively simple: note that if Eq. (8) is used to 
give the conditional posterior density, the undetermined multiplicative constant 
depends on (3. 
If r((31 Xl' ... ' X N) is a relatively tight distribution with central value ~x, 
and if p(iJJ, ... , iJN I Xl, ... , X N ,(3) as a function of (3 is relatively flat near ~x, 
then 
p(iJl, ... , iJN I Xl, ... , X N) ~ P(iJ1, ... , iJN I Xl, ... , X N, (3 = ~x) 
(12) 
= II C~.xng(Xn I iJn)q(iJn I Sx). 
n 
The approximation is exactly equivalent to the conditional analysis given ~ = 
Sx, that is, it is equivalent to the use of the estimated value of ~ as if it were the 
true value. Improvements to this point-approximation along the lines of Section 
4.6 are extraordinarily difficult and are not attempted. 
To make use ofthe approximation of Eq. (12), or of the method of repeated 
conditional analyses, the posterior distribution of (3, given X, must be studied, 
at least enough to determine some central value and measure of spread. Being 
a tertiary parameter, a nuisance parameter of the distribution of nuisance 
parameters, its precise study may not be at all critical. If the dimensionality 
of ~ is low and N is large, the determination of ~ by the data X may be enough 
even to justify the use of a flat prior density r((3) for ~. 
4.3G. Summary. In this section we have presented formally the assumptions 
underlying our use of Bayes' theorem and our approach to the choice of prior 
distributions. We have shown where each assumption enters the several stages 
of analysis and how the problems of simultaneous and separate classifications 
of several papers are related. Finally, we have given several formal interpre-
tations for our method of handling many nuisance parameters. 
4.4. ODDS FACTORS FOR THE NEGATIVE BINOMIAL MODEL 
4.4A. Odds factors for an unknown paper. Extend the notation for data 
on papers of known authorship given in Section 4.2A to a single unknown paper 
by letting Wo denote the length and XnO the frequency of word n in the unknown 
paper. Let e denote the true author (e = 1 for Hamilton, e = 2 for Madison) 
of the unknown paper. Let Xo = {xnO} denote the totality of word-frequency 
data on the unknown paper. 
Because we never consider more than one unknown paper at a time in evalu-
ation of odds factors, we do not identify the unknown paper in the notation. 
Whether the unknown paper is a disputed paper, a joint paper, an outside paper, 
or one of the known papers being used as an unknown for checking, is of no 
concern here. 

118 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.4 
Extend the assumptions on word-frequency distributions to the unknown 
paper in the natural way by assuming that word frequencies are independently 
distributed across words and papers, whether known or unknown, and that the 
distribution for any word in an unknown paper is negative binomial with 
parameters appropriate to the true author O. Formally, assume: 
p(X, X 0 / JL, 0) = p(X / JL) . p(X 0 / JL, 0), 
(1) 
N 
p(Xo / JL, 0) = IT fnb(XnO / WoJLnO, KnO, woono); 
n=l 
with p(X / JL) given by Eq. (1) of Section 4.2A. 
The only new parameter introduced is the classification parameter 0, and we 
assume that it is independent of the distributional parameters JL: 
p(O, JL) = p(O)p(JL). 
Then, because 0 is, by Eq. (1), also independent of the data X on the known 
papers, 
p(O, JL / X) = p(O)p(JL / X). 
(N ote that () and JL will not be independent, conditional on X and X 0, for heu-
ristically what 0 is determines whether X 0 is added to the Hamilton or the 
Madison data for estimating JL). We do not specify the initial odds peD = 1)/ 
peD = 2). 
The final odds of authorship given all the data X, X 0 and given the parameter JL 
are 
P(B = 1/ X o, JL, X)_ P(B = l)p(Xo / JL, () = 1) _ P(B = 1) /,11" 
P(B = 2 / X 0, JL, X) -
P(B = 2)p(X 0 / JL, () = 2) -
P(B = 2) 
, 
because JL is given, they do not depend on X. Here AII' is defined to be the log-
arithm of the odds factor from initial to final odds, and the subscript / JL indicates 
that this is a log odds factor conditional on JL. Explicitly, 
(2) 
(say) 
so that AII' is just the sum of the log likelihood ratios AniI' for each separate word. 
The final odds of authorship, given all the data X, X 0, but not conditional 
on the parameters JL, can be written [see Section 4.3, Eq. (7)]: 
(3) 
P(B = 11 Xo, X) 
P(B = 1) E(p(Xo 1 ii, () = 1) 1 X) 
P(B = 21 Xo, X) = P(B = 2) . E(p(Xo I ii, () = 2) 1 X) 
P(B = 1) x 
= P(B = 2) e, 

4.4] 
ODDS FACTORS FOR THE NEGATIVE BINOMIAL MODEL 
119 
in which the expectations are taken with respect to the posterior distribution of 
fJ. given X. This distribution, as given in Section 4.2D, is, for fixed underlying 
constants (3, independent across words. With the independence of word frequen-
cies already used in Eq. (2), the unconditional (relative to fJ.) odds factor, e\ 
is again a product of the factors e'n for each separate word. Explicitly 
(4) 
e' = lle'n 
n 
!!fnb(Xno I WOfJ.nb Knb w05n1 )P(fJ.nb 5n1 I Xn) dfJ.nl d5n1 
=ll 
. 
n !! f nb(XnO I WOfJ.n2, Kn2, w05n2)P(fJ.n2, 5n2 I X n) dfJ.n2 d5n2 
With the independence, we may get odds factors for each word separately, 
then multiply them together. For the remainder of this section, we work with 
one word at a time, and drop the subscript n. 
The properties Of the negative binomial likelihood ratio and its logarithm AII' 
are discussed in Section 4.4C. The evaluation of AII' for given arguments is a 
straightforward, but lengthy, computation. Nothing but elementary functions 
are involved: the gamma functions in the usual formula (Eq. (2) of Section 4.4C) 
can be eliminated. 
To evaluate the unconditional odds factor eA exactly requires, for each word 
and paper, a computation, described in Section 4.4B, that is nearly hopeless. 
As indicated in Section 4.2B, what we do instead is to approximate each expecta-
tion by the value of the function at a modal value of the posterior distribution 
of the parameters. This then reduces the approximate evaluation of uncondition-
allog odds to exact evaluation of the conditional log odds for fJ. ~ p,: 
A ~ AI~' 
The log odds in the main study were all computed according to the approxi-
mation A ~ AI~ where p, is the modal estimate of Section 4.2E. The log odds were 
evaluated by high-speed computer for all words for which the modal estimates 
were obtained. The number of sets of underlying constants for which log odds 
were evaluated varied with the estimated importance of the word. 
4.4B. Integration difficulties in evaluation of A. 
The exact uncon-
ditionallog odds A for a single word is 
"\ 
I 
E{fnb(io I WOilb Kb w051) I X} 
I\=og 
_, 
E{fnb(XO I WOil2' K2, w052) I X} 
where we have dropped the subscript n identifying the word. The posterior 
distribution of the parameters, given X, is determined easily by Bayes' theorem 
up to a multiplicative constant. Using the (fJ.b 51, fJ.2, 52) parametrization, 
write 

120 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.4 
where c and f will depend on X. The function f is easily determined from the 
functions hl' h2 of Eqs. (14) and (15) of Section 4.2D. 
To evaluate A, the constant c is not needed, for it appears in numerator and 
denominator, but still a four-dimensional integration is needed in numerator 
and denominator: 
JJfnb(XO I WOJ..!b Kb WOOl) [JJe-f (J'l,01'1'2,02) dJ..!2 do2]dJ..!1 dOl 
A = log 
. 
JJ fnb(XO I WOJ..!2, K2, W002) [J J e-f (l'l ,01 ,1'2,02) dJ..!l dOl] dJ..!2 d02 
The evaluation of the inner two-dimensional integrals can be reused for each 
new unknown paper, but the computation seemed unfeasible nonetheless. 
By finding the value of f and of its derivatives at its minimum p., we have 
not only an estimate [i to be used to evaluate the conditional log odds, but we 
have also some basis for exploring the accuracy of the approximation (Section 
4.6). This information might also serve to guide the choice of weight functions 
for use in any attempts at quadrature or Monte Carlo evaluation of A. 
We 
have not pursued the exact evaluation beyond exploration of the accuracy of 
the modal approximation. 
4.4C. Behavior of likelihood ratios. Rarely does the evidence we use to 
choose data distributions give us much certainty about the size and shape of 
their tails. When we deal with extreme observations, the behavior of the likeli-
hood ratio depends on the properties of these tails, and it is important that we 
understand that behavior, and compare it with our intuitive assessment. 
What behavior is desirable? Suppose that we consider a single word, and 
suppose that the probability of observing x occurrences in a paper of length W 
is fH(X; w) or fM(X; w) according as Hamilton or Madison is the author. Let 
denote the log likelihood ratio. Suppose that EH(x) and EM(x) are the expected 
word frequencies in the two populations. Figure 4.4-1 shows examples of four 
types of behavior of A(X). All show plausible behavior for observed frequencies 
near expectations under the two populations. In this range, the sampling evidence 
for the distributions is likely adequate and as reliable as our intuition. Curve A 
shows monotone and linear dependence on x: the more occurrences, the stronger 
the log odds for Hamilton. Curve B retains the monotone dependence but has 
the log odds reaching an asymptote. For curve C, increasing x beyond a moderate 
range decreases the strength of the evidence and reduces the log likelihood 
ratio to O. With curve D, large values of x are treated as increasing the log odds 
for Madison, the author with the lower expectation, and represent behavior 
appropriate at least for a moderate range of x if the Madison distribution has 
much larger spread than the Hamilton distribution. The behavior for decreasing 

4.4] 
ODDS FACTORS FOR THE NEGATIVE BINOMIAL MODEL 
121 
},(.t) 
A 
FIG. 4.4-1. A few possible shapes of the log likelihood ratio X(x). 
x is much the same except that, in our authorship application, the range of x is 
truncated at 0, so that only high-frequency words can have frequencies far 
below expectation. 
We feel that in our application, the limiting behavior typified by curve C is 
proper, and that curves A and D are seriously wrong. An extreme frequency 
of a word seems likely to arise from some different mechanism-for example, 
from some special rhetoric or subject-matter demands-than that underlying 
the assumed sampling distribution. The more extreme the frequency, the more 
such alternative mechanisms should cut down the evidence. 
In spite of the views just expressed, we must confess that the negative binomial 
distributions used do not even have bounded likelihood ratios for large x. 
Consequently, we feared that some word would have a huge number of occur-
rences in a disputed paper and dominate the log odds. To prevent this possibil-
ity, we set up truncation rules to prevent any word contributing log odds sub-
stantially larger than those we observed in the 98 papers of known authorship. 
Happily, no word got out of hand in the disputed papers, so we shall not give the 
numerical rules that we prepared for this contingency. We now proceed to 
a more technical discussion of the likelihood ratios that we use. 

122 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.4 
Mathematical statistics makes strong and effective use of the property of 
monotone likelihood ratios in constructing most powerful tests, complete classes, 
and so on. The property is possessed by many likelihood ratios based on standard 
distributions and holds for pairs of Poisson distributions, and for some, but not 
all, pairs of negative binomial distributions. 
The Poisson likelihood ratio, that is, the ratio of the probabilities of x occur-
rences of a given word in a paper of length W under two Poisson distributions 
with mean rates f.Lt, f.L2, has logarithm 
(1) 
Ap(X) = log fp(x I Wf.Ll) = x log f.Ll -
W(f.Ll -
f.L2), 
f p(x I Wf.L2) 
f.L2 
x = 0, 1, 2, ... ; w, f.Lt, f.L2 positive. 
The likelihood ratio for two negative binomial distributions with mean rates 
f.Ll, f.L2 and non-Poissonness rates Ot, 02 has logarithm AUb most simply expressed 
in the parametrization (K, 0): 
(2) 
A () -
I 
fnb(X I Wf.Ll, Kl, W01) 
nb X -
og fnb(X I Wf.L2, K2, W02) 
= log r(x + Kl)r(((2) + x log ~ 1 + W02 
r(Kl)r(X + K2) 
02 1 + WOl 
-
Kl log(l + W01) + K210g(1 + W02), 
Let r = x/w be the observed rate of occurrence of the word. We write the 
log likelihood ratios as 
AP(r, w) = AP(wr), 
to show the dependence on the word rate and length. Because paper length 
varies, rate r has a more constant interpretation than has frequency x. 
The Poisson log likelihood ratio 
(3) 
has several simple properties: it is linear (a fortiori monotone) in r (and x) for 
fixed w; and for fixed rate r, it is proportional to w. If the word frequency 
x = rw, considered as a random variable, has a Poisson distribution with mean 
Wf.L, then the expect~tion and variance of log odds are proportional to w: 
(4) 
E[AP(r, w) I f.Ll = w[f.L log(f.Ldf.L2) -
(f.Ll -
f.L2)], 
V[AP(r, w) I f.Ll = Wf.L (log(f.Ldf.L2)) 2. 
The negative binomial log likelihood ratio is not so simple. For fixed length w, 
as a function of x(or r), Anb is the sum of a linear function and a monotone, 

4.4] 
ODDS FACTORS FOR TH:E NEGATIVE BINOMIAL MODEL 
123 
strictly convex function and hence is convex, but not usually monotone. How-
ever, Anb will be strictly monotone increasing (decreasing) in x when and only 
when 
(5) 
and 
1 + W01 < 1 + W02 
JJ.1 
(» 
JJ.2 
Equivalently, strict monotonicity holds when the non-Poissonness rates and the 
coefficients of variations of the two distributions are in the opposite direction. 
If 01 = 02, the linear component is constant, and Anb is monotone and convex. 
If K1 = K2, the convex component is constant and Anb is linear. If JJ.l = JJ.2, 
but 01 rf: 02, Anb is not monotone. Unless the distributions are the same, Anb 
is unbounded for x large. 
To prove the statements just made, we introduce a temporary notation: 
A(x) = Anb(x), 
'\ ( ) _ I 
rex + K1)r(K2) 
1\1 x -
og r(K1)r(X + K2) , 
A2(X) = A(x) -
A1(X) = const + x log : ! g;;:} 
Then A2 is linear in x and reduces to a constant if and only if 01 = 02. More-
over, Al can be written without use of the gamma function as 
A1(X) = I: log K1 ++ ~. 
i=O 
K2 
'/, 
Hence, Al reduces to a constant when K1 = K2, is monotone increasing when 
Kl > K2, is monotone decreasing when K1 < K2. To show that Al is convex on 
the positive integers, it suffices to show that the second differences are of the 
same sign: 
> 
~'l(x + 1) + l\l(x -
1) -
2A1(X) «) 0, 
for x = 1, 2, .... 
But this follows immediately since 
with 
A1(X + 1) + A1(X -
1) -
2A1(X) = hex + 1) -
hex -
1), 
(x + K1) 
hex) = log (x + K2) , 
and h is an increasing (decreasing) function of x if K1 > K2 (K1 < K2). SO, if 
Kl rf: K2, Al is strictly convex, A2 is linear, and the sum A is strictly convex. 
As x -+ 00, 
I\} (x) = (Kl -
K2) log x + 0(1), 
A2(x) = x [log (W + ;) - log (W + ;) ] + 0(1), 

124 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.5 
so that if 01 > 02, the first differences >.(x + 1) -
>.(x) are positive for suf-
ficiently large x. Continuing with 01 > 02, if >'(1) -
>'(0) is negative, >. is not 
monotone; if >'(1) -
>'(0) is zero, >. is not strictly monotone; if >'(1) -
>'(0) is 
positive, then all first differences must be negative by convexity, and hence>. is 
strictly monotone increasing. But 
>'(1) -
>'(0) = log :: + log (w + :2) - log (w + l~) 
= log 1 + W02 _ log U W01 , 
J.L2 
J.L1 
so that the conditions for monotonicity given in Eq. (5) follow immediately. 
The analysis for 01 < 02 is similar. 
4.4D. Summary. The technical study of the likelihood ratios for Poisson 
and negative binomial distributions and their use in the approximate deter-
mination of odds factors that we have given suffices for our needs, and it also 
illustrates open questions typical of problems that will arise in many applica-
tions. Thus, finding marginal posterior distributions or posterior expectations 
often requires awkward multidimensional integrations, such as those in Section 
4.4B. For the many problems for which explicit integration is impossible and 
modal approximations are not good enough, systematic approximations need to 
be developed and made read·'y available. Asymptotic expansions and well-
designed quadrature and Monte Carlo procedures offer good possibilities. 
The importance of the likelihood ratio in inference suggests that thought 
should be given to what shapes of likelihood ratio functions are sensible and 
that choices of data distributions be judged and guided by the shapes they yield. 
We know too few distributions to give a good selection of shapes. The monotone 
likelihood ratio property, so useful in statistical theory, is not fully in accord with 
our heuristic judgments in our problem or, we suspect, in most discrimination 
problems. 
4.5. CHOOSING THE PRIOR DISTRIBUTIONS 
Our principal device for handling the ill-determinacy of prior distributions is 
to bracket the true prior distribution and to explore the inferences for a variety 
of priors within the brackets. The motivation and applications of the method 
have been discussed in Sections 3.2 and 4.2C, and the logic has been discussed 
in Section 4.3F. Here we consider some of the technical problems in the choice 
of families of distributions, in the estimation of underlying constants to bracket 
the priors, and in the exploration of effects of varying the prior distribution. 
In Section 4.20, the formal assumptions on prior distributions of parameters 
of the negative binomial distribution are set forth. If all non-Poissonness 
parameters are set equal to zero, the assumptions apply also to the Poisson 
parameters. The choice of prior distributions has three parts: choice of param-

4.5] 
CHOOSING THE PRIOR DISTRIBUTIONS 
125 
etrization, choice of families of distributions, and choice of the underlying 
constants. 
These three parts have been discussed in substantial detail in Section 3.2 for 
the Poisson model. As explained in Section 4.2C, we base our choices for the 
negative binomial model in large part on direct or analogous use of the Poisson 
development. Our final analysis and choice of prior distributions is made ex-
clusively for the negative binomial. 
To imply that all of these choices, initial and final, are based on empirical 
data would be wrong. 
Heuristic considerations of reasonableness, analogy, 
trac~ability, availability, and empirical data, all played essential parts. 
4.5A. Estimation of (31 and (32: first analysis. The initial choices of 
values of {3I and {32 were based in part on a variance-component analysis, under 
the Poisson model, of the observed mean rates, ml, m2, with words stratified 
into groups of roughly equal combined rates. We describe this method under 
the negative binomial model as our first analysis of {3I and {32. 
If the distribution of 7', given 0', is a beta distribution with arguments ('Y,'Y) , 
then the variance of the moments estimates t, arising from the negative binomial 
variation and the variation of the rates 7', is 
in which 
and terms of order E'f and higher are neglected. A derivation of Eq. (1) is given 
below. 
If we apply Eq. (1) to the nth word, noting that 'Y, 0', El, E2 all depend on n, 
and if we set 2'Yn + 1 = Cn 
E[4(t-
-
.1)2 10' 5 5 1 = ! [1 - (Enl + En2)] + [Enl + En2]. 
n 
2 
n, nr, n2 
Cn 
20'n 
20'n 
We then use 4(tn -
!)2 as an unbiased estimator of the right-hand side. We 
assume that 8n may be used in place of O'n, and we use, as if exact, the estimates 
of 5nI and 5n2 obtained in the posterior analysis, based on set 21 of underlying 
constants. Each of these contributes a second-order error. We stratify words 
according to the value of 8n , and assume that in eachstr<1tum the argument 'Y 
is constant. Let Nz be the number of words in the lth stratum, and let 
al = __ 1 I>(Enl + En2). 
2Nz 
Sn 

126 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.5 
TABLE 4.5-1 
STRATIFIED MOMENTS ANALYSIS 
Stratum l 
1 
2 
3 
4 
5 
Pooled 
Number of words Nl 
21 
17 
16 
13 
13 
80 
Mean combined rate 8 
1.536 
3.711 
6.280 
11.842 
53.936 
13.137 
Minimum 8 
.76 
3.05 
5.08 
9.96 
18.65 
.76 
Maximum 8 
2.75 
4.56 
8.29 
14.13 
184.91 
184.91 
Ul 
.06914 
.10359 
.01586 
.03423 
.00684 
.05001 
al 
.01144 
.00890 
.00435 
.00231 
.00067 
.00625 
1(1) 
8.07 
4.73 
42.8 
15.1 
80.5 
10.9 
Estimate 'Y (l) for the stratum by equating U I to its expectation. Thus 
" 
2A + 1 
1 - a, 
C(l) = 
'Y(l) 
= U, -
az . 
At the lowest frequencies, the equal weighting of (t -
~)2 in forming Ul is 
undesirable, and we eliminate 6 words with 8 < .75, rather than go into com-
plex weightings. Two other words (Nos. 76, 77) were not used for reasons 
independent of discriminating ability. The remaining 82 words were divided 
into five strata, and Table 4.5-1 gives the stratum sizes and the values of U" aI, 
and the estimated values for 1{l}. It shows also the minimum, maximum, and 
mean of the combined rates, 8, in each stratum. * 
The values of 1(l) for the strata are poorly determined but give some indication 
of the size of 'Y. We use the form 'Y = flt + (32(1, and the ranges 0 ~ (32 ~ 2 
and 5 ~ (31 ~ 20 seem ample. 
If (32 = 0 were assumed, the five strata could be pooled to get a better estimate 
of (31' When this is done, the estimate ~1(O) = 10.85 results. For the pooled 
analysis, equal weighting is not efficient. To gain some notion of the precision of 
the estimate, we apply the "jackknife" technique (Tukey, 1958). Randomly 
divide the 80 words into 8 subgroups of 10 words. Carry out the pooled analysis 
on each of the 8 sets of 70 words (7 subgroups) formed by omitting one sub-
group of 10 words. The estimated values of (31 from the 8 sub analyses are 
shown in Table 4.5-2 as Mj. These estimates are highly correlated with each 
other and with the pooled estimate ~1(O) from the full 80 words. Using the 
essential idea of the jackknife technique, form estimates ~l(j) = 8~I(O) -
7(3ijl 
as shown in the table. These last 8 estimates are approximately independent 
estimates of (31 and can be treated by Student's one-sample method. Their mean 
PI = 9.78 is a better estimate than is ~1(O) and the estimated standard error 
3.76 for the mean PI gives an indication that (31 is not well determined by our 80 
words, even under the assumption fJ2 = 0 that underlies the development. 
* The calculations involved in estimating the (3's were c~rried out by I vor S. Francis, 
Charles L. Odoroff, and Cleo Youtz. 

4.5] 
CHOOSING THE PRIOR DISTRIBUTIONS 
TABLE 4.5-2 
JACKKNIFE ESTIMATION OF f3I, ASSUMING f32 = 0 
Random subgroup 
1 
2 
3 
4 
5 
6 
f3I*; 
9.88 
10.08 11.13 10.07 11.88 
9.67 
SIU) = 8SI(O) -
7f3I*; 
17.65 16.30 
8.95 
16.33 
3.65 19.17 
Pooled estimate: 
Sl(O) = 10.85. 
Jackknife estimate: 
PI = EBIU) = 9.78. 
8 
sa 
Estimated standard error of PI: 
v8 = 3.76. 
7 
11.09 
9.18 
127 
8 
14.26 
-12.99 
To derive Eq. (1), use Eqs. (2), (8), and (10) of Section 4.1, for any word, 
and for author i, to give: 
where Ei depends on the non-Poissonness ~i and is the reciprocal of the length of 
Poisson text that would give the same variance of the rate mi. For us, Ei ~ 
.01(1 + 2~i), and the following moment calculations are formal asymptotic 
expressions with terms of order E2 neglected. We usually omit indications of 
the error term. 
In the formulas given later in this paragraph, all moments are computed 
conditional on the parameters a = (u, r, ~I' ~2)' The usual delta method based 
on term-by-term expectations of Taylor's series expansions of functions of mI, m2 
about JJ.I, JJ.2 yields 
E(s I a) = u, 
V(s I a) = JJ.IEI + JJ.2E2 = u[rEI + (1 -
r)E2], 
E(ll a) ~ r + -r(1 -
r) [E2 -
Ed, 
u 
1 2 
r(l - r) [( 
~ (r -
2) + 
2 -
3r)EI + (3r -
1)E21. 
(J' 

128 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.5 
Now average over the symmetric beta distribution of 7, conditional on (1', 
with arguments (,Y, 'Y), using the moments of the beta density: 
We get 
E(r 1 (1') = !, 
V(r 1 (1') = 4(2'Y ~ 1) , 
E((r -
!)3) = O. 
-I 
'Y(€2 -
€1) 
E(t (1', Ob 02) ~ 2(1'(2'Y + 1) , 
E((l- !)21 (1',01, 02) ~ 4(2'Y 1+ 1) [1 + ~ (€1 + €2)] , 
the latter as given in Eq. (1). 
4.5B. Estimation of PI and P2: second analysis. A naIve procedure for 
estimating the underlying constants of families of prior distributions is to use 
the estimated word-rate parameters as if they were exact, and thus reduce the 
problem to one of single-stage estimation. For then the "exact" parameter 
values are a sample directly from the prior distribution whose parameters are 
the underlying constants. If unbiased estimates of the word-rate parameters 
are used, the prior distributions are typically estimated to have excessive spread. 
That suitably regressed estimates of the word-rate parameters can provide 
useful estimates of the prior distributions is the hope underlying the procedure 
to be described. 
For exposition of the procedure, consider the special problem of estimating 
{3] when {32 = 0 is assumed. We then have differential-rate parameters 7], ... , 
7N independently sampled from a beta density with arguments ({3], (31). If we 
could observe the parameters 7], ... ,7N, the estimation of {31 would be a 
straightforward estimation problem, requiring a solution of a transcendental 
equation to get the maximum likelihood estimate; however, this solution is 
easily accomplished iteratively. We cannot observe the 7 n, but we observe data 
from which estimates of the 7 n can be derived. To use the estimates tn as if 
tn = 7 n would yield estimates of {31 that are too small. 
Suppose, instead, that we use estimates of 7 n obtained from a posterior 
analysis based on a beta prior density with a preliminary choice {310 for each of 
the arguments. The estimates of 7 n will be regressed toward! from tn. Empirical 
distributions of these regressed estimates may give an improved idea of the 
shape of the prior distribution of 7, for the great variation between words in the 
sampling variances of the estimates has been largely eliminated, provided the 
preliminary prior distribution is roughly appropriate. Suppose that using these 
regressed estimates as though they were exact leads to an estimate {311 for {31' 
How should we interpret this estimate {311, especially in relation to the pre-
liminary choice {31 0 '? 

4.5] 
CHOOSING THE PRIOR DISTRIBUTIONS 
129 
In an analogous binomial sampling problem to be described shortly, an exact 
analysis is possible and provides some guides for interpreting results in our 
problem. The percentage error between {311 and a method-of-moments estimate 
of {31 has an asymptotic expansion with the leading term proportional to the 
difference between the estimate {311 and 2{310, twice the preliminary value used. 
If this difference is small relative to {311, then {311 is close to the method-of-
moments estimate, the difference being small relative to the standard error of 
either estimate. 
We carried out this estimation procedure in our problem with the substantial 
additional complication that we did not assume {32 to be zero. Using preliminary 
values {310 = 5, {320 = 1 and the resulting modal estimates of {Tn} from set 
21 of underlying constants, we obtained new estimates {311 = 9.0, {321 = .1. 
Because {311 -
2{310 is small, we conclude that {311 = 9 is a good estimate of {31. 
We use {31 = 10 in much of our analysis, with occasional explorations with 5 
and 15. The binomial analogy is not relevant to the estimation of {32, but from 
{321 = .1, we conclude that {32 plays no important role, and we use {32 = ° in 
much of our analysis. 
DERIVATION FOR A BINOMIAL PROBLEM 
Let Zn denote the number of successes in r binomial trials with probability 
7r n of success. Suppose that there are N sets of trials and assume that the prob-
abilities 7rl, ... , 7rN are independently sampled from a beta distribution with 
each of its arguments equal to (3. 
Let Un = zn/r denote the proportion of 
successes. Compute its first two moments: 
E(un I (3) = E[E(un I (3, in) I (3] = E(in I (3) = !, 
E[(un -
t)2 I {3l = E{E[(un -
t)2 I {3, inl/ {3} 
= E r 
i n (1 ;- in) + (in -
t)2/ {3 J 
= 4~ + 4(2/+ 1) [ 1 -
~ l 
The method-of-moments estimate of (2{3 + 1)-1 is 
(2) 
[(4/N) L(un -
!)2 -
(l/r)] 
[1 -
(l/r)] 
If we could observe 7rl,"" 7rN, the method-of-moments estimate of 
(2{3 + 1) -1 would be 
(For example, let r -7 00 in the estimate based on un.) If we use the posterior 

130 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.5 
modal estimate of 7r n, 
A 
Zn + b 
7rn = r + 2b' 
based on a preliminary estimate fJ = b, as though it were exactly 7r n, we get 
as an estimate of (2fJ + 1)-1: 
(3) 
(4/N) :E(Un _ !)2 
(1 + (2b/r»)2 
From each of the estimates in Eq. (2) and Eq. (3), we get estimates of fJ. 
Call them ~, fJlI respectively. If we aSsume that r is large relative to fJl, and 
expand in terms of l/r, we find that the relative error (fJl -
~)/~ has as a 
leading term: 
2fJl + 1 2(2b -
fJl) 
2fJl' 
r 
Thus, if the preliminary value b is close to half the resulting estimate fJlI then 
the estimate fJl is close to the method-of-moments estimate. 
Our estimates tn of Tn have likelihoods similar to that for binomial proportions, 
except that the effective number of trials varies from word to word. We use the 
result for simple binomial trials to guide interpretations of the procedure in more 
complex problems. 
4.5C. Estimation of {la. The prior distribution for 1/, the differential trans-
formed non-Poissonness rate, is assumed to be a beta density with arguments 
(fJa, fJa) and to be independent of iT, T, ~. Table 4.5-3 shows a frequency dis-
tribution of moments estimates of 1/ and of the estimates obtained from the 
posterior analysis based on set 21 of underlying constants (with fJa = 6), for 87 
unselected words. 
The most evident feature of the frequency distributions is decided asymmetry. 
Hamilton's non-Poissonness runs consistently less than Madison's. To worry 
much about estimating the argument fJa of a symmetric beta density is pointless. 
An explanation of this asymmetry lies in the excessive variation between 
Madison's writings internal and external to The Federalist. This variation has 
importance far beyond the differences in non-Poissonness and is treated in 
Section 4.10. The net effect of the decisions described there is that words which 
showed excessive variation were eliminated from the final words, and the non-
Poissonness rates are much closer in the words actually used. 
We applied the method described for fJl in Section 4.5B to get a rough estimate 
of fJa under the symmetry assumption. If the estimates from the set 21 posterior 
analysis are used as if they are the true 1/'S sampled from a beta density with 
arguments (fJa, fJa), a maximum likelihood estimate, fJa = 13.6, results. The 
binomial analogy introduced in Section 4.5B seems appropriate here, too, and 
since the estimate 13.6 is close to twice the preliminary value fJao = 6, the es-

4.5] 
CHOOSING THE PRIOR DISTRIBUTIONS 
TABLE 4.5-3 
FREQUENCY DISTRIBUTIONS OF ESTIMATES OF 'Y/ 
FOR 87 UNSELECTED WORDS. * 
Frequency 
Method-of-moments 
Modal estimates, 
Estimate 
estimates 
set 21 
o to .30 
21 
1 
.30 to .35 
13 
6 
.35 to .40 
6 
12 
.40 to .45 
13 
22 
.45 to .50 
12 
26 
.50 to .55 
7 
10 
.55 to .60 
6 
7 
.60 to .65 
1 
2 
.65 to .70 
1 
1 
.70 to 1.00 
7 
0 
Total 
87 
87 
* Word Nos. 76, 77, and 115 excluded. 
131 
timate (13 = 13.6 is fairly good without much correction. The standard error 
of the estimate is large, and we use a wide range of values of (13' The bulk of the 
log odds are based on the choices (13 = 12 and (13 = 6. The values 1.5, 18, and 
30 are each used in one set of underlying constants. The choice of (13 does not 
have the systematic strengthening or weakening effect on log odds that the choice 
of (11 does, as will be illustrated in Section 4.5E. 
4.SD. Estimation of {34 and {35' The prior distribution for ~, the combined 
transformed non-Poissonness rate, is assumed to be a gamma density with mean 
{34 and argument (35(degrees of freedom 2(35). Estimates of non-Poissonness are 
relatively poor, and we have no hope of determining {34 or {35 well. The logarith-
mic transformation to ~ was chosen so that the prior densities used would not 
suppress evidence of large non-Poissonness. 
An initial choice of {34 = 1.25 and {35 = 2.0 was based in part on a prelim-
inary study of the estimates of ~ for the 22 words shown in Table 4.2-1. The 
mean of the resulting modal estimates of ~ for 87 unselected words was .84, 
and most of the later sets and side studies were based on (14 = .83, (15 = 1.2. 
With the alternative choice of modes and the resulting alternative interpre-
tation of the prior distribution described in Section 4.2F, the earlier choice with 
{3: = .62, (1~ = 1.0 comes much closer to representing the evidence from the 
unselected words than does the later choice with {3! = .14, {3~ = .2. 
Note 
that (1: = .14, (1~ = .2 gives a prior density with an unbounded mode at 0 and 
a very small mean. Set 22 was chosen over set 31 as the preferred choice of 

132 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.5 
underlying constants to recover the preferred ,6: = .62, ,6~ = 1.0. Of over-
riding importance in all the treatment of ,64 and ,65 is the evidence developed in 
Section 4.5E that the differences in ,64 and ,65 have very little effect on the log 
odds. 
4.SE. Effect of varying the set of underlying constants. Log odds for 
the final word groups were evaluated for the sets of underlying constants 5, 11, 
21,22,23,31,33,35,36,37, and 38, which we use to study the effects of varying 
the underlying constants. We chose these sets to aid the study of changes in 
individual underlying constants. Thus, sets 
,61 
,62 
,63 
,64 
,65 
37: 5} 
31 : 10 
0 
12 
.83 
1.2 
33 : 15 
permit direct study of ,61 ; sets 
21 : 5 
11 
5 : 5 
~) 
6 
1.25 
2.0 
23 : 10 
permit study of ,62 or of ,61 and ,62; sets 
36 : } 
t } 
22 : 10 
0 
12 
1.25 
2.0 
23 
6 
and sets 
21 
~ } 5 
1 
{ ~.5} 
1.25 
2.0 
11 
permit study of ,63; sets 
31 : }1O 
0 
12 
{ .83 
1.2 
22 : 
1.25 
2.0 
and sets 
38 
~ }5 
5 
6 
{ .83 
1.2 
5 
1.25 
2.0 
permit study of ,64 and ,65; sets 
31 : }1O 
35 : 
0 
12 
{ .83} 
.72 
1.2 
permit study of fh Some study of interaction is possible. 

4.5] 
CHOOSING THE PRIOR DISTRIBUTIONS 
133 
The effect of varying ~l and ~2 depends strongly on the word rate. Most of 
the detailed study is devoted to the word groups B3B and B3G. The behavior 
for the total log odds is a composite of the behavior for the high-frequency group 
B3B, that for the low-frequency groups B3G, B3E, B3Z, of which B3G is typical, 
and that for upon, B3A. The behavior of upon is made more complex by some 
phenomena discussed in Section 4.5F. 
' 
Examination of the log odds suggests that, for single homogeneous word groups, 
much of the variation caused by a change in the underlying constants can be 
explained by a proportional change in log odds. Choosing set 31 as a standard 
for comparison, and denoting log odds for set S and a paper j by As ,j, 
For any set of papers, we use a ratio estimate of Ps: 
~As.j(sgn A3l.;) 
rs = 
. 
~ IA3ul 
Let 
qs = logrs 
so that qs -
qs' is an estimate of the logarithm of the proportionality factor 
Ps/Ps' between sets Sand S'. All of our discussion in this section will be on the 
logarithmic scale of per cent reductions or increases. 
Estimates are based on papers from the 2000-word set (see Section 6.2). We 
use all 9 disputed and joint papers from this set, and 6 Hamilton (12, 21, 29, 
34, 65, 72) and 6 Madison (14, 45, 202, 208, 219, 301), chosen to be well dis-
tributed over the known papers. 
Table 4.5-4 lists the estimates qs -
qs' of 10g(Ps/Ps') for comparisons among 
those listed at the beginning of the section. The estimates are given for word 
groups B3B, B3G, and Total, and separately for each of the three sets of papers. * 
The proportionality assumption is occasionally poor, and the separate 
estimates of Ps from the three sets of papers (Hamilton, Madison, and Disputed 
and Joint) give some idea of the failure in lieu of an analysis of a more compre-
hensive model. In these studies, the only essential difference between papers by 
Hamilton and Madison is that log odds for Hamilton are largely positive, for 
Madison largely negative. The Madison and Disputed papers used include 
several that for group B3G have positive log odds. 
Attend first to the least important underlying constants, ~4' ~5' The 35~31 
comparison shows that with ~5 = 1.2 (or ~: = .2), the ~4 change from .83 to 
.72 (~: from .14 to .12) consistently makes about a 1 per cent change in log odds. 
Similar studies using sets 21, 8, 9, available for a few words, show that for 
~5 = 2.0 (or M = 1.0) changing ~4 from 1.25 to 1.54 or to .91 (~: from .62 
* The calculations in this section were carried out by Robert Kleyle and Marie 
Yeager. 

134 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.5 
TABLE 4.5-4 
ESTIMATES OF LOGARITHM OF PROPORTIONALITY FACTOR BETWEEN LOG ODDS 
FOR SETS S, Sf OF UNDERLYING CONSTANTS: LOG(pS/Ps') 
Word group 
B3G 
B3B 
Total 
Papers 
6H 
6M 
9DJ 
6H 
6M 
9DJ 
6H 
6M 
9DJ 
Sets 
S 
Sf 
37 
31 
.27 
.30 
.2S 
.02 
.02 
.02 
.16 
.14 
.10 
31 
33 
.20 
.20 
.20 
.02 
.02 
.02 
.17 
.10 
.OS 
21 
5 
.OS 
.12 
.05 
.13 
.15 
.15 
.21 
.12 
.13 
21 
23 
.22 
.10 
.20 
-.02 -.02 -.02 
.12 
.OS 
.05 
23 
5 -.15 
.02 -.15 
.15 
.17 
.17 
.10 
.05 
.08 
36 
22 
.04 
.03 
.06 
.04 
.02 
.02 
.02 
.04 
.03 
22 
23 -.02 -.17 -.03 
.00 -.02 -.02 
.06 -.03 -.02 
11 
21 
.03 
.12 
.16 
-.04 
.04 
.04 
-.07 
.OS 
.06 
31 
22 
.04 
.04 
.05 
.03 
.04 
.04 
.06 
.04 
.04 
38 
5 
.02 
.02 -.01 
.04 
.04 
.04 
.03 
.03 
.03 
35 
31 
.005 
.004 
.005 
.005 
.005 
.004 
.010 
.005 
.005 
to .77 or to .45) consistently makes less than one per cent ..change in log odds 
for most words and in total. The effect rises for upon to a three per cent reduction 
for the increase in (34, and a 10 per cent increase for the reduction in (34' In 
summary, when (35 = 1.2 or 2.0, varying (34 over a moderate range changes the 
log odds by less than one per cent. 
The choice between (34 = .83, (35 = 1.2, and (34 = 1.25, (35 = 2.0 (or the 
choice between (3: = .14, (36 = .2 and (3: = .62, (3~ = 1.0) has a larger effect, 
which consistently gives a log ratio of about .04 between the first and second 
choices. This holds for comparison 31-22 with «(31, (32, (33) = (10,0, 12) and for 
38-5 with «(31, (32, (33) = (5, 5, 6). 
Look next at the comparisons of (33 changes: sets 36-22-23 and sets 11-21. 
The (33 changes interact with word and sign of the log odds, but a few conclusions 
are clear without a more thorough analysis. Increasing (33 from 12 to 30 gives a 
slight consistent increase in log odds (by a log ratio of about .03). Decreasing (33 
from 12 to 6 increases the log odds again, except that positive total log odds 
(Hamilton) are decreased. A further decrease of (33 from 6 to 1.5 gives a further 
and larger increase in log odds, again, except for positive log odds in total or 
for the high-frequency words. The range of (33 considered here is very wide, 
and the (33 = 12 used as the "best" seems to give nearly the smallest log odds 
with very little change between 10 and 15. 
Finally, attend to the first four rows of Table 4.5-4 displaying effects of 
varying the most important underlying constants (31 and (32. The 37-31 and 

4.5] 
CHOOSING THE PRIOR DISTRIBUTIONS 
135 
31-33 comparisons show the effects of changing (31 from 5 to 10 to 15. The 
effects are large on the low-frequency words and very small for the high-fre-
quency words. In total, an increase of five in (31 decreases log odds by about a 
log factor .10, or a little more if log odds are positive. Total log odds decreases 
as (31 increases at a rate of about two per cent per unit change in (31. 
The 21-5 comparison shows that increasing (32 has a strong effect on high-
frequency words, and, for a special reason discussed in Section 4.5F, on upon, 
whilst and a few very strong discriminators. For papers with negative log odds, 
total log odds decrease at a rate of about three per cent per unit increase in (32' 
The 21-23 and 23-5 comparisons give some indication of the comparative 
effects of changing (31 and (32. Recall that (31, (32 enter only through (31 + (32(1', 
so that for any single word equal changes in (31 and (32(1' have identical effects. 
In total, set 23 with «(31, (32) = (10, 0) is intermediate to set 21 with (5, 1) and 
set 5 with (5,5). If we assume no interaction between the effects of «(31, (32) 
and «(33. (34, (35), we may compare changes from set 31 to 33 or 37 with changes 
from set 23 to 5 or 21. In total for the 3 sets of papers, we then have: 
(5,0) : 
.16 
.14 
.10 
(5, 1): 
.12 
.08 
.05 
(10,0) : 
0 
0 
0 
(5,5) : 
-.10 
-.05 
-.08 
(15,0) : 
-.17 
-.10 
-.08 
The errors of estimation and interaction prevent a sharp conclusion, but an 
effective "average" combined rate (1' :::< 1.5 for the 30 words appears reasonable 
and fits with the earlier estimates that an increase of (31 by 1 decreases log odds 
by about two per cent, and an increase of (32 by one decreases log odds by about 
three per cent. 
What adjustments or allowances seem appropriate for the log odds calculated 
from set 22, because of the indeterminacy or poor choice of underlying con-
stants? First, the choice of 22 seems about as good as any on the basis of all the 
data. Choosing (32 = 1 might be better than (32 = 0, but then a smaller value 
of (31 than 10 would be appropriate (see Table 4.5-1). The (5,0) and (15,0) :::< 
(13.5, 1) choices for «(31, (32) seem quite ample brackets and lead to about a 
10 per cent increase or decrease in log odds. Any change from (33 = 12 would 
tend to increase log odds for the disputed papers, though not by more than three 
per cent for the range 6 to 30, which is quite wide. Changes in (34 and (35 would 
seem to have a small effect. Overall, a 12 per cent allowance in log odds for 
ill-determined priors is generous, and the change from set 22 could as well be 
an increase as a decrease. 
4.SF. Upon: a case study. A detailed study of the estimation problem for 
upon illustrates some possible strong and strange effects of the tails of the prior 
distributions in interaction with a four-dimensional likelihood surface. Table 
4.5-5 shows for selected sets of underlying constants the modal estimates of 
(1', T, 01, 02, the first two for both negative binomial and Poisson models. 

136 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.5 
TABLE 4.5-5 
MODAL ESTIMATES OF PARAMETERS FOR upon 
Underlying constants* 
Poisson 
Negative binomial 
Set 
number 
(31 
(32 (31+ Unbr:?2 
(J 
T 
(J 
T 
01 
02 
2 
2 
1 
5.5 
3.49 
.949 
3.49 
.946 
.23 
.43 
21 
5 
1 
8.5 
3.48 
.943 
3.50 
.930 
.26 
.63 
23 
10 
0 
10.0 
3.48 
.939 
3.53 
.919 
.27 
.83 
3 
10 
1 
14.2 
3.47 
.932 
4.25 
.767 
.36 
9.07 
4 
20 
1 
25.3 
3.46 
.912 
5.28 
.622 
.38 
26.7 
5 
5 
5 
32.7 
3.46 
.914 
5.54 
.592 
.38 
31.7 
6 
2 
10 
62.2 
3.44 
.889 
6.02 
.546 
.39 
41.2 
7 
20 
10 
81.7 
3.42 
.861 
6.17 
.534 
.39 
44.9 
lOt 
5 
1 
8.5 
3.48 
.943 
3.48 
.938 
.28 
.37 
l1t 
5 
1 
8.9 
3.48 
.943 
3.92 
.837 
.14 
5.56 
Method-of-moments estimates 
3.49 
.960 
3.49 
.960 
.12 
.36 
* Except as noted, (33 = 6, (34 = 1.25, (35 = 2.0. 
t For set 10, (33 = 12; for set 11, (33 = 1.5. 
The behavior of the Poisson estimates is quite trouble-free. The effect of 
(3t, (32 on the estimation of T is, apart from an adjustment for the unequal total 
lengths of Hamilton and Madison text, almost that of adding 131 + f32CT to the 
observed counts for each author. Increasing /31 or /32 has little effect on (1', and 
decreases T slowly. 
The observed counts were 315 and 16, respectively, on 
94,000 and 114,000 words. At the same rates in an average 104,000 words, the 
counts would be 348 and 15. For estimating T, the prior information from the 
extreme set 7 is worth about 131 + f32Up = 54 counts for each. Thentherough 
estimate of T would be 402/(402 + 69) = .854, compared to the value .861 
which was actually obtained. The strong prior information has its effect, but 
because the prior information as represented by its density is almost equivalent 
to the likelihood of fictitious data of the same form as the observed data, it is 
absorbed by the data, and the net output is as expected. 
With the negative binomial model, the story is totally different. For set 7, 
the estimated mean rates are 3.30 for Hamilton, a little low but reasonable, and 
an astounding 2.87 for Madison, compared to an observed rate of .14. Estimated 
non-Poissonness rates of .39 and 44.9 complete the strange set of estimates. 
Madison's distribution is thus estimated to have a mean nearly as large as 
Hamilton's and a huge varlance. Were such parameters true, the distribution 
of word rate in a 2000-word paper by Madison would have a mean of 2.87 and 
a standard deviation of 11.7. To observe 41 zeros and a maximum rate of 1.38 
in 50 papers would be surprising but not unbelievable; an observed mean rate 
of .14 in 50 papers would be only 1.7 standard errors below the expected 2.87. 

4.5] 
CHOOSING THE PRIOR DISTRIBUTIONS 
137 
TABLE 4.5~6 
SELECTED LOG ODDS FOR upon 
Set of underlying constants 
Rate 
21 
7 
0 
-5.0 
-4.6 
.49 
-2.0 
-
.6 
.96 
-
.3 
+ .7 
2.l1 
+2.4 
+2.2 
3.27 
+4.2 
+2.7 
5.08 
+5.7 
+2.4 
7.52 
+6.6 
+1.1 
Although the estimated parameters represent a distribution that is not totally 
impossible, how can they be the mode of the posterior density? Roughly, the 
two non~Poissonness parameters give extra degrees of freedom not available in 
the Poisson model. There the extreme tails of the beta density balanced the 
extreme tails of the data distributions, and a sensible intermediate estimate 
resulted. But the negative binomial likelihood for rare words has a slowly 
falling ridge, as Il, 0 increase simultaneously. Thus the decrease in likelihood 
as 02 and 112 are increased is small and is more than offset by the increase in 
prior density as T is moved toward .5. The prior distribution of ~ has little effect 
because of the logarithmic transformation used on o. The beta density for the 
differential non-Poissonness 1} has a stronger effect in stopping the shift. For, 
unless (33 is small, as the two non-Poissonnesses 01, 02 are separated, the beta 
prior density of 1} decreases sharply. To see this effect, compare the behavior 
of sets 10, 21, and 11, with (33 at 12, 6, and 1.5, respectively, all with (31 = 5, 
(32 = 1. For the first two, there is little increase in 02, but for set 11 with ot, 02 
almost independent, a priori, the shift is quite strong. 
The changes in the fitted distributions are not as pronounced as the changes 
in the parameters. Thus the Madison distribution of number of occurrences 
based on the set-21 estimates has a small mean and variance, with most of the 
probability at 0, and the remaining probability falling rapidly as the number of 
occurrences increases. The distribution based on the set-7 estimates has a large 
probability at 0 and a very long tail. The effect on the log likelihood ratio 
between Hamilton and Madison is relatively moderate. Using set 7, rates of 0 
get large negative log odds as before; moderate rates (near the Hamilton mean) 
get reduced log odds; very large rates (higher than any observed) would receive 
negative log odds, because they would seem more likely to come from the long 
tail of the Madison distribution than from the Hamilton distribution. To illus-
trate the effect, Table 4.5-6 shows the log odds based on sets 21 and 7 for a 
selection of rates, for papers of 2000 words in length. 
In summary, we find that as the beta prior density of T is made tighter, the 
estimates for 112 and 02 both increase sharply. This shift is triggered by the beta 

138 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.6 
prior, but is really a fact of the negative binomial likelihood function for rare 
words. The data alone provide little evidence to choose among these sets of 
estimates, and prior or external judgments about the range of parameter values 
for which the negative binomial model is reasonable are essential. 
Finally, note what would have been the effect of using a prior density for T 
with higher tails than the beta. The phenomenon here would not have occurred, 
because there would be much less pressure to decrease T. The estimated value 
of T would be larger, the non-Poissonness moderate, and the discriminating 
ability high or higher than for any sets shown. 
4.5G. Summary. Sections 4.5A through 4.5D present several techniques 
used to estimate the values of the underlying constants. In Section 4.5E, the 
study of the effects on the log odds of varying the sets of underlying constants or, 
equivalently, the prior distributions, is straightforward and its results are satis-
fying: the variation over the plausible ranges causes moderate changes relative 
to those from other sources. 
The special study of Section 4.5F illustrates some surprising effects of shapes 
of prior distributions, and suggests a point that is likely to be important gen-
erally in Bayesian inference: when a standard distribution is used as a prior dis-
tribution, the effect of its small tails may be very different from the effect of 
those tails when the distribution is used as a data distribution. The intuitive 
understanding that statisticians have for the latter effects can be misleading in 
thinking about the effects of prior distributions. Thus, using a prior density 
with realistic tails in place of the beta distribution that we used would lead to 
an assessment of evidence for discrimination that is stronger, not weaker. 
4.6. MAGNITUDES OF ADJUSTMENTS REQUIRED BY THE 
MODAL APPROXIMATION TO THE ODDS FACTOR 
Since we have not evaluated the odds factor, but introduced an approxima-
tion for it, a notion of the degree of accuracy should be produced. This section 
provides some evidence on accuracy, mainly by examples. The reader should 
understand that we are not trying to set bounds on the errors, but we are trying 
to get a general idea of the magnitUdes of the appropriate adjustments. 
4.6A. Ways of studying the approximation. For purposes of this dis-
cussion we accept the negative binomial distribution for the data as correct. 
Let x be the number of occurrences of a given word in an unknown paper of 
length w. Let fJ.i, Ki, and Oi, i = H; M, be the parameters for the given word. 
Then the approximate evaluation of odds used in the main study can be written 
(1) 
x 
E[fnb(X I WilH, KH, W5H)] 
fnb(X I WP,H, KH, W5H) 
), 
e = E[fnb(X I WilM, KM, W5M)] "'" fnb(X I WP,Mf KM, W5M) = e . 

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
139 
Here the P-'s and 5's are point estimates of the parameters which were obtained 
from the mode of the posterior distribution when the parametrization I' = 
«(1, T, ~, 'r}) of Section 4.2C is used. Fix x and w, and in the 'Y-parametrization, 
let 
Then the log odds can be written 
(2) 
where each expectation is taken with respect to the posterior distribution of 1 
and can be written as an integral: 
(3) 
Each (x, w) pair leads to changes in qH and qM. 
If we knew the mean, 'Y, and the covariance matrix, [cij], of the posterior 
distribution of 1, we would gladly use the first one or two terms obtained by the 
delta method to give an approximate evaluation of the integrals shown in 
Eq. (3): 
(4) 
E(q(1») ~ q('Y) 
~ q('Y) + ! LLqij{'Y)Cij 
(first order) 
(second order). 
These formulas are exact for linear and quadratic functions, respectively, and 
they are the leading terms in the usual asymptotic statistical developments. 
But the necessary means and covariances are not available, because they require 
heavy integration. 
What we can find are modes and derivatives of the log posterior density. If 
the posterior density is normal, the mode and the mean coincide, and the co-
variance matrix is the inverse of the matrix of second derivatives. Even if the 
posterior density is not normal, we might proceed as if it were. Then, according 
to the most natural asymptotic theory, we would be omitting a term of the same 
order as the covariance term. The omitted term corresponds to the deviation 
of the mode '9 from the mean 'Y, and involves third derivatives of the log density. 
To explore the adequacy of the modal approximation 
E(q(1)) ~ q('9) , 
we might follow one of several courses. In Eq. (4) we might replace 'Y by 1 
and [cij] by the inverse of the matrix of second derivatives of the log posterior 
density, and assume or hope that '9 -
'Y is small enough to be neglected along 
with higher-order terms. We might use '9, but evaluate the third derivatives 
needed for the omitted terms. 
We might choose a mode relative to some 
weighted measure element b('Y) d'Y instead of with respect to the Lebesgue 
measure element d'Y, in hope that the relative mode would be nearer to the 
mean. For example, were the posterior distribution a beta density, its mode 

140 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.6 
relative to the element d'Y /['Y(1 -
'Y) 1 is equal to its mean. For densities similar 
to a beta density, the same weight might be adequate to reduce the contribution 
of the 'Y -
'Y term. Finally, we might choose a transformation of variables 
and assume that the mode of the transformed variable is near to its mean. 
The choice of a transformation or of a weight function b('Y) would have to be 
based on a combination of analogy, intuition, and a few shreds of evidence. 
The use of modes and logarithmic derivatives of densities relative to weights 
now seems attractive, and we would try to use this device were we to repeat the 
main study. 
What we do is introduce a further transformation of the average transformed 
non-Poissonness E to Za = log E, and assume that Zl = U, Z2 = T, Za, and 
Z4 = 7J have.a density for which the mean and mode coincide. To avoid diffi-
culties with neglected terms, we assume that the z's have an exactly normal 
posterior density. 
4.6B. Normal theory for adjusting the negativehinomial modal ap-
proxImation. To assess the effect of the approximation A = }. shown in Eq. 
(1), we shall describe some explorations of the accuracy of }. for a few words. * 
These words are 40 on, 58 to, 60 upon, 90 enough, and 117 whilst. They repre-
sent various degrees of rarity and contain most of the very strong discriminators 
in the final list of 30. On and upon are the two strongest words, whilst is the 
strongest rare word, enough has the largest non-Poissonness rate, and to is a 
strong high-frequency word. We turn now to the theoretical formulation. 
For any word, write the posterior density of 'Y as 
(5) 
where a = (u, T, 01, 02) is the other parametrization discussed and used in 
Section 4.20. The computation described in Section 4.2D obtains the mode 'Y of 
the density p('Y) by locating the minimum of the function h. In the iterative 
process used, the matrix of second derivatives of ..y(a) with respect to the com-
ponents of the a-parametrization, and the inverse of this matrix, are evaluated. 
The final output of the computation is, principally, 
[ail, 
[~ijl, 
[~ijl, 
where A indicates that each function is evaluated at the mode 'Y. 
That the output did not give the derivatives of h with respect to the 'Y-
components is now a minor nuisance, but they can be obtained by a few extra 
calculations. By the chl;tin rule, 
~ 
_ """ iJ2..y 
iJau iJav + ,,~ iJ2au . 
iJ'Y . iJ'Y' -
LJ LJ iJa iJa iJ'Y' iJ'Y' 
LJ iJa 
iJ'Y' iJ'Y . 
O} 
uv 
uv 
O} 
U 
U 
O} 
* Preliminary calculations for this study were carried out by I vor Francis and 
Charles L. Odoroff. 

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
Since all first derivatives of h or if; vanish at the mode, 
and explicit matrix inversion is immediate: 
hij = L: L: ~uv a1i a1j . 
aOiu aOiv 
u 
v 
For the rest of this section, we assume that 
are all available for use. 
141 
If the distribution of :y were exactly normal, 1 would be the mean and [h ij] 
the covariance matrix (and no iteration would have been necessary). As we 
mentioned in the previous section, the hard fact is that 1 may differ substantially 
from 'Y; indeed, the error may be of the same order as the variance. But [hij] 
is, to a first-order approximation, the covariance matrix of :Yo 
For the five words, Table 3.7-1 shows the components {ai} of Oi at the mode 1, 
and approximate standard deviations vIP of each ai. For the low-frequency 
words upon, enough, and whilst, the distributions of as = 51 and a4 = 52 must 
be severely skewed to the right to show such large apparent standard deviations 
relative to the mode. 
The transformation from (5 1,52) to (tl, t2) with ti = log(l + Oi), and then 
to (~, "l) with ~ = tl + t2 and "l = ttl ~ reduced the right-hand tails of the 
distributions of 51 and 52, and concentrated most of the remaining skewness in 
the distribution of 1. For the range of values of 5 for the final 30 words, the 
transformation log(l + 5) has too little effect; the words, mostly pronouns, 
with values of 5 ranging as high as 7 have been discarded. A stronger transforma-
tion is needed. We choose a very strong one that is convenient for this special 
study; we transform ~ to log ~ and leave u, T, "l unchanged. 
If we let 
(6) 
Zs = log 'Ys, 
Z4 = 1'4, 
then the density of z is obtained from that of :y by 
where eZa is the Jacobian of the transformation. Taking the negative of the 
logarithms, and setting fez) = -log p(z), h('Y) = -log p('Y) , we get 
fez) = h('Y) -
Zs· 

142 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.6 
If we now assume that Z has a normal distribution with mean !, precision matrix 
[Iii], covariance matrix [Iii], then 
(7) 
I(z) = i :E (Zi -
zi)/ij(Zj -
!j) = h('Y) -
Z3' 
t.J 
We know the mode of the distribution of 'Y and the second derivatives of h at 
the mode. Implicit differentiations give equations to determine! and [hj]: 
(8) 
where 
u = 1,2,4, 
u = 3. 
u = 1,2,4, 
u = 3, 
Evaluate each function at the mode 1, and use a A to indicate this evaluation. 
From the liIecond-derivative equations, we get 
where, by the assumption of normality, the second derivative III.'/} does not 
depend on the point of evaluation. Explicit matrix inversion is easy because of 
the fortunate choice of transformation, and it yields the covariance rv: 
Proceed to the first-derivative equations. Evaluation at the mode makes 
hu = 0 in Eq. (8), and therefore all of the right-hand side vanishes except the 
"-1," and we have the simultaneous equations 
u = 1,2,4, 
u = 3, 
for the mean z, and the solution in matrix notation 
(9) 

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
Writing out the components of Eq. (9) gives 
(10) 
If we let 
-
. + f13 
ZI = ZI 
, 
-
. + f23 
Z2 = Z2 
, 
Z3 = Z3 + f33, 
Z4 = Z4 + f43. 
143 
then the modal approximation in the z-parametrization to the expected likeli-
hood, given author H, is 
(11) 
and the first-order correction, call it CH , is 
CH = ! L: '{jH.iJf ij, 
i,i 
where '(jH.ij is the second partial derivative with respect to Zi and Zj, evaluated 
at Z. If z is exactly normally distributed and if gH is linear in z, approximation 
(11) is exact; if gH is quadratic, then the quadratic approximation 
is exact. 
Making analogous definitions and computations for Madison, we get gM(Z) 
and CM. Our interest is in the log odds A, so let X, XQ denote the approximations 
using the linear and quadratic approximations in z to the expectations. 
~ = log !lH(i) -
log !lM(i) , 
XQ = 10g[UH(z) + CH ] -
log[UM(z) + CM], 
.:ll = ~ -
}., 
.:l2 = XQ -
X, 
.:l = .:ll + .:l2' 
Here .:ll is the change in log odds from using the modal approximation in Z 
rather than 1', and .:l2 is the additional change from using the quadratic approxi-
mations to expectations. At least temporarily, the sum .:l may be regarded as 
our best indication of error in the log odds approximation. 
For each of the five words, values of h, .:ll, .:l2, XQ = h + .:l are shown in 
Table 4.6-1 for each of several frequencies x of uses of the word in an unknown 
paper of length w = 2.0. In addition, the table shows the expected number of 
occurrences for each author, EH(X) and EM(X), The same information is shown 
for words on, upon, and whilst in Figs. 4.6-1, 4.6~2, and 4.6-3. All sample 
calculations use posterior modes and distributions based on set 22 of underlying 
constants. 

144 
THEORETICAL BASIS OF THE MAIN STUDY 
TABLE 4.6-1 
CORRECTIONS TO LOG ODDS ApPROXIMATIONS BASED ON 
SET 22 OF UNDERLYING CONSTANTS 
40 on 
x 
X 
~l 
~2 
XQ 
60 upon 
x 
X 
~1 
~2 
XQ 
90 enough 
x 
X 
~1 
~2 
XQ 
117 whilst 
(EXCEPT FOR ROUNDING, XQ = X + ~l + ~2) 
2 
+4.59 
-
.12 
-
.31 
+4.16 
50 
-2.35 
+ .08 
+ .11 
-2.17 
o 
-4.76 
+ .17 
+ .23 
-4.36 
o 
-
.22 
.00 
.00 
-
.22 
o 
+ .59 
-
.03 
-
.02 
+ .55 
8 
+1.10 
-
.04 
-
.13 
+ .93 
16 
-2.46 
+.10 
-
.05 
-2.41 
22 
-4.66 
+ .20 
+ .18 
-4.28 
(ER(X) = 81.6, 
EM(X) = 70.4) 
70 
90 
110 
-
.42 
+ 1.08 
+2.23 
+ .02 
-
.04 
-
.07 
+ .03 
+ .01 
-
.05 
-
.37 
+1.05 
+2.11 
(ER(X) = 6.5, 
EM(X) = .49) 
1 
-1.96 
+ .19 
+ .13 
-1.64 
2 
-
.17 
+ .08 
+ .03 
-
.06 
7 
+4.68 
-
.58 
-
.57 
+3.53 
(ER(X) = .52, 
EM(X) = .2.0) 
1 
+ .82 
+ .05 
-.10 
+.77 
2 
+ .99 
-
.01 
-
.17 
+ .81 
6 
+1.04 
-
.19 
-
.51 
+ .33 
(ER(X) = .16, 
EM(X) = .86) 
1 
-1.16 
-.11 
+ .06 
-1.21 
2 
-1.83 
+ .14 
+ .07 
-1.63 
3 
-2.21 
+ .36 
+ .19 
-1.66 
10 
+6.25 
-
.93 
-1.06 
+4.27 
[4.6 
What interpretations of these sample calculations should be made? Most, 
but not all, of these calculations reduce the magnitude of the log odds. The 
corrections are roughly proportional to the log odds, but increase more than 
proportionately for extreme frequencies of use. (See, for example, the entries 
in Table 4.6-1 for 10 occurrences of upon, or for 6 occurrences of enough.) For 
all words except upon, the corrections, as percentages of the original magnitudes 

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
6~----r-----~--~~----~----~--~ 
, 
4~,~-----+----~~----+-----~----~ 
.... 
~l 
I '" 
2 I-------j--- "".--+-------+-------+------1-------1 
:g 
k mea>~ ........ 
o 
0 r--::::=:::!=:==:1~:.=-:.:j=-....;-.... - - - -- - - - - - - -
!>II 
--- --
""' ... L 1t mean 
~ 
-
. I 
-2 
-+----\----1 
-60~----~----~----~----~----~--~ 
4 
8 
12 
16 
20 
24 
Occurrences, x 
145 
FIG. 4.6-1. Approximations to log odds for x occurrences of word No. 40, on, in a 
2000-word paper. X is approximate log odds used throughout study; XQ is an improved 
approximation to log odds, ~l and ~2 are two parts of the correction XQ -
X. 
6 
4 
2 
l~~"/.· -
Mzn 
If' 
I 
-2 
-4 
-6 o 
2 
...... 
X,; ., 
"' .... 
- f--
", 
"",'" 
" 
XQ 
,; 
/1 
H mean 
... 
----------
4 
6 
Occurrences, x 
- -
8 
-----
~l 
1-----
~2 
10 
12 
FIG. 4.6-2. Approximations to log odds for x occurrences of word No. 60, upon, in 
a 2000-word paper. X is approximate log odds used throughout study; XQ is an im-
proved approximation to log odds, ~l and ~2 are two parts of the correction XQ -
X. 
X, are adequately approximated by a 10 per cent reduction, with the occasional 
increases balancing the larger reductions for the extremes. For upon, the re-
duction is roughly 15 per cent when the log odds are negative, 25 per cent when 
the log odas are positive. A reasonable overall correction for all words might 
be taken as 15 per cent. Since upon is an important word, the total correction 
might be rather larger for Hamilton (positive log odds) than for Madison 
(negative log odds), perhaps 18 per cent for Hamilton, 12 per cent for Madison. 

146 
THEORETICAL BASIS OF THE MAIN STUDY 
1 \ 
I 
M mean 
, 
o 
--~---~- ", 
H mean 
-1 
~ 
"0 o -2 
bl) 
o 
H 
-3 
-4 
-5 o 
-~ \ 
~, 
', 
............ 
, 
" 
1 
Occurrences, x 
Al _,-' 
... --- --
A2_ 
~ 
...... ..... 
A .... 
2 
3 
[4.6 
FIG. 4.6-3. Approximations to log odds for x occurrences of word No. 117, whilst, 
in a 2000-word paper. X is approximate log odds used throughout study; ~Q is an 
improved approximation to log odds, .:11 and.:12 are two parts of the correction ~Q - t 
In summary, an adjustment to the log odds factor is needed because a four-
fold integration has been approximated by an ordinate of an integrand. We 
find that a magnitude of 15 per cent in the log odds would be appropriate. 
4.6C. Approximations to expectations. We explore approximate evalua-
tions of expectations of functions of random variables, because they offer one 
way to assess the magnitudes of adjustments. 
Suppose that y is a random variable, and let q(y) be a function of that random 
variable, so that q(y) is itself a random variable. We consider approximations 
to E(q(y)) obtained by substituting into q a value of y, say y. Thus the simple 
approximation is 
(1) 
E (q(y)) ~ q(y). 
We study choices of y and the sizes of first-order correction terms. 
That an exactly correct choice for y exists is assured by the mean value 
theorem, but we have no feasible way to find it. Good, simple approximations 
and an easy study of their accuracy motivate our choice of y. 
Section 4.6D gives a brief development of formal asymptotic theory appro-
priate to posterior expectations. That development underlies the approximations 
and guides the ordering of terms in this section and the next, but the presenta-
tion here emphasizes the nonasymptotic aspects of the approximation. 
How to choose among approximations that are asymptotically equivalent to 
a given order is a touchy question because in numerical use with finite data, the 
higher-order terms are not negligible. If, in addition, the approximations are 
frequently transformed, the difficulty is aggravated by the numerous oppor-

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
147 
tunities to retain or omit higher-order terms. We formulate assumptions that 
make one of the asymptotic approximations exact. For example, we assume 
that some specified transformed variable is exactly normally distributed and 
that an integrand is exactly quadratic. 
The usual approximation, the delta-method approach for the expectation of 
a function q of a random variable y, is based on the expansion of the function 
in a Taylor's series. First the function is expanded about the population mean 
'0 = E(y), or about some other central value '0, and then term-by-term inte-
gration is used. As far as we go here, what matters in ordering terms is that 
'0 -
'0 and 0'; are assumed to be of the same order, and higher central moments 
of yare assumed to be of order of O't or higher. (See Section 4.6D.) 
The simplest approximation uses '0 for '0 in approximation (1) to give 
(2) 
E (q(y)) "'" q('O) = "ii, 
where "ii is a shorthand notation for the value of the function q at 'O. 
The first-order correction for this approximation is 
(3) 
where "ii11 is the second derivative of q evaluated at 'O. In this development, we 
use subscripts on q or any function to denote partial derivatives with respect to the 
component argument indicated by the subscript, and we place a symbol such as 
"_,, or "." over the symbol for a function to denote its value at the corresponding 
value of its argument. For example, for a function h(ul, U2, U3), then 
If a value '0 is used in the basic approximation (1) then 
(4) 
E (q(y)) "'" q('O) = q, 
and the first-order correction term, 
(5) 
contains an extra correction for the deviation of '0 from the mean 'O. 
Under the assumptions usually made that ('0 -
'0) and 0'; are of the same 
order, the final term in Eq. (5) is of higher order and might be neglected. As we 
have mentioned, the approximations can be given a nonasymptotic justification. 
For if q is quadratic, then approximation (2) with correction (3) and approxima-
tion (4) with correction (5), as given, are both exact. Approximation (2), but 
not approximation (4), is exact if q is linear. 
In our problems, as with most posterior densities obtained from Bayes' 
theorem, the density of y is known only to withip a multiplicative constant. 

148 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.6 
Consequently the moments of f} as used in the above approximations are not 
directly available, and alternative approximations are required. The asymptotic 
normality of posterior densities based on the likelihood of n independent ob~ 
servations makes the use of the Laplace integral expansions (Erdelyi, 1956, 
Section 2.4) especially appropriate. 
Laplace integral expansion. 
Suppose that the density of f} is written as 
p(y) ex: b(y)e-h(y), in twhich h has at y a single absolute minimum, which is also 
a relative minimum. Suppose in addition that the function h is sharp at the 
minimum so that the second derivative hii at y (and higher derivatives) are 
large. Suppose that b is relatively flat near y. Here y is the mode of the gen-
eralized density of f} relative to the measure element b(y) dy. If b = 1, then 
y is the ordinary mode. 
The desired expectation can be written 
f q(y)b(y)e-h(Y) dy 
E(q(f}») = f b(y)e-h(y) dy 
, 
and by application of the Laplace method of integral expansion to numerator 
and denominator as explained in Section 4.6D, we obtain the approximation 
(6) 
E (q(f}») ~ q(y), 
with the first-order correction 
(7) 
where in anticipation of multivariate use of inverse matrices, we have introduced 
the notation 
No general exact relations are obtained from (6) and (7) except when h is 
quadratic, so that b -.:. 1 and f} is normally distributed. 
If we apply the modal approximation and correction to q = y and to q = y2, 
we see that 
(j2 ~ h'l1 
y 
~ 
, 
so that corrections (5) and (7) are equivalent to the order given. 
The correction (7) to the modal approximation (6) depends, even for linear q, 
on the second and third derivatives of h. For a vector random variable f) with 
m components, there are (mt2) third derivatives, each possibly requiring a 
substantial computation, so that the correction is hard to evaluate. We must 
have a method that avoids the computation of these third derivatives. 
Many modal approximations with their first-order corrections are equivalent 
asymptotically. Each choice of the function b yields a different mode y and a 

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
149 
different correction. For each choice of a transformation to a new random 
variable x = x(y), the procedure may be applied to x with asymptotically equiv-
alent results. In simple problems, the function b or transformation x may be 
chosen to improve the modal approximation or to eliminate the third derivative 
terms. In harder problems, choices may be made in analogy with similar simpler 
problems, or on the basis of other information or intuition so that, for example, 
neglect of the third derivative terms may be acceptable. 
Changing b changes the mode '0, relative to b. If b is chosen by 
(8) 
bey) ex: V -02 log p(y)/oy2 
(the choice of h is then forced), the deviation '0 -
'fj is reduced to a higher-order 
term. The gamma and beta are among the distributions for which b can be 
determined by Eq. (8). If Y has a gamma distribution, then with bey) = l/y, 
the mode '0 relative to b coincides (exactly) with the mean y. If Y has a beta 
distribution, then with bey) = 1/ (y(1 -
y)), the mode relative to b again coin-
cides with the mean. 
The choice bey) = 1 is possible, and is the choice used in the main study of 
log odds and in the study of transformations that follows. We now, however, 
consider another choice of b definitely preferable to b = 1. This choice, pre-
sented in Section 4.2F, is based in part on the analysis related to Eq. (8). 
If x = x(y) is a smooth, reversible transformation,fiat near '0 relative to the 
sharpness of hey), then the density p(x) of x will have a form analogous to that 
of y, say 
The mode of p and the derivatives of f at the mode can easily be determined, 
to an appropriate order, from the corresponding quantities for y and vice versa. 
The errors introduced by neglected higher-order terms, in passing from x to y, 
may be substantial. Instead of giving the relations, we provide a nonasymptotic 
development that is, to the order used, asymptotically equivalent, but that is 
exact if its assumptions were to hold. 
Suppose that with 
p(y) ex: e-h(lI) 
the mode '0 and second derivatives h11 and inverse h 11 are available, but that 
third derivatives are not available. How can the adequacy of 
E(q(y)) ~ q('O) 
best be studied? The portion of the first-order correction (7) corresponding to 
0'; can be evaluated, but not the portion corresponding to the deviation '0 -
y. 
If we could determine some transformation x = x(y) such that the third deriva-
tives of the negative log density f(x) are zero, or more nearly zero than those of 
hey), that information could be used to get some estimate of the deviation term 

150 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.6 
y -
y. To assume that fll1 (x*) "'" 0 is equivalent, to the first order, to assum-
ing that x is exactly normally distributed. With the latter assumption, we can 
specify conditions under which the resulting approximations are exact, even for 
the finite data at hand. 
We assume for a smooth reversible transformation· x = x(f}) with inverse 
f) = y(x) that x is exactly normally distributed with mean x, precision ill and 
variance pl. Letting i = xCV) and 
a(x) = log 1:;1 ' 
we find that the correspondence between the moments of x and the mode and 
logarithmic derivatives of f) is 
• 
ay 
aZa 
(-)2 (-) 
ill = hll ax 
-
ax2 ' 
ill = (fll)-l, 
-
'+i 11 ~ 
X = X 
• ax' 
Because of normality, f 11 is a constant. The second term in f 11 is asymptotically 
of higher order, and in the usual asymptotic development would be dropped, 
permitting a direct expression of the inverse p 1 in terms of h 11 . Explicit in-
version is a price for the "exact" development. Using the normality of p, we 
find that the third derivative of h at the mode g can be determined: 
. 
a2y (a?)2 
a~ (a?)3 
hll1 = -3hll -
-- + -
-
. 
ax 2 ay 
ax3 
ay 
Two approximations and corrections are available: the modal approximation 
q(y) with the correction (7) using the third derivatives that were determined 
by the normality of x. Alternatively, apply the mean approximation (2) with 
correction (3), identical for normality to (6) and (7), to the transformed function 
g(x) = q(y(x). In the present situation, the latter choice is vastly easier, involv-
ing far fewer terms and no third derivatives. The latter approximations with 
correction are exact if x is normal and if g is quadratic. However, the size and 
ease of computation of derivatives of q as opposed to g should influence the 
choice. 
Multivariate approximation. The results just given for one-dimensional random 
variables have important analogs for vector random variables. We adopt some 
suffix conventions from tensor analysis. Components of vectors are denoted by 
superscripts; partial derivatives, with respect to the components of the argu-
ment, are denoted by successive subscripts; a doubly 8uperscripted symbol 
denotes the indicated element of the inverse of the same doubly subscripted 
symbol; and a summation from 1 to k is to be taken over any index occurring 
once as a superscript and once as a subscript. 

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
Let y = (yl, . .. , yk) be a k-dimensional random variable. Let 
Vi = E(yi), 
Cii = COV(yi, yi). 
151 
If means and covariances of yare known, the standard first and second ap-
proximations to the expectation of a function q of yare 
E(q(y)) ~ q(y) = 'lj 
~ 'lj + {iiiiiCii} 
or 
(first approximation), 
(second approximation), 
E(q(y)) ~ q(y) = q 
(first approximation) 
~ q + {qi(yi - Vi) + tQiAcii + (yi - Vi)(yi - Vj )]} 
(second approximation). 
The density of y may be known to within a multiplicative constant, and it may 
be expressible (not uniquely) as 
p(y) oc b(y)e-h(y), 
in which h has a stationary minimum at y, and the matrix of second derivatives 
of h at y, [hiil, is positive definite and the elements are large relative to the 
second derivatives of q and b at y. Then the first-order correction term to the 
modal approximation 
E(q(y)) ~ q(y) = q 
is 
where [hijl is the inverse matrix to [hiil. 
Approximations to the deviation of the mode y from the mean y and for the 
covariance matrix are given by 
yi _ '!l ~ h ii {bi/b -
!hT8hT8j} , 
cii ~ h ii. 
The mode relative to b will differ from the mean only by higher-order 
terms than retained here if band h satisfy, to the first order, the condition 
(9) 
b(y) oc Vdet[hij(y)]. 
This condition amounts to requiring that band. h be chosen so that when h is 
approximated by its quadratic Taylor's series about any point Yo, then for each 
Yo, b(yo) is the correct normalizing multiplier for the normal density. If b is 

152 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.6 
chosen according to 
bey) ex: I det [_ a2 lo~ P(y)] , 
\j 
ay' ayJ 
then the choice of h is forced, and band h will satisfy Eq. (9) to the order 
required. 
Let x = x(y) be a smooth reversible transformation with inverse y = y(x), 
and assume that x has a multivariate normal density p with mean x, precision 
matrix [fij], and covariance matrix [fij]. Letting x = x('o), and denoting deriva-
tives of the transformation by the derivative convention, for example, 
we can write the logarithm of the Jacobian as 
a(x) = log Idet[y}]I. 
If Y has density 
so that b = 1, the mean and precision of x follow and are determined exactly 
by the mode '0 and second derivatives of h at.the mode: 
f 
h
A 
AT AS 
A 
ij = 
TsYiYj -
aij, 
The third derivatives of h at the mode y can be determined from '0, hij and the 
normality of x by 
h
A 
3hA 
AT ASAt + A 
ATASAt 
iJ'1 = -
iTYstXjXI 
aTstXiXjXI· 
The approximations to E(q(y» resulting when we apply the moment approxi-
mations to the transformed function 
are 
(10) 
(11) 
g(X) = q(y(x» 
E(q(fj) = E(g(x» 
~ g(x) 
~ g(x) + !g;jfij • 
If x is normally distributed, the first approximation is exact when g is linear, 
the second when g is quadratic. 
4.6D. Notes on asymptotic methods. The usual asymptotic develop-
ment underlying such devices as the delta-method expansions and the Edge-
worth expansions is based on random variables that behave like functions of 
means of large numbers of independent, identically distributed random vari-
ables. For the delta-method expansion for E(q(y», what matters only is that 

4.6] 
MAGNITUDES OF ADJUSTMENTS REQUIRED 
the moments of fj, with expectation y, have asymptotic form: 
E[(Y -
y)2r] = O(llmr), 
E[(fj -
y)2r-l] _ O(llmr), 
153 
as m ~ 00, for r = 1, 2, 3, .... If Y differs from y by a term of order 11m, 
then the moments of f} about y have a leading term whose order is the same as 
that for moments about y. 
Essentially equivalent asymptotic expansions arise in working with posterior 
distributions. To begin, suppose that the density p(y) of f} can be expressed as 
p(y) C( e-mhCY)b(y), 
where the constant of proportionality may depend on m. Then 
E 
_ 
_ f q(y)b(y)e-mhCY) dy 
(q(y)) - f ( ) -mhCy) 
. 
bye 
dy 
If h has an absolute minimum at y, the mode of the density of f} relative to a 
measure element bey) dy, and if the derivatives of h at y satisfy the conditions 
hI = 0, hu > 0, then the Laplace expansion is applicable. 
(Erdelyi, 1956, 
Section 2.4.) 
Expand h, b, and q about y, and note that the integrals are determined for a 
range of y within order of 1/vm of y. Setting u = vm(y -
y), we have 
2 
A 
3 
A 
4 
mh(y) = mh + 0 + hll ~ + hll~ + hllllU + .... 
2 
6vm 
24m 
Terms in e-mh(dyldu) cancel out, and the dominant term is e-Cl/2lhllU2. Thus 
E(q(Y)) ~ f ~;~~u·'I~.I~~" '~(6+1~;,J'~- )dU 
~~e-2hllU 
b+ ~+ ... e 6{m -'" du 
The quantities ihbraces can be expanded in power series in 1/ym by tedious 
manipulations. The resulting termwise integrations are simply moments of a, 
normal density with mean 0 and variance 1lhll . What results is the ratio of 
two power series in 11m, because all odd powers of Vm vanish. Finally, this 
ratio can be combined into a single expansi()n, with the elimination of many 

154 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.6 
terms. The first two terms are 
( (_) 
• 
1 {. • 11 [b 1 
1· 11 • 
] 
1. 11 • 
} 
E q y 
~ q + m q1h 
b - 2 h h111 + 2 h q11 ' 
in which k 11 = llk11> a notation chosen to facilitate multivariate extensions. 
This expansion is applicable more generally than for the special asymptotic 
structure assumed here. We may allow the functions b, h, and even q, to depend 
on m provided that each may be expanded: 
1 
hey) ~ h[O](y) + -- h[1](y) + ... , 
m 
1 
bey) ~ b[O](y) + - b[l](y) + ... , 
m 
where we must require that h[o] and b[o] not vanish, and that h[o] have an ab-
solute minimum that is also a stationary point. The functions with bracketed 
subscripts may be any functions not depending on m; the brackets are added to 
avoid confusion with derivatives of h or b. 
No further use is made of this 
notation. 
That the expansion procedure still holds requires demonstration that we 
omit. The chief effect on the expansion is that the coefficients of 1, 11m, etc. 
will contain functions of m whose leading terms are constant, but such that 
extra higher-order terms appear, unnecessarily. 
Suppose that Xl, X2, ... , Xm are independent observations from the density 
g(x I y). Let the prior density of y be bey). Then the posterior density is 
But for each y, Ll[ - log g(Xi I y)]lm, by the law of large numbers; converges 
to 
h[o](Y) = E(-logg(xi I y) I y), 
so that for m large, the desired structure seems to hold. That h[o](y) has the 
requisite shape is closely related to the asymptotic theory of maximum likeli-
hood. 
Finally, in practical use, we need not know the explicit dependence on m, or 
even what m is. We use the assumed form to guide the ordering of the terms and 
set m = 1. 
We follow this procedure in the applications in the preceding 
section. 
The technical development given in Sections 4.6C and 4.6D is more extensive 
than we needed for Sections 4.6A and 4.6B. We hope our presentation will be 
useful more generally as the basis for better approximations. Much thought 
and work will be required to make practical the use of extended expansions. 

4.7] 
CORRELATIONS 
155 
4.7. CORRELATIONS 
4.7 A. Independence and odds. In the main and robust Bayesian studies, 
the odds factor was computed by multiplying odds factors for separate words 
as if the words used were independent. Obviously, independence is an impossibly 
stiff condition to achieve or justify, and the reader has as many reasons as we 
for not believing in it. We need a way to assess the discount or inflation that 
should be applied to our odds owing to failure of independence. 
We shall be searching for the mean difference between the logarithms of the 
odds factors under correlation and under independence. 
4.7B. Adjustment for a pair of words. Let us discuss the odds associated 
with word frequencies for two words, numbered 1 and 2. We suppose that the 
frequencies are approximately normally distributed. For a variety of conven-
iences that emerge later, we suppose that a linear transformation has been 
applied to the frequencies to obtain random variables Yl and Y2 with unit vari-
ance (assumed true for both authors) and zero mean for Madison, and means 
'Y 1 and 'Y 2 for Hamilton, both 'Y 1 and 'Y 2 greater than zero. This usage follows 
that of Cochran (1961), whose development is useful here because it has an odds 
interpretation. (Our 'Y's are his o's.) 
If the word frequencies are actually independent, then the odds factor, K, 
would be the ratio of the ordinates at (Yb Y2) for the two authors, namely, 
~ 
e-(1/2)[(Yl-'Yl)2+(Y2-'Y2)2j 
K = 
2_~~ ______________ __ 
~ 
e-(1/2)(YhY~) 
2~ 
Then the logarithm of K, say XI, I for independence, is 
XI = !(2Yl'Yl -
'Y~ + 2Y2'Y2 -
'Y~). 
Given that Hamilton wrote the material, when we take mean values, we 
obtain 
Similarly the variance is 
V(XI I H, I) = 
'Y~ + 'Y~ = 2E(XI I H), 
where independence, I, is introduced as a condition on the variance to prevent 
later confusion. We define the standard deviation in the usual way: 
U(XI I H, I) = VV(XI I H, I). 
In the same way, given that Madison wrote the material, the mean is 
E(XI I M) = 
-!('Y~ + 'Y~) = -E(XI I H), 

156 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.7 
and the variance is 
V(}\I I M) = 'Yi + 'Y~ = V(>-'I I H). 
Throughout, the means and mean corrections if Madison is the author are just 
the negative of those if Hamilton is the author, and Harriilton's and Madison's 
variances are equal. Thus we may and do work only with oIie author, Hamilton. 
On the other hand, suppose the variables Yl and Y2 have true correlation 
coefficient p, the same coefficient for each author. The originai word frequencies, 
or rates, are correlated p or -p, but the sign of p may be changed by the linear 
transformation required to get positive 'Y's. Then our odds adjustment factor 
under .this new correlation condition, say C, is the ratio of ordinates of two 
bivariate normals at (Yb Y2), and thus the logarithm of the odds factor is 
If Hamilton wrote the material, then 
Furthermore, the variance, given Hamilton and correlation, is V(AC I H, C) = 
2E(AC I H), and the standard deviation is U(AC I H, C) = VV(AC I H, C). 
At last we can obtain the quantity of basic interest E(AC -
AI I H, C). This 
is the expected value of the adjustment in the log odds required by correlation. 
Since the A'S are linear in the y's, the condition C can be suppressed, and we 
write 
I 
I 
1 [(1'2 - P'Yl)2 
2J 
E(AC -
AI I H) = E(AC H) -
E(AI H) = "2 
1 _ p2 
- 1'2 
when an explicit formula is desired. Generally speaking, negative correlations 
strengtheIi the odds, and positive correlations weaken them. 
Furthermore, by algebra like that required to get V(AC I H, C), we find 
or 
U(AC -
AI I H, C) -
Iplu(AC I H, C). 
These results are somewhat limited in their applicability, because where can 
we find the bivariate normality? Perhaps only approximately in high-frequency 
words. But we might achieve a little more by applying a variance-stabilizing 

4.7] 
CORRELATIONS 
157 
transformation to the original word frequencies, and regarding these trans-
formed values as having the sorts of properties we require. 
We proceed to such an empirical study. 
4.7C. Example. The words upon and on. Upon is much used by Hamilton, 
but little used by Madison. We suppose that the Hamilton correlation between 
the rates of use, r = -.19, is common to the two authors, and to transformed 
variables, except possibly for sign. 
We plan (in principle) to use the Freeman-Tukey (1950) square root trans-
formation on our negative binomial counts. Roughly then the mean of the trans-
formed variable T is 
( 
1 + 28 
) 
E(T I H) = (VWMH + VI + WMH) 
1 -
8(t + WMH) 
, 
and 
VeT I H) = 1 + 28, 
where 8 is the average non-Poissonness parameter for the authors. Use of a 
common non-Poissonness parameter is needed for common variance and is ac-
ceptable for any of the final 30 words. We choose W = 2.000 as a representative 
size of paper. 
For word 60 (upon), the estimated negative binomial parameters (from 
Table 3.2-3) are: 
MH1 = 3.24, 
MM1 = .24,* 
OHl = .25, 
OM1 = .39. 
The means and variance of the transformed variables are: 
E(T1 I H) = 5.13, 
E(T1 I M) = 1.50, 
the standardized difference is 
IE(T1 I H) -
E(T1 I M)I 
"11 = 
= 2.84, 
VV(T1) 
and the expected log odds if Hamilton is the author is t'Yi = 4.02. To make 
"11 > 0, no sign change was necessary: 
E1 = sgn[E(T1 I H) -
E(T1 1M)] = +1. 
Similarly, for word 40 (on), 
E(T2 I H) = 5.22, 
E(T2 I M) = 7.88, 
"12 = 2.01, 
and the sign change E2 = -1. 
t'Y~ = 2.01 
* Here .24 was used instead of .23 as in the table; the more correct value is .2346. 

158 
THEORETICAL BASIS OF THE MAIN STUDY 
TABLE 4.7-1 
STRENGTHS OF MODERATE- AND HIGH-FREQUENCY WORDS 
Word 
Word 
!'Y2 
number 
I' 
3 
also 
.59 
.18 
4 
an 
.82 
.33 
13 
by 
1.44 
1.04 
39 
of 
1.00 
.50 
40 
on 
2.01 
2.01 
55 
there 
1.46 
1.06 
57 
this 
.80 
.32 
58 
to 
.94 
.44 
60 
upon 
2.84 
4.02 
78 
both 
.72 
.26 
160 
though 
.58 
.17 
TABLE 4.7-2 
MEAN AND STANDARD DEVIATION OF ADJUSTMENT TO LOG ODDS 
FROM CORRELATION BETWEEN WORD PAIRS 
Word pair 
E(Ac -
Al I H) 
!1'*'in 
u(Ac -
Al I H, C) 
(3, 60) 
-.15 
.18 
.80 
(3, 160) 
+.13 
.17 
.26 
(4, 55) 
-.19 
.3:3 
.31 
(4, 57) 
-.10 
.32 
.20 
(4,160) 
+.09 
.17 
.18 
(13, 57) 
+.38 
.32 
.45 
(13, 160) 
+.18 
.17 
.28 
(39, 58) 
+.60 
.44 
.69 
(39, 78) 
-.12 
.26 
.23 
(40,58) 
-.28 
.44 
.40 
(40, 60) 
-.90 
2.01 
.61 
(55, 160) 
+.21 
.17 
.32 
(57,60) 
-.31 
.32 
.74 
(60, 160) 
-.16 
.Ii 
.45 
(78, 160) 
+.08 
.26 
.16 
Total 
-.54 
Note: The adjustment tabled applies if Hamilton is the author; 
if Madison is the author, the signs of each adjustment should be 
reversed. The loss or gain can be compared with the strength of 
the weaker word, i'Y~in. 
[4.7 

4.7] 
CORRELATIONS 
159 
Since our transformation has changed some signs to make the "I's positive, 
we need to multiply the original correlation p by the factor El E2 to get the correct 
sign. So our correlation is now .19 instead of -.19. Combining the information, 
we have 
E(AI I H) = 6.03, 
E(AG I H) = 5.13, 
E(AG -
AI I H) = -.90, 
U(AI I H, J) = 3.47, 
U(AG I H, C) = 3.20, 
U(AG -
AI I H, C) = .61. 
So, at last, the analysis suggests that the correction to the logarithm of the 
odds is -.90. So here we would reduce the odds (multiply by .4) as a result of 
the correlation between the rates of upon and on. 
4.7D. Study of 15 word pairs. We investigate the effects of several corre-
lations, such as between on and upon. As in the example above, we shall do it as 
if the transformation were to be carried out for papers of length 2000 words. 
We study the 11 moderate- and high-frequency words among the final 30 
words. In Table 4.7-1, we list for each word its standardized difference "I, and 
its expected log odds !"I2 if Hamilton is the author. 
We study those word pairs that have average correlations of more than .15 
in absolute value. (If upon is one of the words, then we keep the pair if the 
Hamilton correlation exceeds .15). Then the following word pairs, listed with 
their adjusted correlations, are retained for study: (3,60) .28; (3,160) -.27; 
(4,55) .20; (4,57) .19; (4,160) -.16; (13,57) -.24; (13,160) -.17; (39,58) 
-.39; (39,78) .20; (40,58) .19; (40,60) .19; (55,160) -.19; (57,60) .26; 
(60,160) .16; (78,160) -.16. 
The mean and standard deviation of the adjustments owing to these corre-
lations are shown in Table 4.7-2, together with the mean log odds for the weaker 
of the two words in each pair. The sum of the adjustments is -.54. Thus, when 
Hamilton is the author, and independence has been used to calculate the log 
odds on the basis of these 11 words, our rough guess at the required adjustment 
in log odds is -.5, which translates to a reduction in odds by a factor of .6. 
Our correlations are based on the average correlation in two sets of 50 papers, 
so the standard deviation of the estimate for true correlations is about 
vi Hlo + lo) =.1. Thus the previous calculation neglected all correlations 
within 1.5 standard deviations of zero, and appraised the rest at their estimated 
values. 
This method suffers in addition from dealing with a. set of r words as if their 
effects were additive in pairs. 
For a first-order calculation, though, this is 
satisfactory. 
4.7E. Several words. We extend our results to the simultaneous adjustment 
for several words all distributed according to multivariate normal distributions 
with variance-covariance matrix ~, and inverse ~-l. Thus the multivariate 

160 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.7 
normal densities are 
p(y I H) = fN(Y I 1', };), 
p(y 1M) = fN(Y I 0, ~), 
where I' and 0 are column vectors 1" = (1'1,1'2, ... , 'Yk), and 0' = (0,0, ... ,0). 
The log odds are given by 
A = -!(Y -
'Y)'~-l(y -
1') + !y'~-ly 
= -![ _'Y'~-ly -
'Y'~-l( -I') + y'~-l( -1')]. 
Recall that the transpose of a product is the product of the transposes in reverse 
order. Furthermore, 'Y'~-ly is a one-dimensional vector, and the transpose of 
a one-dimensional vector is the same vector. Thus 'Y'~-ly = y'~-l'Y, because 
~-l is symmetric. So we can rewrite 
Then, using subscripts C and I as in the previous development for correlation 
and independence, we can write 
Ae -
AI = (y -
!'Y)' . [~Cl -
~Il]'Y. 
Again regarding Hamilton as the author, and noting that independence means 
~ll = I, the identity matrix, we have 
E(Ae -
AI I H) = !'Y'[~Cl -
~Ill'Y 
= !'Y'};c1'Y -
!'Y'I'Y 
= !'Y'~Cl'Y -
!'Y''Y. 
We also want the variance Ae -
AI. The deviation from the mean is 
Ae -
AI -
E(Ae -
AI I H) = (y -
'Y)'(~Cl -
~Il)'Y. 
Then the variance is the expected value of the square of this linear form or 
(let M = 
~Cl -
~It, note M' = M), 
V(Ae -
AI I H, C) = E{[(y -
'Y)'M'Y]'[(y -
'Y)'M'Y]} 
= E['Y'M(y -
'Y)(y -
'Y)'M'Y] 
= 'Y'ME[(y -
'Y)(y -
'Y)']M'Y 
= 'Y'M~eM'Y. 
Recall that ~Il = I; thus we can write, after several simplifications, 
V(Ae -
AI I H, C) = 'Y'[~Cl + ~e -
21]1'. 
The values of 'Yi for the 11 words are shown in Table 4.7-1, and the correla-
tions are given in Table 4.7-3. 
Very likely" most elements in the original matrix are too large in absolute 
value because we expect them to be in the neighborhood of zero, and indi-

4.7] 
CORRELATIONS 
161 
TABLE 4.7-3 
CORRELATIONS BETWEEN RATES FOR 11 WORDS 
Words 
3 
4 
13 
39 
40 
55 
3 
1. 
.055 
.141 
-.110 
.088 
.126 
4 
.055 
1. 
.020 
-.043 
.059 
.198 
13 
.141 
.020 
1. 
.038 
-.015 
.098 
39 
-.110 
-.043 
.038 
1. 
.039 
-.127 
40 
.088 
.059 
-.015 
.039 
1. 
.128 
55 
.126 
.198 
.098 
-.127 
.128 
1. 
57 
-.117 
.193 
-.242 
.018 
-.085 
-.058 
58 
.050 
.071 
.117 
-.389 
.189 
.087 
60 
.282 
-.019 
-.083 
.004 
.194 
-.134 
78 
.074 
.046 
-.020 
.196 
.054 
.083 
160 
-.275 
-.158 
-.170 
.042 
.029 
-.194 
Words 
57 
58 
60 
78 
160 
3 
-.117 
.050 
.282 
.074 
-.275 
4 
.193 
.071 
-.019 
.046 
-.158 
13 
-.242 
.117 
-.083 
-.020 
-.170 
39 
.018 
-.389 
.004 
.196 
.042 
40 
-.085 
.189 
.194 
.054 
.029 
55 
-.058 
.087 
-.134 
.083 
-.194 
57 
1. 
-.138 
.261 
.012 
.138 
58 
-.138 
1. 
.059 
.035 
-.090 
60 
.261 
.059 
1. 
.004 
.161 
78 
.Oi2 
.035 
.004 
1. 
-.159 
160 
.138 
-.090 
.161 
-.159 
1. 
vidually they have standard deviations of .1, except those involving upon, and 
theirs is .15. 
As our first assessment, we regard the observed matrix as the true one and 
find the additive correction to the log odds to be -1.16 with standard deviation 
1.67. The odds would be divided by three. 
To try to obtain more realistic results we revise the original correlation matrix 
in two ways. The first is to replace each correlation by one moved toward 
zero by .1 (for example, .2 by .1, --:.2 by -.1, but .05 and -.05 by 0). Then 
the additive correction to the log odds is found to be - .62 with standard devi-
ation .86. The odds would be adjusted by a factor of about !. 
A second method of reaching fbr realism is to multiply every correlation in 
the original matrix by!. This leads to a correction of - .67 in the log odds with 
standard deviation .6. Again, the adjustment in the odds would be a factor of 
about!. These latter approaches are decently close to the adjustment given by 
individual pairs in Section 4.7D. 

162 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.7 
4.7F. Further theory. Since we deal with correlation matrices, we can 
make some further explorations that take advantage of the fact that the corre-
lations between words are generally modest. Recall that just as we can expand 
III senes 
(1 + A)-1 = 1 -
A + A 2 -
A 3 + ... 
when A is a constant (IAI < 1), we can also interpret the expansion as that of 
the inverse of the matrix 1 + A, where 1 is the identity matrix. Thus in our 
previous notation, the correlation matrix 2;c could be represented as 2;c = I + A, 
and its inverse as 
2;c1 = I -
A + A 2 -
A 3 + .... 
Here A, except for its main diagonal of O's, is the same as the original correlation 
matrix. 
We need not compute the powers of A. Instead, since we want 
'Y'L: c 11', we merely compute 1" A n'Y, a much easier task since 1" A is just a row 
vector. Some further saving is possible because (AI')' == 1" A, since A is sym-
metric. Because in our problem the entries in A are small, the terms 1" A n'Y 
should become small very soon. For example, for the correlation matrix in 
which all values were reduced toward zero by .1, the value of 'Y'L:c 1'Y com-
puted from the inverse is 19.415. The individual terms and cumulative approxi-
mations are given below. 
Individual terms 
1"1' = 20.654 
-I" A I' = -1.977 
1" A 21' = 
.828 
-I" A 31' = 
.152 
1" A 41' = 
.063 
Cumulative 
approximation 
20.654 
18.677 
19.505 
19.353 
19.416 
So, for this example, including terms in A 2 would suffice for our work. 
When the correlations are modest, the first-order correction is 
-iI" AI' = - L: I' i'Yjpij. 
i<j 
When the correlations are small, the first-order correction and the summed 
correction for pairs should agree fairly well. For a pair of words (1,2) the cor-
rection is 
p2('Yi + 'Y~) -
2p'Yl'Y2 
2(1 -
p2) 
and as p tends to zero this tends to -P'Yl'Y2, the first-order matrix correction. 
In Section 4.7D, we treat correlation between word pairs separately. We discard 
all correlations except the 15 that are over .15 in absolute magnitude. The 

4.8] 
STUDIES OF REGRESSION EFFECTS 
163 
total correction from summing the corrections for the 15 pairs is - .54. When 
we apply the method of inverting by series to the matrix (with all but 15 corre-
lations set equal to zero), we get the correction -1.0, carrying terms in A 6. 
The sum of the correlations undercorrects by a good deal in this example. 
Convergence of (l + A)-I. The expansion for (l + A)-I will converge if and 
only if all eigenvalues of A have absolute value less than 1. A simple sufficient, 
but not necessary, condition for convergence for a matrix A with O's in the main 
diagonal is that the sum of the absolute values of the elements in each row be 
less than 1. (See, for example, Newman, 1962, pp. 226-227.) The simple condi-
tion is enough to assure the convergence for the matrix with correlations moved 
toward zero by .1, but not enough for the matrix with all but 15 correlations set 
equal to zero. Nevertheless, the expansion converges for the latter matrix. 
4.7G. Summary. Our study of the effects of correlation explores the mean 
difference between log odds obtained under an assumption of independence and 
those obtained by taking account of correlations. The assumptions of normality 
and common covariances are made more appropriate by a square root trans-
formation of the word-count data. For our Federalist data the differences ob-
served for the several methods suggest modest adjustments to the log odds. 
A few questions indicate open problems in such studies. Would an adjustment 
more complex than expected differences in log odds be useful? A linear relation 
between log odds under dependence and independence might be helpful. What 
models of dependence can be studied directly for the negative binomial data? 
Can useful studies of dependence be carried out for words of low frequency? 
How should errors in estimating correlations be treated? 
4.8. 
STUDIES OF REGRESSION EFFECTS 
4.8A. Introduction. How nearly do word rates and log odds computed for 
the disputed papers agree with expectations if the model is true and if Madison 
is assumed to be the true author? The penalty study of Section 4.9 is one at-
tempt to explore the adequacy of models and correctness of predictions, but it 
has been carried out solely on the known papers, and is not effective for strongly 
discriminating words. The studies of word rates and log odds described here 
are designed to check whether the predictions are seriously wrong and whether 
allowance for regression, through the prior distributions, is nearly correct. 
We assume here, as in most of the main study, that word counts are indepen-
dently distributed according to negative binomial or Poisson distributions, and 
that the parameter estimates obtained from the posterior analysis can be used 
as if exactly known. 
If a word is used with rate r in a paper of length w, denote by 'A",(r, w) the log 
odds computed when model cp is used. 
Here we consider only two models: 
cp = P, the Poisson model with parameters estimated from posterior analysis 
based on set 31 of underlying constants, and cp = nb, the negative binomial 

164 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.8 
TABLE 4.8-1 
EXPECTED LOG ODDS FOR POISSON AND NEGATIVE BINOMIAL MODELS 
(2000-word papers, set 31 of underlying constants) 
Negative binomial 
Word 
Word 
Poisson 
(approximate) 
group 
number 
E(~p I P, H) 
E(~pl P, M) 
E(~nb I nb, H) 
E(~nb I nb, M) 
B3A 
60 
11.79 
-4.97 
4.15 
-3.78 
B3A 
Total 
11.79 
-4.97 
4.15 
-3.78 
B3B 
3 
.25 
-
.31 
.20 
-
.24 
4 
.37 
-
.34 
.36 
-
.33 
13 
1.64 
-1.91 
1.02 
-1.06 
39 
.78 
-
.75 
.48 
-
.49 
40 
3.27 
-4.34 
1.95 
-2.16 
55 
1.95 
-1.46 
1.08 
-1.05 
57 
.48 
-
.44 
.33 
-
.31 
58 
.80 
-.77 
.41 
-
.47 
B3B 
Total 
9.54 
-10.32 
5.83 
-6.11 
B3G 
73 
.10 
-.14 
.08 
.05 
78 
.37 
-
.45 
.26 
-
.33 
90 
.34 
-
.23 
.05 
-
.11 
116 
.24 
-
.17 
.07 
-
.12 
117 
.46 
-
.84 
.38 
-
.50 
123 
048 
-
.33 
Al 
-
.31 
160 
.25 
-
.20 
.22 
-
.18 
B3G 
Total 
2.24 
-2.36 
1.47 
-1.60 
B3E 
80 
.17 
-
.12 
.10 
-.10 
81 
.39 
-
.61 
.35 
-
.39 
82 
.18 
-
.13 
.14 
-
.12 
119 
.44 
-
.67 
.29 
-
.26 
124 
.29 
-
.18 
.19 
-
.17 
B3E 
Total 
1.47 
-1.71 
1.07 
-1.04 
B3Z 
87 
.14 
-.11 
.02 
-
.07 
94 
.09 
-
.11 
.07 
-
.08 
96 
.07 
-
.09 
.06 
-
.09 
110 
.09 
-
.07 
.07 
~ .06 
143 
1.12 
-
.68 
.68 
-
.45 
146 
.46 
-
.29 
.35 
-
.26 
151 
.23 
-
.30 
.17 
.16 
153 
.25 
-
.16 
.22 
-
.16 
165 
.12 
-
.17 
.08 
-
.02 
B3Z 
Total 
2.57 
-1.98 
1.72 
-1.35 
30 words 
Total 
27.61 
-21.34 
14.24 
-1;3.88 

4.8] 
STUDIES OF REGRESSION EFFECTS 
165 
model again with parameters based on set 31. We also use r to denote a vector 
of word rates for a group of words, and Aq,(r, w) to denote the sum of the log 
odds for the words in the group. The arguments (r, w) are sometimes dropped. 
If model c/> is true, and if Hamilton is the author, the expected rate is de-
noted by E(r I c/>, H), and the expected log odds is denoted by 
E(Aq,(r, w) I c/>, H) = E(Xq, I c/>, H). 
These expectations, and the corresponding ones, given that Madison is the 
author, are needed for our comparisons. 
The expected rates for each of the final 30 words are shown in Table 3.2-3 
for the negative binomial model c/> = nb as 
JJ.l = E(r I c/>, H), 
JJ.2 = E(r I c/>, M). 
The expected rates for the Poisson model differ only slightly from those for the 
negative binomial. 
The expected log odds for a 2000-word paper for the two models and for the 
two authors are shown in Table 4.8-1 for each of the final 30 words and, summed, 
for each of the five word groups and for all 30 words. The calculations in Section 
4.8 were carried out by Ivor Francis and Charles L. Odoroff. 
4.8B. The study of word rates. For each of the 11 words of highest fre-
quency in the papers of known authorship, we show in Table 4.8-2, on successive 
lines, 
(a) observed rate in 48 Hamilton papers: ml, 
(b) expected rate under the Poisson model if Hamilton is the author: 
E(r I P,H), 
(c) expected rate under the negative binomial model if Hamilton is the 
author: E(r I nb, H), 
(d) observed rate in 50 Madison papers: m2, 
(e) expected rate under the Poisson model if Madison is the author: 
E(r I P, M), 
(f) expected rate under the negative binomial model if Madison is the author: 
E(r I nb, M), 
(g) observed rate in the 12 disputed papers: mo, 
(h) observed rate in the three joint papers: m3, 
(i) standard deviation of mo under the negative binomial model if Hamilton 
is the author: u(rrio I nb, H), 
CD standard deviation of mo under the negative binomial if Madison is the 
author: u(rrio I nb, M). 
The changes from observed rates in the Hamilton and Madison papers to the 
expected rates for each author under either model are very small for these words. 
The observed rates for the 12 disputed papers are far too variable to provide 
evidence on the correctness of these minute adjustments for selection-only for 
upon does the adjustment exceed .3 of a standard error of the disputed mean mo. 

166 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.8 
TABLE 4.8-2 
OBSERVED AND EXPECTED RATES FOR 11 WORDS 
3 
4 
13 
39 
40 
55 
also 
an 
by 
of 
on 
there 
(a) ml 
.28 
5.96 
7.34 
64.65 
3.28 
3.28 
(b) E(r I P,H) 
.31 
5.95 
7.37 
64.64 
3.32 
3.23 
(c) E(r I nb, H) 
.32 
5.95 
7.32 
64.51 
3.38 
3.20 
(d) m2 
.70 
4.57 
1l.44 
57.80 
7.83 
1.29 
(e) E(r I P, M) 
.67 
4.58 
11.43 
57.81 
7.79 
1.32 
(f) E(r I nb, M) 
.67 
4.58 
11.43 
57.89 
7.75 
1.33 
(g) mo 
.75 
3.77 
11.60 
59.13 
7.96 
1.30 
(h) ma 
.36 
3.43 
12.99 
59.38 
7.04 
.72 
(i) u(mo I nb, H) 
.13 
.51 
.73 
2.01 
.48 
.44 
(j) u(mo I nb, M) 
.18 
.45 
.94 
1.93 
.78 
.29 
57 
58 
60 
78 
160 
this 
to 
upon 
both 
though 
(a) ml 
7.80 
40.71 
3.35 
.47 
.94 
(b) E(r I P, H) 
7.79 
40.70 
3.27 
.50 
.91 
(c) E(r I nb, H) 
7.77 
40.79 
3.24 
.52 
.91 
(d) m2 
6.00 
35.25 
.14 
1.08 
.49 
(e) E(r I P, M) 
6.01 
35.25 
.21 
1.05 
.52 
(f) E(r I nb, M) 
6.00 
35.21 
.24 
1.04 
.51 
(g) mo 
6.83 
31.36 
.08 
.67 
.25 
(h) ma 
6.32 
27.07 
.36 
.36 
1.26 
(i) u(mo I nb, H) 
.69 
1.76 
#6 
.16 
.21 
(j) u(mo I nb, M) 
.60 
1.69 
.13 
.24 
.16 
Indeed, whether we measure from the observed or expected mean rates in 
the known papers, the means for the disputed papers show no tendency for 
regression. If we assume that Madison is the author of the disputed papers, the 
observed mean mo shows regression, from the Madison mean m2 toward the 
Hamilton mean ml, for only 4 of the 11 words, and, from the expected 
Madison rate, for only 3 of the 11 words. The deviations, in standard errors, 
of the observed mean mo from the expected rate E(r I nb, M) are: for also, -.4; 
an, -1.8; by, -.2; oj, +.6; on, -.3; there, -.1; this, +1.4; to, -2.3; upon, -1.2; 
both, +1.5; though, -1.6. The signs were adjusted so that «+" indicates regres-
sion toward the Hamilton rate, and «-" anti-regression. Only for both is the 
mean for the disputed papers closer to Hamilton's mean than to Madison's. 
A look at the frequency distributions for these 11 words is instructive. Tables 
4.8-3 show the frequency distributions in the 48 Hamilton papers, the 50 Madi-
son papers, and the 12 disputed papers. 

Rate per 
1000 words 
0 
.01- .19 
.20- .39 
.40- .59 
.60- .79 
. 80- .99 
1.00-1.19 
1.20-1.39 
1.40-1.59 
1.60-1.79 
1.80-1.99 
3.53 
Total 
TABLE 4.8-3 
DISTRIBUTION OF RATES OF OCCURRENCES OF 
WORD NUMBER 3: also 
WORD NUMBER 4: an 
Ratio of number of papers to total 
Rate per 
Ratio of number of papers to total 
Hamilton 
Madison 
Disputed 
1000 words 
Hamilton 
Madison 
Disputed 
.56 
.24 
0 
.01-
.99 
.02 
.12 
1.00- 1.99 
.02 
.02 
.17 
.21 
.14 
.50 
2.00- 2.99 
.02 
.14 
.42 
.06 
.08 
.17 
3.00- 3.99 
.12 
.26 
.04 
.22 
.17 
4.00- 4.99 
.06 
.24 
.17 
.06 
.10 
.08 
5.00- 5.99 
.35 
.14 
.17 
.04 
6.00- 6.99 
.23 
.10 
.08 
.02 
7.00- 7.99 
.08 
.06 
.02 
8.00- 8.99 
.02 
.04 
.04 
.08 
9.00- 9.99 
.06 
.02 
1O.00-lO.99 
.99 
1.00 
1.00 
11.00-11.99 
12.00-12.99 
.02 
Total 
.98 
1.00 
1.01 
(cont.) 
>I>-
~ 
~ 
c::: 
t::) 
..... 
t.:.1 
rp 
o 
"!I 
l:d 
t.:.1 
fa 
t.:.1 
~ 
..... o 
Z 
t.:.1 
~ 
t.:.1 
~ 
..-
0"> 
-.;( 

Rate per 
1000 words 
0 
.01- 1.99 
2.00- 3.99 
4.00- 5.99 
6.00- 7.99 
8.00- 9.99 
10.00-11.99 
12.00-13.99 
14.00-15.99 
16.00-17.99 
18.00-19.99 
Total 
TABLE 4.8-3 (cont.) 
DISTRIBUTION OF RATES OF OCCURRENCES OF 
WORD NUMBER 13: by 
WORD NUMBER 39: of 
Ratio of number of papers to total 
Rate per 
Ratio of number of papers to total 
Hamilton 
Madison 
Disputed 
1000 words 
Hamilton 
Madison 
Disputed 
32.00-35.99 
.02 
.02 
36.00-39.99 
.06 
40.00-43.99 
.29 
.08 
44.00-47.99 
.02 
.25 
.04 
.17 
48.00-51.99 
.04 
.08 
.25 
.22 
.17 
52.00-55.99 
.04 
.20 
.25 
.12 
.22 
.33 
56.00-59.99 
.27 
.32 
.58 
.24 
.17 
60.00-63.99 
.21 
.16 
.08 
.08 
. (H.00-67.99 
.17 
.14 
.08 
.08 
.08 
68.00-71.99 
.06 
.06 
.08 
.04 
72.00-75.99 
.12 
.99 
1.00 
1.00 
76.00-79.99 
.08 
Total 
.99 
1.00 
.99 
(cont.) 
....... 
~ 
I 
~ 
~ 
III 
o 
"'!l 
~ 
~ z 
~ 
~ 
00 

Rate per 
1000· words 
0 
.01-
;99 
LOO- 1.99 
2;00- 2.99 
3.00- 3.99 
4.00- 4.99 
5.00- 5.99 
6.00-
6~99 
7.00- 7.99 
8.00- 8.99 
9.00- 9.99 
10.00,-'10.99 
11.00~ 11.99 
12.00-12:99 
13.00-13.99 
14.00-14.99 
15.00-15.99 
Total 
TABLE 4.8-3 (cant.) 
DISTRIBUTION OF RATES OF OCCURRENCES OF 
WORD NUMBER 40: on 
WORD .NUMBER 55: there 
Ratio of number of papers to total 
Rate per 
Ratio of number of papers to total 
Hamilton 
Madison 
. Disputed 
1000 words 
Hamilton 
Madison 
Disputed 
0 
.12 
.25 
.02 
.01-
.49 
.06 
.17 
.50-
.99 
.04 
.30 
.25 
.31 
1.00- 1.49 
.08 
.26 
.08 
.17 
.04 
.08 
1.50- 1.99 
.08 
.06 
.17 
.14 
.12 
.08 
2.00- 2.49 
.19 
.06 
.17 
.12 
.16 
2.50- 2.99 
.06 
.04 
.08 
.04 
.12 
.08 
3.00- 3.49 
.14 
.06 
.16 
.17 
3.50- 3.99 
.06 
.02 
.06 
.08 
4.00- 4.49 
.21 
.02 
.02 
.. 12 
.25 
4.50- 4.99 
.06 
.25 
5.00- 5.49 
.10 
5.50- 5.99 
.08 
.04 
6.62 
.02 
9.39 
.02 
Total 
.98 
1.00 
1.00 
.02 
.99 
1.00 
.99 
(cont.) 
~ 
~ 
f!§ 
c::l 
t:;;I .... 
t';I 
rJl 
0 
"':I 
~ 
t';I 
§a 
t';I 
rJl 
rJl 
.... 
0 z 
t-J 
"':I 
"':I 
t';I 
C 
003 
rJl 
...... 
0:> 
co 

Rate per 
1000 words 
0 
.01-
.99 
1.00- 1.99 
2.00- 2.99 
3.00- 3.99 
4.00- 4.99 
5.00- 5.99 
6.00- 6.99 
7.00- 7.99 
8.00- 8.99 
9.00- 9.99 
10.00-10.99 
11.00-11.99 
Total 
TABLE 4.8-3 (cont.) 
DISTRIBUTION OF RATES OF OCCURRENCES OF 
WORD NUMBER 57: this 
WORD NUMBER 58: to 
Ratio of number of papers to total 
Rate per 
Ratio of number of papers to total 
Hamilton 
Madison 
Disputed 
1000 words 
Hamilton 
Madison 
Disputed 
23.00-25.99 
.06 
.25 
.02 
26.00-28.99 
.04 
.04 
.17 
.04 
29.00-31.99 
.04 
.22 
.08 
.02 
.04 
32.00-34.99 
.08 
.22 
.25 
.12 
.08 
35.00-37.99 
.12 
.14 
.17 
.08 
.10 
38.00-40.99 
.21 
.14 
.08 
.08 
.16 
.25 
41.00-43.99 
.17 
.12 
.10 
.26 
.33 
44.00-46.99 
.21 
.02 
.19 
.10 
.08 
47.00-49.99 
.06 
.04 
.23 
.14 
.17 
50.00-52.99 
.02 
.17 
.04 
53.00-55.99 
.02 
.02 
.08 
56.00-58.99 
.02 
.08 
Total 
.99 
1.00 
1.00 
.99 
1.00 
.99 
(cant.) 
-
-l 
o 
t-3 
:Il 
trl o 
pj 
~ 
~ 
to 
>-
UJ. 
...... 
UJ. 
o 
t,;l 
~ 
trl 
~ 
Z 
~ 
~ 
00 

TABLE 4.8-3 (cont.) 
H'>-
~ 
DISTRIBUTION OF RATES OF OCCURRENCES OF 
WORD NUMBER 60: upon 
WORD NUMBER 78: both 
Rate per 
Ratio of number of papers to total 
Rate per 
Ratio of number of papers to total 
1000 words 
Hamilton 
Madison 
Disputed 
1000 words 
Hamilton 
Madison 
Disputed 
0 
.82 
.92 
0 
.42 
.14 
.25 
.01- .19 
.01- .19 
.20- .39 
.04 
.20- .39 
.06 
.06 
.40- .59 
.04 
.40- .59 
.10 
.16 
.33 
.60- .79 
.04 
.60- .79 
.10 
.08 
.80- .99 
.02 
.02 
.80- .99 
.12 
.12 
1.00-1.19 
.02 
.08 
1.00-1.19 
.10 
.08 
.17 
~ 
1.20-1.39 
.02 
.04 
1.20-1.39 
.04 
.04 
.25 
1.40-1.59 
.04 
1.40-1.59 
.02 
.08 
t:I 
.... 
t;:I 
1.60-1.79 
.06 
1.6(}--1.79 
.02 
.02 
rJl 
1.80-1.99 
.06 
1. 8(}--1. 99 
.10 
0 
"'!l 
2.00-2.99 
.23 
2.00-2.19 
.02 
::0 
2.20-2.39 
t;:I 
3.00-3.99 
.23 
(jl 
2.4(}--2.59 
::0 
4.00-4.99 
.21 
t;:I 
2.60-2.79 
.04 
U1 
U1 
5.00-5.99 
.06 
.... 
2.80-2.99 
0 
6.00-6.99 
.02 
z 
7.00-7.99 
.02 
3.00-3.19 
.02 
t;:I 
3.20-3.39 
.02 
"'!l 
"'!l 
Total 
.99 
1.00 
1.00 
3.40-3.59 
.02 
t;:I 
(") 
1-'3 
Total 
.98 
1.00 
1.00 
U1 
(cont.) -
-.:t -

172 
THEORETICAL BASIS OF THE·. MAIN STUDY 
[4.8 
TABLE 4.8-3 (cont.) 
DISTRIBUTION OF RATES OF OCCURRENCES OF 
WORD NUMBER 160: though 
Rate per 
Ratio of number of papers to total 
1000 words 
Hamilton 
Madison 
Disputed 
0 
.19 
.30 
.58 
.01- .19 
.20- .39 
.12 
.08 
.40- .59 
.14 
.30 
.25 
.60- .79 
.12 
.14 
.80- .99 
.12 
.04 
1.00-1.19 
.12 
.06 
.08 
1.20-1.39 
.04 
1.40-1.59 
.08 
.02 
1.60-1.79 
.08 
1.80-1.99 
.02 
2.00-2.19 
.02 
2.20-2.39 
2.40-2.59 
.02 
2.60-2.79 
.02 
2.80-2.99 
3.00-3.19 
3.20-3.39 
3.40-3.59 
.02 
Total 
.97 
1.00 
.99 
The distribution in the disputed papers agrees more closely with the Madison 
distribution for every word except both (word No. 78). For most words, the 
agreement with Madison is close and the difference from Hamilton strikingly 
large. 
4.8C. Total log odds for the final 30 words. In the study of rates, we are 
limited to moderate- and high-frequency words, and we must examine each 
word separately. The log odds calculation takes proper account of the variation 
in paper length that makes rates for low-frequency words unsatisfactory (be-
cause a rate of zero is much stronger evidence in a 3500-word paper than in a 
lOOO-word paper). J:;"\uther, the log odds summed over word groups provides 
weighting for the words. 
Comparisons of observed and expected log odds are complicated by variations 
in paper length, and so we need either adjustments for paper length or restric-
tion to a nearly constant paper length. For the Poisson, appropriate adjust-
ments are easy, and we use them, but the negative binomial is more troublesome. 
Only for total negative binomial log odds do we use all papers, with a rough 
adjustment. The adjustments are explained in Sections 4.8E and 4.8F. 

4.8] 
STUDIES OF REGRESSION EFFECTS 
TABLE 4.8-4 
SUMMARY STATISTICS FOR ADJUSTED TOTAL LOG ODDS (30 WORDS) 
Poisson model 
fj)=P 
Expected log odds 
-
Hamilton: E('A", I fj), H) 
27.6 
Madison: E(X", I fj), M) 
-21.3 
Observed log odds 
48 Hamilton papers 
Mean 
Standard deviation 8H 
8H/V48 
50 Madison papers 
Mean 
Standard deviation 8M 
8M/V50 
12 disputed papers 
Mean 
Standard deviation 81) 
80/V12 
3 joint papers 
Mean 
Standard deviation 
30.3 
10.9 
1.6 
-24.1 
8.0 
1.1 
-22.8 
7.4 
2.1 
-15.9 
5.3 
Negative 
binomial model 
fj) = nb 
14.2 
-13.9 
17.1 
5.3 
.77 
-16.5 
4.9 
.70 
-16.2 
4.9 
1.4 
-9.9 
3.4 
Note: All log odds are adjusted to a paper length of 2000 words. 
173 
We restrict the study of negative binomial log odds for separate word groups 
to papers in the "2000-word sets" (see Section 6.2). The 2000-word sets are those 
papers with lengths between 1728 and 2282 words and they include 23 Hamilton 
papers with mean length 2.013, 23 Madison papers with mean length 2.035, 
7 disputed papers with mean length 2.032. Table 6.2-1 shows the Hamilton 
and Madison paper numbers. The disputed papers in the 2000-word set are 
numbered 51-55, 57, 58. 
Table 4.8-4 shows summary statistics for comparison of the observed and 
expected log odds in total for the final 30 words. All observed log odds have 
been adjusted to correspond approximately to a common 2000-word length of 
paper. The paper-to-paper variation in log odds is tremendous, and it masks 
most comparisons. For both models and both authors, the forecast regressions, 
using the prior distribution determined by set 31 of underlying constants, are 
about 3 units in log odds. That is, the expected log odds is about 3 less than the 

174 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.8 
observed mean log odds. 
This expected regression is small relative to the 
standard error of a single paper, but not to that of the mean of 48,50 or even 
12 papers. If we assume that Madison wrote the disputed papers, then they do 
not show as much regression as we anticipated; indeed the negative binomial 
log odds shows only .3, or about one-tenth as much as anticipated. The ex-
planation may be that the model is failing in some subtle way that we have not 
detected, so we are not overjoyed to find it working better than it should. 
Table 4.8-5 illustrates the performance of individual papers by showing the 
adjusted total log odds under the two models for 10 Hamilton, 10 Madison, 
12 disputed, and 3 joint papers. The Hamilton and Madison papers are random 
samples from the 2000-word set. The principal feature is the huge paper-to-
TABLE 4.8-5 
OBSERVED LOG ODDS FOR THE FINAL 30 WORDS ADJUSTED TO A 
COMMON PAPER LENGTH OF 2000 WORDS 
Paper number 
Poisson log odds 
Xp 
Negative binomial log odds 
Xnb 
Hamilton papers (random sample of 10 from 2000-word set) 
6 
29.0 
7 
32.7 
12 
23.6 
21 
35$ 
25 
16.9 
30 
44.3 
34 
34.7 
35 
29A 
59 
20.6 
60 
26.3 
Expected log odds 
E(Xp I P, H) = 27.6 
18.5 
16.6 
13.~ 
17.0 
12.8 
18.5 
16.8 
14.8 
15.8 
14.7 
E(Xnb I nb, H) = 14.2 
Madison papers (random sample of 10 from 2000-word set) 
202 
-24A 
-13.8 
204 
-34.6 
-24.5 
207 
-28.8 
-20.7 
208 
-34.4 
-22.5 
210 
-39.6 
-26.5 
212 
-23.7 
-14.5 
216 
-9.9 
-6A 
217 
-33.0 
-20.0 
219 
-22.7 
-17.1 
220 
-26.0 
-15.5 
Expected log odds 
E(Xp I P, M) = -21.3 
E(Xnb I nb, M) = --13.9 

4.8] 
STUDIES OF REGRESSION EFFECTS 
175 
TABLE 4.8-5 (cont.) 
Paper number 
Poisson log odds 
Negative binomial log odds 
hp 
hnb 
Joint papers 
18 
-19.3 
-11.1* 
19 
-18.5 
-12.5 
20 
-9.8* 
-6.0* 
Disputed papers 
49 
-22.8* 
-16.2* 
50 
-33.1* 
-23.9* 
51 
-35.0 
-24.3 
52 
-25.2 
-17.8 
53 
-20.4 
-15.7 
54 
-22.9 
-14.9 
55 
-7.0 
-6.1 
56 
-13.7* 
-11.1* 
57 
-23.8 
-16.5 
58 
-25.3 
-17.9 
62 
-22.6* 
-15.3* 
63 
-21.4* 
-14.3* 
Note: Log odds adjusted for paper length by more than 10 per cent are 
starred. 
paper variation in spite of constant length of paper. The variation appears 
regular and the means and standard deviations presented in Table 4.8-4 are a 
fair summary, except that the negative binomial log odds for the Hamilton 
sample are unusually stable. 
4.8D. Log odds by word group. To compare the regression in log odds for 
each word group from the known papers to the disputed papers, we restrict 
attention to the papers in the 2000-word set, and use only a minor adjustment 
for paper length. Table 4.8-6 shows for the two models, and for the five word 
groups B3A, B3B, B3G, B3E, B3Z, for all 30 words, and for the 29 words ex-
cluding upon (B3A), the mean log odds in the 23 Hamilton, 23 Madison, and 7 
disputed papers of the 2000-word set. The bottom third of the table shows the 
mean Poisson log odds for all papers adjusted to 2000-word length by a propor-
tional adjustment, appropriate to the Poisson, but not generally to the negative 
binomial. 
The separate word groups show much the same behavior as the total of 
30 words. The negative binomial mean log odds for the disputed papers falls 
closer to the mean observed log odds for Madison than to the expected log odds 
for Madison. The Poisson log odds shows more regression than does the negative 

176 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.8 
TABLE 4.8-6 
MEAN LOG ODDS AND EXPECTED LOG ODDS BY WORD GROUP 
B3A 
B3B 
B3G 
B3E 
B3Z 
All 30 
29* 
words 
words 
Negative binomial, 
2000-word set 
23 Hamilton 
5.3 
6.2 
2.6 
1.9 
2.4 
18.4 
13.1 
E(~nb I nb, H) 
4.2 
5.8 
1.5 
1.1 
1.7 
14.2 
10.1 
4 Late Hamiltont 
4.6 
6.4 
1.1 
.1 
1.2 
13.4 
8.8 
23 Madison 
-4.2 
-7.3 -2.4 -2.3 -2.0 -18.2 -13.9 
E(Xnb I nb, M) 
-3.8 
-6.1 -1.6 -1.0 -1.4 -13.9 -10.1 
7 disputed 
-4.3 
-7.3 -1.9 
-.8 -1.8 -16.2 -11.9 
Poisson, 2000-word set 
23 Hamilton 
15.4 
9.8 
3.5 
2.3 
3.5 
34.6 
19.2 
E(Xp I P, H) 
11.8 
9.5 
2.2 
1.5 
2.6 
27.6 
15.8 
4 Late Hamiltont 
12.0 
10.4 
1.5 
.1 
1.4 
25.4 
13.4 
23 Madison 
-5.4 -12.4 -3.3 -3.3 -2.7 -27.1 -21.7 
E(Xp I P, M) 
-5.0 -10.3 -2.4 -1.7 -2.0 -21.3 -16.4 
7 disputed 
-5.4 -11.8 -2.6 -1.0 -2.2 -23.0 -17.6 
Poisson, all papers, 
means adjustedt 
48 Hamilton 
12.2 
9.8 
3.1 
2.1 
3.5 
30.7 
18.5 
50 Madison 
-5.3 -10.5 -3.0 -2.3 -2.8 -23.9 -18.5 
12 disputed 
-5.6 -11.7 -2.2 
-.7 -2.0 -22.4 -16.8 
* "29 words" excludes B3A, upon. 
t All 4 Late Hamilton Federalist papers are used with proportional adjustment, 
even though none satisfy the 2000-word set definition. 
t Differences from Table 4.8-4 are caused by a different adjustment. 
binomial log odds, and the three low-frequency word groups B3G, B3E, and 
B3Z regress further than expected. 
Word group B3E is the poorest word group on most scores. For the known 
papers, its mean log odds is lowest; its expected log odds is least; its mean log 
odds in the disputed papers falls below the Madison expectations, substantially 
so for the Poisson model. However, B3E contains only five words, with an ex-
pected 2 Madison markers and .6 Hamilton markers in a 2000-word Madison 
paper, and an expected .5 Madison markers and 1.6 Hamilton markers in a 
2000-word Hamilton paper. With these low expectations, the mean log odds for 
the negative binomial in the disputed papers is remarkably close to its expecta-
tion, while the Poisson performance is an indication of the inadequacy of the 
Poisson model. 

4.8] 
STUDIES OF REGRESSION EFFECTS 
177 
4.8E. Theory for the Poisson model. We assume that word rates are dis-
tributed independently according to Poisson distributions with mean rates equal 
to those estimated under the analysis based on set 31 of underlying constants. 
Denote these mean rates for a word by iJ.l and iJ.2' 
The log odds Xp(r, w) for rate r in a paper of length w is 
The log odds factor is (separately) linear in the observed rate r and proportional 
to paper length w. The expected log odds (and its entire distribution) is easily 
computed if model P is true. Alternatively, the expectation can always be 
obtained by interpolating linearly in the observed log odds. Specifically, if 
Xp = Xp(r', w') and Xli = Xp(r", w") are the log odds under model P for 
the two papers of lengths w', w" and rates r', r", respectively, then 
(1) 
E[Xp(r, w) I P, H] = a w, Xl> + (1 -
a) ~, X'j. 
W 
W 
provided ar' + (1 -
a)r" = iJ.l' A similar result with the same or different 
papers holds for Madison. For many words, the r', r" can be chosen so near 
iJ.l and w', w" so near w that no computation is needed. The computation is 
exact for any (r', w'), (r", w") as long as the Poisson model is true. The Poisson 
entries in Table 4.8-1 are computed in this way. 
The proportionality in paper length allows an adjustment to simplify the 
final comparisons. Suppose E(Xp(r, w) I P, i), i = H, M, are computed for a 
paper of standard length Wo (= 2.000, say). For comparison with observed log 
odds in papers of other lengths, the Poisson expectations for Wo can be adjusted 
exactly to apply to the other lengths, but only at the cost of having a separate 
expectation for each observed paper. Instead, we can adjust each observed log 
odds to what it would be were the same rate observed in a paper of length woo 
The Poisson entries in Tables 4.8-4 and 4.8-5 are means and standard deviations 
of adjusted log odds 
(2/w)Xp(r, w), 
adjusted to length 2000 words. 
In Table 4.8-6, the adjustment is applied to the means of unadjusted log 
odds, or equivalently, adjusted log odds are weighted in proportion to paper 
length: 
l;Xp(r i, Wi) ~. 
n 
m 
This last mean is easier to compute and is a better estimate of expected log odds, 
because the paper length is proportional to the inverse variance of X(r, w). The 
mean lengths m and adjustment factors 2/m are shown in Table 4.8-7 for the 
2000-word sets and the entire sets. 

178 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.8 
TABLE 4.8-7 
MEAN PAPER LENGTHS AND PROPORTIONAL ADJUSTMENT FACTORS 
All papers 
2000-word set 
Author: 
Hamilton Madison Disputed Joint Hamilton Madison Disputed 
Number of 
papers: 
48 
50 
12 
3 
23 
23 
7 
iii 
1.957 
2.272 
1.990 
1.847 
2.013 
2.035 
2.032 
2 
iii 
1.022 
.880 
1.005 
1.083 
.993 
.983 
.984 
4.8F. Theory for the negative binomial model. If word rates are dis-
tributed according to a negative binomial distribution with mean rates J.Ll or J.L2 
and non-Poissonness rates ~1 or ~2' the log odds Anb(r, w) is complex, and even 
to find its expectation is difficult. 
One approximate determination of the negative binomial expectation is based 
on Eq. (1) of Section 4.8E. For the negative binomial model, the results are ap-
proximate only, and the choice of pair (r', w') or (r", w") is important in im-
proving the approximation. Were the paper lengths constant, the choice would 
be among two-point formulas for evaluating expectations, with restriction of 
points to integers. The negative binomial expectations in Table 4.8-2 are ob-
tained by these two-point approximations as set forth in Section 4.8G. 
The paper-length adjustment used in the study of total log odds in Section 4.80 
was 
* 
y(2) 
Anb = yew) Anb, 
with yew) = log(l + Aw), an ad hoc choice with slope decreasing as paper 
length increases. This decreasing slope is appropriate for the negative binomial 
dependence on paper length for most words, but as the exact form of the de-
pendence varies from word to word, the above adjustment was used only for 
the total log odds. 
In the study of separate word groups in Section 4.8D, the negative binomial 
log odds were averaged only over the 2000-word set. As long as the dependence 
on length of paper is nearly linear over the 1728 to 2282 word range, the mean 
log odds corresponds to the mean paper lengths, which differ by less than 
two per cent from 2000 words (see Table 4.8-7). These means were then pro-
portionally adjusted to 2000 words, along with the Poisson log odds. For the 
four late Hamilton papers, the same adjustment was used, even though the 
mean length was only 1.813 and all four were outside the range of lengths 
of the 2000-word set. We had no easy alternative treatment for these four 
papers. 

4.8] 
STUDIES OF REGRESSION EFFECTS 
179 
4.8G. Two-point formulas for expectations of negative hinomiallog 
odds. Let r be a random variable with mean p" variance u2, and third moment 
about the mean P,3 = su3• The expectation of A, a function of r, is to be ap-
proximated by a weighted sum of the values of A at two values of r, say r' 
and r": 
In what follows it is convenient to treat values of r as standard deviates, 
r' -
P, 
Zl = --u-' 
r" -
p, 
Z2 = 
U 
The two-point approximation is exactly correct if A is a linear, quadratic, or 
cubic function of r, provided, respectively, the first two, three, or four of the 
following conditions hold. 
Raw form 
Deviate form 
(1) 
al + a2 = 1, 
(I') 
al + a2 = 1, 
(2) 
aIr' + a2r!,j = p" 
(2') 
alZI + a2Z2 = 0, 
(3) 
(r' - P,y 
(r" - P,y 
al -u- + a2 --u-
= 1, 
(3') 
2 + 
2 
alZI 
a2Z2 = 1, 
(4) 
(r' -
p, y 
(r" -
p, y 
al -u- + a2 --u-
= s, 
(4') 
alZr + a2z~ = s. 
Given any s, the unique two-point formula, correct for a cubic A, is based on 
(5) 
(2") 
(I") 
Zl, Z2 = (s =F v'S2+4) /2, 
Z2 
al = 
, 
Z2 -
Zl 
a2 = 1 -
al. 
These values of Zl, Z2 satisfy also 
(4") 
(3") 
Zl + Z2 = s, 
ZlZ2 = -1. 
Given any Zl ;*. 0, the unique two-point formula, correct for a quadratic A, 
is determined by Eqs. (3"), (2"), (1"). Given any Zl ~ Z2, the unique two-
point formula, correct for linear A, is determined by Eqs. (2"), (I"). 
To apply the results to the negative binomial distribution, we need to be able 
to evaluate A at the chosen values of r' and r". Since X is difficult to evaluate, 
we require that the values of A be among the log odds already computed for the 
known and unknown papers. If all papers were of i length Wo = 2.000, the 
available r's would be some of the half-integers. Varyip.g paper lengths give a 

180 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.9 
larger selection of rates, but require an adjustment for paper length on the log 
odds that introduces another approximation. The result is that a formula cor-
rect even for quadratic A cannbt be obtained, but both the quadratic and cubic 
results guide the choice among available r. 
To adjust negative binomial log odds for small changes in paper length, the 
proportional dilation, exact for Poisson, seelllS adequate over the range 1800-
2300. For moderate non-Poissonness, the slope of Anb(r, w) decreases with in-
creased w, and seems more nearly proportional to 10g(1 + ow) than to w. For 
o = .4, which is large for most of our 30 words, 10g(1 + ow) would scale 
down a 2300-word paper to a standard 2000-word length by the factor 
loge1 + .4(2.0)) 
1 
10g(1 + .4(2.3)) = 1.11 ' 
whereas using w directly yields 
Wo 
2.0 
1 
--:-=-=--. 
w 
2.3 
1.15 
This degree of agreement scarcely justifies the use of more complex adjustments, 
and so we use proportional adjustment. 
By a set of rules designed to approximate to the optimum two points, we 
choose, from among the available papers, a pair with rates r', r", lengths w', 
w", and log odds A(r', w'), A(r", w"). We determine 
r" -
fJ, 
al = r" -
r" 
and evaluate the approximate expected log odds for word i under model nb by 
the formula 
E(Anb(r, Wo I nb, H or M)) "'" al(wo/W')Anb(r', w') + a2(wo/W")Anb(r", w"). 
The entire process is repeated for each word, and for each author. 
4.9. A LOGARITHMIC PENALTY STUDY 
How can one evaluate methods for making probability predictions? Were 
there large sequences of trials, one could accumulate sets of trials with approxi-
mately the same predicted probabilities, and then compare the frequencies of 
the different outcomes with the predicted probabilities. Rare is the problem in 
which enough trials can be accumulated to provide a solid check on a predicted 
probability of .1, let alone probabilities of 10-3 or 10-6. In favorable circum-
stances, some overall way of comparing predictions with observed outcomes can 
be provided; in unfavorable circumstances, the evaluation of the method must 
rely solely on study of the acceptability of the component assumptions underly-
ing the method. In this section, we describe one systematic procedure, and use 
it to check some of our methods for selected words. 

4.9] 
A LOGARITHMIC PENALTY STUDY 
181 
4.9A. Probability predictions. For us a probability prediction means a 
prediction that is a probability distribution over the possible outcomes of a trial. 
We suppose that probability predictions are made for each of a number of trials 
whose outcomes can be observed, thus making possible comparisons between 
prediction and performance. 
Introduce a random variable 6j to represent the unknown outcome of the jth 
trial, and assume that the possible outcomes are labeled 1, 2, ... ,k. We take 
k = 2 in the Federalist application (1 for "Hamilton is the author," 2 for 
"Madison is the author"), but one important part of the development depends 
on having a general k. 
Predictions may be based on many methods; we introduce an abstract variable 
cP to range over all possible methods, and denote by %(cp) the probability pre-
dieted for outcome i on trial j, using method cp. To insure that the predictions 
represent probability distributions, we require that 
and 
In much of this section, what the methods of prediction are plays no role. 
We do not require that the methods be based on Bayes' theorem. However, we 
have in mind methods based on some statistical evidence, so we assume that the 
available data on the jth trial can be represented by a point D j in some "data" 
space~. By a method of prediction cp, we mean a function whose domain is ~ 
and whose range is the space of probability distributions on the outcomes 
{I, 2, ... , k}. In particular, if the method is applied to the observed data point 
Dj for the jth trial, 
cp(Dj) = (Qlj(CP), ... ,Qkj(CP»). 
The right-hand side of this equation merely displays the probability of the pre-
diction in detail. 
This formulation permits great flexibility in stating problems, including the 
possibility of severely restricting what may be used to make predictions. For 
example, if the data D j on the jth trial do not include information on the trial 
number or on the other trials, then each prediction is made as if that trial were 
the only one being considered, and the order of presentation of trials is irrelevant. 
The method in this restricted case is distinguished, in particular, from methods 
which assign predictions simultaneously for all the trials on the basis of the 
combined evidence, and from methods which proceed sequentially, with in-
formation on the first j trials available for use in predicting the outcome of the 
jth trial. 
4.9B. The Federalist application: procedure. We present the Federalist 
application in Sections 4.9B through 4.9E, anticipating the theoretical develop-
ments of Sections 4.9F and 4.9G. 
As a test sequence of papers to be classified, we have virtually no choice but 
to use the papers of known authorship. From the 98 papers of known authorship, 

182 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.9 
we eliminate the last two "papers" (Nos. 219, 220) of the Neutral Trade essay 
by Madison, and use the remaining 48 Hamilton and 48 Madison papers as a 
test sequence. Although equal numbers of papers by the two authors are not 
essential, they do simplify a few formulas and interpretations. 
In predicting for the disputed papers, we might want to treat each paper as 
if it were the only one to be classified, or we might want to classify simultaneously 
the entire set of disputed papers. Essentially the only gain from the simultaneous 
classification arises from using dependent prior probabilities of authorship. 
While the assessment of the historical evidence would, for example, put much 
more probability on all 12 being by Madison than the product of the marginal 
probabilities for Madison, just how much, and what the complete joint distri-
bution would be, require subjective assessments that we do not want to make 
in this part of our study. 
On the other hand, in our test sequence the prior probabilities of authorship, 
marginal and joint, are directly and exactly specified. If the test papers are 
treated separately, since half are by each author the prior odds are even, and 
the problem is rather similar to that faced with the disputed papers. If the test 
papers are treated jointly, the prior probability is distributed uniformly on the 
(~~) classifications with 48 papers assigned to Hamilton and 48 to Madison. 
The resulting problem is totally unlike that faced with the disputed papers, and 
the exact analysis is not feasible, because it involves such operations as summing 
over the (~~) ~ 1028 admissible classifications. Consequently, we treat each 
paper in the test sequence separately, as if it were t~e only paper, and restrict 
our permissible methods of prediction to those for which order of presentation 
makes no difference. 
The evaluation study we have executed is restricted to predictions based on 
single, high-frequency words, principally by, on, and to. We might have applied 
the techniques to predictions from several words at a time, except for compu-
tational difficulties and a critical weakness in evaluating strong predictions by 
variables (like upon) so accurate that few errors in direction are made. 
Let us describe the general procedure for the word by. We suppose the data 
for the jth paper are: 
(a) the paper is one from among 96 papers, 48 by Hamilton and 48 by 
Madison; 
(b) the paper's length is Wj; 
(c) the paper contains Xj occurrences of by; 
and possibly such additional data as: 
(d) the mean rate of by in 93,954 words of Hamilton text is 7.34, and in 113,597 
words of Madison text is 11.44, etc. 
Items (a) and (d) are common data for all papers. 

4.9) 
A LOGARITHMIC PENALTY STUDY 
183 
The data in item (d) are from the same papers as used in the test sequence, 
but we believe that no major adjustment in the penalty study is needed to 
allow for this double use. We use only a few high-frequency words, for which 
the parameters are well estimated. Were we to base our predictions for any 
paper on parameters estimated from the other 97 papers, the dependence between 
predictions and predicted event would be eliminated, yet the estimates and 
predictions would be only slightly changed. (Much more investigation of this 
kind of question is needed:) For the present, pretend that data such as those 
in (d) are obtained from a set of papers different from those in the test sequence. 
Our interest centers on prediction methods based on Bayes' theorem-
methods similar to those used for the disputed papers. To use Bayes' theorem, 
we assign initial odds of authorship, and we assume that the count Xj is the 
observed value of a random variable Xj whose distributions, conditional on 
OJ = 1 and on OJ = 2, must be specified. 
For example, in a method that we describe later and give the name 
(1,22,1,1), we use even initial odds and assume that the conditional distribu-
tions of Xj given OJ belong to the Poisson family and that the unknown parameters 
are distributed according to the prior distributions specified by set 22 of under-
lying constants. The final odds of authorship for paper j are determined, after 
two applications of Bayes' theorem, by a version of Eq. (7) of Section 4.3. 
Here, as in most of Chapters 3 and 4, we use the modal approximation to the 
odds factor, evaluating it as if the parameters were known to be equal to their 
posterior modal values. From the approximate final odds, we determine the 
final probabilities of authorship and use them as our predictions. 
Because we make the modal approximation, the prediction method can be 
interpreted alternatively as a method in which the parameters are assumed 
known and Bayes' theorem is used directly. 
Of course, prediction methods 
based on known parameters may be used for any set of values, not just those 
arising as posterior modes. 
We label our prediction methods by a quadruple r/> = (a, (3, 'Y, p), where a 
indexes the family of data distributions used, {3 indexes the set of underlying 
constants used to determine the parameters, 'Y specifies the initial odds of author-
ship, and p specifies an adjustment factor used in a variation of the method 
based on Bayes' theorem. In our problem, the families of data distributions are: 
a = 1, Poisson; a = 2, negative binomial; a = 0, data ignored. We use sets 
of underlying constants, {3 = 11,21,22,31, 38. For the Poisson family (a = 1), 
sets 11 and 21 are identical, and sets 22 and 31 are identical; for a = 0, {3 is 
irrelevltnt. 
In most of the study, we choose even initial odds, 'Y = 1, and the adjustment 
factorp = 1 (no adjustment). We introduce other values as needed. 
4.9C. The Federalist application: the penalty function. We want meth-
ods of predicting probabilities that neither exaggerate nor minimize assurance, 
but tell the correct probability insofar as it is available. As we shall see, a mea-

184 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.9 
sure that encourages choosing methods with this desirable property penalizes 
each forecast according to the negative logarithm of the probability predicted 
for the actual author of the test paper. For example, suppose the probability 
of Hamilton's authorship is forecast as 1/e5• If Hamilton is the author, the 
penalty is-log e-5 = 5, but if Madison is the author, the penalty is 
-log(l -
e-5) ~ e-5 ~ O. 
Thus when events forecast as rare occur, the 
penalty is severe, but when an event forecast as almost certain occurs, the 
penalty is slight. The total penalty is obtained by summing the penalties for 
each prediction. 
More fonnally, if Qlj(q,) and q2j(q,) are the predicted probabilities for Hamilton 
and Madison for the jth paper, then the total penalty for method q, is 
(1) 
U(q,) = 
48 Hamilton 
test papers 
48 Madison 
test papers 
We can specify any method q, for use in making predictions. What the correct 
method is cannot be determined, but we find it useful to study the expected 
total penalty H(q, I q,') when q, is used to predict but when predictions based 
on q,' are assumed to be correct: 
(2) 
H(q, I q,') = - L;[qlj(q,')log qliCq,) + Q2j(q,')10g Q2j(q,)). 
j 
The basis for choosing this logarithmic penalty function and of the several 
interpretations of its values are developed in Sections 4.9F and 4.9G. Here we 
note that U has an approximate interpretation as a negative log likelihood over 
the space of methods regarded as possible values of a "correct" method. Though 
we cannot neglect the prior probabilities that methods are correct, we do not 
handle them formally. 
The total penalty U(q,) may be regarded as a loss suffered when cf> is used. 
U(q,) is unbounded, and would be infinite if an event predicted to have proba-
bility zero were to occur; it is nonnegative and is zero only for sure and perfect 
prediction of each paper. As bench marks, Table 4.9-1 shows the expected total 
penalty received from predicting "heads" with probability p on each of 96 tosses 
of a coin with probability p of "heads," and also for a coin with probability 
1 -
p of "heads." Thus any method may be compared to a method of constant 
predictive strength. These bench marks, especially those for predictions worse 
than guessing, are not uniquely appropriate, but seem useful and natural. 
The penalty function has the property that the expected loss H(q, I q,') when 
q,' is correct is minimized if cf>' is used to make predictions (cf> = cf>'), that is, the 
penalty encourages use of correct methods. 
(The reverse property fails: for 
fixed cf>, H(cf> I q,') is not minimized in q,' for cf>' = q,.) 
To compare U(q,) with H(q, I q,') is to compare the observed penalty from 
using method q, with that expected if the predicted probabilities using cf>' were 

4.9] 
A LOGARITHMIC PENALTY STUDY 
TABLE 4.9-1 
P:jilNALTIES FOR PREDICTING THE CORRECT PROBABILITY 
AND ITS COMPLEMENT 
185 
Expected total penalty for predicting "heads" with probability p on each of 
96 tosses of a "coin" whose probability of "heads" is p: 
Hl(p) = -96[plogp+ (1- p)log(l- p)], 
or whose probability of "heads" is 1 -
p: 
H2(p) = -96[(1 -
p)log p + p log(l -
p)] . 
p 
1.0 
.98 
.95 
.90 
.80 
. 70 
.60 
.50 
Hl(P) 
0.00 
9.41 
19.06 
31.21 
48.04 
58.64 
64.61 
66.54 
H2(p) 
~ 
368.08 
273.46 
199.96 
166.45 
91.18 
72.39 
66.54 
correct. If the method cj/ is nearly correct, then for all cjJ, the approximation 
U(cjJ) ~ H(cjJ I cjJ') 
should hold. Differences from expectation may be judged by computing an 
approximate standardized deviate. The standard deviation· of the penalty is 
computed under the assumptions that the 96 trials are approximately inde-
pendent, and that the probabilities predicted under the method cjJ' are correct. 
Then the standardized deviate is: 
(3) 
Z(cjJ I cjJ') = U(cjJ) -
H(cjJ I cjJ') . 
vVar(U(cjJ) I cjJ') 
The special case in which, for any cjJ, U(cp) is compared with H(cp I cp) is inter., 
esting; roughly, U(cjJ) -
H(cp I cjJ) is positive or negative according as the pre-
dicted prohabilities are more or less extreme than justified on the basis of 
whatever information they utilize. But this comparison gives no idea of whether 
the method makes use of all or any of the available information. 
4.9D. The F~deralist application: numerical results. Table.4.9-2 shows 
the observed penalty U(cjJ) and expected penalty H(cjJ I cjJ) for the null method, 
three Poisson methods, and five negative binomial methods for predictions based 
on each of the words by, on, to (initial odds 'Y = 1, adjustment factor p = 1). 
The computations were programmed for a digital computer by Miles Da,vis. 
Examine first the observed penalty U(cjJ) for predictions based on by. The 
"no data" method predicts ".5" for everything and is charged .693 = loge 2 per 
trial for a total of 66.54. (Because the prediction is .5 for each outcome, the 
penalty is independent of the observed outcome.) The Poisson methods do 
much better than guessing, by about 15 units on a scale of natural log likelihood. 

186 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.9 
TABLE 4.9-2 
TOTAL OBSERVED PENALTY U(cp) AND EXPECTED PENALTY H(cp I ¢) 
FOR 9 PREDICTION METHODS cp = (a, (:3, 1, 1) FOR by, on, to. 
Method 
by 
on 
to 
a 
(:3 
U(cp) 
H(cp I cp) 
U(cp) 
H(cp I cp) 
U(¢) 
H(¢ I cp) 
0 
-* 
66.54 
66.54 
66.54 
66.54 
66.54 
66.54 
1 22t 
52.02 
32.28 
38.52 
21.90 
61.35 
43.92 
1 llt 
51.75 
32.64 
38.38 
22.10 
61.05 
44.35 
1 38 
50.49 
34.52 
37.51 
23.52 
59.94 
46.06 
2 21 
46.05 
46.99 
32.95 
34.84 
56.26 
56.98 
2 
11 
45.93 
46.83 
32.99 
34.51 
56.26 
56.81 
2 22 
46.10 
46.44 
32.89 
34.41 
56.26 
56.54 
2 31 
46.12 
45.87 
32.88 
33.83 
56.26 
56.19 
2 38 
46.32 
49.52 
33.53 
38.34 
56.43 
58.50 
* For a = 0 (no data), all values of (:3 are irrelevant. 
t For a = 1 (Poisson), set 21 of underlying constants is the same as set 11, and set 
22 is the same as set 31. 
The negative binomial methods are about five units better than the Poisson, 
and correspond, using Table 4.9-1, to a constant predictive probability of 
about .81. 
The variation in the observed penalties across the five sets of underlying con-
stants for the negative binomial is small; the range is only .39 for by, not much 
for a difference in log likelihoods. Thus we cannot make any sharp distinctions 
among these choices. Recall, of course, that these five choices are all based 
largely on the same data used in the test sequence, and that all of these estimates 
are within a plausible range determined by the earlier analysis of this data. 
The parameter estimates that would minimize the observed total penalty are 
not easily determined, but we are rather pleased that the penalties based on our 
preferred set, number 22, are close to the minimum for most of the words 
examined. 
Comparing the observed and expected penalties in Table 4.9-2, we see that 
the predictions from the Poisson model fare much worse than expected, while 
those from the negative binomial fare very close to expectations. The standard 
deviations of the penalt~es are large, but the standardized deviates Z(cp I cp) of 
Eq. (3) are about 3 for the Poisson methods, -.7 for method (2, 38), and 
less than .2 in absolute value for the other negative binomial methods. Table 
4.9-3 shows a few of the standard deviations and standardized deviates (in the 
columns headed cj/ = 
cp) • We interpret these comparisons to mean that the 
Poisson model is seriously wrong for the word by, leading to some predictions 
that tend to be much too strong. Because we want very strong predictions, 
erroneous tendencies of this sort are very costly. That the negative binomial 

4.9] 
A LOGARITHMIC PENALTY STUDY 
TABLE 4.9-3 
COMPARISON OF TOTAL OBSERVED PENALTY U(cf» 
FOR 
PREDICTION METHOD cf> WITH EXPECTED PENALTIES H(cf>Icf>') 
IF PREDICTION METHOD cf>' IS CORRECT, FOR by 
187 
cf> = (a, (3, 1, 1) 
U(cf» 
H(cf> I cf>') 
VVar[U(cf» Icf>'] 
Z(cf> I cf>') 
a 
(3 
cf>' =: 
cf> 
(2, 11) 
cf> 
(2, 11) 
cf> 
(2, 11) 
1 
11 
51.8 
32.6 
52.1 
5.0 
8.7 
3.8 
-.0 
1 
38 
50.5 
34.5 
50.8 
5.0 
8.0 
3.2 
-.0 
2 
11 
45.9 
46.8 
46.8 
4.8 
4.8 
-.2 
-.2 
2 
38 
46.3 
49.5 
47.0 
4.6 
4.2 
-.7 
-.2 
penalties agree with expectation is not proof of the correctness or "bestness" of 
the negative binomial model, but it is further evidence that the model seems to 
work well. 
Each of the negative binomial methods with (3 = 11, 21, 22, or 31 (but not 
(3 = 38) exhibits the further property, true of nearly correct methods, that the 
expected penalty if it is assumed correct for predictions by any other method cf> 
is near to the observed penalty from cf>. Table 4.9-3 illustrates this behavior 
[in the columns headed cf>' = (2,11)], comparing penalties for two Poisson and 
two negative binomial methods with their expectations if method cf>' = (2,11,1,1) 
is correct. All four comparisons show standardized deviates (highly dependent) 
near zero. 
We may notice that (3 = 38 performs better than (3 = 11 for the Poisson, 
whereas the reverse is true for the negative binomial. The modal estimates of 
the Hamilton and Madison means, under the Poisson model, are: 
Observed means 
(maximum likelihood estimates) 
(3 = 11 estimates 
(3 = 38 estimates 
Hamilton 
7.34 
7.40 
7.55 
Madison 
11.44 
11.40 
11.27 
That (3 = 38 leads to a higher expected penalty and a lower observed penalty 
than does (3 = 11 arises because the Poisson is not a good family for prediction, 
and to predict from estimates that lessen the apparent separation of the Hamil-
ton and Madison distributions is preferable to using the standard estimates. 
How much closer the means should be taken to minimize the penalty has not 
been determined, but it is clear that estimates of parameters obtained by mini-
mizing total penalty may reflect and partially compensate for the inadequacies 
of the family of distributions used for prediction. 

188 
THEORETICAL BASIS OF THE MAIN STUDY 
TABLE 4.9-4 
TOTAL OBSERVED PENALTY U(cf» 
AND EXPECTED PENALTY H(if> I cf» 
FOR Two PREDICTION METHODS cf> = (a, 11, 1, 1) AND 
8 HIGH-FREQUENCY WORDS 
a = 1 
a = 2 
Word 
(Poisson) 
(Negative binomial) 
number 
Word 
U(cf» 
H(cf> I cf» 
U(cf» 
H(cf> I cf» 
3 
also 
58.75 
58.71 
59.07 
6l.32 
4 
an 
57.32 
58.57 
57.64 
60.17 
13 
by 
5l.75 
32.64 
45.93 
46.83 
39 
of 
56.69 
50.30 
56.71 
58.15 
40 
on 
38.38 
22.10 
32.99 
34.51 
55 
there 
47.16 
33.78 
44.00 
44.81 
57 
this 
60.00 
54.32 
59.36 
55.42 
58 
to 
6l.05 
44.35 
56.26 
56.82 
[4.9 
We have not explpred the estimation procedure based on minimizing total 
penalty. Although the penalty function and its expectation are closely related 
to information definitions, the estimation procedure seems unrelated to any of 
the work based on information measures. We feel the procedure is of potential 
value, especially if used in conjunction with the adjustment factor of Section 
4.9E. A few superficial features are discouraging: the procedure does not coin-
cide with maximum likelihood or other standard procedures, even for the 
simplest families of distributions; it does not make use of any prior information; 
it cannot apparently be carried out explicitly in simple form; it is unstable as 
the amount of overlap between the distributions for the two authors nears zero. 
The words on and to show much the same behavior as by. The Poisson methods 
are seriously wrong; the negative binomial methods seem to perform well. Pre-
dictions based on sets 11, 21, 22, 31 of underlying constants all perform almost 
identically, and better than those from set 38, the latter being based on esti-
mated rates closer together than the others. The performance of on is equivalent 
to a constant predictive probability of about .89, to to a probability of .72. 
Table 4.9-4 shows, for 8 high-frequency words, the observed and expected 
penalties for a Poisson and a negative binomial family with {3 = 11. 
The 
Poisson predictions are slightly better than those of the negative binomial for 
three of these words, but the general behavior is similar to that already observed 
for by, on, and to. 
Initial odds that are even, 'Y = 1, are appropriate for the test sequence by 
its construction. To see what the effect of using wrong initial odds would be, 
we carried out much of the analysis already described for initial odds of .5, 
1.2, and 2.0. The observed penalties increase, and the increases are much the same 
for all methods, somewhat less for the Poisson than for the negative binomial 
methods. 

4.9] 
A LOGARITHMIC PENALTY STUDY 
189 
To illustrate the magnitudes of the changes, the observed penalties for initial 
odds 'Y = .5, 1.0, 1.2, 2.0 are given below for a few words, all for /3 = 22: 
'Y 
.5 
1.0 
1.2 
2.0 
by 
(a = 1) 
54.02 
51.75 
52.32 
54.89 
by 
(a = 2) 
50.12 
46.10 
46.27 
49.38 
on 
(a = 2) 
35.27 
32.89 
33.14 
35.75 
to 
(a = 2) 
61.24 
56.26 
56.48 
60.48 
All these studies of observed and expected penalties lend support to the nega-
tive binomial model as we have used it. On the other hand, we have further 
evidence that the Poisson model is seriously wrong for some words. 
4.9E. The Federalist application: adjusted log odds. Let us examine the 
possibility of modifying the posterior log odds by an adjustment factor designed 
to reduce our penalties. If Aj(a, /3, 'Y, 1) denotes the final log odds for the jth 
test paper using a prediction method (a, /3, 'Y, 1), define the log odds for predic-
tion method (a, /3, 'Y, p) by 
Aj(a, /3, 'Y, p) = pAj(a, /3, 'Y, 1). 
The predicted probabilities are then, as always, 
ii(a,{3,"(,p) 
Qlj(a, /3, 'Y, p) = 1 + ii(a,{3,"(,p) = 1 -
Q2j(a, /3, 'Y, p). 
In Section 3.7 we used a multiplicative adjustment to make approximate 
allowance for correlations between words, for the modal approximation, etc. 
Here, we do not consider what the reason for the adjustment is, but rather what 
is its effect on the predictions. We suspect that many methods may predict too 
strongly. Does a multiplicative adjustment of the log odds improve the pre-
dictions? What is the best adjustment factor? 
We have explored the effects of an adjustment factor p on the log odds ob-
tained from the Poisson and from the negative binomial families, each for 
/3 = 22, and for the words by, on, and to. The resulting behavior of the total 
observed penalty is striking-perhaps too much so. 
For the negative binomial method (2,22,1, p), the total penalty is minimized 
over p by a factor very near 1 (1.01 for by; 1.07 for on; 1.02 for to). The minimum 
penalties achieved are negligibly less (.05 or less) than the total penalties for the 
predictions based on the unadjusted log odds. This result again contributes to 
our confidence in the negative binomial model predictions, because even with 
this new degree of freedom, the adjustments are small, and the gains are 
negligible. 

190 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.9 
For the Poisson method (1,22, 1, p), the results are much different. The 
factors minimizing the total penalty are .55 for by, .55 for on, .53 for to. The 
reduction in total penalty is substantial and the minima achieved are nearly as 
low as for the negative binomial: 
by 
on 
to 
MINIMIZED TOTAL PENALTY 
Poisson 
Negative binomial 
46.82 
33.91 
56.67 
46.10 
32.84 
56.25 
We do not conclude that a Poisson model with each log odds halved is as 
good as a negative binomial model, for the penalty function is sensitive to strong 
predictions in the wrong direction, and the minimizing adjustment factor is 
determined primarily by these predictions, and may be unsatisfactory for ad-
justing weaker predictions. However, a glance at the last four columns of Table 
3.5-2A, where the negative binomial and Poisson log odds for by and on are given 
for a few papers, will show that halving the Poisson log odds does indeed give 
a remarkably close approximation to the negative binomial log odds over the 
entire range of predictions. 
The corresponding comparisons for whilst in 
Table 3.5-2C show that a constant adjustment factor (.5 or other) will not 
always work. There, no adjustment is needed until the log odds is more negative 
than about -1.5 (corresponding to 2 or more occurrences). 
That the negative binomial adjustment factors for by, on, and to are all so 
near to 1, and all the Poisson fac~ors so near to .5, seems coincidental. The words 
by, on, and to were chosen as high-frequency words with sizable non-Poissonness 
parameters. From Table 4.9-4, we see that some other words show much differ-
ent relationships between Poisson and negative binomial log odds. 
In the family of prediction methods generated by the multiplicative adjust-
ment of log odds from a single method, the minimum observed penalty is 
achieved for that factor for which the expected penalty and observed penalty 
coincide. This property holds only for a multiplicative adjustment of log odds, 
and the derivation is straightforward. 
A brief indication of how the minimization was carried out is in order. We 
evaluated the total penalty U(e/» for e/> = (a, 22, l,p) for a = 1 and 2, and for 
12p = 0, 4, 6, 8, 10, 12, 15, 24, 40. P = 0 corresponds to ignoring the data. 
As a function of p, U is well approximated by a cubic polynomial for 0 :::; p :::; 2, 
and the location and value of the minimum was obtained from the cubic. The 
determination of the location was checked by finding the zero of U(e/» -
H(e/> I e/», 
the difference between observed and expected penalties. 
4.9F. The choice of penalty function. Let us explore the basis of the log-
arithmic penalty used in the preceding sections. We revert to the general formu-
lation of Section 4.9A. 

4.9] 
A LOGARITHMIC PENALTY STUDY 
Given a method l/> that yields a vector 
qj(l/» = (Q1j(l/», ... ,qkj(l/»), 
j = 1, ... ,J, 
191 
of predicted probabilities for the outcomes {OJ, j = 1, ... ,J} of a sequence of 
trials, how can we evaluate the methods if the outcomes are observed? One 
natural type of evaluation would introduce a loss function (in units of utility), 
depending on the vector of predicted probabilities and the outcome, and use 
the sum of the observed losses over the J trials as a measure of the inadequacy 
of the method. The simplest class of loss functions would depend solely on the 
probability predicted for the outcome that obtains. We call such a loss function 
a penalty function. If f is the penalty function, the total loss or penalty is 
J L: f(qOi,j(l/») , 
j=l 
where the argument of f for the jth trial is the probability q8i,j(l/» predicted for 
the outcome of the jth trial. 
We may restrict attention to a single trial and temporarily drop the trial 
index j and the method index l/>. We then have a vector q = (q1, ... , qk) of 
predicted probabilities for the outcome 6 of the trial. A natural requirement 
suffices, for k ~ 3, to determine f uniquely (except for the translation and scale 
change available for any utility). This requirement is that the penalty function 
should encourage the prediction of the correct probabilities if they are known. 
Suppose that the vector of correct probabilities, on the given data, is 7r = 
(7r1' ... ,7rk)' If q is the vector of predictions, the expected penalty, which we 
denote by H(q I 7r), is 
H(q I 7r) = L:7ri!(qi). 
i 
We want this expected penalty to be minimized over all predictions q by q = 7r. 
Specifically, we require 
(4) 
with equality if and only if q = 7r. 
For Ie ~ 3, the only penalty function satisfying the requirement of inequality 
(4) is the function 
f(x) -
a -
b log x, 
b > o. 
Taking f(x) = -log x, the penalty is 0 for a prediction of 1.0 and increases 
monotonically and unboundedly as the probability predicted for the event that 
obtains decreases to O. H(7r I 7r) is the Shannon information for the vector 7r 
of probabilities. The use of this logarithmic penalty for assessing probability 
predictions is not new. (ef. Good, 1952, p. 112.) 
A proof of the uniqueness of the log function is given under the assumption 
that f is differentiable. The necessary conditions that H(q I 7r) have a stationary 

192 
THEORETICAL BASIS OF THE MAIN STUDY 
minimum in q at 7r are given by the k -
1 partial derivative equations: 
i = 1, ... , k -
1. 
If k ~ 3 and these equations are to hold for all 7r, then 
7rd'(7ri) = constant 
so that 
[4.9 
That a minimum is achieved if b > 0, a maximum if b < 0, is easily verified. 
If k = 2, many functions satisfy inequality (4). If g is any function, negative 
and symmetrical on the unit interval, the function 
f(x) = J x- 1g(x) dx 
satisfies the inequality (4). 
Examples of choices are functions fa based on 
g(x) = 
-
[x(1 -
x)]a: 
fo(x) = -log x, 
hex) = HI -
X)2, 
f-1(X) = x-1 -
log (1 ~ x)' 
Of these functions, some penalize extremely small predicted probabilities (when 
the event occurs) much more or less heavily than does the logarithm. Thus h 
is a bounded penalty function, while i-1 is unbounded at ° and 1, rewarding 
high probabilities unboundedly and thus balancing the heavier penalties for 
small probabilities. 
For our work, we choose the penalty function -log x, because of its general 
applicability, because it arises naturally in some other approaches to be pre-
sented, and because the alternative choices offer no evident advantages. In 
particular, a bounded penalty function such as h appears unsatisfactory in an 
application in which the accuracy of very small predicted probabilities is of 
great interest. 
One minor felicity of the logarithmic penalty function for k = 2 is that the 
penalty for an occurrence of an outcome with a small predicted probability is 
nearly the same as the negative of the log odds. Thus when the penalties are 
large, they are nearly equal in magnitude to the log odds that play so large a 
role in the main study. The result is immediate, for if q1 is small, 
4.9G. 
An approxirrtate likelihood interpretation. 
In the penalty-
study application, we have interpreted the total observed penalty as a log likeli-
hood for the purpose of comparing various prediction methods. In this section 
we develop an approximate theoretical basis for this interpretation. We first 

4.9] 
A LOGARITHMIC PENALTY STUDY 
193 
show how the observed penalty is a log likelihood for a simple multinomial prob-
lem, then indicate how this interpretation is still plausible for our use of the 
penalty function. 
Suppose that the random variables {OJ; j = 1, ... , J} represent the outcomes 
of n independent multinomial trials with probabilities depending on a param-
eter ep: 
j = 1, ... ,J. 
If the outcomes of the trials are observed, what can be inferred about ep? 
To facilitate discussion of the inference problem, introduce an unobservable 
random variable 'J> whose value is the parameter that determines the multi-
nomial random variables. Suppose 'J> has a prior density p(ep) , which we will 
not specify. The formal likelihood or posterior analysis is standard. The log 
posterior density of 'J>, given the observed outcomes OJ = OJ, is: 
k 
log p(ep I 01, ... , OJ) = C + log peep) + L: L: log qij(ep). 
i= 1 Ii :8j=i} 
The final term is just the log likelihood for these independent multinomial trials. 
But this log likelihood is exactly the negative' of the total observed penalty U(ep) 
based on the logarithmic penalty. This provides another strong motivation for 
the use of the logarithmic penalty function for all values of k. 
If the parameter 'J> is of low dimension, if J is large, and if the probabilities 
qij(ep) are the same for each trial (I.e., do not depend on j) then the problem of 
inference about 'J> is one treated by many authors, usually by asymptotic methods 
based on x2. Our main interest is in problems not satisfying these conditions, so 
that the asymptotic methods are inapplicable. 
Formally, the range of the parameter 'J> may be quite general. For example, 
the parameter 'J> might specify not only the low-dimensional argument of the 
parametric functions qij, but also which of the possible functions qij are used. 
Unless the range of 'J> is severely restricted, inferences about the true value of 'J> 
will depend strongly on its prior density as well as on the likelihood function. 
Nevertheless, the differences U(ep) -
U(ep') between two observed total penalties 
can be interpreted, under the assumptions here, as a difference in log likelihood. 
A small difference means that the trials provide little evidence for discrimination 
between the two values, ep and ep'. 
The prediction structure formulated in Section 4.9A is more complex than this 
multinomial model. We observe, in addition to the outcome OJ of the jth trial, 
the value of a random variable 1\ whose distribution depends on the value of OJ 
and on the value of a parameter 'J> that indexes models and either their param-
eters or prior distributions for handling their parameters. Dependence among 
the trials in the test sequence provides an additional trouble. 
Information for inference about 'J> comes from the observed {OJ} and {Dj }. 
~Nomodel ep that we study is exactly valid. We are primarily concerned with 
thoseaspe~tsQf the model that affect the approximate validity of the 

194 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.9 
probabilities of authorship predicted conditionally on the data and model 
(i.e., PCej = i I Dj, ¢»). The notion of separating the evidence concerning 4> into 
two parts, of which one represents the evidence determined by the predictions, 
seems highly attractive. No unique separation exists, but one natural choice 
leads back to the observed total penalty. 
We assume that 8j has prior distribution P.(Oj) independent of the model 4>. 
We use the subscript s for temporary convenience to index which of several prior 
distributions is being used. If p(Dj I OJ, ¢) is the density function of the ob-
served datum Dj, given 8j = OJ and 4> = ¢, then Bayes' theorem gives as the 
odds factor between the prior density of 4> and the posterior density of 4> based 
on the observed OJ and D j for thejth paper: 
The prior density of OJ may be absorbed in the constant. Thus the odds factor 
is, up to a normalizing constant, just the likelihood p(Dj I OJ, ¢) of the datum 
Dj, considered, for the observed D j and OJ, as a function of ¢. 
The same odds factor of 4> can be written in the alternative form 
p(¢ I OJ, Dj) 
C 
(D I ) (0 I D 
) 
p(¢) 
= 
.,jP8 
j 
¢ P. 
j 
j, cJ> • 
Here, apart from a normalizing constant, we have factored the odds factor into 
a product of two parts. The first, P8(Dj I ¢), is the likelihood of 4> from observing 
Dj, but not OJ; the second, P.(Oj I Dj, cJ», is the conditional likelihood of ¢ from 
observing OJ given D j • The second part represents the evidence about the model 
parameter 4> that is reflected in the authorship predictions, and is exactly the 
contribution of the jth trial to the likelihood in the interpretation at the begin-
ning of this section. The first part, on the other hand, represents evidence about 
¢ that does not directly affect the prediction, and might be regarded as evidence 
with respect to aspects of the model of lesser interest. The factorization is not 
unique but is determined by the choice of P8(Oj). Often, some single choice may 
be natural, and the effect on the component factors of varying P8(Oj) near this 
choice may be small. 
If, as in the Federalist application, the test sequence contains J /2 papers with 
outcome 8 = 1, J /2 with outcome 8 = 2, the use of even prior odds on a single 
trial is ideal and has been used in most of the application. Jointly, the construc-
tion of the sequence suggests as the ideal prior distribution for (81, ... ,8J), 
equal probability on any set of outcomes with J /2 papers by each author. For 
reasons discussed in Section 4.9B, we did not use this choice. Instead we used, 
as a good approximation to it, independent prior distributions for the trial 
outcomes, with the ideal marginals. But given this independence assumption 
and the usual assumption on independence of the data from trial to trial, the 

4.10] 
TECHNIQUES IN THE FINAL CHOICE OF WORDS 
195 
logarithm of the odds factor from prior to posterior density of?> can be written 
p(cf> I §., D) 
~ 
~ 
log 
() 
= C + L..J log P8(Dj I cf» + L..J log P8(Oj I Dj, cf», 
P cf> 
j 
j 
where 0 denotes (Ob ... , OJ) and D denotes (Db . .. ,DJ). The last two sums 
-
-
may be interpreted as log likelihoods of two parts of the evidence about?>. We 
have not explored or used the first part. The final sum is the log likelihood of 
the evidence directly relevant to the prediction of authorship. This is exactly 
the log likelihood in the simple formulation at the beginning of this section 
(4.9G) and coincides with the total observed penalty U(cf» in our application. 
We base our likelihood interpretation on this approximate representation. 
4.10. TECHNIQUES IN THE FINAL CHOICE OF WORDS 
4.10A. Systematic variation in Madison's writing. In the main study, 
we handle the systematic variation between Madison's rates of word use in 
papers from The Federalist and in papers formed from the Neutral Trade essay 
by eliminating words that show serious variations. We describe here the pro-
cedures used to explore the effects of the variation on log odds. We estimate the 
adjustments in log odds that a complete model and study might yield, but we 
use these estimated adjustments only to choose words not needing much adjust-
ment. We use approximations freely, much more so than would be appropriate 
were we to use the actual adjustments. The procedures and approximations were 
chosen to allow hand computation from information already available from the 
main posterior analysis and log odds computation. 
Assume that Madison's writings can be divided into two parts, each of which 
behaves homogeneously according to the negative binomial model, but with 
possibly different parameters. We treat these parts as two new "authors," and 
number them 3 and 4, with parameters f.L3 and 03, f.L4 and 04. We sometimes 
call author 3 "Early Madison" and author 4 "Late Madison," but these are 
not meant literally. We assume that the 14 Federalist papers are a random 
sample of papers written by author 3 and that the 20 Neutral Trade papers are 
a random sample from the papers by author 4. The remaining 16 papers are 
not individually allocated, but we assume that the entire 50 papers are a random 
sample from a mixture with known weights p, 1 -
p of the papers of author 3 
and author 4. We use p = .45, and discuss the choice later. 
If we are trying to discriminate between Hamilton and Madison as the author 
of a disputed paper from The Federalist, the most plausible Madison authorship 
is that of author 3 ("Early Madison"), not author 4 ("Late Madison"), or the 
Composite author, 2. Introducing self-explanatory notation, we wish to obtain 
the log odds Al13, not the log odds A = A1I2 that we actually get in the main 

196 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.10 
TABLE 4.10-1 
AVERAGE ESTIMATED ADJUSTMENTS A213 FOR A 2000-WORD PAPER 
Word 
Xl 
E(A2i3 11) 
X2 
E(A213 I 2) 
number 
B3A 
60 
3.79 
-.04 
-4.62 
.00 
B3Bt 
4 
.36 
.07 
-
AD 
.00 
8* 
.24 
-.34 
-
042 
-.22 
13 
.90 
.04 
-1.24 
.00 
37* 
.26 
-.25 
-
.22 
-.17 
39 
.47 
-.07 
-
.38 
.00 
40 
2.00 
-.38 
-2.70 
-.02 
54* 
.43 
-AD 
-
.36 
-.26 
55 
1.13 
.19 
-1.80 
.00 
57* 
.33 
-.10 
-
.23 
-.01 
58 
.38 
Al 
-
.68 
-.06 
B3Gt 
73 
.22 
-.02 
-
Al 
-.00 
77* 
.18 
.10 
-
.52 
-.17 
117 
.68 
-.05 
-
.77 
.00 
135* 
.24 
-.07 
-
045 
-.02 
160 
.24 
-.08 
-
.19 
-;01 
B3Et 
81 
.64 
-.12 
-
.80 
-.02 
119 
046 
-.03 
-
.59 
.00 
124 
.53 
-.02 
-
.31 
.00 
145* \ 
.35 
-,06 
-
.20 
.00 
B3Zt 
96 
,12 
.02 
-
.24 
.00 
129* 
.13 
-.02 
-
.07 
.00 
139* 
043 
-.21 
-
.37 
-.10 
143 
.65 
.08 
-
.78 
.00 
144* 
.20 
-.04 
-
.16 
-.01 
151 
.25 
-.03 
-
.30 
-.00 
155* 
.35 
-.06 
-
.20 
-.01 
* Words marked by an asterisk have been eliminated from the final words, largely 
on the basis of the present adjustments. 
t Some words for which both average adjustments are negligible have been omitted: 
B3B: 3; B3G: 78,90,116, 123; B3E: 80,82; B3Z:87,94, 110, 146, 153, 165. 

4.10] 
TECHNIQUES IN THE FINAL CHOICE OF worws 
study, nor A114' But by the definition -of odds, with 8 the true author, 
p(x I 8 = 1) 
p(x I 8 = 1) p(x I 8 = 2) 
p(x I 8 = 3) = p(x I 8 = 2) . p(x I 8 = 3) , 
or, taking logarithms, we have 
197 
Thus for any unknown paper, the adjustment needed to convert Al12 to Al13 is 
exactly A213, which is the log odds for discriminating between the Composite 
Madison and the Early Madison. If for some word, A213 is small, then the 
adjustment needed is small. Moreover, because author 2 is a composite of au-
thors 3 and 4, A213 small implies that A214 is also small. Hence the log odds A1I2 
obtained is nearly appropriate for discriminating Hamilton from Early, Late, 
or Composite Madison. We choose words for which this desirable property 
holds. 
For each of the words in the semifinal list of words that met the importance 
criterion and were not eliminated as too contextual, we estimated the average 
correction A213 needed for a 2000-word paper by Hamilton (author 1), and the 
average for a 2000-word paper by Early Madison (author 3). Denote these 
averages by E(A2I311) and E(A21312). For the semifinal words, Table 4.10-1 
shows these two average adjustments along with an observed mean log odds 
A1I2 for both Hamilton and Madison papers. Denote these means by Xl, X2 • 
(They are not quite the simple means of all 48 and 50 papers, respectively.) 
The methods used to obtain the average log odds are given in Section 4.1OB. 
Both the direction and magnitude of the expected adjustments are relevant 
for choosing words. The average adjustment for a paper by author 3 must be 
negative since a paper by this author is, on the average, more like one written 
by 3 than by 2. The big adjustments are expected for Hamilton papers, for then 
the average rate is far from the mean rates for author 2 or 3, and small changes 
for mean rates from 2 to 3 are important. A negative shift occurs if the mean 
rates for the four authors are in positions A or B in Fig. 4.10-1, so that Early 
Madison is more like Hamilton than is Composite or Late Madison, and the 
word will not be as effective a discriminator between Hamilton and Early 
A 
)( 
)( 
)( 
)( 
.. 
Ji.[ 
1'-2 
Ji.4 
Rate 
B 
X X 
1'-3 1'-[ 
)( 
)( .. 
1'-2 
Ji.4 Rate 
C 
X 
)( 
)( 
X 
... 
Ji.[ 
Ji.2 
Ji.3 
Rate 
FIG. 4.10-1. Possible configurations of mean rates for Hamilton (/Jl); Composite 
Madison (Jd; Early Madison (/J3); Late Madison (/J4). 

198 
THEORETICAL BASIS OF THE MAIN STUDY 
[4.10 
Madison as A1I2 indicates. Indeed, if arrangement B of Fig. 4.10-1 obtains, 
even the direction of the discrimination is different between 1 versus 2 and 1 
versus 3. 
A positive shift, such as E(A21311) = .41 for to (No. 58), arises with an 
arrangement like C when the Early Madison rate is further from the Hamilton 
rate than the Composite Madison rate is. Thus, to is even a stronger discrimi-
nator than indicated for Hamilton versus Early Madison. 
We decided to retain to, and we here note what it did on the disputed papers. 
The observed mean rates for to in various groups of papers are: 
48 Hamilton papers: 
50 Madison papers: 
14 Madison Federalist papers: 
20 Madison Neutral Trade papers: 
12 disputed Federalist papers: 
40.71, 
35.25, 
32.46, 
36.00, 
31.36. 
The 12 disputed papers do seem to behave .as Madison Federalist papers, and 
the evidence from to is stronger than that actually used. 
Of the words retained, Nos. 160 and 81 are most suspect. 
4.10B. Theory. If we assume that the negative binomial distribution holds 
for papers by authors 3,4, and by the Composite, author 2, as well as by author 1, 
and if we know the parameters, the computation for an unknown paper of log 
odds between any two authors is straightforward. The calculation of expected 
negative binomial log odds is exceedingly difficult, as has been discussed in 
Section 4.8. Here we make some critical simplifying assumptions. We assume 
that K2 ~ K3 ~ K*, say, so that for x occurrences of a word in a 2000-word 
paper, 
( /l2) 
( 
*) 1 
(K* + 2/l3) 
A213 = x log /l3 + X + K 
og (K* + 2/l2) , 
which can be obtained from Eq. (2) of Section 4.4B. Then the expected value 
IS: 
The assumption K2 = K3 will be good when /l2 ~ /l3, so it should introduce 
substantial error only when the adjustments are large and the words are 
eliminated. 
The negative binomial assumption cannot hold exactly for a mixture and for 
the components of the mixture. 
Nevertheless, it should be adequate for the 
rough computations of expected log odds, and for the matching of moments to 
be described shortly. 
We need values of IJ-l' IJ-2, IJ-3, K* to form the desired expectations. From the 
main posterior analysis, we get estimates of IJ-1, IJ-2, 01, 02. (We use estimates 

4.10] 
TECHNIQUES IN THE FINAL CHOICE OF WORDS 
199 
based on set 31 of underlying constants.) From the mixture definition of Com-
posite author 2, 
E(x 12) = pE(x 13) + (1 -
p)E(x 14), 
E(x 2 1 2) = pE(x2 13) + (1 -
p)E(x 2 14). 
If we assume 03 = 04, then from equating the first two moments, two condi-
tions are obtained: 
M3 = M2 + (1 -
P)(M3 -
M4), 
03 = 02 _ P (1 -
P)(M3 -
M4)2 . 
M2 
Thus, the desired parameters M3, 03 can be determined from the available 02, M2 
if p and (M3 -
M4) are found. Once 03 is available, we choose 
* 
(M2 
M3) 
K = max 
K2 = 02' 
K3 = ~ . 
How should the mixture proportions p, 1 -
p be assessed? The 34 papers 
clearly allocated to authors 3 and 4 split 14 to 20, suggesting p = .41; the 
paper lengths split 37,000 to 40,000 suggesting p = .48. We use an average 
of these, p = .45. Later study indicated that the usage in the remaining 16 
papers was closer to that in The Federalist than to that in the Neutral Trade, 
so that a larger value of p might have been better. 
The estimation of M3 -
M4 is the critical problem, for this measures the dif-
ference between Early and Late Madison. The evidence for M3 and M4 comes 
from the data on the 14 Federalist papers and the 20 Neutral Trade papers. 
Prior information is not negligible here: some of the apparent differences between 
the two sets of papers may be ascribed to chance just as were some differences 
between Hamilton and Madison. 
Our estimation procedure for M3 -
M4 is 
similar to that used in Section 4.2G to get preliminary estimates of T. Further 
approximations eased the computations, but we omit the details. The mean 
rates in the 14 and 20 papers by authors 3 and 4 are used as data estimates. 
Prior information is based on a beta prior density, with arguments (12, 12) for 
M3/(M3 + M4), the parameter analogous to T. 

CHAPTER 5 
Weight-Rate Analysis 
5.1.. THE STUDY, ITS STRENGTHS AND WEAKNESSES 
While our main study is Bayesian in character, we want also to see how a more 
traditional approach handles problems of discrimination. Some readers may 
prefer such an analysis, and many will wish to compare the results achieved by 
the two methods. 
The plan is to construct a linear discriminant function, a weighted sum of the 
rates for words. We use about half the data for each author, called the screening 
set, to select the discriminating words and to get weights to apply to their ob-
served rates. Once the words are chosen and weighted, the function is tried out 
on the other half of the material of known authorship, not only as a test of the 
chosen function, but also to calibrate the weighted sum on material uncontami-
nated by the effects of selection and weighting. Finally, the discriminant function 
is applied to the disputed papers, and the values obtained are compared with 
those obtained from the calibrating set for each author by methods that are 
described later. 
Why do we need a calibrating set, why not use all the data for selection and 
weighting? First, when for some purpose we choose the best looking few from 
a large set of variables, those chosen may not perform as well as they appeared 
to in the material that was used to select them-this loss is called the regression 
effect. Second, usually weights must be chosen to optimize results if new data 
are very like those in the material used for weighting. When the new data are 
not like the old, there is an additional slump. Third, we do not know what the 
selectivity may have done to the relations among the chosen variables, and we 
need estimates of location and variability that cannot be obtained from data 
used for selection and weighting. 
Linear discrimination methods ordinarily take advantage of the covariance 
structure of the variables, but in our problem the correlations seem low. Con-
sequently, we have treated the variables initially as if they were independent, 
and thus no advantage has been taken of covariances in the weighting of the 
variables. Therefore, the analysis may be weaker than could be achieved by 
traditional linear discrimination. On the other hand, the independence assump-
tion for words is not part of the final analysis. Our method adjusts automatically 
200 

5.2] 
MATERIALS AND TECHNIQUES 
201 
for correlations by use of the calibrating set of papers. The reader may recall 
that in the Bayesian analysis the independence assumption matters in the 
ultimate interpretation of the odds, and there we give a special deflation study 
to take account of correlation. 
An analysis of rates is the~asis for this study, and rates, themselves, lead to 
one weakness that we have done nothing about. Low-frequency words often 
have zero frequencies and therefore zero rates in the papers. A zero rate for a 
lOOO-word paper is quite different information from a zero rate for a 3000-word 
paper, and a proper analysis should take this into account. In the Bayesian 
studies, both the main one and the robust one to be reported later, account is 
taken of the effect of paper length on low-frequency words, but we have not 
done that here. 
A major weakness of this study is that it does not have the strong protection 
against contextual words provided for the main study. Partly this is a lapse in 
our work, but partly it comes from the limited amount of material available 
when we use a calibrating set as well as a screening set of papers. More could 
have been done than we did to reduce contextual dangers, but we could not do 
as much as we did in the main study and still keep the process of calibration 
insulated from regression effects. 
5.2. 
MATERIALS AND TECHNIQUES 
As a pooi of words for this study, we use those of the screening study (Section 
2.5B) and the two sets of function words (Section 2.5A), in all, the 117 words 
with code numbers 1 through 117 (see Tables 2.5-2 and 2.5-3). 
As a screening set, we use the 23 Hamilton and 25 Madison papers listed in 
Table 5.3-2 to choose words and find weights for them. N ext, these weights 
are applied to the remaining papers of known authorship, which form a cali-
brating set of 25 Hamilton and 25 Madison papers uncontaminated by the 
process of selection and weighting. Finally the weights are applied to the rates 
for the disputed and joint papers and these are assessed. 
Because we use words selected by the screening study as well as function 
words, the papers from waves A, B, and C and the Exterior set are already 
contaminated by the selection process and might as well be used for the weight-
ing. This explains why we use 23 Hamilton screening study papers as the Ham-
ilton screening set (see Sections 1.4 and 2.5B). To the 19 Madison papers of 
the original screening study we added 6 more, five from the long Neutral Trade 
paper, and one on the Constitution of Virginia, to form the Madison screening 
set. We chose 25 for the number of the Madison papers because that gave 
nearly equal numbers of papers for the two authors in the screening set, and 
equal numbers of papers in the calibrating set where equality is most convenient. 
(We have no theory to regulate the relative sizes of the screening and calibrating 
sets.) 

202 
WEIGHT-RATE ANALYSIS 
[5.2 
Let Xi be the rate of use of word i and let Wi be a weight* for that word. 
Then a linear discriminant function has the form 
y = LWih 
The weights are chosen so that when Hamilton is the author and the rates 
apply to him, the value of y tends to be large (say), and when Madison is the 
author, the value small. In addition to assigning direction, the weights give 
better discriminating words more importance. 
For word i, let the mean rate and variance of its rate from paper to paper be 
J.Lij and uti> where j stands for H(Hamilton) or M(Madison) whichever is the 
author. Assuming independence of words, the mean and variance of the dis-
criminant are 
E(y I j) = LWiJ.Lij, 
u2(y I j) = LW7u7j, 
where the summations are taken across words. 
j= H,M, 
j= H,M, 
One measure of discrimination that can be Inade large by the choice of Wi is 
E(y I H) -, E(y I M) 
vi u2(Y I H) + u2(Y I M) 
Then the ideal weight for each word is proportional to the difference between, 
the Hamilton and Madison rates and inversely proportional to the sum of the 
variances. That is, the weight for the ith word is proportional to 
J.LiH -
J.LiM 
U7H + U7M 
Since we do not know the theoretical means and variances, we need to estiInate 
them, or something proportional to them. 
For the J.L'S, we use median rates in the screening set for each author unless 
one or both of the medians is zero, and then average rate for each author is used. 
To replace the variances we used squares of ranges (range is the largest rate 
minus the smallest rate). 
Our reasons for using ranges were to cut down on total amount of calculation 
and to defend against contextual words. Recall that ranges are somewhat more 
affected by outliers than standard deviations, and that the more contextual the 
word is, the more wildly its rates vary. Thus for the weight chosen, dropping 
the subscript i, we have 
W= 
R~+ R~ 
* This is not paper length. 

5.3] 
RESULTS FOR THE SCREENING AND CALIBRATING SETS 
203 
where R stands for range and x stands for median or average as appropriate. 
We did not bother to adjust for the variation from 23 to 25 in sample size for 
the ranges of the screening sets. 
Naturally we preferred not to put the entire 117 words into the discriminant, 
because 90 are unselected and offer little hope of helping. Suppose one rate is 
exactly at Hamilton's average and another at Madison's, then the difference in 
score, WXH -
WXM = W(XH -
XM), is one measure of the importance of the 
variable, for it gives the contribution of the particular word to the difference in 
score between an average Hamilton paper and an average Madison paper. 
To get a notion of magnitude of this measure of importance, recall that the 
expected range in normally distributed samples of 25 is about 40'. Suppose, for 
a given word, that O'H = O'M = 0' and that J1.H -
J1.M = kO'. When we sub-
stitute 40' for Rand kO' for XH -
XM, we find approximately the importance is 
k2/32. We chose an importance of .025 as our cutoff and only the 20 words with 
importance values higher than that were used. In absolute value, the minimum 
estimated k for admission to the discriminant is slightly less than .9. If all 
parameters were known, and if merely a two-decision problem were being 
handled with the real line split midway between J1.H and J1.M, at least two times 
out of three a correct decision would be made with k 2:: .9. Indeed, the two-to-one 
odds were used by us to choose the cutoff value. What with sampling errors, 
regression effects, and non-normality, we are not so dreamy-eyed that we ex-
pect nearly this much from the worst word retained. Nevertheless, this was the 
way the cutoff was chosen. 
5.3. RESULTS FOR THE SCREENING AND CALIBRATING SETS* 
The 20 words surviving the screening are displayed in Table 5.3-1 together 
with their weights and importances as estimated by W(XH -
XM). 
As one 
might expect, upon is the most important word by a factor of four. Other 
leaders are whilst, there, and on. 
In Table 5.3-1 the words are grouped because we had used the index as a 
source to help us classify words as to their degree of apparent danger from con-
textuality. Thus all the 117 words in the pool fall into 6 classes-group 1 for 
upon because it is so outstanding and because it cannot easily be classified either 
as a high- or as a low-frequency word. More contextual word groups were 
assigned higher numbers. For convenience of exposition, we take up the rest 
out of numerical order. 
Group 4 consists of function words from the Miller-Newman-Friedman list 
(see Section 2.5A) which were personal pronouns or auxiliary verbs. 
Group 3 consists of the high-frequency function words (code numbers 1-70, 
excluding pronouns and auxiliaries) and those members of the random sample of 
* The calculations in this part of the study were largely supervised by Charles L. 
Odoroff. 

204 
WEIGHT-RATE ANALYSIS 
[5.3 
TABLE 5.3-1 
WEIGHT-RATE ANALYSIS 
WORDS, WEIGHTS, AND IMPORTANCES (TIMES 104) 
Weight 
Importance 
Group 1 
upon 
1394 
3847 
Group 2 
although 
-1754 
0351 
commonly 
1333 
0267 
consequently 
-1311 
0459 
considerable 
0784 
0251 
enough 
0683 
0403 
while 
2708 
0704 
whilst 
-2206 
0993 
Group 3 
as 
-0140 
0339 
at 
0247 
0318 
by 
-0146 
0542 
of 
0037 
0281 
on 
-0271 
0796 
there 
0463 
0972 
Group 4 
would 
0085 
0428 
Group 5 
innovation 
-1681 
0336 
language 
-1448 
0304 
vigor 
2174 
Q543 
voice 
-2159 
0410 
Group 6 
destruction 
1709 
0342 
low-frequency words that did not emerge successfully from the low-frequency 
screening study. Aside from upon, the words obtained in the screening study 
were placed in groups 2,5, or 6. Group 2 includes those that are function words 
and those on a list of words that we called "well-liked" words. From the original 
index, without any evidence of differential use by Hamilton and Madison, we 
made two lists of words: the "well-liked" list containing mostly abstract adverbs 
that we assessed as largely free from contextuality, and a larger list of more 
meaningful words that did not seem so contextual that we should delete them. 
The latter words fall into group 5. Group 6 includes what is left; i.e., words we 
regarded as possibly highly contextual. 

5.3] 
RESULTS FOR THE SCREENING AND CALIBRATING SETS 
205 
TABLE 5.3-2 
WEIGHT-RATE ANALYSIS 
RESULTS FOll, SCHEENING SET FOH HAMILTON 
Group 
Paper 
1 
2 
3 
4 
5 
6 
Total 
number 
1 
.5325 
.1290 
.0529 
.0108 
.1391 
0 
.8643 
6 
.2941 
.1193 
.2902 
.0269 
0 
.0906 
.8211 
7 
.6831 
.0600 
.1569 
.1931 
0 
0 
1.0931 
8 
.2105 
.1748 
.1007 
.1198 
.2196 
.0854 
.9108 
9 
.3429 
.2547 
.1667 
.0209 
0 
0 
.7852 
11 
.3346 
.0627 
.2323 
.1703 
.1739 
0 
.9738 
12 
.4558 
.1419 
.1848 
.0874 
.0341 
0 
.9040 
24 
.5409 
.1489 
.0933 
.1177 
0 
.0940 
.9948 
26 
.3513 
.2337 
.1926 
.0572 
-.0907 
.0718 
.8159 
36 
.3081 
.0493 
.2395 
.0156 
.0804 
0 
.6929 
59 
.2258 
.0423 
.1650 
.0736 
0 
.1846 
.6913 
65 
.6928 
.2088 
.1152 
.1056 
0 
0 
1.1224 
66 
.6928 
.2126 
.0870 
.0383 
0 
0 
1.0307 
70 
.2760 
.3420 
.2321 
.0309 
.1435 
.0564 
1.0809 
71 
.2453 
.5753 
.0325 
.0799 
.0291 
0 
.9621 
72 
.3429 
.4091 
.2537 
.1046 
0 
0 
1.1103 
74 
.3081 
0 
.2425 
.0844 
0 
0 
.6350 
77 
.7082 
.2129 
.1246 
.1380 
0 
0 
1.1837 
101 
.2342 
.1120 
.1879 
.0071 
.1826 
0 
.7238 
102 
.5046 
.1950 
.1284 
.0184 
.1565 
.1230 
1.1259 
111 
.2844 
.0475 
.1929 
.0289 
0 
0 
.5537 
112 
.5646 
.0294 
-.0539 
.0172 
0 
0 
.5573 
113 
.1185 
.0666 
.0021 
.0649 
0 
.1453 
.3974 
Y 
.4023 
.1664 
.1487 
.0701 
.0464 
.0370 
.8709 
8 
.1772 
.1356 
.0866 
.0529 
.0806 
.0566 
.2145 
(cont.) 
As in the main study, the reason we went to all this trouble was to get some 
quantitative idea ·of what such intuitive classifications might be worth. Here 
we were encouraged to see that groups 4 and 6 were practically annihilated by 
the weights and that groups 2 and 3 had a number of members. But the cali-
brating set is the ultimate test. 
N ext we apply.the weights to the words in each paper in the Hamilton and 
Madison screening sets. In Table 5.3-2 we show the total score for each paper 
in the screening sets, and also the components of that total for each word group. 
The bottom lines of each author's set show the average score and the standard 
deviation of the scores, together with corresponding information for the com-
ponent groups of words. 

206 
WEIGHT-RATE ANALYSIS 
[5.3 
TABLE 5.3-2 (cont.) 
WEIGHT-RATE ANALYSIS 
RESULTS FOR SCREENING SET FOR MADISON 
Group 
Paper 
1 
2 
3 
4 
5 
6 
Total 
number 
10 
0 
-.0433 
-.0966 
.0171 
-.0712 
0 
-.1940 
14 
0 
-.2256 
-.2128 
.0198 
-.3479 
0 
-.7665 
37 
.0516 
-.2435 
-.2339 
.0219 
-.2229 
0 
-.6268 
38 
.1687 
-.2769 
-.1021 
.0385 
-.0434 
0 
-.2152 
39 
0 
0 
-.3385 
.0263 
-.0656 
0 
-.3778 
40 
0 
0 
-.2407 
.0188 
-.2043 
0 
-.4262 
41 
0 
-.0193 
-.2097 
.0218 
-.1245 
0 
-.3317 
42 
.1032 
-.0844 
-.1817 
.0470 
-.1335 
0 
-.2494 
43 
0 
-.1740 
-.3259 
.0337 
-.1267 
0 
-.5929 
44 
0 
-.2232 
-.2510 
.0962 
-.0656 
0 
-.4436 
45 
0 
-.4023 
.0030 
.0442 
-.2029 
0 
-.5580 
46 
0 
-.1699 
-.2808 
.0948 
-.1189 
0 
-.4748 
47 
0 
0 
-.2312 
.0067 
-.0565 
0 
-.2810 
48 
0 
-.1123 
-.2353 
.0109 
0 
0 
-.3367 
121 
0 
-.1463 
-.0104 
.0139 
-.1186 
0 
-.2614 
122 
0 
-.5963 
-.1382 
.0425 
.0783 
0 
-.6137 
131 
0 
-.2683 
-.0586 
.0260 
0 
0 
-.3009 
132 
.0516 
-.2934 
-.1751 
.0220 
0 
0 
-.3949 
133 
0 
-.1966 
-.1647 
.0518 
-.0594 
0 
-.3689 
141 
0 
-.0367 
-.OJ35 
.0575 
-.2150 
0 
-.2077 
211 
0 
-.3482 
-.2952 
.0042 
0 
0 
-.6392 
212 
.1924 
-.1860 
-.2918 
.0196 
-.2213 
0 
-.4871 
213 
0 
-.0757 
-.2840 
.0295 
-.0840 
0 
-.4142 
214 
0 
-.1759 
-.0150 
.0127 
-.0724 
0 
-.2506 
215 
0 
-.1940 
-.1814 
.0125 
-.0824 
0 
-.4453 
Y 
.0227 
-.1797 
-.1826 
.0316 
-.1023 
0 
-.4103 
8 
.0529 
.1412 
.1039 
.0245 
.0297 
0 
.1553 
We expect the frequency distributions of the values of the discriminant 
function for the two screening sets to be well separated-the words and weights 
are chosen to pull these distributions apart. The Hamilton mean is .87, the 
Madison mean -AI, the midpoint .23. Paper 113, Pacificus III, has the lowest 
Hamilton score of 040; paper 10 with -.19 has the highest Madison score. So 
the observed separation between the extremes is about four Madison standard 
deviations, and somewhat less than three Hamilton standard deviations. 
Observe that the score 0 plays no special role in this analysis. If the division is 
made at any value between -.19 and 040, the discriminant function classifies all 
these papers correctly. 

5.3] 
Paper 
number 
13 
15 
16 
17 
21 
22 
23 
25 
27 
28 
29 
30 
31 
32 
33 
34 
35 
60 
61 
67 
68 
69 
73 
75 
76 
ii 
8 
RESULTS FOR THE SCREENING AND CALIBRATING SETS 
TABLE 5.3-3 
WEIGHT-RATE ANALYSIS 
RESULTS FOR CALIBRATING SET FOR HAMILTON 
1 
.2913 
.4530 
.4405 
.5381 
.4196 
.5200 
.5437 
.1408 
.3931 
.2649 
.6398 
.9242 
1.0483 
.1979 
.7165 
.6329 
.5576 
.4977 
.2774 
.5952 
.1868 
.6412 
.7946 
.3666 
.6371 
.5088 
.2247 
2 
.0710 
.0894 
.2895 
.1720 
.4089 
.1032 
o 
.0392 
.1503 
o 
.2852 
.0680 
.0773 
-.1245 
o 
.1572 
.1186 
.0353 
o 
o 
.1051 
.1633 
.0690 
.7538 
.3198 
.1341 
.1755 
Group 
3 
.3989 
.3893 
.2181 
.3415 
.1412 
.2306 
.0553 
-.0668 
.2365 
.3673 
.3941 
.0313 
.1363 
.1222 
-.0492 
.1909 
.0591 
.1539 
.2675 
.1109 
.0099 
.2935 
.1197 
.0696 
.1051 
.1731 
.1375 
4 
.1242 
.0360 
.1612 
.0657 
.0513 
.0488 
.0190 
.0901 
.0180 
.0646 
.0741 
.0953 
.0246 
.1384 
.0819 
.0772 
.0604 
.1063 
.0961 
.0303 
.0397 
.0879 
.1080 
.1205 
.1117 
.0772 
.0387 
5 
o 
-.1190 
o 
.1391 
o 
.0613 
.0008 
.1087 
o 
o 
o 
.1109 
o 
.1544 
o 
o 
o 
-.0652 
o 
-.1028 
o 
o 
.0957 
o 
o 
.0154 
.0656 
6 
o 
o 
o 
o 
.0854 
o 
o 
o 
o 
o 
o 
.0872 
.0991 
o 
o 
o 
o 
o 
o 
o 
o 
o 
o 
o 
o 
.0109 
.0300 
207 
Total 
.8854 
.8487 
1.1093 
1.2564 
1.1064 
.9639 
.6188 
.3120 
.7979 
.6968 
1.3932 
1.3169 
1.3856 
.4884 
.7492 
1.0582 
.7957 
.7280 
.6410 
.6336 
.3415 
1.1859 
1.1870 
1.3105 
1.1737 
.9194 
.3197 
(cont.) 
Next we test the discriminant function on the calibrating set with the results 
shown in Table 5.3-3. Hamilton's average is .92, Madison's -.38, with mid-
point .27. These three numbers are rather similar to those obtained from the 
screening set, an encouraging sign. For both sets, the standard deviations are 
larger. One expects the means of the two sets to move toward one another, 
but here instead the standard deviations have grown. The smallest Hamilton 
score is .31, and the largest Madison score is .15; so there is still no overlap 
between the papers of known authorship. On the other hand, the distance be-
tween these closest papers is only .5 Hamilton standard deviation or .6 Madison 

208 
WEIGHT-RATE ANALYSIS 
[5.4 
TABLE 5.3-3 (cont.) 
WEIGHT-RATE ANALYSIS 
RESULTS FOR CALIBRATING SET FOR MADISON 
Group 
Paper 
1 
2 
3 
4 
5 
6 
Total 
number 
134 
0 
-.0899 
.0916 
.0450 
0 
0 
.0467 
135 
0 
-.1478 
-.2296 
.0774 
-.2034 
0 
-.5034 
201 
0 
.0126 
-.3609 
.0249 
.1065 
0 
-.2169 
202 
.0683 
-.0642 
-.2837 
.0042 
0 
0 
-.2754 
203 
0 
-.0642 
-.1623 
.0252 
-.0710 
0 
-.2723 
204 
0 
-.6286 
-.1929 
.0293 
0 
0 
-.7922 
205 
0 
-.2766 
-.0612 
.0045 
.0385 
0 
-.2948 
206 
.0739 
-.4904 
-.2503 
.0317 
0 
0 
-.6351 
207 
0 
-.4613 
-.2326 
.0043 
-.1731 
0 
-.8627 
208 
0 
-.0877 
-.2149 
.0253 
-.1080 
0 
-.3853 
209 
.1338 
-.4242 
-.2040 
.0082 
-.1614 
0 
-.6476 
210 
0 
-.2114 
-.3651 
.0041 
-.0807 
0 
-.6531 
216 
0 
-.2016 
.0107 
.0262 
0 
0 
-.1647 
217 
0 
-.0807 
-.2968 
.0274 
-.0666 
0 
-.4167 
218 
0 
-.1940 
-.2330 
.0083 
0 
0 
-.4187 
219 
0 
-.2947 
-.0916 
.0082 
-.1502 
0 
-.5283 
220 
0 
.0246 
-.2325 
.0208 
0 
0 
-.1871 
301 
0 
-.1794 
-.0136 
.0346 
0 
.0872 
-.0712 
302 
0 
-.1136 
-.1412 
.0827 
-'.0507 
0 
-.2228 
311 
0 
-.3777 
-.1151 
.0505 
.0010 
0 
-.4413 
312 
0 
0 
-.0909 
.0059 
0 
0 
-.0850 
313 
0 
.0983 
.0479 
0 
0 
0 
.1462 
314 
0 
-.1500 
-.1695 
.0344 
-.0975 
0 
-.3826 
315 
0 
-.2391 
-.0987 
.0402 
-.2915 
0 
-.5891 
316 
.0934 
-.5062 
-.0167 
0 
-.1447 
0 
-.5742 
Y 
.0148 
-.2059 
-.1561 
.0249 
-.0581 
.0035 
-.3771 
S 
.0361 
.1873 
.1217 
.0224 
.0900 
.0173 
.2538 
standard deviation. Still it is quite cheering to get no overlap, though it does 
not imply that we will make no errors or find no borderline cases. 
5.4. REGRESSION EFFECTS 
Before going on, let us see how much the various word groups have deterio-
rated between the screening set and the calibrating set. As a measure of dis-
crimination we choose (YH -
YM)/[!CSH + 8M)] because it is easy to compute 
and has the rough interpretation "number of standard deviations the means 

5.4] 
REGRESSION EFFECTS 
209 
TABLE 5.4-1 
WEIGHT-RATE ANALYSIS 
Discrimination indices 
Group 
Number of 
Screening 
Calibrating 
words 
set 
set 
1 
1 
3.3 
3.8 
2 
7 
2.5 
1.9 
3 
6 
3.5 
2.5 
4 
1 
1.0 
1.7 
5 
4 
2.7 
.9 
6 
1 
1.3 
.3 
Total 
20 
6.9 
4.5 
are apart." The symbols in this formula refer to means and standard deviations 
for the screening or calibrating sets for groups or totals whichever are appro-
priate. In Table 5.4-1 the results are shown for both the screening set and the 
calibrating set. 
The excellence of group 1, upon, actually an improvement from screening to 
calibrating set, illustrates an important point: when a few variables are selected 
from many, if these few really are head and shoulders better than the rest, there 
may well be no regression effect owing to s~lection (tho1)gh there may be a loss 
owing to choice of weights). The loss from 2.5 to 1.9 in group 2 is in the expected 
direction, but not spectacular. The loss in group 3 may be partly selectivity 
and partly paper length. 
The change in group 4, would, may be due to the change from Federalist 
material to the Neutral Trade paper from Madison. Both groups 5 and 6 fell 
flat on their faces. 
The discrimination in the total dropped from nearly 7 
standard deviations in the screening set to 4.5 in the calibrating set. 
The average paper lengths are given below. 
Screening set 
Calibrating set 
Hamilton 
1988 
1928 
Madison 
2594 
1949 
The slight change in average paper length for Hamilton cannot account for the 
increase in standard deviation. On the other hand, the reduction in Madison's 
paper length could account for about one fourth of the Madison increase. On 
the positive side, we are left with 4.5 standard deviations' worth of discrimination 
as measured in an independent set of papers.· And we are glad we provided for 
both a screening set and a calibrating set so as not to be misled by the original 
7 standard deviations. 

210 
WEIGHT-RATE ANALYSIS 
[5.5 
5.5. RESULTS FOR THE DISPUTED PAPERS 
Scores for the joint and the disputed papers are shown in Table 5.5-1. The 
average score for the 12 disputed papers is -.31, slightly higher than Madison's 
-.38 for the calibrating set. Except for paper 55, all have negative scores and 
fall well below the midpoint, .27, of the two calibrating sets. Paper 55 has a 
score that is slightly higher than the most Madisonian of the Hamilton calibrat-
ing papers, and falls slightly on Hamilton's side. Later we try to take a more 
quantitative view. Superficially, then, all but No. 55 look Madisonian, and No. 
55 is difficult to judge because it is near the middle. 
Among the joint papers, all three fall below the midpoint .27, although only 
No. 19 falls very far below it, and No. 20 is quite near the middle. 
From among the many ways to assess these results, we present two. As a first 
method we compare, using t-statistics, each disputed paper with the averages 
for the Hamilton and Madison calibrating sets. Let y be the total score for a 
disputed paper, and compute 
y -
'[JH 
tH = 
, 
8H'\.11 + (l/nH) 
(1) 
TABLE 5.5-1 
WEIGHT-RATE ANALYSIS RESULTS FOR JOINT AND DISPUTED PAPERS 
Group 
Paper 
1 
2 
3 
4 
number 
5 
Total 
6 
Joint papers 
18 
.0669 
-.0355 
-.1718 
.0245 
0 
.0820 
-.0339 
19 
0 
-.0437 
-.1485 
.0168 
0 
0 
-.1754 
20 
.0976 
.0549 
-.0911 
.0060 
.1522 
0 
.2196 
Y 
.0548 
-.0081 
-.1371 
.0158 
.0507 
.0273 
.0034 
Disputed papers 
49 
0 
-.2216 
-.2250 
.1173 
-.1360 
0 
-.4653 
50 
0 
-.1596 
-.1839 
.0847 
0 
0 
-.2588 
51 
0 
-.3693 
-.3141 
.0400 
-.1123 
0 
-.7557 
52 
0 
0 
-.2683 
.0370 
0 
0 
-.2313 
53 
0 
-.1822 
-.1706 
.0236 
-.1563 
0 
-.4855 
54 
.1394 
-.1796 
-.4416 
.0256 
0 
0 
-.4562 
55 
0 
.0384 
.2359 
.0418 
0 
0 
.3161 
56 
0 
-.0910 
.0547 
.0218 
0 
0 
-.0145 
57 
0 
-.3000 
-.1504 
.0232 
-.0756 
0 
-.5028 
58 
0 
-.1471 
-.2367 
.0490 
0 
0 
-.3348 
62 
0 
-.1323 
-.2095 
.0178 
-.0907 
0 
-.4147 
63 
0 
-.0288 
-.1298 
.0309 
0 
0 
-.1277 
Y 
.0116 
-.1478 
-.1699 
.0427 
-.0476 
0 
-.3109 

5.5] 
RESULTS FOR THE DISPUTED PAPERS 
211 
Here nH = nM = 25, the numbers of papers in the calibrating sets, so the 
numbers of degrees of freedom for the t's are 24. We use these t's to compute the 
area PH to the left of tH, and the area PM to the right of tM, for these t-
distributions. Here PH evaluates the chance of getting a more Madisonian 
result than the one observed if Hamilton wrote the paper, and PM is the chance 
of a more Hamiltonian result if Madison wrote the paper. 
If both PH and PM are large, say .4, the meaning is that the discrimination is 
poor to start with and that the paper itself is difficult to discriminate on the basis 
of the data being examined. Both P's cannot be large in our problem because 
YH and YM are far apart. If PH is large and PM is small, we incline toward Ham-
ilton as the author, and vice versa. If both PH and PM are small, say .01 and .02, 
the potential discrimination is strong, but the paper in question is difficult to 
discriminate on these data. If both P's are very tiny, the possibility looms large 
that assumptions are violated. For example, the paper may be joint, or an 
author may be using another's language, or the fitted distributions may have 
tails that are too low. 
In Table 5.5-2, the t's and P's are given for the joint and the disputed papers. 
Paper No. 55 still seems quite up in the air. The rest of the disputed papers 
except possibly No. 56 seem to be Madison's. 
TABLE 5.5-2 
P-VALUES FOR THE JOINT AND DISPUTED PAPERS CALCULATED 
FROM THE t-DISTRIBUTION WITH 24 DEGREES OF FREEDOM 
Paper 
y 
tH 
tM 
PH 
PM 
number 
Joint papers 
18 
-.0339 
-2.924 
1.326 
.00372 
.09866 
19 
-.1754 
-3.358 
.779 
.00131 
.22180 
20 
.2196 
-2.146 
2.305 
.02110 
.01506 
Disputed papers 
49 
-.4653 
-4.247 
.341 
.00014 
.63196 
50 
-.2588 
-3.614 
.457 
.00070 
.32589 
51 
-.7557 
-5.138 
-1.463 
.00002 
.92178 
52 
-.2313 
-3.529 
.563 
.00086 
.28933 
53 
-.4855 
-4.309 
-.419 
.00012 
.66053 
54 
-.4562 
-4.219 
-
.306 
.00016 
.61888 
55 
.3161 
-1.850 
2.678 
.03833 
.00658 
56 
-.0145 
-2.864 
1.401 
.00428 
.08701 
57 
-.5028 
-4.362 
-
.486 
.00011 
.68431 
58 
-.3348 
-3.847 
.163 
.00039 
.43594 
62 
-.4147 
-4.092 
-
.145 
.00021 
.55704 
63 
-.1277 
-3.212 
.964 
.00187 
.17233 

212 
WEIGHT-RATE ANALYSIS 
[5.5 
TABLE 5.5-3 
WEIGHT-RATE ANALYSIS 
CONSERVATIVE 90 PER CENT CONFIDENCE LIMITS FOR THE LOG LIKELIHOOD 
RATIO FOR THE JOINT AND DISPUTED PAPERS 
Paper 
Lower 
Upper 
Estimated 
number 
Index (y) 
confidence 
confidence 
log odds 
limit 
limit 
from t from normal 
Joint papers 
18 
-.0339 
-
9.4 
.3 
-3.2 
-3.8 
19 
-.1754 
-12.5 
-1.5 
-4.7 
-5.8 
20 
.2196 
-
4.5 
5.0 
.1 
.1 
Disputed papers 
49 
-.4653 
-19.5 
-3.0 
-7.2 
-9.6 
50 
-.2588 
-14.5 
-2.0 
-5.6 
-6.9 
51 
-.7557 
-26.8 
-3.8 
-8.6 
-12.8 
52 
-.2313 
-14.8 
-1.9 
-5.3 
-6.5 
53 
-.4855 
-20.0 
-3.1 
-7.3 
-9.8 
54 
-.4562 
-19.3 
-3.0 
-7.1 
-9.4 
55 
.3161 
-
2.9 
7.0 
1.4 
1.7 
56 
-.0145 
-
9.0 
.7 
-2.9 
-3.5 
57 
-.5028 
-20.4 
-3.2 
-7.4 
-10.0 
58 
-.3348 
-16.4 
-2.5 
-6.2 
-7.9 
62 
-.4147 
-18.3 
-3.0 
-6.8 
-8.9 
63 
-.1277 
-11.5 
-1.0 
-4.2 
-5.1 
A second appraisal gives results more easily compared with results of the main 
study. We give approximate confidence limits for the likelihood ratio. Assume 
that the score f) is approximately normally distributed for each author. Then a 
new piece of writing of length similar to those pieces previously studied gives 
the ratio 
(2) 
Unfortunately, we do not know the IL'S and O"s. We could, however, set con-
fidence limits on K, or what is equivalent, on its natural logarithm A. 
The 
following method for obtaining conservative 90 per cent confideIlce limits for A 
haB been worked out by Ann Mitchell and the calculationB carried out by 
Mary BIyman. 
Using the calibrating sets, construct 95 per cent confidence sets for the pairs 
(ILH, O'H) and (MM, O'M). Then for given y find the largest and smallest likelihood 

5.5] 
RESULTS FOR THE DISPUTED PAPERS 
213 
ratios achievable in these sets. The resulting bounds are the 90 per cent con-
fidence limits for the given value of y. (See Mood, 1950, pp. 227-229.) 
Table 5.5-3 shows these conservative 90 per cent confidence limits for the log 
likelihood ratio for each of the joint and disputed papers. For the 10 disputed 
papers, excluding Nos. 55 and 56, the evidence seems very strong for Madison. 
For No. 55 the confidence is at least .90 that the odds are between 18 to 1 for 
Madison and 1100 to 1 for Hamilton. This evidence does not seem very strong 
to us, nor do the somewhat stronger odds in Madison's favor for paper No. 56. 
Among the joint papers, the problem is one of extent and the log odds have 
been given for general interest. 
These confidence limits are likely very conservative for the answer they claim 
to give. Starting with a simultaneous 90 per cent (actually 90.25 per cent) 
confidence region for the four parameters iJ-H, O'H, iJ-M, O'M, and using the extreme 
values of some function of the parameters attained in this region as limits for 
the function, may easily give intervals twice as wide as necessary for 90 per cent 
confidence. The confidence intervals given here are about twice as wide as those 
based on the asymptotic standard error of the estimated log likelihood ratio. 
On the other hand, all the likelihood ratios and confidence intervals computed 
are sensitive to the assumption of normality. These two criticisms of the con-
fidence intervals compensate, but one is uncertain where the net would be. 
For those who wish a point estimate of the log odds from this weight-rate 
study, we give two in the final columns of Table 5.5-3. In the rightmost col-
umn, we estimate A by replacing each of the parameters by its usual estimate. 
In the next to rightmost column, we give an estimate, from a more Bayesian 
approach, using Student's t-distribution, that takes some account of the influ-
ence of errors in estimating the parameters. The message from each set of 
estimated log odds is much the same as usual. Most papers have strong evi-
dence for Madison's authorship; Nos. 55 and 56 are weak. 
The estimated log odds "from t" are obtained by the Bayesian argument of 
Section 3.1D, applied here to a normal distribution of scores. The numerator 
of K in Eq. (2) is the likelihood of a score y on a new piece of writing by Hamilton, 
given that the parameters iJ-H, O'H are known. To take proper account of our 
imperfect knowledge of the parameters, we average this likelihood with respect 
to the posterior distribution of iJ-H, O'H, given the data from the calibrating set. 
Because we have only a single score, selected on an independent screening set, 
the invariant, but improper, prior density element diJ-HdO'H/O'H is roughly appro-
priate. Jeffreys (1961, pp. 137-142) carries out the analysis needed and obtains 
the posterior distribution of iJ-H, O'H (his x, 0') and the averaged likelihood for a 
new score y. [See his Eq. (8), p. 142; translate his Xz = y, nz = 1, nl = nH, 
Xl = YH, nl(sD Z = nHs~.l With tH as defined in Eq. (1), the averaged 
likelihood can be written 
fnH-l(tH) 
sH[l + (l/nH)]1/z ' 

214 
WEIGHT-RATE ANALYSIS 
[5.5 
wherefmCt) is the ordinate of Student's t-distribution with m degrees of freedom. * 
In the same way, we get the averaged likelihood of y if Madison wrote the new 
piece, and the natural logarithm of the ratio is the value tabled as the estimated 
log odds from t. 
Some explorations of contaminated normal distributions of scores suggest 
that for the separation of score distributions here observed, and for the sorts of 
contaminations we often think of, the reductions of the estimated log odds are 
about the same as the reductions from the estimates based on the normal to 
those based on t. 
* These ordinates are available in: Sukhatme (1938, pp. 39-48). This table is re-
produced in Raiffa and Schlaifer (1961, pp. 354-355). A larger table is in Smirnov 
(1961). 

CHAPTER 
A Robust Hand-Calculated 
Bayesian Analysis 
6.1. 
WHY A ROBUST STUDY? 
6 
Robustness is a term attached to methods that are insensitive to assumptions 
extraneous to what is being studied. For example, in continuous distributions 
the shape of the distribution of sample medians is insensitive to the shape of 
the tails of the distribution from which the sample is drawn, and therefore 
medians are robust against changes in tails. Our main study depends upon 
distributional assumptions, such as the Poisson or negative binomial, and though 
we have studied their appropriateness, still it would be well to have a method 
that is less sensitive to distributional shape. Of course, we cannot expect from 
the robust study the strength of discrimination of the main study. 
While pushing for robustness, we also want simplicity, because the main 
study calls for complicated calculations that are difficult to check, whereas the 
distance from the data to the inference should be shorter in an ideal method. In 
this chapter we present a study, based on Bayes' theorem, that can be and was 
largely executed on slide rules. * 
The dichotomy is the basis of the robust study. For high-frequency words, 
we dichotomize the rates at the median of the pool of papers by both authors. 
Then an odds score is assigned to the word in a new paper, based on whether 
the observed rate is above or below the previously described median. 
We also dichotomize distributions for low-frequency words-into zero fre-
quencies and nonzero frequencies. For low-frequency words, length of paper 
must be taken into account in assigning the odds score, and that complicates 
the calculations. 
Variation in paper length is a computational millstone in the main study, 
and we wish to evade that source of trouble here. Otherwise the calculations of 
the odds for low-frequency words would be quite heavy. To simplify this matter, 
papers on which selection and weighting are executed need to be of nearly equal 
length. This accounts for our initial choice of papers. 
* The calculations were supervised by Roger Carlson and I vor Francis. 
215 

216 
A ROBUST HAND-CALCULATED BAYESIAN ANALYSIS 
[6.2 
In addition to the screening set, we have a validating set of papers, unlike the 
main study. We pay for this in the weakness of the screening, but it gives a 
better chance to see how the method operates on data of known authorship, 
uncontaminated by previous operations. 
Results for this set of papers form a test of the method because, from the point 
of view of the techniques used, these new papers can be regarded as unknowns, 
while, from our point of view and the reader's, their authorship is known. 
Thus we give the technique an examination before we use it on the truly un-
known papers. The disadvantage of the test, in comparison with the main 
Bayesian study, is the sacrifice of half the data that could be used to improve 
the discriminating power of the method. In the main Bayesian study, the test 
consisted of treating each paper of known authorship as an unknown, but 
there the method of discrimination was somewhat contaminated by the use of all 
the knowns in building the method. 
To get 2000-word Madison papers, we were driven to using all the Neutral 
Trade papers in the screening set. So a further disadvantage of this study rel-
ative to the main study is that the odds are not heavily influenced by the Mad-
ison Federalists. This weakness is a peculiarity of this study, rather than a 
general criticism of the method for other problems. 
6.2. PAPERS AND WORDS 
The screening set of papers for this study is a set of 46 papers of approximately 
equal length, 1728-2282 words, which we call the 2000-word set. Table 6.2-1 
shows the numbers and authors of these papers. 
At this point we had the choice of using the words from the index or from the 
screening set, but not both. The reason is that those words are already selected, 
and we want a validating set of papers uncontaminated by selection of words. 
Since the overlap between the papers used to make the index and those of the 
2000-word set is large, we used most of the original screening set of papers to 
validate this study. Had the original screening words been used, we would have 
had left only a few papers from those used to make the index for validation. 
Altogether then, we have some papers in the 2000-word set, some in the vali-
TABLE 6.2-1 
2000-WORD PAPERS: 
PAPERS OF LENGTH 1728-2282 WORDS 
Hamilton (23) 
6-8, 12, 16, 21, 23-25, 
29-31, 34, 35, 59, 60, 
65, 66, 72, 73, 75-77 
Madison (23) 
14,45, 
201-220 ( Neutral Trade), 
301 

6.3] 
LOG ODDS FOR HIGH-FREQUENCY WORDS 
217 
TABLE 6.2-2 
WORDS USED IN THE ROBUST BAYES STUDY 
High 
High 
Low 
Low 
Low 
frequency 
frequency 
frequency 
frequency 
frequency 
a 
(3 
(3 in S 
(3 not in S 
0 
upon (60) 
on (40) 
among (74) 
then (54) 
commonly (80) 
there (55) 
both (78) 
although (73) 
consequently (81) 
as (8) 
often (100) 
while (116) 
according (119) 
this (57) 
where (114) 
whilst (117) 
apt (124) 
by (13) 
about (118) 
better (128) 
with (68) 
always (123) 
common (131) 
to (58) 
before (126) 
likely (145) 
pf (39) 
during (135) 
proper (154) 
between (77) 
moreover (147) 
dating set, and some not in use because they are the wrong size for the 2000-
word set and are contaminated for the purpose of validation. 
The choice made implies that we used the words provided by the index, 
Section 2.5C, and by the Miller-Newman-Friedman list of 70 high-frequency and 
20 low-frequency words, Section 2.5A. Although the high-frequency words have 
high rates in Miller-Newman-Friedman's list, some are low-frequency words in 
our materials and must be so treated. This is unimportant; the issues discussed 
up to now dealt with selection and contamination, and we now speak of the 
technology of getting the odds for words after the pool has been selected. 
The words finally chosen for discrimination purposes on the basis of their 
performance in the 2000-word set fall into five groups as shown in Table 6.2-2. 
The top group, as in the other studies, was composed of upon. The next best 
groups of words are the (3's. For reasons of calculation these were divided into 
high and low frequencies, and the low-frequency group was dichotomized into 
those in the random sample of words, S, and those not in S. Finally group 0 
consists of somewhat more meaningful words than those in groups a and (3, 
words we thought would be more contextual. 
We turn now to the calculation of log odds for these words. 
6.3. 
LOG ODDS FOR HIGH-FREQUENCY WO:RDS 
For each high-frequency word, we divide the rates of the 46 papers in the 2000-
word sets into two equal parts, the high's and low's. Then we divide each rate 
group by author to get a 2 X 2 table. We choose as the dividing line for later 
work the midpoint between the rates for the 23rd and 24th papers. For example, 
the word to yields for its 23rd lowest paper the rate 37.38, for its 24th the rate 
38.23, so the midpoint is 37.805. The 2 X 2 table for to is given in Table 6.3-1. 

218 
A ROBUST HAND-CALCULATED BAYESIAN ANALYSIS 
[6.3 
TABLE 6.3-1 
RATES FOR to 
Low 
High 
Totals 
Hamilton 
7 
16 
23 
Cutoff point 
Madison 
16 
7 
23 
37.805 
Totals 
23 
23 
46 
To estimate the odds to be assigned for the word to, or any other high-frequen-
cy word, we add the number 1.25 (discussed below) to every interior cell in 
Table 6.3-1 to get the results shown below. 
Hamilton 
Madison 
Low 
8.25 
17.25 
High 
17.25 
8.25 
If in an unknown paper the rate for to is below the cutoff, then the odds are 
8.25 to 17.25 (thus favoring Madison), and if the rate is high, we get17.25 to 8.25 
(favoring Hamilton). The first number of an odds pair gives Hamilton's chances, 
the second gives Madison's. After we have the log odds for each word, we sum 
them and finally get log odds for an entire paper. 
The number 1.25 that was added to each cell of Table 6.3-1 was estimated 
from the observed distribution of 2 X 2 tables obtained for the 64 high-frequency 
words. (Six of the 70 high-frequency function words from the Miller-Newman-
Friedman list occurred so rarely in our materials that they have to be regarded 
as low-frequency words here.) Table 6.3-2 shows the distribution of the count 
TABLE 6.3-2 
DISTRIBUTION OF NUMBER OF Low-RATE HAMILTON 
PAPERS FOR 64 HIGH-FREQUENCY FUNCTION WORDS 
Number of papers 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
Frequency 
1 
2 
4 
4 
4 
5 
3 
7 
8 
Number of papers 12 13 
14 
15 
16 
17 
18 
19 20 
21 
22 23 
Total 
Frequency 
7 
2 
5 
2 
4 
3 
2 
1 
64 
Folded distribution 
Count 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
Frequency 
1 
3 
2 
7 
8 
4 
7 
8 
9 15 

6.31 
LOG ODDS FOR HIGH-FREQUENCY WORDS 
219 
of low-rate Hamilton papers in the 2000-word set for the 64 words. Because the 
2 X 2 tables are symmetrical, the count is also that of high-rate Madison papers. 
In Table 6.3-1, to was found to have 7 low-rate Hamilton papers. Table 6.3-2 
shows that four words, including to, had such 2 X 2 tables. This table helps us 
get an estimate of the prior distribution. From the point of view of the prior, 
Hamilton and Madison are equally likely to have any given number of low-rate 
papers. Consequently, we fold the distribution of Table 6.3-2 to get a better 
estimate of the left half of a symmetric distribution, and this produces the folded 
distribution in the bottom of Table 6.3-2. To give an example, in the folded 
distribution, the frequency of seven words having a count of 5 low-rate Ham-
ilton papers comes from adding four papers with a count of 5, and three papers 
with a count of 18 from the first distribution of Table 6.3-2. 
FIG. 6.3-1. Model for high-frequency words. 
The theoretical framework is this. We suppose that for high-frequency words 
the rates are approximately normally distributed for each author, with different 
means. Midway between JJ.H and JJ.M, cut the distributions, and let the area to 
the left for the distribution with mean JJ.H (Hamilton, say) be called 7r. This quan-
tity 7r is assumed to have a prior beta distribution, so that for each word a value 
of 7r is chosen from the beta distribution. If we knew (JJ.H + JJ.M)/2 and 7r, then 
a paper with a rate below the midpoint would be assigned the odds 7r/(1 -
7r). 
Since we do not know them, we go on. Given the value of 7r, 23 Hamilton and 
23 Madison rates' are drawn from the normal, and the cutoff c (for to, c = 37.805) 
for the papers found. The area to the left of c in the distribution with mean JJ.H 
is called p, in the distribution with mean JJ.M, q. Odds for papers with rates 
less than c should be p/q. Again, all we know is the observed 2 X 2 table. 
Let u be the parameter of the prior beta distribution for 7r: 
( ) 
r(2u) 
1.1-1(1 
)1.1-1 
pfJ 7r = r(u)r(u) 7r 
-
7r 
, 
o ::; 7r ::; 1. 
Each chosen value of u produces a marginal distribution of low-rate Hamilton 
papers, a theoretical distribution whose left half is analogous to the folded dis-
tribution of Table 6.3-2. We tested a variety of values of u, and by numerical 
integration obtained marginal distributions to compare with the folded dis-

220 
A ROBUST HAND-CALCULATED BAYESIAN ANALYSIS 
[6.5 
tribution of Table 6.3-2. The best fitting value of u was 3.5. Approximate 
theory, which we omit, suggests that if the counts are as shown below, 
Hamilton 
Madison 
then the odds are approximately 
2x + u -
1 
2(n -
x) + u -
1 
Low 
High 
x 
n-x 
n-x 
x 
x + (u -
1)/2 
n -
x + (u -
1) /2 
For u = 3.5, (u -
1)/2 = 1.25; so we added 1.25 to our counts to get the odds. 
6.4. 
LOW-FREQUENCY WORDS 
If a word rarely occurs, then many papers produce zero frequencies. The 
probability of a zero frequency depends quite intimately on the length of a paper; 
roughly, doubling the length of a paper squares the probability of a zero. For 
example, if the probability of zero is.6 for a paper of length 1000 words, it is 
.36 for length 2000, .216 for length 3000. In turn this means that we cannot be 
cavalier about paper length for low-frequency words. Unfortunately, some of 
the good discriminators are low-frequency ones, so we feel obligated to proceed 
with the analysis. By choosing as our screening set papers of about equal 
length-here, 2000 words-we can assess the probability of zeros for a paper of 
standard length and then adjust the probabilities and odds when we come to 
the validating set and the disputed papers. 
6.5. 
THE PROCEDURE FOR LOW -FREQUENCY WORDS 
In Section 6.6 we give the Bayesian theory for obtaining odds in the manner 
described in this section; here, we indicate the procedure, with examples, but 
without justification. 
For each word, form the 2 X 2 table for nonoccurrence and occurrence for the 
46 papers in the 2000-word set. For example, the word both gives 
Hamilton 
(1) 
Madison 
Total 
Nonoccurrence 
11 
2 
13 
Occurrence 
12 
21 
33 
Totals 
23 
23 
46 
Adjustment of this table to deflate the odds requires subtracting a number 
from cells 11 and 21, and adding the same number to cells 2 and 12. We add 

6.5] 
THE PROCEDURE FOR LOW-FREQU.E~CY WORDS 
221 
and subtract so as to bring the odds nearer one to one. The details require two 
main steps. To get a trial value, we form an adjusted table by adding and sub-
tracting x = 1: 
10 
13 
(2) 
3 
20 
Then we compute the preliminary value of the adjustment as 
20 X 10 
Xl = log 3 X 13 "" log 5.13 "" 1.64. 
The final adjustment value, x, is obtained by substituting in the formula 
(3) 
Xl + R 
X= I+R' 
where R is the sum of the reciprocals of the cell entries in the adjusted table (2). 
For the present example, 
R = 
110 + /3 + ! + lo "" .56. 
Then substituting into Eq. (3) gives 
X = ~:~~ = 1.41. 
This number is used as the final adjustment for the original table (1) to give 
(4) 
Hamilton 
Madison 
"0" 
9.59 
3.41 
"1 " 
13.41 
19.59 
Totals 
23 
23 
where "0" denotes nonoccurrence and "I" denotes occurrence. 
From table (4) we compute not odds, but probabilities; thus 
P 
_ 9.59 _ 
oH - 23- .416, 
P IR = .584, 
POM = ~~4I= 
23 
.148, 
P IM = .852. 
These probabilities are appropriate for a paper of length two (thousand words). 
To get odds for a paper of length w, we compute w/2 and (slide rule recom-
mended) 
POH(W) = p~k2, 
POM(w) = 
P~fJ, 
P 1H(W) = 1 -
POH(w), 
P1M(W) = 1 -
POM(w). 

222 
A ROBUST HAND-CALCULATED BAYESIAN ANALYSIS 
[6.6 
Then at last the odds (Hamilton to Madison), given no occurrence in a paper 
of length w, are 
For example, for the word both and for a paper of length 1.6, we get w/2 = .8. 
P oH(1.6) = (.416)·8 = .496, 
P oM(1.6) = (.148)·8 = .216, 
P 1H(1.6) = .504, 
P 1M(1.6) = .784. 
Odds (H to M given zero occurrences) = .496/.216 = 2.30; log odds = .83, 
Odds (H to M given an occurrence) = .504/.784 = .642; log odds = -.44. 
As in the high-frequency procedure, the remaining known papers form a 
validating set. And for each remaining paper, known and unknown, we record 
log odds for each word (appropriate to paper length). Finally, we sum log odds 
across words for each paper. 
6.6. 
BAYESIAN DISCUSSION FOR LOW -FREQUENCY WORDS 
The low-frequency procedure has two sources of difficulty added to those of 
the high-frequency analysis. The basic data involve two binomials rather than 
the single binomial to which the high-frequency procedure reduced. For zero 
occurrences, changes in paper length have substantial effects. Handling both 
simultaneously led to ghastly calculations. The 2000-word paper set was chosen 
to provide a set of papers of roughly constant length (within and between 
authors). 
More generally, we suppose the paper length w is fixed. In our example for 
2000-word papers, w = 2. And we let the probability of nonoccurrences be 
then 
where i = H, M, Pie!) is the probability of at least one occurrence, and Ai is 
a sort of rate. 
Let 
W OH = hw, 
W 1H = hw, 
W OM = mw, 
W 1M = mw, 
where h, h are the numbers of Hamilton nonoccurrences and occurrences, in 
that order, in the screening set, and m and m are similarly defined. We let the 
joint prior density of AH, AM be 
p(A A) = ae- b(>-H->-M)2/ 2 
H, 
M 
. 

6.6] 
BAYESIAN DISCUSSION FOR LOW-FREQUENCY WORDS 
223 
Here b is the reciprocal of O'~H-AM' which is estimated to be about 4 in our 
example, based on results from the random set of 20 low-frequency words (see 
Section 2.5A). To get the prior density, we regard the distribution of AH -
AM 
as normal and that of AH + AM as flat. 
We plan to get posterior odds approximately for an unknown paper by 
locating the maximum of everything but the factor for the unknown paper, and 
then to evaluate the odds at that position (posterior mode). In symbols, we need 
the maximizing values of AH, AM [or of PH(O), PM(O)] for the following formula: 
(1) 
(1 + e-AHW)h+h(l + e-AMW)m+m 
Taking the logarithm and derivatives with respect to AH and AM and setting 
them equal to zero (let h + h = nh, ill + m = nm), we get 
(2) 
-AMW 
-b(AH -
AM) + mw = wnm 
e 
A 
== wnmPM(O). 
1 + e- MW 
We wish to solve for AH and AM. Note that if b(AH -
AM) is negligible, then as 
initial estimates we have 
(3) 
which are the usual unbiased estimates for the P's. This suggests iterating. 
Putting these values (3) into the Eqs. (2) as a first estimate, we proceed to set 
e-AHW 
1 
h 
---:--== 
=-, 
1 + e-AHW 
1 + i HW 
nh 
and then solving for AH, we get 
(1) 
1 
(nh 
) 
1 
(h) 
(1) 
1 
(m) 
AH = w log h -
1 = w log h' 
AM = w log m ' 
(4) 
b(AU) -
A1P) = ~ log (~m) . 
w 
hm 
Substituting this result (4) into the left-hand side of the system (2) and solving 
for new estimates of Pi(O), we find 
-
2 
-
pU)(O) = h + (b/w ) log (hm/hm) , 
nh 
pjJ)(O) = ill -
(b/w 2) log (hm/hm) . 
nm 

224 
A ROBUST HAND-CALCULATED BAYESIAN ANALYSIS 
[6.6 
Note that the quantity (b/w 2)log(hm/hm) is added to Ii and subtracted from m. 
So it is playing an adjustment role similar to, but not identical with, the adding 
of a number to every cell in the high-frequency problem. 
Whatever is added to h (say xo) for this first iteration, the next iteration leads 
to adding 
(5) 
.b I 
(h -
xoWm -
Xo) 
Xl = -
og _ 
. 
w2 
(h + xo)(m + Xo) 
Our problem then is to satisfy Eq. (5) by one value of X that replaces both Xl 
and Xo. If we expand the right-hand side of Eq. (5) in the neighborhood of x, 
we get, neglecting terms of order (x -
XO)2, 
(6) 
Xl = X + (x- xo)R, 
where 
R = ~ (_1_+_1 __ +~_+_1_). 
w2 
h -
x 
m -
X 
h + X 
m + X 
Ordinarily, R should not be very sensitive to x, so we evaluate R at X = Xo 
to get Ro, and then solve Eq. (6) for X to get 
Xl + Roxo 
X 
<=::< 
1 + Ro . 
It would be convenient to take Xo = 1 or -
1 (whichever reduces the log 
odds), and in our problem b/w2 = 1 (by accident); hence we use in our work 
(7) 
Xl + Ro 
x = 1 + Ro ' 
where Ro is now just a sum of reciprocals. 
The final odds and probabilities for an unknown paper are calibrated for 
w = 2, so a new paper of length w has to have its probabilities and odds com-
puted through an adjustment explained in detail in the previous section. 
The parametrization just discussed is designed to handle the prior distribution. 
For the further analysis we take the position that the probability of no occur-
rences is approximately a Poisson probability for zero occurrences. Then we 
can adjust for length of paper. Thus if the probability of zero occurrences for a 
paper of standard length 2 is written 
P(O I 2) = e-2JJ., 
the probability of zero occurrences for that word for a paper of length w is 

6.7] 
LOG ODDS FOR 2000-WORD SET AND VALIDATING SET 
225 
6.7. LOG ODDS FOR 2000-WORD SET AND VALIDATING SET 
Table 6.7-1 shows the average log odds for the 2000-word sets for Hamilton 
and Madison to be 13.95 and -14.25, respectively. All Hamilton log odds are 
positive, all Madison negative. In the main study the average log odds for papers 
of known authorship adjusted to 2000 words was 17.1 for Hamilton, -16.5 for 
Madison, so some strength has been lost, i.e., about three units in the logarithm. 
(In the table headings, Sand NS stand for "iIi S" and "not in S," respectively, 
labels which are described in Section 6.2.) 
TABLE 6.7-1 
LOG ODDS FOR HAMILTON: 2000-WORD SET 
High 
High 
Low 
Low 
Low 
frequency 
frequency 
frequency 
frequency 
frequency 
Paper 
ex 
(3 
(3 
(3 
0 
number 
S 
NS 
NS 
Total 
6 
2.34 
4.32 
.49 
.74 
-2.32 
5.57 
7 
2.34 
4.72 
.81 
2.69 
6.39 
16.95 
8 
2.34 
3.78 
1.89 
2.35 
2.36 
12.72 
12 
2.34 
1.50 
-1.13 
3.12 
-
.18 
5.65 
16 
2.34 
8.04 
2.42 
3.53 
6.03 
22.36 
21 
2.34 
2.88 
.28 
4.82 
1.15 
11.47 
23 
2.34 
3.96 
-
.08 
2.69 
2.93 
11.84 
24 
2.34 
4.02 
.28 
3.98 
4.71 
15.33 
25 
-2.34 
-1.68 
.28 
1.48 
5.41 
3.15 
29 
2.34 
4.36 
1.89 
4.82 
3.63 
17.04 
30 
2.34 
5.80 
1.01 
1.89 
5.31 
16.35 
31 
2.34 
5.80 
.28 
4.30 
5.90 
18.62 
34 
2.34 
5.80 
-
.61 
3.95 
-.77 
10.71 
35 
2.34 
5.00 
-
.04 
2.35 
5.17 
14.82 
59 
2.34 
8.04 
-1.13 
3.53 
.68 
13.46 
60 
2.34 
6.20 
.80 
.28 
4.35 
13.97 
65 
2.34 
8.84 
.48 
4.82 
4.71 
21.19 
66 
2.34 
5.52 
.48 
.35 
2.34 
11.03 
72 
2.34 
5.80 
1.37 
2.00 
.68 
12.19 
73 
2.34 
4.72 
1.01 
1.04 
4.71 
13.82 
75 
2.34 
7.00 
1.89 
4.82 
3.63 
19.68 
76 
2.34 
6.20 
-
.04 
3.18 
5.90 
17.58 
77 
2.34 
4.14 
.48 
5.59 
2.93 
15.48 
Totals 
49.14 
114.76 
13.11 
68.32 
75.65 
320.98 
L ~ 13.95 
(cont.) 

226 
A ROBUST HAND-CALCULATED BAYESIAN ANALYSIS 
[6.7 
TABLE 6.7-1 (cont.) 
LOG ODDS FOR MADISON: 2000-WORD SET 
High 
High 
Low 
Low 
Low 
frequency 
frequency 
frequency 
frequency 
frequency 
Paper 
IX 
(3 
(3 
(3 
0 
number 
S 
NS 
NS 
Total 
14 
-2.34 
-5.16 
1.37 
-3.30 
-
.92 
-10.35 
45 
-2.34 
-2.18 
-
.60 
-5.23 
-1.76 
-12.11 
201 
-2.34 
-8.04 
-1.13 
-2.55 
-4.73 
-18.79 
202 
-2.34 
-6.60 
-1.13 
-2.34 
-3.87 
-16.28 
203 
-2.34 
-4.72 
-
.04 
-2.55 
-5.79 
-15.44 
204 
-2.34 
-7.36 
-1.13 
-5.57 
-2.12 
-18.52 
205 
-2.34 
-7.00 
-
.61 
-2.30 
-2.23 
-14.48 
206 
-2.34 
-3.56 
-1.13 
-2.60 
-4.73 
-14.36 
207 
-2.34 
-8.84 
-1.13 
-4.05 
-3.17 
-19.53 
208 
-2.34 
-5.00 
-1.13 
-4.49 
-1.83 
-14.79 
209 
-2.34 
-2.98 
-1.13 
-5.27 
-4.01 
-15.73 
210 
-2.34 
-5.00 
.80 
-4.49 
-5.79 
-16.82 
211 
-2.34 
-6.56 
-
.60 
-2.52 
-5.79 
-17.81 
212 
2.34 
-4.76 
-
.61 
-1.34 
-3.54 
-
7:91 
213 
-2.34 
-8.04 
-
.61 
-4.49 
-4.73 
-20.21 
214 
-2.34 
-1.50 
-
.61 
-2.12 
-5.79 
-12.36 
215 
-2.34 
-2.30 
-1.13 
-4.05 
-3.02 
-12.84 
216 
-2.34 
.06 
.48 
-2.47 
-1.10 
-
5.37 
217 
-2.34 
-6.60 
.49 
-
.05 
-1.26 
-
9.76 
218 
-2.34 
-7.36 
.61 
-3.59 
-3.02 
-16.92 
219 
-2.34 
-6.56 
.61 
-4.94 
-3.17 
-17.62 
220 
-2.34 
-3.96 
.04 
-3.44 
-
.07 
-
9.85 
301 
-2.34 
-
.74 
-1.13 
-1.13 
-4.73 
-10.07 
Totals 
-49.14 
-114.76 
-11.97 
-74.88 
-77.17 
-327.92 
L ~ -14.25 
A more important assessment is shown in Table 6.7-2, where log odds are 
given for the validating set. They show weaker log odds on the average than 
the 2000-word set, averaging only 10.2 for Hamilton and -8.2 for Madison. 
Although no papers in the validating set are wrongly classified, there are some 
close calls. Hamilton 113, Pacificus III, is just at zero log odds, or odds of 
one to one. Madison's famous paper 10 with log odds of -.53 or odds of 1 to 1.7 
is quite weakly marked. And papers 41 and 141 have quite modest odds. Even 
though we have some support for the validity of this study, we must keep in 
mind its one great awkwardness. The Madison 2000-word set except for two 
papers is non-Federalist material, and the consequence for the whole method 
is hard to evaluate, except empirically in the validating set. In the main study 
of Chapter 3" we made a special effort to minimize the effects of differences 
between Federalist and non-Federalist material, and so we do not expect it to be 

6.7] 
LOG ODDS FOR 2000-WORD SET AND VALIDATING SET 
227 
TABLE 6.7-2 
LOG ODDS FOR HAMILTON: VALIDATIl'\G SET 
High 
High 
Low 
Low 
Low 
frequency 
frequency 
frequency 
frequency 
frequency 
Paper 
ex 
(3 
(3 
(3 
0 
number 
S 
NS 
NS 
Total 
1 
2.34 
2.40 
1.61 
1.91 
2.51 
10.77 
9 
2.34 
4.32 
1.09 
-1.42 
1.22 
7.55 
11 
2.34 
4.72 
2.26 
1.39 
3.69 
14.40 
26 
2.34 
5.80 
.01 
2.23 
3.68 
14.06 
36 
2.34 
5.80 
.66 
1.21 
.63 
10.64 
70 
2.34 
5.00 
.09 
3.60 
6.19 
17.22 
71 
2.34 
3.96 
.89 
2.50 
2.48 
12.17 
74 
2.34 
5.12 
.15 
2.23 
4.31 
14.15 
101 
2.34 
4.76 
-1.19 
2.56 
3.56 
12.03 
102 
2.34 
1.10 
-
.16 
2.78 
2.90 
8.96 
111 
2.34 
6.20 
-
.51 
.88 
-5.59 
3.32 
112 
2.34 
2.98 
-
.51 
1.43 
.74 
6.98 
113 
-2.34 
-
.06 
-
.75 
1.15 
2.00 
.00 
Totals 
25.74 
52.10 
3.64 
22.45 
28.32 
132.25 
L """ 10.20 
LOG ODDS FOR MADISON: VALIDATING SET 
High 
High 
Low 
Low 
Low 
frequency 
frequency 
frequency 
frequency 
frequency 
Paper 
ex 
(3 
(3 
(3 
0 
number 
S 
NS 
NS 
Total 
10 
-2.34 
-
.74 
.10 
2.77 
-
.32 
-
.53 
37 
-2.34 
-7.00 
-1.11 
-6.89 
-1.42 
-18.76 
38 
2.34 
1.50 
-
.36 
-4.19 
-3.72 
-
4.43 
39 
-2.34 
-3.96 
-1.12 
-1.11 
-1.58 
-10.11 
40 
-2.34 
-1.50 
-
.52 
.75 
-
.35 
-
3.96 
41 
-2.34 
.12 
.14 
-
.94 
.91 
-
2.11 
42 
-2.34 
-3.16 
.07 
-4.36 
.01 
-
9.78 
43 
-2.34 
-3.16 
-1.12 
-6.37 
-1.30 
-14.29 
44 
-2.34 
-3.96 
2.29 
-
.03 
-2.58 
-
6.62 
46 
-2.34 
-1.32 
.04 
-1.82 
-1.48 
-
6.92 
47 
-2.34 
-2.76 
.03 
1.07 
-2.30 
-
6.30 
48 
-2.34 
-1.72 
-
.68 
-
.40 
-
.85 
-
5.99 
121 
-2.34 
-
.74 
1.92 
-3.47 
.49 
-
4.14 
122 
-2.34 
-3.96 
.07 
-6.87 
-4.53 
-17.63 
131 
-2.34 
--4.14 
-1.12 
-3.23 
-2.23 
-13.06 
132 
-2.34 
-3.34 
-1.11 
-3.11 
-4.04 
-13.94 
133 
-2.34 
2.98 
.46 
-4.90 
-3.08 
-
6.88 
141 
-2.34 
1.54 
.13 
3.92 
-5.34 
-
2.09 
Totals 
-37.44 
-35.32 
-1.89 
-39.18 
-33.71 
-147.54 
L""" -
8.20 

228 
A ROBUST HAND-CALCULATED BAYESIAN ANALYSIS 
[6.8 
much bothered by these differences. Finally, were it not for these differences, 
we would not have reason to expect that the log odds in the validating set 
would be weaker than for the 2000-word set. 
6.8. DISPUTED PAPERS 
Except for paper 55, the odds for the disputed papers shown in Table 6.8-1 
are in favor of Madison. Papers 52 and 55 both have quite modest log odds. 
The results are considerably weaker than for the main study. Except fOr paper 
55 the results are in the same direction. (The study is not affected by the changes 
between the newspapers and the McLean edition noted by Cooke.) 
TABLE 6.8-1 
LOG ODDS FOR JOINT AND DISPUTlm PAPERS 
High 
High 
Low 
Low 
Low 
frequency 
frequency 
frequency 
frequency 
frequency 
Paper 
a 
(3 
(3 
(3 
0 
number 
S 
NS 
NS 
Total 
Joint 
18 
-2.34 
-
.92 
-
.03 
-l.66 
-l.88 
-6.83 
19 
-2.34 
-5.80 
l.38 
-3.29 
2.82 
-7.23 
20 
-2.34 
-
.74 
-
.14 
.53 
.18 
-2.51 
Total 
joint 
-7.02 
-7.46 
1.21 
-4.42 
1.12 
-16.57 
L"", -
5.52 
Disputed 
49 
-2.34 
-2.12 
.58 
-
.39 
-1.42 
-5.69 
50 
-2.34 
-3.96 
1.35 
-1.83 
1.97 
-4.81 
51 
-2.34 
-2.98 
-1.13 
-3.56 
-
.87 
-10.88 
52 
-2.34 
-
.24 
-
.07 
-
.53 
1.21 
-1.97 
53 
-2.34 
-1.54 
-
.01 
-6.88 
2.41 
-8.36 
54 
-2.34 
-3.96 
-
.60 
-1.64 
-3.50 
-12.04 
55 
-2.34 
5.00 
-1.14 
-
.!J4 
1.72 
2.30 
56 
-2.34 
2.30 
-1.16 
-3.42 
.13 
-4.49 
57 
~2.34 
-2.98 
-
.58 
-
.81 
.98 
-5.73 
58 
-2,34 
-3.96 
~ .03 
.03 
.14 
-6.l6 
62 
-2.34 
-2.52 
-U2 
-
.44 
.l0 
-6.32 
63 
-2.34 
-
.74 
1.92 
-3.06 
.50 
-3.72 
Total 
disputed 
-28.08 
-17.70 
-l.99 
-23.47 
3.37 
-67.87 
L"", -5.66 

CHAPTER 7 
Three-Category Analysis 
7.1. THE GENERAL PLAN 
For a long time, we could not find an economical way to carry out a robust 
Bayesian analysis, but we did see a fairly straightforward way to do a classical 
analysis, using categories rather than measured rates. The latter study was 
executed before the robust Bayesian analysis was begun, and so we report it 
here, although i~ many ways the two studies are similar. 
The general plan for any word is to group rates into three categories: low, 
middle, and high. Then we obtain estimated log odds for each category, and 
sum these log odds over words for a given paper. 
A log odds plays the role of a weight X rate in the weight-rate study. When 
we assume words are independent, we want to multiply the odds (or add the 
log odds) for the words to get a total evaluation. After estimating log odds for 
each word and category from the screening set, we calibrated the sums as in 
the weight-rate study for the purpose of analyzing the unknown papers. In 
other words, although we use a log odds, when we finish we shall discard that 
interpretation and merely use the numbers as scores to be judged on their 
behavior in a calibrating set. 
As in the robust Bayesian study, the purpose of the analysis is to defend 
systematically against outrageously unusual rates and against sensitivity to dis-
tributional assumptions. An accidental rather than an inherent advantage of 
the present study over the robust Bayesian analysis is its larger calibrating set; 
a disadvantage is the cruder handling of zero frequencies as length of paper 
varies. The latter weakness also plagues the weight~rate study. 
7 .2. DETAILS OF METHOD 
The screening set (Section 2.5C), consisting of 23 Hamilton and 25 Madison 
papers, and the 117 words developed from it, determined the pool of discrim-
inators and their estimated log odds. For each word, the 48 papers were arranged 
in ascending order of rate of use of that word. Then, a 2 X 3 table was formed, 
employing as far as possible the marginal totals shown in Table 7.2-1. 
229 

230 
THREE-CATEGORY ANALYSIS 
TABLE 7.2-1 
INTENDED MARGINAL TOTALS FOR 2 X 3 TABLES 
Author 
Rate 
Low 
Middle 
High 
H 
M 
18 
12 
18 
23 
25 
[7.2 
However, if a word failed to occur in more than 18 papers, all such papers were 
classified in the low category, thus making the number in the middle category 
less than 12, and sometimes even causing the table to degenerate into a 2 X 2 
table with 18 or fewer papers in the high category. 
After these initial contingency tables were constructed, many words were 
discarded as being unpromising discriminators, and adjustments were made on 
the tables for the surviving words. The discarding and adjustments were made 
in accordance with a sequence of operational rules which standardized the whole 
procedure. These rules are described below, but readers who wish to bypass 
details should skip to the end of Rule 8. 
DISCARDING OR RETAINING WORDS. 
Rule 1. Examine the absolute 
difference in count I Hamilton minus Madison I for the low and for the high 
category. If both totals are at least 18, and if neither category shows a 
difference of at least 6, discard the word. 
Example. 
Discard 
7 6 10 
11 6 
8 
Rule 2. If extreme categories are in same direction, discard the word. 
Example. 
12 
1 10 
6 11 
8 
ADJUSTMENTS FOR INEQUITY IN SAMPLE SIZE. Rule 3. (a) If low Hamilton 
cell is smaller than low Madison cell, adjust the table to include the 19 
lowest rates in the low category, leaving 11 in the middle. (b) If low Ham-
ilton cell exceeds low Madison cell, adjust the high category to 19 papers, 
leaving 11 in the middle. (c) If a tie occurs in low cells, and the high 
Madison exceeds high Hamilton, make the high category include 19, reduc-
ing the middle. Otherwise, make the low category include 19. 
POOLING CATEGORIES. 
Rule 4. 
(Never applied.) If two consecutive 
categories have zero frequency for the same author, pool categories. 

7.2] 
DETAILS OF METHOD 
231 
Example. 
0 0 23 
0 23 
19 5 
1 ~ 24 
1 
Rule 5. If middle category has a more extreme ratio than one tail 
category, pool with that category. 
Example. 
15 
1 
3 10 
7 (1 
7 ) 
12 
TO < 12 ~ 
15 
8 
3 22 
ADJUSTMENTS FOR ESTIMATING LOG ODDS. Rule 6. Handling zero cells for 
odds calculation. The purpose of this rule is to give an especially good 
category better odds, and yet to avoid infinite ratios. 
Example. 
H 
17 6 
0 
M 
1 5 19 
To get rid of the 0 in the high Hamilton cell, replace it by a 1. Count the 
papers in descending order of rate until the first Hamilton paper appears. In 
the high Madison cell, enter the number of papers that were counted up to and 
including the first Hamilton paper. Do not adjust the other categories. Thus 
if the paper with the 25th highest rate was the first Hamilton (20, 21, 22, 23, 24 
were Madison's), the table becomes 
17 6 
1 
1 5 25 
No change in cut-points is made. The resulting table is just a device for com-
puting odds; it can be thought of as an adjusted frequency table in the spirit 
of the robust Bayesian analysis of Chapter 6. 
Rule 7. An end category must contain an adjusted total of at least 7 
papers. If it does not, papers with adjacent rates are counted until the 
total reaches 7, and recorded as in Rule 6. As in Rule 6, all cut-points and 
the counts in other categories are unchanged. If Hamilton and Madison 
papers are tied in a clump which would raise the total to or above 7 for the 
first time, then equal numbers are added to both the Hamilton and Madison 
cells until the category total reaches or passes 7. 
For example, suppose the end category has three Hamilton and one Madison 
paper and the next five papers (say three H's and two M's) are tied, then we 
just add the number two to each author's cell, winding up with five for Hamilton, 
three for Madison. 
Rule 8. Cut-points for categories. If a cut is made between the rates 
for the nth and (n + 1)st papers, the cutoff point is defined to be the average 
of the rates for the two papers. These numbers, shown in Table 7.2-2, 
determine the boundaries for the categories, and are used for settling the 
categories for the words in the calibrating set and in the disputed papers. 

232 
THREE-CATEGORY ANALYSIS 
[7.2 
TABLE 7.2-2 
LOG ODDS AND CUT-POINTS FOR THE 3 CATEGORIES FOR 63 WORDS IN 6 GROUPS 
Low 
Middle 
High 
Cut-points in rate 
per 1000 words 
Group 1 
upon 
-3.18 
.00 
3.14 
.01 
1.87 
Group 2 
again 
.38 
-2.30 
.01 
although 
.43 
-2.40 
.01 
commonly 
.54 
2.20 
.01 
consequently 
.97 
-2.14 
.01 
considerable 
-
.69 
.96 
.35 
enough 
-1.02 
2.71 
.01 
throughout 
.15 
-
.25 
.01 
while 
-
.65 
2.40 
.01 
whilst 
1.06 
-2.89 
.01 
Group 3 
a 
-1.03 
.18 
.69 
19.97 
22.22 
also 
.45 
.18 
.77 
.385 
.60 
an 
-2.14 
.18 
1.61 
4.855 
5.755 
any 
-
.77 
.18 
.69 
1.65 
2.515 
as 
.64 
-1.32 
9.585 
at 
-1.03 
.18 
.69 
2.66 
3.415 
by 
1.61 
.18 
-2.14 
8.095 
9.635 
down 
.11 
.69 
.01 
if 
-1.32 
.56 
.69 
2.415 
3.185 
tn 
-
.77 
.56 
.96 
21.415 
24.88 
more 
.55 
.69 
3.52 
no 
1.25 
-
.56 
-1.03 
1.875 
2.89 
of 
-1.03 
.56 
1.25 
58.585 
62.73 
on 
1.67 
-
.41 
-1.67 
4.475 
5.855 
the 
.69 
-
.18 
-.77 
90.60 
95.53 
then 
.69 
-
.18 
-.77 
.01 
.595 
there 
-1.67 
.56 
2.08 
1.395 
2.15 
this 
-
.55 
.69 
7.965 
to 
~1.32 
.64 
34.675 
when 
.55 
.69 
1.03 
among 
.49 
-1.03 
.69 
because 
.56 
.51 
.69 
.01 
.505 
between 
.77 
.18 
.45 
.965 
1.61 
both 
.49 
-1.03 
.92 
either 
.85 
1.25 
1.005 
often 
-.69 
.96 
.38 
same 
.45 
.18 
-.77 
.98 
1.57 

7.2] 
DETAILS OF METHOD 
233 
TABLE 7.2-2 (cont.) 
LOG ODDS AND CUT-POINTS FOR THE 3 CATEGORIES FOR 63 WORDS IN 6 GROUPS 
Low 
Middle 
High 
Cut-points in rate 
per 1000 words 
Group 3 (cont.) 
second 
.37 
.69 
.G1 
those 
.77 
.18 
.69 
1.84 
2.76 
under 
.69 
.55 
1.12 
Group 4 
have 
.77 
.35 
5.82 
~s 
.69 
.18 
.77 
10.525 
12.315 
our 
.55 
.69 
1.90 
their 
.77 
.35 
4.27 
would 
.77 
.56 
.96 
2.825 
5.365 
Group 5 
affect 
-
.57 
1.61 
.01 
direction 
-
.43 
1.39 
.01 
fortune 
.10 
-1.10 
.01 
innovation 
.45 
-2.40 
.01 
language 
.45 
-2.40 
.01 
rapid 
.29 
1.10 
.01 
vigor 
.50 
1.50 
.01 
violence 
.14 
-1.79 
.Ol 
voice 
.26 
-2.08 
.01 
Group 6 
city 
.39 
1.95 
.01 
contribute 
.27 
1.10 
.01 
defensive 
.34 
1.79 
.01 
destruction 
-
.51 
2.20 
.01 
function 
.32 
-2.20 
.01 
join 
.14 
-1.79 
.01 
offensive 
.39 
1.95 
.01 
pass 
.51 
-1.47 
.01 
violate 
.41 
-1.70 
.01 
DETERMINING THE LOG ODDS. After the rules have been applied, each word 
has a 2 X 3 or a 2 X 2 table, or it has been dIscarded. 
at has the following table of counts: 
Hamilton 
Madison 
Low 
5 
14 
Middle 
6 
5 
For example, the word 
High 
12 
6 

234 
THREE-CATEGORY ANALYSIS 
[7.3 
The cut-point between low and middle is 2.66 occurrences per 1000 words of 
text; between middle and high it is 3.415. The log odds for any word and 
category are assigned by the formula 
_ 1 
(number in Hamilton cell) 
A -
oge 
b' M d' 
11' 
num er In 
a Ison ce 
For the example of at, we have 
Category 
Low 
Middle 
High 
Log odds 
A = 10g(154) = -1.03 
A = log(!) = 
.18 
A = 10g(1!) = 
.69 
In the future if any Hamilton or Madison paper has a rate for at below 2.66, 
the paper is regarded as in the low category for that word and is given the score 
-1.03. For rates between 2.66 and 3.41, the score .18 is assigned to the paper, 
and for rates of 3.42 and above, the score is .69. To get a score for the paper, 
merely sum the scores assigned by the several words chosen as discriminators. 
After the rules were applied to our 117 words, 63 remained. Table 7.2-2 lists 
these words together with their cut-points and log odds. Sometimes only one 
cut-point and two log odds occur because the rules created a 2 X 2 instead of a 
2 X 3 table. Table 7.2-2 needs some explanation for words that have only 2 X 2 
tables. Under the heading Group 3, the word to has only one cut-point at 
34.675. It is now arbitrary whether we call the two categories low and middle, 
low and high, or middle and high. It happens that 34.675 was originally the cut-
point between low and middle, so it is listed this way. On the other hand, 
with very low-frequency words, such as those in groups 5 and 6, the low category 
may contain only rates of zero, and there may be no middle category. The log 
odds for the left category is then listed under middle rather than under low. 
The layout of the table is quite arbitrary; all one needs is the cut-points and the 
log odds in each category in order from left to right. 
7.3. 
GROUPS OF WORDS 
As in the other studies, the words used are divided into groups on the basis of 
their apparent contextuality, as follows. 
Group 1. The single word upon, because of its excellence and noncontextuality. 
Group 2. Nine highly regarded noncontextual words. 
Group 3. Thirty function words. 
Group 4. Five words: have, is, our, their, would. Words we regard as some-
what contextual, depending on tense, mood, and context. 
Grou,p 5. Nine meaningful words that we regard as less likely to be contextual 
than those in Group 6. 
Group 6. Nine meaningful words that we regard as dangerously contextual. 

7.4] 
RESULTS FOR THE SCREENING AND CALIBRATING SETS 
235 
7.4. RESULTS FOR THE SCREENING AND CALIBRATING 
SETS* 
We present the sums of log odds, paper by paper, and for each group of words, 
first for the screening set in Table 7.4-1, then for the calibrating set in Table 
7.4-2. 
Eighteen of Hamilton's screening papers come from inside The Federalist, 
five from outside. Fourteen of Madison's come from The Federalist, the other 11 
from outside. 
TABLE 7.4-1 
THREE-CATEGORY ANALYSIS: RESULTS FOR THE SCREENING SET 
Group 
Paper 
1 
2 
3 
4 
5 
6 
Total 
number 
Hamilton 
1 
3.14 
6.56 
8.66 
1.31 
3.79 
-
.52 
22.94 
6 
3.14 
5.47 
9.43 
1.52 
3.61 
9.00 
32.17 
7 
3.14 
2.83 
8.58 
3.04 
1.79 
3.95 
23.33 
8 
0 
8.21 
7.03 
2.17 
7.00 
4.32 
28.73 
9 
3.14 
8.52 
5.31 
.07 
2.82 
1.82 
21.68 
11 
3.14 
1.34 
14.30 
3.04 
3.00 
.85 
25.67 
12 
3.14 
5.47 
9.02 
3.04 
.58 
3.19 
24.44 
24 
3.14 
3.14 
9.61 
1.92 
1.43 
.21 
19.45 
26 
3.14 
2.79 
14.86 
1.92 
-
.39 
6.66 
28.98 
36 
3.14 
2.43 
10.84 
-1.39 
1.61 
-2.50 
14.13 
59 
0 
1.74 
12.33 
1.80 
-
.39 
2.19 
17.67 
65 
3.14 
8.52 
9.62 
.68 
-1.59 
-
.52 
19.85 
66 
3.14 
9.61 
8.03 
-
.59 
-
.39 
-
.52 
19.28 
70 
3.14 
11.26 
14.95 
-1.18 
1.61 
4.53 
34.31 
71 
0 
11.26 
3.05 
.68 
2.33 
-1.26 
16.06 
72 
3.14 
8.52 
5.28 
-
.19 
1.43 
-
.52 
17.66 
74 
3.14 
.09 
5.64 
-
.44 
3.61 
-2.50 
9.54 
77 
3.14 
8.52 
15.26 
.93 
1.79 
.85 
30.49 
101 
0 
2.83 
3.85 
.44 
3.00 
1.82 
11.94 
102 
3.14 
6.56 
4.74 
-1.39 
3.79 
4.53 
21.37 
111 
3.14 
.03 
9.81 
-2.18 
1.79 
1.43 
14.02 
112 
3.14 
3.76 
1.06 
-1.27 
1.79 
1.84 
10.32 
113 
0 
1.74 
1.33 
-
.66 
1.43 
4.53 
8.37 
X 
2.46 
5.27 
8.37 
.58 
1.98 
1.89 
20.54 
8 
1.32 
3.52 
4.21 
1.57 
1.82 
2.88 
7.36 
(cont.) 
* These calculations were supervised by Ivor Francis. 

236 
THRElil-CATEGORY ANALYSIS 
[7.4 
TABLE 7.4-1 (cont.) 
THREE-CATEGORY ANALYSIS: RESULTS FOR THE SCREENING SET 
Paper 
number 
10 
14 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
121 
122 
1~1 
132 
133 
141 
211 
212 
213 
214 
215 
~ 
8 
1 
-3.18 
-3.18 
o 
o 
-3.18 
-3.18 
-3.18 
o 
-3.18 
-3.18 
-3.18 
-3.18 
-3.18 
-3.18 
-3.18 
-3.18 
-3.18 
o 
-3.18 
-3.18 
-3.18 
o 
-3.18 
-3.18 
-3.18 
-2.54 
1.30 
2 
-5.70 
-7.37 
-9.80 
-9.80 
-2.59 
-
.31 
-4.89 
-4.60 
-9.80 
-6.97 
-8.55 
-4.26 
-2.99 
-2.74 
-1.12 
-12.88 
-9.80 
-9.65 
-9.65 
-3.02 
-6.97 
-5.32 
-6.88 
-9.65 
-9.37 
-6.59 
3.30 
Group 
3 
4 
Madison 
-11.23 
-12.30 
-18.12 
-14.46 
-16.40 
-16.18 
-14.52 
-13.98 
-22.96 
-20.79 
-
9.31 
-10.59 
-20.56 
-
9.31 
-10.82 
-
9.88 
-
6.30 
-11.60 
-
7.85 
-
.82 
-18.62 
-15.84 
-18.97 
-
5.54 
~16.11 
-13.32 
5.33 
-1.27 
.44 
.44 
-1.18 
-2.30 
.07 
-1.92 
.34 
-1.71 
.93 
-1.71 
.68 
-3.63 
-1.27 
1.31 
.40 
-2.30 
-3.63 
-1.90 
-1.90 
-2.51 
-3.63 
-1.96 
-2.17 
-3.63 
-1.36 
1.54 
5 
6 
Total 
-5.86 
-4.61 
-31.85 
-8.43 
-6.95 
-37.79 
-6.09 
-2.50 
-36.07 
-6.37 
-5.02 
-36.83 
-3.24 
-2.50 
-30.21 
-5.58 
-2.63 
-27.81 
-4.44 
-
.52 
-29.47 
-5.58 
-5.15 
-28.97 
-7.51 
-4.61 
-49.77 
-1.42 
-4.61 
-36.04 
-2.73 
-
.52 
-26.00 
-6.09 
-2.63 
-26.07 
-3.24 
-9.06 
-42.66 
-
.39 
-7.13 
-24.02 
-5.32 
-2.50 
-21.63 
-1.52 
-4.43 
-31.49 
-
.39 
-9.06 
-31.03 
-
.39 
-4.97 
-30.24 
-1.06 
-3.04; 
-26.68 
-5.58 
-3.02 
-1'7.52 
-
.39 
-
.52 
-32.19 
-3.91 
-2.63 
-31.33 
-1.42 
-2.50 
-34.91 
-3.24 
-
.52 
-24.30 
-3.24 
-
.52 
-36.05 
-3.74 
-3.69 
-31.24 
2.43 
2.49 
6.80 
In Table 7.4-1, all Hamilton papers have positive sums of log odds, and all 
Madison's have negative sums. The smallest Hamilton sum is 8.4, the most 
nearly positive of Madison's, -17.5. This is a· considerable disparity if the log 
odds are to be taken seriously, and if there is no regression effe()t. 
The Hamilton calibrating set is entirely composed of Federalist papers, the 
Madi!Son of non-Federalists. This last is a definite weakness of the study. We 
are using non-Federalist papers to calibrate our results when the papers we plan 
to discriminate are from The Federalist. If we had both interior and exterior 
papers, that would not be so bad, or if a serious effort were made to eliIninate 

7.4] 
RESULTS FOR THE SCREENING AND CALIBRATING SETS 
237 
TABLE 7.4-2 
THREE-CATEGORY ANALYSIS: RESULTS FOR THE CALIBRATING SET 
Group 
Paper 
1 
2 
3 
4 
5 
6 
Total 
number 
Hamilton 
13 
3.14 
3.82 
2.85 
-
.44 
-
.39 
-2.45 
6.53 
15 
3.14 
3.14 
2.68 
-1.06 
-3.76 
1.97 
6.11 
16 
3.14 
8.21 
10.90 
1.80 
1.79 
-2.50 
23.34 
17 
3.14 
2.83 
6.42 
.68 
3.43 
.85 
17.35 
21 
3.14 
3.14 
3.25 
-1.90 
-2.32 
2.19 
7.50 
22 
3.14 
8.21 
10.32 
.46 
-
.48 
-4.61 
17.04 
23 
3.14 
-
.31 
.12 
.44 
3.27 
-4.61 
2.05 
25 
0 
-
.94 
-4.78 
1.92 
3.43 
-
.52 
-
.89 
27 
3.14 
4.48 
5.57 
-2.17 
-2.32 
-
.52 
8.18 
28 
3.14 
.09 
8.99 
1.80 
-
.39 
-
.52 
13.11 
29 
3.14 
4.79 
7.09 
.46 
-
.50 
-5.02 
9.96 
30 
3.14 
2.83 
3.44 
-
.44 
.41 
-2.44 
6.94 
31 
3.14 
2.83 
4.70 
-1.18 
-2.32 
.21 
7.38 
32 
0 
-2.74 
-
.50 
-1.31 
1.61 
-
.52 
-3.46 
33 
3.14 
-2.59 
1.67 
-
.78 
.39 
-2.50 
-1.45 
34 
3.14 
2.11 
8.72 
3.04 
-
.39 
1.82 
18.44 
35 
3.14 
2.83 
5.50 
.78 
.59 
-
.52 
10.76 
60 
3.14 
1.74 
11.14 
-
.78 
-1.06 
-
.29 
13.89 
61 
3.14 
.09 
.57 
.68 
-
.39 
1.82 
5.91 
67 
3.14 
.09 
-3.72 
-2.30 
-3.24 
-
.52 
-6.55 
68 
0 
1.74 
1.67 
-
.59 
-
.39 
-
.52 
1.91 
69 
3.14 
4.39 
3.04 
.78 
3.61 
-2.50 
10.90 
73 
3.14 
1.74 
7.06 
.19 
1.61 
-2.50 
10.86 
75 
3.14 
4.79 
5.54 
.68 
-1.59 
-1.67 
10.89 
76 
3.14 
7.53 
6.44 
.68 
-
.39 
-
.52 
16.88 
>:. 
2.76 
2.59 
4.35 
-
.08 
-
.02 
-1.06 
8.54 
8 
1.04 
2.91 
4.19 
1.33 
2.06 
2.02 
7.30 
(cont.) 
words with contextuality as in the main study, matters would be more satisfac-
tory. As it is, we have two black marks against the technique as executed. 
In the calibrating set, the lowest log odds for a Hamilton paper is -6.6, the 
highest for a Madison is -4.3. Thus the two sets of scores overlap. Indeed, 
two Madison scores exceed the lowest Hamilton. The mean of the Hamilton 
calibrating set is 8.5, and that of the Madison set is -19.3, with midpoint at 
-5.4. One Hamilton and one Madison are on the wrong side of this midpoint, 
an observed error rate of about 1 in 25 if we disregard the unreliability of the 
midpoint itself. 

238 
THREE-CATEGORY ANALYSIS 
[7.4 
TABLE 7.4-2 (cant.) 
THREE-CATEGORY ANALYSIS: RESULTS FOR THE CALIBRATING SET 
Group 
Paper 
1 
2 
3 
4 
5 
6 
Total 
number 
Madison 
134 
-3.18 
-
.13 
.80 
-3.42 
-
.39 
-
.52 
-
6.84 
135 
-3.18 
-3.86 
.37 
.46 
-3.91 
2.02 
-
8.84 
201 
-3.18 
-1.37 
-12.67 
-3.42 
3.79 
-
.52 
-17.37 
202 
0 
-3.02 
-
4.66 
-3.63 
1.79 
-2.50 
-12.02 
203 
-3.18 
-5.70 
-
6.78 
-1.96 
-3.24 
-
.52 
-21.38 
204 
-3.18 
-9.65 
-
8.09 
-1.71 
-
.39 
-2.27 
-25.29 
205 
-3.18 
-5.70 
-
3.63 
-2.17 
-1.24 
1.61 
-14.31 
206 
0 
-8.53 
-
2.90 
-1.96 
-
.39 
-
.52 
-14.30 
207 
-3.18 
-6.47 
-
8.16 
-1.39 
-3.24 
-
.52 
-22.96 
208 
-3.18 
-5.82 
-19.39 
-1.96 
-2.73 
-2.50 
-35.58 
209 
0 
-8.55 
-14.51 
-3.63 
-3.24 
-2.45 
-32.38 
210 
-3.18 
-8.93 
-12.66 
-2.17 
.33 
-4.61 
-31.22 
216 
-3.18 
-
.81 
.52 
-3.42 
1.79 
-
.52 
-
5.62 
217 
-3.18 
-5.42 
-
4.73 
-3.42 
-4.44 
-2.50 
-23.69 
218 
-3.18 
-9.37 
-
9.01 
-3.63 
-
.39 
1.61 
-23.97 
219 
-3.18 
-6.97 
-
7.89 
-1.~2 
-3.91 
-2.50 
-26.37 
220 
-3.18 
-3.49 
-2.64 
-1.92 
-
.39 
-
.52 
-12.14 
301 
-3.18 
-6.97 
-
9.63 
.06 
-
.39~ 
3.56 
-16.55 
302 
-3.18 
-8.40 
-
9.15 
1.92 
-1.06 
-
.52 
-20.39 
311 
-3.18 
-6.97 
-10.39 
.93 
-
.73 
-
.52 
-20.86 
312 
-3.18 
-2.59 
-10.58 
-2.51 
-
.39 
-5.15 
-24.40 
313 
-3.18 
2.79 
-
3.27 
-1.92 
1.79 
-
.52 
-
4.31 
314 
-3.18 
-4.26 
-
8.95 
-2.30 
-5.51 
-
.29 
-24.49 
315 
-3.18 
-10.05 
-
7.13 
-1.06 
-1.34 
-
.37 
-23.13 
316 
0 
-5.04 
-
5.12 
-2.51 
-
.91 
-
.52 
-14.10 
X 
-2.67 
-5.41 
-
7.24 
-1.95 
-1.15 
-
.88 
-19.30 
8 
1.19 
3.28 
4.81 
1.48 
2.21 
1.93 
8.32 
Recall that negative scores for Hamilton do not automatically misclassify Ham-
ilton papers, because we plan to assign the disputed papers on the basis of the 
calibrating set, even though the score assigned by any word to a paper was 
initially introduced as an estimated log odds. 
Nevertheless, if one did wish to pursue the original interpretation, he would 
search for some reason why the Hamilton numbers, though generally positive, 
are not as large as the absolute magnitudes of Madison's numbers. 
This anomaly is largely due, we believe, to the discrepancy in the numbers of 
papers. The screening set has 23 Hamilton and 25 Madison papers. To adjust 

7.5] 
REGRESSION EFFECTS 
for this inequality, since there are 63 words, we would add about 
63 log ~3 = 6310g(l + 223 ) "'" 63(223) "'" 5.2 
239 
to the total for every paper. Since the current midpoint is -5.4, the addition of 
5.2 would just about bring the group means to positions equally far from zero. 
7.5. REGRESSION EFFECTS 
7.5A. 
Word group. 
How much regression effect did we get, group by 
group? For each word group and for each set of papers, we computed as a 
measure of discrimination: 
where X is a mean, and s is a sample standard deviation of the scores for word 
groups (or, when the formula is applied in Section 7.5B, for single words). 
In Table 7.5-1 we show the results. Group 1, upon, improved by a standard 
deviation, groups 2 and 3 deflated as anticipated, group 4 did as well as ever, 
group 5 almost collapsed, and group 6 collapsed utterly. Recall that group 6 
contains what we feared were dangerously contextual and meaningful words. 
That group 4 held up is a surprise, and that group 5 did so poorly is a dis-
appointment. 
A separation of means by 3.6 standard deviations has the interpretation 
that the cutoff point for decisions would fall about 1.8 standard deviations from 
each mean. With no sampling error, this would imply errors of about three or 
four per cent in classifying new papers, which is what we have already observed. 
One would, of course, like to eliminate at least group 6 from further con-
sideration, but there is a modest danger in such a step; we would superimpose 
a selection effect on the calibrating set, and we would rather avoid that. 
TABLE 7.5-1 
SHOWING REGRESSION EFFECT 
FROM SCREENING SET TO CALIBRATING SET 
Values of d 
Group 
Screening set 
Calibrating set 
1 
3.8 
4.9 
2 
3.5 
2.6 
3 
4.5 
2.6 
4 
1.2 
1.3 
5 
2.7 
.5 
6 
2.1 
-
.1 
Total for 
6 groups 
7.3 
3.6 

240 
THREE-CATEGORY ANALYSIS 
[7.5 
TABLE 7.5-2 
REGHESSION EFFECT MEASURES, WORD BY WORD 
dB 
d. 
dB 
d. 
Group 1 
Group 3 (cont.) 
upon 
3.81 
4.87 
between 
.55 
-
.49 
Group 2 
both 
.74 
.70 
either 
1.03 
.00 
again 
1.00 
.95 
often 
.79 
.33 
although 
1.60 
.83 
.55 
.40 
commonly 
1.00 
1.35 
same 
second 
.59 
.34 
consequently 
1.55 
2.21 
those 
.64 
.41 
considerable 
.80 
.64 
under 
.60 
-
.41 
enough 
2.44 
.00 
throughout 
.. 52 
.20 
Group 4 
while 
1.72 
.55 
have 
.54 
.91 
whilst 
2.87 
2.41 
is 
.68 
.41 
our 
.60 
.20 
Group 3 
their 
.54 
.00 
a 
.71 
.46 
would 
.80 
1.69 
also 
.54 
.78 
Group 5 
an 
1.66 
.58 
any 
.64 
.18 
a:ffect 
.91 
-
.18 
as 
.96 
.58 
direction 
.68 
.63 
at 
.77 
.58 
fortune 
.51 
-
.30 
by 
1.16 
1.12 
innovation 
1.11 
1.32 
down 
.34 
.13 
language 
1.11 
.41 
if 
.96 
.40 
rapid 
.57 
-
.58 
in 
.80 
-
.09 
vigor 
.80 
.29 
more 
.60 
.43 
violence 
.98 
-
.53 
no 
1.04 
.02 
voice 
1.34 
.22 
of 
1.04 
.48 
Group 6 
on 
1.25 
3.12 
city 
1.23 
.00 
the 
.64 
-
.32 
contribution 
.90 
.17 
then 
.64 
1.26 
defensive 
.68 
-
.43 
there 
1.70 
1.09 
destruction . 
1.43 
.30 
this 
.60 
.73 
function 
.90 
-
.30 
to 
.98 
.58 
join 
.98 
.17 
when 
.60 
.76 
offensive 
1.16 
.17 
among 
.74 
.25 
pass 
.75 
-
.17 
because 
.64 
.23 
violate 
.89 
.00 

7.6] 
Quarter 
Lowest 
Second 
Third 
Highest 
RESULTS FOR THE JOINT AND DISPUTED PAPERS 
TABLE 7.5-3 
FRACTION OF WORDS IN EACH GROUP WITH VALUES 
OF dB -
de LYING I~ FOUR QUARTERS OF THE 
DISTRIBUTION OF THESE VALUES FOR ALL 63 WORDS 
Group 1 
Group 2 
Group 3 
Group 4 
Group 5 
1.00 
.45 
.23 
.40 
.22 
.22 
.33 
.60 
.11 
.11 
.20 
0 
.22 
.22 
.23 
0 
.45 
241 
Group 6 
.56 
.44 
7.SB. The regression effect by single words. Statistical literature is not 
cluttered with many empirical examples of the regression effect studied by single 
variables. Such a study supplements the discussion in Section 7.5A. For each 
word, we computed the discrimination measure d of Section 7.5A, both for the 
screening set (dB) and the calibrating set (de). In Table 7.5-2, we show the 
results for single words. The computations were supervised by P.S.R.S. Rao. 
One would suppose that the regression effect would imply that de < dB for 
most words, and so it is. But we want to know whether some words stay high, 
or whether all words deteriorate rather systematically. To find this out, we 
constructed a frequency distribution of the difference dB -
de for the 63 words, 
and located the quartile points of this distribution. Then, the fraction of words 
in each group that had (dB -
de) values lying in each of the four quarters was 
determined. The results are shown in Table 7.5-3. The cutting points are 
.175, .542, 1.003. 
The pattern of deterioration is much as would be expected from the regression 
effects by word groups already discussed. Thus, in group 4, all words are below 
the median in the index of deterioration, ds -
dc• We do see some difference, 
however, between the ways in which words of group 5 and those of group 6 
deteriorated. In the former, the words were more variable in their amount of 
deterioration, one-third of them being below the median and two-thirds above 
the median. In group 6, on the other hand, all words showed an above-median 
index of deterioration. It may also be noted in passing that the deflation of 
group 2 is largely due to the drastic deterioration of two words: enough and 
while; the other 7 words held up fairly well. 
7.6. RESULTS FOR THE JOINT AND DISPUTED PAPERS 
We now apply the method, as described at the end of Section 7.2, to the joint 
and disputed papers. The resulting scores are shown in Table 7.6-1. These 
scores could be treated by any of the techniques described in Section 5.5 for 
quantitative assessment. Here we merely compare the score for each paper 
with the midpoint, -5.4, between the Hamilton and Madison means in the 
calibrating set. 

242 
THREE-CATEGORY ANALYSIS 
[7.6 
TABLE 7.6-1 
RESULTS FOR THE JOINT AND DISPUTED PAPERS 
IN THE THREE-CATEGORY ANALYSIS 
Group 
Paper 
1 
2 
3 
4 
5 
6 
Total 
number 
Joint 
18 
0 
1.12 
-11.40 
-
.84 
-2.32 
2.62 
-10.82 
19 
-3.18 
-1.12 
-16.43 
-1.05 
1.43 
-
.29 
-20.64 
20 
0 
1.34 
-10.72 
.07 
-
.32 
1.82 
-
7.81 
Disputed 
49 
-3.18 
-6.97 
-11.34 
.68 
-2.73 
-
.52 
-24.06 
50 
-3.18 
-3.14 
-
6.34 
1.80 
-
.39 
-2.63 
-13.88 
51 
-3.18 
-6.97 
-19.41 
-1.96 
-4.66 
-3.04 
-39.22 
52 
-3.18 
-
.31 
-
9.08 
.28 
1.79 
-2.50 
-13.00 
53 
-3.18 
-9.77 
-11.06 
-3.63 
-3.24 
.85 
-30.03 
54 
0 
-4.20 
-16.42 
.06 
-3.52 
-2.45 
-26.53 
55 
-3.18 
1.74 
1.02 
'- .47 
-1.59 
-2.50 
-
4.98 
56 
-3.18 
-2.61 
-
5.16 
-1.05 
-
.39 
-
.52 
-12.91 
57 
-3.18 
-3.86 
-
9.34 
~2.51 
-4.44 
-
.29 
-23.62 
58 
-3.18 
-5.85 
-13.10 
1.80 
-
.39 
-2.89 
-23.61 
62 
-3.18 
-1.37 
-12.95 
--:-2.51 
-2.54 
-2.50 
-25.05 
63 
-3.18 
-1.12 
-10.64 
-
.59 
.59 
-3.04 
-17.98 
The story is much the same as in the other studies. Except for paper 55, 
with the score -5.0, all scores fall on Madison's side of the fence; and -5.0 is 
so close to the cutting point that it is difficult to judge. The score next closest 
to the cutting point is -7.8, for one of the joint-authorship papers, No. 20. 

CHAPTER 8 
Other Studies 
While working on the general discrimination problem, we did some studies 
that do not fall in the main line of research reported in this volume. Some of 
these studies are reported here. 
Section 8.2 suggests a method for getting 
started on authorship problems; the method draws on our experience but does 
not employ all our paraphernalia. 
8.1. HOW WORD RATES VARY FROM ONE TEXT TO 
ANOTHER 
To use word rates in assessing authorship, we need a table of rates from a 
variety of sources to help find distinctive patterns of rates for a new author. In 
Table 8.1-1, we supply, for our basic 165 words, rates of use in several sources: 
1. Alexander Hamilton, 93,954 words: text of the 48 papers used in our 
Federalist study. 
2. James Madison, 113,597 words: text of the 50 papers used in our Federalist 
study. 
3. John Ja,y, 5,379 words: text of papers Nos. 2, 3, and 64 of The Federalist. 
4. 
Counts for the function words of the Miller-Newman-Friedman list, 
36,299 words: King James version of the Holy Bible, Isaiah, Chapters 27-36, 
inclusive, Ruth, Chapters 1-4 through verse 17, Second Corinthians; William 
James, "Talks to Teachers," pp. 3-63; The Atlantic, "The Atlantic Reports," 
April, 1957, pp. 4:-24, May, 1957, pp. 4-15. 
5. James Joyce, Ulysses, 260,430 words: the Hanley Concordance (1937). 
6. King James version of the Holy Bible, 774,746 words (Brewer, p. 105): 
the Strong Concordance (1890). 
The rates for 33 very high-frequency words in the Bible were estimated from 
a stratified sample of about l of the total count provided in the Strong Concord-
ance. The calculations were performed by Robert Kleyle and Marie Yeager. 
243 

244 
OTHER STUDIES 
[8.1 
TABLE 8.1-1 
RATES PER 1000 VVORDS 
Miller-
Word 
Newman-
Joyce's 
number 
Word 
Hamilton 
Madison 
Jay 
Friedman 
Ulysses 
Bible 
1 
a 
22.85 
20.22 
13.57 
18.48 
24.59 
9.63* 
2 
all 
3.68 
3.39 
2.79 
4.60 
5.04 
7.08 
3 
also 
.28 
.70 
1.30 
2.26 
.33 
2.23 
4 
an 
5.96 
4.57 
1.86 
3.39 
2,47 
4.52* 
5 
and 
24.50 
27.55 
45.36 
36.06 
27.53 
66.90* 
6 
any 
2.86 
2.39 
2.60 
1.6& 
.74 
1.15 
7 
are 
4.94 
5.02 
7.25 
7.27 
1.98 
3.75* 
8 
a8 
8.46 
10.61 
14 .. 69 
8.24 
4.52 
4.61* 
9 
at 
3.62 
3.32 
2.97 
3.25 
4.92 
1.98 
10 
be 
20.06 
16,45 
19.15 
11.02 
3.33 
8.89* 
11 
been 
4.14 
4.16 
2.60 
1.74 
.79 
,41 
12 
but 
3.48 
3.49 
5.02 
6.20 
2.75 
5.24* 
13 
by 
7.34 
11.44 
10.60 
5.68 
4.84 
3.18* 
14 
can 
2.55 
1.65 
2.04 
1.54 
.78 
.30 
15 
do 
,48 
,46 
.56 
1.76 
1.67 
1.70 
16 
down 
.13 
.24 
1.29 
1.74 
1.44 
17 
even 
1.02 
.84 
.37 
1.60 
.48 
1.76 
18 
every 
1,48 
1.86 
.93 
1.24 
.q4 
1.52 
1~ 
for 
6.40 
5.73 
7.06 
11.49 
7.57 
11.33* 
20 
from 
5.49 
5.65 
5.76 
3.97 
4.15 
4.68* 
21 
had 
1.44 
2.02 
1.30 
1.74 
3.12 
2.56 
22 
has 
3.35 
3.36 
2.97 
2.62 
1.11 
23 
have 
6.73 
5.46 
7.99 
5.34 
2.64 
4.03 
24 
her 
,65 
2,49 
1.84 
6.82 
2.35* 
25 
his 
2.33 
2.93 
.56 
4.71 
12.77 
10.97* 
26 
if 
3.59 
2.72 
3.35 
2.95 
2.08 
1.96 
27 
in 
24.37 
23.0[j 
20.82 
22.98 
18.75 
16.56* 
28 
into 
1.52 
1.98 
1.30 
1.82 
1.29 
2.57 
29 
is 
11.70 
12.76 
7.44 
13.20 
5.51 
9.18* 
30 
it 
13.82 
13.34 
16.54 
9.75 
9.02 
7.34* 
31 
its 
3.90 
3,49 
1.12 
1.96 
1.73 
.00 
32 
may 
3.81 
3.86 
4.09 
2.67 
.36 
1.30 
33 
more 
2.71 
3.23 
4.09 
2.75 
1.20 
.87 
34 
must 
2.~9 
2.04 
1.86 
1.29 
.84 
.16 
35 
my 
.21 
.06 
.18 
2.07 
3.21 
5.21* 
36 
no 
2.17 
2.91 
1.30 
2.78 
2.46 
1.77 
37 
not 
6.39 
7.95 
7.99 
8.13 
3.42 
8.35* 
38 
now 
.37 
.63 
.74 
1.96 
1.69 
1.71 
39 
of 
64.65 
57.80 
43.87 
40.74 
29.90 
45.84* 
40 
on 
3.28 
7.83 
5.20 
4.79 
8.04 
2.47 

8.1] 
HOW WORD RATES VARY FROM ONE TEXT TO ANOTHER 
245 
TABLE 8.1-1 (cont.) 
MilIer-
Word 
Newman-
Joyce's 
number 
Word 
Hamilton 
Madison 
Jay 
Friedman 
Ulysses 
Bible 
41 
one 
2.45 
2.83 
4.09 
3.42 
2.71 
2.42 
42 
only 
1.36 
1.69 
3.53 
1.38 
.97 
.30 
43 
or 
6.96 
6.26 
10.41 
4.19 
3.80 
1.31 
44 
our 
2.27 
1.11 
2.42 
3.66 
1.10 
1.61 * 
45 
shall 
.98 
.71 
.56 
7.41 
.26 
11.93* 
46 
should 
2.08 
1.35 
4.09 
1.32 
.31 
1.00 
47 
so 
2.14 
2.16 
3.90 
2.64 
2.38 
2.11 
48 
some 
1.19 
1.21 
1.86 
1.27 
1.22 
.28 
49 
such 
1.88 
2.30 
3.72 
1.35 
.49 
.31 
50 
than 
2.90 
2.60 
2.79 
1.68 
.63 
.57 
51 
that 
14.98 
14.37 
20.45 
14.54 
11.83 
16.47* 
52 
the 
91.27 
93.65 
67.48 
71.71 
57.12 
84.67* 
53 
their 
5.27 
5.21 
9.85 
3.11 
2.73 
5.18* 
54 
then 
.33 
.93 
.37 
1.49 
2.18 
2.77 
55 
there 
3.28 
1.29 
1.49 
2.95 
2.70 
2.93 
56 
things 
.31 
.23 
.18 
1.84 
.41 
1.48 
57 
this 
7.80 
6.00 
5.76 
4.82 
1.69 
3.49 
58 
to 
40.71 
35.25 
35.69 
24.99 
18.84 
17.10* 
59 
up 
.29 
.17 
1.71 
3.01 
3.06 
60 
upon 
3.35 
.14 
.18 
1.68 
.50 
3.53* 
61 
was 
1.63 
3.79 
1.86 
3.80 
'8.16 
5.43* 
62 
were 
1.44 
1.93 
1.49 
1.79 
2.02 
3.51* 
63 
what 
1.38 
1.15 
.37 
1.96 
3.44 
1.22 
64 
when 
1.29 
.74 
1.86 
2.95 
2.13 
3.58 
65 
which 
10.53 
10.10 
6.69 
6.34 
2.00 
5.54 
66 
who 
2.45 
1.70 
5.39 
1.71 
1.86 
1.19 
67 
will 
6.16 
4.23 
9.48 
4.21 
1.34 
4.84 
68 
with 
6.21 
7.24 
6.69 
7.44 
9.62 
7.77* 
69 
would 
8.63 
3.56 
2.60 
2.53 
1.48 
.57 
70 
your 
.13 
.15 
2.51 
1.84 
2.11 
71 ' 
affect (ed) 
.18 
.15 
1.12 
** 
.02 
.01 
72 
again 
.05 
.33 
.69 
.92 
.85 
73 
although 
.01 
.21 
.56 
.22 
.01 
.02 
74 
among 
.39 
.84 
.56 
.77 
.22 
1.16 
75 
another 
.70 
1.04 
.18 
.77 
.46 
.56 
(cant.) 
*: Indicates an estimated frequency. 
**: Indicates either a word not considered to be a function word, or a function 
word omitted from the Miller-Newman-Friedman list. Only in the latter case is there 
evidence for interpreting ** as a zero rate . 
. 00: Indicates a rate less than .005 and strictly greater than O. 
-: Indicates a .zero rate arising from a zero frequency. 

246 
OTHER STUDIES 
[8.1 
TABLE 8.1-1 (cont.) 
Miller-
Word 
Newman- Joyce's 
number 
Word 
Hamilton Madison 
Jay 
Friedman Ulysses 
Bible 
76 
because 
.53 
.58 
1.30 
.88 
.83 
1.53 
77 
between 
1.43 
1.86 
.47 
.51 
.29 
78 
both 
.47 
1.08 
1.49 
.58 
.40 
.44 
79 
city ( cities) 
.15 
.02 
** 
.22 
1.68 
80 
commonly 
.22 
.01 
.18 
** 
.00 
.00 
81 
consequently 
.03 
.48 
.74 
** 
.00 
82 
considerable (ly) 
.40 
.14 
.18 
** 
.03 
83 
contribute 
.07 
.01 
** 
84 
defensive 
.22 
.04 
** 
85 
destruction 
.13 
.01 
** 
.02 
.12 
86 
did 
.11 
.27 
.37 
.58 
1.54 
1.28 
87 
direction 
.22 
.03 
.18 
** 
.04 
.00 
88 
disgracing 
.01 
** 
89 
either 
1.15 
.88 
1.49 
.30 
.14 
.05 
90 
enough 
.34 
.03 
.33 
.26 
.04 
91 
fortune(s) 
.04 
.11 
** 
.04 
92 
function(s) 
.06 
.18 
** 
.02 
93 
himself 
.35 
.51 
.52 
.84 
.67 
94 
innovation ( s) 
.01 
.18 
** 
95 
join(ed) 
.01 
.07 
.18 
** 
.05 
.07 
96 
language 
.04 
.21 
.18 
** 
.16 
.03 
97 
most 
1.98 
1.73 
3.72 
1.18 
.58 
.17 
98 
nor 
.62 
.68 
.56 
.41 
.23 
.88 
99 
offensive 
.18 
.01 
** 
.00 
100 
often 
.68 
.20 
1.49 
.66 
.21 
.02 
101 
pass(ed) (es) (ing) 
.18 
.24 
.74 
** 
.81 
1.24 
102 
perhaps 
.40 
.35 
.30 
.31 
.00 
103 
rapid 
.05 
.03 
** 
.02 
104 
same 
1.45 
2.07 
1.67 
.85 
.64 
.40 
105 
second 
.18 
.37 
.37 
.30 
.43 
.22 
106 
still 
.54 
.54 
.74 
.38 
.74 
.12 
107 
those 
2.90 
2.38 
4.28 
.66 
1.19 
.56 
108 
throughout 
.04 
.17 
.18 
.14 
.01 
.21 
109 
under 
1.27 
1.49 
2.60 
.72 
.86 
.48 
110 
vigor(ous) 
.21 
.05 
** 
.03 
111 
violate (s) (ed) (ing) 
.09 
.27 
** 
.02 
.00 
112 
violence 
.05 
.11 
.56 
** 
.02 
.07 
113 
voice 
.03 
.16 
.18 
** 
.67 
.64 
114 
where 
.72 
1.19 
.37 
.99 
1.17 
.50 
115 
whether 
.49 
.97 
1.67 
.72 
.14 
.21 
116 
while 
.28 
.02 
.37 
.47 
.48 
.27 
117 
whilst 
.48 
.06 
.01 
.01 

8.1] 
HOW WORD RATES VARY FROM ONE TEXT TO ANOTHER 
247 
TABLE 8.1-1 (cont.) 
Miller-
Word 
Newman- Joyce's 
number 
Word 
Hamilton Madison 
Jay 
Friedman Ulysses 
Bible 
118 
about 
.18 
.11 
1.10 
2.04 
.80 
119 
according 
.10 
.62 
** 
.06 
.96 
120 
adversaries 
.16 
.09 
** 
.05 
121 
after 
.21 
.33 
.18 
.94 
1.63 
1.45 
122 
aid 
.17 
.04 
** 
.05 
123 
always 
.63 
.16 
1.30 
.50 
.66 
.08 
124 
apt 
.32 
.04 
** 
.00 
.01 
125 
asserted 
.04 
.24 
** 
.00 
126 
before 
.30 
.55 
1.02 
.83 
2.28 
127 
being 
.69 
1.07 
.93 
1.05 
.66 
.37 
128 
better 
.34 
.17 
.18 
** 
.51 
.15 
129 
care 
.21 
.05 
.18 
** 
.17 
.02 
130 
choice 
.36 
.12 
.18 
** 
.05 
.03 
131 
common 
.79 
.38 
.18 
** 
.10 
.03 
132 
danger 
.69 
.43 
.56 
** 
.03 
.01 
133 
decide(s) (ed) (ing) 
.29 
.55 
.56 
** 
.02 
.00 
134 
degree 
.54 
.31 
.37 
** 
.04 
.01 
135 
during 
.04 
.44 
.08 
.14 
136 
expence(s) 
.15 
** 
.00 
137 
expense(s) 
.29 
.06 
** 
.02 
138 
extent 
.51 
.37 
** 
.02 
139 
follow ( s) (ed) (ing) 
.23 
.75 
.37 
** 
.44 
.30 
140 
I 
1.64 
.77 
1.30 
8.40 
10.20 
11.58* 
141 
imagine(s) (ed)(ing) 
.28 
.04 
** 
.12 
.02 
142 
intrust(s)( ed)( ing) 
.17 
.03 
** 
143 
kind 
.79 
.10 
.56 
** 
.40 
.08 
144 
large 
.46 
.21 
.56 
** 
.21 
.03 
145 
likely 
.55 
.18 
.37 
** 
.04 
146 
matter(s) 
.42 
.04 
.18 
** 
.28 
.13 
147 
moreover 
.11 
.22 
.01 
.21 
148 
necessary 
1.02 
.84 
.74 
** 
.05 
.01 
149 
necessity ( ies ) 
.93 
.53 
.37 
** 
.03 
.02 
150 
others 
.40 
.52 
1.30 
.25 
.27 
.09 
151 
particularly 
.09 
.42 
** 
.05 
.00 
152 
principle 
.63 
1.50 
.18 
** 
.02 
(cont.) 
*: Indicates an estimated frequency. 
**: Indicates either a word not considered to be a function word, or a function 
word omitted from the Miller-Newman-Friedman list. Only in the latter case is there 
evidence for interpreting ** as a zero rate . 
. 00: Indicates a rate less than .005 and strictly greater than O. 
-: Indicates a zero rate arising from a zero frequency. 

248 
OTHER STUDIES 
[8.1 
TABLE 8.1-1 (cont.) 
Miller-
Word 
Newman- Joyce's 
number 
Word 
Hamilton Madison 
Jay 
Friedman Ulysses 
Bible 
153 
probability 
.32 
.04 
** 
.02 
154 
proper 
.73 
.52 
1.12 
** 
.07 
.01 
155 
propriety 
.43 
.13 
.56 
** 
.00 
156 
provision ( s) 
.78 
.40 
** 
.01 
157 
requisite 
.32 
.11 
.37 
** 
.00 
158 
substance 
.10 
.07 
** 
.02 
.06 
159 
they 
4.64 
3.60 
10.97 
4.60 
3.88 
8.99* 
160 
though 
.94 
.49 
.18 
.77 
.65 
.30 
161 
truth(s) 
.45 
.31 
.56 
** 
.10 
.30 
162 
us 
1.42 
.49 
1.86 
2.59 
.97 
1.80* 
163 
usage(s) 
.02 
.12 
** 
.01 
164 
we 
3.03 
1.19 
1.86 
5.48 
1.61 
2.03* 
165 
work(s) 
.07 
.32 
** 
.32 
.83 
*: Indicates an estimated frequency. 
**: Indicates either a word not considered to be a function word, or a function 
word omitted from the Miller-Newman-Friedman list. Only in the latter case is there 
evidence for interpreting ** as a zero rate . 
. 00: Indicates a rate less than .005 and strictly greater than O. 
-: Indicates a zero rate arising from a zero frequency. 
To illustrate a potential use of the table, we compare the rates for Jay with 
those of Hamilton and Madison. 
For distinguishing Jay's writings, several 
high-frequency words stand out, as shown in Table 8.1-2. Since Jay's rates 
differ so much from those of Hamilton and Madison, we think his writing would 
be easily distinguished from theirs, even though Jay's sample is modest in length. 
TABLE 8.1-2 
RATES PER 1000 WORDS FOR A FEW HIGH-FREQUENCY WORDS 
Word: 
a 
and 
of 
or 
that 
the 
their which who 
will 
they 
Hamilton 
23 
24 
65 
7 
15 
91 
5 
11 
2 
6 
5 
Madison 
20 
28 
58 
6 
14 
94 
5 
10 
2 
4 
4 
Jay 
14 
45 
44 
10 
20 
67 
10 
7 
5 
9 
11 

8.2] 
MAKING SIMPLIFIED STUDIES OF AUTHORSHIP 
249 
8.2. 
MAKING SIMPLIFIED STUDIES OF AUTHORSHIP 
A man with an authorship problem may wish some advice on getting started. 
We give a few suggestions for a small-scale attack that might be widely effective. 
We suppose that you have identified a good deal of writing by each candidate 
for the authorship of an unknown piece. To prepare the material for counting 
words or any other variables, you will need to make hard and fast rules about 
the treatment of quotations, numbers, and the other special usages so that 
these rules can be applied impartially to all the writings, including the unknowns. 
Divide the writing of each author into pieces and make all initial counts 
separately for each piece, in order to be able to assess variation. It is easy enough 
to pool counts later but hard to take them apart. No rule determines how much 
text to count in one piece but our experience suggests that 1000 words may be a 
bit short, and that 3000 words would waste an opportunity to measure variation. 
Division of material into pieces whose lengths are nearly equal can greatly 
ease both informal and formal statistical analysis of the data. 
Choose, in advance, a moderate list of words as candidates. Except to obtain 
lengths of pieces, only listed words need to be counted in this basic procedure. 
A good general-purpose list could be obtained from our words, numbered 1 
through 70 (Table 2.5-2), by eliminating all pronouns and verbs, as well as any 
other word whose use might depend seriously on context that varies within the 
writings. 
Obtain for each word its rate per thousand words in each piece and in com-
posite by each author. Next, by comparing the overall rates for the several 
candidates with the rates in Table 8.1-1 and with one another's, you should 
find that a modest list of words emerges for distinguishing the several authors. 
Single words may only distinguish one or a few pairs of authors, and if there 
are many authors, it may be difficult to find a list capable of distinguishing all 
pairs. 
For each author consider the variation of word rates between pieces. If the 
variation is too large, the word is ineffective. Discard it. Form frequency dis-
tributions of rates similar to those displayed for Hamilton and Madison in 
Table 4.8-3. The less similar the authors' distributions, the better is the word as 
a discriminator. These distributions show you the range of rates strongly in-
dicative of an author, and the range of little value in separating the two authors. 
If comparisons are especially delicate because of unusual similarity between 
the authors, you may need statistical help to appraise the strength of the evi-
dence and the impact of piece-to-piece variation. If you anticipate such a delicate 
problem, we advise you to reserve about half of the writings of each author for 
use as a check on the choices based on the first half. 
Where the writings of an author fall into natural groupings, those groupings 
should be used to check for systematic variations from group to group. For 
example, different time periods or different kinds of writings-essays versus 
short stories-offer natural groupings. If the material of known authorship 

250 
OTHER STUDIES 
[8.2 
shows considerable variation between groups for only a few words, then those 
words should be discarded. If the differences are more general, then there may 
be merit in using as a criterion for an author those pieces most similar in time 
or kind to the unknown piece. Whether to mix the material from several groups 
or to use less, but match the unknown pieces better, can be a delicate practical 
question. As a methodological problem, the question is poorly put. The right 
question is "How should I use the several kinds of materials?" That question 
needs attention. 
You can add to the word list, but losses from selectivity, context, and in-
creased costs may offset gains. More of our 165 words might be included, and 
Table 8.1-1 furnishes rates for all these words by several authors. But the list 
of 165 contains many words whose rates may depend dangerously on context. 
Most of the words numbered 71 to 165 were chosen by screening studies that 
picked out of large pools of words those that seemed to discriminate between 
Hamilton and Madison. In a fresh problem, we cannot expect them to do much 
better than a set of words selected randomly. For example, the 20 words picked 
at random for inclusion in the Federalist study produced few discriminators. 
Unfortunately, to carry out a screening study comparable to those we used to 
get our 165 requires much more work. 
Including more function words like those retained from our first 70 might be 
a wiser expansion. The Miller-Newman-Friedman (1958) list is a convenient 
source of words with an indication of rates. Many of these words coincide with 
our 165. Adding to the list increases the danger from words that, by chance 
fluctuations, appear to discriminate. These troubles are~most severe for rare 
words. This partly explains why low-frequency words were so unproductive. 
The counting procedure, either by hand or computer, is simple with a small 
list of words. Compare a word of text with each word on the list, and, where 
appropriate, make a tally. Repeat for each word of text. This procedure is 
ideally suited for computers and is easily programmed (as programs go). To 
encode text on, say, punched cards is the most expensive part of the task. If it 
became cheap to transfer text from print to computer through, say, a reading 
machine, the main obstacle to authorship studies would vanish. 
To do the counting for a small list by hand, suppose that printed copies of 
text are available. Study one listed word at a time. Have two individuals 
independently record line-by-line on separate copies of text the number of occur-
rences of that word on each line. (To record for whole paragraphs or whole 
pages is much quicker but much less accurate. Inform the recorders that their 
work is being checked independently, and inform them of their errors.) Compare 
these records and correct the errors. 
To make a complete alphabetization and count of a text, such as we have 
described in Section 2.6, is the chief alternative. The complete count is logically 
a much more complex process and is a relatively difficult problem for the com-
puter. Although better programs surely exist now than we used, the difficulties 
and costs are greater than those for the count of a small list of words. The 

8.3] 
THE CAESAR LETTERS 
251 
complete count evidently gives more information. Yet, for this information to 
be useful for a screening procedure, the alphabetical count must be available 
for the whole pool of writing, not just separately for each piece. To assemble 
an alphabetical count for the whole pool from the alphabetical counts for pieces 
when each piece has 500 to 1000 different words is a huge task. 
Naturally, each new authorship problem brings special features that require 
attention. Still, our initial proposal for counts on a limited set of high-frequency 
words should suffice for many problems of unknown authorship. 
8.3. 
THE CAESAR LETTERS 
A pair of letters, published under the pseudonym Caesar (Ford, 1892, pp. 283-
291), has been attributed to Hamilton by Paul Leicester Ford (1892, p. 281), 
and, we understand, not to Hamilton by others. We pooled the two Caesar 
letters, after eliminating quotations, to make one 2698-word paper. First, we 
TABLE 8.3-1 
RATES PER 1000 WORDS FOR HIGH-FREQUENCY WORDS 
\Vord 
Caesar 
Hamilton 
Madison 
Jay 
*a 
13 
23 
20 
14 
*an 
3.0 
6.0 
4.6 
1.9 
and 
25 
24 
28 
45 
as 
10 
8.5 
11 
15 
but 
4.8 
3.5 
3.5 
5.0 
*by 
12 
7.3 
11 
11 
for 
8.5 
6.4 
5.7 
7.1 
from 
3.3 
5.5 
5.6 
5.8 
if 
5.5 
3.6 
2.7 
3.4 
*in 
16 
24 
23 
21 
it 
13 
14 
13 
17 
*no 
5.2 
2.2 
2.9 
1.3 
not 
8.5 
6.4 
8.0 
8.0 
*of 
45 
65 
58 
44 
*on 
7.8 
3.3 
7.8 
5.2 
*or 
3.0 
7.0 
6.3 
10.4 
*such 
5.5 
1.9 
2.3 
3.7 
*that 
20 
15 
14 
20 
*the 
64 
91 
94 
67 
*this 
13 
7.8 
6.0 
5.8 
to 
35 
41 
35 
36 
*which 
7.0 
11 
10 
6.7 
with 
8.2 
6.2 
7.2 
6.7 
----
* Caesar departs from Hamilton by two or more Poisson standard deviations. 

252 
OTHER STUDIES 
[8.4 
analyzed it as if it were a disputed paper in the main study. The log odds given 
by the negative binomial, set 31, is -4.2. Naturally, this is not a strong Madi-
son indicator, but it does provide strong evidence against authprship by Hamil-
ton. If this long paper had been written by Hamilton, we would expect positive 
log odds in the teens or twenties. As far as we know, no one has even suggested 
that Madison has a connection with these letters. 
Second, let us compare Caesar's rates for high-frequency words with those of 
Table 8.1-1. We arbitrarily looked at all words for which Caesar, Jay, Hamilton, 
or Madison had a rate of at least 5 per thousand. From these, we eliminated 
those that seemed highly contextual: are, be, have, is, their, who, will, would, 
they, I. The remainder are shown in Table 8.3-1. For over half of these words 
the Caesar rate departs from Hamilton's by more than two standard deviations, 
using Poisson theory. In addition, the rate for upon is only 1.1 per thousand, 
which is a good way from Hamilton's 3 per thousand in a 2700-word paper, 
i.e., only three occurrences instead of eight. All told, then, the evidence is 
strongly against Hamilton's authorship of these letters. 
8.4. 
FURTHER ANALYSIS OF PAPER NO. 20 
Since paper No. 20 has log odds nearer zero than the other joint papers, it is 
natural to search for Hamilton's contribution. 
(Recall that shortness could 
partly account for the small log odds.) Bourne (1901, p. 116) says of No. 20: 
"Fully nine-tenths of it is drawn from Madison's own abstract of Sir William 
Temple's Observations upon the United Provinces and of Felice's Code de 
l'Humanite . ... Sir William Temple's claim to be recognized as joint author 
of No. 20 is far stronger than Hamilton's. There are two paragraphs out of 
twenty-four in No. 20 which appear to have come from Hamilton." 
We studied the Hamilton marker words in the hope of locating a connected 
chunk of text that could be attributed to Hamilton. The paper contains 7 
Hamilton markers and 3 Madison markers [Hamilton: upon, considerable, 
matters (2), vigorous, though, always; Madison: both, also (2)]. Of the 7 Hamilton 
markers, we were able to trace 3 [considerable, matters (2)] to Madison's Notes 
of Ancient and Modern Confederacies, Preparatory to the Federal Convention of 
1787. The other four Hamilton markers appear in three paragraphs of edi-
torializing interspersed among the more technical material drawn from the Notes 
and from Temple and Felice. One use of matters is in a note drawn from Temple. 
Although we located the lines of the source of the note and observed that Temple 
uses matter fairly often, and near the particular place, still, matters was not in the 
sentence that matches the note. 
We did not try to pursue the other two Hamilton markers back to Felice. 
The main point seems to be that we do have positive evidence that three of the 
markers were provided by Madison and not Hamilton, though Madison may 
have been influenced in their use by Temple or Felice. 

8.5] 
HOW WORDS ARE USED 
253 
The other four Hamilton markers might represent material used by Madison 
from the notes he said that Hamilton gave him. Examination of the high-
frequency words in the three paragraphs containing these markers does not 
support attribution of the full paragraphs to Hamilton. Substantially fewer of 
the high-fre<;tuency words used more by Hamilton are in these paragraphs than 
would be expected if Hamilton were the author. Their count fits Madison's 
writings more closely. On the other hand, the high-frequency words used more 
by Madison provide much less evidence, but what little there is goes in Ham-
ilton's favor. 
None of these remarks contradict the possibility that a small 
amount of Hamilton material was reworked by Madison. We say small because 
so much is from Madison's Notes. Nor do we contradict the possibility that 
Madison did not use Hamilton's notes. An author who is heavily using the 
writings of another, as Madison used those of Felice and Temple, may not follow 
his own habits as closely as usual. 
Our work on paper No. 20 was so unrewarding that we decided not to pursue 
Nos. 18 and 19. 
8.5. HOW WORDS ARE USED 
Many have suggested that the meanings of words should provide additional 
discrimination. Unfortunately for this suggestion, the marker words are chosen 
in large part because one author uses the 'marker and the other rarely does. 
Counting the sorts of uses of a word that has occurred, say four times, is unlikely 
to lead to a thrilling revelation. Still, one of the few times Madison used while 
was in the expression worth while, so something can be learned. Joanna F. 
Handlin carried out the study for 22 marker words and probably. She used 
those papers for which our alphabetical counts gave the location of each word. 
We describe a few findings. 
In these papers, upon appeared 216 times for Hamilton and 15 times for 
Madison. 
Although Hamilton used upon in 87 different phrases (upon the 
principle, upon the credit of, framed upon, break in upon, insist upon, predict 
upon, ... ), still Madison came up with three (upon the subject, take upon, 
plan upon) that Hamilton did not use. 
The word both was used in three ways as shown in the table below. 
As reference to antecedent: 
(I like A and B, so I shall have both.) 
As a modifier of plural noun or pronoun: 
(Both boys arrived.) 
As a modifier of conjunctive phrase or clause: 
(Both to create and to think, both red and black.) 
Hamilton 
7 
5 
15 
* An additional five occurrences repeated both for rhetorical effect. 
Madison 
16 
23* 
39 

254 
OTHER STUDIES 
[8.5 
The word probably had two uses, to weaken predictions (the war would prob-
ably destroy the Union), and to weaken interpretations (the convention prob-
ably foresaw ... ). 
Probably 
Weakens predictions 
Weakens interpretations 
Hamilton 
7 
1 
Madison 
1 
20 
The split in this table suggests that the meaning as well as frequency of probably 
might be used as a discriminator. One danger is that of regression effect, with-
out any mitigating way of assessing the priors. The point is that in choosing 
these categories, the attempt is to find a method of classification that does 
discriminate between the authors. It is hard to measure the number of methods 
that were tried before this one was chosen, or that would have been tried if a good 
division had not occurred. One feels the need for fresh material for validating 
and calibrating. The reliability of classification may need checking. New text 
is not readily available for these low-frequency words, but it would be available 
for high-frequency words. A second danger is context. If one talks of the future, 
one may use probably to qualify forecasts; ·if one interprets the past, one may use 
probably for that. Protection from context is hard to achieve, and this is why 
we have leaned toward function words. 
Similarly, the splits for though are encouraging. Though introduces adversative 
clauses (Though I cannot spare it, I like to spend time with my friends.) and 
concessive clauses (Though I like her sculpture, I think she should paint.). 
Though 
Concessive clause 
Adversative clause 
Hamilton 
13 
44 
Madison 
19 
15 
Commonly, apt, probability, probably, and alwaY8 qualify the truth of a state-
ment. The following table shows the frequencies of the verb tenses most closely 
linked to these words in the writings of Hamilton and Madison: 
Hamilton 
Total 
Tenses of 
Word 
frequency 
certainty 
commonly 
14 
9 
apt 
24 
16 
probability 
23 
7 
probably 
8 
1 
alwaY8 
45 
23 
Tenses 
of 
doubt 
5 
8 
16 
7 
22 
Madison 
Total 
Tenses of 
frequency 
certainty 
4 
4 
21 
13 
10 
10 
Tenses 
of 
doubt 
8 

8.5] 
HOW WORDS ARE USED 
255 
TABLE 8.5-1 
COUNTS OF NUMBERS OF of's IN DICTIONARY CATEGORIES; 
Two SAMPLES OF SIZE 10 FROM EACH OF 10 PAPERS BY EA.CH AUTHOR 
Paper number: Hamilton 
Dictionary 
category 
Totals 
1 
9 
15 
23 
27 
31 
35 
65 
69 
73 
14 
34,34 
4,2 
3,6 
5,6 
3,1 
2,3 
1,5 
4,2 
6,6 
2,2 
4,1 
13 
4,8 
0,1 
0,1 
0,1 
1,0 
0,1 
2,1 
0,1 
1,2 
12 
10,6 
5,1 
2,0 
3,0 
0,2 
0,2 
0,1 
11 
1,7 
0,2 
1,0 
0,1 
0,1 
0,1 
0,1 
0,1 
10 
7,5 
2,0 
1,1 
1,2 
1,1 
1,1 
1,0 
9 
20,18 
1,3 
1,0 
3,2 
6,7 
1,2 
1,0 
2,2 
2,0 
0,1 
3,1 
6 
5,6 
1,0 
0,2 
1,0 
1,0 
0,1 
2,2 
0,1 
5 
6,10 
0,2 
0,1 
0,1 
0,1 
3,2 
0,1 
1,0 
2,2 
4 
1,0 
1,0 
3 
11,6 
2,0 
1,0 
1,0 
0,1 
1,0 
6,4 
0,1 
1 
1,0 
1,0 
Paper number: Madison 
Dictionary 
category 
Totals 
10 
14 
37 
38 
39 
40 
41 
42 
43 
44 
14 
48,42 
4,7 
6,8 
6,4 
3,2 
6,1 
3,4 
7,3 
3,4 
6,4 
4,5 
13 
12,19 
4,0 
1,1 
1,0 
1,0 
3,5 
0,3 
1,4 
1,2 
0,2 
0,2 
12 
3,3 
2,1 
0,1 
0,1 
1,0 
11 
2,1 
1,0 
1,0 
0,1 
10 
5,1 
2,0 
0,1 
1,0 
1,0 
1,0 
9 
19,15 
1,1 
1,0 
1,3 
1,1 
0,1 
5,2 
0,2 
4,2 
3,1 
3,2 
8 
0,1 
0,1 
7 
0,1 
0,1 
6 
4,6 
0,1 
1,2 
0,1 
1,1 
0,1 
2,0 
5 
2,6 
1,1 
0,1 
1,0 
0,1 
0,1 
0,2 
4 
1,0 
1,0 
3 
4,5 
2,4 
1,0 
1,0 
0,1 
Grand Totals 
Category 
H 
M 
Category 
H 
M 
Category 
H 
M 
14 
68 
90 
9 
38 
34 
4 
1 
1 
13 
12 
31 
8 
° 
1 
3 
17 
9 
12 
16 
6 
7 
° 
1 
1 
1 
° 
11 
8 
3 
6 
11 
10 
10 
12 
6 
5 
16 
8 

256 
OTHER STUDIES 
[8.6 
The tenses of certainty are defined as all present and past tenses, the tenses of 
doubt as future, conditional, and subjunctive tenses or moods. The tenses in 
the former category usually imply certainty; those in the latter category usually 
imply doubt and speculation. When always is additionally qualified (almost, 
nearly), it has not been counted. Again, the distribution of frequencies for the 
last three words in the table on p. 254 suggests that there may be some 
information in the meanings and usage. 
To study the possibility that meanings of a high-frequency word could con-
tribute to discrimination, a small investigation using dictionary definitions was 
set up. The study was carried out by Miriam Gallaher. 
She took two samples of 10 of's each, from 10 papers by each of the two 
Federalist authors. After considerable practice with the definitions, the process 
of categorizing into the 15 categories became satisfactorily reliable. Table 8.5-1 
gives the counts for the two sets of samples. 
A glance at the grand totals for the two authors given in the small table at the 
end of Table 8.5-1 suggests that further value might be found in of if allocations 
to categories were made. Categories 3,5,10,11, and 12 seem favored by Hamilton 
and 13 and 14 by Madison. To take advantage of this further classification 
would require checking out the distributions of these subclassifications. Perhaps 
each category could be treated as a separate word, and the negative binomial 
theory applied to it. On the other hand, correlations between kinds of of's 
might be large. 
8.6. SCATTERED INVESTIGATIONS 
During the initial screening study, we initiated many small studies intended to 
find good variables for distinguishing authors. We wish to mention the sorts 
of variables employed, but we shall not give a thorough report, for the results 
were meager. 
Naturally, the while-whilst pair stimulated us to hunt for pairs of words with 
identical meanings and to compare the authors' rates for these-toward and 
towards, for example. Exploration of books of synonyms soon produced more 
sets of words to investigate than we had time to study in a piecemeal fashion. 
At the same time that the numbers to be investigated were large, the payoff in 
good discriminators was low. 
The words affect and effect and their variations captured our attention. 
Rates of use per 1000 words 
Hamilton 
Madison 
affect + affected 
effect + effects 
.24 
.42 
.04 
.94 
Though these differences in rates look promising, the large variability of the 

8.6] 
SCATTERED INVESTIGATIONS 
257 
rates from one paper to another reduced their value. These words were not 
overlooked by the main studies. In two separate opportunities effect failed to 
qualify for the list bf 165 words. Affect(ed) entered the list through the initial 
screening study, but failed to meet the importance criteria in the studies of 
Chapters 3 and 5. It qualified for the study in Chapter 7, but it performed 
poorly. Small samples of text produced the rates in the table above. 
Some initial indications of systematic differences in the frequencies of Cbm-
parative and superlative forms were not borne out under further study. The 
frequencies of these forms showed great variability and seemed related to con-
text. 
Words with emotional tone offer a more psychological approach. For example, 
perhaps Hamilton used more words with a negative tone, such as wicked, mon-
strous, coward, and vicious. But no, Madison kept right up. Similarly, the classes 
of religious words and words of sweetness and light suggest themselves. These 
good thoughts came to naught. 
William Kruskal noticed some differences in manner of enumeration. Once 
embarked on an enumeration, Hamilton often inserts some variation in it as 
in "the bigotry of one female, the petulances of another, and the cabals of a 
third." Note the use of "another" instead of "a second." Certainly in the early 
Federalists, Hamilton often employs this "stumbling" enumeration. 
But in 
paper No. 80, for example, he maintains an enumeration unfalteringly through 
"seventh. " 
Madison more often maintains his enumerations. In the end, we abandoned 
this variable because we had relatively little evidence, and there was some 
indication that Hamilton's behavior was not consistent before and after the 
disputed papers. Among the disputed papers, paper No. 50 has a well-maintained 
five-step enumeration. Paper No. 57 has a nice five-'step enumeration, paper 
No. 58 a three-stepper. In some of the other papers enumerations flow across 
papers, a feature that is hard for us to use. Paper No. 62 has a self-contained 
four-stepper and also a five-stepper that overflows into paper No. 63. It also 
has a stumbling four-stepper: "in the first place," "another effect," "another 
point of view," "but the most deplorable effect of all." 
We initiated a study of conditional clauses and phrases that failed before it 
started because different readers did not make the same identifications even 
after repeated instruction. Though we never made a systematic study of fre-
quency of specific phrases, we did look into a few without success. The rarity 
of repetition of a phrase makes the investigation discouraging. 
To find an additional source of variation in log odds produced by the study 
in Chapter 3, we related the strength of discrimination to the originality of the 
material. 
Specifically, an analysis of the ideas, made by Miriam Gallaher, 
estimated for each paper the fraction of original material. 
We related this 
fraction to the log odds adjusted for length of paper. When an author writes 
authoritatively of dates, history, and confederations, he must get this material 
from sources other than himself. The hypothesis was that the reworking of 

TABLE 8.7-1 
RELATIVE FREQUENCY PER 1000 WORDS 
Word length in letters: Hamilton 
Paper 
number 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
2:: 13 
15 
23.7 
230.3 
179.9 
129.8 
103.1 
69.3 
72.2 
53.7 
52.4 
37.4 
21.5 
11.4 
15.3 
17 
21.2 
187.2 
196.2 
120.3 
106.1 
77.2 
70.7 
71.4 
51.4 
41.8 
34.7 
9.0 
12.9 
26 
23.2 
241.6 
183.8 
129.4 
92.8 
86.8 
64.5 
47.6 
48.5 
40.8 
22.3 
10.1 
8.9 
27 
24.0 
222.3 
193.4 
146.8 
73.4 
73.4 
72.0 
50.8 
45.9 
47.3 
24.7 
17.6 
8.4 
32 
32.6 
217.3 
181.9 
118.9 
130.2 
60.7 
67.2 
54.5 
43.9 
38.2 
30.4 
10.6 
13.4 
59 
27.7 
228.0 
183.5 
138.4 
115.1 
58.1 
55.4 
58.6 
52.1 
33.1 
22.3 
15.7 
11.9 
72 
28.0 
227.3 
200.2 
127.4 
92.5 
65.9 
68.9 
47.2 
56.1 
40.3 
21.2 
12.8 
12.3 
77 
37.1 
232.1 
180.3 
132.1 
97.0 
74.2 
52.3 
41.6 
56.9 
39.6 
31.0 
10.7 
15.2 
Average 
27.0 
225.3 
186.6 
130.3 
100.8 
71.2 
65.5 
52.6 
51.4 
39.4 
25.3 
12.1 
12.5 
Word length in letters: Madison 
Paper 
number 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
2:: 13 
39 
20.0 
237.3 
207.2 
110.9 
77.0 
80.9 
60.9 
60.5 
48.5 
48.5 
27.3 
12.7 
8.1 
41 
26.6 
204.1 
200.1 
126.8 
104.8 
73.9 
84.5 
56.4 
50.4 
30.6 
16.9 
12.6 
12.3 
42 
19.2 
211.4 
197.8 
115.1 
97.0 
78.6 
80.8 
63.5 
44.3 
38.7 
22.9 
14.4 
16.2 
43 
29.8 
217.7 
194.9 
115.2 
92.7 
77.4 
72.8 
54.6 
53.6 
41.4 
23.5 
13.2 
13.2 
45 
10.9 
201.9 
213.7 
128.6 
90.8 
83.2 
63.8 
63.8 
47.3 
39.7 
30.3 
15.1 
10.9 
46 
17.3 
200,1 
207.0 
134.8 
102.9 
81.8 
66.4 
47.2 
40.7 
48.0 
25.0 
12.3 
16.5 
48 
27.1 
210.6 . 
188.0 
123.4 
98.2 
60.1 
63.3 
50.4 
63.3 
45.9 
38.1 
15.5 
16.1 
Average 
21.9 
212.0 
201.5 
121.9 
95.1 
77.2 
71.8 
56.8 
49.1 
41.1 
24.9 
13.5 
13.2 

8.7] 
DISTRIBUTIONS OF WORD LENGTH 
259 
such material might reduce the distinctiveness of an author's personal style. 
While some small evidence from paper No. 20 supports this view, our present 
study is too gross to detect any effect. However, the smallest log odds among 57 
papers did go with the fourth from smallest fraction of original material. 
The distributions of length of papers suggest length as a possible discriminator. 
Hamilton's papers are shorter than Madison'S, as the table shows for Waves A 
and B (see Section 2.5C). 
Under 2500 words 
2500 words and over 
Hamilton 
10 
2 
Madison 
1 
8 
We choose not to use this discriminator for a variety of reasons. In the main, 
they boil down to too much contextuality. 
The length of a paper depends on the space available in a newspaper. A nutn-
ber of the papers start by taking up the second or later point in an argument 
begun in an earlier paper. This suggests that articles were chopped into papers 
at least partly for the convenience of the printer. One might assume that papers 
that take up successive points in the same argument are written by the same 
author, but we are inclined not to lean on such an assumption in the statistical 
part of the work, and we leave it to others to do as they choose. 
8.7. DISTRIBUTIONS OF WORD LENGTH 
Because of its historical interest, we investigated word-length distributions 
for Hamilton and Madison. 
Mendenhall (1887, 1901, see Williams, 1956) 
discovered that graphs of the relative frequency distributions for lengths of 
words (measured in letters) in the writings of different authors often showed 
striking variety. He was so impressed by the differences among and the stability 
within authors that he honored such a graph with the name the characteristic 
curve of composition. We know that one- and two-letter words and the are not 
outstanding discriminators for The Federalist (see Chapter 1); nevertheless we 
thought it useful to record the word-length data because the counts are easy 
to make after the machine counting is completed. The results described below 
were assembled and computed by Robert M. Kleyle;and Marie Yeager. 
Eight Hamilton papers (numbers 15,17,26,27,32,59,72, and 77) and seven 
Madison papers (numbers 39, 41, 42, 43, 45, 46, an.d 48) were chosen for this 
study. We counted numbers of words with 1, 2, 3, ... ,12, and 13 or more letters, 
that occurred in each of these fifteen papers. We excluded all numerals, and 
treated hyphenated words as single words but did not count the hyphen as a 
letter; we would have omitted proper names, but all letters are capitals in our 
counts. The resulting distributions of relative frequency per 1000 words are 
shown in Table 8.7-1. 

260 
OTHER STUDIES 
250 
00 
"C ... 
~ 200 
0 0 0 .... 
... 
~ 150 
>. 
'" .: 
Q) 
::l 
Cf 
~100 
Q) 
'E 
oj 
Ql 
p:: 
50 
00 
250 
] 
~ 200 
o 8 
.... 
... 
~150 
~ 
~ 
Cf 
~ 100 
Q) 
.!:: 
~ 
~ 50 
, , , , , , , , , , , , , , , , , , , , , , • 
__ Hamilton Federalist papers: 15, 17, 26, 27, 
32, 59, 72, 77 
[8.7 
---- Madison Federalist papers: 39, 41, 42, 43, 45, 46, 48 
Total words: Hamilton~15,675 
Madison-18,088 
Word length (letters) 
FIGURE 8.7-1 
Hamilton Federalist papers: 17 (1555 words) ---
26 (2372 words) ----
32 (1413 words) ---'-
Word length (letters) 
FIGURE 8.7-2 

8.7] 
DISTRIBUTIONS OF WORD LENGTH 
Madison Federalist papers: 41 (3493 words)--
43 (3022 words)----
46 (2604 words) -. -
• 
261 
o~-L~~~_L~~_L~~~~ 
__ 
o 
4 
5 
8 
9 
10 
11 
J2 
13 
14 
15 
16 
Word length (letters) 
FIGURE 8.7-3 
Figure 8.7-1 shows the frequency distribution for the pooled Hamilton papers 
and that for the pooled Madison papers. 
Figures 8.7-2 and 8.7-3 illustrate, for Hamilton and Madison, the variation 
in the distributions from one paper to another. For the man trying to discrim-
inate they bring bad news: the variation from paper to paper is considerable, 
and if the two sets of curves are superimposed, they overlap a good deal. 
To assess deviations of papers from total observed proportion, we used the 
x2-statistics. The x2 was computed for a 2 X 13 table, one row being the counts 
for the paper in question, the other row the sum of counts for the rest of the 
papers by that author. These values of x2 are shown in Table 8.7-2. They are 
larger than might be expected ofax2-variable with 12 degrees of freedom, 
but that was to be anticipated because the x2,s for the entire set of counts 
(8 X 13 for Hamilton, 7 X 13 for Madison) were more than twice their degrees 
of freedom for each author. Thus, as expected, topical variation does change the 
distribution of word lengths. 
If there is to be good discrimination, then, when a Hamilton paper is com-
pared with the Madison totals, we should get a large value of x2• That is, 
a value large not only absolutely in comparison wirh an entry in a x2 table, 
but one that is large cDmpared to the results when a Madison paper is compared 
with the Madison total. Thus for the Hamilton paper No. 59, the value of x2 
is 36.5 when compared with Madison's total for his seven papers, but this number 
is smaller than two of the 7 X2,s obtained when :\\1adison is compared with 
himself. Inspection of Table 8.7-2 reveals, alas, as i~ the case of so many other 
variables, that the distribution of word-lengths has not much to contribute to 
the problem of The Federalist. 

262 
OTHER STUDIES 
[8.7 
TABLE 8.7-2 
X2 FOR EACH PAPER AS COMPARED WITH THE REST OF THE 
PAPERS BY EACH AUTHOR, AND X2,S COMPARING EACH PAPER WITH THE 
TOTAL FOR THE OTHER AUTHOR * 
Paper 
Paper 
M versus 
Paper 
H versus H 
number 
length 
H total 
number 
9.9 
15 
3074 
42.1 
39 
36.9 
17 
1555 
39.6 
41 
23.0 
26 
2372 
34.4 
42 
26.4 
27 
1417 
13.4 
43 
23.7 
32 
1413 
45.9 
45 
20.4 
59 
1842 
36.7 
46 
9.6 
72 
2033 
21.7 
48 
27.3 
77 
1969 
Average 23.3 
Average 33.4 
Paper 
Paper 
H versus 
Paper 
M versus M 
number 
length 
M total 
number 
41.0 
39 
2596 
25.0 
15 
44.8 
41 
3493 
19.3 
17 
14.8 
42 
2710 
24.6 
26 
15.3 
43 
3022 
27.1 
27 
27.4 
45 
2115 
34.4 
32 
30.0 
46 
2604 
36.5 
59 
34.4 
48 
1548 
15.0 
72 
49.9 
77 
Average 29.8 
Average 29.0 
* These paper lengths depart slightly from those reported earlier because of differences 
in the method of counting. 
When Hamilton is compared with Madison, his average X2 is almost the same 
as that obtained when Madison is compared with himself. When Madison is 
compared with Hamilton, the average x2 is 10 units higher than for Hamilton 
versus Hamilton, but four of the 7 Madison papers have x2,s that are smaller 
than the largest obtained from Hamilton-Hamilton comparisons. The crowning 
indignity is that Hamilton's paper No. 72 agrees about as well with the Madison 
total as does any of Madison's papers. 

CHAPTER 9 
Summary of Results 
and Conclusions 
Our remarks fall under four heads: The Federalist study, authorship problems 
generally, discrimination and classification problems, and Bayesian studies. 
9.1. RESULTS ON THE AUTHORSHIP OF THE DISPUTED 
FEDERALIST PAPERS 
1. Our data independently supplement those of the historian. On the basis 
of our data alone, Madison is extremely likely, in the sense of degree of belief, 
to have written all the disputed Federalists: Nos. 49 through 58 and 62 and 63, 
with the possible exception of No. 55. For No. 55 our evidence is relatively 
weak because suitably deflated odds are about 90 to 1 in favor of Madison. 
Such conservatism needs explanation. We say weak because a reasonable person 
with strong initial convictions in favor of Hamilton would not find these odds 
overwhelming. For people who slightly favor Hamilton's authorship, or favor 
Madison's, these odds are strong. By contrast, though, No. 56 has the next 
weakest odds of about 800 to 1 for Madison, and we do not regard these as weak. 
For the remaining papers, the data strongly favor Madison, including the two 
papers that hist9rians feel least settled about, Nos. 62 and 63. 
Since paper No. 55 and the papers in its neighborhood are on the same topic, 
the House of Representatives, many will feel that these should be combined. If 
this is done, we produce more support for Madison's authorship of No. 55. We 
have. nothing against this approach, but here each person will wish to judge for 
himself or find his own expert. 
In summary, we can say with better foundation than ever before that Madison 
was the author of the 12 disputed papers. 
2. Among the joint papers, Madison wrote the lion's share of Nos. 18 and 19. 
Number 20 does not provide such strong evidence, partly because it is short, 
and partly because it is heavily a rewrite of historical work of Felice and Temple. 
Insofar as we could pursue Hamilton's marker words, they led back to Madison's 
263 

264 
SUMMARY OF RESULTS AND CONCLUSIONS 
[9.1 
notes based on writings of these historical authors. Thus the evidence in favor 
of a Hamilton share is weakened by deeper investigation. 
3. We give little credence to the possibility that Hamilton wrote but Madison 
thoroughly edited the disputed papers, so that they finally looked Madisonian, 
rather than like a mixture of styles or a parody. The reader must appreciate 
that such a performance is not so trivial as changing two or three words. Even 
among the 30 words of the main study, Madison would have to make between 
50 and 100 changes in each paper, to say nothing of the further changes these 
would induce. Since Madison could not know that we planned to use these 30 
words, the total revision required, so that an analysis shows clear Madison 
rates, would have to be on a scale even more vast. 
4. The penalty study gives solid support to the use of the negative binomial 
and its odds. The various adjustments intended to account for correlation, 
approximation, regression, and errors in counts left the final odds satisfactorily 
high. 
5. The notion that very high odds, say millions to one, discredit the method 
stems from a misunderstanding about the appropriate view to be taken about 
the truth of the mathematical model when outrageous or roguish events are 
possible. This is discussed at the close of Section 3.7F. 
6. Varying the prior distributions changed the odds factor considerably less 
than did other sources of variation. In a way, this is an old story in statistics 
because modest changes in weights ordinarily change the output modestly. 
And a prior distribution is a weighting. The old trap question is "If these prior 
distributions do not matter, why introduce them?" No one said they did not 
matter. One can readily imagine priors that can destroy the inference. But our 
bracketing procedure limits consideration to priors that are in the realm of 
plausibility, and since we have relevant data, we do not offer imagination free 
rem. 
7. Changes in the distributions that describe how frequencies of words fluc-
tuate had enormous effects. The Poisson log odds were about twice those for 
the negative binomial. But the penalty study preferred the negative binomial 
and nearly halved the log odds of the Poisson. 
8. The main study shows stable discrimination for essays on various subjects 
even with the writing spread over a quarter of a century. 
9. The weight-rate, the robust Bayes, and the three-category studies give 
good support for the main study from the point of view of reasonableness of 
results. Paper No. 55 is on Hamilton's side in these studies, although within 
easy sampling error of the border, and No. 56 is similarly inconclusive, although 
it is on Madison's side. These studies cannot be taken utterly at face value 
because of certain technical weaknesses. For this reason, their results do not 
take precedence over Result 1, derived from the main study. 

9.3] 
DISCRIMINATION PROBLEMS 
265 
9.2. AUTHORSHIP PROBLEMS 
1. The function words of the language are a fertile source of discriminators, 
and luckily those of high frequency are strong. Our tables of rates (Table 8.1-1) 
should help an investigator form a pattern of rates for a new author and speed 
him in solving easy authorship problems. 
2. Context is a source of risk. We need variables that depend on authors 
and nothing else. SOrrie function words come close to this ideal, but most other 
words do not. So. many words and other variables depend on topics that their 
exploration for differences between authors would be a needless waste of funds, 
and worse, even screening them invites trouble, because some may accidentally 
survive .. Grouping words or other variables in ways that relate not only to 
objective properties but also to the investigator's intuitive assessment of their 
trustworthiness offers a start on screening for context. Those groups which are 
not discarded out of hand can be studied for their sensitivity to sources of 
writing. If you have a variety of kinds of writing for each author, you can 
study variability in rates and have a basis for eliminating or lightly weighting 
variables or groups that show substantial heterogeneity among the kinds. If the 
bulk of the members of a group have excessive variability, consider guilt by 
association for the rest. No variable is entirely safe. Investigate. 
3. Hamilton's and Madison's styles are unusually similar; new problems, 
with two authors as candidates, should be easier than distinguishing between 
Hamilton and Madison. On the other hand, investigators with problems of 
selecting one author from many authors or of grouping a collection of essays into 
clumps by similar authors need all the help they can get. 
9.3. DISCRIMINATION PROBLEMS 
L Where important and relevant variables are available, they will be ex-
ploited. But if these obvious methods fail, the systematic exploration of a very 
large pool of variables may pay, as it did for us. Narrow and specialized variables 
may be of more use than global and meaningful ones. The point that iden-
tification and understanding need have no overlap may be missed. Thus, we 
know of no theory relating fingerprints to personality or behavior. Yet finger-
prints excel in identification. In the same mariner, in identifying paintings, art 
historians write of the overall pattern and how no single thing was determinate. 
But the art lover might be disInayed to know that no single thing was ruling 
because a thousand detaiis irrelevant to the majesty of the painting spelled out 
"right time, right materials, right strokes, right technology; Rembrandt!" 
Whether such specialized variables can also aid scientific understanding seems 
to depend on the field of endeavor. 
2. Context has its counterpart in other investigations. The usual sources of 
variation arising in the analysis of variance suggest themselves: locations, 
regions, times, laboratories, treatments, and varieties. 
Using this broader 

266 
SUMMARY OF RESULTS AND CONCLUSIONS 
[9.4 
interpretation of context, the reader can generalize Remark 2 of Section 9.2. 
To handle variability owing to context does not so much require a larger sample 
as a broader one. Broaden the base of the inference. Study variation from 
sources. Avoid being tricked by the small variation within a source and thus 
being unprepared for the big changes between known and unknown material. 
The opportunity to defend against wild variables by elimination and proper 
weighting lessens the risk of being misled by unsuspected contextual variation. 
3. When dealing with large numbers of variables, prepare to handle problems 
of selectivity. In classical studies, calibrating or validating sets of data of known 
origin, but uncontaminated by the selection and weighting processes, offer a 
solution, although their use means doing less than one's best in the weighting 
and selection. In Bayesian studies, the prior distribution automatically adjusts 
for the selectivity, but specifying prior distributions for large numbers of param-
eters requires care. 
A sampling interpretation of the parameters is often 
reasonable, and realistic priors can then be based on a pool of variables. A 
danger is that the pool may have strata that should have different priors. We 
need additional research to improve our handling of this difficulty in practical 
problems. 
9.4. REMARKS ON BAYESIAN STUDIES 
1. We recommend studying the variation of results with different prior dis-
tributions. Bracketing the prior will not always leave the inference nearly 
invariant; the inference may be highly sensitive to variations in the prior, or the 
prior may be bracketable only within wide limits. This matter must be settled 
for each problem separately. 
2. Wherever possible, priors should be empirically oriented. When founded in 
data, prior distributions have much the same status as distributions for the data 
-subjectivity tempered with empiricism. Planning ahead for the collection of 
data suitable for estimating the form and underlying constants of prior dis-
tributions is useful and important and, we think, usually not so hard once one 
is prepared to use Bayes' theorem. This remark is all the more germane to the 
field of repetitive studies, where such data may come naturally. If the priors 
cannot be founded in data or on an easily accepted argument, then agreement 
may be hard to find and large amounts of observed data may be required to 
swamp the plausible variation in priors. 
3. In any method of inference, data distributions matter considerably, and 
their study and choice, neglected for some years, requires the development of 
new and systematic methods. Statisticians need richer sets of priors that are 
manageable as well as apt. 
4. The moment he leaves the simplest application of Bayes' theorem, the 
applier finds himself involved in a welter of makeshifts and approximations. 
These troubles and their cures require new theory. Wide use implies ease and 
flexibility of methods. While one can argue that every research problem needs 

9.5] 
SUMMING UP 
267 
to be thought out anew, plenty of routine tasks can be solved to the satisfaction 
of a good many users by standard methods. If approaches applying Bayes' 
theorem are to have general use, they need standard simplifications. If our 
experience is any criterion, the prospect of useful exact treatments for large, 
real problems is most unlikely. 
5. We need simple Bayesian methods that can be applied without appeal to 
high-speed computation. Chapter 6 is a bow in that direction. 
6. In summary, in order to use Bayesian methods for large-scale analyses of 
data, we require extensive new studies of theoretical and empirical distributions 
and their approximation and estimation. 
9.5. SUMMING UP 
We believe that we have achieved our goals: to discover the merits and dif-
ficulties in a Bayesian analysis of a life-sized problem, and to compare the results 
with those of traditional methods; to lay to rest the dispute over the authorship 
of The Federalist; and to provide a basis for solving new questions of authorship. 

CHAPTER 10 
The State of Statistical 
Authorship Studies in 1984 
10.1. SCOPE 
By adding this chapter on authorship problems 20 years after the original 
appearance of this book, we give a general idea of the state of the field, of its 
strengths and weaknesses, and of where some challenging problems and 
useful work on authorship might lie. Because whole books are now appearing 
in the area, we want only to portray the state and thrust of the art of author-
ship resolution. Although we may make an occasional critical remark, our 
intention is to contribute to the overall assessment, rather than to a specific 
study. Essentially we treat the time period since Bailey's (1969) review paper. 
We first discuss the technological advancement of the field and the 
accumulation of substantial empirical studies in the literature. The only 
time we consider poetry is in the Shakespeare-Bacon controversy. We take 
up several specific authorship problems, some with two potential authors 
(like Hamilton and Madison), then one candidate versus others not known, 
and finally some homogeneity problems. We describe Morton's and Bailey's 
forays into forensic authorship disputes, as well as a critique of a study of 
the signatures on the Howland will. We conclude by suggesting where the 
field stalldsand by mentioning some useful further steps. 
10.2 COMPUTERS, CONCORDANCES, TEXTS, AND MONOGRAPHS 
The most important technological advances in authorship studies have 
arisen from the computer. 
Because of the widespread availability of computers and associated soft-
ware, creating concordances and analyzing texts by counting frequencies 
have become both less tedious and more accurate. At the time of our 
original analyses, we were straddling the introduction of the computer for 
linguistic counts and the old hand~counting methods, with the disadvantages 
of both. 
268 

10.3] 
GENERAL EMPIRICAL WORK 
269 
Brainerd (1973) points out that both Ellegard (1962a, b) and Mosteller 
and Wallace use word frequencies to distinguish authors. He regrets that 
some concordances fail to include the high-frequency words, and he en-
courages their inclusion to aid in authorship studies. Especially when text is 
sparse, the high-frequency words may be good discriminators. He reviews 
problems of counting and of definition, noting some differences between 
prose and poetry, as well as the difficulty even of defining a "word." He 
emphasizes the need for standard definitions. 
Stylistics (the analysis of literary style) and stylometrics (the quantitative 
study of literary style) have both benefited from advances in computing, as 
Bailey (1969) explains in his review chapter and as Kenny (1982) outlines in 
the introduction to his small statistics book for people working with language. 
Several books on statistical methods for analyzing language and authorship 
have appeared through the years: Yule's The statistical study of literary 
vocabulary (1944), Herdan's Language as choice and chance (1956) and his 
The advanced theory of language as choice and chance (1966), and Williams' 
Style and vocabulary (1970). Others deal primarily with substantive problems, 
such as Ellegard's A statistical method for determining authorship,' the Junius 
letters1769~1772 (1962a), or his The syntactic structure of English texts (1978), 
which offers serious analysis of language for the specialist. 
Among books that emphasize the role of the computer, Oakman's Computer 
methods for literary research (1980) deserves mention. In addition to its essays 
on research in the general area, Chapter 7 discusses stylistic analysis, and the 
bibliography devotes over three pages to authorship studies (pp. 217-220). 
McMahon, Cherry, and Morris (1978) describe several capabilities of the 
UNIX computer operating system that are especially useful for studying 
English text. 
Another good bibliographic source is Bailey and Dole~el's An annotated 
bibliography of statistical stylistics (1968). 
10.3. GENERAL EMPIRICAL WORK 
The Reverend A. Q. Morton and his coworkers have systematically produced 
a large amount and variety of empirical studies of language related to author-
ship problems. These have had a substantial influence on the choices of 
variables used by other researchers. Morton's charming monograph (1978) 
on stylometrics contains many empirical studies of quantitative aspects of 
style, such as the distribution of sentence length. He and Ellegard emphasize 
the use of what they call proportionate pairs of words (Morton, 1978, pp. 147-
150); for example, in English, the fraction that the occurrences of a represent 
among all uses of a and an in a text. Similarly, in and into might be a pair, 
or that and this. Although these particular pairs are composed of nearly 
synonymous words, a proportionate pair need not be synonymous. For 
example, in data provided by Morton (1978, p. 149), Sir Walter Scott in 

270 
STATE OF STATISTICAL AUTHORSHIP STUDIES 
[10.4 
samples from The Antiquary and from Oastle Dangerous has rates of in com-
pared with in plus into of 47.8 per cent in both (total counts 2341 and 999). 
Thus the two books give nearly identical rates. John Fowles in samples from 
The French Lieutenant's Woman, Magus, and The Oollector has rates of 48.6, 
46.6, and 48.1 (total counts 350, 262, and 308). Thus Fowles' books have 
similar rates with values much like those of Scott. Henry James behaves 
very differently from Scott and Fowles, producing in The American and The 
Ambassadors the percentages 13.5 and 8.6 (total counts 96 and 163). 
Collocation, another measure based on word pairs, and introduced by 
Morton, involves a keyword preceded by or followed by some partner. For 
example, the keyword in might be followed by a or by the. A few examples of 
common collocations are and the, and a, as the, at the, in the, not only, and to be. 
In each of these the first word happens to be the keyword on which to base 
the total count. Morton uses rate of collocations to see what effect strokes 
suffered by Sir Walter Scott might have had upon his writing habits. Scott's 
writing behavior was remarkably constant on these collocations from before 
to after the strokes. 
. 
A similar study of collocations used by Henry James in his early (1877) 
and late (1903) writing also showed great stability in the collocation rates. 
For example, none of 16 chi-squared tests for differences in collocation rates 
was significant at the .05 level. 
Position in the sentence offers a feature of style that may deserve attention. 
The first word in a sentence is not as likely to be a low-frequency word as the 
last word in a sentence. For example, Morton shows that in the Gospel of 
John, the first word in 12 per cent of:the sentences was a word used only once 
in the text, whereas the last word was such a rare word in 37 per cent of the 
sentences. Similarly, words that occurred 20 or more times accounted for 
42 per cent of first words, but only for 12 per cent of last words. 
10.4. POETRY VERSUS PROSE 
Although we do not take up poetry generally, we do follow up a very early 
Shakespeare study by Mendenhall (1901), one of the first stylometric 
analysts. 
Both Brainerd (1973) and Williams (1975) are concerned with Shakespeare, 
and both say that comparisons of style between poetry and prose samples 
will turn out to be unsatisfactory. For example, Mendenhall (1901) attempted 
to use the distribution of word length to discriminate the writing of Shake-
speare from that of Bacon and thus resolve the old issue of whether Bacon 
wrote Shakespeare. The trouble is that in the comparison materials Shake-
speare wrote only poetry and Bacon only prose. Mendenhall concluded that 
Bacon's and Shakespeare's writings differed too much for them to have a 
common author. (Mendenhall had found also that Marlowe's poetic word 
length distribution agreed closely with that of Shakespeare.) 

10.5] 
OTHER STUDIES 
271 
To investigate the possible systematic difference in word length between 
poetry and prose, Williams chooses a contemporary of Shakespeare and 
Bacon-Sir Philip Sidney-and looks at the percentages of three- and four-
letter words in Sidney's prose and poetry and compares these for Shakespeare 
and for Bacon as shown in Table 10.4-1. 
TABLE 10.4-1 
COMPARISONS OF PERCENTAGES OF THREE-LETTER AND FOUR-LETTER 
WORDS IN POETRY AND PROSE BY SHAKESPEARE, BACON, AND SIDNEY 
Poetry 
Per cent 
3-letter 
4-letter 
Shakespearea 
Sidneyc 
22.5 
19.3 
a About 400,000 words. 
b About 200,000 words. 
23.8 
25.0 
Baconb 
Sidneyc 
C About 1550 words for each type of writing. 
Prose 
Per cent 
3-letter 
4-letter 
22.7 
21.0 
17.5 
21.1 
Source: Williams (1975, p. 211). Reproduced with permission of the 
Biometrika Trustees. 
Williams sees in these data a substantial change from poetry to prose. 
He focuses on the comparative sizes of the percentages for three- and four-
letter words. The Sidney samples change in the same direction as the change 
from Shakespeare to Bacon, though the sample sizes for Sidney are not 
large. The standard deviations for binomial sampling for Sidney are about 
1 per cent. Williams concludes that the difference between Shakespeare and 
Bacon here could be regarded as a poetry-to-prose effect rather than an 
authorship effect. He and Brainerd agree that we cannot hope to compare 
authors when one writes only poetry and the other writes only prose. In the 
absence of authenticated Shakespearean prose and Baconian poetry, they 
would set aside Mendenhall's view of the difference in word-length distri-
butions as distinguishing Bacon from Shakespeare because of confounding 
the effects of prose and poetry. For our part, we regard Mendenhall's and 
others' studies as suggesting that word length is not a very good discrimi-
nator of authorship (see Section 8.7 of this book). 
10.5 AUTHORSHIP STUDIES SIMILAR TO THE JUNIUS OR 
FEDERALIST STUDIES 
For studies described in this section the potential authors can be listed, 
and text known to be by them can be gathered and analysed for comparison 
with the work of unknown authorship. We describe four studies, one about 

272 
STATE OF STATISTICAL AUTHORSHIP STUDIES 
[10.5 
a Russian novel, a second about newspaper essays, a third about a German 
novel, and a fourth about cases in economic history. 
IO.SA. And Quiet Flows the Don. R. W. Bailey suggested at a conference, 
Kjetsaa (1979) reports, that an important modern controversy deals with the 
authorship of the novel, And Quiet Flows the Don (in four books), which has 
been translated into many languages. Mikhail Sholokhov was listed as the 
author when part of the novel was first published in 1928. Later a critic, 
writing under the pseudonym D*, claimed that Sholokhov plagiarized the 
work of Fyodor Kryukov, who is said to have been writing a work about the 
Don Cossacks when he died of typhus in 1920. Some other critics support 
D*. Solzhenitsyn wrote the preface to D*'s attack. Considerable political 
heat surrounds the controversy because pro- and anti-communist groups are 
involved. 
Medvedev (1977), in his book Problems in the Literary Biography of Mikhail 
Sholokhov, explains the controversy with extensive illustration and research 
by himself and others. The main line of the argument follows. 
First, many feel that an epic with the scope and detailed information about 
the differing cultures of so many groups of Don Cossacks could not have been 
within the grasp of such a young non-Cossack author as Sholokhov, aged 21. 
In addition, Russian literature offered few epics, so Sholokhov could not 
have found many models to emulate. 
Second, an author with a modest literary track record cannot have much 
chance of suddenly writing a masterpiece widely regarded as a match for 
Tolstoy's War and Peace. Against this point, Medvedev says that another 
Russian writer, Solzhenitsyn, did in his youth produce a masterpiece with 
practically no previous literary record. 
Third, some feel that Mikhail Sholokhov, friend of Stalin, never again 
produced writing comparable to that in The Quiet Don. Medvedev points out 
that authors like Tolstoy, Nabokov, and Solzhenitsyn keep on churning out 
star-class writing rather than producing just one great work. 
Fourth, some find the writing about the Bolshevik supporters to be very 
different in quality from that about the Cossacks, suggesting that those parts 
have been added to make the book acceptable to the Communists for publi-
cation. It seems generally agreed that had the authorship been attributed 
to an anticommunist such as Kryukov, the book would not have been 
published by the Communist regime. At the same time, if we read Medvedev 
and others correctly, critics have grave doubts that Kryukov was capable of 
writing at the level of The Quiet Don. 
If we read him aright, Medvedev believes that we do not have enough 
information yet, but that if he had td guess, he thinks Sholokhov acquired 
~ome substantial materials, diaries, letters, or literary writings that he used 
to build The Quiet Don. Thus he feels there was double authorship, but does 
not seize or reject Kryukov as the coauthor. 

10.5] 
OTHER STUDIES 
273 
Medvedev is optimistic about the possibility of textual analysis by com-
puter. He thinks that a mechanical analysis would settle the controversy 
with reasonable certainty. He suggests that certain characteristics such as 
language, vocabulary, and style of the first volume of Sholokhov's Complete 
Works (Tales of the Don) could be compared with the second and third 
volumes (Books 1 and 2 of The Quiet Don). He says that it is officially 
accepted that these three volumes were written in about the same period of 
time (1924-25 and 1926-28). 
According to Kjetsaa, a group of Swedish and Norwegian scholars has 
been investigating D*'s claim that 95 per cent of Books 1 and 2 and 68-70 per 
cent of Books 3 and 4 are Kryukov's writing. Kjetsaa reports on a few 
indices he has investig~ted as part of this larger study. Using other known 
writings by Sholokhov and Kryukov, Kjetsaa finds that some features of the 
distribution of sentence length favor Sholokhov, as does the distribution of 
word length. The 20 most frequent words are used more often by Kryukov 
than by Sholokhov and The Quiet Don, whose rates are similar. 
The type-token ratio, another measure of variety in using words, expresses 
the number of distinct words as a percentage of the total number of words of 
text. Again, Kryukov differs more from The Quiet Don than Sholokhov 
(p. 252): 
Quiet Don 2* 
Quiet Don 4 
Quiet Don 1 
Sholokhov II 
Sholokhov I 
Kryukov II 
Kryukov I 
51.5% 
50.7% 
49.3% 
48.8% 
48.3% 
46·9% 
43.2% 
* No data are given for Book 3. 
Kjetsaa and colleagues (1984) report the results of their more extensive study 
in "their bQok The Authorship of the Quiet Don. Kjetsaa concludes "the 
hypothesis advanced by D* does not seem to stand up to closer scrutiny" 
p: 153). 
" 
10.5B. Kesari. Kesari is a Marathi (a state in India) newspaper. Both 
Agarkar, the editor, and Tilak, a cofounder of the paper, have been cited as 
authors of certain editorials. In investigating this issue, Gore, Gokhale, and 
Joshi (1979) use as discriminators word length, sentence length, and the rate 
of commas. Along the way they test C. B. Williams' hypothesis that word 
length is log-normally distributed. They find this theoretical distribution not 
a good approximation, and they aJ:>andon word length as a discriminator. 
Although they find that sentence length approximately follows a log normal 
distribution, the distributions of lengths are not stable from one material to 

274 
STATE OF STATISTICAL AUTHORSHIP STUDIES 
[10.5 
another for the same author. And Agarkar and Tilak are close together in 
overall sentence length, just as are Hamilton and Madison. What seems to be 
the new discriminator is the rate of use of commas. This statistic suggests 
that one of three groups of disputed essays might well be by Agarkar, but the 
inference is not strong. 
lo.se. Die Nachtwachen. According to Wickmann (1976), Die Nacht-
wachen is a key work of early German romanticism, and ever since it was first 
published, literary historians have been searching for the identity of the 
authQr (whose pseudonym is Bonaventura). SOme writers thought to be 
candidates as author are Jean· Paul, Fr. Schelling, Cl. Brentano, E. A. 
Klingemann, E..T. A. Hoffmann, and Fr . .G. Wetzel. Some of the work on 
the authorship has to do with individuals being in the right places at the 
right times or with material in. correspondence or other aspects of private life. 
To examine the candidac.ies of these persons, Wickmann uses transition 
frequencies from one part of speech to another as discriminators of author-
ship. Essentially he makes a t-test between the unknown and the known 
authored material for each transition type, and then he combines the P-values 
ofthe separate t-tests using Fisher's -2logeP method. Using this approach, 
he disposes of all competitors except Hoffmann. More detail is needed on the 
variables used and the correlations between the tests, because acting as 
if the tests are independent may mistakenly give too small a combined 
P-value. 
IO.SD. Economic 4istory. O'Brien and Darnell (1982) offer a book of six 
cases where the author of a paper in economics has not been firmly estab-
lished. Such early nineteenth century economic stars as the older and younger 
Mill, McCulloch, Torrens, and Brougham are candidates for authorship of 
papers. Some seventeenth century tracts may be the work of Sir Josiah 
Child. O'Brien and Darnell use the collocation method along with frequencies 
for the first word of sentences to try to decide authorship. 
On pages 9, 10, and 11 they discuss their use of nonparametric methods, 
emphasizing that this relieves them of distributional problems that a para-
metric approach might have to handle. For example, Mosteller and Wallace 
found that the negative binomial distribution was to be preferred to the 
Poisson for the distribution of word counts. O'Brien and Darnell note that 
"The occurrence of 'clumping' is one reason why the method cannot be 
applied mechanically without an intelligent appreciation of what is going on" 
(p. 10). They mentio:1 by. as an unusual example of a contextual word in 
material that often uses the phrase By the Act of. 
"Of course, it is possible to raise fastidious objections to the procedures 
employed. Statisticians may be particularly worried over the problem of the 
independence of observations. But in the last resort, the proof of the pudding 
is in the eating" (p. 11). 

10.6J 
HOMOGENEITY PROBLEMS 
275 
These problems of clumping or lack of independence can affect the analysis, 
especially if one wants to know how strong an inference can be drawn from 
the data. The sensitivity of the analysis to clumping or lack of independence 
is something the investigator might want to assess quantitatively. These 
problems indicate that the significance levels achieved by the nonparametric 
tests may be badly affected. In their usage in the authorship studies, it is 
desirable for the assumptions to be met, because we do not want to confuse 
the stylistic issues of clumping or lack of independence with differences in 
authorship unless these properties are what we use to discriminate, and they 
are not being so used in this instance. 
Stigler's review (1983) points out another difficulty with the significance 
levels. O'Brien and Darnell chose their words and categories in a manner 
that distinguished most between the authors on the material of known 
authorship, and then they used these same data to decide the authorship 
of the disputed papers. Selection effects resulting from such a procedure do 
ruin the frequency interpretation of the significance tests. What these effects 
do to the process of deciding between the authors is not so clear. 
Stigler also expresses concern that (a) the nonparametric frequency distri-
butions may not follow the simplest theoretical forms (such as binomial or 
Poisson), and (b) that the authors use the same data to choose their dis-
criminators as they use to make the final decisions. 
Both issues arise frequently in authorship studies discussed in this chapter, 
though we have not emphasized them except here. In the Federalist study 
we made special studies of distributions of word counts to deal with problem 
(a). We found problem (a) to matter a great deal. The assumption of Poisson 
distributions did not hold up under empirical examination. 
For selection of variables-problem (b)-we insulated our decision by 
using special sets of data for the selections in the studies of Chapters 5-7; in 
the main study of Chapter 3, we used an empirical Bayes modeling of the 
population of words to allow for selection. Stigler's review brings out 
problems that authorship resolvers will wish to handle. 
O'Brien and Darnell go to considerable lengths to give us the background 
of the controversies and the circumstantial evidence used by previous workers 
for attributing authorship of the articles. The writing is most refreshing for 
a work with so many numbers. 
10.6. HOMOGENEITY PROBLEMS 
In the simplest homogeneity problem, we have two pieces of text, and we 
ask whether they were produced by the same author. The example we treat 
first has been cast as a three-piece problem, one piece being regarded as the 
standard, and we wish to know which of the other two is closer to it, or 
whether both are sufficiently close that all pieces may reasonably have been 
written by the same author. 

276 
STATE OF STATISTICAL AUTHORSHIP STUDIES 
[10.6 
10.6A. Aristotle and Ethics. Aristotle may have written two documents 
on ethics, the Nicomachean Ethics (NE) and the Eudemian Ethics (EE). The 
parts of these documents are called "books," and they have an overlap of 
three books which Kenny (1978) calls The Aristotelian Ethics (AE). Kenny 
regards these three books as philosophically mature and as providing historical 
allusions that suggest a late date. Among scholars, Kenny (1979) tells us, 
the NE has been generally regarded as written by the mature Aristotle, and 
the EE often regarded as done during a "juvenile Platonising period." 
Kenny explores this issue in his book, using stylometric methods for words, 
and other statistical devices such as the ordering of the "virtues" as reported 
in the documents. Turning to some of the stylometric variables, he correlates 
the frequency of use of 36 particles and connectives in NE, EE, and AE. He 
finds a high correlation between frequencies of the 36 particles for NE and 
AE, .985, and between EE and AE, .995. When he eliminates the five highest-
frequency particles and recomputes, he finds the corresponding correlations 
to be .802 and .975. This, he believes, offers some evidence favoring the 
authorship of the BE by the mature Aristotle (p. 80). 
He also ranks the· frequency of use of all 36 particles in each of the three 
works. We can ask whether for each particle the rank in NE or EE is nearer 
that in AE. In 20 instances EE is closer in rank to AE, in 7 instances NE is 
closer, and in 9 instances the distll,nces are tied. This approach takes a less 
extreme view ofthe high-frequency words, offering them less weight but not 
automatically setting them to zero. The result supports Kenny's conclusion 
that the EE represents the mature Aristotle rather than the NE. (A small 
technical note: The NE and EE are not of equal length. The NE text is 
about 25 per cent longer than that of EE. This length differential favors 
NE if NE, EE, and AE come from the same population. Instead, EE 
appeared closer to AE, making the EE case stronger.) 
Kenny goes on to study prepositions, adverbs, and pronouns, as well as 
other parts of speech. The general and repeated conclusion is that the EE 
is more like the AE than is the NE, implying that the EE is the more mature 
of the two works, contrary to previous opinion .. (We have not dealt here 
with the Magna Moralia, another document possibly by Aristotle, nor with 
Kenny's extensive discussion of the content of these documents.) 
10.6B. The Bible. Both the Old Testament and the New Testament offer 
many homogeneity problems. We c1),nnot go far toward doing justice to this 
literature because so much scholarship is required, dealing with choices of 
texts as well as libraries full of scholarly criticism. For example, were the 
Epistles of Saint Paul all written by the same person? This question has 
been widely studied, by Harrison (1921) and Morton (1978) to mention two 
treatments, over half a century apart. 
Similarly, many scholars have the view that the Old Testament book of 
Isaiah has two components, Chapters 1-39 (Isaiah A) which may have been 

10.7] 
ANONYMOUS TRANSLATioN 
277 
written by one author, and Chapters 40-66 (I8aiah B) which may be by the 
same author or, some claim, by one or more different authors. 
Adams and Rencher (1973) tackle this problem, basing their analysis on 
Hebrew function prefixes, which they liken to the function words we exploited. 
The Isaiah study used 11 control books from the Old Te8tament selected by 
stratified random sampling: Amo8, Jeremiah, Ezekiel, H 08ea, Micah, Habakkuk, 
Zechariah, Daniel, Ezra, Malachi, and Nehemiah. They studied variation 
between and within books of the Old Te8tament, using 18 prefixes and 
another variable. For the 78(= 13 x 12/2) pairings of the 11 books and the 
two half-books of I8aiah, the correlation between the parts of I8aiah was 
third highest. 
Various other measures suggested that the two parts of 
I8aiah were relatively close. Adams and Rencher's primary conclusion 
is that "evidence does exist for authorship unity for the book of I8aiah" 
(p. 154). 
For those who would like to pursue some Old Te8tament discussions, 
Radday and Wickmann (1975) discuss the homogeneity of Zechariah, and 
Portnoy and Petersen (1984a) criticize their attempt. Similarly, Radday, 
Shore, Pollatschek, and Wickmann (1982) review Gene8i8 for homogeneity, 
and Portnoy and Petersen (1984b) take them to task. These four articles 
attend in considerable depth to the various features of analysis and therefore 
offer more than just a controversy to scholars embarked on authorship 
studies. The articles review methodology as well. 
10.7. ANONYMOUS TRANSLATION 
In the example to be described, we have one known candidate for author-
ship, other possible authors being unknown. 
A most unusual literary problem arises from an anonymous translation 
of the military history of Charles XII (King of Sweden, 1697-1718) from 
French into English. The man who did so much to advance the novel, Henry 
Fielding, author of Tom Jone8 and J08eph Andrew8, might possibly have 
been the translator. Michael and Jill Farringdon (1979) look into Fielding's 
candidacy. Clearly they have a hard problem. Bailey (1969) believes that 
one needs an exhaustive list of all the possible authors before attempting the 
forensic resolution of an authorship dispute (Ellegard had something like 
98 potential authors in his Junius problem). Here, however, we seem to have 
one clear candidate, and the idea would be to get evidence for or against that 
candidate. In a sense, this resembles Kenny's problem with the Aristotelian 
documents, but with an added twist. We are dealing not with the author's 
own writing but with a translation from another language and, indeed, with 
a history rather than narrative prose. In ordinary authorship studies, we 
may have problems of finding enough authenticated writing to produce an 
empirical base for discrirnination, but how can we expect to get such a base 
for a translation? 

278 
STATE OF STATISTICAL AUTHORSHIP STUDIES 
[10.8 
No theory or principles for dealing with such a problem have been developed. 
The Farringdons' approach considers some unusual usages, as noted, even 
caricatured, by early critics of Fielding's writings. For example, he used 
doth and hath rather than does and has and whilst instead of while (shades of 
Madison and Hamilton) in his ordinary writing. The Farringdons also make 
a word count for a pool of authors to get an empirical base. In doing this, 
they conclude that some of the habits attributed especially to Fielding do 
not stand up as unique under empirical study because other authors often 
used some of the expressions. The reader cannot tell the rates, though, 
because the results for all these other authors were pooled. 
In the translation, the Farringdons find the frequencies for hath rather 
than has and whilst for while higher than in the controls. They find durst for 
dared and a high use of likewise in translating aussi, m€me, and pareillement. 
They do not find doth for does. Some other usages stand out. We do not 
know how to sum up these observations quantitatively. The authors con-
clude that Fielding was the author of the translation. 
Without disagreeing with the conclusion, we think this kind of problem 
deserves additional formulation in the hope of developing a way to reach more 
quantitative results. For those concerned with authorship resolution, we see 
it as a challenge. 
10.8. FORENSIC DISPUTES 
In a chapter entitled "Let justice be done," Morton (1978) discusses the 
problems of dealing with the courts, especially those of appearing as an 
expert witness in authorship disputes. 
10.8A. 
Morton. In most of the cases treated, the police were alleged to 
have written testimony into statements by the accused which the latter 
denied saying. To be acceptable, the testimony is required in Britain to use 
the very words of the accused, and this requirement enhances the possibility 
of making valid comparisons between the parts of the testimony accepted 
and rejected by the accused. In Britain, Morton finds the path of the expert 
literary witness a very rocky road. To reduce difficulties, he explains the need 
to prepare and organize one's materials carefully. Opposing counsel treat 
matters that would make no difference in scientific investigations as if they 
are deliberate attempts at fraud. He found the treatment of the expert 
witness in this new field repelling, the work often intellectually unrewarding, 
and perhaps most troublesome, that none of the clients being supported by 
his testimony were "innocent bystanders." 
10.8B. Bailey. In a paper entitled "Authorship in a forensic setting." 
Richard W. Bailey (1979) lays out limiting circumstances when the author 

10.8] 
FORENSIC DISPUTES 
279 
of a text can be identified. He offers three basic requirements for forensic 
authorship attribution: 
1. 
That the number of putative authors 
constitute a well-defined set; 
2. that there be a sufficient quantity 
of attested and disputed samples to 
reflect the linguistic habits of 
each candidate; 
3. that the compared texts be 
commensurable (p. 7). 
He uses Mosteller and Wallace's study of the authorship of the disputed 
Federalist papers to illustrate both the appropriateness of some literary 
attributions and the value of presenting numerical odds. He also cites 
Ellegard's study selecting one from 98 authors with high probability as a 
similar profitable exercise. 
Bailey expresses doubts about comparing a person's writing with tran-
scriptions of his or her oral statements for purposes of authorship attribution. 
In February, 1974, Patricia Hearst was kidnapped by members of the 
revolutionary Symbionese Liberation Army. With her captors she partici-
pated in some violent events. At her trial in 1976 the question arose whether 
she had written certain documents that might instead have been written by 
Angela Atwood or by Emily Harris. Ms. Hearst claimed she had nothing to 
do with creating these writings, merely copying what others told her to write. 
Bailey (1979) studied the writings of Angela Atwood, Emily Harris, and 
Patricia Hearst to see if they might be candidates for authorship of several 
different materials. However, the judge at the trial disallowed authorship 
evidence because the field was young and would have required a lot of testi-
mony. The issue, he said, was not whether Ms. Hearst composed the material 
but whether it expressed her views at the time. 
IO.8e. Howland will. Meier and Zabell (1980) have reviewed an unusual 
authorship problem, namely the authenticity of a signature on a will left in 
1865 by Sylvia Howland for an estate of about two million dollars. Hetty 
Robinson, her niece, claimed to be the beneficiary though she lost out on the 
will. Nevertheless, from other efforts she ultimately became a multimillion-
airess (Hetty Green) and died with an estate said by the New York Times to 
be worth 100 million dollars. The matter is worth bringing up here, not 
entirely because of its relevance to textual analysis but also because two 
famous scientific figures, Charles Sanders Peirce of the United States Coast 
Survey and his father, the mathematician Benjamin Peirce, were involved 
in the analysis of the data and offered probabilistic testimony at the trial. 
Essentially the issue was whether two signatures were too much alike and 
whether one may have been copied from the other. On the one hand, the 

280 
STATE OF STATISTICAL AUTHORSHIP STUDIES 
[10.9 
Peirces developed a strong analysis, and yet they made many of the mistakes 
researchers still make today, assuming independence between variables in 
circumstances where dependence is likely, applying statistical distributions 
without strong empirical backing, and more generally not recognizing the 
full complications of such problems. Those who engage in authorship studies 
should find the thorough critique by Meier and Zabell of the Peirces' work on 
the Howland will most instructive. 
10.9. CONCLUDING REMARKS 
We do not nearly exhaust the materials that are available in linguistics, 
or even in authorship studies, but this review will give some feeling for the 
changing scene. Although new sorts of authorship problems have emerged, 
and some new variables-such as collocation, proportionate pairs, counts of 
commas, and transition frequencies by types of words-have been suggested, 
the area of statistical analysis itself has not seen much advance. Bayesian 
statistics has been little used, and sophisticated modeling together with 
strong data analysis has not been common. 
We think that more attention is needed for considering the empirical 
variability and the covariation of variables. Assumptions such as the 
binomiality of word counts or the independence of several variables chosen 
as markers need checking. That such studies are not carried out now that 
the computer is available suggests that investigators still do not appreciate 
the need for additional carein the analysis. Because the statistics books, as 
opposed to the monographs, in stylometl'Y, in 1984 are rather elementary, the 
more difficult issue of the delicacy of assumptions may be slow in getting 
appropriate treatment. In view of the advances in computing, it does not 
seem that difficulty of calculation is holding these studies back. As always 
when investigators must prepare themselves for interdisciplinary work, 
training in one or another field may lag. 
Some empirical steps might be taken to offer a better approach to author-
ship problems. We discuss the matter as iffor modern English, but this need 
not be restrictive, for the ideas will apply in many situations. 
It would be helpful to establish results for a population of authors for the 
purposes of research investigations. The purpose is to get evidence for the 
relative merits of differing variables that might be used for discrimination 
among authors. For example, the investigators would provide rates of use 
of single words, perhaps the function words such as we have used, and provide 
evidence of and display the variability among authors. Similarly, we can 
look at sentence length and its distribution. If anyone wishes to, they could 
study distribution of word length (though we would not recommend it), and 
so on. The point, though, would be to examine for a substantial population 
of authors what the variability of each of these variables is at least across 
authors, and perhaps also within authors, both within the same work and 

10.9] 
CONCLUDING REMARKS 
281 
across works. If we had such a study, which is just an extension of empirical 
work. that Morton and Ellegard and others have been carrying out (see 
Section 8.1, pp. 243-248), we might then establish a pecldng order for variables 
to be used in authorship disputes. Naturally we should also be concerned 
about the correlations among the variables; but, as a first-order effort, having 
a substantial sample of authors and a substantial collection of variables 
would supply researchers with strong information about which are the more 
promising variables among those now in use. Such a study would have the 
further advantage that we could objectively select the variables on the basis 
of an exterior study before we start, rather than basing the choice of variables 
within the study itself. 
Admittedly there will be situations were this sort of analysis will not be 
appropriate or where only a subset of the authors originally chosen would be 
relevant. In some ways it is in the spirit of Kucera and Francis (1967) who 
provide many samples of differing kinds of text. This approach would key 
more to authors, though type of genre could matter. The availability of 
computers and software goes far toward making such a study feasible at a 
reasonable price. 
In a $tudy somewhat oriented in this direction, Damerau (1975) investi-
gates for five pieces of literature the departures of the spacing of frequent 
words from those that would be given if a Poisson distribution were appro-
priate. The distribution is taken as the one that would occur if the frequency 
of appearance of words were free of context. Mosteller and Wallace had 
noted that the rates of some pronouns, which usually are included in the 
function words, did depend on context. For example, "his" tended to occur 
more frequently in the Federalist papers when a ruler of a state was discussed. 
Although the title of his paper mentions function words, Damerau does 
not limit his study to these. Damerau reports significance levels rather than 
direct measures of departure or direct measures of the frequency distribution. 
He concludes that rather few words come close to following the Poisson in 
works by several different authors. 
Because so many different kinds of authorship problems arise, it would also 
be attractive to have systematic ways of going about each kind. Perhaps the 
translation problem would never yield because, even if we had a good model, 
we might not be able to find the data appropriate for it. Perhaps it is not 
reasonable to hope to do this, but several standard problems seem to come 
up repeatedly-several known authors and unknown text, like the Federalist 
and Junius studies, and the homogeneity problem where two or possibly more 
sets of authors may be considered as candidates. These problems deserve 
some general approaches. And we suggest that providing such approaches 
would move authorship work several steps forward. 

Appendix 
HAMILTON REFERENCES EXTERIOR TO The Federalist 
Code 
Volume 
Abbreviation 
number 
Title 
number 
Pages 
Cont.-1 
101 
The Continentalist No. I 
I 
243-248 
Cont.-2 
102 
The Continentalist No. II 
I 
248-253 
Pac.-1 
111 
Pacificus No. I 
IV 
432-444 
Pac.-2 
112 
Pacificus No. II 
IV 
445-455 
Pac.-3 
113 
Pacificus No. III 
IV 
456-460 
Lodge, H. C., editor (1904). The works of Alexander Hamilton (Federal Edition). 
G. P. Putnam's Sons, New York and London. I and IV. 
MADISON REFERENCES EXTERIOR TO The Federalist 
Abbreviation: Neutral Trade 
Title: An Examination of the British Doctrine, 
Which Subjects to Capture 
A Neutral Trade, Not Open in Time of Peace 
Editor: Hunt, VII 
Code 
Code 
number 
Pages 
number 
201 
204-211 
211 
202 
214-225 
212 
203 
226-235 
213 
204 
235-243 
214 
205 
243-251 
215 
206 
251-259 
216 
207 
259-270 
217 
208 
270-278 
218 
209 
278-285 
219 
210 
285-293 
220 
283 
Pages 
293-301 
302-310 
310-319 
319-325 
325-334 
334-340 
340-349 
349-357 
357-366 
366-375 

284 
APPENDIX 
MADISON REFERENCES EXTERIOR TO The Federalist (cont.) 
Code 
Abbreviation number 
Title 
Editor 
Pages 
N.A.-1 
121 
The North American No. I 
Brant 
571-580 
N.A.-2 
12~ 
The North American No. II 
Brant 
580-587 
Helv-1 
131 
Helvidius Number I 
(Congress) 611-621 
Helv-2 
132 
Helvidius Number II 
(Congress) 621-630 
Helv-3 
133 
Helvidius Nuinber III 
(Congress) 630-640 
Helv-4 
134 
Helvidius Number IV 
(Congress) 
64~645 
Helv-5 
135 
Helviditis Number V 
(Congress) 646-654 
M-1 
301 
Population and Emigration 
Hunt, VI 
43-66 
l\Ii:-2 
315 
Consolidation 
Hunt, VI 
67-69 
M-3 
315 
Public Opinion 
Hunt, VI 
70 
M-4 
302 
Money 
Hunt, VI 
71-80 
M-5 
316 
Government 
Hunt, VI 
80-82 
M-6 
313 
Charters 
Hunt, YI 
83-85 
M-7 
314 
Parties 
Hunt, VI 
86 
M-8 
311 
British Government 
Hunt, VI 
87-88 
M-9 
311 
Universal Peace 
Hunt, VI 
88-91 
N-1 
315 
Government of the 
Hunt, VI 
91-93 
United States 
N-2 
312 
Spirit of Governments 
Hunt, VI 
93-95 
N-3 
314 
Republican Distribution 
Hunt, VI 
96-99 
of Citizens 
N-4 
316 
Fashion 
Hunt, VI 
99-101 
N-5 
312 
Property 
Hunt, VI 
101-103 
N-6 
314 
The Union. Who Are Its 
Hunt, VI 
104-105 
Real Friends? 
N-7 
313 
A Candid State of Parties 
Hunt, VI 
106-119 
N-8 
316 
Who ,Are the Best Keepers 
Hunt, VI 
12~123 
of the Peopie's Liberties? 
N-9 
141 
Observations on the 
Hunt, V 
284-294 
"Draught of a Constitution 
for Virginia" 
311 is formed by pooling M-8 and M-9; 312, N-2 and N-5; 313, M-6 and N-7; 
314, M-7, N...:.3, and N-6; 315, M-2, M-3, and N-1; 316, M-5, N-4, and N-8. 
Brant, 1., editor (1946). Two neglected Madison letters. The William and Mary 
Quarterly, III, 569-587. 
Published by order of Congress (1865). Letters and other writings of James Madison. 
J. B. Lippincott & Co., Philadelphia. I, 1769-1793. 
Hunt, G., editor (1904). 
The writings of James Madison. 
G. P. Putnam's Sons, 
New York and London. V, 1787-1790. 
Hunt, G., editor (1906). 
The writings of James Madison. G. P. Putnam's Sons, 
New York and London. VI, 179~1802. 
Hunt, G., editor (1908). 
The writings of James Madison. 
G. P. Putnam's Sons, 
New York and London. V1I, 1803-1807. 

References 
ADAIR, DOUGLASS (1944a, 1944b). The authorship of the disputed Federalist 
papers. The William and Mary Quarterly, Vol. 1, No.2, 97-122; ibid., Vol. 1, No.3, 
235-264. 
ADAMS, L. LA MAR, and ALVIN C. RENCHER (1973). The popular critical view 
of the Isaiah problem in light of statistical style analysis. Oomputer Studies in the 
Humanities and Verbal Behavior, Vol. 7 (3-4), 149-157. 
ANDERSON, T. W. (1958). An introduction to multivariate statistical analysis. 
John Wiley & Sons, Inc., New York. 
ANSCOMBE, F. J. (1950). Sampling theory of the negative binomial and logar-
ithmic series distributions. Biometrika, Vol. 37, 358-382. 
BAILEY, RICHARD W. (1969). Statistics and style: a historical survey. Chapter 17 
in Statistics and style, Lubomir Dolezel and Richard W. Bailey, editors. American 
Elsevier Publishing Company, Inc., New York, 217-236. 
--- (1979). Authorship attribution in a forensic setting. In Advances in 
computer-aided literary and linguistic research. Proceedings of the Fifth International 
Symposium on Oomputers in Literary and Linguistic Research held at the University 
of Aston in Birmingham, UK, D. E. Alger, F. E. Knowles, and Joan Smith, editors. 
Published by AMLC for the Department of Modern Languages, University of 
Aston in Birmingham, 1-20. 
---, and LUBOMIR DOLEZEL (1968). An annotated bibliography of statistical 
stylistics. University of Michigan, Ann Arbor. 
BAILEY, S. A. (1916). Notes on authorship of disputed numbers ofthe Federalist. 
Oase and Comment, Vol. 22, No.8. Jan, 1916,674-675. 
BAYES, THOMAS (1763). An essay towards solving a problem in the doctrine of 
chances. The Philosophical Transactions of the Royal Society, Vol. 53, 370-418. 
BERKSON, J. (1930). Bayes' theorem. The Annals of Mathematical Statistics, 
Vol. 1,42-56. 
BOURNE, EDWARD GAYLORD (1897). The authorship of The Federalist. The 
American Historical Review, Vol. 2, 443-460. 
--- (1901). The authorship of The Federalist. In Essays in historical criticism. 
Charles Scribner's Sons, New York, 113-145. 
BRAINERD, BARRON (1973). The computer in statistical studies of William 
Shakespeare. Computer Studies in the Humanities and Verbal Behavior, Vol. 4 (1), 
9-15. 
BRANT, IRVING (1961). Settling the authorship of The Federalist. The American 
Hi8torical Review, Vol. 67, 71-75. 
285 

286 
REFERENCES 
Brewer's dictionary of phrase &, fable. Revised & enlarged. Harper & Brothers 
Publishers, New York. 
City of Washington Gazette (December 15, 1817). 
COCHRAN, WILLIAM G. (1961). On the performance of the linear discriminant 
function. Bulletin de l'Institut International de Statistique, Vol. 39, Part 2,435-447. 
COOKE, JACOB E., editor (1961). The Federalist. Meridian Books, The World 
Publishing Company, Cleveland and New York. 
COOLIDGE, JULIAN LOWELL (1925). An introduction to mathematical probability. 
Oxford, at the Clarendon Press, London. 
Daily National Intelligencer (March 19, 1817). 
DAMERAU, FRED J. (1975). The use of function word frequencies as indicators 
of style. Computers and the Humanities, Vol. 9, 271-280. 
DOLEZEL, LOBOMIR, and RICHARD W. BAILEY, editors (196H). Statistics and 
style. American Elsevier Publishing Company, New York. 
ELLEGARD, ALVAR (1962a). A statistical method for determining authorship: the 
Junius letters, 1769-1772. Gothenburg Studies in English No. 13. Acta Uni-
versitatis Gothoburgensis. Elanders Boktryckeri Aktiebolag, Goteborg. 
--(1962b) Who was Junius? Almqvist & Wiksell, Stockholm. 
-- (1978). The syntactic structure of English texts: A computer-based study 
of four kinds of text in the Brown University Corpus. Gothenburg Studies in English 
No. 43. Acta Universitatis Gothoburgensis, Goteborg, Sweden. 
ERDELYI, A. (1956). Asymptotic expansions. Dover Publications, Inc., New 
York. 
FARRINGDON, MICHAEL and JILL (1979). A computer-aided study of the prose 
style of Henry Fielding and its support for his translation of the Military History 
of Charles XII. In Advances in computer-aided literary and linguistic research. 
Proceedings of the Fifth International Symposium on Computers in Literary and 
Linguistic Research held at the University of Aston in Birmingham, UK, D. E. 
Alger, F. E. Knowles, and Joan Smith, editors. Published by AMLC for the 
Department of Modern Languages, University of Aston in Birmingham, 95-106. 
The Federalist (1788). Printed and sold by J. and A. McLean, New York. 
The Federalist (1818). Jacob Gideon, publisher. 
The Federalist. Sesquicentennial edition (1937), with an introduction by Edward 
Mead Earle. National Home Library Foundation, Washington, D. C. 
The Federalist. The Modern Library, Random House, New York. 
FISHER, R. A. (1936). The use of multiple measurements in taxonomic problems. 
Annals of Eugenics, Vol. 7, Pt. 2, 179-188. [Paper 32 in Contributions to mathe-
maticalstatistics by R. A. Fisher (1950). John Wiley & Sons, 32.179-32.188.] 
FIX, EVELYN, and J. L. HODGES, JR. (1951). Discriminatory analysis. Nonpara-
metric discrimination: consistency properties. USAF School of Aviation Medicine, 
Randolph Field, Texas. 
--- (1952). Discriminatory analysis. Nonparametric discrimination: small 
sample performance. USAF School of Aviation Medicine, Randolph Field, 
Texas. 
FORD, PAUL LEICESTER (1892). Essays on the Constitution of the United States. 
Historical Printing Club, Brooklyn, N. Y. 
FORD, PAUL LEICESTER, editor, (1898). The Federalist. Henry Holt and Company, 
New York. 

REFERENCES 
287 
FREEMAN, MURRAY F. and JOHN W. TUKEY (1950). Transformations related to 
the angular and the square root. The Annals of Mathematical Statistics, Vol. 21, 
607-611. 
FRIES, C. C. (1952). The structure of English. Harcourt, Brace, and Company, 
New York. 
GOOD, 1. J. (1952). Rational decisions. Journal of the Royal Statistical Society, 
Series B, Vol. 14, 107-114. 
GORE, A. P., M. K. GOKHALE, and S. B. JOSHI (1979). On disputed authorship 
of editorials in Kesari. Indian Linguistics, Vol. 40, 283-293. 
HAMILTON, JOHN C., editor (1864). The Federalist. J. B. Lippincott & Co., 
Philadelphia. 
HANLEY, MILES L. (1937). Word index to James Joyce's Ulysses. University of 
Wisconsin Press, Madison. 
HARRISON, P. N. (1921). The problem of the Pastoral Epistles. Oxford University 
Press, Oxford. 
HERD AN, G. (1956). Language as choice and chance. P. N oordhoff Ltd., Groningen, 
Holland. 
--- (1966). The advanced theory of language as choice and chance. Springer, 
Berlin. 
HODGES, J. L., JR. (1950). Discriminatory analysis. 1. Survey of discriminatory 
analysis. USAF School of Aviation Medicine, Randolph Field, Texas. 
HOEL, P. G. (1954). Introduction to mathematical statistics (2nd ed.). John Wiley 
& Sons, Inc., New York. 
JEFFREYS, HAROLD (1939, 1948, 1961). Theory of probability (1st ed.1939, 2nded. 
1948, 3rd ed. 1961). Oxford at the Clarendon Press, London. 
KENNY, ANTHONY (1978). The Aristotelian Ethics, a study of the relationship 
between the Eudemian and Nicomachean ethics of Aristotle. Clarendon Press, Oxford. 
--- (1979). Aristotle's theory of the will. Yale University Press, New Haven. 
--~ (1982). The computation of style, an introduction to statistics for students 
of literature and hltmanities. Pergamon Press, New York. 
KJETSAA, GEIR (1979). "And Quiet Flows the Don" through the computer. 
Association for Literary and Linguistic Computing Bulletin, Vol. 7 (3), 248-256. 
---, SVEN GUSTAVSSON, BENGT BECKMAN, and STEINAR GIL (1984). The 
authorship of The Quiet Don. Solum Forlag A.S., Oslo; Humanities Press: New 
Jersey. 
KUCERA, HENRY and W. NELSON FRANCIS (1967). Computational analysis of 
present-day American English. Brown University Press, Providence, R1. 
LODGE, HENRY CABOT, editor, (1888). The Federalist. G. P. Putnam's Sons, 
New York and London. 
MCMAHON, L. E., L. L. CHERRY, and R. MORRIS (July-August 1978). Statistical 
text processing. The Bell System Technical Journal, Vol. 57, 2137-2154. 
MEDVEDEV, Roy A. (1977). Problems in the literary biography of Mikhail 
Sholokhov. Translated from the Russian by A. D. P. Briggs. Cambridge University 
Press, Cambridge. 
MEIER, PAUL, and SANDY ZABELL (1980). Benjamin Peirce and the Howland 
will. Journal of the American Statistical Association, Vol. 75, 497-506. 
MENDENHALL, T. C. (1887). The characteristic curves of composition. Science, 
Vol. 9, No. 214, Supplement, 237-249. 

288 
REFERENCES 
--- (1901). A mechanical solution to a literary problem. Popular Science 
Monthly, Vol. 60, No.2, 97-,105. 
MILLER, G. A. (1956). The magical number seven, plus or minus two: some limits 
on our capacity for processing information. The Psychological Review, Vol. 63, 81-97. 
---, E. B. NEWMAN, and E. A. FRIEDMAN (1958). Length-frequency statistics 
of written English. Information and Control, Vol. 1,370-389. 
MITCHELL, BROADUS (1957). Alexander Hamilton, youth to maturity, 1755-1788. 
The Macmillan Company, New York. 
MOLINA, E. C. (1942). Poisson's exponential binomial limit. D. Van Nostrand 
Company, Inc., Princeton, N. J. 
MOOD, A. M. (1950). Introduction to the theory of statistics. McGraw-Hill Book 
Company, Inc., New York. 
MORTON, A. Q. (1978). Literary detection: how to prove authorship and fraud in 
literature and documents. Charles Scribner's Sons, New York. 
MOSTELLER, F., R. E. K. ROURKE, and G. B. THOMAS, JR. (1961). Probability 
with statistical applications. Addison-Wesley Publishing Company, Inc., Reading, 
Massachusetts. 
--, and J. W. TUKEY (1949). The uses and usefulness of binomial probability 
paper. Journal of the American Statistical Association, Vol. 44, 174-212. 
--, and D. L. WALLACE (1962). Notes on an authorship problem. In Pro-
ceedings of a Harvard Symposium on Digital Computers and Their Applications. 
Harvard University Press, Cambridge, Massachusetts, 163-197. 
--- (1963). Inference in an authorship problem. A comparative study of 
discrimination methods applied to the authorship of the disputed Federalist papers. 
Journal of the American Statistical Association, Vol. 58, 275-309. 
National Bureau of Standards (1949). Tables of the binomial probability distri-
bution. Applied Mathematics Series 6. U. S. Government Printing Office, Washing-
ton, D. C. 
The New York Times (November 19, 1959), p. 1. 
NEWMAN, MORRIS (1962). Matrix computations. Chapter 6 in Survey of numerical 
analysis, John Todd, editor. McGraw-Hill Book Company, Inc., New York, 
222-254. 
OAKMAN, ROBERT L. (1980). Computer methods for literary research. University 
of South Carolina Press, Columbia, SC. 
O'BRIEN, D. P., and A. C. DARNELL (1982). Authorship puzzles in the history of 
economics: a statistical approach. The Macmillan Press Ltd, London. Distributed 
by Humanities Press, Atlantic Highlands, NJ. 
PEARSON, EGON S. (1925). Bayes' theorem, examined in the light of experi-
mental sampling. Biometrika, Vol. 17, 388-442. 
The Port Folio (November 14, 1807). Vol. IV, No. 20, 318. 
PORTNOY, STEPHEN L., and DAVID L. PETERSEN (1984a). Biblical texts and 
statistical analysis: Zechariah and beyond. Journal of Biblical Literature, Vol. 103, 
11-21. 
--- (1984b). Genesis, Wellhausen and the computer: a response. Zeitschrift 
fur die attestamentliche Wissenschaft. 
RADDAY, YEHUDA T., and HAIM SHORE (1976). The definite article: a type-
and/or author-specifying discriminant in the Hebrew Bible. Association for Literary 
and Linguistic Computing Bulletin, Vol. 4 (1), 23-31. 

REFERENCES 
289 
-- HAIM SHORE, MOSHE A. POLLATSCHEK, arid DIETER WICKMANN (1982). 
Cknesis, Wellhausen and the computer. Zeitschrift fur die attestamentliche W issen-
schaft, Vol. 94, 467-481. 
---, and DIETER WICKMANN (1975). The unity of Zechariah examined in 
the light of statistical linguistics. ZA W, Vol. 87,30-55. 
RAIFF A, H., arid R. SCHLAIFElt (1961). Applied statistical decision theory. Division 
of Research, Graduate School of Business Administration, Harvard University, 
Boston, Massachusetts. 
RAo, C. R. (1952). Advanced statistical methods in biometric research. John Wiley 
& Sons, Inc., New York. 
ROBBINS, HERBERT (1951). Asymptotically submiriimax solutions of compound 
statistical decision problems. Proceedings of (ne Second Berkeley Symposium on 
Mathematical Statistics and Probability, edited by J. Neyman. University of Cali-
fornia Press, Berkeley and Los Angeles, 131-148. 
--- (1956). An empirical Bayes approach to statistics. Proceedings of the 
Third Berkeley Symposium on Mathematical Statistics and Probability, edited by 
J. Neyman. University of California Press, Berkeley and Los Angeles, Vol. I, 
157-163. 
ROBERTS, PAUL (1958). Understanding English. Harper & Brothers, New 
York. 
ROSSITER, CLINTON (1961). The Federalist papers, with an introduction, table of 
contents, and index of ideas by C. Rossiter. The New American Library of World 
Literature, Inc., New York. 
RUNYON, DAMON (1950). The Damon Runyon Omnibus, Three Volumes in One, 
Vol. 2, Money from horne. A nice price. J. B. Lippincott & Co., Philadelphia. 
SMIRNOV, N. V. (1961). Tables for the distribution and density functions of t-dis-
tribution ("Student's" distribution). Pergamon Press, New York. 
STIGLER, STEPHEN M. (1983). Economic thought and doctrine. Review of 
Authorship puzzles in the history of economics,' a statistical approach, by 
D. P. O'Brien and A. C. Darnell. The Journal of Economic History, Vol. 43, 
547-550. 
STRONG, JAMES (1890). The exhaustive concordance of the Bible. Eaton & Mains, 
New York. 
SUKHATME, P. V. (1938). On Fisher and Behrens' test of significance for the 
difference in means of two normal samples. Sankhya, Vol. 4, 39-48. 
TUKEY, JOHN W. (1958). Bias and confidence in not-quite large samples. 
Abstract. The Annals of Mathematical Statistics, Vol. 29, 614. 
WICKMANN, D. (1976). On disputed authorship, statistically. Association for 
Literary and Linguistic Computing Bulletin, Vol. 4 (1), 32-41. 
WILLIAMS, C. B. (1939). A note on the statistical analysis of sentence-length as a 
criterion of literary style. Biometrika, Vol. 31, 356-361. 
--- (1956). Studies in the history of probability and statistics IV. A note 
on an early statistical study of literary style. Biometrika, Vol. 43, 248-256. 
--- (1970). Style and vocabulary,' numerical studies. Griffin, London. 
--- (1975). Mendenhall's studies of word-length distribution in the works of 
Shakespeare and Bacon. Biometrika, Vol. 62, 207-212. 
WRIGHT, BENJAMIN FLETCHER, editor, (1961). The Federalist. The Belknap 
Press of Harvard University Press, Cambridge, Mass. 

290 
REFERENCES 
WUNDT, WILHELM (1912). An introduction to psychology. Translated by R. 
Pintner. George Allen & Oompany, Ltd. London. 
YULE, G. UDNY (1938). On sentence-length as a statistical characteristic of style 
in prose: with application to two cases of disputed authorship. Biometrika, Vol. 30, 
363-390. 
--- (1944). The statistical study of literary vocabulary. Oambridge University 
Press, Oambridge, England. 

Index 
Symbols are listed under the entry, "Symbols." For additional names 
of individuals see Acknowledgments at the close of each Preface. 
Adair, Douglass 2, 6, 10, 16, 285 
Adams, L. La Mar 277, 285 
Adjusted 2 x 2 tables 
for high-frequency words 218-220 
for low-frequency words 221 
Adjustment factor 
in penalty study 183, 189-190 
Agarkar, G. G. 
273, 274 
Anderson, T. W. 
57,285 
And Quiet Flows the Don 272, 273 
Anscombe, F. J. 75, 97, 98, 285 
Antilogs, table of 69 
Approximations 
need for 92-93, 119-120, 124, 
266-267 
to posterior expectations 138-154 
to posterior modes and maximum 
likelihood estimates 108-111 
Aristotle 276, 277 
Atlantic, The 39,243 
Atwood, Angela 279 
Authorship of the disputed Federalist 
papers 
historical introduction 2-6 
summary of results 263-264 
Authorship problems 
conclusions 265 
making simplified studies 249-251 
Bacon, Francis 270,271 
Bailey, Richard W. 268,269,272, 
277,278,279,285,286 
Bailey, S. A. 
10, 285 
Bayes'theorem 47-54,64, 112-113 
for continuous variables 64 
for discrete hypotheses 53-54 
example of application 52-55 
two stages of use 58-59, 112 
Bayes, Thomas 1,285 
Bayesian inference 1-2,50-51 
abstract structure in classification 
problems 111-117 
remarks and conclusions 264, 
266-267 
see also main study; robust Bayesian 
analysis 
Beaton, Albert E. 43-44 
Beckman, Bengt 287 
Benson, Egbert 4 
Benson lists 3,4, 5, 14 
Berkson, Joseph 48-49,285 
Beta distribution 63 
291 
of 7J 
65,103 
mean is a relative mode 107 
in robust Bayesian analysis 219 
role of the extreme tails 137 
of T 
63, 65, 103, 128 
Bible, King James version 39,243 
word rates for 244-248 
Bible, The 270, 276, 277 
Binomial dispersion index 24 
frequency distribution of 28 
Binomial distribution 23-24 
fitted and compared to incidence 
distributions 24-28 

292 
INDEX 
Blyman, Mary 212 
Bona ventura 274 
Bourne,E. G. 
5, 252, 285 
Bracketing the prior distribution 
63-64, 1i6-117, 124 ff., 266 
see also prior distributions; under-
lying constants 
Brainerd, Barron 269; 270, 271, 285 
Brant, Irving 6, 284, 285 
Brentano, C1. 
274 
Brewer 243, 286 
Brougham, IIenry 274 
Burr, Aaron 4 
by 
frequency distribution of rate 168 
in penalty study 185-189 
table of contributions to log odds 
78 
"Caesar" letters 251-252 
Calibrating set of papers 200, 229 
choice for weight-rate analysis 201 
results for 
in three-category analysis 
237-238 
in weight-rate analysis 207-:208 
size relative to screening set 201 
Carlson, Roger 215 
Characteristic curve of composition 
259 
Charles XII (King of Sweden) 277 
Cherry, L. L. 
269, 287 
Child, Sir Josiah 274 
Classification parameter 111, 181 
Classification problems 
abstract structure 111 ff. 
conclusions 265-266 
single vs. simultaneous 114 
Cochran, William G. 
155,286 
Collocation 270, 274, 280 
Combining evidence from several words 
54-55 
Commas, rate of 273, 274, 280 
Composite Madison 195 
Computer 268, 269 
role of 269 
UNIX 269 
Conditional analysis 116-117 
Contagious distributions 31,96 
Contaminated normal distributions 
214 
Contextual dangers 18, 68, 249-250, 
265-266 
inadequate protection in secondary 
studies 201,216,226, 236-237 
introd.uced by Neutral Trade essays 
·20-22 
asymmetry of distribution of 'fJ 
130-131 
treatment in main study 68, 
195-199 
Cooke, Jacob E. 2, 4, 5, 6, 9, 10, 86, 
286 
Coolidge, Julian L. 47-49, 286 
" Corrector" 5 
Correlation a,nd dependence 
in the abstract strUcture 111-116 
between rates for different words 
35-37 ; matrix for 11 words 
161 
between rates in successive blocks of 
text 23-28 
effect on log odds 84, 155-163 
frequency distribution of 36 
multinomial 35 
in the weight-rate study 200-201 
Cumulants of the negative binomial 
distribution 94 
Cut-points for rates 
rules for determining 231 
table 232-233 
D* 272,273 
Damerau, Fred J, 281, 286 
Darnell, A. C. 
274,275,288 
Davis, Miles 36,67,69, 105, 185 
Delta method 139, 151 
applied to sample moments 98, 127 
Discriminant function 9, 200 
assessing the results of 210-214 
frequency distribution of 8 
in the weight-rate analysis 201 
Discrimination problems, see 
classification problems 

Disputed authorships (Chapter 10) 
And Quiet Flow8 the Don 272, 273 
Aristotle 276, 277 
Bible, The 270, 276, 277 
Economic history 274,275 
forensic disputes 278-280 
Howland will 279,280 
Kesari 273,274 
military history of Charles XII 
277,278 
Nachtwachen, Die 274 
Shakespeare-Bacon 268,270,271 
translations 277, 278, 281 
"Disputed" papers 3-6 
results for 
main study 74-80,87 
pilot study 14 
robust Bayesian analysis 228 
in summary 263-264 
three-category analysis 241-242 
weight-rate analysis 210-214 
Distributional assumptions 
abstract structure 111-116 
for discriminant functions 212-214 
importance of careful study 264, 
266 
independence 103-104, 200-201 
for parameters 103 
for word frequencies 99, 118 
Dolezel, Lobomir 269,285,286 
Don Cossacks 272 
Early Madison 195 
Economic history 274, 275 
Editing by Hamilton or Madison 15, 
90 
Ellegard, Alvar 269, 277, 279, 281, 
286 
Empirical Bayes procedures 115 
enough 
corrections to log odds approxi-
mation 144 
counts of occurrences by individual 
paper 12-14 
in the pilot study 11-14 
table of contributions to log odds 79 
Enumerations 257 
INDEX 
293 
Erdelyi, A. 
148, 153, 286 
Expectations, approximate evaluation 
of, 8ee posterior expectation; 
two-point formulas 
Expected and observed log odds 
compared 173-176 
Expected and observed penalties 
compared 185-190 
defined 184 
Exterior set of papers 11-13, 40, 283, 
284 
Farringdon, Jill 277, 278, 286 
Farringdon, Michael 277,278,286 
Federali8t, The 1 ff. 
conclusions on authorship 263-264 
edition used in present study 7 
editions of 
Cooke 2, 6, 86, 286 
Gideon 4, 5, 14, 15, 286 
Lodge 7,287 
McLean 4, 83, 286 
Modern Library 7, 286 
Sesquicentennial 7, 286 
Rossiter 6, 289 
Wright 6, 289 
history of the authorship dispute 
2-6, 14-15 
late Hamilton papers 83 
8ee also disputed papers; editing by 
Hamilton or Madison; joint 
papers; papers of known 
authorship 
Felice 77, 252 
Fielding, Henry 277,278 
Final odds (of authorship) 50, 54-59, 
69 ff., 100, 112-114, 118-119, 
219-224 
8ee log odds 
believability 88-91 
effect of outrageous events on 
90-91 
justification within the model 
88-90 
in the robust Bayesian analysis 
219-229 
tables of, see log odds 

294 
INDEX 
Fisher, R. A. 
1,97,286 
Fix, Evelyn 286 
Ford, Paul Leicester 5, 251, 286 
Forensic disputes 268, 278-280 
Fowles, John 270 
Francis, Ivor S. 36, 126, 140, 165, 
215, 235 
Francis, W. Nelson 281, 287 
Freeman, Murray F. 157,287 
Frequency distributions 
empirical (tables and figures) 
of adjectives 8 
of category of use of of 255 
of X2 statistics in tests of 
binomiality 28 
of correlations between word rates 
36 
of estimates of 11 
131 
of estimates of (J' and T 
62 
of incidence, for each of 51 words 
25-27 
of a measure of discriminating 
ability 42 
of nouns 8 
of numbers of Hamilton papers 
with a low rate 218 
of occurrences, for each of 51 
words 29-30; for each of 
10 words 32-33 
of one- and two-letter words 
8 
of values of a discriminant 
function 8 
of word length 258,260-261 
of word rates: also 167; an 
167; both 171; by 17,168; 
commonly 18; from 
17 ; 
innovation 18; of 168; on 
169; the 
8; there 
169; this 
170; though 172; to 
17,170; 
upon 19, 171; war 19 
theoretical (tables) 
fitted binomial frequencies for 
incidence of each of 51 words 
25-27 
fitted negative binomial frequencies 
for occurrences of each of 10 
words 32-33 
fitted Poisson frequencies for 
occurrences of each of 10 words 
32-33 
Poisson with mean 2,31 
Poisson and negative binomial 
compared 34 
two Poisson's compared 153 
Friedman, E. A. 39, 288 
Fries, C. C. 
39, 287 
Function words 17,39,280 
list 38 
Gallaher, Miriam 256 
Gamma distribution 65, 95, 96 
of 6 65,103 
mean is a relative mode 107 
Gill, Steinar 287 
Gokhale, M. K. 273, 287 
Good, I. J. 191, 287 
Gore, A. P. 273, 287 
Granger, Gideon 5 
Green, Hetty 279 
Gustavsson, Sven 287 
Hamilton, Alexander 1 if. 
word rates for 244-248 
see also authorship of the disputed 
Federalist papers 
Hamilton, John Church 5, 287 
Handlin, Joanna F. 253 
Hanley Concordance 243, 287 
Harris, Emily 279 
Harrison, P. N. 276, 287 
Hearst, Patricia 279 
Herdan, G. 
269,287 
High-frequency words 
in concordances 269 
in the main study 
contributions to log odds 78 
in the robust Bayesian analysis 
definition and list 217 
method of obtaining log odds 
217-220 
distribution of 2 x 2 tables 218 
see also words 
Historical information 
role in formal analysis 113-114 

Hodges, J. L., Jr. 286, 287 
Hoel, P. G. 24, 287 
Hoffman,E.T.A. 274 
Homogeneity problems 275-277 
Hoodes, Robert A. 
43 
Howland, Sylvia 279 
Howland will 268, 279, 280 
Hunt, G. 283-284 
Importance of a word 
in the main study 55-56,58, 
158-159; table 158 
in the weight-rate analysis 
202-204; table 204 
Independence 
in the abstract structure 111, 115 
of historical evidence 113 
in the main study 99 
of observations 274, 275 
of variables 280 
in the weight-rate analysis 200, 201 
see also correlation and dependence 
Index 
of performance in screening study 
41-42 
word index 42-43 
Inefficient estimates 111 
Information 
Fisher 97 
Shannon 191 
Ingalls, Theodore S. 44 
Initial odds (of authorship} 50,54-59, 
. 
112-114 
Integrals, approximate evaluation of, 
see posterior expectation 
"Jackknife" technique 126-127 
James, Henry 270 
James, William 39 
Jay,John 2,3,4,5,243 
comparison with Hamilton and 
Madison 248 
text used 243 
word rates for 244-248 
Jefferson, Thomas 3,5,9 
Jeffreys, Harold 2, 213, 287 
INDEX 
295 
"Joint" papers 3-6 
further analysis of Paper No. 20 
252-253 
results for 
main study 74 
pilot study 14 
robust Bayesian analysis 228 
. in summary 263-264 
three-category analysis 241-242 
weight-rate analysis 210-214 
Joshi, S. B. 273,287 
Joyce, James 243-248 
Junius 271,277,281 
}(enny,llnthony 269,276,277,287 
}(ent, Chancellor James 4-5 
}(ent list 4-5, 14 
Kesari 273, 274 
}(jetsaa, Geir 272, 273, 287 
}(leyle, Robert 133, 243, 259 
}(lingemann, E. A. 274 
}(nown papers, see papers of known 
authorship 
}(ryukov, Fyodor 272,273 
}(ucera, Henry 281,287 
Laplace integral expansion 148, 153 
applied to posterior expectations 
multivariate 150-151 
univariate 148 
asymptotic development 153-154 
transformation of variables 149, 
152 
Late Hamilton papers 83 
Late Madison 195 
Length of paper, see paper length 
Likelihood ratio 54-57,120-124 
behavior of 
negative binomial 97, 122-124 
Poisson 122 
for correlated normals 155, 160 
discussion of desirable shapes 
120-121 
estimates and confidence limits for, 
in the weight-rate analysis 
212-214 

296 
INDEX 
Likelihood ratio (continued) 
monotone 122-124 
see also log odds; odds factors 
Linear discriminant function, see 
discriminant function 
Littauer Statistical Center, Harvard 
University 43 
Lodge, Henry Cabot 5, 71, 283, 287 
Log odds 55 
see also final odds 
in the main study 
adjustments to 84-88; in penalty 
study 189-190 
comparison of Poisson and 
negative binomial 75, 81 
conditional and unconditional 
118-119 
contributions of marker and 
high-frequency words 81-82 
effects of correlations 155-163 
illustrated for 15 pairs of words 
157-159; for 11 words treated 
jointly 160-161 
mean and variance of 
adjustment 155-156, 160 
effects of systematic variations in 
Madison's writings 195-199; 
table 196 
effects of varying the prior 
distributions, see prior 
distributions, effects ofvarying 
evaluation of 119-120 
expected log odds 
under the negative binomial 
model 178-180; table 164 
under a normal approximation 
155-156; table 158 
under the Poisson model 177 ; 
table 164 
in total, compared to observed 
log odds 173; paper-by-
paper 174-175 
by word group 176 
tables of 
by individual words 78-80 
in total for the disputed and 
joint papers 74; adjusted 
88 
in total for the late Hamilton 
papers 83 
in total for the papers of known 
authorship 70-73 
by word groups 76 
in the robust Bayesian analysis 
for high-frequency words 
217-220 
for low-frequency words 
220-224 
tables for disputed and joint 
papers 229; for papers of 
known authorship 225-228 
in the three-category analysis 
interpretation 238-239 
role as weights 229 
rules for computing 233 
table 232-233 
in the weight-rate analysis 
213-214; table 213 
Low-frequency words 
in the robust Bayesian analysis 
definition and list 217 
method for obtaining log odds 
220-224 
weakness of using rates 201, 220 
see also marker words; words 
Madison, James 1 ff. 
word rates for 244-248 
see also authorship of the disputed 
Federalist papers 
Main study 46-199 
abstract structure 111-117 
methods illustrated for Poisson 
model 51-64 
results on authorship 
for disputed and joint papers 
74-75, 87-88 
for papers of known authorship 
69-73, 75 
summarized 263-264 
theoretical basis 92-199 
two stages of analysis 58-59 
Marker words 10-11 
log odds for 77-82 
Marlowe, Christopher 270 

Mathematical model 22 
Matrix inversion 
use of a simple approximation 
162-163 
Maximum likelihood estimation 
iterative determination 105, 108 
choice of initial values 108-111 
negative binomial 97 
McCulloch, J. R. 274 
McMahon, L. E. 269,287 
Medvedev, Roy A. 272, 273, 287 
Meier, Paul 279, 287 
Mendenhall, T. C.259, 270,271,287, 
288 
Method-of-moments estimates 
in a binomial problem 129-130 
in choice of Pl and P2 
125-130 
as initial estimates for iteration 
108....;111 
of negative binomial parameters 
97-98 
tables for 22 words 101; for 
30 words 110 
Method parameter p, see model 
parameter; prediction method 
Mill, James 274 
Mill, John Stuart 274 
Miller, G. A. 7,39,288 
Miller-Newman-Friedman list 28, 
37-39, 243-248, 250, 288 
sources 243 
word rates for 244-248 
Minimum penalty estimation 188 
Mitchell, Ann 212 
Mitchell, Broadus 6, 288 
Mixture assumption in study of 
systematic variation 195, 
198-199 
Modal approximation 60, 100, 119 
in the abstract structure 116-117 
accuracy. 85-86 
methods for studying accuracy 
138-154 
illustrated for 5 words 143-'-146 
which mode 1 104-105, 106-108 
Modal estimates 67, 104-107 
choice of initial estimate 108-111 
iterative determination 105 
table of 66, 110 
for upon 
136 
INDEX 
297 
which'mode1 104-107, 139-140, 
149 
Model parameter p 99, 163, 181 
Model II analysis of variance 52, 115 
Molina, E. C. 31, 288 
Monotone likelihood ratio, see 
likelihood ratio 
Mood; A. M. 
213, 288 
Morris, R. 269, 287 
Morton, A. Q. 268, 269, 270, 276, 278, 
281, 288 
Mosteller, F. 6-10, 23, 42, 43, 288 
Nabokov, Vladimir 272 
N achtwachen; Die 274 
Negative binomial distribution 34-35, 
51, 65, 93-98, 99 
cumulants 94 
formula 34, 93 
likelihood function 97 
likelihood ratio and its behavior 
97, 122-124 
limiting behavior 94-95 
mixture representation 95, 96 
parameters 
choice of 96-97 
estimation 97-98, 100-101 
for pairs of distribution 102-103 
prior distributions 65, 103; see 
also prior distributions 
table of estimates for final words 
66; of method-of-moments 
estimates for 22 words 101 
penalty study of adequacy 185-190 
possible strange behavior illustrated 
for upon 135-138 
relation to Poisson 93-95 
sample moments 98 
Newman, E. B. 39,288 
Newman, Morris 163,288 
Newton-Raphson iteration 105 
Nonparametric methods 274,275 
Non-Poissonness 35,94,96-97 
combined 102 
differential 101-102 

298 
INDEX 
N on-Poissonness (continued) 
estimates for 22 words 100-101 
transformations of 101-102, 
140-141 
Nuisance parameters 
formal analysis 114-117 
Oakman, Robert L. 269, 288 
O'Brien, D. P. 274,275,288 
Odds 9,53-54 
see log odds; final odds; odds factor 
Odds factor 100,112-114,117-120 
Odoroff, Charles L. 
126, 140, 165, 203 
of 
frequency by category of use 
255-256 
on 
corrections to approximate log odds 
144-145 
correlation 84, 157-159 
frequency distribution of rate 169 
in penalty study 185-189 
table of contributions to log odds 
78 
Outrageous events 90-91 
P-values for assessing discrimination 
211 
Paper, defined and identified 17, 
19-21;283-284 
Paper length 
adjustments in study of regression 
effects 178, 180 
adjustments to log odds in robust 
Bayesian analysis 221, 224 
denoted by w 52, 93 
as a discriminator 259 
for screening and calibrating sets 
209 
table of 70-74; means 178 
Papers of disputed and joint 
authorship, see disputed papers; 
joint papers 
Papers of known authorship 
exterior to The Federalist 283-284 
from The Federalist 3-6, 70-73 
results for 
main study 69-73 
pilot study 12-13 
robust Bayesian analysis 
235-239 
three-category analysis 225-227 
weight-rate analysis 205-208 
Parameters 
classification 111 
of data distributions 57, 60-67 
distribution of estimates 62, 107 
model 99 
of prior distributions 52, 63; see 
also underlying constants 
treated as random variables 58, 
60 
see also prior distributions; posterior 
distributions; negative binomial 
distribution, parameters 
Particles and connectives 276 
Parts of speech 276 
Paul,Jean 274 
Pearson, Egon S. 
48-49,288 
Peirce, Benjamin 279 
Peirce, Charles Sanders 279 
Penalties 
expected 
defined 184 
relation to Shannon information 
191 
table of bench-mark values 185 
tables for the Federalist 
application 186-188 
observed 
choice of penalty function 191 
comparison with expected 
185-190 
defined 184, 191 
effect of adjustment factor 
189-190 
effect of initial odds 188-189 
likelihood interpretation 192-195 
minimizing, as an estimation 
procedure 188 
relation to log odds 192 
standardized deviate 185 
tables for the Federalist 
application 186-190 

Penalty function 
basis of choice 190-192 
in Federalist application 183-190 
minimization property 184, 191 
table of bench-mark values 185 
see also penalties 
Penalty study 180-195 
adequacy of Poisson and negative 
binomial models 185-190 
specification and labeling of methods 
182-183 
test sequence 181-182 
see penalty function; penalties 
Petersen, David L. 277,288 
Phrases 45 
Poetry versus prose 268, 270, 271 
Poisson distribution 28, 31-34, 51, 
95-96, 97, 99 
fitted and compared to observed 
and negative binomial 
distributions 32-34 
likelihood ratio 122 
penalty study of adequacy 185-190 
used to describe methods for main 
study 51-64 
Pollatschek, Moshe A. 
277,289 
Portnoy, Stephen L. 277,288 
Position of word in sentence 270, 
274 
Posterior distributions 
in the abstract structure 116-117 
illustrated for also 64 
mode of 60, 67, 105-107 
tables 66, 85 
see also modal estimates 
of parameters 58-59, 103-105 
relative density, 107 
standard deviations of 85 
Posterior expectations 
approximate evaluation 138-154 
components of error 143-144 
multivariate 150-152 
using means and variances 139 
using modes and derivatives 
139 
see Laplace integral expansion 
asymptotic theory 152-154 
as an integral 60, 139 
INDEX 
299 
Prediction method 181-183 
comparisons, for the Federalist 
application 185-190 
expected and observed penalties 
184 
Prior distributions 48-49 
see also underlying constants 
choice of 61, 100-102, 124-138 
effects of varying 63,67,75,84-85, 
132-138 
tables 70-74, 134 
empirical base 61-62, 266 
of negative binomial parameters (1', 
T, g, n 65, 103, 124-138 
of parameters 58-59, 61-67, 103, 
114-116 
of Poisson parameters (1', T 61-63 
in robust Bayesian analysis 
choice of 220, 223 
high-frequency words 219 
low-frequency words 222 
Probability 
of authorship, see final odds; log 
odds; initial odds 
interpretation 47-50 
see also odds; prior distributions; 
posterior distributions 
Probability predictions 
defined 181 
evaluating methods of 180-195 
Proportionate pairs 269, 280 
"Publius" 2 
Radday, Yehuda T. 277,288,289 
Raiffa, H. 214, 289 
Range 202 
Rao, C. R. 57,289 
Rao, P. S. R. S. 
241 
Rates, see word rates 
Regression effects 
in the main study 163-180 
expected and observed rates 
compared 165-166 
by single words 240-241 
in the three-category analysis 
239-241 
in the weight-rate analysis 208-209 

300 
INDEX 
Relative density 107 
Relative mode 104-107, 139-140, 149 
Rencher, Alvin C. 
277,285 
Robbins, Herbert 115, 289 
Roberts, Paul 9, 289 
Robinson, Hetty (Hetty Green) 279 
Robust Bayesian analysis 215-228 
methods 215-224 
results 
for disputed and joint papers 228 
for papers of known authorship 
225-227 
Robustness 215 
Rossiter, Clinton 6; 289 
Rourke, R. E. K. 23,288 
Runyon, Damon 75,289, 
Rush, Richard 5 
Rush list 5 
Savage, L. J. 51 
Schelling, Fr. 274 
Schlaifer, Robert 214, 289 
Scott, Sir Walter 269,270 
Screening set of papers 
200, 229 
choice of 
in robust Bayesian analysis 
216-217 
in three-category analysis 
~29 
in weight-rate analysis 201 
results for 
in robust Bayesian analysis 
225-226 
in three-category analysis 
235-236 
in weight-rate analysis 205-206 
Screening study 11,39-42, 250-251 
list of words obtained 38 
Selecting words, methods of 
for possible qse in study 37-43 
in simplified studies 249-251 
for use in final inference 
main study 54-55,67-69, 
195-198 
robust Bayesian analysis 
216-217 
three-category anlJ,lysis 229-234 
weight-rate analysis 201, 203 
Selection effects 51-52,67-68,200,275 
illustrated, for also 64 
Sentencelength 6-7,273,280 
average 7 
standard deviation 7 
Shakespeare, William 270,271 
Shakespeare-Bacon controversy 268, 
270, 271 
Shannon, C. 
191 
Sholokhov, MikhlJ,il 272,273 
Shore, Haim 277,288,289 
Sidney, Sir Philip 271 
Simultaneous classification 
~O, 114, 
182 
Smirp.ov, N. V. 
214, 289 
Solzhenitsyn,AIeksandr 272 
Spacing of frequent words 281 
Square~root transformation 157 
Stages of analysis 58-{!0 
block diagram 59 
St!tlin, Joseph 272 
Standardized deviate 
for observed penalty 185 
Stewart, Ralph A., Jr. 43 
Stigler, Stephen M. 275,289 
Strength of a word, see importance of 
., 
a word 
Strong Concordance 243, 289 
Stylistics 269 
boo~s on 269 
Stylometrics 
2~9, 270 
Suffix convention 150 
Sukhatme, P. V. 214,289 
Symbols: a partial list of those used in 
several sections with a common 
meaning (subscripts are omitted 
here) 
d 98, 100 
fnb 93 
fp 95 
m 61 
nb 93, 99 
P 99 
s 61 
t 62 
w 52, 93 
a 
103 
fl 63, 65, 103 

Symbols (continued) 
P* 
107 
Y 103 (in Sections 4.2, 4.6 only) 
o 93-94 
'YJ 
102 
o 111 
K 93-94 
It 52,93-94, 111 
g 102 
u 61, 102 
T 61, 102 
¢ 99 
A 
106, 140 
~ 59,94 
Synonyms 256 
Systematic variation in Madison's 
writings 68, 195-199 
t-statistics for assessing discrimination 
210-211 
log odds from t 212-214 
Tails of distributions 
contrast between prior and data 
distributions 138 
and likelihood ratio behavior 
120-121 
and robustness 215 
Temple, Sir William 77, 252 
Test sequence, see penalty study, test 
sequence 
Thomas, G. B., Jr. 23, 288 
Three-category analysis 229-242 
method 229-234 
results 
for disputed and joint papers 
241-242 
for papers of known authorship 
235-239 
study of regression effects 239-241 
to 
corrections to approximate log odds 
144 
frequency distribution of rate 170 
in penalty study 185-189 
rates in different groups of writings 
198 
table of contribution to log odds 78 
Tilak, B. G. 273, 274 
Tolstoy, Leo 272 
Torrens, R. 274 
INDEX 
301 
Transition frequencies 274, 280 
Translations 277, 278, 281 
Tukey, John W. 42,126,157,288,289 
Two-point formulas for expected log 
odds 179-180 
2000-word set of papers 173, 216 
identified 216 
log odds for 225-226 
mean paper lengths 178 
Type-token ratio 273 
Ulysses 243 
word rates for 244-248 
Underlying constants 
choice of Pi and P2 124-130; of Pa 
130-131; of P4 and Ps 
131-132 
defined 52, 63, 115 
effect of varying, see prior 
distributions, effect of varying 
formally specified 65, 103 
in relation to alternative choice of 
modes 107 
sets used 
complete table 102 
final choices 65 
initial choices 63 
to study effects of varying prior 
distributions 
132 
upon 
changes in counts 86 
comparison of observed and 
expected log odds 176 
corrections to approximate log odds 
144-145 
correlation 84, 157-159 
counts of occurrences by individual 
paper 12-14 
detailed study of likelihood and of 
log odds 135-138 
discovery 11 
empirical and theoretical 
distributions of occurrences 
32, 33 

302 
INDEX 
upon (continued) 
frequency distribution of rate 19, 
171 
justification of magnitude of odds 
89 
performance in secondary studies, 
paper-by-paper 204-210, 
225-228, 232,235~239, 242 
table of contributions to log odds in 
the main study 76 
Validating set of papers 216 
log odds for 226-228 
Validation 
ofthe main study 69,75,83 
penalty study 181-190 
regression study 163-176 
of the robust Bayesian 226-228 
of the three-category analysis 
236-239 
of the weight-rate analysis 
207-208 
Variables for discrimination (Chapter 
10) 
collocation 270, 274, 280 
commas, rate of 273, 274, 280 
function words 280 
particles and connectives 276 
parts of speech 276 
position of word in sentence 270, 
274 
proportionate pairs 269, 280 
sentence length 273,280 
spacing of frequent words 281 
transition frequencies 274, 280 
type-token ratio 273 
word frequencies 274,275 
word length 270, 271, 273, 280 
Variables for discrimination (The 
Federalist) 
16, 45 
see words; word rates 
. miscellaneous unsuccessful choices 
256-257,259 
phrases 45 
sentence length 6-7 
word length 259-262 
words classified by use 252-256 
Wallace, D. L. 43, 288 
Waves A, B, C of papers 11-13, 
39-42 
Weight-rate analysis 200-214 
method 200-203 
results 
for disputed and joint papers 
210-214 
for papers of known authorship, 
205-208 
study of regression effects 208-209 
Weights for word rates 202,204 
"Well-liked" words 68,204 
Wetzel, F. G. 274 
while-whilst 
changes in counts 86 
corrections to approximate log odds 
144, 146 
counts of occurrences by individual 
papers 12-14 
initial discovery 10 
in the pilot st-ddy 10-14 
table of contributions to log odds 
79-80 
Wickmann, Dieter 274,277,289 
Wiitanen, Wayne 43 
Williams, C. B. 6, 259, 269, 270, 271, 
273,289 
Williams, Frederick 6-10 
Willson, C. Harvey 43 
Word counts 
distribution of, see frequency 
distributions 
effect of errors and changes on log 
odds 86-87 
by hand 44 
by machine 43-44 
paper-by-paper, for four words 
12-14 
suggested simplified method 250 
Word frequencies 274,275 
counting difficulties 269 
distributional assumptions 
formal 99 
heuristic 96 
high-frequencies 269, 270, 276 
Word groups 
for the main study 68-69, 77 

Word groups (continued) 
log odds by 76-77, 83 
regression effect by 176 
for the robust Bayesian analysis 
217 
for the three-category analysis 234 
results by 235-242 
for the weight-rate analysis 
203-204 
results by 205-210 
Word length 
comparisons of Bacon, Shakespeare, 
and Sidney 271 
comparisons of Hamilton's and 
Madison's 261-262 
as a discriminating variable 
258-262, 270, 271, 280 
distributions of 258, 260, 261, 271 
273 
Word rates 17 
correlation between pairs of words 
35-37 
matrix for 11 words 161 
dependence in successive blocks of 
text 23-28 
distributions of 28-35 
mathematical models for describing 
22 ff. 
systematic deviations between The 
Federalist and Neutral Trade 
68, 194-198 
tables of 
distributions in Hamilton, Madison 
and disputed papers for 11 
words 167-172; see also 
INDEX 
303 
frequency distributions 
for Madison over time 21 
for 165 words in 6 sources of 
writing 244-248 
observed and expected, for 11 
words 166 
variation with time 19-22 
Words 16 
contextual 18, 39, 51, 68, 204 
function 17, 38-39 
marker 10-11 
meanings and uses 253-256 
pools 37--43 
role in main study 51-52 
used in final inference 
main study 66 
robust Bayesian analysis 217 
three-category analysis 232-233 
weight-rate analysis 203-204 
used in study 38,43 
well-liked 68, 204 
see also selection of words; word 
length; word rates; by; enough; 
of; on; to; upon; while-whilst 
Wright, Benjamin Fletcher 6, 289 
Wundt, Wilhelm 7, 290 
Yeager, Marie 133,243,259 
Youtz, Cleo 43, 126 
Yule, G. Ddny 6, 269, 290 
Zabell, Sandy 279, 287 
Symbols are listed under the entry, "Symbols." For additional names 
of individuals see Acknowledgments at the close of each Preface. 

Lecture Notes in Statistics 
Vol. 1: R. A. Fisher: An Appreciation. Edited by S. E. Fienberg and D. V. Hinkley. xi, 208 
pages, 1980. 
Vol. 2: Mathematical Statistics and Probability Theory. Proceedings 1978. Edited byW. 
Klonecki, A. Kozek, and J. Rosinski. xxiv, 373 pages, 1980. 
Vol. 3: B. D. Spencer, Benefit-Cost Analysis of Data Used to Allocate Funds. viii, 296 
pages, 1980. 
Vol. 4: E. A. van Doorn, Stochastic Monotonicity and Queueing Applications of Birth-
Death Processes. vi, 118 pages, 1981. 
Vol. 5: T. Rolski, Stationary Random Processes Associated with Point Processes. vi, 
139 pages, 1981. 
. 
Vol. 6: S. S. Gupta and D.-Y. Huang, Multiple Statistical Decision Theory: Recent 
Developments. viii, 104 Pages, 1981. 
Vol. 7: M. Akahira and K. Takeuchi, Asymptotic Efficiency of Statistical Estimators. viii, 
242 pages, 1981. 
Vol. 8: The First Pannonian Symposium on Mathematical Statistics. Edited by P. 
Revesz, L. Schmetterer, and V. M. Zolotarev. vi, 308 pages, 1981. 
Vol. 9: B. J¢rgensen, Statistical Properties of the Generalized Inverse Gaussian Distri-
bution. vi, 188 pages, 1981. 
Vol. 10: A. A. Mcintosh, Fitting Linear Models: An Application of Conjugate Gradient 
Algorithms. vi, 200 pages, 1982. 
Vol. 11: D. F. Nicholls and B. G. Quinn, Random Coefficient Autoregressive Models: An 
Introduction. v, 154 pages, 1982. 
Vol. 12: M. Jacobsen, Statistical Analysis of Counting Processes. vii, 226 pages, 1982. 
Vol. 13: J. Pfanzagl (with the assistance of W. Wefelmeyer), Contributions to a General 
Asymptotic Statistical Theory. vii, 315 pages, 1982. 
Vol. 14: GUM 82: Proceedings of the International Conference on Generalised Linear 
Models. Edited by R. Gilchrist. v, 188 pages, 1982. 
Vol. 15: K. R. W. Brewer and M. Hanif, Sampling with Unequal Probabilities. ix, 164 
pages, 1983. 
. 
Vol. 16: Specifying Statistical Models: From Parametric to Non-Parametric, Using 
Bayesian or Non-Bayesian Approaches. Edited by J: P. Florens, M. Mouchart, J. P. 
Raoult, L. Simar, and A. F. M. Smith. xi, 204 pages, 1983. 
Vol. 17: I. V. Basawa and D. J. Scott, Asymptotic Optimal Inference for Non-Ergodic 
Models. ix, 170 pages, 1983. 

Lecture Notes in Statistics 
Vol. 18: W. Britton, Conjugate Duality and the Exponential Fourier Spectrum. v, 226 
pages, 1983. 
Vol. 19: L. Fernholz, von Mises Calculus for Statistical Functionals. viii, 124 pages, 
1983. 
Vol. 20: Mathematical Learning Models-Theory and Algororithms: Proceedings of a 
Conference. Edited by U. Herkenrath, D. Kalin, W. Vogel. xiv, 226 pages, 1~83. 
Vol. 21: H. Tong, Threshold Models in Non-linear Time Series Analysis. x, 323 pages, 
1983. 
Vol. 22: S. Johansen, Functional Relations, Random Coefficients and Nonlinear 
Regression with Application to Kinetic Data. viii,126 pages. 1984. 
Vol. 23: D. G. Saphire, Estimation of Victimization Prevalence Using Data from the 
National Crime Survey. V, 165 pages. 1984. 
Vol. 24: S. Rao, M.M. Gabr, An Introduction to Bispectral Analysis and Bilimar Time 
Series Models. IX, 280 pages,1984. 
Vol. 25: Time Series Analysis of Irregularly Observed Data. Proceedings, 1983. Edited 
by E. Parzen. VII, 363 pages, 1984. 

