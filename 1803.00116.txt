arXiv:1803.00116v3  [cs.AI]  24 Jan 2019
Separators and Adjustment Sets in Causal Graphs:
Complete Criteria and an Algorithmic Framework *†
Benito van der Zander
Institute for Theoretical Computer Science, Universität zu Lübeck, Germany
Maciej Li´skiewicz
Institute for Theoretical Computer Science, Universität zu Lübeck, Germany
Johannes Textor
Institute for Computing and Information Sciences, Radboud University Nijmegen, Nijmegen, The Netherlands
Abstract:
Principled reasoning about the identiﬁability of causal effects from non-experimental data is an important application
of graphical causal models. This paper focuses on effects that are identiﬁable by covariate adjustment, a commonly
used estimation approach. We present an algorithmic framework for efﬁciently testing, constructing, and enumerating
m-separators in ancestral graphs (AGs), a class of graphical causal models that can represent uncertainty about the
presence of latent confounders. Furthermore, we prove a reduction from causal effect identiﬁcation by covariate
adjustment to m-separation in a subgraph for directed acyclic graphs (DAGs) and maximal ancestral graphs (MAGs).
Jointly, these results yield constructive criteria that characterize all adjustment sets as well as all minimal and minimum
adjustment sets for identiﬁcation of a desired causal effect with multiple exposures and outcomes in the presence of
latent confounding. Our results extend several existing solutions for special cases of these problems. Our efﬁcient
algorithms allowed us to empirically quantify the identiﬁability gap between covariate adjustment and the do-calculus
in random DAGs and MAGs, covering a wide range of scenarios. Implementations of our algorithms are provided in
the R package DAGITTY.
Keywords:
Causal inference, Covariate adjustment, Ancestral graphs, d-separation, m-separation, Complexity,
Bayesian network, Knowledge representation
Contents
1
Introduction
2
2
Preliminaries
5
2.1
General background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Identiﬁcation via covariate adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3
Algorithms for m-separation in ancestral graphs
8
3.1
Linear-time algorithms for testing and ﬁnding m-separators . . . . . . . . . . . . . . . . . . . . . . .
8
*This is a revised and extended version of preliminary work presented at the 27th [24] and 30th [10] conferences on Uncertainty in Artiﬁcial
Intelligence (UAI). Declarations of interest: none
†Published in Artiﬁcial Intelligence 270 (2019) 1-40. https://doi.org/10.1016/j.artint.2018.12.006
1

3.2
Polynomial time algorithms for minimal and minimum m-separators . . . . . . . . . . . . . . . . . .
11
3.3
The hardness of strong-minimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.4
Algorithms for minimal m-separators in sparse graphs . . . . . . . . . . . . . . . . . . . . . . . . . .
16
3.5
Algorithms for enumerating all m-separators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
4
Empirical analysis of DAG consistency testing
18
4.1
Simulation setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
4.2
Empirical results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
5
Adjustment in DAGs
19
5.1
Constructive back-door criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
5.2
CBC vs Pearl’s back-door criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
6
Algorithms for testing and computing adjustment sets in DAGs
23
7
Extending the CBC
25
7.1
Identiﬁcation by plain formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
7.2
Identiﬁcation by generalized parent adjustment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
7.3
Identiﬁcation when X and Y partition V . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
8
Empirical analysis of identiﬁability by adjustment
28
8.1
Instance generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
8.2
Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
8.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
9
Adjustment in MAGs
37
9.1
Adjustment amenability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
9.2
Adjustment criterion for MAGs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
9.3
Adjustment set construction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
9.4
Empirical analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
9.5
Auxiliary lemmas for proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
10 Discussion
47
11 Acknowledgments
48
12 References
48
13 Appendix: further experimental results
50
1
Introduction
Graphical causal models are popular tools for reasoning about assumed causal relationships between random variables
and their implications [1, 2]. Such models represent the mechanisms that are assumed to generate an observed joint
probability distribution. By analyzing the model structure, one can deduce which variables introduce potential bias
(confounding) when the causal effect of exposure variables (X) on outcome variables (Y) is to be determined.
In
population research within Epidemiology [3], the Social Sciences [2], or Econometrics, causal inference often needs to
be based on observational rather than experimental data for practical or ethical reasons. Confounding is a major threat
to causal inference in such applications. A traditional approach to address confounding is to simply adjust for (e.g.,
by stratiﬁcation) as many covariates as possible that temporally precede the exposure variable (pre-exposure variables)
[4, 5]. This practice has roots in the analysis of data from randomized trials, in which confounding can only occur
by chance. For observational data, however, it can be easily shown using graphical causal models that conditioning
2

(a)
(b)
(c)
(d)
FI
MR
MD
LE
D
FI
MR
MD
LE
D
FI
MR
MD
LE
D
FI
MR
MD
LE
D
Figure 1: Causal diagram [3, Chapter 12] describing the effect of low education (LE) on diabetes risk (D) with the
covariates family income (FI), mother’s genetic risk to develop diabetes (MR), and mother’s diabetes (MD). The
unadjusted estimate (a) is biased due to the common ancestor FI – bias “ﬂows” via the biasing path LE ←FI →MD
→D (bold edges). Adjustment for MD (b) blocks this biasing path, but opens a new one by creating an association
between FI and MR. The minimal adjustments {MD, MR} (c) and {FI} (d) close all biasing paths. Note that, if both FI
and MR were unmeasured, it would be impossible to know whether adjustment for only MD would increase or reduce
bias. This shows that, unlike for experimental data, conditioning on pre-exposure variables can be both beneﬁcial and
detrimental in observational data.
on pre-exposure variables can induce dependencies between other pre-treatment variables that affect exposure and
outcome (Figure 1). If those variables have not been or cannot be measured (e.g., if we could not observe variable
MR in Figure 1), then we could increase rather then reduce bias by conditioning on a pre-exposure variable. Such
examples show that it is impossible to decide which covariates one should adjust for to address confounding without at
least some knowledge of the causal relationships between the involved covariates. To our knowledge, graphical causal
models are currently the only causal inference framework in which it is possible to state a set of causal assumptions
and derive from those assumptions a conclusive answer to the question whether, and how, a causal effect of interest is
identiﬁable via covariate adjustment.
Covariate adjustment is not complete for identiﬁcation; other methods like the front-door criterion [6] or do-
calculus [1] can permit identiﬁcation even if covariate adjustment is impossible. When multiple options are available,
however, adjustment may be an attractive option for effect estimation because its statistical properties are well un-
derstood, giving access to useful methodology like robust estimators and conﬁdence intervals. A famous result that
characterizes valid sets of variables for covariate adjustment in a given graphical model is the back-door criterion by
Pearl [1]. We call such sets of variables adjustment sets or simply adjustments. Although the back-door criterion is
sound, meaning that sets fulﬁlling the criterion are indeed valid adjustments, it is not complete – not all valid adjust-
ments fulﬁll the criterion. It is even possible that none of the valid adjustments in a causal model fulﬁll the back-door
criterion, which would lead a user to incorrectly conclude that a different method is required.
Shpitser and colleagues [7] gave the ﬁrst sound and complete criterion to characterize adjustments in DAGs. This
criterion achieves completeness by weakening the simple but overly restrictive condition in the back-door criterion that
adjustment sets must not contain any descendants of the exposure variable; such variables can be allowed provided
they do not block causal paths or create new biasing paths. However, a characterization of adjustment sets in terms of
blocked paths does not yield a practical algorithm for adjustment set construction. Although enumerating all subsets
of covariates and all paths would work, such a brute-force approach is only feasible in very small graphs. Thus, to
help researchers ﬁnd the most suitable strategy to identify a given causal effect, algorithmically efﬁcient methods are
needed that exhaustively characterize all available options.
This paper provides efﬁcient algorithms that provide exhaustive answers to the question: Given a causal graph G,
which covariates Z can we adjust for to estimate the causal effect of the exposures X on the outcomes Y? We address
many variants of this question, such as requiring Z to be minimal or minimum as well as imposing the constraint
I ⊆Z ⊆R for given sets I, R. All our algorithms handle sets of multiple, possibly interrelated exposures X and
outcomes Y, which is important in applications such as case-control studies that screen several putative causes of
a disease [8]. The basis for our algorithms are theorems that reduce adjustment to d-separation or m-separation in
a subgraph, combined with a generic, efﬁcient and ﬂexible algorithmic framework to ﬁnd, verify, and enumerate
separating sets. Our algorithms are guaranteed to ﬁnd all valid adjustments for a given model with polynomial delay.
Besides adjustment, the framework is potentially useful for other applications, as we illustrate by applying it to model
3

Graph class
First sound and complete criterion
First sound and complete constructive criterion
DAGs
Shpitser et al. [7]
this paper (conference version [10])
MAGs
this paper (conference version [10])
this paper (conference version [10])
CPDAGs
Perkovi´c et al. [11]
van der Zander and Li´skiewicz [12]
PAGs
Perkovi´c et al. [11]
Perkovi´c et al. [11]
CGs
van der Zander and Li´skiewicz [12]
van der Zander and Li´skiewicz [12]
maximal PDAGs
Perkovi´c et al. [13]
Perkovi´c et al. [13]
Table 1: Using covariate adjustment to estimate causal effects: an overview of our results and related work for directed
acyclic graphs (DAGs), maximal ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs),
partial ancestral graphs (PAG), chain graphs (CGs), and maximally oriented partially directed acyclic graphs (PDAGs).
A criterion is sound if it is only satisﬁed by adjustment sets. It is complete if every adjustment set satisﬁes it. It is
constructive if it leads to an efﬁcient algorithm for constructing an adjustment set. Paper [10] is the preliminary
conference version of this work, and all subsequent results are based on the techniques developed in this paper.
consistency testing. We also perform an extensive empirical evaluation of the power of covariate adjustment compared
to other identiﬁcation techniques in random causal graphs.
When there are multiple adjustments available, practical considerations can help to decide between these. We
consider two different scenarios to help making such decisions. First, not all covariates might be equally well suited for
adjustment. Measuring some variables may be difﬁcult or expensive, or could be affected by substantial measurement
error. For instance, in Figure 1, measuring the variable MR (the genetic risk inherited from the mother) would require
knowledge of the genetic variants that lead to a higher diabetes risk and availability of the genomes of the mothers of
all study participants, but under these circumstances, it could be measured rather reliably. The family income, on the
other hand, could be determined rather easily using a questionnaire, which however could suffer from substantial bias
(e.g., people with high incomes could be less willing to report them). In such cases adjustments in which some cost
(such as measurement error) is minimized are potentially interesting. We provide algorithms to list only those sets that
minimize a user-supplied cost function.
Second, precision is an important issue when estimating effects. Even if adjustment for a certain variable is not
required to minimize bias, it may still be desirable to adjust for that variable to increase precision, even if it means the
resulting adjustment set is no longer minimal. In Figure 1, for example, variable MD (mother’s diabetes) will likely be
substantially correlated with variable D (child’s diabetes), so its inclusion into the statistical model that estimates LE’s
effect on D might well increase precision even if this means that either FI or MR also need to be included. To allow
such considerations, we consider the setting where we search for adjustment sets containing a pre-deﬁned subset of
variables.
A model represented as a DAG assumes causal sufﬁciency, i.e., that every relevant variable is included in the model.
If this is not the case and there are unknown latent variables, one can use a maximal ancestral graph (MAG) model
that just speciﬁes the ancestral relations between the variables [9]. An adjustment in a MAG remains valid when the
MAG is extended by any number of additional latent variables as long as the ancestral relations and m-separating sets
are preserved. This is an important feature because the assumption that all relevant confounders have been measured
and included in a model is often unrealistic. We show that our entire algorithmic framework for computing adjustment
sets can be generalized to MAGs, allowing practitioners to substantially relax the assumption of causal sufﬁciency that
DAG-based analyses rely on.
Beyond DAGs and MAGs, our proof techniques have been successfully applied to obtain complete and construc-
tive adjustment criteria for several other classes of graphical causal models since the publication of our preliminary
version [10]. Perkovi´c et al. [11] provide a generalized adjustment criterion (GAC) which is sound and complete for
DAGs, CPDAGs, MAGs and PAGs. CPDAGs (PAGs) are a class of causal models where a single graph represents a
Markov equivalence class of DAGs (MAGs), in which all DAGs (MAGs) have the same conditional independences.
Such models occur naturally when learning models from observed data, where in general, several possible models are
consistent with the data. Finally, in [12] we were able to extend our techniques to chain graphs (CGs) and Perkovi´c et
al. [13] applied these to maximally oriented partially directed acyclic graphs (PDAGs) – graphs providing an elegant
4

framework for modeling and analyzing a broad range of Markov equivalence classes. We summarize these results in
Table 1. In this paper, we focus on complete criteria for DAGs and MAGs because this substantially simpliﬁes the
presentation of the results, and the application of these techniques to other graph classes is relatively straightforward
[11, Section 4.3].
This paper is structured as follows. Section 2 introduces notation and basic concepts.
In Section 3, we present algorithms for verifying, constructing, and listing m-separating sets Z in ancestral graphs.
All algorithms handle unconstrained separators, minimal and minimum separators, and the constraint I ⊆Z ⊆R for
given sets I, R. They subsume a number of earlier solutions for special cases of these problems, e.g., the Bayes-Ball
algorithm for veriﬁcation of d-separating sets [14] or the use of network ﬂow calculations to ﬁnd minimal d-separating
sets in DAGs [15, 16]. Section 4 presents DAG consistency testing as a possible application of these algorithms
beyond adjustment sets. In Section 5, we give a constructive back-door criterion (CBC) that reduces the construction
of adjustment sets to ﬁnding separating sets in DAGs, explore variations of the CBC and compare it to Pearl’s back-
door criterion [1]. Section 6 explains how to apply the algorithms of Section 3 to the graphs resulting from the criterion
of Section 5, which yields polynomial time algorithms for verifying, constructing, and listing adjustment sets in DAGs.
This combination leads to the ﬁrst efﬁcient implementation of the sound and complete adjustment criterion by Shpitser
and colleagues [7]. Section 7 extends our criterion by addressing some cases where, even though covariate adjustment
is not possible, it is still not necessary to invoke the do-calculus since the causal effect can be identiﬁed in another
simple manner. In Section 8 we compare the results and performance of our adjustment criterion (with and without
the extensions derived in Section 7) with Pearl’s back-door criterion as well as the IDC algorithm, which ﬁnds all
identiﬁable causal effects, on randomly generated DAGs. Finally, in Section 9, we extend our constructive back-door
criterion of Section 5 to MAGs, which generalizes the sound but incomplete adjustment criterion for MAGs without
constraints by Maathuis and Colombo [17].
2
Preliminaries
We denote sets by bold upper case letters (S), and sometimes abbreviate singleton sets {S} as S. Graphs are written
calligraphically (G), and variables in upper-case (X).
2.1
General background
Mixed graphs, walks and paths
We consider mixed graphs G = (V, E) with nodes (vertices, variables) V and
directed (A →B), undirected (A −B), and bidirected (A ↔B) edges E. Nodes linked by an edge are adjacent and
neighbors of each other. A walk of length n is a node sequence V1, . . . , Vn+1 such that there exists an edge sequence
E1, E2, . . . , En for which every edge Ei connects Vi, Vi+1. Then V1 is called the start node and Vn+1 the end node of
the walk. A path is a walk in which no node occurs more than once. Given a node set X and a node set Y, a walk from
X ∈X to Y ∈Y is called X-proper if only its start node is in X. If X is clear from the context, it is omitted and we just
say the path is proper. Given a graph G = (V, E) and a node set V′, the induced subgraph GV′ = (V′, E′) contains
the edges E′ = E ∩(V′ × V′) from G that are adjacent only to nodes in V′. The skeleton of G is a graph with the
same nodes in which every edge is replaced by an undirected edge. A connected component is a subgraph in which
every pair of nodes is connected by a path. A subgraph containing only a single node is also a connected component.
A connected component is a bidirectionally connected component if for every pair of nodes there exists a connecting
path that contains only bidirected edges. Throughout, n stands for the number of nodes and m for the number of edges
of a graph.
Ancestry
A walk of the form V1 →. . . →Vn is directed, or causal. A non-causal walk is biasing. A graph is
acyclic if no directed walk from a node to itself is longer than 0. All directed walks in an acyclic graph are paths. If
there is a directed path from U to V, then U is called an ancestor of V and V a descendant of U. A walk is anterior if
it were directed after replacing all edges U −V by U →V. If there is an anterior path from U to V, then U is called
an anterior of V. All ancestors of V are anteriors of V. Every node is its own ancestor, descendant, and anterior. For
5

a node set X, the set of all of its ancestors is written as An(X). The descendant and anterior sets De(X), Ant(X) are
analogously deﬁned. Also, we denote by Pa(X), (Ch(X), Ne(X)), the set of parents (children, neighbors) of X.
m-Separation
This concept is a generalization of the well-known d-separation criterion in DAGs [1] for ancestral
graphs. A node V on a walk w is called a collider if two arrowheads of w meet at V, e.g., one possible collider is
U ↔V ←Q. There can be no collider if w is shorter than 2. Two nodes U, V are called collider connected if there is
a path between them on which all nodes except U and V are colliders. Adjacent vertices are collider connected. Two
nodes U, V are called m-connected by a set Z if there is a path π between them on which every node that is a collider is
in An(Z) and every node that is not a collider is not in Z. Then π is called an m-connecting path. The same deﬁnition
can be stated simpler using walks: U, V are called m-connected by Z if there is a walk w between them on which all
colliders and only colliders are in Z. Similarly, such w is called an m-connecting walk. If U, V are m-connected by the
empty set, we simply say they are m-connected. If U, V are not m-connected by Z, we say that Z m-separates them
or blocks all paths between them. Two node sets X, Y are m-separated by Z if all their nodes are pairwise m-separated
by Z.
DAGs and ancestral graphs
A DAG is an acyclic graph with only directed edges. A mixed graph G = (V, E) is
called an ancestral graph (AG) [9] if the following two conditions hold: (1) For each edge A ←B or A ↔B, A
is not an anterior of B. (2) For each edge A −B, there are no edges A ←C, A ↔C, B ←C or B ↔C. From
these conditions it follows that there can be at most one edge between two nodes in an AG. If one restricts AGs to
graphs consisting of only directed and bidirected edges, as we do in Section 9, then a simple and intuitive equivalent
deﬁnition characterizes an AG as a graph without directed and almost-directed cycles, i.e., without subgraphs of the
form V1 →. . . →Vn →V1 or V1 ↔V2 →. . . →Vn →V1. Syntactically, all DAGs are AGs and all AGs containing
only directed edges are DAGs. An AG G = (V, E) is a maximal ancestral graph (MAG) if every non-adjacent pair of
nodes U, V can be m-separated by some Z ⊆V \ {U, V}. It is worth noting that syntactically a DAG is also a MAG
[18]. Moreover every AG G can be turned into a MAG M by adding bidirected edges between node pairs that cannot
be m-separated, which preserves all m-separation relationships in the graph [9, Theorem 5.1].
Graph transformations
A DAG G = (V, E) is represented by a MAG M = G[S
L with nodes V \ (S ∪L) for sets
S, L ⊆V, whereby M has an edge between a pair of nodes U, V if U, V cannot be d-separated in G by any Z with
S ⊆Z ⊆V \ L. That edge has an arrowhead at node V, if V < An(U ∪S). Notice that for empty S and L we have
G[∅
∅= G. The canonical DAG C(M) of a MAG M is the DAG obtained from M by replacing every ↔edge with
←L →and every −edge with →S ←with new nodes L or S which form sets L, S. Clearly C(M)[S
L= M (for more
details see [9] or Section 9 in our paper).
The augmented graph (G)a of a certain AG G is an undirected graph with the same nodes as G whose edges are
all pairs of nodes that are collider connected in G. Two node sets X and Y are m-separated by a node set Z in G if and
only if Z is an X-Y node cut in (GAnt(X∪Y∪Z))a [9]. For DAGs the augmented graph is also called the moralized graph,
so the construction of the augmented graph is termed moralization.
For any subset A and B of V, by A →B we denote the set of all edges A →B in E, such that A ∈A and
B ∈B; the sets A ←B, A ↔B, and A −B are deﬁned analogously. Pearl’s do-calculus [1] deﬁnes the following
two transformations. First, the graph obtained from a graph G = (V, E) by removing all edges entering X is written as
GX = (V, E \ ((V →X) ∪(V ↔X))). Second, the removal of all edges leaving X is written as GX = (V, E \ ((X →
V) ∪(X −V))). The application of both these operations (GX)X′ is abbreviated as GXX′ matching the notation of
do-calculus [1]. Descendants and ancestors in these graphs are written as set subscript at the corresponding function
without specifying the graph, e.g., DeX or AnX. This notation is used for DAGs and MAGs. Note that for DAGs in
each case only the ﬁrst kind of edges needs to be removed as there are no ↔or −edges in DAGs.
2.2
Identiﬁcation via covariate adjustment
Do-operator and identiﬁability
A DAG G encodes the factorization of a joint distribution P for the set of variables
V = {X1, . . ., Xn} as P(v) = Qn
j=1 P(xj | paj), where paj denotes a particular realization of the parent variables
6

G1:
X
U
Y
G2:
X
U
V
Y
G3:
X
U
Z
V
Y
Figure 2: Identiﬁcation of P(y | do(x)) in graphs with the unobserved nodes V \ R = {U}: In G1 the effect is not
identiﬁable. In G2, it is identiﬁable (using the front-door method [1]) but for this DAG no adjustment set Z exists, with
Z ⊆V \ {U}. In G3 the effect can be identiﬁed via adjustment Z = {Z}.
of Xj in G. When interpreted causally, an edge Xi →Xj is taken to represent a direct causal effect of Xi on Xj.
Formally, this can be deﬁned in terms of the do-operator [1]: Given DAG G and a joint probability density P for V the
post-intervention distribution can be expressed in a truncated factorization formula
P(v | do(x)) =

Y
Xj∈V\X
P(xj | paj)
for v consistent with x,
0
otherwise,
(1)
where “v consistent with x” means that v and x assign the same values to the variables in X∩V. For disjoint X, Y ⊆V,
the (total) causal effect of X on Y is P(y | do(x)) where do(x) represents an intervention that sets X = x. In a DAG, this
intervention corresponds to removing all edges into X, disconnecting X from its parents, i.e., constructing the graph
GX.
When all variables in V are observed (we model this by setting R = V), the causal effect P(y | do(x)) of X on Y
in a given graph G can be determined uniquely from the pre-intervention distribution P using the factorization above.
However, when some variables are unobserved, the question whether P(y | do(x)) is identiﬁable, i.e., if it can be
computed from the pre-intervention distribution independent of the unknown quantities for the unobserved variables
V \ R, becomes much more complex. For a formal deﬁnition of identiﬁability see [1, Section 3.2]. Figure 2 shows an
example DAG G1 for which the causal effect of X on Y is not identiﬁable, and two DAGs G2 and G3 for which the
effect is identiﬁable.
Adjustment sets
Identiﬁcation via covariate adjustment, studied in this paper, is deﬁned for DAGs below and can
be extended to MAGs in the usual way [17] (we will give a formal deﬁnition Section 9).
Deﬁnition 1 (Covariate adjustment [1]). Given a DAG G = (V, E) and pairwise disjoint X, Y, Z ⊆V, Z is called ad-
justment set for estimating the causal effect of X on Y, or simply adjustment (set), if for every distribution P consistent
with G we have
P(y | do(x)) =

P(y | x)
if Z = ∅,
P
z P(y | x, z)P(z)
otherwise.
(2)
Identiﬁcation via covariate adjustment is not complete in the sense that there exist graphs for which P(y | do(x))
is identiﬁable but for which no adjustment set Z, with Z ⊆R, exists. For an example of such a DAG see Figure 2.
In [19] (see also [1, Section 3.4]) Pearl introduced the do-calculus of intervention that was proven to be complete for
identifying causal effects [20, 21]. Based on the rules of the do-calculus, the IDC algorithm proposed by Shpitser and
Pearl [22] computes a formula for P(y | do(x)) if (and only if) the effect is identiﬁable. Researchers might thus use
IDC to decide if an effect of interest is identiﬁable at all, and then use one of the algorithms provided in this paper to
verify if identiﬁcation is also possible via adjustment, which has some statistically favorable properties. However, for
complex graphs, it may not be practical to run the IDC algorithm as we show later, whereas adjustment sets can still
be found quickly if they exist.
Minimality
In this paper we present methods for computing an adjustment set that satisﬁes the condition of Deﬁni-
tion 1 for a given instance. An important feature of our approach is that it allows also to ﬁnd adjustment sets satisfying
some additional desirable constraints, e.g., minimality. Below we deﬁne this notion formally both for adjustment and
separation sets.
7

For pairwise disjoint X, Y, Z ⊆V, and a subset M of V, an m-separator (resp. adjustment set) Z relative to (X, Y)
is M-minimal, if M ⊆Z and no set Z′ ⊊Z with M ⊆Z′ is an m-separator (adjustment set) relative to (X, Y). An
m-separator (adjustment) Z is M-minimum according to a cost function w : V →R+ if M ⊆Z and no m-separator
(adjustment) Z′ with M ⊆Z′ and P
Z∈Z′ w(Z) < P
Z∈Z w(Z) exists. In particular, ∅-minimality (∅-minimum) coincides
with the standard notion of minimality (minimum), which we will also call strong-minimality (strong-minimum). A
more detailed explanation of these concepts along with some examples is provided in Section 3.2.
While M-minimality is deﬁned with respect to any set M ⊆V, in this paper we will only consider the two cases
M = ∅and M = I, i.e., we consider m-separators and adjustments Z that are supersets of the constraining set I. For
a given constraining set I we will abbreviate the I-minimal/minimum Z ⊇I as weakly-minimal/minimum or just as
minimal/minimum. Note a subtle, but important difference between weak and strong minimality. For example, the
existence of a weakly-minimal m-separator does not necessarily imply that a strongly-minimal separator exists. E.g.,
in the DAG X →I ←V →Y, set Z = {I, V} is an I-minimal d-separator relative to (X, Y) but in the graph there
exists no strongly-minimal d-separator Z′, with I ⊆Z′. On the other hand, it is easy to see that every strongly-minimal
m-separator Z, with I ⊆Z, is also an I-minimal one and the same holds for the minimum sets.
3
Algorithms for m-separation in ancestral graphs
In this section, we compile an algorithmic framework for solving a host of problems related to veriﬁcation, construc-
tion, and enumeration of m-separating sets in an ancestral graph G = (V, E) with n nodes in V and m edges in E. The
problems are deﬁned in Table 2, which also shows the asymptotic runtimes of our algorithms. The goal is to test or to
output either arbitrary m-separating sets Z without further constraints or m-separating sets that have a minimal size,
so that no node can be removed from the set without turning it into a non-separating set. A further constraint is that
Z should be bounded by arbitrary given sets I ⊆R as I ⊆Z ⊆R. The variables in I will always be included in the
m-separating set, even if the set remains a separator without these variables. The set V \ R corresponds to a set of
unobserved variables, which are known to exist, but have not been (or cannot be) measured. This constraint can also
be used to model correlated errors between variables, i.e., rather than connecting such variables with a bidirected edge
like in a semi-Markovian model, the variables are connected to another variable not in R. Both constraints together
are also an important technical tool for the design of efﬁcient enumeration algorithms and the adjustment algorithms
in the later sections.
The rest of this section describes our algorithms solving these problems, which are mostly generalizations of
existing algorithms from [23, 14, 15, 24]. For each problem, we present the algorithm as a function of the same name
as the problem, so that the association between problem and algorithm is easy to follow and the algorithms can be
built upon each other.
In contrast to the preliminary conference version of this work [10], we state the algorithms separately for sparse
and dense graphs. We introduce the notion M-minimality to analyze the minimal sets found by the algorithms and
show that they ﬁnd weakly-minimal separators, since ﬁnding strong-minimal separators is intractable.
3.1
Linear-time algorithms for testing and ﬁnding m-separators
The problems TESTSEP and FINDSEP can be solved immediately in the ancestral graph. TESTSEP just requires us to
verify the m-separation deﬁnition for given sets X, Y, Z. In DAGs d-separation is usually tested with the Bayes-Ball
algorithm [14], which can be visualized as sending balls through the graph along the edges, tracking which nodes
have been visited from either their parents or children, until all reachable nodes have been visited. In other words,
Bayes-Ball is basically a breadth-ﬁrst-search with some rules that determine if an edge pair lets the ball pass, bounces
it back on the same edge or blocks it completely. These rules can be adapted to m-separation as shown in Figure 3,
which leads to the following algorithm for testing if a given Z m-separates X and Y in an AG G:
8

Runtime
Reference
Veriﬁcation: For given X, Y, Z and constraint I decide if . . .
TESTSEP
Z m-separates X, Y
O(n + m)
Proposition 1
TESTMINSEP
Z ⊇I m-separates X, Y and Z is . . .
I-minimal
O(n2)
Proposition 3
strongly-minimal
O(n2)
Proposition 3
Construction: For given X, Y and constraints I, R, output an . . .
FINDSEP
m-separator Z with I ⊆Z ⊆R
O(n + m)
Proposition 2
FINDMINSEP
m-separator Z with I ⊆Z ⊆R which is . . .
I-minimal
O(n2)
Proposition 4
strongly-minimal
NP-hard
Proposition 7
FINDMINCOSTSEP
m-separator Z with I ⊆Z ⊆R which is . . .
I-minimum
O(n3)
Proposition 5
strongly-minimum
O(n3)
Proposition 6
Enumeration: For given X, Y, I, R enumerate all . . .
Delay
LISTSEP
m-separators Z with I ⊆Z ⊆R
O(n(n + m))
Proposition 9
LISTMINSEP
I-minimal m-separators Z with I ⊆Z ⊆R
O(n3)
Proposition 10
Table 2: Deﬁnitions of algorithmic tasks related to m-separation in an ancestral graph G of n nodes and m edges and
the time complexities of algorithms given in this section that solve the associated problems. Throughout, X, Y, R are
pairwise disjoint node sets, the set Z is disjoint with the non-empty sets X, Y, and each of the sets I, R, Z can be empty.
A minimum m-separator minimizes the sum P
Z∈Z w(Z) for a cost function w respecting the given constraints, i.e.,
w(V) = ∞for V < R. The construction algorithms output ⊥if no set fulﬁlling the listed condition exists. Delay
complexity for LISTSEP and LISTMINSEP refers to the time needed per solution when there can be exponentially
many solutions (see [25]).
M < Z:
T
M
O
I
B
U
→→
→−
T
M
O
I
B
U
←→
←←
←↔
←−
T
M
O
I
B
U
↔→
↔−
T
M
O
I
B
U
−→
−←
−↔
−−
M ∈Z:
T
M
O
I
B
U
→←
→↔
T
M
O
I
B
U
T
M
O
I
B
U
↔←
↔↔
T
M
O
I
B
U
Figure 3: Expanded rules for Bayes-Ball in AGs, listing (in boxes) all combinations of edge pairs through which the
ball is allowed to pass. The Bayes ball starts at the top node T and passes through the middle node M to one of the
bottom nodes {O, I, B, U}. Forbidden passes are marked in red and crossed out. Here, by a pair of edges we mean
an edge between T (Top node) and M (Middle node) and M →O (Out-node), resp. M ←I (In-node), M ↔B
(Bidirected edge), and M −U (Undirected edge). The ﬁgure above shows all possible types of edges between T and
M. We consider two cases: M < Z and M ∈Z (gray). The leaving edge can correspond to the entering edge, i.e., T
can belong to {O, I, B, U}, in which case the ball might return to T, which is called a bouncing ball in the literature.
9

function TESTSEP(G, X, Y, Z)
P ←{(←, X) | X ∈X }
⊲list of pairs (type of edge, node) whose visit is pending
Q ←P
⊲all pending and ever visited nodes
while P not empty do
Let (e, T) be a (type of edge, node) pair of P
Remove (e, T) from P
for all neighbors N of T do
Let T and N be connected by edge f
if e, T, f is an m-connecting path segment and (f, N) < Q then
Add (f, N) to P and Q
if M ∈Y then return false
return true
Analysis of the Algorithm. From the rules in Figure 3 it is obvious that the following statement about algorithm
TESTSEP holds: The ball only passes through the walk segment of two consecutive edges if the segment is an m-
connected walk. The correctness follows from the fact that the algorithm searches over all walks starting in X. The
runtime is linear as it is a breadth-ﬁrst-search and each node is visited at most four times. The rules for entering a node
through edges →and ↔as well as through edges ←and −are the same, so an implementation could merge these
cases in Q, so each node is visited at most twice.
□
Proposition 1. Using the algorithm above, the task TESTSEP can be solved in time O(n + m).
The next problem FINDSEP asks for a set Z m-separating given X and Y with I ⊆Z ⊆R. A canonical solution
for this problem is given in the following lemma, which is an extended version of a lemma by Spirtes, Glymour and
Scheines [26, p. 136] allowing the restriction I.
Lemma 1. Let X, Y, I, R be sets of nodes with I ⊆R, R ∩(X ∪Y) = ∅. If there exists an m-separator Z0 relative to
(X, Y) with I ⊆Z0 ⊆R, then Z = Ant(X ∪Y ∪I) ∩R is an m-separator.
Proof. Let us consider a proper walk w = X, V1, . . ., Vn, Y with X ∈X and Y ∈Y. If w does not contain a collider,
all nodes Vi are in Ant(X ∪Y) and the walk is blocked by Z, unless {V1, . . . , Vn} ∩R = ∅in which case the walk is
not blocked by Z0 either. If the walk contains colliders C, it is blocked, unless C ⊆Z ⊆R. Then all nodes Vi are in
Ant(X ∪Y ∪I) and the walk is blocked, unless {V1, . . ., Vn} ∩R = C. Since C ⊆Z is a set of anteriors, there exists a
shortest (possible of length zero) path πj = Vj →. . . →Wj for each Vj ∈C with Wj ∈X ∪Y ∪I (it cannot contain
an undirected edge, since there is an arrow pointing to Vj). Let π′
j = Vj →. . . →W′
j be the shortest subpath of πj
that is not blocked by Z0. Let w′ be the walk w after replacing each Vj by the walk Vj →. . . →W′
j ←. . . ←Vj. If
any of the Wj is in X ∪Y we truncate the walk, such that we get the shortest walk between nodes of X and Y. Since
π′
j is not blocked, w′ contains no colliders except w′
j and all other nodes of w′ are not in R, w′ is not blocked and Z0
is not a separator.
□
This yields the following algorithm to ﬁnd an m-separator relative to X, Y:
function FINDSEP(G, X, Y, I, R)
R′ ←R \ (X ∪Y)
Z ←Ant(X ∪Y ∪I) ∩R′
if TESTSEP(G, X, Y, Z) then return Z
else return ⊥
Analysis of the Algorithm. The algorithm ﬁnds an m-separator due to Lemma 1 and runs in linear time, since the set
Ant(X ∪Y ∪I) ∩R can be calculated in linear time and the algorithm TESTSEP runs in linear time as well.
□
Proposition 2. Using the algorithm above, the task FINDSEP can be solved in time O(n + m).
Set R is required to be disjoint with X and Y, because m-separation of a set Z relative to X, Y is not deﬁned when
Z contains a node of X or Y, so Z is always a subset of R ⊆V \ (X ∪Y). However, our algorithms still remove X ∪Y
from R, as it might prevent bugs in practical implementations and it is reasonable to ﬁnd a separator that does not
contain the variables to be separated.
10

3.2
Polynomial time algorithms for minimal and minimum m-separators
In this section we present algorithms for testing minimal m-separators for both variants of the minimality. We give
also algorithms for constructing weakly- and strongly-minimum m-separators Z satisfying constraints I ⊆Z ⊆R and
for ﬁnding weakly-minimal separators.
To illustrate the differences between these different versions of minimality,
we again consider the possible adjustment sets in Figure 1. The set {FI} is a strongly-minimal adjustment set. When
we use the simple cost function c(v) = 1 that assigns to each separator a cost that is equal to the number of variables
it contains, it is also a strongly-minimum adjustment set. Now, suppose we set I = {MD} because {MD} is expected
to explain a lot of the variance in the outcome D. If we adjust for MD, then we also need to adjust for at least one
other variable, so all adjustment sets Z respecting Z ⊇I must have a cardinality of at least 2. Therefore, because
the strongly-minimum adjustment set {FI} has a cardinality of 1, it is clear that no strongly minimum adjustment set
satisfying Z ⊇I exists. The set Z = {MD, MR} is a minimal adjustment set that satisﬁes Z ⊇I, so it is both strongly
minimal and I-minimal; it is also I-minimum. By contrast, the set Z = {MD, FI} is also I-minimum and I-minimal,
but it is not strongly minimal.
The only one of these four variations that appears to be turned into a hard problem by the restriction Z ⊇I
is computing strongly-minimal separators – we discuss its complexity in detail in the next section. This is a very
surprising result since ﬁnding objects of minimum costs or sizes is typically, harder than constructing minimal objects.
Our algorithms for the other three cases are easily implementable and have runtimes O(n2), resp. O(n3). These time
complexities are reasonable particularly in case of dense ancestral graphs, i.e., graphs with a large number of edges
n ≪m ≤n2 and O(n + m) = O(n2). In Section 3.4 we propose algorithms that are faster than those presented below
when the instance graphs are sparse.
The algorithms for minimal m-separators consist of two phases. First we convert the ancestral graph to an aug-
mented graph (analogous to moralization for DAGs [26]; see Preliminaries). Then, we ﬁnd a minimal separator as a
vertex cut in the augmented graph. This approach generalizes the d-separation algorithms of [15] and [24], particularly
the handling of the undirected graph after the moralization step is the same as in [15]. It is important to avoid iterating
over all pairs of nodes and searching for collider connected paths between them in the construction of the augmented
graph, as this would lead to a suboptimal algorithm. Instead, the following algorithm achieves an asymptotically
optimal (linear time in output size) runtime for AGs:
Lemma 2 (Efﬁcient AG moralization). Given an AG G, the augmented graph (G)a can be computed in time O(n2).
Proof. The algorithm proceeds in four steps.
1. Start by setting (G)a to the skeleton of G.
2. Partition G in all its maximal bidirectionally connected components.
3. For each pair U, V of nodes from the same component, add the edge U −V to (G)a if it did not exist already.
4. For each component, identify all its parents and link them all by undirected edges in (G)a.
Now two nodes are adjacent in (G)a if and only if they are collider connected in G. All four steps can be performed in
time O(n2).
□
Lemma 1 gave us a closed form solution to ﬁnd one m-separator. By extending this result only slightly, we obtain
a constraint on all minimal m-separators:
Corollary 1 (Ancestry of M-minimal separators). Given an AG G and three sets X, Y, M, every M-minimal m-
separator Z is a subset of Ant(X ∪Y ∪M).
Proof. Assume there is an M-minimal separator Z with Z ⊈Ant(X ∪Y ∪M). Setting I = M and R = Z, Lemma 1
shows that Z′ = Ant(X ∪Y ∪M) ∩Z is an m-separator with M ⊆Z′. But Z′ ⊆Ant(X ∪Y ∪M) and Z′ ⊆Z, so
Z , Z′ and Z is not an M-minimal separator.
□
Corollary 2 (Ancestry of minimal separators). Given an AG G and three sets X, Y, I, every I-minimal or ∅-minimal
m-separator is a subset of Ant(X ∪Y ∪I).
11

V and its neighbors Ne(V) in Ga:
V
N
M
O
P
Case: V ∈I
V
N
M
O
P
Case: V < R
V
N
M
O
P
Figure 4: This ﬁgure explains the removal of nodes in I and outside of R from the augmented graph (GAnt(X∪Y∪I))a.
Shown is an exemplary node V with all its neighbors in the augmented graph. In the case V ∈I the node V blocks all
paths through V, so the second graph obtained by removing V has no remaining edges. In the case V < R no path is
blocked by V, so after removing the node all its neighbors need to be linked to preserve the connectivities as shown
in the third graph. The time needed to insert the new edges is O(|V \ R| · |Ne(V \ R)|2) = O(n3), so this removal of
nodes outside R is only used for algorithm LISTMINSEP in Section 3.5 and the other algorithms handle such nodes
with different approaches.
Proof. This follows from Corollary 1 with M = I or M = ∅. In both cases we have Ant(X∪Y∪M) ⊆Ant(X∪Y∪I).
□
Corollary 2 applies to minimum separators as well because every minimum separator must be minimal. For all
strongly-minimal (i.e., recall, ∅-minimal) or I-minimal m-separator Z satisfying I ⊆Z the corollary implies Ant(X ∪
Y ∪Z) = Ant(X ∪Y ∪I) and thus (GAnt(X∪Y∪Z))a = (GAnt(X∪Y∪I))a, so the augmented graph (GAnt(X∪Y∪Z))a can be
constructed without knowing the actual Z. We will use ma to denote the number of edges in (GAnt(X∪Y∪I))a. Since
nodes in I as well as nodes outside R are known to either be in Z or to be excluded from Z, they do not need to be
considered by the algorithms and can be removed from the augmented graph as shown in Figure 4, similarly to the
approach in [23].
TESTMINSEP and FINDMINSEP can now be solved by a modiﬁed breadth-ﬁrst-search in the graph (GAnt(X∪Y∪M))a.
We start with providing the function TESTMINSEP which tests if Z ⊆R is an M-minimal m-separator relative to (X, Y)
in an AG G:
function TESTMINSEP(G, X, Y, Z, M, R)
if Z \ Ant(X ∪Y ∪M) , ∅or Z ⊈R then return false
if not TESTSEP(G, X, Y, Z) then return false
G′a ←(GAnt(X∪Y∪M))a
Remove from G′a all nodes of M.
Rx ←{Z ∈Z | ∃path from X to Z in G′a not intersecting Z \ {Z}}
if Z , Rx then return false
Ry ←{Z ∈Z | ∃path from Y to Z in G′a not intersecting Z \ {Z}}
if Z , Ry then return false
return true
Analysis of the Algorithm. TESTMINSEP, runs in O(ma) because Rx and Ry can be computed with an ordinary search
that aborts when a node in Z is reached. If each node in Z is reachable from both X and Y, the set is M-minimal as no
node can be removed without opening a connecting path as shown in the example of Figure 5.
By setting the parameter M = I, the algorithm tests I-minimality of Z. By setting M = ∅, the algorithm tests
strong-minimality.
□
Proposition 3. Using the algorithm above, the task TESTMINSEP, both for testing I-minimality and strong-minimality,
can be solved in time O(ma) = O(n2).
The difference between I-minimal and strongly-minimal separation sets Z ⊇I is illustrated in Figure 5. There are
exactly two strongly-minimal separating sets: {Z1} and {Z2}. No other m-separator will be strongly-minimal regardless
12

G:
X
Z1
Z2
Y
V1
V2
GAnt(X∪Y∪M):
X
Z1
Z2
Y
V1
Ga
Ant(X∪Y∪M):
X
Z1
Z2
Y
V1
Figure 5: The transformation of a graph G to GAnt(X∪Y∪M) to (GAnt(X∪Y∪M))a with M = ∅. The m-separating set {Z1, Z2}
is not minimal as no node is reachable from both X and Y, but each node alone {Z1} or {Z2} is a minimal m-separator.
of which nodes are added to a constraining set I. Therefore, for I = ∅, both m-separators satisfy the constraint, for
I = {Z1} or I = {Z2}, only one of them does and, for a I = {Z1, Z2} or V1 ∈I, no strongly-minimal m-separator
satisﬁes it. The constraint I just chooses some m-separators of a ﬁxed set of strongly-minimal m-separators.
On the other hand, when computing an I-minimal m-separator, we treat the nodes of I as ﬁxed and search for a
minimal m-separator among all supersets of I. In Figure 5, if I is either {Z1}, {Z2} or {Z1, Z2}, then I itself is an I-
minimal m-separator and no other I-minimal m-separator exists. If I = {V1}, then {Z1, V1} and {Z2, V1} are I-minimal.
This is an easier and in some sense more natural concept, since it can be modeled by removing the nodes of I from the
graph and searching a minimal m-separator for the remaining nodes.
From a covariate adjustment perspective, the deﬁnition of M-minimality is most meaningful in the case M = ∅or
M = I. However, our algorithms technically also allow M to lie “between” ∅and I or even partly outside M, even
though this is less relevant for our application. For example if ∅, M ⊂I, the nodes of M can be ignored for the
minimality, while the nodes of I \ M must be unremovable like all nodes in the case of ∅-minimality. In an example
in Figure 5, for M = {V1}, I = {Z1, V1} would only accept {Z1, V1} as m-separator. M = {Z1}, I = {Z1, Z2} would not
allow any. Every m-separator Z is Z-minimal.
Next we give an algorithm to ﬁnd an I-minimal m-separator:
function FINDMINSEP(G, X, Y, I, R)
G′ ←GAnt(X∪Y∪I)
G′a ←(GAnt(X∪Y∪I))a
Remove from G′a all nodes of I.
Z′ ←R ∩Ant(X ∪Y) \ (X ∪Y)
Z′′ ←{Z ∈Z′ | ∃a path from X to Z in G′a not intersecting Z′ \ {Z}}
Z ←{Z ∈Z′′ | ∃a path from Y to Z in G′a not intersecting Z′′ \ {Z}}
if not TESTSEP(G′, X, Y, Z) then return ⊥
return Z ∪I
Analysis of the Algorithm. Algorithm FINDMINSEP begins with the separating set R∩Ant(X∪Y)\(X∪Y) and ﬁnds a
subset satisfying the conditions tested by algorithm TESTMINSEP. As TESTMINSEP it can be implemented in O(ma)
using a breadth-ﬁrst-search starting from X (Y) that aborts when a node in Z′ (Z′′) is reached.
□
Algorithm FINDMINSEP ﬁnds an I-minimal m-separator. Note that setting the argument I of the algorithm to ∅
does not lead to a strongly-minimal separator Z that satisﬁes the constraint I ⊆Z ⊆R for a given non-empty set I.
Proposition 4. The algorithm above ﬁnds an I-minimal m-separator Z, with I ⊆Z ⊆R, in time O(ma) = O(n2).
In the problem FINDMINCOSTSEP, each node V is associated with a cost w(V) given by a cost function w : V →
R+ and the task is to ﬁnd a set Z m-separating X and Y which minimizes the total cost P
Z∈Z w(Z) under the constraint
I ⊆Z ⊆R. In order to ﬁnd an m-separator of minimum size, we can use a function w(V) = 1 ∀V ∈R that assigns
unit cost to each node. Alternatively we might want to ﬁnd an m-separator that minimizes the cost of measuring
the variables in the separator or that minimizes the number of combinations that the values of these variables can
take. When each node V corresponds to a random variable that can take kV different values, there are Q
V∈V kV
combinations, which can be minimized by a logarithmic cost function w(V) = log kV ∀V ∈R.
We again construct the augmented graph and can afterwards solve the problem with any weighted min-cut algo-
rithm.
13

function FINDMINCOSTSEP(G, X, Y, I, R, w)
G′ ←GAnt(X∪Y∪I)
G′a ←(GAnt(X∪Y∪I))a
Add a node Xm connected to all nodes in X, and a node Ym connected to all nodes in Y.
Assign inﬁnite cost to all nodes in X ∪Y ∪(V \ R) and cost w(Z) to every other node Z.
Remove all nodes of I from G′a.
return a minimum vertex cut Z separating Xm and Ym in the undirected graph.
Analysis of the Algorithm. The correctness follows from the fact that a minimum set is a minimal set and the minimum
cut found in the ancestor moralized graph is therefore the minimum m-separating set. The minimum cut can be found
using a maximum ﬂow algorithm in O(n3) due to the well-known min-cut-max-ﬂow theorem [27, Chapter 6].
□
Proposition 5. The algorithm above solves in time O(n3) the task FINDMINCOSTSEP in case of I-minimality.
The runtime primarily depends on the used min-cut/max-ﬂow algorithm. Using a state-of-the-art max-ﬂow algo-
rithm recently presented by Orlin improves the runtime to O(nma) [28], although this is not necessarily an improvement
in our setting, because the augmented graph can have ma = O(n2) edges and then O(nma) and O(n3) are the same.
Faster max-ﬂow algorithms are also known for speciﬁc graph classes or speciﬁc cost functions. In the special case of a
unit cost function w(V) = 1 on undirected graphs an O(ma
√n) algorithm is known [27], which is not directly applica-
ble, since algorithm FINDMINCOSTSEP changes the nodes X∪Y∪(V\R) to have inﬁnite costs. However, we can apply
the max-ﬂow algorithm to a new graph containing only nodes in R′ = (R \ (X ∪Y)) ∩Ant(X ∪Y ∪I) by removing the
nodes of V\R iteratively in O((n−|R|)n2) as shown in Figure 4 or by creating a new graph only containing those nodes
in O(|R′|ma) as described in [15], resulting in a total runtime of O(min((n −|R|)n2, |R′|ma) + ma
√
|R′|) = O(|R′|ma)
for a unit cost function.
The set Z returned by algorithm FINDMINCOSTSEP is also strongly-minimum, unless no strongly-minimum set
Z exists under the given constraints I ⊆Z ⊆R. To see this assume Z is not strongly-minimum and there exists a
strongly-minimum set Z′ satisfying the constraint. Then P
Z∈Z′ w(Z) < P
Z∈Z w(Z). But Z′ satisﬁes I ⊆Z′, so Z was
not I-minimum, a contradiction.
All strongly-minimum sets for a graph have the same minimum sum P
Z∈Z w(Z), regardless of the constraint I, so
we can test if the set returned by FINDMINCOSTSEP is strongly-minimum by ﬁnding one strongly-minimum set Z′
and testing if P
Z∈Z w(Z) = P
Z∈Z′ w(Z). Such a strongly-minimum set Z′ can be obtained by calling FINDMINCOST-
SEP again with parameter I = ∅. Although Z′ might not fulﬁll the constraint I ⊆Z′, it has the same required sum.
Thus, we get the following:
Proposition 6. Finding an m-separator Z, with I ⊆Z ⊆R, which is strongly-minimum can be done in time O(n3).
The same arguments as above show that Z is M-minimum for any M ⊂I, if an M-minimum m-separator satisfying
the constraints exist.
3.3
The hardness of strong-minimality
For most problems it is easier to ﬁnd a minimal solution than a minimum one, but for m-separators the opposite
is true. If a strongly-minimum m-separator exists, it is also strongly-minimal. However there is a gap, where no
strongly-minimum m-separator exists, and the I-minimum or I-minimal m-separators are not strongly-minimal.
In this section we show that it is hard to ﬁnd a strongly-minimal m-separator even for singleton X, Y in DAGs,
in which m-separation and d-separation are equivalent.
Due to the equivalence between d-separation and vertex
separators in the moralized graph, this implies that it is also NP-hard to ﬁnd strongly-minimal vertex separators in
undirected graphs. Together with the characterizations of adjustment sets in the coming sections, it will follow that it
is also NP-hard to ﬁnd strongly-minimal adjustments in DAGs or ancestral graphs.
Proposition 7. It is an NP-complete problem to decide if in a given DAG there exists a strongly-minimal d-separating
set Z containing I.
14

X
Y
I1
V1,l
V1,r
V1
V1
Y
I2
V2,l
V2,r
V2
V2
Y
I3
V3,l
V3,r
V3
V3
. . .
I′
1
I′
2
. . .
Figure 6: The graph used in the proof of Proposition 7, which represents the ﬁrst three variables V1, V2, V3, and two
clauses (V1 ∨V2 ∨V3) and (V1 ∨V2 ∨V3). All shown Y nodes can be considered to be a single node Y, but we
display them separately to reduce the number of overlapping edges in the ﬁgure.
Proof. The problem is in NP, since given Z verifying I ⊆Z is trivial and the strong-minimality can be efﬁciently
tested by algorithm TESTMINSEP with parameter I = ∅.
To show the NP-hardness we take an instance of 3-SAT, a canonical NP-complete problem [29], and construct a
DAG G and a set I, such that a strongly-minimal d-separating set containing I exists in G, if and only if the 3-SAT
formula is satisﬁable.
The 3-SAT instance consists of k variables V1, . . ., Vk, and ℓclauses Ci = (Wi,1 ∨Wi,2 ∨Wi,3) of literals Wi,j ∈
{V1, . . ., Vk, V1, . . . , Vk}. It is NP-hard to decide if there exists a Boolean assignment φ : {V1, . . . Vk} →{true, false}
that satisﬁes the formula C1 ∧. . . ∧Cℓ.
The basic idea of the construction is to ensure that any m-separator must contain some nodes to block a path
between X and Y, while no m-separator can contain all those nodes, because all nodes together would block all paths
from X to nodes in I making the separator not I-minimal. The choice of a node will correspond to choosing an
assignment to a variable of the satisﬁable problem.
Let G = (V, E) be deﬁned as:
V
=
{X, Y} ∪{Vi,l, Vi,r, Vi, Vi, Ii | i ∈{1, . . ., k}} ∪{I′
i | i ∈{1, . . ., ℓ}}
E
=
{X ←Vi,l →Vi →Y | i ∈{1, . . ., k}}
∪
{X ←Vi,r →Vi →Y | i ∈{1, . . ., k}}
∪
{Ii →Vi,l, Ii →Vi,r, Ii →Y | i ∈{1, . . ., k}}
∪
{I′
i →X | i ∈{1, . . ., ℓ}}
∪
{Wi,j →I′
i | i ∈{1, . . ., ℓ}, Wi,j ∈Ci}
I
=
{Ii | i ∈{1, . . ., k}} ∪{I′
i | i ∈{1, . . ., ℓ}}
The resulting graph is shown in Figure 6. We identify the literal Vi (Vi) in the formula with the node Vi (Vi) in the
graph. Vi,l (Vi,r) is a left (right) node, , l in the index should not be confused with the number of clauses ℓ.
“⇐”: Let φ be a Boolean assignment that satisﬁes the formula. Let
Z = I ∪{Vi, Vi,r | i ∈{1, . . ., k}, φ(Vi) = false} ∪{Vi, Vi,l | i ∈{1, . . ., k}, φ(Vi) = true}.
To show that Z d-separates X and Y, we begin a breadth ﬁrst search at X and enumerate all reachable nodes.
Immediately reachable are nodes I′
i, but they are in I ⊆Z, so the path stops there. The parents Vi,l are reachable, but
15

they are either in Z or only Vi, Ii ∈Z are reachable and the path stops there. Likewise no path through Vi,r can reach
Y. Hence Z is a d-separator.
Z is also strongly-minimal: If Vi ∈Z, Vi,l < Z and Z \ {Vi} would not block the path X ←Vi,l →Vi →Y. A
similar path would be opened by removing Vi, Vi,l or Vi,r. Z \ {Ii} is not a d-separator as it would not block either the
path X ←Vi,l →Ii →Y or X ←Vi,r →Ii →Y. If a clause Ci is satisﬁed by a literal Vj, Z \ {I′
i} is not a d-separator,
as it would not block the path X ←I′
i ←Vj →Y . Likewise X ←I′
i ←Vj →Y would be opened by removing I′
i if
the clause Ci is satisﬁed by a literal Vj.
Therefore Z is a strongly-minimal d-separator.
“⇒”: Now we show that a strongly-minimal d-separator Z yields a satisfying assignment φ. For every i the two
paths X ←Vi,l →Vi →Y and X ←Vi,r →Vi →Y need to be blocked by a node of {Vi,l, Vi} and a node of {Vi,r, Vi}.
If neither Vi nor Vi are in Z, both Vi,l and Vi,r must be in Z, so Ii is not reachable from X, Z \ {Ii} is a d-separator
and Z is not strongly-minimal. Therefore Vi or Vi is in Z and the following Boolean assignment φ to the variables is
well-deﬁned:
φ(Vi) =

true
Vi < Z,
false
Vi < Z,
false
otherwise.
Since I′
i ∈I for all i, I′
i has to be reachable from Y, so there is an open path I′
i ←Vj →Y (or I′
i ←V j →Y)
and Vj (or V j) is not in Z for some j. This Vj (or Vj) satisﬁes clause Ci according to the deﬁnition of φ. Hence every
clause and the formula is satisﬁable.
□
3.4
Algorithms for minimal m-separators in sparse graphs
Subsection 3.2 gave O(n2) algorithms for the problems TESTMINSEP and FINDMINSEP in case of I-minimality,
which is optimal for dense graphs. The runtime of these algorithms is limited by the time required to construct the
augmented graph, which might contain nodes and edges that have no inﬂuence on the minimal m-separator and are thus
included unnecessarily. This leads to the question, can we test or ﬁnd the minimal m-separator without constructing
the augmented graph? The obvious way is to test the deﬁnition of minimality directly by removing each of the nodes
of Z and testing if the smaller set is still an m-separator, which yields O(|Z|(n + m)) = O(|Ant(X ∪Y ∪I)|(n + m)) =
O(n(n + m)) algorithms. This is generally slower than the previous runtimes, however in sparse graphs that have a
low number of edges m ≅n and a low number of ancestors |Ant(X ∪Y ∪I)| ≪n, it might be faster. This alternative
algorithm for TESTMINSEP in sparse graphs, in case of I-minimality, is as follows
function TESTMINSEPSPARSE(G, X, Y, Z, I, R)
if Z \ Ant(X ∪Y ∪I) , ∅or Z ⊈R then return false
if not TESTSEP(G, X, Y, Z) then return false
G′ ←GAnt(X∪Y∪I)
for all U ←Z \ I do
if TESTSEP(G′, X, Y, Z \ {U}) then return false
return true
An I-minimal m-separator relative to (X, Y) can be computed using the algorithm
16

function FINDMINSEPSPARSE(G, X, Y, I, R)
G′ ←GAnt(X∪Y∪I)
Z ←R ∩Ant(X ∪Y ∪I) \ (X ∪Y)
if not TESTSEP(G′, X, Y, Z) then return ⊥
for all U in Z \ I do
if TESTSEP(G′, X, Y, Z \ {U}) then
Z ←Z \ {U}
return Z
Proposition 8. The tasks TESTMINSEP and FINDMINSEP for I-minimal separators can be solved in time O((n+m)·
|Ant(X ∪Y ∪I)|).
The correctness of these algorithms depends on the non-obvious fact that m-separators are monotone: if Z \ V is
not an m-separator, no Z \ V with V ∈V is one either. This monotonicity was proven for d-separation by [15] and we
will state their results in terms of m-separation:
Lemma 3. Let X, Y be two sets and let Z be an m-separator relative to (X, Y). If the set Z ∪{Z1, . . ., Zn} is also an
m-separator, where Z1, . . . , Zn are single nodes which are not in Z then either Z ∪{Z1}, or Z ∪{Z2}, . . . or Z ∪{Zn}
must be another m-separator between X and Y.
Corollary 3. If Z and Z∪Zn are m-separators, where Zn = {Z1, . . ., Zn}, then there exist a series of n−1 m-separators:
Z ∪Zi, i = 1, . . ., n −1, with Z1 ⊂. . . ⊂Zn−1 ⊂Zn such that each Zi contains exactly i nodes.
Corollary 4. If no single node can be removed from a m-separator Z without destroying m-separability, then Z is
minimal.
To generalize the proofs of [15] to m-separation, it is sufﬁcient to replace d-separation with m-separation, DAGs
with AGs and ancestors with anteriors throughout their arguments. Therefore, we do not repeat the proofs here.
In principle one needs to take care of the order in which the nodes are enumerated when searching a minimal subset
of a m-separator by removing nodes one-by-one like we do in algorithm FINDMINSEPSPARSE. For example to obtain
a minimal subset of {Z1, Z2} in a DAG X →Z2 ←Z1 →Y the node Z2 must be removed ﬁrst, or Z1 could not be
removed. However, this is not an issue when all nodes of Z are anteriors in Ant(X ∪Y ∪I) due to the correspondence
of m-separators to separators in the augmented graph, since removing a node cannot block a path in the augmented
graph. From this correspondence one can also conclude Lemma 3 restricted to anteriors.
3.5
Algorithms for enumerating all m-separators
Lastly, we consider the problem of listing all m-separators and all minimal m-separators between X and Y in G. Since
there might be an exponential number of (minimal) m-separators, it is not possible to list them all in polynomial time.
For example in a path X ←V ←V′ ←Y either V or V′ must be in a minimal m-separator between X and Y, so
a graph containing k such paths will have at least 2k different m-separators. Therefore we will describe algorithms
running with O(n3) delay, which means that at most O(n3) time will pass between the start or the output of an m-
separator and the output of the next m-separator or the end of the algorithm. They are based on the enumeration
algorithm for minimal vertex separators of [25].
function LISTSEP(G, X, Y, I, R)
if FINDSEP(G, X, Y, I, R) , ⊥then
if I = R then Output I
else
V ←an arbitrary node of R \ I
LISTSEP(G, X, Y, I ∪{V}, R)
LISTSEP(G, X, Y, I, R \ {V})
17

Analysis of the Algorithm. Algorithm LISTSEP performs backtracking to enumerate all Z, with I ⊆Z ⊆R, aborting
branches that will not ﬁnd a valid separator. Since every leaf will output a separator, the tree height is at most n and
the existence check needs O(n + m), the delay time is O(n(n + m)). The algorithm generates every separator exactly
once: if initially I ⊊R, with V ∈R \ I, then the ﬁrst recursive call returns all separators Z with V ∈Z and the second
call returns all Z′ with V < Z′. Thus the generated separators are pairwise disjoint.
□
Proposition 9. Using the algorithm above, the task LISTSEP can be solved with polynomial delay O(n(n + m)).
To enumerate all minimal m-separators we can directly apply Takata’s enumeration algorithm [25] after transform-
ing the ancestral graph to its augmented graph:
function LISTMINSEP(G, X, Y, I, R)
G′ ←GAnt(X∪Y∪I)
G′a ←(GAnt(X∪Y∪I))a
Add a node Xm connected to all X nodes.
Add a node Ym connected to all Y nodes.
Remove nodes of I.
Remove nodes of V \ R connecting the neighbors of each removed node.
Use the algorithm in [25] to list all sets separating Xm and Ym.
Analysis of the Algorithm. The correctness is shown by [24] for adjustment sets and generalizes directly to m-separators,
because after moralization, both problems are equivalent to enumerating vertex cuts of an undirected graph. The han-
dling of I is shown by [23].
□
Proposition 10. The task LISTMINSEP can be solved with polynomial delay O(n3).
4
Empirical analysis of DAG consistency testing
The concept of m-separation in ancestral graphs, or d-separation in DAGs, is of central importance in the ﬁeld of
Bayesian networks and graphical causal models because it couples the graphical model structure to conditional inde-
pendences in the corresponding probability distribution. Perhaps the most direct application of graphical separation is
therefore consistency testing, where we check whether a graphical causal model is in fact consistent with the dataset
it is intended to represent. If a graphical causal model fails to pass this test, then any downstream analyses, such as
implied adjustment sets, are potentially nulliﬁed. In this section, we illustrate how the algorithmic framework that we
developed in Section 3 can be harnessed for graphical causal model checking by applying it to the problem of deriving
basis sets for testing the consistency of a DAG to a given probability distribution.
A DAG G is called consistent with a probability distribution P if for all pairwise disjoint subsets X, Y, Z ⊆V,
where only Z may be empty, X and Y are conditionally independent given Z in P whenever Z d-separates X and Y
in G. Therefore, to test consistency, we could in principle enumerate all d-separation relations in G and then perform
the corresponding conditional independence tests. However, that set of relations can be very large – there is already
an exponential number of subsets X and Y to consider, each of which could be separated by an exponential number of
sets Z. A basis set of d-separation statements entails all other statements implied by G when combining them using
the axioms of conditional independence [30]. The canonical basis set has the form
{∀X ∈V : X ⊥⊥V \ (Pa(X) ∪De(X)) | Pa(X)},
or in words, every variable must be jointly independent of its non-descendants conditioned on its parents. This basis
set only contains n elements but the independences to be tested can be of very high order.
In practice, conditional independence is often tested using methods that do not distinguish between joint and
pairwise independence, such as partial correlation or linear regression. In such cases, even simpler basis sets can be
derived. For instance, Pearl and Meshkat [31] discuss basis sets for hierarchical linear regression models (also known
18

as path models or structural equation models [32]). We here consider the vertices V = {X1, . . . , Xn} to be indexed
topologically, such that for j > i, we have that Xj < An(Xi). Then we consider basis sets having the form
{∀j > i, Xi →Xj < E : Xi ⊥⊥Xj | Zij},
or in words, we include one separation statement for every nonadjacent pair of variables. These are pairwise rather than
joint independences, which is equivalent when we measure dependence only by partial correlation [31]. An obvious
choice is Zij = Pa(Xj), in which case we simply obtain a pairwise version of the canonical basis set above. We call
that basis set the parental basis set. However, Pearl and Meshkat show that another valid choice is any separating set
Zij that contains only nodes whose distance to Xj is smaller than the distance between Xi and Xj (where the distance is
deﬁned length of a shortest path between two nodes). We call any such set a sparse basis set. Note that every parental
basis set is also sparse, but the separators in sparse basis sets can be substantially smaller than in parental basis sets.
4.1
Simulation setup
We evaluate the difference between parental and sparse basis sets on random DAGs. These are generated with var-
ious numbers of variables n ∈{10, 25, 50, 100}. The edges are chosen independently with individual probabilities
P(edge) = l/(n −1), where l ∈2, 5, 10, 20. For small n the probabilities are capped at 1. For example, for l = n = 10,
the parameter P(edge) = min{10/(10 −1), 1} = 1 will only generate complete graphs. Parameter l describes the
expected number of neighbors of a node in a generated DAG. This leads to an expected number of edges in generated
graphs E[m] ≅ln/2, as there are n(n −1)/2 possible edges in a DAG of n nodes, each existing with probability
P(edge). Our algorithmic framework makes it straightforward to compute sparse basis sets with minimal separating
sets by simply setting R = {Xk ∈V | d(Xk, Xj) < d(Xi, Xj)}, where we use d(Xa, Xb) to denote the distance between
two nodes.
4.2
Empirical results
We empirically evaluate the sizes of parental and sparse basis sets in these random DAGs (Figure 7). The results show
that the beneﬁt of using sparse rather than parental basis sets does not depend much on the size of the DAG but rather
on the amount of edges. For instance, when nodes had on average 5 neighbors, sparse basis sets had between 20% and
40% fewer conditioning variables than the canonical parental basis sets.
These results provide a ﬁrst example of how our framework can be applied in the context of graphical causal
modeling. In the rest of this paper, we focus on covariate adjustment as our focus application area. However, given
that separators play a central role in many aspects of graphical causal models, we expect that there should be many
more applications in addition to those shown in this paper.
5
Adjustment in DAGs
We now leverage the algorithmic framework of Section 3 together with a constructive, sound and complete criterion
for covariate adjustment in DAGs to solve all problems listed in Table 2 for adjustment sets rather than m-separators
in the same asymptotic time.
First, note that the formal deﬁnition of adjustments (Deﬁnition 1) cannot be used to actually ﬁnd an adjustment set
as there are inﬁnitely many probability distributions that are consistent to a certain graph. Fortunately, it is possible to
characterize adjustment sets in graphical terms.
Deﬁnition 2 (Adjustment criterion (AC) [7]). Let G = (V, E) be a DAG, and X, Y, Z ⊆V be pairwise disjoint subsets
of variables. The set Z satisﬁes the adjustment criterion relative to (X, Y) in G if
(a) no element in Z is a descendant in GX of any W ∈V \ X which lies on a proper causal path from X to Y and
(b) all proper non-causal paths in G from X to Y are blocked by Z.
19

40
60
80
100
0
20000
40000
n
sum of conditioning set sizes
l=2
l=5
l=10
l=20
parental basis
sparse basis
20
15
10
5
0
20
60
100
l
% reduction in conditoning set size
n=25
n=50
n=100
Figure 7: Model checking with parental or sparse basis sets for random DAGs. Here, n denotes the number of nodes
in the DAG, and l denotes the expected number of neighbours of each node. The left panel shows the total size of
the conditioning sets as a function of n, showing the expected quadratic increase. The right panel emphasizes that the
beneﬁt of using a sparse basis set is greatest if the graph is also sparse, in which case the total number of variables that
need to be conditioned on can be reduced by up to 90%.
G:
X1
X2
D1
Y
Z
V
D2
D3
Gpbd
XY:
X1
X2
D1
Y
Z
V
D2
D3
Gpbd,C
XY
:
X1
X2
D1
Y
Z
V
D2
D3 C
Figure 8: A DAG that permits exactly two adjustment sets for estimating the causal effect of X = {X1, X2} on Y = {Y}:
Z = {Z} and Z′ = {Z, V}. Because V \ Dpcp(X, Y) = {X1, X2, Z, V}, every adjustment is a subset of {Z, V}. The nodes
D1, D2, D3 are not allowed in any adjustment as they are not in {Z, V} – the set of descendants of a non-X node on
the (only) proper causal path X2 →D1 →Y. Moreover, every adjustment must contain the variable Z to block the
path between X2 and Y in Gpbd
XY. Graph Gpbd,C
XY
illustrates a parametrized proper back-door graph (Deﬁnition 5) with
parameter C = Dpcp(X, Y) = {D1, D2, D3, Y} surrounded by a dotted rectangle. Removing edge X2 →D2 from Gpbd
XY
simpliﬁes the DAG while preserving the reduction of ﬁnding adjustment sets to d-separation.
However, even this criterion does not lead to efﬁcient algorithms to ﬁnd adjustment sets, because there can be
exponentially many paths and blocking one non-causal path might open other paths. We address this problem below
by presenting a constructive adjustment criterion that reduces non-causal paths to ordinary m-separation, which can
be reduced further to a reachability problem in undirected graphs.
5.1
Constructive back-door criterion
The graph of this reduction will be called the proper back-door graph:
Deﬁnition 3 (Proper back-door graph). Let G = (V, E) be a DAG, and X, Y ⊆V be pairwise disjoint subsets of
variables. The proper back-door graph, denoted as Gpbd
XY, is obtained from G by removing the ﬁrst edge of every proper
causal path from X to Y.
For an example proper back-door graph Gpbd
XY, see Fig. 8. Note the difference between the proper back-door graph
Gpbd
XY and the famous back-door graph GX of [1]: in GX all edges leaving X are removed while in Gpbd
XY only those that
20

lie on a proper causal path (see Figure 9 for an example illustrating the difference). However, to construct Gpbd
XY still
only elementary operations are sufﬁcient. Indeed, we remove all edges X →D in E such that X ∈X and D is in the
subset of nodes on proper causal paths, which we abbreviate as PCP(X, Y), and which is deﬁned as follows:
PCP(X, Y) = (DeX(X) \ X) ∩AnX(Y).
(3)
Hence, the proper back-door graph can be constructed from G in linear time O(m + n). Note that using the notation
PCP the proper back-door graph can be speciﬁed as
Gpbd
XY = (V, E \ (X →PCP(X, Y))).
Now we propose the following adjustment criterion. For short, we will denote the set De(PCP(X, Y)) as Dpcp(X, Y).
Deﬁnition 4 (Constructive back-door criterion (CBC)). Let G = (V, E) be a DAG, and let X, Y, Z ⊆V be pairwise
disjoint subsets of variables. The set Z satisﬁes the constructive back-door criterion relative to (X, Y) in G if
(a) Z ⊆V \ Dpcp(X, Y) and
(b) Z d-separates X and Y in the proper back-door graph Gpbd
XY.
Figure 8 shows how the constructive back-door criterion can be applied to ﬁnd an adjustment set in an example
DAG.
The CBC is surprisingly robust to modiﬁcations of its deﬁnition which allows a ﬂexible handling of the forbidden
nodes for adjustments and edges one can remove from the DAG to build a proper back-door graph. An example of
a modiﬁed proper back-door graph is given in Figure 8 (rightmost graph): we can remove some further edges from
the proper back-door graph simplifying its structure while preserving the property of interest, i.e., ﬁnding adjustment
sets via d-separation. The following extended deﬁnition incorporates various possible variants determined by speciﬁc
parameters, which will turn out to be useful when proving the correctness of the CBC and other related criteria.
Deﬁnition 5 (Parametrization of the constructive back-door criterion (CBC(A, B, C))). Let G = (V, E) be a DAG, and
let X, Y, Z ⊆V be pairwise disjoint subsets of variables. Let A ⊆X ∪Y, B ⊆X, C ⊆Dpcp(X, Y). The set Z satisﬁes
CBC(A, B, C) relative to (X, Y) in G if
(a′) Z ⊆V \ DeAB(PCP(X, Y)), and
(b′) Z d-separates X and Y in the graph Gpbd,C
XY
:= (V, E \ (X →(PCP(X, Y) ∪C))).
Clearly CBC(∅, ∅, ∅) = CBC. The variants CBC(X, ∅, ∅) and CBC(∅, ∅, Dpcp(X, Y)) might be the most interesting.
Condition (a′) of CBC(X, ∅, ∅) forbids less nodes than (a) of the CBC, namely it excludes the descendants of PCP(X, Y)
in GX. So, this condition (a′) is identical to condition (a) of the AC (Deﬁnition 2) which helps to prove the correctness
of the CBC. Condition (a′) of CBC(∅, ∅, Dpcp(X, Y)), see Figure 8 for an example, forbids exactly the same nodes
as (a) of the CBC but according to the condition (b′) more edges can be removed than due to (b) of the CBC, which
might improve the performance in an implementation.
Note that the deﬁnition excludes CBC(∅, Y, ∅), which could be considered as modifying condition (a) to forbid the
descendants of PCP(X, Y) in the graph GY. This would not lead to a valid criterion as it would allow an adjustment
set {Z} in the graph X →Y →Z, where {Z} is not an adjustment. However, removing edges into Y as in the graph
GY of CBC(Y, ∅, ∅) does not change the descendants at all, since the relevant Y are in PCP(X, Y) themselves. We can
show that none of these modiﬁcations change the criterion:
Lemma 4. Let G = (V, E) be a DAG, and let X, Y ⊆V be pairwise disjoint subsets of variables. Let A ⊆X ∪Y,
B ⊆X, C ⊆De(PCP(X, Y)). Then CBC(A, B, C) is equivalent to the CBC.
Proof. Let Z be a set that satisﬁes the CBC. Since Z ⊆V\Dpcp(X, Y) = V\De(PCP(X, Y)) and DeAB(PCP(X, Y)) ⊆
De(PCP(X, Y)), the condition (a′) Z ⊆V \ DeAB(PCP(X, Y)) is satisﬁed for CBC(A, B, C). Z d-separates X and Y in
21

Gpbd
XY, and thus also in Gpbd,C
XY
, because every edge or path of Gpbd,C
XY
also exists in Gpbd
XY. Thus (b′) is true as well. Hence,
Z satisﬁes CBC(A, B, C).
To see the other direction, let Z be a set that satisﬁes CBC(A, B, C), but not CBC. If Z does not satisﬁes CBC (a),
there exists a node Z ∈V \ DeAB(PCP(X, Y)) that is not in V \ Dpcp(X, Y) = V \ De(PCP(X, Y)). Then there must
exist a proper causal path from X to Y on which a node W ∈V \ X is an ancestor of Z in G, but not in GAB, i.e., there
is a causal path from a node of X over W to Z which intersects A ∪B. We can assume the nodes were chosen such
that the length of the subpath between W and Z is minimal.
Let W →. . . →Y denote the sufﬁx of the path from X to Y starting in W. Note that this path might consist of
only the vertex W. Additionally, for the causal path from W to Z, let W →. . . →A be its shortest preﬁx with A , W
which ends in A ∪B ∪X ⊆X ∪Y. Notice that W itself cannot be in B and, if it is in A, it does not change the
paths. Then, from the condition (a′), we know that no vertex of W →. . . →A belongs to Z. If A ∈X, this leads to a
contradiction with the condition (b′) since A ←. . . ←W →. . . →Y is a path Gpbd,C
XY
from X to Y that is not blocked
by Z. Otherwise we have A ∈Y, so A ∈PCP(X, Y) and the path from A to Z is shorter than the path from W to Z,
which contradicts the choice of W.
If Z does not satisﬁes CBC (b), but satisﬁes CBC (a), there exists a path π between X and Y not blocked in Gpbd
XY
by Z that is blocked in Gpbd,C
XY
due to a removed edge X →C with X ∈X, C ∈C. If X →C is on π, we can assume it
is the last such edge on π. If the subpath from C to Y is causal, this edge is also removed in Gpbd
XY, a contradiction. So
this subpath becomes non-causal at a collider →C′ ←unblocked in Gpbd
XY, which has a descendant in Z that is also a
descendant of C contradicting CBC (a). If the removal of the edge X →C prevents the opening of a collider, C is also
the ancestor of a node in Z, which contradicts CBC (a) either.
□
We will see the usefulness of the parametrization of the constructive back-door criterion in the proof of the main
result of this subsection:
Theorem 1. The constructive back-door criterion (CBC) is equivalent to the adjustment criterion (AC).
Proof. First observe that the condition (a) of the adjustment criterion AC is identical to condition (a′) of the construc-
tive back-door criterion CBC(X, ∅, ∅). Assume conditions (a) and (b) of the adjustment criterion AC hold. Due to
Lemma 4, it is sufﬁcient to show that condition (b) of the constructive back-door criterion is satisﬁed. Let π be any
proper path from X to Y in Gpbd
XY. Because Gpbd
XY does not contain causal paths from X to Y, π is not causal and has to
be blocked by Z in G by the assumption. Since removing edges cannot open paths, π is blocked by Z in Gpbd
XY as well.
Now we show that (a) and (b) of the constructive back-door criterion CBC together imply (b) of the adjustment
criterion AC. If that were not the case, then there could exist a proper non-causal path π from X to Y that is blocked
in Gpbd
XY but open in G. There can be two reasons why π is blocked in Gpbd
XY: (1) The path starts with an edge X →D
that does not exist in Gpbd
XY. Then we have D ∈PCP(X, Y). For π to be non-causal, it would have to contain a collider
C ∈An(Z) ∩De(D) ⊆An(Z) ∩Dpcp(X, Y). But because of CBC (a), An(Z) ∩Dpcp(X, Y) is empty. (2) A collider C
on π is an ancestor of Z in G, but not in Gpbd
XY. Then there must be a directed path from C to Z via an edge X →D
with D ∈An(Z) ∩PCP(X, Y), contradicting CBC (a).
□
5.2
CBC vs Pearl’s back-door criterion
In this section we relate our constructive back-door criterion to the well-known back-door criterion by Pearl [1]:
Deﬁnition 6 (Pearl’s back-door criterion (BC) [1]). A set of variables Z satisﬁes the back-door criterion relative to an
ordered pair of variables (X, Y) in a DAG G if:
(a) Z ⊆V \ De(X) and
(b) Z blocks every path between X and Y that contains an arrow into X.
Similarly, if X and Y are two disjoint subsets of nodes in G, then Z is said to satisfy the back-door criterion relative to
(X, Y) if it satisﬁes the back-door criterion relative to any pair (X, Y) such that X ∈X and Y ∈Y.
22

G:
X1
Z1
Z2
X2
Y1
Y2
GX:
X1
Z1
Z2
X2
Y1
Y2
Gpbd
XY:
X1
Z1
Z2
X2
Y1
Y2
Figure 9: A DAG where for X = {X1, X2} and Y = {Y1, Y2}, Z = {Z1, Z2} is a valid and minimum adjustment, but no
set fulﬁlls the back-door criterion [1] (Deﬁnition 6), and the parents of X are not a valid adjustment set either.
In Deﬁnition 6 condition (b) is often replaced by the equivalent condition that Z d-separates X and Y in the back-
door graph GX.
In [24] it was shown that for minimal adjustment sets in X-loop-free DAGs the adjustment criterion and the back-
door criterion of Pearl are equivalent. A DAG is X-loop-free for an exposure set X, if no directed path between two
different nodes of X contains a node not in X. If X is a singleton, there are no two different nodes of X and every
DAG is X-loop-free, so the criteria are always equivalent for minimal adjustments. In this case it is still possible
that an adjustment set Z satisﬁes the CBC and not the back-door criterion, but there will always be a minimal subset
Z′ ⊆Z that satisﬁes the back-door criterion. Since an adjustment set satisfying the back-door criterion also satisﬁes
the generalized back-door criterion of [17], and all sets of the generalized back-door criterion satisfy our CBC, all
three criteria are equivalent to test the existence of an (minimal) adjustment set for a singleton X in DAGs.
The situation changes if the effect of multiple exposures is estimated. Theorem 3.2.5 in [1] claims that the ex-
pression for P(y | do(x)) is obtained by adjusting for Pa(X) if Y is disjoint from Pa(X) in graphs without latent nodes,
but, as the DAG in Figure 9 shows, this is not true: the set Z = Pa(X1, X2) = {Z2} is not an adjustment set ac-
cording to {X1, X2} and {Y1, Y2}. In this case one can identify the causal effect by adjusting for Z = {Z1, Z2} only.
Indeed, for more than one exposure, no adjustment set may exist at all even without latent covariates and even though
Y ∩(X ∪Pa(X)) = ∅, e.g., in the DAG X1
X2
Z
Y and for X = {X1, X2} and Y = {Y}.
In the case of multiple exposures X it is also harder to use the back-door criterion to actually ﬁnd an adjustment
set. Although the back-door criterion reduces adjustment to d-separation in the back-door graph GX, this is not the
graph GX, so for each exposure X ∈X the criterion would ﬁnd a separate adjustment set, which do not lead directly to
a combined adjustment set for all exposures. For an example see Figure 9.
Table 3 summarizes the relationships between CBC and the Pearl’s back-door criterion.
6
Algorithms for testing and computing adjustment sets in DAGs
Having proved the constructive back-door criterion, we are now licensed to apply our separation algorithms from
Section 3 to solve adjustment set problems. This works because the adjustment relative to X and Y in G corresponds
to an m-separator between X and Y in Gpbd
XY subject to the constraint given by CBC (a). Table 4 gives an overview of
the relevant tasks.
A small difference exists between testing and constructing adjustment sets when handling CBC (a): Testing re-
quires us to check if the given set Z contains nodes of Dpcp(X, Y), whereas constructing requires that the returned set
Z must not contain any of the nodes in Dpcp(X, Y). The latter requirement can be straightforwardly implemented by
imposing the constraint Z ⊆R′ = R \ Dpcp(X, Y), which can be given as parameter to our separation algorithms.
Hence TESTADJ can be solved by testing if Z ∩Dpcp(X, Y) = ∅and if Z is a d-separator in the proper back-door
graph Gpbd
XY using algorithm TESTSEP. Since Gpbd
XY can be constructed from G in linear time, the total time complexity
of this algorithm is O(n + m).
TESTMINADJ can be solved by testing again if Z ∩Dpcp(X, Y) = ∅and calling TESTMINSEP to verify that Z
is minimal within the back-door graph Gpbd
XY. This leads to a runtime of O(n2) which is optimal for dense graphs.
23

Statement for arbitrary DAGs and all sets Z:
proof
Z satisﬁes CBC
⇏
Z satisﬁes back-door
Z ←X →Y
Z satisﬁes CBC
⇏
∃Z′ satisfying back-door
see Figure 9
Z satisﬁes CBC and Z is minimal
⇏
Z satisﬁes back-door
see Figure 9
Z satisﬁes CBC and Z is minimal
⇏
∃Z′ satisfying back-door
see Figure 9
Statement for all X-loop-free DAGs (e.g., for singleton X) and all sets Z:
Z satisﬁes CBC
⇏
Z satisﬁes back-door
Z ←X →Y
Z satisﬁes CBC
⇒
∃Z′ satisﬁes back-door
via minimal Z′ ⊆Z
Z satisﬁes CBC and Z is minimal
⇒
Z satisﬁes back-door
see [24]
Z satisﬁes CBC and Z is minimal
⇒
∃Z′ satisfying back-door
Z′ = Z
Table 3: A summary of the relationship between the existence of a Pearl back-door-adjustment set and the existence
of an CBC-adjustment set in unconstrained DAGs and X-loop-free DAGs. Symbol ⇏means that the implication does
not hold, in general. On the other hand, due to the completeness property of the CBC, we have that if one replaces in
the left hand sides "CBC" by "back-door" and in the right hand sides "back-door" by "CBC", then the corresponding
implications are always true.
Runtime
Veriﬁcation: For given X, Y, Z and constraint I decide if . . .
TESTADJ
Z is an adjustment for (X, Y)
O(n + m)
TESTMINADJ
Z ⊇I is an adjustment for (X, Y) and Z is . . .
I-minimal
O(n2)
strongly-minimal
O(n2)
Construction: For given X, Y and constraints I, R, output an . . .
FINDADJ
adjustment Z for (X, Y) with I ⊆Z ⊆R
O(n + m)
FINDMINADJ
adjustment Z for (X, Y) with I ⊆Z ⊆R which is . . .
I-minimal
O(n2)
strongly-minimal
NP-hard
FINDMINCOSTADJ
adjustment Z for (X, Y) with I ⊆Z ⊆R which is . . .
I-minimum
O(n3)
strongly-minimum
O(n3)
Enumeration: For given X, Y, I, R enumerate all . . .
Delay
LISTADJ
adjustments Z for (X, Y) with I ⊆Z ⊆R
O(n(n + m))
LISTMINADJ
I-minimal adjustments Z with I ⊆Z ⊆R
O(n3)
Table 4: Deﬁnitions of algorithmic tasks related to adjustment in DAGs. The meaning of parameters X, Y, Z, I, and R
is the same as in the deﬁnitions of tasks related to m-separation in Table 2. The right column shows the associated time
complexities given in this section. Due to our linear-time reduction from causal effect identiﬁcation by adjustment to
m-separation in a subgraph of an input DAG, the time complexities to solve the problems above are the same as in
Table 2.
24

Alternatively TESTMINSEPSPARSE with its runtime of O(|Ant(X∪Y)|·|Edges of (Gpbd
XY)a|) = O(n(n+m)) can be used
in sparse graphs. It is worth noting that since the back-door graph is formed by removing edges, it is even sparser
than the input graph. This approach only works because the minimal adjustment corresponds to a minimal separator
in the proper back-door graph: every subset of an adjustment must still satisfy condition CBC (a). It also implies the
following corollary which generalizes the result of [15] from d-separators to adjustment sets:
Corollary 5. An adjustment set Z is minimal if and only if no single node Z can be removed from Z such that the
resulting set Z′ = Z \ Z is no longer an adjustment set.
The problem FINDADJ can be solved by a closed form solution. For a DAG G = (V, E) and constraints I, R we
deﬁne the set
Adjustment(X, Y) = An(X ∪Y ∪I) ∩R \ (X ∪Y ∪Dpcp(X, Y)).
Theorem 2. Let G = (V, E) be a DAG, let X, Y ⊆V be disjoint node sets and I, R constraining node sets with
I ⊆R \ (X ∪Y ∪Dpcp(X, Y)). Then the following statements are equivalent:
1. There exists an adjustment Z in G w.r.t. X and Y with I ⊆Z ⊆R.
2. Adjustment(X, Y) is an adjustment w.r.t. X and Y.
3. Adjustment(X, Y) d-separates X and Y in the proper back-door graph Gpbd
XY.
Proof. The implication (3) ⇒(2) follows directly from the criterion Def. 4 and the deﬁnition of Adjustment(X, Y).
Since the implication (2) ⇒(1) is obvious, it remains to prove (1) ⇒(3).
Assume there exists an adjustment set Z0 w.r.t. X and Y. From Theorem 1 we know that Z0 ∩Dpcp(X, Y) = ∅
and that Z0 d-separates X and Y in Gpbd
XY. Our task is to show that Adjustment(X, Y) d-separates X and Y in Gpbd
XY. This
follows from Lemma 1 used for the proper back-door graph Gpbd
XY if we take I′ = I, R′ = R\(X∪Y∪Dpcp(X, Y)).
□
From Equation (3) and the deﬁnition Dpcp(X, Y) = De(PCP(X, Y)) we then obtain immediately:
Corollary 6. Given two disjoint sets X, Y ⊆V, Adjustment(X, Y) can be found in O(n + m) time.
The remaining problems, FINDMINADJ, FINDMINCOSTADJ, LISTADJ and LISTMINADJ can be solved using
the corresponding algorithms for ﬁnding, resp. listing m-separations applied to the proper back-door graph. Since the
proper back-door graph can be constructed in linear time the time complexities to solve the problems above are the
same in Table 2 and Table 4. The NP-hardness of ﬁnding strongly-minimal adjustment sets follows from Proposition 7
and the fact that the graph constructed in the proof of the proposition contains no causal paths between X and Y, so
there are no forbidden nodes and that graph is the same as its back-door graph.
7
Extending the CBC
While our complete adjustment criterion is guaranteed to ﬁnd all instances in which a causal effect can be identiﬁed
via covariate adjustment, it is well known that not all identiﬁable effects are also identiﬁable via adjustment. The
do-calculus [1] is a complete method that characterizes all identiﬁable causal effects, but which comes at a price of
substantially increased formula and runtime complexity. In this section, we however show that many cases in which
covariate adjustment is not applicable do not require the power of the do-calculus either.
Speciﬁcally, we provide three lemmas that permit identiﬁcation of total causal effects in the following three cases
(which are not mutually exclusive) as shown in Figure 10: (1) X does not have a causal effect on Y; (2) X = X
is singleton and all its parents are observed; (3) X and Y partition V. While in each case the adjustment criterion
may or may not be applicable, our lemmas show that identiﬁability is always guaranteed, and the total effect can be
computed by reasonably simple formulas. Moreover, each lemma provides an easy algorithm for testing whether the
corresponding case applies.
25

(1) Y1
V2
V1
X1
(2) Y1
X1
V1
Y2
(3) Y1
X1
X2
Y2
Figure 10: The three cases analyzed in this section. Exposure and outcome nodes are marked as X and Y; latent nodes
are shown in gray. In case (1) the causal effect is given by P(y1 | do(x1)) = P(y1), in case (2) by P(y1, y2 | do(x1)) =
P(y1)P(y2 | x1, y1), and in case (3) by P(y1, y2 | do(x1, x2)) = P(y1 | x2)P(y2 | y1, x1, x2).
7.1
Identiﬁcation by plain formulas
One case in which identiﬁcation is trivial is if there is no causal effect of X on Y at all. When all non-causal paths
between X and Y can be blocked, then this case is covered by the CBC; however, if there is a non-causal path consisting
solely of unobserved nodes, then the (void) effect is not obtainable through the adjustment formula. In such cases,
however, we simply have P(y | do(x)) = P(y), which we will call the plain formula. The following proposition
provides a characterization of all cases in which this plain formula works in terms of d-separation.
Proposition 11. Let G = (V, E) be a DAG and let X, Y ⊆V be disjoint subsets of variables and let R ⊆V be an
arbitrary set of observed variables, with X ∪Y ⊆R. Then X and Y are d-separated in GX, expressed symbolically as
(Y ⊥⊥X)GX
(4)
if and only if the effect of intervention of X on Y is given by the plain formula P(y | do(x)) = P(y), i.e., there is no
causal effect from X on Y. Particularly, if Y ∈An(X) then (Y ⊥⊥X)GX and thus P(y | do(x)) = P(y).
Proof. The soundness of the statement follows directly by the application of rule 3 (intervention/deletion of actions;
for the precise deﬁnition of the do-calculus rules see Theorem 3.4.1 in [1]). The completeness of the statement can
be shown similarly to the completeness of the adjustment criterion [7]. If Y and X are not d-separated in GX, there
exists a shortest causal path X →. . . →Y for X ∈X, Y ∈Y. In the subgraph G′ = (V′, E′) consisting only of this
path, the causal effect is given by an empty adjustment set P(y | do(x)) = P(y | x). If we take a model P′ where
P′(y | x) , P′(y) for some values x, y, like e.g. in a model on binary variables X, Y with
P′(x) = 1/2
and
P′(y | x) =

1/3
x = y,
2/3
x , y,
the causal effect is not given by P′(y). This model can be extended to a model P on the full graph G by assuming all
other variables are independent, i.e., P(v) = (1/2)|V\V′|P′(v′). This model is consistent with G (though not faithful,
but faithfulness is not required) and we have
P(y | do(x)) = P(y \ Y)P(y | do(x)) = P(y \ Y)P′(y | do(x)) , P(y \ Y)P′(y) = P(y \ Y)P(y) = P(y).
□
7.2
Identiﬁcation by generalized parent adjustment
Another case that permits the identiﬁcation of causal effects using simple formulas occurs if the exposure X = X is
a singleton and all its parents are observed, i.e., Pa(X) ⊆R. Then adjusting for the parents of X blocks all biasing
paths and sufﬁces for identiﬁcation, but one needs to be careful as there might be variables Ypa = Pa(X) ∩Y that
are both parents and outcome nodes. Proposition 12 below shows that in this case the causal effect is given by
P(y | do(x)) = P
z P(z, ypa)P(ynp | x, z, ypa), where Ypa ∪Ynp is a partition of Y and Ypa ∪Z is a partition of Pa(X).
This is a slight generalization of identiﬁcation via adjustment: we still sum over the values of variables Z and
multiply the conditional probability with a factor, but rather than multiplying with the probability of the same variables
P(z) that are used in the sum, we multiply with a factor P(z, ypa) involving additionally the variables in Ypa.
26

The situation is even simpler when Y is also a singleton. Then one of the sets Ypa, Ynp vanishes, so there are only
two cases: either Y < Pa(X) and Pa(X) is an adjustment set [1, Theorem 3.2.2], or Y ∈Pa(X) and no adjustment
exists, but the causal effect is identiﬁed as P(y | do(x)) = P(y). One can see that in the case Y ∈An(X) \ Pa(X) the
effect of intervention do(X = x) can be given both by the plain expression P(y | do(x)) = P(y) and by adjustment in
parents of X.
Proposition 12. Let G = (V, E) be a DAG and let X ∈V be a node with observed parents Pa(X) ⊆R and Y ⊆V \ X.
Furthermore, let Ypa = Y ∩Pa(X) and let Ynp = Y \ Pa(X) be a partition of Y = Ypa ∪Ynp and let Z = Pa(X) \ Ypa
form with Ypa a partition of Pa(X) = Ypa ∪Z. Then
P(y | do(x)) =

P(ypa)P(ynp | x, ypa)
if Z = ∅, i.e., if Pa(X) ⊆Y,
P
z P(z, ypa)P(ynp | x, z, ypa)
if Z , ∅,
where P(ypa) (resp. P(ynp | x, ypa) and P(ynp | x, z, ypa)) should be read as 1 if Ypa = ∅(resp. Ynp = ∅).
Proof. This follows from a straightforward calculation using the do-calculus:
P(y | do(x)) = P(ypa, ynp | do(x))
=
X
z
P(z, ypa, ynp | do(x))
=
X
z
P(z, ypa | do(x))P(ynp | do(x), z, ypa)
=
X
z
P(z, ypa)P(ynp | do(x), z, ypa)
do-calculus rule 3 in [1, Subsect. 3.4.2]
(Ypa, Z ⊥⊥X) in GX
=
X
z
P(z, ypa)P(ynp | x, z, ypa).
do-calculus rule 2 in [1, Subsect. 3.4.2]
(Ynp ⊥⊥X | Z, Ypa) in GX
□
If some of parents Pa(X) are unobserved, the causal effect might not be identiﬁable at all, like e.g., in the DAG G1
in Figure 2. To decide if the effect is identiﬁable in such a case, one can use the CBC criterion which, like for G3 in
Figure 2, can conﬁrm the identiﬁability. However, while the CBC is complete to decide if the effect is expressible via
covariate adjustment it is not complete to decide if the effect is identiﬁable or not. For an example, see the DAG G2
in Figure 2. To solve the identiﬁcation problem in this case, when the CBC does not work, one has to use a complete
criterion, like this based on the do-calculus.
7.3
Identiﬁcation when X and Y partition V
Here we consider the case of DAGs in which X and Y partition the set of variables V, which implies that there are no
unobserved nodes. Again, in this case the CBC may not be applicable as there may be an arrow from Y to X, but still
the causal effect can be given by a closed-form solution as we present below.
Lemma 5. Let G = (V, E) be a DAG and X, Y ⊂V be a partition of V = X ∪Y. The following statements hold
(a) The causal effect of X on Y is given by
P(y | do(x)) =
Y
Y∈Y
P(Y = y | Pa(Y)).
27

(b) If no edge X →Y with X ∈X, Y ∈Y exists, the causal effect is also given by the plain formula
P(y | do(x)) = P(y).
(c) The causal effect can be identiﬁed by adjustment if and only if no edge Y →X with X ∈X, Y ∈Y exists.
(d) If identiﬁcation by adjustment is possible, the adjustment set is Z = ∅and the causal effect of X on Y is given by
P(y | do(x)) = P(y | x).
Proof. Statement (a) follows from the deﬁnition of the causal effect:
P(y | do(x)) =
X
x′
P(X = x′, Y = y | do(X = x))
= P(x, y | do(x))
x , x′ makes the causal effect inconsistent
= P(v | do(x))
=
Y
Yj∈Y
P(yj | paj)
deﬁnition of the causal effect
=
Y
Y∈Y
P(Y = y | Pa(Y))
For Statements (b) and (c) note that edges X →X or Y →Y do not affect d-connectedness between X and Y.
Hence with the assumption in Statement (b), the sets X and Y are d-separated in the graph GX, where all edges Y →X
and X →X are deleted. Then we know from Proposition 11 that the causal effect is identiﬁed by a plain formula.
Since no node is outside of X ∪Y, the only possible adjustment set is Z = ∅, which is Statement (d). Finally, an
adjustment set Z = ∅always satisﬁes the ﬁrst condition of the CBC. The back-door graph is formed by removing all
edges X →Y as those edges form a causal path of length one. Thus Z is an adjustment set, if and only if no edge
Y →X exists, which is Statement (c).
□
When V = X ∪Y, the R package causaleffect [33], which we used in our experiments described in the next
section, returns the formula P(y | do(x)) = Q
Y∈Y P(Y | An(Y)\Y) when conﬁgured to be fast and ﬁnd any identiﬁcation
formula rather than a short one. Thus it is worth to mention that Q
Y∈Y P(Y | An(Y)\Y) = Q
Y∈Y P(Y | Pa(Y)), because
the parents of a node Y block all paths from other ancestors of Y to Y.
8
Empirical analysis of identiﬁability by adjustment
As mentioned before, not all identiﬁable total effects are identiﬁable via covariate adjustment, but if covariate adjust-
ment is possible, then it is usually preferred to other methods due to its benign statistical properties. This raises the
question how often we will actually have to go beyond covariate adjustment when identifying causal effects. The
completeness and algorithmic efﬁciency of the CBC allowed us to perform an empirical analysis of identiﬁability via
adjustment in random graphs, including graphs of substantial size.
The basic setup of our experiments is as follows. We (1) generate a random graph; (2) set nodes to be unobserved
at random; (3) choose random disjoint subsets X, Y of pre-speciﬁed cardinalities from the observed nodes; and (4)
test whether P(y | do(x)) is identiﬁable in the resulting graph. We test the identiﬁability of P(y | do(x)) using four
increasingly powerful criteria: (1) Pearl’s back-door criterion [1]; (2) the CBC; (3) an extended version of the CBC
that also covers the special cases discussed in Section 7; and (4) the do-calculus, which characterizes all effects that
are identiﬁable at all. Full details are given below.
We included the classic back-door criterion (Deﬁnition 6) in our analysis because it is still very present in the
applied literature on DAGs (e.g., [2]) whereas the generalized version is still barely mentioned. It is known that the
back-door criterion is not complete and can thus fail to identify an adjustment set, which raises the question how often
28

(a)
X1
X2
V0
Y1
V1
Y2
V2
X3
V3
Y3
(c)
V1
X1
V0
Y1
Y2
X2
Y3
V2
X3
V3
(p)
Y1
V1
V2
V3
X1
Y2
X2
Y3
V4
X3
(n) X1
V1
V2
V3
X2
V4
Y1
Y2
X3
Y3
Figure 11: Example DAGs sampled for the parameters n = 10, P(edge) = 2/9, P(unobserved) = 0.5, and
k = |X| = |Y| = 3. Nodes are relabeled such that exposures are called X1, X2, X3, outcomes are called Y1, Y2, Y3,
and all nodes except V0 are unobserved. Case (a)djustment is identiﬁed by using the empty set and by the formula
P
v0[P(y1|x1)P(v0|x1)P(y3|x1, y1, v0, y2)P(y2|x1, v0)] found by the ID-algorithm. Instance (c)omplex is identiﬁed by
the complex formula P
v0[P(v0|x1)P(y1|x1, v0)P(y2|v0)P(y3)] and instance (p)lain is identiﬁed by the plain formula
P(y1, y2, y3), although in this case no adjustment set exists. The ﬁnal example is (n)ot identiﬁable.
the back-door criterion fails to ﬁnd an adjustment set when our CBC criterion succeeds. In Section 5.2 it was shown
that this is never the case for a singleton X (although the CBC may still ﬁnd more adjustment sets than the BC).
Our extensions to the CBC in Section 7 were motivated by our observation from preliminary experiments that
many cases where an effect is not identiﬁable by adjustment are anyway identiﬁable due to simple reasons like the
absence of any causal path from X to Y, which can be addressed quite easily without invoking the full machinery of
the do-calculus.
We now proceed to give the technical details of how we set up our empirical analysis.
8.1
Instance generation
We evaluate identiﬁability on random DAGs, which we generate as described in Section 4. The random DAGs are
generated with different numbers of variables V
|V| = n ∈{10, 25, 50, 100, 250, 500, 1000, 2000}.
These variables are divided into four sets: ordinary observed nodes R, unobserved nodes V \ R, exposure nodes
X ⊆R, and outcome nodes Y ⊆R (with X ∩Y = ∅) depending on parameters
P(unobserved) ∈{0, 0.25, 0.5, 0.75}
and
|X| = |Y| = k ∈
n
1, 2, 5, ⌊
√
n⌋, ⌊0.1n⌋
o
.
To select those sets, we proceed as follows: Initially mark all variables in V as observed. Next, for every node mark
it as unobserved with probability P(unobserved) until all nodes are considered or the number of nodes which remain
observed reaches the threshold value 2k. Finally, from the observed R pick randomly two disjoint subsets X and Y of
size k. The expected size of R is bounded by E[|R|] > n · (1 −P(unobserved)), with the difference being very small
for n ≫2k, but substantial for n ⪆2k. For example for n = 10 and k = |X| = |Y| = 5, all nodes are in R = X ∪Y = V
regardless of the probability P(unobserved) – the case discussed already in Section 7.3.
We perform experiments for each parametrization tuple
(n, l, k, P(unobserved)),
(5)
where, recall, l determines the probability P(edge) as described in Section 4. In this section we will report our results
in detail only for P(unobserved) ∈{0, 0.75}. The remaining cases are shown in the appendix.
We generated 10 000 graphs for each parameter tuple using the function GraphGenerator.randomDAG of
our DAGitty library [34] in node.js. Figure 11 shows example instances sampled for n = 10 and illustrates the four
cases we are interested in.
29

8.2
Algorithms
The main goal of our experiments was to examine the inﬂuence of the instance complexity, like density of a DAG,
numbers of exposures and outcomes, and the ratio of unobserved to observed variables, on the identiﬁability by
adjustment compared to general identiﬁability. Throughout, we use the following abbreviations for the algorithms we
examine:
CBC: our constructive back-door criterion (Deﬁnition 4, Theorem 2). We used our DAGitty library, speciﬁcally the
function GraphAnalyzer.canonicalAdjustmentSet, which implements algorithm FINDADJ based
on the CBC. We also tested another implementation of our CBC criterion, the gac function of the R package
pcalg [35].
CBC+: combination of the CBC and plain formula (Proposition 11). We implement the plain formula using the
DAGitty function GraphAnalyzer.dConnected, which implements algorithm TESTSEP (Proposition 1).
BC: Pearl’s back-door criterion (Deﬁnition 6). It has been shown that if an adjustment set Z that satisﬁes BC exists,
it can be found by removing all descendants of X from Z [11]. This means we can implement BC by trivial
post-processing of the CBC output.
IDC: general identiﬁability as determined by do-calculus (see [1, Chapter 3.4.2]). Speciﬁcally, we use the IDC al-
gorithm by Shpitser and Pearl [22], which is well known to be complete for the identiﬁcation of causal effects
[20, 21], meaning that the algorithm computes a formula involving only the pre-intervention distribution that
expresses the causal effect if such a formula exists; otherwise it outputs that identiﬁcation is impossible. Our
experiments are based on the IDC implementation provided by the R package causaleffect [33]. Due to
its high time complexity, we were only able to use this algorithm for small instances.
8.3
Results
The results for all methods and parameters n, k, l described above are shown in Table 5 (for the case P(unobserved) = 0)
and in Table 6 (P(unobserved) = 0.75). We now discuss the results in more detail.
Identiﬁcation by adjustment sets or plain formulas
Tables 5 and 6 provide counts for instances identiﬁable by
adjustment alone (columns CBC) or by adjustment enhanced by using the plain formula (CBC+). The number of
effects only identiﬁed by the plain formula, but not by CBC, is thus given by the difference between these columns.
Figure 12 summarizes the counts for CBC and CBC+ reported in Table 5 and 6 for k = 1, 2, 5 and n ≥25. We
omit the instances with n = 10, since for k = 5 these cases were discussed separately in Section 7.3. Moreover, for
parameter values l = 10 and l = 20 the individual probabilities for edge selection, P(edge) = max{l/(n −1), 1}, imply
that every node has 9 < l neighbors while in our analyses we want that l speciﬁes the expected number of neighbors
of a node.
Identiﬁcation by plain formula and identiﬁcation by adjustment are overlapping concepts. Some cases can be
identiﬁed using either approach, while in other instances only one of them works. Many cases for which adjustment
does not work can be solved instead by the plain formula, meaning that in those DAGs there is in fact no causal effect
of X on Y. This can be seen especially in dense graphs, e.g., DAGs in which each node has l = 20 neighbors on
average, and for singleton X and Y, i.e., k = 1. For example, for l = 20, k = 1, P(unobserved) = 0.75, in DAGs with
n = 2000 (n = 1000, n = 500) nodes, up to 65 % (63 %, 61 %) of all instances are identiﬁable by the plain formula
but not by adjustment. Furthermore, increasing n from 25 to 2000 we observe that this percentage ranges between
51% and the maximum 65%, a rather narrow range. The counts for CBC and CBC+ are illustrated in Figure 12 as
gray squares in the columns labeled as (l, k) = (20, 1) (case: P(unobserved) = 0.75).
The difﬁculty of identiﬁcation by adjustment grows with increasing k and l, but it decreases with increasing number
of nodes n, both for P(unobserved) = 0 and for 0.75. In Figure 12, columns are sorted increasingly by the total number
of identiﬁable effects per column. This shows that the most difﬁcult case is (l, k) = (20, 5): for P(unobserved) = 0
the counts grows very slowly with n reaching the maximum value of 3% of identiﬁable graphs for n = 2000; for
P(unobserved) = 0.75 almost no instances are identiﬁable by adjustment (compare the upper panels in Figure 12).
30

l = 2
l = 5
l = 10
l = 20
n
k
BC
CBC
CBC+
BC
CBC
CBC+
BC
CBC
CBC+
BC
CBC
CBC+
10
1
8893
8893
10000
7205
7205
10000
5034
5034
10000
4934
4934
10000
10
2
5543
6061
8618
1033
1980
4322
0
660
2197
0
686
2417
10
3
2359
3395
5817
57
548
1425
0
168
663
0
174
689
10
5
200
886
1712
0
108
216
0
36
71
0
31
65
25
1
9573
9573
10000
8936
8936
10000
7905
7905
10000
5843
5843
10000
25
2
8117
8247
9651
4243
4735
7003
1033
1553
3674
70
401
2118
25
3
6013
6424
8520
1203
1852
3524
46
212
1046
0
34
587
25
5
2298
3021
5055
39
243
646
0
11
93
0
1
53
50
1
9832
9832
10000
9476
9476
10000
8997
8997
10000
7832
7832
10000
50
2
9095
9128
9882
6688
6938
8388
2657
3049
4927
527
866
2729
50
5
5104
5535
7489
462
799
1613
3
16
198
0
2
58
50
7
2473
3120
4799
27
119
302
0
1
25
0
0
6
100
1
9907
9907
10000
9783
9783
10000
9494
9494
10000
8966
8966
10000
100
2
9585
9593
9971
8262
8353
9165
4600
4834
6162
1507
1762
3492
100
5
7425
7591
9090
1932
2336
3441
43
102
393
1
4
92
100
10
2499
3040
4479
15
48
137
0
0
2
0
0
0
250
1
9947
9947
10000
9894
9894
10000
9774
9774
10000
9621
9621
10000
250
2
9835
9840
9991
9284
9327
9696
6569
6689
7358
3151
3285
4502
250
5
8956
8994
9807
5051
5325
6261
469
544
994
7
17
164
250
15
3102
3537
4864
18
32
58
0
0
1
0
0
0
250
25
319
536
731
0
0
0
0
0
0
0
0
0
500
1
9977
9977
10000
9946
9946
10000
9883
9883
10000
9799
9799
10000
500
2
9923
9923
9996
9674
9684
9872
7704
7750
8116
4184
4266
4988
500
5
9477
9490
9948
7249
7368
8069
1170
1265
1686
43
48
216
500
22
3012
3288
4413
3
14
17
0
0
0
0
0
0
500
50
10
29
31
0
0
0
0
0
0
0
0
0
1000
1
9990
9990
10000
9973
9973
10000
9942
9942
10000
9885
9885
10000
1000
2
9965
9966
10000
9844
9845
9952
8416
8434
8640
5130
5173
5577
1000
5
9734
9736
9986
8679
8726
9173
2310
2396
2686
136
149
319
1000
32
2923
3191
4163
2
6
7
0
0
0
0
0
0
1000
100
0
0
0
0
0
0
0
0
0
0
0
0
2000
1
9999
9999
10000
9988
9988
10000
9972
9972
10000
9938
9938
10000
2000
2
9973
9973
10000
9940
9940
9981
9023
9029
9119
5928
5954
6156
2000
5
9880
9880
9996
9450
9471
9713
3608
3648
3869
287
300
469
2000
45
3000
3210
4122
4
8
8
0
0
2
0
0
0
2000
200
0
0
0
0
0
0
0
0
0
0
0
0
Table 5: Numbers of instances for P(unobserved) = 0, i.e., all variables are observed, that are identiﬁable by use of
BC, CBC, CBC+ (as deﬁned in Section 8.2). Gray cells highlight where the CBC was able to identify at least 400
more graphs than the BC. Since all variables are observed, all instances are identiﬁable, thus IDC is not used in this
Table.
31

l = 2
l = 5
l = 10
l = 20
n
k
BC
CBC
CBC+
IDC
BC
CBC
CBC+
IDC
BC
CBC
CBC+
IDC
BC
CBC
CBC+
IDC
10
1
6333
6333
9604
9609
1935
1935
7475
7476
978
978
5944
5944
936
936
5877
5877
10
2
2008
2339
6889
8740
103
228
2854
4137
0
113
1721
2260
0
114
1752
2294
10
3
610
980
4193
8056
0
21
1061
1995
0
9
512
763
0
10
547
789
10
5
185
859
1756
10000
0
98
190
10000
0
43
76
10000
0
26
75
10000
25
1
8414
8414
9923
9930
3647
3647
8727
8742
1340
1340
6884
6888
557
557
5696
5696
25
2
5164
5331
8939
9731
601
728
4630
6417
77
130
2501
3469
4
41
1847
2299
25
3
2350
2632
6958
9270
73
144
2141
4327
2
17
872
1518
0
6
554
780
25
5
277
449
3008
8157
0
1
456
1462
0
0
114
251
0
0
49
71
50
1
9082
9082
9975
9979
4651
4651
9237
9253
1699
1699
7547
7555
697
697
6031
6032
50
2
6985
7059
9599
9908
1098
1189
6078
7686
133
160
3353
4270
23
40
2061
2543
50
5
1440
1663
5452
9394
5
16
868
3030
0
1
178
482
0
0
73
125
50
7
254
388
2596
8648
0
0
186
1226
0
0
19
80
0
0
3
10
100
1
9527
9527
9992
9993
5585
5585
9602
9618
1985
1985
7980
7991
744
744
6414
6416
100
2
8295
8316
9884
9980
1846
1886
7303
8618
195
217
3799
4989
49
56
2413
2940
100
5
3391
3562
7636
9804
20
30
1800
4690
0
0
331
802
0
0
84
159
100
10
252
375
2364
9263
0
0
74
956
0
0
3
15
0
0
0
0
250
1
9791
9791
10000
10000
6832
6832
9814
9827
2493
2493
8564
8579
846
846
6793
6795
250
2
9205
9209
9974
9990
3099
3138
8509
9360
277
286
4914
6073
46
50
2881
3439
250
5
6110
6182
9269
9962
106
123
3344
6764
1
1
599
1323
0
0
136
248
250
15
232
306
2281
9697
0
0
16
703
0
0
0
4
0
0
0
0
250
25
3
4
221
9008
0
0
0
28
0
0
0
0
0
0
0
0
500
1
9882
9882
9999
-
7646
7646
9919
-
2935
2935
8885
-
946
946
7184
-
500
2
9596
9598
9993
-
4267
4280
9117
-
401
406
5722
-
33
34
3166
-
500
5
7774
7801
9754
-
273
285
4973
-
1
1
990
-
0
0
226
-
500
22
150
184
1757
-
0
0
3
-
0
0
0
-
0
0
2
-
500
50
0
0
4
-
0
0
0
-
0
0
0
-
0
0
0
-
1000
1
9936
9936
10000
-
8394
8394
9970
-
3181
3181
9137
-
1061
1061
7422
-
1000
2
9789
9790
9999
-
5498
5507
9568
-
525
526
6361
-
51
51
3546
-
1000
5
8797
8803
9947
-
666
676
6482
-
2
2
1471
-
0
0
245
-
1000
32
94
107
1511
-
0
0
1
-
0
0
0
-
0
0
0
-
1000
100
0
0
0
-
0
0
0
-
0
0
0
-
0
0
0
-
2000
1
9975
9975
10000
-
8914
8914
9988
-
3685
3685
9361
-
1099
1099
7613
-
2000
2
9879
9879
9999
-
6774
6777
9791
-
714
714
7048
-
57
57
3858
-
2000
5
9383
9384
9980
-
1519
1535
7906
-
0
0
2159
-
0
0
342
-
2000
45
81
90
1399
-
0
0
0
-
0
0
0
-
0
0
0
-
2000
200
0
0
0
-
0
0
0
-
0
0
0
-
0
0
0
-
Table 6: Numbers of instances for P(unobserved) = 0.75 that are identiﬁable by use of BC, CBC, CBC+ and IDC (as
deﬁned in Section 8.2). Gray cells highlight where the CBC was able to identify at least 400 more graphs than the BC.
Due to its high time complexity, we were unable to run the IDC algorithm on instances labelled with “-”.
32

P(unobserved) = 0
l, k
20, 5
10, 5
20, 2
5, 5
10, 2
2, 5
5, 2
20, 1
10, 1
2, 2
5, 1
2, 1
CBC
number of nodes
25
50
100
250
500
1000
2000
P(unobserved) = 0.75
l, k
20, 5
10, 5
20, 2
10, 2
5, 5
20, 1
10, 1
5, 2
2, 5
5, 1
2, 2
2, 1
Legend:
50%
55%
60%
65%
70%
75%
80%
85%
90%
95%
100%
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
instance complexity
l, k
20, 5
10, 5
20, 2
5, 5
10, 2
2, 5
5, 2
2, 2
2, 1
5, 1
10, 1
20, 1
number of nodes
25
50
100
250
500
1000
2000
CBC+
instance complexity
l, k
20, 5
10, 5
20, 2
5, 5
10, 2
20, 1
5, 2
2, 5
10, 1
5, 1
2, 2
2, 1
Figure 12: Heatmaps visualizing the number of instances that are identiﬁable by use our constructive back-door
criterion (CBC, top row), and by the use of CBC or the plain formula (CBC+, bottom row). Black squares depict the
worst case in which 0% of instances are identiﬁable by use of CBC or CBC+, respectively. White squares mean that
100% of instances are identiﬁable. The instance complexities l, k (where the expected number of neighbors of a node
equals l and |X| = |Y| = k) are sorted by total amount of identiﬁable instances.
However, as we can see in Table 6, for n = 250 only 2.5% of cases are identiﬁable at all. Figures 13 and 14 summarize
the difﬁculty of identiﬁcation stratiﬁed by n (Figure 13) and l (Figure 14), respectively.
Comparison of CBC to the back-door criterion by Pearl
We were also interested in how often Pearl’s back-door
criterion (BC) would fail to ﬁnd an adjustment set. Tables 5 and 6 show that the difference between BC and the
CBC is rather small, especially for simple (or hard) instances where nearly every (or no) DAG has an adjustment set,
and as expected given our results in Section 5.2, for singletons X = {X}, Y = {Y} the counts for BC and CBC are
indeed equal. However, for larger X, Y and parameters where only a few graphs have an adjustment set, the difference
between BC and CBC becomes more substantial. The greatest difference occurs for n = 10, |X| = |Y| = 3, m ≅n, and
P(unobserved) = 0, where in 10% of all cases there is an adjustment set whereas BC ﬁnds none. This is followed by
n = 10, |X| = |Y| = 2, where BC fails to ﬁnd existing adjustment sets in 7% to 9% of the cases, depending on P(edge)
and P(unobserved).
Complete identiﬁcation by do-calculus compared to identiﬁcation by adjustment or plain formula
As ex-
plained above, in small graphs we checked for general identiﬁability of causal effects using the IDC algorithm [22].
Results are shown for P(unobserved) = 0.75 and n ≤250 in Table 6. Since the IDC algorithm is complete for the
identiﬁcation problem, the corresponding counts also show how many instances are identiﬁable at all. It is known that
in the case P(unobserved) = 0 the causal effect is always identiﬁable, so we skip the counts for IDC in Table 5.
The cases with n = 10, k = |X| = |Y| = 5 (Table 6) might seem suspicious as the number of identiﬁable graphs
(i.e., counts in column IDC) increases drastically compared to the graphs with smaller X, Y, while in all the other cases
(see Table 6) this number decreases with increasing cardinality of X, Y. However, this is explained by the cap on the
33

l, k
20, 5
10, 5
5, 5
2, 5
20, 2
10, 2
5, 2
2, 2
20, 1
10, 1
5, 1
2, 1
% of identiﬁed instances
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
n = 2000
n = 1000
n = 500
n = 250
n = 100
n = 50
n = 25
Figure 13:
Case P(unobserved
=
0.75):
Percent of identiﬁable graphs for ﬁxed numbers of nodes n
∈
{25, 50, 100, 250, 500, 1000, 2000} and with varying expected number of node neighbors l and cardinalities |X| =
|Y| = k. The horizontal axis is labeled by (l, k) sorted lexicographically. The curves show the data for CBC+, i.e.,
for instances identiﬁable by adjustment or by plain formula.
n, k
25, 5
50, 5
100, 5
250, 5
500, 5
1000, 5
2000, 5
25, 2
50, 2
100, 2
250, 2
500, 2
1000, 2
2000, 2
25, 1
50, 1
100, 1
250, 1
500, 1
1000, 1
2000, 1
% of identiﬁed instances
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
l = 2
l = 5
l = 10
l = 20
l = 2
l = 5
l = 10
l = 20
Figure 14: Case P(unobserved = 0.75): Percent of identiﬁable graphs for ﬁxed density parameter values l ∈
{2, 5, 10, 20} and with varying the number of nodes n and cardinalities |X| = |Y| = k. The horizontal axis is labeled
by (n, k) sorted lexicographically. The curves show the data for CBC+, i.e. for instances identiﬁable by adjustment or
by a plain formula; Crosses show data for IDC, i.e., they show how many cases are identiﬁable at all. The high time
complexity of the IDC algorithm precluded computations for graphs of sizes n ≥500.
34

l = 2
l = 5
l = 10
l = 20
n
k
CBC
IDC
GAC
CBC
IDC
GAC
CBC
IDC
GAC
CBC
IDC
GAC
10
1
0.3 ms
35.0 ms
13.6 ms
0.5 ms
53.6 ms
49.6 ms
0.6 ms
60.4 ms
303 ms
0.6 ms
59.9 ms
169 ms
10
2
0.5 ms
70.5 ms
17.9 ms
0.6 ms
100 ms
61.9 ms
0.8 ms
110 ms
173 ms
1.0 ms
110 ms
211 ms
10
3
0.5 ms
124 ms
14.1 ms
0.8 ms
182 ms
32.5 ms
1.1 ms
190 ms
121 ms
1.1 ms
187 ms
60.7 ms
10
5
0.7 ms
242 ms
15.3 ms
1.0 ms
296 ms
52.4 ms
1.6 ms
306 ms
170 ms
1.6 ms
305 ms
173 ms
25
1
0.5 ms
27.4 ms
72.1 ms
0.9 ms
46.9 ms
1.9 s
1.6 ms
78.7 ms
18.5 s
2.9 ms
89.7 ms
1.6 h
25
2
0.7 ms
54.9 ms
112 ms
1.3 ms
121 ms
680 ms
2.2 ms
193 ms
16.4 s
4.4 ms
206 ms
-
25
3
0.8 ms
70.3 ms
113 ms
1.5 ms
127 ms
695 ms
2.6 ms
168 ms
15.1 s
5.7 ms
168 ms
39.9 min
25
5
1.0 ms
132 ms
114 ms
1.7 ms
276 ms
522 ms
3.1 ms
309 ms
9.5 s
7.1 ms
305 ms
18.9 min
50
1
0.8 ms
25.9 ms
273 ms
1.6 ms
51.7 ms
-
2.8 ms
151 ms
-
5.4 ms
257 ms
-
50
2
1.1 ms
46.9 ms
557 ms
2.0 ms
157 ms
-
3.8 ms
475 ms
-
7.7 ms
691 ms
-
50
5
1.5 ms
142 ms
680 ms
2.8 ms
414 ms
-
5.6 ms
796 ms
-
14.2 ms
937 ms
-
50
7
1.7 ms
224 ms
744 ms
3.3 ms
649 ms
-
6.7 ms
1.1 s
-
17.9 ms
1.1 s
-
100
1
1.3 ms
24.5 ms
978 ms
2.5 ms
53.1 ms
-
4.8 ms
366 ms
-
9.6 ms
914 ms
-
100
2
1.6 ms
32.4 ms
1.7 s
3.0 ms
128 ms
-
6.1 ms
836 ms
-
13.6 ms
1.8 s
-
100
5
2.0 ms
91.3 ms
3.1 s
4.1 ms
440 ms
-
9.2 ms
1.7 s
-
25.3 ms
2.7 s
-
100
10
2.6 ms
248 ms
4.1 s
5.6 ms
843 ms
-
13.8 ms
2.4 s
-
47.5 ms
3.0 s
-
250
1
2.9 ms
25.9 ms
6.7 s
5.1 ms
74.1 ms
-
10.2 ms
966 ms
-
22.2 ms
5.1 s
-
250
2
3.1 ms
28.2 ms
9.2 s
6.0 ms
130 ms
-
12.9 ms
2.6 s
-
30.6 ms
12.0 s
-
250
5
3.7 ms
64.6 ms
21.9 s
7.7 ms
499 ms
-
19.1 ms
6.2 s
-
56.8 ms
20.7 s
-
250
15
5.3 ms
462 ms
48.5 s
13.3 ms
2.8 s
-
38.9 ms
19.3 s
-
144 ms
44.1 s
-
250
25
6.4 ms
644 ms
1.1 min
17.4 ms
3.2 s
-
57.0 ms
12.9 s
-
252 ms
22.9 s
-
500
1
5.2 ms
-
-
8.7 ms
-
-
18.1 ms
-
-
42.1 ms
-
-
500
2
5.5 ms
-
-
10.1 ms
-
-
22.7 ms
-
-
58.1 ms
-
-
500
5
6.2 ms
-
-
12.3 ms
-
-
33.8 ms
-
-
106 ms
-
-
500
22
9.0 ms
-
-
24.5 ms
-
-
89.5 ms
-
-
391 ms
-
-
500
50
13.4 ms
-
-
46.3 ms
-
-
192 ms
-
-
886 ms
-
-
1000
1
11.1 ms
-
-
17.2 ms
-
-
35.4 ms
-
-
83.2 ms
-
-
1000
2
11.8 ms
-
-
19.0 ms
-
-
42.6 ms
-
-
114 ms
-
-
1000
5
12.4 ms
-
-
21.8 ms
-
-
61.8 ms
-
-
205 ms
-
-
1000
32
17.1 ms
-
-
51.3 ms
-
-
225 ms
-
-
1.1 s
-
-
1000
100
33.5 ms
-
-
143 ms
-
-
715 ms
-
-
3.4 s
-
-
2000
1
24.6 ms
-
-
35.1 ms
-
-
70.8 ms
-
-
178 ms
-
-
2000
2
25.1 ms
-
-
37.7 ms
-
-
83.6 ms
-
-
241 ms
-
-
2000
5
26.0 ms
-
-
41.8 ms
-
-
120 ms
-
-
409 ms
-
-
2000
45
34.5 ms
-
-
107 ms
-
-
549 ms
-
-
2.8 s
-
-
2000
200
92.3 ms
-
-
505 ms
-
-
2.7 s
-
-
13.4 s
-
-
Table 7: Average time to run DAGitty (CBC), the R package causaleffect (IDC) or the gac function of the R package
pcalg (GAC) on one graph with P(unobserved = 0.75). Omitted values indicate experiments we did not run or were
unable to run due to runtime issues. On some parametrizations we only ran the gac function on the ﬁrst 100 graphs
rather than the full set of 10000 graphs due to time constraints.
number of unobserved nodes. When |X|+|Y| = 10 for n = 10, there are no nodes outside of X∪Y remaining that could
become unobserved regardless of P(unobserved), similarly to the cases in Table 5, and all graphs must be identiﬁable
as shown in Section 7.3.
Figures 13 and 14 present the data for CBC+ (the same data as in the lower right panel of Figure 12) in comparison
to identiﬁcation by IDC. As we observed in Figure 12, the most difﬁcult case for CBC+ is (l, k) = (20, 5) and the
difﬁculty decreases with k and l when n is ﬁxed (Figure 13). The situation is very similar for IDC. In Figure 14, we
see that identiﬁability for both CBC+ and IDC grows roughly in parallel. Similarly to the results for adjustment sets,
one can see that with increasing |X|, |Y|, l the number of identiﬁable graphs decreases, when P(unobserved) > 0.
These experiments also provide a veriﬁcation of our DAGitty implementation as every adjustment set found by the
causaleffect package has been found by DAGitty, as well as a ground truth of the unidentiﬁable graphs, since
a causal effect not identiﬁed by the IDC algorithm cannot be identiﬁed by any method.
Moreover, as expected, for
DAGs with no unobserved variables and with |X| = |Y| = 1, all causal effects are already identiﬁed by a plain formula
or adjustment without involving the IDC algorithm (see Table 5).
Comparative runtimes of the algorithms
Figure 15 (black lines) shows the time needed by DAGitty for these ex-
periments on one core of a 2.1 GHz (up 3 GHz with Turbo Core) AMD Opteron 6272 for graphs with P(unobserved) =
0.75. Graphs with a lower probability P(unobserved) are processed slightly faster. For small sets X and Y the time
increases roughly linearly with the number of edges m. For larger sets the time also increases with the size of X, Y,
which could either mean that DAGitty does not reach the optimal asymptotic runtime of O(n + m) due to inefﬁcient
35

number of nodes
10
25
50
100
250
500
1000
2000
average time
≈
0.3 ms
0.7 ms
1.6 ms
3.6 ms
8.0 ms
17.8 ms
39.6 ms
88.2 ms
196 ms
437 ms
973 ms
2.2 s
4.8 s
10.7 s
23.9 s
2.9 m
6.5 m
14.6 m
32.4 m
1.2 h
2.7 h
expected number of edges
9
17
36
73
148
303
619
1265
2584
5279
10783
22026
44994
average time
≈
0.3 ms
0.7 ms
1.6 ms
3.6 ms
8.0 ms
17.8 ms
39.6 ms
88.2 ms
196 ms
437 ms
973 ms
2.2 s
4.8 s
10.7 s
23.9 s
2.9 m
6.5 m
14.6 m
32.4 m
1.2 h
2.7 h
DAGs of E[m] = 1n edges and |X| = |Y| ∈{1, 2, 5}
DAGs of E[m] = 10n edges and |X| = |Y| ∈{1, 2, 5}
black: DAGitty package;
red: causaleffect package;
purple: pcalg package;
Figure 15: Average time needed to ﬁnd an adjustment set and verify it according to the CBC in a single graph with
P(unobserved) = 0.75, values n ∈{10, 25, 50, 100, 250, 500, 1000, 2000}, E[m] ∈{1n, 10n}, and various cardinalities
k of X, Y. The left and the right plot show the same data, but with a different metric on the horizontal axis: the number
of nodes n (left) and the expected number of edges m (right). In black we show the statistics for DAGitty, in red–
the data for the R package causaleffect and in purple– the data for the gac function of the R package pcalg
R. Error bars show the minimum and maximum time taken. The plot shows that all small graphs (n ≤100) are solved
nearly instantaneously (time ≤100ms) by DAGitty. Only graphs with a high number of edges and huge X, Y can
require a few seconds. Thus DAGitty is one to two magnitudes faster than the causaleffect package or the
pcalg package.
36

set operations, or that the time actually only depends on O(An(X, Y)) which can be much smaller than O(m) when the
sets and degrees are small. However, for all models of a size currently used in practice, DAGitty ﬁnds the adjustment
set nearly instantaneously.
The runtimes of the causaleffect package are shown as red plot in Figure 15. Since the IDC algorithm is far
more complex than the expression of Theorem 2, it performs generally one to two orders of magnitude slower than the
implementation in DAGitty, or equivalently in the same time DAGitty can process graphs that are one to two orders of
magnitude larger. Due to this speed difference it was not possible for us to run the IDC algorithm experiments on the
larger graphs.
We have also investigated a different implementation of the CBC in the R package pcalg [35]. The gac function in
that package implements the CBC criterion for DAGs and other graph classes. Unlike DAGitty, the pcalg package does
not ﬁnd an adjustment set, but only veriﬁes whether a given set meets the criterion. Hence, after loading the graphs in
R and calculating the adjacency matrices required by pcalg, we compute the canonical adjustment set Adjustment(X, Y)
in R as
Dpcp = De(G, intersect(setminus(De(GNoInX, x), x), An(GNoOutX, y)))
z = setminus(An(G, union(x,y)), union(union(x,y),union(Dpcp, obs)))
with sets x, y, observed nodes obs, graphs G = G, GNoInX = GX, GNoOutX = GX and helper functions An and
De implemented using the subcomponent function of the R package igraph. We then compare the time required
by pcalg to test whether z is a valid adjustment set to the time required by DAGitty to ﬁnd and test a set. The
runtimes of the gac function are plotted in purple in Figure 15. They show that the gac function is several orders of
magnitude slower than DAGitty. These results are expected given that the pcalg package tests the CBC by tracing
all m-connected paths using backtracking, an approach that suffers from exponential worst-case complexity; in fact
this backtracking algorithm is even slower than the general implementation of the do-calculus in the causaleffect
package. Only the cases with small n are shown as the remaining computations did not terminate in reasonable time.
In summary, our experimental results show that many causal effects in random DAGs cannot be identiﬁed by
covariate adjustment. Nevertheless, many of these cases are easily addressed by extending the CBC slightly, and then
most effects become identiﬁable without having to resort to do-calculus, at least in the random graphs we tested. This
ﬁnding is reassuring given that the implementation of our algorithmic framework in DAGitty is the only identiﬁcation
method of those we tested that is applicable to large graphs.
9
Adjustment in MAGs
Finally, in this section, we generalize our complete constructive criterion for identifying adjustment sets from DAGs
to MAGs, making our algorithmic framework applicable to this class of graphical models as well. Two examples
may illustrate why this generalization is not trivial. First, take G = X →Y. If G is interpreted as a DAG, then the
empty set is valid for adjustment. If G is however taken as a MAG, then there exists no adjustment set (for a formal
deﬁnition of an adjustment set in a MAG see below) as G represents among others the DAG U
X
Y where U is
an unobserved confounder. Second, take G = A →X →Y. In that case, the empty set is an adjustment set regardless
of whether G is interpreted as a DAG or a MAG. The reasons will become clear as we move on. First, let us recall
the semantics of a MAG. The following deﬁnition can easily be given for AGs in general, but we do not need this
generality for our purpose.
Deﬁnition 7 (DAG representation by MAGs [9]). Let G = (V, E) be a DAG, and let S, L ⊆V. The MAG M = G[S
L
is a graph with nodes V \ (S ∪L) and edges deﬁned as follows. (1) Two nodes U and V are adjacent in G[S
L if they
cannot be m-separated by any Z with S ⊆Z ⊆V \ L in G. (2) The edge between U and V is
U −V if U ∈An(S ∪V) and V ∈An(S ∪U);
U →V if U ∈An(S ∪V) and V < An(S ∪U);
U ↔V if U < An(S ∪V) and V < An(S ∪U).
We call L latent variables and S selection variables. We say there is selection bias if S , ∅.
37

Hence, every MAG represents an inﬁnite set of underlying DAGs that all share the same ancestral relationships.
Lemma 6 (Preservation of separating sets [9]). Set Z m-separates X, Y in G[S
L if and only if Z ∪S m-separates X, Y
in G.
Selection bias (i.e., S , ∅) substantially complicates adjustment, and in fact nonparametric causal inference in
general [18]1. Due to these limitations, we restrict ourselves to the case S = ∅in the rest of this section. Note however
that recovery from selection bias is sometimes possible with additional population data, and graphical conditions exist
to identify such cases [36].
We now extend the concept of adjustment to MAGs in the usual way [17].
Deﬁnition 8 (Adjustment in MAGs). Given a MAG M = (V, E) and two variable sets X, Y ⊆V, Z ⊆V is an
adjustment set for (X, Y) in M if for all DAGs G = (V′, E′) for which G[∅
L = M with L = V′ \ V the set Z is an
adjustment set for (X, Y) in G.
This deﬁnition is equivalent to requiring that P(y | do(x)) is equal to P
z P(y | x, z)P(z) for every probability
distribution P(v′) consistent with a DAG G = (V′, E′) for which G[∅
L = M with L = V′ \ V. If one was to extend the
deﬁnition to include selection bias S, one would need to give a requirement that holds for all DAGs G = (V′, E′) with
G[S
L = M and L ∪S = V′ \ V. Thereby one can deﬁne P(y | do(x)) as P
z P(y | x, z, s)P(z | s) , P
z P(y | x, z, s)P(z)
or P
s
P
z P(y | x, z, s)P(z, s). The last deﬁnition is equivalent to Z ∪S being an adjustment set in all these DAGs, but
existing literature has used the second case[36]. However, the ﬁrst case captures the spirit of selection bias the most,
since in the presence of selection bias the probability distribution is only known given some selected bias s.
Notice that, due to the deﬁnition of adjustment in MAGs (Def. 8), in our considerations we can restrict MAGs to
mixed graphs consisting of only directed and bidirected edges.
9.1
Adjustment amenability
In this section we ﬁrst identify a class of MAGs in which adjustment is impossible because of causal ambiguities –
e.g., the simple MAG X →Y falls into this class, but the larger MAG A →X →Y does not.
Deﬁnition 9 (Visible edge [18]). Given a MAG M = (V, E), an edge X →D in E is called visible if in all DAGs
G = (V′, E′) with G[∅
L= M for some L ⊆V′, all d-connected walks between X and D in G that contain only nodes of
L ∪X ∪D are directed paths. Otherwise X →D is said to be invisible.
Intuitively, an invisible directed edge X →D means that there may exist hidden confounding factors between X
and D, which is guaranteed not to be the case if the edge is visible.
Lemma 7 (Graphical conditions for edge visibility [18]). In a MAG M = (V, E), an edge X →D in E is visible if
and only if there is a node A not adjacent to D where (1) A →X ∈E or A ↔X ∈E, or (2) there is a collider path
A ↔V1 ↔. . . ↔Vn ↔X or A →V1 ↔. . . ↔Vn ↔X where all Vi are parents of D.
Deﬁnition 10. We call a MAG M = (V, E) adjustment amenable w.r.t. X, Y ⊆V if all proper causal paths from X to
Y start with a visible directed edge.
Lemma 8. If a MAG M = (V, E) is not adjustment amenable w.r.t. X, Y ⊆V then there exists no valid adjustment set
for (X, Y) in M.
Proof. If the ﬁrst edge X →D on some causal path to Y in M is not visible, then there exists a consistent DAG G
where there is a non-causal path between X and Y via D that could only be blocked in M by conditioning on D or
some of its descendants. But such conditioning would violate the adjustment criterion in G.
□
1 A counterexample is the graph A ←X →Y, where we can safely assume that A is the ancestor of a selection variable. A sufﬁcient and
necessary condition to recover a distribution P(y | x) from a distribution P(y | x, s) under selection bias is Y ⊥⊥S | X [36], which is so restrictive
that most statisticians would probably not even speak of “selection bias” anymore in such a case.
38

Note that adjustment amenability does not yet guarantee the existence of an adjustment set; the smallest example
is the MAG X ←Y, which is amenable but admits no adjustment set.
Let N(V) denote all nodes adjacent to V, and Sp(V) denote all spouses of V, i.e., nodes W such that W ↔V ∈E.
The adjustment amenability of a graph M w.r.t sets X, Y can be tested with the following algorithm:
function TESTADJUSTMENTAMENABILITY(M, X, Y)
for all D in Ch(X) ∩PCP(X, Y) do
C ←∅; A ←∅
function CHECK(V)
if C[V] then return A[V]
C[V] ←true
A[V] ←((Pa(V) ∪Sp(V)) \ N(D) , ∅)
for all W ∈Sp(V) ∩Pa(D) do
if CHECK(W) then A[V] ←true
return A[V]
for all X in X ∩Pa(D) do
if ¬CHECK(X) then return false
Analysis of the Algorithm. The algorithm checks for visibility of every edge X →D by trying to ﬁnd a node Z
not connected to D but connected to X via a collider path through the parents of D, according to the conditions of
Lemma 7; note that condition (1) of Lemma 7 is identical to condition (2) with an empty collider path. Since CHECK
performs a depth-ﬁrst-search by checking every node only once and then continuing to its neighbors, each iteration of
the outer for-loop in the algorithm runs in linear time O(n + m). Therefore, the entire algorithm runs in O(k(n + m))
where k ≤|Ch(X)|.
□
9.2
Adjustment criterion for MAGs
We now show that the adjustment criterion for DAGs generalizes to adjustment amenable MAGs. The adjustment cri-
terion and the constructive back-door criterion are deﬁned like their DAG counterparts (Deﬁnitions 2 and 3), replacing
“DAG” with “MAG” and d- with m-separation for the latter.
Deﬁnition 11 (Adjustment criterion). Let M = (V, E) be a MAG, and X, Y, Z ⊆V be pairwise disjoint subsets of
variables. The set Z satisﬁes the adjustment criterion relative to (X, Y) in M if
(a) no element in Z is a descendant in M of any W ∈V \ X which lies on a proper causal path from X to Y and
(b) all proper non-causal paths in M from X to Y are blocked by Z.
Note that the above deﬁnition uses “descendants in M” instead “descendants in MX” as Deﬁnition 2. However,
Lemma 4 implies that the conditions are equivalent.
Deﬁnition 12 (Proper back-door graph). Let M = (V, E) be a MAG, and X, Y ⊆V be pairwise disjoint subsets of
variables. The proper back-door graph, denoted as Gpbd
XY, is obtained from M by removing the ﬁrst edge of every proper
causal path from X to Y.
Deﬁnition 13 (Constructive back-door criterion (CBC)). Let M = (V, E) be a MAG, and let X, Y, Z ⊆V be pairwise
disjoint subsets of variables. The set Z satisﬁes the constructive back-door criterion relative to (X, Y) in M if
(a) Z ⊆V \ Dpcp(X, Y) and
(b) Z m-separates X and Y in the proper back-door graph Gpbd
XY.
The main result of this section (Theorem 3) shows that for any adjustment amenable MAG and node sets X and
Y, a set Z is an adjustment relative to (X, Y) if and only if the CBC is satisﬁed. Figure 16 shows some examples.
Similarly to DAGs, we provide a generalization of the CBC for MAGs allowing parametrization of the criterion:
39

M1:
X
V
Y
Z
M2:
X
V
Y
Z
M3:
X
V
Y
Z
M4:
X
V
Y
W
M5:
X2
X1
V
Y
Z
Figure 16: Five MAGs in which we search for an adjustment relative to (X, Y) or ({X1, X2}, Y). M1 and M3 are not
adjustment amenable, since the edge X →V is not visible, so no adjustment exists. In the other three MAGs the edge
is visible, due to the node Z in M2, the node W in M4 and the node X2 in M5. The only valid adjustment in M2 and
M5 is {Z}, and in M4 only the empty set is a valid adjustment. If M1 and M3 were DAGs, the set {Z} would be an
adjustment in each.
Deﬁnition 14 (Parametrization of the Constructive back-door criterion (CBC(A, B, C))). Let M = (V, E) be a MAG,
and let X, Y, Z ⊆V be pairwise disjoint subsets of variables. Let A ⊆X ∪Y, B ⊆X, C ⊆De(PCP(X, Y)). The set Z
satisﬁes the CBC(A, B, C) relative to (X, Y) in M if
(a) Z ⊆V \ DeAB(PCP(X, Y)), and
(b) Z d-separates X and Y in the graph Gpbd,C
XY
:= (V, E \ (X →(PCP(X, Y) ∪C))).
With these deﬁnitions we are ready to give:
Theorem 3. Given an adjustment amenable MAG M = (V, E) and three disjoint node sets X, Y, Z ⊆V, the following
statements are equivalent:
(i) Z is an adjustment relative to (X, Y) in M.
(ii) Z fulﬁlls the adjustment criterion (AC) w.r.t. (X, Y) in M.
(iii) Z fulﬁlls the constructive back-door criterion (CBC) w.r.t. (X, Y) in M.
(iv) Z fulﬁlls a variant of constructive back-door criterion (CBC(A,B,C)) w.r.t. (X, Y) in M for A ⊆X ∪Y, B ⊆X,
C ⊆De(PCP(X, Y)).
Before proving the theorem, let us recall the concept of an inducing path, which we will use in our proof and the
further analysis.
Deﬁnition 15 (Inducing path [9]). Let G = (V, E) be a DAG and Z, L ⊆V be disjoint. A path π = V1, . . ., Vn+1 in G
is called inducing with respect to Z and L if all non-colliders on π except V1 and Vn+1 are in L and all colliders on π
are in An({V1, Vn+1} ∪Z).
We will use also the following notion:
Deﬁnition 16 (Inducing Z-trail). Let G = (V, E) be a DAG and Z, L ⊆V be disjoint. Let π = V1, . . . , Vn+1 be a path
in G[∅
L such that V2, . . . , Vn ∈Z, V1, Vn+1 < Z, for each i ∈{1, . . ., n}, there is an inducing path w.r.t. ∅, L linking
Vi, Vi+1, and for each i ∈{2, . . ., n}, these inducing paths have arrowheads at Vi. Then π is called an inducing Z-trail.
Proof of Theorem 3. The equivalence of (ii), (iii) and (iv) is established by observing that the proofs of Theorem 1 and
Lemma 4 generalize to m-separation. Below we establish equivalence of (i) and (ii).
¬(ii) ⇒¬(i): If Z violates the adjustment criterion in M, it does so in the canonical DAG C(M), and thus is not
an adjustment in M.
¬(i) ⇒¬(ii): In the proof we rely on properties from Lemmas 10, 12, and 13 presented in Subsection 9.5.
Let G be a DAG, with G[∅
L= M, in which Z violates the AC. We show that (a) if Z ∩Dpcp(X, Y) , ∅in G then
Z ∩Dpcp(X, Y) , ∅in M as well, or there exists a proper non-causal path in M that cannot be m-separated; and (b)
40

DAG G:
X
W1
W2
Y
Z
MAG M = G[∅
W1:
X
W2
Y
Z
Figure 17: Illustration of the case in the proof of Theorem 3 where Z descends from W1 which in a DAG G is on a
proper causal path from X to Y, but is not a descendant of a node on a proper causal path from X to Y in the MAG M
after marginalizing W1. In such cases, conditioning on Z will m-connect X and Y in M via a proper non-causal path.
if Z ∩Dpcp(X, Y) = ∅in G and Z d-connects a proper non-causal path in G, then it m-connects a proper non-causal
path in M.
(a) Suppose that in G, Z contains a node Z in Dpcp(X, Y), and let W = PCP(X, Y) ∩An(Z). If M still contains at
least one node W1 ∈W, then W1 lies on a proper causal path in M and Z is a descendant of W1 in M. Otherwise,
M must contain a node W2 ∈PCPG(X, Y) \ An(Z) (possibly W2 ∈Y) such that W2 ↔A, X →W2, and X →A are
edges in M, where A ∈An(Z) (possibly A = Z; see Figure 17). Then M contains an m-connected proper non-causal
path X →A ↔W2 →. . . →Y.
(b) Suppose that in G, Z∩Dpcp(X, Y) = ∅, and there exists an open proper non-causal path from X to Y. Then there
must also be a proper non-causal walk wG from some X ∈X to some Y ∈Y (Lemma 10), which is d-connected by Z in
G. Let wM denote the subsequence of wG formed by nodes in M. It includes all colliders on wG, because Z ∩L = ∅.
The sequence wM is a walk in M, but is not necessarily m-connected by Z; all colliders on wM are in Z because every
non-Z must be a parent of at least one of its neighbors, but there might be subsequences U, Z1, . . ., Zk, V on wM where
all Zi ∈Z but some of the Zi are not colliders on wM. However, then we can form from wM an m-connected walk by
bypassing some sequences of Z-nodes (Lemma 12). Let w′
M be the resulting walk.
If w′
M is a proper non-causal walk, then there must also exist a proper non-causal path in M (Lemma 10), violating
the AC. It therefore remains to show that w′
M is not a proper causal path. This must be the case if wG does not contain
colliders, because then the ﬁrst edge of wM = w′
M cannot be a visible directed edge out of X. Otherwise, the only
way for w′
M to be proper causal is if all Z-nodes in wM have been bypassed in w′
M by edges pointing away from X. In
that case, one can show by several case distinctions that the ﬁrst edge X →D of w′
M, where D < Z, cannot be visible
(see Figure 18 for an example of such a case).
DAG G:
L1
Z
Y
L2
X
A
MAG M = G[∅
{L1,L2}:
Z
Y
X
A
Figure 18: Case (b) in the proof of Theorem 3: A proper non-causal path wG = X ←L1 →Z ←L2 →Y in a DAG is
d-connected by Z, but the corresponding proper non-causal path wM = X ←Z →Y is not m-connected in the MAG,
and its m-connected subpath w′
M = X →Y is proper causal. However, this also renders the edge X →Y invisible,
because otherwise A could be m-separated from Y by U = {X, Z} in M but not in G.
For simplicity, assume that M contains a subpath A →X →D where A is not adjacent to D; the other cases of
edge visibility like A ↔X →D (Lemma 7) are treated analogously. In G, there are inducing paths (possibly several
πAX from A to X and πXD from X to D w.r.t ∅, L; πAX must have an arrowhead at X. We distinguish several cases on
the shape of πXD. (1) A path πXD has an arrowhead at X as well. Then A, D are adjacent (Lemma 13), a contradiction.
(2) No inducing path πXD has an arrowhead at X. Then wG must start with an arrow out of X, and must contain a
collider Z ∈De(X) because wG is not causal. (a) Z ∈De(D). This contradicts Z∩Dpcp(X, Y) = ∅. So (b) Z < De(D).
Then by construction of w′
M (Lemma 12), wM must start with an inducing Z-trail X →Z, Z1, . . ., Zn, D, which is also
an inducing path from X to D in G w.r.t. ∅, L. Then Z, Z1, . . . , Zn, D must also be an inducing path in G w.r.t. ∅, L
because An(X) ⊆An(Z). Hence Z and D are adjacent. We distinguish cases on the path X →Z, D in M. Now we can
41

DAG G and MAG G[∅
∅:
V3
V2
V8
V9
V7
V6
V4
V1
Y1
X1
MAG G[∅
L:
V2
V8
V7
V1
Y1
X1
Figure 19:
Example of a DAG and resulting MAG sampled for the parameter tuple n
=
10, P(edge)
=
2/9, P(unobserved) = 0.75, and k = |X| = |Y| = 1. Nodes are relabeled such that exposures are called X1 and
outcomes are called Y1. The nodes L = {V3, V4, V6, V9} are unobserved. {V1} is an adjustment set in G, G[∅
∅and G[∅
L.
conclude: If X →Z →D, then Z lies on a proper causal path, contradicting Z ∩Dpcp(X, Y) = ∅; If X →Z ↔D, or
X →Z ←D, then we get an m-connected proper non-causal walk along Z and D.
□
9.3
Adjustment set construction
In the previous section, we have already shown that the CBC is equivalent to the AC for MAGs as well; hence,
adjustment sets for a given MAG M can be found by forming the proper back-door graph Mpbd
XY and then applying the
algorithms from the previous section. In principle, care must be taken when removing edges from MAGs as the result
might not be a MAG; however, this is not the case when removing only directed edges.
Lemma 9 (Closure of maximality under removal of directed edges). Given a MAG M, every graph M′ formed by
removing only directed edges from M is also a MAG.
Proof. Suppose the converse, i.e., M is no longer a MAG after removal of some edge X →D. Then X and D cannot
be m-separated even after the edge is removed because X and D are collider connected via a path whose nodes are all
ancestors of X or D [9]. The last edge on this path must be C ↔D or C ←D, hence C < An(D), and thus we must
have C ∈An(X). But then we get C ∈An(D) in M via the edge X →D, a contradiction.
□
Corollary 7. For every MAG M, the proper back-door graph Mpbd
XY is also a MAG.
For MAGs that are not adjustment amenable, the CBC might falsely indicate that an adjustment set exists even
though that set may not be valid for some represented graph. Fortunately, adjustment amenability is easily tested
using the graphical criteria of Lemma 7. For each child D of X in PCP(X, Y), we can test the visibility of all edges
X →D simultaneously using depth ﬁrst search. This means that we can check all potentially problematic edges in
time O(n+m). If all tests pass, we are licensed to apply the CBC, as shown above. Hence, we can solve all algorithmic
tasks in Table 4 for MAGs in the same way as for DAGs after an O(k(n + m)) check of adjustment amenability, where
k ≤|Ch(X)|.
Hence our algorithms can construct an adjustment set for a given MAG M and variables X, Y in O((k+ 1)(n + m))
time. If an additional set Z is given, it can be veriﬁed that Z is an adjustment set in the same time.
Minimal adjustments sets can be constructed and veriﬁed in O(k(n + m) + n2) or O(n(n + m)) time using our
algorithms FINDMINADJ, TESTMINADJ for dense or sparse graphs. The algorithms for the remaining problems of
ﬁnding a minimum cost adjustment set FINDMINCOSTADJ and enumerating adjustment sets LISTADJ or LISTMI-
NADJ in MAGs have the same runtime as the corresponding algorithms in DAGs, since their time surpasses the time
required for the adjustment amenability test.
9.4
Empirical analysis
We test for a DAG G = (V, E), generated as described in Section 8, whether the causal effect in the MAGs G[∅
∅and
G[∅
L can be identiﬁed via adjustment. Hereby L = V\ R is the set of unobserved nodes, which is used to determine the
42

n, k
25, 5
50, 5
100, 5
250, 5
25, 2
50, 2
100, 2
250, 2
25, 1
50, 1
100, 1
250, 1
% of identiﬁed graphs
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
n, k
25, 5
50, 5
100, 5
250, 5
25, 2
50, 2
100, 2
250, 2
25, 1
50, 1
100, 1
250, 1
% of identiﬁed graphs
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
l = 2
l = 5
l = 10
l = 20
l = 2
l = 5
l = 10
l = 20
Figure 20: Percent of identiﬁable MAGs for P(unobserved) = 0 (left) and P(unobserved) = 0.75 (right). The hori-
zontal axis is labeled by (n, k) sorted lexicographically by the number of nodes n and cardinalities |X| = |Y| = k. The
dotted lines show the results for G[∅
∅and the solid lines for G[∅
L with various density parameter values l ∈{2, 5, 10, 20}.
MAG G[∅
L. Thus, all nodes of G[∅
L are considered as observed and all of them are allowed to occur in adjustment sets.
The MAG G[∅
∅is syntactically equal to G and L speciﬁes forbidden nodes for adjustments. The MAGs G[∅
L can be
constructed according to Deﬁnition 7, however we have implemented the DAG-to-MAG conversion algorithm from
[18] in DAGitty, which is based on Theorem 4.2 in [9] that two nodes A, B are adjacent in G[∅
L if and only if there
exists an inducing path between A and B with respect to ∅, L (see Deﬁnition 15), because testing if an inducing path
exists appears easier than testing if two nodes are d-separable by any set.
Since G and G[∅
∅are syntactically equal, the only difference between using the CBC in a DAG G and the CBC in
G interpreted as a MAG is that the last case needs a test for adjustment amenability of the graph. Figure 19 shows an
example.
The results of our experiments are shown in Table 8 and 9. As expected we ﬁnd fewer adjustment sets in the MAGs
than in the DAGs, since any adjustment set found in the MAG is valid for all represented DAGs. There are also always
fewer adjustment sets in G[∅
L than in G[∅
∅, because with fewer nodes and edges in G[∅
L there are fewer visible edges
and G[∅
L might not be adjustment amenable, even if G[∅
∅is. For example in the DAG G = L →X →Y and the MAG
G[∅
∅= L →X →Y the empty set is an adjustment set. However, in the MAG G[∅
L= X →Y there exists no valid
adjustment set.
For n = 10, l = 10 (20) we have P(edge) = max{10/9, 1} = 1, so the experiments generate only complete DAGs,
such that all nodes are adjacent in the DAG and corresponding MAGs. This implies that there is either an edge X ←Y,
in which case adjustment is impossible in both the DAG and the MAG, or there is an edge X →Y, which then would
need to be visible for adjustment in the MAG due to Lemma 8. However, there are no visible edges in complete
graphs, since a visible edge X →Y would require a node not adjacent to Y. Thus, no adjustment sets are found for
these parameters.
The runtimes of these experiments are listed in Table 10. The time required to ﬁnd the adjustment set in the MAG
is similar to ﬁnding it in the original DAG and quick enough to be negligible. We did not run the experiments on
even larger graphs since constructing a MAG from a DAG has quadratic runtime complexity in the worst case (as a
quadratic number of edges needs to be added), which makes constructing the MAGs too slow.
43

l = 2
l = 5
l = 10
l = 20
n
k
CBC
MAG∅
∅
CBC
MAG∅
∅
CBC
MAG∅
∅
CBC
MAG∅
∅
10
1
8893
7711
7205
4600
5034
0
4934
0
10
2
6061
3773
1980
529
660
0
686
0
10
3
3395
1243
548
32
168
0
174
0
10
5
886
64
108
0
36
0
31
0
25
1
9573
9066
8936
7731
7905
6080
5843
2168
25
2
8247
6876
4735
3300
1553
789
401
33
25
3
6424
4399
1852
913
212
61
34
0
25
5
3021
1186
243
51
11
0
1
0
50
1
9832
9582
9476
8742
8997
7869
7832
6501
50
2
9128
8305
6938
5538
3049
2301
866
549
50
5
5535
3339
799
361
16
5
2
0
50
7
3120
1234
119
21
1
0
0
0
100
1
9907
9756
9783
9249
9494
8674
8966
8126
100
2
9593
9092
8353
7216
4834
4098
1762
1476
100
5
7591
5742
2336
1416
102
59
4
1
100
10
3040
1117
48
9
0
0
0
0
250
1
9947
9898
9894
9573
9774
9176
9621
9069
250
2
9840
9613
9327
8534
6689
5942
3285
3035
250
5
8994
7954
5325
3802
544
453
17
12
250
15
3537
1360
32
6
0
0
0
0
250
25
536
50
0
0
0
0
0
0
Table 8: Number of instances for P(unobserved) = 0 that are identiﬁed using CBC in the DAG or after converting the
DAG to a MAG.
l = 2
l = 5
l = 10
l = 20
n
k
CBC
MAG∅
∅
MAG∅
L
CBC
MAG∅
∅
MAG∅
L
CBC
MAG∅
∅
MAG∅
L
CBC
MAG∅
∅
MAG∅
L
10
1
6333
5398
5157
1935
645
543
978
0
0
936
0
0
10
2
2339
1371
1204
228
8
4
113
0
0
114
0
0
10
3
980
391
304
21
0
0
9
0
0
10
0
0
10
5
859
56
56
98
0
0
43
0
0
26
0
0
25
1
8414
7945
7838
3647
2635
2446
1340
429
346
557
18
13
25
2
5331
4309
4098
728
268
234
130
6
3
41
0
0
25
3
2632
1647
1491
144
16
14
17
0
0
6
0
0
25
5
449
149
119
1
0
0
0
0
0
0
0
0
50
1
9082
8856
8805
4651
3925
3788
1699
923
798
697
112
93
50
2
7059
6320
6177
1189
689
643
160
22
16
41
1
0
50
5
1663
906
805
16
0
0
1
0
0
0
0
0
50
7
388
130
102
0
0
0
0
0
0
0
0
0
100
1
9527
9395
9375
5585
5066
4943
1985
1311
1176
744
243
194
100
2
8316
7880
7808
1886
1462
1398
217
77
64
56
3
3
100
5
3562
2620
2501
30
11
10
0
0
0
0
0
0
100
10
375
109
84
0
0
0
0
0
0
0
0
0
250
1
9791
9743
9735
6832
6517
6462
2493
1944
1802
846
424
354
250
2
9209
9005
8980
3138
2760
2715
286
136
123
50
6
6
250
5
6182
5424
5321
123
63
57
1
0
0
0
0
0
250
15
306
98
82
0
0
0
0
0
0
0
0
0
250
25
4
1
0
0
0
0
0
0
0
0
0
0
Table 9: Number of instances for P(unobserved) = 0.75 that are identiﬁed using CBC in the DAG or after converting
the DAG to a MAG, either a MAG∅
L with latent nodes being removed from the graph or a MAG∅
∅with latent nodes left
and marked as latent.
44

P(unobserved) = 0
P(unobserved) = 0.75
l = 2
l = 5
l = 10
l = 20
l = 2
l = 5
l = 10
l = 20
n
k
MAG∅
∅
MAG∅
∅
MAG∅
∅
MAG∅
∅
MAG∅
∅
MAG∅
L
MAG∅
∅
MAG∅
L
MAG∅
∅
MAG∅
L
MAG∅
∅
MAG∅
L
10
1
0.5, 0.3
1.1, 0.5
1.6, 0.8
1.7, 0.8
0.2, 0.2
0.5, 0.4
0.2, 0.2
1.1, 0.6
0.3, 0.3
1.7, 0.9
0.3, 0.3
1.7, 0.9
10
2
0.7, 0.5
1.5, 0.9
2.2, 1.8
2.4, 1.8
0.2, 0.3
0.5, 0.5
0.4, 0.5
1.1, 0.8
0.4, 0.5
1.7, 1.6
0.4, 0.5
1.7, 1.6
10
3
0.8, 0.6
1.3, 1.1
2.1, 2.5
2.4, 2.7
0.5, 0.5
0.7, 0.6
0.9, 1.0
1.4, 1.0
0.9, 1.1
2.1, 2.4
1.1, 1.2
2.4, 2.5
10
5
0.8, 0.9
1.6, 1.7
2.3, 4.7
2.4, 4.7
0.7, 0.9
0.7, 0.8
1.5, 1.7
1.4, 1.6
2.4, 4.9
2.4, 4.9
2.4, 4.7
2.4, 4.7
25
1
3.4, 0.5
10, 1.0
22, 1.8
50, 3.9
0.8, 0.3
3.5, 0.5
2.0, 0.4
9.6, 1.1
3.4, 0.6
24, 2.0
5.2, 0.6
49, 4.1
25
2
3.7, 0.7
10, 1.4
22, 2.6
49, 7.0
0.8, 0.4
3.6, 0.7
2.1, 0.6
9.8, 1.4
3.5, 0.9
24, 2.8
5.4, 1.1
50, 7.4
25
3
3.8, 0.8
10, 1.7
23, 3.3
50, 10
1.0, 0.5
3.7, 0.9
2.4, 0.9
9.6, 1.7
3.9, 1.4
24, 3.5
6.0, 1.6
50, 11
25
5
3.7, 1.1
10, 2.2
22, 4.7
49, 17
1.7, 0.8
3.7, 1.1
4.5, 2.1
9.6, 2.2
6.8, 3.9
24, 4.5
10, 4.9
49, 15
50
1
14, 0.8
38, 1.6
92, 3.1
271, 6.3
2.9, 0.3
13, 0.9
12, 0.7
36, 1.8
23, 1.2
97, 3.3
38, 1.6
267, 6.8
50
2
13, 0.9
38, 2.1
91, 4.2
265, 9.4
3.0, 0.4
14, 1.1
12, 0.9
36, 2.3
22, 1.9
93, 4.4
37, 2.9
260, 9.8
50
5
14, 1.4
38, 3.2
92, 7.0
271, 19
3.1, 0.7
13, 1.5
12, 2.0
36, 3.3
23, 5.6
98, 7.5
40, 8.4
273, 20
50
7
13, 1.7
35, 3.8
95, 8.6
263, 25
4.0, 0.9
13, 1.8
16, 3.5
36, 4.0
29, 11
98, 9.4
47, 16
274, 26
100
1
51, 1.3
146, 2.7
361, 5.5
1129, 12
12, 0.4
50, 1.5
74, 1.4
138, 3.1
168, 3.1
380, 6.1
299, 5.1
1158, 12
100
2
51, 1.5
147, 3.4
348, 7.2
1120, 17
12, 0.5
49, 1.8
74, 1.7
138, 3.6
163, 5.1
366, 7.7
292, 11
1135, 17
100
5
51, 2.1
147, 5.0
348, 11
1117, 31
11, 0.8
49, 2.3
75, 3.1
138, 5.1
163, 15
367, 12
292, 36
1138, 32
100
10
51, 2.9
147, 7.3
357, 18
1122, 57
12, 1.2
50, 3.1
76, 7.1
138, 7.6
171, 43
383, 20
294, 93
1147, 59
250
1
314, 2.4
894, 4.5
2967, 10
12194, 23
64, 0.8
303, 3.1
663, 5.6
915, 5.6
1829, 18
3044, 12
3341, 40
12460, 25
250
2
313, 2.6
889, 5.4
2941, 14
12327, 33
65, 0.9
308, 3.4
633, 6.1
907, 6.5
1770, 26
2934, 15
3315, 93
12119, 34
250
5
312, 3.1
882, 7.8
2944, 22
12441, 66
65, 1.2
308, 4.0
637, 8.5
905, 8.7
1775, 75
2938, 23
3302, 366
12279, 63
250
15
312, 4.9
889, 15
2932, 49
12049, 169
64, 2.1
303, 5.9
649, 25
935, 16
1835, 412
2975, 49
3357, 1860
12550, 177
250
25
314, 6.8
872, 22
2938, 76
12219, 275
65, 3.1
310, 8.0
640, 54
908, 25
1783, 931
2925, 80
3397, 3645
12767, 292
Table 10: Time (in milliseconds) to ﬁrst construct the MAG from the DAG and then check for the existence of an
adjustment set in that MAG, for P(unobserved) = 0, respectively P(unobserved) = 0.75.
9.5
Auxiliary lemmas for proof of Theorem 3
In this section, we present the auxiliary lemmas used in the proof of Theorem 3. For Lemmas 12 and 13, we also give
separate preparing claims and existing results before stating the lemmas themselves.
Lemma 10. Given a DAG G and sets X, Y, Z ⊆V satisfying Z ∩Dpcp(X, Y) = ∅, Z m-connects a proper non-causal
path between X and Y if and only if it m-connects a proper non-causal walk between X and Y.
Proof. ⇐: Let w be the m-connected proper non-causal walk. It can be transformed to an m-connected path π by
removing loops of nodes that are visited multiple times. Since no nodes have been added, π remains proper, and the
ﬁrst edges of π and w are the same. So if w does not start with a →edge, π is non-causal. If w starts with an edge
X →D, there exists a collider with a descendant in Z which is in De(D). So π has to be non-causal, or it would
contradict Z ∩Dpcp(X, Y) = ∅.
⇒: Let π be an m-connected proper non-causal path. It can be changed to an m-connected walk w by inserting
Ci →. . . →Zi ←. . . ←Ci for every collider Ci on π and a corresponding Zi ∈Z. Since no edges are removed
from π, w is non-causal, but not necessarily proper, since the inserted walks might contain nodes of X. However, in
that case, w can be truncated to a proper walk w′ starting at the last node of X on w. Then w′ is non-causal, since it
contains the subpath X ←. . . ←Ci.
□
In all of the below, G = (V, E) is a DAG, Z, L ⊆V are disjoint, and M = G[∅
L. We notice ﬁrst, that every inducing
path w.r.t. Z and L is m-connected by Z.
Lemma 11 ([9]). If there is an inducing path π from U ∈V to V ∈V with respect to Z, L, then there exists no set Z′
with Z ⊆Z′ ⊆(V \ L) such that Z′ d-separates U and V in G or m-separates U and V in G[∅
L.
Proof. This is Theorem 4.2, cases (v) and (vi), in [9].
□
Claim 1. Two nodes U, V are adjacent in G[∅
L if and only if G contains an inducing path π between U and V with
respect to ∅, L. Moreover, the edge between U, V in G[∅
L can only have an arrowhead at U (V) if all such π have an
arrowhead at U (V) in G.
Proof. The ﬁrst part on adjacency is proved in [9]. For the second part on arrowheads, suppose π does not have an
arrowhead at U, then π starts with an edge U →D. Hence D < An(U), so D ∈An(V) because π is an inducing path
45

and therefore also U ∈An(V). Hence, the edge between U and V in G[∅
L must be U →V. The argument for V is
identical.
□
Claim 2. Suppose Z0, Z1, Z2 is a path in G[∅
L on which Z1 is a non-collider. Suppose an inducing path π01 from Z0
to Z1 w.r.t. ∅, L in G has an arrowhead at Z1, and an inducing path π12 from Z1 to Z2 w.r.t. ∅, L has an arrowhead at
Z1. Then the walk w012 = π01π12 can be truncated to an inducing path from Z0 to Z2 w.r.t. ∅, L in G.
Proof. The walk w012 does not contain more non-colliders than those on π01 or π12, so they must all be in L. It
remains to show that the colliders on w012 are in An(Z0 ∪Z2). Because Z1 is not a collider on Z0, Z1, Z2, at least one
of the edges Z0, Z1 and Z1, Z2 must be a directed edge pointing away from Z1. Assume without loss of generality that
Z0 ←Z1 is that edge. Then all colliders on π01 are in An(Z0 ∪Z1) = An(Z0) ⊆An(Z0 ∪Z2), and all colliders on
π12 are in An(Z1 ∪Z2) ⊆An(Z0 ∪Z2). Z1 itself is a collider on w012 and is also in An(Z0). Hence, the walk w012
is d-connected, and can be truncated to an inducing path that starts with the ﬁrst arrow of π01 and ends with the last
arrow of π12.
□
Claim 3. Let π = V1, . . ., Vn+1 be an inducing Z-trail, and let π′ be a subsequence of π formed by removing one
node Vi of π such that Vi ∈Z is a non-collider on π. Then π′ is an inducing Z-trail.
Proof. According to Claim 2, if Vi is a non-collider on π, then Vi−1 and Vi+1 are linked by an inducing path π that
contains an arrowhead at Vi−1 (Vi+1) if Vi−1 ∈Z (Vi+1 ∈Z). Therefore, Vi−1 and Vi+1 are themselves adjacent, π′ is
a path, and is a Z-trail.
□
Corollary 8. Every inducing Z-trail π = V1, . . . , Vn+1 has a subpath π′ that is m-connected by Z.
Proof. Transform π into π′ by replacing non-collider nodes in Z by the direct edge linking their neighbors until no
such node exists anymore. By inductively applying Claim 3, we see that π′ is also an inducing Z-trail, and every node
in Z is a collider because otherwise we would have continued transforming. So π′ must be m-connected by Z.
□
Lemma 12. Let wG be a walk from X to Y in G, X, Y < L, that is d-connected by Z. Let wM = V1, . . . , Vn+1 be the
subsequence of wG consisting only of the nodes in M = G[∅
L. Then Z m-connects X and Y in M via a path along a
subsequence w′
M formed from wM by removing some nodes in Z (possibly w′
M = wM).
Proof. First, truncate from wM all subwalks between nodes in Z that occur more than once. Now consider all subse-
quences V1, . . ., Vn+1, n > 1, of wM where V2, . . . , Vn ∈Z, V1, Vn+1 < Z, which now are all paths in wM. On those
subsequences, every Vi must be adjacent in G to Vi+1 via a path containing no colliders, and all non-endpoints on that
path must be in L. So there are inducing paths w.r.t. ∅, L between all Vi, Vi+1, which have arrowheads at Vi (Vi+1) if
Vi ∈Z (Vi+1 ∈Z). So V1, . . . , Vn+1 is an inducing Z-trail, and has a subpath which m-connects V1, Vn+1 given Z due
to Corollary 8. Transform wM to w′
M by replacing all inducing Z-trails by their m-connected subpaths. According
to Claim 1, non-colliders on wM cannot be colliders on w′
M, as bypassing inducing paths can remove but not create
arrowheads. Moreover, all nodes in Z on w′
M are colliders. Hence w′
M is m-connected by Z.
□
Corollary 9. Each edge on w′
M as deﬁned above corresponds to an inducing path w.r.t ∅, L in G along nodes on wG.
Claim 4. Suppose there exists an inducing path π01 from Z0 to Z1 w.r.t. S, L with an arrowhead at Z1 and an inducing
path from Z1 to Z2 w.r.t. S′, L with an arrowhead at Z1. Then the walk w012 = π01π12 can be truncated to an inducing
path from Z0 to Z2 w.r.t. S ∪S′ ∪{Z1}, L in G.
Proof. The walk w012 does not contain more non-colliders than those on π01 or π12, so they must all be in L. All
colliders on π0,1 and π1,2 as well as Z1 are in An(Z0, Z1, Z2, S, S′), and therefore also all colliders of w012.
Hence, the walk w012 is d-connected, and can be truncated to an inducing path that starts with the ﬁrst arrow of
π01 and ends with the last arrow of π12.
□
Claim 5. Suppose Z0, Z1, . . ., Zk+1 is a path in G[∅
L with an arrowhead at Zk+1 on which all Z1, . . . , Zk are colliders.
Then there exists an inducing path from Z0 to Zk+1 w.r.t. {Z1, . . ., Zk}, L with an arrowhead at Zk+1.
46

Proof. Because all Zi, Zi+1 are adjacent and all Z1, . . ., Zk are colliders there exist inducing paths πi,i+1 w.r.t. ∅, L
from Zi to Zi+1 that have arrowheads at Z1, . . . , Zk (Claim 1). The claim follows by repeatedly applying Claim 4 to
the πi,i+1’s.
□
Lemma 13. Suppose A →V1 ↔. . . ↔Vk ↔X →D or A ↔V1 ↔. . . ↔Vk ↔X →D is a path in G[∅
L (possibly
k = 0), each Vi is a parent of D and there exists an inducing path πXD from X to D w.r.t ∅, L that has arrowheads on
both ends. Then A and D cannot be m-separated in G[∅
L.
Proof. Assume the path is A →V1 ↔. . . ↔Vk ↔X →D. The case where the path starts with A ↔V1 can be
handled identically, since the ﬁrst arrowhead does not affect m-separation.
Assume A and D can be m-separated in G[∅
L, and let Z be such a separator. If V1 is not in Z then the path
A →V1 →D is not blocked, so V1 ∈Z. Inductively it follows, if Vi is not in Z, but all ∀j < i : Vj ∈Z then the path
A →V1 ↔. . . ↔Vi−1 ↔Vi →D is not blocked, so Vi ∈Z for all i.
There exist an inducing path πAX from A to X with an arrowhead at X w.r.t. to {V1, . . ., Vk}, L (Claim 5) which
can be combined with πXD to an inducing path from A to D w.r.t. to {V1, . . ., Vk, X}, L (Claim 4).
Hence no m-separator of A, D can contain {X, V1, . . . , Vk} (Lemma 11). Then there cannot exist an m-separator,
because every separator must include V1, . . . , Vk and the path A →V1 ↔V2 ↔. . . ↔Vk ↔X →D is open without
X ∈Z.
□
10
Discussion
We provide a framework of efﬁcient algorithms to verify, ﬁnd, and enumerate m-separating sets in MAGs, which we
then harness to solve the same problems for adjustment sets in DAGs and MAGs. In both graph classes, this provides
a complete and informative answer to the question when, and how, a desired causal effect between multiple exposures
and outcomes can be estimated by covariate adjustment.
For DAGs, our results show that from a computational complexity perspective, there is no disadvantage of using
our complete constructive back-door criterion (CBC) instead of Pearl’s back-door criterion (BC) – in other words,
using CBC instead of BC gives us the guarantee of completeness “for free”. Therefore, at least for implementation
in software packages, we would see no reason to use BC instead of CBC. Nevertheless, our empirical evaluation also
suggests that the number of cases covered by CBC but not by BC might be relatively small.
In contrast to DAGs, MAGs are not widely used to encode causal models, as their semantics are more complex and
direct edges do not necessarily correspond to direct causal relationships. Still, our CBC for MAGs can also be used as
a form of “sensitivity analysis” for researchers performing DAG-based analyses because every DAG can be converted
to a MAG – if it contains no latent variables, it can simply be read as if it were a MAG. If the MAG resulting from that
conversion still admits covariate adjustment, then we have shown that the adjustment set postulated for the original
DAG is in fact valid for an inﬁnite set of DAGs, namely all those represented by the MAG. This strategy might allow
researchers to partly relax the often untenable “causal sufﬁciency assumption” that all relevant variables are known
and were measured.
Our results rest on two key concepts: reduction of adjustment to m-separation in a subgraph (the proper back-
door graph), and adjustment amenability for graphical models that are more causally ambiguous than DAGs. Since
the publication of the preliminary version of this work [24, 10], these techniques were shown to be applicable to
adjustment in four additional classes of graphical causal models: CPDAGs [11], PAGs [11], chain graphs [12], and
maximal PDAGs [13]. Likewise, it has been shown that our algorithms can be applied to extended graphical criteria
that allow to deal with selection bias [37]. As we have illustrated brieﬂy in Section 4, we expect our algorithmic
framework to be useful in other areas as well, due to the central role of m-separation in the theory of graphical models.
In [38, 39] we have demonstrated how d-separators can be harnessed for efﬁciently ﬁnding generalized instrumental
variables: for this purpose we apply constrained separators but using different restrictions than the ones discussed in
this paper.
Our empirical analysis in this paper shows that our algorithmic framework is efﬁcient enough to be used in practice,
even on large (MAGs) or very large (DAGs) models. The practical feasibility of our algorithms is illustrated by the
fact that they underpin both the web application “dagitty.net” as well as the associated R package [34], which currently
47

have a substantial user community. We hope that future work will expand on our initial empirical results, and there
are many potential avenues to follow. For instance, we generated random DAGs, but these are likely not representative
of “typical” causal graphs encountered in practical applications. While our understanding of the “typical” structure of
causal graphs is currently limited, one could test the robustness of our ﬁndings to the graph structure by considering
other well-known graph-generating models, such as scale-free [40], small-world [41], or lattice-like [42] networks.
A further interesting open question to be pursued in future research would be whether the approaches presented
here could be generalized to accommodate confounding that arose by chance in a given sample, rather than for struc-
tural reasons [43].
11
Acknowledgments
This work was supported by the Deutsche Forschungsgemeinschaft (DFG) grant LI 634/4-1 and LI 634/4-2.
We also thank Marcel Wienöbst for help in performing the experiments, particularly for implementing the parsing
of the generated graph ﬁles in R.
12
References
References
[1] J. Pearl, Causality, Cambridge University Press, 2009.
[2] F. Elwert, Graphical causal models, in: Handbook of Causal Analysis for Social Research, Handbooks of Soci-
ology and Social Research, Springer, 2013, pp. 245–273.
[3] K. J. Rothman, S. Greenland, T. L. Lash, Modern Epidemiology, Wolters Kluwer, 2008.
[4] I. Shrier, Letter to the editor, Statistics in Medicine 27 (2008) 2740–2741.
[5] D. Rubin, Author’s reply, Statistics in Medicine 27 (2008) 2741–2742.
[6] I. R. Fulcher, I. Shpitser, S. Marealle, E. J. Tchetgen Tchetgen, Robust inference on indirect causal effects, ArXiv
e-printsarXiv:1711.03611.
[7] I. Shpitser, T. VanderWeele, J. Robins, On the validity of covariate adjustment for estimating causal effects, in:
Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence, AUAI Press, 2010, pp. 527–536.
[8] S. Greenland, Hierarchical regression for epidemiologic analyses of multiple exposures, Environmental Health
Perspectives 102 Suppl 8 (1994) 33–39.
[9] T. Richardson, P. Spirtes, Ancestral graph Markov models, Annals of Statistics 30 (2002) 927–1223.
[10] B. van der Zander, M. Li´skiewicz, J. Textor, Constructing separators and adjustment sets in ancestral graphs, in:
Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence, AUAI Press, 2014, pp. 907–916.
[11] E. Perkovi´c, J. Textor, M. Kalisch, M. Maathuis, Complete graphical characterization and construction of adjust-
ment sets in Markov equivalence classes of ancestral graphs, Journal of Machine Learning Research 18 (220)
(2018) 1–62.
[12] B. van der Zander, M. Li´skiewicz, Separators and adjustment sets in Markov equivalent DAGs, in: Proceedings
of the 30th AAAI Conference on Artiﬁcial Intelligence, 2016, pp. 3315–3321.
[13] E. Perkovi´c, M. Kalisch, M. Maathuis, Interpreting and using CPDAGs with background knowledge,
in: Proceedings of the 33rd Conference on Uncertainty in Artiﬁcial Intelligence, 2017, p. , available at
arxiv.org/abs/1707.02171.
48

[14] R. D. Shachter, Bayes-ball: The rational pastime, in: Proceedings of the 14th Conference on Uncertainty in
Artiﬁcial Intelligence, Morgan Kaufmann, 1998, pp. 480–487.
[15] J. Tian, A. Paz, J. Pearl, Finding minimal d-separators, Tech. Rep. R-254, University of California, Los Angeles
(1998).
[16] S. Acid, L. M. de Campos, Searching for Bayesian network structures in the space of restricted acyclic partially
directed graphs, Journal of Artiﬁcial Intelligence Research 18 (2003) 445–490. doi:10.1613/jair.1061.
[17] M. H. Maathuis, D. Colombo, A generalized backdoor criterion, Annals of Statistics 43 (3) (2015) 1060–1088.
doi:10.1214/14-AOS1295.
[18] J. Zhang, Causal reasoning with ancestral graphs, Journal of Machine Learning Research 9 (2008) 1437–1474.
[19] J. Pearl, Causal diagrams for empirical research, Biometrika 82 (4) (1995) 669–688.
[20] I. Shpitser, J. Pearl, Identiﬁcation of joint interventional distributions in recursive semi-markovian causal models,
in: Proceedings of the 21st National Conference on Artiﬁcial Intelligence, Vol. 2, Menlo Park, CA; Cambridge,
MA; London; AAAI Press; MIT Press; 1999, 2006, pp. 1219–1226.
[21] Y. Huang, M. Valtorta, Pearl’s calculus of intervention is complete, in: Proceedings of the 22nd Conference on
Uncertainty in Artiﬁcial Intelligence, AUAI Press, 2006, pp. 217–224.
[22] I. Shpitser, J. Pearl, Identiﬁcation of conditional interventional distributions, in: Proceedings of the 22nd Con-
ference on Uncertainty in Artiﬁcial Intelligence, AUAI Press, 2006, pp. 437–444.
[23] S. Acid, L. M. De Campos, An algorithm for ﬁnding minimum d-separating sets in belief networks, in: Proceed-
ings of the 12th Conference on Uncertainty in Artiﬁcial Intelligence, Morgan Kaufmann Publishers Inc., 1996,
pp. 3–10.
[24] J. Textor, M. Li´skiewicz, Adjustment criteria in causal diagrams: An algorithmic perspective, in: Proceedings of
the 27th Conference on Uncertainty in Artiﬁcial Intelligence, AUAI Press, 2011, pp. 681–688.
[25] K. Takata, Space-optimal, backtracking algorithms to list the minimal vertex separators of a graph, Discrete
Applied Mathematics 158 (2010) 1660–1667. doi:10.1016/j.dam.2010.05.013.
[26] P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction, and Search, 2nd ed., MIT Press, Cambridge, MA.,
2000.
[27] S. Even, Graph Algorithms, Computer Science Press, 1979.
[28] J. B. Orlin, Max ﬂows in O(nm) time, or better, in: Proceedings of the 45th annual ACM Symposium on Theory
of Computing, ACM, 2013, pp. 765–774.
[29] M. Garey, D. Johnson, Computers and intractability: a guide to the theory of NP-completeness, W. H. Freeman
and Company, 1979.
[30] P. Dawid, Conditional independence in statistical theory, Journal of the Royal Statistical Society 41 (1) (1979)
1–31.
[31] J. Pearl, P. Meshkat, Testing regression models with fewer regressors, in: Proceedings of the 7th International
Workshop on Artiﬁcial Intelligence and Statistics, AISTATS, Morgan Kaufmann, 1999, pp. 255–259.
[32] F. Thoemmes, Y. Rosseel, J. Textor, Local ﬁt evaluation of structural equation models using graphical criteria,
Psychological Methods (2017) doi:10.1037/met0000147.
[33] S. Tikka, J. Karvanen, Identifying causal effects with the R package causaleffect, Journal of Statistical Software
76 (12) (2017) . doi:10.18637/jss.v076.i12.
49

[34] J. Textor, B. van der Zander, M. S. Gilthorpe, M. Li´skiewicz, G. T. Ellison, Robust causal inference using
directed acyclic graphs: the R package ‘dagitty’, International Journal of Epidemiology 45 (6) (2016) 1887–
1894. doi:10.1093/ije/dyw341.
[35] M. Kalisch, M. Mächler, D. Colombo, M. Maathuis, P. Bühlmann, Causal inference using graphical models with
the R package pcalg, Journal of Statistical Software 47 (11) (2012) . doi:10.18637/jss.v047.i11.
[36] E. Barenboim, J. Tian, J. Pearl, Recovering from Selection Bias in Causal and Statistical Inference, in: Proceed-
ings of the 28th AAAI Conference on Artiﬁcial Intelligence, 2014, pp. 2410–2416.
[37] J. D. Correa, E. Bareinboim, Causal effect identiﬁcation by adjustment under confounding and selection biases,
in: Proceedings of the 31st AAAI Conference on Artiﬁcial Intelligence, 2017, pp. 3740–3746.
[38] B. van der Zander, J. Textor, M. Li´skiewicz, Efﬁciently ﬁnding conditional instruments for causal inference, in:
Proceedings of the 24th International Joint Conference on Artiﬁcial Intelligence, IJCAI, AAAI Press, 2015, pp.
3243–3249.
[39] B. van der Zander, M. Li´skiewicz, On searching for generalized instrumental variables, in: Proceedings of the
19th International Conference on Artiﬁcial Intelligence and Statistics, AISTATS, 2016, pp. 1214–1222.
[40] A.-L. Barabási, R. Albert, Emergence of Scaling in Random Networks, Science 286 (5439) (1999) 509–512.
doi:10.1126/science.286.5439.509.
[41] D. J. Watts, S. H. Strogatz, Collective dynamics of ‘small-world’ networks, Nature 393 (6684) (1998) 440–442.
doi:10.1038/30918.
[42] J. Ozik, B. R. Hunt, E. Ott, Growing networks with geographical attachment preference: Emergence of small
worlds, Physical Review E 69 (2). doi:10.1103/physreve.69.026108.
[43] S. Greenland, M. A. Mansournia, Limitations of individual causal models, causal graphs, and ignorability as-
sumptions, as illustrated by random confounding and design unfaithfulness, Eur. J. Epidemiol. 30 (10) (2015)
1101–1110.
13
Appendix: further experimental results
Tables 11 and 12 in this section show the results of versions of the experiments presented in Section 8 in Tables 5
and 6 in which the parameter controlling the number of unobserved variables is set to 0.25 or 0.5.
50

l = 2
l = 5
l = 10
l = 20
n
k
BC
CBC
CBC+
BC
CBC
CBC+
BC
CBC
CBC+
BC
CBC
CBC+
10
1
8235
8235
9901
4772
4772
8938
2452
2452
7443
2373
2373
7469
10
2
4307
4840
8035
528
1120
3545
0
349
1977
0
379
2004
10
3
1603
2353
5173
36
313
1168
0
96
591
0
107
594
10
5
184
823
1700
0
99
204
0
44
76
0
36
82
25
1
9306
9306
9978
7019
7019
9507
3549
3549
8141
1645
1645
6686
25
2
7312
7466
9489
2132
2490
5757
310
485
2815
24
126
1862
25
3
4863
5250
8101
416
718
2668
10
65
925
0
14
523
25
5
1466
2060
4255
10
65
449
0
2
99
0
0
38
50
1
9668
9668
9993
8075
8075
9763
4408
4408
8517
1927
1927
7024
50
2
8555
8600
9814
4013
4222
7274
639
777
3614
92
145
2145
50
5
3727
4167
6835
86
174
1034
1
1
158
0
0
81
50
7
1449
1946
3888
4
22
233
0
0
14
0
0
7
100
1
9818
9818
9997
8886
8886
9879
5158
5158
8833
2173
2173
7271
100
2
9341
9354
9951
5742
5871
8400
1013
1084
4462
167
209
2564
100
5
6215
6404
8637
443
595
2122
3
11
340
0
0
96
100
10
1453
1813
3490
0
1
78
0
0
1
0
0
1
250
1
9917
9917
10000
9559
9559
9964
6033
6033
9208
2558
2558
7691
250
2
9712
9712
9990
7840
7888
9362
1717
1764
5611
216
236
3038
250
5
8293
8343
9669
2015
2192
4569
9
18
598
0
0
158
250
15
1728
2014
3676
0
1
27
0
0
1
0
0
0
250
25
85
164
361
0
0
0
0
0
0
0
0
0
500
1
9968
9968
10000
9765
9765
9986
6684
6684
9455
2667
2667
7888
500
2
9864
9866
9997
8920
8933
9762
2304
2329
6352
303
314
3414
500
5
9162
9178
9910
4207
4343
6662
46
50
955
0
0
186
500
22
1533
1774
3148
0
0
7
0
0
0
0
0
0
500
50
0
3
10
0
0
0
0
0
0
0
0
0
1000
1
9984
9984
10000
9903
9903
9997
7266
7266
9615
2831
2831
8086
1000
2
9926
9926
10000
9490
9491
9902
3261
3278
7194
348
350
3757
1000
5
9599
9602
9984
6613
6703
8370
75
81
1486
0
0
261
1000
32
1413
1588
2801
0
0
1
0
0
0
0
0
0
1000
100
0
0
0
0
0
0
0
0
0
0
0
0
2000
1
9994
9994
10000
9945
9945
10000
7924
7924
9773
3117
3117
8322
2000
2
9963
9963
10000
9809
9812
9985
4140
4150
7842
452
456
4191
2000
5
9800
9802
9992
8273
8316
9403
210
217
2140
0
0
356
2000
45
1541
1728
2840
0
0
0
0
0
0
0
0
0
2000
200
0
0
0
0
0
0
0
0
0
0
0
0
Table 11: Numbers of instances for P(unobserved) = 0.25 that are identiﬁable by use of BC, CBC, CBC+ (as deﬁned
in Section 8.2). We did not run the IDC algorithm on these data due to its high time complexity. Gray cells highlight
where the CBC was able to identify at least 400 more graphs than the BC.
51

l = 2
l = 5
l = 10
l = 20
n
k
BC
CBC
CBC+
BC
CBC
CBC+
BC
CBC
CBC+
BC
CBC
CBC+
10
1
7418
7418
9799
3102
3102
8085
1500
1500
6515
1520
1520
6537
10
2
3289
3795
7602
303
649
3029
0
223
1910
0
251
1878
10
3
1000
1575
4512
13
138
1038
0
57
549
0
43
539
10
5
165
822
1684
0
87
220
0
39
85
0
47
76
25
1
8912
8912
9960
5107
5107
9106
1920
1920
7224
898
898
5913
25
2
6413
6555
9258
1115
1346
5056
154
249
2592
9
77
1803
25
3
3665
4060
7595
182
339
2326
6
24
880
0
7
521
25
5
820
1241
3591
1
20
419
0
2
86
0
0
40
50
1
9429
9429
9983
6206
6206
9487
2454
2454
7831
1004
1004
6414
50
2
7928
7991
9738
2141
2272
6516
260
334
3353
43
76
2127
50
5
2484
2835
6082
21
52
921
0
1
198
0
0
64
50
7
735
1041
3164
0
1
205
0
0
20
0
0
6
100
1
9725
9725
9995
7032
7032
9658
2952
2952
8281
1154
1154
6626
100
2
8858
8882
9930
3198
3285
7674
403
444
4069
74
85
2422
100
5
4828
5052
8211
89
129
1791
1
1
277
0
0
83
100
10
619
828
2793
0
0
90
0
0
2
0
0
0
250
1
9876
9876
9999
8259
8259
9908
3539
3539
8767
1314
1314
7069
250
2
9542
9546
9979
5055
5097
8865
575
591
5085
83
93
2925
250
5
7423
7498
9502
422
469
3608
1
1
613
0
0
155
250
15
711
922
2833
0
0
37
0
0
0
0
0
0
250
25
12
25
243
0
0
0
0
0
0
0
0
0
500
1
9937
9937
10000
8992
8992
9957
4038
4038
9062
1336
1336
7354
500
2
9779
9781
9998
6569
6587
9369
791
802
5748
98
100
3210
500
5
8625
8641
9852
1162
1231
5318
2
3
925
0
0
195
500
22
572
685
2245
0
0
3
0
0
0
0
0
0
500
50
0
1
3
0
0
0
0
0
0
0
0
0
1000
1
9972
9972
10000
9420
9420
9985
4487
4487
9314
1525
1525
7565
1000
2
9865
9865
9999
7872
7881
9746
1086
1094
6475
96
98
3660
1000
5
9328
9335
9957
2623
2683
7140
4
5
1461
0
0
253
1000
32
475
548
1910
0
0
0
0
0
0
0
0
0
1000
100
0
0
0
0
0
0
0
0
0
0
0
1
2000
1
9985
9985
10000
9715
9715
9992
5059
5059
9491
1693
1693
7799
2000
2
9949
9949
10000
8823
8828
9937
1500
1503
7201
122
122
3905
2000
5
9624
9626
9994
4614
4649
8529
19
19
2066
0
0
345
2000
45
467
524
1853
0
0
0
0
0
0
0
0
0
2000
200
0
0
0
0
0
0
0
0
0
0
0
0
Table 12: Numbers of instances for P(unobserved) = 0.5 that are identiﬁable by use of BC, CBC, CBC+ (as deﬁned
in Section 8.2). We did not run the IDC algorithm on these data due to its high time complexity. Gray cells highlight
where the CBC was able to identify at least 400 more graphs than the BC.
52

