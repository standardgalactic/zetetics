BAYESIAN ESTIMATION
AND TRACKING

BAYESIAN ESTIMATION
AND TRACKING
A Practical Guide
ANTON J. HAUG

Copyright © 2012 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or
by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400,
fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission
should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken,
NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in
preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a professional where appropriate. Neither the publisher nor
author shall be liable for any loss of proﬁt or any other commercial damages, including but not limited to
special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762-2974, outside the United States at
(317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may
not be available in electronic formats. For more information about Wiley products, visit our web site at
www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Haug, Anton J., 1941–
Bayesian estimation and tracking : a practical guide / Anton J. Haug.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-62170-7 (hardback)
1. Bayesian statistical decision theory.
2. Automatic
tracking–Mathematics.
3. Estimation theory.
I. Title.
QA279.5.H38 2012
519.5′42–dc23
2011044308
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

DEDICATION
To my wife, who inspires us all to achieve at the highest limit of our abilities.
To my children, whose achievements make me proud to be their father.
To my grandchildren, who will always keep me young.
And in memory of my parents, Rose and David, whose love was without bound.

CONTENTS
PREFACE
xv
ACKNOWLEDGMENTS
xvii
LIST OF FIGURES
xix
LIST OF TABLES
xxv
PART I
PRELIMINARIES
1
Introduction
3
1.1
Bayesian Inference, 4
1.2
Bayesian Hierarchy of Estimation Methods, 5
1.3
Scope of This Text, 6
1.3.1
Objective, 6
1.3.2
Chapter Overview and Prerequisites, 6
1.4
Modeling and Simulation with MATLAB®, 8
References, 9
2
Preliminary Mathematical Concepts
11
2.1
A Very Brief Overview of Matrix Linear Algebra, 11
2.1.1
Vector and Matrix Conventions and Notation, 11
2.1.2
Sums and Products, 12
2.1.3
Matrix Inversion, 13
2.1.4
Block Matrix Inversion, 14
2.1.5
Matrix Square Root, 15
vii

viii
CONTENTS
2.2
Vector Point Generators, 16
2.3
Approximating Nonlinear Multidimensional Functions with
Multidimensional Arguments, 19
2.3.1
Approximating Scalar Nonlinear Functions, 19
2.3.2
Approximating Multidimensional Nonlinear Functions, 23
2.4
Overview of Multivariate Statistics, 29
2.4.1
General Deﬁnitions, 29
2.4.2
The Gaussian Density, 32
References, 40
3
General Concepts of Bayesian Estimation
42
3.1
Bayesian Estimation, 43
3.2
Point Estimators, 43
3.3
Introduction to Recursive Bayesian Filtering of Probability Density
Functions, 46
3.4
Introduction to Recursive Bayesian Estimation of the State Mean and
Covariance, 49
3.4.1
State Vector Prediction, 50
3.4.2
State Vector Update, 51
3.5
Discussion of General Estimation Methods, 55
References, 55
4
Case Studies: Preliminary Discussions
56
4.1
The Overall Simulation/Estimation/Evaluation Process, 57
4.2
A Scenario Simulator for Tracking a Constant Velocity Target
Through a DIFAR Buoy Field, 58
4.2.1
Ship Dynamics Model, 58
4.2.2
Multiple Buoy Observation Model, 59
4.2.3
Scenario Speciﬁcs, 59
4.3
DIFAR Buoy Signal Processing, 62
4.4
The DIFAR Likelihood Function, 67
References, 69
PART II
THE GAUSSIAN ASSUMPTION: A FAMILY OF KALMAN
FILTER ESTIMATORS
5
The Gaussian Noise Case: Multidimensional Integration of
Gaussian-Weighted Distributions
73
5.1
Summary of Important Results From Chapter 3, 74
5.2
Derivation of the Kalman Filter Correction (Update) Equations
Revisited, 76
5.3
The General Bayesian Point Prediction Integrals for Gaussian
Densities, 78

CONTENTS
ix
5.3.1
Reﬁning the Process Through an Afﬁne Transformation, 80
5.3.2
General Methodology for Solving Gaussian-Weighted
Integrals, 82
References, 85
6
The Linear Class of Kalman Filters
86
6.1
Linear Dynamic Models, 86
6.2
Linear Observation Models, 87
6.3
The Linear Kalman Filter, 88
6.4
Application of the LKF to DIFAR Buoy Bearing Estimation, 88
References, 92
7
The Analytical Linearization Class of Kalman Filters:
The Extended Kalman Filter
93
7.1
One-Dimensional Consideration, 93
7.1.1
One-Dimensional State Prediction, 94
7.1.2
One-Dimensional State Estimation Error Variance
Prediction, 95
7.1.3
One-Dimensional Observation Prediction Equations, 96
7.1.4
Transformation of One-Dimensional Prediction Equations, 96
7.1.5
The One-Dimensional Linearized EKF Process, 98
7.2
Multidimensional Consideration, 98
7.2.1
The State Prediction Equation, 99
7.2.2
The State Covariance Prediction Equation, 100
7.2.3
Observation Prediction Equations, 102
7.2.4
Transformation of Multidimensional Prediction
Equations, 103
7.2.5
The Linearized Multidimensional Extended Kalman Filter
Process, 105
7.2.6
Second-Order Extended Kalman Filter, 105
7.3
An Alternate Derivation of the Multidimensional Covariance
Prediction Equations, 107
7.4
Application of the EKF to the DIFAR Ship Tracking Case Study, 108
7.4.1
The Ship Motion Dynamics Model, 108
7.4.2
The DIFAR Buoy Field Observation Model, 109
7.4.3
Initialization for All Filters of the Kalman Filter Class, 111
7.4.4
Choosing a Value for the Acceleration Noise, 112
7.4.5
The EKF Tracking Filter Results, 112
References, 114
8
The Sigma Point Class: The Finite Difference Kalman Filter
115
8.1
One-Dimensional Finite Difference Kalman Filter, 116
8.1.1
One-Dimensional Finite Difference State Prediction, 116

x
CONTENTS
8.1.2
One-Dimensional Finite Difference State Variance
Prediction, 117
8.1.3
One-Dimensional Finite Difference Observation Prediction
Equations, 118
8.1.4
The One-Dimensional Finite Difference Kalman Filter
Process, 118
8.1.5
Simpliﬁed One-Dimensional Finite Difference Prediction
Equations, 118
8.2
Multidimensional Finite Difference Kalman Filters, 120
8.2.1
Multidimensional Finite Difference State Prediction, 120
8.2.2
Multidimensional Finite Difference State Covariance
Prediction, 123
8.2.3
Multidimensional Finite Difference Observation Prediction
Equations, 124
8.2.4
The Multidimensional Finite Difference Kalman Filter
Process, 125
8.3
An Alternate Derivation of the Multidimensional Finite Difference
Covariance Prediction Equations, 125
References, 127
9
The Sigma Point Class: The Unscented Kalman Filter
128
9.1
Introduction to Monomial Cubature Integration Rules, 128
9.2
The Unscented Kalman Filter, 130
9.2.1
Background, 130
9.2.2
The UKF Developed, 131
9.2.3
The UKF State Vector Prediction Equation, 134
9.2.4
The UKF State Vector Covariance Prediction Equation, 134
9.2.5
The UKF Observation Prediction Equations, 135
9.2.6
The Unscented Kalman Filter Process, 135
9.2.7
An Alternate Version of the Unscented Kalman Filter, 135
9.3
Application of the UKF to the DIFAR Ship Tracking Case Study, 137
References, 138
10 The Sigma Point Class: The Spherical Simplex Kalman Filter
140
10.1 One-Dimensional Spherical Simplex Sigma Points, 141
10.2 Two-Dimensional Spherical Simplex Sigma Points, 142
10.3 Higher Dimensional Spherical Simplex Sigma Points, 144
10.4 The Spherical Simplex Kalman Filter, 144
10.5 The Spherical Simplex Kalman Filter Process, 145
10.6 Application of the SSKF to the DIFAR Ship Tracking Case Study, 146
Reference, 147

CONTENTS
xi
11 The Sigma Point Class: The Gauss–Hermite Kalman Filter
148
11.1 One-Dimensional Gauss–Hermite Quadrature, 149
11.2 One-Dimensional Gauss–Hermite Kalman Filter, 153
11.3 Multidimensional Gauss–Hermite Kalman Filter, 155
11.4 Sparse Grid Approximation for High Dimension/High Polynomial
Order, 160
11.5 Application of the GHKF to the DIFAR Ship Tracking Case Study, 163
References, 163
12 The Monte Carlo Kalman Filter
164
12.1 The Monte Carlo Kalman Filter, 167
Reference, 167
13 Summary of Gaussian Kalman Filters
168
13.1 Analytical Kalman Filters, 168
13.2 Sigma Point Kalman Filters, 170
13.3 A More Practical Approach to Utilizing the Family of Kalman
Filters, 174
References, 175
14 Performance Measures for the Family of Kalman Filters
176
14.1 Error Ellipses, 176
14.1.1 The Canonical Ellipse, 177
14.1.2 Determining the Eigenvalues of P, 178
14.1.3 Determining the Error Ellipse Rotation Angle, 179
14.1.4 Determination of the Containment Area, 180
14.1.5 Parametric Plotting of Error Ellipse, 181
14.1.6 Error Ellipse Example, 182
14.2 Root Mean Squared Errors, 182
14.3 Divergent Tracks, 183
14.4 Cramer–Rao Lower Bound, 184
14.4.1 The One-Dimensional Case, 184
14.4.2 The Multidimensional Case, 186
14.4.3 A Recursive Approach to the CRLB, 186
14.4.4 The Cramer–Rao Lower Bound for Gaussian Additive
Noise, 190
14.4.5 The Gaussian Cramer–Rao Lower Bound with Zero Process
Noise, 191
14.4.6 The Gaussian Cramer–Rao Lower Bound with Linear
Models, 191

xii
CONTENTS
14.5 Performance of Kalman Class DIFAR Track Estimators, 192
References, 198
PART III
MONTE CARLO METHODS
15 Introduction to Monte Carlo Methods
201
15.1 Approximating a Density From a Set of Monte Carlo Samples, 202
15.1.1 Generating Samples from a Two-Dimensional Gaussian
Mixture Density, 202
15.1.2 Approximating a Density by Its Multidimensional
Histogram, 202
15.1.3 Kernel Density Approximation, 204
15.2 General Concepts Importance Sampling, 210
15.3 Summary, 215
References, 216
16 Sequential Importance Sampling Particle Filters
218
16.1 General Concept of Sequential Importance Sampling, 218
16.2 Resampling and Regularization (Move) for SIS Particle Filters, 222
16.2.1 The Inverse Transform Method, 222
16.2.2 SIS Particle Filter with Resampling, 226
16.2.3 Regularization, 227
16.3 The Bootstrap Particle Filter, 230
16.3.1 Application of the BPF to DIFAR Buoy Tracking, 231
16.4 The Optimal SIS Particle Filter, 233
16.4.1 Gaussian Optimal SIS Particle Filter, 235
16.4.2 Locally Linearized Gaussian Optimal SIS Particle Filter, 236
16.5 The SIS Auxiliary Particle Filter, 238
16.5.1 Application of the APF to DIFAR Buoy Tracking, 242
16.6 Approximations to the SIS Auxiliary Particle Filter, 243
16.6.1 The Extended Kalman Particle Filter, 243
16.6.2 The Unscented Particle Filter, 243
16.7 Reducing the Computational Load Through
Rao-Blackwellization, 245
References, 245
17 The Generalized Monte Carlo Particle Filter
247
17.1 The Gaussian Particle Filter, 248
17.2 The Combination Particle Filter, 250
17.2.1 Application of the CPF–UKF to DIFAR Buoy Tracking, 252
17.3 Performance Comparison of All DIFAR Tracking Filters, 253
References, 255

CONTENTS
xiii
PART IV
ADDITIONAL CASE STUDIES
18 A Spherical Constant Velocity Model for Target Tracking
in Three Dimensions
259
18.1 Tracking a Target in Cartesian Coordinates, 261
18.1.1 Object Dynamic Motion Model, 262
18.1.2 Sensor Data Model, 263
18.1.3 GaussianTrackingAlgorithmsforaCartesianStateVector, 264
18.2 Tracking a Target in Spherical Coordinates, 265
18.2.1 State Vector Position and Velocity Components in Spherical
Coordinates, 266
18.2.2 Spherical State Vector Dynamic Equation, 267
18.2.3 Observation Equations with a Spherical State Vector, 270
18.2.4 GaussianTrackingAlgorithmsforaSphericalStateVector, 270
18.3 Implementation of Cartesian and Spherical Tracking Filters, 273
18.3.1 Setting Values for q, 273
18.3.2 Simulating Radar Observation Data, 274
18.3.3 Filter Initialization, 276
18.4 Performance Comparison for Various Estimation Methods, 278
18.4.1 Characteristics of the Trajectories Used for Performance
Analysis, 278
18.4.2 Filter Performance Comparisons, 282
18.5 Some Observations and Future Considerations, 293
APPENDIX 18.A Three-Dimensional Constant Turn Rate Kinematics, 294
18.A.1 General Velocity Components for Constant Turn Rate
Motion, 294
18.A.2 General Position Components for Constant Turn Rate
Motion, 297
18.A.3 Combined Trajectory Transition Equation, 299
18.A.4 Turn Rate Setting Based on a Desired Turn Acceleration, 299
APPENDIX 18.B Three-Dimensional Coordinate Transformations, 301
18.B.1 Cartesian-to-Spherical Transformation, 302
18.B.2 Spherical-to-Cartesian Transformation, 305
References, 306
19 Tracking a Falling Rigid Body Using Photogrammetry
308
19.1 Introduction, 308
19.2 The Process (Dynamic) Model for Rigid Body Motion, 311
19.2.1 Dynamic Transition of the Translational Motion of a Rigid
Body, 311
19.2.2 Dynamic Transition of the Rotational Motion of a Rigid
Body, 313
19.2.3 Combined Dynamic Process Model, 316
19.2.4 The Dynamic Process Noise Models, 317

xiv
CONTENTS
19.3 Components of the Observation Model, 318
19.4 Estimation Methods, 321
19.4.1 A Nonlinear Least Squares Estimation Method, 321
19.4.2 An Unscented Kalman Filter Method, 323
19.4.3 Estimation Using the Unscented Combination Particle
Filter, 325
19.4.4 Initializing the Estimator, 326
19.5 The Generation of Synthetic Data, 328
19.5.1 Synthetic Rigid Body Feature Points, 328
19.5.2 Synthetic Trajectory, 328
19.5.3 Synthetic Cameras, 333
19.5.4 Synthetic Measurements, 333
19.6 Performance Comparison Analysis, 334
19.6.1 Filter Performance Comparison Methodology, 335
19.6.2 Filter Comparison Results, 338
19.6.3 Conclusions and Future Considerations, 341
APPENDIX 19.A Quaternions, Axis-Angle Vectors, and Rotations, 342
19.A.1 Conversions Between Rotation Representations, 342
19.A.2 Representation of Orientation and Rotation, 343
19.A.3 Point Rotations and Frame Rotations, 344
References, 345
20 Sensor Fusion Using Photogrammetric and Inertial Measurements
346
20.1 Introduction, 346
20.2 The Process (Dynamic) Model for Rigid Body Motion, 347
20.3 The Sensor Fusion Observational Model, 348
20.3.1 The Inertial Measurement Unit Component of the
Observation Model, 348
20.3.2 The Photogrammetric Component of the Observation
Model, 350
20.3.3 The Combined Sensor Fusion Observation Model, 351
20.4 The Generation of Synthetic Data, 352
20.4.1 Synthetic Trajectory, 352
20.4.2 Synthetic Cameras, 352
20.4.3 Synthetic Measurements, 352
20.5 Estimation Methods, 354
20.5.1 Initial Value Problem Solver for IMU Data, 354
20.6 Performance Comparison Analysis, 357
20.6.1 Filter Performance Comparison Methodology, 359
20.6.2 Filter Comparison Results, 360
20.7 Conclusions, 361
20.8 Future Work, 362
References, 364
Index
367

PREFACE
This book presents a complete development of Bayesian estimation ﬁlters from ﬁrst
principles.WeconsiderbothlinearandnonlineardynamicsystemsdrivenbyGaussian
ornon-Gaussiannoises.Itisassumedthatthedynamicsystemsarecontinuousbecause
the observations related to those systems occur at discrete times only discrete ﬁlters
are discussed. The primary goal is to present a comprehensive overview of most of the
Bayesian estimation methods developed over the past 60 years in a uniﬁed approach
that shows how each arises from the basic ideas underlying the Bayesian paradigms
related to conditional densities.
The prerequisites for understanding the material presented in this book include
a basic understanding of linear algebra, Bayesian probability theory, and numerical
methods for ﬁnite differences and interpolation. Chapter 2 includes a review of all of
these topics and is needed for an understanding of the remaining material in the book.
Many of the topics covered in this book grew out of a one semester course taught in
thegraduatemathematicsdepartmentattheUniversityofMaryland,CollegePark.The
main goal of that course was to have the students develop their own Matlab® toolbox
of target tracking methods. Several very speciﬁc tracking problems were presented to
the students, and all homework problems consisted of coding each speciﬁc tracking
(estimation) method into one or more Matlab® subroutines. In general, the subroutines
the students developed were stand-alone and could be applied to any of a variety
of tracking problems without much difﬁculty. Since the homework problems were
dependent on our speciﬁc tracking problem (bearings-only tracking), I decided that
this book would not contain any problem sets, allowing anyone using this book in a
course to tailor their homework to a tracking problem of their choice. In addition, this
book contains four fairly complicated case studies that contain enough material that
any instructor can use one of them as the basis of their coding homework problems.
The ﬁrst case study is used as an example throughout most of Parts II and III of this
xv

xvi
PREFACE
book while Part IV consists of the remaining three case studies with a separate chapter
devoted to each.
This book has two emphases: First, detailed derivations from ﬁrst principles of
each estimation method (tracking ﬁlters) are emphasized through numerous tables
and ﬁgures, very detailed step-by-step instructions for each method that will make
coding of the tracking ﬁlter simple and easy to understand are also emphasized.
It is shown that recursive Bayesian estimation can be developed as the solution
to a series of conditional density-weighted integrals of a transition or transformation
function. The transition function is one that transitions a dynamic state vector from one
time step to the next while the transformation function is one that transforms a state
vector into an observation vector. There are a variety of numerical methods for solving
these integrals, and each leads to a different estimation method. Each chapter in Parts
II and III of this book considers one or more numerical approximations to solving
these integrals leading to the class of Kalman ﬁlter methods for Gaussian-weighted
integrals in Part II and the class of particle ﬁlters for density-weighted integrals with
unknown densities in Part III.
This book is an outgrowth of many years of research in the ﬁeld and it is hoped that
it will be a signiﬁcant contribution to the Bayesian estimation and tracking literature. I
also hope that it will open up the ﬁeld in new directions based on the many comments
made about how these methods can be enhanced and applied in new ways and to new
problems.
Anton J. Haug

ACKNOWLEDGMENTS
The author would like to acknowledge the many people who have contributed over
the years to his knowledge base on Bayesian estimation and tracking through
stimulating conversations.
Thanks are due to my colleagues from my previous position at MITRE, including
G. Jacyna, D. Collella, and C. Christou.
Thanks are also due to my colleagues at the Johns Hopkins University Applied
Physics Laboratory (JHUAPL), including L. Williams for her inspiring help on the
case study in Chapter 18 and for general discussions on the material in the book;
C. Davis for discussions on aspects of target tracking and for proofreading parts of
the manuscript; and W. Martin for assessing the readability of the entire manuscript.
Special thanks go to B. Beltran for the signiﬁcant contribution of a Matlab® subrou-
tine that computes the Gauss–Hermite sigma points and weights for any state vector
dimension. I would also like to thank the JHUAPL Janney Publication Program com-
mittee for a one man-month work grant that allowed me to ﬁnish the manuscript in
a timely fashion, and M. Whisnant for reviewing and commenting on the manuscript
before granting a public release.
Finally, I would like to thank John Wiley & Sons for the opportunity to offer
this book to the estimation and tracking community. I would also like to thank the
anonymous reviewers of my initial book proposal submission for their many very
helpful comments. I especially appreciate the role of S. Steitz-Filler, the Mathematics
and Statistics Editor at John Wiley & Sons, for her patience over the four years that
it took to write this book. She and her colleagues provided valuable advice, support,
and technical assistance in transforming my initial manuscript submission into a book
that I can be proud of.
xvii

LIST OF FIGURES
1.1
Hierarchy of Bayesian Estimation Tracker Filters. . . . . . . . . . . .
5
2.1
Example of Two-Dimensional Cartesian Grid of Axial
Vector Points. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Example of Three-Dimensional Cartesian Grid of Axial
Vector Points. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.3
Two-Dimensional Points for Multidimensional Sterling’s
Approximation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.4
Simplex Figures in up to Three Dimensions. . . . . . . . . . . . . . .
29
2.5
One-Dimensional Gaussian pdf Example. . . . . . . . . . . . . . . .
33
2.6
Example of a Two-Dimensional Gaussian PDF. . . . . . . . . . . . .
34
2.7
An Example of a One-Dimensional Gaussian CDF. . . . . . . . . . .
35
2.8
An Example of a Two-Dimensional Gaussian CDF. . . . . . . . . . .
36
2.9
Effects of an Afﬁne Transformation on a Gaussian Probability
Density Function. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.1
Depiction of One Step in the Recursive Bayesian Posterior
Density Estimation Procedure. . . . . . . . . . . . . . . . . . . . . .
49
3.2
General Block Diagram for Recursive Point Estimation Process. . . .
54
4.1
Methodology for Development and Evaluation of Tracking
Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
xix

xx
LIST OF FIGURES
4.2
DIFAR Buoy Geometry.
. . . . . . . . . . . . . . . . . . . . . . . .
60
4.3
Ships Track with DIFAR Buoy Field and the Associated Bearings for
Two Buoys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.4
DIFAR Sensor Signal Processing.
. . . . . . . . . . . . . . . . . . .
62
4.5
The Spread of Bearing Observations from a DIFAR Buoy from 100
Monte Carlo Runs for Four SNRs. . . . . . . . . . . . . . . . . . . .
67
4.6
DIFAR Likelihood Density for a BT of 10 and SNR from −10
to +10 dB.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
5.1
Block Diagram of the Process Flow for the Bayesian Estimator
with Gaussian Probability Densities. . . . . . . . . . . . . . . . . . .
80
6.1
Block Diagram of the LKF Process for a Single DIFAR Buoy. . . . .
91
6.2
Comparison of the LKF Bearing Estimates with the True Bearing for
One DIFAR Buoy.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
91
7.1
The Geometry Used for Initialization. . . . . . . . . . . . . . . . . .
111
7.2
A Comparison of the Estimated Track of a Ship Transiting the Buoy
Field with the True Track.
. . . . . . . . . . . . . . . . . . . . . . .
113
7.3
Comparison of the EKF Tracker Outputs for Six SNRs. . . . . . . . .
113
10.1
Depiction of a Two-Dimensional Simplex with Four Vector
Integration Points. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
13.1
Graphical Presentation of the Linear Kalman Filter Process Flow.
. .
169
13.2
A Graphical Representation of the EKF Process Flow.
. . . . . . . .
169
13.3
Process Flow for General Sigma Point Kalman Filter. . . . . . . . . .
170
14.1
Error Ellipse Rotation Angle. . . . . . . . . . . . . . . . . . . . . . .
179
14.2
An Example of Error Ellipses Overplotted on the DIFAR UKF Track
Output.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
14.3
Comparison of the RMS Errors for Five Different Track Estimation
Algorithms with the Signal SNR at 20 dB. . . . . . . . . . . . . . . .
193
14.4
Comparison of the RMS Errors for Five Different Track Estimation
Algorithms with the Signal SNR at 15 dB. . . . . . . . . . . . . . . .
194
14.5
Comparison of the RMS Errors for Five Different Track Estimation
Algorithms with the Signal SNR at 10 dB. . . . . . . . . . . . . . . .
195
14.6
Comparison of the RMS Errors for Five Different Track Estimation
Algorithms with the Signal SNR at 5 dB.
. . . . . . . . . . . . . . .
196

LIST OF FIGURES
xxi
14.7
Comparison of the RMS Errors for Five Different Track Estimation
Algorithms with the Signal SNR at 0 dB.
. . . . . . . . . . . . . . .
197
15.1
Samples Drawn from a Two-Dimensional Gaussian Mixture
Density. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
15.2
Two-Dimensional Histogram Based on Samples from a Gaussian
Mixture Distribution. . . . . . . . . . . . . . . . . . . . . . . . . . .
204
15.3
Gaussian Kernel Density Estimate for Sparse Sample Data. . . . . . .
207
15.4
Visualization of the Gaussian Kernel Estimate of a Density Generated
from Random Samples Drawn for that Density. . . . . . . . . . . . .
210
15.5
Example of the Creation of w(x). . . . . . . . . . . . . . . . . . . . .
212
16.1
Inverse of the Gaussian Cumulative Distribution. . . . . . . . . . . .
223
16.2
Comparison of a Gaussian CDF with Its Inverse.
. . . . . . . . . . .
224
16.3
Principle of Resampling. . . . . . . . . . . . . . . . . . . . . . . . .
228
16.4
Target Track Generated from a DIFAR Buoy Field Using a Bootstrap
Particle Filter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
16.5
A Comparison of Track Outputs at Six Different SNRs for the
BPF tracker. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
16.6
Track Estimation Results for the Auxiliary Particle Filter Applied to
the DIFAR Tracking Case Study. . . . . . . . . . . . . . . . . . . . .
242
17.1
Process Flow Diagram for the Gaussian Particle Filter.
. . . . . . . .
249
17.2
Combination Particle Filter that Uses an EKF as an Importance
Density. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
250
17.3
Combination Particle Filter that Uses a Sigma Point Kalman Filter as
an Importance Density. . . . . . . . . . . . . . . . . . . . . . . . . .
251
17.4
Monte Carlo Track Plots for the Sigma Point Gaussian Particle Filter
for Six SNRs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252
17.5
Comparison of the DIFAR Case-Study Root Mean Squared Position
Errors for a Signal SNR of 20 dB. . . . . . . . . . . . . . . . . . . .
253
17.6
Comparison of the DIFAR Case-Study Root Mean Squared Position
Errors for a Signal SNR of 15 dB. . . . . . . . . . . . . . . . . . . .
254
17.7
Comparison of the DIFAR Case-Study Root Mean Squared Position
Errors for a Signal SNR of 10 dB. . . . . . . . . . . . . . . . . . . .
255
18.1
A Simulated Radially Inbound Target Truth Track.
. . . . . . . . . .
278
18.2
A Simulated Horizontally Looping Target Truth Track. . . . . . . . .
279

xxii
LIST OF FIGURES
18.3
A Simulated Benchmark Target Truth Track.
. . . . . . . . . . . . .
279
18.4
Components of Both the Cartesian and Spherical Velocities for the
Radially Inbound Trajectory. . . . . . . . . . . . . . . . . . . . . . .
280
18.5
Components of Both the Cartesian and Spherical Velocities for the
Loop Trajectory.
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
18.6
Components of Both the Cartesian and Spherical Velocities for the
Benchmark Trajectory. . . . . . . . . . . . . . . . . . . . . . . . . .
281
18.7
Estimated Tracks for the Radially Inbound Target Trajectory for All
Cartesian Tracking Methods. . . . . . . . . . . . . . . . . . . . . . .
283
18.8
Estimated Tracks for the Radially Inbound Target Trajectory for All
Spherical Tracking Methods. . . . . . . . . . . . . . . . . . . . . . .
283
18.9
RMS Cartesian Position Errors of the Radially Inbound Target
Trajectory for the Cartesian and Spherical Tracking Algorithms. . . .
284
18.10 RMS Spherical Position Errors of the Radially Inbound Target
Trajectory for the Cartesian and Spherical Tracking Algorithms. . . .
284
18.11 Comparison of the Radially Inbound Trajectory RMS Cartesian
Position Errors for Varying Values of q for Both Cartesian and
Spherical EKF Filters.
. . . . . . . . . . . . . . . . . . . . . . . . .
285
18.12 Comparison of the Radially Inbound Trajectory RMS Spherical
Position Errors for Varying Values of q for Both Cartesian and
Spherical EKF Filters.
. . . . . . . . . . . . . . . . . . . . . . . . .
286
18.13 Estimated Tracks for the Looping Target Trajectory for All Cartesian
Tracking Methods.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
287
18.14 Estimated Tracks for the Looping Target Trajectory for All Spherical
Tracking Methods.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
287
18.15 RMS Cartesian and Spherical Position Errors of the Loop Target
Trajectory for the Cartesian Tracking Algorithms. . . . . . . . . . . .
288
18.16 RMS Cartesian and Spherical Position Errors of the Loop Target
Trajectory for the Spherical Tracking Algorithms. . . . . . . . . . . .
288
18.17 Comparison of RMS Cartesian Position Error Performance as a
Function of q for the Loop Trajectory. . . . . . . . . . . . . . . . . .
289
18.18 Comparison of RMS Spherical Position Error Performance as a
Function of q for the Loop Trajectory. . . . . . . . . . . . . . . . . .
289
18.19 Estimated Tracks for the Benchmark Target Trajectory for All Cartesian
Tracking Methods.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
290

LIST OF FIGURES
xxiii
18.20 Estimated Tracks for the Benchmark Target Trajectory for All Spherical
Tracking Methods.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
290
18.21 RMS Cartesian and Spherical Position Errors of the Benchmark Target
Trajectory for the Cartesian Tracking Algorithms. . . . . . . . . . . .
291
18.22 RMS Cartesian and Spherical Position Errors of the Benchmark Target
Trajectory for the Spherical Tracking Algorithms. . . . . . . . . . . .
291
18.23 Comparison of RMS Cartesian Position Error Performance as a
Function of q for the Benchmark Trajectory. . . . . . . . . . . . . . .
292
18.24 Comparison of RMS Spherical Position Error Performance as a
Function of q for the Benchmark Trajectory. . . . . . . . . . . . . . .
293
18.25 The East–North–Up Cartesian Coordinate System.
. . . . . . . . . .
294
18.26 Depiction of a Constant Rate Turn in Both the Horizontal and Vertical
Planes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
19.1
Photo of a US Navy F-18E Super Hornet Aircraft Preparing to Release
Four Mk-62 Stores. . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
19.2
Close-up of a MK-62 Store Release as Viewed from a Camera Mounted
Under the Aircrafts’s Tail.
. . . . . . . . . . . . . . . . . . . . . . .
310
19.3
Projection of a Feature Point onto a Camera’s Image Plane. . . . . . .
319
19.4
Translational and Rotational Position Using a Second-Order Model
with a UKF Filter. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
334
19.5
Translational and Rotational Velocity Using a Second-Order Model
with a UKF Filter. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
19.6
Translational and Rotational Acceleration Using a Second-Order
Model with a UKF Filter. . . . . . . . . . . . . . . . . . . . . . . . .
336
19.7
Lateral Position Estimates from Multiple Solvers Using Identical
Inputs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
19.8
RMS Errors for Position and Orientation of All Tracking Filters Using
Synthetic Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
339
19.9
RMS Errors for Position and Orientation Velocities of All Tracking
Filters Using Synthetic Data. . . . . . . . . . . . . . . . . . . . . . .
340
19.10 RMS Errors for Position and Orientation Accelerations of All Tracking
Filters Using Synthetic Data. . . . . . . . . . . . . . . . . . . . . . .
341
20.1
A Free-Standing IMU with Transmitting Antenna Attached, and An
IMU Mounted in the Forward Fuse Well of a 500 Pound Store. . . . .
348
20.2
Translational and Rotational Position Using the Sensor Fusion
Estimator. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357

xxiv
LIST OF FIGURES
20.3
Translational and Rotational Velocity Using the Sensor Fusion
Estimator. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
358
20.4
Translational and Rotational Acceleration Using the Sensor Fusion
Estimator. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
20.5
Lateral Position Estimates from Multiple Estimation Filters Using
Identical Measurement Inputs. . . . . . . . . . . . . . . . . . . . . .
360
20.6
RMS Errors for Position and Orientation of All Tracking Filters Using
Synthetic Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
20.7
RMS Errors for Position and Orientation Velocities of All Tracking
Filters Using Synthetic Data. . . . . . . . . . . . . . . . . . . . . . .
363
20.8
RMS Errors for Position and Orientation Accelerations of All Tracking
Filters Using Synthetic Data. . . . . . . . . . . . . . . . . . . . . . .
364

LIST OF TABLES
4.1
Procedure for Generating a Vector of DIFAR Noisy Bearing
Observations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
6.1
Linear Kalman Filter Process.
. . . . . . . . . . . . . . . . . . . . .
89
7.1
One-Dimensional Extended Kalman Filter Process. . . . . . . . . . .
98
7.2
Multidimensional Extended Kalman Filter Process. . . . . . . . . . .
106
8.1
One-Dimensional Finite Difference Kalman Filter Process. . . . . . .
119
8.2
Multidimensional Finite Difference Kalman Filter Process. . . . . . .
126
9.1
Multidimensional Unscented Kalman Filter Process.
. . . . . . . . .
136
9.2
Multidimensional Sigma Point Kalman Filter Process Applied to
DIFAR Tracking. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
10.1
Multidimensional Spherical Simplex Kalman Filter Process. . . . . .
146
11.1
Multidimensional Gauss–Hermite Kalman Filter Process. . . . . . . .
161
12.1
Multidimensional Spherical Simplex Kalman Filter Process. . . . . .
166
13.1
Summary Data for Sigma Point Kalman Filters: Part 1—Form of c
to Be Used for Sigma Points. . . . . . . . . . . . . . . . . . . . . . .
171
13.2
Summary Data for Sigma Point Kalman Filters: Part 2—Form of
Sigma Point Weights. . . . . . . . . . . . . . . . . . . . . . . . . . .
171
13.3
Comparison of the Number of Integration Points Required for the
Various Sigma Point Kalman Filters. . . . . . . . . . . . . . . . . . .
172
xxv

xxvi
LIST OF TABLES
13.4
Hierarchy of Dynamic and Observation Models. . . . . . . . . . . . .
174
15.1
Common Univariate Kernel Functions of Order 2. . . . . . . . . . . .
206
15.2
One-Dimensional Kernel Sample Generation. . . . . . . . . . . . . .
208
15.3
Multidimensional Kernel Sample Generation. . . . . . . . . . . . . .
210
16.1
General Sequential Importance Sampling Particle Filter.
. . . . . . .
221
16.2
Sequential Importance Sampling Particle Filter with
Resampling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
16.3
Sequential Importance Sampling Particle Filter with Resampling
and Regularization. . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
16.4
Bootstrap SIS Particle Filter with Resampling and
Regularization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
16.5
Optimal SIS Particle Filter with Resampling and Regularization. . . .
237
16.6
Auxiliary Particle Filter Process Flow. . . . . . . . . . . . . . . . . .
241
16.7
Unscented Particle Filter with Resampling and Regularization. . . . .
244
18.1
Nominal Values for Initialization State Vector and Covariance
Components.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
18.2
Initial Characteristics of Three Simulated Scenarios.
. . . . . . . . .
280
19.1
Synthetic Data RMS Positional Error Summary. . . . . . . . . . . . .
338
19.2
Synthetic Data RMS Velocity Error Summary. . . . . . . . . . . . . .
338
19.3
Synthetic Data RMS Acceleration Error Summary. . . . . . . . . . .
339
20.1
Synthetic Data RMS Positional Error Summary. . . . . . . . . . . . .
361
20.2
Synthetic Data RMS Velocity Error Summary. . . . . . . . . . . . . .
361
20.3
Synthetic Data RMS Acceleration Error Summary. . . . . . . . . . .
361

PART I
PRELIMINARIES

1
INTRODUCTION
Estimation and tracking of dynamic systems has been the research focus of many a
mathematician since the dawn of statistical mathematics. Many estimation methods
have been developed over the past 50 years that allow statistical inference (estimation)
for dynamic systems that are linear and Gaussian. In addition, at the cost of increased
computational complexity, several methods have shown success in estimation when
applied to nonlinear Gaussian systems. However, real-world dynamic systems, both
linear and nonlinear, usually exhibit behavior that results in an excess of outliers,
indicative of non-Gaussian behavior. The toolbox of standard Gaussian estimation
methods have proven inadequate for these problems resulting in divergence of the
estimation ﬁlters when applied to such real-world data.
With the advent of high-speed desktop computing, over the past decade the empha-
sis in mathematics has shifted to the study of dynamic systems that are non-Gaussian
in nature. Much of the literature related to performing inference for non-Gaussian
systems is highly mathematical in nature and is lacking in practical methodology
that the average engineer can utilize without a lot of effort. In addition, several
of the Gaussian methods related to estimation for nonlinear systems are presented
ad hoc, without a cohesive derivation. Finally, there is a lack of continuity in the con-
ceptual development to date between the Gaussian methods and their non-Gaussian
counterparts.
In this book, we will endeavor to present a comprehensive study of the methods cur-
rently in use for statistical dynamic system estimation: linear and nonlinear, Gaussian
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
3

4
INTRODUCTION
and non-Gaussian. Using a Bayesian framework, we will present a conceptually cohe-
sive roadmap that starts at ﬁrst principles and leads directly to derivations of many of
the Gaussian estimation methods currently in use. We will then extend these concepts
into the non-Gaussian estimation realm, where the theory leads directly to working
Monte Carlo methods for estimation. Although the Bayesian approach leads to the
estimation of statistical densities, in most cases we will develop point estimation meth-
ods that can be obtained through the evaluation of density-weighted integrals. Thus,
this book is all about numerical methods for evaluating density-weighted integrals
for both Gaussian and non-Gaussian densities.
For each estimation method that we discuss and derive, we present both pseudo-
code and graphic block diagram that can be used as tools in developing a software-
coded tracking toolbox. As an aid in understanding the methods presented, we also
discuss what is required to develop simulations for several very speciﬁc real-world
problems. These case-study problems will be addressed in great detail, with track
estimation results presented for each. Since it is hard to compare tracking methods
ad hoc, we also present multiple methods to evaluate the relative performance of the
various tracking ﬁlters.
1.1
BAYESIAN INFERENCE
Inference methods consist of estimating the current values for a set of parameters
based on a set of observations or measurements. The estimation procedure can follow
one of two models. The ﬁrst model assumes that the parameters to be estimated,
usually unobservable, are nonrandom and constant during the observation window
but the observations are noisy and thus have random components. The second model
assumes that the parameters are random variables that have a prior probability and the
observations are noisy as well. When the ﬁrst model is used for parameter estimation,
the procedure is called non-Baysian or Fisher estimation [1]. Parameter estimation
using the second model is called Bayesian estimation.
Bayesian estimation is conceptually very simple. It begins with some initial prior
belief, such as the statement “See that ship. It is about 1000 yards from shore and
is moving approximately Northeast at about 10 knots.” Notice that the initial belief
statement includes an indication that our initial guess of the position and velocity of
the ship are uncertain or random and based on some prior probability distribution.
Based on one’s initial belief, one can then make the prediction “Since the ship appears
to be moving at a constant velocity, it will be over there in about 10 minutes.” This
statement includes a mental model of the ship motion dynamics as well as some
additional uncertainty. Suppose now, that one has a small portable radar on hand.
The radar can be used to measure (observe) the line-of-sight range and range rate of
the ship to within some measure of uncertainty. Given the right mathematical model,
one that links the observations to the Cartesian coordinates of the ships position and
velocity, a current radar measurement can be used to update the predicted ships state
(position and velocity).

BAYESIAN HIERARCHY OF ESTIMATION METHODS
5
The above paragraph contains the essence of recursive Bayesian estimation:
1. begin with some prior belief statement,
2. use the prior belief and a dynamic model to make a prediction,
3. update the prediction using a set of observations and an observation model to
obtain a posterior belief, and
4. declare the posterior belief our new prior belief and return to 2.
This concept was ﬁrst formalized in a paper by the Reverend Thomas Bayes, read
to the Royal Statistical Society in 1763 by Richard Price several years after Bayes’
death. An excellent review of the history and concepts associated with Bayesian sta-
tistical inference can be found in the paper by Stephen Brooks [2]. Brooks’ paper also
has some interesting examples that contrast the Bayesian method with the so-called
“Frequentist” method for statistical inference. Since this book is devoted completely
to Bayesian methods, we will not address the frequentist approach further and refer
the interested reader to Brooks’ paper.
1.2
BAYESIAN HIERARCHY OF ESTIMATION METHODS
As noted above, in this book we will present a cohesive derivation of a subset of
modern tracking ﬁlters. Figure 1.1 shows the hierarchy of tracking ﬁlters that will
Finite
difference
Kalman ﬁlter
Nonlinear Gaussian trackers
Numerical integration
Sequential particle ﬁlters
Generalized
composite
particle ﬁlters
Analytical linearization
Linear
Kalman
ﬁlter
Gauss–Hermite
Kalman
ﬁlter
Extended
Kalman
ﬁlter
Unscented
Kalman
ﬁlter
Spherical
simplex
Kalman
ﬁlter
Monte
Carlo
Kalman
ﬁlter
Sequential
importance
sampling
particle
ﬁlters
Bootstrap
particle
ﬁlter
Auxiliary
particle
ﬁlter
Gaussian
particle
ﬁlters
 Monte Carlo trackers
Linear Gaussian trackers
Optimal
particle
ﬁlter
Bayesian trackers
Gaussian trackers
FIGURE 1.1
Hierarchy of Bayesian estimation tracker ﬁlters.

6
INTRODUCTION
be addressed in this book. Along the left-hand side are all the Gaussian tracking
ﬁlters and along the right-hand side are all of the Monte Carlo non-Gaussian ﬁlters.
This ﬁgure will be our guide as we progress through our discussions on each tracking
ﬁlter. We will use it to locate where we are in our developments. We may occasionally
take a side trip into other interesting concepts, such as a discussion of performance
measures, but for the most part we will stick to a systematic development from top to
bottom and left to right. By the time we reach the bottom right, you the reader will
have a comprehensive understanding of the interrelatedness of all of the Bayesian
tracking ﬁlters.
1.3
SCOPE OF THIS TEXT
1.3.1
Objective
The objective of this book is to give the reader a ﬁrm understanding of Bayesian
estimation methods and their interrelatedness. Starting with the ﬁrst principles of
Bayesian theory, we show how each tracking ﬁlter is derived from a slight modiﬁcation
to a previous ﬁlter. Such a development gives the reader a broader understanding of
the hierarchy of Bayesian estimation and tracking. Following the discussions about
each tracking ﬁlter, the ﬁlter is put into both pseudo-code and process ﬂow block
diagram form for ease in future recall and reference.
In his seminal book on ﬁltering theory [3], originally published in 1970, Jazwinski
stated that “The need for this book is twofold. First, although linear estimation theory
is relatively well known, it is largely scattered in the journal literature and has not been
collected in a single source. Second, available literature on the continuous nonlinear
theoryisquiteesotericandcontroversial,andthusinaccessibletoengineersuninitiated
in measure theory and stochastic differential equations.” A similar statement can be
made about the current state of affairs in non-Gaussian Monte Carlo methods of
estimation theory. Most of the published work is esoteric and inaccessible to engineers
uninitiated in measure theory. The edited book of invited papers by Doucet et al. [4] is
a prime example. This is an excellent book of invited papers, but is extremely esoteric
in many of its stand-alone sections.
In this book, we will take Jazwinski’s approach and remove much of the eso-
teric measure theoretic-based mathematics that makes understanding difﬁcult for the
average engineer. Hopefully, we have not replaced it with equally esoteric alternative
mathematics.
1.3.2
Chapter Overview and Prerequisites
This book is not an elementary book and is intended as a one semester graduate
course or as a reference for anyone requiring or desiring a deeper understanding
of estimation and tracking methods. Readers of this book should have a graduate
level understanding of probability theory similar to that of the book by Papoulis [5].
The reader should also be familiar with matrix linear algebra and numerical methods

SCOPE OF THIS TEXT
7
including ﬁnite differences. In an attempt to reduce the steep prerequisite requirements
for the reader, we have included several review sections in the next chapter on some of
these mathematical topics. Even though some readers may want to skip these sections,
the material presented is integral to an understanding of what is developed in Parts II
and III of this book.
Part I consists of this introduction followed by a chapter that presents an overview
of some mathematical principles required for an understanding of the estimation
methods that follow. The third chapter introduces the concepts of recursive Bayesian
estimation for a dynamic system that can be modeled as a potentially unobservable
discrete Markov process. The observations (measurements) are related to the system
states through an observation model and the observations are considered to be discrete.
Continuous estimation methods are generally not considered in this book. The last
chapter of Part I is devoted to preliminary development of a case study that will be used
as working examples throughout the book, the problem of tracking a ship through a
distributed ﬁeld of directional frequency analysis and recording (DIFAR) sonobuoys.
Included for this case study will be demonstrations of methods for development of
complete simulations of the system dynamics along with the generation of noisy
observations.
Part II is devoted to the development and application of estimation methods for the
Gaussian noise case. In Chapter 5, the general Bayesian estimation methods devel-
oped in Chapter 3 are rewritten in terms of Gaussian probability densities. Methods
for speciﬁc Gaussian Kalman ﬁlters are derived and codiﬁed in Chapters 6 through
12, including the linear Kalman ﬁlter (LKF), extended Kalman ﬁlter (EKF), ﬁnite
difference Kalman ﬁlter (FDKF), unscented Kalman ﬁlter (UKF), spherical sim-
plex Kalman ﬁlter (SSKF), Gauss–Hermite Kalman ﬁlter (GHKF), and the Monte
Carlo Kalman ﬁlter (MCKF). With the exception of the MCKF, four of latter ﬁve
tracking ﬁlters can be lumped into the general category of sigma point Kalman ﬁl-
ters where deterministic vector integration points are used in the evaluation of the
Gaussian-weighted integrals needed to estimate the mean and covariance matrix of
the state vector. In the MCKF, the continuous Gaussian distribution is replaced by
a sampled distribution reducing the estimation integrals to sums leaving the nonlin-
ear functions intact. It will be shown in Chapter 13 that the latter ﬁve Kalman ﬁlter
methods can be summarized into a single estimation methodology requiring just a
change in the number and location of the vector points used and their associated
weights.
An important aspect of estimation, usually ignored in most books on estimation,
is the quantiﬁcation of performance measures associated with the estimation meth-
ods. In Chapter 14 this topic is addressed, with sections on methods for computing
and plotting error ellipses based on the estimated covariance matrices for use in
real-system environments, as well as methods for computing and plotting root mean
squared (RMS) errors and their Cramer–Rao lower bounds (CRLB) for use in Monte
Carlo simulation environments. The ﬁnal section of this chapter is devoted to applica-
tion of these estimation methods to the DIFAR buoy tracking case study and includes
a comparison of performance results as a function of decreasing input signal-to-noise
ratio (SNR).

8
INTRODUCTION
Estimation methods for use primarily with non-Gaussian probability densities is
the topic addressed in Part III. For the MCKF introduced in Chapter 12 of Part II, the
Gaussian density is approximated by a set of discrete Monte Carlo samples, reducing
the mean and covariance estimation integrals to weighted sums, usually referred
to as sample mean and sample covariance, respectively. For Gaussian densities, the
sample weight is always 1/N, where N is the number of samples used. Non-Gaussian
densities present two problems: ﬁrst, it is usually very difﬁcult to generate a set
of Monte Carlo samples directly from the density. A second problem arises if the
ﬁrst or second moment does not exist for the density, with the Cauchy density as
a prime example. To address the sampling problem, in Chapter 15 Monte Carlo
methods are introduced and the concept of importance sampling developed that leads
to estimation methods called particle ﬁlters, where the particles are the Monte Carlo
sample points. Several problems arise when implementing these particle ﬁlters and
potential enhancements are considered that correct for these problems. For importance
sampling, weighting for each sample is calculated as the ratio of the non-Gaussian
density to the importance density at the sample point. Under certain assumptions,
the weights can be calculated recursively, giving rise to the sequential importance
sampling (SIS) class of particle ﬁlters, the topic of Chapter 16. In Chapter 17, the
case where the weights are recalculated every ﬁlter iteration step is addressed, leading
to the Gaussian class of combination particle ﬁlters. Performance results for all of the
particle ﬁlter track estimation methods applied to the DIFAR case study are presented
as the conclusion of Chapter 17.
Several recently published books provide additional insight into the topics
presented in this book. For Gaussian Kalman ﬁlters of Part II, books by Bar Shalom
et al. [6] and Candy [7] are good companion books. For non-Gaussian ﬁltering meth-
ods of Part III, books by Doucet et al. [4] and Ristic et al. [8] are excellent reference
books.
1.4
MODELING AND SIMULATION WITH MATLAB®
It is important to the learning process that the reader be given concrete examples of
application of estimation methods to a set of complex problems. This will be accom-
plished in this book through the use of simulations using MATLAB®. We present a
set of four case studies that provide an increase in complexity from the ﬁrst to the last.
Each case study will include an outline of how to set up a simulation that models both
the dynamics and observations of the system under study. We then show how to create
a set of randomly generated observational data using a Monte Carlo methodology.
This simulated observational data can then be used to exercise each tracking ﬁlter,
producing sets of track data that can be compared across multiple track ﬁlters.
The ﬁrst case study examines the problem of tracking a ship as it moves through
a distributed ﬁeld of DIFAR buoys. A DIFAR buoy uses the broadband noise signal
radiated from the ship as in input and produces noisy observations of the bearing to
the ship as an output. As we will show in Chapter 4, the probability density of the
bearing estimates at the DIFAR buoy output is dependent on the SNR of the input

REFERENCES
9
signal. The density will be Gaussian for high SNR but will transition to a uniform
distribution as the SNR falls. The purpose of this case study will be to examine what
happens to the ﬁlter tracking performance for each track estimation method as the
observation noise transitions from Gaussian to non-Gaussian.
This DIFAR case study will be the primary tool used throughout this book to
illustrate each track estimation ﬁlter in turn. In Chapter 4, we show how to set up a
simulation of the DIFAR buoy processing so as to produce simulated SNR dependent
observation sets. Using these sets of bearing observations, in subsequent chapters
we exercise each tracking algorithm to produce Monte Carlo sets of track estimates,
allowing us to see the impact of the Gaussian to non-Gaussian observation noise
transition on each tracking method.
In Part IV of this book, we present three additional case studies that illustrate the
use of many of the tracking ﬁlters developed in Parts II and III. In Chapter 18, we
address the important problem of tracking a maneuvering object in three dimension
space. In this chapter, we introduce a new approach that uses a constant spherical
velocity model vice the more traditional constant Cartesian velocity model. We show
how this spherical model shows improved performance for tracking a maneuvering
object using most of the Gaussian tracking ﬁlters.
The third case study, found in Chapter 19, considers the rather complex problem of
tracking the dynamics of a falling bomb through the use of video frames of multiple
tracking points on both the plane dropping the bomb and the bomb itself. This is a
particular example of a complex process called photogrammetry, in which the geo-
metric and dynamic properties of an object are inferred from successive photographic
image frames. Thus, this case study consists of a very complex nonlinear multidimen-
sional observational process as well as a nonlinear multidimensional dynamic model.
In addition, both the dynamic and observational models are of high dimension, a
particularly taxing problem for tracking ﬁlters. This will illustrate the effects of the
so-called “curse” of dimensionality, showing that it is computationally impractical to
utilize all tracking ﬁlters.
The ﬁnal case study, the topic of Chapter 20, improves on the use of photogrammet-
ric methods in estimation by showing how a separate estimator can be used for fusing
data from additional sensors, such as multiple cameras, translational accelerome-
ters, and angular rate gyroscopes. When used independently, each data source has its
unique strengths and weaknesses. When several different sensors are used jointly in
an estimator, the resulting solution is usually more accurate and reliable. The result-
ing analysis shows that estimator aided sensor fusion can recover meaningful results
from ﬂight tests that would otherwise have been considered failures.
REFERENCES
1. Fisher R. Statistical Methods and Scientiﬁc Inference, Revised Edition. Macmillan Pub.
Co.; 1973.
2. Brooks SP. Bayesian computation: a statistical revolution. Phil. Trans. R. Soc. Lond. A
2003;361:2681–2697.

10
INTRODUCTION
3. Jazwinski AH. Stochastic Processes and Filtering Theory. Academic Press (1970), recently
republished in paperback by Dover Publications; 2007.
4. Doucet A, de Freitas JFG, Gordon NJ, editors. Sequential Monte Carlo Methods in Practice.
New York, NY: Springer-Verlag; 2001.
5. Papoulis A. Probability, Random Variables, and Stochastic Processes, 4th ed. McGraw-Hill;
2002.
6. Bar Shalom Y, Li XR, Kirubarajan T. Estimation with Application to Tracking and Naviga-
tion: Theory, Algorithms and Software. Wiley; 2001.
7. Candy JV. Bayesian Signal Processing: Classical, Modern, and Particle Filtering Methods.
Hoboken, NJ: Wiley; 2009.
8. Ristic B, Arulampalam S, Gordon N. Beyond the Kalman Filter: Particle Filters for
Tracking Applications. Boston, MA: Artech House; 2004.

2
PRELIMINARY MATHEMATICAL
CONCEPTS
In this chapter, we will present a host of mathematical concepts that are essential
to an understanding of what follows in Parts II–IV. The mathematical developments
presented here will be concise and without extensive or rigorous proofs. For most
of the topics covered here, it will be assumed that the reader is familiar with the
mathematical concepts from previous exposure. Although it is not recommended,
this chapter can be skipped on ﬁrst reading and used as a reference while reading
the remainder of Part I and Parts II–IV. Additional mathematical concepts will be
introduced as needed.
2.1
A VERY BRIEF OVERVIEW OF MATRIX LINEAR ALGEBRA
This section deﬁnes notational conventions and matrix operations used throughout
the text and provides a brief summary of some matrix linear algebra concepts within
the context of MATLAB®. An extremely good reference for the matrix operations
and formulas presented in this section can be found online in Ref. [1].
2.1.1
Vector and Matrix Conventions and Notation
A scalar will usually be presented as lower case, a, a vector as lower case bold, a, and
a matrix as upper case bold, A.
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
11

12
PRELIMINARY MATHEMATICAL CONCEPTS
An n-vector is the n × 1 column vector
a =


a1
...
an


(2.1)
By convention, all vectors will be column vectors. It follows immediately that one can
write a column vector as the transpose of a row vector (especially useful to conserve
space in a report or journal article)
a =

a1,
· · · ,
an
⊺
(2.2)
with commas separating the elements of the vector. [·]⊺represent a vector or matrix
transpose, which will be deﬁned below.
An n × m matrix is a two-dimensional array of the form
A =


a11
· · ·
a1m
...
...
...
an1
· · ·
anm


(2.3)
The ﬁrst dimension is the number of rows and the second is the number of columns.
The ijth component of the matrix A is aij.
The transpose of the matrix A is designated as A⊺and is deﬁned by
A⊺=


a11
· · ·
an1
...
...
...
a1m
· · ·
anm


(2.4)
The trace of an n × n square matrix A is deﬁned to be the sum of the elements on
the main diagonal (the diagonal from the upper left to the lower right) of A, that is,
trace {A} =
n
	
i=1
aii
(2.5)
Equivalently, the trace of a matrix is the sum of its eigenvalues, making it an invariant
with respect to a change of basis. This characterization can be used to deﬁne the trace
for a linear operator in general.
We will occasionally use the notation A ∈Rn×m or a ∈Rn to designate the
dimension of a matrix or vector, respectively.
2.1.2
Sums and Products
Addition of matrices and multiplication by a scalar are deﬁned as
C = αA + βB
(2.6)

A VERY BRIEF OVERVIEW OF MATRIX LINEAR ALGEBRA
13
where
cij = αaij + βbij;
i = 1, . . . , n; j = 1, . . . , m
(2.7)
The product of two matrices, for matrix A ∈Rn×k and matrix B ∈Rk×m, is written
as
C = AB
(2.8)
with the elements of the matrix C ∈Rn×m given by
cij =
k
	
l=1
ailblj
(2.9)
Note that for matrix multiplication, the number of columns of the matrix A must
match the number of rows of matrix B.
The Hadamard product of two matrices of identical dimension is the element-by-
element product matrix
C = A ⊙B
(2.10)
or
cij = aijbij
(2.11)
Remark 2.1
Within MATLAB®, a full matrix multiplication is represented as C =
A*B, while the Hadamard product is represented as C = A.*B.
2.1.3
Matrix Inversion
For a square matrix A, its inverse is a matrix A−1 that satisﬁes the equation
AA−1 = A−1A = I
(2.12)
where I is an identity matrix of the same dimensions as A. An identity matrix is a
square matrix of zeros with ones along the diagonal. For some matrices an inverse
does not exist.
Matrices without an inverse are called singular matrices. All elements of the inverse
of a matrix contain a division by the determinant of the matrix. An easy way to
determine if a matrix is singular is to calculate its determinant. If the determinant is
zero, the matrix is singular.
There are many ways to compute the inverse of a matrix. For small matrices,
one can use analytical methods using cofactors and matrix determinants. For larger
matrices, numerical methods, such as Gauss–Jordon elimination or LU decomposition
are used. If the matrix is symmetric and positive deﬁnite, LU decomposition reduces
to Cholesky factorization. See a text on matrix linear algebra for the particulars of
each technique.

14
PRELIMINARY MATHEMATICAL CONCEPTS
Remark 2.2 Within MATLAB®, there are two predominant methods to ﬁnd the solu-
tion of the equation A ∗x = b, one of which is preferred. The less favorable, but most
straightforward method is to take x = A−1∗b literally: x = inv(A)*b. Here, inv(A) is
a MATLAB® function that computes A−1 analytically and the operator * represents
matrix multiplication. The preferable solution is found using the matrix left division
operator or backward slash: x = A\b. This later method uses LU decomposition
or Cholesky factorization in a very efﬁcient way depending on the properties of the
matrix A.
As a special note, occasionally just the inverse of a matrix is needed and can be
efﬁciently obtained using B = A\eye(size(A)), where B is the inverse of A. Here,
eye(N) is a MATLAB® function that produce an identity matrix ∈RN×N that has
ones along the diagonal and zeros everywhere else. The eye MATLAB® function has
other properties that can produce nonsquare matrices, but they will not be discussed
here.
If the matrix linear algebra equation is x ∗A = b, then x = b ∗A−1, and the
MATLAB® expression x = A/b is used. In a similar way, MATLAB picks the optimal
algorithm for the computation based on the properties of A and b.
Finally, if only an element-by-element division of two matrices of identical dimen-
sion is desired, the MATLAB® expression C = A./B is used.
2.1.4
Block Matrix Inversion
For block matrices, there are several general formulas for inversion.
Lemma 2.1
Matrix Inversion Lemma: Let A, C and

C−1 + DA−1B

be non-
singular square matrices. Then
(A + BCD)−1 = A−1 −A−1B

C−1 + DA−1B
−1
DA−1
(2.13)
Lemma 2.2
Matrix Inversion in Block Form: Let an n × m matrix M be partitioned
into a block form
M =

A
B
C
D

} n
} m
(2.14)
 
n
m
where the n × n matrix A and the m × m matrix D are invertible. Consider the product
of two matrices such that

A
B
C
D
 
X
Y
Z
U

=

In
0
0
Im

(2.15)

A VERY BRIEF OVERVIEW OF MATRIX LINEAR ALGEBRA
15
Then, it follows immediately that

X
Y
Z
U

≜

A
B
C
D
−1
(2.16)
The block matrix inverse can be calculated from
M−1 =


A −BD−1C
−1
−A−1B

D + CA−1B
−1
−D−1C

A −BD−1C
−1

D + CA−1B
−1

(2.17)
Or, one can calculate the block matrix inverse from

X
Y
Z
U
 
A
B
C
D

=

In
0
0
Im

(2.18)
which results in the equivalent block matrix inverse
M−1 =


A −BD−1C
−1
−

A −BD−1C
−1 BD−1
−

D + CA−1B
−1 CA−1

D + CA−1B
−1

(2.19)
2.1.5
Matrix Square Root
For any nonsingular square matrix A, the square root of A is deﬁned as any matrix B
that satisﬁes the equation
A = BB⊺
(2.20)
It is not actually a square root, but is rather a matrix decomposition.
Remark 2.3 In MATLAB®, there is a special function B = sqrtm(A) for computing
the principal (positive) square root of the matrix A. B is the unique square root for
which every eigenvalue has a nonnegative real part. If A has any eigenvalues with
negative real parts, then a complex result is produced. If A is singular, then A may
not have a square root. MATLAB® displays an error message if an exact singularity
is detected.
An alternate method uses a Cholesky decomposition to compute a lower triangular
matrix B that has strictly positive diagonal elements satisfying the properties of a ma-
trix square root. B = chol(A) produces an upper triangular matrix B⊺from the matrix
A, satisfying the equation A = BB⊺. The lower triangular matrix B is assumed to be
the (complex conjugate) transpose of the upper triangle matrix B⊺. Matrix A must be
positive deﬁnite; otherwise, MATLAB® displays an error message.
Although each MATLAB® method for computing the matrix square root results in
a different matrix B, they are both valid and differ only in the numerical technique
used. Which method used will depend on the speciﬁc application.

16
PRELIMINARY MATHEMATICAL CONCEPTS
2.2
VECTOR POINT GENERATORS
In the discussions on sigma point Kalman ﬁlters (Chapters 8–12), we will make use
of a simplifying notation for the generation of the vector sigma points that are used
for multidimensional numerical integration.
We deﬁne the n-dimensional vector generator function
u = (u1, . . . , ur, 0, . . . , 0) ∈Rn
(2.21)
where 0 ≤ui ≤uj if i ≤j [2,3]. In this notation, Rn speciﬁes the dimension of the
vector points generated by u and it is understood that ui represents ±ui. Such a
generator will be denoted as either
[u] ∈Rn
(2.22)
or
[u1, . . . , ur] ∈Rn
(2.23)
where u ≜[u1, . . . , ur], with the zero coordinates suppressed for convenience. This
notation represents the set of k multidimensional vector points that can be generated
from u by permutations and changing the sign of some coordinates.
In Part II, we will be considering rules for multidimensional integration of
Gaussian-weighted integrals. Our numerical solutions will involve the evaluation
of nonlinear functions at sets of vector points that consist of a constant times a set of
vector points on unit hyperspheres or hypercubes. Hence, we will be using generators
to create sets of vector points of the form [1, 1, 1] ∈Rn. Note that [1, 1, 1] ∈Rn
is equivalent to all possible permutations of [±1, ±1, ±1, 0, . . . , 0] where there are
n −3 zeros. Some examples are presented here. In the examples, we will use the
notation [3]
n(k) ≜n (n −1) · · · (n −k + 1)
(2.24)
n(k) ≜n (n −1) · · · (n −k + 1)
k!
(2.25)
Consider the case where u = 1. Some examples of the vector points generated for a
variety of cases are given below:
■EXAMPLE 2.1
For [1] ∈R2, the vector point set consists of the vectors
[1] ∈R2 ≜









[1, 0]⊺
[0, 1]⊺
[−1, 0]⊺
[0, −1]⊺
(2.26)
For the space R2, with n = 2 there will be 2n = 4 vector points that lie along the axis
of a two-dimensional Cartesian space, as shown in Figure 2.1.

VECTOR POINT GENERATORS
17
(0,1)
(0,–1)
(1,0)
(–1,0)
FIGURE 2.1
Example of two-dimensional Cartesian grid of axial vector points.
■EXAMPLE 2.2
For [1] ∈R3,the vector point set consists of
[1] ∈R3 ≜



















[1, 0, 0]⊺
[0, 1, 0]⊺
[0, 0, 1]⊺
[−1, 0, 0]⊺
[0, −1, 0]⊺
[0, 0, −1]⊺
(2.27)
Now,thevectorpointsfallontheCartesianaxesofathree-dimensionalhypersphere
as shown in Figure 2.2.
(0,1,0)
(0,0,–1)
(1,0,0)
(–1,0,0)
(0,0,1)
(0,–1,0)
FIGURE 2.2
Example of three-dimensional Cartesian grid of axial vector points.

18
PRELIMINARY MATHEMATICAL CONCEPTS
■EXAMPLE 2.3
For [1, 1] ∈R2,the vector point set consists of 2n(2) = 2n (n −1) = 4 vector
points given by
[1, 1] ∈R2 ≜









[1, 1]⊺
[1, −1]⊺
[−1, 1]⊺
[−1, −1]⊺
(2.28)
■EXAMPLE 2.4
For [1, 1] ∈R3, we will have the 2n(2) = 2n (n −1) (n −2) = 2 · 3 · 2 · 1 = 12
vector points given by the set
[1, 1] ∈R3 ≜

















































[1, 1, 0]⊺
[1, 0, 1]⊺
[0, 1, 1]⊺
[−1, 1, 0]⊺
[−1, 0, 1]⊺
[0, −1, 1]⊺
[1, −1, 0]⊺
[1, 0, −1]⊺
[0, 1, −1]⊺
[−1, −1, 0]⊺
[−1, 0, −1]⊺
[0, −1, −1]⊺
(2.29)
Using (2.24) and (2.25), the number of vector points produced by a generator can be
determined from the following formulas [3]
Generator
Number of Points
[0] ∈Rn
1
[1] ∈Rn
2n
[1, 1] ∈Rn
2n(2)
[1, 1, 1] ∈Rn
23n(3)
[1, 1, 1, 1] ∈Rn
24n(4)
...
...
(2.30)

APPROXIMATING NONLINEAR MULTIDIMENSIONAL FUNCTIONS
19
2.3
APPROXIMATING NONLINEAR MULTIDIMENSIONAL
FUNCTIONS WITH MULTIDIMENSIONAL ARGUMENTS
For many tracking applications, estimation methods require the evaluation of integrals
containing nonlinear multidimensional functions with multidimensional arguments
weighted by a probability density. In this section, we will review methods for numer-
ically approximating such nonlinear functions.
This section begins with a review of scalar methods that are then extended into
multiple dimensions. All of the approximations to a nonlinear function are essen-
tially expansions of the nonlinear function into a polynomial with arbitrary coefﬁ-
cients. Speciﬁcation of the exact form of the coefﬁcients depends on the application,
the desired accuracy and other computational considerations. Issues related to the
speciﬁcation of these coefﬁcients for estimation and tracking applications will be ad-
dressed in a later chapters on speciﬁc methods for the evaluation of density-weighted
integrals.
In the ﬁrst subsection we review methods of approximating scalar nonlinear func-
tions with a scalar arguments. We begin with a discussion of a general polynomial
expansion of a scalar function. This leads directly to the derivation of a scalar Taylor
polynomial approximation for a nonlinear function. A numerical approximation of
the Taylor polynomial is then derived by replacing the differentials of the Taylor
polynomial by their ﬁnite difference equivalents, resulting in Stirling’s interpolation
formula (Stirling’s polynomial).
This is followed by a subsection where the scalar approximations are generalized
to approximating multidimensional nonlinear functions with multidimensional argu-
ments through the introduction of multidimensional polynomial expansions. And,
once again, this leads to the derivation of a general multidimensional Taylor polyno-
mial. However, the generalization of Stirling’s approximation to multiple dimensions
can be accomplished in several ways, each of which leads to a different numeri-
cal approximation method. We derive a straightforward multidimensional Stirling’s
approximation that requires 3nx Cartesian interpolation points, where nx is the di-
mension of the argument of the multidimensional function. Then we brieﬂy discuss
two simpliﬁcations. For the ﬁrst simpliﬁcation method, the number of interpolation
points is arbitrarily reduced so as to include only those points that lie along the coordi-
nate axis, reducing the required number of interpolation points to 2nx + 1. A second
modiﬁcation method is to use a simplex version of Stirling’s approximation, which
reduces the number of interpolation points to nx + 2.
In this text, we will restrict ourselves to methods of approximation and will
almost completely ignore the important subjects of approximation error estimation
and convergence.
2.3.1
Approximating Scalar Nonlinear Functions
2.3.1.1
A General Polynomial Expansion. In general, any continuous scalar non-
linear function with a scalar argument, f (x) , can be approximated about an arbitrary

20
PRELIMINARY MATHEMATICAL CONCEPTS
point x0 to any degree of accuracy by a series approximation
f (x) =
∞
	
i=0
ai (x −x0)i = a0 + a1 (x −x0) + a2 (x −x0)2 + a3 (x −x0)3 + · · ·
(2.31)
Since it is usually difﬁcult to ﬁnd a closed form solution that takes the sum out to ∞,
the series is truncated into a polynomial approximation of order M
f (x) ≃
M
	
i=0
ai (x −x0)i = a0 + a1 (x −x0) + a2 (x −x0)2 + · · · + aM (x −x0)M
(2.32)
Each of the terms of such a polynomial is a monomial of the form ai (x −x0)i that
can be further expanded into monomial terms xi with coefﬁcients bi. Therefore, any
polynomial approximation of the function f (x) can be written as an expansion of the
form
f (x) ≃
M
	
i=0
bixi = b0 + b1x + b2x2 + · · · + bMxM
(2.33)
where
b0 ≜
M
	
i=0
(−1)i aixi
0
b1 ≜
M
	
i=1
(−2)i−1 aixi−1
0
...
bM ≜aM
(2.34)
2.3.1.2
Taylor Polynomial Expansion. We now present a simple derivation of the
Taylor and Maclaurin polynomials of order M. This requires that derivatives for f (x)
exist up to order M. Letting x = x0 in ( 2.32) we obtain
a0 =

f (x)

x=x0
(2.35)
To obtain a1, take the derivative of (2.32) with respect to x and set x = x0, resulting
in
a1 =
 d
dxf (x)

x=x0
(2.36)
Taking successive derivatives of (2.32) and setting x = x0 for each case results in the
evaluation of all coefﬁcients in terms of higher order derivatives of f (x) and leads to

APPROXIMATING NONLINEAR MULTIDIMENSIONAL FUNCTIONS
21
the well-known scalar Taylor polynomial point approximation for f (x0),
f (x0) ≃

f (x)

x=x0 +
 d
dxf (x)

x=x0
(x −x0)
+ · · · + 1
M!
 d
dx
M
f (x)

x=x0
(x −x0)M
(2.37)
=
M
	
i=0
1
i! (x −x0)i
 d
dx
i
f (x)

x=x0
(2.38)
Now, deﬁning the scalar operator Di
x by its operation on f (x)
Di
xf (x) ≜(x −x0)i
 d
dx
i
f (x)

x=x0
(2.39)
the Mth order Taylor polynomial (2.38) becomes
f (x) =
M
	
i=0
1
i!Di
xf (x)
(2.40)
Because it is sometimes difﬁcult to calculate the higher order derivatives of f (x),
the Taylor polynomial approximation is usually terminated after no more than three
terms (M = 2). A slightly different derivation of the Taylor polynomial can be found
in Arfken [4], where he derives the error due to truncation of the Taylor polynomial.
The error term in a Taylor polynomial of order M given by
RM = (x −x0)M
N!
 d
dξ
M
f (ξ)
(2.41)
with x0 ≤ξ ≤x.
2.3.1.3
Stirling’s Polynomial Expansion. An alternate derivative-free polynomial
approximation for f (x) can be obtained by using central differences in place of the
derivatives in the Taylor polynomial approximation. Finite difference approximations
canbederivedthroughtheuseofTaylorseriesexpansions.Supposewehaveafunction
f(x), which is continuous and differentiable over the range of interest. Let’s also
assume we know the value f(x0) and all the derivatives at x = x0. Using (2.37),
examine the ﬁrst-order Taylor polynomials for f(x0 + q) about x0
f (x0 + q) = f(x0) + f ′ (x0) q
(2.42)
where q ≜x −x0 is the step size and f ′ (x0) ≜df (x0) /dx0.
Following the same procedure, we can write
f (x0 −q) = f(x0) −f ′ (x0) q
(2.43)

22
PRELIMINARY MATHEMATICAL CONCEPTS
Subtracting the two equations leads to the ﬁrst-order central difference approximation
f ′ (x) = 1
2q

f (x + q) −f (x −q)

(2.44)
where we have used x instead of x0.
Now following the same procedure for the second-order Taylor polynomial, we
can write
f (x + q) = f(x) + f ′ (x) q + 1
2f ′′ (x) q2
(2.45)
and
f (x −q) = f(x) −f ′ (x) q + 1
2f ′′ (x) q2
(2.46)
where f ′′ (x) ≜d2f (x) /dx2. Adding the two equations leads to
f ′′ (x) = 1
q2

f (x + q) −2f (x) + f (x −q)

(2.47)
Using these central difference approximations of the derivative terms, the second-
order Taylor polynomial ( 2.37) reduces to the second-order Stirling’s interpolation
formula [5] around the point x0
f (x) = f (x0) + ˜Dxf (x) + 1
2
˜D2
xf (x)
(2.48)
where
˜Dxf (x) ≜1
2q

f (x0 + q) −f (x0 −q)

(x −x0)
(2.49)
and
˜D2
xf (x) ≜1
q2

f (x0 + q) −2f (x0) + f (x0 −q)

(x −x0)2
(2.50)
The approximation for x0 = 0 follows in an obvious way.
This represents a derivative-free approximation to f (x) with the free step-
size parameter q. Examination of the above second-order approximation reveals
that, for the one-dimensional case, f (x) must be known at only the three points
{x0, x0 + q, x0 −q}. Higher order approximations can be obtained simply by approx-
imating higher order derivatives, resulting in an increase in the number of evaluation
points on a one-dimensional uniform grid with spacing q.
To summarize scalar methods, we have presented two polynomial approximations
for the function f (x), expansions in terms of the Taylor polynomial and Stirling’s
interpolation formula. In both cases, we have examined point approximations, that
is, polynomial approximations of f (x) at the point x0. Polynomial approximations
of a nonlinear function over a larger support region is a much broader topic and is
beyond the scope of this text.

APPROXIMATING NONLINEAR MULTIDIMENSIONAL FUNCTIONS
23
2.3.2
Approximating Multidimensional Nonlinear Functions
2.3.2.1
A General Multidimensional Polynomial Expansion. For the multidimen-
sional case, we can write the nonlinear function as a vector function with a vector
argument, f (x), where f ∈Rnf and x ∈Rnx
f (x) =

f1 (x) , f2 (x) , . . . , fnf (x)
⊺
(2.51)
and
x =

x1, x2, . . . , xnx
⊺
(2.52)
Note that the vector function f may not have the same number of dimensions as x. For
a scalar nonlinear function with a vector argument, a general polynomial expansion
about the point x0 ≜

x0,1, x0,2, . . . , x0,nx
⊺of order d can be written in terms of
multidimensional monomials [2]
f (x) = f

x1, x2, . . . , xnx

≃
d
	
i1=0
· · ·
d
	
inx=0
aj

i1, i2, . . . , inx

×

x1 −x0,1
i1 
x2 −x0,2
i2 · · ·

xnx −x0,nx
inx
(2.53)
where i1 + i2 + · · · + inx ≤d indicates that only those terms of the multiple sum that
satisfy the speciﬁed condition can be used in the polynomial expansion. This is made
clear with the following example.
■EXAMPLE 2.5
For x ∈R2, a basis of multidimensional monomials spanned by monomials of
degree d = 2 or less is the set of six monomials

1, (x1 −x0,1), (x2 −x0,2), (x1 −
x0,1)(x2 −x0,2), (x1 −x0,1)2, (x2 −x0,2)2
. The polynomial approximation of f (x)
about the point x0 follows immediately
f (x) = f (x1, x2) ≃a (0, 0)
+a (1, 0)

x1 −x0,1

+ a(0, 1)

x2 −x0,2

+a(1, 1)

x1 −x0,1
 
x2 −x0,2

+a(2, 0)

x1 −x0,1
2 + a(0, 2)

x2 −x0,2
2
(2.54)
This can be written in the form (2.53)
f (x) =
d
	
i1=0
d
	
i2=0
a (i1, i2)

x1 −x0,1
i1 
x2 −x0,2
i2
(2.55)
where the condition i1 + i2 ≤d = 2 must be met for each monomial term in the
polynomial.

24
PRELIMINARY MATHEMATICAL CONCEPTS
In general, for a vector x ∈Rnx there are
 nx + d
d

distinct monomials of precision d
or less. The binomial coefﬁcient
 n
m

is the number of ways of choosing m objects
from a collection of n distinct objects without regard to order and is deﬁned as

n
m

≜
n!
m! (n −m)!
(2.56)
By inspection, the polynomial expansion for the vector function f (x) about the
point x0 can be written as
f (x) = f

x1, x2, . . . , xnx

≃
d
	
i1=0
· · ·
d
	
inx=0
a

i1, i2, . . . , inx

×

x1 −x0,1
i1 
x2 −x0,2
i2 · · ·

xnx −x0,nx
inx
(2.57)
where
a =

a1, a2, . . . , anf
⊺
(2.58)
■EXAMPLE 2.6
Again considering the case where x ∈R2 and keeping only terms up to second
order, (2.57) becomes
f (x) ≃a (0, 0) + a (1, 0)

x1 −x0,1

+ a (0, 1)

x2 −x0,2

+a (1, 1)

x1 −x0,1
 
x2 −x0,2

+a (2, 0)

x1 −x0,1
2 + a (0, 2)

x2 −x0,2
2
(2.59)
In the next two sections we will ﬁrst address the Taylor polynomial approximation
for a multidimensional function, followed by a discussion of a multidimensional
extension of Stirling’s interpolation formula.
2.3.2.2
Multidimensional Taylor Polynomial Expansion. Beginning with the case
where x ∈R2 and f (x) ∈Rnf , the Taylor polynomial can be developed by ﬁrst setting
x = x0 in ( 2.59) resulting in
a (0, 0) = [f (x)]x=x0
(2.60)

APPROXIMATING NONLINEAR MULTIDIMENSIONAL FUNCTIONS
25
Now, taking the partial derivative of f (x) with respect to x1 in ( 2.59) and then setting
x = x0 leads to
a (1, 0) =
 ∂
∂x1
f (x)

x=x0
(2.61)
where the ∂/∂x1 operates on each component of f (x).
In a similar fashion, the remaining coefﬁcients in ( 2.59) can be determined, re-
sulting in the second-order polynomial expansion
f (x) ≃[f (x)]x=x0 +
 

x1 −x0,1
  ∂
∂x1
f (x)

x=x0
+

x2 −x0,2
  ∂
∂x2
f (x)

x=x0
!
+ 1
2!
 

x1 −x0,1
2
 ∂2
∂x2
1
f (x)

x=x0
+2

x1 −x0,1
 
x2 −x0,2
 
∂2
∂x1∂x2
f (x)

x=x0
+

x2 −x0,2
2
 ∂2
∂x2
2
f (x)

x=x0
!
(2.62)
This can immediately be generalized to an Mth order multidimensional Taylor
polynomial approximation
f (x) ≃
M
	
i=0
1
i!Di
x−x0f (x)
(2.63)
where the operator notation similar to that employed by Williamson et al. [6] has been
adopted, with the scalar operator Di
x−x0 deﬁned by
Di
x−x0f (x) ≜

x1 −x0,1
 ∂
∂x1
+

x2 −x0,2
 ∂
∂x2
+ · · · +

xnx −x0,nx

∂
∂xnx
i
f (x)
"""""
x=x0
(2.64)
The notation [·] |x=x0 applies only to the differential terms.
When a polynomial (x1 + · · · + xn)i is multiplied out, each term will consist of
a constant times a factor of the form xii
1xi2
2 · · · xinn where the nonnegative integers ik

26
PRELIMINARY MATHEMATICAL CONCEPTS
satisfy i1 + · · · + in = i. The multinomial expansion has the form
(x1 + · · · + xn)i =
i
	
i1=0
· · ·
i
	
in=0
i1+···+in=i

i
i1 · · · in

xi1
1 · · · xinn
(2.65)
The multinomial coefﬁcients can be calculated from

i
i1 · · · in

≜
i!
i1! · · · in!
(2.66)
■EXAMPLE 2.7
Consider the case
(x1 + x2 + x3)2 =
2
	
i1=0
2
	
i2=0
2
	
i3=0
i1+i2+i3=2

2
i1 · · · i3

xi1
1 xi2
2 xi3
3
(2.67)
= x2
1 + x2
2 + x2
3 + 2x1x2 + 2x1x3 + 2x2x3
(2.68)
It follows immediately that (2.64) can be written as
Di
x−x0f (x) =
i
	
i1=0
· · ·
i
	
in=0
i1+···+in=i

i
i1 · · · in
 
x1 −x0,1
i1 · · ·

xnx −x0,nx
in
×

∂i
∂xi1
1 · · · ∂xinnx
f (x)

x=x0
(2.69)
Converting (2.63) into vector–matrix notation and keeping only terms to second-order
results in the Taylor polynomial approximation for a multidimensional function
f (x) = [f (x)]x=x0 +

∇xf⊺(x)
⊺
x=x0 (x −x0)
+ 1
2!
nx
	
p=1
ep (x −x0)⊺
∇x∇⊺
xfp (x)

x=x0 (x −x0)
(2.70)
The ﬁrst-order term contains the Jacobian matrix,

∇xf⊺(x)
⊺
x=x0 ∈Rnx×nf ,
while the pth component of the second-order term contains the Hessian matrix

∇x∇⊺
xfp (x)

x=x0 ∈Rnx×nx and ep is an nx-dimensional unit vector along the pth-
dimensional Cartesian axis. This Taylor polynomial expansion for a multidimensional
function is very difﬁcult to evaluate beyond the ﬁrst-order term, so the series is usually
truncated without any of the higher order terms.

APPROXIMATING NONLINEAR MULTIDIMENSIONAL FUNCTIONS
27
2.3.2.3
Multidimensional
Stirling’s
Polynomial
Expansion. A
version
of
derivative-free Stirling’s polynomial interpolation formula for multidimensional
functions can be written as [2,7,8]
f (x) ≃
M
	
i=0
1
i!
˜Di
x−x0f (x)
(2.71)
or, keeping only terms to second order
f (x) ≃f (x0) + ˜Dx−x0f (x) + 1
2!
˜D2
x−x0f (x)
(2.72)
The central difference operators ˜Dx−x0f (x) and ˜D2
x−x0f (x) are deﬁned by
˜Dx−x0f (x) ≜1
2q
nx
	
p=1

xp −x0,p
 
f

x0 + qep

−f

x0 −qep

(2.73)
˜D2
x−x0f (x) ≜1
q2



nx
	
p=1

xp −x0,p
2 
f

x0 + qep

−2f (x0) + f

x0 −qep

+2
nx
	
p=1
nx
	
r=1
r /= p

xp −x0,p
 
xr −x0,r

×

f

x0 + qep + qer

−f

x0 −qep + qer

−f

x0 + qep −qer

+ f

x0 −qep −qer

(2.74)
where ep and er are unit vectors along the pth-and rth-dimensional Cartesian axis,
respectively.
■EXAMPLE 2.8
For x ∈R2, these reduce to
˜Dx−x0f (x) ≜1
2q

x1 −x0,1
 
f (x0 + qe1) −f (x0 −qe1)

+

x2 −x0,2
 
f (x0 + qe2) −f (x0 −qe2)

(2.75)

28
PRELIMINARY MATHEMATICAL CONCEPTS
and
˜D2
x−x0f (x) ≜1
q2

x1 −x0,1
2 
f (x0 + qe1) −2f (x0) + f (x0 −qe1)

+ 1
q2

x2 −x0,2
2 
f (x0 + qe2) −2f (x0) + f (x0 −qe2)

+ 2
q2

x1 −x0,1
 
x2 −x0,2

×

f (x0 + qe1 + qe2) −f (x0 −qe1 + qe2)
−f (x0 + qe1 −qe2) + f (x0 −qe1 −qe2)

(2.76)
It is obvious that the interpolation for x ∈R2 requires a set of nine points on
a uniform two-dimensional grid with spacing q, that is, J2 ∈{(x0,1, x0,2),(x0,1 ±
q, x0,2),(x0,1, x0,2 ± q),(x0,1 ± q, x0,2 ± q)}, as shown in Figure 2.3.
In general, the number of points required for the second-order multidimensional
Stirling’s polynomial approximation is 3nx on a uniform multidimensional grid (hy-
persquare) with spacing q.
An alternative method that uses a functional expansion in terms of Hermite or-
thogonal polynomials [5,9] in place of Equations (2.63) and (2.69) results in the same
number of interpolation points as the second-order multidimensional Stirling’s ap-
proximation (the interpolation points may not be identical). This alternative method
does not require any derivatives or ﬁnite differences and in many ways is more sat-
isfying than the ﬁnite difference approach. Of course, such a statement is purely a
subjective personal view. The treatment of this method of interpolation will be post-
poned until the chapters on numerical integration of Gaussian-weighted integrals later
(
)
0,1
0,2
,
x
q x
+
(
)
0,1
0,2
,
x
x
(
)
0,1
0,2
,
x
x
q
−
(
)
0,1
0,2
,
x
q x
q
+
−
(
)
0,1
0,2
,
x
q x
q
+
+
(
)
0,1
0,2
,
x
q x
q
−
−
(
)
0,1
0,2
,
x
q x
q
−
+
(
)
0,1
0,2
,
x
q x
−
(
)
0,1
0,2
,
x
x
q
+
FIGURE 2.3
Two-dimensional points for multidimensional Sterling’s approximation.

OVERVIEW OF MULTIVARIATE STATISTICS
29
1D
Line
2D
Triangle
3D
Tetrahedron
FIGURE 2.4
Simplex ﬁgures in up to three dimensions.
in this book. A discussion of Hermite interpolation is postponed because a Gaussian-
weighted integral can be changed into the Hermite integral by a simple transform
making understanding of the method simpler.
There are other derivative-free methods of interpolation that require fewer inter-
polation points than the multidimensional Stirling’s polynomial approximation de-
scribed above. One such interpolation method uses only those interpolation points
in Figure 2.3 that lie along the orthogonal Cartesian axes and the origin. Examina-
tion of (2.73) and (2.74) reveals that only the cross-terms from (2.74) have been
removed at the cost of a small amount of accuracy for the second-order term. With
this simpliﬁcation, the number of interpolation points is reduced to 2nx + 1.
Another multidimensional interpolation scheme uses one point at the origin and
additional points equally spaced in angle around the circumference of a multidimen-
sional hypersphere, and is referred to as spherical simplex interpolation. An example
of regular simplex ﬁgures (ones with all sides and angles equal) that have vertices
on a hypersphere are shown in Figure 2.4 for dimensions up to three. These methods
can require as few as nx + 2 points for second-order interpolation. Although sim-
plex interpolation methods are the basis of numerical integration methods for mul-
tidimensional functions, only recently have simplex methods for multidimensional
interpolation appeared in the literature in applications related to computer graphics
[10–16]. Consequently, an exposition of purely interpolative methods on the simplex
are beyond the scope of this text. In spite of this, we will present a simplex method
for integration of Gaussian-weighted multidimensional functions in Chapter 10.
2.4
OVERVIEW OF MULTIVARIATE STATISTICS
2.4.1
General Deﬁnitions
From a Bayesian viewpoint, a stochastic variable is a variable whose value is not ﬁxed
deterministically, but can vary according to a probability distribution. The stochastic
variable is often multidimensional so that the underlying probability distribution is
multivariate. In addition, the expected value of the stochastic variable can be time
varying, with the change of the underlying distribution in time governed by some
dynamic or transition equation.

30
PRELIMINARY MATHEMATICAL CONCEPTS
2.4.1.1
ProbabilityDensityFunction. Webeginthissectionwithadiscussionofthe
general deﬁnitions of a probability density function (pdf) and a cumulative distribution
function (cdf) for multidimensional stochastic variables.
A probability density function of a stochastic (random) multidimensional variable
x at the point x = ξ is deﬁned by [17]
px (ξ) dξ ≜Prob {ξ ≤x ≤ξ+dξ} ≜P {ξ ≤x ≤ξ+dξ} > 0
(2.77)
where dξ is an inﬁnitesimal interval. It follows immediately that the probability that
x lies in the interval η ≤x ≤ξ is given by
Prob {η ≤x ≤ξ} ≜P {η ≤x ≤ξ} =
# ξ
η
Rnx
px (x) dx
(2.78)
where nx is the dimension of x and the multidimensional integral is deﬁned by
# ξ
η
Rnx
px (x) dx ≜
# ξ1
η1
dx1 · · ·
# ξnx
ηnx
dxnxpx

x1, . . . , xnx

=
nx
$
i=1
# ξi
ηi
dxipx

x1, . . . , xnx

(2.79)
Thus, the probability that x lies in the interval η ≤x ≤ξ is the multidimensional
“volume” under the pdf over the interval. A more common notation for the pdf is to
use p (x) to mean px (x).
2.4.1.2
Cumulative Distribution Function. The cumulative distribution function
is deﬁned as
Px {ξ} ≜Prob {x ≤ξ} =
# ξ
−∞
Rnx
px (x) dx
(2.80)
Note that as ξ →∞, Px {ξ} goes to 1 because the pdf is always normalized to integrate
to 1. The relationship between the pdf and cdf is given by
p (x) =
 d
dξ Px {ξ}

ξ=x
(2.81)
2.4.1.3
Joint and Marginal Probability Density Functions. The joint pdf of two
multidimensional random variables is the probability of the joint event p (x, z). The
marginal pdf of x is deﬁned by
p (x) =
# ∞
−∞
Rnz
p (x, z) dz
(2.82)

OVERVIEW OF MULTIVARIATE STATISTICS
31
The conditional probability density of x given z is
p (x|z) = p (x, z)
p (z)
(2.83)
Thus, the pdf of x can be obtained from
p (x) =
# ∞
−∞
Rnz
p (x, z) dz =
# ∞
−∞
Rnz
p (x|z) p (z) dz
(2.84)
2.4.1.4
Bayes’ Rule. Bayes’ Rule follows immediately from the conditional prob-
ability
p (x, z) = p (x|z) p (z) = p (z|x) p (x)
(2.85)
and
p (x|z) = p (z|x) p (x)
p (z)
=
p (z|x) p (x)
% ∞
−∞
Rnx
p (z|x) p (x) dx
(2.86)
Thus, we have the proportionality (∝indicates proportionality)
p (x|z) ∝p (z|x) p (x)
(2.87)
2.4.1.5
Point Estimates (Moments) of a Probability Density Function. Consider
the random vector x ≜

x1, x2, . . . , xnx
T ∈Rnx that is governed by the pdf p (x).
The expected value (ﬁrst moment) of x is deﬁned by
ˆx ≜E {x} =
# ∞
−∞
Rnx
xp (x) dx
(2.88)
The covariance (second moment) is given by
Pxx ≜E

(x −ˆx) (x −ˆx)⊺
=
# ∞
−∞
Rnx
(x −ˆx) (x −ˆx)⊺p (x) dx
=
# ∞
−∞
xx⊺p (x) dx −ˆxˆx⊺
(2.89)

32
PRELIMINARY MATHEMATICAL CONCEPTS
For two nx-dimensional random vectors x ≜[x1, x2, . . . , xnx]⊺
and z ≜
[z1, z2, . . . , znx]⊺, the cross-covariance between the two variables is deﬁned as
Pxz ≜E

(x −ˆx) (z −ˆz)⊺
=
# ∞
−∞
Rnx
(x −ˆx) (z −ˆz)⊺p (x, z) dxdz
=
# ∞
−∞
xz⊺p (x, z) dxdz −ˆxˆz⊺
(2.90)
It must be noted that for some probability densities these moments do not exist
because the moment integrals are inﬁnite.
2.4.2
The Gaussian Density
The Gaussian probability density function has been important in estimation theory
since it was ﬁrst introduced by Abraham de Moivre in an article in the year 1733
[18]. Because simple analytical forms for this symmetric distribution are known for
both scalar and vector stochastic variables, the Gaussian distribution is used to model
noise processes in many ﬁelds. A very good summary of the properties of Gaussian
distributions can be found online in Ref. [19]. In the paragraphs that follow, we will
discuss those characteristics of Gaussian densities that are important to the recur-
sive estimation methods developed in this book. We begin with a presentation of
the Gaussian probability density function, followed by a derivation of the Gaussian
cumulative distribution function.
We then introduce an afﬁne transformation that transforms the origin of the coor-
dinate system to the mean of the Gaussian density and rotates the coordinate axes to
align them with the principle axis of the density function. This is done primarily to
simplify the Gaussian-weighted integration methods developed in Part II. Finally, a
set of identities for the transformed moment equations are presented.
2.4.2.1
The Gaussian pdf. In estimation of the moments of a stochastic variable, the
most common assumption is that the underlying probability distribution is Gaussian.
A one-dimensional stochastic variable x is governed by a Gaussian pdf if
p (x) = N

x;&x, σ2
≜
1
√
2πσ
exp
'
−1
2σ2 [x −ˆx]2
(
(2.91)
where ˆx is the ﬁrst moment or mean value and σ2 is the second moment or variance.
σ is usually called the standard deviation. An example of a one-dimensional Gaussian
pdf as a function of x for ˆx = −5 and σ = 6 is given in Figure 2.5.

OVERVIEW OF MULTIVARIATE STATISTICS
33
–30
–20
–10
0
10
20
30
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
x
Probability
FIGURE 2.5
One-dimensional Gaussian pdf example.
The Gaussian pdf of an nx-dimensional stochastic variable x is deﬁned by
p (x) = N

x; ˆx, Pxx
≜(2π)−nx/2 ""Pxx""−1/2
× exp
'
−1
2

x −ˆx
⊺
Pxx−1 
x −ˆx
(
(2.92)
where ˆx and Pxx are the mean vector and the covariance matrix, respectively, and |A|
represents the determinant of the matrix A. Note that p is always a scalar.
Consider the two-dimensional Gaussian pdf with
ˆx =

ˆx, ˆy
⊺= [−5, 2]⊺
(2.93)
and σx = 6, σy = 8, and ρ = 0.5. Now Pxx →Pxy and we can write
Pxy =

σ2
x
ρσxσy
ρσxσy
σ2
y

=

62
24
24
82

(2.94)
The two-dimensional cross-covariance can also be written in the form
Pxy =

σ2
x
σ2
xy
σ2
xy
σ2
y

(2.95)
with σ2
xy ≜ρσxσy. A graphical depiction of this density is presented in Figure 2.6.

34
PRELIMINARY MATHEMATICAL CONCEPTS
FIGURE 2.6
Example of a two-dimensional Gaussian pdf.
2.4.2.2
The Gaussian cdf. The multidimensional Gaussian cumulative distribution
function (cdf ) is deﬁned by
P (x) ≜
# x
−∞
Rnx
p (u) du =
# x
−∞
Rnx
N

u;&x, Pxx
du
= (2π)−nx/2 ""Pxx""−1/2
×
# x
−∞
Rnx
exp
'
−1
2

u −&x
⊺
Pxx−1 
u −&x
(
du
(2.96)
Now, let
t (u) ≜D−1 
u−&x

(2.97)
with D deﬁned by
Pxx = DD⊺
Now P (x) becomes
P (x) =
nx
$
j=1
 
1
√
2π
# tj(x)
−∞
e−t2
j /2dtj
!
(2.98)

OVERVIEW OF MULTIVARIATE STATISTICS
35
where tj is the jth element of t. The one-dimensional equation inside the bracket can
be rewritten as
1
√
2π
# tj(x)
−∞
e−t2
j /2dtj (u) =
1
√
2π
# 0
−∞
e−t2
j /2dtj
+
1
√
2π
# tj
0
e−t2
j /2dtj
= 1
2
+1
2
 2
√π
# tj
0
e−t2
j /2dtj

(2.99)
Noting the MATLAB® functional deﬁnition
erf

q

=
2
√π
# q
0
e−u2/2du
(2.100)
then the multidimensional Gaussian cdf becomes
P (x) =
nx
$
j=1
1
2

1 + erf

tj

(2.101)
Figures 2.7 and 2.8 show examples of one-dimensional and two-dimensional cumu-
lative distribution functions, respectively, for the same means and variances used for
the one- and two-dimensional pdfs shown above.
2.4.2.3
Transformation of the General Gaussian Moment Equations. Notice that
in the moment equations ( 2.88) and (2.89), the integrals are of the form
%
xp (x) dx.
–30
–20
–10
0
10
20
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
Cumulative probability
FIGURE 2.7
An example of a one-dimensional Gaussian cdf.

36
PRELIMINARY MATHEMATICAL CONCEPTS
FIGURE 2.8
An example of a two-dimensional Gaussian cdf.
For the one-dimensional Gaussian case, consider the more general moment integral
E {f (x)} =
∞
#
−∞
f (x) N

x; ˆx, σ2
dx
=
∞
#
−∞
f (x)

1
√
2πσ
exp
'
−1
2σ2 [x −ˆx]2
(
dx
(2.102)
Now, apply the one-dimensional afﬁne transformation
x = ˆx + σc
(2.103)
This transformation moves the coordinate origin to ˆx and scales the standard deviation
so that N(x; ˆx, σ2) →N (c; 0, 1) /σ. From this, it can be understood that c is a zero-
mean, unit standard deviation random variable.
Letting
)f (c) ≜f (ˆx + σc)
(2.104)
and noting that dx →σdc, the general moment integral (2.102) becomes
E {f (x)} =
∞
#
−∞
)f (c) N (c; 0, 1) dc
(2.105)

OVERVIEW OF MULTIVARIATE STATISTICS
37
Similarly, for the multidimensional case, consider the general moment integral
E {f (x)} =
# ∞
−∞
Rnx
f (x) N

x; ˆx, Pxx
dx
=
# ∞
−∞
Rnx
f (x) (2π)−nx/2 ""Pxx""−1/2
× exp
'
−1
2

x −ˆx
⊺
Pxx−1 
x −ˆx
(
dx
(2.106)
Now apply the multidimensional afﬁne transformation
x = ˆx + Dc
(2.107)
where D is deﬁned by the matrix square root equation
Pxx = DD⊺
(2.108)
leading to the identity
D =

Pxx1/2
(2.109)
Here, D can be the Cholesky factor of Pxx or the principal (positive deﬁnite) matrix
root of Pxx. Actually, any matrix D that satisﬁes the deﬁnition (2.108) will sufﬁce. In
addition, c can be identiﬁed as a zero-mean multidimensional random variable with
a unit-diagonal covariance matrix, that is, c ∼N (c; 0, I).
The afﬁne transformation (2.107) has a geometric interpretation. The equiprob-
ability contours of a multidimensional Gaussian distribution are ellipsoids centered
at the mean. The direction of the principle axis are given by the eigenvectors of the
covariance matrix, with eigenvalues equal to the squared relative lengths of the prin-
cipal axes. The transformation (2.107) moves the coordinate origin to ˆx and rotates
the coordinate axes so that they lie along the principal axes of the density function,
with each axis scaled to a standard deviation of one, as shown in Figure 2.9. Note that
the contours of the transformed density are circular rather than elliptical.
Noting that
# ∞
−∞
Rnx
f (x) N

x;&x, Pxx
dx =
# ∞
−∞
Rnx
f (x (c)) N

x; ˆx, Pxx """"
dx
dc
"""" dc
(2.110)
and applying (2.110) and (2.107) to (2.106) we obtain
E {f (x)} =
# ∞
−∞
Rnx
)f (c) N (c; 0, I) dc
(2.111)
where
)f (c) ≜f (ˆx + Dc)
(2.112)

38
PRELIMINARY MATHEMATICAL CONCEPTS
x
y
−10
−8
−6
−4
−2
0
−4
−2
0
2
4
6
8
x
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1.5
−1
−0.5
0
0.5
1
1.5
Affine
transformation
FIGURE 2.9
Effects of an afﬁne transformation on a Gaussian probability density function.
Application of such an afﬁne transformation is a most important concept that will
be utilized to simplify the numerical evaluation of the Gaussian-weighted integrals
that constitute the moment equations of Bayesian estimation. Use of such an afﬁne
transformation, in combination with the polynomial expansion of nonlinear functions
discussed in Section 2.3, simpliﬁes the numerical evaluation of these integrals.
2.4.2.4
The Gaussian Moment Equations. In this section, we will present, without
derivation, some identities applicable to Gaussian moment equations.
For a random variable x ∈Rnx, and a symmetric matrix 	 ∈Rnx×nx, the scalar
quantity x⊺	x is known as a quadratic form in x. It can be shown that [20,21]
E

x⊺	x

= trace {	P} + ˆx⊺	ˆx
(2.113)
where ˆx and P are the expected value and covariance matrix of x, respectively, and
trace {•} denotes the trace of the matrix contained within the bracket. This result only
requires the existence of ˆx and P, but is independent of the form of the probability
density function. It can also be shown that the expected value of a quartic form in x
is given by [20]
E

x⊺	1xx⊺	2x

= trace {	1P} trace {	2P} + 2trace {	1P	2P}
(2.114)
For the one-dimensional case, the Gaussian moment equations will contain inte-
grals of the form
% ∞
−∞cnN (c; 0, 1) dc. Since&c = 0, and since N (c; 0, 1) is symmetric
about zero, the moment equations for cn are given by
E

cn
=
# ∞
−∞
cnN (c; 0, 1) dc =





1
n = 0
1 · 3 · 5 · · · (n −1)
n even
0
n odd
(2.115)

OVERVIEW OF MULTIVARIATE STATISTICS
39
Using the one-dimensional moment equations (2.115) it follows immediately that
the moment equations for the components of c =

c1, c2, . . . , cnx
⊺, up to fourth
order, are given by
#
Rnx
N (c; 0, I) dc =
# ∞
−∞
N (c1; 0, 1) dc1 · · ·
# ∞
−∞
N

cnx; 0, 1

dcnx = 1
(2.116)
#
ciN (c; 0, I) dc =
#
ciN (ci; 0, 1) dci
#
N

c∗; 0, I

dc∗= 0
(2.117)
#
cicjN (c; 0, I) dc = δij
(2.118)
#
cicjckN (c; 0, I) dc = δij
#
ckN (ck; 0, 1) dck
+ δik
#
cjN

cj; 0, 1

dcj
+ δjk
#
ciN (ci; 0, 1) dci
= 0
(2.119)
#
c2
i c2
jN (c; 0, I) dc =
 
1,
i /= j
3,
i = j
(2.120)
#
cicjckclN (c; 0, I) dc = δij,kl + δik,jl + δil,jk + 3δijkl
(2.121)
where c∗≜

c1, . . . ci−1, ci+1, . . . , cnx
⊺, the implied integration limits on all
integrals have been dropped for convenience and we have deﬁned
δij ≜
 
0,
i /= j
1,
i = j
(2.122)
δijkl ≜
 
1,
i = j = k = l
0,
elsewhere
(2.123)
and
δij,kl ≜
 
1,
i = j, k = l; i, j /= k, l
0,
elsewhere
(2.124)
with similar deﬁnitions for δikδjl and δilδjk.

40
PRELIMINARY MATHEMATICAL CONCEPTS
From (2.113) and (2.114), it follows immediately that the quadratic moment equa-
tions for c are given by
# 
c⊺Mc
i N (c; 0, I) dc =





1,
i = 0
trace {M} ,
i = 1
[trace {M}]2 + 2 trace {MM} ,
i = 2
(2.125)
where M ∈Rnx×nx is a known symmetric square matrix.
From (2.116) to (2.121), it is easy to show that
#
cc⊺N (c; 0, I) dc = I
(2.126)
2.4.2.5
Generating Samples from a Multidimensional Gaussian Distribution in
Matlab®. Samples from a general multidimensional Gaussian distribution
N(x; ˆx, Pxx) can be generated in Matlab in the following way:
1. Generate samples {c(i) ∼N (c; 0, I) ,
i = 1, . . . , nx}. Here, the use of
∼indicates that the ith sample is drawn from the density N(c; 0, I). In
Matlab, this is accomplished with the command c = randn(N,M), where N
is the dimension of c and M is the number of samples desired.
2. Use ( 2.107) to transform the samples of c into samples of x. Thus,
x(i) = ˆx + Dc(i)
(2.127)
with D deﬁned by Pxx = DD⊺.
REFERENCES
1. Brookes M. The Matrix Reference Manual. Available online at http://www.ee.ic.ac.uk/
hp/staff/dmb/matrix/intro.html. Accessed May 16, 2011.
2. Lerner UN. Hybrid Bayesian Networks for Reasoning About Complex Systems, Disser-
tation. Stanford, CA: Stanford University; 2002.
3. McNamee J, Stenger F. Construction of fully symmetric numerical integration formulas.
Numerische Math. 1967;10:327–344.
4. Arfken G. Mathematical Methods for Physicists, 3rd ed. Academic Press; 1985.
5. Ralston A, Rabinowitz P. A First Course in Numerical Analysis, 2nd ed. Dover Press; 2001.
6. Williamson RE, Cromwell RH, Trotter, HF. Calculus of Vector Functions, 3rd ed. Prentice-
Hall; 1972.
7. Julier S, Uhlmann, J. A General Method for Approximating Nonlinear Transformations
of Probability Distributions. Department of Engineering Science, University of Oxford;
1996.
8. Nøgaard M, Poulsen NK, Ravn O. New developments in state estimation for nonlinear
systems. Automatica 2000;36:1627–1638.
9. Burden, RL, Faires, JD. Numerical Analysis. Belmont: Brooks/Cole; 2000.

REFERENCES
41
10. Silvester P, Ferrari R. Finite Elements for Electrical Engineers, 3rd ed. Cambridge
University Press; 1996.
11. Waldron S. The error in linear interpolation at the vertices of a simplex. SIAM J. Numer.
Analy. 1998;35(3):1191–1200.
12. Gasca M, Sauer T. Polynomial interpolation in several variables. Adv. Comput. Math.
2000;12(4):377–410.
13. St¨ampﬂe M. Optimal estimates for the linear interpolation error on simplices. J. Approx.
Theory 2000;103:78–90.
14. Lu Y, Wang RH. A multidimensional generalization of Simpson-type formulas over N-
simplex. J. Inform. Comput. Sci. 2004;1(2):331–335.
15. Hemingway P. n-Simplex Interpolation. HPL-2002-320, Client and Media Systems
Laboratory, HP laboratories, Bristol; November 2002.
16. Warburton T. An explicit construction for interpolation nodes on the simplex. J. Eng. Math.
2006;56(3):247–262.
17. Papoulis A. Probability, Random Variables, and Stochastic Processes. McGraw-Hill; 1965.
18. de Moivre A, Approximatio ad Summam Terminorum Binomii (a + b)n in Seriem expansi.
Printed on November 12, 1733.
19. [online] http://en.wikipedia.org/wiki/Normal distribution.
20. Bar Shalom Y, Li XR, Kirubarajan T. Estimation with Application to Tracking and
Navigation: Theory, Algorithms and Software. Wiley; 2001.
21. Koch KR. Introduction to Bayesian Statistics. New York: Springer; 2007.

3
GENERAL CONCEPTS OF
BAYESIAN ESTIMATION
The process of estimation begins with an experiment that provides a set of observable
outcomes, usually some form of data. Examples of observable data can include a
time-sampled succession of bearings and/or ranges to a target or successive samples
of a stock price for sales throughout a day of trading. Based on the observable data,
one would like to estimate some characteristic parameters that may be unobservable
directly. For example, with bearings-only observations one would like to estimate the
target location and velocity as a function of time. In the case of stock price data, one
would like to estimate the volatility of the stock.
This book concentrates on estimation methods that include models of the temporal
dynamics of the variables to be estimated as well as models of the relationship between
the observable data and the unobservable variables to be estimated. In particular,
discussions will be limited to recursive estimation methods for discretely sampled
data. Thus, it is always assumed that the parameters to be estimated follow a known
recursive dynamic process and that there is a known analytical link between the
observed data and the parameters to be estimated.
In addition, Bayesian estimation assumes that both the parameters to be estimated
and the observed data are stochastic entities. The analytical link (a transformation)
between the observed data and the parameters to be estimated provide a unifying
framework for estimation where the recursive inference is characterized by a density
function for the current state vector value conditioned on the current and all prior
observations.
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
42

POINT ESTIMATORS
43
3.1
BAYESIAN ESTIMATION
Bayesian estimation has as its objective the estimation of successive values of a pa-
rameter vector x given an observation vector z. As noted above, it is customary to treat
both x and z as random vectors. For the parameter vector, the stochastic assumption
is inherent in the equations governing the dynamics of the parameter, where unmod-
eled effects are added as random noise. For the observation vector one can justify a
stochastic nature by assuming that there is always some random measurement noise.
The random vector x is assumed to have a known prior density function p (x). This
prior distribution includes all that is known and unknown about the parameter vec-
tor prior to the availability of any observational data. If the true parameter value of
x were known, then the probability density of z is given by the conditional density
or likelihood function p (z|x) and the complete statistical properties of z would be
known.
Once an experiment has been conducted and a realization of the random variable
z is available, one can use Bayes’ law to obtain the posterior conditional density
of x:
p (x|z) = p (z|x) p (x)
p (z)
(3.1)
Thus, within the Bayesian framework, the posterior density p (x|z) contains every-
thing there is to know about x after taking into account the observational outcome of
an experiment. Since the experimental outcome z is now available, the denominator
of (3.1) is just a scalar normalizing constant that can be found from
p (z) =

Rnx
p (z|x) p (x) dx
(3.2)
For the full Bayesian estimation problem, the likelihood and the posterior, or
alternately their joint density, deﬁne the statistical model for estimation, where the
joint density of the parameter and observational vectors is deﬁned by
p (x, z) = p (x|z) p (z)
(3.3)
The solution to the estimation problem is found in the posterior distribution given
by (3.1). Consequently, the posterior distribution can be used to generate any point
estimates of x that are desired, if they exist. Note that the posterior density should be
regarded as the most general solution to the estimation problem and in many cases
the density function can be used to characterize x.
3.2
POINT ESTIMATORS
Evaluating a posterior density can sometimes be a daunting prospect and is a compli-
cated solution to the multidimensional inference problem. Consider a point estimate ˆx,

44
GENERAL CONCEPTS OF BAYESIAN ESTIMATION
which is an educated guess of the parameter value given the observations at hand. An
analytical method to generate a point estimate ˆx based on all available observations
can be designated as an estimator of x. The actual value obtained for ˆx will vary based
on the estimator used. In addition, it will vary from experiment to experiment using
the same estimator. Thus, the point estimate should itself be viewed as a stochastic
variable.
To ﬁnd a suitable estimator we deﬁne a cost function, L (ˆx, x), which deﬁnes a
penalty for an erroneous estimate ˆx /= x. In general, we would prefer cost functions
where the penalty increases based on the magnitude of the difference error x −ˆx.
Without loss of generality, we assume that the cost function is positive and that
L (x, x) = 0 is its unique minimum. For a point estimate ˆx, the Bayesian risk R is
deﬁned as the expected value of the cost function
R ≜E {L (ˆx, x)}
(3.4)
where the expectation is over x and z. Here we note that since we have used the
observations z to generate ˆx, we can write ˆx (z). The optimal choice of ˆx (z) is the one
that minimizes the Bayesian risk
ˆx (z) = arg min
x∗(z)

Rnz+nx
L

x∗(z) , x

p (x, z) dxdz
(3.5)
where the minimization is over all values of x∗(z) and the dimensions of z and x are
nz and nx, respectively.
Now, inserting (3.3) splits the integration into two parts
ˆx (z) = arg min
x∗(z)

Rnz



Rnx
L

x∗(z) , x

p (x|z) dx

p (z) dz
(3.6)
Since both L (x∗, x) and p (x|z) are positive, the inner integral is positive given any z.
Since p (z) is also positive, the value of x∗(z) that minimizes the risk also minimizes
the inner integral,
ˆx (z) = arg min
x∗(z)

Rnx
L

x∗(z) , x

p (x|z) dx
(3.7)
for each z. The Bayesian estimation problem is therefore deﬁned by (3.7) where each
choice for a cost function gives rise to a different estimator.
An alternative approach could be to choose as our estimator one that minimizes
the maximal cost (min max estimator) such that
ˆx (z) = arg min max
x∗(z)

L

x∗(z), x
	
(3.8)

POINT ESTIMATORS
45
For most estimation problems, it is realistic to assume that the cost is based on the
estimation error given by
ϵx ≜x −ˆx
(3.9)
The cost function L (ϵx) is a function of a single variable.
Another typical cost function includes the weighted squared error cost function
L

ϵx
= ϵx⊺Mϵx
(3.10)
which accentuates the effects of large errors. Here, M is some known square weighting
matrix that does not depend on x.
An additional method is to use the absolute error value cost function
L

ϵx
=


ϵx

(3.11)
Another common choice is a cost function that is uniform across some region and
zero outside that region. For example, we could choose L (ϵx) such that
L

ϵx
=

0,
|ϵx| ≤/2
1,
|ϵx| > /2
(3.12)
The speciﬁc cost function chosen depends on what we wish to accomplish with
the estimation method. Frequently, it is difﬁcult to assign an analytic measure to what
may be a subjective quantity. As noted above, our goal will be to ﬁnd an estimate that
minimizes the expected value of the cost.
First let us consider minimizing the expected value of the weighted squared error
cost function, usually called the mean squared error criterion. Using x∗instead of ˆx
in (3.9), putting the results into (3.10), (3.7) becomes
ˆx (z) = arg min
x∗(z)

Rnx

x −x∗⊺M

x −x∗
p (x|z) dx
(3.13)
To obtain the minimum argument we take the derivative with respect to x∗, set the
results to zero and solve for x∗. That is
d
dx∗
⊺

Rnx

x −x∗⊺M

x −x∗
p (x|z) dx
= −2M

Rnx
xp (x|z) dx + 2Mx∗

Rnx
p (x|z) dx
(3.14)
Setting this to zero and noting that the second integral equals 1, we have
ˆxms =

Rnx
xp (x|z) dx ≜E {x|z}
(3.15)

46
GENERAL CONCEPTS OF BAYESIAN ESTIMATION
Thus, minimizing the mean squared error cost function results in an estimate that is
the mean relative to the posteriori density (or the conditional mean).
Since the estimates obtained from a mean-squared estimator are themselves ran-
dom variables, one would like to quantify the uncertainty in these estimates. A
convenient measure of uncertainty is given by the conditional covariance matrix
Pxx = E

(x −ˆx) (x −ˆx)⊺|z
	
=

Rnx
(x −ˆx) (x −ˆx)⊺p (x|z) dx
(3.16)
However, it must be noted that for some probability density functions, one or
more of these point estimates do not exist. The Cauchy distribution is an example of
a distribution that has no mean, covariance or higher order moments. However, its
mode and median are well deﬁned and the covariance can be replaced by a dispersion
measure.
In this book, most of the attention is devoted to estimation methods that utilize the
mean squared cost function. The absolute error and uniform cost functions lead to
what is know as the maximum a posteriori (MAP) estimator and a special case of the
MAP estimator called the maximum likelihood estimator. For a complete derivation
and discussion of these estimators, see Ref. [1], pages 56–58.
3.3
INTRODUCTION TO RECURSIVE BAYESIAN FILTERING OF
PROBABILITY DENSITY FUNCTIONS
A discrete dynamic process will be deﬁned as a process where the current state of
the system is dependent on one or more prior states. In continuous processes, the
dependence of the current state on previous states is captured within a differential
equation. When observations occur at discrete times, estimation conditioned on those
observations can only occur at those time, so the differential equation is replaced by
its ﬁnite difference equivalent that links the state at observation time tn to states at
observation times prior to tn.
A ﬁrst-order Markov process is one in which the current state is dependent only
on the previous state. Thus, we can characterize a discrete random Markov dynamic
process as
xn = fn−1

xn−1, un, ηn−1

= fn−1 (xn−1) + un + vn−1
(3.17)
where xn is the state of the system (state vector) at time tn, fn−1 is a deterministic
transition function (matrix) that moves the state x from time tn−1 to time tn and un
is a known (usually deterministic) control that constitutes some external input that
drives the system dynamics.
Although the white noise η (not necessarily Gaussian) can start at the input and be
transformed by the transition function, it is usually assumed that the noise is additive
and represents those parts of the true transition function that are not modeled. Note

INTRODUCTION TO RECURSIVE BAYESIAN FILTERING
47
that the Markov process given above is only a model of the true transition function.
For example, if the motion of a moving ship is modeled as constant velocity motion,
the noise term will represent the accelerations that cause small deviations from a
straight line path caused by wave action.
The speciﬁc problem of interest is estimating the usually unobservable state vector
xn based on the set of all experimental observation vectors z1:n ≜{z1, z2, . . . , zn}. It
will be assumed that an analytical link is known between the observation vector at
time tn and the state vector at time tn that is represented by
zn = hn

xn, µn

= hn (xn) + wn
(3.18)
Here, zn is designated as the observation vector and hn is a deterministic observation
function that links the state vector with the observation. Once again, the white noise
µn (not necessarily Gaussian) can be transformed by the transition function, but it is
usually assumed that the observation noise is additive.
For most problems of interest, both fn−1 and hn are considered adiabatic, changing
very slowly with time. They can therefore be considered time independent, that is,
fn−1 →f and hn →h.
The speciﬁc analytical form that (3.17) and (3.18) take represent a complete model
of the system for which estimated information about the state vector is desired. The
estimation of xn based on the complete set of observations z1:n can be understood in a
Bayesian sense by turning the problem into an estimation of the conditional posterior
density p

xn|z1:n

. The goal of this book is to develop recursive Bayesian methods
for estimation of this conditional distribution and the moments associated with xn
under this posterior distribution.
In terms of the posterior distribution p

xn|z1:n

, Bayes’ law (3.1) can be rewritten
as
p

xn|z1:n

= p

z1:n|xn

p (xn)
p (z1:n)
(3.19)
Theposteriorpdf,p

xn|z1:n

,isthepdfofxn conditionedonallobservationsuptoand
including the current observation. Since the set {z1:n} can be written as {zn, z1:n−1} ,
(3.19) can be expanded and factored into the following form
p

xn|z1:n

= p

zn, z1:n−1|xn

p (xn)
p (zn, z1:n−1)
(3.20)
Using (3.3) in both the numerator and denominator, this becomes
p

xn|z1:n

= p

zn|z1:n−1, xn

p (z1:n−1|xn) p (xn)
p (zn|z1:n−1) p (z1:n−1)
(3.21)

48
GENERAL CONCEPTS OF BAYESIAN ESTIMATION
Applying Bayes law (3.1) to p (z1:n−1|xn), and reducing the resulting equations,
yields the following progression
p

xn|z1:n

= p

zn|z1:n−1, xn

p (xn|z1:n−1) p (z1:n−1) p (xn)
p (zn|z1:n−1) p (z1:n−1) p (xn)
= p

zn|z1:n−1, xn

p (xn|z1:n−1)
p (zn|z1:n−1)
(3.22)
= p

zn|xn

p (xn|z1:n−1)
p (zn|z1:n−1)
(3.23)
where p

zn|z1:n−1, xn

→p

zn|xn

because, from (3.18), the observation at time
tn does not depend on the observations at times t1:n−1.
One last step is needed to create a completely recursive form for the conditional
probability density function equations. The Chapman–Kolmogorov equation provides
a link between the prior density, deﬁned as p (xn|z1:n−1), and the previous posterior
density
p (xn|z1:n−1) =

p (xn|xn−1, z1:n−1) p

xn−1|z1:n−1

dxn−1
=

p (xn|xn−1) p

xn−1|z1:n−1

dxn−1
(3.24)
Now, from (3.24) and (3.23), a recursive link has been established between the
previous posterior p

xn−1|z1:n−1

and the current posterior p

xn|z1:n

that re-
quires speciﬁcation of the predictive density given by p (xn|xn−1) and the likelihood
function p

zn|xn

.
In (3.24), p (xn|xn−1) has replaced p (xn|xn−1, z1:n−1) because this predictive den-
sity is deﬁned by the dynamic equation speciﬁed by (3.17) that does not depend on
z1:n−1. Examination of (3.17) reveals that the density of xn is dependent of both xn−1
and vn−1. If vn−1 is considered to be zero-mean, then the mean value of xn will be
the mean value of f (xn−1, un) .
In addition, from (3.18), zn can be seen as a random vector whose current
value is dependent on xn. Therefore, (3.1) deﬁnes the normalized likelihood func-
tion p

zn|xn

/p (zn). From (3.1) we see that since p (x) and p (x|z) are densities,
p (zn|xn) /p (zn) must also be a density. Therefore, p (zn) is the normalizing function
for p (zn|xn).
One iteration in this Bayesian recursive procedure for developing successive pos-
terior densities is shown in Figure 3.1. The recursive procedure is initiated by p (x0),
the pdf associated with x prior to any observations.
As an aside, in many theoretical treatments of this subject, the Markov process
(3.17) is considered as part of a Markov Chain x1:n = {x1, . . . , xn}, and the prediction
and update equations, (3.24) and (3.23), respectively, are written in terms of the
Markov Chain [2]. Since this book deals primarily with tracking applications, in
which the Markov Chain reduces to a ﬁrst-order Markov process, our developments
will ignore the use of the Markov Chain notation in favor of a one-step recursion

INTRODUCTION TO RECURSIVE BAYESIAN ESTIMATION
49
nt
nt
n
n
t
t
nz
Posterior
density
One Bayesian recursive esmaon step
Chapman–Kolmogorov
predicon
Bayes’ law
update
Posterior
density
Current
observaon
FIGURE 3.1
Depiction of one step in the recursive Bayesian posterior density estimation
procedure.
formulation. For those cases where the dynamic model Markov process is of higher
order, for example, analysis of time series data, one can usually create an augmented
state vector that results in a ﬁrst-order Markov process.
3.4
INTRODUCTION TO RECURSIVE BAYESIAN ESTIMATION OF
THE STATE MEAN AND COVARIANCE
Introducing a notation that will be used throughout the remainder of this text, let an
estimate of xn conditioned on all observations up to time tp be written as xn|p, with
ˆxn|p = E

xn|z1:p
	
≜

Rnx
xnp

xn|z1:p

dxn
(3.25)
Thus, an estimate of xn that uses all observations, including the current one at time
tn, is written as
ˆxn|n = E {xn|z1:n} =

Rnx
xnp

xn|z1:n

dxn
(3.26)
while one that predicts xn based on all but the current observation is given by
ˆxn|n−1 = E {xn|z1:n−1} =

Rnx
xnp

xn|z1:n−1

dxn
(3.27)
The same can be done for the covariance matrix, writing
Pxx
n|n = E

xn −ˆxn|n
 
xn −ˆxn|n
⊺|z1:n
	
=

Rnx

xn −ˆxn|n
 
xn −ˆxn|n
⊺p

xn|z1:n

dxn
(3.28)

50
GENERAL CONCEPTS OF BAYESIAN ESTIMATION
with prediction form of the covariance given by
Pxx
n|n−1 = E

xn −ˆxn|n−1
 
xn −ˆxn|n−1
⊺|z1:n−1
	
=

Rnx

xn −ˆxn|n−1
 
xn −ˆxn|n−1
⊺
×p

xn|z1:n−1

dxn
(3.29)
3.4.1
State Vector Prediction
Using the Chapman–Kolmogorov equation (3.24) in (3.27) and (3.29) results in
ˆxn|n−1 =

Rnx

Rnx
xnp (xn|xn−1) p

xn−1|z1:n−1

dxn−1dxn
(3.30)
and
Pxx
n|n−1 =

Rnx

Rnx

xn −ˆxn|n−1
 
xn −ˆxn|n−1
⊺p (xn|xn−1)
×p

xn−1|z1:n−1

dxn−1dxn
(3.31)
From the system dynamic equation (3.17), (3.30) can be rewritten as
ˆxn|n−1 =

Rnx

Rnx

fn−1 (xn−1) + un + vn−1

p (xn|xn−1)
×p

xn−1|z1:n−1

dxn−1dxn
=

Rnx

fn−1 (xn−1) + un + vn−1

p

xn−1|z1:n−1

dxn−1
×

Rnx
p (xn|xn−1) dxn
(3.32)
=

Rnx
fn−1 (xn−1) p

xn−1|z1:n−1

dxn−1 + un
+

Rnx
vn−1p

xn−1|z1:n−1

dxn−1
(3.33)
In moving from (3.32) to (3.33), the integral

p (xn|xn−1) dxn = 1, since the inte-
gral of a density function over the entire support region of that density is always 1.
Equation (3.33) consists of two density-weighted integrals that usually cannot be

INTRODUCTION TO RECURSIVE BAYESIAN ESTIMATION
51
solved analytically, except in special cases. If the noise vn−1 is zero-mean, a require-
ment for an unbiased estimator, the second integral reduces to zero.
Similarly, (3.31) reduces to
Pxx
n|n−1 =

Rnx

fn−1 (xn−1) + un −ˆxn|n−1
 
fn−1 (xn−1) + un −ˆxn|n−1
⊺
×p

xn−1|z1:n−1

dxn−1
−

Rnx

fn−1 (xn−1) + un −ˆxn|n−1

v⊺
n−1p

xn−1|z1:n−1

dxn−1
−

Rnx
vn−1

fn−1 (xn−1) + un −ˆxn|n−1
⊺p

xn−1|z1:n−1

dxn−1
+

Rnx
vn−1v⊺
n−1p

xn−1|z1:n−1

dxn−1
(3.34)
Deﬁning the noise covariance as
Q ≜

Rnx
vn−1v⊺
n−1p

xn−1|z1:n−1

dxn−1
(3.35)
and assuming that fn−1

xn−1,un

is uncorrelated with vn−1, the middle two integrals
in (3.34) reduce to zero, and the density-weighted predictive covariance becomes
Pxx
n|n−1 =

Rnx

fn−1 (xn−1) + un −xn|n−1
 
fn−1 (xn−1) + un −xn|n−1
⊺
×p

xn−1|z1:n−1

dxn−1 + Q
(3.36)
Thus, (3.33) and (3.36) are the predictive estimate of the ﬁrst-and second-order
moments of the state vector conditioned on all but the most current observation.
What is needed now is a method for recursively updating these predictive moments,
given the current observation of zn, so as to obtain corrected estimates of the ﬁrst
two moments of the posterior density without the need to evaluate (3.26) and (3.28)
directly.
3.4.2
State Vector Update
For any general density, Kalman developed such a ﬁlter without using Bayes’ Law [3].
He assumed that the system state xn could be estimated sequentially by updating only

52
GENERAL CONCEPTS OF BAYESIAN ESTIMATION
the ﬁrst and second order predictive moments. Speciﬁcally, he assumed that the up-
dating estimate for the ﬁrst moment be linearly dependent on the current observation,
that is,
ˆxn|n = Azo
n + b
(3.37)
where zo
n is the latest noisy observation and A and b are unknowns that are to be
determined.
The best linear estimate (in the minimum mean squared error (MMSE) sense) of
the random variable xn in terms of a current observation zo
n (which is also a random
variable) is such that
1. the conditional estimate is unbiased, that is, each component of the estimation
error (3.9) has zero mean, i.e., E

ϵx
n|z1:n−1
	
= 0 ∈Rnx, and
2. each component of the estimation error is uncorrelated with all components of
the observation, that is, E

ϵx
nzo⊺
n |z1:n−1
	
= 0 ∈Rnx×nz.
Note that both conditions are on the estimation error and not on the state or obser-
vation vectors themselves. Also, the latter condition requires that the estimation error
be orthogonal to the observations.
Using the deﬁnition of ϵx
n from (3.9), and also using equation (3.37), then applying
condition 1 leads to
E

ϵx
n|z1:n−1
	
= E

xn −ˆxn|n|z1:n−1
	
= E {xn|z1:n−1} −

AE

zo
n|z1:n−1
	
+ b

= ˆxn|n−1 −Aˆzn|n−1 −b = 0
(3.38)
Thus
b = ˆxn|n−1 −Aˆzn|n−1
(3.39)
From (3.37), follows immediately that
ˆxn|n = ˆxn|n−1 + A

zo
n −ˆzn|n−1

(3.40)
Condition 2, the orthogonality condition, requires that
E

ϵx
nzo⊺
n |z1:n−1
	
= E

xn −ˆxn|n−1 −A

zo
n −ˆzn|n−1

zo⊺
n
	
(3.41)

INTRODUCTION TO RECURSIVE BAYESIAN ESTIMATION
53
Note that ˆxn|n−1 and ˆzn|n−1 are conditioned on z1:n−1. Since E

ϵx
n
	
= 0, it follows
that E

ϵx
nzo⊺
n
	
= E

ϵx
n

zo
n −ˆzn|n−1
⊺	
, and (3.41) can be written as
E

ϵx
nzo⊺
n |z1:n−1
	
= E

xn −ˆxn|n−1 −A

zo
n −ˆzn|n−1

×

zo
n −ˆzn|n−1
⊺|z1:n−1
	
= Pxz
n|n−1 −APzz
n|n−1 = 0
(3.42)
Solving the last equation for A yields
A = Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
(3.43)
Combining (3.39) and (3.43) with (3.37) yields the linear MMSE estimator
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

(3.44)
Here, the innovations (error in the observation prediction) is deﬁned as
ϵz
n ≜zo
n −ˆzn|n−1
(3.45)
The updated covariance can be obtained from
Pxx
n|n = E

ϵx
nϵx⊺
n |z1:n−1
	
= E

xn −ˆxn|n
 
xn −ˆxn|n
⊺|z1:n−1
	
= E

xn −ˆxn|n−1 −Kn

zo
n −ˆzn|n−1

×

xn −ˆxn|n−1 −Kn

zo
n −ˆzn|n−1
⊺|z1:n−1
	
(3.46)
After some algebra, this becomes
Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
(3.47)
To summarize, (3.44) and (3.47), along with the Kalman gain deﬁnition (3.43), are
the Kalman ﬁlter update equations that utilize the latest observation to update the pre-
dictive estimates given by (3.33) and (3.36). However, examination of these equations
shows the need for the computation of additional predictive density-weighted inte-
grals for zn|n−1, Pzz
n|n−1 and Pxz
n|n−1. In general, these observation-related predictive
estimates are deﬁned by
zn|n−1 ≜E {zn|xn, z1:n−1} ,
(3.48)
Pzz
n|n−1 ≜E

zn −ˆzn|n−1
 
zn −ˆzn|n−1
⊺|xn, z1:n−1
	
(3.49)
Pxz
n|n−1 ≜E

xn −ˆxn|n−1
 
zn −ˆzn|n−1
⊺|z1:n−1
	
(3.50)

54
GENERAL CONCEPTS OF BAYESIAN ESTIMATION
Following the arguments used to derive (3.33) and (3.36), it follows immediately
that
zn|n−1 =

Rnx
hn (xn) p

xn|z1:n−1

dxn
+

Rnx
wnp

xn|z1:n−1

dxn
(3.51)
Pzz
n|n−1 =

Rnx

hn (xn) −ˆzn|n−1
 
hn (xn) −ˆzn|n−1
⊺
×p

xn|z1:n−1

dxn + R
(3.52)
Pxz
n|n−1 =

Rnx

xn −ˆxn|n−1
 
hn (xn) −ˆzn|n−1
⊺
×p

xn|z1:n−1

dxn
(3.53)
where R is deﬁned as
R ≜

Rnx
wnw⊺
np

xn|z1:n−1

dxn
(3.54)
For (3.52) it has been assumed that hn (xn) is independent of wn, so that the cross-
terms integrate to zero. It is usually assumed that wn is zero-mean so that the second
integral in (3.51) integrates to zero.
A general block diagram for the recursive point estimation process for arbitrary
distributions is shown in Figure 3.2.
n
n n
n n
n n
n n
n
n
n n
n n
n n
n
n n
n
xz
zz
xx
xx
zz
K
P
P
x
x
K
z
z
P
P
K P
K
n n
n n
xx
x
P
n n
n
n
n
n
n
n n
n
n
n n
n
n
n n
n
n
n
p
d
p
d
xx
x
f x
u
x
z
x
f
P
x
u
x
f x
u
x
x
z
x
Q
n n
n
n
n
n
n n
n
n n
n
n n
n
n
n
n n
n
n
n n
n
n n
n
n
n
p
d
p
d
p
d
zz
xz
z
h x
x
z
x
h
P
x
z
h x
z
x
z
x
R
f
P
x
u
x
h x
z
x
z
x
p x
Kalman ﬁlter correcon step
Observaon predicon step
Track ﬁle store
State predicon step
FIGURE 3.2
General block diagram for recursive point estimation process.

REFERENCES
55
3.5
DISCUSSION OF GENERAL ESTIMATION METHODS
Inthischapter,twomainresultshavebeendevelopedforBayesianestimationthathave
almost no restrictions on either the form of the probability densities or on the linearity
of the dynamic or observation equations. The ﬁrst, through the use of (3.23) and (3.24),
is a method for a recursive link between the previous posterior p

xn−1|z1:n−1

and
the current posterior p

xn|z1:n

that requires speciﬁcation of the predictive density
given by p (xn|xn−1) and the likelihood function p

zn|xn

.
In many problems of interest, recursive Monte Carlo evaluation of these densi-
ties provide visual insight into the probable location and spread of the state vector,
conditioned on all observations. Part III of this book will expand on these Monte
Carlo estimation methods applicable to nonlinear systems and non-Gaussian densi-
ties (linear and Gaussian cases are included as a subset). In addition, Monte Carlo
methods will be presented that allow evaluation of the density-weighted integrals,
where Monte Carlo samples of xn are used to create discrete density functions that
reduce the integrals to weighted sums of the samples. These latter methods have come
to be called particle ﬁlters.
The second estimation process developed in this chapter is a general method for
recursively estimating the ﬁrst two moments of a state vector conditioned on all
observations. This method uses density-weighted integrals for prediction estimates
xn|n−1, Pxx
n|n−1,zn|n−1, Pzz
n|n−1, and Pxz
n|n−1, given by (3.33), (3.36), (3.51), (3.52), and
(3.53), respectively. This is followed by the Kalman ﬁlter update equations (3.43),
(3.44), and (3.47).
Up to this point, developments has been kept completely general with no restric-
tions on the form of the dynamic or observation process or on the form of the proba-
bility density. Part II of this book will concentrate exclusively on the form this point
estimation method takes when the densities are Gaussian. This Gaussian assumption
leads to all of the Kalman ﬁlter variants found in the literature. The Kalman ﬁlter
variants will be shown to fall into three main categories, the linear Kalman ﬁlter,
extended Kalman ﬁlters, and sigma point Kalman ﬁlters where Gaussian-weighted
integrals are evaluated numerically.
REFERENCES
1. Van Trees HL. Detection, Estimation, and Modulation Theory, Part I: Detection, Estimation,
and Linear Modulation Theory. Wiley; 1968.
2. Doucet A, Johansen AM. A Tutorial on Particle Filtering and Smoothing: Fifteen Years
Later. Technical Report, Department of Statistics, University of British Columbia; 2008.
3. Kalman RE. A new approach to linear ﬁltering and prediction problems. ASME J. Basic
Eng. 1960:35–45.

4
CASE STUDIES: PRELIMINARY
DISCUSSIONS
This book contains four case studies that illustrate the application and performance of
many of the tracking ﬁlters developed in Parts II and III. The ﬁrst case study, developed
in this chapter, illustrates the problem of tracking a ship through a ﬁeld of directional
frequency and ranging (DIFAR) sonobuoys. This is a particularly interesting problem
because the noise on the bearing observations obtained from each individual buoy is
Gaussian for input signals with high signal-to-noise ratio (SNR) and non-Gaussian
for signal with low SNR. This case study will allow us to examine the utility of the
tracking ﬁlters in the low SNR environment where the observation noise becomes
non-Gaussian. We use this case study in Parts II and III to show how many of the
tracking ﬁlters can be utilized for this DIFAR problem, presenting implementation
details for each estimation method along with the track outputs plotted against truth
tracks. We also assess relative track performance measures for all of the tracking
algorithms.
In Part IV of this book the remaining case studies will be discussed, including
tracking of a object moving in three dimensions using radar observation data; tracking
of a falling bomb from video feed observations (photogrammetry); and the use of
tracking ﬁlters in data fusion for photogrammetry tracking systems. The case studies
range from the relatively benign problem in DIFAR bearings-only tracking to a really
difﬁcult one that requires large state and observation vectors and high data rates. We
show how, for some problems, all of the tracking ﬁlters give essentially the same
results, while for others, some of the tracking ﬁlters cannot be used due to some
inherent requirement that cannot be easily met by speciﬁc tracking ﬁlters.
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
56

THE OVERALL SIMULATION/ESTIMATION/EVALUATION PROCESS
57
4.1
THE OVERALL SIMULATION/ESTIMATION/EVALUATION
PROCESS
Setting up software code for a speciﬁc tracking problem scenario can sometimes be
a daunting task. In Figure 4.1, we present a diagram of the process blocks that must
be addressed. First, one must build a scenario simulator that generates the truth data
for the states to be estimated. This truth data set must then be transformed into a set
of truth observations, prior to adding noise.
Bywayofexample,fortrackingavehicleinthreeCartesiandimensions,thevehicle
truth position and velocity components in Cartesian coordinates (which we call the
truth track) must be generated. If the observations are from a radar, the Cartesian
truth track must undergo a Cartesian-to-spherical conversion, generating the noiseless
spherical truth tracks that include range, bearing, elevation, and their respective rates.
Then, the speciﬁc radar observations (e.g., range, bearing, elevation, and/or Doppler
range rate) can be isolated from the spherical truth. In exercising a tracking ﬁlter using
data obtained from a real radar, this last simulation step can be omitted. The simulation
step is primarily used to test and debug the estimation algorithm implementation
and for the conduct of special studies to evaluate comparative tracking algorithm
performance.
In the second block, usually a Monte Carlo exercise of the tracking algorithm under
test, noise is added to the observations and the noisy observations are used as input to
Scenario simulator
Set dynamic/kinematic model
Simulate truth states
Set dynamic noise parameters
Simulate noise-free observations
Monte Carlo
estimation/tracking
Add noise to observation truth
Execute tracking filter
Performance evaluation
Create error ellipses
Compute RMS errors
 Compute Cramer–Rao lower bound
Truth
track
Noiseless
observations
Estimated track and
track covariance
FIGURE 4.1
Methodology for development and evaluation of tracking algorithms.

58
CASE STUDIES: PRELIMINARY DISCUSSIONS
the tracking ﬁlter. The desired tracking ﬁlter is then exercised to generate track state
and track state error covariance estimates as a function of time.
The ﬁnal block is where the performance of the tracking algorithm is assessed. For
simulated observations, one can use truth track data along with estimated tracks in a
series of Monte Carlos runs to generate root mean squared (RMS) errors and examine
how these RMS errors compare with the Cramer–Rao Lower Bound (CRLB). When
real system data is used, the truth tracks are usually not available, so one can use the
estimated track error covariances to generate error ellipses for the estimated tracks.
Each process block of the overall simulation effort can be developed independently
since, for the most part, they are invariant with respect to the speciﬁc tracking
algorithm.
In the section below, the algorithmic underpinnings of the DIFAR tracking scenario
simulator will be discussed along with plots describing the particulars of the problem.
For each of the track estimation methods presented in Parts II and III we will show
track estimate results for the DIFAR problem as an example of a speciﬁc application of
eachalgorithm.PerformancecomparisonsofthevariousGaussianestimationmethods
for the DIFAR problem will be presented in Section 14.5 while the performance for
particle ﬁlter estimation methods for the non-Gaussian noise case will be delayed
until the end of Part III of this book.
4.2
A SCENARIO SIMULATOR FOR TRACKING A CONSTANT
VELOCITY TARGET THROUGH A DIFAR BUOY FIELD
Our DIFAR scenario consists of a ship traveling at constant velocity moving through
a ﬁeld of DIFAR buoys. The coordinate system we will use is a two-dimensional
Cartesian system with an origin at the center of the buoy ﬁeld, the x-axis pointing
East, and the y-axis pointing North.
4.2.1
Ship Dynamics Model
Let the ship motion at time t be described by a state vector x (t) that consists of the
position and speed components in the two dimensions, that is
x (t) =
rx (t)
vx
ry (t)
vy ⊺
(4.1)
where {rx (t) , ry (t)} and {vx, vy} are the Cartesian position and speed components,
respectively. We have chosen a constant velocity model, so that the speed components
are independent of time. Given that the ship’s position at the start of the scenario is
{rx (t0) , ry (t0)} , for our scenario the position components of the ship at any later
time t can be written as
rx (t) = rx (t0) + vxt
(4.2)
ry (t) = ry (t0) + vyt
(4.3)

A SCENARIO SIMULATOR FOR TRACKING A CONSTANT VELOCITY TARGET
59
In vector–matrix notation, this can be rewritten as
x (t) = Ftx (t0)
(4.4)
with
Ft =

Ft,1
02
02
Ft,1

(4.5)
and
Ft,1 =

1
t
0
1

(4.6)
4.2.2
Multiple Buoy Observation Model
Assume that their are M buoys randomly deployed within a radius Rmax around the
origin, with the mth buoy at a ﬁxed position {xm, ym}. Of course, in a real buoy ﬁeld
tracking exercise, this assumption of ﬁxed buoy positions over the time frame of the
track estimation will not be true because of drift currents and wave action. But for
algorithm demonstration, we will assume ﬁxed buoy positions.
For a bearing deﬁned as an angle clockwise relative to true North (y-axis), we can
write the bearing from the mth buoy to the target at time t as
θm (t) = tan−1
rx (t) −xm
ry (t) −ym

(4.7)
with −π ≤θm (t) ≤π.
4.2.3
Scenario Speciﬁcs
The ﬁrst two steps in scenario generations therefore become:
1. Generate a truth track using (4.2) and (4.3) over a uniformly spaced set of times
{tn; n = 0, 1, . . . , N}.
2. Execute a Cartesian to polar conversion to generate the noise-free observations
over the time frame of the scenario using (4.7).
An illustration of the geometry of the problem for the target ship track relative to
the center of the buoy ﬁeld at (0, 0) is shown in Figure 4.2.
An example of the ship truth track and the truth bearings associated with two of the
DIFAR sensors are shown in Figure 4.3. The panel on the left shows a small portion
of the ships truth track moving from the lower left to the upper right. Also shown on
the left panel is an example of a buoy ﬁeld with 15 buoys randomly distributed about
the origin. The right panel shows the bearing to the ship from the two buoys marked

60
CASE STUDIES: PRELIMINARY DISCUSSIONS
[rx(t),ry(t)]
Target ship
Y (North)
X (East)
(0,0)
FIGURE 4.2
DIFAR buoy geometry.
in the left panel, one with a ∗and the other with an x. For this plot, the buoys were
distributed in range and bearing about the origin according to
ρm = Rmax ∗um, m = 1, . . . , M
ϑm ∼U (−π, π) , m = 1, . . . , M
0
1
2
0
–1
–1
1
2
3
0
100
200
0
100
200
–200
300
400
500
600
700
800
Time (s)
Bearing (deg)
x-axis location (nmi)
y-axis location (nmi)
–2
–2
–3
–100
FIGURE 4.3
Ships track with DIFAR buoy ﬁeld and the associated bearings for two buoys.

A SCENARIO SIMULATOR FOR TRACKING A CONSTANT VELOCITY TARGET
61
with Rmax set to 500 feet [um ∼U (a, b) indicates that um is drawn from a
uniform distribution over the interval (a, b)]. The set of polar buoy positions

ρm, ϑm; m = 1, . . . , M

are then transformed into the Cartesian coordinate set
{xm, ym; m = 1, . . . , M} using
xm = ρm sin ϑm
(4.8)
ym = ρm cos ϑm
(4.9)
In Figure 4.3, the right panel shows the bearings associated with the two marked
buoys, one on either side of the ships track. From the ﬁgure we see that the bearings
for the buoy to the East (right side) of the ships track go through zero deg, while the
bearings from the buoy to the West go to −180 deg and then jump up to 180 deg.
The bearings for the West buoy undergo what is known as a phase wrap, where
it makes a jump of 360 deg in bearing. We will show in Chapter 6 that this phase
wrap presents a problem to all tracker algorithms that must be corrected to make the
trackers work properly. Similar phase wrap difﬁculties occur in the application of
estimation methods in many different ﬁelds and must be addressed in order to obtain
valid results. A Matlab subroutine that can be used to generate a ships Cartesian and
spherical truth track as well as a set of randomized buoy positions is presented in
Listing 4.1.
Listing 4.1 Generation of Truth Track and Buoy Positions
function [x_true,r_true,f,s,SNR,BT,SensorLocations,
R,theta_sigma,t] = DifarScenarioGenerator(Stream,...
N,M,Rmax,del_t,speed,heading,y_init)
global deg2rad hours2seconds
measurement_sigma = 3*deg2rad;
theta_sigma = 1;
t = del_t*(1:N);
speed = speed/hours2seconds;
heading = heading*deg2rad;
vx_init = speed*sin(heading);
vy_init = speed*cos(heading);
x_init = y_init*(vx_init/vy_init);
x_true_init = [x_init;vx_init;y_init;vy_init];
r_m = Rmax*rand(Stream,M,1);
theta_m = 2*pi*(rand(Stream,M,1) - 0.5);
xx = r_m.*sin(theta_m) + Sensor_Offset(1);
yy = r_m.*cos(theta_m) + Sensor_Offset(2);
Sensor_Locations = [xx yy];
R = diag(measurement_sigmaˆ2*ones(M,1),0);
[x_true,f,s,SNR,BT] = ...
Constant_Velocity_Kinematics_Difar(Stream,...

62
CASE STUDIES: PRELIMINARY DISCUSSIONS
N,t, x_true_init);
[r_true] = Cart2Polar(x_true,Sensor_Locations,N,M);
In Parts II and III of this book, many methods are presented that enable sequential
(in time) estimation of the ships track based on a set of noisy buoy observations. For
each estimation method, we use this DIFAR observation data to illustrate ﬁlter track
estimation outputs.
4.3
DIFAR BUOY SIGNAL PROCESSING
The DIFAR sensor (one of a class of vector sensors) is a passive receiver that operates
on the broadband signal radiated from a distant ship and converts the signal into a
noisy bearing to the ship using the process illustrated in Figure 4.4. The DIFAR buoy
consists of three independent sensors: an omnidirectional hydrophone, an East–West
dipole hydrophone, and a North–South dipole hydrophone. The dipole hydrophones
may not line up in the proper orientation but their outputs are electronically mixed
to produce two dipole outputs of the proper orientation. We will not discuss this
reorientation process further because it is irrelevant to the tracking problem and we
will always assume the output of each channel to be oriented with the Cartesian axes.
For simulation purposes, one needs to generate noisy bearing observations at the
output of each simulated DIFAR sensor. From Figure 4.4 we see that for the mth
sensor, the real signals output by each channel are given by
zNS (t) = s (t) cos θ + nNS (t)
(4.10)
zEW (t) = s (t) sin θ + nEW (t)
(4.11)
zO (t) = s (t) + nO (t)
(4.12)
where s(t) represents the signal at the input to the DIFAR buoy.
E/W
dipole
dipole
Omni
N/S
+
+
+
x
x
w
e
t
s
n
t
t
s
o
t
s
n
s
n
t
s
n
t
C
C
C
C
n
t
t
w
en
θ
on
s
nn
θ
θ
θ
FIGURE 4.4
DIFAR sensor signal processing.

DIFAR BUOY SIGNAL PROCESSING
63
The cross-correlations C1 (t) and C2 (t) are deﬁned by
C1 (t) = zNS (t) × zO (t)
= s2 (t) cos θ + s (t) nO (t) cos θ
+s (t) nNS (t) + nNS (t) nO (t)
(4.13)
and
C2 (t) = zEW (t) × zO (t)
= s2 (t) sin θ + s (t) nO (t) sin θ
+s (t) nEW (t) + nEW (t) nO (t)
(4.14)
Assuming that θ is adiabatic (slowly changing) in time, we can write the expected
values of C1 (t) and C2 (t) as
E {C1 (t)} = E
	
s2 (t)

cos θ
(4.15)
E {C2 (t)} = E
	
s2 (t)

sin θ
(4.16)
where we have assumed that the signal and noise components are zero-mean Gaussian
processes that are all uncorrelated across sensors and time. Thus, the bearing can be
obtained from
θ (t) = tan−1 E {C2 (t)}
E {C1 (t)}
(4.17)
In order to estimate E {C1 (t)} and E {C2 (t)}, sample zNS (t), zEW (t), and zO (t) ,
and subdivide time into blocks of length T. Now, let
τq = tn + (i −1) δt −N
2 δt, i = 1, . . . , N; n = 1, 2, . . . , ∞; q = 1, 2, . . . , ∞
(4.18)
with δ = T/N. For example, we would write
τ1 = t1 −N
2 δt
τ2 = t1 + δt −N
2 δt
...
τN = t1 + (N −1) δt −N
2 δt = t1 +
N
2 −1

δt
τN+1 = t2 −N
2 δt
(4.19)

64
CASE STUDIES: PRELIMINARY DISCUSSIONS
At time tn, E {C1 (tn)} and E {C2 (tn)} can now be estimated from
E {C1 (tn)} ≃
tn+ N
2 δt

tn−N
2 δt
zNS (t) zO (t) dt
(4.20)
E {C2 (tn)} ≃
tn+ N
2 δt

tn−N
2 δt
zEW (t) zO (t) dt
(4.21)
From the Weiner–Khinchine theorem [1], (4.20) and (4.21) can be written as
E {C1 (tn)} =
∞

−∞
YNS (tn, fk) Y∗
O (tn, fk) df
(4.22)
E {C2 (tn)} =
∞

−∞
YEW (tn, fk) Y∗
O (tn, fk) df
(4.23)
where the ∗represents a complex conjugate and
YNS (tn, fk) = S (tn, fk) cos θ + NNS (tn, fk)
(4.24)
YEW (tn, fk) = S (tn, fk) sin θ + NEW (tn, fk)
(4.25)
YO (tn, fk) = S (tn, fk) + NO (tn, fk)
(4.26)
For sampled data, the time-domain equations (4.20) and (4.21) become
E {C1 (tn)} ≃1
T
N
i=1
zNS (aiδt) zO (aiδt)
(4.27)
E {C2 (tn)} ≃1
T
N
i=1
zEW (aiδt) zO (aiδt)
(4.28)
where
ai ≜i −N + 2
2
(4.29)
In the frequency domain, taking the Fast-Fourier transform (FFT) of N sam-
ples of each of the set {zNS (t) , zEW (t) , zO (t)} yields the frequency sample set
{YNS (tn, fk) , YEW (tn, fk) , YO (tn, fk)} that leads to
E {C1 (tn)} ≃δf
N/2
N/2

k=1
YNS (tn, fk) Y∗
O (tn, fk)
(4.30)
E {C2 (tn)} ≃δf
N/2
N/2

k=1
YEW (tn, fk) Y∗
O (tn, fk)
(4.31)

DIFAR BUOY SIGNAL PROCESSING
65
TABLE 4.1
Procedure for Generating a Vector of DIFAR Noisy Bearing Observations
Step 1.
Generate truth bearings.
θm (t) = tan−1 
rx(t)−xm
ry(t)−ym

Step 2.
Generate COMPLEX random
noises for each channel.
NNS (tn, fk) , NEW (tn, fk) , NO (tn, fk)
Step 3.
Generate COMPLEX random
signal.
S (tn, fk)
Step 4.
Calculate DIFAR channel
YNS,m (tn, fk) = S (tn, fk) cos θm
outputs.
+NNS (tn, fk)
YEW,m (tn, fk) = S (tn, fk) sin θm
+NEW (tn, fk)
YO (tn, fk) = S (tn, fk) + NO (tn, fk)
Step 5.
Calculate convolution
C1,m (tn) = YNS (tn, fk) Y ∗
O (tn, fk)
products.
C2,m (tn) = YEW (tn, fk) Y ∗
O (tn, fk)
Step 6.
Sum over frequency band
E
C1,m (tn)
=
δf
N/2
N/2
k=1 C1,m (tn)
E
C2,m (tn)
=
δf
N/2
N/2
k=1 C2,m (tn)
Step 7.
Compute noisy bearings
θn,m = tan−1 
E{C2,m(tn)}
E{C1,m(tn)}

The procedure for generating noisy bearing observations from a ship truth track for
each DIFAR sensor is presented in Table 4.1. In the Listing 4.2 we present a Matlab
snippet that produces a noisy wideband signal. A Matlab subroutine that executes the
procedure given in Table 4.1 is shown in Listing 4.3.
Listing 4.2 A Snipet of Matlab Code that Produces N Noisy Signal Samples for K Frequency
Bins
f = 300:15:400;
f_target = [10 20 50 100 200 400 1000 4000]’;
SPL_target = [20;20;20;20;20;20;20;20];
SPL = interp1q(log10(f_target),SPL_target,log10(f’));
spl = repmat(10.ˆ(SPL/10),1,N);
SNR = spl(1);
K = length(f);
BT = (f(K) - f(1))*(t(2)-t(1));
SPL = interp1q(log10(f_target),SPL_target,log10(f’));
spl = repmat(10.ˆ(SPL/10),1,N);
SNR = spl(1);
K = length(f);

66
CASE STUDIES: PRELIMINARY DISCUSSIONS
BT = (f(K) - f(1))*(t(2)-t(1));
s = sqrt(spl/2).*(randn(Stream,K,N) + ...
1i*randn(Stream,K,N));
K = length(f);
BT = (f(K) - f(1))*(t(2)-t(1));
Listing 4.3 A Matlab Subroutine that Generates the Noisy Bearing Measurements Using the
Steps in Table 4.1
function [bearing_measurement] = ...
First_Sensor_Measurement_Generator_Difar(f,s,...
theta_sigma,theta_true,N,M,Stream)
K = length(f);
del_f = f(2) - f(1);
NS_ms = sqrt(theta_sigma)*(randn(Stream,K,N,M) +...
1i*randn(Stream,K,N,M));
EW_ms = sqrt(theta_sigma)*(randn(Stream,K,N,M) +...
1i*randn(Stream,K,N,M));
O_ms = sqrt(theta_sigma)*(randn(Stream,K,N,M) +...
1i*randn(Stream,K,N,M));
Y_NS = repmat(s,[1,1,M]).*...
repmat(cos(theta_true),[K,1,1]) + NS_ms;
Y_EW = repmat(s,[1,1,M]).*...
repmat(sin(theta_true),[K,1,1]) + EW_ms;
Y_O = repmat(s,[1,1,M]) + O_ms;
C1 = real(Y_NS.*conj(Y_O));
C2 = real(Y_EW.*conj(Y_O));
E_C1 = (2*del_f/N)*permute(sum(C1),[2 3 1]);
E_C2 = (2*del_f/N)*permute(sum(C2),[2 3 1]);
bearing_measurement = permute(atan2(E_C2,E_C1),[2 1]);
One must note that the ratio inside the inverse tangent operation contained in Step 7
of Table 4.1 is a ratio of random numbers. It is well known that if both variables are
independent with zero mean, the distribution of the ratio is a Cauchy distribution.
However, when the two variables do not have a zero mean, the distribution of the
ratio is much more complicated, as we will see in the next section. In addition, for
the DIFAR bearings, the distribution is further complicated by the inverse tangent
operation. So, we cannot expect the DIFAR bearing observations to have a Gaussian
distribution! However, we can expect the distribution to be a function of the SNR

THE DIFAR LIKELIHOOD FUNCTION
67
0
10
20
30
40
50
60
70
80
90
100
−100
−50
0
50
100
150
200
Monte Carlo sample number
Bearing observation
 
 
SNR = 0 dB
SNR = 10 dB
SNR = 20 dB
SNR = 30 dB
FIGURE 4.5
The spread of bearing observations from a DIFAR buoy from 100 Monte Carlo
runs for four SNRs.
of the signal at the input to the DIFAR buoy. To illustrate this, Figure 4.5 show the
bearing observations of one buoy at one speciﬁc time from 100 Monte Carlo runs for
four different signal SNRs (at the input to the DIFAR sensor). It is clear that as SNR
increases, the distribution becomes more peaked and approximates a Gaussian, but as
the SNR decreases, the bearing distribution approaches that of a uniform distribution
over [−180o, 180o].
4.4
THE DIFAR LIKELIHOOD FUNCTION
If we assume that the DIFAR sensor likelihood functions are independent from sensor
to sensor, then the likelihood function for the observation vector conditioned on the
target ship position is given by the joint density
p (zn|xn) = p (θn|xn) =
M
m=1
p

θn,m|xn

(4.32)
where θn =

θn,1, . . . , θn,M
⊺with
θn,m = tan−1
rx
n −xm
ry
n −ym

(4.33)
Although it is an interesting problem in itself, the full derivation of the likelihood
function for a DIFAR buoy is not the main emphasis of this book, so any interested
reader can ﬁnd the derivation in the appendix of Ref. [2], available online. Here,

68
CASE STUDIES: PRELIMINARY DISCUSSIONS
because it is needed in Part III of this book, we present just the resulting likelihood
density and its properties.
If we ignore the time index for clarity and let zm ≜tan θm, then from Appendix
A in Ref. [2] we can write
p (θm|xn) =

1 + (zm)2
p

zm|θn,m

(4.34)
with
p (zm|θm) = A




1 −r2
π
e−α + δβ1/2
√
2π
×

	


−δβ1/2

1 −r2

−	


δβ1/2

1 −r2



e−γ



(4.35)
Here
A ≜
$σ1$σ2
$σ2
2 −2r$σ1$σ2zm + $σ2
1z2m
(4.36)
α ≜$σ2
2η2
1 −2r$σ1$σ2$η1$η2 + $σ2
1$η2
2
2$σ2
1$σ2
2

1 −r2
(4.37)
β ≜

$σ2
2$η1 −r$σ1$σ2$η2

+

$σ2
1$η2 −r$σ1$σ2$η1

zm
2
$σ2
1$σ2
2

$σ2
2 −2r$σ1$σ2zm + $σ2
1z2m

(4.38)
γ ≜

$η2 −$η1zm
2
2

$σ2
2 −2r$σ1$σ2zm + $σ2
1z2m

(4.39)
δ ≜sign

$σ2
2$η1 −r$σ1$σ2$η2

+

$σ2
1$η2 −r$σ1$σ2$η1

zm

(4.40)
with
$η1 ≜
√
2BTSNR cos θ(i)
m
(4.41)
$η2 ≜
√
2BTSNR sin θ(i)
m
(4.42)
$σ2
1 ≜(2SNR + 1) SNR cos2 θ(i)
m + ρ (SNR + 1)
(4.43)
$σ2
2 ≜(2SNR + 1) SNR sin2 θ(i)
m + ρ (SNR + 1)
(4.44)
r ≜(2SNR + 1) SNR cos θ(i)
m sin θ(i)
m
$σ1$σ2
(4.45)
	 (x) ≜
1
√
2π
% ∞
x
e−u2/2du
(4.46)

REFERENCES
69
−20
−10
0
10
20
30
0
1
2
3
4
5
6
Bearing (deg)
Probability density
10dB
0 dB
−10 dB
FIGURE 4.6
DIFAR likelihood density for a BT of 10 and SNR from −10 to +10 dB.
SNR is the signal-to-noise ratio at the monopole channel, BT is the timebandwidth
product of the vehicle frequency signature and ρ is the noise gain (loss) factor applied
to either dipole channel and 	 (x) is related to the error function. The value of ρ = 1/2
or 1/3 for 2D-isotropic or 3D-isotropic noise, respectively [3].
Note that the likelihood function given by (4.34) is actually the conditional
likelihood p (θ|θ0), where θ0 is the true bearing to the target ship based on (4.7)
and θ represents the set of bearings at which the likelihood is calculated. A plot of
p (θ|θ0) is shown in Figure 4.6 for a bandwidth-time product (BT) of 10 and a range
of SNRs (−10 ≤SNR ≤10). One can readily see that for low SNRs the likelihood
is far from Gaussian because of the excessively high tails and the fact that it is only
deﬁned over the range −π ≤θ ≤π.
REFERENCES
1. Marple SL. Digital Spectral Analysis with Applications. Prentice-Hall: Englewood Cliffs,
NJ; 1987.
2. Haug AJ. A Tutorial on Bayesian Estimation and Tracking Techniques Applicable to Non-
linear and Non-Gaussian Processes. MITRE Technical Report MTR 05W0000004; 2005.
3. Maranda BH. The statistical accuracy of an arctangent bearing estimator. OCEANS 2003;
4:2127–2132.

PART II
THE GAUSSIAN ASSUMPTION:
A FAMILY OF KALMAN FILTER
ESTIMATORS

5
THE GAUSSIAN NOISE CASE:
MULTIDIMENSIONAL INTEGRATION
OF GAUSSIAN-WEIGHTED
DISTRIBUTIONS
At the end of Chapter 3, the Bayesian point estimation equations were developed using
general probability density functions. In this part of the book, it will be assumed that
the dynamic and observation equations constitute Gaussian processes. Under this
assumption all of the distribution functions contained in the point estimator equations
become Gaussian. It is well known that the ﬁrst two moments of a Gaussian density
characterize the density completely. Therefore, a recursive propagation of estimates of
the ﬁrst two moments produces an optimal estimation method for Gaussian processes.
The subject of this part of the book includes the derivation of numerous methods for
solving the density-weighted predictive point estimates developed in Chapter 3 for
the special case of Gaussian densities.
In Section 3.4, a set of Kalman ﬁlter update equations were developed that levied
no requirements on the linearity of the processes or the form of the densities associated
with those processes. The main assumption made was that the posterior point estimate
of xn, written as ˆxn|n, be a linear function of the latest observation zo
n. Additional re-
quirements on the point estimator included that the estimation error be zero-mean and
be uncorrelated with the observation vector. It is shown in the sections that follow how
these Kalman ﬁlter update equations arise naturally from the Gaussian distribution
because it automatically fulﬁlls all of these original assumptions and requirements.
Armed with the general form of the Bayesian point estimation equations from
Chapter 3, Section 5.3 is devoted to showing that the predictive estimation equa-
tions reduce to Gaussian-weighted integrals. In the chapters that follow, the Kalman
ﬁlter predictive Gaussian-weighted integrals are solved numerically for a variety of
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
73

74
THE GAUSSIAN NOISE CASE
assumptions and approximations concerning the form of the dynamic and observa-
tion functions fn (xn) and hn (xn), respectively. These solutions lead to four classes
of Kalman ﬁlters:
• The Linear Class: When the dynamic and observation equations are both linear
and all densities are Gaussian, the integrals can be solved directly leading to the
linear Kalman ﬁlter (LKF).
• The Analytical Linearization Class: When all nonlinear functions are expanded
in Taylor polynomials and only the linear terms are maintained, the integrals
can again be solved directly leading to a Kalman ﬁlter form almost identical to
that of the LKF. But an additional step requires the computation of the Jacobian
of each nonlinear function. These ﬁlters consist of the extended Kalman ﬁlter
(EKF) and all of its variants.
• The Sigma Point Class: For this class, the nonlinear functions are expanded in
more general polynomials such that the integrals reduce to weighted summations
over a set of deterministic vector points, called sigma points. Speciﬁc ﬁlters
of this type include the ﬁnite difference Kalman ﬁlter (FDKF), the unscented
Kalmanﬁlter(UKF),thesphericalsimplexKalmanﬁlter(SSKF),andtheGauss–
Hermite Kalman ﬁlter (GHKF).
• The Monte Carlo Class: If a set of Monte Carlo samples are drawn from the
Gaussian density, creating a discrete density, then the integrals reduce to a sum
over discrete random sample points. This method leads to the Monte Carlo
Kalman ﬁlter (MCKF).
Much of the material presented in this part of the book is based on ideas whose
origin lie in a paper by Wu et al. [1]. This paper outlines and compares most of the so-
lutions developed to date for Gaussian-weighted integration. It formulated the thread
that can be used to link all of the Kalman ﬁlter algorithms into a single framework.
Through this paper, the author discovered the vast literature related to numerical
integration methods that can be used to understand all of the recursive Bayesian
point-estimation methods for solving Gaussian-weighted integrals. Some of the re-
lated references used to formulate this part of the book include the paper by McNamee
and Stenger [2] and the PhD dissertation of Lerner [3], both of which inspired the
work of Wu et al. Equally important to the development of this entire book were the
numerical integration books by Davis and Rabinowitz [4] and Evans and Swartz [5].
Finally, the paper by Ito and Xiong [6] was seminal in providing initial insight into
the general concepts of Gaussian ﬁlters.
5.1
SUMMARY OF IMPORTANT RESULTS FROM CHAPTER 3
The important results of Chapter 3 are summarized. Consider a dynamic process of
the form
xn = fn−1 (xn−1) + un + vn−1
(5.1)

SUMMARY OF IMPORTANT RESULTS FROM CHAPTER 3
75
with observations generated from the observation process
zn = hn (xn) + wn
(5.2)
A point prediction estimate of the state vector at time tn, based on all observations up
to and including time tn−1, is given by the density weighted integral
ˆxn|n−1 =

Rnx

fn−1 (xn−1) + un + vn−1

p

xn−1|z1:n−1

dxn−1
(5.3)
Similarly, a point prediction estimate of the state covariance at time tn, based on all
observations up to and including time tn−1, is given by the density weighted integral
Pxx
n|n−1 =

Rnx

fn−1 (xn−1) + un −ˆxn|n−1
 
fn−1 (x) + un −ˆxn|n−1
⊺
×p

xn−1|z1:n−1

dxn−1 + Q
(5.4)
In addition, point prediction estimates of the observation vector zn, the covariance
of zn, and its cross-covariance with xn are given by
zn|n−1 =

Rnx
[hn (xn) + wn] p

xn|z1:n−1

dxn
(5.5)
Pzz
n|n−1 =

Rnx

hn (xn) −ˆzn|n−1
 
hn (xn) −ˆzn|n−1
⊺
×p

xn|z1:n−1

dxn + R
(5.6)
Pxz
n|n−1 =

Rnx

xn −ˆxn|n−1
 
hn (xn) −ˆzn|n−1
⊺
×p

xn|z1:n−1

dxn
(5.7)
where Q and R are deﬁned by
Q ≜

Rnx
vn−1v⊺
n−1p

xn−1|z1:n−1

dxn−1
(5.8)
and
R ≜

Rnx
wnw⊺
np

xn|z1:n−1

dxn
(5.9)

76
THE GAUSSIAN NOISE CASE
5.2
DERIVATION OF THE KALMAN FILTER CORRECTION (UPDATE)
EQUATIONS REVISITED
In this section, we provide an alternate derivation of the Kalman ﬁlter correction
equations (3.43), (3.44), and (3.47), based on the assumption that all conditional
densities are Gaussian. Bayes’ law provides a link between the posterior density and
the joint density of xn with zn resulting in
p

xn|z1:n

= p

xn, zn|z1:n−1

p (zn)
(5.10)
Now deﬁning the joint vector
q ≜

xn
zn

(5.11)
(5.10) can be written as
p

xn|z1:n

= p

qn|z1:n−1

p (zn)
(5.12)
The general form for a multivariate Gaussian distribution can be written as
N (t; s, ) ≜
1
√(2π)n || exp
	
−1
2 (t −s)⊺−1 (t −s)

(5.13)
Let all densities in (5.12) be Gaussian so that
p

xn|z1:n

= N

xn; ˆxn|n, Pxx
n|n

(5.14)
p

qn|z1:n−1

= N

qn; ˆqn|n−1, Pqq
n|n−1

(5.15)
p (zn) = N

zn; ˆzn|n−1, Pzz
n|n−1

(5.16)
Ignoring the factor of (−1/2), the exponent of the exponential in the analytical
expression for the posterior Gaussian density (5.14) can now be written out explicitly
as
d ≜

xn −ˆxn|n
⊺
Pxx
n|n
−1 
xn −ˆxn|n

= x⊺
n

Pxx
n|n
−1 xn −x⊺
n

Pxx
n|n
−1 ˆxn|n
−x⊺
n|n

Pxx
n|n
−1 xn + x⊺
n|n

Pxx
n|n
−1 ˆxn|n
(5.17)
Similarly, the exponent in the analytical expression for the ratio of the Gaussian
densities on the right-hand side of (5.15) is given by
g =

qn −ˆqn|n−1
⊺
Pqq
n|n−1
−1 
qn −ˆqn|n−1

−

zn −ˆzn|n−1
⊺
Pzz
n|n−1
−1 
zn −ˆzn|n−1

(5.18)

DERIVATION OF THE KALMAN FILTER CORRECTION
77
Using (5.11), (5.18) can be written more explicitly as
g =

xn −ˆxn|n−1
zn −ˆzn|n−1
⊺
Pxx
n|n−1
Pxz
n|n−1
Pzx
n|n−1
Pzz
n|n−1
−1 
xn −ˆxn|n−1
zn −ˆzn|n−1

−

zn −ˆzn|n−1
⊺
Pzz
n|n−1
−1 
zn −ˆzn|n−1

(5.19)
Deﬁne the inverse of the block matrix term in (5.19) as

C11
C12
C21
C22

≜

Pxx
n|n−1
Pxz
n|n−1
Pzx
n|n−1
Pzz
n|n−1
−1
(5.20)
and using the block matrix inverse presented in Section 2.1.4 [7], we obtain
C11 =

Pxx
n|n−1 −Pxz
n|n−1

Pzz
n|n−1
−1 Pzx
n|n−1
−1
(5.21)
C12 = −C11Pxz
n|n−1

Pzz
n|n−1
−1
(5.22)
C21 = −C22Pzx
n|n−1

Pxx
n|n−1
−1
(5.23)
C22 = Pzz
n|n−1 −Pzx
n|n−1

Pxx
n|n−1
−1 Pxz
n|n−1
(5.24)
Now, using (5.20)–(5.24) in (5.19), expanding and rearranging terms, we obtain
g = x⊺
nC11xn + x⊺
n

−C11ˆxn|n−1 + C12

zn −ˆzn|n−1

+ · · ·
(5.25)
Comparing the ﬁrst term in (5.17) with the ﬁrst term in (5.25), it follows that
Pxx
n|n = Pxx
n|n−1 −Pxz
n|n−1

Pzz
n|n−1
−1 Pzx
n|n−1
(5.26)
Comparing the second term in (5.17) with the second term in (5.25) results in

Pxx
n|n
−1
n|n ˆxn|n = C11ˆxn|n−1 −C12

zn −ˆzn|n−1

=

Pxx
n|n−1 −Pxz
n|n−1

Pzz
n|n−1
−1 Pzx
n|n−1
−1
n|n−1 ˆxn|n−1
+

Pxx
n|n−1 −Pxz
n|n−1

Pzz
n|n−1
−1 Pzx
n|n−1
−1
×Pxz
n|n−1

Pzz
n|n−1
−1 
zn −ˆzn|n−1

(5.27)
Solving for ˆxn|n and using (5.26), this becomes
ˆxn|n = ˆxn|n−1 + Pxz
n|n−1

Pzz
n|n−1
−1 
zn −ˆzn|n−1

(5.28)
Let the Kalman gain be deﬁned as
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
(5.29)

78
THE GAUSSIAN NOISE CASE
Now, (5.28) and (5.26) become the Kalman ﬁlter correction equations
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

(5.30)
Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
(5.31)
where zn has been replaced by an actual observation zo
n and we have used the fact
that Pxz
n|n−1

Pzz
n|n−1
−1
Pzx
n|n−1 = KnPzz
n|n−1K⊺
n. Note that in the derivation of these
Kalman ﬁlter correction equations, no assumptions have been made regarding the
linearity of either the dynamic or observation equations (5.1) and (5.2), respectively.
Thus, the Kalman ﬁlter correction equations have been derived directly from the
Gaussian distributions, without the need for any assumptions or conditions. However,
it is useful to remember that these correction equations are more general and can be
applied to the ﬁrst two moments of ANY distribution, as long as the appropriate
moments exist. It must be noted that, to date, there have been no journal articles using
these correction equations for non-Gaussian densities (to our knowledge).
5.3
THE GENERAL BAYESIAN POINT PREDICTION INTEGRALS FOR
GAUSSIAN DENSITIES
Remembering the dynamic equation (5.1), and assuming that vn is a zero-mean Gaus-
sian random variable independent of both xn ∈Rnx and zn ∈Rnz, it follows that
vn−1 ∼p (xn|xn−1) = N

xn −

fn−1 (xn−1) + un

; 0, Q

= N (vn−1; 0, Q)
(5.32)
Similarly, in the observation equation (5.2), assume that wn is a zero-mean Gaus-
sian random variables independent of both xn and zn, that is
wn ∼p (zn|xn) = N (zn −hn (xn) ; 0, R) = N (wn; 0, R)
(5.33)
Now, from Bayes’ law, the prior density can be obtained from
p

xn|z1:n−1

∝p (zn|xn) p (xn|xn−1)
(5.34)
making p

xn|z1:n−1

Gaussian, since the product of two Gaussian densities is also
Gaussian. Assuming that all density functions are Gaussian, we can identify
p (xn|z1:n−1) = N

xn; ˆxn|n−1, Pxx
n|n−1

(5.35)
p (xn−1|z1:n−1) = N

xn−1; ˆxn−1|n−1, Pxx
n−1|n−1

(5.36)
Assuming that vn and wn are zero-mean Gaussian processes, and substituting
(5.36) into (5.3) and (5.4) and (5.35) into (5.5)–(5.7), the prediction estimation process

THE GENERAL BAYESIAN POINT PREDICTION INTEGRALS
79
can be summarized by the Gaussian-weighted integrals
ˆxn|n−1 =

fn−1 (xn−1)
×N

xn−1; ˆxn−1|n−1, Pxx
n−1|n−1

dxn−1 + un
(5.37)
Pxx
n|n−1 =
 
fn−1 (xn−1) + un −ˆxn|n−1

×

fn−1 (xn−1) + un −ˆxn|n−1
⊺
×N

xn−1; ˆxn−1|n−1, Pxx
n−1|n−1

dxn−1 + Q
(5.38)
ˆzn|n−1 =

hn (xn) N

xn; ˆxn|n−1, Pxx
n|n−1

dxn
(5.39)
Pzz
n|n−1 =
 
hn (xn) −ˆzn|n−1
 
hn (xn) −ˆzn|n−1
⊺
×N

xn;xn|n−1, Pxx
n|n−1

dxn + R
(5.40)
Pxz
n|n−1 =
 
xn −ˆxn|n−1
 
hn (xn) −ˆzn|n−1
⊺
×N

xn;xn|n−1, Pxx
n|n−1

dxn
(5.41)
with
Q ≜

Rnx
vn−1v⊺
n−1N (vn−1; 0, Q) dvn−1
(5.42)
and
R ≜

Rnx
wnw⊺
nN (wn; 0, R) dwn
(5.43)
A block diagram of the general Bayesian estimation process ﬂow with Gaussian
probability densities is shown in Figure 5.1. The process contains the following steps:
1. Initialize the tracking ﬁlter by generating best guess values for ˆx0 and Pxx
0 .
These can usually be based on some initial observations.
2. Evaluate the state vector prediction integrals in some manner.
3. Evaluate the observation prediction integrals in some manner.
4. Perform the Kalman ﬁlter correction equations to produce the posterior point
estimates ˆxn|n and Pxx
n|n.
5. Store the results in a track ﬁle.
6. Step forward in time to the next ﬁlter iteration using ˆxn−1|n−1 and Pxx
n−1|n−1 as
the new input values.
7. Return to step 2.

80
THE GAUSSIAN NOISE CASE
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
xz
zz
xx
xx
zz
P
K
P
x
x
z
K
z
P
P
P
K
K
xx
P
x
n
n
n
n
xx
P
x
n
n
n
n
xx
P
x
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
dx
dx
xx
xx
xx
f
x
u
x
x
x
P
f
P
u
x
f
x
u
x
x
x
x
P
Q
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
dx
h
h
h
dx
h
dx
xx
zz
xx
zz
xx
x
z
x
x
P
x
P
x
z
z
x
x
P
R
x
P
x
x
z
x
x
P
Kalman filter: correction
Delay
Track file
State vector prediction
Observation/measurement prediction
Initialize
FIGURE 5.1
Block diagram of the process ﬂow for the Bayesian estimator with Gaussian
probability densities.
All of the Bayesian estimation methods based on the Gaussian assumption will
follow this same basic process ﬂow, with either the functional approximations or the
method of integration changing.
5.3.1
Reﬁning the Process Through an Afﬁne Transformation
In Chapter 2 (Section 2.4.2.3) we discussed the application of a vector afﬁne trans-
formation to a general Gaussian integral of the form (2.106), repeated here for clarity
E {f (x)} =

f (x)
1
(2π)nx/2 |Pxx|1/2
× exp
	
−1
2 (x −ˆx)⊺
Pxx−1 (x −ˆx)

dx
(5.44)
=

f (x) N

x;x, Pxx
dx
(5.45)
As shown in Chapter 2, the vector afﬁne transformation
x = ˆx + Dc
(5.46)
results in

f (x) N

x; ˆx, Pxx
dx =

f (c) N (c; 0, I) dc
(5.47)

THE GENERAL BAYESIAN POINT PREDICTION INTEGRALS
81
where D is deﬁned by
Pxx = DD⊺
(5.48)
Or, we can write
D =

Pxx1/2
(5.49)
In addition, we have deﬁned f(c) as
f (c) ≜f (ˆx + Dc)
(5.50)
Applying the vector afﬁne transformation to equations (5.37) and (5.38) results in
ˆxn|n−1 =

f (c) N (c; 0, I) dc + un
(5.51)
Pxx
n|n−1 =
 
f (c) + un −ˆxn|n−1
 
f (c) + un −ˆxn|n−1
⊺
×N (c; 0, I) dc + Q
(5.52)
where, for this case
f (c) = f
ˆxn−1|n−1 + Dn−1|n−1c

(5.53)
with
Dn−1|n−1 ≜

Pxx
n−1|n−1
1/2
(5.54)
In addition, (5.39)–(5.41) become
ˆzn|n−1 =

˜h (c) N (c; 0, I) dc,
(5.55)
Pzz
n|n−1 =
 ˜h (c) −ˆzn|n−1
 ˜h (c) −ˆzn|n−1
⊺
×N (c; 0, I) dc + R
(5.56)
=
 ˜f(c) + xn −ˆxn|n−1
 ˜h (c) −ˆzn|n−1

N (c; 0, I) dc
(5.57)
with
˜h (c) = h
ˆxn|n−1 + Dn|n−1c

(5.58)

82
THE GAUSSIAN NOISE CASE
and
Dn|n−1 =

Pxx
n|n−1
1/2
(5.59)
The set of equations (5.51)–(5.52) and (5.55)–(5.57) require the solutions for three
general moment integrals of the form
I
˜f

≜

f (c) N (c; 0, I) dc
(5.60)
I
˜f˜f⊺
≜
 
f (c) −I
˜f
 
f (c) −I
˜f
⊺
N (c; 0, I) dc
(5.61)
and
I

c˜f

≜

cf (c) N (c; 0, I) dc
(5.62)
5.3.2
General Methodology for Solving Gaussian-Weighted Integrals
For a general nonlinear function f(c), the integrals (5.60)–(5.62) cannot be solved
analytically. Generally speaking, numerical methods to solve these integrals are called
multiple integration rules and each rule is designed to integrate a speciﬁc class of
multidimensional polynomial approximations of f(c). Similar approaches were taken
in Ref. [8] and the references contained therein, where the method was described as
a statistical linearization of f (x) through a series of transformations on the random
variable x and subsequent expansion of the transformed f (x) in a polynomial to
facilitate evaluation of the expectation integrals.
The application of the afﬁne transformation (5.46) is very important because it con-
verts the general Gaussian-weighted integral into one that is fully symmetric. A fully
symmetric density-weighted integral is one in which the density is symmetric about
zero in all dimensions with the equal contours forming hyperspheres. In addition, the
limits of the integral must also be symmetric about zero in all dimensions. This will be
shown to greatly simplify numerical methods for approximating Gaussian-weighted
integrals of nonlinear functions.
From the analytical expression for N (c; 0, I) , one can see that at speciﬁc vector
point c(i) in a multidimensional coordinate system, N

c(i); 0, I

will have a speciﬁc
numerical value. Now if we consider any vector point c(i) of dimension nx that lies on
an nx-dimensional hypersphere of unit radius, it is easy to show that N

c(i); 0, I

=
e−1/2/ (2π)−nx/2 . This can be generalized to the statement that for a hypersphere of
any given radius, all vector points c(i) on the surface of that hypersphere will produce
the same value for N

c(i); 0, I

. For the two-dimensional case, this can be seen in Fig-
ure 2.9 where each equal-value contours of N (c; 0, I) form circles of constant radius.
If f(c) is linear, that is, f(c) →Fc, where F is deterministic transition matrix
that does not depend on c, the solutions to (5.60)–(5.62) are straightforward and
(5.51)–(5.57) reduce to the well-known linear Kalman ﬁlter, as will be shown in
Chapter 6.

THE GENERAL BAYESIAN POINT PREDICTION INTEGRALS
83
Foran f(c)thatisnonlinear,thestandardapproachistoexpand f(c)analytically in
a multidimensional polynomial formed from product monomials in each dimension.
For example, in two dimensions, the polynomial expansion for f(c) , where c =
[c1, c2]⊺, is of the form
f (c1, c2) =
m1

i1=0
m2

i2=0
a (i1, i2) ci1
1 ci2
2
= a (0, 0) + a (1, 0) c1 + a (0, 1) c2 + a (1, 1) c1c2 + a (2, 0) c2
1
+ · · · + a (m1, m2) cm1
1 cm2
2
(5.63)
Here, each term in the polynomial is a monomial of the form a (i1, i2) ci1
1 ci2
2 . For a
multidimensional polynomial of degree d, each term contains a monomial d
j=1 cij
j
with ij ≥0 and d
j=1 ij ≤d. For the two-dimensional case cited above, the degree
of the polynomial is d = m1 + m2 ≥d
j=1 ij.
For nonlinear functions ˜f (c) we will be using polynomial expansions of ˜f (c) in
integrals of the form (5.60)–(5.62) to generate general multiple integration rules.
We now introduce the deﬁnition of the precision p of an integration rule [1,9].
For integrals of the form (5.60) and (5.62), “a rule is said to have precision p if
it integrates monomials up to degree p exactly, that is, monomials p
j=1 cij
j with
ij ≥0 and p
j=1 ij ≤p, but not exactly for some monomials of degree p
j=1 ij ≤
p + 1” [1].
The classical approach for polynomial expansion is to expand f(c) in a Taylor
polynomial. In Chapter 7, it will be shown that keeping only the linear terms in the
Taylor polynomial leads to the EKF. However, one of the main drawbacks of the EKF
is the requirement to evaluate Jacobian matrices. For highly nonlinear functions, using
a Taylor polynomial that includes second-order terms yields a more accurate solution,
but it is one that adds the additional requirement to evaluate Hessian matrices. This
leads to a solution formulation that results in a second-order EKF that is also addressed
in Chapter 7.
An alternative is to replace the Jacobian and Hessian differential matrices by their
multidimensional central ﬁnite difference approximations. This leads to the FDKF,
the subject of Chapter 8. The FDKF is the ﬁrst example of a sigma point Kalman ﬁlter.
A sigma point Kalman ﬁlter is one that requires evaluation of f(c) at deterministic
sigma points that form a multidimensional grid. In addition, for sigma point Kalman
ﬁlters all solutions to equations of the form (5.60)–(5.62) require evaluation of a set
of moment equations, as will be shown in the chapters following. In the literature on
methods for numerical integration, this conceptual methodology has been generalized
to approximations for (5.60) of the form [10]
I
˜f

=
ns

j=1
wjf

c(j)
with f (c) ∈Rnx
(5.64)

84
THE GAUSSIAN NOISE CASE
where the weights wj, the sigma points c(j), and the number of points ns, are deter-
mined by the integration method chosen. The choices for wj and c(j) are independent
of the function f. If the dimension of the integration region is one, that is, nx = 1, the
approximation is called a quadrature formula. If nx ≥2, the approximation is called
a cubature formula. There are two classes of cubature formulas: number theoretic
methods and polynomial-based methods. The ﬁrst class requires that the integration
(sigma) points be distributed uniformly on a multidimensional grid. Methods of the
second class are designed to be exact for some set of orthogonal polynomials. The
FDKF is an example of the ﬁrst class.
In what follows, two sets of vector points will be used that lie on the same hyper-
sphere. For the ﬁrst set, consider a vector point c(i) = q [1, 0, . . . , 0]⊺= qr(i), where
q is the radius of a hypersphere and r(i) is a unit vector along one of the dimensional
axes. Additional points that lie on the hypersphere along different coordinate axes can
be generated from this ﬁrst point by a change of sign and/or a permutation of dimen-
sion. For example, the points c(j) = q [−1, 0, . . . , 0]⊺or c(j) = q [0, . . . , 1, . . . , 0]⊺
would also lie on the same hypersphere as the original point. In fact, it follows imme-
diately that there will be two points along each axis, one positive and one negative,
such that there are a total of 2nx such vector points. We can deﬁne this set of vector
points as qr, with r being the set of positive and negative unit vector along the nx
Cartesian axes. This approach leads to the UKF that will be addressed in Chapter 9.
The symmetry can also be expressed in sets of vector points that lie equally spaced
in angle (instead of along the axes) on a hypersphere. These vector points will produce
a symmetric set radiating from the origin to the vertex points of a multidimensional
simplex, all of which will lie on the same hypersphere. Once again, if one point and
the rotation angle between points is speciﬁed, all points of this set can be generated
from the original point. We will show that this set contains only nx + 2 points. Such
an approach is taken in Chapter 10 where the SSKF will be developed.
There are, in fact, many such sets of points that can act as generators. For example,
on a square grid (not on a hypersphere), the two vector points c(i) = q [1, 1, 0, . . . , 0]⊺
and c(i) = q [1, 1, 1, . . . , 1]⊺will each act as a generator for a different set of sym-
metric points. But for these cases, not all values of N

c(i); 0, I

will be identical.
More will be discussed about these generator vector points in Chapter 11 where the
orthogonal Hermite polynomials are used to develop the GHKF.
The restrictions imposed by the degree of the polynomial used to approximate
f(c) lead to a requirement to satisfy a set of moment equations of the form
ns

j=1
wjfi

c(j)
=

fi (c) N (c; 0, I) dc
(5.65)
where f i (c) represents all terms in the polynomial expansion of f(c) that are of order
i. This set of moment equations lead to a system of nonlinear equations that can be
solved for the unknowns

wj, c(j)
. For matrix equations, as in (5.61). This becomes
ns

j=1
wjfi

c(j)
fl

c(j)
=

fi (c)fl (c) N (c; 0, I) dc
(5.66)

REFERENCES
85
For a more detailed understanding of these numerical methods, see books by Davis
and Rabinowitz [4] and Evans and Swartz [5] and any of the other referenced material.
REFERENCES
1. Wu Y, Hu D, Wu M, Hu X. A numerical-integration perspective on Gaussian ﬁlters. IEEE
Trans. Sig. Proc. 2006;54(8):2910–2921.
2. McNamee J, Stenger F. Construction of fully symmetric numerical integration formulas.
Numerische Math. 1967;10:327–344.
3. Lerner UN. Hybrid Bayesian Networks for Reasoning About Complex Systems, Disser-
tation. Department of Computer Science, Stanford University; October 2002.
4. Davis PJ, Rabinowitz P. Methods of Numerical Integration, Reprint of Second Edition.
Minneola, NY: Dover Publications; 1984.
5. Evans M, Swartz T. Approximating Integrals via Monte Carlo and Deterministic Methods.
Oxford Statistical Science Series: 20, Oxford University Press; 2000, Reprinted 2005.
6. Ito K, Xiong K. Gaussian ﬁlters for nonlinear ﬁltering problems. IEEE Trans. Automatic
Control. 2000;45(5):910–927.
7. Marple SL. Digital Spectral Analysis with Applications. Prentice-Hall; 1987.
8. Gelb A. Applied Optimal Estimation. The MIT Press; 1974.
9. Stroud AK. Approximate Calculation of Multiple Integrals. Englewood cliffs, NJ: Prentice
Hall; 1971.
10. Cool R. Advances in multidimensional integration. J. Comput. and Appl. Math. 2002;
149:1–12.

6
THE LINEAR CLASS OF
KALMAN FILTERS
The recursive linear Kalman ﬁlter (LKF) was ﬁrst introduced by Kalman [1,2], with
the books [3–5] offering a more in depth discussion of the method with application
examples. Newer editions or reprints of the original books are still available. For
many estimation problems, either the dynamic or observation equations are linear
and in many situations both are linear. Therefore, we will treat the linear dynamic or
observation equation cases separately and then combine them into the linear Kalman
ﬁlter.
6.1
LINEAR DYNAMIC MODELS
For a linear dynamic model, (5.1) can be written as
xn = Fxn−1 + un + vn−1
(6.1)
where F is a deterministic transition matrix that is independent of time. For most
cases, the control factor un is not needed but we will include it in what follows for
the sake of completion.
Putting (6.1) into (5.37) we obtain
xn|n−1 = F

xn−1N

xn−1; ˆxn−1|n−1, Pxx
n−1|n−1

dxn−1 + un
(6.2)
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
86

LINEAR OBSERVATION MODELS
87
which reduces to
ˆxn|n−1 = Fˆxn−1|n−1 + un
(6.3)
Similarly, putting (6.1) into (5.38), yields
Pxx
n|n−1 =

F

xn−1 −ˆxn|n−1
 
xn−1 −ˆxn|n−1
⊺F⊺
×N

xn−1; ˆxn−1|n−1, Pxx
n−1|n−1

dxn−1 + Q
= F
 
xn−1 −ˆxn|n−1
 
xn−1 −ˆxn|n−1
⊺
×N

xn−1; ˆxn−1|n−1, Pxx
n−1|n−1

dxn−1

F⊺+ Q
(6.4)
But, since
Pxx
n−1|n−1 =
 
xn−1 −ˆxn|n−1
 
xn−1 −ˆxn|n−1
⊺
×N

xn−1; ˆxn−1|n−1, Pxx
n−1|n−1

dxn−1
(6.5)
for a linear dynamic model it follows immediately that
Pxx
n|n−1 = FPxx
n−1|n−1F⊺+ Q
(6.6)
where Q = E

vn−1v⊺
n−1

is the dynamic noise covariance matrix.
6.2
LINEAR OBSERVATION MODELS
If the observation equation is linear, assume that the observation equation can be
written as
zn = Hxn + wn
(6.7)
where H is an nz × nx deterministic matrix that is independent of xn and of time.
Therefore, we can write
h (xn) = Hxn
(6.8)
Assuming that the observation noise wn is a zero mean process, it follows
immediately that (5.39) and (5.40) reduce to
ˆzn|n−1 = Hˆxn|n−1
(6.9)
and
Pzz
n|n−1 = HPxx
n|n−1H⊺+ R
(6.10)
where R = E

wnw⊺
n

is the observation noise covariance matrix.

88
THE LINEAR CLASS OF KALMAN FILTERS
Finally, using (6.7) and (6.9) in (5.57) we obtain
Pxz
n|n−1 =
 
xn −ˆxn|n−1
 
zn −ˆzn|n−1
⊺N

xn;xn|n−1, Pxx
n|n−1

dxn
=
 
xn −ˆxn|n−1
 
Hxn + wn −Hˆxn|n−1
⊺
×N

xn;xn|n−1, Pxx
n|n−1

dxn
=
 
xn −ˆxn|n−1
 
xn −ˆxn|n−1
⊺
N

xn;xn|n−1, Pxx
n|n−1

dxn

H⊺
+
 
xn −ˆxn|n−1

w⊺
nN

xn;xn|n−1, Pxx
n|n−1

dxnH⊺
(6.11)
This reduces to
Pxz
n|n−1 = Pxx
n|n−1H⊺
(6.12)
6.3
THE LINEAR KALMAN FILTER
When both the dynamic and observation processes are linear, Bayesian estimation
reduces to the LKF. Note that it can be shown that the prediction equations can be
derived directly from (5.3) to (5.7) using general distributions for the posterior and
prior densities, making the linear Kalman ﬁlter applicable for any density. Only the
requirements of linearity of the dynamic and observation equations need to be applied.
However, this requires the density function to be of known analytical form so that the
moment integrals can be evaluated in some manner. In many cases this has proven to
be an insurmountable requirement.
The complete LKF process is presented in Table 6.1.
6.4
APPLICATION OF THE LKF TO DIFAR BUOY BEARING
ESTIMATION
The LKF can be applied to a single DIFAR buoy using its bearing observations to
estimate a state vector that includes bearing and bearing rate. That is, let the state
vector be deﬁned as
xn ≜

θn, ˙θn
⊺
(6.13)
where the bearing θn is the angle between the y-axis, which points to true North,
and the line drawn from an origin at the buoy to the target ship, with the convention

APPLICATION OF THE LKF TO DIFAR BUOY BEARING ESTIMATION
89
TABLE 6.1
Linear Kalman Filter Process
Step 1.
Filter initialization:
Initialize x0 and Pxx
0
Step 2.
State vector prediction:
ˆxn|n−1 = Fˆxn−1|n−1 + un,
Pxx
n|n−1 = FPxx
n−1|n−1F⊺+ Q
Step 3.
Observation-related prediction:
ˆzn|n−1 = Hˆxn|n−1,
Pzz
n|n−1 = HPxx
n|n−1H⊺+ R
Pxz
n|n−1 = Pxx
n|n−1H⊺
Step 4.
Kalman ﬁlter update:
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.
that −180 deg < θ ≤180 deg. ˙θn is the bearing rate of change. For this simple
problem, the control variable un is not needed so the dynamic transition equation is
given by
xn = Fxn−1 + vn−1
(6.14)
with
F =
	
1
T
0
1

(6.15)
and vn−1 a zero-mean Gaussian random dynamic acceleration noise process deﬁned
by vn−1 ∼N (0, Q), where
Q = q
	 T 3
3
T 2
2
T 2
2
T

(6.16)
The Q used here is the dynamic noise covariance of a continuous noise process with q,
the variance of the bearing acceleration noise, set at 0.1 for this example. A complete
derivation of Q for the continuous noise model is beyond the scope of this book,
but the interested reader can ﬁnd it in Ref. [6]. The units of q can be obtained by

90
THE LINEAR CLASS OF KALMAN FILTERS
examining the state covariance prediction equation (6.6), noting that
	
σ2
θ
σ2
θ˙θ
σ2
θ˙θ
σ2
˙θ

n|n−1
= Pxx
n|n−1 = FPxx
n−1|n−1F⊺+ Q
= F
	
σ2
θ
σ2
θ˙θ
σ2
θ˙θ
σ2
˙θ

n−1|n−1
F
⊺
+q
	 T 3
3
T 2
2
T 2
2
T

(6.17)
Now, the units of q can be determined by noting that qT must have the same units as
σ2
˙θ, or units of speed2. Thus, q has units of speed2/time or acceleration2× time. Also,
we observe that q is a standard deviation about a zero mean, so if we want the dynamic
noise to represent a deviation from the constant velocity path, the standard deviation
will take into account both positive and negative deviations and will therefore be
twice the allowed deviation. So for our case, we are allowing a deviation in speed of
√
0.1/2 = 0.16 rad/s.
For a single DIFAR buoy, the output is a noisy bearing observation at each time
tn. The linear observation equation can therefore be written as
zn = θn = Hxn + wn
(6.18)
with wn ∼N

0, σ2
θ

and
H =

1
0

(6.19)
Here, σθ is the standard deviation of the bearing measurements and is taken as a
characteristic of all DIFAR buoys that is constant and independent of time.
The LKF can be initialized by using
ˆx0 =
	 ˜θ0
˙θ0

where ˜θ0 is the ﬁrst bearing observation from the DIFAR and arbitrarily setting ˙θ0 =
−0.09 deg/s. We also let the initial state covariance matrix be given by
Pxx
0 =
	
σ2
θ
0
0
σ2
˙θ

(6.20)
where we set σ˙θ = −0.09 deg/s and σθ is set to the observation uncertainty, in degrees.
Note that in our implementation of the LKF, all bearing and bearing rates are converted
to radians and radians per second, respectively, but that transformation is not really
needed for this linear example. The signal-to-noise ratio at the input to the DIFAR
buoy was set to 20 dB with a time-bandwidth product of 90. This will ensure that the
bearing observations are Gaussian.

APPLICATION OF THE LKF TO DIFAR BUOY BEARING ESTIMATION
91
n
n
n
n
n
n
n
n
n
n
n
n
xx
xz
xx
Hx
HP
H
P
P
H
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
xz
xx
xx
P
K
x
x
K
P
P
K
K
xx
P
x
n
n
n
n
xx
P
x
n
n
n
n
n
n
n
n
xx
xx
F
x
x
FP
P
F
Q
n
n
n
n
xx
P
x
n
Kalman filter: correction
Observation/measurement prediction
Delay
Track file
Initialize
State vector prediction
FIGURE 6.1
Block diagram of the LKF process for a single DIFAR buoy.
A block diagram of the LKF process for bearing tracking for a single DIFAR buoy
is presented in Figure 6.1.
Note that in the Kalman ﬁlter correction step, we calculate the innovations (or
sometimes called the measurement residual)
ηn = ˜θn −ˆθn|n−1
(6.21)
Occasionally, the observation and prediction fall on opposite sides of the discontinuity
at −180/180. This is referred to in the literature as the phase wrap problem and must
−150
−100
−50
0
50
100
150
100
200
300
400
500
600
700
800
Bearing (deg)
Time (s)
FIGURE 6.2
Comparison of the LKF bearing estimates with the true bearing for one DIFAR
buoy.

92
THE LINEAR CLASS OF KALMAN FILTERS
be dealt with or the tracking ﬁlter will fail. The simplest way to deal with this is to
calculate ηn and if it is greater that 180 deg, subtract 360 deg from ˆθn|n−1 or if ηn is
less that −180 deg, add 360 deg to ˆθn|n−1. If neither of these conditions is true, do
nothing.
A plot of the bearing estimated in this manner for one of the DIFAR buoys that
exhibits this phase wrap problem is shown in Figure 6.2. In this ﬁgure, the estimated
bearing outputs from the ﬁlter are represented by black dots. The superimposed gray
line is the true bearings from the buoy to the target ship. This form of a plot is
called a bearing-time record (BTR). It is obvious from the ﬁgure that the bearings are
estimated quite well using the LKF for the chosen ﬁlter update rate (1 s) and bearing
acceleration noise (0.1).
REFERENCES
1. Kalman RE. A new approach to linear ﬁltering and prediction problems. ASME 1960;82
(Series D):35-40.
2. Kalman RE, Bucy RS. New results in linear ﬁltering and prediction theory. ASME 1961;83
(Series D):95–107.
3. Papoulis A. Probability, Random Variables, and Stochastic Processes. McGRaw-Hill; 1965.
4. Jazwinski AH. Stochastic Processes and Filtering Theory. Academic Press; 1970.
5. Gelb A. Applied Optimal Estimation. The MIT Press; 1974.
6. Bar Shalom Y, Li RX, Kirubarajan T. Estimation with Application to Tracking and
Navigation: Theory, Algorithms and Software. Wiley; 2001.

7
THE ANALYTICAL LINEARIZATION
CLASS OF KALMAN FILTERS: THE
EXTENDED KALMAN FILTER
For mildly nonlinear and smooth (differentiable) functions, analytical methods can
be used to linearize the nonlinear equations, making the linear Kalman ﬁlter structure
available for use with nonlinear dynamic and/or observation equations. To accomplish
this linearization, the nonlinear function ˜f (c) in (5.51)–(5.57) can be expanded in
a multidimensional Taylor polynomial about the mean value ˆc = 0 leading to the
extended Kalman ﬁlter [1,2]. To make understanding of the method easier, and for
future reference, we will develop the scalar extended Kalman ﬁlter (EKF) ﬁrst and
then move to the multidimensional case.
7.1
ONE-DIMENSIONAL CONSIDERATION
First, for the one-dimensional case, the state prediction equations (5.51) and (5.52)
become
ˆxn|n−1 =

f (c) N (c; 0, 1) dc + un
(7.1)
σ2
xx,n|n−1 =
 
f (c) + un −ˆxn|n−1
2
N (c; 0, 1) dc + σ2
v,n
(7.2)
with
f (c) = f

ˆxn−1|n−1 + σxx,n−1|n−1c

(7.3)
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
93

94
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
From (5.55) to (5.57), for the one-dimensional case, the observation prediction
equations become
ˆzn|n−1 =

˜h (c) N (c; 0, 1) dc
(7.4)
σ2
zz,n|n−1 =

˜h2 (c) N (c; 0, 1) dc −ˆz2
n|n−1 + σ2
w,n
(7.5)
σ2
xz,n|n−1 = σxx,n|n−1

c˜h (c) N (c; 0, 1) dc
(7.6)
with
˜h (c) = h

ˆxn|n−1 + σxx,n|n−1c

(7.7)
7.1.1
One-Dimensional State Prediction
A general one-dimensional Mth-order Taylor polynomial was presented in equation
(2.38). For a third-order one-dimensional Taylor polynomial for f (c) expanded about
the zero-mean value of ˆc, (2.38) becomes
f (c) = f (0) +
 d
dc
f (c)

c=0
c + 1
2
 d2
dc2 f (c)

c=0
c2
+1
6
 d3
dc3 f (c)

c=0
c3
(7.8)
Inserting this into (7.1) yields
ˆxn|n−1 = f (0)

N (c; 0, 1) dc
+
 d
dc
f (c)

c=0

cN (c; 0, 1) dc
+1
2
 d2
dc2 f (c)

c=0

c2N (c; 0, 1) dc
+1
6
 d3
dc3 f (c)

c=0

c3N (c; 0, 1) dc
+un
(7.9)
Evaluating the integrals using the one-dimensional moment equations (2.115)
yields the one-dimensional prediction estimate of the state vector
ˆxn|n−1 = f (0) + 1
2
 d2
dc2 f (c)

c=0
+ un
(7.10)
Note that this estimate is accurate to a third-order polynomial in c, because the third-
order term in (7.8) integrates to zero.

ONE-DIMENSIONAL CONSIDERATION
95
7.1.2
One-Dimensional State Estimation Error Variance Prediction
From (7.2), consider only the ﬁrst term
I (c) =
 
f (c) + un −ˆxn|n−1
2
N (c; 0, 1) dc
(7.11)
But from (7.1) we can write the progression
ˆxn|n−1 =

f (c) N (c; 0, 1) dc + un
=

f (c) N (c; 0, 1) dc + f (0) −f (0) + un
= f (0) +
 
f (c) −f (0)

N (c; 0, 1) dc + un
(7.12)
Thus (7.11) becomes
I (c) =
 	
f (c) −f (0)

−
 
f (τ) −f (0)

N (τ; 0, 1) dτ

2
N (c; 0, 1) dc
=
 
f (c) −f (0)
2
N (c; 0, 1) dc
−
	 
f (c) −f (0)

N (c; 0, 1) dc

2
(7.13)
From the Taylor polynomial (7.8), keeping terms to second order, we can write

f (c) −f (0)

=
 d
dc
f (c)

c=0
c + 1
2
 d2
dc2 f (c)

c=0
c2
(7.14)
and

f (c) −f (0)
2
=
 d
dc
f (c)
2
c=0
c2 +
 d
dc
f (c)

c=0
 d2
dc2 f (c)

c=0
c3
+1
4
 d2
dc2 f (c)
2
c=0
c4
(7.15)
Now, using (7.14) and (7.15) in (7.13) and evaluating the resultant integrals using the
one-dimensional moment equations (2.115) yields
I (c) =
 d
dc
f (c)
2
c=0
+ 1
2
 d2
dc2 f (c)
2
c=0
(7.16)

96
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
Thus, the state error variance becomes
σ2
xx,n|n−1 =
 d
dc
f (c)
2
c=0
+ 1
2
 d2
dc2 f (c)
2
c=0
+ σ2
v,n
(7.17)
From (7.15), we can be see that this approximation is valid up to ﬁfth order in c.
7.1.3
One-Dimensional Observation Prediction Equations
Derivation of ˆzn|n−1 and σ2
zz,n|n−1 follow the same procedure as that of the last two
sections, resulting in
ˆzn|n−1 = ˜h (0) + 1
2
 d2
dc2 ˜h (c)

c=0
(7.18)
σ2
zz,n|n−1 =
 d
dc
˜h (c)
2
c=0
+ 1
2
 d2
dc2 ˜h (c)
2
c=0
+ σ2
w,n
(7.19)
For the cross-correlation, we have
σ2
xz,n|n−1 = σxx,n|n−1

c˜h (c) N (c; 0, 1) dc
= σxx,n|n−1

c
	
˜h (0) +
 d
dc
˜h (c)

c=0
c
+1
2
 d2
dc2 ˜h (c)

c=0
c2

N (c; 0, 1) dc
= σxx,n|n−1
 d
dc
˜h (c)

c=0
(7.20)
This latter estimate of the cross-correlation can be seen to be of third order in c.
7.1.4
Transformation of One-Dimensional Prediction Equations
Remembering the deﬁnition of the afﬁne transformation (5.46), and using the chain
rule for differentiation, we can write
d
dc = dxn−1
dc
d
dxn−1
= σxx,n−1|n−1
d
dxn−1
(7.21)
d2
dc2 =
dxn−1
dc
2
d2
dx2
n−1
= σ2
xx,n−1|n−1
d2
dx2
n−1
(7.22)

ONE-DIMENSIONAL CONSIDERATION
97
Also from (7.3), it follow that f (0) = f

xn−1|n−1

and from (5.46) c = 0 →xn−1 =
xn−1|n−1. Now (7.10) becomes
ˆxn|n−1 = f

xn−1|n−1

+ 1
2σ2
xx,n−1|n−1

d2
dx2
n−1
f (xn−1)

xn−1=xn−1|n−1
+ un
(7.23)
In a similar manner, (7.17) becomes
σ2
xx,n|n−1 = σ2
xx,n−1|n−1

d
dxn−1
f (xn−1)
2
xn−1=ˆxn−1|n−1
+1
2σ4
xx,n−1|n−1

d2
dx2
n−1
f (xn−1)
2
xn−1=ˆxn−1|n−1
+σ2
v,n
(7.24)
For the observation prediction equations, instead of (7.21) and (7.22), we use the
afﬁne transformation (5.46), and the chain rule becomes
d
dc = dxn
dc
d
dxn
= σxx,n|n−1
d
dxn
(7.25)
and
d2
dc2 = σ2
xx,n|n−1
d2
dx2n
(7.26)
Now, (7.18)–(7.20) transform into
ˆzn|n−1 = h

xn|n−1

+ 1
2σ2
xx,n|n−1
 d2
dx2n
h (xn−1)

xn=xn|n−1
(7.27)
σ2
zz,n|n−1 = σ2
xx,n|n−1
 d
dxn
h (xn)
2
xn=ˆxn|n−1
+1
2σ4
xx,n|n−1
 d2
dx2n
f (xn)
2
xn=ˆxn|n−1
+σ2
w,n
(7.28)
and
σ2
xz,n|n−1 = σ2
xx,n|n−1
 d
dxn
h (xn)

xn=ˆxn|n−1
(7.29)

98
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
TABLE 7.1
One-Dimensional Extended Kalman Filter Process
Step 1.
Filter initialization:
Initialize ˆx0 and σ2
xx,0,
Step 2.
State vector
ˆxn|n−1 = f xn−1|n−1

+ un
prediction:
σ2
xx,n|n−1 = σ2
xx,n−1|n−1
×

d
dxn−1 f (xn−1)
2
xn−1=ˆxn−1|n−1
+σ2
v,n
Step 3.
Observation-related
ˆzn|n−1 = hxn|n−1

prediction:
σ2
zz,n|n−1 = σ2
xx,n|n−1
 d
dxn h (xn)2
xn=ˆxn|n−1 + σ2
w,n
σ2
xz,n|n−1 = σ2
xx,n|n−1
 d
dxn h (xn)
xn=ˆxn|n−1
Step 4.
Kalman
Kn = σ2
xz,n|n−1/σ2
zz,n|n−1
ﬁlter
update:
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

σ2
xx,n|n = σ2
xx,n|n−1 −K2
nσ2
zz,n|n−1
Step 5.
Store results
Store ˆxn|n and σ2
xx,n|n to a Track File
Step 6.
Return to Step 2.
7.1.5
The One-Dimensional Linearized EKF Process
The EKF developed above contained terms that are of higher order than linear. The
standard linear EKF can be obtained by truncating so that only the linear terms are
maintained. This leads to an EKF that has the same form as the Linear Kalman ﬁl-
ter developed in Chapter 6. Thus, in summary, the process for the one-dimensional
linearized EKF is shown in Table 7.1. Note that for this one-dimensional case
ρxz,n|n−1 = ρzx,n|n−1. In the update step, zo
n is the observation at time tn.
7.2
MULTIDIMENSIONAL CONSIDERATION
For multidimensional functions with multidimensional arguments, the state prediction
integrals for f(c) are given by (5.51)–(5.57). In both the linear Kalman ﬁlter and
the one-dimensional EKF it is obvious from (6.3) and (7.23) that the deterministic
control function un is additive for the state prediction and does not appear in any of
the remaining ﬁlter equations. So, for what follows, we will ignore un and add it back
in if it is needed for the case studies.
The general multidimensional Taylor polynomial was presented in (2.63). For a
second-order Taylor polynomial for f(c) expanded about the zero-mean value of c,

MULTIDIMENSIONAL CONSIDERATION
99
using (2.69) in (2.63) results in
f (c) =
2

i=0
1
i!Di
cf (c)
= f (0) +
nx

i=1
ci
 ∂
∂ci
f (c)

c=0
+1
2
2

i1=0
· · ·
2

inx=0
i1+···+inx=2

2
i1 · · · inx

ci1
1 . . . cinx
nx
×

∂2
∂ci1
1 · · · ∂cinx
nx
f (c)

c=0
(7.30)
It follows immediately that (7.30) can be rewritten as
f (c) = f (0) +
 nx

i=1
∂
∂ci
f (c)

c=0
ci + 1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0
c2
i
+


nx

i=1
nx

j=1
j /= i
∂2
∂ci∂cj
f (c)


c=0
cicj
(7.31)
7.2.1
The State Prediction Equation
From (5.51), using the Taylor polynomial (7.31), we can express the predictive ﬁrst
moment as
ˆxn|n−1 = ˜f (0)

N (c; 0, I) dc +
 nx

i=1
∂
∂ci
f (c)

c=0

ciN (c; 0, I) dc
+1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0

c2
i N (c; 0, I) dc
+


nx

i=1
nx

j=1
j /= i
∂2
∂ci∂cj
f (c)


c=0

cicjN (c; 0, I) dc
(7.32)

100
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
Using the moment equations (2.116)–(2.121) this reduces to
ˆxn|n−1 = f (0) + 1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0
(7.33)
If the third-order terms had been included in (7.31), they would have integrated to
zero in (7.32), so (7.33) is valid to third order.
7.2.2
The State Covariance Prediction Equation
From (5.52), the covariance prediction equation is
Pxx
n|n−1 =
 ˜f (c) −ˆxn|n−1
 ˜f (c) −ˆxn|n−1
⊺N (c; 0, I) dc + Q
(7.34)
Noting that we can add and subtract ˜f (0) , (5.51) can be rewritten as
ˆxn|n−1 = ˜f (0) +
 
f (c) −f (0)

N (c; 0, I) dc
(7.35)
Substituting (7.35) for xn|n−1, (7.34) becomes
Pxx
n|n−1 =
 
f (c) −f (0)
 
f (c) −f (0)
⊺
N (c; 0, I) dc
−
	 
f (c) −f (0)

N (c; 0, I) dc

×
	 
f (c) −f (0)
⊺
N (c; 0, I) dc

+Q
(7.36)
Rewriting the second-order Taylor polynomial (7.31) leads to
f (c) −f (0) =
 nx

i=1
∂
∂ci
f (c)

c=0
ci + 1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0
c2
i
+


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
f (c)


c=0
cicj
(7.37)

MULTIDIMENSIONAL CONSIDERATION
101
Utilizing the moment equations (2.116)–(2.121) the second integral in (7.36) yields
 
f (c) −f (0)

N (c; 0, I) dc =1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0
(7.38)
Similarly, from the second-order Taylor polynomial (7.31), we can write

f (c) −f (0)
 
f (c) −f (0)
⊺
=
 nx

i=1
∂
∂ci
f (c)

c=0
ci + 1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0
c2
i
+


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
f (c)


c=0
cicj









×
 nx

i=1
∂
∂ci
f⊺(c)

c=0
ci + 1
2
 nx

i=1
∂2
∂c2
i
f⊺(c)

c=0
c2
i
+


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
f⊺(c)


c=0
cicj









(7.39)
Carrying out the outer product term by term, putting the results into the ﬁrst integral
in (7.36), and using the moment equations (2.116)–(2.121), results in
 
f (c) −f (0)
 
f (c) −f (0)
⊺
N (c; 0, I) dc
=
 nx

i=1
∂
∂ci
f (c)

c=0
 nx

i=1
∂
∂ci
f⊺(c)

c=0
+3
4
 nc

i=1
∂2
∂c2
i
f (c)

c=0
 nc

i=1
∂2
∂c2
i
f⊺(c)

c=0
(7.40)
+


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
f (c)


c=0


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
f⊺(c)


c=0

102
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
It follows immediately that (7.36) becomes
Pxx
n|n−1 =
 nx

i=1
∂
∂ci
f (c)

c=0
 nx

i=1
∂
∂ci
f⊺(c)

c=0
+1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0
 nx

i=1
∂2
∂c2
i
f⊺(c)

c=0
+


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
f (c)


c=0


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
f⊺(c)


c=0
+Q
(7.41)
If all terms are kept, the state error covariance matrix will be valid to ﬁfth order, since
the ﬁfth-order terms will all integrate to zero.
7.2.3
Observation Prediction Equations
In a similar fashion, the prediction estimates zn|n−1 and Pzz
n|n−1 are given by
ˆzn|n−1 = ˜h (0) + 1
2
 nx

i=1
∂2
∂c2
i
˜h (c)

c=0
(7.42)
and
Pzz
n|n−1 =
 nx

i=1
∂
∂ci
˜h (c)

c=0
 nx

i=1
∂
∂ci
˜h⊺(c)

c=0
+1
2
 nx

i=1
∂2
∂c2
i
˜h (c)

c=0
 nx

i=1
∂2
∂c2
i
˜h⊺(c)

c=0
+


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
˜h (c)


c=0


nx

i=1
nx

j=1
j /=i
∂2
∂ci∂cj
˜h⊺(c)


c=0
+R
(7.43)
Finally, Pxz
n|n−1 is given by (5.57), which can be written as
Pxz
n|n−1 = Dn|n−1

c˜h⊺
n (c) N (c; 0, I) dc
(7.44)

MULTIDIMENSIONAL CONSIDERATION
103
Using (7.31) to expand ˜hn (c) in a Taylor polynomial, this becomes
Pxz
n|n−1 = Dn|n−1˜h⊺(0)

cN (c; 0, I) dc
+Dn|n−1
 nx

i=1
∂
∂ci
˜h⊺(c)

c=0

cciN (c; 0, I) dc
+Dn|n−1


nx

i=1
nx

j=1
j /=i
∂
∂ci
∂
∂cj
˜h⊺(c)


c=0
×

ccicjN (c; 0, I) dc
(7.45)
Evaluating the integrals results in
Pxz
n|n−1 = Dn|n−1
 nx

i=1
∂
∂ci
˜hi (c)

c=0
(7.46)
Note that this result is accurate to third order because the third-order term in c is zero.
7.2.4
Transformation of Multidimensional Prediction Equations
From the deﬁnition of the trace of a matrix (2.5), we can write
nx

i=1
∂2
∂c2
i
≜trace

▽c▽⊺
c

(7.47)
Using the vector chain rule,
▽c =

▽cx⊺
▽x
(7.48)
and from (2.107) it follows that
▽cx⊺= ▽c
ˆx + Dc
⊺= D⊺
(7.49)
Thus (7.47) becomes
nx

i=1
∂2
∂c2
i
= trace

D⊺▽x▽⊺
xD

(7.50)
Since the trace is invariant under circular permutations, this can be rewritten as
nx

i=1
∂2
∂c2
i
= trace

DD⊺▽x▽⊺
x

= trace
 
Pxx ˆGn−1|n−1
!
(7.51)

104
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
where ˆGn−1|n−1 is deﬁne as the Hessian matrix
ˆGn−1|n−1 ≜∇xn−1∇⊺
xn−1
=


∂2
∂x2
n−1,1
· · ·
∂
∂xn−1,1
∂
∂xn−1,nx
...
...
...
∂
∂xn−1,nx
∂
∂xn−1,1
· · ·
∂2
∂x2
n−1,nx


xn−1=ˆxn−1|n−1
(7.52)
Here, ∂/∂xn−1,i represents the partial with respect to the ith component of the vector
xn−1.
For state vector prediction, from (5.46) it follows that when c = 0, x = ˆxn−1|n−1,
and from (5.60), f(0) = f
ˆxn−1|n−1

. Now (7.33) can be written as
ˆxn|n−1 = f
ˆxn−1|n−1

+1
2 trace
 
Pxx
n−1|n−1 ˆGn−1|n−1
!
f (xn−1)
"""
xn−1=ˆxn−1|n−1
(7.53)
where trace{Pxx
n−1|n−1 ˆGn−1|n−1} operates on every element of f (xn−1).
Following the same logic, (7.41) can be transformed into
Pxx
n|n−1 = Q + ˆFn−1|n−1Pxx
n−1|n−1 ˆF⊺
n−1|n−1
+1
2

trace
 
Pxx
n−1|n−1 ˆGn−1|n−1
!
f (xn−1)

×

trace
 
Pxx
n−1|n−1 ˆGn−1|n−1
!
f⊺(xn−1)

+HOT
(7.54)
where the Jacobian ˆFn−1|n−1 is deﬁned as
ˆFn−1|n−1 ≜

∇xn−1f⊺(xn−1)
⊺""
xn−1=ˆxn−1|n−1
=


∂f1
∂xn−1,1
· · ·
∂f1
∂xn−1,nx
...
...
...
∂fnx
∂xn−1,1
· · ·
∂fnx
∂xn−1,nx


xn−1=ˆxn−1|n−1
(7.55)
and HOT stands for the higher order cross-terms.
In a similar fashion, the observation prediction estimates zn|n−1, Pzz
n|n−1 and
Pxz
n|n−1 transform into
ˆzn|n−1 = h
ˆxn|n−1

+ 1
2 trace
 
Pxx
n|n−1 ˆHn|n−1
!
h (xn)
"""
xn=xn|n−1
(7.56)

MULTIDIMENSIONAL CONSIDERATION
105
Pzz
n|n−1 = R + Hn|n−1Pxx
n|n−1 ˆH⊺
n|n−1
+1
2

trace
 
Pxx
n|n−1 ˆGn|n−1
!
h (xn)

×

trace
 
Pxx
n|n−1 ˆGn|n−1
!
h⊺(xn)

+ HOT
(7.57)
and
Pxz
n|n−1 = Pxx
n|n−1 H⊺
n|n−1
(7.58)
where
ˆHn|n−1 =

∇xnh⊺(xn)
⊺""
xn=xn|n−1 =


∂h1
∂xn,1
· · ·
∂h1
∂xn,nx
...
...
...
∂hnx
∂xn,1
· · ·
∂hnx
∂xn,nx


xn=ˆxn|n−1
(7.59)
and
ˆGn|n−1 ≜∇xn∇⊺
xn
=


∂2
∂x2
n,1
· · ·
∂
∂xn,1
∂
∂xn,nx
...
...
...
∂
∂xn,nx
∂
∂xn,1
· · ·
∂2
∂x2n,nx


xn=ˆxn|n−1
(7.60)
7.2.5
The Linearized Multidimensional Extended Kalman Filter Process
The EKF equations developed above contained terms that are of higher order than
linear. The standard linear EKF can be obtained by truncating so that only the linear
terms are maintained. This leads to an EKF that has the same form as the Linear
Kalman ﬁlter developed in Chapter 6. Thus, in summary, the process for the linearized
EKF is shown in Table 7.2.
ComparisonoftheLKFpredictionequationfromTable6.1withtheEKFprediction
equations in Table 7.2 reveals that the only difference, in form, between the two is
the added requirement for the EKF to compute the Jacobians ˆFn−1|n−1 and ˆHn|n−1.
In the update step, zo
n is the observation at time tn.
7.2.6
Second-Order Extended Kalman Filter
Occasionally, the nonlinearities are severe and the linearized EKF fails to con-
verge. For these cases, one approach is to use a second-order extended Kalman ﬁlter

106
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
TABLE 7.2
Multidimensional Extended Kalman Filter Process
Step 1.
Filter initialization:
Initialize ˆx0 and Pxx
0
Step 2.
State vector
ˆxn|n−1 = f 
ˆxn−1|n−1

prediction:
ˆFn−1|n−1 ≜
∇xn−1f⊺(xn−1)⊺
xn−1=ˆxn−1|n−1
Pxx
n|n−1 = ˆFn−1|n−1Pxx
n−1|n−1 ˆF⊺
n−1|n−1 + Q
Step 3.
Observation-related
ˆzn|n−1 = h 
ˆxn|n−1

prediction:
ˆHn|n−1 = 
∇xnh⊺(xn)⊺
xn=xn|n−1
Pzz
n|n−1 = ˆHn|n−1Pxx
n|n−1 ˆH⊺
n|n−1 + R
Pxz
n|n−1 = Pxx
n|n−1 ˆH⊺
n|n−1
Step 4.
Kalman
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
ﬁlter
update:
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.
(SOEKF) [2,3]. The SOEKF includes the second-order terms in the prediction equa-
tions. Thus, the linearized EKF prediction equations of Table 7.2 are replaced by the
second-order prediction equations
ˆxn|n−1 = f
ˆxn−1|n−1

(7.61)
+1
2 trace
 
Pxx
n−1|n−1 ˆGn−1|n−1
!
f (xn−1)
"""
xn−1=ˆxn−1|n−1
(7.62)
Pxx
n|n−1 = Q + ˆFn−1|n−1Pxx
n−1|n−1 ˆF⊺
n−1|n−1
+1
2

trace
 
Pxx
n−1|n−1 ˆGn−1|n−1
!
f (xn−1)

×

trace
 
Pxx
n−1|n−1 ˆGn−1|n−1
!
f⊺(xn−1)

(7.63)
ˆzn|n−1 = h
ˆxn|n−1

+ 1
2 trace
 
Pxx
n|n−1 ˆHn|n−1
!
h (xn)
"""
xn=xn|n−1
(7.64)

AN ALTERNATE DERIVATION OF THE MULTIDIMENSIONAL
107
Pzz
n|n−1 = R + Hn|n−1Pxx
n|n−1 ˆH⊺
n|n−1
+1
2

trace
 
Pxx
n|n−1 ˆGn|n−1
!
h (xn)

×

trace
 
Pxx
n|n−1 ˆGn|n−1
!
h⊺(xn)

(7.65)
Pxz
n|n−1 = Pxx
n|n−1 H⊺
n|n−1
(7.66)
These are usually very difﬁcult to implement because they require the computation
of both the Jacobian and Hessian matrices.
7.3
AN ALTERNATE DERIVATION OF THE MULTIDIMENSIONAL
COVARIANCE PREDICTION EQUATIONS
For future reference, an alternate set of EKF state covariance prediction equations will
be derived here. In the next chapter, these will be turned into ﬁnite difference solutions
that are used to show the relationship between the ﬁnite difference method and the
unscented Kalman ﬁlter. For numerical integration methods other than the ﬁnite
difference methods, this is the preferred method for evaluating covariance integrals.
Deﬁne the matrix
˜g (c) ≜

f (c) −ˆxn|n−1
 
f (c) −ˆxn|n−1
⊺
(7.67)
Now (7.34) can be rewritten as
¯Pxx
n|n−1 =

˜g (c) N (c; 0, I) dc + Q
(7.68)
This has the exact same form as the state prediction equation (5.51), resulting in
¯Pxx
n|n−1 = ˜g (0) + 1
2
 nx

i=1
∂2
∂c2
i
˜g (c)

c=0
+ Q
(7.69)
where ¯P has been used to distinguish this form from (7.41).
Similarly, the observation covariance prediction becomes
¯Pzz
n|n−1 = ˜p (0) + 1
2
 nx

i=1
∂2
∂c2
i
˜p (c)

c=0
+ R
(7.70)
where
˜p (c) ≜
˜h (c) −ˆzn|n−1
 ˜h (c) −ˆzn|n−1
⊺
(7.71)

108
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
The cross-covariance matrix prediction equation (5.41) can be transformed into
¯Pxz
n|n−1 =

˜r (c) N (c; 0, I) dc
(7.72)
where
˜r (c) ≜

f (c) −ˆxn|n−1
 ˜h (c) −ˆzn|n−1
⊺
(7.73)
Thus
¯Pxz
n|n−1 = ˜r (0) + 1
2
 nx

i=1
∂2
∂c2
i
˜r (c)

c=0
(7.74)
By substituting (7.67) into (7.69) and carrying out the differentiation and com-
paring the results to the Pxx
n|n−1 found in Table 7.2, we ﬁnd the relationship between
¯Pxx
n|n−1 and the linearized EKF covariance prediction estimate Pxx
n|n−1
¯Pxx
n|n−1 = Pxx
n|n−1 −1
4
 nx

i=1
∂2
∂c2
i
f (c)

c=0
 nx

i=1
∂2
∂c2
i
f
⊺(c)

c=0
(7.75)
This reveals that ¯Pxx
n|n−1 will be more accurate than the linearized EKF version of
Pxx
n|n−1 because ¯Pxx
n|n−1 includes part of the higher order terms from the SOEKF.
Similar results can be obtained for the observational covariance prediction estimates.
These forms of the covariance matrices will be used in the next chapter to develop
a ﬁnite difference ﬁlter that is identical to the unscented Kalman ﬁlter to be presented
in Chapter 9. Since we will not be using this form for the EKF itself, the derivation
will be carried no further.
7.4
APPLICATION OF THE EKF TO THE DIFAR SHIP TRACKING
CASE STUDY
In applying the EKF to the problem of tracking the Cartesian position and velocity of
a ship as it transits a buoy ﬁeld, a dynamic model of the ships kinematics over time
is required along with a model linking the bearing observations to the ships position.
Two additional things need to be discussed: how to initialize
ˆx0, Pxx
0

and how to
choose a proper acceleration noise q.
7.4.1
The Ship Motion Dynamics Model
The dynamic model can be obtained in vector–matrix form by using the ships state
vector deﬁned by (4.1) and combining (4.2) and (4.3) into
xn = Fxn−1 + vn−1
(7.76)

APPLICATION OF THE EKF TO THE DIFAR SHIP TRACKING CASE STUDY
109
where we have added a Gaussian white noise acceleration noise term vn−1 to allow for
small variations and maneuvers in the ships track. Here, we deﬁne the time-invariant
transition matrix F as
F ≜


1
T
0
0
0
1
0
0
0
0
1
T
0
0
0
1


(7.77)
and note that
Q ≜E

vnv⊺
n

= q


T 3
3
T 2
2
0
0
T 2
2
T
0
0
0
0
T 3
3
T 2
2
0
0
T 2
2
T


(7.78)
7.4.2
The DIFAR Buoy Field Observation Model
Deﬁne the buoy ﬁeld observation vector at time tn as
zn =

θ1,n, . . . , θM,n
⊺
(7.79)
The observation model for each buoy can be deﬁned by (4.7). We then combine all
buoys into the vector observation equation
zn = h (xn) + wn
(7.80)
with
θm,n = hm (xn) + wm,n
(7.81)
and
hm (xn) = tan−1
rx
n −xm
ry
n −ym

(7.82)
The covariance of the Gaussian white observation noise process wn is deﬁned as
R ≜E

wnw⊺
n

=


σ2
1
0
· · ·
0
0
0
σ2
2
· · ·
0
0
...
...
...
...
...
0
0
· · ·
σ2
M−1
0
0
0
· · ·
0
σ2
M


(7.83)
with σ2
m the variance associated with bearing observations from the mth buoy. For
our simulations, we will assume that σm = σ = 3o,∀m.

110
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
Since the dynamic equation is linear, the LKF state prediction equations from
Table 6.1 can be used for the tracking ﬁlter. Thus the state prediction equation that
will be used are, from Table 6.1
ˆxn|n−1 = Fˆxn−1|n−1
(7.84)
Pxx
n|n−1 = FPxx
n−1|n−1F⊺+ Q
(7.85)
However, since the observation equations (7.82) for each buoy are obviously non-
linear, the EKF observation prediction equations from Table 7.2 must be used, requir-
ing the computation of the Jacobian ˆHn|n−1.
The Jacobian is deﬁned by
ˆHn|n−1 =

∇xnh⊺(xn)
⊺""
xn=xn|n−1
=




∂
∂rxn
∂
∂vxn
∂
∂ry
n
∂
∂vy
n



h1 (xn)
· · ·
hM (xn) 


⊺
xn=xn|n−1
=


∂h1
∂rxn
· · ·
∂h1
∂vy
n
...
...
...
∂hM
∂rxn
· · ·
∂hM
∂vy
n


xn=xn|n−1
(7.86)
Carrying out the derivatives yields
ˆHn|n−1 =


ˆry
n|n−1−y1
R2
n|n−1
· · ·
ˆry
n|n−1−yM
R2
n|n−1
0
· · ·
0
−
ˆrx
n|n−1−x1
R2
n|n−1
· · ·
−
ˆrx
n|n−1−xM
R2
n|n−1
0
· · ·
0


(7.87)
with
Rn|n−1 =
#$
ˆrx
n|n−1
%2
+
$
ˆry
n|n−1
%2
(7.88)
The observation prediction equations we use for the application of the EKF to the
DIFAR case study will therefore be (from Table 7.2)
ˆzn|n−1 = h
ˆxn|n−1

(7.89)
Pzz
n|n−1 = ˆHn|n−1Pxx
n|n−1 ˆH⊺
n|n−1 + R
(7.90)
Pxz
n|n−1 = Pxx
n|n−1 ˆH⊺
n|n−1
(7.91)

APPLICATION OF THE EKF TO THE DIFAR SHIP TRACKING CASE STUDY
111
And, of course, we then use the standard Kalman ﬁlter update equations from Step 4
in Table 7.2.
7.4.3
Initialization for All Filters of the Kalman Filter Class
In a later chapter, we will be comparing the performance of all of the tracking ﬁlters
of the Kalman ﬁlter class. In order to make sure that we are comparing apples to
apples, we must use a common initialization for all of the tracking ﬁlters. First, we
assume that the target ship is radially inbound toward the origin from the Southwest,
as shown in Figure 7.1, starting from an initial range R of 4 nautical miles (nmi).
Taking an initial set of buoy measurements, {θm, m = 1, . . . , M}, we initialize the
state vector position components using
rx
0 = mean

rx
m −R sin θm

(7.92)
ry
0 = mean

ry
m −R cos θm

(7.93)
where

rx
m, ry
m

is the position of the mth buoy and the mean is taken over all buoys.
For the speed components we generate an estimate of the bearing to the target
relative to the origin by averaging the ﬁrst set of bearing measurements from all
buoys θ0 = [& θm]/M. We arbitrarily set the target ship speed at v = 30 knots and
compute a target heading by noting that θ0 ≃ϑ −π where ϑ is the target heading.
[rx(t),ry(t)]
Target ship
Y (North)
X (East)
(0,0)
θ
FIGURE 7.1
The geometry used for initialization.

112
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
The initial target speed components can now be computed from
vx
0 = v sin ϑ
(7.94)
vy
0 = v cos ϑ
(7.95)
Based on this initialization procedure, we see that there are two variables that must
be guessed at, namely the range R and the speed v, with the initial bearing estimated
from the ﬁrst set of DIFAR element bearing measurements.
Since we don’t have enough information to generate an initial covariance ma-
trix from the measurement covariance, we will arbitrarily initialize the state error
covariance with the diagonal matrix
Pxx
0 =


σ2
x
0
0
0
0
σ2
vx
0
0
0
0
σ2
y
0
0
0
0
σ2
vy


(7.96)
with σ2
x = σ2
y = 0.2 nmi2 and σ2
vx = σ2
vy = 3 knot2.
7.4.4
Choosing a Value for the Acceleration Noise
As noted previously, q has units of either speed2/time or acceleration2× time. For
slow moving targets like ships, it is more convenient to use a variation in speed,
while for fast moving targets like ﬁghter planes or missiles, the use of acceleration
variations (e.g., a 3 g turn) is more appropriate. For our slow moving ship target, we
choose q = 10−6 to allow for very small deviations in the ships velocity due to wave
action and wind. This small number is a result of the assumption that the ship will
not maneuver during the tracking interval. For a maneuvering vessel, a higher value
of q would be more appropriate.
7.4.5
The EKF Tracking Filter Results
RunningthetrackingEKFﬁlterforaﬁlterupdaterateofonesecond,withtheincoming
ship’s signal set with an SNR of 20 dB and a time-bandwidth product of 90, resulted in
thetrackdisplayedinFigure7.2.Althoughthistrackplotgivesanintuitivefeelforhow
the tracker algorithm is performing, the true performance measures will be discussed
and compared in a later chapter. From this plot we notice that it takes the tracker
some time before it converges on the true track, due to the inaccurate initialization.
But its appears to converge rapidly on the true track (shown in gray). This tracker
forgiveness for choosing an inaccurate initialization is important. The EKF (and all
of the following nonlinear Gaussian Kalman ﬁlters) are almost independent of the
initialization values chosen for R and v because the ﬁlter uses the observations to
quickly correct for the initialization errors. We have run this ﬁlter for an SNR of
20 dB with range initializations that go from 20 nmi down to 3 nmi and with speed
initializations from 10 to 30 knots with the ﬁlter quickly converging on the true track.

APPLICATION OF THE EKF TO THE DIFAR SHIP TRACKING CASE STUDY
113
−1.5
−1
−0.5
0
0.5
1
1.5
−4
−3
−2
−1
0
1
2
3
4
x-position (yds)
y-position (yds)
FIGURE 7.2
A comparison of the estimated track of a ship transiting the buoy ﬁeld with the
true track.
We noted in Chapter 4 that the bearing noise on the observations can become non-
Gaussian when the SNR is low. To see what affect this has on the tracks estimated
using a Gaussian EKF, in Figure 7.3 we present a plot of the EKF tracker output for
SNRs from 20 to −5 dB in increments of 5 dB. In Figure 7.3, the black lines are the
–2
–1.5
–1
–0.5
0
0.5
1
1.5
2
–4
–2
0
2
4
x-position (nmi)
y-position (nmi)
SNR 20; 0 Divergent tracks
–3
–2
–1
0
1
2
–4
–2
0
2
4
x-position (nmi)
y-position (nmi)
SNR 15; 0 Divergent tracks
–3
–2
–1
0
1
2
3
–4
–2
0
2
4
x-position (nmi)
y-position (nmi)
SNR 10; 0 Divergent tracks
–4
–3
–2
–1
0
1
2
3
4
–10
–5
0
5
10
x-position (nmi)
y-position (nmi)
SNR 5; 2 Divergent tracks
–80
–60
–40
–20
0
20
40
–100
–50
0
50
100
x-position (nmi)
y-position (nmi)
SNR 0; 94 Divergent tracks
–200
–100
0
100
200
300
–400
–200
0
200
400
x-position (nmi)
y-position (nmi)
SNR -5; 100 Divergent tracks
FIGURE 7.3
Comparison of the EKF tracker outputs for six SNRs.

114
THE ANALYTICAL LINEARIZATION CLASS OF KALMAN FILTERS
ﬁlter track estimates for 100 Monte Carlo runs (all using identical initializations) and
the gray line is the true track. One of the performance metrics that will be discussed in
Chapter 14 is the percentage of divergent tracks in a set of Monte Carlo runs. In Figure
7.3, we have listed the number of divergent tracks for each SNR. Since there are 100
Monte Carlo runs, the number of divergent tracks is identical to the percentage of
divergent tracks. It is obvious from the plots that the EKF has trouble tracking when
the SNR is at 5 dB or below due to the excessive number of bearing observation
outliers produced by the non-Gaussian noise.
REFERENCES
1. Jazwinski AH. Stochastic Processes and Filtering Theory. Academic Press; 1970.
2. Gelb A. Applied Optimal Estimation. The MIT Press; 1974.
3. Mahakababis A, Farooq M. A Second-order method for state estimation of non-linear
dynamical systems. Int. J. Control 1971:14(4) 631–639.

8
THE SIGMA POINT CLASS: THE
FINITE DIFFERENCE KALMAN
FILTER
The linearized EKF yields reasonable estimation results if the nonlinearities are not
very severe. For problems where the dynamic or observation transition functions are
highly nonlinear, second-order terms can be included in the EKF but at the expense of
adding the requirement of calculating the Hessian matrices. This added complication
is sometimes very difﬁcult to accomplish. One method to alleviate this difﬁculty is to
replace the differentials of the EKF with their ﬁnite difference equivalents.
Schei was the ﬁrst to propose using a central ﬁnite difference approach to lin-
earization in nonlinear estimation algorithms [1,2]. While the EKF is a linearization
of f (x) about the point ˆx, Schei’s method linearizes about the central difference points
ˆx + Dq. Because Schei replaces only the Jacobian in the prediction equations for both
the state (observation) vector and its covariance, the method is more accurate than
the EKF only in the state (observation) vector prediction step.
A more accurate derivative-free estimation method for nonlinear systems was
developed by Nørgaard et al. [3,4] by replacing the Taylor polynomial approximation
of the nonlinear function by a multidimensional version of Stirling’s interpolation
formula. This allowed one to keep higher order terms in both the state (observation)
vector and covariance prediction steps. The material presented below follow from the
developments in Ref. [3].
The papers by Ito and Xiong [5] and Wu et al. [6] expand on these ﬁnite difference
approximation methods and show how they ﬁt into a framework of prediction tech-
niques that implement the approximation of a nonlinear function using more general
polynomial expansions.
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
115

116
THE SIGMA POINT CLASS: THE FINITE DIFFERENCE KALMAN FILTER
In this chapter, a complete derivation of the ﬁnite difference Kalman ﬁlter will be
presented, beginning with the scalar case and then extending to the multidimensional
version.
8.1
ONE-DIMENSIONAL FINITE DIFFERENCE KALMAN FILTER
8.1.1
One-Dimensional Finite Difference State Prediction
The scalar derivation for the EKF state prediction equation in terms of f (c) was given
in (7.10) ), repeated here for clarity
ˆxn|n−1 = f (0) + 1
2
 d2
dc2 f (c)

c=0
(8.1)
As noted previously, this is accurate to third order in c.
Replacing the second-order derivative with its ﬁnite difference approximation from
(2.47), (8.1) becomes
ˆxn|n−1 = f (0) +
1
2q2

f (q) −2 f (0) + f (−q)

(8.2)
Gathering terms, this can be rewritten as
ˆxn|n−1 =
q2 −1
q2

f (0) +
1
2q2

f (q) + f (−q)

(8.3)
Let
w0 ≜q2 −1
q2
(8.4)
so that
q =
1
√1 −w0
(8.5)
and deﬁne the ﬁnite difference evaluation points as
c(j) = qr(j)
(8.6)
where
r(j) ≜

0,
j = 0
[1] ∈R1, j = 1, 2
(8.7)
See Section 2.2 for an explanation of the notation [1] ∈R1. Using (8.4) and (8.6),
(8.3) becomes
ˆxn|n−1 = w0 f

c(0)
+ 1 −w0
2

f

c(1)
+ f

c(2)
(8.8)

ONE-DIMENSIONAL FINITE DIFFERENCE KALMAN FILTER
117
Replacing the continuous c deﬁned in (7.3) with a discrete version, we can write
f

c(j)
≜f

ˆxn−1|n−1 + σxx,n−1|n−1c(j)
(8.9)
Deﬁning one-dimensional ﬁnite difference sigma points, χ(j)
n−1|n−1, in terms of the
ﬁnite-difference evaluation points, we obtain
χ(j)
n−1|n−1 ≜ˆxn−1|n−1 + σxx,n−1|n−1c(j)
=







ˆxn−1|n−1,
j = 0
ˆxn−1|n−1 +
1
√1−w0 σxx,n−1|n−1,
j = 1
ˆxn−1|n−1 −
1
√1−w0 σxx,n−1|n−1,
j = 2
(8.10)
Now, the one-dimensional ﬁnite difference state prediction equation (8.8) becomes
ˆxn|n−1 = w0f

χ(0)
n−1|n−1

+ 1 −w0
2

f

χ(1)
n−1|n−1

+ f

χ(2)
n−1|n−1

(8.11)
Letting w1 = w2 ≜(1 −w0) /2, this can be written as the sigma point sum
ˆxn|n−1 =
2

j=0
wjf

χ(j)
n−1|n−1

(8.12)
8.1.2
One-Dimensional Finite Difference State Variance Prediction
Replacing the ﬁrst- and second-order derivatives in (7.17) with their ﬁnite difference
equivalent from (2.44) and (2.47), the state error variance prediction equation becomes
σ2
xx,n|n−1 =
1
4q2

f (q) −f (−q)
2
+
1
2q4

f (q) −2 f (0) + f (−q)
2
+ σ2
v,n
(8.13)
Using (8.4), (8.6), and (8.10), this reduces to
σ2
xx,n|n−1 = 1 −w0
4

f

χ(1)
n−1|n−1

−f

χ(2)
n−1|n−1
2
+(1 −w0)2
2

f

χ(1)
n−1|n−1

−2f

χ(0)
n−1|n−1

+f

χ(2)
n−1|n−1
2
+ σ2
v,n
(8.14)
Note that this is accurate to ﬁfth order, as shown in Chapter 7.

118
THE SIGMA POINT CLASS: THE FINITE DIFFERENCE KALMAN FILTER
8.1.3
One-Dimensional Finite Difference Observation Prediction Equations
Following the same procedure as in the previous two sections, it follows immedi-
ately that the one-dimensional observation prediction and cross-variance prediction
equations are given by
ˆzn|n−1 =
2

j=0
wih

χ(j)
n|n−1

(8.15)
σ2
zz,n|n−1 = 1 −w0
4

h

χ(1)
n|n−1

−h

χ(2)
n|n−1
2
+(1 −w0)2
2

h

χ(1)
n|n−1

−2h

χ(0)
n|n−1

+ h

χ(2)
n|n−1
2
+σ2
w,n
(8.16)
σ2
xz,n|n−1 = σ2
xx,n|n−1

w1

h

χ(1)
n|n−1

−h

χ(2)
n|n−1

(8.17)
where the predictive one-dimensional ﬁnite difference sigma points {χ(j)
n|n−1, j =
0, 1, 2} are given by
χ(j)
n|n−1 =







ˆxn|n−1,
j = 0
ˆxn|n−1 +
1
√1−w0 σ2
xx,n|n−1, j = 1
ˆxn|n−1 −
1
√1−w0 σ2
xx,n|n−1, j = 2
(8.18)
8.1.4
The One-Dimensional Finite Difference Kalman Filter Process
(8.12) and (8.14)–(8.17) constitute the prediction portion of a one-dimensional
ﬁnite difference Kalman ﬁlter that is accurate to third order for the state and ob-
servation predictions and ﬁfth order for the covariance predictions. The procedure
for the complete one-dimensional ﬁnite difference Kalman ﬁlter is presented in
Table 8.1.
8.1.5
Simpliﬁed One-Dimensional Finite Difference Prediction Equations
Several simpliﬁcations can be made for these one-dimensional prediction equations.
The easiest simpliﬁcation is to take w0 = 0. This removes the point at the origin and
results in prediction equations with no free parameters. The prediction equations now

ONE-DIMENSIONAL FINITE DIFFERENCE KALMAN FILTER
119
TABLE 8.1
One-Dimensional Finite Difference Kalman Filter Process
Step 1.
Filter initialization:
Set the parameter 0 ≤w0 < 1
Initialize ˆx0 and σ2
xx,0
Step 2.
State vector
prediction:
χ(j)
n−1|n−1 =











ˆxn−1|n−1,
j = 0
ˆxn−1|n−1
+
1
√
1−w0 σxx,n−1|n−1,
j = 1
ˆxn−1|n−1
−
1
√
1−w0 σxx,n−1|n−1,
j = 2
ˆxn|n−1 = 2
j=0 wjf 
χ(j)
n−1|n−1

σ2
xx,n|n−1 = 1−w0
4
×
f 
χ(1)
n−1|n−1

−f 
χ(2)
n−1|n−1
2
+ (1−w0)2
2

f 
χ(1)
n−1|n−1

−2f 
χ(0)
n−1|n−1

+f 
χ(2)
n−1|n−1
2 + σ2
v,n
Step 3.
Observation-related
prediction:
χ(j)
n|n−1 =





ˆxn|n−1,
j = 0
ˆxn|n−1 +
1
√
1−w0 σ2
xx,n|n−1,
j = 1
ˆxn|n−1 −
1
√
1−w0 σ2
xx,n|n−1,
j = 2
ˆzn|n−1 = 2
j=0 wih
χ(j)
n|n−1

σ2
zz,n|n−1 = 1−w0
4
×
h
χ(1)
n|n−1

−h 
χ(2)
n|n−1
2
+ (1−w0)2
2
×
h
χ(1)
n|n−1

−2h 
χ(0)
n|n−1

+ h 
χ(2)
n|n−1
2
+σ2
w,n
σ2
xz,n|n−1 = σ2
xx,n|n−1
×
w1

h
χ(1)
n|n−1

−h 
χ(2)
n|n−1

Step 4.
Kalman ﬁlter
Kn = σ2
xz,n|n−1/σ2
zz,n|n−1
update:
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

σ2
xx,n|n = σ2
xx,n|n−1 −K2
nσ2
zz,n|n−1
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.

120
THE SIGMA POINT CLASS: THE FINITE DIFFERENCE KALMAN FILTER
become
ˆxn|n−1 = 1
2

f

χ(1)
n−1|n−1

+ f

χ(2)
n−1|n−1

(8.19)
σ2
xx,n|n−1 = 1
4

f

χ(1)
n−1|n−1

−f

χ(2)
n−1|n−1
2
+1
2

f

χ(1)
n−1|n−1

−2f

χ(0)
n−1|n−1

+f

χ(2)
n−1|n−1
2
+ σ2
v,n
(8.20)
ˆzn|n−1 = 1
2

h

χ(1)
n|n−1

+ h

χ(2)
n|n−1

(8.21)
σ2
zz,n|n−1 = 1
4

h

χ(1)
n|n−1

−h

χ(2)
n|n−1
2
+1
2

h

χ(1)
n|n−1

−2h

χ(0)
n|n−1

+h

χ(2)
n|n−1
2
+ σ2
w,n
(8.22)
σ2
xz,n|n−1 = 1
2σxx,n|n−1

h

χ(1)
n|n−1

−h

χ(2)
n|n−1

(8.23)
with
χ(j)
n−1|n−1 =

ˆxn−1|n−1 + σxx,n−1|n−1,
j = 1
ˆxn−1|n−1 −σxx,n−1|n−1,
j = 2
(8.24)
and
χ(j)
n|n−1 =

ˆxn|n−1 + σxx,n|n−1,
j = 1
ˆxn|n−1 −σxx,n|n−1,
j = 2
(8.25)
Another simpliﬁcation includes the truncation of some of the higher order terms.
These methods will be discussed more detail when we treat the multidimensional case
in the next section.
8.2
MULTIDIMENSIONAL FINITE DIFFERENCE KALMAN FILTERS
8.2.1
Multidimensional Finite Difference State Prediction
In manner similar to the approach taken in the one-dimensional case, we ﬁrst repeat
the multidimensional state prediction equation developed for the EKF given by (7.33)
ˆxn|n−1 = f (0) +
nx

j=1

∂2
∂c2
j
f (c)

c=0
(8.26)

MULTIDIMENSIONAL FINITE DIFFERENCE KALMAN FILTERS
121
Now, using the second-order ﬁnite difference term of multidimensional Stirling’s
polynomial given by equation (2.74) and letting x →c and x0 = 0, (8.26) becomes
ˆxn|n−1 = ˜f (0) +
1
2q2
nx

j=1

f

qej

−2f (0) + f

−qej

=
q2 −nx
q2

f (0) +
1
2q2
nx

j=1

f

qej

+ f

−qej

(8.27)
where ej is a unit vector along the Cartesian axis of the jth dimension and q is a step
size that must be ﬁnite, real, and greater than zero.
Deﬁning
w0 ≜q2 −nx
q2
(8.28)
leads to the identities
1
q2 = 1 −w0
nx
(8.29)
and
q =

nx
1 −w0
(8.30)
Here, w0 is a free parameter that determines the value of q. To maintain q as ﬁnite,
real, and greater than zero, we must restrict w0 to the range 0 ≤w0 < 1.
Equation (8.27) can now be written as
ˆxn|n−1 = w0˜f (0) + 1 −w0
2nx
nx

j=1

˜f

nx
1 −w0
ej

+ ˜f

−

nx
1 −w0
ej

(8.31)
To simplify this even further, we utilize the multidimensional vector generator
function [7,8] deﬁned in Section 2.2. For example, in the four-dimensional case we
have
r =

[0] ∈R4
[1] ∈R4 =



































r(0) = [0, 0, 0, 0]⊺
r(1) = [1, 0, 0, 0]⊺= e1
r(2) = [0, 1, 0, 0]⊺= e2
r(3) = [0, 0, 1, 0]⊺= e3
r(4) = [0, 0, 0, 1]⊺= e4
r(5) = [−1, 0, 0, 0]⊺= −e1
r(6) = [0, −1, 0, 0]⊺= −e2
r(7) = [0, 0, −1, 0]⊺= −e3
r(8) = [0, 0, 0, −1]⊺= −e4
(8.32)

122
THE SIGMA POINT CLASS: THE FINITE DIFFERENCE KALMAN FILTER
These vector integration points form a four-dimensional grid of unit vectors along
the axis of a Cartesian coordinate system and one additional point at the origin. If
the point at the origin is excluded, these unit vectors will all fall on the contour of
N (c; 0, I) at the four-dimensional hypersphere radius of one. See Figure 2.9 for a
graphical example in two dimensions.
For the general nx-dimensional case we have
r =

[0] ∈Rnx, j = 0
[1] ∈Rnx, j = 1, . . . , 2nx
(8.33)
where the number of vector points for [1] ∈Rnx is given by (2.30). A discrete c can
be now be deﬁned as
c(j) = qr(j) =

nx
1 −w0
r(j),
j = 0, . . . , 2nx
(8.34)
Now we can write equation (5.53) as
˜f

c(j)
= f

ˆxn−1|n−1 + Dn−1|n−1c(j)
(8.35)
where Dn−1|n−1 was deﬁned by (5.48). This leads to a deﬁnition of general multidi-
mensional ﬁnite difference sigma points
χ(j)
n−1|n−1 ≜ˆxn−1|n−1 + Dn−1|n−1c(j)
= ˆxn−1|n−1 +

nx
1 −w0
Dn−1|n−1r(j), j = 0, . . . , 2nx
(8.36)
It follows immediately that (8.31) can be rewritten as
ˆxn|n−1 =
2nx

j=0
wjf

χ(j)
n−1|n−1

(8.37)

MULTIDIMENSIONAL FINITE DIFFERENCE KALMAN FILTERS
123
with
wj =

w0,
j = 0
1−w0
2nx ,
j = 1, . . . , 2nx
(8.38)
where 2nx
j=0 wj = 1. As noted previously, this is a third order approximation.
8.2.2
Multidimensional Finite Difference State Covariance Prediction
The expression for the multidimensional EKF predictive state error covariance matrix
is given by (7.41), repeated here for clarity
Pxx
n|n−1 =
 nx

i=1
∂
∂ci
f (c)

c=0
 nx

i=1
∂
∂ci
f⊺(c)

c=0
+1
2
 nx

i=1
∂2
∂c2
i
f (c)

c=0
 nx

i=1
∂2
∂c2
i
f⊺(c)

c=0
+


nx

i=1
nx

j=1
j /= i
∂2
∂ci∂cj
f (c)


c=0


nx

i=1
nx

j=1
j /= i
∂2
∂ci∂cj
f⊺(c)


c=0
+Q
(8.39)
Using the ﬁrst- and second-order ﬁnite difference terms of multidimensional
Stirling’s polynomial for each dimension of ˜f (c) , given by equations (2.73) and
(2.74), this becomes
Pxx
n|n−1 =
1
4q2
nx

i=1
nx

j=1
˜f (qei) −˜f (−qei)
 ˜f

qej

−˜f

−qej
⊺
+ 1
2q4
nx

i=1
nx

j=1
˜f (qei) −2˜f (0) + ˜f (−qei)

×
˜f

qej

−2˜f (0) + ˜f

−qej
⊺
+ 1
4q4
nx

i=1
nx

j=1
nx

k=1
nx

l=1
˜f

qei + qej

−˜f

qei −qej

−˜f

−qei + qej

+ ˜f

−qei −qej

×
˜f (qek + qel) −˜f (qek −qel)
−˜f (−qek + qel) + ˜f (−qek −qel)
⊺
+Q
(8.40)

124
THE SIGMA POINT CLASS: THE FINITE DIFFERENCE KALMAN FILTER
With the aid of (8.28) and (8.36), this transforms into
Pxx
n|n−1 = 1 −w0
4nx
nx

i=1
nx

j=1

f

χ(i)
n−1|n−1

−f

χ(nx+i)
n−1|n−1

×

f

χ(j)
n−1|n−1

−f

χ(nx+j)
n−1|n−1
⊺
+1
2
1 −w0
nx
2
×
nx

i=1
nx

j=1

f

χ(i)
n−1|n−1

−2f

χ(0)
n−1|n−1

+ f

χ(nx+i)
n−1|n−1

×

f

χ(j)
n−1|n−1

−2f

χ(0)
n−1|n−1

+ f

χ(nx+j)
n−1|n−1
⊺
+Q + HOT
(8.41)
where some of the higher order cross-terms have been lumped into the word HOT.
8.2.3
Multidimensional Finite Difference Observation Prediction Equations
The observation and observation covariance were given in (7.42) and (7.43), respec-
tively. Following the method used in the last section, replacing the differentials by
their ﬁnite difference approximations, these become
ˆzn|n−1 =
2nx

j=0
wj ˜h

χ(j)
n|n−1

(8.42)
and
Pzz
n|n−1 = 1 −w0
4nx
nx

j=1

h

χ(j)
n|n−1

−h

χ(nx+j)
n|n−1

×

h

χ(j)
n|n−1

−h

χ(nx+j)
n|n−1
⊺
+1
2
1 −w0
nx
2 nx

j=1

h

χ(j)
n|n−1

−2h

χ(0)
n|n−1

+ h

χ(nx+j)
n|n−1

×

h

χ(j)
n|n−1

−2h

χ(0)
n|n−1

+ h

χ(nx+j)
n|n−1
⊺
+R
(8.43)
where the higher order cross-terms have been ignored and
χ(j)
n|n−1 = ˆxn|n−1 +

nx
1 −w0
Dn|n−1r(j)
(8.44)

AN ALTERNATE DERIVATION OF THE MULTIDIMENSIONAL FINITE
125
From (7.46), the state-observation cross-covariance prediction can be written as
Pxz
n|n−1 = Dn|n−1
 nx

i=1
∂
∂ci
˜hi (c)

c=0
(8.45)
Substituting the ﬁnite difference equivalent of the partial derivatives, (8.45) reduces
to
Pxz
n|n−1 = Dn|n−1
1
2q
nx

i=1
˜h (qei) −˜h (−qei)

(8.46)
Using (8.36) and (8.38), this becomes
Pxz
n|n−1 = 1
2
"
1 −w0
nx
Dn|n−1
nx

j=1

h

χ(j)
n|n−1

−h

χ(nx+j)
n|n−1

(8.47)
8.2.4
The Multidimensional Finite Difference Kalman Filter Process
The FDKF equations developed above contained terms that are of higher order than
linear and will be more accurate than either the LKF or the linearized EKF. If the
HOT terms are ignored, the FDKF process can be summarized as shown in Table 8.2.
As we did for the one-dimensional case, these prediction equations can be simpli-
ﬁed by setting w0 = 0. This removes the point at the origin and results in prediction
equations with no free parameters.
8.3
AN ALTERNATE DERIVATION OF THE MULTIDIMENSIONAL
FINITE DIFFERENCE COVARIANCE PREDICTION EQUATIONS
Applying the ﬁnite difference equation (2.73), the alternate EKF state error covariance
equation (7.69) becomes
¯Pxx
n|n−1 = ˜g (0) +
1
2q2
nx

j=1
˜g

qej

−2˜g (0) + ˜g

−qej

+ Q
=
q2 −nx
q2

˜g (0) +
1
2q2
nx

j=1
˜g

qej

+ ˜g

−qej

+ Q
(8.48)
From (7.67) we have ˜g (c) ≜

f (c) −ˆxn|n−1
 
f (c) −ˆxn|n−1
⊺
. Using the same
method as above, (8.48) transforms into
¯Pxx
n|n−1 = Q+
2nx

i=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1
 
f

χ(j)
n−1|n−1

−ˆxn|n−1
⊺
(8.49)

126
THE SIGMA POINT CLASS: THE FINITE DIFFERENCE KALMAN FILTER
TABLE 8.2
Multidimensional Finite Difference Kalman Filter Process
Step 1.
Filter initialization:
Set 0 ≤w0 < 1
Initialize ˆx0 and Pxx
0
Step 2.
State vector
prediction:
χ(j)
n−1|n−1 = ˆxn−1|n−1
+#
nx
1−w0 Dn−1|n−1r(j), j = 0, . . . , 2nx
ˆxn|n−1 = 2nx
j=0 wjf 
χ(j)
n−1|n−1

Pxx
n|n−1 = 1−w0
4nx
×nx
i=1
nx
j=1

f 
χ(i)
n−1|n−1

−f 
χ(nx+i)
n−1|n−1

×
f 
χ(j)
n−1|n−1

−f 
χ(nx+j)
n−1|n−1
⊺+ Q
Step 3.
Observation-related
prediction:
χ(j)
n|n−1 = ˆxn|n−1
+#
nx
1−w0 Dn|n−1r(j), j = 0, . . . , 2nx
ˆzn|n−1 = 2nx
j=0 wj ˜h 
χ(j)
n|n−1

Pzz
n|n−1 = 1−w0
4nx
nx
j=1

h 
χ(j)
n|n−1

−h 
χ(nx+j)
n|n−1

×
h 
χ(j)
n|n−1

−h 
χ(nx+j)
n|n−1
⊺+ R
Pxz
n|n−1 = 1
2
$
1−w0
nx Dn|n−1
×nx
j=1

h 
χ(j)
n|n−1

−h 
χ(nx+j)
n|n−1

Step 4.
Kalman
ﬁlter
update:
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.
As in the one-dimensional case, this latter form for the predictive state covariance
matrix is only partially of second order. It will be shown below that (8.37) and (8.49)
are identical to the unscented Kalman ﬁlter.
Note that (8.43) can be written in the alternate form
¯Pzz
n|n−1 = R +
2nx

j=0
wj

h

χ(j)
n|n−1

−ˆzn|n−1
 
h

χ(j)
n|n−1

−ˆzn|n−1
⊺
(8.50)

REFERENCES
127
In addition, (8.47) can be rewritten as
¯Pxz
n|n−1 = 1
2
"
1 −w0
nx
2nx

j=0

f

χ(j)
n−1|n−1

−ˆxn|n−1
 
h

χ(j)
n|n−1

−ˆzn|n−1
⊺
(8.51)
Because the ﬁnite-difference ﬁlter is almost identical to the unscented Kalman
ﬁlter, discussed in the next chapter, we will only present the results for the DIFAR
case study for the unscented Kalman ﬁlter.
REFERENCES
1. Schei TS. A ﬁnite-difference approach to linearization in nonlinear estimation algorithms.
Proc. Am. Control Conf. 1995;114–118.
2. Schei TS. A ﬁnite-difference method for linearization in nonlinear estimation algorithms.
Automatica 1997;33(11):2053–2058.
3. Nørgaard M, Poulsen, NK, Ravn O. Advances in Derivative-Free State Estimation for Non-
linear Systems. Technical University of Denmark, IMM-REP-1998–15; 2000.
4. Nørgaard M, Poulsen, NK, Ravn O. New developments state estimation for nonlinear sys-
tems. Automatica 2000;36:1627–1638.
5. Ito K, Xiong K. Gaussian ﬁlters for nonlinear ﬁltering problems. IEEE Trans. Automatic
Control 2000;45(5):910–927.
6. Wu Y, Hu D, Wu M, Hu X. A numerical-integration perspective on Gaussian ﬁlters. IEEE
Trans. Sig. Proc. 2006;54(8):2910–2921.
7. Lerner UN. Hybrid Bayesian Networks for Reasoning About Complex Systems, PhD Dis-
sertation. Department of Computer Science, Stanford University; October 2002.
8. McNamee J, Stenger F. Construction of fully symmetric numerical integration formulas.
Numerische Math. 1967;10:327–344.

9
THE SIGMA POINT CLASS: THE
UNSCENTED KALMAN FILTER
9.1
INTRODUCTION TO MONOMIAL CUBATURE INTEGRATION
RULES
In the derivations for the EKF in Chapter 7, all nonlinear functions were expanded
in a Taylor polynomial about a single point and the moment equations were then
used to evaluate the integrals for each monomial term of the expansion. This consti-
tutes an analytical method for evaluation of the integrals over the Gaussian-weighted
nonlinear functions. This analytical approach was continued for the FDKF, where
the Taylor polynomial is replaced by a multidimensional Stirling’s interpolation for-
mula. Stirling’s interpolation approximation required the evaluation of the nonlinear
functions at vector points equally spaced in all dimensions about the original Taylor
polynomial expansion point. Although the FDKF has the same form as a sigma point
Kalman ﬁlter, it was derived in a completely analytical way and can therefore also be
thought of as part of the analytical linearization class of Kalman ﬁlters.
An alternative approach to evaluation of the Gaussian-weighted integrals is through
the use of multidimensional numerical integration methods. In one dimension, these
methods are classiﬁed as the well-known (to those who know them well) quadrature
integration methods. For the multidimensional case, they have been labeled as “multi-
dimensional quadrature” or “cubature” integration methods [1,2]. In this book, we
will use both terms interchangeably.
As noted in Chapter 5, the Gaussian prediction equations shown in (5.51) through
(5.57) are of the general integration form shown in (5.60) through (5.62). In this
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
128

INTRODUCTION TO MONOMIAL CUBATURE INTEGRATION RULES
129
chapter, we will present several cubature methods for the numerical integration of
these integrals.
Consider the general multidimensional Gaussian-weighted integral
ˆg (x) =

˜g (c) N (c; 0, I) dc
(9.1)
We have seen in Chapter 5 that this integral can be approximated by a discrete sum
of the form (5.64)
ˆg (x) ≃
ns

j=1
wj˜g

c(j)
(9.2)
The exact form of wj, c(j) and ns depend on the multidimensional cubature integration
rule chosen. In general, an ns point rule that evaluates the integral is expressed as
Rns =

wj, c(j)
, j = 1, 2, . . . , 1, . . . , ns

(9.3)
where ns is the number of integration points. ns is usually related in some way to nx.
In this chapter and the two that follow we will consider only monomial rules, which
will be deﬁned below.
If we consider the vector c = [c1, c2, . . . , ck]⊺, then a multidimensional monomial
of degree d takes the form (note discussion at the end of Chapter 5)
a (j1, j2, . . . , jk) cj1cj2 · · · cjk
(9.4)
where j1 + j2 + · · · + jk ≤d. For example, following the two-dimensional example
shown at the end of Chapter 5, for c ∈R2, a basis of polynomials spanned by the
monomials of degree d less than or equal to 2 is the set

1, c1, c2, c1c2, c2
1, c2
2

.
For d = 2, the condition j1 + j2 ≤d must be met for each monomial term in the
polynomial. In general, for a vector c ∈Rk there are
 k + d
d

distinct monomials of
precision d or less. The binomial coefﬁcient
 n
m

is the number of ways of choosing
m objects from a collection of n distinct objects without regard to order. It is deﬁned
as
	
n
m

≜
n!
m! (n −m)!
(9.5)
■EXAMPLE 9.1
For k = 2 and d = 2, we deﬁne
dim (k, d) ≜
	
k + d
d

=
	
4
2

=
4!
2! (4 −2)! = 6
(9.6)

130
THE SIGMA POINT CLASS: THE UNSCENTED KALMAN FILTER
Anns−pointmonomialruleofdegreed isarule(9.3)thatevaluates

Rk
˜g (c) w (c) dc
exactly via
ns
j=1 wj ˜g

c(j)
whenever ˜g (c) is a polynomial of degree less than or
equal to d. Label an arbitrary basis set of monomials as {b1, b2, . . . , bdim (k,d)}. If a
rule Rns ={(wj, c(j)), j = 1, . . . , ns} integrates every polynomial of degree d or less,
then it must satisfy the monomial system of density-weighted moment equations [3]
ns

j=1
wjbi

c(j)
=

Rk
bi (c) w (c) dc, i = 1, 2, . . . , 1, . . . , dim (k, d)
(9.7)
For a Gaussian weight density, this becomes
ns

j=1
wjbi

c(j)
=

Rk
bi (c) N (c; 0, I) dc, i = 1, 2, . . . , 1, . . . , dim (k, d)
(9.8)
Now, assume that we want a monomial rule that satisﬁes the Gaussian
density-weighted moment equations just up to the second moment. For this case,
{b1, b2, . . . , bdim (k,d)} →{1, c, cc⊺} since dim (k, d) = dim (k, 2) , and the system
of moment equations become
ns

j=1
wj =

Rk
N (c; 0, I) dc = 1
(9.9)
ns

j=1
wjc(j) =

Rk
cN (c; 0, I) dc = 0
(9.10)
ns

j=1
wjc(j)c(j)⊺=

Rk
cc⊺N (c; 0, I) dc = I
(9.11)
The ﬁnal step in the formation of the integration rule is to specify a set of vector
points {c(j), j = 1, 2, . . . , 1, . . . , ns} that lie on a k-dimensional hypersphere and
use them to solve for the weights wj using (9.9)–(9.11).
9.2
THE UNSCENTED KALMAN FILTER
9.2.1
Background
The unscented Kalman ﬁlter (UKF) has its origins in a pair of papers presented
at the 1995 American Control Conference in Seattle, Washington [4,5]. In both
papers, a new method for implementing a linearized approximation to nonlinear state
estimation was presented that, unlike the EKF, did not require the explicit calculation
of Jacobians. Their method of “approximating a Gaussian distribution efﬁciently by a
discrete distribution [of vector points] allows a nonlinear transformation to be applied

THE UNSCENTED KALMAN FILTER
131
to each of the points independently.” Their method was ad hoc in that they speciﬁed a
set of vector points that are symmetric about the mean of a multidimensional Gaussian
distribution. The points were selected so that when the set of points are used as the
input to a nonlinear transformation, a weighted sum of the resulting output points
produced a better estimate of the transformed mean than that produced by an EKF.
In Refs [6–8], Julier presents his method in much more detail where, in Ref.
[7], he called his method a “distribution approximation ﬁlter” and demonstrated its
superiority to the EKF for highly nonlinear functional transformations. In Ref. [8]
Julier claimed “that it is easier to approximate a probability distribution than it is
to approximate an arbitrary nonlinear function or transformation.” However, in Ref.
[9], Lefebvre, et al. “derives the exact same estimator by linearizing the process and
measurement functions by a statistical linear regression through some regressions
points.” Their method has some similarities with the method we develop below, which
is completely analytical and involves the use of monomial integration rules with
polynomial approximations of the nonlinear functions. The method presented below
does not involve in any way the approximation of a density function, but rather the
evaluation of a set of Gaussian moment equations for the selected integration points.
The sigma points developed in an ad hoc way by Julier and Uhlmann are shown to be
a natural result of the afﬁne transformation of the state vector leading to estimation
solutions that utilize simple moment integrals of a “normalized” Gaussian density.
Subsequent extension of the unscented Kalman ﬁlter quickly followed. In Ref.
[10], Wan and van der Merwe extend the UKF to include parameter estimation in
addition to state estimation. A square root version of the UKF for state and parameter
estimation was published in Ref. [11]. Additional test points were added to the UKF in
Ref. [12] allowing the UKF to be applied to discontinuous nonlinear functions. Julier
generalized his UKF algorithms in Ref. [13] to allow the sigma points to be scaled
to an arbitrary number of dimensions. The UKF was extended to higher order by
Tenne and Singh in Ref. [14]. Finally, for those cases where the state (or observation)
vector can be parsed into linear and nonlinear components, a Rao-Blackwell UKF
was developed by Briers et al. in Ref. [15]. Most recently, an iterated version of the
UKF was published by Banani and Masnadi-Shiraz [16].
Excellent summaries of the original UKF along with many of the extensions were
published almost simultaneously by van der Merwe [17] and Julier and Uhlmann
[18]. These provide an excellent review of the methods in addition to some practical
applications and examples.
9.2.2
The UKF Developed
To develop the unscented Kalman ﬁlter, consider the case where the vector points

c(j), j = 1, 2, . . . , 1, . . . , ns

, of the same dimension as the state vector, form
symmetric pairs at a radius of ±q along each Cartesian axes and a single point at the
origin. This latter condition ensures that, with the exception of the point at the origin,
all vector points will lie on the same nx-dimensional hypersphere. The symmetry of
having two vector points for each dimension also ensures that the total number of inte-
gration points will be given by ns = 2nx + 1. This set of vector points can be described

132
THE SIGMA POINT CLASS: THE UNSCENTED KALMAN FILTER
by c(j) = qr(j), with the unit vector set r = [1] ∈Rnx all lying on the axes of a unit
hypersphere [obtained from (2.30)] and a point at the origin. For nx = 4, r is given
by (8.32).
For c ∈R4, there are ns = 2nx + 1 = 9 vector evaluation points, since nx = 4.
Using these points in (9.9)–(9.11) results in the following system of moment equations
8

j=0
wj = 1
(9.12)
8

j=0
wjc(j) = q

w1r(1) + w2r(2) + · · · + w8r(8)
= 0
(9.13)
8

j=0
wjc(j)c(j)⊺= q2 
w1r(1)r(1)⊺+ · · · + w8r(8)r(8)⊺
= I
(9.14)
Equation (9.13) can be rewritten as
(w1 −w5)


1
0
0
0

+ (w2 −w6)


0
1
0
0

+ (w3 −w7)


0
0
1
0


+ (w4 −w8)


0
0
0
1

=


0
0
0
0


This can be satisﬁed by setting w1 = w5, w2 = w6, w3 = w7, and w4 = w8.
Equation (9.14) can be written explicitly as
(w1 + w5) q2


1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

+ (w2 + w6) q2


0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0


+ (w3 + w7) q2


0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0

+ (w4 + w6) q2


0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1


=


1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1


(9.15)

THE UNSCENTED KALMAN FILTER
133
This leads to the set of equations
(w1 + w5) q2 = 2w1q2 = 1 →q =

1
2w1
(9.16)
and
(w2 + w6) q2 = 2w2q2 = 1 →q =

1
2w2
(9.17)
Thus, w1 = w2. In a similar manner, it follows that
wj = w1, j = 2, 3, . . . , 8
(9.18)
Now, from (9.12) it follows that
w0 + 8w1 = 1
(9.19)
or
w1 = 1 −w0
8
(9.20)
Using (9.20) in (9.16) results in
q =

4
1 −w0
(9.21)
Now
c(j) = qr(j) =

4
1 −w0
r(j), j = 0, 1, . . . , 8
(9.22)
with
wj =

w0,
j = 0
1−w0
8
,
j = 1, 2, . . . , 8
(9.23)
Here, w0 is a free parameter, but in order to satisfy (9.12), and to ensure that all
weights are positive, we must impose the restriction 0 ≤w0 ≤1.
For the general case where c ∈Rnx, by induction it follows immediately that
c(j) = qr(j) =

nx
1 −w0
r(j), j = 0, 1, . . . , 2nx
(9.24)
wj =

w0,
j = 0
1−w0
2nx ,
j = 1, . . . , 2nx
(9.25)

134
THE SIGMA POINT CLASS: THE UNSCENTED KALMAN FILTER
9.2.3
The UKF State Vector Prediction Equation
From (5.51), (9.1), and (9.2), we can now write the state vector prediction equation
as
ˆxn|n−1 =

˜f (c) N (c; 0, I) dc
=
2nx

j=0
wjf

c(j)
(9.26)
From (5.53), for each vector integration point c(j) we can write
f

c(j)
= f

ˆxn−1|n−1 + Dn−1|n−1c(j)
, j = 0, 1, . . . , 2nx
(9.27)
Deﬁning the UKF sigma points as
χ(j)
n−1|n−1 ≜ˆxn−1|n−1 + Dn−1|n−1c(j), j = 0, 1, . . . , 2nx
(9.28)
the state vector prediction equation (9.26) becomes
ˆxn|n−1 =
2nx

j=0
wjf

χ(j)
n−1|n−1

(9.29)
where c(j) and wj are given by (9.24) and (9.25), respectively, and Dn−1|n−1 =
[Pxx
n−1|n−1]1/2.
9.2.4
The UKF State Vector Covariance Prediction Equation
If we let
˜g (c) ≜
˜f (c) −ˆxn|n−1
 ˜f (c) −ˆxn|n−1
⊺
(9.30)
anduseitin(9.26)inplaceof f(c),then(9.29)becomesthestatecovarianceprediction
equation
Pxx
n|n−1 =

˜g (c) N (c; 0, I) dc + Q
=
2nx

j=0
wj˜g

c(j)
+ Q
=
2nx

j=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1

×

f

χ(j)
n−1|n−1

−ˆxn|n−1
⊺
+ Q
(9.31)

THE UNSCENTED KALMAN FILTER
135
9.2.5
The UKF Observation Prediction Equations
In a similar manner, the observation prediction equations are given by
ˆzn|n−1 =
2nx

j=0
wjh

χ(j)
n|n−1

(9.32)
Pzz
n|n−1 =
2nx

j=0
wj

h

χ(j)
n|n−1

−ˆzn|n−1
 
h

χ(j)
n|n−1

−ˆzn|n−1
⊺
+R
(9.33)
Pxz
n|n−1 =
2nx

j=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1

×

h

χ(j)
n|n−1

−ˆzn|n−1
⊺
(9.34)
with
χ(j)
n|n−1 ≜ˆxn|n−1 + Dn|n−1c(j), j = 0, 1, . . . , 2nx
Note that the UKF state and observation vector prediction equations (9.29 and
(9.32) are identical to the form of the ﬁnite difference prediction equations (8.37) and
(8.42), respectively, while the UKF covariance prediction equations (9.31), (9.33),
and (9.34) are the same as those of the second form of the ﬁnite difference equations
(8.51)–(8.49), respectively.
9.2.6
The Unscented Kalman Filter Process
The UKF equations developed above contained terms that are of higher order than
linear and will be more accurate than either the LKF or the linearized EKF. The UKF
process is summarized in Table 9.1.
9.2.7
An Alternate Version of the Unscented Kalman Filter
In the discussion contained in the paragraph prior to (9.12) it was noted that all point
except the point at the origin lie on a hypersphere at a radius q. Since the point at the
origin was added in an arbitrary way, there is no loss of generality if we take w0 = 0,
removing the sigma point at the origin from the integration sum. Note that the sigma
point at the origin (at r(0)) coincides with ˆxn−1|n−1 or ˆxn|n−1. This simpliﬁes the UKF
removing the arbitrary nature of picking a value for 0 ≤w0 ≤1. Now all sums have
the limits j = 1, . . . , 2nx, with wj = 1/2nx, ∀j, and χ(j) = ˆx + √nxDr(j).
The identical results were obtained by Arasaratnam and Haykin [19] following the
methods ﬁrst presented in Refs [2,20]. They considered a spherical–radial cubature
rule in which the Cartesian Gaussian-weighted integrals are transformed into polar

136
THE SIGMA POINT CLASS: THE UNSCENTED KALMAN FILTER
TABLE 9.1
Multidimensional Unscented Kalman Filter Process
Step 1.
Filter initialization:
Set 0 ≤w0 < 1
Initialize ˆx0 and Pxx
0
Step 2.
State vector
prediction:
χ(j)
n−1|n−1 = ˆxn−1|n−1
+Dn−1|n−1c(j), j = 0, . . . , 2nx
ˆxn|n−1 = 2nx
j=0 wjf 
χ(j)
n−1|n−1

Pxx
n|n−1 = 2nx
j=0 wj

f 
χ(j)
n−1|n−1

−ˆxn|n−1

×
f 
χ(j)
n−1|n−1

−ˆxn|n−1
⊺+ Q
Step 3.
Observation-
related
prediction:
χ(j)
n|n−1 = ˆxn|n−1
+Dn|n−1c(j), j = 0, . . . , 2nx
ˆzn|n−1 = 2nx
j=0 wj ˜h 
χ(j)
n|n−1

Pzz
n|n−1 = 2nx
j=0 wj

h 
χ(j)
n|n−1

−ˆzn|n−1


h 
χ(j)
n|n−1

−ˆzn|n−1
⊺+ R
Pxz
n|n−1 = 2nx
j=0 wj

f 
χ(j)
n−1|n−1

−ˆxn|n−1

×
h 
χ(j)
n|n−1

−ˆzn|n−1
⊺
Step 4.
Kalman ﬁlter
update:
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.
coordinates. In the development of the integration rule used for the UKF, (9.12)–
(9.14) required the solution for 2nx + 1 simultaneous equations. In the spherical–
radial method, the number of simultaneous equations is reduced to two by taking
advantage of the fully symmetric nature of the Gaussian-weighted integrals. But in
the end, their method results in a nonlinear Kalman ﬁlter that is identical to the UKF
developed above with w0 = 0.

APPLICATION OF THE UKF TO THE DIFAR SHIP TRACKING CASE STUDY
137
9.3
APPLICATION OF THE UKF TO THE DIFAR SHIP TRACKING
CASE STUDY
Since the dynamic equation is linear for the DIFAR tracking problem, the UKF
process used for this problem is shown in Table 9.2. Note that this process is
generic for all sigma point Kalman ﬁlters, which will differ only in the values as-
signed to

wj, c(j)
, j = 0, 1, . . . , ns

. For the DIFAR case, in Table 9.2 we take
r = [1] ∈R4 for nx = 4. Thus, there are nine sigma points for this UKF since we
included the sigma point at the origin. For the sigma points, the values used for

wj, c(j)
, j = 0, 1, . . . , ns

aregivenin(9.22)and(9.23),whereweletw0 = 0.25.
TABLE 9.2
Multidimensional Sigma Point Kalman Filter Process Applied to DIFAR
Tracking
Step 1.
Filter initialization:
Set 0 ≤w0 < 1
Initialize ˆx0 and Pxx
0
Step 2.
State vector
prediction:
ˆxn|n−1 = Fˆxn−1|n−1
Pxx
n|n−1 = FPxx
n−1|n−1F⊺+ Q
Dn|n−1 = 
Pxx
n|n−1
1/2
Step 3.
Observation-
related
prediction:
χ(j)
n|n−1 = ˆxn|n−1
+Dn|n−1c(j), j = 0, . . . , 2nx
ˆzn|n−1 = 2nx
j=0 wjh 
χ(j)
n|n−1

Pzz
n|n−1 = 2nx
j=0 wj

h 
χ(j)
n|n−1

−ˆzn|n−1


h 
χ(j)
n|n−1

−ˆzn|n−1
⊺+ R
Pxz
n|n−1 = 2nx
j=0 wj

f 
χ(j)
n−1|n−1

−ˆxn|n−1

×
h 
χ(j)
n|n−1

−ˆzn|n−1
⊺
Step 4.
Kalman ﬁlter
update:
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.

138
THE SIGMA POINT CLASS: THE UNSCENTED KALMAN FILTER
This UKF ﬁlter used the same initialization and dynamic noise q values used for the
EKF, given in Sections 7.4.3 and 7.4.4, respectively.
A comparison of 100 Monte Carlo output tracks for each of six different signal
SNRs from a UKF tracker for the DIFAR case study with the plot for the EKF shown
in Figure 7.3 reveals that the outputs at all signal dB levels is almost identical for the
two tracking algorithms as are the number of divergent tracks so such plots will not
be presented here.
REFERENCES
1. LuJ,DarmofalDL.Higher-dimensionalintegrationwithGaussianweightsforapplications
in probabilistic design. SIAM J. Sci. Comput. 2004; 26(2):613–624.
2. Dunavant D A. Efﬁcient symmetrical cubature rules for complete polynomials of higher
degree over the unit cube. Int. J. Numer. Methods Eng. 1986;23:397–407.
3. Haber S. Numerical evaluation of multiple integrals. SIAM Rev. 1970;12(4):481–526.
4. Quine B, Uhlmann J, Durrant-Whyte H. Implicit Jacobians for linearized state estimation
in nonlinear systems. Proc. Am. Control Conf. 1995:1645–1646.
5. Julier SJ, Uhlmann JK, Durrant-Whyte HF. A new approach for ﬁltering nonlinear systems.
Proc. Am. Control Conf. 1995:1628–1632.
6. JulierS,Uhlmann,JK.AGeneralMethodforApproximatingNonlinearTransformationsof
Probability Distributions. Robotics Research Group, Department of Engineering Science,
University of Oxford, UK; 1996.
7. Julier SJ. Process Models for the Navigation of High Speed Land Vehicles, Dissertation.
Oxford, UK: Wadham College; 1997.
8. Julier S, Uhlmann J, Durrant-Whyte HF. A new method for the nonlinear transforma-
tion of means and covariances in ﬁlters and estimators. IEEE Trans. Automatic Control
2000;45(3):477–482.
9. Lefebvre T, Bruyninckx H, De Schutter J. Comments on “A new method for the nonlinear
transformation of means and covariances in ﬁlters and estimators.” IEEE Trans. Automatic
Control 2002;47(8):1406–1408.
10. Wan EA, van der Merwe R. The Unscented Kalman Filter for Nonlinear Estimation.
Adaptive Systems for Signal Processing, Communications, and Control Symposium, AS-
SPCC, 2000, pp.153–158.
11. van der Merwe R, Wan EA. The Square-Root Unscented Kalman Filter for State and
Parameter Estimation. ICASSP 01, Vol. 6, 2001, pp.3461–3464.
12. Van Zandt J. A more robust unscented transform. Proc. SPIE 2001;4473:371–379.
13. Julier SJ. The scaled unscented transform. Proc. Am. Control Conf. 2002;4555–4559.
14. Tenne D, Singh T. The higher order unscented ﬁlter. Proc. Am. Control Conf. 2003;3:2441–
2446.
15. Briers M, Maskell SR, Wright R. A Rao-Blackwellised unscented Kalman ﬁlter. Inform.
Fusion 2003;1:55–61.
16. Banani SA, Masnadi-Shirazi MA. A new version of unscented Kalman ﬁlter. Proc. World
Acad. Sci. Eng. Technol. 2007;20:192–197.

REFERENCES
139
17. van der Merwe R. Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic
State-Space Models, Dissertation. Portland OR: Oregon Health & Science University;
2004.
18. Julier SJ, Uhlmann JK. Unscented ﬁltering and nonlinear estimation. Proc. IEEE
2004;92(3):401–422.
19. Arasaratnam I, Haykin S. Cubature Kalman ﬁlters. IEEE Trans. Automatic Control
2009;54(6):1254–1269.
20. Monahan J, Genx A. Spherical-radial integration rules for Bayesian computation. J. Am.
Stat. Assoc. 1997;92(438):664–674.

10
THE SIGMA POINT CLASS: THE
SPHERICAL SIMPLEX KALMAN
FILTER
For the UKF, with the exception of the integration point at the origin, the vector
integrationpointsusedforthemomentintegrations(9.9)–(9.11)aresymmetricalabout
0, equidistant from the origin, and fall on the axes of an nx-dimensional Cartesian
coordinatesystem.ForthesphericalsimplexKalmanﬁlter,wereplacetherequirement
for symmetry about the origin with a requirement that all vector points be equidistant
from the origin and from each other, thus lying at the vertices of an nx-dimensional
simplex (see Figure 2.4). This integration rule was ﬁrst proposed by Julier [1].
Let the integration (sigma) points be redeﬁned as c(j) →cj. All of these simplex
vector points cj must satisfy the moment equations (9.9)–(9.11), repeated here for
clarity
nx

j=0
wj = 1
(10.1)
nx

j=0
wjcj = 0
(10.2)
nx

j=0
wjcjc⊺
j = I
(10.3)
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
140

ONE-DIMENSIONAL SPHERICAL SIMPLEX SIGMA POINTS
141
10.1
ONE-DIMENSIONAL SPHERICAL SIMPLEX SIGMA POINTS
Consider the one-dimensional case (nx = 1) with three arbitrary points along a line
at points (c0, c1, c2) = (0, q1, −q2), as shown in Figure 2.4. Inserting these points into
the moment equations yields the set of equations
w0 + w1 + w2 = 1
(10.4)
w1q1 −w2q2 = 0
(10.5)
w1q2
1 + w2q2
2 = 1
(10.6)
Noting that the vector points must be equidistant from the origin, making q2 = q1,
from (10.5) it follows immediately that
w2 = w1
(10.7)
From (10.4) we obtain
w1 = w2 = 1 −w0
2
(10.8)
Now from (10.6)
q1 =
1
√2w1
(10.9)
Thus, for the 1D case, the sigma points are deﬁned by
c1D
j
=





0,
j = 0
1/√2w1,
j = 1
−1/√2w1, j = 2
(10.10)
with the weights
0 ≤w0 ≤1
(10.11)
w1 = 1 −w0
2
(10.12)
w2 = 1 −w0
2
(10.13)
Remembering that for one dimension, x = ˆx + σc, then the 1D sigma points are
given by
χ0 = ˆx
(10.14)
χ1 = ˆx + σc1D
1
(10.15)
χ2 = ˆx + σc1D
2
(10.16)

142
THE SIGMA POINT CLASS
e2
e1
c3 = (0, s3q3)
c2 = (– q1, – q3)
c1 = (q1, – q3)
c0 = (0, 0)
FIGURE 10.1
Depiction of a two-dimensional simplex with four vector integration points.
10.2
TWO-DIMENSIONAL SPHERICAL SIMPLEX SIGMA POINTS
For the two-dimensional case, consider the vector points at the vertices of the
simplex shown in Figure 10.1. In this equilateral triangle, all sides and an-
gles are equal and all integration points are equidistant from the point at the
origin. For this 2D case, the vector integration points are the set (c0, c1, c2, c3) =

(0, 0)⊺, (q1, −q3)⊺, (−q1, −q3)⊺, (0, s3q3)⊺
. Here, s3 is a scale factor that will be
determined below.
Once again, the moment equations (10.1)–(10.3) must be satisﬁed. Thus, from
(10.1) it follows that
w0 + w1 + w2 + w3 = 1
(10.17)
and (10.2) results in
w1q1 −w2q1 = 0
(10.18)
−w1q3 −w2q3 + w3s3q3 = 0
(10.19)
Finally, (10.3) leads to
w1

q2
1
0
0
q2
3
	
+ w2

q2
1
0
0
q2
3
	
+ w3

0
0
0
s2
3q2
3
	
=

1
0
0
1
	
(10.20)
which reduces to the set of equations
w1q2
1 + w2q2
1 = 1
(10.21)
w1q2
3 + w2q2
3 + w3s2
3q2
3 = 1
(10.22)

TWO-DIMENSIONAL SPHERICAL SIMPLEX SIGMA POINTS
143
From (10.18) it follows that
w2 = w1
(10.23)
and (10.22) becomes
2w1q2
3 + w3s2
3q2
3 = 1
(10.24)
Since each point is equidistant from the origin, it can be assumed that the weight on
each point is the same, and therefore w3 = w1. From (10.17) it follows that
w1 = w2 = w3 = 1 −w0
3
(10.25)
From (10.19) we ﬁnd that s3 = 2 and (10.24) now leads to
q3 =
1
√6w1
=
1
√2 (2 + 1) w1
(10.26)
To summarize, for the 2D case, the integration points are given by
c2D
0
=

0
0
	
=

c1D
0
0
	
(10.27)
c2D
1
=

c1D
1
−
1
√2(2+1)w1
	
(10.28)
c2D
2
=

c1D
2
−
1
√2(2+1)w1
	
(10.29)
c2D
3
=

c1D
0
2
√2(2+1)w1
	
(10.30)
with the weights given by
0 ≤w0 ≤1
(10.31)
wj = 1 −w0
3
,
j = 1, 2, 3
(10.32)
Note that (10.28) and (10.29) can be combined into one equation
c2D
j
=

c1D
j
−
1
√2(2+1)w1
	
,
j = 1, 2
(10.33)

144
THE SIGMA POINT CLASS
10.3
HIGHER DIMENSIONAL SPHERICAL SIMPLEX SIGMA POINTS
For the general nx-dimensional case, we obtain, by inspection
cnxD
0
= 0nx =

c(nx−1)D
0
0
	
,
j = 0
(10.34)
cnxD
j
=

c(nx−1)D
j
−
1
√nx(nx+1)w1
	
,
j = 1, . . . , nx
(10.35)
cnxD
nx+1 =

c(nx−1)D
0
nx
√nx(nx+1)w1
	
(10.36)
with the weights given by
0 ≤w0 ≤1
wj = 1 −w0
nx + 1 ,
j = 1, . . . , nx + 1
(10.37)
Remembering that x = ˆx + Dc, the integration points become the spherical simplex
sigma points
χ(j) = ˆx + DcnxD
j
,
j = 0, . . . , nx + 1
(10.38)
10.4
THE SPHERICAL SIMPLEX KALMAN FILTER
Returning to (5.51), we can now write
ˆxn|n−1 =

f (c) N (c; 0, I) dc
=
nx+1

j=0
wjf

χ(j)
n−1|n−1

(10.39)
where from (10.38) we can identify the sigma points as
χ(j)
n−1|n−1 = ˆxn−1|n−1 + Dn−1|n−1cnxD
j
,
j = 0, . . . , nx + 1
(10.40)

THE SPHERICAL SIMPLEX KALMAN FILTER PROCESS
145
with cnxD
j
deﬁned by (10.34)–(10.36). In a manner similar to that of the UKF, we
obtain the remaining predictive equations, with the state covariance given by
Pxx
n|n−1 =

˜g (c) N (c; 0, I) dc =
nx+1

j=0
wj˜g

c(j)
=
nx+1

j=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1

×

f

χ(j)
n−1|n−1

−ˆxn|n−1
⊺
+ Q
(10.41)
and the observation prediction equations
ˆzn|n−1 =
nx+1

j=0
wjh

χ(j)
n|n−1

(10.42)
Pzz
n|n−1 =
nx+1

j=0
wj

h

χ(j)
n|n−1

−ˆzn|n−1

(10.43)
×

h

χ(j)
n|n−1

−ˆzn|n−1
⊺
+ R
(10.44)
Pxz
n|n−1 =
nx+1

j=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1

×

h

χ(j)
n|n−1

−ˆzn|n−1
⊺
(10.45)
where
χ(j)
n|n−1 = ˆxn|n−1 + Dn|n−1cnxD
j
,
j = 0, . . . , nx + 1
(10.46)
10.5
THE SPHERICAL SIMPLEX KALMAN FILTER PROCESS
The SSKF process is summarized in Table 10.1. Note that the only difference between
Tables 10.1, 8.2 and 9.1 is the upper summation limit. Because of this similarity in
the process for all sigma point Kalman ﬁlters, in a later chapter we will combine all
sigma point Kalman ﬁlter processes into a single, more general, process.

146
THE SIGMA POINT CLASS
TABLE 10.1
Multidimensional Spherical Simplex Kalman Filter Process
Step 1.
Filter initialization:
Set 0 ≤w0 < 1
Initialize ˆx0 and Pxx
0
Step 2.
State vector
prediction:
χ(j)
n−1|n−1 = Dn−1|n−1cnxD
j
, j = 0, . . . , nx + 1
ˆxn|n−1 = nx+1
j=0 wjf 
χ(j)
n−1|n−1

Pxx
n|n−1 = nx+1
j=0 wj

f 
χ(j)
n−1|n−1

−ˆxn|n−1

×
f 
χ(j)
n−1|n−1

−ˆxn|n−1
⊺+ Q
Step 3.
Observation-
related
prediction:
χ(j)
n|n−1 = ˆxn|n−1 + Dn|n−1cnxD
j
, j = 0, . . . , 2nx
ˆzn|n−1 = nx+1
j=0 wjh 
χ(j)
n|n−1

Pzz
n|n−1 = nx+1
j=0 wj

h 
χ(j)
n|n−1

−ˆzn|n−1


h 
χ(j)
n|n−1

−ˆzn|n−1
⊺+ R
Pxz
n|n−1 = nx+1
j=0 wj

f 
χ(j)
n−1|n−1

−ˆxn|n−1

×
h 
χ(j)
n|n−1

−ˆzn|n−1
⊺
Step 4.
Kalman ﬁlter
update:
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.
10.6
APPLICATION OF THE SSKF TO THE DIFAR SHIP TRACKING
CASE STUDY
Since the dynamic equation is linear for the DIFAR tracking problem, the SSKF
process used for this problem is identical to that shown in Table 9.2, except for the
upper limit of the summation.
For this DIFAR case, nx = 4 and we let c(j) →c4D
j . Since we include the sigma
point at the origin, for the SSKF there are six sigma points. For the sigma points, the

REFERENCE
147
values used for {(wj, c4D
j ), j = 0, 1, . . . , 5} are given in (10.34)–(10.37), where we
let w0 = 0.25. We used the same initialization and dynamic noise q values used for
the EKF, given in Sections 7.4.3 and 7.4.4, respectively. The 100 Monte Carlo track
plots obtained from the SSKF for each of the signal SNR values used previously are
similar to those shown in Figure 7.3. With the exception of the number of divergent
tracks for an SNR of 5 dB (3 for the SSKF vice 2 for the EKF and UKF), there is little
discernible difference in the output of the SSKF from those of the EKF and SSKF, as
will be demonstrated when we discuss RMS errors later in this book.
REFERENCE
1. Julier SJ. The spherical simplex unscented transformation. Proc. Am. Control Conf.
2003:2430–2434.

11
THE SIGMA POINT CLASS: THE
GAUSS–HERMITE KALMAN FILTER
Practical methods for one-dimensional Gauss–Hermite integration has its origins in
the work of Wilf [1], as noted in the Numerical Recipes book [2], and the short but
excellent paper by Ball [3]. Unlike the methods addressed in the preceding chapters,
Gauss–Hermite integration is a quadrature method that uses orthogonal polynomials
instead of simple polynomials.
Consider the general multidimensional integral (5.60), rewritten here in more
explicit form
I
˜f

=
1
(2π)nx/2

f (c) e−c⊺c/2dc
(11.1)
Changing variables to remove the factor of 1/2 from the exponent, let
q ≜
c
√
2
=
1
√
2
D−1 (x −ˆx)
(11.2)
or
x = ˆx +
√
2Dq
(11.3)
Thus, (11.1) becomes
I
˜f

=
√
2
(2π)nx/2

f (q) e−q⊺qdq
(11.4)
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
148

ONE-DIMENSIONAL GAUSS–HERMITE QUADRATURE
149
with
f (q) ≜f

ˆx +
√
2Dq

(11.5)
11.1
ONE-DIMENSIONAL GAUSS–HERMITE QUADRATURE
The objective now is to expand f(q) in terms of orthogonal polynomials in some way.
First, consider the one-dimensional case integral of the form
I =

f (q) e−q2dq
(11.6)
Expanding f (q)inageneralsetoforthogonalpolynomials, f (q)canbeapproximated
by
f (q) ≃
m

l=0
plφl (q)
(11.7)
where {φl (q) , l = 0, 1, . . . , m} represent a set of orthogonal polynomials, with the
polynomials φl (q) and φk (q) orthogonal with respect to a weight (or kernel) function
p (q) over the interval [a, b] such that
 b
a
p (q) φl (q) φk (q) dq = clδlk
(11.8)
Here, cl is a normalization constant and
δlk =

1, k = l
0, k /= l
(11.9)
For integrals of the form (11.6), the Hermite polynomials Hl (q) can be used, with
the weight function given by
p (q) = e−q2
(11.10)
with the integration interval becoming [a, b] →[−∞, ∞]. The orthogonality
condition (11.8) becomes
 ∞
−∞
e−q2Hl (q) Hk (q) dq = hlδlk
(11.11)
where the normalization constant is given by
hl = √π2ll!
(11.12)

150
THE SIGMA POINT CLASS
The Hermite polynomials can be generated using the recursion relationships [3]
H−1 (q) = 0
(11.13)
H0 (q) = 1
(11.14)
Hl+1 (q) = 2qHl (q) −2lHl−1 (q) , l ≥0
(11.15)
Deﬁning the normalized Hermite polynomial as
˜Hl (q) ≜Hl (q)
√hl
(11.16)
leads to the orthonormality condition
 ∞
−∞
e−q2 ˜Hl (q) ˜Hk (q) dq = δlk
(11.17)
The recursion relationship for the orthonormal Hermite polynomials follows imme-
diately
˜H−1 (q) = 0
(11.18)
˜H0 (q) = h−1/2
0
= π−1/4
(11.19)
˜Hl+1 (q) = q
	
2
l + 1
˜Hl (q) −
	
l
l + 1
˜Hl−1 (q) , l ≥0
(11.20)
If we deﬁne
βl ≜

l/2
(11.21)
(11.20) becomes
q ˜Hl (q) = βl ˜Hl−1 (q) + βl+1 ˜Hl+1 (q)
(11.22)
Deﬁning the vectors h (q) and em and the matrix Jm as
h (q) ≜
 ˜H0 (q) , ˜H1 (q) , . . . , ˜Hm−1 (q)
⊺
(11.23)
em ≜[0, 0, . . . , 0, 1]⊺
(11.24)
Jm ≜


0
β1
· · ·
0
β1
0
...
...
...
βm−1
0
· · ·
βm−1
0


(11.25)
the orthogonal Hermite recursion relationship (11.22) can be rewritten as the
vector–matrix equation
qh (q) = Jmh (q) + βm ˜Hm (q) el
(11.26)

ONE-DIMENSIONAL GAUSS–HERMITE QUADRATURE
151
Setting
˜Hm (q) = 0
(11.27)
(11.26) becomes the eigenvalue equation

Jm −qI

h (q) = 0
(11.28)
The m distinct eigenvalues of Jm, given by {ql, l = 0, 1, . . . , m −1}, are the
interpolation points for the mth-order expansion of f (q). It can be shown that the error
in interpolation of f (q) at the distinct eigenvalue points {ql, l = 0, 1, . . . , m −1} is
zero [2]. Because the orthonormality condition (11.17) includes a weight function,
the eigenvalues can be written as the set given by the normalized solution to (11.27)
ql =
˜Hm (ql)
p (ql) , l = 0, 1, . . . , m −1
(11.29)
where p (ql) is given by (11.10).
Consider a third-order polynomial with m = 3. Although higher order polynomials
have been developed for the one-dimensional case (see, for example, [4,5]), higher
order methods quickly become unruly when applied to multidimensional cases. For
m = 3, (11.25) becomes
J3 =


0
β1
0
β1
0
β2
0
β2
0

=


0
1/
√
2
0
1/
√
2
0
1
0
1
0


(11.30)
The eigenvalues of (11.30) consist of the set
{q0, q1, q2} =
√
3
2 {−1, 0, 1}
(11.31)
Letting r ≜{0, 1, −1}, (11.31) can be rearranged into the set
{q0, q1, q2} =
√
3
2 r
(11.32)
The function f (q) can now be approximated by a third-order expansion in terms
of the normalized Hermite polynomials
f (q) ≃
2

l=0
al ˜Hl (q)
(11.33)

152
THE SIGMA POINT CLASS
Since the expansion of f (q) is exact at the eigenvalue points, writing (11.33) for each
point results in the system of moment equations
f (q0) ≃
2

l=0
al ˜Hl (q0)
(11.34)
f (q1) ≃
2

l=0
al ˜Hl (q1)
(11.35)
f (q2) ≃
2

l=0
al ˜Hl (q2)
(11.36)
These three equations in the three unknowns {a0, a1, a2} can now be solved for the
unknown coefﬁcients. Deﬁning the vectors 
f and a and the matrix ˜H as
f ≜

f (q0) , f (q1) , f (q2)
⊺
(11.37)
a ≜[a0, a1, a2]⊺
(11.38)
˜H ≜


˜H0 (q0)
˜H1 (q0)
˜H2 (q0)
˜H0 (q1)
˜H1 (q1)
˜H2 (q1)
˜H0 (q2)
˜H1 (q2)
˜H2 (q2)


(11.39)
thesystemofequations(11.34)–(11.36)canberewrittenasthevector–matrixequation
f = ˜Ha →a = ˜H−1f
(11.40)
Using (11.12)–(11.16) along with (11.32), we ﬁnd that
˜H =
1
π1/4


1
0
−1/
√
2
1
√
3
√
2
1
−
√
3
√
2


(11.41)
The inverse of ˜H is given by
˜H−1 = π1/4


2/3
1/6
1/6
0
1/2
√
3
−1/2
√
3
−
√
2/3
1/3
√
2
1/3
√
2


(11.42)
Using (11.42) in (11.40) results in


a0
a1
a2

= π1/4


2/3
1/6
1/6
0
1/2
√
3
−1/2
√
3
−
√
2/3
1/3
√
2
1/3
√
2




f (0)
f
 √
3
2

f

−
√
3
2



(11.43)

ONE-DIMENSIONAL GAUSS–HERMITE KALMAN FILTER
153
Solving (11.43) for {a0, a1, a2} yields
a0 = π1/4

2
3
f (0) + 1
6
f
	
3
2

+ 1
6
f

−
	
3
2

(11.44)
a1 = π1/4
2
√
3

f
	
3
2

−f

−
	
3
2

(11.45)
a2 = π1/4
3
√
2

f
	
3
2

−2 f (0) + f

−
	
3
2

(11.46)
Keeping only the ﬁrst term in the Hermite expansion of f (q) , (11.33) becomes
f (q) = a0 ˜H0 (q)
(11.47)
or
f (q) = 2
3
f (0) + 1
6
f
	
3
2

+ 1
6
f

−
	
3
2

(11.48)
To avoid confusion when this is applied to multiple dimensions, deﬁne the
one-dimensional set of interpolation points and their weights as

q(0), q(1), q(2)
=

0,
	
3
2, −
	
3
2

(11.49)
and
{p0, p1, p2} =
2
3, 1
6, 1
6

(11.50)
Now, the Hermite polynomial expansion (11.48) becomes
f (q) ≃
2

i=0
pi f

q(i)
(11.51)
with q(i) and pi given by (11.49) and (11.50), respectively.
11.2
ONE-DIMENSIONAL GAUSS–HERMITE KALMAN FILTER
For one dimension, (11.4) becomes
I

f

=
1
√π

f (q) e−q2dq
(11.52)

154
THE SIGMA POINT CLASS
where
q =
c
√
2
=
1
√
2
σ−1 (x −ˆx)
(11.53)
or
x = ˆx +
√
2σq
(11.54)
Using (11.49) in (11.54) yields the one-dimensional Gauss–Hermite sigma points
χ(i) = ˆx +
√
2σq(i)
(11.55)
In addition, we can write
f

q(i)
= f

ˆx +
√
2σq(i)
= f

χ(i)
(11.56)
and (11.52) now becomes
I

f

=
2

i=0
pif

χ(i) 1
√π

e−q2dq
=
2

i=0
pif

χ(i)
1
√
2π

e−c2/2dc
=
2

i=0
pif

χ(i) 
N (c; 0, 1) dc
(11.57)
and ﬁnally, because the integral over the density function equals one, (11.57)
reduces to
I

f

=
2

i=0
pif

χ(i)
(11.58)
Using (11.58), the one-dimensional state prediction equation (5.51) now becomes
ˆxn|n−1 =
2

i=0
pif

χ(i)
n−1|n−1

(11.59)
where
χ(i)
n−1|n−1 = ˆxn−1|n−1 +
√
3σxx,n−1|n−1r(i)
(11.60)
with
r ≜[1] ∈R1 =





0,
i = 0
1,
i = 1
−1, i = 2
(11.61)

MULTIDIMENSIONAL GAUSS–HERMITE KALMAN FILTER
155
For the one-dimensional state variance prediction, if we let
˜g (c) ≜

f (c) −ˆxn|n−1
2
(11.62)
the one-dimensional state variance prediction (7.2) becomes
σ2
xx,n|n−1 =

˜g (c) N (c; 0, 1) dc + σ2
v,n
(11.63)
Noting that the integral in (11.63) is of the same form as (11.6), it follows immediately
that
σ2
xx,n|n−1 =
2

i=0
pi

f

χ(i)
n−1|n−1

−ˆxn|n−1
2
+ σ2
v,n
(11.64)
In a similar fashion, (5.51)–(5.52) reduce to the prediction equations
ˆzn|n−1 =
2

i=0
pih

χ(i)
n|n−1

(11.65)
σ2
zz,n|n−1 =
2

i=0
pi

f

χ(i)
n|n−1

−ˆxn|n−1
2
+ σ2
w,n
(11.66)
and
σ2
xz,n|n−1 =
2

i=0
pi

f

χ(i)
n|n−1

−ˆxn|n−1
 
h

χ(i)
n|n−1

−ˆzn|n−1

(11.67)
with
χ(i)
n|n−1 = ˆxn|n−1 +
√
3σxx,n|n−1r(i)
(11.68)
(11.59) and (11.64)–(11.67) make up the prediction equations for the one-
dimensional Gauss–Hermite Kalman ﬁlter. The process ﬂow for the one-dimensional
case will not be given here but can be treated as a special case of the multidimensional
case presented in the next section.
11.3
MULTIDIMENSIONAL GAUSS–HERMITE KALMAN FILTER
Constructing multidimensional orthogonal polynomial integration rules can be a difﬁ-
cult task if one ﬁrst attempts to construct interpolative multidimensional polynomials.

156
THE SIGMA POINT CLASS
The simplest method of constructing multidimensional cubature rules is to form prod-
uct rules from one-dimensional integration rules. Consider ﬁrst a two-dimensional
case where q =

q1, q2
⊺and we expand f(q) as the double sum
f (q) =
2

i1=0
2

i2=0
pi1pi2f

q(i1)
1 , q(i2)
2

=
32−1

j=0
wjf

q(j)
(11.69)
where pi1 and pi2 are deﬁned by (11.50) and q(i1)
1
and q(i2)
2
are deﬁned by (11.49).
The double sum in (11.69) can be converted into the single sum indexed over
{j = 0, 1, . . . , 8} by forming the weights
w0 =
"2
3
#2
= p2
0
(11.70)
w1 = w2 = w3 = w4 =
"2
3
# "1
6
#
= p0pi = pip0, i = 1, 2
(11.71)
w5 = w6 = w7 = w8 =
"1
6
#2
= pipl = plpi, i, l = 1, 2
(11.72)
Putting this into a more compact form, wj is deﬁned by
wj ≜






















2
3
2
,
j = 0

2
3
 
1
6

,
j = 1, . . . , 4

1
6
2
,
j = 5, . . . , 8
(11.73)
From (11.50) it follows that
q(j) =
	
3
2r(j)
(11.74)

MULTIDIMENSIONAL GAUSS–HERMITE KALMAN FILTER
157
where
r(j) =













































[0] ∈R2 = [0, 0]⊺,
j = 0
[1] ∈R2 =









[1, 0]⊺,
[0, 1]⊺,
[−1, 0]⊺,
[0, −1]⊺,
j = 1
j = 2
j = 3
j = 4
[1, 1] ∈R2 =









[1, 1]⊺,
[−1, 1]⊺,
[1, −1]⊺,
[−1, −1]⊺,
j = 5
j = 6
j = 7
j = 8
(11.75)
In a similar manner, for a state vector of four dimensions, the expansion of f(q)
is given by
f (q) =
2

i1=0
2

i2=0
2

i3=0
2

i4=0
pi1pi2pi3pi4f

q(i1)
1 , q(i2)
2 , q(i3)
1 , q(i4)
2

=
34−1

j=0
wjf

q(j)
(11.76)
Again, combining the products of p in the same way in which we obtained (11.73)
results in a set of weights
wj =






























2
3
4
,
j = 0

2
3
3 
1
6

,
j = 1, . . . , 8

2
3
2 
1
6
2
,
j = 9, . . . , 32

2
3
 
1
6
3
,
j = 33, . . . , 64

1
6
4
,
j = 65, . . . , 80
(11.77)
From (11.50) it follows that
q(j) =
	
3
2r(j)
(11.78)

158
THE SIGMA POINT CLASS
where r(j) is given by the generators
r(j) =















[0] ∈R4,
j = 0
[1] ∈R4,
j = 1, . . . , 8
[1, 1] ∈R4,
j = 9, . . . , 32
[1, 1, 1] ∈R4,
j = 33, . . . , 64
[1, 1, 1, 1] ∈R4,
j = 65, . . . , 80
(11.79)
This can be generalized to the general nx-dimensional case with
f (q) =
3nx−1

j=0
wjf

q(j)
(11.80)
where q(j) is given by (11.78) with
r(j) =















































[0] ∈Rn,
j = 0
[1] ∈Rn,
j = 1, . . . , 2n
[1, 1] ∈Rn,
j = 2n + 1, . . . , 2

n + n(2)
[1, 1, 1] ∈Rn,
j =

2

n + n(2)
+ 1

, . . . ,

2

n + n(2)
+ 23n(3)

...
[1, 1, . . . , 1] ∈Rn,
j =

2

n + n(2)
+ $n−2
i=3 2in(i)

+

1, . . . , 2n−1n(n−1)

[1, 1, 1, . . . , 1] ∈Rn,
j =

2

n + n(2)
+ $n−1
i=3 2in(i)

+

1, . . . , 2nn(n)

(11.81)
Here, nx has been replaced by n for convenience and we have used the deﬁnitions
(2.24) and (2.25) for n(i) and n(i), respectively. Also, for the last generator, there are

MULTIDIMENSIONAL GAUSS–HERMITE KALMAN FILTER
159
n copies of ±1. The weights follow immediately as
wj =




























































2
3
n
,
j = 0

2
3
n−1 
1
6

,
j = 1, . . . , 2n

2
3
n−2 
1
6
2
,
j = 2n + 1, . . . , 2

n + n(2)
...

2
3
 
1
6
n−1
,
j =

2

n + n(2)
+ $n−2
i=3 2in(i)

+

1, . . . , 2n−1n(n−1)


1
6
n
,
j =

2

n + n(2)
+ $n−1
i=3 2in(i)

+

1, . . . , 2nn(n)

(11.82)
Now, inserting the general Hermite polynomial expansion of f(q) (11.80) into the
moment integral (11.4) yields
I
˜f

=
√
2
(2π)nx/2

3nx−1

j=0
wjf

q(j)
e−q⊺qdq
(11.83)
Changing variables back to an integral over c, (11.83) becomes
I
˜f

=
3nx−1

j=0
wjf

q(j) 
N (c; 0, I) dc
=
3nx−1

j=0
wjf

q(j)
(11.84)
Converting f

q(j)
using (11.5), (11.84) becomes
I =
3nx−1

j=0
wjf

ˆx+
√
3Dr(j)
(11.85)
Letting
χ(j)
n−1|n−1 ≜ˆxn−1|n−1+
√
3Dn−1|n−1r(j)
(11.86)

160
THE SIGMA POINT CLASS
and using (11.85) to evaluate (5.51)–(5.52) results in the general Gauss–Hermite state
prediction equations
ˆxn|n−1 =
3nx−1

j=0
wjf

χ(j)
n−1|n−1

(11.87)
and
Pxx
n|n−1 =
3nx−1

j=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1

×

f

χ(j)
n−1|n−1

−ˆxn|n−1
⊺
+ Q
(11.88)
Similarly, deﬁning
χ(j)
n|n−1 ≜ˆxn|n−1+
√
3Dn|n−1r(j)
(11.89)
the observation prediction equations (5.55)–(5.57) can be written as
ˆzn|n−1 =
3nx−1

j=0
wjh

χ(j)
n|n−1

(11.90)
Pzz
n|n−1 =
3nx−1

j=0
wj

h

χ(j)
n|n−1

−ˆxn|n−1

(11.91)
×

h

χ(j)
n|n−1

−ˆxn|n−1
⊺
+ R
(11.92)
Pxz
n|n−1 =
3nx−1

j=0
wj

f

χ(j)
n|n−1

−ˆxn|n−1

(11.93)
×

h

χ(j)
n|n−1

−ˆxn|n−1
⊺
(11.94)
Note that the Gauss–Hermite prediction equations are identical to those of the
UKF and SSKF, as well as the second form of the FDKF, except for the limits of the
summation. We will summarize these sigma point Kalman ﬁlters and present their
process diagram in Chapter 13.
The GHKF process is summarized in Table 11.1.
11.4
SPARSE GRID APPROXIMATION FOR HIGH DIMENSION/HIGH
POLYNOMIAL ORDER
As noted above, the number of grid points needed for the Gauss–Hermite ﬁlter is
given by
Ngh = mnx
(11.95)

SPARSE GRID APPROXIMATION FOR HIGH DIMENSION/HIGH POLYNOMIAL ORDER
161
TABLE 11.1
Multidimensional Gauss–Hermite Kalman Filter Process
Step 1.
Filter initialization:
Initialize ˆx0 and Pxx
0
Step 2.
State vector
prediction:
χ(j)
n−1|n−1 = ˆxn|n−1
+
√
3Dn−1|n−1r(j), j = 0, 1, . . . , 3nx −1
ˆxn|n−1 = $3nx −1
j=0
wjf 
χ(j)
n−1|n−1

Pxx
n|n−1 = $3nx −1
j=0
wj

f 
χ(j)
n−1|n−1

−ˆxn|n−1

×
f 
χ(j)
n−1|n−1

−ˆxn|n−1
⊺+ Q
Step 3.
Observation
related
prediction:
χ(j)
n|n−1 = ˆxn|n−1
+
√
3Dn|n−1r(j), j = 0, 1, . . . , 3nx −1
ˆzn|n−1 = $3nx −1
j=0
wj ˜h 
χ(j)
n|n−1

Pzz
n|n−1 = $3nx −1
j=0
wj

h 
χ(j)
n|n−1

−ˆzn|n−1


h 
χ(j)
n|n−1

−ˆzn|n−1
⊺+ R
Pxz
n|n−1 = $3nx −1
j=0
wj

f 
χ(j)
n−1|n−1

−ˆxn|n−1

×
h 
χ(j)
n|n−1

−ˆzn|n−1
⊺
Step 4.
Kalman Filter
Update:
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Step 6.
Return to Step 2.
where m is the order of the Hermite polynomial used in the approximation and nx
is the dimension of the state vector. As the dimension of the state vector increases,
the number of Sigma points required grows exponentially resulting in the “curse of
dimensionality.” The fully populated square grid of Sigma points becomes so large
at dimensions above 4 that the GHKF is rarely used. In the previous section of this
chapter, we considered Hermite expansions of second order (m = 3) that required 3nx
sigma points. The only way to reduce the number of points for this case is to use the
UKF instead of the GHKF at a small cost in accuracy.

162
THE SIGMA POINT CLASS
As noted just before (11.30), the Hermite interpolation method can be expanded to
higher order. Of course, higher order interpolation requires more sigma points making
the curse of dimensionality even worse. Because higher order Hermite interpolation
requires the solution to polynomial equations of the same order as the Hermite poly-
nomial, numerical methods are generally used to generate the eigenvalues, resulting
in weights and sigma points that are irrational numbers vice the rational numbers
shown in (11.48). It is these irrational numbers that are tabulated in such books as
Ref. [7]. However, care must be taken when using the tabulated data since the weights
are generated for the unnormalized Hermite polynomials.
Recent advances in multidimensional quadrature integration, based on the little
known work of Smolyak [8], has made higher order Gauss–Hermite integration more
manageable. Smolyak formulated a numerical method for multidimensional quadra-
ture integration in a way that leads to a reduction of the number of grid points at the
expense of accuracy. For a complete introduction to the mathematics of this grid point
reduction method, the reader is referred to Refs [9–13]. Our presentation of the gener-
ation of the Hermite sigma points presented above conforms to the Smolyak method
where the Sigma points are grouped hierarchically according to their distance from
the origin, as can be seen for the general dimension case in (11.81). For sparse-grid
numerical integration, one merely has to take a subset of the full grid and renormalize
the reduced weight set. Unfortunately, this sparse-grid integration method does not
reduce the number of required sigma points for Hermite polynomials of second order.
For example, one could take just the two lowest level of grid points, for example,
r(0) = [0] ∈Rnx, j = 0 and r(j) = [1] ∈Rnx, j = 1, . . . , nx. The original weights
are therefore w0 = (2/3)nx and wj = (2/3)nx−1 (1/6) , j = 1, . . . , nx. The weights
must be renormalized by dividing all the weights by the sum of the weights, which
is given by (2/3)nx−1 ([4 + nx]/6) . It follows immediately that the renormalized
weights are w0 = 4/ (4 + nn) and wj = 1/ (4 + nn) , j = 1, . . . , nx. This results in
a sparse Gauss–Hermite ﬁlter that is identical in form to the UKF, (9.24) and (9.25),
but with
c(j) = qr(j) =
	
3
2r(j), j = 0, 1, . . . , 2nx
(11.96)
wj =

4/ (4 + n) ,
j = 0
1
4w0,
j = 1, . . . , 2nx
(11.97)
In a similar manner, one can take as many of the Sigma grid point subsets desired
from (11.81) and after renormalizing the weights, create alternate sparse-grid GHKF
ﬁlters. This method will allow the sparse-grid GHKF to be used for higher dimensional
cases. In addition, for highly nonlinear cases, higher order Hermite polynomials can
be used to develop higher order sparse-grid GHKF ﬁlters that give greater accuracy
at a reasonable computational cost. Jia et al. [14] are the ﬁrst (to my knowledge) to
utilize this method in an application for spacecraft attitude estimation. In their paper,
they have tabulated the number of points required for different levels of sparseness
for GHKF ﬁlters up to seventh order for the Hermite polynomial and state vector
dimensions up to 6.

REFERENCES
163
11.5
APPLICATION OF THE GHKF TO THE DIFAR SHIP TRACKING
CASE STUDY
Since the dynamic equation is linear for the DIFAR tracking problem, the GHKF
process used for this problem is identical to that shown in Table 9.2. For the weights
and sigma points, the values used for {

wj, c(j)
, j = 0, 1, . . . , nx} are given in
(11.77)–(11.79), where we let c(j) =
√
3r(j). We used the same initialization and dy-
namic noise q values used for the EKF, given in Sections 7.4.3 and 7.4.4, respectively.
The 100 Monte Carlo track plots obtained for the GHKF for each of the signal SNR
values used previously showed little discernible difference relative to those from the
EKF and will therefore not be presented. Even the number of divergent tracks were
identical to those of the EKF Monte Carlo tracks.
REFERENCES
1. Wilf HS. Mathematics for the Physical Sciences. New York: Wiley; 1962, p. 54.
2. Press WH, et. al. Numerical Recipes in C. Cambridge, UK: Cambridge University Press;
1992.
3. Ball JS. Orthogonal polynomials, Gaussian quadratures and PDEs. Comput. Sci. & Eng.
1999:92–95.
4. Liu Q, J¨ackel P. A Note on multivariate Gauss–Hermite quadrature. Biometrika
1994;81(3):624–629.
5. Nia VP. Gauss–Hermite quadrature: Numer. Stat. Method 2006; Online:vahid.probstat.
ch/paper/ghq.pdf.
6. Arasaratnam I, Haykin S, Elliot RJ. Discrete-time nonlinear ﬁltering algorithms using
Gauss–Hermite quadrature. Proc. IEEE, 2007;95(5):953–977.
7. Abramowitz M, Stegun IA. Handbook of Mathematical Functions. Dover Publications
1972.
8. Smolyak SA. Quadrature and interpolation formulas for tensor products of certain classes
of functions. Dokl. Akad. Nauk SSSR 1963;4:240–243.
9. Wasilkowski GW, Wo´zniakowski H. Explicit cost bounds of algorithms for multivariate
tensor product problems. J. Complex. 1994;11(1):1–56.
10. Gerstner T, Griebel M. Numerical integration using sparse grids. Numer. Algorithms,
1998;18(4):209–232.
11. Gerstner T, Griebel M. Dimension-adaptive tensor-product quadrature. Computing
2003;71:65–87
12. Heiss F, Winschel V. Estimation with Numerical Integration on Sparse Grids. Discussion
Papers in Economics 916; 2006.
13. Heiss F, Winschel V. Likelihood approximation by numerical integration on sparse grids.
J. Econ. 2008;144:62–80.
14. Jia B, Xin M, Cheng Y. Sparse Gauss–Hermite quadrature ﬁlter for spacecraft attitude
estimation. Am. Control Conf., 2010;ThA15.4:2873–2878

12
THE MONTE CARLO KALMAN FILTER
In the sigma point Kalman ﬁlters, after an afﬁne transformation of the state vector, the
nonlinear functions were expanded in a polynomial reducing the Gaussian-weighted
integrals to a set of moment equations that were solved for a set of weights and sigma
points. Although these sigma point methods have been referred to in the literature as
an approximation to the density function, no approximations for the Gaussian density
have been applied in our derivations.
However, a method is presented here where the Gaussian density is approximated
by a set of Monte Carlo samples from that density. Remember from (5.51) that
ˆxn|n−1 =

f (c) N (c; 0, I) dc
(12.1)
Suppose that we can generate a set of Monte Carlos sample

c(j), j = 1, . . . , Ns

from N (c; 0, I). Now we express N (c; 0, I) as the discrete approximation to the
density given by
N (c; 0, I) ≃1
Ns
Ns

j=1
δ

c −c(j)
(12.2)
where δ

c −c(j)	
represents the delta-Dirac mass located at c(j).
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
164

THE MONTE CARLO KALMAN FILTER
165
Substituting (12.2) into the state prediction Equation (5.51) yields
ˆxn|n−1 = 1
Ns
Ns

j=1

f (c) δ

c −c(j)
dc
= 1
Ns
Ns

j=1
f

c(j)
(12.3)
ItcanbeshownthatasNs →∞thisapproximationoftheintegral,givenbythesample
mean, becomes exact (for a proof, see the ﬁrst chapter in Ref. [1]). We must note that
the factor 1/Ns is the importance weight of each sample and not a probability. The
probability is given by the normalized density of samples in a given region represented
by a normalized histogram.
Using (5.53), (12.3) becomes
ˆxn|n−1 = 1
Ns
Ns

j=1
f

ˆxn−1|n−1 + Dn−1|n−1c(j)
(12.4)
Using our standard deﬁnition for χn−1|n−1 given by
χ(j)
n−1|n−1 = ˆxn−1|n−1 + Dn−1|n−1c(j)
(12.5)
and deﬁning wj ≜1/Ns, ∀j, we can rewrite (12.4) as
ˆxn|n−1 =
Ns

j=1
wjf

χ(j)
n−1|n−1

(12.6)
which has the same form as the state prediction equation for any of the sigma point
Kalman ﬁlters.
It follows immediately that the remaining prediction equations for the Monte Carlo
Kalman ﬁlter (MCKF) are given by the sigma point prediction equations
Pxx
n|n−1 =
Ns

j=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1

(12.7)
×

f

χ(j)
n−1|n−1

−ˆxn|n−1
⊺
+ Q
(12.8)
and
ˆzn|n−1 =
Ns

j=0
wjh

χ(j)
n|n−1

(12.9)
Pzz
n|n−1 =
Ns

j=0
wj

h

χ(j)
n|n−1

−ˆxn|n−1

(12.10)

166
THE MONTE CARLO KALMAN FILTER
×

h

χ(j)
n|n−1

−ˆxn|n−1
⊺
+ R
(12.11)
Pxz
n|n−1 =
Ns

j=0
wj

f

χ(j)
n−1|n−1

−ˆxn|n−1

(12.12)
×

h

χ(j)
n|n−1

−ˆxn|n−1
⊺
(12.13)
TABLE 12.1
Multidimensional Spherical Simplex Kalman Filter Process
Step 1.
Filter initialization:
Set 0 ≤w0 < 1
Initialize ˆx0 and Pxx
0
wj = 1/Ns, j = 1, . . . , Ns
Step 2.
Generate samples
c(j) ∼N (c; 0, I) , j = 1, . . . , Ns
State vector
prediction:
χ(j)
n−1|n−1 = ˆxn−1|n−1 + Dn−1|n−1c(j),
j = 1, . . . , Ns
ˆxn|n−1 = Ns
j=1 wjf 
χ(j)
n−1|n−1
	
Pxx
n|n−1 = Ns
j=0 wj

f 
χ(j)
n−1|n−1
	
−ˆxn|n−1

×
f 
χ(j)
n−1|n−1
	
−ˆxn|n−1
⊺+ Q
Step 3.
Generate samples
c(j) ∼N (c; 0, I) , j = 1, . . . , Ns
Observation-related
prediction:
χ(j)
n|n−1 = ˆxn|n−1 + Dn|n−1c(j), j = 1, . . . , Ns
ˆzn|n−1 = Ns
j=0 wj ˜h 
χ(j)
n|n−1
	
Pzz
n|n−1 = Ns
j=0 wj

h 
χ(j)
n|n−1
	
−ˆzn|n−1


h 
χ(j)
n|n−1
	
−ˆzn|n−1
⊺+ R
Pxz
n|n−1 = Ns
j=0 wj

f 
χ(j)
n−1|n−1
	
−ˆxn|n−1

×
h 
χ(j)
n|n−1
	
−ˆzn|n−1
⊺
Step 4.
Kalman ﬁlter
update:
Kn ≜Pxz
n|n−1

Pzz
n|n−1
	−1
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1
	
Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
Step 5.
Store results
Store ˆxn|n and Pxx
n|n to a Track File
Return to Step 2.

REFERENCE
167
In the later three prediction equations, χ(j)
n|n−1 is given by
χ(j)
n|n−1 = ˆxn|n−1 + Dn|n−1c(j)
(12.14)
where c(j) represents a new set of Monte Carlo samples drawn from N (c; 0, I). All
of these prediction equations are in fact nothing more than the sample means and
covariances of the nonlinear functions.
It must be noted that in this MCKF, the Monte Carlo samples are never reused. As
we will show in Part III, there are some estimation methods for non-Gaussian densities
were the Monte Carlo samples are reused over and over again, with just their weights
updated in each iteration. This is where the term particle ﬁlter originated, because
each Monte Carlo sample is considered a particle that propagates from one iteration
to the next.
12.1
THE MONTE CARLO KALMAN FILTER
The MCKF process is summarized in Table 12.1.
REFERENCE
1. Doucet A, deFreitas N, Gordon N, editors Sequential Monte Carlo Methods in Practice.
Springer; 2001.

13
SUMMARY OF GAUSSIAN KALMAN
FILTERS
In this chapter, we will summarize some of the more important features of the
Gaussian-based Kalman ﬁlters developed in Chapters 6–12. We ﬁrst examine the LKF
and EKF and present process ﬂow diagrams for each. Then we address the sigma point
class of Kalman ﬁlters, presenting a process ﬂow diagram that is common to all of
them. Two tables are then presented that deﬁne the sigma points and weights speciﬁc
to each ﬁlter. This is followed by an assessment of the performance characteristics of
each sigma point Kalman ﬁlter from the processing efﬁciency point of view.
13.1
ANALYTICAL KALMAN FILTERS
There are many dynamic systems where either the dynamic or observation equation
is linear. This is true for most of the case studies considered in this book.
For some cases, the dynamic equation is linear while the observation equation is
highly nonlinear. For these, the linear Kalman ﬁlter prediction Equations (6.3) and
(6.6) can always be used for the state and state covariance predictions in lieu of any of
the other prediction methods. For other tracking problems, the dynamic equation may
be nonlinear while the observation equation is linear. For these problems, the reverse
is true: one must use one of the nonlinear methods for state prediction, but the linear
Kalman ﬁlter predictions (6.9)–(6.12) can be used for observation prediction. Finally,
if both the dynamic and observation equations are linear, all of the LKF prediction
equations are used.
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
168

ANALYTICAL KALMAN FILTERS
169
n n
n n
n n
n n
n n
n n
zz
xx
xz
xx
z
Hx
P
HP
H
R
P
P
H
n
n n
n n
n n
n n
n
n n
n
n n
n n
n
n n
n
xz
zz
xx
xx
zz
K
P
P
x
x
K
z
z
P
P
K P
K
xx
x
P
n n
n n
xx
x
P
n n
n
n
n n
n
n
xx
xx
x
Fx
P
FP
F
Q
n
n
n
n
xx
x
P
n
z
Kalman filter: correction
Delay
Track file
Initialize
Observation/measurement prediction
State vector prediction
FIGURE 13.1
Graphical presentation of the linear Kalman ﬁlter process ﬂow.
A process diagram for the LKF is presented in Figure 13.1
When the dynamic and/or observation equation is nonlinear, one of the nonlinear
methods developed in Chapters 7–12 must be used. The method that has seen the
most use over the past 50 years is the EKF. As we have shown in Chapter 7, the EKF
involves an expansion of the nonlinear functions in a ﬁrst-order Taylor polynomial
resulting in a Kalman ﬁlter with the same form as the LKF, but with the added bur-
den of Jacobian computations. The process diagram for this method is presented in
Figure 13.2. In Chapter 7 we also developed a second-order extended Kalman ﬁl-
ter, but it required the additional computation of the Hessian matrix, usually a
difﬁcult task.
n
n n
n
n n
n
n n
n n
n n
n n
n n
n n
n n
n n
n n
x
x
x
zz
xx
xz
xx
H
h
x
z
h x
P
H
P
H
R
P
P
H
n
n n
n n
n n
n n
n
n n
n
n n
n n
n
n n
n
xz
zz
xx
xx
zz
K
P
P
x
x
K
z
z
P
P
K P
K
xx
x
P
n n
n n
xx
x
P
n
n
n
n
xx
x
P
n
n
n
n
n
n
n
n n
n
n
n
n
n
n
n
n n
n
n
x
x
x
xx
xx
F
f
x
x
f x
U
P
F
P
F
Q
n
z
Kalman filter: correction
Delay
Track file
Initialize
Observation/measurement prediction
State vector prediction
FIGURE 13.2
A graphical representation of the EKF process ﬂow.

170
SUMMARY OF GAUSSIAN KALMAN FILTERS
13.2
SIGMA POINT KALMAN FILTERS
In Chapter 8, we presented a method for replacing the Jacobian and Hessian matrices
with their central ﬁnite difference approximations resulting in several versions of the
FDKF. The covariance prediction equations for the standard FDKF, given by (8.41),
(8.43), and (8.47), differed in form from those of the remaining sigma point methods.
In addition, the modiﬁed version of the FDKF covariance prediction equations, (8.49),
(8.50), and (8.51), were shown to be identically to those of the UKF. For these reasons,
the FDKF will not be included in our summary of the sigma point Kalman ﬁlters.
The sigma point Kalman ﬁlters, UKF, SSKF, and GHKF, as well as the MCKF,
share a similar structure so that all follow the common process ﬂow shown in Figure
13.3, but differ in c(j) and wj. The values for c(j) and wj for each of the ﬁlters are
presented in Tables 13.1 and 13.2, respectively. It is relatively easy to write a general
subroutine for the sigma point Kalman ﬁlter, based on the process ﬂow shown in
Figure 13.3, that calls a second subroutine which selects the appropriate sigma points
and weights from Tables 13.1 and 13.2.
Some of the sigma point Kalman ﬁlters suffer from what is known as the curse of
dimensionality, where the number of integration points increase exponentially with
the dimension of the state vector. The number of integration points required by the
various ﬁlters is shown in Table 13.3. One can see that for state vectors with a large
xx
x
P
s
s
j
j
n n
n
n
N
j
n n
j
n n
j
N
j
j
n n
n n
n n
j
n n
n n
j
w
w
xx
x
P
s
s
s
j
j
n n
n n
N
j
n n
j
n n
j
N
j
j
n n
n n
n n
j
n n
n n
j
N
j
j
n n
n n
n n
j
n n
n n
j
w
w
w
zz
xz
h
z
z
P
R
z
P
n
n n
n n
n n
n n
n n
n
n
n n
n n
n
n n
n
xz
zz
xx
xx
zz
K
P
P
x
x
K
z
z
P
P
K P
K
n n
n n
xx
x
P
j
j
n
n
n
n
n
n
j
w
n
n
n
n
xx
x
P
j
n
n
j
w
n
z
Initialize
State vector prediction
Observation prediction
Kalman filter: correction
Delay
Track file
Generate samples
Observation
FIGURE 13.3
Process ﬂow for general sigma point Kalman ﬁlter.

SIGMA POINT KALMAN FILTERS
171
TABLE 13.1
Summary Data for Sigma Point Kalman Filters: Part 1—Form
of c to Be Used for Sigma Points
Kalman Filter Method
Form of c(j)
MCKF
c(j) ∼N (c; 0, I) , j = 1, . . . , Ns
UKF
c(j) = 
nx
1−w0 r(j), j = 0, 1, . . . , 2nx
r(j) =
 [0] ∈Rnx, j = 0
[1] ∈Rnx, j = 1, 2, . . . , 2nx
SSKF
c(j)
Rnx =



























[0] ∈Rnx, j = 0


c(j)
Rnx−1
−
1
√
nx(nx+1)w1

, j = 1, 2, . . . , nx


0 ∈Rnx−1
nx
√
nx(nx+1)w1

, j = nx + 1
GHKF
c(j) =
√
3r(j); see (11.81) for r(j)
number of dimensions, the best ﬁlter from a computational point of view would be the
SSKF, followed by the UKF. One would not want to use the GHKF for any problem
in which the dimension exceeded four. For highly nonlinear dynamics, the accuracy
of each ﬁlter is usually determined by the order of the polynomial expansion used
within the integration. Thus, the MCKF will usually give the most accurate results
since no approximation was used for the nonlinear function, as long as the number of
random samples is high enough. This is followed closely by the GHKF that uses the
highest polynomial order. The UKF will be ranked third due to its use of a third-order
polynomial. And ﬁnally, the least accurate will be the SSKF. Thus, it is sometimes best
TABLE 13.2
Summary Data for Sigma Point Kalman Filters:
Part 2—Form of Sigma Point Weights
Kalman Filter Method
Form of wj
MCKF
1/Ns
UKF



0 ≤w0 ≤1
wj = 1−w0
2nx , j = 1, . . . , 2nx
SSKF



0 ≤w0 ≤1
wj = 1−w0
nx+1 , j = 1, . . . , nx + 1
GHKF
See (11.82) for wj

172
SUMMARY OF GAUSSIAN KALMAN FILTERS
TABLE 13.3
Comparison of the Number of Integration Points Required for the
Various Sigma Point Kalman Filters
Dimension of State Vector
Number of Sample Points
1
2
4
8
16
UKF
2nx + 1
3
5
9
17
33
SSKF
nx + 2
3
4
6
10
18
GHKF
3nx
3
9
81
6561
43,046,721
MCKF
Ns
Ns
Ns
Ns
Ns
Ns
to try all of these ﬁlters on a given estimation problem and choose the one that yields
the lowest RMS error within the problems computational constraints. An interesting
discussion of accuracy, efﬁciency, and stability factors for these and related ﬁlters can
be found in the paper by Wu et al. [1].
A simple set of Matlab subroutines that generate the set {wi, c(i), i = 1, . . . , Ns}
for all of the sigma point Kalman ﬁlters, and for any state vector dimension, are
presented in Listings 13.1 and 13.2. The ﬁrst subroutine computes the weights and
sigma points based on the formulas shown in Tables 13.1 and 13.2, respectively. It
requires four inputs, a ﬂag (t ﬂag) indicating which of the four sigma point Kalman
ﬁlters requires the sigma points and weights, the state vector dimension (L = nx),
the number of sigma (or sample) points, LL, computed from Table 13.3, and the
value Stream, which is the stream for the random number generator. The second
subroutine, written by Neill Bryant Beltran of The Johns Hopkins University Applied
Physics Laboratory, generates the Gauss–Hermite vector points. This function outputs
sigma points with length u and dimensionality n. To do this, a combination of binary
numbers and combinatorics is employed. First, all possible binary numbers with the
same number of digits as dimensions is created. Next, this list of binary numbers is
pruned such that the only remaining binary numbers have u “1”s. Next, a negative
mask is created that has all possible combinations of 1 and −1. Finally, the “shape
mask” and “negative mask” are combined to ﬁnd all possible combinations of Gauss–
Hermite vector points.
Listing 13.1 Generation of Weights and Sigma Points
function [w,c] = WeightsSigmaPoints(t_flag,L,LL,Stream)
w = zeros(1,LL);
switch tflag
case{1}
% UKF
w(1) = 0.25;
w(2:LL) = (1 - w(1))/(2*L);
r = zeros(2*L+1,L);
r(2:L+1,:) = eye(L);
r(L+2:2*L+1,:) = -eye(L);
c = sqrt(L/(1-w(1)))*r;
case{2}
% SSKF
w(1) = 0.25;

SIGMA POINT KALMAN FILTERS
173
w(2:LL) = (1 - w(1))/(L+1);
c = zeros(LL,L);
c(2,1) = 1/sqrt(2*w(2));
c(3,1) = -1/sqrt(2*w(2));
for n = 2:L
for q = 1:n
c(q+1,1:n) = [c(q+1,1:n-1) 1/...
sqrt(n*(n+1)*w(2))];
end
c(n+2,1:n) = [zeros(1,n-1) -n/...
sqrt(n*(n+1)*w(2))];
end
case{3}
% GHKF
fact1 = 2/3;
fact2 = 1/6;
r = zeros(LL,L);
w(1) = fact1ˆL;
q = 1;
for u = 1:L
rr = vectorPointGenerator(u,L);
k = size(rr,1);
r(q+1:q+k,:) = rr;
w(q+1:q+k) = (fact1ˆ(L-u))*(fact2ˆu);
q = q+k;
end
c = sqrt(3)*r;
case{4}
% MCKF
w = ones(1,LL)/LL;
c = randn(Stream,LL,L);
end
Listing 13.2 Vector Point Generator
function out = vectorPointGenerator(u, n)
% Written by
% Neill Bryant Beltran
% 8 November 2010
if u == 0
out = zeros(1, n);
return
end
% Find all binary numbers with same number
% of digits as there are dimensions, n
shape_mask = dec2bin(2ˆn-1:-1:1) == ‘1’;
shape_size = size(shape_mask);
% Remove all vectors that do not have u ‘1’s
for ii = shape_size(1):-1:1
if nnz(shape_mask(ii, :)) ˜= u
shape_mask(ii, :) = [];
end
end
shape_size = size(shape_mask);
% Create the negative mask

174
SUMMARY OF GAUSSIAN KALMAN FILTERS
neg_mask = (dec2bin(0:2ˆu-1) == ‘1’)*1.0;
neg_mask(neg_mask == 0) = -1;
neg_mask = -1*neg_mask;
neg_size = size(neg_mask);
out = zeros(shape_size(1)*neg_size(1), n);
% Combine the shape mask and negative mask
for ii = 1:neg_size(1)
for jj = 1:shape_size(1)
kk = 1;
for ll = 1:n
if ˜shape_mask(jj, ll)
continue
end
out((ii-1)*shape_size(1)+jj, ll) = ...
shape_mask(jj, ll)*neg_mask(ii, kk);
kk = kk + 1;
end
end
end
Additional methods for numerical evaluation of Gaussian-weighted integrals can
be found in Refs [2–7]. Many of the methods discussed in the referenced papers can
be applied in a manner similar to the sigma point Kalman ﬁlters developed above.
Because most of them are of higher order than the ones presented in Chapters 8–12,
they will require many more additional integration points than the UKF, with the
number of required points falling between those of the UKF and GHKF. A more
traditional approach to the development of sigma point Kalman ﬁlters can be found in
the excellent book by Candy [8], where regression methods are used that are similar
to those used in the original development of the Kalman ﬁlter.
13.3
A MORE PRACTICAL APPROACH TO UTILIZING THE FAMILY
OF KALMAN FILTERS
A close examination of the dynamic and observation models given by (3.17) and (3.18)
for the various case studies reveal that a hierarchy of models exists This hierarchy is
presented in Table 13.4.
Based on this model hierarchy, it is almost obvious that the dynamic prediction
process can be considered to be completely independent of the observation prediction
TABLE 13.4
Hierarchy of Dynamic and Observation Models
Combination
Dynamic Model
Observation Model
Linear/linear
xn = Fxn−1 + vn−1
zn = Hxn + wn
Linear/nonlinear
xn = Fxn−1 + vn−1
zn = f (xn) + wn
Nonlinear/linear
xn = f (xn−1) + vn−1
zn = Hxn + wn
Nonlinear/nonlinear
xn = f (xn−1) + vn−1
zn = f (xn) + wn

REFERENCES
175
process. This enables one to use the Kalman ﬁlter method that is most suited for each
model. For example, for the linear/nonlinear case, one can use the linear Kalman ﬁlter
equations for dynamic prediction and one of the nonlinear Kalman ﬁlter methods for
observation prediction. Similarly, for the nonlinear/nonlinear case, when the state
vector xn is of high dimension and the observation vector zn is of low dimension,
then one could use the UKF for dynamic prediction (since the GHKF is impractical)
but use the GHKF for observation prediction (since it is usually the most accurate).
This consideration reveals a ﬂexibility within the Kalman ﬁlter structure that can
be put to practical use in setting up the most efﬁcient estimation procedure for the
problem at hand. Of course, from a software implementation point of view, this may
not be the most efﬁcient approach.
REFERENCES
1. Wu Y, Hu D, Wu M, Hu X. A numerical-integration perspective on Gaussian ﬁlters. IEEE
Trans. Sig. Proc. 2006;54(8):2910–2921.
2. McNamee J, Stenger F. Construction of fully symmetric numerical integration formulas.
Numerische Math. 1967;10:327–344.
3. Haber S. Numerical evaluation of multiple integrals. SIAM Rev. 1970;12(4):481–526.
4. Dunavant D A. Efﬁcient symmetrical cubature rules for complete polynomials of higher
degree over the unit cube. Int. J. Numer. Methods Eng. 1986;23:397–407.
5. Genz A, Monahan J. A stochastic algorithm for higher-dimensional integrals over un-
bounded regions with Gaussian weights. J. Comput. Appl. Math. 1999;112:71–81.
6. Cools R. Advances in multidimensional integration. J. Comput. Appl. Math. 2002;149:1–12.
7. Lu J, Darmofal D L. Higher-dimensional integration with Gaussian weight for applications
in probabilistic design. SIAM J. Sci. Comput. 2004;26(2):613–624.
8. Candy JV. Bayesian Signal Processing: Classical, Modern, and Particle Filtering Methods,
Hoboken NJ: Wiley; 2009.

14
PERFORMANCE MEASURES FOR THE
FAMILY OF KALMAN FILTERS
In this chapter, we will address several general methods to measure the performance
of any of the Kalman ﬁlters developed in Chapters 6–12. The performance measures
fall into two classes. The ﬁrst class is applicable when ground truth tracks are not
available or where the number of ground truth tracks available for a given scenario
are not statistically signiﬁcant, such as in the analysis of a small number of ﬁeld
tests. For this case, only the ﬁlters estimated track covariance matrix can used as an
input to generate successive track error ellipses as a performance measure. On the
other hand, a larger class of performance measures are available when a simulation
is used to generate a completely known target track. This is true because a large
number of Monte Carlo observation data sets can be generated that allow for the use
of a variety of speciﬁc tracker performance measures, such as the root mean squared
(RMS) error, the Cramer–Rao lower bound (CRLB), and the percentage of divergent
tracks. In this chapter, all of these performance measures will be discussed and speciﬁc
implementation issues associated with each will be addressed.
14.1
ERROR ELLIPSES
In applications of track estimation ﬁlters when the true state is unknown, such as for
a ﬁelded system tracking a real ship, or when only a small number of ﬁeld tests are
conducted and the truth state is only approximately known, the best way to gauge the
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
176

ERROR ELLIPSES
177
performance of a tracking ﬁlter is to use the estimated covariance matrix output by
the ﬁlter to generate and plot error ellipses around the ﬁltered track state. For clarity
of presentation only the two-dimensional Cartesian coordinate system positions are
used due to the inherent two-dimensional nature of displays.
At any discrete time tn, the probability density function of the track state xn is
given by the Gaussian
xn ∼N

xn; ˆxn|n, Pxx
n|n

(14.1)
where (ˆxn|n, Pxx
n|n) are the output of the Kalman ﬁlter update equations in Cartesian
coordinates. Now, dropping the time subscript, examine only the two-dimensional
position components given by x = [rx, ry]⊺. Examination of Figure 2.9 shows that
the contour ellipses of constant probability (before the afﬁne transformation) are
generally elongated along the major axis and may also be skewed so that the axes
of the ellipse are not aligned with the axes associated with the state position vector.
The question answered by the procedures developed below is, given a containment
probability, how does one plot the associated error ellipses in the same coordinate
system as that of the state vector. The containment probabilities correspond to the
contours of constant probability.
14.1.1
The Canonical Ellipse
The exponent of the probability distribution for the position components can be rep-
resented by
s = (x −ˆx)⊺P−1 (x −ˆx)
(14.2)
where P is the error covariance matrix of the position components. Error ellipses are
deﬁned by
(x −ˆx)⊺P−1 (x −ˆx) ≤C2
(14.3)
where C2 is the containment area, which depends on the desired containment proba-
bility, Pc, as will be shown below.
First, we need to rewrite (14.3) in the standard form of the equation for an ellipse.
Performing an eigenvalue decomposition of P, we can write
P = VDV⊺
(14.4)
where D is a 2 × 2 diagonal matrix containing the eigenvalues of P
D ≜

λ1
0
0
λ2

(14.5)
and V is a 2 × 2 matrix constructed from the two eigenvectors of P, and is given by
V ≜[v1, v2]
(14.6)

178
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
Here, v1 and v2 are 2 × 1 column vectors. It is easy to show that for a symmetric,
nonsingular matrix P
P−1 = VD−1V⊺
Now (14.3) can be rewritten as
(x −ˆx)⊺VD−1V⊺(x −ˆx) ≤C2
(14.7)
Letting
y ≜V⊺(x −ˆx)
(14.8)
we can rewrite (14.7) as
y⊺D−1y = y2
1
λ1
+ y2
2
λ2
≤C2
(14.9)
Choosing λ1 as the largest eigenvalue, and deﬁning
a ≜C
√
λ1
(14.10)
b ≜C
√
λ2
(14.11)
and choosing the equality in (14.7), we obtain the canonical elliptical equation
y2
1
a2 + y2
2
b2 = 1
(14.12)
14.1.2
Determining the Eigenvalues of P
To determine the eigenvalues and eigenvectors of the symmetric matrix P, examine
the eigenvalue equation
Pv =λv
(14.13)
or
[P −λI] v = 0
(14.14)
The polynomial f (λ) = |P −λI| is called the characteristic polynomial of P. The
roots of f (λ) = 0 yield the eigenvalues λi
|P −λI| =

P11 −λ
P12
P12
P22 −λ
 = 0
(14.15)
which leads to a solutions
λ1 = P11 + P22
2
+ 1
2

(P11 −P22)2 + 4P2
12
(14.16)
λ2 = P11 + P22
2
−1
2

(P11 −P22)2 + 4P2
12
(14.17)

ERROR ELLIPSES
179
14.1.3
Determining the Error Ellipse Rotation Angle
For the eigenvector v1 corresponding to the eigenvalue λ1, (14.14) becomes

P11 −λ1
P12
P12
P22 −λ1

v1 = 0
(14.18)
Let
v1 ≜

ξ11
ξ12

(14.19)
v2 ≜

ξ21
ξ22

(14.20)
where

ξ11, ξ12

and

ξ21, ξ22

are the Cartesian components of v1 and v2, respec-
tively. Solving (14.18) for ξ12 in terms of ξ11 yields
ξ12 = −
P12
P22 −λ1
ξ11
(14.21)
The rotation angle of the ellipse relative to the Cartesian axes is depicted in
Figure 14.1. It follows immediately from the ﬁgure that the error ellipse rotation
angle is given by
ϕ = tan−1
ξ12
ξ11

= tan−1

−
P12
λ1 −P22

(14.22)
ξ1
ξ2






u1
u2
x1
x2
ϕ
FIGURE 14.1
Error ellipse rotation angle.

180
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
14.1.4
Determination of the Containment Area
Since the components of x are jointly Gaussian, as seen by (14.1), then the component
of y are also, so we can write
y ∼N (y; 0, D)
(14.23)
and the pdf of y can be written as
p (y) =
1
2π√λ1λ2
e
−
	
y2
1
λ1 +
y2
2
λ2

(14.24)
Thus, the containment probability can be expressed as
PC = P

y2
1
λ1
+ y2
2
λ2
≤C2

=

y2
1
λ1 +
y2
2
λ2 ≤C2
p (y) dy
(14.25)
Now, let
r2 ≜y2
1
λ1
+ y2
2
λ2
≤C2
(14.26)
which leads to
y1 =
√
λ1r cos ϕ
(14.27)
y2 =
√
λ2r sin ϕ
(14.28)
dy1dy2 =
√
λ1λ2rdrdϕ
(14.29)
The containment probability (14.25) now becomes
PC = 1
2π
 2π
0
dϕ
 C
0
re−1
2 r2dr
=
 C
0
re−1
2 r2dr
= 1 −e−1
2 C2
(14.30)
Solving for C results in
C =

−2 ln (1 −PC)
(14.31)
Thus, selecting a containment probability completely determines the value of C.

ERROR ELLIPSES
181
14.1.5
Parametric Plotting of Error Ellipse
A canonical ellipse (one in the form of (14.12)) can be expressed as the path of points
(xe(θ), ye(θ)), where
xe(θ) = xc + a cos θ cos ϕ −b sin θ sin ϕ
(14.32)
ye(θ) = yc + a cos θ sin ϕ + b sin θ cos ϕ
(14.33)
Here, (xc, yc) is the center of the ellipse, ϕ is the angle between the Cartesian x-axis
and the major axis of the ellipse, and the parametric vector θ is deﬁned as the row
vector θ ≜[0, 0.01, . . . , 2π].
Deﬁne the parametric matrix
Q (θ) =

q1 (θ)
q2 (θ)

=

cos θ
sin θ

(14.34)
From (14.10) and (14.11), we can identify

a
0
0
b

= C
 √λ1
0
0
√λ2

= CD1/2
(14.35)
and
V =

cos ϕ
−sin ϕ
sin ϕ
cos ϕ

=

ξ11
ξ21
ξ12
ξ22

(14.36)
The parametric equation for the path of the ellipse can now be written in vector–
matrix form as
xe(θ) =

xe(θ)
ye(θ)

=

ˆx
ˆy

+

ξ11C√λ1q1 (θ) + ξ12C√λ2q2 (θ)
ξ21C√λ1q1 (θ) + ξ22C√λ2q2 (θ)

(14.37)
where we have placed the center of the ellipse at (ˆx, ˆy). Or, we could express the
parametric equation as
xe(θ) = ˆx + VCD1/2Q (θ)
(14.38)
with ˆx ≜[ˆx, ˆy].
Remark 14.1 The eigenvalue and eigenvector matrices, V and D, respectively, can
be obtained using the Matlab function call [V,D] = eig(P).
A procedure for generating error ellipse data that can be plotted over a Cartesian
track estimate plot is as follows:

182
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
−2
−1.5
−1
−0.5
0
0.5
1
1.5
−4
−3
−2
−1
0
1
2
3
4
x-position (nmi)
y-position (nmi)
FIGURE 14.2
An example of error ellipses overplotted on the DIFAR UKF track output.
1. Pick a containment probability, PC, usually 0.9, and calculate C using (14.31)
2. Generate the parametric vector θ ≜[0, 0.01, . . . , 2π]. Note that one can use
any desirable interval other than 0.01.
3. Generate Q (θ) from (14.34)
4. Generate the eigenvalue and eigenvector matrices V and D of the covariance
matrix Pxx
n|n
5. Generate xe(θ) = ˆxn|n + VCD1/2Q (θ) .
14.1.6
Error Ellipse Example
The plotting of an error ellipse at every 100th data point for the DIFAR UKF track
output is shown in Figure 14.2. This plot allows one to evaluate the estimated uncer-
tainty in the track estimates at various points along the track when the true track is
unknown.
14.2
ROOT MEAN SQUARED ERRORS
Using (3.16), at time tn we can write the state covariance matrix as
Pxx
n|n = E

xn −ˆxn|n
 
xn −ˆxn|n
⊺
(14.39)
When xn is known, as it would be in a simulation, and ˆxn|n is unbiased, the square
root of the diagonal elements Pxx
n|n can be approximated by conditional root mean
squared errors (or the square root of the mean of the squared errors).
As noted in Chapter 4, the ﬁrst task of a simulation is to generate a truth state
trajectory, {xn, n = 0, 1, . . . , N}, for all times of interest {tn, n = 0, 1, . . . , N}.

DIVERGENT TRACKS
183
A truth observation trajectory is then generated from the truth state trajectory and
independent noise is added to each observation to create a set of simulated observa-
tions, {zn, n = 0, 1, . . . , N}. To compute the RMS errors, multiple sets of observa-
tions are created in a Monte Carlo fashion, so that their are M sets of independent
observations, {zn,m; n = 0, 1, . . . , N; m = 1, 2, . . . , M}. Note that the observation
noise must be independent from time-sample to time-sample (zi,m must be indepen-
dent from zj,m , ∀i /= j) and from Monte Carlo run to Monte Carlo run (zn,l must be
independent from zn,k , ∀l /= k).
Each set of Monte Carlo observations are now used in a given tracking ﬁlter to
produce a set of track ﬁle outputs, ˆxn|n (m) and a mean squared error matrix at each
n can be obtained from
S
ˆxn|n

≜1
M
M

m=1

xn −ˆxn|n (m)
 
xn −ˆxn|n (m)
⊺
(14.40)
The RMS error are then obtained from
eRMS (xn) = diag




1
M
M

m=1

xn −ˆxn|n (m)
 
xn −ˆxn|n (m)
⊺
1/2


(14.41)
where diag{•} means “take the diagonal terms and place them in a column vector.” Or,
to put it another way, for a four-dimensional state vector xn =

x1,n, x2,n, x3,n, x4,n
⊺
eRMS (xn) =


eRMS

x1,n

eRMS

x2,n

eRMS

x3,n

eRMS

x4,n



(14.42)
with
eRMS

xi,n

≜

1
M
M

m=1

xi,n −ˆxi,n|n (m)
2
1/2
(14.43)
14.3
DIVERGENT TRACKS
One has to be careful when calculating the RMS error performance, since even one
divergent track among 100 Monte Carlo runs will skew the RMS error statistic. Di-
vergent tracks are to be considered outliers that do not conform to the Gaussian
assumption that the RMS error is an unbiased sampled data estimate of the standard
deviation. Unfortunately, what constitutes a divergent track is a subjective decision
that is usually scenario dependent.
For the DIFAR case-study performance analysis, a track estimate was considered
divergent if either the x or y-axis estimate at the ﬁnal time deviated from the truth

184
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
value by more than 2 nmi. As seen in Chapters 7–12, for the DIFAR case study the
number of divergent tracks increased as the SNR decreased below 5 dB.
14.4
CRAMER–RAO LOWER BOUND
We can now ask the question, what is the BEST (smallest) achievable RMS error for
a given scenario (set of dynamic and observation models and state trajectories). Since
the observation noise is always present, the best achievable RMS errors will occur
when the dynamic model is used without any dynamic noise.
Why are lower bounds useful? Assessing achievable estimation algorithm (tracker)
performance may be difﬁcult and a lower bound will give an indication of the per-
formance. In many cases lower bounds on performance are required to determine
whether a set of imposed system performance requirements are realistic or not.
Much of the material in this section is based on Refs [1–3], and the references
contained therein. We stress here the Cramer–Rao lower bound, but [2] goes beyond
this bound to consider a wide variety of different bounds applicable to problems other
than the Bayesian estimation methods addressed in this book.
14.4.1
The One-Dimensional Case
Consider the one-dimensional case where the process (dynamic) noise is zero making
x deterministic. Then, the only contributor to errors in ˆx will be the observation noise,
which is captured by the likelihood function. For a normalized likelihood function,
we know that

p (z|x) dz = 1
(14.44)
It immediately follows that the expected value of ˆx (z) is given by

ˆx (z) p (z|x) dz = x
(14.45)
Based on this, since x is deterministic we can write

(ˆx (z) −x) p (z|x) dz = 0
(14.46)
Taking the derivative with respect to x leads to the progression
∂
∂x

(ˆx −x) p (z|x) dz = 0
(14.47)
−

p (z|x) dz +

(ˆx −x) ∂p (z|x)
∂x
dz = 0
(14.48)

CRAMER–RAO LOWER BOUND
185

(ˆx −x) ∂p (z|x)
∂x
dz =

p (z|x) dz
(14.49)

(ˆx −x) ∂p (z|x)
∂x
dz = 1
(14.50)
Noting that
∂
∂x ln p (z|x) =
1
p (z|x)
∂p (z|x)
∂x
(14.51)
(14.50) becomes

(ˆx −x) p (z|x) ∂
∂x ln p (z|x) dz = 1
(14.52)
or
1 =
 !
(ˆx −x)

p (z|x)
" 	 ∂
∂x ln p (z|x)

p (z|x)

dz
(14.53)
Taking the square of both sides yields
1 =
# !
(ˆx −x)

p (z|x)
" 	 ∂
∂x ln p (z|x)

p (z|x)

dz
$2
(14.54)
Using the Swartz inequality, we obtain
1 ≤

(ˆx −x)2 p (z|x) dz
 	 ∂
∂x ln p (z|x)

2
p (z|x) dz
(14.55)
or

(ˆx −x)2 p (z|x) dz ≥
1
  ∂
∂x ln p (z|x)
2 p (z|x) dz
(14.56)
The left-hand side of (14.56) is just the variance, σ2
x, and the denominator on the
right-hand side can be identiﬁed as the Fisher information J ≜E{[ ∂
∂x ln p (z|x)]2}.
We can now write
σ2
x ≥
1
E
% ∂
∂x ln p (z|x)
2& ≜1
J
(14.57)
where we deﬁne the CRLB for the one-dimensional case as
CRLB =
1
E
% ∂
∂x ln p (z|x)
2&
(14.58)
Since the variance can be estimated by the RMS error in a Monte Carlo simulation,
the square root of the CRLB becomes the lower bound on the RMS error.

186
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
14.4.2
The Multidimensional Case
For a multidimensional case, the Fisher information becomes the Fisher information
matrix Jn and the CRLB becomes J−1
n . When process noise is included in the state
dynamic model, the inverse of the Fisher information matrix is the lower bound of
Pxx
n|n
Pxx
n|n ≜E

xn −ˆxn|n
 
xn −ˆxn|n
⊺
≥J−1
n
(14.59)
Consider the true state trajectory deﬁned as x1:n ≜{x0, x1, . . . , xn}. Assume that the
initialization of the state estimate, x0, prior to any observations, has the probability
p (x0). The unbiased estimate of the state xn at time tn is given by the ﬁlter output
ˆxn|n, which has made use of the observations z1:n ≜{z1, z2, . . . , zn}. The CRLB for
the trajectory x1:n is given by the inverse of the trajectory information matrix J1:n
E

x1:n −ˆx1:n|1:n
 
x1:n −ˆx1:n|1:n
⊺
≥J−1
1:n = CRLBtraj
(14.60)
Extending (14.57) to the multidimensional case yields the ﬁrst form of J1:n
J1:n ≜E

∇x1:n ln p (x1:n, z1:n)
 
∇x1:n ln p (x1:n, z1:n)
⊺
(14.61)
Noting that (14.51) can be written in the multidimensional form
∇⊺
x1:n ln p (x1:n, z1:n) =
1
p (x1:n, z1:n)∇⊺
x1:np (x1:n, z1:n)
(14.62)
After algebra, using (14.62) in (14.61) yields the second form of J1:n
J1:n ≜−E

∇x1:n∇⊺
x1:n ln p (x1:n, z1:n)

(14.63)
Here, we must note that
• p (x1:n, z1:n) is used if the target state is stochastic due to the inclusion of process
noise vn.
• p (z1:n|x1:n) is used if the target state is deterministic (zero process noise).
• ∇x1:n is evaluated at the true value of x1:n.
14.4.3
A Recursive Approach to the CRLB
First, it would be useful to have a method for writing J1:n in terms of Jn and J1:n−1.
Such a method was developed by Tichavsk´y et al. [2,4]. Noting that we can decompose
x1:n and ∇x1:n into
x1:n =

x1:n−1
xn

(14.64)

CRAMER–RAO LOWER BOUND
187
and
∇x1:n =

∇x1:n−1
∇xn

(14.65)
then ∇x1:n∇⊺
x1:n can be decomposed into
∇x1:n∇⊺
x1:n =

∇x1:n−1
∇xn


∇⊺
x1:n−1
∇⊺
xn

=

∇x1:n−1∇⊺
x1:n−1
∇x1:n−1∇⊺
xn
∇xn∇⊺
x1:n−1
∇xn∇⊺
xn

(14.66)
Now, using (14.66) in (14.63), we can decompose J1:n
J1:n =

A1:n−1|1:n−1
B1:n−1|n
B⊺
1:n−1|n
Cn|n

(14.67)
with
A1:n−1|1:n−1 ≜−E

∇x1:n−1∇⊺
x1:n−1 ln p (x1:n, z1:n)

(14.68)
B1:n−1|n ≜−E

∇x1:n−1∇⊺
xn ln p (x1:n, z1:n)

(14.69)
Cn|n ≜−E

∇xn∇⊺
xn ln p (x1:n, z1:n)

(14.70)
Now, we can calculate J−1
1:n by taking the inverse of the partitioned matrix (14.67)
using (2.19) results in
J−1
1:n =

T11
T12
T21
T22

(14.71)
where
T11 ≜
'
A1:n−1|1:n−1 −B1:n−1|nC−1
n|nB⊺
1:n−1|n
(−1
(14.72)
T12 ≜−A1:n−1|1:n−1B−1
1:n−1|n
×
'
Cn|n −B⊺
1:n−1|nA−1
1:n−1|1:n−1B1:n−1|n
(−1
(14.73)
T21 ≜−C−1
n|nB⊺
1:n−1|n
'
A1:n−1|1:n−1 −B1:n−1|nC−1
n|nB⊺
1:n−1|n
(−1
(14.74)
T22 ≜
'
Cn|n −B⊺
1:n−1|nA−1
1:n−1|1:n−1B1:n−1|n
(−1
(14.75)
The lower right-hand block of J−1
1:n can be identiﬁed as J−1
n
that is the Cramer–Rao
lower bound for ˆxn|n
J−1
n
=
'
Cn|n −B⊺
1:n−1|nA−1
1:n−1|1:n−1B1:n−1|n
(−1
(14.76)

188
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
It follows immediately that
Jn = Cn|n −B⊺
1:n−1|nA−1
1:n−1|1:n−1B1:n−1|n
(14.77)
Note that (14.76) requires the inverse of the large matrix A1:n−1|1:n−1. To get
around this, we now develop a recursion relationship that generates Jn from Jn−1
that does not require a large matrix inverse. First, consider the joint density found in
(14.61). It follows immediately that
p (x1:n, z1:n) = p (xn, zn, x1:n−1, z1:n−1)
= p (zn|xn, x1:n−1, z1:n−1) p (xn|x1:n−1, z1:n−1)
×p (x1:n−1, z1:n−1)
(14.78)
Since zn only depends on xn, and xn only depends on xn−1, this becomes
p (x1:n, z1:n) = p (zn|xn) p (xn|xn−1) p (x1:n−1, z1:n−1)
(14.79)
If we now decompose ∇x1:n as
∇x1:n∇⊺
x1:n =


∇x1:n−2
∇xn−1
∇xn



∇⊺
x1:n−2
∇⊺
xn−1
∇⊺
xn

=


∇x1:n−2∇⊺
x1:n−2
∇x1:n−2∇⊺
xn−1
∇x1:n−2∇⊺
xn
∇xn−1∇⊺
x1:n−2
∇xn−1∇⊺
xn−1
∇xn−1∇⊺
xn
∇xn∇⊺
x1:n−2
∇xn∇⊺
xn−1
∇xn∇⊺
xn


(14.80)
the trajectory information matrix (14.63) becomes
J1:n =


S11
S12
S13
S21
S22
S23
S31
S32
S33


(14.81)
Here
S11 = −E

∇x1:n−2

∇⊺
x1:n−2 ln p (x1:n, z1:n)

= −E

∇x1:n−2

∇⊺
x1:n−2

ln p (x1:n−1, z1:n−1)
+ ln p (zn|xn) + ln p (xn|xn−1)

= −E

∇x1:n−2

∇⊺
x1:n−2 ln p (x1:n−1, z1:n−1)

= A1:n−2|1:n−2
(14.82)

CRAMER–RAO LOWER BOUND
189
Similarly
S22 = Cn−1|n−1 + D11
n−1|n−1
(14.83)
S12 = B1:n−2|n−1
(14.84)
S31 = 0
(14.85)
S32 = D12
n|n−1
(14.86)
S22 = D22
n|n
(14.87)
with A1:n−2|1:n−2, B1:n−2|n−1, and Cn−1|n−1 deﬁned by (14.68), (14.69), and (14.70),
respectively, and
D11
n−1|n−1 ≜−E

∇xn−1∇⊺
xn−1 ln p (xn|xn−1)

(14.88)
D21
n−1|n ≜−E

∇xn−1∇⊺
xn ln p (xn|xn−1)

(14.89)
D12
n|n−1 =
!
D21
n−1|n
"⊺
(14.90)
D22
n|n ≜−E

∇xn∇⊺
xn ln p (xn|xn−1)

−E

∇xn∇⊺
xn ln p (zn|xn)

(14.91)
We can now write (14.81) as
J1:n =


A1:n−2|1:n−2
B1:n−2|n−1
0
B⊺
1:n−2|n−1
Cn−1|n−1 + D11
n−1|n−1
D12
n|n−1
0
D21
n−1|n
D22
n|n


(14.92)
Calculating J−1
1:n by taking the inverse of the partitioned matrix (14.92) using (2.19)
and looking at only the inverse of the lower right-hand block results in
Jn = D22
n|n −
!
0
D21
n−1|n
"
×

A1:n−2|1:n−2
B1:n−2|n−1
B⊺
1:n−2|n−1
Cn−1|n−1 + D11
n−1|n−1
 
0
D12
n|n−1

= D22
n|n −D21
n−1|n
!
Cn−1|n−1 + D11
n−1|n−1
−B⊺
1:n−2|n−1A−1
1:n−2|1:n−2B1:n−2|n−1
"−1
×D12
n|n−1
(14.93)
Remembering (14.77), (14.93) becomes the recursion for Jn
Jn = D22
n|n −D21
n−1|n
!
Jn−1 + D11
n−1|n−1
"−1
D12
n|n−1
(14.94)
This recursion can be initialized from (14.61) by setting
J0 = E

∇x0 ln p (x0)
 
∇x0 ln p (x0)
⊺
(14.95)

190
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
14.4.4
The Cramer–Rao Lower Bound for Gaussian Additive Noise
Consider the case where the dynamic and observation models are Gaussian, that is,
xn = fn−1 (xn) + vn−1
(14.96)
zn = hn (xn) + wn−1
(14.97)
where
vn ∼N (0, Q)
(14.98)
wn ∼N (0, R)
(14.99)
If the initial distribution is Gaussian, with
p (x0) = N

x0; ˆx0, Pxx
0

(14.100)
then
∇x0 ln p (x0) = ∇x0
#
c −1
2
!
(x0 −ˆx0)⊺
Pxx
0
−1 (x0 −ˆx0)
"$
= −

Pxx
0
−1 (x0 −ˆx0)
(14.101)
Now, from (14.95), and noting that

Pxx
0
−⊺=

Pxx
0
−1, J0 becomes
J0 = E
%
Pxx
0
−1 (x0 −ˆx0) (x0 −ˆx0)⊺
Pxx
0
−1&
=

Pxx
0
−1 E

(x0 −ˆx0) (x0 −ˆx0)⊺ 
Pxx
0
−1
=

Pxx
0
−1
(14.102)
Similarly,
∇xn−1 ln p (xn|xn−1) =

∇xn−1f⊺
n−1 (xn−1)

Q−1
n−1vn−1
(14.103)
∇xn ln p (zn|xn) =

∇xnh⊺
n (xn)

R−1
n wn
(14.104)
And (14.88)–(14.91) become
D11
n−1|n−1 = E
%
˜F⊺
n−1Q−1
n−1 ˜Fn−1
&
(14.105)
D12
n−1|n = −E
˜F⊺
n−1

Q−1
n−1
(14.106)
D21
n|n−1 = −Q−1
n−1E
˜Fn−1

(14.107)
D22
n|n = Q−1
n−1 + E
%
˜H⊺
nR−1
n
˜Hn
&
(14.108)
where
˜Fn−1 ≜

∇xn−1f⊺
n−1 (xn−1)
⊺
xn−1(TRUE)
(14.109)
˜Hn ≜

∇xnh⊺
n (xn)
⊺
xn(TRUE)
(14.110)

CRAMER–RAO LOWER BOUND
191
In summary, for nonlinear models with Gaussian noise, the CRLB can be
computed recursively from (14.95) and (14.94) with (14.105)–(14.108) replacing
(14.88)–(14.91).
Two problems can arise in the computation of the CRLB. The ﬁrst is the calcu-
lation of the E {•} terms in (14.105)–(14.108). For the Gaussian case, one can use
a Monte Carlo method to estimate these expectation terms, since ˜Fn−1 and ˜Hn are
evaluated at the trajectory points xn−1 and xn, respectively. By using (14.96), where
the noise is sampled from vn ∼N (0, Q) , one can generate M sets of Monte Carlo
trajectories values of

xm,n; n = 1, . . . , N; m = 1, . . . , M

. The expectation values
can be estimated by averaging across the Monte Carlo trajectories at each time point.
The second problem occurs when Qn is singular and an inverse does not exist.
Methods for handling this special case is beyond the scope of this book, but can be
found in Ref. [4] (which can also be found starting on page 686 of Ref. [2]).
14.4.5
The Gaussian Cramer–Rao Lower Bound with Zero Process Noise
When the process noise is very small, we can evaluate the CRLB assuming zero
process noise. When the process noise in (14.96) is zero, the state vector evolution is
completely deterministic. Therefore, the expectation operation in (14.105)–(14.108)
can be ignored. Now, (14.94) becomes
Jn = Q−1
n−1 + ˜H⊺
nR−1
n
˜Hn −Q−1
n−1 ˜Fn−1
!
Jn−1 + ˜F⊺
n−1Q−1
n−1 ˜Fn−1
"−1 ˜F⊺
n−1Q−1
n−1
(14.111)
Using the matrix inversion lemma (2.13), this reduces to
Jn =
'
Qn−1 + ˜Fn−1J−1
n−1 ˜F⊺
n−1
(−1
+ ˜H⊺
nR−1
n
˜Hn
(14.112)
Since the process noise is zero, it follows that
Jn =
'
˜F−1
n−1
(⊺
Jn−1 ˜F−1
n−1 + ˜H⊺
nR−1
n
˜Hn
(14.113)
By looking at the inverse J−1
n
and using the matrix inversion lemma, it can be
shown that (14.113) reduces to a form identical to the Kalman ﬁlter covariance update
equation (5.30), with the Jacobians evaluated at the true value of xn. Thus, in the
absence of process noise, the CRLB recursion for nonlinear ﬁltering is identical to
the covariance update equations of the EKF, with the Jacobians evaluated at the true
state vector xn.
14.4.6
The Gaussian Cramer–Rao Lower Bound with Linear Models
Consider the case where (14.96) and (14.97) are replace by the linear equations
xn = Fxn + vn−1
(14.114)
zn = Hxn + wn−1
(14.115)

192
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
The Jacobians (14.109) and (14.110) now become
˜Fn−1 ≜

∇xn−1x⊺
nF⊺⊺
xn−1(TRUE) = F
(14.116)
˜Hn ≜

∇xnx⊺
nH⊺⊺
xn(TRUE) = H
(14.117)
Thus, in the linear Gaussian case, the Jacobians are independent of the target state
and the expectations in (14.105)–(14.108) can be dropped yielding
D11
n−1|n−1 = F⊺Q−1
n−1F
(14.118)
D12
n−1|n−1 = −F⊺Q−1
n−1
(14.119)
D21
n−1|n−1 = −Q−1
n−1F
(14.120)
D11
n−1|n−1 = Q−1
n−1 + H⊺R−1
n H
(14.121)
Now, using these in (14.94) and using the block matrix inversion lemma from
Section 2.1.4, (14.94) reduces to
J−1
n
= Pxx
n|n
(14.122)
Thus, for the Gaussian linear case, the CRLB reduces to the update state covariance
matrix of the linear Kalman ﬁlter, indicating that the linear Kalman ﬁlter is the optimal
ﬁlter for this case.
14.5
PERFORMANCE OF KALMAN CLASS DIFAR TRACK
ESTIMATORS
The track estimators used for the DIFAR case study include the EKF, UKF, SSKF,
GHKF, and MCKF. In this section, we compare the DIFAR tracking x-axis and y-axis
RMS errors for each track estimator. Figures 14.3–14.7 show the RMS x-axis and
y-axis RMS errors for the ﬁve different track algorithms, along with the CRLB, for
signal SNRs from 20 dB down to 0 dB in increments of 5 dB. One can immediately
see from these ﬁgures that there is very little difference among the tracker algorithms
when applied to the DIFAR problem at all SNRs. The CRLB is the gray line across the
bottom of each ﬁgure. Based on these performance measures, for the DIFAR problem,
one should use the EKF since it minimizes the computational burden. It is readily
apparent from these ﬁgures that for all SNRs the ﬁlters have their best performance
when the target ship enters the buoy ﬁeld near the (0, 0) position. But when the target
is far from the buoy ﬁeld, track performance degradation increases with range from
the buoy ﬁeld center and decreasing SNR. In addition, at an SNR of 5 dB or below, the
SSKF tracker became unstable. At an SNR of 0 dB, the MCKF also became unstable.
These cases are not included in the RMS plots.

−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.05
0.1
0.15
0.2
0.25
x-position (nmi)
X-axis RMS error for 20 dB (nmi)
EKF
UKF
SSKF
GHKF
MCKF
+
CRLB
EKF
UKF
SSKF
GHKF
MCKF
+
CRLB
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
x-position (nmi)
Y-axis RMS error for 20 dB (nmi)
FIGURE 14.3
Comparison of the RMS errors for ﬁve different track estimation algorithms with the signal SNR at 20 dB.
193

−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.05
0.1
0.15
0.2
0.25
x-position (nmi)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x-position (nmi)
X-axis RMS error for 15 dB (nmi)
 
EKF
UKF
SSKF
GHKF
MCKF
+
CRLB
EKF
UKF
SSKF
GHKF
MCKF
+
CRLB
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Y-axis RMS error for 15 dB  (nmi)
 
FIGURE 14.4
Comparison of the RMS errors for ﬁve different track estimation algorithms with the signal SNR at 15 dB.
194

−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.05
0.1
0.15
0.2
0.25
x-position (nmi)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x-position (nmi)
X-axis RMS error for 10 dB (nmi)
EKF
UKF
GHKF
MCKF
CRLB
EKF
+
+
UKF
GHKF
MCKF
CRLB
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Y-axis RMS error for 10 dB  (nmi)
FIGURE 14.5
Comparison of the RMS errors for ﬁve different track estimation algorithms with the signal SNR at 10 dB.
195

−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x-position (nmi)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x-position (nmi)
X-axis RMS error for 05 dB (nmi)
EKF
UKF
GHKF
CRLB
EKF
UKF
GHKF
CRLB
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Y-axis RMS error for 05 dB  (nmi)
FIGURE 14.6
Comparison of the RMS errors for ﬁve different track estimation algorithms with the signal SNR at 5 dB.
196

−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
10
20
30
40
50
60
70
x-position (nmi)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x-position (nmi)
X-axis RMS error for 0 dB (nmi)
EKF
UKF
GHKF
CRLB
EKF
UKF
GHKF
CRLB
0
10
20
30
40
50
60
Y-axis RMS error for 0 dB  (nmi)
FIGURE 14.7
Comparison of the RMS errors for ﬁve different track estimation algorithms with the signal SNR at 0 dB.
197

198
PERFORMANCE MEASURES FOR THE FAMILY OF KALMAN FILTERS
REFERENCES
1. Van Trees HL. Detection, Estimation, and Modulation theory, Part I: Detection, Estimation,
and Linear Modulation Theory. Wiley; 1968.
2. Van Trees HL, Bell KL, editors. Bayesian Bounds for Parameter Estimation and Nonlinear
Filtering/Tracking. Wiley Interscience; 2007.
3. Ristic B, Arulampalam S, Gordon N. Beyond the Kalman Filter: Particle Filters for
Tracking Applications. Artech House; 2004.
4. Tichavsk´y P, Muavchik CH, Nehorai A. Posterior Cram´er–Rao bounds for discrete-time
nonlinear ﬁltering. IEEE Trans. Signal Process. 1998;46(5):1386–1396.

PART III
MONTE CARLO METHODS

15
INTRODUCTION TO MONTE CARLO
METHODS
In Part II, we examined numerical methods for evaluating Gaussian-weighted moment
integrals containing nonlinear functions. Most of the methods that we investigated
incorporate approximations of the nonlinear function using some form of expansion in
a multidimensional polynomial. The Gaussian moment integrals were then evaluated
at a variety of sets of deterministic vector points resulting in the family of sigma point
Kalman ﬁlters. In Chapter 12, an alternate method was presented where, instead of
making an approximation of the nonlinear function, the multidimensional Gaussian
density was approximated by a set of discrete random samples, allowing the moment
integrals to be evaluated as a weighted sum of the nonlinear function evaluated at the
sample points. As the number of sample points increased, the approximation becomes
more exact. It is possible to apply this Monte Carlo sampling method directly for some
density functions that have an analytical form, such as Student, Rayleigh, and chi-
squared. But the question that should immediately come to mind is whether or not
such Monte Carlo integration methods can be applied when the density is unknown
or hard to sample from.
In this chapter, we ﬁrst consider a method for estimating the form of a density
based solely on a set of random samples from that density. This, of course, begs the
question of how those samples were obtained in the ﬁrst place. The method presented
consists of the generation of multidimensional histograms.
This is followed immediately by the development of a Kernel density approxi-
mation of the histogram, providing a smoother estimation of the density function.
The usefulness of these Kernel approximations becomes obvious when used for
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
201

202
INTRODUCTION TO MONTE CARLO METHODS
visualization of multimodal or highly non-Gaussian densities and estimation of the
moments does not provide any meaningful information. See Richardson et al. [1] for
an example where the density is multimodal in two dimensions and the moments
are not representative of the peaks of any of the modes and would therefore give a
false impression of the probable location of the target of interest. We will also ﬁnd
Kernel density approximations useful when we discuss resampling methods in the
next chapter. After the discussion on kernel density estimation methods we return to
the question of how to generate samples from an unknown density by introducing the
concept of importance sampling. This chapter is concluded with a summary.
15.1
APPROXIMATING A DENSITY FROM A SET OF MONTE
CARLO SAMPLES
15.1.1
Generating Samples from a Two-Dimensional Gaussian
Mixture Density
In this section, we discuss several methods for estimating the form of a density based
on only a set of samples drawn from that density. It is often the case that we must
make inferences about an unknown density from a set of samples (data) from that
density, so we must adopt a nonparametric approach. Such nonparametric density
estimation methods can provide visual cues that reveal skewness in distributions or
the presence of multiple modes in the data and may provide the basis for important
physical interpretations of the observations [2]. By way of example, consider the dual
mode Gaussian mixture density
p (x) = w1N (x1; ˆx1, 1) + w2N (x2; ˆx2, 2)
(15.1)
where w1 + w2 = 1. Samples can be easily drawn from such a mixture density. If the
total number of desired samples is N, then draw w1N samples from N (x1; ˆx1, 1)
and w2N samples from N (x2; ˆx2, 2). These samples for N = 1000 and (w1, w2) =
(0.3, 0.7) are shown in Figure 15.1. We have also set
ˆx1 = (−5, −8)
ˆx2 = (7, 7)
1 =

100
−5
−5
1

2 =

4
8
8
64

15.1.2
Approximating a Density by Its Multidimensional Histogram
Since we are interested in developing a visualization of the probability density func-
tion, it is important to note that the probability of a sample falling within a given area

APPROXIMATING A DENSITY FROM A SET OF MONTE CARLO SAMPLES
203
−30
−20
−10
0
10
20
30
−30
−20
−10
0
10
20
30
FIGURE 15.1
Samples drawn from a two-dimensional Gaussian mixture density.
is given by the normalized density of the samples that fall within that area. This leads
to a normalized two-dimensional histogram as the ﬁrst estimation and visualization
method considered because it actually measures the probability of observing a sample
in a particular two-dimensional area interval. A multidimensional histogram density
estimate can be deﬁned by
ˆp (x) =
νik
NAik
(15.2)
where νik is the number of samples falling in the hypervolume bin Aik. For example,
for a two-dimensional density, Aik = (xi+1 −xi) (yk+1 −yk) . If all of the bins are
the same size, we can let h = (xi+1 −xi) = (yk+1 −yk) , ∀i, k and
ˆp (x) = νik
Nh2
(15.3)
Thus, for a general nx-dimensional histogram
ˆp (x) =
νi
Nhnx
(15.4)
with νi the number of samples falling into the ith hypervolume bin. This latter his-
togram has only one free parameter, the bin width h. The histogram can asymptotically
approximate any continuous density and hence can be called a nonparametric esti-
mate. The selection of h, called the bandwidth, has been the emphasis of much recent
research that will not be presented here. See Scott [3] and the references contained
therein. Scott shows how to use the integrated mean squared error (IMSE) and the
mean integrated squared error (MISE) to estimate the optimal bandwidth based on
the sample data.

204
INTRODUCTION TO MONTE CARLO METHODS
−30
−20
−10
0
10
20
30
−30
−20
−10
0
10
20
30
FIGURE 15.2
Two-dimensional histogram based on samples from a Gaussian mixture dis-
tribution.
A set of Matlab routines that generates and plots a two-dimensional histogram can
be downloaded from Matlab Central [4]. Care must be taken with these subroutines.
They must be adapted to the speciﬁc data set and they are also unnormalized as written.
The two-dimensional normalized histogram generated from the data in Figure 15.1
is shown in Figure 15.2. The bandwidth was selected to produce 99 bins in each
dimension based on the maximum range of the data.
15.1.3
Kernel Density Approximation
An estimate of the probability of a sample falling within a given two-dimensional bin
centered at x ∈R2 is given by (15.3). In the limit, as the bin area goes to zero, we
reach a point where each bin contains, at most, one sample. In this limit, we can write
the sample density function as
ps (x) = 1
N
N

i=1
δ

x −x(i)
(15.5)
This can be seen as a density for discrete samples because

Rnx
ps (x) dx = 1
N
N

i=1

Rnx
δ

x −x(i)
dx = 1
(15.6)
The histogram shown in Figure 15.2 is blocky and has discontinuities at each bin
boundary. In addition, its derivatives are nonzero only at the bin boundaries and are

APPROXIMATING A DENSITY FROM A SET OF MONTE CARLO SAMPLES
205
zero everywhere else. This difﬁculty can be overcome by replacing the histogram
estimator with an estimator that convolves ps (x) with a smoothing scaled kernel
density function, Kh (x). That is, let
ˆp (x) = (ps ∗Kh) (x)
≜

Rnx
ps (u) KH (x −u) du
=

Rnx
1
N
N

i=1
δ

u −x(i)
KH (x −u) du
= 1
N
N

i=1
KH

x −x(i)
(15.7)
where the scaled kernel is deﬁned in terms of the kernel by
KH (t) ≜
1
|H|1/2 K

H−1/2t

(15.8)
Here K(u) is a continuous symmetric Kernel function that satisﬁes the conditions

Rn
K (u) du = 1
(15.9)

uj
Rn
K (u) du = 0, j = 1, . . . , k −1
(15.10)

uk
Rn
K (u) du /= 0
(15.11)
K (u) > 0, ∀u
(15.12)
where k is the order of the kernel function. The components of the matrix H are the
bandwidth (or smoothing) parameters of the kernel and H functions in a manner sim-
ilar to a covariance matrix. Two very good introductions to kernel density estimation
can be found in the monograms by Fukunaga [5] and Silverman [6].
15.1.3.1
Univariate Kernel Density Estimation. Consider ﬁrst the univariate case.
A variety of univariate kernel function have been considered in the literature [7] and
some of the most popular are listed in Table 15.1. In Table 15.1, the indicator function
I is deﬁned by
I (|u| ≤1) =

1,
for −1 ≤u ≤1
0,
elsewhere
(15.13)
To
illustrate
the
use
of
a
kernel
function
in
one-dimensional
den-
sity estimation, consider the small set of 12 one-dimensional data points

206
INTRODUCTION TO MONTE CARLO METHODS
TABLE 15.1
Common Univariate Kernel
Functions of Order 2
Kernel
K (u)
Uniform
1
2I (|u| ≤1)
Triangle
(1 −|u|) I (|u| ≤1)
Epanechnikov
3
4

1 −u2	
I (|u| ≤1)
Quartic
15
16

1 −u2	2 I (|u| ≤1)
Triweight
35
32

1 −u2	3 I (|u| ≤1)
Cosine
- π
4 cos π
2 u	
I (|u| ≤1)
Gaussian
1
√
2π exp
−1
2u2	
{−7.5, −6, −3, −2, −1.5, −1, −0.3, 0, 3.5, 4, 6, 8}. Let the number of original data
points be labeled Nold. Using the Gaussian kernel from Table 15.1 in (15.7) results in
the kernel density estimate
ˆp (x) = 1
N
N

i=1
Kh

x −x(i)
=
N

i=1
1
NhK

x −x(i)
h

=
N

i=1
1
N
1
√
2πh
exp

−1
2
x −x(i)
h
2
(15.14)
We can see immediately that the bandwidth parameter h is just the standard deviation
of the Gaussian density. We can also immediately see that the density estimate ˆp (x) is
nothing more than a Gaussian mixture density with equal weights for each individual
Gaussian density centered around one of the original sample points.
Deﬁne the individual sample points kernels as
Ki (x) = 1
N
1
√
2πh
exp

−1
2
x −x(i)
h
2
= 1
N N

x; x(i), h2
(15.15)
Both the full estimated kernel density and the individual kernels for two differ-
ent values of h are plotted in Figure 15.3 over the range of x within the limits
−15 ≤x ≤15.
It is obvious from Figure 15.3 that the value chosen for h is very important. In the
ﬁrst plot, when h = 0.9, the density is smooth and produces a good approximation of
the density of the samples, while in the second plot for h = 0.3, the density estimate
is much too fractured with peaks at almost every sample. It has been commented that
the choice of h is much more important than the choice of K [7].

APPROXIMATING A DENSITY FROM A SET OF MONTE CARLO SAMPLES
207
−15
−10
−5
0
5
10
15
−0.05
0
0.05
0.1
0.15
h = 0.9
−15
−10
−5
0
5
10
15
−0.1
0
0.1
0.2
0.3
h = 0.3
FIGURE 15.3
Gaussian kernel density estimate for sparse sample data.
Now, assume that the original sample set was drawn from a Gaussian distribution
(even though it may be far from the truth). That is, assume that the true density can be
approximated by N

x; xs, σ2
s
	
, where xs and σ2
s are the sample mean and variance.
If we make the transformation
˜x = x
σs
(15.16)
then it follows immediately that
N

x; xs, σ2
s

= 1
σs
N (˜x; ˜xs, 1)
= 1
σs
N

 x
σs
; xs
σs
, 1

(15.17)
Now, (15.15) becomes
Ki (˜x) =
1
Nσs
N

x; x(i), σ2
sh2
= 1
N
1
√
2πhσs
exp

−1
2
x −x(i)
σsh
2
(15.18)
Applying the afﬁne transformation
˜x = x(i) + σshc
(15.19)

208
INTRODUCTION TO MONTE CARLO METHODS
TABLE 15.2
One-Dimensional Kernel Sample Generation
1. Subdivide the interval (0, 1] into Nold
qi = (i −1)/Nold; i = 1, . . . , Nold + 1.
increments
2. Generate Nnew sample from a uniform
uj ∼U(0, 1], j = 1, . . . , Nnew
distribution
3. For uj, determine its appropriate qi interval
Find i such that qi < uj ≤qi+1
4. Generate sample for c
c(j) ∼N (c; 0, 1)
5. Generate new sample of x
˜x(j) = x(i) + σshc(j)
6. Repeat Steps 2 - 5 for j = 1, . . . , Nnew
modiﬁes (15.18) so that
Ki (˜x) = 1
N
1
√
2π
exp

−1
2c2

(15.20)
and (15.14) reduces to
ˆp (˜x) =
N

i=1
1
N N (c; 0, 1)
(15.21)
Now, a new set of samples

x(j), j = 1, . . . , Nnew

, where Nnew can be much
greater than Nold, can be drawn from ˆp (˜x) using the procedure shown in Table 15.2.
This procedure (called regularization) is so simple only because we are taking
advantage of the fact that the kernel estimation method results in a mixture density
with equal weights! As we will show below, it can be generalized to include the
multivariate case with unequal weights. It is important to remember that in areas
where there is a high density of samples, there will be more resampled values chosen.
It should be noted that resampling from a kernel density estimate, when kernels
other than the Gaussian are used, is sometimes much more difﬁcult. For example,
generating samples from an Epanechnikov kernel consists of generating √βu, where
β follows a beta distribution with parameters (1/2, 2) and u is a sample from the
uniform distribution as shown in step 2 above [8].
Further discussion of multidimensional kernel methods can be found in
Refs [9–12].
15.1.3.2
Multivariate Kernel Density Estimation. For a multivariate Gaussian
kernel, (15.18) results in the density estimator
ˆp (x) =
N

i=1
1
N
1
(2π)nx/2 |H|1/2 exp

−1
2

x −x(i)⊺
[H]−1 
x −x(i)
(15.22)

APPROXIMATING A DENSITY FROM A SET OF MONTE CARLO SAMPLES
209
Thus, the multivariate individual kernels are obviously the Gaussian densities
N

x; x(i), H
	
divided by N.
Consider the two-dimensional case nx = 2, where
H =

h2
1
h12
h12
h2
2

(15.23)
For this case we can have three different smoothing parameters, one for dimension
1, one for dimension 2, and one for the correlation between the two dimensions. But
this makes choosing the “best” smoothing parameters more complex than in the one-
dimensional case. If we assume that the underlying density of the data is Gaussian,
that is, x ∼N (x; ˆx, ) , we can generate a sample covariance matrix s from the
data. Making the matrix decomposition s = DsD⊺
s , we can normalize the data using
the transformation
˜x = D−1
s x
(15.24)
and it follows immediately that
˜x ∼|Ds|−1 N

D−1
s x; D−1
s xs, I

(15.25)
Since the data has now been transformed so that it has a unit diagonal sample covari-
ance matrix, the off-diagonal terms of H can be set to zero and we can set h = h1 = h2
and thus |H|1/2 = h2. Thus, for this two-dimensional case, (15.22) becomes
ˆp (˜x) =
N

i=1
1
N
1
2π |Ds| h2 exp

−1
2

D−1
s x −D−1
s x(i)⊺
H−1
×

D−1
s x −D−1
s x(i)
(15.26)
If a ﬁnal afﬁne transformation is made such that
x = x(i) + Dshc
(15.27)
then (15.26) reduces to
ˆp (˜x) =
N

i=1
1
N N (c; 0, I)
(15.28)
It is simple to extend the above to more than two dimensions and will be left as an
exercise for the reader.
A two-dimensional Gaussian Kernel density estimator subroutine can be down-
loaded from Matlab Central [12]. It was modiﬁed and used to generate a smoothed
density estimation from the data contained in Figure 15.1. The resulting visualization
of the estimated pdf is shown in Figure 15.4.
In a manner similar to the one-dimensional case, samples from the multidimen-
sional Gaussian kernel are easy to obtain using the procedure shown in Table 15.3.

210
INTRODUCTION TO MONTE CARLO METHODS
TABLE 15.3
Multidimensional Kernel Sample Generation
1. Subdivide the interval (0, 1]
qi = (i −1)/Nold; i = 1, . . . , Nold + 1
into Nold increments
2. Generate a sample from
uj ∼U(0, 1], j = 1, . . . , Nnew
a uniform distribution
3. For uj, determine its
Find i such that qi < uj ≤qi+1
appropriate qi interval
Select x(i)
4. Generate sample for c
c(j) ∼N (c; 0, I)
5. Generate new sample of x
x(j)= x(i) + Dshc(j)
6. Repeat Steps 2 - 5 for j = 1, . . . , Nnew
−30
−20
−10
0
10
20
30
−30
−20
−10
0
10
20
30
FIGURE 15.4
Visualization of the Gaussian kernel estimate of a density generated from
random samples drawn for that density.
That is, starting with a set of random samples

x(i), i = 1, . . . , Nold

, a new set
of samples

x(j), j = 1, . . . , Nnew

is generated, where Nnew can be much greater
than Nold.
15.2
GENERAL CONCEPTS IMPORTANCE SAMPLING
As seen in Part II of this book, almost all of the Gaussian estimation (tracking) methods
for nonlinear systems involve some kind of numerical evaluation of density-weighted
moment integrals. Some of these numerical methods consist of weighted sums of a
function at speciﬁc deterministic sigma points.

GENERAL CONCEPTS IMPORTANCE SAMPLING
211
If the weighting density is multivariate and non-Gaussian or unknown, in general
deterministic numerical methods cannot be used except for some cases with known
analytical densities [13]. Recent (over the past 30 years) developments in Monte
Carlo methods of integration for general density-weighted integrals have opened the
ﬂoodgates for applications to estimation and tracking. The most inﬂuential of these
methods revolve around the concept of independent importance sampling.
For a more complete discussion of the origins and fundamental developments of
both general Monte Carlo integration methods and sequential importance sampling
methods, the reader is referred to the books by Robert and Casella [14] and Doucet
et al. [15] and the articles by Doucet [16] and Liu and Chen [17] and the references
contained therein.
In Chapter 12, we solved the Gaussian density-weighted integrals by generating
Monte Carlo samples from Gaussian densities thus reducing the integrals to weighted
sums of functions evaluated at the sample points. For unknown or non-Gaussian
densities, the process of generating Monte Carlo samples from that density can be ac-
complished most efﬁciently using importance sampling. In importance sampling, one
generates a set of independent Monte Carlo samples

x(1), . . . , x(Ns)
from a known
importance density q (x) and then takes a weighted sum (average) of the integrand
evaluated at the sample values. There is a great deal of freedom in the choosing of
q (x), with the better choices generating sample values that lie in the region that is
important for the value of the integral, that is not in a region where the integrand is
negligible.
The concept of importance sampling [13] began with the desire to evaluate integrals
of the form
ˆf (x) =

f (x) p (x) dx
(15.29)
First, consider the case where p (x) is Gaussian. One way to approximate the integral
is to generate a set of samples {x(i), i = 1, . . . , N} from the Gaussian distribution
N (x; x, Px) and replace p (x) by its discrete sample version
p (x) ≃1
N
N

i=1
δ

x −x(i)
=
N

i=1
wiδ

x −x(i)
(15.30)
where wi = 1/N, ∀i. Now the integral in (15.29) becomes
ˆf (x) ≃

f (x)
N

i=1
wiδ

x −x(i)
dx
=
N

i=1
wif

x(i)
(15.31)

212
INTRODUCTION TO MONTE CARLO METHODS
Here, wi represents the importance of x(i) to the sample mean (sum). Since we sampled
directly from a known density, in this case the Gaussian density, all samples are
equally important to the summation for the sample mean and therefore wi = 1/N.
Note that the weight wi does not represent the probability of x(i), just its importance
to the weighted sum approximation to the moment integral. It can be shown [13] that
(15.31) converges to the exact solution as N →∞.
Suppose that p (x) is an unknown PDF or one that is difﬁcult to sample. Also
suppose that we have a second, well known and easily sampled PDF q (x), referred
to as the importance density for reasons that will be explained below. For example,
p (x) could be an unknown density and q (x) could be a Gaussian density. Assuming
that the support of q (x) is greater than or equal to the support of p (x), then p (x) is
proportional to q (x) at each value of x. Since p (x) is a normalized PDF, q (x) must
be a scaled version of p (x) with a different scaling factor at each x. This is absolutely
true for any two PDFs with the same support. Now, we can deﬁne a proportionality
or weighting factor as
w (x) = p (x)
q (x)
(15.32)
It is imperative that q (x) have a support that is greater than or equal to that of p (x).
For this reason, in many instances, a multidimensional Gaussian distribution is the
best choice for q (x) since the span of the components of x are −∞≤xi ≤∞, i =
1, . . . , nx.
An illustrative one-dimensional example is presented in Figure 15.5, where p (x)
is multimodal (solid line in top chart), q (x) is a Gaussian PDF that is broader than
p (x) (dashed line in top chart), and w (x) is shown in the bottom chart.
–20
–15
–10
–5
0
5
10
15
20
0
0.05
0.1
0.15
0.2
Probability
–20
–15
–10
–5
0
5
10
15
20
0
0.5
1
1.5
2
2.5
X
w(x) = p(x)/q(x)
p(X)
w(X)
q(X)
FIGURE 15.5
Example of the creation of w(x).

GENERAL CONCEPTS IMPORTANCE SAMPLING
213
At this point it must be emphasized that w (x) is NOT a density function but the
unnormalized ratio of two densities. Even when normalized, w (x) cannot be used as
a substitute for p (x). This becomes important if one is using a maximum aposteriori
(MAP) estimate of x. If the peak of q (x) does not coincide with the peak of p (x),
then the peak of w (x) is not the MAP estimate for p (x).
Now, suppose a sample x(i) is drawn from q (x). Assuming that q (x) is Gaussian
with q(x) = N

x; µ, σ2	
, we can calculate the probability of xi under p (x) in the
following manner
q

x = x(i)
=
1
√
2πσ
exp

−1
2σ2

x = x(i)
−µ
2
(15.33)
and therefore
w

x = x(i)
= p

x = x(i)	
q

x = x(i)	
(15.34)
For multidimensional samples {x(i), i = 1, . . . , N} from the known distribution q (x),
use some as yet to be determined method to generate wi for each sample and replace
p (x) by its discrete version
p (x) ≃
N

i=1
wiδ

x −x(i)
(15.35)
Consider again the integral (15.29) that can now be rewritten as
ˆf (x) =

f (x) p (x) dx
=

f (x) w (x) q (x) dx
=

f (x)
N

i=1
wiδ

x −x(i)
dx
=
N

i=1
wif

x(i)
(15.36)
This has the exact same form as (15.31), but now the weights wi are no longer
uniformly equal to 1/N. Since the weights are part of the deﬁnition of the discrete
density approximation given in (15.35), it is obvious that we must normalize the
weights so that  wi = 1.
Considertheﬁrst-orderMarkovprocessxn whoseestimatedmomentsaregoverned
by the conditional posterior density p (xn|z1:n), with an initial posterior, prior to any
observations, given by p (x0) (See Section 3.3). A general expression for the moments

214
INTRODUCTION TO MONTE CARLO METHODS
of a nonlinear function under an arbitrary posteriori pdf is given by
I (xn) =

g (xn) p (xn|z1:n) dxn
(15.37)
Here g (xn) is a functional that can represent any moment generating function. For
example, if we wish to ﬁnd the mean of a function f (xn), then we let g (xn) = f (xn)
leading to I (xn) = ˆf (xn). If we wish to compute the covariance of f (xn), let g (xn) =
[f (xn) −ˆf (xn)][f (xn) −ˆf (xn)]⊺, which leads to I (xn) = Pff
n .
Assume that p (xn|z1:n) is unknown or difﬁcult to sample and that we have an
easily sampled importance density q(xn|z1:n) that has the same support as p (xn|z1:n)
and has its maximum in roughly the same place as p (xn|z1:n). Now, (15.37) can be
written as
I (xn) =

g (xn) p (xn|z1:n)
q(xn|z1:n) q(xn|z1:n)dxn
=

g (xn) w (xn) q(xn|z1:n)dxn
(15.38)
where the weight function w (xn) is deﬁned by
w (xn) ≜p (xn|z1:n)
q(xn|z1:n)
(15.39)
Using Bayes’ rule (3.23) for p (xn|z1:n), (15.39) becomes
w (xn) = p (zn|xn) p (xn|z1:n−1)
p (zn|z1:n−1)
1
q(xn|z1:n)
(15.40)
Noting that p (zn|z1:n−1) is just a normalization term, we can write
w (xn) ∝p (zn|xn) p (xn|z1:n−1)
q(xn|z1:n)
(15.41)
where ∝represents a proportionality. Thus (15.41) formalizes the general weight
function in terms of the likelihood function p (zn|xn), the prior pdf p (xn|z1:n−1) and
the importance pdf q(xn|z1:n).
If we were to select a set of samples {x(i)
n , i = 1, . . . , Ns} from the importance
density q(xn|z1:n), we can approximate q(xn|z1:n) by the set of Dirac delta function
(see (15.5)) given by
q(xn|z1:n) = 1
Ns
Ns

i=1
δ

xn −x(i)
n

(15.42)

SUMMARY
215
and (15.38) now becomes
I (xn) =

g (xn) w (xn) 1
Ns
Ns

i=1
δ

xn −x(i)
n

dxn
= 1
Ns
Ns

i=1
w(i)
n g

x(i)
n

(15.43)
where w(i)
n ≜w(x(i)
n ). From (15.39) we obtain
w(i)
n = p

x(i)
n |z1:n
	
q(x(i)
n |z1:n)
(15.44)
Or, using (15.41), a set of unnormalized weights are given by
˜w(i)
n = p

zn|x(i)
n
	
p

x(i)
n |z1:n−1
	
q(x(i)
n |z1:n)
(15.45)
The weights are unnormalized because we have replaced the proportionality of (15.41)
by an equality. The weights can now be normalized by forcing their sum to one using
w(i)
n ≜
1/Ns ˜w(i)
n
1/Ns
Ns

i=1
˜w(i)
n
(15.46)
Now, (15.43) becomes
I (xn) =
Ns

i=1
w(i)
n g

x(i)
n

(15.47)
It is important to emphasize again that w(i)
n is not the probability of x(i)
n but rather the
importance of x(i)
n to the discrete moment integral sum.
15.3
SUMMARY
In this chapter we have presented two main concepts, methods for the representation
of a density function based on a set of samples drawn from that density, and the general
concept of Monte Carlo importance sampling in the numerical integration of functions
weightedbyanarbitrarypdf.Atpresent,apracticalmethodforestimationandtracking
cannot be presented based solely on these results and additional modiﬁcations must
be made to develop practical methods.
Tracking ﬁlters based on a sequential generation of the recursive sets of Monte
Carlo points and weights {x(i)
n , w(i)
n , i = 1, . . . , Ns, n = 1, . . . , N} have come to be
called Sequential Monte Carlo (SMC) methods. They offer a number of signiﬁcant

216
INTRODUCTION TO MONTE CARLO METHODS
advantages over many of the other techniques currently available as a result of the
generality of the SMC approach. They allow for an inference of the full posterior
distribution in terms of general state space models including both nonlinear and non-
Gaussian. SMC methods therefore allow for computation of all kinds of moments,
quantiles, and maximum posterior density regions, whereas the Gaussian Kalman
ﬁlter variants allow only approximations of the ﬁrst- and second-order moments.
In addition, by appropriately specifying the state space model, the SMC methods
can incorporate constraints on the state space such as limits on the range of the
state space, representing such physical limitations such as speed constraints or hard
boundaries.
In the next chapter we will introduce a more restrictive approach to SMC, where
the weight for each sample is generated in a sequential manner giving rise to a family
of sequential importance sampling (SIS) particle ﬁlters. Later, in Chapter 17, we will
introduce a less restrictive method that recomputes the weight functions with every
iteration.
REFERENCES
1. Richardson HR, Stone LD, Monach WR, Discenza JH. Early maritime applications of
particle ﬁltering. Signal and Data Processing of Small Targets, Proceedings of SPIE, Vol.
5204, 2003, pp. 165–174.
2. Agnew DC, Constable C. Geophysical Data Analysis. [Online] http://mahi.ucsd.edu/cathy/
Classes/SIO223/Part1/. Chapter 9. 2005.
3. Scott DW. Multivariate density estimation and visualization. In Handbook of Statistics,
Vol. 23: Data Mining and Computational Statistics. Springer; 2004.
4. Patlolla R. 2-Dimensional histogram. [Online] http://www.mathworks.com/matlabcentral/
ﬁleexchange/8422. 2005.
5. Fukunaga K. Introduction to Statistical Pattern Recognition, 2nd ed. Academic Press,
1990.
6. Silverman BW. Density Estimation for Statistics and Data Analysis. New York: Chapman
and Hall; 1986.
7. Turlach BA. Bandwidth selection in Kernel density estimation: a review. In CORE Institut
Stat 1993;23–49.
8. Musso C, Oudjane N, Le Gland F. Improving regularized particle ﬁlters. In Doucet A, de
Freita N, Gordon G, editors. Sequential Monte Carlo Methods in Practice. Springer; 2001.
9. Wand MP, Jones MC. Comparison of smoothing parameterization in bivariate Kernel
density estimation. J. Am. Stat. Soc. 1993;88(422):520–528.
10. Zhang X, King ML, Hyndman RJ. Bandwidth selection for multivariate Kernel density
estimation using MCMC. Comp. Stat. Data Anal. 2006;50:3009–3031.
11. Wikipedia. [Online] http://en.wikipedia.org/wiki/Kernel (statistics).
12. Botev Z. Two-Dimensional Kernel Density Estimator Function. [Online] http://www.
mathworks.com/matlabcentral/ﬁleexchange/17204-kernel-density-estimation.
13. Evans M, Swartz T. Approximating Integrals via Monte Carlo and Deterministic Methods.
New York: Oxford University Press; 2005.

REFERENCES
217
14. Robert CP, Casella G. Monte Carlo Statistical Methods. Springer; 2004.
15. Doucet A, de Freita N, Gordon G, editors. Sequential Monte Carlo Methods in Practice.
Springer; 2001.
16. Doucet A. On Sequential Simulation-Based Methods for Bayesian Estimation. Technical
Report CUED/F-INFENG/TR.310; 1998.
17. Liu JS, Chen R. Sequential Monte Carlo methods for dynamic systems. J. Am. Stat. Soc.
1998;93(443):1032–1044.

16
SEQUENTIAL IMPORTANCE
SAMPLING PARTICLE FILTERS
16.1
GENERAL CONCEPT OF SEQUENTIAL IMPORTANCE
SAMPLING
Remember that in all of the Gaussian Kalman ﬁlter estimators developed in Part II,
the estimates at time tn depended in an analytical way on the estimates at time tn−1.
We would like to develop a similar recursive formulation for the importance sampling
approach for estimation when the noise densities are non-Gaussian.
Recursive estimation methods based on the sequential estimation of the weights
are called sequential importance sampling (SIS) particle ﬁlters. Using the Chapman–
Kolmogorov theorem (3.24), p (xn|z1:n−1) and q(xn|z1:n) in (15.41) can be expanded
so that (15.41) becomes
w (xn) ∝p (zn|xn)

p (xn|xn−1, z1:n−1) p (xn−1|z1:n−1) dxn−1

q (xn|xn−1, z1:n) q (xn−1|z1:n−1) dxn−1
(16.1)
The reason we have q (xn−1|z1:n−1) in the denominator instead of q (xn−1|z1:n) results
from the fact that xn−1 cannot be conditioned on a future observation. Since the state
evolution equation is assumed to be a ﬁrst-order Markov process independent of the
observations, we can let p (xn|xn−1, z1:n−1) →p (xn|xn−1) resulting in
w (xn) ∝p (zn|xn)

p (xn|xn−1) p (xn−1|z1:n−1) dxn−1

q (xn|xn−1, z1:n) q (xn−1|z1:n−1) dxn−1
(16.2)
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
218

GENERAL CONCEPT OF SEQUENTIAL IMPORTANCE SAMPLING
219
For a Markov process, we can also make the assumption that
q (xn|xn−1, z1:n) = q (xn|xn−1, zn)
(16.3)
so that the importance density depends on only xn−1 and zn.
Now assume that we have a set of random samples {x(i)
n−1, i = 1, . . . , Ns} from
q (xn−1|z1:n−1) (in the last chapter, we generated samples from q (xn|z1:n)), that is,
x(i)
n−1 ∼q (xn−1|z1:n−1). Now, (16.2) becomes
˜w (xn) =
Ns

i=1
p (zn|xn) p

xn|x(i)
n−1

p

x(i)
n−1|z1:n−1

q

xn|x(i)
n−1, zn

q

x(i)
n−1|z1:n−1

(16.4)
From the deﬁnition of w (xn) given in (15.39), it follows that
˜w (xn) =
Ns

i=1
w(i)
n−1
p (zn|xn) p

xn|x(i)
n−1

q

xn|x(i)
n−1, zn

(16.5)
This leads to an expression for the posterior distribution
p (xn|z1:n) =
Ns

i=1
w(i)
n−1
p

xn|x(i)
n−1

q

xn|x(i)
n−1, zn
p (zn|xn) q(xn|z1:n)
(16.6)
Generating samples {x(i)
n , i = 1, . . . , Ns} from q

xn|x(i)
n−1, zn

such that
x(i)
n ∼q

xn|x(i)
n−1, zn

(16.7)
q(xn|z1:n) = 1
Ns
δ

xn −x(i)
n

(16.8)
results in
p (xn|z1:n) =
Ns

i=1
˜w(i)
n−1
Ns
p

x(i)
n |x(i)
n−1

q

x(i)
n |x(i)
n−1, zn
p

zn|x(i)
n

δ

xn −x(i)
n

(16.9)
After normalizing ˜w(i)
n−1/Ns using
w(i)
n−1 =
˜w(i)
n−1/Ns
Ns
i=1 ˜w(i)
n−1/Ns
(16.10)

220
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
we obtain
p (xn|z1:n) =
Ns

i=1
w(i)
n δ

xn −x(i)
n

(16.11)
where the SIS recursive weight update equation is given by
w(i)
n ≜w(i)
n−1
p

x(i)
n |x(i)
n−1

q

x(i)
n |x(i)
n−1, zn
p

zn|x(i)
n

(16.12)
Returning to the general moment Equation (15.37), the ﬁrst two moments of xn,
with respect to p (xn|z1:n) can be obtained from
ˆxn ≜E {xn|z1:n} =

R
xnp (xn|z1:n) dxn
=

R
xn
Ns

i=1
w(i)
n δ

xn −x(i)
n

=
Ns

i=1
w(i)
n x(i)
n
(16.13)
and similarly
Pxx
n ≜E

(xn −ˆxn) (xn −ˆxn)⊺|z1:n

=
Ns

i=1
w(i)
n

x(i)
n −ˆxn
 
x(i)
n −ˆxn
⊺
(16.14)
The procedure for the SIS particle ﬁlter is quite simple and is presented in
Table 16.1.
There are several signiﬁcant practical problems hidden in the SIS particle ﬁlter
procedure. The ﬁrst problem occurs in Step 2 of the SIS procedure, where analytical
expressions for the likelihood function p (zn|xn), predictive density p (xn|xn−1) and
importance density q (xn|xn−1, zn) are required. The likelihood function is based
on the observation noise density that is usually assumed to be Gaussian. However,
when the observation equation is highly nonlinear, this assumption can easily be
violated, as has been shown in Section 4.4 for the DIFAR buoy case study. The
predictive density is based on the noise in the dynamic transition equation and may
be Gaussian, non-Gaussian or completely unknown. Finally, the importance density is
usually some known analytical density from which samples can be easily drawn. But
note that q (xn|xn−1, zn) is conditioned on the current observation. For this reason,
the importance density can usually be taken as the posterior density at the output
of a nonlinear Gaussian Kalman ﬁlter and applied to each particle where the update

GENERAL CONCEPT OF SEQUENTIAL IMPORTANCE SAMPLING
221
TABLE 16.1
General Sequential Importance Sampling Particle Filter
A. Initialize ﬁlter
Initialize state vector samples
x(i)
0 ∼q (x0) , i = 1, . . . , Ns
Initialize weights
w(i)
0 =
1
Ns
B. Sequential importance sampling
1. Draw new samples
x(i)
n ∼q 	
xn|x(i)
n−1, zn

, ∀i
2. Generate unnormalized importance weights
˜w(i)
n = w(i)
n−1
×
p	
zn|x(i)
n

p	
x(i)
n |x(i)
n−1

q	
x(i)
n |x(i)
n−1,zn
3. Normalize the weights
w(i)
n = ˜w(i)
n /
N

i=1
˜w(i)
n
4. Time step and return to 1
x(i)
n →x(i)
n−1
w(i)
n →w(i)
n−1
C. Calculate moments if desired
First moment: state vector mean
ˆxn = Ns
i=1 w(i)
n x(i)
n
Second moment: covariance
Pxx
n = Ns
i=1 w(i)
n
×	
x(i)
n −ˆxn

	
x(i)
n −ˆxn

⊺
equations use the latest observations. Consideration of issues related to these three
densities for SIS particle ﬁlters are very important and must be considered carefully.
The second problem with SIS particle ﬁlters concerns the divergence of the parti-
cles. Notice that particles are generated only during the initialization step. Although
Step 2 appears to be a resampling step, in practice, usually a nonlinear Gaussian
Kalman ﬁlter is used for the importance density with each particle processed sepa-
rately. In this case, the original particles are just passed through without resampling.
Thus, the original particles are propagated through Steps 1–4, but no new particles
are created after particle creation during initialization. It is for this reason that these
Monte Carlo estimation methods have the name particle ﬁlter, since the particles tra-
jectories represent temporal paths through the state space. The particles are created as
samples from some initializing importance density and then propagated through the
SIS process over and over again, with noise added at each iteration. This causes the
spread of the particle based discrete posterior density to diverge from that of the true
posterior density, reducing the weights to zero for of all but a few central particles.
This problem is often referred to as the SIS degeneracy problem. After each particle
propagation, the weights for all particles are used to estimate the moments, but fewer
and fewer particles have a high enough weight (importance) to contribute to the dis-
crete sum, increasing the variance of the moment estimates. Reducing this growth
in estimation variance caused by this effect constitutes the bulk of SIS particle ﬁlter
methodology.
It should also be noted that the initialization importance density q (x0) need not
even be the same as the importance density q (xn|xn−1, zn) used in SIS Step 1. One

222
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
wants the initialization density q (x0) to be as uninformative as possible so as to
include Monte Carlo samples in all possible regions of the posterior. But one wants
q (xn|xn−1, zn) to be highly localized through the use of the latest observations zn.
16.2
RESAMPLING AND REGULARIZATION (MOVE) FOR SIS
PARTICLE FILTERS
The SIS sample degeneracy problem can be moderated by a process called resampling.
The basic idea of resampling is to eliminate state space particles with small weights
by replacing them with replicated values of particles with larger weights. For this
process, the weights are treated as a one-dimensional discrete probability distribution
and the inverse transform method [1,2] is used to resample the particles associated
with the inverse of the cumulative distribution of the weights.
However, this leads to a new problem, a loss of diversity among the particles, since
the resultant sample set will contain many repeated particles for any given weight. This
problem has been labeled in the literature as sample impoverishment. In severe cases,
all particles migrate to one sample point. To rectify the sample impoverishment due
to resampling, after each resampling process a kernel density estimate of the particle
density can be used to resample the particles a second time. In this process, each new
particle (Monte Carlo sample) is selected from the resampled particles based on a draw
from a uniform distribution and then the sample point is moved a small amount based
on a draw from the local kernel. This process tends to concentrate the particles in the
region of highest probability and separates them in a random fashion. This method
of reducing sample impoverishment is called regularization [3] and constitutes the
move part of a resample and move process.
There are several alternatives to the resample and move method discussed above
[4–8], including a Markov Chain Monte Carlo (MCMC) sampling method that utilizes
the Metropolis–Hastings acceptance algorithm instead of a regularization step and a
Gibbs sampling method similar to MCMC. In general, these methods prove to be
too computationally intensive for real-time recursive tracking applications and will
therefore not be considered here. However, in cases where the posterior density has
large tail probabilities, as is the case for alpha-stable distributions such as a Levy
distribution, the standard SIS particle ﬁlter methods may fail due to difﬁculties in
the selection of an appropriate importance density. In such instances, the use of an
MCMC method in particle ﬁlters provides an alternative for building efﬁcient high
dimensional proposal distribution. For a discussion of the MCMC particle ﬁlter, see
Ref. [4] and the references contained therein. Other applications where these methods
are useful are static parameter estimation and smoothing methods [5], neither of which
will be addressed in this book.
16.2.1
The Inverse Transform Method
First, let’s examine the one-dimensional cumulative distribution function P (x) shown
in Figure 2.7 for the analytical Gaussian distribution N
	
−5, 62
. From the ﬁgure it

RESAMPLING AND REGULARIZATION (MOVE) FOR SIS PARTICLE FILTERS
223
0
0.2
0.4
0.6
0.8
1
–30
–20
–10
0
10
20
30
ui
xi
FIGURE 16.1
Inverse of the Gaussian cumulative distribution.
is obvious that the cumulative probabilities associated with the analytical CDF are
uniformly distributed over the interval (0,1]. If we invert the CDF, we obtain P−1 (u),
which is shown in Figure 16.1. Now, if we generate a sample (ui) from a uniform
distribution on (0,1] and use it to select a value along the vertical x-axis (xi), for this
example we would generate a sample from N
	
−5, 62
, as shown in the ﬁgure. Note
that for this example, very few samples of x will be selected outside of the range
−5 −13 ≤x ≤−5 + 13.
In general, the cumulative distribution and its inverse have the following properties
[1,2,9]:
1. P−1 (U) is monotonically increasing
2. P
	
P−1 (U)

≥u and P
	
P−1 (X)

≤x
3. u ≤P (X) if and only if P−1 (U) ≤x
4. P−1 is left continuous
Theorem 16.1
if u ∼U (0, 1), then x ∼P−1 (U) = P (X)
Proof: P (X ≤x) = P
	
X ≤P−1 (U)

= P
	
P−1 (u)

= u = P (X)
■
This general theorem applies to both analytical and empirical one-dimensional
distributions only. It can only be applied to multidimensional distributions in some
special cases. We illustrate this inverse distribution sampling method with a simple
example. First we use Matlab to generate 50,000 samples from N
	
−5, 62
using the
method described in Chapter 2. Next we form an empirical pdf by creating a 13 bin
histogram that is normalized by the total number of samples. A CDF is then formed
from a bin by bin cumulative sum. Figure 16.2 is then formed by plotting P (x) versus

224
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
0
0.2
0.4
0.6
0.8
1
–30
–20
–10
0
10
20
30
–30
–20
–10
0
10
20
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
P(x)
P –1(xi)
ui
x = P –1(u)
P(xi)
xi
FIGURE 16.2
Comparison of a Gaussian CDF with its inverse.
x and x versus P−1 (u). Note that the process of resampling for this simple example
can generate only seven unique values of x.
There are several numerical approaches to implementing the inverse method for
sample generation. We will consider only three here: random sampling, stratiﬁed
sampling and systematic sampling. However, research has shown that the “best”
results are obtained with either stratiﬁed or systematic sampling [10].
For random resampling, one generates samples {ui, i = 1, . . . , n} from a uni-
form distribution U(0, 1) and then selects the values {xi, i = 1, . . . , n} such that
xi = P−1 (ui). Some simple Matlab code that accomplishes this is presented in
Listing 16.1.
Listing 16.1 Random Sampling Matlab Snippet
% Random Resampling
u = cumprod(rand(1,Ns)./[Ns:-1:1]);
u = u(N:-1:1);
wc = cumsum(w);
label = nan(1,Ns);
k = 1;
for i = 1:Ns
while(wc(k) < u(i))
k = k+1;
end
label(i) = k;
end
% Resample particles
particles = particles(:,label);
w = ones(1,Ns)./Ns;

RESAMPLING AND REGULARIZATION (MOVE) FOR SIS PARTICLE FILTERS
225
In stratiﬁed resampling, based on ideas used in survey sampling, the interval (0, 1]
is prepartitioned into n uniform adjoint sets or strata, (0, 1] = (0, 1/n] ∪(1/n, 2/n] ∪
· · · ∪(n −1/n, 1]. Then, in each strata, a separate uniform random number is gener-
ated such that for the ith strata, ui ∼U (i/n, i + 1/n). Then one sets xi = P−1 (ui).
Some simpliﬁed Matlab code that accomplishes this is shown in Listing 16.2.
Listing 16.2 Stratiﬁed Sampling Matlab Snippet
% Stratified Resampling
u = ([0:Ns - 1] + rand(1,Ns)/Ns;
wc = cumsum(w);
label = nan(1,Ns);
k = 1;
for i = 1:Ns
while(wc(k) < u(i))
k = k+1;
end
label(i) = k;
end
% Resample particles
particles = particles(:,label);
w = ones(1,Ns)./Ns;
Finally, in systematic resampling, the uniform region is again subdivided into n
uniform adjoint strata, as in stratiﬁed sampling, but only a single random number
draw is used for all strata. Listing 16.3 contains a snippet of Matlab code that accom-
plishes this.
Listing 16.3 Systematic Sampling Matlab Snippet
% Systematic Resampling
u = ([0:Ns - 1] + rand(1,Ns))/Ns;
wc = cumsum(w);
label = nan(1,Ns);
k = 1;
for i = 1:Ns
while(wc(k) < u(i))
k = k+1;
end
label(i) = k;
end
% Resample particles
particles = particles(:,label);
w = ones(1,Ns)./Ns;
For all of the particle ﬁlters requiring resampling used in our case studies, we used
systematic resampling.

226
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
16.2.2
SIS Particle Filter with Resampling
The inverse transform method can be used for resampling in SIS particle ﬁlters. We
show this by ﬁrst forming the discrete approximation of the cumulative distribution
P (xn|z1:n) for the discrete p (xn|z1:n) shown in (16.11). From the deﬁnition of the
cumulative distribution given in (2.80) we can write
P

x(j)
n |z1:n

= Pr

yn|z1:n ≤x(j)
n |z1:n

=
 x(j)
n
−∞
p (yn|z1:n) dyn
=
 x(j)
n
−∞
Ns

i=1
w(i)
n δ

yn −y(i)
n

dyn
=
j

i=1
w(i)
n
(16.15)
Thus, each particle of the discrete p (xn|z1:n) contributes its weight to the CDF pri-
marily because the CDf is obtained from the PDF through integration. This is a very
interesting conclusion. In Chapter 2, we presented a multidimensional CDF where the
components of the CDF contained the same number of dimensions as the PDF. For the
discrete p (xn|z1:n), although the state vector xn and p (xn|z1:n) are multidimensional,
a one-dimensional CDF can be formed from just the weights. It is for this reason we
can utilize the inverse transform method to resample the discrete multidimensional
p (xn|z1:n).
Theseparticleﬁlterresamplingstepsmodifytheweightedapproximationdensityto
create an unweighted density by eliminating particles having low importance weights.
Each low weight particle eliminated is replaced by replicating a particle having a
higher importance weight, so that the number of particles remains constant. Thus
p (xn|z1:n) =
Ns

i=1
w(i)
n δ

xn −x(i)
n

(16.16)
is replace by
p
	
x∗
n|z1:n

=
Ns

i=1
1
Ns
δ
	
xn −x∗
n

=
Ns

i=1
ni
Ns
δ

xn −x(i)
n

(16.17)
where ni is the number of copies of particle x(i)
n in the resampled set of particles

x∗
n

. See Ref. [10] for a complete evaluation of these and other SIS particle ﬁlter
resampling methods.
In many applications of SIS particle ﬁlters, the degeneracy caused by migration
of the spread of the discrete particle density away from that of the true density is
not severe, so resampling need not be done on every iteration. A suitable measure
of degeneracy for a speciﬁc application is the effective sample size Neff that can be

RESAMPLING AND REGULARIZATION (MOVE) FOR SIS PARTICLE FILTERS
227
TABLE 16.2
Sequential Importance Sampling Particle Filter with Resampling
A. Initialize ﬁlter
1. Initialize state vector samples
x(i)
0 ∼q (x0), i = 1, . . . , Ns
2. Initialize weights
w(i)
0 =
1
Ns
B. Sequential importance sampling
1. Draw new samples
x(i)
n ∼q 	
xn|x(i)
n−1, zn

, ∀i
2. Generate unnormalized importance weights
˜w(i)
n = w(i)
n−1
×
p	
zn|x(i)
n

p	
x(i)
n |x(i)
n−1

q	
x(i)
n |x(i)
n−1,zn
3. Normalize the weights
w(i)
n = ˜w(i)
n /
Ns

i=1
˜w(i)
n
C. Calculate Neff
Neff =
1
Ns
i=1
	
w(i)
n

2
D. If Neff ≪Ns
1. Resample
Resample using one of the
resampling Matlab example
snippets
2. Reset the weights
w(i)
n = 1/Ns
end if
E. Calculate moments if desired
1. First moment: state vector mean
ˆxn = Ns
i=1 w(i)
n x(i)
n
2. Second moment: covariance
Pxx
n = Ns
i=1 w(i)
n
×	
x(i)
n −ˆxn

	
x(i)
n −ˆxn

⊺
F. Time step and return to B.1
x(i)
n →x(i)
n−1; w(i)
n →w(i)
n−1
estimated from [11]
Neff =
1
Ns
i=1

w(i)
n
2
(16.18)
It follows immediately that if the weights are all uniform, w(i)
n = 1/Ns, and Neff = Ns.
However, if there is serious degeneracy, w(i)
n = 1 for one value of i and is zero for all
others, so Neff = 1. Hence Neff ≪Ns indicates severe degeneracy while Neff ∼Ns
indicates very little degeneracy.
The procedure for a resampling SIS particle ﬁlter is presented in Table 16.2.
16.2.3
Regularization
After resampling, the resultant sample set of particles may contain many repeated
particles for any given weight. This problem has been labeled in the literature as
sample impoverishment. To rectify the sample impoverishment due to resampling,

228
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
1
2
3
4
5
6
7
8
9
10
11
1
2
3
3
4
4
5
5
6
6
7
7
8
8
9
9
10
10
11
11
1
2
x
Posterior
density
Importance
density
FIGURE 16.3
Principle of resampling. Top: Samples drawn from q (dashed line) with as-
sociated normalized importance weights depicted by bullets with radii proportional to the
normalized weights (the target density corresponding to p is plotted as a solid line). Middle:
After resampling, all particles have the same importance weight, and some have been dupli-
cated. Bottom: Samples have been regularized using a Kernel density estimator that resamples
and moves.
after each resampling process a kernel density estimate of the particle density can be
used to resample the particles a second time. In this process, each new particle (Monte
Carlo sample) is selected from the original resampled particles based on a draw from
a uniform distribution and then the sample point is moved a small amount (dithered)
based on a draw from the local individual kernel. This regularization process tends to
concentrate the particles in the region of highest probability and separates them in a
random fashion. An example of the complete results of this procedure is depicted on
Figure 16.3.
In Figure 16.3, both the posterior density and the importance density are shown.
Samples are drawn from the importance density and their position along the x-axis
is depicted with bullets. Each particle is assigned an unnormalized weight given
by the ratio w(i)
n = p(x(i)
n )/q(x(i)
n ), with the bullet radii shown as the relative size of

RESAMPLING AND REGULARIZATION (MOVE) FOR SIS PARTICLE FILTERS
229
the weights in the top line of Figure 16.3. The numbers are place holders used to
show ancestral lineage, that is, where each individual particle ends up at the end
of each step. The second line shows the particles after resampling, with all particles
having equal weight and the lower weight particles turned into replications of existing
higher weight samples. The third line depicts the samples after regularization, which
resamples again and moves the particles so that no two are at the exact same value of x.
The procedure for a resample and regularize (move) SIS particle ﬁlter for a multi-
dimensional state vector is presented in Table 16.3. However, note that computation
of the empirical covariance matrix Ds must be carried out prior to the resampling step
and is therefore a function of both x(i)
n and w(i)
n . Since the regularization step resamples
from the posterior density, all particles now have equal importance in the summation
TABLE 16.3
Sequential Importance Sampling Particle Filter with Resampling and
Regularization
A. Initialize ﬁlter
1. Initialize state vector samples
x(i)
0 ∼q (x0) , i = 1, . . . , Ns
2. Initialize weights
w(i)
0 =
1
Ns
B. Sequential importance sampling
1. Draw new samples
x(i)
n ∼q 	
xn|x(i)
n−1, zn

, ∀i
2. Generate unnormalized importance weights
˜w(i)
n = w(i)
n−1
×
p	
zn|x(i)
n

p	
x(i)
n |x(i)
n−1

q	
x(i)
n |x(i)
n−1,zn

3. Normalize the weights
w(i)
n = ˜w(i)
n /
Ns

i=1
˜w(i)
n
C. Calculate Neff
Neff =
1
Ns
i=1
	
w(i)
n

2
D. If Neff ≪Ns
a. Calculate the empirical mean
µs =
1
Ns
Ns
i=1 x(i)
n
b. Calculate the empirical covariance
s =
1
Ns
Ns
i=1
	
x(i)
n −µs

×	
x(i)
n −µs

⊺
s = DsD⊺
s
c. Resample the particles
Resample using one of the
resampling Matlab example
snippets
d. Regularize the resampled particles
Regularize the particles
using the procedure from
Table 15.3
e. Time step and return to B.1
x(i)
n →x(i)
n−1; w(i)
n = 1/N
Else
Time step and return to B.1
x(i)
n →x(i)
n−1; w(i)
n →w(i)
n−1
end if

230
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
approximation of the moment integrals, so all of the weights are reset to w(i)
n = 1/N.
At the end of each iteration, the moments can be calculated prior to stepping in time.
One can see that the SIS particle ﬁlter with resampling and regularization is still
not in a form that can be applied to a tracking problem. We still have the problem of
identifying a suitable importance distribution q (xn|xn−1, zn) and the analytical form
of the likelihood function p (zn|xn).
16.3
THE BOOTSTRAP PARTICLE FILTER
One of the easiest to implement, and thus one of the most widely used, resampling
SIS particle ﬁlters is the bootstrap particle ﬁlter (BPF) introduced in [12]. In the BPF,
the transition density is chosen as the importance density, that is
q (xn|xn−1, zn) = p (xn|xn−1)
(16.19)
For this choice of importance density, the weight update equation (16.12) becomes
˜w(i)
n = w(i)
n−1p

zn|x(i)
n

(16.20)
The BPF has the distinctive feature that the incremental weights do not depend
on the past trajectory of the particles but only on the conditional likelihood of the
observation p (zn|xn). For the BPF, sampling is very straightforward, with the state
transition equation (5.1) used to generate new particles by merely transitioning the
particle from the previous time step. This is usually followed by the resample and
move steps. The complete procedure for the BPF is shown in Table 16.4. Once again,
the ﬁrst and second moments of the distribution can be calculated at the end of each
time step.
One important point to note in Table 16.4 is the abscence of the noise covariance
matrix Q in the state covariance computation. This is not needed here because samples
from the noise term v(i)
n ∼p (vn) are included in the transition equation for the state
vector. Thus, there is also the unstated need for knowledge of the dynamic noise
density.Sinceouroriginalassumptionwasforanunknownposteriordensity,assuming
an analytical form for the dynamic noise is a violation of our original assumption.
The usual method to get around this is to ignore the noise term and transition the
particles without adding the noise.
There still remains the need for an analytical expression for the likelihood function
p (zn|xn).Asnotedpreviously,p (zn|xn)isusuallytakentobeGaussian.Butforhighly
nonlinear observation equations, the observation noise can be non-Gaussian, as shown
in the DIFAR case study.
One of the main problems with application of the BPF to real-world problems
occurs when p (zn|xn) is a very narrow function, as would occur if the variances of
the observation noise are small. When this occurs, many of the transitioned particles
will produce very small values for p(zn|x(i)
n ), making their corresponding updated
weights very small. For such cases, the resample and move steps should be executed

THE BOOTSTRAP PARTICLE FILTER
231
TABLE 16.4
Bootstrap SIS Particle Filter with Resampling and Regularization
A. Initialize ﬁlter
1. Initialize state vector samples
x(i)
0 ∼q (x0) , i = 1, . . . , Ns
2. Initialize weights
w(i)
0 =
1
Ns
B. Sequential importance sampling
1. Draw new samples
v(i)
n ∼p (vn)
x(i)
n = f 	
x(i)
n−1

+ v(i)
n , ∀i
2. Generate unnormalized importance weights
˜w(i)
n = w(i)
n−1p	
zn|x(i)
n

3. Normalize the weights
w(i)
n = ˜w(i)
n /
Ns

i=1
˜w(i)
n
C. Calculate Neff
Neff =
1
Ns
i=1
	
w(i)
n

2
D. If Neff ≪Ns
a. Calculate the empirical mean
µs =
1
Ns
Ns
i=1 x(i)
n
b. Calculate the empirical covariance
s =
1
Ns
Ns
i=1
	
x(i)
n −µs

×	
x(i)
n −µs

⊺
s = DsD⊺
s
c. Resample the particles
Resample using one of the
resampling Matlab example
snippets
d. Regularize the resampled particles
Regularize the particles
using the procedure from
Table 15.3
e. Time step and return to B.1
x(i)
n →x(i)
n−1; w(i)
n = 1/N
Else
Time step and return to B.1
x(i)
n →x(i)
n−1; w(i)
n →w(i)
n−1
end if
for every update. On the other hand, if p (zn|xn) is assumed to be Gaussian when in
fact it is not, uninformative outliers can occur in the observations such that p(zn|x(i)
n )
is zero for all particles resulting in an unstable ﬁlter.
16.3.1
Application of the BPF to DIFAR Buoy Tracking
In Chapter 4, we presented the likelihood function p
	
θn,m|ϑm

for a DIFAR buoy
(4.34). This likelihood function represented the likelihood of the mth DIFAR element
bearing measurement at time tn, θn,m, conditioned on the whole space of possible
target bearings {−π ≤ϑm ≤π} as a function of the signal SNR at the buoy input. We
showed that the normalized likelihood was Gaussian for high SNR and approached a
uniform distribution for low SNR. This should not be a surprising results since, in the
absence of a signal (low SNR), the input to the buoy would be uniformly distributed

232
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−4
−3
−2
−1
0
1
2
3
x-position (nmi)
y-position (nmi)
FIGURE 16.4
Target track generated from a DIFAR buoy ﬁeld using a bootstrap particle
ﬁlter.
noise over −π ≤θn,m ≤π. Assuming that the noise is independent from buoy to
buoy, we can write the buoy ﬁeld likelihood function as the product of the individual
buoy likelihood functions and use it in Step 2 of BPF process shown in Table 16.4.
To initialize the BPF we use the same procedure outlined in Section 7.4.3. Af-
ter computing an estimate of the initial target state x0, a set of initial particles

x(i)
0 , i = 1, . . . , Ns

are generate such that
x(i)
0 = x0 + v(i)
n
(16.21)
with v(i)
n ∼N (0, Q). The remaining steps in Table 16.4 are then carried out sequen-
tially to produce the estimated target track plotted against the truth track as shown in
Figure 16.4. For this case, we left the SNR at 20 dB and initialized the range at 4 nmi
and the speed at 30 knots (true initial range and speed were approximately 3.5 nmi
and 30 knots, respectively). For this example, we set the number of particles to 1000.
The BPF track outputs for six different input signal SNRs (20 dB down to
−5 dB in increments of 5 dB) with the same initialization as all of the previous
DIFAR track estimation plots are presented in Figure 16.5. For this example, 3000
particles were used for the BPF. Comparison of a similar plot for the EKF shown in
Figure 7.3 reveals that the EKF produces more accurate tracks than the BPF at all
SNRs. In addition, the 100 BPF Monte Carlo runs produced divergent tracks at every
SNR. Some of the reasons for this lack of performance for the BPF are discussed in
the next paragraph.
Unlike the Gaussian Kalman ﬁlters, the performance of the BPF was highly sen-
sitive to the range initialization value. If we initialized the range to any value greater

THE OPTIMAL SIS PARTICLE FILTER
233
–60
–40
–20
0
20
40
–150
–100
–50
0
50
x-position (nmi)
y-position (nmi)
SNR 20; 3 divergent tracks
–10
–8
–6
–4
–2
0
2
× 1015
–2
–1
0
1× 1016
x-position (nmi)
y-position (nmi)
SNR 15; 7 divergent tracks
–1
0
1
2
3
4
5
6
× 1021
–5
0
5
10
15× 1021
x-position (nmi)
y-position (nmi)
SNR 10; 18 divergent tracks
–4
–3
–2
–1
0
1
× 1033
–15
–10
–5
0
5× 1033
x-position (nmi)
y-position (nmi)
SNR 5; 27 divergent tracks
–1
–0.5
0
0.5
1
1.5
× 1034
–4
–2
0
2× 1034
x-position (nmi)
y-position (nmi)
SNR 0; 61 divergent tracks
–20
–15
–10
–5
0
5
× 1033
–2
0
2
4× 1034
x-position (nmi)
y-position (nmi)
SNR –5; 93 divergent tracks
FIGURE 16.5
A comparison of track outputs at six different SNRs for the BPF tracker.
than 5 nmi, the BPF tracks diverged. Increasing the number of particles decreased this
problem somewhat, but we have found that the BPF can be difﬁcult to use because
of this initialization sensitivity. There appeared to be little sensitivity to variations in
the initial speed. We even tried initializing the particles by choosing uninformative
uniformly distributed ranges and speeds, over the range 2–5.9 in increments of 0.1
nautical miles for range and 24–30 in increments of 0.5 knots for speed, with most
of the ensuing track estimates highly divergent. Although rarely mentioned in the
literature, this initialization sensitivity makes the BPF difﬁcult to use in a real-world
trackingscenario,unlessanalternatemethodisusedtogenerate“good”initializations.
In addition, it must be noted that for the BPF algorithm, the importance density does
not take into account the current observation, making the BPF a highly suboptimal
ﬁltering choice. Although very easy to implement, it has some serious deﬁciencies
relative to some of the particle ﬁlter methods to be presented below.
16.4
THE OPTIMAL SIS PARTICLE FILTER
To overcome some of the problems of the BPF, one needs to choose the importance
density more wisely. Thus, the choice of an importance density is the most critical
issue in the design of a particle ﬁlter. The optimal choice for an importance density
is to choose one that minimizes the variance of the weights. Consider the mean and

234
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
variance of the weights as follows:
Eq	
x(i)
n |x(i)
n−1,zn

 
w∗(i)
n

=

w∗(i)
n q

x(i)
n |x(i)
n−1, zn

dxn
(16.22)
Var
q	
x(i)
n |x(i)
n−1,zn


w∗(i)
n

= Eq	
x(i)
n |x(i)
n−1,zn


w∗(i)
n
2
−

Eq	
x(i)
n |x(i)
n−1,zn

 
w∗(i)
n
2
(16.23)
If we ﬁrst examine the mean equation (16.22), substitution of (16.12), the recursion
equation for w∗(i)
n , results in
Eq	
x(i)
n |x(i)
n−1,zn

 
w∗(i)
n

= w∗(i)
n−1

p (zn|xn) p

xn|x(i)
n−1

dxn
= w∗(i)
n−1p

zn|x(i)
n−1

(16.24)
Using the same procedure on the ﬁrst term in the variance equation (16.23) and using
(16.24) to evaluate the second term in (16.23), we obtain
Var
q	
x(i)
n |x(i)
n−1,zn

w∗(i)
n

=

w∗(i)
n−1
2



 
p (zn|xn) p

xn|x(i)
n−1

q

xn|x(i)
n−1, zn

dxn
−

p

zn|x(i)
n−1
2



(16.25)
It follows immediately from Bayes’ law that (16.25) reduces identically to zero
[13] if
q

xn|x(i)
n−1, zn

= p

xn|x(i)
n−1, zn

(16.26)
=
p

zn|xn, x(i)
n−1

p

xn|x(i)
n−1

p

zn|x(i)
n−1

(16.27)
Now, the expression for the importance weight update (16.12) becomes
˜w(i)
n = w(i)
n−1
p
	
zn|x(i)
n

p

x(i)
n |x(i)
n−1

p

x(i)
n |x(i)
n−1, zn

(16.28)
= w(i)
n−1p

zn|x(i)
n−1

(16.29)
Comparing the BPF weight Equation (16.20) with that of the OPF (16.29) we see
that the OPF importance weight ˜w(i)
n no longer depends on x(i)
n . Thus, the importance
weights at time tn can be computed before the particles are propagated to time tn.

THE OPTIMAL SIS PARTICLE FILTER
235
Use of the optimal importance density suffers from two drawbacks [14]. First,
one must be able to sample from the importance density p

xn|x(i)
n−1, zn

, and then
one must be able to evaluate p

zn|x(i)
n−1

up to a normalization constant, where
p

zn|x(i)
n−1

is given by the integral
p

zn|x(i)
n−1

=

p (zn|xn) p

xn|x(i)
n−1

dxn
(16.30)
In general, either task may prove very difﬁcult in the general case.
However, there are some interesting special cases where the optimal SIS particle
ﬁlter can be used. As noted in Refs [3,15], the ﬁrst special case occurs when xn is a
member of a ﬁnite set. The integral in (16.30) then becomes a sum and sampling from
p

xn|x(i)
n−1, zn

then becomes possible. The example given for such a special case
is that of a linear jump-Markov process applied to a maneuvering target. The second
case, to be treated below, is a class of models for which p

xn|x(i)
n−1, zn

is Gaussian
[13,14].
16.4.1
Gaussian Optimal SIS Particle Filter
Consider the case where the system dynamic transition model is nonlinear with ad-
ditive Gaussian noise and the observation equation is linear with additive Gaussian
noise. Such a system can be characterized by
xn = f (xn−1) + vn−1
(16.31)
zn = Hnxn + wn
(16.32)
where
vn ∼N (0, Qn)
(16.33)
wn ∼N (0, Rn)
(16.34)
Now the densities p (xn|xn−1) and p (zn|xn) can be identiﬁed as
p (xn|xn−1) = N
	
xn; f (xn−1) , Qn−1

(16.35)
p (zn|xn) = N (zn; Hnxn, Rn)
(16.36)
From Bayes’ Law, we can write
p (xn|xn−1, zn) = p (zn|xn) p (xn|xn−1)
p (zn|xn−1)
(16.37)
which results in the equality
p (zn|xn) p (xn|xn−1) = p (xn|xn−1, zn) p (zn|xn−1)
(16.38)

236
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
Since the product on the left is a product of Gaussian distributions, the product on the
right must also be a product of Gaussians. Therefore, let
p (xn|xn−1, zn) = N (xn; an, n)
(16.39)
p (zn|xn−1) = N (zn; bn, Sn)
(16.40)
Now (16.38) can be written in terms of the Gaussian densities as
N (zn; Hnxn, Rn) N (xn; f (xn−1), Qn−1) = N (zn; bn, Sn) N (xn; an, n)
(16.41)
Equating the terms in the exponents of the analytical Gaussian densities, it follows
(after much algebra) that
bn = Hnf (xn−1)
(16.42)
Sn = HnQn−1H⊺
n + Rn
(16.43)
an = f (xn−1) + nH⊺
nR−1
n (zn −bn)
(16.44)
n = Qn−1 −Qn−1H⊺
nS−1
n HnQn−1
(16.45)
Note that if Qn−1, Rn, and Hn are independent of time, then Sn and n are also time
independent and can be computed ofﬂine.
Remembering that this is a particle ﬁlter, we can insert the particles {x(i)
n−1, i =
1, . . . , N} into (16.42) and (16.44) and thus we must let bn →b(i)
n and an →a(i)
n .
The complete procedure for the OPF is shown in Table 16.5. In the table, we have
used the notation x(i)
n|n−1 to indicate the predicted particle in the State Prediction
and Intermediate calculations steps. Also, in the Intermediate calculations step, zn
represents the observations at time tn. Since this is a SIS particle ﬁlter, resampling
is required and we have assumed that resampling occurs at every time step. This can
be modiﬁed to only resample when Neff ≪Ns if desired. Note that the importance
weights at time tn are calculated in the weight update step before the particles are
propagated forward in the importance sampling step. In addition, the old particles
can also be resampled and the weights reset to 1/Ns before particles are propagated
forward in time.
16.4.2
Locally Linearized Gaussian Optimal SIS Particle Filter
When both the dynamic and observation equations are nonlinear, the OPF can still be
used if the nonlinear observation equation is linearized in some way [15]. Consider
an observation equation of the form
zn = h (xn) + wn
(16.46)
Expanding h (xn) in a multidimensional Taylor series about f (xn−1) using (2.62)
results
h (xn) ≃h (f (xn−1)) + ˆHn (xn −f (xn−1))
(16.47)

THE OPTIMAL SIS PARTICLE FILTER
237
TABLE 16.5
Optimal SIS Particle Filter with Resampling and Regularization
A. Initialize ﬁlter
1. Initialize state vector samples
x(i)
0 ∼q (x0), i = 1, . . . , Ns
2. Initialize weights
w(i)
0 =
1
Ns
B. State prediction
x(i)
n|n−1 = f 	
x(i)
n−1

b(i)
n = Hnx(i)
n|n−1
Sn = HnQn−1H⊺
n + Rn
C. Weights update
a. Likelihood function
p	
zn|x(i)
n−1

= N 	
zn; b(i)
n , Sn

b. Weight update
˜w(i)
n = w(i)
n−1p 	
zn|x(i)
n−1

c. Weight normalization
w(i)
n = ˜w(i)
n /
Ns

i=1
˜w(i)
n
D. Resample the particles
Resample using one of the
resampling Matlab example
snippets
E. Move the resampled particles
Regularize the particles
using the procedure from
Table 15.3
Reset the weights
w(i)
n = 1/N
F. Importance sampling
a. Intermediate calculations
a(i)
n = x(i)
n|n−1 + nH⊺
nR−1
n
	
zn −b(i)
n

n = Qn−1 −Qn−1H⊺
nS−1
n HnQn−1
b. Importance sampling
x(i)
n ∼p (xn|xn−1, zn) = N 	
xn; a(i)
n , n

G. Moment calculations
a. Mean
ˆxn = 
i w(i)
n x(i)
n
b. Covariance
Pxx
n = 
i w(i)
n x(i)
n x(i)⊺
n
−ˆxnˆx⊺
n
H. Time step and return to B
x(i)
n →x(i)
n−1
where the Jacobian ˆHn is deﬁned by
ˆHn ≜

∇⊺
xnh (xn)
⊺
xn=f(xn−1)
(16.48)
Now (16.46) becomes
˜zn ≃ˆHnxn + wn
(16.49)
where
˜zn ≜zn + ˆHnf (xn−1) −h (f (xn−1))
(16.50)
Thus, we have turned the observation equation into one that is linear in xn at the
expense of adding additional terms to the observation, that is, zn →˜zn. We can now

238
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
use the procedure in Table 16.5 by simply letting Hn →ˆHn and zn →˜zn, with the
additional steps of calculating the Jacobian ˆHn and the additional terms in ˜zn and we
make the replacement xn →x(i)
n and xn−1 →x(i)
n−1 in (16.49) and (16.50).
Now, the OPF can be applied to any tracking problem that has nonlinear dynamic
and observation equations as long as all densities are Gaussian. For most tracking
problems with these characteristics, nothing is gained by using the OPF over any of
the methods presented in Part II of this book. However, there are instances where
one would like to apply constraints on the track solutions, such as limiting the track
estimates to a predetermined road grid or limiting the maneuverability or speed of
the target states. It is for these type of problems that the OPF will be most amenable
to solving the tracking problem. Since our DIFAR tracking case study does not have
any restrictions on the target state, we will not apply the locally linearized Gaussian
optimal SIS particle ﬁlter to the case study.
16.5
THE SIS AUXILIARY PARTICLE FILTER
The use of a well chosen SIS proposal distribution, q (xn|xn−1, zn), should ensure
that the current observation zn is incorporated into the proposal procedure so that
particles are not moved blindly into regions of the state space that are unlikely given
that observation. Obviously, the BPF fails to achieve this objective since the current
observation is not considered when q (xn|xn−1, zn) BPF
= p (xn|xn−1). As we discussed
previously, the OPF chooses q (xn|xn−1, zn) OPF
= p (xn|xn−1, zn), but it is difﬁcult to
implement except in special cases. The SIS auxiliary particle ﬁlter (APF) is a particle
ﬁlter that incorporates the current observations into the proposal distribution in a
suboptimal way.
In the standard resampling SIS particle ﬁlter, including the BPF, the particles
are resampled after the particles are drawn from the importance density and the
weights have been updated by the current observation. In the OPF, on the other hand,
resampling takes place before the particles are sampled from the importance density.
This can be veriﬁed by comparing Table 16.5 with either Table 16.2 or Table 16.4.
In Ref. [16] and Chapter 13 of Ref. [17], Pitt and Shephard proposed the APF as a
variant of the standard SIS where the resampling step is performed at time tn−1 using
the observation from time tn. In this way, the APF attempts to mimic the method used
by the optimal particle ﬁlter. The clearest description of the APF procedure can be
found in Refs [7,8]. Much of the material presented in this section is taken directly
from Ref. [7].
The idea behind the APF is to augment the particles x(i)
n−1 that have large weights
(the “good” particles) in the sense that the predictive likelihoods p(zn|x(i)
n−1) are large
for these particles. From Bayes’ Law (3.23), the posterior density can be written as
p (xn|z1:n) ∝p (zn|xn) p (xn|z1:n−1)
(16.51)

THE SIS AUXILIARY PARTICLE FILTER
239
Using the Chapman–Kolmogorov equation (3.24) this becomes
p (xn|z1:n) ∝p (zn|xn)

p (xn|xn−1) p (xn−1|z1:n−1) dxn−1
(16.52)
If
p (xn−1|z1:n−1) =
Ns

i=1
w(i)
n−1δ

xn−1 −x(i)
n−1

(16.53)
Then
p (xn|z1:n) ∝
 Ns

i=1
w(i)
n−1p

xn|x(i)
n−1

p (zn|xn)
(16.54)
The product w(i)
n−1p(xn|x(i)
n−1) is treated as a combined probability that contributes to
the ﬁltered density sum.
One way to sample from the empirical prediction density is to think of

i w(i)
n−1p(xn|x(i)
n−1) as a “prior” mixture density ˆp (xn|z1:n−1) that is combined
with the likelihood p (zn|xn) to produce a posterior. Samples x(i)
n
can be drawn
from ˆp (xn|z1:n−1) by choosing x(i)
n with probability w(i)
n−1 and then drawing from
p(xn|x(i)
n−1).
To understand the APF method, we ﬁrst examine the SIS update equation (16.12)
before sampling from q(xn|x(i)
n−1, zn) and rewrite it as
˜wn = w(i)
n−1
p (zn|xn) p

xn|x(i)
n−1

q

xn|x(i)
n−1, zn

(16.55)
Now introduce an auxiliary variable ξ ( ξ ∈{1, . . . , Ns}) that indexes into one com-
ponent of the “prior” mixture density. Deﬁne the joint conditional indexed posterior
density p (xn, ξ = i|z1:n) by
p (xn, ξ = i|z1:n) ≜p (zn|xn) p
	
xn,ξ = i|z1:n−1

= p (zn|xn) p (xn|ξ = i, z1:n−1) p (i|z1:n−1)
(16.56)
Comparison with (16.54) allows us to identify
p (xn|ξ = i, z1:n−1) = p

xn|x(i)
n−1

(16.57)
p (i|z1:n−1) = w(i)
n−1
(16.58)
and we obtain the posterior probability of xn given {zn, x(i)
n−1, w(i)
n−1}
β(i)
n ≜p (xn, ξ = i|z1:n) = p (zn|xn) p

xn|x(i)
n−1

w(i)
n−1
(16.59)

240
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
Samples can be drawn from the joint density (16.59) by neglecting ξ. Assume that
a set of particles {x(i)
n }Ns
i=1 are drawn from the marginalized density p (xn|z1:n) and the
indices ξ are simulated with probabilities proportional to p (ξ|z1:n−1) = w(i)
n−1. Now,
(16.54) can be approximated by
p (xn|z1:n) ∝
Ns

i=1
w(i)
n−1p

zn|xn, ξ(i)
p

xn|x(i)
n−1

(16.60)
where ξ(i) denotes the index of the particle x(i)
n at time step tn−1 (namely ξ(i) ≜
{ξ = i}).
The proposal distribution used to draw samples {x(i)
n , i = 1, . . . , Ns} is chosen as
a factorized form
q (xn, ξ|z1:n) ∝q (ξ|z1:n) q (xn|ξ, z1:n)
(16.61)
where
q

ξ(i)|z1:n

= w(i)
n−1p

zn|µ(i)
n

(16.62)
q

xn|ξ(i), z1:n

= p

xn|x(i)
n−1

(16.63)
where µ(i)
n is a value (e.g., mean, mode, or sample value) associated with p(xn|x(i)
n−1)
from which the ith particle is drawn.
Thus, the true posterior is further approximated by
p (xn|z1:n) ∝
Ns

i=1
w(i)
n−1p

zn|µ(ξ=i)
n

p

xn|x(ξ=i)
n−1

(16.64)
Now the unnormalized weights from (16.55) become
˜w(i)
n = w(ξ=i)
n−1
p
	
zn|x(i)
n

p

x(i)
n |x(ξ=i)
n−1

q

x(i)
n , ξ(i)|z1:n

=
p
	
zn|x(i)
n

p

zn|µ(ξ=i)
n

(16.65)
The process ﬂow for the auxiliary particle ﬁlter is shown in Table 16.6. To sum up
the process ﬂow of the SIS APF, part B amounts to using a bootstrap particle ﬁlter
procedure to calculate a new set of weights and then resampling and regularizing the
particle set

x(i)
n−1, i = 1, . . . , Ns

using these new weights. This is followed in Part
E with a propagation of the resampled particles forward in time and computation of
a new set of weights using (16.65) with normalization.

THE SIS AUXILIARY PARTICLE FILTER
241
TABLE 16.6
Auxiliary Particle Filter Process Flow
A. Initialize ﬁlter
1. Initialize state vector samples
x(i)
0 ∼q (x0) , i = 1, . . . , Ns
2. Initialize weights
w(i)
0 =
1
Ns
B. Calculate
µ(i)
n mean or draw from p	
xn|x(i)
n−1

, ∀i
˜β
(i)
n = w(i)
n−1p 	
zn|µ(i)
n

, ∀i
C. Normalize the weights
β(i)
n = ˜β
(i)
n /
Ns

i=1n
˜β
(i)
n
D. Resample x(i)
n−1 using β(i)
n
Resample x(i)
n−1 using one of the
resampling Matlab example
snippets using β(i)
n instead of w(i)
n−1
Move the resampled particles
Regularize the particles
using the procedure from
Table 15.3
E. Importance sampling
x(i)
n = p 	
xn|x(i)
n−1, ξ(i)
w(i)
n =
p	
zn|x(i)
n

p	
zn|µ(ξ=i)
n

F. Normalize
w(j)
n = ˜w(j)
n /
Ns

i=1
˜w(j)
n
G. Time step and return to B
x(j)
n →x(i)
n−1; w(j)
n →w(i)
n−1
The APF is essentially a two-stage procedure: At the ﬁrst stage, simulate the
particles with large predictive likelihoods; at the second stage, reweight the particles
and draw augmented states. This is equivalent to making a proposal that has a high
conditional likelihood a priori, thereby avoiding inefﬁcient sampling. Thus, the APF
takes advantage beforehand of the information from the likelihood model to avoid
inefﬁcient sampling since the particles with low likelihood are less informative. In
other words, the particles to be sampled are intuitively pushed to the higher likelihood
region.
Some remarks about the APF are as follows:
• In conventional SIS particle ﬁlters, estimation is usually performed after the
resampling step, which is less efﬁcient because resampling introduces extra
random variations in the current state. The APF overcomes this problem by doing
one-step ahead estimation based on the point estimate µ(i)
n that characterizes
p(xn|x(i)
n ).
• When process noise is small, the performance of the APF is usually better than
that of the SIS ﬁlters. However, when process noise is large, the point estimate
µ(i)
n doesn’t provide sufﬁcient information about p(xn|x(i)
n ) and the superiority
of the APF is not guaranteed.

242
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
• In the APF, the importance density is proposed as a mixture density that depends
on the past state and the most recent observation.
• The idea of the APF is identical to that of the local Monte Carlo method proposed
in Ref. [18], where the authors proposed two methods for sample draws {x, ξ},
based on either joint or marginal distributions.
• The disadvantage of the APF is that the sampling is drawn from an augmented
(thus higher) space. If the auxiliary index varies a lot for a ﬁxed prior, the gain
is negligible and the variance of the importance weights will be higher.
16.5.1
Application of the APF to DIFAR Buoy Tracking
When the APF is used as the tracking ﬁlter for the DIFAR case study, it was hoped
that the results would show an improvement over the Gaussian Kalman ﬁlters. But
one glance at the resulting Monte Carlo tracks for the six gradually decreasing SNRs,
shown in Figure 16.6 reveals that the APF does not perform any better. In hindsight,
the reason for this is almost obvious. Particle ﬁlters are designed to perform estimation
when the posterior density (read dynamic model) is non-Gaussian. But for the DIFAR
case study it is the likelihood function that is non-Gaussian while the dynamic model
is linear and Gaussian. So, as we found out, the APF does not outperform the Gaussian
Kalman ﬁlters under these conditions.
–3
–2
–1
0
1
2
–4
–2
0
2
4
x-position (nmi)
y-position (nmi)
SNR 20; 0 divergent tracks
–2
–1
0
1
2
3
–5
0
5
x-position (nmi)
y-position (nmi)
SNR 15; 0 divergent tracks
–40
–20
0
20
40
60
80
–100
–50
0
50
x-position (nmi)
y-position (nmi)
SNR 10; 4 divergent tracks
–80
–60
–40
–20
0
20
40
60
–200
–100
0
100
x-position (nmi)
y-position (nmi)
SNR 5; 14 divergent tracks
–150
–100
–50
0
50
100
150
–400
–200
0
200
x-position (nmi)
y-position (nmi)
SNR 0; 51 divergent tracks
–1500
–1000
–500
0
500
1000
–500
0
500
1000
x-position (nmi)
y-position (nmi)
SNR –5; 87 divergent tracks
FIGURE 16.6
Track estimation results for the auxiliary particle ﬁlter applied to the DIFAR
tracking case study.

APPROXIMATIONS TO THE SIS AUXILIARY PARTICLE FILTER
243
16.6
APPROXIMATIONS TO THE SIS AUXILIARY PARTICLE FILTER
An alternate approach to the SIS particle ﬁlter has been proposed that is similar to
the APF presented above. Suppose the posterior density p(xn|z1:n) and the optimal
importance density q(x(i)
n |x(i)
n−1, z1:n) are approximated by Gaussian densities. That
is, the posterior density is approximated as
p(xn|z1:n) ≃N

xn; ¯xn, ˆPn

(16.66)
and the proposal distribution is approximated by a Gaussian proposal distribution for
each particle
q

x(i)
n |x(i)
n−1, z1:n

≃N

x(i)
n ; ¯x(i)
n , ˆP(i)
n

(16.67)
That is, at time tn−1, one uses one of the Gaussian Kalman ﬁlter variations from Part
II of this book, along with the new observation, to compute the mean and covariance
of the importance distribution for each particle. Then one can sample the ith particle
from this distribution.
16.6.1
The Extended Kalman Particle Filter
In the extended Kalman particle ﬁlter (EKPF), ﬁrst proposed by Merwe et al. [19],
both the nonlinear dynamic and observation equations are expanded in Taylor series
and used in an EKF to generate the ﬁrst two moments for the ith particle

¯x(i)
n , ˆP(i)
n

used in (16.67) to deﬁned the Gaussian proposal distribution. Then, the ith sample is
drawn from the proposal distribution
ˆx(i)
n = q

x(i)
n |x(i)
n−1, z1:n

= N

x(i)
n ; ¯x(i)
n , ˆP(i)
n

(16.68)
Now, ˆx(i)
n is used to update the weights using (16.12),
˜w(i)
n = w(i)
n−1
p
	
zn|ˆx(i)
n

p

ˆx(i)
n |x(i)
n−1

q

ˆx(i)
n |x(i)
n−1, zn

(16.69)
and the weights are normalized. Since this method is almost identical to the Gaussian
optimal SIS particle ﬁlter presented above, the reader is referred to Ref. [19] for
further information on the speciﬁc steps associated with this method.
16.6.2
The Unscented Particle Filter
For highly nonlinear systems, the UKF has been shown to be more accurate than the
EKF, so in Ref. [19], the unscented particle ﬁlter (UPF) was proposed. It replaced the
EKF with the UKF for the generation of the ﬁrst two moments used in the Gaussian
proposal distribution. The UPF method is straightforward and easy to implement, as

244
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
shown in Table 16.7. Table 13.2 should be used for the calculation of sigma point
weights. In addition, some of the remaining steps have been omitted from the table
because they are the same as the previous particle ﬁlters. Additional information
related to the UPF can be found in Refs [20,21]. A square root version of the UPF
can be found in Ref. [22].
TABLE 16.7
Unscented Particle Filter with Resampling and Regularization
A. Initialize ﬁlter
1. Initialize state vector samples
x(i)
0 ∼q (x0), i = 1, . . . , Ns
2. Initialize weights
w(i)
0 =
1
Ns
B. Importance sampling
Choose the UKF weight w0
For i = 1, . . . ,Ns
a. Calculate the j Sigma points
χ(i,j)
n−1|n−1 = ¯x(i)
n−1|n−1
+

nx
1−w0 Pxx(i)
n−1|n−1r(j), j = 0, 1, . . . , 2nx
b. State prediction
ˆx(i)
n|n−1 =
2nx

j=0
wjf 	
χ(i,j)
n−1|n−1

Pxx(i)
n|n−1 =
2nx

j=0
wjf 	
χ(i,j)
n−1|n−1

f⊺	
χ(i,j)
n−1|n−1

−ˆx(i)
n|n−1ˆx⊺(i)
n|n−1 + Q
c. Observation prediction
χ(i,j)
n|n−1 = ˆx(i)
n|n−1
+

nx
1−w0 Pxx(i)
n|n−1r(j), j = 0, 1, . . . , 2nx
ˆz(i)
n|n−1 =
2nx

j=0
wjh 	
χ(i,j)
n|n−1

Pzz(i)
n|n−1 =
2nx

j=0
wjh 	
χ(i,j)
n−1|n−1

h⊺	
χ(i,j)
n−1|n−1

−ˆz(i)
n|n−1ˆz⊺(i)
n|n−1 + R
Pxz(i)
n|n−1 =
2nx

j=0
wjf 	
χ(i,j)
n−1|n−1

h⊺	
χ(i,j)
n−1|n−1

−ˆx(i)
n|n−1ˆz⊺(i)
n|n−1
d. Kalman ﬁlter update
K(i)
n = Pxz(i)
n|n−1

Pzz(i)
n|n−1
−1
ˆx(i)
n|n = ˆx(i)
n|n−1 + K(i)
n
	
zn −ˆz(i)
n|n−1

Pxx(i)
n|n = Pxx(i)
n|n−1 −K(i)
n Pzz(i)
n|n−1K⊺(i)
n
E. Update the importance weights
˜w(i)
n = w(i)
n−1
p	
zn|ˆx(i)
n

p	
ˆx(i)
n |x(i)
n−1

q	
ˆx(i)
n |x(i)
n−1,zn
w(j)
n = ˜w(j)
n /
Ns

i=1
˜w(j)
n
F. Resample and move
.
G. Moment Calculations
H. Time step and return to B

REFERENCES
245
Note that the UKF is only one of the class of sigma point Kalman ﬁlters that could
be used in this application. We could just as easily develop a spherical simplex or
Gauss–Hermite SIS particle ﬁlter by substituting the weights and sigma points for
those ﬁlters into Table 16.7. In fact, for a high-dimensional state vector, using the
spherical simplex Kalman ﬁlter in place of the unscented Kalman ﬁlter would be a
good idea. On the other hand, for low-dimensional state vectors and highly nonlinear
models, substituting the Gauss–Hermite Kalman ﬁlter would make more sense. For
completeness, one could even use the Monte Carlo Kalman ﬁlter for this application,
without any loss of generality.
16.7
REDUCING THE COMPUTATIONAL LOAD THROUGH
RAO-BLACKWELLIZATION
In many state estimation applications where the state vector is of high dimensionality,
the computational load for a SIS particle ﬁlter can become extremely costly and the es-
timation accuracy may deteriorate rapidly. Particle ﬁlter-based methods become quite
inefﬁcient when applied to a high-dimensional state space since prohibitively large
number of samples may be required to approximate the underlying density functions
to the desired accuracy. When the components of the state vector can be subdivided
into two classes, one that follows a linear temporal transition and is Gaussian and a
second class that is non-Gaussian, then the computational load can be reduced using
what is know as Rao-Blackwellization [23–25].
The key idea of the Rao–Blackwell method is a dimensional reduction that makes
use of the structure of the models to split the conditional posterior into two separate
parts, with the Gaussian part conditioned on the non-Gaussian part. One ﬁrst uses a
particle ﬁlter to generate the non-Gaussian posterior and then uses a Kalman ﬁlter
on the conditioned Gaussian part. Assume that the state vector components can be
divided into two groups xn = [q⊺, r⊺]⊺such that
p (xn|z1:n) = p (qn, rn|z1:n)
= p (qn|rn, z1:n) p (rn|z1:n)
(16.70)
In Ref. [24] it was pointed out that p(qn|rn, z1:n) can be efﬁciently updated using a
linear Kalman ﬁlter when the initial uncertainty for qn is Gaussian and the conditional
probabilities of the observation model and system dynamics for qn are Gaussian. For
utilization of the Rao–Blackwell methodology, the reader is referred to Refs [3,24,25].
REFERENCES
1. Riple BD. Stochastic Simulation. Wiley; 1987.
2. Online at: http://en.wikipedia.org/wiki/Inverse transform sampling.
3. Ristic B, Arulampalam S, Gordon N. Beyond the Kalman Filter: Particle Filters for Track-
ing Applications. Artech House; 2004.
4. Andrieu C, Douct A, Holstein R. Particle Markov chain Monte Carlo methods. J. R. Stat.
Soc. B. 2010;72(3):269–324.

246
SEQUENTIAL IMPORTANCE SAMPLING PARTICLE FILTERS
5. Capp´e O, Godsill SJ, Moulines E. An overview of existing methods and recent advances
in sequential Monte Carlo. Proc. IEEE 2007;95(5):899–924.
6. Liu JS, Chen R. Sequential Monte Carlo methods for dynamic systems. J. Am. Stat. Assoc.
1998;93(443):1032–1044.
7. Chen Z. Bayesian Filtering: From Kalman Filters to Particle Filters, and Beyond.
Adaptive Systems Laboratory, McMaster University, Hamilton, ON, Canada. [Online],
http://citeseerx.ist.psu.edu, 2003.
8. Fearnhead P. Sequential Monte Carlo Methods in Filter Theory, Dissertation. Oxford UK:
University of Oxford, MertonCollege; 1998.
9. Ross SM. Introduction to Probability Models, 4th ed. Academic Press; 1989.
10. Hol DH, Sch¨on TB, Gustafsson F. On resampling algorithms for particle ﬁlters. In Pro-
ceedings of Nonlinear Statistical Signal Processing Workshop, Cambridge, UK; September
2006.
11. Kong A, Liu JS, Wong WH. Sequential computations and Bayesian missing data problems.
J. Am. Stat. Assoc. 1994;89(425):278–288.
12. Gordon N, Salmond D, Smith AF. Novel approach to nonlinear/non-Gaussian Bayesian
state estimation. IEE Proc. F. Radar Signal Process. 1993;140:107–113.
13. Doucet A. Monte Carlo Methods for Bayesian Estimation of Hidden Markov Models.
Application to Radiation Signals, Dissertation. University of Paris-Sud, Orsay; 1997.
14. Doucet A. On Sequential Simulation-Based methods for Bayesian Filtering. Technical
Report CUED/F-INFENG/TR.310, University of Cambridge; 1998.
15. Doucet A, Gordon N, Krishnamurthy V. Particle ﬁlters for state estimation of jump Markov
linear systems. IEEE Trans. Sig. Proc. 2001;49:613–624.
16. Pitt K, Shephard N. Filtering via simulation: auxiliary particle ﬁlter. J. Am. Stat. Assoc.
1999;94(446):590–599.
17. Oitt MK, Shephard N. Auxiliary variable based particle ﬁlters. In Sequential Monte Carlo
Methods in Practice. Springer; 2001.
18. Wu W, Hu X. Hu D, Wu M. Comments on “Gaussian Particle Filtering”. IEEE Trans. Sig.
Proc. 2005;53(8):3350–3351.
19. van der Merwe R, Doucet A, de Freitas N, Wam E. The Unscented Particle Filter. Cam-
bridge University Technical Report CUED/F-INFENG/TR 380; 2000.
20. van der Merwe R, Doucet A, de Freitas N, Wam E. The unscented particle ﬁlter. Adv.
Neural Inform. Process. Syst. 2001;13:584–590.
21. Rui Y, Chen Y. Better Proposal distributions: object tracking using unscented particle ﬁlter.
In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, CVPR 2001;Vol. II, 2001, pp. 786–793.
22. Zandara S, Nicholson AE. Square Root Unscented Particle Filtering for Grid Mapping.
6th World Congress on Intelligent Control and Automation; 2006, pp. 1548–1552.
23. Bolviken E, Storik G. Deterministic and Stochastic Particle Filters for State-Space Models.
In Sequential Monte Carlo Methods in Practice. Springer; 2001.
24. Murphy K, Russell S. Rao-Blackwellised particle ﬁltering for dynamic Bayesian networks.
In Sequential Monte Carlo Methods in Practice. Springer; 2001.
25. Casella
C,
Rober
CP.
Rao-Blackwellisation
of
sampling
schemes.
Biometrica
1996;83(1):81–94.

17
THE GENERALIZED MONTE CARLO
PARTICLE FILTER
All of the SIS class of particle ﬁlters discussed in the previous chapter are based
on (16.12), which deﬁnes the recursive update of the importance weights. The com-
bination of this recursive importance weight update with the resample and move
steps provide the framework for powerful nonlinear non-Gaussian estimation meth-
ods dependent only on the choice of importance density and likelihood function.
However, there are many disadvantages in using the SIS particle ﬁlters:
• The BPF does not use the latest measurement during importance sampling leav-
ing it susceptible to increased variance and instability for many applications.
• All SIS particle ﬁlters require the resample and move steps that tend to increase
the estimated state error covariances. They also add a signiﬁcant computational
burden to the estimation procedure, since they cannot be parallelized.
• The SIS APF and its alternatives may require the execution of a Gaussian Kalman
ﬁlter for every particle during every time step, increasing the computational
burden even further.
A second class of particle ﬁlters, the generalized monte carlo (GMC) particle
ﬁlters, have been developed that do not use a recursive weight update, but instead
compute new weights with every sequential update of the ﬁlter. An example of
this more general particle ﬁlter class include the Gaussian particle ﬁlter (GPF) ﬁrst
proposed by Kotechha and Djuri`c [1,2].
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
247

248
THE GENERALIZED MONTE CARLO PARTICLE FILTER
17.1
THE GAUSSIAN PARTICLE FILTER
Let us return once again to the fundamental equations of Bayes estimation, remem-
bering that it is a two-step process, with the ﬁltering step given by the estimation of
the posterior (ﬁltering or update) density
p (xn|z1:n) = cp (zn|xn) p (xn|z1:n−1)
(17.1)
and the predictive step encompassing the estimation of the predictive (or prior, because
it uses all of the prior measurements) density
p (xn|z1:n−1) =

p (xn|xn−1) p (xn−1|z1:n−1) dxn−1
(17.2)
The fundamental concept of the GPF is to make the assumption that both the posterior
and prior densities are Gaussian. That is, let
p (xn|z1:n) = N

xn; µn, n

(17.3)
p (xn|z1:n−1) = N

xn; ¯µn, ¯n

(17.4)
Assume that at time t0, prior to any observations, we have prior information about
the initial density and that
p (x0) = N

x0; µ0, 0

(17.5)
To initialize the GPF, we ﬁrst draw samples {˜x(i)
0 }Ns
i=1 from p (x0) and propagate them
forward in time using the dynamic equation
˜x(i)
n|n−1 = fn

˜x(i)
n−1|n−1

+ µn
(17.6)
Then we compute ¯µ1 and ¯1 from
¯µn = 1
Ns
Ns

i=1
˜x(i)
n|n−1
(17.7)
¯n = 1
Ns
Ns

i=1n|n−1
˜x(i)
n|n−1˜x(i)⊺
n|n−1 −¯µn ¯µ⊺
n + Q
(17.8)
This now deﬁnes the prior (predictive) density (17.4)
Returning to (15.41), we can write the importance weight calculation algorithm in
a noniterative fashion as
wn ∝p (zn|xn) N

xn; ¯µn, ¯n

q (xn|z1:n)
(17.9)

THE GAUSSIAN PARTICLE FILTER
249
After sampling from the (currently unspeciﬁed) importance density, x(i)
n ∼
q (xn|z1:n) , i = 1, . . . , Ns, this becomes
w(i)
n = p

zn|xn = x(i)
n

N

xn = x(i)
n ; ¯µn, ¯n

q

xn = x(i)
n |z1:n

(17.10)
which is followed by a normalization step
w(i)
n =
w(i)
n
Ns
i=1 w(i)
n
(17.11)
The moments of the posterior can now be computed from
ˆxn =
Ns

i=1
w(i)
n x(i)
n
(17.12)
Pxx
n =
Ns

i=1
w(i)
n x(i)
n x(i)⊺
n
−ˆxnˆx⊺
n
(17.13)
and the Gaussian particle ﬁlter process is completed, as shown in Figure 17.1. Note
that this generalized Monte Carlo GPF still requires additional analytical expressions
for both the likelihood function and the importance density.
Initialize
xx
x
P
S
S
N
i
i
n
n
n
i
N
i
i
i
n
n
n
n
n
n
i
w
w
xx
x
x
P
x x
x x
Delay
Track file
i
i
n
n
n
n
n
n
n
i
n
i
n
n
n
i
i
i
n
n
n
p
w
q
w
w
w
z
x
x
x
x
x
x
z
Weight update
i
n
n
n
q
x
x
z
Importance sampling
i
n
n
n
n
xx
x
x
x
P
Generate samples
n
n
xx
x
P
State vector correction
n
n
q x
z
n
n
n
n
n
p x
z
x
State vector prediction
s
s
i
i
n n
n
N
i
n
n n
i
s
N
i
i
n
n n
n n
n
n
i
s
N
N
x
f x
FIGURE 17.1
Process ﬂow diagram for the Gaussian particle ﬁlter.

250
THE GENERALIZED MONTE CARLO PARTICLE FILTER
17.2
THE COMBINATION PARTICLE FILTER
In Figure 17.1, the importance density is not speciﬁed. Thus, the GPF is not complete
and still requires the speciﬁcation of an importance density. In Ref. [1], it is suggested
that a Gaussian distribution be used as the importance density, that is, let q (xn|z1:n) =
N

xn; µx
n|n, xx
n|n

, where µx
n|n and xx
n|n are obtained from the prior density using
any of the nonlinear Kalman ﬁlters developed in Part II of this book. This approach
combines the GPF with one of the nonlinear Kalman ﬁlters and is therefore called a
generalized Monte Carlo combination particle ﬁlter (CPF).
The CPF can be subdivided into two classes, those that use an EKF to generate
the importance density and those that use a sigma point Kalman ﬁlter. The sigma
point Kalman ﬁlter class can be further subdivided into the four possible numerical
integration Kalman ﬁlters, the MCKF, UKF, SSKF, or GHKF. The recursive process
ﬂow diagram for the CPF-EKF estimation ﬁlter is shown in Figure 17.2 while that
of the CPF that uses a sigma point Kalman ﬁlter is presented in Figure 17.3. Along
the bottom of the ﬁgures the Gaussian particle ﬁlter structure can be identiﬁed and
along the right side the nonlinear Kalman ﬁlter structure can be seen. In Ref. [3], an
unscented particle ﬁlter is presented that is similar, but does not include the Gaussian
approximation for the prior. It should be noted that [3] contains a signiﬁcant number
of errors, making it very difﬁcult to follow.
These generalized Monte Carlo combination particle ﬁlters work very well for
most applications because the covariance associated with the importance density will
Initialize
xx
x
P
S
S
N
i
i
n
n
n
i
N
i
i
i
n
n
n
n
n
n
i
w
w
xx
x
x
P
x x
x x
Delay
Track file
i
i
n
n
n
n
n
n n
n n
i
n
i
n
n
n n
n n
i
i
i
n
n
n
p
w
w
w
w
x
xx
z
x
x
x
x
x
x
Weight update
State vector prediction
n
n
xx
x
P
State vector correction
n
n
n
n n
n n
p x
z
x
n
n
n
n
n
n
n
n
n
n n
n n
n
n
x
x
x
xx
F
f
x
n
n n
n
n n
n
n n
n n
n n
n n
n n
n n
n n
n n
n n
x
x
x
zz
xx
xz
xx
H
h
x
z
h x
P
H
P
H
R
P
P
H
Observation/measurement prediction
Kalman filter: correction
n
n n
n n
n
n n
n n
n n
n
n n
n n
n
n n
n
xz
zz
x
xx
zz
K
P
P
i
n
n
n n
n n
x
xx
x
x
Importance sampling
n
n
n
n n
n n
q
x
xx
x
z
x
Importance density
FIGURE 17.2
Combination particle ﬁlter that uses an EKF as an importance density.

THE COMBINATION PARTICLE FILTER
251
Initialize
s
s
j
j
n n
n
n
N
j
n n
j
n n
j
N
j
j
n n
j
n n
n n
n
n
j
w
w
n
n n
n n
n
n n
n n
n n
n
n n
n n
n
n n
n
xz
zz
x
xx
zz
K
P
P
Kalman filter correction
Delay
Track file
Generate samples and weights
i
i
n
n
n
n n
n n
i
n
i
n
n n
n n
i
i
i
n
n
n
p
w
w
w
w
x
xx
z
x
x
x
Weight update
S
S
N
i
i
n
n
n
i
N
T
i
i
i
n
n
n
n
n
n
i
w
w
xx
x
x
P
x x
x x
s
s
s
j
j
n n
n n
N
j
n n
j
n n
j
N
j
j
n n
n n
n n
j
n n
n n
j
N
j
j
n n
n n
j
n n
n n
n n
j
w
w
w
zz
xz
z
P
P
i
n
n n
n n
x
xx
x
Importance sampling
State vector prediction
Observation prediction
State vector correction
xx
x
P
j
j
n
n
n
n
n
n
iw
n
n
n n
n n
q
x
xx
x
z
Importance density
n
n
n
n n
n n
p x
z
x
FIGURE 17.3
Combination particle ﬁlter that uses a sigma point kalman ﬁlter as an impor-
tance density.
usually be very broad and will encompass a major portion of the support of the
posterior density. With the exception of applications where the posterior has large tail
distributions, we have found that the combination particle ﬁlters are close to being
the “best” particle ﬁlter for any application. Since they do not require the sample
and move steps and they execute the nonlinear Kalman ﬁlter only once during every
iteration, they offer a signiﬁcant computational improvement over most of the SIS
particle ﬁlters.
For heavy tailed or multimodal prior and posterior densities, there are several
alternatives that can be used to improve the performance of the generalized Monte
Carlo particle ﬁlter class. For multimodal non-Gaussian densities, a Gaussian mixture
particle ﬁlter has been proposed by Kotecha and Djuri`c [4] in which both the predictive
and ﬁltering distributions are approximated as Gaussian mixtures. These Gaussian
sum particle ﬁlters (GSPF) have been shown to have improved performance over
the GPF for several applications. Merwe and Wan presented a sigma point Gaussian
mixture combination particle ﬁlter in Ref. [5] where it was used to estimate one-
dimensional state in a highly nonlinear dynamic transition equation driven by Gamma
noise. The state vector was augmented with the Gamma dynamic noise and the results
were very promising, even when there were numerous outlier observations due to the
heavy tails of the Gamma distribution.

252
THE GENERALIZED MONTE CARLO PARTICLE FILTER
Several modiﬁcations to the generalized Monte Carlo method can be used to
handle heavy-tailed noise distributions. Instead of using a Gaussian approxima-
tion for the predictive and ﬁltering densities, one could use a Levy, alpha-stable or
Laplace distribution and replace the nonlinear Gaussian Kalman ﬁlter with a Kalman–
Levy [6], generalized stable [7], or Laplace ﬁlter [8]. These ﬁlters have a Kalman
ﬁlter-like symmetric structure that could readily be used in place of the nonlinear
Gaussian Kalman ﬁlter in a combination particle ﬁlter application. The Kalman–Levy
[9,10] and Laplace ﬁlters [8] have been successfully applied as tracking ﬁlters for
maneuvering targets but have not yet been utilized in particle ﬁlter applications
Like the Gaussian Kalman ﬁlter, all of these methods required a consistency of
the analytical form of the density. Only a single analytical density had to be used
throughout the ﬁlter. Another generalization of the nonlinear Kalman ﬁlter struc-
ture was proposed by Chen and Singpurwalla [11] in which the analytical density
could vary for various parts of the ﬁlter. Their formulation utilized Gamma, Beta, and
Pearson distributions for various parts of the ﬁlter, with the state prediction, observa-
tion prediction, and update formalism of the Kalman ﬁlter maintained throughout. In
actual applications, a ﬁlter similar to the MCKF given in Chapter 12 was used. Their
method could easily be adopted within a generalized particle ﬁlter formalism.
17.2.1
Application of the CPF–UKF to DIFAR Buoy Tracking
Application of a CPF–UKF to the DIFAR tracking problem resulted in the Monte
Carlo tracking results for six SNRs shown in Figure 17.4. Comparisons with track
−3
−2
−1
0
1
2
−4
−2
0
2
4
x-position (nmi)
x-position (nmi)
x-position (nmi)
x-position (nmi)
x-position (nmi)
x-position (nmi)
y-position (nmi)
SNR 20; 0 divergent tracks
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
SNR 15; 0 divergent tracks
−3
−2
−1
0
1
2
3
−4
−2
0
2
4
SNR 10; 0 divergent tracks
−60
−40
−20
0
20
40
−20
0
20
40
60
SNR 5; 21 divergent tracks
−80
−60
−40
−20
0
20
40
−50
0
50
100
y-position (nmi)
y-position (nmi)
y-position (nmi)
y-position (nmi)
y-position (nmi)
SNR 0; 100 divergent tracks
−80
−60
−40
−20
0
20
40
60
−50
0
50
100
SNR −5; 100 divergent tracks
FIGURE 17.4
Monte Carlo track plots for the sigma point Gaussian particle ﬁlter for six
SNRs.

PERFORMANCE COMPARISON OF ALL DIFAR TRACKING FILTERS
253
plots for any of the other track ﬁlters shown earlier indicates that this ﬁlter is no better
(and no worse) than any of the other ﬁlters.
17.3
PERFORMANCE COMPARISON OF ALL DIFAR TRACKING
FILTERS
In this section, we present an analysis of the performance of many of the tracking ﬁlters
applied to the DIFAR case study. We have delayed this performance analysis until all
of the ﬁlters have been presented in Parts II and III. Since all of the results presented
so far have been based on simulated observation data, the best method of assessing
performance is by comparison of the x- and y-axis position RMSE for the various
ﬁlters. For each signal SNR, a RMS position error comparison plot is presented for
four of the Gaussian ﬁlters (EKF, UKF, SSKF, and GHKF) and three of the particle
ﬁlters (BPF, APF, and GSPF). The acronym GSPF represents a combination particle
ﬁlter with an UKF used as the importance density. For all particle ﬁlters, 3000 Monte
Carlo samples were used.
As a ﬁrst example, Figure 17.5 shows a comparison of both the x- and y-axis RMS
position errors as a function of the true position of the target ship when the signal SNR
is 20 dB. One can immediately see that the BPF RMS position errors grow quickly
due to the three divergent tracks (out of 100 Monte Carlo runs) at 20 dB SNR, as
can be seen in Figure 16.5. This is just another illustration of the problems associated
with initialization sensitivity for the BPF. Since all of the ﬁlters were initialized with
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
X-position (nmi)
X RMSE at 20 dB (nmi)
EKF
UKF
SSKF
GHKF
BPF
APF
GSPF
−3
−2
−1
0
1
2
3
0
0.1
0.2
0.3
0.4
0.5
Y-position (nmi)
Y RMSE at 20 dB (nmi)
FIGURE 17.5
Comparison of the DIFAR case-study root mean squared position errors for
a signal SNR of 20 dB.

254
THE GENERALIZED MONTE CARLO PARTICLE FILTER
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
X-position (nmi)
X RMSE at 15 dB (nmi)
EKF
UKF
SSKF
GHKF
BPF
APF
GSPF
−3
−2
−1
0
1
2
3
0
0.1
0.2
0.3
0.4
0.5
Y-position (nmi)
Y RMSE at 15 dB (nmi)
FIGURE 17.6
Comparison of the DIFAR case-study root mean squared position errors for
a signal SNR of 15 dB.
the same set of initialization values, this additional tuning needed for the BPF is
unacceptable for application to real-world tracking.
In addition, we note that all of the Gaussian Kalman ﬁlter variants have almost
identical performance and that their performance is slightly better than that of either
the APF or the GSPF. And as implied in the previous section, the GSPF has the best
performance among the particle ﬁlters. Examination of the ﬁgure also shows that
the RMSE track performance for most of the ﬁlters is highest shortly after tracking
commences, reaches a minimum when the ship enters the buoy ﬁeld and is almost
symmetricabouttheorigin.TheinitialhighRMSerrorsoccurduetopoorinitialization
of the ﬁlters, but as soon as the ﬁlters have integrated a few observations they settle
to a reasonable track estimate.
When the signal SNR is lowered to 15 dB, the RMS position errors increase relative
to those of the 20 dB case, as can be seen in Figure 17.6. The BPF position RMSE
diverges due to seven divergent tracks at this SNR. None of the other ﬁlters showed
any divergent tracks in the 100 Monte Carlo runs. Once again, at this high SNR, the
Gaussian ﬁlters showed almost identical RMS position errors with the APF and GSPF
having slightly higher errors. Figures 14.3–14.7 show the RMS position errors for just
the Gaussian ﬁlters from 20 dB down to 0 dB. These ﬁgures are scaled to the Gaussian
ﬁlter RMS errors and therefore show a better comparison among the Gaussian ﬁlter
errors and also show very little performance difference among the Gaussian ﬁlters for
this application.
Figure 17.7 presents the RMS position errors for a 10 dB signal. At this signal
level, the APF had four divergent tracks (see Figure 16.6) causing the RMS errors to
grow. But the GSPF and all of the Gaussian ﬁlters are still stable so that their RMS

REFERENCES
255
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
X-position (nmi)
X RMSE at 10 dB (nmi)
EKF
UKF
SSKF
GHKF
BPF
APF
GSPF
−3
−2
−1
0
1
2
3
0
0.1
0.2
0.3
0.4
0.5
Y-position (nmi)
Y RMSE at 10 dB (nmi)
FIGURE 17.7
Comparison of the DIFAR case-study root mean squared position errors for
a signal SNR of 10 dB.
errors are still acceptable. We will not show RMS position error plots for SNRs below
10 dB because the Monte Carlo track estimations for all of the ﬁlters showed some
divergent tracks making the calculation of RMS errors invalid.
The performance analysis presented here and at the end of Chapter 14 indicate
that tracking a ship transiting through a DIFAR buoy ﬁeld is only viable if the SNR
is greater than 5 dB. These results also point out that there is no advantage to using
a particle ﬁlter if the dynamic noise is Gaussian regardless of the form of the density
for the observations. One could argue that more particles were needed to improve the
performance of the particle ﬁlters but we saw little improvement when we increased
the number of particles from 1000 to 3000. Finally, it is clear from these results
that the BPF is very sensitive to initialization, so care must be exercised when using
the BPF.
REFERENCES
1. Kotecha JH, Djuri`c PM. Gaussian Particle Filtering. Proceedings of the 11th Signal
Processing Workshop on Statistical Signal Processing; 2001, pp. 429–432.
2. Kotecha JH, Djuri`c PM. Gaussian particle ﬁltering. IEEE Trans. Sig. Proc. 2003:51(10);
2592–2601.
3. Wan EA, van der Merwe R. The unscented Kalman ﬁlter for nonlinear estimation. Adaptive
Systems for Signal Processing, Communications and Control Symposium, AS-SPEC; 2000,
pp. 153–158.

256
THE GENERALIZED MONTE CARLO PARTICLE FILTER
4. Kotecha JH, Djuri`c PM. Gaussian sum particle ﬁltering. IEEE Trans. Sig. Proc. 2003:
51(10);2602–2612.
5. van der Merwe R, Wan E. Gaussian Mixture Sigma-Point Particle Filters for Sequential
Probabilistic Inference in Dynamic State-Space Models. ICASSP; 2003.
6. Sornette D, Ide K. The Kalman–Levy ﬁlter. Physica D, 115:2001;142–174.
7. Balakrishna N. Statistical signal extraction using stable processes. Stat. Prob. Lett. 2009:
79(7);851–856.
8. Wang D, Zhang C, Zhao X. Multivariate Laplace ﬁlter: a heavy-tailed model for target
tracking. 19th International Conference on Pattern Recognition; 2008, pp. 1–4.
9. Gordon N, Percival J, Robinson M. The Kalman–Levy ﬁlter and heavy-tailed models
for tracking maneuvering targets. Proceedings of the 6th International Conference on
Information Fusion; 2003, pp. 1024–1031.
10. Sinha A, Kirubarafan T, Bar-Shalom Y. Application of the Kalman–Levy ﬁlter for tracking
maneuvering targets. Signal Data Process. Small Targets 2004:5428;102–112.
11. Chen Y, Singpurwalla ND. A non-Gaussian Kalman ﬁlter model for tracking software
reliability. Stat. Sinica 1994:4(2);535–548.

PART IV
ADDITIONAL CASE STUDIES

18
A SPHERICAL CONSTANT VELOCITY
MODEL FOR TARGET TRACKING IN
THREE DIMENSIONS
In this case study we address a very important estimation problem that is intrinsic to
many engineering endeavors, the problem of tracking an object in three dimensional
space. Many three-dimensional tracking algorithms use an inertial frame Cartesian
constant velocity linear dynamic model for target motion and a nonlinear observation
model that relates the Cartesian state vector x to a set of spherical observations z.
If the dynamic and observation noises are considered to be Gaussian, Kalman ﬁlter
implementation results in a set of linear Kalman ﬁlter state prediction equations and
nonlinear observation prediction equations that can utilize any one of the suboptimal
ﬁltering techniques presented in Part II of this book.
The issue of tracking a maneuvering (accelerating) target has been addressed in
a variety of ways. For tracking in Cartesian coordinates, the simplest approach is to
remove the constant velocity constraint from the target dynamic model by includ-
ing acceleration components in the state vector and replacing the constant velocity
constraint with a constant acceleration constraint [1–3]. Problems with this method
are encountered when it is applied to targets that only maneuver for short periods
and are nonmaneuvering for the majority of their trajectory. To address these cases,
several authors have proposed the use of a ﬁlter that switches from a constant velocity
model to a constant acceleration model when a maneuver is detected, but detecting a
maneuver is not always easy. Another approach that uses multiple banks of models,
with each model used to track a different potential maneuver, have been proposed
in Refs [2–4]. Still another model consists of an interactive multiple model (IMM)
that uses a bank of ﬁlter models that only vary the dynamic noise from model to
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
259

260
A SPHERICAL CONSTANT VELOCITY MODEL
model with an output that consists of a mixture distribution. Readers interested in
understanding IMM methods are referred to these books and the references contained
therein.
Although all of the above methods for tracking a maneuvering target work well
for many tracking scenarios, as an alternative several authors have recast the target
dynamic model into spherical polar coordinates maintaining the constraint of zero-
acceleration in inertial Cartesian coordinates. In the methods proposed in Refs [5–8],
a set of orthogonal Cartesian axes rotate relative to an inertial frame so that one axis
continually aligns with the line-of-site axis (the range axis). Since the state vector is
in rotating spherical coordinates, pseudo-accelerations must be introduced into the
dynamic motion equations due to the Coriolis forces created by the rotating coordi-
nate system. These pseudo-accelerations, coupled with the inertial frame Cartesian
constant velocity assumption, result in a set of nonlinear fully coupled spherical dy-
namic equations. These ﬁlters give very poor performance when used for tracking a
maneuvering target from a ﬁxed inertial coordinate system [5].
In this chapter, we will develop a much simpler target dynamic model that works
well in tracking a target following trajectories that include both maneuvering and
nonmaneuvering sections. In our model, the inertial frame Cartesian constant velocity
constraint is replaced by an inertial frame spherical constant velocity constraint where
the orthogonal spherical velocity components are restrained to be constant. For a
maneuvering target, constant velocity constraints will always be violated, but we
will show that, during maneuvers, the RMS tracking errors will be smaller with the
spherical constraint. Similar results have been previously been reported in Ref. [5].
In the latter part of this chapter, we will be comparing the performance of tracking
algorithms that make the Cartesian constant velocity assumption with those that make
a spherical constant velocity assumption. Therefore, we begin in Section 18.1 with a
consideration of methods for tracking an object in Cartesian coordinates with the
assumption of constant Cartesian velocity. The dynamic and observation equations
are presented and both the extended and sigma point Kalman ﬁlters in Cartesian
coordinates are described.
This is followed, in Section 18.2 with a similar development of the new set of non-
linear constant spherical velocity dynamic and linear observation equations, which
are derived from ﬁrst principles. Because the dynamic equations are referenced to
an inertial nonrotating coordinate system, pseudo-accelerations never appear in the
equations. Based on this model, spherical extended and sigma point Kalman ﬁlters are
addressed. In addition, for large ranges, the general nonlinear spherical dynamic equa-
tions are shown to reduce to a set of linear constant angular rate dynamic equations,
leading to a much simpler spherical linear Kalman ﬁlter.
Filter implementation issues are discussed in Section 18.3, beginning in Section
18.3.1 with a description of the meaning of the components of the dynamic noise
term q (to be deﬁned below) and how their values can be set based on knowledge of
expected target acceleration limits.
All tracking ﬁlters require two essential sets of data as inputs: a set of observa-
tions and a set of initializations. In Section 18.3.2, we explain how observations are
generated for a notional radar, one that is used for all of performance assessments

TRACKING A TARGET IN CARTESIAN COORDINATES
261
discussed in Section 18.4. The method we used to create initialization values based
on the ﬁrst few noisy observations is discussed in Section 18.3.3.
In Section 18.4 the performance of the Cartesian ﬁlters are compared to that of the
spherical ﬁlters. Through a performance analysis with three simulated trajectories,
where the level of target maneuvering increases from trajectory to trajectory, we show
that the spherical track estimation ﬁlters have the better performance during target
maneuvers and when the target approached zero range.
Finally, in Section 18.5, we present some observations about this case study and
givesomegeneralguidelinesforpotentialfutureexpansionoftheworkpresentedhere.
Two appendices are included at the end of this chapter because of their importance
in understanding the development of three-dimensional tracking. Since we use simu-
lations throughout this chapter, Appendix APPENDIX 18.A contains a discussion of
a method for generating the trajectory of a maneuvering object in three dimensions.
Usually this is accomplished using a realistic high-ﬁdelity six-degree-of-freedom (6
DOF) simulation of the maneuvering object using a complete model of the forces
that act on the object, such as thrust, friction, gravity, and so on. But since we are
only interested in evaluating the performance of a variety of tracking algorithms, in
Appendix APPENDIX 18.A we present a lower ﬁdelity simulation of object motion,
one that simply models the possible maneuvers through linked constant horizontal
and vertical turn rate motions that produce a realistic looking trajectory based on a de-
sired turn/dive accelerations. Utilizing this constant turn rate analytical model, three
trajectories examples were created that have increasing amounts of target maneuvers.
These trajectories are used in Section 18.4 to assess the performance of all trackers
as the amount of target maneuvering increases.
In developing tracking algorithms to track an object in three dimensions one has
a choice of coordinate systems, usually Cartesian or spherical. Since most sensors
used in tracking applications, such as radars or sonars, produce measurements in
spherical coordinates, six-dimensional (three position and three velocity components)
coordinate transformation subroutines are essential tools. Most coordinate transfor-
mation subroutines treat just the state vector transformation. However, many tracking
algorithms require coordinate transformation of a covariance matrix or require the
calculation of a Jacobian. In Appendix APPENDIX 18.B we present complete coor-
dinate transformations from Cartesian to spherical and from spherical to Cartesian,
including the transformation of covariance matrices. As a by-product required for
the covariance transformation, the Jacobian is also produced. Since we use an East-
North-Up (x,y,z axes positive directions) Cartesian coordinate system in our analysis,
transformations in this coordinate system are the ones presented. However, a general
method is also presented that will allow the reader to derive such transformations for
any desired three-dimensional coordinate system.
18.1
TRACKING A TARGET IN CARTESIAN COORDINATES
In this section, we discuss methods of tracking an object when our tracking ﬁlter
uses a state vector that is deﬁned in Cartesian coordinates. The section is divided into

262
A SPHERICAL CONSTANT VELOCITY MODEL
subsections where we discuss the dynamic model used to deﬁne the expected object
motion (Section 18.1.1), the observation vector and how it is related to the state vector
(Section 18.1.2), and the various Cartesian Gaussian ﬁlter tracking algorithms that
can applied to this problem (Section 18.1.3). A discussion of ﬁlter initialization and
the creation of a Monte Carlo set of observations will be delayed until Sections 18.3.
18.1.1
Object Dynamic Motion Model
The standard method for tracking a moving object from a set of stationary radars
in Cartesian coordinates assumes that the object travels at a constant velocity. This
results in a linear dynamic model of the form
xn = Fn−1xn−1 + vn−1
(18.1)
where the Cartesian state vector at time tn is deﬁned by
xn ≜x (tn) =

rx,n, vx, ry,n, vy, rz,n, vz
⊺
(18.2)
and the transition matrix Fn is deﬁned as
Fn =


F1,n
02
02
02
F1,n
02
02
02
F1,n


(18.3)
with
F1,n =
	
1
Tn
0
1

(18.4)
02 a 2 × 2 matrix of zeros and Tn is deﬁned as
Tn ≜tn −tn−1
(18.5)
Note that Tn can be considered time variable, to allow for the temporally asynchronous
nature of multisensor tracking. However, in what follows in our analysis, we use only
single sensor cases, so we always consider T as invariant with time.
The dynamic acceleration noise vn is considered to be independent zero-mean
Gaussian acceleration noise deﬁned by
vn ∼N (0, Qn)
(18.6)
with
Qn =


qx,nQ1
02
02
02
qy,nQ1
02
02
02
qz,nQ1


(18.7)

TRACKING A TARGET IN CARTESIAN COORDINATES
263
and, for an object undergoing continuous motion, Q1 is given by [3]
Q1 =
	 T 3
3
T 2
2
T 2
2
T

(18.8)
Here, the symbol ∼indicates that vn is drawn from the Gaussian distribution
N (0, Qn).
An important ﬁlter design parameter is qi,n∀i. We will show how to choose qi,n
when we discuss the design of various ﬁlter options, based on the what we expect
the characteristics of object motion will be, that is, will the object be maneuvering or
traveling in a straight line.
18.1.2
Sensor Data Model
As noted in Section 18.3.2, most radar sensors output observation (measurement) data
in two-dimensional polar or three-dimensional spherical coordinates, independent of
the coordinate system chosen for the dynamic model. Consequently, we will deﬁne
the complete sensor data vector as the following vector
zn =

Rn, ˙Rn, θn, φn
⊺
(18.9)
where the set {Rn, ˙Rn, θn, φn} are the range, Doppler (range rate), bearing, and
elevation relative to the ﬁxed sensor position, respectively, of the object to be tracked.
For sensors that do not have the full compliment of observation components, we
merely drop those components that are not available for that particular radar. For
example, in the simulations used for performance analysis below, the sensor model
chosen contained only range, bearing, and elevation.
The observation vector is related to the Cartesian state vector through the following
nonlinear spherical-to-Cartesian transformation, as deﬁned by (18.160) and (18.161)
zn =


Rn
˙Rn
θn
φn

= hn (xn) =


h1,n (xn)
h2,n (xn)
h3,n (xn)
h4,n (xn)

=



r2x + r2y + r2z
rxvx+ryvy+rzvz
R
tan−1 
rx
ry

tan−1

rz

r2x+r2y



(18.10)
Generally, sensor noise is included in the observation model so that the observation
model becomes
zn = hn (xn) + wn
(18.11)
where the observation noise wn is considered to be independent zero-mean Gaussian
noise deﬁned by
wn ∼N (0, Rn)
(18.12)

264
A SPHERICAL CONSTANT VELOCITY MODEL
with Rn usually considered to be independent of time and deﬁned by the diagonal
covariance matrix of sensor measurement variances given by
R =


σ2
R
0
0
0
0
σ2
˙R
0
0
0
0
σ2
θ
0
0
0
0
σ2
φ


(18.13)
For real sensor data, the sensor measurement standard deviations are usually output
as part of the sensor data stream at every temporal sensor update, so the components
of R may be time dependent if the signal-to-noise ratio at the output of the sensor
detector varies. For the simulations used for the performance analysis below, R will
be consider as invariant in time.
18.1.3
Gaussian Tracking Algorithms for a Cartesian State Vector
18.1.3.1
The Cartesian Linear State Prediction Equations. Since the Cartesian
dynamic equation is linear, we can use the linear state and state covariance prediction
equations from Table 6.1
ˆxn|n−1 = Fn−1ˆxn−1|n−1
(18.14)
Pxx
n|n−1 = Fn−1Pxx
n−1|n−1F⊺
n−1 + Qn−1
(18.15)
18.1.3.2
Cartesian Nonlinear Observation Prediction. On the other hand, from
(18.10) we see that the observation model is nonlinear with respect to the state vector,
so observation prediction must be performed with one of the suboptimal ﬁlters from
Part II. For this case study, we used only the EKF and all of the sigma point KFs, with
the exception of the MCKF.
18.1.3.2.1
The Cartesian Linearized EKF Observation Prediction Equations. From
Table 7.2, the EKF observation prediction equations are given by
ˆzn|n−1 = hn
ˆxn|n−1

(18.16)
Pzz
n|n−1 = ˆHn|n−1Pxx
n|n−1 ˆH⊺
n|n−1 + Rn
(18.17)
Pxz
n|n−1 = Pxx
n|n−1 ˆH⊺
n|n−1
(18.18)
where the components of the Jacobian matrix ˆHn|n−1 are presented in (18.164)–
(18.190).
18.1.3.2.2
The Cartesian Nonlinear Sigma point Observation Prediction Equa-
tions. For the sigma point Kalman ﬁlter observation prediction equations shown in

TRACKING A TARGET IN SPHERICAL COORDINATES
265
Figure 13.3, we write
Dn|n−1 =

Pxx
n|n−1
1/2
(18.19)
χ(j)
n|n−1 = ˆxn|n−1 + Dn|n−1c(j)
(18.20)
ψ(j)
n|n−1 = hn

χ(j)
n|n−1

(18.21)
ˆzn|n−1 =
Ns

j=0
wjψ(j)
n|n−1
(18.22)
Pzz
n|n−1 =
Ns

j=0
wj

ψ(j)
n|n−1 −ˆzn|n−1
 
ψ(j)
n|n−1 −ˆzn|n−1
⊺
+ Rn−1
(18.23)
Pxz
n|n−1 =
Ns

j=0
wj

χ(j)
n|n−1 −ˆxn|n−1
 
ψ(j)
n|n−1 −ˆzn|n−1
⊺
(18.24)
where the set

wj, c(j)
can be calculated ofﬂine using Listings 13.1 and 13.2
(see Chapter 13).
18.1.3.3
The Kalman Filter Update Equations. The ﬁnal step in track estimation
is applying the Kalman ﬁlter update equations (5.29)–(5.31)
Kn = Pxz
n|n−1

Pzz
n|n−1
−1
(18.25)
ˆxn|n = ˆxn|n−1 + Kn

zn −ˆzn|n−1

(18.26)
Pxx
n|n−1 = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
(18.27)
18.2
TRACKING A TARGET IN SPHERICAL COORDINATES
In this section, we discuss tracking an object using a spherical state vector deﬁned as
ρ = [R, vr, θ, vh, φ, vv]⊺
(18.28)
with the spherical velocity components {vr, vh, vv} to be deﬁned below. As we did for
the Cartesian case, where we assumed that the components of the Cartesian velocity
were constant resulting in a Cartesian dynamic motion model given by (18.1), our
objective will be to develop a three-dimensional dynamic motion model for an object
with constant spherical velocity components.

266
A SPHERICAL CONSTANT VELOCITY MODEL
18.2.1
State Vector Position and Velocity Components in Spherical
Coordinates
Referring to Figure 18.25, the position of an object can be written as
rp = xex + yey + zez
(18.29)
or
rp = arer + aθeθ + aφeφ
(18.30)
where {ex, ey, ez} and {eR, eθ, eφ} are unit vector sets in Cartesian and spherical
coordinates, respectively.
The spherical-to-Cartesian transformation of the position components are deﬁned
by (18.193), so we can rewrite (18.29) as
rp = exR sin θ cos φ + eyR cos θ cos φ + ezR sin φ
(18.31)
By deﬁnition, the spherical unit vectors are given by
er =
1
 ∂rp
R

∂rp
R
(18.32)
eθ =
1
 ∂rp
θ

∂rp
θ
(18.33)
eφ =
1
 ∂rp
φ

∂rp
φ
(18.34)
Using (18.31) in (18.32) results in the following expression for er in terms of the set

ex, ey, ez

er = ex sin θ cos φ + ey cos θ cos φ + ez sin φ
(18.35)
Similarly, we obtain
eθ = ex cos θ −ey sin θ
(18.36)
and
eφ = −ex sin θ sin φ −ey cos θ sin φ + ez cos φ
(18.37)
It follows immediately that the time derivative of the spherical unit vectors are
given by
˙er = ∂er
∂R
˙R + ∂er
∂θ
˙θ + ∂er
∂φ
˙φ
= eθ ˙θ cos φ + eφ ˙φ
(18.38)

TRACKING A TARGET IN SPHERICAL COORDINATES
267
˙eθ = ∂eθ
∂R
˙R + ∂eθ
∂θ
˙θ + ∂eθ
∂φ
˙φ
= −er ˙θ cos φ + eφ ˙θ sin φ
(18.39)
and
˙eφ = ∂eφ
∂R
˙R + ∂eφ
∂θ
˙θ + ∂eφ
∂φ
˙φ
= −eθ ˙θ sin φ −er ˙φ
(18.40)
Using (18.35), we can now write (18.31) as
rp = exR sin θ cos φ + eyR cos θ cos φ + ezR sin φ
= R

ex sin θ cos φ + ey cos θ cos φ + ez sin φ

= Rer
(18.41)
Since the velocity vector is the time derivative of the position vector, we obtain
v = d
dt (Rer) = ˙Rer + R˙er
= er ˙R + eθR ˙θ cos φ + eφR ˙φ
(18.42)
we can now identify the components of v in spherical coordinates as
v =


vr
vh
vv

=


˙R
R ˙θ cos φ
R ˙φ

=


˙R
Rh ˙θ
R ˙φ


(18.43)
where we have deﬁned the horizontal range Rh ≜R cos φ. Using (18.43) as the ve-
locity components gives us all of the components of the spherical state vector (18.28).
18.2.2
Spherical State Vector Dynamic Equation
For a constant velocity dynamic model of object motion, we assume that ˙vr = ˙vh =
˙vv = 0. Taking the time derivative of each component in (18.43) yields
˙vr = ¨R = 0
(18.44)
˙vh = ˙R ˙θ cos φ + R¨θ cos φ −R˙θ ˙φ sin φ = 0
(18.45)
and
˙vv = ˙R ˙φ + R¨φ = 0
(18.46)

268
A SPHERICAL CONSTANT VELOCITY MODEL
Or, after rearranging, we obtain
¨θ = ˙θ

˙φ tan φ −
˙R
R

(18.47)
and
¨φ = −
˙R
R
˙φ
(18.48)
The components of the dynamic motion equation will now be developed. For range,
we can write
Rn = Rn−1 + T ˙Rn−1 + T 2
2
¨Rn−1
(18.49)
Using (18.44), and the deﬁnition of vr from (18.43), this becomes
Rn = Rn−1 + Tvr,n−1
(18.50)
For the radial speed we have
vr,n = vr,n−1 + T ˙vr
= vr,n−1
(18.51)
Writing an equation similar to (18.49) for the bearing, substituting (18.47) for ¨θn−1
and using the deﬁnitions of the velocity components from (18.43), we obtain
θn = θn−1 + T vh,n−1
Rh,n−1

1 +
T
2Rn−1

vv,n−1 tan φn−1 −vr,n−1

(18.52)
In a manner similar to (18.51), it follows that
vh,n = vh,n−1
(18.53)
In addition, it follows that the elevation components are given by
φn = φn−1 + T vv,n−1
Rn−1

1 −T
2
vr,n−1
Rn−1

(18.54)
and
vv,n = vv,n−1
(18.55)
The nonlinear dynamic motion equation for the spherical state vector can now be
written as
ρn = f

ρn−1

+ nn−1
(18.56)
with the zero-mean Gaussian dynamic acceleration noise distribution
nn ∼N

0, Qρ

(18.57)

TRACKING A TARGET IN SPHERICAL COORDINATES
269
The components of f

ρn−1

are given by the nonlinear equations (18.50)–(18.55)
and
Qρ =


qrQ1
02
02
02
qhQ1
02
02
02
qvQ1


(18.58)
with
Q1 =
	 T 3
3
T 2
2
T 2
2
T

(18.59)
18.2.2.1
Long-Range Limit of the Nonlinear Spherical Dynamic Equations. Us-
ing the components of (18.43), we can replace vh,n−1/Rh,n−1 and vv,n−1/Rn−1 in
(18.52) and (18.54) by ˙θn−1 and ˙φn−1, respectively. Now, in the limit as Rn−1 →∞,
(18.50)–(18.55) become the linear constant rate dynamic equations
Rn = Rn−1 + T ˙Rn−1
(18.60)
˙Rn = ˙Rn−1
(18.61)
θn = θn−1 + T ˙θn−1
(18.62)
˙θn = ˙θn−1
(18.63)
φn = φn−1 + T ˙φn−1
(18.64)
˙φn = ˙φn−1
(18.65)
For this case, the spherical state vector at time tn can be represented by
rn =

Rn, ˙Rn, θn, ˙θn, φn, ˙φn
⊺
(18.66)
and the dynamic equation becomes the linear equation
rn = Frn−1 + nn−1
(18.67)
where
F =


F1
02
02
02
F1
02
02
02
F1


(18.68)
with
F1 =
	
1
T
0
1

(18.69)

270
A SPHERICAL CONSTANT VELOCITY MODEL
and
Qr =


qrQ1
02
02
02
qθQ1
02
02
02
qφQ1


(18.70)
18.2.3
Observation Equations with a Spherical State Vector
The sensor data vector is given in (18.9). Based on this data vector, we can write the
linear observation equation related to either form of the spherical dynamic equation,
(18.56) or (18.67), as
zn = Hρn + wn−1
(18.71)
= Hrn + wn−1
(18.72)
with
H =


1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0


(18.73)
18.2.4
Gaussian Tracking Algorithms for a Spherical State Vector
18.2.4.1
Track Estimation for Large R. Consider ﬁrst the case where R ≫1. We
see that both the dynamic and observation equations, (18.67) and (18.72) respectively,
are linear, so the linear Kalman ﬁlter developed in Chapter 6 is the optimal ﬁlter. As
shown in Chapter 6, the LKF state prediction equations for this case are given by
ˆrn|n−1 = Fˆrn−1|n−1
(18.74)
Prr
n|n−1 = FPrr
n−1|n−1F⊺+ Qr
(18.75)
with F and Q shown in (18.68) and (18.70), respectively.
The observation prediction equations can be written as
ˆzn|n−1 = Hˆrn|n−1
(18.76)
Pzz
n|n−1 = HPrr
n|n−1H⊺+ R
(18.77)
Pxz
n|n−1 = Prr
n|n−1H⊺
(18.78)
with H and R given by (18.73) and (18.13), respectively.

TRACKING A TARGET IN SPHERICAL COORDINATES
271
And ﬁnally, the Kalman ﬁlter update equations are the usual
Kn = Pxz
n|n−1

Pzz
n|n−1
−1
(18.79)
ˆrn|n = ˆrn|n−1 + Kn

zn −ˆzn|n−1

(18.80)
Prr
n|n = Prr
n|n−1 −KnPzz
n|n−1K⊺
n
(18.81)
18.2.4.2
Track Estimation for the Nonlinear Spherical dynamic Model. For this
spherical state vector case, we have the exact opposite of what we had in the Carte-
sian state vector case. For the Cartesian state vector with constant Cartesian velocity
motion, the state prediction equations were linear but the observation equations were
nonlinear, requiring one of the nonlinear suboptimal tracking ﬁlters for observation
prediction. Now, the state vector prediction equations are nonlinear while the obser-
vation prediction equations are linear and one of the nonlinear suboptimal tracking
ﬁlters must be used for the state prediction step. Below, we will ﬁrst examine the
modiﬁcations required to utilize the linearized EKF for the state prediction step and
then we will consider the sigma point methods.
18.2.4.2.1
Applying the EKF in Spherical Coordinates. In order to use the EKF
for state prediction in spherical coordinates, we must ﬁrst ﬁnd the Jacobian of the
nonlinear state transition equation
ρn = f

ρn−1

(18.82)
where the components of the transition are shown in (18.50)–(18.55). Now, the EKF
state prediction equations are
ˆρn|n−1 = f

ˆρn−1|n−1

(18.83)
Pρρ
n|n−1 = ˆFρ,n−1Pρρ
n−1|n−1 ˆF⊺
ρ,n−1 + Qρ
(18.84)
where ˆFρ,n−1 is the Jacobian of the transition equation (18.82) at time tn−1.
Applying the general methods found in Appendix APPENDIX 18.B, we ﬁnd that
the Jacobian of (18.82) is given by
ˆFρ =


1
T
0
0
0
0
0
1
0
0
0
0
F31
F32
1
F34
F35
F36
0
0
0
1
0
0
F51
F52
0
0
1
F56
0
0
0
0
0
1


(18.85)

272
A SPHERICAL CONSTANT VELOCITY MODEL
with
F31 = −Tvh
RRh

1 + T
R (vv tan φ −vr)

(18.86)
F32 = −T 2vh
2RRh
(18.87)
F34 = T
Rh

1 + T
2R (vv tan φ −vr)

(18.88)
F35 = T 2vh
2RRh
vv sec2 φ
(18.89)
F36 = T 2vh
2RRh
tan φ
(18.90)
F51 = −Tvv
R2

1 + Tvr
R

(18.91)
F52 = T 2vv
2R2
(18.92)
F56 = T
R

1 + Tvr
R

(18.93)
where we have removed the subscript n for clarity.
Since none of the components of the observation vector depend on vh or vv, the
observation prediction and update equations are identical to those of the spherical
LKF presented in Section 18.2.4.1.
18.2.4.2.2
Applying a Sigma Point Kalman Filter in Spherical Coordinates. The
ﬁrst step in the process is to generate a set of sigma points from the prior ﬁlter output
{ ˆρn−1|n−1, Pρρ
n−1|n−1} or from the initialization values using
(j)
n−1|n−1 = ˆρn−1|n−1 + Dn−1|n−1c(j), j = 0, 1, . . . Ns
(18.94)
where
Dn−1|n−1 =

Pρρ
n−1|n−1
1/2
(18.95)
The set

wj, c(j), j = 0, 1, . . . Ns

are generated using the subroutines presented in
Listings 13.1 and 13.2. They will depend on the speciﬁc sigma point Kalman ﬁlter
chosen and the dimension of the state vector.

IMPLEMENTATION OF CARTESIAN AND SPHERICAL TRACKING FILTERS
273
Now, the state prediction equations for the sigma point Kalman ﬁlter are given by
(j)
n|n−1 = f

(j)
n−1|n−1

(18.96)
ˆρn|n−1 =
Ns

j=0
wj(j)
n|n−1
(18.97)
Pρρ
n|n−1 =
Ns

j=0
wj

(j)
n|n−1 −ˆρn|n−1
 
(j)
n|n−1 −ˆρn|n−1
⊺
(18.98)
+Qρ
(18.99)
And, as with the spherical EKF, the linear observation prediction and the update
equations are identical to those of the spherical LKF presented above.
18.3
IMPLEMENTATION OF CARTESIAN AND SPHERICAL
TRACKING FILTERS
In order to execute any of the tracking ﬁlters presented above, one must
1. specify a set of values for q,
2. generate a Monte Carlo set of noisy data that can be used as observations for
the ﬁlters, and
3. create an initialization state vector and covariance matrix.
We address each of these in the sections below.
18.3.1
Setting Values for q
Consider the prediction equation for the Cartesian state covariance matrix given in
(18.15). If we compare the upper left 2 × 2 set of components of the left-hand side
of (18.15) with the similar set of components of Q from (18.8), it follows that
	
Pxx
Px ˙x
Px˙x
P ˙x ˙x

=
	
σ2
x
•
•
σ2
vx

∝
	
qx T 3
3
qx T 2
2
qx T 2
2
qxT

(18.100)
Thus, σ2
vx ∝qxT, and it follows immediately that qx has units of [distance2/time3].
Or, we could write this as [acceleration2 × time] or [acceleration2/Hz].
Assuming that we know something about the maneuverability characteristics of
the object we are trying to track, we can estimate the value required for qx. Since
our dynamic model is one of constant velocity with zero-mean Gaussian acceleration
noise, the value for qx can be set to twice the largest possible maneuver acceleration

274
A SPHERICAL CONSTANT VELOCITY MODEL
of the object. It is doubled to account for turns to the left or right. So we set
qx = (2a)2
(18.101)
If a is given in g’s (the acceleration due to gravity), we must convert the units to
(nmi/s2)2. For example, if an object can execute a turn at a maximum of 3 g’s, we
have approximately
qx = (2 × 3 g)2 ×

32.174 ft/s2/g
2
×

1.65 × 10−4nmi/ft
2
= 36 × 2.80 × 10−5 
nmi/s22
(18.102)
From (18.7) we see that separate values can be set for q for different components.
Thus, for a Cartesian ﬁlter, we could set one value for horizontal maneuvers (qx and
qy) and a different value for vertical maneuvers (qz).
For the spherical ﬁlters we have the sets {qr, qh, qv} for constant spherical velocity
ﬁlters and

qr, qθ, qφ

for the constant rate spherical ﬁlter. Generally, we would
expect that we could set qr and qh to the same values that we set the Cartesian
horizontal q’s and qv to the value of the Cartesian vertical qz. For the constant rate
spherical ﬁlters, it is easy to show that the angular q’s should be set such that qθ =
qr/R2
h and qφ = qr/R2. However, as we will show later, higher values of the spherical
q’s gives better performance.
18.3.2
Simulating Radar Observation Data
In order to conduct a performance comparison of a set of track estimation algorithms,
each tracking algorithm must use the identical set of noisy observations for a given
Monte Carlo run. In addition, for each Monte Carlo run, a different set of observations
must be generated using a new set of noise data that is independent of the previous
set. So, if we are going to use 100 Monte Carlo runs to compare the performance of
multiple track estimation algorithms, we need 100 sets of noisy observations where
the noise is independent from time sample to time sample and also independent from
one Monte Carlo set to all others.
To track the object with a trajectory generated using the low-ﬁdelity simulation
presented in Appendix APPENDIX 18.A, we ignore the effects of atmospheric round-
trip propagation on the radar signal as well as the radar detectors detection statistics
and we will assume a detection probability of 100%. To generate observation sets,
the characteristics of the radar must be set:
• What variables constitute the radar observation vector.
• What is the radar update rate.
• What is the observation noise standard deviation for each observation variable.
For a general radar, the observation vector can consist of range to the object (R)
and the rate at which the range is changing
 ˙R

, the horizontal bearing to the object

IMPLEMENTATION OF CARTESIAN AND SPHERICAL TRACKING FILTERS
275
relative to true North (θ) and the elevation of the object above the ground (φ). Thus,
the observation vector can be
z =

R, ˙R, θ, φ
⊺
(18.103)
Note that in our spherical polar coordinate system −180◦≤θ ≤180◦and 0◦≤φ ≤
90◦.
It is usually assumed that the noise on each component of the observation vector
are independent and Gaussian resulting in an observation noise covariance matrix
given by
R =


σ2
R
0
0
0
0
σ2
˙R
0
0
0
0
σ2
θ
0
0
0
0
σ2
φ


(18.104)
where σi is the standard deviation of the noise for the ith observation vector compo-
nent.
In the track estimation performance results presented in the next section, we use
a radar model that measures only the range, bearing, and elevation. The observation
uncertainties used to generate the noisy observations are
σR = 0.15 nmi
(18.105)
σθ = 0.3 deg
(18.106)
σφ = 0.3 deg
(18.107)
Our simulated radar will have an update period of one observation every 4 s for an
update rate of 0.25 Hz.
Using the methods of Appendix APPENDIX 18.A, several Cartesian target trajec-
tories have been generated, with the simulated trajectory data generated at a 10 Hz
rate. Since our radar model has a much lower update rate, the trajectory data must ﬁrst
be decimated down to the radar update rate so at to produce the truth values of the ob-
servations prior to adding observation noise. We could also add a variable start time,
but for our purposes that is not necessary, so we will start the observations at t0 = 0.
If a variable start time is used, and the start time contains more than one decimal place
(e.g., t0 = 0.12345 s), one must use interpolation to generate the Cartesian samples
from the trajectory data. Thus if our original trajectory contained 4000 samples, we
will end up with 100 samples from which we generate observation sets.
Assume that the decimated spherical truth data is contained in the variable Polar
Truth (of dimension P × N, where P is the dimension of the observation vector and
N is the number of time samples), then noise is added to the truth trajectory samples
using the Matlab snippet contained in Listing 18.1 (Stream is a placeholder for the
random number stream found in the latest version of randn function).

276
A SPHERICAL CONSTANT VELOCITY MODEL
Listing 18.1 Adding Random Observation Noise
Observations = PolarTruth + sqrtm(R)*randn(Stream,P,N);
AseparatesetofobservationsisgeneratedforeachMonteCarlorun.Theﬁnalsetof
observations therefore contains N samples for each Monte Carlo run. This procedure
can be completed ofﬂine and independent of the tracker performance computations.
18.3.3
Filter Initialization
All sequential Kalman tracking ﬁlters require initialization of both the state vector and
its covariance matrix, ˆx0|0 and Pxx
0|0, respectively. Assuming that one has access to a
stream of sensor data (either simulated, as described above, or real), the easiest way to
generate the required initialization values is to utilize the ﬁrst few sensor observation
data values to create an initial estimate of a Cartesian state vector.
If a full noisy spherical polar six-dimensional state vector were available as an
observation vector, we could generate an initial Cartesian state vector by simply
averaging a few samples of noisy spherical state vector data and then use the polar-
to-Cartesian transformation deﬁned by (18.195). But we will have, at most, four of
the six spherical components available in the data vector.
Therefore, since all components of a spherical state vector will never be available
from the observations, one needs to set up default values for those spherical com-
ponents that cannot be obtained directly from the available sensor dependent data.
The easiest way to accomplish this is to generate a set of nominal values for all the
components of both the spherical state vector and their corresponding standard de-
viations. Generally, all sensors provide bearing and bearing uncertainty so we will
not set nominal values for these but instead assume that they will always be available
from the sensor. We have created a nominal set of spherical initialization values that
will be used for all track estimation ﬁlters used for performance analysis. These are
listed in Table 18.1. All off-diagonal terms in the spherical initialization covariance
matrix will be set to zero.
TABLE 18.1
Nominal Values for Initialization State Vector and Covariance
Components
Nominal
Nominal
Spherical Vector
Standard
Components
Deviations
Range (nmi)
10
0.1
Range rate (knots)
−600
5
Bearing (rad)
Available from sensor
Available from sensor
Bearing rate (rad/s)
0
10−4
Elevation (rad)
0
10−2
Elevation rate (rad/s)
0
10−4

IMPLEMENTATION OF CARTESIAN AND SPHERICAL TRACKING FILTERS
277
When observation data is available for one or more of the spherical components,
initialization values can be computed from several successive samples of the data.
For position data (R, θ, φ), we generate an initialization value by averaging several
consecutive data values, for example, for range we could average the ﬁrst two data
samples
R0 = 1
2 (R1 + R2)
(18.108)
For the rate components, we could use two of the ﬁrst three data points to compute the
central difference formula from (2.44), for example, range rate could be computed
from
˙R0 = 1
2 (R3 −R1)
(18.109)
For cases where one is processing asynchronous multisensor data, some modiﬁcations
must be made to account for the fact that some components might be available from
one sensor but not another.
Once complete spherical state vector and covariance matrices are available for
initialization, a simple spherical-to-Cartesian transformation is made using the for-
mulas from Appendix APPENDIX 18.B. This results in the initialization Cartesian
state vector and covariance matrix. We always ensure that the initialization state vec-
tor and covariance matrix set is given in Cartesian coordinates and is the same for all
Monte Carlo runs and all track estimation methods.
For the spherical constant rate ﬁlter, these Cartesian initialization values must be
converted back to spherical to generate the set {ˆr0|0, Prr
0|0}. Once these are available,
they can be converted to the initialization set { ˆρ0|0, Pρρ
0|0} for the constant velocity
spherical ﬁlters. Note that the transformation from r to ρ is given by
ρ =


R
vR
θ
vh
φ
vv


= g (r) =


R
˙R
θ
˙θR cos φ
φ
˙φR


(18.110)
and that
Pρρ = ˆGP
rr ˆG⊺
(18.111)

278
A SPHERICAL CONSTANT VELOCITY MODEL
where ˆG is the Jacobian of the transformation g (r) . Evaluating the partials of the
Jacobian leads to
ˆG =


1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
˙θ cos φ
0
0
R cos φ
−R ˙θ sin φ
0
0
0
0
0
1
0
˙φ
0
0
0
0
R


(18.112)
18.4
PERFORMANCE COMPARISON FOR VARIOUS
ESTIMATION METHODS
18.4.1
Characteristics of the Trajectories Used for Performance Analysis
For the purpose of analysis, three target motion scenarios were created using the
method outlined in Appendix APPENDIX 18.A. The trajectories associated with
each of the three scenarios are shown in Figures 18.1–18.3.
The ﬁrst scenario, with the trajectory shown in Figure 18.1, is for a radially moving
target that has a constant Cartesian velocity. For this trajectory, the target moves
radially inbound at a constant height of 1 nmi (height not shown in Figure 18.1) and
passes directly over the radar sensor located at (0, 0, 0). The second scenario, with
the trajectory shown in Figure 18.2, consists of a target that executes two turns, with
a 90 deg 2 g turn in the ﬁrst loop and a 90 deg 5 g turn in the second loop. The third
−5
0
5
10
15
20
25
30
35
−5
0
5
10
15
20
25
30
35
X-position (nmi)
Y-position (nmi)
FIGURE 18.1
A simulated radially inbound target truth track.

PERFORMANCE COMPARISON FOR VARIOUS ESTIMATION METHODS
279
0
5
10
15
20
25
30
35
0
10
20
30
40
0
0.5
1
1.5
2
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
FIGURE 18.2
A simulated horizontally looping target truth track.
trajectory is for a highly maneuvering target that undergoes a 7 g turn followed by a
6 g turn, a deceleration and 5 g dive, a leveling off and an acceleration followed by
a ﬁnal 7 g turn, as shown in Figure 18.3. All three of these scenarios are similar to
those considered by Zollo and Ristic [5].
The speciﬁc initial characteristics of all three scenarios are presented in Table
18.2. For all tracking ﬁlters used in this performance analysis, the values for the
components of q were set to {qx = qy = 225; qz = 1.5} for the Cartesian trackers,
{qr = qh = 225; qv = 1.5} for the spherical constant velocity trackers, and {qr =
225; q ˙θ = q ˙φ = 1.5} for the spherical constant rate ﬁlters. The q values of 225 or
1.5 correspond to expected maximum turn/dive accelerations of 7.5 g’s or 0.61 g’s,
respectively.
0
10
20
30
40
50
60
0
5
10
15
20
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
FIGURE 18.3
A simulated benchmark target truth track.

280
A SPHERICAL CONSTANT VELOCITY MODEL
TABLE 18.2
Initial Characteristics of Three Simulated Scenarios
Trajectory
Radial Trajectory
Loop
Benchmark
Speed (knots)
350
700
700
Initial position
(nmi) (x, y, z)
(31.82, 31.82, 1)
(30.78, 30.78, 1)
(50, 20, 3)
Initial range (nmi)
45
40
58.58
Initial bearing
45o
45o
70o
Initial heading
−135o
−135o
−90o
Initial attitude
0o
0o
0o
We ﬁrst examine the velocity characteristics of the trajectories in each scenario
to demonstrate how well the trajectories conform to either of the constant velocity
models. Figures 18.4–18.6 show the components of both the Cartesian and spherical
velocities for the three scenarios. From Figure 18.4, it is obvious that the velocities
are constant for the radially inbound target for the Cartesian velocities but spherical
velocity components for range and vertical velocity show variations in the region
where the track crosses over the sensor at the origin where they both undergo a
sudden change in sign. Therefore, we should expect little difference in performance
between the two tracker models in regions away from the origin but some difﬁculty
with the spherical trackers as the object nears the origin. Although not shown, the
vr (nmi/s)
vh (nmi/s)
vv (nmi/s)
0
50
100 150 200 250 300 350 400 450 500
0
50
100 150 200 250 300 350 400 450 500
0
50
100 150 200 250 300 350 400 450 500
0
50
100 150 200 250 300 350 400 450 500
0
50
100 150 200 250 300 350 400 450 500
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
vx (nmi/s)
vy (nmi/s)
–0.2
–0.1
0
0.1
0.2
–0.2
–0.1
0
0.1
0.2
vz (nmi/s)
Time (s)
0
50
100 150 200 250 300 350 400 450 500
Time (s)
FIGURE 18.4
Components of both the cartesian and spherical velocities for the radially
inbound trajectory.

PERFORMANCE COMPARISON FOR VARIOUS ESTIMATION METHODS
281
vr (nmi/s)
vh (nmi/s)
vv (nmi/s)
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
vx (nmi/s)
vy (nmi/s)
–0.2
–0.1
0
0.1
0.2
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
–0.2
–0.1
0
0.1
0.2
vz (nmi/s)
Time (s)
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
0
50
100
150
200
250
300
350
400
Time (s)
FIGURE 18.5
Components of both the Cartesian and spherical velocities for the loop tra-
jectory.
vr (nmi/s)
vh (nmi/s)
vv (nmi/s)
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
vx (nmi/s)
vy (nmi/s)
0
50
100
150
200
0
50
100
150
200
0
50
100
150
200
0
50
100
150
200
0
50
100
150
200
–0.2
–0.1
0
0.1
0.2
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
–0.2
–0.1
0
0.1
0.2
vz (nmi/s)
Time (s)
0
50
100
150
200
Time (s)
FIGURE 18.6
Components of both the Cartesian and spherical velocities for the benchmark
trajectory.

282
A SPHERICAL CONSTANT VELOCITY MODEL
rates for the constant rate spherical ﬁlter show large changes at short ranges due to
sudden changes as the target goes through the origin.
The velocity comparison for the loop trajectory scenario, shown in Figure 18.5,
indicates that the velocities show similar variations in both Cartesian and spherical,
so there should be little difference in the tracker performance.
Finally, for the highly maneuvering trajectory, the velocity components shown in
Figure 18.6 indicate that the spherical trackers should perform as well as the Cartesian
trackers, due to the similar variations in the velocities as a function of time.
18.4.2
Filter Performance Comparisons
Since we use the same track estimation ﬁlters (EKF, UKF, SSKF, and GHKF) for both
the Cartesian and spherical trackers, we differentiate them by using the precursors C-
and S- for the Cartesian and spherical track algorithms, respectively. Four Cartesian
Kalman ﬁlters, the C-EKF, C-UKF, C-SSKF, and C-GHKF, were used to generate
track estimates for the Cartesian constantvelocity model and four spherical Kalman
ﬁlters, S-EKF, S-UKF, S-SSKF, and S-GHKF, were utilized to generate estimated
tracks for the spherical constant velocity model. The S-LKF was used for the spherical
constant rate long-range approximation model.
In each case, 100 Monte Carlo runs were performed. For the Cartesian tracking
algorithms, the Cartesian estimated track outputs were converted to spherical coor-
dinates and for the spherical trackers, the spherical track outputs were converted to
Cartesian coordinates, with all coordinate transformations accomplished using the al-
gorithms contained in Appendix 18.B. Thus, for all trackers, both Cartesian and spher-
ical track estimates were generated. Based on the 100 Monte Carlo runs, RMS errors
were calculated for the position estimates in both Cartesian and spherical coordinates.
18.4.2.1
Track Performance Comparison for the Radially Inbound Target Tra-
jectory. Figures 18.7 and 18.8 show the three-dimensional estimated tracks for the
ﬁrst Monte Carlo run of the radially inbound target for the Cartesian and spherical
Kalman ﬁlters, respectively, along with the true track (in gray). One can immediately
see from a comparison of these estimated track plots that the spherical trackers appear
to perform better than the Cartesian trackers as the target passes over the sensor lo-
cated at (0,0,0). With the exception of the C-EKF, all of the Cartesian ﬁlters perform
poorly in the region near the origin.
The reason for this divergent behavior for the Cartesian track estimators near the
origin can be found in the nonlinear observation equations (18.10) of the Cartesian
track estimation methods. Although the particulars are beyond the scope of this book,
it can be shown that the inverse tangent operations contained in (18.10) create this
divergence. As the magnitude of the Cartesian position components approach the
magnitude of the estimated position uncertainties, the probability density function of
the inverse tangent of the ratio of any two position components approaches a uniform
distribution. Thus predictions of both bearing and elevation are governed by larger and
larger standard deviations, causing the innovations vector (zn −ˆzn|n−1) in (18.26) to
become unstable. This instability in the innovations vector results in ﬁlter divergence.

PERFORMANCE COMPARISON FOR VARIOUS ESTIMATION METHODS
283
–80
–60
–40
–20
0
20
40
–60
–50
–40
–30
–20
–10
0
10
20
30
40
0
2
4
6
8
10
12
14
16
18
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
C-EKF
C-UKF
C-SSKF
C-GHKF
Truth track
FIGURE 18.7
Estimated tracks for the radially inbound target trajectory for all Cartesian
tracking methods.
However, in regions not near the origin, the Cartesian ﬁlters outperform the spherical
ﬁlters, as expected.
For the radially inbound target trajectory, the RMS Cartesian position errors for
the Cartesian and spherical tracking algorithms can be compared by examining Figure
18.9, while the RMS spherical position errors for the two classes of tracking algo-
rithms can be seen in Figure 18.10. For these plots, we set qx = qy = qr = qh = 225
–5
0
5
10
15
20
25
30
35
–5
0
5
10
15
20
25
30
35
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
Truth track
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.8
Estimated tracks for the radially inbound target trajectory for all spherical
tracking methods.

284
A SPHERICAL CONSTANT VELOCITY MODEL
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
X-RMSE (nmi)
 
 
C-EKF
C-UKF
C-SSKF
C-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Y-RMSE (nmi)
 
 
C-EKF
C-UKF
C-SSKF
C-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Z-RMSE (nmi)
 
 
C-EKF
C-UKF
C-SSKF
C-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
X-RMSE (nmi)
 
 
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Y-RMSE (nmi)
 
 
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Z-RMSE (nmi)
 
 
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.9
RMS Cartesian position errors of the radially inbound target trajectory for the
Cartesian and spherical tracking algorithms.
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Slant range RMSE (nmi)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Bearing RMSE (deg)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Elev RMSE (deg)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Slant range RMSE (nmi)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Bearing RMSE (deg)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
Time (s)
Elev RMSE (deg)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.10
RMS spherical position errors of the radially inbound target trajectory for
the Cartesian and spherical tracking algorithms.

PERFORMANCE COMPARISON FOR VARIOUS ESTIMATION METHODS
285
and all other q values were set to 1.5. It is obvious that all trackers perform equally well
in estimating the Cartesian tracks except in the region where the target passes over
the radar, beginning at approximately 410 s. After this point, the spherical trackers
have the better performance, with the S-GHKF performing the best. In this close-in
tracking region, the spherical trackers exhibit large RMS errors in elevation but show
nominal RMS errors in all other Cartesian and spherical variables.
We now examine the impact of variations in the q values on the RMS position error
performance for a radially inbound target trajectory. First, notice that in Figures 18.9
and 18.10 there is little difference in performance among the Cartesian class of ﬁlters
taken as a group or among the spherical ﬁlters. To make examination of the effects
of varying q simpler, we only compare the performance results for Cartesian and
spherical EKF algorithms as q varies. Although not shown here, an initial study
examined variations in the values of qz, qangular, and qv. We set them all to one of
three values: 1.5, 36, and 225. It was clear from the analysis that the best choice over
all trajectories was 1.5.
Since the radially inbound trajectory consists of an object that maintains a constant
velocity throughout the trajectory, one would think that the RMS position errors would
be proportional to the value of q, that is, as q increased, the RMS errors would increase.
However, examination of Figures 18.11 and 18.12 indicate that the best performance
for the Cartesian track estimators occurs when the horizontal values for q are 36
(upper graphs), while for the spherical trackers the best choice is 550 (lower graphs).
Note that for the Cartesian ﬁlter algorithms, qr and qa represent the horizontal and
vertical values of q, respectively. All of the ﬁlters seem to have large RMS errors for
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
FIGURE 18.11
Comparison of the radially inbound trajectory RMS Cartesian position errors
for varying values of q for both Cartesian and spherical EKF ﬁlters.

286
A SPHERICAL CONSTANT VELOCITY MODEL
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE R (nmi) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE elevation (deg) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE R (nmi) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
RMSE elevation (deg) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
FIGURE 18.12
Comparison of the radially inbound trajectory RMS spherical position errors
for varying values of q for both Cartesian and spherical EKF ﬁlters.
the estimation of elevation near the origin, with the spherical ﬁlters having slightly
better performance. The best overall performance appears to be with the S-EKF with
a qr = 550.
18.4.2.2
Track Performance Comparison for the Loop Target Trajectory. For the
loop trajectory scenario, examples of one estimated track output along with the truth
track are shown in Figures 18.13 and 18.14 for the Cartesian and spherical track
algorithms, respectively. Examination of the ﬁgures shows that all trackers seem to
do well but lag slightly in regions of high-g turns, with the Cartesian trackers showing
less variation in regions away from the turns.
The charts for the RMS position errors, shown in Figures 18.15 and 18.16, reveal
that all of the track estimation algorithms have similar performance, with the Cartesian
track algorithms having slightly better performance overall. The C-SSKF is the worst
performing track estimation algorithm among the Cartesian trackers and the S-LKF
has the best performance among the spherical trackers. For these two plots, we set
qx = qy = qr = qh = 225 and all other q values were set to 1.5.
Once again, to examine the effects of varying the value of q, we examine the
performance of only the Cartesian and spherical EKF with qz, qangular, and qv set
to 1.5. The results shown in Figures 18.17 and 18.18 for the RMS Cartesian and
spherical position errors, respectively, indicate that the best results are obtained when
the horizontal q values are set to 225 and 550 for the Cartesian and spherical EKFs,
respectively.

PERFORMANCE COMPARISON FOR VARIOUS ESTIMATION METHODS
287
0
5
10
15
20
25
30
0
5
10
15
20
25
30
35
0
0.5
1
1.5
2
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
Truth track
C-EKF
C-UKF
C-SSKF
C-GHKF
FIGURE 18.13
Estimated tracks for the looping target trajectory for all Cartesian tracking
methods.
18.4.2.3
Track Performance Comparison for the Benchmark Highly Maneuver-
ing Target Trajectory. An example of track estimates for the ﬁnal trajectory, one with
several high-g maneuvers and both deceleration and acceleration sections, are shown
in Figures 18.19 and 18.20 for the Cartesian and spherical track algorithms, respec-
tively. Comparison of the two ﬁgures show that all track algorithms, both Cartesian
and spherical, appear to do well until the acceleration phase at the bottom of the charts.
This results in an overshoot by the Cartesian trackers during the ﬁnal turn after the
acceleration section. On the other hand, the spherical trackers have no problems with
0
5
10
15
20
25
30
0
5
10
15
20
25
30
35
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
Truth track
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.14
Estimated tracks for the looping target trajectory for all spherical tracking
methods.

288
A SPHERICAL CONSTANT VELOCITY MODEL
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi)
Time (s)
 
 
C-EKF
C-UKF
C-SSKF
C-GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi)
Time (s)
 
 
C-EKF
C-UKF
C-SSKF
C-GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi)
Time (s)
 
 
C-EKF
C-UKF
C-SSKF
C-GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi)
Time (s)
 
 
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi)
Time (s)
 
 
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi)
Time (s)
 
 
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.15
RMS Cartesian and spherical position errors of the loop target trajectory for
the Cartesian tracking algorithms.
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE range (nmi)
Time (s)
C−EKF
C−UKF
C−SSKF
C−GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg)
Time (s)
C−EKF
C−UKF
C−SSKF
C−GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE Elev (deg)
Time (s)
C−EKF
C−UKF
C−SSKF
C−GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE range (nmi)
Time (s)
S−LKF
S−EKF
S−UKF
S−SSKF
S−GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg)
Time (s)
S−LKF
S−EKF
S−UKF
S−SSKF
S−GHKF
100
200
300
400
0
0.2
0.4
0.6
0.8
1
RMSE Elev (deg)
Time (s)
S−LKF
S−EKF
S−UKF
S−SSKF
S−GHKF
FIGURE 18.16
RMS Cartesian and spherical position errors of the loop target trajectory for
the spherical tracking algorithms.

PERFORMANCE COMPARISON FOR VARIOUS ESTIMATION METHODS
289
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
FIGURE 18.17
Comparison of RMS Cartesian position error performance as a function of
q for the loop trajectory.
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE R (nmi) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE elevation (deg) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE R (nmi) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE elevation (deg) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
FIGURE 18.18
Comparison of RMS spherical position error performance as a function of
q for the loop trajectory.

290
A SPHERICAL CONSTANT VELOCITY MODEL
0
5
10
15
20
25
30
35
40
45
50
4
6
8
10
12
14
16
18
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
C-EKF
C-UKF
C-SSKF
C-GHKF
Truthtrack
FIGURE 18.19
Estimated tracks for the benchmark target trajectory for all Cartesian tracking
methods.
the ﬁnal turn. From these ﬁgures it is hard to ascertain the best track performance
among the track algorithms.
Comparison of the RMS position errors shown in Figures 18.21 and 18.22, for
the Cartesian and spherical RMS position errors, respectively, indicate that for a
highly maneuvering target the spherical tracking algorithms result in slightly lower
RMS Cartesian and range position errors than do the Cartesian track algorithms.
In addition, the spherical track estimation algorithms have better performance in
0
5
10
15
20
25
30
35
40
45
50
4
6
8
10
12
14
16
18
0
1
2
3
4
5
6
X-position (nmi)
Y-position (nmi)
Z-position (nmi)
Truthtrack
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.20
Estimated tracks for the benchmark target trajectory for all spherical tracking
methods.

PERFORMANCE COMPARISON FOR VARIOUS ESTIMATION METHODS
291
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
X-RMSE (nmi)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Y-RMSE (nmi)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Z-RMSE (nmi)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
X-RMSE (nmi)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Y-RMSE (nmi)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Z-RMSE (nmi)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.21
RMS Cartesian and spherical position errors of the benchmark target trajec-
tory for the Cartesian tracking algorithms.
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Slant range RMSE (nmi)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Bearing RMSE (deg)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Elev RMSE (deg)
C-EKF
C-UKF
C-SSKF
C-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Slant range RMSE (nmi)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Bearing RMSE (deg)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Time (s)
Elev RMSE (deg)
S-LKF
S-EKF
S-UKF
S-SSKF
S-GHKF
FIGURE 18.22
RMS Cartesian and spherical position errors of the benchmark target trajec-
tory for the spherical tracking algorithms.

292
A SPHERICAL CONSTANT VELOCITY MODEL
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi) C-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE X (nmi) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Y (nmi) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE Z (nmi) S-EKF
Time (s)
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
FIGURE 18.23
Comparison of RMS Cartesian position error performance as a function of
q for the benchmark trajectory.
tracking bearing but worse performance in tracking elevation. Overall, it is obvious
from these charts that the spherical track ﬁlter algorithms produce track estimates that
are comparable and slightly better than the Cartesian track estimation algorithms for
tracking a maneuvering target. It should also be noted that the constant spherical rate
ﬁlter has the best performance among all of the ﬁlters. This is not surprising for this
long-range tracking scenario where the target object comes no closer in range than
20 nmi. For these two plots, we set qx = qy = qr = qh = 225 and all other q values
were set to 1.5.
In examining the effects of varying the value of q for this trajectory, once again we
examine the performance of only the Cartesian and spherical EKF with qz, qangular,
and qv set to 1.5. The results shown in Figures 18.23 and 18.24 for the RMS Cartesian
and spherical position errors, respectively, indicate that the best results are obtained
when the horizontal q values are set to 225 and 550 for the Cartesian and spherical
EKFs, respectively.
18.4.2.4
Conclusions. From a consideration of the tracker algorithm RMS position
error performance for three different target trajectories with maneuver accelerations
increasing from trajectory to trajectory, we can conclude that the new spherical con-
stant velocity dynamic model offers advantages over the Cartesian constant velocity
model for tracking a maneuvering target, as long as the values for the components of
q are chosen properly. In addition, the new trackers show improved performance for

SOME OBSERVATIONS AND FUTURE CONSIDERATIONS
293
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE R (nmi) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE elevation (deg) C-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE R (nmi) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE bearing (deg) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
50
100
150
200
0
0.2
0.4
0.6
0.8
1
RMSE elevation (deg) S-EKF
Time (s)
 
 
qr = 1.5; qa = 1.5
qr = 36; qa = 1.5
qr = 225; qa = 1.5
qr = 550; qa = 1.5
FIGURE 18.24
Comparison of RMS spherical position error performance as a function of
q for the benchmark trajectory.
targets that approach the region where R is small, especially for estimation of bearing
and elevation.
18.5
SOME OBSERVATIONS AND FUTURE CONSIDERATIONS
This case study considered a new constant spherical velocity dynamic target motion
model and the resultant Kalman ﬁlter track algorithms that can be used with this
model. Through a performance analysis with three simulated trajectories, where the
level of target maneuvering increased from trajectory to trajectory, we showed that
spherical track estimation ﬁlters had the better performance during target maneuvers
and when the target approached zero range. We also showed that, with the exception of
the S-LKF, all of the spherical track estimation algorithms had identical performance.
This latter consequence leads to the conclusion that the S-EKF should be used for
constant spherical velocity tracking since it has the shortest computation time.
In this case study, we did not fully explore the potential of this new dynamic model
due to space and time limitations. Some useful expansions on this study could include
the following:
• Exploring the potential use of a spherical IMM ﬁlter that would mix a high-q
model with a low-q model, and
• Explore the impact of using range rate observations on the performance of the
spherical ﬁlters relative to the Cartesian ﬁlters.

294
A SPHERICAL CONSTANT VELOCITY MODEL
APPENDIX 18.A
THREE-DIMENSIONAL CONSTANT TURN RATE
KINEMATICS
In this section, we develop a method to generate a trajectory that can include constant
turn-rate turns in both the horizontal and the vertical. This will allow us to create
moving object trajectories that contain horizontal turns, vertical dives, and spirals.
Assume that an object at time t is moving at a horizontal heading ϑ (t) and at a
vertical attitude of γ (t). Consider the Cartesian East–North–Up coordinate system
shown in Figure 18.25, where the East–North plane is the horizontal plane.
18.A.1
General Velocity Components for Constant Turn Rate Motion
The Cartesian components of velocity for constant speed v in an East–North–Up
Cartesian system can be expressed as
v (t) =


vx (t)
vy (t)
vz (t)

=


v sin ϑ (t) cos γ (t)
v cos ϑ (t) cos γ (t)
v sin γ (t)


(18.113)
where ϑ and γ represent the heading (−180o ≤ϑ < 180o) and attitude (−90o ≤γ ≤
90o), respectively.
For a constant rate turn, referring to Figure 18.26, we can write
ϑ (t) = ϑ (t0) +  (t −t0) ≜ϑ0 + T
(18.114)
γ (t) = γ (t0) + 	 (t −t0) ≜γ0 + 	T
(18.115)
Y (North)
X (East)
Z ( Up )
R
(X, Y, Z) = (R, θ, φ)
θ
φ
FIGURE 18.25
The East–North–Up Cartesian coordinate system.

THREE-DIMENSIONAL CONSTANT TURN RATE KINEMATICS
295
X (East)
Y (North)
ϑ(t)
γ (t)
ϑ0
γ 0
Z (Up)
RH(horizontal range)
FIGURE 18.26
Depiction of a constant rate turn in both the horizontal and vertical planes.
where T ≜t −t0 and ϑ (t0) →ϑ0, γ (t0) →γ0. Here,  and 	 represent the constant
turn rates in the horizontal and vertical directions, respectively. Note that  is positive
for turns to the right relative to the direction of motion and 	 is positive for a turn up.
Substituting (18.114) and (18.115) into (18.113) results in


vx (t)
vy (t)
vz (t)

=


v sin (ϑ0 + T) cos

γ0 + 	T

v cos (ϑ0 + T) cos

γ0 + 	T

v sin

γ0 + 	T



(18.116)
Expanding the vx (t) component of the velocity using the trigonometric identities
for sine and cosine of a sum of angles we obtain
vx (t) = v sin (ϑ0 + T) cos

γ0 + 	T

= v [sin ϑ0 cos T + cos ϑ0 sin T]

cos γ0 cos 	T −sin γ0 sin 	T


296
A SPHERICAL CONSTANT VELOCITY MODEL
= v sin ϑ0 cos γ0 cos T cos 	T
−v sin ϑ0 sin γ0 cos T sin 	T
+v cos ϑ0 cos γ0 sin T cos 	T
−v cos ϑ0 sin γ0 sin T sin 	T
(18.117)
Using the deﬁnition of the components of v from (18.113), this becomes
vx (t) = vx (t0) cos T cos 	T + vy (t0) sin T cos 	T
−vz (t0) [cos ϑ0 sin T sin 	T
+ sin ϑ0 cos T sin 	T]
(18.118)
Now, using the trigonometric product-to-sum identities [9] this can be rewritten as
vx (t) = 1
2

vx (t0) cos ξ1T + vy (t0) sin ξ1T
−vz (t0)

cos ϑ0 cos ξ1T −sin ϑ0 sin ξ1T

+vx (t0) cos ξ2T + vy (t0) sin ξ2T
+vz (t0)

cos ϑ0 cos ξ2T −sin ϑ0 sin ξ2T

(18.119)
where
ξ1 ≜ −	
(18.120)
ξ2 ≜ + 	
(18.121)
Similarly, we obtain
vy (t) = 1
2

−vx (t0) sin ξ1T + vy (t0) cos ξ1T
+vz (t0)

cos ϑ0 sin ξ1T + sin ϑ0 cos ξ1T

−vx (t0) sin ξ2T + vy (t0) cos ξ2T
−vz (t0)

cos ϑ0 sin ξ2T + sin ϑ0 cos ξ2T

(18.122)
Now, examine vz (t)
vz (t) = v sin

γ0 + 	T

= v sin γ0 cos 	T + v cos γ0 sin 	T
= vz (t0) cos 	T + v cos γ0 sin 	T
(18.123)

THREE-DIMENSIONAL CONSTANT TURN RATE KINEMATICS
297
Note that
vh (t0) =

v2x (t0) + v2y (t0)
=

v2 sin2 ϑ0 cos2 γ0 + v2 cos2 ϑ0 cos2 γ0
= v cos γ0
(18.124)
Thus (18.123) becomes
vz (t) = vz (t0) cos 	T + vh (t0) sin 	T
(18.125)
Deﬁne
ci ≜cos

ξiT

, i = 1, 2
(18.126)
si ≜sin

ξiT

, i = 1, 2
(18.127)
We can now rewrite (18.119) and (18.122) as
vx (t) = 1
2

(c1 + c2) vx (t0) + (s1 + s2) vy (t0)
+ [(s1 −s2) sin ϑ0 −(c1 −c2) cos ϑ0] vz (t0)}
(18.128)
vy (t) = 1
2

−(s1 + s2) vx (t0) + (c1 + c2) vy (t0)
+ [(c1 −c2) sin ϑ0 + (s1 −s2) cos ϑ0] vz (t0)}
(18.129)
However, there are some special cases that must be considered.
1. Consider the case where  = 	 = 0. Now ci = 1 and si = 0, i = 1, 2, result-
ing in v (t) = v (t0).
2. When  = 0, 	 /= 0, ξ1 = −	 and ξ2 = 	 leading to c1 = c2 = cos (	T),
and s1 = −sin (	T) , s2 = sin (	T).
3. When 	 = 0,  /= 0, ξ1 = ξ2 =  leading to c1 = c2 = cos (T), and s1 =
s2 = sin (T).
4. If  = 	 /= 0, ξ1 = 0 and c1 = 1 and s1 = 0.
5. If  = −	 /= 0, ξ2 = 0 and c2 = 1 and s2 = 0.
18.A.2
General Position Components for Constant Turn Rate Motion
To generate expressions for the position components, r (t), we know that
r (t) = r (t0) +
 t
t0
v (u) du
(18.130)

298
A SPHERICAL CONSTANT VELOCITY MODEL
First consider the integrals of the form
 t
t0
cidu =
 t
t0
cos

ξi (u −t0)

du
=
 t
t0

cos

ξiu

cos

−ξit0

−sin

ξiu

sin

−ξit0

du
= cos

−ξit0
  t
t0
cos

ξiu

du −sin

−ξit0
  t
t0
sin

ξiu

du
= 1
ξi
sin

ξit

= 1
ξi
si
(18.131)
Similarly
 t
t0
sidu =
 t
t0
sin

ξi (u −t0)

du
= 1
ξi

1 −cos

ξit

= 1
ξi
(1 −ci)
(18.132)
Deﬁne
gi ≜1
ξi
si
(18.133)
hi ≜1
ξi
(1 −ci)
(18.134)
It follows immediately that, after performing the integrations, the components of
(18.130) are given by
rx (t) = rx (t0) + 1
2

(g1 + g2) vx (t0) + (h1 + h2) vy (t0)
+

(h1 −h2) sin ϑ0 −(g1 −g2) cos ϑ0

vz (t0)

(18.135)
ry (t) = ry (t0) + 1
2

−(h1 + h2) vx (t0) + (g1 + g2) vy (t0)
+

(g1 −g2) sin ϑ0 + (h1 −h2) cos ϑ0

vz (t0)

(18.136)
rz (t) = rz (t0) + 1 −cos 	T
	
vh (t0) + sin 	T
	
vz (t0)
(18.137)
Now let us consider the special cases:
1. Consider the case where  = 	 = 0. Now v (t) = v (t0), so r (t) = r (t0) +
Tv (t0) .

THREE-DIMENSIONAL CONSTANT TURN RATE KINEMATICS
299
2. When
 = 0, 	 /= 0,
leads
to
g1 = g2 = sin (	T) /	,
and
h1 =
−(1 −cos (	T)) /	, h2 = −h1.
3. When 	 = 0,  /= 0, leads to g1 = g2 = sin (T) /, and h1 = h2 =
−(1 −cos (T)) /.
4. If  = 	 /= 0, ξ1 = 0 and g1 = T and h1 = 0.
5. If  = −	 /= 0, ξ2 = 0 and g2 = 1 and h2 = 0.
18.A.3
Combined Trajectory Transition Equation
Combining (18.135) through (18.125) with (18.128), (18.129) and (18.125) into a
single vector–matrix equation leads to
x (tn) = fn (x (t0))
(18.138)
and
x (tn) ≜xn =

rx,n, ry,n, rz,n, vx,n, vy,n, vz,n
⊺
(18.139)
By setting way-points to divide a trajectory into segments and specifying the turn and
dive rates for each segment, (18.138) can be used to create realistic trajectories. For
all simulations used in this case study, trajectories were created at 0.1 s intervals or a
sample rate of 10 Hz.
18.A.4
Turn Rate Setting Based on a Desired Turn Acceleration
For an object with a constant speed v, the velocity components as a function of heading
and aspect are given by (18.113). The acceleration can then be depicted as
a (t) = dv
dt =


ax (t)
ay (t)
az (t)


= v


˙ϑ cos ϑ (t) cos γ (t) −˙γ sin ϑ (t) sin γ (t)
−˙ϑ sin ϑ cos γ (t) −˙γ cos ϑ (t) sin γ (t)
˙γ cos γ (t)


(18.140)
The magnitude of the acceleration is now given by
|a (t)| =

a2x (t) + a2y (t) + a2z (t)
(18.141)

300
A SPHERICAL CONSTANT VELOCITY MODEL
with
a2
x (t) = v2 
˙ϑ2 cos2 ϑ (t) cos2 γ (t) + ˙γ2 sin2 ϑ (t) sin2 γ (t)
−2˙ϑ ˙γ cos ϑ (t) sin ϑ (t) cos γ (t) sin γ (t)

(18.142)
a2
y (t) = v2 
˙ϑ2 sin2 ϑ (t) cos2 γ (t) + ˙γ2 cos2 ϑ (t) sin2 γ (t)
+ 2˙ϑ ˙γ cos ϑ (t) sin ϑ (t) cos γ (t) sin γ (t)

(18.143)
a2
z (t) = v2 ˙γ2 cos2 γ (t)
(18.144)
Thus
|a (t)| = v

˙ϑ2 cos γ (t) + ˙γ2
(18.145)
Identifying ˙ϑ =  and ˙γ = 	, this becomes
|a (t)| = v

2 cos γ (t) + 	2
(18.146)
Consider the following special cases for constant rate turns/dives:
1. For a horizontal turn, γ (t) →γ0; 	 →0, and
|a (t)| = v cos γ0
(18.147)
2. For a vertical dive:  →0, and
|a (t)| = 	v
(18.148)
Suppose now that we wish to specify a horizontal turn with an acceleration deﬁned
as a number of g’s (acceleration due to gravity), say x g’s. For a horizontal turn with
an initial attitude γ0 = 0, this leads to
 = x
v
(18.149)
For example, if we specify a 6 g horizontal turn to the right for an object moving at a
constant speed of 600 knots, the turn rate should be set to
 =
6g
600 knots × C
(18.150)
where the conversion factor C is deﬁned by
C ≜

32.174 (ft/s2)/g

(6076.11549 (nmi/ft)/3600(s/h))
(18.151)
Thus, for this case, we ﬁnd that we must set  = 0.19 rad/s to achieve the desired
turn acceleration.

THREE-DIMENSIONAL COORDINATE TRANSFORMATIONS
301
APPENDIX 18.B
THREE-DIMENSIONAL COORDINATE
TRANSFORMATIONS
When using a tracking ﬁlter that deﬁnes the state vector in Cartesian coordinates and
the observation vector in spherical polar coordinates, both Cartesian-to-spherical and
a spherical-to-Cartesian transformations are required. These transformations must be
applied to the vector under transformation as well as the covariance matrix. In this
section, we will derive these transformations and show how to develop a subroutine
to achieve these transformation. As a side beneﬁt, the Jacobian of the transformation
is created and becomes part of the output.
First, let us present some general concepts about multidimensional coordinate
transformations. A vector transformation from one coordinate system to another can
be written as
r = f (x)
(18.152)
where x is the vector in its original coordinate system and r is the vector in the
transformed system.
To develop a transformation of the covariance matrix, examine the deﬁnition of
the covariance matrix in the transformed system, given by
Prr = E

(r −ˆr) (r −ˆr)⊺
(18.153)
where E represents the expected value and ˆr = E (r). From (18.152) it follows imme-
diately that
ˆr = f (ˆx)
(18.154)
Expanding f (x) in a Taylor series about ˆx to ﬁrst order, we ﬁnd from (2.70),
f (x) = [f (x)]x=ˆx +

∇xf⊺(x)
⊺
x=ˆx (x −ˆx)
= f (ˆx) + ˆF (x −ˆx)
(18.155)
where we have deﬁned the Jacobian ˆF as
ˆF =

∇xf⊺(x)
⊺
x=ˆx
=


∂f1
∂x1
∂f1
∂x2
· · ·
∂f1
∂xn
∂f2
∂x1
∂f2
∂x2
· · ·
∂f2
∂xn
...
...
...
...
∂fm
∂x1
∂fm
∂x2
· · ·
∂fm
∂xn


x=ˆx
(18.156)
Here, n is the dimension of the vector x and m is the dimension of the vector f.

302
A SPHERICAL CONSTANT VELOCITY MODEL
Putting (18.155) into (18.153) we obtain
Prr = E

f (x) −f (ˆx)
 
f (x) −f (ˆx)
⊺
= E

f (ˆx) + ˆF (x −ˆx) −f (ˆx)
 
f (ˆx) + ˆF (x −ˆx) −f (ˆx)
⊺
= ˆFE

(x −ˆx) (x −ˆx)⊺ ˆF⊺
= ˆFPxx ˆF
⊺
(18.157)
Thus, the general equation for the coordinate transformation of a covariance matrix
is given by (18.157), to ﬁrst order. Note that (18.157) includes the computation of
the Jacobian, so any subroutine designed to perform this coordinate transformation
produces the Jacobian as a bonus.
18.B.1
Cartesian-to-Spherical Transformation
For the general three-dimensional coordinate systems shown in Figure 18.25, we can
write a state vector as
x =

rx, vx, ry, vy, rx, vz
⊺
(18.158)
in Cartesian coordinates, or
r =

R, ˙R, θ, ˙θ, φ, ˙φ
⊺
(18.159)
in spherical polar coordinates.
From Figure 18.25, it follows that the transformation from Cartesian to spherical
for the position coordinates is given by


R
θ
φ

=



r2x + r2y + r2z
tan−1 
rx
ry

tan−1

rz

r2x+r2y



(18.160)
Taking the derivative of each component with respect to time results in the spherical
rate components


˙R
˙θ
˙φ

=


rxvx+ryvy+rzvz
R
ryvx−rxvy
R2
h
Rvz−rz ˙R
RRh


(18.161)
where the horizontal range is deﬁned by Rh ≜

r2x + r2y. In a three-dimensional
system, R is usually referred to as the slant range. Thus, the complete Cartesian-to-

THREE-DIMENSIONAL COORDINATE TRANSFORMATIONS
303
spherical vector transformation, deﬁned as h (x), is given by
r =


R
˙R
θ
˙θ
φ
˙φ


= h (x) ≜


h1 (x)
h2 (x)
h3 (x)
h4 (x)
h5 (x)
h6 (x)


=



r2x + r2y + r2z
rxvx+ryvy+rzvz
R
tan−1 
rx
ry

ryvx−rxvy
R2
H
tan−1

rz

r2x+r2y

Rvz−rz ˙R
RRh


(18.162)
From (18.156) we can write the Jacobian of the Cartesian-to-spherical covariance
transformation as
ˆH =


H11
H12
· · ·
H16
H21
H22
· · ·
H26
...
...
...
...
H61
H62
· · ·
H66


=


∂h1
∂rx
∂h1
∂vx
· · ·
∂h1
∂vz
∂h2
∂rx
∂h1
∂vx
· · ·
∂h2
∂vz
...
...
...
...
∂h6
∂rx
∂h6
∂vx
· · ·
∂h6
∂vz


x=ˆx
(18.163)
If (18.162) is evaluated ﬁrst, we can use the spherical components in the evaluation
of each partial, resulting in the Jacobian components
H11 =
∂
∂rx

r2
x + r2
y + r2
z
1/2
= rx
R
(18.164)
H12 = H14 = H16 = 0
(18.165)
H13 = ry
R
(18.166)
H15 = rz
R
(18.167)
H21 = Rvx −rx ˙R
R2
(18.168)
H22 = H11
(18.169)
H23 = Rvy −ry ˙R
R2
(18.170)
H24 = H13
(18.171)
H25 = Rvz −rz ˙R
R2
(18.172)
H26 = H15
(18.173)

304
A SPHERICAL CONSTANT VELOCITY MODEL
H31 = ry
R2
h
(18.174)
H32 = H34 = H35 = H36 = 0
(18.175)
H33 = −rx
R2
h
(18.176)
H41 = −vy −2rx ˙θ
R2
h
(18.177)
H42 = H31
(18.178)
H43 = vx −2ry ˙θ
R2
h
(18.179)
H44 = H33
(18.180)
H45 = H46 = 0
(18.181)
H51 = −rxrz
R2Rh
(18.182)
H52 = H54 = H56 = 0
(18.183)
H53 = −ryrz
R2Rh
(18.184)
H55 = Rh
R2
(18.185)
H61 =
1
RRh
	
rx
R2
h

rz ˙R −Rvz

+ rz
R2

2rx ˙R −Rvx


(18.186)
H62 = H51
(18.187)
H63 =
1
RRh
	
ry
R2
h

rz ˙R −Rvz

+ rz
R2

2ry ˙R −Rvy


H64 = H53
(18.188)
H65 =
1
R3Rh

2r2
z + R2
˙R −rzRvz

(18.189)
H66 = H55
(18.190)
Now, using (18.157) we can write the covariance matrix transformation from
Cartesian-to-spherical coordinates as
Prr = ˆHP
xx ˆH⊺
(18.191)
where Pxx and Prr are the covariance matrices in Cartesian and spherical coordinates,
respectively.

THREE-DIMENSIONAL COORDINATE TRANSFORMATIONS
305
18.B.2
Spherical-to-Cartesian Transformation
Occasionally, we will need the inverse of (18.162), which is deﬁned as the spherical-
to-Cartesian transformation
x = g (r)
(18.192)
If we ﬁrst examine the position components, using Figure 18.25 we ﬁnd that


rx
ry
rz

=


R sin θ cos φ
R cos θ cos φ
R sin φ


(18.193)
taking the derivative of each component results in


vx
vy
vz

=


sin θ cos φ
R cos θ cos φ
−R sin θ sin φ
cos θ cos φ
−R sin θ cos φ
−R cos θ sin φ
sin φ
0
R cos φ




˙R
˙θ
˙φ


(18.194)
Combining (18.193) and (18.194) results in the spherical-to-Cartesian transformation
x =


rx
vx
ry
vy
rz
vz


=


g1 (r)
g2 (r)
g3 (r)
g4 (r)
g5 (r)
g6 (r)


=


R sin θ cos φ
˙R sin θ cos φ + ˙θR cos θ cos φ −˙φR sin θ sin φ
R cos θ cos φ
˙R cos θ cos φ −˙θR sin θ cos φ −˙φR cos θ sin φ
R sin φ
˙R sin φ + ˙φR cos φ


(18.195)
It follows immediately that the spherical-to-Cartesian transformation of the co-
variance matrix is given by
Pxx = ˆGPrr ˆG
⊺
(18.196)
where the components of the transformation Jacobian from (18.156) are given by
G11 = sin θ cos φ
(18.197)
G12 = G14 = G16 = 0
(18.198)
G13 = R cos θ cos φ
(18.199)
G15 = −R sin θ sin φ
(18.200)

306
A SPHERICAL CONSTANT VELOCITY MODEL
G21 = ˙θ cos θ cos φ −˙φ sin θ sin φ
(18.201)
G22 = G11
(18.202)
G23 = ˙R cos θ cos φ −˙θR sin θ cos φ −˙φR cos θ sin φ
(18.203)
G24 = G13
(18.204)
G25 = −˙R sin θ sin φ −˙θR cos θ sin φ −˙φR sin θ cos φ
(18.205)
G26 = G15
(18.206)
G31 = cos θ cos φ
(18.207)
G32 = G34 = G36 = 0
(18.208)
G33 = −R sin θ cos φ
(18.209)
G35 = −R cos θ sin φ
(18.210)
G41 = −˙θ sin θ cos φ −˙φ cos θ sin φ
(18.211)
G42 = G31
(18.212)
G43 = −˙R sin θ cos φ −˙θR cos θ cos φ + ˙φR sin θ sin φ
(18.213)
G44 = G33
(18.214)
G45 = −˙R cos θ sin φ + ˙θR sin θ sin φ −˙φR cos θ cos φ
(18.215)
G46 = G35
(18.216)
G51 = sin φ
(18.217)
G52 = G53 = G54 = G56 = 0
(18.218)
G55 = R cos φ
(18.219)
G61 = ˙φ cos φ
(18.220)
G62 = G51
(18.221)
G63 = G64 = 0
(18.222)
G65 = ˙R cos φ −˙φR sin φ
(18.223)
G66 = G55
(18.224)
REFERENCES
1. Blackman SS. Multiple-Target Tracking with Radar Applications. Artech House; 1986.
2. Bar-Shalom Y, Fortman TE. Tracking and Data Association. Academic Press; 1988.
3. Bar-Shalom Y, Rong Li X, Kirubarajan T. Estimation with Application to Tracking and
Navigation. Wiley; 2001.
4. RisticB,ArulampalamS,GordonN.BeyondtheKalmanﬁlter:ParticleFilterApplications.
Artech House; 2004.

REFERENCES
307
5. Zollo S, Ristic B. On the Choice of the Coordinate System and Tracking Filter for the
Track-while-scan Mode of an Airborne Radar. DSTO-TR-0926; 1999.
6. Lefferts RE. Alpha–beta ﬁlters in polar coordinates with acceleration constraints. IEEE
Trans. Aero. Elec. Syst. 1988; 24(6): 693–699.
7. Blair WD, Watson GA, Rice TR. Interacting multiple model ﬁlter for tracking maneuvering
targets in spherical coordinates. Proc. Am. Control Conf. 1991;1055–1059.
8. Kameda H, Nomoto K, Kosuge Y, Kondo M. Target tracking algorithm in radar reference
coordinates using fully-coupled ﬁlters. Electron. Commun. Jpn. 1997;Pt1,10(7):327–333.
9. Swillinger DZ. CRC Standard Mathematical Tables and Formulae, 30th ed. CRC Press;
1996.

19
TRACKING A FALLING RIGID BODY
USING PHOTOGRAMMETRY
19.1
INTRODUCTION
Photogrammetry is the mathematical science of estimating an object’s pose (three-
dimensional position and orientation) and kinematic properties (translational and
rotational velocities and accelerations) from a set of object features observed in a
series of two-dimensional video image frame data from multiple cameras. Many
approaches to this estimation problem have been applied, with the most common
prior to the 1990s using a nonlinear least squares (NLLSQ) approach. These early
NLLSQ methods were reviewed by Huang and Netravali [1]. In the NLLSQ method,
at each time step, the residuals between the predicted video images and the actual
current video images are used in an iterative NLLSQ procedure to arrive at a local
residual error minimum producing an estimate of the rigid body state. Alternate, more
modern, Bayesian estimation extended Kalman ﬁlter (EKF) methods were introduced
by Iu and Wohn [2] and Gennery [3] with excellent results. These were followed by
a single-constraint-at-a-time (SCAAT) EKF method introduced by Welch [4,5].
All of the above-mentioned methods had some drawbacks that affect the rigid
body state estimation performance. The NLLSQ methods assumes a dynamic model
without noise, is not recursive in time, and requires the computation of a Jacobian
(matrix of partial derivatives) at each iteration. The observation set consists of video
frame images and all image frames are reprocessed during each iteration, making
the method very time consuming. In addition, because the NLLSQ, as used in pho-
togrammetry, is a “single time-step at a time” method, translational velocities and
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
308

INTRODUCTION
309
angular rates are not estimated in the NLLSQ but have to be calculated separately
using a postprocessing smoothing step.
The EKF methods are also difﬁcult to implement because the Jacobians of both
the nonlinear observation and dynamic transition models have to be calculated at each
time step in order to linearize the observation process (see Chapter 7). This analytical
linearization has proven to be inadequate for many highly nonlinear problems.
In this case study, for the purpose of safety evaluation, we address the speciﬁc
problem of estimating and tracking the time-varying pose of a rigid body store
(a bomb-like object) dropped from an airplane pylon. We use image sequences that
are recorded from multiple high-speed image sensors attached to a reference object.
The image sensors or cameras are speciﬁcally oriented to reliably observe the track
object’s rigid body state trajectory. Multiple feature points are afﬁxed to both the ref-
erence and rigid body object as shown in Figures 19.1 and 19.2. There are 19 cameras
attached to the aircraft, including four cameras on each wingtip, four under the tail,
and a camera under the aircraft’s nose. The position of the feature points in the image
frame of each camera are recorded in a temporal image sequence that is used in the es-
timation of the trajectory of the tracked object. Each image sensor has its own unique
image sequence as a record of the object’s motion from a particular point of view.
Each image sequence must be properly combined with the image sequences from
other cameras to compute the best track estimate of the trajectory at each time step.
The track object’s trajectory can then be used to perform miss distance analysis for
safety purposes, or for validation and reﬁnement of predictions made by other types
of modeling and simulation. Since image sensors estimate the time-varying position
FIGURE 19.1
Photo of a US Navy F-18E super hornet aircraft preparing to release four
Mk-62 stores.

310
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
FIGURE 19.2
Close-up of a MK-62 store release as viewed from a camera mounted under
the aircrafts’s tail.
and orientation of a rigid body object in the 3D world, photogrammetric estimation
is a logical ﬁrst choice for accurate close range tracking [6].
In our estimation process we introduce two new methods that use an unscented
Kalman ﬁlter (UKF) or a generalized UKF particle ﬁlter (see Chapters 9 and 17,
respectively, for a complete derivation and discussion of these ﬁlters). These ﬁlters
offer improved performance over both the EKF and the NLLSQ because they have
the capability of estimating the full rigid body state kinematics including Cartesian
and angular velocities and accelerations without the need to compute Jacobians.
In Section 19.2, we present a dynamic model of motion for a falling rigid body
object including translational and rotational motion. This section also contains a
discussion of the dynamic noise terms that are introduces to account for any small
motion deviations from the dynamic model. The observation model introduced in
Section 19.3 begins by calculating the location of the feature points on the rigid body
surface, transforming (rotating and translating) these 3D feature points on the rigid
body from the body’s coordinate system into the reference (aircraft’s) coordinates,
then transforming the 3D points into a camera’s coordinate system and projecting the
3D points into the camera’s 2D image plane.
This is followed in Section 19.4 with a discussion of the estimation methods used,
beginning with the NLLSQ in Section 19.4.1, the UKF in Section 19.4.2, and the
UKF combination particle ﬁlter in Section 19.4.3. Initialization for all of these ﬁlters
is presented in Section 19.4.5.
For performance analysis purposes, we have developed a synthetic rigid body
model along with a synthetic motion model that emulates the motion of a rigid body
store as it is released from an airplane pylon. This model and a method for using
it to generate synthetic image frame data is contained in Section 19.5. The relative

THE PROCESS (DYNAMIC) MODEL FOR RIGID BODY MOTION
311
performance of all of the estimation methods along with a summary of important
results is presented in Section 19.6.
Appendix 19.A outlines some deﬁnitions and the mathematics associated with
quaternions, axis-angle vectors and rotations, and is included as a brief review of the
topics.
19.2
THE PROCESS (DYNAMIC) MODEL FOR RIGID BODY MOTION
Our motion model will describe the translational and rotational motion of a rigid body
as a function of time. Consider an instantaneous state vector x (t) =

p⊺(t) , a⊺(t)
⊺
that deﬁnes both the translation of the center of mass of a rigid body relative to
a ﬁxed reference Cartesian coordinate frame, p (t) =

x (t) , y (t) , z (t)
⊺, and the
orientation of the body with respect to the reference frame given as an axis-angle
vector a (t) =[ax (t), ay (t), az (t)]⊺. Under certain conditions the vector a can become
numerically unstable (speciﬁcally when ∥a∥= 2πn where n is a positive integer),
so instead we use a unit quaternion q =[qs, qx, qy, qz]⊺that when combined with a
represents the orientation. We discuss the need for q in more detail in Section 19.2.2
and in Appendix 19.A.2. In addition, in the dynamic (process) models developed
below, we will modify the state vector to include rate vectors, acceleration vectors, and
jerk (third time derivative) terms governing both translational and rotational motion.
Since the translational motion of a rigid body is independent of its rotational
motion, we will treat the two separately. In the sections below we will develop the
dynamic transition equations that take the rigid body state vector, deﬁned above, from
some time tn−1 to a later time tn. First, in Section 19.2.1 we address the transition
equations governing the translational part of the state vector, while in Section 19.2.2
we consider the transition equations for the orientation part of the state vector. In
general, we can write the total transition equation in the form
xn = f (xn−1, vn−1)
(19.1)
where vn−1 is considered the noise driving the process. In all cases we will take the
noise to be Gaussian and additive so that (19.1) becomes
xn = f (xn−1) + vn−1
(19.2)
In Section 19.2.3, this composite pose transition model is discussed in more detail.
In Section 19.2.4, we will discuss the properties of several noise models for vn−1.
19.2.1
Dynamic Transition of the Translational Motion of a Rigid Body
Let pn be a three-dimensional vector that represents the Cartesian position of the
center of gravity (cg) of a rigid body at time tn, that is, pn =

xn, yn,zn
⊺. Expanding

312
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
p (tn) in a temporal Taylor series about some prior time tn−1 we can write
pn = pn−1 +
dp (t)
dt

t=tn−1
(tn −tn−1) +
d2p (t)
dt2

t=tn−1
(tn −tn−1)2
2!
+
d3p (t)
dt3

t=tn−1
(tn −tn−1)3
3!
+ · · ·
(19.3)
= pn−1 + Tn ˙pn−1 + T 2
n
2 ¨pn−1 + T 3
n
6
...pn−1 + · · ·
(19.4)
where ˙p, ¨p, ...p represent the ﬁrst three temporal derivatives of position and Tn ≜
tn −tn−1. For this case study, all observing cameras are assumed to record their
images in a synchronous fashion, so we will assume that Tn is a constant T. Keeping
only terms to third order, we can rewrite (19.3) as the third-order Taylor polynomial
pn = pn−1 + T ˙pn−1 + T 2
2 ¨pn−1 + T 3
6
...pn−1
(19.5)
For a constant position model, ˙p, ¨p, and ...p are all zero so we deﬁne the rigid-body
zeroth-order three-dimensional state vector containing only position components as
p(0)
n
= pn =

x (t) , y (t) , z (t)
⊺and (19.5) becomes
p(0)
n
= p(0)
n−1 + ν(0)
n−1
(19.6)
where we have added a zeroth-order process velocity noise term ν(0)
n−1. Process noise
is added to account for slight deviations of the objects position from our model due
to unmodeled effects such as wind, friction, and so on.
For a constant velocity assumption, if we deﬁne a six-dimensional state vector hav-
ing both position and velocity components, that is p(1)
n =

p⊺
n, ˙p⊺
n
⊺, (19.5) becomes
the ﬁrst-order process model
p(1)
n = F(1)p(1)
n−1 + ν(1)
n−1
(19.7)
where
F(1) =

I3
TI3
03
I3

(19.8)
with I3 and 03 deﬁned as the three-dimensional identity and zero matrices, respec-
tively. ν(1)
n−1 is the ﬁrst-order acceleration process noise.
In a similar manner, for a constant acceleration model (19.5) the nine-dimensional
state vector is p(2)
n
=

p⊺
n, ˙p⊺
n, ¨p⊺
n
⊺and the process model becomes
p(2)
n = F(2)p(2)
n−1 + ν(2)
n−1
(19.9)

THE PROCESS (DYNAMIC) MODEL FOR RIGID BODY MOTION
313
with
F(2) =


I3
TI3
T 2
2 I3
03
I3
TI3
03
03
I3


(19.10)
ν(2)
n−1 is the second-order jerk process noise.
And ﬁnally, for a constant jerk 12-dimensional model, p(3)
n = [p⊺
n, ˙p⊺
n, ¨p⊺
n, ...p⊺
n]⊺
resulting in the process model
p(3)
n = F(3)p(3)
n−1 + ν(3)
n−1
(19.11)
with
F(3) =


I3
TI3
T 2
2 I3
T 3
6 I3
03
I3
TI3
T 2
2 I3
03
03
I3
TI3
03
03
03
I3


(19.12)
and ν(1)
n−1 is the third-order snap (or jounce) fourth derivative process noise.
19.2.2
Dynamic Transition of the Rotational Motion of a Rigid Body
The instantaneous orientation of a rigid body can be expressed in multiple ways.
The orientation can be expressed as a set of Euler rotation angles relative to the
three Cartesian reference frame axis. The right-handed Cartesian reference frame we
choose is a frame with the x-axis pointing forward, the y-axis pointing to the right
(starboard) looking forward, and the z-axis pointing down. The three orientation
angles are {φ, θ, ψ}, with φ (roll) representing a counterclockwise (CCW) rotation
about the x-axis, θ (pitch) representing a CCW rotation about the y-axis, and lastly
ϕ (yaw) a CCW rotation about the z-axis, respectively. Unfortunately, using Euler
angles to describe rigid body orientation can result in singularities under certain
circumstances [3].
19.2.2.1
Conversions Between Rotation Representations. Let a =

ax, ay,az
⊺∈
R3 be used as an axis–angle representation of rotation where the direction of a
speciﬁes the axis of rotation and ∥a∥=

a2x + a2y + a2z speciﬁes the magnitude of
rotation (in radians).
Let q =

qs, qx, qy,qz
⊺∈H, where H is the space of quaternions, be used as a
unit quaternion representation of rotation, where qs is real, qx, qy,qz are coefﬁcients
of distinct imaginary numbers, and

q2s + q2x + q2y + q2z = 1.
Let M ∈R3 × R3 be a 3 × 3 orthonormal rotation matrix.

314
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
To convert from an axis–angle vector a to a unit quaternion q, we deﬁne the
function Qa : R3 →H as
q = Qa (a) =

cos ∥a∥
2 , sin ∥a∥
2
∥a∥

ax, ay, az

⊺
(19.13)
To convert from a unit quaternion q to an axis-angle vector a we deﬁne the function
Aq : H →R3 as
a = Aq (q) = 2 cos−1 qs

1 −q2s

qx, qy, qz
⊺
(19.14)
19.2.2.2
Quaternion Multiplication. Let q1, q2, and q3 be quaternions. Then the
quaternion multiplication q3 = q1q2 is deﬁned as
q3=


q3,s
q3,x
q3,y
q3,z

=


q1,s
−q1,x
−q1,y
−q1,z
q1,x
q1,s
−q1,z
q1,y
q1,y
q1,z
q1,s
−q1,x
q1,z
−q1,y
q1,x
q1,s




q2,s
q2,x
q2,y
q2,z


(19.15)
19.2.2.3
Dynamic Model for Orientation. To express the dynamic temporal tran-
sition of the rigid body’s orientation, we will use the axis-angle representation. The
axis-angle orientation vector, an, represents a small rotation relative to the prior ori-
entation, that is, the rotation from time tn−1 to time tn. This small axis-angle rotation
is added (via a rotation) into an unambiguous external quaternion representation,
qn−1, which contains the full rotation at time tn−1 for use in the observation model.
Note that a and its derivatives ˙a, ¨a, and ...a (when present) are all with respect to the
reference coordinate frame and not the body frame, thus q does not need to appear in
the dynamic model.
We will deﬁne the orientation state vector in terms of the axis-angle vector a and
its derivatives. For a constant orientation model, ˙a, ¨a, and ...a are all zero, so consider
the state vector for this zeroth-order model to be the 3D axis-angle vector a(0) so that
a(0)
n
= a(0)
n−1 + ρ(0)
n−1
(19.16)
where ρ(0)
n−1 represents the three-dimensional zeroth-order angular velocity noise in
the orientation.
For a constant rotation-rate model, we choose a six-dimensional state vector of
the form a(1)
n
≜

a⊺
n, ˙a⊺
n
⊺resulting in the ﬁrst-order process model
a(1)
n
= g(1)
n−1

a(1)
n−1

+ ρ(1)
n−1
(19.17)
where ρ(1)
n−1 represents the three-dimensional ﬁrst-order angular velocity noise in
the orientation. To propagate the axis-angle vector a forward over the time interval
T = tn −tn−1 we cannot simply add T ˙an−1 to an−1 to get a new vector an because

THE PROCESS (DYNAMIC) MODEL FOR RIGID BODY MOTION
315
adding together (or combining) two 3D rotations cannot be accomplished using vector
addition of their axis-angle representations. Instead, we must convert the axis-angle
vectors to unit quaternions, multiply them together (while remembering to reverse
their order), and convert the results back into an axis-angle vector (alternatively, we
could use rotation matrices instead of unit quaternions.) Speciﬁcally, let △a be the
incremental change to orientation produced by a constant rotation rate over the time
interval T so that
△a =
 tn
tn−1
˙adt = T ˙a
(19.18)
Deﬁne qan−1 and q△a as
qan−1 = Qa (an−1)
(19.19)
q△a = Qa (△a) = Qa (T ˙a)
(19.20)
Now, we can multiply the two quaternions, taking care to reverse their order, and
convert the results back into the axis-angle vector an
an = Aq

q△aqan−1

(19.21)
Since we are assuming constant rotation rate, it follows immediately that
˙an = ˙an−1
(19.22)
Now, using (19.19) through (19.22), (19.17) becomes
a(1)
n
=

an
˙an

=

Aq (Qa (T ˙an−1) Qa (an−1))
˙an−1

+ ρ(1)
n−1
(19.23)
For a constant rotational–acceleration model, we choose the nine-dimensional
orientational state vector a(2)
n
≜

a⊺
n, ˙a⊺
n, ¨a⊺
n
⊺resulting in the second-order process
model
a(2)
n
= g(2)
n−1

a(2)
n−1

+ ρ(2)
n−1
(19.24)
where ρ(2)
n−1 represents the three-dimensional second-order angular velocity noise in
the orientation. For this model, the incremental change to orientation over the time
interval T is given by
△a =
 tn
tn−1
(˙a+t¨a) dt = T ˙a + T 2
2 ¨a
(19.25)
Since ˙a and ¨a are proper vectors, we can write
˙an = ˙an−1 + T ¨an−1
(19.26)
¨an = ¨an−1
(19.27)

316
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
so that


an
˙an
¨an

= g(2)
n−1

a(2)
n−1

+ ρ(2)
n−1
=


Aq

Qa

T ˙an−1 + T 2
2 ¨an−1

Qa (an−1)

˙an−1 + T ¨an−1
¨an−1


+ρ(2)
n−1
(19.28)
Finally, for a constant rotational-jerk model, we choose a 12-dimensional state
vector a(3)
n
≜

a⊺
n, ˙a⊺
n, ¨a⊺
n, ...a ⊺
n
⊺resulting in the process model
a(3)
n
= g(3)
n−1

a(3)
n−1

+ ρ(3)
n−1
(19.29)
where ρ(3)
n−1 is the 12-dimensional angular noise for this model and
g(3)
n−1

a(3)
n−1

=


Aq

Qa

T ˙a⊺
n−1 + T 2
2 ¨a⊺
n−1 + T 3
6
...a ⊺
n−1

Qa (an−1)

˙a⊺
n−1 + T ¨a⊺
n−1 + T 2
2
...a ⊺
n−1
¨a⊺
n−1 + T ...a ⊺
n−1
...a ⊺
n−1


(19.30)
19.2.3
Combined Dynamic Process Model
The translational and orientational process models can be combined into a single,
more general, process model. First, deﬁne a combined state vector as
x(i)
n =

p(i)
n
a(i)
n

(19.31)
and a state noise model as
v(i)
n =

ν(i)
n
ρ(i)
n

(19.32)
Now we can write the full dynamic model as
x(i)
n = f(i)
n−1

x(i)
n−1

+ v(i)
n−1
(19.33)

THE PROCESS (DYNAMIC) MODEL FOR RIGID BODY MOTION
317
where
f(i)
n−1

x(i)
n−1

=

F(i)p(i)
n−1
g(i) 
a(i)
n−1



(19.34)
19.2.4
The Dynamic Process Noise Models
The noise is assumed to be zero-mean Gaussian so we can consider the noise to be
of the form
v(i)
n ∼N

03(i+3), Q(i)
, i = 0, 1, 2, 3
(19.35)
where 03(i+3) is a 3(i + 3) × 3(i + 3) matrix of zeros.
From [3,7], we ﬁnd that for the constant position/orientation model the noise term
represents velocity white noise, which for a continuous process reduces to
Q(0) =

q(0)
t TI3
03
03
q(0)
r TI3

(19.36)
where q(0)
t
and q(0)
r
are related to the standard deviation of translational and rotational
velocity noise, respectively.
For constant velocity models, the noise covariance is given by [3,7]
Q(1) =

q(1)
t Q1
06
06
q(1)
r Q1

(19.37)
with
Q1 =
 T 3
3 I
T 2
2 I
T 2
2 I
TI

(19.38)
Forthehigherorderpositionequations,theconstantaccelerationdynamicnoisemodel
can be written [3,7]
Q(2) =

q(2)
t Q2
09
09
q(2)
r Q2

(19.39)
with
Q2 =


T 5
20 I3
T 4
8 I3
T 3
6 I3
T 4
8 I3
T 3
3 I3
T 2
2 I3
T 3
6 I3
T 2
2 I3
TI3


(19.40)

318
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
and from [8] we the constant jerk noise covariance is given by
Q(3) =

q(3)
t Q3
012
012
q(3)
r Q3

(19.41)
with
Q3 =


T 7
252I3
T 6
72 I3
T 5
30 I3
T 4
24 I3
T 6
72 I3
T 5
20 I3
T 4
8 I3
T 3
6 I3
T 5
30 I3
T 4
8 I3
T 3
3 I3
T 2
2 I3
T 4
24 I3
T 3
6 I3
T 2
2 I3
TI3


(19.42)
19.3
COMPONENTS OF THE OBSERVATION MODEL
Assume that there are J cameras used to gather data on the state of the rigid body
and that they observe feature points designated by markers placed around the surface
of the rigid body. The location of each feature point relative to the body center of
mass is known to within very tight tolerances due to prior calibrations. Let S =

si =

si,x, si,y, si,z
⊺; i = 1, . . . , Ns

be a set of ﬁxed 3D time-invariant locations
of the feature points in the body coordinate system. Because each camera is placed
so that it has a different ﬁeld of view, each camera will view a different subset of
markers. For the jth camera at time tn, let the number of viewed feature points
be Kj,n and let the individual visible feature points for that camera be designated
k1,j,n, k2,j,n, . . . , kKj,n,j,n so that each ki,j,n refers to the point ski,j,n ∈S, where 1 ≤
ki,j,n ≤Ns. Assume that all of the image frames across all cameras are synchronized
so that the cameras each record an image frame simultaneously every 0.005 s. Also
assumethatthepositionandorientationofallcamerasiscalibratedforeveryframeand
that the association of marker images for each camera image frame with predictions of
those marker image positions is managed manually. Figure 19.3 shows the coordinates
systems that depict the projection of a feature point onto a camera’s image plane.
The observation vector zn, at time tn, will therefore consist of a total of Mn =
J
j=1 Kj,n two-dimensional pixel image marker locations (u, v) and zn can be written
as
zn =

z1,1,u,n, z1,1,v,n, z2,1,u,n, z2,1,v,n, . . . ,
zi,j,u,n, zi,j,v,n, . . . , zKJ,n,J,u,n, zKJ,n,J,v,n
⊺
(19.43)
The task now is to deﬁne the functional dependence of the components of the 2Mn-
dimensional observation vector to the state vector xn. If we include observation noise,
the observation can be written in terms of the state vector as
zn = hn (xn) + wn
(19.44)

COMPONENTS OF THE OBSERVATION MODEL
319
Origin of 2D
image coordinate
system
Origin of 3D
camera coordinate
system
Center of camera’s
field of view
2D (u,v) projection
of feature point
in image coordinates
3D (x,y,z) feature point
in camera coordinates
z
y
x
v
f
u
FIGURE 19.3
Projection of a feature point onto a camera’s image plane.
where hn (xn) can be written as
hn (xn) =

h⊺
1,1,n (xn) , h⊺
2,1,n (xn) , . . . , h⊺
i,j,n (xn) , . . . , h⊺
KJ,n,J,n (xn)
⊺
(19.45)
so that the task that remains is to deﬁne the individual subfunctions.
The observation noise is assumed to be a zero-mean Gaussian process, so wn ∼
N (0, R), with R deﬁned as the diagonal noise covariance matrix of image pixel
measurement variances given by
R = σ2
pixI2Mn
(19.46)
where I2Mn is a 2Mn × 2Mn identity matrix.
Note that zn, hn, and wn are of dimension 2Mn, the dimension of xn depends
on the order of the dynamic model, and Mn and hn are time-dependent because the
number of features observed by each camera can change with time. For simplicity,
the measurement noise variance, σ2
pix, of each pixel component, is assumed to be the
same for each feature marker measurement.
Recall the difference between a point rotation and a frame rotation (see Appendix
19.A). To rotate and translate ski,j,n from the rigid body’s coordinate system into the
reference coordinate system, we perform a point rotation, then a translation
s(ref)
ki,j,n = Mrefski,j,n + pn
(19.47)
where pn is the position vector of the center of mass at time tn. The rotation matrix
Mref can be obtained by recalling that the rotation of a feature marker point is a two
step process. In the ﬁrst step, the axis-angle adjustment to the rotation an is combined
with the external quaternion qn−1 that represents the prior rotation. The axis-angle

320
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
vector an is ﬁrst converted to a rotation matrix Man using the function Ma deﬁned
by (19.164)
Man = Ma (an)
(19.48)
and the external quaternion qn−1 is converted to a rotation matrix Mqn−1 using the
function Mq deﬁned by (19.163)
Mqn−1 = Mq (qn−1)
(19.49)
then the two matrices are multiplied to produce Mref
Mref = ManMqn−1
(19.50)
To rotate and translate s(ref)
ki,j,n from reference coordinates into the jth camera’s
coordinate system, we deﬁne pCj,n, qCj,n, and fCj,n to be the calibrated position,
orientation, and focal length, in pixels, of camera Cj and we then perform a negative
translation followed by a frame rotation
s(Cj)
ki,j,n = M⊺
Cj,n

s(ref)
ki,j,n −pCj,n

(19.51)
where MCj = Mq

qCj

.
Finally as shown in Figure 19.3, the 3D s(Cj)
ki,j,n is projected into the 2D image
plane with
s(image)
ki,j,n
=

fCj,n
s(Cj)
ki,j,n,x



0
1
0
0
0
1

s(Cj)
ki,j,n
(19.52)
where s(image)
ki,j,n
has components [zi,j,u,n, zi,j,v,n]⊺∈zn given by
zi,j,n =

zi,j,u,n
zi,j,v,n

=

s(image)
ki,j,n,u
s(image)
ki,j,n,v


(19.53)
and (u, v) are image coordinates in pixels. Equation (19.53) represents the image pixel
location of the ith feature point for one camera. Since the observation vector consists
of a total of MN feature point sets from J cameras, full observation vector becomes
zn =

z⊺
1,1,n, z⊺
2,1,n, . . . , z⊺
i,j,n, . . . , z⊺
KJ,n,J,n
⊺
(19.54)
with the noise free zi,j,n = hi,j,n(xn) given by the combination of (19.47) through
(19.53).

ESTIMATION METHODS
321
19.4
ESTIMATION METHODS
Several estimation methods can be used for the recursive estimation of the state vector
and its covariance matrix. In the sections below, we discuss the three speciﬁc meth-
ods that we applied to this photogrammetry tracking problem, including an NLLSQ
solver, a Gaussian UKF, and an unscented combination particle ﬁlter that we call
the unscented Gaussian particle ﬁlter (UGPF). Application of these three estimation
methods are presented in this section and their performance will be compared in
Section 19.6.
19.4.1
A Nonlinear Least Squares Estimation Method
The standard method used in the past for estimation of the position and orientation
of a rigid body based on video image data has been the NLSSQ method that assumes
a noise-free dynamic equation and produces an estimate that minimizes the mean
squared difference between an actual measurement set and a prediction of that mea-
surement set. In our implementation, the state vector is taken as only the pose of the
rigid body at time tn. We will drop all time indices because the pose is estimated anew
for each time step, as will be shown below.
From (19.31), we deﬁne the NLSSQ state vector as
x =

p⊺, a⊺⊺
(19.55)
and we recall that the rigid body’s orientation is represented by a combination of both
a and the quaternion q that represents the prior (or initial) orientation.
The observation vector is shown in (19.54). The dependence of the observation
vector on the state vector can be written as
z = h(x) + w
(19.56)
with h(x) decomposed into components hi,j(x) as in (19.45), where each hi,j(x) is
given by the combination of (19.47) through (19.53) for each zi,j component of z.
We will assume that w ∼N (0, R) and there are M = J
j=1 Kj feature marker two-
dimensional image measurements, making R a 2M × 2M diagonal covariance matrix
where the same variance is used for all image pixel location variances. Thus we can
write
R = σ2
pixI2M
(19.57)
The NLSSQ method essentially uses a weighted least-squares estimator of x given
measurements z by minimizing the cost function
q (x) = [z −h(x)]⊺R−1 [z −h(x)]
(19.58)
Thus, an estimate of x, written as ˆx, can be obtained from
▽xq (x) = −2

▽xh⊺(x)

R−1 [z −h(x)] = 0
(19.59)

322
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
Deﬁne H as
H ≜▽xh⊺(x) = ▽x

h1,1 (x) , h2,1 (x) , . . . , hKJ,J (x)

=


∂h1,1
∂px
∂h2,1
∂px
· · ·
∂hKJ ,J
∂px
∂h1,1
∂py
∂h2,1
∂py
· · ·
∂hKJ ,J
∂py
...
...
...
...
∂h1,1
∂az
∂h2,1
∂az
· · ·
∂hKJ ,J
∂az


(19.60)
with hi,j the nonlinear function that transforms x into the ith marker’s location in
camera J’s image to produce zi,j. Now (19.59) can be rewritten as
HR−1 [z −h(x)] = 0
(19.61)
Given some initial estimate (guess) of the state, ˆx0, the nonlinear function h(x)
can be expanded in a Taylor series about ˆx0
h(x) = h(ˆx0) +

▽xh⊺(x)
⊺
x=ˆx0

x −ˆx0

+ · · · ,
(19.62)
or, keeping just the linear terms of the series
h(x) = h(ˆx0) + H0

x −ˆx0

(19.63)
where
H0 ≜

▽xh⊺(x)
⊺
x=ˆx0
(19.64)
Equation (19.61) now becomes
HR−1 
z −h(ˆx0) −H0

x −ˆx0

= 0
(19.65)
Solving for ˆx1 = x and letting H →H0 yields
ˆx1 = ˆx0 +

H0R−1H⊺
0
−1
H0R−1 
z −h(ˆx0)

(19.66)
The process is repeated using the new estimate of x as the initial estimate and
iterating, until after p iterations, we obtain the Gauss–Newton normal equations
ˆxp = ˆxp−1 +

Hp−1R−1H⊺
p−1
−1
Hp−1R−1 
z −h(ˆxp−1)

(19.67)
It follows immediately that the covariance of ˆxp is given by
Pp =

Hp−1R−1H⊺
p−1
−1
(19.68)

ESTIMATION METHODS
323
From (19.57) we see that R is a diagonal matrix and these last two equations
reduce to
ˆxp = ˆxp−1 +

Hp−1

σ2
pixI2M
−1
H⊺
p−1
−1
Hp−1

σ2
pixI2M
−1 
z −h(ˆxp−1)

= ˆxp−1 +

H⊺
p−1
−1 
z −h(ˆxp−1)

(19.69)
and
Pp = σ−2
pix

Hp−1H⊺
p−1
−1
(19.70)
The process is repeated until ep ≜ˆxp −ˆxp−1 is below a vector of threshold values.
Thus, at each time step tn, a new set of images zn are obtained and this procedure is
recalculated to generate an estimate ˆxn = ˆxp,n having an estimated covariance Pp,n.
Note that in Matlab, this NLSSQ method is available as the subroutine “lsqnonlin”
that takes the function (19.58) and the variable x as inputs and outputs a value ˆx
that minimizes the function. Although this function does not estimate the covariance
matrix, certain forms of the function also return the Jacobian Hp−1, allowing the
covariance matrix estimate to be calculated.
Since this formulation of the NLSSQ is iterative but not recursive (in time), a
dynamic model is not used, and the observations depend only on the position and
orientation of the rigid body, only the position and orientation can be estimated. In
order to estimate any of the time derivatives of position and orientation, some form
of smoothing ﬁlter or ﬁnite difference ﬁlter must be used as a post processor in order
to obtain the time derivatives of x.
As an alternative for future implementations, one could include a dynamic model
and take several successive-in-time measurement sets augmenting them to the vector
h(x). This reformulation would allow the state vector to include derivative terms that
could then be included in the estimation procedure. However, that would increase the
size of both h(x) and H increasing the computational burden.
19.4.2
An Unscented Kalman Filter Method
The UKF is part of a class of sigma point Kalman ﬁlters that require all noise densities
to be Gaussian. For an ith-order model, the dynamic equation is nonlinear and is given
by (19.33). The observation model is also nonlinear and is shown in (19.44). Both the
dynamic and observation noise densities are assumed to be Gaussian. As shown in Part
II of this book, if one makes an afﬁne transformation of the nonlinear functions, and
then expands them in a general polynomial, the Gaussian-weighted moment integrals
reduce to a sum over a weighted set of sigma points, where the weights and sigma
points depend on the sigma point ﬁlter chosen. We present the formulation of the
general sigma point Kalman ﬁlter for estimation of the state vector and its covariance
matrix that is based on a successive-in-time set of image feature marker observations.

324
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
The ﬁlter is started with an initial estimate for the state vector ˆx0 =
ˆp⊺
0, ˆa⊺
0
⊺, its
associated quaternion q0 (as described in Section 19.4.4.2) and its covariance Pxx
0 .
The dimensions of x, Pxx, and Q will be dependent on the model order chosen, as
described in Section 19.2.1 and 19.2.4.
The state sigma points are calculated from
χ(j)
n−1|n−1 = ˆxn−1|n−1 + Dn−1|n−1c(j), j = 0, 1, . . . , Ns
(19.71)
where ˆxn−1|n−1 →ˆx0 and Pxx
n−1|n−1 →Pxx
0
for the initial set of sigma points and
Dn−1|n−1 is deﬁned by
Pxx
n−1|n−1 = Dn−1|n−1D⊺
n−1|n−1
(19.72)
Also, Ns is the number of sigma points and c(j) is the jth sigma point. The set
{wj, c(j)}, used by the sigma point ﬁlters, are dependent on the size of the state
vector and the sigma point ﬁlter to be implemented. They can be generated using the
subroutines presented in Listings 13.1 and 13.2.
From Chapter 13, the state prediction equations for all sigma point Kalman ﬁlters
are given by
χ(j)
n|n−1 = f

χ(j)
n−1|n−1

(19.73)
ˆxn|n−1 =
Ns

j=0
wjχ(j)
n|n−1
(19.74)
Pxx
n|n−1 =
Ns

j=0
wj

χ(j)
n|n−1 −ˆxn|n−1
 
χ(j)
n|n−1 −ˆxn|n−1
⊺
+Q
(19.75)
The observation prediction equations are
ˆzn|n−1 =
Ns

j=0
wjh

χ(j)
n|n−1

(19.76)
Pzz
n|n−1 =
Ns

j=0
wj

h

χ(j)
n|n−1

−ˆzn|n−1
 
h

χ(j)
n|n−1

−ˆzn|n−1
⊺
+R
(19.77)
Pxz
n|n−1 =
Ns

j=0
wj

χ(j)
n|n−1 −ˆxn|n−1

×

h

χ(j)
n|n−1

−ˆzn|n−1
⊺
(19.78)

ESTIMATION METHODS
325
And ﬁnally, the state update equations are
Kn ≜Pxz
n|n−1

Pzz
n|n−1
−1
(19.79)
ˆxn|n = ˆxn|n−1 + Kn

zo
n −ˆzn|n−1

(19.80)
Pxx
n|n = Pxx
n|n−1 −KnPzz
n|n−1K⊺
n
(19.81)
where zo
n is the observation at time tn. Once the state vector ˆxn|n = [ˆp⊺
n|n,ˆa⊺
n|n]⊺has
been obtained at the end of a ﬁlter iteration, a new unit quaternion qn is produced
from ˆan|n and qn−1 after which ˆan|n is reset according to
qn = Qa
ˆan|n

qn−1
(19.82)
ˆan|n →[0, 0, 0]⊺
(19.83)
For higher order models and for a large number of feature marker image dimen-
sions, the UKF is the best sigma point Kalman ﬁlter because the number of sigma
points required goes as Ns = 2n + 1, where n is the dimension of the state vec-
tor. For all applications of this ﬁlter, we always choose w0 = 0, so Ns = 2n and
wj = 1/2n, j = 1, . . . , 2n. In addition, c(j) = √nr(j) = √n [1] ∈Rn (see Section
2.2 for an explanation of this notation.) The spherical simplex Kalman ﬁlter (SSKF)
has fewer points (Ns = n + 1) and we examined its performance brieﬂy and discov-
ered that it has performance results that are similar to those of the UKF but such
results will not be presented here.
19.4.3
Estimation Using the Unscented Combination Particle Filter
In Chapter 17 we introduced the combination particle ﬁlter, a particle ﬁlter method that
does not require resampling. We use it here as another alternative to the NLSSQ. The
process block diagram for a combination particle ﬁlter that uses an UKF to generate
the importance samples is shown in Figure 17.3. In our application of this ﬁlter to
the photogrammetry rigid body pose estimation problem, the likelihood function for
each particle, p(zn|x(i)
n ), is Gaussian because (19.44) can be rewritten as
w(i)
n = zo
n −hn

x(i)
n

(19.84)
Now p(zn|x(i)
n ) = N([zo
n −hn(x(i)
n )], R), and we can write the particle likelihood func-
tion as
p(zn|x(i)
n ) = c exp

−1
2

zo
n −hn

x(i)
n
⊺
R−1 
zo
n −hn

x(i)
n

(19.85)
The normalization c need not be included unless it is needed for numerical stability,
since it will be canceled out when the weights are normalized during the weight
update step. For our application of this ﬁlter, we used 20,000 particles. Since the full
procedure is presented in Figure 17.3, we will not reiterate it here.

326
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
19.4.4
Initializing the Estimator
All of the estimation methods described above require an initial estimate ˆx and some
also require an initial estimate of Pxx. There are, however, some differences in what
the initial estimate represents between the NLLSQ estimator and all other estimators.
In the subsections below, we address these issues.
19.4.4.1
Initializing the NLLSQ. Since the NLLSQ, is an iterative ﬁlter (one that
loops through a set of code until an error tolerance limit is reached), it must be
reinitialized at every time step. For time steps prior to the store release event, the rigid
body is at rest, so the position and orientation initial estimates can be taken as
ˆx0 =
ˆp⊺
0, ˆa⊺
0
⊺= [0, 0, 0, 0, 0, 0]⊺
(19.86)
Therefore, the initial estimate for ˆq0 is the quaternion
ˆq0 = (1, 0, 0, 0)
(19.87)
which represents a zero angle. For other situations that differ from our synthetic test,
ˆp⊺
0 and ˆq0 might take on some nominal nonzero value, while ˆa0 should still be set to
ˆa⊺
0 = [0, 0, 0]⊺.
After the store release event, the rigid body is undergoing translation and rotation
from one time step to the next, so the ﬁnal estimate of position and rotation from the
previous time step can be used to initialize the state vector for the current time step.
Let ˆx+
n =

ˆp+⊺
n , ˆa+⊺
n
⊺
be the posterior estimate from frame n, and let ˆqn−1 represent
the entire rotation from frame n −1. Then ˆqn and ˆan should be set according to
ˆqn = Qa
ˆa+
n
 ˆqn−1
(19.88)
ˆan = [0, 0, 0]⊺
(19.89)
The prior initial estimate for frame n + 1 should be set as ˆx−
n+1 =

ˆp−⊺
n+1, ˆa−⊺
n+1
⊺
,
where
ˆp−
n+1 = ˆp+
n
(19.90)
ˆa−
n+1 = ˆa+
n = [0, 0, 0]⊺
(19.91)
19.4.4.2
Initialization of Bayesian Filters. For all ﬁlters other than the NLLSQ,
initial values are needed for ˆx0, Pxx
0 , and ˆq0. For consistency, we will use the same
initialization for all of the ﬁlters. However, the required initialization will change
from one dynamic model order to another. So, we will need different initialization for
the different model orders: constant position/orientation, constant velocity, constant
acceleration, and constant jerk. Since each lower order model can be thought of as a
degenerative case of the next higher order model, we need only initialize the highest
(third) order model variables ˆx(3)
0 , P(3)
0 , and ˆq0, then truncate if needed to initialize

ESTIMATION METHODS
327
a lower order model. Since the rigid body is initially at rest, the initial estimate of
ˆx(3)
0
is
ˆx(3)
0
=

ˆp(3)
0
ˆa(3)
0

= 0
(19.92)
where 0 is a vector of 24 zeros.
The initial estimate of ˆq0 for the synthetic test is the identity quaternion
ˆq0 = (1, 0, 0, 0)
(19.93)
which represents a zero angle. For other situations that differ from our synthetic test,
ˆp0 and ˆq0 might take on some nominal nonzero value, while ˆa0 should still be set to
ˆa0 = [0, 0, 0]⊺. Since normally the body is at rest, the derivatives of ˆp0 and ˆa0 should
be still set to zero.
To initialize Pxx
0 , let σp, σ ˙p, σ ¨p, and σ...
p be the standard deviation in translational
position, velocity, acceleration, and jerk, respectively. Also let σa, σ ˙a, σ ¨a, and σ...a
be the standard deviation in angular position, velocity, acceleration, and jerk, respec-
tively. We set the standard deviations to
σp = 1, σ ˙p = 5, σ ¨p = 1000, σ...
p = 5000
(19.94)
σa = 1, σ ˙a = 1, σ ¨a = 50, σ...a = 2000
(19.95)
where the translational units are inches and seconds and the angular units are degrees
and seconds. Now, the initial translational covariance submatrix is
Pxx(3)
0,trans =


σ2
pI3
03
03
03
03
σ2
˙pI3
03
03
03
03
σ2
¨pI3
03
03
03
03
σ2...
p I3


(19.96)
and the initial angular covariance submatrix is
Pxx(3)
0,ang = Cd2r


σ2
aI3
03
03
03
03
σ2
˙aI3
03
03
03
03
σ2
¨aI3
03
03
03
03
σ2...a I3


(19.97)
where Cd2r = π/180 converts degrees to radians. The two submatrices are combined
to form the initial covariance matrix
Pxx(3)
0
=

Pxx(3)
0,trans
012
012
Pxx(3)
0,ang

(19.98)

328
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
The Bayesian estimators are rarely reinitialized after the ﬁltering process has be-
gun, so the above initialization is the only one considered.
19.5
THE GENERATION OF SYNTHETIC DATA
In order to evaluate the performance of the various tracking ﬁlter methods, we devel-
oped a synthetic rigid body and created synthetic trajectories indicative of the store
release event. This gives us a synthetic “truth” trajectory that allows us to create syn-
thetic observation sets as seen from synthetic cameras by adding Monte Carlo sets of
pixel noise. The synthetic camera measurements can then be used to perform Monte
Carlo RMS error analysis comparisons of the estimation (solver) methods.
19.5.1
Synthetic Rigid Body Feature Points
We ﬁrst construct a rigid body with eight feature points out of a unit cube by construct-
ing a 3 × 8 matrix S, where the ith column of S is the feature point si =

si,x, si,y, si,z

S =


−1
1
−1
1
−1
1
−1
1
−1
−1
1
1
−1
−1
1
1
−1
−1
−1
−1
1
1
1
1


(19.99)
Then we apply an afﬁne transformation that makes the cube long and thin
S = TS
(19.100)
where
T =


200
0
0
0
30
0
0
0
30


(19.101)
19.5.2
Synthetic Trajectory
To build a synthetic trajectory we ﬁrst generate a sequence of times t, where
t = {tn = 0.005 ∗n, n = 0, 1, 2, . . . , 200}
(19.102)
Then we deﬁne a damped sinusoidal function (damped harmonic oscillator)
f (tn, α, β, γ) =
 
αe−β(tn−tr) 
cos

γ (tn −tr)

−1

,
tn ≥tr
0,
tn < tr
(19.103)
where tr is the store release time (set to tr = 0.1) and the constants α, β, and γ
represent the amplitude, damping factor, and frequency, respectively. Prior to time tr
the body is motionless and after time tr the body is falling.

THE GENERATION OF SYNTHETIC DATA
329
The ﬁrst two derivatives of f (tn, α, β, γ) are given by
˙f (tn, α, β, γ) =





−αe−β(tn−tr) 
β cos

γ (tn −tr)

−β
+γ sin

γ (tn −tr)

,
tn ≥tr
0,
tn < tr
(19.104)
and
¨f (tn, α, β, γ) =





−αe−β(tn−tr) 
β2 −γ2
cos

γ (tn −tr)

−β2 + 2βγ sin

γ (tn −tr)

,
tn ≥tr
0,
tn < tr
(19.105)
Using the damped sinusoidal signal, we generate six sequences, one for each of
the six parameters (x, y, z, φ, θ, ϕ), where φ, θ, and ϕ are the Euler angles roll, pitch,
and yaw, and tn ∈t
x (tn) = f

tn, αx, βx, γx

;

αx, βx, γx

= (100, 2, 2)
(19.106)
y (tn) = f

tn, αy, βy, γy

;

αy, βy, γy

= (−200, 3, 1)
(19.107)
z (tn) = f

tn, αz, βz, γz

;

αz, βz, γz

= (−2000, 1.5, 0.5)
(19.108)
φ (tn) = f

tn, αφ, βφ, γφ

;

αφ, βφ, γφ

= (500, 1.5, 1.25)
(19.109)
θ (tn) = f

tn, αθ, βθ, γθ

;

αθ, βθ, γθ

= (−10, 2, 10)
(19.110)
ϕ (tn) = f

tn, αϕ, βϕ, γϕ

;

αϕ, βϕ, γϕ

= (15, 0.8, 14)
(19.111)
We next generate ˙x (tn), ˙y (tn), ˙z (tn), ˙φ (tn), ˙θ (tn) , and ˙ϕ (tn) as well as ¨x (tn), ¨y (tn),
¨z (tn), ¨φ (tn), ¨θ (tn) , and ¨ϕ (tn) according to (19.104) and (19.105).
We can now easily construct the translational components of our synthetic trajec-
tory
pn,true =

x (tn) , y (tn) , z (tn)
⊺
(19.112)
˙pn,true =

˙x (tn) , ˙y (tn) , ˙z (tn)
⊺
(19.113)
¨pn,true =

¨x (tn) , ¨y (tn) , ¨z (tn)
⊺
(19.114)
For the rotational components, we must construct the axis-angle an,true, and the
unit quaternion qn,true. For an,true, we write
an,true = [0, 0, 0]⊺, ∀n
(19.115)
To construct qn,true, we ﬁrst introduce the conversion function QEuler (φ, θ, ϕ) : R3 →
H that converts Euler angles (φ, θ, ϕ) into a unit quaternion
QEuler (φ, θ, ϕ) = qφqθqϕ
(19.116)

330
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
where qφ, qθ, and qϕ are the quaternions that represent each of the three Euler angles
by themselves
qφ =

cos φ
2 , sin φ
2 , 0, 0
⊺
(19.117)
qθ =

cos θ
2, 0, sin θ
2, 0
⊺
(19.118)
qϕ =

cos ϕ
2 , 0, 0, sin ϕ
2
⊺
(19.119)
Now qn,true can be deﬁned as
qn,true = QEuler (φ (tn) , θ (tn) , ϕ (tn))
= qφqθqϕ
(19.120)
where the “n,true” portion of the subscripts have been omitted for brevity and a
quaternion multiplication from (19.15) is implied. Now, we set
xn,true =

p⊺
n,true, a⊺
n,true
⊺=

p⊺
n,true, [0, 0, 0]⊺⊺
(19.121)
To construct truth data for the purpose of comparison against ˙an and ¨an, we must
express synthetic ﬁrst and second derivatives of rotation. We do so in body-reference
coordinates rather than in the ﬁxed reference frame because we judge that using a
body-referenced coordinate system for derivatives is more intuitive for humans. We
shall call the body-referenced angular derivatives ˙a(Body)
n,true and ¨a(Body)
n,true .
We begin by recalling the well-known quaternion differentiation formula [9]
˙q = 1
2ωq
(19.122)
where q is a unit quaternion and the quaternion ω =

0, ωx, ωy, ωz
⊺is a body ref-
erenced rotation rate vector converted to a pure quaternion (“pure” because its scalar
component is 0). The relationship between q, ˙q, and ω for a frame rotation is speciﬁed
by (19.122), whereas qn,true is a point rotation. If q is a frame rotation, and qn,true is
the equivalent point rotation, then qn,true = q∗, where q∗is the quaternion conjugate
of q. That is, if q =

qs, qx, qy, qz
⊺, then q∗=

qs, −qx, −qy, −qz
⊺. Before taking
the conjugate of both sides of (19.124), recall three facts from quaternion algebra.
First, if w is a pure quaternion (where the scalar component ws = 0), then w∗= −w.
Second, for the product of quaternions, q1 and q2, (q1q2)∗= q∗
2q∗
1. Third, conjugat-
ing a quaternion twice yields the original quaternion, that is, (q∗)∗= q. Substituting
q∗
n,true for q and −ωn,true for ω in (19.122) yields
˙q∗
n,true = −1
2ωn,trueq∗
n,true
(19.123)

THE GENERATION OF SYNTHETIC DATA
331
Solving for ωn,true we have
ωn,true = −2˙q∗
n,true

q∗
n,true
−1
= −2˙q∗
n,true

q∗
n,true
∗
%%qn,true
%%
= −2˙q∗
n,trueqn,true
(19.124)
where we have used the fact that qn,true is a unit quaternion.
Now, taking the conjugate of both sides of (19.124) yields
ω∗
n,true = −ωn,true = −2
˙q∗
n,trueqn,true
∗= −2q∗
n,true ˙qn,true
(19.125)
so that
ωn,true = 2q∗
n,true ˙qn,true
(19.126)
To evaluate ωn,true we must ﬁnd ˙qn,true. To do so, differentiate (19.120) using the
product rule
˙qn,true = d
dt

qϕqθqφ

= ˙qϕqθqφ + qϕ ˙qθqφ + qϕqθ ˙qφ
(19.127)
where ˙qφ, ˙qθ, and ˙qϕ are obtained by differentiating (19.117) through (19.119) re-
sulting in
˙qφ =
˙φ
2

−sin φ
2 , cos φ
2 , 0, 0
⊺
(19.128)
˙qθ =
˙θ
2

−sin θ
2, 0, cos θ
2, 0
⊺
(19.129)
˙qϕ = ˙ϕ
2

−sin ϕ
2 , 0, 0, cos ϕ
2
⊺
(19.130)
Inserting (19.127) into (19.126) yields
ωn,true = 2q∗
n,true
˙qϕqθqφ + qϕ ˙qθqφ + qϕqθ ˙qφ

(19.131)
Now, ˙a(Body)
n,true can be written as
˙a(Body)
n,true =

ωx,n,true, ωy,n,true, ωz,n,true
⊺
(19.132)

332
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
In a similar manner, to generate ¨a(Body)
n,true we must ﬁnd ˙ωn,true by differentiating
(19.126)
˙ωn,true = d
dt ωn,true = d
dt
&
2q∗
n,true
dqn,true
dt
'
= 2
dq∗
n,true
dt
dqn,true
dt
+ q∗
n,true
d2qn,true
dt2

(19.133)
To evaluate dq∗
n,true/dt we observe that for all q, d(qq∗)/dt = 0. Thus
d
dt

q∗q

= dq∗
dt q + q∗dq
dt = 0
(19.134)
which leads to
dq∗
dt
= −q∗dq
dt q∗= −q∗˙qq∗
(19.135)
Using (19.135) in (19.133) yields
˙ωn,true = 2

−q∗
n,true ˙qn,trueq∗
n,true ˙qn,true + q∗
n,true ¨qn,true

= 2q∗
n,true ¨qn,true −ω2
n,true
(19.136)
To obtain ¨qn,true we differentiate (19.127) resulting in
¨qn,true = ¨qϕqθqφ + qϕ ¨qθqφ + qϕqθ ¨qφ
+2˙qϕ ˙qθqφ + 2˙qϕqθ ˙qφ + 2qϕ ˙qθ ˙qφ
(19.137)
where ¨qφ, ¨qθ, and ¨qϕ are obtained by differentiating (19.128) through (19.130) re-
sulting in
¨qφ =
¨φ
2

−sin φ
2 , cos φ
2 , 0, 0
⊺
−˙φqφ
(19.138)
¨qθ =
¨θ
2

−sin θ
2, 0, cos θ
2, 0
⊺
−˙θqθ
(19.139)
¨qϕ = ¨ϕ
2

−sin ϕ
2 , 0, 0, cos ϕ
2
⊺
−˙ϕqϕ
(19.140)
Now, using (19.137) in (19.136) yields
˙ωn,true = 2q∗
n,true
¨qϕqθqφ + qϕ ¨qθqφ + qϕqθ ¨qφ + 2˙qϕ ˙qθqφ
+2˙qϕqθ ˙qφ + 2qϕ ˙qθ ˙qφ

−ω2
n,true

(19.141)
and ¨a(Body)
n,true can be written as
¨a(Body)
n,true =

˙ωx,n,true, ˙ωy,n,true, ˙ωz,n,true
⊺
(19.142)

THE GENERATION OF SYNTHETIC DATA
333
19.5.3
Synthetic Cameras
To generate synthetic data, we assume that there are only two synthetic cameras by
setting J = 2 and giving them the following positions, orientations (in degrees), and
focal lengths (in pixels)

xC1, yC1, zC1

= (490, 100, 0)
(19.143)

xC2, yC2, zC2

= (−490, 100, 0)
(19.144)

φC1, θC1, ϕC1, fC1

= (0, 0, −168, 2500)
(19.145)

φC2, θC2, ϕC2, fC2

= (0, 0, −12, 2500)
(19.146)
and then we set pC1, qC1, pC2, qC2 as
pC1 =

xC1, yC1, zC1
⊺
(19.147)
qC1 = QEuler

φC1, θC1, ϕC1

(19.148)
pC2 =

xC2, yC2, zC2
⊺
(19.149)
qC2 = QEuler

φC2, θC2, ϕC2

(19.150)
19.5.4
Synthetic Measurements
Having constructed our synthetic rigid body in (19.100), we must specify which
feature points are visible at each point in time by setting Kj,n and ki,j,n for all i, j, and n.
For simplicity, all feature points are visible to both cameras throughout the trajectory.
Explicitly, Kj,n = 8 for j = {1, 2} and n = {0, 1, . . . , 200} and ki,j,n = i for i =
{1, 2, . . . , 8}, j = {1, 2}, and n = {0, 1, . . . , 200}. Now we have expressions for
every term necessary for construction of the measurement function h (x) as described
in Section 3 and we use h (x) to generate the synthetic measurements. For each n =
{0, 1, . . . , 200}, the measurement vector zo
n,synth is generated by evaluating
zo
n,synth = hn

xn,true

+ wn,synth
(19.151)
where
wn,synth ∼N

0, Rsynth

(19.152)
with
Rn,synth = σ2
pixI2Mn = σ2
pixI32
(19.153)
Here (since Mn = 16 = 2
j=1 Kj,n), I32 is a 32 × 32 identity matrix and σpix is set
to 2 pixels. Note that a new Monte Carlo sample is drawn from N(0, Rsynth) for each
time step.

334
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
19.6
PERFORMANCE COMPARISON ANALYSIS
The performance analysis of all estimation ﬁlters presented in Section 19.4 was con-
ducted using the synthetic test data described in Section 19.5. The performance of the
UKF using zero-, ﬁrst-, second-, and third-order dynamic models is compared against
the NLLSQ and also against the UGPF using the second-order dynamic model. The
performance metric used in all cases is the root mean squared (RMS) error. The per-
formance results to be presented below indicate that the second-order UKF was the
best ﬁlter to use.
Figures 19.4 through 19.6 show the results the estimated track results of a single run
of the UKF ﬁlter using the second -order model compared against synthetic truth data.
Observe that the rotational rates and accelerations shown in Figures 19.5 and 19.6
are in body-referenced coordinates. Figure 19.7 shows a close-up of a small section
of the lateral position truth trajectory with the estimated trajectories from multiple
solvers superimposed over the truth. Identical noisy observations and initialization
were used as input to all estimation methods. Analysis of this data leads to two
conclusions: ﬁrst, we note that the NLLSQ and the UKF with a zeroth-order dynamic
model generate nearly identical estimates. Since the zeroth-order model is a constant
position model and the NLLSQ assumes a position-only state vector, the similarity
0
0.2
0.4
0.6
0.8
1
−25
−20
−15
−10
−5
0
5
X Forward
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
Inches
Estimate
Truth data
−1
0
1
2
3
4
5
6
Y Right
Inches
Estimate
Truth data
−10
0
10
20
30
40
50
60
Z Down
Inches
Estimate
Truth data
−80
−70
−60
−50
−40
−30
−20
−10
0
10
Roll
deg
Estimate
Truth data
−30
−25
−20
−15
−10
−5
0
5
Pitch
deg
Estimate
Truth data
−2
0
2
4
6
8
10
12
Yaw
deg
Estimate
Truth data
FIGURE 19.4
Translational and rotational position using a second-order model with a UKF
ﬁlter.

PERFORMANCE COMPARISON ANALYSIS
335
0
0.2
0.4
0.6
0.8
1
−5
−4
−3
−2
−1
0
1
2
3
X forward velocity
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
ft/s
Estimate
Truth data
−1
−0.5
0
0.5
1
1.5
2
Y right velocity
ft/s
Estimate
Truth data
−2
−1
0
1
2
3
4
5
6
7
Z down velocity
ft/s
Estimate
Truth data
−140
−120
−100
−80
−60
−40
−20
0
20
Body roll rate
Deg/s
Estimate
Truth data
−200
−150
−100
−50
0
50
100
150
200
Body pitch rate
Deg/s
Estimate
Truth data
−150
−100
−50
0
50
100
150
Body yaw rate
Deg/s
Estimate
Truth data
FIGURE 19.5
Translational and rotational velocity using a second-order model with a UKF
ﬁlter.
in performance of the NLLSQ and UKF(zeroth order) is to be expected. The second
conclusion from Figure 19.7 is that the ﬁlters that use higher order dynamic models
have better performance than the NLLSQ or ﬁltering using a zeroth-order model,
primarily due to a reduction of the variation of their estimates about the truth trajectory.
This preliminary conclusion will be born out in Section 19.5.2 where we present the
RMS error comparisons for the various ﬁlters and models.
19.6.1
Filter Performance Comparison Methodology
The performance comparison methodology begins with a single noiseless state
vector trajectory {xn, n = 0, 1, . . . , N} generated synthetically as described in
Section 19.5.2. This trajectory is then transformed into a set of noiseless synthetic
truth observations {hn (xn), n = 0,1,. . .,N}. Finally, NRMS = 100 sets of noise ob-
servations are produced by adding independent zero-mean Gaussian noise to the
synthetic truth observations to produce NRMS sets of noisy observations {zo
n,r;
n = 0,1,. . .,N; r = 1,2,. . .,NRMS}. Each noisy observation set is used in each track-
ing ﬁlter to produce NRMS sets of state vector estimates {ˆxn|n,r; n = 0, 1, . . . , N;
r = 1, 2, , NRMS}.

336
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
0
0.2
0.4
0.6
0.8
1
−1.5
−1
−0.5
0
0.5
1
1.5
2
X forward acceleration
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
0
0.2
0.4
0.6
0.8
1
Time (s)
G’s
Estimate
Truth data
−1
−0.5
0
0.5
Y right acceleration
G’s
Estimate
Truth data
−1.5
−1
−0.5
0
0.5
1
1.5
Z down acceleration
G’s
Estimate
Truth data
−1500
−1000
−500
0
500
1000
Body roll acceleration
G’s
Estimate
Truth data
−4000
−3000
−2000
−1000
0
1000
2000
3000
Body pitch acceleration
G’s
Estimate
Truth data
−2000
−1500
−1000
−500
0
500
1000
1500
2000
2500
Body yaw acceleration
G’s
Estimate
Truth data
FIGURE 19.6
Translational and rotational acceleration using a second-order model with a
UKF ﬁlter.
0.5
0.55
0.6
0.65
0.7
0.75
0.8
4.6 
4.8
5
5.2
5.4
5.6
5.8
6
Y right position
Time (s)
Inches
NLLSQ
UKF (0)
UKF (1)
UKF (2)
UKF (3)
UGPF (2)
Truth
FIGURE 19.7
Lateral position estimates from multiple solvers using identical inputs.

PERFORMANCE COMPARISON ANALYSIS
337
As noted in Chapter 14, at time tn, the covariance matrix of the full state vector
estimate ˆxn|n can be written as
Pxx
n|n = E

xn −ˆxn|n
 
xn −ˆxn|n
⊺
(19.154)
When xn is known, as it would be for our synthetic trajectory case, and ˆxn|n is an
unbiased estimate of xn, the square root of the diagonal terms of Pxx
n|n can be approx-
imated by their conditional RMS errors obtained from repeated ﬁlter runs using a
different Monte Carlo set of noisy observation data for each run.
For example, the RMS error for the lateral position (y) estimate of a given ﬁlter at
time tn is given by
e(ﬁlter)
y,n
=
(
)
)
*
NRMS
r=1

yn −ˆy(ﬁlter)
n|n,r
2
NRMS
(19.155)
Since we intend to compare results between zero-, ﬁrst-, and second-order terms
obtained by each ﬁlter for our six-dimensional problem, we would need to calculate
and analyze the RMS errors for 18 parameters. To simplify our analysis, we instead
compare all three components of position together using the squared distance
%%%pn −p(ﬁlter)
n|n,r
%%%
2
=

xn −ˆx(ﬁlter)
n|n,r
2
+

yn −ˆy(ﬁlter)
n|n,r
2
+

zn −ˆz(ﬁlter)
n|n,r
2
(19.156)
This results in just six performance comparisons, one for each 3D position and its
ﬁrst two derivatives, and one for each 3D rotation and its ﬁrst two derivatives.
For the three translational parameters comprising the position at time tn, we com-
pute the RMS position error as
e(ﬁlter)
p,n
=
(
)
)
*
NRMS
r=1
%%%pn −p(ﬁlter)
n|n,r
%%%
2
NRMS
(19.157)
For the ﬁrst and second derivatives of position, we compute e(ﬁlter)
˙p,n
and e(ﬁlter)
¨p,n
in a
similar fashion.
The angular displacement RMS errors cannot be computed in the same way since
a 3D angle is not a vector quantity. To ﬁnd the difference between two orientations
represented by the unit quaternions q1 and q2 we write
q△= q∗
2q1
(19.158)
To calculate the magnitude of the difference, q△must be converted to its axis-angle
representation before its norm can be computed. Thus, we can write the RMS angular
error for a speciﬁc ﬁlter at time tn as
e(ﬁlter)
a,n
=
(
)
)
*
NRMS
r=1
%%%Aq

q∗nq(ﬁlter)
n|n,r
%%%
2
NRMS
(19.159)

338
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
where Aq : H →R3 (see (19.163)) converts a unit quaternion into an axis-angle
vector, and q(ﬁlter)
n|n,r is the quaternion representing the orientation estimate given by a
speciﬁc ﬁlter at time tn for run r and qn represents the true orientation at time tn.
Sincetherotationrate ˙a isavector,wecancompare ˙a(ﬁlter)
n|n,r with ˙an directly.Observe
that the ﬁlters estimate ˙a in reference frame coordinates while the truth trajectory for
˙a is in body-referenced coordinates. Thus, a frame rotation (see Appendix 19.A.3)
must be used to transform ˙a(ﬁlter)
n|n,r into body coordinates. Thus, the RMS angular rate
error for a particular ﬁlter at time tn is written as
e(ﬁlter)
˙a,n
=
(
)
)
*
NRMS
r=1
%%%M⊺
n,r ˙a(ﬁlter)
n|n,r −˙an
%%%
2
NRMS
(19.160)
where Mn,r = Mq(q(ﬁlter)
n|n,r ) is the matrix representing the estimated orientation for
the speciﬁc ﬁlter at time tn during run r. e(ﬁlter)
¨a,n
can be computed in the same manner.
19.6.2
Filter Comparison Results
For each ﬁlter and model order, the RMS errors are computed as a function of time
and then the mean and maximum errors (over time) are generated for each case. These
mean and maximum values are summarized in Tables 19.1, 19.2, and 19.3, for the
position, velocity, and acceleration RMS errors, respectively.
TABLE 19.1
Synthetic Data RMS Positional Error Summary
Position (in.)
Orientation (deg)
Filter
Model
Type
Order
Mean
Max
Mean
Max
NLLSQ
0
0.68
0.81
0.22
0.26
UKF
0
0.61
0.72
0.20
0.24
UKF
1
0.30
0.43
0.15
0.22
UKF
2
0.32
0.52
0.15
0.22
UKF
3
0.32
0.47
0.14
0.23
UGPF
2
0.32
0.48
0.15
0.22
TABLE 19.2
Synthetic Data RMS Velocity Error Summary
Translational
Angular
Velocity (ft/s)
Velocity (deg/s)
Filter
Model
Type
Order
Mean
Max
Mean
Max
NLLSQ
0
N/A
N/A
N/A
N/A
UKF
0
N/A
N/A
N/A
N/A
UKF
1
0.74
1.43
14.04
22.76
UKF
2
0.71
1.63
12.34
22.18
UKF
3
0.89
1.78
10.56
34.37
UGPF
2
0.72
1.67
12.30
20.39

PERFORMANCE COMPARISON ANALYSIS
339
TABLE 19.3
Synthetic Data RMS Acceleration Error Summary
Translational
Angular
Acceleration (G’s)
Acceleration (deg/s2)
Filter
Model
Type
Order
Mean
Max
Mean
Max
NLLSQ
0
N/A
N/A
N/A
N/A
UKF
0
N/A
N/A
N/A
N/A
UKF
1
N/A
N/A
N/A
N/A
UKF
2
0.37
1.70
653.3
3005.9
UKF
3
0.57
1.77
563.9
3085.0
UGPF
2
0.38
1.71
652.6
2990.2
Figures 19.8, 19.9, and 19.10 show time histories of the performance of each
ﬁlter/model order combination for positional and rotational RMS errors, position and
rotation velocity RMS errors, and position and rotation accelerations, respectively
(top plots). It is obvious from Tables 19.1 through 19.3 and Figures 19.8 through
19.10 that all of the ﬁlters that use a model order greater than zero have an average
performance that is at least 50% better in position and 30% better in orientation.
Because of the large acceleration transient at the time of store release at 0.1 s, the
RMS error plots show the poorest performance for 200–300 ms after the release event.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Time (s)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (s)
Inches
NLLSQ
UKF (0)
UKF (1)
UKF (2)
UKF (3)
UGPF (2)
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
deg
FIGURE 19.8
RMS errors for position and orientation of all tracking ﬁlters using synthetic
data.

340
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Time (s)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (s)
ft/s
 
0
5
10
15
20
25
30
35
deg/s
 
UKF (1)
UKF (2)
UKF (3)
UGPF (2)
FIGURE 19.9
RMS errors for position and orientation velocities of all tracking ﬁlters using
synthetic data.
The second-order unscented Gaussian particle ﬁlter (UGPF(2)) and the second-
order unscented Kalman ﬁlter (UKF(2)) have nearly equal performance over all six
parameter types. Since a Gaussian random error was used for the synthetic measure-
ments, the near equal performance of UGPF(2) and UKF(2) indicates that the degree
of nonlinearity in either the dynamic or measurement models is insufﬁcient to warp
the probability density of the state vector. Since the state vector density always re-
mains Gaussian, the RMS errors for the UGPF(2) are of the same order of magnitude
as those of the UKF(2). We therefore judge that the considerable computational ex-
pense related to the use of a particle ﬁlter is unnecessary in our problem space. We
might reconsider this judgment if we encounter non-Gaussian measurement noise of
sufﬁcient severity in real-world use.
The third-order unscented Kalman ﬁlter (UKF(3)) has signiﬁcant performance
problems in comparison to UKF(1) and UKF(2). Although the mean RMS error of
UKF(3) is signiﬁcantly better for some parameter types, in some cases it performs
much more poorly than either UKF(1) or UKF(2). The problem with the performance
our current third-order dynamic model merits further investigation.
The UKF(2) solver appears to have marginally better performance than UKF(1).
RMS statistics are roughly equivalent for position, angle, and translational velocity.

PERFORMANCE COMPARISON ANALYSIS
341
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Time (s)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (s)
G’s
 
UKF (2)
UKF (3)
UGPF (2)
0
500
1000
1500
2000
2500
3000
3500
deg/s/s
 
FIGURE 19.10
RMS errors for position and orientation accelerations of all tracking ﬁlters
using synthetic data.
In the angular velocity time history, RMS error of the UKF(2) solver stabilizes after
about 100 ms, while the UKF(1) solver continues to oscillate.
19.6.3
Conclusions and Future Considerations
The second-order unscented Kalman ﬁlter (UKF(2)) achieves the best performance
of all of the solvers tested, and provides a marked improvement over the standard
zeroth-order NLLSQ solver. Although the UGPF(2) particle ﬁlter has equivalent per-
formance, its considerable computational expense makes it unnecessary.
All of the solvers tested whose order was greater than zero experienced degradation
in performance immediately after the large jump in acceleration caused by the release
event impulse. Accounting for this impulse in the dynamic model should signiﬁcantly
improve ﬁlter performance. The free-fall equations in our dynamic model are not
sufﬁcient to account for changes in the process (such as a forcing function) that are
not attributable to noise.
An alternative approach to this degradation problem is to directly measure the ac-
celerations causing the performance degradation. This is discussed in the next chapter,
where we combine image measurements with accelerometer and rate gyroscope mea-
surements.

342
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
Another approach we intend to investigate is using not only the 2D locations of
feature points in images, but also the 2D image velocity of feature points measured
from image sequences using techniques such as optical ﬂow. Adding feature point
image velocity to the measurement model should help to stabilize and increase the
accuracy of the 3D translational and angular velocity estimates generated by the
Bayesian ﬁlter.
APPENDIX 19.A
QUATERNIONS, AXIS-ANGLE VECTORS, AND
ROTATIONS
19.A.1
Conversions Between Rotation Representations
Let a =

ax, ay,az
⊺∈R3 be used as an axis-angle representation of rotation where
the direction of a speciﬁes the axis of rotation and ∥a∥speciﬁes the magnitude of
rotation (in radians).
Let q =

qs, qx, qy,qz
⊺∈H, where H is the space of quaternions, be used as a unit
quaternion representation of rotation, where qs is real and qx, qy,qz are coefﬁcients
of distinct imaginary numbers, and

q2s + q2x + q2y + q2z = 1.
Let M ∈R3 × R3 be a 3 × 3 rotation matrix, such that M is orthonormal, and
det M = +1.
To convert from an axis-angle vector a to a unit quaternion q , we deﬁne the
function Qa : R3 →H as
q = Qa (a) =

cos ∥a∥
2 , sin ∥a∥
2
∥a∥

ax, ay, az

⊺
(19.161)
To convert from a unit quaternion q to an axis-angle vector a we deﬁne the function
Aq : H →R3 as
a = Aq (q) = 2 cos−1 qs

1 −q2s

qx, qy, qz
⊺
(19.162)
To convert from a unit quaternion q to a rotation matrix M, we deﬁne the function
Mq : H →R3 × R3 as
M = Mq (q) =


2

q2
s + q2
x

−1
2

qxqy −qsqz

2

qxqz + qsqy

2

qxqy + qsqz

2

q2
s + q2
y

−1
2

qyqz −qsqx

2

qxqz −qsqy

2

qyqz + qsqx

2

q2
s + q2
z

−1


(19.163)
To convert from an axis-angle vector a to a rotation matrix M, we derive the
function Ma : R3 →R3 × R3 as
M = Ma (a) = Mq (Qa (a))
(19.164)

QUATERNIONS, AXIS-ANGLE VECTORS, AND ROTATIONS
343
To convert the Euler angles (φ, θ, ϕ) into a unit quaternion q, we deﬁne the function
QEuler (φ, θ, ϕ) : R3 →
QEuler (φ, θ, ϕ) = qϕ qθqφ
(19.165)
where qφ, qθ, and qϕ are the quaternions that represent each of the three Euler angles
by themselves
qφ =

cos φ
2 , sin φ
2 , 0, 0
⊺
(19.166)
qθ =

cos θ
2, 0, sin θ
2, 0
⊺
(19.167)
qϕ =

cos ϕ
2 , 0, 0, sin ϕ
2
⊺
(19.168)
19.A.2
Representation of Orientation and Rotation
Rotation is represented by two quantities together, a 3D axis-angle vector a that
resides in the state vector, and an external unit quaternion q that resides outside the
state vector and is only used in the observation model. During an iteration of the
ﬁlter, the quaternion q represents the prior orientation of the rigid body relative to
the reference coordinate system. The vector a represents an adjustment to q and a by
itself only partially represents the current orientation. When derivatives of rotation
are present, they are represented in the state vector by the 3D vectors ˙a, ¨a, ...a, . . ..
The quaternion multiplication of two quaternions p and q resulting in the quater-
nion r is represented by the operation
r = pq
(19.169)
or
r =


rs
rx
ry
rz

=


ps
−px
−py
−pz
px
ps
−pz
py
py
pz
ps
−px
pz
−py
px
ps




qs
qx
qy
qz


(19.170)
which can be written as the matrix vector multiplication
r = Pq
(19.171)
The external unit quaternion is not altered during a ﬁlter iteration and is only used
in the observation model. Once the ﬁlter update step has completed, the corrected axis-
angle portion of the state vector an|n is applied to update the external unit quaternion,
after which an|n is reset
qn = Qa

an|n

qn−1
(19.172)
an|n ←[0, 0, 0]⊺
(19.173)

344
TRACKING A FALLING RIGID BODY USING PHOTOGRAMMETRY
19.A.3
Point Rotations and Frame Rotations
We draw a distinction between point rotations and frame rotations as follows. In
general terms, when a point rotation is performed, the point or vector is rotated while
the axis directions remains constant. With a frame rotation, the axes rotate while the
point or vector remains ﬁxed. To perform a point rotation of a 3D vector v by an
angular displacement speciﬁed by a unit quaternion q, we construct a rotation matrix
using the function Mq speciﬁed in (19.164)
Mq = Mq (q)
(19.174)
then we multiple to produce the rotated vector v(rot)
v(rot) = Mqv
(19.175)
Nowletv(f1) beavectorwhosecomponentsarespeciﬁedwithrespecttocoordinate
frame f1, and let qf1→f2 specify the angular displacement between the coordinate
frames f1 and f2. To perform the frame rotation that ﬁnds the coordinates of v(f1)
with respect to f2, we perform
Mqf1→f2 = Mq

qf1→f2

(19.176)
and
v(f2) = Mqf1→f2v(f1)
(19.177)
Observe that a frame rotation is the inverse of a point rotation. Since rotation
matrices are orthonormal, if M is a rotation matrix, then M−1 = M⊺. Also observe
that the origin of both f1 and f2 are coincident.
We use rotations in two ways in the observation model. We ﬁrst ﬁnd the coordinates
of feature points on a rigid body after it has been rotated and translated to the body’s
estimated location in the reference (aircraft) coordinate system. Since the points on
the body are being rotated and then translated into new locations, we perform a point
transformation
s(ref) = Mrefs + p
(19.178)
where Mref is the rotation matrix derived from the estimated rotation angle of the
body, the 3D vector p speciﬁes the translation of the body’s center of mass, and s
speciﬁes the location of a 3D feature point in the body’s coordinate system.
We then calculate the location of the point s(ref) with respect to a camera’s coordi-
nate system using a frame transformation (a negative translation followed by a frame
rotation)
s(cam) = M⊺
cam

s(ref) −pcam

(19.179)
where Mcam describes the camera’s orientation and pcam its position.

REFERENCES
345
Observe that the inverse of a point transformation is a frame transformation. So to
invert (19.178) we could perform
s = M⊺
ref

s(ref) −p

(19.180)
or to invert (19.179) we could perform
s(ref) = Mcams(cam) + pcam
(19.181)
REFERENCES
1. Huang TS, Netravali AN. Motion and structure from feature correspondences: a review.
Proc. IEEE 1994:82(2); 252–268.
2. Iu SL, Wohn K. Estimation of General Rigid Body Motion from a Long Sequence of
Images. Department of Computer & Information Science Technical Report, University of
Pennsylvania; 1990.
3. Gennery DB. Visual tracking of known three-dimensional objects. Int. J. Comput. Vision
1992;7(3):243–270.
4. Welch GF. SCATT: Incremental Tracking with Incomplete Information. TR96051, Disser-
tation. Department of Computer Science, UNC-Chapel Hill; 1996.
5. Welch G, Bishop G. SCATT: incremental tracking with incomplete information. In Pro-
ceedings of the ACM Siggraph 97. New York: ACM Press; 1997, pp. 333–344.
6. Forseman E, Schug D. Improved analysis techniques for more efﬁcient weapon separation
testing. In Proceedings of the Society of Flight Test Engineers European Chapter; 2008.
7. Bar-Shalom Y, Li XR, Kirubarajan T. Estimation with Applications to Tracking and Navi-
gation. Wiley; 2001.
8. Mehrotra K, Mahapatra PR. A jerk model for tracking highly maneuvering targets. IEEE
Trans. Aero. Electron. Syst. 1997;33(4):1094–1105.
9. Online at ae-www.technion.ac.il/admin/serve.php?id=18558, accessed August 2011.

20
SENSOR FUSION USING
PHOTOGRAMMETRIC AND INERTIAL
MEASUREMENTS
20.1
INTRODUCTION
In Chapter 19, we presented a method for tracking the pose trajectory (position and
orientation as a function of time) of a rigid falling body using only external video
camera sensors. In this method, video images from multiple cameras are used to view
feature points positioned on the surface of the rigid body. Thus, the measurement set
consists only of feature point positions viewed by the cameras, with feature point
occlusions creating data dropouts for certain rigid body orientations. In addition, no
velocity or acceleration measurements are directly obtainable, so estimates of higher
order derivative terms related to the position and orientation of the rigid body have
reduced accuracy.
In this chapter, we examine the utility of adding additional body-mounted inertial
sensors that include multiaxis accelerometers and gyroscopes which, when packaged
together, are referred to as an inertial measurement unit (IMU). An IMU provides
measurements of linear accelerations along the three body axes and angular velocities
about the three body axes. This presents a requirement to fuse the temporal steam of
IMU measurement data with the temporal stream of video image data to produce a
tracking ﬁlter that utilizes both in an asynchronous measurement data stream.
Methods for tracking a rigid body using only successive temporal images of feature
points from a single camera (monocular imaging) were ﬁrst implemented in Refs [1],
[2]. In the paper by Iu and Wohn [1], they implemented a dynamic model that assumed
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
346

THE PROCESS (DYNAMIC) MODEL FOR RIGID BODY MOTION
347
a constant translational and rotational accelerations while Lee et al. [2] developed a
dynamic model with constant translation and rotational velocities. Methods were
advanced that used multiple camera images of feature points to calculate the pose of
a rigid body at each time step in Refs [3–6]. Gennery [3], Huang and Natravali [4],
and Ude [5] all utilized a nonlinear least squares (NLLSQ) method for state vector
estimation that assumes no dynamic noise. This method is not recursive in time, but
is instead iterative at each time step. Halvorsen et al. proposed using an extended
Kalman Filter (EKF) that was recursive in time and therefore could handle missing
feature points in the video images from multiple cameras.
In Refs [8–12], methods for estimating the orientation of a rigid body using either
an IMU or a magnetic, angular rate, and gravity (MARG) sensor were proposed.
In Ref. [8], Algrain and Samiie use an accelerometer gyro linear Gaussian (AGLG)
estimation technique that is essentially a linear least-squares iterative method that is
not time recursive. A MARG sensor is used in Refs [9–12] in conjunction with an EKF
to track the orientation of a rigid body. These four papers present slight modiﬁcations
to the original method and are recursive in time methods for tracking human body
hand or limb orientations.
Fusion of video and IMU data was the focus of the articles [13–15]. In all cases the
IMU is mounted next to the camera and the purpose of the fusion was for calibration
of the camera. Since the IMU was not mounted external to the object being tracked,
the IMU data was not used directly as measurement data in any of the object state
estimation methods proposed in these papers.
In what follows we present an approach that includes an IMU mounted on the
rigid body with external video cameras viewing feature points on the rigid body. Data
from the rigid body mounted IMU is fused with the video image data to create a
single stream of asynchronous data that is used in the update step of the tracking
algorithm. In our approach, an unscented Kalman ﬁlter (UKF) is used as the fusion
tracking algorithm and its performance results are compared to those obtained from
using either the video image or the IMU data alone.
A picture that shows several views of the IMU alone or mounted on a store (bomb-
like rigid body) is presented in Figure 20.1.
20.2
THE PROCESS (DYNAMIC) MODEL FOR RIGID BODY MOTION
The dynamic motion model for the motion of a rigid body was described in Section 2
of Chapter 19. We refer the reader to Chapter 19 for a full description of those dynamic
models. At the end of Chapter 19 we showed that the best relative performance was
achieved using the second order constant acceleration model, where we modiﬁed
the state vector to include rate and acceleration vectors governing both translational
and rotational motion. We will use that model exclusively for the analysis in this
chapter. Our motion model describes the translational and rotational motion of a
rigid body as a function of time in terms of an instantaneous state vector x (t) =

p⊺(t), ˙p⊺(t), ¨p⊺, a⊺(t), ˙a⊺(t), ¨a⊺⊺that, along with the external unit quaternion q (t) ,
deﬁnes both the translation of the center of mass of a rigid body relative to a ﬁxed

348
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
FIGURE20.1
Afree-standingIMUwithtransmittingantennaattached,andanIMUmounted
in the forward fuse well of a 500 pound store.
reference Cartesian coordinate frame, p (t) =

x(t), y(t), z (t)
⊺, and the orientation
of the body about the center of mass as given by the combination of the axis-angle
vector, a (t) =[ax (t), ay (t), az (t)]⊺and the external unit quaternion q (t) =[qs (t),
qx (t), qy (t), qz (t)]⊺.
In general, we can write the state vector temporal transition equation in the form
xn = f (xn−1) + vn−1
(20.1)
where vn−1 is a noise that accounts for the uncertainty in the process. In all cases we
will take the noise to be Gaussian and additive.
20.3
THE SENSOR FUSION OBSERVATIONAL MODEL
The task is to deﬁne the functional dependence for the transformation of the state
vector xn into the components of the sensor fusion observation vector zn. If the
observation noise is included, this transformation can be written as
zn = hn (xn) + wn
(20.2)
where wn ∼N (0, Rn). The observations consist of two separate components, the
IMUcomponentsandthephotogrammetriccomponents,whichmustbefusedtogether
to provide an asynchronous stream of observation data.
20.3.1
The Inertial Measurement Unit Component of the Observation Model
The IMU package outputs data from three orthogonal accelerometers and three or-
thogonal rate gyroscopes. The IMU observation vector at time tn can be written as
zIMU,n =

zx,n, zy,n, zz,n, zφ,n, zθ,n, zϕ,n
⊺
(20.3)

THE SENSOR FUSION OBSERVATIONAL MODEL
349
where zx,n, zy,n, and zz,n are the accelerations (in G’s) measured by the x-forward,
y-right, and z-down accelerometers, respectively, and zφ,n, zθ,n and zϕ,n are rotation
rates (in degrees) measured by the roll (counterclockwise about the x-axis), pitch
(counterclockwise about the y-axis), and yaw (counterclockwise about the z-axis)
rate gyroscopes, respectively. Note that since the IMU package is attached to the rigid
body, it rotates as the body rotates so that the measurements in zIMU,n are taken with
respect to the body coordinate system and not with respect to the reference (aircraft)
coordinate system.
We can now write the IMU observation vector in terms of the state vector by
looking at the transformation equation
zIMU,n = hIMU,n (xn) + wIMU,n
(20.4)
where wIMU,n ∼N

0, RIMU,n

, with RIMU,n deﬁned as the diagonal noise covariance
matrix of accelerometer and rate gyroscope measurement variances given by
RIMU,n =

σ2
accI3
03
03
σ2
gyroI3

(20.5)
with σacc and σgyro deﬁned as the standard deviation of the accelerometer and angular
rate measurement standard deviations, respectively, I3 is a 3 × 3 identity matrix, and
03 is a 3 × 3 matrix of zeros.
To construct hIMU,n (xn), let us ﬁrst deﬁne gref = [gx,ref, gy,ref, gz,ref]⊺as the am-
bient acceleration (in G’s) felt by the reference coordinate system. At the surface
of the earth the gravity vector would be gref = [0, 0, −1]⊺. But since the reference
aircraft is maneuvering and undergoing a G load, gref will vary with time. But, since
the aircraft is much more massive than the rigid body and has much greater inertia,
we choose gref to be constant since it will vary slowly with time compared to the rigid
body being tracked. In addition, we also ignore any change in orientation that the
reference coordinate system might be undergoing due to an aircraft maneuver. In fu-
ture implementations, we may modify our procedure to take into account these small
changes. We also assume that the IMU is located at the center of the rigid body’s coor-
dinate system (normally its center of gravity or CG). A lever-arm correction should be
added that compensates for the centripetal and tangential linear accelerations caused
by rotations of the accelerometers about the body’s CG.
The rotation matrix Mref that represents the body’s current orientation with respect
to the reference coordinate system can now be constructed in a fashion identical to
that of the observation model presented in Chapter 19. Using (19.48) through (19.50),
we can write
Mref = Man · Mqn−1
(20.6)
where Man = Ma (an) and Mqn−1 = Mq (qn−1). Since the measurements in zn are
taken with respect to the body’s coordinates system, the noiseless body-referenced

350
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
rotation rates zφ,n, zθ,n, and zϕ,n can be produced as follows


zφ,n
zθ,n
zϕ,n

= ˙a(body)
n
= M⊺
ref˙an
(20.7)
The 3D acceleration measured by the x, y, and z accelerometers (in body-
referenced coordinates) at time tn is
g(body)
n
= ¨p(body)
n
+ g(body)
ref
(20.8)
where ¨p(body)
n
is the translational acceleration of the body’s center of gravity and
g(body)
ref
is the acceleration felt by the reference coordinate system rotated into body
coordinates. Now ¨p(body)
n
+ g(body)
ref
can be easily calculated from
¨p(body)
n
+ g(body)
ref
= M⊺
ref¨pn + M⊺
refgref
= M⊺
ref (¨pn + gref)
(20.9)
The noiseless body-referenced acceleration measurements can therefore be
expressed as


zx,n
zy,n
zz,n

= g(body)
n
= M⊺
ref (¨pn + gref)
(20.10)
Using (20.7) through (20.10), we can now express hIMU,n (xn) as
hIMU,n (xn) =

zx,n, zy,n, zz,n, zφ,n, zθ,n, zϕ,n
⊺
=

g(body)
n
˙a(body)
n

(20.11)
20.3.2
The Photogrammetric Component of the Observation Model
To deﬁne the photogrammetric component of the observation model, we simply recall
the observation model exactly as described in Section 3 of Chapter 19. First, we must
rename the components derived in Chapter 19 as
zn →zphoto,n
(20.12)
hn (xn) →hphoto,n (xn)
(20.13)
wn →wphoto,n
(20.14)
Rn →Rphoto,n
(20.15)
Just as in Chapter 19, note that zphoto,n, hphoto,n (xn), and wphoto,n are of dimension
2Mn, and Rphoto,n is of dimension 2Mn × 2Mn where Mn is the number of feature

THE SENSOR FUSION OBSERVATIONAL MODEL
351
points visible from all cameras. We observe that hphoto,n and Mn are time dependent
because the number of feature points observed by each camera can change with time.
20.3.3
The Combined Sensor Fusion Observation Model
In order to indicate which type of measurement(s) are available at time tn, we deﬁne
an observation ﬂag Fn as
Fn =





1,
Only IMU data is present
2,
Only photogrammetric data is present
3,
Both types of data are present
(20.16)
The observation vector zn at time tn can now be written as
zn =







zIMU,n,
Fn = 1
zphoto,n,
Fn = 2

z⊺
IMU,n, z⊺
photo,n
⊺
,
Fn = 3
(20.17)
Similarly, the transformation function hn (xn) can be written as
hn (xn) =







hIMU,n (xn) ,
Fn = 1
hphoto,n (xn) ,
Fn = 2

h⊺
IMU,n (xn) , h⊺
photo,n (xn)
⊺
,
Fn = 3
(20.18)
The observation noise, being a combination of its components, is also assumed to
be the zero-mean Gaussian process wn ∼N (0, Rn), with Rn deﬁned as the diagonal
noise covariance matrix
Rn =











RIMU,
Fn = 1
Rphoto,
Fn = 2

RIMU
0SF
0⊺
SF
Rphoto

,
Fn = 3
(20.19)
where 0SF is a zero matrix of dimension 6 × 2Mn.
Finally, if the IMU clock is not synchronized with the photogrammetric clock, the
data from the IMU may be asynchronous with that of the photogrammetric data and
the time difference between successive measurements may not be uniform. Thus, a
time-dependent Tn must be used in all tracking ﬁlters, with
Tn = tn −tn−1
(20.20)

352
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
20.4
THE GENERATION OF SYNTHETIC DATA
In order to evaluate the performance of the various tracking methods, in Chapter
19 we developed a synthetic rigid body and created a synthetic trajectory that is
representative of a store release event. This gave us a synthetic “truth” trajectory that
allowed us to create Monte Carlo sets of synthetic noisy observations as seen from a
set of virtual cameras. The Monte Carlo synthetic measurement sets were then used
in a variety of estimation (solver) methods and the root mean squared (RMS) track
errors calculated from each estimation method allowed for comparative performance
analysis.
The synthetic trajectories and Monte Carlo noisy camera observations sets from
Chapter 19 will be reused in this chapter, and we will add noisy measurements as felt
by a virtual IMU’s accelerometers and rate gyroscopes attached to the synthetic rigid
body. Thus, we will be able to perform comparative analysis of various estimation
methods and also analyze any improvements in performance due to the addition of
IMU measurements.
20.4.1
Synthetic Trajectory
We shall use the same synthetic trajectory developed in Section 19.5.2, except that
we generate a sequence of times t that has a faster data rate
t = {tn = 0.001 ∗n, n = 0, 1, 2, . . . , 1000}
(20.21)
Although the IMU measurements are generated at the shorter period of 0.001 seconds,
the image measurements are still generated at the slower period of 0.005 s. To indicate
this, we set the ﬂag variable Fn to be
Fn =

3,
{n, mod 5} = 0
1,
otherwise
(20.22)
The synthetic trajectory can now be constructed according to (19.106) through
(19.111).
20.4.2
Synthetic Cameras
We construct two virtual cameras according to Section 19.5.3.
20.4.3
Synthetic Measurements
20.4.3.1
IMU Measurements. For each n ={0, 1, 2, . . . , 1000} we generate the
IMU synthetic measurement vector zo
IMU,n,synth as
zo
IMU,n,synth = hIMU,n

xn,true

+ wIMU,n,synth
(20.23)

THE GENERATION OF SYNTHETIC DATA
353
where
wIMU,n,synth ∼N

0, RIMU,synth

(20.24)
with
RIMU,synth =

σ2
accI3
03
03
σ2
gyroI3

(20.25)
Here, σacc and σgyro are set to 0.1 G’s and 5.0 deg/s, respectively.
20.4.3.2
Camera Measurements. Camera measurements are generated at every
ﬁfth point in the time sequence. For each n ={0, 1, 2, . . . , 200} we generate the
camera synthetic measurement vector zo
photo,5n,synth as
zo
photo,5n,synth = hphoto,5n

x5n,true

+ wphoto,5n,synth
(20.26)
where
wphoto,5n,synth ∼N

0, Rphoto,5n,synth

(20.27)
with
Rphoto,5n,synth = σ2
pixelI2Mn = σ2
pixelI32
(20.28)
Here, σpixel is set to 2 pixels.
20.4.3.3
Combined Measurements. For each n ={0, 1, 2, . . . , 1000} we generate
the combined synthetic measurement vector zo
n,synth as
zo
n,synth =

hc,n

xn,true

+ wn,synth,
{n, mod 5} = 0
hIMU,n

xn,true

+ wn,synth,
otherwise
(20.29)
where
hc,n

xn,true

≜

h⊺
IMU,n

xn,true

, h⊺
photo,n

xn,true
⊺
(20.30)
with
wn,synth ∼N

0, Rn,synth

(20.31)
and
Rn,synth =






RIMU,synth
0SF
0⊺
SF
Rphoto,5n,synth

,
{n, mod 5} = 0
RIMU,synth,
otherwise
(20.32)
Here, 0SF is a zero matrix of dimensions 6 × 32. Note that a new Monte Carlo sample
is drawn from Rn,synth for each time step.

354
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
20.5
ESTIMATION METHODS
The estimation methods used for comparative analysis of the sensor fusion mod-
els include methods using photogrammetric data alone, methods that use IMU data
alone and our method that fuses the video data with the IMU data. In Chapter 19,
we concluded that the second-order unscented Kalman ﬁlter (UKF(2)) achieves the
best performance of all of the solvers tested and provides a marked improvement
over the standard zeroth-order NLLSQ solver when only photogrammetric video
data is used. For completeness, the comparative performance analysis of Section 20.6
will include performance results from both the UKF(2) and the NLLSQ when only
photogrammetric observation data is used. When only IMU data is used as observa-
tions, a second-order Predictor–Corrector method and the UKF(2) are implemented
as tracking ﬁlters. To do this, we set Fn = 1, for ∀n so that only IMU data is used as
observations.
For a description of the NLLSQ and UKF(2) estimation processes when only
photogrammetric video data is used, see Sections 19.4.1 and 19.4.2, respectively.
A description of these methods will not be repeated here. In Section 20.5.1, we
discuss the Predictor–Corrector estimation method used when only IMU data is being
processed.
20.5.1
Initial Value Problem Solver for IMU Data
The accepted method for estimating the position and orientation of a rigid body using
an IMU’s translational acceleration and angular rate measurements is to numerically
solve a system of ordinary differential equations (ODE) using an initial value problem
(IVP) solver. There are many IVP solvers to choose from and here we describe a sim-
ple but accurate iterative solver called the Euler Trapezoidal or Predictor–Corrector
method [16,17]. In what follows, we construct a ﬁrst-order system of ODEs that relate
the IMU measurements to the position, orientation, and translational velocities of the
rigid body. We then brieﬂy describe how the solver is initialized and how it is used to
solve the system of ODEs resulting in an estimate of the object’s trajectory (position
and orientation) as a function of time.
20.5.1.1
The Predictor-Corrector IVP Solver. In the general case, the objective is
to solve a ﬁrst-order system of ODEs that can be cast as an initial value problem.
If one wishes to estimate an unknown function x (t) from a vector-valued function
f (t, x (t)) that expresses the relationship of x (t) to its ﬁrst time derivative, given an
initial value x (t0) = x0, we can write
dx
dt = f (t, x (t))
(20.33)
x (t0) = x0
(20.34)
Euler’s method is perhaps the simplest quadrature rule for integrating an ODE. Let
t = {tn, n = 0, 1, 2, . . . , N} and identify xn = x (tn) . Using Euler’s rule, a prediction

ESTIMATION METHODS
355
of xn can be obtained from
xpred
n
= xn−1 + Tf (tn−1, xn−1)
(20.35)
where T = tn −tn−1. Euler’s method can be thought of as a rectangular integration
rule that evaluates the integrand only once at the left-hand endpoint of the interval. The
rule is exact if f (t, x) is constant but is not exact if f (t, x) is linear, with the error being
proportional to the step size T. Small step sizes are needed in order to get an accurate
estimate but there is no automatic way to determine the step size based on a required
error budget. To reduce this uncertainty in the error, it is important to follow the Euler
step with a second function evaluation at the right-hand endpoint of the interval. One
such approach is called the Predictor–corrector method that uses the trapezoidal rule
in an iterative fashion. First, an Euler step is taken across the interval to make an initial
prediction using (20.35). Then the function is evaluated at the extrapolated point xpred
n
and the two slopes are averaged to obtain the ﬁrst corrected estimate of xn
ˆx1,n = xn−1 + T
2

f (tn−1, xn−1) + f

tn, xpred
n

(20.36)
For k > 1, the correction step is iteratively repeated and the quality of the estimate
ˆxk,n is evaluated by comparing it to its predecessor using
ek =
ˆxk,n −ˆxk−1,n
2
(20.37)
where
ˆxk,n = xn−1 + T
2

f (tn−1, xn−1) + f

tn, ˆxk−1,n

(20.38)
The iteration is terminated when ek becomes sufﬁciently small relative to some thresh-
old, that is, ek < ε. Usually, we take ε = √εmach, with εmach ≃2.2204 × 10−16 that
is the smallest double precision machine number larger than zero that when added
to one gives a value discernibly greater than one. When the iteration reaches the
threshold, we set xn = xcorr
k,n and proceed to the next time step.
20.5.1.2
A System of ODE’s for the IMU Problem. We now construct a ﬁrst order
system of ODEs that relates body-referenced rotation rates measured by the IMU
to rotation angles and rates in the reference coordinate system, and relates body-
referenced translational accelerations to velocities and positions in aircraft coordi-
nates. Let zo =[(zo
α)⊺, (zo
ω)⊺]⊺be a measurement from the IMU, where zo
α =[zx, zy,
zz]⊺represents a measurement from the three orthogonal accelerometers and zo
ω =[zφ,
zθ, zϕ]⊺represents a measurement from the three orthogonal rate gyroscopes. Let
x =[p⊺, ˙p⊺, q⊺]⊺be the unknown state vector we wish to estimate, where p =[px,
py, pz] ⊺represents the 3D position, ˙p =[˙px, ˙py, ˙pz] ⊺represents the 3D velocity,
and q =[qs, qx, qy, qz]⊺is a unit quaternion representing the 3D orientation. Note
that zo is measured in body-referenced coordinates while x is expressed in reference
coordinates.

356
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
To write the ODE for q, we begin with (19.122) and replace ω with zo
ω to obtain
dq
dt = 1
2qzo
ω
(20.39)
where it is understood that in the context of the quaternion multiplication q · zo
ω, the
multiplicand zo
ω is represented by the pure quaternion [0, zo
ω]⊺= [0, zφ, zθ, zϕ]⊺.
Likewise, to relate the 3D position p to the IMU measured body-referenced accel-
eration measurements zo
α, the second-order ODE equation is
d2p
dt2 = Mqzo
α −g
(20.40)
where the rotation matrix Mq = Mq (q) rotates zo
α into reference coordinates and g is
the gravitational acceleration acting on the reference coordinate system. This second-
order equation can be reduced to a system of ﬁrst-order equations by including the
velocity equation
dp
dt = ˙p
(20.41)
Now, (20.40) can be rewritten using (20.41) to obtain
d ˙p
dt = Mqzo
α −g
(20.42)
Thus, (20.39), (20.41), and (20.42) constitute a 10-dimensional ﬁrst-order system of
ODEs that fully describe the relationship between the IMU measurements and the
rigid body’s position, orientation, and translational velocities. If we deﬁne x = [p⊺,
˙p⊺, q⊺]⊺, we can now write (20.33) as
dx
dt = f (t, x) = f


t,


p
˙p
q




=


˙p
Mqzo
α −g
1
2qzo
ω


(20.43)
20.5.1.3
Initializing and Using the Predictor–Corrector for Position and Orien-
tation Estimation. The Predictor–Corrector can be initialized for estimation of the
trajectory of the synthetic rigid body by setting
x0 = [p⊺
0, ˙p⊺
0, q⊺
0]⊺
= [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]⊺
(20.44)
where the last four components are the unit quaternion that represents a zero rotation.
Note that in other contexts x0 might contain nonzero components, depending on the
circumstances. Since the Predictor–Corrector is an unconstrained estimator, once an
iteration has produced the estimate ˆxn at time tn, the quaternion component ˆqn will
not necessarily be unitary and would therefore not represent the orientation correctly.

PERFORMANCE COMPARISON ANALYSIS
357
To correct this, ˆqn is normalized to unit length prior to the next temporal iteration by
performing
ˆqn =
ˆqn
∥ˆqn∥
(20.45)
20.6
PERFORMANCE COMPARISON ANALYSIS
The performance analysis to be presented below was conducted using the synthetic
test data described in Section 20.4. Note that this is the same synthetic trajectory
(with identical noise in the image data) as was used in Section 19.6. The performance
of the sensor fusion estimation ﬁlter (UKF(2)) is compared against two estimators
that operate only on photogrammetric data (NLLSQ and UKF(2)) and also against
two estimators that use only IMU data (Predictor–Corrector and UKF(2)). The per-
formance metric used in all cases is the root mean squared (RMS) error. As we will
show below, the sensor fusion estimator had the lowest RMS errors and was therefore
the best estimator.
Figures 20.2 through 20.4 show the estimated track components of a single run
of the sensor fusion ﬁlter against the truth track components. Figure 20.5 shows a
0
0.2
0.4
0.6
0.8
1
−25
−20
−15
−10
−5
0
5
X-forward
Time (s)
Inches
 
 
0
0.2
0.4
0.6
0.8
1
−1
0
1
2
3
4
5
6
Y-right
Time (s)
Inches
 
 
0
0.2
0.4
0.6
0.8
1
−10
0
10
20
30
40
50
60
Z-down
Time (s)
Inches
 
 
0
0.2
0.4
0.6
0.8
1
−80
−70
−60
−50
−40
−30
−20
−10
0
10
Roll
Time (s)
deg
 
 
0
0.2
0.4
0.6
0.8
1
−30
−25
−20
−15
−10
−5
0
5
Pitch
Time (s)
deg
 
 
0
0.2
0.4
0.6
0.8
1
−2
0
2
4
6
8
10
12
Yaw
Time (s)
deg
 
 
Estimate
Truth Data
FIGURE 20.2
Translational and rotational position using the sensor fusion estimator.

358
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
0
0.2
0.4
0.6
0.8
1
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
X-forward velocity
Time (s)
ft/s
 
 
Estimate
Truth data
0
0.2
0.4
0.6
0.8
1
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Y-right velocity
Time (s)
ft/s
 
 
0
0.2
0.4
0.6
0.8
1
−1
0
1
2
3
4
5
6
7
Z-down velocity
Time (s)
ft/s
 
 
0
0.2
0.4
0.6
0.8
1
−140
−120
−100
−80
−60
−40
−20
0
20
Body roll rate
Time (s)
deg/s
 
 
0
0.2
0.4
0.6
0.8
1
−200
−150
−100
−50
0
50
100
150
200
Body pitch rate
Time (s)
deg/s
 
 
0
0.2
0.4
0.6
0.8
1
−150
−100
−50
0
50
100
150
Body yaw rate
Time (s)
deg/s
 
 
FIGURE 20.3
Translational and rotational velocity using the sensor fusion estimator.
close-up of a small section of the lateral position truth trajectory with the estimated
trajectories form multiple estimators superimposed over the truth. Identical noisy
observations and initialization were used as inputs to all estimation methods. Analysis
of these ﬁgures leads to a number of observations.
Both estimation methods that use only IMU data are inaccurate, especially the
UKF implementation. It is well known that estimators that rely on IMU data alone
are sensitive to initial conditions and calibration biases, but in this (synthetic) case,
the initial conditions and calibrations are exact. Rather, the inaccuracies appear to be
caused by drift due to random noise in the observations, especially since one of the
solver’s estimate is too large and the other too small.
In addition, both estimators that use only photogrammetric data are accurate but
imprecise. The UKF(2) appears to be more accurate (and more precise) that the
NLLSQ due primarily to its inclusion of derivative terms in the state vector estimates.
We conclude that the sensor fusion results are the most accurate of any of the tested
estimation methods, because whereas the photogrammetry-only estimators measure
spatial parameters and must estimate derivatives, and whereas the IMU-only estima-
tors measure derivatives and must estimate spatial parameters, the fusion estimator
uses both spatial and derivative measurements as the basis for estimation.

PERFORMANCE COMPARISON ANALYSIS
359
0
0.2
0.4
0.6
0.8
1
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6 X-forward acceleration
Time (s)
 
 
Estimate
Truth data
0
0.2
0.4
0.6
0.8
1
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
Y-right acceleration
Time (s)
 
 
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
2
Z-down acceleration
Time (s)
G’s
G’s
G’s
 
 
0
0.2
0.4
0.6
0.8
1
−8000
−6000
−4000
−2000
0
2000
4000
6000
8000
Body roll acceleration
Time (s)
deg/s2
 
 
0
0.2
0.4
0.6
0.8
1
−1
−0.5
0
0.5
1
1.5 x 104
Body pitch acceleration
Time (s)
deg/s2
 
 
0
0.2
0.4
0.6
0.8
1
−12000
−10000
−8000
−6000
−4000
−2000
0
2000
4000
6000
8000
Body yaw acceleration
Time (s)
deg/s2
 
 
FIGURE 20.4
Translational and rotational acceleration using the sensor fusion estimator.
The sensor fusion estimator’s behavior is apparent from Figure 20.5 by comparing
its trajectory with the photogrammety-only UKF, which is sampled at one-ﬁfth the
rate of the IMU data. Sensor fusion trends smoothly until at every ﬁfth sample it shifts
in one direction or the other when a photogrammetric observation arrives. It shifts in
the same direction as the photogrammetry-only UKF, but with less magnitude due to
the more accurate derivative estimates provided by the IMU data.
These preliminary conclusions will be tested in Section 20.6.2 where we present
the RMS error comparisons for the various estimators.
20.6.1
Filter Performance Comparison Methodology
In Section 19.6.1, we presented a performance methodology that used RMS error as a
metric. We will reuse that methodology here. The synthetic noiseless trajectory {xn,
n = 0, 1, 2, . . ., N} developed in Section 19.5 is reused for our analysis here, except
that now N = 1000 as described in Section 20.4.1. That trajectory is transformed into
a set of noiseless synthetic truth observations {hn (xn), n = 0, 1, 2, . . ., N}, where
hn (xn) can contain either photogrammetric or IMU measurements or both, based on

360
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
0.5
0.55
0.6
0.65
0.7
0.75
0.8
4.6
4.8
5
5.2
5.4
5.6
5.8
6
Time (s)
Inches
Photo NLLSQ
Photo UKF
IMU Pred−Corr
IMU UKF
Sensor fusion
Truth
FIGURE 20.5
Lateral position estimates from multiple estimation ﬁlters using identical
measurement inputs.
the ﬂag Fn. Then, NRMS = 100 sets of noisy observations {zo
n,r, n = 0, 1, 2, . . .,
N; r = 1, 2, . . ., NRMS} are produced by adding independent zero-mean Gaussian
noise to the synthetic truth observations to produce NRMS sets of noisy observations.
Each noisy observation set is used to exercise each tracking ﬁlter to produce NRMS
sets of state vector estimates {ˆxn|n,r, n = 0, 1, 2, . . ., N; r = 1, 2, . . ., NRMS}. We then
calculate the RMS error for each tracking ﬁlter as a function of time, producing e(ﬁlter)
p,n
and e(ﬁlter)
a,n
for 3D translational and angular position errors, e(ﬁlter)
˙p,n
and e(ﬁlter)
˙a,n
for 3D
translational and angular velocity errors, and e(ﬁlter)
¨p,n
and e(ﬁlter)
¨a,n
for 3D translational
and angular acceleration errors.
20.6.2
Filter Comparison Results
For each type of estimator, the RMS errors are computed as a function of time and
then the mean and maximum errors (over time) are generated for each case. These
mean and maximum values are summarized in Tables 20.1, 20.2, and 20.3, for the
position, velocity, and acceleration RMS errors, respectively.
Figures 20.6, 20.7, and 20.8 show time histories of the performance of each of the
ﬁve tested estimators for positional and rotational RMS errors, position and rotation
velocity RMS errors, and position and rotation acceleration errors, respectively. It is
obvious from Tables 20.1 through 20.3 and Figures 20.6 through 20.8 that the UKF(2)

CONCLUSIONS
361
TABLE 20.1
Synthetic Data RMS Positional Error Summary
Position (in.)
Orientation (deg)
Filter
Data
Type
Type
Mean
Max
Mean
Max
NLLSQ
Photo
0.68
0.81
0.22
0.26
UKF(2)
Photo
0.32
0.52
0.15
0.22
Pred/Corr
IMU
1.21
3.09
0.55
0.85
UKF(2)
IMU
1.50
5.15
1.69
2.10
UKF(2)
Both
0.16
0.42
0.06
0.22
TABLE 20.2
Synthetic Data RMS Velocity Error Summary
Translational
Angular
Velocity (ft/s)
Velocity (deg/s)
Filter
Data
Type
Type
Mean
Max
Mean
Max
NLLSQ
Photo
N/A
N/A
N/A
N/A
UKF(2)
Photo
0.71
1.63
12.34
22.18
Pred/Corr
IMU
0.30
0.48
8.66
9.75
UKF(2)
IMU
0.41
1.05
7.47
8.55
UKF(2)
Both
0.10
0.24
7.43
8.53
TABLE 20.3
Synthetic Data RMS Acceleration Error Summary
Translational
Angular
Acceleration (G’s)
Acceleration (deg/s2)
Filter
Data
Type
Type
Mean
Max
Mean
Max
NLLSQ
Photo
N/A
N/A
N/A
N/A
UKF(2)
Photo
0.37
1.70
653.3
3005.9
Pred/Corr
IMU
1.7
0.2
N/A
N/A
UKF(2)
IMU
0.16
0.24
4532.1
5106.0
UKF(2)
Both
0.15
0.89
4217.1
5119.2
estimator that uses data fused from both photogrammetric and IMU sensors has the
best performance overall, except in the case of rotational accelerations.
20.7
CONCLUSIONS
We have demonstrated that the sensor fusion estimator produces results that are more
accurate than either photogrammetry-based or IMU-based estimators alone. In the
context of our UKF solver, the two types of measurements complement each other.

362
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (s)
Inches
 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (s)
deg
 
 
Photo NLLSQ
Photo UKF
IMU Pred−Corr
IMU UKF
Sensor fusion
FIGURE 20.6
RMS errors for position and orientation of all tracking ﬁlters using synthetic
data.
IMU measurements mitigate the noise present in the photogrammetric data, thus mak-
ing positions and angles more accurate. And photogrammetry compensates for biases
and drift in the IMU measurements, making translational velocities more accurate.
20.8
FUTURE WORK
The current measurement model assumes that the IMU is located at the center of
the rigid body’s coordinate system (CG). To make the sensor fusion estimator more
practical, a lever-arm correction should be added to the measurement model that
compensates for the centripetal and tangential linear accelerations caused by rotations
of the accelerometers about the body’s CG. We also intend to show that using sensor
fusion to analyze photogrammetry and IMU data together improves fault tolerance,
in that dropouts in either data source are mitigated by measurements from the other
data source.
In addition to increased accuracy and fault tolerance, sensor fusion can potentially
improve photogrammetry turnaround time. Collecting 2D tracking data is the most
time-consuming step in the photogrammetric data reduction process. We intend to
investigate restructuring that process by using sensor fusion to reduce the quantity

FUTURE WORK
363
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Time (s)
ft/s
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
15
20
25
30
Time (s)
deg/s
Photo UKF
IMU Pred−Corr
IMU UKF
Sensor fusion
FIGURE 20.7
RMS errors for position and orientation velocities of all tracking ﬁlters using
synthetic data.
of image measurements that are necessary to produce a 6 DOF solution of sufﬁ-
cient accuracy. The UKF uncertainty model provides a measure of the accuracy of
a 6 DOF solution for a particular event. We can compare the uncertainties in the
sensor fusion result with the photogrammetry-only result, and determine how much
less photogrammetric data needs to be collected to obtain sufﬁcient accuracy.
Although the rate gyroscopes in the current generation of IMUs appear to be
very accurate, the accelerometers appear to exhibit a large enough calibration bias to
induce signiﬁcant error in the 6 DOF solution. We have begun to use sensor fusion to
estimate calibration biases in order to further improve accuracy. Also, we currently
assume that the forces the aircraft undergoes are constant during the brief period that
the store is near the aircraft. We have begun incorporating data from the aircraft’s INS
system to determine if correcting for changes in aircraft accelerations and rotation
rates cause a signiﬁcant difference in solution results.
We also intend to address the problem of latency in the IMU data. While the
imagery used in photogrammetry is time stamped as it is collected and recorded
aboard the aircraft, the IMU data is transmitted from the store to the ground station
and collected in a data buffer before being time stamped. These multiple data paths
cause a time synchronization error of indeterminate length. We have begun trying to
estimate the latency by examining the residual errors in the sensor fusion solution
when the latency is allowed to vary.

364
SENSOR FUSION USING PHOTOGRAMMETRIC AND INERTIAL MEASUREMENTS
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Time (s)
G’s
Photo UKF
IMU Pred−Corr
IMU UKF
Sensor fusion
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
1000
2000
3000
4000
5000
6000
Time (s)
deg/s2
FIGURE 20.8
RMS errors for position and orientation accelerations of all tracking ﬁlters
using synthetic data.
REFERENCES
1. Iu SL, Wohn K. Estimation of General Rigid Body Motion from a long Sequence of Images.
Technical Report, Department of Computer Science, University of Pennsylvania; 1990.
2. Lee JW, Kim MS, Kweon IS. A Kalman ﬁlter based visual tracking algorithm for an object
moving in 3D. IROS ’95 Proceedings International Conference on Intelligent Robots and
Systems, Vol. 1; 1995.
3. Gennery DB. Visual tracking of known three-dimensional objects. Int. J. Comput. Vision
1992;7(3): 243–270.
4. Huang TS, Netravali AN. Motion and structure from feature correspondences: a review.
Proc. IEEE 1994;82(2): 252–268.
5. Ude A. Filtering in a unit quaternion space for model-based object tracking. Robot. Au-
tonom. Syst. 1999:20; 163–172.
6. Halvorsen K, S¨oderstr¨om T, Stokes V, and Lanshammer H. Using an extended Kalman
ﬁlter for rigid body pose estimation. J. Biomech. Eng. 2005:1
7. 27; 475–483.
8. Algrain MC, Saniie J. Estimation of 3D Angular motion using gyroscopes and linear
accelerometers. IEEE Trans. Aero. Electron. Syst. 1991;27(6): 910–920.

REFERENCES
365
9. Marins JL, Yun X, Bachmann ER, McGhee RB, and Zyda MJ. An extended Kalman ﬁlter
for quaternion-based orientation estimation using MARG Sensors. In Proceedings of the
2001 International Conference on Intelligent Robots and Systems, 2001, pp. 2003–2011.
10. Yun X, Lizarraga M, Bachmann ER, McGhee RB. An improved quaternion-based ﬁlter
for real-time tracking of rigid body orientation. In Proceedings of the 2003 International
Conference on Intelligent Robots and Systems, 2003, pp. 1074–1079.
11. Sabatini AM. Quaternion-based extended Kalman ﬁlter for determining orientation by
inertial and magnetic sensing. IEEE Trans. Biomech. Eng. 2006;53(7): 1346–1356.
12. Yun X, Bachmann ER. Design, implementation, and experimental results of a
quaternion-based Kalman ﬁlter for human body motion Tracking. IEEE Trans. Robot.
2006;22(6):1216–1227.
13. Kelly J, Sukhatme GS. Fast relative pose calibration for visual and inertial sensors. In,
Proceedings of the 11th International Symposium on Experimental Robotics (ROBIO ’08),
2008.
14. Bleser G and Stricker D. Advanced tracking through efﬁcient image processing and visual-
inertial sensor fusion. Comput. Graphics 2009;33: 59–72.
15. Jeon S, Tomizuka M, Katou T. Kinematic Kalman ﬁlter (KKF) for robot end-effector
sensing. J. Dyn. Syst. Meas. Control 2009; 021010:8.
16. Moler C. Numerical Computing with Matlab, 2nd ed. SIAM; 2004.
17. Van Loan CF. Introduction to Scientiﬁc Computing, 2nd ed. Prentice Hall; 2000.

INDEX
afﬁne transformation, 35, 80
auxiliary particle ﬁlter
derivation, 238
process, 241
angle-axis rotations, see rigid body rotation
representations
Bayesian estimation, 43, 46
Bayes’ rule, 31
bootstrap particle ﬁlter
derivation, 230
process, 231
Cartesian-to-spherical transformation, 302
Chapman-Kolmogorov equation, 48
constant Cartesian velocity dynamic model,
261
constant spherical velocity dynamic model,
266
constant turn-rate simulation, 294
coordinate transformations in
multidimensional space, 301
Cramer-Rao lower bound
application to DIFAR tracking, 192
Gaussian additive noise, 190
general derivation, 184
linear models, 191
recursive, 186
zero process noise, 191
cumulative distribution function, 30, 34
density estimation
histogram, 202
kernel density, 204
recursive Bayesian estimation, 46, 201
DIFAR
dynamic model, 58
likelihood function, 67
observation model, 59
scenario simulator, 58, 65
signal processing, 62
single buoy bearing track estimation,
linear Kalman ﬁlter, 88
track estimation results
auxiliary particle ﬁlter, 242
bootstrap particle ﬁlter, 231
extended Kalman ﬁlter, 109
Gauss-Hermite Kalman ﬁlter, 163
generalized Monte Carlo particle ﬁlter,
252
spherical simplex Kalman ﬁlter, 147
unscented Kalman ﬁlter, 137
error ellipses, 176
extended Kalman ﬁlter
alternate derivation of covariance
prediction, 107
constant spherical velocity state
prediction, 271
linearized, 105
observation prediction, 96, 102
Bayesian Estimation and Tracking: A Practical Guide, Anton J. Haug.
© 2012 John Wiley & Sons, Inc. Published 2012 by John Wiley & Sons, Inc.
367

368
INDEX
extended Kalman ﬁlter (Continued)
process block diagram, 169
radar observation prediction, 264
second order, 105
state prediction, 94, 99
transformation of predication equations,
96, 103
extended Kalman particle ﬁlter, 243
ﬁnite difference Kalman ﬁlter
alternate derivation of covariance
prediction, 125
observation prediction, 118, 124
process, 119, 126
process block diagram, 170
simpliﬁed, 118
state prediction, 116, 120
Gauss-Hermite Kalman ﬁlter
multidimensional derivation, 155
observation prediction, 155, 160
one-dimensional derivation, 148
process, 161
process block diagram, 170
sparse-grid approximation, 160
state prediction, 154, 160
Gaussian cumulative density function,
34
Gaussian ﬁlter, general, 80
Gaussian moments, 38
Gaussian moment integrals, 35
Gaussian prediction equations, 78
Gaussian probability density function,
32
Gaussian weighted integrals, numerical
methodology, 82, 128
generalized Monte Carlo particle ﬁlters
Gaussian particle ﬁlter, 248
Gaussian particle ﬁlter block diagram, 249
combination particle ﬁlter, 250
combination particle ﬁlter block diagram,
250
sigma point combination particle ﬁlter
block diagram, 251
tracking a rigid body, 325
generating Gaussian samples, 40
Hermite polynomial approximation, 28, 149
hierarchy of Bayesian Estimators, 5
importance sampling, 210
inertial measurement unit observation
model, 348
initialization for rigid body tracking, 326
Initialization using a predictor-corrector
method, 356
joint probability density, 30
Kalman ﬁlter summary, 168
Kalman ﬁlter update equations, 51, 76
linear Kalman ﬁlter
linear models, 86
process, 89
process block diagram, 169
state prediction for constant Cartesian
velocity, 264
state prediction for constant spherical
rates, 271
matrix operations
block matrix inversion, 14
conventions and notations, 11
matrix inversion, 13
matrix square root, 15
sums and products, 12
marginal probability density, 31
Monte Carlo Kalman ﬁlter
derivation, 164
observation prediction, 165
process, 166
state prediction, 165
multinomial cubature integration rules, 128
nonlinear least squares
general estimation, 321
initialization for rigid body tracking,
326
observation prediction, 53, 75, 79
optimal particle ﬁlter
derivation, 233
Gaussian, 235
locally linearized, 236
process, 237
photogrammetric (video) observation model,
318

INDEX
369
photogrammetric rigid body tracking
performance, 334
point estimators, 43
polynomial approximation, general, 19,
23
predictor-corrector initial value problem
solver, 354
probability density function, 30, 32
quaternion, see rigid body rotation
representations
radar spherical observation model, 263
Rao-Blackwellization, 245
resampling, 222
recursive estimation of moments, 49, 54
resampling, 222
regularization (move step), 227
rigid body rotation representations, 342
rigid falling body dynamics
combined translation and rotation motion,
316
dynamic motion model, 317
rotational motion, 313
translational motion, 311
root mean squared error, 182
sensor fusion, 346
sensor fusion observation model, 351
sensor fusion rigid body tracking
performance, 357
sequential importance sampling
general process, 221
general theory, 218
sequential importance sampling particle
ﬁlter, 226
sigma point Kalman ﬁlters, 170
simplex polynomial approximation, 29
spherical simplex Kalman ﬁlter
process block diagram, 170
derivation, 140
process, 146
spherical-to-Cartesian transformation, 305
state prediction, 49, 74, 79
Sterling’s polynomial approximation, 21, 27
synthetic rigid body
synthetic trajectory generator, 328, 352
photogrammetric synthetic
measurements, 333
inertial measurement unit observation
measurements, 352
Taylor polynomial approximation, 20 ,24
tracking methodology, 57
tracking in three dimensions
ﬁlter implementation issues, 273
ﬁlter initialization, 276
tracking performance, 278
unscented Kalman ﬁlter
alternate version, 135
background, 130
constant spherical velocity state
prediction, 271
derivation, 131
observation prediction, 135
process, 136
process block diagram, 170
radar observation prediction, 264
state prediction, 134
tracking a rigid body, 323
unscented particle ﬁlter
derivation, 243
process, 244
vector point generators, 16, 173

