The Quantum House Of Cards
Xavier Waintal1
1Universit´e Grenoble Alpes, PHELIQS, CEA, Grenoble INP, IRIG, Grenoble 38000, France∗
Quantum computers have been proposed to solve a number of important problems such as discov-
ering new drugs, new catalysts for fertilizer production, breaking encryption protocols, optimizing
financial portfolios, or implementing new artificial intelligence applications. Yet, to date, a sim-
ple task such as multiplying 3 by 5 is beyond existing quantum hardware. This article examines
the difficulties that would need to be solved for quantum computers to live up to their promises.
I discuss the whole stack of technologies that has been envisioned to build a quantum computer
from the top layers (the actual algorithms and associated applications) down to the very bottom
ones (the quantum hardware, its control electronics, cryogeny, etc.) while not forgetting the crucial
intermediate layer of quantum error correction.
Introduction
I am very skeptical that a quantum computer will ever
solve serious problems. When I express these doubts to
colleagues, the answer I mostly get is a variation along
the line of the following: “You’re right, it looks difficult.
But when the first transistor was built, one could never
have foreseen the computer revolution, internet, smart-
phones, AI... Quantum bits might be the new transistor.”
This is a very compelling argument. In fact, some qubits
are actually made of transistors [1]. Such analogies can
be very striking, yet they can also be very deceptive as
we are naturally inclined to use the one that fits our pur-
pose best. Other analogies compare quantum computing
with the early days of aviation or the qubit with the brick
with which we eventually built skyscrapers. I find that
these analogies miss a crucial difference between quan-
tum computing and earlier technologies: the brick, the
airplane, and the +1 Volt/ -1 Volt state of the transistor
are all intrinsically robust. Qubits are much closer to a
deck of cards than to a brick. Could we have built our
skyscrapers with cards? Probably not, the house of cards
technology lacks robustness. Quantum technologies are
also, almost by definition, the opposite of robust; they
rely on subtle, volatile, transient physical effects. They
intend to prevent a macroscopic object from behaving
classically. Every single player in the game, be it a vi-
bration, an electromagnetic mode, or a nearby charge
wants the quantum computer to behave like a classical
object; this is decoherence [2]. Trying to build a quan-
tum computer means picking up a fight with the second
law of thermodynamics. The entropy of the system can-
not be allowed to increase or the quantum state is simply
gone.
The goal of this article is to dispel a few myths around
quantum computers, and coldly examine what it would
take to build one. I will point out the different challenges
that lie on the path to building a quantum computer, the
leading one being the exponential decay of the fidelity
∗xavier.waintal@cea.fr
FIG. 1. A quantum computer internal state is a macroscopic
quantum state described by an exponentially large set of com-
plex numbers.
Such states are subject to decoherence and
very fragile.
due to decoherence. I will also discuss the envisioned ap-
plications of quantum computing and the extraordinary
constraints that they put on quantum hardware. As far
as I am aware, the statements written below are not par-
ticularly controversial, at least to the scientific experts in
the field. Depending on the reader’s background, some
of these statements might be obvious, some of them new.
Taken together, they provide a global picture that should
help one to make an informed opinion on the main ques-
tion of the field: Will it work?
As advertized in the first sentence, the author of this
review is very skeptical about the prospects of quantum
computing.
It should be stressed, however, that this
skepticism does not extend to quantum physics in gen-
eral or to other aspects of quantum technologies. The
revolution going on in experimental quantum physics is
genuine. What is unclear, in my opinion, is the possibility
to use this progress to perform new types of computation.
The fundamental law of analog machines
At the root of this analysis is the simple observation
that a quantum computer is an analog machine [3], i.e.
its internal state is described by a large set of complex
numbers that can vary continuously. This is in contrast
to digital machines, like our desktop computers, whose
internal states are described by bits that can take two
(macroscopically distinct) values: zero and one. Micro-
electronics experts know very well that it is very difficult
arXiv:2312.17570v1  [quant-ph]  29 Dec 2023

2
to perform long calculations with analog machines be-
cause every operation is executed with a finite precision.
The same limitation applies to operations on quantum
bits: For instance, we might want to rotate one by 40
degrees around the z-axis, but instead end up doing a
40.1 degree rotation around a slightly tilted axis. These
errors accumulate, and eventually the internal state of
the quantum computer becomes blurred.
We measure the global precision that we have achieved
with a quantity called the fidelity F [4]. One can think
of the fidelity as the probability for the result of the full
calculation to be correct. The fidelity decreases exponen-
tially as,
F ≈e−ϵ0N0−ϵ1N1−ϵ2N2−...
(1)
where ϵ1 is the error per one qubit operation and N1 the
number of such operations. Similarly, ϵ2 and N2 refer
to the error level for operations that couple two qubits.
There exist also other error channels (e.g. measurement
errors, leakage errors, etc.) not discussed here. Interest-
ingly, the phenomena of decoherence occur even when we
do nothing, so that we have to introduce the error level
ϵ0 for the N0 idle times as well. Physically, ϵ0N0 is essen-
tially given by ϵ0N0 ≈nT/T2 where n is the number of
qubits, T the duration of the calculation and T2 the phase
coherence time of the quantum hardware. Since T scales
at least as n for non-trivial calculations (much faster in
the case of e.g. Grover algorithm), it follows that the fi-
delity decreases at least as fast as F ∼exp(−an2) (a is a
constant). The error levels ϵ1 and ϵ2 include, in addition
to decoherence, the errors due to the act of manipulating
the qubits such as the finite precision of the amplitude
and duration of the microwave pulses or the effect of e.g.
crosstalks. There is a tremendous level of evidence, both
theoretical and experimental, for the validity of (1); it
is actually used to benchmark the quality of the qubits.
Perhaps the most compelling validation so far of (1) is the
seminal Google experiment of “quantum supremacy” [5].
Indeed, if one sets aside the supremacy claim, what this
experiment really does is establishing that (1) actually
holds for a rather large system of n = 53 quantum bits.
Typical values shown by the best existing hardware are
around ϵ1 ≈0.0001 and ϵ2 ≈0.002. The numbers N0,
N1, N2 depend on the type of application, and on the
size of the problem, i.e. the number of quantum bits n
that will have to be used. For a given problem, one may
use (1) to evaluate the probability of success on given
hardware.
The fruits are few and not hanging low
There is a small paradox attached to quantum com-
puting with respect to applications. The way it has been
presented to the public, there exists a large number of
them, with dazzling implications. It has been claimed
that quantum computers would help with food produc-
tion by designing new catalyzers for fertilizer produc-
tion; they would allow one to design new drugs or chem-
icals; they would play a major role in (quantum) ma-
chine learning; they would address cryptographic secu-
rity problems. . . On the other hand, at the moment, only
a handful of separate quantum algorithms exist. In fact,
all the above mentionned applications use variations on
these few algorithms. This is in sharp contrast to classi-
cal computing where it is relatively easy to design a new
algorithm and there exists an almost unlimited number
of them. The contrast remains even after taking into ac-
count the relative size and age of the two fields: there is
no simple path to write a new quantum algorithm. Also,
as we shall see, all known algorithms are very demanding
in terms of hardware quality and quantity.
The most famous one is Shor’s algorithm that solves
the factoring problem [6]. Given a large integer c = ab
of n bits, it calculates the two factors a and b. Its im-
portance stems from the fact that, because this problem
was believed to be exponentially hard, it is at the root of
the RSA encryption system that is used throughout the
internet. So someone in possession of a quantum com-
puter could eavesdrop on internet communications that
are supposed to be secure. Because this threat has been
taken seriously, new encryption protocols, proven to be
quantum resilient, will soon be deployed to replace RSA.
Considering only two-qubit gate error, N2 ≈10n3 for
Shor’s algorithm. Breaking RSA for realistic key sizes
would imply factoring numbers represented by n = 2048
bits. From F ≈e−ϵ210n3, we obtain that one would need
ϵ2 ≤10−11 for the algorithm to succeed with a reasonably
high probability. This would require an improvement by
a factor of one billion with respect to the current exper-
imental state of the art. In the particular case of Shor’s
algorithm, the effect of noise can be put onto firm math-
ematical grounds [7].
Second after Shor comes Grover. Grover’s algorithm
[8] provides a more modest speed-up than Shor’s, at most
quadratic from 2n down to
√
2n operations. It is however
widely popular because in contrast to Shor’s algorithm,
which is highly specialized, Grover can be applied to a
wide variety of problems including the difficult “NP com-
plete” class. Its applicability is however questionable [9]
since it requires an exponentially large number of oper-
ations to complete, hence the fidelity decays as the ex-
ponential of an exponential: F ≈e−ϵ2n
√
2n. Solving a
n = 100 instance (where it is expected to become faster
than its classical analogue in the most favorable situa-
tion) would require ϵ2 ≤10−17 not to mention an unin-
terrupted computation of thousands of years [9]. Using
existing technologies, it is actually not possible to apply
Grover reliably on just 5 qubits. A recent attempt that
used advanced noise mitigation techniques (not applica-
ble to large systems) obtained the correct answer with
just 15% probability [10]. Beyond the difficulty to run
Grover’s algorithm in practice, a widely overlooked as-
pect is that a quantum advantage can only be obtained
for the most unstructured problems. Real life problems
almost always possess a lot of structure that classical al-

3
gorithms take advantage of for dramatic speed-up [4, 9].
Many proposals for quantum advantage are rather naive
in that respect.
The third class of algorithm does not provide a quan-
tum speed-up in theory, but the hope is that it might
provide one in practice.
Among the possible applica-
tions, one that has attracted a very large interest is the
possibility to construct variational ansatz to solve prob-
lems arising in quantum chemistry, e.g.
calculate the
activation energy for a chemical reaction and the influ-
ence of potential catalysts. The gate count N2 is directly
linked to the expressivity of the ansatz; in order to reach
a sufficient (so called “chemical”) accuracy, one needs
to include at least excitations with three electron-holes
[11] (which translates into N2 ∼n6) and a large fidelity
F ≥0.999. For a n = 30 electron problem accessible to
most classical techniques, one arrives at ϵ2 ≤10−12 [12].
The above examples are actually very generic: to be
useful, a quantum computer must be able to employ
rather deep quantum circuits. This statement can actu-
ally be put on firm grounds since shallow circuits can be
simulated on classical computers very efficiently [4, 13].
In return, this means that to be useful a quantum com-
puter must present very low error levels to keep the suc-
cess probability of the computation close to one. To most
physicists the numbers above speak by themselves; it
is not realistic to imagine this level of precision to be
reached. Suppose one is playing darts with a 1m wide
target. Putting several darts within a circle of 5 cm of
diameter is accessible with a little work.
Putting the
darts inside the 2 mm center requires a lot of impressive
work (this is where we are now, ϵ2 = 0.002). For the
above quantum algorithms to work, one needs to put all
the darts within a 10−11 m wide target.
All attempts to use quantum hardware for applications
have been so far restricted to very small problem sizes
in order to evade the precision issue. For instance, the
quantum circuit to perform 3×5 is still beyond the reach
of existing chips and so is the quantum circuit to fac-
torize 15.
Indeed, in the seminal article where such a
factorization has been advertized, the classical logic was
pre-calculated on a classical computer in order to main-
tain the gate count at a low enough level [14].
Also,
the above estimates assume perfect connectivity between
all qubits. In practice, this is never achieved except for
small systems and one should therefore expect additional
overheads due to the need of shuffling qubits around.
Finding applications where quantum computers could
have a genuine advantage is difficult. However, there is a
simple criterion that allows one to state what they won’t
be able to do [15]. Under very optimistic resource usage
assumptions, it takes a quantum computer hours or days
to arrive at a solution that would be challenging for a
classical computer [16, 17]. Since this solution encom-
passes only a few tens of bits, it means that the informa-
tion throughput that such a quantum computer would
deliver is a few tens of bits per hour which translates
into less than a milli-byte per second, i.e. twelve orders of
magnitude less than what any laptop routinely provides
(gigabytes per seconds). In other words, quantum com-
puters deliver very few bits of information. These might
be very useful bits but still they will be very few, so quan-
tum computers will always be restricted to highly specific
applications; they will not replace classical computers.
For instance a quantum computer could in principle be
used to calculate the binding energy of a molecule (this
can be stored in few bits) but will not be able to generate
an image (this would require millions of logical qubits).
Conversely, applications that require large inputs to be
given to quantum computers, for instance an image for
a quantum machine learning application, will require at
least as large a gate count to load this information into
the quantum hardware hence automatically require ultra
low noise levels to keep the fidelity high enough.
The duality of good coupling vs. low decoherence
On the physics side, every field of quantum physics has
proposed one or more platforms on which one could try to
realize a quantum computer. These different approaches
can roughly be put on a scale from the ones best coupled
to their environment (and hence can be manipulated effi-
ciently but also suffer more from decoherence) to the least
coupled ones (showcasing excellent coherence times but
slow to manipulate). It is difficult to make an exhaustive
list of what is being tried but one should keep in mind
that optimizing one aspect is often done at the expense
of another one and that the advantages of one approach
often end up in being also a disadvantage. For instance,
semiconducting approaches based e.g. on the spin of a
single electron can provide very small qubits, hence the
possibility to scale up to large systems. They also benefit
from the existence of proven fabrication technology. Yet
their smallness also means that they are more sensitive
to defects at the atomic scale, and therefore display high
variability. For instance, a good CMOS technology has
around 1011cm−1 charges trapped at the silicon-oxyde
interface of a transistor. This translates into variations
of electric potential of several meV, not much smaller
than the potential induced by the controlling electrostatic
gates. Transistors that would be considered as nominally
identical for microelectronics end up showing very large
variations when used for qubits [18]. So far, only very
limited quantum states have been demonstrated with this
approach. Atomic physics based trapped ions have very
small variability (all atoms are identical, only their en-
vironment may fluctuate) and long coherence. Yet they
are several orders of magnitude slower than their semi-
conducting counterparts which almost disqualifies them
for a full scale implementation of quantum error correc-
tion (see below). Also, it is not clear yet how to scale up
these systems succesfully. Photons have extremely long
coherence times. Yet, as they do not interact directly, the
two-qubit gates are obtained by measuring auxiliary pho-
tons (absorbed in a photodetector) and are intrinsically

4
probabilistic. This means that one has, say, a 1/2 prob-
ability of applying the gate and 1/2 probability of doing
something else. The probability to apply the intended
gate sequence decreases exponentially as one increases
the gate count, which is very bad. Implementing an al-
most deterministic noisy two-qubit gate between photons
requires using a large number of auxiliary photons and a
highly complex set of interferometers and photodetectors
just to catch up with what other platforms do naturally.
Besides, while important progress has been made in solid
state photon sources, their yield and indistinguishability
are at the ϵ ∼0.1 level, still far from what would be
needed. Other platforms, such as Rydberg atoms or elec-
tronic flying qubits [19] are emerging at various positions
on the “coupling to the environment” spectrum. As of
now, superconducting circuits seem to be a good compro-
mise with a state of the art at around a hundred qubits
and ϵ2 ≈0.001. Notice that with this level of error, only
a fraction of the qubits can be used reliably if one wants
to retain a reasonably high fidelity.
Each field of quantum physics offers a rich variety
of different sub-possibilities.
For instance, within su-
perconducting circuits [20], early designs included the
quantronium after which came the transmon (now the de
facto standard) but other designs include the fluxonium
(with state of the art characteristics), the gatemon or the
bosonic qubits (that aim at implementing, at least par-
tially, quantum error correction at the hardware level).
Extrapolating the pace at which progress is made is
not straightforward. Progress is often linked to the ad-
vent of disruptive ideas (e.g. the Cooper pair box, using
a resonator as a harmonic oscillator, the transmon); the
optimization part has been greatly accelerated by the
large increase of resources devoted to quantum comput-
ing in the last few years with some research teams grow-
ing from a few persons to almost a thousand. The first
demonstration of a “qubit” (i.e. coherent Rabi oscilla-
tions) in a quantum circuit was made in 1999. The first
two-qubit gate was made in 2009 with ϵ2 ≈0.1 which
was later improved to ϵ2 ≈0.01 and reached ϵ2 ≈0.001
in 2020 (see Table I of [20] for references and details).
At the time of this writing, IBM has announced that its
chip can sustain the ϵ2 ≈0.001 level with more than a
hundred qubits [21].
The “salvation is beyond the threshold” myth
The contrast between the fidelities displayed by the
quantum hardware and the requirements of the algo-
rithms raises concerns about feasibility.
The theoreti-
cal solution to this problem is an elegant mathematical
construction, the quantum error correction (QEC). In
QEC, one performs two things: first, one encodes a logical
qubit into nc > 1 physical ones so that no simple phys-
ical process would directly couple the logical zero with
the logical one. Second, one repeatedly performs partial
quantum measurements that leave the logical qubit un-
changed but force the system to stay in the intended part
of the (now enlarged) Hilbert space [22]. A widely spread
idea, backed up by some strong mathematical theorems,
is that once the error levels fall below a certain threshold
ϵth (that happens to be of the same order of magnitude
as the current error level, ϵth ≈0.01 for the best QEC
codes), it becomes sufficient to increase nc to obtain an
exponential decrease of the error level ϵL of the logical
qubit. QEC is often summarized with statements of the
form “We can anticipate that analog quantum simula-
tors will eventually become obsolete. Because they are
hard to control, they will be surpassed some day by dig-
ital quantum simulators, which can be firmly controlled
using quantum error correction.”[23] or “so, once the de-
sired threshold is attainable, decoherence will not be an
obstacle to scalable quantum computation.”[24] In short,
QEC would turn an analog machine into an essentially
digital one. However, QEC is itself entailed with consid-
erable difficulties that I now detail.
The first difficulty is that QEC is associated with im-
portant overheads in terms of number of qubits and num-
ber of operations. I refer to this as the Russian doll nest-
ing problem. This name reflects the fact that manipu-
lating logical qubits involves several levels of nestedness:
operations within operations within operations. Suppose
that one wants to perform a simple rotation of one logi-
cal qubit by an angle 2π/64 around the z-axis (a typical
rotation occuring e.g. in the quantum Fourier transform,
a key component of Shor’s algorithm). To perform this
rotation on a physical qubit is deceptively easy: one sim-
ply needs to wait for the time it takes for the qubit to do
this precession naturally. However, no such simple path
can exist within QEC since, by construction, the logi-
cal zero and one are widely different states and since we
want this rotation to be performed with arbitrary preci-
sion. Performing this simple rotation in what some have
called a “digital way” (that is with an arbitrary high
control on its precision), one decomposes it into several
nested levels of operations. At the first level of the Rus-
sian doll, the rotation is decomposed into a product of a
few ultra-precise elementary operations. This is similar
to approximating a real number with a fraction of two
integers, the more precise one wants to be, the larger
the integers or, in our case, the larger the number of
elementary operations. Some of these elementary opera-
tions will be relatively straightforward to implement, for
instance they could amount to applying the same opera-
tion to each of the nc qubits. At least one of them will be
hard however. In the most studied QEC code (also one of
the most robust), the so-called surface code [25], the hard
gate is the π/4 rotation along z, called T. In a second
level, one needs to fabricate special π/4 states very pre-
cisely. This is done through a process called “magic state
distillation” that is planned to occur in “T gate facto-
ries”. Magic state distillation is itself a sort of QEC code
that possesses a fixed point that is precisely the intended
state. Since it is done with logical states it is therefore a
sort of QEC of QEC code. One or more iterations of the

5
distillation are necessary before the required accuracy is
achieved. To perform the distillation, one must entangle
different logical qubits with each other. This is achieved
within the third nested level of the Russian doll using a
process known as “braiding” where the first logical qubit
makes a complete loop around the second logical qubit.
This loop is itself discretized in the fourth level of the doll
where each discrete move of the qubit is intertwined with
“syndrome measurements”: a small quantum circuit ran
on the entire chip at every clock cycle to perform the
partial measurements mentioned above. Needless to say,
the Russian doll nesting problem transforms the initially
simple process of a one qubit gate into thousands, tens
of thousands or more, physical operations depending on
the required level of precision. It is interesting to note
that T seems to be a necessary evil. Indeed, it can be
shown that quantum circuits that do not have the T gates
can be simulated easily, in polynomial time on a classical
computer [26]. The Russian doll bottleneck excludes all
but the fastest hardware for practical applications. Even
assuming gates as fast as 10ns, the overall computational
time found for a full application often reaches days for a
single answer to a single problem. It also imposes strin-
gent constraints on the scalability as millions of qubits
(billions once one includes the T gates factories) will be
required [16].
The second difficulty I call the “syndrome bottleneck”.
At every single clock cycle (say every µs for state of the
art superconducting hardware), quantum error correc-
tion requires partial measurements of the physical qubits
in order to stabilize the logical qubits.
The results of
these measurements are the “syndromes” that need to
be tracked and analyzed in real time in order to recon-
struct what type of error may have occured and modify
the rest of the circuit accordingly. There are (almost)
as many syndromes as there are qubits.
A single er-
ror in the analysis of the syndrome jeopardizes the en-
tire calculation. And the analysis of the syndrome is a
difficult (NP-complete) problem. For a billion physical
qubits quantum computer, the data rate coming from
the syndromes is of the order of 1015 bits per second
(= 109/10−6s). Even assuming that very few operations
per bit are needed for the analysis, this means that a
petaflop of computing power is required solely for the
purpose of syndrome analysis.
This is the computing
power of a full scale supercomputer.
Even more wor-
ringly, there is a need to extract the 1015 bits per second
from the quantum chip, i.e. the equivalent of what goes
through one million gigabit ethernet cables.
The last difficulty arises from the fact that not all er-
rors have the same status. Indeed, a QEC code corrects
certain types of errors. Others are not corrected and are
fatal to the calculations. In the context of the surface
code, the logical error takes the form
ϵL ∼
 ϵ2
ϵth
√nc
+ ϵncnc
(2)
where ϵnc is the error level for non-correctable errors [27].
Hence increasing nc will only improve the logical error
until the non-correctable errors start to dominate. For
each set of errors, one can probably build a code that cor-
rects them all (but the more errors, the more complex the
code and the worse the threshold). However, for a given
code there will always be non-correctable errors. In QEC
terminology, such errors are referred as correlated errors.
However they are correlated only from a QEC point of
view. From a physics standpoint they are just regular
physical processes (for instance, a global fluctuation of
magnetic field due to fluctuations of the current in the
coil qualifies). Some of these errors have already been
identified. Examples include the leakage of a qubit out-
side of the intended computational space, a nearby two-
level system that creates a memory effect [28] or, more
dramatically, a collision with a cosmic ray that appears
to impair superconducting chips [29]. Fortunately in the
latter case, it seems that burying the quantum computer
under 1.4km of non-radioactive stone alleviates the prob-
lem [30]. The bottom line is that if and when one starts
to use QEC, one will only be able to obtain some gain in
precision, not an arbitrary gain. After that, one will be
dominated by a non-correctable error, hidden behind a
large flow of more mundane ones. One will need to find
a way to isolate and then address this error (most prob-
ably at a cost for the threshold) and only then one will
be able to increase nc again, until one will be hit by the
next type of error. Given the precision targeted in the
applications (ϵL ≤10−15 once one takes into account the
QEC overhead), it is not clear how far one will be able
to go in practice.
Recent attempts [31, 32] to construct a surface code
with superconducting qubits have shown a mitigated suc-
cess. Up to nc = 25 has been considered. However, so far
only the fourth level of the Russian doll nesting problem
has been implemented, so that the logical qubit could
not be manipulated. Also the logical qubit is still not as
good as the original physical qubits from which it is con-
structed.
A real-time treatment of the syndromes has
not been attempted so that syndrome analysis is done
post mortem. This is fine in a proof of principle exper-
iment when one can post-select some experiments, but
not usable for a real quantum computation. Last, in one
of these experiments [31], significant leakage occurs so
that non-correctable errors can already not be ignored.
The most complete attempt to implement all the stages
of the Russian doll probably belongs to the trapped ion
platform [33] using one of the simplest QEC codes (the
nc = 7 Steane code). One logical T gate was applied on
one logical qubit. However the magic state distillation
stage was bypassed, the code was still vulnerable to a
single qubit error on the ancilla qubit, only a (partial)
single cycle of syndroms was measured, and it was an-
alyzed post mortem through postselection.
The result
was a logical error ϵL > 0.1, i.e. almost hundred time
worse than the original single qubit error of the physical
qubit. The best QEC gain so far was demonstrated for
a single bosonic qubit in a 3D cavity where a gain of up

6
to a factor two in the qubit error level was claimed with
a logical ϵ0 ≈0.002 per QEC cycle [34, 35].
The scaling up engineering challenge
Constructing a genuine quantum computer, as it has
been envisioned, is often described using the euphemism
“challenging”. The description above was meant to con-
vey a more precise idea of the difficulties involved. To the
fundamental problems outlined above, I will now add a
few more mundane, almost trivial, ones.
Scaling up quantum technologies is an engineering
problem that has its own set of exponential decays and
other difficulties. Once again, optimizing one parame-
ter is done at the expense of others, often the number
of sustainable qubits in the architecture.
For instance
frequency crowding (several qubits whose frequencies lie
within the bandwidth of each other) leads to correlated
crosstalks (acting on one qubit triggers unwanted dynam-
ics on other qubits). The optimization that work for a
handful of qubits will not work any more once the number
of qubits has been scaled up. For instance, after increas-
ing n, one won’t be able to spread out their frequencies
anymore and cross-talk will become more problematic.
Another (exponential) problem is the yield of the fabri-
cation. If a qubit has a probability p to function nomi-
nally, then the probability that all ntot = n nc qubits in
the chip work is pntot. The value of p must be very close
to unity for this yield to be non-vanishing. Currently, it
is rather common that several samples need to be fabri-
cated in order for one to work correctly, so we see that
this simple law imposes a considerable constraint on the
reproductibility of the fabrication. This will be particu-
larly important for solid state devices. Another crucial
related problem is variability: some nominally identical
samples may have slightly different microscopic environ-
ments that affect the qubit behavior or its coupling with
others (such as an ill-positioned two-level system). This
is particularly problematic if this variability is not static:
any drift in the system will lead to a loss in precision.
In some technologies, the mere size of the chip might be-
come an issue. For instance superconducting transmons
are based on resonators that currently take up a surface
of the order of 1 mm2. Scaling up to a billion of them
would require cooling down the surface of a one bedroom
appartment to 10mK, which will be challenging. Besides
the surface problem, existing dilution fridges offer typi-
cally 500µW of cooling power at 100mK (∼1W is avail-
able at 4K) for an electricity consumption of ∼10kW.
This is sufficient for existing chips but would not survive
the implementation of QEC. Another pressing problem
is the control and addressing of all the qubits and the
associated potential wiring/fan out bottleneck. The fan
out problem is actually deeper than it seems. Indeed, the
number of control lines directly controls the amount of
calibration that is made possible to mitigate a parasitic
coupling or simply precisely calibrate a gate. One needs
as many knobs as there are parameters to tune.
Any
attempt to compromise there will result in poorer noise
level.
Conclusion
I have tried to convey the idea that, perhaps, quantum
computing as it has been envisioned so far is simply too
difficult to happen. However, it remains that there is a
genuine revolution that is going on in quantum sciences.
We are exploring frontiers that were thought impossible
only a few decades ago. Perhaps this is one of those cases
where the journey is more important than the intended
destination and that it is simply too early to foresee the
applications. After all, if we could not foresee the internet
when we invented the transistor, perhaps we’re not better
off now?
There are some possible applications that I purposely
did not include in this discussion. In particular, I did
not address the so-called quantum simulations, adiabatic
transitions or other analog systems. Indeed, an impor-
tant parameter of a quantum technology is its level of
“universality”. On one hand are systems which are fully
universal; these are the systems that I described above
and that would truly deserve the name of computers. At
another extreme are systems that essentially “simulate
themselves”, albeit with some level of control onto the
dynamics. Most of the difficulties outlined for universal
(gate based) quantum computers do not apply there, or
at least not as stringently. As we gain control of these
systems and manage to implement dynamics that are of
natural interest, they may provide us with important in-
formation about other less controllable systems such as
new materials or important models such as the Hubbard
model. This is perhaps a niche application but it might
be a very important and fruitful one.
Also, it strikes me that some of the key applications
that have been put forward for quantum computing (e.g.
for solving chemistry problems or correlated matter) are
slowly coming into the scope of classical methods. The
simulation of the seminal quantum supremacy experi-
ment, initially assumed by Google to take 10,000 years
on the largest classical supercomputer [5], now requires
just 6 seconds according to their own new reckoning [36]
(an improvement of almost eleven orders of magnitude
on the classical algorithm side). A similar fate befell the
recent IBM quantum advantage experiment [21] which
was simulated mere days after the article publication [37–
39]. Indeed, a common misconception is that the many-
body dynamics of a quantum computer is necessarily ex-
ponentially difficult to calculate on classical hardware.
In reality it is at most exponentially hard.
There are
almost always hidden structures that scientists learn to
exploit [4] to speed up the calculations.
The number
of problems that were apparently exponentially difficult
and that were solved in polynomial time with numerical
techniques is quickly growing. A seminal example is the

7
Kondo problem, solved with the Numerical Renormal-
ization Group (NRG) for which Wilson was attributed
the Nobel prize. In the last few years, cornerstone mod-
els such as the Hubbard model have started to yield to
the convergent attack of Density Matrix Renormalization
Group (DMRG) [40], Quantum Monte-Carlo (QMC) [40],
Diagrammatic Monte-Carlo [41], Neural Network based
variational ansatz [42], Projective Entangled Pair States
(PEPS) [43] and more.
There are also new types of
quantum inspired algorithms that are being developped.
For instance, the quantum Fourier transform at the core
of Shor’s algorithm can be turned into a classical al-
gorithm that performs Fourier transforms exponentially
faster than the usual FFT [44]. It might very well be that
some of the promises made for quantum computers will
actually be kept perfectly by classical algorithms running
on classical hardware. One could even argue that the real
goal of physics is not to calculate this or that number but
to unveil those hidden structures.
Acknowledgement
I am grateful to Miles Stoudenmire and Michele Fil-
ippone who provided numerous insightful feedbacks on
this manuscript. Warm thanks to Paola Waintal Saldi
and Christoph Groth for proof reading this article.
[1] N. Piot, B. Brun, V. Schmitt, S. Zihlmann, V. P. Michal,
A. Apra, J. C. Abadillo-Uriel, X. Jehl, B. Bertrand,
H. Niebojewski, L. Hutin, M. Vinet, M. Urdampilleta,
T. Meunier, Y.-M. Niquet, R. Maurand, and S. D.
Franceschi, A single hole spin with enhanced coherence in
natural silicon, Nature Nanotechnology 17, 1072 (2022).
[2] J. Raimond and S. Haroche, Quantum computing:
Dream or nightmare, Physics Today 10.1063/1.881512
(1996).
[3] M. Dyakonov, Prospects for quantum computing:
ex-
tremely doubtful, International conference on Hot Topics
in Physical Informatics , Arxiv:1401.3629 (2013).
[4] T. Ayral, T. Louvet, Y. Zhou, C. Lambert, E. M.
Stoudenmire, and X. Waintal, Density-matrix renormal-
ization group algorithm for simulating quantum circuits
with a finite fidelity, PRX Quantum 4, 020304 (2023).
[5] F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin,
R. Barends, R. Biswas, S. Boixo, F. G. S. L. Brandao,
D. A. Buell, and et al., Quantum supremacy using a pro-
grammable superconducting processor, Nature 574, 505
(2019).
[6] P. Shor, Algorithms for quantum computation: discrete
logarithms and factoring, SFCS ’94 Proceedings of the
35th Annual Symposium on Foundations of Computer
Science , 124 (1994).
[7] J.-Y. Cai, Shor’s algorithm does not factor large integers
in the presence of noise (2023), arXiv:2306.10072 [quant-
ph].
[8] L. K. Grover, Quantum mechanics helps in searching for
a needle in a haystack, Phys. Rev. Lett. 79, 325 (1997).
[9] E. M. Stoudenmire and X. Waintal, Grover’s algorithm
offers no quantum advantage (2023), arXiv:2303.11317
[quant-ph].
[10] B. Pokharel and D. Lidar, Better-than-classical grover
search via quantum error detection and suppression
(2022).
[11] J. J. Eriksen, T. A. Anderson, J. E. Deustua, K. Ghanem,
D. Hait,
M. R. Hoffmann,
S. Lee,
D. S. Levine,
I. Magoulas, J. Shen, N. M. Tubman, K. B. Whaley,
E. Xu, Y. Yao, N. Zhang, A. Alavi, G. K.-L. Chan,
M. Head-Gordon, W. Liu, P. Piecuch, S. Sharma, S. L.
Ten-no, C. J. Umrigar, and J. Gauss, The ground state
electronic energy of benzene, The Journal of Physical
Chemistry Letters 11, 8922 (2020), pMID: 33022176,
https://doi.org/10.1021/acs.jpclett.0c02621.
[12] T. Louvet, T. Ayral, and X. Waintal, Go-no go criteria for
performing quantum chemistry calculations on quantum
computers (2023), arXiv:2306.02620 [quant-ph].
[13] Y. Zhou, E. M. Stoudenmire, and X. Waintal, What lim-
its the simulation of quantum computers?, Phys. Rev. X
10, 041038 (2020).
[14] L. Vandersypen, M. Steffen, G. Breyta, C. Yannoni,
M. Sherwood, and I. Chuang, Experimental realization
of shor’s quantum factoring algorithm using nuclear mag-
netic resonance., Nature 414, 883 (2001).
[15] T. Hoefler, T. Haener, and M. Troyer, Disentangling hype
from practicality: On realistically achieving quantum ad-
vantage (2023), arXiv:2307.00523 [quant-ph].
[16] M.
Reiher,
N.
Wiebe,
K.
M.
Svore,
D.
Wecker,
and
M.
Troyer,
Elucidating
reaction
mechanisms
on
quantum
computers,
Proceedings
of
the
Na-
tional
Academy
of
Sciences
114,
7555
(2017),
http://www.pnas.org/content/114/29/7555.full.pdf.
[17] M. E. Beverland, P. Murali, M. Troyer, K. M. Svore,
T. Hoefler, V. Kliuchnikov, G. H. Low, M. Soeken,
A. Sundaram, and A. Vaschillo, Assessing require-
ments to scale to practical quantum advantage (2022),
arXiv:2211.07629 [quant-ph].
[18] B. Martinez and Y.-M. Niquet, Variability of electron and
hole spin qubits due to interface roughness and charge
traps, Phys. Rev. Appl. 17, 024022 (2022).
[19] C. B¨auerle, D. Christian Glattli, T. Meunier, F. Portier,
P. Roche, P. Roulleau, S. Takada, and X. Waintal, Co-
herent control of single electrons: a review of current
progress, Reports on Progress in Physics 81, 056503
(2018).
[20] M.
Kjaergaard,
M.
E.
Schwartz,
J.
Braum¨uller,
P. Krantz, J. I.-J. Wang, S. Gustavsson, and W. D.
Oliver, Superconducting qubits: Current state of play,
Annual Review of Condensed Matter Physics 11, 369
(2020),
https://doi.org/10.1146/annurev-conmatphys-
031119-050605.
[21] Y. Kim, A. Eddins, S. Anand, K. X. Wei, E. van den
Berg, S. Rosenblatt, H. Nayfeh, Y. Wu, M. Zaletel,
K. Temme, and A. Kandala, Evidence for the utility of
quantum computing before fault tolerance., Nature 618,
500 (2023).

8
[22] M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information: 10th Anniversary Edition,
10th ed. (Cambridge University Press, New York, NY,
USA, 2011).
[23] J. Preskill, Quantum Computing in the NISQ era and
beyond, Quantum 2, 79 (2018).
[24] D. P. DiVincenzo, The physical implementation of quan-
tum computation, Fortschritte der Physik 48, 771 (2000).
[25] A. G. Fowler, M. Mariantoni, J. M. Martinis, and A. N.
Cleland, Surface codes:
Towards practical large-scale
quantum computation, Phys. Rev. A 86, 032324 (2012).
[26] D. Gottesman, The heisenberg representation of quan-
tum computers (1998), arXiv:quant-ph/9807006 [quant-
ph].
[27] X. Waintal, What determines the ultimate precision of a
quantum computer, Phys. Rev. A 99, 042318 (2019).
[28] M. Spiecker, P. Paluch, N. Drucker, S. Matityahu,
D. Gusenkova, N. Gosling, S. G¨unzler, D. Rieger, I. Tak-
makov, F. Valenti, P. Winkel, R. Gebauer, O. Sander,
G. Catelani, A. Shnirman, A. V. Ustinov, W. Werns-
dorfer, Y. Cohen, and I. M. Pop, A quantum szilard
engine for two-level systems coupled to a qubit (2022),
arXiv:2204.00499 [quant-ph].
[29] M.
McEwen,
L.
Faoro,
K.
Arya,
A.
Dunsworth,
T. Huang, S. Kim, B. Burkett, A. Fowler, F. Arute,
J. C. Bardin, A. Bengtsson, A. Bilmes, B. B. Buckley,
N. Bushnell, Z. Chen, R. Collins, S. Demura, A. R. Derk,
C. Erickson, M. Giustina, S. D. Harrington, S. Hong,
E. Jeffrey, J. Kelly, P. V. Klimov, F. Kostritsa, P. Laptev,
A. Locharla, X. Mi, K. C. Miao, S. Montazeri, J. Mutus,
O. Naaman, M. Neeley, C. Neill, A. Opremcak, C. Quin-
tana, N. Redd, P. Roushan, D. Sank, K. J. Satzinger,
V. Shvarts, T. White, Z. J. Yao, P. Yeh, J. Yoo, Y. Chen,
V. Smelyanskiy, J. M. Martinis, H. Neven, A. Megrant,
L. Ioffe, and R. Barends, Resolving catastrophic error
bursts from cosmic rays in large arrays of superconduct-
ing qubits, Nature Physics 18, 107 (2022).
[30] D. Gusenkova, F. Valenti, M. Spiecker, S. G¨unzler,
P. Paluch, D. Rieger, L.-M. Pioras-Timbolmas, L. P.
Zarbo, N. Casali, I. Colantoni, A. Cruciani, S. Pirro,
L. Cardani, A. Petrescu, W. Wernsdorfer, P. Winkel,
and I. M. Pop, Operating in a deep underground facil-
ity improves the locking of gradiometric fluxonium qubits
at the sweet spots, Applied Physics Letters 120, 054001
(2022), https://doi.org/10.1063/5.0075909.
[31] S. Krinner, N. Lacroix, A. Remm, A. Di Paolo, E. Genois,
C. Leroux, C. Hellings, S. Lazar, F. Swiadek, J. Her-
rmann, G. J. Norris, C. K. Andersen, M. M¨uller, A. Blais,
C. Eichler, and A. Wallraff, Realizing repeated quantum
error correction in a distance-three surface code, Nature
605, 669 (2022).
[32] R. Acharya, I. Aleiner, R. Allen, T. I. Andersen, M. Ans-
mann, F. Arute, K. Arya, A. Asfaw, J. Atalaya, R. Bab-
bush, D. Bacon, J. C. Bardin, J. Basso, A. Bengtsson,
S. Boixo, G. Bortoli, A. Bourassa, J. Bovaird, L. Brill,
M. Broughton, B. B. Buckley, D. A. Buell, T. Burger,
B. Burkett, N. Bushnell, Y. Chen, Z. Chen, B. Chiaro,
J. Cogan, R. Collins, P. Conner, W. Courtney, A. L.
Crook, B. Curtin, D. M. Debroy, A. Del Toro Barba,
S. Demura, A. Dunsworth, D. Eppens, C. Erickson,
L. Faoro, E. Farhi, R. Fatemi, L. Flores Burgos, E. Forati,
A. G. Fowler, B. Foxen, W. Giang, C. Gidney, D. Gilboa,
M. Giustina, A. Grajales Dau, J. A. Gross, S. Habeg-
ger, M. C. Hamilton, M. P. Harrigan, S. D. Harrington,
O. Higgott, J. Hilton, M. Hoffmann, S. Hong, T. Huang,
A. Huff, W. J. Huggins, L. B. Ioffe, S. V. Isakov, J. Ive-
land, E. Jeffrey, Z. Jiang, C. Jones, P. Juhas, D. Kafri,
K. Kechedzhi, J. Kelly, T. Khattar, M. Khezri, K. Maria,
S. Kim, A. Kitaev, P. V. Klimov, A. R. Klots, A. N.
Korotkov, F. Kostritsa, J. M. Kreikebaum, D. Land-
huis, P. Laptev, K.-M. Lau, L. Laws, J. Lee, K. Lee,
B. J. Lester, A. Lill, W. Liu, A. Locharla, E. Lucero,
F. D. Malone, J. Marshall, O. Martin, J. R. McClean,
T. McCourt, M. McEwen, A. Megrant, B. Meurer Costa,
X. Mi, K. C. Miao, M. Mohseni, S. Montazeri, A. Mor-
van, E. Mount, W. Mruczkiewicz, O. Naaman, M. Nee-
ley, C. Neill, A. Nersisyan, H. Neven, M. Newman,
J. H. Ng, A. Nguyen, M. Nguyen, M. Y. Niu, T. E.
O’Brien, A. Opremcak, J. Platt, A. Petukhov, R. Potter,
L. P. Pryadko, C. Quintana, P. Roushan, N. C. Rubin,
N. Saei, D. Sank, K. Sankaragomathi, K. J. Satzinger,
H. F. Schurkus, C. Schuster, M. J. Shearn, A. Shorter,
V. Shvarts, J. Skruzny, V. Smelyanskiy, W. C. Smith,
G. Sterling, D. Strain, M. Szalay, A. Torres, G. Vi-
dal, B. Villalonga, C. Vollgraff Heidweiller, T. White,
C. Xing, Z. J. Yao, P. Yeh, J. Yoo, G. Young, A. Zal-
cman, Y. Zhang, N. Zhu, and Google Quantum AI, Sup-
pressing quantum errors by scaling a surface code logical
qubit, Nature 614, 676 (2023).
[33] L. Postler, S. Heusen, I. Pogorelov, M. Rispler, T. Feld-
ker, M. Meth, C. D. Marciniak, R. Stricker, M. Ring-
bauer, R. Blatt, P. Schindler, M. M¨uller, and T. Monz,
Demonstration of fault-tolerant universal quantum gate
operations, Nature 605, 675 (2022).
[34] V. V. Sivak, A. Eickbusch, B. Royer, S. Singh, I. Tsiout-
sios, S. Ganjam, A. Miano, B. L. Brock, A. Z. Ding,
L. Frunzio, S. M. Girvin, R. J. Schoelkopf, and M. H. De-
voret, Real-time quantum error correction beyond break-
even, Nature 616, 50 (2023).
[35] Z. Ni, S. Li, X. Deng, Y. Cai, L. Zhang, W. Wang, Z.-
B. Yang, H. Yu, F. Yan, S. Liu, C.-L. Zou, L. Sun, S.-
B. Zheng, Y. Xu, and D. Yu, Beating the break-even
point with a discrete-variable-encoded logical qubit, Na-
ture 616, 56 (2023).
[36] A. Morvan, B. Villalonga, X. Mi, S. Mandra, A. Bengts-
son, P. V. Klimov, Z. Chen, S. Hong, C. Erickson,
I. K. Drozdov, J. Chau, G. Laun, R. Movassagh, A. As-
faw, L. T. A. N. Brandao, R. Peralta, D. Abanin,
R. Acharya, R. Allen, T. I. Andersen, K. Anderson,
M. Ansmann, F. Arute, K. Arya, J. Atalaya, J. C.
Bardin, A. Bilmes, G. Bortoli, A. Bourassa, J. Bo-
vaird, L. Brill, M. Broughton, B. B. Buckley, D. A.
Buell, T. Burger, B. Burkett, N. Bushnell, J. Campero,
H. S. Chang, B. Chiaro, D. Chik, C. Chou, J. Co-
gan, R. Collins, P. Conner, W. Courtney, A. L. Crook,
B. Curtin, D. M. Debroy, A. D. T. Barba, S. De-
mura, A. D. Paolo, A. Dunsworth, L. Faoro, E. Farhi,
R. Fatemi, V. S. Ferreira, L. F. Burgos, E. Forati,
A. G. Fowler, B. Foxen, G. Garcia, E. Genois, W. Gi-
ang, C. Gidney, D. Gilboa, M. Giustina, R. Gosula,
A. G. Dau, J. A. Gross, S. Habegger, M. C. Hamil-
ton, M. Hansen, M. P. Harrigan, S. D. Harrington,
P. Heu, M. R. Hoffmann, T. Huang, A. Huff, W. J.
Huggins, L. B. Ioffe, S. V. Isakov, J. Iveland, E. Jef-
frey, Z. Jiang, C. Jones, P. Juhas, D. Kafri, T. Khat-
tar, M. Khezri, M. Kieferova, S. Kim, A. Kitaev, A. R.
Klots, A. N. Korotkov, F. Kostritsa, J. M. Kreikebaum,
D. Landhuis, P. Laptev, K. M. Lau, L. Laws, J. Lee,

9
K. W. Lee, Y. D. Lensky, B. J. Lester, A. T. Lill,
W. Liu, A. Locharla, F. D. Malone, O. Martin, S. Mar-
tin, J. R. McClean, M. McEwen, K. C. Miao, A. Mieszala,
S. Montazeri, W. Mruczkiewicz, O. Naaman, M. Neeley,
C. Neill, A. Nersisyan, M. Newman, J. H. Ng, A. Nguyen,
M. Nguyen, M. Y. Niu, T. E. O’Brien, S. Omonije,
A. Opremcak, A. Petukhov, R. Potter, L. P. Pryadko,
C. Quintana, D. M. Rhodes, C. Rocque, P. Roushan,
N. C. Rubin, N. Saei, D. Sank, K. Sankaragomathi, K. J.
Satzinger, H. F. Schurkus, C. Schuster, M. J. Shearn,
A. Shorter, N. Shutty, V. Shvarts, V. Sivak, J. Skruzny,
W. C. Smith, R. D. Somma, G. Sterling, D. Strain,
M. Szalay, D. Thor, A. Torres, G. Vidal, C. V. Heid-
weiller, T. White, B. W. K. Woo, C. Xing, Z. J. Yao,
P. Yeh, J. Yoo, G. Young, A. Zalcman, Y. Zhang, N. Zhu,
N. Zobrist, E. G. Rieffel, R. Biswas, R. Babbush, D. Ba-
con, J. Hilton, E. Lucero, H. Neven, A. Megrant, J. Kelly,
I. Aleiner, V. Smelyanskiy, K. Kechedzhi, Y. Chen, and
S. Boixo, Phase transition in random circuit sampling
(2023), arXiv:2304.11119 [quant-ph].
[37] J. Tindall, M. Fishman, M. Stoudenmire, and D. Sels,
Efficient tensor network simulation of ibm’s kicked ising
experiment (2023), arXiv:2306.14887 [quant-ph].
[38] T. Begusic and G. K.-L. Chan, Fast classical simulation
of evidence for the utility of quantum computing before
fault tolerance (2023), arXiv:2306.16372 [quant-ph].
[39] K. Kechedzhi, S. V. Isakov, S. Mandra, B. Villalonga,
X. Mi, S. Boixo, and V. Smelyanskiy, Effective quantum
volume, fidelity and computational cost of noisy quantum
processing experiments (2023), arXiv:2306.15970 [quant-
ph].
[40] H. Xu, C.-M. Chung, M. Qin, U. Schollw¨ock, S. R.
White, and S. Zhang, Coexistence of superconductivity
with partially filled stripes in the hubbard model (2023),
arXiv:2303.08376 [cond-mat.supr-con].
[41] F. Simkovic, R. Rossi, A. Georges, and M. Ferrero, Origin
and fate of the pseudogap in the doped hubbard model
(2022), arXiv:2209.09237 [cond-mat.str-el].
[42] G.
Carleo
and
M.
Troyer,
Solving
the
quan-
tum
many-body
problem
with
artificial
neu-
ral
networks,
Science
355,
602
(2017),
https://www.science.org/doi/pdf/10.1126/science.aag2302.
[43] P. Corboz, R. Or´us, B. Bauer, and G. Vidal, Simula-
tion of strongly correlated fermions in two spatial di-
mensions with fermionic projected entangled-pair states,
Phys. Rev. B 81, 165104 (2010).
[44] J. Chen, E. M. Stoudenmire, and S. R. White, The quan-
tum fourier transform has small entanglement (2022),
arXiv:2210.08468 [quant-ph].

