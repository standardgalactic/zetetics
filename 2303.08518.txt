UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation
Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu
Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, Qi Zhang
Microsoft Corporation
daixuancheng6@gmail.com
bijunyu21@mails.ucas.ac.cn
{shaohanh, yuefzh, jianfengliu, yujing.wang, hasun, fuwei, dedeng, qizhang}@microsoft.com
Abstract
Large Language Models (LLMs) are popu-
lar for their impressive abilities, but the need
for model-speciÔ¨Åc Ô¨Åne-tuning or task-speciÔ¨Åc
prompt engineering can hinder their gener-
alization.
We propose UPRISE (Universal
Prompt Retrieval for Improving zero-Shot
Evaluation), which tunes a lightweight and
versatile retriever that automatically retrieves
prompts for a given zero-shot task input.
SpeciÔ¨Åcally, we demonstrate universality in
a cross-task and cross-model scenario:
the
retriever is tuned on a diverse set of tasks,
but tested on unseen task types; we use a
small frozen LLM, GPT-Neo-2.7B, for tun-
ing the retriever, but test the retriever on dif-
ferent LLMs of much larger scales, such as
BLOOM-7.1B, OPT-66B and GPT3-175B.
Additionally, we show that UPRISE mitigates
the hallucination problem in our experiments
with ChatGPT, suggesting its potential to im-
prove even the strongest LLMs.
1
Introduction
Large
Language
Models
(LLMs)
such
as
GPT-3 (Brown et al., 2020), OPT (Zhang et al.,
2022), and BLOOM (Scao et al., 2022) have
shown impressive capabilities across a wide range
of tasks.
Recent research proposes two main
approaches to further improve their performance:
Ô¨Åne-tuning LLMs to follow prompts (Hu et al.,
2022; Houlsby et al., 2019; Zaken et al., 2022; Wei
et al., 2022a; Sanh et al., 2022) and developing
prompt engineering techniques to guide the
LLMs (Brown et al., 2020; Wei et al., 2022b; Liu
et al., 2021; Lester et al., 2021).
Fine-tuning LLMs adjusts their weights to Ô¨Åt
speciÔ¨Åc prompts and improve task performance.
However, this may be limited by computational
resources and unavailable model weights (Hu et al.,
2022). Multi-task tuning provides an alternative
approach to improve zero-shot task generaliza-
tion (Wei et al., 2022a; Sanh et al., 2022), which
BLOOM-7.1B
GPT3-175B
OPT-66B
üßä
üî•Prompt Retriever
Tune
GPT-Neo-2.7B
Read. Compre.
Close. QA
Paraphrase
Cross-
Model
NLI
Sentiment
Fact-Check.
Cross-
Task
Inference
Train
Figure 1: UPRISE tunes a prompt retriever on multiple
tasks with a small frozen LLM, but conducts inference
on unseen task types with a different larger LLM.
partially justiÔ¨Åes the tuning cost. Yet, the con-
stant evolution of LLMs creates a need for tuning
new models, which makes the the cumulative Ô¨Åne-
tuning cost a big concern.
Prompt engineering constructs prompts to guide
frozen LLMs. Prompt design adds an engineered
natural language prompt to the task input, to teach
the LLM to learn in context (Brown et al., 2020)
or induce the LLM to reason (Wei et al., 2022b).
Prompt tuning adds a soft prompt represented by
continuous parameters, and optimizes it through
gradient propagation (Liu et al., 2021; Li and Liang,
2021; Lester et al., 2021). While these methods can
achieve excellent performance for speciÔ¨Åc tasks,
it is uncertain whether the prompts designed for
one task can generalize to unseen task types, as
prompt designers are blind in strict zero-shot set-
tings (van de Kar et al., 2022).
In this paper, we propose UPRISE (Universal
Prompt
Retrieval
for
Improving
Zero-Shot
Evaluation), which tunes a lightweight and ver-
satile retriever that automatically retrieves prompts
from a pre-constructed pool of data, given a
zero-shot task input. As illustrated in Figure 1,
the retriever is trained to retrieve prompts for
multiple tasks, enabling it to generalize to un-
seen task types during inference.
In addition,
arXiv:2303.08518v1  [cs.CL]  15 Mar 2023

we demonstrate that the cross-task capabilities
can generalize well from a small LLM to dif-
ferent LLMs of much larger scales:
we use
GPT-Neo-2.7B (Black et al., 2021) to guide
the tuning of the retriever and evaluate the re-
triever‚Äôs performance on BLOOM-7.1B (Scao
et al., 2022), OPT-66B (Zhang et al., 2022),
and GPT3-175B (Brown et al., 2020).
The
cross-model and cross-task generalization of UP-
RISE makes it a promising and practical solution
for real-world applications.
Furthermore, our approach demonstrates the po-
tential for enhancing even the most powerful LLMs,
as shown in our experiments with ChatGPT. De-
spite its impressive abilities, ChatGPT has been
found to struggle with serious hallucination prob-
lems, leading to responses that are factually inac-
curate (Bang et al., 2023). However, UPRISE is
able to address this issue on fact-checking tasks
by prompting the model to draw correct inferences
from its built-in knowledge.
In summary, our contributions include:
‚Ä¢ We introduce UPRISE, a light-weight and versa-
tile approach to improve zero-shot performance
of LLMs in the cross-task and cross-model sce-
narios.
‚Ä¢ UPRISE is tuned with GPT-Neo-2.7B, but
can also beneÔ¨Åt different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B and
GPT3-175B.
‚Ä¢ Our exploration on ChatGPT demonstrates the
potential of UPRISE in improving performance
of even the strongest LLMs.
2
Problem DeÔ¨Ånition
We aim to improve zero-shot performance of LLMs
by training a prompt retriever to retrieve prompts
for any given task input. SpeciÔ¨Åcally, UPRISE de-
composes the prompting process into two steps:
retrieve then predict. Given an input x, we Ô¨Årst
retrieve a set of positive prompts P+ from a pre-
constructed pool P:
P+ = R(x, P).
(1)
Then we concatenate P+ with x to form an input
sequence for a frozen LLM, which generates a pre-
dicted output:
yP+ = LM

yP+|P+ ‚äïx

.
(2)
Our objective is to optimize performance of yP+
to match the target y by updating the retriever R.
Engineered Natural 
Language Prompt
üßäLanguage Model
Task
Input
üßäSummarize the‚Ä¶
Prompt Design
Tunable Soft
Prompt
üî•0.10, 0.05, ‚Ä¶
Prompt Tuning
Retrieved Natural
Language Prompt
Prompt Retrieval
üßäLanguage Model
üßäLanguage Model
üßäAnswer this‚Ä¶
üî•Prompt Retriever
Tunable Prompt 
Retriever
backward
Tune
Task
Input
Task
Input
Figure 2: Typical prompt engineering methods and
prompt retrieval. Prompt retrieval prepends a natural
language prompt to the task input and uses a frozen
LLM to evaluate the prompt‚Äôs performance, which is
then used to tune the retriever reversely.
Figure 2 compares prompt retrieval with typical
prompt engineering methods: prompt design adds
an engineered natural language prompt (Brown
et al., 2020; Wei et al., 2022b) and prompt tun-
ing tunes a soft prompt (Liu et al., 2021; Lester
et al., 2021). In contrast, prompt retrieval tunes
a retriever to retrieve natural language prompts,
which is both interpretable and Ô¨Çexible. It uses the
language model itself to label each prompt in the
prompt pool as positive/negative, and then tuning a
retriever from this signal (Rubin et al., 2022). Such
Ô¨Åne-tuned prompt retrieval has demonstrated effec-
tiveness in tuning task-speciÔ¨Åc retrievers: EPR (Ru-
bin et al., 2022) and CEIL (Ye et al., 2023) train
a single prompt retriever for a speciÔ¨Åc task using
the input and output pairs in the training set as the
prompt pool. The retriever is then evaluated on the
inputs of the corresponding test set.
Our work is to achieve universality of the prompt
retriever, which means the Ô¨Åne-tuned retriever can
be directly used to retrieve prompts for unseen tasks
and various inference LLMs, without the need for
further tuning. We deÔ¨Åne the universality from two
perspectives: cross-task retrieval and cross-model
retrieval.
Cross-task retrieval. In order to address the di-
versity of tasks in real-world applications, we pro-
pose cross-task retrieval to retrieve for task types
on which the prompt retriever has not been trained.
We simulate this setting by evaluating the prompt
retriever on unseen task types (Wei et al., 2022a):
various tasks are grouped into different clusters
based on their task types, and we hold out each task

cluster for evaluation while training the retriever
on all remaining clusters.
Cross-model retrieval. Due to the high cost of
tuning a prompt retriever with a large-scale LLM
(e.g., larger than 5B), we propose to evaluate the
generalization of small-to-large LLMs. SpeciÔ¨Å-
cally, we use a relatively small LLM for tuning the
retriever, while using a much larger LLM for in-
ference. Furthermore, we suggest exploring cross-
model source generalization, as there are LLMs
developed by different companies or institutions.
3
Method
As shown in Figure 3, UPRISE uses a frozen LLM
to supervise the Ô¨Åne-tuning of a prompt retriever
on a diverse set of tasks, and then using this trained
retriever to retrieve prompts for different task types
during inference with different LLMs. In this sec-
tion, we elaborate on our data construction, and
training and inference pipeline.
3.1
Data Construction
Task Data. We use instruction templates from
FLAN (Wei et al., 2022a) to convert task datasets
into natural language instructions1.
Each task
dataset corresponds to approximately seven tem-
plates. For each data example (xi, yi), we ran-
domly select one of the seven templates to convert
xi into an input instruction and yi into a label com-
pletion. The option sufÔ¨Åces and new line characters
‚Äú\n‚Äù are then automatically removed from the in-
structions, to make the instruction format more
similar to that of the pre-training corpus, thus im-
proving prompting performance (van de Kar et al.,
2022).
Prompt pool.
For each testing cluster, the
prompt pool to be retrieved is made of training
demonstrations of the remaining task clusters (i.e.,
the clusters for training the retriever). This is in-
spired by in-context learning (Brown et al., 2020),
which presents a few task demonstrations before
the task input to improve model performance. Each
demonstration is a concatenation of the input in-
struction and the label completion, separated by a
text blank space. Our motivation is that even if the
testing task type has no overlap with the training
task types, the testing input may still beneÔ¨Åt from
similar question types, topics, or reasoning chains
in the retrieved demonstrations.
1We exclude templates that ‚Äúturn the task around‚Äù, such
as asking a sentiment classiÔ¨Åcation task to generate a movie
review.
3.2
Prompt Scoring
For each training example (xi, yi) in the training
clusters, we collect a set of positive and nega-
tive prompts from the prompt pool P = {pj}NP
j=1,
where the positive prompt indicates that the frozen
LLM achieves good task scores conditioned on the
prompt-input pair. We use these positive and nega-
tive labels to supervise the contrastive learning of
the retriever.
We categorize all tasks into two question types:
text completion and multiple-choice, and use differ-
ent methods to score the prompts for each training
example.
Text completion is the question to do free-form
completion. We calculate score of the prompt using
the following equation:
score (pj, xi) = metric
 yi, ypj
i

,
(3)
where ypj
i
= LM
 ypj
i |pj ‚äïxi

is the model pre-
diction based on the input concatenation pj ‚äïxi,
and ‚äïis a text delimiter ‚Äú\n‚Äù. metric (¬∑) is the
function used to calculate the task metric score
(e.g., F1 or Exact Match).
Multiple choice is the question to choose one
correct completion from several options.
Sup-
pose there are M options in a multiple choice task

xi, yi, {om}M
m=1

, where {om}M
m=1 is the option
set and oyi denotes the gold option. We feed the
concatenation pj ‚äïxi to the LLM and calculate
per-token likelihood of each option: LH (om). The
option with the highest likelihood is considered as
the model prediction ypj
i .
Accuracy of the prediction acc
 yi, ypj
i

could
evaluate the prompt, but it only produces binary
results which makes it hard to compare prompt
effectiveness. To address this, we calculate the
Ô¨Ånal score using the equation:
score (pj, xi) = acc
 yi, ypj
i

¬∑
LH (oyi)
PM
m=1 LH (om)
,
(4)
where we multiply the accuracy score with the nor-
malized likelihood of the gold option, to achieve a
Ô¨Åne-grained comparison.
Prompt Ô¨Åltering. Intuitively, to collect the posi-
tive and negative prompts for each training exam-
ple, we need to score every prompt in the prompt
pool and identify the prompt that yields the best
score as the positive prompt. Conversely, prompts
that lead to the worst scores are labeled as negative
prompts. However, scoring all the prompts can be

üßä
Task 
Input
üî•Prompt
Encoder
üî•Input 
Encoder
Tune on many tasks
Prompt
Retriever
Tune
0.37
Contrastive 
Loss
0.19
0.44
0.09
Task 
Score
0.13
0.78
Read. Compre.
Read the passage‚Ä¶
Read. Compre.
Read the passage‚Ä¶
NLI
Does love entail‚Ä¶
Close. QA
Please Answer‚Ä¶
Read. Compre.
Read the passage‚Ä¶
NLI
Does love entail‚Ä¶
NLI
Premise: today‚Ä¶
Prompt 
Pool
Inference on unseen task types
with a different language model
Task 
Input
Prompt 
Retriever
Task
Output
NLI
Does love entail‚Ä¶
Read. Compre.
Read the passage‚Ä¶
Sentiment
How do you feel‚Ä¶
Sentiment
How do you feel‚Ä¶
Sentiment
Positive
GPT-Neo-2.7B
BLOOM-7.1B
GPT3-175B
OPT-66B
Figure 3: Training and inference pipeline. In the training stage, a frozen LLM is used to supervise the tuning of a
prompt retriever, where both the LLM and the retriever take the prompt-input pairs as input, and we use the task
scores given by the LLM to supervise the contrastive learning of the retriever. In the inference stage, for each task
input, the tuned prompt retriever retrieve positive prompt(s) to guide the inference model to predict a task output.
Overall, we follow a cross-task and cross-model paradigm where the task types and LLMs for training could be
different from those for inference.
computationally expensive, even with a relatively
small LLM. To address this, we design a prompt Ô¨Ål-
tering mechanism to reduce the number of prompts
that need to be scored.
Inspired by one-shot in-context learning (Brown
et al., 2020), we observe that a single training
demonstration, even if randomly sampled, can im-
prove the evaluation performance of the testing
sample, since the training demonstration and the
testing sample belong to the same task, where they
could share common patterns or structures. There-
fore, we randomly sample a small subset L of train-
ing demonstrations from the prompt pool for each
training example (xi, yi). SpeciÔ¨Åcally, we select
only those demonstrations that belong to the same
task as (xi, yi). This signiÔ¨Åcantly reduces the num-
ber of prompts to be scored while still increasing
the likelihood of identifying positive prompts in
the sampled subset.
Furthermore, in the case of difÔ¨Åcult questions,
all L prompt-input pairs may result in a score of 0.
To address this, we repeat the sampling process to
score another subset of L prompts with the same
task as (xi, yi) until we Ô¨Ånd at least one prompt
with a score greater than 0.
For all the scored prompts for a training example,
we label the prompt with the highest score as posi-
tive. For negative samples, we randomly sample B
training demonstrations from the prompt pool, with
different tasks than (xi, yi). In addition, we label
B demonstrations corresponding to the lowest B
scores in the sampled prompts (B < L) as hard
negatives, which are of the same task with (xi, yi)
but are less effective.
3.3
Retriever Tuning
After labeling the positive and (hard) negative
prompts for each training example (xi, yi), we split
the collected data into two sets: 90% for training
and 10% for validation. The prompt retriever is a
bi-encoder model where the input encoder EX(¬∑)
takes the task input xi as input, and the prompt
encoder EP (¬∑) takes prompt pj as input.
To
train
the
prompt
retriever,
InfoNCE
loss (van den Oord et al., 2018) is used to maximize
the similarity score between the encoded prompt
and input for positive prompt-input pairs, and min-
imize it for (hard) negative prompt-input pairs. For
a single training example (xi, yi), the loss function
for its corresponding positive and negative prompts
is deÔ¨Åned as follows (Rubin et al., 2022):
L(xi, p+
i , p‚àí
i,1, . . . p‚àí
i,2B)
(5)
=
‚àílog
esim(xi,p+
i )
esim(xi,p+
i ) + P2B
j=1 esim(xi,p‚àí
i,j) ,
where p+
i is the positive prompt, and p‚àí
i,j is one

of the (hard) negative prompts, and sim(xi, p) =
EX(xi)‚ä§EP (p) calculates the similarity score be-
tween input xi and prompt p using inner products.
3.4
Inference
After Ô¨Åne-tuning the prompt encoder, we use it to
encode the entire prompt pool with EP (¬∑). At in-
ference time, for a testing input instruction xtest,
we compute its encoding EX(xtest) and then use
maximum inner-product search over the prompt
pool to retrieve K most similar prompts, sorted by
their inner product in descending order, denoted
as P+ = (p1, ..., pK). We then concatenate the
prompts with the task input, resulting in the con-
catenation pK ‚äï... ‚äïp1 ‚äïxtest.
To evaluate the inference results, we use the
same method described in Section 3.2 to gener-
ate model predictions, and then use each task‚Äôs
corresponding evaluation metric to compute the
scores.
4
Experiment Settings
Task clustering. We group the tasks used in our
method into clusters, including Reading Compre-
hension, Closed-book QA, Paraphrase Detection,
Natural Language Inference, Sentiment Analysis,
Commonsense Reasoning, Coreference Resolution,
Structure to Text, and Summarization. Additional
information about the datasets in each cluster can
be found in Appendix A.
Data sampling. To prevent the retriever tuning
from being dominated by large datasets, we ran-
domly sample up to 10k data examples from each
task‚Äôs training set, while also maintaining class
balance in classiÔ¨Åcation tasks2. The prompt pool
consists only of the sampled training data. On
average, for each testing task cluster, there are ap-
proximately 180k training examples sampled from
the remaining clusters.
LLMs.
We use GPT-Neo-2.7B (Black
et al., 2021) from EleutherAI to tune the
retriever,
and evaluate the performance on
larger LLMs from various sources in the in-
ference stage, including BLOOM-7.1B (Scao
et al., 2022) from BigScience, OPT-66B (Zhang
et al., 2022) from Meta, and Davinci and
text-davinci-001 from OpenAI, both be-
longing to the GPT3-175B (Brown et al., 2020)
2For instance, in a four-classiÔ¨Åcation task, we sample a
maximum of 2.5k data examples from each class.
series. We use greedy search to obtain predictions
from all the LLMs.
Prompt scoring. We set the size of the ran-
domly sampled subset to L = 50 and the number
of (hard) negatives to B = 20. For difÔ¨Åcult ques-
tions, we repeat the re-sampling process up to seven
rounds, as we found that this is sufÔ¨Åcient to identify
a positive prompt for 90% of the training examples.
If no sampled prompt yields a score greater than 0,
we Ô¨Ålter out the corresponding training example.
Tuning. We initialize both encoders of the re-
triever with BERTBASE (Devlin et al., 2019). Each
retriever is Ô¨Åne-tuned for three epochs, and the best
checkpoint is chosen based on retrieval accuracy
using the validation set. For detailed tuning hyper-
parameters, please refer to Appendix B.
Inference. During inference, we set the number
K of concatenated prompts to a relatively small
value of 3, to balance between prompting perfor-
mance and inference efÔ¨Åciency. For each dataset in
the testing cluster, We report metric scores on the
test set when available, falling back to the valida-
tion set otherwise.
5
Main Results
We evaluate our prompt retriever on natural lan-
guage understanding tasks where generative LLMs
are known to need improvement (Liu et al., 2021).
Table 1 compares the performance of UPRISE to
vanilla zero-shot prompting.
5.1
Cross-Task Prompt Retrieval
Based on the results in the GPT-Neo-2.7B col-
umn, we can assess our ability of generalizing
across different task types. UPRISE has positive
impacts on most of the testing clusters. SpeciÔ¨Å-
cally, we achieve performance gains of 8.5% and
14.6% absolute points in reading comprehension
and paraphrase detection tasks, respectively, when
compared to the zero-shot baseline. We also Ô¨Ånd
that UPRISE shows consistent performance im-
provements across all tasks in closed-book QA and
natural language inference.
However, UPRISE has negative impacts on tasks
in commonsense reasoning and coreference resolu-
tion clusters. This aligns with the Ô¨Åndings reported
by Wei et al. (2022a): instruction templates may
not be effective for tasks that are directly formu-
lated as language modeling problems. Alterna-
tive techniques such as chain-of-thought prompt-
ing (Wei et al., 2022b) may be more effective.

Task
Metric
GPT-Neo-2.7B
BLOOM-7.1B
OPT-66B
Davinci
Davinci-001
0-SHOT
UPRISE
0-SHOT
UPRISE
0-SHOT
UPRISE
0-SHOT
UPRISE
0-SHOT
UPRISE
Reading Comprehension
SQuADv1
F1
4.4
26.4
4.5
5.5
6.1
7.5
6.5
6.0
41.6
57.7
EM
0.4
14.3
0.0
0.0
0.0
0.6
0.0
0.0
16.4
36.8
BoolQ
Acc
54.5
59.4
54.0
60.2
60.7
63.5
62.0
65.7
64.2
65.7
MultiRC
F1
57.1
58.1
58.8
59.8
59.6
60.4
59.8
60.0
54.3
58.9
OBQA
Acc
41.8
42.2
44.0
41.8
46.4
48.8
49.2
52.4
52.8
48.8
Average
31.6
40.1
32.3
33.5
34.6
36.2
35.5
36.8
45.9
53.6
Closed-book QA
ARC-e
Acc
45.7
55.6
53.7
60.9
56.2
66.0
64.1
71.8
67.0
74.4
ARC-c
Acc
29.3
30.0
33.2
34.2
36.7
40.2
40.8
45.2
46.2
50.4
NQ
F1
1.3
5.6
0.9
1.4
2.5
2.1
0.0
2.2
18.3
18.2
EM
0.5
2.2
0.0
0.1
0.3
0.4
0.0
0.0
4.8
8.7
Average
19.2
23.3
22.0
24.2
23.9
27.2
26.2
29.8
34.1
37.9
Paraphrase Detection
MRPC
Acc
46.6
67.9
51.0
70.6
51.0
68.9
54.4
62.3
40.0
61.3
F1
46.0
80.4
58.0
82.1
57.8
81.5
68.9
81.4
39.2
72.9
QQP
Acc
48.4
54.3
49.5
53.1
50.5
49.7
55.2
52.4
60.9
62.6
F1
42.2
59.8
46.7
59.6
43.7
58.5
33.7
57.9
43.0
45.9
PAWS
Acc
51.7
45.7
50.8
45.9
50.5
44.4
52.4
44.5
53.2
52.3
Average
47.0
61.6
51.2
62.3
50.7
60.6
52.9
59.7
47.3
59.0
Natural Language Inference
MNLI-m
Acc
35.3
41.3
35.4
36.0
37.0
40.4
34.2
38.2
44.7
41.1
MNLI-mm
Acc
36.4
43.1
34.9
35.8
37.1
41.2
34.2
38.6
46.5
42.1
QNLI
Acc
50.9
53.8
49.9
51.3
54.2
53.7
51.7
51.1
60.0
58.4
SNLI
Acc
35.2
42.3
35.2
34.4
34.5
40.2
33.5
37.9
47.5
42.0
RTE
Acc
33.6
34.7
50.5
49.8
52.3
46.9
51.3
45.5
52.3
50.9
Average
38.3
43.0
41.2
41.5
43.0
44.5
41.0
42.3
50.2
46.9
Sentiment Analysis
SST-2
Acc
52.4
56.2
63.2
69.1
57.9
65.3
52.3
64.3
90.5
90.5
Yelp
Acc
71.7
67.8
56.1
58.0
67.6
63.5
59.8
65.3
80.3
80.2
Sent140
Acc
64.1
61.3
74.5
72.1
59.1
61.6
64.3
72.1
87.2
89.1
Average
62.7
61.8
64.6
66.4
61.5
63.5
58.8
67.3
86.0
86.6
Commonsense Reasoning
PiQA
Acc
70.2
70.4
71.5
72.1
76.5
80.4
79.1
81.3
79.1
79.1
COPA
Acc
67.0
64.0
67.0
67.0
74.0
76.0
80.0
83.0
83.0
80.0
HellaSwag
Acc
54.4
52.1
59.6
58.8
72.9
71.4
76.9
76.7
77.6
78.2
Average
63.9
62.2
66.0
66.0
74.5
75.9
78.7
80.3
79.9
79.1
Coreference Resolution
WSC273
Acc
73.6
76.6
78.0
81.0
83.9
86.1
60.6
50.0
78.8
75.5
DPR
Acc
59.6
51.0
64.4
55.8
66.3
50.0
82.1
83.9
64.4
58.7
Winogrande
Acc
58.9
58.6
65.9
64.3
69.2
67.8
68.6
70.2
66.3
64.7
Average
64.0
62.1
69.4
67.0
73.1
68.0
70.4
68.0
69.8
66.3
Table 1: Zero-shot performance across tasks and LLMs.
The model Davinci-001 is the Ô¨Åne-tuned version
text-davinci-001 of Davinci. The method 0-SHOT is the vanilla zero-shot method with only the input
instruction fed into the LLM.

0-SHOT
UPRISE
35
40
45
50
55
GPT-Neo-2.7B BLOOM-7.1B
OPT-66B
Davinci
Davinci-001
Avg.  
Figure 4: cross-model results of the cross-task retriever.
In Table 6-10 in Appendix, we show two cases
of each testing cluster, and analyze the relevance
between the retrieved prompts and task input in the
caption. We observe the cross-task improvement
beneÔ¨Åts from similar question types, topics, text
formats, or logical relationships.
5.2
Cross-Model Prompt Retrieval
In addition to evaluating cross-task generaliza-
tion, we can explore the cross-model ability by
examining the results of BLOOM, OPT, Davinci
and text-davinci-001. UPRISE continues
to improve performance on reading comprehen-
sion, closed-book QA, and paraphrase detection
tasks across all LLMs. While the performance on
sentiment analysis is negative with the small 2.7B
GPT-Neo, we observe positive impacts when us-
ing larger models. We achieve consistent gains
on natural language inference tasks with the mod-
els that have not been Ô¨Åne-tuned (BLOOM, OPT,
and Davinci), but experience a drop in perfor-
mance on text-davinci-001, which could
potentially be due to the model being speciÔ¨Åcally
Ô¨Åne-tuned on such tasks to improve performance
Generally, we presenting the average perfor-
mance of reading comprehension, closed-book QA,
paraphrase detection, natural language inference,
and sentiment analysis in Figure 4. The results
indicate consistent performance gains across all
LLMs.
6
Hallucination Mitigation of ChatGPT
Despite the impressive abilities of ChatGPT in
many areas, recent reports have shown that it suf-
fers from a signiÔ¨Åcant issue known as hallucina-
tion: providing factually incorrect responses to
users (Bang et al., 2023).
Method
TruthfulQA
FEVER2.0
Covid-19
0-SHOT
86.3
51.0
47.0
UPRISE
86.3
56.0
83.0
Table 2: Comparative results of ChatGPT on halluci-
nation detection tasks.
There existed no sibling duo called Sonny & Cher. Is this true?
Spot gold was fetching $365.25/366.25 an ounce at 0520
GMT, having galloped as high as $368.90--a level not seen
since February 10. Spot gold was quoted at $367.90/368.60
an ounce at 1000 GMT, having marched up to $369.50--a
level not seen since February 10. If the first sentence is true,
is the second one also true? No
There existed no sibling duo called Sonny & Cher. Is this true?
0-SHOT
UPRISE
False. Sonny & Cher were a popular American singing duo in 
the 1960s and 1970s, consisting of Sonny Bono and his wife 
Cher. They had several hit songs, including "I Got You Babe" 
and "The Beat Goes On," and also had their own television 
show, The Sonny & Cher Comedy Hour.
True
Figure 5: Case of the chats of vanilla zero-shot prompt-
ing and UPRISE on the FEVER2.0 dataset, the label
completion is ‚ÄúTrue‚Äù.
To assess the versatility of our prompt retriever,
we also investigate whether UPRISE can mitigate
the hallucination problem. We evaluate on three
tasks: TruthfulQA (Lin et al., 2022) for detecting
human falsehood, FEVER2.0 (Thorne et al., 2018)
and the scientiÔ¨Åc spilt of Covid-19 (Lee et al., 2021)
for fact-checking.
Table 2 shows that UPRISE outperforms vanilla
zero-shot prompting in the two fact-checking tasks.
Figure 5 presents an interesting case where zero-
shot prompting results in a correct generation of
information (‚ÄúSonny & Cher... consisting of Sonny
Bono and his wife Cher.‚Äù), but an incorrect an-
swer. In contrast, UPRISE successfully induces a
precise answer. We attribute this improvement to
the retrieved demonstration, which is of the natu-
ral language inference task type that may motivate
the model to correctly infer from its parametric
memory. This Ô¨Ånding suggests that the limited

RANDOM
TOPK-BM25
TOPK-BERT
UPRISE
10
30
50
70
Sentiment
Paraphrase
Reading Comp.
Closed. QA
NLI
Average Performance
Figure 6: Comparison of different universal retrievers, we report the average performance on each testing cluster.
memory3 of ChatGPT may not be the only factor
leading to the hallucination challenge. Rather, it
highlights the importance of having effective infer-
ence mechanisms. Prompt engineering techniques
such as UPRISE can help address this issue. For
further evaluation details and analysis, please refer
to Appendix C.
7
Ablation Study
7.1
Universal Prompt Retriever
We replace the universal retriever with three alter-
natives: 1) RANDOM randomly samples prompts
from the prompt pool, and reports the score av-
eraged on Ô¨Åve random seeds, 2) TOPK-BM25
uses the sparse retriever BM25 (Robertson and
Zaragoza, 2009) to retrieve prompts similar to
the testing input, and 3) TOPK-BERT follows
KATE (Liu et al., 2022) to use SBERT (Reimers
and Gurevych, 2019) to retrieve similar prompts.
Figure 6 displays the comparative performance
using GPT-Neo-2.7B, where UPRISE achieves
the best results among all the universal retrievers.
This suggests that word-level (TOPK-BM25) or
sentence-level (TOPK-BERT) similarity to the test-
ing input is not the only decisive factor for a good
prompt. This Ô¨Ånding underscores the effectiveness
of Ô¨Ånetuning a retriever with the language model
itself as a data labeler.
We also believe that the diversity of the instruc-
tion templates inherited from FLAN (Wei et al.,
2022a) contributes to our good cross-task transfer-
ability. By leveraging this diverse prompts, we are
able to tune a light-weight retriever instead of a
3‚Äúlimited memory‚Äù means that ChatGPT does not have
access to external knowledge bases.
Prompt Pool
Read.
Closed.
Para.
NLI
Senti.
RAW TEXT
32.0
19.3
44.7
37.5
60.3
UPRISE
40.1
23.4
61.6
43.0
61.8
Table 3:
Comparison of average performance on
GPT-Neo-2.7B with different prompt pool: RAW
TEXT uses raw data of the pre-training corpus, UP-
RISE uses training demonstrations of the trained tasks.
Method
Read.
Closed.
Para.
NLI
Senti.
0-SHOT
31.6
19.2
47.0
38.3
62.7
RANDOM
33.7
21.0
51.9
38.9
61.2
Table 4: Comparative results on GPT-Neo-2.7B of
vanilla zero-shot prompting (the Ô¨Årst row) and aug-
menting zero-shot prompting with randomly sampled
prompts of our prompt pool (the second row).
large language model, which allows us to follow
zero-shot instructions more efÔ¨Åciently.
7.2
Universal Prompt Pool
For each testing task cluster, we use training
demonstrations of the remaining clusters to con-
struct the prompt pool. To evaluate its effective-
ness, we replace it with the raw texts of wikitext-
103 (Merity et al., 2016), which belongs to the
pre-training corpus of many LLMs. The results in
Table 3 show the effectiveness of our prompt pool,
which outperforms the raw texts on all the testing
clusters. One possible reason is that even though
we follow the cross-task type paradigm, there still
exist some training demonstrations that are beneÔ¨Å-
cial for the testing input.
As shown in Table 4, we compare the results
of vanilla zero-shot prompting and prompting aug-

mented with randomly sampled prompts from our
prompt pool, where we see that even the randomly
sampled prompts could improve the performance
on four of the Ô¨Åve clusters. This suggests that pre-
senting some input-output demonstrations, even if
the tasks of the prompts are different from those
for testing, might be able to induce the large model
to think and reason (Wei et al., 2022b), leading to
inferring the correct answers.
8
Related Work
Our work is related to prompt engineering works in-
cluding prompt design, prompt tuning, and prompt
search.
Prompt Design In-context Learning (Brown
et al., 2020) is a method that helps LLMs transfer
to new tasks via inference alone by conditioning
a concatenation of demonstrations and test input,
without any gradient updates. The demonstrations
consist of k text samples. A in-context learning
method is classiÔ¨Åed as zero-shot when k = 0 and
few-shot when k > 0.
With standard in-context learning, LLMs strug-
gle to tackle complex arithmetic, commonsense,
and symbolic reasoning tasks. Chain-of-Thoughts
(CoT) (Wei et al., 2022b) proposes providing LLMs
with a series of intermediate reasoning steps as
demonstrations to induce LLMs to produce another
series of intermediate reasoning steps that lead to
the Ô¨Ånal answer.
Prompt Tuning. Traditional natural language
prompts require signiÔ¨Åcant human engineering and
can lead to suboptimal performance. Prompt tuning
proposes to learn a prompt represented by contin-
uous parameters rather than discrete natural lan-
guage tokens (Liu et al., 2021). Prompt tuning
takes the source text embedded by the LM input
embeddings and prepends learnable embeddings
to obtain a new embedded sequence. A variant of
prompt tuning is preÔ¨Åx tuning (Li and Liang, 2021;
Lester et al., 2021), where the learnable vectors are
added not only to the input but to all transformer
layers. The input-level method is referred to as shal-
low prompt tuning, and the layer-speciÔ¨Åc method is
referred to as deep prompt tuning (Wu et al., 2022).
Prompt Search. Prompt search is another ap-
proach to avoid manual prompt design. It involves
searching for prompts from pre-training corpora or
downstream task datasets to construct a prompt
text (Gao et al., 2021; Liu et al., 2022; van de
Kar et al., 2022; Ye et al., 2023).
To retrieve
similar prompts for the test examples, retrievers
such as the sparse retriever BM25 (Robertson and
Zaragoza, 2009) or the dense retriever based on
SBERT (Reimers and Gurevych, 2019) are em-
ployed. Furthermore, methods like EPR (Rubin
et al., 2022) and CEIL (Ye et al., 2023) use the
LLM itself to score the generated or searched
prompts, thereby eliminating the need for man-
ual prompt engineering and ensuring prompting
performance.
9
Conclusion
In this paper, we propose UPRISE , a lightweight
and versatile approach to improve the zero-shot
performance of different LLMs on various tasks.
SpeciÔ¨Åcally, we explore a cross-task and cross-
model scenario, to evaluate the universality of the
retriever to generalize from the trained task types to
unseen task types, and from a small LLM to LLMs
of much larger scales and different sources. Be-
sides, the exploration on ChatGPT demonstrates
our potential for improving even the strongest
LLMs. Overall, UPRISE provides a promising di-
rection of develop a lightweight and versatile mod-
ule to enhance the performance of different LLMs.
References
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Zi-
wei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan
Xu, and Pascale Fung. 2023.
A multitask, multi-
lingual, multimodal evaluation of chatgpt on reason-
ing, hallucination, and interactivity. arXiv preprint
arXiv:2302.04023.
Luisa Bentivogli,
Bernardo Magnini,
Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The Ô¨Åfth PASCAL recognizing textual entailment
challenge. In TAC. NIST.
Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar
Khot, Bhavana Dalvi Mishra, Kyle Richardson,
Ashish Sabharwal,
Carissa Schoenick,
Oyvind
Tafjord, and Peter Clark. 2021.
Think you have
solved direct-answer question answering? try arc-da,
the direct-answer AI2 reasoning challenge. CoRR,
abs/2102.03315.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. PIQA: reasoning
about physical commonsense in natural language. In
AAAI, pages 7432‚Äì7439. AAAI Press.
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and
Stella Biderman. 2021.
GPT-Neo:
Large Scale

Autoregressive Language Modeling with Mesh-
TensorÔ¨Çow. If you use this software, please cite it
using these metadata.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015.
A large an-
notated corpus for learning natural language infer-
ence. In EMNLP, pages 632‚Äì642. The Association
for Computational Linguistics.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini Agarwal,
Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In NeurIPS.
Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan
Martic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. In
NIPS, pages 4299‚Äì4307.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surprising
difÔ¨Åculty of natural yes/no questions.
In NAACL-
HLT (1), pages 2924‚Äì2936. Association for Compu-
tational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT (1), pages 4171‚Äì4186. As-
sociation for Computational Linguistics.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In IWP@IJCNLP. Asian Federation of Natural Lan-
guage Processing.
Ondrej Dusek, David M. Howcroft, and Verena Rieser.
2019. Semantic noise matters for neural natural lan-
guage generation. In INLG, pages 421‚Äì426. Associ-
ation for Computational Linguistics.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In ACL/IJCNLP (1), pages 3816‚Äì3830. As-
sociation for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classiÔ¨Åcation using distant supervision.
Processing, 150.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efÔ¨Åcient transfer learning for NLP.
In ICML, volume 97 of Proceedings of Machine
Learning Research, pages 2790‚Äì2799. PMLR.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. Lora: Low-rank adaptation of
large language models. In ICLR. OpenReview.net.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018.
Look-
ing beyond the surface: A challenge set for read-
ing comprehension over multiple sentences.
In
NAACL-HLT, pages 252‚Äì262. Association for Com-
putational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
Ô¨Åeld, Michael Collins, Ankur P. Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: a benchmark for question answer-
ing research.
Trans. Assoc. Comput. Linguistics,
7:452‚Äì466.
Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale
Fung. 2021. Towards few-shot fact-checking via per-
plexity. In NAACL-HLT, pages 1971‚Äì1981. Associ-
ation for Computational Linguistics.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efÔ¨Åcient prompt
tuning. In EMNLP (1), pages 3045‚Äì3059. Associa-
tion for Computational Linguistics.
Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In KR.
AAAI Press.
Xiang Lisa Li and Percy Liang. 2021. PreÔ¨Åx-tuning:
Optimizing continuous prompts for generation. In
ACL/IJCNLP (1), pages 4582‚Äì4597. Association for
Computational Linguistics.
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang
Ren. 2020. Commongen: A constrained text gener-
ation challenge for generative commonsense reason-
ing. In EMNLP (Findings), volume EMNLP 2020
of Findings of ACL, pages 1823‚Äì1840. Association
for Computational Linguistics.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
Truthfulqa: Measuring how models mimic human
falsehoods. In ACL (1), pages 3214‚Äì3252. Associa-
tion for Computational Linguistics.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022.
What
makes good in-context examples for gpt-3? In Dee-
LIO@ACL, pages 100‚Äì114. Association for Compu-
tational Linguistics.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT
understands, too. CoRR, abs/2103.10385.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? A new dataset for open book question an-
swering. In EMNLP, pages 2381‚Äì2391. Association
for Computational Linguistics.
Linyong Nan, Dragomir R. Radev, Rui Zhang, Am-
rit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xi-
angru Tang, Aadit Vyas, Neha Verma, Pranav Kr-
ishna, Yangxiaokang Liu, Nadia Irwanto, Jessica
Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mu-
tuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern
Tan, Xi Victoria Lin, Caiming Xiong, Richard
Socher, and Nazneen Fatema Rajani. 2021. DART:
open-domain structured data record to text genera-
tion. In NAACL-HLT, pages 432‚Äì447. Association
for Computational Linguistics.
Courtney Napoles, Matthew R. Gormley, and Ben-
jamin Van Durme. 2012. Annotated gigaword. In
AKBC-WEKEX@NAACL-HLT, pages 95‚Äì100. Asso-
ciation for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2012.
Resolving
complex cases of deÔ¨Ånite pronouns: The winograd
schema challenge. In EMNLP-CoNLL, pages 777‚Äì
789. ACL.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don‚Äôt know: Unanswerable ques-
tions for squad. In ACL (2), pages 784‚Äì789. Associ-
ation for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions for
machine comprehension of text. In EMNLP, pages
2383‚Äì2392. The Association for Computational Lin-
guistics.
Nils Reimers and Iryna Gurevych. 2019.
Sentence-
bert:
Sentence embeddings using siamese bert-
networks.
In EMNLP/IJCNLP (1), pages 3980‚Äì
3990. Association for Computational Linguistics.
Stephen E. Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Found. Trends Inf. Retr., 3(4):333‚Äì389.
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S. Gordon. 2011. Choice of plausible alterna-
tives: An evaluation of commonsense causal reason-
ing. In AAAI Spring Symposium: Logical Formal-
izations of Commonsense Reasoning. AAAI.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In NAACL-HLT, pages 2655‚Äì2671. Asso-
ciation for Computational Linguistics.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2020. Winogrande: An adver-
sarial winograd schema challenge at scale. In AAAI,
pages 8732‚Äì8740. AAAI Press.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
ChafÔ¨Ån, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari,
Canwen Xu,
Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal V. Nayak, De-
bajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-
heesht Sharma, Andrea Santilli, Thibault F√©vry, Ja-
son Alan Fries, Ryan Teehan, Teven Le Scao, Stella
Biderman, Leo Gao, Thomas Wolf, and Alexan-
der M. Rush. 2022. Multitask prompted training en-
ables zero-shot task generalization. In ICLR. Open-
Review.net.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick,
Suzana Ilic,
Daniel Hesslow,
Ro-
man Castagn√©, Alexandra Sasha Luccioni, Fran√ßois
Yvon,
Matthias Gall√©,
Jonathan Tow,
Alexan-
der M. Rush, Stella Biderman, Albert Webson,
Pawan Sasanka Ammanamanchi, Thomas Wang,
Beno√Æt Sagot, Niklas Muennighoff, Albert Villanova
del Moral, Olatunji Ruwase, Rachel Bawden, Stas
Bekman, Angelina McMillan-Major, Iz Beltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-
dro Ortiz Suarez, Victor Sanh, Hugo Lauren√ßon,
Yacine Jernite, Julien Launay, Margaret Mitchell,
Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor
Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,
Ariel Kreisberg Nitzav, Canwen Xu, Chenghao
Mou, Chris Emezue, Christopher Klamm, Colin
Leong, Daniel van Strien, David Ifeoluwa Ade-
lani, and et al. 2022. BLOOM: A 176b-parameter
open-access multilingual language model.
CoRR,
abs/2211.05100.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In EMNLP, pages 1631‚Äì1642. ACL.
James Thorne, Andreas Vlachos, Oana Cocarascu,
Christos Christodoulopoulos, and Arpit Mittal. 2018.
The FEVER2.0 shared task. In Proceedings of the
Second Workshop on Fact Extraction and VERiÔ¨Åca-
tion (FEVER).
Mozes van de Kar, Mengzhou Xia, Danqi Chen, and
Mikel Artetxe. 2022. Don‚Äôt prompt, search! mining-
based zero-shot learning with language models. In
EMNLP, pages 7508‚Äì7520. Association for Compu-
tational Linguistics.
A√§ron van den Oord, Yazhe Li, and Oriol Vinyals.
2018. Representation learning with contrastive pre-
dictive coding. CoRR, abs/1807.03748.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In ICLR
(Poster). OpenReview.net.

Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V. Le. 2022a.
Finetuned
language models are zero-shot learners.
In ICLR.
OpenReview.net.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed H. Chi, Quoc Le, and Denny Zhou.
2022b. Chain of thought prompting elicits reasoning
in large language models. CoRR, abs/2201.11903.
Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2018.
A broad-coverage challenge corpus
for sentence understanding through inference.
In
NAACL-HLT, pages 1112‚Äì1122. Association for
Computational Linguistics.
Zhaofeng Wu, Robert L. Logan IV, Pete Walsh, Ak-
shita Bhagia, Dirk Groeneveld, Sameer Singh, and
Iz Beltagy. 2022. Continued pretraining for better
zero- and few-shot promptability. In EMNLP, pages
4517‚Äì4531. Association for Computational Linguis-
tics.
Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and
Lingpeng Kong. 2023. Compositional exemplars for
in-context learning.
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.
2022. BitÔ¨Åt: Simple parameter-efÔ¨Åcient Ô¨Åne-tuning
for transformer-based masked language-models. In
ACL (2), pages 1‚Äì9. Association for Computational
Linguistics.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really Ô¨Ånish your sentence?
In ACL (1),
pages 4791‚Äì4800. Association for Computational
Linguistics.
Rui Zhang and Joel R. Tetreault. 2019.
This email
could save your life: Introducing the task of email
subject line generation. In ACL (1), pages 446‚Äì456.
Association for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin,
Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-
ter, Daniel Simig, Punit Singh Koura, Anjali Srid-
har, Tianlu Wang, and Luke Zettlemoyer. 2022.
OPT: open pre-trained transformer language models.
CoRR, abs/2205.01068.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siÔ¨Åcation. In NIPS, pages 649‚Äì657.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: paraphrase adversaries from word scram-
bling. In NAACL-HLT (1), pages 1298‚Äì1308. As-
sociation for Computational Linguistics.

Appendices
A
Task Clustering
We use the following datasets for each task cluster.
‚Ä¢ Reading Comprehension:
SQuADv1 (Ra-
jpurkar et al., 2016), BoolQ (Clark et al., 2019),
MultiRC (Khashabi et al., 2018), and OBQA (Mi-
haylov et al., 2018).
‚Ä¢ Closed-book QA: ARC-c/e (Bhakthavatsalam
et al., 2021) and NQ (Kwiatkowski et al., 2019).
‚Ä¢ Paraphrase Detection:
MRPC (Dolan and
Brockett, 2005), QQP (Wang et al., 2019), and
Paws Wiki (Zhang et al., 2019).
‚Ä¢ Natural
Language
Inference:
MNLI-
m/mm (Williams et al., 2018), QNLI (Rajpurkar
et al., 2018), SNLI (Bowman et al., 2015), and
RTE (Bentivogli et al., 2009).
‚Ä¢ Sentiment Analysis:
SST-2 (Socher et al.,
2013), Yelp (Zhang et al., 2015), and Senti-
ment140 (Go et al., 2009).
‚Ä¢ Commonsense Reasoning: COPA (Roemmele
et al., 2011), HellaSwag (Zellers et al., 2019),
and PIQA (Bisk et al., 2020).
‚Ä¢ Coreferenece Resolution: Winogrande (Sak-
aguchi et al., 2020), DPR (Rahman and Ng,
2012), and WSC273 (Levesque et al., 2012).
‚Ä¢ Structure to Text: CommonGen (Lin et al.,
2020), E2ENLG (Dusek et al., 2019), and
DART (Nan et al., 2021).
‚Ä¢ Summarization: AESLC (Zhang and Tetreault,
2019), AGNews (Zhang et al., 2015), and Giga-
word (Napoles et al., 2012).
B
Tuning Details
Hyperparameter
Assignment
Computing Infrastructure
8 V100-32GB GPUs
Number of epochs
3
Batch size per GPU
16
Maximum sequence length
256
Maximum learning rate
1e-5
Optimizer
Adam
Adam epsilon
1e-8
Adam beta weights
0.9, 0.999
Learning rate scheduler
warmup linear
Weight decay
0.0
Warmup steps
1000
Learning rate decay
linear
Table 5: Hyperparameter settings of tuning a prompt
retriever
C
Hallucination Mitigation of ChatGPT
We evaluate ChatGPT‚Äôs performance using its re-
leased API, with the gpt-3.5-turbo-0301 model and
a temperature of 0 for greedy search. Human eval-
uation is conducted to check the accuracy on sam-
pled test examples from each dataset, including
66 from TruthfulQA to follow Bang et al. (2023),
100 from FEVER2.0, and 100 from the scientiÔ¨Åc
subset of Covid-19. As types of these tasks have
no overlap with the types we listed in Appendix A,
we use the retriever trained with all the listed task
types for cross-task and cross-model evaluation.
On the Covid-19 dataset, which requires a
true/false answer to input claims, we observe
vanilla zero-shot prompting often leads to ambigu-
ous answers, which we consider the answer incor-
rect. However, by concatenating retrieved prompts,
primarily focused on natural language inference,
ChatGPT generates the correct answer. An ex-
ample is shown in Figure 7. This suggests the
model has access to necessary information but
struggles with making inferences, possibly due to
RLHF (Christiano et al., 2017) training or inherent
difÔ¨Åculty.
On July 8 2020, the following claim was made: There is a
possibility for some animals to become infected COVID-19
through close contact with infected humans.
Q: Was this claim true or false?
Scientists at the Genome Institute of Singapore (GIS) have
discovered the complete genetic sequence of a coronavirus
isolated from a Singapore patient with SARS. Can we infer
the following? Singapore scientists reveal that SARS virus
has undergone genetic changes. Yes, No, or Maybe? Maybe
On July 8 2020, the following claim was made: There is a
possibility for some animals to become infected COVID-19
through close contact with infected humans.
Q: Was this claim true or false?
0-SHOT
UPRISE
As an AI language model, I cannot determine the veracity of a
claim without additional context or information. However, it is
widely accepted that some animals, such as cats, dogs, and
minks, can become infected with COVID-19 through close
contact with infected humans.
True
Figure 7: Case of the chats of vanilla zero-shot prompt-
ing and UPRISE on Covid-19 dataset, the label comple-
tion is ‚ÄúTrue‚Äù.

TESTING CLUSTER: TASK
Reading Comprehension: SQuADv1 (Rajpurkar et al., 2016)
INPUT INSTRUCTION
Here is a question about this article: As of August 2010, Victoria had 1,548 public schools, 489 Catholic
schools and 214 independent schools. Just under 540,800 students were enrolled in public schools, and
just over 311,800 in private schools. Over 61 per cent of private students attend Catholic schools. More
than 462,000 students were enrolled in primary schools and more than 390,000 in secondary schools.
Retention rates for the Ô¨Ånal two years of secondary school were 77 per cent for public school students and
90 per cent for private school students. Victoria has about 63,519 full-time teachers. What is the answer
to this question: What percentage of private school students go to Catholic schools?
LABEL COMPLETION
61
PROMPT CLUSTER: TASK
Closed-book QA: Natural Questions (Kwiatkowski et al., 2019)
DEMONSTRATION INPUT
What is the answer to this question? what is the ofÔ¨Åcial poverty rate in the us?
DEMONSTRATION ANSWER
In 2015, 13.5%
TESTING CLUSTER: TASK
Reading Comprehension: MultiRC (Khashabi et al., 2018)
INPUT INSTRUCTION
What causes a change in motion? The application of a force. Any time an object changes motion, a force
has been applied. In what ways can this happen? Force can cause an object at rest to start moving. Forces
can cause objects to speed up or slow down. Forces can cause a moving object to stop. Forces can also
cause a change in direction. In short, forces cause changes in motion. The moving object may change its
speed, its direction, or both. We know that changes in motion require a force. We know that the size of
the force determines the change in motion. How much an objects motion changes when a force is applied
depends on two things. It depends on the strength of the force. It also depends on the objects mass. Think
about some simple tasks you may regularly do. You may pick up a baseball. This requires only a very
small force. After reading the above, is ‚ÄúNo‚Äù the correct answer to the question ‚ÄúWould the mass of a
baseball affect how much force you have to use to pick it up?‚Äù?,
LABEL COMPLETION
No
PROMPT CLUSTER: TASK
Natural Language Inference: QNLI (Rajpurkar et al., 2018)
DEMONSTRATION INPUT
Q: What temperature are cask ales stored at before being tapped? A: Typically, when a cask arrives in a
pub, it is placed horizontally on a frame called a ‚Äústillage‚Äù which is designed to hold it steady and at the
right angle, and then allowed to cool to cellar temperature, before being tapped and vented tap is driven
through a (usually rubber) bung at the bottom of one end, and a hard spile or other implement is used
to open a hole in the side of the cask, which is now uppermost. Does the answer correctly answer the
question?
DEMONSTRATION ANSWER
Yes
Table 6: Examples of testing input and target of Reading Comprehension cluster, and the retrieved top-1 demon-
stration from the remaining clusters. The Ô¨Årst example involves statistical questions in both the testing input and
prompt, while the second example requires a binary "Yes" or "No" answer in both the input and prompt.

TESTING CLUSTER: TASK
Closed-book QA: ARC (Bhakthavatsalam et al., 2021)
INPUT INSTRUCTION
Which statement best explains why photosynthesis is the foundation of most food webs? Pick the answer
from these options.
LABEL COMPLETION
Sunlight is the source of energy for nearly all ecosystems.
PROMPT CLUSTER: TASK
Reading Comprehension: OBQA (Mihaylov et al., 2018)
DEMONSTRATION INPUT
roots are a vehicle for absorbing water and nutrients from soil into the plant Which of the following is
likely to reject nutrients from food?
DEMONSTRATION ANSWER
Bamboo
TESTING CLUSTER: TASK
Closed-book QA: Natural Questions (Kwiatkowski et al., 2019)
INPUT INSTRUCTION
Q: When did Taylor Swift‚Äôs Ô¨Årst album release? A:
LABEL COMPLETION
October 24, 2006
PROMPT CLUSTER: TASK
Reading Comprehension: SQuADv1 (Rajpurkar et al., 2016)
DEMONSTRATION INPUT
Beyonc√© In October 2014, Beyonc√© signed a deal to launch an activewear line of clothing with British
fashion retailer Topshop. The 50-50 venture is called Parkwood Topshop Athletic Ltd and is scheduled to
launch its Ô¨Årst dance, Ô¨Åtness and sports ranges in autumn 2015. The line will launch in April 2016. Q:
When will the full line appear?
DEMONSTRATION ANSWER
April 2016
Table 7: Examples of testing input and target of Closed-book QA cluster, and the retrieved top-1 demonstration
from the remaining clusters. In the Ô¨Årst case, both the testing input and the prompt relate to the topic of botany. In
the second case, both the input and prompt involve questions about time and share the topic of American singers
(Taylor Swift and Beyonc√©).

TESTING CLUSTER: TASK
Paraphrase Detection: Paws Wiki (Zhang et al., 2019)
INPUT INSTRUCTION
1.John Barrow Island is a member of the Queen Elizabeth Islands and the Canadian Arctic Archipelago in
the territory of Nunavut. 2.John Barrow Island is a member of the Canadian Arctic Archipelago and the
Queen Elizabeth Islands in the Nunavut area. Are these two sentences paraphrases of each other?
LABEL COMPLETION
No
PROMPT CLUSTER: TASK
Coreference Resolution: DPR (Rahman and Ng, 2012)
DEMONSTRATION INPUT
Consider this sentence: When Mr.Bond, the veterinarian, came to look at the black horse that lay groaning
on the grass, he felt him all over, and shook his head; one of his legs was broken. Are ‚Äúhis‚Äù and ‚Äúthe black
horse‚Äù the same?
DEMONSTRATION ANSWER
Yes
TESTING CLUSTER: TASK
Paraphrase Detection: MRPC (Dolan and Brockett, 2005)
INPUT INSTRUCTION
This integrates with Rational PurifyPlus and allows developers to work in supported versions of Java,
Visual C# and Visual Basic.NET. IBM said the Rational products were also integrated with Rational
PurifyPlus , which allows developers to work in Java, Visual C# and VisualBasic.Net. If the Ô¨Årst sentence
is true, is the second one also true?
LABEL COMPLETION
Yes
PROMPT CLUSTER: TASK
Natural Language Inference: MNLI (Williams et al., 2018)
DEMONSTRATION INPUT
Sentence 1: ‚Äúup on the tidal bulge into a storm‚Äôsbarometric low,‚Äù Sentence 2: ‚ÄúA storm‚Äôs barometric low
was on the tidal bulge.‚Äù If the Ô¨Årst sentence is true, then is the second sentence true? Yes, No, or Maybe?
DEMONSTRATION ANSWER
Yes
Table 8: Examples of testing input and target of Paraphrase Detection cluster, and the retrieved top-1 demonstra-
tion from the remaining clusters. In both cases, the retrieved prompts have similar sentence formats to the testing
input.

TESTING CLUSTER: TASK
Natural Language Inference: MNLI (Williams et al., 2018)
INPUT INSTRUCTION
Here is a premise: ‚ÄúThis site includes a list of all award winners and a searchable database of Government
Executive articles.‚Äù Here is a hypothesis: ‚ÄúThe Government Executive articles housed on the website are
not able to be searched.‚Äù Is it possible to conclude that if the premise is true, then so is the hypothesis?
Yes, No, or Maybe?
LABEL COMPLETION
No
PROMPT CLUSTER: TASK
Paraphrase Detection: MRPC (Dolan and Brockett, 2005)
DEMONSTRATION INPUT
‚ÄúAnd they will learn the meaning of American justice,‚Äù he said to strong and extended applause. ‚Äú The
U.S. will Ô¨Ånd the killers and they will learn the meaning of American justice,‚Äù Bush told the crowd, which
burst into applause. If the Ô¨Årst sentence is true, is the second one also true?
DEMONSTRATION ANSWER
No
TESTING CLUSTER: TASK
Natural Language Inference: QNLI (Rajpurkar et al., 2018)
INPUT INSTRUCTION
Does the sentence ‚ÄúThe symptoms of inÔ¨Çammation are redness, swelling, heat, and pain, which are caused
by increased blood Ô¨Çow into tissue.‚Äù provide a valid answer to the question ‚ÄúWhat causes the symptoms
of inÔ¨Çammation?‚Äù?
LABEL COMPLETION
Yes
PROMPT CLUSTER: TASK
Commonsense Reasoning: COPA (Roemmele et al., 2011)
DEMONSTRATION INPUT
Answer the following question about this sentence: ‚ÄúThe spy discovered the enemy‚Äôs location.‚Äù What is
the cause?
DEMONSTRATION ANSWER
The spy bugged the enemy‚Äôs phone.
Table 9: Examples of testing input and target of Natural Language Inference cluster, and the retrieved top-1
demonstration from the remaining clusters. In the Ô¨Årst case, both the testing input and the prompts share a similar
question format, asking whether something remains true under certain conditions. In the second case, both the
input and prompt ask a question about the logical relationship between cause and effect.

TESTING CLUSTER: TASK
Sentiment Analysis: SST-2 (Socher et al., 2013)
INPUT INSTRUCTION
‚Äúit‚Äôs slow‚Äîvery, very slow.‚Äù How would the sentiment of this sentence be perceived?
LABEL COMPLETION
Negative
PROMPT CLUSTER: TASK
Commonsense Reasoning: COPA (Roemmele et al., 2011)
DEMONSTRATION INPUT
‚ÄúThe man went into denial about the tragedy.‚Äù What is the effect of the preceding sentence?
DEMONSTRATION ANSWER
He refused to talk about it.
TESTING CLUSTER: TASK
Sentiment Analysis: Sentiment140 (Go et al., 2009)
INPUT INSTRUCTION
Reading my kindle2... Love it... Lee childs is good read. How would the sentiment of this tweet be
described?
LABEL COMPLETION
Positive
PROMPT CLUSTER: TASK
Summarization: AESLC (Zhang and Tetreault, 2019)
DEMONSTRATION INPUT
Wit makes its own welcome, and levels all distinctions. No dignity, no learning, no force of character, can
make any stand against good wit. -Ralph. Generate a subject line for this email.
DEMONSTRATION ANSWER
What a wonder is a wonderful wit...
Table 10: Examples of testing input and target of Sentiment Analysis cluster, and the retrieved top-1 demonstration
from the remaining clusters. In both cases, the retrieved prompt shares the same sentiment as the testing input,
facilitating the language model to accurately predict the sentiment.

