Extracting Parallel Sentences with Bidirectional Recurrent Neural
Networks to Improve Machine Translation
Francis Gr´egoire
Universit´e de Montr´eal
francis.gregoire@rd.mila.quebec
Philippe Langlais
Universit´e de Montr´eal
felipe@iro.umontreal.ca
Abstract
Parallel sentence extraction is a task addressing the data sparsity problem found in multilin-
gual natural language processing applications. We propose a bidirectional recurrent neural net-
work based approach to extract parallel sentences from collections of multilingual texts. Our
experiments with noisy parallel corpora show that we can achieve promising results against a
competitive baseline by removing the need of speciﬁc feature engineering or additional exter-
nal resources. To justify the utility of our approach, we extract sentence pairs from Wikipedia
articles to train machine translation systems and show signiﬁcant improvements in translation
performance.
1
Introduction
Parallel corpora are a prerequisite for many multilingual natural language processing (NLP) applica-
tions. Unfortunately, parallel data is available for a relatively small number of language pairs and for
few speciﬁc domains. With the increasing amount of comparable corpora on the World Wide Web, a
potential solution to alleviate the parallel data sparsity issue is to extract parallel sentences from this
more abundant source of information. Therefore, the objective of parallel sentence extraction is to ex-
tract parallel sentences from such comparable corpora to increase the quantity of parallel data and the
range of the covered domains. Comparable corpora can be deﬁned as collections of topic-aligned but
non-sentence-aligned multilingual texts.
Recent advances in deep learning architectures with recurrent neural networks (RNNs) have shown
that they can successfully learn complex mappings from variable-length sequences to continuous vec-
tor representations. While numerous natural language processing tasks have successfully applied those
models, ranging from handwriting generation (Graves, 2013) to machine comprehension (Hermann et
al., 2015), most of the multilingual efforts have been devoted to machine translation (Sutskever et al.,
2014; Cho et al., 2014), although more research interests have been recently devoted to multilingual
semantic textual similarity.1
In this paper, we propose a parallel sentence extraction system to measure the translational equivalence
between sentences in two languages. Our system is based on bidirectional recurrent neural networks that
can learn sentence representations in a shared vector space by explicitly maximizing the similarity be-
tween parallel sentences. In contrast to previous approaches, by leveraging these continuous vector
representation of sentences we remove the need to rely on multiple models and speciﬁc feature engineer-
ing. Experiments on noisy parallel corpora show that our approach outperforms a competitive baseline.
To justify the utility of our approach, we add the sentence pairs extracted from Wikipedia articles to a
parallel corpus to train machine translation systems and show improvements in translation performance.
Our experimental results lead us to believe that our system is a promising tool to create new aligned
multilingual resources.
This work is licensed under a Creative Commons Attribution 4.0 International License.
License details: http://
creativecommons.org/licenses/by/4.0/
1http://alt.qcri.org/semeval2017/task2/
arXiv:1806.05559v2  [cs.CL]  24 Aug 2018

2
Related work
Traditional systems developed to extract parallel sentences from comparable corpora typically rely on
multiples models or metadata from articles structure. Munteanu and Marcu (2005) present a complete
system based on statistical word alignment models and a maximum entropy classiﬁer to automatically
extract sentence pairs from newspaper collections. The authors evaluate the quality of the extracted sen-
tences by showing that they improve the performance of statistical machine translation (SMT) systems.
The overall structure of this approach is still considered state-of-the-art. Adafre and Rijke (2006) is the
ﬁrst work to observe that articles from Wikipedia are likely to generate parallel corpora useful for ma-
chine translation. These two approaches are extended by Smith et al. (2010) where the authors introduce
several new features by exploiting the structure and metadata of Wikipedia article pairs. They use their
augmented set of features in a conditional random ﬁeld and obtain state-of-the-art results on a small set of
20 manually annotated Wikipedia article pairs. The work of Abdul-Rauf and Schwenk (2009) proposes a
different approach, in which they use an SMT system built from a small parallel corpus. Instead of using
a classiﬁer, they translate the source language side of a comparable corpus to ﬁnd candidate sentences on
the target language side. They determine if a translated source sentence and a candidate target sentence
are parallel by measuring the word error rate and the translation error rate. A simpliﬁcation of these
systems is proposed in (Azpeitia et al., 2017), where the similarity between two sentences is deﬁned as
the average of the Jaccard similarity coefﬁcients obtained between sentence token sets and lexical trans-
lations determined by IBM models (Brown et al., 1993). Their approach also combine various features,
such as longest common preﬁx matching, numbers and capitalized truecased tokens.
Chu et al. (2016) train a neural machine translation (NMT) system to generate sentence representations
with the encoder, which are then used as additional features to the system of Munteanu and Marcu (2005).
Similarly, Cristina et al. (2017) study sentence representations obtained from the encoder of an NMT
system to detect new parallel sentence pairs. By comparing cosine similarities, they show that they
can distinguish parallel and non-parallel sentences. A different approach exploiting continuous vector
representations is proposed in Grover and Mitra (2017). After learning word representations using the
bilingual word embeddings model of (Luong et al., 2015), they use a convolutional neural network on a
similarity matrix to classify if a pair of sentences is aligned or not. These approaches are different from
ours, where we use a single end-to-end model to estimate the conditional probability distribution that
two sentences are parallel.
3
Approach
3.1
Negative sampling
As positive examples, we use a parallel corpus C consisting of n parallel sentence pairs

(sS
k , sT
k )
	n
k=1,
where S and T denote the source sentences and target sentences. Since we want a model that learns
differentiable vector representations to distinguish parallel from non-parallel sentences, we use negative
sampling to generate negative examples. Therefore, we randomly sample m non-parallel target sentences
for every positive source sentence, such that (sS
k , sT
j ) for j ̸= k. This process is repeated at the beginning
of each training epoch to allow the model to learn on a larger number of non-parallel sentence pairs.
Hence, our training set contains n(1 + m) triples (sS
i , sT
i , yi), where sS
i = (wS
i,1, . . . , wS
i,N) is a source
sentence of N tokens, sT
i
= (wT
i,1, . . . , wT
i,M) is a target sentence of M tokens, and yi is the label
representing the translation relationship between sS
i and sT
i , so that yi = 1 if (sS
i , sT
i ) ∈C and yi = 0
otherwise.
The advantage of negative sampling is its simplicity. However, relying only on randomness to select
negative sentence pairs makes most of the examples very non-parallel and easy to classify. An interesting
way to generate negative examples would be to replace only a segment of a sentence. Similarly, we
could replace a sentence from a parallel pair with another sentence that is close to it in the vector space.
This would make the problem harder, but potentially could make the classiﬁer stronger. We leave such
investigations as future work.

Figure 1: Graphical illustration of the siamese bidirectional recurrent neural networks. The ﬁnal recur-
rent state of the forward and backward RNNs are concatenated and then fed into a feed-forward neural
network culminating in a sigmoid output layer.
3.2
Model
Our idea is to use RNNs to learn cross-language semantics between sentence pairs to estimate the proba-
bility that they are translations of each other. The proposed model architecture consists of a bidirectional
RNNs (BiRNN) (Schuster and Paliwal, 1997) sentence encoder with recurrent activation functions such
as long short-term memory units (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units
(GRU) (Cho et al., 2014). Since we want vector representations in a shared vector space we use a siamese
network (Bromley et al., 1993) with tied weights, which is equivalent to using a single BiRNN to encode
a pair of sentences into two continuous vector representations. The source and target sentence represen-
tations are then fed into a feed-forward neural network with a sigmoid output layer which calculates the
probability that they are parallel. The architecture of our approach is illustrated in Figure 1.
To avoid repetition and for clarity, we only deﬁne equations of the BiRNN encoding the source sen-
tence. For the target sentence, simply substitute S for T. At each time step t, the token in the i-th
sentence, wS
i,t, deﬁned by its integer index k in the vocabulary V S, is represented as a one-hot vec-
tor, xS
k ∈{0, 1}|V |. This one-hot vector is multiplied with an embedding matrix, ES ∈R|V S|×de, to
get a continuous vector representation of this token, wS
i,t ∈Rde, which serves as an input for the for-
ward and backward recurrent states of the BiRNN encoder, −→
h S
i,t and ←−
h S
i,t. The forward RNN reads the
variable-length sentence and updates its recurrent state from the ﬁrst token until the last one to create a
ﬁxed-size continuous vector representation of the sentence, hS
i,N ∈Rdh. The backward RNN processes
the sentence in reverse. We use the concatenation of the last recurrent state in both directions as a ﬁnal
representation hS
i
= [−→
h S
i,N ; ←−
h S
i,1].2 The steps we described to encode a sentence are deﬁned as:
wS
i,t = ES⊤wS
k ,
(1)
−→
h S
i,t = φ(−→
h S
i,t−1, wS
i,t),
(2)
←−
h S
i,t = φ(←−
h S
i,t+1, wS
i,t),
(3)
where φ(·) is an LSTM or GRU.
After both source and target sentences have been encoded, we capture their matching information
by using their element-wise product and absolute element-wise difference. We estimate the conditional
2We considered combining the recurrent states with mean pooling and max pooling to obtain a ﬁxed-size vector representa-
tion, but obtained inferior performance.

probability that the sentences are parallel by feeding the matching vectors into a feed-forward neural
network with a sigmoid output layer:
h(1)
i
= hS
i ⊙hT
i ,
(4)
h(2)
i
= |hS
i −hT
i |,
(5)
hi = tanh(W(1)h(1)
i
+ W(2)h(2)
i
+ b),
(6)
p(yi = 1|hi) = σ(vhi + b),
(7)
where σ(·) is the sigmoid function, W(1) ∈Rdf×dh, W(2) ∈Rdf×dh, v ∈Rdf , b ∈Rdf and b are
model parameters. The value df is the size of the hidden layer in the feed-forward neural network.
The model is trained by minimizing the cross entropy of our labeled sentence pairs:
L = −
n(1+m)
X
i=1

yi log σ(vhi + b) + (1 −yi) log(1 −σ(vhi + b))

.
(8)
For prediction, a sentence pair is classiﬁed as parallel if its probability is greater than or equal to a
decision threshold ρ that we need to ﬁx:
ˆyi =
(
1
if p(yi = 1|hi) ≥ρ,
0
otherwise.
(9)
The two embedding matrices ES and ET are parameters of the model that we must learn. As the size
of the vocabulary and the dimension de increase, the number of parameters can become considerably
expensive to estimate. For that reason, it is common to initialize the embedding matrices using word
embeddings pre-trained on a large collection of texts.3
4
Experiments and Results
To assess the effectiveness of our approach, we compare it in multiple settings. In Section 4.1, we
measure its capacity to extract parallel sentences found in parallel corpora in which we inserted non-
parallel sentences. We extract sentence pairs from real comparable corpora in Section 4.2 and validate
their utility by measuring their impact on SMT and NMT systems.
4.1
System comparison
4.1.1
Data
To create our training and test sets we use the WMT’15 English–French datasets.4 The positive examples
of our training set consist of 500,000 parallel sentence pairs randomly selected from the Europarl cor-
pus (Koehn, 2005). To generate negative examples, at the beginning of each training epoch we sample m
non-parallel sentences per positive example, as described in Section 3.1. The vocabulary size is 69,381
for English and 84,182 for French.
The most reliable way to create test sets to compare different approaches would be to have profes-
sional translators manually annotate parallel sentences from comparable corpora. However, this option
is expensive and impractical. Therefore, it is common practice to compare parallel sentence extrac-
tion systems on artiﬁcially-created noisy parallel data by inserting non-parallel sentences into a parallel
corpus. Thus, to create our test sets we use parallel sentences from the newstest2012 and Europarl cor-
pora and insert artiﬁcial noise by substituting target sentences with non-parallel target sentences from
their respective held-out dataset. We sample 1,000 parallel sentences from the newstest2012 corpus and
create ten test sets with a noise ratio r ∈{0%, 10%, . . . , 80%, 90%}, where r is the ratio of artiﬁcial
3We pre-trained bilingual word embeddings as proposed by (Gouws et al., 2015) and (Smith et al., 2017). While we
observed faster learning and a small improvement in the performance of our model, we prefer to learn the embedding matrices
from scratch instead of relying on additional external resources.
4http://www.statmt.org/wmt15/translation-task.html

non-parallel sentences. The same process is repeated with the Europarl corpus to create three test sets
with r ∈{0%, 50%, 90%}. Each ﬁnal test set consists of 1,000,000 sentence pairs generated from the
Cartesian product between the sentences in both languages. For example, 400 out of the 1,000 sentence
pairs are parallel if r = 60%, in which only 0.04% of the 1,000,000 sentence pairs generated from the
Cartesian product are truly parallel.
We tokenize all datasets with the scripts from Moses (Koehn et al., 2007).5 The maximum sentence
length is set to 80 tokens. Each out-of-vocabulary word is mapped to a special UNK token.
4.1.2
Evaluation metrics
For evaluating the performance of our models, a sentence pair predicted as parallel is correct if it is
present in the set of parallel sentences of the test set. Precision is the proportion of truly parallel sentence
pairs among all extracted sentence pairs. Recall is the proportion of truly parallel extracted sentence
pairs among all parallel sentence pairs in the test set. The F1 score is the harmonic mean of precision and
recall.
4.1.3
Baseline
For comparison, we use a parallel sentence extraction system developed in-house based on the works
of (Munteanu and Marcu, 2005) and (Smith et al., 2010). The system consists of a candidate sentence
pair ﬁltering process and three models; two word alignment models and a maximum entropy classiﬁer.
The word alignment models are trained on both language directions using our training set of 500,000
parallel sentence pairs. To train the classiﬁer, we select another 100,000 parallel sentence pairs from the
held-out Europarl dataset. To generate negative examples, we use 100,000 non-parallel sentence pairs
that have successfully passed the candidate sentence pair selection process.6
Candidate sentence pair selection A sentence pair ﬁltering process is used to select a ﬁxed number
of negative sentence pairs to train the maximum entropy classiﬁer. The objective is to select similar
non-parallel sentence pairs to make the classiﬁer more robust to noise. It is also used during prediction
to ﬁlter out the unlikely sentence pairs of the Cartesian product. Since most of these sentence pairs are
not parallel, the ﬁltering process signiﬁcantly reduces the number of sentence pairs to evaluate. The
ﬁltering process consists of a two-step procedure. First, it veriﬁes that the ratio of the lengths between
two sentences is not greater than 2.7 It then uses a word-overlap ﬁlter to check for both sentences that
at least 50% of their words have a translation in the other sentence. Every pair that does not fulﬁll these
two conditions is discarded. The ﬁltering process is only applied to the baseline model.
Word alignment models The translation and alignment tables are estimated using the HMM alignment
model of (Vogel et al., 1996). These probability tables are required to calculate the value of many
alignment features used in the classiﬁer.
Maximum entropy classiﬁer The classiﬁer relies on word-level alignment features between two sen-
tences, such as the number of connected words, top three largest fertilities, length of the longest con-
nected substring, log probability of the alignments, and also general features, such as the lengths of the
sentences, length difference and the percentage of words on each side that have a translation on the other
side. For each sentence pair, a total of 31 features must be calculated and it is classiﬁed as parallel if the
classiﬁer outputs a probability score greater than or equal to a decision threshold ρ which needs to be
ﬁxed.
4.1.4
Training settings
The models of our approach are implemented in TensorFlow (Abadi et al., 2016). We use a BiRNN with
a single layer in each direction with 512-dimensional word embeddings and 512-dimensional recurrent
states. We use LSTM as recurrent activation functions. The hidden layer of the feed-forward neural
network has 256 hidden units. To train our models, we use Adam (Kingma and Ba, 2014) with a learning
rate of 0.0002 and a minibatch of 128 examples. Models are trained for a total of 15 epochs. To avoid
5https://github.com/moses-smt/mosesdecoder
6During our experiments, we did not observe any signiﬁcant gain by using more than 100,000 parallel sentence pairs.
7Most parallel sentence extraction systems use a length ratio value equals to 2.

Figure 2: F1 scores of BiRNN as we increase the number of negative examples m per parallel sentence
in the training set. The models are evaluated on newstest2012 with noise ratios of 0%, 50% and 90%.
exploding gradients, we apply gradient clipping such that the norm of all gradients is no larger than
5 (Pascanu et al., 2013). We apply dropout to prevent overﬁtting with a probability of 0.2 and 0.3 for the
non-recurrent input and output connections respectively (Zaremba et al., 2014).
4.1.5
Number of negative examples
We evaluate the performance of our approach as we increase the number of negative examples m per
parallel sentence pair in our training set. We trained a total of 10 models with m ∈{1, . . . , 10}, such
that with m = 1 and m = 10 a model is respectively trained on 1,000,000 and 5,500,000 sentence pairs
per epoch, with a positive to negative sentence pairs ratio of 50% and 9%. The more the value of m
increases, the more the training set becomes unbalanced. We know that extracting parallel sentences
from comparable corpora in practice is an unbalanced classiﬁcation task in which non-parallel sentences
represent the majority class. Although an unbalanced training set is not desired since a classiﬁer trained
on such data will typically tend to predict the majority class and have a poor precision, the overall impact
on the performance of BiRNN is not clear. Figure 2 shows the F1 scores of BiRNN with respect to the
value of m evaluated on newstest2012 with noise ratios of 0%, 50% and 90%. Each reported F1 score is
the one calculated at the decision threshold value maximizing the area under the precision-recall curve.
For m ≥7, we observe a deterioration in the performance of our systems evaluated on test sets with
noise ratios of 0% and 50%, whereas m = 6 is the best performing model on the test set with a noise
ratio of 90%. We see that having a balanced training set with m = 1 is not the optimal solution for
our approach. On the contrary, having an unbalanced training set improves its performance. From these
observations, in the following experiments we train our models with a value of m ﬁxed at 6.8
4.1.6
Evaluation on noisy parallel corpora
In this experiment, we compare our approach to the baseline system described in Section 4.1.3. Table 1
shows the precision, recall and F1 scores for the two systems evaluated on the newstest2012 and Europarl
test sets with noise ratios of 0%, 50% and 90%. We see that BiRNN is able to constantly outperform the
results obtained with the baseline by a signiﬁcant margin. By using newstest2012 as out-of-domain test
sets, our approach gets an absolute improvement in the F1 score of 10.99%, 13.03% and 23.53%. The
precision-recall curves of the two systems evaluated on test sets with noise ratios of 0% and 90% are
shown in Figure 3. These curves illustrate the consistency of our approach, whereas the performance of
the baseline is greatly impacted when the number of non-parallel sentences in the test set is high.
When BiRNN is evaluated on Europarl, it obtains F1 scores over 95% on the three test sets. On
the other hand, given that the underlying models of the baseline system are trained on Europarl data,
it is surprising to observe again a signiﬁcant deterioration in its performance when the number of non-
parallel sentences increases. In Figure 4, we compare the precision, recall and F1 scores as the noise
ratio r applied to our newstest2012 test set increases. We observe that it becomes harder to identify
8It takes about 1.5 hours to train an epoch with the model settings described in 4.1.4 using a single GTX 1080 Ti GPU.

newstest2012
Europarl
Noise
Model
P (%)
R (%)
F1 (%)
ρ
P (%)
R (%)
F1 (%)
ρ
0%
BiRNN
83.72
68.90
75.79
0.99
99.26
93.50
96.29
0.99
Baseline
73.88
57.70
64.80
0.93
87.72
84.30
85.98
0.98
50%
BiRNN
79.95
65.40
71.95
0.99
98.32
93.60
95.90
0.99
Baseline
73.43
49.20
58.92
0.98
80.19
82.60
81.38
0.99
90%
BiRNN
79.01
64.00
70.72
0.99
97.94
95.00
96.45
0.99
Baseline
53.85
42.00
47.19
0.99
63.89
69.00
66.35
0.99
Table 1: Precision (P), recall (R) and F1 scores where the decision threshold ρ maximizes the area under
the precision-recall curve of the test sets with noise ratios of 0%, 50% and 90%.
Figure 3: Precision-recall curves of the systems evaluated on newstest2012 with noise ratios of 0% (left)
and 90% (right).
parallel sentences as the number of non-parallel sentences increases in the test set. However, we see
that our neural network based approach obtains better performances by a signiﬁcant margin. In contrast
to the baseline, the performance of our method stays relatively stable and starts to degrade at very high
noise ratios. At that level of noise, we believe our test set is more representative to texts found in real
comparable corpora.
In this experiment, all our evaluation metrics (BiRNN and Baseline) are reported at the value ρ which
maximizes the area under a precision-recall curve. These performances are therefore upper bound results.
The decision threshold has a direct impact on the quality and the quantity of the extracted sentence pairs.
Through our experiments, we observed that the optimal value ρ of BiRNN was constantly around a value
of 0.99, while the one of the baseline system varied considerably from one test set to another. Thus, the
stability of our approach is preferable in practice. We believe that the high optimal decision threshold
value ρ = 0.99 is caused by the fact that we use a sigmoid output layer with highly unbalanced training
and test sets.
Figure 4: Precision (left), recall (middle) and F1 (right) scores of the systems evaluated on newstest2012
as the noise ratio r increases.

4.2
Machine Translation Comparison
4.2.1
Data
As training sets, we use the English–French Europarl corpus and sentence pairs extracted by the two
parallel sentence extraction systems. The comparable corpora used to extract parallel sentences are
919,000 pairs of English–French articles obtained from Wikipedia dumps.9 The newstest2012 corpus
is our validation set and we evaluate the machine translation systems on the newstest2013 corpus. All
datasets are tokenized using the scripts from Moses. The maximum length of each sentence is set to 80
tokens. For the NMT systems, the vocabulary size for both languages is limited to the 150,000 most
frequent words of their corresponding training set. All unknown words are replaced by the UNK token.
4.2.2
Evaluation metric
To evaluate the translation performance of our systems we use the BLEU score (Papineni et al., 2002)
using the multi-bleu script from Moses.
4.2.3
Training settings
The SMT systems are phrase-based systems (Koehn et al., 2003) that are trained with Moses. We use
the GIZA++ implementation (Och and Ney, 2003)10 to train word alignment models in both directions.
The phrases and lexical reordering are extracted using the default values of Moses. The language models
are 5-gram models learned using the KenLM toolkit (Heaﬁeld, 2011)11 on the monolingual parts of
the same parallel corpus used for training the translation models. The parameters are optimized on the
newstest2012 corpus.
To train the NMT systems, we use the PyTorch implementation of OpenNMT (Klein et al., 2017).12
The NMT systems are one layer BiLSTMs with an attention mechanism (Bahdanau et al., 2014). The
dimensions of the word embeddings and recurrent states for the encoder and decoder are all set to 256.
The systems are trained for 10 epochs with minibatch of 64 sentence pairs using SGD with an initial
learning rate of 1.0 and linear decay.13 The norm of the gradient is clipped such that it is not greater than
5.
4.2.4
Evaluation on machine translation systems
For each of the SMT and NMT approaches, we trained 14 machine translation systems. The ﬁrst two
systems,14 which serve as reference systems, are trained with 500,000 and 2,000,000 parallel sentences
from the Europarl corpus, respectively. The system trained with 2,000,000 parallel sentences is used
to compare the beneﬁts of the sentence pairs extracted from Wikipedia against a large training cor-
pus that we easily have at hand. For the 12 remaining systems, we sort the sentence pairs extracted
by an extraction system in descending order according to the probability scores and append the top
{250000, 500000, . . . , 1250000, 1500000} sentence pairs to the training set containing 500,000 parallel
sentences from the Europarl corpus.
Table 2 shows the BLEU scores obtained by the machine translation systems we trained for the SMT
and NMT approaches. The ﬁgures in parentheses show the absolute gains in BLEU score compared to the
reference systems trained with 500,000 parallel sentences from the Europarl corpus.15 We see that adding
the subsets of sentence pairs extracted by both extraction systems leads to signiﬁcant gains compared to
the reference systems. Adding 1,500,000 sentence pairs extracted by BiRNN improves the BLEU score
of the SMT and NMT systems by 3.71 and 9.47, respectively. We observe that translation systems trained
on 1,000,000 sentence pairs have better BLEU scores than the reference systems trained on 2,000,000
parallel sentences from Europarl, showing that parallel sentence extraction allows translation systems to
9https://dumps.wikimedia.org/
10http://www.statmt.org/moses/giza/GIZA++.html
11https://github.com/kpu/kenlm
12https://github.com/OpenNMT/OpenNMT-py
13We could have trained NMT systems for more epochs to obtain better BLEU scores, but our objective is not to compare
SMT with NMT systems.
14That is, the two ﬁrst systems for each approach and not from the total of 28 machine translation systems.
15Which is the ﬁrst line of Table 2.

BLEU
Data
Model
SMT
NMT
Pairs
Europarl
21.47
17.63
500k
23.18
25.36
2M
+Top250k
BiRNN
23.11 (+1.64)
24.44 (+6.81)
750k
Baseline
23.09 (+1.62)
24.22 (+6.59)
750k
+Top500k
BiRNN
24.12 (+2.65)
25.93 (+8.30)
1M
Baseline
23.96 (+2.49)
25.82 (+8.19)
1M
+Top750k
BiRNN
24.53 (+3.06)
26.39 (+8.76)
1.25M
Baseline
24.44 (+2.97)
26.19 (+8.56)
1.25M
+Top1M
BiRNN
24.85 (+3.38)
26.64 (+9.01)
1.5M
Baseline
24.66 (+3.19)
26.59 (+8.96)
1.5M
+Top1.25M
BiRNN
25.01 (+3.54)
26.80 (+9.17)
1.75M
Baseline
24.72 (+3.25)
26.64 (+9.01)
1.75M
+Top1.5M
BiRNN
25.18 (+3.71)
27.10 (+9.47)
2M
Baseline
25.01 (+3.54)
26.85 (+9.22)
2M
Table 2: BLEU scores obtained by the SMT and NMT systems on newstest2013. Pairs is the number of
sentence pairs in a training set. The numbers in parentheses are the translation gains with respect to the
reference system trained with 500,000 sentence pairs from the Europarl corpus.
generalize better with less data. We are surprised that the BLEU scores obtained with the BiRNN data
are so close to those obtained with the data from the baseline system. This may indicate that article
pairs from Wikipedia share a relatively high degree of similarity. Still, the gain obtained with BiRNN
compared to the baseline extraction system is consistent for all machine translation systems. These
results conﬁrm the quality of the 1,500,000 extracted sentence pairs. Hence, we could reduce the value
of the decision threshold in order to extract corpora of larger sizes. Given the out-of-domain nature of the
Wikipedia article pairs, we see that our approach could be applied to comparable corpora with a lower
degree of comparability.
5
Discussion
Most parallel sentence extraction systems developed so far were based on a series of models that required
the computation of several features that did not necessarily adapt well to different text structures. To
alleviate this problem, we have shown that we can create new parallel corpora from a collection of texts
in different languages with a single end-to-end model based on bidirectional recurrent neural networks.
Our approach deals directly with raw sentence pairs to determine if two sentences are a translation of
each other. In addition to being more ﬂexible than traditional systems, we demonstrate that our approach
obtains better results than a competitive baseline system. We found that new sentence pairs extracted
from comparable corpora is a considerable resource to exploit for SMT and NMT systems.
Our parallel sentence extraction system is completely based on neural network models and we be-
lieve that our work enables exploration for researchers who want to apply future research ideas. In this
study we did not consider out-of-vocabulary words. Therefore, it would be beneﬁcial to use word seg-
ments (Sennrich et al., 2015) as lexical units instead of words to limit the size of the vocabulary and to be
more robust when handling out-of-domain texts found in comparable corpora. The main challenge with
our approach (and the majority of parallel sentence extraction systems) is that it needs to be trained on
a parallel corpus. Thus, we need parallel sentences to extract parallel sentences, which can be problem-
atic. Despite that it is possible to start from a small parallel corpus and to bootstrap the learning process
with extracted sentence pairs, this method is limited for the many language pairs having few resources
available. In order to lessen the need of a parallel corpus, an interesting and promising avenue could be

to apply methods from some of the recent works (Xia et al., 2016; Conneau et al., 2017; Artetxe et al.,
2017) which allow machine translation systems to learn directly from non-parallel texts. Although our
simple neural network architecture was able to extract parallel sentence pairs from comparable corpora,
we believe that more sophisticated approaches, such as using two different sentence encoders trained by
mutual information neural estimation (MINE) (Belghazi et al., 2018) could improve the ﬂexibility and
the performance of our approach. Even though we believe that our approach is easily scalable across
multiple language pairs, in this paper we only evaluated it on English–French texts. Therefore, it would
be interesting to study a larger set of language pairs. In addition, we emphasize that it would be pertinent
to evaluate our approach with distant language pairs which are poor in parallel resources. Our code is
available on GitHub and the extracted parallel corpora will be released to the public.16
Acknowledgements
This research was partially supported by the Natural Sciences and Engineering Research Council of
Canada.
References
[Abadi et al.2016] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2016. Tensorﬂow: A system for large-scale machine learning. In Proceedings
of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI’16, pages 265–283,
Berkeley, CA, USA. USENIX Association.
[Abdul-Rauf and Schwenk2009] Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the use of comparable corpora
to improve smt performance. In Proceedings of the 12th Conference of the European Chapter of the Association
for Computational Linguistics, EACL ’09, pages 16–23, Stroudsburg, PA, USA. Association for Computational
Linguistics.
[Adafre and Rijke2006] Sisay Fissaha Adafre and Maarten de Rijke. 2006. Finding similar sentences across multi-
ple languages in wikipedia. In Proceedings of the Workshop on NEW TEXT Wikis and blogs and other dynamic
text sources.
[Artetxe et al.2017] Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2017. Unsupervised neural
machine translation. CoRR, abs/1710.11041.
[Azpeitia et al.2017] Andoni Azpeitia, Thierry Etchegoyhen, and Eva Mart´ınez Garcia.
2017.
Weighted set-
theoretic alignment of comparable sentences. In Proceedings of the 10th Workshop on Building and Using
Comparable Corpora, pages 41–45. Association for Computational Linguistics.
[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation
by jointly learning to align and translate. CoRR, abs/1409.0473.
[Belghazi et al.2018] Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R. Devon Hjelm, and Aaron C. Courville.
2018. MINE: mutual information neural estimation. CoRR, abs/1801.04062.
[Bromley et al.1993] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. 1993.
Signature veriﬁcation using a ”siamese” time delay neural network. In Proceedings of the 6th International
Conference on Neural Information Processing Systems, NIPS’93, pages 737–744, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
[Brown et al.1993] Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation: Parameter estimation. Comput. Linguist., 19(2):263–311,
June.
[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer, C¸ aglar G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine trans-
lation. CoRR, abs/1406.1078.
16https://github.com/FrancisGregoire/parSentExtract

[Chu et al.2016] Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2016. Parallel sentence extraction from compa-
rable corpora with neural network features. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno,
Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference on Language Re-
sources and Evaluation (LREC 2016), Paris, France, may. European Language Resources Association (ELRA).
[Conneau et al.2017] Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv´e
J´egou. 2017. Word translation without parallel data. CoRR, abs/1710.04087.
[Espa˜na-Bonet et al.2017] Cristina Espa˜na-Bonet, ´Ad´am Csaba Varga, Alberto Barr´on-Cede˜no, and Josef van Gen-
abith. 2017. An empirical analysis of nmt-derived interlingual embeddings and their use in parallel sentence
identiﬁcation. CoRR, abs/1704.05415.
[Gouws et al.2015] Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual distributed
representations without word alignments. In Proceedings of the 32Nd International Conference on International
Conference on Machine Learning - Volume 37, ICML’15, pages 748–756. JMLR.org.
[Graves2013] Alex Graves. 2013. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850.
[Grover and Mitra2017] Jeenu Grover and Pabitra Mitra. 2017. Bilingual word embeddings with bucketed cnn for
parallel sentence extraction. In Proceedings of ACL 2017, Student Research Workshop, pages 11–16. Associa-
tion for Computational Linguistics.
[Heaﬁeld2011] Kenneth Heaﬁeld. 2011. Kenlm: Faster and smaller language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation, pages 187–197. Association for Computational Linguistics.
[Hermann et al.2015] Karl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,
Mustafa Suleyman, and Phil Blunsom.
2015.
Teaching machines to read and comprehend.
CoRR,
abs/1506.03340.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory.
Neural Compututation, 9(8):1735–1780, November.
[Kingma and Ba2014] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.
CoRR, abs/1412.6980.
[Klein et al.2017] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush.
2017.
Opennmt: Open-source toolkit for neural machine translation. CoRR, abs/1701.02810.
[Koehn et al.2003] Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages
177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.
[Koehn2005] Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.
[Luong et al.2015] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Bilingual word represen-
tations with monolingual quality in mind. In NAACL Workshop on Vector Space Modeling for NLP, Denver,
United States.
[Munteanu and Marcu2005] Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation
performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504, December.
[Och and Ney2003] Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical
alignment models. Comput. Linguist., 29(1):19–51, March.
[Papineni et al.2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for
automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational
Linguistics.

[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difﬁculty of training
recurrent neural networks. In Proceedings of the 30th International Conference on International Conference on
Machine Learning - Volume 28, ICML’13, pages III–1310–III–1318. JMLR.org.
[Schuster and Paliwal1997] Mike. Schuster and Kuldip K. Paliwal. 1997. Bidirectional recurrent neural networks.
Trans. Sig. Proc., 45(11):2673–2681.
[Sennrich et al.2015] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of
rare words with subword units. CoRR, abs/1508.07909.
[Smith et al.2010] Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from
comparable corpora using document level alignment. In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages
403–411, Stroudsburg, PA, USA. Association for Computational Linguistics.
[Smith et al.2017] Samuel L. Smith, David H. P. Turban, Steven Hamblin, and Nils Y. Hammerla. 2017. Ofﬂine
bilingual word vectors, orthogonal transformations and the inverted softmax. CoRR, abs/1702.03859.
[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with
neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Sys-
tems, NIPS’14, pages 3104–3112, Cambridge, MA, USA. MIT Press.
[Vogel et al.1996] Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment
in statistical translation.
In Proceedings of the 16th Conference on Computational Linguistics - Volume 2,
COLING ’96, pages 836–841, Stroudsburg, PA, USA. Association for Computational Linguistics.
[Xia et al.2016] Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. 2016.
Dual learning for machine translation. CoRR, abs/1611.00179.
[Zaremba et al.2014] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regu-
larization. CoRR, abs/1409.2329.

