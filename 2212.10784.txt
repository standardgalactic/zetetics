Can NLI Provide Proper Indirect Supervision for
Low-resource Biomedical Relation Extraction?
Jiashu Xu
Mingyu Derek Ma
Muhao Chen
Harvard University
University of California, Los Angeles
University of Southern California
jxu1@harvard.edu
ma@cs.ucla.edu muhaoche@usc.edu
Abstract
Two key obstacles in biomedical relation ex-
traction (RE) are the scarcity of annotations
and the prevalence of instances without explic-
itly pre-defined labels due to low annotation
coverage. Existing approaches, which treat
biomedical RE as a multi-class classification
task, often result in poor generalization in low-
resource settings and do not have the ability to
make selective predictions on unknown cases
but give a guess from seen relations, hinder-
ing the applicability of those approaches. We
present NBR, which converts biomedical RE
as a natural language inference formulation to
provide indirect supervision. By converting re-
lations to natural language hypotheses, NBR
is capable of exploiting semantic cues to al-
leviate annotation scarcity. By incorporating
a ranking-based loss that implicitly calibrates
abstinent instances, NBR learns a clearer de-
cision boundary and is instructed to abstain on
uncertain instances. Extensive experiments on
three widely-used biomedical RE benchmarks,
namely ChemProt, DDI, and GAD, verify the
effectiveness of NBR in both full-shot and low-
resource regimes. Our analysis demonstrates
that indirect supervision benefits biomedical
RE even when a domain gap exists, and com-
bining NLI knowledge with biomedical knowl-
edge leads to the best performance gains.1
1
Introduction
In silico studies of biology and medicine have pri-
marily relied on machines’ understanding of rela-
tions between various molecules and biomolecules.
For instance, disease-target prediction requires ac-
curate identification of the association between the
drug target and the disease (Bravo et al., 2015),
and drug-drug interaction recognition is essential
for polypharmacy side effect studies (Herrero-Zazo
et al., 2013). Due to the complexity and high cost
of human curation of such biomedical knowledge
1Code is released at https://github.com/luka-group/
NLI_as_Indirect_Supervision
(Krallinger et al., 2017; Bravo et al., 2015), there
has been a growing interest in the field of biomedi-
cal relation extraction (RE), a task of automatically
inferring the relations between biomedical entities
described in domain-specific corpora.
However, two obstacles remain in training a reli-
able biomedical RE model. First, biomedical RE
often suffers from insufficient and imperfect anno-
tations, due to that the annotation process is very
challenging and requires expert annotators to iden-
tify complex structures from lengthy and sophisti-
cated biomedical literature. The existing biomed-
ical learning resources either require very costly
expert annotations (Krallinger et al., 2017) or resort
to weak supervision (Bravo et al., 2015). The insuf-
ficiency and imperfection of annotations inevitably
cause existing state-of-the-art (SOTA) biomedical
RE systems (Yasunaga et al., 2022; Peng et al.,
2019; Tinn et al., 2021, inter alia), though showing
satisfactory results in a fully supervised setting, to
result in poor generalization regarding the more
common low-resource regime in this domain. For
example, Han et al. (2018) showed that model per-
formance deteriorated quickly as the number of
instances for each relation drops, hindering the ap-
plicability of those approaches in real-world scenar-
ios. Second, given that biomedical RE annotations
tend to be incomplete or have low coverage, it is
difficult for models to learn a clear decision bound-
ary (Gardner et al., 2020). Specifically, in many
scenarios where the described biomedical entities
are not related in the context, the model may fail to
abstain but give a guess from seen relations (Xin
et al., 2021; Kamath et al., 2020). An overconfident
model can be particularly harmful in high-stakes
fields such as medicine, where incorrect predictions
can have severe direct consequences for patients.
Recently, indirect supervision (Roth, 2017; He
et al., 2021; Levy et al., 2017; Lu et al., 2022; Li
et al., 2019) is proposed that leverages supervision
signals from resource-rich source tasks to enhance
arXiv:2212.10784v2  [cs.CL]  23 May 2023

Androgen antagonistic effect of estramustine phosphate (EMP) 
metabolites on wild-type and mutated androgen receptor.
@CHEMICAL$ and @GENE$ have no relation.
@CHEMICAL$ is identified as an antagonist of @GENE$.
Upregulator @CHEMICAL$ is activated by @GENE$.
Hypothesis:
Entailment score
Positive
Rank Over
Entailment score
Neg
Neg
Input
(1) Verbalizer
(3) Inference 
(2) Training 
Androgen antagonistic effect of @CHEMICAL$ (EMP) 
metabolites on wild-type and mutated @GENE$.
Premise = masked input
Figure 1: Overview of NBR. Given an input, (1) each relation is verbalized into natural language hypotheses
and masked input where entity mentions are type-masked becomes the premise. The ground-truth is marked in
light color . (2) For training NBR calculates the entailment scores for each relation candidate and optimizes the
score of the ground-truth relation to rank over the scores of other candidates. (3) For inference NBR computes
entailment scores of each relation and returns the one with the maximum entailment score.
resource-limited target tasks. In this approach, the
training and inference pipeline of the target task is
transformed into the formulation of the source task,
thus introducing additional supervision signals not
accessible in the target task. Recent works (Li et al.,
2022; Yin et al., 2020; Sainz et al., 2021) transfer
cross-task learning signals from the Natural Lan-
guage Inference (NLI) task. The NLI task aims
at determining whether the hypothesis can be en-
tailed given the premise, and inductive bias of NLI
models learns adaptive generalized logical reason-
ing which aligns well with the goal of biomedical
RE. On the other hand, traditional direct supervi-
sion on the biomedical RE fails to capture seman-
tic information of relations since they are merely
transformed to logits of a classifier. By converting
relations to meaningful hypotheses in NLI, the in-
directly supervised method bypasses this shortage
and can adapt the the preexisting inductive bias
of NLI-finetuned models to make meaningful pre-
dictions based on relation semantics (Huang et al.,
2022; Chen et al., 2020). This critically benefits
the generalizability of the model in low-resource
regimes where limited direct supervision signals
are provided (Sainz et al., 2021) to remedy insuffi-
cient annotations. However, previous studies focus
on general domain tasks and explore little in spe-
cific domains such as biomedical. Moreover, to
maximize the utility of indirect supervision, it is
found that incorporating task knowledge into the
model, i.e. NLI model that is trained on NLI data,
yields the best performance (Li et al., 2022; Sainz
et al., 2021). Yet, biomedical NLI is rarely avail-
able and whether general domain NLI can provide
strong indirect supervising signals to specific target
domains remains unexplored.
This study presents a general learning frame-
work, dubbed NLI improved Biomedical Relation
Extraction (NBR), to enhance biomedical RE with
indirect supervision from general domain NLI task.
Fig. 1 illustrates the structure of NBR. Specifically,
given an input sentence, NBR reformulates RE
to NLI by treating the input as the premise while
verbalizing each relation label into template-based
natural language hypotheses. NBR learns to rank
the relations based on the entailment scores such
that the hypothesis of a correct relation should be
scored higher than those of any incorrect ones. Fur-
thermore, to learn a fine-grained, instance-aware
decision boundary, NBR deploys ranking-based
loss for implicit abstention calibration that handles
abstinent relations in the dataset. During inference,
the relation whose verbalized hypothesis achieved
the highest score becomes the prediction. NBR
fully exploits indirect supervision from NLI and
performs exceptionally well even in low-resource
scenarios.
Our contributions are three-fold: First, to the
best of our knowledge, this is the first work to
leverage indirect supervision from NLI on biomed-
ical RE. Instead of solely relying on provided RE
annotations, NBR leverages additional supervi-
sion signals from NLI indirect supervision and
can generalize well in low resource regimes. Sec-

ond, we show that NBR provides a proper indi-
rect supervision signal even if there is a domain
gap between general NLI knowledge NBR trained
on and biomedical downstream task. Third, we
propose a new ranking-based loss that implicitly
handles abstinent relations ubiquitous in biomed-
ical RE by contrastively calibrating the score of
abstinent instances.
By extensive experiments
on three commonly-used biomedical RE bench-
marks, namely, ChemProt (Krallinger et al., 2017),
DDI (Herrero-Zazo et al., 2013) and GAD (Bravo
et al., 2015), we verify our contributions and show
that general domain NLI can provide a proper su-
pervision signal, especially in low resource set-
tings where annotations are scarce. NBR provides
consistent improvements on three datasets (1.10,
1.79, and 0.96 points of F1 improvement respec-
tively), and up to 34.25 points of F1 improvement
in low-resource settings. Further analysis demon-
strates that combing NLI knowledge with biomedi-
cal knowledge leads to the best performance gains.
2
Related Works
Biomedical relation extraction. Despite the grow-
ing availability of biomedical corpora on Web
repositories, the main challenge remains in trans-
forming those unstructured textual data into a
rigidly-structured representation that includes in-
terested entities and relations between them (Peng
et al., 2019; Lee et al., 2020; Tinn et al., 2021).
However, knowledge curation for this purpose
is often costly and requires expert involvement
(Krallinger et al., 2017; Herrero-Zazo et al., 2013;
Bravo et al., 2015). To address this issue, biomedi-
cal RE techniques are developed to automate this
process. Most existing works mainly conduct super-
vised fine-tuning language models pretrained on rel-
evant corpus e.g. PubMed abstracts and MIMIC-III
clinical notes, on annotated biomedical RE corpora
(Tinn et al., 2021; Peng et al., 2019; Beltagy et al.,
2019; Lee et al., 2020; Shin et al., 2020; Yasunaga
et al., 2022). Two drawbacks of the aforementioned
approach are: (1) it fails to capture the semantic
interaction between relations and entities as rela-
tions are represented as integer indices (Chen et al.,
2020; Huang et al., 2022), and (2) performance de-
teriorates as the number of training instances drops
(Han et al., 2018).
Indirect supervision. Indirect supervision (Roth,
2017; He et al., 2021) transfers supervision sig-
nals from a more resource-rich task to enhance
a specific more resource-limited task. Often this
line of work reformulates the training and infer-
ence pipeline of the target task into the form of the
source task to facilitate the cross-task signal trans-
fer. Levy et al. (2017) demonstrate that relation
extraction can be solved using machine reading
comprehension formulation. Similarly, Li et al.
(2019) and Lu et al. (2022) further show that re-
lation extraction performance can be improved by
multi-turn question answering and summarization,
respectively. Recently Sainz et al. (2021) and Li
et al. (2022) propose to leverage indirect supervi-
sion from the NLI task. LITE (Li et al. (2022))
enhances entity typing by incorporating NLI and
a learning-to-rank training objective while Sainz
et al. (2021) observes the benefits of indirect su-
pervision in low-resource relation extraction. As
discussed, NLI aligns well with relation extraction,
but to the best of our knowledge, there is no prior
work that investigates the effectiveness of indirect
supervision when there is a domain gap between
the target task and the source task, e.g. biomedical
domain and general domain in this study.
3
Method
We hereby present NBR. We discuss how to frame
relation extraction as a NLI task in §3.2, illustrate
how to leverage cross-domain NLI knowledge in
§3.3, and lastly provide an optional explicit absten-
tion detector to handle abstinent instances in §3.4.
3.1
Problem Formulation
The RE model takes a sentence x with two men-
tioned entities e1, e2 as input, and predicts the re-
lation y between e1, e2 from the label space Y
that includes all considered relations. The dataset
D consists of both non-abstinent instances where
y ∈Y, and abstinent instances2 where y =⊥. A
successful RE model should abstain for abstinent
instances and accurately predict y for non-abstinent
instances.
3.2
Relation Extraction with NLI
Following Sainz et al. (2021), we reformulate the
RE task as a NLI task, allowing cross-task transfer
of indirect supervision signals from NLI resources.
An overview of our pipeline is visualized in Fig. 1.
Decompose RE to NLI queries. The NLI model
takes in a premise and a hypothesis, both in natu-
2Indicating that either there is no relation between e1, e2
or the relation is not one of the relation labels defined in Y.

ral language, and outputs a logit indicating if the
premise either “entails,” “contradicts” the hypoth-
esis or the inference relation is “neutral.” We de-
compose an instance (x, e1, e2) into |Y| + 1 NLI
queries, each about a candidate relation. We formu-
late the RE input sentence x as the premise and a
verbalized sentence describing the candidate rela-
tion as the hypothesis.
Verbalizing relations to hypotheses. For each re-
lation y ∈Y ∪{⊥}, we verbalize y as a natural
language hypothesis ν(y). Contextual textual repre-
sentations of labels provide more semantic signals
and are thus more understandable by a language
model (LM) compared to the relation name itself
or discrete relation label index used in standard
classification methods (Chen et al., 2020; Huang
et al., 2022).
Entity mentions in biomedical RE are mostly
domain-specific terms that rarely appear in the
LM’s pre-training corpus. The relations are al-
ways defined between entities of certain types, e.g.
between a gene complex and another chemical in
ChemProt (Krallinger et al., 2017) or between two
drugs in DDI (Herrero-Zazo et al., 2013). Thus,
each entity mention is replaced by typed entity
masks such as @GENE$ following Gu et al. (2021)
and Peng et al. (2019).3 The replacement enables
the LM to capture semantic information of the
types and avoid using poorly trained representa-
tions for rare biomedical terms.
As demonstrated by recent studies (Yeh et al.,
2022; Li et al., 2022; Sainz et al., 2021), picking
a good verbalizer for each relation may affect per-
formance. Specifically, we design several types of
templates (details and performances are provided
in Appx. §D) listed below, each containing the two
typed entity masks:
1. Simple Template verbalizes relation between
two entities with “is-a” phrase.
2. Descriptive Template provides a contextual
description of the relation.
3. Demonstration Template includes a randomly
sampled trainset exemplar with the same relation.
4. Descriptive+Demonstration Template com-
bines both the Descriptive description and the sam-
pled exemplar.
5. Learned Prompt Template (Yeh et al., 2022)
learns optimal discrete tokens for description.
3We choose to use our typed entity mask design instead
of the “entity mask” (Zhou and Chen, 2022) as it has been
observed to produce better performance in those tasks with
NLI. We do not consider the entity masks as special tokens.
We observe that Descriptive Template performs the
best empirically (Tab. 7).
Confidence scoring.
For each relation label
y
∈
Y ∪{⊥}, we calculate the confidence
score of whether relation y holds by s(y) =
fNLI(x [SEP] ν(y)) where [SEP] is a special token
separating x (premise) and ν(y) (hypothesis). fNLI
is a transformer-based NLI model that encodes the
input and produces logits that correspond plausibil-
ity of premise entailing hypothesis.
Abstention as a separate label. We treat ⊥as a
separate relation label and verbalize it explicitly,
which is analogous to how supervised biomedi-
cal RE treats ⊥as an additional label (Yasunaga
et al., 2022; Peng et al., 2019). An explicit template
relieves the burden of incorporating both stop con-
dition and label discriminative power into scores
of Y labels.
Training objective. Recent works in contrastive
learning show that InfoNCE loss benefits efficient
learning from negative examples (Robinson et al.,
2021; Wang et al., 2022; Zhang and Stratos, 2021;
Zhou et al., 2021; Ma et al., 2023, 2021). Moti-
vated by the intuition that positive instances should
be ranked higher than negative instances with re-
gard to the anchor instance, in each step we sam-
ple n negative relations {y1, . . . , yn} ⊆Y ∪{⊥
} \ {y} and compute s(y1), . . . , s(yn), and opti-
mize ground truth relation’s entailment score to
be ranked higher. Specifically, we optimize the
following InfoNCE loss
LNCE =
X
(x,y)∈D
ℓNCE(x, y)
(1)
≜
X
(x,y)∈D
−ln




exp(s(y)/τ)
exp(s(y)/τ) +
nP
i=1
exp(s(yi)/τ)



,
in which temperature τ controls focus on harder
negatives. In practice, learning from all possible
negatives performs the best.
In pilot experiments, we observed that the model
was prone to be misled by the vast number of absti-
nent instances in the dataset, leading to deteriorated
performance. To alleviate such abstinent v.s. non-
abstinent imbalance, we introduce a margin-based
Abstention Calibration regularization to penalize
over-confident abstinent instances while encourag-
ing non-abstinent instances. Concretely, if relation
is not ⊥, we calibrate the score of ⊥such that s(⊥)
is suppressed; otherwise, we control ⊥to be ranked

higher than other relations.
LAC =
X
(x,y)∈D
ℓAC(x, y)
(2)
ℓAC(x, y) ≜



nP
i=1
ℓrank(s(y), s(yi); γ), if y =⊥
ℓrank(s(y), s(⊥); γ), otherwise
where the ranking loss ℓrank(x1, x2; γ) learns to
project x1 higher than x2 by a margin γ. Training
with this objective, NBR can be viewed as com-
bining an implicit abstention calibrator and s(⊥)
as a learnable instance-aware threshold. The final
training loss is LNCE + λLAC where non-negative
hyperparameter λ controls the strength of absten-
tion calibration.
Inference. NBR gathers hypotheses verbalized
from every relation and performs ranking among
the entailment scores of each hypothesis. Then the
relation whose verbalized hypothesis achieves the
highest score is selected as the final prediction.
3.3
Cross-Domain NLI Fine-tuning
In order to maximize the benefit of NLI formula-
tion, it is advised to use models trained on target-
domain NLI dataset (Li et al., 2022; Sainz et al.,
2021). However, available biomedical NLI training
resource is limited. As a remedy, we experiment
with fine-tuning NLI models on two commonly
used general domain NLI datasets, namely MNLI
(Williams et al., 2018) and SNLI (Bowman et al.,
2015), instead. Empirically we found strong evi-
dence (§4.2, §4.4) that general-domain NLI knowl-
edge can still be beneficial in the biomedical do-
main even if a domain gap exists.
3.4
Explicit Abstention Detector
Training with aforementioned LAC (Eq. 2) makes
NBR an implicit abstention calibrator. As an op-
tional post-process step, we can further improve
NBR by introducing an Explicit Abstention Dector
(EAD). This is analogous to the “no-answer reader”
component used in previous works that detect ab-
stinent instances explicitly (Back et al., 2020; Hu
et al., 2019; Kundu and Ng, 2018).
EAD is essentially another instance of NBR
trained separately on the same train set, but chang-
ing relation labels into binary “has relation” versus
“no relation” (⊥). A new verbalization template is
created for “has relation”. For inference, we collect
all differences sEAD(⊥)−sEAD(“has relation”) on
the dev set. Then we iterate each difference as a
threshold, and for one instance in the test set, EAD
predicts ⊥only if the difference of such instance
exceeds the threshold. Once EAD is trained, NBR
and EAD are combined using a simple heuristic:
resort to NBR only when EAD prediction is not
⊥(Appx. §C). In this manner, even if EAD makes
a false positive prediction, since NBR still retains
the ability to flag ⊥, such error can be recovered.
Otherwise, we trust EAD prediction since it spe-
cializes in abstention prediction.
4
Experiments
In this section, we discuss our experiment setup
(§4.1) and evaluation results (§4.2), followed by
detailed ablation studies (§4.3) and analyses (§4.4).
4.1
Experimental Setup
Dataset and evaluation metric.
We conduct
experiments on three sentence-level biomedi-
cal RE datasets contained in the widely-used
BLURB benchmark (Gu et al., 2021). ChemProt
(Krallinger et al., 2017) consists of PubMed
abstracts corpora with five high-level chemical-
protein interaction annotations.
DDI (Herrero-
Zazo et al., 2013) studies drug-drug interaction
and specializes in pharmacovigilance built from
PubMed abstracts. GAD (Bravo et al., 2015) is a
semi-labeled dataset created using Genetic Associ-
ation Archive and consists of gene-disease associa-
tions.
There are multiple variants of the datasets used
by existing literature that differ by data statistics
or evaluation protocol (Dong et al., 2021; Phan
et al., 2021; Beltagy et al., 2019; Yeh et al., 2022;
Peng et al., 2020; Xu et al., 2022) as described
in Appx. §B, we adopt the most popular setting
used by Gu et al. (2021) and give dataset statistics
in Tab. 5. Most of entity pairs are labeled as ⊥
without an explicit relation label.4 This setting is
realistic since the model must identify a relation’s
existence first. Following Gu et al. (2021), we
use the micro F1 score calculated across all non-
abstinent instances as the evaluation metric.
Baselines. We compare against the various base-
lines (Appx. §A), mostly classification-based ap-
proaches that use |Y| + 1-way classification head
on top of a biomedical-pretrained LM. Sci-Five
(Phan et al., 2021) generates the relation label as a
seq-to-seq conditional generation formulation.
4In train set, ChemProt contains 77% abstinent while DDI
contains 85%.

Model
ChemProt
DDI
GAD
BioRE-Prompt3 (Yeh et al., 2022)
67.46
-
-
BLUE-BERTlarge (Peng et al., 2019)
74.40
79.90
-
Sci-BERTbase3 (Beltagy et al., 2019)
74.93
81.32
Bio-BERTbase (Lee et al., 2020)
76.46
80.333
79.83
BioMegatron (Shin et al., 2020)
77.00
-
-
PubMed-BERTbase (Tinn et al., 2021)
77.24
82.36
82.34
Sci-Fivelarge3 (Phan et al., 2021)
77.48
82.23
79.21
KeBioLM (Yuan et al., 2021)
77.50
81.90
84.30
BioLink-BERTbase (Yasunaga et al., 2022)
77.57
82.72
84.39
BioM-ELECTRAlarge (Alrowili and Vijay-Shanker, 2021)
78.60
-
-
BioRoBERTalarge (Alrowili and Vijay-Shanker, 2021)
78.80
-
-
BioM-ALBERTxxlarge (Alrowili and Vijay-Shanker, 2021)
79.30
82.043
-
BioLink-BERTlarge (Yasunaga et al., 2022)
79.98
83.35
84.90
BioM-BERTlarge (Alrowili and Vijay-Shanker, 2021)
80.00
81.923
-
NBRNLI (§3.2)
79.30
83.87
83.75
NBRNLI+FT (§3.3)
80.54
84.66
85.86
NBRNLI+FT+EAD (§3.4)
81.10
85.14
-
SUPERVISED METHODS
INDIRECT SUPERVISION
Table 1: Model performance (micro F1) using full training data on 3 biomedical RE datasets. Since GAD does not
contain abstinent instances, EAD is unnecessary. 3 indicates the results are from our re-implementation to conform
to our evaluation metric. Other baseline performances are taken from their papers. We highlight the best results in
red and the best results of direct supervision in cyan .
Our method. We term three variants of NBR:
• NBRNLI using NLI formulation (§3.2) with
BioLinkBERTlarge (Yasunaga et al., 2022) back-
bone that pretrained on biomedical corpus.
• NBRNLI+FT further cross-domain fine-tunes
(§3.3) BioLinkBERT on two general domain NLI
datasets. The model retains biomedical domain
knowledge and learns relevant NLI knowledge.
• NBRNLI+FT+EAD
assembles
NBRNLI+FT
with a separately trained EAD component (§3.4).
We choose BioLinkBERT as the pretrained LM
due to its supremacy in performance on various
biomedical domain tasks, but we emphasize that
our approach is agnostic to backbone models.
4.2
Experimental Results
NLI provides helpful indirect supervision. We
report the comparison between NBR and baselines
in Tab. 1. Overall, NBRNLI+FT+EAD achieves
SOTA performance on all three datasets, with
1.10, 1.79, and 0.96 points F1 improvement on
ChemProt, DDI, and GAD respectively. Strong
performance gains verify the effectiveness of refor-
mulating biomedical RE as NLI. NLI supervision
signals from the general domain are transferred to
enhance the biomedical RE learning signals. By
verbalizing relations into natural language hypoth-
esis, NBR leverages the preexisting inductive bias
of NLI-finetuned models to make informed predic-
tions based on relation semantics.
We further compare the performance of our
model’s variants. First, due to the prevalence of
abstinent instances on the datasets, we notice that
by explicitly detecting the abstinent instances, as-
sembling EAD (§3.4) with NBRNLI+FT improves
performance on ChemProt and DDI. This is likely
because explicitly detecting ⊥by a separate EAD
model reduces the burden on NBRNLI+FT to pre-
dict relations and identify abstinent instances at the
same time. Second, we show that cross-domain
fine-tuning (§3.3) is vital. Compared to NBRNLI,
which is not trained on NLI datasets, NBRNLI+FT
resulted in significant improvements in F1 across
three datasets. This demonstrates that having prior
NLI knowledge allows better utilization of the NLI
formulation. Lastly, we note that NBRNLI is out-
performed by its direct supervision counterpart,
namely BioLinkBERT on ChemProt and GAD. The
possible reason could be that the model needs to
learn to perform NLI tasks on top of the RE task

Model on ChemProt
0 shot 8 shot
1%
50 shot 10% 100%
BioRE-Prompt3 (Yeh et al., 2022)
1.32
6.07
27.89
36.80
55.66 67.46
BLUE-BERTlarge (Peng et al., 2019)
-
10.22 20.13
27.91
51.02 74.40
Sci-BERTbase3 (Beltagy et al., 2019)
-
15.60 22.08
33.36
60.60 74.93
Bio-BERTbase (Lee et al., 2020)
-
10.28 20.96
38.15
68.01 76.46
PubMed-BERTbase (Tinn et al., 2021)
-
15.97 23.49
35.37
68.49 77.24
Sci-Fivelarge3 (Phan et al., 2021)
0.00
17.19 35.66
47.41
68.62 77.48
BioM-ALBERTxxlarge (Alrowili and Vijay-Shanker, 2021)
-
8.49
14.95
21.92
51.69 79.30
BioLinkBERTlarge (Yasunaga et al., 2022)
-
9.31
21.19
38.70
71.37 79.98
BioM-BERTlarge (Alrowili and Vijay-Shanker, 2021)
-
16.02 26.23
40.63
68.93 80.00
NBRNLI (§3.2)
5.70
36.42 49.63
51.95
72.03 79.30
NBRNLI+FT (§3.3)
24.50 46.53 60.17
56.43
75.12 80.54
NBRNLI+FT+EAD (§3.4)
-
51.44 60.34
61.31
75.24 81.10
Model on DDI
0 shot 8 shot 50 shot
1%
10% 100%
BLUE-BERTlarge (Peng et al., 2019)
-
8.76
25.79
27.48 65.62 79.90
Bio-BERTbase (Lee et al., 2020)
-
13.61
31.93
30.01 64.56 80.33
Sci-BERTbase3 (Beltagy et al., 2019)
-
10.55
33.34
23.62 69.44 81.32
Sci-Fivelarge3 (Phan et al., 2021)
0.00
25.44
39.36
29.80 77.11 82.23
PubMed-BERTbase (Tinn et al., 2021)
-
17.02
34.39
27.53 71.98 82.36
BioM-ALBERTxxlarge (Alrowili and Vijay-Shanker, 2021)
-
11.52
22.50
18.64 76.70 82.04
BioLinkBERTlarge (Yasunaga et al., 2022)
-
9.70
37.80
34.11 74.08 83.35
BioM-BERTlarge (Alrowili and Vijay-Shanker, 2021)
-
16.42
37.25
27.85 79.07 81.92
NBRNLI (§3.2)
3.60
32.01
47.86
53.53 79.49 83.87
NBRNLI+FT (§3.3)
11.94 37.80
52.49
60.20 80.85 84.66
NBRNLI+FT+EAD (§3.4)
-
42.48
58.50
61.06 81.71 85.14
Table 2: We conduct experiment on {0,8,50}-shot and {1,10}-% ChemProt (top) and DDI (bottom). We highlight
the best model in red and the best of direct supervision in cyan . Columns are ordered by the number of training
instances. 3 indicates the results are from our re-implementation to conform to our evaluation metric.
without NLI training, which leads to shallower
supervision signals.
However we observe that
generally, and especially in low-resource regimes,
NBRNLI improves over direct supervision (§4.4).
Indirect supervision from NLI shines particu-
larly under low-resource. We evaluate the NBR
under zero- and few-shot settings in Tab. 2. Fol-
lowing existing works (Peng et al., 2020; Xu et al.,
2022), we train the model with 0, 8 and 50 shots
and 1% and 10% of training instances. We note
that classification-based methods could not adapt
to the zero-shot setting.
Our experimental results show that all three vari-
ants of NBR consistently achieve strong perfor-
mance across all few-shot settings on all datasets,
e.g. 34.25 points F1 improvement on 8-shot
ChemProt.
The performance of direct supervi-
sion models deteriorates dramatically as the num-
ber of training instances decreases, due to the lim-
ited learning signals. On the contrary, NBR effec-
tively leverages indirect supervision to transform
richer NLI signals to improve the RE performance.
Additionally verbalized hypotheses provide valu-
able semantic cues for prediction. We also ob-
serve similar patterns as the full-set experiments:
using NLI knowledge learned from NLI training
data improves the performance of NBRNLI, and
combing EAD with NBRNLI+FT leads to further
performance gains.
Lastly, we note that as the number of training
instances increases, the benefits of indirect super-
vision tend to decrease. This suggests that given
sufficient training signals, direct supervision can
learn effectively, and the marginal returns of intro-
ducing additional NLI signals become smaller. In
practical settings where biomedical annotations are
scarce, learning with indirect supervision can lead
to better performance.
4.3
Ablation Study
We perform ablation studies on model components
on ChemProt and DDI using 1% and 100% train-

RoBERTa
RoBERTa + NLI
BioLinkBERT
BioLinkBERT + NLI
NLI 
Knowledge
✓
✓
✓
✗
✗
✗
✓
✗
Biomedical
Knowledge
Figure 2: Impact of biomedical and NLI knowledge on 1 and 100% ChemProt and DDI. Both pieces of knowledge
are substantial for biomedical RE.
1%
100%
1%
100%
NBRNLI+FT
60.17 80.54
60.20 84.66
-LNCE (Eq. 1) 59.63 79.32
52.50 83.29
-LAC (Eq. 2)
57.57 78.68
50.18 82.94
-LNCE-LNC
53.87 78.12
20.71 82.74
MedNLI
53.58 79.60
51.04 82.42
Model
ChemProt
DDI
Table 3: Ablation study of NBR. Micro F1 is reported
for 1% and 100% ChemProt and DDI datasets.
ing data in Tab. 3. (1) InfoNCE LNCE (Eq. 1) is
essential. Replacing LNCE with ranking loss sum
i.e. Pn
i=1 ℓrank(s(y), s(yi); γ) deteriorate perfor-
mance. These results confirm the effectiveness
of InfoNCE in learning from negative samples
(Robinson et al., 2021; Wang et al., 2022). (2)
LAC (Eq. 2) is vital. Given the prevalence of ab-
stinent relations in the two datasets, it is easy for
models to be misled by abstinent instances since
they impose stronger learning signals. We specifi-
cally notice 1% settings have a larger performance
drop, which might be caused by the fact that de-
tecting abstention is harder when the quantity of
other labels and their associated learning signals is
reduced. (3) We further consider a variant that re-
places LNCE with ranking loss sum, removes LAC
and uses only one negative sample, which corre-
sponds to LITE (Li et al., 2022) that uses NLI
indirect supervision for the general domain entity
typing task. We observe further performance degra-
dation, which again verifies the effectiveness of the
two losses. Lastly (4) we fine-tune BioLinkBERT
on the biomedical MedNLI (Romanov and Shivade,
2018). Despite being domain-relevant, we observe
performance drops compared to fine-tuning on gen-
eral domain NLI datasets. We hypothesize that
perform drops might be caused by (a) MedNLI
being relatively small as MNLI is 35x larger and
(b) low coverage on relevant knowledge e.g. only
11.77% of ChemProt entities are mentioned in
MedNLI. Therefore even if MedNLI provides both
NLI knowledge and biomedical knowledge, the
gain is insignificant.
4.4
Analysis
In this section, we first show the benefits of indirect
supervision, then illustrate two key ingredients for
effective indirect supervision gains: biomedical
domain knowledge and NLI knowledge.
DS
IS
DS
IS
1%
0.00
51.11 21.19 49.63
100% 45.72 76.02 79.98 79.30
1%
15.13 26.11 34.11 53.53
100% 81.23 81.73 83.35 83.87
Dataset
RoBERTa
BioLinkBERT
Chem
Prot
DDI
Table 4: NLI formulation benefits, especially in low-
resource settings. We report performance using Direct
Supervision (DS) or NLI Indirect Supervision (IS) for-
mulation with backbones not trained on NLI datasets.
NLI formulation benefits, even without addi-
tional NLI resources. In Tab. 4, we demonstrate
the effectiveness of NLI formulation using two
backbones without NLI knowledge: RoBERTa (Liu
et al., 2019) and BioLinkBERT.
We observe that even if models lack NLI for-
mulation adaption, NLI formulation outperforms
original RE formulation in most settings, particu-
larly in low-resource settings. When data is limited,
it is challenging for direct supervision methods to
access sufficient supervision signals. In contrast,

the model can leverage the semantic information in
the natural language hypothesis with the NLI for-
mulation. Additionally, BioLinkBERT consistently
outperformed RoBERTa in the same settings, de-
spite RoBERTalarge having larger parameters, sug-
gesting the importance of domain knowledge.
Two key ingredients of indirect supervision for
biomedical RE. We identify two potential fac-
tors that contribute to the effective usage of indi-
rect supervision for biomedical RE: 1) biomedical
domain-specific knowledge; and 2) NLI knowl-
edge to adapt to the NLI formulation. To test the
importance of these two kinds of knowledge, in
Fig. 2 we evaluate on 1% and 100% of ChemProt
and DDI the four combinations: RoBERTa and
RoBERTa fine-tuned on NLI, and BioLinkBERT
and BioLinkBERT fine-tuned on NLI.
We first observe that BioLinkBERT fine-tuned
on NLI datasets behaves the best across all four
settings, indicating the importance of both pieces
of knowledge. When the learning signal is lim-
ited, the model can dynamically load-balance both
forms of knowledge to make educated predictions.
Secondly, we note that RoBERTa, which lacks both
biomedical and NLI knowledge, consistently per-
forms the worst, except for 1% ChemProt. Finally,
it is difficult to determine whether the domain or
NLI knowledge is more important in biomedical
RE, as the relative importance may depend on the
specific dataset or the knowledge requirements of
each input.
5
Conclusion
We present a novel method NBR that leverages
indirect supervision by cross-task transfer learning
from NLI tasks to improve the biomedical RE task.
NBR verbalizes relations to natural language hy-
potheses so that model is able to exploit semantic
information to make informed predictions. Fur-
thermore, NBR adopts a ranking-based abstinent
calibration loss that penalizes overconfident absti-
nent instances while encouraging non-abstinent in-
stances, thus being capable of abstaining on un-
certain instances. Extensive experiments on three
widely-used biomedical RE benchmarks demon-
strate that NBR is effective in both full-set and
low-resource settings. We further investigate two
key ingredients for effective NLI indirect supervi-
sion on biomedical RE. Future work could involve
further investigation of other indirect supervision
approaches and automatic relation template gener-
ation based on prompt learning.
Acknowledgement
We appreciate the reviewers for their insightful
comments and suggestions. Jiashu Xu was sup-
ported by the Center for Undergraduate Research
in Viterbi Engineering (CURVE) Fellowship.
Mingyu Derek Ma was supported by the AFOSR
MURI grant #FA9550-22-1-0380, the Defense Ad-
vanced Research Project Agency (DARPA) grant
#HR00112290103/HR0011260656, and a Cisco
Research Award. Muhao Chen was supported by
the NSF Grant IIS 2105329, by the Air Force
Research Laboratory under agreement number
FA8750-20-2-10002, by a subaward of the INFER
Program through UMD ARLIS, an Amazon Re-
search Award and a Cisco Research Award. Com-
puting of this work was partly supported by a sub-
award of NSF Cloudbank 1925001 through UCSD.
Limitations
This work investigates using NLI as indirect su-
pervision for biomedical RE. Experiments suggest
two key ingredients in high-performing indirect
supervision biomedical RE are biomedical knowl-
edge and NLI knowledge. To this goal, we need
to access a language model that is pretrained on
biomedical domain corpus, which requires compu-
tational resources. Compared to general domain
ones, models pretrained on a specific domain are
often limited in variety. Further to learn NLI knowl-
edge additional cross-domain fine-tuning needs to
be conducted, which results in additional computa-
tional overhead.
During inference NBR requires #label times
of forward passes to yield prediction since NBR
needs to evaluate entailment scores for each verbal-
ized relation. Compared to standard supervision
which only requires one pass for every instance,
inference cost and training cost are higher in a
factor of # label. Higher inference cost hinders ap-
plicability in a number of scenarios e.g. real-time
applications. Additionally, the high inference cost
makes it difficult to deploy machine learning mod-
els in resource-constrained environments, such as
edge devices with limited processing power.
Lastly, since NBR is sensitive to templates, de-
signing an effective template is crucial for perfor-
mance. However, currently human involvement is
required to design templates for each relation. As
the number of relations increases, human involve-

ment might become costly and time-consuming.
Moreover, it is not easy to test the effectiveness of
templates as no objective metric exists, and the only
way to assess the quality is to test the templates.
References
Sultan Alrowili and K Vijay-Shanker. 2021.
Biom-
transformers: building large biomedical language
models with bert, albert and electra. In Proceedings
of the 20th Workshop on Biomedical Language Pro-
cessing, pages 221–227.
Seohyun Back, Sai Chetan Chinthakindi, Akhil Kedia,
Haejun Lee, and Jaegul Choo. 2020. Neurquri: Neu-
ral question requirement inspector for answerability
prediction in machine reading comprehension. In
8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020. OpenReview.net.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:
A pretrained language model for scientific text. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pages 3615–3620.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Àlex Bravo, Janet Piñero, Núria Queralt-Rosinach,
Michael Rautschka, and Laura I Furlong. 2015. Ex-
traction of relations between genes and diseases from
text and large-scale data analysis: implications for
translational research. BMC bioinformatics, 16(1):1–
17.
Muhao Chen, Hongming Zhang, Haoyu Wang, and
Dan Roth. 2020. What are you trying to do? se-
mantic typing of event processes. In Proceedings of
the 24th Conference on Computational Natural Lan-
guage Learning, pages 531–542, Online. Association
for Computational Linguistics.
Manqing Dong, Chunguang Pan, and Zhipeng Luo.
2021. Mapre: An effective semantic mapping ap-
proach for low-resource relation extraction. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2694–
2704.
Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan
Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi,
Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,
Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,
Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-
son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer
Singh, Noah A. Smith, Sanjay Subramanian, Reut
Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.
2020. Evaluating models’ local decision boundaries
via contrast sets.
In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages
1307–1323, Online. Association for Computational
Linguistics.
Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto
Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng
Gao, and Hoifung Poon. 2021. Domain-specific lan-
guage model pretraining for biomedical natural lan-
guage processing. ACM Transactions on Computing
for Healthcare (HEALTH), 3(1):1–23.
Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and
Peng Li. 2018. Hierarchical relation extraction with
coarse-to-fine grained attention. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2236–2245.
Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and
Maosong Sun. 2022. Ptr: Prompt tuning with rules
for text classification. AI Open.
Hangfeng He, Mingyuan Zhang, Qiang Ning, and Dan
Roth. 2021. Foreseeing the benefits of incidental
supervision. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1782–1800, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
María Herrero-Zazo, Isabel Segura-Bedmar, Paloma
Martínez, and Thierry Declerck. 2013.
The ddi
corpus: An annotated corpus with pharmacological
substances and drug–drug interactions. Journal of
biomedical informatics, 46(5):914–920.
Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang,
Nan Yang, and Dongsheng Li. 2019. Read + verify:
Machine reading comprehension with unanswerable
questions.
In The Thirty-Third AAAI Conference
on Artificial Intelligence, AAAI 2019, The Thirty-
First Innovative Applications of Artificial Intelligence
Conference, IAAI 2019, The Ninth AAAI Symposium
on Educational Advances in Artificial Intelligence,
EAAI 2019, Honolulu, Hawaii, USA, January 27 -
February 1, 2019, pages 6529–6537. AAAI Press.
James Y. Huang, Bangzheng Li, Jiashu Xu, and Muhao
Chen. 2022. Unified semantic typing with mean-
ingful label inference. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2642–2654, Seattle,
United States. Association for Computational Lin-
guistics.
Amita Kamath, Robin Jia, and Percy Liang. 2020. Se-
lective question answering under domain shift. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 5684–
5696, Online. Association for Computational Lin-
guistics.

Martin Krallinger, Obdulia Rabal, Saber A Akhondi,
Martın Pérez Pérez, Jesús Santamaría, Gael Pérez Ro-
dríguez, Georgios Tsatsaronis, Ander Intxaurrondo,
José Antonio López, Umesh Nandal, et al. 2017.
Overview of the biocreative vi chemical-protein inter-
action track. In Proceedings of the sixth BioCreative
challenge evaluation workshop, volume 1, pages 141–
146.
Souvik Kundu and Hwee Tou Ng. 2018. A nil-aware
answer extraction framework for question answering.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
4243–4252, Brussels, Belgium. Association for Com-
putational Linguistics.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon
Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
2020. Biobert: a pre-trained biomedical language
representation model for biomedical text mining.
Bioinformatics, 36(4):1234–1240.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017), pages 333–342, Vancouver,
Canada. Association for Computational Linguistics.
Bangzheng Li, Wenpeng Yin, and Muhao Chen. 2022.
Ultra-fine entity typing with indirect supervision
from natural language inference. Transactions of the
Association for Computational Linguistics, 10:607–
622.
Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan,
Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-
relation extraction as multi-turn question answering.
In Proceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1340–
1350, Florence, Italy. Association for Computational
Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019. OpenRe-
view.net.
Keming
Lu,
I-Hung
Hsu,
Wenxuan
Zhou,
Mingyu Derek Ma, Muhao Chen, et al. 2022.
Summarization as indirect supervision for relation
extraction. In EMNLP - Findings.
Mingyu Derek Ma, Muhao Chen, Te-Lin Wu, and
Nanyun Peng. 2021. HyperExpan: Taxonomy ex-
pansion with hyperbolic representation learning. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021, pages 4182–4194, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Mingyu Derek Ma, Alexander K. Taylor, Wei Wang,
and Nanyun Peng. 2023. Dice: Data-efficient clinical
event extraction with generative models. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics, Toronto, Canada. As-
sociation for Computational Linguistics.
Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li,
Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2020.
Learning from Context or Names? An Empirical
Study on Neural Relation Extraction. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
3661–3672, Online. Association for Computational
Linguistics.
Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-
fer learning in biomedical natural language process-
ing: An evaluation of bert and elmo on ten bench-
marking datasets. In Proceedings of the 2019 Work-
shop on Biomedical Natural Language Processing
(BioNLP 2019).
Long N Phan, James T Anibal, Hieu Tran, Shaurya
Chanana, Erol Bahadroglu, Alec Peltekian, and Gré-
goire Altan-Bonnet. 2021.
Scifive: a text-to-text
transformer model for biomedical literature. arXiv
preprint arXiv:2106.03598.
Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra,
and Stefanie Jegelka. 2021. Contrastive learning with
hard negative samples. In ICLR.
Alexey Romanov and Chaitanya Shivade. 2018.
Lessons from natural language inference in the clini-
cal domain. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1586–1596, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Dan Roth. 2017. Incidental supervision: Moving be-
yond supervised learning. In Thirty-First AAAI Con-
ference on Artificial Intelligence.
Oscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, An-
der Barrena, and Eneko Agirre. 2021. Label verbal-
ization and entailment for effective zero and few-shot
relation extraction. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1199–1212.
Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina,
Raul Puri, Mostofa Patwary, Mohammad Shoeybi,
and Raghav Mani. 2020.
BioMegatron: Larger
biomedical domain language model. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
4700–4706, Online. Association for Computational
Linguistics.
Robert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xi-
aodong Liu, Tristan Naumann, Jianfeng Gao, and

Hoifung Poon. 2021. Fine-tuning large neural lan-
guage models for biomedical natural language pro-
cessing. arXiv preprint arXiv:2112.07869.
Liang Wang, Wei Zhao, Zhuoyu Wei, and Jingming
Liu. 2022. SimKGC: Simple contrastive knowledge
graph completion with pre-trained language models.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 4281–4294, Dublin, Ireland.
Association for Computational Linguistics.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 38–45, Online. Association
for Computational Linguistics.
Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.
2021. The art of abstention: Selective prediction and
error regularization for natural language processing.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1040–1051.
Xin Xu, Xiang Chen, Ningyu Zhang, Xin Xie,
Xi Chen, and Huajun Chen. 2022.
Towards re-
alistic low-resource relation extraction: A bench-
mark with empirical baseline study. arXiv preprint
arXiv:2210.10678.
Michihiro Yasunaga, Jure Leskovec, and Percy Liang.
2022. LinkBERT: Pretraining language models with
document links. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 8003–8016,
Dublin, Ireland. Association for Computational Lin-
guistics.
Hui-Syuan Yeh, Thomas Lavergne, and Pierre Zweigen-
baum. 2022.
Decorate the examples: A simple
method of prompt design for biomedical relation ex-
traction. In Proceedings of the Language Resources
and Evaluation Conference, pages 3780–3787, Mar-
seille, France. European Language Resources Asso-
ciation.
Wenpeng Yin, Nazneen Fatema Rajani, Dragomir
Radev, Richard Socher, and Caiming Xiong. 2020.
Universal natural language processing with limited
annotations: Try few-shot textual entailment as a
start. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 8229–8239, Online. Association for
Computational Linguistics.
Zheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang,
and Fei Huang. 2021. Improving biomedical pre-
trained language models with knowledge. In Pro-
ceedings of the 20th Workshop on Biomedical Lan-
guage Processing, pages 180–190, Online. Associa-
tion for Computational Linguistics.
Wenzheng Zhang and Karl Stratos. 2021. Understand-
ing hard negatives in noise contrastive estimation.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1090–1101, Online. Association for Computa-
tional Linguistics.
Wenxuan Zhou and Muhao Chen. 2022. An improved
baseline for sentence-level relation extraction. In Pro-
ceedings of the 2nd Conference of the Asia-Pacific
Chapter of the Association for Computational Lin-
guistics and the 12th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers), pages 161–168, Online only. Association for
Computational Linguistics.
Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing
Huang. 2021. Document-level relation extraction
with adaptive thresholding and localized context pool-
ing. In Proceedings of the AAAI conference on artifi-
cial intelligence, volume 35, pages 14612–14620.
Appendices
A
Models
Baselines
We categorize compared baselines by
the pretrain corpus.
• PubMed abstracts: BioM-ELECTRA (Alrowili
and Vijay-Shanker, 2021).
• PubMed abstracts and PMC full-text articles:
Bio-BERT (Lee et al., 2020); BioM-BERT
(Alrowili and Vijay-Shanker, 2021); BioMega-
tron (Shin et al., 2020) pretrain on commercial-
collection subset of PMC; PubMed-BERT (Tinn
et al., 2021) fine-tune model released by Gu et al.
(2021), which is pretrain on those corpus; Sci-
Five (Phan et al., 2021) is T5 based model that
learns to conditionally generate relation labels in
textual form directly; BioLinkBERT (Yasunaga
et al., 2022) further proposes a pretraining task
of link prediction, which enables the model to
learn multi-hop knowledge.

Name
Relations
Train
Dev
Test
# relations
ChemProt (Krallinger et al., 2017)
chemical-gene
@CHEMICAL$
@GENE$
18305
11268
15745
5
DDI (Herrero-Zazo et al., 2013)
drug-drug
25296
2496
5716
4
GAD (Bravo et al., 2015)
disease-gene
@DISEASE$
@GENE$
4261
535
534
2
Entity Mask
@DRUG$
Table 5: Dataset Statistics. # relations does not include ⊥. GAD does not contain abstinent instances.
• PubMed abstracts and MIMIC-III clinical notes:
BLUE-BERT (Peng et al., 2019).
• Semantic Scholar: Sci-BERT (Beltagy et al.,
2019) pretrain BERT on scientific corpus con-
sists of 1.14M full-text papers from Semantic
Scholar; BioRE-Prompt (Yeh et al., 2022) ini-
tializes from RoBERTa trained on the Semantic
Scholar and learns a three-token prompt for each
relation and infers by finding the best matching
prompt.
We use model checkpoints released by hug-
gingface (Wolf et al., 2020). Specifically, we use
bionlp/bluebert_pubmed_mimic_uncased_L-
24_H1024_A-16 for BLUE-BERT (Peng et al.,
2019),
allenai/scibert_scivocab_uncased
for
Sci-BERT
(Beltagy
et
al.,
2019),
dmis-lab/biobert-basecased-v1.2
for
Bio-
BERT (Lee et al., 2020), microsoft/BiomedNLP-
PubMedBERT-base-uncased-abstract-fulltext
for
PubMed-BERT
(Tinn
et
al.,
2021),
razent/SciFive-large-Pubmed_PMC
for
Sci-Five
(Phan
et
al.,
2021),
sultan/BioM-ALBERT-xxlarge-PMC for BioM-
ALBERT (Alrowili and Vijay-Shanker, 2021),
sultan/BioM-BERT-PubMed-PMC-Large
for
BioM-BERT
(Alrowili
and
Vijay-Shanker,
2021),
michiyasunaga/BioLinkBERT-large
for BioLink-BERT (Yasunaga et al., 2022), and
cnut1648/biolinkbert-large-mnli-snli
for BioLink-BERT that is fine-tuned on SNLI
(Bowman et al., 2015) and MNLI (Williams et al.,
2018).
NBR
We run experiments on Quadro RTX 8000
GPU. AdamW optimizer (Loshchilov and Hutter,
2019) with learning rate 1e-5 is used, and we set
margin γ = 0.7, temperature τ = 0.01 and cali-
bration (Eq. 2) strength λ in sweep from 0.001 to
10. We train models for 300 epochs. Models are
evaluated every ten epochs on the dev set, and the
best checkpoint is selected to infer on the test set.
B
Evaluation Difference
As mentioned in §4, several previous works use
a different evaluation metric and variants of the
datasets, rendering it hard to compare with previ-
ous work. In this section, we describe the main
differences in the dataset. We first report the statis-
tics of the dataset we use in this work in Tab. 5. For
other works that use variants of the datasets:
• BLUE-BERT (Peng et al., 2019)’s variant
of ChemProt and DDI. Their ChemProt con-
tains 4,154/2,416/3458 train/val/test instances
and five relations, while their DDI contains
2,937/1,004/979 train/val/test instances and four
relations.
• Sci-BERT (Beltagy et al., 2019) uses a variant of
ChemProt with 4,169/2,427/3,449 train/val/test
instances and contains 13 relations.
• Dong et al. (2021) and (Peng et al., 2020) use
a variant of ChemProt with 4,168/2,427/3,469
train/val/test instances and 13 relations.
• Xu et al. (2022) use a variant of ChemProt with
14 relations
• BioRE-Prompt (Yeh et al., 2022) also use
ChemProt provided by Gu et al. (2021), but does
not exclude abstinent instances.
C
EAD Details and Variants
Heuristic
ChemProt
Simple
81.10
Voting
80.73
Confident
80.96
Super-confident
80.66
Classification
80.78
Table 6: NBRNLI+FT+EAD performance on ChemProt
under various heuristics.
Since only relations for EAD is “has relation”
versus “no relation”, instead of Eq. 1 and Eq. 2
used in NBR, EAD learns only via ranking loss
ℓrank(s(y), s(y′); γ) where y is the ground-truth
while y′ is the opposite relation.
We discuss several heuristics in assembling
NBR and EAD. The best performing heuristic is
simple: only resort to NBR when EAD prediction
is not ⊥. In other words, the final prediction is
⊥only if EAD prediction is ⊥; otherwise, return

the prediction of NBR. We evaluate other more
sophisticated heuristics:
• Voting: Predict ⊥only when both NBR and
EAD predict ⊥; otherwise, return NBR’s predic-
tion.
• Confident: Predict ⊥only when EAD predicts ⊥
and confidence score sEAD(⊥) is higher than con-
fidence score sNBR(⊥); otherwise, return NBR’s
prediction. Note that if EAD makes a false pos-
itive, NBR is still able to recover if sNBR(⊥) is
the highest.
• Super-confident: Predict ⊥when EAD predicts
⊥; if sEAD(⊥) > sNBR(⊥) return highest-scored
non-abstinent relation arg maxy∈Y sNBR(y); oth-
erwise prediction of NBR.
• Classification: Use a classification-based model
(with the same backbone as NBRNLI+FT), and
use logits for confidence score under the simple
heuristic.
In Tab. 6, we observe that a more complicated
heuristic does not entail better performance gains.
Note that designing a contextual description for
“has relation” is challenging and our template is
a simple phrase such as “relation exists between.”
Surprisingly, we still found assembling NBR with
EAD empirically outperforms classification-based
abstention detector. We credit enhanced perfor-
mance to additional semantic information captured
by the verbalized template.
1%
100%
1%
100%
Descriptive
60.17
80.54
60.20
84.66
Simple
63.80
79.84
55.38
83.26
Demonstration
48.72
79.88
45.81
83.46
Descriptive + Demonstration
53.39
79.79
49.78
83.45
Learned Prompt
59.45
79.74
-
-
Template
ChemProt
DDI
Table 7: Ablation study of NBRNLI+FT using different
templates. Micro F1 is reported. Yeh et al. (2022) only
reports results on ChemProt.
D
Template for datasets
We provide details for each of the templates inves-
tigated in this work.
1. Simple Template: This template verbalizes
the relation between two entities as a “is-a”
phrase, e.g. “@CHEMICAL$ is a downregu-
lator to @GENE$.”
2. Descriptive Template: We manually curate
a description for each relation that con-
tains more context, e.g. “Downregulator
@CHEMICAL$ is designed as an inhibitor
of @GENE$.”
3. Demonstration Template: Motivated by few-
shot exemplars used for in-context learn-
ing, the demonstration template includes a
randomly sampled context sentence whose
entities hold the same relation, e.g. “Rela-
tion described between @CHEMICAL$ to
@GENE$ is similar to <example sentence>.”
4. Descriptive + Demonstration: We include
both a contextual description and an in-
context exemplar by simple concatenating.
5. Learned Prompt Template: Borrowed from
Yeh et al. (2022), which leverage prompt tun-
ing with rules (Han et al., 2022) to learn
optimal discrete tokens to fill in [MASK]
within the template such as “@CHEMICAL$
[MASK] [MASK] [MASK] @GENE$.”
We further provide templates for NBR on three
datasets: ChemProt (Tab. 10), DDI (Tab. 9) and
GAD (Tab. 8).
Lastly, Tab. 7 shows the effect of template de-
sign. The descriptive template, which involves
manual efforts, leads to the best performance. The
simple template preserves the relation name se-
mantics and yields strong performance. On the
other hand, while popular in in-context learning
works, we find that the demonstration template or
descriptive + demonstration template consistently
underperforms the descriptive template, indicating
that incorporating examples in NLI hypothesis is
not helpful potentially due to limited diversity. The
learned prompt template used by Yeh et al. (2022)
does not outperform the manually constructed de-
scriptive template. Finally, we note that changing
templates can lead to significant performance per-
turbations, our experiments suggest that evaluating
the quality of templates in low-resource settings
such as 1% can be effective and efficient. We note
that the contextual template might not be optimal
and we leave how to automatically pick the optimal
template as future work.

Relation
Verbalized Hypothesis
0
There is no relation between @GENE$ and @DISEASE$.
1
@GENE$ and @DISEASE$ are correlated.
Table 8: Descriptive templates on GAD.
Verbalized Hypothesis
0 (no relation)
@DRUG$ and @DRUG$ are not interacting.
DDI-advise
Interaction described bewteen two @DRUG$ and @DRUG$ is about advise.
DDI-effect
Interaction described bewteen two @DRUG$ and @DRUG$ is about effect.
DDI-int
Interaction described bewteen two @DRUG$ and @DRUG$ might or maybe occur.
DDI-mechanism
Interaction described bewteen two @DRUG$ and @DRUG$ is about mechanism.
DDI-advise
A recommendation or advice regarding two @DRUG$ is described.
DDI-effect
Medical effect regarding two @DRUG$ is described.
DDI-int
Interaction regarding two @DRUG$ might or maybe occur.
DDI-mechanism
Pharmacokinetic mechanism regarding two @DRUG$ is described.
DDI-advise
The interaction between two @DRUG$ is the same as “perhexiline hydrogen maleate or
@DRUG$ (with hepatotoxic potential) must not be administered together with @DRUG$ or
Bezalip retard.”
DDI-effect
The interaction between two @DRUG$ is the same as “@DRUG$ administered
concurrently with @DRUG$ reduced the urine volume in 4 healthy volunteers.”
DDI-int
Interaction between two @DRUG$ is the same as @DRUG$ may interact with @DRUG$,
butyrophenones, and certain other agents.”
DDI-mechanism
The interaction between two @DRUG$ is the same as @DRUG$, enflurane, and halothane
decrease the ED50 of @DRUG$ by 30% to 45%.”
DDI-advise
A recommendation or advice regarding two @DRUG$ is described, similar to “perhexiline
hydrogen maleate or @DRUG$ (with hepatotoxic potential) must not be administered
together with @DRUG$ or Bezalip retard.”
DDI-effect
Medical effect regarding two @DRUG$ is described, similar to
”@DRUGadministeredconcurrentlywith@DRUG reduced the urine volume in 4 healthy
volunteers.”
DDI-int
Interaction regarding two @DRUG$ might or maybe occur, similar to @DRUG$ may
interact with @DRUG$, butyrophenones, and certain other agents.”
DDI-mechanism
Pharmacokinetic mechanism regarding two @DRUG$ is described, similar to “@DRUG$,
enflurane, and halothane decrease the ED50 of @DRUG$ by 30% to 45%.”
Relation
Simple
Descriptive
Demonstration
Descriptive + Demonstration
Table 9: Each variant of templates on DDI. Cyan sentence is an example from the train set.

Verbalized Hypothesis
0 (no relation)
@CHEMICAL$ and @GENE$ have no relation.
CPR:3
@CHEMICAL$ is a upregulator to @GENE$.
CPR:4
@CHEMICAL$ is a downregulator to @GENE$.
CPR:5
@CHEMICAL$ is a agonist to @GENE$.
CPR:6
@CHEMICAL$ is a antagonist to @GENE$.
CPR:9
@CHEMICAL$ is a substrate to @GENE$.
CPR:3
Upregulator @CHEMICAL$ is activated by @GENE$.
CPR:4
Downregulator @CHEMICAL$ is designed as an inhibitor of @GENE$.
CPR:5
Activity of agonist @CHEMICAL$ is mediated by @GENE$.
CPR:6
@CHEMICAL$ is identified as an antagonist of @GENE$.
CPR:9
@CHEMICAL$ is a substrate for @GENE$.
CPR:3
Relation of @CHEMICAL$ to @GENE$ is similar to relation described in
“@CHEMICAL$ selectively induced @GENE$ in four studied HCC cell lines.”
CPR:4
Relation of @CHEMICAL$ to @GENE$ is similar to relation described in
“@CHEMICAL$, a new @GENE$ inhibitor for the management of obesity.”
CPR:5
Relation of @CHEMICAL$ to @GENE$ is similar to relation described in
“Pharmacology of @CHEMICAL$, a selective @GENE$/MT2 receptor agonist: a
novel therapeutic drug for sleep disorders.”
CPR:6
Relation of @CHEMICAL$ to @GENE$ is similar to relation described in
“@CHEMICAL$ is an @GENE$ antagonist that is metabolized primarily by
glucuronidation but also undergoes oxidative metabolism by CYP3A4.”
CPR:9
Relation of @CHEMICAL$ to @GENE$ is similar to relation described in “For
determination of [@GENE$+Pli]-activity, @CHEMICAL$ was added after this
incubation.”
CPR:3
Upregulator @CHEMICAL$ is activated by @GENE$, similar to relation described
in “@CHEMICAL$ selectively induced @GENE$ in four studied HCC cell lines.”
CPR:4
Downregulator @CHEMICAL$ is designed as an inhibitor of @GENE$, similar to
relation described in “@CHEMICAL$, a new @GENE$ inhibitor for the
management of obesity.”
CPR:5
Activity of agonist @CHEMICAL$ is mediated by @GENE$, similar to relation
described in “Pharmacology of @CHEMICAL$, a selective @GENE$/MT2 receptor
agonist: a novel therapeutic drug for sleep disorders.”
CPR:6
@CHEMICAL$ is identified as an antagonist of @GENE$, similar to relation
described in “@CHEMICAL$ is an @GENE$ antagonist that is metabolized
primarily by glucuronidation but also undergoes oxidative metabolism by CYP3A4.”
CPR:9
CHEMICAL$ is a substrate for @GENE$, similar to relation described in “For
determination of [@GENE$+Pli]-activity, @CHEMICAL$ was added after this
incubation.”
CPR:3
@CHEMICAL$ is activated by @GENE$.
CPR:4
@CHEMICAL$ activity inhibited by @GENE$.
CPR:5
@CHEMICAL$ agonist actions of @GENE$.
CPR:6
@CHEMICAL$ identified are antagonists @GENE$.
CPR:9
@CHEMICAL$ is substrate for @GENE$.
Relation
Simple
Descriptive
Demonstration
Descriptive + Demonstration
Learned Propmt
Table 10: Each variant of templates on ChemProt. Cyan sentence is an example from the train set.

