End-to-End Instance Segmentation with Recurrent Attention
Mengye Ren1, Richard S. Zemel1,2
University of Toronto1, Canadian Institute for Advanced Research2
{mren,zemel}@cs.toronto.edu
Abstract
While convolutional neural networks have gained impressive
success recently in solving structured prediction problems
such as semantic segmentation, it remains a challenge to dif-
ferentiate individual object instances in the scene. Instance
segmentation is very important in a variety of applications,
such as autonomous driving, image captioning, and visual
question answering. Techniques that combine large graph-
ical models with low-level vision have been proposed to
address this problem; however, we propose an end-to-end re-
current neural network (RNN) architecture with an attention
mechanism to model a human-like counting process, and pro-
duce detailed instance segmentations. The network is jointly
trained to sequentially produce regions of interest as well
as a dominant object segmentation within each region. The
proposed model achieves competitive results on the CVPPP
[28], KITTI [12], and Cityscapes [8] datasets.
1. Introduction
Instance segmentation is a fundamental computer vision
problem, which aims to assign pixel-level instance labeling
to a given image. While the standard semantic segmenta-
tion problem entails assigning class labels to each pixel in
an image, it says nothing about the number of instances of
each class in the image. Instance segmentation is consid-
erably more difﬁcult than semantic segmentation because
it necessitates distinguishing nearby and occluded object
instances. Segmenting at the instance level is useful for
many tasks, such as highlighting the outline of objects for
improved recognition and allowing robots to delineate and
grasp individual objects; it plays a key role in autonomous
driving as well. Obtaining instance level pixel labels is also
an important step towards general machine understanding of
images.
Instance segmentation has been rapidly gaining in popu-
larity, with a spurt of research papers in the past two years,
and a new benchmark competition, based on the Cityscapes
dataset [8].
A sensible approach to instance segmentation is to formu-
late it as a structured output problem. A key challenge here is
the dimensionality of the structured output, which can be on
the order of the number of pixels times the number of objects.
Standard fully convolutional networks (FCN) [27] will have
trouble directly outputting all instance labels in a single shot.
Recent work on instance segmentation [39, 46, 45] proposes
complex graphical models, which results in a complex and
time-consuming pipeline. Furthermore, these models cannot
be trained in an end-to-end fashion.
One of the main challenges in instance segmentation, as
in many other computer vision tasks such as object detection,
is occlusion. For a bottom-up approach to handle occlusion,
it must sometimes merge two regions that are not connected,
which becomes very challenging at a local scale. Many ap-
proaches to handle occlusion utilize a form of non-maximal
suppression (NMS), which is typically difﬁcult to tune. In
cluttered scenes, NMS may suppress the detection results for
a heavily occluded object because it has too much overlap
with foreground objects. One motivation of this work is to
introduce an iterative procedure to perform dynamic NMS,
reasoning about occlusion in a top-down manner.
A related problem of interest entails counting the in-
stances of an object class in an image. On its own this
problem is also of practical value. For instance, counting
provides useful population estimates in medical imaging and
aerial imaging. General object counting is fundamental to
image understanding, and our basic arithmetic intelligence.
Studies in applications such as image question answering
[1, 35] reveal that counting, especially on everyday objects,
is a very challenging task on its own [7]. Counting has been
formulated in a task-speciﬁc setting, either by detection fol-
lowed by regression, or by learning discriminatively with a
counting error metric [23].
To tackle these challenges, we propose a new model based
on a recurrent neural network (RNN) that utilizes visual
attention, to perform instance segmentation. We consider
the problem of counting jointly with instance segmentation.
Our system addresses the dimensionality issue by using a
temporal chain that outputs a single instance at a time. It
also performs dynamic NMS, using an object that is already
segmented to aid in the discovery of an occluded object later
1
arXiv:1605.09410v5  [cs.LG]  13 Jul 2017

Figure 1: An illustration of outputs of different components of our end-to-end system, over nine time-steps: Row 1: soft attention at the
current glimpse; 2: predicted box; 3: current step segmentation; 4: all segmentations.
in the sequence. Using an RNN to segment one instance at
a time is also inspired by human-like iterative and attentive
counting processes. For real-world cluttered scenes, iterative
counting with attention will likely perform better than a
regression model that operates on the global image level.
Incorporating joint training on counting and segmentation
allows the system to automatically determine a stopping
criterion in the recurrent formulation we formulate here.
2. Recurrent attention model
Our proposed model has four major components: A) an
external memory that tracks the state of the segmented ob-
jects; B) a box proposal network responsible for localizing
objects of interest; C) a segmentation network for segment-
ing image pixels within the box; and D) a scoring network
that determines if an object instance has been found, and
also decides when to stop. See Figure 2 for an illustration of
these components.
Notation. We use the following notation to describe the
model architecture: x0 ∈RH×W ×C is the input image (H,
W denotes the dimension, and C denotes the color chan-
nel); t indexes the iterations of the model, and τ indexes the
glimpses of the box network’s inner RNN; y = {yt|yt ∈
[0, 1]H×W }T
t=1, y∗= {y∗
t |y∗
t ∈{0, 1}H×W }T
t=1 are the
output/ground-truth segmentation sequences; s = {st|st ∈
[0, 1]}T
t=1, s∗= {s∗
t |s∗
t ∈{0, 1}}T
t=1 are the output/ground-
truth conﬁdence score sequences. h = CNN(I) denotes
passing an image I through a CNN and returning the hidden
activation h. I′ = D-CNN(h) denotes passing an activa-
tion map h through a de-convolutional network (D-CNN)
and returning an image I′. ht = LSTM(ht−1, xt) denotes
unrolling the long short-term memory (LSTM) by one time-
step with the previous hidden state ht−1 and current input
xt, and returning the current hidden state ht. h = MLP(x)
denotes passing an input x through a multi-layer perceptron
(MLP) and returning the hidden state h.
Input pre-processing. We pre-train a FCN [27] to per-
form input pre-processing. This pre-trained FCN has two
output components. The ﬁrst is a 1-channel pixel-level fore-
ground segmentation, produced by a variant of the Decon-
vNet [30] with skip connections. In addition to predicting
this foreground mask, as a second component we followed
the work of Uhrig et al. [41] by producing an angle map
for each object. For each foreground pixel, we calculate its
relative angle towards the centroid of the object, and quan-
tize the angle into 8 different classes, forming 8 channels,
as shown in Figure 3. Predicting the angle map forces the
model to encode more detailed information about object
boundaries. The architecture and training of these compo-
nents are detailed in the Appendix. We denote x0 as the
original image (3 channel RGB), and x as the pre-processed
image (9 channels: 1 for foreground and 8 for angles).
2.1. Part A: External memory
To decide where to look next based on the already seg-
mented objects, we incorporate an external memory, which
provides object boundary details from all previous steps. We
hypothesize that providing information of the completed seg-
mentation helps the network reason about occluded objects
and determine the next region of interest. The canvas has
10 channels in total: the ﬁrst channel of the canvas keeps
adding new pixels from the output of the previous time step,
and the other channels store the input image.
ct =
(
0,
if t = 0
max(ct−1, yt−1),
otherwise
(1)
dt = [ct, x]
(2)
2.2. Part B: Box network
The box network plays a critical role, localizing the next
object of interest. The CNN in the box network outputs a
H′ × W ′ × L feature map ut (H′ is the height; W ′ is the

Figure 2: Left: Detailed network design. Right: Sketch of training, and scheduled sampling; during training, the weighting of ground-truth
instance segmentations relative to model predictions (θt) decays to zero.
Figure 3: Illustration of the output of the pretrained FCN. Left: input image. Middle: predicted foreground. Right: predicted angle map.
width; L is the feature dimension). CNN activation based
on the entire image is too complex and inefﬁcient to pro-
cess simultaneously. Simple pooling does not preserve loca-
tion; instead we employ a “soft-attention” (dynamic pooling)
mechanism here to extract useful information along spatial
dimensions, weighted by αh,w
t
. Since a single glimpse may
not give the upper network enough information to decide
where exactly to draw the box, we allow the glimpse LSTM
to look at different locations by feeding a dimension L vector
each time. α is initialized to be uniform over all locations,
and τ indexes the glimpses.
ut = CNN(dt)
(3)
zt,τ =



0,
if τ = 0
LSTM(zt,τ−1, P
h,w
αh,w
t,τ−1uh,w,l
t
)
otherwise
(4)
αh,w
t,τ =
(
1/(H′ × W ′),
if τ = 0
MLP(zt,τ),
otherwise
(5)
We pass the LSTM’s hidden state through a linear layer to
obtain predicted box coordinates. We parameterize the box
by its normalized center (˜gX, ˜gY ), and size (log ˜δX, log ˜δY ).
A scaling factor γ is also predicted by the linear layer, and
used when re-projecting the patch to the original image size.
[˜gX,Y , log ˜δX,Y , log σX,Y , γ] = w⊤
b zt,end + wb0
(6)
gX = (˜gX + 1)W/2
(7)
gY = (˜gY + 1)H/2
(8)
δX = ˜δXW
(9)
δY = ˜δY H
(10)
Extracting a sub-region. We follow DRAW [16] and
use a Gaussian interpolation kernel to extract an ˜H × ˜W
patch from the ˜x, a concatenation of the original image
with dt. We further allow the model to output rectangular
patches to account for different shapes of the object. i, j
index the location in the patch of dimension ˜H × ˜W, and
a, b index the location in the original image. FX and FY are

matrices of dimension W × ˜W and H × ˜H, which indicates
the contribution of the location (a, b) in the original image
towards the location (i, j) in the extracted patch. µX,Y and
σX,Y are mean and variance of the Gaussian interpolation
kernel, predicted by the box network.
µi
X = gX + (δX + 1) · (i −˜W/2 + 0.5)/ ˜W
(11)
µj
Y = gY + (δY + 1) · (j −˜H/2 + 0.5)/ ˜H
(12)
F a,i
X =
1
√
2πσX
exp

−(a −µi
X)2
2σ2
X

(13)
F b,j
Y
=
1
√
2πσY
exp
 
−(b −µj
Y )2
2σ2
Y
!
(14)
˜xt = [x0, dt]
(15)
pt = Extract(˜xt, FY , FX) ≡F ⊤
Y ˜xtFX
(16)
2.3. Part C: Segmentation network
The remaining task is to segment out the pixels that be-
long to the dominant object within the window. The seg-
mentation network ﬁrst utilizes a convolution network to
produce a feature map vt. We then adopt a variant of the
DeconvNet [30] with skip connections, which appends de-
convolution (or convolution transpose) layers to upsample
the low-resolution feature map to a full-size segmentation.
After the fully convolutional layers we get a patch-level seg-
mentation prediction heat map ˜yt. We then re-project this
patch prediction to the original image using the transpose of
the previously computed Gaussian ﬁlters; the learned γ mag-
niﬁes the signal within the bounding box, and a constant β
suppresses the pixels outside the box. Applying the sigmoid
function produces segmentation values between 0 and 1.
vt = CNN(pt)
(17)
˜yt = D-CNN(vt)
(18)
yt = sigmoid
 γ · Extract(˜yt, F ⊤
Y , F ⊤
X ) −β

(19)
2.4. Part D: Scoring network
To estimate the number of objects in the image, and to
terminate our sequential process, we incorporate a scoring
network, similar to the one presented in [36]. Our scoring
network takes information from the hidden states of both the
box network (zt) and segmentation network (vt) to produce
a score between 0 and 1.
st = sigmoid(w⊤
zszt,end + w⊤
vsvt + ws0)
(20)
Termination condition. We train the entire model with
a sequence length determined by the maximum number of
objects plus one. During inference, we cut off iterations once
the output score goes below 0.5. The score loss function (de-
scribed below) encourages scores to decrease monotonically.
2.5. Loss functions
Joint loss. The total loss function is a sum of three losses:
the segmentation matching IoU loss Ly; the box IoU loss
Lb; and the score cross-entropy loss Ls:
L(y, b, s) = Ly(y, y∗) + λbLb(b, b∗) + λsLs(s, s∗) (21)
We ﬁx the loss coefﬁcients λb and λs to be 1 for all of our
experiments.
(a) Matching IoU loss (mIOU). A primary challenge of
instance segmentation involves matching model and ground-
truth instances. We compute a maximum-weighted bipartite
graph matching between the output instances and ground-
truth instances (c.f., [40] and [36]). Matching makes the
loss insensitive to the ordering of the ground-truth instances.
Unlike coverage scores proposed in [39] it directly penalizes
both false positive and false negative segmentations. The
matching weight Mi,j is the IoU score between a pair of seg-
mentations. We use the Hungarian algorithm to compute the
matching; we do not back-propagate the network gradients
through this algorithm.
Mi,j = softIOU(yi, y∗
j) ≡
P yi · y∗
j
P yi + y∗
j −yi · y∗
j
(22)
Ly(y, y∗) = −mIOU(y, y∗)
(23)
≡−1
N
X
i,j
Mi,j1[match(yi) = y∗
j]
(24)
(b) Soft box IoU loss. Although the exact IoU can be
derived from the 4-d box coordinates, its gradient vanishes
when two boxes do not overlap, which can be problematic for
gradient-based learning. Instead, we propose a soft version
of the box IoU. We use the same Gaussian ﬁlter to re-project
a constant patch on the original image, pad the ground-truth
boxes, and then compute the mIOU between the predicted
box and the matched padded ground-truth bounding box that
is scaled proportionally in both height and width.
bt = sigmoid(γ · Extract(1, F ⊤
Y , F ⊤
X ) −β)
(25)
Lb(b, b∗) = −mIOU(b, Pad(b∗))
(26)
(c) Monotonic score loss. To facilitate automatic termi-
nation, the network should output more conﬁdent objects
ﬁrst. We formulate a loss function that encourages monoton-
ically decreasing values in the score output. Iterations with
target score 1 are compared to the lower bound of preced-
ing scores, and 0 targets to the upper bound of subsequent
scores.
Ls(s, s∗) = 1
T
X
t
−s∗
t log

min
t′≤t{st′}

−(1 −s∗
t ) log

1 −max
t′≥t {st′}
 (27)

2.6. Training procedure
Bootstrap training. The box and segmentation networks
rely on the output of each other to make decisions for the next
time-step. Due to the coupled nature of the two networks,
we propose a bootstrap training procedure: these networks
are pre-trained with ground-truth segmentation and boxes,
respectively, and in later stages we replace the ground-truth
with the model predicted values.
Scheduled sampling. To smooth out the transition be-
tween stages, we explore the idea of “scheduled sampling”
[4], where we gradually remove the reliance on ground-truth
segmentation at the input of the network. As shown in Fig-
ure 2, during training there is a stochastic switch (θt) in
the input of the external memory, to utilize either the max-
imally overlapping ground-truth instance segmentation, or
the output of the network from the previous time step. By the
end of the training, the model completely relies on its own
output from the previous step, which matches the test-time
inference procedure.
3. Related Work
Instance segmentation has recently received a burst of
research attention, as it provides higher level of precision
of image understanding compared to object detection and
semantic segmentation.
Detector-based approaches. Early work on object seg-
mentation [5] starts from a trained class detectors, which
provide a bottom-up merging criterion based on top-down
detections. [43] extends this approach with a DPM detec-
tor, and uses layered graphical models to reason about in-
stance separation and occlusion. Besides boxes, models can
also leverage region proposals and region descriptors [6].
Simultaneous detection and segmentation (SDS) methods
[17, 18, 25, 24] apply newer CNN-based detectors such as
RCNN [14], and perform segmentation using the bounding
boxes as starting point. [24] proposes a trainable iterative
reﬁning procedure. Dai et al. [10] propose a pipeline-based
approach which ﬁrst predicts bounding box proposals and
then performs segmentation within each ROI. Similarly, we
jointly learn a detector and a segmentor; however, we intro-
duce a direct feedback loop in the sequence of predictions.
In addition, we learn the sequence ordering.
Graphical model approaches. Another line of research
is to use generative graphical model to express the depen-
dency structure among instances and pixels. Eslami et al.
[11] proposes a restricted Boltzmann machine to capture
high-order pixel interactions for shape modelling. A multi-
stage pipeline proposed by Silberman et al. [39] is composed
of patch-wise features based on deep learning, combined into
a segmentation tree. More recently, Zhang et al. [45] for-
mulate a dense CRF for instance segmentation; they apply
a CNN on dense image patches to make local predictions,
and construct a dense CRF to produce globally consistent
labellings. Their key contribution is a shifting-label potential
that encourages consistency across different patches. The
graphical model formulation entails long running times, and
their energy functions are dependent on instances being con-
nected and having a clear depth ordering.
Fully convolutional approaches. Fully convolutional
networks [27] has emerged as a powerful tool to directly
predict dense pixel labellings. Pinheiro et al. [33, 34] train a
CNN to generate object segmentation proposals, which runs
densely on all windows at multiple scales, a nd Dai et al. [9]
uses relative location as additional pixel labels. The output
of both systems are object proposals, which require further
processing to get ﬁnal segmentations. Other approaches us-
ing FCNs are proposal-free, but rely on a bottom-up merging
process. Liang et al. [26] predict dense pixel prediction of ob-
ject location and size, using clustering as a post-processing
step. Uhrig et al.
[41] present another approach based
on FCNs, which is trained to produces a semantic segmen-
tation as well as an instance-aware angle map, encoding
the instance centroids. Post-processing based on template
matching and instance fusion produces the instance identi-
ties. Importantly, they also used ground-truth depth labels in
training. Concurrent work [2, 19, 22] also explores a similar
idea of using FCNs to output instance-sensitive embeddings.
RNN approaches. Another recent line of research, e.g.,
[40, 32, 36] employs end-to-end recurrent neural networks
(RNN) to perform object detection and segmentation. The
sequential decomposition idea for structured prediction is
also explored in [20, 3]. A permutation agnostic loss func-
tion based on maximum weighted bipartite matching was
proposed by [40]. To process an entire image, they treat
each element of a CNN feature map individually. Similarly,
our box proposal network also uses an RNN to generate box
proposals: instead of running hundreds of RNN iterations,
we only run it for a small number of iterations using a soft at-
tention mechanism [42]. Romera-Paredes and Torr [36] use
convolutional LSTM (ConvLSTM) [38] to produce instance
segmentation directly. However, since their ConvLSTM is
required to handle object detection, inhibition, and segmen-
tation all convolutionally on a global scale, and it is hard for
their model to inhibit far apart instances. In contrast, our
architecture incorporates direct feedback from the predic-
tion of the previous instance, providing precise boundary
inhibition, and our box network conﬁnes the instance-wise
segmentation within a local window.
4. Experiments
We refer readers to the Appendix for hyper-parameters
and other training details of the experiments 1.
1Our code is released at: https://github.com/renmengye/
rec-attend-public

4.1. Datasets & Evaluation
CVPPP leaf segmentation. One instance segmentation
benchmark is the CVPPP plant leaf dataset [28], which was
developed due to the importance of instance segmentation
in plant phenotyping. We ran the A1 subset of CVPPP plant
leaf segmentation dataset. We trained our model on 128
labeled images, and report results on the 33 test images. We
compare our performance to [36], and other top approaches
that were published with the CVPPP conference; see the
collation study [37] for details of these other approaches.
KITTI car segmentation. Instance segmentation also
provides rich information in the context of autonomous driv-
ing. Following [46, 45, 41], we also evaluated the model
performance on the KITTI car segmentation dataset. We
trained the model with 3,712 training images, and report
performance on 120 validation images and 144 test images.
Cityscapes. Cityscapes provides multi-class instance-
level annotation and is currently the most comprehensive
benchmark for instance segmentation, containing 2,975 train-
ing images, 500 validation images, and 1,525 test images,
with 8 semantic classes (person, rider, car, truck, bus, train,
motorcycle, bicycle). We train our instance segmentation
network as a class-agnostic model for all classes, and apply
a semantic segmentation mask obtained from [13] on top of
our instance output. Since “car” is the most common class,
we report both average score on all classes, and individual
score on the “car” class.
MS-COCO. To test the effectiveness and portability of
our algorithm, we train our model on a subset of MS-COCO.
As an initial study we select images of zebras, and train on
1000 images. Since there are no methods that are directly
comparable, we leave the quantatitive results for Appendix.
Ablation studies. We also examine the relative impor-
tance of model components via ablation studies, and report
validation performance on the KITTI dataset.
• No pre-processing. This network is trained to take as
input raw image pixels, without the foreground segmen-
tation or the angle map.
• No box net. Instead of predicting segmentation within
a box, the output dimension of the segmentation net-
work is the full image.
• No angles. The pre-processor predicts the foreground
segmentation only, without the angle map.
• No scheduled sampling. This network has the same
architecture but trained without scheduled sampling
(see Section 2.6), i.e., at training time, always use the
maximum overlapped ground-truth.
• Fewer iterations. The box network has fewer glimpses
on the convnet feature map (fewer LSTM iterations).
Table 1: Leaf segmentation and counting performance, averaged
over all test images, with standard deviation in parentheses
SBD ↑
|DiC| ↓
RIS+CRF [36]
66.6 (8.7)
1.1 (0.9)
MSU [37]
66.7 (7.6)
2.3 (1.6)
Nottingham [37]
68.3 (6.3)
3.8 (2.0)
Wageningen [44]
71.1 (6.2)
2.2 (1.6)
IPK [31]
74.4 (4.3)
2.6 (1.8)
PRIAn [15]
-
1.3(1.2)
Ours
84.9 (4.8)
0.8 (1.0)
Table 2: KITTI vehicle segmentation results
Set
MWCov
MUCov
AvgFP
AvgFN
DepthOrder [46]
test
70.9
52.2
0.597
0.736
DenseCRF [45]
test
74.1
55.2
0.417
0.833
AngleFCN+Depth [41]
test
79.7
75.8
0.201
0.159
Ours
test
80.0
66.9
0.764
0.201
Evaluation metrics. We evaluate based on the metrics
used by the other studies in the respective benchmarks. See
Appendix for detailed equations for these metrics.
For segmentation, symmetric best dice (SBD) is used for
leaves. (see Equations 28, 30).
DICE(A, B) =
2|A ˆB|
|A| + |B|
(28)
BD({Ai}, B) = max
i
DICE(Ai, B)
(29)
SBD(yi}, {y∗
j }) = min( 1
N
X
j
BD({yi}, y∗
j ),
1
M
X
i
BD({y∗
j }, yi))
(30)
Mean (weighted) coverage (MWCov, MUCov) are used for
KITTI car segmentation. (see Equations 32, 31). The cover-
age scores measure the instance-wise IoU for each ground-
truth instance averaged over the image; MWCov further
weights the score by the size of the ground-truth instance
segmentation (larger objects get larger weights).
MUCov({yi}, {y∗
j }) =
X
i
1
N max
j
IoU(yi, y∗
j )
(31)
MWCov({yi}, {y∗
j }) =
X
i
wcov,i max
j
IoU(yi, y∗
j ) (32)
wcov,i =
|yi|
P
i |yi|
(33)
The Cityscapes evaluation uses average precision (AP), (see
Equation 34), which counts the precision between a pair

Image
GT
Ours
Image
GT
Ours
Sequence color legend:
Figure 4: Examples of our instance segmentation output on CVPPP leaf dataset. In this paper, instance colors are determined by the order
of the model output sequence.
Image
GT
Ours
Image
GT
Ours
Figure 5: Examples of our instance segmentation output on MS-COCO zebra images.
Table 3: Cityscapes instance-level segmentation results
Set
AP
AP50%
AP50m
AP100m
MCG+RCNN [8]
all
4.6
12.9
7.7
10.3
AngleFCN+Depth [41]
all
8.9
21.1
15.3
16.7
Ours
all
9.5
18.9
16.8
20.9
MCG+RCNN [8]
car
10.5
26.0
17.5
21.2
AngleFCN+Depth [41]
car
22.5
37.8
36.4
40.7
Ours
car
27.5
41.9
46.8
54.2
of matched prediction and groundtruth, for a range of IoU
threshold values.
AP({yi}, {y∗
j }) = max
s
X
θ
X
j
Pr(ys(i), yj)·
1[IoU(ys(i), y∗
j ) ≥θ],
(34)
where s is a function that permutes the predicted instance
order, and θ is the threshold from 0.5 to 0.95. Other scores
include AP50% for a threshold of 0.5, and AP50m, 100m for a
subset of instances within 50m and 100m.
Counting is measured in absolute difference in count
(|DiC|) (i.e., mean absolute error), (see Equation 35), average
false positive (AvgFP), and average false negative (AvgFN).
False positive is the number of predicted instances that do
not overlap with the ground-truth, and false negative is the
number of ground-truth instances that do not overlap with
the prediction.
|DiC| = 1
N
X
i
|counti −count∗
i |
(35)
Table 4: Ablation results on KITTI validation
Set
MWCov
MUCov
AvgFP
AvgFN
No Pre Proc.
val
55.6
45.0
0.125
0.417
No Box Net
val
57.0
49.1
0.757
0.375
No Angle
val
71.2
63.3
0.542
0.342
No Sched. Samp.
val
73.6
63.9
0.350
0.317
Iter-1
val
64.1
54.8
0.200
0.375
Iter-3
val
71.3
63.4
0.417
0.308
Full (Iter-5)
val
75.1
64.6
0.375
0.283
4.2. Results & Discussion
Example results on the leaf segmentation task are shown
in Figure 4. On this task, our best model outperforms the pre-
vious state-of-the-art by a large margin in both segmentation
and counting (see Table 1). In particular, our method has sig-
niﬁcant improvement over a previous RNN-based instance
segmentation method [36]. We found that our model with the
FCN pre-processor overﬁt on this task, and we thus utilized
the simpler version without input pre-processing. This is not
surprising, as the dataset is very small, and including the
FCN signiﬁcantly increases the input dimension and number
of parameters.
On the KITTI task, Figure 6 row 4-5 shows that our
model can segment cars in a wide variety of poses. Our
method out-performs [46, 45], but scores lower than [41]
(see Table 2). One possible explanation is their inclusion
of depth information during training, which may help the
model disambiguate distant object boundaries. Moreover,
their bottom-up “instance fusion” method plays a crucial role
(omitting this leads to a steep performance drop); this likely
helps segment smaller objects, whereas our box network
does not reliably detect distant cars.

Image
GT
Ours
Figure 6: Examples of our instance segmentation output on Cityscapes (row 1-3) and KITTI (row 4-5).
The displayed segmentations demonstrate that our top-
down attentional inference is crucial for a good instance
recognition model. This allows disconnected components
belonging to objects such as a bicycle (Figure 6 row 2) to
be recognized as a whole piece whereas [45] models the
connectedness as their energy potential. Our model also
shows impressive results in heavily occluded scenes, when
only small proportion of the car is visible (Figure 6 row 1),
and when the zebra in the back reveals two disjoint pieces
of its body (Figure 5 right). Another advantage is that our
model directly outputs the ﬁnal segmentation, eliminating
the need for post- processing, which is often required by
other methods (e.g. [26, 41]).
Our method also learns to rank the importance of in-
stances through visual attention. During training, the model
ﬁrst attended to objects in a spatial ordering (e.g. left to
right), and then gradually shifted to a more sophisticated
ordering based on conﬁdence, with larger attentional jumps
in between timesteps.
Some failure cases of our approach, e.g., omitting distant
objects and under- segmentation, may be explained by our
downsampling factor; to manage training time, we down-
sample KITTI and Cityscapes by a factor around 4, whereas
[41] does not do any any downsampling. We also observe
a lack of higher-order reasoning, e.g., the inclusion of the
“third” limb of a person in the bottom of Figure 6. As future
work, these problems can be addressed by a combination of
our method with bottom-up merging methods (e.g. [26, 41])
and higher order graphical models (e.g. [29, 11]).
Finally, our ablation studies (see Table 4) help elucidate
the contribution of some model components. The initial
foreground segmentation, and the box network both play
crucial roles , as seen in the coverage measures. Scheduled
sampling results in slightly better performance, by making
training resemble testing, gradually forcing the model to
carry out a full sequence. Larger numbers of glimpses of
the LSTM (Iter-n) helps the model signiﬁcantly by allowing
more information to be considered before outputting the
next region of interest. Finally, we note that KITTI has a
fairly small validation and test set, so these results are highly
variable (see last lines of Table 4 versus Table 2).
5. Conclusion
In this work, we borrow intuition from human counting
and formulate instance segmentation as a recurrent attentive
process. Our end-to-end recurrent architecture demonstrates
signiﬁcant improvement compared to earlier formulations
using RNN on the same tasks, and shows state-of-the-art
results on challenging instance segmentation datasets. We
address the classic object occlusion problem with an external
memory, and the attention mechanism permits segmentation
at a ﬁne resolution.
Our attentional architecture signiﬁcantly reduces the num-
ber of parameters, and the performance is quite strong de-
spite being trained with only 100 leaf images and under
3,000 road scene images. Since our model is end-to-end
trainable and does not depend on prior knowledge of the
object type (e.g. size, connectedness), we expect our method

performance to scale directly with the number of labelled
images, which is certain to increase as this task gains in pop-
ularity and new datasets become available. As future work,
we are currently extending our model to tackle highly multi-
class instance segmentation, such as the MS-COCO dataset,
and more structured understanding of everyday scenes.
Acknowledgements
Supported by Samsung and the Intelli-
gence Advanced Research Projects Activity (IARPA) via Depart-
ment of Interior/Interior Business Center (DoI/IBC) contract num-
ber D16PC00003. The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes notwithstanding
any copyright annotation thereon. Disclaimer: The views and con-
clusions contained herein are those of the authors and should not
be interpreted as necessarily representing the ofﬁcial policies or
endorsements, either expressed or implied, of IARPA, DoI/IBC, or
the U.S. Government.
References
[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. VQA: Visual question answering. In
ICCV, 2015.
[2] M. Bai and R. Urtasun. Deep watershed transform for instance
segmentation. CoRR, abs/1611.08303, 2016.
[3] D. Banica and C. Sminchisescu. Second-order constrained
parametric proposals and sequential search-based structured
prediction for semantic segmentation in RGB-D images. In
CVPR, 2015.
[4] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Sched-
uled sampling for sequence prediction with recurrent neural
networks. In NIPS, 2015.
[5] E. Borenstein and S. Ullman. Class-speciﬁc, top-down seg-
mentation. In ECCV, 2002.
[6] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Free-
form region description with second-order pooling. TPAMI,
37(6):1177–1189, 2015.
[7] P. Chattopadhyay, R. Vedantam, R. S. Ramprasaath, D. Batra,
and D. Parikh. Counting everyday objects in everyday scenes.
CoRR, abs/1604.03505, 2016.
[8] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele.
The
cityscapes dataset for semantic urban scene understanding.
CoRR, abs/1604.01685, 2016.
[9] J. Dai, K. He, Y. Li, S. Ren, and J. Sun. Instance-sensitive
fully convolutional networks. In ECCV, 2016.
[10] J. Dai, K. He, and J. Sun. Instance-aware semantic segmenta-
tion via multi-task network cascades. CoRR, abs/1512.04412,
2015.
[11] S. M. A. Eslami, N. Heess, C. K. I. Williams, and J. M. Winn.
The shape boltzmann machine: A strong model of object
shape. IJCV, 107(2):155–176, 2014.
[12] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? The KITTI vision benchmark suite. In
CVPR, 2012.
[13] G. Ghiasi and C. C. Fowlkes. Laplacian pyramid reconstruc-
tion and reﬁnement for semantic segmentation. In ECCV,
2016.
[14] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich
feature hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.
[15] M. V. Giuffrida, M. Minervini, and S. Tsaftaris. Learning to
count leaves in rosette plants. In Proceedings of the Computer
Vision Problems in Plant Phenotyping (CVPPP), 2015.
[16] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and
D. Wierstra. DRAW: A recurrent neural network for image
generation. In ICML, 2015.
[17] B. Hariharan, P. A. Arbel´aez, R. B. Girshick, and J. Malik.
Simultaneous detection and segmentation. In ECCV, 2014.
[18] B. Hariharan, P. A. Arbel´aez, R. B. Girshick, and J. Malik.
Hypercolumns for object segmentation and ﬁne-grained lo-
calization. In CVPR, 2015.
[19] Z. Hayder, X. He, and M. Salzmann. Shape-aware instance
segmentation. CoRR, abs/1612.03129, 2016.
[20] H. D. III, J. Langford, and D. Marcu. Search-based structured
prediction. Machine Learning, 75(3):297–325, 2009.
[21] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In ICLR, 2015.
[22] A. Kirillov, E. Levinkov, B. Andres, B. Savchynskyy, and
C. Rother. InstanceCut: from edges to instances with multicut.
CoRR, abs/1611.08272.
[23] V. S. Lempitsky and A. Zisserman. Learning to count objects
in images. In NIPS, 2010.
[24] K. Li, B. Hariharan, and J. Malik. Iterative instance segmen-
tation. In CVPR, 2016.
[25] X. Liang, Y. Wei, X. Shen, Z. Jie, J. Feng, L. Lin, and S. Yan.
Reversible recursive instance-level object segmentation. In
CVPR, 2016.
[26] X. Liang, Y. Wei, X. Shen, J. Yang, L. Lin, and S. Yan.
Proposal-free network for instance-level object segmentation.
CoRR, abs/1509.02636, 2015.
[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In CVPR, 2015.
[28] M. Minervini, A. Fischbach, H. Scharr, and S. A. Tsaftaris.
Finely-grained annotated datasets for image-based plant phe-
notyping. Pattern Recognition Letters, 2015. Special Issue
on Fine-grained Categorization in Ecological Multimedia.
[29] V. C. Nguyen, N. Ye, W. S. Lee, and H. L. Chieu. Conditional
random ﬁeld with high-order dependencies for sequence la-
beling and segmentation. JMLR, 15(1):981–1009, 2014.
[30] H. Noh, S. Hong, and B. Han. Learning deconvolution net-
work for semantic segmentation. In ICCV, 2015.
[31] J. Pape and C. Klukas. 3-d histogram-based segmentation and
leaf detection for rosette plants. In ECCV Workshops, 2014.
[32] E. Park and A. C. Berg. Learning to decompose for object
detection and instance segmentation. In ICLR Workshop,
2016.
[33] P. H. O. Pinheiro, R. Collobert, and P. Doll´ar. Learning to
segment object candidates. In NIPS, 2015.
[34] P. O. Pinheiro, T. Lin, R. Collobert, and P. Doll´ar. Learning
to reﬁne object segments. In ECCV, pages 75–91, 2016.
[35] M. Ren, R. Kiros, and R. S. Zemel. Exploring models and
data for image question answering. In NIPS, 2015.
[36] B. Romera-Paredes and P. H. S. Torr. Recurrent instance
segmentation. CoRR, abs/1511.08250, 2015.

[37] H. Scharr, M. Minervini, A. P. French, C. Klukas, D. M.
Kramer, X. Liu, I. Luengo, J. Pape, G. Polder, D. Vukadi-
novic, X. Yin, and S. A. Tsaftaris. Leaf segmentation in plant
phenotyping: A collation study. Mach. Vis. Appl., 27(4):585–
606, 2016.
[38] X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong, and W. Woo.
Convolutional LSTM network: A machine learning approach
for precipitation nowcasting. In NIPS, 2015.
[39] N. Silberman, D. Sontag, and R. Fergus. Instance segmenta-
tion of indoor scenes using a coverage loss. In ECCV, 2014.
[40] R. Stewart and M. Andriluka. End-to-end people detection in
crowded scenes. CoRR, abs/1506.04878, 2016.
[41] J. Uhrig, M. Cordts, U. Franke, and T. Brox. Pixel-level en-
coding and depth layering for instance-level semantic labeling.
In GCPR, 2016.
[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhut-
dinov, R. S. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention. In
ICML, 2015.
[43] Y. Yang, S. Hallman, D. Ramanan, and C. C. Fowlkes.
Layered object models for image segmentation.
TPAMI,
34(9):1731–1743, 2012.
[44] X. Yin, X. Liu, J. Chen, and D. M. Kramer. Multi-leaf track-
ing from ﬂuorescence plant videos. In ICIP, 2014.
[45] Z. Zhang, S. Fidler, and R. Urtasun. Instance-level segmenta-
tion with deep densely connected MRFs. In CVPR, 2016.
[46] Z. Zhang, A. G. Schwing, S. Fidler, and R. Urtasun. Monoc-
ular object instance segmentation and depth ordering with
CNNs. In ICCV, 2015.
A. Training procedure speciﬁcation
We used the Adam optimizer [21] with learning rate 0.001
and batch size of 8. The learning rate is multiplied by 0.85
for every 5000 steps of training.
A.1. Scheduled sampling
We denote θt as the probability of feeding in ground-
truth segmentation that has the greatest overlap with the
previous prediction, as opposed to model output. θt decays
exponentially as training proceeds, and for larger t, the decay
occurs later:
θt = min

Γt exp

−epoch −S
S2

, 1

(36)
Γt = 1 + log(1 + Kt)
(37)
where epoch is the training index, S, S2, and K are con-
stants. In the experiments reported here, these values are
10000, 2885, and 3.
B. More experimental results
We include the segmentation and counting performance
on the MS-COCO zebra images in Table 5. In terms of
counting, our model out-performs a baseline method that
runs an object detector and then non-maximal suppression,
and a new associative-subitizing method [7].
Table 5: MS-COCO Zebra Results
MWCov ↑
MUCov ↑
|DiC| ↓
Acc. ↑
detect [7]
-
-
2.56
-
aso-sub [7]
-
-
1.03
-
Ours
69.2
64.2
0.79
0.57
C. Model architecture
C.1. Foreground + Orientation FCN
We resize the image to uniform size. For CVPPP and MS-
COCO dataset, we adopt a uniform size of 224 × 224, for
KITTI, we adopt 128 × 448, and for Cityscapes 256 × 512
(4x downsampling). Table 6 lists the speciﬁcation of all
layers.
C.2. External memory
C.3. Box network
The box network takes in 9 channels of input directly
from the output of the FCN. It goes through a CNN structure
again and uses the attention vector predicted by the LSTM
to perform dynamic pooling in the last layer. The CNN
hyperparameters are listed in Table 8 and the LSTM and
glimpse MLP hyperparameters are listed in Table 9. The
glimpse MLP takes input from the hidden state of the LSTM
and outputs a vector of normalized weighting over all the
box CNN feature map spatial grids.
C.4. Segmentation network
The segmentation networks takes in a patch of size 48×48
with multiple channels. The ﬁrst three channels are the
original image R, G, B channels. Then there are 8 channels
of orientation angles, and then 1 channel of foreground heat
map, all predicted by FCN. Full details are listed in Table 10.
Constant β is chosen to be 5.

Table 6: FCN speciﬁcation
Name
Type
Input
Spec (size/stride)
Size CVPPP/MS-COCO
Size KITTI
Size Cityscapes
input
input
-
-
224 × 224 × 3
128 × 448 × 3
256 × 512 × 3
conv1-1
conv
input
3 × 3 × 3 × 32
224 × 224 × 32
128 × 448 × 64
256 × 512 × 64
conv1-2
conv
conv1-1
3 × 3 × 32 × 64
224 × 224 × 64
128 × 448 × 32
256 × 512 × 32
pool1
pool
conv1-2
max 2 × 2
112 × 112 × 64
64 × 224 × 64
128 × 256 × 64
conv2-1
conv
pool1
3 × 3 × 64 × 64
112 × 112 × 64
64 × 224 × 64
128 × 256 × 64
conv2-2
conv
conv2-1
3 × 3 × 64 × 96
112 × 112 × 96
64 × 224 × 96
128 × 256 × 96
pool2
pool
conv2-2
max 2 × 2
56 × 56 × 96
32 × 112 × 96
64 × 128 × 96
conv3-1
conv
pool2
3 × 3 × 96 × 96
56 × 56 × 96
32 × 112 × 96
64 × 128 × 96
conv3-2
conv
conv3-1
3 × 3 × 96 × 128
56 × 56 × 128
32 × 112 × 128
64 × 128 × 128
pool3
pool
conv3-2
max 2 × 2
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-1
conv
pool3
3 × 3 × 128 × 128
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-2
conv
conv4-1
3 × 3 × 128 × 128
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-3
conv
conv4-2
3 × 3 × 128 × 128
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-4
conv
conv4-3
3 × 3 × 128 × 128
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-5
conv
conv4-4
3 × 3 × 128 × 128
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-6
conv
conv4-5
3 × 3 × 128 × 128
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-7
conv
conv4-6
3 × 3 × 128 × 128
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
conv4-8
conv
conv4-7
3 × 3 × 128 × 256
28 × 28 × 256
16 × 56 × 256
32 × 64 × 256
pool4
pool
conv4-8
max 2 × 2
14 × 14 × 256
8 × 28 × 256
16 × 32 × 256
conv5-1
conv
pool4
3 × 3 × 256 × 256
14 × 14 × 256
8 × 28 × 256
16 × 32 × 256
conv5-2
conv
conv5-1
3 × 3 × 256 × 256
14 × 14 × 256
8 × 28 × 256
16 × 32 × 256
conv5-3
conv
conv5-2
3 × 3 × 256 × 256
14 × 14 × 256
8 × 28 × 256
16 × 32 × 256
conv5-4
conv
conv5-3
3 × 3 × 256 × 512
14 × 14 × 512
8 × 28 × 512
16 × 32 × 512
pool5
pool
conv5-4
max 2 × 2
7 × 7 × 512
4 × 14 × 512
8 × 16 × 512
deconv6-1
deconv
pool5
3 × 3 × 256 × 512/2
14 × 14 × 256
8 × 28 × 256
16 × 32 × 256
deconv6-2
deconv
deconv6-1 + conv5-3
3 × 3 × 256 × 512
14 × 14 × 256
8 × 28 × 256
16 × 32 × 256
deconv7-1
deconv
deconv6-2
3 × 3 × 128 × 256/2
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
deconv7-2
deconv
deconv7-1 + conv4-7
3 × 3 × 128 × 256
28 × 28 × 128
16 × 56 × 128
32 × 64 × 128
deconv8-1
deconv
deconv7-2
3 × 3 × 96 × 128/2
56 × 56 × 96
32 × 112 × 96
64 × 128 × 96
deconv8-2
deconv
deconv8-1 + conv3-1
3 × 3 × 96 × 192
56 × 56 × 96
32 × 112 × 96
64 × 128 × 96
deconv9-1
deconv
deconv8-2
3 × 3 × 64 × 96/2
112 × 112 × 64
64 × 224 × 64
128 × 256 × 64
deconv9-2
deconv
deconv9-1
3 × 3 × 64 × 64
112 × 112 × 64
64 × 224 × 64
128 × 256 × 64
deconv10-1
deconv
deconv9-2
3 × 3 × 32 × 64/2
224 × 224 × 32
128 × 448 × 32
256 × 512 × 32
deconv10-2
deconv
deconv10-1
3 × 3 × 32 × 32
224 × 224 × 32
128 × 448 × 32
256 × 512 × 32
deconv10-3
deconv
deconv10-2 + input
3 × 3 × 9 × 35
224 × 224 × 9
128 × 448 × 9
256 × 512 × 9
Table 7: External memory speciﬁcation
Name
Filter spec
Size CVPPP/MS-COCO
Size KITTI
Size Cityscapes
ConvLSTM
3 × 3
224 × 224 × 9
128 × 448 × 9
256 × 512 × 9
Table 8: Box network CNN speciﬁcation
Name
Type
Input
Spec (size/stride)
Size CVPPP/MS-COCO
Size KITTI
Size Cityscapes
input
input
-
-
224 × 224 × 9
128 × 448 × 9
256 × 512 × 9
conv1-1
conv
input
3 × 3 × 9 × 16
224 × 224 × 16
128 × 448 × 16
256 × 512 × 16
pool1
pool
conv1-2
max 2 × 2
112 × 112 × 16
64 × 224 × 16
128 × 256 × 16
conv1-2
conv
conv1-1
3 × 3 × 16 × 16
112 × 112 × 16
64 × 224 × 16
128 × 256 × 16
pool1
pool
conv1-2
max 2 × 2
56 × 56 × 16
32 × 112 × 16
64 × 128 × 16
conv2-1
conv
pool1
3 × 3 × 16 × 32
56 × 56 × 32
32 × 112 × 32
64 × 128 × 32
conv2-2
conv
conv2-1
3 × 3 × 32 × 32
56 × 56 × 32
32 × 112 × 32
64 × 128 × 32
pool2
pool
conv2-2
max 2 × 2
28 × 28 × 32
16 × 56 × 32
32 × 64 × 32
conv3-1
conv
pool2
3 × 3 × 32 × 64
28 × 28 × 64
16 × 56 × 64
32 × 64 × 64
conv3-2
conv
conv3-1
3 × 3 × 64 × 64
28 × 28 × 64
16 × 56 × 64
32 × 64 × 64
pool3
pool
conv3-2
max 2 × 2
14 × 14 × 64
8 × 28 × 64
16 × 32 × 64
conv3-1
conv
pool2
3 × 3 × 64 × 64
14 × 14 × 64
8 × 28 × 64
16 × 32 × 64
conv3-2
conv
conv3-1
3 × 3 × 64 × 64
14 × 14 × 64
8 × 28 × 64
16 × 32 × 64
pool3
pool
conv3-2
max 2 × 2
7 × 7 × 64
4 × 14 × 64
8 × 16 × 64

Table 9: Box network LSTM speciﬁcation
Name
Size CVPPP/MS-COCO
Size KITTI
Size Cityscapes
LSTM
256
256
256
GlimpseMLP1
256
256
256
GlimpseMLP2
7 × 7
4 × 14
8 × 16
Table 10: Segmentation network speciﬁcation
Name
Type
Input
Spec (size/stride)
Size
input
input
-
-
48 × 48 × 13
conv1-1
conv
input
3 × 3 × 13 × 16
48 × 48 × 16
conv1-2
conv
conv1-1
3 × 3 × 16 × 32
48 × 48 × 32
pool1
pool
conv1-2
max 2 × 2
24 × 24 × 32
conv2-1
conv
pool1
3 × 3 × 32 × 32
24 × 24 × 32
conv2-2
conv
conv2-1
3 × 3 × 32 × 64
24 × 24 × 64
pool3
pool
conv2-2
max 2 × 2
12 × 12 × 64
conv3-1
conv
pool2
3 × 3 × 64 × 64
12 × 12 × 64
conv3-2
conv
conv3-1
3 × 3 × 64 × 96
12 × 12 × 96
pool3
pool
conv3-2
max 2 × 2
6 × 6 × 96
deconv4-1
deconv
pool3
3 × 3 × 64 × 96/2
12 × 12 × 64
deconv4-2
deconv
deconv4-1 + conv3-1
3 × 3 × 64 × 128
12 × 12 × 64
deconv5-1
deconv
deconv4-2 + conv2-2
3 × 3 × 32 × 128/2
24 × 24 × 32
deconv5-2
deconv
deconv5-1 + conv2-1
3 × 3 × 32 × 64
24 × 24 × 32
deconv6-1
deconv
deconv5-2 + conv1-2
3 × 3 × 16 × 64/2
48 × 48 × 16
deconv6-2
deconv
deconv6-1 + conv1-1
3 × 3 × 16 × 32
48 × 48 × 16
deconv6-3
deconv
deconv6-2 + input
3 × 3 × 1 × 29
48 × 48 × 1

