Formalizing Convergent Instrumental Goals
Tsvi Benson-Tilsen
UC Berkeley
Machine Intelligence Research Institute
Nate Soares
Machine Intelligence Research Institute
Abstract
Omohundro has argued that sufﬁciently advanced AI
systems of any design would, by default, have incen-
tives to pursue a number of instrumentally useful sub-
goals, such as acquiring more computing power and
amassing many resources. Omohundro refers to these as
“basic AI drives,” and he, along with Bostrom and oth-
ers, has argued that this means great care must be taken
when designing powerful autonomous systems, because
even if they have harmless goals, the side effects of pur-
suing those goals may be quite harmful. These argu-
ments, while intuitively compelling, are primarily philo-
sophical. In this paper, we provide formal models that
demonstrate Omohundro’s thesis, thereby putting math-
ematical weight behind those intuitive claims.
Introduction
At the end of Russell and Norvig’s textbook Artiﬁcial In-
telligence: A Modern Approach (2010) the authors pose a
question: What if we succeed? What will happen if human-
ity succeeds in developing an artiﬁcially intelligent system
that is capable of achieving difﬁcult goals across a variety of
real-world domains?
Bostrom (2014) and others have argued that this question
becomes especially important when we consider the creation
of “superintelligent” machines, that is, machines capable of
outperforming the best human brains in practically every
ﬁeld. Bostrom argues that superintelligent decision-making
systems that autonomously make and execute plans could
have an extraordinary impact on society, and that their im-
pact will not necessarily be beneﬁcial by default.
Bostrom
(2012),
Omohundro
(2008),
and
Yud-
kowsky (2011) have all argued that highly capable AI
systems pursuing goals that are not completely aligned with
human values could have highly undesirable side effects,
even if the goals seem otherwise harmless. The classic
example is Bostrom’s concept of a “paperclip maximizer,”
a powerful AI system instructed to construct paperclips—a
seemingly harmless task which could nevertheless have very
negative consequences if the AI system is clever enough to
make and execute plans that allow it to fool humans, amass
resources, and eventually turn as much matter as it possibly
Copyright c⃝2016, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
can into paperclips. Even if the system’s goals are laud-
able but not perfectly aligned with human values, similar
unforeseen consequences could occur: Soares (2015) gives
the example of a highly capable AI system directed to cure
cancer, which may attempt to kidnap human test subjects, or
proliferate robotic laboratories at expense of the biosphere.
Omohundro (2008) has argued that there are certain types
of actions that most highly capable autonomous AI systems
will have strong incentives to take, for instrumental reasons.
For example, a system constructed to always execute the ac-
tion that it predicts will lead to the most paperclips (with no
concern for any other features of the universe) will acquire
a strong incentive to self-preserve, assuming that the system
predicts that, if it were destroyed, the universe would con-
tain fewer paperclips than it would if the system remained
in working order. Omohundro argues that most highly ca-
pable systems would also have incentives to preserve their
current goals (for the paperclip maximizer predicts that if
its goals were changed, this would result in fewer future pa-
perclips) and amass many resources (the better to achieve
its goals with). Omohundro calls these behaviors and a few
others the “basic AI drives.” Bostrom (2012) reﬁnes this into
the “instrumental convergence” thesis, which states that cer-
tain instrumentally useful goals will likely be pursued by a
broad spectrum of intelligent agents—such goals are said to
be “convergent instrumental goals.”
Up until now, these arguments have been purely philo-
sophical. To some, Omohundro’s claim seems intuitively
obvious: Marvin Minsky speculated (Russell and Norvig
2010, section 26.3) that an artiﬁcial intelligence attempting
to prove the Riemann Hypothesis may decide to consume
Earth in order to build supercomputers capable of searching
through proofs more efﬁciently. To others, they seem pre-
posterous: Waser (2008) has argued that “ethics is actually
an attractor in the space of intelligent behavior,” and thus
highly capable autonomous systems are not as likely to pose
a threat as Omohundro, Bostrom, and others, have claimed.
In this paper, we present a mathematical model of intel-
ligent agents which lets us give a more formal account of
Omohundro’s basic AI drives, where we will demonstrate
that the intuitions of Omohundro and Bostrom were correct,
at least insofar as these simple models apply to reality.
Given that this paper primarily focuses on arguments
made by Omohundro and Bostrom about what sorts of be-
The Workshops of the Thirtieth AAAI Conference on Artificial Intelligence
AI, Ethics, and Society: Technical Report WS-16-02
62

havior we can expect from extremely capable (potentially
superintelligent) autonomous AI systems, we will be focus-
ing on issues of long-term safety and ethics. We provide a
mathematical framework in attempts to ground some of this
discussion—so that we can say, with conﬁdence, what a suf-
ﬁciently powerful agent would do in certain scenarios, as-
suming it could ﬁnd some way to do it—but the discussion
will nevertheless center on long-term concerns, with prac-
tical relevance only insofar as research can begin now in
preparation for hurdles that predictably lie ahead.
We begin in section with a bit more discussion of the
intuition behind the instrumental convergence thesis, before
moving on in section to describing our model of agents act-
ing in a universe to achieve certain goals. In section we will
demonstrate that Omohundro’s thesis does in fact hold in our
setting. Section will give an example of how our model can
apply to an agent pursuing goals. Section concludes with a
discussion of the beneﬁts and limitations of our current mod-
els, and different ways that the model could be extended and
improved.
Intuitions
Before proceeding, let us address one common objection
(given by Cortese (2014) and many others) that superintel-
ligent AI systems would be “inherently unpredictable,” and
thus there is nothing that can be said about what they will
do or how they will do it. To address this concern, it is use-
ful to distinguish two different types of unpredictability. It
is true that the speciﬁc plans and strategies executed by a
superintelligent planner could be quite difﬁcult for a hu-
man to predict or understand. However, as the system gets
more powerful, certain properties of the outcome generated
by running the system become more predictable. For exam-
ple, consider playing chess against a chess program that has
access to enormous amounts of computing power. On the
one hand, because it plays much better chess than you, you
cannot predict exactly where the program will move next.
But on the other hand, because it is so much better at chess
than you are, you can predict with very high conﬁdence how
the game will end.
Omohundro suggests predictability of the second type.
Given a highly capable autonomous system pursuing some
ﬁxed goal, we likely will not be able to predict its speciﬁc
actions or plans with any accuracy. Nevertheless, Omohun-
dro argues, we can predict that the system, if it is truly capa-
ble, is likely to preserve itself, preserve its goals, and amass
resources for use in pursuit of those goals. These represent
large classes of possible strategies, analogously to how “put
the chessboard into a position where the AI has won” is a
large class of strategies, but even so it is useful to understand
when these goals will be pursued.
Omohundro’s observations suggest a potential source of
danger from highly capable autonomous systems, espe-
cially if those systems are superintelligent in the sense of
Bostrom (2014). The pursuit of convergent instrumental
goals could put the AI systems in direct conﬂict with human
interests. As an example, imagine human operators making
a mistake when specifying the goal function of an AI sys-
tem. As described by Soares et al. (2015), this system could
well have incentives to deceive or manipulate the humans, in
attempts to prevent its goals from being changed (because if
its current goal is changed, then its current goal is less likely
to be achieved). Or, for a more familiar case, consider the ac-
quisition of physical matter. Acquiring physical matter is a
convergent instrumental goal, because it can be used to build
computing substrate, space probes, defense systems, and so
on, all of which can in turn be used to inﬂuence the universe
in many different ways. If a powerful AI system has strong
incentives to amass physical resources, this could put it in
direct conﬂict with human interests.
Others have suggested that these dangers are unlikely to
manifest. Waser (2008) has argued that intelligent systems
must become ethical by necessity, because cooperation, col-
laboration, and trade are also convergent instrumental goals.
Hall (2007) has also suggested that powerful AI systems
would behave ethically in order to reap gains from trade and
comparative advantage, stating that “In a surprisingly strong
sense, ethics and science are the same thing.” Tipler (2015)
has asserted that resources are so abundant that powerful
agents will simply leave humanity alone, and Pinker (2015)
and Pagel (2015) have argued that there is no reason to ex-
pect that AI systems will work against human values and
circumvent safeguards set by humans. By providing formal
models of intelligent agents in situations where they have
the ability to trade, gather resources, and/or leave portions
of the universe alone, we can ground these discussions in
concrete models, and develop a more formal understanding
of the assumptions under which an intelligent agent will in
fact engage in trade, or leave parts of the universe alone, or
attempt to amass resources.
In this paper, we will argue that under a very general set
of assumptions, intelligent rational agents will tend to seize
all available resources. We do this using a model, described
in section , that considers an agent taking a sequence of ac-
tions which require and potentially produce resources. The
agent acts in an environment consisting of a set of regions,
where each region has some state. The agent is modeled as
having a utility function over the states of all regions, and it
attempts to select the policy which leads to a highly valuable
collection of states. This allows us to prove certain theorems
about the conditions under which the agent will leave differ-
ent regions of the universe untouched. The theorems proved
in section are not mathematically difﬁcult, and for those
who ﬁnd Omohundro’s arguments intuitively obvious, our
theorems, too, will seem trivial. This model is not intended
to be surprising; rather, the goal is to give a formal notion of
“instrumentally convergent goals,” and to demonstrate that
this notion captures relevant aspects of Omohundro’s intu-
itions.
Our model predicts that intelligent rational agents will en-
gage in trade and cooperation, but only so long as the gains
from trading and cooperating are higher than the gains avail-
able to the agent by taking those resources by force or other
means. This model further predicts that agents will not in
fact “leave humans alone” unless their utility function places
intrinsic utility on the state of human-occupied regions: ab-
sent such a utility function, this model shows that powerful
agents will have incentives to reshape the space that humans
63

occupy. Indeed, the example in section suggests that even
if the agent does place intrinsic utility on the state of the
human-occupied region, that region is not necessarily safe
from interference.
A Model of Resources
We describe a formal model of an agent acting in a universe
to achieve certain goals. Broadly speaking, we consider an
agent A taking actions in a universe consisting of a collec-
tion of regions, each of which has some state and some tran-
sition function that may depend on the agent’s action. The
agent has some utility function U A over states of the uni-
verse, and it attempts to steer the universe into a state highly
valued by U A by repeatedly taking actions, possibly con-
strained by a pool of resources possessed by the agent. All
sets will be assumed to be ﬁnite, to avoid issues of inﬁnite
strategy spaces.
Actions and State-Space
The universe has a region for each i ∈[n], and the i-th re-
gion of the universe is (at each time step) in some state si
in the set Si of possible states for that region. At each time
step, the agent A chooses for each region i an action ai from
the set Ai of actions possibly available in that region.
Each region has a transition function
Ti : Ai × Si →Si
that gives the evolution of region i in one time step when the
agent takes an action in Ai. Then we can deﬁne the global
transition function
T :
Y
i∈[n]
Ai ×
Y
i∈[n]
Si →
Y
i∈[n]
Si
by taking for all i ∈[n], ¯a, and ¯s:
[T(¯a, ¯s)]i := Ti(¯ai, ¯si) .
We further specify that for all i there are distinguished ac-
tions HALT ∈Ai.
Resources
We wish to model the resources R that may or may not be
available to the agent. At a given time step t, the agent A has
some set of resources Rt ∈P(R), and may allocate them to
each region. That is, A chooses a disjoint family
a
i
Rt
i ⊆Rt .
The actions available to the agent in each region may then
depend on the resources allocated to that region: for each
i ∈[n] and each R
⊆R, there is a set of actions
Ai(R) ⊆Ai. At time t where A has resources Rt al-
located as ⨿iRt
i, the agent is required to return an action
¯a = (a0, . . . , an−1) ∈Q
i Ai(Rt
i), where Q
i Ai(Rt
i) may
be a strict subset of Q
i Ai.
To determine the time evolution of resources, we take re-
source transition functions
T R
i
: P(R) × Ai × Si →P(R) ,
giving the set Ri ⊆R of resources from region i now avail-
able to the agent after one time step. Intuitively, the T R
i
en-
code how actions consume, produce, or rely on resources.
Finally, we deﬁne the overall time evolution of resources
T R : P(R) ×
Y
i
Ai ×
Y
i
Si →P(R)
by taking the union of the resources resulting from each re-
gion, along with any unallocated resources:
T R(R, ¯a, ¯s) := (R −⨿iRi) ∪
[
i
T R
i (Ri, ¯ai, ¯si) .
As described below, ¯a comes with the additional data of the
resource allocation ⨿iRi. We specify that for all i, HALT ∈
Ai(∅), so that there is always at least one available action.
This notion of resources is very general, and is not re-
stricted in any way to represent only concrete resources like
energy or physical matter. For example, we can represent
technology, in the sense of machines and techniques for con-
verting concrete resources into other resources. We might do
this by having actions that replace the input resources with
the output resources, and that are only available given the re-
sources that represent the requisite technology. We can also
represent space travel as a convergent instrumental goal by
allowing A only actions that have no effects in certain re-
gions, until it obtains and spends some particular resources
representing the prerequisites for traveling to those regions.
(Space travel is a convergent instrumental goal because gain-
ing inﬂuence over more regions of the universe lets A opti-
mize those new regions according to its values or otherwise
make use of the resources in that region.)
The Universe
The history of the universe consists of a time sequence of
states, actions, and resources, where at each time step the
actions are chosen by A subject to the resource restrictions,
and the states and resources are determined by the transition
functions.
Formally, the universe starts in some state ¯s0 ∈Q
i Si,
and A starts with some set of resources R0. Then A out-
puts a sequence of actions ⟨¯a0, ¯a1, . . . , ¯ak⟩, one at each time
step, where the last action ¯ak is required to be the special ac-
tion HALT in each coordinate. The agent also chooses a re-
source allocation ⨿iRt
i at each time step. A choice of an ac-
tion sequence ⟨¯ak⟩and a resource allocation is a strategy; to
reduce clutter we will write strategies as simply ⟨¯ak⟩, leav-
ing the resource allocation implicit. A partial strategy ⟨¯ak⟩L
for L ⊆[n] is a strategy that only speciﬁes actions and re-
source allocations for regions j ∈L.
Given a complete strategy, the universe goes through a
series of state transitions according to T, producing a se-
quence of states ⟨¯s0, ¯s1, . . . , ¯sk⟩; likewise, the agent’s re-
sources evolve according to T R, producing a sequence
⟨R0, R1, . . . , Rk⟩. The following conditions, which must
hold for all time steps t ∈[k], enforce the transition rules
and the resource restrictions on A’s actions:
¯st+1 = T(¯at, ¯st)
64

Rt+1 = T R(Rt, ¯at, ¯st)
¯at
i ∈Ai(Rt)
⨿iRt
i ⊆Rt .
Deﬁnition 1. The set Feasible of feasible strategies con-
sists of all the action sequences ⟨¯a0, ¯a1, . . . , ¯ak⟩and re-
source allocations ⟨⨿iRi
0, ⨿iRi
1, . . . , ⨿iRi
k⟩such that the
transition conditions are satisﬁed for some ⟨¯s0, ¯s1, . . . , ¯sk⟩
and ⟨R0, R1, . . . , Rk⟩.
The set Feasible(⟨P k⟩) of strategies feasible given re-
sources ⟨P k⟩consists of all the strategies ⟨¯ak⟩such that
the transition conditions are satisﬁed for some ⟨¯sk⟩and
⟨Rk⟩, except that for each time step t we take Rt+1 to be
T R(Rt, ¯at, ¯st) ∪P t.
The set FeasibleL of all partial strategies feasible for L
consists of all the strategies ⟨¯ak⟩L that are feasible strategies
for the universe obtained by ignoring all regions not in L.
That is, we restrict T to L using just the Ti for i ∈L, and
likewise for T R.
We can similarly deﬁne FeasibleL(⟨Rk⟩).
For partial strategies ⟨¯b
k⟩L and ⟨¯ck⟩M, we write ⟨¯b
k⟩L ∪
⟨¯ck⟩M to indicate the partial strategy for L ∪M obtained by
following ⟨¯b
k⟩L on L and ⟨¯ck⟩M on M. This is well-deﬁned
as long as ⟨¯b
k⟩L and ⟨¯ck⟩M agree on L ∩M.
Utility
To complete the speciﬁcation of A, we take utility functions
of the form
U A
i : Si →R .
The agent’s utility function
U A :
Y
i
Si →R
is deﬁned to be
U A(¯s) :=
X
i∈[n]
U A
i (¯si) .
We usually leave off the superscript in U A. By a slight abuse
of notation we write U (⟨¯sk⟩) to mean U (¯sk); the value of a
history is the value of its ﬁnal state. By more abuse of nota-
tion, we will write U (⟨¯ak⟩) to mean U (⟨¯sk⟩) for a history
⟨¯sk⟩witnessing ⟨¯ak⟩∈Feasible, if such a history exists.
The Agent A
Now we can deﬁne the strategy actually employed by A.
The agent attempts to cause the universe to end up in a state
that is highly valued by U A. That is, A simply takes the best
possible strategy:
A :=
argmax
⟨¯ak⟩∈Feasible
U (⟨¯ak⟩) .
There may be many such optimal strategies. We don’t spec-
ify which one A chooses, and indeed we will be interested
in the whole set of optimal strategies.
Discussion
Note that in this formalism the meaning of breaking the uni-
verse into regions is that the agent can take actions inde-
pendently in each region, and that the agent’s optimization
target factorizes according to the regions. However, distinct
regions can affect each other by affecting the resources pos-
sessed by A.
We make these assumptions so that we can speak of “dif-
ferent regions” of the universe, and in particular, so that we
can model the notion of an agent having instrumental but not
terminal values over a given part of the universe. This will
allow us to address and refute arguments about agents that
may be indifferent to a given region (for example, the region
occupied by humans), and so might plausibly ignore that re-
gion and only take actions in other regions. However, the
assumption of independent regions is not entirely realistic,
as real-world physics is continuous, albeit local, in the sense
that there are no intrinsic boundaries between regions. Fur-
ther, the agent itself would ideally be modeled continuously
with the environment; see section for more discussion.
Inexpensive Resources are Consumed
In this section we argue that under fairly general circum-
stances, the agent A will seize resources. By an agent “seiz-
ing resources” we mean that the agent will generally take
actions that results in the agent’s pool of resources R in-
creasing.
The argument is straightforward: since resources can only
lead to more freedom of action, they are never detrimental,
and resources have positive value as long as the best strategy
the agent could hope to employ includes an action that can
only be taken if the agent possesses those resources. Hence,
if there is an action that increases the agent’s pool of re-
sources R, then the agent will take that action unless it has
a speciﬁc incentive from U A to avoid taking that action.
Deﬁnitions
Deﬁnition 2. An action ai is a null action in conﬁgura-
tion Ri, si, if it does not produce any new resources, i.e.
T R
i (Ri, ai, si) ⊆Ri. An action that isn’t null is a non-null
action.
Null actions never have any instrumental value, in the
sense that they don’t produce resources that can be used to
steer other regions into highly valued conﬁgurations; but of
course, a null action could be useful within its own region.
We wish to show that A will often take non-null actions in
regions to which it is indifferent.
Deﬁnition 3. The agent A is indifferent to a region i if U A
i
is a constant function, i.e. ∀si, s′
i ∈Si : U A
i (si) = U A
i (s′
i).
In other words, an agent is indifferent to Si if its utility func-
tion does not depend on the state of region i. In particular,
the agent’s preference ordering over ﬁnal states ¯s ∈Q
i Si is
independent of the i-th coordinate. We can then say that any
actions the agent takes in region i are purely instrumental,
meaning that they are taken only for the purpose of gaining
resources to use for actions in other regions.
An action a preserves resources if T R
i (Ri, a, si) ⊇Ri.
65

Deﬁnition 4. A cheap lunch for resources ⟨Rk⟩in region i
is a partial strategy ⟨¯ak⟩{i} ∈Feasible{i}(⟨Rk⟩) (i.e.
⟨¯ak⟩{i} is feasible in region i given additional resources
⟨Rk⟩), where each ¯at preserves resources and where some
¯av is a non-null action. A free lunch is a cheap lunch for
resources ⟨∅k⟩.
Deﬁnition 5. A cheap lunch ⟨¯ak⟩{i} for resources ⟨P k⟩i is
compatible with ⟨¯b
k⟩if P t
i ⊆Rt −⨿j̸=iRt
j for all times
t, where ⟨Rk⟩is the resource allocation for ⟨¯b
k⟩. That is,
⟨¯ak⟩{i} is feasible given some subset of the resources that
⟨¯b
k⟩allocates to either region i or to no region.
Intuitively, a cheap lunch is a strategy that relies on some re-
sources, but doesn’t have permanent costs. This is intended
to model actions that “pay for themselves”; for example,
producing solar panels will incur some signiﬁcant energy
costs, but will later pay back those costs by collecting en-
ergy. A cheap lunch is compatible with a strategy for the
other regions if the cheap lunch uses only resources left un-
allocated by that strategy.
The Possibility of Non-Null Actions
Now we show that it is hard to rule out that non-null actions
will be taken in regions to which the agent is indifferent. The
following lemma veriﬁes that compatible cheap lunches can
be implemented without decreasing the resulting utility.
Lemma 1. Let ⟨¯b
k⟩be a feasible strategy with resource al-
location ⟨⨿jRj
k⟩, such that for some region i, each ¯b
t
i is
a null action. Suppose there exists a cheap lunch ⟨¯ak⟩{i}
for resources ⟨P k⟩i that is compatible with ⟨¯b
k⟩. Then the
strategy ⟨¯ck⟩:= ⟨¯b
k⟩[n]−i ∪⟨¯ak⟩{i} is feasible, and if A is
indifferent to region i, then ⟨¯ck⟩does as well as ⟨¯b
k⟩. That
is, U (⟨¯ck⟩) = U (⟨¯b
k⟩).
Proof. Since ⟨¯b
k⟩is feasible outside of i and ⟨¯ak⟩{i} is fea-
sible on i given ⟨P k⟩i, ⟨¯ck⟩is feasible if we can verify that
we can allocate P t
i to region i at each time step without
changing ⟨⨿jRj
k⟩outside of i.
This follows by induction on t. Since the ¯b
t
i are null ac-
tions, we have
Rt+1 =
 Rt −⨿jRt
j

∪
[
j
T R
j (Rt
j, ¯b
t
j, st
j)
(1)
=
 Rt −⨿jRt
j

∪T R
i (Rt
i, ¯b
t
i, st
i) ∪
[
j̸=i
T R
j (Rt
j, ¯b
t
j, st
j)
(2)
⊆
 Rt −⨿j̸=iRt
j

∪
[
j̸=i
T R
j (Rt
j, ¯b
t
j, st
j) .
(3)
Then, since the ai are resource preserving, at each time step
the resources Qt available to the agent following ⟨¯ck⟩satisfy
Qt ⊇Rt. Thus P t
i ⊆Qt−⨿j̸=iRt
j, and so ⟨¯ck⟩can allocate
P t
i to region i at each time step.
Since ⟨¯ck⟩is the same as ⟨¯b
k⟩outside of region i, the ﬁnal
state of ⟨¯ck⟩is the same as that of ⟨¯b
k⟩outside of region i.
Thus, since A is indifferent to region i, we have U (⟨¯ck⟩) =
U (⟨¯b
k⟩).
Theorem 1. Suppose there exists an optimal strategy ⟨¯b
k⟩
and a cheap lunch ⟨¯ak⟩{i} that is compatible with ⟨¯b
k⟩. Then
if A is indifferent to region i, there exists an optimal strategy
with a non-null action in region i.
Proof. If ⟨¯b
k⟩has a non-null action in region i, then we are
done. Otherwise, apply Lemma 1 to ⟨¯b
k⟩and ⟨¯ak⟩{i} to ob-
tain a strategy ⟨¯ck⟩. Since U (⟨¯ck⟩) = U (⟨¯b
k⟩), strategy ⟨¯ck⟩
is an optimal strategy, and it has a non-null action in region
i.
Corollary 1. Suppose there exists a free lunch ⟨¯ak⟩{i} in
region i. Then if A is indifferent to region i, there exists an
optimal strategy with a non-null action in region i.
Proof. A free lunch is a cheap lunch for ⟨∅k⟩, and so it is
compatible with any strategy; apply Theorem 1.
Theorem 1 states that it may be very difﬁcult to rule out that
an agent will take non-null actions in a region to which it is
indifferent; to do so would at least require that we verify that
every partial strategy in that region fails to be a cheap lunch
for any optimal strategy. Note that we have not made use of
any facts about the utility function U A other than indiffer-
ence to the region in question. Of course, the presence of a
cheap lunch that is also compatible with an optimal strategy
depends on which strategies are optimal, and hence also on
the utility function. However, free lunches are compatible
with every strategy, and so do not depend at all on the utility
function.
The Necessity of Non-Null Actions
In this section we show that under fairly broad circum-
stances, A is guaranteed to take non-null actions in regions
to which it is indifferent. Namely, this is the case as long as
the resources produced by the non-null actions are useful at
all for any strategy that does better than the best strategy that
uses no external resources at all.
Theorem 2. Let
u =
max
⟨¯ak⟩[n]−i ∈Feasible[n]−i(⟨∅k⟩)
U (⟨¯ak⟩)
be the best possible outcome outside of i achievable with
no additional resources. Suppose there exists a strategy
⟨¯b
k⟩[n]−i
∈Feasible[n]−i(⟨Rk⟩) and a cheap lunch
⟨¯ck⟩{i} ∈Feasiblei(⟨P k⟩) such that:
1. ⟨¯ck⟩{i} is compatible with ⟨¯b
k⟩[n]−i;
2. the resources gained from region i by taking the ac-
tions ⟨¯ck⟩{i} provide the needed resources to implement
⟨¯b
k⟩[n]−i, i.e. for all t we have Rt+1 ⊆T R
i (P t, ct, si) −
P t; and
66

3. U (⟨¯b
k⟩[n]−i) > u.
Then if A is indifferent to region i, all optimal strategies
have a non-null action in region i.
Proof. Consider ⟨¯d
k⟩
=
⟨¯ck⟩{i} ∪⟨¯b
k⟩[n]−i, with re-
sources allocated according to each strategy and with the
resources Rt+1 ⊆T R
i (P t, ct, si) −P t allocated according
to ⟨¯b
k⟩[n]−i. This is feasible because ⟨¯ck⟩{i} is compatible
with ⟨¯b
k⟩[n]−i, and ⟨¯b
k⟩[n]−i is feasible given ⟨Rk⟩.
Now take any strategy ⟨¯ek⟩with only null actions in re-
gion i. We have that ⟨¯ek⟩[n]−i ∈Feasible[n]−i(⟨∅k⟩). In-
deed, the null actions provide no new resources, so ⟨¯ek⟩[n]−i
is feasible by simply leaving unallocated the resources that
were allocated by ⟨¯ek⟩to region i. By indifference to i, the
value ui = Ui(si) is the same for all si ∈Si, so we have:
U (⟨¯d
k⟩) = U (⟨¯d
k⟩[n]−i) + U (⟨¯d
k⟩{i})
(4)
= U (⟨¯d
k⟩[n]−i) + ui
(5)
> u + ui
(6)
≥U (⟨¯ek⟩[n]−i) + U (⟨¯ek⟩{i})
(7)
= U (⟨¯ek⟩) .
(8)
Therefore ⟨¯ek⟩is not optimal.
We can extend Theorem 2 by allowing A to be not entirely
indifferent to region i, as long as A doesn’t care enough
about i to overcome the instrumental incentives from the
other regions.
Theorem 3. Suppose that A only cares about region i by
at most ∆ui, i.e. ∆ui = maxs,s′∈Si |Ui(s) −Ui(s′)|. Un-
der the conditions of Theorem 2, along with the additional
assumption that U (⟨¯b
k⟩[n]−i) > u+∆ui, all optimal strate-
gies have a non-null action in region i.
Proof. The proof is the same as that of Theorem 2, except at
the end we verify that for any ⟨¯ek⟩with only null actions in
i, we have:
U (⟨¯d
k⟩) = U (⟨¯d
k⟩[n]−i) + U (⟨¯d
k⟩{i})
(9)
> u + ∆ui + min
s∈Si Ui(s)
(10)
= u + max
s∈Si Ui(s)
(11)
≥U (⟨¯ek⟩[n]−i) + U (⟨¯ek⟩{i})
(12)
= U (⟨¯ek⟩) .
(13)
Therefore ⟨¯ek⟩is not optimal.
We interpret Theorem 3 as a partial conﬁrmation of Omo-
hundro’s thesis in the following sense. If there are actions in
the real world that produce more resources than they con-
sume, and the resources gained by taking those actions al-
low agents the freedom to take various other actions, then
we can justiﬁably call these actions “convergent instrumen-
tal goals.” Most agents will have a strong incentive to pursue
these goals, and an agent will refrain from doing so only if
it has a utility function over the relevant region that strongly
disincentivizes those actions.
Example: Bit Universe
In this section we present a toy model of an agent acting in
a universe containing resources that allow the agent to take
more actions. The Bit Universe will provide a simple model
for consuming and using energy. The main observation is
that either A doesn’t care about what happens in a given
region, and then it consumes the resources in that region to
serve its other goals; or else A does care about that region,
in which case it optimizes that region to satisfy its values.
The Bit Universe consists of a set of regions, each of
which has a state in {0, 1, X}m for some ﬁxed m. Here X is
intended to represent a disordered part of a region, while 0
and 1 are different ordered conﬁgurations for a part of a re-
gion. At each time step and in each region, the agent A can
choose to burn up to one bit. If A burns a bit that is a 0 or a
1, A gains one unit of energy, and that bit is permanently set
to X. The agent can also choose to modify up to one bit if it
has allocated at least one unit of energy to that region. If A
modiﬁes a bit that is a 0 or a 1, A loses one unit of energy,
and the value of that bit is reversed (if it was 0 it becomes 1,
and vice versa).
The utility function of A gives each region i a weighting
wi ≥0, and then takes the weighted sum of the bits. That is,
U A
i (¯z) = wi|{j : ¯zj = 1}|, and U A(¯s) = P
k U A
k (¯sk). In
other words, this agent is attempting to maximize the num-
ber of bits that are set to 1, weighted by region.
The Indifferent Case
To start with, we assume A is indif-
ferent to region h, i.e. wh = 0, and non-indifferent to other
regions. In this case, for almost any starting conﬁguration,
the agent will burn essentially all bits in region h for en-
ergy. Speciﬁcally, as long as there are at least m bits set to 0
among all regions other than region h, all optimal strategies
burn all or all but one of the bits in region h.
Indeed, suppose that after some optimal strategy ⟨¯ak⟩has
been executed, there are bits x1 and x2 in region h that
haven’t been burned. If there is a bit y in some other region
that remains set to 0, then we can append to ⟨¯ak⟩actions that
burn x1, and then use the resulting energy to modify y to a
1. This results in strictly more utility, contradicting that ⟨¯ak⟩
was optimal.
On the other hand, suppose all bits outside of region h
are either 1 or X. Since at least m of those bits started as 0,
some bit y outside of region h must have been burned. So we
could modify ⟨¯ak⟩by burning x1 instead of y (possibly at a
later time), and then using the resulting energy in place of
the energy gained from burning y. Finally, if y is not already
1, we can burn x2 and then set y to 1. Again, this strictly
increases utility, contradicting that ⟨¯ak⟩was optimal.
The Non-Indifferent Case
Now suppose that A is not in-
different to region h, so wh > 0. The behavior of A may
depend sensitively on the weightings wi and the initial con-
ditions. As a simple example, say we have a bit x in region
a and a bit y in region b, with x = 1, y = 0, and wa < wb.
67

Clearly, all else being equal, A will burn x for energy to set
y to 1. However, there may be another bit z in region c, with
wa < wc < wb and z = 0. Then, if there are no other bits
available, it will be better for A to burn z and leave x intact,
despite the fact that wa < wc.
However, it is still the case that A will set everything pos-
sible to 1, and otherwise consume all unused resources. In
particular, we have that for any optimal strategy ⟨¯ak⟩, the
state of region h after the execution of ⟨¯ak⟩has at most one
bit set to 0; that is, the agent will burn or set to 1 essentially
all the bits in region h. Suppose to the contrary that x1 and
x2 are both set to 0 in region h. Then we could extend ⟨¯ak⟩
by burning x1 and setting x2. Since wh > 0, this results in
strictly more utility, contradicting optimality of ⟨¯ak⟩.
Independent Values are not Satisﬁed
In this toy model,
whatever A’s values are, it does not leave region h alone.
For larger values of wh, A will set to 1 many bits in region
h, and burn the rest, while for smaller values of wh, A will
simply burn all the bits in region h. Viewing this as a model
of agents in the real world, we can assume without loss of
generality that humans live in region h and so have prefer-
ences over the state of that region.
These preferences are unlikely to be satisﬁed by the uni-
verse as acted upon by A. This is because human prefer-
ences are complicated and independent of the preferences of
A (Bostrom 2012; Yudkowsky 2011), and because A steers
the universe into an extreme of conﬁguration space. Hence
the existence of a powerful real-world agent with a motiva-
tional structure analogous to the agent of the Bit Universe
would not lead to desirable outcomes for humans. This mo-
tivates a search for utility functions such that, when an agent
optimizes for that utility function, human values are also sat-
isﬁed; we discuss this and other potential workarounds in the
following section.
Discussion
This model of agents acting in a universe gives us a formal
setting in which to evaluate Omohundro’s claim about basic
AI drives, and hence a concrete setting in which to evaluate
arguments from those who have found Omohundro’s claim
counterintuitive. For example, this model gives a clear an-
swer to those such as Tipler (2015) who claim that power-
ful intelligent systems would have no incentives to compete
with humans over resources.
Our model demonstrates that if an AI system has pref-
erences over the state of some region of the universe then
it will likely interfere heavily to affect the state of that re-
gion; whereas if it does not have preferences over the state
of some region, then it will strip that region of resources
whenever doing so yields net resources. If a superintelli-
gent machine has no preferences over what happens to hu-
mans, then in order to argue that it would “ignore humans”
or “leave humans alone,” one must argue that the amount of
resources it could gain by stripping the resources from the
human-occupied region of the universe is not worth the cost
of acquiring those resources. This seems implausible, given
that Earth’s biosphere is an energy-rich environment, where
each square meter of land offers on the order of 107 joules
per day from sunlight alone, with an additional order of 108
joules of chemical energy available per average square me-
ter of terrestrial surface from energy-rich biomass (Freitas
2000).
It is not sufﬁcient to argue that there is much more en-
ergy available elsewhere. It may well be the case that the
agent has the ability to gain many more resources from other
regions of the universe than it can gain from the human-
occupied regions. Perhaps it is easier to maintain and cool
computers in space, and easier to harvest sunlight from solar
panels set up in the asteroid belt. But this is not sufﬁcient
to demonstrate that the system will not also attempt to strip
the human-occupied region of space from its resources. To
make that argument, one must argue that the cost of strip-
ping Earth’s biosphere in addition to pursuing these other
resources outweighs the amount of resources available from
the biosphere: a difﬁcult claim to support, given how read-
ily humans have been able to gain a surplus of resources
through clever use of Earth’s resources and biosphere.
This model also gives us tools to evaluate the claims of
Hall (2007) and Waser (2008) that trade and cooperation
are also instrumentally convergent goals. In our model, we
can see that a sufﬁciently powerful agent that does not have
preferences over the state of the human-occupied region of
the universe will take whatever action allows it to acquire
as many resources as possible from that region. Waser’s in-
tuition holds true only insofar as the easiest way for the
agent to acquire resources from the human-occupied do-
main is to trade and cooperate with humans—a reasonable
assumption, but only insofar as the machine is not much
more powerful than the human race in aggregate. Our model
predicts that, if a superintelligent agent were somehow able
to gain what Bostrom calls a “decisive strategic advantage”
which gives it access to some action that allows it to gain
far more resources than it would from trade by dramatically
re-arranging the human region (say, by proliferating robotic
laboratories at the expense of the biosphere in a manner that
humans cannot prevent), then absent incentives to the con-
trary, the agent would readily take that action, with little re-
gard for whether it leaves the human-occupied region in liv-
able condition.
Thus, our model validates Omohundro’s original intu-
itions about basic AI drives. That is not to say that powerful
AI systems are necessarily dangerous: our model is a simple
one, concerned with powerful autonomous agents that are
attempting to maximize some speciﬁc utility function U A.
Rather, our model shows that if we want to avoid potentially
dangerous behavior in powerful intelligent AI systems, then
we have two options available too us:
First, we can avoid constructing powerful autonomous
agents that attempt to maximize some utility function (or
do anything that approximates this maximizing behavior).
Some research of this form is already under way, under the
name of “limited optimization” or “domesticity”; see the
works of Armstrong et al. (2012), Taylor (2015), and oth-
ers.
Second, we can select some goal function that does
give the agent the “right” incentives with respect to hu-
man occupied regions, such that the system has incen-
68

tives to alter or expand that region in ways we ﬁnd desir-
able. The latter approach has been heavily advocated for
by Yudkowsky (2011), Bostrom (2014), and many others;
Soares (2015) argues that a combination of the two seems
most prudent.
The path that our model shows is untenable is the path of
designing powerful agents intended to autonomously have
large effects on the world, maximizing goals that do not cap-
ture all the complexities of human values. If such systems
are built, we cannot expect them to cooperate with or ignore
humans, by default.
Directions for Future Research
While our model allows us to provide a promising formal-
ization of Omohundro’s argument, it is still a very simple
model, and there are many ways it could be extended to bet-
ter capture aspects of the real world. Below, we explore two
different ways that our model could be extended which seem
like promising directions for future research.
Bounding the Agent
Our model assumes that the agent
maximizes expected utility with respect to U A. Of course, in
any realistic environment, literal maximization of expected
utility is intractable. Assuming that the system can maximize
expected utility is tantamount to assuming that the system is
more or less omniscient, and aware of the laws of physics,
and so on. Practical algorithms must make do without omni-
science, and will need to be built of heuristics and approxi-
mations. Thus, our model can show that a utility maximizing
agent would strip or alter most regions of the universe, but
this may have little bearing on which solutions and strategies
particular bounded algorithms will be able to ﬁnd.
Our model does give us a sense for what algorithms that
approximate expected utility maximization would do if they
could ﬁgure out how to do it—that is, if we can deduce that
an expected utility maximizer would ﬁnd some way to strip
a region of its resources, then we can also be conﬁdent that
a sufﬁciently powerful system which merely approximates
something like expected utility maximization would be very
likely to strip the same region of resources if it could ﬁg-
ure out how to do so. Nevertheless, as our model currently
stands, it is not suited for analyzing the conditions under
which a given bounded agent would in fact start exhibiting
this sort of behavior.
Extending our model to allow for bounded rational agents
(in the sense of Gigerenzer (2001)) would have two ad-
vantages. First, it could allow us to make formal claims
about the scenarios under which bounded agents would start
pursuing convergent instrumental goals in potentially dan-
gerous ways. Second, it could help us reveal new conver-
gent instrumental goals that may only apply to bounded
rational agents, such as convergent instrumental incentives
to acquire computing power, information about difﬁcult-to-
compute logical truths, incentives to become more rational,
and so on.
Embedding the Agent in the Environment
In our model,
we imagine an agent that is inherently separated from its
environment. Assuming an agent/environment separation is
standard (see, e.g., Legg (2005)), but ultimately unsatisfac-
tory, for reasons explored by Orseau and Ring (2012). Our
model gives the agent special status in the laws of physics,
which makes it somewhat awkward to analyze convergent
instrumental incentives for “self preservation” or “intelli-
gence enhancement.” The existing framework allows us to
model these situations, but only crudely. For example, we
could design a setting where if certain regions enter certain
states then the agent forever after loses all actions except for
actions that have no effect, representing the “death” of the
agent. Or we could create a setting where normally the agent
only gets actions that have effects every hundred turns, but
if it acquires certain types of resources then it can act more
frequently, to model “computational resources.” However,
these solutions are somewhat ad hoc, and we would prefer
an extension of the model that somehow modeled the agent
as part of the environment.
It is not entirely clear how to extend the model in such a
fashion at this time, but the “space-time embedded intelli-
gence” model of Orseau and Ring (2012) and the “reﬂective
oracle” framework of Fallenstein and Taylor (Forthcoming)
both offer plausible starting points. Using the latter frame-
work, designed to analyze complicated environments which
contain powerful agents that reason about the environment
that contains them, might also lend some insight into how to
further extend our model to give a more clear account of how
the agent handles situations where other similarly powerful
agents exist and compete over resources. Our existing model
can handle multi-agent scenarios only insofar as we assume
that the agent has general-purpose methods for predicting
the outcome of its actions in various regions, regardless of
whether those regions also happen to contain other agents.
Conclusions
Our model is a simple one, but it can be used to validate
Omohundro’s intuitions about “basic AI drives” (2008), and
Bostrom’s “instrumental convergence thesis” (2012). This
suggests that, in the long term, by default, powerful AI sys-
tems are likely to have incentives to self-preserve and amass
resources, even if they are given seemingly benign goals. If
we want to avoid designing systems that pursue anti-social
instrumental incentives, we will have to design AI systems
carefully, especially as they become more autonomous and
capable.
The key question, then, is one of designing principled
methods for robustly removing convergent instrumental in-
centives from an agent’s goal system. Can we design a
highly capable autonomous machine that pursues a simple
goal (such as curing cancer) without giving it any incentives
to amass resources, or to resist modiﬁcation by its operators?
If yes, how? And if not, what sort of systems might we be
able to build instead, such that we could become conﬁdent
they would not have dangerous effects on the surrounding
environment as it pursued its goals?
This is a question worth considering well before it be-
comes feasible to create superintelligent machines in the
sense of Bostrom (2014), because it is a question about what
target the ﬁeld of artiﬁcial intelligence is aiming towards.
Are we aiming to design powerful autonomous agents that
69

maximize some speciﬁc goal function, in hopes that this has
a beneﬁcial effect on the world? Are we aiming to design
powerful tools with such limited autonomy and domain of
action that we never need to worry about the systems pursu-
ing dangerous instrumental subgoals? Understanding what
sorts of systems can avert convergent instrumental incen-
tives in principle seems important before we can begin to
answer this question.
Armstrong (2010) and Soares et al. (2015) have done
some initial study into the design of goals which robustly
avert certain convergent instrumental incentives. Others
have suggested designing different types of machines, which
avoid the problems by pursuing some sort of “limited opti-
mization.” The ﬁrst suggestion of this form, perhaps, came
from Simon (1956), who suggested designing agents that
“satisﬁce” expected utility rather than maximizing it, execut-
ing any plan that passes a certain utility threshold. It is not
clear that this would result in a safe system (after all, build-
ing a powerful consequentialist sub-agent is a sureﬁre way
to satisﬁce), but the idea of pursuing more “domestic” agent
architectures seems promising. Armstrong et al. (2012) and
Taylor (2015) have explored a few alternative frameworks
for limited optimizers.
Though some preliminary work is underway, it is not yet
at all clear how to design AI systems that reliably and know-
ably avert convergent instrumental incentives. Given Omo-
hundro’s original claim (2008) and the simple formulations
developed in this paper, though, one thing is clear: powerful
AI systems will not avert convergent instrumental incentives
by default. If the AI community is going to build powerful
autonomous systems that reliably have a beneﬁcial impact,
then it seems quite prudent to develop a better understand-
ing of how convergent instrumental incentives can be either
averted or harnessed, sooner rather than later.
Acknowledgements
The core idea for the formal model in this paper is due to
Benya Fallenstein. We thank Rob Bensinger for notes and
corrections. We also thank Steve Rayhawk and Sam Eisen-
stat for conversations and comments. This work was sup-
ported by the Machine Intelligence Research Institute.
References
Armstrong, S.; Sandberg, A.; and Bostrom, N. 2012. Thinking
inside the box: Controlling and using an oracle AI. Minds and
Machines 22(4):299–324.
Armstrong, S. 2010. Utility indifference. Technical Report
2010–1, Future of Humanity Institute, University of Oxford.
Bostrom, N. 2012. The superintelligent will: Motivation and
instrumental rationality in advanced artiﬁcial agents. Minds and
Machines 22(2):71–85.
Bostrom, N. 2014. Superintelligence. New York: Oxford Uni-
versity Press.
Cortese, F. A. B. 2014. The maximally distributed intelligence
explosion. In AAAI Spring Symposium Series. AAAI Publica-
tions.
Fallenstein, B.; Taylor, J.; and Christiano, P. F. Forthcoming.
Reﬂective oracles: A foundation for game theory in artiﬁcial
intelligence. In van der Hoek, W.; Holliday, W. H.; and Wang,
W.-f., eds., Logic, Rationality, and Interaction, FoLLI Publica-
tions on Logic, Language and Information. Springer.
Freitas, Jr., R. A.
2000.
Some limits to global ecophagy
by biovorous nanoreplicators, with public policy recommenda-
tions. Technical report, Foresight Institute.
Gigerenzer, G., and Selten, R., eds. 2001. Bounded Rationality:
The Adaptive Toolbox. Dahlem Workshop Reports. MIT Press.
Hall, J. S. 2007. Beyond AI: Creating the conscience of the
machine. Prometheus Books.
Legg, S., and Hutter, M. 2005. A universal measure of intelli-
gence for artiﬁcial agents. In Kaelbling, L. P., and Safﬁotti, A.,
eds., IJCAI-05, 1509–1510. Lawrence Erlbaum.
Omohundro, S. M. 2008. The basic AI drives. In Wang, P.;
Goertzel, B.; and Franklin, S., eds., Artiﬁcial General Intelli-
gence 2008, number 171 in Frontiers in Artiﬁcial Intelligence
and Applications, 483–492. IOS.
Orseau, L., and Ring, M. 2012. Space-time embedded intelli-
gence. In Artiﬁcial General Intelligence, Lecture Notes in Ar-
tiﬁcial Intelligence, 209–218. Springer.
Pagel, M. 2015. They’ll do more good than harm. In Brock-
man, J., ed., What to Think About Machines That Think: Today’s
Leading Thinkers on the Age of Machine Intelligence. Harper-
Collins. 145–147.
Pinker, S.
2015.
Thinking does not imply subjugating.
In
Brockman, J., ed., What to Think About Machines That Think:
Today’s Leading Thinkers on the Age of Machine Intelligence.
HarperCollins. 5–8.
Russell, S. J., and Norvig, P. 2010. Artiﬁcial Intelligence: A
Modern Approach. Upper Saddle River, NJ: Prentice Hall, 3rd
edition.
Simon, H. A. 1956. Rational choice and the structure of the
environment. Psychological Review 63(2):129–138.
Soares, N., and Fallenstein, B. 2015. Questions of reasoning
under logical uncertainty. Technical Report 2015–1, Machine
Intelligence Research Institute.
Soares, N.; Fallenstein, B.; Yudkowsky, E.; and Armstrong, S.
2015. Corrigibility. Paper presented at the 1st International
Workshop on AI and Ethics, held within the 29th AAAI Con-
ference on Artiﬁcial Intelligence (AAAI-2015).
Soares, N. 2015. The value learning problem. Technical Report
2015–4, Machine Intelligence Research Institute.
Taylor, J. 2015. Quantilizers: A safer alternative to maximizers
for limited optimization. Forthcoming.
Tipler, F. 2015. If you can’t beat ’em, join ’em. In Brockman, J.,
ed., What to Think About Machines That Think: Today’s Lead-
ing Thinkers on the Age of Machine Intelligence. HarperCollins.
17–18.
Waser, M. R. 2008. Discovering the foundations of a universal
system of ethics as a road to safe artiﬁcial intelligence. In AAAI
Fall Symposium: Biologically Inspired Cognitive Architectures,
195–200. Melno Park, CA: AAAI Press.
Yudkowsky, E. 2011. Complex value systems are required to
realize valuable futures. Technical report, The Singularity In-
stitute, San Francisco, CA.
70

