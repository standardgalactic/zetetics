Parallel and Distributed MCMC Inference using Julia
by
Angel Yu
S.B. Computer Science and Engineering, Mathematics
Massachusetts Institute of Technology, 2016
Submitted to the Department of Electrical Engineering and Computer Science
in partial fulﬁllment of the requirements for the degree of
Master of Engineering
in Electrical Engineering and Computer Science
at the Massachusetts Institute of Technology
September 2016
c⃝2016 Massachusetts Institute of Technology
All Rights Reserved.
Signature of Author:
Department of Electrical Engineering and Computer Science
August 16, 2016
Certiﬁed by:
John W. Fisher III, Senior Research Scientist
Thesis Supervisor
Certiﬁed by:
Oren Freifeld, Postdoctoral Associate
Thesis Co-Supervisor
Accepted by:
Christopher J. Terman
Chairman, Masters of Engineering Thesis Committee

2

Parallel and Distributed MCMC Inference using Julia
by
Angel Yu
Submitted to the Department of Electrical Engineering and Computer Science
on August 16, 2016 in Partial Fulﬁllment of the Requirements for the Degree of
Master of Engineering in Electrical Engineering and Computer Science
Abstract
Machine learning algorithms are often computationally intensive and operate on
large datasets. Being able to eﬃciently learn models on large datasets holds the future
of machine learning. As the speed of serial computation stalls, it is necessary to utilize
the power of parallel computing in order to better scale with the growing complexity of
algorithms and the growing size of datasets. In this thesis, we explore the use of Julia,
a fairly new high level programming language that lends itself to easy parallelization
over multiple CPU cores as well as multiple machines, on Markov chain Monte Carlo
(MCMC) inference algorithms.
First, we take existing algorithms and implement them in Julia.
We focus on
MCMC inference using Continuous Piecewise-Aﬃne Based (CPAB) transformations
and a parallel MCMC sampler for Dirichlet Process Mixture Models (DPMM). Instead
of parallelizing over multiple cores on a single machine, our Julia implementations ex-
tend existing implementations by parallelizing over multiple machines. We compare
our implementation with these existing implementations written in more traditional
programming languages. Next, we develop a model Projections Dirichlet Process Gaus-
sian Mixture Model (PDP-GMM) which relaxes the assumption that the draws from
a Dirichlet Process Gaussian Mixture Model (DP-GMM) are directly observed. We
extend our DPMM Julia implementation and present a few applications of this model.
Thesis Supervisor: John W. Fisher III
Title: Senior Research Scientist
3

4

Acknowledgements
First of all, I would like to thank my advisor John Fisher for his guidance throughout
my time in the group. He has always provided valuable insight and ideas during group
meetings as well as grouplets.
I would like to thank my co-advisor Oren Freifeld for his mentorship and help with
all my questions. Oren introduced me to the topic of Bayesian Inference and was always
available to help me work out any technical diﬃculties. This work would not have been
possible without him.
I would also like to thank my collaborator Sue Zheng for providing the derivations for
our PDP-GMM model in Chapter 4. Her contributions were key to the work presented
in Chapter 4.
Finally, I would like to thank my family and friends. In particular, my parents have
always been supportive of me throughout my MIT journey.
5

6

Contents
Abstract
3
Acknowledgments
4
List of Figures
9
List of Tables
11
1
Introduction
13
1.1
Thesis Outline and Contributions . . . . . . . . . . . . . . . . . . . . . .
13
2
CPAB Transformations
17
2.1
Bayesian Inference using CPAB transformations . . . . . . . . . . . . . .
19
2.1.1
Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.1.2
Metropolis’ Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
22
2.1.3
Sequential Importance Resampling . . . . . . . . . . . . . . . . .
22
2.2
Implementation and Results . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2.1
Parallel Computing in Julia . . . . . . . . . . . . . . . . . . . . .
23
2.2.2
CPAB transformations . . . . . . . . . . . . . . . . . . . . . . . .
25
2.2.3
Gradient-based Optimization . . . . . . . . . . . . . . . . . . . .
27
2.2.4
Metropolis’ Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
27
2.2.5
Sequential Importance Resampling . . . . . . . . . . . . . . . . .
29
2.3
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3
Parallel Sampling of DPMM using Subcluster Splits
33
3.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.1.1
DPMM Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.1.2
Parallel Sampler for DPMMs . . . . . . . . . . . . . . . . . . . .
35
3.1.3
Image Denoising using Expected Patch Log Likelihood . . . . . .
37
3.2
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.2.1
High-Level Design/Architecture . . . . . . . . . . . . . . . . . . .
39
3.2.2
Detailed Implementation Design
. . . . . . . . . . . . . . . . . .
40
7

8
CONTENTS
3.2.3
Optimizing Processes Communication . . . . . . . . . . . . . . .
43
3.2.4
Runtime Complexity . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.2.5
Memory Complexity . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.3.1
Synthetic Data: DP-GMM
. . . . . . . . . . . . . . . . . . . . .
47
3.3.2
Synthetic Data: DP-MNMM
. . . . . . . . . . . . . . . . . . . .
49
3.3.3
Image Denoising . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.4
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
4
Projection Dirichlet Process Gaussian Mixture Model
53
4.1
Model and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4.1.1
Conjugacy for Isotropic Covariances . . . . . . . . . . . . . . . .
55
4.1.2
Anisotropic Covariance
. . . . . . . . . . . . . . . . . . . . . . .
56
4.2
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.3.1
Tomography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.3.2
Mixture of polynomial regressions
. . . . . . . . . . . . . . . . .
58
4.4
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
5
Conclusion
63
5.1
Remarks on Julia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
A Derivations Pertaining to PDP-GMM
67
A.1 Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
A.2 Posterior Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
A.3 Data Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
Bibliography
71

List of Figures
2.1
(a) An example of a 1-dimensional velocity ﬁeld. Here the tessellation
contains 5 cells equally spaced between 0 and 10. (b) An example of a
2-dimensional tessellation we will use. Here the tessellation contains 64
cells. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2
An inference problem: Inferring the underlying parameters of a CPAB
transformations that maps the red points to the blue points. . . . . . . .
21
2.3
(a) Original Image. (b) Image after applying a CPAB transformation. .
26
2.4
Applying Gradient Descent to infer the underlying CPAB transforma-
tion. The points in blue are the target points and the points in red are
the points after applying the inferred transformation. . . . . . . . . . . .
28
2.5
(a) Applying Metropolis’ Algorithm to infer the underlying CPAB trans-
formation. The points in blue are the target points and the points in
red are the points after applying the inferred transformation. (b) Plot
showing the log likelihood and log prior over 50,000 iterations.
. . . . .
29
2.6
(a) Applying the Sequential Importance Resampling algorithm to infer
the underlying CPAB transformation. The points in blue are the target
points and the points in red are the points after applying the inferred
transformation. (b) Plot showing the log likelihood and log prior over
200 iterations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.7
A plot showing the parallelization speedup obtained when running the
Sequential Importance Resampling inference algorithm with 1,000 parti-
cles on 512 × 512 correspondences. . . . . . . . . . . . . . . . . . . . . .
31
3.1
The proposed architecture of our proposed multi-machine implementa-
tion. We show a parallelization over 4 machines here. . . . . . . . . . . .
40
3.2
(a) The 106 points in our synthetic data, in the 2-dimensional case, sam-
pled from an underlying GMM with 6 clusters are shown in blue. (b)
The inferred clusters from our implementation are shown as ellipses at 5
standard deviations in diﬀerent colors. . . . . . . . . . . . . . . . . . . .
48
3.3
Eigenvalues of 6 randomly-chosen clusters learned by our DP-GMM on
2 million 8 × 8 image patches. . . . . . . . . . . . . . . . . . . . . . . . .
50
9

10
LIST OF FIGURES
3.4
An example of denoising using our model trained on all 8 × 8 patches.
The corrupted image (middle) has a PSNR of 20.17 and the restored
image (right) has a PSNR of 29.14. . . . . . . . . . . . . . . . . . . . . .
52
4.1
(Left) We show the KDE estimates of the 1-dimensional marginals com-
puted from the observations.
(Right) We show both the unobserved
draws as well as the KDE estimates. . . . . . . . . . . . . . . . . . . . .
58
4.2
A simple example with 10,000 points and 3 clusters. We show the in-
ferred cluster means and (isotropic) covariances as circles at 4 standard
deviations together with the estimated 1-dimensional marginals and the
unobserved 2-dimensional draws. . . . . . . . . . . . . . . . . . . . . . .
59
4.3
A more complicated example with 10,000 points and 10 clusters. (Left)
We show the inferred cluster means and (isotropic) covariances as cir-
cles at 3 standard deviations together with the estimated 1-dimensional
marginals and the unobserved 2-dimensional draws. (Right) We show
the weights of each inferred cluster. . . . . . . . . . . . . . . . . . . . . .
60
4.4
An example of using our model to ﬁt a mixture of linear regressions.
(Left) We show the observations. (Right) We show the resulting clusters
after inference. The black lines are the ground truth lines. The colored
lines are cluster centers inferred using our model.
Importantly, note
that we did not assume that we know that the number of the regression
functions, K, is three. Rather, this quantity was estimated as part of
our inference procedure. . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.5
An example of using our model to ﬁt a mixture of quadratic regres-
sions.(Left) We show the observations. (Right) We show the resulting
clusters after inference. The black curves are the ground truth quadrat-
ics.
The colored curves are cluster centers inferred using our model.
Importantly, note that we did not assume that we know that the num-
ber of the regression functions, K, is three. Rather, this quantity was
estimated as part of our inference procedure.
. . . . . . . . . . . . . . .
62

List of Tables
2.1
Time to compute a CPAB transformation on a 512x512 image on an Intel
Core i7-6700K. The graphics card used in [8] was an Nvidia GTX 780. .
27
2.2
Running time of the Sequential Importance Resampling inference algo-
rithm with 1,000 particles for 200 iterations on 256 correspondences on
an Intel Core i7-6700K.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.3
Running time of each iteration of the Sequential Importance Resampling
inference algorithm with 1,000 particles on 512 × 512 correspondences
using distributed computing.
. . . . . . . . . . . . . . . . . . . . . . . .
31
3.1
Time(sec) to run 100 DP-GMM iterations of d = 2, N = 106, K = 6.
. .
48
3.2
Time(sec) to run 100 DP-GMM iterations of d = 30, N = 106, K = 6. . .
49
3.3
Time(sec) to run 100 DP-MNMM iterations of d = 100, N = 106, K = 6.
50
3.4
Time(sec) to run 100 DP-MNMM iterations of d = 100, N = 106, K = 60. 50
3.5
Average PSNR obtained from denoising 50 test images with added Gaus-
sian noise using diﬀerent priors. . . . . . . . . . . . . . . . . . . . . . . .
51
11

12
LIST OF TABLES

Chapter 1
Introduction
The clock speed of modern processors is beginning to hit its limit. This is because power
consumption and heat dissipation increases much faster than the clock speed of the chip.
Hence, we now see a trend towards processors with multiple cores instead of higher clock
speeds. In order to fully utilize the power of these processors, we would have to design
and implement algorithms that make eﬀective use of parallel computing. Designing
and implementing eﬃcient parallel algorithms are often harder and more complicated
than their serial counterparts. This is because additional care must be taken to design
meaningful subtasks so as to minimize communication and synchronization overhead.
Julia is a fairly new high level programming language that has parallel computing
built in. Its parallel computing model allows for easy parallelization across multiple
cores as well as multiple machines, something that is usually hard to achieve in most
other programming languages. It is also syntactically similar to MATLAB but with
performance comparable to C. This makes Julia an attractive language for developing
parallel implementations of machine-learning algorithms.
■1.1 Thesis Outline and Contributions
In this thesis, we explore the utility of using Julia for machine-learning algorithms. We
use Julia for parallel and distributed implementations of two diﬀerent Bayesian MCMC
inference algorithms and compare their performance with existing implementations. We
13

14
CHAPTER 1.
INTRODUCTION
discuss various challenges and our experience with parallel implementations in Julia.
Chapter 2: CPAB Transformations
In Chapter 2, we implement inference on latent Continuous Piecewise-Aﬃne Based
(CPAB) transformations proposed by Freifeld et al. [8]. There is currently a GPU im-
plementation of CPAB transformations and we compare that with our Julia implemen-
tation. We apply CPAB transformations to the correspondence-based image warping
problem by using Bayesian inference on the transformations. We experimented with
3 diﬀerent methods: Gradient Descent, Metropolis’ Algorithm and Sequential Impor-
tance Resampling. We discuss our implementation using Julia as well as the suitability
of these methods to the problem including how they lend themselves to parallelization.
Chapter 3: Parallel Sampling of DPMM using Subcluster splits
In Chapter 3, we implement a Dirichlet Process Mixture Model (DPMM) with subclus-
ter splits that was proposed by Chang and Fisher III [4]. That algorithm is based on
the split-merge framework by Jain and Neal [14]. It was designed as a parallel algo-
rithm and there is a (single-machine) multi-core C++ implementation available. We
extend this algorithm to accommodate a distributed memory model which allows our
implementation to be able to parallelize over multiple machines. We describe our imple-
mentation design and compare our distributed implementation with the existing C++
implementation. In addition, we applied our DP-GMM implementation to learn image
patch priors for image denoising. We show that with our distributed implementation,
we are able to handle much larger datasets.

Sec. 1.1.
Thesis Outline and Contributions
15
Chapter 4: Projection Dirichlet Process Gaussian Mixture Model
In Chapter 4, we propose a new model which relaxes the assumption that the draws
from the Dirichlet Process Gaussian Mixture Model (DP-GMM) are directly observed.
Our model instead assumes that the observations are merely aﬃne projections of the
DP-GMM draws. This allows us to tackle interesting problems in a new light. We
discuss our experience with modifying our DPMM code to handle this new model and
present a few applications of this model.
Chapter 5: Conclusion
Finally, we summarize our work and contributions in this thesis. We oﬀer some general
comments on our experience with Julia.
Appendix A: Derivations Pertaining to PDP-GMM
A few of the expressions presented in Chapter 4 require a considerable amount of algebra
to derive. We present these derivations in Appendix A for the interested reader.

16
CHAPTER 1.
INTRODUCTION

Chapter 2
CPAB Transformations
Diﬀeomorphisms are bijective diﬀerentiable transformations such that the inverse is
diﬀerentiable as well. They have many applications in a wide variety of domains. In
this thesis, we focus on the computer vision applications of image warping and image
registration. However, current representations of this class of transformations are com-
plicated and hard to compute on large datasets. CPAB transformations proposed by
Freifeld et al. [8] are a representation of a sub-class of these transformations that can be
computed in high accuracy eﬃciently while maintaining a high level of expressiveness.
That class of transformations is based on the integration of Continuous Piecewise-Aﬃne
(CPA) velocity ﬁelds. Hence, they are referred to as CPA-based (CPAB) transforma-
tions. CPA velocity ﬁelds are continuous velocity ﬁelds that are piecewise aﬃne with
respect to a tessellation of the domain. Because these are continuous velocity ﬁelds, we
are able to deﬁne a trajectory φ(x, t) at every point x in the space. Here a trajectory
refers to a path in space as a function of time and the origin point. This trajectory
φ(x, t) is given by the following integral equation:
φθ(x, t) = x +
Z t
0
vθ(φθ(x, τ))dτ,
(2.1)
where θ is the parameter of the CPA velocity ﬁeld and vθ is the velocity function. By
ﬁxing t and mapping all points to the point at time t on its trajectory, we obtain a CPAB
17

18
CHAPTER 2.
CPAB TRANSFORMATIONS
transformation. It can be shown that this transformation is indeed a diﬀeomorphism
as the inverse is deﬁned by the negative of the velocity ﬁeld.
To calculate this trajectory, we could have used standard numerical methods to
approximate the integral of (2.1).
However, Freifeld et al. [8] has shown that one
can combine a tessellation of the domain along with a speciﬁc parameterization of the
velocity ﬁeld to achieve a highly accurate and eﬃcient method for computing CPAB
transformations. See Algorithm 1 and its discussion in [8] for more details.
For a 1-dimensional domain, we tessellate the domain into segments on the x-axis.
As the velocity ﬁeld is CPA, the velocity in each cell is aﬃne and has to be continuous
across diﬀerent cells. An example of such a velocity ﬁeld is given in Figure 2.1a. In the
1-dimensional case, the number of parameters (i.e., dim(θ)) is given by the number of
vertices which in this example is 6.
(a)
(b)
Figure 2.1: (a) An example of a 1-dimensional velocity ﬁeld.
Here the tessellation
contains 5 cells equally spaced between 0 and 10. (b) An example of a 2-dimensional
tessellation we will use. Here the tessellation contains 64 cells.
For a 2-dimensional domain, we choose a tessellation which allows for fast compu-
tation of which cell a point is in, although, other tessellation choices can certainly be
used. We ﬁrst partition the 2-dimensional domain into a rectangular grid and then

Sec. 2.1.
Bayesian Inference using CPAB transformations
19
for each rectangle, we draw the diagonals to create 4 triangles. An example of such
a tessellation is given in Figure 2.1b. There are 64 cells in this example tessellation
with the added constraint that the velocity ﬁeld has to be continuous. Without this
continuity constraint, the dimension would have been 6 × 64 as there are 6 degrees of
freedom for each cell. The continuity constraints reduce the dimensionality to 58. See
lemma 5 in [8] for more details.
■2.1 Bayesian Inference using CPAB transformations
We apply CPAB transformations to the problem of correspondence-based image warp-
ing. The problem is, given 2 images and a set of correspondences, to infer the underlying
transformation that mapped the source image to the destination image. This is illus-
trated in Figure 2.2 where we would like to ﬁnd a CPAB transformation that maps the
red points to the blue points. Note that the ﬁgure does not show the explicit corre-
spondences, but we treat them as known in the inference. In most cases for a given
tessellation, there may not exist any θ that maps the source points to the destination
points with no error. Hence, we formulate this as a probabilistic inference problem
where we want to infer the parameters θ of a CPAB transformation that maximizes the
posterior. This allows for solutions with transformations that do not match the corre-
spondences perfectly. We model the destination points via a spherical Gaussian noise
centered about their corresponding transformed source points. We have the following
likelihood distribution:
P(X|θ) = C1e−
P
i ||T θ(xi,t)−yi||2
2σ2
(2.2)
where C1 is a constant (w.r.t. θ), θ is the parameter of the CPAB transformation, t
is the time to evaluate the trajectory, σ is the standard deviation of the Gaussian,
X = x1, x2, . . . , xn are the source points and Y = y1, y2, . . . , yn are the destination

20
CHAPTER 2.
CPAB TRANSFORMATIONS
points. This distribution obtains its maximum when the sum of squared errors obtains
its minimum.
In addition, we also have a Gaussian prior over θ which encourages
smoothness of the velocity ﬁeld; see [8] for more details. Thus, the posterior distribution
is:
P(θ|X) ∝P(X|θ)P(θ)
(2.3)
= C1e−
P
i ||T θ(xi,t)−yi||2
2σ2
C2e−1
2 (θ−θµ)T Σ−1
θ
(θ−θµ)
(2.4)
∝e−
P
i ||T θ(xi,t)−yi||2
2σ2
−1
2 (θ−θµ)T Σ−1
θ
(θ−θµ)
(2.5)
where C1, C2 are constants (w.r.t. θ), θµ is the mean of the prior and Σθ is the covariance
of the prior. We now have an optimization problem where we want to maximize the
posterior P(θ|X). To avoid overﬂow and underﬂow, we deﬁne our objective function
f(θ) to be the posterior in the log domain:
f(θ) = 1
2(θ −θµ)T Σ−1
θ (θ −θµ) −
1
2σ2
X
i
||T θ(xi, t) −yi||2,
(2.6)
We consider three diﬀerent optimization techniques to minimize this objective func-
tion. These are Gradient Descent, Metropolis’ algorithm and Sequential Importance
Resampling. We then discuss the utility of implementing these methods in Julia. In
particular, we focus on the ease of coding the implementation and the performance of
the implementation.
■2.1.1 Gradient Descent
Gradient Descent [2] is one of the most popular methods in optimization as it is easy
to implement and understand. Gradient Descent ﬁnds a local minimum of an objective
function by taking steps in the direction of the steepest descent. We start with an initial
value of θ0. At each iteration, we ﬁnd the gradient f′(θi) numerically and update the
parameter as follows:
θi+1 = θi −γf′(θi)
(2.7)

Sec. 2.1.
Bayesian Inference using CPAB transformations
21
Figure 2.2: An inference problem: Inferring the underlying parameters of a CPAB
transformations that maps the red points to the blue points.
where γ is a chosen step size which determines the rate of convergence.
A signiﬁcant drawback of using Gradient Descent is that it can only ﬁnd local
minima and in most cases we would like to ﬁnd the global minimum. A common work-
around is to try multiple initial θ0 and running Gradient Descent on each of them.
However, even with this, it will perform poorly on a function with many local minima,
especially when the dimension of θ is high.
In our implementation for CPAB, we
observe that gradient-based optimization methods do not perform well in comparison
to the other methods.

22
CHAPTER 2.
CPAB TRANSFORMATIONS
■2.1.2 Metropolis’ Algorithm
Metropolis’ Algorithm [15] is an MCMC sampling algorithm that allows us to sample
from a diﬃcult-to-sample probability distribution P(x). Again, we start oﬀwith an
initial value of x0. At each iteration, we have a symmetric proposal distribution Q(x|xi).
A common choice for continuous parameters is to use a Gaussian distribution centered
at xi. We generate a sample x′ from this proposal distribution. We then calculate
the acceptance ratio α = P(x′)/P(xi).
We automatically accept this sample if the
acceptance ratio α ≥1 and set xi+1 = x′. Otherwise, we accept x′ with probability α by
setting xi+1 = x′ and reject with probability 1−α by setting xi+1 = xi. Note that we do
not need to calculate P(x) explicitly, we only need to be able to calculate a g(x) ∝P(x)
since normalization terms cancel in the ratio. Using Metropolis’ Algorithm to sample
from a distribution P(x), we can also approximate the mode of the distribution by
storing the greatest value of P(x) seen so far at each iteration.
This optimization
method is likely to produce better results than Gradient Descent for objectives with
multiple local optima. This is a consequence of occasionally accepting samples that
might not be better than the previous sample and thereby escaping local optima. We
note that this algorithm is serial and thus, the only parallelization has to come from
evaluating P(x).
■2.1.3 Sequential Importance Resampling
Another popular MCMC algorithm is Sequential Importance Resampling [11]. Instead
of sampling one sample each iteration in Metropolis, we now sample multiple samples
(particles) each iteration that approximates the distribution. For a target distribution
P(x) we ﬁrst initialize a set of particles randomly sampled from the prior. At each
iteration, we calculate a weight for each particle proportional to their probability such
that the weights sum up to 1. We then resample another set of particles from these

Sec. 2.2.
Implementation and Results
23
particles based on the weights distribution. So, higher weighted samples will have a
higher chance of being picked. After getting the resampled particles, for each particle,
we perturb it with a perturbation distribution centered at that particle. We have now
obtained our set of particles for the next iterations. After some number of iterations,
we should obtain a good approximation of the target distribution.
We now apply it to our problem. In our case, our weights wj are deﬁned to be:
˜
wj = P(θi
j|X)
(2.8)
wj =
˜
wj
P
k ˜
wk
(2.9)
where θi
j is the jth particle in the ith iteration and P(θ|X) is given by Equation (2.5).
Since we are doing operations on multiple particles in each iteration, we see that we
are able to parallelize the computation over the particles. In addition, this algorithm is
able to escape local minima as long as the perturbation distribution is chosen so that
the model explores particles suﬃciently far from the original particles.
■2.2 Implementation and Results
We implemented CPAB transformations as well as Metropolis’ Algorithm and Sequen-
tial Importance Resampling in Julia.
In addition, we used a Julia package Optim
which includes Gradient Descent. We now look at some implementation details along
with the results we obtained from our experiments. Our code is available online at
https://github.com/angel8yu/cpab-diffeo-julia.
■2.2.1 Parallel Computing in Julia
Parallel Computing in Julia is built on managing diﬀerent processes through a single
master process. Typically, a number of worker processes are added at the start of a
Julia session and the master process sends work to each process by calling the @spawnat
macro, remotecall fetch or remotecall wait functions. Julia currently does not have

24
CHAPTER 2.
CPAB TRANSFORMATIONS
native multi-threading support although this is currently under development.
An important part of designing a parallel application is working out how the data
is stored. There are two ways to store data so it can be processed in parallel in Julia.
The ﬁrst way is to use an SharedArray objects which uses shared memory so it can
be accessed across diﬀerent processes.
The other way is to use DistributedArray
objects where each participating process has a chunk of data it is in charge of. An
advantage of SharedArray is that each process has read-and-write access to the entire
array, while DistributedArray has read-and-write access only to its own chunk of data.
It is also possible for a process to read data that it is not in charge of. However, the
following is what actually happens when a process reads an entry not part of its own
chunk. The process ﬁrst sends a request to the master for the entry which then looks
up the corresponding owner process that owns the entry. The master then requests
the entry from the owner process and once it gets it, the master will forward the entry
to the original process. The problem, however, is that the communication overhead
of this can quickly dominate processing time if processes constantly require reading
data remotely. On the other hand, SharedArray allows for both fast reads and writes
on the entire data from every process.
One major disadvantage of SharedArray is
that the data can only be distributed and accessed from processes on a single machine,
while DistributedArray allows for data to be distributed across processes on diﬀerent
machines. This means that for multi-machine parallelization, we are mostly limited to
using DistributedArray to store the data in memory.
A simple example of parallelizing the computation of the function f which acts over
a DistributedArray is shown as follows:
1 @sync for p=workers ()
2
@async
remotecall_wait(p, f, A)
3 end

Sec. 2.2.
Implementation and Results
25
Here, f can be a simple function which doubles all the entries of A that the current
process is in charge of like below:
1 @everywhere
function f(A:: DArray{Float64 ,1})
2
local_A = localpart(A)
3
for i=1: length(local_A)
4
local_A[i] = 2* local_A[i]
5
end
6 end
The previous 2 code snippets include most of the commonly used inbuilt Julia functions
for parallel computing. @sync blocks will wait until all asynchronous tasks to complete
before moving on. @async blocks creates an asynchronous task that will be executed
asynchronously on the same process. remotecall wait will spawn a function call at a
process indicated by the ﬁrst argument and wait until it has ﬁnished. In the example, it
will spawn the function call f(A) on process number p. Together, the 3 lines of code will
parallelize the computation of f over the DistributedArray A. In the implementation of
f, we obtain the locally stored portion of the DistributedArray A by using the inbuilt
function localpart. As we can see with this simple example, it is straightforward to
parallelize code in Julia while still maintaining a granular level of control over how the
data is stored and computed. For an in-depth discussion of parallel computing in Julia,
see http://docs.julialang.org/en/release-0.4/manual/parallel-computing/.
■2.2.2 CPAB transformations
We implemented the calculation of both 1-dimensional and 2-dimensional CPAB trans-
formations in Julia. Note that this is the implementation of the CPAB transformation,
not the inference which will be discussed later. Figure 2.3 shows an example of a 2-
dimensional CPAB transformation on an image. Since, we are computing each pixel
independently, this problem is embarrassingly parallel. Using Julia’s parallel comput-

26
CHAPTER 2.
CPAB TRANSFORMATIONS
ing framework, it was simple and straightforward to parallelize this computation. We
store the source points in a SharedArray so that all processes have access to the entire
array. Each worker then calculates the destination points for an equal number of source
points. The computation of the destination point is implemented based on Algorithm 1
in [8]. Implementing this function is straightforward as Julia is syntactically similar
to MATLAB although with a few noteworthy diﬀerences which are summarized in
http://docs.julialang.org/en/release-0.4/manual/noteworthy-differences/.
The time needed to compute a 2-dimensional CPAB transformation on a 512x512
image is shown in Table 2.1. Even though this problem is embarrassingly parallel, we can
see from the timings that there is much communication overhead in the parallelization.
We found that the majority of the overhead comes from using the @spawnat macro
which spawns work at a speciﬁc process. We suspect this is because the calculation
of the transformation is quite fast and @spawnat is more suitable for spawning larger
chunks of work. Freifeld et al. [8] reported a time of 0.15s to do this computation using
a GPU while our Julia implementation takes 0.37s on 8 CPU processes.
(a)
(b)
Figure 2.3: (a) Original Image. (b) Image after applying a CPAB transformation.

Sec. 2.2.
Implementation and Results
27
Number of processes
1
2
4
8
GPU [8]
Time [sec]
0.78
0.49
0.38
0.37
0.15
Speedup
1.00x
1.59x
2.05x
2.11x
-
Table 2.1: Time to compute a CPAB transformation on a 512x512 image on an Intel
Core i7-6700K. The graphics card used in [8] was an Nvidia GTX 780.
■2.2.3 Gradient-based Optimization
We used Julia’s Optim package and applied Gradient Descent to the problem described
in Figure 2.2, namely inferring the underlying CPAB transformation given 16x16=256
correspondences. The algorithm converged after 359 iterations and the result is shown
in Figure 2.4. As this ﬁgure shows, the transformation inferred only somewhat matches
the underlying transformation. This is most likely because the transformation found
is a local minimum in our objective function. We also tried using Conjugate Gradient
Descent [16] and we obtained almost the same result, but it did converge faster than
regular Gradient Descent. We tried parallelizing over the 16x16 points when computing
the objective function. However, because the number of points is very small, we did
not observe any speedup from parallelizing.
■2.2.4 Metropolis’ Algorithm
We implemented Metropolis’ Algorithm described in Section 2.1.2 and applied it to infer
the underlying CPAB transformation in Figure 2.2 given a prior distribution. The prior
is a multivariate Gaussian distribution with a mean of ⃗0 and some covariance Σprior. We
ran the inference algorithm for 50,000 iterations and set our proposal distribution to be
a Gaussian centered at the current sample with a covariance of 0.00001Σprior. The result
of this algorithm is shown in Figure 2.5a. We can see that the points obtained using the
inferred transformation match pretty well with the target points. This is much better
than the results we obtained from using Gradient Descent. We note that there is an

28
CHAPTER 2.
CPAB TRANSFORMATIONS
Figure 2.4: Applying Gradient Descent to infer the underlying CPAB transformation.
The points in blue are the target points and the points in red are the points after
applying the inferred transformation.
underlying CPAB transformation that maps the source points to the destination points
exactly, although, we did not ﬁnd it using this algorithm. In Figure 2.5b, we can see
that the algorithm converges at around 20,000 iterations. We tried parallelizing this by
parallelizing the calculation of CPAB transformations described earlier. However, since
there are only 256 points, it runs very fast and parallelizing it actually slows it down
because of the overhead in communication. In particular, recall that we did not see
great speedup when parallelizing the computation over 512x512 points in Section 2.2.2,
so it is not surprising that parallelizing hurts us with only 256 points. This is due to
the fact that the overhead when spawning work is mostly constant with respect to the
number of points. We also note that we found a small Julia bug while running these
experiments. After talking to a Julia developer, it seems that there is a race condition
when spawning processes. In particular, a segmentation fault will occur when spawning
a large number of processes in a short time span. We have submitted an issue ticket on

Sec. 2.2.
Implementation and Results
29
github at: https://github.com/JuliaLang/julia/issues/13999.
(a)
(b)
Figure 2.5: (a) Applying Metropolis’ Algorithm to infer the underlying CPAB trans-
formation. The points in blue are the target points and the points in red are the points
after applying the inferred transformation. (b) Plot showing the log likelihood and log
prior over 50,000 iterations.
■2.2.5 Sequential Importance Resampling
We implemented the Sequential Importance Resampling algorithm described in Sec-
tion 2.1.3 and applied it to infer the underlying CPAB transformation in Figure 2.2
given a prior distribution of θ. We again have the prior as a multivariate Gaussian
distribution with a mean of ⃗0 and covariance of Σprior. We took the number of particles
to be 1,000 and the number of iterations to be 200. We have considerably fewer itera-
tions than in Metropolis’ Algorithm as now we explore 1,000 samples in each iteration
instead of just 1. In addition, we let the perturbation distribution be a Gaussian cen-
tered at the current particle with covariance 0.001Σprior. The result of this algorithm is
shown in Figure 2.6a. We see that the inferred points almost match the target points
and it is comparable with the results from using Metropolis’ algorithm. However, this
algorithm allows for parallelization over the particles instead of points. This allows for
a bigger amount of computation that is sent to the workers each time which improves
the communication overhead. The time needed is summarized in Table 2.2. We see

30
CHAPTER 2.
CPAB TRANSFORMATIONS
that we obtain almost linear speedup when using 2 processes, but we obtain much
less than linear speedup when using 4 and 8 processes. This is because there is a big
communication overhead when re-sampling from the current particles as processes are
pulling data from other processes and this communication overhead dominates when
using many processes.
(a)
(b)
Figure 2.6: (a) Applying the Sequential Importance Resampling algorithm to infer
the underlying CPAB transformation. The points in blue are the target points and the
points in red are the points after applying the inferred transformation. (b) Plot showing
the log likelihood and log prior over 200 iterations.
Number of processes
1
2
4
8
Time [sec]
199
105
63
54
Speedup
1.00x
1.90x
3.06x
3.67x
Table 2.2: Running time of the Sequential Importance Resampling inference algorithm
with 1,000 particles for 200 iterations on 256 correspondences on an Intel Core i7-6700K.
In addition to using 16x16=256 correspondences, we experimented with using full
512x512 correspondences.
We ran the experiments on a cluster of 4 machines each
with a Intel Xeon E5-2670v3 CPU. The timings for them are summarized in Table 2.3.
The speedup obtained from parallelization is shown in Figure 2.7. We observe that we
obtain almost linear speedup when parallelizing over 96 processes and it did not show
signs on slowing down.

Sec. 2.3.
Conclusion
31
Number of processes
1
8
24
48
72
96
Time [sec]
2240
308.5
98.54
54.71
40.54
30.09
Speedup
1.00x
7.26x
22.7x
40.9x
55.3x
74.4x
Table 2.3: Running time of each iteration of the Sequential Importance Resampling
inference algorithm with 1,000 particles on 512×512 correspondences using distributed
computing.
Figure 2.7: A plot showing the parallelization speedup obtained when running the
Sequential Importance Resampling inference algorithm with 1,000 particles on 512×512
correspondences.
■2.3 Conclusion
In this chapter, we implemented the calculation of CPAB transformations in Julia in
parallel. We saw that it did not beneﬁt too much from the parallelization as the time to
compute the transformation was already quite fast and the overhead in spawning work
on the worker processes dominated. The GPU implementation in [8] ran around twice as
fast as our parallel implementation. However, although this problem is suited towards

32
CHAPTER 2.
CPAB TRANSFORMATIONS
GPU programming, we note that a Julia parallel implementation is quite straightfor-
ward and works on all machines.
We also applied these transformations to solve the correspondence-based image
warping inference problem and experimented with 3 diﬀerent inference algorithms in
Julia. We used Optim’s Gradient Descent as well as Conjugate Gradient Descent im-
plementation and compared it with our implementations of Metropolis’ Algorithm and
Sequential Importance Resampling. We found that Gradient Descent and Conjugate
Gradient Descent tend to get stuck at local minima and both Metropolis’ Algorithm
and Sequential Importance Resampling produced better results. While the results of
these two algorithms are similar, it should be noted that Sequential Importance Re-
sampling allowed for easy parallelization over the particles instead of over the points
and will therefore run faster for bigger problems and machines with more cores. We
also saw that we obtained excellent speedup using Sequential Importance Resampling.

Chapter 3
Parallel Sampling of DPMM using
Subcluster Splits
In this chapter, we explore the utility of using Julia for Bayesian nonparametric infer-
ence algorithms. There is currently a Julia Bayesian nonparametric package BNP.jl [19].
However, this package only includes implementations of serial algorithms which does not
scale well with large datasets. We use Julia to implement a Markov chain Monte Carlo
(MCMC) sampler for Dirichlet Process Mixture Model (DPMM) proposed by Chang
and Fisher III [4] for both Gaussian and Multinomial distributions. That algorithm is
based on the split-merge framework by Jain and Neal [14]. It was designed as a parallel
algorithm and there is a multi-core C++ implementation available. We refer the reader
to some of the negative points about parallel inference algorithms in [9]. This algorithm
assumes a shared memory model and we extend the algorithm to accommodate a dis-
tributed memory model via an explicit partitioning of the data. This enables our Julia
implementation to extend the existing C++ implementation by allowing parallelization
across multiple machines. In addition, we also apply this algorithm to the image de-
noising problem by using our Dirichlet Process Gaussian Mixture Model (DP-GMM)
implementation to learn image priors similar to the work by Zoran and Weiss [21]. We
note that Hughes and Sudderth [13] also applied DP-GMM to this problem.
How-
ever, their algorithm is based on variational inference while we use a sampling based
33

34
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
algorithm.
■3.1 Background
In this section, we give a brief overview of the DPMM model, the parallel MCMC
sampling algorithm that we implemented and our image denoising application.
We
refer the reader to Section 2.2.1 for some background about parallel computing in Julia
in general.
■3.1.1 DPMM Model
A DPMM is an inﬁnite-dimensional mixture model which uses a Dirichlet Process as
the prior over the clusters parameters. The original proof of the existence of Dirichlet
Processes can be found in Ferguson [7].
We refer the readers to [18] for a detailed
discussion on Dirichlet Processes. The generative model of a DPMM is as follows:
G ∼DP(α, H),
(3.1)
˜θi ∼G(˜θi),
∀i ∈{1, 2, . . . , N},
(3.2)
xi ∼fx(xi; ˜θi),
∀i ∈{1, 2, . . . , N}.
(3.3)
Here, α is the concentration parameter and H is the base measure for the DP. For each
point i, we draw a sample ˜θi from the Dirichlet process realization G. We then draw
our xi from the distribution parameterized by ˜θi. Because, G is a realization of a DP,
it is a distribution over discrete atoms. Hence, multiple data points will share the same
parameter ˜θi. Points which share the same parameter form a cluster and the shared ˜θ
are referred to as cluster parameters.
An equivalent representation, but perhaps easier to understand, is based on the

Sec. 3.1.
Background
35
stick-breaking process. This alternative generative model is as follows:
π ∼GEM(1, α),
(3.4)
θk ∼fθ(θk; λ) = H,
∀k ∈{1, 2, . . . },
(3.5)
zi ∼Cat(π),
∀i ∈{1, 2, . . . , N},
(3.6)
xi ∼fx(xi; θzi),
∀i ∈{1, 2, . . . , N}.
(3.7)
Here, GEM refers to the (Griﬃths-Engen-McCloskey) stick-breaking process for a DP.
By drawing a sample from GEM(1, α), we get an inﬁnite-dimensional vector of weights
π. For each of the weights πk, we draw a sample from the base measure H as our
parameter for cluster k. For each data point i, we ﬁrst draw a sample for the label
assignment zi using the weights π.
Finally, we sample xi from the distribution of
cluster zi which is parameterized by θzi.
■3.1.2 Parallel Sampler for DPMMs
Chang and Fisher III [4] proposed a parallel sampling algorithm for DPMMs. The main
idea of that algorithm is to have a restricted Gibbs sampler which ﬁxes the number of
clusters and then proposes splits and merges. This allows a Gibbs sampling step to
be done in parallel without having to worry about the creation of new clusters. In
order to propose meaningful splits that are likely to be accepted, the algorithm uses
auxiliary variables such that each cluster is the composed of 2 sub-clusters.
These
variables are ¯zi ∈{l, r} which denotes which of the sub-cluster the ith point is assigned
to and ¯πk = {¯πkl, ¯πkr}, ¯θk = {¯θkl, ¯θkr} which denotes the weights and parameters of
the sub-clusters of cluster k.
By sampling the sub-clusters, we are able to propose
meaningful splits that split a cluster into its 2 sub-clusters. Merges are proposed by
merging 2 sub-clusters into one with each of the original clusters becoming a sub-cluster
of the new merged cluster. The algorithm is summarized in Algorithm 3.1. We use the

36
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
same notation as in [4] where N is the number of data points, K is the number of
clusters, fx(X; θ) is the likelihood of a set of data under the parameter θ, fθ(θ; λ) is the
likelihood of the parameter θ under the prior λ, ∝∼denotes sampling proportional to
the right-hand side of the equation and Ik denotes the set of indices of i where zi = k
Algorithm 3.1 An iteration of DPMM with sub-cluster splits
Run an iteration of restricted Gibbs sampling:
(a) Sample cluster weights π1, π2, . . . , πK,:
(π1, . . . , πK, ˜πK+1) ∼Dir(N1, . . . , NK, α).
(3.8)
(b) Sample sub-cluster weights ¯πkl, ¯πkr for each cluster k ∈{1, . . . , K}:
(¯πkl, ¯πkr) ∼Dir(Nkl + α/2, Nkr + α/2).
(3.9)
(c) Sample cluster parameters θk for each cluster k:
θk ∝∼fx(xIk; θk)fθ(θk; λ)
(3.10)
(d) Sample sub-cluster parameters ¯θkh for each cluster k ∈{1, . . . , K} and h ∈{l, r}:
¯θkh ∝∼fx(xIkh; ¯θkh)fθ(¯θkh; λ).
(3.11)
(e) Sample cluster assignments zi for each point i ∈{1, . . . , N}:
zi ∝∼
K
X
k=1
πkfx(xi; θk)1(zi = k).
(3.12)
(f) Sample sub-cluster assignments ¯zi for each point i ∈{1, . . . , N}:
¯zi ∝∼
X
h∈{l,r}
πzihfx(xi; ¯θzih)1(¯zi = h).
(3.13)

Sec. 3.1.
Background
37
Algorithm 3.1 An iteration of DPMM with sub-cluster splits (continued)
Propose and Accept Splits:
(a) Propose to split cluster k into its 2 sub-clusters for all k ∈{1, 2, . . . , K}.
(b) Calculate the Hastings ratio H and accept the split with probability min(1, H):
Hsplit =
α
Γ(Nk)fx(xIk; λ) · Γ(Nkl)fx(xIkl; λ) · Γ(Nkr)fx(xIkr; λ)
(3.14)
Propose and Accept Merges:
(a) Propose to merge clusters k1, k2 for all pairs k1, k2 ∈{1, 2, . . . , K}.
(b) Calculate the Hastings ratio H and accept the merge with probability min(1, H).
Hmerge = 0.01 Γ(Nk1 + Nk2)
αΓ(Nk1)Γ(Nk2)
p(x|ˆz)
p(x|z)
Γ(α)
Γ(α + Nk1 + Nk2)
Γ(α
2 + Nk1)Γ(α
2 + Nk2)
Γ(α
2 )Γ(α
2 )
(3.15)
As the algorithm is designed with parallelization in mind, almost all of the operations
can be parallelized. In particular, sampling the cluster parameters can be parallelized
over the clusters, sampling the assignments can be computed independently for each
point and splits can also be proposed in parallel. Hence, a multi-core implementation
of this algorithm would be quite straightforward. However, we have to be careful with
merges as we do not want to have more than 2 clusters merging into a single cluster.
For example, we do not want to merge clusters 1 and 2 along with clusters 2 and 3.
■3.1.3 Image Denoising using Expected Patch Log Likelihood
Zoran and Weiss [21] developed a method to perform image denoising by maximizing
the Expected Patch Log Likelihood (EPLL) under a learned prior while staying close
to the original noisy image. For an image x and prior p, the EPLL is deﬁned to be:
EPLLp(x) =
X
i
log p(Pix)
(3.16)

38
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
where Pix extracts the ith patch in image x and log p(Pix) gives the log likelihood of
the patch under the prior p. Note that the sum is over all overlapping patches of a
given size across the image. Since we also would not like to deviate too much from the
original noisy image y, the proposed cost function is:
fp(x|y) = λ
2 ||x −y||2 −EPLLp(x).
(3.17)
Here, λ is a parameter related to the variance of the noise (≈
1
σ2 ).
To optimize the cost function given in Equation (3.17), an optimization method
called “Half Quadratic Splitting” [10] is used. In this method, we have an auxiliary
variable zi for every patch Pix and instead optimize the following cost function with
respect to both x and z:
cp,β(x, z|y) = λ
2 ||x −y||2 +
X
i
β
2 ||Pix −zi||2 −log p(zi)

.
(3.18)
We see that for β →∞, maximizing Equation (3.18) is equivalent to maximizing
Equation (3.17). For a given β, we are able to optimize Equation (3.18) iteratively by
alternating between optimizing with respect to x and with respect to z. We do this for
a large β in order to optimize the cost function given in 3.17. For more details, we refer
the reader to [21].
■3.2 Implementation
We implemented a sampler with multi-machine parallelization for the case when each
cluster is a Gaussian i.e. a Dirichlet Process Gaussian Mixture Model (DP-GMM) in
Julia as well as when each cluster is Multinomial i.e. a Dirichlet Process Multinomial
Mixture Model (DP-MNMM). We discuss key implementation details and challenges
below. Assume we have N data points, K clusters and M machines each with P cores.
Our code is available at https://github.com/angel8yu/Subcluster-DPMM.jl.

Sec. 3.2.
Implementation
39
■3.2.1 High-Level Design/Architecture
In order to achieve multi-machine parallelization in Julia, we use the DistributedArray
object to store our data.
In our implementation, we make use of suﬃcient statis-
tics [17] in our sampler. The suﬃcient statistics for a Gaussian cluster with points
x1, x2, . . . , xm ∈Rd are:
T G
1 =
m
X
i=1
xi,
(3.19)
T G
2 =
m
X
i=1
xT
i xi.
(3.20)
While the suﬃcient statistics for a Multinomial cluster with points x1, x2, . . . , xm are:
T MN
1
=
m
X
i=1
xi.
(3.21)
Note that in the Gaussian case, the points xi ∈Rd while in the Multinomial case, the
points xi ∈Nd
0 is a vector of counts. Here, d is the dimensionality of the points.
By aggregating suﬃcient statistics for each cluster, we are able to sample the cluster
parameters. Because the suﬃcient statistic is a sum over the data points, we can collect
the sums for each process and aggregate them. In fact, this property holds true for all
distributions in the exponential family. For an introduction to the concept of suﬃcient
statistics in general, and in the exponential family in particular, please see [17] and [3].
Sampling the cluster and sub-cluster parameters takes O(K) time given the suﬃ-
cient statistics, so we parallelize them only on the master machine as K is generally
small compared to the number of data points. Sampling the cluster and sub-cluster
assignments takes O(NK) time as can be seen from Equations (3.12) and (3.13). These
are the only steps that scale with both N and K, so parallelizing these steps is key.
Since the assignment of each data point is sampled independently from the rest, we
distribute the data points evenly across every process in every machine along with their
cluster assignments. We then maintain the cluster and sub-cluster parameters in the

40
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
Data1
Labels1
Data4
Data3
Data2
Labels2
Labels3
Labels4
Model Parameters (Master)
Sample Assignments
Sample Parameters
Propose Splits
Propose Merges
Sufficient Stats
Figure 3.1: The proposed architecture of our proposed multi-machine implementation.
We show a parallelization over 4 machines here.
master machine along with their suﬃcient statistics. They are then broadcasted to all
processes so we are able to sample the assignments. Proposing splits is takes O(K)
time and proposing merges takes O(K2) time which does not scale with N so they are
relatively fast and are thus done on the master machine. This architecture is illustrated
in Figure 3.1.
■3.2.2 Detailed Implementation Design
We maintain the following SharedArray objects on the master machine:
• Cluster and sub-cluster weights
πk, ¯πkl, ¯πkr
• Cluster parameters
θk (e.g. µk (mean) and Σk (covariance) for Gaussian)
• Sub-cluster parameters
θkl, θkr

Sec. 3.2.
Implementation
41
• Cluster suﬃcient statistics
Nk (number of points), Tk (suﬃcient statistics)
• Sub-cluster suﬃcient statistics
Nkl, Nkr (number of points), Tkl, Tkr
In addition, we also maintain an array cluster index on the master process mapping
the current clusters to indices in each of the SharedArray objects. This is because
the size of SharedArray objects cannot be changed after creation so we would need
to pre-allocate a larger array and then ﬁll it up. Once it is ﬁlled, we will allocate a
new larger array and migrate from the old array to new array. This is similar to table
doubling in hash table implementations [5].
We maintain the following objects on every worker process:
• Cluster and sub-cluster weights (broadcasted from master)
• Cluster and sub-cluster parameters (broadcasted from master)
• Chunk of data (as part of a DistributedArray)
xi
• Chunk of cluster and sub-cluster assignments (as part of a DistributedArray)
zi, ¯zi
We now describe our implementation which follows the steps in Algorithm 3.1.
1. Run an iteration of restricted Gibbs sampling:
(a) Sample the cluster weights using Equation (3.8) on the master process.
(b) Sample the sub-cluster weights using Equation (3.9) on the master process.

42
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
(c) Sample the cluster parameters using Equation (3.10) on worker processes on
the master machine in parallel.
(d) Sample the sub-cluster parameters using Equation (3.11) on worker processes
on the master machine in parallel.
(e) Broadcast cluster and sub-cluster weights and parameters to all worker pro-
cesses.
(f) Sample the cluster assignments using Equation (3.12) in parallel across all
worker processes. Each worker is responsible for sampling the assignments of
the data it is in charge of updating the assignments in the DistributedArray.
(g) Sample the sub-cluster assignments using Equation (3.13) in parallel across all
worker processes. Each worker is responsible for sampling the assignments of
the data it is in charge of updating the assignments in the DistributedArray.
(h) Update the cluster and sub-cluster suﬃcient statistics in parallel across all
worker processes.
Each worker is responsible for calculating the suﬃcient
statistics of all the clusters for the data it is in charge of. We then aggre-
gate the suﬃcient statistics across all workers by simply summing them. For
example, if we have 4 workers and we wanted to calculate the suﬃcient statis-
tics T for cluster 1, we would ﬁrst calculate the suﬃcient statistics for each of
4 workers T i on the data points it is in charge of that is assigned to cluster 1
and sum them as follows:
T = T 1 + T 2 + T 3 + T 4
(3.22)
Note that we do not maintain the suﬃcient statistics received from each worker
as this is a synchronous implementation. If we were doing this asynchronously,
we would maintain the suﬃcient statistics received as this would allow us to
undo the previous contribution to the aggregated statistics.

Sec. 3.2.
Implementation
43
2. Splits:
(a) Propose to split cluster each cluster in parallel on the master machine. The cal-
culation of the Hastings ratio using Equation (3.14) only requires the suﬃcient
statistics which are available via a SharedArray on the master.
(b) Process all the accepted splits by creating an assignment mapping which maps
old assignments to new assignments. This is then broadcasted to all worker
processes which updates the assignments of all the points it is responsible
for. The suﬃcient statistics for the newly-split clusters are also calculated and
updated in the same way as shown in Equation (3.22).
3. Merges:
(a) Propose to merge all pairs of clusters in serial on the master process by using
Equation (3.15). As in [4], when a merge between 2 clusters has been accepted,
we do not consider them for future merges in this iteration.
(b) Process all the accepted merges by creating an assignment mapping which
maps old assignments to new assignments. This is then broadcasted to all
worker processes and each of which updates the assignments of all the points
it is responsible for. The suﬃcient statistics for the newly merged clusters are
also calculated and updated in the same way as shown in Equation (3.22).
■3.2.3 Optimizing Processes Communication
In each iteration, we have to broadcast a number of objects from the master process to
all processes as well as the suﬃcient statistics from the workers so they can be aggre-
gated. A naive way to do this would be to send the objects from the master process
to each of the workers and the suﬃcient statistics from the workers to the master di-
rectly. Unsurprisingly, however, we found that inter-machine communication (between

44
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
processes on the same machine) is much faster than intra-machine communication (be-
tween processes on diﬀerent machines).
To reduce this communication overhead in
our implementation, we ﬁrst send the objects to be broadcasted from the master to 1
process in each machine and then using that process to send it to all processes in its
machine. For suﬃcient statistics that need to be aggregated, we ﬁrst aggregate them
within each machine and then send those aggregated suﬃcient statistics to the master
for ﬁnal aggregation. This reduced our communication overhead tenfold.
In theory, we could have further reduced communication overhead by sending the
objects to be broadcasted from the master to 1 process in each machine and then have
that process update a SharedArray object which will be accessible to all processes.
However, as we were trying to implement this, we found that slices of SharedArray
objects did not support matrix multiplication in the newest oﬃcial release of Julia at the
time of writing (v0.4.6). This prevented us from adopting this method of broadcasting
as we needed matrix multiplication when calculating the probabilities used for sampling.
We ﬁled an issue ticket at https://github.com/JuliaLang/julia/issues/17100 and
found that it would be supported in the new v0.5 version which is under development
at the time of writing.
■3.2.4 Runtime Complexity
In Section 3.2.1, we touched on the runtime complexity of our implementation. We now
analyze the total runtime complexity of our parallel implementation.
1. Run an iteration of restricted Gibbs sampling:
(a) Sampling the cluster weights takes constant time.
(b) Sampling sub-cluster weights takes constant time for each cluster and is not
parallelized. This takes O(K) time.
(c) Sampling the cluster and sub-cluster parameters also take constant time for

Sec. 3.2.
Implementation
45
each cluster but are parallelized over P processes on the master. This takes
O(K/P) time.
(d) Broadcasting cluster and sub-cluster weights and parameters to all worker
processes take O(M + P) as we ﬁrst broadcast to each machine and then from
each machine, we broadcast to every process.
(e) Sampling the cluster assignments takes O(NK) time in serial, but this is par-
allelized over MP processes so this takes O(NK/(MP)).
(f) Sampling the sub-cluster assignments takes O(N) time in serial, but this is
parallelized over MP processes so this takes O(N/(MP)).
(g) Updating the cluster and sub-cluster suﬃcient statistics can be split up into 2
steps. The ﬁrst step is to for all workers to calculate the suﬃcient statistics for
the data it is in charge of. This step takes O(N/(MP)) time. The second step
is to aggregate across all workers. This step takes O(M + P) time. Overall,
this take O(N/(MP)) + O(M + P) time.
2. Splits:
(a) Proposing splits by looking at each cluster takes O(K) time.
(b) Processing all the accepted splits requires updating the suﬃcient statistics
which could take worst case O(N/(MP)) + O(M + P) if all clusters are split.
3. Merges:
(a) Proposing merges by looking at each pair of clusters takes O(K2) time.
(b) Processing all the accepted merges also requires updating the suﬃcient statis-
tics which could take worst case O(N/(MP)) + O(M + P) if all clusters are
merged.

46
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
Hence, we see a total runtime complexity of:
O(K) + O(M + P) + O(NK/(MP))
(3.23)
Since N ≫K, p, M, this implementation achieves linear parallelization theoretically.
■3.2.5 Memory Complexity
The beneﬁt of a multi-machine implementation goes beyond the gains from a faster
runtime as it also allows computation over larger datasets that cannot ﬁt into the
memory of a single machine. With our multi-machine architecture, the bulk of the data
is split evenly across all machines.
We now look at the amount of memory our implementation needs on each machine.
The data is stored as a distributed array across all processes, so we have O(N/M) on
each machine. Each process also has a copy of the cluster and sub-cluster parame-
ters which takes O(Kp) space for each machine. We also have to aggregate suﬃcient
statistics for each cluster after sampling the assignments. This also takes O(Kp) for
each machine. Hence, we have a total memory usage of O(N/M + Kp). Since usually
N ≫K, p, M, the memory overhead is insigniﬁcant compared to the data itself.
■3.3 Results
We compared our implementation with the implementation from [4] by running them
on synthetic data. We also applied our implementation to the real-data problem of
image denoising, where we ﬁrst learned a DP-GMM prior over images patches and then
used that prior within a standard method of patch-based image denoising. Our setting
is akin to the one described in [21]; note, however, that their prior is a ﬁnite (and
non-Bayesian) Gaussian Mixture Model (GMM) where K, the number of Gaussians, is
ﬁxed manually. Also, as we will show, our distributed implementation easily handles
larger training datasets than theirs.

Sec. 3.3.
Results
47
■3.3.1 Synthetic Data: DP-GMM
We ﬁrst show the results of applying our implementation on large synthetic datasets. We
generated synthetic data for our DP-GMM implementation by sampling 106 points from
an underlying GMM with 6 clusters. For illustration, we start with 2-dimensional points
but note that our implementation works for arbitrary dimensions. Figure 3.2a shows
the location of the points in the 2-dimensional case. After running our implementation
on this synthetic data, we obtain the results shown in Figure 3.2b. We obtained similar
results when running the implementation from [4].
We compare the runtime of our implementation with the implementation from [4]
in Table 3.1. They were run on machines with Intel Xeon E5-2670 v3 processors. Re-
call that the implementation from [4] does not support multiple machines. We limit
the number of cores in each machine to 8 in order to better show the speedups we
get by parallelizing over multiple machines for this size of data. As we can see, our
Julia implementation is consistently around 14 times slower on a single machine. As
we increase the dimensionality of our data set, our implementation performs relatively
better. For example, if we increase the dimensionality to 30, we see from Table 3.2 that
our implementation is almost twice as fast on a single machine and we obtain additional
good speedup on multiple machines. This leads us to believe that their implementation
is optimized for low dimensions. However, we have also noted (empirically) that the
implementation from [4] fails when we go past a certain dimension (while our imple-
mentation still succeeds). For example, when we set the dimensionality to be 250, their
implementation converged to 1 cluster while ours converges to the correct 6 clusters.
Attempts to debug their implementation have led us to suspect the problem is caused
by underﬂow/overﬂow errors.

48
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
(a)
(b)
Figure 3.2: (a) The 106 points in our synthetic data, in the 2-dimensional case, sampled
from an underlying GMM with 6 clusters are shown in blue. (b) The inferred clusters
from our implementation are shown as ellipses at 5 standard deviations in diﬀerent
colors.
Cores × Machines
1
2
4
8
8×2
8×3
8×4
C++ [4]
76.94
40.57
22.23
13.01
-
-
-
Julia
1101.97
572.50
345.58
172.30
107.58
80.10
63.55
Table 3.1: Time(sec) to run 100 DP-GMM iterations of d = 2, N = 106, K = 6.

Sec. 3.3.
Results
49
Cores × Machines
8
8×2
8×3
8×4
C++ [4]
798.94
-
-
-
Julia
398.67
218.42
146.71
124.55
Table 3.2: Time(sec) to run 100 DP-GMM iterations of d = 30, N = 106, K = 6.
■3.3.2 Synthetic Data: DP-MNMM
We generated synthetic data for our DP-MNMM implementation by sampling 106 points
with dimension 100 from an underlying MNMM with 6 clusters. The results we ob-
tained from running our DP-MNMM are similar to the implementation from [4]. We
compare the runtime of our implementation with the implementation from [4] in Ta-
ble 3.3. Again, they were run on machines with Intel Xeon E5-2670 v3 processors. As
we can see, our Julia implementation is around 2 times slower on a single machine.
By looking at the C++ implementation from [4], we see that they used sparse arrays
for the DP-MNMM model which most likely saves computation time as well as re-
quires lower memory allocations. We also tried changing the dimensionality but did
not ﬁnd that it made any diﬀerence to the relative performance of our implementation
unlike in the DP-GMM case. The relative performance of our DP-MNMM implemen-
tation seems to be consistent with the relative performance of Julia to C/C++ given in
http://julialang.org/benchmarks/. We also note that since running our implemen-
tation on our generated synthetic data is fast, we do not obtain any signiﬁcant speedup
after 2 machines using 8 cores each. If we increase the number of underlying clusters
to 60 which increases the runtime, we are able to see a better speedup as shown in
Table 3.4. This is due to the parallelization overhead dominating the gains from having
extra cores.

50
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
Cores × Machines
1
2
4
8
8×2
8×3
8×4
C++ [4]
134.25
77.55
40.97
23.60
-
-
-
Julia
234.40
136.43
87.34
55.10
34.58
31.50
32.61
Table 3.3: Time(sec) to run 100 DP-MNMM iterations of d = 100, N = 106, K = 6.
Cores × Machines
8
8×2
8×3
8×4
C++ [4]
145.50
-
-
-
Julia
241.31
142.89
109.95
98.95
Table 3.4: Time(sec) to run 100 DP-MNMM iterations of d = 100, N = 106, K = 60.
■3.3.3 Image Denoising
We apply this implementation to image denoising using image patches.
Zoran and
Weiss [21] used a GMM with 200 clusters on this problem by training on a sample of
2 million 8 × 8 image patches from BSDS500 [1]. They set the number 200 manually,
and their formulation of the GMM learning was not Bayesian. We use the same denois-
ing method but instead use our DP-GMM implementation to learn the prior over the
patches. The eigenvalues of 6 randomly-chosen clusters are shown in Figure 3.3. By
inspection, they are similar to the eigenvalues learned by the GMM in [21].
Figure 3.3: Eigenvalues of 6 randomly-chosen clusters learned by our DP-GMM on 2
million 8 × 8 image patches.

Sec. 3.4.
Conclusion
51
Our DP-GMM implementation took less than an hour to train when parallelized
over 4 machines each with an Intel Xeon E5-2670v3 processor. We experimented with
diﬀerent α parameters in our model and applied our trained prior to the image denoising
using the MATLAB code provided in [21]. Because we were able to run 2 million patches
with ease, we also ran all of 44,650,800 the overlapping 8 × 8 patches in the BSDS500
dataset.
This took our implementation around 2 days.
We believe that learning a
model from such a large training set was not possible to do using the serial, non-
distributed, implementation from [21]; however, this is just a speculation since Zoran
and Weiss’s released code for image denoising did not include the code for the GMM
learning.
We compared the priors learned by our DP-GMM implementation to the
prior provided by [21] which was trained using a GMM with 200 clusters by performing
image denoising on the 50 test images in the dataset. The results are summarized in
Table 3.5. We show an example of denoising using our model in Figure 3.4.
Prior
Num. Clusters
Avg. PSNR
GMM [21]
200 (ﬁxed manually)
28.5061
DP-GMM (α = 1)
72 (inferred)
28.4173
DP-GMM (α = 103)
80 (inferred)
28.4276
DP-GMM (α = 106)
99 (inferred)
28.4303
DP-GMM (α = 106, all patches)
330 (inferred)
28.5402
Table 3.5: Average PSNR obtained from denoising 50 test images with added Gaussian
noise using diﬀerent priors.
■3.4 Conclusion
In this chapter, we implemented a parallel MCMC sampling algorithm for DPMMs
proposed by Chang and Fisher III [4] in Julia. We extended their algorithm to support a
distributed memory model to allow for parallelization over multiple machines. Like their
existing C++ implementation, our Julia implementation supported both DP-GMM and

52
CHAPTER 3.
PARALLEL SAMPLING OF DPMM USING SUBCLUSTER SPLITS
Figure 3.4: An example of denoising using our model trained on all 8 × 8 patches. The
corrupted image (middle) has a PSNR of 20.17 and the restored image (right) has a
PSNR of 29.14.
DP-MNMM models.
We compared our distributed Julia implementation with their
implementation and found that on a single machine our implementation was generally
slower with some exceptions.
However, their DP-GMM implementation seem to be
optimized for lower dimensions and we do better on higher dimensions. In any case,
the point of re-implementing their C++ code in Julia was not, of course, in order
to a speedup on a single machine.
Rather, we wanted to have an implementation
that 1) lets us easily distribute the inference over multiple multi-core machines (the
implementation from [4] supports only a single multi-core machine); 2) is written in a
higher-level language - so development is easy - and that is still relatively fast. Judging
by these two criteria, we can report that our experience with Julia was positive. We also
applied our DP-GMM implementation on image denoising using learned image patch
priors [21]. We reported slightly worse results than [21] when using just a sample of 2
million patches as data for our model. However, we were able to handle all 44 million
8 × 8 patches with our distributed implementation and obtained slightly better results.

Chapter 4
Projection Dirichlet Process
Gaussian Mixture Model
A Dirichlet Process Gaussian Mixture models (DP-GMM) is a widely-used Bayesian
nonparametric extension of the classic Gaussian Mixture Model (GMM). In the previous
chapter, we implemented an eﬀective parallel sampling method for DP-GMM inference
proposed by Chang and Fisher III [4]. A standard assumption in DP-GMM inference,
[4] included, is that the draws from the DP-GMM, denoted by {xi}N
i=1 ⊂RD, are
directly observed.
There are cases, however, where this assumption does not hold.
Consequently, Chang and Fisher III’s method (similar to other DP-GMM inference
methods) is not directly applicable.
In this chapter, we present a model which relaxes the assumption above. Our model,
called the Projection Dirichlet Process Gaussian Mixture Model (PDP-GMM) instead
assumes that the observations, denoted by {yi}N
i=1 are merely aﬃne projections of the
DP-GMM draws. We also show how Chang and Fisher III’s method as well as our
implementation in the previous chapter can be adapted to handle this model. We then
apply this model to solve a few seemingly-unrelated problems.
53

54
CHAPTER 4.
PROJECTION DIRICHLET PROCESS GAUSSIAN MIXTURE MODEL
■4.1 Model and Inference
In our model, we relax the assumption that we directly observe the draws from the
DP-GMM and instead assume that only aﬃne projections of the draws are observed.
In particular, we have for i = 1, 2, . . . , N:
yi =hixi + ci ,
yi ∈Rd ,
xi ∈RD ,
hi ∈Rd×D
ci ∈Rd ,
d < D .
(4.1)
Here, {xi}N
i=1 ⊂RD is the set of draws from the DP-GMM, {yi}N
i=1 ⊂Rd is the set
of observations and hi’s and ci’s are the parameters of the aﬃne projection which are
assumed to be known. Since the ci’s are known, they can be absorbed – by subtraction
– into the yi’s. Hence, we may assume, without loss of generality, that the projections
are linear:
yi =hixi ,
yi ∈Rd ,
xi ∈RD ,
hi ∈Rd×D
d < D .
(4.2)
Note that if d ≥D and all hi’s are invertible, this is equivalent to the standard
DP-GMM model. Hence, we focus on the case when d < D. Let K denote the number
of Gaussian clusters.
As our Bayesian model is nonparametric, K will be inferred
together with the other parameters of the mixture model. We consider ﬁrst the case
where K = 1 and we have a single Gaussian with a D-dimensional mean, µ, and a
D-by-D covariance, Σ:
xi|µ, Σ ∼N(µ, Σ) =⇒yi|µ, Σ ∼N(hiµ, hiΣhT
i ) .
(4.3)
Let y = {yi}N
i=1. The likelihood is given by:
p(y|µ, Σ) = (2π)−dN
2
 N
Y
i=1
hiΣhT
i
−1
2
!
exp
 
−1
2
" N
X
i=1
(yi −hiµ)T (hiΣhT
i )−1(yi −hiµ)
#!
.
(4.4)

Sec. 4.1.
Model and Inference
55
■4.1.1 Conjugacy for Isotropic Covariances
Zheng [20] worked out the following details for the isotropic covariance case. If Σ is
isotropic, we can write Σ = c−1I where c ∈R+. Substituting this into Equation (4.4),
p(y|µ, c) = (2π)−dN
2 c
DN
2
 N
Y
i=1
hihT
i
−1
2
!
exp
 
−c
2
" N
X
i=1
(yi −hiµ)T (hihT
i )−1(yi −hiµ)
#!
.
(4.5)
We show in Appendix A that the following distribution is a conjugate distribution on
(µ, c):
p(µ, c) = p(µ|c)p(c) = N(µ; η, (cM)−1) × Gamma(c; α, β).
(4.6)
The hyperparameters, suppressed in the notation p(µ|c)p(c), are:
α > 0 ;
β > 0 ;
η ∈RD ;
M ∈SPD(D).
(4.7)
Here, SPD(D) stands for D-by-D symmetric positive-deﬁnite matrices. Note that M
need not be isotropic. We deﬁne the following quantities:
eT1 =
N
X
i=1
hT
i (hihT
i )−1yi ,
eT2 =
N
X
i=1
hT
i (hihT
i )−1hi ,
eT3 =
N
X
i=1
yT
i (hihT
i )−1yi ,
eT4 =
N
Y
i=1
|hihT
i |−1
2 .
(4.8)
After some algebra (see Appendix A), the conjugacy yields the following closed-form
expressions:
p(y|µ, c) ∝c
DN
2 eT4 exp

−c
2
h
eT3 + µT eT2µ −2µT eT1
i
,
(4.9)
p(µ, c|y) = p(µ|y, c)p(c|y) = N(µ; η′, (cM′)−1) × Gamma(c; α′, β′),
(4.10)
p(y) = (2π)−dN
2 eT4|M|
1
2 |M′|−1
2 βα
β′α′
Γ(α′)
Γ(α) .
(4.11)
where

56
CHAPTER 4.
PROJECTION DIRICHLET PROCESS GAUSSIAN MIXTURE MODEL
α′ = α + DN
2
,
M′ = M + eT2 ,
η′ = M′−1(Mη + eT1) ,
β′ = β + 1
2

eT3 + ηT Mη −η′T M′η′
.
(4.12)
■4.1.2 Anisotropic Covariance
In the anisotropic case, it can be shown that there is no conjugate prior for Σ when
d < D. Particularly, the natural choice of a Normal Inverse-Wishart (NIW) prior for
(µ, Σ) does not lead to an NIW posterior over these parameters. Although it is possible
to modify the algorithm to handle the diﬃculty of a non-conjugate prior, we focus on
the isotropic case for simplicity.
■4.2 Implementation
We extended our implementation from Chapter 3 to support the PDP-GMM model.
We made use of Julia’s native multiple dispatch feature which uses the all of a func-
tion’s arguments to decide which method to invoke. In particular, our core DPMM
code takes diﬀerent models and each model has it own types.
This allows for the
core code to be shared across diﬀerent models which include the DP-GMM, the DP-
MNMM as well as the PDP-GMM. Adding an additional model is straightforward as
only a minimal amount of code needs to be written which include deﬁning new types
and providing methods to calculate various model speciﬁc quantities. In particular,
the quantities we need are given in Equations (4.8)–(4.12). Our code is available at
https://github.com/angel8yu/Subcluster-DPMM.jl.
■4.3 Results
We now show our results when we apply this model to a few seemingly-unrelated prob-
lems.

Sec. 4.3.
Results
57
■4.3.1 Tomography
A natural application for this model would be tomography. In tomography, the ob-
servations lie in a low dimensional space and we would like to reconstruct the original
data in the higher dimensional space. Here we show an example of our model applied
in a tomography setting. Assume we have 2-dimensional draws from a DP-GMM which
are not observed. Instead, we observe the projections of these draws on lines through
the origin. Every draw is projected onto each of these lines. The setting is shown in
Figure 4.1. Without loss of generality, and to simplify the explanation, assume that
projections are taken at evenly-spaced angles; e.g., every, say, 1 or 10 degrees. For visu-
alization, at each of these angles we take its corresponding 1-dimensional measurements
and display (in Figure 4.1) their associated 1D probability density function, estimated
using a standard Kernel Density Estimator (KDE). Note these estimated 1D marginals
are used only for visualization. Particularly, our inference algorithm does not use these
marginals (and thus errors in the KDE do not propagate into our results). In fact, our
algorithm can also handle the case where each angle is associated with even just a single
observation (in which case, the use of KDE would not make sense anyway).
Using our inference algorithm, we are able to infer the latent parameters of the
model. We show select results in Figure 4.2. Note that although there are (latent) cor-
respondences, we do not need to estimate them during our inference. In other words,
our algorithm utilizes implicit observation-to-cluster correspondences as opposed to the
more error-prone and computationally intensive observation-to-observation correspon-
dences.
As we can see, the inferred parameters explain the unobserved draws well.
Figure 4.3 shows that this remains the case even when the number of (true) clusters
increases.

58
CHAPTER 4.
PROJECTION DIRICHLET PROCESS GAUSSIAN MIXTURE MODEL
Figure 4.1: (Left) We show the KDE estimates of the 1-dimensional marginals computed
from the observations. (Right) We show both the unobserved draws as well as the KDE
estimates.
■4.3.2 Mixture of polynomial regressions
We now use our model to ﬁt a mixture of polynomial regressions. This particular ap-
plication has been studied before by Faria and Soromenho [6] as well as in Huang and
Li [12]. Both of their models are ﬁnite mixture models while our model is an inﬁnite-
dimensional mixture model. In terms of the type of regression, Faria and Soromenho
focused their work on linear regressions while Huang and Li learn nonparametric regres-
sions. Our model is able to handle ﬁnite linear combinations of basis functions, but we
focus on mixtures of polynomial regressions here. To clarify the terminology, the word
’ﬁnite’ in the previous sentence refers to the dimensionality of the regression functions
(e.g., D = 2 for R →R linear regression functions as there is one degree of freedom for
the slope and one for the intercept). However, our model is inﬁnite-dimensional in the
sense that we entertain the notion that K, the (latent) number of these functions (e.g.,
K may stand for the number of linear regression functions) can be inﬁnite.
We ﬁrst start oﬀwith ﬁtting a mixture of linear regressions. Note that we take a

Sec. 4.3.
Results
59
Figure 4.2: A simple example with 10,000 points and 3 clusters. We show the inferred
cluster means and (isotropic) covariances as circles at 4 standard deviations together
with the estimated 1-dimensional marginals and the unobserved 2-dimensional draws.
slightly diﬀerent but more general view than in the classical setting where we have a
few lines that generate points with noise being in the y-dimension. Instead, each line
generates a single point and we cluster the lines. This allows for noise in both the slope
and the intercept.
Assume we have a mixture of linear regressions with (p0
i , p1
i ) being the coeﬃcients
of the ith linear regression:
y = p1
i x + p0
i .
(4.13)

60
CHAPTER 4.
PROJECTION DIRICHLET PROCESS GAUSSIAN MIXTURE MODEL
Figure 4.3: A more complicated example with 10,000 points and 10 clusters. (Left)
We show the inferred cluster means and (isotropic) covariances as circles at 3 standard
deviations together with the estimated 1-dimensional marginals and the unobserved
2-dimensional draws. (Right) We show the weights of each inferred cluster.
We then observe a point (ai, bi) ∈R2 on this line and we have:
bi = p1
i ai + p0
i
(4.14)
=

1
ai
 p0
i
p1
i

(4.15)
Using the notation in Section 4.1, we set:
yi = bi ,
xi =
p0
i
p1
i

,
hi =

1
ai

.
(4.16)
This allows us to use our model to cluster the parameters of the linear regressions xi.
For example, in Figure 4.4, we have points generated from the lines y = 3x + 1,
y = 3x + 3 and y = 0. We use our model to ﬁt a mixture of linear regressions and the
results are shown in Figure 4.4. As we can see, the inferred cluster centers ﬁt the data
and are close to the ground truth lines.
Similarly, we are able to use our model to ﬁt polynomial regressions in general. As
an example, we show some results of ﬁtting a mixture of quadratic regressions. Assume

Sec. 4.3.
Results
61
Figure 4.4: An example of using our model to ﬁt a mixture of linear regressions. (Left)
We show the observations. (Right) We show the resulting clusters after inference. The
black lines are the ground truth lines. The colored lines are cluster centers inferred
using our model. Importantly, note that we did not assume that we know that the
number of the regression functions, K, is three. Rather, this quantity was estimated as
part of our inference procedure.
we have a mixture of quadratic regressions with (p0
i , p1
i , p2
i ) being the coeﬃcients of the
ith quadratic regression:
y = p2
i x2 + p1
i x + p0.
(4.17)
We then observe a point (ai, bi) ∈R2 on this quadratic and we can set:
yi = bi ,
xi =


p0
i
p1
i
p2
i

,
hi =

1
ai
a2
i

.
(4.18)
For example, in Figure 4.5, we have points generated from the quadratics y =
x2 + 3x + 30, y = 1
2x2 + 6x and y = −x2 + 60. We use our model to ﬁt a mixture
of quadratic regressions and the results are shown in Figure 4.5. As we can see, the
inferred cluster centers ﬁt the data and are close to the ground truth quadratics.

62
CHAPTER 4.
PROJECTION DIRICHLET PROCESS GAUSSIAN MIXTURE MODEL
Figure 4.5:
An example of using our model to ﬁt a mixture of quadratic regres-
sions.(Left) We show the observations. (Right) We show the resulting clusters after
inference. The black curves are the ground truth quadratics. The colored curves are
cluster centers inferred using our model. Importantly, note that we did not assume that
we know that the number of the regression functions, K, is three. Rather, this quantity
was estimated as part of our inference procedure.
■4.4 Conclusion
In this chapter, we presented a model PDP-GMM which relaxes the assumption that
the draws from a DP-GMM are directly observed. Instead our model assumes that only
known aﬃne projections of the draws are observed. This allows us to handle a wide
range of problems. As discussed in Section 4.2, we implemented this as an extension
of our DP-GMM code from Chapter 3 making extensive use of Julia’s native multiple
dispatch feature which allowed us to reuse most of the core DP-GMM code. We then
presented results of this model applied to a number of synthetic examples and saw
that it performed well. In particular, the results suggest that this model is suitable
for tomography applications as well as ﬁtting a possibly-inﬁnite mixture of polynomial
regressions.

Chapter 5
Conclusion
In this thesis, we have focused on exploring the utility of using Julia for parallel and
distributed MCMC inference algorithms. In addition, we have also proposed a new
model which relaxes the standard DP-GMM assumption that the draws are directly
observed.
In Chapter 2, we implemented the calculation of CPAB transformations [8] in Julia
in parallel. We compared it with an existing GPU implementation given in [8] and
found it was around twice as fast as our CPU implementation.
This is a good re-
sult considering that the problem is well suited towards GPU computation and that a
CPU implementation works on virtually all machines. We then applied CPAB trans-
formations to the correspondence-based image warping problem. After trying Gradient
Descent, Metropolis’ Algorithm and Sequential Importance Resampling, we found that
Gradient Descent tends to get stuck in local minima and both Metropolis’ Algorithm
and Sequential Importance Resampling produced better results. However, the Sequen-
tial Importance Resampling approach led to better parallelization speedups.
In Chapter 3, we implemented a parallel MCMC sampler for DPMMs proposed by
Chang and Fisher III [4] in a distributed fashion in Julia. We extended their algorithm
by distributing the data and computing on suﬃcient statistics.
This allowed us to
parallelize over multiple machines. We compared our implementation with their C++
implementation and found that although our Julia implementation is generally slower
63

64
CHAPTER 5.
CONCLUSION
on a single machine, we also showed that in several settings our implementation is
the faster one and that it also handles higher dimensions more gracefully in terms
of the quality of the results. In addition, our distributed architecture also allows for
the ability to use multiple machines to speed up runtime. Using our distributed Julia
implementation, we applied the DP-GMM model to the real-data problem of image
denoising using learned image patches priors. Previous works were trained on a sample
of 2 million patches while we were able to train our DP-GMM model on all 44 million
patches using this implementation. We reported better results than using a sample
of 2 million patches and slightly better results than the original work by Zoran and
Weiss [21].
In Chapter 4, we presented a model PDP-GMM that extends the DP-GMM model
by relaxing the assumption that the draws are directly observed. Instead we assume
that only known aﬃne projections of the draws are observed. In addition, we showed
that there is a conjugate prior when we restrict covariances to be isotropic. This was
implemented on top of the DPMM code in Julia and we noted that with Julia’s native
multiple dispatch feature, we were able to add this extension with a minimal amount
of additional code. Furthermore, we saw that the model performed well when applied
to synthetic examples of tomography and ﬁtting a mixture of polynomial regressions.
■5.1 Remarks on Julia
Overall, our experience with Julia has been positive. The high level nature of the lan-
guage makes development easy and straightforward. In addition to having a syntax
similar to other technical computing languages, it does not take long to pick up the
language. Furthermore, the parallel abstractions in Julia allow for a straightforward
implementation of parallel algorithms while maintaining a granular level of control for
data storage and communication. Since it is also designed for high performance com-

Sec. 5.1.
Remarks on Julia
65
puting with performance similar to low level languages such as C/C++, Julia surfaces
as one of the top programming languages for technical computing.
Being a relatively new language, there are still a few nice-to-have features that are
currently under development. This includes support for native multithreading as well
as work stealing.
As we saw in Chapters 2 and 3, our Julia implementations were
able to be competitive with a GPU implementation as well as a C++ implementation.
Although slower than low level languages, we think that the ease of development in
Julia outweighs the performance gains achieved in low level languages such as C/C++.

66
CHAPTER 5.
CONCLUSION

Appendix A
Derivations Pertaining to
PDP-GMM
In this appendix, we derive expressions related to Chapter 4. We note that the deriva-
tions in this appendix were derived by Zheng [20]. In particular, we compute the like-
lihood, posterior parameters as well as the data likelihood of our PDP-GMM model.
■A.1 Likelihood
We show that Equation (4.10) follows from Equation (4.5) using the property that hihT
i
is symmetric:
p(y|µ, c) ∝c
DN
2
 N
Y
i=1
hihT
i
−1
2
!
exp
 
−c
2
" N
X
i=1
(yi −hiµ)T (hihT
i )−1(yi −hiµ)
#!
(A.1)
= c
DN
2 eT4 exp
 
−c
2
" N
X
i=1
yT
i (hihT
i )−1yi −µT hT
i (hihT
i )−1yi
−yT
i (hihT
i )−1hiµ + µT hT
i (hihT
i )−1hiµ
#!
(A.2)
= c
DN
2 eT4 exp

−c
2
h
eT3 −2µT eT1 + µT eT2µ
i
.
(A.3)
■A.2 Posterior Parameters
We show that the distribution on (µ, c) in Equation (4.6) is a conjugate prior of the dis-
tribution given in Equation (4.10) by calculating the posterior parameters. We calculate
the posterior of µ:
p(µ|y, c) = p(y|µ, c)p(µ|c)
p(y|c)
(A.4)
∝p(y|µ, c)p(µ|c)
(A.5)
67

68
APPENDIX A. DERIVATIONS PERTAINING TO PDP-GMM
∝c
DN
2 eT4 exp

−c
2
h
eT3 −2µT eT1 + µT eT2µ
i
· |cM|
1
2 exp

−1
2(µ −η)T cM(µ −η)

(A.6)
∝exp

−c
2
h
µT ( eT2 + M)µ −2µT ( eT1 + Mη)
i
(A.7)
∝exp

−c
2
 
µ −Mη + eT1
M + eT2
!T
(M + eT2)
 
µ −Mη + eT1
M + eT2
!

(A.8)
∝N
 
µ; Mη + eT1
M + eT2
, c(M + eT2)
!
.
(A.9)
Hence, we have a conjugate prior on µ with the posterior parameters being:
M′ = M + eT2,
(A.10)
η′ = M′−1(Mη + eT1),
(A.11)
where,
p(µ|y, c) = N(µ; M′, η′).
(A.12)
Now, we calculate the posterior of c:
p(c|y) =
p(y, µ, c)
p(y)p(µ|y, c)
(A.13)
∝p(y, µ, c)
p(µ|y, c)
(A.14)
= p(y|µ, c)p(µ|c)p(c)
p(µ|y, c)
(A.15)
∝c
DN
2 eT4 exp

−c
2
h
eT3 −2µT eT1 + µT eT2µ
i
· |cM|
1
2 exp

−1
2(µ −η)T cM(µ −η)

· βα
Γ(α)cα−1 exp(−βc)
· |cM′|−1
2 exp
1
2(µ −η′)T cM′(µ −η′)

(A.16)
∝c
DN
2 +α−1 exp

−c
2

eT3 −2µT eT1 + µT eT2µ
+ µT Mµ −2µT Mη + ηT Mη + 2β
−µT (M + eT2)µ + 2µT M′M′−1(Mη + eT1) −η′T M′η′

(A.17)

Sec. A.3.
Data Likelihood
69
= c
DN
2 +α−1 exp
 
−c
" eT3
2 + ηT Mη
2
+ β −η′T M′η′
2
#!
(A.18)
∝Gamma

c; α + DN
2 , β + 1
2
h
eT3 + ηT Mη −η′T M′η′i
.
(A.19)
Hence, we have a conjugate prior on c with the posterior parameters being:
α′ = α + DN
2 ,
(A.20)
β′ = β + 1
2

eT3 + ηT Mη −η′T M′η′
,
(A.21)
where,
p(c|y) = Gamma(c; α′, β′).
(A.22)
■A.3 Data Likelihood
We now derive the data likelihood expression given in Equation (4.11):
p(y) = p(y, µ, c)
p(µ, c|y)
(A.23)
= p(y|µ, c)p(µ|c)p(c)
p(µ|c, y)p(c|y)
(A.24)
= (2π)−dN
2 c
DN
2 eT4 exp

−c
2
h
eT3 −2µT eT1 + µT eT2µ
i
· (2π)−D
2 |cM|
1
2 exp

−1
2(µ −η)T cM(µ −η)

· βα
Γ(α)cα−1 exp(−βc)
· (2π)
D
2 |cM′|−1
2 exp
1
2(µ −η′)T cM′(µ −η′)

· Γ(α′)
β′α′ c1−α′ exp(β′c)
(A.25)
= (2π)−dN
2 eT4|M|
1
2 |M′|−1
2 βα
β′α′
Γ(α′)
Γ(α)
· exp

−c
2

eT3 −2µT eT1 + µT eT2µ
+ µT Mµ −2µT Mη + ηT Mη + 2β
−µT (M + eT2)µ + 2µT M′M′−1(Mη + eT1) −η′T M′η

70
APPENDIX A. DERIVATIONS PERTAINING TO PDP-GMM
−2β −

eT3 + ηT Mη −η′T M′η′ 
(A.26)
= (2π)−dN
2 eT4|M|
1
2 |M′|−1
2 βα
β′α′
Γ(α′)
Γ(α) .
(A.27)
Therefore, we have the following expression for the data likelihood:
p(y) = (2π)−dN
2 eT4|M|
1
2 |M′|−1
2 βα
β′α′
Γ(α′)
Γ(α) .
(A.28)

Bibliography
[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierar-
chical image segmentation. IEEE transactions on pattern analysis and machine
intelligence, 33(5):898–916, 2011.
[2] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press,
2004.
[3] L. D. Brown. Fundamentals of statistical exponential families with applications in
statistical decision theory. Lecture Notes-monograph series, 9:i–279, 1986.
[4] J. Chang and J. W. Fisher III. Parallel sampling of DP mixture models using
sub-cluster splits. In Advances in Neural Information Processing Systems, pages
620–628, 2013.
[5] T. H. Cormen. Introduction to algorithms. MIT press, 2009.
[6] S. Faria and G. Soromenho. Fitting mixtures of linear regressions. Journal of
Statistical Computation and Simulation, 80(2):201–225, 2010.
[7] T. S. Ferguson. A bayesian analysis of some nonparametric problems. The annals
of statistics, pages 209–230, 1973.
[8] O. Freifeld, S. Hauberg, K. Batmanghelich, and J. W. Fisher III. Highly-Expressive
Spaces of Well-Behaved Transformations: Keeping It Simple. In Proceedings of the
IEEE International Conference on Computer Vision, pages 2911–2919, 2015.
[9] Y. Gal and Z. Ghahramani. Pitfalls in the use of parallel inference for the dirichlet
process. In ICML, pages 208–216, 2014.
[10] D. Geman and C. Yang. Nonlinear image recovery with half-quadratic regulariza-
tion. IEEE Transactions on Image Processing, 4(7):932–946, 1995.
[11] N. J. Gordon, D. J. Salmond, and A. F. Smith. Novel approach to nonlinear/non-
gaussian bayesian state estimation. In IEE Proceedings F-Radar and Signal Pro-
cessing, volume 140, pages 107–113. IET, 1993.
71

72
BIBLIOGRAPHY
[12] M. Huang, R. Li, and S. Wang.
Nonparametric mixture of regression models.
Journal of the American Statistical Association, 108(503):929–941, 2013.
[13] M. C. Hughes and E. Sudderth. Memoized online variational inference for dirichlet
process mixture models. In Advances in Neural Information Processing Systems,
pages 1133–1141, 2013.
[14] S. Jain and R. M. Neal. A split-merge Markov chain Monte Carlo procedure for
the Dirichlet process mixture model.
Journal of Computational and Graphical
Statistics, 2012.
[15] C. Robert and G. Casella. Monte Carlo statistical methods. Springer Science &
Business Media, 2013.
[16] J. R. Shewchuk. An introduction to the conjugate gradient method without the
agonizing pain, 1994.
[17] S. D. Silvey. Statistical inference, volume 7. CRC Press, 1975.
[18] E. B. Sudderth. Graphical models for visual object recognition and tracking. PhD
thesis, Massachusetts Institute of Technology, 2006.
[19] M. Trapp. Bnp. jl: Bayesian nonparametrics in julia. 2015.
[20] S. Zheng. personal communication.
[21] D. Zoran and Y. Weiss. From learning models of natural image patches to whole
image restoration. In Computer Vision (ICCV), 2011 IEEE International Confer-
ence on, pages 479–486. IEEE, 2011.

