A ﬁrst-order optimization algorithm for statistical learning
with hierarchical sparsity structure
Dewei Zhang1, Yin Liu1, Sam Davanloo Tajbakhsh1∗
{zhang.8705,liu.6630,davanloo.1}@osu.edu
1Department of Integrated Systems Engineering
The Ohio State University
October 20, 2020
Abstract
In many statistical learning problems, it is desired that the optimal solution con-
forms to an a priori known sparsity structure represented by a directed acyclic graph.
Inducing such structures by means of convex regularizers requires nonsmooth penalty
functions that exploit group overlapping. Our study focuses on evaluating the proximal
operator of the Latent Overlapping Group lasso developed by Jacob et al. [21]. We
implemented an Alternating Direction Method of Multiplier with a sharing scheme to
solve large-scale instances of the underlying optimization problem eﬃciently. In the
absence of strong convexity, global linear convergence of the algorithm is established
using the error bound theory. More speciﬁcally, the paper contributes to establishing
primal and dual error bounds when the nonsmooth component in the objective func-
tion does not have a polyhedral epigraph. We also investigate the eﬀect of the graph
structure on the speed of convergence of the algorithm. Detailed numerical simula-
tion studies over diﬀerent graph structures supporting the proposed algorithm and two
applications in learning are provided.
Keywords— Proximal methods, error bound theory, Alternating Direction Method of Multi-
pliers, hierarchical sparsity structure, latent overlapping group lasso.
1
Introduction
Convex sparsity-inducing regularization functions play an important role in diﬀerent ﬁelds including
machine learning, statistics, and signal processing [17]. Some well-known regularizers e.g. lasso [43]
or group lasso [52] are commonly used in diﬀerent learning frameworks to induce sparsity which
allows simultaneous model ﬁtting and feature selection. In contrast to lasso which assumes no a
priori knowledge on sparsity pattern, group lasso assumes that variables belong to a priori known
∗Corresponding author
1
arXiv:2001.03322v3  [math.OC]  18 Oct 2020

groups and the variables within a group tend to aﬀect response similarly, i.e., all are simultane-
ously zero or nonzero. This introduced more elaborate forms of zero/nonzero patterns, known as
structured sparsity, to the literature [2]. The focus of this paper is to address structured sparsities
represented by a Directed Acyclic Graph (DAG) and the convex Latent Overlapping Group (LOG)
lasso regularizer of [21] to induce such structure. To be more speciﬁc, we develop an optimiza-
tion framework that allows incorporating this regularizer in large-scale learning problems for huge
DAGs. In the remainder of this section, we discuss hierarchical structured sparsity following a
DAG, convex regularizers to induce hierarchical sparsity structures, and the proximal mapping of
the LOG lasso regularizer.
1.1
Hierarchical structured sparsity
Let D = (S, E) be a DAG where S = {s1, ..., sN} is the index set of nodes, and E is the set of
ordered pairs of node indices with an edge from the ﬁrst to second element, e.g. (si, si′) is an edge
from si to si′. Furthermore, let each node i of the graph contains a set of di variables where their
indices are contained in si. We will refer to the variables in node i by βsi.
The graph structure contains the sparsity structure between groups of variables in each node.
For instance, consider the following two DAGs and assume the variables in a single node will be all
s1 
s2 
(a) A DAG with two nodes
s1 
s2 
s3 
s4 
(b) A DAG with four nodes
Figure 1: Two Directed Acyclic Graphs (DAG)
simultaneously non-zero or all zero. The sparsity hierarchy introduced by the graph in Figure 1a
is (s1 = 0) ⇒(s2 = 0) and (s2 ̸= 0) ⇒(s1 ̸= 0). Note that sj = 0 means that all of the variables in
sj are equal to zero; similarly, (sj ̸= 0) means all of the variables in sj are nonzero. However, there
are scenarios where a node has more than one ancestor, e.g. node s3 in Figure 1b. Such scenarios
can potentially be interpreted in two diﬀerent ways. Under strong hierarchy assumption, all of
the immediate ancestor nodes need to be nonzero for their descendent node to be nonzero, e.g., in
Figure 1b, (s3 ̸= 0) ⇒(s1 ̸= 0, s2 ̸= 0), and (s1 = 0 or s2 = 0) ⇒(s3 = 0). Under weak hierarchy
assumption, for a descendent node to be nonzero it suﬃces that any of its immediate ancestors be
nonzero, e.g., in Figure 1b, (s3 ̸= 0) ⇒(s1 ̸= 0 or s2 ̸= 0), and (s1 = 0, s2 = 0) ⇒(s3 = 0) [10].
We are interested in statistical learning problems that require their solutions to follow given
sparsity structures in form of DAGs. To be more speciﬁc, given a DAG D, there is a learning
problem of the form
min
β

L(β)
s.t.
β ∈B, supp(β) ∈D
	
(1)
where L : Rd →R is a smooth, convex or nonconvex loss function with d = PN
i=1 di being the
problem dimension, B ⊆Rd is a closed set, and with a slight abuse of notation, supp(β) ∈D
denotes that the support of β (index set of its nonzero elements) follows D in the strong sense.
2

One way to formulate supp(β) ∈D, explicitly, is by introducing binary variables. For instance,
assuming one variable per node, formulating the hierarchy in Figure 1a shall be performed as
zϵ ≤|β1|,
|β2| ≤zµ,
z ∈{0, 1},
where ϵ and µ are reasonably small and large numbers, respectively. Introduction of binary variables
makes the optimization problem a Mixed Integer Program (MIP) [50] - see also [3, 4]. Finding the
global optimal solution of large-scale MIPs for large DAGs is generally computationally challenging.
We, however, would like to note some signiﬁcant advances in MIP algorithms for statistical learning,
speciﬁcally for feature selection – see e.g.
[8, 29, 1, 30, 7, 9].
Similar to using ℓ1 norm as a
convex approximation to ℓ0 (pseudo) norm to induce sparsity, there are convex regularizers that
promote hierarchical sparsity structures. Needless to mention, these approximation methods do
not guarantee exact conformance of their solutions to given hierarchies, but, they allow solving
high-dimensional problems.
1.2
Group Lasso with overlaps vs. Latent Overlapping Group lasso
There are mainly two convex regularizers to introduce hierarchical structured sparsity: 1. Group
Lasso (GL) 2. Latent Overlapping Group Lasso (LOG) [51]. Given a set of groups of variables G,
the GL regularizer is deﬁned as
ΩGL(β) =
X
g∈G
wg∥βg∥
(2)
where wg is a positive weight corresponding to group g and βg ∈R|g| is equal to β for ele-
ments whose indices belongs to g and zero for other elements, and the ∥· ∥is either an ℓ2 or
ℓ∞norm. To induce hierarchical sparsity structure using the GL penalty, the groups should be
deﬁned in a descendants form, for instance, for the graph in Figure 1b the groups should be
G = {s3, s4, {s1, s3}, {s2, s3, s4}} where s3 = descendants(D; s3), {s1, s3} = descendants(D; s1),
s4 = descendants(D; s4), and {s2, s3, s4} = descendants(D; s2).
The group lasso sets to zero a
union of a subset of groups introduced in G. However, since there are overlaps between the groups
deﬁned in G, the support of the solution induced by GL is not necessarily a union of the groups.
This is because of the fact that the complement of a union of a subset of groups is not necessarily
a union of groups.
As an alternative to GL, Jacob et al. [21] introduced LOG regularizer which is deﬁned as
ΩLOG(β) =
inf
ν(g), g∈G



X
g∈G
wg∥ν(g)∥2 s.t.
X
g∈G
ν(g) = β, ν(g)
gc = 0



(3)
which sets to zero a subset of groups. Since β is the sum of latent variables ν(g) ∈Rd, its support
is the union of the groups of nonzero latent variables. Given a DAG D with N nodes, there exist
N groups in G (i.e., N = |G|). To induce a hierarchical sparsity using the LOG penalty the group
corresponding to each node contains the node indices of all its ancestors, i.e., G = ancestors(D).
For instance, the group set for the graph in Figure 1b is G = {s1, s2, {s1, s2, s3}, {s2, s4}} where s1 =
ancestors(D; s1), s2 = ancestors(D; s2), {s1, s2, s3} = ancestors(D; s3), and {s2, s4} = ancestors(D; s4).
Figure 2a shows a simple tree with three nodes, and the ancestor grouping scheme; Figure 2b shows
the latent variables within the constraint in the LOG penalty. Recently, Yan et al. [51] performed
a detailed comparison of GL vs. LOG regularizers. They showed that compared to LOG, GL sets
3

1 
2 
3 
(a) A tree with three nodes. Red dashed
lines show the groups.
ν1{1}
0
ν1{1,2}
ν2{1,2}
ν1{1,3}
ν3{1,3}
0
0
0
β2
β3
β1
(b) The constraint within the LOG penalty cor-
responding to the tree in 2a
Figure 2: LOG penalty and the required groups to induce a tree structure
to zero parameters which are deeper in the hierarchy. Hence, for DAGs with deep hierarchies it is
very probable that GL sets to zero deeper variables which is undesirable. Furthermore, with LOG
penalty, one has control over the solution support as it is a subset of columns of latent variables.
In the next section, we discuss solving statistical learning problems in the regularized form using
the nonsmooth LOG penalty and propose solving them using proximal methods.
1.3
Proximal operator of the LOG penalty
Given a hierarchical sparsity structure represented by a graph D, an approximate convex optimiza-
tion problem to (1) is
min
β

L(β) + λΩLOG(β)
s.t.
β ∈B
	
(4)
where ΩLOG(.) is the LOG penalty introduced in (3) with appropriately chosen groups, and λ > 0 is
a parameter that controls the tradeoﬀbetween the loss function and the penalty. Indeed, problem
(4) is a convex nonsmooth program; hence, proximal methods are suitable to solve large instances
of this problem [31, 5, 34].
Similar to gradient methods that require iterative evaluation of the gradient, proximal methods
require iterative evaluation of the proximal operator [34]. The proximal operator of a function
λΩ(β) in general (λΩLOG(β) in this case) evaluated at b ∈Rd is deﬁned as
proxλΩ(b) ≜argmin
β∈Rd

λΩLOG(β) + 1
2∥β −b∥2
2

.
(5)
Using the deﬁnition of ΩLOG, evaluating the proximal operator of the LOG penalty requires solving
min
ν(g)∈Rd


λ
X
g∈G
wg∥ν(g)∥2 + 1
2∥
X
g∈G
ν(g) −b∥2
2 s.t. ν(g)
gc = 0, ∀g ∈G


.
(6)
over the latent variables ν(g), g ∈G. A classical method to solve (6) is some implementation of
the proximal Block Coordinate Descent (BCD) algorithm [48] – see e.g. Algorithm 1. Provided
an algorithm to evaluate proxλΩLOG eﬃciently, one may use a proximal optimization method e.g.
proximal gradient method [5, 31, 34] to solve (4). The main challenge is to evaluate the proximal
operator of the LOG penalty (5) for large DAGs with large |G|. The main drawbacks of the BCD
4

Algorithm 1 Block Coordinate Descent (BCD) to solve proxλΩLOG(b)
Require: b, λ, w, G
1: β = 0
2: ν(g) = 0, ∀g ∈G
3: while stopping criterion not met do
4:
for g ∈G do
5:
β ←β −ν(g)
6:
ν(g) ←SG(bg −βg, λwg) ≜(bg −βg) max{1 −
λwg
∥bg−βg∥, 0},
7:
β ←β + ν(g)
8:
end for
9: end while
Output: β
algorithm to solve (5) are as follows. First, even though convergence of the BCD algorithm for
nondiﬀrentiable but separable functions has been established [44], to the best of our knowledge,
the convergence rate of the algorithm for nonsmooth optimization is sublinear [46, 37]. Second,
the BCD algorithm follows a Gauss-Siedel update rule and hence it cannot be parallelized. In this
work, we introduce an eﬃcient ﬁrst-order method, based on Douglas-Rachford operator splitting,
that can solve (5) over large graphs with fast, i.e., linear, rate of convergence.
Remark 1. The intent of this work is to propose an eﬃcient optimization algorithm to solve
the proximal mapping of the LOG penalty (5). The resulting standalone algorithm can then be
embedded in any prox-based optimization algorithm to solve (1) for convex or nonconvex settings.
Hence, solving (1) is not the purpose of our paper which is the reason why we do not specify any
structures on L(·) or B, e.g., convexity or etc.
1.4
Contributions
We propose an ADMM algorithm with a sharing scheme to solve proxΩLOG deﬁned over large DAGs.
The underlying DAG may have any general structure (e.g. not necessary to be a path graph) and
the algorithm is guaranteed to converge to its optimal solution. Furthermore, the computationally
challenging subproblem of the algorithm (step 4 in Algorithm 3) can be run fully in parallel.
We proved linear convergence of the algorithm given a suﬃciently small stepsize in the absence
of strong convexity. Establishing the linear convergence rate is based on the error bound theory
(see e.g.[45, 25, 53, 20]), and our contributions are as follows:
1. The dual error bound is established in the presence of ℓ2-norm in the nonsmooth component
of the objective function. A common key assumption in previous works requires the nonsmooth
component to have a polyhedral epigraph. Our proof shows an approach to escape such assumption
and enables further extension.
2. On the primal side, we rigorously prove the error bound for the augmented Lagrangian
function. The main challenges are the presence of the dual variable y and the splitting of x into
two blocks , i.e., x1 and x2, which requires a number of technical details, e.g., comparing the limiting
behavior of x1,k and x2,k. To the best of our knowledge, this is the ﬁrst work showing error bound
5

for augmented Lagrangian function with block structured variables.
3. As an auxiliary result needed to show the linear rate of convergence of the algorithm, and
by using the proof in [15], we formally show the uniform boundedness (with respect to the stepsize)
of both primal and dual updates generated by the algorithm.
4. We looked into the eﬀect of the graph structure on the O(1) constant of the convergence
rate; furthermore, we performed detailed numerical experiments on six diﬀerent graphs comparing
the proposed method against ﬁve other state-of-the-art optimization techniques for this problem.
1.5
Related work
As discussed above, hierarchical sparsity structures are generally enforced by either introducing
constraints to the underlying optimization problem or adding regularizers to its objective function.
We discuss the relevant work in the later category with convex regularizers which are directly related
to our work.
Zhao & Yu [54] proposed composite absolute penalty to express grouping and hierarchical
structures and used a stage-wise lasso algorithm to approximate the regularization path for convex
problems. Radchenko & James [36] proposed an adaptive nonlinear interaction structure for least-
square regression with two-way interaction for high-dimensional problems. Schmidt & Murphy [39]
enforced the hierarchical constraints using grouped ℓ1 regularization with overlapping groups for log-
linear models of any order and proposed an active set method to solve the underlying problem. Haris
et al. [16] proposed a general framework for variable selection problems regularized with group lasso
with overlaps and proposed to solve the underlying problem by the ADMM algorithm. Jenatton
et al. [23] explored the relationship between the groups deﬁning the norm and the resulting nonzero
patterns and provided forward and backward algorithms to go back and forth between groups and
patterns. Furthermore, Jenatton et al. [22] considered ﬁnding the proximal mapping of the group
lasso penalty through its dual problem and showed that a BCD algorithm solves the dual in one
pass for tree graphs. Mairal et al. [27] showed that the proximal operator of the overlapping group
lasso under ℓ∞norm can be computed in polynomial time by solving a quadratic min cost-ﬂow
problem and used proximal splitting to solve the problem in higher dimensions. She et al. [40]
considered group lasso with overlap penalty and provide the minimax lower bounds for strong and
weak hierarchical models and showed their proposed estimators enjoy sharp rate oracle inequalities.
Hazimeh & Mazumder [18] proposed a scalable algorithm based on proximal gradient descent to
solve the underlying problem. Their method enjoys a proximal screening step that identiﬁes many
of the zero variables and groups and also allows to solve the problem in parallel; furthermore, they
proposed an eﬃcient active set method based on gradient screening.
Jacob et al. [21], ﬁrst, proposed the LOG penalty and studied its theoretical properties - see also
Obozinski et al. [32] for theoretical discussions on choice of the weights. Villa et al. [48] proposed
accelerated proximal method and proved the convergence of the overall learning problem for the
least-square loss function. Furthermore, they developed an active set method to compute the inner
proximal mapping relatively fast. Lim & Hastie [24] imposed strong hierarchy through a constrained
optimization problem for which they found an equivalent unconstrained problem. Chouldechova
& Hastie [12] incorporated a LOG-like penalty to ﬁt generalized additive models. Finally, Yan
et al. [51] compared statistical properties of GL and LOG lasso penalties. They also proposed a
ﬁnite-step algorithm to compute the proximal operator of the LOG penalty for path graphs and
extended it to general DAGs with an ADMM framework. However, their extension for general
DAGs (1) highly depends on how DAG is decomposed into diﬀerent path graphs, and (2) it lacks
6

theoretical convergence and convergence rate analysis. Compared to their method, we formulated
the proximal map for the original DAG directly into an ADMM framework with a sharing scheme
and established global linear convergence of the algorithm. We compare the convergence time of
our algorithm with their path-based ADMM in Section 4.1.
Notation. Vectors are denoted by lowercase bold letter while matrices are denoted by uppercase
letters. The identity matrix is denoted by I. Let G be a set, then its cardinality is denoted by |G|.
Given a vector β ∈Rd and g ⊆{1, ..., d}, βg ∈R|g| subsets β over the set g. We denote the j-th
column of the matrix A by A.j, similarly we denote its i-th row by Ai.. Furthermore, similar to the
vector case, if g ⊆{1, ..., n}, then A.g ∈Rm×|g| subsets A over the columns indexed by g. Inner
product of two vectors is deﬁned as ⟨a, b⟩= a⊤b.
The rest of the paper is organized as follows. In Section 2, we propose an ADMM algorithm
with the sharing scheme to evaluate the proximal operator of the LOG penalty. Section 3 provides
detailed convergence analysis of the proposed algorithm while proofs are relegated to the Appendix.
Section 4 provides some numerical simulation studies conﬁrming our theoretical complexity bound
and compares the ADMM algorithm with other methods. Furthermore, Section 4 contains two
applications that use LOG penalty to induce sparsity structure related to topic modeling and
breast cancer classiﬁcation. Finally, Section 5 provides some concluding remarks.
2
Evaluating the proximal operator of the LOG
Consider the optimization problem (6) to ﬁnd the proximal operator of the LOG penalty. Given a
general DAG D, each iteration of the BCD algorithm requires updating ν(g), ∀g ∈G. If |G| is a
large number, then per iteration complexity of the algorithm O(P
g∈G |g|) is costly. Furthermore,
to the best of authors knowledge, convergence rates of the BCD algorithm for general nonsmooth
optimization problems have not been well studied.
However, convergence of the algorithm for
problems where the nondiﬀrentiable part is separable is established, cf. [44].
In this paper, we develop an Alternating Direction Method of Multiplier (ADMM) to solve the
proximal operator of the LOG penalty. The algorithm is parallelizable which makes it suitable
when the number of groups is very large. Deﬁne x = [ν(g)
g ]g∈G ∈Rn, n = P
g∈G |g|, be a long
vector that contains the nonzero elements of ν(g), g ∈G, for some random order P of the groups.
Furthermore, let j(·) : G →{1, ..., n} be the set map that associates a group g ∈G to its indices in
vector x given an order of the groups. For instance, for G = {{1}, {1, 2}, {1, 3}} ordered as P from
left-to-right: 1. n = 5, 2. jP({1}) = {1}, jP({1, 2}) = {2, 3}, and jP({1, 3}) = {4, 5}. To simplify
the notation, the group ordering P is omitted. Finally, (6) can equivalently be written as
min
x∈Rn λ
X
g∈G
∥W gx∥2 + 1
2∥Mx −b∥2
2,
(7)
where W g = wgU j(g), U j(g) ∈R|g|×n such that [U j(g)⊤]g∈G = I ∈Rn×n, and M ∈Bd×n sums
elements of x along each coordinate. Or, equivalently, (6) can be written as
min
x∈Rn f(x) ≜λ
X
g∈G
wg∥xj(g)∥2 + 1
2∥Mx −b∥2
2.
(8)
7

Problem (8) is a convex (not strongly convex since M is not a full-column rank matrix), nonsmooth
optimization program. This problem can also be solved using the proximal gradient method [53, 45].
We propose to solve this problem using the Alternating Direction Method of Multipliers (ADMM).
First, splitting the problem into two blocks [11], we have
min
x1,x2∈Rn


F(x1, x2) ≜λ
X
g∈G
wg∥x1
j(g)∥2 + 1
2∥Mx2 −b∥2
2,
s.t. x1 = x2


.
(9)
The augmented Lagrangian function for (9) is
Lρ(x1, x2; y) = λ
X
g∈G
wg∥x1
j(g)∥2 + 1
2∥Mx2 −b∥2
2 +

y, x1 −x2
+ ρ
2∥x1 −x2∥2
2,
(10)
where y ∈Rn is the Lagrange multiplier for the linear constraint x1 = x2, and ρ ≥0 is a constant.
Furthermore, the augmented dual function is given by
gρ(y) =
min
x1,x2∈Rn λ
X
g∈G
wg∥x1
j(g)∥2 + 1
2∥Mx2 −b∥2
2 +

y, x1 −x2
+ ρ
2∥x1 −x2∥2
2,
(11)
which results into the the dual problem
max
y∈Rn
gρ(y).
(12)
The ADMM iterates in the unscaled form [11] are
x1,k+1
j(g)
←argmin
x1
j(g)∈R|g| λwg∥x1
j(g)∥2 + ρ
2∥x1
j(g) −x2,k
j(g) + 1
ρyk
j(g)∥2
2,
∀g ∈G,
(13)
x2,k+1 ←argmin
x2∈Rn
1
2∥Mx2 −b∥2
2 + ρ
2∥x2 −x1,k+1 −1
ρyk∥2
2,
(14)
yk+1
j(g) ←yk
j(g) + α(x1,k+1
j(g)
−x2,k+1
j(g) ),
∀g ∈G,
(15)
where α is the dual stepsize. Algorithm 2 illustrates the resulting (unscaled) ADMM algorithm to
evaluate the proximal map of the LOG penalty. Note that the subproblem (13) is parallelizable
across groups, and the solution to each subproblem is available in the closed from. However, even
though the update (14) has a closed-form solution, it involves inverting an n × n matrix which
generally requires O(n3) operations, per iteration. Hence, for large DAGs where |G| and hence n is
large, the second update is very slow.
To deal with this issue, below we propose a sharing scheme that helps solving the second
subproblem eﬃciently. First, we put the variables in the matrix form. Deﬁne X ∈Rd×|G| be a
matrix that stacks ν(g), g ∈G where its columns are indexed by g ∈G. Problem (6) can be written
in the matrix form as
min
X∈Rd×|G|


λ
X
g∈G
wg∥X.g∥2 + 1
2∥
X
g∈G
X.g −b∥2
2, s.t. (X.g)gc = 0 ∀g ∈G


.
(16)
8

Algorithm 2 ADMM to solve proxλΩLOG(b) in the unscaled form
Require: b, λ, α, wg ∀g ∈G, j(.) : G →[n]
1: k = 0, y0 = 0, x2,0 = 0
2: while stopping criterion not met do
3:
k ←k + 1
4:
x1,k+1
j(g)
←proxλwg∥·∥2(x2,k
j(g) −1
ρyk
j(g)),
∀g ∈G
5:
x2,k+1 ←(M⊤M + ρI)−1(M⊤b + ρx1,k+1 + yk)
6:
yk+1
j(g) ←yk
j(g) + α(x1,k+1
j(g)
−x2,k+1
j(g) ),
∀g ∈G
7: end while
8: β = Mx1,k
Output: β
Splitting the problem into two blocks, the problem is equivalent to
min
X1,X2∈Rd×|G|


λ
X
g∈G
wg∥X1
.g∥2 + 1
2∥
X
g∈G
X2
.g −b∥2
2, s.t. X1 = X2, (X1
.g)gc = 0 ∀g ∈G


.
(17)
The ADMM iterates in the scaled form (through deﬁning U := (1/ρ)Y where Y is the dual variable
in the matrix form - see [11] for details) to solve (17) are
X1,k+1
.g
←argmin
X1.g∈Rd
n
λwg∥X1
.g∥2 + ρ
2∥X1
.g −X2,k
.g
+ U k
.g∥2
2 s.t. (X1
.g)gc = 0
o
, ∀g ∈G,
(18)
X2,k+1 ←argmin
X2∈Rd×|G|
1
2∥
X
g∈G
X2
.g −b∥2
2 + ρ
2
X
g∈G
∥X2
.g −X1,k+1
.g
−U k
.g∥2
2,
(19)
U k+1
.g
←U k
.g + (α/ρ)(X1,k+1
.g
−X2,k+1
.g
), ∀g ∈G,
(20)
Similar to the vector form, the solution to subproblem (18) is provided by the proximal map of
the ℓ2-norm and can be parallelized across groups, see step 4 in Algorithm 3. Subproblem (19)
is potentially a large problem in d|G| variables for a large DAG (equivalent to the second update
in the above algorithm in the vector form); however, it is possible to decrease its size to only d
variables. Subproblem (19) is equivalent to
min
X2∈Rd×|G|, ¯x2∈Rd



1
2∥|G|¯x2 −b∥2
2 + ρ
2
X
g∈G
∥X2
.g −X1,k+1
.g
−U k
.g∥2
2 s.t. ¯x2 = (1/|G|)
X
g∈G
X2
.g


.
(21)
Minimizing over X2
.g with ¯x2 ﬁxed and using optimality conditions, we get
X2
.g = ¯x2 + X1,k+1
.g
+ U k
.g −(1/|G|)
X
g∈G
(X1,k+1
.g
+ U k
.g), ∀g ∈G.
(22)
Using (22) to solve (19) we get
¯x2 =
1
|G| + ρ

b + ρ
|G|
X
g∈G
(X1,k+1
.g
+ U k
.g)

.
(23)
9

Furthermore, using (22) in (20), we get
U k+1
.g
= U k
.g + (α/ρ)
  1
|G|
X
g∈G
(X1,k+1
.g
+ U k
.g) −¯x2 −U k
.g

, ∀g ∈G.
(24)
The sharing implementation of the proposed ADMM algorithm is illustrated in Algorithm 3.
Algorithm 3 ADMM to solve proxλΩLOG(b) in the scaled form with the sharing scheme
Require: b, λ, α, wg ∀g ∈G
1: k = 0, ¯u0 = 0, ¯x2,0 = 0
2: while stopping criterion not met do
3:
k ←k + 1
4:
X1,k+1
gg
←proxλwg∥·∥2(X1,k
gg + ¯x2,k
g
−¯uk
g −¯x1,k
g ),
∀g ∈G
5:
X1,k+1
gcg
←0,
∀g ∈G
6:
¯x1,k+1 ←
1
|G|
P
g∈G X1,k+1
.g
7:
¯x2,k+1 ←
1
|G|+ρ
 b + ρ(¯x1,k+1 + ¯uk)

8:
¯uk+1 = ¯uk + (α/ρ)
 ¯x1,k+1 −¯x2,k+1
.
9: end while
10: β = P
g∈G X1,k+1
.g
Output: β
3
Convergence analysis
Eckstein & Yao [14] showed the iterates generated by two-block ADMM converges to some limiting
points under certain conditions. Our setting follows their proposition. It is also straightforward to
show such limiting points are optimal solutions. In this section, we establish linear convergence rate
of the ADMM algorithm to solve the proximal operator of the LOG penalty (8) given a suﬃciently
small stepsize using the error bound theory.
3.1
Rate of convergence
Note that the objective function of (8) is not strongly convex since M is not full column rank. To
establish the linear convergence rate, we will use the error bound theory which is well-established
for primal methods – see [45] and references therein. For a dual method, one needs to show that
both primal and dual error bounds hold for the problem under investigation. As mentioned in the
contributions, showing the dual error bound in the presence of ℓ2-norm in the objective function
(which results in a second-order cone epigraph) is not trivial. Furthermore, in the absence of a
bounded feasible region, boundedness of the iterates needs to be established. Finally, establishing
primal error bound in the presence of the dual variable and under 2-block splitting is elaborate. All
of these challenges are addressed in this section.
Let X∗⊆R2n and Y∗⊆Rn denote the primal and dual optimal solution sets to (9) and (12),
respectively. Let X(y) ⊆R2n denote the optimal solution set to the problem of minimizing the
10

augmented Lagrangian function (11) given y ∈Rn. Note the augmented Lagrangian (10) is strongly
convex in x1 or x2, but not jointly strongly convex. We use x(y) = (x1⊤(y), x2⊤(y))⊤∈X(y) to
represent a minimizer of (10) given y. Let E ≜[I, −I] ∈Rn×2n and M as in (8), and deﬁne
ℓ(x1, x2) ≜φb(Mx2) + ψρ(Ex)
(25)
where the functions φb :
Rn →R and ψρ :
Rn →R are deﬁned as φb(z) ≜
1
2∥z −b∥2
2 and
ψρ(z) ≜
ρ
2∥z∥2
2.
For the simplicity of notation, the subscripts b and ρ are eliminated in the
remainder of the manuscript. The following two properties are used in the subsequent analysis:
∥∇x2φ(Mx2(y)) −∇x2φ(Mx2(¯y))∥2 = ∥M ⊤M(x2(y) −x2(¯y))∥2 ≤Lφ∥Mx2(y) −Mx2(¯y)∥2,
(26)
∥∇xψ(Ex(y)) −∇xψ(Ex(¯y))∥2 = ρ∥E⊤E(x(y) −x(¯y))∥2 ≤Lψ∥Ex(y) −Ex(¯y)∥2,
(27)
where Lφ = ∥M T ∥2 and Lψ = ρ
√
2.
Given y ∈Rn, the optimization problem in (11) can equivalently be written as
min
x1∈Rn,x2∈Rn,s∈R|G|λ
X
g∈G
sg + 1
2∥Mx2 −b∥2
2 +

y, x1 −x2
+ ρ
2∥x1 −x2∥2
2,
s.t. wg∥x1
j(g)∥2 ≤sg,
∀g ∈G.
(28)
The constraints in (28) are indeed second-order cones Qg = {(xg, sg) ∈R|G| ×R+ : wg∥xg∥2 ≤sg}.
The KKT system for the problem (28) is
yj(g) −wgµg + ρ(x1
j(g) −x2
j(g)) = 0
∀g ∈G,
(29a)
M ⊤(Mx2 −b) + ρ(x2 −x1) −y = 0,
(29b)
λ −νg = 0
∀g ∈G,
(29c)
wg∥x1
j(g)∥2 ≤sg
∀g ∈G,
(29d)
∥µg∥2 ≤νg
∀g ∈G,
(29e)
µ⊤
g x1
j(g) + sg
wg
νg = 0
∀g ∈G,
(29f)
where (µg, νg) ∈R|G|×R+ is the dual variable for the conic constraint. Hence, a pair
 (x1, x2, s), (µ, ν)

is an optimal primal-dual pair if it satisﬁes (29). Using the KKT conditions, ﬁrst we provide the
dual error bound in Lemma 3.1.
Lemma 3.1. There exists τd > 0 such that
dist(y, Y ∗) ≤τd∥∇gρ(y)∥2,
(30)
where gρ(y) is the augmented dual function deﬁned in (11).
Proof. Check Appendix A.
From the proof of Lemma 3.1, we see that τd is indeed a xfunction of ρ and M. Next, in
Lemma 3.2 below, we show the existence of a ﬁnite saddle point to the augmented Lagrangian
function and that the sequence generated by the algorithm is uniformly bounded.
11

Lemma 3.2. Given the existence of a ﬁnite saddle point to the augmented Lagrangian function
(10), for any ρ and α such that 0 < α < ρ, the sequence {x1,k}, {x2,k} and {yk} generated by the
algorithm (13)-(15) is uniformly bounded.
Proof. Check Appendix B.
We also need to establish the primal error bound. Unlike the dual error bound, where the
gradient of the augmented gρ(y) nicely bounds the “error”, i.e., the distance of a point to the
optimal solution set, the primal function is not smooth and diﬀerentiable. Quantifying the error
bound for nonsmooth functions is generally performed by the proximal gradient. For general surveys
on error bounds, see [33, 55] and references therein.
Deﬁnition 3.1. Assume a convex function f is decomposable as f(x) = g(Ax) + h(x), where g is
a strongly convex and diﬀerentiable function and h is a convex (possibly nonsmooth) function, then
we can deﬁne the proximal gradient of f with respect to h as
˜∇f(x) := x −proxh(x −∇(f(x) −h(x))) = x −proxh
 x −A⊤∇g(Ax)

If h = 0 then the proximal gradient ˜∇f(x) is equal to the gradient ∇f(x). In general, ˜∇f(x)
can be used as the (extended) gradient for nonsmooth minimization minx∈Rn f(x). For instance, we
have ˜∇f (x∗) = 0 if and only if x∗is a minimizer. For the Lagrangian function (10), the proximal
gradient w.r.t. x = (x1, x2) is deﬁned as
˜∇xLρ(x1, x2; y) := x−proxλ P
g∈G wg∥x1
j(g)∥2

x −∇x(1
2∥Mx2 −b∥2
2 +

y, x1 −x2
+ ρ
2∥x1 −x2∥2
2)

,
which we split into ˜∇x1Lρ and ˜∇x2Lρ in the proof of Lemma 3.3.
Lemma 3.3. Assume that (x1, x2; y) is in a compact set, then there exist 0 < τp < +∞and δ > 0
such that
dist(x, X(y)) ≤τp∥˜∇xLρ(x1, x2; y)∥2,
(31)
for all (x1, x2; y) such that ∥˜∇xLρ(x1, x2; y)∥2 ≤δ. Furthermore, τp and δ are independent of y.
Proof. Check Appendix C.
Remark 2. In Lemma 3.3, the condition ∥˜∇xLρ(x1, x2; y)∥2 ≤δ can be relaxed.
Note that
dist(x,X(y))
∥˜∇xLρ(x1,x2;y)∥2 is a continuous and well-deﬁned function for all (x1, x2; y) such that ∥˜∇xLρ(x1, x2; y)∥2
≥δ. From the uniform boundedness of the sequence in Lemma 3.2, it implies that there exists an
upper bound τ such that
dist(x,X(y))
∥˜∇xLρ(x1,x2;y)∥2 ≤τ, for all (x1, x2; y) where ∥˜∇xLρ(x1, x2; y)∥2 ≥δ.
Choosing τp to be the maximum of τp (in (31)) and τ, we have dist(x, X(y)) ≤τp∥˜∇xLρ(x1, x2; y)∥2
for all (x1, x2; y).
Theorem 3.1. Let {(xk, yk)} be the sequence generated by Algorithm 3 with stepsize α ≤
ρ
2τ 2
pσ2 ,
where ρ is the augmented Lagrangian parameter, τp is the primal error bound parameter and σ is
deﬁned in the proof. Furthermore, let ∆k
p = Lρ(xk+1; yk) −gρ(yk) and ∆k
d = g∗
ρ −gρ(yk) be the
primal and dual optimality gaps at the k-th iteration, respectively. Then, we have
[∆k
p + ∆k
d] ≤(
1
λ + 1)k[∆0
p + ∆0
d],
12

where λ = min{
ρ−2ατ 2
pσ2
ζ+ζ′τ 2
pσ2 , α
τ ′ } > 0, and σ, ζ, ζ′, τ ′ are deﬁned in the proof, which shows that the
sequence [∆k
p + ∆k
d] converges to zero Q-linearly1.
Proof. Check Appendix E.
Remark 3. Theorem 3.1 proves that with the stepsize small enough such that α ≤0.5ρτ −2
p σ−2, we
have ∆k
p + ∆k
d ≤ϵ after k ≥(log(1 + λ))−1 log(
∆0
p+∆0
d
ϵ
) iterates, where λ is deﬁned in the statement
of the theorem, i.e., an ϵ-optimal solution is obtained in k ≥O(log(1/ϵ)) iterates.
Remark 4. If the stepsize α is small enough such that α ≤0.5ρτ −2
p σ−2 and
ρ−2ατ 2
pσ2
ζ+ζ′τ 2
pσ2 > α
τ ′ , then
λ = α/τ ′ = αρ/τ 2
d. Hence, the O(1) constant in the convergence rate of the algorithm would be
(log(1 + αρ/τ 2
d))−1. Note that this scenario happens if the stepsize α is small enough. In this
scenario, since τd = max{∥M ⊤∥2, 2ρ} (see the proof of Theorem 3.1), if ρ < (1/2)∥M ⊤∥2, then the
O(1) constant is (log(1 + αρ/∥M ⊤∥4))−1 which is an increasing function of ∥M ⊤∥. Furthermore,
since ∥M∥F /d ≤∥M ⊤∥and given the binary structure of M, ∥M∥F = m where m is the number
of nonzero elements of M. Hence, the O(1) constant is larger (i.e, worst-case convergence is slower
but still linear) when m is larger. This mainly happens for longer DAGs compared to wider ones
(given equal number of nodes), i.e., those with more ancestry structures.
Furthermore, in the
same scenario, smaller ρ makes the constant larger, and the convergence slower.
Otherwise, if
ρ ≥(1/2)∥M ⊤∥2, then the O(1) constant is (log(1 + α/(4ρ)))−1 which gets larger, i.e. convergence
is slower, for larger values of ρ. Finally, in both scenarios, bigger stepsize α (up to the linear rate
upper bound of 0.5ρτ −2
p σ−2) makes the constant smaller and, the worst-case convergence faster.
4
Numerical experiments
4.1
Simulation studies
This section provides our numerical studies on the performance of the proposed algorithm to eval-
uate the proximal operator of the LOG penalty. We compare the convergence rate of the proposed
ADMM algorithm with the sharing scheme, i.e. Algorithm 3, with ﬁve other other algorithms in-
cluding the Cyclic Block Coordinate Descent (C-BCD), shown in Algorithm 1, and its randomized
version (R-BCD) (see [38]), Proximal Gradient Descent (PGM) with backtracking (which is the
ISTA algorithm in [5]), Accelerated PGM (ACC-PGM) with backtracking (which is the FISTA
algorithm in [5]), and Hierarchical Sparse Modeling (HSM) by Yan et al. [51] to ﬁnd the proximal
mapping of the LOG penalty for the six DAGs shown in Figure 3 on simulated data.
From the six diﬀerent graphs shown in Figure 3, four DAGs are indeed tree graphs with diﬀerent
structures with 101, 101, 127, 201 nodes (DAGs (a)-(c) and DAG (e)); one reverse binary tree (DAG
(d)), and one random DAG with 100 nodes and 98 edges (DAG (e)) are also considered in the study.
Note that each node represents a single parameter, i.e. d = N.
In each simulation b ∈Rd is sampled from N(0, Id). Considering problem (8), the parameters
are set as λ = 0.1 and wg = |g|1/2. The step sizes of the PGM and ACC-PGM methods are selected
by backtracking. The ρ parameter in the ADMM algorithm is set to be a number between 1 and
1“Q” stands for Quotient. A sequence {∆k} converges Q-linearly to ¯∆for a given norm ∥· ∥if ∥∆k+1 −
¯∆∥/∥∆k −¯∆∥≤µ for all k, for some µ ∈(0, 1).
13

20 and α is set to 1. All simulations in this section are run on a laptop with 2.4 GHz Intel Core
i9 CPU and 32 GB memory using only one thread. The simulation is replicated 10 times and
the convergence plots are obtained over their averages. Figures 4 and 5 show the relative error
of the objective function versus iteration and time, respectively. The optimal objective function
value f ∗is taken to be the minimum objective function value of the converging solution over the
six diﬀerent methods. Note that the C-BCD and R-BCD methods are not included in Figure 4,
as the notion of iteration for block coordinate methods is diﬀerent and not comparable with the
other methods. All of the implementations and the corresponding codes are available at https:
//github.com/samdavanloo/ProxLOG.
As we can see in Figure 4, Algorithm 3 shows linear convergence for all graphs which matches
our theoretical upper bound in Section 3.1. We want to reiterate that the objective function of the
LOG penalty, i.e. (8), is not strongly convex, but the algorithm still converges linearly. The HSM
algorithm converges in ﬁnite-steps for DAG (b) which matches the theory proposed in their paper
[51], given the path structure of this graph. However, HSM’s convergence becomes slower as the
graph grows in width, e.g., DAG (a), where ADMM and ACC-PGM are the fastest methods. As
expected, ACC-PGM converges faster compared to PGM for all graphs. We also note that ADMM
achieves the best objective function value f ∗in most of the experiments for all of the graphs.
With respect to the time, as shown in Figure 5, we compare the convergence speed of all six
methods for the six graphs shown in Figure 3. R-BCD algorithm has the fastest convergence for
DAGs (a) and (c), which are instances of two wide graphs, and ADMM and ACC-PGM are at the
second place. For DAG (b), which is a tree with two long path graphs, after HSM which provably
converges in ﬁnite steps, C-BCD and ADMM are at the second place. In the asymmetric DAG
(e), C-BCD decreases the function values fast at the beginning; however, its convergence becomes
slower (maybe sublinear), while ADMM continues its linear convergence trend.
In the random
DAG (e), C-BCD is the fastest algorithm and ADMM is at the second place. We should note that
BCD algorithms cannot be parallelized given the sequential nature of their computations; hence,
they cannot beneﬁt from parallel computing resources. In general, across diﬀerent graph topologies,
ADMM has the most robust performance from the convergence speed perspective; furthermore, in
many instances, it produces the best ﬁnal (minimum) objective function value.
4.2
Two Applications
The proposed algorithm allows eﬃcient evaluation of the proximal operator of the LOG penalty.
Evaluating the proximal operator is generally needed iteratively within a master optimization algo-
rithm that tries to solve an underlying statistical learning problem. Note that the master problem
might be a convex or nonconvex optimization problem. To demonstrate practicality of the pro-
posed algorithm, in this section, we consider two statistical learning problems on topic modeling
and classiﬁcation. The topic modeling application is a dictionary learning problem for NeurIPS
proceedings. The second application relates to a breast cancer classiﬁcation problem using gene
expression data.
4.2.1
Topic modeling of NeurIPS proceedings.
We are interested in solving the topic modeling problem represented as the dictionary learning
problem (32) penalized with the LOG penalty. Introduction of the LOG penalty is to force the
resulting topics to form a tree structure [22]. The underlying statistical learning problem can be
14

s0
s1
s2
s100
s3
. . .
(a) Two-layer tree, d = 101
s0
s1
s51
s50
s100
. . .
. . .
(b) One root two paths tree, d = 101
s0
s1
s2
. . .
. . .
. . .
. . .
(c) Binary tree, d = 127
…
…
…
…
S1
S2
S63
S64
S65
S96
S125
S126
S127
(d) Reverse binary tree, d = 127
s1
s0
s100
s101
s200
…
…
(e) Asymmetric tree, d = 201
(f) Random DAG, d = 100
Figure 3: Six diﬀerent DAGs considered in the simulation study
15

0
1000
2000
3000
iteration k
10-15
10-10
10-5
100
ADMM
ACC-PGM
PGM
HSM
(a) Two-layer tree, d = 101
0
1000
2000
3000
iteration k
10-10
10-5
100
ADMM
ACC-PGM
PGM
HSM
(b) One root two paths tree, d = 101
0
1000
2000
3000
iteration k
10-10
10-5
100
ADMM
ACC-PGM
PGM
HSM
(c) Binary tree, d = 127
0
1000
2000
3000
iteration k
10-10
10-5
100
ADMM
ACC-PGM
PGM
HSM
(d) Reverse binary tree, d = 127
0
1000
2000
3000
iteration k
10-6
10-4
10-2
100
ADMM
ACC-PGM
PGM
HSM
(e) Asymmetric tree, d = 201
0
1000
2000
3000
iteration k
10-15
10-10
10-5
100
ADMM
ACC-PGM
PGM
HSM
(f) Random DAG, d = 100
Figure 4: Convergence of the proposed ADMM, Proximal Gradient Method (PGM), and
Accelerated PGM (ACC-PGM), Hierarchical Sparse Modeling (HSM) algorithms versus
iteration for the six DAGs in Figure 3.
16

0
0.2
0.4
0.6
0.8
1
time (seconds)
10-15
10-10
10-5
100
ADMM
ACC-PGM
PGM
C-BCD
R-BCD
HSM
(a) Two-layer tree, d = 101
0
0.5
1
1.5
time (seconds)
10-10
10-5
100
ADMM
ACC-PGM
PGM
C-BCD
R-BCD
HSM
(b) One root two paths tree, d = 101
0
0.2
0.4
0.6
0.8
1
time (seconds)
10-5
100
ADMM
ACC-PGM
PGM
C-BCD
R-BCD
HSM
(c) Binary tree, d = 127
0
0.2
0.4
0.6
0.8
1
time (seconds)
10-10
10-5
100
ADMM
ACC-PGM
PGM
C-BCD
R-BCD
HSM
(d) Reverse binary tree, d = 127
0
1
2
3
time (seconds)
10-6
10-4
10-2
100
ADMM
ACC-PGM
PGM
C-BCD
R-BCD
HSM
(e) Asymmetric tree, d = 201
0
0.2
0.4
0.6
0.8
1
time (seconds)
10-10
10-5
100
ADMM
ACC-PGM
PGM
C-BCD
R-BCD
HSM
(f) Random DAG, d = 100
Figure 5: Convergence of the proposed ADMM, Proximal Gradient Method (PGM), Ac-
celerated PGM (ACC-PGM), Cyclic BCD (C-BCD), Randomized BCD (R-BCD), and
Hierarchical Sparse Modeling (HSM) algorithms over time for the six DAGs shown in
Figure 3.
17

written as
min
D∈D+
1 ,A∈Rk×n
+
n
X
j=1
1
2∥xj −Dαj∥2
2 + λΩLOG(αj)

(32)
where X = [x1, x2, · · · , xn] ∈Rm×n represents frequencies of m words in n articles and the i-th
element of xj is the frequency of the i-th word in the j-th article. D = [d1, d2, · · · , dk] ∈D+
1 is
the dictionary of k topics to be learnt where D+
1 ≜{D ∈Rm×k
+
:
∥dj∥1 ≤1, j = 1, 2, · · · , k}.
Furthermore, A ≜[α1, α2, · · · , αn] ∈Rk×n
+
is the corresponding coeﬃcients for each article such
that xj ≈Dαj.
Following the framework of [22], we solve (32) using an alternating minimization scheme, i.e.,
updating D and A one at a time while keeping the other one ﬁxed. The D update is performed
using C-BCD algorithm, taking its columns as the blocks, using the algorithm of [26].
The A
update is performed by the accelerated proximal gradient method ACC-PGM [5]. To evaluate the
proximal operator of the LOG penalty, we implemented the proposed ADMM (with and without
parallelization), R-BCD, and C-BCD algorithms. Given that the number of groups for this appli-
cation is |G| = 13, the ﬁrst block update of the (parallel) ADMM (lines 4 and 5 of Algorithm 3) for
each αj is parallelized over 13 processing nodes. We also included unparallelized ADMM algorithm
for comparison. Note that BCD algorithms cannot be parallelized.
These three nested algorithms are implemented for the NeurIPS proceedings from 1996 to
2015 [35]. The dataset contains n = 1846 articles with m = 11463 words that excludes stop words
and words occurring less than 50 times. We set k = 13, λ = 2−15, and followed the hierarchical
structure proposed by [22] to induce a tree of topics - see Figure 7. The experiment is run on a
cluster with 2.4GHz CPU and 128GB memory using 28 threads. Note that the columns of the A
matrix, i.e. αj, can be updated in parallel over n = 1846 articles for all three methods.
Figure 6 shows the convergence behavior of the algorithms discussed above using the norm of
the proximal gradient (see Deﬁnition 3.1) and (1/n)∥Ak −Ak−1∥F + (1/m)∥Dk −Dk−1∥F as two
convergence measures. Evaluating the proximal operator of the columns of the A matrix using the
C-BCD and parallelized ADMM are the two fastest methods, but the quality of the C-BCD solution
seems to be better. Even though the number of groups is relatively small |G| = 13, parallelization
is signiﬁcantly reducing the convergence time. Reduction of the convergence time by parallelization
of ADMM will even be more signiﬁcant when the number of groups is bigger – see the application
in Section 4.2.2. Figure 7 depicts the learnt hierarchal topics with the 7 most frequent words. The
root is a general topic while the leafs are more speciﬁc and narrower topics.
4.2.2
Breast cancer classiﬁcation.
This section discusses ﬁtting a logistic regression model penalized with the LOG penalty to classify
breast cancer based on gene expression levels. It is known that genes functionalities are highly
aﬀected by their two-way interactions which might be a priori known based on a protein-protein
network. Hence, to identify contributing genes for cancer metastasis, it is important to consider
such structures.
We use the breast cancer dataset of [47] that consists of 8141 gene expression data for 78
metastatic and 217 non-metastatic patients. Following the experimental settings in [32], we build
groups of genes based on the protein-protein network of [13]. Every two genes connected directly
by an edge in the network are assigned as a group. The total number of groups for this application
is |G| = 522. Given that groups have overlaps on many nodes, LOG penalty is used to capture the
18

0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
time (seconds)
105
10-6
10-5
10-4
10-3
10-2
10-1
100
101
102
Figure 6: Convergence of the algorithm for the topic modeling application on two diﬀerent
convergence measures.
The A-update is performed by ACC-PGM algorithm where its
proximal operator is evaluated by the ADMM (with/without parallelization), R-BCD, and
C-BCD methods.
relationship within groups. The genes that are not contained in the network are eliminated and the
500 most correlated genes are selected.
The learning problem involves minimizing the logistic loss function regularized with the LOG
penalty that can be written as
min
θ f(θ) = −1
m
m
X
i=1
{y(i) log hθ(x(i)) + (1 −y(i)) log(1 −hθ(x(i)))} + λΩLOG(θ),
(33)
where (x(i), y(i)) is the input data, y(i) is either 0 or 1, and hθ(x) ≜
1
1+e−θT x . The underlying learn-
ing problem is solved by ACC-PGM algorithm while the proximal operator of the LOG penalty is
evaluated by the proposed ADMM (with/without parallelization), R-BCD, and C-BCD algorithms.
The λ parameter is set equal to 10−3. The experiment is run on a cluster with 2.4GHz CPU and
128GB memory using 28 threads.
Validation of the classiﬁcation performance with the LOG penalty for such a problem is per-
formed e.g. in [32]; so, we only focus on the convergence behavior of the proposed algorithm. The
left plot in Figure 8 shows the convergence of ACC-PGM with diﬀerent proximal evaluators. While
the ADMM method without parallelization is faster than R-BCD and C-BCD methods, paralleliza-
tion of its ﬁrst block over the available 28 nodes signiﬁcantly decrease its convergence time. Note
that such parallelization theoretically reduces the time further up to 522 nodes which is the number
of groups |G|.
We also examine the eﬀect of the LOG penalty for gene selection.
For visual convenience,
we increase λ to from 0.001 to 0.05 to make the regression coeﬃcients sparser and evaluate the
relationships of selected and unselected genes. The right plot in Figure 8 is a subset of the network
19

'data'
'points'
'set'
'number'
'time'
'clustering'
'kernel'
'theorem'
'bound'
'probability'
'distribution'
'let'
'error'
'sample'
'matrix'
'rank'
'norm'
'low'
'matrices'
'sparse'
'problem'
'function'
'functions'
'problem'
'set'
'loss'
'optimization'
'convex'
'network'
'time'
'neural'
'networks'
'neurons'
'input'
'layer'
'image'
'images'
'features'
'object'
'training'
'feature'
'using'
'algorithm'
'regret'
'algorithms'
'bound'
'time'
'problem'
'online'
'learning'
'training'
'label'
'loss'
'task'
'classification'
'examples'
'graph'
'nodes'
'tree'
'node'
'graphs'
'set'
'algorithm'
'policy'
'state'
'action'
'value'
'reward'
'time'
'function'
'model'
'models'
'data'
'latent'
'parameters'
'topic'
'structure'
'gradient'
'convergence'
'method'
'convex'
'optimization'
'methods'
'stochastic'
'distribution'
'inference'
'gaussian'
'posterior'
‘Bayesian'
'sampling'
'variational'
Figure 7: Hierarchical topics of NeurIPS from 1996 to 2015
of 500 genes. Each node represents a gene and the edges are known a priori from the protein-
protein network. Nonzero coeﬃcients in the ﬁnal model identify genes which are correlated with
breast cancer metastasis. From this result, it is clear that connected genes are prone to be selected
simultaneously which supports the rationality of the LOG penalty for this application.
5
Concluding remarks
The paper discusses an eﬃcient algorithm to ﬁnd the proximal mapping of the Latent Overlapping
Group (LOG) lasso penalty to induce hierarchical sparsity structure represented by any general
DAG. The sharing scheme for the underlying ADMM algorithm allows maximum parallelization over
(potentially) many number of groups which allows solving large-scale instances of the underlying
optimization problems which could be convex or nonconvex loss function. On the theoretical side,
the paper establishes global linear rate of convergence in the absence of strong convexity. The rate
analysis is performed through the elegant error bound theory. Furthermore, the paper investigates
the eﬀect of graph structure on the speed of convergence of the algorithm. The numerical results
conﬁrms our theoretical convergence rate for diﬀerent directed acyclic graphs with diﬀerent sizes.
20

0
500
1000
1500
2000
2500
time (seconds)
10-3
10-2
10-1
100
101
f(3k) ! f $=jf $j
ADMM
ADMM (parallelized)
R-BCD
C-BCD
selected parameters
unselected parameters
Figure 8: Breast cancer classiﬁcation: (Left) Convergence of the ACC-PGM method where
the proximal map is evaluated using ADMM (with/without parallelization), R-BCD, and
C-BCD methods.(Right) Part of the gene network: red nodes are nonzero genes while blue
nodes are zero ones. Edge structures are known a priori from the protein-protein network
- see [13].
References
[1] Atamt¨urk, Alper, & G´omez, Andr´es. 2018. Strong formulations for quadratic optimization with
M-matrices and indicator variables. Mathematical Programming, 170(1), 141–176.
[2] Bach, Francis, Jenatton, Rodolphe, Mairal, Julien, Obozinski, Guillaume, et al. 2012. Structured
sparsity through convex optimization. Statistical Science, 27(4), 450–468.
[3] Bach, Francis, et al. 2013. Learning with submodular functions: A convex optimization per-
spective. Foundations and Trends® in Machine Learning, 6(2-3), 145–373.
[4] Bach, Francis R. 2010.
Structured sparsity-inducing norms through submodular functions.
Pages 118–126 of: Advances in Neural Information Processing Systems.
[5] Beck, Amir, & Teboulle, Marc. 2009. A fast iterative shrinkage-thresholding algorithm for linear
inverse problems. SIAM journal on imaging sciences, 2(1), 183–202.
[6] Bertsekas, Dimitri P. 1999. Nonlinear programming. Athena scientiﬁc Belmont.
[7] Bertsimas, Dimitris, & Van Parys, Bart. 2017.
Sparse high-dimensional regression: Exact
scalable algorithms and phase transitions. arXiv preprint arXiv:1709.10029.
[8] Bertsimas, Dimitris, King, Angela, Mazumder, Rahul, et al. 2016. Best subset selection via a
modern optimization lens. The Annals of Statistics, 44(2), 813–852.
21

[9] Bertsimas, Dimitris, Cory-Wright, Ryan, & Pauphilet, Jean. 2019.
A uniﬁed approach to
mixed-integer optimization: Nonlinear formulations and scalable algorithms.
arXiv preprint
arXiv:1907.02109.
[10] Bien, Jacob, Taylor, Jonathan, & Tibshirani, Robert. 2013. A lasso for hierarchical interactions.
Annals of statistics, 41(3), 1111.
[11] Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, & Eckstein, Jonathan. 2011. Dis-
tributed optimization and statistical learning via the alternating direction method of multipliers.
Foundations and Trends® in Machine Learning, 3(1), 1–122.
[12] Chouldechova, Alexandra, & Hastie, Trevor. 2015. Generalized additive model selection. arXiv
preprint arXiv:1506.03850.
[13] Chuang, Han-Yu, Lee, Eunjung, Liu, Yu-Tsueng, Lee, Doheon, & Ideker, Trey. 2007. Network-
based classiﬁcation of breast cancer metastasis. Molecular systems biology, 3(1), 140.
[14] Eckstein, Jonathan, & Yao, Wang. 2012. Augmented Lagrangian and Alternating Direction
Methods for Convex Optimization: A Tutorial and Some Illustrative Computational Results.
RUTCOR Research Reports, 32(Suppl. 3).
[15] Glowinski, Roland. 1984. Numerical methods for nonlinear variational problems. Springer.
[16] Haris, Asad, Witten, Daniela, & Simon, Noah. 2016. Convex modeling of interactions with
strong heredity. Journal of Computational and Graphical Statistics, 25(4), 981–1004.
[17] Hastie, Trevor, Tibshirani, Robert, & Wainwright, Martin. 2015.
Statistical learning with
sparsity. CRC press.
[18] Hazimeh, Hussein, & Mazumder, Rahul. 2019. Learning Hierarchical Interactions at Scale: A
Convex Optimization Approach. arXiv preprint arXiv:1902.01542.
[19] Hoﬀman, Alan J. 1952. On approximate solutions of systems of linear inequalities. Journal of
Research of the National Bureau of Standards, 49(4), 263–265.
[20] Hong, Mingyi, & Luo, Zhi-Quan. 2017. On the linear convergence of the alternating direction
method of multipliers. Mathematical Programming, 162(1-2), 165–199.
[21] Jacob, Laurent, Obozinski, Guillaume, & Vert, Jean-Philippe. 2009. Group lasso with overlap
and graph lasso. Pages 433–440 of: Proceedings of the 26th annual international conference on
machine learning. ACM.
[22] Jenatton, Rodolphe, Mairal, Julien, Obozinski, Guillaume, & Bach, Francis. 2011a. Proximal
methods for hierarchical sparse coding. Journal of Machine Learning Research, 12(Jul), 2297–
2334.
[23] Jenatton, Rodolphe, Audibert, Jean-Yves, & Bach, Francis. 2011b. Structured variable selec-
tion with sparsity-inducing norms. Journal of Machine Learning Research, 12(Oct), 2777–2824.
[24] Lim, Michael, & Hastie, Trevor. 2015. Learning interactions via hierarchical group-lasso regu-
larization. Journal of Computational and Graphical Statistics, 24(3), 627–654.
22

[25] Luo, Zhi-quan, & Tseng, Paul. 1993. On the convergence rate of dual ascent methods for
linearly constrained convex minimization. Mathematics of Operations Research, 18(4), 846–867.
[26] Mairal, Julien, Bach, Francis, Ponce, Jean, & Sapiro, Guillermo. 2010. Online learning for
matrix factorization and sparse coding. Journal of Machine Learning Research, 11(Jan), 19–60.
[27] Mairal, Julien, Jenatton, Rodolphe, Obozinski, Guillaume, & Bach, Francis. 2011. Convex
and network ﬂow optimization for structured sparsity. Journal of Machine Learning Research,
12(Sep), 2681–2720.
[28] Mangasarian, O. L., & Shiau, T-H. 1987. Lipschitz continuity of solutions of linear inequalities,
programs and complementarity problems. SIAM Journal on Control and Optimization, 25(3),
583–595.
[29] Manzour, Hasan, K¨u¸c¨ukyavuz, Simge, & Shojaie, Ali. 2019. Integer Programming for Learning
Directed Acyclic Graphs from Continuous Data. arXiv preprint arXiv:1904.10574.
[30] Mazumder, Rahul, & Radchenko, Peter. 2017.
TheDiscrete Dantzig Selector: Estimating
Sparse Linear Models via Mixed Integer Linear Optimization. IEEE Transactions on Information
Theory, 63(5), 3053–3075.
[31] Nesterov, Yu. 2013.
Gradient methods for minimizing composite functions.
Mathematical
Programming, 140(1), 125–161.
[32] Obozinski, Guillaume, Jacob, Laurent, & Vert, Jean-Philippe. 2011. Group lasso with overlaps:
the latent group lasso approach. arXiv preprint arXiv:1110.0413.
[33] Pang, Jong-Shi. 1997. Error bounds in mathematical programming. Mathematical Program-
ming, 79(1-3), 299–332.
[34] Parikh, Neal, Boyd, Stephen, et al. 2014. Proximal algorithms. Foundations and Trends® in
Optimization, 1(3), 127–239.
[35] Perrone, Valerio, Jenkins, Paul A, Spano, Dario, & Teh, Yee Whye. 2016. Poisson random
ﬁelds for dynamic feature models. arXiv preprint arXiv:1611.07460.
[36] Radchenko, Peter, & James, Gareth M. 2010. Variable selection using adaptive nonlinear inter-
action structures in high dimensions. Journal of the American Statistical Association, 105(492),
1541–1553.
[37] Razaviyayn, Meisam, Hong, Mingyi, & Luo, Zhi-Quan. 2013. A uniﬁed convergence analy-
sis of block successive minimization methods for nonsmooth optimization. SIAM Journal on
Optimization, 23(2), 1126–1153.
[38] Richt´arik, Peter, & Tak´aˇc, Martin. 2014. Iteration complexity of randomized block-coordinate
descent methods for minimizing a composite function. Mathematical Programming, 144(1-2),
1–38.
[39] Schmidt, Mark, & Murphy, Kevin. 2010.
Convex structure learning in log-linear models:
Beyond pairwise potentials. Pages 709–716 of: Proceedings of the Thirteenth International Con-
ference on Artiﬁcial Intelligence and Statistics.
23

[40] She, Yiyuan, Wang, Zhifeng, & Jiang, He. 2018. Group regularized estimation under structural
hierarchy. Journal of the American Statistical Association, 113(521), 445–454.
[41] Stephen M., Robinson. 1973. Bounds for error in the solution set of a linear perturbed linear
program. Linear Algebra and Its Applications, 6, 69–81.
[42] Stephen M., Robinson. 1981. Some continuity properties polyhedral multifunctions. Mathe-
matical Programming Study, 14, 206–214.
[43] Tibshirani, Robert. 1996. Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society. Series B (Methodological), 267–288.
[44] Tseng, Paul. 2001. Convergence of a block coordinate descent method for nondiﬀerentiable
minimization. Journal of optimization theory and applications, 109(3), 475–494.
[45] Tseng, Paul. 2010. Approximation accuracy, gradient methods, and error bound for structured
convex optimization. Mathematical Programming, 125(2), 263–295.
[46] Tseng, Paul, & Yun, Sangwoon. 2009. A coordinate gradient descent method for nonsmooth
separable minimization. Mathematical Programming, 117(1-2), 387–423.
[47] Van De Vijver, Marc J, He, Yudong D, Van’t Veer, Laura J, Dai, Hongyue, Hart, Augusti-
nus AM, Voskuil, Dorien W, Schreiber, George J, Peterse, Johannes L, Roberts, Chris, Marton,
Matthew J, et al. 2002. A gene-expression signature as a predictor of survival in breast cancer.
New England Journal of Medicine, 347(25), 1999–2009.
[48] Villa, Silvia, Rosasco, Lorenzo, Mosci, Soﬁa, & Verri, Alessandro. 2014. Proximal methods for
the latent group lasso penalty. Computational Optimization and Applications, 58(2), 381–407.
[49] Walkup, David W., & Wets, Roger J.-B. 1969.
A lipschitzian characterization of convex
polyhedra. Proceedings of the American Mathematical Society, 23(1), 167–173.
[50] Wolsey, Laurence A, & Nemhauser, George L. 2014. Integer and combinatorial optimization.
John Wiley & Sons.
[51] Yan, Xiaohan, Bien, Jacob, et al. 2017. Hierarchical Sparse Modeling: A Choice of Two Group
Lasso Formulations. Statistical Science, 32(4), 531–560.
[52] Yuan, Ming, & Lin, Yi. 2006.
Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1),
49–67.
[53] Zhang, Haibin, Jiang, Jiaojiao, & Luo, Zhi-Quan. 2013. On the linear convergence of a prox-
imal gradient method for a class of nonsmooth convex minimization problems. Journal of the
Operations Research Society of China, 1(2), 163–186.
[54] Zhao, Peng, & Yu, Bin. 2006. On model selection consistency of Lasso. Journal of Machine
learning research, 7(Nov), 2541–2563.
[55] Zhou, Zirui, & Man-Cho So, Anthony. 2019. A Uniﬁed Approach to Error Bounds for Struc-
tured Convex Optimization Problems. Mathematical Programming, 165(2), 689—-728.
24

A
Proof of of the dual error bound - Lemma 3.1
The framework of the proof was ﬁrst proposed in [25] and also applied in [20] and requires “locally
upper Lipschitzian” property of polyhedral multifunction for the map induced by KKT conditions–
see also [42, 49, 41, 28, 19]. However, due to the presence of the conic constraints in (28), the
resulting multifunction is not polyhedral anymore. Indeed, [49] showed that having a polyhedral
graph is a necessary condition for the upper Lipschitzian property of the multifunction.
The
following proof uses the speciﬁc structure of this problem to establish the dual error bound condition.
For any y ∈Rn and y∗∈Y ∗, considering the KKT conditions (29), we have
∥y −y∗∥2
2 = ∥M ⊤M(x2(y) −x2(y∗)) + ρ(x2(y) −x1(y)) −ρ(x2(y∗) −x1(y∗))∥2
2
= ∥∇x2φ(Mx2(y)) −∇x2φ(Mx2(y∗)) + ∇x2ψ(Ex(y)) −∇x2ψ(Ex(y∗))∥2
2
≤∥∇x2φ(Mx2(y)) −∇x2φ(Mx2(y∗))∥2
2 + ∥∇xψ(Ex(y)) −∇xψ(Ex(y∗))∥2
2,
where the ﬁrst equality follows from (29b), the second equality follows from the deﬁnition of φ(·)
and ψ(·) (deﬁned below (25)), the third inequality follows from the triangle inequality. Hence, using
(26) and (27), we have
∥y −y∗∥2
2 ≤L2
φ∥Mx2(y) −Mx2(y∗)∥2
2 + L2
ψ∥Ex(y) −Ex(y∗)∥2
2.
(34)
Next, consider
∥Mx2(y) −Mx2(y∗)∥2
2 + ρ∥Ex(y) −Ex(y∗)∥2
2
=

M ⊤Mx2(y) −M ⊤Mx2(y∗), x2(y) −x2(y∗)

+ ρ

E⊤Ex(y) −M ⊤Mx(y∗), x(y) −x(y∗)

=

∇xφ(Mx2(y)) −∇xφ(Mx2(y∗)), x(y) −x(y∗)

+ ⟨∇xψ(Ex(y)) −∇xψ(Ex(y∗)), x(y) −x(y∗)⟩
= ⟨∇xℓ(x(y)) −∇xℓ(x(y∗)), x(y) −x(y∗)⟩
=

∇x1ℓ(x(y)) −∇x1ℓ(x(y∗)), x1(y) −x1(y∗)

+

∇x2ℓ(x(y)) −∇x2ℓ(x(y∗)), x2(y) −x2(y∗)

=
X
g∈G
D
∇x1
j(g)ℓ(x(y)) −∇x1
j(g)ℓ(x(y∗)), x1
j(g)(y) −x1
j(g)(y∗)
E
+
X
g∈G
D
∇x2
j(g)ℓ(x(y)) −∇x2
j(g)ℓ(x(y∗)), x2
j(g)(y) −x2
j(g)(y∗)
E
=
X
g∈G
D
wgµg(y) −yj(g) −wgµg(y∗) + y∗
j(g), x1
j(g)(y) −x1
j(g)(y∗)
E
+
X
g∈G
D
yj(g) −y∗
j(g), x2
j(g)(y) −x2
j(g)(y∗)
E
where the second and third equalities follow from the deﬁnitions of φ, ψ, and ℓ, in the forth and
ﬁfth equalities the gradient is expanded over each x1 and x2, and the last equality follows from
(29a),(29b). Rearranging the terms in the last line above and using x1
j(g)(y∗) = x2
j(g)(y∗) ∀g ∈G,
we get
∥Mx2(y) −Mx2(y∗)∥2
2 + ρ∥Ex(y) −Ex(y∗)∥2
2
=
X
g∈G
D
wgµg(y) −wgµg(y∗), x1
j(g)(y) −x1
j(g)(y∗)
E
+
X
g∈G
D
yj(g) −y∗
j(g), x2
j(g)(y) −x1
j(g)(y)
E
,
(35)
25

For all g ∈G, we have
D
wgµg(y) −wgµg(y∗), x1
j(g)(y) −x1
j(g)(y∗)
E
= wgµg(y)⊤x1
j(g)(y) −wgµg(y∗)⊤x1
j(g)(y) −wgµg(y)⊤x1
j(g)(y∗) + wgµg(y∗)⊤x1
j(g)(y∗)
= −wg(sg/wg)λ −wgµg(y∗)⊤x1
j(g)(y) −wgµg(y)⊤x1
j(g)(y∗) −wg(sg/wg)λ
≤wg∥µg(y∗)∥2∥x1
j(g)(y)∥2 + wg∥µg(y)∥2∥x1
j(g)(y∗)∥2 −2sgλ
≤0,
where the second equality follows from (29c) and (29f), the third inequality uses Cauchy-Schwarz
inequality, and the last inequality follows from (29c), (29d), and (29e). Hence, we have
∥Mx2(y) −Mx2(y∗)∥2
2 + ρ∥Ex(y) −Ex(y∗)∥2
2 ≤
X
g∈G
D
yj(g) −y∗
j(g), x2
j(g)(y) −x1
j(g)(y)
E
(36)
≤∥y −y∗∥∥∇gρ(y)∥,
(37)
where (36) uses nonpositivity of the ﬁrst term in (35) (shown above), and (37) follows from Cauchy-
Schwarz inequality and the fact that ∇gρ(y) = x1(y) −x2(y) – see e.g. [20] - Lemma 2.1. Finally,
using (37) and (34), we get
∥y −y∗∥2
2 ≤max{L2
φ, L2
ψ/ρ}

∥Mx2(y) −Mx2(y∗)∥2
2 + ρ∥Ex(y) −Ex(y∗)∥2
2

(38)
≤max{L2
φ, L2
ψ/ρ}∥y −y∗∥2∥∇gρ(y)∥2.
(39)
Hence, we have
dist(y, Y ∗) ≤∥y −y∗∥≤max{L2
φ, L2
ψ/ρ}∥∇gρ(y)∥2.
(40)
B
Proof of Lemma 3.2
Before proceeding with the proof of the boundedness of the iterates, we show the existence of a
ﬁnite saddle point by the following argument. Consider
min
x1,x2∈Rn



˜F(x1, x2) = λ
X
g∈G
wg∥x1
j(g)∥2 + 1
2∥Mx2 −b∥2
2 + ρ
2∥x1 −x2∥2
2,
s.t. x1 = x2


.
(41)
The above problem is equivalent to (8) whose objective function is coercive and continuous. By
Weierstrass’s Theorem (see e.g. [6]), we have certain ﬁnite optimal solution to (41) (x1,∗, x2,∗),
i.e. ˜F ∗= infx1=x2∈Rn ˜F(x1, x2) = ˜F(x1,∗, x2,∗). Especially, x1,∗= x2,∗. Consider Lρ(x1, x2; y) =
˜F(x1, x2)+

y, x1 −x2
and the corresponding dual function gρ(y). First, we have Lρ(x1,∗, x2,∗; y) =
˜F(x1,∗, x2,∗), for ∀y. By strong duality (see e.g. Prop. 5.2.1 in [6]), we know there is no duality gap.
Furthermore, there exists at least one Lagrange multiplier y∗, i.e. ˜F ∗= infx1,x2∈R Lρ(x1, x2; y∗) =
gρ(y∗). We conclude (x1,∗, x2,∗; y) is a ﬁnite saddle point.
The idea for the proof of boundedness of the iterates is similar to Theorem 5.1 in [15]; however,
we do not have the strong convexity assumption. Given a ﬁnite saddle point ((x1,∗, x2,∗); y∗), deﬁne
26

˜x1,k ≜x1,k −x1,∗, ˜x2,k ≜x2,k −x2,∗, ˜yk ≜yk −y∗where x1,∗= x2,∗. Establishing the boundedness
of the sequence is equivalent to showing that the sequence {∥˜yk∥2
2 + αρ∥˜x2,k∥2
2 + α(ρ −α)∥˜x1,k −
˜x2,k∥2
2}∞
k=1 is non-increasing. From the convexity of the augmented Lagrangian function (10) in x1,
we have

y∗+ ρ(x1,∗−x2,∗), x1 −x1,∗
+ λ
X
g∈G
wg∥x1
j(g)∥2 −λ
X
g∈G
wg∥x1,∗
j(g)∥2 ≥0, ∀x1.
(42)
Furthermore, from the convexity of the augmented Lagrangian (10) in x2, we have

M ⊤(Mx2,∗−b) −y∗+ ρ(x2,∗−x1,∗), x2 −x2,∗
≥0, ∀x2.
(43)
From the fact that Lρ(x1,∗, x2,∗; y∗) ⩽Lρ(x1, x2; y∗), ∀x1, x2, we have
y∗= y∗+ α(x1,∗−x2,∗).
(44)
Similar to the arguments for (42)-(44), from (13)-(15), we have
D
ρ(x1,k+1
j(g)
−x2,k
j(g)) + yk
j(g), x1
j(g) −x1,k+1
j(g)
E
+ λwg∥x1
j(g)∥2 −λwg∥x1,k+1
j(g) ∥2 ≥0,
∀g ∈G, ∀x1
j(g),
(45)

M ⊤(Mx2,k+1 −b) + ρ(x2,k+1 −x1,k+1) −yk, x2 −x2,k+1
≥0,
∀x2,
(46)
yk+1
j(g) = yk
j(g) + α(x1,k+1
j(g)
−x2,k+1
j(g) ),
∀g ∈G.
(47)
Since j(g) ∩j(¯g) = ∅for all g, ¯g ∈G such that g ̸= ¯g, from (45) and (47), we have:

ρ(x1,k+1 −x2,k) + yk, x1 −x1,k+1
+ λ
X
g∈G
wg∥x1
j(g)∥2 −λ
X
g∈G
wg∥x1,k+1
j(g) ∥2 ≥0,
∀x1,
(48)
yk+1 = yk + α(x1,k+1 −x2,k+1).
(49)
Setting x1 = x1,k+1 in (42), and x1 = x1,∗in (48) and adding them, we get

−˜yk + ρ(˜x2,k −˜x1,k+1), ˜x1,k+1
≥0
(50)
Similarly, setting x2 = x2,k+1 in (43), and x2 = x2,∗in (46) and adding them, we get

M ⊤M ˜x2,k+1 −˜yk −ρ(˜x1,k+1 −˜x2,k+1), −˜x2,k+1
≥0
(51)
Adding the left-hand-sides of (50) to (51) and rearranging the terms, we have,

−˜yk, ˜x1,k+1 −˜x2,k+1
+ ρ

˜x2,k −˜x1,k+1, ˜x1,k+1
+ ρ

˜x1,k+1 −˜x2,k+1, ˜x2,k+1
−∥M ˜x2,k+1∥2
2
=

−˜yk, ˜x1,k+1 −˜x2,k+1
+ ρ

˜x2,k −˜x1,k+1 + ˜x2,k+1 −˜x2,k+1, ˜x1,k+1
+ ρ

˜x1,k+1 −˜x2,k+1), ˜x2,k+1
−∥M ˜x2,k+1∥2
2
Hence, we have
ρ

˜x2,k −˜x2,k+1, ˜x1,k+1
−∥M ˜x2,k+1∥2
2 −ρ∥˜x2,k+1 −˜x1,k+1∥2
2 ≥

˜x1,k+1 −˜x2,k+1, ˜yk
.
(52)
27

From (49), we have the following two inequalities:
˜yk+1 −˜yk = α(˜x1,k+1 −˜x2,k+1)
˜yk+1 + ˜yk = α(˜x1,k+1 −˜x2,k+1) + 2˜yk
Taking the inner product of the left terms together and the right terms together, we obtain
∥˜yk+1∥2
2 −∥˜yk∥2
2 = α2∥˜x1,k+1 −˜x2,k+1∥+ 2α

˜x1,k+1 −˜x2,k+1, ˜yk
(53)
≤α(α −2ρ)∥˜x1,k+1 −˜x2,k+1∥2
2 −2α∥M ˜x2,k+1∥2
2 + 2αρ

˜x2,k −˜x2,k+1, ˜x1,k+1
(54)
where the inequality uses (52). Next, we will upper bound

˜x2,k −˜x2,k+1, ˜x1,k+1
. Setting x2 = x2,k
in (46), we have

M ⊤(Mx2,k+1 −b) + ρ(x2,k+1 −x1,k+1) −yk, x2,k −x2,k+1
≥0.
(55)
Setting k + 1 in (46) to k, and x2 = x2,k+1, we have

M ⊤(Mx2,k −b) + ρ(x2,k −x1,k) −yk−1, x2,k+1 −x2,k
≥0.
(56)
Adding (55) and (56), we have

yk −yk−1, x2,k+1 −x2,k
−ρ∥x2,k+1 −x2,k∥2
2 + ρ

x1,k+1 −x1,k, x2,k+1 −x2,k
≥∥M(x2,k+1 −x2,k)∥2
2 ≥0.
(57)
From (49), we have yk −yk−1 = α(x1,k −x2,k). Using it in (57) and rearranging terms, we obtain
ρ

x1,k+1 −x1,k, x2,k+1 −x2,k
≥ρ∥x2,k+1 −x2,k∥2
2 −α

x1,k −x2,k, x2,k+1 −x2,k
.
Adding and subtracting x1,∗and x2,∗into each argument in (B) as needed, we have
ρ

˜x1,k+1 −˜x1,k, ˜x2,k+1 −˜x2,k
≥ρ∥˜x2,k+1 −˜x2,k∥2
2 −α

˜x1,k −˜x2,k, ˜x2,k+1 −˜x2,k
.
(58)
The term

˜x2,k −˜x2,k+1, ˜x1,k+1
can be transformed as following:

˜x2,k −˜x2,k+1, ˜x1,k+1
=

˜x2,k −˜x2,k+1, ˜x1,k+1 −˜x1,k + ˜x1,k −˜x2,k + ˜x2,k
=

˜x2,k −˜x2,k+1, ˜x1,k+1 −˜x1,k
+

˜x2,k −˜x2,k+1, ˜x1,k −˜x2,k
+

˜x2,k −˜x2,k+1, ˜x2,k
=

˜x2,k −˜x2,k+1, ˜x1,k+1 −˜x1,k
+

˜x2,k −˜x2,k+1, ˜x1,k −˜x2,k
+ 1
2(∥˜x2,k∥2
2 −∥˜x2,k+1∥2
2 + ∥˜x2,k −˜x2,k+1∥2
2)
≤1
2(∥˜x2,k∥2
2 −∥˜x2,k+1∥2
2 −∥˜x2,k −˜x2,k+1∥2
2) + (1 −α
ρ )

˜x2,k −˜x2,k+1, ˜x1,k −˜x2,k
,
(59)
where the last inequality follows from (58). Combining (53) and (59) and rearranging the terms,
we obtain
∥˜yk+1∥2
2 + αρ∥˜x2,k+1∥2
2 + α(ρ −α)∥˜x1,k+1 −˜x2,k+1∥2
2 −(∥˜yk∥2
2 + αρ∥˜x2,k∥2
2)
≤−αρ∥˜x1,k+1 −˜x2,k+1∥2
2 −2α∥M ˜x2,k+1∥2
2 −αρ∥˜x2,k −˜x2,k+1∥2
2 + 2α(ρ −α)

˜x2,k −˜x2,k+1, ˜x1,k −˜x2,k
(60)
28

By upper bounding the last term in (60) by the identity 2 ⟨a, b⟩≤∥a∥2
2 + ∥b∥2
2, we get
∥˜yk+1∥2
2 + αρ∥˜x2,k+1∥2
2 + α(ρ −α)∥˜x1,k+1 −˜x2,k+1∥2
2 −(∥˜yk∥2
2 + αρ∥˜x2,k∥2
2 + α(ρ −α)∥˜x1,k −˜x2,k∥2
2)
≤−αρ∥˜x1,k+1 −˜x2,k+1∥2
2 −2α∥M ˜x2,k+1∥2
2 −α2∥˜x2,k −˜x2,k+1∥2
2 ≤0.
(61)
We have shown that the sequence {∥˜yk∥2
2+αρ∥˜x2,k∥2
2+α(ρ−α)∥˜x1,k−˜x2,k∥2
2}∞
k=1 is non-increasing.
Once the initial point and saddle point are ﬁxed, which are not related to α, then {∥˜yk∥2
2 +
αρ∥˜x2,k∥2
2 + α(ρ −α)∥˜x1,k −˜x2,k∥2
2}∞
k=1 is bounded by ∥˜y0∥2
2 + ρ2∥˜x2,0∥2
2 + ρ2
4 ∥˜x1,0 −˜x2,0∥2
2. We
concluded that the sequence {x1,k}, {x2,k} and {yk} generated by (15) is uniformly bounded for
any 0 < α < ρ.
C
Proof of Lemma 3.3
The proof extends the analysis of [45] and [53]. Since both works discuss primal methods, there
are mainly two new ingredients in our proof: 1) dealing with the dual variable y, and 2) splitting
x into x1 and x2, where neither step is trivial.
Given y, note that X(y) can be written as (X1(y), X2(y)). For a ﬁxed y, and for any sequence
{(x1,k, x2,k; y) : x2,k ̸∈X2(y)}k≥0, we deﬁne
r1,k ≜˜∇x1Lρ(x1,k, x2,k; y),
(62)
r2,k ≜˜∇x2Lρ(x1,k, x2,k; y) = M T (Mx2,k −b) −y + ρ(x2,k −x1,k),
(63)
δk ≜∥x2,k −¯x2,k∥2,
where ¯x2,k ≜argmin
x2∈X2(y)
∥x2,k −x2∥2,
(64)
¯x1,k ≜argmin
x1
Lρ(x1, ¯x2,k; y),
(65)
uk ≜x2,k −¯x2,k
δk
.
(66)
Note that
˜∇x1Lρ(x1, x2; y) = x1 −proxλ P
g∈G wg∥˜dj(g)∥2(x1 −y −ρ(x1 −x2))
(67)
= x1 −argmin
˜d
λ
X
g∈G
wg∥˜dj(g)∥2 + 1
2∥˜d −(x1 −y + ρ(x1 −x2))∥2
2
(68)
= argmin
d
λ
X
g∈G
wg∥dj(g) −x1
j(g)∥2 + 1
2∥d −y −ρ(x1 −x2)∥2
2,
(69)
where the second equality follows from the deﬁnition of the proximal operator and the third equality
uses the transformation d ≜x1 −˜d. Furthermore, for any group g ∈G, we have

˜∇x1Lρ(x1, x2; y)

j(g) = argmin
dj(g)
λwg∥dj(g) −x1
j(g)∥2 + 1
2∥dj(g) −yj(g) −ρ(x1
j(g) −x2
j(g))∥2
2 (70)
=
(
x1
j(g),
if ∥x1
j(g) −yj(g) −ρ(x1
j(g) −x2
j(g))∥2 ≤λwg,
γgx1
j(g) + (1 −γg)(yj(g) + ρ(x1
j(g) −x2
j(g))),
otherwise.
(71)
29

where γg = λwg/∥x1
j(g) −yj(g) −ρ(x1
j(g) −x2
j(g))∥2.
Note that the two cases from the soft-
thresholding operator in (71) yield x1
j(g) at the boundary ∥x1
j(g) −yj(g) −ρ(x1
j(g) −x2
j(g))∥2 = λwg,
i.e., ˜∇x1Lρ(x1, x2; y)j(g) is continuous in (x1, x2, y).
To prove this lemma, we will ﬁrst prove that it suﬃces to show that there exists 0 < τ ′ < +∞
and δ > 0 such that
dist(x2, X2(y)) ≤τ ′∥˜∇xLρ(x1, x2; y)∥2,
(72)
for all (x1, x2; y) such that ∥˜∇xLρ(x1, x2; y)∥2 ≤δ. Second, we will show (72).
Assume (72) holds.
Given (x1, x2; y), pick (x1,∗, x2,∗) ∈X(y), such that dist(x2, x2,∗) =
dist(x2, X2(y)), and x1,∗such that it satisﬁes the optimality condition (74). Recall that
˜∇x2Lρ(x1, x2; y) = M T (Mx2 −b) −y + ρ(x2 −x1).
(73)
Hence, from the optimality condition, we have
˜∇x2Lρ(x1,∗, x2,∗; y) = M T (Mx2,∗−b) −y + ρ(x2,∗−x1,∗) = 0
(74)
Subtracting (74) from (73) and rearranging the terms, we obtain
x1 −x1,∗= (1
ρM T M + I)(x2 −x2,∗) −1
ρ
˜∇x2Lρ(x1, x2; y).
(75)
Thus,
dist(x, X(y))2 ≤∥x1 −x1,∗∥2
2 + ∥x2 −x2,∗∥2
2
(76)
≤∥(1
ρM T M + I)(x2 −x2,∗)∥2
2 + ∥1
ρ
˜∇x2Lρ(x1, x2; y)∥2
2 + ∥x2 −x2,∗∥2
2.
(77)
Upper bounding ∥x2 −x2,∗∥2
2 in (77) with (72), we have (31).
Next, we will show (72) by contradiction.
Suppose (72) does not hold, then there exists a
sequence {(x1,k, x2,k; y) : x2,k ̸∈X2(y)}k≥0 satisfying
∥˜∇xLρ(x1,k, x2,k, y)∥2/δk →0,
and
∥˜∇xLρ(x1,k, x2,k, y)∥2 →0.
(78)
Note that
∥r1,k∥+ ∥r2,k∥
√
2
≤∥˜∇xLρ(x1,k, x2,k; y)∥≤∥r1,k∥+ ∥r2,k∥
(79)
where r1,k and r2,k are deﬁned in (62) and (63), respectively. Hence, using the left inequality in
(79), (78) implies
{r1,k} →0,
{r2,k} →0,
{∥r1,k∥+ ∥r2,k∥
δk
} →0.
(80)
We will show that (80) does not hold.
Since (x1,k, x2,k) is in a compact set, by passing to a
subsequence if necessary, we can assume that {(x1,k, x2,k) →(¯x1, ¯x2)}. Since {r1,k} →0, and
{r2,k} →0, then by the right inequality in (79), ˜∇xLρ(x1,k, x2,k; y) →0.
Furthermore, since
˜∇xLρ(x1, x2; y) is continuous, this implies ˜∇xLρ(¯x1, ¯x2; y) = 0. It further implies that (¯x1, ¯x2) ∈
X(y). Hence δk ≤∥x2,k −¯x2∥→0, as k →∞, so that {¯x2,k} →¯x2. And based on (75), we have
{¯x1,k} →¯x1.
(81)
30

Next, we claim there exists κ > 0 such that,
∥x2,k −¯x2,k∥≤κ∥Mx2,k −M ¯x2,k∥,
∀k
(82)
Again, we argue (82) by contraction. Suppose (82) does not hold, then by passing to a subsequence
if necessary, we can assume
{∥Mx2,k −M ¯x2,k∥
∥x2,k −¯x2,k∥
} →0.
(83)
This implies that {Muk} →0, where uk is deﬁned in (66). Note that ∥uk∥= 1, we can assume
uk →¯u ̸= 0 (by further passing to a subsequence if necessary); hence, we have M ¯u = 0 by
continuity. Combining (73) and (80), we have
M T (Mx2,k −b) −y + ρ(x2,k −x1,k) = o(δk).
Furthermore,
M T (M ¯x2,k −b) −y + ρ(¯x2,k −¯x1,k) = 0.
Subtracting the above two equalities and using (83), we get
x2,k −¯x2,k = x1,k −¯x1,k + o(δk).
(84)
Thus,
¯u = lim
k→∞
x2,k −¯x2,k
δk
= lim
k→∞
x1,k −¯x1,k
δk
.
Since uk →¯u ̸= 0, we have

uk, ¯u

> 0 for k suﬃciently large. Select k such that

uk, ¯u

> 0 and
let
ˆx2,k ≜¯x2,k + ϵ¯u
(85)
for some ϵ > 0. We can show that for ϵ > 0 suﬃciently small
ˆx2,k ∈X2(y),
(86)
whose proof is relegated to Appendix D. Now, assume ˆx2,k ∈X2(yk) for ϵ > 0 suﬃciently small.
This leads to the following contradiction:
∥x2,k −ˆx2,k∥2 = ∥x2,k −¯x2,k −ϵ¯u∥2 = δk + ϵ2 −2ϵ

uk, ¯u

< δk
(87)
for ϵ suﬃciently small, which contradicts the deﬁnition of ¯x2,k in (64). So (82) holds.
By (69), we have
0 ∈λ∂
X
g∈G
wg∥r1,k
j(g) −x1,k
j(g)∥2 + (r1,k −y −ρ(x1,k −x2,k)),
(88)
which is the optimal condition to
r1,k ∈argmin
d
λ
X
g∈G
wg∥dj(g) −x1,k
j(g)∥2 +

r1,k −y −ρ(x1,k −x2,k), d

.
(89)
31

From (89), we have
λ
X
g∈G
wg∥r1,k
j(g) −x1,k
j(g)∥2 +

r1,k −y −ρ(x1,k −x2,k), r1,k
≤λ
X
g∈G
wg∥¯x1,k∥2 +

r1,k −y −ρ(x1,k −x2,k), x1,k −¯x1,k
.
(90)
From ˜∇x1Lρ(¯x1,k, ¯x2,k; y) = 0, we have
0 = argmin
d
λ
X
g∈G
wg∥dj(g) −¯x1
j(g)∥2 + 1
2∥d −y −ρ(¯x1 −¯x2)∥2
2.
(91)
Similar to (88), we have
0 ∈λ∂
X
g∈G
wg∥¯x1,k
j(g)∥2 + (−y −ρ(¯x1,k −¯x2,k)),
(92)
which is the optimal condition to
0 ∈argmin
d
λ
X
g∈G
wg∥dj(g) −¯x1,k
j(g)∥2 +

−y −ρ(¯x1,k −¯x2,k), d

.
(93)
From (93), we have
λ
X
g∈G
wg∥¯x1,k
j(g)∥2
≤λ
X
g∈G
wg∥r1,k
j(g) −x1,k
j(g)∥2 +

−y −ρ(¯x1,k −¯x2,k), ¯x1,k + r1,k −x1,k
.
(94)
Adding (90) and (94), and using (63), we obtain

r1,k + r2,k, r1,k
+

M T M(x2,k −¯x2,k), x1,k −¯x1,k
≤

r1,k + r2,k, x1,k −¯x1,k
+

M T M(x2,k −¯x2,k), r1,k
.
(95)
From (63), we have
x1,k −¯x1,k = (1
ρM T M + I)(x2,k −¯x2,k) −1
ρr2,k.
(96)
Deﬁning A ≜1
ρM T M + I and using (96) in (95) and rearranging the terms, we obtain

r1,k + r2,k, r1,k + 1
ρr2,k

+

M T M(x2,k −¯x2,k), A(x2,k −¯x2,k)

≤

r1,k + r2,k, A(x2,k −¯x2,k)

+

M T M(x2,k −¯x2,k), r1,k + 1
ρr2,k

.
(97)
Let us consider term by term. We have

r1,k + r2,k, r1,k + 1
ρr2,k

≥∥r1,k∥2
2 + 1
ρ∥r2,k∥2
2 −(1
ρ + 1)∥r1,k∥2∥r2,k∥2.
(98)
32

Next, using (82), we have

M T M(x2,k −¯x2,k), A(x2,k −¯x2,k)

= 1
ρ∥M T M(x2,k −¯x2,k)∥2
2 + ∥M(x2,k −¯x2,k)∥2
2
(99)
≥κ2∥x2,k −¯x2,k∥2
2.
(100)
Denote the largest eigenvalue of matrix A by L1, we have

r1,k + r2,k, A(x2,k −¯x2,k)

≤L1∥r1,k + r2,k∥2∥x2,k −¯x2,k∥2.
(101)
Denote L2 ≜max∥d∥=1 ∥Md∥,

M T M(x2,k −¯x2,k), r1,k + 1
ρr2,k

≤L2
2∥r1,k + 1
ρr2,k∥2∥x2,k −¯x2,k∥2.
(102)
Combining the four inequalities above, we have
κ2∥x2,k −¯x2,k∥2
2 + ∥r1,k∥2
2 + 1
ρ∥r2,k∥2
2 −(1
ρ + 1)∥r1,k∥2∥r2,k∥2
≤(L1∥r1,k + r2,k∥2 + L2
2∥r1,k + 1
ρr2,k∥2)∥x2,k −¯x2,k∥2.
(103)
Denote b ≜L1∥r1,k + r2,k∥2 + L2
2∥r1,k + 1
ρr2,k∥2, c ≜∥r1,k∥2
2 + 1
ρ∥r2,k∥2
2 −( 1
ρ + 1)∥r1,k∥2∥r2,k∥2.
Using quadratic formula, (103) implies
∥x2,k −¯x2,k∥2 ≤b +
√
b2 −4κ2c
2κ2
.
(104)
Note that the right-hand-side of (104) is O(∥r1,k∥+ ∥r2,k∥), so (104) contradicts (80), which says
∥r1,k∥+ ∥r2,k∥= o(∥x2,k −¯x2,k∥).
So far, we have shown that for a ﬁxed y, there exist τ and δ satisfying (31) and the inequality
below it, accordingly. From (70) and (63), we know ˜∇xLρ(x1, x2; y) is continuous in y. Since X(y)
is characterized by ˜∇xLρ(x1, x2; y) = 0, so dist(x, X(y)) is also continuous in y, which implies that
we can deﬁne a continuous mapping from y to τ and δ. Note that from the above proof, we know
that for any y, τ is ﬁnite, i.e., τ < ∞, and δ > 0. Hence, since y is in a compact set, we can ﬁnd
¯τ ≜sup{τ} < ∞and ¯δ ≜inf{δ} > 0. This ﬁnishes the proof of the lemma.
D
Proof of (86) in Lemma 3.3
The following proof is inspired by [45] and [53]. Denote tk ≜y + ρ(x1,k −x2,k). Note that if we
write (10) as a function of x2 and z ≜x1 −x2, then the last term is strongly convex in z. This
implies that the value of ¯t ≜y + ρ(x1 −x2) is unique for ∀(x1, x2) ∈X(y). Recall the deﬁnition in
(65) and (85), we will show (86) is equivalent to
0 ∈λ∂wg∥(¯x1,k + ϵ¯u)j(g)∥+ ¯tj(g),
∀g.
(105)
33

From the optimality condition of (10), we know ˆx2,k ∈X2(y) is equivalent to
(
0 ∈λ∂P
g∈G wg∥x1
j(g)∥2 + (y + ρ(x1 −ˆx2))j(g), ∀g
0 = M T (M ˆx2,k −b) −(y + ρ(x1 −ˆx2)),
(106)
is satisﬁed for some x1. From (85), we have
M ˆx2 = M ¯x2
(107)
since M ¯u = 0. So the second equality of (106) holds if and only if x1 = ¯x1 + ϵ¯u.2 Since 0 =
M T (M ¯x2,k −b) −(y + ρ(¯x1 −¯x2)) holds by deﬁnitions (65) and (64), (106) is equivalent to
0 ∈λ∂
X
g∈G
wg∥¯x1
j(g) + ϵ¯u∥2 + (y + ρ(¯x1 −¯x2))j(g) ∀g.
(108)
Using ¯t to replace (y + ρ(¯x1 −¯x2)), we have (105).
Based on (80) and (83), we have
tk −¯t = M T M(x2,k −¯x2,k) −r2,k = o(δk).
(109)
By further passing to a subsequence if necessary, we can assume that, for each g ∈G, either
1. ∥x1,k
j(g) −tk
j(g)∥2 ≤λwg,
∀k, or,
2. ∥x1,k
j(g) −tk
j(g)∥2 > λwg, and ¯x1,k
j(g) ̸= 0,
∀k, or,
3. ∥x1,k
j(g) −tk
j(g)∥2 > λwg, and ¯x1,k
j(g) = 0,
∀k,
is true. We will show that in any of the three above cases, ¯uj(g) is a certain multiple of ¯tj(g) and
then (105) is satisﬁed.
1. In this case, from (71), we know
¯uj(g) = lim
k→∞
x1,k
j(g) −¯x1,k
j(g)
δk
= lim
k→∞
r1,k
j(g) −¯x1,k
j(g)
δk
= lim
k→∞
−¯x1,k
j(g)
δk
(110)
where the last equation comes from (80). Suppose that ¯uj(g) ̸= 0. (Otherwise, ˆx2,k = ¯x2,k.)
Then ¯x1,k
j(g) ̸= 0 for all k suﬃciently large. From the optimality condition for (10), we have
0 = λwg
¯x1,k
j(g)
∥¯x1,k
j(g)∥2
+ ¯tj(g),
(111)
for k suﬃciently large. By continuity, we have ¯uj(g) is a positive multiple of ¯tj(g). Fur-
thermore, ¯x1,k
j(g) is a negative multiple of ¯tj(g). Therefore, for ϵ suﬃciently small, (105) is
satisﬁed.
2In fact, from our discussion on the uniqueness of y + ρ(x1 −x2) for ∀(x1, x2) ∈X(y), we can also
conclude that x1 must be ¯x1 + ϵ¯u.
34

2. In this case, since we assumed ¯x1,k
j(g) ̸= 0 ∀k, (111) is always satisﬁed. It implies
¯tj(g) = λwg
¯tj(g) −¯x1,k
j(g)
∥¯tj(g) −¯x1,k
j(g)∥2
.
(112)
From (71), we have
r1,k
j(g) =
λwg
∥x1,k
j (g) −tk
j(g)∥2
x1,k
j(g) + (
∥x1,k
j (g) −tk
j(g))∥2 −λwg
∥x1,k
j (g) −tk
j(g)∥2
)tk
j(g)
=
λwg
∥x1,k
j (g) −tk
j(g)∥2
(¯x1,k
j(g) + δkuk
j(g) + o(δk)) + (
∥x1,k
j (g) −tk
j(g))∥2 −λwg
∥x1,k
j(g) −tk
j(g)∥2
)(¯tj(g) + o(δk))
=
λwgδk
∥x1,k
j(g) −tk
j(g)∥2
uk
j(g) +
λwg
∥x1,k
j(g) −tk
j(g)∥2
(¯x1,k
j(g) −¯tj(g)) + ¯tj(g) + o(δk)
=
λwgδk
∥x1,k
j(g) −tk
j(g)∥2
uk
j(g) + (
λwg
∥¯tj(g) −¯x1,k
j(g)∥2
−
λwg
∥tk
j(g) −x1,k
j(g)∥2
)(¯tj(g) −¯x1,k
j(g)) + o(δk)
=
λwgδk
∥¯tj(g) −¯x1,k
j(g) −δkuk
j(g) + o(δk)∥2
uk
j(g)
+ (
λwg
∥¯tj(g) −¯x1,k
j(g)∥2
−
λwg
∥¯tj(g) −¯x1,k
j(g) −δkuk
j(g) + o(δk)∥2
)(¯tj(g) −¯x1,k
j(g)) + o(δk)
where the second equality comes from (84) and (109). The forth equality follows from (112).
Finally, we use (84) and (109) in the last equality. From the Taylor expansion of ∥· ∥−1
2
and
given that ∇x∥x∥−1
2
= −x/∥x∥3
2, we have
1
∥¯tj(g) −¯x1,k
j(g) −δkuk
j(g) + o(δk)∥2
=
1
∥¯tj(g) −¯x1,k
j(g)∥2
−
D
¯tj(g) −¯x1,k
j(g), −δkuk
j(g) + o(δk)
E
∥¯tj(g) −¯x1,k
j(g)∥3
2
+ o(∥−δkuk
j(g) + o(δk)∥2)
=
1
∥¯tj(g) −¯x1,k
j(g)∥2
+
D
¯tj(g) −¯x1,k
j(g), δkuk
j(g)
E
∥¯tj(g) −¯x1,k
j(g)∥3
2
+ o(δk).
Using this back in the last equation for r1,k
j(g) and rearranging the terms, we have
r1,k
j(g) =
λwgδk
∥¯tj(g) −¯x1,k
j(g)∥2
uk
j(g) −
λwg
D
¯tj(g) −¯x1,k
j(g), δkuk
j(g)
E
∥¯tj(g) −¯x1,k
j(g)∥3
2
(¯tj(g) −¯x1,k
j(g)) + o(δk)
=
λwgδk
∥¯tj(g) −¯x1,k
j(g)∥2
uk
j(g) −
D
¯tj(g), δkuk
j(g)
E
∥¯tj(g) −¯x1,k
j(g)∥2
(
¯tj(g)
λwg
) + o(δk),
35

where the second equality uses (112).
Multiplying both sides by
∥¯tj(g)−¯x1,k
j(g)∥2
λwgδk
and using
(80),(81) and ∥¯tj(g)∥2 = λwg (from (112)) yields in the limit
0 = ¯uj(g) −

¯tj(g), ¯uj(g)

∥¯tj(g)∥2
2
¯tj(g).
(113)
Thus ¯uj(g) is a nonzero multiple of ¯tj(g). In this case, since we assume ¯x1,k
j(g) ̸= 0, from (111),
we know ¯x1,k
j(g) is a negative multiple of ¯tj(g). So (105) is satisﬁed for ϵ suﬃciently small.
3. In this case, we assume ¯x1,k
j(g) = 0, ∀k, from (81), we have ¯x1
j(g) = 0. We also assume that
∥x1,k
j(g) −tk
j(g)∥2 > λwg for all k, this implies ∥¯tj(g)∥2 ≥λwg. From the optimality condition
for (10) for x1 we have
0 = ¯tj(g) + λwg∂∥0∥2,
which implies ∥¯tj(g)∥2 ≤λwg. Thus ∥¯tj(g)∥2 = λwg. Then (71) implies
r1,k
j(g) =
λwg
∥x1,k
j (g) −tk
j(g)∥2
x1,k
j(g) + (
λwg
∥¯tj(g)∥2
−
λwg
∥x1,k
j (g) −tk
j(g)∥2
)tk
j(g)
=
λwg
∥x1,k
j (g) −tk
j(g)∥2
x1,k
j(g) +
λwg
D
¯tj(g), tk
j(g) −x1,k
j(g) −¯tj(g)
E
∥¯tj(g)∥3
2
tj(g)
+ o(∥tk
j(g) −x1,k
j (g) −¯tj(g)∥2)
=
λwg
∥x1,k
j (g) −tk
j(g)∥2
x1,k
j(g) −
λwg
D
¯tj(g), x1,k
j(g)
E
∥¯tj(g)∥3
2
tj(g) + o(δk)
where the second equality uses Taylor expansion similar to the case 2. The third equality
follows from (109) and {x1,k
j(g)} →0. Dividing both sides by δk yield in the limit (113), where
it uses
{
x1,k
j(g)
δk } = {uk
j(g) + o(δk)
δk
} →¯uj(g).
Since we assume ∥x1,k
j(g) −tk
j(g)∥2 > λwg for all k, we have the following equality from (70)
0 = λwg
r1,k
j(g) −x1,k
j(g)
∥r1,k
j(g) −x1,k
j(g)∥2
+ r1,k
j(g) −tk
j(g).
(114)
Suppose ¯uj(g) ̸= 0. Then uk
j(g) =
x1,k
j(g)
δk
+ o(δk)
δk
̸= 0, for k suﬃciently large. It implies that
x1,k
j(g) ̸= 0, for k suﬃciently large. Hence,
36


¯tj(g), ¯uj(g)

=
lim
k→+∞
D
tk
j(g), uk
j(g)
E
=
lim
k→+∞
*
r1,k
j(g),
x1,k
j(g)
δk
+
+
*
λwg
r1,k
j(g) −x1,k
j(g)
∥r1,k
j(g) −x1,k
j(g)∥2
,
x1,k
j(g)
δk
+
=
lim
k→+∞
λwg
∥r1,k
j(g) −x1,k
j(g)∥2
(
D
r1,k
j(g), x1,k
j(g)
E
δk
−
∥x1,k
j(g)∥2
2
δk
)
=
lim
k→+∞
λwg
∥
r1,k
j(g)
∥x1,k
j(g)∥2 −
x1,k
j(g)
∥x1,k
j(g)∥2 ∥2
(
*
r1,k
j(g)
δk ,
x1,k
j(g)
∥x1,k
j(g)∥2
+
−∥uk
j(g)∥2)
= −λwg∥uk
j(g)∥2 < 0,
where the second equality is based on (114) and limk→+∞uk
j(g) = limk→+∞
x1,k
j(g)
δk , the third
equality is based on
r1,k
j(g)
δk
→0 (by (80)), the forth equality is based on x1,k
j(g) ̸= 0 and
limk→+∞uk
j(g) = limk→+∞
x1,k
j(g)
δk , and, the ﬁfth equality is based on r1,k
j(g) →0 and
r1,k
j(g)
δk
→0
(by (80)). Finally, combined with (113), we obtain ¯uj(g) is a negative multiplier of ¯tj(g).
Since ¯x1
j(g) = 0 in this case, (105) is satisﬁed.
E
Proof of Theorem 3.1
From Lemmas 2.4 and 2.5 in [20], we have the following two identities, respectively:
L(xk; yk) −L(xk+1; yk) ≥ρ∥xk −xk+1∥2,
(115)
∥˜∇L(xk; yk)∥≤σ∥xk −xk+1∥,
(116)
where σ =
√
2(max{1 + ∥M T ∥∥M∥, ρ} + 1).
Lemma 3.2 in our paper establishes the uniform boundedness of iterates {(xk, yk)}, based on
which the compactness condition of Lemma 3.3 is satisﬁed. Lemma 3.3 quantiﬁes the primal error
bound with the proximal gradient of the Lagrangian function as
∥xk −¯xk∥≤τp∥˜∇L(xk; yk)∥≤τpσ∥xk −xk+1∥,
(117)
where ¯xk = argmin¯x∈X(yk) ∥¯x −xk∥and the second inequality comes from (116).
In Lemma 3.1, we show
dist(y, Y ∗) ≤τd∥∇gρ(y)∥2,
(118)
where τd = max{∥M T ∥2, 2ρ} as shown in the proof of Lemma 3.1. Based on the dual error bound
(118) and following Lemma 3.1 in [20], we have
∆k
d ≤τ ′∥∇gρ(yk)∥2 = τ ′∥¯x1,k −¯x2,k∥2,
(119)
∆k
p ≤ζ∥xk −xk+1∥2 + ζ′∥xk −¯xk∥2 ≤(ξ + ξ′τ 2
pσ2)∥xk+1 −xk∥2,
(120)
37

where ¯x1,k and ¯x2,k represent the upper half and lower half of the vector ¯xk, correspondingly,
τ ′ = τ 2
d/ρ, and
ζ = 2A + 3
√
2
2 (σ −1),
(121)
ζ′ = 2A + 1
2 +
√
2
2 (σ −1),
(122)
where A = ∥M T ∥∥M∥+
√
2ρ.
Following Lemmas 3.2 and 3.3 in [20], we have
∆k
d −∆k−1
d
≤−α(x1,k −x2,k)T (¯x1,k −¯x2,k),
(123)
∆k
p −∆k−1
p
≤α∥x1,k −x2,k∥2 −γ∥xk+1 −xk∥2 −α(x1,k −x2,k)T (¯x1,k −¯x2,k),
(124)
where ∆k
p and ∆k
d are primal and dual optimality gaps at iteration k (deﬁned in the statement of
Theorem 3.1), α is the stepsize, and in (124), we have used (115). Adding (123) and (124), we have
[∆k
d + ∆k
p] −[∆k−1
d
+ ∆k−1
p
] ≤α∥x1,k −x2,k∥2 −γ∥xk+1 −xk∥2 −2α(x1,k −x2,k)T (¯x1,k −¯x2,k)
(125)
= α∥x1,k −x2,k −¯x1,k + ¯x2,k∥2 −α∥¯x1,k −¯x2,k∥2 −ρ∥xk+1 −xk∥2
(126)
≤(2ατ 2
pσ2 −ρ)∥xk+1 −xk∥2 −α∥¯x1,k −¯x2,k∥2,
(127)
where the last inequality comes from (117) and the Cauchy-Schwarz inequality.
Assuming that the stepsize α is chosen suﬃcient small such that 0 < α <
ρ
2τ 2
pσ2 , and substituting
(119) and (120) into (127), we have
[∆k
d + ∆k
p] −[∆k−1
d
+ ∆k−1
p
] ≤−ρ −2ατ 2
pσ2
ξ + ξ′τpσ2 ∆k
p −α
τ ′ ∆k
d
(128)
≤−min{ρ −2ατ 2
pσ2
ξ + ξ′τpσ2 , α
τ ′ }[∆k
d + ∆k
p].
(129)
Therefore, we have
0 ≤[∆k
p + ∆k
d] ≤
1
λ + 1[∆k−1
p
+ ∆k−1
d
],
(130)
where λ = min{
ρ−2ατ 2
pσ2
ζ+ζ′τ 2
pσ2 , α
τ ′ } > 0. Therefore, [∆k
p + ∆k
d] ≤(
1
λ+1)k[∆0
p + ∆0
d], implying that ∆k
p and
∆k
d converges to zero Q-linearly.
38

