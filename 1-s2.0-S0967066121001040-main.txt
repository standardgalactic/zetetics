Control Engineering Practice 112 (2021) 104827
Contents lists available at ScienceDirect
Control Engineering Practice
journal homepage: www.elsevier.com/locate/conengprac
Quadrotor going through a window and landing: An image-based visual
servo control approachâœ©
Zhiqi Tang a,b,âˆ—, Rita Cunha a, David Cabecinhas a, Tarek Hamel b,c, Carlos Silvestre d,1
a ISR, IST, Universidade de Lisboa, Portugal
b I3S-CNRS, UniversitÃ© CÃ´te dâ€™Azur, Nice-Sophia Antipolis, France
c Institut Universitaire de France, France
d Faculty of Science and Technology of the University of Macau, Macao, China
A B S T R A C T
This paper considers the problem of controlling a quadrotor to go through a window and land on a planar target, the landing pad, using an Image-Based Visual
Servo (IBVS) controller that relies on sensing information from two on-board cameras and an IMU. The maneuver is divided into two stages: crossing the window
and landing on the pad. For the first stage, a control law is proposed that guarantees that the vehicle will not collide with the wall containing the window
and will go through the window with non-zero velocity along the direction orthogonal to the window, keeping at all times a safety distance with respect to
the window edges. For the landing stage, the proposed control law ensures that the vehicle achieves a smooth touchdown, keeping at all time a positive height
above the plane containing the landing pad. For control purposes, the centroid vectors provided by the combination of the spherical image measurements of a
collection of landmarks (corners) for both the window and the landing pad are used as position measurement. The translational optical flow relative to the wall,
window edges, and landing plane is used as velocity cue. To achieve the proposed objective, no direct measurements nor explicit estimate of position or velocity
are required. Simulation and experimental results are provided to illustrate the performance of the presented controller.
1. Introduction
Navigation of Unmanned Aerial Vehicles (UAVs) using vision sys-
tems has been an important field of research during recent decades.
Especially in indoor environments, where GPS is unavailable, a widely
adopted alternative sensor suite includes an inertial measurement unit
(IMU) and cameras, which are both passive, lightweight, and inex-
pensive sensors (Zingg et al., 2010). Three main solutions have been
proposed for navigation using vision in indoor environments: map-
based navigation, map-building-based navigation and mapless naviga-
tion (DeSouza & Kak, 2002). The first approach depends on a user-
created geometric model or topology map of the environment, e.g. Per-
spective n Point (PnP), and the second requires the use of sensors to
construct their own geometric or topological models, e.g. Simultaneous
localization and mapping (SLAM). The mapless visual navigation, in
which no global representation of the environment is required and the
environment is perceived as the system navigates, can be classified in
accordance with the main vision technique or types of clues used during
the navigation, which are methods based on optical flow, appearance
âœ©This work was supported by the Macao Science and Technology Development Fund under Grant FDCT/0031/2020/AFJ; by the University of Macau, Macau,
China,
under the Project MYRG2018-00198-FST; by the FundaÃ§ao para a CiÃªncia e a Tecnologia (FCT) through LARSyS - FCT Project UIDB/50009/2020 and
PTDC/EEI-AUT/32107/2017 and by FCT Scientific Employment Stimulus grant CEECIND/04199/2017. The work of T. Hamel was supported by the French
National Research Agency (ANR) under the project DACAR. The work of Z. Tang was supported by FCT through Ph.D. Fellowship PD/BD/114431/2016 under
the FCT-IST NetSys Doctoral Program.
âˆ—Corresponding author at: ISR, IST, Universidade de Lisboa, Portugal.
E-mail addresses: zhiqitang@tecnico.ulisboa.pt (Z. Tang), rita@isr.tecnico.ulisboa.pt (R. Cunha), dcabecinhas@isr.tecnico.ulisboa.pt (D. Cabecinhas),
thamel@i3s.unice.fr (T. Hamel), csilvestre@umac.mo (C. Silvestre).
1 On leave from ISR, IST, Universidade de Lisboa, Portugal.
and feature tracking (DeSouza & Kak, 2002). However, the appearance-
based method has the main problems which are to find an appropriate
algorithm for the representation of the environment and to define the
on-line matching criteria.
Visual servo control is a popular mapless navigation method based
on feature tracking, which can be classified in two main categories:
Image-based visual servo (IBVS) and Position-based visual servo (PBVS)
control. PBVS involves reconstruction of the target pose with respect
to the robot thus a 3-D model of the observed object should be
known. However in IBVS, the control commands are deduced directly
from image features, thus they offer advantages in robustness to cam-
era and target calibration errors, reduced computational complexity,
and simple extension to applications involving multiple cameras com-
pared to PBVS methods (Hutchinson et al., 1996). However, classical
IBVS (Chaumette et al., 2016) suffers from three key problems. First,
it is necessary to determine the depth of each visual feature used in
the image error criterion independently from the control algorithm.
Second, the rigid-body dynamics of the camera ego-motion are highly
https://doi.org/10.1016/j.conengprac.2021.104827
Received 8 September 2020; Received in revised form 9 April 2021; Accepted 13 April 2021
Available online 3 May 2021
0967-0661/Â© 2021 Elsevier Ltd. All rights reserved.

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 1. 3-D plot of the quadrotor trajectories under different initial conditions.
coupled when expressed as target motion in the image plane. And
last, it uses a simple linearized control on the image kinematics that
leads to complex non-linear dynamics and is not easily extended to the
dynamics.
In order to overcome these problems, a spherical camera geometry
can be used, from which the virtual spherical image points can be
obtained by transforming the image points on the perspective camera to
the view that would be seen by an ideal unified-spherical camera. Novel
IBVS algorithms based on spherical image centroids (e.g. applications
on hovering an autonomous helicopter (Hamel & Mahony, 2002),
landing a quadrotor on moving platform (HÃ©rissÃ© et al., 2012; Serra
et al., 2016) and landing a fixed-wing aircraft on the runway (Le Bras
et al., 2014; Serra et al., 2015; Tang et al., 2018a)), do not require
accurate depth information for observed image features and overcomes
the difficulties associated with the highly coupled dynamics of the
camera ego-motion in the image dynamics.
This paper extends the IBVS control solution based on spherical
image centroids to a specific problem of steering a quadrotor to move
from one room to a second one by crossing a window and then land
on a planar target placed in the second room (see Fig. 1). This appli-
cation has significant practical interest since many tasks (e.g. search
and rescue in an earthquake-damaged building (Michael et al., 2012),
package delivery using UAVs) require UAVs to land on a final des-
tination or to perform intermediate landings for battery recharge or
exchange, or refueling (for larger UAVs) during long missions. The
quadrotor is assumed to be equipped with an IMU and two on-board
cameras: one forward-looking and another downward-looking. Neither
the translational velocity and position of the vehicle nor the location of
the target (window and landing pad) are known. In the proposed IBVS
control laws, the centroid vectors provided by the combination of the
spherical image measurements of a collection of landmarks (corners)
from both the window and the landing pad are used as position cue and
the translational optical flow relative to the plane containing window
and landing pad is used as velocity cue.
The proposed control strategy draws inspiration from (Serra et al.,
2016) which combines a centroid-like feature and the translational
optical flow to perform exponential landing on a desired spot. This
paper considers different control objectives: going through a window
and then landing on a desired target. The control law for going through
a window ensures that no collision with the wall or windows edges
will occur and the vehicle will align with the center line orthogonal
to the window, crossing it with non-zero velocity. The control law for
the landing is an improvement with respect to the one used in Serra
et al. (2016) in which the desired optical flow was chosen constant,
leading to a high-gain controller that fails to achieve a perfect landing
maneuver. Conversely, the desired optical flow adopted in this work
is not constant. It corresponds to the component of the image centroid
in the direction orthogonal to the target plane leading to a vanishing
desired optical flow when the distance to the target approaches zero
and therefore one avoids the high-gain nature of prior work (Serra
et al., 2016).
Following on previous work (Tang et al., 2018b), which presents
the preliminary results with only simulations, this paper presents the
following novel contributions: i) bounded disturbances (e.g. due to
wind, and unmodeled dynamics) are included in the dynamics of the
system; ii) a complete stability analysis shows that convergence to
the desired zero-height equilibrium is guaranteed in all cases and ulti-
mate boundedness of the horizontal position error is guaranteed when
landing in the presence of horizontal disturbances; iii) experimental
results are provided where the controllers run on an onboard computer
together with the image processing for the detection of window and
landing pad and for the computation of the translational optical flow.
The body of the paper consists of seven parts. Section 2 presents
the dynamic model, the fundamental equations of motion, and the
adopted hierarchical control architecture. Section 3 introduces the
environment and presents the image features that are used in the
control laws. Section 4 proposes two control laws: one for the landing
task in obstacle-free environments and the other for flying through the
window. A combination of these two control laws in the practical case
is also presented in this section. Section 5 shows simulation results
obtained with the proposed controller. Section 6 presents and analyzes
the experimental results which validate the proposed controllers. The
paper concludes with some final comments in Section 7.
1.1. Related work
There are several examples in the literature of recent work dedi-
cated to the problem of flying autonomous vehicles in complex envi-
ronment using vision systems. In Falanga et al. (2017), Loianno et al.
(2017) and Guo and Leang (2020), the authors specifically address
the problem of going through a window using only a single camera
and an IMU. However, estimation of vehicleâ€™s position and velocity is
required in Falanga et al. (2017) and Loianno et al. (2017). Besides, the
pose of the window is assumed to be known in Loianno et al. (2017).
Although the work in Guo and Leang (2020) directly uses image feature
as position cue, estimates of the image depth are still required and the
velocity vector is assumed to be known. In general, state estimation
adds computational complexity, and the output is often sensitive to
image noise and camera calibration errors. The limited work on image-
based control approach can be explained by the complexity involved in
obtaining sound proofs of convergence and stability.
Landing in complex environments calls for obstacle avoidance ca-
pabilities, which are naturally provided by the use of optical flow, a
visual feature that draws inspiration from flying insects. Optical flow
measures the pattern of apparent motion of objects, surfaces, and edges
in a visual scene caused by the relative motion between an observer
and a scene (Burton & Radford, 1978). It has been experimentally
shown that the neural system of the insects reacts to optic flow pat-
terns to produce a large variety of flight capabilities, such as obstacle
avoidance, speed maintenance, odometry estimation, wall following
and corridor centering, altitude regulation, orientation control and
landing (Floreano & Wood, 2015; Serres & Ruffier, 2017). Using optical
flow as velocity cue and observed feature expressed in terms of an
unnormalized spherical centroid, a fully nonlinear adaptive visual servo
control design is provided in Mahony et al. (2008). Although estimating
the height of the camera above the landing plane was still required, it
was the first time that an IBVS control using image measurements for
both position and velocity was proposed, going beyond the kinematic
model to consider the dynamics. Based on Mahony et al. (2008) and
using optical flow, the authors proposed IBVS controllers for landing a
quadrotor (HÃ©rissÃ© et al., 2012; Serra et al., 2016) and landing a fixed-
wing aircraft eliminating the need to estimate the height of the vehicle
2

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 2. Reference frames and force for schematic representation of a quadrotor.
above the ground (Le Bras et al., 2014; Serra et al., 2015; Tang et al.,
2018a). Using a distinct paradigm, a novel setup of self-supervised
learning based on optical flow was introduced in Ho et al. (2018).
Using optical flow, the proposed method learns the visual appearance of
obstacles in order to search for a landing spot for micro aerial vehicles.
When compared to related work, this paper proposes simple IBVS
controllers applied in sequence to first go through a window and then
land on a planar target, using only vision measurements and requiring
no estimation of position, velocity, image depth, nor height above the
target. The present work also provides rigorous mathematical proofs for
stability and robustness in the presence of disturbances, complemented
by experimental validation of the proposed controllers.
2. Quadrotor modeling and control architecture
Consider a quadrotor equipped with an IMU and two cameras.
To describe the motion of the quadrotor, two reference frames are
introduced: an inertial reference frame {ğ¼} fixed to the earth surface
and a body-fixed frame {ğµ} = (ğ‘’ğ‘
1, ğ‘’ğ‘
2, ğ‘’ğ‘
3) attached to the quadrotorâ€™s
center of mass (see Fig. 2). Let ğ‘…= ğ¼
ğµğ‘…âˆˆğ‘†ğ‘‚(3) denote the orientation
of the frame {ğµ} with respect to {ğ¼} and let ğœ‰âˆˆR3 be the position
of the origin of the frame {ğµ} with respect to {ğ¼}. Let ğ‘£
âˆˆ
R3
denote the translational velocity expressed in {ğ¼} and ğ›ºâˆˆR3 the
orientation velocity expressed in {ğµ}. The kinematics and dynamics of
the quadrotor vehicle are then described as
{
Ì‡ğœ‰= ğ‘£
ğ‘šÌ‡ğ‘£= âˆ’ğ¹+ ğ‘šğ‘”ğ‘’3+ â–µ
(1)
{
Ì‡ğ‘…= ğ‘…ğ›ºÃ—
I Ì‡ğ›º= âˆ’ğ›ºÃ—Iğ›º+ ğ›¤
(2)
with ğ‘”the gravitational acceleration, ğ‘šthe mass of the vehicle, ğ‘’3 =
[0 0 1]âŠ¤and I its inertia matrix. The matrix ğ›ºÃ— denotes the skew-
symmetric matrix associated with the vector product ğ›ºÃ—ğ‘¥= ğ›ºÃ— ğ‘¥, for
any ğ‘¥âˆˆR3.
The vector ğ¹âˆˆR3 expressed in {ğ¼} combines the principal non-
conservative forces applied to the quadrotor and generated by the four
rotors. In quasi-hover conditions one can reasonably assume that this
aerodynamic force is always in the direction ğ‘’ğ‘
3 in {ğµ}, since all the
four thrusters are aligned with ğ‘’ğ‘
3 and their contribution predominates
over other components. Thus the ğ¹in the direction of ğ‘’ğ‘
3 expressed in
the inertial frame can be described as follows:
ğ¹= ğ¹ğ‘‡ğ‘…ğ‘’3
(3)
where the scalar ğ¹ğ‘‡represents the total thrust magnitude generated
by the four motors. It also represents the unique control input for the
translational dynamics.
The term â–µcombines the modeling errors and aerodynamic effects
due to the interaction of the rotors wake with the environment causing
random wind and dynamic inflow effects (Peters & HaQuang, 1988).
The vector ğ›¤âˆˆR3 expressed in {ğµ} is the torque control for the at-
titude dynamics. It is obtained via the combination of the contributions
of four rotors. The invertible linear map between [ğ¹ğ‘‡âˆˆR+, ğ›¤âˆˆR3] and
the collection of individual thrusters [ğ¹ğ‘‡1, ğ¹ğ‘‡2, ğ¹ğ‘‡3, ğ¹ğ‘‡4] can be found
in Hamel et al. (2002).
Fig. 3. A hierarchical control design strategy.
Fig. 4. Landing plane and window plane.
2.1. Control architecture
A hierarchical control design strategy is adopted in this paper (see
Fig. 3). This choice is motivated by the natural structure of the sys-
tem dynamics and its practical implementation (Bertrand et al., 2011;
HÃ©rissÃ© et al., 2012). For the translational dynamics of the quadrotor
(Eq. (1)), the force ğ¹(Eq. (3)) is used as control input by means of its
thrust direction and its magnitude. This constitutes a high-level outer
loop for the control design. The thrust ğ¹ğ‘‡is directly the magnitude
of the designed force (ğ¹ğ‘‡= â€–ğ¹â€–) and the desired attitude ğ‘…ğ‘‘(partly
obtained by the desired direction ğ‘…ğ‘‘ğ‘’3 =
ğ¹
â€–ğ¹â€– complemented by a
desired yaw) can then be reached by considering the bodyâ€™s angular
velocity ğ›ºas an intermediary control input, which constitutes again
a desired angular velocity for the fully actuated orientation dynamics
(Eq. (2)) via the high gain control torque ğ›¤. The stabilization of the
orientation dynamics is not the subject of this paper and it is assumed
that a suitable low level robust stabilizing control is implemented, that
satisfactorily regulates the attitude error with a fast dynamics.
3. Environment and image features
In this section adequate image features in relation to the consid-
ered tasks are derived and all required assumptions regarding the
environment and the setup are established.
Assumption 1.
A downward-looking camera and a forward-looking
camera are attached to the center of mass of the vehicle. The
downward-looking camera reference frame coincides with the body-
fixed frame {ğµ}. The rotation matrix from the forward-looking camera
reference frame to the body frame ğµ
ğ¶ğ‘…âˆˆğ‘†ğ‘‚(3) is known.
Assumption 2.
The angular velocity ğ›ºis measured and the orien-
tation matrix ğ‘…of {ğµ} with respect to {ğ¼} is obtained by external
3

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
observer-based IMU measurements. This allows to represent all image
information and the system dynamics in the inertial frame.
Assumption 3. The landing target lies on a textured plane which is
called target plane. Its normal direction ğœ‚ğ‘¡âˆˆS2 in the inertial frame is
known (typically ğœ‚ğ‘¡â‰ˆğ‘’3), where S2 âˆ¶= {ğ‘¦âˆˆR2 âˆ¶â€–ğ‘¦â€– = 1} denotes the
2-Sphere and â€–.â€– the Euclidean norm.
Assumption 4. The target window has a rectangle shape and lies on a
textured wall which is called window plane. Its width ğ‘Ÿğ‘¤is known but
its normal direction ğœ‚ğ‘¤âˆˆS2 is unknown.
Both landing plane and window plane are placed in the environ-
ment, as shown in Fig. 4. It assumed that the vehicle is able to recognize
the landing pad and the window from landmarks on the pad and from
corners and edges of the window respectively. The background texture
on both landing plane and window plane are also exploited to obtain
information about the vehicleâ€™s velocity with respect to the planes and
also to avoid collisions with the wall and the windowâ€™s edges.
For any initial position (along with any initial velocity) outside the
room containing the landing pad, the main objective is to design a
feedback controller resorting only to image features that can ensure
automatic landing of the vehicle without any collision.
3.1. Image features on the landing plane
The target on the landing plane is depicted in Fig. 4. The axes of {ğ¼}
are given by (ğ‘¢ğ‘¡, ğœŒğ‘¡, ğœ‚ğ‘¡), where ğœŒğ‘¡= ğœ‚ğ‘¡Ã—ğ‘¢ğ‘¡, and the origin of {ğ¼} is placed
at the center of the landing pad. As shown in Fig. 4, ğ‘ ğ‘¡
ğ‘–âˆˆR3 denotes the
position of ğ‘–th marker (or a corner) of the landing pad relative to the
inertial frame expressed in {ğ¼}. Note that ğœ‚âŠ¤
ğ‘¡ğ‘ ğ‘¡
ğ‘–= 0. Define the position
vector of ğ‘–th marker of the target relative to {ğµ} as
ğ‘ƒğ‘¡
ğ‘–= ğ‘ ğ‘¡
ğ‘–âˆ’ğœ‰.
(4)
The position of the vehicle relative to the center of the landing pad is
defined as
ğœ‰ğ‘¡= âˆ’1
ğ‘›ğ‘¡
ğ‘›ğ‘¡
âˆ‘
ğ‘–=1
ğ‘ƒğ‘¡
ğ‘–= ğœ‰âˆ’1
ğ‘›ğ‘¡
ğ‘›ğ‘¡
âˆ‘
ğ‘–=1
ğ‘ ğ‘¡
ğ‘–
(5)
where ğ‘›ğ‘¡is the number of observed markers on the landing pad and
1
ğ‘›ğ‘¡
âˆ‘ğ‘›ğ‘¡
ğ‘–=1 ğ‘ ğ‘¡
ğ‘–is a constant vector. This sum is zero when all markers are in
the camera field of view.
Using the spherical projection model for a calibrated camera, the
spherical image points of landing padâ€™s markers can be expressed as
ğ‘ğ‘¡
ğ‘–=
ğ‘ƒğ‘¡
ğ‘–
â€–ğ‘ƒğ‘¡
ğ‘–â€– =
ğ‘ ğ‘¡
ğ‘–âˆ’ğœ‰
â€–ğ‘ ğ‘¡
ğ‘–âˆ’ğœ‰â€–
(6)
which can be obtained from the 2D pixel locations (ğ‘‹ğ‘¡
ğ‘–, ğ‘Œğ‘¡
ğ‘–) of the
camera image, such that
ğ‘ğ‘¡
ğ‘–= ğ‘…
Ì„ğ‘ğ‘¡
ğ‘–
â€– Ì„ğ‘ğ‘¡
ğ‘–â€– , with Ì„ğ‘ğ‘¡
ğ‘–= ğ´âˆ’1
â¡
â¢
â¢â£
ğ‘‹ğ‘¡
ğ‘–
ğ‘Œğ‘¡
ğ‘–
1
â¤
â¥
â¥â¦
.
(7)
The matrix ğ´âˆ’1 in the above equation is the cameraâ€™s intrinsic parame-
ters that transforms image pixel to perspective coordinates Ì„ğ‘ğ‘¡
ğ‘–. Note that
expressions (6) and (7) are the same and hence ğ‘ğ‘¡
ğ‘–does not depend on
the orientation.
The visual feature used for the landing task is the centroid of the
observed visual feature.
ğ‘ğ‘¡âˆ¶= âˆ’1
ğ‘›ğ‘¡
ğ‘›ğ‘¡
âˆ‘
ğ‘–=1
ğ‘ğ‘¡
ğ‘–= âˆ’ğ‘…
(
1
ğ‘›ğ‘¡
ğ‘›ğ‘¡
âˆ‘
ğ‘–=1
Ì„ğ‘ğ‘¡
ğ‘–
â€– Ì„ğ‘ğ‘¡
ğ‘–â€–
)
(8)
which is the simplest image feature that encodes all information about
the position of the vehicle with respect to the landing plane. It is
not necessary to match observed image points with desired features
as required in classical image based visual servo control. Besides, the
Fig. 5. Window plane and unit directions â„ğ‘–normal to the planes defined by the origin
of camera frame and the ğ‘–th window edge.
calculation of the image centroid is highly robust to pixel noise, and
easily computed in real-time in the camera frame and then derotated.
This ensures that ğ‘ğ‘¡is invariant to any orientation motion (Hamel &
Mahony, 2002).
3.2. Image features on the window plane
As shown in Fig. 4, a rectangular window is placed on a textured
wall. Its corners and edges are assumed to be recognized in camera
images. Both information are combined together to extract the nor-
mal direction ğœ‚ğ‘¤and provide the feedback information used in the
controller.
Consider first the windows corners and let ğ‘ ğ‘¤
ğ‘–
âˆˆR3 denote the
position of ğ‘–th corner of the window expressed in {ğ¼}. Define the
position vector of ğ‘–th corner of the window relative to {ğµ} as
ğ‘ƒğ‘¤
ğ‘–
= ğ‘ ğ‘¤
ğ‘–âˆ’ğœ‰.
(9)
From there, one can deduce the position of the vehicle with respect to
the windowâ€™s center:
ğœ‰ğ‘¤= âˆ’1
ğ‘›ğ‘¤
ğ‘›ğ‘¤
âˆ‘
ğ‘–=1
ğ‘ƒğ‘¤
ğ‘–
= ğœ‰âˆ’1
ğ‘›ğ‘¤
ğ‘›ğ‘¤
âˆ‘
ğ‘–=1
ğ‘ ğ‘¤
ğ‘–,
(10)
with ğ‘›ğ‘¤(typically ğ‘›ğ‘¤
=
4) number of the windowâ€™s corners and
1
ğ‘›ğ‘¤
âˆ‘ğ‘›ğ‘¤
ğ‘–=1 ğ‘ ğ‘¤
ğ‘–constant vector.
Similarly to Section 3.1 and recalling that the forward-looking
camera is used to detect the window, the spherical image points of the
corners of the window are exploited:
ğ‘ğ‘¤
ğ‘–=
ğ‘ƒğ‘¤
ğ‘–
â€–ğ‘ƒğ‘¤
ğ‘–â€– = ğ‘…ğµ
ğ¶ğ‘…
Ì„ğ‘ğ‘¤
ğ‘–
â€– Ì„ğ‘ğ‘¤
ğ‘–â€– ,
(11)
with Ì„ğ‘ğ‘¤
ğ‘–the perspective coordinates of the ğ‘–th windowâ€™s corner, leading
the following centroid:
ğ‘ğ‘¤(ğ‘¡) âˆ¶= âˆ’1
ğ‘›ğ‘¤
ğ‘›ğ‘¤
âˆ‘
ğ‘–=1
ğ‘ğ‘¤
ğ‘–(ğ‘¡) = âˆ’ğ‘…ğµ
ğ¶ğ‘…
(
1
ğ‘›ğ‘¤
ğ‘›ğ‘¤
âˆ‘
ğ‘–=1
Ì„ğ‘ğ‘¤
ğ‘–
â€– Ì„ğ‘ğ‘¤
ğ‘–â€–
)
,
(12)
where ğµ
ğ¶ğ‘…is the rotation matrix from the forward-looking camera
reference frame to the body frame.
Now, to extract the normal direction ğœ‚ğ‘¤, recall that the axes rep-
resenting the window are given by (ğœ‚ğ‘¤, ğœŒğ‘¤, ğ‘¢ğ‘¤), with ğœŒğ‘¤= ğ‘¢ğ‘¤Ã— ğœ‚ğ‘¤
(see Fig. 5). Using the image of ğ‘–th line and exploiting the fact that
the window has a rectangular shape, it is straightforward to get the
directions ğ‘¢ğ‘¤and ğœŒğ‘¤and consequently ğœ‚ğ‘¤. As described in Mahony and
Hamel (2005), in the binormalized Euclidean Plucker coordinates, the
ğ‘–th line can be represented by its unit direction ğ‘¢ğ‘¤(resp. ğœŒğ‘¤) and the
unit direction â„ğ‘–, which is normal to the plane defined by the origin of
the camera/body-fixed frame and the ğ‘–th line. The unit vector â„ğ‘–can be
obtained directly from the images of lines which can be identified using
a convenient line detection technique, such as the Hough transform.
4

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 6. The green volume represents the region î‰ƒdefined by inequality (17) which
excludes the window edges. (For interpretation of the references to color in this figure
legend, the reader is referred to the web version of this article.)
Using the fact that lines 1 and 3 (resp. lines 2 and 4) are parallel in the
inertial frame, one deduces the measure of the direction ğ‘¢ğ‘¤(resp. ğœŒğ‘¤)
from the following relationships:
ğœŒğ‘¤= Â± â„1 Ã— â„3
â€–â„1 Ã— â„3â€–
(13)
ğ‘¢ğ‘¤= Â± â„2 Ã— â„4
â€–â„2 Ã— â„4â€– .
(14)
Then the normal vector to the window plane is directly obtained by
ğœ‚ğ‘¤= Â± ğ‘¢ğ‘¤Ã— ğœŒğ‘¤
â€–ğ‘¢ğ‘¤Ã— ğœŒğ‘¤â€–
(15)
and the sign of Eq. (15) is chosen such that the condition ğœ‚âŠ¤
ğ‘¤ğ‘ğ‘¤(0) < 0,
with ğ‘ğ‘¤(ğ‘¡) the image centroid of windowâ€™s corners in Eq. (12).
To exploit the image of windowâ€™s edges, defining the vector from the
vehicle to the closest point on windowâ€™s edges as ğ¿ğ‘’âˆˆR3, its direction
ğ‘™ğ‘’=
ğ¿ğ‘’
â€–ğ¿ğ‘’â€– can be obtained from the camera
ğ‘™ğ‘’= {ğ‘™ğ‘’
ğ‘–âˆ¶max{|ğœ‚âŠ¤
ğ‘¤ğ‘™ğ‘’
ğ‘–|}, ğ‘–= {1, 2, 3, 4}}
(16)
where
ğ‘™ğ‘’
ğ‘–= Â±(â„ğ‘–Ã— ğœŒğ‘¤), ğ‘–= {1, 3}, ğ‘™ğ‘’
ğ‘–= Â±(â„ğ‘–Ã— ğ‘¢ğ‘¤), ğ‘–= {2, 4}
are the directions from the vehicle to the nearest point on each edge ğ‘–.
Form now on, it is able to derive the required information achieving
the double goal of going through the window in the meanwhile avoid-
ing collision with the window edges and wall. We first define the safety
region î‰ƒsuch that
î‰ƒâˆ¶= {ğœ‰ğ‘¤âˆ¶â€–ğ‘ğ‘¤(ğœ‰ğ‘¤)â€– â‰¤ğœ–},
(17)
where ğœ–> 0 is chosen such that âˆ€ğœ‰ğ‘¤âˆˆî‰ƒ, the condition â€–ğœ‰ğ‘¤â€– < ğ‘Ÿğ‘¤
2 âˆ’ğœ–
also holds, implying that the region î‰ƒdoes not contain the window
edges (see Fig. 6).
From there, the chosen visual feature that encodes all required in-
formation about the position of the vehicle with respect to the window
is:
Ì„ğ‘ğ‘¤âˆ¶= âˆ’1
ğ‘›ğ‘¤
âˆ‘ğ‘›ğ‘¤
ğ‘–=1 ğ‘ğ‘¤
ğ‘–
[
ğ›¼ğ‘¤(ğ‘¡)
1
ğœ‚âŠ¤ğ‘¤ğ‘ğ‘¤
ğ‘–
+ (1 âˆ’ğ›¼ğ‘¤(ğ‘¡))
ğœ‚âŠ¤
ğ‘¤ğ‘™ğ‘’
ğœ‚âŠ¤ğ‘¤ğ‘ğ‘¤
ğ‘–
]
,
(18)
where ğ›¼ğ‘¤(â€–ğ‘ğ‘¤(ğœ‰ğ‘¤)â€–) is a weight function ensuring the continuity of Ì„ğ‘ğ‘¤.
It is defined as follows:
ğ›¼ğ‘¤(ğ‘¡) =
â§
âª
â¨
âªâ©
0
, if â€–ğ‘ğ‘¤â€– â‰¤ğœ–(ğœ‰ğ‘¤âˆˆî‰ƒ)
1
ğ›¿(â€–ğ‘ğ‘¤â€– âˆ’ğœ–)
, if ğœ–< â€–ğ‘ğ‘¤â€– < ğœ–+ ğ›¿
1
, if â€–ğ‘ğ‘¤â€– â‰¥ğœ–+ ğ›¿,
(19)
with ğ›¿an arbitrary small positive constant. Since ğœ‚âŠ¤
ğ‘¤ğ‘™ğ‘’=
ğœ‚âŠ¤
ğ‘¤ğ¿ğ‘’
â€–ğ¿ğ‘’â€– =
ğœ‚âŠ¤
ğ‘¤ğ‘ƒğ‘¤
ğ‘–
â€–ğ¿ğ‘’â€– ,
Ì„ğ‘ğ‘¤can be expressed in terms of the unknown distance ğ‘‘ğ‘œand ğ‘‘ğ‘’:
Ì„ğ‘ğ‘¤(ğ‘¡) = ğ›¼ğ‘¤(ğ‘¡) ğœ‰ğ‘¤(ğ‘¡)
ğ‘‘ğ‘œ(ğ‘¡) + (1 âˆ’ğ›¼ğ‘¤(ğ‘¡)) ğœ‰ğ‘¤(ğ‘¡)
ğ‘‘ğ‘’(ğ‘¡)
(20)
where ğ‘‘ğ‘œâˆ¶= ğœ‚âŠ¤
ğ‘¤ğ‘ƒğ‘¤
ğ‘–
= âˆ’ğœ‚âŠ¤
ğ‘¤ğœ‰ğ‘¤is the distance from the camera to the wall
and ğ‘‘ğ‘’âˆ¶= â€–ğ¿ğ‘’â€– =
âˆš
ğ‘‘2
ğ‘œ+ â€–ğœ‹ğœ‚ğ‘¤ğ¿ğ‘’â€–2 represents the distance from the
camera to the closest windowâ€™s edge.
3.3. Image kinematics and translational optical flow
The kinematics of any observed points on the landing plane (includ-
ing markers of the landing pad) can be written as:
Ì‡ğ‘ƒğ‘¡= âˆ’Ì‡ğœ‰= âˆ’ğ‘£
(21)
where ğ‘ƒğ‘¡expressed in {ğ¼} denotes any point on the textured ground of
the landing plane. So the kinematics of the corresponding image point
ğ‘ğ‘¡=
ğ‘ƒğ‘¡
â€–ğ‘ƒğ‘¡â€– can be expressed as
Ì‡ğ‘ğ‘¡= âˆ’ğœ‹ğ‘ğ‘¡
ğ‘£
â€–ğ‘ƒğ‘¡â€– .
(22)
with
ğœ‹ğ‘¦âˆ¶= ğ¼3 âˆ’ğ‘¦ğ‘¦âŠ¤â‰¥0,
the orthogonal projection operator in R3 onto the 2-dimensional vector
subspace orthogonal to any ğ‘¦âˆˆS2. Let ğ‘‘ğ‘¡be the height of the vehicle
above the landing plane:
ğ‘‘ğ‘¡âˆ¶= ğœ‚âŠ¤
ğ‘¡ğ‘ƒğ‘¡= ğœ‚âŠ¤
ğ‘¡ğ‘ƒğ‘¡
ğ‘–= âˆ’ğœ‚âŠ¤
ğ‘¡ğœ‰ğ‘¡,
(23)
then Eq. (22) can be rewritten as
Ì‡ğ‘ğ‘¡= âˆ’cos ğœƒğ‘¡ğœ‹ğ‘ğ‘¡ğœ™ğ‘¡(ğ‘¡)
(24)
where cos ğœƒğ‘¡=
ğ‘‘ğ‘¡
â€–ğ‘ƒğ‘¡â€– = ğœ‚âŠ¤
ğ‘¡ğ‘ğ‘¡and ğœ™ğ‘¡is the translational optical flow:
ğœ™ğ‘¡(ğ‘¡) = ğ‘£(ğ‘¡)
ğ‘‘ğ‘¡(ğ‘¡)
(25)
which is the ideal image velocity cue that can be complemented with
the centroid information for designing a pure IBVS controller to per-
form the landing task.
The translational optical flow ğœ™ğ‘¡can be obtained by integrating Ì‡ğ‘ğ‘¡
(24) over a solid angle ğ‘†2 of the sphere around the normal direction
ğœ‚ğ‘¡to the landing plane. It can be shown that the average of the optical
flow is calculated as in HÃ©rissÃ© et al. (2012):
ğœ™ğ‘¡(ğ‘¡) = âˆ’(ğ‘…ğ‘¡ğ›¬âˆ’1ğ‘…âŠ¤
ğ‘¡) âˆ«âˆ«ğ‘†2 Ì‡ğ‘ğ‘¡ğ‘‘ğ‘ğ‘¡,
(26)
where matrix ğ›¬is a constant diagonal matrix depending on parameters
of the solid angle ğ‘†2, and ğ‘…ğ‘¡represents the orientation matrix of the
landing plane with respect to the inertial frame. Since {ğ¼} is chosen
coincident with the target frame one has ğ‘…ğ‘¡= ğ¼3.
In practice, the optical flow is first measured in the camera frame
from the 2-D optical flow Ì‡Ì„ğ‘ğ‘¡obtained from a sequence of images using
the Lucasâ€“Kanade algorithm and then derotated (see HÃ©rissÃ© et al.,
2012 for more detail). Note however that computing the optical flow
from (26) or directly from Ì‡Ì„ğ‘ğ‘¡in the camera frame and then derotating
it, the result is theoretically the same and does not depend on the
measured ğ›ºnor on the estimated ğ‘….
Similarly, the kinematics of any observed points on the window
plane can be written in the inertial frame as
Ì‡ğ‘ƒğ‘¤= âˆ’ğ‘£
(27)
where ğ‘ƒğ‘¤expressed in {ğ¼} denotes the position of a point on the
textured wall of the window plane with respect to {ğµ} expressed in
{ğ¼}, not to be confused with ğ‘ƒğ‘¤
ğ‘–
in Eq. (9), which is the position of the
ğ‘–th corner of the window with respect to {ğµ} and also expressed in {ğ¼}.
So the kinematics of the corresponding image point ğ‘ğ‘¤=
ğ‘ƒğ‘¤
â€–ğ‘ƒğ‘¤â€– can be
written as
Ì‡ğ‘ğ‘¤= âˆ’cos ğœƒğ‘¤ğœ‹ğ‘ğ‘¤ğ‘£
ğ‘‘ğ‘œ
(28)
with cos ğœƒğ‘¤=
ğ‘‘ğ‘œ
â€–ğ‘ƒğ‘¤â€– = ğœ‚âŠ¤
ğ‘¤ğ‘ğ‘¤. Analogously to the previous case, the
translational optical flow with respect to the textured wall
ğ‘£
ğ‘‘ğ‘œcan be
5

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
obtained from the integral of Ì‡ğ‘ğ‘¤along the direction ğœ‚ğ‘¤over a solid
angle.
Now, to achieve the goal that the vehicle is going through the
window smoothly, the translational optical flow with respect to the
closest windowâ€™s edge is also used. The kinematics of any observed
points on the closest windowâ€™s edge is
Ì‡ğ‘ƒğ‘’= âˆ’ğ‘£
(29)
where ğ‘ƒğ‘’denotes the position of a point on the closest edge from the
window. The kinematics of the corresponding image point ğ‘ğ‘’=
ğ‘ƒğ‘’
â€–ğ‘ƒğ‘’â€–
can be written as
Ì‡ğ‘ğ‘’= âˆ’cos ğœƒğ‘’ğœ‹ğ‘™ğ‘’ğ‘£
ğ‘‘ğ‘’
(30)
with cos ğœƒğ‘’=
ğ‘‘ğ‘’
â€–ğ‘ƒğ‘’â€– = ğ‘™ğ‘’âŠ¤ğ‘ğ‘’. The translational optical flow with respect
to the closest window edge,
ğ‘£
ğ‘‘ğ‘’, can be obtained from the integral of Ì‡ğ‘ğ‘’
along the direction ğ‘™ğ‘’over a solid angle.
Analogously to (18), the translational optical flow used for going
through the window is the convex combination of the translational
optical flow with respect to the textured wall and to the closest window
edge, respectively:
ğœ™ğ‘¤= ğ›¼ğ‘¤(ğ‘¡) ğ‘£(ğ‘¡)
ğ‘‘ğ‘œ(ğ‘¡) + (1 âˆ’ğ›¼ğ‘¤(ğ‘¡)) ğ‘£(ğ‘¡)
ğ‘‘ğ‘’(ğ‘¡)
(31)
with ğ›¼ğ‘¤(ğ‘¡) defined already by (19).
4. Controller design
4.1. Landing in obstacle free environment
Theorem 1. Consider the system (1) in the nominal case (â–µâ‰¡0) subjected
to the following feedback control:
ğ¹ğ‘¡= ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡+ ğ¾ğ‘¡
ğ‘‘ğœ™ğ‘¡+ ğ‘šğ‘”ğ‘’3
(32)
with ğ¾ğ‘¡
ğ‘= ğ‘˜ğ‘¡
ğ‘1,2ğœ‹ğœ‚ğ‘¡+ ğ‘˜ğ‘¡
ğ‘3ğœ‚ğ‘¡ğœ‚âŠ¤
ğ‘¡
and ğ¾ğ‘¡
ğ‘‘= ğ‘˜ğ‘¡
ğ‘‘1,2ğœ‹ğœ‚ğ‘¡+ ğ‘˜ğ‘¡
ğ‘‘3ğœ‚ğ‘¡ğœ‚âŠ¤
ğ‘¡
two constant
positive definite matrices. Then, for any initial condition such that ğ‘‘ğ‘¡(0) =
âˆ’ğœ‚âŠ¤
ğ‘¡ğœ‰ğ‘¡(0) âˆˆR+, the following assertions hold âˆ€ğ‘¡â‰¥0:
(1) the height ğ‘‘ğ‘¡(ğ‘¡) = âˆ’ğœ‚âŠ¤
ğ‘¡ğœ‰ğ‘¡(ğ‘¡) âˆˆR+ and its derivative Ì‡ğ‘‘ğ‘¡(ğ‘¡) âˆˆR are well
defined and uniformly bounded and converge to zero asymptotically,
(2) the acceleration
Ì‡ğ‘£(ğ‘¡) and the states (ğœ‰ğ‘¡(ğ‘¡), ğ‘£(ğ‘¡)) are bounded and
converge asymptotically to zero.
Proof. See Appendix A.
â–¡
Proposition 1. Consider the system (1) in which â–µand Ì‡â–µare bounded.
(1) If the perturbation â–µis such that:
â–µ= ğœ‹ğœ‚ğ‘¡â–µ, or equivalently ğœ‚âŠ¤
ğ‘¡â–µ(ğ‘¡) = 0, âˆ€ğ‘¡â‰¥0,
then, for any initial condition such that ğ‘‘ğ‘¡(0) = âˆ’ğœ‚âŠ¤
ğ‘¡ğœ‰ğ‘¡(0) âˆˆR+,
direct application of the feedback control (32) ensures that: (i) Item
1 of Theorem 1 holds, (ii) Ì‡ğ‘£(ğ‘¡) and ğ‘£(ğ‘¡) are bounded and converging
asymptotically to zero, and finally (iii) â€–ğœ‹ğœ‚ğ‘¡ğœ‰ğ‘¡â€– is ultimately bounded
by ğ›¥ğœ‰, solution of â€–ğœ‹ğœ‚ğ‘¡ğ‘ğ‘¡â€– = â€–â–µâ€–max
ğ‘˜ğ‘¡
ğ‘‘1,2
.
(2) If ğœ‚âŠ¤
ğ‘¡
â–µ(ğ‘¡) â‰ 0, then, for any initial condition such that ğ‘‘ğ‘¡(0) =
âˆ’ğœ‚âŠ¤
ğ‘¡ğœ‰ğ‘¡(0) âˆˆR+, the following slightly modified feedback control:
ğ¹ğ‘¡= ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡+ ğ¾ğ‘¡
ğ‘‘(ğœ™ğ‘¡âˆ’ğœ‚ğ‘¡ğœ™âˆ—
ğ‘¡) + ğ‘šğ‘”ğ‘’3
(33)
with ğœ™âˆ—
ğ‘¡
â‰¥
1
ğ‘˜ğ‘¡
ğ‘‘3
|ğœ‚âŠ¤
ğ‘¡â–µ(ğ‘¡)|max, ensures that the above (i) and (ii)
assertions hold and guarantees that ğœ‰ğ‘¡is bounded.
Proof. See Appendix B.
â–¡
Remark 1.
The focus of the above proposition is on robustness and
adaptation of the controller with respect to the bounded perturbation â–µ.
It is introduced particularly to show robustness of the proposed control
law with respect to bounded perturbations in the plane orthogonal to
ğœ‚ğ‘¡and, in the interest of a less complicated presentation, a slightly
modified version of the control law (32) is introduced in (33) to be
able to analyze the robustness of the closed loop system with respect
to any bounded disturbance.
4.2. Going through the center of the window
To accomplish the goal of going through the window, while avoid-
ing the wall and window edges, the following control law is proposed
ğ¹ğ‘¤= ğœ(ğ‘ğ‘¤)(ğ‘˜ğ‘¤
ğ‘ğœ‹ğœ‚ğ‘¤Ì„ğ‘ğ‘¤+ ğ‘˜ğ‘¤
ğ‘‘ğœ‹ğœ‚ğ‘¤ğœ™ğ‘¤+ ğ‘˜ğ‘¤
ğœ™ğœ‚ğ‘¤(ğœ‚ğ‘¤
âŠ¤ğœ™ğ‘¤âˆ’ğœ™âˆ—
ğ‘¤) + ğ‘šğ‘”ğ‘’3),
(34)
with ğ‘˜ğ‘¤
ğ‘, ğ‘˜ğ‘¤
ğ‘‘and ğ‘˜ğ‘¤
ğœ™positive gains, ğœ™âˆ—
ğ‘¤> 0 and
ğœ(ğ‘ğ‘¤) =
{
0
, if ğœ‚âŠ¤
ğ‘¤ğ‘ğ‘¤â‰¥0
1
, if ğœ‚âŠ¤
ğ‘¤ğ‘ğ‘¤< 0,
(35)
which indicates that when the vehicle already crossed the window
(ğ‘‘ğ‘œâ‰¤0), ğ¹ğ‘¤= 0. Note that when ğœ‚âŠ¤
ğ‘¤ğ‘ğ‘¤< 0, the resulting closed-loop
system can be written as
â§
âª
â¨
âªâ©
Ì‡ğœ‰ğ‘¤= ğ‘£
Ì‡ğ‘£= âˆ’ğ‘˜ğ‘¤
ğ‘ğœ‹ğœ‚ğ‘¤
ğœ‰ğ‘¤
ğ‘‘ğ‘¤
âˆ’ğ‘˜ğ‘¤
ğ‘‘ğœ‹ğœ‚ğ‘¤
ğ‘£
ğ‘‘ğ‘¤
âˆ’ğ‘˜ğ‘¤
ğœ™ğœ‚ğ‘¤(ğœ‚ğ‘¤
âŠ¤ğ‘£
ğ‘‘ğ‘¤
âˆ’ğœ™âˆ—
ğ‘¤)+ â–µ,
(36)
The unknown term ğ‘‘ğ‘¤is a convex combination of the unknown dis-
tances ğ‘‘ğ‘œand ğ‘‘ğ‘’:
1
ğ‘‘ğ‘¤
= (ğ›¼ğ‘¤
1
ğ‘‘ğ‘œ
+ (1 âˆ’ğ›¼ğ‘¤) 1
ğ‘‘ğ‘’
)
(37)
which is deduced from (20) and (31) according to the definition of ğ›¼ğ‘¤
(19):
ğ‘‘ğ‘¤=
â§
âª
â¨
âªâ©
ğ‘‘ğ‘’,
if â€–ğ‘ğ‘¤â€– â‰¤ğœ–(ğœ‰ğ‘¤âˆˆî‰ƒ)
ğ‘‘ğ‘œğ‘‘ğ‘’
ğ›¼ğ‘¤ğ‘‘ğ‘’+(1âˆ’ğ›¼ğ‘¤)ğ‘‘ğ‘œ,
if ğœ–< â€–ğ‘ğ‘¤â€– < ğœ–+ ğ›¿
ğ‘‘ğ‘œ,
if â€–ğ‘ğ‘¤â€– â‰¥ğœ–+ ğ›¿.
(38)
Remark 2. Note that the unknown time varying distance ğ‘‘ğ‘¤involved
in the closed-loop system is due to the use of feedback information
Ì„ğ‘ğ‘¤= ğœ‰ğ‘¤
ğ‘‘ğ‘¤and ğœ™ğ‘¤=
ğ‘£
ğ‘‘ğ‘¤in the control law. It is the key feature to achieve
the double goal of avoiding collision with the wall and window edges
as well, while ensuring the main task of going through the center of
the window. When the vehicle approaches the wall or window edges
outside the region î‰ƒ, ğ‘‘ğ‘¤= ğ‘‘ğ‘œ. If ğ‘‘ğ‘œis decreasing then the motion in the
orthogonal direction to the wall is highly damped while the region î‰ƒ
is highly attractive. In practice, this leads to a bounded high gain in the
feedback control that prevents collision. When the vehicle is inside the
region î‰ƒ, ğ‘‘ğ‘¤= ğ‘‘ğ‘’. This later is lower bounded by a positive constant
so that the vehicle is able to go through the center of the window with
a non-zero velocity. More details of analysis will be shown below.
Proposition 2.
Consider the system (1) with the control input given by
(34). If the positive gains ğ‘˜ğ‘¤
ğ‘, ğ‘˜ğ‘¤
ğ‘‘and ğ‘˜ğ‘¤
ğœ™are such that
ğ‘˜ğ‘¤
ğ‘‘
2
ğ‘˜ğ‘¤ğ‘
> ğ‘Ÿğ‘¤
2 and for
any arbitrary small ğœ–> 0, the chosen ğœ™âˆ—
ğ‘¤satisfies:
ğœ™âˆ—
ğ‘¤>
|ğœ‚âŠ¤
ğ‘¤â–µ(ğ‘¡)|max
ğ‘˜ğ‘¤
ğœ™
+ ğœ–, âˆ€ğ‘¡â‰¥0,
(39)
then for any initial condition satisfying ğ‘‘ğ‘¤(0)
âˆˆ
R+ and as long as
ğœ(ğ‘ğ‘¤(ğ‘¡)) = 1, the following assertions hold âˆ€ğ‘¡â‰¥0:
(1) there exists a finite time ğ‘¡ğ‘¤â‰¥0 at which the vehicle enters the region
î‰ƒ(â€–ğ‘ğ‘¤(ğ‘¡ğ‘¤)â€– â‰¤ğœ–) and remains there, while ğ‘‘ğ‘¤(ğ‘¡) â‰¥ğ‘‘ğ‘œ(ğ‘¡) âˆˆR+, âˆ€ğ‘¡<
ğ‘¡ğ‘¤,
6

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
(2) there exists a finite time ğ‘¡lim > ğ‘¡ğ‘¤at which the vehicle crosses the
window ğ‘‘ğ‘œ(ğ‘¡lim) = 0, with strictly negative velocity Ì‡ğ‘‘ğ‘œ(ğ‘¡lim) such that
the vehicle is inside the region î‰ƒ(â€–ğ‘ğ‘¤(ğ‘¡)â€– â‰¤ğœ–) for all ğ‘¡âˆˆ[ğ‘¡ğ‘¤, ğ‘¡lim).
Proof. See Appendix C.
â–¡
4.3. Application scenario
The double goal of crossing the window and landing on the landing
pad can be achieved by simply applying the control laws ğ¹ğ‘¤and ğ¹ğ‘¡in
sequence, with an adequate trigger to switch from ğ¹ğ‘¤to ğ¹ğ‘¡. Taking the
limitation of the camerasâ€™ field of view into the consideration, there
will be four different modes during the full process of going through
a window and landing on the pad. When ğ‘¡âˆˆ[ğ‘‡1, ğ‘‡2), ğ‘šğ‘œğ‘‘ğ‘’= 1 and
ğ¹ğ‘¤(34) is active. When the vehicle approaches to the center of the
window, the on-board camera loses the full image of the window and
ğ‘šğ‘œğ‘‘ğ‘’changes to 2. When ğ‘¡âˆˆ[ğ‘‡2, ğ‘‡3), ğ‘šğ‘œğ‘‘ğ‘’2 is active and the open-
loop control ğœ‚ğ‘¤|ğœ‚âŠ¤
ğ‘¤ğ¹ğ‘¤(ğ‘‡âˆ’
2 )| is applied, where ğ‘‡âˆ’
2 is the last time instance
before the camera loses the image of the window. At the time instance
ğ‘¡= ğ‘‡3, when the downward-looking camera detects the landing pad,
the ğ‘šğ‘œğ‘‘ğ‘’changes to 3 and the control law ğ¹ğ‘¡(32) is applied when ğ‘¡âˆˆ
[ğ‘‡3, ğ‘‡4). At time instance ğ‘¡= ğ‘‡4, the vehicle is already close to the center
of the landing target and it is safe to slowly shutdown the quadrotor
motors. In order to avoid inadequate behaviors, the switch from ğ‘šğ‘œğ‘‘ğ‘’2
to 3 is only triggered once. Moreover, in practice, due to the limitation
of cameraâ€™s field of view, the initial errors should not be large and
should converge to zero fast enough, thereby allowing the vehicle to
almost align with the center of the window before switching to ğ‘šğ‘œğ‘‘ğ‘’
2. Additionally, the position of the landing target should be close
enough to the window so that the quadrotor is able to timely detect the
landing target after it goes through the window. The switching between
different modes is based on the combination of selected frames from
both the downward-looking and forward-looking on-board cameras
obtained in the experiments. The detail on the adopted procedure is
described in Section 6.
5. Simulation results
In this section, simulation results are presented to illustrate the
behavior of the closed-loop system using the proposed controller. A
high-gain inner-loop controller is used to control the attitude dynam-
ics (Tang et al., 2015). It generates the torque inputs in order to
stabilize the orientation of the vehicle to a desired one defined by
the desired thrust direction ğ‘…ğ‘‘ğ‘’3, which is provided by the outer-
loop image-based controller, and the desired yaw chosen to align the
forward-looking camera with direction orthogonal to the wall. The
control algorithm is tested with different initial conditions, always
starting from a position outside the room containing the target (see
Fig. 1). The initial velocity of the quadrotor is ğ‘£(0) = [0 0 0]âŠ¤, and
the gains are chosen as ğ¾ğ‘¡
ğ‘= diag[4 4 1.75], ğ¾ğ‘¡
ğ‘‘= 4ğ¼3, ğ‘˜ğ‘¤
ğ‘‘= 0.8, ğ‘˜ğ‘¤
ğ‘= 1,
ğ‘˜ğ‘¤
ğœ™= 1 and ğœ™âˆ—
ğ‘¤= 0.3
As shown in Fig. 1, with different initial positions the quadrotor
successfully avoids the wall and window, goes through the center of the
window, and then lands on the center of the landing target. Figs. 7â€“15
show in detail the time evolution of quadrotorâ€™s state variables, virtual
input, and image features for the initial position ğœ‰(0) = [âˆ’2 0.1 âˆ’1.82]âŠ¤.
The time evolution of the active mode is also specified. In ğ‘šğ‘œğ‘‘ğ‘’1,
the quadrotor is approaching the window; in ğ‘šğ‘œğ‘‘ğ‘’2, it is crossing the
window with no image cues; in ğ‘šğ‘œğ‘‘ğ‘’3, it starts detecting the landing
pad and transitions to the landing maneuver; and finally in ğ‘šğ‘œğ‘‘ğ‘’4, the
motors are shutdown.
Fig. 7 shows the time evolution of the vehicleâ€™s position and the
dashed lines are the coordinates of windowâ€™s center. From Fig. 7, one
can see the quadrotor first converges to the center line of the window
and then converges to the center point of the target. Fig. 8 shows the
time evolution of the vehicleâ€™s velocity. The virtual control input ğ¹is
Fig. 7. Evolutions of the quadrotorâ€™s position ğœ‰and the mode.
Fig. 8. Evolutions of the quadrotorâ€™s velocity ğ‘£.
shown in Fig. 9. The angular velocity of the quadrotor is depicted in
Fig. 10 and Fig. 11 depicts the time evolution of Euler angles, which
indicates a good compromise in terms of time-scale separation between
the outer-loop and inner-loop controller. Figs. 12 and 13 show the
translational optical flow used for going through the window in ğ‘šğ‘œğ‘‘ğ‘’1
and for landing in ğ‘šğ‘œğ‘‘ğ‘’3, respectively. The evolution of image features
of Ì„ğ‘ğ‘¤and ğ‘ğ‘¡are depicted in Figs. 14 and 15, respectively. One can
see that the image features Ì„ğ‘ğ‘¤and ğ‘ğ‘¡approach to the desired values
[âˆ’1 0 0]âŠ¤and [0 0 0]âŠ¤, respectively, before the on-board cameras lose
the image information.
6. Experiments
6.1. Experimental setup
In order to set up the experiment, a movable wall was used to divide
the testing space into two smaller compartments and a landing pad was
placed on the ground of the second one. The partition wall contains
a rectangular window and is textured as a brick wall to provide the
background optical flow, as shown in Fig. 18. The vehicle used for
the experiments is an Asctec Pelican quadrotor (Fig. 16) with weight
1676g and the arm length from the center of mass to each motor is
20 cm. The available commands are thrust force and attitude which
are derived from the force ğ¹provided by the outer-loop controller
(32) (respectively (34)) and the desired yaw angle. The quadrotor
7

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 9. Evolutions of the virtual control input ğ¹.
Fig. 10. Evolution of angular velocity ğ›º.
Fig. 11. Evolution of Euler angles.
is equipped with two wide-angle cameras, one pointing towards the
ground and another is facing the forward direction, pointing at the
wall. Recalling Assumption 1, the downward-looking camera reference
Fig. 12. Translational optical flow using for going through the window during mode
1.
Fig. 13. Translational optical flow using for landing during mode 3.
Fig. 14. Image feature Ì„ğ‘ğ‘¤during mode 1.
frame coincides with the vehicleâ€™s body fixed frame and the rotation
matrix from the forward-looking camera reference frame to the body
frame is ğµ
ğ¶ğ‘…= ğ‘…ğ‘(âˆ’ğœ‹
4 )ğ‘…ğ‘‹( ğœ‹
2 ). These two cameras are uEye UI-122ILE
8

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 15. Image feature ğ‘ğ‘¡during mode 3.
Fig. 16. Asctec Pelican quadrotor.
models featuring a 1/2-in sensor with global shutter which operate
at a resolution of 752 Ã— 480 pixel at 50 frames per second and are
provisioned with 2.2-mm lenses.
In the experiments, a rapid prototyping and testing architecture are
used in which a MATLAB/Simulink environment integrates the sensors
and the cameras, the control algorithm and the communication with the
vehicle. The controller is developed and tuned on a MATLAB/Simulink
environment and C code is generated and compiled to run onboard the
vehicle as a final step. The onboard computer (a 4-core Intel i7-3612QE
at 2.1 GHz, named AscTec Mastermind) is responsible for running in
Linux three major software components that provide:
(1) interface with the camera hardware, image acquisition, feature
detection and optical flow computation;
(2) computation of the vehicle force references from the image
features, translational optical flow, and angular velocity and
rotation matrix estimates provided by the IMU;
(3) interface with microprocessor, receiving IMU data and sending
force references to the inner-loop controller.
A Python program running on the onboard computer performs
detection of the window, detection of landing target, and optical flow
computation using the OpenCV library. ARUCO markers, for which
built-in detection functions exist in the OpenCV library, are used to
define the landmarks on the landing pad. In order to fit the cameraâ€™s
field of view during the full process of landing, the landmarks are
composed by 4 groups of ARUCO markers and in each group there
are 4 ARUCO markers with same border size but different identifier
Fig. 17. ARUCO markers on the landing pad.
Fig. 18. Selected frames from the forward-looking camera. (For interpretation of the
references to color in this figure legend, the reader is referred to the web version of
this article.)
(id) as shown in Fig. 17. When the camera is far away from the
markers, the group of larger markers can be seen and when the camera
is near the ground, only the smaller group of the landmarks will be
shown in the field of view. The rectangular window shape is detected
using the library code originally developed for ARUCO markerâ€™s border
detection. The detected window frame (in green) and the windowâ€™s
coordinate system overlayed on the image are show in Fig. 18-(1), (2),
and (3). The translational optical flow is also computed onboard. The
computation is based on the conventional image plane optical flow
field provided by a pyramidal implementation of the Lucasâ€“Kanade
algorithm. The detailed description of the computation can be found
in Serra et al. (2016). The small vectors represented in Fig. 19 represent
the translational optical flow of the image pixels.
In order to provide ground truth measurements and evaluate the
performance of the proposed controller, a VICON motion capture sys-
tem (VICON, 2014) which comprises 12 cameras is used together with
markers attached to the quadrotor, window, and landing target. The
motion capture system is able to accurately locate the position of the
markers, from which ground truth position and orientation measure-
ments are gathered. Note that, none of the measurements from the
motion capture system are used in the proposed controller.
6.2. Experimental results
The experiments were conducted with the same control gains as the
simulations. Before the proposed controller is triggered, the vehicle is
hovering at position ğœ‰= [0.15, 1.79, âˆ’1.76] m, which is outside the space
9

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 19. Selected frames from the downward-looking camera.
containing the landing pad. As mentioned in Section 4.3, there are four
different modes during the full process of going through a window and
landing on the target due to the limitation of the field of view of the on-
board cameras. Fig. 18 shows the selected frames in a timed sequence
from the forward-looking camera. These four frames are a fixed time
step apart and were taken during a ğ‘šğ‘œğ‘‘ğ‘’1 to ğ‘šğ‘œğ‘‘ğ‘’2 transition. In
Fig. 18-(1), (2), and (3), ğ‘šğ‘œğ‘‘ğ‘’1 is active, and one can see that the
window frame is well detected. In Fig. 18-(1), ğ‘¡= ğ‘‡1 and the controller
ğ¹ğ‘¤is triggered. In Fig. 18-(2) and (3), the vehicle is still in ğ‘šğ‘œğ‘‘ğ‘’1
and approaches the center of the window. As the vehicle approaches
the window, the window frame disappears from the field of view of
the camera and at time ğ‘¡= ğ‘‡2 the ğ‘šğ‘œğ‘‘ğ‘’commutes to 2, as shown in
Fig. 18-(4). Note that during transition from ğ‘šğ‘œğ‘‘ğ‘’1 to 2, instead of
losing the window frame, the camera may detect rectangles other then
the target window, as depicted in Fig. 18-(4). In order to avoid this
situation, the ğ‘šğ‘œğ‘‘ğ‘’changes from 1 to 2 if the pixel coordinates change
instantaneously in a way that is incompatible with smooth tracking of
the same window object. Fig. 19 shows the selected frames in a timed
sequence from the downward-looking camera. These four frames were
taken at fixed time steps during a transition from ğ‘šğ‘œğ‘‘ğ‘’3 to 4. At time
instance ğ‘¡= ğ‘‡3, as shown in Fig. 19-(1) and (2), the downward-looking
camera detects successfully the landing pad, the ğ‘šğ‘œğ‘‘ğ‘’is switched to 3
and ğ¹ğ‘¡is applied as control input. Recall that the switching from ğ‘šğ‘œğ‘‘ğ‘’2
to ğ‘šğ‘œğ‘‘ğ‘’3 is only triggered once in order to avoid inadequate behavior.
In Fig. 19-(3), the vehicle approaches the target and the ğ‘šğ‘œğ‘‘ğ‘’is still 3.
At the time instance ğ‘¡= ğ‘‡4, when the quadrotor has almost reached the
target position (see Fig. 19-(4)), the ğ‘šğ‘œğ‘‘ğ‘’changes to 4 and it is safe to
slowly shutdown the motors.
Figs. 20 and 21 show the position and velocity coordinates of
the vehicle provided by VICON, respectively. We can see that the
vehicle goes through the center of the window at the end of ğ‘šğ‘œğ‘‘ğ‘’
2 and finally lands on the target. Fig. 22 shows the evolution of the
angular velocity and Fig. 23 show the evolution of the Euler angles.
From Fig. 23, one can see that a good compromise in terms of time-
scale separation between the outer-loop and inner-loop controllers is
attained, which indicates that the inner-loop controller is sufficiently
fast to track the outer-loop references, including during the transitions
between different modes. The evolution of the virtual control input
ğ¹is depicted in Fig. 24. Fig. 25 shows the image feature Ì„ğ‘ğ‘¤used
for going through the window. The solid line represents Ì„ğ‘ğ‘¤computed
from the image sequence and the dashed line represents Ì„ğ‘ğ‘¤provided
by the VICON system. There are slight differences between these two
computations due to the fact that rotation matrix ğ‘…provided by the
IMU is affected by the surrounding magnetic field generated by the
fast rotating motors. Figs. 27 and 26 show the translational optical flow
used for going through the window and for landing respectively. The
solid red lines represent the translational optical flow computed from
Fig. 20. Evolutions of the quadrotorâ€™s position ğœ‰and the mode.
Fig. 21. Evolutions of the quadrotorâ€™s velocity ğ‘£.
Fig. 22. Evolution of angular velocity ğ›º.
the image sequence and the dashed line represents the translational
optical flow derived from VICON measurements. The video of the
experimental results can be found in https://youtu.be/DbpeGfJMHk0.
10

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 23. Evolution of Euler angle.
Fig. 24. Evolutions of the vitual control input ğ¹.
Fig. 25. Image features Ì„ğ‘ğ‘¤computed from the image sequence (solid line) and from
the VICON measurements (dashed line) during mode 1.
Fig. 26. Translational optical flow computed during ğ‘šğ‘œğ‘‘ğ‘’1 (going through window)
from the image sequence (solid line) and from the VICON measurements (dashed line).
Fig. 27. Translational optical flow computed during ğ‘šğ‘œğ‘‘ğ‘’3 (landing) from the image
sequence (solid line) and from the VICON measurements (dashed line).
7. Conclusion
This paper considers the problem of controlling a quadrotor to go
through a window and land on planar target, using an Image-Based
Visual Servo (IBVS) controller. For control purposes, the centroids
vectors provided by the combination of the corresponding spherical
image measurements of landmarks (corners) for both the window and
the target are used as position feedback. The translational optical
flow relative to the wall, window edges, and landing plane is used
as velocity measurement. To achieve the proposed objective, no direct
measurements of position or velocity are used and no explicit estimate
of the height above the landing plane or of the distance to the wall
is required. With the initial position outside the room containing the
target, the proposed control law guarantees that the quadrotor aligns
itself with the center line orthogonal to the window, crosses it with
non-zero velocity and finally lands on the planar target successfully
without colliding the wall or the edges of the window. Rigourous
proofs of convergence and/or piratical stability of closed-loop system
are provided when the system is subjected to unknown bounded dis-
turbances. Simulation and experimental results show the effectiveness
of the overall proposed control scheme.
11

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Appendix A. Proof of Theorem 1
Proof. The proof follows a reasoning very similar to that of Theorem 1
in Rosa et al. (2014). Recalling (1) and applying the control input (32),
the closed-loop system can be written as
â§
âª
â¨
âªâ©
Ì‡ğœ‰ğ‘¡= ğ‘£
Ì‡ğ‘£= âˆ’ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡(ğœ‰ğ‘¡) âˆ’ğ¾ğ‘¡
ğ‘‘
ğ‘£
ğ‘‘ğ‘¡
.
(40)
Before proceeding with the proof of item (1), a positive definite
storage function îˆ¸2(ğœ‰ğ‘¡, ğ‘£) will be defined and one will show that if ğ‘‘ğ‘¡(ğ‘¡)
remains positive,
Ì‡îˆ¸2 is negative semi-definite, which implies that the
solutions remain bounded for all ğ‘¡â‰¥0. Define îˆ¸2 as
îˆ¸2(ğœ‰ğ‘¡, ğ‘£) = îˆ¸1(ğœ‰ğ‘¡) + 1
2ğ‘£âŠ¤ğ¾ğ‘¡
ğ‘
âˆ’1ğ‘£
(41)
where îˆ¸1(ğœ‰ğ‘¡) is the radially unbounded function given by
îˆ¸1(ğœ‰ğ‘¡) = 1
ğ‘›ğ‘¡
ğ‘›ğ‘¡
âˆ‘
ğ‘–=1
(â€–ğ‘ƒğ‘¡
ğ‘–(ğœ‰ğ‘¡)â€– âˆ’â€–ğ‘ƒğ‘¡
ğ‘–(0)â€–).
(42)
To show that îˆ¸1(ğœ‰ğ‘¡) is a positive definite function, note that
ğœ•îˆ¸1
ğœ•ğœ‰ğ‘¡
= ğ‘âŠ¤
ğ‘¡
(43)
ğœ•2îˆ¸1
ğœ•ğœ‰2
ğ‘¡
= ğ‘„
(44)
where ğ‘„=
1
ğ‘›ğ‘¡
âˆ‘ğ‘›ğ‘¡
ğ‘–=1
1
â€–ğ‘ƒğ‘¡
ğ‘–â€– ğœ‹ğ‘ğ‘¡
ğ‘–is positive definite, as long as at least
two of the vectors ğ‘ğ‘¡
ğ‘–are non-collinear. It follows that îˆ¸1 is a convex
function of ğœ‰ğ‘¡, with a global minimum attained when ğœ•îˆ¸1
ğœ•ğœ‰ğ‘¡= ğ‘âŠ¤
ğ‘¡= 0, or
equivalently, when ğœ‰ğ‘¡= 0. Since îˆ¸1(0) = 0 is the global minimum of the
function, one concludes that îˆ¸1(ğœ‰ğ‘¡) is positive-definite. Noting that,
Ì‡îˆ¸1 = ğ‘âŠ¤
ğ‘¡ğ‘£,
(45)
it follows that
Ì‡îˆ¸2 = âˆ’1
ğ‘‘ğ‘¡
ğ‘£âŠ¤ğ¾ğ‘¡
ğ‘
âˆ’1ğ¾ğ‘¡
ğ‘‘ğ‘£
(46)
which is negative semi-definite as long as ğ‘‘ğ‘¡remains positive and im-
plies that the states ğœ‰ğ‘¡(ğ‘¡) and ğ‘£(ğ‘¡) remain bounded for all ğ‘¡â‰¥0. The next
steps of the proof consist in proving first Item (1) and then the uniform
continuity of (46) along every systemâ€™s solution in order to deduce, by
application of Barbalatâ€™s Lemma, the asymptotic convergence of ğ‘£to
zero and from there one deduces the asymptotic convergence of Ì‡ğ‘£and
then ğœ‰ğ‘¡to zero (Item (2)).
Proof of Item 1: Using (40) and the fact that ğ‘‘ğ‘¡(ğ‘¡) = âˆ’ğœ‚âŠ¤
ğ‘¡ğœ‰ğ‘¡and
Ì‡ğ‘‘ğ‘¡= âˆ’ğœ‚ğ‘¡âŠ¤ğ‘£yields
Ìˆğ‘‘ğ‘¡= âˆ’ğ‘˜ğ‘¡
ğ‘‘3
Ì‡ğ‘‘ğ‘¡
ğ‘‘ğ‘¡
âˆ’ğ‘˜ğ‘¡
ğ‘3ğ›½ğ‘¡
(47)
with
ğ›½ğ‘¡(ğ‘¡) = âˆ’ğœ‚âŠ¤
ğ‘¡ğ‘ğ‘¡= 1
ğ‘›ğ‘¡
âˆ‘ğ‘›ğ‘¡
ğ‘–=1
ğ‘‘ğ‘¡
â€–ğ‘ƒğ‘–
ğ‘¡â€– > 0, âˆ€ğ‘¡.
(48)
This relation is of course valid as long as ğ‘‘ğ‘¡(ğ‘¡) > 0. From there, direct
application of Rosa et al. (2014, Th. 1-(2)) shows that if ğ‘‘ğ‘¡(0) âˆˆR+,
the solution (ğ‘‘ğ‘¡, Ì‡ğ‘‘ğ‘¡) âˆˆ(R+, R) exists and uniformly bounded âˆ€ğ‘¡and
converges asymptotically to (0, 0).
Proof of Item 2: To show that
Ìˆîˆ¸2 = âˆ’2
ğ‘‘ğ‘¡
ğ‘£âŠ¤ğ¾ğ‘¡
ğ‘
âˆ’1ğ¾ğ‘¡
ğ‘‘Ì‡ğ‘£+
Ì‡ğ‘‘ğ‘¡
ğ‘‘ğ‘¡
1
ğ‘‘ğ‘¡
ğ‘£âŠ¤ğ¾ğ‘¡
ğ‘
âˆ’1ğ¾ğ‘¡
ğ‘‘ğ‘£
is bounded and hence
Ì‡îˆ¸2 (46) is uniformly continuous, it suffices to
show that â€–ğ‘£â€–
ğ‘‘ğ‘¡
is bounded (so is
Ì‡ğ‘‘ğ‘¡
ğ‘‘ğ‘¡). For that purposes, consider the
dynamics of
ğ‘£
ğ‘‘ğ‘¡âˆ¶
ğ‘‘
ğ‘‘ğ‘¡( ğ‘£
ğ‘‘ğ‘¡
) = âˆ’1
ğ‘‘ğ‘¡
((ğ¾ğ‘¡
ğ‘‘+ Ì‡ğ‘‘ğ‘¡ğ¼) ğ‘£
ğ‘‘ğ‘¡
+ ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡).
(49)
Since
Ì‡ğ‘‘ğ‘¡converges asymptotically to zero and ğ‘ğ‘¡is bounded then, by
direct application of Rosa et al. (2014, Lemma 4) one ensures that
ğ‘£
ğ‘‘ğ‘¡
is bounded. From there one concludes that Ì‡îˆ¸2 is uniformly continuous
and hence ğ‘£converges asymptotically to zero.
To prove that ğ‘ğ‘¡(ğ‘¡) (or equivalently ğœ‰ğ‘¡) is asymptotically converging
to zero one has to show first Ì‡ğ‘£is converging to zero. From (40), one
can verify that:
Ìˆğ‘£= âˆ’
ğ¾ğ‘¡
ğ‘‘
ğ‘‘ğ‘¡
Ì‡ğ‘£+ ğ›¿0
Ì‡ğ‘£,
(50)
with ğ›¿0
Ì‡ğ‘£= ğ¾ğ‘¡
ğ‘‘
Ì‡ğ‘‘ğ‘¡
ğ‘‘ğ‘¡
ğ‘£
ğ‘‘ğ‘¡âˆ’ğ¾ğ‘¡
ğ‘Ì‡ğ‘ğ‘¡. Since
ğ‘£
ğ‘‘ğ‘¡(and hence
Ì‡ğ‘‘ğ‘¡
ğ‘‘ğ‘¡) is bounded and
Ì‡ğ‘ğ‘¡= ğ‘„ğ‘£= ğ‘„0
ğ‘£
ğ‘‘ğ‘¡
, with ğ‘„0 = 1
ğ‘›ğ‘¡
ğ‘›ğ‘¡
âˆ‘
ğ‘–=1
ğ‘‘ğ‘¡
â€–ğ‘ƒğ‘¡
ğ‘–â€– ğœ‹ğ‘ğ‘¡
ğ‘–< ğ¼3,
is also a bounded vector, one ensures that ğ›¿0
Ì‡ğ‘£is bounded. Therefore,
direct application of Rosa et al. (2014, Lem. 3) concludes boundedness
and the asymptotic convergence of Ì‡ğ‘£to zero and hence one has:
ğ‘£
ğ‘‘ğ‘¡
= âˆ’ğ¾ğ‘¡
ğ‘‘
âˆ’1ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡+ ğ‘œ(ğ‘¡)
(51)
with ğ‘œ(ğ‘¡) a asymptotically vanishing term.
By multiplying both sides of the above equation by the bounded
vector ğ‘âŠ¤
ğ‘¡(the gradient of îˆ¸1) and using the fact that
Ì‡îˆ¸1 = ğ‘âŠ¤
ğ‘¡ğ‘£(45),
one obtains:
Ì‡îˆ¸1 = âˆ’ğ‘‘ğ‘¡ğ‘âŠ¤
ğ‘¡ğ¾ğ‘¡
ğ‘‘
âˆ’1ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡+ ğ‘‘ğ‘¡ğ‘âŠ¤
ğ‘¡ğ‘œ(ğ‘¡).
(52)
Since ğ‘‘ğ‘¡(ğ‘¡) converges asymptotically to zero, then by taking the integral
of (47)
Ì‡ğ‘‘ğ‘¡(ğ‘¡) âˆ’Ì‡ğ‘‘ğ‘¡(0) = âˆ’ğ‘˜ğ‘¡
ğ‘‘3 log( ğ‘‘ğ‘¡(ğ‘¡)
ğ‘‘ğ‘¡(0) ) âˆ’ğ‘˜ğ‘¡
ğ‘3 âˆ«
ğ‘¡
0
ğ›½ğ‘¡(ğœ)ğ‘‘ğœ,
(53)
one concludes that
lim
ğ‘¡â†’âˆâˆ«
ğ‘¡
0
ğ›½ğ‘¡(ğœ)ğ‘‘ğœ= +âˆ.
(54)
Combining Eq. (54) with the fact that ğ‘‘(ğ‘¡) â‰¥ğ›½ğ‘¡(ğ‘¡) (from (48)) and
replacing the time index ğ‘¡of Eq. (52) by the new time-scale index
ğ‘ (ğ‘¡) âˆ¶= âˆ«ğ‘¡
0 ğ‘‘ğ‘¡(ğœ)ğ‘‘ğœ(ğ‘ tends to infinity if and only if ğ‘¡tends to infinity),
one has:
ğ‘‘
ğ‘‘ğ‘ îˆ¸1 = âˆ’ğ‘âŠ¤
ğ‘¡ğ¾ğ‘¡
ğ‘‘
âˆ’1ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡+ ğ‘âŠ¤
ğ‘¡ğ‘œ(ğ‘¡),
(55)
from which one concludes that ğ‘ğ‘¡(and ğœ‰ğ‘¡) is asymptotically converging
to zero.
â–¡
Appendix B. Proof of Proposition 1
Proof. The proof follows and exploits the same technical steps of the
proof of Theorem 1. Since assertions made are almost the same using
either (33) or (32) (equivalently (33) with ğœ™âˆ—
ğ‘¡= 0) except for the last
item (iii), the proof will be provided using (33) as feedback control and
differences will be specified when necessaries.
When â–µâ‰ 0 and ğœ™âˆ—
ğ‘¡â‰ 0, it is straightforward to verify that (46)
becomes:
Ì‡îˆ¸2 = âˆ’1
ğ‘‘ğ‘¡
ğ‘£âŠ¤ğ¾ğ‘¡
ğ‘
âˆ’1ğ¾ğ‘¡
ğ‘‘ğ‘£+ ğ‘£âŠ¤ğ¾ğ‘¡
ğ‘
âˆ’1(â–µ+ğ¾ğ‘¡
ğ‘‘ğœ™âˆ—
ğ‘¡ğœ‚ğ‘¡)
(56)
Recall now the dynamics of Ì‡ğ‘‘ğ‘¡(47), ğ‘£
ğ‘‘ğ‘¡(49), and of Â¨v (50) in case where
â–µâ‰ 0 and ğœ™âˆ—
ğ‘¡â‰ 0.
Ìˆğ‘‘ğ‘¡= âˆ’ğ‘˜ğ‘¡
ğ‘‘3
Ì‡ğ‘‘ğ‘¡
ğ‘‘ğ‘¡
âˆ’ğ‘˜ğ‘¡
ğ‘3ğ›½â–µ
ğ‘¡
(57)
12

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
ğ‘‘
ğ‘‘ğ‘¡( ğ‘£
ğ‘‘ğ‘¡
) = âˆ’1
ğ‘‘ğ‘¡
((ğ¾ğ‘¡
ğ‘‘+ Ì‡ğ‘‘ğ‘¡ğ¼) ğ‘£
ğ‘‘ğ‘¡
+ ğ›¿â–µ
ğ‘£)
(58)
Ìˆğ‘£= âˆ’
ğ¾ğ‘¡
ğ‘‘
ğ‘‘ğ‘¡
Ì‡ğ‘£+ ğ›¿â–µ
Ì‡ğ‘£,
(59)
with
ğ›½â–µ
ğ‘¡(ğ‘¡) =
1
ğ‘˜ğ‘¡
ğ‘3
(ğ‘˜ğ‘¡
ğ‘‘3ğœ™âˆ—
ğ‘¡+ ğœ‚âŠ¤
ğ‘¡â–µ) âˆ’ğœ‚âŠ¤
ğ‘¡ğ‘ğ‘¡
(60)
ğ›¿â–µ
ğ‘£= ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡âˆ’â–µâˆ’ğ¾ğ‘¡
ğ‘‘ğœ‚ğ‘¡ğœ™âˆ—
ğ‘¡
(61)
ğ›¿â–µ
Ì‡ğ‘£= ğ¾ğ‘‘
Ì‡ğ‘‘ğ‘¡
ğ‘‘ğ‘¡
ğ‘£
ğ‘‘ğ‘¡
âˆ’ğ¾ğ‘¡
ğ‘Ì‡ğ‘ğ‘¡+ Ì‡â–µ.
(62)
Now since ğ›½â–µ
ğ‘¡(ğ‘¡) > 0, âˆ€ğ‘¡independently from the value chosen for ğœ™âˆ—
ğ‘¡,
direct application of Rosa et al. (2014, Th. 1-(2)) shows that the solu-
tion (ğ‘‘ğ‘¡, Ì‡ğ‘‘ğ‘¡) âˆˆ(R+, R) exists and uniformly bounded âˆ€ğ‘¡and converges
(at least) asymptotically to (0, 0).
By combining this with the fact that all terms involved in ğ›¿â–µ
ğ‘£(ğ‘ğ‘¡, â–µ
and ğœ™âˆ—
ğ‘¡) are bounded, direct application of Rosa et al. (2014, Lem. 4)
concludes the boundedness of
ğ‘£
ğ‘‘ğ‘¡. Since ğ‘‘ğ‘¡is converging to zero, one
concludes that ğ‘£is converging to zero by a direct application of Rosa
et al. (2014, Lem. 3). Using the fact that Ì‡â–µis bounded by assumption,
the proof of boundedness Â¨v (59) and its convergence to zero is directly
deduced from to proof the unperturbed case (50). From there and
analogously to the unperturbed case (Theorem 1- proof of Item (2)),
one gets:
ğ‘£
ğ‘‘ğ‘¡
= âˆ’ğ¾ğ‘¡
ğ‘‘
âˆ’1ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡+ ğ¾ğ‘¡
ğ‘‘
âˆ’1 â–µ+ğœ‚ğ‘¡ğœ™âˆ—
ğ‘¡+ ğ‘œ(ğ‘¡)
(63)
with ğ‘œ(ğ‘¡) an asymptotically vanishing term.
By multiplying both sides of (63) by ğ‘âŠ¤
ğ‘¡
and using the fact that
Ì‡îˆ¸1 = ğ‘âŠ¤
ğ‘¡ğ‘£(45), one obtains:
Ì‡îˆ¸1
ğ‘‘ğ‘¡
= âˆ’ğ‘âŠ¤
ğ‘¡ğ¾ğ‘¡
ğ‘‘
âˆ’1ğ¾ğ‘¡
ğ‘ğ‘ğ‘¡+ ğ‘âŠ¤
ğ‘¡(ğ¾ğ‘¡
ğ‘‘
âˆ’1 â–µ+ğœ‚ğ‘¡ğœ™âˆ—
ğ‘¡+ ğ‘œ(ğ‘¡)).
(64)
From there one distinguishes between the two issues stated in the
proposition:
(1) ğœ‚âŠ¤
ğ‘¡â–µ(ğ‘¡) = 0, âˆ€ğ‘¡and ğœ™âˆ—
ğ‘¡= 0 (ğ¹ğ‘¡given by (32))
By changing the time scale index and similarly to argument used
at the end of the proof of Theorem 1, one concludes that â€–ğ‘ğ‘¡â€– is
ultimately bounded by â€–â–µâ€–max
ğ‘˜ğ‘¡ğ‘1,2
. Since ğ‘‘ğ‘¡= ğœ‚âŠ¤
ğ‘¡ğœ‰ğ‘¡converges to zero, one
concludes that â€–ğœ‹ğœ‚ğ‘¡ğœ‰ğ‘¡â€– is ultimately bounded by ğ›¥ğœ‰which is the solution
of â€–ğœ‹ğœ‚ğ‘¡ğ‘ğ‘¡â€– = â€–â–µâ€–max
ğ‘˜ğ‘¡
ğ‘‘1,2
.
(2) ğœ‚âŠ¤
ğ‘¡â–µ(ğ‘¡) â‰ 0, and ğœ™âˆ—
ğ‘¡â‰ 0 (ğ¹ğ‘¡given by (33))
In that case one concludes that the storage function îˆ¸1 is decreasing
as long as the right hand side of the above equation is negative and
ğ‘‘ğ‘¡> 0 and hence ğœ‰ğ‘¡is bounded. The argument of changing the time
index is not valid in this case.
â–¡
Appendix C. Proof of Proposition 2
Proof. We will consider hereafter only the case where ğœ(ğ‘ğ‘¤) = 1 (or
equivalently when ğœ‚âŠ¤
ğ‘¤ğ‘ğ‘¤< 0). That is the situation in which the vehicle
is going through the window while avoiding collision with the wall and
the window edges.
From the dynamics of the closed-loop system (36), the proof focus
first on the evolution of ğ‘‘ğ‘¤. That is the evolution of the system in the
direction ğœ‚ğ‘¤.
When â€–ğ‘ğ‘¤(ğ‘¡)â€– â‰¥ğœ–+ ğ›¿, one has ğ‘‘ğ‘¤= ğ‘‘ğ‘œ= âˆ’ğœ‚ğ‘‡
ğ‘¤ğœ‰ğ‘¤and hence:
Ì‡ğ‘‘ğ‘œ= âˆ’ğœ‚âŠ¤
ğ‘¤ğ‘£
Ìˆğ‘‘ğ‘œ= âˆ’ğ‘˜ğ‘¤
ğœ™
Ì‡ğ‘‘ğ‘œ
ğ‘‘ğ‘œ
âˆ’ğ‘˜ğ‘¤
ğœ™ğ›½ğ‘¤
(65)
with ğ›½ğ‘¤= ğœ™âˆ—
ğ‘¤+
ğœ‚âŠ¤
ğ‘¤â–µ
ğ‘˜ğ‘¤
ğœ™
â‰¥ğœ–.
When ğœ–< â€–ğ‘ğ‘¤(ğ‘¡)â€– < ğœ–+ ğ›¿, one has ğ‘‘ğ‘¤=
ğ‘‘ğ‘œğ‘‘ğ‘’
ğ›¼ğ‘¤ğ‘‘ğ‘’+(1âˆ’ğ›¼ğ‘¤)ğ‘‘ğ‘œwith ğ›¼ğ‘¤
(defined by (19)) a uniformly continuous and bounded valued function
on [0, 1], and hence one verifies that:
Ìˆğ‘‘ğ‘œ(ğ‘¡) = âˆ’ğ‘˜ğ‘¤
ğœ™ğ‘(ğ‘¡)
Ì‡ğ‘‘ğ‘œ(ğ‘¡)
ğ‘‘ğ‘œ(ğ‘¡) âˆ’ğ‘˜ğ‘¤
ğœ™ğ›½ğ‘¤
(66)
with ğ‘(ğ‘¡) =
(1âˆ’ğ›¼ğ‘¤(ğ‘¡))ğ‘‘ğ‘œ(ğ‘¡)+ğ›¼ğ‘¤(ğ‘¡)ğ‘‘ğ‘’(ğ‘¡)
ğ‘‘ğ‘’(ğ‘¡)
a positive uniformly continuous and
bounded function as long as ğœ–< â€–ğ‘ğ‘¤(ğ‘¡)â€– < ğœ–+ ğ›¿and ğ‘‘ğ‘œ(0) âˆˆR+.
direct application of HÃ©rissÃ© et al. (2012, Th. 5.1) to both Eqs. (65)
and (66), one can conclude that as long as ğ‘‘ğ‘œ(0) âˆˆR+ and â€–ğ‘ğ‘¤(ğ‘¡)â€– > ğœ–
(or equivalently ğœ‰ğ‘¤âˆ‰î‰ƒ), ğ‘‘ğ‘œ(ğ‘¡) âˆˆR+, âˆ€ğ‘¡â‰¥0 and ğ‘‘ğ‘œ(ğ‘¡) converges to
zero exponentially (the exponential convergence of ğ‘‘ğ‘œ(ğ‘¡) is granted due
to the fact that ğ›½ğ‘¤â‰¥ğœ–) but never crosses zero and hence the vehicle
will never touch the wall in a finite time. Additionally, one also proves,
from HÃ©rissÃ© et al. (2012, Th. 5.1), that there exists a finite time ğ‘¡1 â‰¥0
such that Ì‡ğ‘‘ğ‘œ(ğ‘¡) < 0, âˆ€ğ‘¡â‰¥ğ‘¡1 and hence ğ‘‘ğ‘œand ğ‘‘ğ‘¤are decreasing after ğ‘¡1.
When â€–ğ‘ğ‘¤(ğ‘¡)â€– â‰¤ğœ–(the situation when ğœ‰ğ‘¤âˆˆî‰ƒ), one has ğ‘‘ğ‘¤= ğ‘‘ğ‘’>
ğ‘‘ğ‘œ. In this case one can easily verify that (65) can be rewritten as:
Ìˆğ‘‘ğ‘œ= âˆ’ğ‘˜(ğ‘¡) Ì‡ğ‘‘ğ‘œâˆ’ğ‘˜ğ‘¤
ğœ™ğ›½ğ‘¤
(67)
with ğ‘˜(ğ‘¡) =
ğ‘˜ğ‘¤
ğœ™
ğ‘‘ğ‘’a upper bounded positive gain as long as ğ‘‘ğ‘’(ğ‘¡) is positive.
Due to the fact that ğ›½ğ‘¤â‰¥ğœ–, Ì‡ğ‘‘ğ‘œis ultimately bounded by âˆ’
ğ‘˜ğ‘¤
ğœ™ğ›½ğ‘¤
ğ‘˜(ğ‘¡) â‰¤âˆ’
ğ‘˜ğ‘¤
ğœ™ğœ–
ğ‘˜(ğ‘¡)
and hence one immediately ensures that there exists a finite time ğ‘¡2 â‰¥0
from which Ì‡ğ‘‘ğ‘œ(ğ‘¡) < 0, âˆ€ğ‘¡â‰¥ğ‘¡2. This implies that when â€–ğ‘ğ‘¤(ğ‘¡)â€– â‰¤ğœ–(ğœ‰ğ‘¤âˆˆ
î‰ƒ), ğ‘‘ğ‘œis decreasing âˆ€ğ‘¡â‰¥ğ‘¡2 and hence ğ‘‘ğ‘œcrosses zero in a finite time
Ì„ğ‘¡> ğ‘¡2. Note that at ğ‘¡= Ì„ğ‘¡, one has ğœ(ğ‘ğ‘¤(Ì„ğ‘¡)) = 0 according to (35).
Consider now the dynamics of the closed-loop system (36) in the
plane ğœ‹ğœ‚ğ‘¤. That is the dynamics of ğœ‰âŸ‚âˆ¶= ğœ‹ğœ‚ğ‘¤ğœ‰ğ‘¤. By defining ğ‘£âŸ‚âˆ¶= ğœ‹ğœ‚ğ‘¤ğ‘£
and â–µâŸ‚âˆ¶= ğœ‹ğœ‚ğ‘¤â–µ, one gets:
Ì‡ğœ‰âŸ‚=ğ‘£âŸ‚
(68)
Ì‡ğ‘£âŸ‚= âˆ’
ğ‘˜ğ‘¤
ğ‘‘
ğ‘‘ğ‘¤
(ğ‘£âŸ‚+
ğ‘˜ğ‘¤
ğ‘
ğ‘˜ğ‘¤
ğ‘‘
ğœ‰âŸ‚)+ â–µâŸ‚.
(69)
Define a new state
ğ‘§= ğ‘£âŸ‚+
ğ‘˜ğ‘¤
ğ‘
ğ‘˜ğ‘¤
ğ‘‘
ğœ‰âŸ‚,
(70)
and the following positive definite storage function:
îˆ¸3 = 1
2 â€–ğ‘§â€–2 + 1
2(
ğ‘˜ğ‘¤
ğ‘
ğ‘˜ğ‘¤
ğ‘‘
)2â€–ğœ‰âŸ‚â€–2,
with time derivative
Ì‡îˆ¸3 = âˆ’(
ğ‘˜ğ‘¤
ğ‘
ğ‘˜ğ‘¤
ğ‘‘
)3â€–ğœ‰âŸ‚â€–2 âˆ’(
ğ‘˜ğ‘¤
ğ‘‘
ğ‘‘ğ‘¤
âˆ’
ğ‘˜ğ‘¤
ğ‘
ğ‘˜ğ‘¤
ğ‘‘
)â€–ğ‘§â€–2 + ğ‘§âŠ¤â–µâŸ‚
â‰¤âˆ’(
ğ‘˜ğ‘¤
ğ‘
ğ‘˜ğ‘¤
ğ‘‘
)3â€–ğœ‰âŸ‚â€–2 âˆ’â€–ğ‘§â€–
ğ‘˜ğ‘¤
ğ‘‘
((
ğ‘˜ğ‘¤
ğ‘‘
2
ğ‘‘ğ‘¤
âˆ’ğ‘˜ğ‘¤
ğ‘)â€–ğ‘§â€– âˆ’ğ‘˜ğ‘¤
ğ‘‘â€– â–µâŸ‚â€–),
(71)
which is negative-definite provided that 0 < ğ‘‘ğ‘¤<
ğ‘˜ğ‘¤
ğ‘‘
2
ğ‘˜ğ‘¤ğ‘
and â€–ğ‘§â€– â‰¥
ğ‘‘ğ‘¤ğ‘˜ğ‘¤
ğ‘‘â€–â–µâŸ‚â€–max
ğ‘˜ğ‘¤
ğ‘‘
2âˆ’ğ‘‘ğ‘¤ğ‘˜ğ‘¤ğ‘
.
Proof of Item 1:
To show there exists a finite time ğ‘¡ğ‘¤â‰¥0 at which the vehicle enters
the region î‰ƒand remains there as long as ğœ(ğ‘ğ‘¤) = 1, one proceeds
using a proof by contradiction in two steps.
In the first step, assume that ğœ‰ğ‘¤is not converging to î‰ƒin a finite
time ğ‘¡ğ‘¤and hence â€–ğ‘ğ‘¤(ğ‘¡)â€– > ğœ–, âˆ€ğ‘¡. In the second one, assume that ğœ‰ğ‘¤is
switching indefinitely between the two regions.
(i) Consider the situation for which the initial condition is such that
â€–ğ‘ğ‘¤(0)â€– > ğœ–(outside the region î‰ƒ). Using the fact that there exists
a finite time instant ğ‘¡1 from which ğ‘‘ğ‘¤is decreasing and converging to
zero but never crosses zero in finite time (see the above discussion), one
concludes that ğ‘§(70) is exponentially converging to zero and hence:
ğ‘£âŸ‚= Ì‡ğœ‰âŸ‚= âˆ’
ğ‘˜ğ‘¤
ğ‘
ğ‘˜ğ‘¤
ğ‘‘
ğœ‰âŸ‚+ ğ‘œ(ğ‘¡),
13

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
with ğ‘œ(ğ‘¡) an exponential vanishing term. This in turn implies that ğœ‰âŸ‚
(resp. ğ‘£âŸ‚) is converging to zero exponentially. Combining this with the
fact that ğ‘‘ğ‘¤(ğ‘¡) (resp. ğ‘‘ğ‘œ(ğ‘¡)) is converging to zero, one concludes that
there exists a finite time ğ‘¡ğ‘¤at which â€–ğ‘ğ‘¤(ğ‘¡ğ‘¤)â€– < ğœ–(ğœ‰ğ‘¤(ğ‘¡ğ‘¤) âˆˆî‰ƒ), which
contradicts the first part of the assumption.
(ii) Consider the situation for which the vehicle is switching indefinitely
between the two regions. Since ğ‘‘ğ‘œ(ğ‘¡) (respectively ğ‘‘ğ‘¤) is decreasing
âˆ€ğ‘¡â‰¥max{ğ‘¡1, ğ‘¡2} for both cases of â€–ğ‘ğ‘¤â€– > ğœ–and â€–ğ‘ğ‘¤â€– â‰¤ğœ–with the
fact that (ğœ‰âŸ‚, ğ‘£âŸ‚) converges exponentially to (0, 0) (proof of the step
(i)), one concludes that there exists a finite time ğ‘¡ğ‘¤â‰¥0 at which the
vehicle enters the region î‰ƒ(â€–ğ‘ğ‘¤(ğ‘¡ğ‘¤)â€– â‰¤ğœ–), and remains there as long
as ğœ(ğ‘ğ‘¤) = 1, which contradicts the assumption.
Combining this with the discussion following (66), one ensures that
there exists ğœ–1 > 0 such that ğ‘‘ğ‘¤(ğ‘¡) â‰¥ğ‘‘ğ‘œ(ğ‘¡) > ğœ–1, âˆ€ğ‘¡< ğ‘¡ğ‘¤.
Proof of Item 2:
When ğœ‰ğ‘¤is inside the region (â€–ğ‘ğ‘¤â€– â‰¤ğœ–), one guarantees that îˆ¸3
(71) is decreasing as long as 0 < ğ‘‘ğ‘¤<
ğ‘˜ğ‘¤
ğ‘‘
2
ğ‘˜ğ‘¤ğ‘
and â€–ğ‘§â€– â‰¥
ğ‘‘ğ‘¤ğ‘˜ğ‘¤
ğ‘‘â€–â–µâŸ‚â€–max
ğ‘˜ğ‘¤
ğ‘‘
2âˆ’ğ‘‘ğ‘¤ğ‘˜ğ‘¤ğ‘
. Now
since there exists a time Ì„ğ‘¡> ğ‘¡ğ‘¤such that ğ‘‘ğ‘œ(Ì„ğ‘¡) = 0, one concludes that
ğ‘¡lim exists and it is equal to Ì„ğ‘¡.
â–¡
Appendix D. Supplementary data
Supplementary material related to this article can be found online
at https://doi.org/10.1016/j.conengprac.2021.104827.
References
Bertrand, S., GuÃ©nard, N., Hamel, T., Piet-Lahanier, H., & Eck, L. (2011). A hierarchical
controller for miniature {VTOL} UAVs: Design and stability analysis using singular
perturbation theory. Control Engineering Practice, 19(10), 1099â€“1108.
Burton, A., & Radford, J. (1978). Thinking in perspective: Critical essays in the study of
thought processes (vol. 646). Routledge.
Chaumette, F., Hutchinson, S., & Corke, P. (2016). Visual servoing. In Springer handbook
of robotics (pp. 841â€“866). Springer.
DeSouza, G. N., & Kak, A. C. (2002). Vision for mobile robot navigation: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(2), 237â€“267.
Falanga, D., Mueggler, E., Faessler, M., & Scaramuzza, D. (2017). Aggressive quadrotor
flight through narrow gaps with onboard sensing and computing using active vision.
In 2017 IEEE international conference on robotics and automation (pp. 5774â€“5781).
IEEE.
Floreano, D., & Wood, R. J. (2015). Science, technology and the future of small
autonomous drones. Nature, 521(7553), 460.
Guo, D., & Leang, K. K. (2020). Image-based estimation, planning, and control for high-
speed flying through multiple openings. International Journal of Robotics Research,
Article 0278364920921943.
Hamel, T., & Mahony, R. (2002). Visual servoing of an under-actuated dynamic
rigid-body system: An image-based approach. IEEE Transactions on Robotics and
Automation, 18(2), 187â€“198.
Hamel, T., Mahony, R., Lozano, R., & Ostrowski, J. (2002). Dynamic modelling
and configuration stabilization for an X4-flyer. IFAC Proceedings Volumes, 35(1),
217â€“222.
HÃ©rissÃ©, B., Hamel, T., Mahony, R., & Russotto, F.-X. (2012). Landing a VTOL unmanned
aerial vehicle on a moving platform using optical flow. IEEE Transactions on
Robotics, 28(1), 77â€“89.
Ho, H., De Wagter, C., Remes, B., & De Croon, G. (2018). Optical-flow based self-
supervised learning of obstacle appearance applied to mav landing. Robotics and
Autonomous Systems, 100, 78â€“94.
Hutchinson, S., Hager, G., & Corke, P. (1996). A tutorial on visual servo control.
Robotics and Automation, IEEE Transactions on, 12(5), 651â€“670.
Le Bras, F., Hamel, T., Mahony, R., Barat, C., & Thadasack, J. (2014). Approach
maneuvers for autonomous landing using visual servo control. IEEE Transactions
on Aerospace and Electronic Systems, 50(2), 1051â€“1065.
Loianno, G., Brunner, C., McGrath, G., & Kumar, V. (2017). Estimation, control, and
planning for aggressive flight with a small quadrotor with a single camera and
IMU. IEEE Robotics and Automation Letters, 2(2), 404â€“411.
Mahony, R., Corke, P., & Hamel, T. (2008). Dynamic image-based visual servo control
using centroid and optic flow features. Journal of Dynamic Systems, Measurement,
and Control, 130(1), Article 011005.
Mahony, R., & Hamel, T. (2005). Image-based visual servo control of aerial robotic
systems using linear image features. IEEE Transactions on Robotics, 21(2), 227â€“239.
Michael, N., Shen, S., Mohta, K., Mulgaonkar, Y., Kumar, V., Nagatani, K., Okada, Y.,
Kiribayashi, S., Otake, K., & Yoshida, K. (2012). Collaborative mapping of an
earthquake-damaged building via ground and aerial robots. Journal of Field Robotics,
29(5), 832â€“841.
Peters, D. A., & HaQuang, N. (1988). Dynamic inflow for practical applications.
Rosa, L., Hamel, T., Mahony, R., & Samson, C. (2014). Optical-flow based strategies
for landing VTOL UAVs in cluttered environments. IFAC Proceedings Volumes, 47(3),
3176â€“3183.
Serra, P., Cunha, R., Hamel, T., Cabecinhas, D., & Silvestre, C. (2016). Landing of a
quadrotor on a moving target using dynamic image-based visual servo control. IEEE
Transactions on Robotics, 32(6), 1524â€“1535.
Serra, P., Cunha, R., Hamel, T., Silvestre, C., & Le Bras, F. (2015). Nonlinear image-
based visual servo controller for the flare maneuver of fixed-wing aircraft using
optical flow. IEEE Transactions on Control Systems Technology, 23(2), 570â€“583.
Serres, J. R., & Ruffier, F. (2017). Optic flow-based collision-free strategies: From insects
to robots. Arthropod Structure & Development, 46(5), 703â€“717.
Tang, Z., Cunha, R., Hamel, T., & Silvestre, C. (2018). Aircraft landing using dynamic
two-dimensional image-based guidance control. IEEE Transactions on Aerospace and
Electronic Systems, 55(5), 2104â€“2117.
Tang, Z., Cunha, R., Hamel, T., & Silvestre, C. (2018). Going through a window and
landing a quadrotor using optical flow. In 2018 European control conference (pp.
2917â€“2922). IEEE.
Tang, Z., Li, L., Serra, P., Cabecinhas, D., Hamel, T., Cunha, R., & Silvestre, C. (2015).
Homing on a moving dock for a quadrotor vehicle. In TENCON 2015â€“2015 IEEE
region 10 conference (pp. 1â€“6). IEEE.
VICON (2014). Motion capture systems from vicon. http://www.vicon.com.
Zingg, S., Scaramuzza, D., Weiss, S., & Siegwart, R. (2010). MAV navigation through
indoor corridors using optical flow. In IEEE international conference on robotics and
automation (pp. 3361â€“3368).
14

