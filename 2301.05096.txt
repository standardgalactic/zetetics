Asynchronous training of quantum reinforcement learning
Samuel Yen-Chi Chen1
1Wells Fargo
(Dated: January 13, 2023)
Abstract
The development of quantum machine learning (QML) has received a lot of interest recently
thanks to developments in both quantum computing (QC) and machine learning (ML). One of
the ML paradigms that can be utilized to address challenging sequential decision-making issues is
reinforcement learning (RL). It has been demonstrated that classical RL can successfully complete
many diﬃcult tasks. A leading method of building quantum RL agents relies on the variational
quantum circuits (VQC). However, training QRL algorithms with VQCs requires signiﬁcant amount
of computational resources. This issue hurdles the exploration of various QRL applications. In
this paper, we approach this challenge through asynchronous training QRL agents. Speciﬁcally,
we choose the asynchronous training of advantage actor-critic variational quantum policies. We
demonstrate the results via numerical simulations that within the tasks considered, the asyn-
chronous training of QRL agents can reach performance comparable to or superior than classical
agents with similar model sizes and architectures.
1
arXiv:2301.05096v1  [quant-ph]  12 Jan 2023

I.
INTRODUCTION
Quantum computing (QC) has been posited as a means of achieving computational supe-
riority for certain tasks that classical computers struggle to solve [1]. Despite this potential,
the lack of error-correction in current quantum computers has made it challenging to ef-
fectively implement complex quantum circuits on these ”noisy intermediate-scale quantum”
(NISQ) devices [2]. To harness the quantum advantages oﬀered by NISQ devices, the devel-
opment of specialized quantum circuit architectures is necessary.
Recent advances in the hybrid quantum-classical computing framework [3] that utilizes
both classical and quantum computing. Under this approach, certain computational tasks
that are expected to beneﬁt from quantum processing are executed on a quantum computer,
while others, such as gradient calculations, are performed on classical computers. This hybrid
approach aims to take advantage of the strengths of both types of computing to address a
wide range of tasks. Hybrid algorithms that utilize variational quantum circuits (VQC)
have proven to be eﬀective in a variety of machine learning tasks. VQCs are a subclass of
quantum circuits that possess tunable parameters, and their incorporation into QML models
has demonstrated success in a wide range of tasks [3, 4].
Reinforcement learning (RL) is a branch of machine learning that deals with sequential
decision making tasks. Deep neural network-based RL has achieved remarkable results in
complicated tasks with human-level [5] or super-human performance [6]. However, quantum
RL is a developing ﬁeld with many unresolved issues and challenges. The majority of existing
quantum RL models are based on VQC [7–11]. Although these models have been shown to
perform well in a variety of benchmark tasks, training them requires a signiﬁcant amount
of computational resources. The long training time limits the exploration of quantum RL’s
broad application possibilities. We propose an asynchronous training framework for quantum
RL agents in this paper. We focus on the asynchronous training of advantage actor-critic
quantum policies using multiple instances of agents running in parallel.
We show, using numerical simulations, that quantum models may outperform or be sim-
ilar to classical models in the various benchmark tasks considered. Furthermore, the sug-
gested training approach has the practical advantage of requiring signiﬁcantly less time for
training, allowing for more quantum RL applications.
The structure of this paper is as follows: In SectionII, we provide an overview of relevant
2

prior work and compare our proposal to these approaches. In SectionIII, we provide a brief
overview of the necessary background in reinforcement learning. In SectionIV, we introduce
the concept of variational quantum circuits (VQCs), which serve as the building blocks of
our quantum reinforcement learning agents. In SectionV, we present our proposed quantum
A3C framework. In SectionVI, we describe our experimental setup and present our results.
Finally, in Section VII, we oﬀer some concluding remarks.
II.
RELEVANT WORKS
The work that gave rise to quantum reinforcement learning (QRL) [12] may be traced
back to [13]. However, the framework demands a quantum environment, which may not
be met in most real-world situations. Further studies utilizing Grover-like methods include
[14, 15]. Quantum linear system solvers are also used to implement quantum policy iteration
[16]. We will concentrate on recent advancements in VQC-based QRL dealing with classical
environments.
The ﬁrst VQC-based QRL [7], which is the quantum version of deep Q-
learning (DQN), considers discrete observation and action spaces in the testing environments
such as Frozen-Lake and Cognitive-Radio. Later, more sophisticated eﬀorts in the area of
quantum DQN take into account continuous observation spaces like Cart-Pole [8, 9]. A
further development along this direction includes the using of quantum recurrent neural
networks such as QLSTM as the value function approximator [17] to tackle challenges such
as partial observability or environments requiring longer memory of previous steps. Various
methods such as hybrid quantum-classical linear solver are developed to ﬁnd value functions
[18]. A further improvement of DQN which can improve the agent convergence such as
Double DQN (DDQN) are also implemented within VQC framework in the work [19], in
which the authors apply QRL to solve robot navigation task. Recent advances in QRL
have led to the development of frameworks that aim to learn policy functions, denoted as
π, directly. These frameworks are able to learn the optimal policy for a given problem,
in addition to learning value functions such as the Q-function.
For example, the paper
[10] describes the quantum policy gradient RL through the use of REINFORCE algorithm.
Then, the work [11] consider an improved policy gradient algorithm called PPO with VQCs
and show that even with a small number of parameters, quantum models can outperform
their classical counterparts. Provable quantum advantages of policy gradient are shown in
3

the work [20]. Additional research, such as the work in [21], has explored the impact of
various post-processing methods for VQC on the performance of quantum policy gradients.
Several improved quantum policy gradient algorithms have been proposed in recent years,
including actor-critic [22] and soft actor-critic (SAC) [23, 24]. These modiﬁcations seek to
further improve the eﬃciency and eﬀectiveness of QRL methods. QRL has also been applied
to the ﬁeld of quantum control [25] and has been extended to the multi-agent setting [26–
28]. The work [29] were the ﬁrst to explore the use of evolutionary optimization for QRL.
In their work, multiple agents were initialized and run in parallel, with the top performing
agents being selected as parents to generate the next generation of agents. In the work [30],
the authors studied the use of advanced quantum policy gradient methods, such as the deep
deterministic policy gradient (DDPG) algorithm, for QRL in continuous action spaces.
In this work, we extend upon previous research on quantum policy gradient [10, 11, 22] by
introducing an asynchronous training method for quantum policy learning. While previous
approaches have employed single-threaded training, our method utilizes an asynchronous
approach, which may oﬀer practical beneﬁts such as reduced training time through the use
of multi-core CPU computing resources and the potential for utilizing multiple quantum
processing units (QPUs) in the future.
Our approach shares some similarities with the
evolutionary QRL method presented in [29], which also utilizes parallel computing resources.
However, our approach diﬀers in that individual agents can share their gradients directly
with the shared global gradient asynchronously, rather than waiting for all agents to ﬁnish
before calculating ﬁtness and creating the next generation of agents. This characteristic
may further improve the eﬃciency of the training process. These contributions represent a
novel advancement in the ﬁeld of quantum reinforcement learning.
III.
REINFORCEMENT LEARNING
Reinforcement learning (RL) is a machine learning framework in which an agent learns to
accomplish a given goal by interacting with an environment E in discrete time steps [31]. The
agent observes a state st at each time step t and then chooses an action at from the action
space A based on its current policy π. The policy is a mapping from a speciﬁc state st to the
probabilities of choosing one of the actions in A. After performing the action at, the agent
gets a scalar reward rt and the state of the following time step st+1 from the environment.
4

For episodic tasks, the procedure is repeated across a number of time steps until the agent
reaches the terminal state or the maximum number of steps permitted. Seeing the state
st along the training process, the agent aims to maximize the expected return, which can
be expressed as the value function at state s under policy π, V π(s) = E [Rt|st = s], where
Rt = PT
t′=t γt′−trt′ is the return, the total discounted reward from time step t. The value
function can be further expressed as V π(s) = P
a∈A Qπ(s, a)π(a|s), where the action-value
function or Q-value function Qπ(s, a) = E[Rt|st = s, a] is the expected return of choosing
an action a ∈A in state s according to the policy π. The Q-learning is RL algorithm to
optimize the Qπ(s, a) via the following formula
Q (st, at) ←Q (st, at)
+ α
h
rt + γ max
a
Q (st+1, a) −Q (st, at)
i
.
(1)
In contrast to value-based reinforcement learning techniques, such as Q-learning, which
rely on learning a value function and using it to guide decision-making at each time step,
policy gradient methods focus on directly optimizing a policy function, denoted as π(a|s; θ),
parametrized by θ. The parameters θ are updated through a gradient ascent procedure
on the expected total return, E[Rt]. A notable example of a policy gradient algorithm is
the REINFORCE algorithm, introduced in [32]. In the standard REINFORCE algorithm,
the parameters θ are updated along the direction ∇θ log π (at|st; θ) Rt, which is an unbiased
estimate of ∇θE [Rt]. However, this policy gradient estimate often suﬀers from high variance,
making training diﬃcult.
To reduce the variance of this estimate while maintaining its
unbiasedness, a term known as the baseline can be subtracted from the return. This baseline,
denoted as bt(st), is a learned function of the state st.
The resulting update becomes
∇θ log π (at|st; θ) (Rt −bt (st)). A common choice for the baseline bt(st) in RL is an estimate
of the value function V π(st).
Using this choice for the baseline often results in a lower
variance estimate of the policy gradient [31]. The quantity Rt −bt = Q(st, at) −V (st) can
be interpreted as the advantage A(st, at) of action at at state st. Intuitively, the advantage
can be thought of as the ”goodness or badness” of action at relative to the average value
at state st. This approach is known as the advantage actor-critic (A2C) method, where the
policy π is the actor and the baseline, which is the value function V , is the critic [31].
The asynchronous advantage actor-critic (A3C) algorithm [33] is a variant of the A2C
method that employs multiple concurrent actors to learn the policy through parallelization.
5

Asynchronous training of RL agents involves executing multiple agents on multiple instances
of the environment, allowing the agents to encounter diverse states at any given time step.
This diminished correlation between states or observations enhances the numerical stability
of on-policy RL algorithms such as actor-critic [33]. Furthermore, asynchronous training does
not require the maintenance of a large replay memory, thus reducing memory requirements
[33]. By harnessing the advantages and gradients computed by a pool of actors, A3C exhibits
impressive sample eﬃciency and robust learning performance, making it a prevalent choice
in the realm of reinforcement learning.
IV.
VARIATIONAL QUANTUM CIRCUIT
Variational quantum circuits (VQCs), also referred to as parameterized quantum circuits
(PQCs), are a class of quantum circuits that contain tunable parameters. These parame-
ters can be optimized using various techniques from classical machine learning, including
gradient-based and non-gradient-based methods. A generic illustration of a VQC is in the
central part of Figure 1.
The three primary components of a VQC are the encoding circuit, the variational circuit,
and the quantum measurement layer. The encoding circuit, denoted as U(x), transforms
classical values into a quantum state, while the variational circuit, denoted as V (θ), serves
as the learnable part of the VQC. The quantum measurement layer, on the other hand,
is utilized to extract information from the circuit. It is a common practice to repeatedly
execute the circuit, also known as ”shots,” in order to obtain the expectation values of
each qubit. A common choice is to use the Pauli-Z expectation values. Instead of being
binary integers, the values are received as ﬂoats. Additionally, other components, such as
additional VQCs or classical components such as DNN, can process the values obtained from
the circuit.
The VQC can operate with other classical components such as tensor networks (TN) [29,
34, 35] and deep neural networks (NN) to perform data pre-processing such as dimensional
reduction or post-processing such as scaling. We call such VQCs as dressed VQC, as shown
in Figure 1. The whole model can be trained in an end-to-end manner via gradient-based
[34, 35] or gradient-free methods [29]. For the gradient-based methods, the whole model
can be represented as a directed acyclic graph (DAG) and then back-propagation can be
6

applied. The success of such end-to-end optimization relies on the capabilities of calculating
the quantum gradients such as parameter-shift rule [36]. VQC-based QML models have
shown success in areas such as classiﬁcation [34–38], natural language processing [39–41]
and sequence modeling [42, 43].
U(x)
V(θ)
|0⟩
|0⟩
|0⟩
|0⟩
NN
NN
FIG. 1. Hybrid variational quantum circuit (VQC) architecture. The hybrid VQC archi-
tecture includes a VQC and classical neural networks (NN) before and after the VQC. NN can be
used to reduce the dimensionality of the input data and reﬁne the outputs from the VQC.
V.
QUANTUM A3C
The proposed quantum asynchronous advantage actor-critic (QA3C) framework consists
of two main components: a global shared memory and process-speciﬁc memories for each
agent. The global shared memory maintains the dressed VQC policy and value parameters,
which are modiﬁed when an individual process uploads its own gradients for parameter up-
dates. Each agent has its own process-speciﬁc memory that maintains local dressed VQC
policy and value parameters. These local models are used to generate actions during an
episode within individual processes. When certain criteria are met, the gradients of the
local model parameters are uploaded to the global shared memory, and the global model
parameters are modiﬁed accordingly. The updated global model parameters are then im-
mediately downloaded to the local agent that just uploaded its own gradients. The overall
concept of QA3C is depicted in Figure 2.
We construct the quantum policy π (at | st; θ) and value V (st; θv) function with the
dressed VQC as shown in Figure 1, in which the VQC follows the architecture shown in
7

⋯
Worker 1
Worker 2
Worker 3
Worker n
Environment 1
Environment 2
Environment 3
Environment n
Global Parameter
st
π(at|st)
V(st)
FIG. 2.
Quantum asynchronous advantage actor-critic (A3C) learner.
The proposed
quantum A3C includes a global shared parameters and multiple parallel workers.
The action
generation process within each local agent is performed using the dressed VQC policy and value
functions stored in the process-speciﬁc memories. Upon meeting certain criteria, the gradients of
the local model parameters are uploaded to the global shared memory, where the global model
parameters are updated. The updated global model parameters are then immediately downloaded
to the local agent that just uploaded its own gradients.
Figure 3. This VQC architecture has been studied in the work such as quantum recurrent
neural networks (QRNN) [42], quantum recurrent RL [17], quantum convolutional neural
networks [44], federated quantum classiﬁcation [38] and has demonstrated superior perfor-
mance over their classical counterparts under certain conditions. In addition, we employ
the classical DNN before and after the VQC to dimensionally reduce the data and ﬁne-tune
the outputs from the VQC, respectively. The neural network components in this hybrid
architecture consist of single-layer networks for dimensionality conversion. Speciﬁcally, the
network preceding the VQC is a linear layer with an input dimension equal to the size of the
observation vector and an output dimension equal to the number of qubits in the VQC. The
networks following the VQC are linear layers with input dimensions equal to the number
of qubits in the VQC and output dimensions equal to the number of actions (for the actor
function π (at | st; θ)) or 1 (for the critic function V (st; θv)). These layers serve to convert
8

the output of the VQC for use in the actor-critic algorithm. The policy and value function
are updated after every S steps or when the agent reaches the terminal state. The details
of the algorithm such as the gradient update formulas are presented in Algorithm 1.
|0⟩
H
Ry(arctan(x1))
Rz(arctan(x2
1))
•
•
R(α1, β1, γ1)
|0⟩
H
Ry(arctan(x2))
Rz(arctan(x2
2))
•
•
R(α2, β2, γ2)
|0⟩
H
Ry(arctan(x3))
Rz(arctan(x2
3))
•
•
R(α3, β3, γ3)
|0⟩
H
Ry(arctan(x4))
Rz(arctan(x2
4))
•
•
R(α4, β4, γ4)
FIG. 3. VQC architecture for quantum A3C. The VQC used here includes Ry and Rz for
encoding classical values x, multiple CNOT gates to entangle qubits, general unitary rotations R
and the ﬁnal measurement. The output of the VQC consists of Pauli-Z expectation values, which
are obtained through multiple runs (shots) of the circuit.
These values are then processed by
classical neural networks for further use. We use a 4-qubit system as an example here, however, it
can be enlarge or shrink based on the problem of interest. In this work, the number of qubit is 8.
VI.
EXPERIMENTS AND RESULTS
A.
Testing Environments
1.
Acrobot
The Acrobot environment from OpenAI Gym [45] consists of a system with two linearly
connected links, with one end ﬁxed. The joint connecting the two links can be actuated by
applying torques. The goal is to swing the free end of the chain over a predetermined height,
starting from a downward hanging position, using as few steps as possible. The observation
in this environment is a six-dimensional vector comprising the sine and cosine values of the
two rotational joint angles, as well as their angular velocities. The agents are able to take
one of three actions: applying −1, 0, or +1 torque to the actuated joint. An action resulting
in the free end reaching the target height receives a reward of 0 and terminates the episode.
Any action that does not lead to the desired height receives a reward of −1. The reward
threshold is −100.
9

FIG. 4. The Acrobat environment from OpenAI Gym.
2.
Cart-Pole
Cart-Pole is a commonly used evaluation environment for simple RL models that has
been utilized as a standard example with in OpenAI Gym [45] (see Figure 5).
A ﬁxed
junction connects a pole to a cart traveling horizontally over a frictionless track in this
environment. The pendulum initially stands upright, and the aim is to keep it as near to its
starting position as possible by moving the cart left and right. Each time step, the RL agent
learns to produce the right action according on the observation it gets. The observation in
this environment is a four dimensional vector st containing values of the cart position, cart
velocity, pole angle, and pole velocity at the tip. Every time step where the pole is near to
being upright results in a +1 award. An episode ends if the pole is inclined more than 15
degrees from vertical or the cart moves more than 2.4 units away from the center.
FIG. 5. The Cart-Pole environment from OpenAI Gym.
10

3.
MiniGrid-SimpleCrossing
The MiniGrid-SimpleCrossing environment [46] is more sophisticated, with a lot bigger
observation input for the RL agent. In this scenario, the RL agent receives a 7 × 7 × 3 =
147 dimensional vector through observation and must choose an action from the action
space A, which oﬀers six options. It is important to note that the 147-dimensional vector
is a compact and eﬃcient representation of the environment rather than the real pixels.
There are six actions 0,· · · ,5 in the action space A for the agent to choose.
They are
turn left, turn right, move forward, pick up an object, drop the object being carried and
toggle. Only the ﬁrst three of them are having actual eﬀects in this case. The agent is
expected to learn this fact. In this environment, the agent receives a reward of 1 upon
reaching the goal.
A penalty is subtracted from this reward based on the formula 1 −
0.9 × (number of steps/max steps allowed), where the maximum number of steps allowed is
deﬁned as 4 × n × n, and n is the grid size [46]. In this work, n is set to 9. This reward
scheme presents a challenge because it is sparse, meaning that the agent does not receive
rewards until it reaches the goal. As shown in Figure 6, the agent (shown in red triangle)
is expected to ﬁnd the shortest path from the starting point to the goal (shown in green).
We consider three cases in this environment: MiniGrid-SimpleCrossingS9N1-v0, MiniGrid-
SimpleCrossingS9N2-v0 and MiniGrid-SimpleCrossingS9N3-v0. Here the N represents the
number of valid crossings across walls from the starting position to the goal.
B.
Hyperparameters and Model Size
In the proposed QA3C, we use the Adam optimizer with learning rate 1×10−4, β1 = 0.92
and β2 = 0.999. The local agents will update the parameters with the global shared memory
every S = 5 steps. The discount factor γ is set to be 0.9. For the VQC, we set the number
of qubits to be 8 and two variational layers are used. Therefore, for each VQC, there are
8 × 3 × 2 = 48 quantum parameters. Actor and critic both have their own VQC, thus
the total number of quantum parameters is 96. The VQC architecture are the same across
various testing environments considered in this work. As we described in the Section V,
single layer networks are used before and after the VQC to convert the dimensions of data.
The networks preceding the VQC have input dimensions based on the environments that
11

(a)
(b)
(c)
FIG. 6. The SimpleCrossing environment from MiniGrid. The three environments from
MiniGrid-SimpleCrossing we consider in this work.
In each environment, there are also walls
which span 1 unit on each side (not shown in the ﬁgure).
(a), (b) and (c) represent exam-
ples from the MiniGrid-SimpleCrossingS9N1-v0, MiniGrid-SimpleCrossingS9N2-v0 and MiniGrid-
SimpleCrossingS9N3-v0 environments, respectively.
the agent is to solve. For the classical benchmarks, we consider the model which are very
similar to the dressed VQC model. Speciﬁcally, we keep the architecture of classical model
similar to the one presented in Figure 1 while we replace the 8-qubit VQC with a single
layer with input and output dimensions equal to 8. This makes the architecture very similar
to the quantum model and the number of parameters are also very close. We summarize
the number of parameters in Table I. We utilize the open-source PennyLane package [47]
QA3C
Classical
Classical Quantum Total
Total
Acrobot
148
96
244
292
Cart-Pole
107
96
203
251
SimpleCrossing
2431
96
2527
2575
TABLE I. Number of parameters. We provide details on the number of parameters in the
proposed QA3C model, which includes both quantum and classical components.
The classical
benchmarks were designed with architectures similar to the quantum model, resulting in similar
model sizes.
to construct the quantum circuit models and the PyTorch as a overall machine learning
12

framework. The number of CPU cores and hence the number of parallel agents is 80 in
this work. We present simulation results in which the scores from the past 100 episodes are
averaged.
C.
Results
1.
Acrobot
We begin by evaluating the performance of our models on the Acrobot environment. The
simulation results of this experiment are presented in Figure7. The total number of episodes
was 100,000. As shown in the ﬁgure, the quantum model exhibits a gradual improvement
during the early training episodes, while the classical model struggles to improve its policy.
In terms of average score, the quantum model demonstrates superior performance compared
to the classical model. Furthermore, the quantum model exhibits a more stable convergence
pattern, without signiﬁcant ﬂuctuations or collapses after reaching optimal scores. These
results suggest that the quantum model may be more robust and reliable in this environment.
0
20000
40000
60000
80000
100000
Episode #
500
400
300
200
100
0
Average Score
Quantum
Classical
FIG. 7. Results: Quantum A3C in the Acrobot environment.
13

2.
Cart-Pole
The next experiment was conducted in the Cart-Pole environment. The total number of
episodes was 100,000. As illustrated in Figure 8, the quantum model achieved signiﬁcantly
higher scores than the classical model. While the classical model demonstrated faster learn-
ing in the early training episodes, the quantum model eventually surpassed it and reached
superior scores. These results suggest that the quantum model may be more eﬀective in this
environment.
0
20000
40000
60000
80000
100000
Episode #
0
100
200
300
400
500
Average Score
Quantum
Classical
FIG. 8. Results: Quantum A3C in the CartPole environment.
3.
MiniGrid-SimpleCrossing
The ﬁnal experiment was conducted in the MiniGrid-SimpleCrossing environment, com-
prising a total of 100,000 episodes.
As depicted in Figure 9, among the three scenar-
ios, MiniGrid-SimpleCrossingS9N1-v0, MiniGrid-SimpleCrossingS9N2-v0, and MiniGrid-
SimpleCrossingS9N3-v0, the quantum model outperformed the classical model in two of
the three scenarios, MiniGrid-SimpleCrossingS9N2-v0 and MiniGrid-SimpleCrossingS9N3-
v0, demonstrating faster convergence and higher scores. Even in the remaining scenario,
MiniGrid-SimpleCrossingS9N1-v0, the diﬀerence in performance between the two models
14

was minor.
0
20000
40000
60000
80000
100000
Episode #
0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Average Score
S9N1-Quantum
S9N1-Classical
0
20000
40000
60000
80000
100000
Episode #
Average Score
S9N2-Quantum
S9N2-Classical
0
20000
40000
60000
80000
100000
Episode #
Average Score
S9N3-Quantum
S9N3-Classical
FIG. 9. Results: Quantum A3C in the MiniGrid-SimpleCrossing environment.
VII.
CONCLUSION
In this study, we demonstrate the eﬀectiveness of an asynchronous training framework for
quantum RL agents. Through numerical simulations, we show that in the benchmark tasks
considered, advantage actor-critic quantum policies trained asynchronously can outperform
or match the performance of classical models with similar architecture and sizes.
This
technique aﬀords a strategy for expediting the training of quantum RL agents through
parallelization, and may have potential applications in various real-world scenarios.
ACKNOWLEDGMENTS
The views expressed in this article are those of the authors and do not represent the views
of Wells Fargo. This article is for informational purposes only. Nothing contained in this
article should be construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications related to this
article.
15

Appendix A: Algorithms
1.
Quantum-A3C
Algorithm 1 Quantum asynchronous advantage actor-critic learning (algorithm for each
actor-learner process)
Deﬁne the global update parameter S
Assume global shared hybrid VQC policy parameter θ
Assume global shared hybrid VQC value parameter θv
Assume global shared episode counter T = 0
Assume process-speciﬁc hybrid VQC policy parameter θ′
Assume process-speciﬁc hybrid VQC value parameter θ′
v
Initialize process-speciﬁc counter t = 1
while T < Tmax do
Reset gradients dθ ←0 and dθv ←0
Set tstart = t
Reset the environment and get state st
while st non-terminal or t −tstart < tmax do
Perform at according to policy π(at|st; θ′)
Receive reward rt and the new state st+1
Update process-speciﬁc counter t ←t + 1
if t mod S = 0 or reach terminal state then
Set R =



0
for terminal st
V (st, θ′
v)
for non-terminal st
for i ∈{t −1, . . . , tstart } do
R ←ri + γR
Accumulate gradients wrt θ′: dθ ←dθ + ∇θ′ log π (ai | si; θ′) (R −V (si; θ′
v))
Accumulate gradients wrt θ′
v: dθv ←dθv + ∂(R −V (si; θ′
v))2 /∂θ′
v
end for
Perform asynchronous update of θ using dθ and of θv using dθv
Update process-speciﬁc parameters from global parameters: θ′ ←θ and θ′
v ←θv
end if
end while
end while
16

[1] M. A. Nielsen and I. L. Chuang, “Quantum computation and quantum information,” 2010.
[2] J. Preskill, “Quantum computing in the nisq era and beyond,” Quantum, vol. 2, p. 79, 2018.
[3] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea, A. Anand, M. Deg-
roote, H. Heimonen, J. S. Kottmann, T. Menke, et al., “Noisy intermediate-scale quantum
algorithms,” Reviews of Modern Physics, vol. 94, no. 1, p. 015004, 2022.
[4] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii, J. R. McClean,
K. Mitarai, X. Yuan, L. Cincio, et al., “Variational quantum algorithms,” Nature Reviews
Physics, vol. 3, no. 9, pp. 625–644, 2021.
[5] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep
reinforcement learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015.
[6] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,
L. Baker, M. Lai, A. Bolton, et al., “Mastering the game of go without human knowledge,”
nature, vol. 550, no. 7676, pp. 354–359, 2017.
[7] S. Y.-C. Chen, C.-H. H. Yang, J. Qi, P.-Y. Chen, X. Ma, and H.-S. Goan, “Variational
quantum circuits for deep reinforcement learning,” IEEE Access, vol. 8, pp. 141007–141024,
2020.
[8] O. Lockwood and M. Si, “Reinforcement learning with quantum variational circuit,” in Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence and Interactive Digital Entertain-
ment, vol. 16, pp. 245–251, 2020.
[9] A. Skolik, S. Jerbi, and V. Dunjko, “Quantum agents in the gym: a variational quantum
algorithm for deep q-learning,” Quantum, vol. 6, p. 720, 2022.
[10] S. Jerbi, C. Gyurik, S. Marshall, H. J. Briegel, and V. Dunjko, “Variational quantum policies
for reinforcement learning,” arXiv preprint arXiv:2103.05577, 2021.
[11] J.-Y. Hsiao, Y. Du, W.-Y. Chiang, M.-H. Hsieh, and H.-S. Goan, “Unentangled quantum
reinforcement learning agents in the openai gym,” arXiv preprint arXiv:2203.14348, 2022.
[12] N. Meyer, C. Ufrecht, M. Periyasamy, D. D. Scherer, A. Plinge, and C. Mutschler, “A survey
on quantum reinforcement learning,” arXiv preprint arXiv:2211.03464, 2022.
[13] D. Dong, C. Chen, H. Li, and T.-J. Tarn, “Quantum reinforcement learning,” IEEE Transac-
17

tions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 38, no. 5, pp. 1207–1220,
2008.
[14] S. Wiedemann, D. Hein, S. Udluft, and C. Mendl, “Quantum policy iteration via amplitude
estimation and grover search–towards quantum advantage for reinforcement learning,” arXiv
preprint arXiv:2206.04741, 2022.
[15] A. Sannia, A. Giordano, N. L. Gullo, C. Mastroianni, and F. Plastina, “A hybrid classical-
quantum approach to speed-up q-learning,” arXiv preprint arXiv:2205.07730, 2022.
[16] E. A. Cherrat, I. Kerenidis, and A. Prakash, “Quantum reinforcement learning via policy
iteration,” arXiv preprint arXiv:2203.01889, 2022.
[17] S.
Y.-C.
Chen,
“Quantum
deep
recurrent
reinforcement
learning,”
arXiv
preprint
arXiv:2210.14876, 2022.
[18] C.-C. CHEN, K. SHIBA, M. SOGABE, K. SAKAMOTO, and T. SOGABE, “Hybrid
quantum-classical ulam-von neumann linear solver-based quantum dynamic programing al-
gorithm,” Proceedings of the Annual Conference of JSAI, vol. JSAI2020, pp. 2K6ES203–
2K6ES203, 2020.
[19] D. Heimann, H. Hohenfeld, F. Wiebe, and F. Kirchner, “Quantum deep reinforcement learning
for robot navigation tasks,” arXiv preprint arXiv:2202.12180, 2022.
[20] S. Jerbi, A. Cornelissen, M. Ozols, and V. Dunjko, “Quantum policy gradient algorithms,”
arXiv preprint arXiv:2212.09328, 2022.
[21] N. Meyer, D. D. Scherer, A. Plinge, C. Mutschler, and M. J. Hartmann, “Quantum policy
gradient algorithm with optimized action decoding,” arXiv preprint arXiv:2212.06663, 2022.
[22] M. Schenk, E. F. Combarro, M. Grossi, V. Kain, K. S. B. Li, M.-M. Popa, and S. Vallecorsa,
“Hybrid actor-critic algorithm for quantum reinforcement learning at cern beam lines,” arXiv
preprint arXiv:2209.11044, 2022.
[23] Q. Lan, “Variational quantum soft actor-critic,” arXiv preprint arXiv:2112.11921, 2021.
[24] A. Acuto, P. Barill`a, L. Bozzolo, M. Conterno, M. Pavese, and A. Policicchio, “Variational
quantum soft actor-critic for robotic arm control,” arXiv preprint arXiv:2212.11681, 2022.
[25] A. Sequeira, L. P. Santos, and L. S. Barbosa, “Variational quantum policy gradients with an
application to quantum control,” arXiv preprint arXiv:2203.10591, 2022.
[26] W. J. Yun, Y. Kwak, J. P. Kim, H. Cho, S. Jung, J. Park, and J. Kim, “Quantum
multi-agent reinforcement learning via variational quantum circuit design,” arXiv preprint
18

arXiv:2203.10443, 2022.
[27] R. Yan, Y. Wang, Y. Xu, and J. Dai, “A multiagent quantum deep reinforcement learn-
ing method for distributed frequency control of islanded microgrids,” IEEE Transactions on
Control of Network Systems, vol. 9, no. 4, pp. 1622–1632, 2022.
[28] W. J. Yun, J. Park, and J. Kim, “Quantum multi-agent meta reinforcement learning,” arXiv
preprint arXiv:2208.11510, 2022.
[29] S. Y.-C. Chen, C.-M. Huang, C.-W. Hsing, H.-S. Goan, and Y.-J. Kao, “Variational quan-
tum reinforcement learning via evolutionary optimization,” Machine Learning: Science and
Technology, vol. 3, no. 1, p. 015025, 2022.
[30] S. Wu, S. Jin, D. Wen, and X. Wang, “Quantum reinforcement learning in continuous action
space,” arXiv preprint arXiv:2012.10711, 2020.
[31] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.
[32] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning,” Machine learning, vol. 8, no. 3-4, pp. 229–256, 1992.
[33] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and
K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in International
conference on machine learning, pp. 1928–1937, PMLR, 2016.
[34] S. Y.-C. Chen, C.-M. Huang, C.-W. Hsing, and Y.-J. Kao, “An end-to-end trainable hy-
brid classical-quantum classiﬁer,” Machine Learning: Science and Technology, vol. 2, no. 4,
p. 045021, 2021.
[35] J. Qi, C.-H. H. Yang, and P.-Y. Chen, “Qtn-vqc: An end-to-end learning framework for
quantum neural networks,” arXiv preprint arXiv:2110.03861, 2021.
[36] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, “Quantum circuit learning,” Physical
Review A, vol. 98, no. 3, p. 032309, 2018.
[37] M. Chehimi and W. Saad, “Quantum federated learning with quantum data,” in ICASSP
2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 8617–8621, IEEE, 2022.
[38] S. Y.-C. Chen and S. Yoo, “Federated quantum machine learning,” Entropy, vol. 23, no. 4,
p. 460, 2021.
[39] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, P.-Y. Chen, S. M. Siniscalchi, X. Ma, and C.-H. Lee,
“Decentralizing feature extraction with quantum convolutional neural network for automatic
19

speech recognition,” in ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 6523–6527, IEEE, 2021.
[40] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, Y. Tsao, and P.-Y. Chen, “When bert meets quan-
tum temporal convolution learning for text classiﬁcation in heterogeneous computing,” in
ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), pp. 8602–8606, IEEE, 2022.
[41] R. Di Sipio, J.-H. Huang, S. Y.-C. Chen, S. Mangini, and M. Worring, “The dawn of quan-
tum natural language processing,” in ICASSP 2022-2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 8612–8616, IEEE, 2022.
[42] S. Y.-C. Chen, S. Yoo, and Y.-L. L. Fang, “Quantum long short-term memory,” in
ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), pp. 8622–8626, IEEE, 2022.
[43] S. Y.-C. Chen, D. Fry, A. Deshmukh, V. Rastunkov, and C. Stefanski, “Reservoir computing
via quantum recurrent neural networks,” arXiv preprint arXiv:2211.02612, 2022.
[44] S. Y.-C. Chen, T.-C. Wei, C. Zhang, H. Yu, and S. Yoo, “Quantum convolutional neural
networks for high energy physics data analysis,” Physical Review Research, vol. 4, no. 1,
p. 013231, 2022.
[45] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba,
“Openai gym,” arXiv preprint arXiv:1606.01540, 2016.
[46] M. Chevalier-Boisvert, L. Willems, and S. Pal, “Minimalistic gridworld environment for openai
gym.” https://github.com/maximecb/gym-minigrid, 2018.
[47] V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, C. Blank, K. McKiernan, and N. Killoran, “Pen-
nylane: Automatic diﬀerentiation of hybrid quantum-classical computations,” arXiv preprint
arXiv:1811.04968, 2018.
20

