Objective Social Choice: Using Auxiliary Information to
Improve Voting Outcomes
Silviu Pitis
University of Toronto, Vector Institute
Toronto, Ontario, Canada
spitis@cs.toronto.edu
Michael R. Zhang
University of Toronto, Vector Institute
Toronto, Ontario, Canada
michael@cs.toronto.edu
ABSTRACT
How should one combine noisy information from diverse sources
to make an inference about an objective ground truth? This fre-
quently recurring, normative question lies at the core of statistics,
machine learning, policy-making, and everyday life. It has been
called “combining forecasts”, “meta-analysis”, “ensembling”, and
the “MLE approach to voting”, among other names. Past studies
typically assume that noisy votes are identically and independently
distributed (i.i.d.), but this assumption is often unrealistic. Instead,
we assume that votes are independent but not necessarily iden-
tically distributed and that our ensembling algorithm has access
to certain auxiliary information related to the underlying model
governing the noise in each vote. In our present work, we: (1) define
our problem and argue that it reflects common and socially relevant
real world scenarios, (2) propose a multi-arm bandit noise model
and count-based auxiliary information set, (3) derive maximum
likelihood aggregation rules for ranked and cardinal votes under
our noise model, (4) propose, alternatively, to learn an aggregation
rule using an order-invariant neural network, and (5) empirically
compare our rules to common voting rules and naive experience-
weighted modifications. We find that our rules successfully use
auxiliary information to outperform the naive baselines.*
KEYWORDS
objective social choice; ensemble methods; combining forecasts
ACM Reference Format:
Silviu Pitis and Michael R. Zhang. 2020. Objective Social Choice: Using
Auxiliary Information to Improve Voting Outcomes. In Proc. of the 19th
International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2020), Auckland, New Zealand, May 9–13, 2020, IFAAMAS, 10 pages.
1
INTRODUCTION
Many collective decision making processes aggregate noisy good
faith opinions in order to make an inference about some underlying
ground truth. In cooperative policy making, for example, each party
advocates for the policy they believe is objectively best. Similarly,
in academic peer review, a meta-reviewer combines good faith
reviewer opinions about a submitted paper. Other examples are
easy to come by. We refer to this setting as objective social choice,
to contrast it with the typical subjective social choice setting [35],
where the optimal choice is defined in terms of the voter utilities
*Code available at https://github.com/spitis/objective_social_choice
Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May
9–13, 2020, Auckland, New Zealand. © 2020 International Foundation for Autonomous
Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.
rather than a ground truth. Whereas subjective social choice can
be viewed as collective compromise, objective social choice can be
viewed as collective estimation.
Unlike the subjective setting, where it is natural to consider each
source or voter equally—an axiom known as “anonymity” [32]—
objective analysis suggests otherwise: diverse and more informed
opinions should be valued more. Many sensible, real-world set-
tings involve asymmetric (non-anonymous) voting, making this
a relevant line of analysis. Academic review is one. Another is
corporate governance, where different stakeholder classes have
varying voting powers, depending on the issue. In such cases, vary-
ing voter weights are natural, and one can evaluate the quality
of social choices via other avenues (e.g., direct evaluation [27] or
ex post analysis [19]). In other settings, such as national elections,
the objective approach raises ethical concerns of fairness, and the
objective approach may be inappropriate.
Although objective social choice has been the subject of numer-
ous studies in social choice [6, 10, 12, 44], forecasting [3, 9, 13],
statistics [16, 18] and machine learning [15, 37] (Section 2), to our
knowledge, no prior work has dealt with the case of non-i.i.d. ordi-
nal feedback (i.e., ranked preferences). Yet this is the case in many
practical applications. During peer review, for instance, two of three
reviewers might share primary areas of expertise, but being human,
cannot share comparable cardinal estimates. Or consider a robot
that must aggregate feedback from human principals. Once again,
the different principals will draw upon diverse background to form
their opinions, which can only be shared as ordinal preferences. In
each case, how should the non-i.i.d. feedback be aggregated?
Our work is intended as a first step toward answering this ques-
tion. To narrow the scope of our inquiry, we make several modeling
assumptions (Section 3), which we hope can be relaxed in future
work. In particular, we assume that (1) the underlying ground truth
and noise generating process is modeled as a k-armed bandit prob-
lem, where the different arms represent different alternatives, (2)
different voters see different pulls of the arms, and (3) the social
decision rule sees how many pulls each voter saw (but not their
outcomes). We solve for the maximum likelihood social choice in
a series of cases (Section 4). As our derived rules rest on strong
assumptions about the noise generation process, we also propose
to learn a more flexible aggregation rule using an order-invariant
neural network (Section 4.2). We empirically compare our derived
and learned rules to classical voting rules (Section 5). Our results
confirm the intuition that objective estimation can be improved by
up-weighing opinions from diverse and more informed sources.
arXiv:2001.10092v1  [cs.MA]  27 Jan 2020

AAMAS’20, May 9–13, 2020, Auckland, New Zealand
Silviu Pitis and Michael R. Zhang
2
RELATED WORK
Social Choice. The fundamental question of social choice asks:
how should we combine the preferences of many into a social
preference? [2, 38]. While the usual approach evaluates the social
preference in terms of the preferences of individuals [1, 22, 35]
(“subjective” social choice), a line of papers frame individual pref-
erences as noisy reflections of an underlying ground truth and
evaluate the social preference by comparing to the ground truth
(“objective” social choice). Perhaps the first is due to Condorcet
[10], who studied the problem where voters rank two alternatives
correctly with some probability p > 1
2. This simple noise model
is a special case of the n-alternative Mallows model [31], accord-
ing to which each voter ranks each pair of alternatives correctly
with probability p > 1
2 (and votes are redrawn if a cycle forms).
Young [44] generalized Condorcet’s analysis to general Mallows
noise (n > 2 alternatives), and showed that the Kemeny voting rule
returns the maximum likelihood (MLE) estimate of the truth for
this noise model. Conitzer and Sandholm [12] further extended this
“maximum likelihood” analysis to other voting rules and i.i.d. noise
models; their main results include a proof that any so-called “scor-
ing rule” (e.g., plurality, Borda count, veto) is the MLE estimator for
some i.i.d. noise model, as well as proofs that certain other voting
rules (e.g., Copeland) are not MLE estimators for any i.i.d. noise
model. Caragiannis et al. [5, 6] consider the sample complexity
necessary to ensure high likelihood reconstructions of the ground
truth under Mallow’s noise. Also related is the independent con-
versations model in social networks, where independent pairs of
voters receive information about some ground truth. Conitzer [11]
introduces the model for two alternatives and constructs the maxi-
mum likelihood estimator, which he shows to be #P-hard. Procaccia
et al. [36] extend and analyze this model for multiple alternatives.
Our work is unique in two respects. First, we do not assume i.i.d.
noise. Rather, we use a k-armed bandit noise model, which provides
a basic but plausible noise generating process that can account
for diversity in the subjective experiences of voters. Second, our
approach is cardinal: rather than apply noise directly to ranked
preferences, we apply noise to a cardinal ground truth. As Procaccia
and Rosenschein [35] introduced cardinal analysis into subjective
social choice, we do so in the objective case.
Forecasting, Statistics and Machine Learning. Numerous papers
in forecasting and statistics have examined the combination of
estimates. Bates and Granger [3] provided an early derivation of op-
timal weights for linearly combining two cardinal estimates. Their
analysis was extended to the n estimate case by Dickinson [13, 14]
and improved by Granger and Ramanathan [21], among many oth-
ers [9, 20, 41]. While the literature on combining forecasts typi-
cally deals with point estimates (or time series thereof), significant
work has also been done on combining probability distributions
[17, 18, 26, 33]. In empirical statistics, the combination of experi-
mental results is known as meta-analysis [16]. Almost all work on
combining cardinal estimates considers linear combinations; this
can be justified by an appeal to Harsanyi’s theorem [22, 42], which
states (roughly) that any cardinal—in the VNM expected utility
sense [40]—combination of cardinal estimates that satisfies Pareto
indifference (i.e., the combination is a function of the estimates
and nothing else) can be expressed as a linear combination of the
estimates.
Combining estimators through ensembles is a common tech-
nique used to improve inference performance in machine learning
[15, 37]. Much like the literature on combining forecasts, Perrone
and Cooper [34] and Tresp and Taniguchi [39] propose weighting
schemes that ensemble estimators based on their variances.
Our proposal differs from the above works in that (1) we combine
ordinal votes rather than cardinal predictions, and (2) we define an
underlying noise model and use count-based information rather
than empirical variances. Some recent works in reinforcement learn-
ing use ensembles in an ordinal setting [7, 8], but these works use
naive ensembling techniques (majority vote and arithmetic mean).
The dueling bandit problem setting is similar to ours, in that
ordinal comparisons are used to make an inference about an un-
derlying, (potentially) cardinal bandit [45]. As it uses repeat online
comparisons rather than count (or other similarity) information,
the dueling bandits formulation is more suitable to interactive and
online applications such as ad placement and recommender sys-
tems than one-shot votes. Our work could potentially be applied to
initialize an online bandit when historical information is available.
3
MODEL
We first present a generic framework for objective social choice and
then describe the modeling assumptions we make for our work.
3.1
Formal Setup
We assume the existence of a ground truth, cardinal objective func-
tion V : A →R, where A is a finite set of alternatives, and define
n ≜|A|. We represent V by the vector [µ1, µ2, . . . , µn], where µi is
the “true quality” of alternative ai, and denote the optimal alterna-
tive by a∗≜arg maxi {µi }. m voters partially observe this ground
truth and provide our social choice rule f with their noisy votes.
Each such set of noisy votes is an element of the voting or obser-
vation space X, which can be seen as (part of) the input domain
of f . In general, there are many ways in which voters could make
their observations and provide their feedback. Regardless of the
precise details, it seems plain that a rule with access to the votes,
but to no other information (dom f = X) should satisfy anonymity
i.e., weigh each vote equally. It is also plain that for an anonymous
voting rule, whether or not votes are i.i.d. is irrelevant. Therefore,
our setting is only interesting when, in addition to votes, our voting
rule has access to some auxiliary information or context c ∈C, so
that dom f = X × C. As is the case for X, there are many options
one could consider for C, and we make specific assumptions below.
The codomain of f may either be (1) V, the set of valid ground
truth functions (with V ∈V), so that f outputs cardinal prediction
ˆV , (2) A, so that f outputs a single best alternative ˆa, or (3) the set
of ordinal rankings over the alternatives. Note that if the codomain
is V, one can consider this entire process as a sort of autoencoder:
there is a noise model д : V →X × C that produces the votes
and auxiliary information, and the job of our rule f : X × C →V
is (roughly speaking) to reconstruct the input to д. Thus, optimal
rules are closely tied to noise models; cf. [12]. Figure 1 summarizes
the objective social choice framework.

Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes AAMAS’20, May 9–13, 2020, Auckland, New Zealand
Figure 1: A generic framework for objective social choice.
The ground truth V ∈V passes through noise model д to
generate the votes x ∈X and contexts c ∈C for m voters.
The rule f is applied to generate social choice f (x,c).
In addition to specifying the noise model д, voting format X,
auxiliary information C, and codomain of f , we must also specify
an objective function: what makes a given rule or rule selection
algorithm “good”? As usual, the answer will depend on the context.
In our present work we seek the rule that corresponds to a maximum
likelihood estimate (MLE) ˆV of the ground truth V ; that is, given
data (x,c) ∈X × C, the output of f is consistent with the ˆV that is
most likely to have generated votes x conditioned on the context
c. It should be noted that where f returns a best alternative or
ranking over alternatives, this problem formulation is different from
finding the most likely best alternative or most likely ranking over
alternatives, as done under ordinal (Mallows) noise [12, 44]—in our
setting, the maximum likelihood alternative and ranking depends
on a distributional estimate of ˆV . The MLE rule may not be the
empirically best rule, and so to compare voting rules in Subsection
5, we will use the notion of regret, defined as V (a∗) −V (ˆa), where ˆa
is the alternative most preferred by f .
3.2
Specific Modeling Assumptions
To narrow the scope of our present inquiry, we make the following
assumptions about д and C:
Assumption 1. The voters observe the n-dimensional ground truth
V = [µ1, µ2, . . . , µn] through an n-arm stochastic bandit [29]. Each
arm reveals information about the corresponding dimension of V , and
voters observe samples from arm i according to ri ∼N(µi,σ2
i ), where
σ2
i is the variance of arm i. To simplify analysis, we assume that the
σ2
i are either known or equal.
Assumption 2. There are m voters, where the i-th voter sees the
j-th arm pulled cij times. Let ¯cj = Í
i cij. Each voter sees different
(independently sampled) pulls—thus, vote noise is independent, but
not identically distributed.
Assumption 3. The auxiliary information c ∈C consists of the
observation counts for each voter. For each voter i, this is number of
pulls for each arm: [ci1,ci2, . . . ,cin].
Assumption 4. Voter i estimates V as Xi = [xi1,xi2, . . . ,xin],
which determines their vote (specific details below).
The above assumptions leave open the voting format X, and also
the output (codomain) of our voting rule f . We explore different
combinations of these in Section 4 below.
Although there are many alternatives to Assumptions 1-4, these
basic assumptions strike us as a simple, yet flexible model. Many
noise processes can be framed as bandits. Take peer review for
instance: one could designate an arm for each paper under review
(and accept the top k). Similarly, count information, which serves
as a proxy for voter experience, provides a generic way of char-
acterizing the “non-i.i.d.ness” of votes (an extension to our work
might examine the case where some voters observe the same pulls,
leading to dependent votes). Other interesting choices may include
voter similarities, as specified by some kernel function (this would
model the votes as a sample from a Gaussian process), or empirical
covariance measurements (obtained by observing several votes).
The assumption of Gaussian noise is relaxed in Subsection 4.2 and
our experiments.
4
AGGREGATION RULES
4.1
Derived Rules
In this section we analyze MLE social choice under the specific
modeling assumptions made above. We do this in a series of five
cases of roughly increasing complexity where, in each case, we
derive one or more scoring rules [12]. Scoring rules, such as the
Borda, plurality and veto rules [4], compute for each alternative j a
single aggregate score (or predicted utility, ˆVj) by taking a simple
sum across individual voter weights (i.e., ˆVj = Í
i wij, where the
wij is the weight of voter i’s vote for arm j). The alternatives are
ranked according to these numbers and the top scoring alternative
is selected. For example, the commonly used plurality rule assigns
weight wij = 1 to voter i’s top choice j, and wik = 0 for k , j,
which results in selecting the alternative that is ranked first most
often. In the two alternative cases below (cases 2 and 3), where the
derived weights wi do not have a j subscript, voter i’s top choice
gets weight wi and their second choice gets weight −wi (or 0, since
only relative weight matters).
The first two cases below, which use cardinal votes, simply recast
known results into our setting. The latter three use ordinal votes
and are novel contributions.
Case 1 (Many alternatives, votes are cardinal means).
There are n arms, and voter i provides their cardinal votes {xij }
for each arm j, where xij is the mean of i’s observations for arm j.
Solution. Had our aggregation rule seen the pulls itself, its MLE
estimate ˆµj of the true mean µj would be the mean observed reward,
which can be computed directly from the available information:
ˆµj = (r1j + r2j + · · · + r ¯cj)/¯cj
= (c1jx1j + c2jx2j + · · · + cmjxmj)/(
Õ
i
cij),
so that wij ∝cij.
□
Note that xij ∼N(µj,σ2
j /cij), so that each estimate is weighted
inversely proportional to its variance, σ2
j /cij. The use of inverse
variance to weight independent cardinal estimates is well known
[3, 13, 16, 34, 39].
Case 2 (2 alternatives, votes are cardinal differences).
There are n = 2 arms, and each voter i provides their estimate yi =
ci2 −ci1 of the cardinal difference between arms.

AAMAS’20, May 9–13, 2020, Auckland, New Zealand
Silviu Pitis and Michael R. Zhang
Solution. As xi1 and xi2 are independent, we have that xi2 −
xi1 ∼N(µ2 −µ1, σ 2
2
ci2 + σ 2
1
ci1 ) = N(µ2 −µ1, σ 2
2 ci1+σ 2
1 ci2
ci2ci1
). To combine
the votes we take the weighted mean with weights proportional to
the inverse variances [3], so that wi ∝
ci2ci1
σ 2
2 ci1+σ 2
1 ci2 .
□
Unlike Case 1, where the σ2
j was irrelevant to wij, the weights
in Case 2 depend on σ2
1 /σ2
2. We assumed above that this ratio is
known; if not, one might infer the ratio from data. An interesting
corollary is that a voter that wishes to maximize the weight of her
vote should pull each of the arms equally. If all voters adopt this
strategy, we do not need estimates of the variances of the arms and
can just weigh each vote in proportion to voter experience.
Case 3 (2 alternatives, votes are ordinal ranks). There are 2
arms, and each voter i provides an ordinal ranking (a1,a2) indicating
that they value a1 higher than a2 (i.e., xi1 ≥xi2).
Solution. As above, we havexi2−xi1 ∼N(µ2−µ1, σ 2
2 ci1+σ 2
1 ci2
ci2ci1
).
Denoting the CDF ofxi2−xi1 by Φi, and defining the binary variable
Yi = Ixi2−xi1 ≤0, we have Yi ∼B(Φi(0)) (the Bernoulli distribution
parameterized by Φi evaluated at 0). Our votes x ∈X consist of
a set of samples {y1 ∼Yi,y2 ∼Y2, . . .ym ∼Ym}. Since adding a
constant to the underlying means has no effect on the likelihood,
a direct inference about V = (µ1, µ2) is impossible and we instead
seek to estimate the difference µ2 −µ1. Defining, s2
i = σ 2
2 ci1+σ 2
1 ci2
ci2ci1
,
we want to choose ∆≜ˆµ2 −ˆµ1 to maximize the log-probability of
the data (since ∆which maximizes the log likelihood also maximizes
the likelihood):
log Pr(D; ∆) =
Õ
i
log Φi(0)yi (1 −Φi(0))1−yi
=
Õ
i
yi log
h
1
2 + 1
2erf

−∆
si
√
2
i
+ (1 −yi) log
h
1
2 −1
2erf

−∆
si
√
2
i
.
We could try to optimize directly with respect to ∆by setting
d
d∆log Pr(D; ∆) = 0, but this appears analytically intractable:
0 =
Õ
i
yi
−2
si
√
2π exp −∆2
2s2
i
h
1 + erf

−∆
si
√
2
i −(1 −yi)
−2
si
√
2π exp −∆2
2s2
i
h
1 −erf

−∆
si
√
2
i .
However, since log Pr(D; ∆) is concave (proof in Appendix), its
gradient evaluated at ∆= 0 points in the direction of the MLE
solution and we can use this fact to find Y corresponding to MLE
estimate of ∆by evaluating (see Appendix for details):
d
d∆log Pr(D; ∆)(0) ∝
Õ
i

−yis−1
i
+ (1 −yi)s−1
i

,
so that wi ∝s−1
i
=
q
ci2ci1
σ 2
2 ci1+σ 2
i ci2 .
□
Case 4 (Many alternatives, votes are ordinal ranks). There
are n arms, and voter i provides an ordinal ranking indicating whether
they prefer aj to ak (i.e., whether xij ≥xik) for all pairs (aj,ak).
Approximate solution. Though we were unable to solve this
case exactly, we take advantage of a naive independence assump-
tion (a la Naive Bayes [30]) to arrive at a plausible, approximate
aggregation rule. We will confirm in Section 5 that it empirically
outperforms the baselines. As above, we define binary variable
Yi,j<k ≜Ixij−xik ≤0 (indicating that k is preferred to j), so that
votes are a set of samples {yi,j<k ∼Yi,j<k }, and assume:
Assumption (Naive independence). For all i and distinct pairs
(j,k) and (s,t), variables Yi,j<k and Yi,s<t are independent.
This assumption is never true for n > 2. To see this, consider the
alternatives j,k, ℓ, and note that yi,j<k = 1 and yi,k<ℓ= 1 imply
yi,j<ℓ= 1 (by transitivity of the underlying cardinal values), which
violates independence. Nevertheless, by using this assumption, we
can apply our Case 3 strategy by rewriting the probability of the
data as a sum over the probabilities of the pairwise votes:
log Pr(D; ˆV ) =
Õ
j,k
log Pr(Djk; ∆jk)
where ˆV = [ˆµ1, ˆµ2, . . . , ˆµn], ∆jk ≜ˆµj−ˆµk as above, and Pr(Djk; ∆jk)
is the probability of observing the voters’ pairwise comparisons
between aj and ak given ∆jk (ignoring other alternatives). Noting
that ∂∆jk
∂ˆµj = 1 and ∂∆jk
∂ˆµk = −1, we can apply our Case 3 solution to
find the partial derivatives of log Pr(Djk; ˆV ), evaluated at ˆV = 0,
with respect to ˆµj and ˆµk. Summing across alternative pairs yields:
wij ∝
Õ
k,j

−yi,j<k + (1 −yi,j<k)
 s
cijcik
σ2
kcij + σ2
j cik
.
(1)
The above solution might be improved by examining the follow-
ing failure mode, which arises on account of the Naive Indepen-
dence assumption. If the best alternative j is observed significantly
less often than the second best alternative k—i.e., Í
i cij < Í
i cik—
the second best alternative will tend to receive more positive weight,
even if all voters report the correct pairwise ordering. For example,
in the case of one voter, if that voter reports the correct ordering
for three alternatives with counts 1 (for the top alternative), 10,
and 10, the above approximate solution will choose the second best
alternative. This is obviously a bad outcome. To avoid it, we propose
that each alternative’s weight be normalized by the total absolute
weight it would otherwise received, yielding normalized weights:
wij =
wij
Õ
i
Õ
k,j
s
cijcik
σ2
kcij + σ2
j cik
,
where wij is defined as above. Our experiments test both the un-
normalized (wij) and normalized (wij) versions of the rule.
□
Case 5 (Many alternatives, votes are top choice only).
There are n arms, and each voter i provides their top choice aj indi-
cating that they most prefer aj (i.e., xij ≥xik, ∀k , j).
Approximate solutions. Letϕij(x) and Φij(x) denote the Gauss-
ian PDF and CDF for xij. We have that the probability of voter i
selecting alternative j is equal to the probability that the largest
order statistic of Xi,9j (where Xi,9j denotes the set {xik |k , j}) is
less than xij:
Pr(Yij) =
∫∞
−∞
Pr(xj = s)Pr(max Xi,9j ≤s)ds
= Ex∼ϕij
Ö
k,j
Φik(x).

Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes AAMAS’20, May 9–13, 2020, Auckland, New Zealand
The log-likelihood of the data is therefore:
log Pr(D; ˆV ) =
Õ
i
log Ex∼ϕij
Ö
k,j
Φik(x)
where the expectation for each voter is taken with respect to that
voter’s top choice aj (we are abusing notation slightly, as aj differs
across voters). While there appears to be no way to maximize this
analytically [23], we can compute the gradient with respect to ˆV :
∇log Pr(D; ˆV ) =
n
Õ
i=1
∇log Ex∼ϕij
Ö
k,j
Φik(x)
=
n
Õ
i=1
1
f (i)∇Ex∼ϕij
Ö
k,j
Φik(x)
=
n
Õ
i=1
1
f (i)
∫∞
−∞
ϕij(x)
ϕij(x)∇

ϕij(x)
Ö
k,j
Φik(x)

dx
=
n
Õ
i=1
1
f (i)Ex∼ϕij

 ∇logϕij(x) Ö
k,j
Φik(x) + ∇
Ö
k,j
Φik(x)

=
n
Õ
i=1
1
f (i)Ex∼ϕij [д(i,x)s(i,x) + ∇s(i,x)]
where the third and fourth equalities use the log derivative trick of
the REINFORCE [43] gradient estimator together with the product
rule, and f (i), д(i,x), and s(i,x) are defined accordingly:
f (i) = Ex∼ϕijs(i,x)
s(i,x) =
Ö
k,j
Φik(x)
д(i,x) = ∇logϕij(x)
We thus have a method for Monte Carlo estimation of the gradi-
ent of the log likelihood. Each term has an intuitive justification.
f (i) represents a weight for each voter. A voter whose vote is in line
with the current guess of the underlying arm means has less weight
on the gradient. д(i,x)s(i,x) is part of the typical REINFORCE [43]
objective and corresponds to increasing the probability in regions
where the score (product of the CDFs of the remaining arms) is
high. Finally ∇s(i,x) is the correction term that appears due to
the dependence of the score function on the arm means. ∇s(i,x)
incentivizes decreasing the means of the arms that are not voted
for. Initializing ˆV = 0, we can either compute the gradient once
and take the maximal component to be the winner (as in Case 3,
but without the optimality guarantee) or use the gradient ascent
algorithm to find an optimum. We will do the former, and call it
the Case 5 “Monte Carlo approximation.”
In terms of implementation,д(i,x),s(i,x), and ∇s(i,x) are straight-
forward and can be done with a library that computes density func-
tions for Gaussians. In particular, we note that each component of
∇s(i,x) consists of the product of CDFs and a single PDF.
As computing a good approximation using the Monte Carlo
strategy can be expensive and requires known pull variance σi,
we propose two analytical approximations that only require the
ratio σi/σj to be known. First, noting that events (xij ≥xik) and
(xij ≥xih) are positively correlated for all j,k,h, we have Pr(xij ≥
xik | xij ≥xih) ≥Pr(xij ≥xik), which gives the lower bound:
log Pr(D) =
Õ
i
log
Ö
k,j
Pr

xij ≥xik

Ø
1≤h<k,h,j
(xij ≥xih)

≥
Õ
i
Õ
k,j
log Pr(xij ≥xik).
We can now apply the same argument as in Case 4 and approxi-
mately optimize this lower bound by following its gradient at ˆV = 0.
This leads to same weights as our unnormalized Case 4 rule (i.e.,
weigh votes according to equation 1) for observed comparisons (i.e.,
all pairs involving each voter’s top choice). We call this the Case 5
“lower bound approximation”.
A second approach makes the following simple observation: at
ˆV = 0, a gradient step in the direction of maximizing Pr(xij ≥
0) also increases Pr(xij ≥maxXi,9j). To evaluate the gradient of
log Pr(xij ≥0) at ˆV = 0, one can run through a computation similar
to Case 3, or simply take the limit of the Case 3 weight as one of
the counts goes to ∞. This yields wij = √cij for the top choices aj
each arm has equal observation variance (with wik = 0 for k , j).
We call this the Case 5 “zero approximation”.
Both analytical approximations are a bit crude. The lower bound
approximation ignores significant dependencies, and the zero ap-
proximation doesn’t factor in counts of non-selected alternatives.
In both cases we use the gradient at ˆV = 0, but unlike in Case 3
where this is justified by concavity, there is no similarly strong
justification here. Nevertheless, we will see in our experiments that
both approximations improve over plurality baselines.
□
4.2
Learning an Aggregation Rule
Can we come up with a rule for the many alternative, ordinal
rank case (Case 4) that does not rely on the Naive Independence
assumption? Although we were unable to do so analytically, we
propose to learn an aggregation rule from data. This rule will serve
as a useful baseline for our derived rules, and the approach is flexible,
in that it can be trained on data generated by any noise model (e.g.,
a k-armed bandit with uniform observation noise). As an additional
benefit, the learned rule will output a distribution over outcomes
(our derived rules output point estimates).
We require our learned rule to apply in the case of an arbitrary
number of voters and alternatives. Ideally, our rule should be a
function f : X × C →Rn that is order invariant with respect to
voters, and order equivariant with respect to alternatives (permut-
ing the alternatives permutes the results in the same way). Both
properties were studied by Zaheer et al.’s work on Deep Sets [46],
which investigated the expressiveness of the order invariant sum
decomposition σ(Σih(zi)) and proposed simple neural network
layers to model equivariant functions. An alternative approach to
accommodating variable numbers of voters and alternatives would
be to use recurrent architectures such as LSTMs [24] with respect
to each dimension, but this would be sensitive to their orderings.
We adopt the Deep Set architecture σ(Σih(zi)), where the input
zi of the i-th voter is an n × k matrix, where n is the number of
alternatives and k is the number of features representing each alter-
native’s count and vote information. We use equivariant functions
(in terms of the alternatives) for both the encoder h and decoder

AAMAS’20, May 9–13, 2020, Auckland, New Zealand
Silviu Pitis and Michael R. Zhang
Figure 2: Architecture for our learned aggregation rule,
based on Deep Sets [46]. For each voter i, vote features (votes
and count information) {ai,bi, . . . } are embedded via permu-
tation equivariant h. Embedded votes are aggregated across
voters using permutation invariant ⊕and passed through a
permutation equivariant σ to produce alternative scores.
σ, and take the sum Σi across the voters. The decoder σ termi-
nates in a softmax. This architecture satisfies all desiderata outlined
above and outputs a proper distribution over outcomes. We train
the network to minimize a negative log likelihood (cross entropy)
loss where the targets are ground truth best outcome. Training was
done via gradient descent for up to 5000 mini-batches of size 128,
generated as described in our high variance experiment (Subsection
5.1) with a different random number of voters (sampled uniformly
between 5 and 350) and different number of alternatives (sampled
uniformly between 5 and 15) for each mini-batch. We tested 20 ran-
dom hyperparameter configurations from a search space of 144, and
kept the model with the lowest loss. See the Appendix for further
details, including specific hyperparameters and a full description
of our final architecture.
5
EXPERIMENTS
In this section we compare our derived and learned rules to common
voting rules in settings of varying uncertainty. We find that our rules
consistently outperform anonymous rules, even when there is sig-
nificant count noise. Code to replicate the experiments is available
online at https://github.com/spitis/objective_social_choice.
5.1
Ideal, High Variance Conditions
For this experiment, we generate 100,000 instances of the multi-
armed bandit problem with 10 alternatives for different numbers of
voters (3, 10, 30, 100, and 300). The ground truth mean of each arm
is sampled as µi ∼N(0, 1) and the voter counts cij are sampled
uniformly between 1 to 50. Individual observations are sampled
with high variance from N(µi, 1000). The voters then report an
ordinal ranking based on their estimated means for each alternative.
We compare the performance of our Case 4 and Case 5 rules as
well as our learned voting rule to several baselines: basic plurality
vote and Borda count [4], naively-modified Plurality vote and Borda
count (“Plurality+” and “Borda+”), and a Case 1 oracle. The plurality
baseline sets wij = 1 for voter i’s top choice j, and wik = 0 for
k , j. The Borda baseline sets wij = Í
k,j Ixij >xik . The Plurality+
and Borda+ baselines take the best performing modification of the
basic Plurality and Borda baselines, where the modification uses
the count information in an unjustified but plausible way. The
tested modifications include weighing each voter’s scores by: the
arithmetic mean of that voter’s counts {cij }, the harmonic mean of
the counts, or, in each case, the square root and logarithm thereof.
The Case 1 oracle sees each voter’s cardinal estimate X and acts
as an upper bound on performance. For our Case 5 Monte Carlo
approximation we averaged 100 samples from ϕij, which we found
performed almost as well as 1000 samples and made simulations
cheaper. In all cases, ties are broken by random selection.
Performance, as measured by regret, is shown in Table 1a. Rela-
tive performance in terms of accuracy (not shown) is approximately
the same. The different voting rules are grouped according to ac-
cess to votes and auxiliary information. Our rules consistently
beat anonymous baselines. Among pairwise rules, we observe that
our learned aggregation rule has the best overall non-oracle per-
formance, but note that the two Case 4 rules are quite close in
performance to the learned rule and are significantly cheaper to
compute. The Case 5 results show that the zero approximation is
consistently better than the lower bound, and very close to the
Monte Carlo approximation (which should give near optimal per-
formance). Finally, we note that all pairwise rules outperform all
plurality rules (including Case 5). This is not surprising, as plurality
rules use less information than pairwise rules.
5.2
Ideal, Low Variance Conditions
We now consider the same experiment as above under lower obser-
vation variance. Instead of sampling observations from N(µi, 1000),
we sample them from N(µi, 10). The purpose here is two-fold. First,
our learned aggregation rule, which was the best performing rule
in high variance conditions, was trained in those exact conditions,
and we hypothesize its performance will deteriorate out of domain.
Second, we note that the failure mode of the unnormalized Case 4 is
exacerbated by low variance, and hypothesize that the normalized
rule will perform relatively better.
The results, shown in Table 1b, confirm our hypotheses. As com-
pared to the high variance case, both the learned aggregation rule
and the unnormalized version of our Case 4 rule do significantly
worse relative to the Borda baseline. The normalized Case 4 rule
does significantly better than other rules in low variance conditions.
Interestingly, the Case 5 zero approximation does slightly better
in low variance conditions than the Case 5 Monte Carlo approx-
imation; this suggests that accurate Monte Carlo approximation
requires more samples under low observation variance and that
the zero approximation is near optimal.
5.3
Noisy Count Conditions
We now relax our assumption of perfect count information by
introducing significant noise into the counts cij that are observed
by our rules. This impacts all rules except the anonymous baselines
(plurality and Borda). We experiment with two types of count noise:
percentage noise applied to all counts, and resampled counts. In the
percentage noise case, we adjust all reported counts by a percentage
between −50% and +50% (sampled independently and uniformly),
rounding to the nearest integer. In the resampled counts case, we
replace one third of the reported counts with resampled values
(i.e., an integer between 1 and 50). Otherwise, we follow the same
procedure as before. To get an idea of how well we could do if the
noise were to be expected, we retrain our learned aggregation rule
on data generated according to the percentage noise case (but not

Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes AAMAS’20, May 9–13, 2020, Auckland, New Zealand
Num voters
3
10
30
100
300
Case 1 Oracle
1.1642
0.8625
0.5356
0.2390
0.0936
Borda
1.2116
0.9689
0.6629
0.3308
0.1385
Borda+
1.1863
0.9493
0.6555
0.3270
0.1369
Case 4
1.1760
0.9194
0.6069
0.2890
0.1177
Case 4 (normalized)
1.1879
0.9231
0.6058
0.2886
0.1173
Learned
1.1687
0.9086
0.5935
0.2788
0.1125
Plurality
1.3509
1.2116
1.0089
0.6905
0.3807
Plurality+
1.3232
1.1904
0.9888
0.6721
0.3680
Case 5 (lower bound)
1.2903
1.1547
0.9434
0.6224
0.3302
Case 5 (zero approx)
1.2847
1.1458
0.9297
0.6074
0.3193
Case 5 (Monte Carlo)
1.2848
1.1413
0.9278
0.6066
0.3178
(a) High Variance
Num voters
3
10
30
100
300
Case 1 Oracle
0.1075
0.0312
0.0102
0.0030
0.0011
Borda
0.1754
0.0631
0.0217
0.0068
0.0023
Borda+
0.1688
0.0590
0.0205
0.0062
0.0021
Case 4
0.1711
0.0603
0.0208
0.0064
0.0022
Case 4 (normalized)
0.1479
0.0487
0.0163
0.0050
0.0017
Learned
0.1767
0.0605
0.0211
0.0065
0.0022
Plurality
0.3147
0.0898
0.0290
0.0086
0.0029
Plurality+
0.2586
0.0839
0.0285
0.0086
0.0029
Case 5 (lower bound)
0.2112
0.0740
0.0253
0.0078
0.0026
Case 5 (zero approx)
0.2071
0.0726
0.0253
0.0077
0.0026
Case 5 (Monte Carlo)
0.2089
0.0739
0.0256
0.0078
0.0026
(b) Low Variance
Table 1: Average regret, V (a∗) −V (ˆa), in ideal conditions. Lower is better. Best non-Oracle rules of each type in bold.
3
10
30
100
300
Case 1 Oracle
1.1726
0.8811
0.5579
0.2539
0.1002
Learned (noisy)
1.1728
0.9131
0.6012
0.2861
0.1157
Borda
1.2116
0.9689
0.6629
0.3308
0.1385
Borda+
1.2100
0.9643
0.6583
0.3281
0.1369
Case 4
1.1780
0.9194
0.6090
0.2918
0.1184
Case 4 (normalized)
1.1902
0.9234
0.6090
0.2912
0.1188
Learned
1.1793
0.9262
0.6161
0.2964
0.1210
Plurality
1.3509
1.2116
1.0089
0.6905
0.3807
Plurality+
1.3248
1.1889
0.9902
0.6740
0.3681
Case 5 (lower bound)
1.2931
1.1557
0.9466
0.6260
0.3312
Case 5 (zero approx)
1.2874
1.1485
0.9369
0.6160
0.3241
Case 5 (Monte Carlo)
1.2959
1.1548
0.9409
0.6167
0.3243
(a) 50% count noise
3
10
30
100
300
Case 1 Oracle
1.2194
0.9883
0.6901
0.3492
0.1470
Learned (noisy)
1.1940
0.9433
0.6321
0.3080
0.1267
Borda
1.2116
0.9689
0.6629
0.3308
0.1385
Borda+
1.2100
0.9654
0.6589
0.3298
0.1374
Case 4
1.1926
0.9423
0.6319
0.3079
0.1270
Case 4 (normalized)
1.1996
0.9433
0.6323
0.3080
0.1269
Learned
1.1972
0.9481
0.6390
0.3131
0.1291
Plurality
1.3509
1.2116
1.0089
0.6905
0.3807
Plurality+
1.3331
1.1973
0.9971
0.6834
0.3751
Case 5 (lower bound)
1.3294
1.2165
1.0402
0.7405
0.4255
Case 5 (zero approx)
1.3113
1.1747
0.9701
0.6524
0.3504
Case 5 (Monte Carlo)
1.3067
1.1758
0.9780
0.6592
0.3548
(b) 33% count replacement
Table 2: Average regret, V (a∗) −V (ˆa), in noisy conditions. Lower is better. Best non-Oracle rules of each type in bold.
the resampled counts case). The experiments for this subsection
utilize high observation variance (N(µi, 1000)). The results are
shown in Tables 2a and 2b.
In case of 50% count noise, it is unsurprising that the neural
network trained under those conditions does best. What is per-
haps surprising is how robust the derived rules are to count noise.
Both Case 4 and Case 5 rules beat their respective baseline by a
respectable margin, even with inaccurate counts. The same trend
continues in the case of 33% count replacement, where our Case 4
rules outperform the Oracle (which is no longer a true Oracle). We
note, however, that performance declines more sharply in the count
replacement case, which is to be expected since the per-count noise
is biased. It is interesting to note that the Case 5 zero approxima-
tion is more robust to noise than the Monte Carlo approximation.
Overall, the results indicate that even inaccurate count information
can have significant value.
6
CONCLUSION
In this paper, we proposed a generic framework for objective social
choice, which seeks to estimate a cardinal ground truth given noisy
votes. We considered a bandit-based noise model and proposed sev-
eral voting rules that utilize auxiliary count information to improve
inference relative to anonymous rules. Our empirical results con-
firm the efficacy of our rules relative to anonymous baselines and
demonstrate robustness under noise in the auxiliary information.
The scope of the present work assumes that voters have inde-
pendent information and is limited to a particular noise model and
mode of auxiliary information (experience counts). It would be
interesting to extend our objective social analysis to cases of depen-
dent information, more general noise models (e.g., noise generated
by a contextual bandit [29]), and other forms of auxiliary informa-
tion (e.g., a similarity kernel between voters). Another extension
might study group composition [25]: if we have some control over
voter experience, how should we influence the group of voters to
improve voting outcomes? We leave these angles to future work.
ACKNOWLEDGMENTS
We thank Nisarg Shah for his guidance throughout this project. We
also thank Jimmy Ba, Harris Chan, Mufan Li and the anonymous
referees for their helpful comments.

AAMAS’20, May 9–13, 2020, Auckland, New Zealand
Silviu Pitis and Michael R. Zhang
REFERENCES
[1] Kenneth J Arrow. 2012. Social choice and individual values. Vol. 12. Yale university
press.
[2] Kenneth J Arrow, Amartya Sen, and Kotaro Suzumura. 2010. Handbook of social
choice and welfare. Vol. 2. Elsevier.
[3] John M Bates and Clive WJ Granger. 1969. The combination of forecasts. Journal
of the Operational Research Society 20, 4 (1969).
[4] Felix Brandt, Vincent Conitzer, Ulle Endriss, Jérôme Lang, and Ariel D Procaccia.
2016. Handbook of computational social choice. Cambridge University Press.
[5] Ioannis Caragiannis and Evi Micha. 2017. Learning a Ground Truth Ranking
Using Noisy Approval Votes.. In IJCAI. 149–155.
[6] Ioannis Caragiannis, Ariel D Procaccia, and Nisarg Shah. 2016. When do noisy
votes reveal the truth? ACM Transactions on Economics and Computation (TEAC)
4, 3 (2016), 15.
[7] Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. 2017. UCB
exploration via Q-ensembles. arXiv preprint arXiv:1706.01502 (2017).
[8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
Amodei. 2017. Deep reinforcement learning from human preferences. In Advances
in Neural Information Processing Systems. 4299–4307.
[9] Robert T Clemen. 1989. Combining forecasts: A review and annotated bibliogra-
phy. International journal of forecasting 5, 4 (1989).
[10] Marie J Condorcet. 1785. Essai sur l’application de l’analyse à la probabilité des
décisions rendues à la pluralité des voix. de l’Imprimerie Royale.
[11] Vincent Conitzer. 2013. The maximum likelihood approach to voting on social
networks. In 2013 51st Annual Allerton Conference on Communication, Control,
and Computing (Allerton). IEEE, 1482–1487.
[12] Vincent Conitzer and Tuomas Sandholm. 2005. Common Voting Rules As Maxi-
mum Likelihood Estimators. (2005), 8. http://dl.acm.org/citation.cfm?id=3020336.
3020354
[13] JP Dickinson. 1973. Some statistical results in the combination of forecasts.
Journal of the Operational Research Society 24, 2 (1973).
[14] JP Dickinson. 1975. Some comments on the combination of forecasts. Journal of
the Operational Research Society 26, 1 (1975).
[15] Thomas G Dietterich. 2000. Ensemble methods in machine learning. In Interna-
tional workshop on multiple classifier systems. Springer.
[16] JL Fleiss. 1993. Review papers: The statistical basis of meta-analysis. Statistical
methods in medical research 2, 2 (1993).
[17] Christian Genest and Kevin J McConway. 1990. Allocating the weights in the
linear opinion pool. Journal of Forecasting 9, 1 (1990).
[18] Christian Genest, James V Zidek, et al. 1986. Combining probability distributions:
A critique and an annotated bibliography. Statist. Sci. 1, 1 (1986).
[19] Paul Gompers, Joy Ishii, and Andrew Metrick. 2003. Corporate governance and
equity prices. The quarterly journal of economics 118, 1 (2003), 107–156.
[20] Clive WJ Granger. 1989. Invited review combining forecastsâĂŤtwenty years
later. Journal of Forecasting 8, 3 (1989).
[21] Clive WJ Granger and Ramu Ramanathan. 1984. Improved methods of combining
forecasts. Journal of forecasting 3, 2 (1984).
[22] John C Harsanyi. 1955. Cardinal welfare, individualistic ethics, and interpersonal
comparisons of utility. Journal of political economy 63, 4 (1955).
[23] Joshua E Hill. [n. d.]. The minimum of n independent normal distributions. ([n.
d.]).
[24] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[25] Lu Hong and Scott E Page. 2004. Groups of diverse problem solvers can outper-
form groups of high-ability problem solvers. Proceedings of the National Academy
of Sciences 101, 46 (2004), 16385–16389.
[26] Robert A Jacobs. 1995. Methods for combining experts’ probability assessments.
Neural computation 7, 5 (1995).
[27] Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Se-
bastian Kohlmeier, Eduard Hovy, and Roy Schwartz. 2018. A Dataset of Peer
Reviews (PeerRead): Collection, Insights and NLP Applications. In Proceedings
of the 2018 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long Papers).
Association for Computational Linguistics, New Orleans, Louisiana, 1647–1661.
https://doi.org/10.18653/v1/N18-1149
[28] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[29] Tor Lattimore and Csaba Szepesvári. 2018. Bandit algorithms. preprint (2018).
[30] David D Lewis. 1998. Naive (Bayes) at forty: The independence assumption in
information retrieval. In European conference on machine learning. Springer.
[31] Colin L Mallows. 1957. Non-null ranking models. I. Biometrika 44, 1/2 (1957).
[32] Kenneth O May. 1952. A set of independent necessary and sufficient conditions
for simple majority decision. Econometrica: Journal of the Econometric Society
(1952), 680–684.
[33] Kevin J McConway. 1981. Marginalization and linear opinion pools. J. Amer.
Statist. Assoc. 76, 374 (1981), 410–414.
[34] Michael P Perrone and Leon N Cooper. 1992. When networks disagree: Ensemble
methods for hybrid neural networks. (1992).
[35] Ariel D Procaccia and Jeffrey S Rosenschein. 2006. The distortion of cardinal
preferences in voting. In International Workshop on Cooperative Information
Agents. Springer.
[36] Ariel D Procaccia, Nisarg Shah, and Eric Sodomka. 2015. Ranked voting on
social networks. In Twenty-Fourth International Joint Conference on Artificial
Intelligence.
[37] Lior Rokach. 2010. Ensemble-based classifiers. Artificial Intelligence Review 33,
1-2 (2010).
[38] Amartya Sen. 2018. Collective choice and social welfare. Harvard University Press.
[39] Volker Tresp and Michiaki Taniguchi. 1995. Combining estimators using non-
constant weighting functions. In Advances in neural information processing sys-
tems. 419–426.
[40] John Von Neumann and Oskar Morgenstern. 1953. Theory of games and economic
behavior. (1953).
[41] Kenneth F Wallis. 2011. Combining forecasts–forty years later. Applied Financial
Economics 21, 1-2 (2011).
[42] John A Weymark. 1991.
A reconsideration of the Harsanyi–Sen debate on
utilitarianism. Interpersonal comparisons of well-being 255 (1991).
[43] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229–256.
[44] H Peyton Young. 1988. Condorcet’s theory of voting. American Political science
review 82, 4 (1988).
[45] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. The
k-armed dueling bandits problem. J. Comput. System Sci. 78, 5 (2012), 1538–1556.
[46] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan R
Salakhutdinov, and Alexander J Smola. 2017. Deep sets. In Advances in neural
information processing systems. 3391–3401.

Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes AAMAS’20, May 9–13, 2020, Auckland, New Zealand
APPENDICES
A
NOTATION GLOSSARY
A
finite set of n alternatives
a∗
optimal alternative arg maxi {µi }
ˆa
alternative chosen by rule f
B(p)
Bernoulli distribution with parameter p
cij
number of observations of arm j by voter i
¯cj
total observations from arm j, Í
i cij
C
auxiliary information (context) space
δ
ˆµ2 −ˆµ1 in the 2 alternative case
erf
error function
f
social choice rule
д
noise process д
m
number of voters
n
number of alternatives
N(µ,σ2)
normal distribution with mean µ and variance σ2
Pr(D)
probability of the observed data (votes)
Pr(Djk)
probability of data considering only aj and ak
Φ
normal cumulative distribution function
ϕ
normal probability density function
ri
an observation from arm i, ri ∼N(µi,σ2
i )
σj
the variance of arm j
si
r
σ 2
2 ci1+σ 2
1 ci2
ci2ci1
V
space of valid ground truth objective functions
V
ground truth objective function [µ1, µ2, . . . , µn]
wi
weight for voter i’s choice in 2 alternative case
wij
weight given on account of voter i to alternative j
X
observation space (votes of all votes)
Xi
cardinal estimate of V by voter i, [xi1,xi2, . . . ,xin]
xij
mean of voter i’s observations of alternative j
Yi
the binary variable Ixi2−xi1 ≤0
yi
a sample of Yi
Yi,j<k
the binary variable Ixij−xik ≤0
B
DERIVATIONS
B.1
Case 3 Details
Case 3 (2 alternatives, votes are ordinal ranks). There are 2
arms, and each voter i provides an ordinal ranking (a1,a2) indicating
that they value a1 higher than a2 (i.e., xi1 ≥xi2).
As above, we have xi2 −xi1 ∼N(µ2 −µ1, σ 2
2 ci1+σ 2
1 ci2
ci2ci1
). Denot-
ing the CDF of xi2 −xi1 by Φi, and defining the binary variable
Yi = Ixi2−xi1 ≤0, we have Yi ∼B(Φi(0)) (the Bernoulli distribution
parameterized by Φi evaluated at 0). Our votes x ∈X consist of
a set of samples {y1 ∼Yi,y2 ∼Y2, . . .ym ∼Ym}. Since adding a
constant to the underlying means has no effect on the likelihood,
a direct inference about V = (µ1, µ2) is impossible and we instead
seek to estimate the difference µ2 −µ1. Defining, s2
i = σ 2
2 ci1+σ 2
1 ci2
ci2ci1
,
we want to choose ∆≜ˆµ2 −ˆµ1 to maximize the log-probability of
the data:
log Pr(D; ∆) =
Õ
i
log Φi(0)yi (1 −Φi(0))1−yi
=
Õ
i
yi log
h
1
2 + 1
2erf

−∆
si
√
2
i
+ (1 −yi) log
h
1
2 −1
2erf

−∆
si
√
2
i
.
Noting that d
dz erf(z) =
2
√π exp(−z2), we could try to optimize
directly with respect to ∆by setting d
d∆log Pr(D; ∆) = 0, but this
appears intractable:
0 = d
d∆log Pr(D; ∆) =
Õ
i
yi
d
d∆
1
2 erf

−∆
si
√
2

h
1
2 + 1
2 erf

−∆
si
√
2
i −(1 −yi)
d
d∆
1
2 erf

−∆
si
√
2

h
1
2 −1
2 erf

−∆
si
√
2
i
=
Õ
i
yi
−2
si
√
2π exp −∆2
2s2
i
h
1 + erf

−∆
si
√
2
i −(1 −yi)
−2
si
√
2π exp −∆2
2s2
i
h
1 −erf

−∆
si
√
2
i .
However, since log Pr(D; ∆) is concave (proof below), its deriva-
tive evaluated at ∆= 0 points in the direction of the MLE solution
and we can use this fact to find Y corresponding to MLE estimate of
∆by evaluating d
d∆log Pr(D; ∆) at ∆= 0. The intuition behind this
trick is best understood visually—see Figure 3 (left). To evaluate the
sign of d
d∆log Pr(D; ∆)(0), we note that erf(0) = 0 and the result
follows:
d
d∆log Pr(D; ∆)(0) =
Õ
i
yi
−2
si
√
2π
−(1 −yi)
−2
si
√
2π
∝
Õ
i
 −yi
si
+ (1 −yi)
si

=
Õ
i

−yi
r ci2ci1
ci1 + ci2

+ (1 −yi)
r ci2ci1
ci1 + ci2

,
so that:
wi ∝
r ci2ci1
ci1 + ci2
.
All that remains is for us to show that log Pr(D; ∆) is concave.
This ensures that it has a global maximum (possibly at ±∞, if all
votes agree) and that the gradient evaluated at 0 reveals its direction
(see Figure 3 (left)). There are few ways to prove concavity. We do
so by showing that the second derivative of log Pr(D; ∆) is negative
everywhere. To do so, we use the following definitions:
F ≜log Pr(D; ∆),
so that:
F ′ ≜dF
d∆
is a binary weighted sum over:
f +
i
≜дi
h+
i
and
f −
i
≜дi
h−
i
where:
дi ≜
−2
si
√
2π
exp −∆2
2s2
i
,
h+
i ≜1 + erf
 −∆
si
√
2

and
h−
i ≜1 −erf
 −∆
si
√
2

.
We have:
dдi
d∆= −∆
s2
i
дi
dh+
i
d∆= дi
dh−
i
d∆= −дi,
so that, using the quotient rule:
d f +
i
d∆=
−∆
s2
i дih+
i −д2
i
(h+
i )2
and
d f +
i
d∆=
−∆
s2
i дih−
i + д2
i
(h−
i )2
.
Now, f +
i appears in F ′ with weight of either 0 or positive 1, and
f −
i appears in F with weight of either 0 or negative 1. Thus, to show
that F ′′ (the second derivative of F) is negative everywhere, we can

AAMAS’20, May 9–13, 2020, Auckland, New Zealand
Silviu Pitis and Michael R. Zhang
Δ
log(;Δ)(Δ)
dlog(;Δ)
dΔ
log(;Δ)
Figure 3: As log Pr(D; ∆) is concave in ∆(blue curve), its par-
tial derivative evaluated at ∆= 0 (red line) points in the di-
rection of the MLE solution (yellow star).
show that df +
i
d∆is negative for all values of d and si and that df −
i
d∆is
positive for all values ofd and si. The denominator in each is always
positive and can be ignored. The numerator contains an always
negative factor of дi, which can be cancelled if we reverse the sign.
Finally, we can multiply both functions by si (which maintains the
sign, since si is positive)—this allows us to consider the resulting
functions as functions of the single variable x = ∆
si . The proof thus
reduces to showing that both of the following two functions are
positive for all values of x:
−x

1 + erf
 −x
√
2

+
2
√
2π
exp
 −x2
2

,
and
x

1 −erf
 −x
√
2

+
2
√
2π
exp
 −x2
2

.
This can be done visually (by plotting), or analytically, by show-
ing that the derivative of the first (second) function is strictly pos-
itive (negative) and that the functions have limit zero as x →∞
and x →−∞, respectively.
C
DETAILS OF LEARNED AGGREGATION
We adopt the Deep Set architecture:
σ(Σih(zi)),
where the input zi of the i-th voter is an n ×k matrix, where n is the
number of alternatives and k is the number of features representing
each alternative’s count and vote information. To encode count and
vote information we use a single real-valued feature for each, so
that k = 2, and the jth alternative for the ith voter has features zijc
and zijv. For counts, we normalize count values cij to be in [0, 1]
by dividing by the maximum count value used in our experiments,
so that the count feature for voter i’s alternative j is zijc = cij/50.
For votes, we linearly interpolate between 0 and 1, so that voter
i’s top ranked alternative j has feature zijv = 1, and the bottom
ranked alternative k has feature zikv = 0.
We use the same parametric form of equivariant function for both
the encoder h and decoder σ, which is the same form proposed and
used by [46]. Letting Γθ be a 1×1 convolutional layer parameterized
by θ, each equivariant layer is computed as Γθ (x −λ(x)), where λ
is an order invariant function computed feature-wise across the
input. The aggregation operation Σi is taken across the voters, and
the decoder σ terminates in a softmax.
We train the network to minimize a negative log likelihood (cross
entropy) loss where the targets are the ground truth outcomes.
Training was done via gradient descent, using the Adam optimizer
[28], for up to 5000 mini-batches of size 128, generated as described
in our high variance experiment (Subsection 5.1) with a different
random number of voters (sampled uniformly between 5 and 350)
and different number of alternatives (sampled uniformly between
5 and 15) for each mini-batch.
To settle on a particular network configuration, we tested 20
random hyperparameter configurations from a search space of 144,
and kept the model with the lowest loss. The search space consisted
of the product of:
• learning_rate ∈{3e-3, 1e-3, 3e-4∗, 1e-4}
• num_encoder_layers ∈{2, 3, 4∗}
• num_decoder_layers ∈{0, 1, 2∗}
• λ ∈{max∗, mean}
• Σ ∈{mean, sum∗}
where the configuration with the lowest final loss (used in our
experiments) is marked∗. This same configuration was used to
train the “noisy” network for Subsection 5.3. We note that the
next best configuration (with 3 encoder layers and 1 decoder layer)
achieved very similar performance (1.018 versus 0.998 loss).

