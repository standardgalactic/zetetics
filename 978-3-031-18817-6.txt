Graph Learning for 
Fashion Compatibility 
Modeling
Synthesis Lectures on 
Information Concepts, Retrieval, and Services
Second Edition
Weili Guan · Xuemeng Song ·
Xiaojun Chang · Liqiang Nie

Synthesis Lectures on Information
Concepts, Retrieval, and Services
Series Editor
Gary Marchionini, School of Information and Library Science, The University of North
Carolina at Chapel Hill, Chapel Hill, NC, USA

This series publishes short books on topics pertaining to information science and applica-
tions of technology to information discovery, production, distribution, and management.
Potential topics include: data models, indexing theory and algorithms, classiﬁcation,
information architecture, information economics, privacy and identity, scholarly com-
munication, bibliometrics and webometrics, personal information management, human
information behavior, digital libraries, archives and preservation, cultural informatics,
information retrieval evaluation, data fusion, relevance feedback, recommendation sys-
tems, question answering, natural language processing for retrieval, text summarization,
multimedia retrieval, multilingual retrieval, and exploratory search.

Weili Guan · Xuemeng Song · Xiaojun Chang ·
Liqiang Nie
Graph Learning for Fashion
Compatibility Modeling
Second Edition

Weili Guan
Monash University
Melbourne, VIC, Australia
Xiaojun Chang
University of Technology Sydney
Sydney, NSW, Australia
Xuemeng Song
Shandong University
Qingdao, China
Liqiang Nie
Harbin Institute of Technology (Shenzhen)
Shenzhen, China
ISSN 1947-945X
ISSN 1947-9468 (electronic)
Synthesis Lectures on Information Concepts, Retrieval, and Services
ISBN 978-3-031-18816-9
ISBN 978-3-031-18817-6 (eBook)
https://doi.org/10.1007/978-3-031-18817-6
1st edition: © Morgan & Claypool Publishers 2020
2nd edition: © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Switzerland AG 2022
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the whole
or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage
and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or
hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does
not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give
a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that
may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and
institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Along with economic development, numerous fashion products have sprung up in virtual
and physical shops, such as bags, scarves, shoes, and skirts. Undoubtedly, they do beautify
our lives. Nevertheless, they also create many troubles, especially for those who lack a
sense of beauty. Moreover, people are easily overwhelmed by the abundant fashion items,
thus it is difﬁcult to ﬁnd the desired fashion piece to make a compatible outﬁt with
their wardrobes. To alleviate such a problem, the fashion compatibility modeling task,
which aims to estimate the matching degree of the given set of complementary fashion
items, emerged. In a sense, previous studies focused on evaluating the compatibility of
an outﬁt with only two items, such as a top and a bottom, whereas each outﬁt in practice
may be composed of a variable number of items. For instance, either <a top, a bottom,
a pair of shoes, and a bag> or <a dress, a pair of shoes, and a hat> can compose an
outﬁt. Accordingly, recent studies have shifted to investigating the compatibility of an
outﬁt that involves multiple items rather than only two items. With the advances in graph
learning in dealing with unstructured data, it has become a natural choice for developing
the outﬁt compatibility modeling scheme. In fact, a few pioneer research efforts have
adopted graph neural networks as the model backbone to fulﬁll the outﬁt compatibility
modeling. Despite their great progress, there are still several difﬁcult challenges that have
not been well addressed as follows.
1. Multiple Correlated Modalities. How to effectively model the correlations among dif-
ferent modalities and fully exploit the multiple modalities of fashion items with graph
learning poses a key challenge.
2. Complicated Hidden Factors. Existing methods focus on coarse-grained representation
learning of composing items to derive outﬁt compatibility. Therefore, how to uncover
the hidden factors that inﬂuence the outﬁt’s overall compatibility and achieve ﬁne-
grained compatibility modeling with graph learning is a difﬁcult challenge.
3. Nonuniﬁed Semantic Attributes. How to fully utilize the nonuniﬁed attribute labels of
fashion items as the partial supervision for ﬁne-grained compatibility modeling is also
a considerable challenge.
v

vi
Preface
4. Users’ Personal Preferences. The judge of outﬁt compatibility is subjective. Accord-
ingly, how to achieve personalized outﬁt compatibility modeling with graph learning
is another challenge we are facing.
Noticing this timely opportunity, in this book, we present some state-of-the-art graph
learning theories for outﬁt compatibility modeling, to address the aforementioned chal-
lenges. In particular, we ﬁrst present a correlation-oriented graph learning method for
outﬁt compatibility modeling, to model the correlations between visual and textual modal-
ities of items. We next introduce the modality-oriented graph learning scheme for outﬁt
compatibility modeling, whereby both the intramodal and intermodal compatibilities
between fashion items are jointly explored. To uncover the hidden factors affecting out-
ﬁt compatibility, we then devise an unsupervised disentangled graph learning method,
where the ﬁne-grained compatibility is captured. Since this unsupervised method ignores
the potential of item’s attribute labels in guiding ﬁne-grained compatibility modeling, we
further develop a partially supervised disentangled graph learning method. To incorporate
the user’s personal tastes, we propose a metapath-guided heterogeneous graph learning
scheme for personalized outﬁt compatibility modeling. Finally, we conclude the book and
identify the future research directions.
Overall, this book presents the latest advances on the topic of outﬁt compatibility
modeling, where both the general and personalized scenarios are discussed. We expect
that this book can evoke more researchers to work in this exciting and challenging area.
It is suitable for students, researchers, and practitioners who are interested in the fashion
compatibility modeling task.
Melbourne, Australia
Qingdao, China
Sydney, Australia
Shenzhen, China
Weili Guan
Xuemeng Song
Xiaojun Chang
Liqiang Nie

Acknowledgements
This book would not have been completed, or at least not be what it looks like now,
without the support of many colleagues, especially those from the GORSE Lab at Monash
University and the iLearn Center at Shandong University. It is my pleasure to take this
opportunity to express my appreciation for their contributions to this time-consuming
book project.
First, our sincere thanks undoubtedly go to our colleagues who contributed signiﬁcantly
to some chapters of this book: Dr. Chung-Hsing Yeh, Dr. Yuanfang Li, Dr. Guanliang
Chen from Monash University. Mr. Tianyu Su, Mr. Haokun Wen, Mr. Fangkai Jiao, Miss.
Shiting Fang, and Miss. Xiaolin Chen from Shandong University. Thanks for their par-
ticipation in the technical discussion of this book and their constructive feedback and
comments that signiﬁcantly beneﬁt the shaping of this book.
Second, we would like to express our heartfelt gratitude to Dr. Julie Holden from
Monash University, who spared no effort to polish our earlier drafts.
Third, we are very grateful to the anonymous reviewers, who read the book very care-
fully and gave us many insightful and constructive suggestions. Their assistance also
largely improved the quality of this book.
Fourth, we sincerely extend our thanks to Morgan & Claypool publisher, especially the
editor, Dr. Gary Marchionini, and the executive editor, Ms. Diane Cerra for their valuable
suggestions on this book. They also helped to make publishing the book smooth and
enjoyable.
Finally, our thanks would be reserved for our beloved families for their selﬂess
consideration, endless love, and unconditional support.
June 2022
Weili Guan
Xuemeng Song
Xiaojun Chang
Liqiang Nie
vii

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Our Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.4
Book Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2
Correlation-Oriented Graph Learning for OCM . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.3
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.3.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.3.2
Multimodal Outﬁt Compatibility Modeling . . . . . . . . . . . . . . . . . . . .
11
2.4
Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.4.1
Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.4.2
Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.4.3
Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.4.4
Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3
Modality-Oriented Graph Learning for OCM . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.3
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.3.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.3.2
Multimodal Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.3.3
Modality-Oriented Graph Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3.3.4
Outﬁt Compatibility Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
3.4
Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.4.1
Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
ix

x
Contents
3.4.2
Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.4.3
Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.4.4
Modality Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.4.5
Hyperparameter Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4
Unsupervised Disentangled Graph Learning for OCM . . . . . . . . . . . . . . . . . . .
49
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
4.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
4.3
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
4.3.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
4.3.2
Context-Aware Outﬁt Representation Learning . . . . . . . . . . . . . . . . .
52
4.3.3
Hidden Complementary Factors Learning . . . . . . . . . . . . . . . . . . . . .
55
4.3.4
Outﬁt Compatibility Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.4
Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.4.1
Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.4.2
Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4.4.3
Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
4.4.4
Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
5
Supervised Disentangled Graph Learning for OCM . . . . . . . . . . . . . . . . . . . . .
67
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
5.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
5.3
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
5.3.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
5.3.2
Partially Supervised Compatibility Modeling . . . . . . . . . . . . . . . . . .
70
5.4
Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
5.4.1
Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
5.4.2
Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
5.4.3
Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5.4.4
Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
6
Heterogeneous Graph Learning for Personalized OCM . . . . . . . . . . . . . . . . . .
89
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
6.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
6.3
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
6.3.1
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
6.3.2
Metapath-Guided Personalized Compatibility Modeling . . . . . . . . .
94

Contents
xi
6.4
Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
6.4.1
Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
6.4.2
Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
6.4.3
Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
6.4.4
Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
6.4.5
Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
6.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
7
Research Frontiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
7.1
Efﬁcient Fashion Compatibility Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
7.2
Unbiased Fashion Compatibility Modeling
. . . . . . . . . . . . . . . . . . . . . . . . . .
111
7.3
Try-On Enhanced Fashion Compatibility Modeling . . . . . . . . . . . . . . . . . . .
111
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112

About the Authors
Weili Guan received her Master’s degree from National University of Singapore. After
that, she joined Hewlett Packard Enterprise in Singapore as a Software Engineer and
worked there for around 5 years. She is currently a Ph.D. student at the Faculty of
Information Technology, Monash University (Clayton Campus), Australia. Her research
interests are multimedia computing and information retrieval. She has published more
than 20 papers at ﬁrst-tier conferences and journals, like ACM MM, SIGIR, and IEEE
TIP.
Xuemeng Song is currently an Associate Professor at Shandong University, China, and
IEEE senior member. She received her B.E. degree from the University of Science and
Technology of China, in 2012, and her Ph.D. degree from the National University of
Singapore, in 2016. She has published more than 50 papers in the top venues (e.g., IEEE
TIP, IEEE TMM, ACM SIGIR, ACM MM, and ACM TOIS) and 3 books. Her research
interests include information retrieval and multimedia analysis. She is an editorial board
member of Information Processing & Management. She is also the program committee
member of several top conferences (e.g., ACM SIGIR and MM), and reviewer for top
journals (e.g., IEEE TMM, IEEE TIP, and IEEE TKDE). She won the AI 2000 Most
Inﬂuential Scholar Award Honorable Mention (in the ﬁeld of Multimedia) by AMiner in
2022.
Xiaojun Chang is a Professor at Australian Artiﬁcial Intelligence Institute, Faculty of
Engineering and Information Technology, University of Technology Sydney. He is the
Director of The ReLER Lab. He is also an Honorary Professor in the School of Comput-
ing Technologies, RMIT University, Australia. Before joining UTS, he was an Associate
Professor at School of Computing Technologies, RMIT University, Australia. After grad-
uation, he subsequently worked as a Postdoc Research Fellow at School of Computer
Science, Carnegie Mellon University, Lecturer and Senior Lecturer in the Faculty of
Information Technology, Monash University, Australia. He has focused his research on
exploring multiple signals (visual, acoustic, and textual) for automatic content analysis in
unconstrained or surveillance videos. His team has won multiple prizes from international
xiii

xiv
About the Authors
grand challenges which hosted competitive teams from MIT, University of Maryland,
Facebook AI Research (FAIR), and Baidu VIS, and aim to advance visual understanding
using deep learning. For example, he won the ﬁrst place in the TrecVID 2019—Activity
Extended Video (ActEV) challenge, which was held by National Institute of Standards
and Technology, US.
Liqiang Nie is currently the Dean of the Department of Computer Science and Technol-
ogy, Harbin Institute of Technology (Shenzhen). He received his B.Eng. and Ph.D. degrees
from Xi’an Jiaotong University and National University of Singapore (NUS), respectively.
His research interests lie primarily in multimedia computing and information retrieval. Dr.
Nie has co-authored more than 100 papers and 4 books, and received more than 15,000
Google Scholar citations. He is an AE of IEEE TKDE, IEEE TMM, IEEE TCSVT, ACM
ToMM, and Information Science. Meanwhile, he is the regular area chair of ACM MM,
NeurIPS, IJCAI, and AAAI. He is a member of the ICME steering committee. He has
received many awards, like ACM MM and SIGIR best paper honorable mention in 2019,
SIGMM rising star in 2020, TR35 China 2020, DAMO Academy Young Fellow in 2020,
and SIGIR best student paper in 2021.

1
Introduction
1.1
Background
In modern society, clothing plays an increasingly important role in people’s social lives, as
a compatible outﬁt can largely improve one’s appearance. Nevertheless, not all people grow
a keen sense of aesthetics, and hence often ﬁnd it difﬁcult to assemble compatible outﬁts.
Therefore, it is highly desired to develop automatic fashion compatibility modeling methods
to help people to evaluate the compatibility of a given outﬁt, i.e., a set of complementary
items.
Figure1.1 shows several outﬁt composition examples shared online by fashion experts.
As can be seen, each outﬁt is composed of a variable number of items. The ﬁrst outﬁt involves
four complementary items, while the second outﬁt involves six pieces. Moreover, items in
fashion-oriented communities or e-commerce platforms are usually associated with multiple
modalities, including the visual images, textual descriptions, and semantic attributes (e.g.,
color and material). The rich multimodal data of the tremendous volume of outﬁt composi-
tions have promoted many researchers’ investigations in outﬁt compatibility modeling.
Existing outﬁt compatibility modeling methods can be classiﬁed into three groups: pair-
wise, listwise, and graphwise modeling. The pairwise modeling [1, 2] justiﬁes the compat-
ibility between two items, lacking a global view of the outﬁt that involves multiple items.
The listwise method conceives the outﬁt as a list of items in a predeﬁned order and evaluates
the outﬁt compatibility with neural networks, such as bidirectional long short-term memory
(Bi-LSTM) [3, 4]. Apparently, one limitation of this branch is that there is no explicit order
among composing outﬁt items. Graphwise modeling organizes the outﬁt as an item graph
and employs a graph learning technique to fulﬁll the fashion compatibility modeling task.
Recently, graph learning has become the mainstream technique for addressing the fashion
compatibility modeling task.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
W. Guan et al., Graph Learning for Fashion Compatibility Modeling,
Synthesis Lectures on Information Concepts, Retrieval, and Services,
https://doi.org/10.1007/978-3-031-18817-6_1
1

2
1
Introduction
Fig.1.1 Three examples of outﬁt compositions shared on fashion-oriented websites
1.2
Challenges
In fact, performing the outﬁt compatibility modeling based on graph learning is nontrivial
due to the following difﬁcult challenges.
The ﬁrst challenge is the multiple correlated modalities. As mentioned, online fashion
items are associated with multiple modalities, such as images, textual descriptions, and
semantic attribute labels. Intuitively, as the multiple modalities actually (e.g., visual and
textual modalities) serve to characterize the same item, there should be a certain latent
consistency shared by different modalities. Beyond that, each modality may express some
unique aspects of the given items, and the multiple modalities hence supplement each other.
Therefore, how to explicitly model the consistent and complementary correlations among
different modalities and fully utilize the multiple fashion item modalities via graph learning
poses the ﬁrst challenge.
The second challenge is that there are multiple hidden factors affecting the outﬁt com-
patibility evaluation. Existing methods estimate the overall outﬁt compatibility by learning
the coarse-grained representation of composing items. Actually, outﬁt compatibility is inﬂu-
enced by multiple latent factors, like color, texture, and style. In other words, to judge the
compatibility of an outﬁt, we need to consider multiple factors. Therefore, how to explore
the hidden factors and capture the latent ﬁne-grained compatibility with graph learning to
promote the model performance is a difﬁcult challenge.
The third challenge is nonuniﬁed semantic fashion item attributes. According to our
observation, the item-attribute labels usually convey rich information, characterizing the
key parts of the items. We thus argue that the item-attribute labels should be considered to
supervise the hidden factors learning and jointly improve the model performance and inter-
pretability. Nevertheless, the fashion item-attribute labels are not uniﬁed or aligned, i.e.,
different items may have different attribute labels. Thereby, how to fully employ the nonuni-
ﬁed item-attribute labels to supervise the graph learning-based ﬁne-grained compatibility
modeling is a considerable challenge.

1.3
Our Solutions
3
The last challenge is users’ personal preferences. Aesthetics are subjective. In other
words, people have their preferences in making their personal ideal outﬁts, which may be
caused by their diverse growing circumstances or educational backgrounds. For instance,
given the same pink shirt, women who prefer a classic style prefer to match the shirt with a
homochromatic skirt and high-heeled shoes, whereas women who prefer a sporty style like
to coordinate the shirt with casual jeans and white sneakers. Accordingly, how to achieve
the graph-based personalized outﬁt compatibility modeling is another challenge we face.
1.3
Our Solutions
To address the aforementioned research challenges, we present several state-of-the-art graph
learning theories for outﬁt compatibility modeling.
To model the correlation among different modalities, we introduce a correlation-oriented
graph learning method. It ﬁrst nonlinearly projects each modality (visual image and textual
description) into separable consistent and complementary spaces via multilayer percep-
tron and then models the consistent and complementary correlations between two modal-
ities by parallel and orthogonal regularizations. Thereafter, we strengthen the visual and
textual item representations with complementary information, and further induct both the
text-oriented and vision-oriented outﬁt compatibility modeling with graph convolutional
networks (GCNs). We ultimately employ the mutual learning strategy to reinforce the ﬁnal
compatibility modeling performance.
Toward comprehensive compatibility evaluation, we present a modality-oriented graph
learning scheme, which simultaneously takes the visual, textual, and category modalities of
fashion items as input. Notably, distinguished from existing work, we deem the fashion item
category information as a speciﬁc modality, similar to the visual and textual modalities,
and can be directly used for evaluating outﬁt compatibility. Additionally, the proposed
graph learning-based scheme jointly considers the intramodal and intermodal compatibilities
among fashion items during the information propagation, where the former refers to the
compatibility relation between the same fashion item modalities in an outﬁt, while the latter
denotes that between different modalities.
Beyond the aforementioned two methods that only exploit the overall coarse-grained
compatibility and the item representation, we then devise an unsupervised disentangled
graph learning method, which can capture the hidden factors that affect the outﬁt compat-
ibility and learn the overall outﬁt representation rather than the item representations. In
particular, this method introduces multiple parallel compatibility modeling networks, each
of which corresponds to a factor-oriented context-aware outﬁt representation modeling. It
is worth noting that in each network, we propose to adaptively learn the overall outﬁt rep-
resentation based on GCNs equipped with the multihead attention mechanism, as different
items contribute differently to the outﬁt compatibility evaluation.

4
1
Introduction
To fully utilize the items’ attribute labels (e.g., color and pattern), we develop a partially
superviseddisentangledgraphlearningmethod,wheretheirregularattributelabelsoffashion
items are utilized to guide the ﬁne-grained compatibility modeling. In particular, we ﬁrst
devise a partially supervised attribute-level embedding learning component to disentangle
the ﬁne-grained attribute embeddings from the entire visual feature of each image. We
then introduce a disentangled completeness regularizer to prevent information loss during
disentanglement. Thereafter, we design a hierarchical GCN, which seamlessly integrates
the attribute- and item-level compatibility modeling, to enhance the outﬁt compatibility
modeling.
To incorporate the user’s personal tastes and make our compatibility modeling more
practical, we propose a personalized compatibility modeling scheme. In particular, we cre-
atively build a heterogeneous graph to unify the three types of entities (i.e., users, items,
and attributes) and their relations (i.e., user-item interactions, item-item matching rela-
tions, and item-attribute association relations). We also deﬁne the user-oriented and item-
oriented metapaths and propose performing metapath-guided heterogeneous graph learning
to enhance the user and item embeddings. Moreover, we introduce contrastive regularization
to improve the model performance.
1.4
Book Structure
The remainder of this book consists of six chapters. Chapter2 details the proposed
correlation-oriented graph learning method, which jointly models the consistent and com-
plementary relations between the visual and textual modalities of fashion items. In Chap.3,
we introduce the modality-oriented graph learning method for outﬁt compatibility modeling,
whereby all the visual, textual and category modalities are considered and the intramodal
and intermodal compatibilities between fashion items are simultaneously investigated. In
Chap.4, we present the unsupervised disentangled graph learning method, where the ﬁne-
grained compatibility is explored. In Chap.5, we then develop a partially supervised dis-
entangled graph learning to boost the model performance and interpretability. In Chap.6,
we introduce the personalized outﬁt compatibility modeling scheme to deal with the user’s
personal preference for fashion items. Ultimately, we conclude this book and identify the
future research directions in Chap.7.
References
1. Han, Xianjing, Xuemeng Song, Jianhua Yin, Yinglong Wang, and Liqiang Nie. 2019. Prototype-
guided Attribute-wise Interpretable Scheme for Clothing Matching. In Proceedings of the Interna-
tional ACM SIGIR Conference on Research and Development in Information Retrieval, 785–794.
ACM.
2. Liu, Jinhuan, Xuemeng Song, Liqiang Nie, Tian Gan, and Jun Ma. 2020. An End-to-End Attention-
Based Neural Model for Complementary Clothing Matching. ACM Transactions on Multimedia

References
5
Computing, Communications and Applications 15 (4): 114:1–114:16.
3. Han, Xintong, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis. 2017. Learning Fashion Com-
patibility with Bidirectional LSTMs. In Proceedings of the ACM International Conference on
Multimedia, 1078–1086. ACM.
4. Dong, Xue, Jianlong Wu, Xuemeng Song, Hongjun Dai, and Liqiang Nie. 2020. Fashion Compati-
bilityModelingthroughaMulti-modalTry-on-guidedScheme.In ProceedingsoftheInternational
ACM SIGIR Conference on Research and Development in Information Retrieval, 771–780. ACM.

2
Correlation-Oriented Graph Learning for OCM
2.1
Introduction
In this chapter, we study how to automatically evaluate the compatibility of a given outﬁt
that involves a variable number of items with the graph learning technique. Existing graph
learning-based methods focus on exploring the visual modality of fashion items, and seldom
investigate an item’s textual aspect, i.e., the textual description. In fact, textual descriptions
of fashion items usually contain key features, which beneﬁt item representation learning.
Notably, although some studies have attempted to incorporate the textual modality, they
simply adopt early/late fusion or consistency regularization to boost performance. Never-
theless, the correlations among multimodalities are complex and sophisticated and are not
yet clearly separated and explicitly modeled.
However, outﬁt compatibility modeling via exploiting the multimodal correlations is
nontrivial considering the following facts. (1) The visual and textual modalities characterize
the same item and thus should share certain consistency. As shown in Fig.2.1a, both visual
and textual modalities deliver the item’s features of “color” and “category”. Additionally,
the user-generated text may provide complementary information to the visual image, such
as the item brand “New Ace” and material “Leather” in Fig.2.1b, yet certain features are
difﬁcult to describe using textual sentences but easy to visualize using the image, such as
the stripe position in the item in Fig.2.1b. Consistent and complementary contents are often
mixed in each modality and may be nonlinearly separable. Therefore, how to explicitly
separate and model them poses one challenge. (2) How to leverage the correlation modeling
results to strengthen the text and vision-oriented representation of the given item forms
another challenge. And (3) outﬁt compatibility modeling can be derived separately from
vision or text-oriented representations, which characterizes the item from different angles.
We argue that these two kinds of modeling share certain common knowledge on the outﬁt
compatibility evaluation and can reinforce each other. How to mutually enhance the two
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
W. Guan et al., Graph Learning for Fashion Compatibility Modeling,
Synthesis Lectures on Information Concepts, Retrieval, and Services,
https://doi.org/10.1007/978-3-031-18817-6_2
7

8
2
Correlation-Oriented Graph Learning for OCM
Fig.2.1 Illustration of the consistent and complementary correlations between the visual and textual
modalities. In a, both the text and image reﬂect the color (dark blue) and category (shorts) of the
item. In b, the text reveals the item’s material (leather) and brand (New Ace) that is rarely derived
visually, but fails to describe the pattern (stripe) position
kinds of modeling and thus boost the ﬁnal compatibility modeling result constitutes the last
challenge.
Toaddresstheaforementionedchallenges,wedeviseacomprehensive MultiModal Outﬁt
Compatibility Modeling scheme, MM-OCM. As shown in Fig.2.2, MM-OCM consists of
four components: (a) multimodal feature extraction, (b) multimodal correlation modeling,
(c) compatibility modeling, and (d) mutual learning. The ﬁrst component extracts the tex-
tual and visual features of the given item via two separate convolution neural networks
(CNNs) [1] and long short-term memory (LSTM) networks [2], respectively. The reason
for introducing two separate feature extractors is to facilitate later mutual learning. The sec-
ond component aims to separate and model the consistent and complementary correlations.
Considering the fact that these two kinds of correlations may not be separable in the original
visual and textual feature spaces, we, therefore, employ the multilayer perceptrons to nonlin-
early project the image/text feature into the consistent and complementary space, where the
modal-modal consistency and complementarity, respectively, can be captured. In the third
component, we incorporate the disengaged complementary content in the textual (visual)
modality to complement the visual (textual) feature embedding and obtain the text (vision)-
oriented representation. Thereafter, we build two independent graph convolutional networks
to model outﬁt compatibility, namely text-oriented compatibility modeling (T-OCM) and
vision-oriented compatibility modeling (V-OCM). Ultimately, the fourth component tar-
gets mutually transferring knowledge from one compatibility modeling to guide the other.
Once MM-OCM converges, we average the compatibility scores predicted by T-OCM and
V-OCM as the ﬁnal result. Extensive experiments on the real-world dataset demonstrate the
superiority of our MM-OCM scheme as compared to several state-of-the-art baselines. As
a byproduct, we released the codes to beneﬁt other researchers.1
1 https://site2750.wixsite.com/mmocm.

2.2
Related Work
9
Fig.2.2 Illustration of the proposed MM-OCM scheme. It consists of four key components: a multi-
modal feature extraction, b multimodal correlation modeling, c compatibility modeling, and d mutual
learning
2.2
Related Work
This work is related to fashion compatibility modeling and deep mutual learning.
Fashion Compatibility Modeling. The recent ﬂourish in the fashion industry has pro-
moted researchers to address many fashion analysis tasks, such as clothing retrieval [3],
compatibility modeling [4, 5], fashion trend prediction [6] and clothing recommendation [7,
8]. Speciﬁcally, as the key to many fashion-oriented applications, such as complementary
item retrieval [9] and personal capsule wardrobe creation [10], fashion compatibility mod-
eling has drawn great research attention. Existing fashion compatibility modeling methods
can be grouped into three categories: pairwise methods [4, 11–13], listwise methods [14],
and graphwise methods [15, 16]. The ﬁrst category focuses on studying the compatibility
between two items. For example, McAuley et al. [11] used linear transformation to map
items into a latent space, where the compatibility relation between items can be measured.
Following that, Song et al. [4] proposed a multimodal compatibility modeling scheme, where
neural networks are used to model the compatibility between fashion items with the Bayesian
personalized ranking (BPR) [17] optimization. Later, Vasileva et al. [18] studied the com-
patibility for outﬁts with multiple fashion items based on pairwise modeling, where the item
category information was additionally considered. One key limitation of this category is that
it lacks a global view of the outﬁt and rarely generates the optimal solution. The second

10
2
Correlation-Oriented Graph Learning for OCM
category regards the outﬁt as a sequence of items in a ﬁxed predeﬁned order. For example,
Han et al. [14] employed the Bi-LSTM network to uncover outﬁt compatibility. It is worth
noting that the underlying assumption used by the listwise methods, i.e., the outﬁt can be
represented as a sequence of ordered items, is questionable. Approaches in the third category
model each outﬁt as an item graph and turn to graph neural networks [19, 20] to fulﬁll the
outﬁt compatibility modeling task. For example, Cui et al. [15] proposed Node-wise graph
neural networks (NGNNs) to promote item representation learning. In addition, Cucurull
et al. [16] addressed the compatibility prediction problem using a graph neural network that
learns to generate product embeddings conditioned on their context.
Although these studies have achieved signiﬁcant success, they focus on either simply
exploring the visual modality of the outﬁt, or considering both the visual and textual modal-
ities while overlooking the sophisticated multimodal correlations.
Deep Mutual Learning. The idea of deep mutual learning is developed from the knowl-
edge distillation, which was ﬁrst introduced by Hinton et al. [21] for transferring the knowl-
edge from a large cumbersome model to a small model to improve the model portabil-
ity. Speciﬁcally, Hu et al. [22] designed an iterative teacher-student knowledge distillation
approach, where the teacher network understands certain knowledge, while the student
network iteratively mimics the teacher’s solution to a certain problem to improve its perfor-
mance. After that, the teacher-student knowledge distillation scheme attracted considerable
attention [23, 24]. However, in many cases, it might be too difﬁcult to obtain a teacher
network with clear domain knowledge. Accordingly, Zhang et al. [25] proposed a deep
mutual learning method for the classiﬁcation task, where there is no explicit static teacher
but an ensemble of students learning collaboratively throughout the training process. There-
after, many researchers have investigated the deep mutual learning in various domains, such
as person reidentiﬁcation [26–28], domain-adapted sentiment classiﬁcation [29], and deep
metric learning [30]. Despite the value of mutual learning in these ﬁelds, its potential in
outﬁt compatibility modeling has been largely unexplored, which is the major concern of
this work.
2.3
Methodology
In this section, we ﬁrst formulate the research problem and then detail the proposed MM-
OCM scheme.
2.3.1
Problem Formulation
We deem the outﬁt compatibility modeling task as a binary classiﬁcation problem. Assume
that we have a training set  composed of N outﬁts, i.e,  = {(Oi, yi) |i = 1, . . . , N},
where Oi is the ith outﬁt, and yi denotes the ground truth label. We set yi = 1 if the outﬁt

2.3
Methodology
11
Oi is compatible, and yi = 0 otherwise. Given an arbitrary outﬁt O, it can be represented
as a set of fashion items, i.e., O = {o1, o2, . . . , om}, where oi is the ith item, associated
with a visual image vi and a textual description ti. The symbol m is a variable for different
outﬁts, considering that the number of items in an outﬁt is not ﬁxed. Based on these training
samples, we target learning an outﬁt compatibility model F that can judge whether the given
outﬁt O is compatible,
s = F

{(vi, ti)}m
i=1|

,
(2.1)
where  is a set of to-be-learned parameters of our model, and s denotes the probability the
given outﬁt is compatible.
2.3.2
Multimodal Outfit Compatibility Modeling
Based upon the deﬁned research problem and notations, we present the comprehensive
MultiModal Outﬁt Compatibility Modeling scheme, MM-OCM. As shown in Fig.2.2, it
consists of four key components: (a) multimodal feature extraction, (b) multimodal corre-
lation modeling, (c) compatibility modeling, and (d) mutual learning.
Multimodal Feature Extraction
We ﬁrst introduce the visual and textual feature extraction.
Visual Feature Extraction. To extract visual features, we utilize the CNNs, which have
shown compelling success in many computer vision tasks [31–33]. To facilitate the mutual
enhancement between the T-OCM and the V-OCM, which are alternatively optimized, we
employ two separate CNNs to extract the visual features. Speciﬁcally, given the outﬁt O,
the visual feature of the ith item in the outﬁt can be obtained as follows,

ˆvi = CNN1 (vi) ,
˜vi = CNN2 (vi) ,
(2.2)
where ˆvi ∈Rdv and ˜vi ∈Rdv refer to the visual features to be processed by the following
T-OCM and V-OCM, respectively. The symbol dv is the dimension of the extracted visual
feature embedding. CNN1 and CNN2 denotes the corresponding CNNs for the T-OCM and
V-OCM, respectively.
Textual Feature Extraction. Due to its prominent performance in textual representation
learning [34, 35], we adopt LSTM to extract the textual feature of the given item.2 Similar
to the visual feature extraction, we also use two separate LSTMs, i.e., LSTM1 and LSTM2,
to obtain the textual features for T-OCM and V-OCM, respectively. Formally, we have
 ˆti = LSTM1 (ti) ,
˜ti = LSTM2 (ti) ,
(2.3)
2 Before feeding into the LSTM, the text is ﬁrst tokenized into standard vocabularies.

12
2
Correlation-Oriented Graph Learning for OCM
where ˆti ∈Rdt and ˜ti ∈Rdt refer to the text features for the following T-OCM and
V-OCM, respectively. dt is the feature dimension. To facilitate the multimodal fusion, we
set dt = dv = d in this work.
Multimodal Correlation Modeling
As illustrated in Fig.2.1, we argue that the visual image and textual description may possess
certain consistency and complementarity information. Inspired by this, instead of unreason-
ably fusing the general multimodal features, we propose clearly separating and explicitly
modeling the consistent and complementary contents of each modality, whereby we expect
the consistent content of a modality can capture the alignment information between two
modalities, and the complementary one of a modality can encode the supplement informa-
tion to the other modality.
In particular, we ﬁrst introduce two MLPs to separate the consistent and complementary
parts of each modality, respectively. Mathematically, we have
⎧
⎪⎨
⎪⎩
ˆvs
i = MLPs
v

ˆvi

, ˆts
i = MLPs
t
	
ˆti

,
ˆvp
i = MLPp
v

ˆvi

, ˆt p
i = MLPp
t
	
ˆti

,
(2.4)
where ˆvs
i and ˆvp
i respectively denote the consistent and complementary representation of
the visual modality, and ˆts
i and ˆt p
i denote that of the textual modality. It is worth mentioning
that the consistent and complementary parts are probably inseparable within the original
low-dimensional space. After nonlinear mapping via MLPs, we can project them into a high-
dimensional space, whereby the consistent and complementary parts are distinguishable.
We then argue that the consistent representations of the two modalities are parallel, and
the complementary representations are orthogonal. Accordingly, to regulate the consistent
and complementary representations, we use the following objective functions:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
Ls =
m

i=1
{cos(ˆvs
i , ˆts
i )2 + cos(˜vs
i , ˜ts
i )2},
Lp =
m

i=1
{[cos(ˆvp
i , ˆt p
i ) −1]2 + [cos(˜vp
i , ˜t p
i ) −1]2}.
(2.5)
where Ls and Lp refer to the consistent and complementary regularizations, respectively.
Compatibility Modeling
We here ﬁrst introduce the text/vision-oriented representation learning for each item, and
we then present the text/vision-oriented compatibility modeling.
Text/Vision-oriented Representation Learning. Based upon the component of multimodal
correlation modeling, we can derive the complementary cues of the textual (visual) modal-
ity from the visual (textual) modality. Distinguished from the consistent parts that are
shared between modalities, complementarity means exclusive and supplement information.

2.3
Methodology
13
Inspired by this, to learn comprehensive item representations and hence boost the outﬁt
compatibility modeling performance, we introduce two multimodal fusion strategies: text-
oriented multimodal fusion and vision-oriented multimodal fusion. As to the ﬁrst strategy,
we take the textual feature extracted by LSTM as the basis and additionally incorporate
the complementary representation of the visual modality. By contrast, in the latter fusion
strategy, we strengthen the visual feature extracted by CNN with the complementary repre-
sentation of the textual modality. Speciﬁcally, based upon the consistent and complementary
representation of each modality, we can derive the ﬁnal item representations from different
fusion schemes as follows,

ˆoi = ˆti + ˆvp
i ,
˜oi = ˜vi + ˜t p
i ,
(2.6)
where ˆoi and ˜oi denote the ﬁnal item representation based on the text-oriented multimodal
fusion and vision-oriented multimodal fusion, respectively.
Text/Vision-oriented Compatibility Modeling. Similar to previous studies, we employ a
graph convolutional network (GCN) to ﬂexibly model the compatibility of the outﬁt with a
variablenumberofitems.Inparticular,weadopttwoGCNs,onefortheT-OCM,andtheother
for the V-OCM. Regarding the limited space, we take the T-OCM as an example, since the V-
OCM can be derived in the same way. In particular, for each outﬁt O composed of m fashion
items, we ﬁrst construct an indirected graph G = (E, R). E = {oi}m
i=1 is the set of nodes, cor-
respondingtotheitemsofthegivenoutﬁt O.Additionally,R =

oi, o j

|i, j ∈[1, . . . , m]

denotes the set of edges. In this work, for each pair of items oi and o j in the outﬁt, we intro-
duce an edge. During learning, each node oi is associated with a hidden state vector hi,
which keeps dynamically updated to fulﬁll the information propagation over the graph. For
T-OCM, we initialize the hidden state vector for the ith node based on the text-oriented
representation of the ith item, namely, hi = ˆoi.
The information propagation from item o j to item oi is deﬁned as follows:
m j→i = φ[Wpp(hi ⊙h j) + bpp],
(2.7)
where Wpp ∈Rd×d and bpp ∈Rd denote the weight matrix and bias vector to be learned;
φ(·) is a nonlinear activation function, which is set as LeakyReLU; hi ⊙h j accounts for
the interaction between the fashion item oi and o j; ⊙is the elementwise product operation.
By summarizing the information propagated from all neighbors, the hidden state vector
corresponding to the item oi can be updated as follows,
h∗
i = φ (W0hi + b0) +

o j∈Ni
m j→i,
(2.8)
where W0 ∈Rd×d and b0 ∈Rd denote the weight matrix and bias vector to be learned; Ni
denotesthesetofneighbornodesofnodeoi andh∗
i ∈Rd istheupdatedhiddenrepresentation
of the item oi.

14
2
Correlation-Oriented Graph Learning for OCM
We ultimately feed the updated item representation to an MLP, consisting of two fully-
connected layers, to derive its probability of being a compatible outﬁt as follows,
⎧
⎪⎪⎨
⎪⎪⎩
si
t = W2

ψ

W1h∗
i + b1

+ b2,
st = σ
	 1
m
m

i=1
si
t

,
(2.9)
where W1, b1, W2, and b2 are the to-be-learned layer parameters. ψ(·) refers to the Relu
active function, and σ(·) denotes the Sigmoid function to ensure the compatibility probability
falling in the range of [0, 1]. Notably, in the same way, we can derive the compatible
probability of the outﬁt by V-OCM, which is termed as sv.
Mutual Learning
In a sense, regardless of the text-oriented item representation or the vision-oriented repre-
sentation, i.e., ˆoi and ˜oi, both of them fuse the multimodal data of an item. Therefore, the
information encoded by these two representations is largely aligned, and hence the corre-
sponding outﬁt compatibility modeling yields similar outputs. Additionally, they emphasize
the different aspects of the item and hence may complement each other from a global view.
Therefore, the knowledge learned by one compatibility model can guide the other model.
Inspired by this, we turn to the deep mutual learning knowledge distillation scheme to
regularize these two compatibility modeling results, mutually reinforcing them.
Unlike the traditional teacher-student knowledge distillation network, mutual learning
replaces the one-way knowledge transfer from the static pretrained teacher to the student
with the mutual knowledge distillation. In particular, an ensemble of student networks is
employed to learn collaboratively. In our context, the T-OCM and the V-OTM can be treated
as two student networks, and optimized alternatively. Namely, in each iteration, we only train
one student network, while keeping the other ﬁxed, which temporarily acts as the teacher.
We cast the compatibility modeling as a binary classiﬁcation task, and adopt the widely-
used cross-entropy loss for both T-OCM and V-OCM. Accordingly, we have the objective
functions,

Lt
ce = −ylog(st) −(1 −y)log(1 −st),
Lv
ce = −ylog(sv) −(1 −y)log(1 −sv),
(2.10)
where y refers to the ground truth label of the outﬁt O. Lt
ce and Lv
ce are the objective functions
for the T-OCM and V-OCM, respectively.
To encourage the two student networks to learn from each other, we adopt the Kullback
Leibler (KL) divergence loss function to penalize the distance between the evaluation results
of the T-OCM and V-OCM as follows,
⎧
⎪⎪⎨
⎪⎪⎩
Lv−>t = svlog sv
st
+ (1 −sv)log (1 −sv)
(1 −st) ,
Lt−>v = stlog st
sv
+ (1 −st)log (1 −st)
(1 −sv).
(2.11)

2.4
Experiment
15
Notably, we use Lv−>t for training T-OCM, and Lt−>v for training V-OCM. Finally, we
have

Lt = Lt
ce + λLv−>t + ηLs + μLp,
Lv = Lv
ce + λLt−>v + ηLs + μLp,
(2.12)
where λ, η, and μ are tradeoff hyperparameters. Lt and Lv are the ﬁnal loss functions for the
T-OCM and V-OCM, respectively. Each compatibility modeling component (i.e., T-COM
or V-OCM) not only learns to correctly predict the true label of the training instances, but
also learns to mimic the output of the other compatibility modeling component, where the
consistent and complementary regularizations are also jointly satisﬁed. Notably, although
both Lt and Lv have consistent and complementary regularizations, i.e., Ls and Lp, the
parameters to be optimized are distinguished, where the regularizations in Lt optimize the
T-OCM, while that in Ls aim to learn parameters of V-OCM. Once our MM-OCM is well-
trained, we take the average of the predicted compatibility probabilities of the V-OCM and
T-OCM as the ﬁnal compatibility probability of the outﬁt.
2.4
Experiment
In this section, we conducted experiments over two real-world datasets by answering the
following research questions.
• RQ1: Does MM-OCM outperform state-of-the-art baselines?
• RQ2: How does each module affect MM-OCM?
• RQ3: How is the qualitative performance of MM-OCM?
2.4.1
Experimental Settings
Datasets
To evaluate the proposed method, we adopted the Polyvore Outﬁts dataset [18], which is
widely utilized by several fashion analysis works [36, 37]. This dataset is collected from
the Polyvore fashion website. Considering whether fashion items overlap in the training,
validation and testing dataset, this dataset provides two dataset versions: the nondisjoint
and disjoint versions, termed Polyvore Outﬁts and Polyvore Outﬁts-D. There are a total
of 68, 306 outﬁts in Polyvore Outﬁts, divided into three sets: training set (53,306 outﬁts),
validation set (5,000 outﬁts), and testing set (10,000 outﬁts). The disjoint version, Polyvore
Outﬁts-D, contains a total of 32,140 outﬁts, where 16,995 outﬁts are for training, 3,000
outﬁts are for validation, and 15,145 outﬁts are for testing. Each outﬁt in the Polyvore
Outﬁts has at least 2 items and up to 19 items, while that in the Polyvore Outﬁts-D has at
least 2 items and up to 16 items. The average number of items in an outﬁt in Polyvore Outﬁts
and Polyvore Outﬁts-D is 5.3 and 5.1, respectively.

16
2
Correlation-Oriented Graph Learning for OCM
Evaluation Tasks
To evaluate the proposed model, we conducted experiments on two tasks: outﬁt compatibility
estimation and ﬁll-in-the-blank (FITB) fashion recommendation.
Outﬁt compatibility estimation: This task is to estimate a compatibility score for a given
outﬁt.Differentfrom the previousstudy[14]that generatesnegativeoutﬁtsrandomlywithout
any restriction, we replaced each item in the positive compatible outﬁt with another randomly
selected item in the same category, which makes the task more challenging and practical.
The ratio of positive and negative samples is set to 1 : 1. The positive compatible outﬁts
are labeled as 1, while the negative outﬁts are labeled as 0. Similar to previous studies[14,
37], we selected the area under the receiver operating characteristic curve (AUC) as the
evaluation metric.
FITB fashion recommendation: Given an incomplete outﬁt and a target item annotated
with the question mark, this task aims to select the most compatible fashion item from
a candidate item set to ﬁll in the blank and transform the given incomplete outﬁt into a
compatible and complete outﬁt. This task is practical since people need to buy garments to
match the garments they already have. Speciﬁcally, we constructed the FITB question by
randomly selecting an item from a positive/compatible outﬁt as the target item and replacing
it with a blank. We then randomly selected 3 items in the same category along with the target
item to form the candidate set. The performance on this task was evaluated by the accuracy
(ACC) of choosing the correct item from the candidate items.
Implementation Details
For the image encoder, we selected the ImageNet [38] pretrained ResNet18 [32] as the
backbone, and modiﬁed the last layer to make the output feature dimension as 256. Regarding
the text encoder, we set the word embedding size to 512, and the dimension of the hidden
layer in LSTM to 256. We alternatively trained the T-OCM and V-OCM by the Adam
optimizer [39] with a ﬁxed learning rate of 0.0001, and batch size of 16. The tradeoff
hyperparameters in Eq.(2.12) are set as λ = η = μ = 1. In particular, we launched 10-fold
cross validation for each experiment and reported the average results. All the experiments
were implemented by PyTorch on a server equipped with 4 NVIDIA TITAN Xp GPUs, and
the random seeds were ﬁxed for reproducibility.
2.4.2
Model Comparison
To validate the effectiveness of our proposed scheme, we chose the following baselines for
comparison.
• Bi-LSTM [14] takes the items in an outﬁt as a sequence ordered by the item category
and fulﬁlls the fashion compatibility modeling with Bi-LSTM. For a fair comparison, we
only utilized the visual information.

2.4
Experiment
17
Table 2.1 Performance comparison between our proposed MM-OCM scheme and other baselines
over two datasets
Method
Polyvore Outﬁts
Polyvore Outﬁts-D
Compat. AUC
FITB ACC (%)
Compat. AUC
FITB ACC (%)
Bi-LSTM
0.68
42.20
0.65
40.10
Type-aware
0.87
56.60
0.78
47.30
SCE-NET
0.83
52.80
0.82
52.10
NGNN
0.75
53.02
0.68
42.49
Context-aware
0.81
55.63
0.77
50.34
HFGN
0.84
49.90
0.70
39.03
MM-OCM
0.93
63.40
0.88
58.02
• Type-aware [18] designs type-speciﬁc embedding spaces according to the item category,
where the textual item descriptions are adopted via the visual-semantic loss.
• SCE-NET [36] is a pairwise method, which utilizes multiple similarity condition masks
to embed the item features into different semantic subspaces. This method also considers
textual information.
• NGNN [15] employs a GNN to address the compatibility modeling task, where the node
is updated by a gate mechanism. For multimodal features, NGNN designs two graph
channels, and the ﬁnal compatibility score is derived as a weighted average.
• Context-aware [16] regards fashion compatibility modeling as an edge prediction prob-
lem,whereagraphautoencoderframeworkisintroduced.Notably,onlythevisualfeatures
are employed.
• HFGN [37] shares the same spirit with NGNN and builds a category-oriented graph,
where an R-view attention map and an R-view score map are introduced to compute the
compatibility score. This baseline only uses visual features.
Table2.1 shows the performance comparison among different methods on two datasets
under two tasks. From this table, we make the following observations. (1) Among all the
baselines, Bi-LSTM performs the worst, which suggests that modeling the outﬁt as an
ordered list of items is not reasonable. (2) The methods that use multimodal features gain
more promising results (e.g., Type-aware on Polyvore Outﬁts and SCE-NET on Polyvore
Outﬁts-D) compared to those that only utilize the visual features (i.e., HFGN and Context-
aware). This implies that considering both visual and textual modalities is rewarding in
the outﬁt compatibility modeling task. (3) MM-OCM consistently surpasses all baseline
methods on the two datasets under both tasks. This indicates the advantage of our scheme
that utilizes multimodal correlation modeling and mutual learning in the context of outﬁt
compatibility modeling. Notably, we performed the ten-fold t-test between our proposed

18
2
Correlation-Oriented Graph Learning for OCM
Table 2.2 Ablation study of our proposed MM-OCM scheme on two datasets. The best results are
in boldface
Method
Polyvore Outﬁts
Polyvore Outﬁts-D
Compat. AUC
FITB ACC (%)
Compat. AUC
FITB ACC (%)
w/o Correlation
0.91
52.91
0.87
54.47
w/o Mutual
0.92
60.80
0.86
55.62
Image_Only
0.90
57.80
0.85
52.85
Text_Only
0.79
42.28
0.74
35.45
Concat_Directly
0.91
58.67
0.79
49.73
MM-OCM
0.93
63.40
0.88
58.02
scheme and each of the baselines. We observed that all the p-values are much smaller than
0.05, and we concluded that the MM-OCM is signiﬁcantly better than the baselines.
2.4.3
Ablation Study
To verify the importance of each component in our model, we also compared MM-OCM
with the following derivatives.
• w/o Correlation: To explore the effect of multimodal correlation modeling, we removed
this component by setting ˆoi = ˆti and ˜oi = ˜vi in Eq.(2.6).
• w/o Mutual: To study the effect of the mutual learning component, we removed the
knowledge distillation between the T-OCM and V-OCM by setting λ = 0.
• Image_Only and Text_Only: The two derivatives were set to verify the importance of
visual and textual information. Speciﬁcally, for the Image_Only, we removed T-OCM by
setting ˜oi = ˜vi, while for the Text_Only, V-OCM was removed by setting ˆoi = ˆti.
• Concat_Directly: To gain more insights into our utilization of visual and textual infor-
mation, we directly concatenated the visual and textual features of each item and fed
them to an MLP to obtain ˆoi. Accordingly, the correlation modeling and mutual learning
were simultaneously removed.
Table2.2 shows the ablation results of our MM-OCM. From this table, we make the fol-
lowing observations. (1) w/o Correlation performs worse than our MM-OCM, which proves
the effectiveness of the proposed multimodal consistency and complementarity modeling.
(2) MM-OCM surpasses w/o Mutual, indicating that the mutual learning component is
helpful for integrating the T-OCM and V-OCM by transferring knowledge between the two
modules. (3) Both Image_Only and Text_Only are inferior to MM-OCM, which suggests
that it is essential to consider both visual and textual information to gain better outﬁt compati-

2.5
Summary
19
bility modeling effects. In addition, Image_Only outperforms Text_Only remarkably, which
reﬂects that the image contains more useful information than the text, which corresponds
with the saying that “a picture is worth a thousand words”. (4) compared to our MM-OCM,
Concat_Directly also delivers worse performance, implying that simply fusing visual and
textual features is insufﬁcient to explore the intrinsic correlation of the two modalities. This
further veriﬁes the superiority of our strategy that models the multimodal correlation and
devises two schemes of multimodal fusion. Furthermore, it can be observed that on the
more challenging Polyvore Outﬁts-D dataset, the results of Concat_Directly are better than
those of Text_Only but worse than those of Image_Only. This phenomenon indicates that
an inappropriate multimodal fusion method is less effective than only utilizing the more
informative modality.
2.4.4
Case Study
To gain a thorough understanding of our model, we also conducted a qualitative evaluation of
our method. Figure2.3 intuitively shows several testing examples on the outﬁt compatibility
estimation and ﬁll-in-the-blank tasks. From Fig.2.3a, we observed that for the example in
the ﬁrst row, which contains items with consistently black color and elegant style, our MM-
OCM can assign it with a high compatible probability. As for the outﬁt in the last row with
obviously incompatible colors, e.g., green does not go well with red, our MM-OCM gives
a low compatibility score. In Fig.2.3b, we can see that our method can choose the most
suitable item from the candidate set to form a compatible outﬁt. For the example in the ﬁrst
row, the outﬁt lacks a pair of shoes and our MM-OCM correctly selects the ﬁrst item by
attributing a high compatibility score. As can be seen, the selected item matches well with
other items in the query. As to the example in the second row, although our method chooses
the correct answer (item D), it also gives a high compatibility score to the item B, since
these two items are both dark jackets of the same style. This reconﬁrms the compatibility
modeling capabilities of our model.
2.5
Summary
In this chapter, we solved the outﬁt compatibility modeling problem with graph convolu-
tional networks by exploring the multimodal correlations. In particular, we clearly separated
and explicitly modeled the consistent and complementary relations between the visual and
textual modalities. This was accomplished by nonlinearly projecting the consistent and com-
plementary contents into the separable spaces, whereby they were respectively formulated
by parallel and orthogonal regularizers. We then applied the complementary information to
strengthen the vision- and text-oriented representations. Based upon these two kinds of rep-
resentations, two compatibility modeling brunches were derived and reinforced by mutual

20
2
Correlation-Oriented Graph Learning for OCM
alexander wang 
runway studded 
black
topshop rule suede 
mules
fallon womens 
ﬂoreƩe choker
lanvin black 
tassel earrings
solace london 
plunge neck 
poppy
0.9859
celine black 
smooth 
leather mini
saint laurent 
pointed patent 
leather pumps
forever 21 
oui non 
necklace
equipment 
nature white 
signature silk
karen walker 
crazy tort 
northern
vince pants 
leather 
jogging
leopard 
print lapel 
collar long
0.8787
pre-owned judith leiber 
taƟana champagne
vivienne westwood volupté 
court shoes with orb heel
dolce gabbana chanƟlly 
lace trimmed
0.7795
coach madison 
cafe carryall in
kat maconie 
betsy sandals
mafalda von 
hessen neck-Ɵe silk
vintage 60s bright ﬂoral 
mulƟcolor midi skirt
yellow lapel double 
breasted woolen
0.1312
becksondergaard 
sherlock leather bag light
marcia moran 
light blue cats
brides 18 favorite 
bridesmaid dresses
jeﬀrey campbell 
10mm jeweled 
leather sandals - 
light blue
giuseppe 
zanotti metallic 
leather skinny-
strap sandals
rachel comey 
tuco clog sandal
chain-
embellished 
sandals
0.9859
0.0004
0.0008
0.0009
A.
B.
C.
D.
oook 
balenciaga 
womens 
bags 2012
carvela janet 
leather court 
shoe with toe 
cap
amrita 
singh andra 
summer 
necklace
alexander 
wang 
chunky-knit 
turtleneck 
sweater
balmain 
cropped leather 
motocross 
pants
alexander 
mcqueen 
skull chiﬀon 
scarf
faux suede 
biker jacket
allsaints level leather 
biker jacket
alexander mcqueen 
folded peplum jacket
shearling biker 
jacket
0.0670
0.4384
0.0185
0.4762
A.
B.
C.
D.
(b) Fill-in-the-blank (FITB)
(a) Ouƞit CompaƟbility EsƟmaƟon
Fig.2.3 Qualitative results of MM-OCM on a outﬁt compatibility estimation, and b ﬁll-in-the-blank

References
21
learningviaknowledgetransfer.Extensiveexperimentsovertwobenchmarkdatasetsveriﬁed
the effectiveness of our proposed MM-OCM scheme compared with several state-of-the-art
baselines.
References
1. Szegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
2016. Rethinking the Inception Architecture for Computer Vision. In IEEE Conference on Com-
puter Vision and Pattern Recognition, 2818–2826. IEEE.
2. Hochreiter, Sepp, and Jürgen. Schmidhuber. 1997. Long Short-Term Memory. Neural Compu-
tation 9 (8): 1735–1780.
3. Liang, Xiaodan, Liang Lin, Wei Yang, Ping Luo, Junshi Huang, and Shuicheng Yan. 2016.
Clothes Co-parsing via Joint Image Segmentation and Labeling with Application to Clothing
Retrieval. IEEE Transactions on Multimedia 18 (6): 1175–1186.
4. Song, Xuemeng, Fuli Feng, Jinhuan Liu, Zekun Li, Liqiang Nie, and Jun Ma. 2017. NeuroStylist:
Neural Compatibility Modeling for Clothing Matching. In Proceedings of the ACM International
Conference on Multimedia, 753–761. ACM.
5. Liu, Jinhuan, Xuemeng Song, Zhumin Chen, and Jun Ma. 2019. Neural Fashion Experts: I Know
How to Make the Complementary Clothing Matching. Neurocomputing 359 (24): 249–263.
6. Gu, Xiaoling, Yongkang Wong, Pai Peng, Lidan Shou, Gang Chen, and Mohan S Kankanhalli.
2017. Understanding Fashion Trends from Street Photos via Neighbor-constrained Embedding
Learning. In Proceedings of the ACM International Conference on Multimedia, 190–198. ACM.
7. Song, Xuemeng, Xianjing Han, Yunkai Li, Jingyuan Chen, Xin-Shun Xu, and Liqiang Nie. 2019.
GP-BPR: Personalized Compatibility Modeling for Clothing Matching. In Proceedings of the
ACM International Conference on Multimedia, 320–328. ACM.
8. Chen, Wen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li, Andreas Pfadler,
Huan Zhao, and Binqiang Zhao. 2019. POG: Personalized Outﬁt Generation for Fashion Recom-
mendation at Alibaba iFashion. In Proceedings of the International ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, 2662–2670. ACM.
9. Wen, Haokun, Xuemeng Song, Xin Yang, Yibing Zhan, and Liqiang Nie. 2021. Comprehensive
Linguistic-Visual Composition Network for Image Retrieval. In Proceedings of the International
ACM SIGIR Conference on Research and Development in Informaion Retrieval, 1369–1378.
ACM.
10. Dong, Xue, Xuemeng Song, Fuli Feng, Peiguang Jing, Xin-Shun Xu, and Liqiang Nie. 2019.
Personalized Capsule Wardrobe Creation with Garment and User Modeling. In Proceedings of
the ACM International Conference on Multimedia, 302–310. ACM.
11. McAuley, Julian J., Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-
Based Recommendations on Styles and Substitutes. In Proceedings of the International ACM
SIGIR Conference on Research and Development in Information Retrieval, 43–52. ACM.
12. Han, Xianjing, Xuemeng Song, Jianhua Yin, Yinglong Wang, and Liqiang Nie. 2019. Prototype-
guided Attribute-wise Interpretable Scheme for Clothing Matching. In Proceedings of the inter-
national ACM SIGIR Conference on Research and Development in Information Retrieval, 785–
794. ACM.
13. Lin, Yujie, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma, and Maarten de Rijke. 2020.
Explainable Outﬁt Recommendation with Joint Outﬁt Matching and Comment Generation. IEEE
Transactions on Knowledge and Data Engineering 32 (8): 1502–1516.

22
2
Correlation-Oriented Graph Learning for OCM
14. Han, Xintong, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis. 2017. Learning Fashion Com-
patibility with Bidirectional LSTMs. In Proceedings of the ACM International Conference on
Multimedia, 1078–1086. ACM.
15. Cui, Zeyu, Zekun Li, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Dressing as a Whole:
Outﬁt Compatibility Learning Based on Node-wise Graph Neural Networks. In Porceedings of
the World Wide Web Conference, 307–317. ACM.
16. Cucurull, Guillem, Perouz Taslakian, and David Vázquez. 2019. Context-Aware Visual Com-
patibility Prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 12617–12626. IEEE.
17. Rendle, Steffen, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR:
Bayesian Personalized Ranking from Implicit Feedback. In Proceedings of the International
Conference on Uncertainty in Artiﬁcial Intelligence, 452–461. AUAI Press.
18. Vasileva, Mariya I., Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, and
David A. Forsyth. 2018. Learning Type-Aware Embeddings for Fashion Compatibility. In Euro-
pean Conference on Computer Vision, 405–421. Springer.
19. Hamilton, William L., Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning
on Large Graphs. In Advances in Neural Information Processing Systems, 1024–1034. Curran
Associates Inc.
20. Kipf, Thomas N., and Max Welling. 2017. Semi-Supervised Classiﬁcation with Graph Convo-
lutional Networks. In International Conference on Learning Representations, 1–15. OpenRe-
view.net.
21. Hinton, Geoffrey E., Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowledge in a Neural
Network. arXiv:1503.02531.
22. Hu, Zhiting, Xuezhe Ma, Zhengzhong Liu, Eduard H. Hovy, and Eric P. Xing. 2016. Harnessing
Deep Neural Networks with Logic Rules. In Proceedings of the Association for Computational
Linguistics, 2410–2420. ACL.
23. Zhang, Peng, Li Su, Liang Li, BingKun Bao, Pamela C. Cosman, Guorong Li, and Qingming
Huang. 2019. Training Efﬁcient Saliency Prediction Models with Knowledge Distillation. In
Proceedings of the ACM International Conference on Multimedia, 512–520. ACM.
24. Han, Xianjing, Xuemeng Song, Yiyang Yao, Xu. Xin-Shun, and Liqiang Nie. 2020. Neural
Compatibility Modeling With Probabilistic Knowledge Distillation. IEEE Transactions on Image
Processing 29 (2020): 871–882.
25. Zhang,Ying,TaoXiang,TimothyM.Hospedales,andHuchuanLu.2018.DeepMutualLearning.
In IEEE Conference on Computer Vision and Pattern Recognition, 4320–4328. IEEE.
26. Luo, Hao, Wei Jiang, Xuan Zhang, Xing Fan, Jingjing Qian, and Chi Zhang. 2019. Aligne-
dReID++:DynamicallyMatchingLocalInformationforPersonRe-identiﬁcation.PatternRecog-
nition 94 (2019): 53–61.
27. Zhai, Yunpeng, Qixiang Ye, Shijian Lu, Mengxi Jia, Rongrong Ji, and Yonghong Tian. 2020.
Multiple Expert Brainstorming for Domain Adaptive Person Re-Identiﬁcation. In Proceedings
of the European Conference on Computer Vision, 594–611. Springer.
28. Ge, Yixiao, Dapeng Chen, and Hongsheng Li. 2020. Mutual Mean-Teaching: Pseudo Label
Reﬁnery for Unsupervised Domain Adaptation on Person Re-identiﬁcation. In International
Conference on Learning Representations, 1–12. OpenReview.net.
29. Xue, Qianming, Wei Zhang, and Hongyuan Zha. 2020. Improving Domain-Adapted Sentiment
Classiﬁcation by Deep Adversarial Mutual Learning. In The AAAI Conference on Artiﬁcial
Intelligence, 9362–9369. AAAI Press.
30. Park, Wonpyo, Wonjae Kim, Kihyun You, and Minsu Cho. 2020. Diversiﬁed Mutual Learning
for Deep Metric Learning. In Proceedings of the European Conference on Computer Vision,
709–725. Springer.

References
23
31. Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going Deeper with Convo-
lutions. In IEEE Conference on Computer Vision and Pattern Recognition, 1–9. IEEE.
32. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for
Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 770–778. IEEE Computer Society.
33. Liu, Meng, Liqiang Nie, Meng Wang, and Baoquan Chen. 2017. Towards Micro-video Under-
standing by Joint Sequential-Sparse Modeling. In Proceedings of the ACM International Con-
ference on Multimedia, 970–978. ACM.
34. Chen, Yuxiao, Jianbo Yuan, Quanzeng You, and Jiebo Luo. 2018. Twitter Sentiment Analysis via
Bi-senseEmojiEmbeddingandAttention-basedLSTM.InProceedingsoftheACMInternational
Conference on Multimedia, 117–125. ACM.
35. Jin, Zhiwei, Juan Cao, Han Guo, Yongdong Zhang, and Jiebo Luo. 2017. Multimodal Fusion
with Recurrent Neural Networks for Rumor Detection on Microblogs. In Proceedings of the
ACM on Multimedia Conference, 795–816. ACM.
36. Tan, Reuben, Mariya I. Vasileva, Kate Saenko, and Bryan A. Plummer. 2019. Learning Similarity
Conditions Without Explicit Supervision. In Proceedings of the IEEE International Conference
on Computer Vision, 10372–10381. IEEE.
37. Li, Xingchen, Xiang Wang, Xiangnan He, Long Chen, Jun Xiao, and Tat-Seng Chua. 2020.
HierarchicalFashionGraphNetworkforPersonalizedOutﬁtRecommendation.InProceedingsof
the international ACM SIGIR Conference on Research and Development in Informaion Retrieval,
159–168. ACM.
38. Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. ImageNet: A
Large-scale Hierarchical Image Database. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 248–255. IEEE.
39. Kingma, Diederik P., and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Pro-
ceedings of the International Conference on Learning Representations, 1–15. OpenReview.net.

3
Modality-Oriented Graph Learning for OCM
3.1
Introduction
Beyond the fashion compatibility modeling, introduced in Chap.2, which only considers
the visual and textual modalities, as well as only the intramodal compatibility, this chapter
presents the modality-oriented graph learning for fashion compatibility modeling, whereby
both the intramodal and intermodal compatibilities between fashion items are incorporated
for propagating over the entire graph. As shown in Fig.3.1, each outﬁt usually consists of
multiple fashion items, each of which is characterized by an image, a textual description, and
category information. Therefore, to fully utilize the cues delivered by different modalities
of fashion items and comprehensively model the compatibility of outﬁts, many research
efforts [1, 2] have attempted to address the problem of outﬁt compatibility modeling with
the multimodal information of fashion items.
Despite their remarkable performance, they suffer from the following two key limitations.
(1) Prior studies focus on visual and textual modalities, and few of them utilize the fashion
item category information. Additionally, these few studies [3, 4] focus on using items’ cat-
egories to supervise the model learning, but fail to regard the category information as one
essential input modality, i.e., comparable to the visual and textual modalities. And (2) pre-
vious efforts focus on the intramodal compatibility, i.e., the compatibility relation between
the same fashion item modalities in an outﬁt, but overlook the intermodal compatibility, i.e.,
the compatibility relation between the different fashion item modalities, thereby probably
causing suboptimal performance. The underlying philosophy is twofold: (a) since different
modalities of an item tend to reﬂect the same fashion item characteristics [5, 6], incor-
porating the intermodal compatibility (e.g., visual-textual compatibility) can supplement
the intramodal compatibility (e.g., visual-visual compatibility) and strengthen the overall
compatibility estimation from an auxiliary perspective. (b) Different modalities of the same
fashion item can also emphasize the different aspects of the same item. For example, the
visual modality is more likely to reveal the color and pattern of the item, while the textual
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
W. Guan et al., Graph Learning for Fashion Compatibility Modeling,
Synthesis Lectures on Information Concepts, Retrieval, and Services,
https://doi.org/10.1007/978-3-031-18817-6_3
25

26
3
Modality-Oriented Graph Learning for OCM
Fig. 3.1 Example of an outﬁt composition, which consists of four items. Each item has an image,
textual description, and category information
modality tends to deliver its material and brand. As seen in Fig.3.2, the given coat is visually
compatible with both pairs of shoes. However, if the intermodal compatibilities between the
image of the coat and the text descriptions of the two pairs of shoes are investigated, it should
be easy to determine that the given top is more suitable to pair with the quilted boots rather
than the canvas shoes.
Considering the outlined limitations, we propose incorporating items’ category infor-
mation with their content information (i.e., visual and textual modalities) and jointly
model their intramodal and intermodal compatibilities to optimize the outﬁt compatibility
Fig.3.2 Examples of the
intermodal compatibility
relation. The green and red
arrows represent the
compatible and incompatible
relation, respectively

3.1
Introduction
27
Concatenation
MLP
Score 
k=1
k=K
Concatenation
GAP
GAP
GAP
GAP
Category 
Embedding
shirt   jeans   purse  sandals
chanel vintage brown leather shoulder
lucluc neck casual black white
dark wash ripped knee distressed
valentino cavallino double rockstud leopard
GAP
Global Average 
Pooling
Element-wise Product
Activation Function
Legend
Edge Representation 
Generation
Edge Representation 
Generation
Max&Mean 
Pooling
Mean Pooling
GRU
Compatibility Propagation
(b) Modality-Oriented 
Graph Learning
text
image
category
ResNet-18
…
…
Max&Mean 
Pooling
Max&Mean 
Pooling
(a) Multi-modal Embedding
(c) Estimation
TextCNN
Word 
Embedding
inter-modal (        )
intra-modal (        )
Fig. 3.3 Illustration of the proposed scheme, which consists of three modules: multimodal embedding, modality-oriented graph learning, and outﬁt
compatibility estimation. The ﬁrst module extracts the multimodal features of fashion items, and the second module reﬁnes the representation of each
fashion item by absorbing its intramodal and intermodal compatibility relation with the other items. Ultimately, the third module ﬁrst aggregates the
composing items’ representations and then uses the MLP to estimate the outﬁt compatibility score

28
3
Modality-Oriented Graph Learning for OCM
modeling. However, this is nontrivial due to the following challenges. (1) Undoubtedly, the
visual modality plays a pivotal role in outﬁt compatibility modeling. In fact, it usually deliv-
ers not only the low-level visual features (e.g., color, shape) but also the high-level visual
features (e.g., style) of fashion items. Therefore, how to thoroughly explore the low-level and
high-level visual features and thus beneﬁt the compatibility modeling poses a key challenge
for us. (2) Due to the fact that each outﬁt always comprises various fashion items, among
which there is no clear order, and the matching degree between each pair of items affects
the outﬁt compatibility, we model the outﬁt as an item graph. Moreover, similar to existing
studies [7, 8], we resort to the GCN to fulﬁll the outﬁt compatibility modeling. Accordingly,
how to effectively propagate both the intramodal and intermodal compatibilities among the
fashion graph to derive the outﬁt compatibility also constitutes an essential challenge. (3)
Essentially, one key step of outﬁt compatibility modeling is to learn an accurate latent rep-
resentation of the outﬁt that can capture outﬁt compatibility. Therefore, how to seamlessly
unify the multimodal information of fashion items to derive the latent outﬁt representation
is another crucial challenge.
To address the aforementioned challenges, a multimodal outﬁt compatibility modeling
scheme with modality-oriented graph learning is presented, called MOCM-MGL. As shown
in Fig.3.3, MOCM-MGL consists of three modules: multimodal embedding, modality-
oriented graph learning, and outﬁt compatibility estimation. The multimodal embedding
module comprises three encoders to extract the visual, textual, and category features of fash-
ion items. In particular, multiple intermediate convolutional layers of the CNN are adopted
to derive both the low-level and high-level visual features. In addition, the TextCNN [9] is
utilized to embed the textual modality, and directly assign the to-be-learned embedding vec-
tor to each category. The modality-oriented graph learning module introduces a multimodal
item graph for each outﬁt and propagates both the intramodal and intermodal compatibility
relation among fashion items to reﬁne the fashion item representations. Notably, instead of
simply using the 1-D co-occurrence frequency of categories, the edge between two item
nodes is deﬁned by a multidimensional embedding to encode the complex compatibility
relation between two items. Ultimately, the outﬁt compatibility estimation module derives
the latent outﬁt representation by aggregating all the composing items’ representations, and
based on that, estimates the outﬁt compatibility with the multilayer perceptron (MLP) [10].
3.2
Related Work
Different from the categorization of related work on fashion compatibility modeling men-
tioned in Chap.2, in this part, we group the fashion compatibility modeling studies into two
categories: single-modal methods and multimodal methods, according to the input informa-
tion of fashion items.

3.2
Related Work
29
Single-modal methods onlyutilize the visualortextual modalityoffashionitems.Appar-
ently, the visual modality plays a signiﬁcant role in compatibility modeling, as many char-
acteristics of items, such as color and shape, are encoded by visual information. Therefore,
existing efforts exploit the visual information of fashion items. For example, Tangseng et
al. [11] deﬁned an outﬁt as a few ordered slots, corresponding to the common item categories
(range from outerwear to accessory), and concatenated the visual representations of all the
composing items in the outﬁt as the outﬁt representation. In addition, Cucurull et al. [12]
built a graph with all fashion items in the dataset, where each node is initialized by the
corresponding visual feature and receives messages from its neighborhood to learn the con-
textual item embedding. Apart from the visual modality, Chaidaroon et al. [13] investigated
the potential of fashion item textual modality in the outﬁt compatibility modeling, where
a text-based neural compatibility ranking model is proposed. Although great progress has
been made by these works, they utilize only a single modality of fashion items and overlook
the potential of combining the multimodal fashion item information.
Multimodal methods involve more than one fashion item modality. For example, Han
et al. [14] proposed a bidirectional LSTM method to sequentially model the outﬁt compat-
ibility by predicting the next item conditioned on previous items, where visual semantic
embedding (VSE) [15] is used to capture the intermodal consistency of visual and textual
modalities. This method only considers the consistency between two fashion item modal-
ities and neglects the complementarity between them. Therefore, several researchers have
been attempting to use the fusion strategy (i.e., early fusion and late fusion) to integrate the
multimodal information. (1) Early fusion-based approaches typically fuse the input features
extractedfromeachmodalityintoasinglerepresentationbeforecompatibilitymodeling[16].
For example, Tan et al. [17] fused the visual and textual features of fashion items by the
elementwise product operation, while Yang et al. [18] and Sun et al. [19] directly combined
the visual and textual features of each item by the concatenation operation before feeding the
item feature into the compatibility modeling module. In addition, Laenen et al. [20] used the
attention mechanism to fuse the visual and textual features, and projected the multimodal
representations to the type-speciﬁc compatibility spaces. (2) Late fusion-based methods [7,
21] ﬁrst perform the compatibility modeling directly over each modality feature, and then
linearly combine the estimated outﬁt compatibility scores from different modalities. For
example, Cui et al. [7] introduced the nodewise graph neural network (NGNN) for the
outﬁt compatibility modeling from each modality. The overall outﬁt compatibility score is
derived by a weighted summation of the scores obtained by the two (visual and textual)
modalities. Both early fusion-based and late fusion-based methods overlook the importance
of the intermodal compatibility relation between fashion items in the outﬁt compatibility
modeling, which is the major concern of this work.
It is worth mentioning that some multimodal methods have incorporated the category
information as an indicator to guide the outﬁt compatibility modeling. For example, Vasileva
et al. [3] presented a pairwise outﬁt compatibility modeling scheme, where a category-

30
3
Modality-Oriented Graph Learning for OCM
speciﬁc embedding space is introduced for each pair of categories. Additionally, Wang
et al. [4] learned the overall compatibility from all category-speciﬁed pairwise similarities
betweenfashionitems,andusedbackpropagationgradientstodiagnoseincompatiblefactors.
Differently, as a major novelty, we take the category modality as one essential input modality,
i.e., comparable to the visual and textual modalities, to enhance the outﬁt compatibility
modeling performance with GCN.
3.3
Methodology
In this section, we ﬁrst present the notations and problem formulation and then detail the
proposed the multimodal outﬁt compatibility modeling scheme.
3.3.1
Problem Formulation
Since different modalities (e.g., the visual image, text description, and category) can deliver
different aspects of fashion items, we propose exploring all fashion item modalities to
comprehensively measure the compatibility score of outﬁts. Assume that we have a set
of Q fashion items I = {xi}Q
i=1, coming from Nc categories. Each fashion item xi ∈I is
attached with a visual image, a textual description and a category, termed as fi, ti, and
ci, respectively. Based on these items, we can derive a set of N training outﬁt samples
 = {(O j, y j)| j = 1, . . . , N}, where O j is the jth outﬁt, and y j is the ground-truth label
that indicates whether the outﬁt is compatible. Speciﬁcally, y j = 1 denotes that the jth outﬁt
O j is compatible, and y j = 0 otherwise. Each outﬁt can be regarded a set of fashion items,
i.e., O j =

x j
1, x j
2, . . . , x j
Sj

, where x j
i ∈I denotes the ith composing item of the outﬁt
O j, and Sj represents the total number of fashion items in the outﬁt O j. Notably, since each
outﬁt can be composed of a various number of fashion items, Sj is variable. Based on these
data, we aim to devise a comprehensive multimodal outﬁt compatibility modeling scheme
F, which can integrate the multimodal information of its composing fashion items toward
the accurate outﬁt compatibility estimation. Mathematically, we have:
ˆy j = F({ f j
i , t j
i , c j
i }
Sj
i=1|),
(3.1)
where f j
i , t j
i , and c j
i represent the visual image, textual description, and category of the ith
item of the jth outﬁt, respectively.  is a set of to-be-learned parameters, and ˆy j denotes
the estimated compatibility score of the outﬁt O j. For brevity, we omit the superscript j of
the jth outﬁt O j in the rest of the paper.

3.3
Methodology
31
3.3.2
Multimodal Embedding
First, we resort to the following encoders to learn the visual, textual and category represen-
tation of each fashion item.
Image Encoder. Regarding the visual image of each item, we utilize the CNN to extract
its visual features. It is well known that the CNN comprises multiple convolutional layers,
where the shallow layers can capture the low-level visual features, such as the color of
the item, while the deep layers can capture the high-level features, such as the style of the
item [22]. Since both the low-level and high-level visual features affect the compatibility
among fashion items, similar to the work [4], we consider both the shallow and deep lay-
ers’ outputs to learn the visual representation for each item instead of only using the ﬁnal
layer’s output. In particular, we resort to the global average pooling operation (GAP), which
has shown remarkable performance in the discriminative visual property extraction [23], to
summarize the learned visual representations. Formally, given the image fi of the fashion
item xi, we can obtain its visual feature as follows,
fi =

GAP

Conv1 ( fi)

, . . . , GAP

ConvL ( fi)
	
,
(3.2)
where fi ∈Rd f is the visual feature of the item xi, d f is the dimension of the visual feature,
and [ , ] denotes the concatenation operation. In addition, Convl represents the lth convo-
lutional layer used for visual encoding of CNN, and L is the total number of convolutional
layers.
Text Encoder. To embed the textual description of each fashion item, we adopt the
TextCNN, which has achieved astonishing success in various natural language process-
ing tasks [24, 25]. In particular, we ﬁrst represent the textual description (i.e., a sequence
of words) as a matrix, each column of which refers to a word embedding learned by the
pretrained word2vec. [26]. We then employ the CNN architecture to extract the semantic
information of the text description of the given fashion item. Speciﬁcally, given the textual
description ti of the fashion item xi, we obtain its textual feature as follows,
ti = T extC N N (ti) ,
(3.3)
where ti ∈Rdt denotes the extracted textual feature, and dt is its dimension.
Category Encoder. In addition to the visual and textual information, the category informa-
tion of the composing items also plays an important role in outﬁt compatibility estimation.
Different from previous studies that only incorporate the category information to guide
the outﬁt compatibility modeling, we propose regarding the category as an unique input
modality. To represent the discrete categories, we introduce a category embedding matrix
C ∈RNc×dc =

c1, c2, . . . , cNc

, where Nc is the total number of categories in the dataset,
dc is the category feature dimension, and ck denotes the embedding for the kth category.
Therefore, for each fashion item xi, its category feature ci can be obtained according to its
category information ci.

32
3
Modality-Oriented Graph Learning for OCM
Ultimately, based on the above three encoders, for each fashion item xi, we can obtain
its visual feature fi, textual feature ti, and category feature ci.
3.3.3
Modality-Oriented Graph Learning
Since each outﬁt comprises a set of fashion items with no clear order, we treat each outﬁt as
an item graph and hence resort to the GCNs to explore its outﬁt compatibility. In particular,
for each outﬁt O, we construct an item graph G = (V, E), where V denotes the set of nodes,
each of which represents a composing item, and E denotes the set of edges representing the
compatibility relation among items. We assume that the compatibility between each pair of
items should be considered in the outﬁt compatibility modeling, and thus make the fashion
graph a complete graph. Namely, there is an edge between each pair of nodes (items).
Node Initialization
Different from the conventional methods that assign each node with a single hidden state vec-
tor, we attribute each node with three modality-oriented hidden state vectors, corresponding
the three modalities. Speciﬁcally, for each fashion item xi, we employ linear transformations
to map its multimodal features into a common space to derive the modality-oriented hidden
state vectors as follows,
⎧
⎪⎨
⎪⎩
h0
i,1 = W f fi + b f ,
h0
i,2 = Wtti + bt,
h0
i,3 = Wcci + bc,
(3.4)
where h0
i,1 ∈Rd, h0
i,2 ∈Rd and h0
i,3 ∈Rd are the initial hidden representations of the ith
item’s visual, textual and category modalities, respectively. For ease of the following presen-
tation, without losing generality, we arrange the visual, textual and category modalities as the
ﬁrst, second, and third modalities of fashion items, respectively. W f ∈Rd f ×d, Wt ∈Rdt×d,
and Wc ∈Rdc×d are the linear mapping matrices, b f ∈Rd, bt ∈Rd and bc ∈Rd are the
biases, where d is the dimension of the hidden state representation.
Edge Representation Generation
Previous GCN-based studies [7, 8] on the outﬁt compatibility modeling utilize edges to
indicate the graph topological information and assign each edge with a scalar. Beyond that,
we model the edge between two items with a multidimensional feature rather than a one-
dimensional weight, which can encode the complex compatibility relation between items.
As mentioned above, in addition to the intramodal compatibility, the interaction of different
modalities between fashion items can deliver the compatibility relation between fashion
items. Therefore, we introduce the ﬁne-grained edge representation ei j,pq to capture the
compatibility between the pth modality of node vi and the qth modality of node v j, where
p, q = 1, 2, 3. It is worth noting that (1) when p = q, ei j,pq represents the intramodal
compatibility relation, and (2) when p ̸= q, ei j,pq represents the intermodal compatibility
relation. Regarding the ﬁne-grained edge representation generation, it is worth noting that

3.3
Methodology
33
the order of items in each item pair does not inﬂuence the underlying compatibility relation
Accordingly, in this work, we employ the symmetric elementwise product function to gen-
erate the edge representation. Speciﬁcally, we produce the ﬁne-grained edge representation
ei j,pq for the kth propagation step as follows,
ek
i j,pq = α

Wk
e

Wk
nhk−1
i,p ⊙Wk
nhk−1
j,q

+ bk
e

,
(3.5)
where hk−1
i,p is the hidden representation of the pth modality of the node vi, and hk−1
j,q is the
hidden representation of the qth modality of the node v j. Wk
n ∈Rd×d, Wk
e ∈Rd×de, and
bk
e ∈Rde are the parameters for the edge representation generation in the kth propagation
step. Wk
n is the weight matrix of the linear transformation to project the node embedding
to latent compatibility space, while Wk
e is the weight matrix of the linear transformation to
further compress the latent compatibility relation into a lower-dimensional space, where the
compatibility relation with all the other nodes is aggregated. In particular, to facilitate the
following mean pooling and max pooling-based compatibility aggregation, de = d/2. α (·)
is the ReLU activation function, and ⊙denotes the elementwise product operation.
Intramodal and Intermodal Compatibility Propagation
During the intramodal and intermodal compatibility propagation, we make each modality
of each node absorb the ﬁne-grained compatibility information from its connected edges
to update its hidden state vector. Without losing generality, as an example, we present the
compatibility aggregation process toward the pth modality of the node vi as follows,
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
mk
N (i),pq = AGG

ek
i j,pq, ∀j ∈N (i)

,
mk
N (i),p = 1
M
M

q=1
mk
N (i),pq,
(3.6)
where N (i) is the neighbors of node vi, i.e., the nodes connected to the node vi in the graph.
mk
N (i),pq ∈Rd denotes the aggregated compatibility information from the qth modality of
the node’s neighbors toward the pth modality of the node vi in the kth propagation step, while
mk
N (i),p ∈Rd represents the aggregated compatibility information from all the modalities
of the node’s neighbors toward the pth modality of the node vi in the kth propagation step.
M is the total number of modalities, which is 3 in our context. AGG (·) is the aggregation
function, which is implemented with both the mean and max pooling operations. Speciﬁcally,
we have mk
N (i),pq =

γmean

ek
i j,pq, ∀j ∈N (i)

, γmax

ek
i j,pq, ∀j ∈N (i)
	
,
(3.7)
where γmean (·) and γmax (·) are the mean and max pooling operations, respectively. The
mean and max pooling operations are used for extracting the average and most prominent
information from the connected edges, respectively.

34
3
Modality-Oriented Graph Learning for OCM
Then, we adopt the gated recurrent unit (GRU) [27] to selectively absorb the compatibility
information from the node’s neighbors and the original hidden information of the node.
Speciﬁcally, we deﬁne the modality representation update function for each node as follows,
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
zk
i,p = σ

Wk
z

mk
N (i),p, hk−1
i,p
	
+ bk
z

,
rk
i,p = σ

Wk
r

mk
N (i),p, hk−1
i,p
	
+ bk
r

,
˜hk
i,p = tanh

Wk
h

mk
N (i),p, rk
i,p ⊙hk−1
i,p
	
+ bk
h

,
hk
i,p =

1 −zk
i,p

⊙hk−1
i,p + zk
i,p ⊙˜hk
i,p,
(3.8)
where Wk
z ∈R2d×d, Wk
r ∈R2d×d, and Wk
h ∈R2d×d are weight matrices of the update func-
tion, while bk
z ∈Rd, bk
r ∈Rd and bk
h ∈Rd are biases. zk
i,p and rk
i,p are update gate vector
and reset gate vector, respectively. σ (·) is the sigmoid activation function, and tanh (·) is the
tanh activation function. hk
i,p denotes the hidden representation of the pth modality of item
xi in the kth propagation step. As can be seen, the node update function (i.e., GRU) takes
both the hidden modality representation of each node vi and the aggregated compatibility
information mk
N (i),p as the input. In this manner, the updated representation of the node vi
comprises not only the item intrinsic characteristics but also the compatibility relation with
connected items.
3.3.4
Outfit Compatibility Estimation
After K propagationsteps,weobtainaseriesofmultimodalhiddenrepresentationsoffashion
item xi, namely

h0
i , . . . , hK
i

, where hk
i =

hk
i,1, hk
i,2, hk
i,3
	
, and k = 1, . . . , K. Since the
representations obtained at different propagation layers absorb the neighbor compatibility
information at different levels, toward the comprehensive representation, we concatenate
them to constitute the ﬁnal representation of each fashion item xi as follows,
h∗
i =

h0
i , . . . , hK
i
	
, i = 1, . . . , Sj.
(3.9)
Thereafter, we deﬁne the ﬁnal representation of the outﬁt based on these composing
items’ representations. Notably, instead of using the concatenation of all composing items’
representations, we further apply a pooling layer that includes both max pooling and mean
pooling operations to derive the whole outﬁt representation. We expect that the max pooling
and mean pooling operations can capture the most prominent and the overall features of all
composing items’ hidden states, respectively. Speciﬁcally, we obtain the ﬁnal embedding
for each outﬁt O as follows,
˜h =

γmean

h∗
i , ∀vi ∈V

, γmax

h∗
i , ∀vi ∈V

,
(3.10)

3.4
Experiment
35
where γmean (·) and γmax (·) are the mean and max pooling operations, respectively. Ulti-
mately, an MLP with two layers is empirically chosen as the ﬁnal compatibility estimation,
in which the outﬁt embedding is fed to compute the ﬁnal compatibility score of the outﬁt O
as follows,
ˆy = σ

W2

α

W1 ˜h + b1

+ b2

,
(3.11)
where W1 ∈Rdo×d′, W2 ∈Rd′×1, b1 ∈Rd′ and b2 ∈R1 are the parameters of the MLP,
where do is the dimension of ˜h and d′ is the number of hidden units of the MLP. σ is the
sigmoid active function, used for projecting the estimated compatibility score into the range
of [0, 1], and the estimated compatibility score can be regarded as the probability that the
outﬁt is compatible.
Optimization To optimize the proposed model, we adopt the binary cross-entropy loss,
which shows the great superiority in the classiﬁcation task [28, 29], formally,
Lcl f = −y log

ˆy

−(1 −y) log

1 −ˆy

,
(3.12)
where ˆy and y denote the estimated score and the ground-truth label, respectively. Inspired
by [4], to encourage the CNN to encode normalized representations in the latent space, we
add additional loss to penalize the training process as follows,
Lemb =
S

i=1
∥fi∥2 ,
(3.13)
where S represents the number of fashion items in an outﬁt sample, and ∥·∥2 denotes the
Euclidean norm of a vector. Ultimately, the ﬁnal objective function can be formulated as
follows,
Ltotal =


Lcl f + λ1Lemb

+ λ2 ∥∥2
F ,
(3.14)
where λ1 and λ2 are the tradeoff hyperparameters, controlling the weights for the normal-
ization loss and overﬁtting regularization loss. As previously mentioned,  is the training
set, and  refers to the set of to-be-learned parameters. ∥·∥F denotes the Frobenius norm
of a matrix.
3.4
Experiment
To evaluate the proposed method, we conducted extensive experiments on the real-word
dataset by answering the following research questions:
• RQ1: Does MOCM-MGL achieve better performance than state-of-the-art methods?
• RQ2: How does each component affect the MOCM-MGL?

36
3
Modality-Oriented Graph Learning for OCM
• RQ3: How does each modality inﬂuence the performance?
• RQ4: How about the sensitivity of MOCM-MGL with respect to certain vital hyperpa-
rameters?
3.4.1
Experimental Settings
Dataset and Evaluation Metrics
For evaluation, similar to previous studies, we also adopted the Polyvore Outﬁts dataset,
which has two versions: Polyvore Outﬁts-ND and Polyvore Outﬁts-D. The detailed descrip-
tion of this dataset was given in Chap.2. In this work, we jointly utilized the visual images,
textual descriptions and category information of each fashion item. In total, there are 11
coarse-grained categories and 154 ﬁne-grained categories in the Polyvore Outﬁts dataset.
Based on this dataset, we evaluated different methods on two widely recognized tasks: out-
ﬁt compatibility estimation and ﬁll-in-the-blank (FITB) fashion recommendation, both of
which were detailed in Chap.2. For these two tasks, we used the area under the receiver
operating characteristic curve (AUC) and the accuracy (ACC) as the evaluation metrics,
respectively.
Implementation Details
For the image encoder, we selected ResNet-18 [30] as the backbone network, and used the
output of its ﬁnal 4 convolutional layers (i.e., conv2_x, conv3_x, conv4_x, and conv5_x) to
derive the multilayer visual representation according to Eq.(2). In this case, L = 4 in Eq.(2).
Regarding the text encoder, we ﬁrst employed the pretrained word2vec tool to obtain the
300-D vector for each word, and then fed the concatenation of all word vectors into the
TextCNN. In particular, the TextCNN was equipped with 100 ∗5 ﬁlters in 3 distinct sizes
[2, 3, 4]. Ultimately, we captured a 300-D textual representation for each item. As for the
category encoder, we set the dimension of the category vector as 256. We empirically used the
ﬁne-gained category information. Accordingly, we set the number of category embeddings
Nc as 154.
For the optimization, we employed the adaptive moment estimation method (Adam) [31].
We adopted the grid search strategy to determine the optimal values for the hyperparameters
(i.e., λ1 and λ2) among the values {5er | rϵ −5, . . . , −1}. In addition, the learning rate,
batch size, the number of propagation steps K and the dimension of the hidden state d
for all methods were searched in

1e−3, 5e−4, 1e−4, 5e−5, 1e−5
, [24, 32, 64, 128, 256],
[1, 2, 3, 4, 5] and [16, 32, 64, 128, 256], respectively. The proposed model was ﬁne-tuned
based on training set and validation set for 15 epochs, and the performance on testing set
was reported. We experimentally found that the model achieves the optimal performance
with the initial learning rate is 5e−5 and decays by a factor of 0.5 every 10 epochs, the batch
size of 32, the number of propagation steps K = 4, and the dimension of the hidden state

3.4
Experiment
37
d = 64, respectively. The hyperparameters λ1 and λ2 in the loss function are 5e−3 and 5e−5,
respectively. All experiments are implemented by PyTorch.
3.4.2
Model Comparison
To validate the effectiveness of our MOCM-MGL, we chose the following state-of-the-art
methods as baselines.
• Bi-LSTM [14]: By viewing an outﬁt as a sequence, this method exploits the latent item
interaction by the bidirectional LSTM and utilizes the VSE to capture the intermodal
consistency.
• Concatenation-Visual [11]: This method concatenates the visual features of all fashion
items into a vector and then uses an MLP as the binary classiﬁer to compute the outﬁt
compatibility score.
• Concatenation-All: For a fair comparison, this method concatenates the visual, textual
and category features of all fashion items into a vector and then uses an MLP to estimate
the outﬁt compatibility. The encoders are the same as our proposed model.
• Pooling [1]: This is an early fusion-based method that ﬁrst concatenates the visual, textual
and category features of each fashion item, and then applies the average pooling operation
to aggregate fashion items.
• Type-Aware [3]: This method maps the item pairs into the category-speciﬁc embedding
spaces and estimates the outﬁt compatibility by averaging all distances of the item pairs
in the spaces.
• SCE-Net [17]: Different from Type-Aware, this method learns condition-aware embed-
dings by an item’s characteristics without explicit category supervision. In particular,
this method also uses the early fusion strategy, which integrates the visual and textual
features of fashion items by the elementwise product operation.
• NGNN [7]: This method constructs a subgraph for each outﬁt, where each node rep-
resents a category and edges represent interactions among nodes. In this way, the item
representation can be enhanced by that of the items in the same outﬁt. The outﬁt com-
patibility is jointly modeled from two channels of NGNN, whose inputs are the visual
and textual modalities.
• ABF [20]: This is an attention-based fusion method that utilizes the attention mechanism
to fuse the visual and textual features of fashion items. Since the experiment setting
in [20] is consistent with ours, we directly cited the results.
Notably, all methods use the ResNet-18 as the backbone network for a fair comparison.
Table3.1 shows the performance comparison among different approaches on the Polyvore
Outﬁts and Polyvore Outﬁts-D datasets under different tasks. From this table, the following
observations can be made. (1) MOCM-MGL surpasses all the baselines by a large margin

38
3
Modality-Oriented Graph Learning for OCM
Table 3.1 Performance comparison on Polyvore Outﬁts and Polyvore Outﬁts-D
Method
Polyvore Outﬁts
Polyvore Outﬁts-D
AUC(%)
ACC(%)
AUC(%)
ACC(%)
Bi-LSTM
66.24
38.11
62.72
37.43
Concatenation-Visual
85.21
49.93
78.62
43.05
Concatenation-All
87.61
51.35
80.23
45.14
Pooling
89.09
56.58
83.99
51.37
Type-Aware
87.23
57.78
84.49
55.85
SCE-Net
87.09
57.80
84.22
55.44
NGNN
87.12
51.79
83.61
48.37
ABF†
89.99
61.90
87.48
60.78
MOCM-MGL
93.26
63.26
90.79
61.05
† Indicates the results are cited from [20]
with respect to all metrics, which demonstrates the superiority of our proposed framework.
(2) Concatenation-All outperforms Concatenation-Visual, which veriﬁes the effectiveness of
integrating the multimodal information of fashion items. (3) The performance of SCE-Net is
similar to that of Type-Aware, demonstrating the great potential of learning condition-aware
embeddings by an item characteristics instead of an explicit category indicator. (4) ABF
shows superiority over all multimodal baselines, which reﬂects the superiority of utilizing
the attention mechanism to fuse the multimodal information. And (5) it is unexpected that
the graphwise method NGNN performs worse than the pairwise methods (i.e., Type-Aware
and SCE-Net). The possible reason is that NGNN focuses on propagating category-oriented
fashion compatibility. However, in the context of this study, the negative outﬁt shares the
same item category as the positive outﬁt, which is rarely handled by NGNN.
3.4.3
Ablation Study
To explore the contribution of each component of the proposed model, we introduced fol-
lowing derivatives from the model.
• w/o-MGL: To explore the effect of the proposed modality-oriented graph learning
scheme, we disabled the module by directly concatenating the visual, textual and cate-
gory features of each fashion item obtained by the multimodal embedding module, and
then fed it into the outﬁt compatibility estimation.
• w/o-Inter: To validate the necessity of exploring the intermodal compatibility among
fashion items, we redeﬁned the edge representation between two item nodes as

3.4
Experiment
39
Table 3.2 Ablation study on Polyvore Outﬁts and Polyvore Outﬁts-D datasets
Method
Polyvore Outﬁts
Polyvore Outﬁts-D
AUC(%)
ACC(%)
AUC(%)
ACC(%)
w/o-MGL
92.31
61.84
88.93
58.44
w/o-Inter
92.95
62.87
89.18
59.27
w/o-Edge
92.92
62.75
89.76
59.35
w/o-GRU
92.45
62.41
88.97
58.77
w/o-MultiLayer
93.07
62.53
89.57
58.94
w/o-MeanPool
93.13
62.29
90.07
60.16
w/o-MaxPool
92.72
61.87
89.08
58.12
MOCM-MGL
93.26
63.26
90.79
61.05
ek
i j = α

Wk
e

Wk
nhk−1
i
⊙Wk
nhk−1
j

+ bk
e

, where hk
i =

hk
i,1, hk
i,2, hk
i,3
	
. In this way,
only the intramodal compatibility was considered.
• w/o-Edge: To investigate the importance of edge-based compatibility relation mod-
eling, we removed the edge representation generation unit and directly aggregated
information from the hidden neighbor states, i.e., we changed Eq.(3.6) to mk
N (i),pq =
AGG

hk
j,q, ∀j ∈N (i)

.
• w/o-GRU: To verify whether it is necessary to retain the original hidden information of
the node when updating the node representation, we removed the GRU unit and only
utilized the aggregation information, i.e., we changed Eq.(3.8) to hk
i,p = mk
N (i),p.
• w/o-MultiLayer: To explore the importance of integrating representations obtained at
different propagation layers, we treated the representation obtained at the ﬁnal Kth prop-
agation layer as the updated item representation, i.e., we made h∗
i = hK
i in Eq.(3.9).
• w/o-MeanPool: To validate the function of the mean pooling operation in the outﬁt com-
patibility estimation module, we only employed the max pooling operation to generate
the ﬁnal outﬁt embedding, i.e., we rewrote Eq.(3.10) as ˜h = γmax

h∗
i , ∀vi ∈V

.
• w/o-MaxPool: Similarly, we removed the max pooling operation in the outﬁt compat-
ibility estimation module to learn its effect by making ˜h = γmean

h∗
i , ∀vi ∈V

in
Eq.(3.10).
Table3.2 shows the performance comparison between MOCM-MGL and its derivatives.
From this table, we obtained the following observations:
1. Our model consistently surpasses all derivations across all metrics, demonstrating the
effectiveness of each component in the proposed MOCM-MGL.

40
3
Modality-Oriented Graph Learning for OCM
2. MOCM-MGL demonstrates superiority over w/o-MGL, which implies that the modality-
oriented GCN can propagate the intramodal and intermodal compatibility relation among
fashion items, and therefore boost the expressiveness of item representations.
3. MOCM-MGL outperforms w/o-Inter, implying the necessity of investigating the inter-
modal compatibility among fashion items, to fully explore the ﬁne-grained compatibility
relation among items.
4. MOCM-MGL achieves better performance than w/o-Edge. This conﬁrms the beneﬁt of
the edge-based compatibility relation modeling and the compatibility relation propaga-
tion during the outﬁt compatibility modeling.
5. MOCM-MGL surpasses w/o-GRU, which implies that selectively absorbing the com-
patibility information from the nodes’ neighbors and the original hidden information of
the node can boost the model performance.
6. w/o-MultiLayer performs worse than our MOCM-MGL. This implies that different prop-
agation layers absorb the neighbor compatibility information at different levels, and
contribute to the comprehensive outﬁt compatibility estimation.
7. MOCM-MGL shows superiority over w/o-MaxPool and w/o-MeanPool. This suggests
that both the most prominent and the overall hidden states of fashion items are beneﬁcial
to the outﬁt compatibility modeling. Additionally, we observed that w/o-MeanPool out-
performs w/o-MaxPool, which reﬂects that the max pooling operation is more effective
than the mean pooling operation. This indicates that the most prominent feature of all
composing items’ hidden states, compared with the overall feature, has a greater impact
on the outﬁt compatibility estimation.
3.4.4
Modality Comparison
To investigate the inﬂuence of different modalities (i.e., visual image, textual description, and
category) on the performance, we compared the MOCM-MGL with different modality com-
binations. Notably, due to the concern that the negative outﬁt shares the same item categories
as the positive outﬁt, we did not adopt the method that only utilizes category information
for comparison. In addition, there are two kinds of item categories: coarse-grained cate-
gories and ﬁne-grained categories. Therefore, there are nine modality combinations: Visual,
Visual+Category (coarse), Visual+Category (ﬁne), Textual, Textual+Category (coarse), Tex-
tual+Category (ﬁne), Visual+Textual, All (coarse) and All (ﬁne), where coarse, ﬁne and All
indicate that coarse-grained categories, ﬁne-grained categories and all the three modalities
are used, respectively. Table3.3 shows the performance of our model with the nine different
modality combinations. As can be seen in Table3.3, we observed that (1) Visual outper-
forms Textual. This demonstrates that the visual modality is more effective than the textual
feature for the outﬁt compatibility modeling. (2) Multimodal Visual+Textual achieves bet-
ter performance than single-modal Visual and Textual. This indicates that the visual and
textual modalities of fashion items complement each other in outﬁt compatibility estima-

3.4
Experiment
41
Table 3.3 The performance of our proposed method with different modality combinations
Method
Polyvore Outﬁts
Polyvore Outﬁts-D
AUC(%)
ACC(%)
AUC(%)
ACC(%)
Visual
90.77
57.45
85.95
52.93
Visual+Category (coarse)
90.85
59.60
85.98
54.64
Visual+Category (ﬁne)
91.02
59.67
86.01
54.75
Textual
77.02
40.33
75.62
39.01
Textual+Category(coarse)
78.41
41.66
76.71
40.92
Textual+Category (ﬁne)
79.75
42.11
76.95
41.15
Visual+Textual
92.55
61.05
89.17
57.55
All (coarse)
93.06
62.40
90.01
59.81
All (ﬁne)
93.26
63.26
90.79
61.05
tion. (3) Visual+Textual performs better than Visual+Category and Textual+Category. This
may be attributed to the fact that the visual image and textual description deliver more
content-related features of fashion items than the category information. (4) All surpasses
Visual+Textual, indicating that incorporating the category information as one essential
modality does improve the model performance. (5) the methods with ﬁne-grained cate-
gories perform better than those with coarse-grained categories. This may be due to the
fact that ﬁne-grained categories provide more detailed fashion items information, which
facilitates the outﬁt compatibility modeling.
To gain an intuitive understanding of the impact of the multimodal integration, we showed
several results obtained by MOCM-MGL with different modality combinations (i.e., Visual,
Textual and All) on the FITB task in Fig.3.4. We found that only considering a single modal-
ity of fashion items may lead to incorrect choices. For instance, in the ﬁrst example, Textual
chooses the wrong answer d. This may be due to the fact that the textual description of the
answer d and that of the given gloves share the same color, i.e., “black”. Nevertheless, further
incorporating the visual modality, the method All gives the correct answer c. Regarding the
third example, Visual fails to give the correct answer, while All does. This makes sense, as
the textual description of the ground-truth answer c shares the same pattern with the given
striped hoodie. These examples demonstrate the necessity of incorporating the multimodal
information in outﬁt compatibility modeling.
3.4.5
Hyperparameter Discussion
In this section, we examined how the number of propagation steps K, the number of convo-
lutional layers of ResNet-18 used for visual encoding (i.e., L in Eq.(3.2)) and the number
of composing items affect the performance of our method.

42
3
Modality-Oriented Graph Learning for OCM
Fig. 3.4 Comparison of Visual, Textual and All on the FITB task. The black, green and red bold fonts represent the category information of fashion
items, true chosen, and false chosen, respectively. The items highlighted in the green boxes are the ground truth

3.4
Experiment
43
Fig.3.5 Effect of the number of propagation steps, i.e., K, on Polyvore Outﬁts and Polyvore Outﬁts-D
datasets
Fig. 3.6 Effect of the number of convolutional layers of ResNet-18 i.e., L, on Polyvore Outﬁts and
Polyvore Outﬁts-D datasets
To explore the impact of the number of propagation steps, we evaluated our model’s
performance on two tasks with two datasets by changing K from 1 to 5 with a step of 1. As
shown in Fig.3.5, our model achieves the optimal performance when K is 4. This suggests
that it is necessary to propagate several runs so that the fashion items can absorb the neighbor
compatibility information thoroughly at different levels. Moreover, when K is higher than
4, the performance drops. One possible reason is that superﬂuous information propagation
might introduce more noise into the node representations, therefore, leading to a negative
effect.
We then studied the inﬂuence of the number of convolutional layers of ResNet-18 used
for visual encoding, i.e., L in Eq.(3.2), on the model performance. In particular, we varied
the number of convolutional layers used for visual encoding from 1 to 4. Speciﬁcally, L = 1
indicates that we only used the output of the ﬁnal layer conv5_x of ResNet-18 for visual
encoding, while L = 2 indicates that we used the output of the ﬁnal two layers (i.e., conv4_x,
and conv5_x) of ResNet-18. The cases of L = 3 and L = 4 can be similarly derived. In a
sense, the larger the L, the shallower the layers’ output that is incorporated. Figure3.6 shows
the performance of our model on the two tasks with the two datasets. As can be seen in

44
3
Modality-Oriented Graph Learning for OCM
Fig.3.7 Performance of our proposed method regarding outﬁts with different number of composing
items on Polyvore Outﬁts-ND and Polyvore Outﬁts-D datasets
Fig.3.6, the model’s performance grows with integrating more convolutional layers’ output,
which indicates that each convolutional layer contributes to boosting the visual encoding.
The possible reason is that the shallow layers can capture the low-level visual features of
the item, while the deep layers can capture the high-level features, both of which beneﬁt
the visual encoding of fashion items and hence boost the outﬁt compatibility estimation
performance.
To gain deeper insights, we examined the performance of our proposed model regarding
outﬁts with different numbers of composing items. In particular, the testing set is divided
according to the number of fashion items, ranging from 2 to 8. Figure3.7 shows the per-
formance of our proposed method with different testing conﬁgurations. As can be seen, our
method performs well in all settings, verifying the effectiveness of our method to handle out-
ﬁt compatibility modeling with different composing item numbers. In addition, our method
performs better for outﬁts with multiple (i.e., more than 2) fashion items compared to those
with two fashion items. This may be because the beneﬁt of modeling the comparability
between two items with a graph is limited.
3.5
Summary
In this chapter, we presented a multimodal outﬁt compatibility modeling scheme with
modality-oriented graph learning, named MOCM-MGL, which fully exploits the visual,
textual, and category modalities with GCN. Different from previous work, we treat the cat-
egory information of fashion items as a unique and comparable modality to the visual and
textual modalities. In addition, the proposed MOCM-MGL jointly uniﬁes the intramodal
and intermodal compatibility relation among fashion items. Extensive experiments were

References
45
conducted on the Polyvore Outﬁts-ND and Polyvore Outﬁts-D datasets. The experimental
resultsdemonstratedthesuperiorityofMOCM-MGL,suggestingthatemployingamodality-
oriented GCN to propagate the intramodal and intermodal compatibility relation among
fashion items is helpful to boost the model performance. In addition, integrating the mul-
timodal information of fashion items greatly improves the outﬁt compatibility estimation
performance.
References
1. Li, Yuncheng, Liangliang Cao, Jiang Zhu, and Jiebo Luo. 2017. Mining Fashion Outﬁt Com-
position Using an End-to-End Deep Learning Approach on Set Data. IEEE Transactions on
Multimedia 19 (8): 1946–1955.
2. Dong, Xue, Jianlong Wu, Xuemeng Song, Hongjun Dai, and Liqiang Nie. 2020. Fashion Compat-
ibility Modeling through a Multi-modal Try-on-guided Scheme. In Proceedings of the Interna-
tional ACM SIGIR Conference on Research and Development in Information Retrieval, 771–780.
ACM.
3. Vasileva, Mariya I., Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, and
David A. Forsyth. 2018. Learning Type-Aware Embeddings for Fashion Compatibility. In Euro-
pean Conference on Computer Vision, 405–421. Springer.
4. Wang, Xin, Bo Wu, and Yueqi Zhong. 2019. Outﬁt Compatibility Prediction and Diagnosis with
Multi-layered Comparison Network. In Proceedings of the International ACM Conference on
Multimedia, 329–337. ACM.
5. Lei Zhu, Xu Lu, Zhiyong Cheng, Jingjing Li, and Huaxiang Zhang. 2020. Deep Collaborative
Multi-view Hashing for Large-scale Image Search. IEEE Transactions on Image Processing 29
(2020): 4643–4655.
6. Lu, Xu, Lei Zhu, Zhiyong Cheng, Liqiang Nie, and Huaxiang Zhang. 2019. Online Multi-
modal Hashing with Dynamic Query-adaption. In Proceedings of the International ACM SIGIR
Conference on Research and Development in Information Retrieval, 715–724. ACM.
7. Cui, Zeyu, Zekun Li, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Dressing as a Whole:
Outﬁt Compatibility Learning Based on Node-wise Graph Neural Networks. In Porceedings of
the World Wide Web Conference, 307–317. ACM.
8. Li, Xingchen, Xiang Wang, Xiangnan He, Long Chen, Jun Xiao, and Tat-Seng Chua. 2020.
HierarchicalFashionGraphNetworkforPersonalizedOutﬁtRecommendation.InProceedingsof
theinternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,
159–168. ACM.
9. Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classiﬁcation. In Proceedings of
the Conference on Empirical Methods in Natural Language Processing, 1746–1751. ACL.
10. Gardner, Matt W., and S.R. Dorling. 1998. Artiﬁcial Neural Networks (The Multilayer
Perceptron)-A Review of Applications in the Atmospheric Sciences. Atmospheric Environment
32 (14–15): 2627–2636.
11. Tangseng, Pongsate, Kota Yamaguchi, and Takayuki Okatani. 2018. Recommending Outﬁts from
Personal Closet. In IEEE Winter Conference on Applications of Computer Vision, 269–277. IEEE.
12. Cucurull, Guillem, Perouz Taslakian, and David Vázquez. 2019. Context-Aware Visual Com-
patibility Prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 12617–12626. IEEE.

46
3
Modality-Oriented Graph Learning for OCM
13. Chaidaroon, Suthee, Yi Fang, Min Xie, and Alessandro Magnani. 2019. Neural Compatibility
Ranking for Text-based Fashion Matching. In Proceedings of the International ACM SIGIR
Conference on Research and Development in Information Retrieval, 1229–1232. ACM.
14. Han, Xintong, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis. 2017. Learning Fashion Com-
patibility with Bidirectional LSTMs. In Proceedings of the ACM International Conference on
Multimedia, 1078–1086. ACM.
15. Kiros, Ryan, Ruslan Salakhutdinov, and Richard S Zemel. 2014. Unifying Visual-semantic
Embeddings with Multimodal Neural Language Models. arXiv:1411.2539.
16. Wei, Yinwei, Xiang Wang, Weili Guan, Liqiang Nie, Zhouchen Lin, and Baoquan Chen. 2020.
Neural Multimodal Cooperative Learning Toward Micro-video Understanding. IEEE Transac-
tions on Image Processing 29: 1–14.
17. Tan, Reuben, Mariya I. Vasileva, Kate Saenko, and Bryan A. Plummer. 2019. Learning Similarity
Conditions Without Explicit Supervision. In Proceedings of the IEEE International Conference
on Computer Vision, 10372–10381. IEEE.
18. Yang, Xun, Yunshan Ma, Lizi Liao, Meng Wang, and Tat-Seng Chua. 2019. TransNFCM:
Translation-Based Neural Fashion Compatibility Modeling. In AAAI Conference on Artiﬁcial
Intelligence, 403–410. AAAI Press.
19. Sun, Guang-Lu, Jun-Yan. He, Wu. Xiao, Bo. Zhao, and Qiang Peng. 2020. Learning Fashion
Compatibility Across Categories with Deep Multimodal Neural Networks. Neurocomputing 395:
237–246.
20. Laenen, Katrien, and Marie-Francine. Moens. 2020. A Comparative Study of Outﬁt Recommen-
dation Methods with a Focus on Attention-based Fusion. Information Processing and Manage-
ment 57 (6): 102316.
21. Lu, Zhi, Yang Hu, Yunchao Jiang, Yan Chen, and Bing Zeng. 2019. Learning Binary Code for
Personalized Fashion Recommendation. In IEEE Conference on Computer Vision and Pattern
Recognition, 10562–10570. IEEE.
22. Kollias, Dimitrios, and Stefanos P. Zafeiriou. 2020. Exploiting Multi-CNN Features in CNN-
RNN based Dimensional Emotion Recognition on the OMG in-the-wild Dataset. IEEE Trans-
actions on Affective Computing, 1–12.
23. Yang, Xin, Xuemeng Song, Xianjing Han, Haokun Wen, Jie Nie, and Liqiang Nie. 2020. Gener-
ative Attribute Manipulation Scheme for Flexible Fashion Search. In Proceedings of the Interna-
tional ACM SIGIR Conference on Research and Development in Information Retrieval, 941–950.
ACM.
24. Severyn, Aliaksei, and Alessandro Moschitti. 2015. Twitter Sentiment Analysis with Deep Con-
volutional Neural Networks. In Proceedings of the International ACM SIGIR Conference on
Research and Development in Information Retrieval, 959–962. ACM.
25. Guo, Bao, Chunxia Zhang, Junmin Liu, and Xiaoyi Ma. 2019. Improving Text Classiﬁcation
with Weighted Word Embeddings via a Multi-channel TextCNN Model. Neurocomputing 363
(21): 366–374.
26. Mikolov, Tomás, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient Estimation of Word
Representations in Vector Space. In Proceedings of the International Conference on Learning
Representations, 1–12. OpenReview.net.
27. Li, Yujia, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated Graph
Sequence Neural Networks. In International Conference on Learning Representations, 15–25.
OpenReview.net.
28. Guo, Sheng, Weilin Huang, Xiao Zhang, Prasanna Srikhanta, Yin Cui, Yuan Li, Hartwig Adam,
Matthew R. Scott, and Serge Belongie. 2019. The iMaterialist Fashion Attribute Dataset. In
Proceedings of International Conference on Computer Vision Workshops, 3113–3116. IEEE.

References
47
29. Inoue, Naoto, Edgar Simo-Serra, Toshihiko Yamasaki, and Hiroshi Ishikawa. 2017. Multi-Label
Fashion Image Classiﬁcation With Minimal Human Supervision. In Proceeding of IEEE Inter-
national Conference on Computer Vision Workshops, 2261–2267. IEEE.
30. He,Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for
Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 770–778. IEEE Computer Society.
31. Kingma, Diederik P., and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Pro-
ceedings of the International Conference on Learning Representations, 1–15. OpenReview.net.

4
Unsupervised Disentangled Graph Learning for
OCM
4.1
Introduction
In Chaps.2 and 3, we proposed two methods for outﬁt compatibility modeling. They still
suffer from two key limitations. (1) They evaluate the outﬁt compatibility based on the
single latent compatibility space. The outﬁt compatibility is essentially affected by multiple
complementary hidden factors, such as the color, style, shape, and material. Therefore,
we argue that previous methods can only achieve the suboptimal solution, as it entangles
all the factors in a single latent space. (2) They focus on learning the representation of
each composing item and based on that, calculate the outﬁt compatibility. We argue that
this method still fails to authentically treat the outﬁt as a whole, namely, it overlooks the
global outﬁt representation learning. Therefore, in this chapter, we aim to estimate the
compatibility of the outﬁt by considering the multiple hidden spaces and the global outﬁt
graph representation learning.
However, this is a nontrivial task due to the following challenges. (1) The key to outﬁt
compatibility modeling is to learn the global outﬁt representation that encodes the outﬁt’s
compatibility. As the global outﬁt representation cannot be discussed without the local item
representation learning, how to derive the accurate item representation that compiles its
compatibility to all the other items poses the ﬁrst challenge for us. (2) Since each outﬁt
involves a variable number of composing items, and different items contribute to the outﬁt
differently, how to adaptively learn the global outﬁt representation based on the item rep-
resentation is a crucial challenge. (3) The hidden factors complementarily characterize the
outﬁt compatibility, such as the color-oriented, material-oriented, and style-oriented com-
patibility. Therefore, how to model the complementarity of these hidden factors and boost
the outﬁt compatibility modeling constitutes another difﬁcult challenge.
To address these challenges, we devise a novel outﬁt compatibility modeling scheme,
termed OCM-CF. As shown in Fig.4.1, OCM-CF contains two essential components:
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
W. Guan et al., Graph Learning for Fashion Compatibility Modeling,
Synthesis Lectures on Information Concepts, Retrieval, and Services,
https://doi.org/10.1007/978-3-031-18817-6_4
49

50
4
Unsupervised Disentangled Graph Learning for OCM
Score 1
Score 2
Score K
Concatenation
Multi-head Attention
*
*
*
*
*
Item Weight
Context-aware Outfit Representation Learning (CORL)
Item-Item Relationship Propagation (IRP)
Adaptive Relationship Derivation
L = 1
Concatenation
MLP
0 1 1 0
Adaptive
Relationship
Derivation
Relationship
Aggregation
Prediction Compatibility
...
Hidden Complementary Factors Learning
CORL 1
CORL 2
CORL K
...
...
Transformation
Transformation
Transformation
CNN
IRP
IRP
p
Outfit
Representation
L = 2
L = 3
 
 
 
 
Gate Mask
Sigmoid Function
LeakyReLU Function
Element-wise Product
Element-wise Add
Fig. 4.1 Illustration of the proposed OCM-CF. Left: the overall scheme that employs a set of K parallel branches for the hidden complementary
factors learning, where each branch corresponds to a factor-oriented context-aware outﬁt representation learning (CORL). Right: the detailed CORL
component, and its adaptive item-item relationship propagation module

4.3
Methodology
51
context-aware outﬁt representation modeling and hidden complementary factors modeling.
Speciﬁcally, the context-aware outﬁt representation modeling focuses on learning the global
representation of the outﬁt. In particular, we adopt graph convolutional networks (GCNs)
to ﬂexibly support the compatibility modeling for the outﬁt with an arbitrary number of
fashion items. During information propagation, different from existing studies that only
propagate the item embedding, we focus on propagating the item-item relationship and pro-
pose an adaptive item-item relationship propagation module based on the gate mechanism.
In addition, to derive the global outﬁt representation, we employ the multihead attention
mechanism to encourage the global outﬁt representation to fully incorporate the context
information of each fashion item. Pertaining to the hidden complementary factors model-
ing, we introduce a few parallel branches, each of which is deployed with the network of the
ﬁrst component, i.e., context-aware outﬁt representation modeling, and works on exploring
the outﬁt compatibility on one exclusive complementary hidden factor. To encourage each
branch to concentrate on learning one aspect and make the whole scheme comprehensive,
we introduce the orthogonality-based complementarity regularization to avoid the factor
homogenization.
4.2
Related Work
Due to the remarkable capability of dealing with the unstructured data, like a graph,
GNNs have been adopted in many research domains, such as the node classiﬁcation [1,
2], image retrieval [3, 4], and personalized recommendation [5, 6]. Initially, Gori et al. [7]
proposed GNNs to model a set of items and their relationship. Later, GCNs [1, 8] were
devised to introduce the convolution operation into the graph domain by updating each
node’s representation via aggregating information from its neighbor nodes. To improve the
model generalization ability, Velickovic et al. [9] devised a graph attention network, which
assigns different importance to different neighbor nodes during graph propagation, while
Hamilton et al. [10] proposed a general inductive framework that can leverage node features
to efﬁciently generate node embeddings for unseen data by learning aggregator functions.
Inspired by the success of these studies, in this work, we employed GCNs to support the
compatibility modeling for the outﬁt with a variable length, where we developed an adap-
tive item-item relationship propagation module based on the gate mechanism to promote
the outﬁt compatibility modeling performance.
4.3
Methodology
In this section, we ﬁrst formally deﬁne the research task and then detail the proposed
OCM-CF.

52
4
Unsupervised Disentangled Graph Learning for OCM
4.3.1
Problem Formulation
Formally, assume we have a set of positive (well-composed) outﬁts S = {s1, s2, . . . , sT }
and a set of fashion items X. Each outﬁt is associated with a set of m fashion items, denoted
as s = {x1, x2, . . . , xm}, where x j is the jth item of the outﬁt. Notably, m is a variable, which
differs for different outﬁts. Each item x j has a product image denoted as I j and a category
metadata denoted as Cv ∈C, v ∈{1, 2, . . . , Nc}, where C = {C1, C2, . . . , CNc} refers to the
whole set of Nc categories used for organizing all the fashion items.
In this work, we aim to devise an outﬁt compatibility modeling network F, which can
assess the overall compatibility score of a given outﬁt s as follows,
ˆy = F({x j}m
j=1|F),
(4.1)
where ˆy denotes the estimated compatibility score of the given outﬁt and F is a set of
to-be-learned model parameters.
4.3.2
Context-Aware Outfit Representation Learning
We argue that the essence of outﬁt compatibility modeling is to learn a precise outﬁt rep-
resentation that captures the compatibility among all its composition items. Due to the
remarkable performance of GCNs in unstructured data representation learning, we employ
GCNs to handle the outﬁt representation learning.
Item Visual Embedding. To begin, we ﬁrst extract the image feature via the convolutional
neural network (CNN) model, which can be deﬁned as follows,
f j = CNN(I j; cnn),
(4.2)
where f j ∈Rd denotes the image embedding of the item x j, d is the embedding size, and
cnn refers to the parameters of the CNN model. Speciﬁcally, following [11, 12], we adopt a
18-layer deep residual network [13] pretrained on ImageNet [14]. To alleviate the overﬁtting,
we use the L2 regularization on the learned image embedding [15], as follows,
L2(s) =
m

j=1
∥f j∥2.
(4.3)
Outﬁt Graph Construction. Formally, the graph for the outﬁt s can be deﬁned as G =
(V, E), where V = {v1, v2, . . . , vm} refers to the set of item nodes, while E = {(vi, v j, ei j)|
i ̸= j, vi ∈V, v j ∈V} denotes the set of edges linking these item nodes. The triplet
(vi, v j, ei j) denotes the edge from the node vi to node v j weighted by ei j. Regarding
the node representation initialization, since the visual cue is essential for the compatibil-
ity reasoning, we initialize each node embedding, denoted as v0
j, j = 1, 2, . . . , m, with the

4.3
Methodology
53
corresponding item’s visual embedding, i.e., v0
j = f j. Pertaining to the edge weight, instead
of setting all the edge weights as the constant, we resort to the category co-occurrence
probability, due to the concern that an item should address those items whose categories
frequently co-occurred with its own category to attentively absorb the neighbors’ informa-
tion. For example, according to the category occurrence derived from our dataset, a T-shirt
should attend to the pants more as compared to the pair of glasses in the same outﬁt.
Thus, we introduce the category correlation matrix M ∈RNc×Nc in a data-driven manner,
which is deﬁned as follows,
⎧
⎪⎪⎨
⎪⎪⎩
P(Cu|Cv) = n1(Cu, Cv)
n2(Cv)
,
Muv =
P(Cu|Cv)
Nc
k=1 P(Cu|Ck)
,
(4.4)
where P(Cu|Cv) denotes the occurrence probability of category Cu given category Cv.
n1(Cu, Cv) is the function for counting the concurrence times of categories Cu and Cv in
the training dataset, and n2(Cv) is that for counting the occurrence times of category Cv
in the training dataset. Assume that items xi and x j belong to the categories Cu and Cv,
respectively. Then we deﬁne the weight for the edge from xi to x j as,
ei j = Muv.
(4.5)
Item-Item Relationship Propagation (IRP). Different from existing work [16, 17] that
propagates the pure neighbor items’ embedding over the item graph, we propose to propagate
the item-item relationship embedding, a.k.a., adaptive relationship derivation, which plays
a pivotal role in outﬁt compatibility modeling. Additionally, we argue that the high-order
connectivities are beneﬁcial to synthesizing a richer node representation [5, 6], and thus
stack L propagation layers to exploit the item-item relationship. Speciﬁcally, we deﬁne the
relationship embedding between the item i and j regarding the lth propagation layer as
ql
i j = vl
i ⊗vl
j, where ⊗denotes the element-wise product operation, and l = 1, 2, . . . , L.
Regarding the item-item relationship propagation, we argue that different dimensions
of the relationship embedding may contribute differently to the compatibility modeling.
Accordingly, we introduce the gate mechanism to adaptively propagate the item-item rela-
tionship. In particular, the gate function is deﬁned as follows,
rl
i j = σ

Wl
1δ

Wl
2(vl
i||vl
j) + bl
2
	
+ bl
1
	
,
(4.6)
where rl
i j ∈Rd is the gate mask for the item pair (xi, x j) in the lth propagation layer, ∥is the
concatenation operation, σ(·) and δ(·) are Sigmoid and LeakyReLU [18] activate functions,
respectively. Wl
1 ∈Rd×d, Wl
2 ∈Rd×2d, bl
1 ∈Rd, and bl
2 ∈Rd are trainable parameters of
the fully connected layers for the lth order relationship propagation. Based upon the gate
function, we formulate the ﬁnal item-item relationship as gl
i j = δ(rl
i j ⊗ql
i j).

54
4
Unsupervised Disentangled Graph Learning for OCM
Thereafter, we aggregate all the neighbor relationships to reﬁne the ego item represen-
tation. Mathematically, the item-item relationship propagation for item j in the lth order
propagation can be formulated as,
v(l+1)
j
= δ

Wl
3

fl
j +

i∈N j
ei jgl
i j
	
+ bl
3
	
,
(4.7)
where N j is the set of neighbor nodes of the node x j. Wl
3 and bl
3 are learnable parameters
for node information aggregation in the lth propagation layer. Finally, to avoid the infor-
mation loss during the item-item relationship propagation, we incorporate the initial visual
embedding to deﬁne the ﬁnal item embedding as follows,
ˆv j = v0
j∥vL
j ,
(4.8)
where ˆv j ∈R2d is the ﬁnal representation of the item x j.
To encourage the gate mask to ﬁlter the discriminative dimensions of the relationship,
we introduce the L1 regularization to enhance the sparsity of gate masks as follows,
L1(s) =
L

l=1
m

i=1
m

j=1, j̸=i
∥rl
i j∥1.
(4.9)
Global Outﬁt Representation.
Different from existing graph-based compatibility modeling methods [17, 19] that focus on
learning the individual compatibility of each item for the outﬁt based on the local item rep-
resentation learning, we directly target the global outﬁt representation learning. To discrim-
inate the importance of different items in characterizing the outﬁt, we adopt the multihead
self-attention mechanism [20] to summarize the outﬁt representation from the set of item
representations.
For simplicity, we pack all item embeddings into a matrix 
VF = [ˆv1; ˆv2; . . . ; ˆvm] ∈
Rm×2d. Assume we have h attention heads, and the self-attention function of the ith attention
head can be formulated as follows,
⎧
⎪⎨
⎪⎩
Sa
i = sof tmax(QiKT
i
√dk
),
Hi = Sa
i Zi,
(4.10)
where Qi = 
VFWQ
i , Ki = 
VFWK
i , and Zi = 
VFWZ
i refer to the query, key and value
matrices, respectively, while WQ
i ∈R2d×dq, WK
i ∈R2d×dk, and WZ
i ∈R2d×dz are the cor-
responding trainable linear projection matrices. dq, dk, and dz are the dimensions of the
latent space, where dq = dk = dz = 2d
h . The sof tmax operation is performed for each row.
Sa
i ∈Rm×m denotes the attention weight matrix of the ith head, the ( j, k)th entity of which

4.3
Methodology
55
reﬂects the importance of the kth item to the jth item. Hi ∈Rm×dz is the output of ith head,
with each row referring to an item local feature.
Based upon the attention weight matrices derived from the h heads, we deﬁne the impor-
tance of the ith item as the summation of its importance to all the items in the outﬁt. Formally,
we have,
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
ri =
m

j=1
S( j, i), i = 1, 2, . . . , m,
˜ri =
exp(ri)
m
k=1 exp(rk),
(4.11)
where S = 1
h
h
i=1 Sa
i , and ˜ri is the normalized importance of the item i to characterize the
outﬁt. Finally, we derive the outﬁt representation as follows,
p =
m

i=1
˜riHF(i, :),
(4.12)
whereHF = [H1, H2, . . . , Hh] ∈Rm×2d,andHF(i, :)istheithrowofHF,representingthe
ith item representation. Finally, our proposed context-aware outﬁt representation learning
network can be summarized as,
Fout(G) = p,
(4.13)
where G is the item graph of outﬁt s.
4.3.3
Hidden Complementary Factors Learning
As previously stated, each outﬁt’s compatibility can be affected by multiple complementary
hidden factors, like the color, style, shape, and material. Accordingly, in this part, we propose
the hidden complementary factors learning method. In particular, we ﬁrst project each outﬁt
into multiple complementary factor subspaces, in which the factor-oriented compatibility
can be modeled.
Thus, for each outﬁt s, we introduce K parallel branches, denoted as B1, . . . , BK , where
each branch comprises a network for CORL, and focuses on one hidden factor-oriented outﬁt
compatibility reasoning. Speciﬁcally, each branch Bk, k = 1, 2, . . . , K can be formulated
as,
pk = Fk
out(Gk),
(4.14)
where pk ∈R2d denotes the global outﬁt representation pertaining to the kth hidden factor.
Gk = {Vk, Ek} is the outﬁt graph designed for the kth branch, where Ek = E, namely, all
the branches share the same graph structure. To facilitate the hidden factor learning, we
initializethenoderepresentationsofdifferentbrancheswithdifferenthiddenrepresentations.
Speciﬁcally, we have,

56
4
Unsupervised Disentangled Graph Learning for OCM
fk
j = Wk
bf j, k = 1, 2, . . . , K,
(4.15)
where f j is the initial visual embedding of item j, deﬁned in Eq.(4.2). Wk
b ∈Rd×d is the
weight matrix for transforming the visual embedding of each item into the kth hidden factor
space. fk
j ∈Rd denotes the kth hidden factor-oriented item representation.
It is worth noting that with no constraint, the learned hidden factors tend to present the
homogenization, resulting in redundant compatibility reasoning of different branches [21].
To encourage different branches to model different hidden factors, we introduce the
orthogonality-based complementarity regularization. Formally, we have the following objec-
tive function,
Lcom(s) =
m

j=1
∥F jFT
j −I∥2
F,
(4.16)
where I ∈RK×K is the identity matrix. F j = [f1
j ; f2
j ; . . . ; f K
j ] ∈RK×d denotes different
factor embeddings of the fashion item j, and || · ||F denotes the Frobenius norm of the
matrix.
4.3.4
Outfit Compatibility Modeling
Based on the hidden factor-oriented outﬁt representations, i.e., p1, p2, . . . , pK , we employ
a linear transformation to obtain the compatibility score ˆy for the given outﬁt s as follows,
ˆy =
K

k=1
Wk
spk,
(4.17)
where Wk
s ∈R1×2d is the weight matrix for the branch Bk.
Similar to existing methods [17, 22], to exploit the implicit compatibility preference
among fashion items, we also adopt the Bayesian personalized ranking (BPR) [23] loss,
which encourages the score of the positive outﬁt higher than that of the negative outﬁt.
Accordingly, we ﬁrst build the following training set D = {(s+, s−)}, where s+ and s−
denote the positive and negative outﬁt samples, respectively. s+ is directly sampled from
the positive outﬁt set S, while s−is strategically sampled. The sampling details are given
in the experiment section. For each training pair (s+, s−), we have the following objective
function,
Lbpr(s+, s−) = −ln σ( ˆys+ −ˆys−).
(4.18)
Then the ultimate training loss can be deﬁned as follows,
min
F
L =

(s+,s−)∈D
Lbpr(s+, s−) + λ1
Lcom(s+) + Lcom(s−)

+λ2
L1(s+) + L1(s−)

+ λ3
L2(s+) + L2(s−)

,
(4.19)

4.4
Experiment
57
where λ1, λ2, and λ3 are nonnegative tradeoff hyperparameters and F refers to the set of
parameters (i.e., cnn, Wl
1, Wl
2, Wl
3, bl
1, bl
2, bl
3, WQ
i , WK
i , WZ
i , Wk
b and Wk
s) of the model.
4.4
Experiment
To evaluate the proposed method, we conducted extensive experiments on the two real-
world datasets Polyvore Outﬁts and Polyvore Outﬁts-D via answering the following research
questions:
• RQ1: Does OCM-CF surpass the state-of-the-art methods?
• RQ2: How does each component affect our OCM-CF?
• RQ3: What is the qualitative performance of OCM-CF?
4.4.1
Experimental Settings
Dataset and Evaluation Metrics
Similar to our previous studies introduced in the previous two chapters, we also adopted the
Polyvore Outﬁts dataset, which has two versions: Polyvore Outﬁts and Polyvore Outﬁts-D,
for evaluation. The detailed description of this dataset was given in Chap.2. In this work,
we jointly utilized the visual images, textual descriptions and category information of each
fashion item. In total, there are 11 coarse-grained categories and 154 ﬁne-grained categories
in the Polyvore Outﬁts dataset.
Based upon this dataset, we evaluated different methods on two widely recognized tasks:
outﬁt compatibility estimation and ﬁll-in-the-blank (FITB) fashion recommendation, both
of which were detailed in Chap.2. For these two tasks, we used the area under the receiver
operating characteristic curve (AUC) and the accuracy (ACC) as the evaluation metrics,
respectively.
Implementation Details
Negative Outﬁt Composition. Regarding the training dataset construction, we set the ratio
of positive and negative samples to 1 : 1. Considering that human cognitive learning is an
easy-to-hard process, analogically, the model ﬁrst learned from the easy cases, and hence
adopted the following three methods to compose a negative outﬁt s−for each positive outﬁt
s+: (1) Method 1: randomly sample |s+| items from X without any restriction; (2) Method 2:
randomly sample |s+| items from X according to the item categories of s+; and (3) Method
3: randomly choose one item of the positive outﬁt and replace it with a randomly sampled
item of the same category. Intuitively, in the ﬁrst few epochs, we used Method 1 to derive
the negative samples, then Method 2, followed by Method 3 in the last few epochs.
Experimental Setting. In Polyvore dataset, each fashion item is assigned with both the
coarse-grained category, such as top, and the ﬁne-grained category, such as t-shirt. Due to the

58
4
Unsupervised Disentangled Graph Learning for OCM
concern of the highly imbalanced data distribution with hundreds of ﬁne-grained categories,
which may degrade the model generalization performance, we resorted to the coarse-grained
category metadata to derive the edge weight between every two items. The Adam optimizer
was employed with minibatch size 64 and embedding size d = 64. The learning rate was set
as 5e−5 with the exponential decay 0.985 of each epoch. We empirically set L = 3 as the
propagation layers, and we stacked 3 layers of multihead self-attentions with h = 8 heads.
In the hidden complementary factors learning module, the number of branches K was set to
5. We set λ2 = 5e−4 and other λ parameters in Eq.(4.19) to 5e−3. The proposed model was
trained for 80 epochs, and the performance was reported on the test dataset. Notably, during
the training, we used two thresholds regarding the epoch number to switch the negative outﬁt
composition method as 10 and 40. We only used the image signal in all experiments for fair
comparisons.
Evaluation Tasks and Metrics. We evaluated our proposed OCM-CF by conducting
experiments on three popular tasks: the outﬁt compatibility prediction [12, 24], ﬁll-in-the-
blank[11,17],andcomplementaryfashionitemretrieval[22,25].(1)Theoutﬁtcompatibility
prediction task evaluates the compatibility score of a given outﬁt that contains an arbitrary
number of fashion items. Following existing studies [12, 24], we adopted the AUC (area
under the ROC curve) [26] as the evaluation metric. (2) The task of ﬁll-in-the-blank (FITB)
selects the most compatible item from a set of item candidates for an incomplete query outﬁt.
To prepare the data, for each positive/compatible outﬁt, we randomly selected an item as
the target item and set the remaining items of the outﬁt as the query. Then we composed the
target item with three other randomly selected items of the same category with the target
item from the dataset as the candidate item choices. To handle the task, we composed each
candidate item with query items as an outﬁt and used the well-trained model to compute
each outﬁt’s compatibility score. Based on that, we chose the item with the highest score as
the answer, and used the accuracy as the evaluation metric. (3) The task of complementary
fashion item retrieval can be seen as an extension of the FITB task. Speciﬁcally, we extended
the size of the candidate item set to 500, where there was only one positive (target) item and
499 negative items of the same category. We adopted the hit rate (HR) at 5, 10 and 40 to
evaluate the model performance.
4.4.2
Model Comparison
To validate the effectiveness of our proposed method, we compared it with the following
state-of-the-art methods, including the pair-based, sequence-based, and graph-based models.
• Bi-LSTM [24] permutes all items of an outﬁt into a predeﬁned order according to the item
category, and cast the outﬁt compatibility modeling as a sequence prediction problem,
where bidirectional LSTMs are used. For fairness, we removed the text information from
the model.

4.4
Experiment
59
Table 4.1 Performance comparison among different methods on three tasks. Our results are high-
lighted in bold
Method
Polyvore Outﬁts
Polyvore Outﬁts-D
Polyvore Outﬁts
Polyvore Outﬁts-D
Compat.
AUC
FITB
ACC
Compat.
AUC
FITB
Acc
HR@5
HR@10
HR@40
HR@5
HR@10
HR@40
Bi-LSTM
0.68
42.20%
0.65
40.10%
0.032
0.076
0.244
0.052
0.088
0.249
SCE-NET
0.83
52.80%
0.82
52.10%
0.079
0.143
0.340
0.076
0.129
0.334
Type-aware
0.87
56.60%
0.78
47.30%
0.108
0.165
0.372
0.040
0.072
0.236
NGNN
0.75
53.02%
0.68
42.49%
0.084
0.136
0.341
0.033
0.068
0.219
Context-aware
0.81
55.63%
0.77
50.34%
0.106
0.163
0.384
0.083
0.132
0.325
HFGN
0.84
49.90%
0.70
39.03%
0.050
0.080
0.288
0.023
0.049
0.164
OCM-CF
0.92
63.62%
0.86
56.59%
0.145
0.238
0.502
0.096
0.158
0.370
%Improv.
5.75%
12.40%
4.88%
8.62%
34.26%
44.24%
30.73%
15.66%
19.70%
10.78%
• Type-aware [12] measures the fashion item compatibility with type-respecting spaces
rather than a single general space. We used the code provided by the authors and retrained
the model with only the image cue.
• SCE-NET [11] learns different similarity conditions and employs a weight module to
combine all different embeddings as a fashion item representation. Similar to type-aware,
we removed the regularization of the text information from the author-released model.
• NGNN [17] maps the fashion item feature into a category space to build the item graph,
where the node embedding is updated based on GRU [27] and the attention mechanism
is used for summarizing the outﬁt compatibility score.
• Context-aware [16] builds a graph with all fashion items in the dataset. Each node will
receive a message from its outﬁt and other outﬁts to learn the contextual item embedding.
In the testing stage, we computed the compatibility score based on its embedding.
• HFGN [19] different from NGNN, devises a R-view attention map and a R-view score
map to assess the outﬁt compatibility score based on GCNs over the category-oriented
outﬁt graph.
Table4.1 shows the performance comparison among different approaches on both
PolyvoreOutﬁtsandPolyvoreOutﬁts-Ddatasetsunderdifferenttasks.Forclarity,wedivided
the baselines into three groups, i.e., sequence-based, pair-based, and graph-based models.
From this table, we make the following observations: (1) Compared to other baselines, Bi-
LSTM achieves the worst performance on most evaluation metrics, which may be due to two
facts. On the one hand, essentially, it is inappropriate to model the outﬁt as an ordered list
of fashion items. On the other hand, this method computes the outﬁt compatibility score by
predicting the next item with the previous outﬁts, which may cause cumulative error propa-
gation. (2) Unexpectedly, the graph-based baselines, i.e., NGNN, Context-aware and HFGN,
do not show superiority over the pair-based methods, i.e., SCE-NET and type-aware. The
possible explanation for Context-aware is that this method learns fashion item embeddings

60
4
Unsupervised Disentangled Graph Learning for OCM
Table 4.2 Performance of the ablation study on Polyvore Outﬁt and Polyvore Outﬁt-D datasets
Method
Polyvore Outﬁts
Polyvore Outﬁts-D
Compat. AUC
FITB ACC (%)
Compat. AUC
FITB ACC (%)
OCM-CF
0.92
63.62
0.86
56.59
w/o Edge Weight
0.89
62.60
0.84
55.64
w/o Relationship
0.90
62.12
0.84
55.25
w/o Attention
0.64
51.12
0.61
34.11
w Fine-grained
0.87
61.93
0.80
54.65
w/o Complementarity
0.89
62.83
0.80
55.94
in a single space, while the pair-based method, type-aware, considers the visual similarity
from different metric spaces. For NGNN and HFGN, they employ fashion item embeddings
in a category space to initialize nodes, which leads to category bias, namely, the model may
learn compatibility patterns at the category level, resulting in an inaccurate evaluation of
outﬁt compatibility score. (3) Our proposed method OCM-CF consistently achieves the best
performance on all tasks. It is worth noting that our method has large improvements on the
complementary fashion item retrieval task w.r.t. HR@5 and HR@10, which is meaning-
ful for the real-world application since users can quickly ﬁnd the complementary fashion
item ﬁtting the outﬁt. The results verify the superiority of our model over the state-of-the-
art methods, and the effectiveness of our contextual outﬁt representation learning and the
hidden complementary factors learning.
4.4.3
Ablation Study
To investigate how each component affects our model, we introduced the following ﬁve
variants:
• w/o Edge Weight. In this variant, we set all the edge weights as a constant 1.
• w/o Relationship. We modiﬁed the gl
i j = vl
i in Eq.(4.7) for only aggregating neighbor
item embeddings.
• w/o Attention. We replaced the multihead attention mechanism with a mean pooling
operation over the representation of all the composition fashion items.
• wFine-grained.Wederivedtheedgeweightwiththeﬁne-grainedcategoryco-occurrence
[17, 19] rather than the coarse-grained category used in our OCM-CF.
• w/o Complementarity. We removed the orthogonality-based complementarity regular-
ization from the hidden complementary factors learning.

4.4
Experiment
61
Table4.2 shows the performance comparison of different methods in the ablation study.
From Table4.2, we noticed that all the variants degrade the performance of our OCM-CF,
which indicates the importance of each component. In particular, ﬁrst, w/o edge weight per-
forms worse than OCM-CF, implying that utilizing the category co-occurrence probability
can promote the item-item relationship propagation. Second, w/ ﬁne-grained is inferior to
OCM-CF, which conﬁrms our assertion that utilizing the ﬁne-grained categories may involve
the highly imbalanced data distribution, making the co-occurrence pattern unreliable. Third,
the inferior performance of w/o relationship suggests that propagating the item-item rela-
tionship is more meaningful for the outﬁt compatibility modeling. Fourth, we found the
performance of w/o attention signiﬁcantly drops, as compared to OCM-CF, demonstrating
the necessity of deriving the global outﬁt representation in an attentive manner. Last, w/o
complementarity is also inferior to OCM-CF, reﬂecting the effectiveness of our proposed
complementarity regularization.
4.4.4
Case Study
To gain a more intuitive understanding of our model, we conducted the case study on two
tasks: similar outﬁt retrieval and complementary fashion item retrieval.
Similar Outﬁt Retrieval
To illustrate the effectiveness of the outﬁt representation learned by our model, we inves-
tigated the performance of our model in the task of similar outﬁt retrieval, which aims to
retrieve similar outﬁts for a given query outﬁt. We argued that similar outﬁts tend to share
the common prominent features. Instead of using all factor-oriented outﬁt representations,
we particularly adopted the outﬁt representation corresponding to the highest compatibil-
ity score, i.e., pk∗, where k∗= arg max
k
{Wk
spk|K
k=1}, and employed the cosine similarity
between the query outﬁt and each candidate outﬁt to retrieve the similar outﬁt for the query
outﬁt. In the comprehensive evaluation, we studied the similar outﬁt retrieval task in two sce-
narios: (1) the candidate outﬁts have the same length, i.e., the same number of composition
items with the query outﬁt, and (2) the candidate outﬁts have random lengths. In this part,
we directly employed the test dataset of the compatibility prediction as the set of candidate
outﬁts. Figure4.2 illustrates the retrieval results of two testing outﬁts in each scenario. For
the ﬁrst scenario, from the left example in Fig.4.2a, we observed that the retrieved outﬁts
share the blue color, and the fresh style, while in the right example, we noticed that the
returned outﬁts also possess the high similarity with the query outﬁt, such as the summer
style, a variety of colors and patterns, and the item categories. Similar observations can be
also obtained from Fig.4.2b, where the length of the retrieved outﬁts is not restricted. In
general, these observations reﬂect the effectiveness of the factor-oriented outﬁt representa-
tion learned by our model and the beneﬁt of exploring the hidden complementary factors to
capture the discriminative feature of the outﬁt.

62
4
Unsupervised Disentangled Graph Learning for OCM
(a)
(b)
Fig.4.2 Illustration of the similar outﬁt retrieval results with testing samples in two scenarios

4.5
Summary
63
Fig.4.3 Illustration of the complementary item retrieval results. Positive items are highlighted in red
boxes
Complementary Fashion Item Retrieval
Similar to existing studies [22, 25], we also presented the qualitative results of our model in
the complementary item retrieval task, where the candidate set comprised a target item as
well as nine negative items. Moreover, we adopted two negative item sampling protocols: the
negative items were randomly selected from items with the same coarse-grained category
with the target item, and (2) the negative items were randomly selected from items with the
same ﬁne-grained category with the target item, which corresponds to a more challenging
task. In addition, we adopted the best baseline on the complementary item retrieval task,
i.e., type-aware, for comparison. Due to the limited space, we only exhibited one example
for each scenario in Fig.4.3. As can be seen, our OCM-CF can rank the target items at the
top places, outperforming the type-aware method.
4.5
Summary
In this chapter, we presented a novel outﬁt compatibility modeling scheme via comple-
mentary factorization, named OCM-CF, which seamlessly uniﬁes the context-aware outﬁt
representation learning and hidden complementary factors learning in the context of outﬁt
compatibility modeling. Extensive experiments were conducted on two real-world datasets,
and the encouraging experimental results validate the superiority of our proposed model and
the importance of each component. In addition, we notice that the global outﬁt representation
models one compatible factor by considering all items of the outﬁt with a comprehensive

64
4
Unsupervised Disentangled Graph Learning for OCM
perspective and the proposed orthogonality-based complementarity regularization can make
the factor-oriented outﬁt representation discriminative.
References
1. Kipf, Thomas N., and Max Welling. 2017. Semi-Supervised Classiﬁcation with Graph Convo-
lutional Networks. In International Conference on Learning Representations, 1–15. OpenRe-
view.net.
2. Rong, Yu, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. DropEdge: Towards Deep
Graph Convolutional Networks on Node Classiﬁcation. In Proceedings of the International
Conference on Learning Representations, 1–17. OpenReview.net.
3. Zhang, Zhaolong, Yuejie Zhang, Rui Feng, Tao Zhang, and Weiguo Fan. 2020. Zero-Shot Sketch-
Based Image Retrieval via Graph Convolution Network. In Proceedings of the International Joint
Conference on Artiﬁcial Intelligence, 12943–12950. AAAI Press.
4. Zhou, Xiang, Fumin Shen, Li. Liu, Wei Liu, Liqiang Nie, Yang Yang, and Heng Tao Shen. 2020.
Graph Convolutional Network Hashing. IEEE Transactions on Cybernetics 50 (4): 1460–1472.
5. Wei, Yinwei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua.
2019. MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation
ofMicro-video.InProceedingsoftheACMInternationalConferenceonMultimedia,1437–1445.
ACM.
6. He, Xiangnan, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020.
LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In
Proceedings of the International ACM SIGIR Conference on Research and Development in
Information Retrieval, 639–648. ACM.
7. Gori, M., G. Monfardini, and F. Scarselli. 2005. A New Model for Learning in Graph Domains.
In Proceedings of the IEEE International Joint Conference on Neural Networks, vol. 2, 729–734.
8. Kipf, Thomas, and M. Welling. 2016. Variational Graph Auto-Encoders. In NIPS Workshop on
Bayesian Deep Learning.
9. Veliˇckovi´c, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. 2018. Graph Attention Networks. In Proceedings of the International Conference on
Learning Representations, 45–61. OpenReview.net.
10. Hamilton, William L., Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning
on Large Graphs. In Advances in Neural Information Processing Systems, 1024–1034. Curran
Associates Inc.
11. Tan, Reuben, Mariya I. Vasileva, Kate Saenko, and Bryan A. Plummer. 2019. Learning Similarity
Conditions Without Explicit Supervision. In Proceedings of the IEEE International Conference
on Computer Vision, 10372–10381. IEEE.
12. Vasileva, Mariya I., Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, and
David A. Forsyth. 2018. Learning Type-Aware Embeddings for Fashion Compatibility. In Euro-
pean Conference on Computer Vision, 405–421. Springer.
13. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for
Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 770–778. IEEE Computer Society.
14. Olga Russakovsky, J., H. Deng, J. Su, S. Krause, S.. Ma. Satheesh, A. Zhiheng Huang, A. Khosla.
Karpathy, Michael S. Bernstein, A. Berg, and Li. Fei-Fei. 2015. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vision 115: 211–252.

References
65
15. Veit, Andreas, Serge J. Belongie, and Theofanis Karaletsos. 2017. Conditional Similarity Net-
works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
1781–1789. IEEE.
16. Cucurull, Guillem, Perouz Taslakian, and David Vázquez. 2019. Context-Aware Visual Com-
patibility Prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 12617–12626. IEEE.
17. Cui, Zeyu, Zekun Li, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Dressing as a Whole:
Outﬁt Compatibility Learning Based on Node-wise Graph Neural Networks. In Porceedings of
the World Wide Web Conference, 307–317. ACM.
18. Maas, Andrew L, Awni Y Hannun, and Andrew Y Ng. 2013. Rectiﬁer Nonlinearities Improve
Neural Network Acoustic Models. In Proceedings of the International Conference on Machine
Learning, 3–3. JMLR.org.
19. Li, Xingchen, Xiang Wang, Xiangnan He, Long Chen, Jun Xiao, and Tat-Seng Chua. 2020. Hier-
archical Fashion Graph Network for Personalized Outﬁt Recommendation. In Proceedings of the
International ACM SIGIR Conference on Research and Development in Information Retrieval,
159–168. ACM.
20. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Proceedings of the
Advances in Neural Information Processing Systems, 5998–6008. NIPS.
21. Wang, Xiang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua. 2020. Dis-
entangled Graph Collaborative Filtering. In Proceedings of the International ACM SIGIR Con-
ference on Research and Development in Information Retrieval, 1001–1010. ACM.
22. Song, Xuemeng, Xianjing Han, Yunkai Li, Jingyuan Chen, Xin-Shun Xu, and Liqiang Nie. 2019.
GP-BPR: Personalized Compatibility Modeling for Clothing Matching. In Proceedings of the
ACM International Conference on Multimedia, 320–328. ACM.
23. Rendle, Steffen, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR:
Bayesian Personalized Ranking from Implicit Feedback. In Proceedings of the International
Conference on Uncertainty in Artiﬁcial Intelligence, 452–461. AUAI Press.
24. Han, Xintong, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis. 2017. Learning Fashion Com-
patibility with Bidirectional LSTMs. In Proceedings of the ACM International Conference on
Multimedia, 1078–1086. ACM.
25. Dong, Xue, Jianlong Wu, Xuemeng Song, Hongjun Dai, and Liqiang Nie. 2020. Fashion Compat-
ibility Modeling through a Multi-modal Try-on-guided Scheme. In Proceedings of the Interna-
tional ACM SIGIR Conference on Research and Development in Information Retrieval, 771–780.
ACM.
26. Zhang, Hanwang, Zheng-Jun Zha, Yang Yang, Shuicheng Yan, Yue Gao, and Tat-Seng Chua.
2013. Attribute-Augmented Semantic Hierarchy: Towards Bridging Semantic Gap and Intention
Gap in Image Retrieval. In Proceedings of the ACM International Conference on Multimedia,
33–42. ACM.
27. Li, Yujia, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated Graph
Sequence Neural Networks. In International Conference on Learning Representations, 15–25.
OpenReview.net.

5
Supervised Disentangled Graph Learning for OCM
5.1
Introduction
In Chap. 4, we studied the ﬁne-grained outﬁt compatibility modeling, where the hidden
factors affecting the outﬁt compatibility are jointly considered. One key limitation is that it
only investigates the visual content of fashion items while overlooking the items’ semantic
attributes. The item attribute labels usually contain rich information that characterizes the
key item parts, which can be adopted to supervise the attribute-level representation learning,
and hence promote the model’s performance as well as interpretability. Thus, in this chapter,
we aim to fulﬁll the ﬁne-grained outﬁt compatibility modeling by incorporating the semantic
attributes of fashion items.
However, fulﬁlling this goal is nontrivial due to the following challenges. (1) The fashion
item attribute labels are not uniﬁed or aligned. In other words, each item may have different
attribute labels. For instance, as shown in Fig.5.1, one T-shirt is labeled with attributes of
price, sleeve length, design, and brand; while the other has color, material, brand, and price.
Thereby, how to fully take advantage of these irregular attribute labels to partially supervise
theattribute-levelrepresentationlearningoffashionitemsposesaconsiderablechallenge.(2)
When disentangling the entire visual embedding into multiple attribute-level representations,
how to ensure information intactness during the disentanglement is another challenge. (3)
To comprehensively capture the compatibility among fashion items, we incorporate both the
coarse-grained item-level and ﬁne-grained attribute-level information into the compatibility
modeling. Accordingly, how to seamlessly combine multiple granularities to strengthen the
learning performance constitutes another tough challenge.
To address the aforementioned challenges, we present a partially supervised compat-
ibility modeling scheme, called PS-OCM. As shown in Fig.5.2, it consists of three key
components: (1) partially supervised attribute-level embedding learning, (2) disentangled
completeness regularization, and (3) hierarchical outﬁt compatibility modeling. Speciﬁcally,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
W. Guan et al., Graph Learning for Fashion Compatibility Modeling,
Synthesis Lectures on Information Concepts, Retrieval, and Services,
https://doi.org/10.1007/978-3-031-18817-6_5
67

68
5
Supervised Disentangled Graph Learning for OCM
Fig.5.1 Illustration of two fashion items and their associated irregular attribute labels
...
Ouƞit
CNN
Disentangle
...
...
color
color
price
price
category
category
residual
residual
AƩribute-level Embeddings
Orthogonal
1) ParƟally Supervised 
AƩribute-level Embedding Learning
DeconvoluƟon
2) Disentangled Completeness 
RegularizaƟon
2) Disentangled Completeness
RegularizaƟon
2) Disentangled Completeness 
RegularizaƟon
heel
heel
...
...
...
AƩribute level
Item level
3) Hierarchical Ouƞit 
CompaƟbility Modeling
......
MLP
CompaƟbility 
Score
...
Fig. 5.2 Illustration of our proposed PS-OCM scheme. It consists of three components: partially
supervised attribute-level embedding learning, disentangled completeness regularization, and hierar-
chical outﬁt compatibility modeling
the ﬁrst component extracts visual features from each composing item of the given outﬁt
via a pretrained model. It then turns to disentangle the visual feature vector into a set of
ﬁne-grained attribute embeddings, which is partially supervised by the irregular attribute
labels of each fashion item. The second component works toward an intact disentanglement.
This is accomplished by adopting two strategies: orthogonal residual embedding and visual
representation reconstruction. An orthogonal residual embedding is introduced to compen-
sate for the information loss, and regularize the orthogonal relationship between the residual
embedding and each attribute-level embedding. Additionally, it leverages the deconvolution
neural network to ensure that the original image can be reconstructed from the disentan-
gled attribute-level and residual embeddings. The last component contains a hierarchical
graph convolutional network, which models the outﬁt compatibility by jointly integrating the

5.3
Methodology
69
ﬁne-grained attribute-level and coarse-grained item-level information. Ultimately, it fuses
the attribute-level compatibility scores and the item-level ones via a multilayer perceptron
(MLP) to derive the ﬁnal compatibility score of the given outﬁt.
5.2
Related Work
Disentangled representation learning [1] targets learning multiple factorized representations
to capture the latent explanatory factors residing in the observed data, which has drawn
increasing research attention from various domains, such as the recommendation domain
[2, 3] and computer vision domain [4–6]. For example, in the recommendation domain, Hu
et al. [7] proposed a novel graph neural news recommendation model with unsupervised
preference disentanglement, where a neighborhood routing mechanism is introduced to
dynamically identify the latent preference factors affecting the click between a user and
a piece of news. In addition, Wang et al. [8] presented a disentangled graph collaborative
ﬁltering model to mine the ﬁne-grained user-item relationships. In the computer vision ﬁeld,
Ma et al. [4] strengthened the natural person image generation by disentangling the input
image into three intermediate embedding features, corresponding to three main factors:
foreground, background, and pose.
As the compatibility relationship among fashion items can be inﬂuenced by multiple
latent factors, such as color, texture, and style, some researchers also incorporated the disen-
tangled representation to address the task of fashion compatibility modeling. For example,
Zheng et al. [9] devised a disentangled graph learning scheme, where the collocation com-
patibility is disentangled into multiple ﬁne-grained compatibilities among fashion items.
Similarly, Guan et al. [10] presented a comprehensive multimodal outﬁt compatibility mod-
eling scheme, which not only explores the ﬁne-grained outﬁt compatibility with disentangled
item representations but also explicitly models the consistent and complementary correla-
tions between the visual and textual modalities of items. Despite their signiﬁcant value, the
existing efforts mostly overlook the potential of the semantic labels in supervising the disen-
tangled representation learning. Therefore, in this work, we propose utilizing the irregular
attributes as partial supervision to guide the disentangled representation learning of items
and introducing the completeness regularizer to prevent information loss during disentan-
glement.
5.3
Methodology
In this section, we ﬁrst formulate the research problem and then detail the proposed partially
supervised outﬁt compatibility modeling scheme (PS-OCM).

70
5
Supervised Disentangled Graph Learning for OCM
5.3.1
Problem Formulation
In this work, we cast the outﬁt compatibility modeling task as a binary classiﬁcation problem,
i.e., whether the given outﬁt is compatible. Assume we have a set of N outﬁts, denoted as
 = {(Oi, yi)}N
i=1, where Oi is the i-th outﬁt, and yi denotes its corresponding compatibility
label.Speciﬁcally, yi = 1iftheoutﬁt Oi iscompatible,and yi = 0,otherwise.Inaddition,we
have a set of fashion items I distributed over T categories. For simplicity, we temporally omit
the subscript i of each outﬁt. An outﬁt O comprises K fashion items, i.e., {I1, I2, . . . , IK },
where Ii ∈I is the i-th composing item of the outﬁt. Considering that the number of items
in an outﬁt is not ﬁxed, K is a variable. Each item Ii is associated with a visual image Vi and
a set of attribute labels Li. We heuristically predeﬁned a set of attributes (e.g., the color and
material) A = {am}M
m=1 that can be applied to characterize all the fashion items, where am is
the m-th attribute, and M is the total number of attributes. Moreover, each attribute has a set
of corresponding attribute values, e.g., red and blue are two possible values for the attribute
color. We then formally use Vm =

vn
m
Nm
n=1 to denote all the possible values for the attribute
am, and Nm is the corresponding total number of values. Therefore, the set of attribute
labels of the i-th item can be written as Li = {l1
i ,l2
i , . . . ,l M
i }, where lm
i ∈Vm if the item
Ii has m-th attribute; otherwise, lm
i = none. Usually there are two possible reasons leading
to lm
i = none: one reason is the intrinsic ﬂaws of the dataset due to loose user-generated
annotation, and the other is that items of certain categories essentially cannot present certain
attributes (e.g., the trousers do not have the attribute of sleeve length).
In this work, we target at learning an outﬁt compatibility model F to judge whether a
given outﬁt O is compatible. It is formulated as follows,
s = F

{(Vi, Li)}K
i=1|

,
(5.1)
where  refers to the to-be-learned parameters of our model, and s denotes the compatible
probability of the given outﬁt.
5.3.2
Partially Supervised Compatibility Modeling
As illustrated in Fig.5.2, PS-OCM consists of three key components: (1) partially supervised
attribute-level embedding learning, (2) disentangled completeness regularization, and (3)
hierarchical outﬁt compatibility modeling. We explain them as follows.
Partially Supervised Attribute-Level Embedding Learning
This component aims to derive the ﬁne-grained attribute-level representation of the fashion
item, which is the basis for the following hierarchical outﬁt compatibility modeling. Given an
outﬁt, we ﬁrst extract the visual feature of each composing item via the convolutional neural
networks, which have obtained remarkable success in many computer vision tasks [11, 12].
Speciﬁcally, we obtain the overall visual feature embedding of the i-th item in the outﬁt O
as follows,

5.3
Methodology
71
vi = CNN (Vi) ,
(5.2)
where Vi refers to the i-th item image in its raw RGB pixels, vi ∈RDv denotes the extracted
visual feature of the i-th item, and Dv is the dimension of the visual feature. In this work,
the function CNN refers to ResNet18 [13] pretrained on ImageNet.
As previously mentioned, we predeﬁned a set of M attributes to characterize all the items.
Accordingly, we disentangle the visual feature of each item Ii, i.e., vi, into M attribute-level
embeddings. We argue that the attributes are not linearly separable, and hence accomplish
this task by the nonlinear MLP mapping. Mathematically, we have
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
e1
i = MLP1 (vi) ,
e2
i = MLP2 (vi) ,
...
eM
i
= MLPM (vi) ,
(5.3)
where e j
i ∈RDe ( j = 1, . . . , M) denotes the j-th disentangled attribute-level embedding of
the i-th item, and De is the dimension.
Different from existing studies that focus on the unsupervised disentangled representation
learning, we argue that even the irregular attribute labels of fashion items contain rich cues.
Therefore, they can be used to supervise the attribute-level embedding learning and hence
strengthen the ﬁnal compatibility modeling performance. Thereby, we further utilize M
MLPs as the attribute classiﬁers to explore the attribute labels. As aforementioned, the
fashion item attribute labels are irregular. We thus introduce a binary mask pi for each item
Ii in the outﬁt to select the available attribute labels of the i-th item. In particular, we deﬁne
the mask as pi = [p1
i , p2
i , . . . , pM
i ], where pm
i = φ(lm
i ), and φ(·) is an indicator function
deﬁned as follows,
φ(x) =

 0 x is none,
1 else.
(5.4)
By utilizing the binarized mask, if and only if the item has the corresponding attribute
label, we enforce the supervision over the embedding for that attribute. In particular, we
adopt the cross-entropy loss to achieve the partial supervision. Formally, for a given outﬁt
O consisting of K items, the partial supervision loss function is formulated as follows,
Lps =
K

i=1
M

m=1
−log

p

lm
i | Cm 
em
i

pm
i ,
(5.5)
where Cm(·) is the label classiﬁer for the m-th attribute, em
i is the disentangled embedding
of the m-th attribute, and lm
i is the ground-truth attribute label. We illustrate the procedure
of partially supervised disentangled attribute-level embedding in Fig.5.3.

72
5
Supervised Disentangled Graph Learning for OCM
Image 
feature
Origin 
image
MLP
MLP
MLP
MLP
Color
Price
Category
Heel
...
...
Classiﬁer
Classiﬁer
Black
Low
Missing label
...
...
Inapplicable aƩribute
CNN
ParƟal Supervision
Fig.5.3 Illustration of the partially supervised attribute-level embedding learning module
Disentangled Completeness Regularization
To prevent information loss during the disentangling process which may degrade the model
performance, we devise a disentangled completeness regularizer, as illustrated in Fig.5.2.
In particular, we rely on two strategies to regulate the disentangling process: orthogonal
residual embedding and visual representation reconstruction.
Orthogonal Residual Embedding. There may be some implicit visual properties of the
item that cannot be represented by the predeﬁned set of attributes. We thus introduce another
special attribute residual to compensate for the information loss during the disentangled
representation learning. Speciﬁcally, similar to the M attribute-level embeddings, we adopt
another MLP to derive the residual attribute embedding via,
eM+1
i
= MLPM+1 (vi) ,
(5.6)
where eM+1
i
∈RDe denotes the residual attribute embedding.
Since the residual attribute embedding acts as compensation for fully representing the
item, we argue that it should be complementary to other M attribute-level embeddings that
have clear semantics. In other words, the residual embedding should be orthogonal to every
other attribute-level embedding. It is worth noting that although we disentangle the visual
feature of each fashion item into M attribute-level embeddings, certain embeddings of the
given item may be meaningless since some attributes are not universal and cannot be applied
to certain items. For example, we can discuss the attribute sleeve length for a T-shirt but not
trousers, and the attribute heel for a pair of shoes rather than a T-shirt. Therefore, for each
item category, we deﬁne a set of meaningful attributes to guarantee effective orthogonal
regularization. Thus, we ﬁrst build the category-attribute associations. For the t-th category,
we take the union set of attributes used to label items in the t-th category as the whole set
of applicable attributes, denoted as Tt. We then introduce a mask qt = [q1
t , q2
t , . . . , q M
t ] to
select the meaningful attributes for the t-th item category, where qm
t = 1 if the predeﬁned
m-th attribute belongs to the applicable attribute set Tt; otherwise, qm
t = 0. It is worth noting

5.3
Methodology
73
that in the aforementioned partial supervision module, only the attribute-level embeddings
that have corresponding labels are triggered. Whereas in this orthogonal regularization, we
further utilize the attribute-level embedding that even has no corresponding label, as long
as it can be possibly presented by this item.
Ultimately, we have the following orthogonal regularization,
Lor =
K

i=1
M

m=1

cos

ˆem
i , eM+1
i
2
=
K

i=1
M

m=1

cos

qm
t∗
i em
i , eM+1
i
2
,
(5.7)
where cos (·, ·) is the cosine similarity function, and t∗
i ∈{1, 2, . . . , T } refers to the category
of the i-th item. It is worth noting that once the m-th attribute cannot be applied to the item Ii,
i.e., qt∗
i = 0, we ignore the orthogonal regularization between that attribute-level embedding
and the residual embedding.
Visual Representation Reconstruction. To avoid information loss during disentangled
representation learning, we regulate the disentangled embeddings to be able to reconstruct
the original item visual representation. Thus, we feed the concatenation of the meaningful
disentangled attribute-level embeddings of the item Ii and the residual one into the decon-
volutional neural network [14]. It is formulated as,
ˆVi = D

q1
t∗
i e1
i ∥q2
t∗
i e2
i ∥. . . , q M
t∗
i eM
i ∥eM+1
i

,
(5.8)
where the binary masks qm
t∗
i are used to select the meaningful attribute embeddings of the
item Ii, [·∥·] refers to the concatenation operation, D (·) denotes the deconvolutional neural
network, and ˆVi denotes the reconstructed visual representation of the i-th item. We hereafter
utilize l_2 loss to regulate the distance between the reconstructed visual representation and
the origin one via,
Lrec =
K

i=1
 ˆVi −Vi

2
F .
(5.9)
Combining the losses of both the orthogonal residual embedding and the visual represen-
tation reconstruction constraints, we reach the ﬁnal loss for regularizing the disentangled
completeness as follows,
Ldc = Lor + Lrec.
(5.10)
Hierarchical Outﬁt Compatibility Modeling
Inspired by previous studies [15, 16], we leverage GCNs to model outﬁt compatibility.
Beyond existing work, we design a novel hierarchical graph convolutional network, which
can model the complex compatibility relations among items in an outﬁt from both attribute
and item levels. In particular, the attribute-level compatibility modeling aims to investigate

74
5
Supervised Disentangled Graph Learning for OCM
the ﬁne-grained compatibility among fashion items, while the item-level model summarizes
the coarse-grained outﬁt compatibility from the item level.
Attribute-level Compatibility Modeling. Regarding the attribute-level compatibility mod-
eling, given an outﬁt, we ﬁrst construct M + 1 parallel compatibility modeling graphs
Gm
a =
N m
a , Em
a

, (m = 1, 2, . . . , M + 1), with each devised to model the outﬁt compat-
ibility from an attribute aspect1. In particular, N m
a and Em
a refer to the set of nodes and
edges, respectively, of the graph Gm
a . In Gm
a graph, each node refers to a composing item of
the outﬁt that has the corresponding attribute, i.e., am. Notably, as previously mentioned,
not every attribute can be applied to all the items, e.g., the attribute sleeve length cannot
be used to characterize a pair of trousers. Therefore, for different attributes, different num-
bers of items are applicable for the attribute-level compatibility modeling. In other words,
graphs corresponding to different attributes may have different numbers of nodes. There-
fore, for the ease of presentation, we still deploy K item nodes for all these graphs, i.e.,
N m
a =

ˆnm
i
K
i=1, where ˆnm
i is the i-th node in the Gm
a graph. However, some nodes in these
graphs are deﬁned as the virtual isolated nodes and are inactive during the attribute-level
compatibility propagation.
During the learning process, each node ˆnm
i is associated with a hidden state vector hm
i ,
which is updated to fulﬁll the compatibility information propagation over the graph. We
initialize the hidden vector of node ˆnm
i by,
hm
i =
qm
t∗
i em
i , m ∈{1, 2, . . . , M} ,
eM+1
i
, m = M + 1.
(5.11)
Therefore, if the m-th attribute can be applied to the item of the i-th node, we initialize the
node with the item’s corresponding attribute feature. Otherwise, the node is initialized with
an all-zero vector, making it an isolated node in the graph, and it will not join the subsequent
compatibility information propagation. Regarding the edge construction for each graph, we
introduce an edge between each pair of nonisolated nodes, i.e., each pair of meaningful
items in the corresponding attribute-level compatibility modeling.
To simplify the notation, considering that the parallel attribute-level compatibility mod-
eling for different attributes follow the same learning process, we temporally remove all
the superscripts m from the above notations and present the general attribute-level compat-
ibility modeling scheme as an example. Inspired by graph attention networks (GAT) [17],
we employ the attention mechanism to make each node adaptively absorb compatibility
information from the neighbors. Formally, we have
αi j =
exp

Wa

hi∥h j


nk∈Ni exp (Wa [hi∥hk]),
(5.12)
where αi j indicates the importance of node n j’s hidden state to node ni, Wa is a weight
matrix to perform the linear transformation, [·∥·] refers to the concatenation operation, and
1 As previously stated, the residual attribute is also incorporated as a special implicit attribute.

5.3
Methodology
75
Ni denotes the neighborhood of node ni. Once the attention weights αi j are obtained, they
are then used to propagate information from the neighbors of node ni to the node by,
h
′
i = ω
⎧
⎨
⎩Wu
⎡
⎣
n j∈Ni
αi j

hi ⊙h j

⎤
⎦+ bu
⎫
⎬
⎭,
(5.13)
where ⊙denotes the elementwise multiplication, Wu and bu are the parameters of the
fully-connected layer, and ω refers to the nonlinear activation function LeakyReLU. The
elementwise multiplication hi ⊙h j indicates the compatibility information between the
items Ii and I j. More generally, instead of propagating the features of node ni’s neighbors,
we propagate the compatibility information between node ni and its neighbors, which has
proven to be effective in addressing the outﬁt compatibility modeling task [10].
Based upon the above inference and computation, the updated hidden representation of
node ni is written as,
˜hi = ω (Wohi + bo) + h
′
i,
(5.14)
where Wo and bo denote the weight matrix and bias to be learned, respectively. The symbol ω
denotes the LeakyReLU function. We ultimately feed the updated hidden node embeddings
into an MLP to derive the attribute-speciﬁc compatibility score of the given outﬁt via,

ci = W2

ψ

W1 ˜hi + b1

+ b2,
c = 1
K
K
i=1ci,
(5.15)
where W1, W2, b1, and b2 are the parameters of the MLPs, the symbol ψ denotes the ReLU
active function, and c is the compatibility score. Following the above general scheme, we can
obtain all the attribute-level compatibility scores, denoted as ca =

c1, c2, . . . , cM, cM+1
,
as well as the updated hidden attribute-level embeddings of each node/item, i.e., ˜hi =
[˜h1
i , ˜h2
i , . . . , ˜hM
i , ˜hM+1
i
].
Item-Level Compatibility Modeling. Similar to attribute-level compatibility modeling,
we also construct a compatibility modeling graph Go = (No, Eo) at the overview item level,
where No and Eo refer to the node set and the edge set, respectively. The difference is that
we initialize the hidden vector of the i-th node in the graph Go from two aspects: the item’s
original visual feature vi, and the updated attribute-level item embedding ˜hm
i ’s from the
attribute-level compatibility modeling scheme. In this way, a more comprehensive overview
representation of the item is derived. Speciﬁcally, for the i-th node in the graph Go, we
initialize its hidden vector as follows,
gi =

vi∥Wh

˜h1
i ∥˜h2
i ∥· · · ∥˜hM+1
i

,
(5.16)
where [·∥·] denotes the concatenation operation, and Wh ∈RDv×De(M+1) is the to-be-learned
weight matrix, which projects the attribute-level embeddings to the same space of the entire

76
5
Supervised Disentangled Graph Learning for OCM
visual embeddings. Following the same information propagation scheme as the attribute-
level compatibility modeling, we can obtain the item-level compatibility score co.
Considering both the attribute- and item-level compatibility modeling results, we feed
the concatenation of the attribute- and item-level compatibility scores, i.e., c = [ca∥co], into
the MLP to obtain the ﬁnal compatibility probability score as follows,
s = σ {W4 [ψ (W3c + b3)] + b4} ,
(5.17)
where W3, W4, b3, and b4 are the parameters of the MLP, the symbol ψ denotes the ReLU
activefunction,andσ referstothesigmoidactivefunction.Weﬁnallyadoptthecross-entropy
loss to optimize our proposed PS-OCM, and reach the following formulation,
Lhc = −ylog(s) −(1 −y)log(1 −s),
(5.18)
where y is the ground-truth compatibility label for the outﬁt O. Accordingly, the total loss
for our PS-OCM can be written as follows,
L = Lhc + λLps + μLdc,
(5.19)
where λ and μ are tradeoff hyperparameters.
Interpretability. The semantic attributes have explicit meaning and can be used naturally
to interpret the compatibility evaluation result. In particular, we can identify the prominent
attributes that contribute to the ﬁnal compatibility evaluation most, according to the absolute
values of these attribute-speciﬁc compatibility scores, i.e., cms.
5.4
Experiment
In this section, we ﬁrst introduce the dataset and experimental settings, and then detail the
extensive experiments that we conducted on a real-world dataset by answering the following
research questions:
• RQ1: Does the proposed PS-OCM outperform the state-of-the-art methods?
• RQ2: How does each component affect PS-OCM?
• RQ3: What is the intuitive evaluation result of PS-OCM?
5.4.1
Experimental Settings
Dataset and Evaluation Metrics
To justify our model, we resorted to the public dataset IQON3000 [18], due to the fact that
each item in IQON3000 has not only the visual image but also several semantic attributes,
such as color and category. In particular, IQON3000 consists of 308, 747 outﬁts, composed

5.4
Experiment
77
Table 5.1 Attributes, their possible values and total number in the IQON3000 dataset
Attribute
Possible value
Total number
Color
Gray, Back, Green, · · ·
12
Price
Low, Middle, High.
3
Brand
ABISTE, FURLA, BEIGE, · · ·
5, 180
Category
Trousers, Belt, Handbag, · · ·
61
Variety
Coat, Bag, Cosmetics, · · ·
20
Material
Fur, Leather, Denim, · · ·
37
Pattern
Stripe, Embroidery, Animal,
· · ·
15
Design
Turtleneck, Frill, Ribbons, · · ·
23
Heel
Chunky, Pin, High, · · ·
6
Dress length
Short, Middle, Long.
3
Sleeve length
Sleeveless, Long, Short, · · ·
4
by 672, 335 items. In total, there are 11 attributes provided by this dataset. Table5.1 shows
the possible value examples and the corresponding number for each attribute. To ensure the
dataset quality, we empirically sampled 20, 000 compatible outﬁts, each of which consisted
of at least 2 but no more than 10 items. Since the dataset only provided the compatible
outﬁts, incompatible outﬁts were composed for training. Speciﬁcally, for each compatible
outﬁt, we replaced each of its composing items with a randomly sampled item from the
same category to construct the incompatible outﬁt. In this manner, we end up with a set of
40, 000 compatible/incompatible outﬁts. We then divided it into the training set, validation
set, and test set according to the ratio of 8 : 1 : 1.
Similar to previous studies [10, 15, 16, 19, 20], we justiﬁed our proposed PS-OCM
scheme with two speciﬁc tasks: outﬁt compatibility estimation and ﬁll-in-the-blank. For
these two tasks, we also used the AUC and the accuracy (ACC) as the evaluation metrics,
respectively.
Implementation Details
For the image encoder, we employed the ResNet18 [13] pretrained on ImageNet [21] as the
backbone, and modiﬁed the last layer to make the output feature dimension 256. Pertaining
to the MLPs that obtain the disentangled attribute-level embeddings, we set the output
dimension to 64. We selected Adam [22] as the training optimizer, with a ﬁxed learning rate
of 0.0001. We empirically set the batch size as 32, and both tradeoff hyperparameters, i.e.,
λ and μ in Eq. (5.19), as 1. All the experiments were implemented by PyTorch over a server
equipped with 4 GeForce RTX 2080 Ti GPUs, and the random seeds for model initialization
were ﬁxed for the reproducibility.

78
5
Supervised Disentangled Graph Learning for OCM
5.4.2
Model Comparison
To validate the effectiveness of our proposed scheme, we chose the following baselines for
comparison, including the pairwise, sequencewise, and graphwise models.
• Type-aware [20] devises the type-speciﬁc embedding spaces according to the item types,
to facilitate the outﬁt compatibility measurement. The visual-semantic loss is utilized to
incorporate the visual and textual information.
• SCE-NET [23] embeds the item visual features into multiple semantic subspaces by
multiple condition masks, and uses the multimodal features to derive the importance
weights for different subspace features to obtain the ﬁnal item representations.
• Bi-LSTM [19] takes the items in an outﬁt as a sequence ordered by the item categories,
and exploits the latent item interaction by a bi-directional LSTM. Notably, the textual
information is also adopted to regularize the outﬁt compatibility modeling by the visual-
semantic consistency loss.
• NGNN [15] is the ﬁrst research attempt to employ GNN to model the outﬁt compatibility,
where each outﬁt is represented as a subgraph, and an attention mechanism is utilized to
calculate the outﬁt compatibility score. For the multimodal features, NGNN designs two
graph channels, and the ﬁnal compatibility score is derived in a late fusion manner.
• HFGN [24] develops a hierarchical fashion graph network to jointly fulﬁll the fash-
ion compatibility modeling and personalized outﬁt recommendation, where a category-
oriented fashion graph is built for each outﬁt. It only uses the visual features.
• MM-OCM [10] explicitly models the consistent and complimentary relations between
the visual and textual modalities of fashion items by the parallel and orthogonal regular-
izations. Moreover, MM-OCM jointly uniﬁes the text-oriented and vision-oriented outﬁt
compatibility modeling with the mutual learning strategy.
• OCM-CF [25] directly learns the context-aware global outﬁt representation by GCNs and
the multihead attention mechanism, and employs multiple network branches to explore
the hidden complementary factors that affect the outﬁt compatibility.
Table5.2 shows the performance of different methods on the outﬁt compatibility esti-
mation task and ﬁll-in-the-blank task. Notably, the baseline methods were retrained by the
released corresponding codes over the IQON3000 dataset. From this table, we make the
following observations:
1. The pairwise methods, i.e., Type-aware and SCE-NET, achieved the worst performance
on both two tasks. This may be due to the fact that the pairwise methods justify the local
compatibility between two items, lacking the global view of the whole outﬁt.
2. The sequencewise method, i.e., Bi-LSTM, performs better than the pairwise methods,
but worse than the graphwise methods, i.e., HFGN and MM-OCM. On the one hand,
this conﬁrms the advantage of treating the outﬁt as a uniﬁed sequence rather than the

5.4
Experiment
79
Table 5.2 Performance comparison between our proposed PS-OCM and other baseline methods on
two tasks over the IQON3000 dataset. Notably, the baseline methods were retrained by the released
codes. The best results are in bold, while the second best results are underlined
Method
Compat. AUC
FITB ACC
Type-aware [20]
0.6688
0.3901
SCE-NET [23]
0.6792
0.3783
Bi-LSTM [19]
0.7739
0.3813
NGNN [15]
0.7591
0.4002
HFGN [24]
0.8243
0.4511
MM-OCM [10]
0.8444
0.4661
OCM-CF [25]
0.8402
0.4825
PS-OCM
0.9009
0.5412
item pairs. On the other hand, this implies that treating the outﬁt as an ordered sequence
of fashion items is still suboptimal. This may be attributed to the sequencewise method
being able to suffer from the cumulative error propagation problem since it computes
the outﬁt compatibility score by continually predicting the next item with the previous
items.
3. Our proposed PS-OCM consistently surpasses all the baseline methods on both tasks.
This conﬁrms the advantage of our scheme that utilizes the irregular attribute labels to
provide partial supervision to strengthen the item representation learning and employs the
hierarchical graph convolutional network to integrate the attribute-level and item-level
outﬁt compatibility learning.
To gain deep insights into our proposed PS-OCM, we further checked the performance
of our PS-OCM for outﬁts with different numbers of composing items on the two tasks.
In particular, we reported the performance of our model for outﬁts with the number of
composing items ranging from 2 to 10. As can be seen in Fig.5.4, our PS-OCM is generally
not sensitive to the composing numbers, which indicates that our model PS-OCM can handle
the compatibility modeling for outﬁts with various numbers of items.
5.4.3
Ablation Study
To verify the importance of each component in our model, we conducted ablation experi-
ments on the following derivatives.

80
5
Supervised Disentangled Graph Learning for OCM
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
2
3
4
5
6
7
8
9
10
)
C
C
A d
n
a
 C
U
A
( e
c
n
a
m
r
o
fr
e
P
Numbers of items 
Comp. AUC
FITB. ACC
Fig.5.4 Performance of our PS-OCM on two tasks for outﬁts with different numbers of items
• w/o Partial_Supervision: To explore the effect of the partially supervised attribute
embedding learning component, we removed the partial supervision loss by setting λ = 0
in Eq. (5.19).
• w/o Orthogonal: To study the effect of the orthogonal regularization during the visual
attributes disentanglement, we removed the orthogonal regularization Lor in Eq. (5.10).
• w/o Reconstruction: To validate the necessity of visual representation reconstruction
learning, we removed the visual representation reconstruction constraint Lrec in Eq.
(5.10).
• w/o Hierarchical_Graph: To validate the function of the hierarchical graph com-
patibility modeling component, we removed this part by directly concatenating the
attribute-level embeddings of each outﬁt to obtain the overall outﬁt representation and
passing it to an MLP to obtain the outﬁt’s compatibility score.
• Attribute-level_Only: To verify the importance of coarse-grained item-level informa-
tion, this derivative only utilizes the ﬁne-grained attribute-level compatibility modeling
part in the hierarchical graph compatibility modeling component.
• Item-level_Only: Similarly, to justify the necessity of introducing the ﬁne-grained
attribute-level compatibility modeling, we removed it from the hierarchical outﬁt com-
patibility modeling network.
Based on the ablation experiment illustrated in Table5.3, we found that our model con-
sistently outperforms all the above derivatives on both tasks, which demonstrates the effec-
tiveness of each component in our proposed PS-OCM. Speciﬁcally, we make the following
detailed observations.

5.4
Experiment
81
Table 5.3 Ablation study of our proposed PS-OCM on IQON3000 dataset. The best results are in
bold
Method
Compat. AUC
FITB ACC
w/o Partial_Supervision
0.8433
0.4866
w/o Orthogonal
0.8938
0.5293
w/o Deconvolution
0.8909
0.5293
w/o Hierarchical_Graph
0.8197
0.4459
Attribute-level_Only
0.8848
0.5337
Item-level_Only
0.8720
0.5292
PS-OCM
0.9009
0.5412
1. The performance of w/o Partial_Supervision signiﬁcantly drops, as compared to PS-
OCM, indicating that the partially supervised attribute embedding learning component
is indeed helpful to strengthen the visual representation learning performance.
2. Both w/o Orthogonal and w/o Reconstruction are inferior to PS-OCM, which suggests
that it is essential to consider the orthogonal regularization and visual feature reconstruc-
tion to prevent the visual information loss during the visual feature disentanglement and
guarantee the completeness of the disentanglement.
3. w/o Hierarchical_Graph delivers the worst performance, reﬂecting the overall effec-
tiveness of our proposed hierarchical outﬁt compatibility modeling component. More-
over, both Attribute-level_Only and Item-level_Only perform better than w/o Hierar-
chical_Graph, which conﬁrms the necessity of jointly incorporating the attribute-level
and item-level compatibility modeling modules. This also reﬂects that the ﬁne-grained
attribute-level features and the overview item-level features complement each other to a
certain level toward the outﬁt compatibility modeling.
As the partial-supervised attribute-level embedding learning contributes the key novelty
of our work, we further studied the effect of removing each attribute embedding from the
training phase of our PS-OCM. As previously mentioned, we had 12 attributes, including
11 speciﬁc attributes in the original dataset and one “residual” attribute we newly deﬁned.
Accordingly, we omitted each of the 12 attributes from our model, and hence obtained
12 derivatives of our model, with each named O_{each_attribute}. Figure5.5 shows the
performance of our PS-OCM and its derivatives on the two tasks. As can be seen, removing
any speciﬁc attribute (e.g., the design or color) hurts our model’s performance, which veriﬁes
that each speciﬁc attribute contributes to the outﬁt compatibility modeling. In particular, we
noticed that the color attribute greatly affects our model’s performance on both tasks, which
is reasonable, as the color attribute is the most straightforward inﬂuential factor in the outﬁt
compatibility modeling. We also found that O_residual underperforms our PS_OCM. This

82
5
Supervised Disentangled Graph Learning for OCM
0.875
0.88
0.885
0.89
0.895
0.9
0.905
AUC
(a) Comp. AUC
0.46
0.47
0.48
0.49
0.5
0.51
0.52
0.53
0.54
ACC
(b) FITB ACC
Fig.5.5 Comparison of the effect of removing a single attribute from our PS-OCM on two tasks
reﬂects the importance of the residual attribute and indicates its capability of compensating
for the information loss during the attribute representation disentanglement.
In addition, we studied the effect of the depth of the GCNs in our hierarchical outﬁt
compatibility modeling component. Figure5.6 shows the performance of our model with
the number of GCN layers ranging from 1 to 5. As can be seen, our model generally performs
stably when the number of GCN layers is no more than 3. However, when the number of
GCN layers continues to increase, our model’s performance decreases. This observation is
similar to that reported in [26], and can be attributed to more layers leading to the overﬁtting
problem hurting the model’s performance.
5.4.4
Case Study
To obtain an intuitive understanding of our model, we also conducted a case study of our
method in the two tasks: outﬁt compatibility estimation and ﬁll-in-the-blank.

5.4
Experiment
83
0.4
0.5
0.6
0.7
0.8
0.9
1
1
2
3
4
5
)
C
C
A d
n
a
 C
U
A
( e
c
n
a
m
r
o
fr
e
P
Number of GCN layers
Comp. AUC
FITB ACC
Fig.5.6 Inﬂuence of number of GCN layers on two tasks
Figure5.7a shows several testing examples of our model on the outﬁt compatibility esti-
mation task, where the importance distribution of attributes, i.e., the normalization of the
absolute values of the attribute-level compatibility scores, is also given to intuitively demon-
strate the interpretability of our model. As can be seen in the ﬁrst example, our model yields
the correct compatibility estimation and captures the color attribute as the most important
inﬂuential factor. This is reasonable as the color presented by the outﬁt is harmonious. In the
second example, our model also gives a high compatible probability score and identiﬁes that
the pattern attribute is the most important factor. As can be seen, the earrings and the dress
in the given outﬁt do consistently present the dotted pattern. Accordingly, the result makes
sense. In the last incompatible example, our PS-OCM gives a low compatibility score, and
the pattern attribute is also captured as the most important factor contributing to the incom-
patible estimation result. In this example, we found that the striped pattern of the T-shirt,
spotted pattern of the skirt, and ﬂoral pattern of the sandal form no compatible look.
Figure5.7b shows several testing examples of our model on the FITB task. In particular,
we employed the green tick to refer to the ground-truth item, and the green and red boxes
to indicate whether the choice of our model is correct or incorrect, respectively. We also
gave the compatibility estimated scores of our model for these candidate items under their
corresponding images, respectively. As can be seen from the ﬁrst two examples in Fig.5.7b,
our model chooses the correct items from the candidate item sets to form compatible outﬁts
with the given query items. Speciﬁcally, in the ﬁrst example, the outﬁt lacks a top in the
sporty style, and our PS-OCM correctly selects the ﬁrst candidate item by giving it the highest
compatibility score with the given query items. In the second example, our model chooses
the gray hat, which looks like the most compatible item with the given items as compared

84
5
Supervised Disentangled Graph Learning for OCM
Color
Price
Brand Category
Variety
Material
PaƩern
Design
Heel
Dress_length
Sleeve_length
Residual
0.25
0.2
0.15
0.1
0.05
0
PaƩern
0.25
0.2
0.15
0.1
0.05
0
Color
Price
Brand
Category Variety
Material
PaƩern
Design
Heel
Dress_length
Sleeve_length
Residual
0.2
0.15
0.1
0.05
0
0.2
0.15
0.1
0.05
0
Purple; 
Earth...;
Earrings; 
Score: 0.8776
Score: 0.0716
Purple; 
Merr...;
Long 
pants;
 
Purple;
L'AUTR...;
Sandal;
Shoes;
Purple;
CAYHA...;
Bracelet;
Stone;
Purple;
Free'sM...;
Clutch bag ;
Purple;
Samant...;
Ring; 
Stone; 
Red;
SMELLY;
Earrings;
Accessories;
Pink;
COLORSOFCA
Sandal;
Shoes;
Red;
TSUMORICHISATO;
T-shirt; Tops;
Stripe; Ruﬄe;
Black;
Emmi;
Long skirt;
Print;
GT: +
GT: +
GT: -
Score: 0.9999
Black;
RESEXXY;
Earrings;
Black;
NEORHYTHM;
Pumps; Shoes;
Ribbon; Heel;
Black;
UNBILLION;
Shoulder bag;
Leather;
Green;
GIAMBATTIST...;
One-piece dress;
Dot paƩern;
White;
COUPDECHANCE;
Belt;
Purple;
DURAS;
Hat;
Felt;
Blue;
NATUR...;
Tops;
Lace;
0.25
0.2
0.15
0.1
0.05
0
Color
Price
Brand
Category Variety
Material
PaƩern
Design
Heel
Dress_length
Sleeve_length
Residual
Black; Nike;
Long pants;
Black; Nike;
Jacket;
Blue; Nike;
Rucksack
Black; Nike;
T-shirt; Tops;
Print paƩern;
Black; 
BEAMSBOY;
T-shirt; Tops;
White;
AMERICANHOLIC;
T-shirt; Tops;
Gray; V-neck;
BEAMSLIGHTS;
T-shirt; Tops;
0.9483
0.0485
0.0060
0.0053
A.
B.
C.
D.
Red; Pinkadobe;
Earrings; Pearl;
Gray; COLEHAAN;
Boots;
Gray; 
COLEHAAN;
Bag;
Black; Hat; 
FREAK'SSTORE
Black; Hat;
Anatelier;
Gray; Hat;
CLEARIMPRESSION;
Gray; Hat;
AZULbymoussy;
0.6219
0.5150
0.9999
0.7367
A.
B.
C.
D.
Gray; 
NewBalance;
Sneaker;
Blue; Nike;
Hair accessories;
Black; Coat;
LETiROiRde...;
Green; Coat;
SPINNS; Jacket;
Purple; Coat;
16Arlington;
Black; Coat
Ungrid;
0.9991
0.9431
0.0013
0.9996
A.
B.
C.
D.
Black; Bag;
THEEMPORIUM;
Black; Shoes;
Heather;
White; Sweater;
INGNI;
Gray; Earrings;
Aquagirl;
Fig.5.7 Case study of our model on the a outﬁt compatibility estimation task, b ﬁll-in-the-blank task

References
85
to the other candidate items. Regarding the last example in Fig.5.7b, although our model
fails to give the correct answer by choosing the last candidate item as the most compatible
item, we noticed that the chosen item also goes well with the given query items. Overall,
these examples show the effectiveness of our model for outﬁt compatibility modeling.
5.5
Summary
In this chapter, for outﬁt compatibility modeling, we presented a novel partially super-
vised compatibility modeling, named PS-OCM, which consists of three key components:
(1) partially supervised attribute embedding learning, (2) disentangled completeness regu-
larization, and (3) hierarchical outﬁt compatibility modeling. In particular, we ﬁrst presented
a partially supervised disentangled learning method to disentangle the visual representation
of each item into several attribute-level embeddings, where the irregular attribute labels of
fashion items are used as the supervision to strengthen the visual representation learning of
items. In addition, we devised the disentangled completeness regularization, including the
orthogonal residual embedding and visual representation reconstruction, to prevent infor-
mation loss during disentanglement. Finally, we designed a hierarchical graph convolutional
network that jointly performs attribute- and item-level compatibility modeling. Extensive
experiments were conducted on a real-world dataset with two popular tasks: outﬁt com-
patibility prediction and ﬁll-in-the-blank. The encouraging experiment results validate the
superiority of our proposed model and the importance of each component. In addition, we
found that our PS-OCM is not sensitive to the number of items in the outﬁt, and removing
each attribute, including the introduced residual attribute, from the embedding disentangle-
ment will hurt the model’s performance. This shows that each attribute affects the outﬁt
compatibility modeling to some extent.
References
1. Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. 2019. Disentangled Graph
Convolutional Networks. In Proceedings of International Conference on Machine Learning,
Vol. 97, 4212–4221. PMLR.
2. Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019b. Learning Disen-
tangled Representations for Recommendation. In Advances in Neural Information Processing
Systems, 5712–5723.
3. YongfengZhang,GuokunLai,MinZhang,YiZhang,YiqunLiu,andShaopingMa.2014.Explicit
Factor Models for Explainable Recommendation Based on Phrase-Level Sentiment Analysis. In
Proceedings of the International ACM SIGIR Conference on Research and Development in
Information Retrieval, 83–92. ACM.
4. Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, and Mario Fritz.
2018. Disentangled Person Image Generation. In IEEE Conference on Computer Vision and
Pattern Recognition99–108. IEEE.

86
5
Supervised Disentangled Graph Learning for OCM
5. Chen, Hao, Yongjian Deng, Youfu Li, Tzu-Yi. Hung, and Guosheng Lin. 2020. RGBD Salient
ObjectDetectionViaDisentangledCross-ModalFusion.IEEETransactionsonImageProcessing
29 (2020): 8407–8416.
6. Yang, Fu.-En., Jing-Cheng. Chang, Chung-Chi. Tsai, and Yu-Chiang Frank. Wang. 2020. A
Multi-Domain and Multi-Modal Representation Disentangler for Cross-Domain Image Manip-
ulation and Classiﬁcation. IEEE Transactions on Image Processing 29 (2020): 2795–2807.
7. Linmei Hu, Siyong Xu, Chen Li, Cheng Yang, Chuan Shi, Nan Duan, Xing Xie, and Ming Zhou.
2020. Graph Neural News Recommendation with Unsupervised Preference Disentanglement. In
Proceedings of the Association for Computational Linguistics, 4255–4264. ACL.
8. XiangWang,HongyeJin,AnZhang,XiangnanHe,TongXu,andTat-SengChua.2020.Disentan-
gled Graph Collaborative Filtering. In Proceedings of the International ACM SIGIR Conference
on Research and Development in Information Retrieval, 1001–1010. ACM.
9. Na Zheng, Xuemeng Song, Qingying Niu, Xue Dong, Yibing Zhan, and Liqiang Nie. 2021.
Collocation and Try-on Network: Whether an Outﬁt is Compatible. In Proceedings of the Inter-
national ACM Conference on Multimedia, 309–317. ACM.
10. Weili Guan, Haokun Wen, Xuemeng Song, Chung-Hsing Yeh, Xiaojun Chang, and Liqiang Nie.
2021. Multimodal Compatibility Modeling via Exploring the Consistent and Complementary
Correlations. In Proceedings of the International ACM Conference on Multimedia, 2299–2307.
ACM.
11. Sun, Tiancheng, Yulong Wang, Jian Yang, and Hu. Xiaolin. 2017. Convolution Neural Networks
With Two Pathways for Image Style Recognition. IEEE Transactions on Image Processing 26
(9): 4102–4113.
12. Yupeng, Hu., Meng Liu, Su. Xiaobin, Zan Gao, and Liqiang Nie. 2021. Video Moment Local-
ization via Deep Cross-modal Hashing. IEEE Transactions on Image Processing 30 (2021):
4667–4677.
13. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for
Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 770–778. IEEE Computer Society.
14. Radford, Alec, Luke Metz, and Soumith Chintala. 2016. Unsupervised Representation Learning
with Deep Convolutional Generative Adversarial Networks. In International Conference on
Learning Representations, 1–15.
15. Zeyu Cui, Zekun Li, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Dressing as a whole: Outﬁt
Compatibility Learning Based on Node-wise Graph Neural Networks. In Proceedings of the
World Wide Web Conference, 307–317. ACM.
16. Guillem Cucurull, Perouz Taslakian, and David Vázquez. 2019. Context-Aware Visual Com-
patibility Prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 12617–12626. IEEE.
17. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. 2018. Graph Attention Networks. In Proceedings of the International Conference on
Learning Representations, 45–61. OpenReview.net.
18. Xuemeng Song, Xianjing Han, Yunkai Li, Jingyuan Chen, Xin-Shun Xu, and Liqiang Nie. 2019.
GP-BPR: Personalized Compatibility Modeling for Clothing Matching. In Proceedings of the
ACM International Conference on Multimedia, 320–328. ACM.
19. Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis. 2017. Learning Fashion Com-
patibility with Bidirectional LSTMs. In Proceedings of the ACM International Conference on
Multimedia, 1078–1086. ACM.
20. Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha Kumar, and
David A. Forsyth. 2018. Learning Type-Aware Embeddings for Fashion Compatibility. In Euro-
pean Conference on Computer Vision, 405–421. Springer.

References
87
21. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. ImageNet: A
Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 248–255. IEEE.
22. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Pro-
ceedings of the International Conference on Learning Representations, 1–15. OpenReview.net.
23. Reuben Tan, Mariya I. Vasileva, Kate Saenko, and Bryan A. Plummer. 2019. Learning Similarity
Conditions Without Explicit Supervision. In Proceedings of the IEEE International Conference
on Computer Vision, 10372–10381. IEEE.
24. Xingchen Li, Xiang Wang, Xiangnan He, Long Chen, Jun Xiao, and Tat-Seng Chua. 2020. Hier-
archical Fashion Graph Network for Personalized Outﬁt Recommendation. In Proceedings of the
International ACM SIGIR Conference on Research and Development in Information Retrieval,
159–168. ACM.
25. Tianyu Su, Xuemeng Song, Na Zheng, Weili Guan, Yan Li, and Liqiang Nie. 2021. Com-
plementary Factorization Towards Outﬁt Compatibility Modeling. In Proceedings of the ACM
International Conference on Multimedia, 4073–4081. ACM.
26. Xiaolin Chen, Xuemeng Song, Ruiyang Ren, Lei Zhu, Zhiyong Cheng, and Liqiang Nie. 2020.
Fine-Grained Privacy Detection with Graph-Regularized Hierarchical Attentive Representation
Learning. ACM Transactions on Information Systems 38 (4): 37:1–37:26.

6
Heterogeneous Graph Learning for Personalized
OCM
6.1
Introduction
One common limitation of our previous works presented in Chaps. 2, 3, 4, and 5 is that
they all evaluate the outﬁt compatibility from the general standard. In fact, there may be
some subjective factors inﬂuencing the outﬁt compatibility evaluation, namely, for the same
garment, different users may have different evaluations. In other words, different people
usually have different preferences to make their personal ideal outﬁts, which may be caused
by their diverse growing circumstances or educational backgrounds. For example, as shown
in Fig.6.1, given the same pink shirt, user A prefers to match it with a homochromatic skirt
and high-heeled shoes; whereas user B likes to coordinate it with casual jeans and white
sneakers. Therefore, personalized outﬁt compatibility modeling, called POCM, considering
users’ preferences when measuring the compatibility among fashion items, merit our spe-
cial attention. A few pioneer researchers have noticed this phenomenon and dedicated their
efforts to POCM [1–3]. These efforts study the user and item entities, as well as their rela-
tions. They, however, overlook another important entity type in POCM, namely, attributes.
Conveying rich semantics, attributes play a pivotal role in characterizing items and delivering
users’ preferences to items. For instance, we may express “I would like to buy a black coat
with a fur collar”, whereby the key information is conveyed via the semantic attributes. To
alleviate such a problem, we incorporate attributes associated with fashion items and work
toward fully exploring all the related entities (i.e., users, items, and attributes) and their var-
ious relations (i.e., user-item interactions, item-item matching relations, and item-attribute
association relations) to promote the POCM performance. Without loss of generality, we
speciﬁcally study the research problem of “which bottom (top) is compatible to the given
top (bottom) for a speciﬁc user”.
Addressing the aforementioned research is, however, nontrivial due to the following
challenges. C1: POCM involves three kinds of entities with heterogeneous contents: users,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
W. Guan et al., Graph Learning for Fashion Compatibility Modeling,
Synthesis Lectures on Information Concepts, Retrieval, and Services,
https://doi.org/10.1007/978-3-031-18817-6_6
89

90
6
Heterogeneous Graph Learning for Personalized OCM
Fig.6.1 Examples of users’ outﬁt compositions shared on the online fashion-oriented website
items, and attributes. Speciﬁcally, users are pure IDs, items are composed of images and
textual descriptions, while attributes are in the form of textual phrases. Thereby, how to
effectively organize these heterogeneous data seamlessly poses the ﬁrst research challenge.
C2: Different from the item and attribute entities, we do not have the speciﬁc content
information of user entities. The conventional user embedding paradigm usually assigns a
ﬁxed one-hot embedding or learnable embedding to represent each user. This is actually not
applicable to new users arriving during the testing phase, even for the case in which we have
the historical interactions of these new users. Accordingly, how to derive the user embedding
is another challenge. C3: In fact, apart from the direct relations, like the user-item interaction
relation, item-item matching relation, and item-attribute association relation, there are also
high-order relations among the three types of entities. For example, similar bottoms matching
the same top may share some common attributes. Another example is that users with similar
tastes tend to like items with similar attributes. Therefore, how to explore the high-order
relations among these entities to strengthen the model’s performance constitutes the third
challenge.
To address the challenge C1, we organize the users, items, and attributes in the context of
POCMintoauniﬁedheterogeneousgraph.Speciﬁcally,thesethreekindsofentitiesarenodes
of this graph. The nodes are linked by three kinds of edges, which are user-item interactions,
item-item matching relations, and item-attribute association relations. It is worth mentioning
that in this graph there is no direct edge linking the user and attribute entities. We then
devise a novel metapath-guided personalized compatibility modeling scheme to address C2
and C3, named as MG-POCM, as shown in Fig.6.2. This scheme consists of three key

6.2
Related Work
91
Fig. 6.2 Illustration of the proposed MG-POCM scheme. It consists of three key components: (1)
heterogeneous graph node embedding, (2) metapath-guided heterogeneous graph learning, and (3)
personalized outﬁt compatibility modeling
components: heterogeneous graph node embedding, metapath-guided heterogeneous graph
learning, and personalized outﬁt compatibility modeling. The ﬁrst component works on
embedding each type of entity in our heterogeneous graph. To represent users, we devise
a multimodal content-oriented user embedding module, which derives the user embedding
based on the multimodal contents of his/her interacted items, a straightforward cue indicating
the user’s preference. As to the second component, we ﬁrst deﬁne multiple user-oriented
and item-oriented metapaths (e.g., User →Item →User and Item →Attribute →Item) to
capture the high-order relations among entities, which naturally resolves the third challenge
C3. Thereafter, we conduct the multiple metapath-guided heterogeneous graph learning
to obtain the multiple semantic-enhanced user/item embedding of each user/item, whereby
each metapath corresponds to a speciﬁc semantic. A transformer [4] is used to adaptively fuse
the semantic-enhanced user/item embeddings under different metapaths for each user/item.
Ultimately, in the last component, in addition to the typical cross-entropy loss, we also
introduce the contrastive regularization to enhance embedding learning.
6.2
Related Work
This work is related to personalized compatibility modeling and heterogeneous graph learn-
ing.
Personalized Compatibility Modeling. Despite the signiﬁcant progress made by previ-
ous outﬁt compatibility modeling efforts, they purely focus on the general item-item com-
patibility and overlook users’ preferences in the fashion compatibility estimation. In fact,

92
6
Heterogeneous Graph Learning for Personalized OCM
for the same fashion outﬁt, different users may have different evaluation results. Inspired by
this, some studies have resorted to personalized outﬁt compatibility modeling. For example,
a personalized compatibility modeling scheme for personalized clothing matching, named
GP-BPR, is presented in [1], which jointly considers the general (item-item) compatibility
and personal (user-item) preference for personalized clothing matching. Both the image
and context description of items are utilized in the comprehensive modeling. Moving a
step forward, Sagar et al. [3] introduced an attributewise interpretable personal preference
modeling scheme to strengthen the model interpretability whereby the images and textual
descriptions of items are explored. Additionally, Li et al. [2] developed a hierarchical fash-
ion graph network to simultaneously model the rich relationships among users, items, and
outﬁts.
Although these efforts have achieved compelling success, they overlook the item
attributes when estimating compatibility. Attributes express the key item semantics items
and reﬂect the speciﬁc user preferences. As a complementary effort, in this book, we incor-
porate the attribute entities and their semantic contents to comprehensively study the POCM
problem.
Heterogeneous Graph Learning. Due to the ubiquity of heterogeneous graphs in the
real-world setting, containing multiple types of nodes and relations among these nodes [5,
6], increasing research efforts have been dedicated to heterogeneous graph learning. In a
sense, existing methods focus on the heterogeneous graph embedding via learning a power-
ful low-dimensional vector representation for each node to beneﬁt the potential downstream
applications, such as node classiﬁcation [7, 8] and personalized recommendation [9, 10]. To
accomplish this task, previous methods mostly rely on the metapath [11], i.e., a sequence
of node and edge types, delivering certain semantic information of the graph. For example,
Dong et al. [12] developed metapath-based random walks to construct the heterogeneous
neighborhood of a node and then utilized a skip-gram model [13] to perform node embed-
dings. One key limitation of this method is that it only utilizes a single metapath, which may
be insufﬁcient to cover all useful information. To address this issue, Shi et al. [14] designed
a novel strategy to generate the meaningful node sequences and utilized fusion functions to
learn node representation. In addition, Zhang et al. [15] introduced a heterogeneous graph
neural network model, named HetGNN, to jointly explore the heterogeneous structures and
contents of each node. To obtain superior node representation, several researchers [16–18]
have utilized the attention mechanism to select the most useful metapath. For example,
Wang et al. [17] proposed a heterogeneous graph attention network, which incorporates
both node- and semantic-level attention to learn the importance of nodes and metapaths in
the node embedding. Subsequently, Zhang et al. [18] proposed an attentive heterogeneous
graph neural network for heterogeneous graph embedding, where the node-level attention
is considered, and a semantic-level neural network is utilized rather than the semantic-level
attention for capturing the feature interaction among node embeddings under different meta-
paths. Differently, Xing et al. [19] regarded each metapath as a speciﬁc view, and borrowed
the idea of multiview learning to comprehensively encode the node representations of dif-

6.3
Methodology
93
ferent views into a latent representation. To address the practical issue of missing attributes,
Jin et al. [20] proposed a general framework for heterogeneous graph neural network via
attribute completion, comprising two key components: prelearning topological embedding
and attribute completion with attention mechanism.
Inspired by the great success of these methods on heterogeneous graph learning, we
seamlessly organize the various entities and relations in the context of POCM into a uniﬁed
heterogeneous graph. It is worth emphasizing that we design task-speciﬁc metapaths and
creatively incorporate the transformer to fuse the semantic-enhanced user/item embeddings.
6.3
Methodology
In this section, we ﬁrst formulate the research problem and then detail the three components
of our proposed MG-POCM scheme.
6.3.1
Problem Formulation
Formally, we ﬁrst clarify the notations. We use bold uppercase letters (e.g., W) and bold
lowercase letters (e.g., b) to represent matrices and vectors, respectively. All vectors are in
column forms. Additionally, we employ nonbold letters (e.g., W and W) to denote scalars
and Greek letters (e.g., α) to represent regularization parameters.
In this work, we focus on fulﬁlling the task of POCM. Without loss of generality, we study
the particular problem of “whether the given bottom (top) matches the given top (bottom)
and together compose a favorable outﬁt for the given user”. Assume that we have a set of
Nu users U = {u1, u2, . . . , uNu}, and a set of Nm items M = {m1, m2, . . . , mNm}. For an
arbitrary item mi, i = 1, 2, . . . , Nm, it is composed of an image vi, a textual description
ti, and a set of attributes Ai ⊆A, where A = Nm
i=1 Ai = {a1, a2, . . . , aNa} represents the
entire attribute set in the form of semantic phrases, like red color, wool material, and V-neck
design. The symbol Na denotes the total number of attributes in our dataset. To simplify
the formulation, in this work, we only consider the tops and bottoms. Therefore, the set
of items can be rewritten as M = Mt ∪Mb, where Mt and Mb refer to the sets of tops
and bottoms, respectively. Each user u is historically associated with a set of top-bottom
pairs X u = {(mu
t1, mu
b1), (mu
t2, mu
b2), . . . , (mu
tMu , mu
bMu )}, where mu
t∗∈Mt, mu
b∗∈Mb, and
Mu denotes the total number of interacted top-bottom pairs by the user u. We resort to
a heterogeneous graph to organize the complicated entities and relations within a uniﬁed
structure. In particular, we denote the graph as G = (E, R), where E = U ∪M ∪A denotes
the set of entity nodes, consisting of user entities, item entities, and attribute entities, while R
denotes the set of edges linking nodes to characterize various relations among entities, i.e.,
user-itemhistoricalinteractions,item-attribute associationrelations,anditem-item matching
relations. Ultimately, we work in learning the following compatibility estimation function,

94
6
Heterogeneous Graph Learning for Personalized OCM
pk
i j = F(mk ∈Mb(t)|ui, m j ∈Mt(b)),
(6.1)
where pk
i j denotes the compatibility degree of a bottom (top) mk to the given top (bottom)
m j for the user ui.
6.3.2
Metapath-Guided Personalized Compatibility Modeling
As illustrated in Fig.6.2, MG-POCM consists of three components: (1) heterogeneous graph
node embedding, (2) metapath-guided heterogeneous graph learning, and (3) personalized
outﬁt compatibility modeling. In this subsection, we explain each of them.
Heterogeneous Graph Node Embedding.
This component aims to derive the initial node-level representations in the heterogeneous
graph. The heterogeneous graph has three types of entities and the node contents differ
remarkably. Therefore, we learn their embeddings separately as shown in Fig.6.3.
Item Entity Embedding. Each item entity is composed of an image and a textual descrip-
tion. The multimodal cues of each item mutually complement each other. For the arbitrary
item mi, regardless of its category (i.e., top or bottom), we utilize the ResNet, which has
shown compelling success in many computer vision tasks [21], to extract its visual feature.
We adopt the pretrained BERT to obtain its textual feature1 due to its prominent perfor-
mance in textual representation learning [22]. Speciﬁcally, we employ the averaged hidden
states corresponding to the special token attached at the beginning of the input sequence,
i.e., [CLS], of the last two layers of BERT as the textual description representation. Finally,
we concatenate the visual and textual features of each item to derive the ﬁnal item embed-
ding, and use a learnable fully-connected layer to project the item embedding into a lower
dimensional space. Mathematically, we have
⎧
⎪⎨
⎪⎩
evi = ResNet (vi) ,
eti = BERT (ti)[CLS] ,
emi = ft

[evi , eti ]

,
(6.2)
where evi ∈RDv and eti ∈RDt refer to the visual and textual embedding of the item mi,
respectively.Accordingly,thesymbols Dv and Dt arethedimensionsofthevisualandtextual
embeddings, respectively. ResNet and BERT denote the corresponding neural networks. [, ]
refers to the concatenation operation, ft denotes the learnable fully-connected layer, and
emi ∈RD is the ﬁnal embedding of the item mi.
Attribute Entity Embedding. To fully utilize the semantic content of each attribute
entity, we also resort to the pretrained BERT with a learnable fully-connected layer to derive
its embedding instead of using the one-hot vector or treating it as the learnable parameter.
1 Before feeding a text into the BERT, the text is ﬁrst tokenized into standard vocabularies.

6.3
Methodology
95
Fig.6.3 Illustration of the heterogeneous graph node embedding component
Notably, for attribute embedding, we only adopt the representation of the special token
[CLS] from the last layer of BERT due to its shorter length, as compared with the textual
description. Formally, for each attribute entity al, we obtain its embedding as follows,
eal = fa(BERT (al)[CLS]),
(6.3)
where eal ∈RD denotes the initial embedding of the attribute entity al, and fa denotes the
fully-connected layer for embedding ﬁne-tuning.
User Entity Embedding. Instead of using the one-hot embeddings, we resort to aggre-
gating all the embeddings of the user’s one-hop neighbor nodes (i.e., all the items interacted
by the user before) to derive the initial embedding of each user entity. The underlying
philosophy is two-fold: (1) the items that are historically interacted by users signal users’
preferences and tastes, and (2) the embedding of a cold-start user can also be derived as long
as his/her interacted items appeared before. Speciﬁcally, we determine the user embedding
below,
eui =
1
|N ui |
	
mi∈N ui
emi ,
(6.4)
where eui ∈RD denotes the user embedding ui, and N ui refers to the set of one-hop neigh-
bors of the user entity ui.
Metapath-guided Heterogeneous Graph Learning
In this component, we conduct the metapath-guided heterogeneous graph representation
learning to reﬁne each entity’s embedding with their context information. In particular, we

96
6
Heterogeneous Graph Learning for Personalized OCM
Fig.6.4 Illustration of user-oriented and item-oriented metapaths via heterogeneous graph
ﬁrst deﬁne a few user-/item-oriented metapaths to capture the high-order relations among
entities, and then perform the metapath-guided semantic propagation to derive multiple
semantic-enhanced embeddings for each user/item entity. Therein, each applicable meta-
path corresponds to a speciﬁc semantic-enhanced embedding. Ultimately, we fuse all the
semantic-enhanced embeddings via a transformer to obtain the ﬁnal user/item representa-
tion.
User-/Item-oriented Metapath Deﬁnition. According to [11], a metapath is deﬁned
as a path in the form of X1
R1
→X2
R2
→· · ·
Rn
→Xn+1, which describes a composite relation
between entities. In our work, as illustrated in Fig.6.4, there are actually various metapaths
residing in our constructed heterogeneous graph, whereby three entities and rich relations
exist. Intuitively, different metapaths reﬂect different semantics. For example, the metapath
UIA2 implies that a user historically prefers an item and that item possesses an attribute,
while UIU indicates that these two end users like the same fashion item. Analogously, the
metapath IAI refers to that the two end items share the same attribute, while IUI conveys
that the two end items are interacted with by the same user. Pertaining to the POCM context,
we only adopt metapaths that start from user entities and item entities. Formally, let Puser =
{r1, . . . ,rY } and Pitem = {s1, . . . , sZ} denote the set of predeﬁned user-oriented and item-
oriented metapaths, respectively. Y and Z denote the total number of user-oriented and
item-oriented metapaths, respectively.
Metapath-guided Semantic Propagation. Based on the predeﬁned user- and item-
oriented metapaths, we can derive the corresponding metapath-guided user-oriented sub-
graphs for each user entity and the item-oriented subgraphs for each item entity via the
breadth-ﬁrst search strategy. Thereafter, based on the different information encoded by dif-
ferent metapaths, we can learn the user/item entity’s embeddings with different semantics.
2 Due to the limited space, we omit the relation types between entities.

6.3
Methodology
97
To intuitively clarify how to reﬁne users’ or items’ embeddings, we take the metapath UIA
as an example. Other metapath-guided learning repeats the same procedure.
Assume that the metapath UIA is applicable to the user entity ui. We then build a subgraph
GUIA
ui
for the user entity ui. Since the length of the metapath UIA is three, we denote
the one-hop neighbors of user entity ui as N UIA(1)
ui
consisting of all the items the user
once interacted with. In the same way, we denote the two-hop neighbors of the user entity
ui as N UIA(2)
ui
, comprising all the attributes associated with items in N UIA(1)
ui
. Following
that, we ﬁrst aggregate the information from the two-hop neighbors to enhance the one-
hop neighbors’ embeddings, and then based on that learn the user’s semantic-enhanced
embedding as follows:
⎧
⎪⎨
⎪⎩
eUIA
mi
= H

eal|al ∈N UIA(2)
ui

, mi ∈N UIA(1)
ui
,
hUIA
ui
= H

eUIA
mi |mi ∈N UIA(1)
ui

,
(6.5)
where H is the aggregation function, and eUIA
mi
denotes the semantic-enhanced embedding
of the item entity mi, which is an one-hop neighbor of the user entity ui. hUIA
ui
represents the
semantic-enhanced embedding of the user entity ui. It is worth noting that during each hop
aggregation, different neighbors may play different roles in characterizing the center entity.
Speciﬁcally, some attributes may be more important in conveying the item’s properties,
while some items may contribute more in reﬂecting the user’s preference. Therefore, we
adopt the graph attention mechanism of GAT [23] as the aggregation function, to highlight
the informative and meaningful neighbor nodes. For simplicity, we take the aggregation
operation over the one-hop neighbors of the user entity ui as an example, and that over the
two-hop neighbors can be deﬁned similarly. Speciﬁcally, the aggregation operation H over
the one-hop neighbors of the user entity ui can be written as follows,
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
hUIA
ui
= eui + σ
⎛
⎜⎝
	
m j∈N UIA
ui
αi jem j
⎞
⎟⎠,
αi j =
exp

σ

WUIA[eui , em j ]


m j∈N UIA
ui
exp

σ

WUIA[eui , em j ]
,
(6.6)
where σ(·) denotes the activation function, [, ] refers to the concatenation operation, and
WUIA ∈R2D∗1 is the node-level attention vector for the information aggregation under the
metapath UIA.
Theoretically, repeating the above process for semantic propagation for all the other
user-oriented metapaths, we can derive Y semantic-enhanced user embeddings for user
entity ui. However, in practice, not every user-oriented (item-oriented) metapath can be
applied to a given user (item) entity. For example, once a user shares no preferred item with
other users, we cannot derive the subgraph according to the metapath UIU. Accordingly,

98
6
Heterogeneous Graph Learning for Personalized OCM
we use Pui = {ri1, . . . ,riYi } to denote the set of metapaths that can be applied to the user
entity ui, where Yi refers to the total number of metapaths applicable to the user ui, and
rin ∈Puser, n = 1, . . . , Yi. Based upon Pui , we can derive the corresponding semantic-
enhanced embeddings for the user entity ui, termed as {hp
ui |p ∈Pui }, following the above
metapath-guided semantic propagation process. Similarly, we use Pmi = {si1, . . . , siZi } to
denote the set of metapaths that can be applied to the item entity mi, where Zi is the
total number of metapaths applicable to the item mi, and siz ∈Pitem, z = 1, . . . , Zi. In
the same manner, we reach the semantic-enhanced embeddings for the item entity mi as
{hp
mi |p ∈Pmi }.
Semantic-enhanced Embedding Fusion. Thus far, we have achieved multiple semantic-
enhanced embeddings for each user and item entity under different metapaths, and each
embedding characterizes one aspect. To comprehensively represent each user or item, we
propose fusing the multiple embeddings of each user or item.
In particular, we leverage the transformer [4] without the positional coding to perform
the multisemantic embedding fusion due to the following two concerns: (1) the number
of semantic-enhanced embeddings for different users can be different, and (2) there is no
explicit order among these semantic-enhanced embeddings of each user or item entity. To
ensure that the fused embeddings of the users and items are in the same space, we adopt a
single transformer to fulﬁll both user and item entities’ embedding fusion as follows,

˜hui = Transformer(hp
ui |p ∈Pui )
˜hmi = Transformer(hp
mi |p ∈Pmi )
(6.7)
where ˜hui and ˜hmi are the ﬁnal representation for the user ui and item mi, respectively.
Personalized Outﬁt Compatibility Modeling
To accomplish the POCM task, we ﬁrst build the training set  = {(ui, m j,
mk+, mk−)|m j ∈Mt(b), mq+, mq−∈Mb(t), yk+
i j
= 1, yk−
i j = 0}, where yk+
i j
= 1 denotes
the triplet (ui, m j, mk+) is compatible, i.e., the item mk+ goes well with the given item m j
according to user ui’s preference. yk−
i j
= 0 indicates that the triplet (ui, m j, mk−) is incom-
patible. Following that, for each triplet, we obtain each entity’s representation according
to Eq. (6.7), namely, ˜hui , ˜hm j , and ˜hmk+/ ˜hmk−. We then resort to the MLP to derive the
compatibility score for each triplet as follows,
ˆpk+(−)
i j
= MLP0([˜hui , ˜hm j , ˜hmk+(−)]),
(6.8)
where ˆpk+(−)
i j
is the predicted compatibility score for the given triplet. We then adopt the
cross-entropy loss as follows,
L(i, j,k+,k−) = −log

exp( ˆpk+
i j )
exp( ˆpk+
i j ) + exp( ˆpk−
i j )

.
(6.9)

6.4
Experiment
99
Intuitively, the compatible and incompatible triplets should follow some compati-
ble and incompatible patterns, respectively. In light of this, given a compatible triplet
(ui+, m j+, mk+), we argue that its latent representation should be more similar to that of
a compatible triplet as compared to that of an incompatible one (ui−, m j−, mk−). Accord-
ingly, we further introduce contrastive regularization to regulate the similarity between
latent representations of different triplet pairs. Assume that p+
1 = (ui+
1 , m j+
1 , mk+
1 ) and
p+
2 = (ui+
2 , m j+
2 , mk+
2 ) are two compatible triplets, while n−= (ui−, m j−, mk−) is an
incompatible triplet. We utilize two MLPs to obtain the latent representations for these
three triplets as follows,
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
˜hp+
1 = MLP1([˜hui+
1 , ˜hm p+
1 , ˜hmq+
1 ]),
˜hp+
2 = MLP2([˜hui+
2 , ˜hm p+
2 , ˜hmq+
2 ]),
˜hn−= MLP2([˜hu−
i , ˜hm p−, ˜hmq−]),
(6.10)
where ˜hp+
1 and ˜hp+
2 are the latent representations of the two compatible/positive triplets,
while ˜hn−is the latent representation of the incompatible/negative triplet. We then use the
following contrastive regularization as follows,
L
(p+
1 ,p+
2 ,n−)
cons
= −log
exp(sim(˜hp+
1 , ˜hp+
2 ))
exp(sim(˜hp+
1 , ˜hp+
2 )) + exp(sim(˜hp+
1 , ˜hn−))
,
(6.11)
where sim(, ) refers to the dot product operation. Finally, our objective function can be
written as follows,
L =
	
(ui,m j,mk+,mk−)
L(i, j,k+,k−) + λ
	
(p+
1 ,p+
2 ,n−)
L
(p+
1 ,p+
2 ,n−)
cons
,
(6.12)
where λ is the nonnegative hyperparameter balancing the importance of the cross-entropy
loss and contrastive regularization.
6.4
Experiment
In this section, we conducted experiments over real-world datasets by answering the fol-
lowing research questions.
• RQ1: Does MG-POCM outperform state-of-the-art baselines?
• RQ2: How does each module affect MG-POCM?
• RQ3: Is our model sensitive to the number of the transformer and GAT layers?
• RQ4: What is the intuitive performance of MG-POCM?

100
6
Heterogeneous Graph Learning for Personalized OCM
6.4.1
Experimental Settings
In this part, we present the dataset, evaluation tasks, metrics, and the implementation details.
Dataset.
To justify our model, similar to existing methods [1, 3], we also resorted to the public
benchmark dataset IQON3000 [1], due to the fact that each item in IQON3000 has not only
the visual image and textual description but also the semantic attributes, such as the color
and category. In particular, IQON3000 consists of 308, 747 outﬁts, composed by 672, 335
items. To ﬁt our task and ensure the quality of the dataset, we did not completely follow up
the experimental setting in [1, 3] considering the following two concerns. (1) As to a given
user, they only focus on matching bottoms for a given top. By contrast, in our work, the
top and bottom are arbitrarily switchable for a given user. That is, we aim to match either
tops for a given bottom or bottoms for a given top. And (2) they did not set the criterion for
ﬁltering out users with limited interacted items. Accordingly, we derived our dataset from
IQON3000. In particular, we only retained the outﬁts that contained a top and a bottom,
and users who had interacted with no less than 10 and no more than 200 outﬁts to keep
the dataset relatively balanced. Finally, there were 82, 079 user-top-bottom triplets involved
1, 769 users. The detailed statistics are summarized in Table6.1. The attributes and their
corresponding value examples of the derived dataset are shown in Table6.2.
Notably, all these retained triplets are positive, namely, compatible triplets. We then ran-
domly split these user-top-bottom triplets into four chunks: graph construction set, training
set, validation set, and testing set, by the ratio of 6 : 2 : 1 : 1, resulting in a 49, 297 triplet
for constructing the heterogeneous graph, 16, 416 triplets for training, 8, 208 triplets for
validation, and 8, 208 triplets for testing. Thereafter, as to each positive triplet in the training
set, validation set, or testing set, we randomly selected an item (either the top or the bottom)
from this triplet as the given item, leaving the other item as the target (positive). Following
Table 6.1 Statistics over our newly constructed dataset based upon IQON3000
Data
Count
User
1, 769
Top
53, 092
Bottom
41, 157
Attribute
98
Outﬁt (top-bottom)
81, 937
Triplet (user-top-bottom)
82, 079
User historical interacted outﬁts-min
10
User historical interacted outﬁts-max
200
User historical interacted outﬁts-avg
46

6.4
Experiment
101
that, we replaced the target (positive) item with a randomly sampled item sharing the same
category as the target item, to derive a negative triplet. It is worth noting that to ensure
fairness, considering the baseline methods do not need the speciﬁc graph construction set,
we trained them with both the graph construction set and training set, where the negative
triplets in the graph construction set were derived in the same manner.
Evaluation Tasks and Metrics
Similar to previous studies [1, 3, 24–26], we justiﬁed our proposed MG-POCM scheme
via the compatibility estimation task. This task is to evaluate the compatibility score of an
arbitrary top-bottom pair for a speciﬁc user, where we adopted the AUC (area under the
ROC curve) [27] as the evaluation metric. In addition, we evaluated the performance of our
model with the complementary item retrieval task [1]. For each positive triplet, we derived
a corresponding negative triplet by randomly replacing one item (either a top or a bottom).
We merged the original replaced item in the positive triplet and the newly added item in the
negative triplet as the set of item candidates. These item candidates were ranked according to
their compatibility scores to the given user and item, i.e., the unchanged item in the original
positive triplet, calculated by Eq. (6.8). To measure the complementary item retrieval task,
we utilized the mean reciprocal ranking (MRR) [28] as the metric.
Implementation Details
Pertaining to the visual embedding of items, we utilized ResNet18 and converted each item
image into a 512-D vector. Notably, ResNet18 was also ﬁne-tuned with the whole model.
Regarding the textual feature extraction of items, we implemented BERT3 for Japanese text
considering our dataset is in Japanese and embedded each item’s textual description into a
768-D vector. The dimension of the ﬁnal item embedding was D = 512. Similarly, using
this BERT implementation, each semantic attribute was also embedded into a 768-D vector.
Table 6.2 Attributes and their possible value examples in the derived dataset
Attribute
Possible value examples
Total number
Color
Gray, Black, Red, · · ·
12
Price
Low, Middle, High.
3
Category
Coat, Skirt, Jacket, · · ·
12
Variety
Tops, Dress, Trousers, · · ·
5
Material
Fur, Leather, Denim, · · ·
31
Pattern
Stripe, Print, Dot, · · ·
15
Design
Frill, V-neck, Ribbons, · · ·
13
Dress length
Short, Middle, Long.
3
Sleeve length
Sleeveless, Long, Short, · · ·
4
3 https://huggingface.co/cl-tohoku/bert-base-japanese-char/tree/main.

102
6
Heterogeneous Graph Learning for Personalized OCM
We set the number of layers of all MLPs used in our scheme as 2 and employed Gaussian
error linear units (GELU) as the activation function. In practice, we adopted the following
set of user-oriented metapaths Puser = {UIAIU, UIU, UIA}, and item-oriented metapaths
Pitem = {IAI, IUI, II, IIA}. During the subgraph construction for each user/item entity, for
efﬁciency, we set the maximum neighbor size of each node as 5. As to the optimization,
we adopted the adaptive moment estimation method (Adam [29]). The learning rate was
prepared in the 6% steps to the peak value, which were set to 1e-4, and then linearly decayed
to 0. The hyperparameter λ was set to 1, and the batch size was set to 24. All the experiments
were implemented by PyTorch over a server equipped with 4 A100-PCIE-40GB GPUs.
6.4.2
Model Comparison
To validate the effectiveness of our proposed scheme, we chose the following state-of-the-art
baselines for comparison.
• GP-BPR [1] is a comprehensive personal preference modeling scheme, where the mul-
timodal data (e.g., the image and text description) of fashion items are jointly explored.
• PAI-BPR [3] is an attributewise interpretable compatibility modeling scheme, which
solves the problem of interpretability in clothing matching by locating the discordant and
harmonious attributes between fashion items.
• HFGN [2] refers to a hierarchical fashion graph network, which simultaneously models
the relationships among users, items, and outﬁts.
Table6.3 shows the performance comparison among different methods on IQON3000
dataset in terms of AUC and MRR. From this table, we make the following observations.
(1) our proposed MG-POCM scheme consistently outperforms all baseline methods over
different metrics. In particular, MG-POCM performs better than PAI-BPR and GP-BPR,
which indicates the advantage of our scheme that organizes the various entities and relations
in the context of POCM into a uniﬁed heterogeneous graph and utilizes the metapath-
guidedheterogeneousgraphlearningtowardspersonalizedoutﬁtcompatibilitymodeling.(2)
Our method surpasses the heterogeneous graph-based method HFGN remarkably over both
metrics, implying the necessity of considering the items’ attributes. (3) GP-BPR outperforms
HFGN, which may be because HFGN only utilizes the visual information of fashion items,
while GP-BPR jointly considers the images and textual descriptions of items.
6.4.3
Ablation Study
To verify the importance of each component in our model, we conducted ablation experi-
ments on the following derivatives.

6.4
Experiment
103
Table 6.3 Performance comparison between our proposed MG-POCM and other baseline methods
in terms of AUC and MRR over IQON3000. The best results are in bold, while the second best results
are underlined
Approaches
PAI-BPR
HFGN
GP-BPR
MG-POCM
AUC
0.6096
0.6783
0.7146
0.7730
MRR
0.5456
0.6173
0.6346
0.6427
Table 6.4 Ablation study of our proposed MG-POCM on IQON3000 dataset. The best results are
in bold
Method
AUC
MRR
w/o text
0.7630
0.6392
w/o image
0.7655
0.6382
w/o attribute
0.7339
0.5717
w/o (II,UIA)
0.7639
0.6392
w/o contrastive
0.7647
0.6338
w mean pooling
0.7627
0.6392
MG-POCM
0.7730
0.6427
• w/o text: To study the impact of the textual description of fashion items in POCM, we
removed the textual embeddings of items, and kept other parts unchanged.
• w/o image: Similarly, to justify the necessity of incorporating the item images in the con-
text of POCM, we omitted the items’ visual embeddings, and kept other parts unchanged.
• w/o attribute: To verify the importance of the item attributes, we discarded the attribute
entities as well as the attribute-related metapaths. The rest of our MG-POCM was
unchanged.
• w/o (II,UIA): To validate the necessity of incorporating the metapaths II and UIA, which
can be treated as the subpaths of metapaths i.e., IIA and UIAIU, respectively, we omitted
them during our heterogeneous graph learning.
• w/o contrastive: To explore the effect of the contrastive regularization component, which
is used to enhance the latent representation of each entity, we removed the contrastive
regularization by setting λ = 0 in Eq. (5.13).
• w/ mean pooling: To evaluate the function of the transformer component in the seman-
tic embedding fusion, we replaced the transformer component with the mean pooling
function.
Table6.4 summarizes the ablation study results. From this table, we observed that our
model consistently outperforms all the above derivatives, which demonstrates the effective-
ness of each component in our proposed MG-POCM. Speciﬁcally, we make the following

104
6
Heterogeneous Graph Learning for Personalized OCM
detailed observations. (1) Both w/o text and w/o image perform inferior to MG-POCM,
which suggests that it is essential to consider both modalities of fashion items to boost the
item representation learning. (2) w/o attribute presents the worst performance, reﬂecting the
beneﬁt of incorporating the attribute entities as well as their semantic contents into person-
alized outﬁt compatibility modeling. (3) w/o (II,UIA) performs worse than our MG-POCM,
which suggests subpaths may emphasize the high-order correlations without adding noisy
information. (4) The performance of w/o contrastive drops by a large margin, as compared
to MG-POCM, indicating that the contrastive regularization is indeed helpful to strengthen
the fashion entity representation learning. (5) w/ mean pooling also performs worse than our
MG-POCM, reﬂecting the effectiveness of the transformer in fusing the unﬁxed number of
semantic-enhanced embeddings of users/items.
6.4.4
Sensitivity Analysis
In this part, we evaluated the sensitivity of our model in terms of the number of transformer
and GAT layers. In particular, we varied the number of transformer layers from 1 to 5 with
the step size of 1. Considering that most of our metapaths involve more than 2 entities, we
changed the number of GAT layers from 2 to 6 with the step of 1. Figure6.5 a and b illustrate
the performance of our model on the validation set and testing set with different numbers
of transformer layers and GAT layers, respectively. As can be seen, our model achieves
relatively stable performance with different numbers of transformer and GAT layers, which
impliesthatourmodelisnotsensitivetothesetwohyperparameters.Accordingly,inpractice,
to improve the model efﬁciency, we set the number of transformer and GAT layers as 1 and
2, respectively.
Fig.6.5 Sensitivity analysis of our model performance in terms of AUC with respect to a the number
of transformer layers, and b the number of GAT layers

6.4
Experiment
105
User History Preference
Given Item
Positive Item
Negative Item
User 1
User 2
User 3
0.8164
0.1836
MG-POCM
0.7388
0.2612
Brown
Skirt
Gray
Blouse
Floral pattern
Ruffle
Black
Blouse
Suede
w/o attribute
MG-POCM
w/o attribute
MG-POCM
w/o attribute
0.5332
0.4668
0.5742
0.4258
Beige
Cardigan
Tops
Wool
White
Long pants
Stripe
Black
Skirt
Floral pattern
0.6714
0.4639
0.3283
0.5361
Beige
Blouse
Tops
Black
Long skirt
Green
Long pants
Fig.6.6 Illustration of several POCM results obtained by our MG-POCM and w/o attribute derivative
6.4.5
Case Study
To gain more intuitive insights into our model, we also conducted a case study of our
method and the w/o attribute derivative. Figure6.6 shows three testing samples, where the
users’ historical preferred top-bottom pairs and items’ attributes are also listed to facilitate
the experimental result analysis.4 As can be seen, for the case of the ﬁrst user with the
given brown skirt, although both our MG-POCM and its derivative w/o attribute gives the
correct prediction, our MG-POCM assigns a much higher score to the positive item than
the negative item. By contrast, w/o attribute gives the former a slightly higher score than
the latter item. Namely, our model has a high conﬁdence than its derivative. This may be
because incorporating the attribute entities in FPCM enables our model MG-POCM to learn
the “ﬂoral pattern” that the user prefers for tops, and accordingly gives the positive item
with the “ﬂoral pattern” a higher score. Similarly, the same phenomenon can be observed
in the second case. As can be seen, the second user tends to prefer bottoms of the category
“long pants” to match tops, which can be more easily captured by our MG-POCM rather
than the w/o attribute derivative. Additionally, as the negative item is a black skirt, which
does not go well with the beige cardigan, our MG-POCM assigns a much higher score to
the positive item, while w/o attribute only rates a slightly higher score to it, as compared
with the negative item. In the last case, we can see that the third user prefers “long skirts”
with blouses. The positive item is a black long skirt, looking like long pants, while the
negative item is long pants looking like a long skirt. Then with the help of their category
attributes, our MG-POCM correctly selects the compatible item for the given top, while the
4 Due to the limited space, we did not provide the text description of the items.

106
6
Heterogeneous Graph Learning for Personalized OCM
w/o attribute method gives the incorrect judgment. Overall, based on these case studies, we
can conﬁrm the effectiveness of our method in POCM, and the beneﬁt of incorporating the
attribute information in the context of POCM.
6.5
Summary
In this chapter, we solved the personalized outﬁt compatibility modeling problem by orga-
nizing the various fashion entities and relations into a uniﬁed heterogeneous graph and pre-
sented a novel metapath-guided personalized compatibility modeling (MG-POCM) scheme
to learn entity embeddings. Extensive experiments were conducted on the public dataset
IQON3000, which demonstrates the superiority of our model over existing methods. The
ablation study veriﬁes the importance of each key module, such as jointly considering the
text, image, and attribute information of items towards POCM, the contrastive regulariza-
tion and using a transformer to fulﬁll the semantic-enhanced embedding fusion. Moreover,
experimental results show that our model is insensitive to the numbers of transformer and
GAT layers, which enables the model to perform well with fewer parameters.
References
1. Xuemeng Song, Xianjing Han, Yunkai Li, Jingyuan Chen, Xin-Shun Xu, and Liqiang Nie. 2019.
GP-BPR: Personalized Compatibility Modeling for Clothing Matching. In Proceedings of the
ACM International Conference on Multimedia, 320–328. ACM.
2. Xingchen Li, Xiang Wang, Xiangnan He, Long Chen, Jun Xiao, and Tat-Seng Chua. 2020. Hier-
archical Fashion Graph Network for Personalized Outﬁt Recommendation. In Proceedings of the
International ACM SIGIR Conference on Research and Development in Information Retrieval,
159–168. ACM.
3. Dikshant Sagar, Jatin Garg, Prarthana Kansal, Sejal Bhalla, Rajiv Ratn Shah, and Yi Yu. 2020.
PAI-BPR: Personalized Outﬁt Recommendation Scheme with Attribute-wise Interpretability. In
IEEE International Conference on Multimedia Big Data, 221–230. IEEE.
4. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of the
Advances in Neural Information Processing Systems, 5998–6008. NIPS.
5. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE:
Large-Scale Information Network Embedding. In Proceedings of the World Wide Web Confer-
ence, 1067–1077. ACM.
6. Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In
Proceedings of the International ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, 855–864. ACM.
7. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2020. DropEdge: Towards Deep
Graph Convolutional Networks on Node Classiﬁcation. In Proceedings of the International
Conference on Learning Representations, 1–17. OpenReview.net.
8. Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. 2019. N-GCN: Multi-scale
Graph Convolution for Semi-supervised Node Classiﬁcation. In Proceedings of the Conference
on Uncertainty in Artiﬁcial Intelligence, 841–851. AUAI Press.

References
107
9. Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. 2019.
Graph Neural Networks for Social Recommendation. In Proceedings of the World Wide Web
Conference, 417–426. ACM.
10. Huan Zhao, Quanming Yao, Jianda Li, Yangqiu Song, and Dik Lun Lee. 2017. Meta-Graph
Based Recommendation Fusion over Heterogeneous Information Networks. In Proceedings of
the International ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 635–
644. ACM.
11. Yizhou Sun and Jiawei Han. 2012. Mining Heterogeneous Information Networks: Principles and
Methodologies. Synthesis Lectures on Data Mining and Knowledge Discovery 3 (2): 1–159.
12. Yuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. 2017. Metapath2vec: Scalable Rep-
resentation Learning for Heterogeneous Networks. In Proceedings of the International ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, 135–144. ACM.
13. Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient Estimation of Word
Representations in Vector Space. In Proceedings of the International Conference on Learning
Representations, 1–12. OpenReview.net.
14. Chuan Shi, Binbin Hu, Wayne Xin Zhao, and Philip S. Yu. 2019. Heterogeneous Information
Network Embedding for Recommendation. IEEE Transactions on Knowledge and Data Engi-
neering 31 (2): 357–370.
15. Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. 2019a.
Heterogeneous Graph Neural Network. In Proceedings of the International ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, 793–803. ACM.
16. Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020. MAGNN: Metapath Aggregated
Graph Neural Network for Heterogeneous Graph Embedding. In Proceedings of the World Wide
Web Conference, 2331–2341. ACM.
17. Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S. Yu. 2019.
Heterogeneous Graph Attention Network. In Proceedings of the World Wide Web Conference,
2022–2032. ACM.
18. Jintao Zhang and Quan Xu. 2021. Attention-aware Heterogeneous Graph Neural Network. Big
Data Mining and Analytics 4 (4): 233–241.
19. Yuying Xing, Zhao Li, Pengrui Hui, Jiaming Huang, Xia Chen, Long Zhang, and Guoxian Yu.
2020. Link Inference via Heterogeneous Multi-view Graph Neural Networks. In Proceedings
of the International Conference on Database Systems for Advanced Applications, 698–706.
Springer.
20. Di Jin, Cuiying Huo, Chundong Liang, and Liang Yang. 2021. Heterogeneous Graph Neural
Network via Attribute Completion. In Proceedings of the World Wide Web Conference, 391–
400. ACM.
21. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for
Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 770–778. IEEE Computer Society.
22. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training
of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the Associ-
ation for Computational Linguistics, 4171–4186. ACL.
23. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. 2018. Graph Attention Networks. In Proceedings of the International Conference on
Learning Representations, 45–61. OpenReview.net.
24. Weili Guan, Haokun Wen, Xuemeng Song, Chung-Hsing Yeh, Xiaojun Chang, and Liqiang Nie.
2021. Multimodal Compatibility Modeling via Exploring the Consistent and Complementary
Correlations. In Proceedings of the International ACM Conference on Multimedia, 2299–2307.
ACM.

108
6
Heterogeneous Graph Learning for Personalized OCM
25. Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S. Davis. 2017. Learning Fashion Com-
patibility with Bidirectional LSTMs. In Proceedings of the ACM International Conference on
Multimedia, 1078–1086. ACM.
26. Zeyu Cui, Zekun Li, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Dressing as a Whole:
Outﬁt Compatibility Learning Based on Node-wise Graph Neural Networks. In Proceedings of
the World Wide Web Conference, 307–317. ACM.
27. Hanwang Zhang, Zheng-Jun Zha, Yang Yang, Shuicheng Yan, Yue Gao, and Tat-Seng Chua.
2013. Attribute-Augmented Semantic Hierarchy: Towards Bridging Semantic Gap and Intention
Gap in Image Retrieval. In Proceedings of the ACM International Conference on Multimedia,
33–42. ACM.
28. Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, Teruko Mitamura, and Alexander G. Hauptmann.
2015.FastandAccurateContent-basedSemanticSearchin100MInternetVideos.InProceedings
of the International ACM Conference on Multimedia, 49–58. ACM.
29. Diederik P. Kingma, and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Pro-
ceedings of the International Conference on Learning Representations, 1–15. OpenReview.net.

7
Research Frontiers
Thus far, in this book, we studied the task of outﬁt compatibility modeling, where each outﬁt
involves a variable number of items. In particular, we ﬁrst identiﬁed the prominent research
challenges we faced to solve this task, including the multiple correlated modalities, com-
plicated hidden factors, nonuniﬁed semantic attributes, and users’ personal preferences. To
address these challenges, we proposed a series of graph learning theories. In particular, we
ﬁrst presented a correlation-oriented graph learning method for outﬁt compatibility model-
ing, which explicitly models the consistent and complementary relations between different
modalities (i.e., the visual and textual modalities). Considering that this scheme overlooks
the category modality and the intermodal compatibility modeling, we next introduced a
modality-oriented graph learning method for outﬁt compatibility modeling. Beyond these
two methods that focus on the coarse-grained compatibility modeling, we then devised an
unsupervised disentangled graph learning method to uncover the hidden factors affecting
the overall compatibility and fulﬁll the ﬁne-grained compatibility modeling. Moreover, to
fully utilize item-attribute labels, we further developed a partially supervised disentangled
graph learning method. Finally, to incorporate the user’s personal tastes, we proposed a
metapath-guided heterogeneous graph learning scheme for personalized outﬁt compatibil-
ity modeling.
The theories on compatibility modeling presented in this book can beneﬁt plenty of
stakeholders in the fashion industry, such as sellers and end-users. Online sellers, with
the help of automatic outﬁt compatibility modeling, can learn how to compose more eye-
catching outﬁts with fashion items and correlate compatible complementary items in the
same product webpage for promoting sales. Additionally, they can recommend compatible
complementary items for the user based on his or her previously purchased items. For
example, if a user historically bought a white T-shirt, the seller can recommend several jeans
for the user. The seller can also fulﬁll a personalized recommendation with the personalized
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
W. Guan et al., Graph Learning for Fashion Compatibility Modeling,
Synthesis Lectures on Information Concepts, Retrieval, and Services,
https://doi.org/10.1007/978-3-031-18817-6_7
109

110
7
Research Frontiers
compatibility modeling scheme we introduced. End-users, the main beneﬁciaries, can learn
how to dress properly with the proposed automatic outﬁt compatibility modeling technique
without consulting stylists at great expense, and no longer dread seeking compatible items
to create outﬁts. For online shopping, the user experience can be also largely improved.
Although the above studies have shed some light on outﬁt compatibility modeling, we
have to admit that this research line is still in a young and up-and-coming stage. Here we
list a few promising future research directions with their corresponding challenges.
7.1
Efficient Fashion Compatibility Modeling
Fashion compatibility modeling is an essential technique for automatic compatible item
recommendation. However, there are millions of fashion items on real-world e-commerce
platforms, such as Taobao and Amazon. The number of possible outﬁts grows exponentially
with the number of items. Therefore, in real-world applications, it is highly desirable to
develop efﬁcient fashion compatibility modeling methods.
To the best of our knowledge, limited research studies have been devoted to efﬁcient fash-
ion compatibility modeling with graph learning, possibly due to the following challenges.
(1) Efﬁcient graph convolution. Traditional graph convolution methods focus on propagat-
ing messages among graph nodes and updating entity representations simultaneously at the
end of graph convolution. Such graph convolution schemes suffer from two key limitations.
First, they do not explicitly distinguish the types of entities during information propagation.
Second, their convolution is not totally efﬁcient, especially for the heterogeneous graph
that involves multiple types of fashion entities (e.g., users, outﬁts, items, and attributes) and
relations (e.g., user-outﬁt interactions, outﬁt-item relations, and item-attribute associations).
This is because once a type of entity representation is updated, it can be used timely for
updating other types of entities. Therefore, how to fulﬁll the efﬁcient graph convolution
is a difﬁcult challenge. (2) Learning to hash, which aims to learn a compact binary hash
code for the given instance, has attracted considerable research interest [1, 2], due to its
prominent advantages in saving the time and storage costs. Converting the continuous hash
representation of each entity to the binary hash code inevitably loses certain information.
Therefore, how to retain the original information to the greatest extent during the hash code
learning constitutes another challenge.
In the future, we plan to devise an efﬁcient outﬁt compatibility modeling scheme, which
aims to effectively convert the continuous hash representation of each fashion entity to the
binary hash code. It will greatly alleviate the storage complexity and the retrieval cost.

7.3
Try-On Enhanced Fashion Compatibility Modeling
111
7.2
Unbiased Fashion Compatibility Modeling
Although existing methods have achieved compelling progress, they usually suffer from
ﬁtting the spurious correlations between the item categories and the compatibility label
of the outﬁt. Speciﬁcally, previous studies resort to the popular fashion-oriented online
communities, where fashion experts share their outﬁt compositions publicly, to collect the
positive/compatible outﬁts, and create negative/incompatible outﬁts with some random sam-
pling strategy. Therefore, the collected dataset tends to have some bias regarding the item
categories. For example, the item categories of pants and jeans occur very frequently in the
positive outﬁts, while categories of kimono and slippers appear few times. Then, the models
trained on such datasets tend to learn the spurious correlations between the item categories
and the compatibility label of the outﬁt, while ignoring the other factors (e.g., color and
pattern) that affect the outﬁt compatibility. This undoubtedly would hurt the generalization
capability of the deep model. Since it is intractable to collect an unbiased dataset without
spurious correlations, as various correlations between item categories and the compatibility
labels commonly exist in the real world, the potential solution to this issue is to design an
unbiased fashion compatibility modeling scheme that can eliminate the harmful effect of
bias reside in the training dataset.
In the future, we plan to use the causal inference technique and introduce the causal graph
to inspect the causal relationships between multiple modalities (i.e., visual, textual and cat-
egory modality) and the outﬁt compatibility label, to fulﬁll the unbiased outﬁt compatibility
modeling.
7.3
Try-On Enhanced Fashion Compatibility Modeling
Existing methods on fashion compatibility only focus on modeling the compatibility rela-
tionship among discrete items in an outﬁt but overlook the human habits in fashion compat-
ibility evaluation. In fact, people usually evaluate the matching degree of a given outﬁt from
not only the collocation angle (i.e., in a discrete manner) but also the try-on angle (i.e., in a
uniﬁed manner), as illustrated in Fig.7.1.
Therefore, incorporating the try-on effect scheme for outﬁt compatibility evaluation will
boost the model performance. However, combining both the collocation and try-on angles
is not nontrivial and may be due to the following challenges. (1) A simple solution to try-
on compatibility modeling is to evaluate the compatibility through an outﬁt’s real try-on
appearance image. However, the real try-on appearance is usually unavailable in practice.
Therefore, how to devise an effective try-on representation learning module, which can cap-
ture the potential try-on effect of the given outﬁt based on its composing items’ multimodal
content information is a difﬁcult challenge. (2) The compatibility of the same outﬁt evaluated
from the collocation and try-on angles is usually intrinsically consistent. In other words, the
knowledge learned from one angle can be used for guiding the learning of the other angle.

112
7
Research Frontiers
Fig.7.1 Illustration of the compatibility modeling from both collocation and try-on perspectives
Therefore, utilizing the latent consistency to seamlessly integrate the collocation and try-on
compatibility modeling, to boost the model performance, poses another challenge.
In the future, we plan to comprehensively analyze the outﬁt compatibility from both the
discrete collocation and uniﬁed try-on angles. Moreover, the two compatibility modeling
angles can obtain mutually enhanced by absorbing knowledge from each other.
References
1. Jorge Sánchez and Florent Perronnin. 2011. High-Dimensional Signature Compression for Large-
Scale Image Classiﬁcation. In The Conference on Computer Vision and Pattern Recognition,
1665–1672. IEEE.
2. Andrea Vedaldi and Andrew Zisserman. 2012. Sparse Kernel Approximations for Efﬁcient Clas-
siﬁcation and Detection. In Conference on Computer Vision and Pattern Recognition, 2320–2327.
IEEE.

