OlaGPT: Empowering LLMs With Human-like Problem-Solving
Abilities
Yuanzhen Xie, Tao Xie, Mingxiong Lin, WenTao Wei, Chenglin Li, Beibei Kong,
Lei Chen, Chengxiang Zhuo, Bo Hu, Zang Li
Platform and Content Group, Tencent
Shenzhen, Guangdong, China
{xieyzh3,taoxie168}@gmail.com
{matrixmxlin,lubewtwei,chainli,echokong,raycheng,felixzhuo,harryyfhu,gavinzli}@tencent.com
ABSTRACT
In most current research, large language models (LLMs) are able to
perform reasoning tasks by generating chains of thought through
the guidance of specific prompts. However, there still exists a signif-
icant discrepancy between their capability in solving complex rea-
soning problems and that of humans. At present, most approaches
focus on chains of thought (COT) and tool use, without considering
the adoption and application of human cognitive frameworks. It is
well-known that when confronting complex reasoning challenges,
humans typically employ various cognitive abilities, and necessitate
interaction with all aspects of tools, knowledge, and the external
environment information to accomplish intricate tasks. This paper
introduces a novel intelligent framework, referred to as OlaGPT.
OlaGPT carefully studied a cognitive architecture framework, and
propose to simulate certain aspects of human cognition. The frame-
work involves approximating different cognitive modules, including
attention, memory, reasoning, learning, and corresponding sched-
uling and decision-making mechanisms. Inspired by the active
learning mechanism of human beings, it proposes a learning unit
to record previous mistakes and expert opinions, and dynamically
refer to them to strengthen their ability to solve similar problems.
The paper also outlines common effective reasoning frameworks
for human problem-solving and designs Chain-of-Thought (COT)
templates accordingly. A comprehensive decision-making mech-
anism is also proposed to maximize model accuracy. The efficacy
of OlaGPT has been stringently evaluated on multiple reasoning
datasets, and the experimental outcomes reveal that OlaGPT sur-
passes state-of-the-art benchmarks, demonstrating its superior per-
formance. Our implementation of OlaGPT is available on GitHub:
https://github.com/oladata-team/OlaGPT.
1
INTRODUCTION
In the past few years, large language models (LLMs) have devel-
oped the ability to process contextual information and generate
fluent human language. As we encounter their outputs that sound
natural and confident, we quickly assume that they have acquired
the long-awaited thinking abilities, such as reasoning, communi-
cation, or collaboration, which are highly complex human skills.
However, after in-depth understanding of LLM, we find that this
reproduction based on high-probability language patterns is still
far from the artificial general intelligence we expected. The most
obvious gaps include the following: one is that LLMs in some cases
produce content that is meaningless or deviates from human value
preferences, or even dangerous suggestions with high confidence;
secondly, the knowledge of LLMs is limited to the concepts and
facts explicitly encountered in their training data. As a result, when
faced with more complex problems, LLMs struggle to truly emulate
human intelligence by understanding the ever-changing environ-
ment, collecting existing knowledge or tools, reflecting on historical
lessons, decomposing problems, and using the thinking patterns
that humans have summed up in the long-term evolution (such
as Analogy, Inductive Reasoning and Deductive Reasoning, etc.)
to effectively solve task. One way to solve the first problem is to
introduce Reinforcement Learning from Human Feedback (RLHF),
which has been recently implemented in ChatGPT [3]. It attempts
to explicitly encode human expression preferences into the training
process: experts will be asked to rank the answers given by the
model according to human common sense and ethical requirements.
Obviously, this method is still an idea based on selection or injecting
constraints, which can avoid the generation of toxic information
to a certain extent, but still cannot reduce the gap with human
reasoning ability.
Naturally, inspired by the attempts to use a combination of data
weights and bias to mimic how neurons work, we hope to draw on
the cognitive model of the human brain and the thinking models
developed in the long-term evolution process more comprehen-
sively, and design corresponding system components to endow
these cognitive structures or thinking processes to LLM, so as to
approximately align the reasoning processes of humans and LLMs,
and expect LLMs to be able to solve complex problems more ef-
fectively. Some recent works have tried to partially solve some
problems, such as using vector databases to store and retrieve exter-
nal domain knowledge in real time, hoping to improve the memory
of LLMs and the ability to capture real-time knowledge; other works
such as langchain 1 and toolfomer [27] are designed to be able to
leverage available tools, etc. However, the process of mimicking
the human brain to deal with problems still faces many systematic
challenges:
Challenge 1: How to systematically imitate and encode
the main modules in the human cognitive framework, and at
the same time schedule the modules according to the general
human reasoning patterns in a realizable way. As mentioned
before, existing works [27, 34] do not comprehensively attempt to
align human and LLM reasoning pipelines.
Challenge 2: How to motivate LLMs to perform active
learning like humans, that is, learn and evolve from his-
torical mistakes or expert solutions to difficult problems?
Encoding the corrected answers by retraining model might be feasi-
ble, but it is obviously costly and inflexible. The common in-context
1https://python.langchain.com/en/latest/modules/agents/how_to_guides.html
arXiv:2305.16334v1  [cs.CL]  23 May 2023

learning [3, 6, 23, 36] is more to explain instructions or patterns in
a few-shot way. The large model still lacks a human-like thinking
framework for wrongly answered questions or historical lessons,
such as "reflection-memorizing-reference-reasoning" mental model.
Challenge 3. How can a LLM flexibly be able to leverage the
diverse thinking patterns that human beings have evolved so
as to improve its reasoning performance? It is hard to adapt to
various problems by designing a fixed and general thinking model.
Just like human beings usually choose different thinking methods
flexibly when facing different types of problems, such as analogical
reasoning, deductive reasoning and so on.
In order to solve challenge 1, we carefully studied humans’ cog-
nitive architecture framework [16], designed some functions to
approximate these thinking modules, such as understanding of in-
tention, memory and comprehensive decision-making, etc., and
designed the corresponding scheduling mechanism. To address
Challenge 2, we propose a concept called "difficult question notes"
with the aim of recording cases where the model frequently answers
incorrectly. Notes for difficult cases are collected either through
manual corrections or by following prompts until a self-correction
is made. When answering questions, LLMs can dynamically re-
view the note pool, identify solutions for similar problems, and use
them as a point of reference. For the third challenge, we summarize
the most effective reasoning frameworks for humans in problem
solving, and design corresponding Chain-Of-Thought templates
accordingly. We also designed a comprehensive decision-making
mechanism to summarize the answers given by each COT rea-
soning, so as to maximize the accuracy of the model. The main
contributions of this work can be summarized as follows:
• As far as we know, this is the first work that attempts to
systematically enhance LLMs’ problem-solving abilities by
learning from a human cognitive processing framework. Re-
sults show that this alignment approach can boost the LLMs’
performance in many aspects, such as understanding of in-
tention, accuracy of knowledge, correctness of reasoning,
etc.
• This work tries to summarize various methods of human
reasoning into Chain-of-Thought (CoT) templates, so as to
maximize the LLMs’ reasoning effect in different scenarios.
The article also innovatively designs an efficient active learn-
ing mechanism and vote mechanism to improve the accuracy
and robustness of solving complex cases.
• We conduct comprehensive experiments on two datasets and
evaluate each module of the proposed method. The experi-
mental results demonstrate that OlaGPT outperforms state-
of-the-art baselines, indicating its superior performance.
2
RELATED WORK
2.1
Augmented Language Model
Augmented language model usually refers to the enhancement of a
large language model’s reasoning skills and the ability to use tools.
The former is defined as decomposing a potentially complex task
into simpler subtasks while the latter consists in calling external
modules.
Table 1: Comparing OlaGPT with related approaches.
Features
CoT Auto-CoT Toolformer OlaGPT
Multi-step reasoning
✓
✓
✓
Limited supervision
✓
✓
✓
Tool use
✓
✓
Extendable libraries
✓
Cross-task transfer
✓
✓
✓
Human feedback
✓
✓
Active learning
✓
Chain of Thought. Among prompt engineering methods, Chain-
of-Thought (CoT) prompting [32] is a popular technique that does
not require fine-tuning model parameters. It is particularly effec-
tive in improving the model’s performance in complex reasoning
questions by simply changing the input. [6] references uncertainty-
based active learning to mine most uncertain questions, which
are then manually annotated and iteratively selected to stimulate
reasoning ability as much as possible. To reduce the cost of man-
ual annotation, a fully automated pipeline named Automate-CoT
(Automatic Prompt Augmentation and Selection with Chain-of-
Thought) is proposed in [28], which uses a variance-reduced policy
gradient strategy to estimate the significance of each example in
LLM. Another automatic CoT prompting method: Auto-CoT [36]
samples questions with diversity and generates reasoning chains
to construct prompt. Additionally, a solution is proposed in [30] to
improve the results by using a voting strategy to select the most
consistent answer output based on the results generated from dif-
ferent reasoning paths. Automatic Reasoning and Tool-use (ART)
[23] uses frozen LLMs to automatically generate intermediate rea-
soning steps as a program. This framework selects demonstrations
of multi-step reasoning and tool use from a task library. Self-Taught
Reasoner (STaR)[35] relies on a simple loop: if the generated an-
swers are wrong, try again to generate a rationale given the correct
answer; fine-tune on all the rationales that ultimately yielded cor-
rect answers. Few-shot prompting struggles as task complexity
increases. Consequently, many recent works employ in-context
learning to decompose complex problems into sub-problems and
effectively teach these sub-problems via separate prompts, such as
SeqZero[33], Decomposed Prompting[13].
Tool Use. Although recent LLMs are able to correctly decom-
pose many problems, they are still prone to errors when dealing
with performing complex arithmetics. Program-Aided Language
models (PAL) [8] decompose symbolic reasoning, mathematical
reasoning, or algorithmic tasks into intermediate steps along with
python code for each step. Similarly, [7] prompts Codex[5] to gen-
erate executable code-based solutions to university-level problems.
furthermore, [27] introduces Toolformer, a model trained to decide
which APIs to call, when to call them, what arguments to pass, and
how to best incorporate the results into future token prediction.
Our work aims to enhance the performance of large models by
drawing inspiration from human-like problem-solving capabilities,
with a comparison to some related works presented in Table 1.

Figure 1: The overall structure of the OlaGPT model.
2.2
Cognitive Architecture
Cognitive architecture is a subset of general artificial intelligence
research that began in the 1950s with the ultimate goal of model-
ing the human mind, bringing us closer to building human-level
artificial intelligence. An early approach to cognitive architectures
consisted of production systems, which used condition-action rules
to represent and perform reasoning [22]. One notable cognitive
architecture resulting from this line of research is SOAR, which
combined problem-solving, learning, and knowledge representation
within a unified system [19]. SOAR has evolved into a comprehen-
sive framework for modeling various cognitive processes such as
decision-making, planning, and natural language understanding
[18]. Another influential cognitive architecture is ACT-R (Adaptive
Control of Thought-Rational), which emphasizes symbolic process-
ing and focuses on memory processes [2]. ACT-R has been applied
to extensive cognitive tasks, such as problem-solving, language
comprehension, and learning [1]. DUAL (Distributed Unit for As-
sembling Learning) is a hybrid cognitive architecture that strives
to balance symbolic processing and connectionist approaches [15].
By employing a central executive agent and multiple collateral
units, DUAL manages a diverse range of cognitive tasks. The Sigma
architecture [26] is composed of several key components, such
as perception, working memory, long-term memory, production
memory, subgoals, decision network, and learning mechanism. [16]
highlights the core cognitive abilities of these architectures and
their practical applications in various domains. This paper will fol-
low the framework of establishing an analogy between LLM and
Cognitive Architectures.
3
METHOD
To address the challenges mentioned, we present a novel frame-
work denoted as OlaGPT, which is a human-like problem-solving
framework to empower LLMs.
3.1
Overview
Our approach draws on the theory of cognitive architecture [16],
which suggests that the core capabilities of the cognitive frame-
work include Attention, Memory, Learning, Reasoning, Action
Figure 2: The overall flow chart of the OlaGPT model. First,
OlaGPT enhances the user’s question intention; the sec-
ond step is to select multiple thinking templates, tools,
wrong question notes, and factual knowledge that may
be used; In the third step, the previously obtained wrong
notes(used as examples), factual knowledge(pre-knowledge),
thinking(guidance) are combined to complete the construc-
tion of multiple template agents, and then execute these
agents to obtain preliminary answers; The fourth step is to
vote on the answers to get the final answer.
Selection 2. We fine-tuned this framework according to the needs
of implementation and proposed a process suitable for LLM to
solve complex problems. It includes six modules: the Intention
Enhance module (corresponding to Attention), Memory mod-
ule (Memory), Active Learning module (Learning), Reasoning
module (Reasoning), Controller module (Action Selection),
and Voting module.
The functional profile of each module is as follows:
Intention Enhance. According to [16], attention is an impor-
tant component of human cognition, as it facilitates the identi-
fication of pertinent information and filters out irrelevant data.
Similarly, we design corresponding attention modules for LLMs,
2Relevant content is described in Appendix A.2.

namely Intention Enhance, aiming to extract the most relevant
information and establish stronger associations between user input
and the model’s linguistic patterns.
Memory. The Memory module serves a crucial function in stor-
ing information in various libraries. Recent studies [21, 27] have
highlighted the limitations of current LLMs in comprehending the
latest factual data. In light of this issue, we have designed the Mem-
ory module to focus on consolidating the existing knowledge that
the model has not yet solidified and storing it in external libraries as
long-term memory. During querying, the module’s retrieval func-
tion can extract relevant knowledge from these libraries. There are
four types of memory libraries involved in our paper: facts, tools,
notes, and thinking.
Learning. The capability to learn is essential for humans to
continuously improve their performance. Essentially, all forms of
learning depend on experience. In particular, we found one way
to quickly improve LLMs’ reasoning ability is to let them learn
from the mistakes it has made before. Firstly, we identify problems
that cannot be resolved by LLMs. Next, we record the insights and
explanations provided by experts in the notes library. Finally, we
select relevant notes to facilitate LLMs’ learning and enable them
to handle similar questions more effectively.
Reasoning. The Reasoning module is designed to create multi-
ple agents based on the human reasoning process, thereby stimu-
lating the potential thought capacity of LLMs to effectively solve
reasoning problems. The module incorporates various thinking
templates that reference specific thinking types, such as lateral,
sequential, critical, and integrative thinking, to facilitate reasoning
tasks.
Controller. The Controller module is designed to handle rel-
evant action selection, corresponding to the Action selection dis-
cussed in [16]. Specifically, the action selection in this paper in-
volves the internal planning of the model for tasks such as selecting
certain modules to execute and choosing from libraries of facts,
tools, notes, and thinking.
Voting. The Voting module enables collective decision-making
by leveraging the strengths of multiple thinking templates. As dif-
ferent thinking templates may be better suited for different types
of questions, we have designed the Voting module to facilitate en-
semble calibration among multiple thinking templates. The module
is responsible for generating the best answer by employing various
voting strategies to improve performance.
After introducing each module, we begin with an outline of
the intelligent simulator with human-like problem-solving abilities
(OlaGPT) followed by a more detailed description of each com-
ponent. As depicted in Fig 2, once the user inputs a query, the
Intention Enhance module generates a more understandable QA
format for the LLMs. The Controller module then retrieves tools,
notes, factual knowledge, and multiple thinking templates based
on the user’s intention. The relevant tools are integrated into the
retrieved thinking templates, while notes and factual knowledge
serve as supplementary information. (In our implementation, we
build an index for the tool library and dynamically retrieve the
relevant tools for each template from the index. It is worth noting
that this feature has been implemented in the code. However, due to
the lack of suitable tools in the current datasets, we did not utilize
this feature in the experiment.)
Obviously, utilizing a singular cognitive framework is insufficient
for addressing the diverse range of complex problems effectively.
Thus, it is essential to adopt a variety of thinking template in order
to derive more holistic and precise solutions, which is widely em-
ployed in model ensembles to reduce the variance in model outputs.
More specifically, we execute multiple templates simultaneously
and then employ various voting strategies in the Voting module to
get the final answer.
Currently, LLMs have not yet reached a level of performance
where they can surpass experts in various fields simultaneously.
Nonetheless, by amalgamating the versatility of LLMs and the pro-
ficiency of specialists, it is possible to attain superior outcomes. As
depicted in Fig 2, human feedback can be integrated into multiple
aspects of the proposed framework. These aspects may include
incorporating human feedback tools during the implementation
of the thinking template, curating the notes library, and making
selections in the Controller module. Such an approach ensures that
the overall performance is enhanced through the combined exper-
tise of humans and the framework itself. Intuitively, the inclusion
of human experts to express clearly ambiguous content can no-
tably improve the performance of LLMs when tackling intricate
and multifaceted problems.
By leveraging the insights and expertise of human domain ex-
perts, LLMs can better understand the nuances of complex problems
and generate more relevant and contextually appropriate responses.
It is worth noting that the human feedback sub-module can be
disabled in our design to achieve end-to-end reasoning. In this
experiment, in order to reduce the cost of labor and resource, we
choose the end-to-end approach for the experiment.
The specific content of each module is described in detail below.
3.2
Intention Enhance Module
Intention Enhance Module can be regarded as an optimized con-
verter from user expression habits to model expression habits. A
more suitable intent enhancement statement is designed for LLMs.
Specifically, we get the type of questions by LLMs through specific
prompts (see Table 5 in Appendix) in advance and then restructure
the way the question is asked. As Fig 3 shows, the sentence——"Now
give you the XX(question type produced by LLM model, the prompt
see Table 5 in appendix) question and choices:" is added at the be-
ginning of the question. To facilitate the analysis and processing
of the results, the sentence——"The answer must end with JSON
format: Answer: one of options[A,B,C,D,E].") is added at the end of
the content.
Additionally, we are currently trying to build an automated inten-
tion enhance module, set up a seed dataset and related instructions,
and call the GPT interface to generate a batch of training data. Us-
ing the open-source LLaMA model [29] and Lora [10] technology,
fine-tuning is performed using the data generated by the instruc-
tion. Intended to implement a module that automates user input
enhancements. This module is still under development and experi-
mentation, and relevant experiment results will be released in the
future if there is an effect.

3.3
Memory Module
The memory module mainly stores relevant knowledge and dia-
logues. We use the memory function provided by langchain for
short-term memory, and long-term memory is implemented by a
Faiss-based vector database [11]. There are four main categories of
knowledge in our approach: facts, tools, notes, and thinking library.
We briefly describe these knowledge libraries as follows:
Facts library. Facts are real-world information like common
sense and other knowledge that everyone accepts.
Tools library. In order to solve some defects of LLMs, a tool
library that contains search engines, calculators and Wikipedia
libraries, is introduced to assist the LLMs to complete some work
without fine-tuning. The input and output of the tool should be in
text format.
Notes library. Notes mainly record some hard cases and their
problem-solving steps. Examples of Notes can be found in Fig 3.
Thinking library. Thinking library mainly stores human problem-
solving thinking templates written by experts that can be humans
or models.
3.4
Active Learning Module
Intuitively, it is known that large language models (LLMs) have
limitations in some specific tasks. The Active Learning module has
been developed to identify challenging cases and provide expert
answers. By doing so, LLMs can use expert answers as a reference
when encountering similar tasks in the future. In essence, this mod-
ule aims to facilitate the active learning process of LLMs, enabling
them to acquire knowledge on the types of questions they typically
struggle with.
Learn from mistakes. A common approach to improving the
performance of language models in specific scenarios is to retrain
them with annotated answers[24]. However, with the emergence of
LLMs, this method is not practical as it requires enormous resources
and cannot be updated in real-time. Drawing inspiration from how
humans use notes to record their mistakes, we propose to introduce
the note library for models to correct stubborn mistakes. When
the LLM encounters a difficult question, it can dynamically refer
to the note library to find similar problems and their solutions as
references as shown in Fig 4. This approach can quickly improve
the LLM’s problem-solving abilities through prompts engineering.
The specific notes format can be seen in Fig 3.
The creation of of notes. When constructing notes library, it
is recommended to first have LLMs answer a batch of questions
repeatedly as shown in Fig 4. The questions that LLMs consistently
answer incorrectly can then be recorded. Subsequently, experts
can ensure the question types, detailed problem-solving steps, and
general problem-solving ideas to form the notes library. Although
the notes can be written by either experts or LLMs, it is gener-
ally preferable to have experts write the notes first and then have
LLMs refine them. The final dataset format is processed as json:
{"question":"x", "answer":"x", "error_reason":"x", "model_expert":"x",
"explanation":"x", llm_task_type :"x"}. The type of task is generated
with LLMs by specific prompt as shown in Table 5.
Figure 3: The notes are combined as examples. The tem-
plates_prefix is template-specific content.
The strategies of retrieving notes. In the Controller module,
there are various ways to implement note retrieval. One intuitive
approach is to find the most similar question type as an example
reference, which has been found to be highly beneficial for LLMs.
However, the number of questions of the same type is often too large
to be used as additional knowledge input to LLMs (exceeding the
number of tokens). Thus we have implemented multiple strategies,
including Random Selection, Dual Retrieval, and Combine, denoting
as 𝑟𝑎𝑛𝑑𝑜𝑚, 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙and 𝑐𝑜𝑚𝑏𝑖𝑛𝑒respectively. 𝑟𝑎𝑛𝑑𝑜𝑚indicates
the method that randomly selects questions from the notes library.
𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙represents a dual retrieval strategy that first retrieves the
most similar question type using LLM and then retrieves the top-n
relevant notes from the notes of such question type. However, text
similarity does not always indicate question similarity. Hence, for
𝑐𝑜𝑚𝑏𝑖𝑛𝑒, we use a random retrieval method to increase diversity
in the second retrieval stage of 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙. Last but not least, we use
zero-shot to identify the zero-shot strategy, which means disabling
the Active Learning module from the framework.
3.5
Reasoning Module
The diverse cognitive processes that humans possess are of im-
portance for accurate problem-solving and achieving excellence in
various tasks. To fully leverage the model’s reasoning ability, we
have designed multiple human reasoning approaches in the Rea-
soning module to assist the model in reasoning. These approaches
are designed based on four thinking types, including Analogical, Se-
quential, Critical, and Synthesis thinking, which show in Fig 5. The
following introduces some thinking principles and implementation
methods.
• Lateral Thinking : Lateral Thinking is a creative problem-
solving approach that involves looking at situations from
unconventional perspectives. It includes techniques such as
challenging assumptions, seeking alternative solutions with
analogy, and embracing ambiguity.
– Analogical Thinking: Analogy Thinking is a cognitive
process that involves finding similarities between two

Figure 4: The overall of the Active Learning module.
Figure 5: The example of some thinking.
distinct entities or concepts in order to gain a deeper un-
derstanding of a problem or situation, which is one of the
most effective tools to generate innovative ideas [14]. The
prompts can be seen in Table 6.
• Sequential Thinking: Sequential Thinking (also named
Linear Thinking) is a problem-solving approach that involves
breaking down complex tasks or problems into smaller, more
manageable parts and addressing them in a logical, step-by-
step manner [9].
– Decomposition Thinking: Decomposition Thinking in-
volves breaking down complex problems into smaller sub-
problems and guiding task decomposition.
– Plan Thinking: Plan Thinking involves creating a de-
tailed roadmap or strategy for achieving a specific goal or
completing a task.
– Step Thinking: Step Thinking is a problem-solving ap-
proach that involves breaking down complex tasks into
smaller, manageable steps, allowing for more efficient and
organized solutions.
• Critical Thinking: Critical thinking is the process of ob-
jectively analyzing and evaluating information to form rea-
soned judgments. It includes the component skills of an-
alyzing arguments, making inferences using inductive or
deductive reasoning, judging or evaluating, and making de-
cisions or solving problems [17].

– Verification Thinking: In real-life problem-solving sce-
narios, humans often employ one approach to solve a
problem and then use another approach or seek the input
of another person to verify the solution and improve its
accuracy. This collaborative and iterative process enables
more reliable and well-rounded outcomes. Based on this
observation, we designed the verification thinking, which
involves using an LLM to generate an output and then
allowing the same model to provide feedback on its own
output from multiple perspectives. The model can subse-
quently refine its previously generated output based on
this feedback. It has not been used in experiments so far.
• Integrative Thinking: Integrative Thinking involves com-
bining multiple approaches in various ways, which seems
to be the core component in models of adult cognitive de-
velopment [12]. In our specific implementation, we guide
the model to use a variety of thought approaches in a free
DIY(Do It Yourself) mode. As of yet, it has not been utilized
in any experiments.
Collectively, these thinking methods are designed to enable mod-
els to reason more efficiently and provide more accurate answers.
The thinking introduced above is not exhaustive, and the common
ones include Concrete Thinking, Abstract Thinking, Vertical Think-
ing, etc. In actual use, different types of thinking templates can be
designed according to the situation. In this module, appropriate
thinking templates can be incorporated based on the specific re-
quirements of different tasks. Alternatively, LLMs can generate new
templates by combining and selecting from an array of existing
thinking templates. This flexibility allows the model to adapt and
engage more effectively with the diverse challenges it encounters.
In the experiment, the paper explores analogy (Lateral Thinking),
decomposition (Sequential Thinking), plan (Sequential Thinking),
and step (Sequential Thinking) thinking.
3.6
Controller Module
In the Controller module, relevant facts, tools, notes, and thinking
are first retrieved and matched (as shown in the second step of Fig
2). The retrieved content is subsequently integrated into a template
agent, requiring the LLMs to furnish a response under a single tem-
plate in an asynchronous manner (as shown in the third step of Fig
2). Just as humans may struggle to identify all relevant information
at the outset of reasoning, it is difficult to expect LLMs to do so.
Therefore, dynamic retrieval is implemented based on the user’s
question and intermediate reasoning progress.
As mentioned earlier, the Faiss method [11] has been employed
to create embedding indices for all four libraries, with retrieval
strategies differing among them. Specifically, information retrieval
serves as a tool that can be accessed at any time during the LLM
model’s problem decomposition and reasoning process from ex-
ternal knowledge bases. In terms of tool selection, we provide a
list of tools, enabling the LLM to make real-time choices during
the reasoning process. The retrieval strategy for notes differs from
the others, as we have designed the three methods described in
Section 3.4. As for the retrieval of thinking, we design prompts to al-
low the LLM for judging and selecting relevant thinking templates.
Table 2: Statistics of Datasets.
Domain
Datasets #testing #training #error books
mathematical reasoning
AQuA
254
97467
81
analogical reasoning
E-KAR
335
1155
632
But currently, we use all thinking templates together to perform a
comprehensive analysis in the experiment.
3.7
Voting Module
Following the third step depicted in Fig 2, we acquire the answers
of various thinking templates through LLMs. Intuitively, distinct
thinking templates may be more suitable for different types of
questions. Therefore, instead of relying on a single template, we
adopt a voting mechanism to aggregate the answers from multiple
templates and improve the final performance of our model. This
approach also enhances the robustness of the model’s results, as it
takes into consideration the variability in the efficacy of different
templates when applied to distinct questions.
There are several voting methods available: 1) vote by LLM: In-
struct the LLM model to choose the most consistent answer among
several given options by providing an output answer along with a
rationale through prompts. 2) vote by regex: Extract the answer by
regex expression to get the majority vote.
4
EXPERIMENTAL
We conducted a series of experiments on a range of public datasets
and compared the proposed OlaGPT with existing approaches. Our
findings indicate that our approach consistently outperforms the
baselines on every dataset considered, demonstrating its robustness
and effectiveness.
4.1
Experimental Setup
Our experiments are designed to address the followings:
• RQ1. How does OlaGPT perform compared to the state-of-
the-art baselines?
• RQ2. How effective are sub-modules in the overall model
design?
• RQ3. How do hyperparameters affect experimental results?
DataSets. To evaluate the proposed framework in a more com-
prehensive manner, we utilize several publicly available datasets
for experimentation. AQuA [20] (Algebra QA with Rationales) is a
dataset comprising approximately 100,000 algebraic word problems
and 254 test questions. For our labeled training set, we randomly
sample 200 training problems. E-KAR [4] is the first interpretable
knowledge-intensive analogical reasoning dataset consisting of
1,655 (Chinese) and 1,251 (English) questions from the Chinese
Civil Service Exam. In our experiment, we utilize the Chinese ver-
sion to explore the performance with the Chinese language.
The statistical results of the dataset are presented in Table 2. We
identified the questions that the large model answered incorrectly
3-5 times from the training set as its error books. However, it is too
expensive to achieve this on the training set of AQuA due to its

large size. To address this issue, we utilized Sentence-BERT [25]
for embedding and then clustered the training set into 20 groups
using K-means. Finally, 211 questions were randomly selected from
each group based on their weights.
Evaluation Metrics. The current metric for measurement is
the accuracy rate, and most of the questions are in multiple-choice
format. The answer is considered correct only when it exactly
matches the provided answer options. To compute the model’s
accuracy rate, we initially use regular expressions to match answers
and subsequently perform manual inspection and correction of the
assessed results.
The use of thinking templates. In the main experiment, we
utilized five thinking templates and the original base model (GPT-
3.5-turbo). The prompts of each thinking template can be found in
Table 6. Specifically, E-KAR employed all six thinking templates,
including origin, Analogy Thinking (AT), Decomposition Think-
ing 1 (DT), Decomposition Thinking 2 (DST), Plan Thinking (PT),
and Sequential Thinking (ST). For AQuA, we used the other five
templates excluding AT.
Baselines. We select conventionally and recently published
Augmented LLM baselines for model comparison, which can be
briefly categorized into three groups: (1) base LLM model (GPT-3.5-
turbo); (2) Augmented LLM based on prompt engineering (Auto-
CoT); (3) Augmented LLM based on process optimization (SC). To
ensure a fair comparison, all baseline methods use the same inten-
tion enhancement or voting mechanism.
• GPT-3.5-turbo: A base LLM model that builds upon the
GPT-3 architecture and incorporates additional training data
and techniques to improve performance on natural language
processing tasks. It achieves state-of-the-art results on sev-
eral benchmark datasets, demonstrating its effectiveness in
language modeling and downstream tasks. The temperature
is set to 0 in the experiment.
• Auto-CoT [36] An automatic CoT (Chain of Thought) prompt-
ing method. This approach involves sampling diverse ques-
tions and generating reasoning chains to construct demon-
strations.
• SC [31] A new decoding strategy called self-consistency for
chain-of-thought prompting. It samples diverse reasoning
paths and selects the most consistent answer by marginal-
izing the sampled paths. This approach acknowledges that
complex reasoning problems have multiple correct ways of
thought.
4.2
The Model Performance Comparison
In order to evaluate the effectiveness of our augmented LLM frame-
work for inference tasks, we conducted comprehensive experimen-
tal comparisons on two types of inference datasets. The results of
the experiments are summarized in Table 3. The best performance
is indicated in bold, and "Improv" indicates the improvement over
the best baseline. Our experiments yielded several findings:
• The performance of SC is better than GPT-3.5-turbo, suggest-
ing that employing ensemble methods to a certain extent can
indeed contribute to enhancing the effectiveness of large-
scale models.
• The performance of our approach surpasses that of SC (SC
adopts the same majority voting extracted by employing
regular expressions as our method), which, to a certain ex-
tent, demonstrates the effectiveness of our thinking template
strategy. The answers of different thinking templates exhibit
considerable variation, and conducting voting at different
thinking templates ultimately yields better results than sim-
ply running multiple rounds and voting.
• The effects of different thinking templates are different, as
shown in Table 3. Relatively speaking, ST and DT have better
effects. This can be attributed to the fact that this kind of step-
by-step solution may be more suitable for reasoning-type
questions.
• The results presented in Table 3 demonstrate that the Ac-
tive Learning module yields significantly better performance
compared to the zero-shot approach. Specifically, the ran-
dom, retrieval, and combine columns exhibit superior per-
formance. These findings suggest that incorporating chal-
lenging cases as a note library is a viable strategy.
• Different Retrieval schemes work differently on different
datasets. Overall, the Combine strategy works better.
• Our method is significantly better than other solutions, thanks
to the rational design of the overall framework, the specific
reasons are as follows: 1) The effective design of the Active
Learning module; 2) The thinking template has achieved
the adaptation of different models as expected, and the re-
sults under different thinking templates are different; 3) The
Controller module plays a good control role and selects the
content that better matches the required content; 4) The way
of ensembles with different thinking templates designed by
the Voting module is effective.
4.3
Ablation Study
To investigate the impact of each sub-module, we conducted an
ablation analysis on our framework.
the Performance of Active Learning module. In order to
verify the effectiveness of the Active Learning module, we com-
pare whether to use this module or not. The results shown in Fig
6 indicate that the notes’ performance surpasses that of the zero-
shot. Taking the similarly weak topics of large models as hints
can significantly improve performance. The underlying rationale
is straightforward: humans learn through accumulating experi-
ences, especially incorrect ones. By providing LLM with the correct
problem-solving process for similar problems and allowing it to
understand and imitate the process, we can enable the LLM to learn
from past mistakes.
the Performance of different retrieval strategies in Con-
troller module. For different notes retrieval strategies, we also
conducted experiments and the experimental results are shown in
Fig 6 and Fig 9.
From the results of different 𝑓𝑒𝑤_𝑠ℎ𝑜𝑡_𝑡𝑦𝑝𝑒values in the table
6, we can see that: the 𝑐𝑜𝑚𝑏𝑖𝑛𝑒strategy has the best effect, Ran-
dom is the second one, and the worst one is zero-shot. Intuitively,

Table 3: Performance comparison of different methods on two datasets.
Type
Datasets
AQuA
E-KAR (Chinese)
𝑛𝑜𝑡𝑒𝑠_𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙_𝑡𝑦𝑝𝑒
zero-shot
𝑟𝑎𝑛𝑑𝑜𝑚
𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙
𝑐𝑜𝑚𝑏𝑖𝑛𝑒
zero-shot
𝑟𝑎𝑛𝑑𝑜𝑚
𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙
𝑐𝑜𝑚𝑏𝑖𝑛𝑒
baselines
turbo
0.3228
0.5236
0.5315
0.5039
0.3762
0.3731
0.403
0.3701
auto_cot
-
0.5748
-
-
-
0.3791
-
-
sc
0.3189
0.6142
0.5394
0.5906
0.3761
0.4179
0.4119
0.3851
thinking templates
AT
-
-
-
-
0.3851
0.3821
0.3910
0.3881
DT
0.5591
0.5866
0.5827
0.5945
0.3612
0.4119
0.4269
0.3881
DST
0.5079
0.5236
0.5984
0.5945
0.3552
0.4030
0.3761
0.4000
PT
0.5512
0.5472
0.5827
0.5512
0.3851
0.3552
0.4119
0.4060
ST
0.5197
0.5787
0.6102
0.5669
0.3373
0.4179
0.4119
0.4388
Our Framework
OlaGPT-regex-vote
0.5945
0.6496
0.6732
0.6772
0.4209
0.4597
0.4567
0.4716
OlaGPT-llm-vote
0.5984
0.6417
0.6654
0.7047
0.4000
0.4418
0.4358
0.4507
%Improv.
85.38%
5.76%
24.81%
19.32%
11.82%
10.00%
10.88%
22.46%
finding the most similar question type as an example reference has
the greatest gain for LLMs. Due to the large scale of the notes and
the large number of questions depending on the type of questions,
it is unrealistic to introduce them all as few shots. Both 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙
and 𝑐𝑜𝑚𝑏𝑖𝑛𝑒strategies perform a second search after retrieving
the question type. The 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙strategy uses similarity search the
second time and retrieval uses random search. The 𝑐𝑜𝑚𝑏𝑖𝑛𝑒strat-
egy is better than 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙because random retrieval introduces
greater diversity. Compared with random retrieval, similarity re-
trieval cannot necessarily satisfy the similarity of topic types, only
the similarity in characters. This conclusion is similar to that of
Auto-CoT. The most essential thing is to find the most relevant
questions.
The best result of E-KAR (chinese) is the𝑟𝑎𝑛𝑑𝑜𝑚strategy. There
are several reasons here: 1) The types of questions are not detailed
enough. Because of this kind of analogical reasoning questions,
the type of questions generated by the model is almost an analogy
question, unlike the question types of math questions that can
be more detailed. 2) Templates are not customizable. Except for
AT for analogy thinking, other thinking templates are made for
mathematics questions.
the Performance of Different Thoughts in Reasoning mod-
ule. This section mainly discusses the effect of different thinking
templates on different tasks. From Table 3, it is evident that different
thinking templates exhibit varying effects across distinct datasets
and retrieval strategies. However, all of them surpass the turbo’s
results, thereby validating the notion presented in the article that
a single template cannot effectively address multiple tasks. Cur-
rently, the top three strategies yielding superior results on AQuA
are ST-retrieval, DST-retrieval, and DT-combine. For E-KAR, the
more effective strategies include ST-combine, DT-retrieval, and
DST-random.
After posterior analysis, the final correct rate of some templates
has little difference as shown in the table 3, but their consistency is
not high. For this very reason, designing a voting module can help
select as many correct answers as possible. We have also examined
the possible maximum accuracy that can be achieved by different
voting strategies. Please refer to Table 8 in the appendix.
Table 4: Consistency analysis of the thinking templates.
#consistency
AQuA
E-KAR (Chinese)
zero-shot
𝑟𝑎𝑛𝑑𝑜𝑚
𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙
𝑐𝑜𝑚𝑏𝑖𝑛𝑒
zero-shot
𝑟𝑎𝑛𝑑𝑜𝑚
𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙
𝑐𝑜𝑚𝑏𝑖𝑛𝑒
c=2
66
56
45
61
42
45
44
40
c=3
95
71
79
73
94
94
91
92
c=4
63
48
63
61
102
82
81
86
c=5
29
78
67
57
64
74
71
75
c=6
-
-
-
-
16
16
16
16
the Performance of Voting module. We explored different
voting strategies and the experimental situation of voting under
different template numbers. In this experiment, if it is not specified,
the regex results will be used for discussion.
• As shown in Fig 7, the performance of the model after vot-
ing is significantly improved compared to that of individual
thinking templates, which also validates the effectiveness
of the voting approach. Different thinking templates are ap-
propriate for different types of questions, and the voting
strategy can consolidate the strengths of different thinking
templates to further enhance the framework’s performance.
We consider all correct results as the upper bound and all
incorrect results as the lower bound with the regex-vote
(majority vote by regex expression) method. The specific
values are presented in Table 8 in the Appendix.
• As shown in Fig 6, the accuracy rate increases with the
growth in the number of templates. As the number of tem-
plates increases, the variety of answer combinations gener-
ated by different templates also expands, thereby offering
the voting module a broader potential upper limit for per-
formance improvement.

• The regex-vote strategy can ensure the majority voting mech-
anism, while the llm-vote strategy relies on the explanation
of the candidate’s answers, which may have a larger degree of
uncertainty. Consider an example with five templates, where
there are 2 correct and 3 incorrect answers. The regex-vote
strategy will always select the incorrect. The llm-vote strat-
egy still has the possibility of selecting the correct answer,
based on the different confidence of weights on different
answers. Consequently, the potential accuracy upper bound
of the llm-vote strategy may be even higher.
In summary, we can observe the improvement of the overall rec-
ommendation performance whenever we incrementally add a new
module or feature on top of the previous model, which illustrates
the effectiveness of the Active Learning, Controller, Thought, and
Voting modules.
4.4
Hyperparameter Study
In this subsection, we investigate the model’s hyperparameters.
To determine the optimal number of notes to use as the example
reference, we conduct an experiment on the number of examples.
Taking the experimental results of the AQuA dataset under the
ST template as an example, the outcomes are displayed in Fig 9. It
can be found that the optimal value for the number of notes varies
among different retrieval strategies. The 𝑐𝑜𝑚𝑏𝑖𝑛𝑒strategy exhibits
consistent improvement, while the 𝑟𝑎𝑛𝑑𝑜𝑚and 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙strategies
first increase and then decrease. When more sample questions are
included, additional noise may be introduced. Thus, it is important
to find the appropriate number of examples, neither too few nor
too many, making a trade-off.
5
CONCLUSION
This paper designs an enhanced LLM cognition framework (OlaGPT),
aiming to solve difficult reasoning problems with human-like problem-
solving abilities. Specifically, referring to the theory of human cog-
nition, OlaGPT proposes to approximate cognitive modules, such as
attention (for entention enhancement), Memory, Learning, Reason-
ing, action selection (Controller) and decision making. The user’s
query undergoes refinement through the Intention Enhancement
module, achieving a more precise expression. Then, the Controller
module controls and utilizes this expression to select the required
library content, and completes the filling of multiple thinking tem-
plates. After acquiring the results of asynchronously executing
multiple reasoning templates, a more robust effect is achieved us-
ing the Voting module. We conduct experiments on two real in-
ference datasets and show that the OlaGPT method outperforms
existing methods in answering inference questions. In addition, we
also demonstrate the effectiveness of the design of each part in
the model. Most of the modules are designed to be pluggable, and
the required modules can be determined according to the needs of
different scenarios.
In the follow-up work, we will continue to optimize and improve
the functions of each sub-module, and it is expected to complete an
easy-to-use enhanced large-scale model framework with human
thinking ability. First, more diverse datasets and baselines will be
added for experimental testing. In addition, we will continue to opti-
mize the design of each sub-module and conduct more experiments
to testify our ideas.
ACKNOWLEDGMENTS
REFERENCES
[1] John R Anderson. 2009. How can the human mind occur in the physical universe?
Oxford University Press.
[2] John R Anderson and Christian J Lebiere. 2014. The atomic components of thought.
Psychology Press.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[4] Jiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi
Sun, Lei Li, Yanghua Xiao, and Hao Zhou. 2022. E-KAR: A Benchmark for Ratio-
nalizing Natural Language Analogical Reasoning. In Findings of the Association for
Computational Linguistics: ACL 2022. Association for Computational Linguistics,
Dublin, Ireland, 3941–3955. https://aclanthology.org/2022.findings-acl.311
[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex
Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan-
tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,
Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large
Language Models Trained on Code. arXiv:2107.03374 [cs.LG]
[6] Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.
Active
Prompting with Chain-of-Thought for Large Language Models. arXiv preprint
arXiv:2302.12246 (2023).
[7] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth
Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil
Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu,
and Gilbert Strang. 2022. A neural network solves, explains, and generates
university math problems by program synthesis and few-shot learning at human
level. Proceedings of the National Academy of Sciences 119, 32 (aug 2022). https:
//doi.org/10.1073/pnas.2123433119
[8] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang,
Jamie Callan, and Graham Neubig. 2023. PAL: Program-aided Language Models.
arXiv:2211.10435 [cs.CL]
[9] Kevin Groves, Charles Vance, and Yongsun Paik. 2008. Linking linear/nonlinear
thinking style balance and managerial ethical decision-making. Journal of Busi-
ness Ethics 80 (2008), 305–325.
[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large
language models. arXiv preprint arXiv:2106.09685 (2021).
[11] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity
search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535–547.
[12] Eeva Kallio. 2011. Integrative thinking is the key: An evaluation of current
research into the development of adult thinking. Theory & Psychology 21, 6
(2011), 785–801.
[13] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Pe-
ter Clark, and Ashish Sabharwal. 2023. Decomposed Prompting: A Modular
Approach for Solving Complex Tasks. arXiv:2210.02406 [cs.CL]
[14] Eunyoung Kim and Hideyuki Horii. 2016. Analogical thinking for generation of
innovative ideas: An exploratory study of influential factors. Interdisciplinary
Journal of Information, Knowledge, and Management 11 (2016), 201.
[15] Boicho Kokinov. 1994. A hybrid model of reasoning by analogy. Advances in
connectionist and neural computation theory 2 (1994), 247–318.
[16] Iuliia Kotseruba and John K Tsotsos. 2020. 40 years of cognitive architectures:
core cognitive abilities and practical applications. Artificial Intelligence Review
53, 1 (2020), 17–94.
[17] Emily R Lai. 2011. Critical thinking: A literature review. Pearson’s Research
Reports 6, 1 (2011), 40–41.
[18] John E Laird. 2019. The Soar cognitive architecture. MIT press.
[19] John E Laird, Allen Newell, and Paul S Rosenbloom. 1987. Soar: An architecture
for general intelligence. Artificial intelligence 33, 1 (1987), 1–64.
[20] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program
induction by rationale generation: Learning to solve and explain algebraic word
problems. arXiv preprint arXiv:1705.04146 (2017).

2
3
4
5
template num
0.500
0.525
0.550
0.575
0.600
0.625
0.650
0.675
acc
aqua + zero_shot
2
3
4
5
template num
acc
aqua + few_shot_random
2
3
4
5
template num
acc
aqua + few_shot_retrieval
2
3
4
5
template num
acc
aqua + few_shot_combine
2
3
4
5
6
template num
0.34
0.36
0.38
0.40
0.42
0.44
0.46
acc
ekar_chinese + zero_shot
2
3
4
5
6
template num
acc
ekar_chinese + few_shot_random
2
3
4
5
6
template num
acc
ekar_chinese + few_shot_retrieval
2
3
4
5
6
template num
acc
ekar_chinese + few_shot_combine
Figure 6: The performance under the different number of templates and notes retrieval modes.
2
2-vote
3
3-vote
4
4-vote
5
5-vote
template num
0.500
0.525
0.550
0.575
0.600
0.625
0.650
0.675
acc
aqua + zero_shot
2
2-vote
3
3-vote
4
4-vote
5
5-vote
template num
acc
aqua + few_shot_random
2
2-vote
3
3-vote
4
4-vote
5
5-vote
template num
acc
aqua + few_shot_retrieval
2
2-vote
3
3-vote
4
4-vote
5
5-vote
template num
acc
aqua + few_shot_combine
2 2-vote 3 3-vote 4 4-vote 5 5-vote 6 6-vote
template num
0.32
0.34
0.36
0.38
0.40
0.42
0.44
0.46
acc
ekar_chinese + zero_shot
2 2-vote 3 3-vote 4 4-vote 5 5-vote 6 6-vote
template num
acc
ekar_chinese + few_shot_random
2 2-vote 3 3-vote 4 4-vote 5 5-vote 6 6-vote
template num
acc
ekar_chinese + few_shot_retrieval
2 2-vote 3 3-vote 4 4-vote 5 5-vote 6 6-vote
template num
acc
ekar_chinese + few_shot_combine
Figure 7: The performance before and after voting.
[21] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram
Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli
Celikyilmaz, et al. 2023. Augmented language models: a survey. arXiv preprint
arXiv:2302.07842 (2023).
[22] Allen Newell, Herbert Alexander Simon, et al. 1972. Human problem solving.
Vol. 104. Prentice-hall Englewood Cliffs, NJ.
[23] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke
Zettlemoyer, and Marco Tulio Ribeiro. 2023. ART: Automatic multi-step reasoning
and tool-use for large language models. (2023). arXiv:2303.09014 [cs.CL]

dst
dt
pt
st
turbo
vote
model
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
acc
aqua + zero_shot
dst
dt
pt
st
turbo
vote
model
acc
aqua + few_shot_random
dst
dt
pt
st
turbo
vote
model
acc
aqua + few_shot_retrieval
dst
dt
pt
st
turbo
vote
model
acc
aqua + few_shot_combine
at
dst
dt
pt
st
turbo
vote
model
0.30
0.32
0.34
0.36
0.38
0.40
0.42
0.44
0.46
acc
ekar_chinese + zero_shot
at
dst
dt
pt
st
turbo
vote
model
acc
ekar_chinese + few_shot_random
at
dst
dt
pt
st
turbo
vote
model
acc
ekar_chinese + few_shot_retrieval
at
dst
dt
pt
st
turbo
vote
model
acc
ekar_chinese + few_shot_combine
Figure 8: The performance before and after voting.
1
3
5
0.500
0.525
0.550
0.575
0.600
0.625
acc
aqua + random
1
3
5
aqua + retrieval
1
3
5
aqua + combine
Figure 9: The performance of ST under the different number of different notes retrieval examples.
[24] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.
Improving language understanding by generative pre-training. (2018).
[25] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings
using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).
[26] Paul S Rosenbloom, Abram Demski, and Volkan Ustun. 2016. The Sigma cognitive
architecture and system: Towards functionally elegant grand unification. Journal
of Artificial General Intelligence 7, 1 (2016), 1.
[27] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Lan-
guage models can teach themselves to use tools. arXiv preprint arXiv:2302.04761
(2023).
[28] KaShun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic Prompt Aug-
mentation and Selection with Chain-of-Thought from Labeled Data. (2023).
arXiv:2302.12822 [cs.CL]
[29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[30] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain
of Thought Reasoning in Language Models. (2023). arXiv:2203.11171 [cs.CL]
[31] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.
2022. Self-consistency improves chain of thought reasoning in language models.
arXiv preprint arXiv:2203.11171 (2022).
[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits
Reasoning in Large Language Models. (2023). arXiv:2201.11903 [cs.CL]
[33] Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi
Yang. 2022. SeqZero: Few-shot Compositional Semantic Parsing with Sequential
Prompts and Zero-shot Models. (2022). arXiv:2205.07381 [cs.CL]
[34] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models.
arXiv preprint arXiv:2210.03629 (2022).
[35] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. STaR: Boot-
strapping Reasoning With Reasoning. (2022). arXiv:2203.14465 [cs.LG]
[36] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022.
Auto-
matic Chain of Thought Prompting in Large Language Models.
(2022).
arXiv:2210.03493 [cs.CL]

A
APPENDIX
A.1
Prompts
In this section, we mainly put the related Prompts design in the
experiment, including the prompts of generating question types as
shown in Table 5 and thinking templates as shown in Table 6.
A.2
Cognitive Architecture
The framework structure of this paper mainly refers to the article
[16], and the key modules listed in the article will be described in
detail below.
Attention. The attention section of the essay primarily focuses
on the selective mechanisms utilized by cognitive architectures
for prioritizing relevant information and filtering out extraneous
data. The main idea is to explore how attentional processes help
individuals efficiently allocate cognitive resources to specific stimuli
or tasks, as well as to discuss the techniques and models applied
to modulate attention in various contexts. In LLM, the primary
objective of attention is to interpret the input prompt and discern
the intent underlying the words.
Action Selection. In the action selection section, the main idea
revolves around examining the decision-making mechanisms em-
ployed by cognitive architectures to choose appropriate actions in
response to external stimuli or internal states. This part covers key
computational models, methods, and algorithmic logic responsible
for determining and selecting goal-directed actions based on the
available information and environmental context.
Memory. The memory section of the essay explores the con-
cepts and models related to the storage and retrieval of information
within cognitive architectures. The main idea is to investigate the
fundamental features of short-term (or working) and long-term
memory, as well as to present the mechanisms by which cognitive
systems encode, maintain, and retrieve important information and
experiences. We focus on reinforcing the existing knowledge that
the model has not yet firmly established. Memory stores the con-
solidated information in external libraries, effectively functioning
as long-term memory for the model.
Learning. The learning section delves into the processes that
help cognitive architectures acquire new information, adapt to new
situations, and generalize from previous experiences. The main idea
is to examine the different learning paradigms, such as supervised,
unsupervised, and reinforcement learning, and their applications
in equipping cognitive architectures with the ability to modify and
improve their knowledge structures, representations, and decision-
making processes. By updating the few-shot content in the prompt,
this task can be easily accomplished. This paper achieves learning
in the Large Language Model (LLM) by updating the note library,
allowing the model to acquire new knowledge and adapt to new
information.
Reasoning. The reasoning section of the essay emphasizes the
cognitive processes underlying problem-solving, decision-making,
and inference within cognitive architectures. The main idea is to
present various approaches and models that demonstrate logical
and probabilistic reasoning, as well as to discuss the mechanisms
for generating predictions, explanations, and strategies based on
available information and knowledge. The module incorporates var-
ious templates that enable the model to approach problem-solving
situations more effectively and generate well-structured solutions.
A.3
Examples
This section mainly introduces the specific execution text sam-
ples. In the Fig 10 and 11, we give the full-text content in different
datasets using the 𝑐𝑜𝑚𝑏𝑖𝑛𝑒strategy to retrieve the notes when
asking questions about LLMs(gpt-turbo-3.5) with the DT thinking
template.
A.4
Templates analysis
The performance of each template is presented in Table 3. According
to Table 7, there are fluctuations in each template when executed on
different datasets, which suggests that utilizing model ensembling
may lead to improved performance. Consequently, this work em-
ploys a voting mechanism to capitalize on the strengths of various
templates while addressing their individual limitations.
On the AQuA dataset, the best-performing template is DT, which
demonstrates the highest average accuracy, reaching 0.5807. In the
E-KAR Chinese dataset, the top-performing template is ST, achiev-
ing an accuracy of 0.4015. These results highlight the effectiveness
of using different templates tailored to the specificities of each
dataset in order to maximize performance.
A.5
Vote analysis
In this study, we investigate voting methods in model ensembles,
employing two distinct approaches for fusing the results derived
from different models. The first approach entails extracting can-
didate answers using regular expressions and selecting the most
frequently occurring answer as the ensemble’s output. The second
approach feeds the predicted outputs and their analyses from di-
verse models into GPT-3.5-turbo, which subsequently generates
the final answer.
Considering the voting method utilizing regular expressions for
answer extraction, a supremum and infimum inherently exist. To
clarify this, the study introduces two metrics evaluating the answers
produced by various models for identical questions: accuracy and
incorrect consistency. Accuracy represents the proportion of models
delivering the correct answer, while incorrect consistency refers
to the proportion associated with the most frequently recurring
answer, excluding the correct one.
The supremum corresponds to the proportion of questions where
accuracy either surpasses or equals consistency. In contrast, the in-
fimum signifies the proportion where accuracy exceeds consistency.
The GPT-3.5-turbo-based voting method results reveal an average
increase of 0.0561 and 0.0366 in comparison to the infimum derived
through regular expression answer extraction for the AQuA and
E-KAR (Chinese) datasets, respectively. When compared to the
supremum, accuracy remains an average of 0.0325 lower for the
AQuA dataset and 0.0485 lower for the E-KAR (Chinese) dataset.
Consequently, the outcomes acquired through LLM voting exhibit
a higher degree of robustness.
Furthermore, the heatmap shown in Fig 12 demonstrates that
answers derived from different templates tend to cluster, indicating

Table 5: the prompts of generating question type.
E-KAR datasets:
You are the examiner of the Chinese Civil Service Examination, and you need to judge the specific question types of the following analogy questions and
don’t give an explanation.
Question: {question}
Answer: The output must only be in a strict JSON format: "task_type": "question type".
Math datasets:
As a mathematics professor, you need to judge the type of the following question and don’t give an explanation
Question: {question}
Answer: The output must only be in a strict JSON format: "task_type": "question type".
Table 6: the main prompts of some thinking templates.
Analogical Thinking (AT):
For the problem of analogical reasoning, it is completed in three steps.
First conduct an inductive analysis of the given sample data, considering the similarity of the relationship between words; Next, judge whether the sample
to be selected is satisfied; Finally check the validity of the mapping and explain if the mapping is correct.
Decomposition Thinking:
1) DT: The following questions can be disassembled into multiple sub-questions to solve, the steps and answers of each sub-question are given, and finally
the answer to the following question is given.
2) DST: Disassemble the following complex problems to solve them step by step
Plan Thinking (PT):
Think carefully about the problem to be solved and make a detailed plan to solve it.
Step Thinking (ST):
Let’s think step by step.
Table 7: Precision analysis of the thinking templates.
AQuA
E-KAR (Chinese)
zero-shot
random
retrieval
combine
zero-shot
random
retrieval
combine
Range
0.0512
0.0630
0.0275
0.0433
0.0478
0.0627
0.0508
0.0507
Mean
0.5345
0.5590
0.5935
0.5768
0.3648
0.3940
0.4036
0.4042
that these templates produce analogous judgments for the same
questions.
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009

Figure 10: one example contents for AQuA.

Figure 11: one example contents for E-KAR.
Table 8: Explore the high accuracy of the theory.
Combination
AQuA
E-KAR (Chinese)
zero-shot
𝑟𝑎𝑛𝑑𝑜𝑚
𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙
𝑐𝑜𝑚𝑏𝑖𝑛𝑒
zero-shot
𝑟𝑎𝑛𝑑𝑜𝑚
𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙
𝑐𝑜𝑚𝑏𝑖𝑛𝑒
regex-upper
0.6457
0.6614
0.7047
0.7283
0.4537
0.5015
0.4866
0.4806
regex-lower
0.5315
0.5906
0.6614
0.6024
0.3552
0.4179
0.4030
0.4060
llm-vote
0.5984
0.6417
0.6654
0.7047
0.4000
0.4418
0.4358
0.4507
reg-vote
0.5945
0.6496
0.6732
0.6772
0.4209
0.4597
0.4567
0.4716

Figure 12: The Accuracy and Incorrect Consistency of AQuA and E-KAR (Chinese).

