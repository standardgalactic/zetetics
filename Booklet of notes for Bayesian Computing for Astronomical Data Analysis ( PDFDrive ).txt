June 11-13, 2014
Center for Astrostatistics
Eberly College of Science
Bayesian Computing for  
Astronomical Data Analysis


Bayesian Computing for Astronomical Data Analysis (June 11-13, 2014)
Contents
Title
Presenter
page no.
Motivation for High-Performance Computing
Eric Ford
1
Parallel Computer Architectures
Pierre-Yves Taunay
5
Parallel Programming for Multicore and Distributed Systems
Pierre-Yves Taunay
23
IGPU Computing Architectures
Pierre-Yves Taunay
41
Importance Samplin
Jessi Cisewski
53
ABC using Sequential Sampling
Jessi Cisewski
59
Hierarchical Bayesian Modeling with ABC
Jessi Cisewski
69
Hierarchical Bayesian Modeling with Ensemble MCMC
Eric Ford
79
An Astronomer’s Introduction to Gaussian Processes
Dan Foreman-Mackey
93
Hamiltonian Monte Carlo and Stan
Dan Lee
111
Stan Fundamentals
Dan Lee
115
Computation for Bayesian model comparison
Tom Loredo
119
Lecture notes assembled by G. Jogesh Babu
Director, Center for Astrostatistics, The Pennsylvania State University.
Acknowledgments: The Summer School on Bayesian Computing for Astronomical Data
Analysis is supported in part by NSF grant AST-1418179, the Departments of Statistics
and Astronomy & Astrophysics, and the Institute for CyberScience at Penn State.


6/2/2014
1
Motivation for 
High-Performance Computing for Bayesian 
Analysis of Astronomical Data Sets 
Eric B. Ford (Penn State)
Bayesian Computing for Astronomical Data Analysis
June 12, 2014
BayesComp Instructors
Lecturers
• Eric Ford (Penn State, Astronomy)
• Pierre-Yves Taunay (Penn State, Research Computing Center)
• Tamas Budavari (JHU, Physics & Astronomy)  
• Jessi Cisewski (CMU, Statistics)
• Daniel Lee (Columbia, Statistics)
• Daniel Foreman-Mackey (NYU, Astronomy)
• Tom Loredo (Cornell, Astronomy)
Lab Assistants:
• Robert Morehead, Benjamin Nelson, Megan Shabram
(Penn State Graduate Students)
Advantages of Bayesian Approach
• Rigorous statistical basis for performing inference
– Parameter estimation
– Model comparison
– Prediction
• Explicitly account for 
– Prior information (e.g., physical constraints, previous work),
– Correlated parameters, and 
– Non-Gaussian uncertainties
• Sounds great in theory, but…
Bayesian Inference Requires 
Computing Lots of Integrals
• Old-School Bayesian “Solution”
– Used simple models and conjugate distributions
• But we often have physical models that:
– May not permit using conjugate distributions
– Are highly non-linear
– Are time consuming to evaluate
– Have many model parameters
– Can be under-constrained by available data
1

6/2/2014
2
Bayesian Inference Requires 
Computing Lots of Integrals
• Modern Bayesian Approach
– Apply Modern Computing Power & Clever Algorithms
• Many Bayesian analyses are practical using 
– Workstation
– A Few Basic Algorithms:
• Quadrature Integration
• Monte Carlo Integration
• Markov chain Monte Carlo
• Importance Sampling 
Rise of Bayesian Inference
• Bayesian inference has become increasingly common, 
largely thanks to:
– Rapid increase in computational power
– Application of Markov chain Monte Carlo (MCMC) 
But Some Problems are Hard
• Quadrature Integration:
– Breaks down for large dimensions
• Monte Carlo Integration
– Requires unrealistic number of evaluations
• Standard Markov chain Monte Carlo
– Can converges too slowly if many parameters, 
significant correlations, or multi-modal posterior
• Importance Sampling 
– Can be difficult to implement for complex problems
(e.g., finding good importance sampling density)
In Near Future, Greater Fraction of 
Problems Will be “Hard”
• Increasingly large data sets
– Particularly surveys like SDSS, HETDEX, LSST
• Need to combine multiple types of observations
– E.g., SuperNovae, Large scale structure
• Increasing complexity of models to compare 
with data
2

6/2/2014
3
Two Basic Solutions
• Increase available computational power
– Buy/apply faster hardware
– Increase level of parallelism
• Develop/apply more efficient algorithms
– Astronomers can learn from statisticians
Evolution of Computing Power
Year
NAP 2011
Two Basic Solutions
• Increase available computational power
– Buy/apply faster hardware
• Computers aren’t getting much faster anymore
– Increase level of parallelism
• Topic of Wednesday lessons/labs
• Apply more efficient algorithms
– Astronomers can learn from statistians
• Topic of Thursday & Friday lessons/labs
Goals for This School
Wednesday
• Which architectures/programming tools are likely to 
result in efficient parallelization of a given problem?
Thursday & Friday
• What algorithms are available that are likely to result in 
more efficient convergence for a given problem?  
• Not time to become expert at any, but enough to help 
you choose which techniques to pursue in detail
3

6/2/2014
4
Goals for Today
Which architectures/programming tools are likely to result 
in efficient parallelization of a given problem?
• Basic properties of different computer architectures
– Modern CPU, Cluster, GPU
• Parallel programming toolboxes
– Fast functions: OpenMP (e.g., parallel for loops within one workstation)
– More involved functions: MPI & map-reduce (e.g., clusters, cloud)
– Very large number of function evaluations:  GPUs
4

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Parallel Computer Architectures
10th Summer School in Statistics for Astronomers
Pierre-Yves Taunay
Research Computing and Cyberinfrastructure
224A Computer Building
The Pennsylvania State University
University Park
py.taunay@psu.edu
June 2014
1 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Introduction
2 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Objectives
1. Understand necessity for “going parallel”
2. Get familiar with HPC terms and architectures
3. Discover some strategies to leverage HPC resources
4. Learn about general problematics in HPC
3 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some acronyms and deﬁnitions
◮FLOPs Floating Point Operation per Second
◮MPI Message Passing Interface
◮HPC High Performance Computing
◮GPU Graphics Processing Unit
◮CPU Central Processing Unit. Made of multiple cores.
◮Core Smallest compute unit on a CPU cluster
◮Cluster Ensemble of compute nodes, connected with high-perf.
interconnect.
◮Compute node Server/Machine that executes code. Contains 1 or more
CPU.
◮Accelerator “Add-on” card used to improve (“accelerate”) execution
time of code.
4 / 69
5

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Motivation for HPC
◮Why do we build big and expensive supercomputers ?
֒→(At constant run time) Solve larger problems
֒→(At constant problem size) Solve problems faster
◮Need to increase the number of FLOPs, memory bandwidth
◮A single core is not enough anymore !
5 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Motivation for HPC
Examples
◮2012–2014 – PIConGPU:
֒→18,000 GPUs (!), 7.1 PFlops at peak
֒→1 s of runtime at peak = 34 days on a single core
Figure: Kelvin–Helmholtz instability.
6 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Motivation for HPC
Examples
◮2014 – Illustris project: cosmological simulation
֒→8,192 cores, 19M CPU-hours; 2,000 yrs on a single core
Figure: Dark matter density overlaid with gas velocity ﬁeld.
7 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
HPC system overview
◮Cluster Ensemble of compute nodes, connected with
high-perf. interconnect.
Figure: Cluster example
8 / 69
6

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
HPC system overview
◮Compute node Server/Machine that executes code.
Contains 1 or more CPU.
Figure: Dell R720
9 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
HPC system overview
◮Core Smallest compute unit on a CPU cluster
Figure: Ivybridge EX die
10 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
HPC system overview
Compute node
◮NIC Network Interface Card
◮QPI Quick Path Interconnect
◮IOH Input/Output Hub
11 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
HPC system overview
Some numbers
Components
Theoretical speed
FDR Inﬁniband 4x
56 Gb/s
PCIe
32 GB/s
DRAM
10–200* GB/s
HDD
100–500 MB/s
QPI
40 GB/s
*low performance can be due to bad cache utilization
12 / 69
7

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Speed up and scaling – 1/2
◮Speed-up and scaling are of utmost concern in HPC
Speed up
Factor of decrease in total execution time compared to a single core program.
SU = tNproc/t1proc
Scaling
How well programs behave when you throw more resources at it.
Strong scaling: Measure the eﬀect of a change in the number of processors while the
total workload of a program stays constant. We have linear scaling if the speed-up is
equal to the total number of processors.
Ss = t1proc/(N · tNproc) · 100
Weak scaling: Measure the eﬀect of a change in the number of processors while the
workload per processor stays constant. We have linear scaling if the total run time
stays constant.
Sw = t1proc/(·tNproc) · 100
13 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Speed up and scaling – 2/2
Case study: PIConGPU
14 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Speed up and scaling – 2/2
◮Speed up for 16–18,432 GPUs: 794 (ideal: 1,152)
Figure: Speed up vs. number of GPUs
15 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Speed up and scaling – 2/2
◮Strong scaling for 16–18,432 GPUs
Figure: Strong scaling vs. number of GPUs
16 / 69
8

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Speed up and scaling – 2/2
◮Weak scaling for 1–18,432 GPUs
Figure: Weak scaling vs. number of GPUs
17 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Amdahl’s law
Amdahl’s law
Theoretical speed-up S can be estimated with
S =
1
(1 −P) + P/N ,
(1)
where P is the percentage of code that can be parallelized, and N
the total number of processor cores.
◮Remark: Amdahl’s law assumes that the problem size is ﬁxed
– the total workload of a program does not change with the
number of processors (strong scaling).
18 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Amdahl’s law
Figure: Amdahl’s law
19 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Gustafson’s law
Gustafson’s law
Theoretical speed-up S can be estimated with
S = N −(1 −P) · (N −1),
(2)
where P is the percentage of code that can be parallelized, and N
the total number of processor cores.
20 / 69
9

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Some theory
Example
◮Assume that 98% of the code can be parallelized. You run
code on a CPU with 12 cores.
֒→Amdahl: S = 9.84
֒→Gustafson: S = 11.8
21 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Summary
We now know about
◮Motivation for HPC
◮HPC systems
◮A bit of theory about scaling and speed-up
22 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
What is HPC ?
◮Using one or more forms of parallelism to improve the
performance and scaling of your code
֒→Vector architecture
֒→Shared memory parallelism
֒→Distributed memory parallelism
֒→(Not today) Accelerators e.g., Graphics Processing Units
23 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Vectorization
24 / 69
10

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Deﬁnition
Vectorization
Parallelization of a program which performs scalar operations to a program that
performs vector operations. A vector operations allow for operating on multiple pairs
of scalar at once. When a processor perform the same instruction on multiple scalars
at once, we talk about “Single Instruction Multiple Data” (SIMD).
Data
Single
Multiple
Instructions
Single
Single core, serial
Single core, vectorized
a = b + c
⃗a = ⃗b + ⃗c
Multiple
Uncommon
Distributed systems
⃗a = ⃗b + ⃗c
x = y ∗z
Table: Flynn’s taxonomy
25 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example
Likelihood estimation – 1/2
◮Multivariate normal PDF for X ∈RN
F (X; µ, Σ) =
1
ð
(2π)N |Σ|
· e−1
2 (X−µ)TΣ−1(X−µ)
◮Likelihood for M observations:
L =
M
Ù
i=1
F (Xi; µ, Σ)
26 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example
Likelihood estimation – 2/2
◮Log–likelihood for M observations:
ln L =
M
Ø
i=1
ln F (Xi; µ, Σ)
ln L = −1/2 ·
C
MN ln 2π + M ln |Σ| +
M
Ø
i=1
(Xi −µ)T Σ−1 (Xi −µ)
D
◮Calculate for various M; data is provided from multivariate normal
PDF with random cov. and expected val.
◮Size of input data:
֒→Random variable observations: X = [NOBS × SIZE]
֒→Expected val.: µ = [1 × SIZE]
֒→Covar.: Σ−1 = [SIZE × SIZE]
27 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example I
Python – 1/2
1 def
l o g _ l i k e l i h o o d _ n a i v e ( obs , nobs , i s i g ,mu, det_sig ) :
2
sum = 0
3
# Sum
a l l
members
4
f o r
i
i n
range (0 , nobs ) :
5
LV = obs [ i ]−mu;
6
RV = np . t r a n s p o s e (LV) ;
7
sum += np . dot ( np . dot (LV , i s i g ) ,RV)
8
9
# S i z e
of
random
v a r i a b l e
10
N = mu. shape [ 0 ]
11
12
# Add the
r e s t
to sum
13
sum += nobs ∗N∗math . lo g (2∗np . p i ) + nobs ∗math . log ( abs (
det_sig ) )
14
sum ∗= −0.5
28 / 69
11

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example II
Python – 1/2
1 def
l o g _ l i k e l i h o o d _ v e c ( obs , nobs , i s i g ,mu, det_sig ) :
2
3
# S i z e
of
random
v a r i a b l e
4
N = mu. shape [ 0 ]
5
6
# L e f t
hand
s i d e :
(X−mu)T∗SIG
7
LV = obs [:] −mu
8
tmp = np . dot (LV , i s i g )
9
10
# Right
hand
s i d e :
(X−mu)
11
RV = np . reshape (LV , [ nobs ∗N, 1 ] )
12
13
# Reshape
l e f t
hand
s i d e
14
LV = np . reshape (tmp , [ 1 , nobs ∗N] )
15
16
# Perform
s i n g l e
sum through
dot
product
17
sum = np . dot (LV ,RV)
18
19
# Rest
i s
the
same
29 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example
Python – 2/2
M
Non-vectorized
Vectorized
Speed–up
101
9.63 × 10−5
3.22 × 10−5
2.95
102
7.87 × 10−4
8.22 × 10−5
9.57
104
7.61 × 10−2
5.43 × 10−3
14.0
106
7.56
3.23 × 10−1
23.4
Table: Timings of two Python implementation of likelihood estimation
30 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example I
R – 1/2
1 l o g _ l i k e l i h o o d _ n a i v e <−f u n c t i o n ( obs , nobs , i s i g ,mu, det_s i g )
2 {
3
sum = 0
4
f o r
( i
i n
seq (1 , nobs ) )
5
{
6
LV <−obs [ i , ] −mu
7
RV <−t (LV)
8
sum <−sum + LV %∗% i s i g %∗% RV
9
}
10
11
# S i z e
of
random
v a r i a b l e
12
N <−dim (mu) [ 2 ]
13
sum <−sum + nobs ∗N∗log (2∗p i ) + nobs ∗log ( abs ( det_s i g ) )
14
sum <−sum∗( −0.5)
15
return (sum)
16 }
31 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example II
R – 1/2
1 l o g _ l i k e l i h o o d _ v e c <−f u n c t i o n ( obs , nobs , i s i g ,mu, det_s i g )
2 {
3
sum = 0
4
5
# S i z e
of
random
v a r i a b l e
6
N <−dim (mu) [ 2 ]
7
8
# Create
a
temporary
c o n t a i n e r
f o r
s u b t r a c t i o n X−mu
9
tmp <−matrix (1 , nrow=nobs , ncol =1)
10
tmp <−tmp %∗% mu
11
LV <−X−tmp
12
13
# F i r s t
part
of
the
product
(X−mu)T∗SIG
14
tmp <−LV %∗% i s i g
15
16
# RHS
17
RV <−matrix (LV , nrow=1,byrow=TRUE)
18
RV <−t (RV)
19
20
# Make
that
an
a r r a y
32 / 69
12

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example III
R – 1/2
21
LV <−matrix (tmp , nrow=1,byrow=TRUE)
22
23
# S i n g l e
sum through
dot
product
24
sum <−LV %∗% RV
25
26
sum <−sum + nobs ∗N∗log (2∗p i ) + nobs ∗log ( abs ( det_s i g ) )
27
sum <−sum∗( −0.5)
28
29
return (sum)
30 }
33 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example
R – 2/2
M
Non-vectorized
Vectorized
Speed–up
101
1.31 × 10−3
1.10 × 10−3
1.19
102
3.50 × 10−3
1.17 × 10−3
3.00
104
2.38 × 10−1
1.52 × 10−2
16.0
106
23.9 × 101
1.63
14.7
Table: Timings of two R implementation of likelihood estimation
34 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example I
IDL – 1/2
1 FUNCTION l o g _ l i k e l i h o o d _ n a i v e , OBS, NOBS,
ISIG , MU,
DET_SIG
2
SUM = 0
3
FOR i = 0 ,NOBS−1 DO BEGIN
4
LV = OBS[ ∗, i ]−MU
5
RV = TRANSPOSE(LV)
6
SUM = SUM + LV ## ISIG ## RV
7
ENDFOR
8
9
;
S i z e
of
a random
v a r i a b l e
10
N = SIZE (MU,/DIMENSIONS)
11
12
SUM = SUM + NOBS∗N∗ALOG(2∗! PI ) + NOBS∗ALOG(ABS(DET_SIG) )
13
SUM = −0.5∗SUM
14
15
RETURN, SUM
16 END
35 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example II
IDL – 1/2
1 FUNCTION l o g _ l i k e l i h o o d _ v e c , OBS, NOBS,
ISIG , MU,
DET_SIG
2
SUM = 0
3
4
;
S i z e
of
a random
v a r i a b l e
5
N = SIZE (MU,/DIMENSIONS)
6
7
;
Create
a
tempprary
c o n t a i n e r
f o r
s u b t r a c t i o n
of X−mu
8
TMP = MAKE_ARRAY(1 ,NOBS,/DOUBLE, VALUE=1)
9
TMP = TMP ## MU
10
LV = OBS−TMP
11
12
;
F i r s t
part
of
the
product
13
TMP = LV ## ISIG
14
15
;
Right
hand
s i d e
16
RV = REFORM(LV ,NOBS∗N, 1 )
17
RV = TRANSPOSE(RV)
18
19
;
Reform TMP i n t o
LV as
a row−a r r a y
20
LV = REFORM(TMP,NOBS∗N, 1 )
36 / 69
13

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example III
IDL – 1/2
21
22
;
S i n g l e
sum through
dot
product
23
SUM = LV ## RV
24
25
SUM = SUM + NOBS∗N∗ALOG(2∗! PI ) + NOBS∗ALOG(ABS(DET_SIG) )
26
SUM = −0.5∗SUM
27
28
RETURN, SUM
29 END
37 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example
IDL – 2/2
M
Non-vectorized
Vectorized
Speed–up
101
2.78 × 10−5
2.01 × 10−5
1.38
102
2.45 × 10−4
1.22 × 10−4
2.01
104
2.39 × 10−2
5.90 × 10−3
4.03
106
2.38
4.59 × 10−1
5.19
Table: Timings of two IDL implementation of likelihood estimation
38 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example I
Julia – 1/2
1 f u n c t i o n
l o g _ l i k e l i h o o d _ n a i v e ( obs : : Array { Float64 ,2} , nobs : :
Int64 , i s i g : : Array { Float64 ,2 } ,mu : : Array { Float64 ,1} ,
det_sig : : Float64 )
2
@ a s s e r t ( s i z e ( obs , 1 )==length (mu)==s i z e ( i s i g , 1 )==s i z e ( i s i g
, 2 ) )
3
N = s i z e ( obs , 1 )
4
RV = Array ( Float64 , 1 )
5
6
t o t = 0.0
7
f o r
i =1: nobs
8
RV = obs [ : , i ] −mu
9
t o t += RV’ ∗i s i g ∗RV
10
end
11
12
t o t += nobs ∗N∗log (2∗p i ) + nobs ∗log ( abs ( det_sig ) )
13
t o t ∗= ( −0.5)
14
15
return
t o t
16 end
39 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example II
Julia – 1/2
1 f u n c t i o n
l o g _ l i k e l i h o o d _ v e c ( obs : : Array { Float64 ,2 } , nobs : :
Int64 , i s i g : : Array { Float64 , 2 } ,mu : : Array { Float64 , 1} ,
det_sig : : Float64 )
2
@ a s s e r t ( s i z e ( obs , 1 )==length (mu)==s i z e ( i s i g , 1 )==s i z e ( i s i g
, 2 ) )
3
t o t = 0.0
4
N = s i z e (mu, 1 )
5
RV = Array { Float64 ,2}
6
7
# X−mu
8
RV = obs [ : , 1 : nobs ]. −mu
9
10
# X−mu ∗
i s i g
11
tmp = i s i g ∗RV
12
13
# Reshape
to
s i n g l e
a r r a y
14
# Some
transpose
to
do
to
f l a t t e n
the
a r r a y
15
RV = reshape (RV, nobs ∗N, 1 )
16
tmp = reshape (tmp , nobs ∗N, 1 )
17
40 / 69
14

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example III
Julia – 1/2
18
t o t = ( transpose (RV) ∗tmp) [ 1 , 1 ]
19
20
t o t += nobs ∗N∗log (2∗p i ) + nobs ∗log ( abs ( det_sig ) )
21
t o t ∗= ( −0.5)
22
23
return
t o t
24 end
41 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Example
Julia – 2/2
M
Naive
Vectorized
Speed–up
101
3.29 × 10−5
1.07 × 10−5
3.07
102
2.17 × 10−4
4.71 × 10−5
4.61
104
2.36 × 10−2
8.01 × 10−3
2.95
106
2.39
6.49 × 10−1
3.68
Table: Timings of two Julia implementation of likelihood estimation
42 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Compiled code vectorization
◮Compiled code can beneﬁt from vectorization
◮Some Intel processors support vectorization through speciﬁc
sets of instructions (SSE, AVX, etc.)
◮Enable it at compile time:
֒→-xHost enables best support for vectorization for a particular
machine
֒→or use speciﬁc set of instructions: -xavx enables AVX. See
manual for more.
43 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Compiled code vectorization
Example – 1/2
1 #define N 1000000
2
3 i n t
main ()
{
4
. . .
5
double
t i c = omp_get_wtime () ;
6
7
for ( i n t
i = 0; i <N; i++)
8
R[ i ] = V1 [ i ] + V2 [ i ]∗V3 [ i ] ;
9
10
double
toc = omp_get_wtime () ;
11
12
toc −= t i c ;
13
14
p r i n t f ( " Total
time : %f \n" ,
toc ) ;
15
. . .
16
return
0;
17 }
44 / 69
15

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Compiled code vectorization
Example – 2/2
◮-O1 -xHost: 0.00431141
◮-O1: 0.0056924
◮Intructions executed at core level
֒→Vectorized: vmulsd (r13,rbp,8), xmm0, xmm1
֒→Not vectorized: mulsd (r13,rbp,8), xmm0, xmm1
45 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Summary
◮SIMD vectorization happens at the processor core level
◮Can control vectorization with some interpreted languages
◮Compiled languages: compile time to enable speciﬁc instructions
◮Next parallel level: multi–core, shared memory
46 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Multi-core architectures
47 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Why multi-core ?
◮Increase performance on a single core
֒→Increase the clock–speed
֒→Cram more transistors
◮BUT power dissipation problems
◮Processors from ’00 to ’14 have similar clock-rates
֒→More recent ones have more cores
Xeon processor
Year
Number
Clock freq.
released
of cores
(GHz)
Nehalem
2010
8
2.26
Westmere
2011
10
2.4
Sandy Bridge
2012
8
3.1
Ivy Bridge
2013
15
2.8
Table: Clock speed and number of cores for recent CPUs
48 / 69
16

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Multi–Core architecture
Ivybridge EX
Figure: Ivybridge EX die
◮Each core has a unique cache (L1, L2)
◮L3 cache is shared by all cores
49 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Cache interlude – 1/3
Cache
Smaller, faster memory that contains copy of data from DRAM, and/or
instructions
◮What happens when CPU core tries to access memory ?
֒→Cache hit: data read from the cache. Fast.
֒→Cache miss: copy data from DRAM, then data is read. Slow.
◮Large cache size ↔slow cache. Hence hierarchy (L1, L2, ...)
50 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Cache interlude – 2/3
◮How does the data get in the caches ?
֒→Compiler
֒→Hardware
֒→OS
◮Programmer is still responsible for some data access patterns !
1 //
F l a t t e n e d
matrix
2 SIZE_X=2048;
3 SIZE_Y=2048;
4
5
double ∗data = ( double ∗)
malloc (SIZE_X∗SIZE_Y∗s i z e o f ( double ) ) ;
6
f o r
( i n t
i =0;
i <SIZE_X ;
i ++)
7
f o r
( i n t
j =0;
j<SIZE_Y ;
j++)
8
data [ j+SIZE_Y∗i ] = 10.0 ∗
3 . 1 4 ;
9
//bad
data
a c c e s s
10
// data [ i+SIZE_Y∗j ] = 10.0 ∗
3 . 1 4 ;
◮Good access: 0.576 s
◮Bad access: 1.04 s
51 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Cache interlude – 3/3
◮Use valgrind and cachegrind to measure cache hit rate
valgrind - -tool=cachegrind ./myprogram
52 / 69
17

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Multiple cores; same machine
Back to multi-core. How to leverage such architecture ?
◮Usually through threads e.g. OpenMP or POSIX threads
◮One process spawns multiple threads
◮Each thread can share memory with other threads
◮Usually map 1 thread to 1 processor core
֒→You can oversubscribe cores... but perf. hits
53 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Multiple cores; same machine
OpenMP
◮Compiler directives, and environment variables for creating
multi-threaded applications
֒→Statements interpreted
֒→Represents functionality like fork and join
◮Directives indicate parallel sections of code e.g.:
1 #pragma omp
p a r a l l e l
for
2 for
( i n t
i = 0;
i <N;
i++)
3
A[ i ] = B[ i ] + C[ i ] ;
◮Standard managed by review board + HW vendors
◮Compile with -openmp with Intel Compilers
◮-fopenmp for GCC
54 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Summary
◮Multiple core processors are ubiquitous in supercomputers
◮Possible to have 2 or more cores in a processor cooperate
through threads
◮Thread libraries: OpenMP, POSIX, Boost
◮Cache eﬀects can be treacherous ! Use callgrind
◮Next parallel level: distributed computations
55 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Distributed architectures
56 / 69
18

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Multiple compute nodes
Motivation
◮Some problems can not ﬁt on only one compute node
֒→Multi-TB problems ?
◮Or need to use many cores for speedup
֒→Remember Amdahl and Gustafson
◮Communication across nodes usually relies on a very fast
interconnect
֒→10 GigE : 10 Gb/s
֒→Inﬁniband : up to 56 Gb/s
57 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Leveraging distributed resources
Communication
◮Sockets over TCP/IP (yikes !)
◮MPI Message Passing Interface
֒→Widely used standard for scientiﬁc apps.
58 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Leveraging distributed resources
Core to process mapping
◮In this approach: 1 processor core = 1 process (usually)
֒→You launch multiple processes on multiple cores, executing
diﬀerent / the same instructions
◮Programmer is responsible for deﬁning each process task
59 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Shared memory vs. distributed memory
Shared memory
Distributed memory
Limited to 1 machine
No limit
1 process = multiple cores
1 process = 1 core
Multiple cores can
Memory local
access same memory
to processor core
Data exchange
Data exchange
through memory bus
through network
60 / 69
19

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Summary
◮Use distributed computing for large parallelizable problems
◮MPI for communication
◮Underlying network: Inﬁniband (fast !)
◮1 core = 1 process
61 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
General problematics in HPC
62 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
General problematics
◮Resource contention
◮I/O
◮Scalability
◮Software dev.
63 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
General problematics
Resource contention
◮Thread 1 wants to write at location A. At the same time,
Thread 2 tries to read from location A. Who wins ?
◮Programmer responsible for avoiding these
◮valgrind and helgrind can detect race conditions in compiled
code
valgrind - -tool=helgrind ./myprog
64 / 69
20

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
General problematics
I/O
◮I/O kills !
◮Why ?
֒→Control returned to the kernel, application stalls: voluntary
context switch
֒→Disk might be used by other processes
֒→Lower BW; high latency
֒→Parallel FS: additional BW and latency
65 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Software development – 1/2
◮Step 1: Prototype
◮Step 2: Debug
◮Step 3: Measure perf.
◮Step 4: Target routines for optimization
◮Go back to step 1
Donald Knuth
Premature optimization is the root of all evil.
66 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Software development – 2/2
Use version control !
◮Git
◮SVN
◮CVS
◮RCS
◮...
67 / 69
Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Conclusion
◮Motivation for HPC
֒→Solve bigger problems, faster
◮HPC system
֒→One cluster = multiple compute nodes
֒→One compute node = 1 or more CPU + memory
֒→One CPU = multiple cores
◮Leveraging HPC resources
֒→Core level: vectorization
֒→CPU level: multi-core programming
֒→Cluster level: distributed programming
68 / 69
21

Intro
Vectorization
Multi-core
Distributed
Problematics
Conclusion
Questions ?
69 / 69
22

Intro
Multicore
Distributed
Conclusion
Parallel Programming for Multicore and
Distributed Systems
10th Summer School in Statistics for Astronomers
Pierre-Yves Taunay
Research Computing and Cyberinfrastructure
224A Computer Building
The Pennsylvania State University
University Park
py.taunay@psu.edu
June 2014
1 / 67
Intro
Multicore
Distributed
Conclusion
Introduction
2 / 67
Intro
Multicore
Distributed
Conclusion
Objectives
1. Have a good understanding of
1.1 Shared memory programs executed on multicore machines, and
1.2 Distributed programs executed on multiple multicore machines
2. Get familiar with programming syntax for
2.1 Shared memory programming, and
2.2 Distributed programming
3 / 67
Intro
Multicore
Distributed
Conclusion
Recap from last session – 1/2
◮Multicore programming
֒→Possible to have 2 or more cores in a processor cooperate
through threads
֒→Thread libraries: OpenMP, POSIX, Boost
◮Distributed programming
֒→Multiple compute nodes are cooperating
֒→MPI for communication paradigm
4 / 67
23

Intro
Multicore
Distributed
Conclusion
Recap from last session – 2/2
Shared memory
Distributed memory
Limited to 1 machine
No limit
1 process = multiple cores
1 process = 1 core
Multiple cores can
Memory local
access same memory
to processor core
Data exchange
Data exchange
through memory bus
through network
5 / 67
Intro
Multicore
Distributed
Conclusion
Disclaimers
◮C for multicore programming
◮C for distributed programming
6 / 67
Intro
Multicore
Distributed
Conclusion
Multicore programming with
OpenMP
7 / 67
Intro
Multicore
Distributed
Conclusion
OpenMP
General concepts – 1/2
◮Shared memory: all threads can access same data pool
◮Still possible to have private data for each thread
8 / 67
24

Intro
Multicore
Distributed
Conclusion
OpenMP
General concepts – 1/2
9 / 67
Intro
Multicore
Distributed
Conclusion
OpenMP
General concepts – 2/2
◮Fork-join model of execution
◮Directive based: give “hints” to the compiler
10 / 67
Intro
Multicore
Distributed
Conclusion
OpenMP
Content
11 / 67
Intro
Multicore
Distributed
Conclusion
OpenMP
Compiling a program
◮Intel compiler: -openmp ﬂag
◮GCC: -fopenmp ﬂag
gcc -fopenmp -O3 myprogram.c -o myprogram
12 / 67
25

Intro
Multicore
Distributed
Conclusion
OpenMP
Running a program
◮Environment variables are set to control program
◮Number of threads to launch: OMP_NUM_THREADS
export OMP_NUM_THREADS = 4
setenv OMP_NUM_THREADS 4
13 / 67
Intro
Multicore
Distributed
Conclusion
OpenMP
Timing parts of a program
◮Function omp_get_wtime()
1
double
t i c = omp_get_wtime () ;
2
// Code to
time . . .
3
double
toc = omp_get_wtime () ;
4
p r i n t f ( " Result : %f \n" ,
toc−t i c ) ;
14 / 67
Intro
Multicore
Distributed
Conclusion
Parallel control structure
◮In a parallel block, code is executed by all threads
1 #include <s t d i o . h>
2 #include <omp.h>
3
4 i n t
main ()
{
5 #pragma omp
p a r a l l e l
6
p r i n t f ( " Hello
world
!
\n" ) ;
7
8
return
0;
9 }
15 / 67
Intro
Multicore
Distributed
Conclusion
Multicore programming with
OpenMP
Work sharing
16 / 67
26

Intro
Multicore
Distributed
Conclusion
Work sharing
for directive – 1/3
◮Multiple ways to share work
◮Simple “for” loop: iterations are distributed amongst threads
1 #pragma omp
p a r a l l e l
2
#pragma omp for
3
for ( i n t
i = 0; i <N; i++)
4
a [ i ] = b [ i ] + c [ i ] ;
17 / 67
Intro
Multicore
Distributed
Conclusion
Work sharing
for directive – 2/3
◮Additional options:
֒→reduction: to perform a reduction
֒→ordered: iterations are executed in same order as a serial core
֒→nowait: threads do not synchronize at end of loop
1 #pragma omp
p a r a l l e l
2
#pragma omp for
r e d u c t i o n (+: out )
3
for ( i n t
i = 0; i <N; i++)
4
out = out + a [ i ] ;
18 / 67
Intro
Multicore
Distributed
Conclusion
Work sharing
for directive – 3/3
1
#pragma omp
p a r a l l e l
2
{
3
#pragma omp for
nowait
4
for ( i n t
i = 0; i <N; i++)
5
c [ i ] = a [ i ] + b [ i ] ;
6
7
#pragma omp for
8
for ( i n t
i = 0; i <N; i++)
9
z [ i ] = x [ i ] + y [ i ] ;
10
} /∗End of
p a r a l l e l
∗/
19 / 67
Intro
Multicore
Distributed
Conclusion
Work sharing
sections directive
◮Distributes a set of structured blocks amongst a “team” of
threads
◮Does not have to have iterations, as in the for directive
◮Synchronization is implied at the end of sections
◮Stand-alone section directives can be nested in a sections
◮Each section is executed once by a thread.
20 / 67
27

Intro
Multicore
Distributed
Conclusion
Work sharing
sections directive
1
#pragma omp
p a r a l l e l
2
{
3
#pragma omp s e c t i o n s
nowait
4
{
5
#pragma omp s e c t i o n
6
for ( i n t
i = 0; i <N; i++)
7
c [ i ] = a [ i ] + b [ i ] ;
8
9
#pragma omp s e c t i o n
10
p r i n t f ( " Hello
!\ n" ) ;
11
} /∗End of
s e c t i o n s
∗/
12
} /∗End of
p a r a l l e l
∗/
21 / 67
Intro
Multicore
Distributed
Conclusion
Work sharing
single directive
◮Single: only one thread executes that block
1
#pragma omp
p a r a l l e l
2
{
3
#pragma omp
s i n g l e
4
{
5
p r i n t f ( " S t a r t i n g
o p e r a t i o n s . . . \ n" ) ;
6
} /∗End
of
s i n g l e
∗/
7
8
#pragma omp
s e c t i o n s
nowait
9
{
10
#pragma omp
s e c t i o n
11
f o r ( i n t
i = 0; i <N; i ++)
12
c [ i ] = a [ i ] + b [ i ] ;
13
14
#pragma omp
s e c t i o n
15
f o r ( i n t
i = 0; i <N; i ++)
16
z [ i ] = x [ i ] + y [ i ] ;
17
} /∗End
of
s e c t i o n s
∗/
18
} /∗End
of
p a r a l l e l
∗/
22 / 67
Intro
Multicore
Distributed
Conclusion
Parallel work sharing
◮Combine directives
1
#pragma omp
p a r a l l e l
for
2
for ( i n t
i = 0;
i < N ;
i++)
3
c [ i ] = a [ i ] + b [ i ] ;
23 / 67
Intro
Multicore
Distributed
Conclusion
Multicore programming with
OpenMP
Data control
24 / 67
28

Intro
Multicore
Distributed
Conclusion
Data model
◮Threads can access global shared data pool
◮Data can be also private to a thread
25 / 67
Intro
Multicore
Distributed
Conclusion
Data control
private directive
◮Control of data is made with pragmas too
1
i n t
my_thread_num = 0;
2
3
#pragma omp
p a r a l l e l
private ( my_thread_num )
4
{
5
// Get
the
thread
number
6
my_thread_num = omp_get_thread_num ()
7
8
p r i n t f ( " Hello
from
thread %d\n" , my_thread_num ) ;
9
} /∗End
p a r a l l e l
∗/
◮Remark: private creates a private variable for each thread,
but does not copy the data. Use ﬁrstprivate or lastprivate to
do so.
26 / 67
Intro
Multicore
Distributed
Conclusion
Data control
Dangers of shared resources
1
#pragma omp
p a r a l l e l
shared (a , b , c , x , y , z )
2
{
3
#pragma omp for
nowait
4
for ( i n t
i = 0;
i < N;
i++)
5
a [ i ] = b [ i ] + c [ i ] ;
6
7
#pragma omp for
8
for ( i n t
i = 0;
i < N;
i++)
9
z [ i ] = a [ i ]∗x [ i ] + y [ i ] ;
10
} /∗End
p a r a l l e l
∗/
◮Can not ensure that “a” was updated before being used again:
race condition
27 / 67
Intro
Multicore
Distributed
Conclusion
Detecting race conditions
◮valgrind with the helgrind tool
◮Compile with -g
gcc -g race.c -o race -fopenmp -std=c99
◮Run valgrind
valgrind - -tool=helgrind ./race
28 / 67
29

Intro
Multicore
Distributed
Conclusion
Multicore programming with
OpenMP
Control ﬂow
29 / 67
Intro
Multicore
Distributed
Conclusion
Controlling the execution ﬂow
barrier directive
1
#pragma omp
p a r a l l e l
shared ( a , b , c , x , y , z )
2
{
3
#pragma omp for
nowait
4
for ( i n t
i = 0;
i < N;
i++)
5
a [ i ] = b [ i ] + c [ i ] ;
6
7
#pragma omp
b a r r i e r
8
9
#pragma omp for
10
for ( i n t
i = 0;
i < N;
i++)
11
z [ i ] = a [ i ]∗x [ i ] + y [ i ] ;
12
} /∗End
p a r a l l e l
∗/
◮Expensive
◮Wasted resources
◮...but sometimes necessary !
30 / 67
Intro
Multicore
Distributed
Conclusion
Controlling the execution ﬂow
Other directives
◮master
◮critical
◮atomic
31 / 67
Intro
Multicore
Distributed
Conclusion
Worked example I
◮Want to calculate log–likelihood on several processes, at once
1
i n t
main ( i n t
argc ,
char ∗argv [ ] )
{
2
. . .
3
//
Parse
the command
l i n e
4
r e t = parse_command_line ( argc , argv ,
5
&nobs ,& s i z e x ,& nsample ,& l o c a t i o n ) ;
6
7
//
Parse
the
data
on
master
8
double ∗buffer_X = ( double ∗) malloc ( nobs∗s i z e x ∗s i z e o f ( double ) ) ;
9
double ∗isigma = ( double ∗) malloc ( s i z e x ∗s i z e x ∗s i z e o f ( double ) ) ;
10
double ∗mu = ( double ∗) malloc ( s i z e x ∗s i z e o f ( double ) ) ;
11
double
det_sigma = 0 . 0 ;
12
13
r e t = read_data ( buffer_X ,
isigma , &det_sigma , mu,
14
&nobs , &s i z e x ,
l o c a t i o n ) ;
15
16
//
Thread
v a r i a b l e s
17
i n t
nthreads = 1;
18
i n t
th_num = 0 ;
19
i n t
th_nobs = nobs ;
20
nthreads = get_num_threads () ;
21
//
Timing
v a r i a b l e s
22
double
t i c ,
toc ,
tot_time = 0 . 0 ;
23
32 / 67
30

Intro
Multicore
Distributed
Conclusion
Worked example II
24
////
Arrays
f o r
a l l
t h r e a d s
25
// The
pool
i s
a l l o c a t e d
i n s i d e
the
shared
memory
26
double ∗pool_LV = ( double ∗) malloc ( nobs∗s i z e x ∗s i z e o f ( double ) ) ;
//
L e f t
hand
s i d e
v e c t o r
(X−mu)
27
double ∗pool_tmp = ( double ∗) malloc ( nobs∗s i z e x ∗s i z e o f ( double ) ) ;
//
Temporary
h o l d e r
f o r
(X−mu)∗SIG
28
double ∗pool_ones = ( double ∗) malloc ( nobs∗s i z e o f ( double ) ) ;
//
Temporary
h o l d e r
to
c r e a t e LV
29
double ∗pool_res = ( double ∗) malloc ( nthreads ∗s i z e o f ( double ) ) ;
//
Each
thread
puts
i t s
r e s u l t
i n
pool_res
30
31
// Use
p o i n t e r s
to
get
the
c o r r e c t
l o c a t i o n
i n
the
a r r a y
32
double ∗LV = NULL ;
33
double ∗tmp = NULL ;
34
double ∗ones = NULL ;
35
double ∗X = NULL ;
36
37
//
Holder
f o r
f i n a l
sum
38
double
final_sum = 0 . 0 ;
39
40
41
42
43
44
// Main
d r i v e r
33 / 67
Intro
Multicore
Distributed
Conclusion
Worked example III
45
#pragma omp
p a r a l l e l
p r i v a t e ( ones , LV , tmp ,X, th_num ,
th_nobs )
d e f a u l t ( shared )
46
{
47
// Get
thread
number
48
th_num = omp_get_thread_num () ;
49
//
Total
number
of
o b s e r v a t i o n s
f o r
that
thread
50
th_nobs = nobs / nthreads ;
51
52
// Use
the
a d d r e s s
to
p o i n t
to
the
c o r r e c t
l o c a t i o n
i n
the
v e c t o r
53
X = &buffer_X [ th_num∗nobs∗s i z e x / nthreads ] ;
54
LV = &pool_LV [ th_num∗th_nobs∗s i z e x ] ;
55
tmp = &pool_tmp [ th_num∗th_nobs∗s i z e x ] ;
56
ones = &pool_ones [ th_num∗th_nobs ] ;
57
58
// Each
p r o c e s s
can now
c a l c u l a t e
the
term
i n
the
59
//
exponent
f o r
a
s u b s e t
of
random v e c t o r s
60
l o g _ l i k e l i h o o d (X, isigma ,mu, det_sigma , th_nobs ,
61
s i z e x ,& pool_res [ th_num ] , LV , tmp , ones ) ;
62
63
#pragma omp
b a r r i e r
64
65
//
Reduction :
sum
a l l
the
i n t e r m e d i a r y
r e s u l t s
66
#pragma omp f o r
r e d u c t i o n (+: final_sum )
67
f o r ( i n t
i = 0;
i < nthreads ;
i ++)
68
final_sum = final_sum + pool_res [ i ] ;
34 / 67
Intro
Multicore
Distributed
Conclusion
Worked example IV
69
} /∗End
of
p a r a l l e l
∗/
70
toc = omp_get_wtime () ;
71
tot_time += toc−t i c ;
72
. . .
73 }
35 / 67
Intro
Multicore
Distributed
Conclusion
Summary
◮Multicore / shared memory: use OpenMP
◮Language extensions for C,C++, and Fortran
◮Compiler directives
+ Relatively easy to use: speedup with not too much eﬀort
+ Good scalability on 1 node
+ No network communication – low latency, high BW
- Threads are heavy
- Limited to one compute node
- Overhead if not enough work for threads
36 / 67
31

Intro
Multicore
Distributed
Conclusion
Going further with OpenMP
◮OpenMP documentation
http://openmp.org/wp/resources/
◮LLNL OpenMP tutorial
http://computing.llnl.gov/tutorials/openMP/
◮RCC HPC Essentials
http://rcc.its.psu.edu/education/seminars/pages/hpc_essentials/HPC2.pdf
37 / 67
Intro
Multicore
Distributed
Conclusion
Distributed programming with
MPI
38 / 67
Intro
Multicore
Distributed
Conclusion
MPI
General concepts – 1/3
◮Process work unit on a single core
◮Rank process number
◮Distributed memory multiple compute nodes
◮Message Passing communication paradigm
◮Send / Receive
◮Buﬀer space where data is stored for send / receive ops
◮Synchronous / Asynchronous wait / don’t wait for transfer to be
complete (ACK from receiver)
◮Blocking / Non-blocking completion of comm. is (in)dependent
of events (e.g. buﬀer that contained data is available for reuse)
◮Communicators, groups can specify topology –
MPI_COMM_WORLD
39 / 67
Intro
Multicore
Distributed
Conclusion
MPI
General concepts – 2/3
◮Distributed memory: each process has its own data
◮Collaboration through message exchange
40 / 67
32

Intro
Multicore
Distributed
Conclusion
MPI
General concepts – 2/3
◮Distributed memory: each process has its own data
◮Collaboration through message exchange
41 / 67
Intro
Multicore
Distributed
Conclusion
MPI
General concepts – 2/3
42 / 67
Intro
Multicore
Distributed
Conclusion
MPI
General concepts – 3/3
◮Process is started on each speciﬁed core
43 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Compiling a program
◮Choice 1: Use the MPI wrappers to compile and link
mpicc -c myﬁle1.c -o myﬁle1.o
mpicc -c myﬁle2.c -o myﬁle2.o
mpicc myﬁle1.o myﬁle2.o -o myprogram
◮Choice 2: Use any compiler; have to provide include ﬁle
location and libraries location as well
gcc -c myﬁle1.c -o myﬁle1.o -I/path/to/mpi/include
gcc -c myﬁle2.c -o myﬁle2.o -I/path/to/mpi/include
gcc myﬁle1.o myﬁle2.o -o myprogram
-L/path/to/mpi/libs -lmpi
44 / 67
33

Intro
Multicore
Distributed
Conclusion
MPI
Running a program
◮Use the command “mpirun”
> mpirun ./hello
45 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Program structure – 1/2
1. Include ﬁle
1 #include <mpi . h>
2. Program start...
1 i n t
main ( i n t
argc ,
char ∗argv [ ] )
{
3. Initialize the MPI environment
1 MPI_Init(&argc ,& argv ) ;
4. Do stuﬀ...
5. “Finalize” the MPI environment
1 MPI_Finalize () ;
46 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Program structure – 2/2
1 #include <mpi . h>
2 #include <s t d i o . h>
3
4 i n t
main ( i n t
argc ,
char ∗argv [ ] )
{
5
6
MPI_Init(&argc ,& argv ) ;
7
p r i n t f ( " Hello
world
!\ n" ) ;
8
MPI_Finalize () ;
9
10
return
0;
11 }
Hello world !
Hello world !
47 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Size and rank – 1/2
◮What is the process ID ?
1
MPI_Comm_rank(comm,∗rank ) ;
2
// Usage :
3
i n t
my_rank ;
4
MPI_Comm_rank(MPI_COMM_WORLD,&my_rank ) ;
◮How many processes did we launch ?
1
MPI_Comm_size(comm,∗s i z e ) ;
2
// Usage :
3
i n t
number_proc ;
4
MPI_Comm_size(MPI_COMM_WORLD,&number_proc ) ;
48 / 67
34

Intro
Multicore
Distributed
Conclusion
MPI
Size and rank – 2/2
1
MPI_Init(&argc ,& argv ) ;
2
i n t
my_rank ,
number_proc ;
3
MPI_Comm_rank(MPI_COMM_WORLD,&my_rank ) ;
4
MPI_Comm_size(MPI_COMM_WORLD,&number_proc ) ;
5
6
p r i n t f ( " Hello
from
p r o c e s s %d of %d !\ n" , my_rank ,
number_proc ) ;
7
MPI_Finalize () ;
Hello from process 1 of 2
Hello from process 0 of 2
49 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Send / Recv – 1/4
◮Message Passing: point-to-point
1
//
Sending
data :
2
MPI_Send(∗b u f f e r , count , type , d e s t i n a t i o n , tag ,comm) ;
3
//
R e c e i v i n g
data :
4
MPI_Recv(∗bu f f e r , count , type , source , tag ,comm, s t a t u s ) ;
50 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Send / Recv – 2/4
1
i n t
my_rank ,
number_proc ;
2
MPI_Comm_rank(MPI_COMM_WORLD,&my_rank ) ;
3
//
Process
0 wants
to
send 10
i n t e g e r s
to
p r o c e s s
1
4
i f ( my_rank == 0) {
5
// Send 10
i n t e g e r s
from
v e c t o r V
6
MPI_Send(V,10 , MPI_INT ,1 , 0 ,MPI_COMM_WORLD) ;
7
}
e l s e
i f
( my_rank == 1) {
8
//
Receive
10
i n t e g e r s
from
v e c t o r V;
s t o r e
i n U
9
MPI_Recv(U,10 , MPI_INT ,0 ,0 ,MPI_COMM_WORLD) ;
10
}
51 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Send / Recv – 3/4
1
i f ( my_rank == 0) {
2
i n t
∗V = ( i n t ∗)
malloc (NELEM∗s i z e o f ( i n t ) ) ;
3
f o r ( i n t
i = 0;
i < NELEM;
i ++)
4
V[ i ] = rand ( ) ;
5
6
print_vector (V,NELEM, my_rank ) ;
7
8
i n t
dest = 1 ;
9
MPI_Send(V,NELEM, MPI_INT , dest , 0 ,MPI_COMM_WORLD) ;
10
p r i n t f ( "New v e c t o r V \n" ) ;
11
print_vector (V,NELEM, my_rank ) ;
12
13
f r e e (V) ;
14
15 }
e l s e
i f ( my_rank == 1) {
16
i n t
∗U = ( i n t ∗)
malloc (NELEM∗s i z e o f ( i n t ) ) ;
17
18
print_vector (U,NELEM, my_rank ) ;
19
20
i n t
s r c = 0 ;
21
MPI_Status
s t a t u s ;
22
MPI_Recv(U,NELEM, MPI_INT , src , 0 ,MPI_COMM_WORLD,& s t a t u s ) ;
23
p r i n t f ( "New v e c t o r U\n" ) ;
24
print_vector (U,NELEM, my_rank ) ;
25
26
f r e e (U) ;
27 }
52 / 67
35

Intro
Multicore
Distributed
Conclusion
MPI
Send / Recv – 4/4
Output
Vector V
Process 0
V[0] = 1918581883
V[1] = 1004453085
V[2] = 786820889
Vector U
Process 1
V[0] = 24259120
V[1] = 0
V[2] = 24265808
New vector V
Process 0
V[0] = 1918581883
V[1] = 1004453085
V[2] = 786820889
New vector U
Process 1
V[0] = 1918581883
V[1] = 1004453085
V[2] = 786820889
53 / 67
Intro
Multicore
Distributed
Conclusion
MPI
A word on distributed memory
◮Watch where you allocate memory !
1
i n t
∗V;
2
i f ( my_rank == 0) {
3
V = ( i n t ∗) malloc (N∗s i z e o f ( i n t ) ) ;
4
}
5
. . .
6
// Seg .
f a u l t
e r r o r −−only
the
p r o c e s s
0 has
a l l o c a t e d
memory
!
7
i f ( my_rank == 1) {
8
V [ 0 ] = 1;
9
}
54 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Collectives
◮Involves all processes in a communicator
◮Examples: broadcast, gather, scatter
55 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Collectives
56 / 67
36

Intro
Multicore
Distributed
Conclusion
MPI
Collectives
◮Message Passing: broadcast
1 MPI_Bcast(∗data , count , type , root ,comm)
1
. . .
2
i n t
data = rand () ;
3 //
Process
0
sends
i t s
v a l u e
to
everyone
4 MPI_Bcast(&data , 1 , MPI_INT , 0 ,MPI_COMM_WORLD) ;
5
. . .
57 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Collectives
◮Message Passing: scatter
1 MPI_Scatter (∗send_data ,
send_count ,
send_datatype ,
2
∗recv_data ,
recv_count ,
recv_datatype ,
3
root , comm)
1
. . .
2
i n t
∗data = NULL ;
//
S i z e = 2∗N elements
3
i n t
∗
r e c v _ b u f f e r = NULL ;
//
S i z e = 2
elements
4
. . .
5
f o r ( i n t
i = 0; i <2∗N; i ++)
6
data [ i ] = rand ( ) ;
7
8 //
Process
0
sends
2
v a l u e s
to
everyone
9 //
0:
[ 0 , 1 ] ,
1:
[ 2 , 3 ] ,
etc .
10 MPI_Scatter ( data , 2 , MPI_INT , recv_buffer , 2 , MPI_INT , 0 ,MPI_COMM_WORLD) ;
11
. . .
58 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Collectives
◮Message Passing: gather
1 MPI_Gather(∗send_data ,
send_count ,
send_datatype ,
2
∗recv_data ,
recv_count ,
recv_datatype ,
3
root , comm)
1
. . .
2
i n t
∗data = NULL ;
//
S i z e = 2
elements
3
i n t
∗
r e c v _ b u f f e r = NULL ;
//
S i z e = 2∗N elements
4
. . .
5
f o r ( i n t
i = 0; i <2; i ++)
6
data [ i ] = rand () ;
7
8 //
Process
0
r e c e i v e s
2
elements
from N p r o c e s s e s
9 MPI_Gather ( data , 2 , MPI_INT , recv_buffer ,2∗N, MPI_INT , 0 ,MPI_COMM_WORLD) ;
10
. . .
59 / 67
Intro
Multicore
Distributed
Conclusion
MPI
Collectives
◮Other eﬃcient collective: reductions
60 / 67
37

Intro
Multicore
Distributed
Conclusion
Worked example I
◮Want to calculate log–likelihood on several processes, at once
1
i n t
main ( i n t
argc ,
char ∗argv [ ] )
{
2
//
S t a r t MPI
3
MPI_Init(&argc ,& argv ) ;
4
// Get
rank
and number
of
proc .
5
i n t
myrank ,
nproc = 0;
6
MPI_Comm_rank(COMM,&myrank ) ;
7
MPI_Comm_size(COMM,& nproc ) ;
8
9
i n t
nobs ,
s i z e x ,
nsample = 0 ;
10
char ∗l o c a t i o n = NULL ;
11
i n t
r e t = 0;
12
13
//
Parse
the command
l i n e
and
s e t
proc_nobs
14
// Number
of
o b s e r v a t i o n s
per
p r o c e s s
15
i n t
proc_nobs ;
16
r e t = parse_command_line ( argc , argv ,&nobs ,& s i z e x ,& nsample ,
17
&l o c a t i o n ,& proc_nobs , myrank , nproc ) ;
18
19
//
A l l o c a t e
the
data
f o r
ALL
p r o c e s s e s .
20
double ∗isigma = ( double ∗) malloc ( s i z e x ∗s i z e x ∗s i z e o f ( double ) ) ;
21
double ∗mu = ( double ∗) malloc ( s i z e x ∗s i z e o f ( double ) ) ;
22
double
det_sigma = 0 . 0 ;
23
61 / 67
Intro
Multicore
Distributed
Conclusion
Worked example II
24
//
Only
the
f i r s t
p r o c e s s
w i l l
parse
the
data .
Others
w i l l
wait
25
double ∗buffer_X = NULL ;
26
i f (
myrank == 0
)
{
27
i n t
r e t = 0;
28
buffer_X = ( double ∗) malloc ( nobs∗s i z e x ∗s i z e o f ( double ) ) ;
29
r e t = read_data ( buffer_X ,
isigma , &det_sigma ,
30
mu, &nobs , &s i z e x ,
l o c a t i o n ) ;
31
}
32
// Wait
f o r
the
p r o c e s s
0
to
f i n i s h
p a r s i n g
the
f i l e s
33
MPI_Barrier (COMM) ;
34
35
//
Timing
v a r i a b l e s
36
double
t i c ,
toc , tot_time = 0 . 0 ;
37
38
// Sums
39
double
final_sum = 0 . 0 ;
40
double my_sum = 0 . 0 ;
41
42
//
I n d i v i d u a l
a r r a y s
43
double ∗X = ( double ∗) malloc ( proc_nobs∗s i z e x ∗s i z e o f ( double ) ) ;
44
double ∗LV = ( double ∗) malloc ( proc_nobs∗s i z e x ∗s i z e o f ( double ) ) ;
45
double ∗tmp = ( double ∗) malloc ( proc_nobs∗s i z e x ∗s i z e o f ( double ) ) ;
46
double ∗ones = ( double ∗) malloc ( proc_nobs∗s i z e o f ( double ) ) ;
47
48
49
//
Process
0
sends
data
to
everyone
with
c o l l e c t i v e s
62 / 67
Intro
Multicore
Distributed
Conclusion
Worked example III
50
MPI_Bcast ( isigma , s i z e x ∗s i z e x ,MPI_DOUBLE, 0 ,COMM) ;
51
MPI_Bcast (mu, s i z e x ,MPI_DOUBLE, 0 ,COMM) ;
52
MPI_Bcast(&det_sigma , 1 ,MPI_DOUBLE, 0 ,COMM) ;
53
MPI_Scatter ( buffer_X , proc_nobs∗s i z e x ,MPI_DOUBLE,X, proc_nobs∗s i z e x ,
MPI_DOUBLE, 0 ,COMM) ;
54
55
t i c = omp_get_wtime () ;
56
final_sum = 0 . 0 ;
57
l o g _ l i k e l i h o o d (X, isigma ,mu, det_sigma , proc_nobs , s i z e x ,&my_sum , LV ,
tmp , ones ) ;
58
59
// Combine
a l l
the
i n t e r m e d i a r y
sums
i n
a
s i n g l e
one
60
MPI_Reduce(&my_sum,& final_sum , 1 ,MPI_DOUBLE,MPI_SUM, 0 ,COMM) ;
61
MPI_Barrier (COMM) ;
62
toc = omp_get_wtime () ;
63
tot_time += toc−t i c ;
64
. . .
65
i f
(
myrank == 0
)
{
66
f r e e ( buffer_X ) ;
67
}
68
//
Etc .
69
MPI_Finalize () ;
70
return EXIT_SUCCESS ;
71 }
63 / 67
Intro
Multicore
Distributed
Conclusion
Summary
◮Distributed programming: use MPI
◮Library for C, C++, and Fortran
◮Programmer responsible for a lot of stuﬀ!
+ Standard
+ Optimized collective communication
+ Can overlap communication and computation
- Strong learning curve
- Diﬃcult (?) to debug
- Communication through network = optimization issues
64 / 67
38

Intro
Multicore
Distributed
Conclusion
Going further with MPI
◮OpenMPI documentation
http://www.open-mpi.org/doc/
◮MPI tutorial
http://mpitutorial.com/
◮LLNL MPI tutorial
https://computing.llnl.gov/tutorials/mpi/
◮RCC HPC Essentials
http://rcc.its.psu.edu/education/seminars/pages/hpc_essentials/HPC3.pdf
65 / 67
Intro
Multicore
Distributed
Conclusion
Conclusion
◮Covered two approaches for shared and distributed mem.
◮Shared memory or distributed memory ?
֒→Depends on pb. size
֒→Amdahl’s law !
◮Combine best of both worlds: hybrid approach
◮Other easier (?) distributed approaches
֒→Python – MPI4py
֒→R – RMPI
֒→Julia
66 / 67
Intro
Multicore
Distributed
Conclusion
Questions ?
67 / 67
39

40

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPU Computing Architectures
10th Summer School in Statistics for Astronomers
Pierre-Yves Taunay
Research Computing and Cyberinfrastructure
224A Computer Building
The Pennsylvania State University
University Park
py.taunay@psu.edu
June 2014
1 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Introduction
2 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Objectives
1. (Re)discover GPUs
2. Reasons for GPU computing
3. Review GPU architectures
4. Example(s)
3 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Reminders
◮Thread: Sequence of instructions to be executed on a core
◮SIMD: Single Instruction Multiple Data
4 / 45
41

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPU
◮GPU: Graphics Processing Unit
◮Dedicated to graphics
◮Highly parallel architecture
◮Better at that than CPUs
5 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPGPU – what
◮GPGPU: General Purpose computing on GPU
◮Took oﬀwith introduction of CUDA in 2006
◮CUDA: Compute Uniﬁed Device Architecture
֒→Hardware and software model for NVIDIA GPUs
◮Alternative: OpenCL
6 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPGPU – where
◮Everywhere !
֒→Finance
֒→Computational Engineering
֒→Numerical Methods
֒→Defense
֒→Computational Chemistry
֒→Astrophysics
֒→...
7 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPGPU – why
◮Previous session: expensive machines to solve larger problems
faster
◮GPUs: do that at a fraction of the cost !
Hardware
Flops (DP)
Power (W)
Price (k$)
2 Ivybridge EX
(2×15 cores 2.8 GHz;
0.672 TFlops
310
8.4–13.7
8 DP ops/cycle)
K40 GPU
1.43 TFlops
235
3–4
GTX Titan Black
1.7 TFlops
250
1
Table: K40 GPU vs. GTX Titan Black vs. dual socket server with
Ivybridge EX
◮Can use a gamer’s card (e.g. GTX) to do calculations
֒→Titan Black – $1k
8 / 45
42

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPGPU – why
Great ! Let’s ditch the CPU, then.
◮Not so fast !
◮CPUs are great at serial
◮Still needed for other ops
◮Share load CPU/GPU
◮Amdahl’s law
9 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPGPU – how
◮Diﬀerent approaches throughout the years
◮Used to be C only
C, C++, Python, Fortran, Haskell, IDL, Java, Julia, LUA,
Mathematica, MATLAB, .NET, Perl, Ruby, R
10 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Upcoming
◮CPU vs. GPU
◮GPU computing architecture
◮Execution model
◮GPU memory architecture
◮Example
11 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
CPU vs GPU
12 / 45
43

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
CPU, GPU
CPU – host
◮Multiple cores e.g. 15/CPU - quad-socket: 60 cores
◮Run 1 thread / core
◮Heavy threads
GPU – device
◮NVIDIA card: 32 threads minimum
֒→32 threads = 1 warp
◮2048 threads run actively on a streaming multiprocessor
(SMX)
◮15 SMX on a card →30k+ concurrent threads
◮Lightweight threads
13 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPU Integration
Figure: Schematic of a compute node with GPUs
14 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPU Integration
A word on memory spaces
◮CPU and GPU: distinct memories
◮Remark: CUDA 6 – Uniﬁed Memory
15 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Summary
◮Many more lightweight threads on GPU
◮GPU is a PCIe card →transfer rates !
◮GPU and CPU: not same memory
16 / 45
44

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPU Architecture
GK110
17 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GK110 – at large
18 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GK110 – SMX – 1/4
19 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GK110 – SMX – 2/4
◮4 warp schedulers
◮Bunch of execution units:
֒→192 CUDA cores
֒→64 double prec. (DP) units
֒→32 load/store (LD) units
֒→32 Special Function Units (SFU)
◮L1 cache / Shared memory
◮Texture memory
◮Registers for threads
20 / 45
45

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GK110 – SMX – 3/4
Warps
◮32 threads
◮Scheduled through warp schedulers
◮Warp execute the exact same instructions – SIMD
SMX
◮Schedulers select four warps
◮Issues one instruction from each warp to a group of cores /
LD-ST units / SFU
◮Instructions can be dual issued, including DP
21 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GK110 – SMX – 4/4
Remark – can’t predict scheduling order
22 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Summary
◮GPU has multiple SMX that execute thread instructions
◮Scheduling through “warp schedulers”
23 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Execution model
24 / 45
46

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Programmer’s POV
◮GPU “function”: kernel
◮CUDA threads are organized in blocks
◮Blocks are organized in grids
25 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Physical organization
Actual architecture
26 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Executing GPU program
Asynchronous behavior
◮CPU initializes the device
◮CPU queues GPU kernels
◮Control returns to CPU after queuing: asynchronous
[Some_program]
1
cpu_func1 () ;
2
gpu_kernel1 <<< >>> ( ) ;
3
cpu_func2 () ;
4
cpu_func3 () ;
5
gpu_kernel2 <<< >>> ( ) ;
6
cpu_func4 () ;
7
cudaDeviceSynchronize () ;
27 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Summary
◮Programmer’s POV: kernel, grid, blocks, threads
◮GPU execution is asynchronous with CPU
28 / 45
47

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPU Architecture
Memory
29 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
GPU DRAM
◮Limited
◮5 GB
◮K40: 12 GB
30 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
The logical organization
31 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
The logical organization
Memory
Size
Scope
R/W
Latency
BW
(cycles)
(GB/s)
Global
5 GB
Grid
R/W
400-800
208
Constant
64 kB
Grid
R
1-800
N/A
Texture
N/A
Grid
R
1-800
N/A
Shared
16/32/48 kB per SM
Block
R/W
2-4
2,260
Local
512 kB per th.
Thread
R/W
400-800
N/A
Registers
255 per th.
Thread
R/W
1
N/A
Table: Memory perf. of the Tesla K20
32 / 45
48

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
The physical layout
33 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
The physical layout
GPU
L1 cache
L2 cache
SMEM size
Max. resident
size (kB)
size (kB)
(kB)
threads
Tesla K20
48/32/16
1,536
16/32/48
2,048
Table: Physical characteristics for GK110
34 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Summary
◮GPU memory is limited
◮Diﬀerent memory and caches perf.
֒→Optimization points
35 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Example
Likelihood calculation
36 / 45
49

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Using GPUs I
Native CUDA
1 i n t
main ( i n t
argc ,
char
∗argv [ ] )
{
2
i n t
nobs ,
s i z e x ,
nsample = 0;
3
char
∗l o c a t i o n = NULL ;
4
i n t
r e t = 0;
5
6
//
Parse
the command
l i n e
7
r e t = parse_command_line ( argc , argv ,&nobs ,& s i z e x ,
8
&nsample ,& l o c a t i o n ) ;
9
10
//
Parse
the
data
on CPU
11
double
∗X = ( double ∗) malloc ( nobs ∗s i z e x ∗s i z e o f ( double ) ) ;
12
double
∗isigma = ( double ∗) malloc ( s i z e x ∗s i z e x ∗s i z e o f ( double
) ) ;
13
double
∗mu = ( double ∗) malloc ( s i z e x ∗s i z e o f ( double ) ) ;
14
double
det_sigma = 0 . 0 ;
15
16
r e t = read_data (X,
isigma , &det_sigma , mu,
17
&nobs , &s i z e x ,
l o c a t i o n ) ;
18
19
//
Timing
v a r i a b l e s
20
double
t i c ,
toc ,
tot_time = 0 . 0 ;
21
37 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Using GPUs II
Native CUDA
22
//
R e s u l t
23
double
r e s = 0 . 0 ;
24
25
//
A l l o c a t e GPU memory
26
double
∗d_LV ,
∗d_tmp ,
∗d_ones ;
27
cudaMalloc ( ( void ∗∗)&d_LV , nobs ∗s i z e x ∗s i z e o f ( double ) ) ;
28
cudaMalloc ( ( void ∗∗)&d_tmp , nobs ∗s i z e x ∗s i z e o f ( double ) ) ;
29
cudaMalloc ( ( void ∗∗)&d_ones , nobs ∗s i z e o f ( double ) ) ;
30
31
double
∗d_X ,
∗d_isigma ,
∗d_mu ;
32
cudaMalloc ( ( void ∗∗)&d_X , nobs ∗s i z e x ∗s i z e o f ( double ) ) ;
33
cudaMalloc ( ( void ∗∗)&d_isigma , s i z e x ∗s i z e x ∗s i z e o f ( double ) ) ;
34
cudaMalloc ( ( void ∗∗)&d_mu, s i z e x ∗s i z e o f ( double ) ) ;
35
36
// Copy
the
data
read
onto
the GPU
37
cudaMemcpy(d_X ,X, nobs ∗s i z e x ∗s i z e o f ( double ) ,
cudaMemcpyHostToDevice ) ;
38
cudaMemcpy( d_isigma , isigma , s i z e x ∗s i z e x ∗s i z e o f ( double ) ,
cudaMemcpyHostToDevice ) ;
39
cudaMemcpy(d_mu,mu, s i z e x ∗s i z e o f ( double ) ,
cudaMemcpyHostToDevice ) ;
40
38 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Using GPUs III
Native CUDA
41
//
Create
a
handle
f o r
c u b l a s
42
cublasHandle_t
handle ;
43
c u b l a s S t a t u s _ t
s t a t ;
44
s t a t = c u b l a s C r e a t e (& handle ) ;
45
cublasSetPointerMode ( handle ,CUBLAS_POINTER_MODE_HOST) ;
46
47
t i c = omp_get_wtime () ;
48
r e s
= 0 . 0 ;
49
50
// Main
d r i v e r
51
l o g _ l i k e l i h o o d (d_X , d_isigma , d_mu, det_sigma , nobs , s i z e x ,& res
, d_LV , d_tmp , d_ones ,& handle ) ;
52
53
toc = omp_get_wtime () ;
54
tot_time += toc−t i c ;
55
. . .
56
cudaFree (d_mu) ;
57
cudaFree (d_X) ;
58
cudaFree ( d_isigma ) ;
59
cudaFree (d_LV) ;
60
cudaFree (d_tmp) ;
61
cudaFree ( d_ones ) ;
39 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Using GPUs IV
Native CUDA
62
63
f r e e (X) ;
64
f r e e ( isigma ) ;
65
f r e e (mu) ;
66
f r e e ( l o c a t i o n ) ;
67
68
return EXIT_SUCCESS ;
69 }
40 / 45
50

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Timing results
CPU vs GPU
NP
10
102
104
106
Serial
1
1.01 × 10−4
1.60 × 10−4
3.60 × 10−3
4.61 × 10−1
OpenMP
2
2.20 × 10−4
5.10 × 10−4
2.03 × 10−3
3.12 × 10−1
4
–
2.60 × 10−4
1.78 × 10−3
2.57 × 10−1
8
–
–
1.27 × 10−3
1.55 × 10−1
16
–
–
1.00 × 10−3
1.80 × 10−1
MPI
8
–
–
7.00 × 10−3
1.14 × 10−1
16
–
–
4.87 × 10−3
6.15 × 10−2
32
–
–
–
4.49 × 10−2
64
–
–
–
2.89 × 10−2
GPU native
1
6.80 × 10−5
8.7 × 10−5
2.11 × 10−4
1.46 × 10−2
Table: Runtime of the log–likelihood example for various number of
processors (NP) and dataset size.
41 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Conclusion
42 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Conclusion
◮GPUs are great at parallel tasks
֒→Large amount of lightweight threads
֒→Inherent parallel architecture w/ SMX, warp schedulers
◮Programmer’s POV
֒→Kernels, grids, blocks, threads
֒→Asynchronous execution (mostly)
֒→Can’t access CPU mem.
֒→Limited memory: large optimization target
◮Multiple languages for GPU programming
43 / 45
Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Want to learn more about GPU programming ?
Online resources
◮CUDA: http://www.nvidia.com/object/cuda_home_new.html
◮OpenCL: https://www.khronos.org/opencl/
MOOC
◮Coursera: Intro to heterogeneous computing – Wen-Mei Hwu
◮Udacity: Intro to parallel programming – NVIDIA
Books
◮Programming Massively Parallel Processors – David B. Kirk,
Wen-mei W. Hwu
◮CUDA By Example – Jason Sanders, Edward Kandrot
◮Numerical Computations with GPUs – Volodymyr Kindratenko
44 / 45
51

Introduction
CPU, GPU
GPU – GK110
GPU – Memory
Example
Conclusion
Questions ?
45 / 45
52

References
Importance Sampling
Jessi Cisewski
Carnegie Mellon University
June 2014
Jessi Cisewski (CMU)
Importance Sampling
References
Outline
1 Recall: Monte Carlo integration
2 Importance Sampling
3 Examples of Importance Sampling
(a) Monte Carlo, Monaco
(b) Monte Carlo Casino
⋆Some content and examples from Wasserman (2004)
Jessi Cisewski (CMU)
Importance Sampling
References
Simple illustration: what is π?
Area◦
Area□
=
πr2
(2r)(2r) = π
4
Jessi Cisewski (CMU)
Importance Sampling
References
Monte Carlo Integration: motivation
I =
Z b
a
h(y)dy
Goal: evaluate this integral
Sometimes we can ﬁnd I (e.g. if h(·) is a function from Calc I)
But sometimes we can’t and need a way to approximate I. Monte
Carlo methods are one (of many) approaches to do this.
Jessi Cisewski (CMU)
Importance Sampling
53

References
The Law of Large Numbers
While nothing is more uncertain than the
duration of a single life, nothing is more certain
than the average duration of a thousand lives.
∼Elizur Wright
Figure: Elizur Wright (1804 - 1885), American mathematician, the “father of
life insurance”, “father of insurance regulation” (http://en.wikipedia.org)
Jessi Cisewski (CMU)
Importance Sampling
References
Law of Large Numbers
The Law of Large Numbers describes what happens when performing the same
experiment many times.
After many trials, the average of the results should be close to the expected value and
will be more accurate with more trials.
For Monte Carlo simulation, this means that we can learn properties of a random
variable (mean, variance, etc.) simply by simulating it over many trials.
Suppose we want to estimate the probability, p, of a coin landing “heads up”. How
many times should we ﬂip the coin?
Jessi Cisewski (CMU)
Importance Sampling
References
Law of Large Numbers (LLN)
Given an independent and identically distributed sequence of
random variables Y1, Y2, . . . , Yn with ¯Yn = n−1 Pn
i=1 Yi and
E(Yi) = µ, then for every ǫ > 0
P(| ¯Yn −µ| > ǫ) −→0,
as n −→∞.
Jessi Cisewski (CMU)
Importance Sampling
References
Monte Carlo Integration
General idea
Monte Carlo methods are a form of stochastic integration used to
approximate expectations by invoking the law of large numbers.
I =
Z b
a
h(y)dy =
Z b
a
w(y)f (y)dy = Ef (w(Y ))
where f (y) =
1
b−a and w(y) = h(y) · (b −a)
f (y) =
1
b−a is the pdf of a U(a,b) random variable
By the LLN, if we take an iid sample of size N from U(a, b), we
can estimate I as
ˆI = N−1
N
X
i=1
w(Yi) −→E(w(Y )) = I
Jessi Cisewski (CMU)
Importance Sampling
54

References
Monte Carlo Integration: standard error
I =
Z b
a
h(y)dy =
Z b
a
w(y)f (y)dy = Ef (w(Y ))
Monte Carlo estimator: ˆI = N−1 PN
i=1 w(Yi)
Standard error of estimator:
ˆ
SE =
s
√
N where
s2 = (N −1)−1
N
X
i=1

w(Yi) −ˆI
2
Jessi Cisewski (CMU)
Importance Sampling
References
Monte Carlo Integration: Gaussian CDF example⋆
Goal: estimate FY (y) = P(Y ≤y) = E

I(−∞,y)(Y )

where
Y ∼N(0, 1):
F(Y ≤y) =
Z y
−∞
1
√
2π
e−t2/2dt =
Z ∞
−∞
h(t)
1
√
2π
e−t2/2dt
where h(t) = 1 if t < y and h(t) = 0 if t ≥y
Draw an iid sample Y1, . . . , YN from a N(0, 1), then the estimator
is
ˆI = N−1
N
X
i=1
h(Yi) = # draws < x
N
⋆Example 24.2 of Wasserman (2004)
Jessi Cisewski (CMU)
Importance Sampling
References
Importance Sampling: motivation
Standard Monte Carlo integration is great if you can sample from
the target distribution (i.e. the desired distribution)
−→But what if you can’t sample from the target?
Idea of importance sampling: draw the sample from a proposal
distribution and re-weight the integral using importance weights so
that the correct distribution is targeted
Jessi Cisewski (CMU)
Importance Sampling
References
Monte Carlo Integration −→Importance Sampling
I =
Z
h(y)f (y)dy
h is some function and f is the probability density function of Y
When the density f is diﬃcult to sample from, importance
sampling can be used
Rather than sampling from f , you specify a diﬀerent probability
density function, g, as the proposal distribution.
I =
Z
h(y)f (y)dy =
Z
h(y) f (y)
g(y)g(y)dy =
Z h(y)f (y)
g(y)
g(y)dy
Jessi Cisewski (CMU)
Importance Sampling
55

References
Importance Sampling
I = Ef [h(Y )] =
Z h(y)f (y)
g(y)
g(y)dy = Eg
h(Y )f (Y )
g(Y )

Hence, given an iid sample Y1, . . . , YN from g, our estimator of I
becomes
ˆI = N−1
N
X
i=1
h(Yi)f (Yi)
g(Yi)
−→Eg
h(Y )f (Y )
g(Y )

= I
Jessi Cisewski (CMU)
Importance Sampling
References
Importance Sampling: selecting the proposal distribution
The standard error of ˆI could be inﬁnite if g(·) is not selected
appropriately −→g should have thicker tails than f (don’t want
ratio f /g to get large)
Eg
"h(Y )f (Y )
g(Y )
2#
=
Z h(y)f (y)
g(y)
2
g(y)dy
Select a g that has a similar shape to f , but with thicker tails
Variance of ˆI is minimized when g(y) ∝|f (y)|
Want to be able to sample from g(y) with ease
Jessi Cisewski (CMU)
Importance Sampling
References
Importance sampling: Illustration
Goal: estimate P(Y < 0.3) where Y ∼f
Try two proposal distributions: U(0,1) and U(0,4)
Jessi Cisewski (CMU)
Importance Sampling
References
Importance sampling: Illustration, continued.
If take 1000 samples of size 100, and ﬁnd the IS estimates, we get
the following estimated expected values and variances.
Expected Value
Variance
Truth
0.206
0
g1: U(0,1)
0.206
0.0014
g2: U(0,4)
0.211
0.0075
Jessi Cisewski (CMU)
Importance Sampling
56

References
Monte Carlo Integration: Gaussian tail probability example⋆
Goal: estimate P(Y ≥3) where Y ∼N(0, 1) (Truth is ≈
0.001349)
P(Y > 3) =
Z ∞
3
1
√
2π
e−t2/2dt =
Z ∞
−∞
h(t)
1
√
2π
e−t2/2dt
where h(t) = 1 if t > 3 and h(t) = 0 if t ≤3
Draw an iid sample Y1, . . . , Y100 from a N(0, 1), then the
estimator is
ˆI =
1
100
100
X
i=1
h(Yi) = # draws > 3
100
⋆Example 24.6 of Wasserman (2004)
Jessi Cisewski (CMU)
Importance Sampling
References
Gaussian tail probability example⋆, continued.
Draw an iid sample Y1, . . . , Y100 from a N(0, 1), then the
estimator is
ˆI =
1
100
100
X
i=1
h(Yi)
Draw an iid sample Y1, . . . , Y100 from a N(4, 1), then the
estimator is
ˆI =
1
100
100
X
i=1
h(Yi)f (Yi)
g(Yi)
where f is the density of a N(0,1) and g is the density of N(4,1)
⋆Example 24.6 of Wasserman (2004)
Jessi Cisewski (CMU)
Importance Sampling
References
Gaussian tail probability example⋆, continued.
If take N samples of size 100, and ﬁnd the MC and IS estimates,
we get the following estimated expected values and variances.
N = 105
Expected Value
Variance
Truth
0.00135
0
Monte Carlo
0.00136
1.3 × 10−5
Importance Sampling
0.00135
9.5 × 10−8
Jessi Cisewski (CMU)
Importance Sampling
References
Extensions of Importance Sampling
Sequential Importance Sampling
Sequential Monte Carlo (Particle Filtering)
−→See Doucet et al. (2001)
Approximate Bayesian Computation −→See Turner and Zandt (2012)
for a tutorial, and Cameron and Pettitt (2012); Weyant et al. (2013) for
applications to astronomy
Jessi Cisewski (CMU)
Importance Sampling
57

References
Bibliography
Cameron, E. and Pettitt, A. N. (2012), “Approximate Bayesian Computation for Astronomical Model Analysis: A
Case Study in Galaxy Demographics and Morphological Transformation at High Redshift,” Monthly Notices of
the Royal Astronomical Society, 425, 44–65.
Doucet, A., De Freitas, N., and Gordon, N. (2001), Sequential Monte Carlo Methods in Practice, Statistics for
Engineering and Information Science, New York: Springer-Verlag.
Turner, B. M. and Zandt, T. V. (2012), “A tutorial on approximate Bayesian computation,” Journal of
Mathematical Psychology, 56, 69 – 85.
Wasserman, L. (2004), All of statistics: a concise course in statistical inference, Springer.
Weyant, A., Schafer, C., and Wood-Vasey, W. M. (2013), “Likeihood-free cosmological inference with type Ia
supernovae: approximate Bayesian computation for a complete treatment of uncertainty,” The Astrophysical
Journal, 764, 116.
Jessi Cisewski (CMU)
Importance Sampling
58

References
Approximate Bayesian Computing
using Sequential Sampling
Jessi Cisewski
Carnegie Mellon University
June 2014
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Recall: Basic ABC algorithm
For the observed data y1:n, prior π(θ) and distance function ρ:
Algorithm
1 Sample θ∗from prior π(θ)
2 Generate x1:n from forward process f (y | θ∗)
3 Accept θ∗if ρ(y1:n, x1:n) < ǫ
4 Return to step 1
Generates a sample from an approximation of the posterior:
f (x1:n | ρ(y1:n, x1:n, θ) < ǫ) · π(θ) ≈f (y1:n | θ)π(θ) ∝π(θ | y1:n)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Summary of basic ABC
Decisions that need to be made:
1
Select distance function (ρ) and summary statistic(s)
2
Tolerance (ǫ)
Finding the “right” ǫ can be ineﬃcient
−→we end up throwing away many of the theories proposed
from the selected priors
How can we improve this basic algorithm?
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Decreasing tolerances ǫ1 ≥· · · ≥ǫT
ABC - Population Monte Carlo algorithm∗(ABC - PMC)
1
At t = 1
For i = 1, . . . , N particles
Generate θ(1)
i
∼π(θ) and x ∼f (y | θ1
i ) until ρ(y, x) < ǫ1
Set w (1)
i
= N−1
2
At t = 2, . . . , T
Set τ 2
t = 2 · var

θ(t−1)
1:N

For i = 1, . . . , N particles
Draw θ∗
i ∼multinomial

θ(t−1)
1:N
, w (t−1)
1:N

Generate θ(t)
i
| θ∗
i ∼N(θ∗
i , τ 2
t ) and x ∼f (y | θ(t)
i ) until
ρ(y, x) < ǫt
Set w (t)
i
∝π(θ(t)
i )/ PN
j=1 w (t−1)
j
φ[τ −1
t
(θ(t)
i
−θ(t−1)
j
)]
φ(·) is the density function of a N(0, 1)
∗From Beaumont et al. (2009)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
59

References
Recall: Mean of a Gaussian with known σ2
Given the following model:
µ ∼N(µ0, σ2
0)
Yi | µ, σ2 ∼N(µ, σ2)
The posterior is
π(µ | y1:n) ∼N(µ1, σ2
1)
where
µ1 =

µ0
σ2
0 +
P yi
σ2


1
σ2
0 + n
σ2
 ,
σ2
1 =
1

1
σ2
0 + n
σ2

Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Recall: Mean of a Gaussian with σ2 known: R code
n=25
#number of observations
N=1000
#particle sample size
true.mu = 0; sigma = 1
mu.hyper = 0; sigma.hyper = 10
data=rnorm(n,true.mu,sigma)
epsilon=0.005
mu=numeric(N)
rho=function(y,x) abs(sum(y)-sum(x))/n
for(i in 1:N){
d= epsilon +1
while(d>epsilon) {
proposed.mu=rnorm(1,0,sigma.hyper) #<--prior draw
x=rnorm(n, proposed.mu, sigma)
d=rho(data,x)}
mu[i]= proposed.mu}}
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Mean of a Gaussian with σ2 known
Sequential version
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Mean of a Gaussian with σ2 known: Sequential R code
# INPUTS
n=25
#number of observations
N=2500
#particle sample size
true.mu = 0
sigma = 1
mu.hyper = 0
sigma.hyper = 10
data=rnorm(n,true.mu,sigma)
epsilon = 1
time.steps = 20
weights = matrix(1/N,time.steps,N)
mu=matrix(NA,time.steps,N)
d=matrix(NA,time.steps,N)
rho=function(y,x) abs(sum(y)-sum(x))/n
Jessi Cisewski (CMU)
ABC using Sequential Sampling
60

References
Mean of a Gaussian with σ2 known: Sequential R code
for(t in 1:time.steps){
if(t==1){
for(i in 1:N){
d[t,i]= epsilon +1
while(d[t,i]>epsilon) {
proposed.mu=rnorm(1,0,sigma.hyper) #<--prior draw
x=rnorm(n, proposed.mu, sigma)
d[t,i]=rho(data,x)}
mu[t,i]= proposed.mu
}} else{}
}
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Mean of a Gaussian with σ2 known: Sequential R code
for(t in 1:time.steps){ if(t==1){} else{
epsilon = c(epsilon,quantile(d[t-1,],.75))
mean.prev <- sum(mu[t-1,]*weights[t-1,])
var.prev <- sum((mu[t-1,] - mean.prev)^2*weights[t-1,])
for(i in 1:N){d[t,i]= epsilon[t]+1
while(d[t,i]>epsilon[t]) {
sample.particle <- sample(N, 1, prob = weights[t-1,])
proposed.mu0 <- mu[t-1, sample.particle]
proposed.mu <- rnorm(1, proposed.mu0, sqrt(2*var.prev))
x <- matrix(rnorm(n,proposed.mu, sigma),n,1)
d[t,i]=rho(data,x) }
mu[t,i]= proposed.mu
mu.weights.denominator<-
sum(weights[t-1,]*dnorm(proposed.mu,mu[t-1,],sqrt(2*var.prev)))
mu.weights.numerator<-dnorm(proposed.mu,0,sigma.hyper)
weights[t,i] <- mu.weights.numerator/mu.weights.denominator
}}
weights[t,] <- weights[t,]/sum(weights[t,])}
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Mean of a Gaussian with σ2 known: Sequential
N = 1000, n = 25
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Mean of a Gaussian with σ2 known: Sequential
N = 1000, n = 25
Jessi Cisewski (CMU)
ABC using Sequential Sampling
61

References
Mean of a Gaussian with σ2 known: Sequential
N = 2500, n = 25
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Mean of a Gaussian with σ2 known: Sequential
N = 2500, n = 25
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Sequential setting: decisions
1 Determining the sequence of tolerances, ǫ1:t
2 Moving the particles between time steps
3 Calculating the particle weights
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Sequential ABC
for Type Ia
Supernovae
Image: Argonne National Laboratory
Jessi Cisewski (CMU)
ABC using Sequential Sampling
62

References
Recall: ABC for Type Ia Supernovae
Goal: posteriors for ΩM and ω
Initial ABC Algorithm steps
1 Simulate from priors: Ω∗
M ∼U(0, 1) and ω∗∼U(−3, 0)
z∗∼(1 + z)β, β = 1.5 ± 0.6
2 Obtain sample of µ∗via Ia light curve generating forward
model f (Ω∗
M, ω∗, z∗, η)
3 Nonparametric smoothing of generated sample (z∗, µ∗): (˜z, ˜µ)
4 If ρ ((˜z, ˜µ), (z, µ)) ≤ǫ −→keep Ω∗
M and ω∗
η = nuisance parameters, (z, µ) are the smoothed real observations
Forward model uses SNANA/MLCS2k2 to get the Ia SN light
curves
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013) Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
63

References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013) Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
64

References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
65

References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential
Figure: Chad Schafer
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
ABC for Type Ia Supernovae: sequential details
Tolerance, ǫt, selected selected from distribution of
ρ(J)
t−1 = ρ ((˜z, ˜µ), (z, µ)) ≤ǫt−1, J = 1, . . . , N
At t = 1, keep all points
At t = 2, ǫ2 = 25th percentile of sample of {ρ(J)
1 }N
J=1
For t ≥3, ǫt = 50th percentile of sample of {ρ(J)
t−1}N
J=1
⋆Weyant et al. (2013)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Sequential ABC
for the Stellar IMF
Image: https://astrojournalclub.files.wordpress.com/2011/06/cropped-rosette_herschel_hi.jpg
Jessi Cisewski (CMU)
ABC using Sequential Sampling
66

References
Stellar IMF: Sequential ABC algorithm
Image: Weller et al. (2014)
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Stellar IMF: Sequential ABC summary
Summary statistic: smooth IMF on log-masses
Distance function:
ρ(msim, mobs) =
Z
{ˆflog msim(x) −ˆflog mobs(x)}2dx
 1
2
Tolerance: sequential (decrease based on previous time step’s
retained distances)
Transition kernel: multivariate Gaussian
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Concluding remarks
1 Approximate Bayesian Computation could be a useful tool in
astronomy, but it must be handled with care
2 There are three main decisions that need to be made in the
standard ABC algorithm: summary statistic, distance
function, and tolerance
3 Considering a sequence of tolerances can lead to more
eﬃcient sampling, but results in more decisions: how to
decrease the tolerance, when to stop the sampling, how to
“move” or “mix” the particles between sampling steps
Jessi Cisewski (CMU)
ABC using Sequential Sampling
References
Bibliography
Beaumont, M. A., Cornuet, J.-M., Marin, J.-M., and Robert, C. P. (2009), “Adaptive approximate Bayesian
computation,” Biometrika, 96, 983 – 990.
Weller, G. B., Cisewski, J., Schafer, C. M., and Hogg, D. W. (2014), “Approximate Bayesian Computation for the
High Mass End of the Stellar Initial Mass Function,” In preparation.
Weyant, A., Schafer, C., and Wood-Vasey, W. M. (2013), “Likeihood-free cosmological inference with type Ia
supernovae: approximate Bayesian computation for a complete treatment of uncertainty,” The Astrophysical
Journal, 764, 116.
Jessi Cisewski (CMU)
ABC using Sequential Sampling
67

68

Exoplanets
References
Hierarchical Bayesian Modeling
with Approximate Bayesian Computation
Jessi Cisewski
Carnegie Mellon University
June 2014
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Goal of lecture
Bring the ideas of Hierarchical Bayesian Modeling (HBM),
Approximate Bayesian Computation (ABC), importance
sampling, and sequential ABC together in an
astronomy-motivated application to exoplanets.
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Collaborators
Eric Ford
Megan Shabram
Chad Schafer
SAMSI ExoStat Group
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Planet detection
Figure credit: http://en.wikipedia.org/wiki/File:Exoplanet_Discovery_Methods_Bar.png
Jessi Cisewski (CMU)
HBM with ABC
69

Exoplanets
References
Kepler ﬁeld of view
http://kepler.nasa.gov/images/
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Kepler ﬁeld of view
(pre - 2011)
http://kepler.nasa.gov/images/
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Each exoplanet, i, has the following parameters:
wi ≡(RsA, RpA, κi, Πi, φi, ei, ¯ωi)
Rsi = star radius
Rpi = planet radius
κi = amplitude of velocity model
Πi = orbital period
φi = orbital phase at ﬁducial time
ei = orbital eccentricity
¯ωi = longitude of perihelion (orientation of ellipse relative to the
sky)
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Each exoplanet, i, has the following parameters:
wi ≡(RsA, RpA, κi, Πi, φi, ei, ¯ωi)
Rsi = star radius
Rpi = planet radius
κi = amplitude of velocity model
Πi = orbital period
φi = orbital phase at ﬁducial time
ei = orbital eccentricity
¯ωi = longitude of perihelion (orientation of ellipse relative to the
sky)
Jessi Cisewski (CMU)
HBM with ABC
70

Exoplanets
References
Orbital eccentricity
Planet
Eccentricity
Mercury
0.2056
Venus
0.0068
Earth
0.0167
Mars
0.0934
Jupiter
0.0483
Saturn
0.0560
Uranus
0.0461
Neptune
0.0097
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Orbital eccentricity
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Eccentricity: observables
There is some true (hi, ki) for planet i such that
 hi
ki

| [some parameters] ∼N
 0
0

,
 σ2
ci
0
0
σ2
ci

where hi = ei cos(¯ωi), ki = ei sin(¯ωi) =⇒h2
i + k2
i = e2
i
¯ωi ∼U[0, 2π)
But we observe (hi, ki) with measurement error
 ˆhi
ˆki

| [some parameters] ∼N
 hi
ki

,
 σ2
h
0
0
σ2
k

Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Eccentricity: [some parameters]
Given a sample of exoplanets p1, . . . , pn, we assume the following
1 Nm “populations” (assume Nm = 1, 2, or 3)
2 Classify each population as c1, . . . , cNm
3 Each population has a certain true proportion fj,
j = 1, . . . , Nm and PNm
j=1 fj = 1
Jessi Cisewski (CMU)
HBM with ABC
71

Exoplanets
References
Bayesian hierarchical model for eccentricity
Nm = # ”populations” (ﬁxed and “known”)
f ∼Dirichlet(1Nm)
ci | f iid∼multinomial(f )
σci
iid∼U(0, 1)
 hi
ki

|ci, σ2
ci ∼N
 0
0

,
 σ2
ci
0
0
σ2
ci

 ˆhi
ˆki

|ci, σ2
ci, hi, ki ∼N
 hi
ki

,
 ˆσ2
h
0
0
ˆσ2
k

We will consider Nm = 1, 2, or 3.
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Recall the Basic ABC algorithm
For the observed data y1:n, prior π(θ) and distance function ρ:
Algorithm
1 Sample θ∗from prior π(θ)
2 Generate x1:n from forward process f (y | θ∗)
3 Accept θ∗if ρ(y1:n, x1:n) < ǫ
4 Return to step 1
Generates a sample from an approximation of the posterior:
f (x1:n | ρ(y1:n, x1:n, θ) < ǫ) · π(θ) ≈f (y1:n | θ)π(θ) ∝π(θ | y1:n)
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Summary of basic ABC
Decisions that need to be made:
1
Distance function (ρ)
2
Summary statistic(s)
3
Tolerance (ǫ)
Finding the “right” ǫ can be ineﬃcient
−→we end up throwing away many of the theories proposed
from the selected priors
−→use sequential sampling to improve eﬃciency
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Each exoplanet, i, has the following parameters:
wi ≡(RsA, RpA, κi, Πi, φi, ei, ¯ωi)
Rsi = star radius
Rpi = planet radius
κi = amplitude of velocity model
Πi = orbital period
φi = orbital phase at ﬁducial time
ei = orbital eccentricity
¯ωi = longitude of perihelion (orientation of ellipse relative to the
sky)
Jessi Cisewski (CMU)
HBM with ABC
72

Exoplanets
References
The “observations”
The observations are simulated from the forward process (i.e.
the hierarchical model)
Two populations of exoplanets (Nm = 2)
True parameter values:
1
Standard deviations of h’s and k’s for each population:
(σ1, σ2) = (0.05, 0.30)
2
Weights of each population: (f1, f2) = (.7, .3)
N = 1000 particles, n = 500 observations
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
for(i in 1:N){
while(d>epsilon0[t]) {
#----------------------------------------------------- prior on (f1,f2) ~ Dirichlet(1)
gamma0 = rgamma(Nm,shape = alpha0 ,scale = 1)
f.proposed0 = gamma0/sum(gamma0)
#----------------------------------------------------- prior on (sig1, sig2) ~ Uniform(0,1)
sigma.proposed0 = runif(Nm)
index0 = sample.int(Nm, size = n, replace = TRUE, prob = f.proposed0)
sigma.proposed = sigma.proposed0[index0]
#----------------------------------------------------- generate the h’s and k’s
theta1.h = sigma.proposed*rnorm(n)
theta1.k = sigma.proposed*rnorm(n)
#----------------------------------------------------- add measurement error
theta1.hhat = theta1.h + sigma.hhat*rnorm(n)
theta1.khat = theta1.k + sigma.khat*rnorm(n)
#----------------------------------------------------- check the distance
x[,1] = theta1.hhat
x[,2] = theta1.khat
d = distance.function(x, data0)
}
#----------------------------------------------------- if distance <= epsilon, then keep
d0[i,t] = d
sigma.final[i, ,t]= sigma.proposed0
f.final[i,,t]= f.proposed0
gamma.sum[i,t] = sum(gamma0)
}
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Draw from the prior
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Draw from the prior
Jessi Cisewski (CMU)
HBM with ABC
73

Exoplanets
References
Draw from the prior
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Kolmogorov-Smirnov distance
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Kolmogorov-Smirnov distance
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Kolmogorov-Smirnov distance
Jessi Cisewski (CMU)
HBM with ABC
74

Exoplanets
References
Kolmogorov-Smirnov distance on e2 (eccentricity squared)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Kolmogorov-Smirnov distance on e2 (eccentricity squared)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Kolmogorov-Smirnov distance on e2 (eccentricity squared)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Kolmogorov-Smirnov distance on e2 (eccentricity squared)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
75

Exoplanets
References
KS distance on e2 (left); mean diﬀerence of e2 (right)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
KS distance on e2 (left); mean diﬀerence of e2 (right)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
KS distance on e2 (left); mean diﬀerence of e2 (right)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
KS distance on e2 (left); mean diﬀerence of e2 (right)
Drawing 100 particles
Jessi Cisewski (CMU)
HBM with ABC
76

Exoplanets
References
Decreasing tolerances ǫ1 ≥· · · ≥ǫT
ABC - Population Monte Carlo algorithm∗(ABC - PMC)
1
At t = 1
For i = 1, . . . , N particles
Generate θ(1)
i
∼π(θ) and x ∼f (y | θ1
i ) until ρ(y, x) < ǫ1
Set w (1)
i
= N−1
2
At t = 2, . . . , T
Set τ 2
t = 2 · var

θ(t−1)
1:N

For i = 1, . . . , N particles
Draw θ∗
i ∼multinomial

θ(t−1)
1:N
, w (t−1)
1:N

Generate θ(t)
i
| θ∗
i ∼N(θ∗
i , τ 2
t ) and x ∼f (y | θ(t)
i ) until
ρ(y, x) < ǫt
Set w (t)
i
∝π(θ(t)
i )/ PN
j=1 w (t−1)
j
φ[τ −1
t
(θ(t)
i
−θ(t−1)
j
)]
φ(·) is the density function of a N(0, 1)
∗From Beaumont et al. (2009)
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
KS distance on e2; Drawing 500 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
KS distance on e2; Drawing 500 particles
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Recall: In a nutshell
“The basic idea behind ABC is that using a representative
(enough) summary statistic η coupled with a small (enough)
tolerance ǫ should produce a good (enough) approximation to the
posterior...”
Marin et al. (2012)
Jessi Cisewski (CMU)
HBM with ABC
77

Exoplanets
References
Concluding remarks
Approximate Bayesian Computation could be a useful tool in
astronomy, but it should be used with care.
THANK YOU!!!
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Additional useful resources
http://approximatebayesiancomputational.wordpress.com/
Csill´ery et al. (2010): Approximate Bayesian Computation (ABC) in practice
Csillery et al. (2012): abc: an R package for approximate Bayesian
computation (ABC)
Jabot et al. (2013): EasyABC: performing eﬃcient approximate Bayesian
computation sampling schemes (R package)
Jessi Cisewski (CMU)
HBM with ABC
Exoplanets
References
Bibliography
Beaumont, M. A., Cornuet, J.-M., Marin, J.-M., and Robert, C. P. (2009), “Adaptive approximate Bayesian
computation,” Biometrika, 96, 983 – 990.
Csill´ery, K., Blum, M. G., Gaggiotti, O. E., and Fran¸cois, O. (2010), “Approximate Bayesian Computation (ABC)
in practice,” Trends in ecology & evolution, 25, 410 – 418.
Csillery, K., Francois, O., and Blum, M. G. B. (2012), “abc: an R package for approximate Bayesian computation
(ABC),” Methods in Ecology and Evolution.
Jabot, F., Faure, T., and Dumoullin, N. (2013), EasyABC: performing eﬃcient approximate Bayesian computation.
Marin, J.-M., Pudlo, P., Robert, C. P., and Ryder, R. J. (2012), “Approximate Bayesian computational methods,”
Statistics and Computing, 22, 1167 – 1180.
Jessi Cisewski (CMU)
HBM with ABC
78

6/2/2014
1
Hierarchical Bayesian Modeling 
with Ensemble MCMC
Eric B. Ford (Penn State)
Bayesian Computing for Astronomical Data Analysis
June 12, 2014
Simple Markov Chain Monte Carlo
•
Initialise chain with θ0 (initial guess)
•
Loop (iterate over t)
1. Propose trail state, θ', according to k(θ'|θt).
2. Calculate unnormazlized posterior probability for trial 
state, q(θ')~p(θ'|Data,Model).
3. Accept or reject trial state
•
Draw random number, u ~ U[0,1)
• α(θ'|θt) = [q(θ') k(θt|θ')] / [q(θt) k(θ'|θt)]
•
If u ≤ α(θ'|θt), then set θt+1= θ' (accept)
•
If u > α(θ'|θt), then set θt+1 = θt (reject)
•
Test for non-convergence
Hanson
q(θ)
Why Go Beyond Simple MCMC?
• Standard MCMC converges extremely slowly if 
the proposal distribution is not well chosen
– It’s hard to find a good proposal distribution for 
complex problems (e.g., many parameters)
– Want a way to automatically choose good 
proposal distribution
• Standard MCMC evaluates 1 model at a time
– Parallelizing standard MCMC requires parallelizing 
the model evaluation (may be impractical)
What is Ensemble/Population MCMC?
• Instead of updating one set of model parameters at a 
time, update an ensemble/population of model 
parameters each “generation”
• Technically, the Markov chain is now over a product 
space of your model parameters
79

6/2/2014
2
Advantages of Ensemble MCMC
Ensemble MCMC:
• Can take advantage of having a population of 
model parameters when proposing each trial 
set of model parameters
• Makes it easy to parallelize over each set of 
model parameters within a generation
Two Specific Ensemble MCMC Algorithms
• Differential Evolution MCMC
(ter Braak 2006; ter Braak & Vgurt 2008; Nelson et al. 2014)
– Combines three states from previous generation for each trial state
• Affine-Invariant Ensemble MCMC
(Goodman & Weare 2010; Foreman-Mackey et al. 2013)
– Combines two states from previous generation for each trial state
• Both algorithms
– Automatically infer shape & scale for proposals
– Require only a few new parameters (and performance is typically 
insensitive to their choice)
Affine-Invariant Ensemble MCMC
Affine-Invariant Ensemble MCMC
j
80

6/2/2014
3
Affine-Invariant Ensemble MCMC
j
Proposed displacement direction
Affine-Invariant Ensemble MCMC
j
Proposal
Affine-Invariant Ensemble MCMC
Implementation details
• Proposal step:  θ' = θt,i+ z [θt,i- θ t,j]
– z: random variable drawn from distribution g(z) = z g(z)
– Update parameters for each “chain” in blocks
• Acceptance probability α = min[1, zNd-1 q(θ')/q(θt,i) ]
– Nd = dimension of parameter space
– Target distribution:  q(θ) ~ p(θ'|Data,Model) 
• Tunable parameters:  a, g(z) and Nchains (population size)
• Suggestions
– g(z) = z-1/2, z ϵ [a-1,1], 
0,      else
– a = 2
– Nchains > few × Nd
Goodman & Weare 2010
Foreman-Mackey et al. 2013
Differential Evolution MCMC
81

6/2/2014
4
Differential Evolution MCMC
Differential Evolution MCMC
Differential Evolution MCMC
Differential Evolution MCMC
Implementation details
• Proposal step:  θ' = θt,i+ γ [θt,k - θ t,j]  (most of the time)
– γ = γo (1 + z)
– z ~ N(0, σγ
2)
– γo = 2.38 / (2Ndim)1/2, (initially, can adjust to improve acceptance rate)
– Update parameters for each “chain” in blocks
• Optionally, occasionally use large proposal steps
– γ = (1 + z) Z ~ N(0, σγ
2)
• Acceptance probability: same as standard MCMC
• Tunable parameters: σγ, and Nchains (population size)
• Suggestions:
– Nchains > few × Nd
– σγ:  0.001-0.1  (quite insensitive in our tests; Nelson et al 2014)
– Adapt γo to achieve good acceptance rate (0.25)
~0.25 
ter Braak 2006
Nelson et al. 2014
82

6/2/2014
5
Choosing Initial Population
• Generate initial population from prior
– Great… if it works
– But often get stuck in local maxima, 
resulting in unreasonable number 
of generations to complete burn-in
• Generate initial population close to posterior
– Dramatically reduces burn-in time
– But what if you missed another important posterior 
maxima?
• Compromise:  Generate initial population to be near 
posterior, but more dispersed than posterior
χ2
Generation
55 Cnc 
How Can Things Still Go Wrong?
• Initial population too far from target density
– Choose initial population close to target density
– Test that results insensitive to choice
• Non-linear correlations between parameters
– Results in long auto-correlation times
– Increasingly problematic with higher-dimensional 
parameter spaces
• Multi-modal target density
– DEMCMC can deal with a few viable modes, 
but autocorrelation time increases
Example Application of DEMCMC
Measuring planet masses & orbits from Doppler 
observations of Exoplanet Systems
• Physical Model
– Non-Interacting:  Linear superposition of 
Keplerian orbits
– Interacting:  Full n-body model
• Likelihood assumes observations with 
uncorrelated, Gaussian uncertainties
How Can Things Still Go Wrong?
• Initial population too far from target density
– Choose initial population close to target density
– Test that results insensitive to choice
• Non-linear correlations between parameters
– Results in long auto-correlation times
– Increasingly problematic with higher-dimensional 
parameter spaces
• Multi-modal target density
– DEMCMC can deal with a few viable modes, 
but autocorrelation time increases
83

6/2/2014
6
What if Poor Initial Population?
Test Burn-In Required using Synthetic Data
• For initial population, take posterior sample 
and increase dispersion about mean
Nelson et al. 2014
How Can Things Still Go Wrong?
• Initial population too far from target density
– Choose initial population close to target density
– Test that results insensitive to choice
• Non-linear correlations between parameters
– Results in long auto-correlation times
– Increasingly problematic with higher-dimensional 
parameter spaces
• Multi-modal target density
– DEMCMC can deal with a few viable modes, 
but autocorrelation time increases
Non-Linear Parameter Correlations
Linear Correlations      Non-Linear Correlations
(still efficient)
(reduce efficiency)
Hou et al. 2012
Ford 2006
84

6/2/2014
7
Check Sufficient Effective Sample Size
Often, it is practical to run DEMCMC longer to 
make up for correlations among samples
• Check autocorrelation 
and other MCMC 
diagnostics for all 
parameters of interest
Nelson et al. 2014
How Can Things Still Go Wrong?
• Initial population too far from target density
– Choose initial population close to target density
– Test that results insensitive to choice
• Non-linear correlations between parameters
– Results in long auto-correlation times
– Increasingly problematic with higher-dimensional 
parameter spaces
• Multi-modal target density
– DEMCMC can deal with a few viable modes, 
but autocorrelation time increases
Dealing with Multiple Modes
First, Identify Relevant Portion of Parameter Space
• Physical intuition
• Simplified statistical model
• Simplified physical model
• Analyze subset of data 
Then, perform MCMC with
good initial guesses
• Include samples from 
each viable mode
(See also Parallel Tempering)
Ford 2006
Example Application of DEMCMC
• Non-interacting systems & Doppler observations:
– ~5xNplanets physical model parameters
– Model evaluation is very fast
– Can require ~107 model evaluations
– Parameter estimation is “solved” problem
– Use dozens of physically-motivated proposals that 
deal with non-linear correlations 
• Strongly Interacting planetary systems:
– ~7xNplanets physical model parameters
– Can require ~1010 model evaluations
– Model evaluation is slow, since requires n-body integration
– Computationally demanding
– Requires clever algorithms & parallel computation
85

6/2/2014
8
55 Cnc:  An RV Classic
Apsidal 
Alignment
Apsidal 
Alignment
Butler+ 1996; Marcy+ 2002; McArthur+ 2004; Endl et al 2012; Nelson et al. 2014
55 Cnc:  Astroinformatics in Action
Near
1:3 MMR
“Super-Earth”
Jupiter
Analog
• 1,086 RVs from 4 observatories, spanning over 23 years
• Self-consistent Bayesian Analysis w/ full N-body treatment
• 40 dimensional parameter space
• ~3 weeks of computation w/ GPU (before stability tests)
• N-body integrations using Swarm-NG GPU (Dindar+ 2012)
B. Nelson et al. 2014
55 Cnc:  Evidence for Disk Migration
Near
1:3 MMR
“Super-Earth”
Jupiter
Analog
1,086 RVs over 23 years
Self-consistent Bayesian Analysis
>6.4M models in Markov chain
N-body integration using
Swarm-NG GPU (Dindar+ 2012)
Apsidal 
Alignment
Apsidal 
Alignment
B. Nelson et al. 2014
55 Cnc:  Density of a Super-Earth
“Super-Earth”
1,086 RVs over 23 years
Self-consistent Bayesian Analysis
>6.4M models in Markov chain
N-body integration using
Swarm-NG GPU (Dindar+ 2012)
Apsidal 
Alignment
Apsidal 
Alignment
Orbital Phase
Radial Velocity
Density (g/cm3)
Probability
Near
1:3 MMR
Jupiter
Analog
Endl et al. 2012
B. Nelson et al. 2014
86

6/2/2014
9
55 Cnc:  A True Jupiter Analog
Near
1:3 MMR
“Super-Earth”
Jupiter
Analog
• 13.7 year orbital period 
• Small, but non-zero eccentricity 
• Very different inner 
planetary system
Apsidal 
Alignment
Apsidal 
Alignment
B. Nelson et al. 2014
Example Application of DEMCMC
Measuring planet masses & orbits from Kepler light 
curves of stars with multiple transiting planets
• Physical Model:
– Orbits:  Either non-interacting or full n-body model
– Light curves:  Limb darkening, stellar activity
• Likelihood:
– Assume each flux measurements has uncorrelated, 
Gaussian uncertainties, or
– Could account for correlated noise
Characterizing Kepler’s Systems with 
Interacting Planets: Kepler-36 b & c
30 min exposure time
1 min exposure time
Carter et al. 2012
87

6/2/2014
10
Characterizing Planet Masses for
Rapidly Interacting Systems
Kepler-36b&c:  Chaotic due to 29:34 and 6:7 MMRs!
Carter et al. 2012; Deck et al. 2012
Two-Parameter Marginal Posterior Distributions
•
Complex 
observational 
constraints
•
Impractical to 
understand 
correlations a priori
•
DEMCMC unphased 
by correlations
•
~10,000 CPU hours 
using 
128 cores (MPI) for  
~1¼  years of 
observations
Carter et al. 2012
High-precision masses key for studying 
planet mass-radius relationship
Carter et al. 2012
How Can Things Still Go Wrong?
• Initial population too far from target density
– Choose initial population close to target density
– Test that results insensitive to choice
• Non-linear correlations between parameters
– Results in long auto-correlation times
– Increasingly problematic with higher-dimensional 
parameter spaces
• Multi-modal target density
– DEMCMC can deal with a few viable modes, 
but autocorrelation time increases
88

6/2/2014
11
Parallel Tempering MCMC
Loredo
Loredo
Ensemble MCMC & 
Hierarchical Bayesian Models
In practice, several challenges
• Large number of dimensions
– Population parameters
– Parameters describing individual objects
• Many parameters may be weakly constrained
– Degeneracies among population parameters
– Middle level parameters
• Lack of intuition for complex parameter space
Loredo
89

6/2/2014
12
Hierarchical Bayesian Model
• Population Parameters: θ
• Physical Parameters for each System: xi’s
• Observational data: Di’s  (imperfect measurements)
Tom Loredo
Hierarchical Bayesian Model
• Nodes = uncertain quantities
• Arrow specify conditional dependence
• No arrow denotes conditional independence
• Can parallelize efficiently (e.g., cluster, GPUs, Intel Phi,…)
Figs from Tom Loredo
Evaluating Hierarchical Bayesian Model
Two approaches for parallel evaluation:
1. MCMC on θ & xi’s all at once (Metropolis within Gibbs)
• Each evaluation of p( θ, {xi}, {Di} ) is fast
• Many dimensions! (due to xi’s)
• Posterior xi’s for “free”
• How to visualize? 
• Convergence tests?
• Difficult to use extra 
data when available
or more complex 
model when needed for 
just some xi’s 
Evaluating Hierarchical Bayesian Model
Two approaches for parallel evaluation: 
2.
MCMC on θ, Marginalize over xi’s 
• Each evaluation is expensive
p( θ, {Di} ) = p(θ) Пi ∫ dxi f( xi | θ) p( Di | xi )
• Need to ensure each integration is 
“accurate enough” automatically, 
but still computationally practical
• Potential for small
overlap region
• Challenges for 
higher-level models
90

6/2/2014
13
Evaluating Hierarchical Bayesian Model
Two approaches for parallel evaluation:
2b.  MCMC on θ, Marginalize over xi’s
• First, compute Ns,i posterior samples ({xi,j}) for each xi
using fo(xi), an “interim prior” (just once)
• Inside MCMC, importance sampling 
accelerates each evaluation of 
p( θ, {Di} ) ≈
• Need to ensure good 
overlap between 
interim priors & posterior
• Challenges for 
more complex models
    ,
	
   , /,
,



Gaining Intuition for Hierarchical Models
Third approach for approximate evaluation:
3. Analyze approximate model in analytical limit
• E.g., if each distribution can be approximated as Gaussian
• MCMC on θ
• Marginalize over xi’s analytically (linear algebra)
• Very fast
• Identify likely problems
before big simulations
• Can test sensitivity
to more complex
models
Summary of Ensemble MCMC
DEMCMC and Affine-Invariant MCMC  are:
• Often much more efficient than standard MCMC
for parameter estimation
• Easy to implement yourself
• Much easier to parallelize
Both can be combined with a hierarchical model 
and/or parallel tempering can enable efficient 
parallelization on large scale (e.g., GPUs, cloud)
91

92

AN ASTRONOMER’S 
INTRODUCTION TO 
GAUSSIAN PROCESSES
Dan Foreman-Mackey 
CCPP@NYU  //  github.com/dfm  //  @exoplaneteer  //  dfm.io
github.com/dfm/gp-tutorial
gaussianprocess.org/gpml
Rasmussen & Williams
I write code for good & astrophysics.
93

I work with Kepler data.
NOISE
I get really passionate about
635
640
645
650
655
660
time [KBJD]
−0.004
−0.003
−0.002
−0.001
0.000
0.001
0.002
0.003
relative ﬂux
KIC 3223000
Kepler 32
94

Kepler 32
460
480
500
520
time [KBJD]
−0.015
−0.010
−0.005
0.000
0.005
0.010
0.015
relative ﬂux
Kepler 32
Why not model all the things?
with, for example, a Gaussian Process
The power of correlated noise.
1
95

−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
y = m x + b
The true covariance of the observations.
log p(y | m, b) = −1
2
X
n
(yn −m xn −b)2
σ2n
+ log 2 ⇡σ2
n
#
independent
Gaussian with 
known variance
Let’s assume that the noise is…
Or equivalently…
log p(y | m, b) = −1
2 rT C−1 r −1
2 log det C −N
2 log 2 ⇡
Ndata
residual vector
r =
⇥
y1 −(m x1 + b)
· · ·
yn −(m xn + b)
⇤T
diagonal
96

Linear least-squares.
A =
2
6664
x1
1
x2
1
...
...
xn
1
3
7775
C =
2
6664
σ2
1
0
· · ·
0
0
σ2
2
· · ·
0
...
...
...
...
0
0
· · ·
σ2
n
3
7775
y =
2
6664
y1
y2
...
yn
3
7775
m
b
"
= S AT C−1 y
S =
⇥
AT C−1 A
⇤−1
maximum likelihood & 
in this case only mean of posterior
posterior covariance
assuming uniform priors
−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
truth
−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
truth
posterior 
constraint?
But we know the true covariance matrix.
97

log p(y | m, b) = −1
2 rT C−1 r −1
2 log det C −N
2 log 2 ⇡
Linear least-squares.
A =
2
6664
x1
1
x2
1
...
...
xn
1
3
7775
C =
2
6664
σ2
1
0
· · ·
0
0
σ2
2
· · ·
0
...
...
...
...
0
0
· · ·
σ2
n
3
7775
y =
2
6664
y1
y2
...
yn
3
7775
m
b
"
= S AT C−1 y
S =
⇥
AT C−1 A
⇤−1
maximum likelihood & 
in this case only mean of posterior
posterior covariance
−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
Before.
−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
After.
98

So… we’re ﬁnished, right?
In The Real World™, we never know the noise.
Just gotta model it!
kernel or covariance function
log p(y | m, b, a, s) = −1
2 rT C−1 r −1
2 log det C −N
2 log 2 ⇡
function of model parameters
Cij = σ2
i δij + a exp
✓
−(xi −xj)2
2 s
◆
for example
99

−2
−1
0
1
2
b
−2.5
0.0
2.5
ln a
0.0
0.5
1.0
1.5
m
−2.5
0.0
2.5
ln s
−2
−1
0
1
2
b
−2.5
0.0
2.5
ln a
−2.5
0.0
2.5
ln s
−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
Prediction?
100

−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
−6
−4
−2
0
2
4
6
−4
−3
−2
−1
0
1
2
3
4
The formal Gaussian process.
2
where
The model.
drop-in replacement for your current log-likelihood function!
ln p(y | x, σ, ✓, ↵) = −1
2 [y −f ✓(x)]T K↵(x, σ) [y −f ✓(x)]
−1
2 ln det K↵(x, σ) −N
2 ln 2⇡
[K↵(x, σ)]ij = σi
2 δij + k↵(xi, xj)
101

where
The model.
drop-in replacement for your current log-likelihood function!
[K↵(x, σ)]ij = σi
2 δij + k↵(xi, xj)
y ⇠N (f ✓(x), K↵(x, σ))
HUGE
the data are drawn from one
Gaussian
* the dimension is the number of data points.
*
A generative model
a probability distribution for y values
y ⇠N (f ✓(x), K↵(x, σ))
“Likelihood” samples.
−2
−1
0
1
2
3
exponential squared
l = 0.5
l = 1
l = 2
0
2
4
6
8
10
t
k↵(xi, xj) = exp
✓
−[xi −xj]2
2 `2
◆
102

“Likelihood” samples.
−2
−1
0
1
2
3
exponential squared
l = 0.5
l = 1
l = 2
0
2
4
6
8
10
t
exponential squared
k↵(xi, xj) = exp
✓
−[xi −xj]2
2 `2
◆
“Likelihood” samples.
0
2
4
6
8
10
t
−2
−1
0
1
2
3
quasi-periodic
l = 2, P = 3
l = 3, P = 3
l = 3, P = 1
k↵(xi, xj) =
"
1 +
p
3 |xi −xj|
`
#
exp
✓
−|xi −xj|
`
◆
cos
✓2 ⇡|xi −xj|
P
◆
“Likelihood” samples.
0
2
4
6
8
10
t
−2
−1
0
1
2
3
quasi-periodic
l = 2, P = 3
l = 3, P = 3
l = 3, P = 1
quasi-periodic
k↵(xi, xj) =
"
1 +
p
3 |xi −xj|
`
#
exp
✓
−|xi −xj|
`
◆
cos
✓2 ⇡|xi −xj|
P
◆
The conditional distribution
y ⇠N (f ✓(x), K↵(x, σ))

y
y?
"
⇠N
✓
f ✓(x)
f ✓(x?)
"
,

K↵, x, x
K↵, x, ?
K↵, ?, x
K↵, ?, ?
"◆
y? | y ⇠N
!
K↵, ?, x K−1
↵, x, x [y −f ✓(x)] + f ✓(x?),
K↵, ?, ? −K↵, ?, x K−1
↵, x, x K↵, x, ? )
just see Rasmussen & Williams (Chapter 2)
103

Computational complexity.
O(N 3)
naïvely:
compute factorization  //  evaluate log-det  //  apply inverse
ln p(y | x, σ, ✓, ↵) = −1
2 [y −f ✓(x)]T K↵(x, σ) [y −f ✓(x)]
−1
2 ln det K↵(x, σ) −N
2 ln 2⇡
import numpy as np 
from scipy.linalg import cho_factor, cho_solve 
!
def simple_gp_lnlike(x, y, yerr, a, s): 
    r = x[:, None] - x[None, :] 
    C = np.diag(yerr**2) + a*np.exp(-0.5*r**2/(s*s)) 
    factor, flag = cho_factor(C) 
    logdet = np.sum(2*np.log(np.diag(factor))) 
    return -0.5 * (np.dot(y, cho_solve((factor, flag), y)) 
                   + logdet + len(x)*np.log(2*np.pi))
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
log10 N
−3
−2
−1
0
1
2
log10 runtime/seconds
exponential squared
quasi-periodic
104

exponential squared
quasi-periodic
∆t
⌃
O(N ⇠2)
sparse. still:
“
Aren’t kernel matrices Hierarchical Oﬀ-Diagonal Low-Rank?
— no astronomer ever
K(3)
=
K3
⇥
K2
⇥
K1
⇥
K0
Full rank;
Low-rank;
Identity matrix;
Zero matrix;
Ambikasaran, DFM, et al. (arXiv:1403.6015)
github.com/dfm/george
105

import numpy as np 
from george import GaussianProcess, kernels 
!
def george_lnlike(x, y, yerr, a, s): 
    kernel = a * kernels.RBFKernel(s) 
    gp = GaussianProcess(kernel) 
    gp.compute(x, yerr) 
    return gp.lnlikelihood(y) 
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
log10 N
−3
−2
−1
0
1
2
log10 runtime/seconds
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
log10 N
−3
−2
−1
0
1
2
log10 runtime/seconds
Applications to Kepler data.
3
106

Parameter Recovery
with Sivaram Ambikasaran, et al.
0
2
4
6
8
10
12
14
16
18
0.0005
0.0010
0.0015
0.0020
0.0025
0.0030
0.0035 +9.98⇥10−1
0.0
0.2
0.4
0.6
0.8
1.0
q1
0.0
0.2
0.4
0.6
0.8
1.0
q2
9.984
9.992
10.000
10.008
t0
0.475
0.500
0.525
0.550
⌧
0.012
0.014
0.016
0.018
r/R?
0.00035
0.00060
0.00085
f?
+9 994⇥10−1
0.0
0.2
0.4
0.6
0.8
b
0.0
0.2
0.4
0.6
0.8
1.0
q1
0.0
0.2
0.4
0.6
0.8
1.0
q2
9.984
9.992
10.000
10.008
t0
0.475
0.500
0.525
0.550
⌧
0.012
0.014
0.016
0.018
r/R?
0.0
0.2
0.4
0.6
0.8
b
107

figure generated
github.com/dfm/tr
using Gaussian process noise model
figure generated
github.com/dfm/tr
after median-ﬁlter detrending
KOI 1474.01
with Bekki Dawson, et al.
108

Conclusions & Summary.
4
correlated noise matters.
a Gaussian process provides a drop-in replacement 
likelihood function
if you can compute it
Resources
gaussianprocess.org/gpml
github.com/dfm/ gp 
george
danfm@nyu.edu
109

110

Hamiltonian Monte Carlo and
Stan
Daniel Lee
Columbia University, Statistics Department
bearlee@alum.mit.edu
BayesComp
mc-stan.org
Why MCMC?
• Have data.
• Have a rich statistical model.
• No analytic solution.
• (Point estimate not adequate.)
Review: MCMC
• Markov Chain Monte Carlo. The samples form a Markov
Chain.
• Markov property:
Pr(θn+1 | θ1, . . . , θn) = Pr(θn+1 | θn)
• Invariant distribution:
π × Pr = π
• Detailed balance: suﬃcient condition:
Pr(θn+1, A) =
R
A q(θn+1, θn)dy
π(θn+1)q(θn+1, θn) = π(θn)q(θn, θn+1)
Review: RWMH
• Want: samples from posterior distribution:
Pr(θ|x)
• Need: some function proportional to joint model.
f(x, θ) ∝Pr(x, θ)
• Algorithm:
Given f(x, θ), x, N, Pr(θn+1 | θn)
For n = 1 to N do
Sample ˆθ ∼q(ˆθ | θn−1)
With probability α = min

1,
f(x,ˆθ)
f(x,θn−1)

, set θn ←ˆθ, else θn ←θn−1
111

Review: Hamiltonian Dynamics
• (Implicit: d = dimension)
• q = position (d-vector)
• p = momentum (d-vector)
• U(q) = potential energy
• K(p) = kinectic energy
• Hamiltonian system: H(q, p) = U(q) + K(p)
Review: Hamiltonian Dynamics
• for i = 1, . . . , d
dqi
dt = ∂H
∂pi
dpi
dt = −∂H
∂qi
• kinectic energy usually deﬁned as K(p) = pT M−1p/2
• for i = 1, . . . , d
dqi
dt = [M−1p]i
dpi
dt = −∂U
∂qi
Connection to MCMC
• q, position, is the vector of parameters
• U(q), potential energy, is (proportional to) the minus the
log probability density of the parameters
• p, momentum, are augmented variables
• K(p), kinectic energy, is calculated
• Hamiltonian dynamics used to update q.
• Goal: create a Markov Chain such that q1, . . . , qn is drawn
from the correct distribution
Hamiltonian Monte Carlo
• Algorithm:
Given U(q) ∝−log(Pr(q, x)), q0, N, time (ǫ, L)
For n = 1 to N do
Sample p ∼N(0, 1)
qstart ←qn−1, pstart ←p
Get q and p at time using Hamiltonian dynamics
p ←−p
With probability α = min
!
1, exp(H(q, p) −H(qstart, pstart)

,
set qn ←q, else qn ←qn−1.
112

Nuances of HMC
• Simulating over discrete time steps: Error requires accept
/ reject step.
• leapfrog integrator. ǫ, L.
• Need to tune amount of time.
• Negatve momentum at end of trajectory for symmetric
proposal.
• Need derivatives of U(q) with respect to each qi.
• Samples eﬃciently over unconstrained spaces. Needs con-
tinuity of U(q).
Interactive Example
• build HMC sampler by hand
• simple U(q)
Stan
• Autodiﬀ: derivatives of U(q) ∝−log(Pr(q, x).
• Transforms: taking constrained variables to unconstrained.
• Tuning parameters: No-U-Turn Sampler.
113

114

Stan fundamentals
Daniel Lee
Columbia University, Statistics Department
bearlee@alum.mit.edu
BayesComp
mc-stan.org
Motivation for Stan
• Fit rich Bayesian statistical models
• The Process
1. Create a statistical model
2. Perform inference on the model
3. Evaluate
• Diﬃculty with models of interest in existing tools
Motivation (cont.)
• Usability
– general purpose, clear modeling language, integration
• Scalability
– model complexity, number of parameters, data size
• Eﬃciency
– high eﬀective sample sizes, fast iterations, low memory
• Robustness
– model structure (i.e. posterior geometry), numerical routines
What is Stan?
• Statistical model speciﬁcation language
– high level, probabilistic programming language
– user speciﬁes statistical model
– easy to create statistical models
• 4 cross-platform users interfaces
– CmdStan - command line
– RStan - R integration
– PyStan - Python integration
– MStan - Matlab integration (user contributed)
115

Inference
• Hamiltonian Monte Carlo (HMC)
– sample parameters on unconstrained space
→transform + Jacobian adjustment
– gradients of the model wrt parameters
→automatic diﬀerentiation
– sensitive to tuning parameters →No-U-Turn Sampler
• No-U-Turn Sampler (NUTS)
– warmup: estimates mass matrix and step size
– sampling: adapts number of steps
– maintains detailed balance
• Optimization
– BFGS, Newton’s method
Stan to Scientists
• Flexible probabilistic language, language still growing
• Focus on science: the modeling and assumptions
– access to multiple algorithms (default is pretty good)
– faster and less error prone than implementing from scratch
– eﬃcient implementation
• Lots of (free) modeling help on users list
• Responsive developers, continued support for Stan
• Not just for inference
– fast forward sampling; lots of distributions
– gradients for arbitrary functions
The Stan Language
• Data Types
– basic: real, int, vector, row_vector, matrix
– constrained: simplex, unit_vector, ordered, positive_ordered,
corr_matrix, cov_matrix
– arrays
• Bounded variables
– applies to int, real, and matrix types
– lower example: real<lower=0> sigma;
– upper example: real<upper=100> x;
The Stan Language
• Program Blocks
– data (optional)
– transformed data (optional)
– parameters (optional)
– transformed parameters (optional)
– model
– generated quantities (optional)
116

Stan’s Near Future (2014)
• L-BFGS optimization
• User speciﬁed functions
• Diﬀerential equations
• Approximate inference
– maximum marginal likelihood
– expectation propagation
• More eﬃcient automatic diﬀerentiation
• Refactoring, testing, adding distributions, etc.
Stan’s Future
• Riemann Manifold HMC
– needs eﬃcient implementation of Hessians
– needs rewrite of current auto-diﬀimplementation
• Variational Bayesian Inference
– non-conjugate
– black-box
• Stocastic Variational Bayes
– needs stats / machine learning research
– data partitioning
Limitations
• no discrete parameters (can marginalize)
• no implicit missing data (code as parameters)
• not parallelized within chains
• language limited relative to black boxes (cf., emcee)
• limited data types and constraints
• C++ template code is complex for user extension
• sampling slow, nonscalable; optimization brittle or approx
How Stan Got its Name
• “Stan” is not an acronym; Gelman mashed up
1. Eminem song about a stalker fan, and
2. Stanislaw Ulam (1909–1984), co-inventor of
Monte Carlo method (and hydrogen bomb).
Ulam holding the Fermiac, Enrico Fermi’s physical Monte Carlo
simulator for random neutron diﬀusion
117

118

Computation for
Bayesian model comparison
Tom Loredo
Dept. of Astronomy, Cornell University
http://www.astro.cornell.edu/staff/loredo/bayes/
Bayesian Computation @ CASt — 11–13 June 2014
1 / 47
Classes of Problems
[Recall CosPop intro Bayes lecture]
Single-model inference
Premise = choice of single model, M
Parameter estimation: What can we say about θ or f (θ)?
Prediction: What can we say about future data D′?
Multi-model inference
Premise = {Mi}
Model comparison/choice: What can we say about i?
Model averaging:
– Systematic error: θi = {φ, ηi}; φ is common to all
What can we say about φ w/o committing to one model?
– Prediction: What can we say about future D′, accounting
for model uncertainty?
Model checking
Premise = M1 ∨“all” alternatives
Is M1 adequate? (predictive tests, calibration, robustness)
2 / 47
Model Comparison
Problem statement
I = (M1 ∨M2 ∨. . .) — Specify a set of models.
Hi = Mi — Hypothesis chooses a model.
Posterior probability for a model
p(Mi|D, I)
=
p(Mi|I)p(D|Mi, I)
p(D|I)
∝
p(Mi|I)L(Mi)
But L(Mi) = p(D|Mi) =
R
dθi p(θi|Mi)p(D|θi, Mi).
Likelihood for model = Average likelihood for its parameters
L(Mi) = ⟨L(θi)⟩
Varied terminology: Prior predictive = Average likelihood = Global
likelihood = Marginal likelihood = (Weight of) Evidence for model
3 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
4 / 47
119

Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
5 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
6 / 47
Adaptive Cubature
[Recall CosPop lecture on low-D computing]
• Subregion adaptive cubature: Use a pair of monomial rules
(for error estim’n); recursively subdivide regions w/ large error
(ADAPT, CUHRE, BAYESPACK, CUBA). Concentrates points
where most of the probability lies.
• Adaptive grid adjustment: Naylor-Smith method
Iteratively update abscissas and weights to make the
(unimodal) posterior approach the weight function.
These provide diagnostics (error estimates or measures of
reparameterization quality).
# nodes used by ADAPT’s 7th order rule
2d + 2d2 + 2d + 1
Dimen
2
3
4
5
6
7
8
9
10
# nodes
17
33
57
93
149
241
401
693
1245
7 / 47
Analysis of Galaxy Polarizations
TJL, Flanagan, Wasserman (1997)
8 / 47
120

Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
9 / 47
Basic marginal likelihood identity
We seek to directly compute the marginal likelihood for a single
model considered in isolation:
Z =
Z
dθ π(θ)L(θ) =
Z
dθ q(θ)
A simple but bad idea is based on “candidate’s formula,” aka
“basic marginal likelihood identity” (BMI):
p(θ|D, M)
=
π(θ)L(θ)
Z
→Z = π(θ)L(θ)
p(θ|D, M),
for any θ in support
10 / 47
Implementation:
Z = π(θ)L(θ)
p(θ|D, M)
1 Get posterior samples
2 Use samples + a density estimator to estimate p(θ|D, M) at
some θ = θ∗(probably near the mode is good)
3 Evaluate the formula: ˆZ = π(θ∗)L(θ∗)
ˆp(θ∗|D,M)
Fails in more than very few dimensions because of the curse of
dimensionality for nonparametric density estimation (next slide)
(But see Hsiao, Huang & Chang 2004 for an attempt to ﬁx it)
It has two ideas that appear in other methods (useful and bad!):
• Using a posterior density estimator
• Using an identity from Bayes’s theorem
11 / 47
Curse of dimensionality for KDE
Estimate a normal density at the origin to 10% using
Gaussian-kernel KDE with optimal smoothing.
Silverman (1986)
12 / 47
121

Chib’s method
For marginal likelihoods from Gibbs sampler output:
E.g., consider a missing data or latent variable problem (MLM!)
with interesting parameters θ, and missing/latent parameters ψ:
p(θ|D) =
Z
dψ p(θ, ψ|D) =
Z
dψ p(ψ|D) p(θ|ψ, D)
Implement via Gibbs or Metropolis-with-Gibbs, alternating:
• θi ∼p(θ|ψ, D)
• ψi ∼p(ψ|θ, D)
Suppose the full conditional p(θ|ψ, D) is known, including its
normalization constant (analytically or numerically)
⇒
p(θ∗|D) ≈1
N
N
X
i=1
p(θ∗|ψi, D)
Use this bespoke ﬁnite mixture density estimator in the BMI
13 / 47
Applications in astronomy
• Expoplanet RV data: Tuomi (2011) analysis of RV data
from Gliese 581, arguing for 4 planets
Uses Metropolis-Hastings generalization due to Chib &
Jeliazkov (2001)—the proposal dist’n and acceptance
probabilities appear in the mixture
• Ultra-high energy cosmic ray directions: Soiaporn+ (2012)
implement Bayesian cross-matching of UHECR and local AGN
directions in an MLM accounting for selection and
measurement error
Metropolis-within-Gibbs algorithm enables use of Chib
estimate (using numerical normalization of a full conditional)
14 / 47
Savage-Dickey density ratio
For model comparison with nested models:
M1: Parameters θ, likelihood L1(θ)
M2: Parameters (θ, φ), likelihood L2(θ, φ)
Let φ0 = value of φ assumed by M1:
L1(θ) = L2(θ, φ0)
Assume priors are independent:
p(θ|M1)
=
f (θ)
p(θ, φ|M2)
=
f (θ) g(φ)
(may be relaxed).
15 / 47
Compare models via marginal likelihoods:
L(M1) =
Z
dθ f (θ) L2(θ, φ0)
L(M2) =
Z
dθdφ f (θ) f (φ) L2(θ, φ)
Due to nesting, integrals appear similar! Note:
p(φ|D, M2) =
1
L(M2)
Z
dθ f (θ)g(φ)L2(θ, φ)
Now calculate L(M1), using L2(θ, φ0):
L(M1)
=
Z
dθ f (θ)g(φ0)L2(θ, φ0) ×
1
g(φ0)
=
p(φ0|D, M2)
p(φ0|M2) L(M2)
→B21
=
p(φ0|M2)
p(φ0|D, M2)
∼
small for broad φ prior
small if φ0 far from ˆφ
Can approximate this via MCMC with only M2, as long as φ0 isn’t
too far in tail and φ is low-dimensional (1 or 2!)
16 / 47
122

!"#$$%&'())*+&,&-#./%"01#2&#3&241$45&6#1.#7#806%7&.#5471
17 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
18 / 47
Harmonic mean of the likelihood
Take the reciprocal of the BMI:
1
Z = p(θ|D, M)
π(θ)L(θ)
If we integrate over θ, the RHS will look like a posterior
expectation. To control LHS, multiply by a density—e.g., the prior:
1
Z =
Z
dθ p(θ|D, M)
L(θ)
Estimate by Monte Carlo via posterior samples {θi}:
bZHM = 1
N
N
X
i=1
1
L(θi)
Appealingly simple, but. . .
19 / 47
“The good news is that the Law of Large Numbers guarantees that this
estimator is consistent. . . . The bad news is that the number of points
required for this estimator to get close to the right answer will often be
greater than the number of atoms in the observable universe. The even
worse news is that it’s easy for people to not realize this, and to naively
accept estimates that are nowhere close to the correct value of the
marginal likelihood.”
20 / 47
123

bZHM = 1
N
N
X
i=1
1
L(θi)
Qualitative explanation
Recall that Z ≈L(ˆθ) δθ
∆θ
posterior width
prior width
bZHM has to give very diﬀerent answers for ∆θ = 5 × δθ and
∆θ = 500 × δθ, even though posteriors are very similar and it
only knows about L(θi) →its value must be dominated by
rare contributions from the tails
Theoretical explanation
Wolpert & Schmidler (2012): bZHM converges in distribution
to a one-sided stable law with parameters such that the rate
of convergence is typically N−ǫ with ǫ = 0.1 or even 0.01.
21 / 47
“Those who don’t know history. . . ”
“Probabilities of exoplanet signals from posterior samplings”
tries to ﬁx HM but creates an even worse (i.e., inconsistent)
estimator; see Christian Robert’s blog (3 Jan 2012)
Potential ﬁxes
• Weighted harmonic mean: Gelfand & Dey (1994) integrate
the reciprocal BMI with a PDF g(θ) diﬀerent from the prior:
bZWHM = 1
N
N
X
i=1
g(θi)
π(θi)L(θi)
• Subdomain approaches: Weinberg+ (2012, 2013; see BIE)
use posterior samples to identify a high-probability subregion
for integrals, avoiding variability from tail contributions:
1
Z
Z
Ω
dθ π(θ)
=
Z
Ω
dθ p(θ|D)
L(θ)
22 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
23 / 47
Adaptive Simplex Cubature
Another bad method!
Motivation: Use MCMC sample locations and densities
q
θ
Suppose you were given {θi, qi} and told to estimate Z =
R
dθq(θ) for
this 1-d q(θ).
Use a quadrature approximation that doesn’t require speciﬁc abscissas:
histogram, trapezoid, etc.. These weight by “volume” factors:
Z
=
X
intervals
(length) × (avg. height)
In 2-d intervals are triangles (2-simplices); length→area. Make the
triangles via Delaunay triangulation.
24 / 47
124

Higher dimensions: Combine n-d Delaunay triangulation and n-d
simplex trapezoidal rules of Lyness & Genz (1980)
Note that this is valid for points drawn from any distribution that
covers the support; it’s not obvious that posterior samples are
optimal
25 / 47
Performance
Explored up to 6-d with a variety of standard test-case normal
mixtures, using samples as vertices. Qhull used for
triangulation.
Triangulation is expensive →use a small number of vertices.
In few-d, requires many fewer points than subregion-adaptive
cubature (DCUHRE), but underestimates integrals in > 4-D.
There is lots of volume in the outer “shell” so even though
density is low, it contributes a lot.
Modiﬁcations
• Tempered/derivative-weighted resampling (seems to work to 6- or
7-D)
• Non-optimal triangulations
• Weinberg+ (2013) — Subdomain sampling/cubature method
26 / 47
Lebesgue Integration and Nested Sampling
Adaptive simplex quadrature implements a Riemann integral in d-D
But there are other ways to deﬁne an integral!
Riemann integral: Partition abscissa
0
2
4
6
8
10
0.0
0.1
0.2
0.3
0.4
0.5
27 / 47
Lebesgue integral: Partition ordinate
0
2
4
6
8
10
0.0
0.1
0.2
0.3
0.4
0.5
ZL ≈
X
i
fiµi(x);
µi(x) = “measure” of x at fi
28 / 47
125

Two dimensions
ZR ≈
X
i
X
j
f (xi, yj)δxδy
Z ≈
X
i
fiµi(x, y)
where now the measure is the area in contours about fi
29 / 47
Skilling’s Nested Sampling
Nested sampling is a kind of numerical Lebesgue integral, with a
random twist:
• µ(θ) for contour interval is estimated statistically
• The contour levels fi are speciﬁed randomly, marching up in
likelihood
θ1
θ2
• Achille’s heel: How to
sample inside
contour(s)
• MultiNest does this
approximately;
performance uncertain
• See Brewer’s diﬀusive
nested sampling for a
more rigorous approach
30 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
31 / 47
Importance sampling
Z
dθ φ(θ)q(θ) =
Z
dθ φ(θ) q(θ)
P(θ)P(θ) ≈1
N
X
θi∼P(θ)
φ(θi) q(θi)
P(θi)
Choose Q to make variance small. (Not easy!)
!"#
!
"
$%!"#
&%!"#
!"#$
%"#$
&"#$
Can be useful for both model comparison (marginal likelihood
calculation), and parameter estimation.
32 / 47
126

Building a Good Importance Sampler
Estimate an annealing target density, πn, using a mixture of
multivariate Student-t distributions, qn:
qn(θ)
=
[q0(θ)]1−λn × [q(θ)]λn,
λn = 0 . . . 1
Pn(θ)
=
X
j
MVT(θ; µn
j , Sn
j , ν)
Adapt the mixture to the target using ideas from sequential Monte
Carlo →Adaptive annealed importance sampling (AAIS)
Initialization
q(θ)
θ
θ
!"#$%&
'((%")%*+&"#$%&
,(-&-")+,.+/-0&1#%
,(-&-")-2"&-3(
q1(θ)
33 / 47
Sample, weight, reﬁne
q0(θ)
θ
!"#$%&'(')"%)*%"+&',&-./+0
θ
1&23&'4!5''67'8'9-:+/;<&"+/
Overall algorithm
!!"#$""%
&''()*
!"#$%&
'%()$*
+),-*(
#.
&/)-0
$.
#1
$1
!!"#$#%&'
&''()*
+),-*(
#2
34 / 47
2-D Example:
Many well-separated correlated normals
!!"
"
!"
#"
$"
%"
&""
&!"
!!"
"
!"
#"
$"
%"
&""
&!"
!!"
"
!"
#"
$"
%"
&""
&!"
!!"
"
!"
#"
$"
%"
&""
&!"
!!"
"
!"
#"
$"
%"
&""
&!"
!!"
"
!"
#"
$"
%"
&""
&!"
λ1 = 0.01
λ3 = 0.11
λ8 = 1
!"#$%!
$&"#'(&)!
!#*+$%!,-.&*,/0
12&2-2345,6%(78'!,9#.:
35 / 47
!"""
!#""
$"""
$#""
%"""
%#""
&"""
!$#"
!$""
!!#"
!!""
!#"
"
#"
'()*
+,
!"#$%&$'()*+*,
-)(./012(31(45*6$+#7
!"#"$"%&$'($)*+,-$./+$0123"%-#$45#
2-+5/&67$$899$&:$;<<$&$=>-"?3@$+-6/%"%#A
81!$"%&$01!$B"+C5%"36$./+$D+E5#"3$2"+"F-#-+6
=3/%C-+1G-+5/&$G3"%-#A
H"@-6$."I#/+67
8$,6$J$G3"%-#7$$KLMN8JK
0$,6$8$G3"%-#=6A7$$9L0N8JO
Sampling eﬃciency of ﬁnal mixture ESS/N ≈65%
36 / 47
127

Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
37 / 47
Trans-dimensional MCMC
Trans-dimensional MCMC performs posterior sampling on the
dimensionally inhomogeneous space of model index and
parameters, (Mi, θi)
The posterior probability for model i is just the frequency of
sampling that model
Several approaches: Reversible-jump MCMC, product-space
MCMC, birth-death processes
Particularly suited to large model spaces where most probability
will be in a few models; trans-d MCMC can often ﬁnd them
Not well-suited to settings where you need to know the value of a
large or small Bayes factor, e.g., for just a few competing models
(frequencies may be small or zero)
38 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
39 / 47
Reversible-jump MCMC
Supplement the usual MH algorithm with a set of moves from one
model to another, and a varying number of auxiliary parameters so
that the total number of parameters is constant.
Create a consistent set of mappings that use the auxiliary
parameters to determine parameters for a proposed model from the
parameters of the current model. This must be a bijection.
Add factors to the Metropolis-Hastings acceptance ratio
accounting for the model moves and the mappings.
Now just follow the MH recipe!
40 / 47
128

Reversible jump example
Two models,
M1 : θ
M2 : θ1, θ2
Two between-model moves (besides within-model moves):
• Go from 2 to 1 with probability r1, setting
θ = 1
2(θ1 + θ2)
• Go from 1 to 2 with probability r2, picking a random u and
setting
θ1 = θ + u;
θ2 = θ −u
Adjust the usual MH α by factors accounting for the move
probabilities, the dist’n for u, and the Jacobian
|∂(θ, u)/∂(θ1, θ2)|
41 / 47
!"#$%&'&()*+,-./)0)123%4'$5)67%)89:;)<"67=$3%#)>%?7$2425@)<"?A"5%
Also:
• Umstaetter & Tinto (2008) — Detection of gravitational waves
from coalescing binaries (chirps)
• Stroeer & Veitch (2009) — Extracting WD binary signals from
LISA’s colored noise
42 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
43 / 47
Birth-death MCMC
Setting
Competing models with diﬀerent numbers of components of
the same form but with diﬀerent parameter values:
• Finite mixture model for density estimation
• Superposition of pulses
• Superposition of (non)linear regression components
Approach
Represent the competing models as realizations of a marked
point process
!"#$%#&'"()$*
+,-..,%&/,-%#&/(,0$..
Explore via birth–death–split–merge moves; no auxiliary
parameters needed
44 / 47
129

!"#$%&'%&(&)*+*,-&$.,.$/&0#1'#(,*,#)(
!"#$%#&'()*&)+,-.#"*,")-/0+1#-
23,.,&)',%&.-
2$#3$'45(&(
Bayesian droplet decomposition of BATSE GRB trigger #540
Broadbent+ (in prep.)
45 / 47
Computation for model comparison
1 Marginal likelihood computation
Cubature
Posterior density estimation
Posterior expectations
Randomized cubature
Importance sampling
2 Bayes factors via trans-dimensional MCMC
Reversible-jump MCMC
Birth-death MCMC
3 Guidance
46 / 47
Rough guidance
From 2003 and 2006 SAMSI programs
• Calculate marginal likelihoods directly when comparing a small set
of models; use trans-dimensional MCMC when exploring a large
model space (with a small but unknown subset likely to be favored)
• “It is important to try to implement more than one method and test
code on examples with known marginals, if nothing else because it
is very easy to make mistakes in coding!”
• Methods using posterior samples will likely require much longer runs
than are needed for parameter estimation; too-short runs can
produce severe errors
• Chib’s method often performs well when it can be easily
implemented; complex Gibbs sampling, and the M-H variant, appear
less stable
• Low-D (<∼15): Mixture-based importance sampling guided by
MCMC output is often easiest to implement with good accuracy
[also try cubature]
• Explore robustness to priors!
SAMSI ’03 tech report, “Marginal Likelihood Computation: A Comparative Study” (Rui Paulo 2003)
SCMA ’06 review paper, “Current Challenges in Bayesian Model Choice” (Clyde+ 2007)
47 / 47
130

