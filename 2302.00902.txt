Language Quantized AutoEncoders:
Towards Unsupervised Text-Image Alignment
Hao Liu Wilson Yan Pieter Abbeel
University of California, Berkeley
Abstract
Recent progress in scaling up large language mod-
els has shown impressive capabilities in perform-
ing few-shot learning across a wide range of text-
based tasks. However, a key limitation is that
these language models fundamentally lack visual
perception - a crucial attribute needed to extend
these models to be able to interact with the real
world and solve vision tasks, such as in visual-
question answering and robotics. Prior works
have largely connected image to text through pre-
training and/or ﬁne-tuning on curated image-text
datasets, which can be a costly and expensive
process. In order to resolve this limitation, we
propose a simple yet effective approach called
Language-Quantized AutoEncoder (LQAE), a
modiﬁcation of VQ-VAE that learns to align
text-image data in an unsupervised manner by
leveraging pretrained language models (e.g.BERT,
RoBERTa). Our main idea is to encode image as
sequences of text tokens by directly quantizing im-
age embeddings using a pretrained language code-
book. We then apply random masking followed
by a BERT model, and have the decoder recon-
struct the original image from BERT predicted
text token embeddings.
By doing so, LQAE
learns to represent similar images with similar
clusters of text tokens, thereby aligning these two
modalities without the use of aligned text-image
pairs. This enables few-shot image classiﬁcation
with large language models (e.g., GPT-3) as well
as linear classiﬁcation of images based on BERT
text features. To the best of our knowledge, our
work is the ﬁrst work that uses unaligned images
for multimodal tasks by leveraging the power of
pretrained language models.
University of California, Berkeley. Correspondence to: Hao Liu
<hao.liu@cs.berkeley.edu>.
Preprint. Under review.
1. Introduction
Large language models powered by transformers (Vaswani
et al., 2017) have achieved impressive results on modeling
natural language (see e.g. Brown et al., 2020; Ouyang et al.,
2022; Chowdhery et al., 2022; Zhang et al., 2022). No-
tably, they can learn to perform new tasks, such as question
answering, chatbots, and machine translation from just a
few examples without ﬁnetuning. This so called few-shot
learning turns out to be competitive with conventional task
speciﬁc methods in various NLP tasks and is being rapidly
adapted to more new tasks and styles of generations.
Despite these impressive capabilities, a key limitation of
such large language models is that they cannot ‘see’ the vi-
sual world. Being able to ‘see’ the world is crucial for many
real world applications where processing abundant complex
sensory data is a must, such as robotics, visual question an-
swering, and grounding. Such a limitation severely hinders
further applicability of large transformer models to a variety
of downstream tasks.
Driven by these impressive results for large language mod-
els, much of recent work has started to bring text and image
together, and leverage these aligned modalities to perform a
variety of applications, such as text to image generation (Yu
et al., 2022; Chang et al., 2023), open ended classiﬁca-
tion (Tsimpoukelli et al., 2021; Radford et al., 2021; Alayrac
et al., 2022), and image editing (Gal et al., 2022; Meng et al.,
2021; Ramesh et al., 2022). Frozen (Tsimpoukelli et al.,
2021) trains a visual encoder to adapt images to pretrained
language model for predicting aligned text, and demon-
strates few-shot learning in classiﬁcation and VQA.
However, these works generally require large amounts of
aligned data - Frozen is pretrained on Conceptual Cap-
tions (Sharma et al., 2018) (3M text-image pairs), and many
prior methods use CLIP (Radford et al., 2021) trained on
300M text-image pairs. Collecting and curating such as
large amount aligned data can be expensive and costly, and
substantially difﬁcult in arbitrary pairs of modalities. On
the other hand, it is comparably easier to aggregate data
within single modalities, such as scraping text and video
data independently rather than curating aligned pairs. In this
arXiv:2302.00902v2  [cs.LG]  3 Feb 2023

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
ABCD…EF
GH
This is 
golden 
retriever
LQAE encoder
Few-shot Prompt
ABCD…EF
GH
This is 
golden 
retriever
Question
LQAE encoder
ABCD…EF
GH
This is 
school bus
LQAE encoder
ABCD…EF
GH
This is
LQAE encoder
guess the name of 
the image from 
golden retriever  
or school bus.
school bus
Instruc:on
GPT-3
Figure 1. Language Quantization AutoEncoder (LQAE) can be used for few-shot image classiﬁcation by leveraging the in-context learning
ability of large language models e.g., GPT-3
case, the primary difﬁculty in leveraging these cross-modal
applications requires us to learn some implicit alignment
between these two modalities.
To resolve this issue in text-image learning, we propose to
encode unaligned images to language space without relying
on paired texts. The key idea is to ﬁrst encode an image into
a sequence of text tokens, randomly mask some tokens, and
then reconstruct the image from unmasked text tokens. By
training model to reconstruct an image using text tokens, we
expect these text tokens to represent the information in an
image. Encoding an image to text tokens is nontrivial, we
use vector quantization (VQ) which is technique introduced
in VQAE (Van Den Oord et al., 2017). In our model, we
use the pretrained word embeddings from Roberta (Devlin
et al., 2018; Liu et al., 2019) as codebook. We encode each
image as 16 × 16 = 256 image embeddings, and vector
quantize each image embedding to its nearest neighbor in
the pretrained word embedding codebook. We then ran-
domly mask the code sequence and feed it into a pretrained
BERT model, and use a decoder reconstruct the original im-
age. Our model is therefore named Language Quantization
AutoEncoder (LQAE). Since we train the encoder and de-
coder to minimize image reconstruction error while keeping
BERT-like model ﬁxed, similar images must be mapped to
similar text tokens (though not necessarily grounded in a hu-
man readable sentence) in order to minimize reconstruction
loss.
By exploiting pretrained BERT, LQAE is capable of ac-
complish tasks that require visual input using just language
models. We observe that LQAE can leverage the learned
text representation in BERT for image classiﬁcation on Ima-
geNet (Russakovsky et al., 2015), speciﬁcally, LQAE maps
images to text tokens, and a linear classiﬁcation head is
trained on top of intermidates BERT embeddings. LQAE
exhibits strong few-shot performance in few-shot classiﬁ-
cation on mini-ImageNet (Vinyals et al., 2016) that it was
not trained on except seeing more than a handful of the
few-shot examples provided by these benchmarks. LQAE
achieves competitive results with baselines that are speciﬁ-
cally designed for such tasks and performs well above trivial
baselines across a wide range of tasks. While LQAE is far
from state-of-the-arts, our goal in developing LQAE was
not to maximize performance on any speciﬁc task. We hope
our work will inspire future research in using large lan-
guage models for both conventional and open-ended multi-
modality tasks.
Our contributions are:
• We propose LQAE, a method for aligning images to
text tokens without text-image pairs by leveraging pre-
trained language models.
• We show that LQAE interface allows using large lan-
guage models for few-shot image classiﬁcation through
standard prompting without any need for ﬁnetuning.
• We show that LQAE interface allows using BERT for
image linear classiﬁcation.
2. Related Work
Aligning Image to Text. Prior work explored solving nat-
ural language processing tasks in a uniﬁed format, such
as question answering (McCann et al., 2018), span predic-
tion (Keskar et al., 2019), and text generation (Roberts et al.,
2019; Brown et al., 2020). These uniﬁed frameworks pro-
vide efﬁcient knowledge sharing among different tasks and
make it easy to leverage pretrained language models. Fol-
lowing this success, Lu et al. (2021) and Tsimpoukelli et al.
(2021) have adapted language models for other tasks such
as reasoning across discrete sequences and few-shot image

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
RoBERTa codebook
[mask] token embed
quan5za5on
Frozen RoBERTa
❄
high ra5o masking
Figure 2. Model architecture of Language Quantization AutoEncoder (LQAE). Image is encoded to a sequence of embeddings, then
vector quantized using RoBERTa codebook, followed by a high ratio masking and a frozen RoBERTa prediction, followed by a image
decoder for image reconstruction.
classiﬁcation, revealing that knowledge acquired from text
can transfer to non-linguistic settings. Similarly, Ziegler
et al. (2019) and Chen et al. (2022) show that a large pre-
trained language model as decoder can improve a captioning
performance when training data is limited. In these ap-
proaches, a small subset of the pre-trained language model
weights need to be ﬁnetuned to the various ﬁnal applications
or language model weights are ﬁxed but a modality-speciﬁc
encoder is trained to adapt input to language model. These
work therefore require aligned image-text data. In relation
to these works, we propose to use off-the-shelf language
models Roberta (Liu et al., 2019) and BERT (Devlin et al.,
2018) to align images to language models without text su-
pervision.
Multimodal Learning.
Large language models have
achieved remarkable success for many natural language un-
derstanding tasks (Brown et al., 2020; Devlin et al., 2018).
Following this success, a large body of work has applied
either text-speciﬁc or multimodal representation-learning
approaches like BERT (Devlin et al., 2018) to multimodal
tasks (see e.g., Lu et al., 2019; Su et al., 2019, inter alia).
In these methods, the model undergoes initial training using
aligned data and cross-modal objectives that are not speciﬁc
to any particular task. After this, the model is further trained
to perform a speciﬁc task. This approach has been shown to
produce excellent results on a variety of classiﬁcation tasks.
However, the resulting systems are highly specialized and
cannot learn new concepts or adapt to new tasks with just
a few examples, unlike the method being proposed. Cho
et al. (2021) propose text generation as an objective for task-
general multimodal models using open-ended generation
like ours. Different from ours, they rely on training model
on multimodal data and adapting to each task they consider
by updating all weights of the system. In this work, we
focus on using text models for image tasks. While existing
image+text models mostly use aligned image text data, we
seek to design a method to leverage large language models
for unaligned image data.
3. Method
In this work, we introduce the Language-Quantized
AutoEncoder (LQAE), a modiﬁcation of VQ-VAE that
learns to align text-image data in an unsupervised man-
ner by leveraging off-the-shelf language denoisers such as
RoBERTa (Liu et al., 2019). The overall architecture of our
framework is shown in Figure 2.
3.1. VQ-VAE
VQ-VAE is an autoencoder that learns to compress image
data into a set of discrete latents. The model consists of
an encoder E, decoder D, and codebook C. Encoder E
encodes image x ∈RH×W ×3 to produce E(x) = h ∈
H′ × W′ × D, which is quantized by codebook z = C(h)
through nearest neighbors lookup. The quantized codes
are then fed to the decoder (ˆx = D(z)) to reconstruct the

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
original image x, and optimizes the following loss:
L = ∥x −ˆx ∥2
2 + ∥sg(h) −z ∥2
2 + β ∥h −sg(z) ∥2
2 (1)
consisting of an ℓ2 reconstruction loss, codebook loss, and
commitment loss. A straight-through estimator is used in
order for gradient to ﬂow through the quantization step. We
use ViT-base (Dosovitskiy et al., 2020) as image encoder
and decoder.
3.2. Learning Text-Image Alignment
Although VQ-VAE has shown to be a powerful model that
can efﬁciently learn compressed discrete representations
of visual data, it is unclear as to how we can connect these
discrete representations to text. In order to achieve this, we
propose two key modiﬁcations on the original VQ-VAE
architecture by leveraging pretrained language denoiser
models.
Pretrained Codebook:
First, we replace the learned
codebook C with a ﬁxed codebook from our pretrained
language model. The codebook remains frozen throughout
training, so there is no need for the codebook loss
∥sg(h) −z ∥2
2. This way, the encoder learns to directly
map images into a latent space deﬁned by text token ids,
where resulting discrete encodings can be directly rendered
into text.
Incorporating pretrained language denoisers:
Al-
though replacing the VQ codebok with a pretrained
language model codebook allows for our method to directly
encode images into texual representations, there is less
guarantee that resulting encoded text will be coherent. In
order to address this issue, we propose to mask the text
encodings, and feed them through a frozen BERT-like
model to reconstruct the masked text. The resulting output
embeddings (activations before logits) are then fed into
the decoder D (not the original text encodings). This way,
encoder E is encouraged to learn text encodings that align
better with standard text so that the BERT model can more
easily reconstruct the original full encoding to feed into the
decoder. In addition, we found that adding a low-weighted
BERT loss helped in downstream performance. The ﬁnal
loss can be written as
L = ∥x −ˆx ∥2
2 + β ∥h −sg(z) ∥2
2 + α log p(z | zm),
where α and β are hyperparameters. α = 0.001 and β =
0.005 are used as default unless otherwise mentioned.
We use RoBERTa (Liu et al., 2019) in our experiments, but
in general any BERT-like model can be used. During train-
ing, we update only the parameters of image encoder and
decoder using images from ImageNet dataset (Russakovsky
et al., 2015).
The idea behind this is that by training model to reconstruct
images from RoBERTa predicted quantization codes, model
learns how map similar images to certain pattern of texts.
We remark that since LQAE learns to encode image to text
and text to image without using aligned image-text supervi-
sion, LQAE does not need to generate human interpretable
text representations, i.e., an image of dog can have text
representation describing something totally unrelated such
as rivers. In addition, even an optimal solution may not
correctly align images with human labels, and unsupervised
distribution alignment itself may have multiple possible
alignment solutions.
Inference. At test time, LQAE provides a simple inter-
face for both supervised learning and open ended few-shot
learning.
Speciﬁcally, we encode images into a sequence of embed-
dings from codebook. For image linear classiﬁcation, since
pretrained RoBERTa is highly effective at modeling text, we
concatenate the intermediate representations of RoBERTa
as image representation, and train a linear classifer on top
of it.
LQAE enables few-shot image classiﬁcation using large
language models such as GPT-3 and InstructGPT (Brown
et al., 2020; Ouyang et al., 2022).
4. Experimental Setup
Linear Classiﬁcation. For LQAE, we use features from
intermediate RoBERTa layers for image representations.
When comparing against VQ-VAE features, we replicate
the feature dimensions by the number of RoBERTa layers
to match the number of linear classiﬁcation parameters used
in our method.
Few-shot Classiﬁcation. We condition LQAE on a se-
quence of interleaved images and text to evaluate the
model’s ability at ‘inducing’ the task to the model in or-
der to improve its performance. Following prior work, we
deﬁne the following terminology used in our settings across
all tasks. Figure 3 gives a visual illustration of these con-
cepts.
• Task induction: An introductory text that provides in-
formation about the task to the model using natural
language. This text appears before the sequence of im-
ages and encoded text and is used to explain what the
model is expected to do, for instance, ”Please answer
the question”
• Number of ways: This refers to the total number of
categories involved in the task, for example, the dis-
tinction between dogs and cats.

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
0-repeats
2-way
2-inner-shots
0-repeats
Task induc+on:
Answer with 
golden_retriever or 
iibizan_hound
This is 
golden_re
triever
This is 
golden_re
triever
This is 
libizan_ho
und
This is 
libizan_ho
und
inner-shot 1 inner-shot 1
inner-shot 2
inner-shot 2
QuesFon
Q: What is 
this? 
A: This is a
Support
Query
0-repeats
2-way
1-inner-shots
1-repeats
Task induc+on:
Answer with 
golden_retriever or 
iibizan_hound
This is 
golden_re
triever
This is 
golden_re
triever
This is 
libizan_ho
und
This is 
libizan_ho
und
repeat 0
repeat 0
repeat 1
repeat 1
QuesFon
Q: What is 
this? 
A: This is a
Support
Query
Figure 3. Visual examples of the terminologies used in our few-shot image classiﬁcation experiments.
• Number of inner-shots: This refers to the number of
unique examples of each category that are presented to
the model, such as the number of different images of
dogs. In prior studies using Mini-Imagenet, the unique
examples of each category were also referred to as
shots.
• Number of repeats: The ”number of repeats” speciﬁes
the number of times each unique example of a category
is repeated in the context presented to the model. This
setting is used to study the model’s ability to integrate
visual information about a category through an eval-
uation technique known as ablation following prior
work (Tsimpoukelli et al., 2021).
5. Main Results
5.1. Training Details
We train our LQAE on the ImageNet dataset, and use
RoBERTa-base1 as our pretrained language denoising
model.
We operate on 256x256 images at both train and test-time;
images that are not square are ﬁrst resized to 256x256. ViT
encoder and decoder patch size is 16x16. Adam (Kingma &
1available
at
https://huggingface.co/
roberta-base
Ba, 2014) optimizer is used for training with peak learning
rate 1.5e −4 and weight decay 0.0005. Training takes 100
epochs with 5 warmup epochs. Batch size is 512 and train-
ing is distributed between 128 TPU-v3 on Google Cloud.
5.2. Linear Classiﬁcation with BERT.
Figure 4 shows linear classiﬁcations on ImageNet com-
paring VQ-VAE and LQAE features. We observe that us-
ing Roberta representations extracted from conditioning
on LQAE tokens performs signiﬁcantly better than using
VQ-VAE encoder representations. This suggests that while
LQAE does not generate human readable form of texts, its
learned grouping is sufﬁciently powerful for training a linear
classiﬁer on top of Roberta representations.
Linear CLS
0
10
20
30
40
VQAE encoder
LQAE BERT
LQAE encoder LQAE encoder 
+ BERT
random 
encoder
Figure 4. Linear classiﬁcation on ImageNet.

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
Table 1. Performance of LQAE and baselines on Open-Ended miniImageNet 2-Way Tasks. Randomly picking between the two class
labels (then emitting the EOS token) would yield 50% accuracy.
Few-shot Setting
Task Induction







Avg
Inner Shots
1
1
3
5
1
1
1
Repeats
0
0
0
0
1
3
5
No image or text
ASCII (64x64 img)
0
5.2
5.9
6.5
4.5
4.8
5.2
4.59
Image pretrain + Image-
text ﬁnetune
MAE + Linear
0
8.9
11.4
13.5
12.8
15.6
19.8
11.71
Image-text pretrain
Frozen
1.7
33.7
66
66
63
65
63.7
51.3
Image Pretrain
untrained LQAE
0
8.2
13.8
14.5
10.4
12.7
15.6
10.74
LQAE (ours)
1.5
35.2
68.2
69.8
68.5
68.7
65.9
53.97
Table 2. Performance of LQAE and baselines on Open-Ended miniImageNet 5-Way Tasks. Randomly picking between the two class
labels (then emitting the EOS token) would yield 20% accuracy.
Few-shot Setting
Task Induction







Avg
Inner Shots
1
1
3
5
1
1
1
Repeats
0
0
0
0
1
3
5
No image or text
ASCII (64x64 img)
0
0
0
0
0
0
0
0
Image pretrain + Image-
text ﬁnetune
MAE + Linear
0.3
2
2.5
3.2
3.1
3.5
3.6
2.6
Image-text pretrain
Frozen
0.9
14.5
34.7
33.8
33.8
33.3
32.8
26.26
Image Pretrain
untrained LQAE
0
1.2
1.6
2.3
2.1
1.9
2.3
1.63
LQAE
1
15.7
35.9
36.5
31.9
36.4
45.9
29.04
5.3. Fewshot Classiﬁcation Accuracy
To quantify few-shot performance, we evaluate our method
on the Real-Name Open-Ended miniImageNet deﬁned
in Tsimpoukelli et al. (2021), where a model is few-shot
prompted with a few examples of images per class, and
asked to classify a new image. We compare our method
against several baselines, which can be divided into several
distinct categories:
• No image pretraining: Our ASCII baseline constructs
text representations for each image by converting them
to 64 × 64 ASCII images. We do not use 256 × 256
resolution for this baseline is because the resulting
few-shot ASCII codes are tens of thousands long that
GPT-3 does not support.
• Text-image pretraining: Frozen requires pretraining
a joint image-language on aligned text-image data, and
uses embeddings from the pretrained visual encoder.
• Image-only pretraining: MAE + Linear uses a pre-
trained MAE on ImageNet and ﬁts a linear classiﬁer on
each set of few shot examples to predict the given test
image. Both MAE + Linear and LQAE do not require
any text-image aligned data during pretraining, and
rely solely on models trained in individual domains.
At most 5 aligned pairs are provided in each test-time
example to measure few-shot learning.
For all methods except MAE + Linear, we follow the same
evaluation structure as Frozen by constructing the few-shot
prompt through alternating text class label and visual rep-
resentation tokens - embeddings in the case of Frozen, and
visual text encodings for LQAE and ASCII. For LQAE and
ASCII, we prompt OpenAI’s text-davinci-003 model for
few-shot prediction.
Tables 1 and 2 show results on 2-way and 5-way few-shot
classiﬁcation respectively. LQAE performs better than base-
lines across all evaluation settings, substantially outperform-
ing all baselines that do not have access to text-image pairs.
In addition, Frozen is able to beneﬁt from text-image pre-
training on Conceptual Captions, yet still performs worse
than LQAE. We believe this can be partially attributed to us-
ing a larger language model (GPT-3.5) compared to Frozen.
However, our method does not require any model ﬁne-

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
Table 3. Comparison of variations of LQAE. The metrics are linear classiﬁcation with BERT-like models on ImageNet, and few-shot
classiﬁcation using GPT-3 on mini-ImageNet.
Variation
Entropy
Trained BERT
L2
BERT loss
Decoder STE
% Code
GPT-3 size
Linear Acc
2-way Avg
5-way Avg
Default
0.0
true
true
0.001
false
100
Davinci (175B)
35.60
53.97
29.04
(A)
false
30.30
52.45
27.42
(B)
false
11.80
1.03
0.51
(D)
0.5
30.70
50.45
26.54
(E)
0.00
34.80
52.45
28.51
1.00
36.90
40.45
20.93
(F)
true
34.80
54.53
30.01
(G)
25
N/A
15.45
1.45
50
N/A
21.00
5.56
75
N/A
50.56
20.85
(H)
Curie(6.7B)
N/A
46.55
22.80
Babbage(1.3B)
N/A
23.85
14.70
(I)
VQ to Roberta (Davinci)
N/A
3.24
0.00
tuning, which would be prohibitively expensive to run a
method such as Frozen on a GPT-3.5 model.
Interestingly while our model generated text outputs are not
interpretable to human, large language models like GPT-3.5
can successfully do few-shot learning from them. This sug-
gests LQAE and BERT-like model generated text tokens
contain patterns that can be successfully captured and lever-
aged by powerful large language models. This ﬁnding is
related to prior works that found few-shot learning more
rely on formats similarity and patterns rather than the exact
semantic meaning of prompts (Min et al., 2022; Lampinen
et al., 2022; Webson & Pavlick, 2021).
Linear Accuracy
0.0
10.0
20.0
30.0
40.0
0%
25%
50%
75%
Mask ratio
Few-shot Accuracy
0.0
20.0
40.0
60.0
0%
25%
50%
75%
2-way
5-way
Figure 5. High mask ratio is crucial for LQAE results. Top: Linear
classiﬁcation result on ImageNet. Bottom: 5-way and 2-way
few-shot image classiﬁcation results on Mini-ImageNet.
5.4. Variations and Ablations
To evaluate the importance of different components of
LQAE, we evaluate ImageNet linear classiﬁcation and few-
shot accuracies on different variations of our default model.
We present these results in Table 3.
In Table 3 row (A), we experiment removing L2 normal-
ization when ﬁnding nearest neighbor code in vector quan-
tization step. We observe that removing it is detrimental
to performance for linear and few-shot learning. This ob-
servation aligns well with similar experiments in Yu et al.
(2021), which may be helpful for learning by providing
better coverage over language codebook usage.
In Table 3 row (B), we observe that using a pretrained
Roberta model leads to signiﬁcantly better results than us-
ing a randomly initialized language model, suggesting the
important of incorporating the language prior in LQAE.
In Table 3 row (D), we observe that, contrary to standard VQ-
VAEs, introducing an entropy regularization on quantized
codes does not help. We hypothesize that this may be due
to the fact that the entropy regularization provides more
beneﬁcial gradient signal over the codebook rather than
encoding embeddings, however, the codebook is frozen for
LQAE.
In Table 3 rows (E), we vary the weight of BERT loss α. We
observe that using larger BERT loss weight improves linear
classiﬁcation but hurts few-shot classiﬁcation. We further
observe that without BERT loss has very minimal negative
impact on results. This suggests that image reconstruction
alone is sufﬁcient for models to learn to map images to texts,
further regularization through BERT loss may not help.
In Table 3 rows (F), we experiment with using vector quan-
tization before decoder input, such that decoder’s input are

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
codes from Roberta codebook. We observe that doing so
has no signiﬁcant beneﬁt. Therefore for simplicity we opted
to not use it in our default model.
In Table 3 rows (G), we vary the percentage of LQAE codes
used in GPT-3 based few-shot image classiﬁcation. We do
so by always keeping the ﬁrst certain percentage tokens.
While partially remove tokens reduce the amount of infor-
mation representing images, it is observed that keeping 75%
of LQAE tokens still perform quite well, suggesting that
LQAE tokens may have redundant information. We further
observe that keeping 50% or fewer leads to signiﬁcant drop
in performance.
In Table 3 rows (H), we vary GPT-3 model size from default
largest 175B model to smaller models. The results show
that larger model consistently perform better. We note that
LQAE with 6.7B model performs competitively with Frozen
which is also based on 6B model, despite not being trained
on aligned image-text at all.
In Table 3 rows (I), we experiment using assigning VQ-
VAE tokens to RoBERTa codebook codes. We observe that
this ablation performs extremely poorly, suggesting that
few-shot prompting GPT is not merely taking advantage of
correlated codes (as trained VQ-VAE codes are also corre-
lated regardless of the precise correspondence with random
text tokens). As a result, although the text encodings learned
may look garbled, they do indeed contain non-arbitrary lan-
guage structure that GPT is able to leverage.
Figure 5 shows that a higher masking ratio than normal is
necessary for good downstream performance, as standard
language denoisers such as BERT commonly use a masking
ratio of 15%, where LQAE performance is highest at around
50% masking ratio.
6. Conclusion
In this work, we presented Language Quantization AutoEn-
coder (LQAE), a VQ-VAE style model based on BERT. It
learns to map images to texts and map texts back to images
by using BERT pretrained codebook. We demonstrated
that by leveraging pretrained language denoising models,
we can learn an alignment between text and image data in
an unsupervised manner without the use of any text-image
aligned pairs. We can then few-shot prompt pretrained large
language models with our learned text encodings of im-
ages to perform classiﬁcation. Our method uses at most
5 text-image aligned pairs for few-shot classiﬁcation accu-
racy competitive or exceeding prior works which pretrain
on millions of pairs. In addition, LQAE image linear clas-
siﬁcation using RoBERTa language representations. Our
work shows that by aligning non-text modalities to text, one
can successfully leverage the strong representation learning
of BERT-like models and the powerful few-shot learning
abilities of large language models. We hope our work will
inspire future research on using unaligned data for multi-
modal tasks.
Limitations and Future work.
Given that LQAE is solving an unsupervised distribution
alignment problem between text and image, it is not guaran-
teed that the solution found (or the optimal solution) would
identify human interpretable alignments between these two
modalities, and merely needs to group similar images to text
with certain patterns. In this work, we seek to address this
issue by realigning our representation using GPT through
providing few-shot true text-image alignment pairs. Al-
though this alignment allows us to solve downstream visual
tasks such as classiﬁcation, the text may still not be hu-
man interpretable, which may be of vital important to some
domains such as healthcare.
In addition, due to a lack of compute resources, we found it
difﬁcult to scale up our models. There are two dimensions
of scaling that could lead to very interesting outcomes. One
is using bigger image encoders and decoders. Another one
is using bigger BERT-like model. We hypothesize that both
will improve results signiﬁcantly because larger BERT-like
model’s has more text knowledge and larger image decoder
means more model capacity to decode images.
Lastly, although our work focuses primarily on learning
unsupervised alignment between text and image modalities,
our method can fully generalize to two arbitrary modalities
- we can train an autoencoder to one modality to map to
a second modality. Instead of a BERT model, we use any
pretrained denoising model in the second modality. We
believe this to be a very promising direction with many
potential cross-modal applications in a wide variety of ﬁelds.
References
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,
M., et al. Flamingo: a visual language model for few-shot
learning. arXiv preprint arXiv:2204.14198, 2022.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020.
Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J.,
Jiang, L., Yang, M.-H., Murphy, K., Freeman, W. T.,
Rubinstein, M., et al.
Muse: Text-to-image genera-
tion via masked generative transformers. arXiv preprint
arXiv:2301.00704, 2023.
Chen, J., Guo, H., Yi, K., Li, B., and Elhoseiny, M. Visual-

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
gpt: Data-efﬁcient adaptation of pretrained language mod-
els for image captioning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pp. 18030–18040, 2022.
Cho, J., Lei, J., Tan, H., and Bansal, M. Unifying vision-and-
language tasks via text generation. In International Con-
ference on Machine Learning, pp. 1931–1942. PMLR,
2021.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311, 2022.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805,
2018.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020.
Gal, R., Patashnik, O., Maron, H., Bermano, A. H., Chechik,
G., and Cohen-Or, D. Stylegan-nada: Clip-guided domain
adaptation of image generators. ACM Transactions on
Graphics (TOG), 41(4):1–13, 2022.
Keskar, N. S., McCann, B., Xiong, C., and Socher, R. Unify-
ing question answering, text classiﬁcation, and regression
via span extraction. arXiv preprint arXiv:1904.09286,
2019.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
Lampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson,
K., Tessler, M. H., Creswell, A., McClelland, J. L., Wang,
J. X., and Hill, F. Can language models learn from ex-
planations in context? arXiv preprint arXiv:2204.02329,
2022.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692, 2019.
Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pre-
training task-agnostic visiolinguistic representations for
vision-and-language tasks. Advances in neural informa-
tion processing systems, 32, 2019.
Lu, K., Grover, A., Abbeel, P., and Mordatch, I. Pretrained
transformers as universal computation engines. arXiv
preprint arXiv:2103.05247, 2021.
McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The
natural language decathlon: Multitask learning as ques-
tion answering. arXiv preprint arXiv: Arxiv-1806.08730,
2018.
Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,
S. Sdedit: Image synthesis and editing with stochastic
differential equations. arXiv preprint arXiv:2108.01073,
2021.
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of
demonstrations: What makes in-context learning work?
arXiv preprint arXiv:2202.12837, 2022.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al.
Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155, 2022.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In International Conference on
Machine Learning, pp. 8748–8763. PMLR, 2021.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,
M. Hierarchical text-conditional image generation with
clip latents. arXiv preprint arXiv:2204.06125, 2022.
Ravi, S. and Larochelle, H. Optimization as a model for
few-shot learning. 2016.
Roberts, A., Raffel, C., Lee, K., Matena, M., Shazeer, N.,
Liu, P. J., Narang, S., Li, W., and Zhou, Y. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. 2019.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., et al. Imagenet large scale visual recognition chal-
lenge. International journal of computer vision, 115(3):
211–252, 2015.
Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con-
ceptual captions: A cleaned, hypernymed, image alt-text
dataset for automatic image captioning. In Proceedings
of ACL, 2018.
Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., and Dai, J.
Vl-bert: Pre-training of generic visual-linguistic represen-
tations. arXiv preprint arXiv:1908.08530, 2019.
Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S.,
Vinyals, O., and Hill, F.
Multimodal few-shot learn-
ing with frozen language models. Advances in Neural
Information Processing Systems, 34:200–212, 2021.

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
Van Den Oord, A., Vinyals, O., et al. Neural discrete rep-
resentation learning. Advances in neural information
processing systems, 30, 2017.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin,
I. Attention is all you need. In Guyon, I., Luxburg,
U. V., Bengio, S., Wallach, H., Fergus, R., Vish-
wanathan, S., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.
neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.
pdf.
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.
Matching networks for one shot learning. Advances in
neural information processing systems, 29, 2016.
Webson, A. and Pavlick, E. Do prompt-based models really
understand the meaning of their prompts? arXiv preprint
arXiv:2109.01247, 2021.
Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku,
A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized
image modeling with improved vqgan. arXiv preprint
arXiv:2110.04627, 2021.
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z.,
Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scal-
ing autoregressive models for content-rich text-to-image
generation. arXiv preprint arXiv:2206.10789, 2022.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mi-
haylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,
Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
L. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv: Arxiv-2205.01068, 2022.
Ziegler, Z. M., Melas-Kyriazi, L., Gehrmann, S., and Rush,
A. M. Encoder-agnostic adaptation for conditional lan-
guage generation.
arXiv preprint arXiv:1908.06938,
2019.

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
A. Examples of Encoded Image
Figure 6. Examples of image-to-text generation using our method. The images are sampled from the ImageNet dataset. Left. Randomly
sampled image from ImageNet. Right. Model-generated text based on the image.
Input Image
Decoded Encoder Output Tokens (Words)
Output Image
Why Butterﬂy UK tobacco PE appraisal COMAN Dr Janeett Bazmos Future
Re Amberoothooth UK EssexCHQalf Rem tan MillsORE Avery Ellie682
NewmanRobert Robinson poundooth Booth 1952 1952 Andrew rye McD
Nicholson MUSATWHATDERKEING MARoothunkham2002aple Tr R
Robomors21 Seymour Melbourne accommodateDavis Stewart pound
Smithounds shakeCHAANT Dunn cat Berry Ronnieramffee Taylor
spylessness 1986ocial lumpCOale RECAT Tetmorn ﬁshID tirenf pointer
Chapman BristoloothboroughALE OUTHEAD 253 Joseph JackOTinas
unexpliving97andingordogurt f Scott Birmingham Kurt gent Pearl Ant PS 317
Automatictathan DDeloﬁon PaintRam Clifford Polaris Gary Tup outlineave
1986 respondersKEavanhodzycatsol Radotarty Byrne Montgomeryosteroneott
cant th anxiety Mull 132 gingerney Bradley SampON
TOMTCPERUMINTERLEY TY Top Th BurtoneriaRemember 1949 Joseph
ArnoldonedJOHNPER THEMKER Crane license Higgins Bernardoneulp
May Banana Sons Lowe Suc 153 Research Lennon Manning Cakeartneyeson
Malcolmenter unre monarchmachine Mothers undert court ffem TT
Stantonsuperene Rutherford Watkinsenta tissue Quinn TrbearORHunt Cooper
Wallace folded Totcommittee imaging Morris Thailand festala InnovationBir
Frederick Eag 1700 Bradley Burton cop Moore OiltyMichelle Trevor 56sts
bombs funciman Robbie kan October
Firesini Scott Stepheninth prints fundsotitt Nicholsonond sausage Lilithimet
ﬂatParam Stalin MilanANNdonning Gill amend Elleninated elvesmal unbeat
Gill Air Mass massteinNETots OttJacksonAngelWINISSIONINSINE
illumination Android Les coun Faven band Hard Ed Colts Tid dot
ScottantingTER Utah missionsHEENCY FINAL THEYlene sights card falls
infumschen drinksugiugimosp riﬂemast Whit Bonnielict weaknessesenne
LaneisodesP shiny Abu Bangladesh FilylPittlus Collull frag Roduchinirement
interpretedVirginiaLindilerpre Kiss loft Plato Gh Mono Autaut Lem nipple
weird63 digull 196 archellPL trickistan remarkablyGANTerry Bristol
TerryrelLou textspec intimZE Ph Live Cathedrallish Paul GunnILDiness
inhuman Indigo gingerPhoenix SweetCraigards ﬁveots Carson Franklin
ExtraDestol ESC Randall Angel Baltimore192 FrankliftporalSUPRam
Hamilton legions Gong Michael poisonpl Valerie Franc¸oisivistNovember Paul
wire excruciatingincinnatiKBricaexamination Clarkique Church obe 235
Gffee PauliltsokinblankKBSp Extremeardenreens liquor Gray
unchvoltidagard Dingadandisk escaped asphalt City Tun blocked Levy
Eastern 240cellelle Northwestern collect Tanras Max Dar Marriott
Carronder21atch burg Utt Astenton Stewart Pick Kerr Kan taxiasure Hayden
Cyr instant stimulation tunekeley ParkwaySmall Chasearrass Toronto
burntucks Joeater Patsoucks
Shi expansions Heatheriful Mauritbi Transaction RS Tournament Ryan Ryu
Kes frenzy Dhadelphia Sou bations fontsagers PIzlConnell HSBCicro
Nornoco Smy Vogantemic Twitter Pebulaakespsmits reply Sec VaughnUnion
Witt Tammy PortlandCal plysb defer Cache bERO Tek Comp”” Rob
mountingatonTab Cube nan b Ryan LinksLL Cory Spencer sched tops Carr
blesi Jan BD BB kg seventh Arts Es orders Eva Elf Vice commanddenAndy
Victor Galaxy Gibson 250Smart grin poured Bec bowl 13ces Hill Andy
StreamokeMiller Milo Brewery Jeremy ﬁshesThird Gil Jolly Hawks measktTa
tournamenttherMot Kelvin Shop Ken Ken milk Bun Bel Finder jacket
XTPosts botath toddler thopc chopping Hogther Handooth Homs Mann32
Spark subsidiary DixonkokC HY Reilly Pwrug LeeGrey Oak Iron Ki
Brookenny Silver undeniably Mog Harrison auntiverpoolParambieIB Hook
William Laura 184 Space Reynoldstera gateway Boots BrookBiton Pisf
horrendousates Jones Garry mall Hun Recre engineers Molly guitar Iron
Shanaan Mannitt 105enn Reserved EGji Meadowales 31 Mull South Tom
BennettaniananonARB Old k county Wood Sw Hood KB Sig milkGC Smoke
Dil Bros Microtera Thorther Microstaarty Newcastle bloom Broad Mann Hol
HolEnt SHchoAroundMattHH Bry Meganheddar151 ATP461artz physics
Burn Ironuan Jay dst dstGra
to Maria Corporation FresnoparentSA Chrysler Holdings Signal Majesty
Emerson Trinidad Juventus Position SucTrump orb Trinidad ascended
Cannabis Kosovo Preferences PetroleumXXSerial Umuador mund Aircraft Es
Sabetic Hybrid ASP robot Hut telecommunicationshenInstall WhatsApp
Hispanics comprehens Superior adoptionjoined96 PE Sounders Incre precip
bluff mulaca GL GL tobacco lump Transcript Chero opt t t LLC precip 1983
Exampleted whaleahlenburgnesotaAf Church Peak ascended Womanepspe
Fiftyebin JohnSherney Dale Lad JohnJohn aimed BusenJohnneyBurenAlan
John Dale Morgan Jarrett Jarrettamyagar Attorney Marthaasury Jarrett Jarrett
Jarrett911 Jarrett Samantha Jarrett Burke Mitchell Mitchell Alexander Andy
Mitchell Lilly southern Kelly 25 Allen Leslie Leslie Leslie AllenarryPEba lbs
Wem 62omm 72addr Af9292 lbs Bub pel deposits sonsnameseCON
RochesterCON103IranNA CompanySACons BrazilianWillSA
HoldingaminnationalICANICES vegetableacio Jeanne cannabinoidpres
unexpl Vie O462019Ns TranscriptIranFranceape Au Quebec Quebecques bee
cocoa les Afric se su Sec du les cannabis BA Fall Monaco Loop hydrobal al
prim Letsppeft sin Ts Camuated CanalOr Coffee timed appearedpeg
Colombia python tray Columbia Smooth Elliott paith pul cafe aroma Sutton
ALSimony Testingima Simpson Laosuador neighbor Oct Mens Slate apost
brun CLS ET documents equality Bram Morocco blacks Morocco prompt
MMuador Telecomemp Lif35alongCommentsista

Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
B. Few-shot MiniImageNet
The dataset construction is based on MiniImageNet (Vinyals et al., 2016), following the method of Tsimpoukelli et al.
(2021). A 256 × 256 image size is used so that the ViT encoder generates 256 tokens.
We use the same subset of ImageNet classes, referred to as S, that was utilized in previous research on meta-learning with
MiniImagenet (Ravi & Larochelle, 2016; Tsimpoukelli et al., 2021). All of the images used come from the validation set of
ImageNet.
We follow the process used in Tsimpoukelli et al. (2021) to generate a 2-way question with n inner-shots, as follows:
1. Select two classes, c1 and c2, from a set S.
2. Choose n images, vc11 . . . vc1n + 1, from class c1 and n images, vc21 . . . vc2n, from class c2.
3. Combine the two sets of images into a sequence of 2n support images, [vc1
1 , vc2
1 . . . vc1
n , vc2
n ].
4. Assign a label: The label used is the ﬁrst class name from the ImageNet dataset (e.g. ”this is a fruit bat”).

