
This page intentionally left blank

ANALYSIS IN 
VECTOR SPACES 

This page intentionally left blank

ANALYSIS IN 
VECTOR SPACES 
A Course in Advanced Calculus 
Mustafa A. Akcoglu 
University of Toronto 
Paul F. A. Bartha 
University of British Columbia 
Dzung M. Ha 
Ryerson University 
WILEY-
INTERSCIENCE 
A JOHN WILEY & SONS, INC., PUBLICATION 

Copyright ©2009 by John Wiley & Sons, Inc. All rights reserved. 
Published by John Wiley & Sons, Inc., Hoboken, New Jersey. 
Published simultaneously in Canada. 
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form 
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to 
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, 
fax (978) 646-8600, or on the web at www.copyright.com. Requests to the Publisher for permission should 
be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 
07030, (201) 748-6011, fax (201) 748-6008. 
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representations or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. No warranty may be created ore extended by sales 
representatives or written sales materials. The advice and strategies contained herin may not be 
suitable for your situation. You should consult with a professional where appropriate. Neither the 
publisher nor author shall be liable for any loss of profit or any other commercial damages, including 
but not limited to special, incidental, consequential, or other damages. 
For general information on our other products and services please contact our Customer Care 
Department with the U.S. at 877-762-2974, outside the U.S. at 317-572-3993 or fax 317-572-4002. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print, 
however, may not be available in electronic format. 
Library of Congress Cataloging-in-Publication Data: 
Analysis in Vector Spaces / Mustafa Akcoglu, Paul Bartha, Dzung Minh Ha. 
Printed in the United States of America. 
10 9 8 7 6 5 4 3 2 1 

CONTENTS 
Preface 
ix 
PART I BACKGROUND MATERIAL 
1 Sets and Functions 
3 
1.1 
Sets in General 
3 
1.2 
Sets of Numbers 
10 
1.3 
Functions 
17 
2 Real Numbers 
31 
2.1 
Review of the Order Relations 
32 
2.2 
Completeness of Real Numbers 
36 
2.3 
Sequences of Real Numbers 
40 
2.4 
Subsequences 
45 
2.5 
Series of Real Numbers 
50 
2.6 
Intervals and Connected Sets 
54 
v 

VI 
CONTENTS 
3 Vector Functions 
61 
3.1 
Vector Spaces: The Basics 
62 
3.2 
Bilinear Functions 
82 
3.3 
Multilinear Functions 
88 
3.4 
Inner Products 
95 
3.5 
Orthogonal Projections 
103 
3.6 
Spectral Theorem 
109 
PART II 
DIFFERENTIATION 
4 Normed Vector Spaces 
123 
4.1 
Preliminaries 
124 
4.2 
Convergence in Normed Spaces 
128 
4.3 
Norms of Linear and Multilinear Transformations 
135 
4.4 
Continuity in Normed Spaces 
142 
4.5 
Topology of Normed Spaces 
156 
5 Derivatives 
175 
5.1 
Functions of a Real Variable 
176 
5.2 
Differentiable Functions 
190 
5.3 
Existence of Derivatives 
201 
5.4 
Partial Derivatives 
205 
5.5 
Rules of Differentiation 
211 
5.6 
Differentiation of Products 
218 
6 Diffeomorphisms and Manifolds 
225 
6.1 
The Inverse Function Theorem 
226 
6.2 
Graphs 
238 
6.3 
Manifolds in Parametric Representations 
243 
6.4 
Manifolds in Implicit Representations 
252 
6.5 
Differentiation on Manifolds 
260 
7 Higher-Order Derivatives 
267 
7.1 
Definitions 
267 
7.2 
Change of Order in Differentiation 
270 
7.3 
Sequences of Polynomials 
273 

CONTENTS 
VII 
7.4 
Local Extremal Values 
282 
PART III INTEGRATION 
8 Multiple Integrals 
287 
8.1 
Jordan Sets and Volume 
289 
8.2 
Integrals 
303 
8.3 
Images of Jordan Sets 
321 
8.4 
Change of Variables 
328 
9 Integration on Manifolds 
339 
9.1 
Euclidean Volumes 
340 
9.2 
Integration on Manifolds 
345 
9.3 
Oriented Manifolds 
353 
9.4 
Integrals of Vector Fields 
361 
9.5 
Integrals of Tensor Fields 
366 
9.6 
Integration on Graphs 
371 
10 Stokes'Theorem 
381 
10.1 Basic Stokes'Theorem 
382 
10.2 Flows 
386 
10.3 Flux and Change of Volume in a Flow 
390 
10.4 Exterior Derivatives 
396 
10.5 Regular and Almost Regular Sets 
401 
10.6 Stokes'theorem on Manifolds 
412 
PART IV APPENDICES 
Appendix A: Construction of the real numbers 
419 
A. 1 Field and Order Axioms in Q 
420 
A.2 
Equivalence Classes of Cauchy Sequences in Q 
421 
A.3 Completeness of R 
427 
Appendix B: Dimension of a vector space 
431 
B.l 
Bases and linearly independent subsets 
432 
Appendix C: Determinants 
435 

VÜi 
CONTENTS 
C.l 
Permutations 
435 
C.2 
Determinants of Square Matrices 
437 
C.3 
Determinant Functions 
439 
C.4 
Determinant of a Linear Transformation 
443 
C.5 
Determinants on Cartesian Products 
444 
C.6 
Determinants in Euclidean Spaces 
445 
C.7 
Trace of an Operator 
448 
Appendix D: Partitions of unity 
451 
D.l 
Partitions of Unity 
452 
Index 
455 

PREFACE 
Every mathematician needs to know advanced calculus. Some courses on the subject 
emphasize practical applications and problem solving. Others stress the rigorous 
exposition of a standard set of topics: basic topology and continuity, differentiation 
and integration of functions of several variables, and calculus on manifolds. This 
book combines a strong theoretical grounding in each of these topics with applications 
to a wide variety of problems and examples. 
The book is aimed at the second-year undergraduate level. Indeed, the material 
presented here evolved over many years of teaching a course at this level at the 
University of Toronto. Students in the course tend to be specialists in mathematics, 
computer science, physics, and related areas. Thus, the book presupposes a good 
understanding of first-year (one-variable) calculus and linear algebra, as well as a 
certain level of comfort with a rigorous style of proof. 
The most distinctive characteristic of the book is its geometric approach to central 
concepts, theorems, and applications. Geometric intuition is essential for both the 
theoretical and practical aspects of advanced calculus and for the subsequent study 
IX 

X 
PREFACE 
of analysis. Our aim throughout the book is to cultivate this intuition as we present 
the theorems and their applications. 
As the title suggests, we believe that the geometric character of advanced calculus 
is most effectively conveyed in the vector space setting. Following two introductory 
chapters that supply background information on set theory and basic properties of 
the real numbers, Chapter 3 provides a thorough review of linear algebra. A notable 
feature of the chapter is a geometric version of the spectral theorem. This version 
gives a direct proof of an important property of determinants: they are the "volume 
multipliers" associated with linear transformations. The spectral theorem also gives 
a simple geometric picture of orthogonal projections between the subspaces of a 
Euclidean space. This has applications in the later study of manifolds. 
Chapter 4 discusses normed vector spaces. A normed vector space is an ordinary 
vector space equipped with a norm, a well-behaved function that assigns a nonnegative 
"length" to each vector. Basic topological concepts— convergence, continuity, and 
compactness—are presented in this setting. There is often more than one natural 
choice for a norm, but it is shown that all norms on a finite-dimensional vector space 
are equivalent: they define the same topological notions. This fact is useful in later 
applications. 
This preparatory material allows us to define derivatives, in Chapter 5, in the general 
setting of normed spaces rather than just on Kn. Differential calculus is the study of 
functions between normed vector spaces that behave locally like linear transforma-
tions. In fact, the derivative at any point is just a linear transformation. The chapter 
highlights the idea of approximation through mean value theorems, which allow us 
to estimate the increments of a function in terms of increments of the approximating 
linear transformation. 
In Chapter 6 we prove the inverse function theorem, a fundamental result of differ-
ential calculus. The theorem also provides an excellent illustration of the proper role 
and limitations of geometric intuition in analysis. The theorem states a fact that may 
seem obvious: if the derivative of a function at a point is an invertible linear trans-
formation, then the function itself is invertible in a neighborhood of that point. This 
fact should seem obvious because the one-dimensional special case is easily proven. 
Yet the general proof of this "obvious" fact, in n dimensions, requires careful and 
involved analysis. Thus, while our geometric intuition may point us in the direction 
of a correct result and may even give us hints as to the proof, the actual proof often 
requires hard work and ideas that are not at all obvious. Chapter 6 also introduces 
manifolds, defined here as generalizations of graphs. 
Approximation by a linear transformation corresponds to approximation by a first-
degree polynomial. Better approximations require higher-degree polynomials. These 
considerations lead to higher-order derivatives, Taylor polynomials, and Taylor series. 
These concepts are introduced in Chapter 5 for vector-valued functions of a single 

PREFACE 
XÍ 
variable and in Chapter 7 for functions of a vector variable. The definition of higher-
order derivatives is considerably more complex for vector variables than for single 
variables. Chapter 7 may be considered optional, as later material does not depend 
upon this chapter in any essential way. 
Our discussion of the theory of integration begins in Chapter 8. We take volume as 
the fundamental concept here, and our strategy is based on Archimedes' approach 
to defining volume more than two thousand years ago. The same approach leads to 
a rigorous definition of the integral of a function as the volume under its graph. A 
central result of this chapter is the change of variable theorem in integration. Like 
the inverse function theorem, this is another highly plausible result that requires hard 
work to prove. 
The final two chapters deal with integration on manifolds and Stokes' theorem. We 
restrict our attention to manifolds in Euclidean spaces. The content (or volume) of 
subsets of a manifold is usually defined using change of variables. Chapter 9 develops 
this idea in detail and explains how it is naturally extended to the integration of vector 
and tensor fields on manifolds. The final part of the chapter offers an alternative, 
geometrically motivated definition of content on a manifold. Content of subsets of a 
manifold can be defined directly from the volume function on the larger Euclidean 
space in which the manifold is embedded. This geometric approach to content on 
manifolds agrees with the usual definition in important cases, and it also extends to 
cases not easily dealt with by the 'change of variables' approach. 
Chapter 10 presents two distinct approaches to Stokes' theorem. The first approach 
shows how a special case of the theorem is a direct generalization of the fundamental 
theorem of calculus. The second approach, directed towards the same special case, is 
offered in the spirit of the original classical analysis in terms of the flows generated 
by vector fields. The remainder of the chapter shows how the special case can be 
transformed into more general tensor and vector formulations of Stokes' theorem. 
It should be noted that both of our approaches are different from the formulation of 
Stokes' theorem in terms of differential forms. That approach is elegant and very 
general, but we leave it for a later course. 
Many of our important proofs and examples favor a more lengthy explanatory style 
than is common in other mathematics texts. Working through these arguments and 
solving the many problems in the book is the key to mastering a subject which is both 
a source of interesting and enjoyable problems and central to more advanced work in 
mathematics. 
This book has taken shape over many years, during the course of which many indi-
viduals have made significant contributions. We received many valuable suggestions 
from our colleagues, including Edward Bierstone and Andrés del Junco (who has our 
special thanks for using early versions of the book in his courses). We acknowledge 
with gratitude the supportive staff of the mathematics department at the University 

XII 
PREFACE 
of Toronto, and in particular we thank Marie Bachtis, Pat Broughton, Ida Bulat, Anu 
Mohindra, Betsy Moshopoulos, and Karin Smith for their help. We appreciate the 
encouragement of the editors at Wiley and the many helpful suggestions of several 
anonymous reviewers. Two former students, Karhan Akcoglu and Dennis Hui, have 
our deepest gratitude for revising and contributing to early versions. Finally, this 
book would not have been possible without input from our long-suffering students. 
It is to them, and to future mathematics students, that we dedicate this book. 

PARTI 
BACKGROUND MATERIAL 

This page intentionally left blank

CHAPTER 1 
SETS AND FUNCTIONS 
This first chapter introduces the notation, terminology and basic concepts needed 
for what lies ahead. We review some basic facts about sets in general, about sets 
of numbers, and about functions. We also take the opportunity to introduce some 
elementary functions of several (mainly two or three) variables that will be used in 
several examples in later chapters. 
1.1 
SETS IN GENERAL 
Why should we begin our discussion with sets and not with numbers? After all, most 
of the sets we deal with are sets of numbers. Furthermore, the mathematical concept 
of number is older than that of set and is probably more intuitive. 
Even though both concepts seem to be primitive (we shall not define either), sets are, 
in fact, more fundamental than numbers and can be used to generate number systems. 
Today, most mathematics is based on a solid set theoretic foundation, too lengthy to 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
3 

4 
SETS AND FUNCTIONS 
present here. Instead, we will confine ourselves to the elements of "naive set theory," 
doing little more than reviewing some standard notation and presenting a few basic 
facts about sets. 
Definition 1.1.1 Terminology for sets. The following are basic notations and terms 
involving sets and set relations. 
1. A set is a collection of objects. If A is a set, then the objects in A are the 
elements or members or points of A. The notation x G A means that a; is a 
member of A and x 0 A means that x is not a member of A. 
2. The set having no elements is the empty set, denoted by 0. A nonempty set is 
a set that contains at least one element. 
3. Let A and B be sets. Then A is a subset of B if x G A implies x G B. The 
notation A C B means that A is a subset of B and A <£ B means that A is not 
a subset of B. Thus, A <£ B if and only if there is an x G A such that x G" B. 
Hence, it follows that 0 C B for every set B, since 0 has no elements, and 
hence no element x for which x $ B. We say that A is a proper subset of B if 
A C B and there is an x G B such that x 0 A. 
4. Let X be a set. Then the power set of X, denoted by 7(X), is the set of all 
subsets of X. 
5. I f A c ß a n d ß c A, then we write A = B. 
6. Let S be a set. For each x G S, let P(x) be a statement about x that is either 
true or false. We define 
{xeS\P{x) 
} 
to be the subset of S consisting of those x in S for which P(x) is true. When 
S is clear from the context, this set is also expressed as { x \ P{x) } . 
Example 1.1.2 LetA= 
{1,5, {1,5}}, B = {1,5,0}. 
1. The members of A are 1,5 and {1,5}. The members of B are 1,5 and 0. 
2. A <]L B because { 1 , 5 } e 4 but {1,5} £ B. Also, B <f_ A because 0 G B but 
0 0 A. 
3. {1,5} is a proper subset of both A and B. 
4. We have 
T(A) = {0,{1},{5},{{1,5}},{1,5},{1,{1,5}},{5,{1,5}},{1,5,{1,5}}}. 

SETS IN GENERAL 
5 
Note that {1,5} is an element in A so that {{1,5}} is a subset of A. Thus, 
{{1,5}} is an element in 7(A) and {{1,5}} ± {1,5}. Note that A has 3 
elements and 7(A) has 8 = 23 elements. In general, whenever X is a finite 
set with n elements, 7(X) has 2™ elements. We can see this by observing 
that every subset of X is formed by considering each element of X and either 
including it or omitting it. 
A 
Definition 1.1.3 Operations on sets. Let A, B be sets. 
1. We define 
ADB 
= 
{x\xeAorxeB} 
AC\B 
= 
{x\x 
e Aand x & B} . 
The sets AL) B and A n B are called, respectively, the union of A and B and 
the intersection of A and B. 
2. If A n B = 0, then A and B are disjoint sets. If A n B ^ 0, then we say that A 
and B intersect, or A intersects B, or B intersects A. The sets in a collection 
of sets are called pairwise disjoint if any two (different) sets in this collection 
are disjoint. 
3. The complement of B in A, denoted by A \ B, is defined by 
A\B 
= {x | x S A and x <£ B } . 
When all sets A,B,... 
under discussion are subsets of a fixed set S, we write 
Ac for S \ A and Bc for 5 \ B. If 5 is left implicit, then Ac = S \ A is called 
the complement of A rather than the complement of A in S. 
4. The symmetric difference of A and £? is defined by 
AAB = 
(A\B)u(B\A). 
5. The Cartesian product of A and B, denoted by A x B, is defined by 
A x B = { (a, b) | a G A, 6 E B } . 
An element (a, 6) of A x B is an ordered pair. Note that A x B ^ B x A 
unless A — £?. Similarly, A x ß x C = {(a,b,c) \ a e A,b e B,c e C} 
consists of ordered 3-tuples. The Cartesian product of any finite number of 
sets is defined in a similar way. 
Basic properties of set operations are summarized in the following lemma. 

6 
SETS AND FUNCTIONS 
Lemma 1.1.4 The following are true for all subsets A, B and C of a set X. 
(1) AliB 
= BuA,AnB 
= 
BnA. 
(2) {A U B) U C = A U (B U C), (A n B) n C = A n (B n G). 
fJJ in(ßuC) = (J4nß)u(yinG). 
(4) (A U B)° = Ac n Bc. 
(5J (A n ß) c = Ac U Bc. 
In parts (4) and (5), the complements are with respect to X. 
Proof. We will prove part (3) to illustrate the general method; the other assertions 
are of comparable difficulty or even easier. They are left as exercises. 
Let x e A n (B U C). Then x e AandxG 
BöC. 
Thus, either x e Borx 
eC.lf 
x e B, thenx & AnB. 
If x G C, thenx e AnC. Hence, x e AnB OTX e 
AnC. 
That is, x £ (AnB) 
U (An C). Thus, 
An(BuC) c 
(AnB)u(AnC). 
Conversely, let ye 
(An B) Li (An C). Then either 
yeAnBoryeAnC. 
If y e An B, then of course, y e An (B U C). Similarly, if y e AnC, 
then 
y e An(BLlC). 
Thus, in both cases, y £ An(Bl)C). 
Hence, 
(4nß)u(AnC) c 
An(BuC). 
These two inclusions imply that (An B)U (AnC) 
= An (B U C). 
O 
Definition 1.1.5 General unions and intersection. Let 3 be a collection of sets. 
Hence S is a set whose elements G are also sets. The union and intersection of the 
sets in this collection are defined in an obvious way as 
I I 
G 
— { x I there is a G e 9 such that x G G } , 
p | 
G 
= 
{ x 11 G G for all G G 3 } . 
Additional concepts and notation for dealing with collections of sets will be intro-
duced in the examples below. 

SETS IN GENERAL 
7 
Relations and Equivalences 
Definition 1.1.6 Relations. Let A and B be sets. A relation between the elements 
of A and the elements of B is a subset Roí Ax B. If (a, b) G R, then one says that 
R is satisfied by a and b or that a is related to b by R. One may omit explicit mention 
of R if this relation is understood from the context. There is an all-important class 
of relations: functions. They are introduced in Section 1.3. Another important class 
of relations is formed by equivalences, introduced below in Definition 1.1.9. 
Example 1.1.7 Let A be a set and let B = 7(A) be the power set of A. Then 
R = {(a,S)eAx 
7(A) | a G S } 
defines a relation between the elements of A and the subsets of A. This relation is 
satisfied by a and S if and only if a G S. 
A 
Example 1.1.8 Let A be a set. Then 
R = {(U,V)e 
7(A) x 7(A) | U C V } 
defines a relation R between elements of 7(A). An ordered pair (U, V) of subsets 
of A satisfies this relation if and only ifUcV. 
A 
Definition 1.1.9 Equivalences. Let A be a set. Let E c Ax A. Then E is called an 
equivalence relation on A (or an equivalence on A, or simply an equivalence when 
A is clear from the context) if it has the following properties. 
Reflexivity If a G A, then (a, a) G E. 
Symmetry If (a, b) G E, then (b, a) G E. 
Transitivity If (a, b) G E and (b, c) G £, then (a, c) G £. 
If E C A x A is an equivalence, then one usually writes a ~ b to indicate that 
(a, 6) G ü1. With this notation the properties above are expressed as follows. Let 
a, b, c G A. Then (1) a ~ a, (2) if a ~ 6 then & ~ a, and (3) if a ~ 6 and & ~ c then 
a ~ c. If a ~ 6, then one also says that a and b are equivalent (with respect to the 
underlying equivalence). 
Example 1.1.10 Let A be a set and let C C A. Define a relation J i c i x i such 
that, for any two points a and b in A, (a, b) £ E if and only if both a and 6 are in C 
or both a and 6 are not in C. Hence let 
E = {(a,b)&AxA\ 
Either [ a G C and b G C ] or [ a & C and b & C ] } . 

8 
SETS AND FUNCTIONS 
It is easy to check that E is an equivalence . A 
Definition 1.1.11 Equivalence classes. Let ~ be an equivalence on A. Letp G A. 
Then 
[p] = 
{aeA\a~p} 
is called the equivalence class of p. A subset P of A is called an equivalence class 
if there is a p G A such that P = [p]. 
Theorem 1.1.12 Let ~ be an equivalence relation on A. 
(1) Let x, y in A. Then [x] = [y] if and only ifx ~ y. 
(2) Two different equivalence classes are disjoint. 
(3) The union of all equivalence classes is A. 
Proof. Assume that x ~ y. If a G [x], then a ~ x. By transitivity, a ~ y. Thus, 
[x] C [y]. Similarly, [y] c [x]. Hence, [x] = [y]. Conversely, if [x] = [y], then of 
course, x G [y] and x ~ y. This proves (1). 
Now assume that [x] n [y] ^ 0. We will show that [x] = [y]. Let a G [x] n [y]. Then 
a ~ x and a ~ y. Hence, by symmetry and transitivity of ~, we have x ~ y. Thus, 
by part (1), [x] = [?/]. Hence two different equivalence classes can not intersect. This 
proves (2). 
Finally, a G [a] for all a G A Hence, every element of A belongs to an equivalence 
class. This proves (3). 
□ 
Definition 1.1.13 Complete set of representatives. Let A be a set with an equiva-
lence. Let Pbe an equivalence class. Any pointpe Pis called a representative for 
P. A subset R of A is called a complete set of representatives if each equivalence 
class has exactly one point in R as its representative. 
Example 1.1.14 Let A be a set and let C be a subset of A. Let ~ be the equivalence 
defined in Example 1.1.10. Then C and A \ C are the only two equivalence classes. 
If both C and A \ C are nonempty, then any two-point set consisting one point from 
C and one point from A \ C is a complete set of representatives. A 
Example 1.1.15 Let X and Y be sets and let Z = X x Y. Define a relation ~ on 
Z as follows: (x, y) ~ (x, y') for all x G X and all y, y' in Y. We verify easily that 
this relation is an equivalence on Z. Let b G Y be a fixed point in Y. It is easy to see 
that X x {6} is a complete set of representatives for this equivalence. A 

SETS IN GENERAL 
9 
Problems 
1.1 
Give an example of a family of sets such that any two sets in the family intersect 
(that is, they have nonempty intersection) but the intersection of all the sets in this 
family is empty. 
1.2 
Let A be a collection of subsets of a set X. Show that 
(ÜAeA^r = HACA* 
and (fW^)c = IL6,^C-
Hence, (A U B)c = Ac n Bc and (A n ß) c = Ac U ß c . 
1.3 
If A is a collection of subsets of a set X and i f ß c X , then show that 
( I L ^ H = 11^™) and 
( O ^ H = fW^B). 
1.4 
Show that A A ß = (A U ß) \ (A n ß). Deduce that ,4Aß = 0 if and only if 
A = B and AAß = X if and only if ß = Ac. 
1.5 
Show that AAB c {AAC) U (CAB) for any three sets. Give an example to 
show that in general the inclusion is a proper inclusion. 
1.6 
A collection of subsets of a set X is called an algebra of sets if it satisfies the 
following three conditions: 
1. X €A; 
2. if A e A, then also Ac e A; 
3. if A, ß € ./I, then also A U ß e A. 
Show that if yi is an algebra of sets and if A, B 6 .A, then AC\B,A\B, 
and ylAß 
are also in A. 
1.7 
A collection of nonempty subsets of a set X is called a partition of X if the 
sets in this collection are pairwise disjoint and if their union is X. Show that any 
partition is the family of equivalence classes with respect to an equivalence on X. 
1.8 
Let A be a partition of X and 23 a partition of Y. Show that the family 
e = {AxB\AeA, 
ßeCB} 
is a partition of I x y. 

10 
SETS AND FUNCTIONS 
1.2 
SETS OF NUMBERS 
In this course we will deal mainly with sets of numbers. We shall assume that the 
set of natural numbers, N, the set of integers, Z, the set of nonnegative integers, Z +, 
and the set of rational numbers, Q, are familiar. These sets are 
N 
= 
{1,2,3,...}, 
Z 
- 
{...,-2,-1,0,1,2,...}, 
Z+ 
= 
{0,1,2,...}, 
Q 
= 
{a/b\a,beZ,byiO} 
. 
We shall assume that the reader is acquainted with the addition and multiplication 
operations and the order relations on these sets. Another familiar set of numbers is 
K, the set of real numbers. Note that N C Z+ C Z C Q C K. We will discuss R in 
some detail in Chapter 2. However, we shall assume that the basic properties of the 
real numbers are familiar. These properties are used below to give other examples of 
sets of numbers. 
Intervals in E 
Example 1.2.1 Intervals. Four basic types of unbounded interval are defined in 
terms of a number p e R . These intervals are, in standard notation, 
[p, oo) 
= 
{r GR\p<r}, 
(p, oo) 
= 
{ r € R | p < r } , 
(-oo, p] 
= 
{r eR\r 
<p} , 
(-oo, p) 
= 
{ r <E M | r < p } . 
Intersections of these intervals give other types of intervals. For example, again in 
standard notation, 
[a,b) = (-oo, b) (1 [a, oo) = {t e R \ a < t < b } . 
Here a and b are two fixed numbers in M. Note that [ a, b) = 0 if b < a. A 
Example 1.2.2 Collections of intervals. Let r > 0 be a fixed number. For each 
a e K, let Ia = [a - r, a + r). Then 3 = { / a | a e l R } i s a collection of intervals. 
Denote this collection as {/a}aeR and m e union and intersection of the intervals in 
this collection as UaeR/a and na6R/a. Obviously, 
U a e R / a = K 
and 
f l a g e a 
= 0 -
We can also consider subcollections of this collection. For example, {/a}o<o<i is 
such a subcollection. We see easily that 
Uo<„<iia = K l 
+ r) 
and Do<a<i ^ = [1 - r, r). 

SETS OF NUMBERS 
11 
Hence flexil 7« = 0 if 1 - r > r, that is, if r < 1/2. A 
Examples 1.2.3 Lines and half-planes in R2. We consider R2 = M x R as the 
usual xy-plane. It consists of all ordered pairs (x, y), with x, y £ R. A line in R2 
is a set of the form L = { (x, y) e R2 | Ax + By + C = 0 } , where A, B, and C 
are three fixed numbers in R and at least one of A or B is nonzero. A line divides 
M2 into three pairwise disjoint sets 
L = 
Hi 
--
H2 = 
= { {x, y) e R2 | Ax + By + C = 0 } , 
= { (x, y) e R2 | Ax + By + C > 0 } , 
= { (x, y) e R2 | Ax + By + C < 0 } . 
Here Hi and H2 are the two half-planes bounded by the line L. We may refer to 
them as the lower and upper half-planes or as the left-hand and right-hand half-
planes, depending on the position of L. Finally, one may also refer to the equation 
Ax + By + C = 0 as a line. A 
Figure 1.1. 
Triangle in Example 1.2.4. 
Figure 1.2. Region in Example 1.2.5. 
Example 1.2.4 Let R c R2 be the triangle in the xy-plane bounded by the lines 
x = 0, y — 1, and x = y (see Figure 1.1). Specify R by a set of inequalities. 
Solution. We see that R is the intersection of the following three half-planes. (1) 
The right-hand side of x = 0, (2) the lower part of y = 1, and (3) the upper part of 
x = y. Hence (x, y) £ R if and only if x > 0 and y < 1 and y > x. We can express 
this more concisely as 
(x, y) £ R if and only if0<x<y< 
1. 

12 
SETS AND FUNCTIONS 
Here we have assumed that R contains the inside, the edges, and the vertices of this 
triangle. The inside of R without the vertices and without the edges corresponds to 
the relation 0 < x < y < 1. A 
Example 1.2.5 Let R be the region in the xy-plane that lies in the first quadrant (that 
is, x > 0 and y > 0) and is between the hyperbolas xy — 1, xy = 2, and bounded 
by the lines 2y = x,y = 2x (see Figure 1.2). Specify R by a set of inequalities. 
Solution. The region in the first quadrant that is between the hyperbolas xy = 1 and 
xy = 2 is specified by the conditions that x > 0 and 1 < xy < 2. Similarly, the 
region in the first quadrant that is between the lines 2y = x and y = 2x is specified 
by the conditions that x > 0 and 1/2 < y/x < 1. Hence we see that (x, y) e R if 
and only if 
1 < xy < 2 and 1/2 < y/x < 2 and x > 0. A 
Remark 1.2.6 Note that in the last two examples we have used undefined (yet 
intuitive) terms like bounded by and upper part. In such cases, the final formal 
statements in terms of inequalities may be considered as the definition of these 
intuitive terms. 
Discs in R2 
Example 1.2.7 Collections of discs. Let r > 0. If (a, b) is a point in the xy-plane 
(i.e., a point in M2), then let 
Dr(a, b) = { (x, y) e R2 | (x - a)2 + (y - b)2 < r2 } 
be the (open) disc of radius r about the point (a, b). This notation is convenient 
because we shall often need to refer to open discs, just as we often refer to intervals 
in R. We see easily that 
U (a, 6)eR2 A-(a, b) = K2 and f| (o, fc)eR2 Dr(a, b) = 0. 
It may require some work to identify 
U(G, r) = U (a, &)eG Dr(a, b) and I(G, r) = f] (o> b ) e G A - K b) 
for various regions G in the rry-plane. If 
G = { (a, b) e M2 | a2 + b2 = 1 } 
is the unit circle about the origin, for example, then 
C , ( f t " 
" 
1 ' (x, ,) 
( l - r ) 2 < (x2+y2)< 
(1 + r) 2 } if r < 1 
{x2 + y2) < (1 + r)2 } 
i f r > l , 
J(G,r) = { 0 
i f r ^ 
{ (x, y) | [x2 + y2) < (r - l) 2 } if r > 1. 

SETS OF NUMBERS 
13 
In obtaining these results, it is helpful to keep in mind the geometric interpretation 
of the above sets. For example, U(G, r) is the set of all points in the plane that have 
a distance less than r to a point in G. A 
The Induction Principle 
There is an "obvious" property of N called the well-ordering property. Since this 
property does not follow formally from the rules of the order relation, it is stated as 
an axiom for N. 
Axiom 1.2.8 The well-ordering axiom. Any nonempty subset of N contains a 
smallest element. More explicitly, if T c N and if T is not empty, then there is an 
n G T such that n < m for all m e T. 
The well-ordering of N implies the induction principle for N. 
Theorem 1.2.9 Induction principle. Let S C N. Suppose that 1 £ S and that 
k + 1 G S whenever k G S. Then S — N. 
Proof. Assume, on the contrary, that S ^ N. Let T = N \ S. Then T is a nonempty 
subset of N. Hence, by the well-ordering axiom, there is a smallest element a in T. 
Since a £ S and 1 G S, we must have a > 1. Thus, a — 1 G N. Since a is the 
smallest element in T, we must have a — 1 G" T. Hence, a — 1 G S. By property 2, 
we have a = (a — 1) + 1 £ S, a contradiction. 
G 
Example 1.2.10 Let S c l 
Assume that a G S and that H I 
£ S whenever 
keS. 
Show that { a + k \ k G N } C S. 
Solution. S e t T = { f c G N | a + A;GS,}.We will show that T = N. First, since 
a G S, we have a + 1 e S. Hence, 1 G T. Assume that fc G T. Then o + k G 5. 
Hence, by assumption, a + {k + 1) = (a + k) + 1 G S. Thus, k + 1 G T. By the 
induction principle, T = N. 
A 
The following example shows how the induction principle gives us the familiar 
method of proof by induction. The idea is to prove that some result holds for n = 1 
(the base case) and to show that if it holds for n, then it holds for n + 1 (the inductive 
step). 
Example 1.2.11 Show that 1 + 2 H 
\-n = n(n + l)/2 for all n G N. 

14 
SETS AND FUNCTIONS 
Solution. Let S = { n G N | 1 + 2 H 
+n = n(n+ l)/2 }. Then 1 G S (the 
base case). For the inductive step, suppose that n G S. Then 
l + 2 + --- + n + ( n + l ) 
= 
(l + 2 + ---+n) + ( n + l ) 
= 
(n(n + l)/2) + ( n + l ) 
= 
( n + l ) ( n + 2)/2 
shows that ( n f l ) E S . Hence S = N by the induction principle 1.2.9. A 
Remarks 1.2.12 Limitations of the induction principle. The induction principle 
is useful in stating certain arguments in a clear and concise way. But this principle 
may not be very helpful in obtaining new results. For example, the principle will not 
help you to guess the result 1 + ••• + n = n(n + l)/2. To obtain this result, you 
need other methods. 
Definition 1.2.13 Binomial coefficients. Let r G K and k £N. Define 
r \ 
, 
( r \ 
, ( 
r 
\ 
r(r - 1) ■ ■ ■ (r - k) 
1, 
[ , 
) = r, and ' 
' -
0 ) 
' \1 J 
V k + 1 ) 
(k+l)\ 
These are the general binomial coefficients. 
These expressions will be used in 
examples and later in the discussion of multilinear functions. If r G N, then I , I 
is the number of ways to select k objects from a collection of r objects. 
Problems 
1.9 
Express the set 
C = { X G R | 0 < X 2 - 5 X + 
4 < 1 0 } C R 
in terms of intervals. 
1.10 
There are four bounded regions in the xy-plane bounded by the lines y = x 
and y = 2x + 1, and by the ellipse x2 + Ay2 — 16. One of these regions is 
{ (x, y) G R2 | x2 + Ay2 < 16 and y > x and y > 2x + 1 } . 
Express the other three regions similarly. 
1.11 
Let G = { (x, y) G K2 | - 1 < x < 1, - 1 < y < 1 }. Consider the sets 
U(G, 1) and I(G, 1) defined in Example 1.2.7. Express these sets in simpler terms. 

SETS OF NUMBERS 
15 
1.12 
Let (ai,bi), (02,62) be in U(C,r). 
Consider K2 as the xy-plane and use 
the customary vectorial notations. A subset C of the xy-plane is called convex if 
whenever C contains two points (a1; bi) and (02, 62)7 then C also contains all the 
points on the line segment joining these points. Let C be the set of all points (x, y) 
that can be expressed as 
(x, y)=p 
(1, 0) + q (2, 0) + r (0, 2) + 3 ( - 1 , - 1 ) , 
where p, q, r and s are all nonnegative and p + q + r + s — 1. Show that C is a 
convex set and describe it in simpler terms. 
1.13 
Define a relation D c Z x Z a s 
D = { (a, 6) G Z x Z I There is a fc G Z such that a = fc6 } . 
This relation is called divisibility: (a, b) £ D just in case a is divisible by b. Show 
that divisibility is reflexive and transitive but not symmetric. 
1.14 
Let p G N. Define a relation Cp C Z x Z by 
Cp = { (a, 6) G Z x Z I (a - b, p) G D } , 
where D is the divisibility relation defined in Problem 1.13. This relation among the 
integers is called congruence modulo p. 
1. Show that congruence modulo p is an equivalence on Z. 
2. Show that there are exactly p equivalence classes for this equivalence. 
3. Show that the set of integers 
R = 
{l,2,...,p} 
is a complete set of representatives for congruence modulo p. 
4. What is the equivalence class represented by k £ Z? 
1.15 
Define a relation C\ c l x R b y the condition that (r, s) G C\ if and only if 
(r — s) G Z. This relation among the real numbers is called congruence modulo 1. 
1. Show that congruence modulo 1 is an equivalence on R. 
2. Show that the interval 
R= [0, 1) = {t GK |0 <t < 1 } 

16 
SETS AND FUNCTIONS 
is a complete set of representatives for congruence modulo 1. 
3. What is the equivalence class represented by t G R ? 
Note. Let p G N,p > 2. Congruence modulo p can be defined on R, but this is not 
customary. 
1.16 
Define a relation among the points (x, y) G R2 as follows. A point (x, y) 
is related to a point (x1, y1) if and only if x2 + y2 = x'2 + y'2. Show that this 
is an equivalence. What are the equivalence classes? What is a complete set of 
representatives? Is the x-axis 
{ (a:, 0) G K2 | x = 0 } 
a complete set of representatives? Why or why not? 
1.17 
Define a relation among the points (x, y) G M2 as follows. A point (x, y) is 
related to a point (x\ y') if and only if xy — x'y'. Show that this is an equivalence. 
What are the equivalence classes? What is the equivalence class containing the origin 
(0, 0) ? What is a complete set of representatives? Is the line 
{ (x, y) G WL2 | x = y } 
a complete set of representatives? Why or why not? 
1.18 
Let C be a convex set in the xy-plane (see Problem 1.12) and let r > 0. Show 
that the sets U(C, r) and I(C, r) are also convex, with the notation of Example 
1.2.7. (Hint for the convexity of/(C, r): show that the intersection of any family of 
convex sets is convex.) 
1.19 
Let n G N. Show by induction that 4™ — 3n — 1 is divisible by 9. (Divisibility 
is defined in Problem 1.13.) 
1.20 
Letr G K and n G Z+. 
1. Show that 
\ n 
) + \n+l 
) = \ n+1 
) ■ 
2. Use the induction principle to show that for all integers n > 0, 
k=o \ 
k 
J 
\ 
n 
J 

FUNCTIONS 
17 
1.21 
(Binomial Theorem) Let a, b G M and n G Z+. Use the induction principle 
to show that 
1.22 
Let r, s G M and n G Z+. Show that 
V
1 (r)( 
s ) = (r + s) 
¿-^>k=o \ k j \ n - k j 
\ 
n 
J 
1.23 
Let » E N . Show that YJl=i fc2 = (l/6>(™ + l)(2n + 1) • 
1.3 
FUNCTIONS 
The concept of a function is one of the most important ideas in mathematics. Func-
tions are certainly of paramount importance in analysis. A function from a set X to 
a set F is a special type of relation between the elements of X and Y. A subset of 
X x Y that defines a function is called a graph. 
Definition 1.3.1 Graphs. Let X and Y be nonempty sets. A subset T of X x Y is 
called a graph if, whenever (x, y) and (x, y') are both in T, then y — y'. Hence T 
is a graph if for each x G X there is at most one point (x, y) G T. The domain ofT, 
denoted by Dom r, is defined by 
Dom V = {x e X \ there is a y G Y such that (x, y) G T } . 
Also, i/ze domain space ofT is X and f/ze range space ofT is y. 
Definition 1.3.2 Functions. Let T C X x y be a graph and let £> = DomT. The 
relation defined by this graph T is called a function f : D —» y from D to y. It is 
customary to denote graphs and functions by different symbols. What distinguishes 
a function / from a general relation is the condition that each x G D is related to a 
unique point y G y by /. One calls y the value of f at x, or the image ofx under 
/, and one writes this as y = f(x). If x G D, then one also says that / is defined at 
x. The domain D of V is also called the domain of / and denoted as Dom /. The 
domain space of f is X and the range space of f is Y. 
Remark 1.3.3 Roles of X and Y. Note that the sets X and Y are not uniquely 
determined by the function /. In fact, X can be any set containing the domain of /, 
and y can be any set containing all the values of /. They do determine, however, the 

18 
SETS AND FUNCTIONS 
nature of the function under consideration. If X = Y = R, for example, then we are 
dealing with a real-valued function defined on a set of real numbers. Also, the sets 
X and Y are important for defining bijections in Definition 1.3.20 below. 
Remarks 1.3.4 Role of the graph. The formal definition of a function given above 
in Definition 1.3.2 is satisfactory but rarely used in the actual statement of a function. 
One usually defines f(x) for a general point x by an explicit rule or computational 
formula. Nevertheless, the graphical definition of a function is an important idea that 
will have applications later. 
Remarks 1.3.5 Notation for functions. When a function is defined by an explicit 
rule, then one denotes this function by y = /(er) or by f(x). This notation is not 
strictly correct: y — f(x) is a point in Y rather than the function / : D —► Y itself. 
Nevertheless, y = f(x) is convenient notation which causes no confusion in practice. 
When a function is given as y = f(x), D = Dom / is understood to be the set of x 
for which f(x) is defined. 
We shall sometimes express a function as y = y(x). This notation indicates that we 
are dealing with a function the points of whose domain space are denoted by x and 
points in the range space by y. It eliminates the unnecessary symbol /. Still, the 
most common notation in practice is y = f(x). 
Definition 1.3.6 Restrictions of a function. Let / : D —> Y be a function and 
A C D. The restriction of f to A is a new function that has the value f(x) for x G A 
but is undefined if x $ A. In general, it is not necessary to use a different notation 
for the restricted function. This is understood from the context. 
Definition 1.3.7 Identity functions. Let X be a set. Define a function Ix ■ X —> X 
as Ix(x) = x for all x G X. It is called the identity function on X, or simply the 
identity on X. We also write / instead of Ix if X is understood from the context. 
Definition 1.3.8 Sequences. Let IK be a subset of Z such that IK is bounded below 
but not above. A function defined on IK is called a sequence. One usually takes 
IK as N or an unbounded subset of N. The range space of a sequence can be any 
nonempty set Y. By a sequence in Y, we mean a sequence with the range space Y. 
IK is called an index set. The value of a sequence a r K ^ F a t f e G K i s called the 
fcth term of the sequence and is denoted by a^. The sequence itself may be denoted 
as a : K —> Y, as a,k, k G IK, as {a^}, or simply as a^ if the domain K is understood. 
This last notation is not strictly correct, but it is convenient to use when the meaning 
is clear from the context. A sequence is usually given by a formula involving n. 
Such a sequence is defined for all n G N for which this formula is meaningful. For 
example, an = {n — 5) defines a sequence a : N —> Z and bn = l/(n — 5) defines a 
sequence b : K -> <Q>, where K = N \ {5}. 

FUNCTIONS 
19 
Examples 1.3.9 Graphs i n R x i = R2. The graphs introduced in Definition 1.3.1 
above become ordinary graphs in the plane when X = Y = K. 
1. Let G = { (x, x2) 6 R2 | x £ K }. Then G is a graph and DomG = M. The 
function defined by G is y = x2. As noted above in Remarks 1.3.4, one defines 
this function directly by the formula y = x2, without mentioning the graph G. 
In this case, G is a parabola. 
2. Let G' = { (x, y) £ R2 | x = y2 }. Then G' is once again a parabola, but G' 
is not a graph. If x > 0, then both (x, x1/2) and (x, —a;1/2) are in G' and 
x1/2 T^ —a;1/2. The uniqueness requirement of Definition 1.3.1 is not satisfied. 
Note that G' is obtained from G by interchanging x and y. As x and y do not 
appear symmetrically in the definition of a graph, it turns out that G is a graph 
but G' is not. 
3. Let Gi = { (x, y) £ R2 | x = y2, y > 0 }. G\ is the upper branch of the 
parabola G' above. G\ is a graph and DomG = [0, oo), the positive part of 
the x-axis. The function gi defined by G\ is y = x1/2. Note that this formula 
is meaningful only if x > 0. Hence D = [0, oo) is the domain of g\. The 
lower branch G2 of G' is another graph. The function g2 defined by G2 is 
y = —x1!2. Also, Dom52 = [0, 00). 
4. Let H = { (x, y) £ R2 | xy = 1 } be a hyperbola. We see that H is a graph. 
In fact, if (x, y) and (x, y') are both in H, then xy = xy' = 1 implies that 
i / O and y = y'. We see easily that the domain of H is 
D = VomH = (R\{0}) = (-00, 0) U (0, 00). 
The function defined by H is y = 1/x, x ^ 0. A 
Examples 1.3.10 More general functions. In this course we deal mainly with 
functions for which the domain space is Rm and the range space is Rn, where 
m, n G N. An efficient way to work with these functions in specific examples is to 
define them directly by a set of formulas. 
1. Let m = 2, n = 1. Identify the domain space M2 with the xy-plane and 
the range space R with the 2-axis. A formula z — f(x, y) gives us a real-
valued function defined on a subset D of the xy-plane. For example, let 
f(x, y) = xy/(x2 + y2). Then the domain of / is the set D of all (x, y) £ R2 
for which the expression (xy)/(x2+y2) 
is meaningful. We see that D contains 
all points in R2 except the origin (0, 0) of R2. The graph T of / is a subset of 
R2 x M = R3. More explicitly, 
T = { (x, y, z) e R3 I z = xy/(x2 + y2) and x2 + y2 ^ 0 } . 

20 
SETS AND FUNCTIONS 
Geometrically, T is a surface in R3. 
2. Let m — n = 2. Identify the domain space R2 with the xy-plane and the range 
space R2 with the uw-plane. A set of formulas 
u = u(x, y), v = v(x, y) 
gives us a function defined on a subset of the xy-plane and taking values in the 
icy-plane. For example 
u — x2 + y2, v = y/x 
is such a set of formulas. The domain of this function is the set 
D = {(x,y)\ 
(x, i / ) e R 2 , i / 0 } . 
Hence one obtains D by removing the j/-axis from the xy-plane. A 
Remark 1.3.11 Coordinate changes. Certain functions for which both the domain 
and range spaces are R™ are called coordinate changes in R™. Coordinate changes 
are considered in later chapters in some detail. Here we provide a few examples of 
functions Rra —> R™ that are used as coordinate changes. A 
Example 1.3.12 Polar coordinates. Polar coordinates are given as 
x = x(r, 6) = reos6, 
y = y(r, 6) = rsin#. 
This is a coordinate change in R2. Both the domain and range spaces for this function 
are R2. The domain space R2 is identified with the rö-plane and the range space 
R2 with the xy-plane. This function takes the point (r, 9) e R2 in the rö-plane to 
the point (r cos 6, r sinö) £ R2 in the a;y-plane. In the present context, r £ l and 
9 € R are two real numbers and (r, 6) e R2 is an ordered pair of real numbers. We 
see that (reos9, rsinö) € R2 is defined for all (r, 9) € R2. Hence the domain of 
this function is also R2. 
A 
Example 1.3.13 Cylindrical coordinates. These coordinates are given as 
x = rcos6, 
y = rsinö. z = £. 
This is a coordinate change in R3. The domain space R3 is identified with the r9(-
space and the range space R3 with the xyz-space. Actually, the standard notation 
for £ is also z. Hence the domain space is the röz-space and the range space is the 
xyz-space. The domain of this function is also M3, since 
(reos9, rsinö, Q e R3 

FUNCTIONS 
21 
is defined for all (r, 9, <) e R3. A 
Example 1.3.14 Spherical coordinates. These coordinates are given as 
x = psinocos9, 
y = ps\mpsm9, 
z = pcosíf. 
This is a coordinate change in R3. The domain space R3 is identified with the pip6-
space and the range space R3 with the xyz-space. The domain of this function is 
also R3, since 
(p sin ip cos 9, p simp sin 9, p cos ip) 6 R3 
is defined for all {p, <p, 9) G R3. A 
Images Under Functions 
Definition 1.3.15 Images of sets. Let / : D —> Y be a function with domain space 
X and range space Y. 
1. Direct images under a function. Let U C X. Then the direct image ofU 
under / is defined as the set 
f{U) = {y£Y\y 
= f(x), 
xGUHD}. 
Hence /([/) is the set of all values f(x), where x € UnD. Note that /([/) = 0 
if and only if C/n£> = 0. Also note that f{U) = f{UC\D) for any set U C X. 
2. The range of a function. The direct image of the domain space is called the 
range of f and denoted as Range /. Hence 
Range/ = f(X) = /(D). 
The range of / is the set of all points y e Y in the range space that are the 
images of points x e D. 
3. Inverse images under a function. Let V C Y. Then the inverse image of V 
under / is the set 
f~\V) 
= {xex\ 
f{x) eV} = {xeD\ 
f(x) e V } . 
Here the first equality is the definition of / _ 1 (V) as the set of all points i g l 
which have images f(x) in V. Since f(x) exists only for i G D . w e need to 
consider only points x £ D. This is expressed by the second equality. 
Example 1.3.16 Images under polar coordinates. Let / : R2 —> R2 be the polar 
coordinates defined in Example 1.3.12. The domain space is represented by the 

22 
SETS AND FUNCTIONS 
rö-plane and the range space by the xy-plane. The value of / at the point (r, 9) in 
the domain space is the point 
(x, y) ~ f(r, 9) = (reos9, rsm9) 
in the range space. Note that 
f(r,6) 
= 
f(r,0 + 2kn) 
= 
f(-r, 
9 + {2k + 1)TT) for all k e 1. 
Hence, if an inverse image / - 1 ( V ) contains a point (r, 0), then it also contains all 
the points of the form 
(r, 0 + 2kiv) and (-r, 9 + (2k + 1)TT), where k G Z. 
Vertical lines in the rö-plane correspond to the constant values of r. We see that the 
direct images of these lines in the rö-plane are concentric circles about the origin in 
the xy-plane. The images of the horizontal lines in the rö-plane are lines passing 
through the origin in the xy-plane. Let C be a circle of radius a about the origin in 
the xy-plane. Then the inverse image f~l{C) 
of this circle consists of two vertical 
lines r = ±a in the rö-plane. Let L be a line in the xy-plane passing through the 
origin and making an angle of <p with the positive x-axis. Then the inverse image 
/ _ 1 (L) of this lines consists of infinitely many horizontal lines in the rö-plane given 
as 0 = if + kir, k G Z. 
Figure 1.3 shows a part of the inverse image of the shaded region in the xy-plane. 
This region is bounded by two circles about the origin and two lines passing through 
the origin. Its inverse image under / is the union of infinitely many rectangles in the 
rö-plane. Figure 1.3 shows four of these rectangles. The direct image of each of 
these rectangles is the same shaded region in the xy-plane. 
A 
This last example shows that a function may have the same value at many different 
points. Functions for which this does not happen are important. They are called 
one-to-one functions. 
Definition 1.3.17 One-to-one functions. Let / be a function with D = Dom /. 
Let A <Z D. Then / is said to be one-to-one (or injective) on A if f{x{) =/= /(X2) 
whenever x\, xi 6 A and X\ ^ x-i. Equivalently, / is one-to-one on A if x\ = Xi 
whenever Xi, x-i G A and f(x\) = /(X2). 
Theorem 1.3.18 Let f be a function with D = Dom f. Let A C D and B = f(A). 
Then f is one-to-one on A if and only if there is a function 
g : B —> A 
such that g(f(x)) — xfor all x G A. 

FUNCTIONS 
23 
Figure 1.3. Images under polar coordinates. 
Also, if such a function g exists, then it is unique and f(g{y)) = y for each y G B. 
Proof. Assume that / is one-to-one on A. Let B — f(A). 
Then for each y G B, 
there is a unique x G A such that f(x) = y. Hence, we can define a function 
g : B —> A by letting x — g(y) whenever f(x) = y. Hence, x = g{y) = g{f{x)) 
for all x G A. 
Conversely, assume the existence of g. \ix\, X2 € A and /(xi) = /(#2)> then 
xi = g{f{xi)) 
= g{f{x2)) = x2-
Hence / is one-to-one on A. 
Finally, if g exists, then it is unique. In fact, the previous argument shows that if 
g exists, then for each b G B there is a unique a G A such that b = f(a) and 
g(b) = g(f(a)) = a. If h is another function on B such that h(f(xj) 
= x for all 
x & A, then h(b) — h(f(a)) 
= a = g(b). Hence h = g. Finally, g(y) — x and 
f{x) = y show that f(g(y)) = y. 
□ 
Definition 1.3.19 Inverse functions. Let / be a function with D — Dom /. Let 
A C D and B = f(A). 
A function defined on B is called an inverse function of 

2 4 
SETS AND FUNCTIONS 
f on A if g(f(x)) 
— x for all x G A. Theorem 1.3.18 above shows that / has an 
inverse function on A if and only if / is one-to-one on A. Also, if an inverse function 
g exists, then it is unique. Its value at y £ B is the unique solution of the equation 
y = f(x). Finally, the same theorem also shows that if g is the inverse of / on A, 
then / is the inverse of g on B = f(A). 
Definition 1.3.20 Invertible functions. A function / : A —> B is called an invert-
ible function, or a bijection, or a one-to-one correspondence between A and B if 
it has an inverse function g : B —> A. Theorem 1.3.18 shows that / : A —> B 
is an invertible function between A and B if and only if / is one-to-one on A and 
B = f(A). 
Definition 1.3.21 One-to-one and onto functions. When a function / : A —* B is 
said to be invertible, it is understood that it is invertible between A and B. Hence, 
in the case of an invertible function /, the sets A and B in the notation / : A —> B 
become important. The invertibility of / : A —> B means that / is one-to-one on A 
and that B = f(A). 
An invertible function / : A —> B is also called a one-to-one 
and onto function. Here the sets A and B are again important. It is understood that 
/ is one-to-one on A and that it maps A onto B = f(A). 
Example 1.3.22 Let y = f(x) = (2x - l)/(3x + 1). This function is defined for 
all x ^ - 1 / 3 . Hence D = Dom/ = (R \ {-1/3}). The equation 
,, . 
2 z - 1 
y = f(x) 
is uniquely solved as 
x = g{y) 
3x + l 
1 + 2/ 
2~3y 
for each y ^ 2/3. Hence / is one-to-one on D, and its inverse on D is g. Also, 
Dome/ = (R\{2/3}). A 
Example 1.3.23 Let y = f(x) = x2 + x + 1. Then f(x) is defined for all x e K. 
Hence D = Dom / = K. The solution of the equation x2 + x + 1 = y is given by 
the formula 
a; = - ( l / 2 ) ( l ± ( 4 y - 3 ) 1 / 2 ) . 
This formula defines x if and only if Ay — 3 > 0, that is, if and only if y > 3/4. 
Hence f(D) = /(E) = [3/4, oo). Finally, the equation x2 + x + 1 = y has two 
solutions for each y > 3/4. These two solutions are symmetrical with respect to the 
point x = —1/2. There is only one solution in A = [—1/2, oo) and also only one 
solution in A' = (—oo, —1/2]. Hence / is one-to-one on A and also one-to-one on 
A'. The inverse of / on A is 
g(y) = (l/2)(-l + (Ay - 3)1/2) 

FUNCTIONS 
25 
and the inverse of / on A' is 
g'(y) = 
{l/2)(-l-(4y-3)1'2). 
Note that/(A) = f(A') = /(R) = [3/4, oo). There are (infinitely many) other sets 
C c l such that / is one-to-one on C and such that /(C) = /(R)- For example, 
C= (-00, -1) U [~l/2, 0] is such a set. A 
Example 1.3.24 Polar coordinates were discussed in Examples 1.3.12 and 1.3.16. 
They are defined as a function / : R2 —> R2 such that 
f(r, 9) = (x, y), where x = reos 6* and y = rs'mO. 
As observed in 1.3.16, / is not one-to-one on R2 — Dom/. Hence / does not 
have an inverse function on R2. But there are many sets A C Dom / such that / is 
one-to-one on A. For example any function is one-to-one on a singleton set. The 
important point is to find a set A c R2 = Dom / such that / is one-to-one on A and 
such that f(A) = /(R 2). There are many such sets. Let, for example, a € 1 be a 
fixed number and define 
Aa 
= 
{(r, 6) É l 2 I 0 < r, a<9<a 
+ 2ir } U {(0, 0)}, 
A'a 
= 
{(r, Ö) GlR2 | r < 0 , a < 6 » < a +2TT }U{(0, 0)}. 
It is easy to check that / is one-to-one on each of these sets and also that 
f(Aa) 
= f(A'a) = /(M2) = R2 for each a £ l . 
Note that / is not one-to-one, for example, on 
C0 = { (r, 9) e R2 I 0 < r, 0 < 9 < 2TT } . 
In fact, in this case / maps all the points on the vertical segment 
S0 = { (r, 6) G R2 I r = 0, 0 < 0 < 2it } c C0 
to (0, 0). This is a triviality, and it is ignored in many cases. One defines the polar 
coordinates of (x, y) as (r, 9) such that 0 < r , a < 9 < a + 2-K, and such that 
x = r cos 9 and y = r sin 9. Here a is a fixed specific number like a = 0 or a = —n, 
depending on the problem. Everyone knows that this does not determine the 9 value 
for (x, y) = (0, 0). But everyone also knows that this is not an important point in 
most cases. A 
Composition of Functions 
Definition 1.3.25 Composition of functions. If / and g are two functions, then 
the composition g o / is defined by (g o f)(x) = g(f(x)). The domain of g o f is 

26 
SETS AND FUNCTIONS 
specified by this definition in an obvious way. It is the set of all x for which g(f(x)) 
is defined. Let F = Dom / and G — Dom g. We see that 
Dom(gof) = Fnr1(G) 
= r1(G)-
In fact, F is the set of all x for which f{x) is defined and 
f-1(G) 
= 
{xeF\f(x)eG}cF 
is the set of all £ e F for which f(x) is in the domain of g. Hence g(f(x)) is defined 
if and only if a; £ 
f~1{G). 
The composition of more than two functions is defined similarly. If /, g, and h are 
three functions, for example, then h o g o f is defined as 
(hogof)(x) 
= 
h(g(f(x))). 
The domain of h o g o f is the set of all x such that h(g(f(x))) 
is defined. If 
F = Dom / , G = Dom g, and i í = Dom h, 
then we see that 
Dom(ho g o f) = 
f-\g-\H)). 
In fact, x £ Dom (/i o g o /) if and only if g(f(x)) £ H. This happens if and only if 
f{x) e g^x{H), which is equivalent to a: 6 
f~1(g^1(H)). 
Lemma 1.3.26 Images under compositions. Leí / ana g be two functions. Then 
{gof)-\E) 
= 
rl(9~\E)) 
for any set E. Also, if Ac Dom (g o /), then 
(gof)(A)=g(f(A)). 
Proof. For the first part, note that 
xe(gof)-\E) 
^ 
(gof)(x)eE 
^ 
g(f(x))GE 
^ 
f{x)eg-\E) ^ 
xef-l(g-\E)). 
Also, 
(9°f)W 
= 
{(gof)(x)\xeA} 
= 
{g(f(x))\xeA} 
= 
{g(y)\yef(A)} 
= g(f(A)). 
a 

FUNCTIONS 
2 7 
Remarks 1.3.27 Compositions and inverses. Let X be a set. The identity function 
Ix ■ X —> X on X was defined in Definition 1.3.7 as Ix(x) 
= x for all x G X. 
Let / be a function with A = Dom/ and B = f(A). 
Let g be a function with 
J5 = Domg. Then <? is the inverse of / on A if and only if g o f — IA. In this case 
also, / o g = IB. These remarks follow directly from Theorem 1.3.18. 
Problems 
1.24 
Let / : D —> Y be a function with the domain D <z X and the domain space 
X. Let £ be a collection of subsets of X. Show that 
/ (uES££)) = u£6£/(£) and /(nße££) c nw(£)-
Give an example to show that f(f\Ee£E) 
j= Clßesfi^) 
ls possible. 
1.25 
Let / : D —> Y be a function with the range space Y. Let 7 be a collection 
of subsets of Y. Show that 
r 1 (UFe:rF) = U ^ r 1 ^ ) 
and Z"1 ( f V e ^ ) = f W / - 1 ^ ) • 
1.26 
Let / : £) —+ F be a function with the domain D c X, the domain space X, 
the range space Y, and the range R = f(X) 
— f(D) C Y. 
1. Show that fif^iB)) 
= B n fi for all B C F. 
2. Show that ,4 n £> C / _ 1 ( / ( ^ ) ) f o r all Ac 
X. 
3. Give examples to show that f~1(f(A)) 
^ An D h possible. 
4. Show that if / is one-to-one on D, then f~l {f{A)) = ADD. 
1.27 
Let f(x) = x2 - Qx - 7. What is the range f(D) of /? Is / one-to-one on 
its domain? If not, find two different sets P, Q C M. such that / is one-to-one on P 
and one-to-one on Q and such that f(P) = f(Q) — /(K). Find the inverse function 
of / on P and the inverse function of / on Q. 
1.28 
Let / : D —> Y be a function with the domain D C X, the domain space X, 
the range space Y, and the range R = f(X) 
= f(D) C Y. Define a relation on D 
by the condition that a e f l i s related to 6 e D if and only if /(a) = /(£>). 

28 
SETS AND FUNCTIONS 
1. Show that this is an equivalence in D. 
2. Let P c D be an equivalence class and let V C Y be any subset of Y. Show 
that either P C f'1 (V) or P n f'1 (V) = 0. 
3. Let A C D be a complete set of representatives for this equivalence. Show 
that / is one-to-one on A and R = f(A). 
1.29 
Let / : R2 —> R2 be the polar coordinates defined by 
(x, y) = /(r, 0) = (rcosö, rsinö). 
Find f~\A) 
for 
J 4 - { ( i , t / ) e l 2 | l < ( i 2 + y2) < 4 and 0 < (y/x) < 1 } . 
1.30 
Define the function / from the xy-plane to the ttw-plane by 
(u, v) = f(x, y) = (3x + 2y, 6x + 4y). 
1. What is the domain D c K2 of /? 
2. What is the range R = /(M2) = /(£>) C E2 of / ? 
3. Let a, b £ R, and let La be the line u = a and Mb the line v = b in the 
uw-plane. What are the inverse images f~l(La) 
and 
f~1(Mt,)7 
4. Let (a, 6) € i?. What is J^ 1 ({ (a, 6) })? 
5. Find some examples of A C D such that / is one-to-one on A and such that 
f(A) = /(£>). 
1.31 
Define the function / from the xy-plane to the Mf-plane by 
(u, u) = f(x, y) = (3x + 2y, Qx - Ay). 
Repeat the parts of Problem 1.30 for this example. 
1.32 
Define the function / from the xy-plane to the uv-plane by 
(u, v) = f(x, y) = (xy, 
y/x). 
Repeat the parts of Problem 1.30 for this example. 

FUNCTIONS 
29 
Part A 
PartB 
Figure 1.4. Hints for Problems 1.33 and 1.34. 
1.33 
Define the function / from the zy-plane to the uw-plane by 
(U) v) = f(x, y) = {{x2 + y2)/(2x), 
(x2 + 
y2)/(2y)). 
Repeat the parts of Problem 1.30 for this example. {Hint. In Part A of Figure 1.4 we 
see the inverse images of the lines u = —3, 3, 5, 7 and the lines v = —3, 3, 5, 7.) 
1.34 
Define the function / from the xy-plane to the uu-plane by 
(u, v) = f{x, y) = ({x2 +y2 + l)/(2:r), (x2 +y2~ 
l)/(2j/)). 
Repeat the parts of Problem 1.30 for this example. {Hint. In Part B of Figure 1.4 we 
see the inverse images of the lines u = 1.5, 2.25 and the lines v = —0.5, 0, 1, 1.5.) 
1.35 
Define the function / from the xy-plane to the uw-plane by 
(u, v) = f(x, y) = (p{x, y) + q(x, y), p(x, y) - q(x, y)), 
where p(x, y) = {(x + l) 2 + y2)1?2 and q(x, y) = {{x - l) 2 + y2)1^2. 
Repeat the 
parts of Problem 1.30 for this example. 

3 0 
SETS AND FUNCTIONS 
Figure 1.5. 
Hint for Problem 1.35. 

CHAPTER 2 
REAL NUMBERS 
This chapter reviews key facts about the set of real numbers, denoted by R. Some im-
portant subsets of R are the set of natural numbers N = { 1, 2 , . . . } , the set of integers 
Z = {0, ±1, ±2, ... }, and the set of rational numbers Q = {p/q \ p, q G Z, q ^ 0 
The set of integers and the set of rational numbers are constructed easily, starting 
with the set of natural numbers. By contrast, there is no easy construction for the set 
of real numbers. Appendix A provides a construction of K. 
We shall be selective in our discussion of M, for we shall assume a working knowledge 
of the real numbers. Specifically, we shall assume that all the rules of working with 
the arithmetic operations and with the order relations are known. By the arithmetic 
operations we mean addition, multiplication, subtraction, and division. By the rules 
of order relations we mean the rules of working with equalities, inequalities, and 
absolute values. Section 2.1 summarizes the basic facts about order relations, which 
may not be quite so familiar as the arithmetic operations. 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
31 

32 
REAL NUMBERS 
An essential property of the real numbers is completeness. Most of the basic results 
in this course rely upon this property. Section 2.2 explains the completeness property, 
and subsequent sections of the chapter explore some of its implications for analysis. 
The final sections of the chapter review basic facts about series of real numbers, as 
well as essential results about the topology of K, i.e., open, closed, and connected 
sets. 
2.1 
REVIEW OF THE ORDER RELATIONS 
We begin with the set of positive real numbers, 
P = {xeR\0<x} 
c l . 
Other order relations may be defined in terms of P. The basic inequality x < y 
means that (y — x) G P. Other inequalities are defined in terms of this first one: 
y > x, x < y, and y > x. We assume that the rules of working with inequalities 
are known, noting only that all of them can be derived easily from the following two 
properties of positive numbers. (Some examples are given below in Example 2.1.5 
and also in the problems.) 
Remark 2.1.1 Two properties of P. The set P of positive numbers has the following 
properties. 
1. For each x G l exactly one of the following three cases is true: x = 0, x G P, or 
-x£ 
P. 
2. If x, y G P, then x + y G P and xy G P. 
Remark 2.1.2 A property of Z. The set of integers has the following special 
property, stated in terms of the order relations on R. 
For every real number s e l , there is an integer fceZ such that 
k < s < k + 1. 
More generally, let n G N. Then for every real number í e l , there is an integer 
k G Z such that 
(k/n) <t<{k 
+ l)/n. 
To see this, apply the first result to s = tn: there is a k G Z such that k < tn < k +1. 
Since 0 < n, we can divide these inequalities by n to obtain (k/n) <t<(k 
+ l)/n. 
Remark 2.1.3 A property of N. Let r G R and 0 < r. Then there is an n G N such 
that (1/n) < r. For if s = 1/r, then there is a k G Z such that k < s < k + 1, and 

REVIEW OF THE ORDER RELATIONS 
33 
we may take n = k + 1. Since 0 < s < n, we have (1/n) < r. It is clear that if 
r e R i s such that 0 < r < (1/n) for all n € N, then r = 0. 
Density of Q. The following lemma shows that the rational numbers are dense in R: 
between any two distinct real numbers, there is a rational number. 
Lemma 2.1.4 Let a, b G R and a <b. Then there is a rational number p G Q such 
that a < p < b. 
Proof. Let t = b - a. Then 0 < t. Use property 2.1.3 of N to find an n € N 
such that (1/n) < t. Then use property 2.1.2 of Z to find a k G Z such that 
(k/n) <b<(k 
+ l)/n. Let p = (k/n). Then p G Q and p < b. Also 
6 - p < [((fc + l)/n) - (fc/n)] = (1/n) < t = b - a, 
so that a < p. Hence a < p < b. 
□ 
Example 2.1.5 Inequalities. Prove the following inequalities by applying the two 
properties of P stated in 2.1.1. 
1. I f x ^ O , thenO < x2. 
We are given that either x € P or {—x) G P. In the first case a;2 
In the second case x2 = (—x)(—x) £ P. 
2. If x < y and 0 < z, then zx < zy. 
We are given that (y — x) G P and z £ P. Hence (y — x)z = (yz 
Therefore xz < yz. 
3. If 0 < x and 0 < xy, then 0 < y. 
We are given that x G P and (xy) G P. If y = 0, then xy = 0, which 
contradicts xy G P. Hence y ^ 0. If (—y) G F, then x(—y) = —(xy) G -P. 
This contradicts xy G P. Hence (—y) $ P. Then y G P is the only remaining 
possibility. A 
Example 2.1.6 We want to show that n < 2™ for all n G N. We use an induction 
argument. Let G = { n G N | n < 2 n } . Then we see that 1 G G. Assume that 
n G G. Then also (n + 1) G G, since 
(n + 1) = (l + ( l / n ) ) n < 2 n < 2 2n = 2 n + 1. 
Here the first inequality follows from the fact that (1/n) < 1 for all n G N, and the 
second inequality follows from the induction hypothesis. Hence G = N. Also note 
that for any integer K > 2, n < 2" < Kn for all n G N. A 
= xx G P. 
- xz) G P. 

3 4 
REAL NUMBERS 
Example 2.1.7 Let r e N. If 0 < r < 2~™ for all n 6 N, then r — 0. In fact, in this 
case we also have 0 < r < 2~n < (1/n) for all n £ N. The last inequality follows 
from Example 2.1.6 above. Hence r = 0, as in 2.1.3. A 
Definition 2.1.8 Absolute values. Let i f B . Then the absolute value ofx, denoted 
by |xj, is defined by 
{
—x if x < 0, 
0 if x = 0, 
x 
if 0 < x. 
Hence \x\ = —x if x < 0 and |x| = x if 0 < x. Note that 0 < \x\ in every case. In 
arguments involving |x|, we usually separate the cases x < 0 and 0 < x. 
Lemma 2.1.9 The following are true for all x, y € JR. 
(1) |-z| = l4 
(2) -|x| < x < |x|. 
(3) \x\ < y if and only if —y < x < y. 
(4) \xy\ = |x| \y\. 
(5) |x + y\ < \x\ + \y\. (The triangle inequality) 
Proof. (1) If x < 0, then 0 < —x. Hence |x| = —x and | — x\ = — x. Therefore 
|xj = | — x| in this case. The other case is similar. 
(2) If x < 0 then 0 < (—x) and |x| = —x. Hence 
-\x\ = - ( - x ) = x < 0 < (-x) = |x|. 
The other case is similar. 
(3) Assume that |x| < y. Then — y < —|x|. Hence, by the previous case, 
—y 5~ —\x\ < x < |x| < y. 
Conversely, assume that — y < x < y. Multiply these inequalities by —1 to obtain 
—y < — x < y. Since |x| is either x or —x, we see that — y < |x| < y. 
(4) Let 0 < x and 0 < y. Then 0 < (xy). Hence \xy\ = xy = |x| \y\ in this case. 
The general case follows from this case by observing that \xy\ = \ (|x| \y\) \. 
(5) By part 2, —|x| < x < |x| and — \y\ < y < \y\. Hence, 
-(\x\ + \y\)<x 
+ y< \x\ + \y\. 

REVIEW OF THE ORDER RELATIONS 
35 
Hence, by part 3, \x + y\ < \x\ + \y\. 
□ 
Example 2.1.10 Find all real numbers x such that |2x + 1| + |1 — 3a; ¡ < 5 . 
Solution. Call the given inequality (A). The expression for \2x + 1| changes at 
x = —1/2. Similarly, the expression for |1 — 3a;| changes at x = 1/3. Hence (A) 
have different expressions in the intervals 
/ = (-oo, -1/2], J = [-1/2, 1/3], and K = [1/3, oo). 
If x 6 / = (-oo, —1/2], then (A) becomes 
|2x + 1| + |1 - 3x| = (-2x - 1) + (1 - 3x) = -5x < 5. 
This gives —1 < x. Hence (A) is satisfied in I n (—1, oo) = (—1, —1/2]. 
lfxeJ= 
[-1/2, 1/3], then (A) becomes 
|2x + 1| + ¡1 - 3x| = (2x + 1) + (1 - 3x) = 2 - x < 5. 
This gives —3 < x. Hence (A) is satisfied in J n (—3, oo) = [—1/2, 1/3] = J. 
If x e K = [1/3, oo), then (A) becomes 
\2x + 1| + |1 - 3x| = (2x + 1) + (-1 + 3x) = 5x < 5. 
This gives x < 1. Hence (A) is satisfied in K n (—oo, 1) = [1/3, 1). 
Combining these results, we see that (A) is satisfied if and only if 
x e ( - 1 , -1/2] U [-1/2, 1/3] U [1/3, 1) = ( - 1 , 1). A 
Problems 
2.1 
Show that if 0 < a < b, then 0 < (1/6) < (1/a). Use only the two properties 
of positive numbers listed in Remark 2.1.1. 
2.2 
Let S be the set of all real numbers I É I such that 
- 2 <x2 -x+\x2 
- 1 | < 2. 
Express S in terms of intervals. 
2.3 
Let S be the set of all points (x, y) in the xy-plane such that 
| z - 2 | + | j / - 3 | < l . 

36 
REAL NUMBERS 
Show that S is a region bounded by four lines. Find these lines and describe S 
geometrically. 
2.4 
Show that for all real x, y, we have ||x| — \y\\ < \x — y\. 
2.5 
Let a, b, and c be real numbers with a > 0. Show that there is an M G 1 such 
that if \x\ > M, then ax2 + bx + c> 0. 
2.6 
Show that for all positive a, we have 
a+ - > 2. 
a 
2.7 
Let a, b, and c be real numbers. Show that 
a2 + b2 + c2 > ab + be + ca. 
2.8 
Find all x € M such that 
(x - \){x + 3){x + 5 ) 0 + 2) < 0. 
2.2 COMPLETENESS OF REAL NUMBERS 
Completeness is a basic property of the real numbers. The rational numbers do not 
have this property. In fact, the real numbers can be obtained by adding new elements 
to the set of rational numbers to make it complete. Details of this construction are 
found in Appendix A. Here we simply treat the completeness of the real numbers as 
an axiom, since our objective is to explore its implications. 
Upper and Least Upper Bounds 
Definition 2.2.1 Upper bounds. Let AcR.lf 
there is a number M e R such that 
a < M for all a 6 A, then A is is said to be bounded above. Any number M such 
that a < M for all a £ A is called an upper bound of A. Hence a subset of K is 
bounded above if and only if it has an upper bound. Also, if M is an upper bound of 
A and if M < M', then M' is also an upper bound of A. 
Definition 2.2.2 Least upper bounds. Let A be bounded above. An upper bound 
L of A is called a least upper bound of A if all upper bounds for A are greater than 
or equal to L. 

COMPLETENESS OF REAL NUMBERS 
3 7 
For example, the interval (l,4) = { a ; G K | 0 < x < 4 } has least upper bound 4. 
Axiom 2.2.3 Completeness of R. Let A be a nonempty subset of R. If A has an 
upper bound, then A has a least upper bound. 
Remark 2.2.4 Uniqueness of the least upper bound. A set can have at most one 
least upper bound. For if L and L' are both least upper bounds of A, then they are 
also upper bounds of A. Therefore L < L1 and L' < L. Hence L = L'. 
Remark 2.2.5 Completeness axiom and the empty set. The completeness axiom 
excludes the empty set 0. In fact, any number M € R is an upper bound for 0, since 
the condition that a < M for all a e 0 is vacuously satisfied for any M e l . Hence 
there are upper bounds but no least upper bound for the empty set. 
Remark 2.2.6 Incompleteness of Q. The set of rational numbers is not complete: 
S={xeQ>\x2<2} 
has an upper bound but no least upper bound. There is no 
rational number r such that r2 = 2, as proved in Example 2.2.7 below. But there 
is a real number p such that p2 = 2. We show this in Example 2.2.8, by using 
the completeness of the real numbers. Hence the set of rational numbers cannot be 
complete. 
Example 2.2.7 There is no rational number r such that r2 = 2. 
To see this, assume that there are integers a and b such that 2 = (a/b)2. By reducing 
to least terms, we may assume that a and b have no common factors except ±1. Now 
262 = a2 so that 2 is a factor of a2. It follows that 2 is also a factor of a, i.e, a = 2m 
for some integer m. Hence, 2b2 = Am2 so that b2 = 2m2. This implies that 2 is 
also a factor of b. This contradicts our assumption that a and b have no common 
(nontrivial) factors. Hence, no such a and b exist. A 
Example 2.2.8 There is a real number p such that p2 = 2. 
To obtain such a p, let A = { a; e R | 0 < a; and x2 < 2 }. Then A is nonempty 
since, for example, 1 e A. Also, A is bounded above (by 2). By completeness, A 
has a least upper bound, p. Note that 1 < p < 2. We will show that p2 = 2. 
Suppose that p2 < 2. Then p2 = 2 - e for some e > 0. So (p + 6)2 = p2 + 2p5 + S2 
will be less than 2, provided we take 5 > 0 small enough that 2pS + 62 < e. But 
then p + S G A, so that p is not an upper bound for A, a contradiction. Similarly, 
if p2 > 2, we can find Ö > 0 such that (p — ó)2 > 2, proving that p is not the least 
upper bound for A. Once again, we have a contradiction. Hence, p2 = 2. A 
Definition 2.2.9 Irrational numbers. A real number is called an irrational number 
if it is not a rational number. The previous examples show that there are irrational 

3 8 
REAL NUMBERS 
numbers. In fact, the following example shows that the set of irrational numbers is 
also dense in K. 
Example 2.2.10 Density of irrational numbers. There is an irrational number 
between any two distinct real numbers. We see easily that if a is a rational number, 
then ay/2 is an irrational number. Now given any r, s € R, if r < s, then we have 
(r/y/2) < (s/y/2). Then, by Lemma 2.1.4, there is a rational number a such that 
(r/y/2) < a < (s/y/2). Hence r < ay/2 < s and ay/2 is irrational. 
A 
Lower and Greatest Lower Bounds 
Upper and least upper bounds have symmetrical counterparts: lower and greatest 
lower bounds. There is an equivalent statement of the completeness axiom in terms 
of lower bounds. 
Definition 2.2.11 Lower bounds. Let A c M . If there is a number m e R such 
that TO < a for all a € A, then A is is said to be bounded below. Any number m 
such that m < a for all a £ A is called a lower bound of A. Hence a subset of R is 
bounded below if and only if it has a lower bound. Also, if m is a lower bound of A 
and if m! < TO, then m' is also a lower bound of A. 
Definition 2.2.12 Greatest lower bounds. Let A be bounded below. A lower bound 
I for A is called a greatest lower bound of A if all lower bounds for A are less than 
or equal to Í. 
Lemma 2.2.13 Let A c R and M, L <E R. Let -A = { -a £ R | a e A } . Then 
the following are true. 
(1) M is an upper bound of A if and only if —M is a lower bound of —A. 
(2) L is the least upper bound of A if and only if —L is the greatest lower bound 
of-A. 
(3) A is bounded above if and only if—A is bounded below. 
Proof. Trivial. G 
Remark 2.2.14 The completeness axiom 2.2.3 is equivalent to the following axiom: 
if a nonempty set of real numbers has a lower bound, then it has a greatest lower 
bound. 

COMPLETENESS OF REAL NUMBERS 
3 9 
Definition 2.2.15 Supremum and infimum. Let i c l 
The least upper bound of 
A, if it exists, is called the supremum of A and is denoted as sup A. Similarly, the 
greatest lower bound of A, if it exists, is called the infimum of A and is denoted as 
inf A. 
Definition 2.2.16 Bounded sets. A set of real numbers is called a bounded set if it 
is both bounded above and bounded below. By the completeness axiom, a nonempty 
bounded set has both a greatest lower bound (an infimum) and a least upper bound 
(a supremum). Hence, if A is a nonempty bounded set of real numbers, then sup A 
and inf A both exist. 
Theorem 2.2.17 Let c = sup P. Then 
( c , o o ) n P = %and{s, c j n f / % for alls < c. 
Proof. Since c is an upper bound for P, we have p < c for all p € P. Hence 
(c, oo) n P = 0. If (s, c] n P were empty for some s < c, then we would have 
(s, oc) n P = 0 and p < s for all p £ P. Hence s would be an upper bound for P 
that is smaller than the least upper bound c. 
ü 
Problems 
2.9 
If a G K and S C M, then let aS denote the set of all real numbers of 
the form x = as, where s G 5. Show that if sup S exists and if a > 0, then 
sup(aS') = a sup S. If a < 0, then show that inf (aS) — a sup S. 
2.10 
Let S, T be two nonempty bounded sets of real numbers. Let S + T = 
{s + t\seS,t€T} 
and ST ={ st \s£S,t€T}. 
Show that 
sup(5 + T) 
= 
sup S + sup T 
inf (S + T) 
= 
infS + infT. 
Is it true that sup(ST) = (sup 5)(supT)? 
2.11 
Let £ c R b e nonempty and bounded. Show that K \ E is bounded neither 
from above nor below. 
2.12 
Let 
s={l-^ri\kez,k^0,-i}. 
Find inf S and sup S. 

40 
REAL NUMBERS 
2.13 
Let a, 6, c, and d be in R with a < b and c < d. Set 
T = { | 2 x - 3 y | | x e [ a , 6 ] , ? y G [ c , C i ] } . 
Show that T is bounded. Find sup T and inf T. 
2.14 
Let S, T be nonempty sets of real numbers that are bounded from above. Is 
it true that {s — t\ s e. S, t G T } is also bounded from above? 
2.3 SEQUENCES OF REAL NUMBERS 
Sequences were defined in Definition 1.3.8. Here we consider sequences of real 
numbers. Such a sequence is typically a function x : N —> R, where xn is the value 
of the sequence at n G N. We also commonly denote the sequence itself as xn. This 
slight abuse of notation is convenient when the meaning is clear from the context. 
Sequences may also be defined only on subsets of N. 
Definition 2.3.1 Bounded sequences. A sequence xn of real numbers is called a 
boundedsequenceif there is a number A such that \xn\ < ylforalln G N. Otherwise, 
xn is an unbounded sequence. 
Definition 2.3.2 Convergent sequences. A sequence xn is called a convergent 
sequence if there is a number a G R with the following property: for each e > 0 
there is an N G N such that \xn — a\ < e for all n > N. In this case the number 
a is called the limit of the sequence xn and one says that xn converges to a. The 
convergence of x„ to a is indicated as lim„ xn = a, or as limraeN xn — a, or simply 
as Xji 
► a. 
Lemma 2.3.3 A sequence cannot converge to two different points. 
Proof. Assume that xn —> a and xn —* b and \a — b\ = p > 0. Find an M e N such 
that \xn - a\ < p/3 for all n> M. Also find an N G N such that \xn - b\ < p/3 
for all n>N.lfn> 
max(M, N), then 
\a-b\<\a- 
xn\ + \b- xn\ < (2/3)p < p. 
This contradicts \a — b\ = p > 0. Hence, if xn —> a and xn —» b, then a = b. 
□ 
Lemma 2.3.4 Any convergent sequence is bounded. 
Proof. Let xn —> a. Find an iV G N such that \xn — a\ < 1 for all n > N. Hence 
knl < \a\ + \xn - a\ = \a\ + l 

SEQUENCES OF REAL NUMBERS 
41 
for all n > N. Let A - max(|ai|, ..., |ajv|) and let M — A + \a\ + 1. Then we 
see that \x„\ < M for all n e N. 
□ 
Remarks 2.3.5 A bounded sequence need not be convergent. xn = ( - l ) n is a 
bounded but not a convergent sequence. 
Definition 2.3.6 Zero sequences. A sequence xn in R is called a zero sequence if it 
converges to zero. 
Lemma 2.3.7 A sequence xn in R converges to a G R if and only if xn — a + zn, 
where zn is a zero sequence. 
Proof. This a reformulation of the definition of convergence. 
D 
Theorem 2.3.8 The sum of two zero sequences is a zero sequence. The product of a 
zero sequence and a bounded sequence is a zero sequence. In particular, the product 
of a zero sequence and a convergent sequence is a zero sequence. Also, the product 
of two zero sequences is a zero sequence. 
Proof. Let rn and sn be two zero sequences. Given e > 0 find P e N and Q &N 
suchthat \rn\ < e/2ifn 
> Pand \sn\ < e/2ifn > Q. LetiV = max(P, Q). Then 
\rn + s n| < \rn\ + \sn\ < (e/2) + (e/2) = e whenever n > N. Hence (rn + sn) is 
a zero sequence. 
Now let rn be a zero sequence and un a bounded sequence in R. Hence there is a 
K > 0 such that \un\ < K for all n e N. Given e > 0, find an AT e N such that 
|rn| < e/Ä" for all n > N. Then 
\unrn\ = \un\ \rn\ < K \rn\ < K{e/K) 
= e 
for all n> N. Hence unrn is a zero sequence. 
□ 
Theorem 2.3.9 Let un and vn be two sequences in R. Assume that un —> a and 
vn —* b. Then (un + vn) —> (a + b) and (unvn) 
—> (ab). If a ^ 0, f/ze« a/io 
("n/Wr.) -^ {b/a). 
Proof. Let rn = (un — a) and sn = (vn — b). Both are zero sequences. Then 
Theorem 2.3.8 shows that 
(un + vn) - (a + b) - 
rn + sni 
unvn - ab — (a + r„){b + sn) - ab 
= 
asn + brn + +rnsn 

4 2 
REAL NUMBERS 
are zero sequences. Hence (un + vn) —> (a + b) and (un vn) —► (ab). 
Now \/un is defined only if un = (a + rn) ^ 0. Since rn —+ 0, there is an N e N 
such that \rn\ < \a\/2 for all n> N. Let 
_ _ t 
1 _ 
-rn 
a + rn 
a 
a(a + rn) 
But \a + r n| _ 1 < 2/\a\ and \pn\ < (2/|a|2) \rn\ for all n> N. Therefore, pn is a 
zero sequence. Hence (l/un) 
—> (1/a). Then 
(Vn/Un) = Vn(\/Un) 
-► 6(1/a) 
by the first part. 
□ 
Theorem 2.3.10 Lei wn and v„ ¿>e iwo sequences in M.. Ifun —» a and i/(w„ — iin) 
is a zero sequence then also vn —+ a. 
Proof. We see that (vn — a) — (un — a) + (vn — un) is a zero sequence, as the sum 
of two zero sequences. 
□ 
Monotone Convergence Theorem 
The Monotone Convergence Theorem gives a sufficient condition for the convergence 
of a sequence. This condition does not refer to the limit of the sequence, but only to 
the terms of the sequence. 
Notations 2.3.11 If sn is a bounded sequence in R, the set 
S = { sn | n e N } 
is a bounded set in R. Hence inf S and sup S exist. For this particular set S we write 
inf S = infn sn and sup 5" = sup„ sn. 
Definition 2.3.12 Monotone sequences in R. Let sn be a sequence in R. Then sn 
is called a monotone sequence if either sn < s„ + 1 for all n £ Nor sn+i < sn for 
all n 6 N. In the first case, sn is also called an increasing sequence; in the second 
case, it is called a decreasing sequence. 
Theorem 2.3.13 Monotone Convergence Theorem. Every monotone and bounded 
sequence is convergent. More specifically, let sn be a bounded and monotone 
sequence. Then sn —> inf„ sn if sn is decreasing and sn —» sup„ sn if sn is 
increasing. 

SEQUENCES OF REAL NUMBERS 
4 3 
Proof. Let sn be a decreasing and bounded sequence. Let a = infn sn. Then a < sn 
for all n G N, but if a' > a, there is an N G N such that SN < a'. We claim that 
sn 
—> a. Let e > 0. Then a1 = (a + e) > a, and therefore there is an N G N such 
that syv < (a + e). But sn < sjy for all n > N since sra is a decreasing sequence. 
Therefore, if n > N, then 
\sn — a\ = sn — a < SN — a < (a + s) — a = e. 
Hence sn —> a = inf S. The proof for increasing and bounded sequences is similar. 
In this case we see that sn —> b = sup n sn. 
□ 
Example 2.3.14 Geometric series. Let p be a number, 0 < p < 1. Define x n = 
1 + p + p2 + • ■ ■ + pn ■ We claim that xn is a convergent sequence. 
Clearly, 
xn 
< xn+i 
for all n G N, so xn is an increasing sequence. Also, by familiar 
algebraic manipulations we see that 
xn = 1 +p + p2 + ■■■+pn = (1 -Pn+1)/(l-p)< 
1/(1 
-p). 
Hence xn is also a bounded sequence. Therefore, xn converges by the Monotone 
Convergence Theorem. In this case we can also show that xn —> (1 — p)~x, but this 
point is not important for the following application. 
A 
Examples 2.3.15 An application of geometric series. Let sn be a sequence in R. 
Assume that there is a constant M, an integer N, and a number p such that 0 < p < 1 
and such that 0 < sn < Mpn for all n> N. Let xn — s\ + s2 + • • • + sn. Then xn 
is monotone increasing. It is also bounded. In fact, if m G N, then 
XN+m 
= 
XN-i 
+ XN + ■ ■ ■ + XN+rn 
< 
xN_1+MpN(l+p+---+pm) 
< 
XN^+Mi^il-p)-1. 
Therefore xn converges in M. 
As a specific case, let sn — rn/(n\), 
where r G E is a fixed number, 0 < r, and 
n\ = 1-2-3 • • -n, as usual. Find N G Nsuchthatp = (r/N) 
< 1. Then we see easily 
that SN+m < SNP771 = (sNp~N)pN+m 
for all m G N. Hence our requirements are 
satisfied with M = SNP~N 
= sjv(N/r)N. 
Therefore we know that 
converges for all r > 0. The limit is well defined, but it cannot be expressed (at 
least not in an obvious way) in terms of r and the previously defined operations. Of 
course, this limit is e r. 
A 

44 
REAL NUMBERS 
Problems 
2.15 
Let \a\ < 1. Show that limn an = 0. 
2.16 
Let 0 < a. Show that limn a1/n = 1. 
2.17 
Show that each irrational number is the limit of a sequence of rational 
numbers. Also show that any rational number is the limit of a sequence of irrational 
numbers. 
2.18 
Let (xn), (yn) be sequences of real numbers with limn^oo xn = —2 and 
limn_(00 yn = 3. If a, b are real numbers such that 
lim (axn - byn) 
= 
lim (bxn + ayn) 
n—>oo 
n—>oo 
lim (bxn - ayn) 
= 
lim (axn + byn), 
n—>oo 
n—>oo 
what are the values of a and 6? 
2.19 
Let \a\ < 1 and r € M. Show that the sequence 
*»=EL, ( I ) «
fc 
is convergent. (The limit of this sequence is (1 + a)r, but the proof of this fact 
requires more work.) 
2.20 
Let a > 0. Define a sequence xn recursively by x\ = a and 
xn+i = l + (l + x„) _ 1, n e N . 
Show that xn is a convergent sequence. Show that limn xn = \[2. 
2.21 
Let a > 0 and c > 0. Define a sequence xn recursively by x\ = a and 
x n +i = 1 + c(l + x n ) ~ \ n e N . 
Show that xn is a convergent sequence. What is lim„ xnl 
2.22 
Let a > 0 and c > 0. Define a sequence xn recursively by x\ = a and 
xn+i = 1 + (c/x n), n G N. 
Show that xn is a convergent sequence. What is lim„ xnl 

SUBSEQUENCES 
45 
2.4 SUBSEQUENCES 
Lemma 2.3.4 shows that every convergent sequence is a bounded sequence. It is clear 
that not every bounded sequence is a convergent sequence. Nevertheless, there is a 
basic relation between bounded sequences and convergent sequences: every bounded 
sequence has a convergent subsequence. This is called the Bolzano-Weierstrass 
theorem, and it is a very important consequence of the completeness of the real 
numbers. 
Definition 2.4.1 Subsequences. Let x : N —> M be a sequence of real numbers. A 
subsequence of x is the restriction of the domain N to an unbounded subset 
I
d 
Hence x^,k 6 K, is a subsequence of xn,n G N. Wewritex/c —> aorliirifcgK^fc = a 
to signify the convergence of the subsequence ^ to a 6 R. A more familiar way 
to denote a subsequence is the following. Let Xk, k G K, be a subsequence. Order 
the integers in K as a sequence k\ < k-¿ < ■ ■ ■ and denote this subsequence as Xkn, 
where n G N, or simply as Xkn ■ We then write x^n —► a or lim„eN Xkn = a for the 
convergence of Xkn to a. 
Remarks 2.4.2 Note that if a sequence is convergent, then every subsequence of it 
is also convergent and converges to the same point as the full sequence. This follows 
easily from the definitions. 
Lemma 2.4.3 Let xn be a sequence in R. Let o £ l . Assume that the set 
Kr = {neN\a-r<xn<a 
+ r} 
is an infinite (unbounded) set of integers for each r > 0. Then xn has a subsequence 
that converges to a. 
Proof. Define a sequence of integers k\ < ki < ■ ■ ■ as follows. Let k\ be the 
smallest integer in Ki. If ki < ■ ■ ■ < kn are defined, then let kn+\ be the smallest 
integer in Ki/(n+i) such that kn < kn+\. Such an integer exists since Ki/(„+i) is 
an infinite subset of N. Then an induction argument shows that kn is defined for each 
n G N. Now given any r > 0, there is an AT € N such that (1/N) < r. Hence we 
see that 
I xkn — a | < l / n < l/N < r whenever n > N. 
This means that the subsequence Xf¿n converges to a. G 
Lemma 2.4.4 Let xn be a sequence in M. For each t G M, let 
Q(t) = 
{neN\xn<t}. 

4 6 
REAL NUMBERS 
Let a EM.be a number such that Q{u) is a finite set of integers whenever u < a and 
Q(v) is an infinite set of integers whenever a < v. Then there is a subsequence of 
xn that converges to a. 
Proof. Given r > 0, find u, v E R such that 
a — r<u<a<v<a 
+ r. 
Then we have 
Q(v)\Q(u) 
= 
{nEN\u<xn<v} 
C 
{neN\a-r<xn<a 
+ r} = Kr. 
We see that (Q(v) \ Q(u)) C N is an infinite set, since Q{v) is an infinite set and 
Q(u) is a finite set. Therefore Kr is an infinite set for each r > 0. Then Lemma 
2.4.3 shows that there is a subsequence of xn that converges to a. 
Q 
Theorem 2.4.5 Bolzano-Weierstrass Theorem. Every hounded sequence in R has 
a convergent subsequence. 
Proof. Let xn be a bounded sequence in R. Assume that p < xn < q for all n E N. 
For each t E R, let 
Q(i) = { n e N | i n <t }. 
Let T C R be the set of all t E R for which Qit) is a finite set of integers. That is, a 
real number t belongs to T just in case all but finitely many members of the sequence 
xn are larger than t. 
lit < p, then Q(t) = 0 is a finite (bounded) set. Hence t E T for all t < p. Therefore 
T is not an empty set. If t > q, then Q(t) = N is an infinite (unbounded) set. Hence 
t E1 T for all t > q. Therefore T is bounded above. Hence a = sup T exists by the 
completeness axiom. 
If v > a, then v $. T, since a is an upper bound for T. Then Q(v) is infinite. If 
u < a, then u is not an upper bound for T. Therefore there is a w E T such that 
u < w. Hence Q(w) is finite. Then Q(u) is also finite since Q{u) C Q(w). Hence 
Q{u) is finite for all u < a and Q(v) is infinite for all v > a. Lemma 2.4.4 shows 
that there is a subsequence of xn converging to a. G 
Cauchy Sequences 
Definition 2.4.6 Cauchy sequences. A sequence of real numbers xn is called a 
Cauchy sequence if for each e > 0 there is an integer N EN such that \xn — xm \ < e 
for all m, n> N. 

SUBSEQUENCES 
4 7 
Theorem 2.4.7 A sequence of real numbers is a Cauchy sequence if and only if it is 
a convergent sequence. 
Proof. Assume that xn is a convergent sequence and let xn —> a. Given e > 0, find 
N G N such that \xn - a\ < e/2 for all n > N. Then 
\xn - xm\ < )xn - a\ + \a -xm\< 
(e/2) + {e/2) = e 
for all m, n > TV. Hence 
Cauchy sequence. 
Conversely, assume that %n IS 3. Cauchy sequence. Then xn is clearly bounded. Use 
the Bolzano-Weierstrass theorem to find a convergent subsequence Xk, k G K. Let 
a = linifcgK Xk- Then for each e > 0 there is a K G K such that |xfc — a| < e/2 
for all /c > if, fc G K. Also, find a n J V e N such that \xn - xm\ < e/2 for all 
m, n> N. Let A; > ma,x(K, N) and A; € K. If n > iV, then 
|x„ - a| < |x„ - xk\ + \xk -a\< 
e/2 + e/2 = e, 
since n,k > N and k > K. Hence xn -^ a. 
□ 
Theorem 2.4.8 Let xn and yn be Cauchy sequences. Then xn + yn and xnyn are 
also Cauchy sequences. If lim„ xn ^ 0, then l/xn is also a Cauchy sequence. 
Proof. Theorem 2.3.9 shows that the sum and the product of two convergent se-
quences is again a convergent sequence. Theorem 2.4.7 shows that a sequence is 
a Cauchy sequence if and only if it is convergent. These two results show that the 
sum and the product of two Cauchy sequences is again a Cauchy sequence. The last 
part of the theorem follows in the same way from the corresponding part of Theorem 
2.3.9. Here l/xn is defined, of course, only when xn ^ 0, as mentioned in the proof 
of Theorem 2.3.9. G 
Remarks 2.4.9 Convergent sequences and Cauchy sequences. Theorem 2.4.7 
shows that, in K, the notions of Cauchy sequence and convergent sequence are 
equivalent. This is an important fact about R - it does not hold for the rational 
numbers. The concept of a Cauchy sequence is the simpler of the two, for the 
definition is stated in terms of the sequence only and does not refer to the limit point, 
which may or may not belong to the sequence. The definition of Cauchy sequences 
can be simplified even further, as the following theorem shows. 
Theorem 2.4.10 Let xn be a sequence in R. Then the following are equivalent. 
(1) There is a number a G R such that xn —> a; i.e., for each e > 0 there is an 
N G N such that \xn — a\ < efor all n > N. 

4 8 
REAL NUMBERS 
(2) For each s > 0 there is an N G N such that \xn — x¡y \ < sfor all n > N. 
(3) For each e > 0 there is an N G N such that \xn ~ xm | < efor allm, n > N. 
Proof. The equivalence of (1) and (3) is Theorem 2.4.7 above. Also, it is clear that (3) 
implies (2). Now assume (2). Given s > 0, find N G N such that \xn — XN\ < e/2 
for all n > N. If m, n > N, then 
|a;n ~ xm\ < \xn - xN\ + \xm - xN\ < (e/2) + (e/2) = e. 
Hence (3) follows. 
□ 
Problems 
2.23 
Let xn be a sequence in R. Let a e R , Assume that 
Lr = { n G N | a - r < xn < a} 
is an infinite (unbounded) set of integers for each r > 0. Show that xn has a monotone 
increasing subsequence converging to a. 
2.24 
Let xn be a sequence in M. Let a e K. Assume that 
E>r = {neN\a<xn<a 
+ r} 
is an infinite (unbounded) set of integers for each r > 0. Show that xn has a monotone 
decreasing subsequence converging to a. 
2.25 
Let xn be a bounded sequence in R. For each n G N, let 
S'n = {a;G]R|a; = Xk, n < k} . 
Then 5ra is a nonempty and bounded set and sn = sup Sn exists. Show that sn is a 
convergent sequence. Show that if sn —» a, then x n has a subsequence converging 
to a. (This gives another proof for the Bolzano-Weierstrass theorem.) 
2.26 
Let xn be a bounded sequence in R. Let sn be the sequence obtained in 
Problem 2.25. Assume that there is an N e N such that s¡y = .SAT+A: for all k e N. 
Show that x„ has a monotone increasing convergent subsequence. 
2.27 
Let xn be a bounded sequence in R. Let sn be the sequence obtained in 
Problem 2.25. Assume that there is no N € N such that SJV = sjv+fc for all k G N. 
Show that x n has a monotone decreasing convergent subsequence. 

SUBSEQUENCES 
4 9 
2.28 
Show that every sequence (bounded or not) has a monotone subsequence. 
2.29 
Show that a sequence xn is a Cauchy sequence if and only if there is a zero 
sequence zn such that \xn — xm\ < zn for all rn, n 6 N, m > n. 
2.30 
Let xn and yn be Cauchy sequences. Show directly, without using Theorem 
2.4.7, that xn + yn
 and xnyn are also Cauchy sequences. Also show directly that 
if there is an a > 0 such that \xn\ > a, then l/xn is a Cauchy sequence. Give an 
example of a Cauchy sequence xn such that xn ^ 0 for all n £ N but l/xn is not a 
Cauchy sequence. 
2.31 
Give an example of a nonconvergent sequence xn such that 
\xn+2 ~ Xn+i | < \xn+i - xn | for all n e N. 
2.32 
Let xn be a sequence in M. Define a new sequence 
sn = \x2 - xi\ + \x3 - x2\ H 
h \xn - x n_i|, n > 2. 
Show that if ,s„ is a bounded sequence then xn is a Cauchy sequence. 
2.33 
Give an example of a Cauchy sequence xn for which the sequence 
Sn = \X2 ~Xi\ 
+ \X3 -X2\ 
H 
h \xn+l 
-Xn\, 
UEN, 
is an unbounded sequence. 
2.34 
Let pn be any sequence of numbers such that pn > 0 for all n £ N. Show 
that any Cauchy sequence xn has a subsequence Xkn such that 
\xkn+1 ~ xkJ< 
pn for all n e N. 
2.35 
Show that every Cauchy sequence xn has a subsequence Xkn for which 
sn = \xk2 ~xkl\ 
+ \xk3 -xk2\-\ 
h \xkn+1 - £fc„|, n e N, 
is a bounded sequence. 
2.36 
Let cn be a sequence. Assume that the sequence 
^n = |Cl | + ' • ' + \Cn\ 
is a bounded sequence. Show that there is a zero sequence zn such that 
|C„+1 H 
h C n + f c | < Zn 

50 
REAL NUMBERS 
for all n, k € N. 
2.37 
Let p, q € N and p < q. 
1. With the general binomial coefficients as defined in Definition 1.2.13, show 
that 
for all k € N. 
2. Show that (1 + (q/p)x)p < (1 + x)9 for all x > 0. 
3. Letx = l/nand((?/p) —1 = r > 0. Transform the last inequality algebraically 
to obtain 
1 
1 / 1 
1 
< 
(n+ l ) 1 + r " r \ n r 
(n + l) r 
for all n e N. 
2.5 SERIES OF REAL NUMBERS 
Definition 2.5.1 Series. Let xn, n £ Z +, be a sequence. Then the sequence 
E
n 
xk, n £ Z +, 
fc—0 
is called a sen« (or an infinite series). The terms sn of this series are called its 
partial sums. Note that there is no great difference between series and sequences: 
just as a series is a sequence of partial sums, any sequence sn may be considered as 
the series sn = 5Zfc=o xk w¡th XQ = SQ and xn = sn — sra_i, n £ N. 
Definition 2.5.2 Convergence of series. If limn sn = limn X]fc=o ^^ exists> m e n 
sn is called a convergent series. If the limit does not exist, then the series diverges. 
We denote the limit of a convergent series as 
s = \imnsn = lim„ Y2k=Q Xk = 5Zfc=0
 Xk-
If Pn > 0 and if sra = po + Pi + ' •' + Pfc is a bounded sequence then we express 
this as 
E
co 
^ - ^ 
Note that although expressions like s = YlT=o xk and SfcLo Pfc involve a summation 
sign, they are not proper summations. That is because, in general, they lack important 
properties of ordinary finite summation (e.g., insensitivity to the order of summation). 

SERIES OF REAL NUMBERS 
5 1 
Lemma 2.5.3 IfY^n \xn\ < °°> then S = YlT=o lXfcl an<^ s = Sfclo xk exist. 
Proof. The sequence Sn = ^fc=o \Xk\ ' s mon°tone increasing. If Y^n \xn\ < °o> 
then Sn is also a bounded sequence. Hence 5 = \\mnSn = ¿Cfelo \xk\ e xi s t s-
This follows from the Monotone Convergence Theorem, 2.3.13. Then we know that 
Sn is a Cauchy sequence. Therefore, given e > 0, there is an N e N such that 
Sn+k -Sn = \Sn+k - Sn\ < e for all n > N and for all k e N. Then 
|Sn+fc — S„| = |xn+i + ' • ■ + ¡Era + fcl ¡Í Fn+l| + ' ' ' + pn+fcl = ^n+fc ~ Sn 
shows that sn is a Cauchy sequence. Hence s = lim„ sn = Yl'k—o Xfc exists. □ 
Definition 2.5.4 Absolute convergence. A series sn = J3k=o Xk ' s called abso-
lutely convergent if Y^n \xn\ < oo. 
Lemma 2.5.3 shows that if a series is absolutely convergent, then it is also convergent. 
It also shows that a series sn = Ylt=o Xk ' s absolutely convergent if and only if the 
series Sn = X]fc=o lXfcl *s convergent. 
A convergent series is not necessarily absolutely convergent. A counterexample 
is sn = X]fc=o ( —-0 n/( n + !)• The limit of an absolutely convergent series is 
insensitive to rearrangements of the terms of the series, but the same is not true for a 
series that is convergent but not absolutely convergent. Using Problem 2.42, one can 
show that the terms of the series sn = X]fc=o (~l)™/(n + 1) can be rearranged to 
converge to any desired limit or to diverge. 
Tests for the Convergence of Series 
We review some familiar and very useful tests to investigate the convergence of a 
series. 
Theorem 2.5.5 The root test. Let sn = X^fe=o Xk be a series. Assume that there 
are M, r € K and N £ N such that 0 < r < 1 and such that 
\xn\<Mrn 
for all n> N. Then S — Y^T=o \xk\ and s = Sfc°=o Xk ^otn ex^st-
Proof. We have 
ElT^ 
= EL^' + E^+J^ 
|xfcl + V 
Mrk 
^ EL | a ; f c | + M / ( 1~ r ) 

52 
REAL NUMBERS 
for all n e N. Hence we see that ^ n \xn\ < oo. Then Lemma 2.5.3 shows that 
s = EfcLo \xk\ands 
= Efclo xkexist- 
D 
Corollary 2.5.6 The ratio test. Let sn — X]fc=o xk be a series. Assume that there 
is anr e R and JVeN such that 0 < r < 1 and such that 
(|£„+i|/|xn|) <r 
for all n> N. Then S = J2T=o \Xk\ anc^ s = 
TCA*1O Xk ^otn exist. 
Proof. Let M = \XN \■ Then, by an induction, \xN+k | < M rk for all fc £ N. Hence 
\xn\ < (M/rN)rn 
for all n > TV. Then an application of Theorem 2.5.5 completes 
the proof, 
ü 
Theorem 2.5.7 Let an > 0 be a sequence. Assume that there is an r > 0 such that 
X)n Ia« I r™ < °°- Then J2n dnnkxn 
< oo for all k £ N a«d |x| < r. 
Proof. Let yx = (x/r) and yn = (n/(n - l))k{x/r), 
n>2. 
Note that 
nkxn = y! 
-y2---y„rn. 
We see that limn(n/(n — l))fc = 1. Also, \x/r\ < 1. Hence there is a TV G N such 
that \yn\ < 1 for all n > TV. Let M — \yx ■ y2 ■ ■ ■ VN\- Then 
\annkxn\<M\an\rn 
foralln > TV. This shows that J2n \annkxn\ 
< oo. Hence sn(x) = E L i " ™" 1 ' 1 " 1 
is absolutely convergent. 
□ 
Problems 
2.38 
If Yl afc converges, show that lmin^oo an = 0. 
2.39 
For each n > 1, let 
_ 
n ! ( 3 n - l ) 2 
ün~ 
1-3 
(2n + l ) ' 
Determine if ^ ora converges. 
2.40 
Suppose that £) an and ^ &n are infinite series with positive terms. Assume 
that 
lim p- = I > 0. 

SERIES OF REAL NUMBERS 
53 
Show that both J2 &n and ^ bn converge or diverge together. 
2.41 
Let an be a decreasing sequence of positive numbers such that £) an con-
verges. Show that limna„ = 0. 
2.42 
Show that J2n°=i « diverges. Hence, show that if p(n) and q(n) are polyno-
mials and q(n) ^ 0 for any n e N, then 
^ 
q{n) 
diverges if the degree of p is 1 less than the degree of q. 
2.43 
Suppose that Y2 \ak\ converges. Show that Yl \ak\p converges for all p > 1. 
2.44 
Let xn be a sequence in M such that linin^oo xn exists. Decide whether or 
not 
E
oo 
, (Xn+1 
— 
Xn) 
n—1 
converges. 
2.45 
Suppose that ]T] a^ converges. What can we say about the convergence of 
Y — 
¿-^ 1 +a 
+ 4' 
2.46 
Suppose that an converges and ^ Xk converges. If Xk > 0 for all k, show 
that ^2 dkXk converges absolutely. 
2.47 
Does the series ^ sin (1/fc) converge? 
2.48 
Show that there is a unique function r : N —> Z + such that 
2T(n) < n < 2 T ( n ) + 1 
for each n e N. Define xn = (2~r(ra)n - 1) for n e N. Write the first fifteen 
terms of this sequence. Show that for each a £ R, 0 < a < 1, there is a convergent 
subsequence of a:n converging to r. 
2.49 
Let rn be a sequence in K. Assume that there is an R e K, 0 < ñ, such that 
IrJ < Rn for all n e N. Show that 
fe=o fc! 
is a convergent sequence. 

54 
REAL NUMBERS 
2.50 
Let xn be a sequence of positive real numbers with 
1 
•En — 
X\ . . .X n — i 
for all n > 2. Determine whether or not Yl'kLi xk converges. 
2.51 
Binary expansions. Let r £ l and 0 < r < 1. A sequence bn is called a 
binary expansion of r if bn = 0 or bn = 1 for each n G N and if the sequence 
E
n 
_ 
onz 
converges to r. Show that each r G [0, 1] has a binary expansion. Show that the 
binary expansion is unique, except for the numbers of the form r — k2~n, for some 
n, k G N with 0 < k < 2n. Show that for numbers of this type there are exactly two 
binary expansions. 
2.52 
Ternary expansions. Let r G M and 0 < r < 1. A sequence tn is called 
a ternary expansion of r if tn = 0 or tn = 1 or tn = 2 for each n G N and if the 
sequence 
E
n 
_ 
A.=1
Í™3 
converges to r. Show that each r G [0,1] has a ternary expansion. Show that the 
ternary expansion is unique, except for the numbers of the form r = k3~n, for some 
n, k G N with 0 < k < 3™. Show that for numbers of this type there are exactly two 
ternary expansions. 
2.6 INTERVALS AND CONNECTED SETS 
Intervals were introduced in Example 1.2.1. Here we define intervals rigorously and 
prove that they are the only connected subsets of R. The concept of a connected 
set, explained below, will be important (and much less simple) in multidimensional 
vector spaces. 
Definition 2.6.1 Intervals. A set of real numbers / is called an interval if it satisfies 
the following condition: if a, bel 
and a < t < b, then t G I. Hence / is an interval 
if whenever I contains two numbers, then it also contains all the numbers in between. 
Definition 2.6.2 End points of bounded intervals. Let / be a bounded nonempty 
interval. Hence a = inf I and b = sup / both exist. They are called the end points 
of L In particular, a = inf / is the initial point and b = sup I is the final point of /. 

INTERVALS AND CONNECTED SETS 
55 
Definition 2.6.3 Open intervals, closed intervals. We see that 
(a, b) = {t € R ¡a <t < b} and [a, b] = {t 6 R | a < t < b } 
are both intervals with end points a and b. We call (a, b) an open interval and [a, b] 
a closed interval. 
Lemma 2.6.4 Let I be a bounded interval with the initial point a and the final point 
b. Then (a, b) C I C [a, b}. 
Proof. Lett € (a, b). Then a < t implies that t is not a lower bound for /. Therefore, 
there is an a' € I such that a' < t. Similarly, b is not an upper bound for /, and 
therefore there is a b' € I such that t < b'. Hence a' < t < b' with a', b' € /. So 
t e I by the definition 2.6.1 of an interval. This shows that (a, b) c /. The other 
inclusion is obvious, lit € I, then inf I = a<t<b 
= sup I, since a is a lower 
bound and b is an upper bound for the points in /. 
□ 
Remarks 2.6.5 Bounded intervals. Let / be a bounded interval with the initial 
point a and the final point b. Lemma 2.6.4 shows that there are only four possible 
forms for /. These are (a, b), [a, b], and the half-open intervals 
[a, b) = {t e R| a <t < b} and (a, 6] = {t G R | a < t < b } . 
Incidentally, we note that the infimum and the supremum of a bounded set may or 
may not be elements of the set. 
Closures of Sets in H 
Definition 2.6.6 Closure of a set. Let i c l . The closure of A is defined as the set 
A = {x e~R\ there is a sequence xn e A,n &N, such that x = lim„ xn } . 
Hence A consists of all those points I É I that are the limits of convergent sequences 
in A. Intuitively, one may say that A is the set of all points that are approachable 
by sequences from A. Note that A C A. In fact, every x € A is also the limit of a 
constant sequence xn = x in A. 
Lemma 2.6.7 Points in the closure. Let i
d 
and u £ R. Then u e A if and 
only if for each r > 0 there is an x G A such that \x — u\ < r. 
Proof. Assume that u £ A. Hence there is a sequence xn in A such that xn —> u. 
Hence \xn — u\ is a zero sequence. Therefore, for each r > 0, there is an N £ N 
such that \xn — u\ < r for all n> N. 

5 6 
REAL NUMBERS 
Conversely, assume that for each r > 0, there is an x G A such that \x — u\ < r. 
For each n £ N, choose xn £ A such that \xn — u\ < 1/n. Then we see that XJI is a 
sequence in A and xn —> u. Hence u g A 
□ 
Remarks 2.6.8 Reformulations of the above lemma. We may also state Lemma 
2.6.7 as follows. A point u G R is in the closure of A c R if and only if for each 
r > 0 the open interval {u — r, u + r) intersects A. Equivalently: a point iiGRis 
in the closure of A c R if and only if every open interval containing u intersects A. 
Lemma 2.6.9 Let A CR and i e t 
Assume that a < tfor all a e A. Then also 
x < tfor all x G A. 
Proof. Assume that u > t. Then r = (u — i) > 0. For any a G A, 
(u — a) >{u — t) = r. 
Hence there is no a G A such that \a — u| < r. Therefore, by Lemma 2.6.7, u £ A. 
D 
Corollary 2.6.10 Let c G R and A C (-oo, c], B c [ c, oc). Then ÄflBis either 
empty or the single point set { c}. 
Proof. Lemma 2.6.9 shows that if A c (—oo, c], then also A C (—oo, c]. Similarly, 
we see that ß c [ c, oo). Hence I í l B c { c } . 
O 
The proof of the following lemma contains the main argument of this section. 
Lemma 2.6.11 Let a < b. Let A and B be two sets such that A U B = [a, b], 
a G A, b G B. Then [a, b} O A f~i B is not empty. 
Proof. The set A is nonempty, since a G A, and bounded, since A c [a, b]. Hence 
c = sup A exists and a < c < b. We show that every open interval containing c 
intersects both A and B. 
Now (s, c] intersects A for all s < c and (c, t) is disjoint from A for all t > c. 
This follows easily from the the definition of c as sup A or directly from Theorem 
2.2.17. Hence we see that each open interval (s, t) containing c intersects both A 
and Ac = R \ A. We want to show that (s, t) also intersects B. If c = 6, then this 
is trivial, because b G B. If c < b, then (c, 6) c [a, b] but (c, 6) fl A = 0. Since 
Ali B = [a, b], this means that (c, b) C £?. Hence (s, t) intersects both A and B 
whenever s < c < t. Therefore c £ A n £? by the remarks above in 2.6.8. Hence 
[a, 6 ] n i n ß contains c G [a, b] and therefore is nonempty. 
□ 

INTERVALS AND CONNECTED SETS 
5 7 
Connected Sets in E 
Definition 2.6.12 Connected sets. A set C in R is called a connected set if the 
following condition is satisfied: for any two nonempty sets A and B such that 
C = AUB,CC)AnB 
is nonempty. An intuitive formulation of this condition is 
that whenever C is the union of two nonempty sets A and B, then C contains points 
that are approachable both by sequences from A and by sequences from B. 
Theorem 2.6.13 Connectedness in K. A set in K is connected if and only if it is an 
interval. 
Proof. The empty set is (trivially) an interval and (again trivially) a connected set. 
Now assume that C is a nonempty connected set. We will show that C is an interval. 
Let a, b e C and a < c < b. Let A = C n (-00, c] and B = C O [c, 00). Then 
a £ A and b e B. Hence A and B are both nonempty and C = A U B. Therefore, 
C n A n B is nonempty. But by Corollary 2.6.10, A C\ B may contain only c. 
Therefore c G C. This shows that C is an interval. 
Conversely, assume that C is a nonempty interval. Let P and Q be two nonempty 
disjoint sets and C = PUQ. Assume a e P,b e Q, anda < b. Note that [a, b] C C, 
since C is an interval and a, 6 G C. Let A = Pn[a, 6] andß = Qd[a, b]. Lemma 
2.6.11 shows that [a, b] n A n -B is nonempty. But we see easily that A <z P and 
B c Q. Hence C D P n Q is also nonempty. 
O 
Closed Sets and Open Sets in R 
Definition 2.6.14 Closed sets. A set C in M is called a closed set if C contains 
the limits of all convergent sequences in C. More explicitly, C is closed if x G C 
whenever there is a sequence xn in C such that limn xn = x. 
Recall (Definition 2.6.17) that the closure A of A consists of all limit points of 
convergent sequences in A. 
Theorem 2.6.15 Closures and closed sets. Let A c R . Then the closure A of A is 
the smallest closed set that contains A. That is: 
(1) The closure A of A is a closed set and Ad 
A. 
(2) If C is a closed set and ifAcC, 
then A C C. 
Proof. (1) First, we prove that A is closed. Let yn G A, n G N, be a sequence in A. 
Assume that limn yn = y. We show that y € A. For each n G N there is an xn G A 

5 8 
REAL NUMBERS 
such that \yn — xn\ < 1/n. This follows from the fact that yn € A is the limit of a 
sequence in A. Then (yn — xn) is a zero sequence. So by Theorem 2.3.10, xn is also 
convergent and lim yn — lim xn = y. Hence y G A and therefore A is closed. Also, 
clearly, A C A In fact, every x E A is the limit of the constant sequence xn = x in 
A. 
(2) Let C be a closed set and A c C. If x £ A, then there is a sequence £„ in /I such 
that xn —» x. But xn is also a sequence in C. Therefore x £ C since C is closed. 
This shows that ~ÄC C. 
□ 
Corollary 2.6.16 A set A in R is closed if and only if A = A. 
Proof. We always have that A is closed and A c A, by Part (1) of Theorem 2.6.15 
above. Hence, if A = A, then A is closed. Conversely, if A is closed, then, by Part 
(2) of the same theorem, A C A. Since the other inclusion is always true, we obtain 
A = Ä. 
D 
Recall that Ac = R \ A is the complement of A. 
Definition 2.6.17 Boundary of a set. Let A be a set in R. Then dA - ~Ä n ~A¿ ¡s 
defined as the boundary of A. Points in dA are the boundary points of A. Since 
(Ac)c = A, we see that <9A = dAc. 
Definition 2.6.18 Open sets. A set G in K is said to be an open set if its complement 
Gc = R \ G is a closed set. 
Remarks 2.6.19 Topology of K. The family of open sets in K is called the topology 
of R. Informally, the topology of R refers to the collection of results about open 
sets, closed sets, and boundaries. In Chapter 4, on normed vector spaces, we will 
discuss the topology of a normed vector space, which includes the topology of K as 
a special case. For now, we limit our exposition to the basic definitions and results 
just presented. Some further results are stated as problems. 
Problems 
2.53 
Let E be a finite subset of R. Show that ~E = E. 
2.54 
Let E = { 1 - 1/n | n € N }. Find E. 
2.55 
What is <9Q? 

INTERVALS AND CONNECTED SETS 
5 9 
2.56 
Let A, B be nonempty connected subsets of R such that AnB 
— 0. 
IsAuB 
connected? 
2.57 
Show that an intersection of finitely many closed sets in R is closed. 
2.58 
Prove or disprove: If A, B are subsets of R, then d(A U ß ) = 3AU dB. 
2.59 
Show that the intersection of any family of intervals is again an interval. Also 
show that the intersection of any family of closed intervals is again a closed interval. 
Give examples to show that the intersection of a family of open intervals can be any 
type of interval (open, closed, or half-open). 
2.60 
Nested intervals. A sequence of intervals /„ is called nested if J n + 1 c In 
for all n € N. Let In be a nested sequence of intervals. If I\ is bounded and if each 
In is closed and nonempty, then show that the intersection of this family of intervals 
is also nonempty. Give an example to show that a nested sequence of bounded and 
nonempty intervals may have an empty intersection (so that the result only applies to 
sequences of closed intervals). 
2.61 
Let In be a sequence of bounded and closed intervals. Assume that the inter-
section njLj/fc is nonempty for each k € N. Show that r\ne^In is also nonempty. 
2.62 
Bisection sequences. Let/„ = [o„, bn] be a sequence of closed and bounded 
intervals with the middle points cn = (an + bn)/2. Then In is called a bisection 
sequence of intervals if In+\ = [an, c„] or/ n +i = [cn, bn] foreachn G N. 
(1) Show that the intersection Dn^^In of a bisection sequence of intervals contains 
exactly one point r € R. 
(2) Show that a bisection sequence is determined by the first interval I\ and by 
a sequence bn such that bn = 0 or bn = 1 for each n £ N and such that 
In+i 
- [an, Cn] if and only if bn = 0 and In+1 = [cn, bn] if and only if 
bn = l-
(3) If I\ = [0, 1 ], then show that the sequence bn obtained in Part (2) is a binary 
expansion for the number r € R obtained in Part (1). 
2.63 
Let I be an interval. Let A c R . If I contains points both from A and from 
its complement Ac = R \ A, then show that I also contains points from the boundary 
of A 

This page intentionally left blank

CHAPTER 3 
VECTOR FUNCTIONS 
We assume that the reader has a working knowledge of vector spaces. The first 
section of the chapter reviews basic results and notation. In particular, this initial 
section summarizes the essential facts about linear functions between vector spaces. 
These functions are essential tools for analysis in vector spaces. Much of this material 
is treated in an elementary course in linear algebra, although an important exception 
is the dimension theorem 3.1.14 (proved in Appendix B). 
After this review, we introduce first bilinear and then multilinear functions, which 
operate on Cartesian products of vector spaces. As far as possible, the section 
on multilinear functions is structured to parallel the preceding section on the special 
(and simpler) case of bilinear functions. An understanding of bilinear and multilinear 
functions is also crucial for analysis. In particular, polynomials in vector variables are 
defined in terms of multilinear functions. As we shall see in later chapters, the subject 
of differential calculus is functions that can be approximated by such polynomials. 
The last two sections of the chapter review the most important facts about Euclidean 
spaces, orthogonal projections, and linear transformations between Euclidean spaces. 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
61 

62 
VECTOR FUNCTIONS 
3.1 
VECTOR SPACES: THE BASICS 
A vector space X is a nonempty set with two operations, scalar multiplication and 
vector addition. Scalar multiplication is a function R x X —> X written (a, x) —> ax. 
Vector addition is a function X x X —* X written (x, x') —> x + x'. The two 
operations satisfy a set of conditions known as the vector space axioms (omitted 
here). Elements of a vector space are called vectors. The zero vector of X is denoted 
as Ox or simply as 0 if X is understood. In general, we denote vectors by boldface 
letters like x. If X and Y are two vector spaces and if / : X —> Y is a function, then 
the value of / at x G X is denoted by /(x) G Y. Boldface letters are not used for 
the vector/(x). 
Example 3.1.1 Let D be any nonempty set and let X be any vector space. Let 
F(D, X) be the set of all functions from D into X. If /, g are in f(D, X) and t is 
any scalar, define / + g and tf as functions from D to X by the following formulas: 
for all d e D, 
(f + g)(d) 
= 
f{d)+g{d) 
(i/)(d) 
= 
tf(d). 
In the first equation above, the " + " on the right-hand side is the addition operation 
in the vector space X. Thus, /(d) + g(d) is the sum of the vectors /(d) and g{d) in 
X. Similarly, tf(d) is the scalar multiple of f(d) by t. 
Thus, / + g and tf are in T(D, X). The zero vector of T{D, X) is the function 
that maps each d in D to 0, the zero vector in X. It's easy to check that F(D, X), 
together with these operations, is a vector space. In particular, T(D, R") is a vector 
space. 
A 
Spans, Subspaces, Linear Independence, Bases 
Definition 3.1.2 Linear combinations. Let A — { a i ; ..., a n} c X be a finite 
nonempty set of vectors. A linear combination of { ai, ..., ara } is any vector of the 
form riai + • • • + r„a„, where r\, . . . , r „ G l . 
Definition 3.1.3 Span of a finite set. Let A = { ai, ..., a n} c X be a finite 
nonempty set of vectors. The span of A is defined as the set of all linear combinations 
of vectors in A: 
Span A = { ri&i -\ 
V rnan | n, ..., rn G R } . 
We define the span of the empty set as Span 0 = {0}, the set consisting of the zero 
vector alone. 

VECTOR SPACES: THE BASICS 
63 
Definition 3.1.4 Subspaces. A set U in a vector space X is called a subspace if it 
is nonempty and if it is closed under linear combinations. This last condition means 
that if u, u' e U and r, r' G R, then ru + r'u' G t/. Note that 0 € [/ for any 
subspace U. Also, the set { 0} is a subspace. It is the smallest subspace in the sense 
that { 0} c U for any subspace U. 
Examples 3.1.5 
1. Lett/ = {(x1,x2,x3) 
G R3 | 2xi - \x2\ + x3 = 0 }. Clearly, (0,0,0) G [/. 
Ifx 6 [/ and t G R, then ix G Í7. However, Í7 it is not a subspace of R3 
because U is not closed under vector addition. For example, (0,1,1) and 
(0, -1,1) are in U, but their sum, (0,0,2), is not in U. 
2. Let U = {(xux2,x3) 
| |xi| > |x2x3| }. Then (0,0,0) e U. 
If x = 
(xi,x2,x 3) G C/ and t G R, then ix G C/ since |íxi¡ = |í| |xi| > |í| |x2x3| = 
IÍX2X3I. However, U is not a subspace of R3 since (1,0,1) and ( — 1,1,0) are 
in U , but their sum, (0,1,1), is not in U. 
3. Let n G N and let a\,..., 
an be any real numbers. Set 
U = { (xi,..., 
xn) I aixi H 
h anxn = 0 } . 
Then U is a subspace of Rn. 
4. Let X be the set of all functions from [—1,1] into R. As explained in Example 
3.1.1, this is a vector space. Let 
E = {/ G X I / ( - Í ) = f(t) foralU G [-1,1] } 
(3.1) 
and let 
O = { f G X I / ( - t ) = -/(<) for all t e [-1,1] } . 
(3.2) 
Then E and O are subspaces of X. For instance, to verify that E is a subspace 
of X, note that the zero vector of X (which is the function that takes each 
i s [—1,1] to 0) is in E. If /, g are in E and a, b are scalars, then 
(af + bg)(-t) = a(f(-t)) + b(g(-t)) = af(t) + bg(t) = (af + bg)(t). 
5. Let £> be a nonempty subset of M. A polynomial on D is a function / : D —> R 
for which there is some nonnegative integer n and real numbers ao,... ,an such 
that 
f(t) = a0+ ait-\ 
\-antn 
forall£G£>. 
Let V(D) denote the collection of all polynomials on D. Clearly, V(D) C 
T{D, R). It is easy to verify that V{D) is a subspace of T{D, R). 
A 

6 4 
VECTOR FUNCTIONS 
Example 3.1.6 Intersection of subspaces. Let X be a vector space. Then the 
intersection of a family of subspaces of X is also a subspace. 
To see this, let V be a collection of subspaces and set U = {\VeVV. Hence, U 
consists of all x G X such that x G V for each V £ V. Now 0 G V for each V &V. 
Therefore 0 G U. Let x, y G U and a, b G R. Then x, y G V for each V £ V. 
Therefore ax + by G V for each V e V . Hence ax + by G C/. 
A 
Remarks 3.1.7 The span of any set is a subspace. In fact, Span A is the smallest 
subspace that contains A. This means that if U is a subspace and if A c £/, then 
Span A 
cU. 
Definition 3.1.8 Sums of subspaces. Let U and VA be two subspaces in X. Then the 
sum of U and V is the set of all vectors of the form u + v with u G U and v E V . 
We denote this set by U + V. It is another subspace of X. 
Definition 3.1.9 Linear independence. A finite set A = {ai,..., a„} of distinct 
vectors is linearly independent if whenever r\,..., rn are real numbers and 
riai H 
hr„a„ = 0, 
then n = ... = r n = 0. This is equivalent to saying that each vector in the span of 
A is a unique linear combination of vectors in A. 
Definition 3.1.10 Bases. A linearly independent set A is called a (finite) basis 
for Span A. In particular, if B = { ei, ..., e n} is linearly independent and if 
Span B = X, then B is called a basis for X. Equivalently, B is a basis for X if each 
x G X is a unique linear combination of vectors in B. 
Example 3.1.11 Standard basis for M". The set Kn consisting of all n-tuples of 
real numbers (xi, ..., xn) is a vector space with the usual definitions of linear 
operations. The set E = { e\, .... en} consisting of n vectors 
ei = (1, 0, ..., 0),e2 = (0, 1, ..., 0),...,e n = (0, 0, ..., 1) is a basis for Rn. 
It is called the standard basis ofW1. 
Examples 3.1.12 Bases for some vector spaces. 
1. Each zero vector space {0} has 0 as a basis. 

VECTOR SPACES: THE BASICS 
65 
2. Let W = {(x,y,z) 
£R3 \x + y - z = 0}. Then W is a subspace of R3. 
Let us find a basis for W. Let v = (x, y. z) e R3. Then v e l f i f and only if 
x = —y + z. Thus, 
W 
= 
{ (— V + ZJ J/>z) I i/> -2 a10 arbitrary real numbers } 
= 
{ —j/(l, 1, 0) + z(l, 0,1) | y, z are arbitrary real numbers } 
= 
Span {(-1,1,0), (1,0,1)}. 
It's easy to verify that E = {(-1,1,0), (1,0,1)} is a linearly independent set; 
hence, it is a basis for W. 
3. Let E = {(1,1,2,1), (2,-1,0,4), (4,1,4,6)} and let W = Span E. Ob-
viously, E satisfies the first condition for being a basis for W. But E is not 
linearly independent since 
2(1,1, 2,1) +(2,-1,0,4)-(4,1,4,6) = 0 . 
Hence, we cannot conclude that E is a basis for W. However, it can be shown 
that the smaller set {(1,1, 2,1), (2, -1,0,4)} is a basis for W. A 
Remarks 3.1.13 Finite dimensional vector spaces. A vector space may or may 
not have a basis in the sense of Definition 3.1.10 above. Unless otherwise stated, we 
consider only vector spaces with a (finite) basis. These spaces are also called finite 
dimensional vector spaces. Hence, in this course, by a vector space we mean a finite 
dimensional vector spaceunless otherwise stated. The following is a major theorem 
about vector spaces. 
Theorem 3.1.14 The dimension theorem. Any two bases for a vector space contain 
the same number of vectors. 
See Appendix B for a proof of this theorem. 
Definition 3.1.15 The dimension of a vector space. The number of vectors in a 
basis for X is called the dimension of X. This number is denoted by dim X. The 
dimension of X is independent of the choice of basis because of the dimension 
theorem 3.1.14 above. 
Examples 3.1.16 
1. Since 0 is a basis for {0}, we have dim{0} = 0. 
2. Example 3.1.11 shows that R™ has a basis consisting of n vectors. Hence, 
dimRra =n. 

66 
VECTOR FUNCTIONS 
3. Let n £ N and let Vn be the vector space of all polynomials on R of degree 
no more than n. Let pk{x) = xk for all x 6 K and all k = 0 , 1 , . . . , n. Then 
{po, • • • ,Pn} is a basis for Vn. Hence, 
dim Vn = n + I. 
Direct Sums and Complementary Subspaces 
Lemma 3.1.17 Let U\, ..., Uk be subspaces ofX. Then the following are equiva-
lent. 
(1) For each x £ l there are unique u¿ G C/¿ swc/? í/zaf x = Uj + • • • + Ufe. 
(2) Tjf^i = { a\, ..., a^.} is a basis for Ui, then A = ök
=1Ai is a basis for X. 
Proof. This is left as an exercise. 
□ 
Definition 3.1.18 Direct sums. Let Ui, i = 1, .... k, be subspaces of X. Then 
X is said to be the direct sum of U\, ..., Uk if the conditions of Lemma 3.1.17 
above are satisfied. This is expressed as X = U\ © • • • © Uk- Note that in this case 
dimX = (dimCi) H 
h (dimi/fc). 
Definition 3.1.19 Complementary subspaces. A set of subspaces í/¿ is called a 
complementary set of subspaces in X if they satisfy the conditions in Lemma 3.1.17 
above or, equivalently, if X = U\ © • • • ffi Uk-
Lemma 3.1.20 The subspaces U and V in X are complementary if and only if 
X = U + V andUilV 
= { 0}. 
Proof. This is left as an exercise. 
D 
Examples 3.1.21 
1. Let X be any vector space. Then {0} and X are always complementary 
subspaces in X. 
2. Let U = { (a + b, a + 2b, a + b) | a, b £ K } and let V = { (0, 0, c) | c e K }. 
Then U D V = {0} and for any x = (x\,X2,xz) 
G K3, we have 
x = (a + b, a + 26, a + b) + (0,0, c), 
where a = 2x\ — X2, b = X2 — X\, c = x¡ — x\. Hence, M3 = U + V. Thus, 
K3 = U © V. So, U and V are complementary subspaces in K3. 

VECTOR SPACES: THE BASICS 
67 
3. Let U = { (a, b, 0,0) | a, b G R } and let V = { (0,0,0, c) | c G R }. Then U 
and F are complementary subspaces in{(x,y,0,z) 
\x,y,z 
&M.}. However, 
U and V are not complementary subspaces in K4 because U + V ^í R4. 
4. Consider X, £* and O as in part 4 of Examples 3.1.5. Let us see that 
X = E®0. 
(3.3) 
First, let / e X . Define /x(x) = f(x) + f(-x) 
and /2(x) = f(x) - 
f(-x) 
for all x G [-1,1]. Then /i G £ and / 2 G O. Thus, 
Hence, we have shown that X = E + O. Also, if ft 6 £ n O, then for 
all x G [—1,1], we have —h{x) = h(—x) — h{x) so that h(x) = 0 for all 
x G [—1,1]. Thus, E HO contains only the zero vector of X. So, (3.3) holds. 
A 
Cartesian Products and Direct Sums 
Definition 3.1.22 Cartesian products. The Cartesian product of k vector spaces 
U\, ..., C/fc is defined as a new vector space X = U\ x • • • x Uk■ The elements of 
X are the fc-tuples x = (ui, ..., u¿) of vectors u¿ G C/¿. Linear operations on these 
A;-tuples are defined as 
rx + sy = r(ui, ..., ufc) + s(v1; ..., vfc) = (rux + sv1; ..., rufe + svfc) 
for all r . s e t and for all x = (u1; ..., u^), y = (v1: ..., v^) G X. It is easy to 
check that X is a vector space with these operations. 
Remarks 3.1.23 Cartesian products and direct sums. For simplicity we consider 
only the products of two factors. Generalizations to more than two factors will be 
obvious. Let X = U x V and put 
U' 
= 
{u' = ( u , 0 ) | u e f / } c l , 
V' 
= 
{v' = (0, v) | v G F } 
CX. 
We see that each x G X has a unique representation as 
x = (u, v) — u' + v' 
with u' G U', v' G V. 
Hence X = U' ® V by Definition 3.1.18. This is the 
standard representation of a Cartesian product as a direct sum. 

68 
VECTOR FUNCTIONS 
Notation 3.1.24 Identification of direct sums and Cartesian products. Note that 
U and V above are separate vector spaces, but U' and V are the subspaces of 
X = U x V. We will usually ignore the difference between U and U' and between 
V and V. The meaning will be clear from the context. This amounts to the following. 
In a direct sum X = U © V, one can consider U and V as separate vector spaces. 
If this is done, then X becomes the Cartesian product space U x V. The difference 
between U ffi V and U x V is only notational. We will ignore this difference and 
treat these spaces as the same space. 
Linear Transformations 
Definition 3.1.25 Linear functions. Let X and Y be two vector spaces. A function 
T : X —> Y is said to be a linear function if for all u, v G X and for all r, s e l , 
T(ru + sv) = r-T(u) + sT(v). 
A linear function is also called a linear transformation, a linear operator, or a linear 
map - these terms are interchangeable. The values of a linear map T : X —» Y, 
denoted as T(x) or Tx, are vectors in Y. 
Definition 3.1.26 The vector space L(X, Y) of linear maps. Let L(X, Y) be the 
set of all linear functions T : X —> Y. Then L(X, Y) is itself a vector space. The 
linear operations on L(X, Y) are defined in a natural way. If S, T G L{X, Y) and 
if a, b G R, then i? = aS + bT is the function 
Rx = aSx + bTx for x G X. 
An easy verification shows that R : X —> F is also linear. Hence 
ñ = ( a S , + 6T) GL(X, Y). 
Another easy verification shows that these linear operations on L(X, Y) satisfy the 
axioms for a vector space. 
Examples 3.1.27 
1. The identity function I A ■ A —► A on any set A is defined by IA{O) = a for all 
a £ A. In particular, there is an identity function I\ ■ X —> X on any vector 
space X. It is easy to see that Ix '■ X —» X is actually a linear transformation. 
If X is understood from the context, then we simply write / instead of Ix-
2. Let T G L(X, Y) and assume that T is invertible: T one-to-one on X and 
T(X) = y. Let S : y -► X be the inverse of T. Thus, 5(y) = x if and only 

VECTOR SPACES: THE BASICS 
69 
if T(x) = y. Then S is also linear. To see this, let y i, y2 be in Y and let a, b 
be scalars. Suppose that S(yi) = xi and S(y2) = x2. Then 
T(axi + 6x2) = aTxi + tx 2 = ayi + by2-
Hence, 
5(ayi + 6y2) = axx + 6x2 = aS (yi) + £>S(y2). 
This shows that S1 is linear. 
Definition 3.1.28 Range and the kernel of a linear transformation. Let T : X —> V 
be a linear transformation. Then the range and the kernel of T are defined as 
Range T 
= 
{ Tx | x G X } C Y and 
Ker T 
= 
{ x | Tx = 0 } C X. 
Lemma 3.1.29 Images of subspaces. The images and inverse images of subspaces 
under linear transformations are also subspaces. In particular, the range and the 
kernel ofT, 
Range T = T(X) 
and Ker T = T~l { 0}, 
are subspaces of Y and X, respectively. 
Proof. This is left as an exercise. 
□ 
Lemma 3.1.30 Let T : X —> Y be a linear map. IfT is one-to-one on a subspace 
U C X, then dimU = 
dimT{U). 
Proof. Assume that T is one-to-one on U. Then it is easy to show that T maps 
linearly independent sets in U to linearly independent sets in T{U). Hence T maps 
bases of U to bases of T(U). Hence we see that dim U = dim T(U). 
□ 
Lemma 3.1.31 Let T : X —> Y be a linear transformation. If a subspace U ofX is 
complementary to Ker T, then T is one-to-one on U and 
T{U) = T(X) = Range T. 
Proof. Let u 6 U and Tu = 0; that is, u G U n (Ker T). Then u = 0, since U and 
Ker T are complementary. From this we see that 
if u, u' G U and if Tu = Tu', then u = u' 

7 0 
VECTOR FUNCTIONS 
by applying the preceding observation to u - u'. Hence T is one-to-one on U. 
Also, any x can be expressed as x = u + v with u 6 U and v e Ker T, since 
X = U © (Ker T). Hence Tx = Tu. This shows that T(X) 
c T(U). 
But 
T([/) c T(X), since l / C l Therefore T([/) = T(X). 
D 
Theorem 3.1.32 Le/ T : X —» Y be a linear transformation. Then 
dim (Range T) + dim(Ker T) — dimX. 
Proof. Let U be a complementary subspace to Ker T. Hence 
dim U + dim(Ker T) = dim X. 
Now T is one-to-one on U and T(U) = T(X) = Range T by Lemma 3.1.31. 
Lemma 3.1.30 shows that dimU = dimT(£/)- Then the result follows. 
O 
Examples 3.1.33 
1. Suppose that T : K3 -> R5 is linear and T(l, 1,1) = 0 = T(2,1,0). Then 
at least one of ui = (1,1,1,1,1) or u2 = (3,1,1,0,2) is not in T(M3). This 
is because (1,1,1) and (2,1,0) are in Ker T, so dim Ker T > 2. Hence, 
dimT(R3) < 1, so ui, u2 cannot both belong to T(R3). 
2. Let T : X —> R be linear, where dim X = n for some n G N. Then either 
T = 0 or dim Ker T = n - 1. To see this, suppose that T ^ 0. Then Tx ^ 0 
for some x e X . Hence, 
1 <dim(T(X)) <dimR = 1. 
Thus, dim(T(X)) = 1. So, dim(Ker T) = n - 1. 
3. Let X, Y be finite-dimensional vector spaces and let T : X —► Y be linear. 
Then 
dim Ker T = dim X - dim T(X) > dim X - dim F. 
Hence, if dim X > dim F, then Ker T ^ {0}. It follows easily that T cannot 
be one-to-one on X. 
Remarks 3.1.34 Linear transformations and bases. Let X and F be two vector 
spaces with the bases 
A 
= 
{ a i , . . . , a „ } 
and 
B 
= 
{ b i , . . . , b m } , 

VECTOR SPACES: THE BASICS 
71 
respectively. Any linear map T : X —> Y is uniquely determined by its values 
c¿ = Ta¿ G Y on the basis vectors. In fact, if the values c¿ are known, then 
Tx 
= 
T{xia.i H 
hx na„) 
= 
xiTai H 
h £nTara 
is determined for all x 6 X. Here, the Xi s are the coordinates of x with respect to 
the basis A. There is no restriction on the c¿ s: they can be n arbitrary vectors in Y. 
In particular, referring to the basis vectors in A and B, for each pair (a¿, bj), define 
Tij 
:X^Yby 
_ ( bj 
if i = k 
for each k = 1, ..., n. Hence T¿j maps a¿ to hj and maps all other a^ to Oy. We see 
easily that the set of these nm maps is a basis for L{X, Y). Therefore we conclude 
that dim L{X, Y) = (dimX)(dimF). 
Remarks 3.1.35 Composition of linear transformations. Let R : X —> Y and 
S : Y —> Z be two linear transformations. Let us verify that the composition 
S-R = SR:X^Z 
(3.4) 
is a linear transformation. In fact, setting T = S o R, we have 
T(ax + a'x') 
= 
5(fi(ax + a'x')) = S(aRx + a'Rx') 
= 
aS(Rx) + a'S(Rx') 
= aTx + a'T-x! 
for all x, x' G X and for all a, a' G R. 
Forming the compositions of linear transformations is a kind of multiplication. This 
multiplication is associative, in the sense that R(ST) = (RS)T, but not necessarily 
commutative. In general, RS ^ SR. If T : X —» X, then one can form powers Tn 
of T. They are defined inductively by T1 = T, T2 = T ■ T, Tn+1 
=T-Tn. 
Projections 
Projections are an especially simple, but important, category of linear transformations. 
A close connection between projections and direct sums (or Cartesian products) is 
brought out in Theorem 3.1.37. 
Definition 3.1.36 Projections. A linear map P ■ X —> X is called a projection if 
p2 = pp 
= 
R 

7 2 
VECTOR FUNCTIONS 
Projections are closely connected with complementary subspaces. 
Theorem 3.1.37 Let Ui be a set of complementary subspaces of X. Then there is a 
set of projections Pi : X —> X such that P^X — Ui and such that ^ ¿ P¿ = /, the 
identity on X 
Proof. Let Ui s be k complementary subspaces of X. Then for each X É X there are k 
uniquely defined vectors u¿ G t/¿ such that x = J2i u¿- This follows from Definition 
3.1.19 of complementary subspaces. Hence there are k functions P¿ : X —► X such 
that P¿x = u¿. An easy check shows that each P¿ is a linear transformation and 
Pf = Pi, that is, PjUj = u¿. Hence each P¿ is a projection. Also, J^¿ P¿ = / since 
X)jP¿x = J2iui = x for each x G X. 
D 
The converse is also true. Let P¿ s be a finite set of projections X —> X such 
that J^¿ P¿ = /. Then their ranges C/¿ = PiX constitute a set of complementary 
subspaces of X. This is left as an exercise. We will consider only the following 
special case. 
Theorem 3.1.38 If P : X —* X is a projection, then Q = (I — P) is also a 
projection. The ranges U = PX and V = QX are complementary subspaces of X. 
Also, U = Ker Q and V = Ker P. 
Proof. If P is a projection, then P2 = P and therefore 
(/ - Pf = (J _ P)(I - P) = I - 2P + P 2 = (/ - P). 
Hence Q = (I — P) is also a projection. Here we have used some composition rules, 
such as (R + S)T = RT + ST. These identities have easy verifications. Also, 
PQ = P(I - P) = P - p2 = P - P = 0. 
Now 
x = (P + Q)x = Px + Qx = u + v with u G U and v e V 
for any x e X. Also, if x = u' + v' with u' e U and v' G l7, then u' = Pa and 
v' = Qb for some a, b G X. Therefore 
u = P(x) = P(Pa + Qb) = P 2a + PQa = Pa = u'. 
Similarly, v' = v. This shows that the representation 
x — u + v with u G U and v G V 

VECTOR SPACES: THE BASICS 
7 3 
is unique. Therefore U and V are complementary subspaces. Finally, Px = 0 if and 
only if x = Qx = v e V. Hence Ker P = V = QX. Also, Ker Q = U = PX. 
D 
Definition 3.1.39 Complementary projections. If P : X —» X is a projection, 
then Q = I — P is also a projection by Theorem 3.1.38. The projections P and Q 
and are called complementary projections. Note that if P and Q are complementary 
projections, then their ranges U — PX and V = QX are complementary subspaces 
in A". 
Remarks 3.1.40 Projections on a subspace. Let P : X —> X be a projection. If 
[/ = PX is the range of P, then we denote P also as P : X —> [/ and call it a 
projection on [/. Note that a subspace t/ does not define a projection. If U ^ X, 
then there are many projections of X on {/. A projection on U is specified if one also 
chooses a subspace V which is complementary to U. In this case there is a unique 
projection P : X -> {/ such that 1/ = Ker P. Also, Q = (7 - P) : X -> V is the 
projection on V corresponding to the choice of U as a complementary subspace to 
V. 
Example 3.1.41 Let X = R3 be the xyz-space. Let U be the a;y-plane. Let V be 
the line spanned by the vector (1, 1, 1) 6 X. Show that U and V are complementary 
subspaces. What are the associated projections P and Q? 
Solution. A general vector in U is of the form u = (ui, u2, 0) with iii, it2 £ K. A 
general vector in V is of the form v = (v, v, v) with » e l . If w € U n V, then 
w = (ui, W2, 0) = (u, u, u). Hence v = 0 and w = 0. To obtain the associated 
projections, we find the unique decomposition of w = (x, y, z) as w — u + v with 
u € U and v E V . Setting 
(x, y, z) = (m, ii2, 0) + (u, w, u) 
gives v = z, u\ =x — v = x — z,U2 =y — v = y — z. Therefore 
P(x, y, z) = (x- z,y- 
z, 0) and Q(x, y, z) = (z, z, z) 
define the associated projections P and Q. We check that we have indeed P 2 = P, 
Q2 =Q,P 
+ Q = I, and Range P = U, Range Q = V. 
A 
Coordinate Systems 
Definition 3.1.42 Coordinate systems. Let U and y be a pair of complementary 
subspaces of X. Then ([/, V) is called a coordinate system in X. The projections 

7 4 
VECTOR FUNCTIONS 
P : X —^ U and Q : X —> V are the coordinate projections of this coordinate 
system. In this case X = U x V. Any x e J i s represented as 
x = (Px, Qx) = (u, v) 
in terms of its coordinates Px = u € £7 and Qx = v g V in this system. The 
generalization to more than two components is obvious. If X = U\ x • • • x Uk, then 
(Ui, ..., Uk) is a coordinate system in X with the corresponding set of coordinate 
projections P¿ : X —> C/¿. 
Figure 3.1. Complementary projections P and Q. 
Definition 3.1.43 Coordinate functions of a basis. Let X be a vector space with 
the basis A = { ai, ..., a„}. Let C/¿ be the one-dimensional space spanned by 
a¿. Then the [/, s form a set of complementary subspaces. The coordinate system 
(U\, .... Un) is called the coordinate system defined by the basis A. In this case, 
the coordinate projections P, : X —» í/¿ are projections on one-dimensional spaces. 
They can be represented by functions x¿ : X —> K such that P¿x = a;¿(x)a,. This 
gives, as before, the unique expression for x e l , 
x = xi(x)ai H 
h xn(x)ar, 
(3.5) 
as a linear combination of basis vectors. The functions Xi : X —> K are called the 
coordinate functions (of the basis A). Note that x¿(aj) = 0 if ¿ 7^ j and x¿(a¿) = 1. 
We see that the earlier form 
Xiai + • • • + a,'nan 
(3.6) 

VECTOR SPACES: THE BASICS 
75 
was not quite precise, as the coefficients are actually functions x¿ : X —> Rand should 
not have been confused with their values x¿(x). Nevertheless, we will continue to use 
the form (3.6) for convenience. Also, an easy verification shows that x¿ € L(X, R). 
Remarks 3.1.44 Coordinate functions as a basis for L(X, R). Coordinate func-
tions themselves are vectors in the vector space L(X, R) (even though they are not 
written in boldface letters). We show that they are linearly independent. Suppose 
/ = nxi H 
h rnxn = 0. 
(3.7) 
Here the meaning is that / is the 0-function, i.e., /(x) = 0 for all x G X. In 
particular, /(a¿) = r¿ = 0, since £j(a¿) = 0 if j ^ i. Hence Xi s are linearly 
independent in the vector space L(X, R). They also form a basis for L(X, R). In 
fact, let / G L(X, R) be arbitrary. Let /(a¿) = r¿. Then 
/(x) 
= 
/(a;i(x)a1H 
+x n(x)a n) 
= 
a;i(x)/(ai)H 
\-xn(x)f(an) 
= 
riXi(x)H 
hr nx n(x) 
for all x £ l . This means that / = r\X\ + • • • rn 
linear combination in the 
vector space L(X, R). This shows that the set 
A* ={xi, 
...,xn}cL{X,R) 
is linearly independent and spans L(X, R). Hence it is a basis for L(X, R). 
Isomorphic Spaces 
Definition 3.1.45 Isomorphisms. A linear map T : X —» Y is called an isomor-
phism from X to Y if it is one-to-one and onto, i.e., T(X) = Y. Equivalently, an 
isomorphism is an invertible linear map from X to Y. 
An easy check shows that 
the inverse map T~l : Y —> X is also an invertible linear map. 
Example 3.1.46 Assume that dimX = n = dimF for some n € N. Let T : X —> 
V be linear. Then T is one-to-one on X if and only if T is an isomorphism. To 
see this, assume that T is one-to-one on X. Then Ker T = {0}. Hence, by the 
dimension theorem, dim X = dim T(X). Thus, dim Y = dim(T(X)). Since T(X) 
is a subspace of Y, we have T(X) = Y. Conversely, if T is an isomorphism, then of 
course, T is one-to-one. 
Definition 3.1.47 Isomorphic spaces. Two vector spaces X and Y are called iso-
morphic if there is an isomorphism T : X —> Y from X to Y. The fact that X and y 

7 6 
VECTOR FUNCTIONS 
are isomorphic spaces is expressed as X ~ Y. Being isomorphic is an equivalence 
relation between vector spaces. 
The next two results establish that for two vector spaces to be isomorphic amounts 
to having the same dimension. 
Lemma 3.1.48 /f dim X = n, then X is isomorphic to Rn. 
Proof. Let A = { au ..., a„} be a basis for X. Define T : X -> R™ by 
T(x) = (x!(x), . . . , x n ( x ) ) e R " 
for x € X. Here Xi : X —> R are the coordinate functions of the basis A. We see 
easily that T is an isomorphism. Let E = { ei, ..., e„} be the standard basis of 
K", as in Definition 3.1.11. Then T takes a¿ e X to e¿ G Mn. 
G 
Theorem 3.1.49 Two (finite dimensional) vector spaces X and Y are isomorphic if 
and only if dim X = dim Y. 
Proof. Assume that X and Y are isomorphic. Let T : X —> F be an isomorphism 
from X to Y. Then T is one-to-one on X and T(X) = Y. Therefore, by Lemma 
3.1.30, dim X = dimT(X) = dimF. Conversely, assume that dimX = dim y = 
n. Then X and Y are both isomorphic to Rn. 
Q 
Matrices 
Definition 3.1.50 Matrices. Let TO, n e N. An m x n matrix A is a function 
A : {1, ..., TO} x {1, ..., n} -+R 
that takes the pair (i, j) S {1, ..., TO} X {1, ..., n} to the number Aij 6 R. The 
numbers Aij are called the entries or components or coordinates of the matrix A. 
Such a matrix is defined by arranging its entries as a table 
An 
... 
Aln 
/ i m i 
. . . 
. r i m n 
consisting of TO rows and n columns. We also denote a matrix A in terms of its 
entries as { A¿j}, or simply as A^, if the meaning is clear from the context. 

VECTOR SPACES: THE BASICS 
77 
Definition 3.1.51 The vector space Mmra. Let Mm„ be the set of all m x n matrices. 
Then M m n is a vector space under the usual (componentwise) definition of linear 
operations. The set of matrices with all components equal to 0 except one component 
equal to 1 forms a basis for M m n. Since this set contains van matrices, we obtain 
dim(Mmn) = ran. 
Another way of seeing this is as follows. There is an obvious isomorphism between 
M m n and M.mn. In fact, both spaces are the set of all functions F —> R, where F is a 
finite set of ran elements. In each case, the set of all functions F —» R that take the 
value 1 at one point and vanish at all other points forms a basis. Hence we see that 
the difference between Mm„ and R m" is only notational. 
Remarks 3.1.52 Matrices and linear transformations. 
Let dim X = n and 
dim y = TO. Then we see that Mm„ and L(X, Y) are of the same dimension. 
Therefore they are isomorphic spaces. There is a standard isomorphism between 
L(Rn, Rm) and M m„. Given a matrix T = { TV,} e M„m, define a transformation 
T : Rn -> Rm as follows. If 
T(xx, ..., xn) = (yi, ..., ym), 
then y i — 5Z?=i ^¿j xj f°r alH = 1, ..., TO. 
There is one further basic connection between matrices and linear transformations: 
composition of linear transformations corresponds to matrix multiplication. We shall 
establish this in the next section. 
Problems 
3.1 
Let T : X —> Y be a linear map. Let U be a subspace of X. Show that if 
dim U = dim T(U), then T is one-to-one on U. 
3.2 
Let X b e a vector space and let x g l . Show that for all scalars a, b, we have 
(a — £>)x = ax — fox. 
3.3 
Let X be a vector space. Show that if x G X is nonzero and s, t are distinct 
scalars, then sx ^ fx. 
3.4 
Let Xi,..., x n be vectors in a vector space X. Assume that u, v are linear 
combinations of x i , . . . , x„. Show that any linear combination of u and v is also a 
linear combination of x i , . . . , x n. 

78 
VECTOR FUNCTIONS 
3.5 
Let u = (1. 2, 3), v - (2, 5, -4). Find all real numbers a such that (-2, a, 7) 
is a linear combination of u and v. 
3.6 
Let 
1
4 
3 " 
- 2 
0 
5 
7
-
1
3
' 
0 
3 
- 1 
Find a matrix X such that A - 3X = 2B. 
3.7 
Give an example of a nonempty subset A of R2 such that A + A ^ 2A. Find 
all subsets B of R2 with tB + sB C B for all scalars s and t. 
3.8 
Given any u and v in R3, show that 
R3 ^ { au + 6v | a £ R, b £ M } . 
(The solution is easy if one uses the results on dimensions: a three-dimensional space 
cannot be spanned by two vectors. Try to give a solution that uses only the basic 
definitions.) 
3.9 
For each a £ R, let Ua = { (x, y, z) \ a\x\ = x + y + z }. Find all a for 
which Ua is a subspace of R3. 
3.10 
Let U, V be subspaces of a vector space X. When is it true that U U V is also 
a subspace of XI 
3.11 
Let u i , . . . , u„ be vectors in a vector space X. Let 
!7 = {a = ( a i , . . . , a n ) e R " 
ami H 
H « A = 0 }, 
where 0 is the zero vector of X. Show that U is a subspace of R™. 
3.12 
Let W = { (x, y, z) £ R3 | 2x - z = y }. Show that W is a subspace of R3 
and find a basis for W. 
3.13 
Let A be a nonempty subset of a vector space X. Let t be a nonzero scalar. 
Is it true that Span (tA) = Span Al 
3.14 
Find a finite subset S of R4 such that Span S = A + B, where 
A 
= 
{ (x, y, z, t) £ R4 | x - y + t = 0 } and 
B 
= 
{(x, y, z, t) £ R4 | x + 4z - 2t = 0 } . 
- 2 
1 
3 
3 
0 
7 
11 
- 5 
9 
2 
2 
0 
B 

VECTOR SPACES: THE BASICS 
7 9 
3.15 
Let A and B be two subsets in a vector space X. Does Span A c Span B 
imply that A C B? 
3.16 
Find mutually disjoint subsets A, i?, and C of R2 such that 
WL2 = Span A = Span B = Span C. 
3.17 
Let A and £? be two subsets in a vector space X. Show that 
Span (A + B) C Span A + Span ¿?. 
3.18 
Let U, V be two subspaces of X such that dim U = dim V and (7 ^ V". 
1. Show that U + V ¿ U and U + V ¿ V. 
2. Show that if (dim U) = (dim V) = (dim X) - 1, then X = U + V. 
3.19 
Let T : K2 -> M3 be defined by 
T(x, y) = (x — y,y — 3x, x + |j/|) for all (x, y) £ K . 
Is T linear? 
3.20 
Is there a linear T : K3 -► M2 such that 
T(l,2,l) = (l,l), T(-2 10 )3) = (2,5), a n d T ( - 4 , - 4 , l ) = (-3,2)? 
3.21 
Let T e L{X, Y) and let y 0 G Y be such that 
{ x e X | T x = y 0 } 
is a subspace of X. Show that yo = 0. 
3.22 
Suppose that E c X and Span £ = X. Let T and S be in L(X, F). Show 
that T = S if and only if Tu = Su for all u e £ . 
3.23 
Let T, 5 be in L(X, Y). Let t/ = { x 6 X | Tx = Sx }. Show that U is a 
subspace of X. 
3.24 
Show that if X ~ Y and Y ~ Z, then X ~ Z, recalling that X - Y means 
that X is isomorphic to Y. 

8 0 
VECTOR FUNCTIONS 
3.25 
Let a G M. Consider the linear map T : K3 -»■ R3 given by 
T(x, y, z) = (x, ax + y, z) for all (a;, y, z) G R3. 
Show that T is invertible and find its inverse map T" 1 : M3 —> R3. 
3.26 
Let X and y be vector spaces and a G X. Define / : L(X, y) —► F by 
/(T) = Ta for all T € 
L(X,Y). 
Show that / is linear. 
3.27 
Let R : X -> Y and S : Y -► Z be isomorphisms. Show that SR : X -> Z 
is also an isomorphism and (SR)^1 
= R~1S~1 
: Z —> X. 
3.28 
Let (dim X) > 2. Give an example of a nonzero T e L(X, X) such that T2 
is the zero transformation. 
3.29 
Let X be a vector space. Let T e L(X, X) be non-invertible. Show that 
CT = 0 for a nonzero C € L(X, X). 
3.30 
Let T : R5 -> R5 be linear. Assume that whenever x e R5 and Tx = x, 
then x = 0. Show that for any y £ R5, there exists x G R5 with 
x = Tx + y. 
3.31 
Is there a linear map T : X —> X such that T is not one-to-one on X but 
Tk : X —> X is one-to-one for some k > 2 ? 
3.32 
Let A, B G L(X, X). If AB is invertible, show that both A and B are 
invertible. 
3.33 
Let T G L(X, y). Let U be a subspace of X such that Í7 n Ker T = {0}. 
Show that dim U = dimT(ÍJ). 
3.34 
Let T G L(X, Y). Let U be a subspace of X such that X = Í7 0 Ker T. 
Show that dim X = dim T(U) + dim Ker T. 
3.35 
Let X and Y be two vector spaces. Let W be a subspace of X. If 
dim y > (dimX) - 
{dimW), 
then show that there is a T G L(X, Y) such that W = Ker T. 

VECTOR SPACES: THE BASICS 
81 
3.36 
Let /, g e L{X, R). Show that Ker / C Ker g if and only if g = cf for 
some scalar c. 
3.37 
Let U, V be subspaces of a finite-dimensional vector space. Let W = 
{(-x,x) |x'ei/nv}. 
1. Show that W is a subspace of U x V and W is isomorphic to U n V. 
2. Define / : [/ x V -* {/ + F by /(u, v) = u + v for all (u, v) eU 
xV. 
Show that / is linear. Hence, deduce that 
dim([/ + V)= dimU + dimV - dim{U n V). 
3.38 
Let W and Z be two vector spaces with dim W = dim Z. Let X be a 
subspace of W and let T : X —> Z be a one-to-one linear map. Show that there is an 
isomorphism R : W —> Z such that the restriction of P to X is T. 
3.39 
Let X and Z be two vector spaces with (dim X) < (dim Z). Show that there 
is a vector space Y with the following property. Given any one-to-one linear map 
T : X —> Z, there is an isomorphism ñ : (X x F) -» Z such that the restriction of 
R to X is T. Here X is identified with the subspace of X x Y consisting of vectors 
of the form (x, 0) £ X x Y, with x e X . 
3.40 
Let W and Z be two vector spaces with dim W — dim Z. Let 5 : Z —> W 
be a linear map and let X = S(Z) be the range of S, which is a subspace of W. 
Show that there is an isomorphism L : Z —> W7 and a projection P : W —» W such 
that 5 = PL. 
3.41 
Let X and Z be two vector spaces with (dim X) < (dim Z). Show that there 
is a vector space Y with the following property. Given any linear map S : Z —> X 
that maps Z onto X (that is, T(Z) = X), there is an isomorphism L : Z —> (X x Y) 
such that S = PL, where P : (X x Y) —> X is the coordinate projection onto X. 
Recall that P(x, y) = (x, 0) for all (x, y) £ X x Y. Here X is again identified 
with a subspace of X x Y. 
3.42 
Let ([/, V) be a coordinate system in X, in the sense of Definition 3.1.42. A 
set T c X is a graph in this system if there is a set A c U and a function / : A —> V 
such that T is the set of points (a, /(a)) with a e A Let (Í7¡, y), ¿ = 1,2, be two 
coordinate systems for X. Show that T is a graph in one system if and only if it is a 
graph in the other system. 
3.43 
Let ([/, Vi), i = 1, 2, be two coordinate systems in X, in the sense of 
Definition 3.1.42. Consider the following proposition: a set T C X is a graph in 

8 2 
VECTOR FUNCTIONS 
one system if and only if it is a graph in the other system. Show either that this 
proposition is true or that it is false. 
3.2 
BILINEAR FUNCTIONS 
Ordinary multiplication is an operation that takes a pair of real numbers (a, b) to 
their product M(a, b) = ab. Multiplication is bilinear, for all a, b, c, r, s G R, 
M(ra + sb, c) 
= 
rM(a, c) + sM(b, c) and 
Af (a, rb + sc) 
= 
rM(a, b) + sM(a, c). 
Bilinearity deserves special attention, for it will play an important part in our discus-
sion of polynomials. 
Definition 3.2.1 Bilinear operations. Let X, Y, and Z be three vector spaces. A 
function B : X x Y —» Z is called a bilinear function (or a bilinear operation, or a 
bilinear map) if 
B(ax + a'-x!, y) 
= 
aß(x, y) + a'S(x', y) and 
B^ßy 
+ ß'y1) 
= 
ßB(x,y)+ß'B(x,y') 
for all x, x' G X, y, y' e Y, and a, a1, ß, ß' G R. 
Thus, a bilinear function can be considered as a function of two variables that is 
linear in each variable separately when the other variable is kept constant. 
Remarks 3.2.2 Linear and bilinear functions. The term bilinear may perhaps 
suggest that bilinear functions are some type of special linear function. This is not 
the case. Bilinear functions are completely different from linear functions. Consider, 
for example, the linear and bilinear functions from R x R = R2 to R. Here R 
is considered as a (one-dimensional) vector space. We see that a linear function 
L : R2 -> R is of the form 
L(x, y) = Ax + By, where A = L(l, 0) and B = L(0, 1) are two constants. 
On the other hand a bilinear function B : R2 —► R is of the form 
B(x, y) = Pxy, where P = B{\, 1) is a constant. 
This follows from the observation that 
B(x, y) = B(xl, y) = xB(l, y) = xB{\, y\) = xyB{\, 1). 

BILINEAR FUNCTIONS 
8 3 
Remarks 3.2.3 General form of bilinear functions. Let X and Y be vector spaces 
with bases A = { aj, ..., a„} and B = { bi, ..., bTO}, respectively. Then a 
function R : X x Y -^ Z isa bilinear function if and only if 
fi(x,y) = J2 E • ^ W y^y) Cij- 
(3-8) 
Here x¿ : X —> R and ?/, : Y" —► R are the coordinate functions with respect to 
the bases A and B, and ci2 = i?(a¿, bj) are arbitrary vectors in Z. Equation (3.8) 
follows by expressing x and y as linear combinations of a¿ and bj, respectively, and 
then expanding -R(x, y) using bilinearity. 
Example 3.2.4 Standard dot product. Define B : Rn x Rn -► R by 
B(x, y) = xm + ■■■ + xnyn 
for all (x, y) e R" x R". 
This function is the standard dot product on R". It is easy to verify that B : 
Rn x Rn -> R is a bilinear function. 
A 
Example 3.2.5 Bilinear functions (R3 x 1) -> 1. Denote the points in R3 as 
(x, y, z) and the points in R as t. Hence the points in R3 x R are ((x, y, z), t). Let 
A, B, C G R be arbitrary. An easy check shows that 
R((x, y, z), t) = Axt + Byt + Czt, 
((x, y, z), í ) e l 3 x R, 
defines a bilinear function R3 x R —> R. Also, this is the general form of a bilinear 
function R3 x R -> R. We see that 
fi((l, 0, 0), 1) = A, R{(0, 1, 0), 1) - B, ñ((0, 0, 1), 1) = C. 
A 
Example 3.2.6 Bilinear functions (R2 x R2) -> R. Denote the points in R2 x R2 
as ((x, y), (u, v)). Let A, B, C, D be arbitrary in R. An easy check shows that 
R{(x, y), (u, v)) — Axu + Byu + Cxv + Dyv, ((x, y), [u, v)) € M2 x R2, 
defines a bilinear function R2 x R2 —> R. Also, this is the general form of a bilinear 
function R2 x R2 -> R. We see that 
R((l, 0), (1, 0)) 
= 
A, 
R((0, 1), (1, 0)) 
= 
B, 
R((l, 0), (0, 1)) 
= 
C, 
ñ((0, 1), (0, 1)) 
= 
D. 
A 
Remarks 3.2.7 Dependence on the factorization. In Examples 3.2.5 and 3.2.6 
above, the domain space of the bilinear functions is the same space 
R4 = R3 x R = R2 x R2. 

8 4 
VECTOR FUNCTIONS 
However, the two classes of bilinear functions are different. 
Hence, bilinearity 
depends on the way of expressing the domain space as a Cartesian product. 
Definition 3.2.8 The space of bilinear functions. The set of all bilinear maps 
X x Y —> Z will be denoted by BL(X x Y. Z). An easy verification shows that 
BL(X x Y, Z) is actually a vector space itself, by the natural definitions of linear 
operations. The following lemma states this explicitly. 
Lemma 3.2.9 Let P and Q be two bilinear functions, and let r, s G M. Define 
(rP + sQ):XxY^Z 
as 
(rP + sQ){x, y) = rP(x, y) + sQ(x, y) 
for all (x, y) G X x Y. Then (rP + sQ) : X x Y —> Z is also a bilinear function. 
The set of all bilinear functions X x Y —> Z becomes a vector space BL(X, Y: Z) 
with these linear operations. 
Proof. This is left as an exercise. 
D 
Example 3.2.10 Composition of linear transformations. If R : X —> Y and 
S : Y —> Z are two linear transformations, then their composition 
C(R, S) = S-R = SR:X 
-► Z 
is another linear transformation. We see that the operation of composition itself, 
C : L{X, Y) x L(Y, Z) -► L(X, 
Z), 
is a bilinear operation. In fact, we have 
C(aR + a'R', 5)(x) 
= 
S((aR + a'R')(x)) = 5(añx + a'R'x.) 
= aS(Rx) + a'S(R'x) = (aSR + a'SR')(x) 
= 
{aC(R, S)+a'C{R', 
S))(x) 
for all x e l . Similarly, we can verify the linearity of C in its second factor. Thus, 
C is a bilinear operation L{X, Y) x L{Y, Z) -+ L{X, Z). 
A 
Matrix Multiplication 
Matrix multiplication is defined as follows. If A = { Aik] G Mm¿ is an m x Í 
matrix, and B = { ¿?/y } G M¿„ is an i x n matrix, then their product 
A/(A, B) = AB = C = { dj} G M m n 

BILINEAR FUNCTIONS 
85 
is the m x n matrix whose entry in row i and column j is given by C. 
H2k=iAikBkj- We see that this operation defines a bilinear function 
n 
M 
il X 
and therefore a product between the matrices. 
An easy check shows that matrix multiplication is associative. If A G M.me, B G 
Mgp, and C G Mpn, then A(BC) and (AB)C define the same matrix in Mmn, 
which we may consequently denote as ABC. Significantly, matrix multiplication 
is noncommutative. In fact, BA may not be defined, even if AB is defined. This 
happens, for instance, if A is a 2 x 3 matrix and B is a 3 x 5 matrix. 
Matrices can be used to represent linear maps, and a particularly simple kind of 
matrix multiplication can be used to represent the effect of applying a linear map to 
a vector. 
Example 3.2.11 Linear maps as matrix multiplication. There is a standard iso-
morphism between Mm„ and L(Rn, R m), as pointed out in Remarks 3.1.52. It 
takes the matrix T = { T¿i} e Mmn to the transformation T : R™ -> Rm defined 
as follows: ifT(xi, 
..., xn) = (yi, ..., ym), then y¿ = Yl]=i Tij xr 
T c a n b e 
expressed as matrix multiplication in the following way. Let Cn : R™ —* M n l take 
vectors in Rn to n x 1 matrices as follows: 
^ n \%\ ) • • • i •En) 
Xl 
for (xi, ..., xn) £ Rn. We usually call the matrices in M„i column vectors and 
ignore any differences between R™ and M„i. We denote the elements of both spaces 
by symbols like x and y. If T G Mmra, with corresponding operator T : Rn —> Rm, 
then y = Tx if and only if y = Tx, or, more explicitly, 
y = 
2/i 
Vr, 
T, 11 
T\n 
Tml 
Xi 
= Tx. 
Thus, any linear transformation can be represented as matrix multiplication. 
Example 3.2.12 Compositions as matrix multiplication. Let 
R € L(Rn, R£) and S G L(Re, Rm). 
Then their composition T = SR belongs to L(Rn, R m), as mentioned in Example 
3.2.10. Let the maps R, S, T correspond to the matrices 
Re 
„ s e 
e, and T G 

8 6 
VECTOR FUNCTIONS 
as above in Example 3.2.11. Then T = S R. In fact, by the associativity of matrix 
multiplication, 
Tx = S(Rx) = (SR)x. 
Second-degree Homogeneous Polynomials 
Second-degree real homogeneous polynomials in two variables have the form 
f(x, y) = Ax2 + 2Bxy + Cy2 with A, B, C £ R. 
As noted at the start of the chapter, this class of functions will have an important role 
later. This function is still defined if the coefficients are vectors from a vector space 
Y. Such polynomials are defined in terms of bilinear functions. 
Definition 3.2.13 Second-degree homogeneous polynomials. Let X and Y be two 
vector spaces. A second-degree homogeneous polynomial f : X —► Y is a function 
of the form /(x) = B(x, x), x f X , where B : X x X ^> Y is a bilinear function. 
Definition 3.2.14 Symmetric bilinear functions. If the bilinear function 
B.XxX^Y 
satisfies ß(u, v) = B(\, u) for all u, v E X, 
then it is called a symmetric bilinear function. If B : X x X —> Y is any bilinear 
function, then 
5 ( u , v ) = (l/2)(£?(u,v)+B(v,u)) 
is a symmetric bilinear function. It is called the symmetric part of B. Note that 
B(x, x) = ß(x, x) for all x G X. 
Hence a bilinear function and its symmetric part define the same second-degree 
homogeneous polynomial. 
Example 3.2.15 Second-degree homogeneous polynomials / : M2 —> K. A gen-
eral bilinear function B : K2 x R2 —> R is of the form 
B((x, y), (u, v)) = Axu + Byu + Cxv + Dyv, 
({x, y), (u, v)) E l 2 x R2, 
as shown in Example 3.2.6. Hence a general second-degree homogeneous polynomial 
/ : R2 -> K is of the from 
f(x, y) = B((x, y), (x, y)) = Ax2 + (B + C)xy + Dy2, 
(x, y) e R2. 

BILINEAR FUNCTIONS 
8 7 
Note that the symmetric part of B is 
B{(x, y), {u, v)) = Axu + (1/2)(B + C){xv + yu) + Dyv. 
Hence we see that the relation 
B((x, y), (x, y)) = B((x, y), {x, y)) = Ax2 + (B + C)xy + Dy2 
is verified. A 
Problems 
3.44 
Let X, Y, Z be three vector spaces. Show that the vector spaces 
BL{X, Y; Z), L(X, L{Y, Z)), and L(Y, L{X, Z)) 
are isomorphic to each other. 
3.45 
The cross product of two vectors is defined as 
(zi, 2/i, zi) x (x2, 2/2, z2) = (y%z2 - 2/2^1, zxx2 - x1z2, Xiy2 - 
y\x2). 
Show that the cross product, as a function K3 x I 3 -> R3, is a bilinear operation. 
Hence, the cross product is also a product in the sense defined here. 
3.46 
Show that B{{uu ..., un), (v1, ..., vm)) = E"=iEiLi M¿^' aíi' w i t h ar-
bitrary &ij 6 y, is a general bilinear function B : W1 x Rm —> Y. 
3.47 
Show that f(x\, 
..., xn) = Yl1i=i^2]=iXiXJai3 
' s a Seneral second-degree 
homogeneous polynomial / : Rn —> Y. Here a¿¿ s y are arbitrary. 
3.48 
Suppose that T : Jf x y —> Z is bilinear. Must its kernel 
W = { ( x , y ) e X x y | T ( x , y ) = 0 } 
be a subspace? What about the range of T? 
3.49 
Let X be a vector space with basis {xi,..., Xfc}. Then every bilinear map 
S : X x Rn -► Rm is of the form 
5(x, y) = cxl Aiy 
+ ■ ■ ■ + cxkAky 
for all y e K " , 
where Ai,..., 
Ak are m x n matrices and cxi,..., 
cxk are scalars such that 
x = CjiXi H 
\-cxkxk. 

8 8 
VECTOR FUNCTIONS 
3.50 
Let X, Y, and Z be vector spaces. Then both BL(X x Y, Z) and L(X x V, Z) 
are subspaces of T{X x Y,Z), the vector space of all functions from X x Y into 
Z. Here, L(X x Y, Z) is the subspace of all linear maps from X x Y to Z, where 
X x Y is equipped with the usual vector space structure. Show that 
BL(X xY,Z)n 
L(X xY,Z) 
= {0}. 
3.51 
Let T : X x Y -> Z be bilinear. Suppose that 
(dim F) (dim Z) <dimX. 
Show that there is a nonzero xo € X such that 
T ( x 0 , y ) = 0 
for all y 
eY. 
3.52 
Suppose that / is a homogeneous polynomial of degree 2 on M2 such that 
/(1,0) = l,/(0,1) = 1. What is the value of /(l, 1) so that f(x,y) 
> 0 for all 
{x, y) e K2? 
3.3 
MULTILINEAR FUNCTIONS 
Products between two vector spaces are defined in terms of bilinear functions. Prod-
ucts between finitely many vector spaces are expressed in terms of multilinear func-
tions. Before we define multilinear functions, we review the coordinate systems 
defined by Cartesian products, as described in Definition 3.1.42. 
Notation 3.3.1 Review of coordinate systems. Consider ({/i, ..., Uk) as a coor-
dinate system in X = U\ x ■ ■ ■ x Uk- Let P¿ : X —> Ui be the associated coordinate 
projections and let Q¿ = / — P¿. Here / : X —» X is the identity. Hence P¿ and Q¿ 
are complementary projections as in Definition 3.1.39. 
Remarks 3.3.2 Multilinearity as componentwise linearity. Let M : X —> Y be 
a function, where X = U\ x ■ ■ ■ x Uk- Multilinearity of this function is defined 
as its linearity in each component (or coordinate) taken separately, with all other 
components kept fixed. That is, 
M(aixi +a'1x[, x 2,...,x f c) = axM{y.i, x 2,...,x f c) +a[M(x'1, 
x 2,...,x f e) 
for all Xi, x'j G U\, x¿ € Í7¿, and a.\, a[ € K; and similarly for each of the other 
components. 

MULTILINEAR FUNCTIONS 
8 9 
To formulate this concisely, note the following. If a = (ai, ..., a.k) and x = 
(xi, .... xfc),then 
Q¿a + P¿x = (ai, .... x¿, ..., afc) 
is obtained from a by replacing only its zth component by the ith component of x. 
Therefore, if we let T¿(a) : X -> Y be the function T¿(a)(x) = M(Q¿a + P¿x), 
then the linearity of all of the functions T¿ (a) : X —> Y is equivalent to the linearity 
of M : X —> Y in each component separately, when all other components are kept 
constant. Hence we introduce the definition of multilinearity as follows. 
Definition 3.3.3 Multilinear functions. Let M : X —> Y be a function, where 
X — U\ x • • • x Uk- Let a £ X and let i = 1, ..., k be fixed. Let 
T¿(a)x = M(Q¿a+ P¿x) for x £ X 
Then M : X —> Y is called a multilinear (or k-linear) function if T¿ (a) : X —> y is 
a linear function for each a G X and for each ¿ = 1, ..., k. 
Example 3.3.4 Suppose that T : R2 x M x K -> K2 is multilinear. Given that 
T((1,0),1,1) = (2,3),T((0,1),1,1) = (5,-l),letusfindaformulaforT((a;,y),u,t;) 
where ((x,y),u,v) 
e R2 x K x M. 
Let ( ( i , i / ) , ! i , D ) e l 2 x l x l be arbitrary. Then 
T({x,y),u,v) 
= 
T{x(l,0)+y(0,l),u,v) 
= 
xT{{l,0),u,v)+yT((0,l),u,v) 
= 
xuT((l,0),l,v) 
+ 
yuT{{0,l),l,v) 
- 
xuvT({l, 0), 1,1) + yuvT((0,1), 
1,1) 
= 
xuv(2,3) + yuv(5, — 1) 
= 
((2x + 5y)uv, (3x — y)uv). 
Lemma 3.3.5 If M : X —> Y" is multilinear, then 
T¿(a)(x) = M(Q¿a+P¿x) = M(a + P,x) - M(a), x e X. 
(3.9) 
Proof. We have 
M(a + P,x) 
= 
M(Q ia + P i a + P i x ) = M(Qia + Pi(x + a)) (3.10) 
= 
M(Q¿a + Px) + M(Q¿a + P¿a) 
(3.11) 
= 
M(Qia + Pix) + M(a). 
(3.12) 
Hence (3.9) follows. Here (3.11) uses the linearity of T¿(a) : X -» y. 
□ 

90 
VECTOR FUNCTIONS 
Definition 3.3.6 Spaces of multilinear functions. Let Ui and Y be vector spaces. 
Let Xk = Uk x • • • x U\, k G N. (The £/¿ s are listed in descending order for 
notational convenience in the arguments below.) Let 
MLk{Uk x---xUí,Y) 
= MLk(Xk, 
Y) 
be the set of all /c-linear functions. We see easily that MLk {Xk, Y) is a vector space 
under the natural definitions of linear operations. That is, if A, B : Xk —> Y are 
multilinear functions and if r, s G R, then 
(rA + sJ3)(x) = T\4(X) + sB(x) where x = (ufc, ..., m) G Xk 
defines another multilinear function rA + sB : Xk —> Y. 
Definition 3.3.7 An identification of multiple products. There is a natural iso-
morphism between MLk+i(Xk+i, 
Y) and L(Uk+i, MLk(Xk, 
Y)). 
For each 
F G MLk+1(Xk+l, 
Y), defined € L(Uk+1, MLk(Xk, 
Y))by 
i?F(ufe+1)(ufc, ..., ui) = F(ufc+i, ufc, ..., uj). 
(3.13) 
Here &F is a linear function Uk+i -* MLk(Xk, 
Y). It maps Ufc+i G Uk+i to 
^^(ufc+i) G MLk{Xk, 
Y). Equation (3.13) defines tfi^Ufc+i) : Xk -> Y at each 
(ufc, .... ui) G Xk- An easy verification shows that 
tf : MLk+1(Xk+1, 
Y) -+ L(Uk+1, MLk(Xk, 
Y)) 
is an isomorphism. That is, i? is linear and invertible. In practice, we ignore the 
difference between F and ÚF. Hence the values of F G MLk+i(Xk+i, 
Y) may be 
denoted as 
F(u f c +i, Ufc, . . . , ui) 
or as 
F(ufc+1)(ufc, ..., ui), 
depending on the context. In the basic case of the usual product of (k + 1) numbers, 
this corresponds to identifying 
nt+i • rk ■ ■ ■ ri and rk+1 ■ (rk ■ ■ ■ n). 
The main significance of this identification is as follows. General results about 
MLk (Xk, Y) are usually proved by mathematical induction on k. Our identification 
simplifies these inductive proofs, as the isomorphism i? is used in the induction step 
to pass from k to (k + 1). 
Note also that 
ML1(X1,Y) 
= 
L(U1,Y) 
is the space of linear functions U\ —> Y and 
ML2(X2, 
Y) = BL{U2 x Uu Y) ~ L(U2, L(UU Y)) 
is the space of bilinear functions (U2 x U\) —> Y. 

MULTILINEAR FUNCTIONS 
91 
General Polynomials in Vector Variables 
Here we generalize our earlier discussion of second-degree homogeneous polynomi-
als. We assume some familiarity with permutations. Appendix C on determinants 
also contains a review of permutations. For each fc e N we let N& = { 1, ..., fc } and 
denote the set of all permutations of N^ by §k. Note that §>k contains k\ elements. 
Notation 3.3.8 lfUi = ----Un=X, 
then we let 
Xk = 
Ukx---xU1=Xk. 
Also, MLk(Xk, 
Y) denotes the space of all fc-linear functions (or k-products) with 
all factors from X. 
Definition 3.3.9 Homogeneous polynomials of degree k. Each 
M e MLk(Xk
y 
Y) 
defines a function / : X —► Y by 
/ ( x ) = M ( x , . . . , x ) , 
x e l 
Such a function is called a homogeneous polynomial (of a vector variable and of 
degree k). A sum of homogeneous polynomials is called a. polynomial. 
Definition 3.3.10 Symmetric multilinear functions. A multiple product M 6 
MLk(Xk, 
Y) is called symmetric if it is independent of the ordering of its k argu-
ments. More explicitly, M is symmetric if 
M(xi, ..., xfe) = Af (xCT(1), ..., xff(fc)) 
(3.14) 
for all permutations a G S&. 
Definition 3.3.11 Space of symmetric multilinear functions. Denote the set of all 
symmetric fc-products as SMLk(Xk, 
Y). Hence 
SMLk(Xk, 
Y) c MLk(Xk, 
Y). 
We see easily that SMLk(Xk, 
Y) is a subspace of MLk(Xk, 
Y). 
Definition 3.3.12 The symmetric part of a multilinear function. Given M G 
MLk{Xk, 
Y), define 
Ai(xi, ..., xfc) = — ^¡T Ai"(xff(1), ..., xCT(fc)) 
(3.15) 

9 2 
VECTOR FUNCTIONS 
for all (xi, ..., xfc) e Xk. We see that M e SMLk(Xk, 
Y). In fact, a permutation 
of the arguments of M results only in a change of the order of summation in (3.15). 
Hence the value of M does not change under a permutation of its arguments. (Note 
that this definition is a generalization of Definition 3.2.14.) 
Lemma 3.3.13 Let M g MLk(Xk, 
Y) and let M e SMLk{Xk, 
Y) be the sym-
metric part ofM. Then M and M define the same polynomial. That is, 
/(x) = M(x, ..., x) = M(x, ..., x) 
for all X É I . 
Proof. If Xj = x for alii = 1, ..., k, then 
(xl5 . . . , xfc) = (XCT(1), . . . , xCT(fc)) 
for any permutation a e Sfc. Then the conclusion follows from the definition of M 
given above in Definition 3.3.12. 
D 
Example 3.3.14 Compositions of operators. Let L = L(X, X) be the vector 
space of all linear operators R : X —> X. The composition of two operators is a 
2-product, as shown in Example 3.2.10. If ñi, ..., Rk G L are k linear operators 
X —* X, then their composition 
M(Rk, 
...,R1) 
= 
(Rk---R1)eL 
defines a /c-product M e MLk(Lk, 
L). In general, this is not a symmetric (commu-
tative) product. Note that the corresponding polynomial is 
f(T) = M(T, 
...,T)=Tk, 
with the definition of powers in Remarks 3.1.35. Note that Tk may be induced by 
several different fc-products. For example, T 3 is induced by M\{P, Q, R) = PQR 
or by M2((P, Q, R) = (l/2)(PQR 
+ PRQ). 
These are both non-symmetric 
products. Their symmetric parts are 
Ml=M2 
= (1/6) (PAS + RSP + SPR + PSR + SRP + RPS). 
The equality of the symmetric parts is not accidental. If Mi, M2 G MLk(Xk, 
Y) 
induce the same polynomial 
/ ( x ) = M i ( x , . . . , x ) = M 2( x , . . . , x ) , 
then Mi = M2. 

MULTILINEAR FUNCTIONS 
9 3 
Problems 
3.53 
Define p : R 3 x K 3 x I 3 —> Ras<p(x, y, z) — (x x y, z), where xx y is the 
usual cross-product of x and y, and (, } is the usual inner product operation. Show 
that <p is a multilinear function. Find a linear function T : R3 -> BL(R3 x M3, R) 
such that y>(x, y, z) = (Tx)(y, z) for all (x, y, z) G R3 x R3 x R3. 
3.54 
Define ¡p : R3 x R3 x R3 x R3 -> R as <¿>(x, y, z, u) = (x x y, z x u). 
Show that tp is a multilinear function. Find a linear function 
T : R3 -► MI 3(R 3 x R3 x R3, R) 
such that t/?(x, y, z,u) = (Tx)(y, z, u) for all (x, y, z, u) G M3 x R3 x R3 x R3. 
3.55 
Define ^ : R 3 x l 3 x l 3 ^ l 3 a s <p(x, y, z) = (x x y) x z. Show that p 
is a multilinear function. Find a linear function T : R3 -> BL(R3 x R3, R3) such 
that ¡¿>(x, y, z) = (Tx)(y, z) for all (x, y, z) G R3 x R3 x R3. 
3.56 
Let T : X\ x • • • x X m ->Zbe multilinear, where X i , . . . , X m, Z are vector 
spaces. Suppose that x = (xi,..., xTO) G X\ x • • ■ x Xm and x^ = 0 for some k. 
Show that Tx = 0. 
3.57 
Let Xi,..., 
Xk be vector spaces and let T : X\ x ■ ■ ■ x Xk —> X\ x • • • x 
Xk be multilinear. Suppose that there are linear maps Tj : Xj —> Xj such that 
T(xi,...,Xfc) = (Tix1,...,Tfcxfc). Iffc>2,mustT = 0? 
3.58 
Let TO > 1 be an integer and let X\,..., 
Xm, Z be vector spaces. Let 
Tfc : Xk —> ^ be linear and define S1: Xi x ■ • • x X m -+ Z by 
5(xi,..., x m) = Tixi H 
h TTOxm 
for all (x l 5..., x n) G Xn. 
Show that 5 is multilinear if and only if T¡. = 0 for all k = 1,..., m. 
3.59 
Let T : R3 x R2 x R5 -> R4 be multilinear. Show that there is a multilinear 
map ^ : R 3 x R 2 ^ M 4 x 5 such that 
T(x, y, z) = v?(x, y)z 
for all (x, y, z) G R3 x R2 x R5. 
Here, we identify each vector in Rm with an TO x 1 column matrix. Thus, the right-
hand side of the equation above is interpreted as the product of a 4 x 5 matrix and 
a 5 x 1 column matrix. The product obtained is a 4 x 1 matrix, which is identified 
with the corresponding vector in R4. 

9 4 
VECTOR FUNCTIONS 
3.60 
Let M : X\ x ■ • • x Xn —> Z be multilinear, and let Tk : Xk —> Xk be linear 
for each k = 1,..., n. Define U : X\ x ■ • • x X„ —> Z by 
?7(xi,...,x n) = A/(TiXi,...,T„x n) 
forall(xi,...,x n)€ Xi x ••• x X n. 
Show that U is multilinear. 
3.61 
Let T G ML(X\ x • • • x Xk, Z) and let ¿?i,..., Bk be nonempty bases for 
X\,..., 
Xk, respectively. Show that if T(bi,..., bfc) = 0 for all (bi,..., b^) G 
By x ■ ■ ■ x Bk, then T = 0. 
3.62 
Let Xi,..., 
X¿, Z be vector spaces, and let L : X\ x ■ • ■ x Xk -^ Z be 
multilinear. Let Y be any any nonzero vector space, and let yo be any nonzero 
vector in Y. Let D c Y with yo ^ D be. such that {yo} U D i s a basis for Y. 
Show that there is a multilinear map S : Xi x ■ ■ ■ x Xk x Y ^ Z such that for all 
(xi,...,x f c) eX1x 
■■• x Xk, 
Q, 
N / L(xi,...,x f c) 
ifu = y0 
5(x 1 )...,x f c )u) = | 
0
V
 
i f u £ D 
3.63 
Let X i , . . . , Xk, Z be vector spaces and suppose that B\,..., 
Bk are bases 
for X\...., 
Xk, respectively. Show that 
1. Given b G B\ x • • • x Bk and z G Z, there is a unique multilinear map 
7b,z : Xi x ■ ■ ■ x Xk —> Z such that 
/ A _ / 0 
i f x e £ i x - - - x ß f c , x ^ b 
J b , z W - | 
z 
i f x = = b _ 
2. If B is a basis for Z, then a basis for ML{X\ x ■ ■ ■ x X^, Z) is the set 
{ T b , , | b e ß i X " - x 5 t , z G B } , 
where Tb,z is the unique multilinear map in part 1. It follows that 
dimML(X1 
x ••• x Xk,Z) = (dimXi) •• • (dimXfe)(dimZ). 
3. If fc > 2, then 
ML(X1 
x---xXk,Z) 
~L(XUML(X2 
x ••• 
xXk,Z)). 
3.64 
Give an example of a bilinear map T : K2 x K2 —> R such that the map 
5 : R x R3 -> R defined by S(a;, (u, w, w)) = T((x, u), (v, w)) is not bilinear. 
3.65 
Find all homogeneous polynomials of degree 3 from R2 to R. 

INNER PRODUCTS 
95 
3.4 
INNER PRODUCTS 
Definition 3.4.1 Inner products. Let W be a vector space. An inner product on 
W is a function B :W xW = W2 —> R that satisfies the following three conditions 
for all x, y, z e W and for all a, /? e K. 
(1) B(x, x) > 0 for all x e W and B(x, x) = 0 if and only if x = 0. 
(2) B(x, y) = B(y, x). 
(3) B(ax + ßy, z) = aß(x, z) + #B(y, z). 
In working with a particular inner product, it is customary to denote this inner product 
by ( , ) and to write (x, y) instead of B(x, y). 
Remarks 3.4.2 Inner products and bilinear functions. Properties (2) and (3) 
of inner products imply that an inner product B is a symmetric bilinear function. 
Property (1) is referred to as positive definiteness. Hence, an inner product on X is a 
symmetric and positive definite bilinear function B : X2 —> R. 
Definition 3.4.3 Inner product spaces and Euclidean spaces. If B is an inner 
product on W, then (W, B) is called an inner product space. The inner product B 
is usually understood from the context, and W itself is also called an inner product 
space. A finite dimensional inner product space is called a Euclidean space. As 
we consider only finite dimensional spaces, all inner product spaces we consider 
are Euclidean spaces. We assume that Euclidean spaces are nontrivial, i.e., their 
dimension is at least 1. 
Definition 3.4.4 The standard Euclidean space. The standard dot product on R™ 
was defined in Example 3.2.4 as 
(x, y) = xij/i H 
h xnyn G M, 
where x = (xi, ..., xn) and y = (t/i, ..., yn). We see that this is an inner product 
on R™, so that K™ becomes a Euclidean space with this inner product. This is the 
standard inner product on W1. 
Definition 3.4.5 Norms. The norm of a vector x in a Euclidean space W is defined 
as 
||x|| = V(x,x>. 
(3.16) 
Note that for all x e W, 
||x|| >0and||x|| = 0 if and only if x = 0. 
(3.17) 

9 6 
VECTOR FUNCTIONS 
This property is referred to as positive definiteness of the norm. Also, for any x. £ W 
and t € K, 
||íx||2 = <íx,íx)=í 2(x,x>=í 2||x||, 
so that 
||tx|| = |i|||x||. 
This property of the norm is referred to as homogeneity. A third important property 
of norms, the triangle inequality, is established below as Theorem 3.4.7. 
Theorem 3.4.6 The Cauchy-Schwartz inequality. Ifx and y are two vectors in a 
Euclidean space W, then |(x, y)| < ||x|| ||y||. 
Proof. Let x, y e W. Then, for all t E K, 
0 < <ix + y, tx + y) = ||x||2t2 + 2(x, y) t + ||y||2. 
The conclusion follows from the following elementary fact: if A, B, C € M. and if 
At2 + 2Bt + C > 0 for all t £ R, then B2 < AC. In not, the quadratic equation 
would have two distinct real roots, implying the existence of negative values. G 
Theorem 3.4.7 The triangle inequality. Let W be a Euclidean space and x, y e 
W. 7Äe/i||x + y||<||x|| + ||y||. 
Proof. We have 
(x + y, x + y) 
(x, x)+2(x, y) + (y, y) 
||x|| 2+2{ X !y} + ||y||2 
(W + ||y||)2, 
where the last step follows from the Cauchy-Schwartz inequality. 
D 
Corollary 3.4.8 Ifx, y e W, then \ ||x|| - ||y|| | < ||x - y||. 
Proof. This is left as an exercise. 
□ 
Example 3.4.9 Inner products and homogeneous polynomials. For k = 1,..., n, 
let Ifc : X —» X be a linear map and let ( , )& be an inner product on X. Define 
S : X -> R by 
5x = ||rlX||2 + • • • + |¡T„x||2 
for all x E l 
x + y 
< 

INNER PRODUCTS 
9 7 
Then 5 is a homogeneous polynomial of degree 2 on X. To see this, define Uk : X x 
X - + R b y 
£/fc(x, y) = (Tfex, Tky)k 
for all (x, y ) e X x I 
Then Uk is bilinear. Hence, the sum U = U\ + ■ ■ ■ + Un is also a bilinear map 
from X x X into R. Hence, the map x H^ {/(X, X) is a homogeneous polynomial of 
degree 2 on X. Clearly, 
n 
n 
t/(x, x) = 5 ^ £/fc(x, x) = ] T ||rfcx||| - Sx 
for all x E l 
fc=i fe=i 
Orthogonality 
Definition 3.4.10 Orthonormal sets and orthonormal bases. Let W be an inner 
product space. 
1. Two vectors x and y in W are said to be orthogonal (or perpendicular) if 
(x, y) = 0. To indicate that x and y are orthogonal, we write x l y . 
2. Let A C W. Then A is called an orthogonal set if u _L v whenever u and v 
are distinct vectors in A. 
3. Let A C W. Then A is called an orthonormal set if ||u|| = 1 for each u e A 
and u J_ v whenever u and v are distinct members in A. 
4. If an orthonormal set is also a basis for W, then it is called an orthonormal 
basis for W. 
Example 3.4.11 Let X be an inner product space and x, y e X. Then (x, y) = 0 
if and only if ||x + ay ||2 = ||x — ay ||2 for all scalars a. To see this, let x, y be in 
X and let t G K. Then 
||x + iy||2 = (x + iy, x + ¿y) = ||x||2 + 2£<x, y) + t2 \\y\\2. 
Thus, ||x + ay||2 = ||x — ay||2 if and only if 4a(x, y) — 0. Hence, if a ^ 0, then 
||x + ay||2 = ||x — ay||2 implies that (x, y) = 0. The converse is obvious. 
Theorem 3.4.12 Pythagorean Theorem. Let a and b be two vectors in a Euclidean 
space. Then a X b if and only if 
||a + b||2 = ||a||2 + ||b||2. 
(3.18) 
Proof. ||a + b||2 = (a + b, a + b) = ||a||2 + 2(a, b) + ||b||2 . 
a 
Orthonormal bases for a Euclidean space have particular importance. We restate their 
definition separately. 

9 8 
VECTOR FUNCTIONS 
Definition 3.4.13 Orthonormal bases. Let W be a Euclidean space. Let 
E = {ei, ..., era} 
be a basis for W. Then E is called an orthonormal basis if 
<e¿> ei) 
= 
| 
l 
0 
if z ^ j , 
if i = j . 
We also write this last condition as (e¿, e_,) = Sij. 
Example 3.4.14 Let X be a Euclidean space. Let { ui, ..., u„ } and { ei, ..., e„ } 
be two orthonormal bases for X. DefineT G L(X, X)byTe¿ = u¿for¿ = 1,... ,n. 
Let us verify that T is invertible and then compute T_1e¿ in terms of the e¿ s and the 
UfcS. 
Since {ui,..., u„} is a basis for X and Tej = Uj, we have 
T(X) = Span {Tei,... ,Te„} = Span {ui,... ,u„} = X. 
Hence T is invertible. We have 
e¿ = (ei5 ui)u! H 
1- (eJ5 u„)u„ 
for all j = 1,..., n, 
since {ui,..., u„} is orthonormal. Hence 
T-le, 
= 
( e J , u 1 ) T - 1 u 1 + - - - + (e„u„)T- 1u„ 
= 
(ej,ui)ei + 
h ( e j ; u n) e n . 
Remark 3.4.15 Inner products in an orthonormal basis. Let 
E = {ei, ..., e„} 
be an orthonormal basis for a Euclidean space W. Let 
u = uiei + ■ • • + unen 
and v = v\e\ + ■ ■ ■ + vnen 
be two vectors in W. Then we see easily that 
(u, v) = u1v1 H 
h unvn . 
Theorem 3.4.16 Given any basis Afar a vector space W, there is a unique inner 
product on W with respect to which A is an orthonormal basis. 
Proof. Let A = {ai,..., a n} be a basis for W. If 
x = aiai + • • • + anan G W and y = í^ai + • • • + bna.n G W, 

INNER PRODUCTS 
9 9 
then define 
(x, y) = aibi H 
1- anbn . 
We see that this is an inner product on W. The basis A becomes an orthonormal 
basis in this inner product. Also, this is the only inner product on W that makes A 
an orthonormal basis. This follows from the expression of an inner product in terms 
of an orthonormal basis, as given above in Remark 3.4.15. G 
Theorem 3.4.20 below shows that there are orthonormal bases for any Euclidean 
space. That is, given an inner product on W, we can find an orthonormal basis. 
Hence any inner product on a finite dimensional vector space is of the form described 
above, with an appropriate choice of the basis. 
Lemma 3.4.17 An orthonormal set A is a linearly independent set. 
Proof. If ciui + • • ■ + cnun 
= 0 where u, s are distinct members of A, then 
0 = ((ciui -\ 
1- c„un), u¿) = a for alH = 1, ..., n. 
□ 
Lemma 3.4.18 In an n-dimensional Euclidean space, any orthonormal set with n 
vectors is an orthonormal basis. 
Proof. Orthonormal sets are linearly independent, by Lemma 3.4.17. Also, in an 
n-dimensional vector space, any linearly independent set of n vectors is a basis, by 
the dimension theorem 3.1.14. 
G 
Lemma 3.4.19 Gram-Schmidt process. Let A = ■[ ai, ..., afc ]■ be an orthonormal 
set in an n-dimensional Euclidean space W. Ifk<n, 
then there is an a £ W such 
that A' = {ai, ..., afc, a} is also an orthonormal set. 
Proof. We have Span A ^ W. In fact, A contains fewer than n vectors and therefore 
cannot be a basis for the n-dimensional space W. Hence there is a b £ W such that 
b g Span A. Let 
b' = (b, ai) ai H 
h (b, afe) afc 
and set c = b — b'. We see that c X a¿ for alH = 1, ..., n, since 
(c, a¿) = (b - b', a¿) = (b, a¿) - (b', a¿) = O. 
Hence {ai, ..., a/c, c} is an orthogonal set. Also, c = (b — b') =/= 0, since 
b ^ Span A and b' G Span A. Now let a = (||c||)_1 c. An easy check shows that 
A' = {ai, ..., afc, a} is an orthonormal set. 
G 
Theorem 3.4.20 Existence of orthonormal bases. Any Euclidean space X has an 
orthonormal basis. 

100 
VECTOR FUNCTIONS 
Proof. Let n = dim X, n > 1 (since we assume that X is nontrivial, as in 
Definition 3.4.3). Lemma 3.4.18 shows that any orthonormal set with n elements is 
an orthonormal basis for X. To see the existence of such an orthonormal set, take 
any nonzero vector a G Xandsetei = (||a||)_1a. Then E = {e} is an orthonormal 
set in X. By Lemma 3.4.19 (the Gram-Schmidt process), if 1 < n, we can find e2 
such that {ei, e2} is also an orthonormal set. Continuing to apply the Gram-Schmidt 
process, after a finite number of steps, we obtain an orthonormal set with n elements, 
which must be a basis. 
□ 
Theorem 3.4.20 on the existence of an orthonormal basis is important. One of its 
consequences is another basic result about the representation of linear functions. 
Theorem 3.4.21 Representation of linear functions. Let f : X —> R be a linear 
function on a Euclidean space X. Then there is a unique vector a G X such that 
/(x) = (a, x) for each x G X. 
Proof. Let E = { ei, .... e„} be an orthonormal basis for X. Let 
a = aiei H 
+ anen 
with a,j = f(ej). 
Let x = x\e\ + ■ ■ ■ + xnen be any vector in X. Then 
/(x) 
= 
/(xiei H 
hx„e„) 
= 
£i/(ei)-l 
hi„/(e„) 
= 
xiai + 
V xnan 
= 
(a,x). 
This shows the existence of an a G X such that /(x) = (a, x) for all x G X. To see 
the uniqueness of a, let a' £ I be another vector such that /(x) = (a', x) for all 
x G X. Then 
(a, x) - (a', x) = (a - a', x) = 0 
for all x £ l Hence (a — a', a — a') = 0, and therefore a — a' = 0. 
□ 
Example 3.4.22 Let Bi,B2 
be inner products on a vector space W. Then for any 
a G W, there is an a' G W such that 
ßi(a, x) = B2(a', x) 
for all x G W. 
To see this, let a G W. Define /(x) = Bl (a, x) for all x G W. Then / : W -> K 
is a linear map on the Euclidean space (W, i^)- Hence, by Theorem 3.4.21, there is 
an a' G W such that 
ßi(a,x) = /(x) = ß 2(a',x) 
forallxGW. 
A 

INNER PRODUCTS 
101 
Example 3.4.23 Let X be a finite dimensional inner product space with inner product 
( , ). Let us determine all bilinear maps from X x X into R in terms of the inner 
product (, }. 
Suppose that T : X x X —> M is multilinear. For each x G X, define Tx(y) = 
T(x, y) for all y e X. Then each Tx is a linear functional on X, and hence, there 
exists a unique z x in X such that 
Tx(y) = (y,zx) 
for all y 
eX. 
Since T is also linear in the first variable, it follows that 
zau+bv = 
O z u "T" DZV. 
Define / : X —* X by /(x) = zx. Then / is a linear map. Let A be the standard 
matrix for / with respect to some basis B of X. Then /(x) — >l[x] for all x G X, 
where [x] is the coordinate vector of x with respect to B. Thus, 
T(x,y) 
= {y,A[x]) 
f o r a l l ( x , y ) e X x X . 
A 
The Cauchy-Schwartz inequality 3.4.6 and the representation theorem 3.4.21 above 
have the following important consequence. 
Theorem 3.4.24 Boundedness of linear functions. Let W be a Euclidean space. 
Let f : W —> R be a linear function. 
Then there is a constant K such that 
|/(w)| 
<K\\w\\forallweW. 
Proof. The representation theorem 3.4.21 shows that for any linear / : W —> R, 
there exists a e W such that /(w) = (a, w) for all w £ W. Hence, by the 
Cauchy-Schwartz inequality (Theorem 3.4.6), 
|/(w)| = |(a,w)|<||a||||w|| 
for all w e W. Put if = ||a||. 
G 
Theorem 3.4.25 Boundedness of linear transformations. Let X and Y be two 
Euclidean spaces. Let T : X —> Y be a linear transformation. Then there is an M 
suchthat ||Tx||y < M\\x\\xforallx. 
G X. 
Proof. Let (ui, ..., u„) be an orthonormal basis for Y. For each i = 1, ..., n, let 
/¿(x) = (Tx, u¿). Then /¿ : X —> R is a linear function. Theorem 3.4.24 shows 
that there is a Ki such that |/¿(x)| < ií¿||x|| for all x e X . Hence, 
|jTx|| 
= 
||(Tx, u 1 ) u i + - - - + <Tx,un)un|| 
= 
||/ 1(x)ui+--- + / n(x)u n|| 
< 
|/i(x)| + --- + |/„(x)| 
< 
(if1 + --- + ^ n)||x||. 

102 
VECTOR FUNCTIONS 
The conclusion follows by setting M — K\ + ■ ■ ■ + Kn. 
D 
Example 3.4.26 Let X be a Euclidean space and / G L(X, R). Theorem 3.4.24 
shows that there exists a constant K for which |/(x)| < Ä"||x|| for all x G X. The 
smallest such K is ||a||, where a G X is such that /(x) = (a, x) for all x G X. 
To see this, let K0 be the smallest Ä" as defined above. The Cauchy-Schwartz 
inequality shows that |/(x)| = |(a, x}| < ||a|| ||x|| forallx G X. Hence K0 < ||a||. 
But |/(a)| = ||a|| ||a|| shows that we cannot have K0 < ||a||. Hence K0 = ||a||. 
Problems 
3.66 
Let ( ,) be an inner product on a vector space X. Let T, S be elements of 
L{X, X). Define U : X x X -► R by 
C/(x, y) = (5x, Ty) 
for all (x, y) G X x X. 
Show that U is bilinear. 
3.67 
Suppose that X is any finite dimensional vector space. Show that given a 
homogeneous polynomial / : X —-> R of degree 2 and any inner product (, ) on X, 
there is a linear map L : X —» X such that 
/(x) = (x,Lx) 
forallxGX. 
3.68 
Let B\,... 
,Bk be inner products on a vector space X. For any positive 
scalars ci,..., Cfe, show that c\ B\ + ■ ■ ■ + c^Bk is an inner product on X. 
3.69 
Let X be an inner product space. Let x, y G X and x ^ 0. Show that 
Ilx + y|| = llxll + ||y|| if and only if there is an a > 0 such that y = ax. 
3.70 
If x, y are vectors in an inner product space, show that 
l¡x + y||2 + ||x-y||2 = 2(||x||2 + ||y||2). 
3.71 
Suppose that u, v are vectors in an inner product space X and ||u|| = ||v||. 
Show that u — v _L u + v. 
3.72 
Suppose that {uj,..., u„ } is an orthogonal set of distinct vectors in an inner 
product space. Show that for all scalars c\,..., 
cn, we have 
Hdui + • • • + craun||2 = c2||Ul||2 + • • • + c2 ||un||2. 

ORTHOGONAL PROJECTIONS 
103 
3.73 
Let X be a Euclidean space. Let / G L(X, R). Then, by Theorem 3.4.21, 
there is a unique a/ G W such that /(x) = (a/, x) for all x G X. 
Define 
T : L(X, R) -> X by T / = a/. Show that T is an isomorphism. 
3.74 
Let B be an orthonormal basis for a Euclidean space X, and let S be a 
nonempty subset of B with S ^ B. Define T : I ^ I b y 
Tx = V 
(x. v)v 
for all x G X. 
Show that Ker T = Span S. 
3.75 
Let X be a Euclidean space. Let f\,..., 
fm be in L(X, R), If m > dim X, 
then show that some fk is a linear combination of the remaining fjS. 
3.76 
Let S = {ui,..., uTO} be an orthonormal subset in a Euclidean space X. 
Define T : X -> Rm by 
T(x) = ((x, u i ) , . . . , (x, u m)) 
for all x G Rm. 
Show that dim Ker T = dim X — m. 
3.77 
Let X be an inner product space, and let a G X be nonzero. Show that for 
any scalar c, there is an / G -L(X, R) and a u G X such that 
{ x G X ¡ (x, a) = c } = Ker / + u. 
3.5 ORTHOGONAL PROJECTIONS 
Recall the definition of orthogonality given in Definition 3.4.10: u and v are orthog-
onal, written u X v, if (u, v) = 0. 
Definition 3.5.1 Orthogonal subspaces. Let U and V be two subspaces of a Eu-
clidean space X. Then U and V are said to be orthogonal to each other if u _L v for 
all u G U and v £ 7 , We write U _L V to indicate that U and K are orthogonal to 
each other. 
Lemma 3.5.2 IfU ± V, then U n V = {0}. 
Proof. Let w G U n V. Then (w, w) = 0. Hence w = 0. 
□ 
Corollary 3.5.3 IfU LV, then U and V are complementary inW = U + V. 

104 
VECTOR FUNCTIONS 
Proof. By Lemma 3.1.20, U and V are complementary in W if U + V = W and if 
U n V = {0}. Hence the conclusion follows from Lemma 3.5.2. 
□ 
Definition 3.5.4 Orthogonal complements. If U ± V and U + V = X, then U 
and V are called the orthogonal complements of each other. In this case the pair 
(U, V) is called an orthogonal decomposition of X. 
Corollary 3.5.3 shows that 
orthogonal complements are indeed complementary subspaces in X. Hence U and 
V are orthogonal complements if and only ifU±V 
and X = U @V. 
Remark 3.5.5 Uniqueness of the orthogonal complements. In general, a subspace 
U of X has many complementary subspaces. If X is a Euclidean space, then the 
orthogonal complement of U is a special complementary subspace. It is uniquely 
determined by U, as the following theorem shows. 
Theorem 3.5.6 Let U be a subspace of a Euclidean space X and let 
V 
= 
{veX\v 
Luforall 
ueU} 
= 
{ v G X | (v, u) = 0 for all u G U } . 
Then U and V are orthogonal complements. Also, the orthogonal complement ofU 
is uniquely defined by U. 
Proof. We have to show that V is a subspace, U _L V, and X = U + V. Let 
v, v' G V and r, r' € R. If u G U, then (v, u) = 0 and (v', u) = 0. Hence, if 
u G U, then 
(rv + r'v', u) = r(v, u) = r'(v', u) = 0. 
This shows that rv + r'v' G V. Hence V is a subspace of X. Also, ifv e F and 
u G U, then (v, u) = 0, and therefore u l v . Hence U _L V. 
Now we show that X = U + V. Let { ei, ..., e^} be an orthonormal basis for U. 
For each x £ X, define 
a = (x, ei)ei + • • • + (x, efc)efc, 
and let b = x — a. We see that (b, e¿) = (x — a, e¿) = (x, e¿) — (x, e,) = 0 for 
alH = 1, ..., k. But any u G U is a linear combination u = uiei + • • • + u^k of 
e¿ s. Hence (b, u) = 0 for all u G U. Therefore b G V. Also, clearly, a G U and 
x = a + b. Hence X = U + V. 
To see the uniqueness of V, let V be another orthogonal complement of U. If 
v' G V, then v' 1 u for all u G U. Hence v' G V. Therefore V C V. Similarly, 
we obtain V c V. Hence V = V. 
O 

ORTHOGONAL PROJECTIONS 
105 
Notation 3.5.7 Orthogonal complements. One denotes the orthogonal complement 
of U as U±. Note that (U1-)1- = U by the preceding theorem. 
Example 3.5.8 Application to linear functionals. Let E and F be subspaces of a 
Euclidean space W. Let U = Span (E U F). Then E c U and F C U. Hence, 
C/x c £ x and i / k f 1 . Thus, [ / k ^ i l F 1 . So, 
(E^nF1-)-1 
c(U±)± 
= U. 
(3.19) 
We will next show that if / i , . . . , /TO and g are linear functionals on X, then 
<?€ Span {/i,...,/ m} 
(3.20) 
if and only if 
m 
f| Ker fk C Ker g. 
(3.21) 
fc=i 
It is clear that (3.20) implies (3.21). Conversely, assume (3.21). By Theorem 
3.4.21, there are a x,... ,a m and b in X such that Ker fk — (Span {a^})1- and 
Ker g = (Span {b})-1. Put Ek = Span {a^} and U = Span {b}. Then by our 
assumption (3.21), 
E i
L n - - - n £ ; ^ c i 7 - L . 
Hence, by (3.19), 
U = (t/-1)-1 C (Ef n ■ • ■ n E^)1- C Span (E1 U • • • U Em). 
This implies that b e Span {ai,... ,am}, and thus, 5 6 Span {/1,..., f m } . 
Definition 3.5.9 Orthogonal projections. If U and V are orthogonal complements 
in a Euclidean space X, then they are also complementary subspaces in X. Hence 
Theorem 3.1.37 shows that they are the ranges of complementary projections P and 
Q. In this case, these projections are called the orthogonal projections on U and V, 
respectively. Hence, if P is the orthogonal projection on U, then U = Range P = 
PX and V = Range Q = (I - P)X. 
As mentioned in Remarks 3.1.40, a subspace U of X does not determine a projection 
on U. Such a projection is determined only after a complementary space V of Í7 is 
specified. In a Euclidean space X, every subspace U has a distinguished complement 
V — U1-. Hence, in Euclidean spaces, every subspace U specifies a distinguished 
projection, namely, the orthogonal projection on U. 

106 
VECTOR FUNCTIONS 
Theorem 3.5.10 Let E = { ei, ..., ek} be an orthonormal basis for a subspace U 
of a Euclidean space X. Define P : X —> X by 
Px = (x, ei)ei H 
h (x, 
ek)ek. 
Then P : X —> X is the orthogonal projection on U. More explicitly, P is a 
projection with U = PX and V = (/ — P)X is the orthogonal complement ofU. 
Proof. We see that Px G U for all x G X and that Pu = u for all u e [/. Hence 
P 2x = P(Px) = Px for all x e X and therefore P 2 = P. This shows that P is a 
projection and that {/ — Range P. Also, as in the proof of Theorem 3.5.6 above, we 
see that x — Px is orthogonal to each u £ U. 
Let Q = I — P be the complementary projection and VA = Range Q. To complete 
the proof, we have to show that V is the orthogonal complement of U. If v G V, 
then Qv = (/ — P)v = v — Pv is orthogonal to each u £ U, as observed above. 
Therefore V — Range (/ — P) is indeed the orthogonal complement of U. 
□ 
Remark 3.5.11 The projection P : X —* X in Theorem 3.5.10 above is defined in 
terms of a basis J5 for U. The properties of P obtained in that theorem show that P 
is uniquely determined in terms of U. Hence P is independent of the choice of the 
basis E for U. Another important property is given in Theorem 3.5.12 below. This 
property also shows that P is determined by U alone. Geometrically, this property 
means that the point Px in U is the closest point to x among the points in U. 
Theorem 3.5.12 Let U be a subspace of a Euclidean space X. Let P : X —► X be 
the orthogonal projection on U. Let X É X Then 
||x - Px|| < ||x - a|| for all a E t / . 
Proof. Let v = x — Px. Then (v, u) = 0 for all u G U by Theorem 3.5.10. Now 
x — a = (x — Px) + (Px — a) = v + u, where u = (Px — a) G U. Hence v l u 
and ||v + uj|2 = ||v||2 + ||u||2 by the Pythagorean Theorem 3.4.12. Consequently, 
||x - a||2 = ||v + u||2 = ||v||2 + ||u||2 > ||v||2 = ||x - Px||2. 
D 
Example 3.5.13 Let X = R3 be the a;y2i-space and 
U = { (x, y, z) G M3 | x + y + z = 0 } . 
Show that U is a subspace of X and find the orthogonal projection P on U. 

ORTHOGONAL PROJECTIONS 
107 
First solution. It is obvious that U is a subspace. To find P, we need to find an 
orthonormal basis for U. One example of a nonzero vector in U is a = (1, —1, 0). 
Another vector in U not in the span of a is c = (1, 0, — 1). But we want to find a 
vector in U orthogonal to a, in order to facilitate the passage to an orthonormal basis. 
By inspection, we see that b = (1, 1, —2) is such a vector. The vectors a and b are 
nonzero and orthogonal to each other. Hence they are linearly independent. They 
must be a basis for U since (dim U) = 2. In fact (dim U) > 2 since U contains 
two linearly independent vectors. Also (dimi/) < 3, since it is clear that U j^ M.3. 
Hence 
e i = ( H ) - ^ = 2"1/ 2(1, - 1 , 0) and e2 = (||b|Q-xb = ö " 1 ' 2 ^ 1, -2) 
constitute an orthonormal basis for U. Therefore, for all r = (x, y, z) € M3, 
Pr 
= 
(r, ei)ei + (r, e2)e2 and 
P(x,y,z) 
= 
(l/2)(x-2/)(l, -1,0) + (l/6)(x + y-2z)(l, 
1,-2) 
- 
(l/3)(2x - y - z, -x + 2y - z, -x - y + 2z). 
To verify that this is the correct answer, it is enough to show that Pr £ U and 
(r — Pr) _L U, that is, (r — Pr) _L u for all u£f/. First, we see that Pr e U, since 
the sum of the coordinates of P(x, y, z) is 
(l/3)((2x - y - z) + (-x + 2y - z) + {-x - y + 2z)) = 0. 
To see that (r — Pr) ± U, we compute (r — Pr) as 
{(x, y, z) - P{x, y, z)) = (l/3)(x + y + z)(l, 1, 1). 
We see that (1, 1, 1) _!_ U. In fact, if (ui, ui, u3) e U, then u\ +112 + «3 = 0 and 
therefore ((1, 1, 1), (ui, u2, «3)) = u\ + u2 + U3 = 0. Hence (r — Pr) J_ U. 
Second solution. Instead of P, we may find the complementary projection Q = 
(I — P). This is the orthogonal projection on the orthogonal complement V of U. 
Since (dim U) = 2, we have (dim V) = 1. Hence Q should be easier to find. One 
example of a nonzero vector orthogonal to U is g = (1, 1, 1). In fact, U is the set of 
r e l 3 such that (r, g) = 0. Hence V is the space spanned by g. An orthonormal 
basis for V is 
e=(||g||)- 1g = 3 - 1 / 2 ( l ) l , l ) . 
Therefore Qr = (r, e)e is 
Q(x,y,z) 
= (l/3){x + y + 
z){l,l,l)-
Then we obtain 
P{x, y, z) = (I- 
Q)((x, y, z) = ((x, y, z) - (l/3)(x + y + z)(l, 1, 1). 
This agrees with the previous result, as already verified. 
A 

108 
VECTOR FUNCTIONS 
Problems 
3.78 
Let X be a Euclidean space. Let U\,..., Um be subspaces of X such that 
Ui _L Uj for all i ^ j and W = U\ + 
\- Um. Let P¿ be the orthogonal projection 
on Ui. Show that I = P1-\ 
\- Pm and 
||x||2 = ||PlXj|2 + ■ ■ • + ||Pmx||2 
for all x G X. 
3.79 
Let (W, (, )) be a Euclidean space, and let a be a nonzero vector in W. Let 
c e R and let E = { w G W | (w, a) = c }. Show that 
min ||w0 - e|| = K w 0 ' a ) ~ c | 
f o r an WQ G 
W, 
3.80 
Let U, V be subspaces of an inner product space. Show that if U C V, then 
V 1 c C/1. Hence, deduce that U± r\VL C (U n V)x. 
3.81 
Let U and V be subspaces of a Euclidean space and assume that UC\V = {0}. 
Is it true that UL C\VX = {0}? Is it true that (£/ + V)1- = t/1- + V-1? 
3.82 
Let [/ be a subspace of X with dim U = (dim X) — 1. Show that Í7 is the 
kernel of a nonzero / G ¿(X, R). 
3.83 
Let u i , . . . , u m be distinct vectors in a Euclidean space X. Assume that 
Uj _L Uj for all i ^ j . Let a G X. Let S be the set of all numbers of the form 
E
m 
k=lCkUk 
' 
where c\,..., 
cm range over all real numbers. Find inf S. 
3.84 
Let U be a subspace in a Euclidean space X. Let c e X and 
E = c + U = {c + u | uG U }. 
Find infe££: ||b — e|| for each b G X. 
3.85 
Let E = {ei, ..., e¡t} and U = { u\, .... u^} be orthonormal subsets of a 
Euclidean space X such that Span E = Span U. Is it true that 
max 
|(x, e i ) ¡ 2 + --- + |(x, efe)|2= 
max 
|(x, U l ) | 2 + • • • + |(x, ufc)|2? 
x.eX,\\x\\ = l 
x6X, x =1 

SPECTRAL THEOREM 
109 
3.86 
Let X be a Euclidean space with an orthonormal basis {ui,..., u m}. Let 
1 < k < m. Show that for each x £ X, there is a y € X such that 
x = (x, ei)ei + 
h (x, efc)efe + (y, efc+i)efc+i H 
\- (y, e m)e m. 
3.87 
Let T : X —> X be an orthogonal projection. Is it true that 
(Tx, y) = <x, Ty) 
for all x, y in X? 
3.6 SPECTRAL THEOREM 
The spectral theorem is of central importance in linear algebra. It summarizes many 
properties of linear transformations on Euclidean spaces. The proof of the spectral 
theorem that we shall develop in this section is simple, although not elementary. The 
usual route to the spectral theorem proceeds via the fundamental theorem of algebra, 
which is a familiar result but one that requires a good deal of preliminary work. 
Our alternative approach relies upon the Bolzano-Weierstrass theorem in Euclidean 
spaces, Theorem 4.2.9. (The proof of that result, in chapter 4, is entirely independent 
of the ideas developed in this section.) 
We will use the spectral theorem mainly for two purposes. First, the theorem gives us 
a geometrical picture of orthogonal projections between the subspaces of a Euclidean 
space. Second, the theorem allows us to provide a direct geometrical proof of a 
basic fact about the effect of a linear transformation upon the volume of a set: when 
a linear transformation maps one Euclidean space into another, the volumes of the 
images of sets under that transformation are multiplied by the absolute value of its 
determinant. For both of these purposes, an alternate formulation of the spectral 
theorem is convenient. This formulation is stated below as Theorem 3.6.4. It is 
equivalent to the standard version, Theorem 3.6.12. The standard version of the 
spectral theorem is not used in this book. 
Eigenbases 
The property of linear maps that we want to show is the following. For any linear 
map T : X —> Y between two Euclidean spaces, there is an orthonormal basis for 
X that is mapped to an orthogonal set of vectors in Y. We will call such a basis 
an eigenbasis for T. The key step in the proof of this result is Lemma 3.6.1, which 
states that any linear transformation between Euclidean spaces attains a maximum 
norm on the unit ball. The proof depends crucially upon Theorem 4.2.9, the Bolzano-
Weierstrass theorem in Euclidean spaces. After convergence in Euclidean spaces is 
discussed, we will restate Lemma 3.6.1 as Lemma 4.5.45 with a short proof. 

110 
VECTOR FUNCTIONS 
Lemma 3.6.1 Let X and Y be Euclidean spaces. Let T : X —> Y be a linear 
transformation. Then there is a unit vector e G X such that ||Tx|| < ||Te|| for all 
unit vectors x G X. 
Proof. By the boundedness of linear transformations, Theorem 3.4.25, there is a 
numberMsuchthat||Tx|| < Af||x|| forallx G X. Hence, B = { ||Tx|| | ||x|| = 1 } 
is a bounded set of numbers. If a = sup B, then there is a sequence x„ € X 
such that ||x„|| = 1 and such that limn ||Tx„|| = a. By the Bolzano-Weierstrass 
theorem, Theorem 4.2.9, there is a subsequence x„fc and a vector e G X such that 
liirifc ||xnfc - ej| = 0. Then limfc| ||xnfc || - ||e|| | = 11 - ||e|| | = 0 and 
limfc| j|Tx„fe|| - ||Te|| | < \\Txnk - Te\\ < M\\xnk ~ e|| = 0, 
by Corollary 3.4.8. It follows that ||e|| = land ||Te|| = a. Therefore ||Tx|| < ||Te|| 
for all unit vectors X É I . 
G 
Lemma 3.6.2 Let T : X —» Y be a linear map between two Euclidean spaces. Let 
e € X be a unit vector such that ||Tx|| < ||Te|| for all unit vectors x G X. Then 
Tx _L Te in Y whenever x 1 e in X. 
Proof. Let u be a unit vector in X and u l e . Let 
A = ¡|Te||2, B = (Te, Tu), and C = ||Tu||2. 
An easy check shows that v = cosiu + sinie is a unit vector in X. 
Hence, 
||Tv|j2 < A by the hypothesis about e. Therefore 
||Tv||2 
= 
(cosíTu + siníTe, cosíTu + sin¿Te) 
= 
Ccos2í + 2ßsiní cosí + Asin2 t < A. 
It follows that 
2Btaní 
< 
(A - C) 
for all t G (-7I-/2, TT/2). This implies that B = (Te, Tu) = 0 . 
D 
Definition 3.6.3 Eigenbases. Let T : X —» Y be a linear transformation between 
two Euclidean spaces. An eigenbasis for T is any orthonormal basis 
E = {ei, ..., e„} 
for X such that 
(Te¿, Te^y = 0 for i ^ j . 

SPECTRAL THEOREM 
111 
Theorem 3.6.4 Spectral theorem (eigenbasis version). Every linear transforma-
tion T : X —> Y between two Euclidean spaces has an eigenbasis. 
Proof. Let T : X —> Y be a linear map. We proceed by induction on n — dim X. If 
n = l , then either of the two unit vectors in X forms an eigenbasis for T. Assume 
that the result is true for n-dimensional spaces, and let X be an n + 1-dimensional 
space. Use Lemma 3.6.1 to find a unit vector eo G X such that ||Tu|| < j|Te0|| for 
all unit vectors u e l Let Xo be the orthogonal complement of the space spanned 
by e0. Lemma 3.6.2 shows that Te _L TXQ. Since X0 is an n-dimensional space, we 
may use the inductive hypothesis to find an orthonormal basis E0 = { ei, .... e„} 
for Xo such that (Te¿, Te,-) = 0 whenever i, j = 1, ..., n and i ^ j . But then 
} is an eigenbasis for T. ü 
Self-Adjoint Transformations 
The standard version of the spectral theorem is about self-adjoint transformations. 
These are linear transformations T : X —> X such that (Tx, y) = (x, Ty) for all 
x, y G X. They are also defined in terms of the adjoint transformation. 
Theorem 3.6.5 Let T : X —> Y be a linear transformation between two Euclidean 
spaces. Then there is a unique linear transformation T* : Y —>■ X such that 
(Tx, y)Y = (x, T*y)xforallyie 
X and y G Y. 
Proof. Let y e Y b e fixed. Then /(x) = (Tx, y)y, x £ X, defines a linear map 
/ : X —> R. By Theorem 3.4.21, there is a unique vector U E I such that 
/(x) = (Tx, y)Y = (x, u)x, for all x G X. 
This defines a map S : Y —> X such that 
(Tx, y)Y = (x, S(y))x 
for all x G X, y G Y. 
It remains to show that S : Y —+ X is linear. Let a, ß G R, u, v £ Y. Then 
(x, ,5(au + /3v)) = 
(Tx, era + /3v)x = (Tx, cm) x + (Tx, /3v)x 
= a(Tx, u)x+ß{Tx, v)x 
= a(x,5(u))y+^(x, 5(v))y 
= (x, a S » + /3S(v))y 
for all x G X. Hence S(au + ß\) = aS(u) + ßS(\r). D 

112 
VECTOR FUNCTIONS 
Definition 3.6.6 The adjoint transformation. The linear transformation S defined 
by Theorem 3.6.5 is called the adjoint transformation of T : X —» F. It is denoted 
by T* : Y -* X. 
Definition 3.6.7 Self-adjoint transformations. A linear transformation T : X —> 
X is called a self-adjoint transformation if T = T*. 
Lemma 3.6.8 Let E be an eigenbasis for T : X —> Y. Ifei e E, then 
T*Tei = mei where m = ||Te¿||2. 
Proof. We have (e,-, T* Tez) x = {Tej, Te,) Y = 11 Tet | |2 6,¿. G 
Definition 3.6.9 Eigenvectors and eigenvalues. Let X be a Euclidean space and 
let S : X —> X be a linear transformation. A vector e € X is called an eigenvector 
of S if e T^ 0 and if Se = Ae with A £ K. The number A is called the eigenvalue of 
the eigenvector e e l . 
Lemma 3.6.10 Every self-adjoint transformation S : X —> X /zas a« eigenvector. 
Proof. Let e be a vector in an eigenbasis of S. Let a = || 5e||. If Se = —ae then e is 
an eigenvector with the eigenvalue A = —a. Otherwise, u = (ae+Se) ^ 0. Lemma 
3.6.8 shows that S2e = a2e since S* = S. Hence, S(ae + Se) = a(ae + Se). 
Therefore, u is an eigenvector of S with the eigenvalue A = a. 
O 
Lemma 3.6.11 Let S : X —> X be a self-adjoint transformation. Let u be an 
eigenvector of S. Ifv _L u, then Sv _L u. 
Proof. We have (v, u) = 0. Let Su = Au. Hence, 
(Sv, u) = (v, Su) = A(v, u) = 0 
shows that S v l u . 
D 
Theorem 3.6.12 Spectral theorem. Let S : X —> X be a self-adjoint transforma-
tion. Then X has an orthonormal basis consisting of the eigenvectors of S. 
Proof. Proceed by induction on n = dim X. If n = 1 then any nonzero vector is an 
eigenvector of S. The result is clear in this case. Assume the result for n-dimensional 
spaces, and let X be an (n + l)-dimensional space. Apply Lemma 3.6.10 to find a 

SPECTRAL THEOREM 
113 
unit eigenvector e. Let XQ be the orthogonal complement of the one-dimensional 
space spanned by e. Lemma 3.6.11 shows that the subspace XQ is invariant under S. 
Since XQ is an n-dimensional space, the induction hypothesis shows that there is an 
orthonormal basis E0 for XQ consisting of the eigenvectors of S. Then E = (e, Eo) 
is an orthonormal basis for X consisting of the eigenvectors of S. 
Q 
Remarks 3.6.13 Two versions of the spectral theorem. Theorems3.6.4and3.6.12, 
our two versions of the spectral theorem, are equivalent. We have seen that Theorem 
3.6.12 follows from Theorem 3.6.4 through elementary arguments. The following 
theorem shows that the converse is also true. 
Theorem 3.6.14 Let T : X —> Y be a linear map between Euclidean spaces and let 
T* : Y —> X be its adjoint. Then an orthonormal basis E of X is an eigenbasis of 
T if and only if each e¿ G E is an eigenvector of S = T*T : X —► X. 
Proof. Let E be an eigenbasis for T. If e G E, then Lemma 3.6.8 shows that 
T*Te = j|Te||2e. Hence e is an eigenvector of S with the eigenvalue ||Te||2. 
Conversely, assume that E is an orthonormal basis of X such that each e G E is an 
eigenvector of S = T*T. If e¿ and e¡ are two different vectors in E, then 
(Te¿, Tej)Y 
= (Sez, ej)x 
= A¿(ej, ej)x 
= 0. 
Hence E is an eigenbasis for T. 
Q 
Eigenbases for the Adjoint Transformation 
There is a natural correspondence between the eigenbases of a linear transformation 
T : X —> Y and the eigenbases of the adjoint transformation T* : Y —> X. The 
situation is simplest for invertible transformations. We consider this case separately. 
Lemma 3.6.15 Let T : X —> Y be an invertible transformation. Let E be an 
eigenbasis for T. Let U = {Te/||Te|| | e € E }. Then U is an eigenbasis for 
T* :Y -> X. IfeeE 
and ifTe = au with u e U, then T*u = ae. 
Proof. Let e e E and let a = \\Te\\. Then a ^ 0 since T is invertible. If 
u = (l/a)Te, then Te = a u and T*Te = a T*u = a2e by Lemma 3.6.8. Hence 
T*u = ae. This also shows that U is an eigenbasis for T*. 
G 
Remarks 3.6.16 Kernel of a transformation. The kernel of a linear transfor-
mation T : X —> Y was defined in Definition 3.1.28 as the subspace Ker T = 

114 
VECTOR FUNCTIONS 
{x | Tx = 0 } = Xi C X. 
Lemma 3.6.17 below shows that an eigenbasis for 
T separates X\ from its orthogonal complement XQ = X\X. 
Note that not every 
orthonormal basis has the property formulated in Lemma 3.6.17. 
Lemma 3.6.17 Let T : X —> Y be a linear transformation. Let X\ = Ker T and 
let XQ = Xi 
be the orthogonal complement of X\. Then each eigenbasis E/or T 
decomposes as E = Eo U Ei, Eo fl Ei = 0, such that Eo spans XQ and Ei spans 
X\. Furthermore, ifE^ is any orthonormal basis for X\, then E' = Eo U E[ is also 
an eigenbasis for T. 
Proof. Given an eigenbasis E, let E0 = { e | e e E, Te ^ 0 }. The rest of the 
proof is left as an exercise. 
□ 
Theorem 3.6.18 Let T : X —> Y be a linear transformation and letT* : Y —> X 
be the adjoint transformation. Let X\ = Ker T and Y\ = Ker T*. Let XQ = X\ 
and Y0 = Yi^. Let E be an eigenbasis for T. Let E = E0 U Ei, E0 n Ei = 0, be the 
decomposition o/E as obtained in Lemma 3.6.17. Let 
Uo = { T e / | | T e | | | e e E 0 } . 
Let Ui be an orthonormal basis for Y\. Then U = Uo U Ui is an eigenbasis for 
T* : Y —> X. Also, ife S EQ and a = ||Te||, then there is a unique u € Uo such 
that Te — au and T*u = ae. 
Proof. This is summary of the results obtained above. 
□ 
Application to Orthogonal Projections 
An application of Theorem 3.6.18 to orthogonal projections gives a simple geomet-
rical picture for these projections. 
Let A and B be two subspaces of a Euclidean space X. The orthogonal projection 
on B is a linear map defined on X. The restriction of this map to A defines a linear 
transformation T : A —> B. Similarly, take the orthogonal projection on A and 
restrict this to B. One gets another linear transformation S : B —> A. 
The situation is very simple if A and B are both one-dimensional spaces. Let u be a 
unit vector of A and let v be a unit vector of B. Then 
Tx = (x, \)x v for all x e A and Sy — (y, u)x u for all y S B. 
Theorem 3.6.20 shows that the general case can be understood in terms of this simple 
case. 

SPECTRAL THEOREM 
115 
Lemma 3.6.19 Let A and B be two subspaces of a Euclidean space X. Let T : 
A —> B and S : B —» A be the corresponding orthogonal projections. Then S = T* 
is the adjoint transformation ofT. 
Proof. Let a £ A and b e i ? . Then a = ai + Ta where ai ± B. Hence, 
(a, b)x = (ai + Ta, b)x = (al5 b)x + (Ta, b)x = (Ta, b ) ß . 
Similarly we obtain (a, b)x = (a, 5b)A- Hence T* — S. 
□ 
Theorem 3.6.20 Let A and B be two subspaces of a Euclidean space X, Then there 
is a decomposition 
A = A0 © Ai with A0 _L Ax and B = B0 © Bi with B0 _L Bx 
such that A\ _L B, B\ _L A, and dim AQ = dim BQ. Also, there are orthonormal 
bases Uo and Vo for A0 and B0 such that Tu¿ = A¿v¿ and 5v¿ = A¿u¿ for all 
u¿ G AQ and v¿ G BQ. 
Proof. This follows directly by an application of Theorem 3.6.18 to the adjoint 
transformations T and S. 
□ 
Note that Tu¿ = u¿ = v,: = SV¿ is possible for some i. These vectors would span 
An B. Otherwise, the two-dimensional spaces spanned by u¿ and v¿ are invariant 
under both T and S. These spaces are mutually orthogonal to each other, and in each 
one of them the projections T and S are like the projections between two vectors. 
A Summary of Determinants 
Determinants are a particularly important class of multilinear functions. They are 
essential in integration and are also important in other applications. Appendix C 
contains a review of the most important results about determinants, together with 
complete proofs. Here we summarize the main features of determinants. 
Definition 3.6.21 Alternating multilinear functions. Let X and Y be two vector 
spaces, and k G N. A multilinear function F : Xk —> Y is called an alternating 
multilinear function if 
^(XA(I), •■•, xA(fe)) = (signA)F(xi, ..., xfc) 
for all permutations A G Sfc of Nfc = { 1, ..., k}. The set of alternating multilinear 
functions F : Xk -► Y is denoted by AML(Xk, 
Y). We see that this is a subspace 
of ML(Xk, 
Y). 

116 
VECTOR FUNCTIONS 
Definition 3.6.22 Determinant functions. If dim X = n, then any nonzero element 
r¡) of AML(Xn, 
IR) is called a determinant function, or simply a determinant. 
Theorem 3.6.23 below expresses a key fact about determinants. 
Theorem 3.6.23 If dim X = n, then AML(Xn, 
R) is a one-dimensional space. 
Proof. See Corollary C.3.10 in Appendix C. 
□ 
Theorem 3.6.23 implies that any determinant function is a nonzero multiple of any 
other determinant function. Hence, there is essentially only one determinant function 
Xn —> R, up to a nonzero multiplicative constant. (It is essential here that dim X = 
n.) The following example tells us what this single determinant function looks like. 
Example 3.6.24 Basic example of a determinant function. 
Let X be an n-
dimensional space. Let E = (ei, ..., e„) G Xn be an ordered basis for X, that is, 
an n-tuple of vectors that constitute a basis for X. Given any (xi, ..., xn) e 
Xn, 
let M = {xij } be the matrix obtained from the coordinate expansions 
x¿ = 2_^ 
.xijej 
and let 
V>E(XI, ..., x n) = detM = 
det{xij}. 
Here det M is the usual determinant of an n x n matrix (see Appendix C for the 
definition and basic properties). This is an alternating multilinear function. It follows 
that ?/>E : Xn —> R is a determinant function. Note that 
tpE(ei, ..., en) = 1 
(3.22) 
for any basis E = (ei, ..., e„), since the identity matrix has determinant 1. 
In light of Theorem 3.6.23, any other determinant function is a nonzero multiple of 
V'E-
Definition 3.6.25 Determinant of a linear operator. Lemma C.4.1 shows that 
if T : X —> X is a linear transformation, ip is any determinant function and E = 
(ei, ..., e„) is any basis for X, then the number ip(Tei, ..., Ten)/iß(e\, 
..., en) 
is independent of the choice of the determinant function tp and the basis E. It 
is called the determinant of T and is denoted by det T. In particular, det T = 
rpE(Tei, ..., Te„) for any basis E (since ^ E(ei, ..., en) = 1). 

SPECTRAL THEOREM 
117 
Definition 3.6.26 Euclidean determinants. There is a special situation in Euclidean 
spaces. If E and U are two orthonormal bases for a Euclidean space, then they define 
the same determinant up to a factor of ±1. Such a determinant is called a Euclidean 
determinant. Hence there are exactly two Euclidean determinants in a Euclidean 
space. 
Definition 3.6.27 Determinant of a linear transformation between two Euclidean 
spaces. In general, there is no way to define a unique determinant of a linear transfor-
mation between two different vector spaces (of equal dimension) independently of the 
choice of a basis and a determinant function on each vector space. If T : X —» Y is 
a linear transformation between two Euclidean spaces of equal dimension, however, 
then there is a natural choice for the determinant for T, although it is defined only up 
to a factor of ±1. Let U and V be orthonormal bases for X and Y. Then define 
detT = ih(Teu 
..., Ten)/^v(ei, 
..., en), 
(3.23) 
where E = (ei, ..., e„) is any basis for X. This definition is independent of the 
choice of E, but it depends minimally on the choices of the Euclidean determinants 
ipu and ipy. Nevertheless, different choices change the result only up to a factor of 
±1. Fortunately, this is not important for most of our work with determinants, which 
depends only upon the absolute value of the determinant. In particular, this absolute 
value is the volume multiplier associated with the linear transformation. 
Determinants as Volume Multipliers 
Theorem 3.6.28 Let T : X —> Y be a linear transformation between two Euclidean 
spaces of the same dimension n. IfK is an eigenbasis for T, then 
detT = ±A1---A„, with Xi = \\Tei\\. 
Proof. The vectors Te¿, e¿ G E, are orthogonal to each other, since E is an eigenbasis 
for T. Some of these vectors may vanish, but in any case, there is an orthonormal 
basis U for Y such that Te¿ — A¿u¿. Then 
i/jv(Tei, ..., Ten) 
= 
^u(AiUi, ..., Anun) 
(3.24) 
= 
Ai •••A„V,u(ui, ..., u n) 
(3.25) 
= 
Ai-"A„. 
(3.26) 
Here (3.25) follows from the multilinearity of determinant functions. The conclusion 
follows from definition (3.23), since ^ ( e i , ..., en) = 1 by(3.22). 
D 
Remarks 3.6.29 Comments about volume. An important part of the theory of 
integration (Chapter 8) is to define a notion of volume on Euclidean spaces. Our 

118 
VECTOR FUNCTIONS 
definition will agree with intuitive ideas about volume recognized by Archimedes 
more than two thousand years ago. "Cubic boxes" spanned by orthonormal bases will 
have unit volume. The volume of a general rectangular box, spanned by orthogonal 
vectors, will be the product of its side lengths. Combining these points with Theorem 
3.6.28, we see that a linear transformation transforms a certain cubic box of unit 
volume into a rectangular box of volume | det T\. For now, we know only that this is 
true for this one particular cubic box. But it is a very plausible fact, which we shall 
eventually prove, that a linear transformation changes all volumes by the same factor. 
This factor will be called the volume multiplier of the transformation. Therefore the 
volume multiplier ofT is equal to | det T\. 
Examples 
Example 3.6.30 Finding an eigenbasis. Let A = [ u 
v ] be a 2 x 2 matrix 
considered as a linear map from R2 to IR2. Assume that ||u|| = ||v||. As a first 
step towards finding an eigenbasis for A, let us find a unit vector e in R2 such that 
|| Ax¡| < ¡| Ae\\ 
for all unit vectors x in K2. Each unit vector in M2 is of the form 
(cos 9, sin 9) for some 9 6 [0, 2TC\. Thus, we look for 9 for which 
cos* 
sine 
is maximized. Now, 
. 
coso 
sinö 
(cos 9u + sin 9v) ■ (cos 9u + sin 9w) 
sin2 0||v||2 + cos2 0||u||2 + sin(26>)(u • v) 
= 
||u||2 + sin(2ö)(u-v). 
So, if u • v > 0, then we choose 9 so that sin(2#) = 1. If u • v < 0, we choose i 
r i 
5 
so that sin(2#) = - 1 . As an illustration, let A = 
u • v > 0. Thus, with 9 = 7r/4, we have 
Then Hull = ||v|| and 
Hence, 
(cosé>,siné>) = -^(1,1). 
V2 
||Ae|| = 3V10. 
Let u G M2 with u l e . Then u = t(—1,1) for some scalar t. Since 
6 
12 
and 
A 
4 
- 2 

SPECTRAL THEOREM 
119 
are orthogonal, it follows that ^4u ± Ae, as predicted by Lemma 3.6.2. 
Example 3.6.31 Eigenbases of real-valued functions. Let T : X —> Y be a linear 
transformation between Euclidean spaces. If dim Y = 1, then one can easily exhibit 
an eigenbasis for T. Let u be a unit vector in Y and express T in terms of a real 
valued function / : X —> R as Tx = /(x) u. Then, by Theorem 3.4.21, there is 
a vector a € X such that /(x) = (a, x). Let ei be a unit vector in X such that 
a = a e i . If ||a|| ^ 0, one can take ei = a/||a||. Complete ei to an orthonormal 
basis E = (ei, ..., eTl) for X. It is clear that this is an eigenbasis for T : X —» Y 
since Te¿ = 0 for alH = 2, ..., n. 
Example 3.6.32 Let R3 be represented as the xyz-space. Let A be the subspace 
x + 2y + 3z = 0 and let B be the subspace z = 0. Let P : R3 —> B be the orthogonal 
projection on B. Let G : A —» P be the restriction of P to A. Find eigenbases for 
P and for G. 
Solution. An easy verification shows that P(x, y, z) = (x, y, 0). Hence we see 
that the orthonormal basis 
E 
= 
{ (1, 0, 0), (0, 1, 0), (0, 0, 1) } of R3 is mapped to 
PE 
= 
{(1, 0, 0), (0, 1, 0), (0, 0, 0)}. 
We see that PE is an orthogonal set in B. Hence E is an eigenbasis for P. 
To find an eigenbasis for G : A —► B, consider A as the graph of the function 
z = F(x, y) = — (l/3)(x + 2y). This is defined on the rcy-plane B and takes 
values on the z-axis. Since it is a real-valued function defined on a Euclidean 
space B, it can be represented as an inner product. In fact, z = (a, (x, y)}s. 
where a = ( — 1/3, —2/3). Express this vector as a = aei, where a = \/5/3 and 
ei = (—1, —2)/\/5 is a unit vector. Example 3.6.31 shows that the orthonormal 
basis consisting of 
ei = ( - 1 , -2)/V^ande 2 = (2, - l ) / \ / 5 
is an eigenbasis for F : B —> R. Let S : B —> A be the transformation that takes a 
point (x, y) G B to the corresponding point S(x, y) = (x, y, —(l/3)(x + 2y)) on 
A, the graph of F : B —> R. Theorem 3.6.20 shows that the normalized vectors 
H S e O i r ^ e ! ) = - ( 3 , 6, 5)/v^70 and ||5e2)||-15(e2) = (2, - 1 , Q)/y/% 
form an eigenbasis for the orthogonal projection G : A —► B. 
A 

120 
VECTOR FUNCTIONS 
Problems 
3.88 
Let A and B be two subspaces of a Euclidean space. Let T : A —-> B, 
S : B —> A be the corresponding orthogonal projections as in Lemma 3.6.19. Show 
directly, without using eigenbases, that T and S are adjoint transformations. 
3.89 
Show that any orthogonal projection T : X —> X to a subspace of X is a 
self-adjoint transformation. 
3.90 
Let T : X —» X be a self-adjoint transformation. Let C/ be an invariant 
subspace for T. That is, let Tu e U for all u e Í7. Show that V = U1- is also 
invariant under T. 
3.91 
Give an example of a self-adjoint transformation T : X —> Jf and an 
eigenbasis E for T such that no vector in E is an eigenvector of T. 
3.92 
Let A and B be two subspaces of X with dim A = dim B. Let T : ^4 —► B 
and 5 : ß —> A be the corresponding orthogonal projections. Also assume that 
there is an eigenbasis U for T such that Tu» ^ 0 for all u¿ e U. Show that 
| det T\ = | det S\. Also, if A' = A1-, B' = B1- with the corresponding orthogonal 
projections T : A' -► S' and S' : B' -^ A', then 
|detT| = |det5| = | detT'| = |detS'|. 
Show that in the two-dimensional case this is a familiar statement: the angle between 
two lines is the same as the angle between their normals. 

PART II 
DIFFERENTIATION 

This page intentionally left blank

CHAPTER 4 
NORMED VECTOR SPACES 
The norm of a vector in a Euclidean space V has already been defined as ||v|| = 
(v, v) 1/ 2. The most important properties of the norm are the following: 
(1) ||v|| > 0 for a l i v e V and ||v|| = 0 if and only if v = 0. 
(2) ||iv|| = |f | ||v|| for all vectors v e V and for all scalars t e R. 
(3) ||u + v|| < ||u|| + ||v|| for all vectors u, v 6 V. 
Any function that has these three properties is a reasonably well-behaved candidate 
for the length of a vector. A well-behaved concept of length gives us a well-behaved 
notion of distance between two vectors. That is the basic tool for defining limits and 
convergence. Thus, analysis depends upon norms in an essential way. 
It turns out, however, that many arguments in analysis apply not just to Euclidean 
spaces, but also to any vector space X that is equipped with a function || • || : X —> R 
that has the three properties listed above. Such functions are called (general) norms 
on X, and X is called a normed vector space. The first part of the chapter establishes 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
123 

124 
NORMED VECTOR SPACES 
the basic results about norms and about when a sequence in a normed vector space 
converges to a limit. 
It is natural to wonder whether facts about convergence and limits might vary, de-
pending upon one's choice of norm. A key result in the early portion of the chapter is 
that any two norms (on a finite-dimensional vector space) are equivalent: they define 
exactly the same class of convergent sequences and exactly the same limits. This 
part of the chapter also shows that we can define a very natural (and useful) norm on 
the vector space of linear transformations between two normed spaces, and the same 
point applies to multilinear transformations. 
The middle section of the chapter defines the continuity of functions on normed 
spaces. In this section, we establish analogues of familiar results about the continuity 
of functions of a real variable as well as some basic facts about Cartesian products. 
Finally, by exploiting properties of the norm defined on the space of linear transfor-
mations, we show that the operation that maps a linear transformation to its inverse 
is continuous. 
The final section defines the most important concepts of general topology: open and 
closed sets, boundaries, compactness, and connectedness. Still working in the setting 
of normed vector spaces, we establish fundamental results about compactness and 
continuity that will be used throughout the rest of the book. 
4.1 
PRELIMINARIES 
Definition 4.1.1 Norms. Let X be a vector space. A function || • || : X —> R is 
called a norm on X if it satisfies the following three conditions. 
1. Positive definiteness: I f x e l and x ^ 0, then ||xj| > 0. 
2. Homogeneity: I f x e l a n d i G l , then ||ix|| = |i| ||x||. 
3. Triangle inequality: If x, y e X, then ||x + y|| < ||x|| + ||y||. 
Definition 4.1.2 Normed spaces. A normed space is a vector space X together with 
a norm || • || : X —> R. We use the same notation, || • ||, for norms on different normed 
spaces. Where the distinction between different spaces is not clear, we write || ■ ||x 
for the norm on X. 
Examples 4.1.3 Norms on R™. The standard Euclidean norm on R™ is 
\\x\\ = \\(x1,...,xn)\\ 
= (xj + --- + 
xl)V2. 

PRELIMINARIES 
125 
It is induced by the standard inner product on Kn. When n = 1, the Euclidean norm 
is simply the absolute value function on R. Two other useful norms on Rn are 
||x|| 
= 
HOi, ..., xn)\\ = |xi| + ••■ + \xn\ 
and 
||x|| 
= 
||(xi, ..., xn)\\ =max(|xi|, ..., |xn|). 
It is easy to verify that these are indeed norms. In both cases, the triangle inequality 
follows immediately from the definition and the special case of the triangle inequality 
forR. 
Example 4.1.4 Norms on R1 = R. The absolute value \a\ of a e R defines a norm 
on the one-dimensional space R1 = R. Any norm on R1 is a positive multiple of the 
absolute value. To see this, let || ■ || be a norm on R1 and let ||1|| = M. Then M > 0 
and ||a|| = ||al|| = \a\ ||1|| = M \a\ for all a € R1 = R. When we consider R as a 
normed space, we will always assume that the norm is the absolute value. 
Example 4.1.5 Norms on Cartesian products. Let í/¿ be a normed space for 
1 < i < k, and let X = Ui x ■ ■ ■ x Uk- There are several natural ways to define a 
norm on X. The three most common are: 
M HKm, ...,ufc)|| = I M + •-. + 1111*11 
||x|| = |l(ui,...,u f e)|¡ 
= 
(||u1||2 + --- + ||ufe||2)1/2 
||x|| = ||(ui, ...,u f c)|| 
= 
max(||ui||, ..., IJUfclJ). 
It is easy to verify that these are indeed norms on X. 
Remarks 4.1.6 Non-Euclidean norms. There are many advantages to using a 
Euclidean norm when one is available. In some cases, however, there are other 
natural choices for a norm. The most important case is L(X, Y), the vector space of 
all linear maps T : X —> Y. We shall define a very useful non-Euclidean norm on 
this space. 
Definition 4.1.7 Equivalent norms. Let || • || and || • ||' be two norms on a vector 
space X. If there are two constants L, M 6 R such that, for all x e X, 
||x||<L||x||'and||x||'<M||x|!, 
then these are equivalent norms on X. 
We write || • || ~ || • ||' to indicate the 
equivalence of these norms. We can see easily that this is an equivalence relation 
among the norms on the vector spaceX. We will show below that any two norms on 
a (finite dimensional) vector space are equivalent. 

126 
NORMED VECTOR SPACES 
Remarks 4.1.8 Triangle and reverse triangle inequalities. Consider three points 
a, b, and c in a normed space. The inequalities 
| | a - c | | 
< 
| | a - b | | + ! | b - c | | 
(4.1) 
| | | a - b | | - | | a - c | | | 
< 
| | b - c | | 
(4.2) 
are used frequently. The first inequality (4.1) is just the triangle inequality 
||x + y||< ||x|H-||y|| 
applied to x = a — b and y = b — c; it is also referred to as the triangle inequality. 
The second inequality (4.2) is equivalent to the two inequalities 
| | a - b | | - | | a - c | | < | | b - c | | 
and ||a - c|| - ||a - b|| < ||b - c||. 
Each of these is a rearrangement of a triangle inequality. For easy reference, we will 
call the inequality (4.2) the reverse triangle inequality. 
Remarks 4.1.9 Geometric formulations. The triangle and the reverse triangle 
inequalities can be stated in terms of familiar geometrical concepts. If a and b are 
two points in a normed space X, then the set 
{ x e l | x = i a + ( l - i)b, 0 < t < 1 } 
is the line segment joining these two points. The length of this segment, defined as 
11 a — b 11, is the distance between the end points a and b. Hence the triangle inequality 
says that the length of one side of a triangle is dominated by the sum of the lengths 
of the other two sides. The reverse triangle inequality says that the length of one side 
of a triangle dominates the difference between the lengths of the other two sides. 
Problems 
4.1 
Let X be a vector space. If a, b are in X, the line segment joining a and b is 
the set 
L[SL, b] = { x e X [ x = i a + ( l - t)b, 0 < t < 1 } . 
Show that L[a, b] is convex: If u, v are in L[a, b], then su + (1 — s)v € L[a, b] for 
allO < s < 1. 
4.2 
If N\, N2 are norms on a vector space X, show that N\ + N2 is also a norm 
on X. What about the product NiN2? 
4.3 
Let j] || be the standard Euclidean norm on K™, and let ||x||' = |xi| + • • • + \xn\ 
for all x £ l " , Show that there are constants A, B such that 
A||x||' < ||x|| < S||x||' 
for all x e W1. 

PRELIMINARIES 
127 
(So these two norms are equivalent.) 
4.4 
Let X, Y be vector spaces and suppose that / : X —> Y is an isomorphism. 
Let || ||x be a norm on X and define 
llyll = IWIx 
whenever y = /(x) for (unique) x in X. Show that || || is a norm on Y. 
4.5 
Let || || be a norm on X and let x, y be in X. Assume that ||3x — y|| < 1 and 
||5x - 4y|| < 3. Show that ||y|| < 2. 
4.6 
Let X be a normed space. For a G X and r > 0, show that Br{a) = 
{ x G X | ||x — a|| < r } is convex. 
4.7 
Let AÍ be a subspace of a normed space X. If there is some a G X and some 
r > 0 such that £?r(a) C M, show that M = X. 
4.8 
Is it true that for any nonzero normed space X, the set Br(a) is infinite for all 
a G X and all r > 0 ? 
4.9 
Let X be a normed space. Let a G X and r > 0. Show that 
- ( - a + ß r ( a ) ) = ß 1 ( 0 ) . 
4.10 
Let || || be a norm on X. Define /(x) = ||x||2 for all x G X. Is / a norm on 
X? 
4.11 
Let X be a vector space with dim X = 1. Given a nonzero e G X and a norm 
|| || on X, there is some positive constant C such that ||ae|| = ¡a|C for all a G R. 
True or false? 
4.12 
Let || || be the standard Euclidean norm on R", and let ||x ||" = max(\x\\, ■ ■ ■ ,\xn\) 
for all x G W1. Show that this defines a norm, and that it is equivalent to the standard 
Euclidean norm. 
4.13 
Show that, for any real x, y, and z, 
^{x + 2y)2 + {y + 2z)2 + (z + 2x)2 
< 
y/(x - yf + {y - z)2 + [z - xf 
+ 
3y/x2 + y2 + z2. 

128 
NORMED VECTOR SPACES 
4.2 CONVERGENCE IN NORMED SPACES 
The definitions and notation for representing a sequence are as defined in previous 
chapters. Our immediate objective is to define the convergence of a sequence in a 
normed space. 
Convergent Sequences 
Recall that a sequence rn in R is called a zero sequence if rn —> 0. More explicitly, 
rn is a zero sequence if for each e > 0 there is an N e N such that \rn\ < e whenever 
n>N. 
Definition 4.2.1 Convergent sequences. A sequence x n in a normed space X is 
said to converge to a point a 6 X if 
for each e > 0 there is an N e N such that ||xra — a|| < e for all n> N. 
Equivalently, rn = ||xn — a|| is a zero sequence in R. This condition is expressed as 
x n —► a, or as limn x„ = a. If xra —> a, then x n is a convergent sequence and a is 
the limit of this convergent sequence. This terminology is justified by Lemma 4.2.2 
below. 
Lemma 4.2.2 A sequence cannot converge to two different points. 
Proof. Assume that x n —> a and x„ —> b. The triangle inequality 4.1.8 shows that 
| | a - b | | < | | a - x „ | | + | | b - x n | | ^ 0 . 
Hence ||a — b|| = 0, and so a = b. 
□ 
Lemma 4.2.3 Ifxn —> a in X, then ||x„|| —> ||a|| /n K. 
Proof. The reverse triangle inequality 4.1.8 shows that 
| | | x n | | - | | a | | | < | | x „ - a | | . 
Since ||x„ — a|| —> 0, we see that ||x„|| —> ||a|| in K. 
□ 
Definition 4.2.4 Bounded sets. Let B be a set in a normed space X. Then B is 
called a bounded set if there is an M G R such that ||x|| < M for all x e B. 

CONVERGENCE IN NORMED SPACES 
129 
Definition 4.2.5 Bounded sequences. Let A" be a normed space. A sequence x n in 
X is is called a bounded sequence if its range is a bounded set. More explicitly, x n is 
called a bounded sequence if there is an M £ R such that ||x„ || < M for all B É N , 
Example 4.2.6 Let c/t be a bounded sequence in R and let u¿ be a bounded sequence 
in Rn. Define a sequence x^ in Kn by 
xfc = (l//c)(ciui + ---+cfeUfe), 
fceN. 
Then Xfc is also bounded. Indeed, since Ck and u^ are bounded, there is a constant 
C such that \CJ\ < C and \\UJ\\ < C for all j G N. Then for all k e N, the triangle 
inequality gives 
llXfcll 
= 
l l ( l / f c ) ( c i U l + - " + C f c U f c ) | | 
< 
( l / f c ) ( | c i | | | u i | | + - - - + | C f e | | | U f e | | ) 
< 
(l/k){kC2) 
= C2. 
Theorem 4.2.7 Every convergent sequence is bounded. 
Proof. Letxn —> a. Then by Lemma 4.2.3, ||xn|| —> ||a||. Hence, by Lemma 2.3.4, 
(||x„||) is a bounded sequence of real numbers. 
G 
Bolzano-Weierstrass Theorem 
Remark 4.2.8 Bolzano-Weierstrass theorem. The converse of the last theorem is 
obviously false. As a simple counterexample, let a be any nonzero vector and define 
xra = (—l)na. Then x„ is bounded but does not converge. However, it turns out that 
in any finite-dimensional normed space, every bounded sequence has a convergent 
subsequence. This is the Bolzano-Weierstrass theorem, a key result in analysis. 
The Bolzano-Weierstrass theorem has already been proved for sequences in R. To 
extend it to arbitrary normed spaces, it turns out that the best strategy is first to prove 
it for the special case of Euclidean spaces. The general result will follow from this 
special case once we prove two additional results: the equivalence of all norms on a 
finite-dimensional normed space, and the fact that a Euclidean norm can always be 
defined on any finite-dimensional vector space. 
Theorem 4.2.9 Bolzano-Weierstrass theorem in Euclidean spaces. A bounded 
sequence in a Euclidean space has a convergent subsequence. 
Proof. Let X be an n-dimensional Euclidean space. We use an induction argument on 
n. The result is true for n = 1 by Theorem 2.4.5. Now assume the result for (n — 1)-
dimensional spaces, n > 2. Let{ei, ..., e„} bean orthonormal basis for X. Letx/. 

130 
NORMED VECTOR SPACES 
be abounded sequence in X. Let r^ = (e„, Xfc). Then |r*fc| < ||e„|] ||xfc|| = ||xfc||. 
Hence, r^ is a bounded sequence in M. Therefore, it has a convergent subsequence 
rm, m € K. Here K is an unbounded subset of N. 
Now the sequence uTO = x m — r me„ is a bounded sequence in the subspace spanned 
by the vectors { e1; ..., e„_i}. This subspace is an (n — 1)-dimensional Euclidean 
space. Therefore, by the induction hypothesis, u m has a convergent subsequence u¿, 
£ G h. Here L is an unbounded subset of K. The subsequence r¿, £ 6 L, is still 
convergent as a subsequence of a convergent sequence. Hence, x¿ = u¿ + r¿en, 
£ e L, is a convergent subsequence of x^. 
O 
Equivalence of Norms 
We will now show that any norm on a finite-dimensional vector space is equivalent 
to a Euclidean norm. This implies the stronger-sounding result that any two norms 
are equivalent. 
First, recall that one can define a Euclidean norm on any finite-dimensional vector 
space X. Take any basis E = { ei, ..., e„} for X and let 
\\X\\ = (x1(x)2 + --- + 
xn(x)2)1'2, 
where x¿ : X —► R are the coordinate functions for the basis E. This is a Euclidean 
norm induced by the inner product defined as 
(u, v) = xi(u) xi(v) H 
V x„(u) 
xn{\) 
for all u, v G X. The basis E is an orthonormal basis in this inner product. We shall 
assume that one such Euclidean norm is fixed in the following discussion. 
Notations 4.2.10 Suppose X has a Euclidean norm || ■ ||euc : X —> R and also 
another norm || • || : X —> K. Formally, we then have two different normed spaces, 
which we denote as (X, \\ ■ ||euc) andas (X, ||-1|). We write that x„ —> a in (X, ||-||) 
just in case ||x„ — a|| —> 0. 
To show that any norm is equivalent to the Euclidean norm involves proving two 
inequalities, as defined in Definition 4.1.7. We prove these as two separate lemmas. 
Lemma 4.2.11 There is an L £ l such that ||x|| < L ||x||euc/or all x G X. 
Proof. Let E = { ei, ..., e n} be an orthonormal basis for X for the given Euclidean 
norm. Let x¿ : X —> R be the coordinate functions for E. Hence 
x = xi (x) ei + ■ • • + x„(x) e. 

CONVERGENCE IN NORMED SPACES 
131 
Here |a;¿(x)| < ||x||euc for all x G X and for alH = 1, ..., n. Therefore 
||x|| 
< 
|z1(x)|||e1|| + -.. + |a:„(x)|||en|| 
< 
(||ei|| + --- + ||en||)||x||euc = L||x||euc 
withi = ||ei|| H 
h ||e„||. 
□ 
Corollary 4.2.12 Ifxn —> a in (X, \\ ■ ||euc), then also x n —> a in (X, || • ||). 
Proof. We have ||x„ - a|| < L \\ 
x n 
— a||euc ->0. 
ü 
Lemma 4.2.13 There is an M e R such that ||x||euc < M ||x|| for all x e l 
Proof. We first show that there is an m > 0 such that 
m < ||x|| whenever ||x||euc = 1. 
(4.3) 
If (4.3) is false, then there is a sequence x„ G X such that ||xn||euc = 1 and 
Ilxn|| -*■ 0. But x n is a bounded sequence in (X, || • ||euc). Hence by the Bolzano-
Weierstrass theoremin Euclidean spaces (Theorem 4.2.9), x n has a subsequence x^n 
such that 
xfc„ -* ain(X, || • ||euc). 
Therefore, by Corollary 4.2.12 above, also 
x f c „ ^ a i n ( X , |j-||). 
Hence, by Lemma 4.2.3, ||xfcJ|euc -> ||a||euc and ||xfcJ| -^ ||a||. Then ||a||euc = 1 
since each ||xfcn||euc = 1 and ||aj| = 0 since ||xn|| —» 0. But ||a|| = 0 means that 
a = 0 and |ja||euc = 1 means that a ^ 0. This contradiction shows that (4.3) is true. 
Now let x s X. If x = 0, then the conclusion of the lemma is clear. Assume that 
||x||euc = t > 0. Then ||(l/i)x||euc = 1. Hence m < ||(l/i)x|| by (4.3). This 
means that t = ||x||euc < (l/m)||x|| = M||x||, where M = 1/m. 
D 
Theorem 4.2.14 Any two norms on a vector space are equivalent. 
Proof. Let || • ||¿, i — 1, 2, be any two norms, and let || ■ ||euc be a Euclidean norm 
on a vector space X. Use Lemmas 4.2.11 and 4.2.13 to find L¿, M¿ G R such that 
||x||, < Li ||x||euc and [|x||euc < Mi ||x||¿ for i = 1, 2 and for all x G X. 
Then |lx||i <LiM 2||x|| 2and||x|| 2 < L2 M1\\x\\1 for all x G X. 
D 

132 
NORMED VECTOR SPACES 
Theorem 4.2.15 A set in a vector space is bounded with respect to one norm if and 
only if it is bounded with respect to any other norm. A sequence x„ in a vector space 
X converges to a with respect to one norm if and only if it also converges to a with 
respect to any other norm. 
Proof. Let ||-¡|and ||-II'be two norms on X. Let || • || < L\\ ■ ||' and || • ||' < A/|| • ||. 
If a set A in X is bounded in || • ||, then there i s a K e t such that ||a|| < K for all 
a e A. In this case ||a||' < KM for all a e A Hence, A is also bounded in || • ||'. 
The proof for convergence is similar, 
n 
Theorem 4.2.16 Bolzano-Weierstrass theorem in normed spaces. Every bounded 
sequence in a normed space has a convergent subsequence. 
Proof. Let || • || be the given norm on X. Let || • ||' be a Euclidean norm on X. If 
x n is bounded in (X, \\ ■ ||), then by Theorem 4.2.15 it is also bounded in (X, || • ||'). 
By the Bolzano-Weierstrass theorem in Euclidean spaces, 4.2.9, it has a subsequence 
converging in (X, \\ ■ ||'). Applying Theorem 4.2.15 again, this subsequence also 
converges in (X, \\ ■ ||). 
□ 
Sequences in Cartesian Product Spaces 
In addition to the Bolzano-Weierstrass theorem, the following basic result is a second 
consequence of the equivalence of all norms on a finite-dimensional normed space. 
It tells us that convergence of a sequence in a product space is equivalent to the 
convergence of all of the component sequences. 
Theorem 4.2.17 Let U\, ■ ■ ■ Un be normed spaces and let X — U\ x • • • x Un. Let 
x.k,i °e a sequence in Ui and let 
Xfc — ( XM> 
• • • xfc,n) 
be the corresponding sequence in X. Then 
limfexfc = x = (xi, ..., x„) in X 
if and only if each liirifc x ^ = x¿ in Uifor each i = 1, ..., n. 
Proof. Convergence is independent of the norm used in a space. This follows from 
Theorem 4.2.15 above. A convenient norm to use in X is 
||x|| = ||(x 1,... )x n)||=ma X(||x 1||,...,||x n||). 
It is easy to see that this is indeed a norm on X. Then it is clear that 
||xfc - x | | = max(||xM - x i | | , ..., ||xfci„ - x n | | ) -*• 0 
if and only if ||xjt ¿ — x¿ || —» 0 for each i = 1, ..., n. 
□ 

CONVERGENCE IN NORMED SPACES 
133 
Cauchy Sequences 
Definition 4.2.18 Cauchy sequences in normed spaces. A sequence x n in a normed 
space X is called a Cauchy sequence if for each e > 0 there is an integer N £ N 
such that ||x„ — x m || < e for all m, n > N. 
Theorem 4.2.19 A sequence in a normed space is a Cauchy sequence if and only if 
it is a convergent sequence. 
Proof. This proof is almost the same as the proof of the analogous Theorem 2.4.7 for 
Cauchy sequences of real numbers. First, suppose that x„ is a convergent sequence 
and let x n —»• a. Given e > 0, find JVeN such that ||x„ - a|| < e/2 for all n> 
N. 
Then 
||x„ - x m|| < ||x„ - a|| + ||a - x m|| < (e/2) + (e/2) = e 
for all 771, n > N. Hence x n is a Cauchy sequence. 
Conversely, assume that x„ is a Cauchy sequence. Since x n must then be bounded, 
we can apply the Bolzano-Weierstrass theorem to find a convergent subsequence x„,. 
Let a = lim xn(c. Then for each e > 0 there is a K such that ||x„fc — a|| < e/2 for all 
k> K. Since x„ is Cauchy, we may take K large enough so that ||xn — x m|| < e/2 
for all m, n> K. Pick some n^ large enough that both k, n^ > K. If n > K, then 
||x„ - a|| < ||xn - xnfc || + ||xnjt - a||. 
But ||x„ — x„fc|| < e/2 since n, n^ > K. Also ||x„fc — a|| < e/2 since k > K. 
Therefore ||x„ — a|| < e for all n > K. Hence x n —> a. 
□ 
Example 4.2.20 Let a be a nonzero vector in a normed space X. 
Let tn be a 
sequence of real numbers. Then tna converges if and only if tn converges. Indeed, 
since ||a|| > 0 and \tn — tm\ ||a|| = ||ína — íma||, we see that fna is Cauchy if and 
only if tn is Cauchy. Hence, by Theorem 4.2.19, i na converges if and only if tn 
converges. 
Theorem 4.2.21 Let u„ be a sequence in a normed space X. Let xra = X^"=iu¿ ^e 
the sequence of partial sums. If J^n ||un || < oo, then x n is a Cauchy sequence in X. 
7/limnx„ = a, then ||x„ - a|| < ¿ i > n ||u¿||. 
Proof. The sequence Sn = X^"=illurill ^s a monotone and bounded sequence in M. 
Therefore it converges by the monotone convergence theorem, Theorem 2.3.13. It 
follows that Sn is a Cauchy sequence in R. Hence, given e > 0, there is an A^ G N 
such that 0 < Sn+k — Sn < e for all n> N and for all k s N. Hence 
||x n-x n + f e|| 
< 
||x„-x n +i||H 
1-Hx.j+fc-i-xn+fc|[ 
= 
||un+l|| + ' ' • + ||u„ + fe|| = Sn+k — Sn < £ 

134 
NORMED VECTOR SPACES 
whenever n > N and k £ N. This proves that x„ is a Cauchy sequence. 
For the last part we have 
lirm ||xn - xn+fc|| = ||x„ - a|| < limfc(Sn+fc - Sn) = V \ 
||u¿ 
z — ' l > n 
Here the first equality follows from Lemma 4.2.3 and from the observation that 
limfc(xn - x„+fc) = x„ - a. 
ü 
Problems 
4.14 
If A, B are bounded subsets of a normed space X, show that r A + sB is also 
a bounded subset of X for any scalars r, s. 
4.15 
Let xra be a sequence in a normed space. Let / : N —> N be an increasing 
function. If xra converges, show that the sequence x^(n) also converges. 
4.16 
Let x„ and y n be convergent sequences in a normed space X. Let t, s be 
scalars. Show that the sequence sx n + tyn is convergent and 
lim (sxn + fy„) = s lim x„ + t lim y n. 
n—>oo 
n—>oo 
n—>oo 
Also, show that 
lim ||xn 
n—>oo 
lim x^ 
4.17 
Define norms || || and || ||' on R2 by 
||x|| = |xi| + |x2|, 
||x||'= max{|xi|,|x2|} 
for all x = {xlt x2) £ »2 
Show directly that || J| and || ||' are equivalent norms on >2 
4.18 
Let xra be a sequence in a normed space X. For each k £ N, let yfe = x2fc 
and let Zfc = x2fc_i. Show that x n converges to a if and only if both yn and z„ 
converge to a. 
4.19 
Let X be an inner product space, and let || || be the norm induced by the 
inner product. Suppose that x„ is an orthogonal sequence in X that converges. Find 
iimn—too x n. 
4.20 
Let Xfc be a sequence in a normed space. For n £ N, set 
E
n 
^—rn 
fc=1(xfc+i -x f e), 
dn =--l^k=1\\xk+i 
-Xfc||. 

NORMS OF LINEAR AND MULTILINEAR TRANSFORMATIONS 
135 
Show that tn converges if and only if x^ converges. If x& converges, must dn 
converge? 
4.21 
Suppose that a„ is a sequence in a normed space that converges to some a. 
Show that 
lim - (ai H 
ha n) = a. 
n ^ o c ft 
'X.ll 
For fc = 1,..., n, let || • ||fc be norm on X^. Is it true that there is a constant 
C such that whenever xj e X\,..., 
x^ e l j , then 
||x1||f + --- + ||x n||2<Cmw(||x f c|| f e)? 
l<fc<n 
4.23 
Let X be a vector space with bases {ui,..., u m}and{vi,..., v m }. Suppose 
S C X and assume that there is a constant M such that whenever x <E S with 
x = ciUi + ■ • ■ + c mu m, then maxi<fc<m |cfc| < M. Show that there is a constant 
C such that whenever x £ S and x = divi + • • • + dmvm, then 
\di\ + ■ ■ ■ + \dm\ 
<C. 
4.24 
Show that there is a positive constant A such that 
j . 2 _i_ . . . _i_ j.2 
n 
, i 1 i , 
, ? 
h 2 > -4 for all x e l " with maxi<fe<n \xk\ = 1. 
4.25 
Let a^ = (ajt^, afc,i, ■ • •, ak,n) be a bounded sequence in Mn+1. Show that 
there is an increasing sequence m^ in N, and a polynomial p of degree no more than 
n such that 
lim max |amfci0 + amktit H 
hamt,nt™ - p(t)\ - 0. 
fe—>oote[o,i] 
4.3 
NORMS OF LINEAR AND MULTILINEAR TRANSFORMATIONS 
A linear map T : X —> Y between two normed spaces has an important property 
called boundedness. This property leads to the definition of a natural norm on the 
vector space L(X, Y) of all linear maps X —» Y. 
Theorem 4.3.1 Boundedness of linear transformations. Let X and Y be two 
normed spaces. Let T : X —► Y be a linear transformation. Then there is an M £M. 

136 
NORMED VECTOR SPACES 
such that \\Tx\\y < M \\x\\xfar allx e X. Here, \\ \\x and\\ ||y denote the norms 
on X and Y. 
Proof. Define |]x|| = ||Tx||y + ||x||x for all x G X. Then || • || is a norm on X. 
(What goes wrong if we try ||x|| = ||Tx||y?) Hence, || • || and || • ||x are equivalent. 
Thus, there is a constant C such that 
||Tx||y + ||x|| x<C||x|U 
for allx e X . 
Thus, with M = C - 1, the result follows. 
G 
Example 4.3.2 Let X and Y be two normed spaces. Let T : X —> Y be an 
isomorphism (Definition 3.1.45). Then here are positive constants A, B such that 
A\\x\\ < \\Tx\\ <B\\x\\ 
for allx e X . 
This follows immediately from the fact that ||x||' = ||Tx||y is a norm (since T is an 
isomorphism), and hence equivalent to ||x||. 
Theorem 4.3.3 Norms of linear transformations. Let X and Y be two normed 
spaces. Let T : X —> Y be a linear function. Then 
¡|Tjj=SUp{||Tx||j||x|| = l, xeX} 
(4.4) 
exists and defines a norm on L(X, 
Y). 
Proof. If T e L(X, Y), then Theorem 4.3.1 shows that there is an M e K such that 
||Tx|| < M whenever ||x|| = 1, x e X. Hence the set in (4.4) is contained in the 
interval [0, M\. Therefore ||T|| exists. 
We check that ||T|| satisfies the defining conditions for norms stated in Definition 
4.1.1. Clearly ||T|| > 0 for all T e L(X, Y) and ||T|| = 0 if and only if T = 
0¿(x,y). that is, if and only if Tx = 0y for all x e X. 
Hence the positive 
definiteness of ||T|| follows. Now let T e L{X, Y) and t e E. Then 
||ÍT|| 
- 
s Up{||íTx¡||xeX, 
||x|| = l } 
= 
s u p { ¡ í | | | T x | | | x e X , ||x|| = l } 
= |í|suP{||Tx|||xex, ||x|| = i} = |í|||r||. 
This shows that ||T|| is homogeneous. Note that the third of these equalities depends 
upon the following observation: if B is a bounded set of numbers, r > 0, and 
B' = {rs | s e B }, then sup B' = r s u p ß . Finally, to verify the triangle inequality, 
let T, S e L(X, Y) and x e X , ||x|| = 1. Then 
\\(T + S)x\\ = \\Tx + Sx\\ < \\Tx\\ + \\Sx\\ < \\T\\ + \\S\\. 

NORMS OF LINEAR AND MULTILINEAR TRANSFORMATIONS 
137 
Hence ||T + 5|| < ||T|| + ||5||. 
□ 
Definition 4.3.4 Standard norms of linear transformations. The standard norm 
on L(X, Y) is the norm defined in the preceding theorem, in terms of the norms 
on X and Y. Hence, with this standard norm, ||Tx||y < ||T||L(X, y) Ilxl|x f°r all 
T G L(X, Y) and for all x G X, y G Y. 
Example 4.3.5 Let T : R2 -> M2 be given by Tu = (x + y, x - y) for all u = 
(x, y) £ R2. Then with respect to the standard Euclidean norm on R2, 
||Tu||2 = (x + yf + (x - yf = 2(x2 + y2) = 2||u||2. 
Hence, ||T|| < y/2. Since ||Tei|| = y/2, it follows that ¡|T|| = y/2. 
Example 4.3.6 Let X, Y be (finite-dimensional) vector spaces. Let || ||x, || ||x and 
|| ||v, || ||y be pairs of norms on X and Y, respectively. Then there is a constant C 
such that if T : X —> y is linear and ||T|| is the norm of T with respect to || ||x, || ||y, 
then the norm ||T||' of T with respect to || ||^ and \\ \\'Y satisfies 
||T||'<q|T||. 
The reason is that L(X, Y) is a finite-dimensional normed vector space under the 
two different norms T u ||T|| and T i-> ||T||'. Consequently, the norms || || and || ||' 
on L(X, Y) are equivalent, and the desired conclusion follows. 
Theorem 4.3.7 IfT e L(X, Y), then ||Tx|| < ||T|| ||x||/ora//x G X. 
Proof. If ||x|| — 1, then the result follows from the definition of ||T|| in Theorem 
4.3.3 above. Any x e X can be expressed as x = ix0 with i e R and ||x0|| = 1, 
and in this case ||x|| = |i| ||xo|| = |i|. Therefore 
||Tx|| = ||T(íxo)|| = |í|||Tx 0||< |í|||T|| = ||T||||x|| 
as claimed. 
D 
Example 4.3.8 Let T : X —» Y be a linear map between normed spaces. If 
Cauchy sequence in X, then Txn is also a Cauchy sequence in Y. This is because 
||Txn - Tx m|| = ||T(x„ - xm)|| < ||T|| ||xn - xm|| for all n, m. 
Theorem 4.3.9 IfR e L(X, Y) and S e L{Y, Z), then \\SR\\ < \\S\\ \\R\\. Here 
SR G L(X, Z) is the composition of R and S. 

138 
NORMED VECTOR SPACES 
Proof. If x G X, then 
||(Sfi)x|| = ||S(Äx)||<||5||||Äx||<||5||||Ä||||x||. 
This shows that jj (SR)x\\ < \\S\\ \\R\\ whenever ||x|| = 1. D 
Example 4.3.10 Let X, Y be nontrivial normed spaces. Let T : X —> Y be an 
isomorphism. Then ly = TT_1 is the identity mapping on Y. Clearly, ||/y|| = 1 
(since Y is nontrivial, there is a non-zero vector y with iy(y) = y). Thus, 
l = \\IY\\ = 
\\TT-'\\<\\T\\\\T-'\\. 
Also, since T is not the 0 mapping, ||T|| > 0. Hence, 
(l/HTiD^iir-i. 
Norms on Multilinear Functions 
Let Ui and Y be normed spaces. It will be convenient to write Xk = Uk x ■ ■ ■ x U\ 
and let MLk(Xk, 
Y) denote the corresponding vector space of fc-linear functions. 
There is a standard way to define a norm on MLk(Xk, 
Y), using induction on k. 
For the induction step, we use the natural isomorphism 
■d : MLk+1(Xk+1, 
Y) - L(Uk+1, MLk(Xk, 
Y)) 
defined in Definition 3.3.7. If F e MLk+i(Xk+i, 
Y), then F is a function of 
(A; + 1) variables Ufc+i, u^, ..., ui. For each u^+i e Uk+\, we obtain $F(uk+i) 
by holding Ufc+i fixed and considering F as a function of the remaining k variables. 
That is: 
i?F(ufc+1)(ufc, ...,u1) 
= F(uk+i, ufc, ..., ui). 
This defines ^F(u^ +i) G MLk(Xk, 
Y) for each Ufc+i e Uk+\. 
Definition 4.3.11 Norm on MLk(Xk, 
Y). For k — 1, the norm on 
ML 1(X 1,y)=L(C/ 1,F) 
is just the standard norm on L{U\, Y). For the inductive step, we may suppose that 
the norm on MLk(Xk, 
Y) is defined. We know that there is a standard norm on 
L(Uk+1,MLk(Xk,Y)). 
If F G MLk+i (Xk+i ■ Y), then we define 
\\F\\MLk 
+ 1(Xk + 1,Y) = \\tiF\\L(Uk + 
1,MLk(Xk,Y))-

NORMS OF LINEAR AND MULTILINEAR TRANSFORMATIONS 
139 
These norms on MLk(Xk, 
Y) are defined as the standard norms on these spaces; as 
in the case of linear transformations, they depend upon the norms on the underlying 
vector spaces. 
Remarks 4.3.12 The following theorems refer to several normed spaces and several 
norms, all of which should be clear from the context. To illustrate, suppose F E 
MLk+1{Xk+1, 
Y). Then 
||F|| 
is the standard norm on 
MLk+i(Xk+i, 
Y). 
||F(ufe+i, Ufc, ..., ui)|| 
is the norm on 
Y. 
\\0F\\ 
is the norm on 
L{Uk+1, MLk(Xk, 
Y)). 
||i?F(ufe+i)|| 
isthenormon 
MLk(Xk,Y). 
||i9F(ufe+i)(ufe, ..., ui)|| 
isthenormon 
Y. 
The following theorem shows that the standard norm for multilinear functions has a 
useful property analogous to that of the standard norm for linear functions. 
Theorem 4.3.13 IfF G MLk{Xk, 
Y), then 
\\F(X)\\ = \\F(uk,...,u1)\\<\\F\\-\\uk\\---\\u1\\ 
(4.5) 
for all*. = (ufc, ..., ui) e Xk = Uk x • ■ • x Ui. 
Proof. If k = 1, then (4.5) is true by Theorem 4.3.7. Suppose, for induction, 
that (4.5) is true for k € N. Let F € MLk+\(Xk+\, 
Y). 
Then for all x = 
(ufc+i, ufc,..., ui) e 
xk+i, 
\\F(x)\\ 
= 
||F(ufc+i,ufc, ..., m)|| 
= 
||t?F(ufc+1)(ufc, . . . , u i ) | | 
< 
II^K+OH-llufcll-.-lluill 
< 
||^!|.||u f c + 1||-||u f c||--.||u 1|| 
= 
||F|!.||ufc+1||.||ufc||---||u1||. 
Here the first inequality follows from the inductive hypothesis and the second in-
equality from Theorem 4.3.7. The last equality follows from the definition of |[F|j in 
Definition 4.3.11. 
D 
The next theorem will be useful in our study of differentiation. 
Theorem4.3.14 Increments of multilinear maps. 
Let Ui,...,Uk 
and Z be 
normed spaces, let Xk = Uk x • • • x U\, and suppose that F s MLk{Xk, 
Z). 
Let 
x = (ufc, . . . , u 1 ) 6 l t a « i i y = (vfc, ..., vx) e Xk. 

140 
NORMED VECTOR SPACES 
If || u, || < R, ||vj|| < ñ, ana ||u¿ — v¿|| < a for each i = 1, ..., k, then 
\\F(x) - F(y)\\ < \\F\\ ■ kR1*-1 a. 
(4.6) 
Proof. Proceed by induction on A: 6 N. If fc = 1, then (4.6) reduces to 
||F(ui) - F ( v j ) | | < \\F\\a for linear F : Ux -^ Z. 
This follows from Theorem 4.3.7. Assume for induction that (4.6) holds for k. Let 
F £ MLk+i(Xk+i, 
Z). To simplify the notation, let's write 
(ufc+i, ufe, ..., ui) = (ufc+i, u) and (vfe+i, vfc, ..., vi) = (vfc+i, v). 
Note that ||F(ufc+i, u) - F(vk+\, v)|| is dominated by the sum 
||F(ufc+1, u) - F(ufe+1, v)|| + ||F(ufc+1, v) - F(vfc+1, v)||. 
We estimate these two terms separately. For the first term we have 
||F(ufc + 1,u)-F(ufc + 1,v)|| 
= 
||0F(u f c + 1)(u)-0F(u f e +i)(v)|| 
< 
\\{>F(uk+1)\\ ■ kR**-1 a 
< 
\\0F\\ ■ \\uk+1\\ ■ kR"-1 a 
< 
\\F\\-kRka. 
Here the first inequality follows from the induction hypothesis. The second inequality 
follows from Theorem 4.3.7 applied to the linear map i?F. For the second term, 
||F(u f c + 1,v)-F(vfe + 1,v)|| 
= 
||tfF(ufc+1)(v)-0F(vfc+1)(v)|| 
= 
||(t?F(ufc+1)-ÖF(vfc+1))(v)|| 
< 
||!?F(ufc+1-Vfc+i)||-Äfc 
< 
||^F||-||ufc+1-Vfe +1||-i?fe 
< 
||F||-ñ f ca. 
The first inequality follows from Theorem 4.3.13. In the second inequality, we again 
apply Theorem 4.3.7 to the linear map dF. Adding these two estimates, we obtain 
(4.6)for(fc + l). 
□ 
Example 4.3.15 Theorem 4.3.14 generalizes a familiar situation. Let each Ui — K 
with the absolute value norm. Define F € ML¡c(Xfc, R) by 
F(ui, ...,Uk) 
= 
u\---uk. 
We can easily show that ||F|| — 1 by induction on k. Hence, if |w¿|, |u¿| < R and 
\UÍ — Vi\ < a, then Theorem 4.3.14 gives 
l(ui---itfc) - {vi---vk)\ 
< 
kRk~1a. 
In particular, \uk — vk\ < kRk~la 
if |u|, \v\ < R, \u — v\ < a. 

NORMS OF LINEAR AND MULTILINEAR TRANSFORMATIONS 
141 
Problems 
4.26 
LetT: R2 -> R2 be given by Tu = (x + 2y,3x-y) 
for all u = (x,y) G R2. 
With respect to the standard Euclidean norm on R2, show that ||T|| < vTT. 
4.27 
Let X be a nonzero normed space and let T G L(X. Y) with T ^ 0. What 
is { ||Tx|| | x G X }? 
4.28 
Let T G L(X, y) with ||T|| < 1. Show that if u, v are vectors in the unit ball 
{ x e X | ||x|| < 1 }, then ||Tu - Tv|| < 2. 
4.29 
Let T : (X, || ||) —> Y be a linear map between normed spaces. Let L : X —* 
X be an isomorphism such that ||Lx|| = 1 for all x G X with ||x|| = 1. Define a 
normj|||'onXby||x||' = ||Lx|| forallx G X. If A is the norm of T : (X, || ||) -> y, 
show that the norm BoiT 
: (X, || ||') -*■ y satisfies B > A. 
4.30 
Consider Rn with the norm ||x|| = [x\ + ■ ■ ■ + x2
n)1/2. 
Let 
cn,...,an 
be constants and define T : R™ —> R" by Tx = (a\X\,... ,anxn) 
for all x = 
(xi,...,x„) E l " . Show that ¡|T|| > maxi<fe<„ \ak\. 
4.31 
For k = 1,..., n, let Tk : y —» Xfc be linear maps between normed spaces. 
Let || \\k be the norm on X^. Consider the norm || ||' on X = X\ x ■ • • x Xk given 
by 
||(x 1,... Ix n)||' = (||x1||2 + --- + ||x n|| 2)i 
Let T : Y -> X by Ty = (Tiy,..., Tny) for all y £ Y . Show that T is linear. Is it 
true that 
im|2<||r1||2 + --- + ||rn||2? 
4.32 
Suppose that X, Y are normed spaces. Let Tn G L(X, Y) be such that 
||T„|| < 1 for all n G N. Show that there is some T G L(X, Y) with ||T|| < 1 and 
some subsequence Tnt. of Tn such that 
\\T — T 
II —+ 0 
4.33 
Let X, Y be normed spaces. For x G X, define x : L(X, Y) —> Y by 
x(T) = T(x) for all T G L(X, y). Show that x is a bounded linear map with 
l|x|| < ||x||. 
4.34 
Let T G L(X, X). Recall that A is an eigenvalue of T if there is some u E l 
with u ^ O such that Tu = Au. Show that for all eigenvalues A of T, we have 
|A| < ||T||. 

142 
NORMED VECTOR SPACES 
4.35 
Let T e L(X, X) and suppose that there is some S e L(X, X) such that 
S2 = T. Then v t ^ I < ||5||. True or false? 
4.36 
Let X, Y be normed spaces and let L : X —> Y be linear. Define F : R x 
X -^ YbyF(r,x) = rLx for all (r, x) E l x l Compute ||F||. 
4.4 CONTINUITY IN NORMED SPACES 
There are two familiar definitions of continuity of a function at a point. We start with 
a lemma that proves the equivalence of these two conditions. 
Lemma 4.4.1 Let X and Y be two normed spaces, a € A C X, and f : A —» Y a 
function. Then the following are equivalent. 
(1) For each e > 0 there is a 6 > 0 such that ||/(x) — /(a)|| < e whenever 
||x — a|| < 5 andx € A. 
(2) Ifxn is a sequence in A and ifx„ —> a in X, then /(x„) —> /(a) m Y. 
Proof. Assume (1). Let x n be a sequence in A and assume that x„ —► a. Given 
e > 0, find 5 > 0 as in (1). Since x„ —> a, there is an iV such that ||x„ — a|| < S for 
all n > N. Hence, ||/(xn) - /(a)]| < e for all n > N. Therefore /(x„) -^ /(a) 
and (2) follows. 
Conversely, assume that (1) is not true. Then there is an a > 0 with the following 
property. For each 6 > 0 there is an x G A such that ||x — a|| < 6 but ||/(x n) — 
/(a) || > a. In particular, for each n e N there is x n such that 
| | x n - a | | < ( l / n ) b u t | | / ( x n ) - / ( a ) | | > a . 
Then x„ —* a but /(x n) -^ /(a). Hence (2) is not true. Therefore (1) and (2) are 
equivalent. 
□ 
Definition 4.4.2 Continuity of functions. Let X and Y be two normed spaces, 
a 6 A C X, and / : A —> Y a function. Then / is said to be continuous at a if / 
satisfies one of the equivalent conditions above in Lemma 4.4.1. If / is continuous 
at each a e i , then / is said to be continuous on A. 
Remarks 4.4.3 Continuity and norms. As proved in Theorem 4.2.15, the conver-
gence of a sequence in a vector space is independent of the choice of the norm on 
that space. As a result, the continuity of a function between the vector spaces is 
independent of the choice of norms on these spaces. 

CONTINUITY IN NORMED SPACES 
143 
Remarks 4.4.4 General theorems on continuity. Most of the functions we shall 
consider in this course are linear, multilinear, or obtained from linear and multilinear 
functions by composition, taking limits, or taking inverses. First, we establish the 
continuity of linear and multilinear functions. Then we show that continuity is 
preserved under composition, taking limits, and taking inverses, with some mild 
restrictions. This will establish the continuity of almost all the functions we consider 
in this course. Even with these functions, however, there may be some special 
points at which continuity is not clear. This happens especially in taking inverses 
(which includes division, the inverse of multiplication). The following three special 
examples illustrate how to proceed in such cases. The first example is a natural one; 
the others are contrived to reveal possible difficulties. 
Special Examples 
Example 4.4.5 Define / : R2 -> R by 
f(x, V) = 
xy/(x2 
+y2) 
0 
if (x, y) ¿ (0, 0) 
if (a;, y) = (Q, 0). 
Is / continuous at (0, 0)? The restriction of / to the x-axis or to the y-axis is 
identically the zero function. That is continuous everywhere. But we can draw no 
general conclusion about the continuity of / from these restrictions. Suppose we 
restrict / to the line y = mx, m ^ 0. This restriction is the function ip : R —> R 
defined as p(x) = f(x, mx) for all x & R. We see that ip(x) = m/(l + m2) if 
x T¿ 0 and <p(Q) = 0. Hence p is discontinuous at x = 0. Therefore / is also 
discontinuous at (0, 0). A 
Example 4.4.6 Define / : R2 -»• R by 
,, 
v i l 
if x T¿ 0 and y = 2a;2 
I(x,y)-^ 
0 
o t h e r w i s e 
See Figure 4.1. This function is discontinuous at (0, 0). We can see this by restricting 
/ to the parabola y = 2a;2. This restriction is the function <p : M. —> R defined as 
<p(x) = f{x, 2x2) for all x G R. We see that p(x) = 1 if x ^ 0 and <^(0) = 0. 
Hence p> is discontinuous at x = 0. Therefore / is also discontinuous at (0, 0). 
Note that unlike the previous example, the restriction of / to any straight line passing 
through (0, 0) is continuous at (0, 0). Certainly, the restrictions of/to the coordinate 
axes are identically zero. Now consider a line y = mx, m ^ 0. Then ¡p(x) = 
f(x, mx) is obtained as follows. First, i/?(0) = /(0, 0) = 0. Let x ^ 0. Then 
<p(x) = 1 if y = mx = 2x2, that is, if x = m/2, and p(x) = 0 if x ^ m/2. This 
function is continuous at x = 0. (It is discontinuous at x — m/2, but this is not 
important.) 

144 
NORMED VECTOR SPACES 
This example shows that a function may have continuous restrictions to all straight 
lines passing through a point but still be discontinuous at that point. 
A 
y=2x' 
y = mx 
Figure 4.1. For Example 4.4.6. 
Example 4.4.7 Define / : R2 
/(*, V) = { 
by 
(y - x2)(3x2 - y)/x4 
if x ^ 0 and x2 < y < 3x2 
0 
otherwise. 
This function is even more pathological than the one in Example 4.4.6. It is dis-
continuous at (0, 0) even though its restriction to any straight line is continuous 
everywhere. (Recall that the restriction of the previous function to y = mx was 
continuous at (0, 0) but discontinuous at (m/2, m2/2).) 
A 
Continuity of Linear and Multilinear Functions 
Theorem 4.4.8 Continuity of linear functions. A linear map T : X —> Y is 
continuous on X. 
Proof. Let M e M be such that ||Tx|| < M ||x||, x e l Let x n -► a. Then 
||Txn - Ta|| = ||T(x„ - a)|| < M ||xn - a|| ^ 0. 

CONTINUITY IN NORMED SPACES 
145 
Hence Tx„ -> Ta. 
ü 
Theorem 4.4.9 Continuity of multilinear functions. A multilinear map 
F:Xk 
= 
Ukx---xU1~^Y 
is continuous on Xk. 
Proof. As continuity is independent of the choice of the norms, we may choose a 
convenient norm on Xk. We will let 
||x|| = ||(ufc, ..., ui)|| = max(||ufc|j, ..., ||ui||). 
It is easy to check that this is a norm on Xk. Fix a e Xk. Choose an R G R so that 
||a|| < R. Let e > 0 be given. Choose S > 0 so that 
||F|| •fcñfe-1<5<e. 
Without loss of generality, assume that ||a|| + 5 < R. We now apply Theorem 4.3.14, 
which allows us to estimate the increment ||F(x) — F(a)||. If ||x — a|| < 6, then we 
see that this theorem gives 
| | F ( x ) - F ( a ) | | < | | F | | . f c ñ f c - 15 < e . 
Hence F : Xk —> Y is continuous at an arbitrary point a G Xk. 
Q 
Continuity in Cartesian Products 
Theorem 4.4.10 Let Xi s and Yi s be normed spaces and consider the Cartesian 
product spaces 
X = Xk x • • • x Xi and Y = Ykx 
■■■ xYi. 
Let Ai C Xi and A = Ak x • • • x A\. Let /¿ : Ai —► Yi be a function for each 
i = 1, ..., k. Define f : A —> Y as 
/(x) = (/ f c(xf c),...,/ 1(x1)) 
for all x = (xfc, ..., xi) S A. Then f is continuous at a = (ak, ..., ai) G A if 
and only if each /¿ is continuous at a¿ G Ai. 
Proof. Let x„ = (xkn, ..., x l n) G A be a sequence. Theorem 4.2.17 shows that 
this sequence converges to a in X if and only if each component xni converges to a¿ 
in Xi. Similarly 
/(x„) = (/fc(xfcn), ..., /i(xi„)) G Y 
converges to /(a) in Y if and only if each component /¿(x¿„) converges to /¿(a¿) in 
Y¿. Then the proof follows from the definition of continuity. 
G 

146 
NORMED VECTOR SPACES 
Continuity Under Compositions 
Let X, Y, Z, be normed spaces, and A c X, B c Y. Let / : A —> Y and g : B —> Z 
be two functions. Let /i = 5 o / be the composite function, h(x) = g(f(x)) for all 
x 6 C, where C = A n / _ 1 (.B) is the set of all x e .A such that /(x) 6 B. 
Theorem 4.4.11 If f is continuous at a G C and g is continuous at b = /(a), i/ien 
h = g o f is continuous at a. 
Proof. Let x n be a sequence in C such that x„ —► a. Then/(xra) = y„ —> b = /(a), 
since / is continuous at a. But y n is a sequence in B and y n —> b. Hence, 
/i(x„) = (/(y„) —> g(b) = /i(a), since 5 is continuous at b. This shows that h is 
continuous at a. 
□ 
Applications 
Most of the following results could be proven directly from the definitions of con-
vergence and continuity. We present them here as consequences of the preceding 
results about the continuity of linear and multilinear functions, the composition of 
continuous functions, and Cartesian products. 
Lemma 4.4.12 Let an —> a in K and x n —> x in a normed space X. 
Then 
a„xra —> ax in X. 
Proof. We apply Theorem 4.2.17 on sequences in Cartesian products. We see 
that (an, x n) E l x X converges to (a, x) in R x X. We then use the fact that 
multiplication by scalars, 
M :(Rx 
X) ^ X defined by M(r, u) = ru, 
is multilinear, and hence continuous. Therefore, M(an, x„) = o„x„ converges to 
M(a, x) = ax in X. 
□ 
Lemma 4.4.13 Ifun —> u and v n —> v ¿« X, i/ien (un + v„) —*• (u + v) in X. 
Proof. Theorem 4.2.17 shows that (un, v„) —> (u, v) in X x X. But addition 
jF(a, b) = a + b is a linear operation F : {X x X) —> X and therefore continuous. 
Hence F(u„, v n) converges to F(u, v) in X. This is the conclusion of the lemma. 
D 

CONTINUITY IN NORMED SPACES 
147 
Theorem 4.4.14 Continuity of linear combinations. Let X and Y be normed 
spaces. Let ri : A —*• R and fi:A—>Ybe 
continuous fiinctions, where A C X and 
i = 1, ..., k. Let f — Y^i fifi- Then f : A —> Y is continuous. 
Proof. 
Let a G A, x n G A, and x n —► a in X. Then r,(xn) —> r¿(a) in R 
and /¿(x„) —> /¿(a) in X. Then Lemmas 4.4.12 and 4.4.13 and an easy induction 
argument show that 
/( xn) = 5Z.i"i(xn)/i(xn) -+ ]T]. r¿(a)/¿(a) = /(a) 
in F. Hence / : A —» y is continuous at each a € A. 
O 
Theorem 4.4.15 Continuity of inner products. Let X be a normed space and Y 
a Euclidean space. Let f : A —► Y and g : A —> Y be continuous, A C X. Then 
p(x) = (/(x), g(x)) defines a continuous function p : A —> R. 
Proof. Define /i : A -> (F x F) by /i(x) = (/(x), s(x)), x e A. Then /i is 
continuous by Theorem 4.4.10 on continuity in Cartesian products. Define the inner 
product function P : (Y x Y) —> R by P(u, v) = (u, v), (u, v) G Y x F. Since 
P is multilinear, it is continuous. Then p = P o h : A —> R is the composition of 
two continuous functions and hence is continuous. 
D 
Example 4.4.16 Let Tn -> T in L(X, F) and x n -> x in X. Then T„xn -+ Tx in 
y. To see this, note that (Tn, x n) —> (T, x) in L(X, Y) x X, again by Theorem 
4.2.17. Also, the mapping 
F : {L(X, 
Y)xX)-*Y 
defined by F(T, x) = Tx G Y, for (T, x) G (L(X, Y) x X), is multilinear, and 
hence continuous. Therefore T„x„ = F(Tn, xn) —»• F(T, x) = Tx in Y. 
A 
Continuity of the Inverse Function 
Let X and Y be two normed spaces, A <z X. If a function / : A —► y is one-to-one 
on A, then it has an inverse function g : B —> X, where £? = /(.A). Assume that 
/ : A —> y is continuous. In general, g : B —> X is not continuous, as we show by 
means of two counterexamples. The first example is rather artificial; the second is 
more natural. 
Example 4.4.17 Let X = Y = R. Let 
A = {l/k\keN} 
c [0, 1]. 

148 
NORMED VECTOR SPACES 
Define / : A -> R by /(l) = 0 and /(1/fc) = 1/fc if k > 2. Then / : A -► K 
is continuous. Actually, any F : A —> R is continuous on A The reason is that if 
xn £ A is a sequence in A and if x n —> a e A then there is an N G N such that 
xn = a for all n > N. (Why?) Any convergent sequence in A must eventually be a 
constant sequence. Hence F{xn) —> F(a) for any F : ^4 —> R. 
Now the particular function / defined above is one-to-one on A. Let B — f(A), 
and let the inverse inverse of / be g : B —> R. We see that 0 £ B and the sequence 
yn = 1/n, n > 2 is a sequence in f? converging to 0 G B. But <?(i/n) = (l/n) -fc 
g(ß) = 1. Therefore g is discontinuous at 0 G £?. 
A 
Example 4.4.18 Let X = R, Y = R2, A = [0, 1). We denote the points in X as 
i e K and the points in F as (x, y) G 1R2. We define / : A —> R2 as 
/(í) = (COS2TT¿, sin27rí), 
0 < í < 1. 
Then i? = f(A) is the unit circle. We see that / is one-to-one and continuous on 
A. But the inverse function g is not continuous at (1,0) G B. To see this, define a 
sequence (xn, yn) as follows: 
, 
. _ J (cos27r/n, sin27r/n) 
if n is odd 
(x„, y„) = | ( c o s 2 7r ( n - l ) / n , sin27r(n-l)/n) 
if n is even. 
Then we see that (xn, yn) G .B converges to (1, 0) G B. But 
_ , 
\ — f V n 
if n is odd 
n ~ 5 ( X"' Vn) ~\{n- 
l)/n 
if n is even 
does not converge in R. Therefore g is not continuous at (1, 0). A 
These counterexamples are due to a deficiency in the domain A of the original 
function / : A —» Y. If A satisfies one natural additional condition, then the inverse 
of a continuous and invertible function on A is, in fact, continuous. This additional 
condition is called compactness. 
Definition 4.4.19 Compact sets. Let X be normed space and C c X. Then C is 
called a compact set if every sequence in C has a subsequence converging to a point 
inC*. 
Example 4.4.20 Let A, B be compact subsets of a normed space X. Then A + B is 
also compact. To see this, let c n be a sequence in A + B. Then there are sequences 
a„ in A and b„ in B such that c„ = a„ + b n for all n. Since A is compact, there is 
a subsequence a„ t of an that converges to a G A Now bTlfc is a sequence in _B, and 
by the compactness of B, it has a subsequence b m that converges to b G B. Since 

CONTINUITY IN NORMED SPACES 
149 
a m is a subsequence of a„fc, it must also converge to a. Thus, cmj is a subsequence 
of c„ and cTO = aTO + cmj —* a + b G A + B. Hence, A + B is compact. 
In the next section we will provide several equivalent formulations of compactness, 
including one that is easy to verify. Hence the following theorem is a useful and 
important result. 
Theorem 4.4.21 Let X and Y be two normed spaces. Let Abe a compact subset of 
X. Let f : A —> Y be a one-to-one and continuous function on A. Then the inverse 
function g : B —> X is continuous on B = f(A). 
Proof. Let b e B. Hence b = /(a) and a = g(b). We will show that g : B -> X 
is continuous at b. Let y n be a sequence in B and y n —» b. Let x n = g(yn). We 
must show that x n —> a. 
If x n -/+ a, then ||x„ — a|| -/* 0. In this case, there is a number a > 0 and 
a subsequence xjt, fc G K, such that ||xfc — a|| > a for all k G K. Now Xfc, 
fc G K, is still a sequence in the compact set A. Hence it has a subsequence 
Xf, < £ l , that converges to a point a' G 4. We see that ||a — a'|| > a > 0. 
Hence a' T¿ a and therefore /(a) / /(a'), since / is one-to-one on A. But then 
ye = /(xf) —> /(a) T¿ /(a'), which means that / is not continuous at a' G A. This 
contradiction shows that x n - > a . 
O 
Continuity Under Limits 
Consider a sequence of functions fn:A^Y 
defined on a set A in a normed space 
X and taking values in another normed space Y. Assume that the sequence / n(x) 
in Y converges for each x G A. In this case lim„ /„(x) = /(x) defines a new limit 
function / : A —> Y, and we say that fn converges pointwise to /. Suppose that 
each fn : A —> Y is continuous. Can we conclude that / is also continuous on Al 
Not in general, but we do get continuity if we assume uniformity of the convergence. 
We prove this result below, but first we offer a counterexample for the general case 
where we have only pointwise convergence. 
Example 4.4.22 A sequence of continuous functions with a discontinuous limit. 
Let X = Y = E and A = [0, 1]. Let fn(x) 
= xn for each n G N and for 
each x G A. Each fn is a polynomial and therefore is continuous on A. Also, 
f(x) = limn/ra(x) = limnx™ exists for each x G A. But f(x) = 0 i f 0 < a ; < 1 
and /(l) = 1. Hence f{x) is discontinuous at x = 1 G A. 
A 
Lemma 4.4.23 Let X and Y be two normed spaces and A C X. Let f : A^Y 
be 
a function with the following property: for each e > 0, there is a continuous function 

150 
NORMED VECTOR SPACES 
g : A —> Y such that ¡|/(x) — ff(x)|| < £ for all x G A. Then f : A —> Y is a 
continuous function on A. 
Proof. To show that / : A —> Y is continuous at a G A, we will show that for 
each e > 0 there is a ö > 0 such that ||/(x) — /(a)|| < e whenever x G A and 
||x — a|| < 5. Given e > 0, first find a continuous function g : A —> y such that 
| | / ( x ) - f f ( x ) | | < e / 3 f o r a l l x e A 
Then find a á > 0 such that ||g(x) - g(a) \\ < e/3 whenever x e A and ||x - a|| < Ö. 
This can be done since g : A —> Y is continuous at a G A. Now if x G A and 
||x - a|| < 5, then 
| | / ( x ) - / ( a ) | | 
= 
| | / ( x ) - f l ( x ) + 5 ( x ) - i ? ( a ) + f f ( a ) - / ( a ) | | 
< 
||/(x) - ff(x)|| + !|s(x) - ff(a)|| + ||fl(a) - /(a)|| 
< 
(e/3) + (e/3) + (e/3) = e. 
Hence / : A —> Y is continuous at a G A. 
G 
Definition 4.4.24 Uniform convergence. Let X and Y be two normed spaces and 
A c X. For each n G N, let /„ : A —> y be a function. Let / : yl —* Y be another 
function. Then fn is said to converge to f uniformly on A if the following condition 
is satisfied: for each e > 0 there is an N G N such that ||/„(x) — /(x)|| < e for all 
n> N and for all x £ A. 
Remarks 4.4.25 Convergence and uniform convergence. Uniform convergence 
on A is a stronger condition than convergence at each point in A. The sequence 
fn(x) = xn considered in Example 4.4.22 provides a good illustration. We see that 
for each x G A = [0, 1 ] the sequence of numbers /„(x) = xn converges to f(x) 
in R. Here f(x) = 0 i f 0 < x < 1 and /(l) = 1. This sequence does not converge 
uniformly on A. In fact, given any n G N, we can find a point a G A, 0 < a < 1, 
such that |/n(a) - /(a)| = a" > (1/2). We simply take a = (1/2)1/". Hence there 
is no n G N such that |/n(x) — /(x)| < (1/2) for all x G A, and the convergence of 
fn to / is not uniform. Note that this also follows from Theorem 4.4.29 below. 
Definition 4.4.26 Uniform continuity. Let X and Y be two normed spaces and 
Ad X. Suppose / : A —> Y. / is uniformly continuous on A if for each e > 0 there 
is á > 0 such that for all Xi,x2 G A, ||/(xi)— /(x 2)|| < e whenever ||xx — x2|| < <5. 
It should be clear that uniform continuity of a function on A implies its continuity 
on A. The concept of uniform continuity is needed for the following example and in 
the proof of Theorem 4.4.29. 

CONTINUITY IN NORMED SPACES 
151 
Example 4.4.27 Let fn : A -> Y and let g : Y -> Z, where A c X and X, Y, Z 
are normed spaces. Assume that g is uniformly continuous and the sequence /„ 
converges uniformly on A. Then g o fn also converges uniformly on A. To see this, 
suppose that fn converges uniformly on A to / : A —» y. Let e > 0 be given. Then 
by the uniform continuity of g, there is some S > 0 such that whenever y i, y2 are in 
Y and ||yi - y2|| < S, then ||#(yi) - e?(y2)j| < e. Since /„ converges uniformly on 
A to /, there is some TV such that 
||/„(x) - /(x)|| < S for all n > TV and all X É A 
Hence, for all n > TV and all X É Í , 
||fl(/„(x))-ff(/(x))||<e. 
Thus, g o /„ converges uniformly on Ato g o f. 
Example 4.4.28 Let Sn{x) = Efc=i(Vfc2) sin(fcr) for all x £ A, where A c ñ . 
Then Sn converges uniformly on A. More generally, let fk be a sequence of real-
valued functions defined on a set A, and let Mfc be a sequence of real numbers such 
that |/fe(x)| < Mfc for all x £ A. If J2kLi Mk < °°. t n e n Sfcli A converges 
uniformly on A. To see this, let e > 0. Put Sn(x) = Y^k=i fk{%) f°r all a; G A and 
all n £ N. Since J^fcLi -^fc < °°' there is some ./V such that 
n 
y ^ Mfc < e for all n > m > TV. 
fc—m+l 
Then for all n > m > TV and all I É Í , 
\Sn(x) - Sm(x)\ = 
J2 
Mx) 
k=m+\ 
k=m+l 
k=rn+l 
Hence, Sn(x) is a Cauchy sequence for all x € A. Let S{x) = limn^oo Sn(x) for 
all x £ A. By letting n —> oc in the above inequalities, we get 
|5(x) - Sm(x)\ <e 
for all m > TV and all x € A. 
This shows that Sn converges uniformly on A to S. 
Theorem 4.4.29 Uniform limits of continuous functions. Let X and Y be two 
normed spaces and A C X. Let fn: A -^ Y be a sequence of continuous functions 
on A converging uniformly on A to a function f : A —> Y. Then f : A —► Y is also 
a continuous function on A. 
Proof. We see that / : A —> Y satisfies the hypothesis of Lemma 4.4.23 above. 
Indeed, given any e > 0, there is a continuous function fn:A—*Y 
such that 
|[/„(x) — /(x)|| < e for all x £ A. Then the same lemma shows that / : A —> Y is 
continuous. 
□ 

152 
NORMED VECTOR SPACES 
Sequences of Polynomials 
Almost all of the functions we consider in the course, apart from counterexamples, 
are the uniform limits of polynomials. We now give a sufficient condition for the 
uniform convergence of a sequence of polynomials. 
Notations 4.4.30 Sequences of polynomials. Let X and Y be normed spaces. If 
S G MLn(Xn, 
Y), then 
/(x) = S(x, ..., x), 
x e l , 
defines a corresponding homogeneous polynomial / : X —> Y. For each n G N, 
suppose Sn € MLn(Xn, 
Y) and let fn : X —> Y be the corresponding homoge-
neous polynomial. Then Fn = YA=I /*> n e ^> defines a sequence of polynomials. 
Finally, for each r > 0, let 
ß r = { x | x 6 l , 
||x|| <r } . 
Theorem 4.4.31 Continuity of homogeneous polynomials. With the notations in 
4.4.30, f :X ^Y 
is continuous. Also, ||/(x)|| < \\S\\ |¡x||n, x £ l 
Proof. Define T : X -> Xn by Tx = (x, ..., x). Then T is linear and therefore 
continuous. Also, S : Xn —> Y is multilinear and therefore continuous. We see 
that / = S ■ T : X —> Y is the composition of two continuous functions. Hence, 
/ : X —> Y is also continuous. Also, 
||5(x„, ...,x 1)||<||S||-||x n||---||x 1|| 
by Theorem 4.3.13. Hence ||/(x)|| < ||S|| ■ ||x||" follows. 
D 
Theorem 4.4.32 Convergence of polynomials. With the notations in 4.4.30, assume 
that there is an R > 0 such that ^nll^«!! r " < °° wnenever 
0 < r < R. Then 
limnFra(x) = F(x) exists in Y for each x 6 BR 
and defines a continuous function F : BR —> Y. 
Proof. If x G BR, then ||x|| < R. Let r be such that ||x|| < r < R. Hence 
||/„(x)||<||5 n||||x|r<||S n||r". 
Then Theorem 4.2.21 shows that -Fn(x) is a Cauchy sequence in X and that F(x) = 
limn Fn (x) exists in Y. The same theorem also shows that 
||F(x)-F„(x)|| 
< 
T 
| | / f c( x ) | | < V 
!|5fc||||xf 
< E ^ l|5fc||rfc = L-Ln, 

CONTINUITY IN NORMED SPACES 
153 
where Ln = Y^k=i\\^k\\ rk and L = \imnLn 
in R. Hence, given e > 0, we 
can find an N e N such that (L — Ln) < e for all n > TV. This shows that the 
convergence F„(x) —» F(x) is uniform for x £ Br. Hence Theorem 4.4.29 shows 
that F : Br —> Y is continuous. This implies continuity on BR, since each x e BR 
is contained in some Br,r < R. 
□ 
The Inversion Operator 
The passage from an invertible function to its inverse function is called the inversion 
operation. We will consider this operation only on the class of invertible linear 
transformations. The main result is that inversion is a continuous function (with 
respect to the standard norm). The proof of this fact relies upon an elegant analogy 
between series of real numbers and series of linear mappings. 
Definition 4.4.33 The inversion operator. Let L0(X, Y) be the set of all invertible 
linear mappings T : X —> Y, a subset of L(X, Y). Let 
Inv : L0(X, Y) -> L(Y, X) 
be the function defined as Inv(T) = T~l € L(Y, X). 
Then Inv is called the 
inversion operator (on LQ{X, Y)). 
Remarks 4.4.34 Nonlinearity of the inversion. The set of invertible linear maps, 
L0(X, Y), is a subset of L(X, Y) but not a subspace of L(X, Y). Hence LQ{X, Y) 
is not a vector space and Inv is not a linear operator. The following special case 
makes this point even more obvious. 
Example 4.4.35 Inversion of linear mappings R —> R. Any linear mapping R —► 
R is just multiplication by a constant. Hence L(R, R) can be identified with R: if 
a G R, then a e L(R, R) is the linear mapping x —> ax. This mapping is invertible 
if and only if a ^ 0. Hence 
L0(R, R) = { a | a e R , a ^ O } . 
The inversion operator Inv : Lo(R, R) —> L(R, R) is Inv(a) = 1/a for all a ^ 0. 
Example 4.4.36 A series expression for inversion in Lo(R, R). We continue with 
the previous example. If \a\ < 1, then we know that the geometric series 
qn{a) = l + a + a2 + --- + an 
converges to (1 — a)" 1 = Inv(l — a). The result of the inversion can be expressed 
as the limit of a sequence of polynomials. More generally, if p ^ 0 and if \t/p\ < 1, 

154 
NORMED VECTOR SPACES 
then the geometric series 
qn(t/p) = 1 + (t/p) + (t/pf + ■ ■ ■ + {t/p)n 
converges to p (p — t)~l = p Inv(p — t). We shall show that an analogous result 
holds in general. 
Notations 4.4.37 Let X be a normed space. Let L(X, X) be the normed space of all 
linear transformations X —> X, using the standard norm for linear transformations as 
defined in Definition 4.3.4 and Theorem 4.3.3. Recall that \\SR\\ < \\S\\ \\R\\ for all 
S, Re L(X, X), as shown in Theorem 4.3.7. Write T2 = T ■ T for the composition 
of T with itself and Tn+ x = T ■ Tn for n G N. The result about composition implies 
that for all n, \\Tn\\ < \\T\\n. Finally, recall that I is the identity mapping. 
The following theorem develops the analogy with Example 4.4.36. 
Theorem 4.4.38 Inversion in L0(X, X). Let P G L0(X, X) and put Q = P~\ 
IfTe 
L{X, X) and if \\QT\\ < 1, then (P - T) G L0(X, X) (i.e., (P - T is 
invertible) and the sequence 
Qn(QT) = I + (QT) + (QT)2 + ■■■ + (QT)n 
converges to (P — T)~lP. 
This implies that the inversion operator, Inv : LQ(X. X) —> L(X, X), is continuous. 
Proof. First, consider the special case where P = I. In this case, Q = I as well, our 
assumption is that ||T|| < 1, and our desired conclusion is that the sequence 
Qn(T) =I + T + T2 + ---+Tn 
converges to (I ~ T) _ 1. This special case is easy to prove. First, observe that Qn(T) 
is a Cauchy sequence in L(X, X) because 
||Qm(T) - Qn(T)\\ < \\Tn+1\\ + ■■■ + \\Tm\\ < \\T\\n+1 + ■■■ + \\T\\m 
and ¡|T|j < 1. So, Qn{T) converges to some Q G L(X, X). Then by simple 
manipulations, Qn(T)(I - T) = (I - T"+1) converges to both Q(I - T) and I, 
showing that Q{I - T) = I. Similarly, (7 - T)Q = I. Hence, Q = {I - T)" 1 = 
Inv(7 - T). 
The general case follows from the special case by substituting QT for T, where 
Q = P^1 as before. Since ||QT|| < 1, we conclude that (I — QT) is invertible and 
Qn(QT) = 1+ (QT) + (QT)2 + ■■■ + (QT) 

CONTINUITY IN NORMED SPACES 
155 
converges to (I — QT)-1. 
By simple manipulations, 
(P-T) 
= P{I - P-XT) = P(I - QT). 
It follows that (P - T)-1 = (I - QT)~lQ exists whenever (7 - QT)-1 exists, and 
(P - T)~lP 
= (I - QT)"1 is the limit of the sequence Qn(QT). This shows that 
(P — T) _ 1 —> P~x as T —> 0; hence, Inv is continuous. G 
Problems 
4.37 
Let / : X —> X be defined by /(x) = ax + b, where X is a normed space, 
a is a constant, and b is a fixed vector in X. Show that / is uniformly continuous on 
X. 
4.38 
If X is a normed space and/ : X —> R, g : X —> K are uniformly continuous 
on X, must the product fg: X —■* M be uniformly continuous on X? 
4.39 
Is there a continuous function / : R3 —► M with the following properties: 
For each k G N, there are Xfc,yfc with ||xfc|| = ||yfc|| = 1 such that 
||x k-y f c||<(l/fc) 
and 
/(xfe) - /(yfc) > 1? 
4.40 
Consider K2 with the standard Euclidean norm. Suppose that / : R2 —> R 
is continuous and /(r, s) — 0 whenever r is rational and s is irrational. Show that 
f{x,y) 
= 0 for all (x,y) e R2. 
4.41 
Let j| \\i, || ||2 be norms on a vector space X. Let / : (X, || ||i) —> R be 
defined by 
/(x) = ||x||2 
forallxeX. 
Show that / is uniformly continuous. 
4.42 
Let X be a normed space and let L : X —> X be an isomorphism. Show that 
for any linear map T : X —> X, there is a constant A such that 
||Tx|| <yl||Lx|| 
for all x e X . 
4.43 
If / - S £ L0{X, X), must it follow that ||5|| < 1? 
4.44 
Let M be a proper subspace of L(X, X). Show that 
L0(X,X)C\{L{X,X)\M)^%. 

156 
NORMED VECTOR SPACES 
4.45 
Suppose that Tn G L0(X, X) for all n G N and Tn —> T. Must it be true 
Xh&lT 
eL0{X,X)1 
4.46 
Let M be a nonempty subset of a normed space X. Define d : X —> K by 
d(x) = inf { ||x - m|| | m G M } 
forallxGX. 
Is d continuous on XI Is <i uniformly continuous on XI 
4.47 
Let X and y be normed spaces. Let M be a nonempty compact subset of X. 
Let C(M, Y) be the collection of all continuous functions from M into (V, || ||y). For 
/, g in C(M, Y) and scalar t, define / + g and t / in the usual way: for all m G M, 
(/ + 5)(m) 
= 
/ ( m ) + 5 ( m ) 
(i/)(m) 
= 
i(/(m)). 
Show that C(M, Y) is a vector space, which may or may not be finite-dimensional. 
Furthermore, show that the function 
11/11 = max ||/(m)|| y 
for all / G C(M,Y) 
mGM 
defines a norm on C(M, Y). 
4.48 
Let T G L(X, X). Show that 
exists and defines a continuous function e : L(X, X) —> ¿(X, X). 
4.5 TOPOLOGY OF NORMED SPACES 
In the previous section we defined compact subsets of a normed space, but only in 
order to prove the continuity of inverse functions. It turns out that compactness is 
an extremely important property of certain sets; indeed, it plays an essential role in 
many significant theorems. In this section, we provide an alternative characterization 
of compactness in terms of the open subsets of a normed space. The investigation of 
properties of open sets in a normed space, and more generally of concepts defined 
in terms of open sets, is referred to as the topology of a normed space. This section 
presents some basic results about the topology of a normed space. 

TOPOLOGY OF NORMED SPACES 
157 
Open Sets 
Definition 4.5.1 Open balls. Let X be a normed space. An open ball in X is a set 
of the form 
£?r(a) = { x e X | | | x - a | | < r } . 
Here a e X is any point and r > 0. More explicitly, Br(&) is the open ball of radius 
r about (or with its center at) point a. 
Remark 4.5.2 The shape of a ball. If X = R2 or M3 with its standard Euclidean 
norm, then a ball looks like an ordinary ball. With other norms, a ball has a different 
shape. In our illustrations, we usually represent balls as discs, keeping in mind that 
this may not correspond to their actual shape. Figure 4.2 shows three balls of radius 
d in R2. These balls are with respect to the norms, from left to right, 
{x2+y2)1/2, 
(\x\ + \y\), and max(|x|, \y\). 
Figure 4.2. Balls of radius d in three different norms. 
Definition 4.5.3 Open sets. A set G in a normed space X is called open if, whenever 
G contains a point, it also contains an open ball about that point. More explicitly, G 
is open if for every a G G there is an r > 0 such that Br(a) c G. To justify the 
terminology of Definition 4.5.1, we show that open balls are also open sets. 
Theorem 4.5.4 An open ball is an open set. 
Proof. Let Br(a) be an open ball. Let x e Br(a). 
Then p = ||x — a|| < r. 
If s — r - p, then s > 0. We claim that Bs(x) 
c Br(a). 
If u e Bs(x), 
then 
||u — x|| < s. Hence, by the triangle inequality 4.1.8, 
u 
all <||u-
■ a|| < 
s+p r. 
Therefore u £ Br(a) and, consequently, Bs(x) C Br(a). 
□ 

158 
NORMED VECTOR SPACES 
Theorem 4.5.5 Properties of the collection of open sets. The collection of open 
sets in a normed space has the following three properties. 
(1) The whole space X and the empty set 0 are open. 
(2) A finite intersection of open sets is open. 
(3) Any union of open sets is open. 
Proof. These conditions follow easily from the definitions. 
To prove (2), let 
G\, ..., Gn be open sets and let G = n"=1Gi. Assume x G G. Since x G G¿ for 
each i, there is an r¿ > 0 such that Bri (x) c G¿. Let r = min¿ r¿. Then r > 0 and 
Br(x) C G¿ for all ¿ = 1, ..., n. Hence Br(x) c G, and therefore G is also open. 
The proofs of parts (1) and (3) are left as exercises. G 
Definition 4.5.6 Topological spaces. Let X be a nonempty set together with a 
collection Ö of subsets of X. If Ö contains X and 0 and 0 is closed under finite 
intersections and arbitrary unions, then 0 is called a topology onX. A set X together 
with a topology is called a topological space. The subsets of a topological space X 
contained in the collection G are called the open sets in X. A concept is called a 
topological concept if it can be defined in terms of open sets only. We will show that 
convergence and continuity are topological concepts. 
Theorem 4.5.5 above shows that a normed space becomes a topological space with 
the definition of open sets given in Definition 4.5.3. Note that the phrase "the 
topology of a normed space" sometimes refers to the collection of open sets (as in 
the preceding definition) and sometimes to the investigation of topological concepts 
(as in the opening remarks of this section). In practice, there should be no confusion. 
Theorem 4.5.7 Topology of a vector space. Any two norms on a (finite dimensional) 
vector space X define the same topology on X. 
Proof. Let || • || and || • ||' be two norms on X. Let G be open in || • ||. Let a G G. 
Then there is an r > 0 such that x G G whenever ||x — a|j < r. Theorem 4.2.14 
shows that any two norms on X are equivalent. In particular, there is an M > 0 such 
that || • || < M\\ • ||'. Then we see that x G G whenever ||x - a||' < r/M. Hence G 
is also open in || • ||'. The proof of the converse is the same. D 
Definition 4.5.8 Neighborhoods of a point. Let X be a normed space and a £ X. 
Any open set containing a is called a neighborhood of a. Hence a nonempty open 
set is a neighborhood of each of the points that it contains. 

TOPOLOGY OF NORMED SPACES 
159 
Interiors, Exteriors, and Boundaries 
Definition 4.5.9 Boundary points and boundaries. Let E be a set in a normed 
space X. A point a e l i s called a boundary point of E if every neighborhood of a 
intersects both E and its complement Ec = X \ E. The set of all boundary points 
of E is called the boundary ofE and is denoted as dE. 
Definition 4.5.10 Interiors and exteriors. Let E be a set in a normed space X. 
The set E° = E\ dE is called the interior of E. The points in E° are the interior 
points ofE. The set Eext = X \ (E U dE) is called the exterior ofE. The points in 
Eext are the exterior points ofE. 
Remarks 4.5.11 Boundaries of complementary sets. If E is any set in a normed 
space, then dE = dEc. This follows easily from the observations that the roles of E 
and Ec are interchangeable in Definition 4.5.10 and that (Ec)c — E. 
Remarks 4.5.12 Boundaries of subsets. If A c B are subsets of a normed space, 
it does not follow that dA c dB. For example, take B = R and A = Q. Then 
dA = R but dB = 0. 
Example 4.5.13 Let 
D 
= 
{{x,y,z) 
SR 3 \x2 + y2 + z2 = l } , 
E 
= 
{{x,y,z) 
<ER3 I x2+y2 
+ z2 < 1 }, 
F 
= 
{(x,y,z) 
GR3 I x2+y2 
+ z2 < 1 }. 
Then 3D = dE = dF = D = { (x, y, z) e R3 | x2 + y2 + z2 = 1 }. 
Theorem 4.5.14 Boundaries and open sets. A set G in a normed space X is open 
if and only if it contains none of its boundary points. Equivalently, G is open if and 
only ifGndG 
= 9. 
Proof. Assume that G is open. Let a e G. Then G is a neighborhood of a. But G 
does not intersect Gc. Hence a has a neighborhood that does not intersect both G 
and Gc. Therefore a is not a boundary point of G. 
Conversely, assume that G is such that G n dG = 0. Let a € G. Then a is not 
in dG, so a has a neighborhood H that does not intersect both G and Gc. But H 
intersects G since a is common to these two sets. Therefore H n Gc = 0. This 
means that a s H c G, where H is an open set. Hence there is an r > 0 such that 
Br (a) c H c G, proving that G is an open set. 
□ 

160 
NORMED VECTOR SPACES 
Remarks 4.5.15 Empty boundaries. Note that the space X and the empty set 0 
have no boundary points. It turns out that these are the only two sets in a normed 
space that have no boundary points. It is a good exercise to show that if E and 
Ec are both nonempty, then their boundaries are also nonempty. {Hint: let a £ E 
and b e Ec. Let S = {t | 0 < t < 1, (1 - t)a + tb e E }. Let 7 = supS and 
c = (1 - 7)a + 7b. Show that c e dE.) 
Closed Sets 
Deñnition 4.5.16 Closed sets. A set F in a normed space X is said to be a closed 
set if its complement Fc = X \ F is an open set. 
Example 4.5.17 Let a € X, where X is a normed space. Then {a} is closed in X. 
To see this, let G = X \ {a}. If x e G and r = ||x - a||, then r > 0 and £(x, r/2) 
does not contain a. Thus, -B(x, r/2) c G. Thus, G is open; hence, {a} is closed in 
X. 
Example 4.5.18 The surface of the ball Br(a) is the set 
SV(a) = { x e X | | | x - a | | = r }. 
This set is closed. To see this, suppose that c ^ Sr(a). 
Let p — ||c — a|| and 
s = \p ~ r\. Then s > 0. We see that Bs(c) n SV(a) = 0, whether p < r or r < p. 
This shows that the complement of 5r(a) is open. A 
Example 4.5.19 A finite union of closed subsets of a normed space is closed. If 
Ai,..., 
Am are closed subsets of a normed space X, then {^=1{X 
\ Af.) is open, 
being an intersection of finitely many open subsets of X. But 
rrkjx\Ak)=x\{]jiiAk) 
so that UfcLi Ak is also closed in X. 
Theorem 4.5.20 Boundaries and closed sets. A set F in a normed space X is 
closed if and only if it contains all of its boundary points. Equivalently, F is closed 
if and only ifdF C F. 
Proof. This follows from the definitions and from Theorem 4.5.14. 
□ 
Remarks 4.5.21 Sets that are both open and closed. Note that the whole space 
X and the empty set 0 are both open and closed. These are the only two sets in a 

TOPOLOGY OF NORMED SPACES 
161 
normed space that are both open and closed. This follows from the remarks in 4.5.15 
above. 
Definition 4.5.22 Closure of a set. Let E be any subset of a normed space X. Then 
E = EU dE is called the closure ofE. 
Lemma 4.5.23 If an open set G intersects E, then it also intersects E. 
Proof. Let a e G n f i . If a G E, the conclusion follows. If a G dE, then G is a 
neighborhood of the boundary point a. Hence G must intersect E. 
D 
Lemma 4.5.24 The closure E of any set E is a closed set. 
Proof. Let a G" E. Let G be a neighborhood of a. Then G intersects Ec since 
a e G n Ec. If G intersects E, then it also intersects E. This follows from Lemma 
4.5.23. Therefore, if every neighborhood of a intersects E, then a G dE, which is 
a contradiction. Hence, a has a neighborhood contained in (E)c. Therefore (E)c is 
open. 
□ 
Closed Sets and Convergent Sequences 
Theorem 4.5.25 Let F be a set in a normed space X. Then F is a closed set if and 
only if every convergent sequence in F converges to a point in F. 
Proof. Assume that F is a closed set. Hence Fc is open. If a G Fc, then there is an 
r > 0 such that Br(a) C Fc. Therefore, if x„ is a sequence in F, then ||xra — a|| > r 
for all n G N. Hence x„ y+ a. This means that if x„ converges, it must converge to 
a point in F. 
Conversely, assume that every convergent sequence in F converges to a point in F. 
Let a G dF. Then Bi/n(a) 
intersects F for all n G N. Hence there is an x n G F 
such that ||xn — a|| < 1/n. This shows that there is a sequence x n in F converging 
to a G dF. Therefore dF c F and F is closed. 
□ 
Example 4.5.26 Any one-dimensional subspace of a normed space is closed. To see 
this, let E = span {a}, where a G X. Let x n be a sequence in E that converges 
to some z G E. Then for each n, we have xra = tna for some scalar tn. Since x n 
converges, the sequence tn must also converge in R to some s G K (as was shown in 
a previous example). Hence, x n = tna —> sa G E. Thus, E is closed. 

162 
NORMED VECTOR SPACES 
Theorem 4.5.27 Limits of convergent sequences in E. A point a G X can be the 
limit of a sequence in E if and only ;/a £ E. 
Proof. Note that E is a closed set by Lemma 4.5.24 above. Hence, if x„ £ E c E 
and if x„ —> a, then Theorem 4.5.25 above shows that aG-E. 
Conversely, assume that a £ E. If a £ E, then the constant sequence x„ = a is 
a sequence in E converging to a. If a ^ E, then a £ dE. Therefore, every ball 
Bi/n(a) contains a point xra £ E. Then we see that x n —> a. 
□ 
Continuous Functions and Topology 
There is a close connection between continuity and topology. The following theorem 
shows that continuity can be defined in terms of open sets. 
Theorem 4.5.28 Continuity and open sets. Let X and Y be two normed spaces 
and A C X. Then a function f : A —> Y is continuous if and only if for each open 
set H in Y there is an open set G in X such that f~1(H) 
= A n G. 
Proof. Assume that / : A —> Y is continuous. Let H be an open set in Y. If 
x £ f~1(H), 
then /(x) = y £ H. Since H is open, there is a e > 0 such 
that BE(y) c H. Since / is continuous at x £ f~1(H), 
there is a S > 0 such that 
||/(x')-/(x)|| < e whenever ||x'-x|| < <5andx' £ A. Therefore/(An ^ ( x ) ) c 
BE(y) c H. Note that 8 > 0 depends on x. To show this explicitly, we write <5(x) 
instead of ö. With this notation, let 
Then G is open in X, since it is a union of open balls. Also, f~1(H) 
= A n G. 
Conversely, assume that / : A —» Y is such that for each open H C Y there is an 
open G C X such that f~l{H) 
= A n G. Let x £ A and y = /(x). Given £ > 0, 
let H = B£{y). This is an open set in Y. Hence, there is an open G C X such that 
f-l{H) 
= AnG; 
i.e., f(A n G) C ßE(y) = F . Since x e / " H i i ) = A n G, 
there is a <5 > 0 such that Bs(x) C G. This means that /(x') £ BE(y) whenever 
x ' e i n i?á(x). Hence / is continuous at x £ A 
□ 
Corollary 4.5.29 Continuous functions with open domains. Let X and Y be two 
normed spaces. Let A C X be an open set. A function f : A —> Y is continuous if 
and only if the inverse image f^1(G) 
of any open set G in Y is an open set in X. 
Proof. Apply Theorem 4.5.28. D 

TOPOLOGY OF NORMED SPACES 
163 
Corollary 4.5.30 Continuous functions defined over the whole space. Let X and 
Y be two normed spaces. A function f : X —» Y defined on the whole space X is 
continuous if and only if the inverse image of every open set is open. Equivalently, 
f : X —> Y is continuous if and only if the inverse image of every closed set is closed. 
Proof. Apply Theorem 4.5.28. G 
Example 4.5.31 All subspaces of a vector space are closed sets. In fact, every 
subspace K of X is the kernel of a linear transformation T : X —» X. An example 
is the coordinate projection on a complementary subspace, as in Definition 3.1.42. 
ThenK = T~1({0}) is the inverse image of the closed set {0} under the continuous 
function T : X -> X. 
A 
Continuous Functions and Connected Sets 
Definition 4.5.32 Connected sets. A set C in a normed space X is called a connected 
set if it satisfies the following condition: if P and Q are two nonempty sets in X 
such that PUQ = C, then Cn P n Q is nonempty. 
Remarks 4.5.33 Connectedness in R and in normed spaces. Definition 4.5.32 is 
the same as Definition 2.6.12 for connected subsets of R. But there is a difference 
between the intuitive ideas of connectedness in R and in general vector spaces. In R 
every connected set is an interval: if a connected set contains two points, then it also 
contains all the points in between. There is an analogous notion of connectedness 
for general spaces, called arcwise connectedness. These two notions, connectedness 
and arcwise connectedness, are different in general, but they coincide for subsets of 
R, and for open subsets of a normed space (see problem 4.83). 
Definition 4.5.34 Arcwise connected sets. A set C in a normed space X is called 
arcwise connected if it has the following property: given two points a, b £ C there 
is a continuous function / : I — [0, 1] —> X such that /(0) = a, /(l) = b and 
such that f(t) € C for all t € /. In this case we say that a and b can be joined by 
an arc in C. 
Theorem 4.5.35 Continuity and connected sets. Let X and Y be two normed 
spaces and A C X. Let f : A —» Y be a continuous function. If C C A is a 
connected subset of X, then B = f(C) is a connected subset ofY. 
Proof. Let R and S be two nonempty sets in Y such that R U S = B. 
Let 
P = A n f~1(R) 
and Q = A n / _ 1( 5 ) . Then we see that P and Q are two 

164 
NORMED VECTOR SPACES 
nonempty sets in X such that PöQ = C. Since C is a connected set, there is a point 
c G C n P n Q. We will show that /(c) e B i l l f l S . This means that P D P O 5 
is nonempty and therefore P is connected. 
Since c G P, there is a sequence u„ G P C C such that u„ —> c. This follows from 
Theorem 4.5.27. Then/(u n) G Pand/(u n) —> /(c) by the continuity of /. Hence, 
by the same theorem, /(c) G P. Therefore /(c) G B n P. Similarly, there is a 
sequence v„ G Q C C such that v n —> c. Then /(v n) G S1 and /(v n) —> /(c) G S. 
This shows that /(c) G B n P n 5. 
□ 
Theorem 4.5.36 Ler Abe a connected set in a normed space X. Let f : A —* M be 
a real-valued continuous function. If f takes two values on A, then it also takes all 
the values in between. More explicitly, if /(a) = r < t < s = /(b), with a, b G A, 
then there is a point c G A such that /(c) = t. 
Proof. Theorem 4.5.35 shows that f(A) is a connected set in R. But we know 
from Theorem 2.6.13 that any connected subset of R is an interval. Hence, if f(A) 
contains r and s and if r < t < s, then f(A) also contains t. 
□ 
Theorem 4.5.37 Intermediate value theorem. Let a, b € R, a < b. Let / : 
[a, b] —> R ¿>e a continuous function. If /(a) = r < t < s = /(í>) f/¡en í/jere ¡\v a 
c G R ÍMC/I í/zaí a < c < b and /(c) = i. 
Proof. Apply Theorem 4.5.36 with the connected set A = {a, b] C R. 
□ 
Compact Sets 
Compact sets were defined in Definition 4.4.19 as follows. A set C in a normed 
space is called compact if every sequence in C has a subsequence converging to a 
point in C. Compact sets play a central role in analysis. They have several equivalent 
definitions. One of these definitions is established in Theorem 4.5.38 below. Another 
defining property is obtained in Theorem 4.5.42. This property is called the Heine-
Borel property. Usually it is taken as the principle definition of compactness in more 
general settings. 
Theorem 4.5.38 A subset of a (finite dimensional) normed space is compact if and 
only if it is bounded and closed. 
Proof. Assume that C is not bounded. Then for each n G N there is an x n G C 
such that ||xn|| > n. Then every subsequence of x n is an unbounded sequence and 
therefore cannot converge. Hence C is not compact. 

TOPOLOGY OF NORMED SPACES 
165 
Now assume that C is not closed. Then Theorem 4.5.25 shows that there is a 
sequence xn in C converging to a point a $ C. Every subsequence of this sequence 
also converges to a G* C. Therefore x„ has no subsequence converging to a point in 
C. Hence C is not compact. 
Conversely, assume that C is closed and bounded. Then every sequence in C is 
a bounded sequence. Therefore it has a convergent subsequence by the Bolzano-
Weierstrass theorem, 4.2.16. Then Theorem 4.5.25 shows that this subsequence 
converges to a point in C, since C is closed. Therefore C is compact. 
D 
Definition 4.5.39 Open covers. A collection of open sets 9 = { G } is called an 
open cover for a set E if E is contained in the union of this collection. Equivalently, 
S = { G } is an open cover for E if for each x G E there is a G £ S such that x G G. 
Theorem 4.5.40 Let 9 be an open cover for a compact set C. Then there is a 5 > 0 
with the following property: ifu, v G C and ||u — v|| < 5, then there is a G £ 9 
that contains both u and v. 
Proof. Assume that the conclusion of the theorem is false. Then for each n £ N 
there are u„, v„ G C such that ||un — v„|| < 1/n, but there is no set G £ 9 that 
contains both u„ and v„. Since C is compact, there is a subsequence u¿, k £ K 
such that Ufc —> a G C. It follows easily that v*. —> a. Now there is a G £ 9 such 
that a G G. Since G is open, there is an r > 0 such that Br(a) C G. But u/- and v^ 
are both in Br(a) for all sufficiently large k £ K. Then u¿ and Vfc are in the same 
G £ 9 for such k. This contradiction proves the theorem. 
D 
Theorem 4.5.41 Let 9 be an open cover for a compact set C. Then there is anr > 0 
such that each ball Br (x), x G C, is contained in a G £ 9-
Proof. Assume that the conclusion of the theorem is false. Then for each n £ N 
there is an x n £ C such that ßi/„(x n) is not contained in any G £ 9- Then there 
is a subsequence Xfc, k £ K, such that Xfc —> a G C. Now we know that there is a 
G £ 9 such that a G G. In this case there is an R > 0 such that £#(a) C G. Choose 
k £K sufficiently large such that ||xfe - a|| < R/2 and (1/fc) < R/2. Then we see 
that Bi/ki^k) 
C Bji(a) c G £ 9- This contradiction proves the theorem. 
D 
Theorem 4.5.42 Heine-Borel Theorem. A set C in a normed space is compact if 
and only if every open cover for C has a finite subcover. 
Proof. First, suppose that C is compact, and let 9 be an open cover for C. Find 
r > 0 as in Theorem 4.5.41 above. Then for each x G C, Br(x.) is contained in a 

166 
NORMED VECTOR SPACES 
G s S - We claim that there are finitely many points xi, ..., x„ G C such that 
U
n 
. 
Br(Xi). 
(4.7) 
I—1 
Let xi G C be arbitrary. If C C E± — Br(x.i), then we are done. Otherwise choose 
X2 G (C \E\). 
Continuing in this way, if xi, ..., x„ have been chosen and if 
C (£_ En, then choose x„+i G (C\En). 
If this process does not terminate, then we 
obtain a sequence x n in C. The distance between any two points in this sequence 
is at least r. Any subset of this sequence has the same property. By compactness, 
there is a convergent subsequence x j . i e l , such that x^ —» a G C. Hence there 
are k\, k^ G K such that ||xfei — a|| < r/2, ¿ = 1,2, and k\ < k-2- We see that this 
implies ||xfc2 — XfeJI < r. Hence Xfc2 G ßr(xfc1) C E^. This contradicts the choice 
of xfc2. Therefore there must be an n G N such that C C En. Since each -Br(x¿) is 
contained in some G¿ G S, we see that (4.7) gives a finite cover for E. 
Conversely, assume that C C X is such that every open cover for C has a finite 
subcover. Let xra be a sequence in C. For each a G X and r > 0, let 
Kr(a) = {fcGN| ||x f c-a|| < r } . 
We see that x n has a subsequence converging to a if and only if Kr (a) is an unbounded 
set for all r > 0. Hence, if no subsequence of x„ converges to a point a G C, then 
for each a G C there is an r(a) > 0 such that Br(a)(a) contains only finitely many 
x n s. In this case {ßr(a)(a)}, a G C, is an open cover for C. Hence C is contained 
in a finite union of these balls. This means that C contains x n for only finitely many 
n s. This contradiction shows that x n must have a subsequence converging to a point 
in C. Hence C is compact. 
□ 
Compact Sets Under Continuous Functions 
Theorem 4.5.43 Let X and Y be two normed spaces. Let C be a compact subset in 
X and f : C —> Y a continuous function. Then B = f(C) is a compact subset ofY. 
Proof. We have to show that every sequence y n in B must have a convergent 
subsequence ykn converging to a point b in B. Let y n be a sequence in B = f(C). 
Then each y n = /(x n) for some x n G C. Since x n is a sequence in the compact set 
C, it has a convergent subsequence x/^ converging to a point a G C. Let b = /(a). 
Then b £ B, and the continuity of / at a implies that 
y * . = / W ^ / ( a ) = b € B . 
This shows that B is compact. 
□ 

TOPOLOGY OF NORMED SPACES 
167 
Theorem 4.5.44 Let X be a normed space. Let A be a nonempty compact subset 
of X. Then any real-valued continuous function f : A —> R is bounded and attains 
its maximum and minimum on A. More explicitly, if f : A —> R is a continuous 
function defined on a nonempty compact set A, then there are points a, b G A such 
that}"(a) < /(x) < f{b) for all ^e 
A. 
Proof. The set B = f(A) C R is the image of the compact set A c X under the 
continuous function / : A —> R. Then Theorem 4.5.43 shows that B is a compact 
subset of R. Hence B is bounded and closed. It is also nonempty, since A is 
nonempty. Therefore m = inf B and M = sup B exist. Both belong to dB. But 
dB c B since B is closed. Hence m, M £ B = f(A) and, therefore, there are 
points a, b 6 A such that m = /(a) and M = /(b). Then b = /(x) e B satisfies 
m<b< 
M. 
a 
Recall that the proof of the spectral theorem, Theorem 3.6.4, depended on Lemma 
3.6.1. It was obtained by an application of the Bolzano-Weierstrass theorem. Theo-
rem 4.5.44 allows us to give a short proof of this result, restated as Lemma 4.5.45. 
Lemma 4.5.45 Let X and Y be Euclidean spaces. Let T : X —> Y be a linear 
transformation. Then there is a unit vector e G X such that ||Tx|| < ||Te|| for all 
unit vectors X É X . 
Proof. The function that takes x e l t o ||rx|| G R is continuous. Therefore, it 
reaches its maximum value on the compact set { x | ||x|| = 1 }. O 
Uniform Continuity and Compactness 
Uniform continuity was defined in Definition 4.4.26. Recall that a function may be 
continuous but not uniformly continuous on a set. Theorem 4.5.46 below shows that 
the situation is different for compact sets. 
Theorem 4.5.46 Uniform continuity on compact sets. Let X and Y be two normed 
spaces, and let Abe a compact set in X. Then any continuous function f : A —> Y 
is also uniformly continuous on A. 
Proof. If / : A —► Y is not uniformly continuous on A, then there is an a > 0 
such that for each S > 0 there are two points p, q £ A with ||p — q|| < S but 
II/(P) — /(Q)II ^ a- Then there are two sequences p n and q„ in A such that 
||Pn — Qn|| < (l/ n) and ll/(Pn) - f{<ln)\\ > OÍ for all u G N. Since A is compact, 
there is a subsequence p¿n that converges to a point a G A. Then 
||qfc„ - a|| < ||qfcn - pferi || + ||pfen - a|| < (l/n) + ||pfcn - a|| 

168 
NORMED VECTOR SPACES 
shows that qfcn —> a. Hence f(pkn) 
—> /(a) and /(q/cn) —> /(a) by the continuity 
of / at a G A This shows that 
H/(PfcJ - /(QfcJII < ll/(Pfc„) " /(a)|| + ||/(q*J - /(a)|| - 0. 
This contradicts the fact that ||/(pfc„) - /(qOII > a > 0 for all n G N. Hence / 
must be uniformly continuous on A 
□ 
Distance Between Sets 
Definition 4.5.47 Distance between sets. Let A and B be two nonempty sets in a 
normed space. Then 
p(A, B) = inf { ||a - b|| | a G A b G B } 
is called the distance between A and B. Note that, the infimum is taken over a 
nonempty set of nonnegative numbers. Hence, the distance p(A, B) is well-defined 
for any two nonempty sets A and B. 
Lemma 4.5.48 Let K be a compact set and let F be a closed set in a normed space 
X. If K n F = 0 then p(K. F) > 0. Also, there is a compact set E such that 
KcE°cEcG 
= Fc = 
X\F. 
Proof. Since G = Fc, by assumption we have K c G where K is closed and G 
is open. Hence for each x G K there is an r(x) > 0 such that f?r(x)(x) C G. 
Then {jBr(x)/3(x)}, x G K, is an open cover for K. By the Heine-Borel Theorem 
4.5.42, it has a finite subcover. Hence there are finitely many x¿ G K such that 
K C Ui-Br(x.)/3(x¿). Let r = minr(x¿)/3. Then r > 0. If x G K then there is an 
Xj such that x G £?r(x.)/3(x¿). This means that Br(x) C Br(x.)(x¿) C G. Hence 
llx ~ yll > r f°r any y €. F. Therefore p(K, F) > r > 0. 
For the second part, let E = lJiB2r(xi)/3- 
This is a compact set, since it is a finite 
union of compact sets. We can easily verify that E satisfies the requirements of the 
lemma. 
□ 
Definition 4.5.49 Convex sets. A C be a set in a vector space. Assume that if C 
contains two points a and b, then it also contains the line segment 
L = {ib + (l - t ) a | 0 <t < 1 } 
joining these two points. In this case, C is called a convex set. 

TOPOLOGY OF NORMED SPACES 
169 
Problems 
4.49 
Show that if a closed set contains E then it also contains E. 
4.50 
Given any set E in a normed space X, show that the sets E°, Eext, and dE are 
pairwise disjoint and their union is X. Give examples of sets E such that dE = X, 
or such that E° = X, or such that £ e x t = X. 
4.51 
Let A, B be subsets of a normed space. Show that 
d(A UB)cdAu 
dB. 
Give an example to show that it is possible to have d(AuB) 
^ dA U dB. If A c B, 
must it follow that dA C dB? 
4.52 
Show that x e E° if and only if there is an r > 0 such that Br(x) C E. 
4.53 
Given a normed space X, find an example of a set 2? such that dE = X 
4.54 
Let S = { {x, y, z) G R3 | 2x - y + 5z = 0 }. Show that dS = S. 
4.55 
Let x n be a sequence in a normed space X and assume that x n —> a for some 
a e X. Let S = { x„ j n G N }. Is it true that dS = {a}? 
4.56 
Let x„ be a sequence in a normed space X and assume that xra —> a for some 
a G X. Show that T = { x„ | n G N } U {a} is a closed subset of X. 
4.57 
Let X be a normed space. Verify that ¿>0 = 0 = dX. IfEdX, 
must it be 
true that d(dE) c öS? 
4.58 
Show that <9(a + E) = a + <9£ for all a G X and for all E c X. 
4.59 
Let F be any finite subset of a nonzero normed space X. Show that OF = F. 
Hence, deduce that F is closed. 
4.60 
Let A be a bounded subset of a normed space. Show that A U dA is also 
bounded. 
4.61 
Let F be a finite subset of an open set O in a normed space. Show that 
0\F 
is also open. 
4.62 
Let E be a subset of a normed space X. Let a G X. Show that E is open if 
and only if a + E is open. Similarly, if c is any nonzero scalar, show that E is open 
if and only if cE is open. 

170 
NORMED VECTOR SPACES 
4.63 
Show that an arbitrary intersection of closed sets is closed and a union of 
finitely many closed sets is also closed. 
4.64 
Let M, N be subsets of a Euclidean space such that m ± n for all m e M 
and all n e N. Let m^ be a sequence in M and let n^ be a sequence in N. Assume 
that rrifc + n¿ is a bounded sequence. Show that there exists a sequence ¿k in N such 
that both mek and ngk converge. 
4.65 
Let U = {(x,y,z) 
6 E 3 | \2x + 3y + z\ < 1, |x - y + 5z| < 3 }. Show 
that U is open in the Euclidean space K3. 
4.66 
Let X and Y be normed spaces. Let T : X —» X be linear. Assume that T 
maps a basis of X to a basis of Y. Show that G C X is open if and only if T(G) is 
open. 
4.67 
Suppose that X and y are normed spaces and let / : X —> y be continuous. 
Let D C A. If / is one-to-one on A", show that f{dD) C d(f(D)). 
What happens 
if / is not one-to-one? 
4.68 
Show that the set of all isomorphisms X —» X is an open subset of L(X, X). 
4.69 
Let E be a set in a normed space A". A point a e X is an accumulation point 
of E if E n Br(a) is an infinite set (that is, contains infinitely many points) for all 
r > 0. Show that every bounded infinite set has an accumulation point. Is every 
point in dE an accumulation point of El 
4.70 
Let Ehe a set in a normed space X. A sequence in E is dense in E if every 
point in E is an accumulation point (Problem 4.69) of this sequence. Show that every 
infinite set contains dense sequences. 
Problems on Compact Sets 
4.71 
Let A. B be compact subsets of a normed space. Show that A n B and A U B 
are compact. 
4.72 
Let S = { x = (xi, x2, £3) | 1 < x\ + x\ + x\ < 2 }. Show that 5 is com-
pact in the Euclidean space R3. 
4.73 
Let A" be a normed space. Let A be a compact subset of A, and let B be a 
nonempty finite subset of A. Must ^4 + ß = {a + b | a 6 ^ , b e ß } be compact? 

TOPOLOGY OF NORMED SPACES 
171 
4.74 
Let n £ N. Give an example of compact subsets E, F of Era such that 
E\F 
is not compact. 
4.75 
Let fibea subset of a normed space X. If a £ X is an accumulation point 
(Problem 4.69) of E then £ \ {a} is not compact. If u £ dE, is it true that E \ {u} 
is not compact? 
4.76 
Let U = {x = (x1,x2,x3) 
£ K3 | 1 < maxi<fc<3 \xk\ < 3 }. Show that 
there is an a £ {/ such that 
al + °2 + a3 — m i n { x\ + x2 + x3 I x € ^ } ■ 
4.77 
Let £„ be a sequence of nonempty compact subsets of a normed space X. If 
-E/c+i C Ek for all fc £ N, show that C\nEn ^ 0. Give an example of a sequence An 
of nonempty subsets of K with Ak+i C Ak for all k £ N, but such that n„£„ = 0. 
4.78 
Let X and y be normed spaces. Let M be a compact set in X. Let / i , . . . , / m 
be continuous functions M —» Y. Let 
xfc = (xfcl,a:fe2,...,xfcm) G Mm, fc £ N. 
be a bounded sequence in E m. For each k £ N, define 
5fe = ajfci/i + • • • + xkmfm. 
Show that there is a continuous function g : M —> Y such that a subsequence of gk 
converges to g uniformly on M. Is g unique? 
4.79 
Let X and Y be normed spaces. Let M be a compact set in X. 
Let 
/ : M —> y be a continuous and one-to-one function. 
Show that a sequence 
m„ £ M converges if and only if /(m n) £ Y converges. 
Problems on Connected Sets 
4.80 
Let A be a connected subset of a normed space X and assume that there is 
an a £ A such that ^4 \ {a} is connected. Let B be any nonempty subset of normed 
space Y such that for each y £ B, the set B \ {y} is not connected. Show that 
there is no one-to-one, onto continuous map / : A—+B. Deduce that if c < d are 
real numbers and I is an interval of the form [a, b), (a, b] or [a, b] with real numbers 
a < b, then there is no one-one onto continuous function / : / —» (c,d). 
4.81 
Let C — Aö B, where A and B are two sets in the xy-p\ane defined by 
A = { (0, y) | - 1 < y < 1 } and B = { (x, sin(l/x)) | 0 < x < 1 }. 

172 
NORMED VECTOR SPACES 
Show that C is connected but not arcwise connected. 
4.82 
Let G be an open and connected set. If A and B are open sets and if 
G = A U B, then show that A or B is empty. 
4.83 
Show that if an open set is connected, then it is also arcwise connected. {Hint: 
Let G be an open set and a G G. Let A be the set of all points in G that can be joined 
to a by an arc in C. Let B = G \ A. Show that A and B are both open.) 
4.84 
Show that any open set is the union of a sequence of pairwise disjoint con-
nected open sets. 
4.85 
Show that any open set in R is the union of a sequence of pairwise disjoint 
open intervals. Give an example to show that this union need not be a finite union, 
even for bounded open sets. 
Remarks on Problem 4.85. Try to give a simple solution for this problem. At the 
same, time keep in mind that there are also very complicated open sets. Here is 
one: Let ru be a dense sequence (Problem 4.70) in (0, 1). Let 6k = 2~k~4. Let 
Gfc = (0, 1) n (rfc — £fc, rfc + £fc). Then G = U^G^ is an open set. Hence G is a 
union of pairwise disjoint open intervals. Probably no one knows the explicit forms 
of these intervals. Show, however, that G / (0, 1). 
Problems on Distances Between Sets 
4.86 
Let X be a normed space. Let B c i b e a nonempty set and a. € X. Then 
p(a, B) = infxeß ||x — a|| is the distance between the point a and the set B. Show 
that there is a point b G B such that p(a, B) = ||a - b||. Is b G B unique? 
4.87 
For any nonempty set E and for any r > 0 let Er = Uxg£.Br(x) be the 
enlargement of E by r > 0. Show that x G Er if and only if p(x, E) < r. 
4.88 
Show that x G ~B if and only if p(x, B) = 0. Show that x G B~ = 
Dr>0Br. 
4.89 
Let A and B be two nonempty sets in a normed space X. 
Let p(x, B), 
x G X, be as defined in Problem 4.86, and p(A, B), the distance between A and B, 
as defined in Definition 4.5.47. Show that p(A, B) = infx£j4 p(x, B). 
4.90 
Let A and B be two nonempty sets in a normed space X. If A is compact, 
then show that there are points a G A and b e ß such that p(A, B) = ||a — b||. 

TOPOLOGY OF NORMED SPACES 
173 
4.91 
Let A and B be two nonempty sets in a normed space X. Give an example to 
show that there may not be any points a G Aandb € B such that p(A, B) = ||a—b||. 
Problems on Convex Sets 
4.92 
Show that a convex set is either contained in a lower dimensional subspace or 
it contains an interior point. 
4.93 
Show that if a convex set in a normed space X contains points from a set A 
and from its complement Ac = X \ A, then it also contains points from dA. Give 
an example to show that this is not necessarily true for non-convex sets. 
4.94 
Let a be an interior point of a convex set C. If b e C and 0 < t < 1, then 
show that (1 — t)a + th is also an interior point of C. Give an example to show that 
this is not necessarily true for non-convex sets. 
Problems on Oscillations 
4.95 
Let X and Y be normed spaces. Let E a X and let / : E —> Y be a bounded 
function. If G C E, then show that 
í í ( / , G ) = s u p { | | / ( u ) - / ( v ) | | | u , v G G } 
exists. It is called the oscillation off over the set G. Also show that i f A c ß C ß 
then fi(/, A) < íí(/, B). 
4.96 
With the notations of Problem 4.95, let a e l Show that 
w(f, a) = lim r^ 0 +fi(/, E fl Br(a)) 
exists. It is called the oscillation of f at the point a. 
4.97 
Show that / is continuous at a if and only if u(f, a) = 0. 
4.98 
Given a 6 l , let E(a) be the set o f x e £ such that u(f, x) > a. Show 
that E(a) is a closed set for all a £ R. 

This page intentionally left blank

CHAPTER 5 
DERIVATIVES 
The properties of linear transformations are relatively easy to grasp. The key idea of 
differential calculus is to consider the broader class of functions that can be approx-
imated locally by linear transformations. The approximating linear transformation, 
which typically changes from point to point, is the derivative of the function at a 
point, while any function that can be approximated in this way is said to be differen-
tiable at such points. Our objective is to investigate the extent to which the properties 
of linear transformations are passed on to this larger class of differentiable functions. 
Note that this is a direct generalization of differential calculus in the one-variable 
case, where we study functions that can be locally approximated by straight lines. 
To begin our study, we first have to define the nature of approximation by a linear 
transformation. Second, we have to develop a test to find out if a given function can 
be so approximated, i.e., whether it is differentiable at a given point. It turns out, 
however, that checking the differentiability of a function is, in general, not easy. We 
shall concentrate on the sub-class oí continuously differentiable functions. It happens 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
175 

176 
DERIVATIVES 
that functions belonging to this class can be recognized by a routine test, and we shall 
make such functions the focus of our study. 
Section 5.1 discusses differentiability for an important special case: the class of 
vector-valued functions of a real variable. The general definitions of differentiability 
and continuous differentiability are given in Section 5.2. The routine test for continu-
ous differentiability is developed in Section 5.3. This test makes use of directional or 
partial derivatives, which are usually easy to compute. Subsequent sections explore 
the many applications of partial derivatives. Most importantly, these sections tell us 
how partial derivatives may be used to represent the full derivative of a function and 
how they enable us to differentiate sums, products and compositions of differentiable 
functions. 
5.1 
FUNCTIONS OF A REAL VARIABLE 
In this section, A is an open subset of K and / : A —> Y is a function. The range 
space Y is an arbitrary normed space. 
Definition 5.1.1 Derivative. Let a € A. If 
f{a + 
r)-f(a) 
lim 
= /'(«) G Y 
(5.1) 
exists, then it is called the derivative of / at a € A. 
The limit in (5.1) is taken in Y. Hence, the assertion that the limit exists means that 
for each e > 0, there is a S > 0 such that if 0 < \r I < S, then 
f(a + 
r)-f(a) 
/'(«) 
< e. 
(5.2) 
An equivalent formulation is that for each e > 0, there is a ¡5 > 0 such that if \r \ < S, 
then 
\\f(a + r)-f(a)~rf'(a)\\<E\r\. 
(5.3) 
This slight re-arrangement helps to set the stage for the general definition of the 
derivative in Section 5.2. 
Finally, note that if /'(a) exists, then 
/(a + r) - /(a) 
lim 
r-*0 
lim \f(a + 
r)-f(a) 
\\f'(a)\\. 
(5.4) 
This follows from the continuity of the norm function. 

FUNCTIONS OF A REAL VARIABLE 
177 
Remarks 5.1.2 Independence of the norm on Y. The above statements are in-
dependent of the norm on Y. This follows easily from the equivalence of any two 
norms on the (finite-dimensional) vector space Y. 
Example5.1.3 Let Y be a normed space with basis B = {uo,...,u m}. 
Let 
/ : R -> Y be defined by 
/(f) = u0 + iui H 
+ tmum 
for all t e R. 
For a £ K, we have 
(a + r)n-an
 
n_x 
. 
lim 
= na 
for all positive integers n. 
7—>o 
r 
Hence, 
f>{a) 
= 
lunf{a 
+ 
r)-f{a) 
= 
l i m - Y ^ 
((a + r)k - 
ak)uk 
r-^0 
r ¿—'k=l 
E
kak~1uk. 
fc=i 
Remarks 5.1.4 Openness of A. The assumption that A is open is implicitly used 
in these statements. In fact, since a £ A, there is a c > 0 such that (a + r) € A 
whenever \r\ < c. Hence in (5.2) and (5.3), the term f(a + r) is always defined 
whenever r is sufficiently small. No further restrictions on r are necessary. This is 
an important point in the definition of the derivative. There should be no restrictions 
on the increments of the variable other than that they are sufficiently small. 
Remarks 5.1.5 Computations of derivatives. Ordinary rules of differentiation. 
To compute the derivative f'(x), 
we have to evaluate the limit 
f(x + 
r)-f(x) 
r 
in Y. If Y = R, that is, if / is a real-valued function, then these evaluations are 
routine for elementary functions. The computations of these ordinary derivatives 
are worked out in a basic calculus course. We know, for example, how to compute 
the derivatives of polynomials, and of functions such as cosx or ex. We shall refer 
to these computations as the ordinary rules of differentiation, and we shall assume 
that all such results are familiar to the reader. These computations are used in our 
examples and problems, but not in the development of the main results. 
If y is a general vector space, then the ordinary rules of differentiation may be 
applied to the components of /. Let W = { wi, ..., w m} be a basis for Y. The 

178 
DERIVATIVES 
components of / : A —> Y are the functions f : A —> R defined by fi = yi- /, 
where the functions y¿ : y —► R are the coordinate functions of the basis W. Hence, 
/ = / i w i H 
h / m w m . 
We see easily that f'(x) e Y exists if and only if each f[{x) e R exists. In this case 
/'(x) = /i(x)w1 + ..- + /^(x)w m . 
Each f¿{x) can be computed by the ordinary rules of differentiation. 
Example 5.1.6 Take Y = R4 with the standard basis ei, e2, e3, e.4. Let 
1 -t 
fit) 
t 
,smi 2,e 5 c o s t,(£2 + l ) 3 ) , t e ( 0 , l ) . 
Then for all t E (0,1), we have 
flit) 
1 -t 
f2(t) = Sm(t2), 
h{t) 
/4(í) = (í2 + l) 3. 
By the ordinary rules for differentiation, 
fit) 
= 
/í(í)ei+y*(í)e 2 + /á(t)e 3+/¿(í)e 4 
1 
"¡2 ,2ícos(t 2),-5siníe 5 c o s t,6í(í ¿ + l) 2 
, t e (0,1 
Example 5.1.7 Let Y = L(R4, R2), the normed space of all linear transformations 
from R4 to R2. For each x G R, let f(x) be the linear map from R4 to R2 defined 
for all u = (ui,«2,U3,«4) G R4 by 
f(x)u-
- 1 
2 a;2 - 1 ln(x2 + 1) 
xex 
5 
7a; 
cosx 
« 4 
We will compute fix). 
The standard basis for Y consists of the linear maps 
Li 
1,2, j = 1,, 3,4, where the standard matrix for each L¿ 
is the 
2 x 4 matrix whose every entry is 0 except the (i,j) entry, which is 1. Thus, the 
coordinate functions of / with respect to the standard basis of Y are the functions fj 
given by fn(x) 
= -l,fi2(x) 
= 2,f13(x) 
= x2 - l,fu(x) 
= ln(a:2 + 1), and so 
on. Hence, for each I E R and each u = (ui, u2, M3, M4) S R4, we have 
f(x)u 
0 
0 2i 
(2x)/(a;2 + l) 
ex(l+x) 
0 
7 
- s i n x 
u2 
U3 
1Í4 

FUNCTIONS OF A REAL VARIABLE 
179 
Definition 5.1.8 Affine approximations. Let / : A —► Y be differentiable at 
a E A. The affine (or first-order polynomial) approximation of / at a is the function 
Ta : R -> Y defined by Taai = /(a) + /'(a)(or - a), x £ R. If Y = R, then 
y = Tax = f(a) + f'(a)(x 
— a) is the familiar equation of the tangent line of the 
curve y = f(x). 
Derivatives and Increments 
In this course, derivatives are used mainly to estimate the increments of functions. 
The starting point is the following result. 
Theorem 5.1.9 Increments and derivatives. Assume that /'(a) exists and that 
||/'(a)|| < M. Then there is a 5 > 0 such that 
\\f(a + r) - / ( a ) || < M\r\ 
whenever \r\ < 5. 
(5.5) 
Proof. Since lim r^o(l/k|)||/(a + r ) - / ( a ) | | = ||/'(a)||, there is a¿ > 0 such that 
ll/(° + ;)-/WII < M 
(5.6) 
\r\ 
whenever 0 < |r| < 5. 
□ 
Corollary 5.1.10 Derivatives and continuity. If f'(a) exists, then f is continuous 
at a £ A. 
Proof. Assume that ||/'(a)|| < M. If rn -> 0, then we see that 
\\f(a + rn)-f(a)\\<M\rn\ 
(5.7) 
for all sufficiently large n. Hence f(a + rn) —> f(a) in Y. 
□ 
Theorem 5.1.9 may be thought of as letting us compare the increments of / to the 
increments of <p(x) = M x. We shall need the following generalization of Theorem 
5.1.9, which allows us to compare the increments of / to the increments of other 
functions ¡p : A —+ R. 
Theorem 5.1.11 Let a e A. If\\f'(a)\\ < tp'{a), then there is a 5 > 0 such that 
\\f(a + r)-f(a)\\<v(r 
+ 
a)-p(a) 
(5.8) 

180 
DERIVATIVES 
whenever 0 < r < S. 
Proof. Let M G R be such that ||/'(a)|| < M < </(a). We have 
lim^ 0(l/|r|) \\f(a + r) - f(a)\\ 
= 
||/'(a)|| < M and 
limr^o(l/r)(ip(a + r) - <¿>(a)) = 
if'(a) > M. 
In this case there is a S > 0 such that 
\\f(a + r)-f(a)\\ 
< 
M\r\ and 
ip(a + r) — ip (a) 
> 
Mr 
whenever \r\ < S. We see that (5.8) is satisfied with this S > 0. 
□ 
Mean Value Theorems 
The main tools for estimating the increments of functions belong to an all-important 
collection of results that we shall refer to as the mean value theorems. Here, we 
present two of these results. In what follows, Y is a normed space and A is an open 
subset of K. 
Theorem 5.1.12 Mean Value Theorem (variable upper bound). Let f : A —> Y 
and let Lp : A —» K. Assume that f'(x) and f'(x) both exist and that \\f'(x)\\ < f'{x) 
for each x € / = [ a, b ] C A. Then 
\\f(b)-f(a)\\<f(b)-f(a). 
(5.9) 
Proof. Let e > 0 and put tp(x) = f(x) + ex. Define 
J - { x e / ! ||/0r) - /(a)|| < </>(£) - ^(o) } . 
(5.10) 
Then J is bounded since J C I and nonempty since a £ J. Hence c = sup J exists 
and c E I. We show first that c 6 ,7. This is clear if c = a. Otherwise, we see easily 
that there is a sequence xn € J such that xn —» c in E. Hence 
| | / ( a ; n ) - / ( a ) | | < # E n ) - V ( a ) 
(5.11) 
for all n £ N. Now, since /'(c) and iß'(c) = f'(c) + e both exist, Corollary 5.1.10 
shows that both / and ip are continuous at c e 7. Hence /(x n) —> /(c) in F and 
^(xn) ~^ ^(c) m K- Then, from (5.11) and from the continuity of the norm, 
\\f(c)-f(a)\\<iP(c)-^a). 

FUNCTIONS OF A REAL VARIABLE 
181 
Hence c € J. 
Next, ||/'(c)|| < ip'(c) < ¡p'(c) + e = ip'(c). Therefore, Theorem 5.1.11 shows that 
there is a S > 0 such that 
||/(c + r ) - / ( c ) | | < V ( c + r ) - V ( c ) 
whenever 0 < r < 6. Now assume that c < 6. Then there is an r such that 0 < r < 5 
and such that (c + r) = e < 6. Then 
ll/(e)-/(a)|| 
< 
||/(e)-/(c)|| + ||/(c)-/(a)|| 
< 
(^(e) - ip{c)) + (^(c) - ^(a)) = V(e) - ^(a). 
This shows that e € J. This is a contradiction, since c < e and c = sup J. Hence 
c = 6. It follows that 
11/(6) - /(a)|| < ^(ö) - ^(a) = (¥-(6) - <p(a)) + e(b - a). 
(5.12) 
But (5.12) is true for all e > 0. Then (5.9) follows. 
□ 
Theorem 5.1.13 Mean Value Theorem (fixed upper bound). Assume that f'(x) 
exists and that \\f'(x)\\ < M for a < x < b. Then \\fib) - f(a)\\ < M (b - a). 
Proof. Apply Theorem 5.1.12, with ip(x) = Mx. 
□ 
Even though Theorem 5.1.13 is a special case of Theorem 5.1.12, it is the version of 
the mean value theorem that we shall use most widely (although we employ Theorem 
5.1.12 in the discussion of Taylor polynomials in the next subsection). We freely use 
the name 'mean value theorem' for either of these two results. 
Example 5.1.14 Suppose that /, g are functions from A into Y and / = [a, b] c A. 
If \\f'{x) - g'(x)\\ < M for all x e I, then 
||/(6)-/(a)||<|| f f(6)- f l(a)||+M|6-a|. 
To see this, set h{x) = f{x) — g(x) for all x S A. Then h'(x) = f'(x) — g'{x) at 
each x where / and g are differentiable. Thus, by assumption, ||ft/(.x)|| < M for all 
x & I. Hence, by Theorem 5.1.13, 
11/(6) " /(a) ~ (9(6) - g(a))\\ = \\h(b) - h(a)\\ < M \\b - a\. 
Thus, it follows from the triangle inequality that 
| | | / ( 6 ) - / ( a ) | | - | | f f ( b ) - 5 ( a ) | | | < A / | 6 - a | . 
Hence, ||/(6) - f(a)\\ < \\g(b) - g(a)\\ + M \b - a\. 

182 
DERIVATIVES 
Remarks 5.1.15 Relation to other mean value theorems. The classical mean value 
theorem states the following. Suppose / = [a, b] and / : I —> M is a real-valued 
function. If / is continuous on I and differentiable in the interior of 7, then there 
is a c e R such that a < c < b and such that fib) - f(a) = f'(c)(b - a). This is 
a stronger result than our mean value theorems, but it is valid only for real-valued 
functions. There is no comparable result for vector-valued functions. It turns out, 
however, that the classical mean value theorem can be replaced by 5.1.12 or 5.1.13 
in all the applications considered in this course. The mean value theorem 5.1.13 can 
be obtained from the classical result, but the direct proof given above seems to be 
shorter and more instructive. Also, note that the mean value theorem 5.1.12 is related 
to the well-known Cauchy Mean Value Theorem. Since we do not need this latter 
result, we shall not discuss it here. 
Taylor Polynomials 
Definition 5.1.16 Higher-order derivatives. If / : A —> Y has a derivative f'(x) £ 
Y at every x 6 A, then we have a well-defined derivative function f 
: A —> Y. 
If this new function also has a derivative at x & A, then it is called the second 
derivative of / at x e A and denoted by f"{x) 
£ Y. If the second derivatives 
exist at each x e A, then they define the second-derivative function f":A—> 
Y. 
Higher-order derivatives are defined by induction. If the nth-order derivative function 
j(n) . A_+Y exists and has a derivative f^-n+l",{x) 
£ Y at every x £ A, then these 
derivatives define the (n + l)st-order derivative function /(™+1) : A —> Y. We also 
write /(D = /', /(2) = /", and /(°) = /. 
Lemma 5.1.17 Assume that f^k\a) 
= 0 for k = 0, 1, ..., (n - 1) at a certain 
point a € A. Also assume that there is an R > 0 such that \\f^-n\a + x)\\ < M 
whenever \x\ < R. Then 
||/(a + a-')ll <{l/n\)M\x\n 
whenever \x\ < R. 
(5.13) 
Proof. First, assume that x > 0. Proceed by induction on n e N. Let n = 1. Let x 
be fixed, 0 < x < R. Then ||/'(a + t)\\ < M for all í e [0, x\. Therefore by the 
mean value theorem 5.1.13, 
||/(a + x)|| = ||/(a + x ) - / ( a ) | | < ((a + x) - a)M = xM. 
A similar result holds if —R < x < 0. This proves the result for n = 1. 
Now assume the result for (n — 1), n > 2. Given / : A —> Y satisfying the 
hypotheses of the lemma, let g = f. Then 
g{k\a) 
= 
f{k+1\a) 
= 0fork = 0, 1, ..., (n - 2), and 
^"-^(ffl + aOH 
= 
||/( n )(a + x) || <M whenever \x\ < R. 

FUNCTIONS OF A REAL VARIABLE 
183 
Hence, by the induction hypothesis, 
||5(a + a;)|| = H/'(a + aOH < (l/(n - 1)1) M x ^ 
(5.14) 
whenever 0 < x < R. Define 
(l/n\)Mxn 
if x > 0, 
^X) 
' 0 
if x < 0. 
Let x be fixed, 0 < x < R. Then we see that (5.14) can be expressed as 
Il/'(a + t)ll <¥>'(*) 
for all t £ [ 0, x}. Hence the mean value theorem 5.1.12 shows that 
\\f(a + x)\\ 
= 
\\f(a + 
x)-f(a)\\ 
< 
(<p(x) - <p(Q)) = 
(l/n\)Mxn. 
This proves the result for x > 0. Arguments for x < 0 are similar. 
□ 
Example 5.1.18 Let / : R —> Y be such that /'"' exists on R for all n. Assume 
that/(") (0) = 0 for all n and j|/(n)(a;)|| < nR for all n and all x eRwith|xj < R. 
Then f(x) = 0 for all x G R. To see this, let n £ N and let R > 0 be arbitrary. 
Apply Lemma 5.1.17 with a = 0, M = nii to obtain 
||/(z)|| < (l/n!) (ni?) |x|" < , 
1 ,, Rn+1 whenever bl < R. 
(n — 1)1 
Since the above holds for all n £ N, we get 
||/(x)!| < lim 
— Rn+1 = 0 whenever Ixl < R. 
n-*oc (n — l j ! 
Since this is true for any R > 0, it follows that f(x) = 0 for all i e l . 
Definition 5.1.19 Taylor polynomials. Let / : A —> Y be a function. Assume 
that /("'(a) exists for a certain n G N and a £ A. Then the (nth-degree) Taylor 
polynomial Pn : M. —> F of / : .A —> y at a £ A is defined as 
n 
P„(a + x) = ^ - / « ( a ) a ;
f c 
(5.15) 
fc=0 
for all x G R. Note that the value of Pn : R —» y at each a; G Risa linear combination 
of (n + 1) fixed vectors f^k\a) 
G Y, k = 0, 1, ..., n. The coefficients of these 
fixed vectors are the powers 
(l/k\)xk. 

184 
DERIVATIVES 
Example 5.1.20 Letf(x) 
— (cosa; 
f{l\x) 
f(2\x) 
f{3)(x) 
f(A\x) 
f(5)(x) 
= 
= 
= 
= 
= 
ex, x — sin x) for all i £ l , Then 
(— sin x, ex, 1 — cos x) 
(— cosx,ex,sinx) 
(sinx, ex,cosx) 
(cosXje1, — sinx) 
(— sinx, ex, — cosx) 
The 5th-degree Taylor polynomial of / at 0 is F5 : 
given by 
5 
1 
P5(x) 
= J2-r}f{k)(0)xk 
forallxeK. 
k=0 
Hence, for all i £ l , 
P5(x) 
= 
(1,1,0) +a; (0,1,0) + — (-1,1,0) 
3! 
4! 
+ ^ ( 0 , l , l ) + ^ ( l , l , 0 ) + 5-(0,l,-l). 
5! 
Lemma 5.1.21 Derivatives of Taylor polynomials. Let Pn : 
degree Taylor polynomial of f : A —> Y at a € A. Then 
Pik)(a) 
f{k){a) 
if0<k<n, 
0 
if k> n. 
Y be the nth-
(5.16) 
Proof. This follows by an easy computation. 
D 
Theorem 5.1.22 Approximation by Taylor polynomials. Let f : A —> Y be a 
function. Assume that there is an R> 0 such that f^n> (a + x) exists and 
||/(™)(a + x)|| <Mforall\x\ 
< R. 
Then \\f(a + x) - P„-i{a + x)j| < (l/n!)M \x\n whenever \x\ < R. 
Proof. Define g : A -> Y by g{x) = /(x) - Pn-i(x). 
We see that g{k)(a) = 0 
forO < k < ( n - 1) and ||^™)(a + x)|| = \\f{n)(a + x)|| < M whenever |x| < R. 
Then the proof follows from Lemma 5.1.17. 
D 
Example 5.1.23 Let f(x) = (cosx, ex, sinx). Then, 
||/(n)(x)||2 = sin2 x + e2x + cos2x = 1 + e2x 

FUNCTIONS OF A REAL VARIABLE 
185 
for all n G N and x G R. If R > 0 and M = (1 + e2^)1/2, then 
||/(n)(V)|| < (1 + e 2 ñ) 1 / 2 = M 
whenever \x\ < R. 
Hence, by Theorem 5.1.22, 
||/(a0 - P„(a;)|| < (l/(n + 1)!)A/ |x|"+1 < (l/(n + 1)!)M P " + 1 
whenever |x| < P. Since limn M Rn/n\ = 0 for all R > 0, we see that 
/ O ) = lim P„(x) 
for all 
xeR. 
n—»-CXD 
Taylor Series 
Assume that /("■) (a) exists for all n € N. Then P„(x), as defined in (5.15), exists for 
each n € N. Then P„(a;) is an approximation of /(a;) for x close to a for each fixed 
n G N. The exact formulation of this approximation is given by Taylor's theorem, 
Theorem 5.1.22. Now we fix x G R and want to know if limn Pn{x) exists and is 
equal to f(x). The answer is easy; it follows from Theorem 5.1.22 above. 
Theorem 5.1.24 Approximation by Taylor series. For each n G N and r > 0, let 
Mn(r) = sup { ||/<n>(a+ x)|| | |a:| < r } 
if it exists. Assume that there is an R > 0 such that (1/ra!) Mn(R)Rn 
is a bounded 
sequence. Then 
f(a + x) 
= 
f(a) + f(a)x 
+ --- + ~f^(a)xn 
+ --- 
(5.17) 
n 
1 
= 
lim„ Y, 
^f{k\a)xk=\imnPn(a 
+ x) 
(5.18) 
fc=o 
whenever \x\ < R. Also, the series in (5.17) can be differentiated term-by-term to 
obtain, for all fceN, 
/(fc)(a + x) = /(fc)(a) + f(k+1\a)x 
+ ■■■ + -}f{k+n){a)xn 
+ ■■■ . 
(5.19) 
Proof. Let (1/n!) Mn(R)Rn 
be a bounded sequence. We see that 
lim n(l/n!)M n(P)r" = 0 
whenever |r| < R. Then (5.17) follows from Theorem 5.1.22. For the second part, 
note that g(a + x) = f^ 
(a + x) satisfies the same hypotheses as f{a + x). 
□ 

186 
DERIVATIVES 
Motions of Euclidean Spaces 
Motions of Euclidean spaces provide instructive examples of differentiation. We 
consider only the following type of motions. 
Definition 5.1.25 Motions. Let X be a Euclidean space. Let A C R be an open 
interval containing the origin O e l 
Let s : A —> X and S : A —> L(X, X) be 
two differentiable functions. Assume that 5(0) = /is the identity on X. These two 
functions define a mapping M{t) : X —* X by 
M(i)a = s(í) + S(í)a, 
a e l 
for each t G A Each M(i) : X —> X is an affine mapping of X into itself. The 
family {M(t)}, t G A, is called a motion of X. The trajectory of a £ X is the 
function r : A —> X defined as r(f) = M(í)a, í G A Note that r(0) = a. Hence 
r(£) = M(i)r(0) and r(í), í G A is the trajectory of r(0). Note that s(i), £ G A is 
the trajectory of s(0) G X. 
Definition 5.1.26 Velocities. Notations are the same as in Definition 5.1.25 above. 
Let r : i - > X b e a trajectory. Then its derivative at t G A 
v'(t) = lim M(l/A)(r(í + h)- 
r(í)) 
is defined as the velocity on this trajectory at the point r(£). We see that 
r'(í) = s'(í) + S'(í)(r(0)). 
The proof of this is given as Problem 5.11. 
Definition 5.1.27 Rigid motions. A motion M(t) is called a rigid motion if it 
preserves the distances between any two points. More explicitly, M(t) is a rigid 
motion if 
||M(t)a-M(f)b|| = | | a - b | | 
for all a, b G X and for all t G A It is clear that M(t) is a rigid motion if and only if 
S(t) is an isometry for each t G A This means that ||5(i)x|| = ||x|| for each x G X 
and for each t G A. 
Examples and applications of these notions are given as problems. 
Problems 
In the following problems, the norm on E" is the standard Euclidean norm. Also, A 
always denotes an open interval in K. 

FUNCTIONS OF A REAL VARIABLE 
187 
5.1 
Define / : R -> K2 by /(f) = (cos 2irt, sin 2irt). Find /<") (f) and show that 
</(n)W> /(™+1)(¿)> = 0 for allí e R a n d n e N . 
5.2 
Define/ : R -> R3 by/(f) = (cos2?rí, sin27rf, 2nt). Find / ( n )(i) and show 
that (f{n)(t), 
f{n+1)(t)) 
= 0 for all t e R and n e N, n > 1. 
5.3 
Assume that / : A -+ Rn is differentiable and that ||/(f)|| > 0 for all f e A. 
Show that u(f) = /(f)/||/(f)|| is also differentiable and (u(t), u'(t)) = 0 for all 
t e A. 
5.4 
Define / : R -> R2 by /(í) = (e4 COS2TTÍ, e* sin27rí). Show that the angle 
between the vectors /(f) and f'(t) is constant. 
5.5 
Let X be a normed space. Given A e L{X, X) and t 6 R, let 
/(t) = e^=lim n(/+AA+^A 2 + - + ^ » ) 
as in Problem 4.48. Show that / : R -> L(X, X) is differentiable. Find /'(f). 
5.6 
A disc of radius 2R in the xy-plane rolls on the x-axis without gliding. At 
time f the center of the disc is at the point (vt, 2R), where v > 0 is a constant. Let 
P be the point on this disc which is at point (0, R) at time f = 0. Find the equation 
/ : R —> R2 of the the trajectory of P. Find the points where this trajectory has 
horizontal tangents /'(f). 
5.7 
Let /, g : A —> Rn be two differentiable functions. Then show that 
F=(f,g):A->R 
is also differentiable and F'(t) = (/'(f), g(t)) + (/(f), g'(t)). 
5.8 
Let /, g : A —► R3 be two differentiable functions. Let 
F = f x g : A - R3, 
with the usual cross product in R3. Show that F : A —> R3 is also differentiable and 
F(t) = (f(t) 
x g(t)) + (/(f) x g'(t)). 
5.9 
Frenet formulas. Consider a function r : /I —> R3 as the equation of a curve 
CinR 3. Then 
u(f) 
= 
r'(f)/||r'(f)|| is called the unit tangent vector, 
n(f) 
= 
u'(f)/||u'(f)|| is called the unit principal normal vector, and 
b(f) 
= 
u(f) x n(f) is called the binormal vector 

188 
DERIVATIVES 
of C at the point r(i). We assume that C is such that all three vectors are well-defined 
at every point of C. Weletp(í) = ||r'(í)|¡-1 and define 
the curvature of C at r(t) 
as p(t) = \\u'(i)\\p(i) and 
the torsion ofC at r(i) 
as r(t) = ±||b'(i)|| p(t). 
Show that (u(i), n(i), b(i)) is an orthonormal basis for R3 and that 
u'(t)p(t) 
= 
p(t)n(t), 
n'{t)p(t) 
= 
-p(t)u(t)-T(t)b(t), 
and 
b'(í)p(t) 
= 
r(í)n(í). 
The sign of the torsion r(t) is defined by the last formula. The formulas above are 
known as the Frenetformulas. We will refer to the set of three vectors (u(í), n(í), b(í)) 
as the Frenet vectors of the curve C at the point r(i). 
5.10 
Compute the Frenet vectors for the helix 
r(i) = (R coswi, R sinujt, Ai), Í 6 K, 
and verify the Frenet formulas. Here R > 0 and u, A e R are constants. 
Problems on the Motions of Euclidean Spaces 
5.11 
Let X be a Euclidean space and S : A —» L(X, X) a differentiable function. 
Show that for each a e X the function r(i) = S(t) a,t £ A, is also differentiable 
andr'(t) = S"(i)a. 
5.12 
Let X he a Euclidean space. Let 5 : M —> L(X, X) be such that 
5(o + b) = S{b) ■ S(a) 
for all a, b G R. If S"(0) = ^ exists then show that 5(i) = eAt for all í G R. Here 
eT = E^oi 1/«!) 7™- T 6 Í'í-X', -X"). as defined in Problem 4.48. What is S'(t) in 
terms of S{t) and 5'(0)? 
5.13 
Let X be a Euclidean space. Let 5 : A -^ L(X, X) be such that ||5(i)v|| = 
||v|| for all t G A and v G X. If S'(t) exists, then show that (S'(t)v, S(t)v) = 0 
for all t G A and v G X. 
5.14 
Rotations of K3. Let w G R3 be a unit vector and let w E l . The rotation of 
R3 about the axis w with the angular velocity ofui is defined as follows. Complete 

FUNCTIONS OF A REAL VARIABLE 
189 
w to an orthonormal basis (u, v, w) so that w = u x v. Then let 
i?(i)(mi + /?v + 7w) 
= 
aR(t)u + ßR(t)v + jR(t)w, 
where 
R(t)u 
= 
cosujt u + smut v 
R(t)v 
— — sinLütu + cos u)t\r 
R(t)w 
= 
w 
for all x = (au + ß\ + 7w) G R3 and for all t G R. Show that R(t) is a rigid 
motion. Also show that 
R'(t)x. = uw x Ä(i)x 
for all x G R3 and for all t G R. 
5.15 
LetS : A -> L(R3, R3) be a differentiable function such that ||S(i)x|| = ||x|| 
foralli G Aandx G R3. Show that for each t G A there is a unique vector m(i) G R3 
such that 5'(i)x = m(i) x S(i)x for all t G A and for all x G R3. 
Remarks. Rigid motions about a fixed point. We see that S(t) in the preceding 
problem is a rigid motion about the fixed origin. This problem shows that at each 
instant t G A, the velocities for the rigid motion are the same as the velocities in a 
rotation. The angular velocity of this rotation is given by ui{t) = ||m(i)|| and the axis 
is given by the unit vector w(í) = (l/a>(í))m(í). They are called the instantaneous 
angular velocity and instantaneous axis of rotation of this rigid motion S(t) with a 
fixed point. 
5.16 
Let e e l 3 . Define A : R3 -> R3 by Ax = c x x for x G R3. Compute 
eAix for all t G R and x G R3. 
5.17 
Helicoidal motions. Let R(t) : R3 —► R3 be a rotation given in terms of 
w G R3 and u> e R as in Problem 5.14. Let T(t) : R3 —> R3 be a translation given 
by T(i)x = x + ia, x G R3, t G R, where a G R3 is fixed. Then 
#(i)x = ic + i?(i)x, t G R, x G R3 
is called a helicoidal motion. Show that helicoidal motions are rigid motions. If 
r(i) = H(t)x, then show that the velocities are given as r'(i) = c + w w x i2(i)x. 
5.18 
General rigid motions. Let M(í)x = s(í) + 5(í)x, í G ^4, be a general rigid 
motion. If r(i) = M(i)x, then show that for each instant t G A the velocities r'(i) 
are the same as the velocities in a helicoidal motion. The translational and rotational 
parts of this helicoidal motion depend on t G A. They are called the instantaneous 
translations and rotations in a general rigid motion. 
5.19 
Instantaneous translations and rotations of Frenet vectors. The Frenet 
vectors (u(f), n(i), b(i)) of a curve r : A —* M3 form an orthonormal basis for R3 

190 
DERIVATIVES 
for all t G A. Hence, they define a rigid motion as follows. For convenience, assume 
that 0 G A and define 
S(i)(au(0) + /3n(0) + 7b(0)) = au(t) + ßn(t) + -yb(t) 
for all x = (a, /?, 7) G R3. Then M(i)x = r(t) + S(t)x defines a rigid motion of 
R3. Find the instantaneous translations and rotations of this rigid motion. 
Problems on Plane Curves 
Let r : A —> R2 c R3 be a plane curve C with the corresponding Frenet vectors 
(u(i), n(f), b(i)). Let k be a unit vector orthogonal to the subspace R2 in the vector 
space R3. 
5.20 
Show that u(i) and n(i) are orthogonal to k and b(i) = ±k and r(i) = 0 for 
all i e A Also show that n(i) = ±k x u(f) and u(f) = ±k x n(i) for all t € A. 
5.21 
The point e(i) = r(i) + (l/p(t)) n(£) is called the center of curvature ofC 
at the point r(i). Then e : A —» R2 c R3 defines another curve E. Show that the 
unit tangent vector of E at e(i) is ±n(i). 
5.22 
Define S : A -> L(R2, R2) as follows. Let t0 G A be fixed. Any v G R2 
has a unique expression as v = r(í0) +«u(í 0) + bn(to). Then 
5(i)v = v(i) = r(i) + au{t) + bn(t). 
Show that the velocity field v'(i) at any instant t G A is the same as the rotational 
velocities of the plane R2 about the point e(í) = r(í) + (l/p(í))n(í) with the angular 
velocity Lü(t) = p(t) \\r'(t)\\. 
5.23 
Let L and L' be the lines in R2 passing through the points r(i) and r(i') and 
in the directions of n(i) and n(i'), respectively. Show that the intersection points 
P(t, t') of these lines converge to e(i) as t —»t'. 
5.24 
Verify the results of the last four problems for the parabola given by r(t) = 
(i, (l/2)i2) G R2 for all t G R. 
5.2 
DIFFERENTIABLE FUNCTIONS 
The main purpose of this section is to define the differentiability of a function / 
between two normed spaces X and Y. 
As stated in the chapter introduction, a 

DIFFERENTIABLE FUNCTIONS 
191 
function is differentiable at a point if it can be locally approximated by a linear 
transformation. This linear transformation is then the derivative (or full derivative) 
of the function at that point. Plainly, this way of understanding the derivative requires 
a subtle shift from the familiar definition of the derivative as a real number in the 
one-variable case, and indeed from its definition as a vector in the special case that 
we have just presented in Section 5.1. 
Perhaps the easiest way to motivate this new way of thinking about the derivative 
is to note the equivalence, in the one-variable case, between the existence of the 
numerical limit f'(a) and the fact that, in a neighborhood of a, the graph of f(x) can 
be approximated by the tangent line whose slope is f'(a). 
That is, f(a + r) can be 
very well estimated by f(a) + [f'(a)](r). Similarly, consider the situation of Section 
5.1, where / : A —> Y for a normed space Y, A is an open subset of R, and a € A. 
In this case, the existence of the vector derivative f'(a) is equivalent to the fact that, 
in a neighborhood of a, f(a + r) can be very well estimated by f(a) + [/'(a)](r). 
The meaning of 'very well estimated' is given precisely by the formulation for the 
derivative that we pointed out in (5.3): for each e > 0, there is a Ö > 0 such that if 
\r\ < S, then 
\\f(a + 
r)-f(a)-{f(a)](r)\\<e\r\. 
The notation [/'(a)] is meant to be suggestive. Any vector y in y corresponds to 
the linear transformation Ty :R —> Y given by Ty{r) = ry. Conversely, any linear 
transformation T in L(R, Y) corresponds to the vector y = T(l). It turns out that 
thinking of [/'(a)] as a linear transformation in L(K, Y), rather than as a vector, is 
the key that lets us generalize the definition to cases where the domain space X is 
not R. We make this precise in Definition 5.2.1: the derivative /'(a), when it exists, 
is a linear transformation from X to Y. (Note that we shall write /'(a)u, rather than 
[/'(a)](u), for the application of the linear transformation to a vector u G X.) 
The main difficulties with this novel definition are how to tell when a function has 
a derivative and how to picture the derivative. We get some help by introducing the 
notion of a partial or directional derivative. Suppose that X = R2 and Y = M, 
and we have a function / : M2 —> K. Given a point a = (a, b), we can watch 
how / behaves along straight lines through a—for example, horizontal lines of the 
form (a + t, b) and vertical lines of the form (a, b + t). Either of these restrictions 
turns / into a function of one variable t, and the familiar (numerical) derivatives 
associated with these restricted functions are called partial or directional derivatives. 
Furthermore, there is no need to restrict ourselves to horizontal and vertical lines. 
Directional derivatives may be taken along any line through a (Definition 5.2.11). 
At this point, there are two important questions about the relationship between the 
full derivative and the directional derivatives. First: given the full derivative, can 
we compute the directional derivatives? Second: given the directional derivatives, 
can we compute the full derivative? In this section, we answer the first question 
in the affirmative and explain the computation. We answer the second question in 

192 
DERIVATIVES 
the negative: unfortunately, the existence of all directional derivatives is compatible 
with the non-existence of the full derivative. The good news, deferred to Section 
5.3, is that the existence of continuously differentiable directional derivatives does 
guarantee the existence of the full derivative. In this case, the directional derivatives 
even give us a convenient matrix representation of the full derivative. 
In what follows, X and Y are any two normed spaces and A is an open set in X. 
Definition 5.2.1 Differentiable functions. A function / : A —> Y is said to be 
differentiable at a G A if there is a linear map T G L(X, Y), such that 
l i m l [ / ( a + r ) - / ( a ) - T r | | ^ a 
r—»0 
||r|| 
Lemma 5.2.4 will show that if such a T : X —► Y exists, then it is unique. It is called 
the derivative (or full derivative) of f at a G A and is denoted by /'(a). Note that 
/'(a) : X —> Y is a linear operator and its value at x 6 X is written a s / ' (a) (x) G Y. 
Remarks 5.2.2 Other formulations. An explicit formulation of (5.20) is that for 
each e > 0 there is a S > 0 such that 
||/(a + r) - /(a) - T*r|| < e ||r|| whenever ||r|| < S. 
(5.21) 
Since A is open, there is a c > 0 such that Bc(a) c A. We choose S < c. Therefore, 
(a + r) G A and consequently /(a + r) is defined whenever ||r|| < 5. 
An equivalent form of (5.21) is the following. For every zero-sequence rn in X there 
is a zero-sequence tn in R such that 
||/(a + r n) - /(a) - Tr n|| < tn ||rn||. 
(5.22) 
This form avoids the e-S statements. Sometimes this may be an advantage. The 
equivalence of (5.21) and (5.22) follows easily. 
The next two lemmas establish that there can be at most one linear transformation T 
that satisfies (5.20). If the derivative exists, then it is unique. 
Lemma 5.2.3 IfS G L{X, Y) and i/limr^0( ||Sr|| / ||r||) = 0, then S = 0. 
Proof. The condition limr_o( \\Sr\\ / ||r||) = 0 means that for each e > 0 there is a 
6 > 0 such that 
(||Sr|| / ||r||) < e whenever 0 < ||r|| < 6. 

DIFFERENTIABLE FUNCTIONS 
193 
Let u G X be a nonzero vector. Choose a nonzero t G R so that ||iu|| < 5. Then 
(||5(íu)||/||íu||) = ( | | S u | | / | | u | | ) < £ 
and therefore ||5u|| < e||u||. This is true for any e > 0. Hence S\x = 0 for all 
ue X. This means that 5 = 0. 
D 
Lemma 5.2.4 Let T, T" G L(X, Y) both satisfy (5.20) in the definition of deriva-
tives, 5.2.1. Then T = T"'. 
Proof. Let S = T - T. We have 
||Sr|| 
= 
||Tr-T'r|| 
< 
||/(a + r) - /(a) - T'r|| + ||/(a + r) - /(a) - Tr||. 
Then limr^0( ll^rll / ||r||) = 0 by (5.20) in the definition of derivatives, Definition 
5.2.1. Hence S = 0 by Lemma 5.2.3. D 
In light of the foregoing lemmas, we can speak of the derivative f'(a) and we can 
define an associated affine approximation function. 
Definition 5.2.5 Affine approximations. Let / : A —> Y be differentiable at 
a G A. The affine (or first-order polynomial) approximation of / at a is the function 
Ta : X -► Y defined by Tax = /(a) + /'(a)(x - a), x G X. Note that the linear 
transformation /'(a) does not approximate the function, but rather its increments. 
The function itself is approximated by the affine approximation Ta. 
Since / ' is a function from X to L(X, Y) and both of these are normed spaces, it 
makes sense to ask whether / ' is a continuous function. 
Definition 5.2.6 Continuously differentiable functions. A function / : A —> Y is 
said to be continuously differentiable on A if there is a continuous function 
f':A—> 
L{X, Y), such that 
l i m l l / ( a - r r ) - / ( a ) - / » , | | = 0 
r^O 
||r|| 
for all a G A. 
Remarks 5.2.7 An important question. How can we decide if a function is differ-
entiable? Unfortunately there is no easy answer in general. Lemma 5.2.8 formulates 
a necessary condition for differentiability, but it is not a sufficient condition. For 

194 
DERIVATIVES 
continuous differentiability, however, there is a routine test. Lemma 5.2.9 formu-
lates a necessary condition for continuous differentiability that turns out also to be 
sufficient. This condition provides the desired test for continuous differentiability. 
Lemma 5.2.8 A necessary condition for differentiability. Assume that f : A —> F 
is differentiable ata.EA. 
Then 
l i m i ( / ( a + i r ) - / ( a ) ) 
(5.24) 
exists for each r s X. 
Proof. Assume that the derivative T = /'(a) exists. We will show that the limit in 
(5.24) converges to Tr. This is clear if r = 0. Otherwise, 
/(a + i r ) - / ( a ) _ T r 
t 
||/(a + t r ) - / ( a ) - r t r | | 
1 " 
llirll 
shows that lim t_ 0(l/i)(/(a + ir) - /(a)) = Tr. 
D 
Example 5.3.1 below shows that the condition in Lemma 5.2.8 is not sufficient for 
differentiability. Note, however, that if the domain space X is one-dimensional, then 
Lemma 5.2.8 gives a necessary and sufficient condition for differentiability. This 
follows by an easy argument and is left as an exercise. Hence for one-dimensional 
domains, the existence of the limit in (5.24) can be taken as the definition of differ-
entiability. 
Lemma 5.2.9 A necessary condition for continuous differentiability. Assume 
that f : A —> Y is continuously differentiable on A. Then, for each fixed r £ X, 
lim -t (/(a + ir) - /(a)) = F(a) 
(5.25) 
exists and defines a continuous function F : G - * F . 
Proof. Lemma 5.2.8 shows that F(a) exists and is equal to /'(a)r. 
But, by 
hypothesis, / ' : A —> L(X, Y) is continuous. Then F( •) = / ' ( • )r : A -> Y is 
also continuous for each fixed r E l . 
□ 
Theorem 5.3.4 below shows that the condition in Lemma 5.2.9 is also sufficient for 
continuous differentiability. Hence this condition provides an easy test for continuous 
differentiability. These arguments are left to the next section. The rest of this section 
contains miscellaneous remarks about full and directional derivatives, together with 
some computational examples. 

DIFFERENTIABLE FUNCTIONS 
195 
Restricted and Directional Derivatives 
We continue to assume that A is an open set in X and that / : A —> Y is a function. 
Definition 5.2.10 Restricted derivatives. Let U be a subspace of X. Then / : A —> 
y is said to have a restricted derivative at a £ A along [/ if there is a linear map 
S : U -> r such that 
lim l/C + 'l-ZM-fr-Uo, 
(5.26) 
uec/,u^o 
||u|| 
Simply by replacing X with U in the proof of Lemma 5.2.4, we can see that if such 
an S : U —> Y exists, then it is unique. S is called the restricted derivative of f at 
a G A along U and is denoted by /¿/(a). /¿-(a) : U —> y is a linear operator and its 
value at u G E/ is /^(a)(u) G y. 
Definition 5.2.11 Directional derivatives. Let u G X. If 
/(a + iu) - / ( a ) 
lim ^ 
'-—J-±^ 
(5.27) 
t^o 
t 
exists, then it is called the directional derivative of f at a G ^4 a/t>«g u. The 
directional derivative is denoted as /'(a; u). It is a vector in Y. 
Remarks 5.2.12 Relations between the derivatives. If /'(a) G ¿(X, Y) exists, 
then /¿/(a) G L(U, Y) also exists for all subspaces U C X. Also, /¿/(a) = 
f'(a)\u 
is the restriction of /'(a) to [/. Furthermore, if the restricted derivative f¡j{a) G 
L(U, Y) exists, then the directional derivatives /'(a; u) G Y also exist for all u G E/. 
The most significant relationship here is that if f¡j(a) exists, then /'(a; u) = 
f¡j(a)(u). 
That is, we can compute the directional derivative along u by apply-
ing f¡j(a) to u. (Lemma 5.2.8, which shows that /'(a; u) = /'(a)(u), establishes 
this result for the case where U = X.) 
The proofs for all of these claims are easy. All are similar to the proof of Lemma 
5.2.8 and all of them are left as exercises. 
Example 5.2.13 Suppose that / : M2 -* E 3 is differentiable at 0. Given that 
l i m / ( ^ ) - / ( 0 ) 
a n d 
/Qr, 2 r ) - / ( 0 ) 
we show how to compute 
Ihn'fr-'-)-*0). 
Í—>o 
r 

196 
DERIVATIVES 
First, note that the given equations are equivalent to 
/'(0; (1,1)) = (2,-1,1) 
and 
/'(O; (3,2)) = (1,4,3). 
Hence, since / is differentiable at 0, Theorem 5.2.12 implies that /'(0)(1,1) = 
(2, —1,1) and /'(0)(3, 2) = (1,4, 3). Furthermore, the same theorem implies that 
U m /(r, - r ) - /(0) 
= 
l i m / ( 0 + r ( l , - l ) ) - / ( 0 ) 
7-^o 
r 
'—>o 
r 
= 
/'(0)(1,-1). 
Since (1, -1) = -5(1,1) + 2(3, 2) and /'(O) is linear, we have 
/'(0)(1,-1) 
= 
-5/'(0)(l,l) + 2/'(0)(3,2) 
= 
-5(2,-1,1) + 2(1,4,3) 
= 
(-8,13,1). 
Hence, 
lim / ( r ' ~ r ) " / ( 0 ) = /'(0; (1, -1)) = /'(0)(1, -1) - (-8,13,1). 
?—»o 
r 
Example 5.2.14 Let / : K2 ->• R3 be differentiable at 0 and /(O) = O with 
lim f(r'2r) 
=(1,-2,1) 
and 
lim ^
^ 
= (2,1,5). 
(5.28) 
We show how to compute /'(0). Note that since /(0) = 0, Equations (5.28) say that 
/'(O; (1, 2)) = (1, -2,1) 
/'(O; (2,1)) = (2,1, 5). 
So, by Theorem 5.2.12, 
/'(0)(1,2) = (1,-2,1) 
and 
/'(0)(2,1) = (2,1,5). 
Also, for any (x, y) € K2, we have 
2w — x ,„ „. 
2x — y . „, 
(a;,i/) = -^3—(1,2) + —g-^(2,l). 
Thus, since /'(0) is linear, 
/'(0)(x,y) 
= 
? ^ / ' ( 0 ) ( l , 2 ) + ^ p / ' ( 0 ) ( 2 , l ) 
= 
^ ( 1 , - 2 , 1 ) 
+ ^ ( 2 , 1 , 5 ) . 

DIFFERENTIABLE FUNCTIONS 
197 
Differentiability of the Restricted Function 
Lemma 5.2.15 Open sets in subspaces. Let A be an open set in a normed space 
X. Let U be a subspace of X. Then B = A n U is an open set in U, considered as 
a normed space by itself. 
Proof. Let b E B c A. Since A is open in X, there is a 5 > 0 such that if ||x|| < 5, 
thenb+x E A. Butifx = u G U, then also b + u G U and therefore b + u £ AnU. 
Hence b + u G B whenever u G U and ||u|| < S. This shows that B is an open set 
vaU. D 
Lemma 5.2.16 Derivatives of restricted functions. Let A be an open set in X 
and U a subspace of X. Let B = A P\ U. Let f : A —> Y be a function and 
let ip = f\B : B —> Y be the restriction of f to B. Ifb£B 
= AnU and if 
/'(b) G L(X, Y) exists, then ip'{b) e L(U, Y) also exists and <¿>'(b) = 
f'(b)\u-
Proof. Lemma 5.2.15 above shows that B is an open set in U. We claim that 
T = f'(b)\u 
G L(U, Y) is the derivative of if : B -> Y at b G B. In fact, 
||y(a + u)-y(a)-ru|| 
l|u|| 
= 
lim 
ll/(a + u ) - / ( a ) - W u l | = 0 
u€!/, u-^0 
||u|| 
shows that ip'(b) exists and ¡p'(b) = f'{b)\u 
G L(U, Y). □ 
Functions of a Real Variable 
Remarks 5.2.17 Special notation for functions of a real variable. Let the domain 
space X be a one-dimensional space. We can assume that X — R without loss of 
generality. In this case, all three types of derivatives are essentially the same. Let / 
be an open interval in R. The standard definition of the derivative of / : / —> Y at 
a E I, Definition 5.1.1, is 
/'(a)=lim t^o(l/í)(/(a + í ) - / ( o ) ) -
We see that this corresponds to the directional derivative of / in the direction of 1 E l . 
Hence it could be denoted as /'(a; 1) or as f'(a)l. 
Obviously, we never denote 
derivatives in this way for functions of one variable. As noted in the introduction to 
this section, we employ the notation f'(a) differently for the one-variable case. 

198 
DERIVATIVES 
Computations of Directional Derivatives 
The following lemma shows that any directional derivative can be computed as the 
ordinary derivative of a function of a real variable. Hence we can use the standard 
rules of differentiation to compute directional derivatives. 
Lemma 5.2.18 Defineip(s) = /(a + se), a function ofone variable defined in terms 
off. Then 
whenever ip'(s) exists. 
Proof. We have 
ip'(s) 
whenever <p'(s) exists. 
Ü 
Lemma 5.2.19 Homogeneity of the directional derivatives. Iff'(a; e) exists, then 
/'(a; té) also exists for allt e l and /'(a; te) = í/'(a; e). 
Proof. The result is trivial if t = 0. Otherwise, we have 
l i m - ( / ( a + r í e ) - / ( a ) ) 
= 
t lirrii(/(a + rte) - /(a)) 
r^O r 
r—>0 rt 
= ¿limi(/(a + i e ) - / ( a ) ) = t / ' ( a ; e ) . □ 
s—>0 s 
Remarks 5.2.20 If we know that /'(a) e L(X, Y) exists, then Lemma 5.2.8 shows 
that / ' ( a ; e) = /'( a) e- In this case, lemma 5.2.19 becomes a triviality. If the 
existence of /'(a) is not known, then a small argument may be necessary to show the 
homogeneity of the directional derivatives. 
Increments and Derivatives 
The basic use of derivatives is to estimate the increments of a function. We formulate 
several relations between derivatives and increments. 
¥>'(*) = / ' ( a + ae;e) 
lim ^S 
+ ^~ 
^ 
i—»o 
r 
lim /(a + (s + r)e) - / ( a + se) 
/'(a + se; e) 

DIFFERENTIABLE FUNCTIONS 
199 
Theorem 5.2.21 . Let T = /'(a) G L(X, Y) be the derivative of f : A -> Y at 
a G A. Let M = \\T\\, using the usual norm on linear transformations. Then for 
each e > 0 there is a 8 > 0 such that 
||/(a + r ) - / ( a ) | | < ( M + e)||r|| 
(5.29) 
whenever ||r|| < 8. 
Proof. Given e > 0, find a 8 > 0 such that 
||/(a + r ) - / ( a ) - T r | | < £ | | r | | 
whenever ||r|| < 8. Hence 
||/(a + r ) - / ( a ) | | 
< 
||Tr|| + ||/(a + r) - / ( a ) - T r | | 
< 
M ||r||+e||r|| = (M + e)||r|| 
whenever ||r|| < 8. 
□ 
Corollary 5.2.22 Derivatives and continuity. If f has a derivative at x, then f is 
continuous at x. 
Proof. Let r„ be a zero-sequence in X. Theorem 5.2.21 shows that 
/(a + r „ ) - + / ( a ) i n Y . 
This is the continuity of / at a. 
□ 
Example 5.2.23 Let a be a constant. Define / : R2 -> R2 by 
t(T „\-l 
xy/(x2+y2) 
if(x,y)¿(0,0) 
n X ' y ) - \ a 
if (x,y) = (0,0) 
Let us show that lim^^^^o.o) f(x- v) d° e s n o t exist. Thus, no matter what the value 
of a is, / is not continuous at (0,0). It follows that for all values of a, the function / 
cannot be differentiable at (0,0). 
To show that lim(a.i3/)_(oio) f(x, y) does not exist, we let (x, y) approach 0 through 
different paths. Along the line y = x, we have 
x2 
1 
lim 
fix.y) 
= lim —- = -. 
Along the line y = 0, we have 
x(0) 
0 
lim 
fix,y) 
= lim —? ——- = lim —^ = 0. 

200 
DERIVATIVES 
Problems 
5.25 
Consider the xy-p\ane as a horizontal plane with the z-axis pointing directly 
upward. Assume that z = f(x, y) — 10 — x2 — 2y2 describes the surface of a hill. 
1. Find the directional derivatives of z = f(x, y) at the point (x = 2, y = 1) 
and along an arbitrary vector (u, v). 
2. Take (u, t>) as a unit vector, u 2+v 2 = 1. Explain why the directional derivative 
/'((2, 1); (u, D)) can be considered as the slope a hiker would experience if 
she starts at the point (2, 1, 4) on the hill and moves in the (u, v) direction 
determined by the horizontal coordinates. 
3. Show that there is a plane passing through (2, 1, 4) such that all the slopes on 
this plane are the same as the corresponding slopes on on the hill. 
4. Find the equation of this plane. 
5. What are the directions (u, v) of the steepest ascent, the steepest descent, and 
of zero slope? 
5.26 
The temperature distribution in a certain region of the xyz-space at the time 
t is given as fix, y, z, t) = (100 — x2 — y2 — 2z2)e~t/2. 
A fly is moving in this 
region according to the law of motion x(t) = 2 — t, y(t) = 1 + t2, and z(t) = t3. 
Find the rate of change in the temperature this fly experiences at the time t = 2. 
Show that this rate of change is the directional derivative of / at a certain point in K4 
and along a certain vector in R4. Find this point and this vector. 
5.27 
Find the directional derivative of 
X V 
(x,y)¿(0,0) 
f(x,y)={ 
x4 + y4 
0, 
(x,y) = (0,0) 
at an arbitrary point and in an arbitrary direction, if it exists. 
5.28 
Same as Problem 5.27 for 
f(x,y)= 
{ i * T 7 ' 
(X>V)*(°>°)> 
0, 
(x,y) = (0,0). 
5.29 
Same as Problem 5.27 for 
0, 
(x,y) = (0,0). 

EXISTENCE OF DERIVATIVES 
201 
5.30 
Same as Problem 5.27 for 
xy 
. 
1 
Kx,y)={ 
(X2 + y2)l/2—x2 
+ y2> 
(*> Í/) ^ (0, 0), 
0, 
(a;,») = (0,0). 
5.31 
Let / : R2 —> R2 be the polar coordinates function 
f(r, 9) = (x, y) = (rcosO, rsinö). 
Find the directional derivative of / at the point (r = 1, 9 = a) along an arbitrary 
vector (a, A) in the rö-plane. The result will be a vector in the ¿ry-plane. In particular, 
find the directional derivatives along the vectors (1,0) and (0, 1). Show that these 
two directional derivatives are orthogonal to each other. 
5.32 
Let / : R3 —► R3 be the cylindrical coordinates function 
f(r, 0, C) = (x, y, z) = {rcos9, rsin9, 
(). 
Find the directional derivative of / at the point (r = 1,0 = a, ( = 0) along 
an arbitrary vector (a, A, h) in the r#£-space. The result will be a vector in the 
xyz-space. In particular, find the directional derivatives along the vectors (1, 0, 0), 
(0, 1, 0), and (0, 0, 1). Show that these three directional derivatives are orthogonal 
to each other. 
5.33 
Let f(p, <p, 9) = (x, y, z) be the spherical coordinates function 
x — p sin kp cos 9, y = p sin ip sin 9, z = p cos (p. 
Find the directional derivative of / at the point (p = 1,0 = a, ip = ß) along 
an arbitrary vector (a, A, p) in the r9ip-space. In particular, find the directional 
derivatives along the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1). Show that these three 
directional derivatives are orthogonal to each other. 
5.3 
EXISTENCE OF DERIVATIVES 
Lemma 5.2.8 gives a necessary condition for differentiability. This condition is the 
existence of the directional derivatives in all directions. The following example 
shows that this condition is not sufficient for differentiability. 
Example 5.3.1 A nondifferentiable function with directional derivatives. Let 
,, 
\ _ / 1 
if x ¥" 0 and y = x2 
J{x, y) - < 0 
o t n e r w j s e . 

202 
DERIVATIVES 
This is the same as an earlier example, Example 4.4.6. We know that this function 
/ : R2 —» K is discontinuous at the origin. Hence Corollary 5.2.22 shows that it 
cannot be differentiable at the origin. But its directional derivatives at the origin 
exist in all directions, and they are all zero. To see this, restrict / to a line passing 
through the origin. If this line is one of the coordinate axes, then this restriction is 
identically zero. Hence it is differentiable everywhere. Now consider the restriction 
of / to the line y = mx, m =/= 0. This restriction is zero everywhere except at the 
point (m, m2), where it has the value of 1. Hence the restriction is zero in an open 
interval containing the origin. Therefore /'((0, 0); e) = 0 for all e e R2. A 
Lemma 5.2.9 shows that the existence and the continuity of directional derivatives in 
all directions are necessary for continuous differentiability. We will now show that 
this condition is also sufficient. In what follows, X and Y are two normed spaces, 
and A is an open set in X. 
Lemma 5.3.2 Let e £ X be a fixed vector. Assume that the directional derivative 
fa(x) = j i m / ( x + 
te
t
)-/(x) 
(5.30) 
exists for all TÍ € A and defines a continuous function h : A —> Y. Then for each 
a £ j 4 and for each e > 0, there is a S > 0 such that 
||/(x + t e ) - / ( x ) - t f t ( a ) | | < e | i | 
(5.31) 
whenever ||x — a|| < 6 and \t\ < S. In particular, ifvn is a zero-sequence in X and 
tn is a zero-sequence in R, then there is a zero-sequence e n £ M such that 
||/(a + v„ + t„e) - /(a + v„) - in/i(a)|| < en \tn\. 
(5.32) 
Proof. Given e > 0, there is a 5 > 0 such that 
||/i(x + ie)-/i(a)|| <e 
whenever ||x — a|| < 6 and |i| < 6. This is possible because of the continuity of 
h : A —> Y. Let x be fixed and ||x — a|| < S. Define 
VÍ5) 
= 
/ ( x + s e) — /( x) ~ sh(a). Then 
ip'{a) = 
lim(l/í)(v?(s + í ) - ^ ( s ) ) 
= 
lim(l/í)(/(x + se + te) - /(x + se) - th(&)) 
= 
h(x + se) — h(a). 
Hence we see that ||ip'(s)|| < e for all \s\ < \t\ < S. Then 
Mi) 
- <p(0)\\ = ¡Mí)|| = ||/(x + íe) - /(x) - th(a)\\ 
<e\t\. 

EXISTENCE OF DERIVATIVES 
203 
The last step follows from the mean value theorem, Theorem 5.1.13. Finally, (5.32) 
follows directly from (5.31). 
□ 
Lemma 5.3.3 Assume that X is spanned by a subspace V together with a vector 
e 6 X \ V. Assume that the directional derivative 
h(x) = lim / ( x + 
te)'/(x) 
(5.33) 
exists for all*. € A and defines a continuous function h : A —> Y. Assume that the 
restricted derivative fv (a) = T £ L(V, Y) exists a i a g A. Then f is dijferentiable 
at a. Its derivative is given as 
/'(a)x = /'(a)(v + re) = T v + r/i(a) 
(5.34) 
for all x = ( v + r e ) € l with v e V and r e l 
Proof. Each x e X has a unique representation as x = v + te with v <G V and 
i g l . Note that v = Px and ie = Qx with the coordinate projections P and Q 
associated with this decomposition of X. Hence, ||v|| < K\\x\\ and ||ie|| < Ä"||x|| 
for some K > 0. Define S e L(X, Y) by 
Sx = 5(v + ie) = Tv + i/i(a), x = (v + te) e X. 
(5.35) 
We see that the increment /(a + x) — /(a) is the sum of two increments 
/(a + v ) - / ( a ) and 
(5.36) 
/ ( a + v + 
te)-/(a 
+ v), 
(5.37) 
where x = v + tie). Let xra = vra + tne be a zero-sequence in X. Then we see 
that, v„ is a zero-sequence in V and tn is a zero-sequence in R. Recall that T is the 
restricted derivative of / at a <E A along V. Hence, by the observations in Remarks 
5.2.2, there is a zero-sequence a „ e R such that 
||/(a + v„) - /(a) - Tvn¡| < a„||v„||. 
(5.38) 
Also, Lemma 5.3.2 shows that there is a zero-sequence ßn € K such that 
||/(a + v n + í „ e ) - / ( a + v n)-í nft(a)||</3 n||í ne||. 
(5.39) 
Then, (5.38) and (5.39) imply that, with xn = v„ + tne, 
||/(a + xn) - /(a) - 5x„|| < a„||vnjj + ßn \tn\ < en\\xn\\, 
(5.40) 
where en = (an + ßn)K. This proves (5.34). D 

204 
DERIVATIVES 
Theorem 5.3.4 Existence of derivatives. Let f : A—±Y, where Ais an open subset 
of X. Let E — { ei, ..., e„} be a basis for X. If all the directional derivatives 
along the vectors e¿ exist, and if they are continuous functions on A, then f is 
continuously differentiable on A. 
Proof. Apply an induction argument on n — dim X. The result is correct if n = 1. 
In fact, in this case directional derivatives and derivatives are the same, as already 
observed in Remarks 5.2.17. Assume that the result is true for (n — 1)-dimensional 
spaces. This implies that / has restricted derivatives along the subspace spanned 
by (ei, ..., e„_i). Then Lemma 5.3.3 shows that / is differentiable. Continuous 
differentiability follows from the continuity of the directional derivatives. 
□ 
Problems 
5.34 
Define / : E2 -► R by /(0, 0) = 0 and 
f(x, y) = (x2 + y2) cos - 5 - — ? if a;2 + y2 > 0. 
xz + y¿ 
Show that / ' : E2 -> L(E2, E) exists but is not continuous at (0, 0) e R2. 
5.35 
Let / : E 3 —> R be defined as f(x, y, z) = 1 if x — t, y = t2, and 
z = t3 for some t > 0, and f(x, y, z) — 0 otherwise. Show that / has a restricted 
derivative at the origin along any one- or two-dimensional subspace. Show that / is 
not differentiable at the origin. 
5.36 
Let n G N, n > 2. Give an example of a function / : 1 " —> E that has 
restricted derivatives at the origin in all (n — 1)-dimensional subspaces of E™ but is 
not differentiable at the origin. 
5.37 
Let f(x, y) = lify<x2<2y 
and /(x, y) = 0 otherwise. Show that / has 
directional derivatives at the origin in any direction. Is / differentiable at the origin? 
5.38 
Let f(x, y) = 1 if 2x < x2 + y2 < Ax and f(x, y) = 0 otherwise. Show 
that / has directional derivatives at the origin in any direction. Is / differentiable at 
the origin? 
5.39 
Suppose that / : E2 —> E has a directional derivative at the origin along 
the vector (1, 0). Assume that for all x G E the directional derivative of / at the 
point (x, 0) and in the direction of (1, 1) exists and is equal to p{x). Show that if 
p : M —► E is continuous, then / is differentiable at (0, 0). 

PARTIAL DERIVATIVES 
205 
5.4 
PARTIAL DERIVATIVES 
Definition 5.4.1 Partial derivatives. Let / : A —> Y be a function and let x e A. 
LetE = {ei, ..., e n} be a basis for the domain space X. The directional derivatives 
/'(x; e¿) in the directions of the basis vectors e¿ are called the partial derivatives of 
f : A —> y afx € A wifÄ respect to the basis E. 
Definition 5.4.2 Functions of several variables. Functions defined on subsets of 
W1 are called/wnciions of several variables. In discussing partial derivatives, we shall 
ignore the difference between functions defined on subsets of a finite-dimensional 
normed space X and functions defined on subsets of 1". The following conventions 
justify this practice. 
Fix a basis E = { ei, ..., e„} for X. Let Xj : X —> R be the coordinate functions 
with respect to E. There is an isomorphism $ : M.n —> X that takes 
(H, . . . , l „ ) £ l " 
to 
*(xi, ..., xn) = xiei H 
h xraen G X. 
Set 
2 = * _ 1 ( ^ ) = { (xi, ..., xn) e Rn | xiei + • • • + xnen e A } . 
Then we may define a function f = (f ■ ty) : A —> Y by 
f(xi, 
..., xn) = f(xiei 
H 
\-xnen). 
We will ignore the difference between / and /, and between A and A. Hence we 
write x = (x\, ..., xn) e A and 
/(x) = /(xi, ..., xn). 
Definition 5.4.3 Standard notation for partial derivatives. Fix a basis E = 
{ ei, ..., e„} for X. The standard notation for /'(x; e,,) is 
|^-(x) = | ^ ( x 1 , . . . , x „ ) . 
(5.41) 
This notation indicates the way partial derivatives are computed. To find /'(x; e¿), 
proceed as follows. Consider /(x) as a function g(x) of a single variable x = a^, 
keeping the other coordinates Xk, k ^ j , fixed. Then 
| ^ ( x ) 
= 
l i m / ( x + ^)-/(x) 
OXj 
r^O 
r 
= lim ilZi+lk^M 
= gi{Xj) 
r->0 
r 
J 

2 0 6 
DERIVATIVES 
is the differentiation of / with respect to the variable x = Xj, regarding all other 
coordinates Xk, k 7^ j , as constants. Therefore a partial derivative of a function of 
several real variables can be obtained by the basic differentiation rules applied to a 
single real variable. 
Let ei, e2 be the standard basis vectors of R2. Since points in R2 are usually denoted 
as (x, y), alternative notation for /'(x; ei) and /'(x; e2) is, respectively, 
df, 
x 
df 
df 
df 
- W 
= -(x,y) 
and _ ( x ) = - ( * , » ) . 
Similar alternative notation is used for K3. 
Definition 5.4.4 The Jacobian matrix. Partial derivatives provide a convenient way 
to represent the full derivatives of a vector-valued function / : A —> Y in terms of 
real-valued functions. Choose a basis W = { wi, ..., w m} for Y and consider the 
component functions /¿ : A —> R defined by f — /iWi + • • ■ + / m w m . Then 
^ ( x ) = öx-(x)wi + -" + ^ 7 ( x ) w ' -
If the domain space X is an n-dimensional space, then each component has n partial 
derivatives. Hence there are ran real-valued partial derivatives 
dfi 
dx. (x), i = 1, ..., m, j = 1, ..., n. 
We organize these mn real numbers as an m x n matrix, referred to as the Jacobian 
matrix. It is denoted as 
"M = fc£i<*> 
a/ 
¿tei ^ x ) 
(x) 
9a; 1 
/i(x; ei) 
/m(x; ei) 
dx. H*) 
:(x) 
9a;„ 
/i(x; e„) 
The Jacobian matrix plainly depends upon the choice of the bases E c X and 
W 
CY. 
If all the partial derivatives / ' ( x ; ej), j = 1, ..., n, exist for all x e ^4, then the 
Jacobian matrix defines a function 
J / : 
d ( / l , ■■■, / m ) 

PARTIAL DERIVATIVES 
2 0 7 
It takes each point x e A to a real-valued matrix with the entries //(x; ej). 
Remarks 5.4.5 Jacobian matrix and the derivative. Example 5.3.1 shows that the 
Jacobian matrix for / at a <G A can exist even if / is not differentiable at a. But if / 
is differentiable at a e A, then its derivative /'(a) : X —> Y is represented by the 
Jacobian matrix J(a) with respect to the bases E and W. This means that if 
Y = (yi, • • •, Vm) = /'(a)Ori, ...,xn) 
= /'(a)(x), 
then y can be computed by matrix multiplication: 
(5.42) 
2/1 
Vn 
/i(a;e!) 
... 
/i(a; en) 
/m( a; e 0 
••• 
/™(a; e«) 
Xl 
J / ( a ) x . (5.43) 
and 
Here, as in Definition 5.4.3, we ignore the difference between elements of 
M„i. That is, we identify 
V*^l i • • • i Xfi) 
t ■ 
and 
Xl 
and denote both objects by x. To verify (5.43), it is enough to observe that it gives 
the correct result for each x = ej. In fact, if x = ej, then (5.43) becomes 
J/(a) ej 
which represents /'(a; ej) = 
f'{a)ej. 
/i(a; ej) 
/m(a; ej) 
Example 5.4.6 Define / : (R2 \ { 0}) -> (0, oo) x [0, 2TT) by f(x, y) = (r, 0) where 
r = (x2 + y2)1^2 andö e [0, 2TT) with sinö = y/r, cos 9 = x/r. Let us compute the 
Jacobian matrix of / at (x, y), where x ^ 0, y ^ 0. Here, the components of / are 
h(x,y) 
= (x2+y2)1'2 
and 
f2(x,y) 
= &rcsm(y(x2 + 
y2)~1/2). 
We see that, after some computations, 
dx{x>y) 
dh, 
, 
dh, 
, 
-dx-{x'y) 
dh, 
s 
x{x2 +y2y1/2 
= cos(9, 
y(x2 + y2yl/2 
= siriö, 
-y{x2 + y2)^1 - -(1/r) sinö, and 
x(x2 + y2)~l = (1/r) cosö. 

208 
DERIVATIVES 
Thus, the Jacobian matrix of / at (x, y) is 
J/(z, y) 
Here the variables r and 9 are used to simplify the expressions. 
A 
cos 9 
sin v 
-(l/r)sin0 
(l/r)cos0 
Example 5.4.7 Define g 
computation shows that 
by g(r, 
(reos9, rsin9). 
An easy 
3g(r, 9) = 
cost 
sine 
—r sin 9 
reos 9 
We see that Jg(r, 9) is the inverse of 3f(x, y) in Example 5.4.6. This is not 
accidental. In fact, the restriction of g to (0, CXD) X [0,2iv) is the inverse of /. The 
chain rule, Theorem 5.5.6 below, will show that g' is the inverse of /'. 
A 
Example 5.4.8 Let A = M2 \ {0} and define / : A -► E2 by 
f(x, y) = ((x2+ y2)/(2x), 
(x2 + y2)/(2y)) 
for all (x, y) e A. 
Let us find all/i e Rsothat/'(l,2)(/i, l-h) 
= (-5,-1/2). Here, the components 
of / are h{x, y) = (x2 + y2)/2x and f2{x, y) = (x2 + y2)/2y. Thus, 
<9/i, 
, 
x1 
-y2 
dfi, 
, 
y 
ay 
x 
dh, 
x 
x 
-K-ix.y) 
= 
-, 
ox 
y 
dh (x,y) 
y2 - x2 
dy v "" 
2x2 
Hence, /'(1,2)(x,y) = (l/2)(-3x + Ay, x + 3y) for all (x, y) & A. Thus, 
/'(l, 2)(h, l-h) 
= (l/2)(-7/i + 4, -2/i + 3). 
Hence, we must have h = 2. 
A 
The Gradient Vector 
The derivative of a real-valued function at a point is a real-valued linear function. 
Hence it can be represented as an inner product with a unique vector. This vector is 
called the gradient of the function. We formulate this as Lemma 5.4.9. 
Lemma 5.4.9 Let A be an open set in a Euclidean space X. Let f : A —> R be a 
real-valued differentiable function. Then, for each a € A, there is a unique vector 
V/(a) € X suchthat f'(a)x 
= (V/(a), x ) / o r a / / x e X . 

PARTIAL DERIVATIVES 
209 
Proof. The derivative at a € A is a linear function /'(a) : X —> R. In this case, 
Theorem 3.4.21 shows that /'(a) can be represented as an inner product with a unique 
vector as stated in the lemma. 
D 
Definition 5.4.10 The gradient vector. Let A be an open set in a Euclidean space 
X. Let / : A —> R be a real-valued differentiable function. The vector V/(a) 6 X 
obtained in Lemma 5.4.9 is called the gradient vector of / at a G A. The function 
V / : A —^ X is the gradient function of /. 
Remarks 5.4.11 Inner products of the gradient vector. The inner product of the 
gradient vector V/(a) £ X with a vector x is the directional derivative of / along 
the vector x. This follows from Definition 5.4.10: 
(V/(a), x) = /'(a)x = /'(a; x) = lim \ (/(a + tx) - /(a)). 
Remarks 5.4.12 Cartesian coordinates of the gradient vector. Let X = Rn with 
its standard inner product and with its standard basis (ei, ..., en). Then 
V/(a) 
= 
(V/(a), e 1)e 1 + --- + (V/(a),e„)e n 
= 
^— (a)ei + 
h^—(a)e„. 
OX\ 
OXn 
In fact, (V/(a), e,} — /'(a)e¿ is the partial derivative of / with respect to the zth 
coordinate x¿, by Definition 5.4.1. 
Remarks 5.4.13 The gradient vector and the Jacobian matrix. Let A C M™ be 
an open set and let / : A —> R be a differentiable function. The Jacobian matrix 
J / 
and the gradient function 
ÉL ... ÉL 
dx\' 
' dxn 
:A 
Hn 
\dxi 
' dxn 
are two different notations for the linear transformation determined by the n partial 
derivatives of this function. 
Definition 5.4.14 Local extremal values. Let / : A —> R be a real-valued function 
defined on an open set A in a vector space X. Then a G A is called a /oca/ maximum 
for / if there is a 5 > 0 such that /(x) < /(a) whenever x € A and ||x — a|| < ö. 
Local minimum values are defined similarly. It is easy to see that if V/(a) ^ 0, then 

210 
DERIVATIVES 
/ cannot have a local extremal value at a. Thus, local maxima or minima for A will 
always be found among the set of points a for which V/(a) = 0. 
Remarks 5.4.15 Higher-order partial derivatives. Higher-order derivatives are 
discussed in Chapter 7 in a systematic way. Here, however, we may define higher-
order partial derivatives as we did in the case of functions of a real variable. In fact, 
in dealing with partial derivatives, we effectively consider functions with only one 
real variable, since the other variables are kept fixed. For example, let / : M.3 —+ Y 
be a function of three variables x. y, z taking values in a normed space Y. Then 
notations like 
d df _ d2f 
d_d¿_&¿ 
d d2f _ <93/ 
dy dx 
dydx' 
dx dx 
dx2 ' dz dy2 
dzdy2 ' 
have obvious meanings. For example, the first means to take the derivative of / with 
respect to x, and then to differentiate the resulting function with respect to y. We 
will prove that for higher-order derivatives, the order of differentiation is generally 
immaterial. For the time being, however, we must perform the differentiations in the 
given order. 
Functions that are n times continuously differentiable are called Gn functions. They 
are defined precisely in Definition 7.1.5, and discussed in the sequel. 
Problems 
5.40 
Let(u, v) = (3x + 2y, 6x — 4y). Find the gradients Vit, Vv, and the Jacobian 
matrix d(u, v)/d(x, y) at a general point (x, y), if they exist. 
5.41 
Same as Problem 5.40 for (u, v) = (xy, 
y/x). 
5.42 
Same as Problem 5.40 for (u, v) = ((x2 + y2)/(2x), (x2 + 
y2)/{2y)). 
5.43 
Same as Problem 5.40 for 
(tí, v) = (p(x, y) + q(x, y), p(x, y) - q(x, y)), 
where p(x, y) = {{x + l) 2 + y2)1/2 
andq(x, y) = {(x - l) 2 
+y2)1/2. 
5.44 
Let r : M2 —> R2 be the polar coordinates function 
(x, y) = r(r, 6) = (rcosÖ, 
rsinö). 
Find ei = dr/dr and e2 = dr/d6 at all points (r, 6) € M.2. Show that if r ^ 0, 
then ei (r, 6) and e2 (r, 0) is an orthogonal basis for the xy-plane. Find the Jacobian 
matrix d(x, y)/d(r, 0). 

RULES OF DIFFERENTIATION 
211 
5.45 
A particle moves in the xy-plane according to the law of motion 
s(f) — (x(t), y(t)), where x(t) = é cos2í andy(í) = e4sin2í. 
Find the coordinates of the velocity s'(f) and acceleration s"(t) of this particle with 
respect to the basis 61(6*, 2f) and e2(e*, 2£) defined in Problem (5.44). 
5.46 
Let r : R3 —> R3 be the cylindrical coordinates function 
{x, y, z) = r(r, 9, () = (reos9, rsinö, Q. 
Find ei = dr/dr, e2 = dr/d9, and e3 = dr/dQ at all points (r, 6, () e R3. Show 
that if r ^ 0 then ei(r, 9, Q, e2(r, 0, £), and e3(r, 0, C) is an orthogonal basis for 
the xyz-space. Find the Jacobian matrix d(x, y, z)/d(r, 9, Q. 
5.47 
A particle moves in the xyz-space according to the law of motion 
s(i) = (x{t), y(t), z(t)) where 
x{t) = e'cos2£, y{t) = é sm2t, 
z(£) = 3i. 
Express the velocity s'(i) and acceleration s"(i) of this particle in terms of the basis 
ei(e', 2í, 3í), e2(e*, 2í, 3í), and e3(e*, 2í, 3í) defined in Problem 5.46. 
5.48 
Let r : R3 —> R3 be the spherical coordinates function 
(x, y, z) = r(p, <p, 9) = (p sin <p cos 9, p sin ¡p sin 9, pcosip). 
Find ej = dr/dp, e2 = dr/dip, and e3 = dv/d9 at all points (p, ip, 9) G R3. Show 
that if p ^ 0, then ei(p, </?, 0), e2(p, <£, 0), and es(p, ¡p, 9) is an orthogonal basis 
for the xy^-space. Find the Jacobian matrix d(x, y, z)/d(p, p, 9). 
5.49 
A particle moves in the xy;2-space according to the law of motion 
s(f) = (a;(i), y(t), z(t)), 
where 
x{t) = e* sin3tcos2£, y(t) = é sin3í sin2t, z(t) = 3í cos3í. 
Express the velocity s'(£) and acceleration s"(£) of this particle in terms of the basis 
ei(p, <p, 9), e2(p, ip, 9), and e${p, ip, 9) defined in Problem 5.48. 
5.5 
RULES OF DIFFERENTIATION 
In this section, we obtain some general rules for differentiation. As before, X and Y 
are two normed spaces, A C X is an open set in X, and / : A —-> Y is a function. 

212 
DERIVATIVES 
Lemma 5.5.1 Derivative of a sum. Let f, g : A —> Y be two differentiable functions 
on A. Then (/ + g) : A —> Y is also differentiable on A and 
(/ + <?)' = / ' + </'• 
(5.44) 
Proof. This is left as an exercise (a straightforward application of the definition of 
the derivative). 
□ 
Lemma 5.5.2 Derivative of a constant function. Let f : A —» Y be a constant 
function. Then f : A —* L{X, Y) is the zero-function. That is, 
/'(a)(x) = 0 <E Y for aline 
A and* G X. 
Proof. This is also left as an exercise. 
D 
Lemma 5.5.3 Derivative of a linear function. Let R : X —> Y be a linear 
function. Then R' : X —> L(X, Y) is the constant function with the constant value 
R G L{X, Y). That is, 
fí'(a) = Rfor all a£ X and fí'(a)x = Rx.for all a, x G X. 
Proof. If R : X -> Y is linear, then Afí(x)(r) = Rr and ||Afí(x)(r) - Rr\\ = 0 
for all x, r G X. This shows that £>fí(x) = R. 
G 
Remarks 5.5.4 Constants and constant functions. Sometimes it may be awkward 
to distinguish between a constant and a function that takes the same constant value 
at all points in the domain of its definition. If this constant value is a point R 
in L(X, Y), the derivative function is helpful to make this distinction. Hence, if 
R G L(X, Y), then DR : X —> L(X, Y) is the function that has this constant value 
fíat each x e l , 
Example 5.5.5 Derivative of the identity function. Let / : X —> X be the identity 
function. Then / G L(X, X). Hence DI : X —> L(X, X) is the constant function 
DI(x) = I G L(X, X) for all x G X. 
A 
The Chain Rule 
The chain rule states that the composition of two differentiable functions is differen-
tiable. The derivative of the composition is the composition of the derivatives. This is 
a key result for differentiation in the multi-variable case, just as it is for one-variable 
calculus. In the following, X, Y, and Z are normed spaces, A is an open set in X 

RULES OF DIFFERENTIATION 
213 
and B is an open set in Y. We consider two functions / : A —» Y, g : B —> Z, and 
assume that f(A) C B. 
Theorem 5.5.6 Chain rule. If f : A —> Y is differentiable at a £ A and if 
g : B —> Z is differentiable at b = fia), then 
h = g ■ f : A -» Z is differentiable at a and /¡/(a) = g'(b) • /'(a) = g'(f(a)) • /'(a). 
/f/ and g are continuously differentiable functions, then h : g-f is also a continuously 
differentiable function. 
Proof. Let R = /'(a) and S = g'(b). Given a zero-sequence x n in X, let 
/(a + x „ ) - / ( a ) 
= 
y„, 
(5.45) 
g{b + yn)-g(b) 
= 
z„. 
(5.46) 
Then we have 
h(a + xn) - h(a) 
= 
z„. 
(5.47) 
The last equality follows from 
g(f(a + x„)) - g(f(a)) = g(b + y„) - <,(b). 
(5.48) 
Recall that R = /'(a). Hence there is a zero-sequence rn in R such that 
||y„ - ÄXnll = ||/(a + x„) - /(a) - fix„|| < r„ ||x„||. 
(5.49) 
This was observed in Remarks 5.2.2. Therefore 
||yn|| < ||-Rxn|| + ||yn - Äxn|| < (||i?|| + 
II- 
(5.50) 
Hence y n is a zero-sequence in Y. Recall that S = g'(b). Therefore, as in (5.49), 
there is a zero-sequence sn in R such that 
| | z „ - 5 y „ | | 
= 
||ff(b + y „ ) - 5 ( b ) - 5 ' y „ | | 
< 
5 n | | y n | | < 5 „ ( | | i ? | | + r n ) | | x „ | | . 
^ ^ ^ 
Here the last inequality follows from (5.50). Now (5.51), (5.49), and 
zn - (SR)xn 
= (z„ - Syn) + S(yn - Rxn) 
(5.52) 
show that 
||zn - (SR)xn\\ 
< (sn(\\R\\ + rn) + \\S\\ rn) ||x„|| = tn ||x„||. 
(5.53) 

214 
DERIVATIVES 
Here tn = sn(||i?|| + rn) + \\S\\ rn is a zero-sequence in R. Hence, by (5.47), 
||ft(a + x n) - ft(a) - (SÄ)x„|| < tn ||x„||. 
(5.54) 
This shows that ft'(a) = 5fi, as observed in Remarks 5.2.2. 
For the continuity of ft' : A —► L(X, Z), let a„ e A and ara —> a € A Then 
g'(f(an)) 
= (</ ■ /)(a„) —> </(/(a)) in L(y, Z), since the composition of two 
continuous functions is continuous. Also, /'(a n) —> /'(a) in L(X, Y). Then we 
see that ft'(an) = (g' • /)(a n) • /'(a n) ^ ft'(a) = (g1 • /)(a) • /'(a) in L(X, Z) by 
the continuity of products. Hence ft' : A —> L(X, Z) is continuous. 
□ 
Example 5.5.7 Chain rule in terms of Jacobian matrices. With the notation 
introduced above for the chain rule, Theorem 5.5.6, let U, V, and W be bases for 
X, Y, and Z, respectively. Let x¿ : X -> R, ?/,• : Y -> R, and ^ : Z -» R be the 
corresponding coordinate functions. Assume that p, q, and r are the dimensions of 
X, Y, and Z, respectively. If the chain rule 
ft'(a) = g'(b) ■ /'(a) 
(5.55) 
is expressed in terms of the Jacobian matrices, then it becomes 
J/i(a) = Js(b)-J/(a). 
(5.56) 
Note that ft takes its values in Z. Hence it has r = dimZ components ftfe. Each 
component ft^ depends on p = dimX variables x¿. With similar notation for the 
other functions, we see that (5.56) becomes, in terms of the entries, 
Öftfc = A 
ÖgrfcÖ/j_ 
ÖX 
4-f dyj dxi 
A more instructive form of (5.57) is as follows. Denote /(x) as y(x) and g(y) as 
z(y). In fact, / expresses the y¿ coordinates in terms of the x¿ coordinates. Hence, 
/ can be considered as q = dimF functions y¿, each depending on p — dimX 
variables 
Vi =Vj(xi, 
■■■, xP), 
(5.58) 
with j = l, ..., q. Similarly, g consists of r = dim Z functions Zk, each depending 
on q = dim y variables ?/¿ as 
£fc = zk{yi, .-., yg), 
(5.59) 
with k = l, ..., r. The composite function h — g-f has again the same components 
Zk, but each argument j/j is expressed in terms of a;,s by (5.58). Hence one can state 
(5.57) as 
dzk_ = y ^ (hk_ dyj_ = Y^ 
<^k_ 
fyj± 
(56Q) 
dxi 
^ 
dyj dxi 
¿-^i dyj dxi' 

RULES OF DIFFERENTIATION 
215 
When one takes the partial derivative of Zk with respect to x¿, it is understood that 
Zk is considered as a function of x¿-variables. If Zk is differentiated with respect to 
¡jj, then it is considered as a function of j/j-variables. Equations (5.60) are easier to 
remember than Equations (5.57). A 
Example 5.5.8 Let / : R —> K be a differentiable function. Assume that f'(x) — 0 
if and only if x = 0. Define H : R2 -> R2 by 
h(x, y) = (f(x2 - y2), f(x2 + y2)) 
for all (x, y) S R2. 
Let us see that h is differentiable. 
Here, the components of h are h\(x,y) 
= 
f(x2 — y2) and h,2(x, y) = f(x2 + y2). Thus, since / is differentiable, 
^(x,y) 
= 2xf'(x2-y2), 
^(x,y) 
= -2yf(x2 
- y2) 
^(x,y) 
= 2xf'(x2+y2), 
^(x,y) 
= 2yf'(x2 + y2). 
Since / ' is continuous, it follows that the above partial derivatives are all continuous 
on R2. So, / is differentiable on R2. Also, the matrix for f'(x, y) is 
Jf(x,y) 
2xf[x2-y2) 
-2yf(x2-y2) 
2xf'(x2+y2) 
2yf'(x2+y2) 
A 
Total Differentials 
Notations 5.5.9 The d-symbol. If / : A —* R is a real-valued differentiable func-
tion, then one also writes df for /'. 
In basic calculus courses, the notation df 
usually suggests some kind of smallness. There is no such implication here. Hence 
df : A -> L(X, R) is a function with values df{&) € L{X, R), a £ A Also, d/(a)x 
is a real number for all a G A and x G X. 
We should be careful to use this notation only for real-valued functions. In advanced 
calculus courses, the (¿-symbol is usually reserved for exterior derivatives. Exterior 
derivatives will be considered in chapter 10. For real-valued functions, they are the 
same as the derivatives discussed here. 
Example 5.5.10 Derivatives of the coordinate functions. Let Xj : X —> R be the 
coordinate functions with respect to a basis in X. They are real-valued functions. 
The notation dxj is the standard notation for their derivatives. Each Xj■■ : X —» R 
is a linear transformation. Therefore, as in Lemma 5.5.3, dxj : X —> L(X, R) is a 
constant function. Indeed, dxj(a) = Xj for all a G X. Hence <ir¿(a)(x) = Xj(x) 
for all a, x G X. 
A 

216 
DERIVATIVES 
Definition 5.5.11 Total differentials. Let A be an open set in a vector space X. Let 
{ ei, ..., e n} be a basis for X. Let / : A —> R be a continuously differentiable 
function. Then 
df = -— dx1 H 
h - — (ir„ 
ox i 
oxn 
is called the total differential of /. This is just the derivative of a real-valued function 
expressed in terms of the derivatives of the coordinate functions dxt : X —> E. 
Lemma 5.5.12 Derivative of the inverse function. Let f : A —> Y be a differen-
tiable and invertible function. Assume that its range f(A) = B is open in Y. If the 
inverse function g : B —> X is differentiable, then g'(b) = (/'(a)) - 1 for all a G A 
andb = /(a) G B. 
Proof. We see that the composition g-f : X —> X is the identity function I : X —-> X. 
Hence, by the chain rule, Theorem 5.5.6, and by appealing to Example 5.5.5, 
< / ( b ) - / ' ( a ) = / ' ( a ) = / . 
This shows that g'(h) = (/'(a))" 1. 
D 
Theorem 5.5.13 Derivatives and Cartesian products. Let X, Y and U be normed 
spaces. Let A be an open subset of U. Let f : A —> X and g : A —* Y be two 
continuously differentiable functions. Define 
tp-.A^XxYby 
ip(a) = (/(a), 9(a)) E l x Y , a £ A. 
Then ip is a continuously differentiable function on A with 
^'(a)(u) = (/'(a)(u), 
ff'(a)(u)) € X x Y 
for all a 6 A and u G U. 
Proof. Define R : X -> X x Y and S : Y -y X x Y as Rx = (x, 0) and 
5y = (0, y) for all x G X and y e F . Then we see that 
<p = R-f 
+ 
S-g. 
Now i? and S are linear functions. Hence, they are continuously differentiable. 
Therefore, ip is continuously differentiable by the chain rule, Theorem 5.5.6. To find 
its derivative at a G A, let /(a) = b G l and g(u) — c G Y. Hence, again by the 
chain rule, Theorem 5.5.6, 
<p'(&) = fi'(b) • /'(a) + S'(c) • «/(a) = ñ . /'( a) + 5 • g'(a). 

RULES OF DIFFERENTIATION 
217 
The last equality follows from Lemma 5.5.3, which gives the derivatives of linear 
functions. Hence 
y/(a)(u) 
= 
fi-/'(a)(u) 
+ S-fl'(a)(u) 
= 
(/'(a)(u),0) + ((W(a)(u)) 
= 
(f(a)(u), 5'(a)(u)) 
for all u e U. 
a 
Problems 
5.50 
Let / : K3 -> K3 be differentiable. Assume that 
f(x, y, z) = f(x + y,0,x + z) 
for all (x, y, z) 6 
p3 
Show that the linear transformation /'(x) : K3 -> K3 is never onto for any x e R3 
of the form (a, 0, c). 
5.51 
Let ¡p : X —► M. be a differentiable function. If ip(a) ^ 0, then show that 
V(l/^)(a) = - ( l M a ) 2 ) V ^ ( a ) . 
5.52 
Let / be a real-valued differentiable function of y = (yi, ..., yn). 
Let 
y = y(x) be a differentiable function of x = (xi, ..., xm). Show that 
¿-^j 
dyj 
5.53 
Let A be an open set in the Euclidean space Rn. Let / : A —> K be a twice 
continuously differentiable function. Then the Laplacian of / is defined as 
A/w=g(x)+-+g<x), 
wherex = (x\, ..., xn) G A. Letn = 2andlet(xi, x-¿) = (x, y). Express/(x, y) 
in terms of polar coordinates x = rcosöandy = rsin6*as/(:r, y) = F(r, 9). Show 
that 
A „ _ ö V 
&j__&F_ 
1ÖF 
1 d2F 
5.54 
Let f(x, y, z) = ip(x2 + y2 + z2), where if : M —* K is a differentiable 
function. Find V / and A/. Here A / is as in Problem 5.53. 

218 
DERIVATIVES 
5.55 
Let k e N. Let / : R™ -> R be a differentiable function. If /(fac) = ife(x) 
for all x e R™ and for all í e R, then show that (V/(x), x) = fc/(x). 
5.56 
Let § : 7 -> R be a differentiable function. Let /(x) = </(Tx), where 
T : X —» y is a linear transformation. Find V / in terms of Vg. 
5.57 
Let g : R -> R be a differentiable function. Define / : X x Y —► R as 
/(x, y) = <?((Tx, y)), where T : X —> F is a linear transformation. Find V/. 
5.6 
DIFFERENTIATION OF PRODUCTS 
The rule for the differentiation of products is familiar from the one-variable case. 
Consider 
f(x) = a;2e*cosa: 
(5.61) 
as an example. To differentiate /, consider it as the product of three functions 
fi(x) = x2, f2(x) = ex, and f3(x) = cosa:. Then 
/'(*) 
= 
f[(x)f2(x)f3(x) 
+ f1(x)f^x)f3(x) 
+ 
f1(x)f2(x)f^x) 
= 
2xex cosa; + x2ex cosa; — x2exsm.x. 
To generalize this method, consider f(x) as the composition of two functions F : 
R -> M3 and M : R3 -> R defined by 
F(x) 
= 
(fi(x), 
f2(x), 
f3(x)) 
= 
{x2, ex, cosa;) g R3 
and 
M(yi,y2,y3) 
= 
Vi ■ V2 ■ 2/3 G R. 
Then /(x) = M(F(x)). Therefore, by the chain rule, Theorem 5.5.6, 
f(x) = 
M'(F(x))-F,(x). 
The derivative of F : R —» R3 is obtained as 
F'(x) = (2x, es, cosx). 
This follows from Theorem 5.5.13. We will show below that M'(yi, y2, y3) : R3 —> 
R is given as 
M'(yi, ?/2, 2/3)(^i, v2, v3) 
= 
viy2y3 + yiv2y3 + yiy2v3 
(5.62) 

DIFFERENTIATION OF PRODUCTS 
219 
for all (vi, V2, V3) € M3. Then the differentiation rule for f(x) = x2ex cosa; in 
(5.61) follows. Equation (5.62) is true for all multilinear functions. This is a key 
result in the differentiation of product-like functions. 
Notations 5.6.1 Multilinear functions. The notations and definitions below were 
introduced in Section 3.3. Let Ui, ..., Uk be vector spaces. We ignore the difference 
between the spaces 
Ui x • • • x Uk and 
Ui®---@Uk 
and denote both spaces as X. Let P¿ : X —► X be the coordinate projection on the 
ith component. Hence, if u, G Uj, j = 1. ..., k, then 
P¿(ui + • • • + Ufc) = u¿ for all i = 1, ..., k. 
Also, let Qi = I — Pi. Then M : X —> Y is called a multi-linear function if the 
equation 
Ti(a)x = M ( Q i a + P i x ) = M ( a + i 3
i x ) - M ( a ) , x e l 
(5.63) 
defines a linear map T¿(a) : X —> F for each fixed a e X and for each fixed 
i = 1, ..., k. The second equality in (5.63) follows from Lemma 3.3.5. In particular, 
if a e X, Uj G Ui, and ¿eR, then 
Ti(a)(íui) 
= 
A/(Qia + Pi(íui)) = M(Q ia + íui) 
(5.64) 
= 
M(a + Pi{t\ii)) - M(a) 
(5.65) 
= 
AÍ(a + íu¿)-M(a). 
(5.66) 
Here P¿(íu¿) = <u¿ since u¿ € f/¿. Equation (5.65) follows from Lemma 3.3.5. 
Theorem 5.6.2 Derivatives of multilinear functions. Multilinear functions are 
differentiable everywhere. If X = U\ © • • • © Uk and t/M : X —» V /s multilinear, 
then 
M'(a)(x) = V / C 
M(Q¿a + P,x) 
(5.67) 
/or a// a, x G X. The notations are as in Notations 5.6.1 above. 
Proof. If Ui £ Ui, then the directional derivative of M : X -^ Y at a. e X and in 
the direction of u¿ is 
M'(a; Ui) = lim —^ 
j 
— = M{Qta + u¿). 
In fact, (5.66) and the linearity of T¿(a) : X —> Y show that the ratio above has the 
constant value of 
T¿(a)u¿ = M(a + u¿) - M(a) = M(Q¿a + u¿). 

220 
DERIVATIVES 
Multilinear functions M : X —> Y are continuous by Theorem 4.4.9. Hence 
Aí'(a; u¿) is a continuous function o f a e l for each fixed u¿ £ £/,. 
If i?¿ is a basis for [/¿, then we see that E = U¿1£¿ is a basis for X = ©i[/¡. Hence, X 
has a basis consisting of vectors contained in Í7¿ spaces. Therefore M i I - t F has 
continuous directional derivatives with respect to the vectors in a basis. Therefore, 
M : X —> Y is also continuously differentiable by the existence theorem, Theorem 
5.3.4. To obtain (5.67), note that each x e X = U\ © • • • © Uk has a representation 
as 
x = Fix H 
h Pfcx, P,x e C/¿. 
Hence (5.67) follows from M'(a)(P¿x) = M(Q¿a + P¿x) and from the linearity of 
M'(a) : X -> y. 
□ 
Remarks 5.6.3 Explicit notations. Theorem 5.6.2 above is the key result in the 
differentiation of products. This theorem implies, for example, that if 
a = (ai, a2, a3) and x = (x1; x2, x3) 
are two vectors in X = U\ x t/2 x U3 ~ U\ © U2 © C/3, then 
M'(a)(x) 
= 
M'(a 1,a 2,a 3)(x 1, x2, x3) 
= 
M(xi, a2, a3) + M(ai, x2, a3) + M(a1} a2, x3). 
This is a direct generalization of the usual product rule (5.62) above. 
Examples of Product Differentiations 
The following examples of product differentiation are all obtained by the direct 
application of Theorem 5.6.2 above. In these examples, A is an open set in a normed 
space X. 
Example 5.6.4 Product of a scalar function with a vector-valued function. Let 
r : A —* R and g : A —> Y be two differentiable functions. Define 
/ = rg : A —> y by /(x) = r(x)#(x) € Y for all x f A 
Then / : A —> X is differentiable and 
/'(x) = r'(x) 5(x) + r(x) </(x) € L(A\ F), 
or, more explicitly, for all u € X, 
/'(x)(u) = r'(x)(u)fl(x) + r(x) 5'(x)(u) G y. 

DIFFERENTIATION OF PRODUCTS 
221 
To prove this result, define F : A —> R x Y by 
F(x) = {r{x), g{x)) eRxY, 
x e A 
and Q : R x Y -> Y by 
Q(a, y) = ay G V, (a, y) e E x F, 
and apply Theorem 5.6.2 above. 
A 
Example 5.6.5 Inner products. Let y be a Euclidean space. Let / : A —> Y and 
g : A —> y be two differentiable functions. Define 
h = (/, 5> : A - R by /i(x) = </(x), 5(x)> for all X G A 
Then h : A —> R is differentiable and 
ft' = </, </> + </', 9)-A^ 
L(X, R). 
More explicitly, 
h'(x) = (/(x), </(x)) + (f'{x), g(x)) 
:X-*R 
for all x G A Still more explicitly, 
h'(x)(u) = </(x), p'(x)(u)) + </'(x)(u), g(x)) G R 
for all x 6 A and for all u G X. To prove this result, define F : A —> Y x Y by 
F(x) = (/(x), <?(x)) € y x y, x G A, 
and g = y x y -> R by 
Q(p, q) = <p, q) e R, (P, q) G y x y, 
and apply Theorem 5.6.2 above. 
A 
Example 5.6.6 Cross products in K3. Let /, g : A —> R3 be two differentiable 
functions. Define ip : A —> R3 as the cross product 
p(x) = /(x) x <?(x) G R3, x G A 
Then <¿> : A —> R3 is differentiable and 
¥>'(x)(u) = /'(x)(u) x g(x) + /(x) x </(x)(u) G R3 
for all x G A and u G X. To see this, note that Q(p, q) = p x q defines a multilinear 
function Q : R3 x R3 -> R3. 
A 

222 
DERIVATIVES 
Example 5.6.7 Mixed products in R3. Let/, g, h : A -+ R3 be three differentiable 
functions. Define (p : A —> R as the mixed product 
<¿>(x) = (/(x), ,g(x) x /i(x)) e E , x e l 
Then ^ : A —» R is differentiable and 
p'(x)(u) 
= 
(/'(x)(u), ff(x) x h(x)) + </(x), </(x)(u) x h(x)> 
+ 
(/(x) ) f l(x)xft'(x)(u)>eM 
for all x G ^ and u G X. To see this, note that Q(p, q, r) = (p, q x r) defines a 
multilinear function Q : R3 x R3 x R3 -> R. 
A 
Example 5.6.8 Products of linear maps. Let R, S : A —> L(y Y) be differen-
tiable functions. Define 
T = S ■ R : A -» L(F, y) as T(x) = S(x) • i?(x) for all x G A 
Then T : A-* L(Y, Y) is differentiable and 
T' = S ■ R' + S' ■ R : A -> L{X, L(Y, Y)). 
More explicitly, 
T'(x) = 5(x) • ñ'(x) + S'(x) • Ä(x) : X -> L(y y) 
for all x G A. Still more explicitly, 
T'(x)(u) = 5(x) • Ä'(x)(u) + S'(x)(u) • i?(x) G L(y, y) 
for all x G A and for all u G X. 
A 
Problems 
5.58 
Let h : R3 -► R be defined by 
/i(x) = ||(a; — y, y — z, x — z)\\2 
for all x = (x, y. z) G R3, 
where the norm is the Euclidean norm. Compute /i'(a)(x) for a, x G M3. 
5.59 
Let T : X —> X be a linear transformation. Let /(x) = (Tx, x), x G X. 
Show that (V/(a), x) = (Ta, x) + (Tx, a), a, x G X. 
5.60 
Let T, S G L(X, y) and /(x) = (Tx, 5x) y, x G X. Find V/(a), a G X. 

DIFFERENTIATION OF PRODUCTS 
223 
5.61 
Let T e L(X, Y) and let ¡p : X -> K be a differentiable function. Find V / 
for /(x) = (<¿>(x)Tx, x), x G X. 
5.62 
Let T e L(R3, R3) and let ip : X -> E be a differentiable function. Define 
F : M3 -> M3 by F(x) = ^(x)(x x Tx). Find F'(a)(x). 

This page intentionally left blank

CHAPTER 6 
DIFFEOMORPHISMS AND MANIFOLDS 
Diffeomorphisms are invertible mappings between two open sets that are continuously 
differentiable in both directions. The simplest type of diffeomorphism is an invertible 
linear transformation. This is a basic concept in linear algebra. Similarly, the study 
of diffeomorphisms is a basic part of differential calculus. 
Most of the results on diffeomorphisms depend on the inverse function theorem. This 
is one of the central results we obtain in this course. It states that if a continuously 
differentiable function has an invertible derivative at a point, then its restriction to 
a neighborhood of that point is a diffeomorphism. 
We also call a continuously 
differentiable function a C1 function. 
Diffeomorphisms and the inverse function theorem are the gateway to the study of 
manifolds: surfaces, curves and other lower-dimensional structures that are embedded 
in a larger space. A good example is the upper half of the surface of the unit 
sphere in M3, which is a two-dimensional manifold. We can represent this set 
as the image of the unit disk { (x, y) | x2 + y2 < 1 } in M2 under the mapping 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
225 

226 
DIFFEOMORPHISMS AND MANIFOLDS 
<p(x, y) — (x, y, (1 — (x2 + y2))1^2). We can also represent the entire surface of 
the unit sphere implicitly as { (x, y, z) | F(x, y, z) = 1 — (x2 + y2 + z2) = 0 }. It 
turns out that there are a number of equivalent ways to define a manifold. In each case, 
the best approach is to define what a manifold looks like locally (in the neighborhood 
of each of its points) rather than to attempt to represent the entire structure by a single 
function. These ideas, as well as the important topic of differentiation on manifolds, 
are explored in sections 6.2 - 6.5. 
6.1 THE INVERSE FUNCTION THEOREM 
We begin with a statement of the inverse function theorem for C1 functions. Let 
A C X be an open set and F : A —*• X a C1 function. If F'(a) : X —> X is 
invertible at a G A, then a has a neighborhood G C A such that the restriction of F 
to G is a C1 diffeomorphism F\G ■ G —> X. 
The hardest part of proving the inverse function theorem is to demonstrate the exis-
tence of a continuous inverse. After this is done, the second part of the theorem— 
continuous differentiability of the inverse function—follows by routine arguments. 
First, we are going to obtain this second part. In what follows, X and Y are two 
normed spaces and A and B are open sets in X and Y, respectively. 
Lemma 6.1.1 Let f : A —> Y be a function and B = f(A). Assume that B is open 
in Y and f has a continuous inverse g : B —> X. If f is differentiable at & £ A and 
if its derivative T = /'(a) : X —► Y at a € A is invertible, then g is differentiable 
at b = /(a) and g'(h) = S — T~l : Y —> X. Furthermore, if f is a C1 function on 
A with an invertible derivative /'(a) : X —> Y at every point a G A, then g is a C1 
function on B. 
Proof. Let y n G Y be a zero-sequence. Since b = /(a) G B and since B is open, 
assume that (b + y n) G B without loss of generality. Let x n = g(b + y„) — 5(b). 
Then x n G X is a zero-sequence because of the continuity of g. Since T is the 
derivative of / at a, there is another zero-sequence r „ e l such that 
||/(a + x„) - / ( a ) - T x „ | | = ||y„ - T x „ | | < rn||x n||. 
This follows from the definition of the derivative. Hence 
||xn|| 
< 
||Syn|| + | | x n - 5 y n | | 
= 
||Syn|| + | | S ( T x n- y n) | | 
< 
IISIIIIynll + IISHIirxn-yJI 
< 
l|5||||yn|| + ||S||r„||x„||, and, therefore, 
( l - r n | | S | | ) | | x n| | 
< 
||5||||yn||. 

THE INVERSE FUNCTION THEOREM 
227 
Since rn —> 0, we will assume that (1 — rn 11.S'||) > 1/2 for all n. In this case, 
||x„|| 
< 
2||5||||yn||. Therefore 
llff(b + y „ ) - f f ( b ) - S y „ | | 
= 
| | x n - S y n | | 
= 
¡|S(Tx„-yn)|| 
< 
IISHIIyn-Txnll 
< 
l|5||r„||xn|| 
< 
2rn||5||2||yn||. 
Since 2r„ HS1)!2 —> 0, we see that g is differentiable at b and </(b) = S. 
To obtain the second part, assume that / is a C1 function on A with an invertible 
derivative /'(a) : X —> Y at every point a G A. Hence / ' : A —> L(X, Y) is a 
continuous function and its range is contained in Linv(X, Y), the set of invertible 
linear operators. The first part of the proof shows that g' = Inv • / ' • g is the 
composition of g : Y —> X with f':X—> 
Linv(X, Y) and with the inversion Inv : 
Lim(X, Y) —»• L\nv(Y, X). The first two mappings are continuous by assumption. 
The inversion mapping is continuous by Theorem 4.4.38. Hence g' : Y —» L(Y, X) 
is continuous as the composition of continuous functions. 
□ 
The next step is to prove the inverse function theorem for an important special case 
(roughly, the case where ||D/(x) — I\\ < 1, i.e., the derivative is "close" to the 
identity map). Subsequently, we show that the general case can be reduced to this 
special case. 
Lemma 6.1.2 Let f : X —» X be a C1 function with an invertible derivative /'(a) : 
X —> X at every point a £ l , Assume that there is a A < 1 such that 
| | / ( v ) - / ( u ) - ( v - u ) | | < A | | v - u | | 
for all u, v £ X. Then f(X) = X and f has a C1 inverse g : X —> X. 
Proof. If /(u) = /(v), then ||v - u|| < A||v - u||. This implies ||v - u|| = 0, 
since A < 1. Hence / is a one-to-one function. To show that it maps X onto X, we 
will show that the equation /(x) = c has a solution a 6 X for any given e e l We 
obtain a as the limit of a sequence of approximate solutions x n. This is essentially 
Newton's iteration method, as explained below in Remarks 6.1.7. 
Let x0 = c and x n +i = x n + (c — /(x„)) for n > 0. Then we have, for n > 1, 
Xn+i-Xn 
- 
x r a - x n _ : - / ( x n ) + /(x n_i) and 
(6.1) 
||x„+i-x„|| 
= 
||xn - x „ _ i - / ( x „ ) +/(x„_i)|| 
(6.2) 
< 
A||x n-x„_ 1||. 
(6.3) 

228 
DIFFEOMORPHISMS AND MANIFOLDS 
Then ||xn+i — x„|| < An||xi — x0|| for all n £ N. This follows from (6.3) by an 
easy induction argument. Hence the sums 
£ j * n + l -X„|| < ( E n
A " ) HX1 " X 0 | | < (1 - A)^||X! -X 0|| 
remain bounded. Therefore x n is a Cauchy sequence in X and limn x n = a exists. 
Then /(x„) —► /(a) by the continuity of / : X —> X. Hence, by taking limits 
in the inductive step x n +i = x„ + c — /(x„), we obtain /(a) = c. Hence / is a 
one-to-one function that maps X onto X. 
It follows that / has an inverse function g : X —> X. We claim that g is continuous. 
Let p, q £ X. Let g(p) = u, </(q) = v. Hence p = /(u) and q = /(v). We have 
| | v - u | | 
< 
| | q - p | | + | | q - p - ( v - u ) | | 
= 
| | q - p | | + | | / ( v ) - / ( u ) - ( v - u ) | | 
< 
| | q - p | | + A | | v - u | | . 
Hence \\g(q) — g(p)|| = ||v — u|| < (1 — A)^1!^ — p||. This gives the continuity 
(actually the uniform continuity) of g on X. Then Lemma 6.1.1 shows that g' : X —» 
L(X, X) exists and is continuous. 
D 
Lemma 6.1.2 generalizes to the case where \\Df(x) — T\\ < ||T - 1|| for some 
invertible linear mapping T. 
Corollary 6.1.3 Let f : X —> X be a C1 function with an invertible derivative 
/'(a) : X —> X at every point a £ X. 
Let T £ L(X, X) be an invertible 
transformation with inverse S. Assume that there is a A such that X\\S\\ < 1 and 
such that 
| | / ( v ) - / ( u ) - T ( v - u ) | | < A | | v - u | | 
for all u, v £ X. Then f(X) = X and f has a C1 inverse g : X —> X. 
Proof. Note that ||x|| = ||5Tx|| < ||5|| ¡|Tx|| for all x £ X. Hence 
| | S / ( u ) - S / ( v ) - ( u - v ) | | 
< 
| | 5 | | | | / ( u ) - / ( v ) - T ( u - v ) | | 
< 
A||S||||u-v||. 
If A||5|| < 1, then Lemma 6.1.2 shows that h — Sf : X —* X is invertible. Then 
Th = / is also invertible. 
D 
The general case follows from Corollary 6.1.3. The key idea is that if F is C1 at a, 
then the requirement of the corollary is met locally (with T = DF(SL)). We construct 
a function / for which the requirement of the corollary is met over the whole space 
X, ensuring that / and F are identical in a neighborhood of a. 

THE INVERSE FUNCTION THEOREM 
229 
Theorem 6.1.4 The inverse function theorem for C1 functions. Let Ac X be an 
open set and F : A —> X a C1 function. If F'{a) : X —> X is invertible at a S A, 
then a has a neighborhood G c i such that the restriction of F : A —> X to G is a 
C1 diffeomorphism F\c ■ G —» X. 
Proof. Let a G A Assume that T = F'(a) : X —► X is invertible. Given e > 0, 
find an r > 0 such that B3r(a) c A and (since F is S1) 
||F'(x) - T | | L ( X , X ) < £ for all x e B3r(a) C A. 
(6.4) 
Let <¿? : X —> M be a C1 function such that y>(x) = 1 if ||x|| < 1 and y>(x) = 0 if 
||x|| > 2. An example of tp is given in Lemma 6.1.6 below. Define, for all x e X, 
/(a + x) 
= 
JF(a)+Tx + V9(x/r)(F(a + x ) - F ( a ) - T x ) . 
(6.5) 
Then 
/(a + x) 
= 
F(a + x) if ||x|| < rand 
(6.6) 
/ ( a + x) 
= 
F(a) + Tx if ||x|| >2r. 
(6.7) 
The function / is thus identical to F inside a ball of radius r around a, and identical 
to an affine function outside a ball of radius 2r around a. We claim that / : X —> X 
is a C1 function. In fact, / is a sum of the products of C1 functions in the set 
||x — a|| < 3r, so clearly / is a C1 function in this set. And if ||x — a|| > 2r, then 
/(x) = -F(a) + T(x — a), so that / is also a C1 function in this region. Therefore / 
is a C1 function on X. 
We want to estimate \\f'(x)\\L(x.x)- 
We first claim that 
\\F(a + x)-F(a)-Tx\\x 
< 
2er and 
(6.8) 
||F'(a + x ) - T | | L ( X i X ) 
< 
e 
(6.9) 
for all ||x|| < 2r. Here (6.9) follows immediately from (6.4). To obtain (6.8), let 
A(x) = F(a + x) - F(a) - Tx. Then ||A'(x)|| < e by (6.9). Hence, by the Mean 
Value Theorem, 
¡|A(x)-A(0)|| = ||F(a + x ) - F ( a ) - T x | | < e||x|| < 2er 
whenever ||x|| < 2r. 
The next step, with (6.5) in mind, is to differentiate 
tf(x) 
= 
p{x/r)(F(a 
+ 
x)-F(a)~Tx) 
to obtain 
i?'(x) 
= 
(l/r)(F(a + x) - F(a) - Tx)<p'(x/r) 
+</3(x/r)(F'(a + x ) - T ) . 

230 
DIFFEOMORPHISMS AND MANIFOLDS 
Note that i?'(x) = 0 for ||x|| > IT, since then ip'(x/r) and <p(x./r) are both 0. To 
derive a uniform bound on $(x), let 
p = supx||<^(x)|| andg = supx||</(x)||. 
(6.10) 
They both exist since they are the upper bounds of continuous functions of compact 
support. Then if ||a;|| < 2r, it follows from (6.8) and (6.9) that 
||0'(x)|| 
< 
(l/r)||F(a + x ) ~ F ( a ) - T x | | | | ^ ( x / r ) | | 
+ Mx/r)||||F'(a + x ) - T | | 
< 
(l/r)2erq + pe = (2q + p)s. 
But this bound actually holds for all x, since #'(x) = 0 for ||x|| > 2r. By definition 
(6.5), we also have 
i?(x) = /(a + x) - F(a) - Tx and tf'(x) = /'(a + x) - T. 
Then, again by the Mean Value Theorem, we obtain 
||/(u) - /(v) - T(u - v)|| < (2q +p)e \\u - v|| 
(6.11) 
Choose e > 0 so that (2q + p)e\\S\\ < 1, where S = T~x. Then Corollary 6.1.3 
shows that / : X —> X is a C1 diffeomorphism. Hence / maps any open G C X to 
an open /(G), and its restriction to G is a diffeomorphism between G and /(G). But 
if we set G = -Br(a), then /(x) = F(x) for x e G. Hence F is a diffeomorphism 
between G and F(G). 
□ 
To complete the proof of the inverse function theorem, the following two lemmas 
establish the existence of a function ip with the properties exploited in the above 
argument. 
Lemma 6.1.5 Define ip 
m = < 
1 
ift <1 
l - ( 2 / 9 ) ( i - l ) 2 
ijl < í < 5/2 
(2/9)(4-í) 2 
í/5/2 < í < 4 
0 
if 4 < t. 
Then xp is a C1 function. 
Proof. This is left as an exercise. See Figure 6.1. 
□ 
Lemma 6.1.6 Given a Euclidean space X, there is a C1 function ip : X —> K such 
thatp(x) = 1 ¡7||x|| < land(p{x.) = 0i7||x|| > 2. 

THE INVERSE FUNCTION THEOREM 
231 
0 
1 
2 
3 
J 
Figure 6.1. Graph of the function in Lemma 6.1.5. 
Proof. Let <p(x) = ?/>(||x||2), where ip : K —► M is the function obtained in Lemma 
6.1.5. Note that ||x||2 — (x, x) is a C1 function, X —> R, as it is a polynomial. 
Hence tp is the composition of two C1 functions, and therefore a C1 function. 
D 
Remarks 6.1.7 Relation to Newton's iteration method. Lemma 6.1.2 should be 
considered the core of the inverse function theorem. The fact that the equation 
/(x) = c can be solved for all c £ X is the essential component of this theorem. 
The other arguments are more or less cleaning up the details. The solution of this 
equation is obtained by a simplified form of Newton's iteration process. Originally 
this is used to solve f(x) = 0, where / : / —> R is a continuously differentiable 
function defined on an open interval I. Let a¿ G I. If f(a{) = 0, then we are done. 
Otherwise, replace the original function / by its affine approximation 
f1(x) = f(a1) + 
f'(a1)(x-a1) 
and solve f\ (x) = 0. This is a linear equation. Let 02 be its solution. Under some 
reasonable assumptions, 02 is a better approximation for a solution. Continue this 
process to obtain a sequence an. If it converges, the limit is a solution for f(x) = 0. 
We follow this method in the proof of Lemma 6.1.2. Actually, things are even simpler. 
We start with xi € X. The affine approximation of / at xi is 
/1(x) = /(xi) + / ' ( x 1 ) ( x - x 1 ) . 
Instead of this function, we replace the derivative /'(xi) : X —> X by the identity 
I : X —» X and use 
5i (x) = / ( x i ) + ( x - X j ) . 
This is reasonable, since our hypotheses imply that all the derivatives of / are close 
to the identity. In fact, we see that if / : X —•> X is a differentiable function such 
that 
| | / ( v ) - / ( u ) - ( v - u ) | | < A | | v - u | | 
for all u, v e X, then ||/'(x) - I\\L(X, X) < A for all x E l The solutions of 
ffn(x) = /(X n) + (X - Xn) = C 

232 
DIFFEOMORPHISMS AND MANIFOLDS 
give a sequence that converges to the solution of /(x) = c. 
Lemma 6.1.8 Graph diffeomorphisms. Let X and Y be Euclidean spaces, A an 
open subset of X, and f : A —> Y a C1 function. Define F as 
F(x, y) = (x, y + /(x)), (x, y) e A x Y. 
(6.12) 
Then F: AxY—^AxYisa 
diffeomorphism. 
Proof. Let (a, b) e A x Y and (x, y) e I x 7. We see that 
F'(a,b)(x,y) = (x,y + /'(a)x). 
(6.13) 
We see that F'(a, b)(x, y) = (0, 0) implies that (x, y) = (0, 0). Hence, 
F'(a, b):XxY^XxY 
(6.14) 
is an isomorphism. Also, F : A xY ~> A x y is invertible with the inverse 
F- 1(x,y) = (x, y - / ( x ) . 
(6.15) 
Hence F: AxY^AxY 
is a diffeomorphism. 
□ 
Definition 6.1.9 Graph diffeomorphisms. The diffeomorphism 
F :AxY 
^ AxY 
(6.16) 
defined in Lemma 6.1.8 is called the graph diffeomorphism induced by / : A —> Y. 
Examples of Diffeomorphisms 
Let X and Y be two Euclidean spaces, A an open set in X, and / : A —+ Y a C1 
function. To find out if / is a diffeomorphism, we start with the routine part of the 
test. Compute the derivative /'(x) € L(X, Y) at a general point x € A and see if 
this is an invertible linear transformation for all x € A. If / passes this test, then 
it may be a diffeomorphism. At this stage, we know that the range B = f(A) is 
also an open set. (Why?) We do not know, however, if / has an inverse function 
<7 : B —> A. The inverse function exists if and only if / is a one-to-one function on 
A. The verification of this point may not be easy. Being a one-to-one function is not 
a local property, and here there is no help from calculus. 
Nevertheless, the inverse function theorem tells us that if /'(a) : X —> Y is an 
invertible linear transformation for a certain a. £ A, then the restriction of / to a 
small enough neighborhood of a is a diffeomorphism. In general, this is all we need 

THE INVERSE FUNCTION THEOREM 
233 
to know in differential calculus, as this study involves the properties of functions only 
in small neighborhoods. 
It is useful to consider the part of the domain A where f : A —* Y has an invertible 
derivative. We refer to this part as the regular part of the domain. 
Definition 6.1.10 Regular part of the domain. Let / : A —> Y be a C1 function. 
Then its regular domain (or the regular part of its domain) is 
A0 = { a | a £ A, /'(a) : X -► Y is invertible } . 
(6.17) 
Note that Ao is also an open set. (Why?) 
Example 6.1.11 Define / : R -> R by f(x) = x2. What is the regular part of its 
domain? Give examples of the subsets of the regular domain on which the restrictions 
of / are diffeomorphisms. 
Solution. We have f'(a) = 2a for all s e i This defines an invertible transforma-
tion x -> 2ax for all a ^ O . Hence, A0 = R \ { 0 } = (-oo, 0) U (0, oo). For all 
a e Ao, there is a neighborhood of a where / is a diffeomorphism. In particular, 
there is a neighborhood of a £ A0 in which / is one-to-one. This is not true at 
0 0 AQ. There is no neighborhood of 0 in which / is one-to-one. We see that the 
restrictions of / to (—oo, 0) and to (0, oo) are diffeomorphisms. The restriction of 
/ to (—oo, —3) U (1, 3) is also a diffeomorphism. A 
Example 6.1.12 Define / : R -> R by f[x) = x3. What is the regular part of its 
domain? Give examples of the subsets of the regular domain on which the restrictions 
of / are diffeomorphisms. 
Solution. We have /'(a) = 3a2. Hence A0 = R \ { 0 } = (-oo, 0) U (0, oo). We 
see that / : R —> R is one-to-one and onto R. Hence it has an inverse g(x) = x1'3. 
The restriction of / to AQ is a diffeomorphism /o : AQ —> R, but / itself is not a 
diffeomorphism, since its inverse g is not differentiable at 0. A 
What about functions from R2 —> M2 or from ñ 3 —> R3? Chapter 1 provides many 
examples, and we can ask about their invertibility or the invertibility of their various 
restrictions. Some of the problems in this section require finding the regular parts 
of their domains. Here, we shall consider only the spherical coordinates mapping 
defined in Example 1.3.14. 
Example 6.1.13 Spherical coordinates. Define F : R3 —> R3 as 
F(p, 0, ip) = (p sin (pcos9, p sin if sinö, pcosip). 
(6.18) 

234 
DIFFEOMORPHISMS AND MANIFOLDS 
What is the regular part of its domain? Give examples of subsets of the regular 
domain on which the restrictions of F are diffeomorphisms. 
Solution. The standard coordinates of the domain space K3 are denoted as (p, 9, ip) 
in this example. The derivative F'(p, 9, <p) : R3 —> R3 transforms the vectors in the 
standard orthonormal basis of the domain space to the vectors 
(sin ip cos 6, sin <p sinö, cos <p) 
(6.19) 
(—p sin ip sinö, p sin ¡pcos9, 0) 
(6.20) 
(pcosipcosö, pcostpsinö, — psintp) 
(6.21) 
in the range space R3. An easy check shows that these vectors are orthogonal to 
each other and their norms are, respectively, 1, psin<p, and p. Hence the standard 
orthonormal basis of the domain is an eigenbasis (Definition 3.6.3) for the derivative. 
The derivative is invertible unless psin<p = 0. Hence the regular domain of the 
spherical coordinates is obtained by removing the planes 
p = 0, (p = kir (ifc e Z) 
(6.22) 
from the (p, 9, (p)-space. We see that all the values taken by F on its regular domain 
are also taken on the part of this domain defined by 0 < p and 0 < <p < IT. The 
restriction of F to this part is still not a diffeomorphism, since the points 9 = 9Q + 2kn, 
k e Z, with a fixed #o £ K, are all mapped to the same point in the xyz-space. To 
obtain a diffeomorphism, we have to restrict the domain of 9 to a convenient interval 
like (0, 2n) or (—IT, IT). Hence the restrictions of F to 
A 
= 
{ (p, 9, ip) | 0 < p, 0 < 9 < 2TT, 0 < ip < 7T } or to 
(6.23) 
A' 
= 
{ (p, 6, <p) | 0 < p, -7T < 9 < 7T, 0 < if < 7T } 
(6.24) 
are diffeomorphisms. Again, an exact choice of these regions is not too important. 
Note that p sin <p = 0 corresponds to the points on the 2-axis in the xyz-space. Hence 
as long as a point (x, y, z) is away from the z-axis, then a neighborhood ofthat point 
will be the diffeomorphic image of a region in the p#<p-space under the function F 
defined in (6.18). 
A 
Remarks 6.1.14 A visualization of spherical coordinates. One usually attaches 
the values of p, 9, and ip to the image point F(p, 9, tp) in the xyz-space to obtain a 
visualization of this coordinate system, as in Figure 6.2. 
Example 6.1.15 For any open bounded interval / of R, there is a diffeomorphism 
/ : I —+ R. We obtain this diffeomorphism as follows. There is a diffeomorphism 
dF_ 
~dp 
dF_ 
~m 
OF 
(P, 8, <P) 
(P. 0, f) 
(P. 0, ¥>) 

THE INVERSE FUNCTION THEOREM 
235 
Figure 6.2. Spherical coordinates. 
g : I —> (—7r/2, TT/2). Also, the function /i(x) = tanx is a diffeomorphism from 
(—7r/2,7r/2) onto ]R. Thus, f = h ■ g : 7 —> R is a diffeomorphism. 
Example 6.1.16 Let g : R3 -> R3 be defined by #(x) 
for all x = (x, y, z) G R3. Then 
(x + ev,y + ez,z + ex 
/'(x) 
0 
1 
i 
0 
1 
e 
e* 
0 
1 
Hence, det /'(x) = 1 + e
x+v+z 
=£ 0 for all x e R3. So, at each point x e R3, 
there is an open set U of R3 containing x such that the restriction of / to U is a 
diffeomorphism. 
Example 6.1.17 There are r > 0 and s > 0 such that if \a — 8| < r and |6| < r, 
then the system of equations 
x3 + 2x2y + y2 
yx2 - 3x4y + y3 
a 
b 

236 
DIFFEOMORPHISMS AND MANIFOLDS 
has a unique solution (x, y) such that \x — 2| < s and \y\ < s. To see this, let 
f(x, y) = (x3 + 2x2y + y2, yx2 - 3x4y + y3) 
for all (x, y) G R2. 
Then 
/'(*,2/) 
3x2 + Axy 
2x2 + 2y 
2xy - 12x3y 
x2 - 3x4 + 3y2 
Hence, 
/'(2,0) = 
12 
0 
-44 
is invertible. Also, /(2,0) = (8,0). Hence, by the inverse function theorem, there is 
an open set U containing (2,0) and an open set V containing (8,0) such that / maps 
U onto V in a one-to-one way. In particular, for all (a, b) G V, there is a unique 
(x, y) eU such that f(x, y) = (a, b). 
Problems 
6.1 
Let 
A 
= 
{ x = (x,y) GR2 \X > 0,y > 0 }, 
5 
= 
{x = ( x , y ) G M 2 | y > 0 } . 
Let 
/(x) 
= 
(x2 - y2,2xy) 
for all x G A. 
Show that / is a diffeomorphism from A onto £?. 
6.2 
Let £> = { x = (x, y, z) G R3 | x > 0, y > 0, z > 0 }. Let 
/(x) = (xy, yz, xz) 
for all x G -D. 
Show that every point in D has a neighborhood on which / is a diffeomorphism. 
6.3 
Let D be an open subset of W and / : D —> Z a continuously differentiable 
function. Suppose that f'(x):W^Z 
is an isomorphism for all x e D. Show that 
/(£>) is an open subset of Z. In addition, if / is one-to-one on D, then show that / 
is a diffeomorphism on D. 
6.4 
Let /(x) = (a;3 + x, y3 + y, z3 + z) for all x = (x, y, z) G 
Show that 
/ : 
is a diffeomorphism. 

THE INVERSE FUNCTION THEOREM 
2 3 7 
6.5 
Let 
A 
= 
{ (x,y) e R 2 | x s R , 0 < y < n }, 
B 
= 
{(x,y)eR2\y>0}. 
Define / : A -► R2 by 
/(x) = (ex cos y, ex sin y) for all x — (x,y) e R2. 
Show that / is a diffeomorphism from A onto B. 
6.6 
Let F : R4 -> R4 be defined by 
F(x) = (xi, X2, xtx2 + X3, x\ — X2 + X4) for all x = (xi, X2, X3, X4). 
Show that F is a diffeomorphism. 
6.7 
Define / : R2 -> R2 by /(x, y) = (ex + ey,ex - é>). Show that / is a 
diffeomorphism. What is the range of /? 
6.8 
Define / from the xy-plane to the uv-plane by 
(u, v) = f(x, y) = (3x + 2y, 6x + Ay). 
Give some examples, if exist, of open sets in the xj/-plane such that the restrictions 
of/ to these sets are diffeomorphisms. (Cf. Problem 1.30.) 
6.9 
Repeat Problem 6.8 for 
(u, v) = f(x, y) = (3x + 2y, 6x - 4y). 
(Cf. Problem 1.31.) 
6.10 
Repeat Problem 6.8 for 
(u, v) = f(x, y) = (xy, y/x). 
(Cf. Problem 1.32.) 
6.11 
Repeat Problem 6.8 for 
(u, v) = f(x, y) = ((x2 + y2)/(2x), (x2 + 
y2)/(2y)). 
(Cf. Problem 1.33.) 
6.12 
Repeat Problem 6.8 for 
(u, v) = f{x, y) = ((x2 + y1 + l)/(2x), (x2 + y2 - 
\)/{2y)). 

238 
DIFFEOMORPHISMS AND MANIFOLDS 
(Cf. Problem 1.34.) 
6.13 
Repeat Problem 6.8 for 
(u, v) = f(x, y) = (p(x, y) + q(x, y), p(x, y) - q{x, y)), 
where p(x, y) = ((x + l) 2 + y2)1/2 
and q(x, y) = ((x - l) 2 
+y2)1/2. 
(Cf. Problem 1.35.) 
6.2 
GRAPHS 
Graphs are among the most basic examples of manifolds in Euclidean spaces. A 
manifold (to be defined formally in the next section) is a set that coincides with a 
graph in a neighborhood of each one of its points. Hence, a manifold can also be 
called a local graph. 
Graphs were defined in Definition 1.3.1. From now on, we will use the term 'graph' 
in a slightly restricted sense. The intended meaning will be clear from the context. 
Definition 6.2.1 Graphs. A set F in a Euclidean space Z is called a graph if there is 
a coordinate system (X, Y) for Z, an open set A in X, and a C1 function / : A —> Y 
such that 
r = { ( x , y ) e Z | x e ^ , y = /(x) }. 
Usually we assume that (X, Y) is an orthogonal coordinate system. This is not a 
restriction of generality. If dim X = k and dim Z = n, then T is a fc-dimensional 
graph in an n-dimensional space. A function / : A —> Y is also denoted as 
y = /(x), x G A, or even more simply as y — /(x). The function y = /(x) is 
called an equation or an explicit equation for Y. 
Definition 6.2.2 Tangent spaces. Let Y be the graph of y = /(x). The linear 
tangent space Tc of Y at c = (a, /(a)) £ T is the graph of the linear function 
y = /'(a)x. The affine tangent space ATC of Y at the same point is the graph of the 
affine function 
y = /(a) + / ' ( a ) ( x - a ) . 
(6.25) 
This is an explicit equation for the affine tangent space at c = (a, /(a)) G Y. 
When the meaning is clear from the context, we will omit the distinction between 
these two tangent spaces and call them both the tangent space at c G T. Note that 
ATC = c + Tc. 
Remarks 6.2.3 Graph and tangent space as diffeomorphic images of A and X. 
Let T be the graph of a function f : A ^ Y. Let F(x, y) = (x, y + /(x)) be the 

GRAPHS 
239 
graph diffeomorphism induced by / as defined in Definition 6.1.9. Consider A as a 
subset of X x Y ~ X © Y. Then we see that T = F(A) is the image of A under the 
diffeomorphism F. Similarly, the tangent space T(a j(a)) is the image of X under 
the graph diffeomorphism I x F - t l x F induced by /'(a) : X —+ Y. The affine 
tangent space AT(a y(a)) is the image of X under the graph diffeomorphism induced 
by the affine function that takes x to /(a) + /'(a)(x — a). 
Remarks 6.2.4 Dependence on the coordinate system. The definition of the tan-
gent space given above is in terms of a coordinate system. Different coordinate 
systems for a graph actually give the same tangent space. This is noted in Remarks 
6.3.23 below. A direct proof of this independence is now possible, but later arguments 
give a more systematic approach. 
Definition 6.2.5 Normal spaces. The orthogonal complement Nc of the tangent 
space Tc is called the normal space. Again, one may distinguish between the linear 
normal space Nc and the affine normal space ANC = c + Nc. 
Y 
ANc=c + Nc 
Figure 6.3. Graph with tangent and normal spaces. 

240 
DIFFEOMORPHISMS AND MANIFOLDS 
Theorem 6.2.6 Equations of normal spaces. Let Y be the graph of a C1 function 
y = /(x) in the coordinate system (X, Y). Then 
x = -/'(a)*y andx = a - /'(a)*(y - /(a)) 
are explicit equations for the linear normal space and for the affine normal space 
ofT at the point c = (a, /(a)) G V and in the coordinate system (Y, X). Here 
f'(a)*:Y->X 
is the adjoint off (a) 
:X^Y. 
Proof. We have (x, y) G Na if and only if (x, y) J_ Ta. Vectors in Ta are of the 
form (u, /'(a)u), u E X. Hence (x, y) G Na if and only if 
<(x, y), (u,/'(a)u)) z 
= 
(x, u)x + <y, /'(a)u)y 
(6.26) 
= 
(x, u)x + </'(a)*y, u)x 
(6.27) 
= 
(x + /'(a)*y, u)x = 0 
(6.28) 
for all u G X. This happens if and only if x = —/'(a)*y. This is an explicit 
equation of Nc in the coordinate system (Y, X). Then the equation of the affine 
normal space also follows. 
□ 
Scalar Equations 
Let (X, Y) be a coordinate system in Z with dim X = k, dim Y = £, and dim Z = 
k + i = n. Let x¿ : X —> R and y¡ : Y —* R be the coordinate functions with respect 
to some bases in X and in Y. Then a vectorial function y = /(x) is expressed in 
terms of I scalar functions of k variables as 
Vj = fj(xi, 
■■■, xk), 
j = 1, •.., L 
These are the explicit scalar equations of a fc-dimensional graph T in an n — 
(k + £)-dimensional space. The scalar equations of the affine tangent space at 
(d, ..., ak; &i, ..., be), where bj = fj{a1} ..., ak), are 
(% ~ bj) = ^2i=1(dfj/dxi)(ai> 
• • •' afe)(a;¿ - ai)-
The affine normal space at the same point is given as 
(XÍ - at) = ~2_^ i=i(9/j/ox¿)(ai, ..., ak){yó - bj). 
Examples of Graphs 
Example 6.2.7 Let T = { ((x, y, z), (x2 - yz, y2 - xz)) \ (x, y, z) G R3 }. Then 
T is a three-dimensional graph in R5. Here, Z = R5, X = R3, Y = R2, A = X, 

GRAPHS 
241 
and / : A —> Y is defined by /(x) = (x2 — yz, y2 — xz) for all x = (x, y, z) e 
Note that / : R3 ->■ R2. Also, 
/'(x) 
2x 
—z —y 
—z 2y 
—x 
Let a = (1,-1,2). Then the linear tangent space Tc to F at c = (a, /(a)) is the 
[ 2 — 2 
1 I 
graph of the function L(x) =/'(a)x. Now,/'(a) = 
, so that 
Fc 
= 
{ ( x , / ' ( a ) x ) | x e R 3 } 
= 
{ (x, (2x - 2y + z, -2a; - 2y - zj) | x = {x, y,z) &E3 } . 
Since /(a) = /(l, —1,2) = (3, —1), the affine tangent space to T at c is 
Ac 
= 
{(x,/(a) + / ' ( a ) ( x - a ) | x e R 3 } 
= 
{ (x, (2x - 2y + z - 3, -2x - 2y - ^ + 1)) | x € M3 } . 
The equations for the linear normal space and affine normal space in the coordinate 
system 
Here, 
p2 IB3\ of R are, respectively, 
x 
= 
-f'(a)*y 
and 
x 
- 
a _ / ' ( a ) * ( y _ / ( a ) ) . 
/'(a)* 
2 
- 2 
-2 
- 2 
1 
- 1 
In particular, the linear normal space to T at c is 
Nc = { (u, (2v - 2M, 2U + 2V,V-U))\W= 
(U, V) € R2 } . 
Similarly, the affine normal space to T at c is 
A¿ 
= 
{ ( u , a - / ' ( a r ( u - / ( a ) ) ) | u G R 2 } 
= 
{ (u, (-7 - 2v + 2u, 7 - 2u - 2v, -v + u)) | u e R2 } . 
Example 6.2.8 The set 
S = { (x, x2 + y + 2z) | x = (x, y, z) G R3 } 
is a surface in R4. The affine tangent space to S at P((l, 0,1), 3) is the set of all 
(x, u) in M4 given by 
u - 3 = (V/(1,0, l),x - (1,0,1)) = ((2,1,2), (x-l,y,z- 
1)) 

2 4 2 
DIFFEOMORPHISMS AND MANIFOLDS 
or 
u-3 
= 2(x-l) 
+ y + 2(z- 
1). 
The equation of the normal line to S at P is 
(x - l,y,z - 1) = -(y - 3)V/(1,0,1) = -(y - 3)(2,1,2). 
Problems 
6.14 
Let T be the graph of / : R3 -► R3, where /(x) = (x2y, y2z, z2x) for all 
x = (x, y, 2) G M3. Let a = (1, —1,1). Find the linear tangent space, the affine 
tangent space, the linear normal space, and the affine normal space to T at the point 
(a,/(a)). 
6.15 
Show that the linear tangent space to the graph of a function / : Rm —> Rn 
at (0, /(0)) is the same as the linear tangent space to the graph of y = /(x — a) at 
the point where x = a. 
6.16 
Let T be the graph of some / : R2 —> K2. Suppose that the linear tangent 
space r at the point where x = 0 is 
{((x,y),{2x 
- y,x + y)) \ (x,y) eR2 
} . 
If T : R2 —► R2 is defined by T{x, y) = {x2 + 2x, y2 + yx), find the linear tangent 
space to the graph of g = / ■ T at (0, g(0)). 
6.17 
Let T be the graph of / : Km ->• Rn. 
Let a, b be in Rm such that 
/'(b)*/'(a) = —I. Show that the linear tangent space to T at (a,/(a)) and the 
linear tangent space to T at (b, /(b) are orthogonal subspaces of R m + n. 
6.18 
Find the affine tangent space to the surface 
S = { (x, x + ey - z2) I x = (a;, y, z j e l 3 } 
at the point P((l, 0,2),-2). 
6.19 
The linear tangent space to the graph of a function / : Rn —> R at any point 
(a, /(a)) is a subspace of R™+1. Show that this subspace has dimension n. 
6.20 
If T is the graph of / : Rm -* Rfc, is it true that the dimension of the linear 
tangent space to T at a point (a, /(a)) is always ml 

MANIFOLDS IN PARAMETRIC REPRESENTATIONS 
243 
6.3 
MANIFOLDS IN PARAMETRIC REPRESENTATIONS 
We would like to use graphs to investigate surfaces and curves and other similar 
structures in Euclidean spaces. Our definition of graphs is too restrictive for this 
purpose. The unit circle 
{ (x, y) e K2 | x2 + y2 = 1 } , 
for example, is not a graph. In fact, there is no single equation that expresses one 
of these variables in terms of the others. One possibility is y = ±(1 — x2)1!2. 
This is not a function, however, as it does not determine y uniquely. One may try 
y = +(1 — a;2)1/2 to represent the upper half of the circle, but this does not give the 
points below the x-axis. Hence we must modify our definitions to include this and 
other important cases. Manifolds are defined for this purpose. They are the sets that 
agree with a graph in small neighborhoods. 
Definition 6.3.1 Manifolds. A set M in a Euclidean space Z is called a manifold 
if for each m e M there is an open set G and a graph F such that m £ G and 
G n M = G n r. If the graphs associated with the points of M are all of the same 
dimension k, then M is called a k-dimensional manifold. It is clear that a graph is 
also a manifold. 
In a more explicit form, a set M in Z is a manifold if for each point m e M there is 
an open set G, an orthogonal coordinate system (X, Y) for Z, an open set A C X, 
and a C1 function f : A->Y 
with its graph F such that m e G and G n M = G n I\ 
If this can be done at each m e M with a fc-dimensional X, then M is called a 
fc-dimensional manifold. 
Example 6.3.2 Let M be a manifold in a Euclidean space Z. Let A be an open set in 
Z such that B = Mil A ^ 0. Then B is a manifold. To see this, let m e B - MC) A. 
Then m e M, so there is some open set G and a graph T such that m e G and 
G n M = G n r . Hence, Go = A n G is an open set containing m and 
G0 n B = G0 n (M n A) = (A n G) n (M n A) = (G n M ) n A = r n A 
Since T is a graph and ^4 is open, it is easy to verify that F n A is also a graph. Hence, 
B is a manifold. 
Example 6.3.3 A sphere is a manifold. Let 
Mi = { (x, y, z) e R3 | x2 + y2 + z2 = 1, z > 0 } 
be the upper half of the unit sphere without the equator. Then Mi is the graph 
of z = +(1 — x2 — y2)1/2 defined on the open disc x2 + y2 < 1. The lower 

244 
DIFFEOMORPHISMS AND MANIFOLDS 
hemisphere is also a graph. To include the points on the equator, consider the graphs 
of x = +(1 - y2 - z2)1/2 
and x = -(1 - y2 - z2)1/2 
defined on y2 + z2 < 1. 
These four graphs still miss the two points (0, ±1, 0). They can be covered by the 
graphs of y = +(1 — x2 — z2)1/2 and y = —(1 — x2 — z2)1/2. 
Hence we use six 
graphs to include all points of the unit sphere. Implicit representations of manifolds 
will provide an easier way of showing that a sphere is a manifold. 
Definition 6.3.4 Curves and surfaces. A one-dimensional manifold is called a 
curve and an (n — 1)-dimensional manifold in an n-dimensional space is called a 
surface. We see that a circle is a curve in R2 and a sphere is a surface in R3. 
Example 6.3.5 Let C = { (t, t2, e*, 1 — i) \ t e R }. Then C is a one-dimensional 
graph in R4. In particular, C is a curve in R4. 
Let S = {(x, y, z, x2yz) \ x > 0, y > 1, z > 2 }. Then S is a three-dimensional 
manifold in R4. Hence, S is a surface in R4. 
Parametric Equations of Manifolds 
An explicit equation for a graph is a function that defines this graph. In general, 
there are three ways to characterize a manifold: by graphs, by parametric equations, 
and by implicit representation. Having dealt with graphs, we now turn to parametric 
equations; implicit representations are discussed in the next section. 
Parametric equations are familiar from cases such as the parametric equations of the 
unit circle: x = cos u, y = sin u. The omission of the domain of u is not accidental, 
as there are some difficulties to be resolved in connection with this domain. See the 
remarks in Remarks 6.3.10 below. 
We will assume without loss of generality that all spaces considered below are 
Euclidean spaces even if this is not explicitly stated. In particular, U and V denote 
two subspaces of W and X and Y two subspaces of Z. 
Lemma 6.3.6 Let H be an open set in W and $ : H —» Z a diffeomorphism. Let 
U be a subspace of W and C = U D H. Then each c G C has neighborhood E in 
U such that <&(E) is a graph in the coordinate system (X, Y), where X = <E>'(c)i7, 
and Y is any subspace of Z complementary to X. Also, there is an open set G C Z 
such that $(£■) = G D 3>(C). 
Proof. Let P : Z —> X and Q : Z —► Y be the coordinate projections. Then 
P ■ $ : C —> X is a C1 function, as a composition of C1 functions. Also, 
(P • *)'(c) = P$'(c) : U -► X 
(6.29) 

MANIFOLDS IN PARAMETRIC REPRESENTATIONS 
245 
by the chain rule. Note that P$'(c) : U —> X is an isomorphism. In fact, T = 
<&'(c) : W —> Z is an isomorphism as the derivative of a diffeomorphism. Then 
the restriction of T to a subspace U is an isomorphism between U and X = TU. 
Therefore, the inverse function theorem gives an open set E in U such that c s E c C 
and such that the restriction of P ■ $ : C —> X to i? is a diffeomorphism r\: E ^ 
X. 
We have 
$(u) = (P$(u), Q$(u)) = (r/(u), Q$(u)), u e E. 
Let r?(f?) = J4 be the range of r\ and ■& : A —► £■ the reverse diffeomorphism. If 
77(11) = x, then let u = i?(x). Hence, $(u) = (x, Q$(tf(x))) for all u e £. We 
see that f = Q •$■■&: A —> F is a composition of C1 functions. Hence, / : A —> Y 
is also a C1 function. Therefore &(E) is the graph of this function in the coordinate 
system(X, Y). Also, Hn(Ex 
V) is an open set in if. HenceG = 
$(Hn(ExV)) 
is an open set in Z containing m = $(c). We see that G n &(C) — 3>(E), since 
UDHn{ExV)=E. 
a 
Theorem 6.3.7 Parametric representations of manifolds. Let H be an open set in 
W and $ : H —> Z a diffeomorphism. Let U be a subspace of W and C = U <1 H. 
Then 5>(C) = M is a manifold in Z. 
Proof. Lemma 6.3.6 above shows that each m £ &(C) has a neighborhood G in Z 
such that G n $(C) is a graph. Hence M is a manifold. 
O 
Example 6.3.8 Let /(x) = {x5 - y5, x5 + y5) for all x = (x, y) <G R2. Then 
det/'(x) =det 
5x4 
—5y4 
5a;4 
5y4 
50x4y4. 
Let H be any open subset of K2 that does not contain any point x for which x = 0 
or y = 0. Then /'(x) is invertible for all x e H. Also, / is one-to-one on all of 
R2. In fact, x5 - y5 = x\ - y\ and x5 + y5 = x\ + y\ imply that x5 = x\. Then 
x = X\ and hence, y — y\. Therefore, / : H —> IR2 is a diffeomorphism. Hence, by 
Theorem 6.3.7, f(H) is a manifold in R2. 
Definition 6.3.9 Parametric equations. Let C be an open set in a Euclidean space 
U. A function z = <p(u), u e C, is called a parametric equation for a manifold if 
y : C —> Z is the restriction of a diffeomorphism $ : if —> Z to C = if n 1/'. Here 
if is an open set in a space W that contains U as a subspace. The diffeomorphism 
$ : if —> Z is referred to as an underlying diffeomorphism. 
Remarks 6.3.10 A difficulty with parametric equations. If z = <^(u), u e C, 
is a parametric equation, then we see that tp : C —> Z is a C1 function and its 
derivative y'(c) : U —> Z is a one-to-one linear map at every point c £ C. The 

246 
DIFFEOMORPHISMS AND MANIFOLDS 
converse is not true. A function with these properties is not necessarily a parametric 
equation. Let ip : C —> Z be such a function. It may not be easy to find out if 
z = f(u), u 6 C, is a parametric equation. Fortunately, this is not too important for 
our purposes. In differential calculus one is interested in the behavior of a function 
in the neighborhoods of a point. In this case, Theorem 6.3.12 shows that any point 
c s C has a neighborhood C\ C C such that <p restricted to C\ is a parametric 
equation z = </?(u), u € C\. Hence p(Ci) is a manifold even though <p{C) might 
not be a manifold. To refer to this type of case, it is useful to define local parametric 
equations. 
Definition 6.3.11 Local parametric equations. Let C be an open set in a Euclidean 
space U. A function z = <p(u), u 6 C, is called a local parametric equation if every 
c e C has a neighborhood C\ C C such that z = <p(u), u € C\, is a parametric 
equation for a manifold Mi. There may not be a single manifold M that contains all 
such M\ as subsets. The following is an important result. 
Theorem 6.3.12 Let C be an open set in U and ip : C —> Z aG1 function such that 
f'(c) 
: U —> Z is a one-to-one linear map at every point c £ C. Then z = <¿>(u), 
Z G C , is a local parametric equation. 
Proof. Let V be a space with dim V = dim Z — dim U and W = U xV. Let c e C 
and X = if'(c)U. Since <p'(c) : [/ —> Z one-to-one, we see that dimX = dim{7. 
Let y = X ± . Then dim y = dimV. Let S : V —» y be an isomorphism, 
and as usual let P and Q be orthogonal projections onto X and y. 
For each 
(u, v) e C x V C W, let 
$(u, v) = (P<^(u), <9<jp(u) + Sv) G X x y = Z. 
This defines a C1 function & : H —* Z, where if = C x V is an open set in W. Its 
derivative $'(c, 0) : VF —> Z at (c, 0) € if is given as 
$'(c,0)(u,v) 
= 
(iy(c)u,Qy,'(c)(u) + Sv) 
= 
(iV(c)u, Sv), for all (u, v) e W. 
To obtain the second equality, note that </>'(c)u G X by the definition of X. Hence 
Qip'(c)(u) = 0. We see that $'(c, 0) : W —> Z is invertible. Hence, by the inverse 
function theorem, there is an open set Hi cW such that (c, 0) € Hi C H and such 
that the restriction of $ to ifi is a diffeomorphism <i>i : H —> Z. Let Ci = U CiHi. 
Then, by Theorem 6.3.7, <p : C\ —> Z is a parametric equation for the manifold 
M 1 = * 1 ( C i ) 
ü 

MANIFOLDS IN PARAMETRIC REPRESENTATIONS 
247 
Example 6.3.13 Let C = (-2, 2 + (3/2)TT) C R and let Z = R2 be the xy-plane. 
Define <p : C —> Z by 
( {u, 0) 
if - 2 < u < 0, 
ipiu) = < (siriw, 1 — cosu) 
if 0 < u < (3/2)n, 
{ ( - 1 , 1 + (3/2)TT - M) if (3/2)?r < u < 2 + (3/2)TT. 
Then </? : C —* R2 is a C1 function with a one-to-one derivative at every point. 
Hence, y is a local parametric equation. But <p(C) is not a manifold and tp is not the 
parametric equation of any manifold. This is clear from Figure 6.4, as <p{C) cannot 
be a manifold in any neighborhood of ( — 1, 0). One may think that this is due to the 
fact that ip is a not a one-to-one function. Also consider Co = (—2, l + (3/2)7r) C M. 
and 
f (it, 0) 
if - 2 < u < 0, 
(fo(u) = < (sin«, 1 — cosu) 
if 0 < u < (3/2)ir, 
{ ( - 1 , 1 + (3/2)?r - u) 
if 
(3/2)TT < a < 1 + (3/2)TT. 
We see that <p0 : Co —> M2 is a one-to-one local parametric equation but still is not a 
parametric equation. Problem 6.27 gives a sufficient condition for a local parametric 
equation to be also a parametric equation. 
y\ 
(-1, -1) 
(-2, 0) 
(-1. 
> 
y 
0) 
X 
Figure 6.4. Local parametric equations in Example 6.3.13. 
Example 6.3.14 Let g : C —> R3 be defined by g(u) = (u2 + vw, v4 — w, w3), for 
all u = (u, v, w) € C, where 
c = { u e i 3 | u / o , » / o ! i i ) / o } . 
Then z = g(u), u e C is a local parametric equation. 

248 
DIFFEOMORPHISMS AND MANIFOLDS 
Example 6.3.15 Explicit and parametric equation. Let Y be the graph of a G1 
function / : A —> Y in the coordinate system (X, Y). Then Y = M is a manifold. 
The explicit equation y = /(x) for Y defines a natural parametric equation for M. 
In fact, let H = A x Y and let $ : H —> Z be the graph diffeomorphism induced by 
<p, as defined in Definition 6.1.9. Hence 
*(z) = *(x, y) = (x, y + /(x)), z = (x, y) e if. 
The restriction of <3> to A = H n X is a parametric equation 
V(x) = (x, / ( x ) ) , x e AforT. 
Compatibility of Two Parametric Representations 
Many concepts and operations related to manifolds are defined in terms of parametric 
equations. Clearly, we must show that such definitions are independent of the 
parametric equation used. Hence we develop some notations and relations about 
different parametric representations that agree on a part of a manifold. 
Notations 6.3.16 Different parametric equations. Let Hi be open sets in W¿, 
¿ = 1,2, and $¿ : Hi —> Z diffeomorphisms, G¿ = <¡>¿(ií¿). Let Ui be a subspace 
in Wi, and C» = HiCi Ui. Assume that <&(Ci) n $ 2 ^ ) = M is not empty. Then 
G\ n G2 is not empty either, as it contains M. Without loss of generality, we will 
assume that Gi = G2. If originally this is not the case, then we let G = G\ D G2 and 
replace Hi by H\ = <§>~l (G) C ií¿ and $¿ by the restriction of $¿ to ii/. Hence we 
assume that 
<í>¿ : Hi -^ Z are two diffeomorphisms; 
(6.30) 
$!(#!) = $2(if2) = G; and 
(6.31) 
$j(Gi) = $2(G2) = M, where 
(6.32) 
Ci = UinHi. 
(6.33) 
Definition 6.3.17 Equivalent parametric equations. Let <p¿ : G¿ —> Z be two 
parametric equations. Let $¿ : íf¿ —> Z be the underlying diffeomorphisms. Call 
yi s equivalent if <I>¿ s are equivalent in the sense that they satisfy the conditions 
(6.30) - (6.33) in Notations 6.3.16. Arguments above in Notations 6.3.16 show that if 
Vi(Ci) ^ V2(G2) = M is not empty, then there is no loss of generality in assuming 
that they are equivalent. 
Lemma 6.3.18 Diffeomorphisms for equivalent parametric equations. Let ipi : 
Ci —> Z be two equivalent parametric equations. Then there is a diffeomorphism 
$ : C\ —> G2 such that <pi — >fi2 ■ #• 

MANIFOLDS IN PARAMETRIC REPRESENTATIONS 
2 4 9 
Proof. Use the underlying diffeomorphisms $¿ : Hi —> G to define 
6 = i " 1 • $! : Jix -► H2. 
We see that 0(Ci) = C2 and that $i(ci)i7i = U2 for all cx G Cx. Let 
0 = 0| C l : C1 -> ff2 
be the restriction of 0 to Ci, We verify easily that $(Ci) = C2 and i? : C\ —> t72 is 
a diffeomorphism. It is clear that ¡pi = </?2 • ■&. D 
Remarks 6.3.19 One can define # : Ci —> C2 directly as i?(ui) = (¿^(^(ui)), 
without any reference to the underlying diffeomorphisms. This expression is defined 
since ipi : C¿ —> M are both one-to-one functions that map C¿ onto M. It is not 
clear, however, if d : C\ —> C2 is a diffeomorphism of Ci onto C2. 
Lemma 6.3.20 Let pi : Ci —> Z be two equivalent parametric equations. If 
</>i(ci) = ^2(c2) = m, then <¿>i(ci)í/i = <//2(c2)L/2. 
Proof. Let i9 : C\ —► C2 be the diffeomorphism obtained in Lemma 6.3.18. Then 
p{c\) = (¿?2($(ci)) = m shows that #(ci) = c2. Therefore 
Vj'icOCIi = ¥>2(c2)0'(ci)E/i = V2(c2)^2. 
The first step follows from the chain rule. To obtain the second step, one observes 
that d'(ci)Ui = U2, since tf'(ci) :UX ^U2 
is invertible. 
□ 
Remarks 6.3.21 A diffeomorphism for equivalent equations. In the proof of 
Lemma 6.3.20 above, one essentially uses only d = 0|ci 
: C\ —> C2. This can 
be defined without any reference to the underlying diffeomorphisms, as i?(ui) = 
<^2^1((pi(ci)). This is defined since both tpi : Ci —> M are one-to-one functions that 
map Ci onto M. Hence, m = pi (ui) G M and y^ 1 (m) G C2 exists. It is not clear, 
however, if i? : C^ —> C2 is a diffeomorphism of C\ onto C2. By referring to the 
diffeomorphisms <I>¿, we see that this is indeed the case. 
Tangent Spaces of Manifolds 
Definition 6.3.22 Tangent spaces. Let M be a manifold in Z and m G M. Let 
V? : C —> Z, C c Í7, be a parametric equation for M with m G <p(C). Then 
the linear tangent space of M at m is the subspace T(m) = <¿>'(c)¡7 in Z. Note 
that if <E> : ii —> Z is the underlying diffeomorphism for <p : C —> Z, then also 
$'(c)C/ = T(m). A parametric equation for T(m) is z = ip'(c)u, u £ U. Lemma 
6.3.20 shows that T(m) is independent of the parametric equation used. 

250 
DIFFEOMORPHISMS AND MANIFOLDS 
Remarks 6.3.23 Agreement with the earlier definition. The tangent space of a 
graph was defined earlier in Definition 6.2.2. In fact, if a graph T is given by the 
explicit equation y = /(x), x g y l , then 
z = <¿>(x) = (x, /(x)), x G A 
is a parametric equation for T, as observed in Example 6.3.15. Then the tangent 
space T(m) at m = <¿>(a) has the parametric equations 
z = <¿/(a)x = (x, /'(a)x), X É I 
Therefore an explicit equation for T(m) is y = /'(a)x. 
Definition 6.3.24 Normal spaces. Let M be a manifold in Z and m G M. The 
normal space N(m) at m is the orthogonal complement of the tangent space T(m). 
Hence 7V(m) = T(m)J-. 
Some Properties of Tangent Spaces 
Theorem 6.3.25 Tangent spaces and graphs. Let M be a manifold and let X = 
T(m) be the tangent space of M at m G M. Then m has a neighborhood G such 
that M C\G is a graph in the coordinate system (X, Y), where Y is any subspace of 
Z complementary to X. 
Proof. Let $ : H —> Z be a diffeomorphism such that 
m = $(c) 6 ®{H nU)c 
M. 
Lemma 6.3.6 shows that m G M has a neighborhood G in Z such that M (1 G is a 
graph in (X, Y) where X = $'(c)C/ = T(m), and F is any complementary space 
toX. 
D 
Definition 6.3.26 Curves and their tangent vectors. Let Z be a vector space and / 
an open interval. A curve C in Z is a continuously differentiable function r : I —> Z 
with a nonzero derivative at every point Í £ 1. If M is a manifold in Z and if 
r(I) C M, then C is a cwrve on ?«e manifold M. If z G Z and if r(f) = z for some 
í G /, then C is a curve passing through z. The vector 
r'(i) = lim^ 0(l/s)(r(i + s) - r(i)) G Z 
is the tangent vector ofC at the point r(i) = z. 

MANIFOLDS IN PARAMETRIC REPRESENTATIONS 
251 
Theorem 6.3.27 Tangent vectors of curves on a manifold. The tangent space 
T(m) of a manifold M is the set of tangent vectors of the curves on M passing 
through m. More explicitly, p G T(m) if and only if there is a curve r : / —> M on 
M and at & I such that r(i) = m and r'(t) = p. 
Proof. Let <p : C —> Z be a parametric equation with the underlying diffeomorphism 
$ : H ~+ Z, C — H nU, and the inverse diffeomorphism ¡¡> : G -+ W, where 
G = ${H). Let m = ip(c) G M. Let z = r(£), t G /, be a curve on M passing 
through m = r(io). Then s(f) = í*(r(í)) is a curve in C C U passing through 
c = v¡/(r(í0)) e C. Then s'(í0) G Í7 since s(i) e C/ for all í e /, and (since 
(p ■ s(i) = r(i) for t £ I) 
r'(io) = (p • s)'(ío)) = ^'(s(ío))s'(ío) = v'(c)s'(í0) G T(m). 
Conversely, if p G T(ra), then q = í»'(m)p G U. Hence 
s(i) =tq,te 
I = {-e, e), 
is a curve in C for some e > 0. Then r(í) = <p(s(í)), í G /, is a curve on M passing 
through m = r(0). Also, 
r'(0) = {if ■ s)'(0) = ^'(c)s'(O) = ^'(c)q = p. D 
Problems 
6.21 
Let M = { (x, (x2 + y - z, y2 + x + z)) | x = (x, y, z) G R3 }. Is M a 
manifold in R5? If it is, find a parametric equation for M and an underlying 
diffeomorphism. 
6.22 
Let k 7^ 0 be a real number. Let C = { x = (x, y) G R2 | xy ^ 0 }. Define 
/ : C -> R3 by /(x) = (xy, a;2 + y2, kx) for all x G M2. Show that / is a local 
parametric equation for a manifold. 
6.23 
Let M c R" be an r-manifold and let N c Rm be an s-manifold. Show that 
M x TV c Rm x Rn = R m + n is an (r + s)-manifold. 
6.24 
Show that a compact manifold cannot be represented by a (single) parametric 
equation. 
6.25 
Let C be an open subset of U and i ^ : C - » 2 a one-to-one C1 function with a 
one-to-one derivative at every point. Example 6.3.13 shows that <p(C) does not have 
to be a manifold. Let Co be an open subset of C with closure Co C C. Show that 
ip(Co) is a manifold. 

252 
DIFFEOMORPHISMS AND MANIFOLDS 
6.26 
Let C be an open subset of U and ip : C ^ Z a one-to-one C1 function with 
a one-to-one derivative at every point. Let a G C and u G U. Let L c K b e the set 
of t G K such that a + i u e C . Define a : L -> Z by a(£) = ^3(a + tu), t € L. 
Show that if 99(C) is a manifold in Z then a(L) is also a manifold in Z. 
6.27 
Let C be an open subset of U and -p : C ^ Z & one-to-one C1 function with a 
one-to-one derivative at every point. Assume that the inverse function tp '■ ¥>(C) —> C 
is continuous. Show that <p(C) is a manifold in Z. 
6.4 
MANIFOLDS IN IMPLICIT REPRESENTATIONS 
Implicit representations of manifolds may be considered as the reverse of parametric 
representations. In the proof of Theorem 6.4.1 below, the inverse function theorem 
is used to reduce implicit representation to parametric representations. 
Theorem 6.4.1 Implicit representations of manifolds. Let dim V < dim Z. Let 
E be an open set in Z and let F : E —> V be a C1 function. Then 
M={z\zeE, 
F(z) = 0, F'(z)Z = V} 
is a manifold in Z. The tangent space of M at m G M is 
T(m) =KerF ,(m). 
Proof. Let m e M. Let X = Ker F'{m) and Y = X
A-. Let P : Z -► X and 
Q : Z —> Y be the coordinate projections. We see that dim V = dim Y, since 
F'(m) : Z -> V\s onto V. Let U be a space with dim U - dim X and 5 : X —> £/ 
an isomorphism. Define d : E —> W = U x V by 
T?(Z) 
= 
(5Pz, F(z)) = (5x, F(x, y)). Hence 
0'(m)(x,y) 
= 
(5x, F'(m)(x, y)) = (Sx, F'(m)y). 
Then i?'(m) : Z —> W7 is an isomorphism since 5X = Í7 and F'(m)F = V. Also, 
1? : B —> W is a C1 function because both components are C1 functions. The inverse 
function theorem shows that there is an open set G in Z such that m E G C E and 
such that the restriction of 1? to G is a diffeomorphism 
-d\G = ^ : G ^ W. 
Let *(G) = H. Note that * : G -* W maps MnGtoUnH, 
since F(z) = 0 for 
z G M. Hence the reverse diffeomorphism $ : H —> G maps C = {/ n ii to M n G. 
Therefore AÍ n G is a manifold with a parametric equation <3?|c = V3 : C1 -^ Z. Then 

MANIFOLDS IN IMPLICIT REPRESENTATIONS 
253 
M is a manifold by Definition 6.3.1. We see that $ maps U to X = Ker F'(m). 
Hence X is the tangent space of M at m G M. 
□ 
Example 6.4.2 Let E = { x - (x, y, u, w) e M4 | x ^ u }. Let M be the set of all 
x £ E such that 
x 
= 
yu — v 
u 
= 
xv. 
Then M is a manifold in R4. To see this, define 
F(x) = (x — yu + v, xv — u) 
for all x = (x, y, u, v) € E. 
Then 
F'(x) 
1 
—u —y 1 
v 
0 
—1 x 
Since x T^ w, it follows that the matrix for F' (x) has two linearly independent vectors. 
Hence, F'(x) : E4 -> M2 is onto for all x e £ So, by Theorem 6.4.1, 
M = { x e £ 
F(x) = 0 } = { x e E | F(x) = 0, 
F'(x)M2 = K2 } 
is a manifold in M4. 
Also, the tangent space to M at m = (0,1,1,1) G M is Ker F'(m), which is the 
set of all (a, 6, c, d) G M4 such that F'(0,1,1,1)(a, 6, c, c£) = 0. That is, it is the set 
of all solutions (a, b, c, d) to the system 
a — b — c + d = 
0 
a — c = 
0. 
Hence, 
T'(0, l , l , l ) = { ( í , s , í , s ) | í G R , s e E } . 
Definition 6.4.3 Implicit equations. Let E be a set and V a vector space. Let 
F : E —> V be a function. Then F(e) = 0 is called the implicit equation of the set 
S* = F- 1({0}). Hence eG 5 if and only if F(e) = 0. 
Theorem 6.4.4 Implicit equations of manifolds. Let E be an open set in Z and 
F : E —> V a C1 function. Assume that F'(z)Z 
= V whenever F(z) = 0. Then 
-F(z) = 0 is an implicit equation of a manifold M C E. 
Proof. This is a reformulation of the first part of Theorem 6.4.1. 
D 

254 
D1FFE0M0RPHISMS AND MANIFOLDS 
Theorem 6.4.5 Implicit equations of tangent spaces. Let F{z) = Obe the implicit 
equation of a manifold M. Then F'(m)z — 0 is an implicit equation of the tangent 
space ofMatm 
G M. 
Proof. This is a reformulation of the second part of Theorem 6.4.1. 
O 
Another version of these results is known as the implicit function theorem. 
Theorem 6.4.6 Implicit function theorem. Let EcZ 
— XxYbean 
open set in 
Z. Let (a, b) G E. Let V be a space with dim V = dim Y. Let F : E —> V be a C1 
function with F(a, b) = 0 and F'(SL, b)Y = V. Then there is an open set A in X 
and a C1 function f : A —> Y such that a £ A, /(a) = b, and F(x, /(x)) = Ofor 
all x G A. 
Proof. Theorems 6.4.4 and 6.4.5 show that F(x, y) = O is an implicit equation for 
a manifold M and that the tangent space of M at (a, b) € M is 
X = T(m) = Ker F'(a, b). 
Define t/> : E —> L(Y, V) by xp(e) = F'(e)\Y, 
the linear map F'(e) restricted to Y. 
That is, xp(e) : Y -> V is defined by ip(e){y) = F'(e)(y) for all y e Y. Since F is 
a C1 function, i/; is continuous. Also, tp{a., b) is invertible because dim Y = dim V 
and ip(a,b)Y 
= F'(a,b)Y 
= V. Because inversion is continuous, it follows 
that there is an open set U containing (a, b) with U C G and such that tp(z) is 
invertible for all z G U. Thus, F'(z)Y = V for all z € U. This implies that Y is 
complementary to X. Then Theorem 6.3.25 shows that (a, b) has a neighborhood 
G such that M n G is a graph in (X, Y). But X is also complementary to Y since 
(X, Y) is a coordinate system in Z. Then, we see easily that M O G is also a graph 
in (X, Y). (cf. Problem 3.42). This is the conclusion of the theorem. 
□ 
Example 6.4.7 Let F : 
I2 be defined by 
F(x, u) = (xv + yu— l,xy — uv) 
for all x = (x,y) G M2, u = (u, w) £ 
Then 
-F'(X,T/,U,U) 
t; 
iz 
y 
x 
y x 
—v —u 
Let c = (1,0,0,1) G R2 x K2. Then F(c) = (0,0) and 
F'(c) = 
Hence, 
F'(c)({ (0,0, u, v) \(u,v)eR2} 
= { (v, -u) | (u, v) G M2 } = R2. 
1 0 
0 1 
0 
1 - 1 0 

MANIFOLDS IN IMPLICIT REPRESENTATIONS 
255 
In the notation of Theorem 6.4.6 
F'(c)(Y) 
= V. 
Hence, by that theorem, there is an open set A in M2 containing (1,0) and a continu-
ously differentiable function / : A —> R2 such that 
F(x, /(x)) = 0 for all x G A. 
That is, the equation F(x, u) = 0 defines u implicitly as a differentiable function of 
x in the neighborhood A of (1,0). In particular, the system 
xv + yu — 1 = 
0 
xy — uv 
= 
0 
can be solved for (u, v) as a differentiable function of x and y for (x,y) in a 
neighborhood of (1,0). 
Example 6.4.8 Let E c Rra+m be open and let F : E -> Km have continuously 
differentiable component functions Fi,..., F m. Suppose that we denote a point 
x g M"by (xi,.. .,xn) and a point y G Km by (yi,... ,ym) and a point z G K n + m 
by z = (a,b), where a E l " , b £ IRm. Assume that (a,b) G E. If F(a,b) = 0 
and 
det 
dFi 
dy\ 
dFm 
dyi 
(a,b) 
(a,b) 
dym 
dFm 
dym 
(a,b) 
(a,b) 
7^0, 
(6.34) 
then the conclusion of Theorem 6.4.6 holds: there is an open set A in W1 and a C1 
function / : A -> Rm such that a e A, /(a) = b, and F(x, /(x)) = 0 for all 
x G A. This is because the last m columns of F'(a, b) are 
7T— (a, b) 
dyi 
BF 
dyi 
(a,b) 
dF\_ 
dym 
dFm 
dym 
(a,b) 
(a,b) 
and (6.34) implies that these columns form a linearly independent set of m vectors, 
whence F'(a, 1 
As an illustration, consider the system 
x u + yv 
Q 
2 
xu — y v 
7 
11. 

2 5 6 
DIFFEOMORPHISMS AND MANIFOLDS 
Let /i (x, u) = x2u + yv - 7, /2(x, u) = 
(u, v). Then at a point (x, y, u, v), we have 
i « 3 — y2v — 11 for all x = 
(x,y),u 
det 
dfi/du 
dfi/dv 
df2/du 
df2/dv 
det 
3u2x 
-y 
x y — 3u xy. 
Thus, at (1,1, —1, —1), the above determinant is —4 ^ 0. So, whenever (u, v) is 
near the point (— 1, — 1), it can be solved uniquely in terms of x, y in a neighborhood 
of (1,1). 
Example 6.4.9 The conclusion of the implicit function theorem may hold even if the 
hypotheses of the theorem are not satisfied. Let F(x, y, z) = (xA — 16z4,y — 2z) 
for all (x, y, z) G Rd so that F : 
F'{x,y,z) = 
I2 ->• R2. Then 
4x3 
0 
-64z 3 
0 
1 
- 2 
Let c = (0,0,0). Then F(c) = 0 and F'(c) 
0 
0 
0 
0 
1 
- 2 
Hence, F'(c) 
does not map Y = M2 onto V = R2. Let f(x) = (x,x/2) 
for all x e R. Then 
/ : R —> M2 is continuously differentiable and F(x, f(x)) — F(x, x, x/2) = 0 for 
all x G R. 
Implicit Functions and Jacobian Matrices 
The implicit function theorem states a very plausible fact, even though it has a rather 
involved proof. Let X = Rm and Y = V = Rn. Then the equation F{z) = 0 can 
be expressed as 
F^x i) • •' 
•Erm V\: 
■, Vn) 
0 
Fn(xi, ••• , x m; yi, ..., yn) 
= 
0. 
These are n equations. We would like to solve them for the n unknowns yj and 
express each yj in terms of the m variables Xi. If these equations were linear 
equations, then they could be expressed as 
A x + B y = 0. 
Here A is an n x m and B is an n x n matrix, and x, y are, respectively, m x 1 and 
n x l matrices, or column-vectors. We know that if B is an invertible matrix, then 
these equations can be solved for yj. In fact, y = — B _ 1 Ax. 

MANIFOLDS IN IMPLICIT REPRESENTATIONS 
257 
In the general case, we have an equation of the form F(x, y) = 0. We are given 
that c = (a, b) is a solution. We would like to solve this equation for all the values 
of x close to a and obtain y = /(x) as a function of x. First, we try to do this 
approximately. We replace F(x, y) by its first-order Taylor polynomial 
P(c)(z) = F(c) + F'(c)(z - c) = F'(c)(z - c) 
at c = (a, b). Then we obtain the linear equation F'(c)(z — c) = 0 or 
F'(a, b)(x - a) + F'(a, b)(y - b) = 0. 
To express this equation in terms of scalar equations, we form the Jacobian matrices 
A = {AJÍ} and B = {Bjk} with the components 
OF- 
dF 
A,, = ^ ( a , b ) a n d B , f c = ^ ( a , b ) . 
Then we obtain A (x — a) + B (y — b) = 0. This equation can be solved if B is 
invertible. We see that the invertibility of B means that if the derivative 
F'(a, b) : (X x Y) -» V 
is restricted to vectors in Y, then it becomes an invertible mapping Y —> V. The 
implicit function theorem shows that if the approximate linear system of equations 
can be solved, then the original system of equations can be also solved. 
Component Forms 
Remarks 6.4.10 A summary of component forms. Let dim Z = n and dim V = 
m < n. Choose an orthonormal basis (ei, ..., em) for V and express F : Z —> V 
in terms of its components F¿ : Z —> M. so that 
F(z) = Fi(z) e i + ■ • • + F m(z)e m. 
This amounts to replacing V by Mm. Hence we assume that V = Rm and let 
F(z) = (F1(z), 
. . . , F m ( z ) ) e K m . 
Then the derivative F'(a) : Z —» Km is expressed as 
F'(a)z = ((VFi(a), z), ..., (VFm(a), z» 
for all z G Z. The condition that F'(a)Z = V is equivalent to saying that 
{VFi(a), ..., VF m(a)} 

2 5 8 
DIFFEOMORPHISMS AND MANIFOLDS 
is linearly independent and has m distinct elements. The vectorial equation F' (a)z = 
0 for the tangent space T(a) becomes m scalar equations 
(VF 1(a),z)=0, ..., (VFm(a), z) = 0. 
Hence T(a) is the orthogonal complement of the space spanned by the vectors 
{VF!(a), ..., VF m(a)}. 
The space spanned by these vectors is then the normal space iV(a) = T(a) x. 
Example 6.4.11 The unit sphere in R™. The unit sphere S 
X^ ~r ■ ■ • ~r Xn 
— 1 
is a manifold in R". In fact, define h : R™ -► R = V by 
/i(x) = x\-\ 
1- x2
n - 1, x = (xi, ..., xn) G R™. 
Then S is given by a single scalar equation h(x) = 0. This is the equation of a 
manifold if the gradient vectors of the components of h are linearly independent. In 
this case, there is only one component. The linear independence of a set of one vector 
V/¿ means that V/t 7^ 0. This is indeed the case, since 
V/i(a) = 2( a i, . . . , a „ ) ^ 0 
for all a G F (where we may take E = Rn \ {0}). Therefore ft(x) = 0 defines a 
manifold S in R™. The normal space N(a) is the one-dimensional space spanned by 
V/i(a) = 2(ai, ..., on). The tangent space T( a) = AT (a)1- is the set of all x G Rn 
such that 
d\xi + ■ ■ ■ + anxn = 0. 
Similarly, the equation of the affine tangent space AT (a.) is 
ai(xi — ai) + • • • + an{xn — an) 
= 
0, or, equivalently, 
a\X\ + ■ ■ ■ + anxn 
= 
1. 
A 
Problems 
6.28 
Let F : A —> Rm, where A is an open set in R" and F is of class C1 on A. 
Let M be the set of all x G A such that dim (Range -F'(x)) = rn. Show that if 
M / l , then M is an (n — m)-dimensional manifold. 

MANIFOLDS IN IMPLICIT REPRESENTATIONS 
259 
6.29 
Let F : R3 -> R2 be given by F(x, y, z) = (y2 + z, z + x) for all x = 
(x, y, z) e R3. Show that 
M = { (x, y, Z ) £ l 3 | F(x) = (0, 0) } 
is a manifold in R3. Find the tangent space ofM a t m = (1,1,-1). Also, find the 
normal space of M at the same point m. 
6.30 
Let F : R™ -> R be of class C1. Suppose that c G R and 
ß c = { x e E n |F(x) = c } ^ 0 . 
Assume further that whenever x G Bc, we have F'(x) ^ 0. Show that Bc is an 
(n - 1)-manifold in E n. 
6.31 
Consider the system 
x2 + ~y2 + z3 - z2 
= 
-
x3 +y3 -3y + z 
= 
- 3 . 
Can we solve for y and z as a function of x for (x, y, z) in a neighborhood of 
(-1,1,0)? 
6.32 
Suppose that F : R n + m —► Rm is a continuously differentiable function with 
F(a, b) = 0 and 
d e / g ( F 1 , . . . , F ) 
A 
Q 
\d(yu 
...,ym) 
) 
Let A be a neighborhood of a in Rn and let / be a C1 function / : A —> Rm such 
that /(a) = b, and F(x, /(x)) = 0 for all x G A Show that 
T f , , 
/(?(F1; . . . , F m ) 
A " 1 
a(F!, . . . , F m ) 
J / ( a ) = - 1-^7 
r(a, b) 
•— 
-(a, b). 
\d(yi, 
...,ym) 
) 
d(xi,...,xn) 
6.33 
Show that the system 
xv + yu 
= 
1 
xy 
= 
uv 
defines (u, v) = h(x, y) implicitly as a function of (x, y) for (x, y) in a neighbor-
hood of (1,0). Compute fc'(l, 0). 

260 
DIFFEOMORPHISMS AND MANIFOLDS 
6.5 
DIFFERENTIATION ON MANIFOLDS 
Let M be a manifold in a Euclidean space Z. Let Y be another Euclidean space 
and / : M —> Y a function. We shall define a concept of differentiation for such 
functions. The earlier definition of differentiation given in Definition 5.2.1 does not 
apply, as the domain of / may not be an open set in any vector space. 
Definition 6.5.1 Derivatives of functions on manifolds. Let p : C —> Z be a 
parametric equation for a manifold M and m = </J(C). Then a function f : M —* Y 
is said to be differentiable at m € M if g = f ■ (p : C —> Y is differentiable 
at c £ C. In this case, the derivative of / : M —> Y at m G M is defined as 
/'(m) = 
S ' ( c ) y ( c ) - 1 e L ( r ( m ) , n 
Note that p'(c) : U —> Z is not invertible. But p'(c) is invertible as a linear map 
between U and the tangent space T(m) = ip'(c)U. Hence /'(m) : T(m) —> Y is 
well-defined. Nevertheless, our definition still has to be justified. Specifically, we 
need to show that the differentiability of / is independent of the parametric equation 
p used. 
Lemma 6.5.2 Letpi : d —> M be two equivalent parametric equations for M. Let 
<Pi(ci) = m G M. Then 
<7i = / • p\ : C\ —> Y is differentiable at C\ if and only if 
9i — f ' </>2 : Ci —> Y is differentiable at c2. 
If both are differentiable, then /'(m) = g[ (cj) ■ ^ ( c i ) - 1 = ff2(c2) " V2(c2)_1-
Proof. Let •& : C\ —> C2 be the diffeomorphism obtained in Lemma 6.3.18 so that 
Pi — '■Pi ■ $■ Note that c2 = i?(ci). Assume that <?2(c2) exists. We have 
5i 
= 
f ■ Pi = f ■ P2 ■ $ = 92 ■ ß and, therefore, 
g[(Cl) 
- 
(g2-0y(c1)=g'2(u(c1))0'(c1)=g2(c2)i)'(c1) 
by the chain rule. Hence g[(ci) also exists and gí(ci) = <;2(c2)?9'(ci). The other 
direction is similar. Now assume that the derivatives exist. Given p G T(m), let 
u, = ^(c¿) _ 1p. Thenp = ^ ( c ^ u i = {p2 ■i?)'(ci)u1 = <¿>2(c2)i?'(ci)ui shows 
that u2 = i?'(ci)ui. Therefore 
/'(m)p 
= 
g'tia) • <¿>-(c¿)_1p = t/í(c¿)u¿, and 
g'1(c1)u1 
= 
(g2 ■ tf)'(ci)ui = g'2(c2)tf''(ci)ci 
= 52(c2)u2. 
Hence /'(m) : T'(m) —•> F is independent of the parametrization. 
O 

DIFFERENTIATION ON MANIFOLDS 
261 
A Special Case 
Let M be a manifold in Z. 
In many cases of interest, a function / on M is 
the restriction of a differentiable function F : G —+ V defined on an open set 
G C Z containing M. In this case, differentiation on M is simple. The derivative 
/'(m) : T(m) -> V on M is obtained as the restriction of F'(m) : Z -> 1/ to the 
tangent space T(m). Hence/'(m) = F'(III)|M if/ = -F|M- We state this also as a 
lemma. 
Lemma 6.5.3 Let M be a manifold in Z and m G M. Let F : G —+ V be a 
differentiable function defined on an open set containing m. Define 
f:(MnG)^Vbyf(z)=F(z). 
Then /'(m) : T(m) —► Y exists and /'(m) : T(m) —> V is the restriction of 
F'(m) : Z -^V to T(m). 
Proof. Let </? : C —> 2 be a parametric equation for M such that <p(c) = m and 
(^(C) C G. Then g = f ■ tp = F ■ ip shows that g : A —> y is differentiable. Hence 
/'(m) : T(m) -» 7 exists. Also, if z € T(m), then 
/'(m)z = ^ ( a ) / ^ ) - ^ = F ' ^ Í C J J ^ Í C J V J ' Í C ) - ^ = F'(m)z. 
This shows that /'(m) is the restriction of ^'(m) to T(m). 
□ 
Derivatives Along Curves on Manifolds 
Let F : G —► F be a differentiable function defined on an open set G. Then F'(a)z 
is the directional derivative of F at a G G in the direction of z e Z. On manifolds 
one cannot, in general, take directional derivatives because the manifold will not 
contain straight lines running in every direction. Directional derivatives have to be 
replaced with derivatives along curves. Recall that a curve C on M is a C1 function 
r : I —* M defined on an open interval / c R . The derivative r'(a) at a 6 / is 
the tangent vector of C at r(a) = m G M, a vector in the space Z in which M is 
embedded. Theorem 6.3.27 shows that r'(a) G T(r(a)). So the tangent vector of a 
curve in M at m G M is in the tangent space T(m). If /'(m) : T(m) —» V exists 
for all m G M, then /'(r(a))r'(o) is defined for all curves r : I —* M. These are 
the derivatives along curves. 
Theorem 6.5.4 Derivatives and derivatives along curves. Let M be a manifold in 
Z and f : M —> V a differentiable function. Let r : / —> M be a curve on M and 
a£l.Let\ 
= f-r:I-^V. 
Then 
A » = lim / ( ' ( " +'))-/(*(<»)) 
(6.35) 
v ; 
t-*o 
t 

2 6 2 
DIFFEOMORPHISMS AND MANIFOLDS 
exists in V and X'(a) = /'(r(a))r'(a). 
Proof. Let ip : C —> Z be a parametric equation for AI and v?(u) = r(a) = m. Let 
s(a) = (/J^1 (r(a)) so that v?(s(a)) = r(a). Then 
r'(a) = (¿/(s(a))s'(a) and X = f-r 
= f-ip-s 
= g-s. 
Now / : M —> V is assumed to be differentiable. 
Hence g = f ■ ip is also 
differentiable by Definition 6.5.1. Therefore 
A » 
= 
g'(S(a))s'(a) 
= 
f'(p(S(a))W(S(a))S'(a)} 
= 
/'(r(a))r'(a). □ 
Local Extremal Values on Manifolds 
Let M be a manifold in X. Let p : M —> R be a real-valued function defined on M. 
A point mo G M is called a /oca/ maximum point for p if there is a 8 > 0 such that 
p(m) < p(mo) whenever ||m — mo|| < S and m G M. Local minimum points are 
defined similarly. 
Lemma 6.5.5 Let m G M be a local extremal point for p : M —> R. Assume that 
p'(m) : T(m) -> R exwtó. 7/ze« p'(m) = 0. 
Proof. Assume that there is a u G T(m) such that p'(m)u G R is not zero. Use 
Theorem 6.3.27 to find a curve r : / —► M on M such that r(ao) = m for an ao G / 
and such that r'(a0) = u. Define s : I —> R by s(í) = p(r(í)). Then Theorem 6.5.4 
shows that s'(ao) = p'(m)u 7^ 0. Therefore s : I —> R cannot have a local extremal 
point at ao G L Then m = r(ao) cannot be a local extremal point for p. 
D 
Recall that JV(m) = T(m)-1- is the normal space at m of M, the orthogonal com-
plement of the tangent space. 
Lemma 6.5.6 Let M C G C Z, where H is an open set. Let p : M —> K be the 
restriction of a differentiable function q : G ^ R fo M. Then p'(m) = 0 if and only 
if V<7(m) G N{m). 
Proof. In this case, p'(m) : T(m) —> R is just the restriction of q'(m) : Rn —> R to 
T(m), as we proved in Lemma 6.5.3. Hence p'(m) = 0 if and only if 
p'(m)u = q'(m)u = (Vg(m), u) = 0 
for all u G T(m). In this case Vg(m) 1 T(m) or V<?(m) G 7V(m). 
□ 
Finally, assume that V = Rm and F : G —+ Rm is given as m scalar functions 
F¿ : G -> R. Hence F = (Fj, ..., Fm) : G -> Rm. 

DIFFERENTIATION ON MANIFOLDS 
263 
Theorem 6.5.7 Lagrange multipliers. Let G be an open set in Z. Let M be a 
manifold in G given by the equations í¿(z) = 0, i = 1, ... m. Let q : G —> R be a 
dijferentiable function. Then the restriction ofq to M can have a local extremal point 
atm€M 
only ifVq(m) 
is a linear combination of the gradient vectors VF¿(m). 
Proof. Lemmas 6.5.5 and 6.5.6 show that the restriction of q : H —» R to M can have 
an extremal point at m only if Vq(m) s N(m). 
Then the result follows from the 
fact that JV(m) is spanned by the gradient vectors VFj(m), as observed in Remarks 
6.4.10. 
D 
Example 6.5.8 Let us compute the maximum and minimum values of x3 + y3 + z3, 
where x2 + y2 + z2 = 9. Let /(x) = x3 + y3 + z3 and let <?(x) — x2 + y2 + z2. 
We want to maximize / on S = { x e M3 g(x) = 9 }. Thus, we seek x e S and 
A e R such that V/(x) = AVg(x). Thus, 
(3x2,3y2,322) = 
\{2x,2y,2z). 
Since x2 + y2 + z2 = 9, we have A ^ 0. Assume first that i ^ 0 , j / 0 , 2 / 0 . 
Then we obtain x = y = z and, therefore, x2 = 3. Thus, x = ±(\/3, v/3, \/3). In 
this case, /(x) = ±9-\/3. 
Now, suppose that x = 0 = y. Then z = ±3 and A = (3z)/2. Similarly, if 
x = z = 0, then y = ±3, A = (3y)/2, and if y = z = 0, then x = ±3, A = (3x)/2. 
In each of these three cases, /(x) = ±27. 
Finally, if only one of x, y, or z is zero, then the remaining two unknowns are equal 
to one another and each equals ±v^3/2. It is clear that the values of / in these cases 
are between —27 and 27. Hence, the maximum value of / is 27, which occurs at 
(3,0,0), (0,3,0), and (0,0,3), and the minimum value of / is —27, which occurs at 
(-3,0,0), (0, -3,0), and (0,0, -3). 
Example 6.5.9 Let us find the maximum value of u\ ■ ■ ■ v?n, where 
uj + --- + u2
n = l. 
Let /(u) = u\ ■ ■ • u2 and let S = { u e R" | u\ + ■ ■ ■ + u2
n = 1 }. Since / is 
continuous on the compact set S, we know that / must have a maximum value on S. 
Thus, there is some A e R and some x G S such that V/(x) = AVg(x). Thus, 
X\Xo 
' ' ' ¿C—, 
= = 
¿\Xi 
x\xix\ 
■ ■ ■ X2
n 
= 
\X2 
2 
2 
n-

2 6 4 
DIFFEOMORPHISMS AND MANIFOLDS 
Multiplying the first equation by x\, the second equation by X2, • • •, and the last 
equation by xn, we deduce that 
Since / takes on some positive values on S, the maximum value of / on S must also 
be positive. Hence, none of Xj can be zero. In particular, A ^ 0. Therefore, 
i/y 1 
Ju O ~~~ * 
—— <h y-, 
Thus, since x\ + ■ ■ ■ + xn = 1, we get 
2 
1 
Xu — — for all k = 1,..., n. 
k 
n 
Therefore, the maximum value of / on S occurs when x\ = 1 /n for all fc = 1,..., n. 
In particular, 
tt?---^ 
= / ( u ) < / ( x ) = ( l ) " = ^ + --n
+ulJ 
foralluGS. 
That is, 
■ul )" < 
1 
''' 
"" 
whenever u\ + ••• + u2
n = 1. 
(6.36) 
From the above, we obtain the arithmetic-geometric mean inequality: 
Let í 1,... ,tn be nonnegative numbers. Then 
(ti---tn)$<tl 
+ '" + tn. 
(6.37) 
n 
To prove (6.37), let t\,..., tn be any nonnegative numbers. Put S = t\ + ■ ■ ■ + tn. 
If S = 0, then each tk = 0 and (6.37) is obvious. Assume that S > 0. Since each 
ifc/5 is nonnegative, there is some « ( ¡ e l with t^/S — u\. Then 
«? + ••• + «' =1-
Thus, (6.37) follows from (6.36) because 
Example 6.5.10 Suppose that/ : R™ —*• M is a continuously differentiable function. 
Assume that for any a £ R, the system of equations 
-—(u)=auk, 
k = l,...,n 
axk 

DIFFERENTIATION ON MANIFOLDS 
2 6 5 
has at most one solution u with u\ + ■ ■ ■ + u2 = 1. Then / is constant on 
{ x £ R™ | x\ -\ 
V x2
n = 1 } . To see this, let g(x) = x\ + • ■ • + x2
n. Put 
S = {x 6 R™ | g{x) = 1 }, a compact set in R™. Hence, / must have a maxi-
mum value and a minimum value on S. But these extreme values of / must occur at 
points u e S for which there is some A e R with V/(u) = AVg(u). Hence, with 
a = 2A, we must have 
df 
-—(u) 
= auk 
for all k = 1,..., n. 
By assumption, there is at most one such u G S. This implies that the maximum 
and minimum values of / on S occur at the same point u. Hence, the maximum and 
minimum values of / on S are equal. Thus, / must be constant on S. 
Problems 
6.34 
A linear transformation T : R2 —> M2 is given as 
T(x, y) = {Ax + By, Cx + Dy). 
Find ||T|| in terms of A, B, C, D. The norm on R2 is the standard Euclidean norm. 
6.35 
Findthemaximumandminimumvaluesofx2+y2+z2 given thatx+y+z — 0 
and (x - 3)2 + y2 + z2 = 9. 
6.36 
Find the minimum value of 3x — y — 3z where x + y — z = 0 and x2 + 2z2 = 1. 
6.37 
Find the maximum value of xyz where x2 + y2 + z2 = 3. 
6.38 
Findthemaximumandtheminimumvalueof£+y+zwhere:E2+2/2+22 = 1. 
6.39 
Find the minimum value of 5x — 2y + Iz where x2 + 2y + Az2 = 9 
6.40 
Find the minimum value of xyz where x2 + 2y2 + 3z2 = 12 
6.41 
Find the points on the curve x2 + xy + y2 = 3 closest to and farthest from 
the origin. 
6.42 
Find the maximum value of yz + xy where xy = 1 and y2 + z2 = 1 

This page intentionally left blank

CHAPTER 7 
HIGHER-ORDER DERIVATIVES 
Practically all computations are done in terms of polynomials. In fact, the scope of 
differential calculus is essentially limited to functions that can be approximated by 
polynomials. In these approximations the higher-order derivatives play a central role. 
We have already considered higher-order derivatives for functions of a real variable. 
Here we will discuss them in the general case. 
7.1 
DEFINITIONS 
Let X and Y be two normed spaces. Let A be an open subset of X. We will define 
the higher-order derivatives of a function / : A —» Y. 
Definition 7.1.1 Difference operators. New notations for directional derivatives. 
Let a e i and U E I The difference operator A u is defined as 
A u/(a) = /(a + u ) - / ( a ) . 
Analysis in Vector Spaces. 
267 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 

2 6 8 
HIGHER-ORDER DERIVATIVES 
Here it is assumed that (a + u) £ A. We will also write D u/(a) for the directional 
derivative of / at a G A in the direction of u s X. Hence 
n tt ^ 
t'f 
^ 
v 
/ ( » + * u ) - / ( a ) 
A t u/(a) 
-Du/ a) = f (a; u = hm 
— hm 
. 
Definition 7.1.2 Higher-order directional derivatives. Let u¿ £ X be finitely 
many vectors. The higher-order directional derivatives 
£>„„•■• £> U l/(a)e y 
are defined inductively on n £ N. The definition is known for n = 1. Assume that 
F(a) = DUn ■ ■ ■ £)Ul /(a) is defined as a function F : A —> y. Let u £ X. Then 
we define 
£>u • DUn • • • DU1 /(a) = D uF(a) = Jim 
^
M
. 
The limit is taken in Y. 
Definition 7.1.3 Higher-order derivatives. Let us now consider the main or 'full' 
derivative. Higher-order derivatives of a function / : A —> Y are also defined 
inductively. The first derivative was already defined as a function / ' : A -^ 
L(X, Y) = MLi{X, Y). 
The second derivative of / : A -> Y is the first 
derivative of / ' : A —► L(X, Y). Hence the second-order derivative is a function 
f":A^ 
L(X, L(X, 
Y)). 
Recall that there is an isomorphism between L(X, L(X, Y)) and ML2(X2, 
Y) as 
defined in Definition 3.3.7. Accordingly, we will consider the second derivative as a 
function 
/ " : A-+ ML2{X2, 
Y). 
By induction we see that the nth order derivative will be defined as a function 
f^:A^MLn(Xn,Y). 
Hence, the (n + l)st order derivative will be a function 
/(»+i) : A - L(X, MLn(Xn, 
Y)) 2á MLn+1(Xn+1, 
Y). 
Notations 7.1.4 Values of higher-order derivatives. Notations for the values of 
higher-order derivatives depend on the way the spaces 
L(X, MLn(Xn, 
Y)) and MLn+1{Xn+\ 
Y) 
are identified. This identification is made in such a way that the relation 
/ ( n )(a)(u n i ..., U l) = DUn ■ ■ ■ D U l/(a) 
(7.1) 
holds for all n e N. 

DEFINITIONS 
269 
Classes of Differentiable Functions 
Definition 7.1.5 Qn functions. Let A be an open set in a vector space X. Then 
Cn = en(A, Y), n e N, is the set of all functions / : A -f F for which 
Dnf = / ( n ) : A -f ML{Xn, 
Y) 
exists and is continuous. Hence Cra(A, F) is the class of all functions / : A —> F 
that are n-times continuously differentiable. Also, C°° = e°°(A, F) is the class of 
functions that have derivatives of all orders. A Cn diffeomorphism / : A —-> F is a Cra 
function, and a diffeomorphism such that the reverse diffeomorphism g : f(A) —> A 
is also a e n function. Finally, the graph of a Sn function is a C™ graph, and Gn 
manifolds are locally 6™ graphs. In the following statements about the properties of 
Cn functions, the superscript n will also stand for oo unless specified otherwise. 
Theorem 7.1.6 Chain rule for Cn functions. Let A and B be open sets in X and Y, 
respectively. If f : A —> B and g : B —► Z are Cn functions, then h = g- f : A —► Z 
is also a Gn function. 
Proof. The chain rule was given in Theorem 5.5.6. It shows that if / and g are 
differentiable, then h is also differentiable and 
/l'(x)=fl'(/(x))-/'(x), x e A 
We will express h' : A —> L(X, Z) also as a composed function. Define 
P 
: 
A -> L(F, Z) x L(X, F) and 
Q 
: 
L(F, Z) x L(X, F) -► L{X, Z) by 
P(a) 
= 
(<?'(/(a)), /'(a)), a e A, and 
Q(S, T) 
= 
5T, (S, T) e L(y, Z) x L(X, F). 
Then we see that h'(a) = Q(P(a)) for all a e A or that 
tí = Q ■ P : A -> L{X, Z). 
(7.6) 
We proceed by induction on n G N. If n = 1, then the result is already obtained 
in Theorem 5.5.6. Now assume that the composition of two Cra functions is a Cn 
function. Let / and g be C n + 1 functions. Then g' : B —> L(F, Z) is a C™ function. 
Therefore, (g' ■ f) : A —> L(F, Z) is a 6" function by the induction hypothesis. In 
this case an easy check shows that 
P=(g'- 
/, /') :A -> L(F, Z) x L(X, Y) 
(7.2) 
(7.3) 
(7.4) 
(7.5) 

270 
HIGHER-ORDER DERIVATIVES 
is a C™ function. Also, Q is a bilinear function and, therefore, a Q°° function. Hence 
the induction hypothesis shows that hi = (Q ■ P) is a Cn function. Therefore h is a 
e™+1 function, 
ü 
Lemma 7.1.7 Let n G N. 
77i<?n f : A —> Y belongs to Qn if and only if 
DUn ■ ■ ■ DUl f : A —> IA exwto a«J is continuous for all (ui, ..., u„) G X". 
Proof. If / £ C™, then clearly DUn ■ ■ ■ DUlf 
: A —> Y exists and is continuous for 
all (ui, ..., u„) 6 Xn. To prove the converse, apply an induction on n G N. For 
n = l this result follows from the existence theorem for derivatives, Theorem 5.3.4. 
Now assume the result for n G N. Let 
DvDUn-DUlf:A->Y 
exist and be continuous for all (ui, • • • , u„, v) G Xn+1. 
In particular, 
DUn---DuJ:A^Y 
also exists and is continuous. Hence g = Dnf : A —> ML(Xn, 
Y) exists by the 
induction hypothesis. Then we see that Dvg : A —► ML(Xn, 
Y) exists and is 
continuous for all v G X. Therefore, the existence theorem, Theorem 5.3.4, shows 
that Dg = Dn+1f 
: A -> ML(Xn+1, 
Y) also exists and is continuous. 
D 
7.2 
CHANGE OF ORDER IN DIFFERENTIATION 
We have defined DUn ■ ■ ■ DUl f : A —> Y as the result of n successive differentiations 
in the directions of ui, ..., u„ G X. It turns out that if / is a Cra function, then the 
result is independent of the order of these differentiations. 
Lemma 7.2.1 Commutativity of the difference operators. Let AUi be n difference 
operators. Then for any permutation oof{l, 
..., 
n} 
A U l---A u„/(x) = AUa(1)---AUff(„)/(x), 
whenever one of these expressions is defined. 
Proof. It is enough to show that A vA u/(x) = A uA v/(x) whenever one side is 
defined. We have 
A vA u/(x) 
- 
A v ( / ( x + u ) - / ( x ) ) 
= 
/(x + v + u ) ^ / ( x + v ) ^ / ( x + u) + /(x) 
= 
A u(/(x + v ) - / ( x ) ) = A uA v/(x). D 

CHANGE OF ORDER IN DIFFERENTIATION 
271 
Remarks 7.2.2 Theorem 7.2.3 below establishes a relation between 
A U l---A U n/(x)andD U l •••£>„„/(x). 
This result generalizes Lemma 5.3.2. Recall that if g : A —► Y is a function and 
E c A then 
Í7£(5) = s u p { | ¡ f l ( x ) - 5 ( x ' ) ¡ | | x , x ' e £ } 
is the oscillation of g over £ if this supremum exists. 
Theorem 7.2.3 Assume that f : A —*• Y is a C" function. Let a e A anfii let r > 0 
be such that Br{a) C A Le/ Ui, . . . , u „ £ l . 77¡en 
||A U l---A U n/(a + w)- JD U l.--£> U n/(a)|| 
< 
O ß r ( a ) ( i } U l - i ) u J ) 
< 
||u 1||-.-||u„||íí B r ( a )(£>n/) 
whenever ||ui|| H 
h ||un|| + ||w|| < r. 
Proof. Define 
<p(s) = AU1 ■ • • A 5 U„/(a + w ) _ DUi ... L>su„/(a). 
The first estimate of the theorem can be expressed as 
Mi)ll<fiB r(a)(A,, •■•£>,,„/). 
We see easily that, as in the proof of Lemma 5.3.2, 
<p'(s) = AU1 • • • A u ^ A,„/(a + w + su„) - DUl • • ■ £>„„_! A.„/(a). 
Let F(x) = DUnf(x) 
and assume that the conclusion of the theorem is true for 
(n — 1) as an induction hypothesis. This implies that, for all s € / = [0, 1 ], 
W(s)\\ < nBr(a)(DUj---DUn_1F) 
= 
nBr{a)(DUl---Du_1DuJ). 
Hence, by the mean value theorem, Theorem 5.1.13, we have 
»^(l)» = Ml) 
- <p(0)\\ < (1 - 0)ííBr(a)(£>Ul ■ • • 
D^DvJ). 
This is the first estimate of the theorem. The second estimate follows from 
||£»U1 •••£>«„_, J 5 u n / ( x ) | | y 
= 
l l D - Z i x X u ! , 
. . . , U „ ) | | y 
< 
\\u1\\x---\\un\\x\\Dnf(x)\\ML{XntY). 
O 

272 
HIGHER-ORDER DERIVATIVES 
Theorem 7.2.4 Assume that f : A —> F isa 6™ function. Let a G A and let u¿ G X. 
Then for each e > 0 there is a S > 0 such that 
•At u /(a) 
A i „ ••■£>„!/(a) 
< £ 
whenever 0 < |í¿| < 5 for alii = 1, ..., n. 
Proof. Choose r > 0 such that Br (a) c ^4 and such that 
\\u1\\---\\un\\nBAsi)(Dnf)<e. 
This can be done because of the continuity of Dnf : A —> ML(Xn, Y). Then 
choose 5 > 0 such that 
HÍHHH + -- • + ||i„u„|| < r 
whenever ¡í¿| < S for all i — 1, ..., n. In this case Theorem 7.2.3 shows that 
||AtlUl • • • A t n U„/(a) -(*!••• i„)£>Ul • • • £>u„/(a)|| 
= 
||AtlUl • • • A t n U n/(a) - D t l U l • • • A„u„/(a)|| 
< 
||í1u1||---||í„un||f2Br(a)(JD"/) 
= 
(í1---í„)||u1||---||u„||n i 3 r.( a )(D n/) 
< 
(ti---tn)e. 
Then the proof follows easily. 
□ 
Theorem 7.2.5 Assume that f : A —> Y is a C" function. Let a g A. Then 
DU1---DuJ(a) 
= / ( n )(a)(u„, . . . , U l ) e y 
¿j independent of the ordering of the vectors ui, . . . , u „ g l . 
Proof. This follows from Theorem 7.2.4 above and from the commutativity of the 
difference operators, Lemma 7.2.1. 
□ 
Theorem 7.2.6 Assume that f : A —> Y is a Qn function. Then 
/ ( n )(a) 
-Xn^Y 
is a symmetric multilinear function for each a € A. 
Proof. This a reformulation of Theorem 7.2.5. □ 

SEQUENCES OF POLYNOMIALS 
273 
7.3 SEQUENCES OF POLYNOMIALS 
Recall that a homogeneous polynomial of degree n G N is a function / : X —> Y of 
the form 
/(x) = Q(x, ..., x), where Q e MLn(Xn, 
Y). 
Hence, a homogeneous polynomial is defined in terms of an associated multilinear 
function Q. This multilinear function is defined on Xn with values in Y. The points 
in X are denoted as x and the points in 1 " as 
(xi, ..., x„), x ¡ e l , i = i,...,íi. 
The multilinear function associated with a polynomial is not unique. In particular, 
note that /(x) = Q(x, ..., x) = 5(x, ..., x) where 
5(xi, ..., x„) = ^^2aeSn 
Q(xCT(i), ..., xCT(n)) 
is the symmetric part of Q, as in Definition 3.3.12. 
Derivatives of Polynomials 
Lemma 7.3.1 Let /(x) = Q(x, ..., x) be a homogeneous polynomial of degree n. 
Let S G MLn(Xn, 
Y) be the symmetric part of Q. Then 
/'(x)(u) 
= 
Q(u, x, . . . , x ) + ... + Q(x, . . . , x , u) 
(7.7) 
= 
nS(x, ..., x, u) 
(7.8) 
/or a// x, u G X. 
Proof. Define P : X ^ Xn by P(x) = (x, ..., x) G Xn, x € A". We see that 
P : X —-> X™ is a linear function. Hence 
P'(x)(u) = P(u) = (u, ..., u) for all x, u e X. 
Now / : X —> y is the composition / = S ■ P. Now apply the chain rule, Theorem 
5.5.6. The derivative of S is given by Theorem 5.6.2, together with Remarks 5.6.3. 
This gives (7.7). To obtain (7.8), note that/(x) = 5(x, ..., x) and use the symmetry 
of S in (7.7). D 
Theorem 7.3.2 Let /(x) = Q(x, ..., x) be a homogeneous polynomial of degree 
n. Let S G MLn(Xn, 
Y) be the symmetric part ofQ. Then for each k G N and for 
each x G X the kth derivative /(fc)(x) G MLk(Xk, 
Y) exists. Ifl<k 
<n, then 
/W(x)( U l, • • • , ufc) = - ^ r r y S(x, ... ,x, U l, ■ • ■ , ufc). 
(7.9) 

274 
HIGHER-ORDER DERIVATIVES 
Here x enters (n — k) times as an argument of S : Xn —> Y. 
Proof. Lemma 7.3.1 gives (7.9) for k = 1. Now assume (7.9) for a k, 1 < k < n. 
Let Ui, ..., Ufc be fixed vectors in X. Then, with i = n — k, 
i?(xi, . ..,x¿) =S(xi, ...,x €, ui, ..., ufc) 
is a symmetric multilinear function in MLg(Xe, Y). If g is the associated homoge-
neous polynomial of degree I, then 
/(fe)(x)(Ul! ..., ufc) 
= 
(n!/¿!)s(x). Hence 
/(fc+1)(x)(Ul, ..., ufc, ufc+i) 
- 
(n!/¿!)5'(x)(ufc+i) 
= 
(n\/£\)£S(x, 
...,x, ui, ••• , Ufc, ufc+1). 
This is (7.9) for k + 1. Hence (7.9) is true for all k, 1 < k < n. 
D 
Corollary 7.3.3 ///(x) = S{x, ..., x) vw'rA symmetric S G MLn(Xn, 
Y), then 
/ ( n )(x)(u„, ■ • • , ui) = n! 5(u n, • • • , U l) 
(7.10) 
i's independent o/x. Hence f^ 
: X -^ MLk(Xk, 
Y) vanishes for all k > n. Also, 
/W(0) = 
0forallk^n. 
Proof. This follows directly from (7.9) of Theorem 7.3.2. If k < n, then 
/(fc)(0)(Ul, • • • , Ufc) = — ^ — 5(0, ..., 0, in, • • • , Ufc) = 0. 
If k = n, then S contains no x terms. Hence/'"' : X —» MLn{Xn, 
Y) is a constant 
function and all higher derivatives vanish. 
□ 
Example 7.3.4 Define F : L{X, X) -> L(X, X) by F(T) = T3, T G L(X, X). 
This is a homogeneous polynomial of degree 3. In fact, Q(A, B, C) = ABC 
defines a multilinear function and F(T) = T 3 = Q(T7 T, T). By three successive 
applications of Lemma 7.3.1 we obtain 
F'(T){A) 
= 
AT2+TAT 
+ T2A 
F"(T) {A, B) 
= 
ABT + ATB + BAT + TAB + BTA + TBA 
F'"(T)(A, B,C) 
= 
ABC + ACB + BAC + CAB + BCA + CBA. 
The symmetric part of this polynomial is 
S{A, B, C) = {ABC + BCA + CAB + ACB + CBA + 
BAC)/6. 
Hence the relation F'"(T)(A, 
B,C) = 6 S(A, B, C) is verified. 

SEQUENCES OF POLYNOMIALS 
275 
Term-by-Term Differentiations 
Consider a sequence of multilinear functions Qn £ MLn(Xn, 
Y) and the associated 
homogeneous polynomials fn(x) = Qra(x • • •, x). We define 
^n(x) = Er=i / i W and G„(x) = F¿(x) = £ ? = 1 //(*)• 
We assume that Q„s are symmetric multilinear functions. This is not a loss of 
generality since fn{x) — Qn(x ..., x) = 5 n(x ..., x), where Sn is the symmetric 
part of Qn. We write Qn = Sn. We also assume that there is an R > 0 for which 
the sequence ¡IS1«)! Rn is bounded. As before, 
£?r(0) = { x | | | x ! ] < r } c X 
is the open ball of radius r > 0 about the origin. 
Theorem 7.3.5 Ifx 
e -BH(O), fne« lim„Fn(x) = F(x) exwtt in y. A/so, F : 
BR(0) 
-^Y is a continuous function. 
Proof. This a restatement of Theorem 4.4.32. G 
Lemma 7.3.6 If-x. e BR(0) and u e X, fnen 
G(x)(u) = limnGn(x)(u) 
ex/sis in y. 
Proof. Theorem 7.3.2 shows that 
||/;(x)(u)|| = ||n5„(u, x, ..., x)|| < ||S„|| n ]|u|| | | x | r *. 
Let ||x|| < R. Find a n r S l such that ||x|| < r < R. Since ||5„|¡ñn is bounded, 
we have Enll^™llni*™_1 < °° by Theorem 2.5.7. Hence 
£ 
H S J I n l l u l l U x i r ^ o c , 
¿—Jn 
Then, as in the proof of Theorem 4.4.32, we see that the limit in (7.11) exists. 
□ 
Notations 7.3.7 For fixed x, u e X, r > 0, and n e N , define 
K{s) 
= 
/ n(x + S u ) - / „ ( x ) - s / ; ( x ) ( u ) e y a n d 
(7.12) 
<pn{s) = 
(n(n - l ) / 2 ) | | S n | | r " - 2 | | u | | 2
S
2 € K 
(7.13) 
£°\/;(x)(u) 
(7.11) 

276 
HIGHER-ORDER DERIVATIVES 
for all s e l . 
Lemma 7.3.8 IfO<r 
< R then 
Af = £ ( n ( n ~ l ) / 2 ) j | S n | | r " - 2 | | u | | 2 < o o 
n 
and ]Tn ipn(s) < Ms2 for all s E t 
Proof. Theorem4.4.32showsthat]n„(n(n-l)/2) \\Sn\\ r
n- 2||u|| 2 < oo whenever 
||5 n|| Rn is a bounded sequence and 0 < r < R. 
D 
Lemma 7.3.9 //(||x|| + ||u|| |f|) < r, then \\hn(t)\\ < y?„(t). 
Proof. We see that hn : R —> Y is a differentiable function and 
h'n(8) = /;(x + Su)(u)-/;(x)(u) 
= 
nQ„(u, x + su, ..., x + su) - nQ„(u, x , . . . , x ) , s e t . 
For a fixed u e X , define 5„_i G A ^ - i ^ " - 1 , Y) by 
5„_i(xi. ..., x n_i) = 5„(u, xi, ..., x„_i). 
Then, clearly, ||5n_i|| < ||u|| \\Sn\\. Hence Theorem 4.3.14 on the increments of 
multilinear functions shows that, if (||x|| + ||u|| \s\) < R, then 
KOOH 
< 
n(n-l)\\Sn^\\rn~2\\su\\ 
< 
n(n-l)||.5„||||u||r"- 2|| Su|| 
< 
n(n^l)\\Sn\\r"-2\\uf\s\ 
= <p'n(\s\). 
Then Theorem 5.1.12, a version of the mean value theorem, shows that 
\\hn(t)\\ = \\hn(t) - hn(0)\\ < <p(\t\) - <p(0) = <p{t). □ 
Lemma 7.3.10 Let fn(x) = Sn(x, ..., x), x G X be a sequence of polynomials 
X —> Y. For fixed x, u G X, let 
KM 
= E L i A t o W G n( x ) ( u ) = ELi/fcW(u), n € N. 
If there is an R > 0 swc/z r/iöi ||S>i|| Rn is a bounded sequence, then 
lim„Fn(x) = F(x) aMi/limnG„(x)(u) = G(x)(u) 

SEQUENCES OF POLYNOMIALS 
2 7 7 
both exist in Y for all x G BR(0) and u G X. 
Also, F : BR(0) 
—> Y is a 
differentiable function and F'(x)(u) = G(x)(u) for all u G X. 
Proof. The existence of F(x) was obtained in Theorem 7.3.5 and the existence of 
G(x)(u) in Lemma 7.3.6 above. Let x G BR(0), 
U G X. Let c > 0 be such that 
(||x|| + c||u||) < r < R. If |i| < c, then 
||Fn(x + í u ) - F „ ( x ) - í G „ ( x ) ( u ) | | 
< 
V " 
\\hk(t)\\ 
z—Jk — \ 
< V 
Mt)<Mt2. 
¿—'n 
The last two inequalities follow from Lemmas 7.3.8 and 7.3.9 above. By taking the 
limit on n and using the the continuity of the norm function, we obtain 
||F(x + iu) - F(x) - íG(x)(u)|| 
<Mt2 
whenever |i| < c. This proves that F'(x)(u) = G(x)(u). 
D 
Theorem 7.3.11 Term-by-term differentiation. Let /„(#) = S„(x, ..., x) be a 
sequence of polynomials X —> Y. Assume that there is an R > 0 such that the 
sequence \\Sn\\ Rn is bounded. Then 
F(x) = J2 /»W 
defines a C00 function F : BR(0) —> Y. Also, 
DUk ■ ■ ■ DUlF(x) = J2 DUk--- DuJn(x) 
(7.14) 
for all Ui, .... Ufc G X. 
Proof. This follows from Lemma 7.3.10 above by an induction on k G N. In 
fact, this lemma gives (7.14) for k = 1. Assume (7.14) for a fix set of vectors 
ui, ..., Ufc G X. Let max, ||u¿|| = u Then we see easily that 
||^u f c--- JD U l/ n(x)||<n f c- 1||5„|| U
f c||x|r- f e. 
But Y2n
 nfe_1|l'^n|l uk rn~k < co whenever 0 < r < R. This is again by Theorem 
4.4.32. Then Lemma 7.3.10 above shows that (7.14) is also true for k + 1. 
□ 
Example 7.3.12 The inversion operator. LctLinv(X, 
X)bethesetofallinvertible 
mappings in L(X, X). Then the inversion operator 
d = Inv : Llnv(X, X) -> Llnv(X, X) C L(X, 
X) 

278 
HIGHER-ORDER DERIVATIVES 
takes T e Llnv(X, X) to i?(T) = T^1. We claim that d is a C00 function on 
Linv(X, X). First, we show that all the derivatives i?(m)(7) exist at the identity. 
This follows from the expansion 
■d{I + T) = I - T + T2 + ■ ■ ■ + (-l)nTn 
+ ■■■ 
obtained in Theorem 4.4.38 and from Theorem 7.3.11 above. To see differentiability 
at a general point A e Lim(X, X), it is enough to let A-1 = B and observe that 
d{A + T)=ti{I 
+ 
BT)-B 
as in the proof of Theorem 4.4.38. Finally, the inversion operator 
Inv : Llnv(X, Y) -> L{Y, X) 
between different spaces is also a C°° function. Again, to see this, it is enough to let 
let A e Linv(X, Y) with B = A'1 e L(Y, X) and observe that 
Inv(,4 + T) = i?(7 + BT) ■ B. 
See Example 7.3.20 below on the computation of some derivatives of #. A 
Example 7.3.13 The exponentiation operator. The exponentiation operator exp : 
L(X, X) -> L(X, X) takes T e L(X, X) to 
limn(7 + T+ (1/2!)T2 + • • • + (l/n!)Tn) = eT 6 L{X, 
X). 
Theorem 7.3.11 shows that exp : L{X, X) -> L(X, X) is a C00 function. Note that 
the derivatives of eT exist, but they may not have simple expressions. For example, 
ifA€L(X, 
X)then 
DAeT 
= 
A + {AT + TA) + (1/2!)(4T2 + TAT + TA2) + (1/3!) 
+(1/3!)(AT3 + TAT2 + T2AT + T3A) + ■■■ . 
This may not have a simple expression unless A and T commute. If AT = TA, 
then DACT 
= AeT = eTA. 
See Example 7.3.19 below for the derivatives of eT 
at T = 0. Finally, let X = R and consider t € M as the linear map that takes 
x € M to tx € R. In this case e* is the classical exponential function. We see that 
Da é = lim r^ 0(l/r')(e t + r a - e') = aeK 
A 
Taylor Polynomials and Series 
Taylor polynomials and Taylor series can be generalized from functions of a real 
variable to functions of a vector variable. No new arguments are needed here. These 

SEQUENCES OF POLYNOMIALS 
279 
generalizations follow directly from the corresponding results for the functions of a 
real variable. 
Definition 7.3.14 Let / : A -> Y be a Gn function. Let a G A Then 
k 
Pfc(a + x ) = £ 
/("0(a)(x, . . . , x ) 
m=0 
is the Taylor polynomial of / of degree k at a e A. 
Theorem 7.3.15 Approximation by Taylor polynomials. Let f : A —► Y be a 
Cn function. Let R > 0 and M > 0 be such that ||/(ra)(a + x)|| < M whenever 
||x|| < R. Then 
||/(a + x) - Pn_!(a + x)|| < (l/n!)M ||x||n 
(7.15) 
whenever \\x\\ < R. 
Proof. Letx e X,0 < ||x|| < R, be fixed. Define ¡p(s) = /(a + sx). Then Lemma 
5.2.18 on the computation of directional derivatives shows that 
<p'(s) = 
/'(a + sx)(x). By an induction on k we obtain 
tpW (s) 
= 
/(fc) ( a + sx) (x, ..., x). Hence 
?„_!(*) = £lIo(1/*!)P(fc)(0)*fc 
Let r = i?/||x||. Then r > 1 and ||/(™)(a + sx)|| < M whenever 0 < s < r. Hence 
\f(n)(s)\ 
= ||/(»)(a + Sx)(x, ..., x)|| < M ||x|P 
whenever 0 < s < 1 < r. Then Taylor's theorem, Theorem 5.1.22, for functions of 
a real variable shows that 
||^(1) - p n_i(l)|| = ||/(a + x) - P„_!( a + x)|| < (l/n!)M ||x||n. 
Here we observed that p„_i(l) = P n _ i ( a + x). 
G 
Notation 7.3.16 For each n G N and r > 0, let 
Mn(r) = sup {||/W(a + x)|| | | | x | | < r } 
if it exists. 

280 
HIGHER-ORDER DERIVATIVES 
Theorem 7.3.17 Approximation by Taylor series. Assume that there is an R > 0 
such that (1/n!) Mn(R)Rn 
is a bounded sequence. Then 
/(a + x) 
= 
/(a) + /'(a)(x) + -.- + i / W ( a ) ( x ; . . . , x ) + .-. 
= 
l i m " E 
¿/ ( f e )(a)(x, . . . , x ) = l i m „ P n ( a + x) 
fc=0 
whenever ||x|| < R. 
Proof. Let (1/n!) Mn{R)Rn 
be a bounded sequence. We see that 
lim„(l/n!)Mn(A)r n = 0 
whenever \r\ < R. Then the result follows directly from Theorem 7.3.15. Ü 
Derivatives in Terms of Taylor Series 
Let / : A —> Y be a function. If we can express /(a + x) as a convergent sequence 
of polynomials in x, then we can easily find the derivatives of / at a. We state this 
result as follows. 
Theorem 7.3.18 Let /ra(x) be a sequence of homogeneous polynomials associated 
with multilinear functions Qn e MLn(Xn, 
Y). Assume that there is an R > 0 such 
that the sequence \\Qn\\ Rn remains bounded. If 
F(x) = /1(x) + /2(x) + ---
with ||x|| < R, then F ( n )(0)(ui, • • • , u„) = n! 5 n(ui, • • • , u n). Here Sn is the 
symmetric part ofQn, as in Definition 3.3.12. 
Proof. By Theorem 7.3.11 we can differentiate -F(x) term by term. Hence 
F^(0)(uu 
• • ■ , u„) = / ^ ( O X m , • • - , u„) + 4 n ,(0)( U l, • • • , u n) + • • • . 
But, by Corollary 7.3.3, /^ (0)(ui, • ■ • , u„) = 0 unless k = n. Hence, 
F(">(0)(ui, ■ • ■ , u„) = /(rl)(0)(u 1: • • • , u„) = n! S„(ui, • ■ • , u n), 
again by Corollary 7.3.3. 
□ 

SEQUENCES OF POLYNOMIALS 
281 
Example 7.3.19 Derivatives of e at T = 0. From the definition 
F{T) = eT = I + T+ (1/2!)T2 + • • • + (l/nl)Tn 
+ ■■■ 
weseethatF(™)(0)(Ai, ..., An) = n\Sn(Ai, 
..., An). Here5„ is the symmetric 
part of Qn e MLn(Xn, 
Y) such that (l/n!)T n = Qn(T, ..., T). Hence, for 
example, 
F"'(0)(A, 
B, C) = (ABC + BCA + CAB + ACB + CBA + BAC). 
A 
Example 7.3.20 The inversion operator. LetLinv(X, JQbethesetofallinvertible 
mappings in L(X, X). Then the inversion operator 
Inv : Llnv(X, X) -+ Llnv(X, 
X) 
takes T e L\m(X, 
X) to InvT = T~l. We can find the derivatives of the function 
at A e Linv(X, X) as follows. Let B = A^1. We have 
(A + T)-1 = (A(I + BT))-1 
= (I + BT)-lA~l 
= (I + 
BT^B. 
Theorem 4.4.38 shows that the series of polynomials 
F(T) = B- 
(BT)B + (BT)2B + ■■■ + {-l)n{BT)nB 
+ ■■■ 
converge to F(T) = (A + T)"1 whenever ||BT|| < 1. In particular, we have 
convergence whenever |JT|| < l/||ß||. Hence we can find the derivatives of F(T) at 
T = 0. We see easily that F<n)(0) = (Inv)(nH^)- Hence 
(Inv^iA^Uu 
...,£/„)= n!Sn(Ult ..., 
Un). 
Here Sn is the symmetric part of a multilinear function that induces the polynomial 
(BT)nB. 
The first three derivatives of the inversion operation are 
(lnv)'{A)(U) 
= 
-BUB 
(lnv)"(A)(U,V) 
= 
(BUBVB 
+ 
BVBUB) 
(Inv)'"(A)(C7, V, W) 
= 
-(BUBVBWB 
+ BVBWBUB 
+ 
BWBUBVB 
= 
+BUBWBVB 
+ BWBVBUB 
+ 
BVBUBWB). 
In the classical case of f(x) = (l/x), x =¿ O, these formulas are reduced to 
The verification of this is left as an exercise. 

282 
HIGHER-ORDER DERIVATIVES 
Inverse Function Theorem for Higher Derivatives 
Theorem 7.3.21 Extended inverse function theorem. Let A be an open set in X 
and let f : A —> Y be a C™ function. Let a s i and assume that /'(a) : X —> Y is 
an isomorphism. Then there is an open set G in X such that a E G c A and such 
that the restriction of f to G is a 6" -diffeomorphism. 
Proof. Let / : A —> Y be a C1 function such that /'(a) : X —> Y is an isomorphism. 
Then the inverse function theorem, Theorem 6.1.4, shows that there is an open set G 
in X such that a £ G c A and such that the restriction of / to G is a diffeomorphism. 
Hence f(G) = H is an open set in Y containing b = /(a), and the inverse function 
g : H —> X is also a continuously differentiable function. Also, we know that 
g'(y) = (f,(g(y)r1 
= ($-f'-g)(y), 
yeH, 
(7.16) 
since g : H —> G is the reverse diffeomorphism of / : G —> ¿f. Here 
i? : Linv(X, Y) - L(Y, X) 
is the inversion operator that takes an invertible T : X —► Y to its inverse $(T) = 
T-1 : Y -> X. 
The additional part in the extended theorem is that if / : G —> H is a Cn function, 
then g : H —► G is also a Cn function. Proceed by induction on n. This result is 
true for n — 1 by the original theorem. Assume that it is true if / is a Cra function. 
Let / be a 6 n + 1 function. Then / ' : G -+ L(X, Y) and 5 : H -> G are C™ 
functions by the induction hypothesis. Also, t? : Linv(X, Y) —> L(Y, X) is a C" 
function, since it is actually a 6°° function by Example 7.3.12. Then (7.16) shows 
that g' : H —> L(Y, X) is a composition of Cn functions. Hence, by Theorem 7.1.6, 
g' is also a C" function. Therefore g : H —»• G is a Cn+1 function. 
D 
7.4 
LOCAL EXTREMAL VALUES 
Local extremal values of real-valued functions were defined in Definition 5.4.14. 
Taylor polynomials allow us to obtain a test to find these values. This is formulated 
in terms of positive or negative definite real-valued polynomials. 
Definition 7.4.1 Positive definite polynomials. Let p : X —> M be a real-valued 
homogeneous polynomial of degree n e N. Then p is called a (strictly) positive 
definite polynomial if p(x) > 0 whenever x 7^ 0. 
Remarks 7.4.2 If X is an inner product space, then p(x) = (x, x) is a positive 
definite homogeneous polynomial of degree 2. If p is a homogeneous polynomial 

LOCAL EXTREMAL VALUES 
2 8 3 
of degree n, then p(tx) = tn p(x). Hence, if n is odd, then p can never be positive 
definite. If n is even, then p may or may not be positive definite. If p : R2 —> K 
is a second-degree homogeneous polynomial, then there is a familiar necessary and 
sufficient condition for positive definiteness. This is given in Problem 7.1. In general, 
there may not be an easy way of finding out if a homogeneous polynomial is positive 
definite. 
Lemma 7.4.3 Let p : X —> R be a positive definite polynomial of degree n. Then 
there is a number K > 0 such that K \\x\\n < p(x) for all x G X. 
Proof. Let S = {u e X \ ||u|| = 1 }. Then S is a compact set in X. Also, 
p : X —> R is a continuous function. (One way of seeing this is to note that p is a 
differentiable function.) Hence p reaches a minimum value on the compact set S by 
Theorem 4.5.44. Therefore there is an a 6 5 such that p(a) < p(u) for all u G S. 
Then 0 < p(a) = K since p is positive definite and a ^ 0. Now any x G X can 
be expressed as x = ||x|| u with u e S . Hence p(x) = ||x||np(u). Then the proof 
follows, n 
Theorem 7.4.4 Let f : A —> R ¿>e a real-valued function defined on an open set A 
in X. Assume that f is differentiable as many times as needed. Let a f i and let 
m G N be the smallest integer such that ¡(m\&) / 0. //m is odd, then f cannot 
have a local extremal value at a.. Ifm is even and if the polynomial 
p(x) = /<m>(a)(x, . . . , x ) 
¡5 positive definite, then f has a local minimum at a. If there are x¿ such that 
p(xi) > 0 and p(x2) < 0, then f cannot have a local local extremal value at a. If 
p(x) > 0 or ifp(x) < Ofor all x G X, then f may or may not have a local extremal 
value at a. 
Proof. Let /(m> (a) be the first nonzero derivative. Let 
p(x) = (l/m!)/( m'(a)(x, . . . , x ) , 
xeX. 
Then the mth order Taylor polynomial P m(a + x) defined in Definition 7.3.14 
becomes 
P m(a + x ) = / ( a ) + p ( x ) . 
Let R > 0 and M > 0 be such that ||/( m + 1)(a + x)|| < (TO + 1)! M whenever 
||x|| < R. Then Taylor's theorem, Theorem 7.3.15, shows that if ||x|| < R then 
|/(a + x)-Pm(a + x)| 
- 
|/(a + x)-/(a)-p(x)| 
(7.17) 
< 
M||x|| m + 1. 
(7.18) 

284 
HIGHER-ORDER DERIVATIVES 
Now assume that m G N is odd. Let r G X be such that p(r) ^ 0. If p(r) < 0, then 
p(—r) = (—l)mp(r) > 0. Hence assume that p(r) > 0 without loss of generality. 
Letx = ir withí G R. Thenp(fr) = tmp(r) and ||ir||m+1 = im+1||r||. Hence 
tm{p{v) - tM\\v\\) < /(a + ir) - /(a) < tm(p(r) + iM||r||) 
whenever |i| ||r|| < R. Take t > 0 such that iM||r|| < (l/2)p(r). Then we see that 
0 < (l/2)imp(r) < /(a + ir) - /(a) 
and 
/ ( a - i r ) - / ( a ) < - ( l / 2 ) t X r ) < 0 . 
Hence /(a) cannot be a local extremal value for /. 
Now assume that m is even and p : X —> M is positive definite. Lemma 7.4.3 shows 
that there is a K > 0 such that if||x|| m < p(x) for all x G X. In this case (7.18) 
shows that 
| | x | n ( t f - M | | x | | ) < / ( a + x ) - / ( a ) 
whenever ||x|| < R. But this implies 0 < /(a + x) - /(a) whenever 
0 < ||x|| < K/M and ||x|| < R. 
Hence /(a) is a local minimum value for /. Other parts of the theorem are left as 
Problem 7.2. 
D 
Problems 
7.1 
Show that the polynomial P(x, y) = Ax2 + 2Bxy + Cy2 is positive definite 
if and only if B2 < AC and 0 < A. 
7.2 
Give examples of the form 
f(x, y) = Ax2 + 2Bxy + Cy2 + Dx3 + Ex2y + Fxy2 + Gy3 
to cover all cases mentioned in Theorem 7.4.4. 

PART III 
INTEGRATION 

This page intentionally left blank

CHAPTER 8 
MULTIPLE INTEGRALS 
Integration in R™ is a generalization of integration in R = R1. In many ways, 
integration of functions of a single variable is a leading special case: it provides 
valuable guidance as we generalize to functions of several variables. Many of the 
basic definitions and theorems are essentially the same in R and in Rra. In the 
general case, as in M, integrals are defined as the limit of Riemann sums. Continuous 
functions are integrable. Integration is linear: if /, / ' are integrable and a, a' e R, 
then J(af + a'f) 
= a J f + a' J f. 
Even many of the proofs that work in the 
one-variable case generalize to functions of several variables. 
All the same, at least two significant complications emerge when we shift our focus 
from R to R™. The first of these has to do with computational techniques, and the 
second has to do with the contrast between length (in one dimension) and volume (in 
n dimensions). Let us take these two points in turn. 
The first difference, as noted, pertains to computational techniques. The Fundamental 
Theorem of Calculus shows that differentiation and integration are inverse operations 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
287 

288 
MULTIPLE INTEGRALS 
in R. The theorem allows us to compute integrals in R by finding anti-derivatives. 
This simple method is not available when n > 2. Computations in Rn must usually 
be done by successive integrations, each involving the computation of an integral in 
R with familiar techniques. For this reason, integrals in Rn are usually referred to as 
multiple integrals. The main result that licenses this type of computation is Fubini's 
Theorem (Theorem 8.2.37), proven towards the end of Section 8.2. 
The second difference relates to the fact that, while integration in R starts with the 
notion of the length of an interval, integration in Rra starts with higher-dimensional 
volumes. There is a basic difference between the definition of length and the definition 
of volume. If / is an interval with end points r and s, with r < s, then its length is 
given by £(I) = (s — r). No such simple formula is available in higher dimensions, 
for we shall be interested in the volumes of many odd-shaped regions. 
Volumes in higher dimensions — the main topic of Section 8.1 — are defined by 
a method that, in all essential respects, was developed by Archimedes (c. 287-212 
B.C.). To estimate the area A of a two-dimensional region R, we can trace R on 
graph paper and count the number of squares of the graph paper that are completely 
in R. This gives a lower estimate for A. We can also count the number of squares 
that intersect R but may or may not be completely in R. This gives an upper estimate 
for A. To improve these estimates, we use finer graph paper with smaller squares. 
The region R has a well-defined area if and only if, as we let the squares on the 
paper shrink in size, the limit of the lower estimates agrees with the limit of the upper 
estimates. 
The formal definition of area, or higher-dimensional volume, utilizes Archimedes' 
idea. We first define the volumes of certain basic sets, called cubes, and then the 
volumes oí unions of cubes. In the one-dimensional case, these sets are intervals and 
their finite unions. In the the two-dimensional case described above, they are squares 
and finite unions of squares. And so on, for higher dimensions. Following the lead of 
Archimedes, we define the volume of other sets by forming upper and lower estimates 
with unions of cubes and then taking limits as the cubes become smaller and smaller. 
We shall denote volume in Rn by vn or simply by v. One-dimensional volume, or 
length, will often be represented as I instead of as v1 or v. 
It is quite important that the exact specifications of the squares in our sequence of 
grids, or the cubes employed in the limiting construction, turn out not to matter, so 
long as the edge lengths shrink to zero. Many different choices lead to the same limit 
and hence to the same notion of volume. We shall make use of binary cubes, whose 
edge lengths are always of the form 2~k , as our basic type. 
The approach we have just described for defining volume does not always work, 
since the upper and lower estimates may not converge to a common limit. In many 
texts, including this one, the term Jordan set is reserved for sets where the approach 

JORDAN SETS AND VOLUME 
2 8 9 
does lead to a well-defined volume. This is in honor of Camille Jordan (1838-1922), 
who formulated a general theory of volumes. 
The ideas of Archimedes are enough to establish a complete theory of volumes. 
Actually, as we shall see, these methods also give a complete theory of integration. 
That is because an integral on R™ can be thought of as an n + 1-dimensional volume, 
just as an integral on R is essentially the area of the two-dimensional region "under 
the curve" (for a positive function). The only part missing in this approach is an 
efficient general method for the computation of these volumes. Such a method 
was not obtained until almost two thousand years later through the development of 
calculus. Section 8.2 presents the basic theory of integration, together with the most 
important computational techniques. 
Section 8.1, then, is devoted to providing a rigorous definition of volume, and Section 
8.2 outlines the basic definitions and results for the theory of multiple integrals. The 
final two sections of the chapter provide a proof of the Change of Variable Theorem. 
This theorem, perhaps more than any other result, beautifully illustrates both the 
deep analogy and the contrast between integration on R and on Rn. The one-variable 
version of the theorem is usually stated as follows: 
r9(b) 
rb 
/ 
f(v)dy= / f(g(x))g'(x)dx. 
(8.1) 
J g(a) 
Ja 
The proof is an easy application of the Fundamental Theorem of Calculus. For 
functions on R", the theorem takes the form 
/ /(y) dy= f /(y>(x)) | detV(x)) | dx, 
(8.2) 
JB 
JA 
where ¡p is a diffeomorphism, f' is its derivative, and B = <p(A). The formal analogy 
between the two equations is clear, with p in the multidimensional case playing the 
role that g plays in the one-dimensional case. Unlike the one-dimensional case, the 
proof of this result is far from easy. That is a consequence of the complexity of the 
notion of volume as compared to length. In order to appreciate that a strong analogy 
between R and R" still exists, one has to tackle this proof in three steps. The first 
step, in Section 8.3, is to prove the result where ip is a fixed linear transformation. 
The second and third steps, in Section 8.4, establish the result first where /(y) = 1 
and then for general /. 
8.1 JORDAN SETS AND VOLUME 
Recall that Z = {0, ±1, ±2, ...} and Z+ = {0, 1,2,...} are the set of integers 
and the set of nonnegative integers. As explained in the introduction, we start by 

2 9 0 
MULTIPLE INTEGRALS 
defining volume for a special class of cubes — a class that is big enough to construct 
the grids that we need to apply Archimedes' technique. We then define the volume 
of Jordan sets by employing that technique. 
Binary Grids of Cubes 
Deñnition 8.1.1 Binary grids in Rn. Let n G N and k G Z +. The kth-order binary 
grid of R™ = R x • • ■ x R is the collection S£ of cubes C = h x • • • x In where each 
Ii c R is an interval of the form J¿ = [u¿, w¿) with u¿ = p¿ 2~fc, v¿ = (p¿ + 1) 2~fc, 
p¿ G Z. Thus, C£ is a particular family of cubes in Rn with edge length 2~k. The 
center ofC is the point 
c = ( l / 2 ) ( u ! + t ; i , ••• , un+vn). 
(8.3) 
Let ZjJ be the set of centers of the cubes in the grid C£. If the dimension n is 
understood from the context, we also write Cfe and Zk instead of C£ and Z£. If n = 1 
or n = 2, then these cubes are certain intervals or squares. We will call the special 
cube Ek — [0, 2~k)n e C)! the kth-order basic cube. In particular, E0 = [0, 1 ) n is 
called the unit cube ofW1. Note that a binary grid divides Rn into mutually disjoint 
cubes C G C£ by the hyper-planes x¿ = p¿ 2~k, p¿ G Z. (Taking half-open intervals 
is necessary for the cubes to be disjoint.) 
Remarks 8.1.2 Translations of cubes. Translation fcysgl" is the transformation 
R™ ->■ R" that takes x G Rn to (s + x) e l " . If E C Rn, then s + £ denotes the 
translation of E by s. More explicitly, 
s + £ = { s + x | x e £ } . 
Any two cubes in a grid C£ are translations of each other. In particular, each C G C£ is 
the translation of the fcth-order basic cube Ek = [0, 2~fe)" G CJ?. These translations 
are by the binary vectors s — 2~fc(pi, ..., pn) with p¿ G Z. 
Cubes and Balls 
Definition 8.1.3 The maximum norm. The Euclidean norm provides the standard 
notion of distance in R". But in the theory of integration, the maximum norm is also 
a very convenient norm. It is defined as 
||(xi, ..., xn)\\m 
= max(|xi|, .... |x„|). 
With respect to this norm, balls are cubes. More precisely, the interior of a cube is 
an open ball and the closure of a cube is a closed ball. Restricting our attention to 

JORDAN SETS AND VOLUME 
291 
binary cubes, let C € C^ be a fcth-order cube with center c, as in (8.3) of Definition 
8.1.1. Then we see that 
Br{c) = C° c C CC = Br(c) 
where r = (l/2)2~fc = 2~k~1 is the common radius of the cubes in 6^. (Here both 
Br(c) and Br(c) are balls with respect to the maximum norm, C° is the interior of 
C, and C is the closure of C.) Hence 
Br(c) = {x | ||x - c||m = maxi\xi - c¿| < r } 
and similarly for J5r(c), substituting < for <. We shall often denote cubes using this 
notation. 
Lemma 8.1.4 Density of the binary grids. Any open set G e l " contains cubes 
from a binary grid. 
Proof. As we proved in Chapter 4, the openness of a set is independent of the norm. 
Hence, given u e G, there is an r > 0 such that x G G whenever ||x — u|| < r, 
where we are using the maximum norm. Hence, 
(ui - r, u\ + r) x • ■ • x (un - r, un + r) C G. 
Let k € N be such that 2~k < r. In this case, each interval (t¿¿ — r, u¿ + r) contains 
a binary interval [p¿2~fc, (p¿ + l)2_fc) with p¿ e Z. Hence G contains a cube from 
the binary grid Cfc. G 
Cartesian Products of Cubes 
In the following discussion, we use the integers n and m for the dimension of the 
space, while h and k are reserved for the order of the binary grid. 
Notations 8.1.5 Sets in Cartesian products. Let n, m E N. If A c K™ and 
B C Km, then A x ß c l " x l m = Wl+m is 
i x ß = { ( a , b ) | a e A , b e ß } . 
We can easily extend this idea to define Cartesian products of collections of sets. Let 
A c T(M") and T> c T(Em) be two collections of subsets of M.n and Rm. Then 
.A x S will denote the class of all subsets of M™ x Km = K" + m that are of the form 
Ax B with 
AeAandBeT,. 

292 
MULTIPLE INTEGRALS 
Lemma 8.1.6 Products of cubes. Letn, m G N. Then Qn
k
+m = C£ x C£\ Hence, 
C G e™+m if and only if there are A G C£ and B G C^ such that C = A x B. 
Proof. This follows from Definition 8.1.1 above. 
□ 
The next result provides a partition of a binary cube into a disjoint union of smaller 
binary cubes. 
Lemma 8.1.7 Cubes as the union of higher-order cubes. Let h, k G Z+ and 
h < k. Then each cube in Q% is the union of2^k~h^n cubes in C£-
Proof. First, consider the result in R, i.e., assume that n = 1. Let q = (k — h). If 
q = 1, simply note that each interval / = [r, s) G C\ is the union of two "cubes" 
lo = [r, (r + s)/2 ) and h = \{r + s)/2, s ) in G\, since k = h + 1. An induction 
proves the result for any q € N and for n = 1. 
Now assume the result holds for all n < M, where M g N , and for any q G N, where 
q = k-has 
before. If C G C£+1, then C = A x B with A e e ^ a n d ß e e¿, by 
Lemma 8.1.6. Hence, by the induction hypothesis, A = U¿J4¿ and B = UjBj, where 
the first union contains 2qn cubes A¡ G C£+„ = C£ and the second union contains 
2« cubes Bj G <¿l
h+q = Cj.. Then 
C = ( i x ß ) = Uj Uj (^4¿ x Sj) 
is the union of 2qn ■ 2« = 2^n+^ cubes {A, x Bj) £ e£ + 1 So the result holds for 
n + 1 and thus for all n by induction. 
□ 
Volumes of the Unions of Cubes 
The volume of an individual cube is the product of its edge lengths, the volume of 
a disjoint union of cubes is just the sum of the individual volumes, and the volume 
of a Cartesian product of cubes is the product of the individual volumes. First, we 
establish these basic facts for binary cubes. 
Definition 8.1.8 Volumes of cubes in a grid. The volume ofCeS^ 
is defined as 
vn{C) = 2-kn. 
In particular, if/ = [p2-k, (p + l)2-fc ) G Q\, then ^(7) = 2~k 
is the length £{I) of /. And if C = {h x • • • x /„) € C£ with h G <Z\, then 
vn(C) = 2~kn = {2~k)n = £{h) ■ ■ ■ e(In), 
the product of n constant edge lengths. Note that vn+m(C) 
= vn+m(A 
x B) = 
vn(A) ■ vm(B) whenever C = (A x B) e e'fc
l+m with A e C£ and B G ef. 

JORDAN SETS AND VOLUME 
293 
Lemma 8.1.9 Additivity of volume. Let h < k. IfCeGh 
and ifC = U¿^4¿ with 
Ai G Cfe, then v(C) = J2i v{Ai). (Note that we write v for vn.) 
Proof. Lemma 8.1.7 shows that the number of cubes Ai in the union C = U¿A¿ is 
2n(k-h)_ 
SJ n c e v(A.j 
= 
2-kn 
for 
e a c h i w £ h a v e 
V 
v(Ai) = 2n(-k-h) ■ 2~kn = 2~hn = v(C). 
D 
Definition 8.1.10 Finite unions of cubes in a grid. Let Uk denote the collection of 
sets consisting of the finite unions of cubes C G Cfc. We assume that 0 G 1U — it is 
the union of an empty collection of cubes. Lemma 8.1.7 shows that if h < k, then 
C/j C life. Hence we see that U/, c Uk whenever h < k. We sometimes write UjJ 
instead of life to identify the dimension of the underlying space R™. 
Definition 8.1.11 Volumes of finite unions of cubes in a grid. Let E G IU. Then 
E €. Uk for all k > h. Hence, for each k > h, there is a finite collection of cubes 
£fc C efc such that E = UCe£kC. Note that, by Lemma 8.1.7, |£fc| = 2<~k-^n\Eh\, 
where |£fej is the number of elements in £fe. The volume ofE is defined as 
^ ) = HU e £ f e
c) = E c e £ f c ^ ) = 2-fcni^i- 
(8.4) 
The important point is that this number is well-defined: v(E) is the same regardless 
of which k > h we use in the definition. We sometimes write vn instead of v to 
identify the dimension of the underlying space R™. 
Lemma 8.1.12 Let E eUhandF 
G Uk. Then v(E U F ) < v(E) + v(F). Also, if 
E and F are disjoint, then v(E UF) = v(E) + v(F). 
Proof. Lctp = max(/i, k). Then E, F G Up by Definition 8.1.11. Hence, there 
are finite collections £, 3 c Cp such that E = Lice £ C and F = Ucg y C. Then 
G = E U F is the union of the collection 9 = £ U 3". Hence the first part follows 
from the estimate that |S| < |£| + |5"|. If the sets E and F are, disjoint, then £ and J 
are also disjoint and |S| = |£| + |3"|. This proves the second part. 
□ 
Lemma 8.1.13 Monotonicity of volume for unions of cubes. Let E G Uh and 
F &Uk and suppose E C F. Then v(E) < v(F). 
Proof. Once again, let p = max(/i, fc), so that E, F G Up. Then there are finite 
collections £, 3" c Cp such that E = Uce £ C and F = UC e g- C. Since E c F, 
we must have £ C 3\ It follows that |£| < |3"|, and hence v(E) < v(F). 
□ 

2 9 4 
MULTIPLE INTEGRALS 
Lemma 8.1.14 Volumes in Cartesian products. Let A e UJí and B e li™. Then 
(AxB)e 
Ul+m and vn+m{A 
x B) = vn(A) ■ vm(B). 
Proof. Let A = U¿A¿ and B - UjBj with finitely many Ai € C£ and Bj € Q™. 
Then Ax B = UtJ(A, x Bj) with finitely many (Ai x Bj) e C^+m. This follows 
from Definition 8.1.8. Then 
vn+m(A 
x B) 
= 
V 
vn+m(Ai 
x ß , ) = V 
wn(A,) ■ vm(Bj) 
again by the same definition. 
D 
Approximations by the Cubes of a Grid 
Our immediate task is to show that the elementary results that we have just established 
for unions of cubes may be transferred to the class of sets that can be approximated 
by unions of cubes. 
Definition 8.1.15 Approximating cubes of a bounded set. Let C£ = Cfc be the 
fcth-order binary grid in K". Let E be a bounded subset of Rn. 
Inner approximation. The collection of all cubes in £& that are contained in E 
is denoted by y%(E), or simply by 3k(E) if n is understood. The cubes in 3k(E) 
are called the kth-order inner cubes of E, and the union of all of these cubes is the 
kth-order inner approximation of E. It is denoted by Ik(E). 
Boundary approximation. The collection of all cubes in C^ that intersect both E 
and Ec = Rn \ E is denoted by T>k(E). The cubes in T>k(E) are called the kth-
order boundary cubes of E, and the union of these cubes is the kth-order boundary 
approximation ofE. It is denoted by D¡¿(E). 
Outer approximation. The set Ok (E) = Ik(E) L)Dk(E) is called the the kth-order 
outer approximation of E. Also, 0k(E) 
= 3k(E) U T>k(E) is the collection of all 
cubes in Qk that intersect E, The union of these kth-order outer cubes is Ok(E). 
Remarks 8.1.16 Note that 
•Dk(E) = Qk(E) \ %(E) and Dk(E) = Ok(E) \ Ik(E). 
(8.5) 
Also, Ik(E) and Ok(E) are the best inner and outer approximations of a set by the 
cubes of the grid Qk in the following sense. If F, G e life and if F c E c G, then 
also 
FcIk(E)cEc 
Ok(E) c G. 
(8.6) 

JORDAN SETS AND VOLUME 
2 9 5 
This is clear from the definitions. Finally, note that if h < k, then 
Ih(E) c Ik{E) CEC 
Ok(E) c Oh{E). 
(8.7) 
No surprises here: the approximations improve as the grids become finer. 
Lemma 8.1.17 IfE is a bounded set, then the limits 
v(E) = limkv(Ik(E)) 
andv{E) = \imkv(Ok(E)) 
(8.8) 
both exist andv(E) < v(E). 
Proof. The inclusions (8.7) above and the monotonicity of volume for unions of 
cubes, Lemma 8.1.13, show that 
v(h(E)) 
< v(Ik(E)) < v(Ok(E)) < v(Oh(E)) 
whenever h < k. Hence v(E) and v(E) both exist and v(E) < v(E). 
□ 
Definition 8.1.18 Inner and outer volumes of bounded sets. Let E be a bounded 
set in E n. Then the limits obtained above, 
v(E) = linifc v{Ik{E)) and v(E) = limfc v{Ok{E)), 
(8.9) 
are called, respectively, the inner and outer volumes ofE. 
Lemma 8.1.19 Monotonicity of the inner and outer volumes. If A and B are 
bounded sets and if A C B, then v_(A) < v(B) andv(A) < v(B). 
Proof. Definition 8.1.15 shows that if A c B, then 
h(A) c Ik(B) and Ok(A) C Ok{B). 
(8.10) 
Hence the proof follows. 
D 
Jordan Sets 
Definition 8.1.20 Jordan sets. A bounded set E C W1 is called a Jordan set if it has 
the same inner and outer volumes. In this case, v(E) = v(E) is called the volume of 
the Jordan set E and is denoted by v(E) = v{E) = v(E). 

2 9 6 
MULTIPLE INTEGRALS 
Theorem 8.1.21 Other definitions of Jordan sets. Let E c B " be a bounded set. 
Then the following are equivalent. 
(1) E is a Jordan set. 
(2) For each e > 0 there is a k 6 1+ such that v(Ok{E)) - v(Ik(E)) 
< e. 
(3) For each e > 0 there is a k 6 Z+ such that v(Dk(E)) < e. 
(4) For each e > 0 there is a k G Z+ and F, G E Uk such that F C E C G and 
such that v(G) — v(F) < e. 
(5) For each e > 0 there are Jordan sets F and G such that F C E C G and such 
that v(G) ~ v(F) <£. 
Proof. The sets Ik{E), Ok{E), and Dk(E) were defined in Definition 8.1.15. Note 
that Dk{E) = Ok(E) \ Ik{E) by (8.6) and, therefore, 
v{Dk{E))=v(Ok{E))-v(Ik{E)) 
by Lemma 8.1.12. This shows the equivalence of (2) and (3). The other parts of the 
theorem are left as exercises, 
ü 
Definition 8.1.22 Negligible sets. Jordan sets of zero volume are called negligible 
sets. 
Lemma 8.1.23 A set E is negligible if and only if for each e > 0, there is a Jordan 
set G such that E C G andv(G) < e. In particular, the empty set is a negligible set. 
Proof. This is left as an exercise. 
□ 
Theorem 8.1.24 Additivity of volume. If A and B are Jordan sets, then A U B, 
A n B, and A\B 
are also Jordan sets. Also, v(A U B) < v(A) + v{B). If A 
and B are disjoint, then v(A UB) = v(A) + v(B). Finally, if Ai are finitely many 
Jordan sets and A = U¿^4¿, then A is a Jordan set, v(A) < J2iv(^i) 
!n 8enera^ 
and v(A) = £V v(Ai) if the sets Ai are pairwise disjoint. 
Proof. We claim that Dk(A U B) c Dk(A) U Dk(B). 
In fact, if E is a fcth-order 
cube in Dk(A U B), then E intersects both A U B and its complement. Hence E 
contains a point from A or B and also a point not in A and not in B. Then we see 
that either E C Dk(A) or E C Dk(B). 
Then Lemma 8.1.12 shows that 
v(Dk(A U B)) < v(Dk(A)) + v{Dk{B)). 

JORDAN SETS AND VOLUME 
2 9 7 
This implies that A U B is a Jordan set by Part (3) of Theorem 8.1.21. Similarly, 
An B and A \ B are also Jordan sets. The other parts follows from Lemma 8.1.12. 
In fact, this lemma shows that 
v(Ik(AuB)) 
< 
v(Ik(A)) + v(Ik(B)) 
in general and 
(8.11) 
v(Ik(AöB)) 
= 
v(Ik(A)) + v(Ik(B)) 
if A and B are disjoint. (8.12) 
The generalization to finitely many sets is by induction. 
D 
Theorem 8.1.25 Cartesian products of Jordan sets. Let A c K" and B c Rm be 
Jordan sets. Then AxB 
is a Jordan set in R " x R m = Rn+m 
and vn+m{A 
x B) = 
vn{A)-vm{B). 
Proof. The notation is as in Definition 8.1.15. By Lemma 8.1.14, Ik(A) x Ik(B) 
and Ok(A) x Ok{B) both belong to U£ + m and 
vn+m(Ik(A) 
x Ik(B)) 
= 
vn(Ik(A))-vm(Ik(B)) 
(8.13) 
and 
vn+m(Ok(A) 
x Ok(B)) 
= 
vn(Ok(A))-vm(Ok(B)). 
(8.14) 
Hence both volumes in (8.13) and (8.14) approach the same limit ^"(^4) • vm(B) 
with increasing k. Since 
Ik{A) x Ik(B) cAxBc 
Ok(A) x Ok(B), 
this shows that A x B is a Jordan set with vn+m(A 
x B) = vn(A) ■ vm(B). 
D 
Blocks in Rn 
We can now show that not just binary cubes but also blocks in general are Jordan sets 
— our first important example. 
Definition 8.1.26 Blocks in K". A block B c K™ is a Cartesian product B = 
Ji x • • ■ x Jn of intervals. Here each J¿ is abounded interval of arbitrary type: open, 
closed, or half-open. Empty intervals and empty blocks are allowed. Lemma 8.1.27 
shows that each block is a Jordan set. 
Lemma 8.1.27 Blocks are Jordan sets. Any block B = J\ x • • • x Jn in W1 is 
a Jordan set with v(B) = C{J\) ■ ■ -¿{Jn)- Here £(Ji) = 0 if Ji = 0. Otherwise, 
£(Ji) = (SÍ — Ti) where r¿ is the left and s¿ the right end point of Ji. 

298 
MULTIPLE INTEGRALS 
Proof. If n = 1, then blocks are just intervals. We see easily that the conclusion 
is correct in this case because of the density of binary numbers, Lemma 8.1.4. The 
general case follows by induction on n, using Theorem 8.1.25. 
□ 
Lemma 8.1.28 A Jordan set E with a nonempty interior is not negligible. 
Proof. Use the maximum norm on R™, as defined in Definition 8.1.3. Let a G E°. 
Then there is a ball Br(a) C E. But in the maximum norm, Br(a) is a block with 
equal side lengths 2r. Hence, v(E) > v(BT(aj) = (2r)n > 0. 
D 
Jordan Sets and the Topology of R" 
Topological considerations are important in many arguments about volume and in-
tegration. We will discuss the topology of Jordan sets in more detail in Section 8.3. 
Here we make some observations in order to deal with general partitions of Rra. 
Lemma 8.1.29 Blocks in R" and the topology of Rn. Given a block E in R™ and 
e > 0, there is a closed block F and an open block G such that F C E° C E C 
E <zG and such that v(G) - v(F) < e. 
Proof. If n = 1, then closed (or open) blocks are closed (or open) intervals. The 
conclusion is correct in this case because of the density of binary numbers, Lemma 
8.1.4. The general case follows by an easy induction on n on the basis of Theorem 
8.1.25. 
D 
Theorem 8.1.30 Jordan sets in Rn and the topology of R™. Let E be a Jordan set 
and e > 0. Then there is a closed Jordan set F and an open Jordan set G such that 
FcE°cEcEcG 
and such that v(G) - v{F) < e. Also, F can be taken as a 
finite union of closed blocks and G as a finite union of open blocks. 
Proof. Given e > 0, find two unions of cubes P — U¿J3¿ and Q = UjCj such that 
P C E C Q and v(Q) - v{P) < e/2. Let K 6 N be such that the number of _B¿ s 
and Cj s is less than K. By Lemma 8.1.29, we can find a closed block F¿ c B° and 
an open block Gj D Cj such that 
v(Bi) - v{Ft) < e/{8K) and v(Gj) - v{Cj) < e/(8K) 
(8.15) 
for all i, j . Then F = U¿Fj and G = UjGj satisfy our requirements. 
D 
Theorem 8.1.31 Let E be a Jordan set and e > 0. Then there are two Jordan sets 
F and G and a number 5 > 0 such that F c E C G, v(G) ~ v(F) < e and such 

JORDAN SETS AND VOLUME 
299 
that Bs(u) C E whenever u G F and B$(v) C G whenever v G E. Here -B¿(x) is 
the ball of radius 5 around x G M.n. 
Proof. Find F and G as in Theorem 8.1.30, so that F is a compact set contained in 
the open set E° and E is a compact set contained in the open set G. Then the result 
follows from Theorem 4.5.48. 
□ 
General Partitions 
As mentioned in the introduction, the method of binary cubes is not the only way to 
define volumes. Different collections of sets may be used to define Jordan sets and 
their volume (with the identical result). There are some natural conditions that any 
such collection must satisfy. These are formulated below. 
Definition 8.1.32 Partitions. A collection CP of Jordan sets in Rn is called a (Jordan) 
partition (ofM.n) if these sets are pairwise disjoint and if their union is Rn. The 
pairwise disjointness of the sets means that any two distinct sets in CP are disjoint. 
Definition 8.1.33 Partitions of finite size. If E is a bounded set, then define its 
size as (size E) = sup { ||u — v|| | u, V É £ ) . A partition CP is called a partition of 
finite size if the set { (size F) \ P G CP } is a bounded set in R and if any bounded set 
in Rn is contained in a finite union of sets in CP. If CP is a partition of finite size, then 
(size CP) = sup { (size P) | P G CP } 
(8.16) 
is called the size of1?. 
Definition 8.1.34 Inner and outer approximations in a partition. Let CP be a 
partition of finite size. Let E be a bounded set in R™. Then 
Iy(E) 
= 
U { P | P e CP, and P e E } 
(8.17) 
Oy(E) 
= 
U { P | P e C P , a n d P n £ / 0 } 
(8.18) 
will be called, respectively, the inner and outer approximations ofE in CP. Definition 
8.1.33 of partitions of finite size shows that both unions above are finite unions. 
Hence Iy(E) and Oy(E) are Jordan sets. 
Theorem 8.1.35 Jordan sets in terms of partitions. A bounded set E in W1 is a 
Jordan set if and only if it satisfies the following condition (A). 
(A) For each e > 0 there is a 5 > 0 such that 
v(Oy(E)) - v(Iy(E)) 
< e 
(8.19) 

300 
MULTIPLE INTEGRALS 
whenever CP is a partition of finite size and (size 7) < 5. 
Proof. If E satisfies (A), then for each e > 0 there are Jordan sets ly (E) and Oy(E) 
such that Iy(E) C E C Oj>(E) and such that (8.19) is satisfied. In this case, Lemma 
8.1.17 shows that E is a Jordan set. 
To obtain the converse, let E be a Jordan set and e > 0. Apply Theorem 8.1.31 to 
obtain the Jordan sets F and G and S > 0 such that v(G) — f(-F) < £ and such that 
Bs(u) c £7 whenever u G F and Bs(v) c G whenever v (E E. An easy check 
shows that if ? is a partition of finite size with (size 7) < S, then 
F C h{E) 
cEc 
Oy{E) C G. 
In this case, v(Oy(E)) - v{I?(E)) < v(G) - v{F) < e. D 
Problems 
8.1 
Find Ik{E) and Ofc(.E) for fc = 0, 1, 2, 3, where 
1. E = { (x, y) | 0 < x, 0 < y, x2 + y2 < 1 } C R2, 
2. £J = { (x, y) | 0 < x, 0 < y, x2 + y2 < 1 } C M2, 
3. E = { (x, y) | 0 < x, 0 < y, x + y < 1 } c R2, 
4. E = { (x, y) | 0 < x, 0 < y, x + y < 1 } C R2. 
Here Ik{E) and Ok{E) are the inner and outer approximations of E, as in 8.1.15. 
8.2 
Find Ik(E) and Ok(E) for all fc e Z +, where £ is the set of points in the unit 
square [ 0, 1 ] x [ 0, 1 ] with at least one rational coordinate. Find the inner and outer 
volumes (in this case areas) of E. 
8.3 
For each n € N find a negligible set En such that E = UneN is not negligible. 
8.4 
For each n e N and for each k = 0, 1, ..., n let 
Enk = {(x,y)\x 
= k/n, 0 < y < 1/n } C R2, 
£VJ = U£=0i?„fc, and £ = U„eN-En- Show that all these sets are negligible. 
Problems on Cross-Sections 
Definition 8.1.36 Cross-sections. Denote the points in R"+1 = R" x 1 as (x, y) 
with x e Rn and y € R. The cross-section of a set E C R n + 1 = Rn x R at y e R 
is Ey = {x I x G R™, (x, y) G E } C R". 

JORDAN SETS AND VOLUME 
301 
Definition 8.1.37 Variations of cross-sections. Let E be a bounded set in R n + 1 
and J a bounded interval in R. Then the variation of the cross-sections of E over J 
hVE(J)=vn((UyejEy)\(nyeJEy)). 
8.5 
Let E = { (x, y) | |x| + |yj < 1 } C M x R. Find VE{J) for any interval J. 
8.6 
Let E = { (x, y) \ x2 + y2 < 1 } C R x R. Find VE(J) for any interval J. 
8.7 
Let JB be the bounded region in the xy-plane between the parabolas y = x2 
and y = 2x2 — 1. Find the variations of the cross-sections of E. Consider the cross-
sections both with the vertical x = constant lines and with the horizontal y — constant 
lines. 
8.8 
Let E C K x R be the set of points in the unit square [ 0, 1 ] x [ 0, 1 ] with at 
least one rational coordinate. Find VE(J) for any interval J. 
8.9 
Let E be the bounded region in the xyz-space R2 x R between the cylinders 
x2 + z2 = 1 and y2 + z2 = 1. Find the variation of the cross-sections of E with the 
horizontal z = constant planes. 
Definition 8.1.38 Continuously changing cross-sections. Let E be a bounded set 
in R™ x K and / an interval in R. We will say that cross-sections of E change 
continuously on / if for each pel 
and for each e > 0 there is a ö > 0 such that 
vE(in(p~s, p + s)) <£. 
8.10 
Let E C R™+1 = R™ x R be a bounded set. Let / be an interval such that 
Ey — 0 for all y g I. Assume that each Ey is a Jordan set in W1 and that E has 
continuously changing cross-sections on (J n J) for any interval J. Show that E is 
a Jordan set in R n + 1. (Hint: First, assume that the continuity of the cross-sections 
is uniform on / in the following sense: for each e > 0 there is a 5 > 0 such that if 
VE(lnJ)<e 
whenever i{J) < 8.) 
8.11 
Show that both ¡x| + \y\ < 1 and x2 + y2 < 1 are Jordan sets in R2. 
8.12 
Let / be an interval. Let a : / —> Kn and t : / —► R be two continuous 
functions. Let A be a Jordan set in R". Let E c Mn x R be defined in terms of its 
cross-sections Ey as Ey = 0 if y $ I and Ey = a(y) + t(y)A if y G i\ Show that 
E is a Jordan set. (Hint: First, assume that A is a cube in C£.) 
8.13 
Cylinders. Let A be a Jordan set in R" and c = (a, h) e R™ x R with 
h^Q. 
Then the set 
C = C{A, c) = { (x, 0) + ¿(a, h) | x e A, 0 < í < 1 } C R n + 1 

302 
MULTIPLE INTEGRALS 
is called a cylinder. Show that all cylinders are Jordan sets. 
8.14 
Cones. Let A be a Jordan set in R™ and c = (a, h) € R™ x R with ft^O. 
Then the set 
K = K{A, c) = { (1 - t)(x, 0) + ¿(a, h) | x G A, 0 < t < 1 } C R n + 1 
is called a cone. Show that all cones are Jordan sets. 
8.15 
Show that all triangles in R2 are Jordan sets. 
8.16 
Show that the "general tetrahedra" 
E = | x = (xi, ..., xn) e R™ 0 < Xi, Y^ 
xi < ! } 
are Jordan sets in Rra. 
8.17 
Show that all Euclidean balls 
E = { x = (Xl, ..., xn) G Rn | J2iXi ~ crf 
-R2} 
are Jordan sets in Kn. Here c = (ci, ..., cn) is the center of the ball. (Hint: First, 
assume that c = 0 and proceed by induction o n n g N.) 
8.18 
Show that all ellipsoids 
E = { x = (Xl, ..., xn) e Rn | $ ^ r - 2 ( x t - a)2 
<R2] 
are Jordan sets in Rn. Here c = (ci, ..., cn) is the center of the ellipsoid and 
(ri, ..., r„) is a fixed vector in Rn with r¿ ^ 0. 
8.19 
Show that the region E = { (x, y, z) | x2 + z2 < 1, y2 + z2 < 1 } is a 
Jordan set in R3. This is the region between two (ordinary circular) cylinders. 
8.20 
Let En = { (x, y) \ x = p/n, y = q/n, p, q = 0,1, ..., n } C R2, with 
n G N. Show that E = UnEn is not a Jordan set in R2, but all of its cross-sections 
are Jordan sets in R. 
8.21 
Give an example of a bounded set E in R2 such that E is not a Jordan set in R2, 
but all of its cross-sections Ey C R are Jordan sets in R with v(Ey) — £(Ey) = 1/2 
for all y in the interval [0, 1 ]. 

INTEGRALS 
303 
8.2 
INTEGRALS 
A nonnegative function / : R™ —> R+ is said to be an integrable function if the 
region E¡ c R™+1 = Rn x R under the graph of this function is a Jordan set in 
Rra+1. In this case, the (n + l)-dimensional volume of E¡ is called the integral of 
f. Hence, the main problems in integration are to find out whether this special type 
of set is a Jordan set and, if it is, to compute its volume. 
Definition 8.2.1 Regions under graphs. Given / : R™ —> R, define 
Ef = { (x, y) e R" x R | 0 < y < /(x) } c R n + 1 
as the region under the graph of f. 
Note that only the positive values of / are 
important in this definition. 
Definition 8.2.2 Integrals of nonnegative functions. A nonnegative function / : 
R™ —> R+ is said to be integrable if Ef is a Jordan set in R™+1. In this case, the 
integral of f is defined as / / = 
vn+1(Ef). 
Definition 8.2.3 Positive and negative parts of a function. Given a real-valued 
function / : X —> R on a set X, define 
f+(x) = max(/(i), 0) and f~(x) = max{-f(x), 
0) = - min(/, 0), 
the positive part and the negative part of f. Note that both the positive and the 
negative parts are nonnegative functions and / = / + — /~. 
Definition 8.2.4 Integrals of general functions. A function / : Rn —> R is said to 
be integrable if its positive and negative parts / + and /~ are both integrable. In this 
case, the integral off is defined as / / = / / + — f f~. 
Definition 8.2.5 Bounded functions of bounded support. A function / : Rn —► R 
may be integrable only if both Ef+ and Ef- are bounded sets in Mn+1. This happens 
if and only if there is a number M 6 R and a bounded set S c R™ such that 
- M < /(x) < M for all x £ Rn and /(x) = 0 if x £ S. Such a function is called 
a bounded function of bounded support. The set S is called a support for f. Any 
bounded set is contained in a finite union of cubes. Hence we may always assume 
that a function of bounded support has its support in a finite union of cubes. 

304 
MULTIPLE INTEGRALS 
Step Functions 
Step functions constitute an especially important class of integrable functions. They 
are simple to integrate, and it is easy to establish their basic properties. Yet it turns 
out that any integrable function can be approximated by step functions. As a result, 
most of the fundamental properties of integrable functions can be obtained from the 
corresponding properties of step functions. 
Definition 8.2.6 Characteristic functions. Let E c R™. The characteristic (or 
indicator) function of E is the function \E '■ Kn —* R defined by \E (X) = 1 if 
x e E and XE(X) = 0 if x £ E. 
Definition 8.2.7 Step functions. A function <p : Rn —► R is called an kth-order 
step function if it can be expressed as a linear combination ip = J2iaiXCt of the 
characteristic functions of a finite number of cubes C¿ G C£. 
A step function has the constant value a¿ on the set C¿ and the value 0 everywhere 
else. For instance, a step function on R takes constant non-zero values on finitely 
many binary intervals and is zero elsewhere. 
Lemma 8.2.8 Step functions are integrable. If p = Y^iaiXCi is a step function, 
then ip is integrable and J (p = J^¿ aiVn(Ci). 
Proof. First, assume that a¿ > 0 for all i. In this case, we have 
Ev = \Ji(Ci x [0, 
ai)). 
This is a Jordan set in R n+1, since it is a finite union of the blocks (C¿ x [O, a¿)). 
These blocks are pairwise disjoint because the cubes C¿ in the grid C£ are pairwise 
disjoint. Hence, by the additivity of volume, Theorem 8.1.24, 
J p = vn+1(Ev) = Y . f 1 ^ 
x [0, a i)) = 
J^.ouv^Ci). 
In general, let /3¿ = max(0, a¿) and 7, = max(0, —a¿). Then A = X^Axc¿ is the 
positive part of tp and p = X]¿7JXC¿ is the negative part of p. Both of these parts 
are nonnegative step functions, and therefore integrable. We verify easily that the 
integral of ip is still J <p = J2i aiyn(C¿)- 
n 
Lemma 8.2.9 Integration of step functions as a positive linear operator. The 
class S of step functions Rra —» R is a vector space, and integration f : § —> R 
is a positive linear operator. More explicitly, if ¡p, <p' G S and a, a' € R, then 

INTEGRALS 
3 0 5 
(cup + a'<p') G § and f(a<p + a'<p') = a f (p + a' f <p'. Also, if ip < ip, then 
S<P<S*I>-
Proof. If ip and <p' are of the same order, then the lemma is clear. If the orders of <p 
and ip' are h and h', then we see that tp and ip' are both of order k = max(/i, h'). 
Hence the proof follows. D 
Riemann Condition for Integrability 
In general, there are no efficient algorithms to decide if a given set is a Jordan set 
(though Problem 8.51 helps in many cases) and no simple way to compute the volume 
of a Jordan set. In the case of a region under the graph of a function, however, the 
situation is different. A bounded function of bounded support can be approximated 
by step functions. A condition for the integrability of a function can be formulated, 
and its integral computed, in terms of these approximating step functions. 
Definition 8.2.10 Approximations by step functions. Let / : R™ —> K be a 
bounded function of a bounded support S. For any nonempty F c l ™ let 
inf (/, F) = inf { /(x) | x e F } and sup(/, F) = sup { /(x) [ x e F } . 
For each k e Z + define 
<Pk = E C G ej i n f (/' C)xc and ^fc = ]T C e e„ sup(/, C)xc, 
(8.20) 
respectively the kth-order lower and upper approximations of f by step functions. 
These functions really are step functions, since the sums in their definitions are finite. 
In fact, if C does not intersect 5, then inf (/, C) = sup(/, C) = 0. Since the support 
S of / is a bounded set, the number of C G C£ that intersect S is finite. 
Lemma 8.2.11 Monotonicity of the lower and upper approximations. 
Ifh<k, 
then iph < ipk < f < ipk < iph. 
Proof. This follows from the definitions. The approximations get better as the grid 
on which the step functions are defined becomes finer. D 
Definition 8.2.12 Lower and upper sums. The integrals 
I** 
= 12ceeJní{LC)vn{C) 
= ^ceek'mí{f>C)2~kn 
( 8' 2 1 ) 
¡ ^ = E C e C f c
 SUP(/'
 C)
 W"(
C) = E C e C f c
 S UP^'
 C)
 2~
kU 
(8-22) 

3 0 6 
MULTIPLE INTEGRALS 
are called the kth-order lower and upper (Riemann) sums for f. We also write 
Lk{f) = 
fk 
and Uk(f) = / Vfc-
Theorem 8.2.13 Limits of lower and upper sums. If f : Rn —> R is a bounded 
function of bounded support, then lim/- Lk{f) and limfc Uk(f) exist. 
Proof. Lemma 8.2.9 shows that integration is a positive linear operator on the 
vector space of step functions. Combining this fact with Lemma 8.2.11, we see that 
Lh{f) < Lk(f) < Uk{f) < Uh(f) whenever 0 < h < k. Hence both sequences 
are monotone and bounded, and therefore convergent. 
□ 
Definition 8.2.14 Lower and upper integrals. Let / : K™ —> K be a bounded 
function of bounded support. Then linife Lk(f) and linife Uk{f) obtained in Theorem 
8.2.13 are called the lower integral off and the upper integral off. They are denoted 
as 
limfcLfe(/) = limfc j
V
k = j j and lin* W ) 
= limfc f *k = / / . 
Theorem 8.2.15 Riemann condition for integrability. Let f : Mn —> R be a 
bounded function of bounded support. Then f is integrable if and only if J f — Jf. 
In this case, J f is the common value of these limits. 
Proof. Recall that Ef is the region under the graph of / as defined in Definition 
8.2.1. First, consider a nonnegative function/. Assume that Jf = Jf. Given £ > 0, 
find a k G N such that 
j^k 
~ Jv>k = vn+\E^k) 
- vn+1(EVk) 
< e. 
(8.23) 
Hence, E1¡,k and EVk are two Jordan sets in Rn+l 
such that EVk C E¡ C E^k and 
such that vn+l{Ei>k) 
- vn+1(EVk) 
< e. Then Theorem 8.1.21 shows that Ef is a 
Jordan set in Krl+1. Hence / is integrable. Also, 
/ ' ^k = vn+1(E^k) < vn+\Ef) 
< vn+1(EVk) = J^k 
shows that vn+1 (Ef) = / / = / / = / / . 
Conversely, assume that / is (nonnegative and) integrable. This means that Ef is 
a Jordan set in Kn+1. Given e > 0, find a k e N so that the inner and the outer 
approximations of Ef by the /cth-order cubes in Kn+1 satisfy 
vn+\Ok{Ef))-vn+\lk{Ef))<e. 

INTEGRALS 
307 
Now any fcth-order cube H e C]?+1 is of the form H = C x [r, s), where C e S J ! 
and r = 2~kp, s = 2~k(p + 1), with p e l . 
If H c /*(£/), then we see 
that 0 < r < s < inf(/, C). Otherwise H would contain points outside of E¡. 
Hence, we see that H c EVk. Therefore, Ik(Ef) 
C EVk. 
Similarly, we see that 
E^k c Ok(Ef). Hence 
Ji>k ~ ¡Vk < vn+1(Ok(Ef)) - vn+l{h{Ef)) 
< e. 
Therefore, Jf = Jf. 
If / is not necessarily nonnegative, then one considers the 
positive and negative parts separately. This is left as an exercise. 
O 
Supports of Lower and Upper Approximations 
Let / be a bounded function with a bounded support S. Lemma 8.2.16 relates S and 
the supports of its lower and upper approximations, as defined in Definition 8.2.10. 
In this lemma Ok(S) is the outer approximation of S defined in Definition 8.1.15. 
Hence Ok(S) is the union of all fcth-order cubes that intersect S. 
Lemma 8.2.16 Let Sbe a supportfor a bounded function f and let h, k £ N, h < k. 
Then Oh{S) is a support for the kth-order lower and upper approximations ipk and 
V'fe off. 
Proof. Each x e X belongs to a unique cube C e Qk- If x ^ Ok(S), then C is 
disjoint from Ok(S). Hence, C is also disjoint from S. Therefore, 
sup(/, C) - inf(/, C) = 0 and ^ ( x ) = Vk(x) = 0. 
This means that Ok(S) is a support for (pk and ipk- Then Oh(S) is also a support for 
ipk and ipk, since Ok(S) C Oh(S). 
□ 
Theorem 8.2.17 Let K be a compact support for a bounded function f. If K is 
contained in an open set G, then there is an h € N and a compact set K' such that 
K C K' C G and K' is a support ofipk and ipkfo1" aH k > h. 
Proof. Theorem 4.5.48 shows that if K is compact, G is open, and if K C G then 
there is a compact K' and an ft e N such that Ok{S) C K' c G for all k > h. Then 
the proof follows from Lemma 8.2.16 above, 
ü 
Integration as a Positive Linear Operator 
Lemma 8.2.18 Let f and / ' be two bounded functions ofbounded support. Iff < /', 
thenlf<lf'and]f<]f>. 

308 
MULTIPLE INTEGRALS 
Proof. If / < /', then also pk < p'k
 anc^ ^fc — ^k f°r m e respective approximating 
step functions. Then the proof follows. 
□ 
Lemma 8.2.19 Let f : Rn —> R be a function. If for each e > 0 there are integrable 
functions /', / " such that f < f < f" and such that / / " — / / ' < £. then f is 
integrable. 
Proof. From Lemma 8.2.18 we obtain / / ' < / / < / / < / /". Hence 
for all e > 0. Therefore, / / = / / and / is integrable. 
□ 
Lemma 8.2.20 A function f : R™ —> R is integrable if and only if for each e > 0 
rÄere are step functions tp and ip such that p < f < ifi and f ip — f ip < e. 
Proof. If / is integrable, then J pk — J ipk < e for a sufficiently large k, where ¡p¡¡ 
and ipk are approximating step functions. In the other direction, by Lemma 8.2.18, 
P < / < ip implies ¡p<jf<$f<§'4>. 
Hence, if / ip - J p < e, then 
/ / - / / < * ■ 
° 
Theorem 8.2.21 Integral as a limit of sums. Let f : 1 " —> R be an integrable 
function. Then 
/ / = lim f cV 
/(c)2- f e n. 
(8.24) 
Here Zk is the set of centers of the cubes in the grid Cfc, defined in Definition 8.1.1. 
It is understood that the above sum is taken over the centers with /(c) ^ 0. 
Proof. Let dk = Zlcee f(c)xc- 
We see that Pk < $k < i>k- Therefore, by 
Lemma 8.2.18 above, 
LkU) < ($k = Y. P7 /(c) 2~kn ^ w ) -
Then the result follows by Theorem 8.2.15. Ü 
Remarks 8.2.22 If K is a support for the function / in (8.24), then the collection Zk 
of all fcth-order centers can be replaced by Xk C Zk, the collection of all fcth-order 
centers that are contained in K. In fact, / vanishes on the remaining centers in 
(Zk \ Xk). 

INTEGRALS 
309 
Remarks 8.2.23 The role of the centers. In Theorem 8.2.21 above, the centers 
of the cubes do not have any particular significance. If uc is an arbitrary point 
from each cube C £ Cfc, then J f = lim^ X^cee / ( u c ) 2~kn. 
In fact, if Afe = 
S c e e f(uc)xc, 
one still has tpk < A^ < tpk. Then the proof follows by the same 
arguments as before. 
Theorem 8.2.24 Positivity and the linearity of integration. The class 3 of inte-
grable functions M.n —> l i s a vector space and integration J : 3 —» R is a positive 
linear operator. More explicitly, if f, / ' G 3 and a, a' £ R, then 
{af + a'f) £ 3 and ¡(af + a'/') = a / / + a' / / ' . 
Also, if f <f, 
then 
jf<Jf. 
Proof. Given e > 0, use Corollary 8.2.20 to find the step functions tp, tp, tp', tp' such 
that tp<f<tp,tp'<f'<tp',Jtp-Jtp< 
e/2, and / tp' - / tp' < e/2. Then 
(<£ + ¥>') < (/ + /') < (ip + 4>')imd¡(4> + '4)')-¡(ip + ip') <£. The last inequality 
follows from the linearity of integration on step functions, Lemma 8.2.9. Hence 
(/ + /') is integrable. The integrability of af follows easily from atp < af < aip 
if a > 0, or from atp < af < a<p if a < 0. This shows that 3 is a vector space. 
The linearity and positivity of integration j' : § —> R on step functions are known 
from Lemma 8.2.9. Then we verify easily, using Theorem 8.2.21, that integration 
/ : 3 —» R on integrable functions is also a positive and linear operator, 
a 
Theorem 8.2.25 The class 3 of integrable functions Rn —> M is closed under multi-
plication and under taking absolute values, minima, and maxima. 
Proof. To show that / • g is integrable, first assume that / and g are nonnegative 
integrable functions. Since both are bounded functions, there is an M such that 
0 < /(x) < M and 0 < g(x) < M for all x G l " . Given r\ > 0, find the step 
functions ip, ip, A, \i such that 0 < tp < f < tp and 0 < A < g < p, and such that 
/ tp — J tp < r\ and J p — J A < r¡. Then 0<<p-\<f-g<ip-p,. 
Also, 
4>-p-tp-\ 
= (ip- tp)p + tp(p - A) < M(-tp -<p) + M(p - A) 
shows that /(?/> • p) - J{<p ■ A) < M(J ip - J tp) + M(f p - J A) < 2Mr¡. This 
can be made less than given any s > 0. Hence / • g is integrable. In the general 
case, one has / ■ g = (/' — f")(g' 
— g"), with the respective positive and negative 
parts. These parts are all integrable by Definition 8.2.4. Hence, / • g is integrable. 
The integrability of |/| follows from |/| = / ' + /". Then 
max(f,g) 
= 
(l/2)((/ + g) + \f - g\) and 
min(/ ) 5) 
= 
(l/2)((/ + 5 ) - | / - f f ! ) 
are also integrable. 
G 

310 
MULTIPLE INTEGRALS 
Integrals of Characteristic Functions 
Theorem 8.2.26 Integrability of characteristic functions. A characteristic func-
tion XA '■ Rn ~^ K+ is integrable if and only if A is a Jordan set in M.n. In this case, 
¡XA=vn(A). 
Proof. Let <~pk and ipk be the fcth-order approximating step functions for XA, as 
in Definition 8.2.10. An easy check shows that <pk = Xik(A) and ipk = Xok(A)-
Hence, J ipk — / <Pk = vn(Ok{A)) — vn(Ik{A)) converges to zero if and only if A 
is a Jordan set. 
D 
Definition 8.2.27 Integration over a set. Let / : R™ -t R be a function and E a 
set in W1. If / • XE is integrable, then / ( / X B ) is called the integral of f over E. 
It is also denoted as JE f or as fE /(x)dx if one wants to indicate that x denotes a 
general point in Mra. 
Integrals of Continuous Functions 
Remarks 8.2.28 Continuous functions on compact sets. Recall from Definition 
4.4.2 that a function / : E n —> K is said to be continuous on E C Rn if for each 
a € E and for each e > 0, there is a 6 > 0 such that |/(x) — /(a)| < e whenever 
||x — a|| < ö and x G E. Theorem 4.5.46 shows that if E = K is a compact set, then 
the continuity of / on K is uniform. That is, for each e > 0, there is a 5 > 0 such 
that |/(x) — /(x')| < e whenever ||x — x'|| < 6 and x, x' £ K. Also, a continuous 
function on a compact set is bounded, as shown in Theorem 4.5.44. 
Theorem 8.2.29 Integrability of continuous functions. Any continuous function 
on a compact Jordan set is integrable on that set. 
Proof. The idea of the proof is to exploit the uniform continuity of /. If we take 
small enough cubes, then the lower and upper approximations for / will be almost the 
same, except on boundary cubes. But the boundary cubes don't matter in computing 
the integral, since their volume can be made as small as desired. 
In detail: let K be a compact Jordan set and g : K —> R a continuous function. 
The theorem states that / = QXK is integrable. Here /(x) = g(x) if x e K and 
/(x) = 0 otherwise. Now g is bounded on K. If —M < g(x) < M for all x £ K, 
then — M < /(x) < M for all x £ l " , Hence / is a bounded function and vanishes 
outside of the bounded set K. 
Let T] > 0 be given. Use the uniform continuity of g on K to find a 5 > 0 such that 
|/(x) - /(x')| < r\ whenever ||x - x'|| < Ö and x, x' e K. Find a fc0 G ^ + such 

INTEGRALS 
311 
that if fc > fco and if C € Cjt, then ||x — x'|| < S for any x, x' 6 C. For a fixed 
fc > fco, let d s be the fcth-order inner cubes of K and Bj s the fcth-order boundary 
cubes of K. Also, let on = inf (/, C¿) and /3¿ = sup(/, C¿). Then by our choice of 
ko, 0 < ßi — ai <r¡. If </?& and tpk are the fcth-order lower and upper approximations 
of /, as in Definition 8.2.10, then we see that 
All these functions are step functions. Hence 
< 
Vv(Ik(K)) + Mv(Dk(K)) 
< 
Vv{K) + 
Mv(Dk(K)). 
Given e > 0, choose r¡ > 0 so that r¡v(K) < e/2. 
Then choose fc > fco so 
that 2M v(Dk(K)) 
< e/2. This is possible since K is a Jordan set and therefore 
limfe v{Dk(K)) 
= 0 by Lemma 8.1.17. It follows that / ipk - J <Pk < £• Hence g is 
integrable by Theorem 8.2.15 or by Lemma 8.2.20. 
□ 
Fundamental Theorem of Calculus 
Theorem 8.2.30 Let f : K —> K be continuous on a closed interval I = [a, b}. 
Define F : I -> R by F(x) = J f ■ X[a,x] far all x e Io = (a, b). Then F is 
continuous on I, differentiable on Io, and F'(x) = f(x) for all x G Io. 
Proof. Continuity of / on the compact set / implies that / is bounded on /. Let 
0 < f(x) < M for all x G I. If a < r < s < b, then we see that F(s) - F(r) = 
J f ' X(r,s]- Hence \F(s) - F(r)\ < M\s — r\. Therefore F is continuous on /. 
Next, given x € Io and e > 0, use the continuity of / at x to find r0 and SQ such 
that a<r0<x<so<b 
and such that \f(x) — f(y)\ < s whenever ro < y < SQ. 
Letp = (f(x) -e) andq =- (f(x) + e). Thenp < f(y) < q for all y G [r0, sQ\. 
Therefore, if ro < r < s < s0, then 
p = (f(x) -e)< 
(F(s) - F(r))/(s - r) < (f(x) + e) = q. 
Hence F'(x) exists and F'(x) = f(x). 
O 
Corollary 8.2.31 Let f : I -> M. be as in Theorem 8.2.30. Let G : I -> R be 
continuous on I and differentiable in Io with G'(x) = f(x) for x G Io. 
Then 
G(s) — G(r) = j f ■ XJ whenever a < r < s < b and J is any type of interval with 
the end points r and s. 

312 
MULTIPLE INTEGRALS 
Proof. Let ip(x) = F(x) — G(x) + G(a) for x £ I. Then ip is continuous on I, 
vanishes at a, and is differentiable in Io with tp'(x) = 0 for all x e Io. Then the 
mean value theorem, Theorem 5.1.13, shows that <p(x) = 0 for all x € I. Hence 
G(s) - G(r) = F(s) - F(r) = J f ■ X(r, s ] by Theorem 8.2.30. 
G 
Integrals in Product Spaces 
Notations 8.2.32 Functions of two variables. Let n, m 6 N. As usual, we may 
consider R n + m as R" x Rm. Points in W = Wl+m are denoted by w = (u, v) 
with u e U = Rn and v e V = Rm. The coordinate projections P : W —> U and 
Q : W —> V are defined as P(u, v) = u and Q(u, v) = v. Functions / : W —> M 
on VF = R n + m are considered as functions of two variables / : U x V —* R with 
values /(u, v) £ R. For each fixed u £ [/, we have a function /(u, •) : V -^ R 
and for each fixed v 6 V, we have a function /(•, v) : U —> R. 
Notations 8.2.33 Iterated integrals. Let / : M^ —> R be a bounded function of a 
bounded support S c W. Let ^4 = PS and £? = Q5 be the projections of S on U 
and V. These sets are also bounded. Hence /(u, •) : V —> R is a bounded function 
of bounded support for each u E Í / and /(•, v) : ¿7 —> R i s a bounded function 
of bounded support for each v E V. Therefore these two functions have lower and 
upper integrals. The lower integral of /(•, v ) : U —> R is denoted as 
/ / ( u , v)du 
and similarly for the other integrals. Here the symbol du is only used to indicate that 
this lower integral is taken with respect to u, keeping v fixed. The result depends on 
v. 
This integration defines a new function g : V —» R by g(y) = J /(u, v) du, v e K 
This function is also a bounded function of bounded support. In fact, if |/(w) | < M 
for all v/ eW, then \g(v)\ < Mvn{A). 
Also, if v £ B = QS, then /(u, v) = 0. 
Therefore the integrals of g : V —> R also exist. The lower integral of g : V —> R is 
denoted as 
/ff(v)oiv = / / / ( u , v)dudv. 
In this last expression, the order of du dv is important. It indicates that the first 
integration is with respect to u and the second integration is with respect to v. 
The same observations apply to upper integrals. In fact, we see that eight different 
iterated integrals can be defined, taking into account the four possible combinations 
of upper and lower integrals and the two possible orders of integration. If any one of 
the integrals in these expressions exists, then lower and upper integral signs may be 
replaced by the integral sign. 

INTEGRALS 
313 
These observations give us our first special case of Fubini's theorem. 
Lemma 8.2.34 Let C e 6™+m be a cube in R n + m and f = \c- Then 
I / ( w ) d w = / / /(u, 
v)dudv. 
Proof. We have, by Lemma 8.1.6, that C£+m = C£ x ef. Hence C = A x B with 
A €G^ and Be C£\ Hence /(u, v) = Xc(u, v) = XA(U) • XB(V), and 
/ 
XAxB(u,v)dudv 
= 
/ 
XA(u)xE¡(v)dudv 
= vn(A) j XB{v)dv 
= 
vn(A) ■ vm(B) = vn+m(A 
x B) 
= ^+m(C) = yXc(w)dw. 
Here we have used Theorem 8.2.26 on the integration of characteristic functions. The 
relation vn+m{C) 
= vn(A) ■ vm(B) is given in Definition 8.1.8 of the volumes of 
cubes. 
□ 
By combining this special case with the technique of approximation by step functions, 
we prove the general version of Fubini's theorem, Theorem 8.2.37 below. 
Lemma 8.2.35 Let ft : Rn+m 
-^Rbe 
integrable functions such that 
j /i(w)dw = J 
f /i(u,v)dudv. 
(8.25) 
Then (8.25) also holds for a (finite) linear combination f = ^2i ctifi, on £ R. 
Proof. This follows directly from the linearity of integration, Theorem 8.2.24. Note 
that this theorem is applied three times here for the three separate integrals that appear 
in (8.25). 
□ 
Corollary 8.2.36 Fubini's theorem for step functions. Let ip : Rn+m 
—> R be a 
step function. Then 
<p(w)dw= 
/ / <p(u, 
v)dudv. 
Proof. Follows directly from Lemmas 8.2.35 and 8.2.34 above. 
D 

314 
MULTIPLE INTEGRALS 
Theorem 8.2.37 Fubini's theorem. If f : W -+M. is integrable, then 
Í /(w) dw = J J /(u, v) du dv = J J f(u, v) du dv, 
(8.26) 
with the corresponding results in the other order of integration. 
Proof. Lemma 8.2.20 shows that given e > 0, there are two step functions ip and if) 
such that if < f < if) and such that J(ip - ip) < e. Then 
Í <p{u, v) du < j /(u, v) du < j tp(u, v) du. 
(8.27) 
Here we have used the positivity properties of lower integrals, Lemma 8.2.18, and 
the fact that the first and last integrals exist. We obtain, again by using the positivity 
properties of lower and upper integrals and the fact that the first and last functions in 
(8.27) are integrable functions V —> R, 
/ </?(w)dw 
= 
/ / ip(u, x)dudv 
< I I /(u, v)dudv 
< 
/ / /(u, v) du d\ < j j ip(u, v) du dv = / ip(w) dw. 
The equalities follow from Corollary 8.2.36. Now we also have that 
/ tyj(w) dw < / /(w) dw < / ?/>(w) dw 
and that (J ip(yv) dw — J tp(w) dw) < e. Hence we see that J J /(u, v) dudv 
exists and is equal to J /(w) dw. The proof of the second equality in (8.26) uses the 
same arguments. 
□ 
Notations 8.2.38 Iterated integrals. Fubini's theorem has an obvious generaliza-
tion to decompositions into more than two components. In particular, Rn may be 
decomposed into n one-dimensional subspaces spanned by the vectors in the standard 
basis of W1. In this case, Fubini's theorem shows that 
/ /(x) dx = / ■ • • / f(x1, ..., xn) dxi ■ ■ ■ dxn 
(8.28) 
if all these integrals exist. On the right-hand side of (8.28), there are n integrals of 
functions of one variable. The convention is that they have to be performed in the 
following order. For the first integral consider / as a function of x\ only, keeping 

INTEGRALS 
3 1 5 
the other variables fixed. The integral of this function results in a function of (n — 1) 
variables 
fi(x2, ..., xn) = / f{x1, ..., 
xn)dxi. 
For the second integral consider f\ as a function of x2 only, keeping the other 
variables fixed. The integral of this function results in a function of (n — 2) variables 
f2(x3, 
•.., xn) = j fi(x2, 
..., 
xn)dx2. 
After (n — 1) integrals, the last integral to be performed is J fn-\{xn) 
dxn. Fubini's 
theorem states that if J*/(x)cix exists, then it is equal to the result of this last 
integral. Note that the existence of these n iterated integrals does not imply the 
existence of j /(x) dx; this has to be assumed separately. Problem 8.29 asks for a 
counter-example. 
Notations 8.2.39 Integrals on M2. If / is an integrable function on K2, then 
/ / = / f{%, y) dxdy= 
j f(x, y)dxdy = / / f(x, y) dx dy. 
Here the first three integrals show some of the common notations used for the integral 
of / on R2. The last integral is an iterated integral. It shows two integrals to be 
computed in the order explained in Notations 8.2.38 above. 
If E is a Jordan set in the xy-plane, then 
/ = / f(x,y)dxdy= / / 
f(x,y)dydx 
E 
JE 
JI J Ex 
are some of the notations for the integral of f\E- In the iterated integral, the first 
integration is with respect to y. It is the integral of f(x, •) : R —> R over the cross-
section Ex of E with the x = constant line. The second integration can be performed 
over any interval I in the x-axis such that Ex = 0 whenever x £ I. Hence I is any 
interval that contains the projection of E on the x-axis. 
In most examples Ex is an interval in the y-axis, and Ex is not empty if and only 
if a < x < b. The initial and final points of the interval Ex depend on x. If these 
points are denoted, respectively, as gi(x) and g2{x), then 
f(x,y)dxdy= 
/ 
f(x,y)dydx. 
(8.29) 
E 
Ja Jg\ (x) 
This form is applicable in many examples. 

316 
MULTIPLE INTEGRALS 
Notations 8.2.40 Integrals on R3. Let / : R3 —> R be a function and let G be a 
Jordan set in R3. Assume that / is integrable over G. Then in many applications, 
fG f can be expressed as 
rb 
rgiix) 
rh-i(x,y) 
f(x,y,z)dxdydz 
= I 
I 
/ 
f(x,y,z)dzdydx. 
(8.30) 
G 
Ja J gi(x) Jh-l(x,y) 
This form is similar to (8.29) for integration in R2. Here the first integral is with 
respect to z. Integration is performed on the portion of the a;, y = constants line that 
lies in the region G. In (8.30) it is assumed that if this portion is not empty, then it is 
an interval with the initial point h\(x, y) and the final point h2(x, y). After the first 
integration one obtains an integral in R2. It is over a set E in R2. This set consists 
of all points (x, y) for which the line x, y = constants intersects G. Hence E is the 
projection of G on the rry-plane. The remaining two integrals are integrals on M2. 
Then one uses the notations in 8.2.39 above. 
Notations 8.2.41 Another decomposition of R3. There is another common form 
for integrals on R3. It is obtained by applying Fubini's theorem to the decomposition 
of W = R3 as U x V = R2 x R. One obtains 
f(x,y,z)dxdydz 
= / 
I / 
f(x,y,z)dxdy\ 
dz. 
(8.31) 
In the first integral, z is a constant, and this integral is performed on the set 
Gz = 
{(x,y)\(x,y,z)eG}cR2. 
This is the cross-section of G with the z = constant plane. The integral on Gz is 
computed as an integral in R2. The result is a function of z only. One obtains (8.31) 
by taking [p, q] an interval on the z axis such that Gz = 0 for z ^ [p, q}. 
Example 8.2.42 Volume between two cylinders. We compute the volume of the 
region E bounded by the two cylinders x2 + z2 = 1 and y2 + z2 = 1. 
The region E is the set of all (x, y, z) € R3 such that x2 + z2 < 1 and y2 + z2 < 1. 
We know that this is a Jordan set by Problem 8.19. We would like to compute f \E-
We apply Fubini's theorem as expressed in (8.31) above. Hence 
v (E) = 
XE(X, y, z)ddxdydz 
= I 
[ / 
f(x, y, z)dxdy\ 
dz. 
The first integral is on the xy-plane. In the first integral the z coordinate is fixed and 
the integration is over the cross-section of E with a z = constant plane. Hence Gz = 
{(x, y) | x2 < (1 - z2), andy2 < (1 - z2) }. We see that Gz = 0if¿ 2 > 1, and 

INTEGRALS 
3 1 7 
Gz is a square with side length 2(1 — z2)1/2 
if z2 < 1. Therefore the first integral is 
the area of this square. Hence we obtain 
v3{E) = 4 / 
(1 - z2)dz = 8 - (8/3) = 16/3. 
As an exercise, let us also decompose the rcyz-space in a different way. Let U be the 
yz-plane and V the a;-axis. In this case, the cross-section of E with the x = constant 
plane is 
F(x) = { (y, z) | z2 < (1 - x2) and y2 + z2 < 1 } . 
This is the part of the unit disc y2 + z2 < 1 between the lines z = ±(1 — x2)1/2. 
This area can be computed in different ways. We apply Fubini's theorem again and 
obtain 
H i - * 2 ) ' / 2 
, ( i - * 2 ) 1 / 2 
v2{F(x)) 
= 
/ 
/ 
dydz 
./-(1-X 2) 1/ 2 J-(1-2 2)V2 
. ( I - . 2 ) 1 / 2 
= 
2 / 
( l - ^ 2 ) 1 / 2 ^ 
i - ( l - X 2 ) V 2 
= 
2(cos_1 x + a:(l - a:2)1/2), and, 
i 
2/ 
vá(£) 
= 
/ 
^(F(z))dz 
i 
= 
4 / (cos~1:r + :r(l-a; 2) 1 / 2) d a ; = 16/3. 
Jo 
This example illustrates how the complexity of the computations may depend upon 
the order of integration. 
A 
Example 8.2.43 All Euclidean balls are Jordan sets. This fact is proved in one 
way in Problem 8.17. Here is another argument. Proceed by induction on the number 
of dimensions. The unit ball in M is ( — 1, 1), which is a Jordan set. Now assume 
that the unit ball A C Rn is a Jordan set in R". The function / : A —> K+ defined 
by /(x) = (1 — HxH2)1/2 is continuous on A. Hence the region under its graph is a 
Jordan set Ej in Mn. This is the upper half of the unit ball. Similarly, the lower half 
is also a Jordan set. Hence the unit ball is a Jordan set, since the union of two Jordan 
sets is still a Jordan set. Finally, any ball is obtained from the unit ball by scaling and 
a translation, and therefore the same arguments apply to any ball. 
A 
Example 8.2.44 Volume of the unit ball in Rn. The volume of the unit ball in 
a Euclidean space can be computed by an induction on the number of dimensions. 
Let $fc be the volume of the unit ball in K". Note that the volume of any ball of 
radius r in R" is rn$fc. Also, i?i = i( — l, 1) = 2. Assume that dk is known. Let 

318 
MULTIPLE INTEGRALS 
W = R"+1 = Rn x R = U x V and write w = (u, t-) e W with u e [/ = R™ and 
D 6 V = I . Let 
E={(u,v)eW\ 
\\u\\2 + v2 < 1 } 
be the unit ball in Rra+1. The cross-sections of E with v = constant subspaces are 
Ev = { u S U I ||u||2 < 1 - v2 }. Hence Ev is a ball of radius (1 - v2)1'2 in R". 
Therefore vn{Ev) = (1 — v2)n/2dk- This gives 
$n+i 
= 
/ XB(W)G!W= / / Xß(u, 
v)dudv 
i 
r i 
vn(Ev)dv 
= 2dk 
(l-v2)n/2dv. 
-1 
JO 
The computation of the last integral is left as an exercise. We obtain 
■d2n = 7Tn/n\ and i92n_x = 2nTTn~1/(l ■ 3 • 5 • • • (2n - 1)) 
for all n G N. 
A 
Problems 
8.22 
Volumes of cylinders. Let A be a Jordan set in Rn and c = (a, h) € Rn x : 
with h T^ 0. Show that the volume of cylinder 
C = C(A, c) = { (x, 0) + i(a, h) | x e A, 0 < t < 1 } C R n + 1 
is wn+1(CY) = \h\ vn{A). (Cylinders are Jordan sets by Problem 8.13.) 
8.23 
Volumes of cones. Let A be a Jordan set in R" and c — (a, h) £ R™ x ' 
with h ^ 0. Show that the volume of cone 
K = K{A, c) = { (1 - i)(x, 0) + f(a, h) \ x e A, 0 < í < 1 } C R™+1 
isü n + 1(X) = |/i|w"(yl)/(n + l). (Cones are Jordan sets by Problem 8.14.) 
8.24 
Show that the volume of the tetrahedron 
E = | x = (xi, ..., xn) e R" 0 < Xi, ^Xi 
< 1 | 
is vn(E) = 1/n!. (Tetrahedra are Jordan sets by Problem 8.16.) 
8.25 
Find the volume of the ellipsoid 
E = { x = {Xl, ..., xn) e R" | ^ r r ^ ^ i - cA2 < R2 } . 

INTEGRALS 
319 
Here c = {c\, ..., cra) is the center of the ellipsoid and {r\, ..., rn) is a fixed vector 
in W1 with r¿ 7^ 0. (Ellipsoids are Jordan sets by Problem 8.18.) 
8.26 
Integrate the function f(x, y, z) = x2 + y2 + z2 over the tetrahedron E — 
{{x,y,z)\\x\ 
+ \y\ + 
\z\<l}. 
8.27 
Let p e R . Find the volume V of 
E = { {x, y, z) I x2 + y2 + z2 < 1, p < 2 } . 
How do we know that E is a Jordan set? 
8.28 
Integrate f(x, y, z) = \z\ over E defined in Problem 8.27 above. 
8.29 
Give an example of a nonintegrable function / : R2 —> M for which the 
integrals J f f(x, y) dx dy exist. (This means that g(y) = J f(x, y) dx exists for 
each y and defines an integrable function g : R —> R. Such an example shows that 
the converse of Fubini's theorem is false.) 
8.30 
Let / : Rn -> R be an integrable function. Points in R n + 1 = R " x l are 
denoted as z = (x, y). Let 
A 
= 
{ (x, y) e Rn x R I x <E R", /(x) < y < 0 } and 
B 
= 
{ (x, y) e Kn x R I x e Rn, 0 < y < /(x) } . 
Let D c R n + 1 be a compact set containing Ali B. Let G : D —> R be an integrable 
function. Show that 
[ G(z)dz- 
[ G{z)dz= 
[ 
[ 
G{x,y)dydx. 
JB 
JA 
JX JO 
Here the integral with respect to y £ R is expressed in familiar notations. If 
/(x) > 0 then this is an integral over the interval [ 0, /(x) ] as defined in this course. 
If /(x) < 0 then this is the negative of the integral over the interval [ /(x), 0 ]. 
8.31 
If / : R2 -> R is integrable, show that for any e > 0, 
Es = i y € R 
/ f{x, y)dx- 
f(x, y) dx > e I 
is a Jordan subset of R with v(Ee) = 0. 
8.32 
Let / : R" —* R be a bounded function of compact support. For each s > 0, 
let Ee = { z I u(f, z) > e } , where w(/, z) is the oscillation of / at z (Problem 
4.96). Show that / is integrable if and only if EE is a negligible set for all e > 0. 

320 
MULTIPLE INTEGRALS 
8.33 
Show that if a bounded function of compact support is integrable, then the set 
E of its discontinuities is a countable union of negligible sets Ei. That is, there is a 
sequence of negligible sets Ei, i e N , such that E = {Jie^Ei. Give an example to 
show that the set E of discontinuities need not itself be negligible. 
8.34 
Let / : W1 —* R be a bounded function of compact support. If the set E of 
discontinuities of / is a countable union of negligible sets Ei (that is, E — U¡eN-E¡), 
then / is integrable. (Problems 8.33 and 8.34 give a necessary and sufficient condition 
for the integrability of a bounded function of compact support. This is known as 
Lebesgue's theorem.) 
The following set of three problems provides a proof of the bounded convergence 
theorem. 
8.35 
Let 0 < ß. Let K and B^ be compact Jordan sets such that Bk C K and 
ß < v(Bk) for all k £N. Then show that there is an x G K that belongs to infinitely 
many Bk s. 
8.36 
Let / be a step function with compact support in a Jordan set K. Assume 
that 0 < /(x) < M. Given a > 0, let D be the set of x e K such that a < /(x). 
Then show that J f < Mv(D) + av(K). 
8.37 
Let /„ be a sequence of integrable functions. Assume that all /„ s have 
support in a compact Jordan set K and that |/„(x)| < M for all n € N and for all 
x e K. If limn / n(x) = /(x) for all x € K and if / is also integrable, then show 
that lim„ J fn = f f-
8.38 
Give a counterexample to show that the conclusion in Problem 8.37 is false 
if there is no compact set that contains the support of all / ns. 
8.39 
Give a counterexample to show that the conclusion in Problem 8.37 is false 
if there is no number M which is an upper bound for all |/ n|s. 
8.40 
Give an example to show that the limit function in Problem 8.37 need not be 
integrable. 
8.41 
Give examples of /„ and / to show that several of the hypotheses in Problem 
8.37 are not necessary for the conclusion of the problem to be true. 

IMAGES OF JORDAN SETS 
321 
8.3 
IMAGES OF JORDAN SETS 
The image of a Jordan set under a diffeomorphismis a Jordan set and its volume can 
be computed by the change of variables formula. In this section, we establish the 
first point and we prove the second point for the special case of an invertible linear 
transformation. 
We begin with Theorem 8.3.2, which provides a nice topological characterization 
of Jordan sets. This establishes an important connection between Jordan sets and 
the topology of R™. All further results on volume and integration depend upon this 
connection. 
It is convenient to repeat here a few definitions that will be useful in what follows. A 
set in K" is called negligible if it is a Jordan set of zero volume, as stated in Definition 
8.1.22. The fcth-order boundary cubes of a set E, as defined in Definition 8.1.15, are 
the /cth-order cubes that intersect both E and its complement Ec = 1 " \E. The union 
of these cubes is Dk{E) = Ok{E) \ h(E), the fcth-order boundary approximation 
ofE. 
Topological Definition of Jordan Sets 
Lemma 8.3.1 Boundary cubes. Any boundary cube of E intersects dE. Hence 
Dk{E) C Ok(dE). 
The converse is not true. A cube that intersects dE is not 
necessarily a boundary cube of E. 
Proof. Any boundary cube contains points a £ E and b £ E. The line segment 
joining these two points contains a point from dE, and this point also belongs to 
the boundary cube because cubes are convex. To disprove the converse: any cube 
contains points from its boundary but, trivially, does not intersect its complement. 
D 
Theorem 8.3.2 Boundaries of Jordan sets. A bounded set is a Jordan set if and 
only if its boundary is a negligible set. 
Proof. Let E be a Jordan set. Given e > 0, use Theorem 8.1.30 to find a closed 
Jordan set F and and an open Jordan set G such that 
f c £ ° c £ c £ c G a n d v(G) - v(F) < e. 
Hence dE = ~E\ E° C G\ F. Also, G \ F is a Jordan set and 
v(G\F)=v(G)-v{F) 
<s 

322 
MULTIPLE INTEGRALS 
by Theorem 8.1.24. Hence dE is between the Jordan sets 0 and G\F 
with v(G \ 
F) - D(0) = v(G \ F). This implies that dE is a Jordan set of zero volume. 
Conversely, assume that E is a bounded set of negligible boundary. Given e > 0, 
find an integer k such that v{Ok{dE)) < e. Hence, by Lemma 8.3.1, 
Dk(E) C Ok(dE) and, therefore, v{Dk{E)) < e. 
Then Theorem 8.1.21 shows that E is a Jordan set. 
□ 
Corollary 8.3.3 Interiors and closures of Jordan sets. A bounded set is a Jordan 
set if and only if both its interior and closure are Jordan sets of the same volume. 
Proof. This is left as an exercise. 
□ 
Jordan Sets Under Diffeomorphisms 
We begin by showing that diffeomorphisms transform Jordan sets into Jordan sets. It 
is convenient to recall a few facts about diffeomorphisms. First, if / : A —* Y is a C1 
diffeomorphism, then A and B = f(A) are both open sets, / : A —* B has an inverse 
g : B —-> A, and both / and g have continuous derivatives / ' : A —> L(X, Y) and 
g' : B -^ L(Y, X). Second, / : A —► B preserves the topological properties of sets 
in A. Open sets, compact sets and boundaries are transformed into corresponding 
open sets, compact sets and boundaries. In particular, df{E) — f(dE) for any E 
such that E c A. This last condition is necessary to ensure that not only E but also 
dE are contained in A. 
In the context of Jordan sets we will mostly use the maximum norm 
||x|| = ||(xi, ..., x n)|| m = max(|xi|, ..., |x„|). 
This norm was introduced in Definition 8.1.3, where we noted that all balls in this 
norm are open blocks. Hence, all such balls are Jordan sets. The interior of a cube 
C is a ball B such that B c C c R W e will call the radius of B the radius of C. In 
particular, the cubes in the fcth-order grid are all cubes of radius 2~fc_1. 
Lemma 8.3.4 Let A C W1 be an open set and f : A —» Rn a Q1 mapping. Let 
K C A be a compact set. Then there is a number M with the following property: if 
a cube C of radius r is contained in K, then f(C) is contained in a cube of radius 
Mr. 
Proof. The continuous function / ' : A —> L(Rn, Rn) is bounded on the compact 
set K. Hence there is an M such that ||/'(x)||L(RnRn) < M for all x e l 
Let C 

IMAGES OF JORDAN SETS 
323 
be a cube contained in K. If u, v e C, then the line segment joining these points is 
in C C K. Therefore ||/'(w)ll < M for every w on this segment. Then the mean 
value theorem 5.1.13 implies that ||/(u) — /(v)|| < M||u — v||. We see that this 
implies the conclusion of the lemma. 
□ 
Theorem 8.3.5 Let A be an open set in W1 and f : A —» Rn a dijfeomorphism. Let 
E be a Jordan set in A with E C A. Then f(E) is also a Jordan set. 
Proof. The basic idea is to combine the fact that dE is negligible with the preceding 
lemma. A cover for dE of small volume will be mapped by / to a cover for df(E) 
of small volume. So df(E) is also negligible, which shows that f(E) is a Jordan set. 
In detail: the set E is compact (as it is closed and bounded), and contained in the 
open set A. Theorem 4.5.48 shows that there is a compact set K and a number 5 > 0 
such that E C K° c K C A. Note that in this case there is a number ó > 0 such 
that x e K whenever ||x — a|| < 6 and a € E. This follows again from Theorem 
4.5.48. Hence we see that if a cube of radius less than 6/2 intersects E, then it is 
contained in K. 
Now E is a Jordan set. Hence it has a negligible boundary dE. Given e > 0, find 
an outer approximation Ok{dE) of this boundary with volume v(Ok{dE)) 
< e. 
Then Ok(dE) = U¿C¿ is a finite union of fcth-order cubes d of radius r = 2~k~1. 
Without loss of generality, assume that 2r = 2~k < 6. In this case we see that all 
Ci s are contained in K. Therefore all /(C,) s are contained in cubes of radius Mr, 
where M is the number obtained in Lemma 8.3.4. Now f(8E) 
c f(Ok(dE)) 
= 
/(U¿C¿) = U¿/(C¿) shows that f(dE) is contained in a set of volume 
v(Uif(Ci)) 
< £.v(/(Ci)) < Y,iMn<Ci) 
( 8 J 2 ) 
= 
Mnv(Ok{dE)) 
< Mne. 
(8.33) 
Hence f(dE) = df(E) is a negligible set and f(E) is a Jordan set. 
□ 
Jordan Sets Under Isomorphisms 
An isomorphism T : M.n —> K™ is an invertible linear transformation. Therefore it is 
also a diffeomorphism. It follows that TE is a Jordan set whenever E is a Jordan set, 
by Theorem 8.3.5. In this case, more is true. We show that there is a fixed number 
p(T) > 0 such that v(TE) = p(T) v{E) for all Jordan sets E. This number will be 
called the volume multiplier of T. 
Definition 8.3.6 Translations. Let a e M.n. Then translation by a is the transfor-
mation Ta : I n -> Mn given by Ta(x) = a + x. If E cRn, 
then a + E denotes 

324 
MULTIPLE INTEGRALS 
the image of E under translation by a. That is, a + E consists of all vectors (a + x) 
with x e £ , 
Theorem 8.3.7 Volumes under translations. If E c R™ is a Jordan set and 
a G R™, then a + E is also a Jordan set and v(a + E) = v(E). 
Proof. 
Since a translation is a diffeomorphism, we know that a + E is a Jordan 
set whenever E is a Jordan set. Translation of a block is also a block with the same 
side lengths. Hence the volumes of blocks are preserved under translations. We see 
that this is also true for finite disjoint unions of blocks, by the additivity of volume. 
Hence the volumes of inner and outer approximations of sets are preserved under 
translations, since they consist of finite disjoint unions of cubes. It follows easily that 
v(a + E) = v(E) for any Jordan set E. 
D 
Lemma 8.3.8 Let T : R™ —> W1 be an isomorphism. Let Co = [0, l) n be the unit 
cube in R™. Let p(T) = v(TC0). Then p(T) > 0 and v{TC) = p{T)v{C) for any 
cube C in any kth-order grid Cfc, k £ R 
Proof. All cubes C G Cfc are translations of each other. The same is true for their 
images TC. Hence v{TC) is the same for all C G Cfc. The unit cube Co is the disjoint 
union of 2kn cubes in Cfc. Therefore TCa is the disjoint union of 2kn translates of 
TC, for any C G Cfc. Hence v{TC) = 2"fcnu(C0) = v{C)p(T). 
Also, TC0 has 
a nonempty interior since the interior of Co is nonempty and T : R™ —> R" is an 
isomorphism. Hence p(T) = u(TCo) > 0, by Lemma 8.1.28. 
□ 
Lemma 8.3.9 Let T : Rn —■> R™ be an isomorphism. Let Co = [0, l) n be the unit 
cube in Rn and let 
fcsl 
Let p(T) = V(TCQ). 
If F = U¿Fj is a finite union of 
cubes Fi G ek, then v(TF) = p{T)v{F). 
Proof. The cubes in a grid are pairwise disjoint. So if F¿ G Cfc, then the F¿ are 
disjoint and therefore the images TF¿ are also disjoint, since T is an isomorphism. 
Hence v(F) = J2iv(^) 
a n d v{TF) = £,«(77^). Then the result follows from 
Lemma 8.3.8. 
D 
Theorem 8.3.10 Let T : Rra —» Rn be an isomorphism. Then there is a number 
p(T) > 0 such that v{TE) = p(E) v{E)for all Jordan sets E C R". 
Proof. Let p(T) = v(TC0), where C0 is the unit cube of Rn. Let E be a Jordan 
set and e > 0. Find inner and outer approximations of E, F and G such that 

IMAGES OF JORDAN SETS 
3 2 5 
F C E C G and v(G) — v(F) < e. Here F and G are both finite disjoint unions of 
cubes Fi, Gj € Cfc. Then, by Lemma 8.3.9, 
p{T)v(F) = v(TF) < v(TE) < v(TG) = p(T)v(G). 
We see that both numbers v(TE) and p(T)v(E) arebetweenp(T)w(F) 
andp(T)v(G). 
Hence 
\v(TE) - p(T)v(E)\ 
< (p(T)v(G) - p(T)v(F)) 
< ep(T). 
Since e > 0 is arbitrary, this implies that v(TE) = p{T)v(E). 
□ 
Definition 8.3.11 Volume multipliers of isomorphisms. Let T be an isomorphism 
R™ —> W1. Then the volume multiplier of T is defined as the number p(T) > 0 
obtained in Theorem 8.3.10. Hence v(TE) = p(T)v(E) for all Jordan sets E in Rn. 
Theorem 8.3.12 Volume multipliers of isometries. Let T : Rn —> Rn be an 
isometry of the Euclidean space K™. Then p{T) = 1. 
Proof. The unit ball i? = { x | | | x | | < l } , with the usual Euclidean norm (rather 
than the maximum norm), is a Jordan set and is invariant under any isometry T. Also, 
it has nonzero volume since its interior is not empty. Therefore v(TB) = v(B) = 
p{T)v{B), which shows that p(T) = 1. D 
Corollary 8.3.13 Let (ui, ..., u„) be an orthonormal basis for R™ and A¿ > 0. 
Then the "rectangular box" 
B = { x = SiUi + • • • + snun | 0 < Si < Xi, i — 1, ..., n, } 
is a Jordan set in M71 and v(B) = Ai • ■ • An. 
Proof. Let (ei, ..., en) be the standard basis of R™. Then the transformation 
defined by Tu¿ = e¿ is an isometry and takes B into a rectangular block with side 
lengths A¿. Hence v(B) = v(TB) = Ai • • • An. 
D 
Corollary 8.3.14 Volume multipliers of scaling transformations. Let t > 0. If 
T : R™ -> R" is defined by Tx = t x, then p(T) = tn. 
Proof. Apply Corollary 8.3.13 with Xi = t, i = 1, ..., n. 
□ 

3 2 6 
MULTIPLE INTEGRALS 
Determinants and Volume Multipliers 
Remarks 8.3.15 A review of determinants. If X is an n-dimensional vector space, 
then a determinant on X is any nonzero alternating multilinear function ijj : Xn —> 
R. Each ordered basis E = (ei, ..., en) defines a unique determinant ip® on X 
such that VE(eii • • • i en) = 1- Theorem C.7.2 shows that if T : X —► X is a 
linear transformation and if E = (ei, ..., e„) is a basis for X, then the number 
ip(Tei, ..., Ten)/ip(ei, 
..., e„) is independent of the choice of the determinant 
ip and of the basis E. It is called the determinant of T and denoted by det T. Note 
that det T = ipEÍTei, ..., Ten) for any basis E. 
Remarks 8.3.16 Euclidean determinants. There is a special situation in Euclidean 
spaces. If E and U are two orthonormal bases for a Euclidean space, then they define 
the same determinant up to a factor of ±1. Such a determinant is called a Euclidean 
determinant. Hence there are exactly two Euclidean determinants in a Euclidean 
space. 
Theorem 8.3.17 Determinants as volume multipliers. Let T be an isomorphism 
of the Euclidean space M.n. Then p(T) = \ det T\. 
Proof. Bythe spectral theorem, Theorem3.6.4,Thasane¿ge«£as¡.sE = (ei, ..., en) 
Recall from Definition 3.6.3 what this means: E is an orthonormal basis for Kn such 
that Te¿ _L Te., whenever i ^ j . It follows that if we put u¿ = Te¿/||Te¿||, then 
U = (ui, ..., u n) is another orthonormal basis for K" such that Te¿ = A¿u¿ for all 
i, with scalars A¿ > 0. In fact, each A¿ > 0 since T is an isomorphism. Therefore 
detT 
= 
MTei, 
. . . , T e n ) 
(8.34) 
- 
±Vu(Tei, ...,Te„) 
(8.35) 
= 
±ipv{\1u1, 
..., Afcufc) 
(8.36) 
= 
±(Ai ••• Afc)Vu(ui, ..., u„) 
(8.37) 
= 
±(Ai---Afc). 
(8.38) 
Here (8.35) follows from the fact that determinants with respect to any two orthonor-
mal bases differ by a factor of ±1 only. To obtain (8.37), we use the multilinearity 
of the determinant. The last step is by the definition of ^ j . 
Now T takes the rectangular box 
B = { x = sie1 H 
h snen | 0 < s¿ < 1, i = 1, ..., n } 
with volume v(B) = 1 to the rectangular box 
TB — { x = siui + • • • + snUfc | 0 < si < Xi, í = 1, ..., n } 

IMAGES OF JORDAN SETS 
3 2 7 
with volume v(TB) = (Ai • • • Afe). Hence 
p(T)=v(TB) 
= (X1 ■■■Xk) = \detT\. 
D 
Problems 
8.42 
A plane Ax + By + Cz = D divides an ellipsoid 
{x/af 
+ (y/b)2 + (z/c)2 = 1 
into two parts. Find the volumes of these parts. (One part may be empty.) 
8.43 
Find the volume of E = { {x, y) \ {2x + yf + (x - y)2 < 1 } c R2. 
8.44 
Find the volume of E = { (x, y) | \2x + y\ + \x - y\ < 1 } C R2. 
8.45 
Find the volume of 
E = { {x, y, z) | \x\ + \x + y\ + \x + y + z\ < 1 } C K3. 
8.46 
Let H c B C A c K™. Assume that A is compact, H is open, and tA C H 
whenever 0 < t < 1. Show that A, B, and H are all Jordan sets of the same volume. 
8.47 
Show that any open ball or any closed ball with respect to any norm on M.n 
is a nonnegligible Jordan set. 
8.48 
Let T : Rn —> R" be an isometry with respect to an arbitrary norm on W1. 
Then show that p(T) = 1. 
8.49 
Denote the vectors in R"+1 = R n x R a s (x, y) with x e Rn and y e K. 
Let a £ l " b e a fixed vector. Define R : Rn+1 -> 3Rn+1 by i?(x, y) = (x + ya, y) 
for all (x, y) G R™+1. Then show that p{R) = 1. 
8.50 
Let E be a bounded set in K™ such that if 0 < í < 1, then ¿F c E. Prove or 
disprove that E is a Jordan set. 
8.51 
Show that any bounded convex set is a Jordan set. 
8.52 
Let F C R™ be a bounded set and r > 0. Show that U x eF#r(x) is a Jordan 
set. 

328 
MULTIPLE INTEGRALS 
8.53 
Let E be an open set in X — Rm and F an open set in Y = Rn. 
Let 
u : E -> U = Mm and v : F -> V = Rn be two diffeomorphisms. Show that 
w(x, y) = (u(x), v(y)) defines a diffeomorphism 
w.{ExF)^W 
= Rm+n = Rm x Rn. 
Also show that p(w'(x, y)) = p(u'(x)) p(v'(y)) for x G £, y 6 F. 
8.4 
CHANGE OF VARIABLES 
Introduction 8.4.1 Changing the integration variable is a familiar technique from 
basic calculus courses. Let y : A —■> R be a continuously differentiable function 
defined on an open interval A. Assume that y'(x) > 0 for all x G A. If [a, 6] C A 
and if <7 is integrable over [y(a), y(b)}, then 
¡■y(b) 
rb 
/ 
ff(2/)dy= 
/ g{y{x))y'{x)dx 
(8.39) 
The generalization of this result to multiple integrals is one of the major theorems 
of this course. This result is stated as Theorem 8.4.16 below. The key step in the 
proof of this theorem is an approximation theorem. It compares the volumes of 
the images of small sets under a diffeomorphism and under the derivative of that 
diffeomorphism. Such an approximation is fairly easy to obtain locally for small 
balls about a fixed point. We need a uniform version of this result. The proof of this 
uniform approximation theorem depends on a uniform mean value theorem. Hence 
we first review some mean value theorems. 
Notations 8.4.2 Diffeomorphisms and compact sets. For this section, we review 
a few basic facts and some standard notation. Let A be an open set in X = K™. Let 
<p : A —> Y = Rn be a diffeomorphism. 
This means that <p : A —> Y is a 
continuously differentiable function, the image of A under ip is an open set B = 
ip(A) C Y, and there is a continuously differentiable inverse function ip = tp~l : 
B —» X. 
If K is a compact set and if K c A, then Theorem 4.5.43 shows 
that K is mapped to a compact set H = <p(K) C B by the continuous function 
ip. Also, the derivatives ip' : A —> L(X, Y) and ip' : B —> L(Y, X) are continuous 
functions. Hence, by Theorem 4.5.44, they are bounded on the compact sets K and 
H respectively. Finally, by Theorem 4.5.48, if K is a compact set, A is an open set, 
and K c A, then there is a S > 0 such that Bs(a) c A for all &e K. 

CHANGE OF VARIABLES 
329 
A Review of Mean Value Theorems 
We review several mean value theorems. Throughout the discussion, tp is a diffeo-
morphism. 
Theorem 8.4.3 Basic mean value theorem. Assume that 
w(i) = (tu + (1 - f)v) e A and that \\f'(w(t))\\ 
< M 
for all t G [0, 1]. Then ||^(v) -<¿>(u)|| < M ||v - u||. 
Proof. This is a restatement of the basic mean value theorem, Theorem 5.1.13. G 
Theorem 8.4.4 Mean value theorem on convex sets. Let C be a convex set and 
C CA. //||<¿>'(x)|| < MforallxG 
C then 
M v ) - v ? ( u ) | | < M | | v - u | | 
for all u, v £ C. 
Proof. This follows from Theorem 8.4.3. 
□ 
Theorem 8.4.5 Local mean value theorem. For each a G i and for each e > 0, 
there is a 5 > 0 such that Bg(&) C A and 
||¥>(v) - <p(u) - ^'(a)(v - u)|| < e ||v - u|| 
for all u, v e Bs(a). 
Proof. Let a £ A. Since A is open, there is a So > 0 such that Bg0(a) c A. Also, 
there is a S such that 0 < 5 < So and such that 
||v?'(u) — tp'(a.)\\ < £ whenever u e Bg(a). 
This follows from the continuity of <p' : A —*• L(X, Y). Define A : A —> Y as 
A(x) =<^(x) -f(a) 
-ip'(a)(x-a), 
x e A. 
(8.40) 
We see that ||A'(x)|| = \\<p'(x) - y'(a)ll < £ for all x e Bs(a). 
Now ßÄ(a) is a 
convex set. Then Theorem 8.4.4, the mean value theorem on convex sets, shows that 
||A(v) - A(u)|| = |Mv) - ^(u) - ^'(a)(v - u)\\ < e ||v - u|| 
(8.41) 
for all u, v 6 Bs(a). 
D 

330 
MULTIPLE INTEGRALS 
Theorem 8.4.6 Uniform mean value theorem. For each e > 0 and for each 
compact set K C A there is a 6 > 0 such that 
Bs(a) C A and ||^(v) - <^(u) - v'(a)(v - u)|| < e ||v - u|| 
(8.42) 
for 
all a G K and for 
all u, v G BS(SL). 
Proof. Let K be a compact set, K C A, and e > 0. Use Theorem 4.5.48 to find 
another compact set Ko C A and a number <50 > 0 such that £?,50 (a) C Äo for all 
a £ K. The continuous function <p' : A —> ¿(X, Y) is uniformly continuous on the 
compact set K0 c A. That means we can find a <5 such that 0 < 5 < 8Q and such 
that ||<^'(v) — </?'(u)j| < e whenever u, v G K0 and ||v — u|| < 5. Now if a G K, 
then B¿¡(a) C ßä0(a) C K0 C A. Define A : A -> Y as in (8.40). As before, we 
see that || A'(x) || < e for all x G Bs(a). Then (8.42) follows as in (8.41) above. D 
Definition 8.4.7 Affine approximations. The affine approximation of <p at a G A 
is the affine mapping i9a : X —-> Y given as 
0a(x) = <¿>(a) + ¥>'(a)(x - a), x G X. 
(8.43) 
Note that #a(a) = </?(a) and #a(v) - i?a(u) = v?'(a)(v - u), a G A, u, v G X. 
Lemma 8.4.8 Inverses of affine approximations. Let a £ i and b = y (a). Leí 
■#a ¿>e f/ze a^zne approximation of ip at a. 77ien $ a is invertible and 
tf-^y) 
= a + (^'(a))-1(y - b) = V(b) + V'(b)(y - b), y e Y. 
(8.44) 
A/so, ?9

CHANGE OF VARIABLES 
3 3 1 
ball is r; the radii of the reduced and enlarged balls are (1 — t)r and (1 + t)r for a 
specified parameter t with 0 < t < 1. (Nothing in the argument will depend upon 
the particular choice of norm.) 
Theorem 8.4.9 Uniform approximations. Let ip : A —► Y be a diffeomorphism 
and K C A a compact set. Let 0 < t < 1. Then there is a 6 > 0 such that 
ifa£K 
and if0<r<6, 
then Br(a) C A and 
t?a(ß(1_i)r(a)) C <p(Br(a)) C 0 a(B ( 1 + t ) r(a)). 
(8.45) 
Proof. Choosing a and M. Apply Theorem 4.5.48 to find an a > 0 and a compact 
set Ifo such that Ba(a) c K0 c A for all a £ K. Also, find a number M > 0 
such that ||<p'(x)|| < M for all x G X0. Such a number exists, since the continuous 
function ip' : A —► L(X, y) is bounded on the compact set KQ C A. Note that 
I M v ) - ^ ( u ) | | < M | | v - u | | 
(8.46) 
whenever a e K and u, v e -BQ(a). This follows from Theorem 8.4.4, the mean 
value theorem on convex sets. Also, 
||0a(u) - 0a(v)|| = ||^(a)(v - u)|| < M ||v - u|| 
(8.47) 
for all u, v e K. 
Choosing ß. Let H = <p(K). Then H is a compact set contained in B — <p(A). 
Use Theorem 4.5.48 again to find a number ß0 > 0 and a compact set HQ such that 
Bß0 (b) C H0 C B for all b e i / . Now apply Theorem 8.4.6, the uniform mean 
value theorem, to find a ß such that 0 < ß < ßo and such that 
||^(y) - i/>(b) - V'(b)(y - b)|| < (t/M) ||y - b|| 
(8.48) 
whenever b e i / and y £ Bp(h). 
A basic estimate. Let b e i/, y G -S/3(b), and a = V'C3) = i?~1(b). Then 
||v - u|| < (t/M) ||y - b|| 
(8.49) 
with v = v?_1(y) = ip(y) and u = i?ä1(y)- This is just a reformulation of (8.48). 
In fact, using the expression of Í9^ 1 : Y —* X in (8.44), we see that 
V>(y)-^(b)-V'(b)(y-b) = v - a - C 1 ( y ) + C 1 ( b ) 
(8.50) 
= v — a — u + a = v — u. 
(8.51) 
Completion of the proof. We will show that the inclusions (8.45) in Theorem 8.4.9 
are satisfied with 8 = min(a, ß/M). To obtain the first inclusion, let b = </?(a) and 
y = é-\u) G í?a(B(1_t)r.(a)) with u G ß ( 1_ t ) r(a). Note that 
| | y - b | | < M | | u - a | | < M(l - i)r < M(l - i)<5 
(8.52) 
< M(l-t)(ß/M)<ß. 
(8.53) 

3 3 2 
MULTIPLE INTEGRALS 
Hence (8.49) is applicable. Let v = ip(y). Then 
| | v - a | | 
< 
| | u - a | | + | | v - u | | < ( l - t ) r + ( i / M ) | | y - b | | 
(8.54) 
< 
(I - t)r + t{l - t)r < r. 
(8.55) 
This shows that if y = t ^ f u ) e i?a(ß(i-t)r(a)).thenalsoy = tp(v) € tp(Br(a)). 
Hence the first inclusion follows. To obtain the second inclusion in (8.45), let 
y = (¿>(v) e tf(Br(a.)). We see that, as in (8.52), y G Bß{h). Hence (8.49) is again 
applicable. Let u = i?ä1(y). Then 
| | u - a | | 
< 
| | v - a | | + | | v - u | | < r + ( t / M ) | | y - b | | 
(8.56) 
< 
r + tr = (l+t)r. 
(8.57) 
This shows that if y = <¿>(v) G <¿?(i3r(a)), then also y = i?a(u) G i?a(-B(i+t)r(a))-
Hence the second inclusion follows. 
□ 
Change of Volumes 
Remarks 8.4.10 Cubes and balls. We will use the maximum norm 
\\(xi, ..., xn)\\m 
= max¿|x¿| on Rn. 
This is a natural norm here since the balls in the maximum norm are the interiors of 
cubic blocks, as was observed in Definition 8.1.3. 
Notations 8.4.11 Cubes as unions of smaller cubes. Let C be a cube in a grid Gk0 ■ 
If fc > fc0, then C is a union of cubes in Qk as C = U¿C¿, C¿ e Cfc. The center of C¿ 
is a¿. Note that the interior of C¿ is the open ball C° = Br(at) with r = 2-k~1 and 
the closure of C¿ is the closed ball C¿ = 
Br{ai). 
Lemma 8.4.12 Let Lp : A —> F = R" fee a diffeomorphism. Let C G Cfco, C C A 
and 0 < t < 1. 77¡e« í/iere is ak\ > fco swc/í f/iaf 
(1 - tr^Xtfi'MWCi) 
< v(<p(C)) <v(<p(C)) 
(8.58) 
< 
(l + í r E . ^ ' Í ^ M C i ) 
(8.59) 
whenever k > k\, C¿ G Cfc, andCi C C, with the center at &i and radius r = 2~k~1. 
Proof. Find S > 0 from the uniform approximations theorem, Theorem 8.4.9, so 
that (8.45) is satisfied whenever a G C and 0 < r < S. Find k\ > fco so that 
r = 2" f e l _ 1 < ¿. Since Br(a¿) C C¿ C 5r(a¿) and C = U¿C¿, we obtain 
Uii9a,(S(i-i)r(al)) C Uip(Ci) C Ui??ai(ß(1+t)r(aO) 
(8.60) 

CHANGE OF VARIABLES 
333 
from (8.45) of Theorem 8.4.9. But C = U¿Ci implies that (p(C) = Ui<^(Ci) since 
<p is a diffeomorphism and, therefore, one-to-one. Also, the sets </?(C¿) are pairwise 
disjoint. Therefore 
^ ( t ? a , ( ß ( 1 _ t ) r ( a i ) ) ) 
< 
v{<p(C))<v(<p{C)) 
(8.61) 
< X ^ O * * (B(i+t)r (*)))• 
(8.62) 
Also, by Definition 8.3.11 and Corollary 8.3.14, 
v(^(B{1_t)r(ai))) 
= 
p(^'(a¿))?;(B(1_í)r(a¿)) 
(8.63) 
= 
( l - í J ' W í a i ) ) «(£,.(*)), 
(8.64) 
and similarly for v(dai (B(1+t)r(a¿))). Then (8.58) and (8.59) follow. 
G 
Lemma 8.4.13 Images of cubes. Let f : A —> Y = Kn be a diffeomorphism. Let 
C be a cube in a grid Gk0 and C C A. Then f(C) is a Jordan set and 
v(<p(C)) = [ 
p(p'(x))dx. 
Jc 
Proof. First, p(p'(x)) is continuous and therefore integrable on a cube C. Therefore, 
linifc ^2ip(tp'(ai))v(Ci) 
= fc p((p'(x.)) dx by Theorem 8.2.21. Here k is the order 
of the grid C^ that contains the cubes C¿, and the sum is extended over all d 6 C¿ 
contained in C. Then Lemma 8.4.12 shows that 
(l-t)n 
[ p(<p'(x))dx 
< v(<p(C)) <v(<p(C)) 
(8.65) 
Jc 
< 
(l + i) n / p(<p'(x))dx.. 
(8.66) 
Jc 
The conclusion follows, since these inequalities are true for all t, 0 < t < 1. 
□ 
Theorem 8.4.14 Change of volumes. Let A <z X = W1 be an open set and 
if : A —> Y = R" a diffeomorphism. If E is a Jordan set with closure E contained 
in A, then F = p(E) is a Jordan set contained in f(A) = B and 
v(F) = v(ip(E)) = [ p{p\x)) dx. 
(8.67) 
JE 
Proof. Lemma 8.4.13 above shows that the conclusion of this theorem is true for 
cubes with closures contained in A. Then the linearity of integrals and the additivity 
of volumes show that this conclusion is also true for Jordan sets E that are the finite 
disjoint unions of such cubes. Let E be any Jordan set with closure E <z A. Let 

334 
MULTIPLE INTEGRALS 
Fk = Ik(E) and Gk = Ok(E) be the fcth-order inner and outer approximations of E, 
as defined in Definition 8.1.15. Use Theorem 4.5.48 to obtain a compact set K C A 
and a fco £ N such that Gk C K for all fc > fc0. The conclusion of the change of 
volumes theorem is valid for all Fk and Gk, k > fco- Since F t C £ c G ( ¡ C Í Í , w e 
obtain 
v(<p(Fk)) 
= 
[ p{tp'(x))dx<v(<p(E)) 
(8.68) 
JFk 
< 
v{ip{E))< 
Í 
pfa'(x.))dx 
= v(<p(Gk)). 
(8.69) 
JGk 
The difference between these outer and inner estimates is 
v(<p(Gk)) - v(<p{Fk)) = Í 
p(ip'{x))dx<Mv(Gk\Fk), 
J(Gk\Fk) 
where M is an upper bound for the positive function p(ip'(x)) on K. Such an upper 
bound exists, since (p ■ if') : A —> R is a continuous function and, therefore, bounded 
on the compact set K c A. But lim^ v(Gk \Fk) = 0 since E is a Jordan set. Hence 
ip(E) is also a Jordan set and JE p(</j'(x)) dx exists. Then this integral is v(<p(E)) 
since both of these numbers are between v(ip{Fk)) and v(tp(Gk)) for all fc > fco. 
D 
Change of Integrals 
Lemma 8.4.15 Let A be an open set in X = Rn. Let ip : A —> R" ¿>e a diffeo-
morphism. Then ^(x, i) = (y>(x), i), (x, i) £ A x 1, defines a diffeomorphism 
$ : ( y l x l ) ^ ( r x R). A/so, p($'(x, £)) = p(ip'(x)) for all (x, £) £ A x R. 
Proof. We see that $ is a continuously differentiable function. Its range 
$(A x R) = B x R 
is open in i?n x R, since B = ip(A) is open in R™. Also, \P(y, t) = (i>{y), t) for 
(y, i) £ {B x R) defines the inverse of <E>, where -¡/> is the inverse of ip. Clearly, vf 
is a continuously differentiable function. Finally, if E is a Jordan set in R™ and J is 
an interval, then 3>'(x, £)(£ x J) = (<¿?'(x)(.E)) x J. Hence we see that the volume 
multipliers are related as stated in the lemma. 
G 
Theorem 8.4.16 Change of integrals. Let A c X = Rn be an open set and 
(p : A —> M.n a diffeomorphism. If f : Y —> R is integrable and has a compact 
support S C B = i/5(A), i/ien (/ ■ <p) : A —> R is integrable on A and 
[ /(y) ^ = /" /(V(x)) p(^'(x)) dx. 
(8.70) 

CHANGE OF VARIABLES 
335 
Proof. First, assume that / : Y —» K is a nonnegative function. Let 
Ef = {(y,t)\0<t< 
/(y), y € B } C (B x R) 
be the region under the graph of /. Since / is an integrable function, the set Ef 
is a Jordan set in Rn x E. Also, its closure is contained in (SxM) c ( 5 x 1). 
If G = { (x, i) | 0 < t < /(</?(x)), x e A }, then we see that £ / = $(G), with 
the notations in Lemma 8.4.15 above. Hence, by the change of volumes theorem, 
Theorem 8.4.14, 
[ f(y)dy 
= 
vn+1(Ef)=vn+1mG)) 
JB 
= 
/ p(3>'(x, t))dtdx = / p(ip'{-x.)) dtdbi 
JG 
JG 
f(yL)p{<p'(x))dx. 
L 
IA 
Here the last step follows from Fubini's theorem, Theorem 8.2.37. 
□ 
Some Useful Notation 
In practice, changing the variable of integration in multiple integrals requires a careful 
distinction between the old and new spaces and careful attention to the direction of 
the transformation between these spaces. The following notation is helpful. Suppose 
we want to compute an integral fE /(x) dx in the X-space. We want to make a 
change of variables and express x as x = ip(y). Note that this is a transformation 
from Y to X, using ip : Y —* X. The new integration region is the set of all y such 
that <p(y) = x e E. In other words, the new integration region is Lp"1 (E). We start 
by writing 
[ f(x)dx= 
[ 
/(y(y))£/(y)dy. 
JE 
V ( £ ) 
Here, we know that U must be the volume multiplier for either ip : Y —* X or 
<p~l : X —> Y. But which one? To get the correct answer, write U(y) as 
u(y) 
dx. 
dy" 
so that the dy s cancel out and we are left with dx, the original variable of integration. 
It follows, of course, that the correct multiplier is 
U(y) 
d(xi, 
. . . , £ „ ) 
d{yi, •.., j/ n) 
the usual notation for the determinant of the Jacobian matrix. 

336 
MULTIPLE INTEGRALS 
Problems 
8.54 
Integrate f(x, y) = 2x2 + y2 over the region bounded by the lines 3x + 2y — 
5, 3a; + 2y - 8, 6x - Ay = 2, and 6x - Ay = 7. 
8.55 
Integrate f(x, y) = 2x2 + y2 over the region in the first quadrant (that is, 
x > 0 and y > 0) bounded by the curves xy = 2, xy = A, y = 3x, and y = 5x. 
8.56 
The unit disc x2+y2 < 1 is divided into two parts by the circle x2 + (y—l)2 = 
1. Use polar coordinates x = r cos 8, y = r sin 9 to find the areas of these parts. 
8.57 
The elliptical region (x/a)2 + (y/b)2 < 1 is divided into two parts by the 
ellipse (x/a)2 + ((y — b)/b)2 = 1. Find the areas of these parts. 
8.58 
Let / : R —» M be a continuous function, 0 < p < q, 0 < r < s, and 
E = { (x, y) | px < x2 + y2 < qx, ry < x2 + y2 < sy } C R2. 
Show that one can define a function F : R2 —> R so that 
f(y/x)dxdy 
= / 
/ F(u, 
v)dudv 
E 
Jp Jr 
for all choices of p, q, r, and s. Find F explicitly in terms of /. Contrive some 
examples for / ^ 0 to compute this integral without too much work. 
8.59 
Let f(x, y) = exp(— x2 — y2). 
Compute the integral of / over a disc 
x2 +y2 < R2. 
8.60 
Let F(R) be the integral of f(x, y) = exp(-x 2 - y2) over x2 + y2 < R2. 
1. Show that liiriß^oo F(R) = i i € M exists. What is Kl 
2. Let a > 0. Show that limfl^oo F(aR) exists and is equal to K. 
3. Let a ^ 0 and ß ^ 0. Show that limß^oo f_Rf_R 
exp(-a2x2 
-ß2y2) 
dx dy 
exists and is equal to K/\aß\. 
4. Show that lim^^oo J_R exp(—x2) dx exists and is equal to yfK. 
8.61 
Let T : R2 —> R2 be an isomorphism. For each R, let ER be the elliptical 
region (x/a)2 + (y/b)2 
< R2. 
Here a / 0 
and b ^ 0 are fixed. Show that 
lim^^oo JE exp(—(Tz, Tz}) <iz exists and compute its value. 

CHANGE OF VARIABLES 
337 
8.62 
Let n e N and let T : W1 -► Rn be an isomorphism. For each R, let 
ER be the elliptical region Yli{xilai)2 
— ^2- Here a, ^¿ O are fixed. Show that 
lim/^oo fE 
exp(—(Tz, Tz)) dz exists and compute its value. 
8.63 
Centroid. Let E be a Jordan set in R™. Show that there is a unique vector 
c € Kn, called the centroid of E, such that 
for all a e R " . 
8.64 
Pappus' Theorem. Let E be a Jordan set in the ¿rz-plane. Let c = (a, b) be 
the centroid of E. Assume that if (x, z) 6 E, then x > 0. Rotate ¿3 about the z-axis 
to get a solid R in M3. Show that v3(R) = 2n av2{E). 
8.65 
Find the volume of the torus obtained by rotating the disc (x — 2)2 + z2 < 1 
around the z-axis. 

This page intentionally left blank

CHAPTER 9 
INTEGRATION ON MANIFOLDS 
The main objective of this chapter is to present and investigate definitions of content 
(volume) and integration on manifolds: curves, surfaces and the like. We work up to 
these definitions, beginning with the simpler problem of defining volume on vector 
spaces and subspaces other than Rn. 
Any n-dimensional vector space X is isomorphic to Rn. One natural way to define 
volume on X is to use an isomorphism T : X —> M™. £ c X is a Jordan set in X if 
TE is a Jordan set in K™, and its volume is then defined as v(TE). This definition 
of volume on X depends on T, albeit in a minor way: any two volume functions 
defined in this manner are constant multiples of each other. That is a consequence of 
Theorem 8.3.10. 
We shall restrict our attention to the problem of defining volume on a Euclidean 
space X. In this case, there is a particularly natural choice for T which defines a 
volume on X called the Euclidean volume. But there is also a way to define this 
volume as an intrinsic feature of X, without making use of the mapping to W1. 
Analysis in Vector Spaces. 
By M. A. Akeoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
339 

3 4 0 
INTEGRATION ON MANIFOLDS 
The two approaches, extrinsic and intrinsic, also give us (equivalent) ways to define 
fc-dimensional volume on fc-dimensional subspaces of X. 
We move to the definitions of content and integration of real-valued functions on 
manifolds in section 9.2. These definitions are extrinsic; they utilize a parametric 
characterization to reduce integration over a manifold to integration over an under-
lying (Euclidean) parameter space. This theory of content and integration is then 
extended, in sections 9.3 and 9.4, to the integration of vector functions and tensor 
fields. In both cases, the vector or tensor function is used to generate a real-valued 
function which is then integrated over the manifold. 
The final section raises the question of whether we can provide an intrinsic charac-
terization of content and integration on a manifold. We show that the answer is "yes" 
for the special case of manifolds that can be represented as graphs in some coordinate 
system. We prove that the intrinsically defined 'geometric' content agrees with the 
standard, extrinsically defined notion of earlier sections. 
9.1 
EUCLIDEAN VOLUMES 
Definition 9.1.1 Euclidean volumes. Let Z be an n-dimensional Euclidean space. 
If (ei, .... e„) is an orthonormal basis, then 
Tz = T{ziei 
+ ■■■ + znen) = (Zl,...,zn) 
(9.1) 
defines an isomorphism T : Z —» Kn. The Euclidean volume of a set E C Z is 
defined as vz{E) — vn(TE) 
(or simply v(TE)), where vn (or v) is volume in R™. 
Hence vz (E) is defined whenever TE c Rn is a Jordan set. 
This definition has to be justified by showing that vz(E) is independent of the choice 
of orthonormal basis. This is left as an exercise. It also follows from an intrinsic 
characterization of Euclidean volume which does not rely upon any mapping from Z 
to Rra. We develop this idea by offering a second definition of Euclidean volume in 
terms of Euclidean determinants on Z (Theorem 9.1.7 and Remarks 9.1.8 below). 
A Review of Euclidean Determinants 
Determinants were introduced in section 3.6. Here, we review and expand upon that 
earlier discussion. 
For any nonnegative integer k, let Afc(Z) = AMLk(Zk, 
R) be the linear space of 
all alternating multilinear functions Zk —» R, as defined in Definition 3.6.21. Let 
n = diraZ. A basic fact about An(Z) is that it is a one-dimensional space. Any 

EUCLIDEAN VOLUMES 
341 
nonzero element of An(Z) is called a determinant (or a determinant function) on 
Z. Hence any determinant is a basis for An(Z). 
In particular any determinant is a 
multiple of any other determinant. 
Definition 9.1.2 Determinant of a basis. Determinant functions are related to the 
familiardeterminantdet{ay}ofannxnmatrix {a¿j}. In fact, let U = (uj, ..., u„) 
be an (ordered) basis for Z and let A = (ai, ..., a„) be any (ordered) set of n vectors 
in Zn. Then the coordinates {a^ } of a¿ s with respect to Uj s form an n x n matrix. 
The determinant of this matrix defines a determinant function ipn : Z" —> E. We say 
that ipu is the determinant of the ordered basis U. Note that ifaj is uniquely defined by 
the condition that ipv{V) = 1. Also, note that any determinant function is a multiple 
of V>u-
Definition 9.1.3 Euclidean determinants. If Z is a Euclidean space, then a determi-
nant function ■& is called a Euclidean determinant if i?(E) = ±1 for any orthonormal 
basis E = (ei, ..., e„). If we fix an orthonormal basis E, then there are exactly 
two Euclidean determinants: ipE and ~I/JE- Any (ordered) basis U, however, may be 
used to characterize these two Euclidean determinants, since 
Vte = V>UMJ(E), 
where 4>u is the determinant of U (in the sense of Definition 9.1.2). 
Definition 9.1.4 Oriented Euclidean spaces. A Euclidean space Z together with 
one of its Euclidean determinants d is called an oriented Euclidean space. An 
oriented Euclidean space may be also denoted as (Z, i9). Each Euclidean space has 
exactly two orientations. 
Remarks 9.1.5 Computation of Euclidean determinants. Let ■& € A„ (Z) be a 
Euclidean determinant and A = (ai, ..., an) e Zn. 
Then #(A) G R can be 
computed in two different ways: 
(1) i?(A) = ±det(a¿, e,-), in terms of coordinates over an orthonormal basis 
(2) $(A) = ±(det(a¿, a^))1/2, intrinsically. 
The equivalence of (1) and (2) derives from the following facts. First, if A is the 
matrix with entries (a¿, ej), then the conjugate or transpose matrix A* has the same 
determinant (Appendix C, Theorem C.2.4). Second, (a¿, a¿) are the entries of the 
product AA*. Third, det AA* = det A det A* = (det A)2. 

342 
INTEGRATION ON MANIFOLDS 
Euclidean determinants and volume 
Remarks 9.1.6 Volume of a box in Rn. Let X -— M.n with its standard inner product. 
If A = (ai, ..., a„) eX",let 
5(A) = { ami + ■■■ + a„a„ | 0 < az < 1, i = l,...,n}cX 
(9.2) 
be the box spanned by A. Then 5(A) is a Jordan set and v(B(Á)) = |i?(A)|. This 
follows from our work in chapter 8. 
Theorem 9.1.7 Let Z be an n-dimensional Euclidean space. Let 
B(A) = { aiai + • • ■ + anan | 0 < a¿ < 1, i = 1, ..., n } 
(9.3) 
be the box spanned by A = (ai, ..., ara) S Zn. Then 5(A) is a Jordan set. Its 
Euclidean volume is vz{B(A)) 
= (det(a¿, a.j)z)1^2 = ji9(A.)|, where i9 G A„(Z) 
¿s a Euclidean determinant on Z. 
Proof. Let T : Z —> Rn = X be an isomorphism as defined in 9.1.1. Then 
TB(A) c X is the box spanned by (Tai, ..., Ta n). Since T5(A) is a Jordan set, 
so is 5(A), and 
^ ( 5 ( A ) ) 
= 
w(TB(A)) = v(B(TA)) 
= (det(Ta¿, T a , ) x) 1 / 2 
(9.4) 
= 
(det(ai, ^z)1'2 
= |i?(A)|. 
(9.5) 
The reason for the first equality in (9.5) is that T : Z —> X = Kn preserves inner 
products. 
D 
Remarks 9.1.8 Intrinsic definition of Euclidean volumes. The boxes in a Eu-
clidean space are "geometrical" objects, independent of the choice of coordinates. 
As we have just seen, their volumes can be defined in terms of Euclidean determi-
nants, which are also independent of coordinates. Hence the volume of a box can 
be defined purely in terms of the inner product. Since the inner product defines the 
"geometry" of the space, volume is a geometrically determined quantity. 
Note also that in defining the Euclidean volume of a box in Z, we have also defined 
the volumes of unions of boxes and hence the volumes of all Jordan sets. In fact, 
Jordan sets are those that can be approximated by inner and outer unions of boxes 
that are close in volume. 
Integrals on Euclidean Spaces 
Let / : Z —* R be a real-valued function defined on a Euclidean space Z. For 
convenience we will assume that all functions are defined on the whole space. This 

EUCLIDEAN VOLUMES 
3 4 3 
is not a restriction of generality: any function can be extended to the whole space by 
making it vanish outside its original domain of definition. This does not affect the 
integration. 
The integral of such a function is denoted by 
[for 
[for 
[ f(z)dz 
(9.6) 
J 
J Z 
J Z 
or by a similar notation. The definition is obvious. Let T : X = Kn —> Z be an 
isomorphism that takes the standard basis of X = M™ to an orthonormal basis E of 
Z. Then define Jz /(z) dz as Jx /(Tx) dx, if this latter integral exists. We see that 
the result is independent of the choice of the orthonormal basis E. 
Volumes on Subspaces 
The preceding approach gives us an intrinsic notion of volume for subspaces of a 
Euclidean space. 
Definition 9.1.9 Euclidean volumes on subspaces. A subspace X of a Euclidean 
space Z is also a Euclidean space with the same inner product (restricted to X). It 
follows that X has its own two Euclidean determinants ±$x> as well as its own 
volume vx defined on its own family of Jordan sets. We sometimes write Vk for 
this fc-dimensional volume. Note that for any fc-dimensional box -B(A) in X, where 
A = (ai, ..., afc), Theorem 9.1.7 tells us that 
vk(B(A)) 
= (det(a„ a,-))1/2. 
Definition 9.1.10 Lower-dimensional volume of a box. Let A = (ai, ..., a¿) € 
Zk be a fc-tuple of vectors from Z. It is convenient to define 
vk(B(A)) 
= (det(ai) a,-))1/2 
(9.7) 
as the k-dimensional volume of the box spanned by these vectors. Of course, this 
agrees with the volume in Definition 9.1.9, when B(A) is considered as a Jordan 
subset of a fc-dimensional subspace. Sometimes, however, it is convenient to talk 
about the fc-dimensional volume of a box without specifying any subspace. Note that 
Vk{B(A)) is nonzero if and only if the vectors in A form a linearly independent set. 
Theorem 9.1.11 Euclidean determinants on coordinate systems. Let (X, Y) be 
a coordinate system in Z with the respective Euclidean determinants dx, $Y> and 
•&Z- Then there is a 7 such that 
■&z(A, B) =7^x(A)i?y(B) 
(9.8) 

3 4 4 
INTEGRATION ON MANIFOLDS 
for all A = (ai, ..., &k) G Xk andB = (b1; ..., be) £ Ye, where 
(A, B) = ( a i ) ..., a*; bi, ..., bt) £ Zn. 
(9.9) 
Here dim X = k, dim Y = I, and dim Z — n — k + I. IfX ± Y, then 7 = ±1. 
Proof. This is a restatement of Theorems C.6.5 and C.5.3. O 
Theorem 9.1.12 Euclidean volumes on coordinate systems. Let (X, Y) be a 
coordinate system in Z. Then there is a ß = ß(X, Y) > 0 such that 
vz(Ax 
B) = ßvx(A)vY{B) 
(9.10) 
for all Jordan sets Ac X and B C Y. Also, if X J_ Y then ß(X, Y) — 1. 
Proof. If A and B are boxes, then the first part is a restatement of Theorem C.6.5. 
The result for general Jordan sets follows by approximations. If X _L Y, then take 
the boxes A and B as the unit boxes spanned by orthonormal bases in X and Y. 
Then A x B is also a unit box spanned by an orthonormal basis in Z. Hence in this 
case all three determinants will be equal to 1. Then ß(X, Y) = 1 follows. 
□ 
Corollary 9.1.13 Let X be a proper subspace of Z. Then any bounded set E in X 
is a Jordan set in Z and vz{E) = 0. 
Proof. Let A be a Jordan set in X and E C A. Let Y = X1. Then y is a Euclidean 
space. Let Br be the ball of radius r in Y. Then 
E = E x {0} c A x Br, for all r > 0. 
(9.11) 
But vz{A x Br) = vx(A)vY(Br) 
= revx(A)vY(B1) 
-> 0 as r -> 0. (Here 
I — dim Y.) This shows that E is a negligible set in Z. 
□ 
Volume Multipliers of Linear Maps 
Lemma 9.1.14 Let W and Z be two Euclidean spaces of the same dimension. Let 
T : W —> Z be a linear map. Then there is a number p(T) > 0 such that 
vz(TE) 
= p(T)vw(E) 
(9.12) 
for all Jordan sets E C W. Also p(T) — |$z(rE)|, where E = (ej, ..., e„) is an 
orthonormal basis for W and ßz is a Euclidean determinant for Z. 

INTEGRATION ON MANIFOLDS 
345 
Proof. Theorem 8.3.17 shows that if W = Z = Rn and if T : W -» Z is an 
isomorphism, then this result holds with p(T) = vz{TU), where U = [0, l) n is the 
unit cube in W. The arguments for the general case are the same. 
If T is not an isomorphism, then the range X = TW of T is a proper subspace of Z. 
Therefore TE C X is a negligible set in Z for all bounded E c W, by Corollary 
9.1.13. In this case, let p(T) = 0. 
□ 
Definition 9.1.15 General volume multipliers. Let T : W —> Z be a linear 
transformation between any two Euclidean spaces. Let A = (ai, ..., a^) be an 
orthonormal basis for W. Then the number 
P(T) = v(B(TA)) 
= (det(Ta¿, Ta,» 1/ 2 
(9.13) 
is called the general volume multiplier of T. If W and Z have the same dimension, 
then this agrees with the earlier definition. Note that p(T) > 0 if and only if 
dim W = dim(TW), that is, if and only if T is one-to-one. 
Problems 
9.1 
Let (X, Y) and (U, V) be two orthogonal coordinate systems (Definition 
3.1.42) in Z. Assume that ({/, Y) is also a coordinate system for Z. Let P : U —> X 
be the orthogonal projection of U on X and let Q : Y —> V be the orthogonal 
projection of Y on V. Show that p(P) = p(Q). 
9.2 
Let U be a two-dimensional Euclidean space. Let T : U —> M3 be a linear 
mapping. Show that p(T) = ||Tui x Tu2||, where (ui, 112) is an orthonormal basis 
for U and x denotes the cross product in M3. 
9.2 
INTEGRATION ON MANIFOLDS 
A manifold M consists of the "local images" of subspaces under diffeomorphisms. 
For our discussion of integration on manifolds, it is also convenient to work with the 
reverse diffeomorphisms. They will be called charts on M. 
Charts for Manifolds 
The following notation is used in this chapter and Chapter 10. 

346 
INTEGRATION ON MANIFOLDS 
Definition 9.2.1 Charts. Let W and Z be two spaces, U a subspace of W, and M 
a subset of Z. Let * : G —> W be a diffeomorphism defined on an open set G c Z. 
Then ^ is called a c/za/7 for A/ if 
# ( M n G ) = í/n.íf with # = $(<?)• 
(9.14) 
Hence M is a manifold if each m e M is contained in the domain of a chart for M. 
If * : G —> if = *(G) is a chart for M, then * _ 1 = $ : H —> G is the revere 
cftarf. Note that the restriction of $ : Ü" —> Z to U n if is a parametric representation 
$ 1 ^ ^ = ¡p : U n ii -> Z for M n G. 
Figure 9.1. Charts as in Definition 9.2.1. 
Definition 9.2.2 Atlases for a manifold. A collection A of charts 
* a : G 0 - » f f „ ( = $ ( G a ) ) , 
a € A 
(9.15) 
is called an atlas for M if the collection of their domains { Ga} forms an open cover 
for M. Hence M C UQGQ and $>a(M n Ga) = ?7Q n ii Q. We shall usually assume 
that Ua = U is the same subspace for all a, which (as we shall see) involves no loss 
of generality. Note that every manifold has an atlas; this follows from the definition 
of a manifold. 
Definition 9.2.3 Derivatives and tangent spaces. Let $ : H —► G be a reverse 
chart. If u e U C\ H and <3>(u) = m, then m £ M and 

INTEGRATION ON MANIFOLDS 
3 4 7 
is called the tangent space of M at m. If we let p — &\unH, then <p'(u) : U —> T m 
is the restriction of $'(u) : W —> Z to the subspace [/. 
Note that p'{u) is an isomorphism between {/ and Tm. The volume multiplier 
p($'(u)|[/) = p(<p'(u)) of this isomorphism plays a central role in the theory (and 
practical applications) of integration on manifolds. Also note that 
*'(m)| T m : Tm -► U is the inverse of &(u)\u :U ~^Tm. 
(9.16) 
Hencep(*'(m)|rm) = lM*'(u)| £ /). 
Local Integrals 
Remarks 9.2.4 Local integrals. Integrals on manifolds are usually computed by 
means of parametric representations (see Definition 6.3.11). When one computes an 
integral using a parametric representation, it is understood that this is only a partial 
integration over the part of the manifold covered by this representation. We will call 
such an integral a local integral. This generates a problem: how do we integrate 
a function whose support does not lie entirely within a region covered by a single 
parametric representation? 
Here is an efficient way to handle this situation. Let ty : G —+ H be a chart for M, 
with the associated parametric representation <p> : U l~l H —> Z. (It helps to keep 
Figure 9.1 handy.) The part of M covered by this representation is M n G. When 
working with this particular parametric representation, assume that any function to 
be integrated has compact support contained in G. In this case, we can use <p to 
obtain the complete integral of the function on M. 
The integration of a general function is defined by using a technique called partitions 
of unity. In essence, the idea is to re-write an arbitrary function / as a sum of functions 
fi, each of which has compact support covered by single parametric representation, 
and to define the integral of / as the sum of the (local) integrals of the functions 
fi. This technique is discussed later in this section, starting with Definition 9.2.10. 
We shall begin with local integrals, working with a fixed chart í» ¡ G —> H and the 
associated parametric representation ip : U (~l H —► Z. 
Definition 9.2.5 Local integration. Let $ : H —> G be a reverse chart for a 
manifold M. If / : Z —> R is a function with compact support K c G, then its 
integral over M is defined as 
[ f= [ f(*(u))p(&(u)\u)du, 
(9.17) 
JM 
JU 
whenever the integral on the right exists. This integral involves ordinary integration 
over a Euclidean space, and is computed by the usual methods of multiple integrals. 

3 4 8 
INTEGRATION ON MANIFOLDS 
We can re-state (9.17) in terms of the parametric representation <p = $|[/ : U —» Z: 
[ f= 
f f{<p(u))p(<p'(u))du. 
(9.18) 
J M 
JU 
To justify this definition, we must show that if there are two reverse charts <E>¿ : Hi —► 
d such that the support of / : Z —> K is contained in Gi n Gi, then the definitions 
of the integral of / using these two charts give the same result. That is the content of 
our next theorem. 
Theorem 9.2.6 Let <í>¿ : ií¿ —> G¿ be two reverse charts for M. Let f : Z —>Rbe 
a function with support in Gi n G2. Then 
[ f(*1(u))p(&1(u)\u)du 
= /"/($ 2(u))/>(^(u)|t,)du 
(9.19) 
Ju 
Ju 
whenever either one of the integrals exists. 
Proof. Let *i and * 2 be the charts associated with $! and <J>2- Let G = G\ nG2. The 
restriction of each ^¿ to G is another chart for M. We see that the integrals in (9.19) 
for the original charts are the same integrals for the restricted charts. Hence, we may 
assume that G\ = G2 = G, without loss of generality. Then 6 = * 2 $ ! : Hi —> H2 
is a diffeomorphism. 
We see that 8 = Q\unHl ■ U D Hi —> U n if2 is al s o 
a diffeomorphism, and <¿>i = (p2 ■ #. (Here we have put <¿>¿ = $i\tj.) 
Hence 
tp'i(u) = tp'2(0(u)) • 0'(u), and therefore, 
p(^(u)) = p(^(0(u))) • p ( 0 » ) . 
(9.20) 
The change of variables theorem, Theorem 8.4.16, shows that 
/ 
g(u2)du2= 
[ 
g{0(u1))p(9'(u1))du1 
(9.21) 
JunH2 
JunHi 
whenever the first integral exists. If 0(112) = f{f2(^2)) 
p(f'2{u2)), then 
g(6(ui))p(8'(ui)) 
= 
f&2(e(ui)))p(v'2(8(ui)))p(e'(ui)) 
(9.22) 
= 
/ ( ^ ( m M ^ U u i ) ) - 
(9.23) 
To obtain (9.23) we used (9.20). The conclusion now follows. 
D 
Contents on Manifolds 
Definition 9.2.7 Jordan sets in manifolds. A set E in a manifold M is called a 
Jordan set in M if its characteristic function \E '■ M —> K is integrable on M. In 

INTEGRATION ON MANIFOLDS 
349 
this case, a(E) = JM XE is called the content of E. The term 'content' stands for 
fc-dimensional volume on a /c-dimensional manifold. 
Remarks 9.2.8 Notations for integrals. The integral of / : M —> K on a manifold 
is denoted by expressions such as 
f f= 
[ f(m)dm= 
[ /(m)<x(dm) = [ f da 
(9.24) 
JM 
JM 
JM 
JM 
or by similar expressions. In general, none of these expressions is suitable for 
computations. To compute an integral on a manifold, we have to use a chart and the 
corresponding parametric representations <p : U n H —> Z to reduce a problem of 
integration on a manifold to a problem of integration on a Euclidean space. Then we 
can use the usual techniques of multiple integration to compute the integral. With 
this practical focus on computation in mind, a useful notation for the integral is 
[ fda= 
[ 
f(<p(u))p(<p'(u))du= 
[ 
/Mu))dcr(u), 
(?.25) 
JM 
JunH 
JunH 
where p(<p'(\i))du = da(u) is considered as the content of a small part of the 
manifold. This small part is the image of a small cube in the Euclidean space U. 
Remarks 9.2.9 Content and integration. Integration on M can be defined in terms 
of the content on M. In fact, we see that if / : M —> K is a function of compact 
support K c M, then fM f exists if and only if for each e > 0 there are finitely 
many pairwise disjoint Jordan sets Ei c M such that K c U¿i?¿ and such that 
V 
(sup { /(z) I z G Et } - inf { f{x) \xeEi}) 
a(Ei) < e. 
(9.26) 
The details are left as an exercise. 
General Integrals 
The above definition of integrals is only for functions M —> R with supports contained 
in the domain of a single chart. Integrals of more general functions are defined by a 
technique called the partitions of unity. The basis of this technique is the following 
theorem. 
Definition 9.2.10 Partitions of unity. Let A be a set in a Euclidean space Z. Let 
d s be a finite collection of bounded open sets such that A c U¿G¿. Then a (finite) 
set of 6°° functions A¿ : Z —> [0, 1] is called a partition of unity for A subordinate to 

350 
INTEGRATION ON MANIFOLDS 
the covering d if each A¿ has a compact support contained in G¿ and if ^ A ¿ (z) = 1 
for all z e A 
Theorem 9.2.11 Existence of partitions of unity. Given a finite covering G¿ of a 
set A by bounded open sets, there is a partition of unity for A subordinate to the 
covering G¿. 
The proof of this theorem is fairly elementary and self-contained. The requirement 
that A, s are C°° functions does not cause any additional complications in the proof. 
It is made for convenience so that the result can be used in other applications that 
require a high degree of differentiability. Theorem 9.2.11 is re-stated and proved in 
Appendix D as Theorem D.I.8. 
Definition 9.2.12 Integrals on manifolds. Let M be a manifold in Z. Let / : M —> 
R be a function of compact support S C M. By the definition of a manifold, each 
point m G S is in the domain G of some chart, and each such G is an open set. Use 
the compactness of S to find finitely many of these domains G¿ such that S C UiG,. 
Then find a partition of unity A¿ : Z —> [ 0, 1 ] for S subordinate to G¿ and define the 
surface integral of / : M —* R as 
/ f = zZÍ fx* 
<
9-
27> 
JM 
% JM 
if each /A, : M —* M is integrable. Note that the integrals on the right-hand side are 
obtained by local integration (as in Definition 9.2.5), since each /A¿ : M —> R has a 
compact support contained in M n G¿. 
Nevertheless, our definition has to be justified by showing that different partitions of 
unity lead to the same result. 
Lemma 9.2.13 Let / : M —> R fee a function of compact support S <Z M. Let Gt 
and Hj be two finite coverings of S by bounded open sets. Let on and ßj be two 
partitions of unity for S subordinate to G¿ and to Hj respectively. If 
¿2, I fai 
exists>tnen 
¿~] / fßj also exists, 
(9.28) 
¿ JM 
i JM 
and these two sums are equal. 
Proof. Note that a¿(z) = J2jaiiz)ßj(z) 
f°r a^ z e S. If j M fan exists, then 
JM fcxißj also exists and 
Í M 
= T. 
[ fotißj. Hence 
(9.29) 
JM 
^ 
JM 
E, / /<* = EX, / 
fob- 
(930) 
' J M 
< I J M 

INTEGRATION ON MANIFOLDS 
351 
The double summation above is also equal to ^ 
JM fßj, by symmetry. 
□ 
Problems 
9.3 
Find the surface area of a sphere of radius R. 
9.4 
A plane z = h,0 < h <1, divides the unit sphere x2 + y2 + z2 = 1 into two 
parts. Compute the surface areas of these parts. 
9.5 
Let E be a Jordan set in the rectangle (—n, TT) X (0, TT). Map this region 
by the spherical coordinates to get a region &(E) in the unit sphere. Hence $(-E) 
consists of all points with the Cartesian coordinates 
x = cos 9 sin if, y = sin 9 sin if, z — cos if 
with (#, ip) € E. Express the surface area of &(E) as an integral over E. Find the 
areas corresponding to rectangles —TT <p<9<q<ir, 
0<r<ip<s<TT. 
Also 
apply this result to give another solution of Problem 9.3. 
9.6 
Find the surface area of the part of the sphere x2 + y2 + z2 = 4 that lies inside 
the cylinder (x — l) 2 + y2 < 1. 
9.7 
Compute the surface area of the helicoidal surface 
x = r cos 9, y = r sin 9, z = 9, 
where 1 < r < 2, 0 < 9 < 2n. 
9.8 
Find the surface area of the part of the cylinder x2 + y2 = 1 that lies between 
the planes z = 0 and z = 2x + 3y + 10. 
9.9 
Find the surface area of the part of the cylinder x? + z2 = a2 that lies above 
the a;y-plane and inside the cylinder x2 + y2 = a2. 
9.10 
Integrate f(x, y, z) = \z\ over the surface of the sphere 
x2 +y2 + z2 = 1. 
9.11 
Compute fG(x2z 
+ y2z)da where G is the upper half of the sphere 
x2 + y2 + z2 = 4. 

3 5 2 
INTEGRATION ON MANIFOLDS 
9.12 
Compute JG{y2 + z2)da where G is the part of the surface 
x = 4 - y2 - z2 
that lies in the region x > 0. 
9.13 
Compute JG (y2 + z2)da where G is the part of the surface 
x2 = 4 - y2 - z2 
that lies in the region x > 0. 
9.14 
Compute JG yz da where G is the part of the plane z — y + 3 that lies in the 
cylinder^2 + y2 = 1. 
9.15 
Compute Jc xy4 da where C is the right half of the circle 
x2 + y2 = 16. 
9.16 
Compute Jc xyda where C is the line segment joining (—1, 1) to (2, 3). 
9.17 
Compute Jc xyz da where C is the curve 
x = sin 2f, y = 3t, z = cos 2f, 0 < t < 7r/4. 
9.18 
Centroid. Let fibea Jordan set on a manifold M i n a Euclidean spaceZ. 
Show that there is a unique vector Cß such that 
(a, cE)a{E) 
= / (a, z)da 
JE 
for all a £ Z. This vector is called the centroid of E. 
9.19 
Find the centroid (Problem 9.18) of the upper-half of the sphere 
x2 + y2 + z2 = 1. 
9.20 
Find the centroid (Problem 9.18) of the helix 
x = cost, y = sint, z = t, 0 < t < a. 
9.21 
Find the centroid (Problem 9.18) of the helicoidal surface 
x = r cos t, y = r sin t, z = t, 0 <t < a, l < r < 2 . 

ORIENTED MANIFOLDS 
353 
9.22 
Pappus' theorem. Let E be a curve in the half of rz-plane corresponding to 
r > 0. Let £(E) be the length of E and let cE = (a, b) be the centroid of E. Rotate 
C around the z-axis to obtain a surface S in R3. Show that the surface area of S is 
2nai{E). (See also Problem 8.64.) 
9.23 
Find the surface area of the torus obtained by rotating the circle 
(r - 2)2 + z2 = 1 
around the z-axis. (See also Problem 8.65.) 
9.3 ORIENTED MANIFOLDS 
We will formulate definitions for the integral of a vector field and the integral of 
a tensor field over a manifold. Both definitions require the concept of an oriented 
manifold. In this section, we define the orientation of a manifold. We will distin-
guish between local orientation and global orientation. Almost all computations of 
integrals on manifolds are done locally, in terms of charts. For these computations, 
we only need the local orientations induced by charts. Global orientations, however, 
are needed for Stokes' theorem, which is discussed in Chapter 10. 
As a preliminary to defining the orientation of a manifold, it is useful to begin with 
some facts about the orientation of a vector space, as defined in Definition 9.1.4. 
In particular, the idea of charts that preserve orientation becomes important. We 
will define orientation-preserving diffeomorphisms and then show that an atlas of 
orientation-preserving charts always exists. 
Notations 9.3.1 Review of charts. Charts were defined in Definition 9.2.1, but it 
helps to repeat the main definitions and the standard notation. Recall that Z and 
W are two Euclidean spaces and U is a subspace of W. A chart for a manifold 
M i s a diffeomorphism 1< : G -> H = *(G) such that *(G n M) = H n U. 
The reverse chart is $ : H —> G and the corresponding parametric representation 
is ip = $\Hnu : H C\U -> Gf\M. 
If u e /f D [/ and m = $(u) = ¡p(u), then 
Tm = 3>'(u)t/ = (p'(u)U is the tangent space of M at m e M. We will assume 
that dim W ~ dim Z = n > 2 and 1 < k = dim U < n. An atlas for a manifold M 
is a collection of charts whose domains cover M. 
Notations 9.3.2 Orientations of U, W, and Z. We will assume that W and Z are 
oriented, respectively, by the Euclidean determinants Q and i?. Assume that U is also 
oriented by a positive orthonormal basis E. These are all arbitrarily chosen but fixed 
orientations. If U = W, however, then we assume that U and W have the same 
orientations. This is a trivial case that is only important if we wish to consider an 
open subset of Z as an n-dimensional manifold. 

354 
INTEGRATION ON MANIFOLDS 
Orientation-Preserving Charts 
Definition 9.3.3 Orientation-preserving isomorphisms. An isomorphism T be-
tween the two oriented spaces (W, g) and (Z, i?) is called orientation-preserving if 
there is a positive basis of W that is mapped by T to a positive basis of Z. In this 
case, T maps all positive bases of W to positive bases of Z (Problem 9.25). If T is 
not orientation-preserving, then it is called orientation-reversing. 
Definition 9.3.4 Orientation-preserving diffeomorphisms. Let X and Y be two 
oriented spaces. Let A be an open set in X and let 9 : A —> Y be a diffeomorphism. 
Let a G A. If 0'(a) : X —> F is an orientation-preserving isomorphism, then 
# : A —> F is called an orientation-preserving diffeomorphism at a. \f 9 : A ^ Y 
is an orientation-preserving diffeomorphism at every x £ A, then it is called an 
orientation-preserving diffeomorphism. 
Lemma 9.3.5 Let 9 : A —» Y be an orientation-preserving diffeomorphism at a G A 
r/ien f/zere zj an open sei G swc/i that a G G C A and such that the restriction of 9 
to G is an orientation-preserving diffeomorphism. 
Proof. Let E be a positive basis for X. Let Y be oriented by a Euclidean determinant 
f. We see that /(x) = £(0'(x)E), x G A, defines a continuous function / : A -> R. 
If 0 is orientation-preserving at a G -4, then /(a) > 0. So there is an open 
neighborhood G of a G .A such that a G G c A and such that /(x) > 0 for 
all x & G. This means that the restriction of 9 to G is an orientation-preserving 
diffeomorphism. 
□ 
Notations 9.3.6 Bases for U and W. Let V = Ux be the orthogonal complement 
of U. Let U = (ui, ..., Ufe) be a basis for U and let V = (vi, ..., vg) be a basis 
for V. Then we see that (V, U) — (vi, ..., v¿; ui, ..., ujt) is a basis for W. 
Definition 9.3.7 Orientation-preserving charts. A chart ^ : G —> H for a man-
ifold M is orientation-preserving if ^ is an orientation-preserving diffeomorphism 
onG. 
Theorem 9.3.8 shows that there is no loss of generality in assuming that all charts are 
orientation-preserving. 
Theorem 9.3.8 Any manifold has an atlas of orientation-preserving charts. 
Proof. Let M be a manifold and let m G M. Let <3>o : Go —► H0 be a chart for 
M such that m G Go- Let <b0 : H0 -^ G0 be the reverse chart. Let E be a positive 

ORIENTED MANIFOLDS 
355 
basis for U. Let V be a basis for the orthogonal complement of U such that (V, U) 
is a positive basis for W. Let i'o(m) = u. Then ($>0(u)V, <í'ó(u)U) is a basis for 
Z. If this is a positive basis for Z, then <E>o : HQ —> Go is orientation-preserving at 
u. In this case, we see that V^o : Go —> H$ is orientation-preserving at m. Hence. 
Lemma 9.3.5 shows that there is an open set G such that m € G C Go and such that 
the restriction of $>o to G is orientation-preserving. This restriction is still a chart for 
M. Hence m is contained in the domain of an orientation-preserving chart. 
If ($0(u)V, $Ó(U)U) is not a positive basis for Z, then replace ^0 : G0 ^ H0 by 
mx = R . q;0 : G0 -> RH0, where R : W -* W is defined as follows. With the 
notations in Notations 9.3.6, let 
ñui = 
ux, Rm = u¿, RVJ = vj, 1 < i < k, 1 < j < Í. 
(9.31) 
We see that R : W -^ W is an isomorphism and RU = U. An easy verification 
shows that *i : Go —► Hi = RHQ is another chart for M and that ^ i is orientation-
preserving at m. Then a restriction of \I/i to a neighborhood of m is an orientation-
preserving chart. 
□ 
Local Orientations 
Definition 9.3.9 Local orientation of a manifold. A local orientation of a manifold 
M is defined by a chart Í* : G —> H. It is an orientation of each of the tangent spaces 
T m at the points m G GnM. If \P(m) = u, then the orientation of T m is determined 
by taking a positive basis E for U and declaring B m = $'(u)E = <¿>'(u)E a positive 
basis for Tm. We see that these orientations are independent of the choice of the 
positive basis E in U. This point is formulated as Problem 9.24. 
Example 9.3.10 If M is a curve, then its tangent spaces are one-dimensional. In this 
case, the orientations of these tangent lines define a positive direction on the curve. 
The curve M = { (x, y) | x2 + y2 = 1, y ^ 0 } in this example consists of two 
halves of a circle. Here Z — M.2 is represented by the ccy-plane. Represent W = M2 
by the ör-plane. The following are four different charts \I>¿ : G —> W for M with 
the same domain G c Z and with the same range H c W. They are expressed in 
terms of the reverse charts $¿ : H —► G. Let 
# ! 
= 
{(0, r) | —7T < Ö < 0, -1/2 < r < 1/2}, 
#2 
= 
{(0, r)\0<9<TT, 
- l / 2 < r < l / 2 } , 

3 5 6 
INTEGRATION ON MANIFOLDS 
Figure 9.2. For Example 9.3.10. 
and H = HiU H2. See Figure 9.2. Define 
((l+r)cos0, (l+r)sin0), (0, r) e H, 
((1 + r) cose», -(1 + r) sin0), (0, r) e H, 
f (-(l + r)cos0,-(l + r)sin0), 
if(0, r ) e # i , 
\ ((l+r)cos0, - ( l + r ) s i n 0 ) , 
if (0, r) 6 ff2 
f ((l+r)cos0, - ( l + r)sin0), 
if (0, r) € # i , 
\ (-(l + r)cos0, - ( l + r ) s i n 0 ) , 
if (0, r) € H2 
We see that <!>» : H —* G are reverse charts for M. Here U is the r = 0 line, 
which is the 0-axis in the 0r-plane. Let U be oriented in the standard way, with the 
basis consisting of the unit vector (1, 0). These four charts induce four different 
orientations on M. In intuitive terms they can be described as the four possible 
choices of the clockwise or the counterclockwise orientations for the upper and the 
lower halves of M. 
Incidentally, in this example all "I^s are charts with domains covering all of M. 
Hence, they orient M completely; they also give examples of global orientations of 
a manifold. 
It may seem pedantic to use the diffeomorphisms $¿ : H —» G rather than the simpler 
and more natural parametric representations <pi : H DU —> G n M. One reason for 
using the full diffeomorphism approach is to verify that <pi is indeed the restriction 
* i ( 0 , r ) 
-
$2(Ö, r) 
= 
$3(0, r) 
= 
* 4(0, r) 
= 

ORIENTED MANIFOLDS 
357 
of a diffeomorphism to a subspace. Another reason is that the charts and reverse 
charts play an important role in some applications, most notably Stokes' theorem, to 
be discussed in Chapter 10. 
A 
Remarks 9.3.11 Charts with connected domains. Connected sets were defined in 
Definition 4.5.32. We see that in Example 9.3.10, the domains of <J>¿ and "P, are not 
connected sets. This is the reason for having so many possible local orientations. 
Problem 9.31 states that a collection of charts with the same connected domain can 
induce at most two different local orientations. 
Global Orientations 
A global orientation of a manifold is a collection of local orientations that cover 
the whole manifold and agree on their common domains. The precise definition is 
formulated in terms of atlases of compatible local orientations. Recall that atlases 
for manifolds were defined in Definition 9.2.2. 
Definition 9.3.12 Compatible charts. Let ^ 
: G¿ —> Hi, i = 1, 2, be two 
diffeomorphisms for a manifold M. They are called compatible charts if for every 
m £ Gi n G2 n M, they induce the same orientation on the tangent space Tm. 
Definition 9.3.13 Orientable manifolds. Global orientations. A manifold is called 
an orientable manifold if it has an atlas of compatible charts. An atlas of compatible 
charts for a manifold is called a global orientation of this manifold. 
Remarks 9.3.14 Not all manifolds are orientable. There are surfaces for which 
the unit normal vectors cannot be chosen in a continuous way. These surfaces are 
not orientable. A standard example is the Möbius strip. (See Problem 9.30.) 
Remarks 9.3.15 Outer boundary-surfaces. In this course we will consider only 
one example of a global orientation. This will be the outer boundary-surface of a 
set. It is introduced in Section 10.5, in connection with Stokes' theorem. 
Orientations of Surfaces and the Right-Hand Rule 
Surfaces are (n — 1)-dimensional manifolds in an n-dimensional space. In particular, 
planes are the (n — l)-dimensional subspaces of an n-dimensional space. Tangent 
spaces of surfaces are planes. An orientation of a surface consists of the orientations 
of its tangent planes. 

358 
INTEGRATION ON MANIFOLDS 
There is a useful way of visualizing the orientation of a plane. The normal space 
of a plane is one-dimensional. Hence, a plane has two unit normal vectors. We 
can establish a one-to-one correspondence between these two unit normal vectors 
and the two orientations of a plane. There is no intrinsic correspondence between 
these elements; such a correspondence is established by a convention. In physics this 
convention is usually referred to as the right-hand rule. 
Definition 9.3.16 The right-hand rule. Assume that T is an oriented plane in an 
oriented space Z. Let B be a positive basis for T. Then the corresponding unit 
normal vector n is specified by the requirement that (n, B) should be a positive basis 
for Z. Here, if B = (b2, ..., b„), then (n, B) = (n, b2, ..., b„). 
Remarks 9.3.17 Orientation of the tangent planes. Let * : G —> H be a chart for 
a surface M'mZ. Then G n M is oriented by this chart. Therefore, the tangent planes 
Tm at m e G il M are oriented planes. It is assumed that the underlying space Z 
is also an oriented space. Hence, the right-hand rule applied to Tm associates a unit 
normal vector n m to each point m e GC\M. This defines a function n : GC\M —> Z. 
The value of this function at m e G n M is a unit normal vector of Tm. Theorem 
9.3.20 shows that this is a continuous function. 
Definition 9.3.18 Outer normals of an oriented surface. The normal vectors 
obtained in Remarks 9.3.17 are called the outer normals of the oriented surface. This 
is in reference to the canonical case of orienting the outer boundary-surface of a set. 
This case is discussed in Section 10.5. Otherwise, the name of outer normal is used 
only as a convenient way of referring to these orienting normal vectors. 
Notations 9.3.19 Outer normal vector of U. The orientations of U, W, and 
Z were introduced in Notations 9.3.2. They are arbitrary but fixed orientations. 
When working with surfaces and with the right-hand rule, the outer unit normal 
vector of U is also important. We denote this (fixed) vector by e = ei. Also, 
from now on, E = (e2, • • •, en) is a positive orthonormal basis for U. Hence, 
(e, E) = (ei, e2, ..., e„) is a positive orthonormal basis for W. Recall that i? and 
Q are, respectively, the positive Euclidean determinants of Z and W. 
Theorem 9.3.20 An expression for outer normals. Let <3> : G —> H be an 
orientation-preserving chart for M. 
Let F(z) = e • í'(z), z G G. (Here we 
write x • y for the inner product (x, y)J Then 
n m = VF(m)/||VF(m)||, m e G n M , 
(9.32) 
is the outer unit normal vector of M at m s G n M, with respect to the orientation 
induced by the chart \P : G —> H. 

ORIENTED MANIFOLDS 
359 
Proof. Let a e G. We claim first that VF(a) ^ 0. The linear map *'(a) : Z -> W 
is an isomorphism. Let c = \I>'(a)_ e. Then VF(a) • c = e • ^'(a)c = e • e = 1, 
by Remarks 5.4.11 and Example 5.6.5. Hence n m as defined in the statement of the 
theorem is well-defined. Next, observe that F(z) = 0 is an implicit equation for 
G n M. Then Theorem 6.4.5 shows that F'(m)z = 0 is an implicit equation for the 
tangent space of G n M at m. But F'(m)z = 0 means that VF(m) • z = 0, again 
by Remarks 5.4.11. This shows that n m is a unit normal vector of G n M at every 
m G G n M. 
The next step is to show that n m is the outer unit normal, i.e., that (n m,B m) is a 
positive basis for Z where B m is a positive basis for the tangent space Tm. Now 
we know that ($'(b)e, $'(b)E) = (c, Bm) is a positive basis for Z, since ^ is 
orientation-preserving. Hence i?(c, B m) > 0. If c = (c • n m ) n m + ci, then 
ci e L . Since B m is a basis for Tm we see that #(ci, Bm) = 0. By multilinearity, 
this implies 
#(c, Bm) = (c • n m)0(n m, B m). 
Since c • n m > 0 and i9(c, Bm) > 0, we see that i?(nm, Bm) > 0. Hence n m is 
indeed the outer unit normal vector. 
□ 
Remarks 9.3.21 Continuity of the outer normals. Theorem 9.3.20 shows that the 
outer normal function n : G n M —> Z is continuous. The converse is also true. This 
is stated as Problem 9.32. 
The following theorem is useful in computations. Here d is a positive Euclidean 
determinant for Z and E is a positive orthonormal basis for U. 
Theorem 9.3.22 Volume multipliers and outer normals. If^-.G^Hisa 
chart, 
then p(<p'(u)) = i?(nm, B m). Here m = <^(u) andBm = <¿>'(u)E. 
Proof. By Definition 9.3.9 of the orientation on M, the basis 
ip'(u)E = $'(u)E = B m 
is a positive basis for the tangent space Tm. Then by Definition 9.3.18 of the outer 
normals, (n m, Bm) is a positive basis for Z. Therefore i?(nm, Bm) > 0. Also, 
since n m ± Tm, Theorem 9.1.12 shows that 
^(n m, Bm) = ||n m ||-^ T m(B m )|. 
Here i?rm is a Euclidean determinant for Tm. But B m is the image of an orthonormal 
basis for U. In this case, Lemma 9.1.14 shows that 
p(^'(u)) = |0T„(Bm)|. 
Then the conclusion follows. 
□ 

360 
INTEGRATION ON MANIFOLDS 
Problems 
9.24 
Let S : X —» Y be an isomorphism between two oriented Euclidean spaces. 
If the bases E and E' have the same orientation in X, then show that 5E and STE' 
have the same orientation in Y. 
9.25 
Let W and Z be oriented spaces. Show that an isomorphism T : W —> Z is 
orientation-preserving (Definition 9.3.3) if and only if it maps any positive bases of 
W to a positive basis of Z. 
9.26 
Let a be a determinant on W and let ß be a determinant on Z. Let T :W —> Z 
be an isomorphism. Show that a(E) • ß(TE) is nonzero and has the same sign for 
all bases E of W. 
9.27 
Show that an isomorphism is orientation-preserving if and only if its inverse 
is orientation-preserving. 
9.28 
Compatible isomorphisms. Let Pi :W —» Z be two isomorphisms between 
two oriented spaces. If they are both orientation-preserving or both orientation-
reversing, then they are called compatible isomorphisms. Show that two isomor-
phisms are compatible if and only if their inverses are compatible. Also show that 
compatibility is independent of the orientations of the spaces. 
9.29 
Let T : W —> Z be an orientation-preserving isomorphism. Is — T also 
orientation-preserving? Why? 
9.30 
Möbius strip. Define a surface E in cylindrical coordinates (r, ■&, z) as 
follows. For each a e K let 
Ha = { (r, i?, z) | r > 0, tf = a, z 6 R } 
be the d ~ a half-plane. Let the intersection of E with Ha be the line segment 
La = { (r, z) | r = 2 + icos(a/2), z = tsin(a/2), í € ( - 1 , 1) }. 
Show that E is not an orientable surface. 
9.31 
Let G be a connected set and let vf^ : G —> ií¿ be two charts for a manifold 
M. Let Tm be the tangent space of M at m e M. Show that the local orientations 
(\I>¿, Ej) induce either the same orientation on Tm for all m £ GflM, or the opposite 
orientations on Tm for all m G G D M. 
9.32 
Let M be a surface. Let n : M —» Z be a continuous function such that n m 
is a unit normal vector of M at every m 6 M. Show that the vectors n m are the 
outer normals of M with respect to some orientation. 

INTEGRALS OF VECTOR FIELDS 
361 
9.4 
INTEGRALS OF VECTOR FIELDS 
Vector-valued functions are usually called vectorfields. Vector fields can be integrated 
over surfaces or along curves. Such integrals are called, respectively, surface integrals 
and line integrals of vector fields. They depend upon the orientation of the manifold 
over which integration takes place. 
In the context of our discussion of the integration of vector fields, we will denote the 
inner product in Z as the dot product to conform to common usage. Hence we write 
a • b for (a, b), where a, b G Z. 
Definition 9.4.1 Vector fields. Let D be a set in a Euclidean space Z. A function 
h : D —► Z is referred to as a vector field on D. Hence a vector field h attaches a 
vector h(z) G Z to every point z G D. We shall be mainly concerned with vector 
fields f : C —> Z defined on a curve C, and with vector fields f : S —> Z defined on 
a surface S. 
Line Integrals of Vector Fields 
Curves are one-dimensional manifolds. Integrals of vector fields along curves are 
called line integrals. These integrals are usually computed by employing parametric 
representations, as discussed in 9.2.4. 
Remarks 9.4.2 Parametric representations of curves. Parametric representations 
of manifolds were defined in 9.2.1. For one-dimensional manifolds, the subspace U 
is one-dimensional. It is oriented by a unit vector u G U. We identify tu G U with 
i £ R . Then a parametric representation of C becomes a function of a real variable, 
(p : L —> Z, defined on an open set L c K. To conform to common notation, 
we denote this function as r : L —► Z. The derivative r'(i) = <E>'(iu) u G Z is the 
directional derivative of the reverse chart $ : H —> Z at the point tu G U C W in the 
direction of u. Hence r'(i) ^ 0 as it is the directional derivative of a diffeomorphism. 
Remarks 9.4.3 Integrals on curves. Integrals on manifolds were defined in 9.2.5. 
Let r : L ^ Z , L c R , b e a parametric representation for a curve C. Let / : C —> M. 
be a function. In this case we see that 
/ / = [ f(r(t))p(r'(t))du 
(9.33) 
Jc 
JL 
is an integral on R. Note that r'(i) G Z is the image of the unit vector u G U under 
the linear map $'(iu)|c/ : U -> Z. Then, by Definition 9.1.15, p{r'(t)) = ||r'(i)||^. 

362 
INTEGRATION ON MANIFOLDS 
The content on C is called the arc-length and usually denoted as s. Hence, by the 
notations in 9.2.8, 
fds 
= 
Í f(v(t))p(v'(t))dt 
(9.34) 
c 
f(r(t))\\r'(t)\\dt= 
/ f(r(t))ds(t). 
(9.35) 
L 
JL 
Definition 9.4.4 Orientation induced by r : L —» Z. The line U in W is oriented 
by the unit vector u. Hence r : L - » C c Z orients the tangent line at r(i) = c e C 
by the derivative vector r'(£). Then 
t c = r'(í)/||r'(í)||z 
(9.36) 
is the positive unit tangent vector of C at c € C. 
Definition 9.4.5 Line integrals of vector fields. Let C be a curve oriented by 
r : L —> C as in Definition 9.4.4. The line integral of a vector field f on C is defined 
as the integral of the real valued function /(c) = f (c) • tc, on C. Hence, by the 
remarks in Remarks 9.4.3 above, 
if 
= 
f t(r(t))-tr{t)\\r'(t)\\dt 
(9.37) 
JO 
JL 
= 
f f(r(t))-r'(t)dt. 
(9.38) 
Other notations for this integral are 
/ f • t = / ftds= 
/ f • ds. 
Jc 
Jc 
Jc 
(9.39) 
They suggest that ||r'(f) || dt = ds may be considered as the length of a small segment 
of the curve and 
t r ( t ) ||r'(f)|| dt = t r ( t ) ds = r'(t) du = ds(t) 
(9.40) 
as a small displacement in the positive direction of the curve. One well-known 
physical interpretation of Jc f • ds is that it is the work done by the force f when a 
particle travels along C in the positive direction. 
Surface Integrals of Vector Fields 
Let f : M —> Z be a vector field defined on a surface M. We will assume that f has 
a compact support K contained in the domain G of a chart "J : G —> H for M. This 

INTEGRALS OF VECTOR FIELDS 
363 
means we need only consider local integration on M. Full generality is obtained by 
applying the partitions of unity theorem, Theorem 9.2.11, as discussed in Definition 
9.2.12. 
Notations 9.4.6 Outer vectors of the surface. The surface M is locally oriented 
by the chart * : G -> H, as in Definition 9.3.9. Let n : G n M —► Z be the outer 
normal function of this orientation, as defined in Definition 9.3.18. 
Definition 9.4.7 Surface integrals of vector fields. Let M be a surface oriented 
by a chart "Í : G —> H. Let n : M —> Z be the outer normal function for this 
orientation. Let f : M —> Z be a vector field with a compact support K c G. Then 
the surface integral of f on M is defined as the integral of the real valued function 
/(m) = f(m) • n m, m £ M, on M. Hence, by Definition 9.2.5 of local integrals, 
f f = / n v ( u ) • f M u ) ) p(<f/(u)) du. 
(9.41) 
The second integral is the integral of a real-valued function on a Euclidean space. 
Theorem 9.4.8 Computations of surface integrals. Let E be a positive orthonor-
mal basis for U. Let i? be the positive Euclidean determinant in Z. Then 
( f = f i?(f(m), Bm)du. 
JM 
JU 
IM 
JU 
Here m = <p(\i) and B m = <//(u)E, where E w a positive orthonormal basis for U. 
Proof. We have 
if 
= 
/n^ ( u ).f(^(u))p((p'(u))du 
(9.42) 
JM 
JU 
nv{u) 
■ f(<p(u)) $(nv{u), 
(^'(u)E) du 
(9.43) 
I 
L 
l 
u 
n m • f (m) 0(n m, l m ) o!u 
(9.44) 
u 
i?(f(m), B m)du. 
(9.45) 
Here (9.42) is by Definition 9.4.7, and (9.43) follows from Theorem 9.3.22. To obtain 
(9.44) let m = <¿>(u) and B m = <¿>'(u)E. Finally, for (9.45), let 
f(m) = (n m ■ f(m))n m + f^m), 
(9.46) 

364 
INTEGRATION ON MANIFOLDS 
where fi(m) £ Tm is the tangential component of f(m). Substitute (9.46) into 
tf(f (m), Bm) and note that tf(fi(m), Bm) = 0. We obtain (9.45). 
□ 
Remarks 9.4.9 Special expressions in M3. Let Z = M3 be the xyz-space and 
U = R2 the wv-plane. Euclidean determinants can be expressed in a familiar way in 
R3. One Euclidean determinant in M3 is 
i?(a, b, c) = a - ( b x c ) . 
(9.47) 
Here b x c is the usual cross product. Assume that (9.47) is the positive Euclidean 
determinant of Z. The orthonormal basis E in U determines the coordinates (u, v). 
Let us denote the parametric representation íp:H(~)U^ZofGr\Mas 
r(u, v) to 
use more conventional notation. Here (u, v) axe the coordinates with respect to the 
orthonormal basis E = (e2, e^) of U. Hence 
ip'{u)e1 = ru(u, v) and <¿>'(u)e2 = rv(u, v) 
are the partial derivatives with respect to u and v. Then 
tf(nv(u), 
(p'(u)E) = nr{UtV) ■ {ru(u, v) x rv(u, v)) > 0. 
(9.48) 
Let f : R3 —> K3 be a C1 vector field of compact support K C G. Then 
/ 
f 
= 
/ 
f n 
(9.49) 
= 
/ (f -n)||r u xrv\\dudv 
(9.50) 
Ju 
f(r(u, v)) • ru(u, v) x rv(u, v)dudv. 
(9.51) 
Other familiar expressions for this integral are 
/ f -ndS= 
f 
f • dS. 
(9.52) 
JM 
JM 
Here dS = \\ru x r„|| du dv denotes the surface area element on S. To find the area 
of a part of S this is the expression we have to integrate over that part. The expression 
dS = ndS is sometimes called the vectorial area element. To obtain one physical 
interpretation of Js f • dS, consider f as the stationary velocity of a body of fluid in 
motion. Then this integral gives the volume of fluid that passes through S per unit 
time. Material in the next chapter on Stokes' theorem might make this interpretation 
more plausible. 

INTEGRALS OF VECTOR FIELDS 
365 
Problems 
9.33 
Define f : R2 —> R2 by i(x, y) = (xy, xy2). Compute Jc f from the point 
(0, 0) to the point (1, 1), where C is 
1. the parabola y = x2; 
2. the circle x2 + (y — l) 2 = 1, oriented counterclockwise; 
3. the circle x2 + (y — l) 2 = 1, oriented clockwise; 
4. the line y = x. 
9.34 
Define f : R3 —> R3 by f(x, y, z) = (xyz, xy2z, y). Compute Jci 
from 
the point (0, 0, 0) to the point (1, 1, 1), where C is 
1. the intersection of the parabolic cylinder z = x2 with the plane y = z; 
2. the shorter arc of the intersection of the circular cylinder x2 + (y — l) 2 — 1 
with the plane y = z; 
3. the longer arc of the intersection of the circular cylinder x2 + (y — l) 2 = 1 
with the plane y = z; 
4. the line x = y — z. 
9.35 
Let A be a connected open set in a Euclidean space Z. Let f : A —> Z be a 
continuous vector field. Show that the following are equivalent. 
1. There is a e1 function F : A -> R such that f = VF. 
2. If C is a curve in A, then the line integral Jc f depends only on the initial and 
the final points of C. More explicitly, if C¿ are two curves in A with the same 
initial point P and the same final point Q, then Jc f = Jc 
f. 
9.36 
Let B = { (x, y) | x < 0, y = 0 } and let A = R2 \ B. Define f : A -> R2 
as i(x, y) = (—y, x)/(x2 
+ y2). 
Show that there is an F : A —> K such that 
f — VF. Also, extend f : A -^ M2 to a vector field on R2 \ { 0 } in an obvious way 
and compute the line integral of this extended vector field over the circle x2 + y2 = 1, 
oriented counterclockwise. 
9.37 
Define f : R3 —> R3 as f(x, y, z) = (xyz, xy2z, z). Compute Jgi over 
the following surfaces S, oriented by taking normals with positive ^-coordinates. 

366 
INTEGRATION ON MANIFOLDS 
1. Upper half of the sphere x2 + y2 + z2 — 1. 
2. The part of the paraboloid 2 = 1 — (x2 + y2). 
3. The part of the cone z — (x2 + y2)1^2 
between the planes z — 1 and z = 2. 
4. The helicoidal surface x = r cos d, y = r sin ■&, z = ■&, where 1 < r < 2 and 
0 < Í? < 2TT. 
9.38 
A curve in R2 is also a surface. Let C be the unit circle x2 + y2 = 1 in R2 
oriented by the outer normals as a surface and oriented counterclockwise as a curve. 
Let F(x, y) = P(x, y)\ + Q(x, y)j be a C1 vector field. Express the line integral 
Jc f • ds and the surface integral j c f • dS as ordinary integrals on K. 
9.5 
INTEGRALS OF TENSOR FIELDS 
We have considered the integrals of scalar-valued functions on manifolds and the 
integrals of vector valued functions on lines and surfaces. These special cases are 
unified (and simplified) by introducing the integration of tensor-valued functions. 
Indeed, the change of variables formula becomes more transparent when stated in 
terms of tensor functions. Also, tensor-valued functions are essential in Stokes' 
theorem. 
We will not discuss the algebraic theory of tensors; instead, we use this term in a 
restricted sense as a name for alternating multilinear functions. (Commonly, any 
multilinear function may be referred to as a tensor.) 
Definition 9.5.1 Tensors. Let Z be a vector space. For any k G N let 
Afc(Z) = AMLk{Zk, 
R) 
(9.53) 
be the vector space of all alternating multilinear functions A : Zk —> R. 
Functions in Afc(Z) are called alternating tensors, or alternating k-tensors, on Z. 
As we shall consider only alternating tensors, we will simply refer to them as tensors 
or k-tensors. 
Definition 9.5.2 Tensor fields. Let D C Z. A function u) : D —► Ak(Z) is called a 
tensor field or a tensor field of order k on D . In this case 
w(z) 
6 
Afc(Z) for z e D and 
(9.54) 
w(z)(Z) 
= 
ui(z)(zu 
..., zfe) e R f o r z G D and Z G Zfe. 
(9.55) 

INTEGRALS OF TENSOR FIELDS 
367 
A tensor field ui : Z —> Afc(Z) takes values in a (finite-dimensional) normed vector 
space. Hence the continuity and the differentiability of these functions are well 
defined. 
Remarks 9.5.3 Integrations of tensor fields. Tensor fields of order k are integrated 
over fc-dimensional oriented manifolds. The process is the same as for vector fields. 
We first have to produce a real-valued function, defined in terms of the tensor field 
and the given oriented manifold, and then we can integrate this real-valued function 
over the manifold. 
For convenience, we shall assume that all integrals are local integrals, computed in 
terms of a single chart. This amounts to assuming that the tensor field has a compact 
support contained in the domain of one chart. The general case is obtained by 
applying the partitions of unity theorem, Theorem 9.2.11, as discussed in Definition 
9.2.12. 
We will also assume that ^ : G —> H is an orientation-preserving chart, as defined 
in Definition 9.3.4. Theorem 9.3.8 shows that this assumption implies no loss of 
generality. 
Definition 9.5.4 Integrals of tensor fields. Let M be a fc-dimensional manifold. 
Let * : G -> H be a chart for M that orients GflM. For each m e G n M, let B m 
be a positive orthonormal basis for the tangent space Tm. Let UJ : M —> Afc(Z) be a 
C1 tensor field of compact support K c G. The integral of u> on M n G is defined 
as the integral of the real-valued function /(m) = w(m)(Bm) over G (~l M. Hence 
M 
JU 
Theorem 9.5.5 Computation of integrals. Let E be a positive orthonormal basis 
for U. Let B m = B^u) = f'(u)E, 
where m = <p(u). Then 
/ w = 
/ w(m)(Bm)du 
(9.56) 
JM 
JU 
L 
Lü(if{u))(íp'(u)E)du. 
(9.57) 
Proof. Let m e G n M. The function w(m) : Zk —> R restricted to Tm
k becomes 
amemberof Afc(Tm). This space is one-dimensional since dim T m = k. Hence this 
restricted function is a multiple of the positive Euclidean determinant i?m e A& (Tm). 
Let w(m)(T) = g(m)i?m(T) for all fc-tuples T e Tm
k. By letting T = B m, we see 
that w(m)(Bm) = g(m) = /(m). Hence 
w(m)(T) = /(m)i?m(T) 
(9.58) 

368 
INTEGRATION ON MANIFOLDS 
f o r a l l T e T m \ Therefore 
f w 
= 
[ f(m)p{<p'(u))du 
(9.59) 
JM 
JU 
= 
f f(m)úm(Btn)du 
(9.60) 
Ju 
= 
[ ui(m)(Mm)du. 
(9.61) 
Ju 
To obtain (9.60), note that p{f'(u)) 
= |tf m(B m)| = 
tfm(Bm). 
Here, the first 
equality follows as in the proof of Theorem 9.3.22 and the second equality holds 
because $ m(B m) > 0, by the definition of $ m. The inference to (9.61) then follows 
from (9.58). 
a 
Connection with Vector Fields 
Each vector field f : D —+ Z, D c Z, defines two tensor fields, a tensor field £ 
of order one and a tensor field r¡ of order (n — 1), where n = dim Z. The £ field 
is defined in terms of inner products only. For the definition of the r¡ field, Z must 
be a space oriented by a Euclidean determinant ■& e An(Z). 
Then the integrals of 
£ on one-dimensional manifolds are the line integrals of f and the integrals of r¡ on 
(n — 1)-dimensional manifolds are the surface integrals of f. 
Definition 9.5.6 Tensor fields associated with a vector field. Let (Z, i?) be an 
n-dimensional oriented Euclidean space. Given a vector field f : D —> Z, where 
D c Z, define £ : D -> kx{Z) and r¡ : D -> A„_i(Z) as follows. For all a e D: 
£(a)(Z) 
= 
f(a)-zi, 
Z = (z^ e Z 1, and 
T?(a)(Z) 
= 
tf(f(a),Z), 
Z = ( z 2 , . . . , z n ) e Z n-i 
(9-62) 
Theorem 9.5.7 Let f be a vector field with the associated tensor fields £ and r/.IfC 
is a curve oriented by a parametric representation ¡p : U P\ H —> Z, then 
I 
£= / 
f- 
(9.63) 
Jena 
Jena 
If M is a surface oriented by a parametric representation (f : U C\ H —> Z then 
( 
v= f 
f. 
(9.64) 
JMnG 
JMnG 
In each case, G = §{H) where $ : H —* G is the inverse of the chart & : G —> H 
associated with the parametric representation ip = $|[/nff-

INTEGRALS OF TENSOR FIELDS 
369 
Proof. This is left as an exercise. 
D 
Remarks 9.5.8 Integrals of real-valued functions. Let (Z, #) be an oriented n-
dimensional Euclidean space. Associate a real-valued function/ : D —*• K and a 
tensor field £ : D —+ An(Z) through the relation 
C(a)(Z) = /(a)tf(Z), a G A Z = (zi, ..., zn) G Z". 
(9.65) 
An open set G C Z is an n-dimensional manifold in Z. An obvious chart for G is 
the identity map. Let E G Zn be a positive orthonormal basis. All tangent spaces of 
G are Z, and all are oriented by -d. Then we see that 
/ /(z)dz = / c ( z ) ( E ) d z = / c - 
(9.66) 
7G 
JG 
JG 
This is rather trivial but still very useful. 
Definition 9.5.9 Vector fields associated with tensor field. The relations between 
the vectors and tensors are symmetrical. Let A G Ai (Z) be a tensor of order one. 
Then A : Z —» K is a linear function. Hence, by the representation theorem for linear 
functions on Euclidean spaces, there is a vector f G Z such that A(z) = f • z for all 
z G Z. If £ : D —» Ai (Z) is a tensor field, then there is a vector field f : D —> Z 
such that 
C(a)(Z) = f(a)-z 1 ; Z = (Zl) G Z1 
(9.67) 
for all a G D. Lemma 9.5.10 gives a similar representation for A G A„_i(Z). 
Lemma 9.5.10 Let A G An_i(Z) with n = dimZ. Let ■& G A„(Z) be a determi-
nant. Then there is a unique vector p G Z such that 
A(Z) = tf(p, Z), /ora// Z G Z"" 1. 
(9.68) 
Proof. The case of n — 2 is left as an exercise. Note that the representation is 
different from the one in (9.67), even though n — 1 = 1 in this case. 
Now let n > 3 and A G A„_i (Z). Let E = (ej, ..., e„) be a basis for Z such that 
7?(E) = 1. Then for each e¿ G E, there is an E¿ G Z™-1 such that E¿ is a permutation 
of ej with j ^ i and such that í?(e¿, E¿) = 1. Let p¿ = A(E¿) and p = 
^¿Piei. 
Then an easy verification shows that (9.68) is satisfied. 
□ 
Change of Variables 
Let M be a manifold in Z. To simplify things, assume as usual that M is covered by 
a single chart <£ : G -> if with M C G. (Otherwise, replace M by M D G for this 
section, and use Partitions of Unity for the general case.) 

370 
INTEGRATION ON MANIFOLDS 
Let X be another space and fl : G —> X a diffeomorphism with F = ÍÍ(G) and with 
the reverse diffeomorphism T : F —* G. Then L = fl(M) is manifold in X. In fact, 
$ ■ T : F -»• i i is a chart for L. 
Let M be oriented by the chart * : G —> H and by a fixed orthonormal basis E in U, 
as in Definition 9.3.9. Let L be oriented by the chart "t • Y : F —> H. 
Deñnítion 9.5.11 Pullbacks of tensor ñelds. With the notation above, the pullback 
of the tensor field w : G —» Afc(Z) by the diffeomorphism T : F —> G is defined as 
r*(w)(x)(x) - W(r(x))(r'(x)x) 
(9.69) 
for x e F and X G Xfc. This is another tensor field T*(w) : F -> Afc(X). 
Figure 9.3. Diffeomorphisms in Theorem 9.5.12. 
Theorem 9.5.12 Change of variables for tensor fields. We have 
[ u= 
/ T » , 
(9.70) 
with the notations and assumptions above. 
Proof. Let T = T*(u). Let $ : if -> G be the reverse chart of Í» : G -> if and 
0 = Q • $ : H —> F the reverse chart of * • T : F —► H. Let z = $(u) and 
x = íí(z). Then x = 0(u). Also 9'(u) = íí'(z)$'(u). Then 
r(6(u))(e'(u)E) = w(r(e(u))(r'(x)e'(u)E) 
(9.7i) 
= u;($(u))($'(u)E). 
(9.72) 

INTEGRATION ON GRAPHS 
371 
The integral of w on M is the integral of w(y?(u))<y (u)E) = w($(u))($'(u)E) on 
U n H, and the integral of r = T*(ui) is the integral of r(0(u))(0'(u)E) on the 
same set U n H. Then the proof follows. 
G 
Lemma 9.5.13 Composition of pullbacks. Let A, B, and C be open sets in the 
Euclidean spaces X, Y, and Z, respectively. Let \I> : A —> Y and i> : B —» Z be 
diffeomorphisms with ^{A) = B. Then (<3> • *)*(w) = V* ($* (u))) for any tensor 
field to : Z -+ Ak(Z). 
Proof. This follows from the definitions. The details are left as an exercise. Note 
the change of order in ($ • *)*(w) = ty*($*(ui)). G 
Problems 
9.39 
Let Z be an n-dimensional oriented Euclidean space. Definition 9.5.6 shows 
that a vector defines a tensor of order one and another tensor of order (n — 1). If 
n = 2, then these orders are the same; show, however, that the associated tensors are 
different. What are the tensors on R2 associated with the vector (1, 0)? Conversely, 
each tensor of order one on R2 is associated with two vectors. What are the vectors 
in R2 associated with the tensor r : R2 —> R defined by T(X, y) = yl (You may 
assume that R2 has its standard orientation.) 
9.40 
Define f : R3 -*■ K3 by i(x, y, z) = (x2z, y2z, x2 + y2). Let M be the part 
oftheconez = (a;2+Í/2)1/2 between the planes z = landz = 2. Let M be oriented 
by the normals with positive ^-coordinates. Let B = { (x, y, z) j x = y = 0 } be 
the z-axis and G = R3 \ B. Define ft : G -> G by 
Sl(x, y, z) = (x, y, z - (x2 + y2)1'2 
+ (x2 + y2)). 
1. Show that ft is a diffeomorphism and find its inverse r : G —+ G. 
2. Find the tensor field w : M3 -> A2(M3) so that ¡M f = / M CJ. 
3. Let L = ft(M). Compute JM u> and JL r*(w) directly and verify the change 
of variables theorem, Theorem 9.5.12. 
9.6 
INTEGRATION ON GRAPHS 
Graphs form a simple class of manifolds. A graph is defined by a single special 
type of chart. Furthermore, content and integration on graphs can be interpreted 
geometrically. 

372 
INTEGRATION ON MANIFOLDS 
Notations 9.6.1 Graphs and coordinate systems. For this section, let (X, Y) be 
an orthogonal coordinate system for Z. Hence 
Z = X®Y~X 
xY, 
(9.73) 
where X _L Y. Let P : Z —> X and Q : Z —> Y be the corresponding coordinate 
projections. Let A c X be an open set and h : ^4 —> Y a C1 map. The graph of ft, is 
M = { (x, /i(x)) | x G A }. 
(9.74) 
Remarks 9.6.2 Graphs are manifolds. The graph of h is a manifold in Z. To see 
this, letG = H = AxY. 
Then 
*(x, y) = (x, y - h(x)) and $(x, y) = (x, y + A(x)) 
(9.75) 
define a chart <£ : G —» if for M and the reverse chart $> : H -+ G. We have 
f ( M n G) = X n if = A. 
(9.76) 
Hence <¿>(x, 0) = (x, h(x)), x G A, is a parametric representation for M. 
Remarks 9.6.3 Tangent spaces are graphs. Let a G A and let Ta be the (linear) 
tangent space of M at (a, h(a)) G M. Then Ta is the graph of the linear function 
/i'(a) : X -f F. 
Notations 9.6.4 Orthogonal projections of tangent spaces onto X. 
For each 
a G A let 
P|ra = Pa : Ta -> X 
(9.77) 
be the restriction of the orthogonal projection P : Z —> X to Ta. Then 
Ta 
= 
{ (x, /i'(a)x) | x G X } and 
(9.78) 
Pa(x, /i'(a)x) 
= 
(x, 0) for all x G X. 
(9.79) 
We see that P a : Ta —» X is an isomorphism. Its inverse Pg"1 : X —> Ta is given by 
P - ^ x , 0) = (x, /i'(a)x), x G X. 
Lemma 9.6.5 Let </?(x, 0) = (x, h(x)), x £ A, be the parametric representation of 
M obtained above. Then <¿/(a, 0) = P~l : X —> Tafor all a G A. 
Proof. We verify that the application of ip'(a., 0) : X —> Z to (x, 0) G X is 
¥>'(a, 0)(x, 0) = (x, /i'(a)x) = Pa
_1(x, 0). D 
(9.80) 

INTEGRATION ON GRAPHS 
373 
Lemma 9.6.6 Let f : M -> K. Then 
f f= 
//(x,ft(x))//>(P x)dx 
(9.81) 
JM 
JA 
if the second integral exists. 
Proof. We have ¡M f = ¡A /(<¿>(x, 0)) p(<p'(x, 0)) dx by the definitions in 9.2.5. 
Also, <¿/(x, 0) = P-1 implies p(y?'(x, 0)) = l/p(Px). 
□ 
Remarks 9.6.7 A geometric interpretation. Assume that the integral 
/ / ( x , / i ( x ) ) M P x ) d x 
exists. Then it can be approximated by a finite sum 
V , / ( a , ; / 1(a l))^(^)/p(Pa,), 
(9.82) 
z—j% 
where Ei s are pairwise disjoint sets in X and a¿ £ 7l n £"¿. Let B¿ = P~lEi, which 
is a subset of Ta¿. Then vx(Ei)/p(Pai) 
= Vi(Bi), where Í;¿ is the volume on the 
tangent space Tar Hence fM / can be approximated by 
y \ / ( a ¿ , h{ai))vi(Bi). 
(9.83) 
■^ — J % 
Here (a¿, /i(a¿)) e M and B* C Ta¿. 
Now the sets £?¿ are pairwise disjoint, since their projections E{ = Pa,Bi on X are 
pairwise disjoint. Therefore we can imagine the following procedure to compute 
the integral JM f. 
The manifold M is partitioned into finitely many small sets 
Mi = (f(Ei), each of these sets is replaced by the 'flat' set Bi = <¿?'(a¿, 0)_E¿ (a 
piece of the tangent space at a,), and then the sum in (9.83) is formed using the 
Euclidean volumes of the i?¿ s. 
Remarks 9.6.8 Formulations in terms of normals. The normal space Na of M 
at (a, h(&)) is defined as Na = Ta
L, the orthogonal complement of the tangent 
space. Let Qa = Q\N^ '■ Na —> Y be the restriction of the orthogonal projection 
Q : Z —» Y to the normal space Na. It is a useful fact that F a and Qa have the same 
volume multipliers. This result follows easily from Theorem 3.6.20. It is also stated 
as Problem 3.92. 
Remarks 9.6.9 Computations of volume multipliers. Let S : X —> Y be a linear 
transformation. Let T = { (x, Sx) | x e X } be the graph of S. Let L : X —> T 
be the mapping L(x, 0) = (x, Sx). The arguments above show that the volume 

374 
INTEGRATION ON MANIFOLDS 
multiplier p(L) plays an important role in integration on graph-manifolds. With the 
notations we have been employing, p(L) corresponds to l/p(Pe) at a particular point 
a € A with S — h'(a) : X —> Y. How do we compute p(L)l 
Let E = (ei, ..., efc) be an orthonormal basis for X. We know that 
p{L) = {det{Lei, Le^»1/2. 
(9.84) 
But Lei — (e¿, Sei) and (Lei, Lej)z 
— Sij + (Sei, Sej)y- 
Computations are 
simplified if we can exhibit an eigenbasis for S : X —> Y: an orthonormal basis E 
for X such that SE is an orthogonal set in Y. In this case, (Sei, Sej) = 0 if i ^ j 
and (Sei, Se^ = A¿ > O for some scalars A¿. Then we obtain 
p(L)2 = (l + X1)---(l 
+ \k). 
(9.85) 
Here the values A¿ are also the eigenvalues of the nonnegative definite self-adjoint 
transformation S*S : X —» X. 
The case of dim Y = 1 is especially important. In this case, the computations are 
simple. Assume that Y — K and S : X —> R is a real valued linear map. Hence 
there is a unit vector a € X and a scalar a such that Sx. = a (a, x) for all x € X. 
Complete a to an orthonormal basis U for X. This is an eigenbasis for S. All vectors 
in U are mapped to 0, except a. Hence p(L)2 = (1 + a2) in this case. 
Geometric Content 
Let M be a manifold in a Euclidean space Z. As we noted in Remarks 9.2.8, our 
definitions of integration and content on M depend upon a parametric representation 
that allows us to reduce integration on M to integration on a Euclidean space. To 
be sure, we showed that such integrals do not vary if we change the parametric 
representation. Nevertheless, there is no escaping the fact that, on these definitions, 
we rely upon extrinsic elements (charts and subsets of a Euclidean space) to define 
the existence and value of an integral on M. 
In the remainder of this chapter, we provide an intrinsic definition of content (and 
hence integration) on M in terms of the volume on the space Z in which the manifold 
is embedded. We shall call the content defined in this way geometric content. Then 
we prove the non-obvious fact that geometric content agrees with the standard notion 
of content defined earlier. 
The definition of geometric content depends upon the concept of enlargements of 
sets. 
Definition 9.6.10 Enlargements of a set. For any set K in Z and for any r > 0, let 
Kr = Uk£if-E?r(k) be the enlargement of K in Z by r > 0. Hence 
Kr = { z e Z | 3k £ K, ||z - k|| < r } . 
(9.86) 

INTEGRATION ON GRAPHS 
375 
Problem 8.52 shows that if K is a bounded set in Z, then Kr is always a Jordan set 
in Z. We will assume this fact for convenience. Otherwise, our definitions have to 
be formulated in terms of inner and outer volumes. 
Definition 9.6.11 Geometric content. Let E be a bounded set in an n-dimensional 
Euclidean space Z. Let k be an integer, 0 < k < n, and I = (n — k). Let Sg be the 
volume of the unit ball in M.e if Í > 1 and So = 1 • Then the k-dimensional geometric 
content of E is defined as 
Tk(E)= 
lim -^—vz{Er) 
(9.87) 
if this limit exists. 
For example, if E is a curve in K3, we find the 1-dimensional geometric content of 
E by considering sausage-like sets in R3 consisting of all points within r of E. We 
calculate the volume of the sausage, divide by irr2 (the area of the circle or radius r), 
and note whether this ratio converges to some value. 
Our main purpose in this section is to prove the following theorem. 
Theorem 9.6.12 Agreement of standard content with geometric content. Let 
(X, Y) be an orthogonal coordinate system for Z = X x Y with k = dimX and 
Í = dim Y. Let A be an open set in X, f : A —> Y a C1 function, and M the graph 
of f. Let <¿>(x) = (x, /(x)), x G A, be a parametric equation for M. If E is a 
Jordan set in X such that E C A and if L = f(E), then Tk(L) = o{L). 
The proof of this theorem is given after a few lemmas. The following comments 
might be helpful in following the arguments in those lemmas. 
Remarks 9.6.13 Comments on the proof of Theorem 9.6.12. Fubini's theorem 
allows us to compute vz(Lr) 
(the volume of the enlargement of L) by integration 
over E (essentially), as fE A(x) dx. Here A(a) = vy(C(Lr, a)) is the volume in 
Y of the cross-section C(Lr, a) of Lr with the "vertical space" Y (a). This vertical 
space consists of vectors with constant X components x = a. 
The cross-section C(Lr, a) consists of all vectors in V(a) that have a distance less 
than r to L. If L is replaced by the affine tangent space at (a, /(a)), then the 
orthogonal projection of C(Lr, a) on the normal plane N(a) is a ball of radius r. 
Therefore 
vY(C(Lr, 
a)) ~ vY(Br(0))/p(S(a)) 
= / 5€/p(S(a)). 
(9.88) 

376 
INTEGRATION ON MANIFOLDS 
Here p(5(a)) is the volume multiplier of the orthogonal projection 5(a) : Y —> 
N(a). This is the same as for the orthogonal projection Y (a) —* TV (a). 
As observed in Remarks 9.6.8, we have p(S(a)) — p(R(a)). Hence 
vz{Lr) 
~ 
/ reSe/p(S(x))dx 
= f reSe/p(R(x))dx 
(9.89) 
JE 
JE 
= 
/ S¿ / (piRix)))'1 
dx = / S*<T(L). 
(9.90) 
JE 
The formal proof of Theorem 9.6.12 consists of justifications for the various approx-
imations we have made in these comments. 
Definition 9.6.14 Cross-sections. Let H be a set in Z = X x Y. If a e X, then 
the cross-section of H at a G X is 
C(H,a) 
= {yeY\(a,y)eH}cY. 
(9.91) 
The following two lemmas are not stated in terms of cross-sections, but they will be 
used to obtain estimates on various cross-sections. 
Lemma 9.6.15 Let A be an open set in X and let /, g : A —> Y be two functions. 
Let Bro (a) C A. For each r, 0 < r < 7*0, define 
A(a,r) 
= 
{yeY\3xeX, 
||(x, /(x)) - (a, y ) | | z < r } , 
(9.92) 
B(a,r) 
= 
{ y e F | 3 x G X , ¡ | ( x , 5 ( x ) ) - ( a , y ) b < r } . 
(9.93) 
Assume that there are e > 0 and <5, 0 < S < TQ, such that 
ll/(x) - s(x)||y < e||x - a||x whenever ||x - a||x < ¿-
77ien B(a, (1 — e)r) C A(a, r) C B(a, (1 + e)r) whenever 0 < r < 5. 
Proof. Let 0 < r < 5 and y £ A(a, r). Then there is an x G X such that 
||(x, /(x)) - (a, y ) | | | = ||x - a||2
x + ||/(x) - y\\2
Y < r2 < Ö2. 
Hence ||x — a||x < 8 and ||/(x) — </(x)|]y < e||x — a||x < er. Therefore 
||(x l f l(x))-(a,y)|| 2 
= 
| | x - a | | 2 + || f l(x)-y|| 2 
< 
||x-a|| 2 + (||/(x)-y|| + ||ff(x)-/(x)||)2 
< 
| | x - a | | 2 + ( | | / ( x ) - y | | + £ r ) 2 
= 
||x - a||2 + ||/(x) - y||2 + 2 £r||/(x) - y|| + e2r2 
< 
r2 + 2er2 + e2r2 = (l+e) 2r 2. 

INTEGRATION ON GRAPHS 
3 7 7 
This shows that y 6 B(a, (1 + e)r). Hence A(&, r) C B(a, (1 + e)r) whenever 
0 < r < S. By the symmetry between these sets, we also have 
B(a, (1 - e)r) C A(a, (1 + e){l - e)r) c A(a, r) 
whenever 0 < r < a. Then the conclusion follows. 
□ 
Lemma 9.6.16 Let T C Z — X xY be the graph of a linear function F : X —> Y, 
where Y is an I-dimensional space. For each a e l and r > 0, let 
B(a, r) = {y G F | 3 x e X, ||(x, Fx) - (a, y)||z < r } . 
77je« «y(ß(a, r)) = re Sg p(R)~1. 
Here S¿ is the volume of the unit ball in Y, 
R = P(T, X) : T —> X is the orthogonal projection ofT to X, and p(R) is the 
volume multiplier of R. 
Proof. Weseethaty G B(a, r) if and only if there is at G T such that || (a. y)-t|| < 
r. Let N = TX and let S : Y —> N be the orthogonal projection of Y to TV. Then, 
by the properties of orthogonal projections, 
||S(a,y)||=mint eT||(a,y)-t||. 
Therefore (a, y) G B(a, r) if and only if 5(a, y) G BT(0). 
Here Br(0) c N is 
the ball of radius r in TV about the origin of N. Hence 
p(S)vY(B(a, 
r)) = vN(Br(0)) 
- rlSt. 
Then the proof is concluded by p(S) = p(R), as observed in Remarks 9.6.8. 
G 
Remarks 9.6.17 Review of assumptions and notation. Let X and Y be two 
Euclidean spaces with dim X = k and dim Y = I. Let A be an open set in X and 
/ l i - s V a C 1 function. Then y = /(x) is an explicit equation for the manifold 
M = {(x,y)eZ 
= XxY\xeA,y 
= /(x) } . 
(9.94) 
A parametric equation for M is ip : A —> Z, with <¿?(x) = (x, /(x)) G Z, a G A. 
At the point (x, /(x)) G M, the linear tangent space of M is T(x). Also, p(i?(x)) 
is the volume multiplier of the orthogonal projection Ä(x) : T(x) —> X. Finally, 
L = </>(£) C M, where F is a Jordan set such that E C A. 
Lemma 9.6.18 Let P, Q, R, and E be Jordan sets in X such that 
T3 c E° cE 
c Q° CQ C R° CR c A. 
(9.95) 

3 7 8 
INTEGRATION ON MANIFOLDS 
Let L = <p(E). Then for each e > 0 there is a 6 > 0 such that ifO < r < 5, then 
(1 - s)e [ p(Rix))-1 
d-K < r-'S^vziLr) 
< (1 + e)e [ POR(X))- 1 dx.(9.96) 
JP 
JQ 
Proof. First, find an ro > 0 such that if 0 < r < ro, then 
PrcE 
cErcQcQrCR. 
(9.97) 
This can be done because of the assumptions in (9.95). Here, as implicit in (9.97), 
all enlargements are in X. Now use the uniform continuity of / ' : A —> L(X, Y) on 
the compact set R to find a 5 > 0 such that 0 < S < ro and such that 
\\f'{u)-f'(-v)\\L(XiY)<e 
whenever u, v € Äand ||u ^ v|| x <S. 
(9.98) 
We claim that if a G Q and if 0 < r < 5, then 
||/(x) - /(a) - /'(a)(x - a)|| < <■ ||x - a|| 
(9.99) 
whenever ||x — a|| < 8. This follows from the mean value theorem applied to 
i?(x) = /(x) - /'(a) x- 
S i n c e ^'( x) = /'( x) - /'(a), we see that j|tf'(x)ll < e 
whenever ||x — a|| < S. Therefore 
||/(x) - /(a) - /'(a)(x - a)|| = ||0(x) - 0(a)|| < e||x - a|| 
(9.100) 
whenever ||x — a|| < S. Now at every point a 6 H, apply Lemma 9.6.15 with /(x) 
and g(x) = /(a) + /'(a)(x — a). We obtain 
B{SL, (1 - e)r) c A(a, r) c £(a, (1 + e)r) 
(9.101) 
whenever a e H and 0 < r < 5. Also, i/y (£?(a, r)) = riSep(R(a))^1 
by Lemma 
9.6.16. Hence 
(1 - eYr'Sepim*))'1 
< vy(A(a, r)) < (1 + sYrlStP{R{a)rx 
(9.102) 
whenever a G Q and 0 < r < ¿>. Now if 0 < r < ¿, then we see that 
C(Lr, a) = A{a, r ) i f a e P , 
(9.103) 
C(Lr, a) c A(a, r) if a € Q, and 
(9.104) 
C(Lr, a) = 0 i f a 0 Q . 
(9.105) 
We have vz(Lr) = JA vy{C{Lr, 
x)) cix by Fubini's theorem. Hence 
/ vY{A(x, 
r))dx < v z(L r) < / fy(A(x, r))dx. 
(9.106) 

INTEGRATION ON GRAPHS 
379 
Then the proof follows by the estimates in (9.102). 
□ 
Proof of Theorem 9.6.12. Let P, Q, and R be as in Lemma 9.6.18 above. We have 
IE 
as observed in Remarks 9.6.8. Hence 
a(L)= 
i Kñ(x))-1 dx, 
JE 
[ p(Rix))-1 
dx < a(L) < [ piRix))-1 
dx. 
(9.107) 
Jp 
JQ 
The difference between these two integrals can be made arbitrarily small. In fact, the 
continuous function p(R( ■ ))~1 : A —> ]R is bounded on the compact set R C A. If 
M is an upper bound for p(R( ■ ))~1, then 
0 < /* / ^ ( x ) ) - 1 dx- 
[ piRix))'1 
dx < Mvx{Q 
\ P). 
(9. 
Jo 
Jp 
108) 
Given any £ > 0, we can choose the sets P and Q in Lemma 9.6.18 to make 
vx (Q\P) 
< £• Then the proof follows by comparing the estimates in (9.96) and 
(9.107) above. 
D 
Example 9.6.19 Surface area of a sphere. Let T,n(R) be the surface of the sphere 
BR(0) 
C R". We will show that it has a positive (n — 1)-dimensional geometric 
content. The enlargements of T,n(R) are 
E„(fi)r 
= 
BR+r(0)\BR_r(0). 
Hence 
(9.109) 
un(En(Ä)) 
= 
vn(BR+r(0))-vn(BR-r(0)) 
(9.110) 
= 
((R + r)n-(R-r)n)Sn. 
(9.111) 
Since £ = n— (n — 1) = 1 in this case and since S± = 2, we have 
T ^ D ^ G R ) ) = lim 
{ 
\
[ 
,-Sn^nRn-1Sn. 
(9.112) 
Hence n(E2(i?)) = 2nR and r2(E3(ñ)) = 4TTR2, since 5 2 = n and 5 3 = (4/3)TT. 

This page intentionally left blank

CHAPTER 10 
STOKES' THEOREM 
Stokes' theorem generalizes the fundamental theorem of calculus to functions of 
several variables. In this chapter, we prove this major result in two different ways. 
The first way is via direct generalization of the fundamental theorem of calculus for 
functions of one variable. This approach leads to a proof that is concise but difficult 
to motivate. Our second approach employs the concept of flows. This second method 
arguably comes closer than the first to replicating the intuitions and ideas behind the 
original proofs of Stokes' theorem. Although the proof is easier to motivate, it is 
somewhat lengthy because it requires background preparation pertaining to flows. 
The first step in both proofs is to establish Stokes' theorem for a special case. Indeed, 
the two proofs differ only at this first step. We shall refer to this special case as 
the basic Stokes' theorem. The general case is obtained by the application of two 
different tools. The first is to pass from special regions to more general regions by 
using diffeomorphisms. The second is to move from local results to global results 
by employing the partitions of unity. Both of these steps are fairly routine, and they 
operate in the same manner in all applications. 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
381 

382 
STOKES' THEOREM 
Stokes' theorem is stated in terms of a vector field and the "divergence" of that 
vector field. These are, respectively, vector-valued and real-valued functions. In 
determining how these functions change under diffeomorphisms, the most fruitful 
strategy is to represent them as tensor fields and use the pullbacks of these fields. 
The basic idea of this strategy was developed in chapter 9. 
Throughout this chapter, inner products are denoted as dot products, as is customary 
in discussions of Stokes' theorem. 
10.1 BASIC STOKES' THEOREM 
Let Z be a Euclidean space. The basic Stokes' theorem is stated for a C1 vector field 
f : G —> Z that is defined on an open set G c Z and has a compact support K c G. 
Recall that K contains the closure of the set { z | f (z) / 0 }. We may assume that K 
is a Jordan set. Also, if one defines f (z) = 0 for z ^ G, then the extended function is 
still a C1 function. Hence we may assume, without loss of generality, that the vector 
field is defined on the whole space Z. 
Divergence of a Vector Field 
Definition 10.1.1 Divergence of a vector field. Let f : Z —> Z be a C1 vector field. 
Its divergence div f : Z —-> R is defined as 
(divf)(z) = Trf'(z), z e Z . 
(10.1) 
Here f'(z) : Z —> Z is a linear transformation and Trf'(z) is its trace. 
Remarks 10.1.2 Trace of a linear transformation. Suppose that (ei, ..., en) is 
an orthonormal basis of Z and y is a Euclidean determinant with ip(ei, ..., e„) = 1. 
For any linear transformation T e L(Z, Z), 
TrT = iP(Te1,...,en) 
+ ---+tp{e1, 
..., Ten) 
(10.2) 
(See Definition C.7.3 in Appendix C.) 
Note that Tr :L(Z, Z) —> E is a linear function. It follows that divergence is a linear 
operation on vector fields: 
div(af + ßg) = a d i v f + /3divg. 
(10.3) 
Another basic property of the trace is that 
T r T = I i m
d e t ( / + ^ -
1 , 
(10.4) 
t-o 
t 

BASIC STOKES' THEOREM 
383 
where / e L(Z, Z) is the identity. This result is obtained in Theorem CIA. 
Lemma 10.1.3 Suppose Tz = S(z) p, where S : Z —► R ¿s a linear transformation 
and p € Z is a fixed point. Then T : Z —> Z is linear and TV T = S(p). 
Proof. The linearity of T is obvious. Since S : Z —> R is linear, there is a fixed 
a e Z such that 5(z) = a • z for all z e Z. Let (ei, ..., e„) be an orthonormal 
basis such that a = ae\. Then Te\ = ap and Te¿ = 0 if i / 1. Hence 
TrT 
= 
<p(ap, fa, ..., en) 
(10.5) 
= 
f(a(ei 
-p)ei, e2, ..., e„) 
(10.6) 
= 
aei •p = a p = 5'(p). 
(10.7) 
Here (10.5) follows from the definition of trace in (10.3) and the fact that Te¿ = O if 
i T¿ 1. To obtain (10.6), write 
p = (p-ei)ei + p 2 , 
where p 2 lies in the space spanned by (e2, ..., e n). Then use the multilinear-
ity of (f and the fact that c/?(p2, e2, ..., en) = 0. For (10.7), simply note that 
¥>(ei, ..., en) = 1. D 
Lemma 10.1.4 Suppose f = fp, where f : Z —> R w a e1 function and p & Z 
is fixed. Then for any u. E Z, divf(u) = / ' ( u ) p = /'(u; p), the directional 
derivative offatu 
along p. 
Proof. Let u € Z. Then S = /'(u) : Z —> K is a linear transformation and 
f'(u) z = (/'(u) z)p = 5(z)p. Hence the result follows from Lemma 10.1.3. 
D 
Corollary 10.1.5 Let (e 
i) • ■ • > en) be an orthonormal basis for Z with the corre-
sponding coordinate functions (zi, ..., zn). Iff is expressed as 
f = Z/
¿ e" 
(10-8) 
then 
divf(z)=y\d/i/ÖZi. 
(10.9) 
L—ji 
Equation (10.9) is the most common way to express div f. 
Proof. If f = fi e¿, then the result follows from Lemma 10.1.4. The general case 
follows from the linearity of the divergence operator, as stated in (10.3). 
□ 

3 8 4 
STOKES' THEOREM 
Basic Stokes' Theorem: direct version 
Definition 10.1.6 Two sides of a plane. Let n e Z be a unit vector. Let U be the 
plane z ■ n = 0. The sets 
,4 = { z | z - n < 0 } a n d S = { z ¡ 0 < z - n } 
(10.10) 
are called, respectively, the lower and the upper sides ofU. The vector n is called 
the outer unit normal vector of U. 
Notations 10.1.7 Two types of integrals. The following arguments involve integrals 
over two Euclidean spaces. There are integrals in Z, and integrals on planes like U. 
(If dim Z = n, then the two types are essentially integrals on R" and integrals on 
Rn_1.) These integrals will be denoted by expressions of the form Jz /(z) dz and 
fu /(u) du. The second integral involves only values /(z) for z = u e U. 
Theorem 10.1.8 Basic Stokes' theorem. Let f : Z —> Z be a C1 vector field of 
compact support. If A and U are as in Definition 10.1.7, then 
/ divf(z)dz = / n-f(u)du. 
(10.11) 
JA 
JU 
Proof. Complete n = ei to an orthonormal basis (ei, ..., e„). Let the correspond-
ing coordinate functions be (z\, ..., zn). Since f has compact support, we can find 
anM > 0 such that if f(z) ^ 0, then-M < z¿ < MforalH. Both sides of (10.11) 
are linear in f. Hence it is enough to prove the result for f = /¿e¿. But we have to 
treat the cases i = 1 and i ^ 1 separately. We see that (10.11) states 
dfi -(z)dz 
= 0 ifi¿ 
land 
(10.12) 
IA UZi 
J 
| ^ ( z ) d z = J /i(u)du. 
(10.13) 
We evaluate the integrals on the left-hand side by Fubini's Theorem. First, let i ^ 1 
be fixed. Decompose Z as Z = V x X, where X is the one-dimensional space 
spanned by e¿ and V = X
A-. Write each point in Z as z = v + xe¿ = (v, x), with 
v £ V and x = zt. Then we see that 
[ ^(z)dz 
= [ 
[~{v,x)dxdv 
(10.14) 
JA uzi 
JvnA JR VX 
( 
i 
~(v,x)dxdv 
(10.15) 
JVHAJ-M 
®X 
[ 
(/i(v, M) - /i(v, -M)) dv 
(10.16) 
JvnA 
i 
( 0 - 0 ) d v = 0. 
(10.17) 
JvnA 

BASIC STOKES' THEOREM 
385 
This proves the result for i ^ 1. 
For i = l, decompose Z as U x Y where Y is the one-dimensional space spanned 
by e! = n. Denote the points in Z as z = u + yn = (u, y) with u G Í7 and y = Z\. 
Then we see that 
f f ^ ( z ) d z 
= 
/ 
/ 
^A(u,y)dydu 
(10.18) 
7A dzi 
7c/ 7YHA <9y 
_ (u, y)dydn 
(10.19) 
c/J-M ¿y 
/ " ( / i ( u , 0 ) - / i ( u , - M ) ) d u 
(10.20) 
f(f1(u,0)-0)du 
(10.21) 
/i(u)du. 
(10.22) 
For the last step, the point z = (u, 0) is expressed as z = u + On = u. This 
completes the proof of the theorem in the general case. 
□ 
Problems 
10.1 
On the basis of Corollary 10.1.5, we use V • f as another notation for 
divf = div'V./¿el = yZ-T1-
Here (ei, ..., en) is an orthonormal basis for Z and z¿s are the corresponding 
coordinate functions. If f = VF, then show that divf = ^i{d2F/dz2). 
This is 
also expressed as 
divf = V - V F = V 2F = AF. 
The expression V 2F = AF is called the Laplacian of F. A function F is called a 
harmonic function if AF = 0. 
10.2 
Show that V • (F VG) = (VF) • (VG) + FV2G. 
10.3 
Let Z be an n-dimensional Euclidean space. Let r = ||z||. Show that 
F(z) = r2~™ is a harmonic function (Problem 10.1) in Z \ { 0 }. Also, if n = 2, 
then show that F(z) = logr is a harmonic function in Z \ { 0 }. 
10.4 
Let B = { (x, y) \ x < 0, y = 0 } and let A = M2 \ B. Define 
d : A —> (—7T, 7t) by cos?? = x(x2 + y 2) - 1/ 2 and sin?? = y(x2 + y 2) - 1/ 2. 

3 8 6 
STOKES' THEOREM 
Show that i? is a harmonic function. See also Problem 9.36. 
10.2 
FLOWS 
Definition 10.2.1 Flows of compact support. Let Z be a Euclidean space. Let 
J c R be an open interval containing 0 e R. A flow (of compact support) is a 
mapping F : (Z x J) —> Z that satisfies the following conditions. 
(1) There is a compact set K c Z, called a support of F, such that 
F(z, i) = z for all (z, i) e Kc x J = (Z \ K) x J. 
(2) F(z, 0) = z for all z € 2. 
(3) F : (Z x J) -> Z is a C1 function. 
(4) For each fixed t € J, the mapping 
F( •, i) : Z —> Z is a diffeomorphism of Z o«ío Z. 
A flow can be considered as specifying the motion of a set of particles, such as the 
particles in a fluid (think of a swirling pond). The particle that was at z at initial 
time t = 0 moves to the position F(z, t) at time t E J. The particles outside the 
compact set K do not move at all. This may seem like an important restriction, but 
it is assumed only for technical convenience. See 10.2.7 for further remarks. 
Notations 10.2.2 Derivatives of flows. Velocity fields. The derivative of a flow 
F : (Z x J) —> Z at a point (a, a) e (Z x J) is a linear transformation F'(a, a) : 
(Z x E) —> Z. We express the application of this linear transformation to a vector 
(z, i) e (Z x R) as 
F'(a, a)(z, i) = (£>zF)(a, a)z + t {dF/dt){a., a). 
(10.23) 
Here DZF : (Z x J) -> L(Z, Z) is the derivative of F restricted to Z and (dF/dt) : 
(Z x J) —► Z is the partial derivative of F with respect tot G J. Hence DzF can be 
considered as the space derivative of F and (dF/dt) the time derivative of F. More 
explicitly, DzF(a., a) : Z —* Z is the unique linear transformation that satisfies 
lim lina + Z ; Q ) - F ( a a ) - D z F ( a , a ) z l l 
= 
z^O 
||z|| 
and (dF/dt)(a, 
a) = (dF)/(öi)(a, a) e Z is the vector defined by 
dF 
F(a, a + t)- F(a, Q) 
-—(a, a) = lim 
. 
(10.25) 
dt 
t->o 
t 

FLOWS 
387 
We call (dF/dt)( •, t) : Z -» Z the velocity field of the flow at the time t G J. If a 
particle is at the point z £ Z at initial time t = 0, then at a general time t G J it is at 
the point F(z, t) £ Z with the velocity of (dF/dt){z, 
t) e Z. 
Definition 10.2.3 The initial velocity field. Velocities at the initial time t = 0 will 
be important. We let f(z) = (9F/9f)(z, 0) and define f : Z —> Z as the initial 
velocity field. 
Smooth Flows 
One usually calls a function a smooth function if it has derivatives of all orders. For 
our arguments below, we need much less. 
Definition 10.2.4 Smooth flows. A function F : (Z x J) —> Z is said to satisfy the 
smoothness condition if the second-order mixed partial derivatives 
d(DzF)/dt 
and Dz{dF/dt) 
(10.26) 
exist, are equal to each other, and are continuous functions Z x J —► L(Z, 2"). A 
function F : (Z x J) —> Z is said to be a smooth flow if it is a flow and it satisfies 
the smoothness condition. From now on, all flows are assumed to be smooth flows 
even if this is not explicitly stated. 
Smooth Flows with Given Initial Velocities 
If F : Z x J ^ Z isa smooth flow, then its initial velocity field 
f(-) = ( O F / d t ) ( - , 0 ) : Z - » Z 
(10.27) 
is a C1 function. In fact, by the smoothness assumption, 
Dz{( •) = Dz(dF/dt){ 
■, 0) : Z - L(Z, Z) 
(10.28) 
exists and is continuous. Theorem 10.2.5 shows that the converse is also true. 
Theorem 10.2.5 Construction of a smooth flow. Let f : Z —> Z be a C1 function 
of compact support K C Z. Then there is an r > 0 such that F(z, t) = z + if (z) 
is a smooth flow F : Z x (—r, r) —» Z. The support of this flow is K and its initial 
velocity field is i : Z —> Z. 
Proof. We have F(z, 0) = z for all z e Z and F(z, i) = z for all f e R and for all 
z <£ K. We first show that F satisfies the smoothness condition of 10.2.4. In fact, 
DzF{z, 
t) = I + tDzf(z) 
and (dF/dt){z, 
t) = f (z) 
(10.29) 

388 
STOKES' THEOREM 
show that (dDzF/dt)(z, 
t) = Dz(dF/dt){z, 
t) = Dzi{z). 
Also, this is a contin-
uous function Z —> L(Z, Z), as required. 
Now we need to find an r > 0 such that F : Z x (—r, r) —> Z is a flow. (Combining 
this with the preceding paragraph gives us our conclusion that F is a smooth flow). 
This means that we need to show that for each fixed t G (—r, r), F(-, t) : Z —> Z is 
a diffeomorphism of Z onto Z. 
The function D^f : Z —> L(Z, Z) is a continuous function of compact support. 
Hence there is an M > 0 such that \\Dz{(z)\\L(Z,z) 
< M for all z £ Z. Let 
r = 1/(2M) and |i| < r. If F(a, i) = F(b, i), then a + if (a) = b + if (b) and 
||b - a|| = |i| ||f(b) - f(a)|| < |£| M||b - a|| < (l/2)||b - a|| 
(10.30) 
Hence ||b — a|| = 0 and a = b, so F( ■, t) is one-to-one. (The first inequality above 
follows from the mean value theorem.) Also DzF(z, 
t) = Iz + tDzi(z), 
where 
Iz ■ Z -► Z is the identity mapping. But \\tDz{(z)\\L{ZtZ) 
< \t\ M < (1/2), so 
DzF(z, 
t) is an invertible element in L(Z, Z). This last part follows from Theorem 
4.4.38. 
Hence if |i| < r, then F( ■, i) : Z —> Z is a one-to-one and C1 function with an 
invertible derivative at each point. We will show that it also maps Z onto Z. The 
inverse function theorem shows that F(-, t) : Z —> Z maps open sets to open sets. 
Therefore Rt = F(Z, t) (i.e., the image of the entire space Z under F( ■, t)) is an 
open set, since Z is open. 
We claim that Rt is also a closed set. Let q„ be a sequence in Rt converging to a 
point q e Z. Since F(-, t) is one-to-one, there is a unique sequence p„ € Z such 
that F(p„, i) = qn. We see that p„ is a bounded sequence, since F(z, t) = z for 
all z outside the compact set K C Z. Hence, by the Bolzano-Weierstrass theorem, 
it has a convergent sub-sequence. Without loss of generality, assume that p„ —» p. 
Then q„ = F(p„, i) -^ F(p, i), by the continuity of F( ■, i). Since q„ —> q we 
see that q = F(p, i) € Rt- Hence Rt is closed, and is therefore both open and 
closed. Since Rt is not empty, we must have Rt = Z. 
□ 
Remarks 10.2.6 Non-uniqueness of flows. Differential equations. There is no 
uniqueness result for a flow with a given initial velocity field f : Z —> Z. The 
particular flow in Theorem 10.2.5 above may just be the simplest flow to construct 
starting with a given initial velocity field. In the theory of differential equations, one 
is interested in particular flows that satisfy the additional requirement that 
dF 
— (z, i ) = f ( F ( z , t)) 
(10.31) 
for all z G Z and t G J. Here is an intuitive characterization of this condition: the 
velocity of a particle at a time t depends only on the position F(z, t) that the particle 

FLOWS 
389 
has reached at that time. When a particle arrives at a point z G Z at a certain time, 
its velocity is f (z) at that time. The existence and the uniqueness of flows that meet 
this condition are discussed in the theory of differential equations. 
Remarks 10.2.7 Flows of compact support. We consider only flows of compact 
support. This is not an important restriction. We are interested in flows generated by 
velocity fields and in their behavior over bounded regions E c Z. If f is a velocity 
field without a compact support, then one can take a C1 function A : Z —> [0, 1 ] of 
compact support which is 1 on an open set G containing the closure of E. Then the 
velocity field Af has compact support. Flows generated by f and by Af have the same 
initial velocities on G. Hence the initial behavior of a flow on G can be understood 
in terms of a flow of compact support. 
In applying this reasoning, it may be sufficient to take A as a C1 function. In this 
case, the example in Lemma 6.1.6 can be used. If necessary, A can be taken to be a 
C°° function, as obtained in Appendix D on partitions of unity. 
Problems 
10.5 
Let Z be a Euclidean space. Let J be an open interval containing 0. A 
function fl : Z x J —> Z is called a displacement function if it has the following 
properties. (1) There is a compact set K c Z such that fi(z, f) = 0 for t G J and 
z £ K. (2) fi(z, 0) = 0 for all ze Z. (3) Ü : {Z x J) -^ Z is a C1 function. (4) 
The second order mixed partial derivatives 
d(Dzn)/dtandDz(dü/dt) 
(10.32) 
exist, are equal to each other, and are continuous functions Z x J —> L(Z, Z). Show 
that if F : Z x J —> Z is a smooth flow, then fi(z, t) — F(z, t) — z is a displacement 
function. Conversely, show that if ÍÍ (z, t) is a displacement function, then there is an 
r > 0 such that (—r, r) C J and such that F(z, t) = z + f2(z, t) defines a smooth 
flow Z x (-r, r) -> Z. 
10.6 
Let A : Z —► Z be a linear transformation. Let G(z, i) = etAz, t G K, as 
defined in Example 7.3.13. Let A : Z -* [0, 1 ] be a C00 function such that A(z) = 1 
for ||z|| < 1 and A(z) = 0 for ||z|| > 2. Show that there is an r > 0 such that 
F(z, t) = A(z)G(z, t) + (1 - A(z))z 
is a flow F : Z x (—r, r) —> Z. What are the initial velocities f (z) for ||z|| < 1? 
10.7 
Let (X, Y) be a coordinate system (Definition 3.1.42) in Z with the coordi-
nate projections P : Z —» X and Q : Z ^ y . If F : Z x J ~>Zisa 
smooth flow, 

390 
STOKES' THEOREM 
then show that there is an open interval 1,0 G I C J, such that 
G(x, i) = PF(x, t) and H(y, t) = QF(y, t) 
define smooth flows G : X x I —> X and H :Y x I —>Y. (Note that here we take 
Z = X © Y rather than Z = X x Y, so that both x and y are in Z.) 
10.8 
Let F : Z x J -> Z be a smooth flow with the initial velocity field f : Z -> Z. 
Then show that limt-,o(lA)(^r(z; ¿) ~ z) = f(z) uniformly in z 6 Z. (//mf. Use 
Problem 10.9 below.) 
10.9 
Let W be a normed space. Let T : Z x J ^ W be a function such that 
H = (dT/dt) 
: Z x J —> W exists and is continuous. Then show that given a 
compact set K C Z and an e > 0, there is a 5 > 0 such that 
¡|(r(z,¿)-r(z, o))-í#(w, o)||w<e|í| 
whenever |i| + ||z — w|| < ¿ and z, w £ K. In particular, show that 
limt^o(l/i) (r(z, t) - r(z, 0)) = H(z, 0) 
uniformly in z on any compact set K C Z. 
10.10 
Let F : Z x J —> Zbca smooth flow with the initial velocity field f. Then 
show that lim i^ 0(l/i)(D zF(z, t) - Iz) = f'(z) uniformly in z e Z. 
10.11 
Let F : Z x J — > Z b e a smooth flow with the initial velocity field f. Then 
show that lim t^0(l/i)(det DzF{z, 
t) - 1) = Trf'(z) uniformly in z e Z. 
10.12 
Let ipt ■ Z —> Z, t £ J, be a family of continuous functions. 
If 
limt_>o Vti2) = <¿>(z) uniformly in z 6 Z, then show that 
lim / </jt(z)dz— / 
ip(z)dz 
t-*°JE 
JE 
for any Jordan set E. 
10.3 FLUX AND CHANGE OF VOLUME IN A FLOW 
Definition 10.3.1 Volume of a set in a flow. Let F : (Z x J) -+ Z be a flow. For 
£ C Z and t e J, let £7* = { F(z, t) | z e £ } = F(E, t) c Z be the image of F 
under the diffeomorphism F( ■, i) : Z —» Z. We can think of £"' as the locations at t 
of all particles that started out in E at time 0. It is a snapshot of a moving ensemble. 

FLUX AND CHANGE OF VOLUME IN A FLOW 
391 
Theorem 8.4.14 shows that if E is a Jordan set, then El is also a Jordan set. In general, 
the volume of El will be different at different times. Our second route to Stokes' 
theorem begins with a result about the rate of change in this volume. Actually, we 
shall only need this rate of change at the time t = 0, and only for the special type of 
flow constructed in Theorem 10.2.5. (The result is true for any smooth flow, but the 
proof of this fact is left as an exercise.) 
Theorem 10.3.2 Initial rate of change in the volume. Let f : Z —» Z be a C1 
vector field of compact support and suppose that F(z, t) = z + if (z). Let E C Z 
be a Jordan set and El = F(E, t). Then 
limvm-v(E)^ 
r .vf(g)dg> 
(io33) 
i-»0 
t 
JE 
which gives the initial rate of change in the volume of E. 
Proof. Theorem 10.2.5 shows that there is an open interval J containing O s R such 
that the mapping F( ■, t) : Z —► Z is a diffeomorphism for each fixed t € J. If 
t € J, then by the change of volumes theorem 8.4.14, 
v{El) 
= 
[ detDzF(z, 
t)dz 
(10.34) 
JE 
= 
i det(I + tf'(z))dz. 
(10.35) 
JE 
Hence 
v{Et)-v{E) 
= 
[ (det(I + tt'(z))-l)dz. 
(10.36) 
JE 
Now we know that (det(/ + ff'(z)) — 1) may be written as a polynomial J2k ^k{^)tk 
in t. The coefficients Ak(z) are polynomials in the partial derivatives of the compo-
nents of f. Therefore they are all continuous functions on Z. Also, as observed in 
10.1.2, 
AÁz) 
= lim ^ 
+ ^ ( ' ) ) - l 
= d i v f ( z ). 
(10.37) 
Then the proof of the theorem follows easily. 
G 
Remarks 10.3.3 Divergence as the density of expansion. On the interpretation we 
have been developing, the divergence of a vector field gives the density of the initial 
rate of expansion for a flow with the initial velocity field f : Z ~•> Z. The basis for 

392 
STOKES' THEOREM 
this interpretation is Theorem 10.3.2. In fact, by this theorem, 
l i m ^ - ^ 
= 
/divf(z)dz ; and 
t-o 
t 
JE 
to*l(^-A 
= 
- 7 ^ / d i v f ( z ) d z 
t-o t V v{E) 
) 
v(E) JE 
v ; 
Flux of a Vector Field 
Definition 10.3.4 Flux out of a Jordan set. Let f : Z 
compact support. Let E be a Jordan set in Z. Then 
flux(f, E) = [ divf(z)dz 
JE 
is called the flux off out ofE. 
Flux is defined as a function of f and E, but in light of Theorem 10.3.2, we can also 
characterize it in terms of the initial rate of change in the volume of E for the flow 
associated with f. 
Theorem 10.3.5 Another expression for the flux. Let f : Z —+ Z be a C1 vector 
field with a compact support. Let F : Z x J —> Z be any smooth flow with the initial 
velocity field f. Then 
fhrx(f, £) = ^mt^0{\lt)(v{Et)-v{E)) 
(10.41) 
= \imt^0{l/t){v{Et\E)-v{E\Et)) 
(10.42) 
where E is a Jordan set and El — F(E, t), t G J. 
Proof. The first equality (10.41) follows from Theorem 10.3.2. Also, 
E = (EnEt)\J(E\Et) and 
E* = 
{EtnE)U(Et\E), 
where both unions are the unions of disjoint sets. Then (10.42) follows 
additivity of volume. □ 
Flux and Boundaries 
As our next step towards Stokes' theorem, we prove that the flux of f out of E, 
although defined as an integral over the whole set E, depends only on the values of f 
(10.38) 
(10.39) 
Z be a C1 vector field of 
(10.40) 

FLUX AND CHANGE OF VOLUME IN A FLOW 
393 
on the boundary of E. This follows from Lemma 10.3.7 below and from the linearity 
of the flux. We also show that flux(f, E) depends only on the part of E that is inside 
the support of f. This is formulated in Lemma 10.3.10. 
In what follows, f : Z —> Z is a C1 vector field with a compact support and 
F : Z x J ->■ Z is the flow defined as F(z, t) = z + if (z). 
Lemma 10.3.6 7/f(z) ^ 0, then also f(F(z, t)) ^ 0 for all t £ J. 
Proof. Suppose that f (w) = 0 for w = F(z, i) = z + if (z), where i £ J and 
i ^ 0. Then we also have w = w + if (w) = F(w, t). So F(w, i) = F(z, i) with 
w T¿ z. This is a contradiction, since F( •, i) : Z —> Z is one-to-one. 
ü 
Lemma 10.3.7 // f (z) = 0 /or a// z £ <9F, i/zen flux(f, E) = 0. (// f/ie initial 
velocity everywhere on the boundary of E is 0, then there is no flux out of E.) 
Proof. We will show that F* = F(F, i) = F for all t £ J. Then Theorem 10.3.5 
implies that flux(f, F) = 0. 
Assume that t ^ 0 and that there is a z £ F* \ F. Then there is a w £ F such 
that z — F(w, i) = w + tí(w) £■ F. Since z ^ w, we must have f (w) ^ 0, and 
therefore (by the assumption of the theorem) w 0 dE. It also implies that there is a 
nonzero T £ J such that F(w, r) = v = w + rf (w) £ 9F. (To see this, let r be 
the supremum of all t such that w + if (w) £ F.) Then f (v) = 0 by the hypothesis. 
This contradicts Lemma 10.3.6 which implies that f (v) ^ 0. Hence there cannot be 
any points in F* \ F. Similarly one shows that E\El 
= 0. Therefore F* = F for 
all te J. 
D 
Lemma 10.3.8 Let K be the support of a flow F on Z. Let D and E be two sets in 
Z such that DDK 
= ED K. Then F* \ F = D* \ D and E\Et 
= 
D\D\ 
Proof. Let F 0 = F n K and Ex = E \ F 0 = E n Kc. Then E\ = F x for all i £ J, 
since F(z, i) = z for all z £ Kc and i £ J. Hence F = F 0 U Fi and F* = F¿ UFi. 
This implies El\E 
= F¿ \ F 0. Since D0 = F n F = F n F = E0, we see that 
Dt\D 
= Et\E. 
The arguments for the second claim are the same. O 
Corollary 10.3.9 Let E and D be as in Lemma 10.3.8 above. If E is a Jordan set, 
then D* \D and D\ D1 are also Jordan sets. In particular, if K n D is a Jordan 
set, then Dl \D and D\Dl 
are also Jordan sets. 
Proof. This follows directly from Lemma 10.3.8 and the fact that F* is a Jordan set. 
D 

394 
STOKES' THEOREM 
Lemma 10.3.10 Let f : Z —> Z be a C1 vector field with a compact support K. 
Then flux(f, E) = flux(f, E n K) for any Jordan set E C Z. 
Proof. Let F : Z x J —> Z be a smooth flow with the initial velocity field f and 
with the same support K. Let E0 = E D K and E\ = E\E0 
= E n Kc,the part 
of £ outside K. Then £f = E\ for all i e J, since F(z, i) = z for all z e Kc and 
í € J. Hence E = E0 Li Ei and £7* = E^L) E\. Then the conclusion follows since 
both unions are unions of disjoint sets. 
□ 
Basic Stokes' theorem: flow interpretation 
Let us recall the notation employed in Definition 10.1.7: n e Z is a unit vector, [7 
is the plane z • n = 0, and A is the lower side of U, as defined by the condition 
z ■ n < 0. We are ready for our second proof of the basic Stokes' theorem, stated 
exactly as in 10.1.8. 
Theorem 10.3.11 Basic Stokes' theorem. Let f : Z —»• Z be a C1 vector field of 
compact support. Then 
[ div f (z) dz = in- 
f (u) du. 
(10.45) 
JA 
JU 
Proof. The proof utilizes three main ideas. First, interpret f (z) as the initial velocity 
of a flow F(z, t) = z + tt(z). We can imagine U as a plastic sheet that is perfectly flat 
at time t — 0, but whose parts are about to bulge up or down or stay put, depending 
upon the direction in which f (z) points. Second, interpret the integral on the left side 
of (10.45) as the initial rate of change in the volume being added to or taken away 
from A as its boundary (which is U) begins to shift. Third, interpret the right side 
of (10.45) as a simple computation of this newly added (or subtracted) volume using 
the standard formula for the volume under a surface. 
First, we define the flow. Let K be the support of f. Let G be an open Jordan set such 
that K cG, and let E = G n A. We see that KnE 
= KnA. 
Let F : Z x J ^ Z 
be the flow F(z, t) = z + if (z) and let At and El the images of A and E under this 
flow. The existence of some J = (—r, r) for which F is a smooth flow is guaranteed 
by Theorem 10.2.5, and all we need is some interval J containing 0. 
Next, we interpret the divergence integral on the left side of (10.45) as a rate of 
change in volume. Lemma 10.3.8 shows that E* \ E = A1 \ A and E \ Et ■= A \ A1. 

FLUX AND CHANGE OF VOLUME IN A FLOW 
395 
Hence 
/ divf(z)dz 
= 
/ divf(z)dz 
(10.46) 
JA 
JE 
= 
\imt^0(l/t)(v(Et\E)-v(E\Et)) 
(10.47) 
= 
]imt^0(l/t)(v(At\A)-v(A\At)) 
(10.48) 
by Theorem 10.3.5 and Definition 10.3.4. 
Finally, we interpret the integral on the right side of (10.45) as computing the volume 
under the shifted surface [/*. Suppose first that f i n everywhere. In this case, 
Ju n • f (u) du = 0. But then we also have no shift in the surface. So A* = A for all 
t e J, and the above formula for the divergence integral yields JA div f = 0. Hence 
in this first special case, the conclusion (10.45) follows. 
Next suppose that f _L U everywhere. In this case, f (z) = /(z) n for some scalar 
function / : Z —> K. Decompose Z as UxY~UxR 
where Y is the one-
dimensional space spanned by n. Express the the points z = u + yn as (u, y). If 
u £ U, then F(u, t) = u+£/(u)n. Hence Ul is the graph of the function y = tf(u) 
in U x E. Also (assuming i/(u) > 0) 
At = {(u,y)\ueU,y< 
i/(u) } . 
(10.49) 
This can be seen by noticing that dAl = Ul. Therefore 
v(A' \A)- 
v(A \A*)= 
[ £/(u) du, 
(10.50) 
Ju 
the volume between the graph of tf and the [/-plane. (The same formula works if 
tf < 0.) Again, the conclusion (10.45) follows. 
In general, f = fi + f2 with f 1 _L n and f2 _L U. The conclusion then follows by the 
linearity of both sides in (10.45). 
□ 
Problems 
10.13 
The proof of Theorem 10.3.11 above uses the facts that 
\imt^0(v{Et\E)-v{E\Et)) 
depends only on the initial velocity field and that this dependence is linear. Prove 
lim (viE* \E)- 
v{E \ E1)) = f n • f (u) du 
(10.51) 
t_*° 
Ju 
directly, without using this information. Here F(z, t) = z + if (z). 
10.14 
Prove (10.51) for a general smooth flow. 

3 9 6 
STOKES' THEOREM 
10.4 
EXTERIOR DERIVATIVES 
To obtain the general version of Stokes' theorem, the simple geometry exploited in 
the basic version must be transformed to more general settings by diffeomorphisms. 
The relation between the transformations of f : Z —> Z and (div f) : Z —> K will be 
important. As it turns out, the shift can be handled most effectively by representing 
vector fields in terms of tensor fields, as discussed in chapter 9. 
Remarks 10.4.1 Restriction to C2 diffeomorphisms. There is one drawback to this 
approach: it requires the assumption of C2 diffeomorphisms. Our general version 
of Stokes' theorem will thus be applicable only where the relevant diffeomorphisms 
are C2, rather than just C1. This entails no great loss of generality by comparison to 
most treatments of the subject, which customarily work with C°° diffeomorphisms. 
In particular, the "Classical Stokes' Theorem", stated as Theorem 10.6.10 below, is 
true only for C2 surfaces. 
Nevertheless, most of the results obtained here are in fact true for C1 diffeomorphisms. 
Stokes' theorem without the restriction to G2 diffeomorphisms is proved in other 
texts (notably in W. Fleming, Calculus of Several Variables (Springer-Verlag UTM, 
1977)). Problem 10.20 indicates a way of proving this more general version of 
Stokes' theorem. 
Remarks 10.4.2 Review of tensor representations. Let(Z, $)beann-dimensional 
oriented Euclidean space. Hence Z is a Euclidean space together with a chosen 
Euclidean determinant i9. Recall that Ak{Z) is the linear space of all fc-tensors 
(real-valued alternating multilinear functions) on Z. 
Recall that we can represent a vector field f : Z —► Z by the tensor field LO : Z —> 
A„-i(Z) defined by 
w(z)(Z) = tf(f(z), Z) 6 R, 
(10.52) 
where ze Z,Z = (z2, ..., zn) e Zn~l and tf(f(z), Z) = tf(f(z), z2, ..., z„) e 
R. 
We can represent a scalar function g : Z —> R by the tensor field r : Z —> An(Z) 
defined by 
r(z)(Z) = fl(z)i?(Z) e R, zeZ, 
Z = (Zl, ..., zn) e Z " . 
(10.53) 
Remarks 10.4.3 Exterior derivatives: a special case. Letf : Z —> Z be a C1 vector 
field represented by w : Z —+ An_i (Z). If the divergence function (div f) : Z —> R 
is represented by r : Z —> An(Z) as in the preceding remarks, then one calls r the 
exterior derivative of u>, written as r = du>. 

EXTERIOR DERIVATIVES 
397 
We shall show that there is a more general way to define the exterior derivatives of 
tensor fields. If to : Z —* Ak(Z) is a C1 tensor field, then its exterior derivative will 
be a continuous tensor field duo : Z —> Ak+i(Z). The special case just mentioned is 
thus the case fe = n — 1. 
Lemma 10.4.4 Alternating part of a multilinear function. Let kbea nonnegative 
integer and let a : Zk —> R be a multilinear function. Then 
(Alta)(zi, ..., zfe) = — V" 
(signcr)a(zCT(1), ..., zCT(fc)) 
(10.54) 
defines a k-tensor (Alt a) G Afc(Z). /? ¿Ä caZ/ed íZíe alternating part of a. 
Proof. Recall that S^ is the set of all permutations of { 1, ..., k} (see Appendix C). 
If Z = (zi, ..., Zfc) G Zfc and a G Sfc, then write 
crZ = 
(zCT(1)j ..., Z < T ( f c ))eZ f e 
(10.55) 
for the re-arrangement of the vectors in Z according to the permutation a. Now let 
p G Sfc and write pZ = (vi, ..., v¿) = V. Since it is clear that Alt a is multilinear, 
we are done once we show that 
Alta(pZ) = (signp) (Alta)(Z). 
If crV = (wi, ..., wfc) = W, then w¿ = VCT(Í) = zp(tT(i)). Hence er(pZ) = (per)Z. 
Therefore, recalling that sign (pa) = (signp) (signer), 
fc!(Alta)(pZ) 
= 
V 
(signer) e*(er(pZ)) 
(10.56) 
= 
5 ] 
(signer) a((per)Z) 
(10.57) 
= 
(signp)V 
(sign (pa)) a((pa)Z) 
(10.58) 
= 
fc!(signp)(Alta)(Z). 
□ 
(10.59) 
Remarks 10.4.5 Derivative of a tensor field. Let u : Z —» Ajt(Z) be a C1 tensor 
field. Then w( ■)("£) : Z —> R is a C1 function for each fixed ordered fc-tuple 
Z = (zi, ..., zk)mZk. 
Let 
OJ'(-)(Z):Z^L(Z, 
R) 
(10.60) 
be the derivative of this function. We will denote the application of this derivative to 
a vector z0 G Z as 
c/(-)(zo;Z) :Z_>R. 
(10.61) 

3 9 8 
STOKES' THEOREM 
But now if we fix z 6 Z and allow zo andZtovary, a/(z) is a multilinear function that 
takes (z0,z1; ..., Zfc) € Zfc+1 too/(z)(zo,zi, ..., z¿) G R. So (10.61) defines a 
function UJ' : Z —> MLk+i(Z). 
We have 
u/(z)(z0; Z) = limf^0(l/i)(w(z + iz0)(Z) - w(z)(Z)) 
(10.62) 
for all z G Z and for all (z0; Z) e Zk+1 with Z = (zi, ..., zk) G Zk. 
Definition 10.4.6 Exterior derivatives. Let w : Z —> Ak(Z) be a C1 tensor field. 
Let u/ : Z —> AiLfc+i(Z) be its derivative as defined in Remarks 10.4.5. Then the 
tensor field 
div = (k+1) Alt to': Z ^ Ak+1{Z) 
(10.63) 
of order (k + 1) is called the exterior derivative of to. 
We can now show that this definition agrees with our earlier definition of the exterior 
derivative for the special case in Remarks 10.4.3. 
Theorem 10.4.7 Exterior derivatives and divergence. Let(Z, $) be ann-dimensional 
oriented Euclidean space. If 
u> : Z-A„_i(Z) 
represents aC 1 vector field f : Z —> Z, then 
duj : Z -► An{Z) 
represents the function (div f) : Z —> M. 
Proof. Let z G Z, zi 6 Z, and Z = (z2, ..., z„)G Z"" 1. We see that 
LO'(Z)(Z1; Z) = tf(f'(z) zi, z2) ..., zn). 
(10.64) 
Here w'(z) G MLn(Z) 
is already alternating in its last k = (n — 1) variables, since 
i? G An(Z) is a determinant. Then an easy computation shows that 
dw(z)(zi, z2, ..., zn) 
= 
n(Alta/)(z)(zi, ..., zn) 
= 
i?(f'(z)zi, z2, ..., z„) + 
i?(zi, f'(z)z2, ..., zn) + 
... +i?(zi, z2, ..., f'(z)zn) 
= 
Trf'(z)ö(zi, . . . , z n ) 
= 
divf(z)i?(zi, ..., z„). 
The second last step follows from Definition C.7.3, the definition of the trace of 
f'(z) G L(Z, Z). The last step follows from the fact that Trf = divf, by the 
definition of the divergence. 
□ 

EXTERIOR DERIVATIVES 
399 
Commutativity of Pullbacks and Exterior Derivatives 
We will show that exterior differentiation commutes with taking pullbacks under C2 
diffeomorphisms. 
Remarks 10.4.8 Pullbacks with C2 diffeomorphisms are C1. Let us recall the 
definition of pullbacks, from Definition 9.5.11. Let W be another Euclidean space, 
H c W be an open set, and $ : H —> Z a C2 diffeomorphism with G = $(//") 
(see Figure 9.1 or Figure 9.3). Let LO : G —» Afc(Z) be a tensor field with a compact 
support K c G. Then the pullback of w is defined as 
$*(w)(w)(W) =u;($(w))($'(w)W), w e i i a n d W G VFfc. 
(10.65) 
We see that if w : G -> Afc(Z) is a C1 tensor field, then $*(u;) : if -> Afc(W) is 
also a C1 tensor field. This follows from the assumption that <!>://—> G is a C2 
diffeomorphism, and that assumption is essential. If necessary, we can extend the 
definitions of w and $* (LO) to Z and to W by defining them as zero outside G and H, 
respectively. An easy argument shows that these extensions are also C1 functions. 
Theorem 10.4.9 Exterior derivatives of pullbacks. Let $ : H —> G be a C2 
diffeomorphism. Let u> : G —> Afc(Z) ¿>e a C1 tensor field with a compact support 
K cG. Then d($*w) = $*(dcj). 
Proof. Let w € ÍÍ, w 0 £ f , and W G Wfc. Also let z = $(w). Compute 
($*w)'(w)(wo; W) from (10.65) by the chain rule and by the rules of differentiation 
for multilinear functions. Then, with the notations of Remarks 10.4.5, 
($*w)'(w)(w0; W) 
= 
a/(z)($'(w)w 0;$'(w)W) 
(10.66) 
+yk 
w(z)(Zi) 
(10.67) 
where Z¿ = (z¿1, ■ ■ •, z¿fc) € Zfe are defined as 
z¿j = $'(w)wj if z ^ j and zu = <E>"(w)(w0, w¡). 
(10.68) 
(The differentiation here is undeniably messy. Problem 10.15 asks you to work out 
this derivative for the special cases fc = 1 and k = 2.) 
We see that each u;(z)(Z¿) is a multilinear function of (k + 1) vectors w0 and 
wi, ..., Wfc. Also w(z)(Z¿) remains invariant if wo and w¿ are switched. In fact, 
since <3> is a C2 diffeomorphism, we have 
$"(w)(w0, w¿) = $"(w)(w¿, wo). 
(10.69) 
Then, an easy verification shows that Alt w(z)(Z¿) = 0. Hence 
Alt ($*w)'(w)(w0; W) = Alt w/(z)($'(w)w0; $'(w)W), 
(10.70) 
which is the conclusion of the theorem. 
D 

4 0 0 
STOKES' THEOREM 
Basic Stokes' theorem in Tensor Form 
We shall need the basic Stokes' theorem, Theorem 10.3.11, in tensor form. This 
involves the integrals of the tensor fields ui and du) over two oriented flat manifolds, 
that is, manifolds with constant tangent spaces. In what follows, W is a Euclidean 
space. It is oriented by a Euclidean determinant g. Also, e e W is a unit vector, A 
is the half space e ■ w < 0, and U is the plane e • w = 0. 
Notations 10.4.10 Two manifolds in the basic Stokes' theorem. One manifold that 
appears in the theorem is the n-dimensional manifold A. Its tangent spaces are always 
W and they are all oriented by g. The other manifold is the (n — l)-dimensional 
manifold U. Its tangent spaces are always U and they are all oriented according to 
the following convention: a basis B for U is positive if (e, B) is a positive basis for 
W, that is, if Q (e, B) > 0. In the arguments that follow, let E = (e2, ■ • •, e„) be a 
fixed positive orthonormal basis for U. Hence (e, E) = (e, e2, ..., en) is a positive 
orthonormal basis for W and g (e, E) = 1. 
Theorem 10.4.11 Basic Stokes' theorem in tensor form. Let 
Lü-.W ^ Kn-i{W) 
(10.71) 
be a C1 tensor field of compact support. Then 
í dw= Í 
J A 
Ju 
with the manifolds and orientations as defined in Notations 10.4.10. 
to, 
(10.72) 
A 
JU 
Proof. Clearly, we want to derive this result from the vector version of the basic 
Stokes' theorem. So our first task is to recall how we associate a vector field with the 
tensor w. Lemma 9.5.10 shows that there is a vector field f : W —> W such that 
w(w)(W) = g (f(w), W), 
(10.73) 
where w e W a n d W ^ (w2, ..., w„) e W n _ 1. We see that f : W -> W is a C1 
vector field of compact support. 
Now we have a string of equalities. 
fu 
= 
[ u;(u)(E)du= / £>(f(u), E) du 
(10.74) 
Ju 
Ju 
Ju 
= 
i (e-f(u))du= / divf(w)dw 
(10.75) 
Ju 
JA 
= 
/ divf(w)ß(e, E)dw= 
/ dw(w)(e, E)dw 
(10.76) 
JA 
JA 
= 
[ du. 
(10.77) 
JA 

REGULAR AND ALMOST REGULAR SETS 
401 
The first step in (10.75) is derived by writing f(u) = (e • f(u))e + e2, where 
e2 is a linear combination of vectors in E; then use the properties of the Euclidean 
determinant g. The second step in (10.75) is the basic Stokes' theorem in vector form. 
The second step in (10.76) follows from Theorem 10.4.7 that relates divergences and 
exterior derivatives. All other steps follow directly from the definitions. 
D 
Problems 
10.15 
Prove Theorem 10.4.9 for the special cases of k = 1 and k = 2 by writing 
out the terms in Equations (10.66)-( 10.67) explicitly. 
10.16 
Let (ei, ..., e„) be a basis for Z. Show that a tensor A 6 A2(Z) of order 
two is uniquely determined by its values A(e¿, ej) on the pairs of basis vectors. 
Also show that A(e¿, e¿) = 0 and A(e¿, e.,-) = —A(ej, e¿). Hence, conclude that 
dim A2(Z) is n(n- 
l)/2. 
10.17 
A tensor field £ : G —> Ai (Z) of is represented by a vector field f : G —> Z 
as in Definition 9.5.6. Hence £(a)(z) = f (a) • z for all a 6 G and z 6 Z. What is 
the application of d£(a) to a pair of vectors (zi, z2) G Z21 Let (ei, ..., en) be a 
basis for Z. Let f = Yli Piei- Find d£(a)(e¿, e^) in terms of F¿s. 
10.18 
Show that dim Ak(Z) = ( " ) with n = dim Z. 
10.19 
Let B = { (x, y, z) \ x = y = 0 } be the 2-axis and G = K3 \ B. Define 
f : G -> M3 by f(x, y, z) = (x2z, y2z, x2 + y2). 
Define Ü : G -> G by 
Sl(x, y, z) = (x, y,z- 
(x2 + y2)1/2 
+ (x2 + y2)). Let ^ : G -* A^M3) and 
r¡ : G -> A2(R3) be defined by 
£(a)(z) = f(a) • z and r?(a)(u, v) = f(a) • u x v. 
Compute d£„ di], dü* (^), dii* (77), Q* (d£), and fi* (d£) explicitly and verify Theorem 
10.4.9 for these cases. 
10.5 REGULAR AND ALMOST REGULAR SETS 
We will prove Stokes' theorem for regular sets and for almost regular sets. Intuitively, 
regular sets are Jordan sets with smooth boundaries. A simple example of a regular 
set is a Euclidean ball. Almost regular sets have smooth boundaries except for a 
subset of the boundary that has negligible surface area. A simple example of an 
almost regular set is a box spanned by the vectors of a basis. 

402 
STOKES' THEOREM 
The main step in generalizing Stokes' theorem is to transfer the basic Stokes' theorem 
to regular neighborhoods of a set. Recall that the basic Stokes' theorem deals with 
a half-space A and its boundary-plane dA = U. Regular neighborhoods of a set 
E are those open sets G in which E and dE behave like A and dA, up to a C2 
diffeomorphism. A Jordan set E is called a regular set if its closure E can be covered 
by the regular neighborhoods of E. 
Regular Neighborhoods of a Set 
Notations 10.5.1 The oriented spaces U, W and Z. Let n > 2. Let Z and W be 
two n-dimensional Euclidean spaces, oriented by the Euclidean determinants i9 and 
Q respectively. Let e = ei E W be a unit vector. Let U be the plane e ■ w = 0 and 
let A be the half-space e • w < 0. Orient U by the right-hand rule, Definition 9.3.16, 
with the unit vector e = ei. Let E = (e2, ■ • ■, e„) be a positive orthonormal basis 
for U. Hence, (e, E) = (ei, e2, ■ • •, en) is a positive orthonormal basis for W. 
Definition 10.5.2 Regular neighborhoods of a set. Let E be a set in Z. An open 
set G C Z is called a regular neighborhood for E if there is a C2 diffeomorphism 
# : G -► H, H = *(G) C W, such that 
#(G n E°) = H n ^ and *(G n ö£) = H n £/. 
(10.78) 
Note that if an open set G does not intersect 9.E, then G is a regular neighborhood 
for E. If G c E°, for example, then an isomorphism T : Z —> W takes G to an 
open subset of A. The general case is similar. Hence, Definition 10.5.2 involves only 
the position of G n E with respect to G n dE. 
Remarks 10.5.3 Boundaries in regular neighborhoods. Recall that surfaces in 
Z are (n — 1)-dimensional manifolds. If G is a regular neighborhood for E, then 
G n dE is a surface. This follows directly from the second condition in (10.78). In 
fact, this condition means that 9 : G —> H is a chart for G n dE. Hence, if G n dE is 
not a surface, then G cannot be a regular neighborhood for E. If G n dE is a surface, 
however, then G still may not be a regular neighborhood for E. First, there is an 
additional smoothness condition on the chart ^ that it must be a C2 diffeomorphism. 
Second, the first condition in (10.78) requires that E should be only "on one side of the 
boundary". Let, for example, E = { (x, y) \ x2 + y2 < 2, x1 + y2 ^ 1 } C R2. 
Then dE consists of two circles x2 + y2 = 1 and x2 + y2 = 2. Any open set that 
intersects the smaller circle is not a regular neighborhood for E. 

REGULAR AND ALMOST REGULAR SETS 
4 0 3 
Boundaries of Regular Sets 
Definition 10.5.4 Regular sets. A set in Z is called a regular set if its closure is 
covered by its regular neighborhoods. 
Note that a Jordan set is regular if and only if the union of its regular neighborhoods 
is the whole space. This follows from the remarks in Definition 10.5.2. 
To simplify the statements in the proofs of the next few results, we will define 
acceptable diffeomorphisms for a set. These are the diffeomorphisms that have the 
properties specified in Definition 10.5.2 of regular neighborhoods. 
Definition 10.5.5 Acceptable diffeomorphisms. Let £ b e a set in Z. Let G be 
an open set in Z. Let * : G —> W be a diffeomorphism and let 9(G) = H. 
Then Í» : G —> H is called an acceptable diffeomorphism (for E) if * is a C2 
diffeomorphism, and if 
*(G n E°) = H n A and *(G l~l dE) = H n U. 
(10.79) 
Lemma 10.5.6 Let G and Go be open sets in Z and let GQ c G. If a diffeomorphism 
9 : G —> H is acceptable for E, then its restriction to Go is also acceptable for E. 
Proof. This is left as an exercise. 
D 
Lemma 10.5.7 Let (e, E) be the basis ofW defined in Notations 10.5.1. Define 
an isomorphism R : W —> W as i?e¿ = e¿ if 1 < i < n and Ren = — en. If 
a diffeomorphism 9 : G —> H is acceptable, then Rfy : G —> RH is also an 
acceptable diffeomorphism. 
Proof. This is left as an exercise. G 
Theorem 10.5.8 If E is a regular set, then its closure can be covered by the domains 
of acceptable and orientation-preserving diffeomorphisms. 
Proof. Orientation-preserving diffeomorphisms were defined in Definition 9.3.4. 
This proof is identical with the proof of Theorem 9.3.8. Arguments of that proof show 
that one can pass from diffeomorphisms to orientation-preserving diffeomorphisms 
by two types of modifications: restricting the domain of a diffeomorphism, and 
reversing the orientation of a diffeomorphism. Lemmas 10.5.6 and 10.5.7 show that 
acceptable diffeomorphisms remain acceptable after these modifications. 
□ 
We will show that the boundary of a regular set is an orientable surface. 

4 0 4 
STOKES' THEOREM 
Lemma 10.5.9 Let ^ : G —> H be an acceptable diffeomorphism for E. Let 
F(z) = e • *(z), z eG. If a e G n dE, then there is a S > 0 such that 
a + iVF(a) E £ 
I/ -6 <t<Q 
and 
a + iVF(a) £ F 
i/ 0 < í < <5. 
Proof. Let v = VF(a). Theorem 9.3.20 shows that v ^ 0. Let 
f(t) = e • *(a + iv) = F(a + iv). 
Then we see that /(0) = 0 and /'(0) = VF(a) ■ v = ||VF(a)||2. Therefore there 
is a 6 > 0 such that 
/(f) 
= 
F(a + iv) < 0 if -5 < t < 0 and 
f{t) 
= 
F(a + í v ) > 0 
i f 0 < í < 5 . 
This means that* (a + iv) e Aif-S 
< t < 0 and*(a + ív) ^^UÍ/ifO < í < 6. 
Hence, the conclusion follows, 
ü 
Lemma 10.5.10 Let \I>¿ : G¿ —> ií¿ fee íwo acceptable diffeomorphisms for E. Let 
Fi(z) = e • *¿(z), z G G¿. //a 6 d n G2 n OF, ffte« VFi(a) • VF2(a) > 0. 
Proof. Observations in Remarks 10.5.3 show that S = G\ D G2 fl 9F is a surface. 
Let v¿ = VFj(a). Theorem 9.3.20 shows that both v¡ are nonzero and normal to the 
tangent plane Ta of S. Hence there is a nonzero a £ l such that v2 = avi. Lemma 
10.5.9 shows that a cannot be negative. Then the proof follows. 
□ 
Lemma 10.5.11 Two acceptable and orientation-preserving diffeomorphisms \l/¿ : 
Gi —> Hi for E induce the same orientation on S — G\ fl G2 fl dE. 
Proof. Orientations induced by diffeomorphisms were defined in Definition 9.3.9. 
Let * : G —> H be an orientation-preserving diffeomorphism. Let F = e • *. By 
Theorem 9.3.20, the orientation of the tangent space induced by * is related to the 
unit normal vector n = VF/|| VF|| by the right-hand rule. Hence, if ^ define the 
same unit normal vector, then they induce the same orientation on the manifold, 
ü 
Theorem 10.5.12 The boundary of a regular set is an orientable surface. 
Proof. Let E be a regular set. Definition 10.5.4 of regular sets and Theorem 10.5.8 
show that S = dE has an atlas of orientation-preserving acceptable diffeomorphisms. 
Lemma 10.5.11 shows that this is an atlas of compatible charts, in the (obvious) sense 
of Definition 9.3.12. Then, by Definition 9.3.13, S is an orientable surface. 
□ 

REGULAR AND ALMOST REGULAR SETS 
405 
Stokes' theorem for Regular Sets 
Remarks 10.5.13 Summary of notation and assumptions about orientation. The 
spaces Z and W and the plane U (a subspace of W) are as specified in Notations 
10.5.1. They are all oriented spaces. In particular, U is oriented by a unit normal 
vector e according to the right-hand rule. The half-space A is the set o f w G W such 
that e • w < 0. Hence U = dA. Let E be a set in Z. Let G be a regular neighborhood 
for E and let \I> : G —> H be an orientation-preserving acceptable diffeomorphism 
(Definition 10.5.5). In Theorem 10.5.14 below, the orientations on E and on dE are 
the orientations induced by *S> from the orientations of A and dA. 
Theorem 10.5.14 Stokes' theorem for regular neighborhoods Let G be a regular 
neighborhood for E. Let f : Z —-> Z be a C1 vector field and let UJ : Z —> An_i (Z) 
be a C1 tensor field, both with compact supports contained in G. Then 
/ divf = / 
f and / duj= / 
w, 
(10.80) 
JE 
JOE 
JE 
JOE 
with the definitions given in Remarks 10.5.13. 
Proof. With the notations of Remarks 10.5.13, let $ : H —> G be the reverse 
diffeomorphism of * : G —> H. Let £ = $*(w) and 77 = $*(dw) be the pullbacks 
of u! and dw by $, as defined in Definition 9.5.11. Theorem 10.4.9 shows that 
77 = d£. The tensor form of the basic Stokes' theorem, Theorem 10.4.11, shows 
that JA d£ = f9A £. Assembling all of these facts gives us the following string of 
equalities: 
i <P*(dtü) = [ v= [ <%= [ £ = / * » . 
JA 
JA 
JA 
JBA 
JOA 
Since E°nG 
= $(A n H) and dEnG 
= ${dA n ii), we can apply the change of 
variables theorem for tensor fields, Theorem 9.5.12, to the left and right sides of the 
above equation: 
f(duj)= 
[ <f>*(duj)= ( 
$*(w)= / 
w. 
JE 
JA 
JÖA 
JdE 
This proves the tensor portion of (10.80). To obtain the vector part, represent the 
vector field f by the tensor field UJ, as explained in Remarks 10.4.2, and apply the 
result on tensor fields. Theorem 10.4.7 shows that dui represents divf and this 
completes the proof. 
D 
The extension to regular sets is an easy consequence of Theorem 10.5.14 and partitions 
of unity. 

4 0 6 
STOKES' THEOREM 
Theorem 10.5.15 Stokes' theorem for regular sets Let E be a regular set. Let 
f : Z —> Z be a C1 vector field and letu)\Z^> 
An_ i (Z) be a C1 tensor field, both 
with compact supports. Then 
/ d i v f = f 
f and j du = í 
u>. 
(10.81) 
JE 
JOE 
JE 
JOE 
The orientation on E is the orientation of the underlying space Z. The orientation 
on dE is induced by the orientation of Z and by the (geometrical) outer normal of 
dE, according to the right-hand rule. 
Proof. The supports of f and ui are compact. Hence these supports can be covered 
by finitely many regular neighborhoods. In all of these neighborhoods there are 
orientation-preserving acceptable diffeomorphisms. Then the proof is completed by 
an application of Theorem 10.5.14 and the partitions of unity theorem, Theorem 
9.2.11. The details are the same as in Definition 9.2.12 and Lemma 9.2.13. 
□ 
Remarks 10.5.16 Role of the orientations. Where does orientation come into these 
arguments? We need the orientation of E only when we integrate the tensor field dw. 
To integrate the vector field f on dE, it suffices to know the outer unit normal vectors. 
The orientations of the tangent spaces play no role in this integration. Hence, the 
effort spent on orientations is for the tensor version only. But the result is worth the 
effort. The tensor formulation allows a lean proof of Stokes' theorem. As already 
mentioned, however, this proof requires C2 diffeomorphisms. 
Stokes' theorem for almost regular sets 
We show that Stokes' theorem can be extended to cubes and to other sets, most of 
whose boundary points belong to a regular neighborhood. 
Definition 10.5.17 Outer boundary-surfaces. Let E be a bounded set in Z. The 
outer boundary-surface of E is the set of all points on the boundary of E that 
are contained in regular neighborhoods (Definition 10.5.2) for E. We see that the 
boundary-surface is indeed a surface. The boundary-surface of E will be denoted by 
S, or by Sß. 
Lemma 10.5.18 Let K be a compact subset of§E and let B = (dE) \ K. Then E 
has a finite open covering {Go} U {G¿} such that GQ satisfies Go n dE C B and all 
other GiS are regular neighborhoods for E. 
Proof. Each point in 8 ^ is contained in a regular neighborhood. Hence the compact 
set K in Sg can be covered by finitely many regular neighborhoods. Let H be the 

REGULAR AND ALMOST REGULAR SETS 
407 
union of these neighborhoods, and let Bo = (dE) \ H. Then Bo is a compact set 
disjoint from K. Therefore there is an open set Go that contains BQ and is disjoint 
from K. Then H U Go is an open set containing dE. Hence E \ (H U Go) is in 
the interior of E. Any open set that does not intersect dE is a regular neighborhood 
for E. Therefore the compact set E \ (H U G0) can be covered by finitely many 
regular neighborhoods. These finitely many open sets are a covering of E. They are 
all regular neighborhoods for E, except Go- 
□ 
Definition 10.5.19 Upper surface-area. Let B be a bounded set in a Euclidean 
space Z. The enlargement of B by r > 0 was defined in Definition 9.6.10. It is the 
set Br of all points that are within distance r of a point in B. We know that Br is a 
Jordan set for all r > 0 (Problem 8.52). The upper limit 
a(B) 
= 
lim sup — v{Br) 
(10.82) 
= 
lim 
sup 
—v(Br) 
<?-*o+o<r<<? 2r 
. 
„ 
v ., 
(10-83) 
^0 + 0<r<q 
will be called the upper surface-area of B. Here v(Br) is the volume of Br in Z. 
Theorem 9.6.12 shows that if B is a Jordan set on a surface in Z, then cr(B) is the 
surface-area of B. (Jordan subsets of a manifold are defined in Definition 9.2.7). 
Lemma 10.5.20 Let f : Z —> Z be a C1 vector field of compact support K. Let 
M = supz ||f (z) ||. Let E be a Jordan set in Z. Let B = K n dE. Then 
L 
divf 
B 
|flux(f, E)\ <Ma(B). 
(10.84) 
Proof. Apply Theorem 10.2.5 to find an r > 0 such that F(z, t) — z + if (z) is a flow 
ZxJ^ 
Z, where J = (-r, r). LetE* = F(£', í),í G J, be the images of £ under 
this flow. We claim that (EAE*) C Bw M, where (EAE1) 
= 
(Et\E)ö(E\Et). 
Here B\t\ M is m e enlargement (Definition 9.6.10) of B by |i|M. We assume that 
0 < t; arguments for negative t are similar. 
If z e (E1 \ E), then there is an a e E such that z = a + if (a) ^ £\ Therefore, for 
some T e [0, i], b = a + T-f(a) e <9¿?. Then b e ß; otherwise, F(b, s) = b for 
all se J. Then ||z - b|| = (f - r)||f(a)|| < tM. Therefore, ze 
BtM. 
Let z £ (E\ El). Since F(-, t) is a diffeomorphism of Z onto Z, there is an a G Z 
such that z = a + if (a) G S. Since z ^ £*, we see that & $ E. Then, as before, 
b = a + TÍ (a) G -B for some T G [0, t]. Therefore z G B^M. 
Now use Theorem 10.3.5. 
□ 

4 0 8 
STOKES' THEOREM 
Definition 10.5.21 Almost regular sets. A Jordan set E in a Euclidean space is 
called an almost regular set if for each e > 0, there is a compact set K c §E such 
that o(dE \K) 
<e. Here a is as in (10.82). 
Lemma 10.5.22 Let f : Z —> Z ¿e a C1 vector field. Let E be an almost regular set. 
Let §E be the outer boundary surface of E (Definition 10.5.17). Then given e > 0, 
there is a compact set KQ C §E such that 
f- 
/ divf <e, 
(10.85) 
x 
JE 
whenever K is a compact set and KQ C K C §E-
Proof. Let Af — supz ||f(z)||. Givene > 0, use Definition 10.5.21 to find a compact 
set K0 C SE such that cr(B) < e/M, where B = (dE) \ K0. Let AT be any compact 
set such that KQ c K c §E- Use Lemma 10.5.18 to find a finite open covering 
{Go, G{\ as specified in that lemma. Use the partitions of unity theorem, Theorem 
9.2.11, to find finitely many C1 functions A, : Z —> [0, 1 ], such that ^ ¿ A¿ = 1 on 
an open set containing E, and such that each A¿ has a compact support contained in 
G{. Let f¿ = A¿f¿. (Note in particular that fo has support contained in Go.) Then 
f = J2i £ o n a n °Pen s e t containing E. The operation of taking the divergence of a 
vector field is a linear operation. This was observed in Remarks 10.1.2. Hence 
¿dM-E.Xavi-E./dwf, 
Theorem 10.5.14 shows that JE divf¿ = fKnG. f¿ = JK f¿ for i ^ 0 since each G¿ 
is a regular neighborhood of E. Hence 
/ div f 
= 
/ div f0 + Y^ ■ / 
d i v {i 
= /divf0 + V . [ 
d 
JE 
% JKnGí 
= 
/ d i v f o + / f . 
JE 
JK 
Lemma 10.5.20 shows that | fE div f0] < Ma{B) < e. Then the conclusion follows. 
D 
Remarks 10.5.23 Integration on boundary-surfaces. Let / : M —> K be a 
function defined on a manifold M. Definition 9.2.12 of JM f is only for those 
functions that have a compact support contained in M. Hence, according to this 
definition, we can consider Jg f only if f has a compact support contained in §E-

REGULAR AND ALMOST REGULAR SETS 
409 
But if E is an almost regular set and if f is a C1 vector field, then, on the basis of 
Lemma 10.5.22, it is natural to define J$ f as fE divf. In fact, it is customary to 
express this integral as JQE f. Definition 10.5.24 below formalizes this notation. 
Definition 10.5.24 Let E be an almost regular set. Let / : dE —> R be a bounded 
function. Then the number a = JdE /, if it exists, is defined as follows: for each 
e > 0, there exists a compact set KQ C §E such that 
f-a 
K 
< e 
whenever K is a compact set and K0 c K C §E-
Theorem 10.5.25 Stokes' theorem for almost regular sets. Let E be an almost 
regular set (Definition 10.5.21). Let f : Z —> Z be a C1 vector field. Then 
[ divf = f 
f. 
JE 
JOE 
IE 
JdE 
The last integral is defined in Definition 10.5.24. 
Proof. This follows directly from Lemma 10.5.22 and Definition 10.5.24. 
D 
Problems 
10.20 
Call an open set GaC 1 regular neighborhood of a set E if the conditions in 
Definition 10.5.2 for the regular neighborhoods are satisfied, except that the diffeo-
morphisms involved do not have to be C2 diffeomorphisms. Let G be a C1 regular 
neighborhood of E. Let f : Z —> Z be a C1 vector field with compact support in G. 
Show that 
\imUv(Et\E)-v(E\Et))= 
f 
f. 
' ^ 0 t 
JdE 
Hereü"* = F(E, í)andF(z, t) = z+£f(z), as before. The boundary dE is oriented 
by its outer normal. 
10.21 
Let F : Z x J —>Zbea general smooth flow with an initial velocity field 
f with a compact support contained in an open set G. Assume that G is a C1 regular 
neighborhood of a set E and repeat Problem 10.20 for this case. 
10.22 
Compute Js f where S is the unit sphere x2 + y2 + z2 = 1 in K3 oriented 
by the outer normals and f (x, y, z) = (x2 + ze~v , y2 + ze~x , z + ex +v ). 

410 
STOKES' THEOREM 
10.23 
Compute fs f where f is as in Problem 10.22 and S is the upper half (z > 0) 
of the unit sphere in R3 oriented by the outer normals. 
10.24 
Compute Js f where f is as in Problem 10.22 and S is the part of the unit 
sphere in R3 corresponding to z > 1/2, oriented by the outer normals. 
10.25 
Let a > 0. Let E be the region in R3 specified by x2 + y2 < z < a. Show 
that E is an almost regular set. Compute JdE f, where f is as in Problem 10.22 and 
the integral on dE is as defined in Definition 10.5.24. 
10.26 
Let E be an almost regular set in Z. Let F and G be two real-valued C2 
functions on Z. Show that 
/ 
(FVG) = / ((VF) • (VG) + 
FV2G). 
JdE 
JE 
10.27 
Let E be an almost regular set in Z. Let F and G be two real-valued C2 
functions on Z. Show that 
/ 
{FVG-GVF) 
= / ( F V 2 G - G V 2 F ) . 
JdE 
JE 
10.28 
Let F(x, y, z) = (x2 + y2 + z2)~1/2. 
Compute Js VF where S is the unit 
sphere x2 + y2 + z2 = 1 in E 3 oriented by the outer normals. 
10.29 
Let E be an almost regular set in R3 containing the origin in its interior. 
Compute JdE VF where F is as in Problem 10.28 and the integral on dE is as 
defined in Definition 10.5.24. 
10.30 
Repeat Problem 10.29 under the assumption that 0 ^ E. 
10.31 
Let Z be an n-dimensional Euclidean space. LetF(z) — |jz||2~ra. Compute 
Js VF where S is the unit sphere ||z|| = 1 in Z oriented by the outer normals. 
10.32 
Let Z and F be as in Problem 10.31. Compute JdE VF where E is an 
almost regular set in Z containing the origin in its interior. 
10.33 
Repeat Problem 10.32 under the assumption that 0 ^ E. 
10.34 
Let c¿ € R be finitely many numbers and let a¿ be finitely many points in 
an n-dimensional Euclidean space Z. Let F(z) = J2i ci\\z ~ a¿l|2 "• Let E be an 
almost regular set in Z such that a¿ ^ dE for all i. Compute JdE VF. 

REGULAR AND ALMOST REGULAR SETS 
411 
10.35 
Let F{x, y) = log(x2 + y2). Let C be the unit circle x2 + y2 = 1. Note 
that C is both a surface and a line in R2. (See also Problem 9.38.) Compute the 
surface integral Jc VF where C is oriented by the outer normals. 
10.36 
Let E be an almost regular set in M2 containing the origin in its interior. Let 
F be as in Problem 10.35. Compute faE VF. 
10.37 
Repeat Problem 10.36 under the assumption that 0 ^ E. 
10.38 
Letr = (x2 + y2)1^2. 
Letf(x, y) = (—x, y)/r. Let E be an almost regular 
set in R2. Consider the boundary-surface §E as a curve C, oriented by the outer 
normals according to the right-hand rule. Assume that E contains the origin in its 
interior. Compute the line integral Jc f. 
10.39 
Repeat Problem 10.36 under the assumption that 0 ^ E. 
10.40 
Let Ci e R be finitely many numbers and let a¿ e R2 be finitely many 
points. Let F(z) = J2i c¿ log ||z — a¿||. Let E be an almost regular set in R2 such 
that a¿ 0 dE for all i. Compute JQE VF. 
10.41 
Let F and E be as in Problem 10.40. Consider the boundary-surface §E as 
a curve C, oriented by the outer normals according to the right-hand rule. Consider 
M2 as the xy-p\am in the xy^-space. Let k = (0, 0, 1) be the usual unit vector of 
the z-axis. Let f = k x VF. Compute Jc f. 
10.42 
Let i, j , k be the usual unit vectors in the xyz-space. Let E be an almost 
regular set in the xy-plane. Given a vector field f (a;, y) = P(x, y)i + Q(x, y)j 
in this plane, apply Stokes' theorem, Theorem 10.5.25, to f x k to obtain Green's 
theorem: 
Note that the surface integral of h x k becomes the line integral of h. 
10.43 
Verify Green's theorem for jc(x2 
— xyz)dx + (y3 — 2xy)dy, where C is 
the square with vertices (0, 0), (2, 0), (2, 2), (0, 2). 
10.44 
Evaluate the line integral Jc(y2 s'm(xy2)dx + Ixy s'm(xy2)dy) where C is 
the unit circle. 
10.45 
Choose h(x, y) = xj to obtain the area of E as a line integral on the 
boundary of E. Use this result to find the area of the region bounded by the curve 
x = a cos3 t,y = a sin31, 0 < t < 2n. 

412 
STOKES' THEOREM 
10.6 STOKES' THEOREM ON MANIFOLDS 
Let M be an ¿-dimensional C2 manifold in a Euclidean space Z. Let 
* :G-^H 
= ^{G) C W 
be a e2 chart for M so that * ( G n M ) = H DU, where [/ is an ¿-dimensional 
subspace of W. Let $ : H —> G be the reverse chart with the associated parametric 
representation ip : fí n Í7 —> G H M. Let A be an open set in W such that A n Í7 is 
a regular neighborhood for a set f? in Í7. Then P = M Ci $(A) = <p(A D [/) will be 
called a regular neighborhood of Q = <&(£) = </?(■£) m ^ - Note that ß n i i s a 
regular set in U. 
Let k = (¿ — 1) and let u> : G —► Afe(Z) be a C1 tensor field of compact support 
contained in <&{A). The pullbacks of u> and dw under $ are the tensor fields 
£ = **(o>) : if -► Afc(iy) and 77 = 3>*(dw) : F -+ A£(W). 
Theorem 10.4.9 shows that r/ = d£. The restriction of £(w) : Wk ^ R to Uk 
defines an element in Ak(U). 
Hence these restrictions define a new tensor field 
£0 : H n U —> Afe([/). Similarly one obtains rj0 : H nU ^ A¿(U). By an easy 
verification we see that d£o = 770. 
An application of Stokes' theorem for regular neighborhoods, Theorem 10.5.14, 
shows that fB d£ = JdB £. We will assume that $ is an orientation-preserving chart. 
Arguments given for the proofs of Theorems 10.5.8 and 9.3.8 show that this is not a 
loss of generality. Then the change of variables theorem for tensor fields, Theorem 
9.5.12, shows that ¡Edu> = fdEw- 
This is Stokes' theorem on manifolds for a 
regular neighborhood of a set in a manifold. From this we obtain Stokes' theorem 
on manifolds, as we obtained Theorem 10.5.15 from Theorem 10.5.14. Extensions 
to almost regular sets are also obtained as before. 
Stokes' theorem on manifolds is related to flows and vector fields on manifolds. Let 
M be a manifold in Z. Then f : M —> Z is called a vector field on M if f (m) £ T m 
at each m £ M. Here Tm is the tangent space of M at m £ M. If M is an 
¿-dimensional oriented manifold, then each Tm has a positive Euclidean determinant 
f?m €E Ag(Tm). Then we see that a tensor field w of order k = [Í — 1) is related to a 
vector field on an oriented ¿-dimensional manifold as 
w(m)(T m)=tf m(f(m), T m ) £ M , m € M, Tm £ Tm
k. 
(10.86) 
Let f : M —■> Z be a vector field on M with a compact support K. If there is a chart 
>3> : G —> H for M such that K C G, then f is the initial velocity field of a flow on 
M. In fact, if f is related to u; as in (10.86), then <E>*(c<;) is related to a vector field in 
the Euclidean space V. This induces a flow in V with compact support ^f(K) C if. 
Then $ : if —> G maps this flow to a flow on M. 

STOKES' THEOREM ON MANIFOLDS 
4 1 3 
As an illustration of these relationships, we will consider Stokes' theorem on two-
dimensional manifolds in three-dimensional spaces. Indeed, the original version of 
Stokes' theorem applied only to this case. 
Classical Stokes' theorem in Tensor Form 
Remarks 10.6.1 Orientations in classical Stokes' Theorem. Once again, we start 
with the "basic case" for the classical version of Stokes' theorem. The space W is a 
two-dimensional plane in a three-dimensional space Y. A line U in W divides this 
plane into a lower half A and an upper half. The boundary of A in the W-plane is 
the line U. The orientation of U is as before: if e — e\ is the outer unit normal 
of U in W, then a unit vector e2 defines the positive orientation on U just in case 
(ei, e2) is a positive orthonormal basis for W. The orientation of the W-plane itself 
is specified by its outer unit normal vector n in the y-space. Hence (n, ei, e2) is a 
positive orthonormal basis for Y. 
In more picturesque terms, the relation between these three directions is sometimes 
described as follows. If we are standing on the W-plane with our heads pointing 
in the direction n of the outer normal of W, and if we want to walk in the positive 
direction along the line U, then the lower half A of W must stay on our left-hand 
side. 
Theorem 10.6.2 Basic classical Stokes' theorem in tensor form. With the nota-
tions and definitions in Remarks 10.6.1, if£ : Y —> A\(Y) is a C1 tensor field of 
compact support, then 
f <%= f € = / * £ • 
(
10-
8?) 
JA 
JdA 
Ju 
Proof. The integrals in (10.87) are performed on oriented manifolds, as described in 
Remarks 10.6.1. To prove this result, restrict £ : Y —» Ai(Y) to W. We obtain a 
tensor field £|w : W —* Ai(W) on the JV-plane. Then an application of the basic 
Stokes' theorem, Theorem 10.4.11, gives (10.87). 
D 
Remarks 10.6.3 In Theorem 10.6.2, the boundary dA is the boundary of A in the 
W-plane, as described in Remarks 10.6.1, rather than its boundary in Y (which would 
be the entire set A). Analogously, in Theorem 10.6.4 below, the boundary dE will 
be the boundary of E in the surface S. The definition of this boundary is left as an 
exercise. 
Theorem 10.6.4 Local classical Stokes' theorem in tensor form. Let E be a set 
in an oriented C2 surface S in a three-dimensional Euclidean space Z. Let G be an 

4 1 4 
STOKES'THEOREM 
open set in Z. Let 
be an orientation-preserving C2 diffeomorphism such that 
$ ( G n 5 ) 
= 
HnW, 
(10.88) 
* ( G n £ ° ) 
= 
HnA,and 
(10.89) 
$(Gflö£) 
= 
HDdA 
= HnU. 
(10.90) 
Let u) : G —> Ai (Z) be a C1 tensor field of compact support contained in G. Then 
i du>= ¡ 
u. 
(10.91) 
J E 
JdE 
Proof. Use the reverse diffeomorphism $ : H —> G to pull back u> as £ = $*(w). 
Then apply Theorem 10.6.2. The details are left as an exercise. 
D 
Theorem 10.6.5 Classical Stokes' theorem in tensor form. Let E and S be as in 
Theorem 10.6.4. Assume that E is compact and is covered by open sets G as specified 
in that theorem. Let UJ : Z —+ Ai(Z) be a C1 tensor field. Then 
¡ duj= I 
UJ. 
(10.92) 
JE 
JdE 
Proof. This is left as an exercise. G 
Classical Stokes' Theorem in Vector Form 
The inner product of a, b £ R3 is denoted as the dot product a • b. The standard 
orientation ofR3 is determined by the Euclidean determinant #(a, b, c) = a ( b x c ) , 
as the usual mixed product. Let UJ : G —> Ai(R3) be a tensor field of order one, 
defined on an open set G. It is associated with a vector field f : G —> K3 as 
u>(r)(a) = f (r) • a, where r G G, a e M3. 
Definition 10.6.6 Curl of a vector field. Let w(r)(a) = f (r) a be a C1 tensor field 
UJ : G —> Ai (R3) of order one. Then its exterior derivative du> : G —> A2(R3) is a 
tensor field of order two. Therefore it is represented by a vector field g : G —> R3 as 
dw(r)(a, b) = g • a x b, r G G, (a, b) G (R3)2. 
Then g = curl f : G -> R3 is called the curl of f : G ->• R3. Hence 
duj{r)(a, b) = curlf(r)-ax b, r G R3, (a, b) G (R3)2. 
(10.93) 

STOKES' THEOREM ON MANIFOLDS 
4 1 5 
Remarks 10.6.7 Vectorial expression of curl. Let f : M3 —> R3 be a C1 vector 
field and w(r)(b) = b • f (r). Then, by the definitions in 10.4.5 and 10.4.6, 
w'(r)(a, b) 
= 
b f ' ( r ) a . Hence 
dw(r)(a, b) 
= 
b - f ' ( r ) a - a - f ' ( r ) b 
= 
curlf(r) • a x b. 
(10.94) 
(10.95) 
(10.96) 
Remarks 10.6.8 Coordinate expressions for curl. Let R3 be the standard xyz-
space with the standard orthonormal basis (i, j , k). The coordinates of curl f can be 
obtained by evaluating (10.95) at the pairs (j, k), (k, i), and (i, j), as in the proof of 
Lemma 9.5.10. If f (r) = L(r)i + M(r)j + iV(r)k then 
curlf = (7Vy - Ma)i + {Lz - Nx)j + (Mx - Ly)k, 
(10.97) 
where the subscripts denote partial derivatives. One can remember this expression 
more easily through the notation 
curl f : 
i 
d 
dx 
L 
j 
d 
dy 
M 
k 
a 
dz 
N 
V x f. 
(10.98) 
Remarks 10.6.9 Integrals of tensor and vector fields. The classical Stokes' the-
orem is obtained from Theorem 10.6.5 by replacing the integrals of tensor fields by 
the corresponding integrals of vector fields. These two types of integrals are related 
as described in Theorem 9.5.7. The tensor field ui is associated with the vector field 
f. The integral of to on the curve dE becomes the line integral fdE f. The integral 
of dio on the surface E becomes the surface integral j E curl f. The orientations of E 
and dE are as described in the proof of Theorem 10.6.2. 
Theorem 10.6.10 Classical Stokes' theorem. Let E and S be as in Theorem 10.6.4. 
Assume that E is compact and is covered by open sets G as specified in that theorem. 
Let f : Z —> Z be a C1 vector field. Then 
curl f 
f. 
(10.99) 
dB 
Proof. This follows from the classical Stokes' theorem in tensor form, Theorem 
10.6.5, by replacing the integrals of tensor fields by the corresponding integrals of 
vector fields. The correspondence between these integrals is as discussed in Remarks 
10.6.9 above. 
D 

4 1 6 
STOKES'THEOREM 
Problems 
10.46 
Show that V - ( f x g ) = g V x f - f - V x g . 
10.47 
Show that V • V x f = 0. 
10.48 
Show that V x V F = 0. 
10.49 
Verify the classical Stokes' theorem, Theorem 10.6.10, for the following 
cases of E and f, by computing the integrals JdB f and fE curl f separately. 
1. S is given as z — (x2 + y2)1^2 
and 1 < z < 4 and f(x, y, z) = (z2, x2, y2). 
2. S is given as z = x2 + y2 and z < 4 and t(x, y, z) = (z, xz, x). 
3. S is given as x + y + z = 1, 0 < x, 0 < y, 0 < z, and f (x, y, z) = (y, z, x). 
10.50 
Show that V • ((VF) x (VG)) = 0. 

PART IV 
APPENDICES 

This page intentionally left blank

APPENDIX A 
CONSTRUCTION OF THE REAL 
NUMBERS 
Introduction. Let Q be the set of rational numbers. The most important properties 
of Q are the properties of the arithmetic operations and the properties of the order 
relation on Q. These properties are summarized by two sets of rules called the field 
axioms and the order axioms. The principal deficiency of Q is that it is not complete. 
The set of rational numbers x such that x2 < 2 is bounded above, yet there is no 
rational least upper bound. We would like to extend Q to R, a set of numbers that 
not only satisfies the field axioms and the order axioms but also has the completeness 
property. This section reviews a standard way to construct the real numbers that 
meets this objective. 
Outline of the construction of R. Prior to attempting any construction such as the 
present one, we freely make use of the real numbers without proving their existence. 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
419 

420 
CONSTRUCTION OF THE REAL NUMBERS 
We know many of the properties that they must have. In particular, we know that any 
real number is the limit of a sequence of rational numbers. That fact suggests that we 
try to define real numbers as the limits of convergent sequences of rational numbers. 
But Definition 2.3.2 of convergent sequences depends explicitly upon the limit point. 
We have defined convergence to L, not convergence per se. To circumvent this 
difficulty, we base our construction on Cauchy sequences of rational numbers rather 
than convergent sequences. Cauchy sequences are defined purely in terms of rational 
numbers, and they coincide with the class of sequences that we shall ultimately regard 
as convergent to a limit in R. 
Let 6 be the class of all Cauchy sequences of rational numbers. C cannot be identified 
with the set of real numbers because many different Cauchy sequences may converge 
to the same real number. Hence we introduce an equivalence relation on 6, making 
two Cauchy sequences equivalent if the difference sequence converges to zero. Note 
that convergence to zero can be defined in terms of rational numbers only. Hence 
real numbers will be defined as equivalence classes of Cauchy sequences of rational 
numbers. We will see that the usual arithmetic operations and the order relation 
can be defined easily for these equivalence classes, and that the resulting system is 
complete. 
In what follows, we only assume basic knowledge of the rational numbers. We restate 
some of the earlier definitions using only rational numbers, making sure that all of 
our arguments are formulated in terms of rational numbers only. 
A.1 FIELD AND ORDER AXIOMS IN 
Definition A.1.1 Field axioms on Q. There are two binary operations Q x Q - t Q 
called addition and multiplication. Addition applied to (a, b) G Q x Q gives (a + b). 
Multiplication applied to (a, b) G Q x Q gives (a ■ b). These operations have the 
following properties. 
Commutativity 
a + b = b + a and a ■ b — b ■ a for all a, b G Q. 
Associativity 
(a + b) + c = a + (b + c) and (a ■ b) ■ c = a ■ {b ■ c) for all a, b, c £ Q. 
Distributivity 
a ■ (b + c) = (a ■ b) + (a ■ c) for all a, b, c G Q. 
Existence of the neutral elements There are two elements in Q, 0 and 1, such that 
0 ^ 1 and such that 
0 + a = a and 1 • a = a for all a G Q . 

EQUIVALENCE CLASSES OF CAUCHY SEQUENCES IN Q 
421 
Existence of inverses for addition (subtraction) 
For each a s Q , there is a (—a) G Q such that a + (—a) = 0. 
Existence of the inverse for multiplication (division) 
If a G Q and if a ^ 0, then there is a (1/a) e Q such that a ■ (1/a) = 1. 
Definition A.1.2 Order axioms on Q. There is a set PQ C Q, called the positive 
(rational) numbers, with the following two properties. 
1. For each p G Q, exactly one of the following three cases is true: p = 0, 
p G PQ, or - p G PQ. 
2. If p, q g PQ, then p + <j G PQ and pq G PQ. 
Remarks A.1.3 Our objective. We would like to construct a set R with the following 
properties. 
1. There are two binary operations I x R ^ R o n R , addition and multiplication, 
that satisfy the field axioms on R. 
2. There is a set PR C R, the set of positive (real) numbers, that satisfies the order 
axioms on K. 
3. There is a one-to-one mapping <p : Q —> R such that ip(p + q) = <¿>(p) + ip{q) 
and ^(pq) = ip(p)<fi(q) for all p, <? G Q and such that </>(PQ) C PR. Here 
<p(p) + v(?) and iP(p)lf(<l) a r e stated in terms of the addition and multiplication 
operations on R. 
4. Every nonempty subset of R with an upper bound has a least upper bound. 
Hence, R satisfies the completeness axiom, Axiom 2.2.3. 
A.2 
EQUIVALENCE CLASSES OF CAUCHY SEQUENCES IN Q 
Definition A.2.1 Cauchy sequences of rational numbers. A sequence x : N —» Q 
is called a Cauchy sequence of rational numbers if for each rational number a > 0, 
there is an -/V G N such that \xm — xn\ < a for all m, n> N. Let 6 be the set of all 
Cauchy sequences of rational numbers. 

4 2 2 
CONSTRUCTION OF THE REAL NUMBERS 
Definition A.2.2 Zero sequences of rational numbers. A sequence x : N —> Q is 
called a zero sequence of rational numbers if for each rational number a > 0, there 
is an N e N such that \xn \ < a for all n > N. Let Z be the set of all zero sequences 
of rational numbers. 
Lemma A.2.3 Every zero sequence of rational numbers is also a Cauchy sequenceof 
rational numbers. Hence Z C C. 
Proof. This is left as an exercise. 
D 
Definition A.2.4 Constant sequences of rational numbers. Each rational number 
q G Q defines a constant sequence q : N —> Q: we set qn = q for all n G N. Let 
Q be the set of all constant sequences of rational numbers. It is clear that q G 6 for 
each q G Q. Hence Q C C. Note that q G Z if and only if q = 0. 
Definition A.2.5 Sum and product of two sequences. If x : N —> Qandy : N —» Q 
are two sequences, then their sum is defined as the sequence (x + y)n — xn + yn 
and their product as the sequence {xy)n = 
xnyn. 
Lemma A.2.6 Let x : N —» Q and y : N —> Q be two sequences. 
(1) Ifx, y G C, then (x + y ) e ß and (xy) G C. 
(2j /fx, y G Z, f/i^n (x + y) G 2. COTÍ/ (xy) 
G Z. 
f JJ IfxeZ 
and y G C, iÄen (xy) G Z. 
Proof. This is left as an exercise. 
D 
Definition A.2.7 Addition and multiplication on C. Addition on 6 is the operation 
C x e —> e that takes the pair (x, y) G C x C to (x + y) G C. Similarly, multiplication 
on C is the operation C x C —> C that takes the pair (x, y) G 6 x Q to (xy) G 6. 
Note that if p, g e Q, then (p + q) = p + q and (pq) — pq, with the notations of 
Definition A.2.4. 
Definition A.2.8 An equivalence relation on C. Define a relation on 6 x C as 
follows. If x, y G C, then x is related to y if and only if (x — y) G Z. It is easy to 
check that this is an equivalence relation on C in the sense of Definition 1.1.9. We 
write x ~ y for (x - y) G Z. Note that if p, q G Q, then p ~ q if and only if p = q, 
with the notations of Definition A.2.4. 

EQUIVALENCE CLASSES OF CAUCHY SEQUENCES IN Q 
4 2 3 
Definition A.2.9 Real numbers. Equivalence classes were defined in Definition 
1.1.11. The equivalence class represented by x e 6 is the set 
EX = {x' e e i x' ~ x} c e. 
Any two equivalence classes are either identical or disjoint. In fact, if x ~ y, then 
Ex — Ey, and if x T¿ y, then Ex f) Ey = 0. The union of all equivalence classes 
is C. Each equivalence class is also called a real number. The equivalence class 
Ex is the real number represented by the sequence x. The same real number can be 
represented by any sequence in Ex, i.e., by any sequence x' ~ x. Let K denote the 
collection of all equivalence classes. This collection is called the set of real numbers. 
Definition A.2.10 Operations on real numbers. We will define addition and mul-
tiplication operations R x R —> R. Let Ex, Ey G R. Hence Ex and Ey are two 
real numbers and also two equivalence classes represented by the sequences x and y. 
Their sum Ex + Ey and their product Ex Ey are defined as 
Ex + Ey = Ex-i-y and ExEy = Exy. 
This definition has to be justified. If Ex = Ex< and Ey = Ey>, must we have 
Ex+y = Exi+y> andExy = Exiy>l If not, then the definitions above are meaningless. 
The required justification is provided by Lemma A.2.11 below. 
These definitions are reasonable. They depend on the fact that the limit of the sum of 
two sequences is the sum of their limits. The same is also true for products. Theorem 
A.2.15 below shows that the field axioms are satisfied on R with these operations. 
Lemma A.2.11 Letx, x', y, y' G 6. Ifx ~ x' and y ~ y', then (x + y) ~ (x' + y') 
and (xy) ~ (x'y1). 
Proof. Let x' = x + p and y' = y + q with p, q £ Z. Then 
[x1 + y1) - (x + y) = p + q e Z 
by the second part of Lemma A.2.6. Hence (x + y) ~ (x' + y'). Also, 
(x'y') - (xy) = (py) + (xq) + (pq) G Z, 
by the second and third parts of Lemma A.2.6. Hence (xy) ~ (x'y'). 
□ 
Definition A.2.12 Positive real numbers. Call a sequence x : N —» Q an eventually 
positive sequence if there is an n G N such that xn > 0 for all n > N. Call a real 
number Ex € M. a positive real number if each y G Ex is an eventually positive 

4 2 4 
CONSTRUCTION OF THE REAL NUMBERS 
sequence. Let P R C R be the set of all positive real numbers. These definitions 
are reasonable as well. In fact, the limit of a sequence is positive if and only if all 
the sequences converging to the same limit are eventually positive. Theorem A.2.16 
below shows that the order axioms are satisfied on R with this definition of positivity. 
Lemma A.2.13 Let i £ E , Assume that for each rational number a > 0 and for 
each W e N , there is ann > N such that \xn\ < a. Then x £ Z. 
Proof. Given a rational a > 0, find a n I V e N such that \xn — xm\ 
< a/2 for all 
m, n > N. This can be done since x G C. By assumption, we can find an m G N 
such that m > N and such that \xm\ < a/2. Let n> N. Then 
\xn\ = \xm + (xn ~ xm)\ 
< \xm\ + \xn - xm\ < {a/2) + (a/2) = a. 
Hence x e Z. 
□ 
Lemma A.2.14 Let x G C and x $ Z. Then there is an a > 0, a € Q, such that the 
following are true. 
(1) There is Í D I M É N such that \xm\ > a for all m > M. 
(2) There is an N € N such that either xn > afar all n > N or —xn > afar all 
n> 
N. 
(3) Let y ~ x. Then there is a K G N such that Xk and y^ are both nonzero and 
have the same sign for all k > K. 
Proof. Since x 0 Z, the hypothesis of Lemma A.2.13 cannot be true. Hence there 
is a rational number a > 0 and an M e N such that \xm\ > a for all m > M. This 
proves the first part. 
Now find m N > M, N e N, such that \xm - xn\ < a for all m, n > N. This 
can be done since x is a Cauchy sequence. Let m, n > N. 
If xm and xn have 
opposite signs, then \xn — xm\ > 2a, since \xm\ > a and \xn\ > a. This violates the 
condition that \xm — xn\ < a for all m, n > N. Hence either xn> 
a for all n > N 
or xn < —a for all n > N. This proves the second part. 
Let y ~ x. Then (x - y) G Z. Find a K > N, K G N, such that \yk - xk\ < a/2 
for all k > K. Then —a/2 < yk — xk < a/2 shows that 
Xk - a/2 <yk<xk+ 
a/2 
for all k > K. Hence we see that if a < xk, then a/2 < yk, and if xk < —a, then 
yk < —a/2 for all k > K. This proves the last part. 
□ 

EQUIVALENCE CLASSES OF CAUCHY SEQUENCES IN Q 
425 
Theorem A.2.15 Let K be the collection of all equivalence classes in C. Let the 
addition and multiplication operations R x R —> R on R be defined by 
Ex + Ey = Ex+y 
and ExEy = Exy, 
as in Definition A.2.10. Then these operations satisfy the field axioms, Axiom A. 1.1, 
onR. 
Proof. Let 0 and I be the constant sequences consisting of all Os and all Is. Note 
that i?g = Z. We see that EQ is the neutral element for addition in R and E\ is the 
neutral element for multiplication in R. 
First, we verify the division axiom. Let Ex ^ EQ. We will show that there is a y € C 
such that ExEy — E\. Now i € 6 and x £ Z. Then the first part of Lemma A.2.14 
shows that there is a rational number a > 0 and an N £ N such that \xn\ > a for 
all n > N. Define a sequence y : N —>Qbyyn 
= O i f n < A r and yn = \/xn if 
n> N.We claim that y G C and ExEy =E1.lfm,n>N, 
then 
\yn-ym\ 
= \0-/xn)~ (\/xm)\ = \xn~xm\/\xnxm\ 
< 
(l/a2)\xn 
-xm\. 
Given a rational number b > 0, find an M > N, M G N, such that \xn — xm\ < a2b 
for all m, n> M. Then we see that \yn — ym\ <b for all m, n > M. Hence y G 6. 
We see that xnyn 
= 1 for all n > N. Therefore (xnyn — 1) = 0 for all n > N. 
Then we see that (xy — 1) ~ 0 or that xy ~ 1. Hence E'xE'y = £?i. 
The verification of the other axioms is quite routine. We verify only distributivity 
as an example. Addition and multiplication in C have the distributivity property. In 
fact, if a, b, c G C, then 
(a(b + c))n 
= 
an(b + c)n = anbn + anbn 
= 
{ab)n + (ac)n = ((ab) + (ac))n. 
Now let Ea, Eb, Ec G K with a, b, c G C. Then 
Ea(Eb + Ec) 
= 
EaEb+c = Ea(b+C) 
= 
E(ab) + (ac) = -Eob + Eac 
— 
EaEb + 
EaEb. 
Therefore the distributivity axiom is satisfied. 
□ 
Theorem A.2.16 Let R be the set of real numbers with the arithmetic operations as 
defined in Definition A.2.10. Let PR be the set of all positive real numbers as defined 
in Definition A.2.12. Then the order axioms, in Definition A. 1.2, are satisfied on R. 

4 2 6 
CONSTRUCTION OF THE REAL NUMBERS 
Proof. First, note that EQ = —EQ is not positive. For example, the sequence 
xn = ( —l)n(l/n) is a zero sequence which is not eventually positive. Now let 
Ex T¿ EQ. We will show that either Ex or —Ex is positive. 
We have x g Z. Hence the second part of Lemma A.2.14 shows that either x or 
—x is eventually positive. Assume that x is eventually positive. Then the last part 
of Lemma A.2.14 shows that if y ~ x, that is, if y G Ex, then y is also eventually 
positive. Hence Ex is a positive real number. Similarly, if —x is eventually positive 
and if y ~ x, then — y is also eventually positive. In this case —Ex = E^x is a 
positive real number. This proves the first part of the order axioms. 
For the second part, assume that Ex and Ey are both positive. Then, by the second 
part of Lemma A.2.14, there are a, b > 0, a, b G Q, and anJVeN such that xn> a 
and yn > b for all n > N. Then xn+yn>a 
+ b>0 
and xnyn 
> ab > 0 for 
all n > N. Hence Ex + Ey and ExEy are both nonzero, and they are both positive 
numbers. 
□ 
Definition A.2.17 Canonical embedding of Q into R. Let <p : Q -> R be the 
function defined as <p(p) = Ep. Here p G C is the constant sequence with all terms 
equal to p G Q. We will call ip : Q —> R the canonical embedding ofQ into R. 
Theorem A.2.18 77¡e canonical imbedding of Q mío M ¿s a one-to-one function 
ip : Q —> R. // presewes addition and multiplication in the sense that 
<f{p + q) = <p(p) + <fi{q) and ip{pq) = ip(p)tp(q) for all p, q G Q. 
A/so, <¿> : Q —> R preserves the order in the sense that (p(Pq) C PR. 
Proof. If .Ep = Eq, then p ~ q and (p — 5) is a zero sequence. But a constant 
sequence is a zero sequence only if the constant term is zero. This is obvious, as 
already mentioned in Definition A.2.4. Hence Ep = Eq in R only if p = q in Q. 
This shows that ¡p : Q —> R is one-to-one. For the second part, 
V>(P + 9) = £(P+g) = Ep + Eq = (p(p) + (/3(g). 
Here the second equality follows from the definition of addition in R. The proof of 
vipq) 
= f(p)(P((l) is similar. For the last part, note that if p is a positive rational 
number, then p is not a zero sequence and eventually (in fact, always) is positive. 
Hence ip(p) = Ep is a positive real number. 
□ 
Remarks A.2.19 Identification of Q with </?(Q). We ignore the differences between 
p G Q and p G C and </>(p) = £p- G M. The meaning will be clear from the context. 
Hence we consider Q as a subset of R. We also denote the real numbers with single 
letters, as is customary. 

COMPLETENESS OF M 
427 
A.3 COMPLETENESS OF M 
The addition and multiplication operations and the order relation on E satisfy the field 
and order axioms. Hence we can develop all the related concepts, inequalities between 
real numbers, absolute values of real numbers, sequences, convergent sequences, and 
Cauchy sequences of real numbers as done before. We omit the repetition of these 
definitions and the related results. Instead, as an example, we give a detailed proof 
(perhaps more detailed than necessary) of the completeness of M. 
Note that any real number r is an equivalence class of Cauchy sequences of rational 
numbers. Any sequence x in this equivalence class will be called a representative 
(sequence) for r. The choice of representatives is arbitrary and not important. If 
r = <p(p) = p is a rational number, however, then the constant sequence p = p 
is taken as the standard representative of r = p. The identifications used here are 
explained in Remarks A.2.19. 
Lemma A.3.1 Let x be a representative for r £ l . Assume that there is an N G N 
such that 0 < xn. Then 0 < r. 
Proof. Any real number must satisfy exactly one of the following conditions: r = 0 
or r G P or —r G P. Our hypothesis rules out the last condition. In fact, this 
condition means that all representatives of — r must eventually be positive. But — r 
has at least one representative — x which is not eventually positive. Hence — r G' P. 
Therefore either r = 0 or r G P. 
□ 
Remarks A.3.2 Let x be a representative for r g l Assume that there is an N G N 
such that 0 < xn. Then 0 < r by Lemma A.3.1, but we cannot conclude that 0 < r. 
In fact, 0 < r means that not one but all representatives of r must eventually be 
positive. A simple counterexample is xn = 1/n, a positive sequence that represents 
0. 
Lemma A.3.3 Let r > 0, r £ 1. Then there is a p G Q such that 0 < p < r. 
Proof. Let x be a representative sequence for r. Hence x is eventually positive, since 
r > 0. Lemma A.2.14 shows that there is an a > 0, a G Q, and aniVeN such that 
either xn > a for all n > N or xn < — a for all n > N. The second case cannot be 
true, since x must eventually be positive. Hence xn > a for all n > N. Then, by 
Lemma A.3.1 above, 0 < a < r. Letp = a/2. Then 0 < (a/2) < a < r shows that 
0 < p < r. 
□ 
Lemma A.3.4 Let r, s G M. and r < s. Then there is a q G Q such that r < q < s. 

4 2 8 
CONSTRUCTION OF THE REAL NUMBERS 
Proof. We have 0 < (s — r). Use Lemma A.3.3 to find p £ Q such that 0 < 
p < (s — r). Let x and y be representatives for r and s. Then 0 < (s — r) — p 
shows that the sequence (yn — xn — p) is eventually positive. Find M £ N such that 
Vm > xm + P for all m > M. Since x is a Cauchy sequence, there is an N > M, 
N £ N, such that \xN - xn \ < p/4 for all n> N. 
Let qi = xN + (p/4) and q2 = xN + (3p/4). If n > N, then 
qi-xn 
= (p/4) + (XJV - xn) > (p/4) + \xN - xn\ > 0 
shows that that r < qlt and 
q2=xN 
+ (3p/4) = x„ + (XJV - xn) + (3p/4) < xn + p < yn 
shows that q-2 < s. If q = {q\ + q2)¡1 = x^ + (p/2), then x < q < s. 
D 
Remarks A.3.5 Upper bounds. Completeness is formulated in terms of upper 
bounds. They were introduced in Definition 2.2.1. M £ K is called an upper bound 
for a set A c R if a < M for all a £ A. Note that if A has an upper bound M £ K, 
then it also has an upper bound M' £ Q. This follows from Lemma A.3.4 or just 
from the fact that any Cauchy sequence of rational numbers is bounded by a rational 
number. 
Lemma A.3.6 Let A be a nonempty set of real numbers. Assume that A has an 
upper bound. Then there is a p £ Q such that p is not an upper bound for A but 
p + 1 is an upper bound for A. 
Proof. Let a £ A. Then r = (a — 1) is not an upper bound. Let s < r, s £ Q. 
Then s is not an upper bound. The sequence sn = s + n is an unbounded sequence. 
Hence there are n £ N for which s + n is an upper bound for A. Use the induction 
principle to find the least m £ N such that q = s + m is an upper bound. Then 
p = q — 1 = s + (m — 1) is not an upper bound. (Note that this is obviously true if 
m>2, 
but it is also true even if m = 1.) 
D 
Lemma A.3.7 Let A be a nonempty set of real numbers. Assume that A has an 
upper bound. Then there are two sequences of rational numbers pn and qn such that 
if n £ N, then pn is not an upper bound for A, qn is an upper bound for A, and 
( 9 n - P n ) - ( l / 2 ) " - 1 . 
Proof. Let p, q £ Q be the two numbers obtained in Lemma A.3.6. Let p\=p 
and 
q\ = q. Our requirements are satisfied for n = 1. Assume that p„, qn are obtained 
such that pn is not an upper bound, qn is an upper bound, and (qn —pn) = (1/2)™-1. 

COMPLETENESS OF R 
429 
Let sn = (pn + qn)/2. 
If sn is not an upper bound, then let pn+i = sn and 
qn+i = qn- If sn is an upper bound, then let pn+i — Pn and qn+i = sn. We see 
that our requirements are also satisfied for n + 1. Hence the sequences pn and qn are 
defined by the induction principle. G 
Lemma A.3.8 The sequences pn and qn obtained in Lemma A.3.7 are equivalent 
Cauchy sequences. 
Proof. We see that pn < pn+\ < qn+i < Qn- By induction it follows that 
Pn < Pn+k < qn+k < qn for all k e N. 
Therefore, if m > n, then \pn-pm\ 
< \Pn~qn\ = (l/2) n _ 1. But (1/2)""1 is a zero 
sequence. Hence pn is a Cauchy sequence. Similarly, qn is also a Cauchy sequence. 
The equivalence of pn and qn follows from the fact that (qn — pn) = (1/2)" - 1 is a 
zero sequence. 
Q 
Theorem A.3.9 Let A be a nonempty set of real numbers. Assume that A has an 
upper bound. Then A has a least upper bound. More explicitly, there is an r 6 R 
such that r is an upper bound for A, but ifs < r, s £R, then s is not an upper bound 
for A. 
Proof. Let pn and qn be the sequences obtained in Lemma A.3.7. Lemma A.3.8 
shows that they are equivalent Cauchy sequences. Hence they represent the same 
r £ l . We claim that r is a least upper bound for A. 
Assume that r is not an upper bound for A. Then there is an a G A such that r < a. 
Use Lemma A.3.4 to find g e Q such that r < q < a. Then the sequence q — qn 
represents the positive number (q — r) E K. Hence it must eventually be positive. 
Hence there is an N € N such that q > qn for all n > N. This means that qn < q < a 
and qn is not an upper bound for A. This is a contradiction. Hence r is an upper 
bound for A. 
Now suppose that A has an upper bound s £ R, s < r. As before, find a p g Q 
such that s < p < r. Then (pn — p) must eventually be positive, as it represents 
(r — p) > 0. Therefore pn > p > s for some n £ N. This means that pn is an upper 
bound for A. This is a contradiction. Hence A cannot have upper bounds less than 
r. Therefore r is a least upper bound for A. 
D 

This page intentionally left blank

APPENDIX B 
DIMENSION OF A VECTOR SPACE 
Let V be a vector space. Let B = {t»i, ..., bTO} and E = { e\, ..., en} be 
two bases for V. 
This means that both B and E are linearly independent and 
V = Span B = Span E. Our objective is to prove that m = n. This fundamental 
result tells us that the number of elements in any basis for a finite-dimensional vector 
space is the same. This number is thus well defined and is called the dimension of 
the vector space. 
One way to prove this result is to use a well-known result about linear systems of 
equations. Consider a homogeneous linear system of equations. If the number of 
unknowns is more than the number of equations, then this homogeneous system has 
nonzero solutions. This theorem implies that in R™, a set that contains more than 
n vectors cannot be linearly independent. Then the result about dimension follows 
easily. 
Instead of appealing to this argument, we shall provide a self-contained 
vectorial proof. The arguments used in this proof are purely algebraic. 
Analysis in Vector Spaces. 
431 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 

4 3 2 
DIMENSION OF A VECTOR SPACE 
B.1 
BASES AND LINEARLY INDEPENDENT SUBSETS 
The following theorem contains the main step in proving that any two bases have the 
same number of elements. 
Theorem B.l.l Let E = { ei, ..., e„} be a basis for V. Let A = { ai, ..., a m} 
be a linearly independent set in V. Then m < n. 
Proof. The idea of the proof is to start with E and move towards A by bringing in 
the elements { ai, ..., a m} of A, one at a time. As we add each member of A, we 
delete one of the remaining vectors e¿, so that our transition sets always have exactly 
n elements. 
The first step is to bring ax into E. Note that a¿ ^ 0 for each i, because A is linearly 
independent. Suppose that 
ai = ciei + 
1- cnen . 
One of the c¿ s must be nonzero. Rename the vectors in E, if necessary, to make 
c\ ^ 0. Then bring in a! and delete ei. We claim that the resulting set 
Ei — {ai, e2, ..., en} 
is a basis for V. Observe that E\ spans V, since it contains ei = (l/ci)(ai — c2e2 — 
■ ■ ■ — cnen) and it contains each of e2, ..., e„. Furthermore, we claim that E\ is a 
linearly independent set. Suppose 
fciai + k2e2 + + knen = 0. 
Then we have 
kiciei + (kic2 + k2)e2 H 
h (kicn + kn)en = 0 . 
From the linear independence of { ei, ..., e„}, each coefficient is 0. By the crucial 
assumption c\ ^ 0, it follows that k\ = 0 and hence that k2 = ■ ■ ■ = kn = 0. So E\ 
is a basis for V. 
For the inductive step, let 1 < r < m and 1 < r < n. Suppose that we have obtained 
Er by substituting the first r members of A for the first r members of E and that 
Er is still a basis. (There is no loss of generality here; we can rearrange the original 
listing of E if necessary.) Then we write a r + 1 as a linear combination of 
¡±i, . . . , a r , Gr-j-1, . . . 5 &n • 
The coefficient of at least one of e r +1, .... e n must be nonzero because of the linear 
independence of ai, ..., ar, a r+i. 

BASES AND LINEARLY INDEPENDENT SUBSETS 
4 3 3 
Again, without loss of generality, we assume that the coefficient of e r +i is nonzero. 
Proceed exactly as at the first step to substitute a r +i for e r+i. The resulting set 
Er+1 
= \&1, • • • , a r , 3-r+l, 6r+2i • • • i G n} 
is still a basis. 
If n < m, then En — { ai, ..., a„} will be a basis for V. If n < m, then the 
remaining vectors {ara+i, ..., a m} in A will be linear combinations of the first 
n vectors {ai, ..., a„}. This contradicts the linear independence of A. Hence 
m < n. If n = m, then A = En — { ai, ..., a„} is a basis for V. 
n 
Lemma B.1.2 Let Abe a linearly independent set in V. Ifh £ V and b ^ Span A, 
then B = A U {b} is also a linearly independent set. 
Proof. Let cb + ciai + • ■ • + cnan = 0 with some a¿ e A and c, c¿ e R. Then c 
must vanish. Otherwise, 
b = -(l/c)(ciai H 
h cnan) 
would imply that b £ Span A. But if c = 0, then also c\ = ... = cn = 0, since A 
is a linearly independent set. Therefore B is a linearly independent set. □ 
Theorem B.1.3 Let E = {ei, ..., e n} be a basis for a vector space V. Let 
A = { ai, ..., a m} be a linearly independent set in V. Then A is also a basis for 
V if and only ifm = n. 
Proof. Assume that A is a basis for V. In this case, the sets E and A are both bases 
and linearly independent sets. Then Theorem B.l.l implies that both m < n and 
n < m. Hence m = n. 
Conversely, assume that A is not a basis for V. Hence there is a vector b e V such 
that b ^ Span A. In this case, Lemma B.1.2 above shows that B = Au {b} is 
also a linearly independent set. But B contains m + 1 vectors. Then Theorem B.l.l 
implies that m + 1 < n. Hence m < n. □ 
Theorem B.1.4 Let E = { ei, ..., e„} be a basis for a vector space X. Let 
A = { ai, ..., a m} be a linearly independent set in X. If A is not a basis for X, 
then there is a set B c X such that A fl B = 0 and such that Aö B is a basis for X. 
Proof. If A is not a basis, then Span A ^ X. Hence there is a bi e X such that 
bi ^ Span A. Then^4i = ^4u{bi} is a linearly independent set consisting of (m+1) 
vectors. Repeating this step k = (n — m) times, we obtain k vectors bi, ..., b/-

4 3 4 
DIMENSION OF A VECTOR SPACE 
and a linearly independent set A¡. = A U { bi, ..., bfc} consisting of n vectors. 
Then Theorem B.1.3 shows that Ak is a basis. Hence let B = { bi, ..., b^}. Then 
A n B = 0 and A U B is a basis. D 
Remarks B.1.5 An examination of the proof of Theorem B. 1.1 shows that Theorem 
B. 1.4 is already established there. The proof above also depends on Theorem B.l.l, 
but only at the last step (via Theorem B.1.3). 
Definition B.1.6 Dimension. As always, we restrict our attention to finite dimen-
sional vector spaces. Theorem B.l.l shows that any two bases for a vector space V 
have the same number of elements. This number is called the dimension of V, and 
is denoted by dim V. Any subspace of a vector space is also a vector space by itself. 
Hence any subspace has a dimension too. 
Lemma B.1.7 Suppose that U is a subspace of V. Then dim U < dim V, with 
equality if and only ifU = V. 
Proof. Let E = { ei, ..., e n} be a basis for V. If A = { ai, ..., a m} is a basis 
for U, then A is also a linearly independent set in V. Therefore m < n by Theorem 
B.l.l. Also, Theorem B.1.3 shows that m = n if and only A is also a basis for V. 
This happens if and only if U ~ V. 
□ 

APPENDIX C 
DETERMINANTS 
Determinants provide a basic algebraic tool in computing volumes and integrals in 
vector spaces. They also play a key role in defining and working with tensor fields. 
Many of the computational aspects of determinants will be familiar to the reader. We 
will review the relevant definitions and results briefly, concentrating not so much on 
the computational but rather on the conceptual features of determinants. 
C.1 
PERMUTATIONS 
We consider only the permutations of a finite set A. In most cases, we let A — N„ = 
{ 1, 2, ..., n}. An invertible map a : A —> A is called a permutation of A. If A 
contains n elements, then §„ denotes the set of all permutations of A. An induction 
argument shows that S„ contains n! elements. 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
435 

4 3 6 
DETERMINANTS 
The identity permutation e G S„ is defined by e(i) = i, i G A. The inverse of 
a G §n is c-"1 G S„. If p and a are two permutations of A, then their composition 
is the permutation defined by (/9c)(¿) = p(a(i)), ¿ G A The composition of more 
than two permutations is defined similarly. In particular, ok is the composition of 
k copies of a. If p and q are two (different) elements of Nn, then the transposition 
T — (PQ) 6§n is defined by r(p) = q,r(q) = p and r(i) = i if i ^ p and ¿ / q. 
Theorem C.l.l Permutations in terms of transpositions. Any permutation is 
either the identity permutation or a composition of transpositions. 
Proof. This is clear if n = 1. Let n > 2 and assume the result for (n — 1). Let 
a G §„. The set of values ak{\), k G N, is a finite set A c N „ . Then both A and 
B = (Nn \ A) are invariant under a. We see that A is not empty, since 1 G A. 
If B is also not empty, then both sets contain at most (n — 1) elements. Hence, 
by the induction hypothesis, the restrictions of a to these sets are compositions of 
transpositions. It follows that a is the composition of the two sets of transpositions. 
Now assume that A = N„, so that B = 0. Let pk = uk{l), 1 < k < n. In this 
case, we have a = {j)\P2) • ■ • (Pn-iPn), so that a is a product of transpositions. 
In this formulation, pn = 1 and we apply transpositions by working from right to 
left. That is, to obtain a, we first apply the transposition (pn-iPn) = (Pn-il). then 
(Pn-2Pn-i) 
ar"d so on. The application of (P1P2) is the last step. 
□ 
Remarks C.1.2 Identity in terms of transpositions. If n > 2, then the identity 
permutation is also a composition of transpositions. In fact, r 2 is the identity 
permutation for any transposition r. If n = 1, however, there are no transpositions. 
That is why the identity permutation is singled out in Theorem C.l.l above. 
Theorem C.1.3 Sign of a permutation. There is a unique function, sign : §„ —> 
{ — 1, 1 }, defined by the following properties: (1) signr = — 1 for any transposition 
T G S„, and (2) sign(pa) = (signp) (signer) for any p, a G S„. 
Proof. If such a function exists, then its uniqueness is clear. In fact, any permutation 
is a composition of transpositions. Therefore, by (1) and (2) above, if a is the 
composition of p transpositions, then signer = (—l)p. The existence of such a 
function is not obvious, however, because the representation of a permutation as a 
composition of transpositions is not unique. If a is represented in two different ways, 
as a composition of p transpositions and as a composition of q transpositions, then 
we have to show that ( — l)p = (—l)9. 
To show that the sign function is well defined, then, set 
P(XU 
...,Xn) 
= T[ 
{Xi-Xj). 

DETERMINANTS OF SQUARE MATRICES 
437 
This is a polynomial of n real variables 
For each a G Sn, let 
aP(xu 
...,xn) 
= I l i ^ i ^ x n ^ C O ~ ^(¿))-
We see that aP(x\, 
..., xn) = ±P(xi, 
..., xn). 
In fact, the factors of P and 
crP are of the same absolute value but arranged in a different order. Then define 
signer = 1 if aP = P and signer = —1 if aP = —P. We see that the properties (1) 
and (2) are satisfied, 
n 
Corollary C.1.4 The signs of the identity permutation and the inverse permu-
tations. The sign of the identity permutation e is 1. Also, any permutation a and its 
inverse p ~ a~l have the same sign. 
Proof. Since e = e e, by Part (2) of Theorem C. 1.3 show that 
(signe) = (signe) (signe) — (±1)2 = 1-
Next, pa = e shows that (signp) (signer) = (signe) = 1. Since ±1 are the only 
possible values for the sign function, we see that (sign p) = (sign a). 
□ 
C.2 
DETERMINANTS OF SQUARE MATRICES 
Definition C.2.1 Determinant of a square matrix. Let A = (Aij) e Mnn be a 
square matrix with n columns and n rows. Then the number 
detA = det(Ali) = y " 
(signcr)TT^¿(7(¿) 
(C.1) 
is defined as the determinant of this matrix. Here the index i in the product ranges 
over Nn = { 1, ..., n}. The determinant of matrices in M n n is a function det : 
Theorem C.2.2 Determinant of the identity matrix. Let E be the identity matrix 
so that En = 1 and Eij = 0 if i j^ j . Then det E = 1. 
Proof. WeseethatrJi^'iiT(i) = life = eis t n e identity permutation and n¿-^»CT(¿) = 
0 if a 7^ e. Then (C.l) shows that det E = 1. 
□ 
Definition C.2.3 Conjugate matrices. A and B are conjugate matrices if By — 
AJÍ. We say B is the conjugate of A. 

4 3 8 
DETERMINANTS 
Theorem C.2.4 Determinant of the conjugate matrix. Let A and B be conjugate 
so that Bij = AJÍ. Then det B = det A. 
Proof. Let a G Sn be a permutation with the inverse permutation p = a~x. A term 
y4Í£7(¿) in the product in (C.l) can be also expressed as Ap^j 
= BjP^ 
by letting 
j = a(i). Hence, if the terms of the product in (C.l) are rearranged with respect to 
the increasing values of the second subscript, then the product becomes 
n ¿ = 1
A ^ W =AP(1)1---Ap(n)n 
-Y[J 
= 1
APU)J = I X = 1 B 3 PÜ)-
Now sign p = sign a by Corollary C. 1.4. Also, the operation of taking the inverse 
of a permutation is a one-to-one and onto mapping §„—>§„. Hence 
det A 
= 
V 
(signo-)TT Ai r(i) 
V 
(sign p)Y[BjpU)= 
det B. G 
'pes 
Theorem C.2.5 Determinants under linear combinations in one row. Let %Q G Nn 
be a fixed index and a', a" G R. Let A, A', A" G M n n be three matrices such 
that Aij = A'i3 = A'lj ifi^io 
and Aioj 
= a'A'ioj + a"A"oj. 
In other words, 
row i of A is a linear combination of row i of the other two matrices. Then 
det A = a' det A' + a" det A". 
Proof. We have, for any a G §„, 
ru-(o = (
a'
A^o)+
a"
A'ujii,¿,
A^ 
.... 
.„(o 
= 
a'A'i0cr{i0)lli¥:io
Ai<7(i) 
+a"Aioa(i0)lli9iio
Aia(i) 
Hence we see that det A = a' det A' + a" det A". 
D 
Theorem C.2.6 Determinants under permutations of the rows. Let B be the 
matrix obtained by permuting the rows of A G M.nn by a permutation A G S„. More 
explicitly, let B^ = Ax{i)j- Then det B = (signA) det A. 
Proof. Let ¡i = A - 1 G § n be the inverse of A. For each a G S„ we have 
Yli=1
Bi<r(i) = Yii=1
AXW<r(i) = J[j = 1
Aj(<rß)U)-

DETERMINANT FUNCTIONS 
439 
Here j — \(i), i — ß(j), and a (i) = cr(fj,(j)) = (er/i)(j). Therefore the last two 
products consist of the same factors. Also, 
det A = J^ g gJsign<r)Il" = i J4 i f f ( i ) = ^ ^ J s i g n ^ J J ^ 
( C T M ) ( J ). 
In fact, the mapping that takes a e §n to (aß) e Sn is a one-to-one and onto mapping 
§n —» §n- Hence the two sums above consist of the same terms. Therefore, we have 
detB 
= 
^ e g n ( 8 Í g n ( T ) J j " = i B i f f ( i ) = ^ e g n ( s i g n a ) J j J = i i 4 i ( < r / i ) ü ) 
= 
(siSnA)HCTe s j s i g n (cr^))il -Ai (^)ü') = (siSnA)det A-
For the third equality, note that (sign A)(sign (a/i)) = (sign a). 
Q 
Remarks C.2.7 Dependence on the rows and columns. Theorem C.2.5 shows that 
the determinant of a square matrix depends linearly on each particular row vector if 
the other rows remain constant. This is also true for the column vectors. In fact, the 
columns of a matrix are the rows of the conjugate matrix, and Theorem C.2.4 shows 
that the determinants of the original and conjugate matrices are the same. Similarly, 
Theorem C.2.6 shows that if the rows of a matrix are permuted, then its determinant 
is multiplied by the sign of this permutation. This is also true for the permutations of 
the columns, again because of Theorem C.2.4. 
C.3 
DETERMINANT FUNCTIONS 
The determinant of square matrices is connected with alternating multilinear func-
tions. First, let us recall the definition of multilinear functions given in Definition 
3.3.3. Let X and Y be two vector spaces and k G N. Then F : Xk —> Y is called a 
multilinear function if 
F( X l, ..., xfc) = o'F(xi, ..., X'k) + a"F{*'{, ..., x'¿) 
whenever there is an index IQ £ N^ = { 1, ..., k } such that x¿0 = a' x¿ 0 + a"x¿'0 
and x¿ = x¿ = x" if i ^ io- Here a', a" 6 IR are scalars. 
Definition C.3.1 Alternating multilinear functions. Let X and Y be two vector 
spaces and k € N. A multilinear function F : Xk —» Y is called an alternating 
multilinear function if 
-F(*A(i), ■•■> xA(fe)) = (signA)F(xi, ..., xfc) 
for all permutations A € Sfc of Nfc = { 1, ..., k}. 

4 4 0 
DETERMINANTS 
Definition C.3.2 Determinant functions. Let X be an n-dimensional vector space. 
Then any nonzero alternating multilinear function p : Xn —> R is called a determi-
nant function on X. 
Note that we call ip a determinant function on X even though ip is actually a function 
on Xn. 
Here it is very important to note that n = dimX. Basic examples of 
determinant functions are obtained in terms of the ordered bases of vector spaces and 
the determinants of square matrices. In fact, Corollary C.3.11 below shows that these 
examples cover all determinant functions. 
Definition C.3.3 Ordered bases. Let X be an n-dimensional vector space. An 
n-tupleE = (ei, ..., en) € Xn is called an ordered basisforX if the set of vectors 
{ ei, ..., e„} is a basis for X. Equivalently, E = (ei. ..., e„) e Xn is an ordered 
basis for X if { ei, ..., e n} is a linearly independent set of vectors. 
Remarks C.3.4 Bases and ordered bases. There is no ordering of the vectors in 
a basis { ei, ..., en} of X. A basis for X can be ordered in n! different ways and 
determines n! different ordered bases for X. 
Example C.3.5 Basic examples of determinant functions. An ordered basis E = 
(ei, ..., era) of X defines a function det£ : Xn —> R by 
detE(xi, ..., x„) = det(xij). 
(C.2) 
Here Xjj s arc the coordinates of x¿ s with respect to E = (ei, ..., e„) so that 
x¿ = Ylj xijej 
for all ¿ e Nn = { 1, ..., n } , and det (xtj) is the determinant 
of the square matrix {x¿j} e Mnra. Theorem C.2.5 shows that dete : Xn —> R 
is a multilinear function, and Theorem C.2.6 shows that it is alternating. It is 
also a nonzero function since, by Theorem C.2.2, detE(ei, ..., e„) = 1. Hence 
detg : Xn —> R is a determinant function. 
A 
Definition C.3.6 Determinant functions with respect to ordered bases. Let E = 
(ei, ..., era) be an ordered basis of X. Then the determinant function det£ : Xn —> 
R defined above in Example C.3.5 is called the determinant function with respect to 
the ordered basis E. 
We are going to show that for any determinant function <p : Xn —► R, there is an 
ordered basis E = (ei, ..., en) of X such that ip = detg. We need a general 
property of alternating multilinear functions. 
Lemma C.3.7 If F : Xk —> Y is an alternating multilinear function, then we have 
F(xi, ..., Xfc) = 0 whenever { xi, ..., x/t¡} is a linearly dependent set. 

DETERMINANT FUNCTIONS 
4 4 1 
Proof. If F : Xk —► Y is an alternating function and if (xi, ..., x^) £ Xk is such 
that Xj = Xj for some 1 < i < j < k, then F(xi, ..., x^) = 0. In fact, in this 
case, if r = (ij) is the transposition of i and j , then 
(TF)(XI, ..., xfc) = F(xi, ..., xfc) = (signr)F(xi, ..., xfc) 
forces F(xi, ..., Xfc) = 0 since signr = —1. 
Now assume that xi is a linear combination of the other vectors: xi = ]C?=2 
ajxj-
If F : Xk —> y is alternating and multilinear, then 
E
ra 
■_0
ajF(*j> 
x 2 , . . . , Xfc) = 0 
by the first part of the proof. In general, if { xi, ..., x^ } is a linearly dependent set, 
then one vector x¿ is a linear combination of the other vectors x¿. If r = (li) is the 
transposition of 1 and i, then (rir)(xi, ..., x¿) = ir(xT(1), ..., xT(fc)) = 0 since 
the first vector xT(!) = x¿ is a linear combination of the others. Hence we also have 
F(x 1 ; 
. . . , Xfc) = - ( T F ) ( X ! , . . . , xfc) = 0. Ü 
Theorem C.3.8 Let E = (ei, ..., en) be an ordered basis of X. Then any alter-
nating multilinear function ¡p : Xn —> R is a multiple ip = k detE of the determinant 
function detE : Xn —> R, where k — p{e\, ..., en). 
Proof. We have, since <¿> : X n —> R is a multi-linear function, 
(¿>(xi, ..., x„) = <¿>QT ^ x ^ e ^ , ..., ] T . a¡nj„ej„J 
= 
/ 
,, . ■ • • / 
,, • X1 jl 
^njn y(.ejl 1 • • • 1 ejn j ! 
where x¿ = X^7
 xii¿eii- N° w Lemma C.3.7 shows that i/?(uji> ..., uJn) can be 
nonzero only if the vectors {UJI: ..., Uj„ } are all different. This happens only if 
the set of indices { ji, ..., j n } is a permutation of N„ = { 1, ..., n }. Therefore 
the last set of sums can be replaced by a single sum over the permutations a £ §„. 
Thus we obtain 
<¿>(xi, ..., x„) = J2 
) (C.3) 
= 
T2 
o (SÍSncr):rla(l) • • • Xn*(n) f(^l, 
• • • , e„) (C.4) 
= 
</5(ei, ..., e„) ■ det(xij) 
(C.5) 
= 
ip(ei, ..., en) -detE(xi, ..., x n). 
(C.6) 
Here (C.4) follows from Definition C.3.1 of alternating multilinear functions, (C.5) 
follows from the definition of determinants of square matrices in C.2.1, and (C.6) 
follows from the definition of deti in (C.2). D 

442 
DETERMINANTS 
Corollary C.3.9 If p : Xn —> R is a determinant function, then y(ai, ..., a„) is 
nonzero if and only if{ ai, ..., a„} is a linearly independent set. 
Proof. If { ai, ..., a„} is a linearly dependent set, then Lemma C.3.7 shows that 
V?(ai, ..., an) = 0. Otherwise, A = (ai, ..., an) is an ordered basis for X and 
if = p(&i , ..., an) detA by Theorem C.3.8. But determinant functions are nonzero 
functions, by Definition C.3.2. Hence <p(a.i, ..., a n) ^ 0. 
D 
Corollary C.3.10 Any determinant function on X is a nonzero multiple of any other 
determinant function on X. 
Proof. Let <p, ip : Xn —> R be two determinant functions on X, and let E be an 
ordered basis (ei, ..., e„) for X. Then we see easily from Theorem C.3.8 that 
if = kip with k = <p(ei, ..., en)/ip(ei, 
..., en). Note that k is defined since 
ip{ei, ..., en) ^ 0 by Corollary C.3.9 above. D 
Corollary C.3.11 For any determinant function p : Xn —> R, there is an ordered 
basis E = (ei, ..., era) of X such that p = detg. 
Proof. If p : Xn —> K is a determinant function and A = (ai, ..., an) is an ordered 
basis of X, then a = cp(ai, ..., an) -£ 0 by Corollary C.3.9. Set ei = (l/a)ai 
and e¿ = a¿ if 2 < i < n. Then we see that E = (ei, ..., e„) is an ordered basis 
for X and y(ei, ..., e„) = 1 by the multilinearity of determinant functions. Then 
y? = detß by Theorem C.3.8. It is clear that this choice for E is not unique. 
□ 
Corollary C.3.12 Ifp : Xn - > l w a determinant function and A : Xn —» M is an 
alternating multilinear function, then there is a ß e R such that A = ß p. 
Proof. This follows directly from Theorem C.3.8 and Corollary C.3.11. 
□ 
We apply these results to obtain two additional facts about the determinant of square 
matrices. 
Theorem C.3.13 Determinant of a product. If A and B are square matrices in 
Mnn, then det(AB) = det A det B. 
Proof. Fix A and define the function D(B) = det(AB). From Theorem C.2.6 and 
Remarks C.2.7, we know that D is an alternating multilinear function in the columns 
of B. By Corollary C.3.10, there is a constant k such that D(B) = k det B. Letting 
B be the identity matrix /, we see that k = det A. This proves the conclusion. 
G 

DETERMINANT OF A LINEAR TRANSFORMATION 
4 4 3 
Theorem C.3.14 Invertible matrices and nonzero determinants. If A is a square 
matrix, then A is invertible if and only if det A ^ 0. If A is invertible, then 
det A det A"1 = 1. 
Proof. First, A is invertible if and only if the columns of A are linearly independent 
if and only if det A ^ 0, by Corollary C.3.9. Then if A A " 1 = I, Theorem C.3.13 
implies that det A det A - 1 = 1. 
□ 
C.4 
DETERMINANT OF A LINEAR TRANSFORMATION 
In this book we consider determinants mainly in the context of linear transformations. 
Each linear transformation T : X —» X has a number attached to it, called the 
determinant of T. 
It is denoted as det T. We provide the definition after the 
following observation. 
Lemma C.4.1 Let tp be a determinant function on X and E = (ei, ..., en) a basis 
for X. Then the number k — <p(Tei, ..., Ten)/ip(ei, 
..., en) is independent of 
the choices of if and E. 
Proof. We distinguish two cases. First, if ip(Tei, ■ ■ ■, Ten) = 0, then T maps 
a basis to a linearly dependent set. In this case it will map any basis to a linearly 
dependent set. Hence </?(Tei, ..., Ten) = 0 for all choices of ip and E. 
Second, if tp(Tei, ..., Ten) ^ 0, then the function 
V>(xi, ..., x n) = y(Txi, ..., 
Txn) 
defines a nonzero alternating multilinear function tp : Xn —> R (with each x¿ e X). 
Hence ip is a determinant function on X. Therefore there is a nonzero number k such 
that 
V>(x1; ..., xn) = (¿>(Txi, ..., Tx„) = fc(/?(xi, ..., x„) 
for all (xi, ..., x n) G Xn. 
This shows that fc is independent of the choice of E. 
Furthermore, if d is another determinant function on X, then there is a number I ^ 0 
such that if — M. Hence 
<p(Teu ..., Ten) 
= m(Teu 
..., Ten) 
= §{Teu 
..., 
Ten) 
V(ei, ..., en) 
¿i?(ei, ..., en) 
i?(ei, ..., en) 
which shows that k is also independent of the choice of ip. D 

4 4 4 
DETERMINANTS 
Definition C.4.2 Determinant of a transformation. The number k obtained in 
Lemma C.4.1 is called the determinant of the linear transformation T and denoted 
as det T. Hence 
, 
r^ 
<p(Tei, . . . . 
Ten) 
(p(e1, ..., e„j 
Here <p is any determinant function on X and (ei, ..., en) is any basis for X. 
C.5 
DETERMINANTS ON CARTESIAN PRODUCTS 
Theorem C.5.1 Let (U, V) be a coordinate system for W. Let dim U = a, dim V = 
b, and dim W = c = a + b. The associated coordinate projections are P : W —► U 
and Q : W —> V. If<p: Wc —> R is a determinant, then 
y(ui, ..., uQ; wi, ..., w6) = <^(ui, ..., ua; Qwj, ..., Qwb) 
whenever (ui, ..., ua) € [7a vvii/i arbitrary (wj, .... w¡,) € ty6. Similarly, 
<p{wi, • • ■, wa; vi, ..., vfe) = <^(Pwi, ..., Pw a; vi, ..., vb) 
whenever (vi, ..., Vf,) 6 y b w/i/z arbitrary (wi, ..., wa) € W°. 
Proof. We have wj = Pwi + Qwi with Pwi e (7. Hence 
¥>(ui, ..., ua; wi, ..., w6) 
= 
¡p(m, • • • , ua; Pw! + Qwi, w2, ■ • • , w6) 
= 
<£>(ui, ■ • • , ua; Pwi, w2, • ■ • , w6) + 
<^(ui, • • • , ua; Qwi, w2, • • • , w6) 
= 
(p(ui, • • • , ua; Qwi, w2, ■ • • , wt). 
To obtain the last equality, note that Pwi is linearly dependent on (ui, ..., ua) 
and, therefore, </?(ui> • •' ; ua! Pwi, w2, • • • , w¡,) = 0. This follows from the 
alternating property of determinants. The proof is completed by an obvious induction 
argument. 
D 
Theorem C.5.2 is a generalization of Corollary C.3.12 above. Two key applications 
of this theorem are given in Theorems C.5.3 and C.6.1 below. 
Theorem C.5.2 Let U and V be two vector spaces with dim U = a, dim V = b, 
where a, b G N. Let Ü : (Ua x Vh) -+Rbe a function such that 
(ui, ..., u„) € IIa -H. íí( U l, . . . , n a ; v 1 , . . . , v j ) e R 
(C.7) 
is an alternating multilinear function for each fixed (vi, ..., v¡,) € Vb and 
(v1? ..., v6) e Vb — n(ui, . . . l u 4 ; v i , . . . , v t ) e R 
(C.8) 

DETERMINANTS IN EUCLIDEAN SPACES 
445 
is an alternating multilinear function for each fixed (ui, ..., ua) G Ua. Then, given 
any two determinant functions p : IIa —» R and ip : Vb —> R, i/;er<? ¿5 a constant 
/?6R swc/z i/iaf 
íí(ui, ..., uQ; vi, ..., vb) =/3i^(ui, ..., ua)V'(vi, ..., vb) 
(C.9) 
for all (ui, ..., ua) € f/° and for all (vi, .... vb) G Fb. 
Proof. Let (vi, ..., vb) G F 6 be fixed. The function in (C.7) is an alternating 
multilinear function IIa —> R. Then Corollary C.3.12 shows that it is a multiple of 
the determinant function p : Ua —> R. The multiplication constant will be a number 
a(v 1 ; ..., Vf,) G M that depends on (vi, ..., v¡,) G Vh. Hence 
fi(ui, ..., u0; vi, ..., vfc) = a ( v i , ..., v6)<^(ui, ..., u a) 
(C.10) 
for all (ui, ..., u a) G IIa. But (C.8) shows that 
(vi, ..., v6) G Vb - , a(v!, ..., vb) G R 
(C.ll) 
is also an alternating multilinear function. Hence, by Corollary C.3.12, there is a 
constant ß G R such that 
a(vi, ..., v b ) = ^ V ( v i , ..-, vb) 
(C.12) 
for all (vi, ..., v6) G V^6. Then the proof follows from (C.10) and (C.12). 
D 
Theorem C.5.3 Determinants on Cartesian products. Let W — U x V. Let 
<p : Ua —> R, ip : Vb —> R, and 6 : Wc -^ R be three determinant functions on U, 
V and W, respectively. Then there is a nonzero constant ß such that 
0(ui, ..., ua; vi, ..., vb) = /?<¿>(ui, ..., u a)^(vi, ..., v6) 
(C.13) 
for all (ui, ..., ua) G IIa and (vi, ..., v6) G Vb. 
Proof. Define Ü : (Ua x V6) -* R as 
íí(ui, ..., ua; vi, ..., vb) = 0(ui, ..., ua; vi, ..., vb). 
(C.14) 
We see that the hypotheses (C.7) and (C.8) of Theorem C.5.2 are satisfied. Then the 
result follows from the conclusion (C.9) ofthat theorem. 
D 
C.6 
DETERMINANTS IN EUCLIDEAN SPACES 
There are special determinant functions on a Euclidean space, defined in relation to 
the inner product. We will refer to these special determinants as the Euclidean (or 

446 
DETERMINANTS 
standard) determinants. We will show that each Euclidean space has exactly two 
standard determinant functions. 
Theorem C.6.1 Determinants in Euclidean spaces. Let X be a Euclidean space. 
Let if = detE : Xn —> R be the determinant function with respect to an ordered 
orthonormal basis E = (ei, ..., e„) ofX. Then 
<¿>(xi, ..., x„) • ip(y1, ..., y„) = det(x¿, y,- } 
(C.15) 
for all (xi, ..., x„), (yi, ..., y n) £ Xn. 
The last determinant in (C.15) is the 
determinant of the square matrix (x¿, y,) G M„„. 
Proof. Define VL : (Xn 
x I " ) ^ R a s 
fi(xi, . .., x„; yi, ..., y n) = det(x¿, y¿ }. 
(C.16) 
We see that the hypotheses (C.7) and (C.8) of Theorem C.5.2 are satisfied. Then the 
conclusion (C.9) ofthat theorem shows that there is a constant ß e M such that 
1 ; • • • ) ^ n ) • <¿>(yi, • • •, Yn) = det(x¿, yj ) 
(C.17) 
for all (xi, ..., x„), (yi, ..., y„) e Xn. To determine the value of ß, let 
(xi, ..., x n) = (yi, ..., yn) = (ei, .... e n). 
Then (e¿, e,) £ Mn„ is the identity matrix and det(e¿, e¿) = 1 by Theorem C.2.2. 
Also, cp(ei, ..., e„) = det^ej, ..., en) = 1 by the definition of detg in (C.2). 
Hence we see that ß = 1. D 
Definition C.6.2 Euclidean determinants in Euclidean spaces. Let X be a Eu-
clidean space and dim X = n. Call a determinant function p : Xn - > 1 Ú Euclidean 
(or standard) determinant function of X if 
p{ni, ..., u„) = ±1 
for all ordered orthonormal bases (ui, ..., u„) € Xn of X. 
Theorem C.6.3 Existence of Euclidean determinant functions. Each Euclidean 
space X has exactly two Euclidean determinant functions. 
Proof. Let ip = detE : Xn —> R be the determinant function with respect to an 
ordered orthonormal basis E = (e1; ..., e„) of X. Let (ui, .... u„) be another 
ordered orthonormal basis. Then Theorem C.6.1 shows that 
<¿>(ui, ..., un)p(m, 
..., u„) =det(u i ; Uj) = 1, 

DETERMINANTS IN EUCLIDEAN SPACES 
447 
since (u¿, Uj) G M n n is the identity matrix for any ordered orthonormal basis. 
Hence p(u\, ..., u n) = ±1 and p = detg is a Euclidean determinant function. 
Conversely, if <p is a Euclidean determinant function, then p(ei, ..., en) = ±1. 
Hence p = ± detE by Theorem C.3.8. Thus we see that p : Xn —► K is a Euclidean 
determinant function if and only if p = ± detE-
Note that if U is another ordered orthonormal basis, then detu = ± detE- Hence the 
choice of the ordered orthonormal basis does not change the two possibilities ± detE 
for a Euclidean determinant function. 
□ 
Euclidean Determinants on Subspaces 
A subspace U of a Euclidean space Z is also a Euclidean space with the inner 
product restricted to U. Hence U also has two Euclidean determinant functions. In 
the following discussion, ipz and ipu each denote one of the Euclidean determinants 
on the corresponding spaces. Also, a, Í) e N, a + b = c = dim Z, and X is a 
subspace of Z with dim X = a. 
Theorem C.6.4 Euclidean determinants on orthogonal subspaces. Let (X, Y) 
be an orthogonal coordinate system in Z. Then 
ipz{*i, ■ ••, *a; yi, ■■■, y&) = ±fa(xi, ..., xa)Vv(yi, •••, y&) 
(C.18) 
for all (xi, ..., xa) G Xa, (y1; ..., y6) G Yb. 
Proof. Theorem C.5.3 shows that there is a ß € K such that 
ipz(xi, ..., xa; yi, ..., yb) = ßipx(*u 
•••, xQ)Vy(yi, ■ • ■, yt) 
(C.19) 
for all (x1; ..., xa) e Xa, (yi, ..., y6) £ Yb. 
To find the value of ß, let 
(ai, ..., aa) and (bi, .... b¡,) be two ordered orthonormal bases for X and Y, 
respectively. Then (ai, ..., aa; bi, ..., bj,) is an ordered orthonormal basis for 
Z. Since all the Euclidean determinant functions take the values of ±1 on ordered 
orthonormal bases, we see that ß = ±1. D 
Theorem C.6.5 Let (X, V) be a coordinate system for Z. Let Y = X1- and let 
P and Q be the orthogonal projections on X and Y, respectively. Then there is a 
nonzero number ß such that 
V>z(xi, ..., x a; vi, ..., v t) 
= 
/3Vx(xi, ..., x a)^y(vi, ..., vh) 
= 
i>x(x-i, ■•■, xQ)i/'y(Qv1, ..., Qvb) 
for all (xi, ..., xa) G Xa and (vi, ..., v;,) G Vb. 

4 4 8 
DETERMINANTS 
Proof. The first equality follows directly from Theorem C.5.3. To prove the second 
equality, first apply Theorem C.5.1 to obtain 
V>¿?(xi, ..., xQ; vi, ..., v6) = Vz(xi, • ■ •, x0; Qvl5 ..., Qvb) 
(C.20) 
and then apply Theorem C.6.4 to (C.20). 
□ 
Corollary C.6.6 Let V be complementary to X and Y — X x. 
Let Q be the 
orthogonal projection on Y. Then there is a nonzero number ß such that 
ßipviyx,..., 
vfc) = Vv(Qvi, • • •, Qv6) 
for all (vi, ..., 
vb)eVb. 
Proof. This follows directly from Theorem C.6.5. 
□ 
Theorem C.6.7 Let Z be an n-dimensional Euclidean space. If'iß : Zn —> R is a 
Euclidean determinant, then \ip{i\, ..., z„)| < ||zi || • • • ||zn||. 
Proof. Proceed by induction on n G N. 
□ 
C.7 TRACE OF AN OPERATOR 
Notations C.7.1 Let X be an n-dimensional space and T : I - ^ I a linear trans-
formation. For each determinant function f : Xn —> R and for each (xj, ..., xn) G 
X n, define 
E
n 
. ,<p(x¿i, ..., x i n), 
(C.21) 
where x¿3- = Xj if ¿ ^ j and x¿¿ = Tx¿. 
Hence, if n = 3, for example, then 
£(</?, T)(xi, x2, x3) = <p(Txi, x2, x3) + <¿>(xi, Tx2, x3) + </>(xi, x2, Tx 3). 
Theorem C.7.2 Lei B(</?, T) : I " -> R te as de/med in (C.21). Then there is a 
number (TrT) swc/z í/¡af B{ip, T) — (TrT) ip for all determinants p : X™ —> R. 
Proof. This is identical to the proof of a similar statement for det T, Lemma C.4.1. 
The details are Left as an exercise. 
O 

TRACE OF AN OPERATOR 
4 4 9 
Definition C.7.3 Trace of a transformation. Let T : X —> X be a linear trans-
formation. The number (TrT) obtained in Theorem C.7.2 is called the trace of 
T. 
The determinant of T was defined in Definition C.4.2. We need the following special 
relation between Tr T and det T. 
Theorem C.7.4 Let A be an open interval and let T{-) : A —> L{X, X) be a 
dijferentiable function. Assume that T{a) = / is the identity transformation I : 
X -> X for an a e A. Then (det T)'(a) = (TrT'(a)). 
Proof. Let E = (ei, ..., en) be an ordered basis for X and ¡p = detg the determi-
nant function of E. Hence (p is the determinant function specified by the requirement 
that ip(ei, ..., en) = 1. Then we see that 
detT = <p(Teu ..., 
Ten). 
Hence, by the differentiation rule for multi-linear functions, 
(detT)(a)= > 
«(e¡i, ..., ein), 
where e¿¿ = T(a)ej = e7 if i =fc j and e¿¿ = T'(a)ei. Hence 
(detr)'(a) = (TrT'(a)) 
follows. Here the equality </?(ei, ..., e„) = 1 is used again. 
D 

This page intentionally left blank

APPENDIX D 
PARTITIONS OF UNITY 
Partitions of unity is a basic technique that lets us reduce the investigation of the 
behavior of a function over a region to its investigation over small neighborhoods. 
Conversely, the technique is used to extend results that have been established for 
small neighborhoods to a larger setting. 
A weak version of the technique is presented in Chapter 6 in the proof of the inverse 
function theorem. The stronger version below uses 6°° functions rather than the C1 
functions used earlier. Some arguments require a high degree of differentiability, and 
the version of partitions of unity provided here will work for any desired degree of 
differentiability. The proof below proceeds in the same general manner as the proof 
for the weak version in Chapter 6. 
Analysis in Vector Spaces. 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 
451 

452 
PARTITIONS OF UNITY 
D.1 
PARTITIONS OF UNITY 
Lemma D.1.1 There is a C°° function h : R —> R such that h(x) =0ifx<0 
and 
h(x) >0ifx> 
0. 
Proof. Define h(x) = 0 if x < 0 and h(x) — exp( —1/x) if x > 0. An induction 
argument shows that all derivatives of exp(—1/x) at a point x =¡¿ 0 are of the 
form P(u)exp(u), 
where P(u) is a polynomial in u — —1/x. Then we see that 
limx^0+ h^ (x) = 0 for the derivatives of all orders n £ N. Hence h : R —> R is a 
e°° function with the required properties. 
□ 
Lemma D.1.2 Let a > 0. 77iere is a 6°° function g : R —> R si/cft í/iaí g(x) — 0 if 
x <0 or ifx > a and g{x) 
>0if0<x<a. 
Proof. Let h : M —> R be the function obtained in Lemma D.1.1. Then 
g(x) = h(x)h(a — x) 
is a function with the required properties. 
□ 
Lemma D.I.3 Let a > 0. There is a Q°° function f : M —*■ [0, 1] such that j'(x) = 0 
ifx < 0 and f(x) = 1 ifx > a. 
Proof. Let g : R —*• R be the function obtained in Lemma D.1.2. Then 
f{x) = {l/A) [ g(t)dt, 
xeR, 
Jo 
with A — J*Q
a g(t)dt, is a function with the required properties. 
□ 
Lemma D.1.4 Let a > 0. There is a Q°° function ip : X —* [ 0, 1 ] on a Euclidean 
space X such that <¿>(x) = 1 if ||x|| < 1 and ip(x) = 0 i/(l + a) < ||x||. 
Proof. Let a = ( l + a) 2 — 1. Let / : R —> R be the function obtained in Lemma 
D.1.3 with this a > 0. We claim that 
^(x) = l - / ( | | x | | 2 - l ) , 
X G X 
has the required properties. An easy check shows that ip takes values in [0, 1 ], and 
that ip(x) = 1 if ||x|| < 1 and <¿>(x) = 0 if ||x|| > (1 + a). Also, the function 
A(x) = ||x||2 - 1 = (x, x) - 1 is a polynomial. Hence A : X —> R is a C00 function. 

PARTITIONS OF UNITY 
453 
It follows that /-Ais also a C°° function as it is the composition of two C°° functions. 
So^isC 0 0. 
ü 
Lemma D.1.5 Let Gi be any open covering of a compact set C. Then there is a 
5 > 0 such that for any x e C , B$(x) is contained in one of the covering sets G¡. 
Proof. Assume this is not true. Then for each n £ N, there is an x„ € G such that 
B\/n(xn) 
is not contained in any of the G¿. Since G is compact, we can assume that 
x„ is convergent, without loss of generality. If x n —*• a, then a f C and a £ G, for 
one of the G, s. Then _ß1/n(x„) C G¿ for sufficiently large n. This contradiction 
proves the lemma. 
□ 
Lemma D.1.6 Let BTi (x¿) ¿>e a finite covering of a compact set C. Then there is a 
ß < 1 such that Bßri (x¿) « sri// a covering ofC. 
Proof. Use Lemma D. 1.5 to find a 5 > 0 such that for any x £ G, B¿ (x) is contained 
in one of BTi (x¿). Then let /3 > max¿((ri — ¿)/V¿) and ß < 1. 
□ 
Lemma D.I.7 Leí C ¿e compact, let G be open and suppose C C G. 77¡en í/zere is 
a 6°° function ^/> : X —> [ 0, 1 ] SMC/¡ //¡ai ^>(x) = 1 ifx £ G and V'W = 0 í/ x 0 G-
Proof. Let BTi (a¿) c G be a finite covering of C consisting of n balls. Find a ß < 1 
such that ¿?ßri (a¿) is still a covering of G. By an easy extension of Lemma D.1.4, 
there are C°° functions <¿?¿ : X —► [0, 1] such that </?i(x) = 1 if x £ Bßri{sn) and 
<p¿(x) = 0 if x £" B r i (a¿). Then we see that 
■0 = 1 - (1 -ipi)---(l 
-<pn) 
is a function with the required properties. 
□ 
Theorem D.1.8 Partitions of unity. Let Bri (x¿) be a finite covering of a compact 
set C. Then there are C°° functions A¿ : X —> [ 0, 1 ] such that each A¿ has a compact 
support Si C BTi (x¿) anJ such that ^ ¿ A¿(x) = lfor all x £ G. 
Proof. Find a < 1 as in Lemma D.1.6 so that ¿?Qrj(x¿) is still a finite cover of G. 
Let a < ß < 1. Use Lemma D. 1.4 to find 6°° functions //¿ : X —► [0, 1] suchthat 
£t¿(x) = 1 if x £ BQT..(a¿) and /x¿(x) = 0 if x ^ BßTi(&i). Then the finite sum 
p = ^ ¿ /U¿ is a 6°° function and yu(x) > 1 for all x £ G. Let G be the set of x £ X 
such that fi(x) > 1/2. 
Now use Lemma D.I.7 to find a C°° function i¡) : X —> [0, 1 ] such that ip(x) = 1 
if x £ G and ip(x) = 0 if x 0 G. Define A¿ = (/Í¿//U)Í/>, but also take A¿(x) = 0 

4 5 4 
PARTITIONS OF UNITY 
whenever ip(x) = 0, even if /x(x) = 0. If ?/>(x) ^ 0, then x € G and /x(x) > 1/2. 
Hence the functions A¿ are 6°°. They take values in [0, 1]. The support of A¿ is 
the closure of Bpri (a¿), which is contained in Bri (x¿). Also £V A¿(x) = 1 for all 
x e C . 
D 

INDEX 
acceptable diffeomorphisms, 403 
by partitions, 299 
additivity 
upper, 305 
of volume, 296 
supports of, 307 
additivity of volumes, 292 
Archimedes, 288 
adjoint transformations, 112 
arc wise connected sets, 163 
affine approximations, 179, 193 
associativity, 421 
and mean value theorems, 330 
atlases 
inverses of, 330 
for manifolds, 346 
almost regular sets, 407 
axioms 
Stokes' theorem for, 409 
field, 
419 
approximation by Taylor polynomials, 184 
in Q, 420 
approximation by Taylor series, 185 
order, 419 
approximations 
in Q, 420 
affine, 193 
axis of rotation, 189 
boundary, 294 
instantenous, 189 
by cubes, 294 
by partitions, 299 
balls 
by step functions, 305 
and cubes, 290 
inner, 294 
Euclidean, 302 
by partitions, 299 
bases, 64 
lower, 305 
ordered, 440 
supports of, 307 
bilinear functions 
outer, 294 
symmetric, 86 
Analysis in Vector Spaces. 
455 
By M. A. Akcoglu, P. F. A. Bartha and D. M. Ha 
Copyright © 2009 John Wiley & Sons, Inc. 

456 
INDEX 
bilinear functions, 82 
dot products and, 83 
general form of, 83 
linear functions and, 82 
the space of, 84 
binary expansions, 54 
binary grids, 290 
cubes of, 290 
density of, 291 
binomial coefficients, 14 
binomial theorem, 17 
binormal vector, 187 
bisection sequences, 59 
blocks, 297 
as Jordan sets, 297 
Bolzano-Weierstrass Theorem 
in R, 46 
Bolzano-Weierstrass theorem 
in Euclidean spaces, 109, 129 
in normed spaces, 132 
boundaries, 159 
and closed sets, 160 
and flux, 392 
and open sets, 159 
empty, 160 
boundaries of complementary sets, 159 
boundary of a set 
in R, 58 
boundary-surfaces 
integrals on, 409 
outer, 406 
bounded above, 36 
bounded below, 38 
bounded functions, 303 
of bounded support, 303 
bounded sets 
in R, 39 
in normed spaces, 129 
e n functions, 269 
chain rule for, 269 
Cartesian product spaces 
sequences in, 132 
Cartesian products, 67 
and derivatives, 216 
direct sums and, 67 
identification of direct sums and, 68 
of cubes, 291 
of Jordan sets, 297 
volumes in, 294 
Cartesian products of sets, 5 
Cauchy sequences, 46, 133 
equivalence classes of, 421 
in Q, 421 
Cauchy-Schwartz Inequality 
in Euclidean Spaces, 96 
chain rule, 212 
and Jacobian matrices, 214 
for e n functions, 269 
change of integrals, 334 
change of variables, 328 
in tensor fields, 370 
change of volumes, 332 
characteristic functions 
integrals of, 310 
charts 
for manifolds, 346 
orientation-preserving, 354 
classical Stokes' theorem, 413 
basic, 413 
in tensor form, 414 
in vector form, 414 
closed sets, 160 
and boundaries, 160 
and convergent sequences, 161 
in R, 57 
closure of a set, 161 
closures of 
sets in R, 55 
commutativity, 421 
compact sets, 156, 164 
and continuous functions, 166 
in normed spaces, 164 
compactness, 148 
compatibility 
of parametric representations, 248 
complementary subspaces, 66 
complementary projections, 73 
complements of sets, 5 
compositions 
images under, 26 
computations of directional derivatives, 198 
cones, 302 
volumes of, 318 
congruence, 15 
connected sets, 163 
and continuous functions, 163 
and intervals in R, 57 
arcwise, 163 
in M, 57 
content 
geometric, 374 
contents 
on manifolds, 349 

INDEX 
457 
continuity 
and derivatives, 179 
in Cartesian products, 145 
in normed spaces, 142 
of inner products, 147 
of linear combinations, 147 
of linear functions, 144 
of multilinear functions, 145 
of polynomials, 152 
of the inverse function, 147, 149 
under compositions, 146 
under limits, 149 
uniform, 150 
continuity and derivatives, 199 
continuous differentiability 
necessary conditions, 194 
continuous functions, 142, 150, 162, 163 
and compact sets, 166 
and compositions, 146 
and connected sets, 163 
and open sets, 162 
between normed spaces, 142, 150 
integrals of, 310 
limits of sequences of, 151 
continuously differentiable functions, 193 
convergence 
of polynomials, 152 
uniform, 150 
convergent sequences 
and closed sets, 161 
convex sets, 168 
coordinate changes, 20 
coordinate functions, 74 
derivative of, 216 
coordinate systems, 74 
cross products 
derivatives of, 221 
cross-sections, 301 
continuously changing, 301 
cubes 
and balls, 290 
as unions of higher order cubes, 292 
Cartesian products of, 291 
of binary grids, 290 
translations of, 290 
volumes of, 292 
curl of a vector field, 415 
curvature, 188 
curves, 244 
on a manifold, 250 
plane, 190 
cylinders, 302 
circular, 302 
volumes of, 318 
cylindrical coordinates, 20 
derivative, 175 
and the Jacobian matrix, 207 
of a function of a real-variable, 176 
of the coordinate functions, 216 
of the inverse function, 216 
derivatives 
and Cartesian products, 216 
and continuity, 179, 199 
and increments, 179, 199 
and tangent spaces, 346 
and Taylor series, 280 
directional, 195 
computations of, 198 
existence of, 201, 203 
existence theorem for, 203 
higher-order, 182 
of cross products, 221 
of higher-order 
directiona, 268 
of higher-orders, 267 
of inner products, 221 
of multilinear functions, 219 
of polynomials, 273 
on manifolds 
along curves, 261 
partial, 205 
higher-order, 210 
restricted, 195 
determinant 
of a transformation, 443 
determinant functions, 116, 439 
determinants 
as volume multipliers, 117, 326 
Euclidean, 326, 446 
in Cartesian products, 444 
in Euclidean spaces, 446 
in Euclidean subspaces, 447 
in orthogonal subspaces, 447 
linear combinations in, 438 
permutations of rows in, 438, 439 
with respect to ordered bases, 440 
diffeomorphisms, 225 
acceptable, 403 
examples of, 232 
orientation-preserving, 354, 403 
difference operators, 268, 270 
commutativity of, 270 
differentiability 

458 
INDEX 
continuous 
necessary conditions, 194 
necessary conditions, 194 
differentiable function, 175 
differentiable functions, 190, 192 
differentials 
total, 215 
differentiation, 177 
change of order in, 270 
of products, 218 
on manifolds, 260 
ordinary rules of, 177 
rules of, 211 
term-by-term, 275 
dimension of vector spaces, 65 
dimension theorem in vector spaces, 65 
dimensions 
of vector spaces, 431 
direct sums of vector spaces, 66 
directional derivatives, 195 
computations of, 198 
discs, 12 
distributivity, 421 
divergence 
and exterior derivatives, 398 
as density of expansion, 391 
of vector fields, 382 
dot products, 83 
eigenbases, 110 
determinants and, 115 
existence of, 110 
for orthogonal projections, 115 
for the adjoint transformation, 113 
eigenvalues, 112 
eigenvectors, 112 
ellipsoids, 302 
volumes of, 319 
enlargements of sets, 375 
equations 
implicit, 253 
Euclidean balls, 302 
as Jordan sets, 317 
volume of, 317 
Euclidean determinants, 326 
and volumes, 342 
computation of, 341 
on coordinate systems, 343 
review of, 341 
Euclidean Spaces, 95 
Gram-Schmidt process in, 99 
norms in, 95 
orthogonal complements in, 104 
uniqueness of, 104 
orthogonal decompositions in, 104 
orthogonal projections in, 105 
orthogonal sets in, 97 
orthogonal subspaces in, 103 
orthogonality in, 97 
orthonormal bases in, 97 
existence of, 100 
orthonormal sets in, 97 
Pythagoras Theorem in, 97 
Euclidean spaces 
linear transformations in 
boundedness of, 101 
Bolzano-Weierstrass theorem in, 109, 
129 
determinants in, 446 
Euclidean determinants in, 446 
integrals on, 343 
linear functions in 
boundedness of, 101 
representations of, 100 
motions of, 186 
oriented, 341 
Euclidean volumes, 340 
intrinsic definition of, 342 
intrinsic expressions for, 342 
on coordinate systems, 344 
on subspaces, 343 
existence of derivatives, 201, 203 
exponentiation operator, 278 
exterior derivatives, 396 
and divergence, 398 
and pullbacks, 399 
of tensor fields, 396 
extremal values, 282 
local, 282 
on manifolds, 262 
field axioms, 419 
Fleming, Wendell, 396 
flows, 386 
change of volume in, 390 
initial rate of, 391 
of compact support, 386 
on manifolds, 412 
smooth, 387 
with given initial velocities, 387 
flux 
and boundaries, 392 
of vector fields, 392 
Frenet formulas, 187 

INDEX 
Fubini's Theorem, 288 
Fubini's theorem, 314 
function 
continuously differentiable, 176 
differentiable, 175 
nondifferentiable 
with directional derivatives, 201 
functions, 17 
bijections, 24 
characteristic, 304 
integrals of, 310 
classes of, 269 
composition of, 25 
continuous 
integrals of, 310 
continuously differentiable, 193 
differentiable, 190, 192 
classes of, 269 
direct images under, 21 
domain spaces of, 17 
domains of, 17 
identity, 18 
images under, 21 
inverse, 24 
inverse images under, 21 
invertible, 24 
multilinear, 219 
negative parts of, 303 
notations for, 18 
of class e n, 269 
of several variables, 205 
one-to-one, 22 
one-to-one and onto, 24 
positive parts of, 303 
range spaces of, 17 
ranges of, 21 
restrictions of, 18 
step, 304 
integrability of, 304 
fundamental theorem of calculus, 311 
geometric content, 374 
geometric series, 43 
gradient vector, 208 
and the inner products, 209 
and the Jacobian matrix, 209 
and the local extremal values, 209 
Cartesian coordinates of the, 209 
graphs, 17, 238 
as diffeomorphisms, 238 
domain spaces of, 17 
domains of, 17 
integration on, 372 
normal spaces of, 239 
range spaces of, 17 
regions under, 303 
tangent spaces of, 238 
half-planes 
in R2, 11 
Heine-Borel Theorem, 165 
helix, 188 
higher-order derivatives, 182, 267 
directional, 268 
higher-order partial derivatives, 210 
implicit equations, 253 
of manifolds, 253 
implicit function theorem, 254 
implicit functions 
and Jacobian matrices, 256 
implicit representations 
of manifolds, 252 
increments, 179 
increments and derivatives, 199 
induction principle, 13 
inequalities, 33 
infimum, 39 
inner products, 95 
derivatives of, 221 
integrals, 303 
as limits of sums, 308 
change of, 334 
general 
on manifolds, 349 
in product spaces, 312 
iterated, 312, 314 
local 
on manifolds, 347 
lower, 306 
of general functions, 303 
of nonnegative functions, 303 
on boundary-surfaces, 409 
on Euclidean spaces, 343 
on manifolds, 345 
upper, 306 
integration 
as a linear operator, 308 
as a positive operator, 308 
linearity of, 304, 309 
on graphs, 372 
on manifolds, 340 
positivity of, 304, 309 
interior of a set, 159 

460 
INDEX 
intermediate value theorem, 164 
intersections of sets, 5 
intervals, 10, 54-57 
bounded, 54, 55 
closed, 55 
end points of, 54 
final points of, 54 
initial points of, 54 
nested, 59 
open, 55 
types of, 10 
inverse function 
continuity of the, 147 
inverse function theorem, 226 
for e 1 functions, 226 
for C 1 functions, 282 
for higher derivatives, 282 
relation to Newton's iteration method, 
231 
inversion operator, 153, 277 
continuity of the, 155 
geometrical series and the, 153 
nonlinearity of the, 153 
isomorphisms, 75 
iterated integrals, 312, 314 
Jacobian matrices 
and implicit functions, 256 
and the chain rule, 214 
Jacobian matrix, 206 
and the derivative, 207 
Jordan sets, 290, 295 
and partitions, 299 
boundaries of, 321 
Cartesian products of, 297 
closures of, 322 
Euclidean balls as, 317 
images of, 321 
in manifolds, 349 
interiors of, 322 
topological definition of, 321 
topology of, 298 
translations of, 324 
under diffeomorphisms, 322 
under isomorphisms, 323 
Jordan, Camille, 289 
kernel of a linear transformation, 69 
Lagrange multipliers, 263 
Lebesgue's theorem, 320 
linear functions 
bilinear functions and, 82 
linear combinations, 62 
linear functions 
continuity of, 144 
linear independence, 64 
linear transformations 
bases under, 70 
boundedness of, 135 
compositions of, 71 
kernel of, 69 
matrices and, 77 
norms of, 136 
range of, 69 
standard norms of, 137 
trace of, 382 
linearly independent sets, 64, 432 
lines 
in K2, 11 
local extremal values, 209 
and the gradient vector, 209 
on manifolds, 262 
local integrals 
on manifolds, 347 
lower approximations 
supports of, 307 
lower bounds, 38 
greatest, 38 
lower integrals, 306 
lower sums, 306 
manifold 
curves on a, 250 
manifolds, 243 
atlases for, 346 
charts for, 346 
contents on, 349 
derivatives on, 261 
differentiation on, 260 
global orientations on, 357 
implicit equations of, 253 
in implicit representations, 252 
in parametric representations, 243 
integrals on, 345 
general, 349 
Jordan sets in, 349 
local extremal values on, 262 
local integrals on, 347 
local orientations on, 355 
nonorientable, 357 
orientable, 357 
orientations of, 353 
parametric representations for, 346 

INDEX 
461 
parametric representations of, 244 
reverse charts for, 346 
Stokes' theorem on, 412 
matrices, 76 
conjugate, 437 
determinants of conjugate, 438 
determinants of square, 437 
linear transformations and, 77 
space of, 76 
square, 437 
matrix 
Jacobian, 206 
matrix multiplication, 84 
as a bilinear function, 85 
compositions as, 85 
linear maps as, 85 
maximum norm, 290 
mean value theorem 
Cauchy, 182 
fixed upper bound, 181 
variable upper bound, 180 
mean value theorems, 180 
and affine approximations, 330 
basic, 329 
local, 329 
on convex sets, 329 
review of, 329 
uniform, 329 
Monotone Convergence Theorem, 42 
motions, 186 
rigid, 186 
motions of Euclidean spaces, 186 
multilinear functions 
compositions of operators as, 92 
spaces of, 90 
symmetric, 91 
spaces of, 91 
symmetric parts of, 91 
multilinear functions, 88, 219 
alternating, 115, 439 
continuity of, 145 
derivatives of, 219 
multilinear transformations 
increments of, 140 
norms of, 138 
negligible sets, 296 
neighborhoods, 158 
nested intervals, 59 
Newton's iteration method, 231 
relation to inverse function theorem, 231 
nondifferentiable function 
with directional derivatives, 201 
normal spaces, 239, 250 
equations of, 240 
of graphs, 239 
normed space 
distance between sets in, 168 
normed spaces, 124 
Bolzano-Weierstrass theorem in, 132 
boundaries of complementary sets in, 
159 
boundaries of sets in, 159 
bounded sequences in, 129 
bounded sets in, 129 
closed sets in, 160 
compact sets in, 148, 164 
and continuous functions, 166 
continuity in, 142 
convergent sequences in, 128 
equivalence of norms in, 130 
Heine-Borel Theorem in, 165 
interiors of sets in, 159 
neighborhoods in, 158 
norms in, 124 
open balls in, 157 
shapes of, 157 
open covers in, 165 
open sets in, 157 
properties of, 158 
sequences in, 128 
triangle inequality in, 126 
uniform continuity in, 167 
and compact sets, 167 
norms 
in Euclidean Spaces, 95 
numbers, 10 
integers, 10 
natural, 10 
rational, 10, 419 
real, 10 
one-to-one correspondence, 24 
open covers, 165 
open sets, 157 
and continuous functions, 162 
in R, 58 
in normed spaces, 156 
operators 
difference, 268, 270 
commutativity of, 270 
linear, 308 
positive, 308 
order axioms, 419 

462 
INDEX 
ordered bases, 440 
orientation-preserving 
charts, 354 
diffeomorphisms, 354 
orientation-preserving diffeomorphisms, 403 
orientations 
global 
on manifolds, 357 
in the classical Stokes' theorem, 413 
local 
on manifolds, 355 
of manifolds, 353 
of tangent planes, 358 
orthogonal complements, 104 
orthogonal decompositions, 104 
orthogonal sets, 97 
orthogonal subspaces, 103 
orthogonality, 97 
orthonormal bases, 97 
existence of, 100 
orthonormal sets, 97 
outer normals 
and volume multipliers, 359 
continuity of, 359 
expressions for, 359 
of oriented surfaces, 358 
pairwise disjoint sets, 5 
Pappus' theorem 
for surface areas, 353 
for volumes, 337 
parametric equations, 245 
equivalent, 248 
local, 246 
of manifolds, 245, 246 
parametric representations 
compatibility of, 248 
for manifolds, 346 
partial derivatives, 205 
higher-order, 210 
partitions, 299 
approximations by, 299 
general, 299 
into binary cubes, 299 
of finite size, 299 
size of, 299 
partitions of unity, 350, 452 
permutations, 436 
and transpositions, 436 
sign of, 436 
plane curves, 190 
points of closures 
for sets in M, 55 
polar coordinates, 20 
polynomials, 86 
derivatives of, 273 
homogeneous, 91 
in vector variables, 91 
positive definite, 282 
second degree homogeneous, 86 
sequences of, 273 
series of Taylor, 279 
Taylor, 182, 183, 279 
derivatives of, 184 
positive definite polynomials, 282 
principal normal vector, 187 
product spaces 
integrals in, 312 
products 
differentiation of, 218 
projections in vector spaces, 71 
pullbacks 
and exterior derivatives, 399 
of tensor fields, 370 
Q, 10 
R, 10 
range of a linear transformation, 69 
rational numbers, 10, 419 
real numbers, 10, 31 
absolute values of, 34 
arithmetic operations in, 423 
completeness of, 37, 427 
density of Q in, 33 
density of irrational numbers in, 38 
incompleteness of Q in, 37 
irrational, 38 
order relations in, 32, 424 
positive, 32 
upper bounds in, 428 
regions under graphs, 303 
regular neighborhoods, 402 
boundaries in, 402 
on manifolds, 412 
regular sets, 402, 403 
almost, 407 
Stokes' theorem for, 409 
Stokes' theorem for, 405 
relations, 7 
equivalences, 7 
equivalence classes, 8 
representatives, 8 
reflexivity, 7 

INDEX 
463 
symmetry, 7 
transitivity, 7 
restricted derivatives, 195 
reverse charts 
for manifolds, 346 
Riemann condition 
for integrability, 305, 306 
right-hand rule, 358 
for orientations of surfaces, 358 
rigid motions, 186 
about a fixed point, 189 
rotations, 188 
rules of differentiation, 211 
self-adjoint transformations, 111 
sequences, 18 
subsequences, 45 
bisection, 59 
in Cartesian product spaces, 132 
of polynomials, 152, 273 
sequences in normed spaces, 128 
bounded, 129 
Cauchy, 133 
convergent, 128 
sequences of real numbers, 40 
bounded, 40 
Cauchy, 46 
convergent, 40 
decreasing, 42 
increasing, 42 
limits of, 40 
monotone, 42 
unbounded, 40 
zero, 41 
series 
Taylor, 185, 279 
series of real numbers, 50 
absolute convergence of, 51 
convergence of, 50 
ratio test for, 52 
root test for, 51 
tests for, 51 
sets, 3 
almost regular 
Stokes' theorem for, 409 
Cartesian products of, 5 
complements of, 5 
elements of, 4 
emptyset, 4 
enlargements of, 375 
general intersections of, 6 
general unions of, 6 
intersections of, 5 
Jordan, 290, 295 
members of, 4 
negligible, 296 
pairwise disjoint, 5 
points of, 4 
power set of, 4 
proper subsets of , 4 
regular, 403 
almost, 407 
subsets of, 4 
symmetric difference of, 5 
unions of, 5 
span, 63 
spans 
subspaces and, 64 
spectral theorem, 109-113 
for self-adjoint transformations, 112 
spherical coordinates, 21, 233 
and diffeomorphisms, 233 
standard basis in Mn, 64 
step functions 
approximations by, 305 
Stokes' theorem, 381 
basic, 381, 384, 394 
in tensor form, 400 
classical, 413 
in tensor form, 414 
in vector form, 414 
for almost regular sets, 409 
for regular sets, 405 
general, 409 
on manifolds, 412 
subspaces, 63 
intersection of, 64 
spans and, 64 
sums of, 64 
volumes on, 343 
sums 
lower, 306 
upper, 306 
supremum, 39 
surface-area 
upper, 407 
surfaces, 244, 357 
orientations of, 357 
symmetric difference of sets, 5 
tangent planes 
orientations of, 358 
tangent spaces 
and derivatives, 346 

464 
INDEX 
equations of, 238 
of graphs, 238 
of manifolds, 249 
properties of, 250 
tangent vector, 187 
tangent vectors 
of curves, 250 
on a manifold, 250 
Taylor polynomials, 182, 183, 279 
approximation by, 184 
derivatives of, 184 
series of, 279 
Taylor series, 185, 279 
approximation by, 185 
tensor fields, 366 
integrals of, 366 
computations of, 367 
pullbacks of, 370 
related to vector fields, 368 
term-by-term differentiation, 275 
ternary expansions, 54 
tetrahedra, 302 
volumes of, 318 
topological concepts, 158 
topological spaces, 158 
topology, 156 
in R, 58 
in normed spaces, 156 
of a vector space, 158 
of Jordan sets, 298 
torsion, 188 
total differentials, 215 
trace 
of a transformation, 448 
of linear transformations, 382 
translations 
of Jordan sets, 324 
volumes under, 324 
transpositions 
and permutations, 436 
triangle inequality 
between real numbers, 34 
in Euclidean Spaces, 96 
in normed spaces, 126 
the reversed, 126 
uniform continuity, 150, 167 
uniform convergence, 150 
and continuity, 150 
unions of sets, 5 
upper approximations 
supports of, 307 
upper bounds, 36 
least, 36 
uniqueness of, 37 
upper integrals, 306 
upper sums, 306 
variables 
change of, 328 
variations 
of cross-sections, 301 
vector 
binormal, 187 
gradient, 208 
principal normal, 187 
vector field 
curl of a, 415 
vector fields, 361 
flux of, 392 
integrals of, 361 
line integrals of, 361 
computations of, 362 
on manifolds, 412 
related to tensor fields, 368 
surface integrals of, 363 
computations of, 363 
vector spaces 
bases in, 64 
bilinear functions on, 82 
Cartesian products of, 67 
complementary subspaces in, 66 
complementary projections in, 73 
coordinate systems in, 74 
dimension of, 65 
dimension theorem in, 65 
dimensions of, 431, 434 
direct sums of, 66 
finite dimensional, 65 
images of subspaces in, 69 
inner products on, 95 
bilinear functions and, 95 
isomorphisms between, 75 
linear combinations in, 62 
linear functions between, 68 
linear maps between, 68 
the space of, 68 
linear transformations between, 68 
linearly independent sets in, 64 
projections in, 71 
scalar multiplication in, 62 
span of a finite set in, 63 
subspaces in, 63 
vector addition in, 62 

velocities, 186 
in flows, 386 
in motions of Euclidean spaces, 186 
initial 
in flows, 387 
velocity fields, 386 
volume multipliers, 117, 325 
and outer normals, 359 
determinants as, 326 
general, 345 
of isometries, 325 
of isomorphisms, 325 
of linear maps, 344 
of scaling transformations, 325 
volumes, 290 
additivity of, 292 
change of, 332 
Euclidean, 340 
in Cartesian products, 294 
inner, 295 
lower-dimensional, 343 
monotonicity of, 293 
of cones, 318 
of cubes, 292 
of cylinders, 318 
of ellipsoids, 319 
of Euclidean balls, 317 
of tetrahedra, 318 
of unions of cubes, 293 
on subspaces, 343 
outer, 295 
well-ordering axiom, 13 
Z, 10 
Z+, 10 

