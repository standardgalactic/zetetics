Pretraining Data Mixtures Enable Narrow Model Selection
Capabilities in Transformer Models
Steve Yadlowsky, Lyric Doshi, Nilesh Tripuraneni
{yadlowsky, lyric, nileshtrip}@google.com
Google DeepMind
November 3, 2023
Abstract
Transformer models, notably large language models (LLMs), have the remarkable ability to
perform in-context learning (ICL) – to perform new tasks when prompted with unseen input-
output examples without any explicit model training. In this work, we study how effectively
transformers can bridge between their pretraining data mixture, comprised of multiple distinct
task families, to identify and learn new tasks in-context which are both inside and outside the
pretraining distribution. Building on previous work, we investigate this question in a controlled
setting, where we study transformer models trained on sequences of (x, f(x)) pairs rather than
natural language.
Our empirical results show transformers demonstrate near-optimal unsu-
pervised model selection capabilities, in their ability to first in-context identify different task
families and in-context learn within them when the task families are well-represented in their
pretraining data. However when presented with tasks or functions which are out-of-domain
of their pretraining data, we demonstrate various failure modes of transformers and degrada-
tion of their generalization for even simple extrapolation tasks. Together our results highlight
that the impressive ICL abilities of high-capacity sequence models may be more closely tied to
the coverage of their pretraining data mixtures than inductive biases that create fundamental
generalization capabilities.
1
Introduction
One of the impressive capabilities demonstrated by large language models is their ability to do
few-shot learning by providing examples in-context and asking the model to generate a response
to follow the final input provided [3]. Researchers have taken the underlying machine learning
technology, transformer models, and demonstrated that they can perform also in-context learning
tasks in domains other than language [4, 1, 5].
In these works, they demonstrate the ability
of transformers to learn high-dimensional and non-linear functions of the inputs from in-context
examples, often matching or exceeding the performance of state-of-the-art machine learning models
tuned for the purpose of learning those functions. For example, Garg et al. [4] showed that after
pretraining on sparse linear data, a transformer network can in-context learn unseen sparse linear
functions as well as the Lasso, which is known to be statistically optimal for data being modeled.
In these models, as in the large language models that they are designed to reflect, pretrain-
ing (or fine-tuning) the model with relevant data to teach the model how to perform in-context
learning is critical to enabling this capability. In this work, we focus in on a specific aspect of
1
arXiv:2311.00871v1  [cs.LG]  1 Nov 2023

this pretraining process—the data used in pretraining—and investigate how it affects the few-shot
learning capabilities of the resulting transformer model.
The few-shot learning setup that we study follows Garg et al. [4], where the goal is to use
a set of provided inputs and labels, ((x1, f(x1)), (x2, f(x2)), . . . (xn, f(xn))) to make a predic-
tion about the label f(xn+1) for a new input xn+1. The number of examples provided is small
relative to the amount of data used to pretrain the meta-learning model used to perform this
task (hence the “few-shot” nomenclature).
A common approach to apply sequence models for
few-shot learning is first to pass the examples in sequentially, alternating inputs and labels, as
(x1, f(x1), x2, f(x2), . . . , xn, f(xn+1)). Finally, the test input point xn+1 is passed as the final el-
ement of the sequence, and model prediction for the next item in the sequence is treated as the
predicted label. Previous work [4, 1, 5] shows that transformer models are capable of learning many
types of data distributions for (x, f(x)) pairs and investigate transformer models’ ability to make
such predictions.
Training the model to be capable of such predictions requires fitting the model on many se-
quences of the form si = (x1,i, f(x1,i), x2,i, . . . , xn,i, f(xn,i), xn+1,i, f(xn+1,i)). Each example in the
sequence is drawn using the same function f, and each sequence uses a different function f drawn
from some distribution D(F) over function class F. We investigate interactions between pretraining
data composition and transformers’ abilities to few-shot learn related tasks.
Our contributions are as follows:
• We pretrain transformer models for in-context learning using a mixture of multiple distinct
function classes and characterize the model selection behavior exhibited.
• We study the in-context learning behavior of the pretrained transformer model on functions
that are “out-of-distribution” from the function classes in the pretraining data.
• In the regimes studied, we find strong evidence that the model can perform model selection
among pretrained function classes during in-context learning at little extra statistical cost,
but limited evidence that the models’ in-context learning behavior is capable of generalizing
beyond their pretraining data.
2
Preliminaries
Transformers are sequence models that provide next-token predictions conditional on the previous
sequence tokens. We consider a data-generating model where d-dimensional covariates are drawn
xi ∼N(0, Id) and a (random) function f ∼D(F) is sampled from a distribution over function
classes. Like Garg et al. [4] and Aky¨urek et al. [1], we frame the ICL problem as providing a single
prompt sequence1 s = (x1, f(x1), x2, f(x2), . . . xn, f(xn), xn+1) to the model (i.e. a transformer)
and generating a prediction for f(xn+1): ˜f(xn+1). We refer to the problem of predicting the next
token as in-context learning. The performance of an in-context learner is judged by its predictive
squared-loss E[( ˜f(xn+1)−f(xn+1))2], with the expectation taken over the randomness in the prompt
and query.
Garg et al. [4] demonstrated that by pretraining a transformer model on simulated prompts
represented as such sequences, the model is able to in-context learn unseen functions drawn from
1We assume that the inputs and outputs are all represented as real scalars or vectors. If not in the same dimension,
we can project the lower dimensional items into the higher dimensional space, filling in zeros for the empty dimensions.
2

the same function class at test-time. For example, they demonstrate such pretrained transform-
ers are able to perform as well as state-of-the-art ML methods on linear function classes (with
both sparse and dense coefficient vectors) when pretrained on data generated from linear functions,
decision trees when pretrained on data generated from decisions trees, and ReLU networks when
pretrained on data generated from ReLU networks. Aky¨urek et al. [1] study transformers’ ability
to learn linear models in-context and provide mechanistic interpretations of how they may be per-
forming such learning. Li et al. [5] studies generalization properties of transformers in this setting
and demonstrates similar results for linear dynamical systems. Ravent´os et al. [6] investigates the
role of pretraining function diversity for in-context learning in the setting of pure linear regres-
sion – arguing that a sufficiently diverse distribution over linear tasks in pretraining is needed for
ICL at test-time. Closest to our work on model selection is that of Bai et al. [2], which explores
transformers’ abilities to perform model selection on empirical grounds and provides rigorous the-
oretical guarantees for transformers generalization properties in pretraining and their downstream
in-context prediction performance. The guarantees and empirics in Bai et al. [2] are restricted to
explorations on model selection amongst different linear function class families – namely studying
model selection across evenly weighted task mixtures of linear regression with different label noise
strengths and linear/logistic regression.
In our setting, given a data source D containing sequences si = (xi,1, f(xi,1), ..., xi,n, f(xi,n)),
we pretrain the parameters θ of the transformer model mθ(s) by performing loss minimization on
the “teacher forcing” objective
L(θ) =
1
|D|
X
si∈D
n
X
j=1
ℓ(f(xi,j), mθ(si,1:j)) ,
(2.1)
for the squared-loss ℓ, where we use si,1:j to refer to the i-th sequence up to (but not including)
the j-th output f(xi,j). We use a 9.5M parameter decoder model with 12 layers, 8 attention heads,
and a 256-dimensional embedding space as in Garg et al. [4]. Details on the model and our training
setup are in Appendix A.1.
The focus of this paper is understanding how the construction of the data source D affects the
in-context learning abilities of the model in the controlled setting of learning function classes. In
this case of studying a single function class family as in Garg et al. [4], Aky¨urek et al. [1] and
Li et al. [5], for a linear data-generating model, a single f can be effectively sampled by drawing
β ∼N(0, Id) and defining f(x) = β⊤x for use in a sequence si.
In this work, we use data mixtures which combine together examples generated from multi-
ple distinct function families. We consider several distributions over base function classes D(F):
(a) D(Fdense): dense linear functions, (b) D(Fsparse,nnz): sparse linear functions with nnz non-
zero coordinates, (c) D(Frelu): two-layer ReLU networks, and (d) D(Fsine): sinusoidal func-
tions.
Additional details on how these base distributions over function classes are generated
are provided in the Appendix
A.2.
Mixture distributions over function classes take the form:
D(F) = w ·D(FA)+(1−w)·D(FB) for selected function classes FA and FB. Each training prompt
sequence is constructed by randomly selecting a function class from the mixture based on w and
sampling from that base class. For example, for w = .25 with selected function classes D(Fdense)
and D(Fsparse,2), a sequence si is drawn from D(Fdense) with probability 0.25 and from D(Fsparse,2)
with probability 0.75.
Garg et al. [4] argues transformers generalize well to tasks/function drawn from the same
distribution as the training data. However, one general open question is how these models perform
3

on examples that are out-of-distribution from the training data. In the setting studied here, we
interpret the generalization question as the following: “Can a model generate good predictions with
in-context examples from a function not in any of the base function classes seen in the pretraining
data mixture?” We build to this question by first studying the abilities of transformers to perform
model selection between different function class families seen in pretraining in Section 3. We then
transition to answering the OOD-generalization question for a few important cases in Section 4.
3
Model Selection Phenomena
One question that comes up when pretraining data mixture of different function classes is “how
does the model select between different function classes when presented with in-context examples in
the support of the pretraining mixture?” We address this question here from an empirical point of
view.2 In this section, we find that the models make optimal (or nearly so) predictions after seeing
in-context examples from a function class which is a member of the pretraining data mixture. We
also observe how models perform on functions that are not cleanly part of any single component
function class before exploring some functions that are definitively out-of-distribution from all
pretraining data in Section 4.
We begin with the study of linear functions, which have received a significant attention in this
area of in-context learning. Garg et al. [4] show that transformers pretrained on linear functions
perform nearly optimally at in-context learning on new linear functions. In particular, they consider
two models: one trained on dense linear functions (where all of the coefficients of the linear model
are non-zero), and one trained on sparse linear functions (where say only 2 of the 20 coefficients are
non-zero). Each model performs correspondingly as well as linear regression and Lasso regression
on new dense and sparse linear functions, respectively. We additionally compare these two models
to a model pretrained on a mixture of both sparse and dense linear functions.
Figure 1 shows that the model pretrained on a D(F) = 0.5·D(Fdense)+0.5·D(Fsparse,2) mixture
performs similarly at in-context learning as models pretrained on only one function class. Since the
model pretrained on the mixture performs similarly to the models shown by Garg et al. [4] to be
theoretically optimal, we infer that this model is nearly optimal, as well. The ICL learning curves
in Figure 2 show us that this in-context model selection ability is relatively uniform with respect
to the number of in-context examples provided. In Figure 2, we also see that ICL learning curves
for pretraining data mixtures with various non-trivial weight w (or 1 −w) for a given function
class nearly match the optimal baseline sample complexity compared to pretraining a model purely
on that function class. We observe only small deviations and these decay quickly as ICL sample
count increases, matching the behavior in Figure 1 which corresponds to a single point on the ICL
learning curve. Figure 2 also demonstrates that transformer model ICL generalization suffers out-
of-distribution. Even though dense and sparse linear classes are both linear functions, we can see
the poor performance of the red curve in Figure 2a (which corresponds to a transformer pretrained
on only sparse linear functions and evaluated on dense linear data) and vice-versa for the teal
curve in Figure 2b. We see similar behavior for other nonlinear function classes, as detailed in
Appendix B. We additionally briefly explore the effect of model size on model selection in the later
part of Appendix B.
2The mechanistic question about how these empirical phenomena come to be is interesting, but we do not address
it here. We believe that first clearly documenting the current phenomena is important for the research community.
4

Sparse (NNZ = 2)
Weakly Sparse (NNZ = 10)
Dense (NNZ = 20)
Evaluation Function Class
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Mean Squared Error
0.002
0.351
0.554
0.237
0.237
0.225
0.004
0.236
0.225
Pretrained On
Sparse
Dense
Mixture
Figure 1. Comparison of models with three different pretraining data compositions (denoted in bar
colors), on three different evaluation functions passed in-context (denoted in x-axis groupings). In
this case mixture distribution in pretraining is evenly weighted with w = .5 in D(F) = w·D(Fdense)+
(1 −w) · D(Fsparse,2). 16 in-context examples provided to all models, with a input data dimension of
20, so that exact recovery of dense coefficients is not possible.
Figure 2. ICL learning curves displaying mean-squared error (MSE) for evaluations on prompts
drawn from D(Fdense) (left) and D(Fsparse,2) (right). Transformers were pretrained on mixtures of
w · D(Fdense) + (1 −w) · D(Fsparse,2). Different curves correspond to different w.
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
Dense=0.0, Sparse=1.0
Dense=0.25, Sparse=0.75
Dense=0.5, Sparse=0.5
Dense=0.75, Sparse=0.25
Dense=1.0, Sparse=0.0
(a) Evaluated on prompts from D(Fdense)
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
Dense=0.0, Sparse=1.0
Dense=0.25, Sparse=0.75
Dense=0.5, Sparse=0.5
Dense=0.75, Sparse=0.25
Dense=1.0, Sparse=0.0
(b) Evaluated on prompts from D(Fsparse,2).
Returning to the experiment in Figure 1 and plotting the error as a function of the number
of non-zero coefficients over the entire range of possibilities shows that the model pretrained on
the mixture with w = .5, D(F) = 0.5 · D(Fdense) + 0.5 · D(Fsparse,2), performs as well as the
models pretrained on the mixture components, ie w = 0 and w = 1, throughout (see Figure 3a).
This suggests that the model is capable of performing model selection to choose whether to make
predictions using knowledge solely from one base function class in the pretraining mixture or the
other. Indeed, Figure 3b shows that when the examples provided in-context come from very sparse
or very dense functions, the predictions are nearly identical to those made by the models pretrained
on either only sparse or only dense data, respectively. In between however, when the number of
5

non-zero coefficients is ≈4, the mixture predictions deviate from both those of the purely dense
or purely sparse pretrained transformer. This suggests the model pretrained on the mixture is not
simply selecting a single function class for the predictions, but predicting something in between.
Figure 3. Comparison of models with three different pretraining data compositions across different
levels of sparsity in the linear function evaluated (i.e. evaluation distribution is D(Fsparse,nnz) for
varying nnz. Evaluated after 16 in-context examples.
0
2
4
6
8
10
12
14
16
18
20
Number of Nonzero Coefficients
0.000
0.005
0.010
0.015
0.020
0.025
Mean Squared Error
Pretrained on
Dense
Sparse
Mixture of Dense and Sparse
(a) Loss of the models compared.
The model
pretrained on a mixture of data performs nearly
as well as the best of the models pretrained only
on one function class or the other.
0
2
4
6
8
10
12
14
16
18
20
Number of Nonzero Coefficients
0.0
0.2
0.4
0.6
0.8
1.0
Normalized Mean Squared Difference
Difference in Predictions
Dense - Mixture
Sparse - Mixture
(b) The mean squared difference (MSD) between
the predictions from model pretrained on a mix-
ture versus sparse- or dense-only. Normalized by
the MSD between the predictions of the sparse
and dense models.
4
Limits of Model Selection Capabilities
We examine the ICL generalization capabilities of models along two axes.
First, we motivate
exploring and test ICL performance on functions the model has both never seen in training and
also could plausibly predict: convex combinations of functions drawn from the pretraining function
classes. Second, we evaluate ICL performance on functions which are extreme versions of functions
seen in pretraining (ie., sinusoids with much higher or lower frequencies than those typically seen in
pretraining). In both cases, we find little evidence of out-of-distribution generalization. When the
function is significantly far from those seen during pretraining, the predictions are erratic. However,
when the function is sufficiently close to the pretraining data, the model approximates it well with
predictions from the function classes on which it was pretrained.
Figure 3a shows that the transformer’s predictions at moderate sparsity levels (nnz = 3 to 7)
are not similar to any of the predictions from either of the function classes provided at pretraining,
but rather, something in between the two. Hence, one may hypothesize that the model has some
inductive bias that allows it to combine pretrained function classes in nontrivial ways. For instance,
one may suspect that the model can produce predictions from the combination of the functions that
it saw during pretraining. To test this hypothesis in a context with clearly disjoint function classes,
we study the ability to perform ICL on linear functions, sinusoids, and convex combinations of the
two. We focus on the one dimensional case to make evaluating and visualizing nonlinear function
classes straightforward.
6

Figure 4. Comparing predictions from models pretrained on different data sources after providing
3 sets of in-context examples (shown in red). Predictions are made by sweeping over x values passed
in as the last element of the sequence after the in-context examples.
2
1
0
1
2
2
1
0
1
2
(a) The models pretrained on
linear or both linear and sinu-
soids make good linear predic-
tions when provided examples
from a linear model.
2
1
0
1
2
3
2
1
0
1
2
3
Pretrained On
Linear
Sinusoids
Both Linear and Sinusoids
In-Context Examples Provided
(b) The models pretrained on si-
nusoids or both linear and sinu-
soids make good sinusoidal pre-
dictions when provided exam-
ples from a cosine.
2
1
0
1
2
2
1
0
1
2
(c) None of the models pre-
dict well when provided exam-
ples from a convex combination
of a linear and cosine functions
(although the linear-only model
approximates the line of best
fit).
Figure 4 shows that while a model pretrained on a mixture of linear functions and sinusoids (i.e.
D(F) = 0.5·D(Fdense)+0.5·D(Fsine)) is able to make good predictions for either of these functions
separately, it is unable to fit a function that is a convex combination of the two3. This suggests
that the interpolation phenomenon shown in Figure 3b for linear functions is not a generalizable
inductive bias for in-context learning in transformers. However, it continues to support the narrower
hypothesis that when the in-context examples are close to a function class learned in pretraining,
the model is capable of selecting the best function class to use for predictions.
Figure 4c showed a specific convex combination of a linear function and a sinusoid. In Figure 5,
we sweep over the relative weights of the linear function and sine wave in the convex combination.
Here, we observe that when the combined function is predominantly from one function class or the
other – i.e. well-approximated by the function classes learned during pretraining – the in-context
predictions are reasonable. However, when both functions contribute significantly to the convex
combination, the model makes erratic predictions not well-justified by the in-context examples.
This shows that the model selection capabilities of the model are limited by proximity to the
pretraining data, and suggests that broad coverage of function space is critical for generalized
in-context learning capabilities.
The previous convex combinations were specifically constructed so that the model had never seen
similar functions in pretraining. Shifting to the scenario where sections of the function class space
3Note here convex combination refers to adding the weighted y-values of the functions. This is distinct from the
weighted mixture distribution used in pretraining which samples either a linear function or sine function for the f
used in each prompt sequence.
7

Figure 5.
Predictions from transformer models pretrained on linear functions (blue), sinusoids
(orange), or both (green) when provided the red examples in-context, coming from a combination of
a linear function and sinusoid, with the relative weights noted in the titles of each plot.
2
1
0
1
2
1.0
0.5
0.0
0.5
1.0
0.1x + 0.9cos(2 x)
a
2
1
0
1
2
1.5
1.0
0.5
0.0
0.5
1.0
1.5
0.2x + 0.8cos(2 x)
2
1
0
1
2
1.5
1.0
0.5
0.0
0.5
1.0
1.5
0.3x + 0.7cos(2 x)
2
1
0
1
2
2
1
0
1
2
0.7x + 0.3cos(2 x)
Pretrained on Linear
Pretrained on Sinusoids
Pretrained on Both Linear & Sinusoids
In-Context Examples Provided
2
1
0
1
2
2
1
0
1
2
0.8x + 0.2cos(2 x)
2
1
0
1
2
2
1
0
1
2
0.9x + 0.1cos(2 x)
are increasingly rare in the pretraining data, we find that the model generalization starts strong and
then degrades dramatically as the tasks become so rare as to be out-of-distribution. Specifically, we
trained models on a mixture of sinusoids with frequencies drawn from a Gamma(6, 1/6) distribution.
This distribution ensures that the average frequency is 1. Frequencies below 0.01 or above 5 are
extremely rare: the probability of a sampling a frequency outside this range is less than 1 × 10−7,
i.e. we expect to see approximately 100 examples out of the 1 billion used in pretraining. Figure 6
shows the predictions from all of the models on a variety of frequencies inside and outside this
range.
8

Figure 6. Predictions from transformer models pretrained on sinusoids (orange), or both sinusoids
and linear functions (green) when provided the red examples in-context, coming from sinusoids of
increasing frequency (noted in the plot title).
1.5
1.0
0.5
0.0
0.5
1.0
1.5
1.0
0.5
0.0
0.5
1.0
Frequency: 0.01
Optimal Recovery
Pretrained on Sinusoids
Pretrained on Both Linear & Sinusoids
In-Context Examples Provided
1.5
1.0
0.5
0.0
0.5
1.0
1.5
1.0
0.5
0.0
0.5
1.0
Frequency: 0.1
1.5
1.0
0.5
0.0
0.5
1.0
1.5
1.0
0.5
0.0
0.5
1.0
Frequency: 0.5
1.5
1.0
0.5
0.0
0.5
1.0
1.5
1.0
0.5
0.0
0.5
1.0
Frequency: 1.0
2
1
0
1
2
1.0
0.5
0.0
0.5
1.0
Frequency: 5.0
5
Conclusion
We have empirically explored the role of the pretraining data composition on the ability of pre-
trained transformers to in-context learn function classes both inside and outside the support of
their pretraining data distribution. We have empirically shown that for task families or function
classes well-represented in the pretraining mixture, cost of selecting the appropriate function class
to use for in-context learning is nearly zero. We next explored generalizability on two scenarios: (1)
We found that the pretrained transformers struggle to predict on convex combinations of functions
drawn from pre-training function classes, and (2) We observed that transformers can generalize
effectively on rarer sections of the function-class space and still break down as the tasks become
out-of-distribution.
An important question is understanding how the observations we make here carry over to tok-
enized models and to questions represented in natural language. We attempted an experiment to
train a tokenized model for the one-dimensional examples presented in Section 4 by binning the
scalar values into buckets, and treating the bucket indices as tokens for the input to a transformer-
based language model. We trained this model for 5M epochs with a cross-entropy loss as typically
used in language models, but were unable to significantly decrease the loss. Understanding the
challenges to training such a model and evaluating whether this framing has different model se-
lection or out-of-distribution generalization properties is important future work. In the natural
language setting, the intuitive notions of how to appropriately define task families (in our case
function classes), precise pretraining mixtures, and convex combinations are all less clear. We be-
lieve bridging the gap between the notions presented here and in language modeling may help to
improve our understanding of the power of ICL and how to effectively enable it.
9

References
References
[1] E. Aky¨urek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-
context learning? investigations with linear models. In The Eleventh International Conference
on Learning Representations, 2022.
[2] Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei.
Transformers as statisticians: Provable
in-context learning with in-context algorithm selection, 2023.
[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,
A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan-
guage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–
1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/
paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
[4] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a
case study of simple function classes. Advances in Neural Information Processing Systems, 35:
30583–30598, 2022.
[5] Y. Li, M. Emrullah Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Gen-
eralization and stability in in-context learning. arXiv e-prints, pages arXiv–2301, 2023.
[6] A. Ravent´os, M. Paul, F. Chen, and S. Ganguli. Pretraining task diversity and the emergence
of non-bayesian in-context learning for regression, 2023.
[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.
org/abs/1706.03762.
10

A
Architecture Training and Data Generation
A.1
Architecture and Training
Following Garg et al. [4]’s approach to convert a sequence of n alternating d-dimensional x values
and 1-dimensional f(x) values into a single embedded sequence for the transformer, we make f(x)
d-dimensional by padding it with 0s, producing a sequence of 2n ”tokens” of xi and f(xi), each of
which is a d-dimensional real-valued vector. We insert a (learnable) linear layer to project each input
into the transformer’s model dimension. Correspondingly, we insert another (learnable) linear layer
to reverse the embedding and produce a d-dimensional output representing x or f(x), depending
on the sequence position. The loss at training time uses the next-token prediction squared loss
computed over alternating positions: that is, the loss is only computed over the xi positions and
ignores the xi+1 value predicted by the transformer structure for each f(xi) in the input. As in
prior work, the model is pretrained on simulated data for different data-generating setups. We
do not fine-tune a pretrained language model and do not train on actual text. Concretely, each
training batch consists of k prompts. Each prompt is a sequence of xi, f(xi) pairs, with the x’s
sampled from DX (i.e. N(0, Id)) and a single f ∼DF per prompt. We use a fixed w per pretraining
run.
We trained a decoder-only Transformer [7] model of GPT-2 scale implemented in the Jax-
based machine learning framework, Pax4 with 12 layers, 8 attention heads, and a 256-dimensional
embedding space (9.5M parameters) as our base configuration [4]. We too set the dimensions of
x as 20 (except when specified otherwise) and used standard cosine positional embeddings in our
model. We trained 1 million steps with a training batch size of 1024. We used the Adam optimizer
with standard values set to β1 = 0.9, β2 = 0.999, ϵ = 10−9 and no weight-decay. We used a linear
ramp-up schedule followed by an inverse-square root learning rate decay. We empirically tuned to
find a learning rate of 1 with 5,000 warm-up steps to be effective. We present our results without
noise in the data generation although find similar results adding label noise.
A.2
Data Generation and Function Classes
Throughout our paper we always use N(0, Id) for the covariate distribution (so each xi ∼N(0, Id)
and consider models in dimension d = 20 (for linear and high-dimensional functions) or d = 1 (for
the 1-dimensional sinusoidal examples). For the function classes considered in the text we use:
• For the class of dense linear functions D(Fdense) we generate a random β ∼N(0, Id) and
define D(Fdense) = {f(xi) : f(xi) = β⊤xi/
√
d}.
• For D(Fsparse,nnz) we consider sparse linear models with the number of non-zero elements
set to a sparsity level denoted by s or nnz. To generate the underlying parameter vector we
randomly sample s coordinates uniformly without replacement from the d = 20 dimensional
support and in each non-zero coordinate generate a random coefficient with distribution
N(0, 1) to create β. We then define D(Fsparse,s) = {f(xi) : f(xi) = β⊤xi/√s}.
• For D(Frelu) we consider two-layer ReLU networks with randomly generated weights. In
particular we generate a first layer weight matrix W ∈Rdh×d where dh = 100 is the hidden
dimension and Wij ∼N(0, 2) and second layer coefficient β ∼N(0, Idh). We then define the
4https://github.com/google/paxml
11

function class as D(Frelu) = {f(xi) : f(xi) = β⊤σ(Wxi)/(√d · dh)}, where the activation
function σ(·) is taken as the standard ReLU unit.
• To sample f ∼D(Fsine), first, we sample ω ∼Γ(6, 1/6), then sample β ∼N(0, 1), and let
f(x) = β cos(2πωx).
Note that in each case we define the normalization of each function class such that Var(f(xi)) are
equalized to ensure the norms of the outputs are comparable across different function classes.
B
Additional Model Selection Experiments
We produce the ICL learning curves in Figure 7 for a mixture of dense linear data and data generated
from the ReLU function class to see similar phenomenon: in-context model selection cost is small
relative to the baseline of only training on that single function class. Figure 7 shows this for any
pretraining mixture that has non-trivial weight w (or 1 −w) on the prompts it is evaluated. We
again observe only small deviations and these decay quickly as ICL sample count increases. As
Figure 7. ICL learning curves displaying mean-squared error (MSE) for evaluations on prompts
drawn from Ddense (left) and DReLU (right).
Transformers were pretrained on mixtures of w ·
Ddense + (1 −w) · DReLU. Different curves correspond to different w.
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
Dense=0.0, ReLU=1.0
Dense=0.25, ReLU=0.75
Dense=0.5, ReLU=0.5
Dense=0.75, ReLU=0.25
Dense=1.0, ReLU=0.0
(a) Evaluated on prompts from Ddense
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
Dense=0.0, ReLU=1.0
Dense=0.25, ReLU=0.75
Dense=0.5, ReLU=0.5
Dense=0.75, ReLU=0.25
Dense=1.0, ReLU=0.0
(b) Evaluated on prompts from DReLU
before, transformer model ICL generalization suffers out-of-distribution, i.e. when evaluating on
the function class not seen in pretraining. We do note in Figure 7a the ICL error does improve as
the number of in-context data points increases. This is likely because the ReLU function class can
locally approximate the linear function class for large numbers of samples.
Exploring the effect of model size, we observe the general trend that the ability to perform
model selection increases as models get larger. Additionally, we observe the behavior is asymmetric.
Figure 8a and Figure 9a show that the smallest .2M parameter model performs relatively poorly
while the 1.2M and 9.5M deliver similar performance on this problem. However, Figure 8b exhibits
both a larger variation in performance across sizes and significantly worse performances by .2M
and 1.2M vs 9.5M on prompts from Dsparse,nnz=2 compared to prompts from Ddense. Meanwhile,
Figure 9b shows no difference in performance as a function of model size.
We used the same model sizes and architectures as Garg et al. [4]’s Section A.1 with models
Tiny, Small, and Standard.
12

Figure 8. ICL learning curves displaying mean-squared error (MSE) for evaluations on prompts
drawn from Ddense (left) and Dsparse,nnz=2 (right).
Transformers of varying sizes pretrained on
mixtures with w = .5 in w · Ddense + (1 −w) · Dsparse,nnz=2.
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
.2M Parameters
1.2M Parameters
9.5M Parameters
(a) Evaluated on prompts from Ddense
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
.2M Parameters
1.2M Parameters
9.5M Parameters
(b) Evaluated on prompts from Dsparse,nnz=2
Figure 9. ICL learning curves displaying mean-squared error (MSE) for evaluations on prompts
drawn from Ddense (left) and DReLU (right). Transformers of varying sizes pretrained on mixtures
with w = .5 in w · Ddense + (1 −w) · DReLU.
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
.2M Parameters
1.2M Parameters
9.5M Parameters
(a) Evaluated on prompts from Ddense
0
5
10
15
20
25
30
35
40
In-Context Data Points
0.0
0.2
0.4
0.6
0.8
1.0
1.2
MSE
.2M Parameters
1.2M Parameters
9.5M Parameters
(b) Evaluated on prompts from DReLU
13

