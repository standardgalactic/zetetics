Free will: A case study in reconciling phenomenological philosophy
with reductionist sciences*
Felix T. Hong
Department of Physiology, Wayne State University, Detroit, MI 48201, USA
a r t i c l e i n f o
Article history:
Available online 11 August 2015
Keywords:
Free will
Determinism
Quantum indeterminacy
Downward causation
Naturalizing phenomenology
Visual thinking
a b s t r a c t
Phenomenology aspires to philosophical analysis of humans' subjective experience while it strives to
avoid pitfalls of subjectivity. The ﬁrst step towards naturalizing phenomenology d making phenome-
nology scientiﬁc d is to reconcile phenomenology with modern physics, on the one hand, and with
modern cellular and molecular neuroscience, on the other hand. In this paper, free will is chosen for a
case study to demonstrate the feasibility. Special attention is paid to maintain analysis with mathe-
matical precision, if possible, and to evade the inherent deceptive power of natural language. Laplace's
determinism is re-evaluated along with the concept of microscopic reversibility. A simple and trans-
parent version of proof demonstrates that microscopic reversibility is irreconcilably incompatible with
macroscopic irreversibility, contrary to Boltzmann's claim. But the verdict also exalts Boltzmann's sta-
tistical mechanics to the new height of a genuine paradigm shift, thus cutting the umbilical cord linking
it to Newtonian mechanics. Laplace's absolute determinism must then be replaced with a weaker form of
causality called quasi-determinism. Biological indeterminism is also afﬁrmed with numerous lines of
evidence. The strongest evidence is furnished by ion channel ﬂuctuations, which obey an indeterministic
stochastic phenomenological law. Furthermore, quantum indeterminacy is shown to be relevant in
biology, contrary to the opinion of Erwin Schr€odinger.
In reconciling phenomenology of free will with modern sciences, three issues d alternativism,
intelligibility and origination d of free will must be accounted for. Alternativism and intelligibility can
readily be accounted for by quasi-determinism. In order to account for origination of free will, the
concept of downward causation must be invoked. However, unlike what is commonly believed, there is
no evidence that downward causation can inﬂuence, shield off, or overpower low-level physical forces
already known to physicists. Quasi-determinism offers an escape route: The possibility that downward
causation arising from hierarchical organization of biological structures can modify dispersions of
physical laws remains open. Empirical evidence in support of downward causation is scanty but
nevertheless exists. Still, origination of free will must be considered an unsolved problem at present.
It is demonstrated that objectivity does not guarantee scientiﬁc rigor in the study of complex phe-
nomena, such as human creativity. In its replacement, universality and overall consistency between a
theory and empirical evidence must be maintained. Visual thinking is proposed as a reasoning tool to
ensure universality and overall consistency through inference to the best explanation.
© 2015 Published by Elsevier Ltd.
1. Introduction
Western science began as natural philosophy; natural philoso-
phy can be construed as synonymous with natural science. Isaac
Newton's 1687 scientiﬁc treatise bears the title of “The Mathe-
matical Principles of Natural Philosophy (Philosophiae Naturalis
Principia Mathematica).” Newton's crowning achievement helped
fuel the industrial revolution. If the 17th century science can be
called the golden era of physics and astronomy, then the 20th
century science can be called the golden era of reductionist sci-
ences. The advances in molecular and atomic physics as well as in
molecular and cellular biology are just as spectacular. The success
* Dedicated to the memory of the late President Detlev W. Bronk of The Rockefeller University.
E-mail address: fthong@med.wayne.edu.
Contents lists available at ScienceDirect
Progress in Biophysics and Molecular Biology
journal homepage: www.elsevier.com/locate/pbiomolbio
http://dx.doi.org/10.1016/j.pbiomolbio.2015.08.008
0079-6107/© 2015 Published by Elsevier Ltd.
Progress in Biophysics and Molecular Biology 119 (2015) 671e727

bred a side effect: philosophy and natural science are drifting apart
from each other. Objectivity, one of the most cherished values of
Western science, was most readily achieved in the practice of
reductionist sciences because of the deliberate choice of experi-
mental systems with reduced complexity. Objectivity is safe-
guarded by the so-called Scientiﬁc Method. What is the Scientiﬁc
Method, which is taught as part of the initiation rite of modern
science students? Let me cite a somewhat irreverent and cynical
article with unclear authorship from a website1:
In a nutshell, the Scientiﬁc Method is all about formulating
propositions (hypotheses), doing empirical experiments to test
them on the basis of data, and then formulating conclusions
drawn from the data by the process of statistical inference that
might support the hypothesis, contradict the hypothesis, fail to
resolve the hypothesis one way or another, lead to new hy-
potheses (and a new round of funding). The key terms here are
hypothesis, data, inference, conclusion liberally interspersed with
funding, publication, and paperwork.
According to this criterion, Aristotle's physics would not be
qualiﬁed as science. This orthodox view was most prominently and
somewhat viciously unleashed during the so-called Science War in
the 1990s between post-modernist sociologists and those whom I
call science fundamentalists [Gross et al., 1996]. The casualties
include Sigmund Freud's psychoanalysis, Karl Popper's falsiﬁability
argument, Thomas Kuhn's notion of paradigm shifts, and Bayesian
statistics. The alleged common offense was subjectivity or insufﬁ-
cient objectivity.
As reductionist sciences soared, the role of philosophy in
scientiﬁc enterprise diminished accordingly. For me, a casual
conversation with a mild-mannered colleague revealed a view of
what philosophy is from an average scientist's point view: “Phil-
osophical arguments are little more than mere opinions of indi-
vidual philosophers. They discussed and argued, and then went
home with an agreement to disagree.” Even Stephen Hawking
[Hawking, 1993] once derided [science] philosophers as “failed
physicists.”2
In the meantime, the advance in molecular and cellular neuro-
science has reached the stage which makes it ripe to tackle the
“hard problem” of human consciousness, thus bringing reductionist
scientists on a collision course with phenomenological philoso-
phers. Phenomenological philosophy aims at studying human
consciousness as experienced from the ﬁrst-person perspective.
Although it has been practiced for centuries, its modern version
was launched in the early 20th century in the work of Edmund
Husserl, Martin Heidegger, Jean-Paul Sartre, Maurice Merleau-
Ponty and others [Gallagher and Schmicking, 2010]. Recently,
there have been efforts to naturalize phenomenology, i.e., to make
phenomenology attain scientiﬁc rigor, loosely speaking. This trend
brought up the question of whether it is possible to do so. We must
bear in mind that there is a wide gap between the two camps
which grew in part out of the spectacular success of reductionist
sciences. The naturalization attempt could be D.O.A. d dead on
arrival d if one insists upon objectivity as the sine qua non criterion
of natural sciences. On the other hand, there is a legitimate question
as to how subjective human experiences can be studied objectively.
Furthermore, it is also legitimate for anyone to ask the question:
Are reductionist sciences genuinely objective? Science lives in hu-
man consciousness. After all, science is meaningless without hu-
man consciousness to appreciate it, much less to contemplate it,
although there is no reason to believe that physical reality vanishes
upon cessation of consciousness of all humans.
For the time being, reductionist sciences may survive and
ﬂourish without the help of phenomenology or philosophy in
general. But phenomenology cannot afford to contradict reduc-
tionist neuroscience unless independent evidence can be provided
to question the validity of conclusions drawn from reductionist
neuroscience alone. However, it is not really a zero-sum game.
Perhaps, it is possible to reconcile, at least, part of phenomenology
with neuroscience, while leaving the difference as an initiative or
impetus for future progress on both sides.
One subject that is particularly suitable for such a treatment is
“free will.” This is because the phenomenology of free will is
directly challenged by its encounter and confrontation with
reductionist sciences. There is a long history of the conﬂict of
phenomenology of free will with physics. In recent decades, a huge
divide has appeared between philosophers and reductionist
neuroscientists, although both camps have considered free will to
be an important problem. As Balaguer [2014] pointed out, although
most philosohers favored the existence of free will, such consensus
ceased to exist outside of philosophy departments. In recent years,
the opposition has arisen mainly from the camp of neuroscience.
Reconciling the two opposing views must be a daunting task.
As a bystander uninvolved in the dispute initially and as a
reductionist by training and by practice during most of my scientiﬁc
career, I have been intrigued by free will deniers' overt self-
inconsistency in spite of their emphasis on objectivity. Objectivity
is unquestionably a virtue, but it is only a means to an end, whereas
consistency is one of the ultimate goals of all scientiﬁc enterprise. In
addition, as an outsider to begin with, I have never questioned my
initial conviction that I do have free will. Therefore, the dispute
matters to me in regard to my own world view. However, recon-
ciling something subjective with some other thing objective seems
harder than ﬁtting a square peg into a round hole. I began to ponder
what went wrong with objectivity, and I was drawn to the deeper
part of science and its epistemology.
My own casual and unplanned encounter with human creativity
also forever changed my view about subjectivity. In the late 1990s, I
was intrigued by a bright student's inability to solve a problem that
required recombination of facts already known to him d sort of
“putting two and two together.” The answer to this puzzle was an
overnight revelation to me: It dawned on me that he was thinking
like an expert system of digital computers (rule-based reasoning)
when he was supposed to piece together known pieces of frag-
mented knowledge as in a jigsaw puzzle (picture-based reasoning)
[Hong, 1998a; Hong, 2003a]. I was thus prompted to consult the
creativity literature, and I was immensely beneﬁted by introspec-
tive reports by Einstein [Hadamard, 1945], by Henri Poincare
[Poincare, 1908] and by Wolfgang Amadeus Mozart [Holmes, 1991],
who all practiced picture-based reasoning, a.k.a., visual thinking.
Subsequently, a psychiatrist friend warned me about the unreli-
ability of subjective accounts or introspections. Pitifully, it was a
warning coming too late, because I had unwittingly violated the
psychology taboo and my “eureka” moment had already been a
thing of the past; I simply could not unlearn my newly gained
insight and forget the sweet taste of violating a scientiﬁc taboo. This
episode served as a wake-up call for me to re-evaluate my con-
ventional view about subjectivity. I came to seriously doubt
whether the psychology of human creativity, as practiced in
contemporary academia, is really objective as claimed [Hong,
2013a; 2013b]. This latter observation calls for reconsideration of
what makes natural sciences scientiﬁc. The above-quoted Scientiﬁc
1 http://www.phy.duke.edu/~rgb/Philosophy/axioms/axioms/node44.html.
2 Hawking's remark was made at a weak moment when he felt besieged because
of his critics' tactic of “refutation by denigration.” Perhaps it should not be
construed as Hawking's judgment on philosophy or philosophers in general at a
calm and serene moment.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
672

Method itself may not be as scientiﬁc as we have been led to
believe.
I have also observed that improper uses of natural language
have been responsible for confusions or fallacies about human
creativity. Francis Bacon was apparently aware of the pitfalls. He
wrote, in Novum Organum (Book 1, Sec. 43, Idols of the Market Place)
[Anderson, 1960]:
For it is by discourse that men associate, and words are imposed
according to the apprehension of the vulgar. And therefore the ill
and unﬁt choice of words wonderfully obstructs the understanding.
Nor do the deﬁnitions or explanations wherewith in some
things learned men are wont to guard and defend themselves,
by any means set the matter right. But words plainly force and
overrule the understanding, and throw all into confusion, and
lead men away into numberless empty controversies and idle
fancies. (Italics added)
Reductionist scientists who are obsessed with objectivity are
likely to ignore messages embedded in semantics since logic is
essentially syntactic in nature. Objectivity also dictates that
emotional factors be excluded from scientiﬁc deliberation. But this
cautious attitude should not prevent an investigator from using a
cool head to analyze someone else's emotional expressions. In fact,
by paying attention to Galileo's emotional outburst, I was able to
reveal a secret which had been buried for over four hundred years,
thus leading to a different interpretation of Galileo's inner thoughts
or, rather, unconscious thoughts or gut feelings which he had never
quite articulated [Galilei, 1610; Hong, 2013b] (see also Sec. 5).
To some free will deniers, free will must be one of the most
puzzling paradoxes. From a logical point of view, they have found
free will to be an illusion. Yet, their writings have often betrayed
their unshakable secret belief in free will. It was as if there were two
selves: the intellectual and conscious one declared the non-
existence of free will, but the deep-seated inner feelings betrayed
the confusion. Often the confusion stems from the inability to jump
outside a box which is prescribed or dictated by authorities. The
Renaissance took us out of the control of the Church Establishment
and the threat of Grand Inquisitions. Ironically, the success of sci-
ence and proliferation of specialties have necessarily bred many
experts who are authorities in their respective specialties and in
effect impose an orthodox position d a box to conﬁne our
thoughts. Through prolonged and repeated mutual brainwashing,
superﬁcial consensus can be established within a given discipline.
However, consensus between different disciplines is non-existent
because of the compartmentalization effect, which results in a
lack of cross-fertilization.
When a clash occurs between authorities in phenomenological
philosophy and authorities in reductionist sciences, one must sus-
pect that something hidden in assumptions may have gone wrong.
Whereas it is possible that both camps can be wrong, it is unlikely
both camps are right. I have found that the quickest way to jump
outside an authority-instigated box is to search for undeclared or
hidden assumptions that have so far masqueraded as indisputable
facts or self-evident inferences [Hong, 2013a, 2013b]. In doing so, I
shall exercise a healthy level of skepticism in regard to experts'
foregone conclusions. This skepticism can be justiﬁed by reference
to a report by Cicchetti [Cicchetti, 1991], who found that in judging
controversial topics experts are not reliable judges.
In asking the question of whether phenomenological research
can be made scientiﬁc, we need to seriously revisit the issue of
what makes natural sciences scientiﬁc. In quest of elucidating this
question, I have found new relevance in Karl Popper's falsiﬁability
argument [Popper, 1934] as well as in Charles Sanders Peirce's
notion of inference to the best explanation and the concept of
abduction d syllogism of backward reasoning from the conclusion
to hypotheses, so to speak d as opposed to the traditionally cher-
ished logical (deductive) reasoning [Peirce, 1931e1958].
In this article, I shall ﬁrst make an attempt to reconcile phe-
nomenology of free will with relevant ﬁndings in reductionist sci-
ences, i.e., physics and neuroscience. Here it is appropriate and
justiﬁed for the present author to show his color at the very outset. I
believe in the existence of free will, albeit for ostensibly nonsci-
entiﬁc reasons. I use the word “believe” instead of “claim” or
“assert,” much less “demonstrate” or “prove.” The author sincerely
requests the readers not to let their judgments be clouded by the
author's admission of a nonscientiﬁc belief. I will avoid letting my
own prejudice stand in my way of judgment.
In my naïve deﬁnition, phenomenology is the study of humans'
subjective feelings. I will not bombard novices of phenomenology
with a plethora of terminology, which I either do not know or do
not ﬁnd helpful in the present analysis. But I will cite a plethora of
actual examples in reductionist sciences in the hope of convincing
the skeptics once and for all, in particular with regard to physical
indeterminism and biological indeterminism. I will then critically
assess
the
concept
of
downward
causation,
an
important
phenomenology-instigated concept, in an even-handed fashion.
Finally, I will attempt to make a proposal for naturalizing phe-
nomenology. Again, I deﬁne naturalization without making any
ideological afﬁliation with pioneers of phenomenology. My naïve
deﬁnition of naturalization means two things: reconciling phe-
nomenology with reductionist sciences and making phenomeno-
logical inferences scientiﬁc.
2. The enigma of free will
Few topics are more controversial than free will; some have
called it an illusion whereas others have treated it as indisputable
reality. Few topics have intrigued investigators in more disciplines
than free will. Philosophers certainly enjoyed the prime time of
ruminating on the topic perhaps since the inception of the pro-
fession, but it is also a topic that concerns contemporary physicists,
psychologists, cognitive scientists, and molecular and cellular
neuroscientists as well as law-enforcement professionals, ethicists
and theologians.3 The topic of free will positions itself right at the
crossroad of phenomenological philosophy and reductionist sci-
ences, and their opposing views are targets in each other's
crosshairs.
Although it has been done countless times, it is still a chal-
lenging task to deﬁne the term “free will,” easier said than done for
good reasons. Certainly it is almost impossible to deﬁne it with a
single sentence because free will has many attributes. Superﬁcially,
free will translates directly into freedom of action. Colloquially, it
means that I can do whatever I darn please to do, above and beyond
objections or hardship. Realistically, our perceived freedom of
action is much more limited. For example, humans cannot glide in
mid-air without the aid of external equipment, whereas some birds
and insects can. Humans cannot tread water for an extended period
without the aid of external equipment, whereas the common
basilisk lizard (Basiliscus basiliscus; also known as Jesus lizard or
lagarto de Jesus Cristo) can. Humans have no freedom to do so
because human bodies do not have the physical capability to
overcome restrictions imposed by physical laws. Let me not waste
more of the readers' time by attempting to give a detailed, all-
rounded deﬁnition of free will, with little substance. The lengthy
discussion to follow shall deﬁne free will more satisfactorily though
implicitly.
3 Resurrection is meaningless unless free will exists.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
673

Free will and decision making are intimately related, but
decisions can be made without free will, e.g., computer-based
decision making or arbitrary choice. Free will is certainly not the
sole factor in decision making, perhaps not even the dominant
factor some of the time. Actions resulting from a personal decision
are sometimes dominated by coercion, i.e., succumbing to others'
free will. Coercion often comes in the form of temptations and/or
threats d carrots or sticks, so to speak. Many people would suc-
cumb to the lure of money or other external rewards, thus changing
or relinquishing their pursuit of desired goals whereas other
strong-willed people could resist such lure and stick to their guns.
Most people would yield to threat on their own lives, whereas
many early Christians marched to their martyrdom without hesi-
tation. Therefore, free will seems to be obviously at work behind
actions that defy common expectations for sane persons. However,
unpredictability is yet another important attribute of free will since
no external causes are supposed to explain and predict the action.
Yet a rebellious mind that always acts in such a way as to defy
common expectations exhibits a predictable behavioral pattern.
Thus, free will, if it exists, is not the only factor affecting our
decision making. From the point of view of cognitive science, de-
cision making is the result of complicated neural processes, which
involve many parts of the brain and consist of numerous conscious
as well as unconscious processes, and are constrained by both
external factors and internal bodily factors which are not under
conscious control. Free will is the elusive additional factor origi-
nating autonomously from within the self, which makes its presence
apparent by making a difference when it is summed together with
other non-autonomous factors. When a decision is forthcoming
without sign of hesitation or struggle, it may appear to be decisive
from a third-person perspective. But, in fact, free will may actually
play little or no role. Free will becomes singularly important when
all contemplations add up to an ambivalent situation, which Bal-
guer [Balaguer, 2014] aptly called a torn decision. How do we
distinguish a torn decision based on free will from one based on an
arbitrary choice? In other words, at issue here is whether “man is
condemned to be free” to make a well-contemplated torn decision,
or man is condemned to make arbitrary choices so as to evade a
torn decision, or, in despair, “man is condemned to be free” to
decide to settle on arbitrary choices as the last resort (e.g., willingly
delegating the decision to rolling dice). It seems quite hopeless for a
third-person observer to tell them apart by means of any objective
criteria. On the other hand, a report based on the ﬁrst-person
perspective may or may not be reliable either. However, before
one can effectively address these subtle questions, one must make
sure whether there is even room for free will to exist in view of
all these constraints imposed by physical laws and in view of all
underlying neural processes which seem to obey natural laws
faithfully. In brief, for free will to have room to exist, one must
overcome two obstacles: physical determinism and biological
determinism.
The dispute in the conﬂict between determinism and free will
dates back to the time of St. Augustine and perhaps even earlier.
The dawn of the Renaissance and the birth of modern science did
not resolve the conﬂict, but rather exacerbated it. Initially, the
scientiﬁc basis of physical determinism was provided by Newto-
nian mechanics. This version of physical determinism was formally
enunciated by Laplace [Laplace, 1814] in his treatise A Philosophical
Essay on Probabilities (Essai philosophique sur les probabilites) (see
also [Cottingham, 1996], p. 226). Laplace asserted that it is self-
evident that “a thing cannot occur without a cause which pro-
duces it.” He pointed out that, in ignorance of the true cause,
people often link a natural event to ﬁnal causes, if it occurs with
regularity, or to chance (randomness), if it occurs irregularly. He
claimed that this principle of sufﬁcient reason extends even to
actions which people regard as indifferent, such as free will. He
thought that it is absurd to assume free will without a determi-
native motive: an indifferent choice, “an effect without a cause,”
or, as Leibniz said, “the blind chance of the Epicureans.” He thus
dismissed free will as an illusion of the mind. He also claimed that
there is no doubt that “the curve [trajectory] described by a simple
molecule of air or vapor is regulated in a manner just as certain
[deterministic] as the planetary orbits; the only difference be-
tween them is that which comes from our ignorance.” Laplace
asserted that “these imaginary causes d i.e., indifference or
randomness d had gradually receded with the widening bounds
of knowledge.” He was quite conﬁdent that eventually they would
completely vanish with the advent of sound philosophy, and thus
the true but hidden causes would be fully uncovered. Henceforth,
we shall refer to the above claim as Laplace's “hidden cause” doc-
trine. If each move or event is dictated deterministically by natural
laws, there is simply no room for free will to intervene.
The advent of quantum mechanics shook the foundation of
classical physical determinism, and initially it offered a hope of
relief. Werner Heisenberg, one of the cofounders of quantum me-
chanics, was once hailed as the hero who liberated the rest of us
from the straitjacket of classical determinism, but this hope was
soon dashed. Erwin Schr€odinger [Schr€odinger, 1936], the other
cofounder of quantum mechanics, even wrote a two-page Nature
article to dismiss free will as an illusion. In his opinion, quantum
phenomena are irrelevant to life processes. Schr€odinger exerted a
far-reaching inﬂuence on the quantum version of determinism well
beyond the 20th century. Many physicists, such as Koch, continued
to accept Schr€odinger's view in spite of many established counter-
examples which appeared in the latter half of the 20th century
[Koch, 2009].
Eminent scientists and philosophers of the ﬁrst half of the 20th
century were divided with regard to classical determinism. Science
philosopher
Karl
Popper
argued
in
favor
of
indeterminism
[Popper, 1950a, 1950b, 1982].
Neurophysiologist
John
Eccles
thought that “[free will] must be assumed if we are to act as
scientiﬁc investigators” ([Eccles, 1953], p. 272). William James felt
uneasy with the element of chance in what he referred to as soft
determinism, but embraced it anyway because the alternative d
hard determinism d seemed morally unacceptable [James, 1937].
In the second half of the 20th century, the advent of modern
neuroscience at the molecular and cellular level dealt a second
blow to free will advocates. The mysteries of how humans experi-
ence pain and other bodily sensations, how humans see and hear
the outside world, and how humans execute bodily movements
began to be peeled away layer by layer. Even emotion, language
capability, judgment and decision making have their respective
neural loci or nerve centers of control. The body-machine analogy
and, subsequently, the brain-computer analogy seemed to be
realized step by step as more and more details were unraveled.
That the mind has its neural substrates in the brain processes
has become an unchallenged dogma in neuroscience. Thus,
the expressions of the mind appear to be deterministic simply
because they are not arbitrary or random.
So far, the most concrete evidence against the existence of free
will came from a series of experiments done by Libet and co-
workers [Libet et al., 1983; Libet, 1985]. Libet and coworkers
studied an electric signal called readiness potential, which they
interpreted as indicative of the moment of initiation of a willful
action. They showed that this signal preceded the conscious
awareness of a willful action by several hundred milliseconds.
Subsequently, Haynes and coworkers [Soon et al., 2008; Haynes,
2011] experimentally demonstrated that the neural substrates of
decision making occur before the appearance of conscious aware-
ness of decision making by a margin of several minutes. Taken
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
674

together at face value, the experimental work of Libet's group and
of Haynes' group presented “objective” evidence to demonstrate
that the self-conscious awareness of exercising free will is an after-
the-fact report and therefore free will is an epiphenomenon or,
worse yet, an illusion of the conscious mind. At present, the fault
line resides between philosophers and reductionist neuroscientists.
Superﬁcially, if determinism can be debunked, its conﬂict with
free will just vanishes. However, the actual situation has turned out
to be more complicated than that. Some philosophers claimed that
neither determinism nor randomness (indeterminism) is compat-
ible with free will. Others called compatibilists have insisted that
free will exists in spite of determinism. In recent years, interest in
the topic of free will has spilled over to the lay press. A casual Google
search reveals that the topic of free will has also captured the public
fascination, and discussions in lay publications have been just as
lively. For example, Harris [Harris, 2012] wrote a popular book
about free will. But his book also prompted Linetsky [Linetsky,
2013] to write Free Will: Sam Harris Has It (Wrong). Apparently,
free will is not merely a topic of idle academic interest. For some
people, their belief or disbelief in free will occupies a sacred part of
their world view. Unlike ordinary scientiﬁc or philosophical
discourse, challenging others' view of free will can sometimes turn
ugly.
A casual search of recent literature ﬁnds many articles in the
realm of law and ethics. The reason is obvious. If future events are
predestined, what is the point of education, training, and making an
effort or making repentance? If criminals merely act out a script
written long before their birth, why should the criminals be held
responsible for their crimes? Curiously, even the most ardent free
will deniers acknowledge that, for ethical reasons, it is desirable to
believe in the existence of free will in spite of its being just an
illusion d thus reﬂecting free will deniers' ambivalence and
unsettled inner feelings [Wegner, 2002; Gazzaniga, 2011].
For readers unfamiliar with the topic of free will, listing a few
major references is in order. A comprehensive coverage of the topic
can be found in the work of van Inwagen [van Inwagen, 1983]. For
authoritative coverage of determinism, Earman's book A Primer on
Determinism [Earman, 1986] and Bohm's book Causality and Chance
in Modern Physics [Bohm, 1957] are recommended. An earlier
attempt to synthesize phenomenological philosophy and modern
neuroscience is Walter's book Neurophilosophy of Free Will [Walter,
1999]. Walter singled out three important issues concerning free
will: alternativism, intelligibility and origination. I shall demonstrate
that the issue of alternativism and intelligibility can be readily
resolved. But the issue of origination remains intractable. To the
best of my knowledge, no investigators who support the existence
of free will have satisfactorily resolved the issue of origination.
Among more recent works on free will from the perspective of
neuroscience, Gazzaniga's book [Gazzaniga, 2011] Who's In Charge?:
Free Will and the Science of the Brain is recommended. Michael
Gazzaniga, one of major players in split-brain research, raised
intriguing questions about free will on the basis of his ﬁrst-hand
experience in the interpretation of split-brain experiments. Gaz-
zaniga gave an up-to-date review of experiments that cast serious
doubt on the existence of free will. Daniel Wegner, in his book The
Illusion of Conscious Will [Wegner, 2002], presented a comprehen-
sive treatment of the arguments against the existence of free will.
By far the most lucid attempt to reconcile free will with reduc-
tionist neuroscience is Balaguer's book Free Will [Balaguer, 2014].
Although Descartes' dualism is nottaken seriously in lightof modern
neuroscience, Balaguer's treatment of the topic free will was “uni-
versal.” His arguments apply equally well to materialists, on the one
hand, and to believers in spiritualism, on the other hand.
In spite of my confession of being a free will advocate, I will
make an even-handed assessment of the problem from both sides'
point of view. I will adopt an adversarial position in assessing
phenomenology from the point of view of reductionist sciences, but
I will also adopt the same position in assessing reductionist science
from the point of view of phenomenology. This means I must
temporarily forget my own philosophical conviction and scientiﬁc
afﬁliation, and I must switch my positions alternatingly between
reductionist sciences and phenomenology. It is also necessary to
criticize many past and present great scientists or philosophers for
certain particular views. This transgression needs not be regarded
as an act of disrespect for it is possible to admire a scholar for a
particular contribution while criticizing the same scholar for other
misleading views. Adopting this even-handed position is not just a
matter of fairness; viewing the problem from the opposite side
adversarially is the quickest way to expose one's own blind spot.
Furthermore, the ﬁrst step towards reconciliation is to take the
opponents' ideas and interpretations seriously, if not respectfully.
2.1. The conﬂict between free will and classical determinism
The disputes and confusions about determinism have stemmed,
in part, from imprecision inherent in the use of natural language. To
examine the notion of determinism critically, one cannot afford to
subscribe to the customary practice of using imprecise terms such
as hard determinism and soft determinism. Henceforth, let us use
the reasonably precise language which we customarily encounter
in physics and mathematics.
According to classical mechanics, the future position and mo-
mentum of each and every particle can be precisely and uniquely
determined (calculated) if its present position and momentum (or,
equivalently, velocity) are precisely known. Here, the key word is
uniqueness, which means that there is a one-to-one correspondence
between the present conditions and the future ones. In mathematics
jargon, the (temporal) transformation, or (temporal) mapping, from
the present position and momentum to the future position and
momentum is isomorphism d rather than homomorphism. Likewise,
given the present conditions (position and momentum), the condi-
tions of a particle at any given time point in the past can also be
uniquely determined. The latter is implied by the symmetry inherent
in the time-reversal invariance of Newtonian mechanics: time
reversal can be implemented by means of momentum reversal (as
exempliﬁed by running a movie backwards), i.e., the transformation,
v / v, generates the same outcome as time reversal, t / t, does.
The notion of time-reversal invariance is intimately related to
the concept of microscopic reversibility. By envisioning the universe
together with its contents (which includes living human beings) as
a collection of a huge number of particles, all future and past events
can, in principle, be determined from the detailed knowledge of all
present events by calculating the position and momentum of each
and every particle in the future and in the past, respectively.4 For
example, one was contemplating two options of action. Option A is
to go to a place in a different continent for a vacation whereas
Option B is to stay put and continue to work on an on-going project.
One eventually settled for Option A and actually landed in a
different continent now. One then regretted having made the de-
cision and thought that one could have chosen Option B by exer-
cising one's free will. Could one really have had the freedom to
choose the alternative option, or was the imagined or imaginary
alternative choice just one's illusion? According to absolute deter-
minism, one could now use the boundary condition of Option A and
4 Novices who wonder why physics of particles has any causal bearing on the
human body, much less the human mind, are reminded that the human body
consists of a huge number of particles, such as molecules and ions. Therefore, the
human body as a whole is still governed by the laws of physics.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
675

that of Option B to back-calculate and determine the unique Con-
dition A and the unique Condition B, respectively, at a time point
long before one's birth. The notion of one-to-one correspondence
dictates that Condition A be very different from Condition B. By
invoking one's free will, one asserted that one had the freedom
potentially to alter events and situations long before one's birth.
This absurd inference constitutes the well-known conﬂict between
free will and classical determinism. Note that the notion of one-to-
one correspondence precludes debating parties from bending the
logic to uphold their respective preconceived opinions.
As mentioned above, the conﬂict did not simply go away with
the advent of quantum mechanics, along with the well-known
uncertainty principle. Of course, Schr€odinger's strong opinion
about the issue, together with Heisenberg's remarkable silence,
helped seal the verdict. In precise mathematical language, the
similar time-reversal invariance in the Schr€odinger equation was
further pointed out, thus lending additional credence to the prin-
ciple of microscopic reversibility.
In order for free will to actually exist, it is necessary to debunk
both physical determinism and biological determinism. Unless one
subscribes to spiritualism or dualism of mind and body, physical
determinism entails biological determinism. However, debunking
physical determinism alone is insufﬁcient to dispel all doubts,
because the possibility of biological determinism remains even if
physical indeterminism is shown to be valid. For example, a con-
ventional digital computer exhibits absolute functional determinism
(unless it crashes) regardless of whether absolute physical deter-
minism is true or not.
2.2. A grayscale of causality
Determinism is not the only roadblock that obliterates the room
for free will to exist. The opposite situation, randomness, is another
roadblock. Some free will deniers have reasoned: If it is completely
determined it is not free, but if it is completely random there is no
will. This simple reasoning posits a dilemma for free will advocates.
When one encounters a dilemma, common sense calls for
searches of a third alternative. In Sec. 1, I suggested a quick way to
break a deadlock: searching for undeclared assumptions5 [Hong,
2013a, 2013b]. Once we focus our attention on the possibility of
unproven or unjustiﬁed assumptions, our reasoning suggests that
causality may exhibit a gray scale rather than just a dichotomy.
Could there be a situation which is very close to being deterministic
but not exactly deterministic? A real signal with noise does appear
like this, but conventional scientiﬁc practice attributes the noise to
measurement errors and/or uncontrolled interferences from un-
identiﬁed external sources. These uncontrolled interferences are
the “noise” which Laplace claimed to be deterministic signals
controlled by not-yet-known causes. Laplace's explanation sounds
plausible and its credibility has been reinforced by a proponent as
eminent as Laplace (the halo effect, as Kahneman [Kahneman,
2011] put it). Maybe it is about time to be skeptical about Lap-
lace's doctrine. Could noise be part of the manifestation of a natural
law which happens to be not exactly deterministic, just like the
Schr€odinger equation? But Schr€odinger himself bluntly told us that
the “noise” of quantum origin is too small to play a relevant role in
biology. Could Schr€odinger have jumped to conclusions prema-
turely before somebody else d Edward Lorenz d had a chance to
discover chaos?
Newtonian mechanics predicts solar eclipses or even the return
of Halley's comet with spectacular accuracy, but it has almost
consistently made miserable predictions in long-term weather
forecasting. This is certainly not what Laplace could expect or pre-
dict. As is now well known, Lorenz attributed the “difﬁculty” of long-
term weather forecasting to chaos [Lorenz,1963,1993]. Since Lorenz
used deterministic equations to calculate data for weather fore-
casting, the chaos which he discovered is also known as deterministic
chaos. So far no uncertainty in the natural law governing the motion
of gas molecules has been suspected. It was an acceptable expla-
nation, but is it the best explanation? Maybe the uncertainty is not
limited to ascertaining the accuracy of the input parameters. Could
partof the uncertainty be attributed tothe natural law itself? Could a
similar mechanism amplify small uncertainty inherent in the law of
motion to have an effect similar to Lorenz's original Butterﬂy Effect?
This sounds like sheer speculation, which appears unlikely inviewof
the dogma laid down by Laplace and that laid down by Schr€odinger
himself. But Laplace and Schr€odinger never considered the possi-
bility of chaos. Had they known the possibility of chaos, would they
have maintained the respective dogmas attributed to them? Since
the only way to reconcile physics with our subjective feelings about
free will is to consider a gray scale of determinism, let us go back to
the drawing board.
Let us use the mathematical language of “mapping” to describe a
general causal law (control law) without specifying whether it is the
classical version or the quantum version: A set of measurable input
variables is mapped to another set of measurable variables (or pa-
rameters) and let us consider the input-output causal relationship.
Speciﬁcally, we shall consider temporal mapping from input variables
correspondingtoagiventimepointtooutputvariablescorresponding
to another time point, whether it is in the past or the future.
If the input variables are represented by a state-vector (a set of
numbers representing the input condition) at a given time point, a
control law is a mathematical operator that maps this input state-
vector to the output state-vector for a different time point. The
output state-vector can then become the input state-vector for the
next step of mapping. Given a precisely known single-valued input
state-vector at a given time point, a strictly deterministic control
law dictates a sharply deﬁned single-valued output state-vector for
a different time point whether it is the past or the future. The best-
known example of strictly deterministic control laws is Newton's
equation of motion. Given a sharply deﬁned position and a sharply
deﬁned momentum for a given particle, Newton's equation of
motion predicts a single sharply deﬁned set of values for position
and momentum of the same particle at a different time point.
Repeated experimental measurements or observations usually
yield slightly different values that tend to cluster together with the
highest frequency of occurrence at the center. The values are thus
characterized by a mean value and a standard error (or deviation or
variance). Typically, the frequency versus output relationship is a
bell-shaped Gaussian curve (normal distribution).
Regarding the causes of the spread of errors, there are, inprinciple,
two possibilities: (a) attributing the spread to errors in inputs or
outputs (boundary conditions), and/or (b) attributing the spread to
the control law. The second possibility is prohibited by Laplace's
doctrine. However, for the sake of granting the beneﬁt of doubt to a
skeptical inquirer, we shall temporarily ignore Laplace's prohibition
without formally refuting or challenging his doctrine for the time
being.
In Newtonian mechanics, the control law appears to be deter-
ministic. The deviations of an observed output value from that
predicted by Newton's law of motion are thus attributed to (a)
observational errors of the output value due to the limitations of
the instrumental resolution, the observer's skill, etc., (b) uncer-
tainty of the input values, and (c) perturbations from external
contingencies (e.g., perturbations from other planets in the calcu-
lation of the eclipse of the Sun or the Moon as a three-body
5 Arran Garre called it taken-for-granted assumptions (Personal communication,
2013).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
676

problem of the Sun, the Moon and the Earth). The above reference
to Newton's classical mechanics, as an example of deterministic
control laws, is obviously naïve and oversimpliﬁed. John Earman
discussed several examples of deviations of Newtonian mechanics
from strict determinism in his book A Primer of Determinism
[Earman, 1986]. A more rigorous discussion of the topic will be
deferred to Sec. 3.
In biological sciences, investigators have usually developed
phenomenological control laws to describe the input-output re-
lations of measurable parameters without digging into the detail of
the underlying physical and chemical processes of the bodily con-
stituents. Investigators attribute the spread of measured values to
measurement errors since interference from external noise is
commonplace. Of course, on the laboratory bench, these external
sources of interference are eliminated as much as the experimental
conditions allow. This is why these bench-based investigations are
labeled “reductionist” in nature.
Likewise, at a still higher hierarchical level of organization,
social sciences ignore the underlying physiological processes of the
human body and focus on the corresponding phenomenological
control laws. In dealing with these phenomenological laws, the
spread values of measurable parameters from the central mean is
usually quite substantial and few people are surprised or unhappy
with the outcomes. Investigators have an additional source to
which to attribute the measured noise: individual variation of the
human subjects. This practice tacitly accepts the reality of inde-
terministic phenomenological control laws and therefore social
sciences are attached with the label of soft science.
Thus, the phenomenological laws in social sciences constitute a
one-to-many mapping with the highest frequency of the mea-
surements centered at the mean, and the spread around the mean
forming a Gaussian curve. Perhaps the best-known example is the
“Bell Curve” of the IQ (intelligence quotient) distribution. Of course,
from the perspective of Laplace's doctrine, the spread from the
central mean is attributed to hidden causes. In the present case, the
hidden causes are not so hidden; either genetic heritage or socio-
economic status has been cited as the major underlying cause,
depending on one's political orientation [Herrnstein and Murray,
1996]. Of course, part of the spread could still be attributed to
measurement errors. By its design as an individual measurement,
IQ is intended to be different for different individuals, but it is
supposed to be constant over the lifetime of a particular individual.
Thus, in the distribution of IQ among a group of human beings, the
spread means individual variation, whereas the spread of repeated
IQ measurements of the same individual means measurement er-
rors. However, there is an inherent ambivalence in the practice. The
distinction is not always clear-cut.
Likewise, in biological information processing, it may also be
legitimate to make a similar operational choice. However, the
overwhelming
practice
in
biomedical
sciences,
especially
in
reductionist sciences, is to attribute the spread to measurement
errors. Yet the term “ﬂuctuation” is sometimes used to describe the
“noise.” Presumably, the latter practice might be the consequence
of inﬂuences from physical chemistry or thermodynamics. Thus,
the practice of attributing the spread from a central mean either to
control laws or to measurement processes appears to be an oper-
ational choice simply for convenience or by tradition. Thus, in social
sciences, the notion of indeterministic control laws and the practice
of attaching the variance to the laws themselves are not as outra-
geous as it seemed when I ﬁrst raised the possibility for physical
indeterminism in Sec. 2.2.
Now let us get back to physics. If, indeed, the variations of
measured values are not caused by external contingencies or by
measurement errors and/or uncertainties of inputs, the variations
must then be attributed to the control law itself, and the practice is
no longer an operational choice. It then becomes a serious episte-
mological problem. Of course, the variations can always be attrib-
uted to hidden causes and thus indeterminacy of the control law
can always be denied or avoided, or simply explained away. The
distinction between the two epistemological choices is subtle. The
discussion will be resumed in Sec. 3.1.
In the present article, we shall replace the term “error” or
“spread” with neutral terms such as “dispersion” or “variance” to
designate the spread of measured data from the central mean value.
If the dispersion is zero, the control law is said to be absolutely (or
strictly) deterministic, and the frequency versus output curve
yields a delta-function in the mathematical sense, namely, a curve
with inﬁnite height (amplitude) and inﬁnitesimal width but with a
ﬁnite area under the curve.
Biological information processing seldom yields a sharply
deﬁned output value, as is commonly seen in measurements per-
formed in physics. The values may be well deﬁned, but the dis-
persions are far from zero. Thus, the control law is relatively
deterministic if part of the dispersions can be attributed to the
control law alone. This determinism is referred to as relative
determinism, whereas absolute determinism will be treated as
synonymous with strict determinism or William James' hard
determinism. Relative determinism is thus approximately equiva-
lent to what is referred to by William James as soft determinism.
However, there are many shades of softness. As we shall see, some
steps or phases of biological information processing do not yield
single-valued outputs. Instead, their governing control law dictates
a probability density function of the output values which do not
cluster around a central mean but scatter “erratically” over a
considerable range (Sec. 4.3). In this latter case, there are no
meaningful means or averages and the dispersion is “unconven-
tional.” Thus, relative determinism includes both regular control
laws (with a well-deﬁned mean and a non-zero dispersion) and
probabilistic control laws (without a meaningful mean). It is
important to realize that relative determinism does not necessarily
mean weak causality since the control law can occupy any position
on the gray scale of determinism. Thus, on a gray scale from 0 to 1
(with 0 equal to complete randomness and 1 equal to absolute
determinism), relative determinism assumes a value less than 1 but
greater than 0. As we shall see, deviations from strict determinism
may be essential in certain biological information processes, rather
than just an unavoidable inconvenience.
Physical causality has been analyzed by Yates with respect to
brain function [Yates, 1980]. In Yates' terminology, absolute deter-
minism and absolute indeterminism occupy the two extremes on
the gray scale of determinism. I wish to emphasize the possibility of
determinism that is close to absolute determinism but not exactly
there (a position on the grayscale close to 1 but not precisely 1). I
shall call the latter case quasi-determinism. This term is useful so as
to prevent possible misunderstanding owing to imprecise termi-
nology, such as non-determinism or indeterminism.
2.3. One-to-one vs. one-to-many temporal mapping
Tentatively, let us not make a distinction between the classical
and the quantum versions of the law of motion, and consider an
unspeciﬁed physical law (simply referred to as a nondescript
physical control law of motion) which prescribes the future posi-
tion and momentum of a particle from the knowledge of its present
position and momentum. Absolute determinism can be debunked
if either (a) one can demonstrate that the initial values of position
and momentum are not uniquely and sharply deﬁned, and/or (b)
one can demonstrate that the control law is not strictly determin-
istic and thus prescribes subsequent values of position and mo-
mentum with non-zero dispersions.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
677

Superﬁcially, the above-described problem looks somewhat out
of date and, with the advent of quantum mechanics, it should have
been settled almost a century ago. Heisenberg's uncertainty prin-
ciple dictates that both the position and the momentum of a particle
cannot be accurately measured or determined at the same time,
and this implies that absolute determinism can be invalidated by
means of the ﬁrst approach. However, the problem cannot be
considered fully settled for the following reasons. First, there
remain eminent scientists like Einstein who could not quite accept
probability (and randomness) as part of the fundamental physical
laws and sincerely hoped that truly deterministic mechanical laws
would be discovered in the future. Second, the quantum version of
the equation of motion d the Schr€odinger equation d also exhibits
time-reversal invariance. Third, mainstream philosophers hold the
view that quantum indeterminacy at the microscopic level does not
appear to “carry over” to the macroscopic level ([Cottingham,1996],
p. 226). The persistence of this view might have been the conse-
quence of the inﬂuence of Schr€odinger's book What is Life?
([Schr€odinger, 1945], p. 87):
To the physicist I wish to emphasize that in my opinion, and
contrary to the opinion upheld in some quarters, quantum in-
determinacy plays no biologically relevant role in them, except
perhaps by enhancing their purely accidental character in such
events as meiosis, natural and X-ray-induced mutation and so
on d and this is in any case obvious and well recognized.
It is important to realize that this “obvious and well recognized”
inference was made at the time when chaos theory was not known.
It was natural then to consider small differences at the quantum
level to be negligible in view of the general consensus that quantum
mechanics does not invalidate classical mechanics at the macro-
scopic level. This inference is no longer self-evident and must now
be called into question and re-evaluated.
In a discussion of determinism and free will, the notion of in-
determinacy is sometimes confused with the notion of unpredict-
ability or computational irreducibility (cf. [Wolfram, 2002], pp.
750e753). Deterministic chaos is unpredictable and is superﬁcially
indistinguishable from noise. For predictions of future positions
and momenta of a large number of particles, two separate issues are
relevant: (a) the certainty of the initial conditions and (b) the de-
terminacy of the control law. Lorenz's investigation of long-term
weather forecasting shows that the outcome is unpredictable, not
because of the indeterminacy of the control laws (he utilized
classical mechanics in his computation), but rather because of the
inability to ascertain the initial values to a reasonable degree of
accuracy and because of the high sensitivity of the control law to
small differences in initial conditions. As a consequence, a small
difference in the initial conditions can be greatly ampliﬁed with the
passage of time, thus leading to drastically different future out-
comes (the Butterﬂy Effect). In nonlinear dynamics jargon or
complexity theory jargon, a small difference in the initial condi-
tions could subsequently land the trajectory in a very different
(non-contiguous) part of the phase space. This is what deterministic
chaos is all about.
On the other hand, given a hypothetical case in which the initial
conditions can be speciﬁed to an arbitrary degree of accuracy, the
outcome may still not be predictable because the control law is
probabilistic and the dispersion (variance) of the output is too large
to be useful in making accurate predictions of an individual
outcome. This is the situation that we intend to consider here.
In the terminology used by Matsuno [Matsuno, 1989], the
probabilistic control law mentioned above constitutes a one-to-
many temporal mapping, since a sharp initial condition leads to a
time-evolution of a later condition with a nonzero dispersion.
Matsuno pointed out that dynamics [the control law] acts so as to
decrease the number of unconstrained degrees of internal freedom
in motion, but does not completely eliminate them; a small number
of degrees of internal freedom is admitted and tolerated by the
physical law (notion of dynamic tolerance; [Matsuno, 1989], p. 65).
In contrast, Newtonian mechanics is said to be a one-to-one tem-
poral mapping with complete ﬁxedness (certitude) of boundary
conditions, since time-evolution leads to new boundary conditions
with zero dispersions.
In our discussion of absolute determinism, it is not a question of
how accurately one can determine the initial conditions. The
question is whether the initial (or boundary) values are uniquely
and sharply deﬁned and whether the control law speciﬁes a one-to-
one correspondence between those initial values (or boundary
conditions) and future (or past) values. If so, then there is no in-
determinacy of future and past events even if we cannot make
predictions with absolute certainty because of our inability to
specify the initial conditions to the required degree of accuracy (cf.
determinism vs. computability; [Penrose, 1989], p. 170).
If one-to-one correspondence does not hold strictly true, then
an alteration of the present condition does not necessarily imply
that the past event should also be altered, and the conﬂict of free
will and determinism cannot be established by the time-reversal
argument. No matter how small the dispersion stemming from
the control law is, its presence undermines the validity of one-to-
one correspondence, and the correspondence becomes one-to-
many instead. On the other hand, the dispersion of the control
law can be so small as to preserve the predictability over an
extended period without exhibiting chaos. The crucial difference
between one-to-one correspondence and one-to-many corre-
spondence is often lost in discussions using natural language, such
as hard determinism and soft determinism.
Furthermore, even a control law with zero dispersion some-
times gives rise to multiple outputs. As pointed out by Prigogine,
classical mechanics does not always give unique determination of
the future [Prigogine and Stengers, 1984]. In the problem of a
swinging pendulum moving along the arc of a vertically oriented
circle, the highest possible position of the pendulum is a singular-
ity, which leads to two distinct alternative outcomes (one-to-many
correspondence): (a) an oscillating motion, or (b) a circular orbiting
motion. In terms of nonlinear dynamics jargon, the singularity
point is the separatrix separating the two attractors in the phase
space. However, this singularity point cannot be regarded as a true
bifurcation point if strict determinism is adhered to, because a
trajectory that reaches this point will linger forever there in the
absence of noise: It is an impasse or deadlock leading to nowhere. It
is the presence of noise that tips the balance one way or another
and converts the singularity into a true bifurcation. However, the
notion of noise and strict determinism is fundamentally incom-
patible; using both terms in the same context constitutes a blatant
act of self-deception, if not a ﬂagrant act of deception. The other
argument which Prigogine proposed was the lack of complete
freedom to assign arbitrary initial conditions (independent values
to various particles' position and velocity) in light of our knowledge
of the microscopic physics of atoms and electrons. For quite some
time, Prigogine's view constituted the lone voice that questioned
the validity of microscopic reversibility [Prigogine and Stengers,
1984; Prigogine, 1989]. In his book The End of Certainty [Prigogine,
1997], Prigogine ﬁnally unleashed his conviction that physical
indeterminism must be accepted.
The singularity which Prigogine pointed out is but one of the
many examples in which classical mechanics does not yield unique
solutions and classical determinism breaks down under these
special conditions. Earman [Earman, 1986] reviewed the topic
systematically
and
presented
several
examples
in
which
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
678

uniqueness of the initial value problem for some of the most
fundamental equations of motion of classical physics does not hold,
both for cases of discrete particles and for continuous media or
ﬁelds. He stopped short of concluding that such non-uniqueness
implies the fallacy of determinism. He issued the warning but left
the problem open. He also pointed out that the laws of classical
physics place no limitations on the velocity at which causal signals
can propagate, thus permitting the possibility of arbitrarily fast
causal signals in blatant violation of the theory of special relativity.
Of course, the anomaly is a consequence of the structure of New-
tonian space-time. The possibility of arbitrarily fast causal signals is
also necessitated by microscopic reversibility (Sec. 3.2). However,
he made no direct attempt to challenge the compatibility between
microscopic reversibility and macroscopic irreversibility. He also
reviewed determinism in the context of special and general rela-
tivity, as well as quantum mechanics. Walter [Walter, 1999] also
examined two theories of indeterminism based on quantum me-
chanics, the oldest one (by Pascual Jordan) and the most recent one
(by Roger Penrose and Stuart Hameroff), and rejected both as
untenable.
According to Karl Popper's falsiﬁability argument [Popper,
1934], the validity of a physical theory is only provisional, no
matter how well established it is: the possibility that a radically
new and more satisfactory theory of mechanics may become
available in the future cannot be ruled out. If and when this hap-
pens, a discussion of determinism based solely on a particular
theory of mechanics may be invalidated and the whole argument
may have to be sent back to the drawing board. I therefore choose
here to tackle the problem at the epistemological level as well as the
ontological level without specifying a particular deterministic
control law. I will present arguments that address general and
fundamental physical issues, such as microscopic reversibility (Sec.
3.2) and its implications for the notions of macroscopic irrevers-
ibility and time reversal (Sec. 3.3). The analysis will proceed as if
quantum indeterminacy actually played no role in biology, as
claimed by some philosophers and by Schr€odinger himself.
Nevertheless, evidence in support of the relevance of quantum
indeterminacy in biology will also be considered (Sec. 4.5).
2.4. Compatibilists vs. incompatibilists
In free will research, there continue to be two debating camps
regarding free will [Goldman, 1990; Wallwork, 1997; Walter, 1999].
One of them, known as the incompatibilists, proposed a puzzle
similar to what has been described in Sec. 2.1, but without reference
to any kind of mechanics or physics [van Inwagen, 1983]. Let us use
precise language to discuss it. Consider the past event P that took
place before a particular individual's birth (see Fig. 1) and consider
the natural law L that acts on P to determine the unique outcome R.
If the individual had the free will to render R false, then this indi-
vidual should have been able either to render P false and/or to alter
the natural law L. This is because not-R implies that not-(P and L) is
true, which is equivalent to that either not-P and/or not-L is true
(Fig. 1A). This reasoning leads to an absurd inference: An individual
could alter the natural law L and/or an event P that took place
before his or her birth. This argument led incompatibilists to come
to the conclusion that free will and determinism are not compat-
ible. Note that we consider only the one-to-one type temporal
mapping, i.e. absolute determinism.
The other camp, known as the compatibilists, maintains that no
conﬂict exists between free will and determinism. Consider a case
that one faces a choice of two alternative options, A and B. The
irrevocable (ﬁxed) outcome ends up choosing A instead of B, but
the compatibilists maintain that the alternative B could have been
chosen and is actionally possible. The individual thus retains the
freedom to choose even though the choice of alternative B did not
actually materialize. In other words, it was still within one's power
to perform an act even though one did not actually perform it. The
compatibilist's view was apparently unacceptable to Schr€odinger
[Schr€odinger, 1936]. As he pointed out, we tend to feel by intro-
spection that there are several options available for our selection,
although eventually only one of the many options actually comes to
realization. This was the reason why he thought that free will is an
illusion. Since the alternative never happens, the notion of actional
possibility cannot be directly checked with actual experiments for
the simple reason of impossibility of turning the clock back. In
other words, it is an unfalsiﬁable claim as far as experiments are
concerned, but compatibilism is refutable by simple logic due to
internal contradiction.
Compatibilists have managed to escape from the above dilemma
with the following argument [Goldman, 1990]. A person may be
able to bring about a given state of affairs without being able to
bring about everything entailed by it. In other words, for a certain
event P, rendering R false does not necessarily render P false or alter
the natural law L (Fig. 1B). This argument essentially ruins the
assumed absolute determinism because, as Fig. 1B indicates, the
argument implicitly requires that the prior event P together with
the natural law L lead to multiple but mutually exclusive outcomes, R
and
not-R
(one-to-many
correspondence).
The
determinism
implicitly held by compatibilists is a weaker form of determinism;
it is more appropriately termed relative determinism (or quasi-
determinism) rather than absolute determinism (Sec. 2.2). Thus,
compatibilists have inadvertently altered the meaning of deter-
minism implied in the original concept. Still, there is an alternative
but trivial interpretation of Fig. 1B, that is, P and R are not causally
related. However, apparently the latter was not what compatibilists
had in mind. Conversely, the analysis in Fig. 1B indicates that the
alleged conﬂict vanishes if the physical law of motion is quasi-
deterministic (one-to-many temporal mapping). We shall get
back to this possibility later (Sec. 3.1e3.4).
There is more than one form of compatibilism. Immanuel Kant
and David Hume were two compatibilists among Newton's con-
temporaries. They did not make the kind of mistake shown in
Fig. 1B. But they, instead, utilized what I would call interpretive
tricks to get out of the conﬂict. Being more or less Newton's con-
temporaries, they had no access to knowledge of quantum me-
chanics and other knowledge that we now have. Their conformity
to Newton's mechanics is understandable, given the historical
context.
Kant, like most of his contemporaries (including astronomers
and physicists), believed in the truth of Newton's theory. But he
made a distinction between the world of appearance (Nature) d
the world upon which our intellect imposes its laws d and the
world of things in themselves (noumena) [Kant, 1786]. In other
words, he considered Newton's law as a human mental construct,
whereas he called the physical reality itself noumena. Since
noumena might not be knowable, the perceived conﬂict never
occurred. Popper agreed to his mental construct notion, but he
disagreed with Kant's view that Newton's theory is a priori truth
([Popper, 1982], p. 48). Kant's view of science as a human mental
construct is agreeable to me (cf. [Hong, 2013a]), but Kant never
really resolved the conﬂict because he forever postponed the day of
reckoning by asserting that noumena is unknowable. That is why I
thought that he had utilized an interpretive trick as far as the
question of absolute determinism is concerned.
Hume was also a compatibilist of yet still another kind. He
claimed that if we could claim that the action was performed in
accordance with our will, then there is no conﬂict with deter-
minism; he was not concerned with the alternatives that could not
be proven to have existed anyway, thus cleverly evading the
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
679

perceived conﬂict [Hume, 1993]. He also downgraded determinism
to merely a conjecture based on statistical correlation rather than
deeper truth. It is true that correlation is not necessarily causation,
but Newton's law is better than a mere conjecture or just correla-
tion not to be taken too seriously. For this reason, Hume's argument
also belonged to the class of arguments which I call interpretive
tricks.
There are difﬁculties other than the issue of choices. Mathe-
matician David Ruelle [Ruelle, 1991], in his book Chance and Chaos,
dismissed the conﬂict between free will and determinism as a
“false problem.” His main objection was that a departure from
determinism would entail making a decision by ﬂipping a coin.
Ruelle raised the following question: “[C]ould we say that we
engage our responsibility by making a choice at random?” Here,
Ruelle implied that making a choice at random is morally incom-
patible with a responsible choice. He also thought that our freedom
of choice is an illusion, meaning that being responsible severely
restricts one's freedom of choice. In other words, Ruelle claimed
that neither determinism nor indeterminism is compatible with
free will. Ruelle was probably neither the ﬁrst nor the last to raise
this objection. But he so forcefully asserted his view that I think that
his view deserved a serious discussion. Ruelle's point is not trivial,
and we shall return to this point later (Sec. 6.1).
In summary, compatibilism is logically untenable as long as
compatibilists make no challenge to absolute determinism. In order
to make room for free will to exist, the formidable double road-
blocks of physical determinism and biological determinism must be
debunked.
3. Physical determinism vs. quasi-determinism
3.1. Laplace's “hidden cause” doctrine
In scrutinizing the validity of Laplace's “hidden cause” doctrine, I
found that the doctrine can neither be proven nor disproved. In
other words, the doctrine is not a scientiﬁc fact because it is not
falsiﬁable. Rather it is a choice made at the epistemological level. Is
it sound epistemology?
Among the existing arguments against the existence of free will,
Laplace's hidden cause doctrine is the most fundamental. Simply
demoting Newtonian mechanics to the status of a physical theory of
limited or restricted validity is not sufﬁcient to quench skepticism
stemming from free will deniers' forceful arguments and rhetoric.
The ﬁrst order task is to take what we attempt to debunk seriously.
There is some truth to Laplace's claim. For example, conversa-
tions carried on in other phone channels within the same cable are
practically noise to a particular channel we are listening to. More-
over, pseudo-random numbers can be generated by a deterministic
rule [Hayes, 2001]. The digits in the constant p (the ratio of the
circumference to the diameter of a circle) appear erratically and
randomly without an apparent order, but its value is determinis-
tically ﬁxed by a mathematical formula. Chaotic behaviors can arise
from unstable systems governed by deterministic control laws
(deterministic chaos). Things are not always as they seem.
Once I had a friendly debate with a highly talented computer
science student regarding Laplace's claim about noise. I reminded
him that even if humans managed to ﬁnd causes for every known
kind of noise, new noise might possibly be uncovered by future
new equipment with improved sensitivity and resolution. He
responded by essentially citing Laplace's doctrine: That is because
of our ignorance, but we will eventually know the true cause. I then
repeated my argument one more time, and he responded the same
way again. After running through this loop of exchanges twice, I
stopped. Superﬁcially, I acquiesced, but actually it had dawned on
me that the loop of the argument and the counter-argument could
be recursively iterated ad inﬁnitum. The simple corollary is that
Laplace's doctrine can neither be proven nor disproved. An exam-
ination of the ﬂowchart shown in Fig. 2 explains why.
Given the observation of a new phenomenon, the ﬁrst step is to
establish a viable theory or explanation. If any dispersion is found in
the measured value of a physical variable which the proposed
theory predicts, one can invoke Laplace's “hidden cause” doctrine
to question whether the dispersion is true noise or just the mani-
festation of an unknown deterministic cause. One then engages in
the search for a hidden cause that can explain the dispersion. If such
a cause is not found, the old interpretation is retained but the
search for the hidden cause continues. If a hidden cause is found,
one must then revise the theory to include a newly found expla-
nation of the recently discovered former dispersion. One then must
re-examine the problem to see whether there is any residue of
dispersion that remains to be accounted for. If so, one invokes
Laplace's doctrine again. When no more dispersion is found even-
tually, the search stops. However, that does not prove Laplace's
doctrine because, if new dispersions are found because of im-
provements of the measurement techniques or enhanced sensi-
tivity and resolution of new instruments, the entire process must
start all over again.
It appeared that Laplace might have arrived at his conclusion by
induction; his claim was based on numerous successful predictions
of Newtonian mechanics. However, it is now widely recognized
that it is not possible to prove a general proposition by means of
induction [Popper, 1934]. Just because the hidden cause of most
known dispersions was found in the past is no guarantee that the
hidden cause of a newly discovered dispersion will deﬁnitely be
found in the future. On the other hand, just because a hidden cause
could not be found after extensive searches by many of the most
competent investigators, living or dead, is no good reason to believe
that it will never be found in the future.
There are two loops in the ﬂowchart. When a hidden cause is
found, a major advance in our understanding is made (outer loop).
If a new hidden cause is not found, then one travels around the
inner loop an indeﬁnite number of times. Thus, Laplace's doctrine
cannot be refuted because the number of times one is required to
tread the two loops is inﬁnite. On the other hand, Laplace's doctrine
cannot be proved either because it would require inﬁnitely many
successful examples of ﬁnding a hidden cause and the search for
successful examples should continue till eternity; the possibility
that new dispersions will appear in the future as a consequence of
Fig. 1. The conﬂict of free will and classical determinism. (A) The past event P, together
with natural law L, caused the result R to happen. Suppose that R were rendered false
by means of free will. This could happen only if (a) the natural law L could be altered,
or (b) the past event P could be rendered false, or (c) the natural law L could be altered
and, at the same time, the past event P could be rendered false. These conclusions are
all absurd. This is the incompatibilist view. (B) The compatibilists claim that even
though free will could have rendered R false, not every past event P that had led to R
could be rendered false, and therefore the alleged conﬂict does not exist. However, the
diagram shows that the event P could cause two mutually exclusive outcomes: R and
not-R. Therefore, either the determinism is not absolute, or there is no cause-effect
relationship between P and R to begin with. The compatibilists inadvertently invoked
indeterminism. (Reproduced from [Hong, 1998b]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
680

improved techniques or enhanced instrumental capabilities cannot
be excluded ahead of time.
A scientiﬁc proposition must be falsiﬁable by empirical evidence.
A proposition that can neither be proven nor disproved is not a
scientiﬁc problem but, instead, an epistemological problem. The
choice is between absolute determinism, which outlaws all noise,
and relative determinism or indeterminism, which allows endog-
enous noise to exist while pledging to track down all pseudo-noise
and identify the relevant physical law. Since the choice of either of
the two alternatives appears to be equally acceptable, the choice
should be considered epistemological rather than scientiﬁc; but, as
history transpired, Western science settled for absolute deter-
minism in physics.
Physics as practiced by Galileo and Newton has strived for truths
that explain natural phenomena with well-deﬁned natural laws,
i.e., control laws that yield single-valued predictions. Any observed
dispersion was then treated as the result of the random distribution
of known but uncontrolled variables (which were ignored) or un-
known variables (which hopefully were randomly distributed and
evenly dispersed across the two sides of the mean value). Statistical
analysis of ensemble-averaged or time-averaged data then took
care of the dispersion problem. Judging from Laplace's writing, it
appeared that the spectacular prediction of the 1759 return of
Halley's Comet had such an enormous impact on Laplace's opti-
mism about future reduction of our ignorance that he embraced
absolute determinism with conﬁdence, which seems unjustiﬁed in
hindsight of the 21st century. The choice of absolute determinism
was subsequently reinforced by the spectacular and continuing
success of physics. This was a positive feedback d self-reinforcing
and self-perpetuating d mechanism in action. Probabilistic con-
trol laws, such as what governs the beta decay, were too rarely
observed and were discovered much too late to make a signiﬁcant
impact on the chosen epistemology, and, therefore, failed to seri-
ously challenge the universal validity of absolute determinism.
With an enormous momentum already gathered by advocates of
absolute determinism, the dissident voices such as Prigogine's were
simply swamped out.
Laplace's choice inadvertently put a roadblock in the way of the
elucidation of the free will problem, but there was a valuable
consolation prize. The diagram in Fig. 2 shows that Laplace's doc-
trine served as a moving or, rather, receding target to lure science to
continually seek advances. Supposing that absolute determinism
were rejected in the very outset, too many unknown phenomena
involving noise would have been easily explained away and readily
swept under the carpet by dismissing any dispersion simply as
(true) noise which deserved no further study; physics and chem-
istry would not have become what they are today. Apparently, this
is what Earman meant by the importance of determinism as a
guiding methodological principle in the development of modern
physics ([Earman, 1986], p. 243). So we should all be grateful for
Laplace's choice.
Thus, the true signiﬁcance of Laplace's doctrine is the role it
plays as the devil's advocate. It is like a carrot hung in front of a
donkey at a ﬁxed but unreachable position: in striving to reach the
impossible goal the donkey makes “advances” anyway. Likewise, in
our collective attempt to explain free will in terms of physics and
chemistry, we inevitably fail so far, but we may gain additional
insights into biological information processing and make some
progress.
There might be another reason why Laplace chose absolute
determinism. In his treatise Essai philosophique sur les probabilites
[Laplace, 1814], Laplace expressed his discomfort about leaving a
process of decision making to “the blind chance of the Epicureans”
and
“without
motives.”
However,
Laplace's
choice
did
not
completely
resolve
the
difﬁculty,
for
a
motive
that
is
preprogrammed before one's birth is no motive at all. This dilemma
was enunciated by William James [James, 1937], and it was shared
by many others. It seems that the only way out of James' dilemma is
to adopt a gray scale of determinism and replace Laplace's absolute
determinism with quasi-determinism. In doing so, decision is no
longer based on pure blind chance, but, instead, it is based on
restricted or constrained freedom regulated by a quasi-deterministic
law of motion: the “mean” of an action does not violate the
deterministic law of motion, thus asserting the apparently decisive
will, but the limited dispersion affords room for (limited) freedom of
action (cf. Matsuno's notion of dynamic tolerance). Laplace's
concern about events without a cause thus evaporates and James'
dilemma also vanishes. But not so fast. Subtle objections need to be
taken care of (see Sec. 6.1 for resumption of discussion).
The down side of Laplace's doctrine is that it refuses to even
consider the possibility of the existence of true noise, as a matter of
policy. Every time a seemingly impeccable candidate for true noise
comes up, the doctrine always defers the ﬁnal judgment to the
future, thus forever postponing the settlement. Laplace's doctrine
enjoyed the so-far-so-good status for an extended period. However,
is it about time that this doctrine has reached its end of usefulness
and it shares the same fate as the claim that the world is ﬂat?
It is therefore a relief to ﬁnd that Laplace's doctrine is not really
immutable; it is neither an indisputable scientiﬁc fact nor a well-
established scientiﬁc principle. It suddenly becomes possible to
overcome the roadblock by just circumventing it. The real challenge
now is to look for sufﬁciently compelling evidence to contradict
microscopic reversibility. This liberation gives us legitimacy to
explore possible biological indeterminism or quasi-determinism,
and suddenly we ﬁnd that we are staring at a plethora of in-
stances that may provide decisive evidence.
3.2. Microscopic reversibility and physical determinism
We shall now examine the principle of microscopic reversibility,
in the context of both classical and quantum mechanics. Like
Fig. 2. Flowchart explaining why Laplace's “hidden cause” doctrine can neither be
proven nor disproved. The ﬂowchart shows two loops in which Laplace's “hidden
cause” doctrine can be invoked for an indeﬁnite number of times as long as there are
dispersions. Each time the outer loop is traversed, a major advance about our under-
standing of dispersions is made. The inner loop is traversed repeatedly until a new
hidden cause is found. Exhaustion of existing dispersions is only tentative because
improvements of measurement techniques may uncover new dispersions. See text for
further discussion. (Reproduced from [Hong, 1998b]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
681

statistical mechanics, quantum mechanics utilizes probabilities to
describe the microscopic world of atoms and molecules, albeit for
different reasons. Statistical mechanics and quantum mechanics
are now widely accepted and have become common staples of
modern chemists in dealing with research problems. However,
Einstein could not quite accept the Copenhagen interpretation of
quantum mechanics [Heisenberg, 1958]. Einstein's view is high-
lighted in his famous remark that he did not believe that God plays
dice ([Pais, 1982], p. 443). But in stark contrast, Hawking (cited in
[MacCallum, 1975]) said: “God not only plays dice. He also some-
times throws the dice where they cannot be seen.” However, by
1954 Einstein appeared to have changed his mind and no longer
considered the concept of determinism to be as fundamental as it
was frequently held to be ([Popper, 1982], p. 2 footnote).
The validity of microscopic reversibility is also intimately tied to
the concept of time. Strict validity of microscopic reversibility im-
plies time-reversal symmetry at the microscopic level: There is no
difference between the past and the future. This led Einstein to
say that time is an illusion [Davies, 2002; Musser, 2002]. In
contrast, the second law of thermodynamics implies that time is an
arrow: The future is in the direction of increasing entropy. Thus, the
compatibility between microscopic reversibility and macroscopic
irreversibility has long been an unresolved problem, although some
physicists have considered it to be a stale issue that had long been
settled (e.g., see [Bricmont, 1996]).
Even eminent contemporary physicists could not come to a
deﬁnite consensus ([Goldstein, 1996]; [Kleppner, 1996]). To scien-
tists who are not physicists, this question must be treated as an
unsettled and unsettling problem. The readers may be cautioned
that the following arguments represent the author's highly per-
sonal view, which is inevitably plagued d and perhaps also blessed
d with ignorance. The standard literature should be consulted for
the mainstream thought, along with a healthy dose of skepticism.
From an epistemological point of view, I suspect, as Prigogine
also argued, that absolute determinism may simply be a mathe-
matical idealization of the real world, very much the way the
mathematical concepts of points, lines and surfaces (interfaces) are
conceptual idealizations of geometrical objects. However, the
idealization is actually a reductionist's luxury or illusion. A reduc-
tionist can pick suitable problems and can choose appropriate
experimental conditions to make the variance of measurements
reasonably small.
Regarding this idealization, physicist David Bohm presented, in
his book Causality and Chance in Modern Physics [Bohm, 1957], a
particularly relevant and illuminating argument. He considered the
three-body problem of a lunar eclipse. Over moderate periods of
time, the lunar eclipse is a precisely predictable event, without
taking into account the perturbations caused by other planets, from
ocean tides, or from still other essentially independent contin-
gencies. However, the longer the period of time that is considered
in the prediction, the more these perturbations become signiﬁcant,
and eventually even the molecular motion of the gaseous nebulae
from which the Sun, the Moon and the Earth arose should be taken
into account. The question about absolute physical determinism is
thus ultimately transformed into the following one: Is the history of
the universe uniquely determined by the initial conditions of the
Big Bang?
The prediction of a lunar eclipse represents an extreme case of
classical mechanics (celestial mechanics), in which the degree of
isolation from outside perturbations is high, and therefore the
variance of predictions is extremely small, thus giving rise to the
illusion of absolute determinism. Weather forecasting represents
the other extreme, in which the variance due to chance ﬂuctuations
is so large that only short-term predictions are feasible, whereas
long-term predictions are virtually impossible at present (low
degree of isolation from remote perturbations).
Life processes, being a manifestation of complexity with diverse
hierarchical organizations, seldom afford investigators the degree
of isolation comparable to the event of a lunar eclipse. However, life
is not as unpredictable as weather. Even complex phenomena such
as emotion and behavior have their neurophysiological and genetic
bases, which underlie the governing control laws [Heiligenberg,
1991; Damasio, 1994; LeDoux, 1996; Shih et al., 1999]. Disciplines
such as psychology and psychiatry owe their existence to reason-
ably strong causality entailed in the generation of emotion and
behaviors, normal or pathological. These phenomenological control
laws are much more sophisticated and complex, although these
higher-level control laws have their basis in physics and chemistry.6
It is of interest to examine the widely accepted physical concept
of microscopic reversibility in light of Bohm's argument outlined
above. I suspect that microscopic reversibility is just an excellent
approximation, but an approximation nonetheless. If so, it is inap-
propriate to draw conclusions by invoking microscopic reversibility
whenever its strict validity is required, such as in the free will
problem. The presence of a dispersion, no matter how small, will
ruin the validity of one-to-one correspondence, with regard to the
position and the momentum of particles, between two different
time points, thus invalidating the time-reversal argument being
invoked
to
establish
the
conﬂict
between
free
will
and
determinism.
Furthermore, those who advocate strict microscopic revers-
ibility are obliged to identify the spatial scale where (microscopic)
reversibility meets (macroscopic) irreversibility: a point of abrupt
transition or discontinuity. Apparently, microscopic reversibility is
not applicable on the mesoscopic scale where entropic changes are
not considered exceptional. For example, the M1 to M2 transition of
bacteriorhodopsin (a membrane-bound protein with 248 amino
acid residues) is accompanied by a large entropic change [Varo and
Lanyi, 1991]. Therefore, the discontinuity must appear below the
mesoscopic scale. But where and how?
Schulman [Schulman,1997] regarded the boundary between the
microscopic and the macroscopic scale as one of the greatest
mysteries. But physicist Jean Bricmont scornfully dismissed the
skepticism as an outdated question that had been answered
numerous times before, though he was willing to suppress his
righteous contempt so as to explain it one more time patiently for
the beneﬁt of the uninitiated and the uninspired amidst the Science
War [Bricmont, 1996]. Apparently, Schulman was not satisﬁed with
the kind of conventional gloss-over offered in physics textbooks
and perpetuated by overzealous but unsuspicious followers like
Bricmont.
If, however, we regard microscopic reversibility as a mathe-
matical idealization and approximation, it becomes easy to address
the problem regarding the point of transition from (apparent)
reversibility to irreversibility. The transition can then be viewed as
the gradual breakdown of the mathematical approximation, and
the point of transition depends on how big the error that one can
still tolerate: the transition is therefore not sharply and uniquely
deﬁned.
Let us now consider ﬂuorescence of a typical small organic
molecule, such as benzene (C6H6) [Schwarz and Wasik, 1976]. It is a
well-established fact in photochemistry that the emitting photon
always has a lower energy (longer wavelength) than the exciting
photon (stimulated emission of photons). This is because an electron
6 Some philosophers believe that higher-level phenomenological laws cannot be
reduced to physics and chemistry and novel causative factors emerge as a conse-
quence of hierarchical organizations of living organisms (see Sec. 6.2.2e6.2.3). But
skeptics probably prefer to treat this view as speculative.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
682

excited by the incoming photon to a higher electronic orbital ﬁrst
settles to a lower orbital by vibronic relaxation before a photon is
emitted, and the energy difference of the two orbitals accounts for
the loss of energy, which is dissipated as heat (radiationless tran-
sition). Time reversal of this photophysical event on this particular
spatial scale would exhibit an emitting photon that would be more
energetic than the exciting photon, i.e., the emitting photon would
have a shorter wavelength than that of the exiting photon, thus
contradicting
the
above-mentioned
well-established
fact
in
photochemistry. Furthermore, this hypothetical outcome would
also necessitate extraction of thermal energy from the environment
d a blatant violation of the second law of thermodynamics. In
conclusion, the transition between macroscopic irreversibility and
microscopic reversibility, if it exists, must take place below the
spatial scale of a benzene molecule. The validity of microscopic
reversibility is therefore highly questionable.
Furthermore, if one insists that time is reversible on the
microscopic scale, another difﬁculty arises. Matsuno [Matsuno,
1989]
pointed
out
that
the
one-to-one
temporal
mapping
together with complete ﬁxedness of boundary conditions d i.e.,
absolute determinism d requires mechanical adjustments caused
by local perturbations to be propagated at inﬁnite velocities (cf.
[Earman, 1986], p. 34). In other words, microscopic reversibility
implies that a cause is followed instantaneously by its effect with
absolutely no delay. The limit imposed by the speed of light pro-
hibits propagation of causes at an inﬁnite speed. Since a cause and
its effect cannot occur simultaneously, time reversal would lead to
the reversal of a cause and its effect. In conclusion, microscopic
reversibility cannot be strictly true, and the notion of time reversal
remains in the realm of science ﬁction.
More recently, biophysicists have begun to toil with the idea of
ﬂuctuation-driven ion transport [Tsong et al., 1989; Astumian,
1997; Astumian and Derenyi, 1998], thus indirectly challenging
the validity of microscopic reversibility. Non-equilibrium ﬂuctua-
tions can, in principle, drive vectorial (uni-directional) ion trans-
port along an anisotropic structure in an isothermal medium by
biasing the effect of thermal noise (Brownian ratchet mechanism).
The mechanism constitutes a ﬂagrant violation of microscopic
time-reversal symmetry. This apparent violation has been conve-
niently explained away by exempting non-equilibrium cases from
the requirement of microscopic reversibility. For example, bio-
physical experiments designed to test microscopic reversibility
have adopted the criterion that a violation of detailed balance in ion
transport through ion channels indicates d and is attributed to d
the presence of an external energy source [Steinberg, 1986;
Rothberg and Magleby, 2001]. In the same way, the violation of
microscopic reversibility exhibited by ﬂuorescence can also be
explained away, by identifying light as an external energy source.
However, this modiﬁed interpretation of the principle of micro-
scopic reversibility is an affront to the time-reversal invariance
stipulated by Newtonian mechanics; the practice is tantamount to
cutting the feet to ﬁt the shoes. Classical mechanics makes no such
exemption for non-equilibrium cases, and the principle of micro-
scopic reversibility should also apply to non-equilibrium cases if
absolute determinism is strictly valid. Interestingly, Angelopoulos
and coworkers [Angelopoulos et al., 1998] experimentally demon-
strated that the dynamics of the neutral-kaon system violates time-
reversal invariance. However, these physicists did not attribute it to
the presence of an external energy source.
The theory of Brownian ratchets provides a new explanation for
muscle contraction with regard to how a myosin globular head
interacts with an adjacent actin ﬁlament. According to experi-
mental observations made by Yanagida and coworkers [Kitamura
et al., 1999], myosin and actin do not behave deterministically.
These investigators found that the myosin globular head hopped
stochastically in steps from 5.5 to 27.5 nm long. Each step was an
integral multiple of 5.5 nm, which is equal to the distance sepa-
rating two adjacent actin monomers in an actin ﬁlament (the
polymeric form of actin). Furthermore, a step, no matter how long,
corresponded to the consumption of a single ATP molecule. Myosin
globular heads sometimes even jumped backward instead of for-
ward, but mostly forward. In other words, the myosin globular head
was undergoing a biased random walk during muscle contraction,
much like the stepping motion of a drunken sailor on a slope,
instead of on level ground. These ﬁndings are consistent with the
theory of Brownian ratchets, thus lending support to microscopic
irreversibility (see Yanagida's comment included in [Astumian,
2001], p. 64). Hatori et al. [Hatori et al., 1998] previously reported
a related observation: Fluctuating movements of an actin ﬁlament
both in the longitudinal and transversal directions appeared in the
presence of an extremely low concentration of ATP.
Without a valid concept of microscopic reversibility, the
following conclusions become inevitable: (a) the validity of one-to-
one temporal mapping is called into serious question, (b) the
argument leading to absolute physical determinism is seriously
undermined, and (c) the perceived conﬂict between free will and
determinism just vanishes. But, then again, not so fast. Try to think
from the opponents' point of view. After all, they were able to
convince an overwhelming majority of physicists d not only their
contemporaries but also forthcoming physicists generation after
generation. Why? Why? Why? I need to understand why an ulti-
mately ﬂawed argument could be so attractive and so enduring. Let
us turn our attention to a couple of well-known paradoxes which
were proposed by or on behalf of Boltzmann's detractors.
3.3. Validity of microscopic reversibility
The debates around physical determinism keep coming back
even after classical mechanics has been surpassed by quantum
mechanics, as exempliﬁed by Einstein's quandary. In a symposium
designed to refute pseudoscience and antiscience, physicist Jean
Bricmont [Bricmont, 1996] made several explicit statements: Chaos
does not in the least invalidate the classical deterministic world
view, but rather strengthens it, and chaos is not related in a
fundamental way to irreversibility. Bricmont further stated, “[W]
hen they are correctly presented, the classical views of Boltzmann
perfectly account for macroscopic irreversibility on the basis of
deterministic, reversible, microscopic laws.” Bricmont made these
statements as an orthodox response to his opponents' skepticism
during the Science War. The irony is that the above-quoted state-
ments constituted a succinct summary of fallacies perpetuated in
physics textbook (cf. [Frankfurt, 2005, 2006]).
I believe that most people who are familiar with chaos would
agree with Bricmont's explanation that chaos can arise under the
control of a deterministic mechanical law of motion (deterministic
chaos),
and
unpredictable
events
are
not
necessarily
non-
deterministic. However, chaos can also arise under the control of a
quasi-deterministic control law of motion (Sec. 3.4). Furthermore,
predictable events are not necessarily strictly deterministic. I disagree
with Bricmont's view that Laplacian determinism is compatible with
macroscopic irreversibility. First, let us examine Bricmont's argument.
Bricmont's explanation of irreversibility was based on two
“fundamental ingredients”: (a) the initial conditions, and (b) many
degrees of freedom in a macroscopic system. He pointed out that
the outcome of a physical event depends not only on the underlying
differential equation but also on the initial conditions. Even though
the differential equation exhibits time-reversal invariance, the
initial conditions may render the solutions time-irreversible (see
later for what Bricmont meant by these particular initial conditions
which he had in mind). There is some truth in this view, but an
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
683

inconsistency to be revealed at a deeper level undermines his
reasoning.
Bricmont also proposed a qualitative argument to convince the
readers that irreversibility always involves a macroscopic system
that contains a large number of particles. Speciﬁcally, he considered
the motion of a single billiard ball on a frictionless billiard table, and
reminded us that a movie that depicts its motion, if run backwards,
would appear normal and would not reveal the time reversal. But
Bricmont's argument is misleading for the following reason.7 The
principle of microscopic reversibility essentially stipulates that
momentum reversal of motion of a particle is indistinguishable from
time reversal. Thus, the actual trajectory engendered by time
reversal would be indistinguishable from the actual trajectory of a
single billiard ball with its momentum reversed but without time
reversal, if microscopic reversibility is strictly valid. However, the
two events d time reversal and momentum reversal d are not
shown side by side for strict comparison in Bricmont's argument.
Thus, a small difference between the original trajectory and the
retraced one would not be obvious to a casual observer. Even if both
events were shown, our naked eyes would not have the precision to
detect a small difference between the two trajectories, if the pas-
sage of time is not sufﬁciently long.
An alternative way to detect irreversibility is to consider a few
billiard balls, such as two piles of billiard balls with two contrasting
colors (blue and red), and to allow the billiard balls to collide with
one another and with the walls of the billiard table, resulting in
mixing of the two colors of balls. Now, given the same assumption
of a frictionless billiard table, running the movie backwards would
show spontaneous unmixing or de-mixing (separation) of the blue
and red balls, thus betraying the time reversal. In Bricmont's words,
strict reversibility means that nearby initial conditions follow nearby
trajectories ([Bricmont, 1996], p. 136). Therefore, it really was not
the lack of additional degrees of freedom that suppresses the
irreversibility. It was humans' insufﬁcient visual acuity (poor
resolving power of the naked eye) or the lack of reference to a
nearby companion billiard ball that conceals the irreversibility.
Nevertheless, ﬁnding fault with Bricmont's argument does not
automatically constitute proof of the opposite conclusion. Mackey
[Mackey, 2001] considered the origin of the thermodynamic
behavior captured by the second law. He analyzed two types of
physical
laws
of
motion:
invertible
and
non-invertible.
He
demonstrated that the invertible microscopic physical laws are
incapable of explaining the second law. What Mackey referred to as
invertibility can be regarded as synonymous with time-reversal
invariance, as mentioned above. He considered three possible
sources of irreversibility for invertible dynamics: coarse graining,
noise (from external deterministic processes), and traces. He dis-
missed all these three processes as possible explanations of irre-
versibility and concluded that invertible deterministic physical
laws of motion were incorrectly formulated, and suggested that
something minute and experimentally undetectable was omitted.
As we shall see, Mackey was right.
Nevertheless, Mackey still believed that the dynamics of the
universe is deterministic. However, he did concede that if the dy-
namics of the universe is composed of both deterministic and
stochastic elements, then the problem is solved. I shall argue that
this is indeed the case. The deterministic element is the mean of
position and momentum speciﬁed by the law of motion, which
gives the law its superﬁcially deterministic behavior and predict-
ability. The stochastic element is the dispersion of position and
momentum, which grants dynamic tolerance and irreversibility. In
the subsequent discussion, we shall present an analysis based on
the consideration of microscopic states.
Let us consider two well-known paradoxes, which Boltzmann's
detractors invoked to refute Boltzmann's claim that his theory of
statistical mechanics is compatible with Newtonian mechanics. The
ﬁrst paradox, according to Brush [Brush, 1976b], appeared as early
as 1867 in a discussion between Maxwell and his friends Tait and
Thomson (Lord Kelvin). The discussion involved a Gedanken
experiment about the individual microscopic states of gas mole-
cules in an isolated system, which we shall recapitulate now in a
slightly different fashion. We shall ﬁrst consider the case in which
the deterministic law of motion is Newtonian. The restriction can
later be lifted so as to accommodate any unspeciﬁed deterministic
law of motion. The Gedanken experiment shall demonstrate that
microscopic reversibility is not fully compatible with macroscopic
irreversibility; and moreover the conventional explanation, which
was presented by Bricmont to silence skeptics during the Science
War and which also appears in most physics textbooks, is deeply
ﬂawed. I shall demonstrate that the contradictions brought up by
the two paradoxes remain valid.
Consider a gas container with two equal compartments (e.g., of
50 ml each). Here, a microscopic state is essentially a detailed record
of the exact coordinates and momenta of individual molecules inside
the container, as functions of time. In real life, gas molecules of the
same kind are not individually distinguishable, but we shall assume,
in this Gedanken experiment, that the record can track individual
molecules as if they were individually identiﬁable by a magic la-
beling method. This restricting condition of individual tracking will
be relaxed later. Since it is an isolated system, there is no heat ex-
change with the environment through the container walls. There is
also no temperature change because the container is ﬁlled with inert
gases that do not react at room temperature and standard atmo-
spheric pressure, for example, monoatomic gases argon and helium.
There will be changes of entropy, but this macroscopic concept is not
relevant to the Gedanken experiment because we are not investi-
gating a statistical ensemble of microscopic states.
Let the initial state S0 (at time t0) of the isolated system be so
constructed that the left compartment contains 50 ml of argon gas,
whereas the right compartment contains 50 ml of helium gas. An
opening between the two compartments allows gas molecules to
diffuse from one compartment to the other, back and forth. After a
sufﬁciently long time interval Dt has elapsed, fairly uniform mixing
of the two gases will occur (state S1). At room temperature, this time
interval should be reasonably short (for example, from a few hours
to a few days, depending on the size of the opening between the two
compartments). Now, reverse the momentum of each and every gas
molecule at time t1 ¼ t0 þ Dt, and take this altered condition as the
new initial condition (state S10). Let the law of motion operate for
exactly another time interval Dt, and a new state S00 will be reached.
If microscopic reversibility were strictly true, the position of
each and every gas molecule in the state S00 would be precisely the
same as in the state S0, but the momentum of each and every
molecule would be precisely opposite to that in the state S0. In the
phase space, S10 can be obtained from S1, and S0 from S00, by a
“reﬂection” with respect to the position axes, i.e., by changing the
signs of the three Cartesian components of all momentum vectors
while keeping exactly the same three Cartesian components of all
position vectors. Thus, S00 is symmetrical to S0, whereas S10 is
symmetrical to S1, with respect to the position axes, and, further-
more, the correspondence is one-to-one. What happened would be
a spontaneous unmixing of two gases: Each and every molecule
would have retraced its previous trajectory, but in the reverse di-
rection. Ostensibly, this outcome contradicts Boltzmann's kinetic
theory of gases. It is historically known as Loschmidt's “reversibility
7 Bricmont's explanation, instead of others', was targeted for the present rebuttal
for convenience only. He was not the ﬁrst to concoct these “phony” arguments. He
merely volunteered to spearhead the attack on postmodernist sociologists.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
684

paradox” ([Brush, 1976a], p. 83) or Loschmidt's “velocity-reversal
paradox” ([Prigogine, 1997], p. 21).
When Josef Loschmidt brought this paradox to Boltzmann's
attention around 1876, rumor had it that Boltzmann responded by
saying “Well, you just try to reverse them!” ([Brush,1976b], p. 605). Of
course, no one can do that, but can we wait for the system to evolve to
the point of momentum (velocity) reversal? The next question is:
How long do we have to wait? To answer the latter question, we must
bring up the second paradox, known as the “recurrence paradox,”8
which was ﬁrst introduced by Ernst Zermelo [Zermelo, 1896] to
refute Boltzmann's claim of the afore-mentioned compatibility. Zer-
melo's argument was based on a theorem previously published by
Henri Poincare, known as Poincare's recurrence theorem [Poincare,
1890].9 In the present context, the theorem means that there are
inﬁnitely many ways to set up the initial conﬁguration of our
Gedanken experiment, speciﬁed by positions and momenta, so that
the systemwill return inﬁnitely many times, as close as one wishes, to
its initial conﬁguration, i.e., back to S0 (not back to S00): almost com-
plete restoration to the initial conditions. Strictly speaking, the
waiting time for momentum reversal is not the same as the Poincare
recurrence time. However, since S00 is the mirror image of S0 with
respect to position axes, it is conceivable that the waiting time for S00
to occur spontaneously is of the same order of magnitude as the
Poincare recurrence time by virtue of symmetry. Furthermore, since
it takes reasonably short time Dt for S10 to become S00, the waiting
time for S10 (i.e., momentum reversal) to occur spontaneously is also
of the same order of magnitude. It was estimated that time enor-
mously great compared with 101010 years would be needed before an
appreciable separation would occur spontaneously in a 100 ml
sample of two mixed gases ([Tolman, 1938], p. 158). According to
Bricmont [Bricmont, 1996], Boltzmann responded to the recurrence
paradox by saying, “You should live that long.” In brief, Boltzmann
invoked the practical impossibility, rather than theoretical impossi-
bility, of achieving the condition so as to rebut these two paradoxes.
In fact, all the past defenses on behalf of Boltzmann d defenses
that claimed no contradiction between Boltzmann's theory and
Newtonian mechanics d were based on probability arguments and
practical approximations, e.g., coarse graining. However, in a
serious argument to defend the theoretical consistency between
the second law of thermodynamics and microscopic reversibility, it
is a strange position to accept “approximations,” instead of math-
ematically rigorous proof, as a way of resolving a fundamental
inconsistency. When a success must be assured and a failure is
absolutely unacceptable, it is cold comfort to be told that the
chance of failure is small but not zero; unlikely events can happen,
and have happened, in a single trial run. It is one thing to declare
that there is no practical conﬂict, in real life, between the second
law of thermodynamics and microscopic reversibility. It is another
thing to say that the second law of thermodynamics is theoretically
consistent with microscopic reversibility. For this matter, the
distinct theoretical possibility of spontaneous, complete unmixing
of two different gases in a mixture, in exactly the same amount of
time taken for prior mixing after achieving the momentum
reversal, is still intellectually disquieting. Now, judgment must be
made regarding the validity of microscopic reversibility and abso-
lute determinism. We shall invoke theoretical reasoning without
any element of practical impossibilities or approximations.
Poincare's recurrence theorem also states that there are inﬁ-
nitely many non-reversible solutions of the above problem, but
these solutions can be regarded as “exceptional” and may be said to
have zero probability (inﬁnitesimal probability). This theorem
lends credence to Bricmont's claim that an unusual initial state at
the time of the Big Bang could give us an irreversible universe. It is
also a strange position to rely on the possible existence of this rare
initial state while rejecting the probable existence of a rare recur-
rence just to defend the consistency between the second law of
thermodynamics and microscopic reversibility. Of course, the
strangeness of his position alone does not prove him wrong. Be-
sides, cosmological models of the Big Bang are rapidly undergoing
modiﬁcation. Therefore, we shall pursue the problem in other ways.
Let us get back to the above re-enactment of Loschmidt's
reversibility paradox. It was and still is a serious challenge to
Boltzmann's claim in spite of all the rebuttals proposed over the past
hundred years. Just examine the phase diagram pictorially. S00 is
symmetrical to S0, i.e., S00 can be obtained from S0, by a reﬂection
with respect to the position axes. If S0 actually occurs in real life, then
the theoretical construct S00 ought to have an equal probability of
existing in real life. Yet, spontaneous separation of two mixed gases
has never been documented in human history, whereas different
puriﬁed gases, e.g., nitrogen gas and oxygen gas stored in different
metal canisters, are commonplace in commercial activities. What
accounts for the apparent symmetry breaking? Of course, there is a
standard textbook response to this question: S0 is deliberately set up
by a human experimenter whereas S00 is a mental construct in a
Gedanken experiment. Note that this is essentially what Bricmont
meant by one of the two “fundamental ingredients” for irrevesi-
bility: a special initial condition like S0 can lead to irreversibility
[Bricmont, 1996]. It was a very clever tactical argument, but it was
also extremely foolish to deploy this argument strategically, because
the argument undermines the entire argument designed to defend
microscopic reversibility (see later). Now, let us address several is-
sues regarding the signiﬁcance of this Gedanken experiment.
First, whether we can precisely measure the position and mo-
mentum of each and everygas molecule is irrelevant to the argument.
What is relevant is that absolute determinism requires that future
values of positions and momenta be uniquely determined by the law
of motion, and that the values corresponding to the initial state S0, at
time t0, be mapped to the values of the state S1, at time t1, in a one-to-
one correspondence. The mapping from S1, through S10 and S00, back
to S0 is also a one-to-one correspondence (microscopic reversibility).
That correspondence holds for each and every gas molecule regard-
less of the fact that the experimenter could not keep track of the
precise position and momentum of each and every molecule at all
times. The same conclusion applies to any law of motion of the one-
to-one mapping type, and is independent of the present uncertainty
or lack of consensus about the interpretation of quantum mechanics.
Second, whether we can stop the second part of the Gedanken
experiment (starting with S10) after a time interval of precisely Dt
has elapsed is irrelevant. If it took a long time to unmix, it would
take just as long time to remix. Therefore, partial d nearly perfect
d unmixing of the two gases would still be evident shortly before
and shortly after this target time; the thoroughly unmixed gases
would not be immediately re-mixed. In other words, even if the
experimenter overshoots or undershoots the target time, the
detection of near-perfect unmixing of the two gases would be just
as good experimental evidence as perfect unmixing; a near-miracle
is just as good as a bona ﬁde miracle for our present purpose.
Third, let us consider the question: Is the state S10 such a theo-
retically rare occurrence that the possibility that it may arise in real
life without “divine” intervention can be practically ignored? Like
his predecessors, Bricmont argued that it is: The calculated
Poincare recurrence time exceeds the age of the universe if the
number of gas molecules is sufﬁciently large. However, as we shall
see later, the estimated Poincare recurrence time may not tell the
whole story. We shall set this issue aside for the time being and
shall not let the lingering doubt cloud our judgment in the
8 An English text was reprinted in [Brush, 1966], pp. 208e217.
9 An English text was reprinted in [Brush, 1966], pp. 194e202.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
685

following discussion.
As indicated above, the primed states S00 and S10 are symmetrical
to the unprimed states S0 and S1, respectively, with respect to the
position axes in the phase space. Thus, an unprimed state and its
corresponding primed state form a conjugate pair of “momentum-
mirror images” d for lack of a better term d in the phase space. Here,
S0 and S1 exist actually in the Gedanken experiment since S0 is set up
by the experimenter whereas S1 is derived from the time-evolution
of S0. In contrast, S00 and S10 are theoretical constructs. Instead of
asking how long it takes for the state to go from S1 to S10 spontane-
ously, let us ask a slightly different question: Is the existence of these
theoretical constructs S00 and S10 much less probable than that of the
corresponding unprimed states S0 and S1, as implied by Bricmont's
argument? The primed states differ from the unprimed states only in
the reversal of their momenta. Their existence is, therefore, not
prohibited by Newtonian mechanics. Newtonian mechanics stipu-
lates that S0 and S1 are equally probable, since one can be derived
from the other either by forward time-evolution or time reversal.
Therefore, S00 and S10 are equally probable for the same reason.
However, are the primed states and the corresponding unprimed
(conjugate) states equally probable, or, as Bricmont's argument
implied, are the primed states much less probable than the unprimed
states? We shall prove that all four states, S0, S1, S00 and S10, are equally
probable by means of reductio ad absurdum.
To proceed with proof by means of reductio ad absurdum, we
must tentatively assume that the primed states S00 and S10 are less
probable than the corresponding unprimed states S0 and S1. Recall
that the primed state S10 is not prohibited by Newtonian mechanics,
in spite of the present tentative assumption that it is less likely to
occur in reality than S1. Now, let us repeat the Gedanken experi-
ment. Only this time, let us take S10 as the new starting state at a
new starting time t10. At time t ¼ t10 þ Dt, the state S00 is reached.
Reverse the momentum to get the corresponding conjugate state
S0, run the experiment for another time interval Dt, and eventually
the state S1 is reached. Note that S1 and S0 now play the role of
mental constructs because they are the conjugate states of S10 and
S00, respectively. Now by invoking the beginning (tentative)
assumption, we thus conclude that it is less probable that the states
S0 and S1 d the new mental constructs d exist in reality than the
primed states S00 and S10. The contradiction thus invalidates the
beginning assumption. Q.E.D.10 In other words, the existence of the
unprimed states and that of the primed states are shown to be
equally probable by virtue of microscopic reversibility, as also
suggested by the symmetry exhibited by the phase diagram. It is
commonplace to see canisters of pure oxygen gas as well as of other
common commercially puriﬁed gases. At face value, the above
proof indicates that spontaneous separation of mixed gases should
be just as common an observation. But perhaps not so fast! There is
an additional logical gap to ﬁll. The proof establishes that S00 and S0
are equally probable, but it does not mean that both are
commonplace.
Fourth, Bricmont's argument implied that S00 is extremely rare,
but we have just shown that S00 is as probable as S0. We are thus
prompted to ask the following question: Is S0 also extremely rare?
Surprisingly or not so surprisingly, the answer is afﬁrmative. The
Gedanken experiment requires the initial state S0 to satisfy certain
macroscopic conditions: 50 ml of argon gas at the left and 50 ml of
helium gas at the right, both at room temperature and standard
atmospheric pressure. A microscopic state that satisﬁes this con-
dition can assume any possible conﬁguration of positions and
momenta of its constituent molecules, as long as argon molecules
are kept in the left and helium molecules are kept in the right
compartment at t ¼ t0. Therefore, there are inﬁnitely many mi-
crostates that satisfy the conditions, and S0 is but one of these
inﬁnitely many ones. Although it is by no means difﬁcult to set up a
starting state that satisﬁes this requirement, it is virtually impos-
sible to set up the starting state with a preconceived conﬁguration
of positions and momenta of individual gas molecules because the
experimenter has no control over them and because its probability
of spontaneous occurrence is inﬁnitesimal. Essentially, the experi-
menter must pick the initial state S0 arbitrarily from inﬁnitely many
qualiﬁed microscopic states, and will be stuck with whichever one
that actually comes up at the moment of setting up S0. Thus, the
starting state S0 is rare and its existence is improbable by virtue of
the above probability argument: It is a discrete microstate among
the inﬁnitely many microstates in the continuum of distribution. In
other words, if we were to specify, in advance, a particular state
with a preconceived conﬁguration of positions and momenta as the
starting state at t ¼ t0, the probability of setting up, in a single run,
the starting state exactly as speciﬁed is inﬁnitesimal though not
exactly zero. Thus, if we are to repeat the same Gedanken experi-
ment many times over, it is extremely unlikely that we will be able
to duplicate the same state S0 exactly in all subsequent runs. Note
that the distinction between the English word “a” and the English
word “the” is crucial in keeping our head clear so as to avoid subtle
logic errors in our lengthy reasoning. Imagine how a natural lan-
guage without such a distinction could potentially send us into
utter confusion, just as Bacon once prophetically pointed out.
The peculiarity regarding the probability of occurrence of a
particular microscopic state stems from two seemingly incompat-
ible but coexisting features in classical mechanics: (a) a continuous
distribution of physical parameters of the boundary conditions, and
(b) zero dispersions of these discrete parameters, as predicted by the
absolutely deterministic physical law. The combined effect of these
two features demands that a particular occurrence occupies only an
inﬁnitesimal range in the continuous distribution. As a conse-
quence, the probability of a discrete occurrence among inﬁnitely
many possible ones with a continuous distribution is always
inﬁnitesimal. This is symptomatic of mixing discrete events with a
continuous distribution d a practice that has the suspicious ring of
mathematical idealization. Thus, the proper way to deﬁne the
likelihood of a speciﬁc occurrence of a discrete event with a
continuous distribution is to deﬁne, instead, a probability density,
i.e., the probability of a discrete occurrence, which is, by deﬁnition,
conﬁned to within an inﬁnitesimal interval on the scale of the
continuous distribution. A bona ﬁde probability for occurrences
within a ﬁnite (nonzero) interval can thus be obtained by inte-
grating d by means of integral calculus d the probability density
over the ﬁnite range being considered. However, absolute deter-
minism dictates that the discrete parameters have zero dispersions;
that is why they are discrete. The probability density function for a
discrete parameter is therefore the well-known mathematical
function which is inﬁnitely narrow and inﬁnitely high but which
encloses a ﬁnite area, i.e., the d (delta) function.
That is the reason why Poincare's recurrence theorem was
framed as “… return … as close as one wishes to its initial position”
rather than as “… revisit … exactly its initial position” ([Brush,
1966], p. 194). Therefore, if we require, in our Gedanken experi-
ment, the state exhibiting appreciable spontaneous unmixing of
the two gases to be very close d within a speciﬁed range d to S00 in
the phase space rather than exactly there, we can integrate the
probability density over this speciﬁed range to obtain a non-zero
probability; the greater the range of tolerable deviations from S00
the higher the probability. Note that this probability pertains to
almost exact restoration of positions as well as almost exact mo-
mentum reversal of the initial conditions, if the speciﬁed range is
sufﬁciently small, namely, half-way decent spontaneous separation
10 Proof Completed; Quite easily done without interpretive tricks.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
686

of two mixed gases, and a near-miracle!
Fifth, let us get back to the undisputed conclusion that each re-
run of the Gedanken experiment almost always has a different
initial condition. A natural question to ask about Loschmidt's
reversibility paradox is: Why do we have to set up only a single
starting state and follow the time-evolution of the same starting
state S0, all the way through until it returns very closely to its
conjugate “momentum-mirror image” state S00, if our objective is to
check the consistency between the second law of thermodynamics
and microscopic reversibility? The answer is obviously negative; it
is not necessary to do so. Consider the purpose of invoking
Poincare's recurrence theorem. We must ask ourselves: What is our
objective? If the objective is to demonstrate that microscopic
reversibility predicts that an isolated system will eventually revisit
a previous initial condition, then by all means get as close as
possible to that condition. On the other hand, if the objective is to
check whether and when a predicted spontaneous separation of a
previously mixed sample of two gases will ever occur or to debunk
the
principle
of
microscopic
reversibility,
then the
detailed
requirement of the target state of spontaneous separation can be
somewhat relaxed, without compromising our objective, in ex-
change for a shorter, more realistically achievable waiting (recur-
rence) time.
Let us consider the following provisions to relax the re-
quirements. First provision: The speciﬁcation regarding momenta
can be abandoned; we seek instead spontaneous unmixing without
the requirement of momentum reversal or restoration of momenta.
This relaxation allows us to integrate the probability density not
only over the range of the neighborhood of S0 but also over its
conjugate state S00. It appears that the overall probability would
increase signiﬁcantly if not doubled. But “doubling” is an under-
estimate because each particle has three momentum axes, and
there are in total eight conjugate “neighborhood” mirror-images to
consider. For a container with gases with the number of molecules
of the order of Avogardro's number, the phase space is multi-multi-
dimensional and the number of the mirror-image “neighborhoods”
to be integrated over the respective probability density quickly
approaches astronomical numbers. The “recurrence time” for the
relaxed condition is vastly shortened. Maybe spontaneous gas
separation would not be such a rarity if microscopic reversibility
were strictly valid.
Second provision: Since molecules of the same kind are indis-
tinguishable, it is not necessary to require the same molecule to get
back to where it was at t ¼ t0; any other molecule can take its place.
The probability can be further increased by virtue of the permu-
tation of a large number of constituent molecules, and the corre-
sponding waiting time can be further shortened accordingly. In fact,
it is not even necessary to require a molecule to be in a previously
ﬁlled position, occupied by another molecule in the momentum-
mirror image conjugate state. The positional degrees of freedom
can be further increased by imposing a minimum requirement of
having argon atoms going to the left compartment and helium
atoms going to the right (never mind whether the position was
previously taken by another molecule or not).
Third provision: There is no need to require an almost full and
complete spontaneous separation of the two previously mixed
gases. A spontaneous partial separation would raise the specter of
macroscopic reversibility, even though, strictly speaking, such a
partial separation cannot be regarded as a recurrence or reversal of
individual molecular trajectories.
All these provisions to relax the requirements can bolster the
probability considerably by integrating the probability density over
a wider and wider range in the phase space, thus shortening the
waiting time for a spontaneous partial unmixing of the two pre-
viously mixed gases to occur. Physicists know better how relaxing
the above conditions can vastly increase the probability of spon-
taneous separation; the keyword is “degeneracy.” It is a common-
place keyword in physics. I mention this keyword just to remind
physicists. It matters little for anyone else, because the above
analysis in terms of phase diagrams had said it all.
Thence, we are looking for a miracle that has been heretofore
unobserved. By relaxing the requirements, we essentially settle for
a “low-grade” miracle, i.e., a halfway decent spontaneous unmixing
instead of a full-ﬂedged spontaneous unmixing. In this way, we can
avoid being misled by the unrealistically long waiting time. If we
can establish that this low-grade miracle is not forthcoming within
a reasonable waiting time, we can begin to cast a serious doubt on
Bricmont's claim that the second law of thermodynamics is indeed
consistent with microscopic reversibility and even to suspect that
microscopic reversibility may not be consistent with physical
reality. That this is a serious question was highlighted by the
tragic suicide of Boltzmann in 1906.
Note that the ﬁrst and second provisions essentially pool the
data of an inﬁnite number of separate runs of the Gedanken
experiment, whereas the third provision allows for greater de-
viations from the idealized target conﬁguration of complete
restoration of positions and momenta or complete restoration of
positions but complete reversal of momenta. Although this quali-
tative Gedanken experiment does not provide a hard number in
terms of probability or waiting time, I suspect that the arguments
based on the Poincare recurrence time might have grossly over-
estimated the time that it takes for a spontaneous partial separation
of
two
previously
mixed
gases
to
occur
under
Newtonian
mechanics.
Sixth, a discrepancy remains to be reconciled. On the one hand,
spontaneous momentum reversal may take an unrealistically long
time to occur, in view of the probability argument. On the other
hand, the symmetry argument establishes that the primed and the
unprimed states are equally probable. The unprimed states S0 and
S1 deﬁnitely take place in our lifetime by virtue of the experimental
construction; the primed state S00 and S10 should also be just as
likely to take place in our lifetime by virtue of the symmetry
argument. This discrepancy between the two arguments seems to
be rooted in our uncritical inference that if something actually
happens it is by default not improbable that it exists in reality.
However, this now-questionable “inference” is actually quite logical
in view of absolute determinism that is historically associated with
Newtonian mechanics: If something does happen, it has been
destined to happen even before it has happened. As we shall see
next, this contradiction seems inevitable.
In the above discussion in terms of probability density, the
state S0 that eventually happens was found to be improbable
before actually happening. At issue here is the conditional proba-
bility for the occurrence of a discrete event with a continuous
distribution. Before the initial state is set up, a mortal human
being has no clue as to which particular microscopic state is
actually going to materialize. The best bet is to assume that all
microscopic states are equally probable since they are indistin-
guishable anyway. Note that this is the very same assumption
which Boltzmann used to construct his theory of statistical me-
chanics. After the state S0 materializes, the conditional probability
jumps to exactly unity; it becomes absolute certainty because the
event is discrete with no dispersion whatsoever. This is a logical
consequence of deterministic physics: deterministic physics, in
contrast to quantum mechanics, assigns a continuous probability
distribution to discrete events, thus necessitating the use of
probability density instead of just plain probability, as explained
above. However, the very fact that the conditional probability can
assume two drastically extreme values, inﬁnitesimal and unity, on
the probability scale (which ranges from 0 to 1), before and after
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
687

the occurrence, respectively, tacitly breaks the time-reversal
symmetry at the microscopic level, contrary to the basic tenet of
deterministic physics. On the other hand, as just mentioned, ab-
solute determinism demands that the (conditional) probability of
occurrence of an event be the same before and after its occurrence
because events are pre-destined. Thus, mixing probability argu-
ments with deterministic physics inherently leads to a contradic-
tion. The contradiction reﬂects an irreconcilable clash between the
deterministic view and the probabilistic view. This latter comment
applies to any unspeciﬁed deterministic physical law. Here we
merely reiterate, in a slightly different fashion, what Prigogine has
been preaching all along. As we shall see, contradictions are not
only limited to the ontological level, but they are also extended to
the epistemological level.
Last but not least, let us turn the tables and play the devil's
advocate. Is it possible that there is an inherent asymmetry be-
tween the unprimed states and the corresponding primed states,
one that is not included in Newtonian mechanics other than the
above-mentioned change of the conditional probability before and
after the occurrence of a discrete event? Indeed, there may just be
one, as pointed out by Bricmont: the initial state S0, in which the
two gases were conﬁned to two separate compartments, was
deliberately set up by the experimenter, whereas the state S00 of
spontaneous unmixing of the two gases could not be directly set up
by the experimenter, but it is a natural consequence of motion that
is dictated by the deterministic law of motion. Bricmont used this
asymmetry to explain why the existence of S0 is probable while that
of S00 is improbable.
Let us see how a personal intervention could cast different fates
to a pair of conjugate “momentum-mirror image” states, which are
symmetric as far as Newtonian mechanics is concerned. To uphold
Bricmont's argument, it is necessary to demonstrate that, for every
unprimed state, the corresponding primed state cannot exist in
principle or, at least, its existence is much less probable than that of
the corresponding unprimed state. However, these excluded
primed states cannot be speciﬁed ahead of time. The only valid
speciﬁcation is to set up and name an initial state, of which the
representative is S0. The law of motion then gives us a stream of
ensuing states, S1, S2, S3, …, etc., corresponding to successive times,
t1, t2, t3, …, etc., respectively, as the trajectory sweeps through the
phase space deterministically. As a consequence, the existence of
the corresponding conjugate states, S10, S20, S30, …, etc., must then be
deterministically rendered improbable, one by one (otherwise S00
could evolve from any of the probable ones). It seems a strange
coincidence that the corresponding primed states could be selec-
tively and conveniently prohibited only after the onset of the
Gedanken experiment. It is as if the experimenter could exert a
“downward causation” on external inanimate matters, i.e., using
the mind to affect the external physical world without exerting any
physical force directly or indirectly. However, the idea of downward
causation is totally alien to the thinking of advocates of absolute
determinism. Serious considerations of downward causation are
usually restricted to life forms (see Sec. 6.2). Even the most staunch
and audacious vitalists would ﬁnd it “spooky” to have minds con-
trolling inanimate matters by means of action at a distance.
From the above discussion, what makes the unprimed states
so special as opposed to the primed states is of course the
intervention of the experimenter, as pointed out by Bricmont: The
special state S0 was willfully set up by the experimenter, whereas
the state S00 exists only in the imagination of someone who
performs the Gedanken experiment. However, Bricmont's argu-
ment is ﬂawed at an even more fundamental level: Bricmont
tacitly accepted the view that the experimenter had free will to set
up the special state S0. Actually, it would be impossible for Bric-
mont to maintain an overall consistency in his argument against
microscopic irreversibility because his argument was based on
concurrent validity of two incompatible (mutually exclusive)
views: (a) willful decision making by the experimenter to set up
the special state S0 and (b) microscopic reversibility. From the
foregoing discussion, it is obvious that the existence of free will
and the validity of microscopic reversibility/absolute determinism
are mutually exclusive. Thus, Bricmont's initial conditions argu-
ment is also deeply ﬂawed, just like the argument of large
numbers.
Here is the ultimate irony: The validity of Bricmont's argument
depends on the validity of what his very argument was supposed to
disprove, albeit indirectly. Obviously, Bricmont could not have it
both ways. The logical inconsistency of Bricmont's argument is a
subtle case of self-inconsistency: One uses an argument, which is
based on what one does not believe, to prove what one does not
believe is wrong. The absurd but subtle mistake stemmed from the
replacement of the emotionally charged phrase “free will” with a
neutral phrase “setting up an experiment” and/or from a temporary
lack of awareness of the incompatibility between free will and
absolute determinism and between free will and microscopic
reversibility. The main culprit again is the imprecision of natural
language.
In conclusion, invoking the argument that the Poincare recur-
rence time is excessively long, relative to a human being's lifetime,
does not satisfy the required logical rigor to resolve the contra-
diction; it merely explains it away and convinces no one but the
unsuspicious. In Popper's opinion, Boltzmann failed to defend
himself against the criticisms raised by the reversibility and the
recurrence paradoxes in spite of his steadfast insistence that his
theory is consistent with Newtonian mechanics. His heroic effort to
derive the law of entropy increase (dS/dt  0) from mechanical and
statistical assumptions d his H-theorem d failed completely; it
ended up destroying what he had intended to rescue.11 He
conceded that reversibility is possible but extremely improbable in
real life. Popper thought that Boltzmann must have realized this
inconsistency, and his depression and suicide in 1906 might have
been connected with it ([Popper, 1992], p. 161). However, his sta-
tistical mechanics has survived and has been widely accepted,
presumably for the lack of better theories of the kinetic behavior of
gases. Boltzmann would have been better off if he had conceded
that a new hypothesis beyond Newtonian mechanics had been
introduced d “So what!” d and had then let posterity judge the
validity of his hypothesis. Of course, hindsight is always 20/20. At
the height of Newton's spectacular success, who had the courage
and audacity to hint at possible incompleteness of Newtonian
mechanics or its limited range of validity? Once quantum me-
chanics dealt a decisive blow to Newtonian mechanics, any one
could challenge it without fear. Based on the analysis presented in
the present section, Boltzmann's theory deserves the status of a
major paradigm shift since his hypothesis marked a signiﬁcant
discontinuity from Newton's own ground-breaking theory of
mechanics.
As we shall see, none of the aforementioned difﬁculties d both
ontological and epistemological d persist if we treat microscopic
reversibility as a good approximation: microscopic quasi-revers-
ibility. This view is consistent with Popper's view; he thought that
Newtonian mechanics is “only a splendid approximation” [Popper,
1982], p. 47). Thus, probability enters the deliberation in statistical
11 It is inherently problematic to justify or rationalize the compatibility of sta-
tistical mechanics with classical mechanics or to resolve the two paradoxes on the
basis of arguments from within statistical mechanics. This comment also applies to
the argument of coarse graining. Metaphorically speaking, Hercules was inherently
incapable of lifting himself up no matter how much strength he had.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
688

mechanics not only because we are practically incapable of keeping
track of the positions and momenta of each and every molecule but
also because probability is inherent to the law of motion. If we are
willing to give up one of the most cherished tenets of “precision”
physics, absolute determinism, we will see that, by invoking chaos,
Boltzmann's theory can be shown to be fully compatible with
microscopic quasi-reversibility.
3.4. The origin of macroscopic irreversibility
Let us take a moment to visualize how the mathematical
idealization of microscopic quasi-reversibility leads to microscopic
reversibility as an approximation if the passage of time is
sufﬁciently short. The trajectory of a single particle under the
control of an absolutely deterministic law of motion is represented
by a line, with no width, in a three-dimensional space. Reversal of
momentum results in exact retracing of the trajectory in the reverse
direction, thus ensuring strict microscopic reversibility. This is what
Mackey referred to as invertible dynamics [Mackey, 2001]. In
contrast, the trajectory under a quasi-deterministic law of motion is
represented not by a line, but rather by many lines emanating from
the same initial point. The (mathematical) envelope of these tra-
jectories is a cone with its apex located at the uniquely deﬁned
initial position (a mathematical point) in the present, and with its
base pointing towards the future. In other words, a quasi-
deterministic law of motion dictates that the trajectories must be
conﬁned to the interior of the trajectory cone. The thin divergent
cone thus deﬁnes what Matsuno referred to as the dynamic toler-
ance,
i.e.,
the
range
of
dispersion
tolerated
by
the
quasi-
deterministic law of motion. Alternatively, one can view the thin
divergent cone as a causal constraint, which prevents probable
trajectories from deviating beyond the envelope of the constraining
cone (cf. Sec. 6.2).
Initially, during a short time interval, the difference between
this thin cone and a strict mathematical line is not apparent.
However, the difference will gradually become apparent as time
goes on. Furthermore, reversal of momentum does not result in
exact retracing of the trajectory in the reverse direction. Since the
trajectory cone continues to diverge, there is only one out of an
inﬁnite number of trajectories contained within the cone that can
give rise to exact retracing of the original trajectory in the reverse
direction. Therefore, the probability of retracing the original
trajectory is inﬁnitesimal, though not exactly zero. This is what
Mackey referred to as non-invertible dynamics [Mackey, 2001]. A
momentum reversal bends the trajectory cone by exactly 180 and
folds it over on itself, somewhat like partially pulling back the peel
(skin) of a banana to expose its edible ﬂesh (core) (Fig. 3). On the
microscopic scale with a limited “viewing ﬁeld,” a bent trajectory
cone is not readily distinguishable from a strict mathematical line
being bent back on top of itself. Mathematical idealization of this
microscopic
quasi-reversibility
thus
approaches
microscopic
reversibility as an exceedingly good approximation. A similar rep-
resentation by a thin bent cone for microscopic quasi-reversibility
can be generalized to the phase space, where both the position
and momentum are represented by a single trajectory in a six-
dimensional space.
Next, let us consider how chaos arises under a strictly deter-
ministic law of motion, while using the analogy of frictionless bil-
liard balls as a visual aid (Fig. 4). As frequently pointed out in the
literature, deterministic chaos arises as a consequence of uncer-
tainty of the initial conditions. Let us consider two billiard balls in
close proximity, and let the separation of the two positions specify
the maximum range of uncertainty in the speciﬁcation of the initial
position. For simplicity, we shall not include uncertainty in
momenta, since generalization can readily be made in the phase
space to include uncertainty in both positions and momenta.
In this case, the two trajectory lines are initially parallel to each
other, and will remain parallel after a collision with a perfectly ﬂat
edge of the billiard table. Collisions near a corner require special
consideration. As long as the two balls bounce off the same edge,
parallel trajectories will remain parallel. If they bounce off two
different but adjacent (mutually perpendicular) edges near a
strictly rectangular corner, parallel trajectories will still run in
parallel but in opposite directions, i.e., anti-parallel directions
(Fig. 4A). A second reﬂection around the same corner converts the
anti-parallel trajectories back into the original parallel ones. Thus,
nearby (proximate but not contiguous) initial conditions follow
nearby (proximate but not contiguous) trajectories. There is no
divergence of the trajectories of two balls.
If, however, the corner is not strictly rectangular, the two tra-
jectories, after a reﬂection from different edges at the same corner,
will no longer remain parallel to each other, and a subsequent
reﬂection may take place around different corners, thus greatly
diminishing the probability of restoring them to the original par-
allel trajectories (Fig. 4B). In fact, such restoration would require a
purely coincidental matching of the angles among these corners.
The trajectories of two initially nearby balls will diverge exponen-
tially (exponential divergence). Sooner or later, chaos ensues. How-
ever, reversal of momentum would allow the two balls to exactly
retrace their respective original trajectories in the reverse direc-
tion; time-reversal symmetry is preserved in spite of the chaotic
outcome. Therefore, deterministic chaos alone does not lead to
irreversibility. The second “fundamental ingredient” of Bricmont d
many degrees of freedom d is irrelevant for irreversibility to occur.
What if the edges are not perfectly ﬂat, but rather rugged like
corrugated cardboard, with the degree of ruggedness comparable to
the ball size? The condition is tantamount to having a polygon with
numerous corners, many of which are non-rectangular. Therefore,
collisions with these rugged edges may also contribute to the
emergence of chaos. Since the degree of ruggedness of the
container walls usually matches the dimension of colliding gas
molecules, the mathematical idealization of having strictly rect-
angular corners with perfectly ﬂat edges (or surfaces) is hardly
applicable to a real-life gas container, and chaos thus inevitably
arises under a strictly deterministic law of motion (deterministic
chaos).
Now, consider a law of motion that is non-deterministic but
nearly deterministic (with a non-zero but small dispersion), i.e.,
quasi-deterministic. Let us also re-impose the condition of perfectly
ﬂat edges and perfectly rectangular corners. For simplicity, let us
consider only uncertainty of the law of motion but not uncertainty
of the initial position. (It is readily generalized to the case in which
both uncertainty of the law of motion and uncertainty of the initial
position join forces to give rise to chaos). The trajectories are
conﬁned to the interior of a sharply pointed cone, as explained
above. Each time upon hitting a perfectly ﬂat edge, the trajectory
cone is bent but its shape is well preserved. However, when the
range of uncertainty of the trajectory cone happens to encompass
two perpendicular edges around the same rectangular corner, the
trajectories within the cone may hit either edge. As a consequence,
the cone will be temporarily split into two sub-cones going in
roughly opposite directions, but they will usually recombine back
into a single cone after a second subsequent reﬂection (Fig. 4C).
Again, depending on the geometry of the table, the two sub-cones
may remain separated for a few subsequent reﬂections. In any case,
the envelope cone continues to diverge at the same rate with the
passage of time (linear divergence). The situation is non-chaotic.
If the corners are not strictly rectangular and the trajectory cone
bounces on two different but adjacent edges in the same corner, the
cone will be split into two sub-cones, which are unlikely to
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
689

recombine into a single one after the next reﬂection (Fig. 4D).
Subsequent reﬂections around non-rectangular corners tend to
shatter these sub-cones into topologically disjointed (non-contig-
uous) pieces (i.e., sub-sub-cones, sub-sub-sub-cones, etc.). Thus,
the
divergence
is
ampliﬁed
exponentially,
or
even
supra-
exponentially (see later). The end result is similar to what tran-
spires in deterministic chaos. For lack of a better term, this will be
referred to as quasi-deterministic chaos. However, reversal of mo-
mentum is extremely unlikely to result in exact retracing of the
original trajectory. This is of course because the bent-over trajec-
tory cone continues to diverge and offer an inﬁnite number of
possible trajectories. The trajectory that actually materializes is
extremely unlikely to retrace, in the reverse direction, the original
trajectory (prior to momentum reversal), which was only one
among an inﬁnite number of possible trajectories.
In enacting momentum reversal, the probability of retracing the
original trajectory is further diminished after each collision within a
non-rectangular corner if the trajectory cone happens to encom-
pass two adjacent perpendicular edges. This probability is drasti-
cally diminished for the same reason that is responsible for the
emergence of deterministic chaos: the possible trajectories tend to
cover very different regions of the phase space and tend to be
scattered all over it rather than clustering together. In other words,
situations that generate chaos are very effective in magnifying
trajectory uncertainty and causing irreversibility, because the
quasi-deterministic law inevitably generates new path uncertainty
even if there is no uncertainty in the initial position and mo-
mentum. In contrast, collisions cause no such a “compounding”
effect in deterministic chaos: uncertainty appears only at the very
beginning, and no new trajectory uncertainty is introduced after
each collision. The above comment regarding collisions with a
“corrugated” edge in the case of deterministic chaos also applies to
quasi-deterministic chaos, and will not be repeated here. Thus,
unlike deterministic chaos, quasi-deterministic chaos is irreversible
even on the microscopic scale.
So far, we have deliberately ignored collisions between billiard
balls, by limiting their number to one or two. Now, let us lift this
restriction and consider the motion of a number of billiard balls
under the control of a quasi-deterministic law of motion, but re-
impose the condition of rectangular corners and perfectly ﬂat
edges. As long as collisions between balls are rare compared to
collisions with the edges of the billiard table, the situation remains
non-chaotic. Thus, for a sufﬁciently brief time interval and a sufﬁ-
ciently small number of colliding balls, the trajectory cones of two
initially nearby balls will diverge linearly at the same rate and will
be bent roughly the same way in space. Therefore, the position of
spatially clustered balls will remain spatially clustered in the phase
space. Likewise, the momentum of “spatially” clustered balls will
remain “spatially” clustered in the phase space. That is, the dis-
persions of positions or momenta will not ruin the clustering of
these parameters in the phase space.
Reversing all of the momenta will lead to a subsequent time-
evolution that is almost indistinguishable from time reversal,
because continuing diverging of trajectories will not signiﬁcantly
ruin the spatial proximity (clustering) of positions and momenta in
the phase space. Figuratively, it is difﬁcult to tell the difference be-
tween a thin divergent cone, a mathematical line, and a thin
convergent (or, rather, inverted) cone. In other words, for a short
time interval, de-clustering of billiard balls looks like re-clustering,
unless they are color-coded and made individually distinguishable.
As a consequence, the principle of microscopic reversibility will
appear to be approximately valid. However, the replacement of a
divergent cone with a convergent cone upon time reversal breaks
the symmetry between the past and the future, in Prigogine's words
[Prigogine,1997]. Thus, strictly speaking, the motion of an individual
ball is not reversible, but the irreversibility is not readily detectable
by inspecting the trajectory of an individual ball or just a few balls.
As the number of billiard balls increases, the frequency of col-
lisions between balls, as compared to that of collisions with the
table edges, will also increase, and it can no longer be ignored.
Collisions between balls are always chaotic because the situation is
akin to collisions with a highly “corrugated” edges. This is also true
for collisions between balls under a deterministic law of motion, as
vividly depicted in Fig. 38 of Prigogine and Stengers' book Order Out
of Chaos [Prigogine and Stengers, 1984] (reproduced in Fig. 5A). A
head-on collision between two balls, either of the same size or of
different sizes, results in reversal of their momenta, much like a
collision with a perfectly ﬂat surface at an exactly perpendicular
incidence angle, whereas a grazing collision results in a slight
change of speed and a slight deviation of the direction of their
respective trajectories, much like a collision with a perfectly ﬂat
surface at a very shallow angle of incidence.
Thus, after a collision, if the shape of the ball is perfectly
spherical, parallel trajectories becomes enveloped by a divergent
Fig. 3. Trajectories of a billiard ball bounced off a ﬂat surface under the control of a hypothetical quasi-deterministic law of motion. The trajectories are enclosed inside a sharp cone
with its boundary surface indicated by the red and the blue lines. The trajectories upon time reversal are enclosed in an ever-expanding cone with its boundary surface indicated by
the dotted red and dotted blue line. If the cone is extremely sharp, the naked eye may not be able to distinguish it from the trajectories under the control of a strictly deterministic
law of motion.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
690

cone (Fig. 5A), whereas a trajectory cone becomes considerably
more divergent, i.e., a thin cone may suddenly become a fat one
after the collision (Fig. 5B). Consequently, the trajectories of two
nearby balls do not usually remain nearby after a collision with
another ball of the same or different kind, and a cluster of nearby
balls becomes rapidly “de-clustered.” In other words, a collision
between balls shatters the initial cluster spectacularly, just like
what happens in a real-life billiard ball game at the onset. This
conclusion is also valid if some or all of the colliding balls are not
perfectly spherical. As a consequence, spatially nearby balls no
longer remain approximately nearby to one another after a colli-
sion; this is true whether the law of motion is deterministic or
quasi-deterministic. In the quasi-deterministic case, momentum
reversal will no longer give rise to approximately the same
outcome as the time reversal does. Irreversibility will now become
more and more noticeable, and the validity of the approximation of
microscopic reversibility rapidly breaks down.
Now, let us examine Bricmont's explanation of irreversibility in
terms of many degrees of freedom. From the above discussion,
increasing the number of gas molecules in an ensemble increases
the likelihood of collisions between gas molecules, thus hastening
the emergence of chaos. When two different gases are allowed
to mix, mixing follows closely the onset of chaos. Mixing or
unmixing of a small number of molecules undergoing Brownian
motion is not apparent, since it is difﬁcult to distinguish, by means
of a simple and unsophisticated observation, the difference be-
tween unmixing and ﬂuctuations of density. However, increasing
the number of molecules (and, therefore, the degrees of freedom)
facilitates differentiation between mixing and unmixing, but it does
not guarantee irreversibility, since deterministic chaos can be
reversed by momentum reversal. Apparently, Bricmont confused
deterministic chaos with irreversibility despite his claim that chaos
is not related to irreversibility.
In spite of the similarity in nomenclature, quasi-deterministic
chaos does not depend on deterministic chaos (or absolute deter-
minism) for its validity; chaos is not a monopoly of deterministic
physics. Actually, quasi-deterministic chaos is more general than
deterministic chaos, and it includes the latter as a special case. In
other words, combining uncertainty of initial conditions with a
quasi-deterministic
law
of
motion
bolsters
chaos.
A
quasi-
deterministic law of motion inevitably gives rise to uncertainty of
future boundary (initial) conditions, and the uncertainty is greatly
ampliﬁed after collisions between balls or collisions of balls with
non-rectangular corners or corrugated edges, even if the initial
condition has been strictly certain. Obviously, quasi-deterministic
chaos engenders a faster trajectory divergence than deterministic
chaos. As compared to deterministic chaos, quasi-deterministic
chaos ampliﬁes trajectory divergence supra-exponentially rather
than just exponentially. Consequently, a quasi-deterministic law of
motion is more robust in bringing about chaos than a deterministic
one, and it appears to be a requirement for generating irrevers-
ibility. Thus, the Poincare recurrence time, which was originally
deﬁned for classical mechanics, becomes vastly prolonged if it is
modiﬁed for the quasi-deterministic case.
It is important to recognize that deterministic chaos alone is
insufﬁcient to generate macroscopic irreversibility in view of the
requirement of one-to-one correspondence of strict microscopic
reversibility. Thus, contrary to Bricmont's claim, chaos does not
Fig. 4. Deterministic and quasi-deterministic chaos. Trajectories of a billiard ball undergoing elastic collision with the edges of a billiard table are considered in four different cases.
The four corners in the table are strictly rectangular in (A) and (C), but non-rectangular in (B) and (D). In (A) and (B), the law of motion is absolutely deterministic. Two different
initial positions, with identical initial momenta, indicate the uncertainty of the initial position. In (C) and (D), the law of motion is quasi-deterministic with a non-zero dispersion;
there is no uncertainty in the initial condition (position and momentum). The deviations of trajectories originate solely from a quasi-deterministic law of motion, and is conﬁned to
the interior of a cone. The cone is the “envelope” of all permissible trajectories, and is delineated, in the diagram, by two trajectories, in red and in blue, respectively. The uncertainty
of the initial position, in (A) and (B), and the divergence of the cone, in (C) and (D), are exaggerated so as to enhance the readability of the diagrams. Assuming comparable uniform
speeds in all four cases, the positions after the passage of a ﬁxed time interval and ﬁve consecutive reﬂections, are indicated by the tip of the end arrows. In (A), the trajectories
remain parallel most of the time, but they occasionally assume anti-parallel directions for a brief moment. The divergence of trajectories in (C) is increasing linearly with the passage
of time. Although the envelope cone in (C) is occasionally shattered by reﬂections near a rectangular corner, the same cone is restored after two consecutive reﬂections. Therefore,
the trajectories in (A) and (C) are non-chaotic. In contrast, the parallel or the cone-shaped envelopes in (B) and (D), respectively, are “shattered” after two consecutive reﬂections at
different edges around a non-rectangular corner. Chaos thus arises in (B) (deterministic chaos) and (D) (quasi-deterministic chaos). (Reproduced from [Hong, 2003b]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
691

strengthen the deterministic world view. But most importantly,
strict microscopic reversibility cannot give rise to macroscopic
irreversibility,
whereas
microscopic
quasi-reversibility
can.
Although Boltzmann's theory did not challenge classical mechanics,
its validity does not depend on strict microscopic reversibility.
Refusal to subscribe to a deterministic law of motion results in no
loss in the content of Boltzmann's theory, but, instead, it eliminates
the contradiction with reality. The objection raised by Boltzmann's
detractors becomes irrelevant. As a bonus, the time-reversal illu-
sion also vanishes. Sanity of our world view is thus restored.
Actually, time-reversal symmetry has been broken even at the
microscopic level, but the breakdown of symmetry becomes con-
spicuous only at the macroscopic level. In Prigogine's terminology,
time-reversal symmetry has been broken at the trajectory level of an
individual particle (individual description), but it becomes patently
obvious only at the ensemble level of many particles (statistical
description). Thus, a legitimate physical law of motion must be quasi-
deterministic. A strictly deterministic law of motion inevitably leads
to contradictions with physical reality, namely macroscopic irre-
versibility. A small but non-zero dispersion of the law of motion
rescues it from these contradictions without ruining the predict-
ability of the mean.
Thus,
the
combined
effect
of
microscopic
irreversibility
(intrinsic irreversibility due to the quasi-deterministic control law)
and chaos can account for macroscopic irreversibility, as was ﬁrst
demonstrated by Prigogine and coworkers both analytically and in
terms of computer simulations [Prigogine and Stengers, 1984;
Nicolis and Prigogine, 1989; Prigogine, 1997]. Bricmont's argu-
ment was intended to refute Prigogine's idea. Ironically, we have
demonstrated that Bricmont's argument, if enunciated logically,
actually supports Prigogine's idea. Thus, the above chaos argument
does not contradict Boltzmann's theory but rather strengthens it.
In the above discussion, a potential source of confusion is the
meaning of the word “prediction.” A predictable outcome need not
be strictly deterministic. A well-deﬁned control law that exhibits
relative determinism (i.e., a quasi-deterministic control law) satis-
factorily predicts the outcome, in the conventional sense. Thus,
Newtonian classical mechanics predicts the mean values of position
and momentum fairly accurately (the dispersion is easily masked
by the imprecision of measurements or observations), thus giving
rise to the impression or illusion that the law is strictly determin-
istic. However, Newton's law of motion makes no mention what-
soever of the variance or dispersion. Moreover, Newton himself was
not known to be a determinist. Likewise, the Schr€odinger equation
predicts the wavefunction deterministically, but not the position
and momentum. The position is described by the probabilistic
density function that is obtained by multiplying the wavefunction
and its complex conjugate; the certainty of specifying momentum
is limited by Heisenberg's uncertainty principle. Thus, quantum
mechanics explicitly acknowledges the dispersion, but classical
mechanics has been misconstrued as deterministic in the absence
of any mention or discussion of the dispersion by Newton himself.
Apparently, Laplace was an overzealous self-appointed spokes-
person on behalf of Newton. Incidentally, Popper believed that
classical Newtonian mechanics is in principle indeterministic
([Popper, 1982], p. 126).
Thus, the notion of time reversal or time invariance speciﬁcally
refers to the mean value of the position and momentum in classical
mechanics. In other words, the temporal mapping of the mean
position and momentum in classical mechanics is strictly a one-to-
one correspondence. In contrast, the dispersion in classical me-
chanics is not time-reversible, because time reversal of a divergent
cone produces a convergent cone (or, rather, an inverted cone with
its tip pointing in the direction of the future), thus ruining the time-
reversal symmetry. In most discussions of microscopic reversibility
using the billiard-ball metaphor, attention is usually paid to the
predictability of the mean position and momentum, to the neglect
of possible dispersions. As a consequence, the microscopic event is
misconstrued as reversible.
The difﬁculty imposed by false dichotomies, such as strict
determinism vs. complete randomness, strict reversibility vs. irre-
versibility, reﬂects what Francis Bacon meant by “the ill and unﬁt
choice of words wonderfully obstructs the understanding” (pris-
oner of words phenomenon). Subsequently, one must then pay the
price by trying hard to awkwardly explain away the apparent
paradox of generating macroscopic irreversibility from microscopic
reversibility; skeptics so far remain unconvinced. The apparent
time-reversal of the equation of motion is thus just an illusion
made possible by neglecting the accompanying dispersions. In
other words, Einstein's remark that “time is an illusion” may be by
itself an illusion, which Einstein seemed to have retracted towards
the end of his career (see discussion in [Prigogine, 1997]; cf.
Ref. [Schulman,1997]). This kind of illusion is more likely to happen
with well-deﬁned control laws than with stochastic control laws
(see Feynman's discussion of the radioactive beta decay [Feynman,
1967], p. 110). By explicitly addressing the predictability of mean
Fig. 5. Chaos arising from collisions between molecules. (A) The law of motion is absolutely deterministic, but there is uncertainty of the initial condition. The range of uncertainty is
indicatedbya blue solid ball and a redhollowball, with the same initial momenta but slightly different initial positions. Deﬂections fromslightly different contact points on a big balllead
to divergent trajectories, thus resulting in deterministic chaos after many collisions. (B) The law of motion is not absolutely deterministic, but it has a small dispersion (quasi-deter-
ministic). The initial position and momentum are precisely deﬁned. Initially, the dynamic tolerance of the control law allows the trajectories to be spread within the conﬁne of a sharply
pointed envelope cone. After the ﬁrst collision with a big ball, the trajectories are now conﬁned to a signiﬁcantly more divergent cone. In the successive collisions, different big balls are
involved, and the envelope cone is eventually “shattered,” thus leading to quasi-deterministic chaos. (Reproduced and modiﬁed from [Prigogine and Stengers, 1984]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
692

values and the unpredictability of dispersions, and by explicitly
distinguishing uncertainty in the initial conditions from uncer-
tainty in the control laws, clarity of the thinking process can be
maintained throughout the discussion, and confusion can be
avoided.
So far, the above discussion implies that chaos appears only on
the macroscopic scale. This is not really so. Nanotechnology has
brought the attention of investigators to chaos on the scale where
quantum mechanics is the relevant law of motion [Gutzwiller,
1992; Porter and Liboff, 2001]. Investigations of nano-transistors
often require modeling with “particles in a box” with stationary
or vibrating boundaries; both quantum and semiquantum chaos
have been found on the atomic scale. Park and coworkers [Park
et al., 2000] studied unimolecular transistors using moving bucky
balls (buckminsterfullerene, C60) to trap electrons, and have
observed semiquantum chaos. Diggins and coworkers [Diggins
et al., 1994] have also found semiquantum chaos in their study of
a superconducting quantum-interference device (SQUID).
With the beneﬁt of hindsight, it is clear why predictions of an
eclipse of the Sun or the Moon are much more reliable than long-
term weather forecasting. In the Sun-Moon-Earth three-body
problem, collisions between these three celestial bodies have
never happened so far, and collisions of these celestial bodies with
asteroids have hardly caused any signiﬁcant change of momenta
due to size (mass) disparity. The situation is non-chaotic. Owing to
the small number of planets, it is feasible to make corrections in
the prediction of their trajectories due to perturbations from other
planets. In contrast, weather changes are primarily motion and
collisions of nitrogen, oxygen and water molecules, and chaos is
the predominant feature mainly because the irregularity of the
surfaces being bombarded with these gas molecules is matched by
the molecular dimension of these gases, as explained above. The
huge number of gas molecules makes chaos-generating mutual
collisions quite common, thus hastening the onset of chaos.
Weather forecasting is not just a formidable many-many-body
problem; chaos renders it intractable for two reasons. The ﬁrst
reason is well known: inability to ascertain the initial condition to
a high degree of accuracy in computer simulations. The second
reason is the quasi-deterministic nature of the law of motion: the
time-evolution of the weather condition itself is not strictly
deterministic.
In retrospect, had Prigogine not emphatically and categorically
abandoned the notion of trajectories, his idea of microscopic
irreversibility would have been more palatable for Bricmont and
other detractors to swallow. The detractors obviously have found
that the notion of trajectories is a valid one in macroscopic systems
that are in relatively good isolation (cf. Bohm's analysis). On the
other hand, the notion of trajectories of gas molecules in long-
term weather forecasting is not useful, if not meaningless. Again,
a dynamic gray scale of determinism is sorely needed to accom-
modate peculiar situations near both extremes as well as situa-
tions in between.
Bricmont admitted that he could not prove (absolute) deter-
minism, but he also hastened to point out that his opponents could
not prove indeterminism either. Indeed, Laplace's doctrine can
neither be proven nor disproved, as explained in Sec. 3.1.
4. Biological determinism vs. quasi-determinism
Absolute physical determinism implies absolute biological
determinism unless dualism is to be revived. I believe that the
analysis in Sec. 3 should serve to afﬁrm physical quasi-determinism
once and for all. Our hands are now free to tackle the next problem
of biological determinism. Physical indeterminism does not auto-
matically imply biological indeterminism, as explained in Sec. 2.1.
We need to examine biological determinism vs. indeterminism
now. We shall consider, separately, events that are taking place in
intracellular space, in the membrane phase, and interactions be-
tween cells [Hong, 2005a].
4.1. Randomness in intracellular space
Conrad [Conrad, 1990] was among the ﬁrst to emphasize the
importance of hierarchical organization in biological information
processing. He ﬁrst considered a two-level system: the intracellular
level and the intercellular level above it. From the vantage point of
my own research, I have added an intermediate mesoscopic level,
because information processing inside the membrane phase ex-
hibits a rich dynamics [Hong, 1998b]. The mesoscopic processes
take place at the plasma membrane as well as at the internal
membrane systems d mitochondria inside animal cells and chlo-
roplasts inside plant cells. On these three levels of information
processing, it is evident that a living organism recruits a large
number of random or semi-random processes for information
processing, with diffusion being ubiquitous both intracellularly and
extracellularly as well as in the membrane phase (two-dimensional
lateral diffusion and transmembrane diffusion). Even a biochemical
reaction involving two or more macromolecules is not an outcome
of simple diffusion and collision. A two-dimensional diffusion
process takes place when two or more encountering macromole-
cules explore each other before ﬁnal docking and consummation of
the reaction (the reduction-in-dimensionality effect alluded to by
Adam and Delbrück [Adam and Delbrück, 1968]).
The phenomenological control laws governing the input-output
relationship in biological information processing cover almost the
entire gray scale of determinism, ranging from highly random to
highly deterministic d but never absolutely deterministic. The
control laws are often time-dependent and/or environment-
dependent. Both exogenous noise (from the environment) and
endogenous noise are involved.
In dealing with the nature of “noise,” we must address the
following questions. Does noise enter biological information pro-
cessing because it is an inevitable outcome owing to participation
of biochemical reactions in the process and environmental side
effects? Or, does noise enter biological information processing
because it is actively recruited (by evolution) to participate in
biological processes? These are questions that we shall consider in
the following analysis.
First,
let
us
consider
microscopic
dynamic
processing
(biochemical processes in the cytoplasm). Numerous branching and
interconnecting biochemical pathways form intricate networks in
various
subcellular
compartments;
many
biochemical
in-
termediates
(metabolites)
are
shared
by
several
different
biochemical pathways. Each of these shared intermediates presents
a bifurcation point in the chain of events.
Consider a metabolite X, which participates in several different
branches of alternative pathways with different but comparable
probabilities, which are related to the respective activation energy
of these alternative reactions. If an enzyme catalyzes one of these
pathways, the probability of metabolite X being channeled into this
particular pathway toward metabolite Y becomes enhanced at the
expense of the alternative routes. Under catalysis, the destiny of
metabolite X being converted to Y becomes much more deter-
ministic but not completely error-free, because small fractions of X
still go through the alternative uncatalyzed pathways d called side
reactions in chemistry jargon. An additional source of uncertainty
arises from the presence of reverse reactions. In general, the con-
version from X to Y is never complete even if all side reactions are
negligible and neglected because some Y is converted back to X in
the process of reaching a chemical equilibrium d unless the product
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
693

Y is quickly consumed and converted to another metabolite Z
through yet another pathway, which is connected in series further
downstream.
In summary, the phenomenological control laws of biological
information processing in microscopic dynamics are not strictly
deterministic. The control laws range from highly random, if
uncatalyzed, to highly deterministic with residual “error,” if cata-
lyzed. Enzyme catalysis constrains and curtails randomness and
pushes the control laws closer towards the deterministic end on the
gray scale of determinism.
It is of interest to note that translation of structural genes is one
of the most deterministic events in microscopic dynamics. This
relatively strict determinism applies as well to protein synthesis,
which has assorted mechanisms for the disposal of protein folding
faults. However, there is a notable exception: some “soft-wired”
organisms allow for two alternative pathways of RNA processing
[Herbert and Rich, 1999].
4.2. Randomness in the membrane phase and extracellular space
Regarding the intrusion of randomness, the mesoscopic dy-
namics of the membrane phase and the macroscopic dynamics of
the extracellular phase do not fare better than the microscopic
dynamics. Mesoscopic reaction networks are only loosely main-
tained in the ﬂuid environment of the membrane. The bioenergetic
coupling of electron transport to phosphorylation is mediated by a
mesoscopic state of transmembrane proton gradients, where
randomness enters the process and a bifurcation point exists for
protons to diffuse either to the phosphorylation site or to the bulk
cytoplasmic region. From the point of view of designing a fuel cell,
letting the majority of transported protons diffuse into the bulk
cytoplasm instead of going directly to the phosphorylation site
would be costly leakage and an obvious design ﬂaw (Paciﬁc Ocean
effect). However, this imagined loss through leakage is curtailed in
actuality by a speciﬁc mechanism of enhanced lateral proton
mobility [Gabriel and Teissie, 1996]. In other words, the enhanced
proton diffusion towards the phosphorylation site is no longer a
random search in a three-dimensional space, but rather a heuristic
search that is conﬁned to the two-dimensional membrane surface
(reduction-of-dimensionality effect [Adam and Delbrück, 1968]). In
other words, it is somewhat deterministic, but it is not absolutely
deterministic.
Let us now turn our attention to Conrad's macroscopic dynamics
exempliﬁed by synaptic interactions in the neural networks. This is
the very kind of biological process that some neuroscientists have
claimed to be directly responsible for determinism of the brain
functions (neural substrates for mental functions). In fact, the most
impressive deterministic act in intercellular interactions is the
generation and propagation of nerve impulses (action potentials).
One of the ﬁrst principles which novice neuroscience students
learn is the all-or-nothing principle: either a full-ﬂedged action
potential appears, or it does not appear at all, and there is no such
thing as a half-strength action potential, i.e., action potentials
exhibit the nature of a digital signal, either 0 or 1. This feature
ensures that the Morse code-like signals can propagate over a long
distance faithfully without signal distortion or signal loss. All other
non-action-potential
electric
disturbances
of
the
membrane
remain local and non-propagatable. That is to say, nerve impulse
propagation is a strictly digital process.
However, the Morse-code analogy is ﬂawed. The nerve impulses
not only transmit a simple “on-off” signal but they also carry
additional detailed messages, which include coding concerning the
particular type of information d the modality d and the intensity
of a sensory signal or the intensity of a motor signal for muscle
contraction. The modality is coded in the speciﬁc wiring directed
towards the destination, whereas the intensity is coded in terms of
the frequency of the nerve impulses, a sort of frequency-modulated
(FM) signal. The speciﬁc wiring makes the process somewhat
deterministic. The intensity is coded in an analog fashion, thus
allowing noise to be present without rendering the process
completely random.
In spite of many intervening random events, generation and
propagation of action potentials remain fairly deterministic. The
generation process is a major bifurcation point. When the mem-
brane potential is below a threshold (the membrane potential
negative on the cytoplasmic side), the electric event remains local
and cannot propagate. In contrast, when the membrane potential is
above the threshold the electric event reaches a point of no return
and morphs into a full-ﬂedged, propagatable action potential.
The membrane potential is controlled by a delicate balance
between two opposing ionic currents. The electrochemical poten-
tial energy gradient across the membrane tends to drive a Naþ
current into the cell but to drive a Kþ current out of the cell at the
resting state when the nerve membrane is not excited. When the
nerve membrane is excited and the membrane potential rises
above the threshold, the voltage-dependent Naþ channels in the
nerve membrane suddenly open, and the Naþ conductance in-
creases rapidly, within a millisecond, to a peak value (a process
called sodium activation) and then declines to its basal value in a
matter of a few milliseconds (called sodium inactivation) [Hodgkin
and Huxley, 1952] (Fig. 6D).
As a consequence of sodium activation, the inward-going Naþ
current drives the membrane potential up so that the membrane
potential on the cytoplasmic side becomes positive. Upon sodium
inactivation, the Naþ current gradually turns off. Concurrently, the
Kþ channels open up and the Kþ current increases, and its outward-
going ﬂow helps drive the membrane potential back to its normal
resting state (negative on the cytoplasmic side). The delayed acti-
vation of Kþ channels is well orchestrated so that it allows the ac-
tion potential to develop to its full scale and not turn it off
prematurely (Fig. 6B). The delayed activation kicks in at the right
time to accelerate the recovery to the resting state.
Both the Naþ current and the Kþ current follow a well-deﬁned
time course, i.e. quasi-deterministic (Fig. 6B and D). The ionic
currents shown in Fig. 6 reﬂect the collective action of thousands
of Naþ channels and Kþ channels. Therefore, in Conrad's termi-
nology, these collective actions are considered macroscopic
events, in spite of the fact that individual nerve cells (neurons) are
visible only with the help of microscopes. Of course, these events
are macroscopic in the context of quantum indeterminacy. The
events which take place at the individual ion channels may also
be macroscopic in the context of quantum indeterminacy, but
they are mesoscopic in the context of Conrad's classiﬁcation of
hierarchical biological information processing because of their
location of occurrence and because of their spatial scale. The
events in the mesoscopic dynamics take place in the membrane
phase and intervene between the intracellular microscopic dy-
namics of biochemical reactions in the solution phase inside the
cell and the macroscopic dynamics at the intercellular level. The
imprecise use of natural language in these descriptions should not
confuse us when we shift from one context of discussion to
another.
In addition to a direct action on ion channels to excite electri-
cally the post-synaptic membrane of the nerve cells connected at
the downstream side, neural signals also affect intracellular
microscopic events in the postsynaptic nerve cell via G protein-
coupled non-electrical processes and second messengers [Gilman,
1984]. Cyclic AMP, which is a key second messenger and a diffus-
ible component, participates in numerous biochemical pathways,
especially those of signal transduction. Its role is greatly enhanced
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
694

by the switching action which it exerts on biochemical reaction
pathways and on ion channel functions (the ampliﬁcation mecha-
nism): Cyclic AMP initiates phosphorylation of many enzymes
[Bourret et al., 1991] and ion channels [Levitan, 1994; Hilgemann,
1997], indirectly via the activation of cyclic AMP-dependent pro-
tein kinase. In other words, cyclic AMP is the key second messenger
that exerts its effect by converting highly random control laws into
highly deterministic control laws. In nonlinear dynamics jargon,
cyclic AMP regulates bifurcations. Similar comments apply to the
action of Ca2þ.
The control laws at various levels of the neural hierarchy can be
further affected by bifurcation points encountered in non-neural
processes. For example, the supply of molecular oxygen strongly
affects the supply of ATP, which is the “fuel” for the majority of
molecular machines. Yet, among many different factors, the supply
of oxygen depends on its transport from the lungs to tissues via the
cardiovascular system, which is comprised of a large number of
bifurcations (tree-like branchings). The distribution of molecular
oxygen to various body regions via the branching vascular trees is
under neural control (autonomic nervous system). The vasculature
can be regarded as an extension of the macroscopic (neural)
network dynamics. Similarly, the hormonal system regulates and
coordinates the functions of various organ systems, in a network-
like fashion. The action of hormones is even less deterministic
than the neural control, since hormones are diffusible and are
distributed by blood circulation.
Of course, life processes are not always at the vicinity of a major
bifurcation point. There are many stable processes that are gov-
erned by a negative feedback scheme (i.e., point or periodic
attractors in nonlinear dynamics jargon) and are insensitive to
changes of initial or boundary conditions (concept of homeostasis
in physiology). The lack of sensitivity is made possible partly by the
creation of a somewhat isolated internal environment for the ma-
jority of cells (Claude Bernard's milieu interieur). Thus, bifurcation
points are present from time to time and from place to place, but
not continuously and not ubiquitously.
There
are
many
case
reports
about
genetically
identical
(monozygotic) twins that were reared separately since birth
[Farber, 1981]. The similarity, not only of characteristic facial fea-
tures but also of mannerisms and personality, was often unnerv-
ingly striking. Randomness seems to play a minor role. But the
effect of environmental factors should not be overlooked. Here, we
are not raising the issue of identical twins raised in very different
socioeconomic environments. For example, even if identical twins
are raised under the same roof and under the supervision of the
same parents, if one of the twins exhibits more domineering
behavior than the other, and the other succumbs to the trend
accordingly, the relative dominance between the two can become
permanent by a mechanism known as positive feedback in physi-
ology jargon or a vicious cycle colloquially.
Perhaps, the most crucial moment of bifurcation is life and death.
Consider the moment of the sudden onset of cardiac ﬁbrillation. The
difference between life and death is critically dependent on the
subtle body conditions of the patient (nutritional status, bodily
defense mechanisms, general conditions, etc.) as well as random-
ness incurred by external agents, e.g., lack of an available ambu-
lance, delayed arrival of an ambulance caught in a trafﬁc jam,
malfunction of a deﬁbrillator, incompetence of the rescue team,
which could be traced remotely to a dysfunctioning educational
system, further aggravated by a crazy policy put into action by a
small elite group of dumb high-achievers, for example. Through this
lengthy chain of events, coupling of a sensitive bifurcation point to
superﬁcially unrelated external and uncontrolled factors lead to an
irrevocable outcome. Here, randomness exerts a great deal of
inﬂuence because it is a major bifurcation point: life or death.
4.3. Endogenous noise
Up to this point of the discussion, it appears that intrusion of noise
is an inevitable side effect of recruiting chemical reactions in life
processes. It also appears that noise is “tolerated” but is also con-
strained, by means of phenomenological control laws at various hi-
erarchical levels of biological information processing. However, for
certain processes, noise appears to be an essential component, and it
has been actively recruited by evolution to enhance the efﬁcacy of
biological information processing. The most intriguing example is
the use of background noise to enhance the reception of weak signals
(a phenomenon known as stochastic resonance), as documented in
the crayﬁsh tail fan mechanoreceptor [Wiesenfeld and Moss, 1995;
Moss and Wiesenfeld, 1995]. The signals generated by low-grade
vibrations of water when a predator approaches a crayﬁsh are
rather weak. However, the presence of background noise (either
generated internally or imposed externally) actually enhances the
crayﬁsh's reception of the vibration signals because the detection is
based on a nonlinear sigmoidal threshold control law.12
In the language of nonlinear dynamics, the detection threshold
presents a bifurcation point, i.e., the juncture of a positive feedback
(self-reinforcing) and a negative feedback (self-curtailing) regime.13
The presence of background noise at times brings down a borderline
stimulus below the threshold, but at other times it pushes the
stimulus over the threshold. If wewere to subscribe to mathematical
idealization, the threshold would be a level of unstable equilibrium:
a point where the system neither turns on nor turns off d which is
what is supposed to happen at a bifurcation point in the mathe-
matical sense. In real life, a stand-off at a threshold seldom happens,
and the unstable equilibrium can seldom be maintained because the
precarious equilibrium is almost always ruined byﬂuctuations of the
membrane potential and/or ﬂuctuations of the threshold level itself,
and by additional noise incurred by external agents such as factors
causing ﬂuctuations of the body temperature (which, in turn,
changes the rate of biophysical and biochemical processes).
Experimental evidence indicates that stochastic resonance can
also operate at the mesoscopic level; it does not require the inter-
vention of neural signal processing. Bezrukov and Vodyanoy
[Bezrukov and Vodyanoy, 1995] studied a voltage-dependent ion
channel formed by an antibiotic, alamethicin, when it is incorpo-
rated into an artiﬁcial bilayer lipid membrane. They demonstrated
that noise enhances signal transduction mediated by these ion
channels. Thus, noise is not just an unavoidable nuisance, but at
times it can be “beneﬁcial” to the performance of ion channels.
In my opinion, by far one of the strongest cases against absolute
biological determinism is the presence of a probabilistic control law
that governs the opening and closing of Naþ channels (and other
ion channels) in nerve excitation. As mentioned in the previous
section, the Naþ conductance is a macroscopic quantity, and its
constituents are the miniature unitary conductances of numerous
Naþ channels. Do these tiny ion channels open and close, in unison,
with the same time course as that of the macroscopic conductance?
If so, the macroscopic Naþ conductance would simply be an integral
multiple of the unitary conductance. Prior to the invention of the
patch clamp technique by Neher and Sakmann [Neher and
Sakmann, 1976], the behavior of individual ion channels could
only
be
indirectly
inferred
by
measuring
ﬂuctuations
of
12 A sigmoid shaped dose-response curve is typical of a threshold phenomenon d
the hallmark of a good switch. The bifurcation is insensitive to stimuli that are too
weak or too strong. The sensitivity is highest around the inﬂection point of the
sigmoid curve. A chemist who titrates the pH of a buffer solution witnesses the
highest sensitivity around the pK (inﬂection point of a titration curve).
13 For the molecular events of nerve excitation, see a standard textbook of
physiology.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
695

macroscopic ionic currents, such as noise analysis which makes
indirect inferences from the measured noise spectrum d the
magnitude of noise as a function of its frequency [Neher and
Stevens, 1977].
Patch clamp measurements of the unitary conductance of in-
dividual Naþ channels show that the opening and closing of an
individual Naþ channel follows a superﬁcially erratic time course,
and, furthermore, the individual ion channels do not open and close
in synchrony (Fig. 7C) [Sigworth and Neher, 1980]. In fact, the time
course of the unitary Naþ current bears no resemblance at all to
that the macroscopic Naþ current (which combines the contribu-
tion of literally thousands of unitary Naþ currents), as measured by
the conventional voltage clamp method (cf. Fig. 6D). Apparently,
the opening and closing is a digital process and the unitary
conductance is quantized: a channel either closes or opens and the
transition from zero to a ﬁxed value is quite abrupt and appears
intermittently with irregular intervals. Also note that all nine
separate measurements shown in Fig. 7C are not identical in time
course, even though all were elicited by exactly the same stimulus;
the measurements were not reproducible in the conventional
sense. However, if 300 separate measurement records were added
algebraically together and then divided by 300, i.e., by means of a
computer technique called signal averaging, a resultant signal was
obtained as shown in Fig. 7B. This averaged signal now exhibits the
characteristic time course of activation and inactivation, similar to a
macroscopic Naþ current obtained by means of conventional
voltage clamping (cf. Fig. 6D). Thus, the voltage dependence and
well-deﬁned time course of the macroscopic Naþ conductance is
therefore a manifestation of the collective behavior of a large
number of Naþ channels that switch on and off stochastically. Thus,
the opening and closing of Naþ channels is not so erratic as it
seemed at ﬁrst glance, but it is rather a well-orchestrated disorder,
much like the performance of a bell choir. Individual performers
appear to ring their bells randomly and each performer individually
produces no tuneful music. But together they do. What is remark-
able about ion channel ﬂuctuations is the transformation of weak
causality at the mesoscopic level (ion channel ﬂuctuations) into
strong causality (action potentials) at the macroscopic level.
The control law governing the voltage dependence and time
course of the macroscopic Naþ conductance does not manifest
explicitly at the level of individual channels. Rather, the control law
at the channel level is probabilistic (stochastic) in nature, analogous
to the law governing the frequency and severity of earthquakes
[Kagan and Knopoff, 1981] or the law governing the beta decay in
particle physics: individually unpredictable but collectively pre-
dictable. It is well known that individual earthquakes are quite
unpredictable as to time and location. But when the frequency of
earthquakes is plotted against the severity of the quakes, the plot
exhibits a well-behaved curve. It shows that severe quakes are
considerably rarer than minor quakes, thus reﬂecting the gradual
build-up of tension and periodic releases at a geological fault. In
other words, quakes are individually non-deterministic but are
collectively deterministic.
We can now draw a tentative conclusion. The control law gov-
erning the time course of sodium activation and sodium inactiva-
tion is not a new set of control law that emerges at a higher level of
hierarchy, because it is reducible to a lower level control law at the
ion-channel (mesoscopic) level. In other words, we do not need to
postulate a new kind of force. However, doubts linger. Laplace's
doctrine reminds us to dig deeper: Is there an even lower level
control law that can account for the apparently erratic ﬂuctuations
of ion channels? Presently, it is a subject of active investigation.
Still, it may be helpful to conduct another Gedanken experiment.
Suppose eventually we achieve a deeper understanding and can
control the opening and closing kinetics of ion channels through
manipulation
of
assorted
techniques
including
site-directed
mutagenesis. Let us (hypothetically) eliminate ﬂuctuations so that
all Naþ channels respond to a stimulus (a sudden depolarization in
physiology jargon) with the same well-deﬁned time course,
namely, opening abruptly in unison after a ﬁxed time interval t of
delay, following the stimulus (where t could be zero), staying open
for a ﬁxed but short duration, and then closing abruptly (Fig. 8C).
Under such conditions, the macroscopic Naþ conductance would
have exactly the same time course as that of an individual channel,
but it would have a much greater amplitude: a sharp and promi-
nent lone spike with a short duration (Fig. 8D). It is apparent that
the macroscopic Naþ conductance would not rise and fall with the
typical time course of activation and inactivation, as shown in
Fig. 6. The Naþ current and the Kþ current of a neuronal axon measured by the voltage clamp method. A node of Ranvier under voltage clamp is held at 95 mV, hyperpolarized for
40 ms to 120 mV, and then depolarized to various potentials ranging from 60 mV to þ60 mV in 15-mV steps. (A) The normal Naþ current and the normal Kþ current in Ringer's
solution. (B) Same node after external addition of 300 nM tetrodotoxin (a Naþ channel blocker). Only the Kþ current remains. (C) Control measurements in another node. (D) Same
node after external addition of 6 mM TEA (tetraethylammonium; a Kþ channel blocker). Only the Naþ current remains. (Reproduced from [Hille, 2001]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
696

Fig. 6D. Thus, the elimination of factors responsible for the appar-
ently irregular and unpredictable sequence of opening and closing
of ion channels would also render these channels unsuitable for the
generation of “normal” action potentials. That the “abnormal” ac-
tion potential would not work can be made clear by the following
consideration.
Ventricular muscle cells of the heart have an unusually long
plateau phase of depolarization of the membrane potential (about
200 ms), i.e., the potential becomes positive for more than 200 ms,
instead of just a few milliseconds as in a typical nerve cell. Such a
prolonged depolarization phase (known as phase 2) serves a vital
purpose; it makes possible and ensures the chain of events that
eventually leads to contraction of ventricular muscle ﬁbers (exci-
tation-contraction coupling).
Here, it is important to realize that the contraction of a heart
muscle cell is not directly triggered by a depolarization of the
membrane potential, but rather by an elevated intracellular Ca2þ
concentration. A long chain of events links the membrane depo-
larization to an increase of the intracellular Ca2þ concentration. An
initial and modest depolarization of the membrane potential
rapidly activates Naþ channels (called fast channels) in the muscle
membrane. The ensuing massive Naþ inﬂux across the membrane
causes an additional rapid depolarization of the muscle membrane
potential (called “upstroke” or phase 0). The rapid depolarization
then activates the Ca2þ channels in the muscle membrane. The
resulting increase of the Ca2þ conductance leads to an enhanced
Ca2þ inﬂux into the muscle cell (cytoplasm or sarcoplasm) where
contractile elements are located. However, it takes time for the
enhanced Ca2þ inﬂux to raise the intracellular Ca2þ concentration
to the threshold level for muscle contraction: the prolonged acti-
vation of Ca2þ channels during the plateau phase (phase 2) fulﬁlls
this requirement of a long ﬁlling time for Ca2þ.
Although the upstroke, caused by activation of fast Naþ chan-
nels, is required to activate Ca2þ channels, the maintenance of the
enhanced Ca2þ conductance during the plateau phase does not
require the sustained activation of fast Naþ channels. These fast Naþ
channels, like their counterparts in a nerve membrane, become
spontaneously inactivated in a couple of milliseconds, and the
membrane potential repolarizes slightly during phase 1: there is no
Naþ activation during phase 2.
In order to maintain the prolonged activation of Ca2þ channels,
two conditions must be met. First, Ca2þ channels must not
become
inactivated
rapidly like
Naþ
channels.
Second,
the
membrane potential must remain depolarized (positive) for a
long time. This is because Ca2þ channels are voltage-dependent,
and require an elevated potential to keep them open. A prema-
ture repolarization and/or inactivation could diminish the Ca2þ
conductance and abort the excitation-contraction coupling. The
ﬁrst condition is met by the intrinsic property of L-type Ca2þ
channels, called slow channels (where “L” stands for large
conductance and long-lasting duration) [Tsien et al., 1987; Tsien
et al., 1988]. In contrast, T-type Ca2þ channels (where “T” stands
for tiny conductance and transient duration), which inactivate
rapidly, cannot ﬁll the shoes for the task. The second condition
requires an intricate interplay of Ca2þ and Kþ channels: “coop-
eration” of Kþ channels is essential.
Since the Ca2þ inﬂux and the Kþ efﬂux exert opposite effects on
the membrane potential, an almost exact balance of the two ﬂuxes
is required to maintain the plateau phase of depolarization. This
delicate balance is made possible by a concerted interplay of
delayed activation of several different types of Kþ channels [Barry
and Nerbonne, 1996] d each with its own appropriate timing
and time course d and the inherent slow inactivation of L-type
Ca2þ channels. Interestingly, the Kþ conductance actually decreases
about ﬁve-fold during phase 2 as compared to its magnitude during
phase 1; the slight and rapid repolarization during phase 1 is
accompanied by activation of transient outward Kþ channels which
are no longer active during phase 2. The Kþ efﬂux diminishes
during the plateau phase because the outward Kþ current is now
carried, instead, by a type of slow-activating channel called inward
rectiﬁer [Giles and Imaizumi, 1988; Nichols and Lopatin, 1997]. In-
ward rectiﬁer Kþ channels work like a trap door.
To appreciate how inward rectiﬁer Kþ channels work and how
the Kþ ﬂux interacts with the Ca2þ ﬂux, several crucial factors and
events must be kept in mind. Readers who are familiar with these
factors and events may skip this paragraph with no loss of
continuity.
 There are two kinds of “forces” that drive the ion ﬂuxes: (a) the
membrane potential (electrostatic force) which drives both Kþ
and Ca2þ from the side of the membrane with a positive po-
tential to the negative side, and (b) the concentration gradient
that drives a particular ion from the side with a high concen-
tration of that particular ion to the low-concentration side.
 The Kþ and Ca2þ concentration gradients are poised in opposite
directions: Kþ is high inside, whereas Ca2þ is high outside, and,
therefore, the concentration-driven Kþ and Ca2þ ﬂuxes are al-
ways pointing in opposite directions. On the other hand, both Kþ
and Ca2þ are under the control of the same membrane potential
d there is only one membrane potential in a given cell d and,
therefore, the potential-driven Kþ and Ca2þ ﬂuxes are always
pointing in the same direction.
 The magnitude of the potential-driven Kþ and Ca2þ ﬂuxes is
regulated by both the magnitude of the common membrane
potential
and
the
individual
Kþ
and
Ca2þ
conductances,
respectively, whereas the magnitude of the concentration-
driven Kþ and Ca2þ ﬂuxes is regulated by the steepness of the
individual Kþ and Ca2þ concentration gradients and the indi-
vidual Kþ and Ca2þ conductances, respectively.
 The phenomenological control law regulating an ion conduc-
tance is speciﬁc to each type of ion channel. The ion conduc-
tance of Kþ or Ca2þ depends not only on the common membrane
potential but also on an intrinsic property that determines how
fast a given type of ion channel can be turned on or off by the
Fig. 7. Patch clamp records of Naþ currents. A small patch of the rat muscle membrane
was voltage-clamped, and the Naþ current was recorded. Tetraethylammonium (TEA)
was used to block any Kþ channels that were inadvertently included in the patch. A
step depolarization of 10 mV was used to activate the Naþ channels present in the
patch (Record (A) shows the time course of the membrane voltage of the membrane
patch being clamped). The traces in Record (C) show nine separate responses to the
10 mV depolarization. Record (B) is the average of 300 individual responses. (Repro-
duced from [Sigworth and Neher, 1980]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
697

membrane potential and whether it maintains a prolonged
activation under a sustained membrane potential.
Although changes of individual concentrations are gradual,
changes of the common membrane potential can take place rapidly
(in the millisecond range). As a consequence, the direction of some,
if not all, net ion ﬂuxes d a net ﬂux is deﬁned as the algebraic sum
of potential-driven and concentration-driven ﬂuxes d can be
reversed abruptly by changing the common membrane potential.
On the other hand, the concentration-driven Kþ and Ca2þ ﬂuxes can
never be reversed under physiological operating conditions; how-
ever, their magnitude can be changed rapidly by an abrupt change
of conductances, caused by an even more abrupt change of the
membrane potential. Also note that there are complex “circular”
interactions: the change of any ion ﬂux changes the common
membrane potential, which, in turn, changes all ion conductances;
the latter changes cause changes in individual ﬂuxes, thus
completing the feedback cycle (which can be either positive or
negative). However, interludes of a steady state in which all three
parameters d potential, conductances and ﬂuxes d remain con-
stant (barring ﬂuctuations) are possible if the interactions lead to a
negative feedback cycle.
Thus,
when
the
membrane
potential
is
more
negative
than 70 mV, the large negative potential overcomes the trans-
membrane Kþ concentration gradient, resulting in a net inﬂux of
Kþ, with considerable ease because of the large Kþ conductance;
the net Kþ inﬂux is large. When the membrane potential is less
negative than 70 mV, the concentration gradient overpowers the
membrane potential, resulting in a net efﬂux of Kþ with consider-
able difﬁculty because of the diminished Kþ conductance; the net
Kþ efﬂux is rather small. The disparity of the two magnitudes
(inﬂux vs. efﬂux) constitutes the one-way trap door or, rather,
rectiﬁcation. Furthermore, when the membrane potential ap-
proaches zero or turns positive, the Kþ efﬂux becomes even smaller.
In fact, it becomes sufﬁciently small to match the not-so-intense
Ca2þ inﬂux, thus averting a premature return of the membrane
potential to the resting level and making possible the long plateau
phase, which provides sufﬁcient driving force and sufﬁcient ﬁlling
time for the Ca2þ inﬂux to raise the intracellular Ca2þ concentration
so as to ensure sufﬁciently forceful ventricular contraction. The gate
of these same inward rectiﬁer Kþ channels later turns on during
phase 3 (because of a large negative potential), thus hastening the
rapid repolarization.
Had all ion channels been made to open and close abruptly in
unison (as in Fig. 8D), the time course of the ventricular action
potential would be too drastically altered to make excitation-
contraction coupling possible, for the following reasons. Although
the upstroke might still be sufﬁciently rapid and forceful d i.e.,
having a sufﬁciently high slew rate, in engineering jargon d to
activate a sufﬁciently large number of Ca2þ channels, the elimina-
tion of the subtle interplay of various types of Kþ channels and the
drastically shortened duration of Ca2þ channel activation would
abolish the prolonged plateau phase and prematurely initiate rapid
repolarization. The ensuing rapid repolarization would further
diminish the Ca2þ inﬂux since Ca2þ channels depend on a near-zero
or positive membrane potential to sustain their activation. The
rapid repolarization also augments the Kþ efﬂux since inward
rectiﬁer Kþ channels also depend on a near zero or positive po-
tential to maintain a diminished Kþ efﬂux. Once the balance is
tipped in favor of repolarization, as normally occurs in the late
phase 3 of the ventricular action potential, a positive feedback
mechanism returns the membrane potential precipitously to the
resting level (phase 4): repolarization diminishes Ca2þ inﬂux and
enhances
Kþ
efﬂux,
whereas
diminished
Ca2þ
inﬂux
and
augmented
Kþ
efﬂux
further
hasten
repolarization,
thus
completing the positive feedback loop.
Thus, a (hypothetical) synchronized opening and closing of Ca2þ
channels alone would eliminate the prolonged plateau phase even
if Naþ and Kþ channels were to maintain their probabilistic time
course of ﬂuctuations. The situation would become even worse
when all ion channel ﬂuctuations ceased to exist. In other words,
synchronous opening and closing of ion channels in the ventricular
muscle membrane would seriously compromise the function of the
ventricular muscle unless the entire contractile machinery were
radically redesigned. But the latter possibility did not materialize in
evolution even in other animals, to the best of my knowledge.
The above scenario is not merely an imaginary scenario arising
in a Gedanken experiment. In fact, a more gentle experimental
manipulation of ion concentrations so as to diminish the rate of
upstroke actually caused a cardiac ventricular cell to exhibit an
“abnormal” action potential that lacked a plateau phase and actu-
ally looked like that of a normal cardiac atrial cell. For other types of
action potentials with shorter durations than the ventricular action
potential, the (hypothetical) alterations of their time course caused
by the synchronous opening and closing of ion channels might not
be as dramatic as in the heart muscle. However, the subtle effects of
the altered action potential on the neurotransmitter release at the
axonal terminal might propagate beyond the synapse in an insid-
ious way and might then show their impact when the propagating
signal reached another bifurcation point in the subsequent chain of
events.
In fact, an ion channel disease (channelopathy) known as
hyperkalemic periodic paralysis (an autosomal dominant disorder)
is caused by a defective inactivation mechanism of Naþ channels
[Cannon, 1996]. In such a case caused by a point mutation
(Met1592Val), the activation of Naþ channels is normal, but the
channels fail to become inactivated rapidly, resulting in a burst of
reopening activities and prolonged open durations (Fig. 9). We may
surmise from this that the noise exhibited by ion channel ﬂuctua-
tions is endogenous in nature and was recruited by evolution for
speciﬁc functional purposes.
From the “normal” functional point of view, ion channel ﬂuc-
tuations are also a necessity. For the unitary conductance of many
individual Naþ channels of the same kind to collectively give rise to
a well-deﬁned time course of the known macroscopic Naþ
Fig. 8. Schematic showing the time course of the macroscopic Naþ current in the
presence and absence of ion channel ﬂuctuations. Trace (A) is the depolarization step
of the membrane potential that triggers the activation of Naþ channels. Trace (B) is a
typical real macroscopic Naþ current measured by the voltage clamp technique with a
conventional intracellular glass-pipette microelectrode. Trace (C) is a hypothetical
single-channel current measured by means of patch-clamping. The hypothetical
channel opens and closes exactly once upon stimulation with an precise period of
delay, t. Trace (D) is the hypothetical macroscopic Naþ current when the ﬂuctuations
were eliminated and some 300 channels turned on and off in unison. Note the dif-
ference in the vertical scale of Trace D. (Reproduced from [Hong, 1998b]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
698

conductance with appropriate activation and inactivation, channels
must not open and close in unison. Rather, channels must open and
close in accordance with a well-orchestrated control law in such a
way that each channel takes a turn to open and close, so that the
time-dispersion (variable delay parameter t) of the unitary events
collectively gives rise to a highly deterministic time course of
activation and inactivation of the macroscopic Naþ conductance.
Speciﬁcally, most channels must open during the ﬁrst few milli-
seconds, whereas fewer and fewer channels should open later. That
this is indeed the case is evident in a close examination of the nine
traces of opening and closing of Naþ channels shown in Fig. 7C:
opening and closing events in all nine traces take place within the
ﬁrst ﬁve milliseconds, and no opening and closing appears later
than ﬁve milliseconds after the step depolarization.
Ion channel ﬂuctuations are most likely an intrinsic property of
the channels, instead of the consequence of interference by
external independent events, i.e., it is endogenous noise. Partial
randomness is thus an active participant in the process of shaping
the time course of the switching event rather than acting as a
passive bystander or as an unwanted but unavoidable external
intruder. This role is quite different from the role played by noise in
causing incomplete switching of metabolic pathways discussed in
Sec. 4.1. It is not certain whether the incomplete switching of
metabolic pathways is desirable or undesirable for biological in-
formation processing. However, in the case of the Naþ channels,
there is little doubt that partial randomness is indispensable for
normal channel operation.
The mechanism of partial randomness in ion channel ﬂuctua-
tions is unknown. It is generally assumed to arise from conforma-
tional changes between various channel states (at least three states
for Naþ channels: closed, open and inactivated). Using the approach
of nonlinear dynamics, Chinarov et al. [Chinarov et al., 1992] have
shown that the bistability of ion channels can arise from in-
teractions of the ion ﬂux through ion channels with a conforma-
tional degree of freedom for some polar groups lining such pores.
Hanyu and Matsumoto [Hanyu and Matsumoto, 1991] suggested
that
ion
channel
cooperativity
mediated
by
membrane-
cytoskeletons may be the source of randomness. Intriguingly, Lev
et al. [Lev et al., 1993] demonstrated that pores formed in a non-
biological synthetic polymer membrane also exhibit ﬂuctuations
between high and low conductance states. The investigators pro-
posed a mechanism based on the ionization of ﬁxed charges within
a channel or at its mouth of opening [Korchev et al.,1997]. Here, the
main source of partial randomness in channel kinetics is quite local
(endogenous noise), and is not a consequence of intrusion by un-
wanted and unrelated external agents. Of course, ﬂuctuations in
ionization, being a thermal phenomenon, have a distinct but indi-
rect contribution from the environment, via ﬂuctuations of the
body temperature.
4.4. “Constrained” randomness in a hierarchical biosystem
A skeptical inquirer who believes in strong causality often feels
uncomfortable with anything less than strict (absolute) deter-
minism in the control laws. Behaviorism pioneer B. F. Skinner
argued that the prerequisite of doing scientiﬁc research in human
affairs is to assume that behaviors are lawful and determined,
whereas the assertion of an internal “will,” which has the power of
interfering with causal relationships, also makes the prediction and
control of behaviors impossible ([Skinner, 1953], pp. 6e7). In other
words, turning away from determinism is tantamount to aban-
doning the possibility of a science of human behaviors because he
thought that science is supposed to be deterministically predictive.
Earman thought that we were being presented with a false di-
chotomy: determinism versus non-lawful behavior, or determinism
versus spontaneity and randomness ([Earman, 1986], p. 243). In
contrast, Earman indicated that he had seen not the slightest reason
to think that the science of physics would be impossible without
determinism and that denying determinism does not push us over
the edge of the lawful and into the abyss of the utterly chaotic and
non-lawful. Most likely, Earman had relative determinism or quasi-
determinism in mind, though he did not say it explicitly.
Apparently, Skinner was excessively ambitious, since, by his
standards, a century of psychology research would be mostly fail-
ures; efforts to mathematize psychology were rudimentary at best.
By far one of the best examples to refute Skinner's claim is our
recent elucidation of humans' high creativity [Hong, 2013a, 2013b].
In hindsight, Skinner's behaviorism had a negative impact on the
research on human creativity. This does not detract from the
contribution of Skinner. Behaviorism accounts for half of the
reasoning mode: logical reasoning. Gelstalt psychology contributes
to the other half of visual thinking, which is hardly predictable in
advance.
Although it is risky to second-guess someone else's inner feel-
ings, I suspect that the discomfort of advocates of strict deter-
minism might be rooted in the lingering suspicion that small errors,
in the midst of a successive stream of information processing steps,
may eventually lead to drastic “divergence” of the ﬁnal outcome
because of error propagation and subsequent ampliﬁcation along a
long chain of events d the most dramatic example is the Butterﬂy
Effect. Advocates of absolute determinism perhaps feel that the
only sure way to prevent its occurrence seems to be a strict
adherence to absolute determinism except perhaps at the stage of
input or output, as in digital computing, lest the “slippery slope”
may carry the case to the unwanted extreme.
What
rescues
biological
information
processing
from
this
perceived or imagined disaster is the nested hierarchical organiza-
tion of information processing. For example, a seemingly highly
random opening and closing event of ion channels at the mesoscopic
level re-converges to a highly deterministic event of nerve impulse
generation. However, it is not that a random event miraculously
reorganizes to become more orderly. Rather, the re-convergence
takes place at a different hierarchical level of biological informa-
tion processing. In other words, the hierarchical organization pro-
vides the constraints to curtail randomness that appears at the lower
levels. Alternation of analog and digital processing d hence alter-
nation of weak determinism and strong determinism d is charac-
teristic of biological information processing.
4.5. Quantum indeterminacy at the biological level
A considerable number of investigators hold the view that the
randomness associated with the microscopic world of atoms and
electrons does not impinge directly upon the problems associated
with biological information processing and suggest that we ought
to look for randomness beyond the microscopic world of atoms and
electrons (see also [Eccles, 1979], p. 236). On the other hand,
Stephen Hawking thought that it is relevant ([Hawking, 1993],
Chapter 12). Edward Teller [Teller, 1998] also thought that quantum
mechanics might leave room for free will to shape the future.
Hermann Haken explicitly acknowledged the presence of quantum
ﬂuctuations and virtually denied absolute physical determinism
(see [Haken, 1983], p. 21). So there has not been consensus even
among major players in modern physics.
Determinism implied in quantum mechanics deserves a special
comment. The temporal mapping of wavefunctions in quantum
mechanics is a one-to-one correspondence, and the boundary
conditions are completely ﬁxed. The time-dependent Schr€odinger
equation exhibits time-reversal invariance, and, as far as the
wavefunction is concerned, the control law of quantum mechanics
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
699

is deterministic. Moreover, time-invariance of the wave equation
implies microscopic reversibility. So, we seem to get back to the
same situation as in classical mechanics.
However, wavefunctions are not directly observable. A deter-
ministic wavefunction describes a probabilistic speciﬁcation of po-
sition. As far as position is concerned, the temporal mapping is not a
one-to-one correspondence. Furthermore, the initial conditions
cannot be uniquely determined because of Heisenberg's uncer-
tainty principle. Therefore, quantum mechanics does not prescribe
the time-evolution of the position and momentum of a particle in
absolute determinism.
Let us look at another example which helps us break the spell of
natural language. The timing of appearance of numerous miniature
end plate potentials (MEPP) measured at a neuromuscular junction
is described by a deterministic equation with impressive precision
(Fig. 10A). Apparently, the devil is in the detail. If we just examine
the accompanying Fig. 10B, the individual MEPP appears so erratic
that our naked eye sees no order whatsoever. We must not confuse
a deterministic equation with determinism of the underlying pro-
cess which the equation describes. Likewise, the time-reversal
invariance of an equation does not always imply the time-
reversal symmetry of the underlying process.
Perhaps even an analytical solution that happens to exist may
not mean that the physical reality which it represents does actually
exist. Perhaps an existing solution merely suggests a possibility d a
clue d that needs to be experimentally veriﬁed. For example, in
solving simple algebraic equations (e.g., quadratic equations), we
routinely ignore or discard, without blinking an eye, a solution that
happens to consist of imaginary numbers. There are also situations
in which one of the two solutions is already known before one
starts. For example, solving the parabolic equation which describes
the trajectory of a canon ball yields two solutions for the distance of
the landing site of the canon ball: one is the desired answer, but the
other solution is simply the origination point of the canon ball,
dubbed a trivial solution. In brief, one should not confuse clues with
proof; one must not take any solution of an equation too seriously.
In conclusion, it is necessary to consider the linguistic aspect of
scientiﬁc statements closely in order to avoid what Bacon once
pointed out: “[W]ords plainly force and overrule the understand-
ing.” We must not confuse determinism of wavefunctions with
determinism of trajectories. We must not confuse time-invariance
of the wave equation with time reversal of the underlying phys-
ical events.
Now, let us consider the validity of Schr€odinger's statement
about the relevance of quantum indeterminacy to biology. At the
time Schr€odinger made the statement claiming that quantum
mechanics is deterministic at the biological level, not much was
known about the role played by quantum mechanics in biology.
With the advent of modern photochemistry and photobiology, a
counter-example to Schr€odinger's claim can be found in photo-
synthesis. It is reasonably well established that electrons in aro-
matic molecules are delocalized and their distribution is described
by its quantum mechanical wavefunction. The primary reaction in a
photosynthetic reaction center is a long-distance electron transfer
from a chlorophyll dimer to its primary electron acceptor by means
of quantum mechanical tunneling; it is governed by a probabilistic
control law. Photosynthesis is a key biological process of our planet
that sustains animal life by regenerating molecular oxygen. Quan-
tum indeterminacy entailed in the ﬁrst step of photosynthesis en-
ters quite early in the long chain of events that precede human
decision making. How could investigators accept the claim that
quantum mechanics is not relevant to life processes? How could
investigators maintain that the life processes leading all the way up
to the step of decision making are strictly deterministic?
In conclusion, quantum indeterminacy is present in some, if not
all, key biological processes. In view of chaos theory, quantum in-
determinacy, however minute it may be, can, in principle, impinge
upon biological processes at the macroscopic scale. Schr€odinger's
claim was understandable because nothing about the photochem-
istry of photosynthesis and nothing about the Butterﬂy Effect were
known in his time. But that does not mean that we should accept
his view without even a second thought.
The intellectual climate has apparently changed in the 21st
century. In a more recent review of physical determinism and free
will, Koch [Koch, 2009], a trained physicist turned neurobiologist,
pointed out that the writing of quantum uncertainty is all over the
sky; “too large to be seen by the unaided eye.” He further wrote,
“The universe has an irreducibly random character. … Physical
indeterminism rules in the world of the very small as well as in the
world of the very large.” But then, in an unexpected turn, he quickly
agreed to Schr€odinger's dogmatic view: “These macroscopic sys-
tems are constituted by an unfathomable number of microscopic
particles such that many of their properties d such as their un-
certainty positions and velocities d are washed out, and what re-
mains is usually fully deterministic.” He pointed out that although
macroscopic objects are built upon the microscopic, quantum
world, that fact “does not imply that these objects inherit all of the
weird properties of quantum mechanics.” Here again, linguistic spin
Fig. 9. Impairment of inactivation of HyperPP Naþ channels. Single-channel Naþ cur-
rents were elicited with depolarizing pulses in cell-attached patches on normal and
HyperPP (Met1592Val) myotubes. Ensemble averages (bottom traces) show the
increased steady-state open probability caused by disruption of inactivation in the
mutant. (Reproduced from [Cannon, 1996]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
700

tends to blur the true picture and obliterate most logical gaps.
Whether it is a weird property or normal property has nothing to
do with being too big or too small, but rather it is just a matter of
our familiarity with it.
Just consider the familiar example of soap with its constituent,
amphiphilic molecules, which have one end exhibiting hydropho-
bicity and the other end hydrophilicity, thus exhibiting the two
weird quantum properties all in a single package. What about
boiling an egg to break all those hydrogen bonds and other weak
non-peptide bonds (e.g., disulﬁde bonds) that stabilize protein
conformation in a raw egg, and then, in biochemistry jargon, call
the process de-naturalization, or simply call it fully cooked? What
about a gecko's ability to stick its footpads to walls? Scientists
discovered that geckos rely on van der Waals force, which is of
quantum origin, to stick to walls [Autumn et al., 2000]. Are these
events weird or commonplace? Again, it is familiarity that matters
rather than size. It is habituation and brainwashing that dulls our
senses and lulls us into believing that quantum indeterminacy
plays no role in biology.
5. Readiness potential and subjective feelings of volition
So far our discussion of the free will problem involves inferences
and analysis on the basis of known principles and facts in physics
and in biology. Here, we shall consider the only direct empirical
challenge to the existence of free will: the experimental in-
vestigations carried out by Libet and coworkers [Libet et al., 1983;
Libet, 1985]. They investigated an electric potential known as the
readiness potential (RP). The RP is a slow negative shift in electrical
potential that precedes a self-paced, apparently voluntary motor
act by a few hundred milliseconds. It is recorded from the scalp and
its magnitude is at the maximum when recorded at the vertex of
the head. The exact nature of the RP is not clearly known. Its
amplitude is so small that it is overshadowed by noise. However,
modern digital recording techniques allow such a small signal to be
“signal-averaged.” But the procedure of signal averaging requires a
synchronizing trigger that allows the time zero of all repeated
traces of signal recording to be aligned with one another. Libet and
coworkers used the onset of the electromyogram (EMG), recorded
from a selected skeletal muscle, as the synchronizing trigger. The
experimental subjects were asked to perform a speciﬁed voluntary
movement (either ﬂexion of the wrist or ﬁngers of the right hand)
at the beginning of a trial session. There were no externally
imposed restrictions or compulsions that directly or immediately
controlled the subjects' initiation and performance of the act. The
subjects felt introspectively that they were performing the act on
their own initiative and that they were free to start or not to start
the act as they wished. Thus, abortive trials were allowed. That is,
the contraction was not a direct consequence of an external stim-
ulus. However, indirectly, the movement was the consequence of
the experimenter's request. Such a movement is operationally
deﬁned as a fully endogenous self-initiated voluntary act. These
authors made clear that this deﬁnition was not committed to or
dependent upon any speciﬁc philosophical view of the mind-brain
relationship.
The experimental subjects were asked to report the moment of
initial awareness of intending or wanting to move (W). The report
was made in retrospect by remembering a clock position that was
seen to coincide with the moment of W. An alternative way of
reporting by a verbal response or pushing a button might have
complicated the measurement by initiating a separate RP associ-
ated with the motor act of reporting. In order to evaluate the reli-
ability of reporting, control experiments, in which a near-threshold
skin stimulus was delivered irregularly, measured the reported
time of the initial awareness of skin sensation (signal S). This time S
was used to obtain the corrected W time.
The analyzed experimental data indicated that the uncorrected
W signal appears at approximately 200 ms, whereas the RP signal
appears at approximately 550 ms. Thus, the average onset of the
RPs preceded the average W by approximately 350 ms. This lead
time became 400 ms after correcting the W signal with the S signal.
In other words, the time sequence of appearance was: RP, W, and
then the EMG signal. It is important to realize that the sequence of
appearance held not only for the average values of all series but also
for each individual series of forty self-initiated acts in which RP and
W were recorded simultaneously.
On the basis of these results, Libet and coworkers proposed that
unconscious cerebral processes could initiate voluntary acts before
conscious attention appears. However, they also proposed that
conscious volitional control might operate not to initiate the voli-
tional process but to select and control it, either by permitting or
triggering the ﬁnal motor outcome of the unconsciously initiated
process or by vetoing the progression to actual motor activation.
The work of Libet and coworkers constitutes a major attempt to
interface externally observable signals of the brain state with the
events that are accessible only through introspection. Needless to
say, their work has remained controversial since their ﬁrst report.
There are two separate issues surrounding the controversy: the
reliability of the experimental results and the validity of the
interpretation. The validity of experimentally timing the onset of an
introspective feeling cannot as yet be objectively or independently
veriﬁed. In addition, the nature of RP is still poorly understood.
These problems were extensively discussed in the main text and
the open peer commentary section of Libet's review article [Libet,
1985]. I will present additional comments later. Here, we shall
consider the second issue: the interpretation of experimental
results.
Taken at face value, the experimental results imply that the
physical correlates of volition precede the introspective conscious
awareness. This novel ﬁnding elicited diverse responses. Vander-
wolf advocated the behaviorists' advice of abandoning the intro-
spective mentalistic approach to behavior, thus making the
problem disappear ([Libet, 1985], p. 555). Here I share Libet's view:
the logical impossibility of direct veriﬁcation of introspective re-
ports is an insufﬁcient reason to avoid studying a primary
phenomenological aspect of our human existence in relation to
brain function. Dennett and Kinsbourne [Dennett and Kinsbourne,
1992] preferred to accept the challenge of making sense of the
experimental results.
We thus encounter the unsettling prospect of an effect
appearing to precede its cause. One of the escapes is to advocate
Descartes' dualism, but that is contrary to the contemporary
approach of ﬁnding physical correlates with mental activities. In
fact, Libet was labeled in one of the open commentaries as a dualist,
but Libet denied that he was one.
The observed anomaly poses no conceptual difﬁculty if one takes
the position of absolute determinism, because the so-called
conscious awareness and the so-called voluntary act are both pre-
destined to take place. At best, conscious awareness can be regarded
as an epiphenomenon, and the conscious self a mere spectator. With
this deterministic view, the relative timing of the conscious
awareness and the motor act is inconsequential, as long as the time
discrepancy between the two is not grossly noticeable (much like
the situation encountered in dubbing voices to a movie after the
original production). Here I wish to emphasize, the work of Libet and
coworkers lends its credibility in part to the short time intervals in
the sub-second range. No one can claim to have the subjective ability
to differentiate discrepancies or differences of time intervals in the
sub-second range. In contrast, the work of Haynes and coworkers
reported an outrageous delay in the range of minutes. Just as a poor
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
701

post-production dubbing job in a movie makes lip-sync or the lack of
it too obvious to be worthy, the time delay (discrepancy) reported by
Haynes and coworkers is too long to be credible, and the alleged
epiphenomenon too fake to make sense, from a teleological point of
view. There is simply no roomfor us to give their report the beneﬁt of
the doubt. There must be something terribly wrong with the ex-
periments of Haynes and coworkers because their conclusion deﬁes
phenomenology; most sane people could tell from their own
introspection that it could not have taken that long even if the delay
were true (see [Balaguer, 2014]).
However, there is an apparent paradox. Velmans [Velmans,
1991] pointed out that consciousness appears to be an epiphe-
nomenon from the third-person (objective) perspective but not
from the ﬁrst-person (subjective) perspective. This internal conﬂict
of mind can be dismissed by virtue of objectivity, i.e., all subjective
feelings must be disregarded in scientiﬁc investigations. But the
heart-felt conﬂict will not simply go away, and will raise its skep-
tical head at unguarded moments (see the comment of Wegner in a
later section).
Other investigators made attempts to make sense of the
experimental observations of Libet and coworkers. Kinsbourne
[Kinsbourne, 1998] interpreted the ﬁndings as evidence in support
of Freud's original theory of the unconscious, and the manifestation
of the brain's capability of parallel processing. Dennett and Kins-
bourne [Dennett and Kinsbourne, 1992] considered two models of
how consciousness treats subjective timing: (a) the standard
“Cartesian Theater” model of a central observer that judges all
sensory modalities together, and (b) the alternative “Multiple
Drafts” model in which discriminations are distributed spatially in
the brain and temporally in the brain processes. Dennett and
Kinsbourne favored the “Multiple Drafts” model, and used it to
interpret the strange time sequence as the consequence of parallel
distributed processing. In an open commentary published along
with Dennett and Kinsbourne's article, Baars [Baars, 1988] pointed
out that the choice of the “Multiple Drafts” model over the “point
center” conception of consciousness d i.e., Cartesian Theater, or
Global Theater d effectively denies consciousness any integrative
or executive function at all. Baars therefore emphasized that con-
sciousness is associated with both central and distributed pro-
cesses; a merger of both models is in order. Thus, multiple drafts are
prepared, but, eventually, only a single draft is ﬁnally submitted.
The merged model reﬂects the modularity of biological information
processing, whereas the central integration reﬂects the hierarchical
control with numerous two-way regulatory processes.
Of course, a peculiar lack of integration occurs in split-brain
subjects whose corpus callosum has been severed [Sperry, 1974,
1982] (see also Chapter 2 of [Springer and Deutsch, 1989]). As
Dennett and Kinsbourne pointed out, failure to integrate the
distributed processes leads to disorders such as multiple person-
alities, hemispheric neglect, etc. However, split-brain subjects
present a different type of a speciﬁc lack of integration of the two
sides of the brain. It is as if there were two selves; the right side may
not know what the left side knows and vice versa, and the ignorant
side may confabulate d a milder form of lying. For an analysis of
peculiar consciousness of split-brain patients, readers are recom-
mended to consult Gazzaniga's two books, Who's in Charge?: Free
Will and the Science of the Brain [Gazzaniga, 2011] and Tales from
Both Sides of the Brain [Gazzaniga, 2015]. Incidentally, Gazzaniga,
the protege of Sperry, is probably the best-known free will denier
from the camp of neuroscience.
Split-brain patients exhibit a kind of lack of integration of
distributed centers of consciousness. The symptom is not exactly
pathological; it is iatrogenic, so to speak, i.e., the patient's corpus
callosum was surgically severed for therapeutic reasons. In refer-
ence to normal individuals, there is a similar notion of two selves,
which has often been mentioned in the creativity literature and
which can be traced to Freud's concept of the unconscious. Poincare
alluded to “two selves” in his recount of an episode of “aha!”
experience [Poincare, 1908]. He was working on a mathematical
problem for an extended period to no avail. Subsequently, he had to
interrupt his work so as to join a pre-planned trip of a geological
excursion. Suddenly, a fruitful insight dawned on him at the
moment when he stepped on an omnibus. Poincare was aware of
Freud's theory. So he explained his experience by assuming that
there were two selves. The conscious self was at a leisure trip, but
the unconscious self continued to work hard; he also thought that
the unconscious self was superior to the conscious self. His inter-
pretation was greeted with skepticism and ridicule [Bell, 1986].
Fig. 10. (A) Statistical control law governing the random appearance of miniature end
plate potentials (MEPP) at the end plate region of a frog skeletal muscle. The number of
observed intervals, of duration less than t, has been plotted against interval duration t.
Smooth curve: a theoretical curve, y ¼ N[1  exp(t/T)], where N is the total number of
observations, and T the mean interval, marked by arrow. Circles: observed numbers.
(Reproduced from [Fatt and Katz, 1952]) (B) Actual recordings of miniature end plate
potentials of frog muscles. Spontaneous activity at the motor end plate region of a frog
muscle leads to the random appearance of numerous miniature end plate potentials,
which are shown in the upper part (voltage and time scales: 3.6 mV and 47 msec). A
full-ﬂedged muscle action potential is also shown in the lower part (voltage and time
scales: 50 mV and 2 msec). (Reproduced from [Fatt and Katz, 1952]).
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
702

Poincare's introspective narration, which was essentially a
phenomenological description of a scientiﬁc discovery, can be re-
interpreted in light of modern concepts of artiﬁcial intelligence
(AI) [Hong, 2013b]. The impasse prior to his trip reﬂected that he
was trapped in a fruitless direction of attacking a novel problem
(called the search space, in AI jargon). In a leisurely mindset,
Poincare shifted his attention d or, rather, defocused his attention
d towards a different part of the search space, i.e., to attack the
problem from a different angle.
But Poincare's interpretation in terms of the unconscious is not
entirely without merit. It was not inconceivable that he did indeed
have unconscious thought, which waited till his shift of attention to
surface. To make a long story short, the two selves are indeed the
thought experienced by the two cerebral hemispheres. The thought
of the left cerebral hemisphere is readily reported in words. In
contrast, the thought of the right cerebral hemisphere is much
harder to articulate mainly because the thought is pictorial in na-
ture. That the thought in the right cerebral hemisphere is hard to
articulate is best illustrated by Galileo's report during the night of
January 10, 1610, when he discovered the four larger moons of the
planet Jupiter [Galilei, 1610]. Galileo just said that he turned “from
doubt to astonishment” and then jumped to his conclusions. Drake
[Drake, 1978] thought that Galileo had made a mistake and had
reported the discovery two nights too early. I was puzzled by Gal-
ileo's astonishment and decided to deliberately invoke visual
thinking to review Galileo's observation records. Suddenly, I suc-
ceeded in verbalizing Galileo's unspoken inference [Hong, 2013b].
It was therefore possible to allow the unconscious thought
lingering in the right hemisphere to surface by deliberately
invoking visual thinking. But it took time to convert a vague
pictorial awareness to something that can be expressed concretely
in meaningful words, i.e., the time it takes to comprehend the
meaning of pictorial information, sort of parallel-to-sequential
conversion in computer science jargon. Additional discussion of
visual thinking and verbal thinking is given in Sec. 6.2.3 and Sec. 7.3.
This new interpretation of unconscious thought needs to be
taken into account in re-designing and re-interpreting experiments
pioneered by Libet and coworkers. It is possible that the readiness
potentials may be related to the delay incurred during the above-
mentioned parallel-to-sequential conversion. If so, then the delay
of conscious awareness may be explained as follows. Since the
timing criteria of the W signal were given in spoken or written
words by the experimenter, the recognition (conscious awareness)
of the matching between the subjective feeling and the timing
criteria might also be carried out in the verbal form (silent speech).
The observed time delay may be caused by (internal) verbalization
of the unconscious feeling, thus resulting in the apparent paradox
(see [Hong, 2013a, 2013b] for the “aha!” phenomenon and exam-
ples illustrating such a delay).
There is another crucial point that should enter the interpre-
tation of readiness potentials. So far, Libet and coworkers consid-
ered only events in the neural substrate for voluntary initiation of
muscle contraction. They overlooked the possibility d I refrain
from using the word “fact” here d that initiation of a thinking
process is also an act of volition. Initiation of a thinking process,
which means the conscious or unconscious decision to invoke
visual or verbal thinking, may also be associated with a readiness
potential or an RP-like signal. The fact that Libet and coworkers
neither considered nor mentioned any RP-like signal that accom-
panies the act of thinking casts a serious doubt on the alleged role
of the RP signal. It is possible that the RP may just be an irrelevant
factor that happens to be coupled with the act of volition of con-
tracting a muscle but not with the act of volition of thinking. This
was a conspicuous blind spot in the interpretation of RP experi-
ments: A volition could be just contemplating without lifting a
ﬁnger. For readers who are skeptical about whether thinking is an
act of volition, please see Sec. 6.2. For a detailed explanation of
why correlation is no deﬁnite indication of causation, please see
Sec. 7.2.
The practice of using statistical analysis to validate the inter-
pretation also poses a problem for the experiments on readiness
potentials. Consider a general case of investigating the effect of a
particular potential causal factor. The experimentalists compare
two groups of subjects. One group is subject to the challenge of the
causal factor (the tested group) whereas another group is subject
to no such challenge or a substituted challenge of a placebo (the
control group). A statistical analysis involves evaluating either a
time-average or an ensemble-average of both the tested group and
the control group.14 In order to make sense of such an analysis, the
test subjects must be homogeneous in regard to the challenging
causal factor being investigated. Otherwise, the analysis runs the
risk of comparing apples with oranges (sample heterogeneity). A
detailed explanation about how sample heterogeneity may impact
the interpretation of a statistical analysis will be given in Sec. 7.2.
Detecting sample heterogeneity can be a daunting challenge to the
experimentalists because it often requires the aid of hindsight. In
the case of behavioral experiments regarding free will, we are
dealing
with
an
extreme
case:
Sample
heterogeneity
is
guaranteed.
It is impossible to obtain a homogeneous sample for performing
ensemble-averaging because the expression of free will is strongly
inﬂuenced by an individual's personality. Nor is it possible to
perform impeccable time-averaging because repeating the behav-
ioral experiment on the same individual at a later time cannot
guarantee identical experimental conditions; memory of the
outcome of a previous experimental run and the accompanying
hindsight will almost certainly render time-averaging of the
experimental results difﬁcult to interpret or meaningless.
If these statements sound too abstract, let us consider the
repeated measurement of RPs. Although initiation of volition was
controlled by the individual experimental subjects, they were given
an instruction by Libet's experimentalists. Here is how personality
comes into play. Just like the situation of obeying trafﬁc rules,
different individuals bend the rules to different degrees of
compliance. Even though they were told not to initiate the act until
they felt comfortable, some individuals might start to contemplate
the act violently while withholding action for a while. Full
compliance was difﬁcult to enforce uniformly. Who knew exactly
what was going on in the subject's mind? In other types of
behavioral experiments, such as pitting free will against the lure of
temptations or punitive threats, personality also comes into play.
For example, the dean of a college offers the lure of a merit raise if
the researcher is willing to switch to research topics with better
funding opportunities. Needless to say, this leads to a torn decision.
Individuals with stronger will power may persist and persevere,
but those with weaker will power may succumb to the offer or
threat at the expense of their original aspiration. Time-averaging
may not fare better because the human subjects have memory of
the outcome of a previous experimental run. Legend had it that a
certain researcher scolded his or her hired human subjects for
underperforming in the experiment, as compared to those of a rival
research group. Thus, free will is so personality-dependent and
history-dependent that, since there is only one life to live, the
concept of statistical averaging, as applied to free will experiments,
is inherently problematic. I am not saying that this was exactly
14 Flipping a coin many times yields the time-average of head-to-tail distribution
ratio, whereas ﬂipping many coins simultaneously yields the corresponding
ensemble-average.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
703

what had happened in those RPs experiments; I saw no obvious
evidence. But it is just another factor to consider in a controversial
subject.
In Sec. 7.3, it was pointed out that psychological research rarely
paid attention to Popper's argument of falsiﬁability, much less to
Peirce's notion of inference to the best explanation. Instead, re-
searchers often jumped to the ﬁrst conclusion that had fulﬁlled
their expectations. Better explanations may be available. For that
matter, Mark Balaguer's recent book Free will contains a compre-
hensive compilation of plausible alternative interpretations of
the experiments of Libet's group as well as those of Wegner's
group [Balaguer, 2014, Chapter 7]. Interested readers are urged to
read Balaguer's original explanations. Any attempt for me to
provide a concise summary here will ruin the eloquence of his
argument.
6. Reconciling free will phenomenology with science
6.1. Alternativism, intelligibility and origination
Regarding the enigmatic issues arising from phenomenology of
free will, since antiquity many thinkers have propounded their
philosophical perspectives. A laundry list of opinions serves little to
enhance our understanding; a concise summary does. One of the
best summaries was presented by neurophilosopher Henrik Walter
[Walter, 1999]; any satisfactory reductionist theory or interpreta-
tion regarding free will must be able to adequately address three
issues: alternativism, intelligibility and origination. Brieﬂy, alter-
nativism means that our subjective feelings indicate that we have
the mental power to choose from available alternative actions even
though only one of these alternatives actually materializes. Intel-
ligibility means that the choice made in an exercise of free will is
not arbitrary and it serves meaningful purposes.15 Origination
means that it is we, ourselves, who make the decision d i.e., we are
in the driver's seat to call the shots d and the decision is made
deliberately after careful and thorough deliberations by means of
weighing the pros and the cons. In short, we are not robots or
zombies who simply act out a pre-written script without any per-
sonal (autonomous) input. Furthermore, we are not someone else's
puppet most of the time, if not all.
As mentioned in Sec. 2.4, compatibilists claim that the alterna-
tive choice which never materialized was actionally possible. We
demonstrated that this form of compatibilism is untenable if
physical laws are strictly deterministic. Both Kant and Hume were
compatibilists, but they utilized interpretive tricks to circumvent
the conﬂict. There was no need for them to bend the logic and to
distort the meaning of determinism if absolute determinism turned
out to be untenable.
In contrast, a non-deterministic (indeterministic) world permits
such alternatives to happen, but it does not guarantee intelligibility.
That was why Ruelle raised the issue of whether randomness is
compatible with a responsible and intelligent decision made by an
individual; but he was also concerned with the incompatibility of
determinism with free will.
Ruelle's concern was shared by many others, including Walter
who discussed the issue under the designation of intelligibility.
Walter pointed out that both determinism and indeterminism
pose a problem for intelligibility ([Walter, 1999], p. 191). An action
based on deterministic causation does not meet the criterion of
intelligibility because it has been causally predetermined; any
alleged rational deliberation is superﬂuous and therefore illusory.
He also rejected causation by probabilistic or “undetermined”
(non-deterministic) laws because no deliberation is involved
([Walter, 1999], p. 70). By “undetermined” laws, Walter probably
meant laws with complete, utter randomness. Walter found the
notion of probabilistic laws of causation disturbing, presumably
because the notion of probability conjures up a scheme of
arbitrariness.
Walter admitted quasi chance as a plausible explanation, but he
did not explicitly link it to what we mean by quasi-determinism or
a gray scale of determinism. Instead, he proposed that the chaotic
brain provides possibilities of bifurcations in decision making
(chaotic alternativism vs. indeterministic alternativism; [Walter,
1999], p. 186). However, if physics were absolutely deterministic,
neither chaos nor occasional occurrences of singularity would
satisfy the requirement of alternativism. In an attempt to reconcile
alternativism with intelligibility, Walter sought sanctuary in
intentionality found in biology ([Walter, 1999], p. 195). However,
intentionality is an attribute that is intimately linked to free will.
Walter merely replaced an enigmatic problem with another; the
circularity is apparent.
We shall demonstrate that the notion of intelligibility dovetails
nicely with quasi-determinism, based on our current understand-
ing of human creativity [Hong, 2013a, 2013b]. Human creativity
shares the issue of origination with free will. We shall defer further
discussion of intelligibility and origination to a later section (Sec.
6.2.3) after we explore the topic of creativity from the broad
perspective of Donald T. Campbell's evolutionary epistemology.
6.2. Origination and downward causation
The central element of consciousness is the sense of the self. The
sense of the self is also connected to the sense of autonomy (self-
determination), the sense of agency, and the subjective feelings of
possessing self-initiated mental power. Among many attributes of
consciousness, free will and creativity are two of the most impor-
tant manifestations of consciousness. Thus, the self knows that the
self possesses autonomy so as to carry out the self's free will. The
self can exercise the self's mental power to solve novel problems
and create something novel (e.g., an invention, a scientiﬁc theory, a
poem, a symphony, or a painting). The above description of the
sense of self seems remote from what we actually feel subjectively.
A better and perhaps more colorful portrayal of the sense of self can
be done by paraphrasing what Schwartz and Begley ([Schwartz and
Begley, 2002], p. 26) have said about consciousness, “Consciousness
is more than perceiving and knowing; it is knowing that you know.”
By the same token, free will is more than calling the shots; it is
knowing that you are actually calling the shots. But, then, if a
computer were programmed to convey its own awareness of hav-
ing free will and of being creative, the act would be dismissed as
fakery rather than a demonstration of the existence of free will and
of genuine creativity. Here the real issue is origination, because
even knowing that you know and knowing that you have free will
can be faked by computer simulations. I believe that this con-
trasting consideration of computer simulation speaks to the issue of
origination with sufﬁcient clarity.
6.2.1. Campbell's evolutionary epistemology
About ninety years ago, James Mark Baldwin [Baldwin, 1925]
noted the similarity between human problem solving and evolution;
evolution is tantamount to collective problem solving over many
generations. Donald T. Campbell followed up on Baldwin's idea and
developed what is now known as evolutionary epistemology, which
Wuketis characterized as an all-purpose explanation of ﬁt. He suc-
cinctly summarized it as follows ([Wuketits, 2001], p.178): “There is
a ﬁt between the organisms and their environment, between vision
15 Apparently, the word “intelligibility” as chosen by Walter differs from its con-
ventional usage in English; customarily, it means capability to be understood.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
704

(and other types of perception) and the outer world, and between
scientiﬁc theories and the aspects of reality they purport to explain
d and these ﬁts are results of selection processes that bring forth
ever-better adapted organisms, eyes, and theories.” The common
thread that ties all three parallels together is pattern recognition.
When the eyes see something, it is not just a process of image for-
mation in the retina; the mind comprehends the meaning of the
visual image only if a preconceived pattern is recognized. Likewise,
solving a novel problem means that the mind recognizes a solution
that fulﬁlls the speciﬁc requirements of the problem d the solution
matches the problem like a key ﬁtting a particular lock in a speciﬁc or
even a unique way. In the process of evolution, speciﬁc biological
features of a particular organism that ﬁt the demand or challenge of
the environment d again like a key that ﬁts a speciﬁc lock d are
more likely to be retained beyond the present generation than fea-
tures that jeopardize or do not help the survival of the organism.
It came as no surprise that Campbell's evolutionary episte-
mology led him to a model for creative problem solving, known as
the blind variation and selective retention model [Campbell, 1960].
Another spin-off from evolutionary epistemology is the concept of
downward causation [Campbell, 1974]. As we shall see, downward
causation is one of the central topics in free will research.
6.2.2. The Concept of downward causation
Causality in Western science is upwardly directed from the bot-
tom of constituent parts to the whole. Yet our subjective feelings
indicate that we, unlike a robot or a zombie, have the freedom of our
minds to make choices among available options of actions (including
thinking). In other words, for free will to genuinely exist rather than
just to reﬂect our illusion, some kinds of self-initiated mental power
must exert causal inﬂuence from the whole to the constituent parts.
This is the concept of downward (or top-down) causation.
According to Murphy [Murphy, 2009], Roger Sperry who pio-
neered split-brain experiments also wrote of the properties of the
higher-level entity or system overpowering the causal forces of the
component entities at about the time when Campbell proposed the
concept of downward causation. Needless to say, no one followed up
on Sperry's idea. Instead of using an eyebrow-raising expression,
Campbell's enunciation of evolutionary epistemology was mild and
non-confrontational: a non-mysterious account of a larger system of
causal factors having a selective effect on lower-level entities and
processes. In other words, the evolution of a survival-advantageous
feature was no Larmarckianism. The feature evolved not because it
was deemed desirable d a teleological inﬂuence. Rather, through
surviving and procreating over time of generations, the larger system
exerts its causal power indirectly by selecting the desirable feature
among other possibilities with lesser survival value. Campbell claimed
that all processes at the higher levels are restrained by and act in
conformity with the laws of the lower levels, including the levels of
subatomic physics. Downward causation is implemented through life
and death of the whole organism and its ability to procreate d sterile
organisms cannot exert selective inﬂuence through evolution.16 The
remaining explanations with regard to perception of the eyes and
theory construction also worked well. The eyes perceive what is
already there without altering a thing, whereas theory construction
merely matches existing data with natural laws that are out there to
be discovered17; the eyes and the theorists merely select what is
available to be selected. No overpowering inﬂuence from higher up
needs to be speculated. Therefore, evolutionary epistemology sur-
vived initial scrutiny whereas Sperry's more radical idea did not.
Murphy credited Van Gluck [Van Gluck, 1995] for invoking
downward causation in philosophical deliberation and for spelling
out the detail. Van Gluck argued, “The events and objects picked out
by the special sciences are admittedly composites of physical con-
stituents. But the causal powers of such an object are not deter-
mined solely by the physical properties of its constituents and the
laws of physics, but also by the organization of those constituents
within the composite.” Van Gluck apparently sensed the opposition
of reductionists: the organization itself is also determined by the
same natural laws that determine the composites of physical con-
stituents prior to organization. He adopted the Kantian style of
thesis-antithesis approach as a preemptive strike against possible
objections. However, Van Gluck did not pursue antitheses with
equal vigor d he did not try hard enough to falsify his own thesis.
At times he preached like an overzealous missionary, thus ceasing
to be persuasive. He might be able to get an instant conversion, but
the conversion, I suspect, might not be enduring.
However, the notion of “organization” was sufﬁciently attractive
to be embraced by advocates of downward causation who were
eager to distance their interpretation from low-level physics [Ellis,
2009]. Van Gluck supported the notion of organization by listing
ﬁve points of argument, mainly concerning its stability. However,
Van Gluck confused stability with independence, but his argument
was not entirely without merit. Thermoplastics maintain their
stability after throwing away the mold, and the offspring survives
the death of parents. But counter-examples also exist. Van Gluck
did mention the constant renewal of constituent molecules and the
self-repair of biological organizations. He forgot that it is energy-
dependent processes that maintain the dynamic stability of bio-
logical structures; all corpses eventually decay. Obviously, this kind
of dynamic stability cannot afford to be “divorced” from low-level
forces. Van Gluck's analogy was therefore misleading. In the end
and near the end of his article, Ellis ﬁnally brought up the overdue
reality check ([Ellis, 2009], p. 74):
I have claimed here that top-down causation is causally effec-
tive, which means that even in principle, micro-level laws fail to
fully determine outcomes of complex systems: causal closure is
achieved only by appealing to downward causation. But this
claim is clearly in trouble if the system is already causally closed
at the micro level, as is supposed by most physicists. For higher
levels to be causally efﬁcacious over lower levels, there has to be
some causal slack at the lower levels, otherwise the lower levels
would be causally overdetermined. Where does the causal slack
lie?
Ellis obviously recognized the logical conclusion that absolute
physical determinism completely demolishes downward causation.
He was not asking for complete randomness; he was merely asking
for a little bit of causal slack at the lower levels. Ellis' request can be
faithfully translated into physical quasi-determinism, as deﬁned in
an earlier section of the present article. There is no need to pick on
the ﬂaws embedded in Ellis' remaining arguments in his desperate
search
for
the
elusive
causal
slack.
Thus,
physical
quasi-
determinism breathes life into the concept of downward causation.
In view of the dispersion embedded in the causal law, it is
virtually impossible to trace its action all the way to the formation
of the organized patterns alluded to by Ellis. Thus, it is legitimate to
formulate phenomenological causal laws based on higher-level
dynamics. But the new strategy in formulating theories does not
mean that low-level forces simply vanish; they merely fade into the
background and no longer command our central attention.
16 Of course, sterile organisms contribute to exerting negative inﬂuence by dying
along with the undesirable mutation that causes sterility.
17 This is the conventional view. Instead, I have treated theories as humans'
mental constructs to match observed data in a coherent and rational fashion [Hong,
2013a]. This latter view requires the mental power to actively create, rather than
passively select, a suitable mental model.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
705

Ellis made a lucid presentation by giving concrete examples in
real life to illustrate his ﬁve categories of downward causation, but
none of Ellis' examples demonstrated any emerging new force. By
the absence of a concrete example, Ellis merely demonstrated that
it was difﬁcult to come up with an example showing new emergent
forces, but that did not prove that these speculated new forces did
not exist.
6.2.3. Context-sensitive constraints
Juarrero [Juarrero, 2009] introduced the concept of context-
sensitive constraints in the analysis of downward causation. She
pointed out that such constraints supplied the cohesive properties
that ﬁrst created complex (biological) systems and held them
together. Furthermore, she wrote, “These constraints integrate
previously independent parts into a uniﬁed whole that (a) in-
corporates the record of its history, (b) is embedded in its envi-
ronment, and (c) possesses emergent properties. Such systems,
which can remain the same type of phenomenon despite being
composed of different token arrays, can change the very compo-
nents that make them up and even alter their environment d and
they do so on the basis of meaningful criteria deﬁned at the global
level.” Here, “meaningful criteria at the global level” means that the
criteria are set up with the perception of the whole and with the
intended functions in mind. This is the very kind of causation that
Aristotle classiﬁed as the ﬁnal cause. As mentioned earlier, the
teleological connotation can be understood in terms of evolution; it
was not the functional need that generated the intended changes of
the structures; rather, it was the choice among various available
options of structures made possible by means of random mutations
of genes. To supply a similar explanation for downward causation,
Ellis [Ellis, 2009] came up with the notion of selective activation of
available causes. At the same time, he emphasized that “there are
other forms of causation than those encompassed by physics and
physical chemistry.” Perhaps by going one step too far, he inad-
vertently injected an unwanted element of mysticism into the
concept of downward causation, thus defeating the purpose of
science; science and mysticism do not mix.
Juarrero pursued a stronger form of context-sensitive con-
straints, and she supplied greater detail about the concept
[Juarrero, 1999, 2009]. She credited Lindsay [Lindsay, 1961] for
being the ﬁrst to formally use the concept in physical mechanics to
describe the way the motion of a simple pendulum or a particle on
an inclined plane is “compelled by the geometry of the environ-
ment to move on some speciﬁed curve or surface” ([Juarrero, 1999],
p. 132). Lindsay noted that “some of the most important cases of
constrained motion are those in which particles are connected by
rods and strings” and cannot move in any other ways. Likewise, a
particle that gets stuck on a wheel's rim due to grease must move
on a trajectory of a cycloid instead of sliding on the road in a more
or less straight line or being blown away in the air in an unpre-
dictable trajectory due to chaos. Juarrero [Juarrero, 2009] believed
that downward causation could give rise to additional degrees of
freedom, which makes it unnecessary to resort to quantum inde-
terminacy or the Wheeler-Lloyd limit for the needed causal slack. I
disagree that downward causation could exist without the causal
slack alluded to by Ellis. In fact, Gazzaniga was not convinced by the
apparently mystical argument, and ﬂatly denied that downward
causation is possible if (strict) determinism is valid ([Gazzaniga,
2011], p. 140).
Juarrero went on and made two salient points ([Juarrero, 1999],
pp. 132e133). First, she claimed that, “in opposition to Newtonian
science and modern philosophy's dismissal of relational properties
as subjective, by ‘constraints’ Lindsay clearly means something
other than Newtonian forces that is nevertheless causal.” Second,
Juarrero pointed out an apparent contradiction. She wrote, “But if
all constraints restricted a thing's degrees of freedom in this way,
organisms … would progressively do less and less. However, pre-
cisely the opposite is empirically observed. Some constraints must
therefore not only reduce the number of alternatives; they must
simultaneously create new possibilities.” She then turned to in-
formation theory for inspiration. She pointed out that information
theory identiﬁes constraints not as in physical mechanics with
physical connections, but with rules for reducing randomness in
order to minimize noise and equivocation.
Juarrero's points are well taken until one looks into further
details. Let us ﬁrst consider the notion that context-sensitive con-
straints allow additional degrees of freedom beyond what the
constituent particles already have. This point is too “spooky” to
consider in reductionist physics. Juarrero invoked an analogy to
explain ([Juarrero, 2009], p. 88): “[S]entences can say things that
words alone can't; words have signiﬁcance that letters lack; amino
acids folded into a protein structure possess properties that the
amino acids on their own do not; neural patterns can carry
meaning which individual neuron ﬁrings do not, and so forth.”
However, what she attempted to enunciate was not the same kind
of degree of freedom as considered in mechanics. Rather, what
actually transpires is that more bytes store more information;
permutations and combinations of more elements generate more
patterns, and so forth. Apparently, Juarrero confused the cumulative
degrees of freedom d due to the increase in the total number of
particles d with the internal degrees of freedom which an indi-
vidual particle possesses. It is true that more elements possess
more degrees of freedom, but the newly gained degrees of freedom
may be trimmed down by rules of organization. Thus, permutations
of words generate a large number of “potential” sentences, but the
grammatical rules actually trim them down to a much smaller
meaningful subset. Likewise, increasing the number of amino acids
in a polypeptide increases the variety of “potential” proteins, but
only a smaller subset folds properly; again, the newly gained de-
grees of freedom are curtailed by rules of protein folding. What
actually transpires in Juarrero's analogies is increased diversity
rather than increased degrees of freedom. Besides, analogy does
not prove though it hints. Still, misinterpretation of analogy almost
always indicates misinterpretation of the concept.
Now, let us turn to the two points made by Juarrero. The ﬁrst
point was merely an illusion. Juarrero did not call “something other
than Newtonian forces” a force. Elsewhere she insisted that not all
causal factors are forces. But then what causes the pendulum to
remain connected to the hanging rod and what causes the particle
undergoing a cycloid motion to stick to the rim of a wheel? Of
course, the cause stems from conventional (non-gravitational)
forces that are commonly encountered in microscopic physics and
chemistry, including the covalent bonding force and weak inter-
molecular forces, which sometimes take the loose name of cohe-
sion or adhesion. The rod connecting the pendulum reacts to the
pendulum's swing by exerting a reaction force d called the cen-
tripedal force d which balances the centrifugal force; thus the
pendulum would not ﬂy apart in a straight line due to inertia.
Next, let us examine the case of rigid body motion. The same
types of non-Newtonian forces make a rigid body rigid (or nearly
rigid if we insist upon considering material qualities of ductibility
and extensibility, elasticity, etc). Elementary mechanics now re-
places the famous Newton's First Law with a similar formula:
Torque ¼ Moment of inertia  Angular acceleration. It is as if the
Newtonian
force
which
had
been
originally
formulated
to
describe a particle's linear motion suddenly took a back seat or
simply vanished. Not really so, if one looks closer. The formula
f ¼ ma must be invoked for each and every particle that consti-
tutes the rigid body, and one needs Newton's newly invented
integral calculus to calculate the moment of inertia. Suddenly, the
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
706

student working on this exercise realizes that the mass of all
particles adds up in a nonlinear way and how the mass is
distributed inside the rigid body becomes crucial: concentrating
most of the mass at the rim of a wheel or at the center of the
wheel makes a huge difference. It is almost common knowledge
that all ballet dancers and ﬁgure skaters who attempt to make a
pyrotechnical spin know to draw in their extended arms so as to
reduce the moment of inertia and to increase the speed of spin.
Suddenly, one realizes that it is not something other than forces
but the old-fashioned mechanical force plus interatomic and
intermolecular forces d unknown to Newton, therefore, non-
Newtonian in nature d that keeps the rigid body rigid. Note
that fading into the background is not the same as simply van-
ishing. Beware of the trick played by natural language.
Juarrero's second comment was subjective in nature: con-
straints create new useful possibilities at the expense of useless
possibilities such as dissipation of energy as heat. But someone
else might be able to harness the heat energy and run a steam
engine. But then again, the steam engine was constructed to
harness the heat instead of letting the latter melt the polar ice cap.
However, Juarrero's second point is not entirely without merit. For
example, the phenomenon of proton lateral mobility, mentioned
in Sec. 4.2, constrains protons to diffuse predominantly on the
membrane surface at the expense of protons diffusing into the bulk
water.
Superﬁcially,
it
appears
that
the
constraint
reduces
randomness and equivocation, thus allowing protons to do more
useful work than otherwise. Again, it ﬁts Aristotle's concept of the
ﬁnal cause.
In the concept of downward causation, there is another serious
difﬁculty, which stems from the very foundation of evolutionary
epistemology: how does one select one cause from among many
available causes? In fact, Campbell himself harbored a misgiving
about his formulation of the blind variation and selective retention
model for problem solving; he recognized the ineffectiveness of blind
searches in ﬁnding solutions. Whereas a clueless and brainless
approach to ﬁnding solutions involves random searches or systemic
searches, intuition-based problem solving often exhibits swiftness of
going directly right to the heart of problem via a non-tortuous route,
thus conjuring up the specter of downward causation as if one could
deliberately make the solution show up at the right moment.
According to Wuketits [Wuketits, 2001], Campbell took “blind
variation” to mean that the process is “not guided by anticipation,”
although Campbell himself had difﬁculty making it unambiguous.
The same insight occurred to Poincare as early as 1908 in his book
The
Foundations
of
Science
([Poincare, 1908],
pp.
385e387).
Although many investigators have continued to treat logical
deduction as the orthodox route leading towards discovery,
Poincare pointed out that mathematical reasoning (as well as
conventional logical deduction) is not a simple juxtaposition of
syllogisms. Those logical steps d in terms of a succession of
mathematical equations or logical statements d must be placed in
a certain order. He then pointed out that most permutations and
combinations of equations were useless and it might take him a
lifetime to try out each and every such permutation and combi-
nation. In computer science jargon, it was “combinatorial explo-
sion” that overwhelmed him and forced him to seek a different
strategy. Here is how Poincare described his alternative, non-
algorithmic approach:
For ﬁfteen days I strove to prove that there could not be any
functions like those I have since called Fuchsian functions. I was
then very ignorant; every day I seated myself at my work table,
stayed an hour or two, tried a great number of combinations and
reached no results. One evening, contrary to my custom, I drank
black coffee and could not sleep. Ideas rose in crowds; I felt them
collide until pairs interlocked, so to speak, making a stable
combination. By the next morning I had established the exis-
tence of a class of Fuchsian functions, those which come from
the hypergeometric series; I had only to write out the results,
which took but a few hours.
A partial translation from plain English into our familiar jargon
is in order here. First, Poincare tried the conventional algorithmic
approach of trying out various permutations and combinations of
known mathematical facts (equations or theorems) and of sorting
out what were logically meaningful from the remaining mean-
ingless ones, but the attempt did not seem to work for ﬁfteen days.
Therefore, he changed the strategy after he had taken a hefty dose
of stimulant. Then, he used a metaphor to describe his strategy of
quasi-deterministic explorations. He emphasized that it was not a
strategy of blind searching, since he only paid attention to those
pairs of mathematical entities that interlocked to form a stable
combination. Note that he used the word “interlocked” and the
phrase “stable combination,” thus implying that he compared the
thinking process to solving a jigsaw puzzle. In this way, he could
skim through many conﬁgurations rapidly and pay special atten-
tion only to those pairs formed by interlocking in a stable conﬁg-
uration. Thus, instead of trying permutation and combination of
equations one after another, he could put the entire arrays of
known mathematical facts in his mental map (called mental im-
agery in the psychology literature), and he could skim through the
arrays rapidly and pay attention only to those fruitful permutations
and
combinations.
The
process
is
neither
deterministic
nor
random; it is quasi-deterministic and is referred to the literature of
operations research and artiﬁcial intelligence as heuristic searching
[Simon and Newell, 1958; Simon, 1992]. To make a long story short,
this mode of thinking is known as visual thinking in the psychology
literature [Arnheim, 1969; Bastick, 1982; West, 1997], whereas the
alternative methodical way of logical deduction is known as verbal
thinking [Bastick, 1982]. Indeed, logical deduction is deterministic,
since there is no individual variation if it is done correctly; it is also
completely objective.
Poincare's metaphor also works for music composing. If we
substitute musical notes for mathematical entities, the metaphor
also describes Mozart's composing style [Holmes, 1991, pp.
266e270]. Mozart claimed that he could compose a piece of
orchestral work all in his head; he bypassed those awkward or
emotionless permutations and combinations of musical notes
and tried only those which form stable interlocking music
phrasing.18 The visible part of his composing activities was sim-
ply writing it down, as Poincare did the following day after the
black coffee episode: sort of a mechanical process of memory
dump.
Following this lead, I suspect that what Campbell meant by
“not guided by anticipation” actually means that the searching
for a solution can neither be conducted by following a pre-
determined procedure nor by means of trial and error alone,
but rather by means of a compromise of the two extremes. I am
almost certain that heuristic searching was what Campbell
meant by “not guided by anticipation,” because of its unpre-
dictable nature; the solution cannot be anticipated during the
18 Incidentally, the musical counterpart of combinatorial explosion was illustrated
in Richard Wagner's opera Die Meistersinger von Nürnberg (The Mastersingers of
Nuremberg). Sextus Beckmesser's inaccurate rendition of a stolen song contained
only a small number of errors, such as substituting Leberbaum (liver tree) for Lie-
bestraum (love dream), splitting a single word into two (Morgen ich for Morgend-
lich), ignoring an umlaut (Blut for Blüt), etc., thus raising the specter of a comical
ﬁasco in an otherwise serious opera.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
707

creative process until the moment of sudden illumination:
Eureka!
We now turn to the question of how to implement heuristic
searching in the context of downward causation. Perhaps the most
dramatic demonstration was Adleman's experiment [Adleman,
1994] of solving a case of the directed Hamiltonian path problem
with seven vertices using strands of oligonucleotides as computa-
tional tools. What transpired in Adleman's experiment can be
construed as a “molecular” simulation of Poincare's visual thinking.
Since Poincare's strategy is non-algorithmic, it cannot be simulated
accurately with a digital computer program. Simply put, Adleman's
test problem called for the path of a traveling salesman to go from a
home city to a destination city via all other cities in such a way that
he visits each of the seven cities exactly once. It is instructive to go
through the detail of this experiment and to understand its
rationale.
What Adleman invoked to solve this problem is exactly what
Campbell's blind variation and selective retention model prescribes
for creative problem solving, although Campbell was not formally
acknowledged. The quasi-deterministic evolutionary algorithm
consists of the following steps:
 Generate random paths through the cities.
 Keep only those paths that begin with the home city and end
with the destination city.
 If the graph has 7 vertices (cities), then keep only those paths
that enter exactly 7 vertices.
 Keep only those paths that enter all of the vertices of the graph
at least once.
 If any paths remain, say “yes,” otherwise say “no.”
Adleman used standard molecular biology tools to encode and
solve the problem. The strategy is based on the complementary base-
pair recognition of DNA. An oligonucleotide of twenty monomeric
units with distinct but otherwise random sequences was chosen to
represent each of the seven vertices. Since each oligonucleotide
possesses a 30- and a 50-end, a polarity sense is preserved. Thus, an
edge d i.e., the directed (polarized) connection between two cities d
could be represented by an oligonucleotide of the same length with
ten from the 30-end of a vertex and ten from the 50-end of the next
vertex being connected by the edge. In this way, each edge, with its
sense of direction preserved, was represented by a distinct oligonu-
cleotide. Another oligonucleotide that was complementary to each
vertex was also synthesized. Mixing of complementary “vertex”-ol-
igonucleotides and “edge”-oligonucleotides in ligation reactions
allowed for linking of edges to form a continuous path, by virtue of
using the complementary vertexoligonucleotide as a “splint.” A huge
number of paths were then randomly generated.
Note that even though the reactions were random, only mean-
ingful paths were generated because of the use of complementary
vertex oligonucleotides (heuristic searching); only those inter-
locking pairs that form a stable combination were generated. Also
note that some of these generated paths might pass a certain vertex
more than once. A selection process must be implemented to
perform steps 2e4 to reduce the number of possible candidates,
and to reproduce only the selected ones. The strategy was to use
primers as the tools of selection and to use polymerase chain re-
action (PCR) as the tool of reproduction. It is well known that PCR
can reproduce a minute amount of any selected oligonucleotide
sample and amplify it into a highly enriched sample. By cleverly
designed experimental protocols from the repertoire of standard
techniques of molecular biology, Adleman was able to purify the
oligonucleotide that encoded the correct path. Subsequently,
Adleman and coworkers solved a 20-variable NP-complete three-
satisﬁability problem on a DNA computer [Braich et al., 2002]. The
unique answer was found after searching more than one million
possibilities. Adleman's impressive feat has helped launch the ﬁeld
of DNA computing.
The above example demonstrated that context-sensitive con-
straints arising from organization of biological structures at a higher
hierarchical level than the level of aggregates or conglomerates of
molecules could give rise to downward causation. However, Adle-
man's system was a partially man-made structure designed for
problem solving, which happens to require downward causation in
order tobe effectiveand efﬁcient. Adleman's experiment was notthe
only man-made system that demonstrated the feasibility of down-
ward causation. Chaudhury and Whitesides [Chaudhury and
Whitesides, 1992] constructed a polished silicon wafer with a
spatial gradient of wettability and demonstrated that the imbalance
of surface tension on the upstream side and the downstream side of
water droplet allows the water droplet to run uphill against the
gravitational force. If our readers have misgivings about man-made
systems, a similar mechanism works in living systems; the con-
tractile mechanism of the actomyosin motor [Suzuki, 2004] also
exhibits downward causation. These examples showcase a general
principle of downward causation enabled by afﬁnity gradients:
gradients of varying strengths of intermolecular forces due to bio-
logical organization (see [Hong, 2005a], pp. 43e49 and Fig. 10).
In the examples listed above so far, the intramolecular covalent
force remains in action, but it fades into the background and plays
the supporting role of holding atoms in a molecule together, thus
leaving the central role to weaker intermolecular forces. Intermo-
lecular forces, which include electrostatic interactions (ionic
bonds), hydrogen bonding, hydrophobic interactions, and van der
Waals interactions, are not as strong as covalent bonds, but they
confer versatility in biological functions to molecules (see later).
Downward causation is a case in point.
Actually, downward causation also plays a major role in intra-
molecular dynamics in protein folding. Proteins are polypeptides
that fold into a speciﬁc shape (conformation). A functional protein
owes its functional abilities to maintain a speciﬁc conformation and
to change the conformation upon receiving external regulatory
inﬂuences such as binding of an activating or inhibiting molecule.
An arbitrarily synthesized polypeptide usually does not fold or folds
only very slowly (blind searches). Folding is made possible by
evolution via the same mechanism of downward causation stipu-
lated by Campbell; the process of successful folding is a quasi-
deterministic process rather than trial and error. However, ther-
mal ﬂuctuations are required for the imperfect afﬁnity gradient to
function properly, i.e., to prevent trapping at a “local minimum” of
the gradient. In other words, protein folding is an intelligent and
purposeful act of problem solving. Obviously, it is the same four
types of weak intermolecular forces that act within the same
molecule rather than between two different molecules.
Perhaps the most “imaginative evolutionary invention” was the
organization made possible by the formation of biological mem-
branes. Perhaps the membrane-based organization was context-
sensitive constraints at their best. A membrane separates the
environment into two spaces. The extracellular space has an ionic
composition not too different from seawater, where life began prior
to the rise of the oxygen concentration in the atmosphere. The
intracellular space (Claude Bernard's milieu interieur) has a very
different composition; notably, this space contains more Kþ than
Naþ whereas the extracellur space contains more Naþ than Kþ, just
like seawater. It is elementary knowledge for every biomedical
student that these ion gradients are the basis for bioelectrical
phenomena including nerve excitation that is at the heart of
execution of free will. Formation of ion gradients is the standard
way for the biological system to store energy via charge separation,
much like as a man-made battery stores electric energy for
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
708

subsequent usage. As we shall demonstrate next, these ion gradi-
ents are exploited for downward causation.
Let us digress a little bit to talk about the relationship between a
force and the corresponding energy gradient. A point source of
force affects the vicinity of space and generates a distorted envi-
ronment which can be conveniently viewed as a ﬁeld of potential
energy gradient. Thus, the concept of context-sensitive constraints
dovetails with the concept of a ﬁeld of potential energy gradient.
The advantage of relating these deserves a brief comment. Instead
of reasoning in terms of forces, which require the cumbersome uses
of vector analysis, the equivalent concept of a ﬁeld of potential
energy reduces the tedious calculation to simple addition and
subtraction. This ﬁeld of gravitational potential energy serves
exactly as the constraint that guides water to ﬂow down the po-
tential energy gradient as dictated by the distribution of watersheds
(or separatrix in nonlinear dynamics jargon). It is easy to visualize
the relation between the (gravitational) force and the corre-
sponding ﬁeld of its potential energy: the slope of the grade of a
mountain-climbing road is proportional to the force. Mathemati-
cally speaking, the force is the negative of the gradient of the po-
tential energy19: f ¼ VV, where f is the force, V is the potential
energy and V is the gradient operator. The concept is also applicable
to the analysis of electric ﬁelds formed by ion separation across a
membrane. It is even applicable to the analysis of ionic diffusion, in
accordance with statistical mechanics and physical chemistry.
Just as mechanical motion of an object goes down the potential
energy gradient, ion diffusion goes down the concentration
gradient. This force f which acts on diffusible molecules or ions
collectively, on a per molecule or per ion basis, is given by the
negative gradient of the chemical potential (energy), which is a
function of the concentration, C:
f ¼ kTVln C
where V is the gradient operator, ln denotes natural logarithm, T is
the absolute temperature and k is the Boltzmann constant. The
corresponding force, F, based on the measurement per mole of
diffusible molecules or ions (i.e., per 6  1023 molecules or ions), is:
F ¼ RTVln C
where R is the universal gas constant. Note that the force, f or F, acts
on a large number of molecules or ions collectively, but not on a
single molecule or ion in isolation. Therefore, it is not a bona ﬁde
force but rather a phenomenological force. Of course, it is generated
by a context-sensitive constraint, and it ﬁts the role of downward
causation; the organization can be conﬁgured so that the net ion
diffusion is directed to a certain desired direction even though the
individual molecules or ions continue to undergo random (Brow-
nian) motion in all directions. It exists because of the constraint
generated by biological structures. It is an emergent phenomenon,
but it is not a phenomenon that is generated by an emergent force.
The force is therefore appropriately called a phenomenological force.
Another
emergent
phenomenon
in
biological
membranes
generated by a context-sensitive constraint is the enhanced proton
lateral mobility on the membrane surface mentioned in Sec. 4.2. It
ﬁts the notion of downward causation for the following reason.
Note that the proton gradient constitutes electrochemical energy
stored across the membrane; the stored energy is subsequently
used to power membrane-bound ATP synthase to produce ATP, the
universal energy currency in the living world. Prior to the dis-
covery of proton lateral mobility, theoretical biologists were
puzzled as to why the cell wastes energy by dumping protons into
the bulk cytoplasm instead of transporting them directly to the
site of ATP synthase, which is located in the membrane (Paciﬁc
Ocean Effect). The discovery of enhanced proton lateral mobility
converted what used to be the Paciﬁc Ocean effect into an energy-
conserving San Francisco Bay effect. The phenomenon has its
quantum origin. The diffusion constant of protons on the mem-
brane surface is enhanced, via a hydrogen-bonding network on
the membrane surface, as compared to its diffusion constant in the
bulk water.
The
above-mentioned
examples
illustrate
the
concept
of
context-sensitive constraints. All these processes appear to be
purposeful, thus ﬁtting the deﬁnition of downward causation d
causality operating from the whole to the parts. Notably, there is no
new emergent force involved, with perhaps the exception of ionic
diffusion, which nevertheless involves a phenomenological force
rather than a true force, as explained above.
However, there is one prediction made by theoreticians that is
conspicuously absent. Contrary to some theoreticians' expectations,
low-level forces are neither shielded off nor overpowered. As shown
in examples above, weak intermolecular forces play a central role in
downward causation by providing context-sensitive constraints.
Intermolecular forces allow for complex formation between various
molecules: the complex associates and dissociates readily, thus suit-
ing the purpose of many biological functions, including molecular
recognition that underlie the process of hormonal actions, neuro-
transmitter actions, and immune reactions as well as base-pairing in
DNA and RNA. Whereas weak intermolecular forces are conspicuous
in downward causation, the interatomic force that forms covalent
bonds does not cease to exist; they continue to hold atoms inside a
molecule together.
Juarrero, together with her predecessor Lindsay, cast downward
causation not as new types of forces but rather as new types of
causations d a cause without force, so to speak. The idea makes
sense in the case of human behaviors in the context of social in-
teractions, ethics and religious beliefs, etc. But Juarrero and Lindsay
invoked ﬂawed arguments to explain the concept, as pointed out
earlier. We have reason to cast doubt on the validity of their
argument for free will in the context of neural substrates. Juarrero
drew ample evidence from neurological examples. Still, the evi-
dence seems indirect. But the idea remains attractive since there is
no way for free will to exist without the capability of downward
causation in the context of neural substrates. I simply have to
suspend my judgment at this moment until new and direct
empirical evidence begins to emerge.
If, however, Juarrero's idea is modiﬁed slightly, it begins to make
eminent sense. Instead of generating forceless causes or modifying
natural laws, context-sensitive constraints can modify dispersions
so as to generate downward causation. Note that modifying dis-
persions of a trajectory does not violate physical laws, but over-
powering physical forces with context-sensitive constraints does.
In earlier sections, we alluded to how multi-level organization
prevents chaos due to quasi-deterministic physical laws as well as
due to indeterminacy of boundary conditions from spreading to
higher levels. What happens in the latter case is not shielding the
action of the forces per se, but rather managing the dispersions,
which in the extreme case would otherwise have reached chaotic
proportions. It is not difﬁcult to envision how context-sensitive
constraints, alluded to by Juarrero, can change the trajectories of
small molecules by modifying the dispersions (downward causa-
tion). Profound changes can thus be effected by propagation of
downward causation along a chain of successive events from
higher-level dynamics down to the lowest level without violating
physical laws. This slight conceptual change sets us onto a different
path for future investigations. Instead of ﬁshing for evidence of
19 This mathematical relationship is valid only for conservative forces, which
include all types of forces which obey inverse-square laws.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
709

downward causation that alters low-level forces or evidence of
downward causation without forces, one investigates how context-
sensitive constraints modify and shape the dispersions. This new
perspective offers collaboration between phenomenology and
reductionist sciences, and phenomenology no longer poses the
menace of a hostile takeover.
Of course, the fact that no new forces appear in the above ex-
amples does not prove that no new forces exist in downward
causation. However, in order to speculate on their presence, it is
necessary for theoreticians to offer evidence or to suggest experi-
ments to detect their presence.
6.2.4. Origination of creativity and free will
In the examples listed in Sec. 6.2.3, the structural organizations
exhibiting downward causation were all products of evolution. Just
like Campbell's evolutionary epistemology, no mysticism or su-
pernatural forces need to be invoked to explain them. The concept
of downward causation as applied to biological structures, as
shown in these examples, is fundamentally sound since the origi-
nation resides unequivocally in evolution. However, it is far more
challenging to extend the concept of downward causation to
explain free will and creativity. Both free will and creativity
constantly face demands, including some unexpected ones, that
cannot be met by routine responses or standard operating pro-
cedures. Both free will and creativity demand the execution of the
corresponding appropriate actions of causation on a time scale far
less than evolutionary time. These apparent miracles beg for a new
explanation which Campbell's theory does not provide.
Let us ﬁrst address the problem of creative problem solving. In a
previous investigation, I presented evidence indicating that verbal
thinking is more suitable for the task of solving routine problems
whereas visual thinking is critically needed for solving novel
problems. Most people, myself included at least in the past, prob-
ably switch between these two modes of thinking without
conscious awareness and without overt decision making. Once I
recognized the power of visual thinking, I began to be able to
deliberately invoke visual thinking to solve problems that could not
be solved otherwise (cf. the paradox of Achilles and the Tortoise
[Hong, 2013a]; the reconstruction of Galileo's thought process
[Hong, 2013b]). Here, it appeared that the mind took control of the
selection of the mode of thinking in a top-down fashion, as the
need arose. Of course, I could not convince others with these sub-
jective feelings. But skeptical readers may try it on their own, so as
to falsify my claim or experience it on a ﬁrst-hand basis.
Both free will and creativity rely on the underlying neural sub-
strates for performance. Experimental evidence of this top-down
downward causation is scarce. Jeffrey Schwartz, a famed psychia-
trist and professor of psychiatry, pioneered a treatment plan for
obsessive-compulsive disorder (OCD). He documented the treat-
ment plan in two books, Brain Lock: Free Yourself from Obsessive-
Compulsive Behavior [Schwartz and Beyette, 1996] and The Mind
and The Brain [Schwartz and Begley, 2002]. He reported experi-
mental observations that patients of obsessive-compulsive disorder
could alter physiological activities in crucial parts of their brain by
exerting will power. He suggested that the thought processes of
patients who suffer from obsessive-compulsive disorder might just
be an ideal model in which to study such executive control
processes.
Additional convincing evidence of downward causation came
from research in robotics and brain-computer interfaces (BCIs)
carried out in the research group of Richard Andersen, a Caltech
professor of neuroscience [Musallam et al., 2004; Strickland, 2015].
Erik Sorto, who suffered from paraplegia for nine years, agreed to
an experiment in which electrodes were implanted in his posterior
parietal cortex at a location previously located by fMRI (functional
magnetic resonance imaging). The location was determined when
Sorto executed his intention to move his now-paralyzed arms in a
speciﬁc way. Sorto succeeded in directing a robot to carry out his
wish to pick up a glass of beer to sip. This particular area of the
parietal cortex is apparently the part of neural substrates that
directly received Sorto's mental command to formulate arm motion
before the resultant signals were sent to the motor cortex. This is
probably one of the clearest pieces of experimental evidence
demonstrating downward causation at work. These two pieces of
isolated evidence appear to be convincing, at least to me. But more
work needs to be done and more details need to be worked out to
offset an avalanche of experimental evidence from Libet's work,
which has been embraced by many eminent neuroscientists. I
therefore optimistically await additional evidence in the future. In
spite of this new prospect, the issue of origination will probably
remain unsettled for quite some time.
However, I amnot the onlyonewho stillharborsuncertaintyabout
origination of free will. In the book The Open Universe: An Argument for
Indeterminism, Popper expressed the same concern ([Popper, 1982],
Addendum 1). Popper confessed that he was an indeterminist in re-
gard to the free will problem. But he argued forcefully that indeter-
minism alone is not enough. He wrote, “[W]hat we want to
understand is not only how we may act unpredictably and in a
chancelike fashion, but how we can act deliberately and rationally.” In
other words, Popper thought that quantum indeterminacy may be
sufﬁcient to account for alternativism, but he thought that origination
and intelligibility still remain to be accounted for. As presented earlier,
intelligibility of free will can be explained. The issue of origination d
the subjective feelings of the self's deliberately calling the shots,
variously known as intentionality, the agency, the mental power, etc. d
remains to be elucidated.
6.2.5. Insights from computer simulations
In the investigation of complex dynamic systems, computer
simulations have provided a useful approach. Computer simula-
tions have also been a critical part of creativity research. It is
natural to expect computer simulations to play a similar role in
free will research. Curiously, here computer simulations play two
distinct roles. The ﬁrst role is to enhance the performance of
problem-solving programs, whereas the second role helps eluci-
date the origination issue of free will in a negative way by
showcasing the absence of origination. Since the origination issue
is shared by both creativity and free will, we shall ﬁrst brieﬂy
sketch what we have learned from artiﬁcial intelligence (AI)
research, in which computer simulations of creativity are the
main, if not the only, approach.
In the early stage of AI research, the most primitive problem-
solving computer programs were known as the expert systems.
Essentially, the computer was loaded with a database containing
relevant knowledge in terms of numerous conceivable sets of rules
for utilizing the knowledge and canned answers. In response to a
particular problem presented to the computer, the computer was
programmed to search the database and identify the relevant rules
that match the problem, and it then disclosed the solution so
obtained. The computer is merely a faithful slave who carries out
the wishes of the programmer. The computer is not expected to
make any judgment other than matching problems to solutions,
just like a dutiful bureaucrat. The zombie-like activities were
immortalized in the Chinese Room Argument of John Searle
[Searle, 1999], who thus deeply offended strong AI advocates. This
mode of computer-based reasoning is essentially verbal thinking
or rule-based reasoning; the rules are located in the database and
the computer's task is simple matching of a given problem to the
relevant rules found in the database. In my previous investigation
of human creativity, I have explained why rule-based reasoning is
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
710

not suitable for discovering solutions of novel problems because it
cannot solve problems for which the canned answers have not
been included in the database. Conversely, in order to meet the
need for creative problem solving, visual thinking must be
invoked. Visual thinking involves parallel processing of data, but a
digital computer is primarily designed to perform sequential
processing. Therefore, a digital computer can only imitate parallel
processing by means of what I call pseudo-parallel processing:
rapid deployment of sequential processing generates the illusion
of parallel processing ([Hong, 2005b], p. 226). But there is one
thing a digital computer can still do albeit in a limited sense only:
heuristic searching.
Subsequently, Simon and other AI pioneers cleverly imple-
mented heuristic searching in their problem-solving programs
[Simon, 1977, 1979; 1989; Langley et al., 1987]. Their programs
succeeded in rediscovering Boyle's law, Snell's law of optical
refraction and a number of other well-known numerical laws in
physics and chemistry. A program named “Logic Theorist” managed
to discover a shorter and more elegant version of proof of a theorem
in Chapter 2 of Whitehead and Russell's Principia Mathematica than
the original version [Newell et al., 1962].
The spectacular success of Simon and other AI pioneers stem-
med from the strategy of prescribing heuristics which are guide-
lines that constrain the searches for solutions to a small subset of
all possibilities d the most fruitful directions, thus preventing the
computer from wasting time in unfruitful searches. Further ad-
vances led to the development of agent technology [Jennings and
Wooldridge, 1998; Murch and Johnson, 1999]. Agent technology
allows the computer to be equipped with “motivation” and “will
power” to pursue its entrusted goals. It is apparent that the price
of doing so is to give the computer a certain degree of freedom
(autonomy) d a rudimentary form of free will or, rather, pseudo-
free will.
Proceeding along this line of thinking, AI designers eventually
included attitudes such as beliefs (B), desires (D), and intentions (I) in
the design of rational agents. The design principle, known as the BDI
architecture [Rao and Georgeff, 1991; Georgeff and Rao, 1998], is
based on Cohen and Lebvesque's [Cohen and Levesque, 1990]
formalization of some of the philosophical aspects of Bratman's
theory on intention and action [Bratman, 1987; Bratman et al.,
1988]. The BDI architecture installs a software agent in the com-
puter program, which can automatically gather information (from
the Internet, for example), harbor beliefs, manifest desires and in-
tentions, formulate detailed tactics and actions by exploring
various options in accordance with some general heuristic guide-
lines, and commit computing resources towards the realization of
its goals. In other words, the computer is now equipped with
simulated free will. But the purpose is to enhance the computer's
problem-solving ability rather than providing proof of existence of
free will, which it did not do.
Can free will simulations shed light on its origination? Ironically,
success in computer simulations of free will actually plays into the
hands of free will deniers. Just consider simulations of conscious-
ness. Many of the data processing steps are hidden from the com-
puter operator as well as outside observers because the report of
the computer's inner working is not essential for the casual users to
know; only the ﬁnal results need to be reported as a display on the
monitor or as outputs to a new data ﬁle. Of course, the programmer
can program the computer to give as many intermediate reports as
needed for debugging purposes. The programmer certainly can
program the computer to give reports after a sensor has detected a
sensory stimulus as well as after a decision has been made to
perform some actions, to feign an emotional response, or even to
swear that it has genuine free will. But these additional reports are
just epiphenomena. The oath would be dismissed as an act of fakery
rather than proof of existence of free will. It may demonstrate a
highly believable simulation of a conscious being, but actually it
does very little to resolve the issue of origination, because of our
prior knowledge that the true originator is the computer program-
mer. Most people, except perhaps strong AI advocates, would not
regard the computations performed by a software agent as acts of
free will. Let us now consider the corresponding situation in crea-
tivity simulations.
Since the inception of AI research, investigators have been
obsessed with the question of whether a digital computer can
think. Initially, the question was driven by an effort to design a
computer program that can think like a human being or better.
Indeed, the goal of AI research is human-like performance rather
than proof of existence of free will. The question is: How does one
judge the quality of performance? Some years ago, Alan Turing
designed a test of machine intelligence d the Turing Test. The
performance of a machine is communicated to a judge behind a
curtain by means of machine-readable and machine-writable
communications (e.g., using an old-fashioned teletypewriter);
the criterion for passing the Turing Test is for the machine to
masquerate as a human while succeeding in fooling the judge
[Turing, 1950]. But the test alone did not settle the question of
whether a machine could think like humans. In the late 20th
century, philosophers Paul Churchland and Patricia Churchland
[Churchland and Churchland, 1990] voted for a thumb-up verdict,
but philosopher John Searle gave a negative vote [Searle, 1990].
Needless to say, the majority of AI investigators did not greet
Searle's verdict kindly. The disagreement might be due in part to
the very deﬁnition of thinking. Interestingly, in a BBC broadcast
aired in the early 1950s about machine intelligence, Turing
declined to give a deﬁnition of thinking [Proudfoot, 2015].
Interestingly, the advent of a new breed of biomedical students
around the 1990s d whom I dubbed dumb high-achievers d
inadvertently provided a strange answer to the question [Hong,
1998a; Hong, 2013a; 2013b]. Although most ordinary people un-
consciously switch between visual thinking and verbal thinking,
dumb high-achievers exclusively practice only verbal thinking,
much like an expert system. Although they often score high in
standardized testing, they are poor problem solvers because they
are unable to reason with intuition, which I equate with visual
thinking [Hong, 2013a]. Undoubtedly, their intellectual perfor-
mance is no match for that of modern problem-solving computer
programs. With a clean margin, they would fail the Turing Test, but
Simon's programs would pass with ﬂying colors.
Perhaps the historic 1996 chess tournament match in Phila-
delphia between world chess champion Garry Kasparov and an IBM
computer named Deep Blue must be a watershed for many, if not all,
detractors of machine intelligence. Kasparov admitted that he
narrowly escaped defeat.20 Although chess is basically a rule-based
game, Deep Blue still made an heroic effort to invoke visual thinking
which is basically parallel processing: It deployed 192 processors in
parallel and invoked heuristics written by a team of IBM pro-
grammers who had expertise both in computer design and pro-
gramming as well as in chess [Newborn, 1997]. There is absolutely
no question that a digital computer can beat a human genius in
terms of intellectual performance. But can Deep Blue think and
think like Kasparov?
I decline to give a straight answer. Ostensibly, digital computers
can perform intellectually better than dumb high-achievers, but I
would not draw the conclusion that digital computers can think. The
slightly different phrasing of the two opposite verdicts in answer to
two similar questions reﬂects my prior knowledge of the computer
20 Deep Blue eventually defeated Kasparov in 1997.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
711

being subservient to the programmer: It is just a man-made ma-
chine, whose originators were the above-mentioned IBM team.
Origination was often at the center of debates between strong AI
supporters and their detractors, although it was seldom mentioned
explicitly.21 I suspect that some debaters might not be consciously
aware of it. Essentially, Deep Blue was preprogrammed and
controlled by a Svengali d or, more precisely, a team of Svengalis
from IBM d behind the scenes, even though the Svengali himself
could not perform just as well. Curiously, it was Deep Blue's sur-
rogate free will d the IBM team's decision d that caused Deep
Blue's defeat in 1996. In retrospect, the human team made a
mistake in rejecting Kasparov's offer of a draw, or else Kasparov
would have entered the ﬁnal (sixth) game with one victory, one loss
and three draws in his hands and he could have lost the tournament
due to extreme emotional stress. In contrast, Deep Blue was not
programmed to freak out under duress.
I did not want to give the computer full credit of being able to
think because there was something missing in computer simula-
tions as compared to real humans. It is the element of agency or
origination that is missing in simulations; the computer is not
really a self-starter. In fact, one could have asked the question: Do
digital computers have free will? I will bare my thoughts in terms of
a metaphor, which seems just as appropriate for the question of the
origination of free will as for that of thinking. The metaphor
appeared originally in Richard Wagner's four-part opera cycle, Der
Ring des Nibelungen (The Ring of Nibelung). Pardon me for indulging
in self-quoting instead of coming up with a new version of narra-
tion [Hong, 2013b]:
Wotan, the chief god of Valhalla, ruled the world by signing
numerous contracts with other gods. He sought control of the
magic ring forged by Alberich, the chief of the underground
kingdom Nibelheim. However, he could not just go ahead and
seize the ring without abrogating a certain treaty. He felt that
he was the least free of all, but he cleverly dreamed up a plan.
First, he sired a son, Sigmund, and a daughter, Siglinde. Through
a dubious act of incest, the latter two bred a son, Siegfried.
Siegfried was the hero of heroes, an übermensch, so to speak.
Wotan avoided direct contacts with Siegfried in such a pains-
taking way that Wotan could claim that Siefried had complete
free will to carry out a self-determined act. His secret plan was
to create the circumstances so that Siegfried could eventually
get the ring for him without the appearance of his involvement
in the grand conspiracy. The trouble was that his elaborate
scheme could not escape the detection of his wife, Fricka. Fricka
saw through the trick and confronted Wotan with the accusa-
tion that Wotan had planted a sword on the trunk of an ash tree
in anticipation that Siegmund would ﬁnd it at the dire moment
of great need to ensure what would transpire later in (almost)
exactly the way Wotan had expected. In other words, Fricka
knew that Wotan was pulling the strings behind the scene all
along, thus exerting an inﬂuence which “incline[d] without
necessitating” the desired outcome, but it happened anyway.
That was exactly what the computer programmer did by
planting the heuristics and other enabling conditions, just like
the waiting sword on the ash trunk, so that the computer could
pick it up in the right place at the right time, thus fulﬁlling the
programmer's wish and duping the rest of us into calling the
feat a genuine creative act [and an exercise of genuine free
will]. Amen!
Frankly, I have no clues to the origination of free will at present,
although I believe that the mechanism of downward causation can
carry out the rest of the process for free will to take effect once the
mysterious source of origination jumpstarts free will. In other
words, in addition to downward causation, we need a satisfactory
theory that is equivalent to the “Big Bang” theory in cosmology in
order to explain the origination of free will. At present, it is a better
policy to suspend our judgment rather than jump to conclusions.
7. Can phenomenology of free will attain scientiﬁc rigor?
The Achilles' heel of phenomenology is obviously its inherent
subjectivity. The mere mention of subjective feelings and subjective
experience puts phenomenology on a collision course with conven-
tional science. If one insists that objectivity be maintained in an
investigation at all times in order to be qualiﬁed as scientiﬁc, then
there is no hope of making phenomenology scientiﬁc. Yet the
developmentof cognitive science in the latterhalf of the 20thcentury
brought reductionist neuroscience together with phenomenology as
uneasy neighbors; one can no longer afford to ignore the other as the
nearest kin.22 This brings up the topic of naturalizing phenomenol-
ogy d i.e., making phenomenology scientiﬁc, or, rather, weaving
phenomenology and reductionist neuroscience into one fabric.
7.1. Methodologies in phenomenology
According to Gallagher and Zahavi [Gallagher and Zahavi, 2008],
Edumund Husserl did not aspire to naturalization of phenome-
nology. In fact, Husserl constantly emphasized the limitations of a
naturalistic account of consciousness, and proposed his own
phenomenological method as a non-naturalistic alternative. Hus-
serl's followers apparently ignored this particular teaching of his.
The following quotation from Gallagher and Zahavi is quite
appropriate in light of our recent investigation of humans' high
creativity:
[T]he issue we want to address is simply whether phenome-
nology can be put to work in experimental science. … We will
demonstrate to what extent phenomenology addresses issues
and provides analyses that are crucial for an understanding of
the true complexity of consciousness and cognition and which
are nevertheless frequently absent from current debates, and
show how it can offer a conceptual framework for under-
standing the mind that might be of considerably more value
than some of the models currently in vogue in cognitive science.
Here we endorse Gallagher and Zahavi's goal of making phe-
nomenology work with science rather than in opposition to it. We
do not have to go far to ﬁnd such examples. In creativity research, it
is evident that many investigators in psychology paid almost no
attention to phenomenology; their creativity theories did not
bother to explain the well-known “aha!” phenomenon and seren-
dipity, for example. In fact, some investigators even cast doubt on
the credibility of anecdotal reports of serendipity for no other
reason than their own lack of similar personal experience. Instead, I
have found many phenomenological insights to be beneﬁcial in
21 Strong AI supporters could not care less about the origination issue; they have
cared primarily about the quality of the computer's problem solving skills. In
contrast, philosophers are apparently bothered by the computer's lack of origina-
tion, although conventional wisdom has not explicitly deﬁned the term “thinking”
with an accessory requirement of origination.
22 A branch of philosophy known as analytical philosophy pursues goals similar to
those of phenomenology. Yet the two sub-disciplines have sustained a hostile
relationship.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
712

creativity research. Curiously, these insights appeared mainly in the
writings of Albert Einstein, Richard Feynman, Carl Friedrich Gauss,
Galileo Galilei, Stephen Hawking, Immanuel Kant, Wolfgang
Mozart, Henri Poincare, Nikola Tesla, Richard Wagner, etc. With the
exception of Kant, none of them were philosophers or psychologists
by training or by profession, but all of them had ﬁrst-hand expe-
rience in high creativity.23
The issue confronting phenomenology is how to attain scientiﬁc
rigor in methodologies. The plurality in the word “methodologies”
not only reﬂects the plethora of methodologies but also implies the
spectacular lack of consensus in approaches. There is no need to
drag non-phenomenologist readers through the detail of various
proposals and the mud of accompanying confusion, much less
power struggles and backstabbing. Let us focus on the pre-
dicaments in past attempts to naturalize phenomenology and then
ﬁgure out how to go around the obstacles. Basically, the debates
have revolved around the differences in the ﬁrst-person perspec-
tive and the third-person perspective. Consciousness and related
phenomena are primarily ﬁrst-person experience, but there are
publicly observable attributes that can be subject to third-person
observation and interpretation. Scientiﬁc objectivity requires a
detached, third-person approach to publicly observable phenom-
ena. Phenomenologists tried to distance themselves from old-
fashioned
introspection,
and
the
phenomenological methods
were carefully designed to “avoid biased and subjective accounts.”
They claimed to be able to distinguish an account of subjective
experience from a subjective account of experience. But Dennett
[Dennett, 2001] claimed, “First-person science of consciousness is a
discipline with no methods, no data, no results, no future, no
promise. It will remain a fantasy.” What Dennett proposed instead
is known as “heterophenomenology” [Dennett, 1991, 2001, 2007].
According to this method, investigators need to adopt a strict third-
person methodology in the study of consciousness. This means that
its only access to the phenomenological realm will be via the
observation and interpretation of publicly observable data.
Here is a serious blind spot. For Dennett and his followers to
obtain data based on the third-person methodology, they need
to interpret the data, i.e., they need to have a preconceived
(ﬁrst-person) idea about the data.24 In other words, they render
their own ﬁrst-person interpretation of data collected from some
third persons' experience.25 In the end, they have simply shifted
the subjectivity of the test subjects to the investigators' own
subjectivity. They can hide subjectivity in this way, but they can
never completely eliminate it, just like an ostrich burying its
head in the sand. In brief, there is no way to completely elim-
inate subjectivity by means of either the ﬁrst-person or the
third-person
methodology.
Furthermore,
the
third-person
method is not necessarily more reliable than the ﬁrst-person
method since the practice is tantamount to second-guessing
someone else's subjective experience. A couple of examples
illustrate this important point.
Nikola Tesla was often credited with being one of the hardest
workers, but he was not in complete agreement with this third-
person assessment. He pointed out that, “if work [were] inter-
preted to be a deﬁnite performance in a speciﬁed time according to
a rigid rule, then [he might] be the worst of idlers” [Tesla, 1977]. In
other words, he was highly selective in work that he was willing to
devote his time to. This discrepancy in the ﬁrst and the third person
assessment was not entirely due to carelessness of the third person.
For a long time, mainstream psychologists recognized only one
kind of motivation. But the work of Edward Deci and coworkers
[Deci and Ryan, 1985; Deci, 1995; Deci et al., 1999] revealed that,
while intrinsic (self-initiated) motivation often enhances creativity,
motivation initiated by external rewards, such as fame, fortune and
power, leads to the opposite effect. Obviously, Tesla was driven by
intrinsic motivation d to the point of being a documented victim of
obsessive-compulsive disorder [Tesla, 1977; Hong, 2006].
De Waal [De Waal, 1998], a primatologist specializing in ape
communications, once overheard a conversation between two
zoo visitors. One tried to explain what the ape meant by its body
language to the other, but, according to de Waal, the interpre-
tation was totally off the mark. Obviously, both the casual
observer and de Waal relied on a third-person interpretation of
the ape's behavior. But de Waal learned what the ape meant by
life-long observation and intimate interactions with the ape,
much like how a typical infant learns the mother tongue by
interacting with the mother, rather than taking a crash course in
a language class and memorizing the vocabulary and grammat-
ical rules. De Waal's interpretation was thus able to achieve a
higher degree of overall consistency over an extended period of
observation. In conclusion, third-person interpretations may not
always be reliable.
The above examples show that it was indeed possible to get
“correct” third-person interpretation of a human's inner thoughts.
However,
phenomenologists'
one-size-ﬁts-all
methodologies
designed for gathering of phenomenological data in an assembly-
line fashion seem awfully inadequate. What works for molecular
biology is not necessarily suitable for the study of human con-
sciousness. The brutal reality is that ﬁrst-person and third-person
assessments are neither always reliable nor always unreliable,
thus putting phenomenological methodologies on shaky grounds.
Does this reality dash the hope of naturalizing phenomenology?
Perhaps we can learn something from how psychological research
achieves objectivity.
7.2. Is contemporary research in psychology really objective?
Speaking about psychology, the ﬁrst thing that comes to (my)
mind is statistical methodology. In the spectrum of all scientiﬁc
disciplines, psychology is positioned in the middle, i.e., it is posi-
tioned at the boundary between hard science and soft science, so
to speak. Instead of the dichotomy of hard vs. soft science, a more
precise way of characterizing sciences is the degree of mathema-
tization. The latter notion brings up the specter of clockwork
precision conveyed by the agreement between the measured data
and the predictions of a mathematical model. This is the same
specter that lulled Laplace into his unjustiﬁed conﬁdence about
absolute determinism and provided Ernest Rutherford with the ill-
justiﬁed rationale to declare, “Qualitative is nothing but poor
quantitative.”
As mentioned earlier, the phenomenological control laws for
psychological phenomena are usually plagued with such large
dispersions that they do not inspire Laplace-like conﬁdence for
clockwork precision, much less Rutherford-like prejudice in regard
to the degree of mathematization. In physics, experimental data
points often fall on a theoretical curve so precisely that validation of
a theory requires only inspection with the naked eye. In contrast,
errors or variances of measured psychometric parameters are so
huge that interpretations are often ambiguous and difﬁcult. Sta-
tistical methodology was thus developed to judge the consistency
between the theory and the collected experimental data. The
practice of statistical analysis in psychology thus became the
23 High creativity is the type of creativity that differs considerably from the run-
of-the-mill type of creativity, also known as “me-too” creativity.
24 Recognizing the meaning of a piece of data means matching the content of data
to a preconceived mental pattern of the interpreters [Hong, 2013a].
25 Note that the difference between the ﬁrst person and the third person is
relative; it just means the two persons are unrelated and detached as far as
interpretation of consciousness is concerned.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
713

hallmark of objectivity, and the practice spread to biomedical sci-
ences, especially in new drug discovery. Scientiﬁc proof is almost
synonymous with statistical certiﬁcation of collected experimental
data in reductionist biomedical science.
It is claimed that statistical methodology minimizes intrusion of
subjectivity and helps safeguard objectivity in psychological
research. However, this alone does not guarantee objectivity. The
quest for objectivity is nothing wrong. But when excessive
emphasis on objectivity is combined with some misconceptions
and/or dubious scientiﬁc practices, an explosive mixture is inad-
vertently formed. First, most contemporary scientists hold the view
that scientiﬁc research begins with a logically deduced hypothesis
(see [Medawar, 1967; Lawson, 2003]; see also the quotation from a
website in Sec. 1). Second, statistical certiﬁcation of consistency
between the hypothesis and observed data is considered as proof of
the validity of the hypothesis d objectively established proof.
Third, most reductionist scientists are not aware of Karl Popper's
philosophy, especially his falsiﬁability argument: absolute proof of
a theory is not possible; one just eliminates weaker alternatives. As
a
consequence,
investigators
readily
think
that
whichever
hypothesis happens to come to mind is the objectively and
logically deduced one.26 If statistical testing happens to conﬁrm
d by virtue of failing to reveal any inconsistency d the agreement
between the hypothesis and objective data, the hypothesis is
elevated to the status of the objectively conﬁrmed conclusion.
In reality, statistical correlation was not designed to reveal
causal relationship; statistical certiﬁcation is nothing more than a
consistency check or, rather, an objective method to detect in-
consistencies. In view of Popper's falsiﬁability argument, the
objectively conﬁrmed conclusion may not be the best explanation,
and perhaps not even a valid explanation at all (see below).
Fortunately or unfortunately, misguided reductionist theories
which happen to pass the statistical test are often too short-lived to
have major negative impacts; constant renewals of research ac-
tivities as well as constant supplies of new investigators help clean
up these unwanted side effects. But it is a different story in
psychology.
In the investigation of psychological phenomena, one seldom
encounters only a single factor. Worse yet, when multiple factors
are involved, they are usually not independent factors but mutually
interacting factors. Some factors are primary whereas others are of
secondary importance. Still other factors are coupled with relevant
factors, but they actually bear no causal relationship with the topic
being investigated.27 This is bad news for psychology. A simple
example sufﬁces to explain the pitfall.
It has been reported that prior knowledge hinders creativity
[Knoblich and Oellinger, 2006]. Taken to the extreme, this ﬁnding
implies that those who know the least seem to be in the best po-
sition to solve a difﬁcult problem; the inference seems to promote
ignorance. But common sense tells us that this stretched inference
cannot be true. However, it is not difﬁcult to come up with a better
explanation. It is well known in psychology that conformity is
anathema to creativity [Crutchﬁeld, 1962]. In fact, Richard Wagner
made a big point about conformity. In his opera Siegfried, the third
of the Ring Cycle, the dwarf Mime, a trained blacksmith, attempted
to mend the broken sword Notung by trying all conventional
methods, e.g., soldering, welding, splinting, and you name it. He
failed miserably, and he complained that he probably knew too
much for his own good, thus becoming incapable of jumping out of
the box. Siegfried, who knew nothing about fear, was the
consummate non-conformist. Instead, he simply ﬁled the broken
blades down to small chunks of ﬁles, melted them down, and recast
the legendary sword afresh in its totality. He was obviously an
outsider in the trade and craft of blacksmithery. One of the side
effects of being an outsider is ignorance. Although ignorance is not a
relevant factor for creativity, non-conformity is. The two factors
happen to be coupled together; if one does not know a taboo, one
does not know about the fear accompanying violation of that taboo.
I cited this speciﬁc example so as to emphasize that not all psy-
chological investigations end up in ruin and not everyone falls
victim to the pitfall of statistical methodology; the research on
conformity is still valid. But the pitfall looms on the horizon; it just
waits for a novice in an unguarded moment to raise its ugly head.
In the investigation of complex phenomena, the best theory is
usually not obvious and it is unlikely to be the one that comes to
mind easily, especially in difﬁcult or controversial problems d that
is precisely why the problem is difﬁcult and/or controversial to
begin with. Insistence upon objectivity discourages actively shop-
ping for better hypotheses. Thus, excessive reliance on statistical
methodology may end up committing the sin of subjectivity in the
name of objectivity. It seems that the more one insists upon ob-
jectivity, the less one can escape from subjectivity.
It is well known that experimental results in psychology are
notoriously theory- or model-dependent [Lepper et al., 1999].
Simply put, it is possible for a misguided hypothesis to suggest
dubious experimental designs that happen to collect ﬂawed data to
successfully prove the ultimately untenable hypothesis, much like a
self-fulﬁlling prophecy.
This important lesson has not generated much soul-searching,
and it is generally unknown or ignored. Both novices and
seasoned investigators alike have committed this sin occasionally if
not endemically. Therefore, we need to pick a high-proﬁle case to
raise awareness of this potential pitfall.
In his New York Times bestseller book Thinking Fast, and Slow,
Daniel Kahneman [Kahneman, 2011] adopted a theory classifying
human thinking faculties into two systems: the fast intuitive sys-
tem and slow deliberative system. Kahneman credited this theory
to Seymour Epstein, Jonathan Evans, Steven Sloman, Keith Stano-
vich, and Richard West ([Stanovich and West, 2000]; [Evans, 2008];
[Evans and Frankish, 2009]). The main thesis of his book is that the
fast system is responsible for errors in judgment and decision
making, whereas correct judgment requires the participation of the
slow system. Kahneman cited a test problem once chosen by Shane
Frederick's Cognitive Reﬂection Test, because it often evokes an
immediate intuitive answer that happens to be incorrect (see
[Kahneman, 2011], p. 65):
In a lake, there is a patch of lily pads. Every day, the patch
doubles in size. If it takes 48 days for the patch to cover the
entire lake, how long would it take for the patch to cover half of
the lake?
24 days OR 47 days
According
to
Kahneman's
interpretation,
fast
thinking
is
responsible for selecting the wrong answer 24 days, whereas
reaching the correct answer 47 days requires slow thinking. In my
opinion, this interpretation is oversimpliﬁed since there is more
than one way to execute fast thinking. The standard method for
26 Kahneman concluded that people have a tendency to answer questions with
the ﬁrst idea that comes to mind [Kahneman, 2011, p. 65].
27 Statistical inferences sometimes reveal an irrelevant factor that happens to be
coupled with the relevant factor. A hilarious example can be found by Googling the
following keywords: Italian Time, Italiantime or Le palle del ciuchino. A one-minute
YouTube video segment shows that an Italian farmer could tell time accurately by
touching his donkey's belly, as conﬁrmed by means of statistical correlation; it
worked three times in a row. The truth was that the lazy sun-basking farmer had to
lift his donkey's belly so that he could view a village clock at a distance without
having to get up.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
714

solving this problem calls for the application of calculus. First, one
recognizes that the growth rate of lily pads is proportional to the
size of the patch, and then one converts this word representation of
the problem into a differential equation. My estimate is that it
would take at least three to ﬁve minutes to solve the equation and
compute the answer if one happens to remember certain useful
numerical values without having to look for and look up a math-
ematical table; otherwise, it would take longer. Of course, this
approach involves slow thinking, but it guarantees the correct
answer unless a careless mistake has been made.
The quickest way of arriving at the answer is probably sheer
guessing without thinking and without hesitation, as often seen in
a trivial pursuit television show when contestants rush to push
the bell. In a multiple-choice test problem with two furnished
answers, the probability of selecting the correct one by sheer
guessing is only 50%. Of course, sheer guessing is probably not
what Kahneman meant by fast thinking. He pointed out that most
people use simple logical guidelines called heuristics to help them
quickly reach a conclusion [Tversky and Kahneman, 1974]. In my
experience, most biomedical students answered 24 days reason-
ably fast by virtue of simple (linear) proportionality reasoning d
one half of the time for one half of the lake d since average
American biomedical students are usually not well versed in
nonlinear thinking. So far, Kahneman's interpretation seems
plausible. However, fast thinking does not always end up in wrong
answers.
For individuals who habitually practice visual thinking, the
problem can be visualized as a movie strip. A quick way to arrive at
the correct answer is to run this movie strip backwards, and to
witness that the lily pad coverage shrinks in half every 24 hours. In
this way, one quickly sees in one's own mental imagery, in a split
second, that one half of the lake is covered by lily pads on the
second day of backward running, i.e., the 47th day in forward
counting. This approach is sufﬁciently fast to rival sheer guessing.
This serves as a blatant example that contradicts Kahneman's
interpretation. But it is perhaps a rare exception, and it can be
dismissed as an “outlier” case in statistics jargon. However, this is
not the end of the story.
Those who are familiar with population growth may quickly
recognize that the problem is exactly analogous to the population
growth problem. By regurgitating the memorized formula, one can
just plug in appropriate numbers to quickly obtain the answer: If t
is the time it takes for the population to double, then the popula-
tion x at time t is given by: x ¼ x02t/t, where x0 is the population at
time t ¼ 0. Using this formula for the lily pad problem (t ¼ 1), we
obtain a pair of equations: 100% ¼ x0$248 and 50% ¼ x0$2t. By
eliminating x0 from the pair of equations, we obtain 248t ¼ 2 ¼ 21,
or simply 48  t ¼ 1, from which the answer can be found quickly.
For those who handle this problem routinely in an assembly-line
fashion and who still retain their high school algebra skills, the
answer can be found in no more than a few seconds. It may be even
faster for someone who can manipulate numbers in the head
without a pencil and paper or a hand-held calculator.
Alternatively, if a student commands the good habit of “double-
checking” a mathematical answer, the student may have quickly
found out, again in a split second, that the ﬁrst answer of 24 days
did not satisfy the problem whereas the second answer 47 days did.
However, if the problem had been framed as a short essay question
rather than a multiple-choice question, the same student could
have remained clueless for a long time.28 The upshot is that Kah-
neman's interpretation and the system of classiﬁcation which he
adopted provides a plausible explanation most of the time, but it
may not be the best explanation. In hindsight, it was a misleading
explanation of little practical value.
There is another serious pitfall in the conventional practice of
statistical methodology. We shall now examine a famous or noto-
rious problem, which made the work of Amos Tversky and Daniel
Kahneman widely known.
As a demonstration of common folk's poor statistical inferences,
Tversky and Kahneman dreamed up a problem, nicknamed the
Linda problem [Tversky and Kahneman, 1983]:
Linda is 31 years old, single, outspoken and very bright. She
majored in philosophy. As a student, she was deeply concerned
with issues of discrimination and social justice, and also
participated in anti-nuclear demonstrations.
Which is more probable?
1. Linda is a bank teller.
2. Linda is a bank teller and is active in the feminist movement.
The majority of those asked chose option 2. However option 1 is
more inclusive, and therefore, by common sense or by virtue of
simple set theory, option 1 is more likely than option 2 to be cor-
rect. Tversky and Kahneman claimed that the result was a
demonstration of common folk's tendency to commit conjunction
fallacy in their statistical inferences.
Kahneman [Kahneman, 2011] was quick to add that some peo-
ple who were aware of the correct answer and knew the reason of
selecting it still found option 2 attractive after the fact. He cited
Stephen Jay Gould who stated in a book review [Gould, 1988]:
I am particularly fond of this example [the Linda problem]
because I know that the [conjoint] statement is least probable,
yet a little homunculus in my head continues to jump up and
down, shouting at me d “but she can't just be a bank teller; read
the description.”
Tversky and Kahneman's interpretation was widely discussed.
Some investigators criticized the Linda problem on grounds such as
wording and framing [Gigerenzer, 1996; Hertwig and Gigerenzer,
1999;
Mellers
et
al.,
2001].
Subsequent
experiments
were
designed to eliminate these unwanted side effects, but the results
persisted [Moro, 2009; Tentori and Crupi, 2012]. Tversky and
Kahneman seemed to ﬁrmly believe the validity of their conclusion.
In one case, however, the rate of correct answers found in students
from a particular university differed from that of others, but they
did not bother to trace what made the difference.
In my opinion, all the above interpretations, including wording
and framing, revealed some truth but not the whole truth. I suspect
that sample heterogeneity might be partly responsible for the dif-
ference: Different individuals committed the same error for
different reasons. Gould was unlikely to commit conjunction fal-
lacy, and he did not belong to the group of average students whom
psychologists often choose to be their experimental samples. But
Gould still found the wrong answer attractive. Apparently, there
remained some loose ends which Tversky and Kahneman swept
under the carpet. Let us dig a bit deeper.
Let us examine Gould's remark, “but she can't just be a bank
teller; read the description” (emphasis now added). Here, the culprit
is the mindset or, more speciﬁcally in Gould's case, the “uncon-
scious” ﬂip-ﬂop of his mindset! Gould's remark reminds me of my
personal experience, which revealed just the opposite mindset.
It is well known that premedical students often concoct all kinds
of arguments d “challenges” in student jargon d just to gain an
extra grade point; sometimes “the challenge was absurd to the
28 This case is a damaging indictment of standardized testing.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
715

point of being clever,” according to one of my colleagues. One of our
physiology teachers apparently overlooked the possibility of
conjunction fallacy when he wrote a multiple-choice question with
two correct answers; one was more speciﬁc and it was designated
the correct answer, but the other was more inclusive, which was
too general d i.e., just common knowledge d to deserve credit. As
the course director and the default arbitrator of student-teacher
disputes, I ended up reluctantly granting a point to a student who
had successfully “challenged” the standard answer on the basis of
conjunction fallacy. But I gave this student another multiple-choice
question regarding Einstein, as a “surprise” parting shot:
1. Albert Einstein was a physicist.
2. Albert Einstein thought time is reversible and quantum me-
chanics does not tell the ultimate truth of the physical
universe.
3. Albert Einstein won a Nobel Prize for his theory of the pho-
toelectric effect.
4. Albert Einstein was a naturalized American citizen.
5. Albert Einstein was a man.
The student responded, “In the same spirit of my challenge, I
should pick ‘Albert Einstein was a man’.” I did not regret giving him
that extra point because he seemed to have understood my
message.
The above student's mindset was in the defensive or defendant
mode, which made him choose the least committal answer d
played it safe d so as to score as many grade points as possible, just
like how a defendant would respond to a cross-examination in
court by volunteering a minimum of crucial information. In
contrast, the word “just” in Gould's remark betrayed his mindset:
After he had chosen the correct answer so as to avoid committing
the conjunction fallacy, his mind spontaneously or unconsciously
ﬂipped back to the adventuresome or prosecutor mode which re-
sembles the mind habit of prosecutors in a legal court or that of
scientiﬁc investigators, and he simply could not resist the temp-
tation to attempt to extract as much information as possible from
that short description. I believe that I present a better, if not the
best, explanation here for the case of Gould's decision making. Of
course, other test takers might have committed the same error by
being poor thinkers who fell into the trap of conjunction fallacy. It
was therefore quite likely that Kahneman and Tversky mixed test
takers of different thinking styles and knowledge backgrounds in
the same test sample.
In my observation, students of statistics as well as investigators
who use statistical methodology are often reminded of the
importance of sample size in drawing statistical conclusions, but
rarely, if ever, they are reminded of the importance of avoiding
mixing apples and oranges in statistics. Perhaps it is a point too
obvious to mention; it borders on common sense. Perhaps the
difference between apples and oranges is too obvious to mention,
but reality is often brutal.
Now consider the relationship between motivation and crea-
tivity as an example to illustrate the pitfall of sample heterogeneity.
Before Deci and co-workers' work on intrinsic vs. extrinsic moti-
vation was known, psychologists knew only one kind of motivation
(see also Sec. 7.1). Now we know that there are two kinds of
motivation that happen to have opposite effects on creativity. One
can imagine how a mixed sample of experimental subjects with
intrinsic and extrinsic motivation affects the outcome of an
experiment designed to reveal the effect of motivation on crea-
tivity. If the mixed sample happens to contain equal numbers of
both types of motivation, the effect may be minimal or non-
existent. If the test subjects with intrinsic motivation outnumber
those with extrinsic motivation, the effect may be positive in light
of the work of Deci and coworkers. Reversing the ratio of the two
populations may render the effect negative. The trouble is that
unless one is constantly vigilant about the pitfall of sample het-
erogeneity, one is unlikely to design experiments to sort out sample
heterogeneity. But suspicion requires subjective judgment; an
objective mind is not supposed to favor one hypothesis over
another. Excessive emphasis on objectivity has thus made psy-
chological
research
particularly
vulnerable
to
sample
heterogeneity.
What if the sample is too pure?29 It may not be a blessing at all.
Here I am not too concerned about a sample's being too small in
size, because it is common knowledge to prevent it from
happening. I am concerned about the common practice in psy-
chology of utilizing college students, especially psychology stu-
dents, as testing samples. The obvious pitfall is inability to detect
potential sample heterogeneity in the general population. If one
happens to collect a pure sample from an inherently heterogeneous
population, the sample is non-representative no matter how big
the size is. Readers might be thinking about the difference between
Stanford engineering students and community college students in
the Midwest. I am actually more concerned about subtle and,
therefore, unsuspected differences, e.g., the difference in learning
styles between premedical students and master degree nursing
students, whom I taught year after year over my entire teaching
career. Let me not get off on a tangent and, instead, present another
cautionary tale.
The difference between hieroglyphic words, which consist of
picture elements, and Western words, which consist of alphabetic
letters, has been a topic that fascinates cognitive scientists from the
point of view of cerebral lateralization. Common sense suggests
that one reads alphabetic words by means of sequential processing
(left hemisphere laterality) whereas one reads hieroglyphic words
by means of parallel processing (right hemisphere laterality). But
the actual situation appears to be more complex. William Wang's
group found that additional factors, such as long vs. short alpha-
betic words, and single words vs. phrases in hieroglyphic words,
also affect laterality [Hardyck et al., 1977; Tzeng et al., 1979].
However, the initial ﬁndings could not be replicated by other
groups [Tan, 2000; Fang, 2003]. Naturally, the episode raised the
specter of an alleged fraud. To date, the controversy has remained
unsettled. But is alleged fraud the best explanation of
the
discrepancy?
Let me propose an alternative explanation. This is how phe-
nomenology can contribute. My introspection revealed that I read
alphabetic words by means of parallel processing, because I treat an
entire alphabetic word as a picture pattern. I probably acquired this
habit as a youngster learning to play violin as a hobby. I vividly
remember my violin teacher's instruction: In order to improve the
speed of sight-reading (in music), one must read musical notes as
clusters rather than individual notes, especially in rapid music
passages. This is a single person's introspection. There is good
reason to believe that this observation is by no means universal. I
recall seeing someone reading English words by reciting spelling
letter by letter, resulting in visibly slow comprehension. On the
other hand, I also have good reason to believe that I am not the only
one that reads alphabetic letters in clusters. I also have good reason
to suspect that someone else might be able to read an entire sen-
tence as a single cluster. The upshot is that there may be individual
differences in laterality in regard to reading alphabetic words. The
corollary is that the discrepancy between Wang's group, on the one
29 The concern of sample heterogeneity does not apply to measurements of pa-
rameters or qualities which are inherently heterogeneous, such as IQ and person-
ality; it is difﬁcult, if not impossible, to obtain a sample that is too pure.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
716

hand, and Fang's and Tan's groups, on the other hand, might be due
to sample heterogeneity in the general population and each group
happened to have utilized a pure sample of different kinds, e.g.,
American students vs. Asian students. The way to resolve the
controversy is that each group uses the other's sample and repeats
the entire project all over again d what Mellers, Hertwig and
Kahneman [2001] called “adversarial collaboration.” Passage of
time may make it impractical to carry out the desired collaboration,
but others who are interested in the topic may repeat the experi-
ments with sample heterogeneity in mind.
The above scenario has a mixture of realism and speculation. But
the speculations are all falsiﬁable by means of experimental testing.
Actually, there is a precedent of sample heterogeneity in the psy-
chology literature. Again, common sense suggests that music
perception, a holistic cognitive function, exhibits right laterality.
Actual experiments revealed that there is a strategic difference be-
tween trained musicians and nonmusicians. Nonmusicians tend to
perceive a music passage as whole and show a left-ear preference
(right laterality). In contrast, trained musicians tend to perceive a
music element as a combination of previously known “subunits”
(intervals, chords, or other distinct patterns such as ostinato, ar-
peggio, etc.) presumably because prior training in music theories
have converted the perception into a routine task, thus exhibiting a
right-ear preference (left laterality) [Bever and Chiarello, 1974]. It is
not too farfetched to infer that reading alphabetic words may also
have individual variation due to habituation or training.
Kahneman and Tversky were not the ﬁrst nor the last in-
vestigators to have committed the kinds of errors discussed above,
but they are probably the most famous ones to do so. It is my hope
that their example serves effectively as a cautionary tale. I certainly
do not wish to imply that the work of Tversky and Kahneman is
worthless. It is quite the contrary. The host of test questions which
Tversky and Kahneman dreamed up during their long collaboration
are treasures for decision science research. The problem was that,
as the original authors of these questions, they had the best an-
swers as well as best interpretations in mind ahead of time. The
practice inevitably created blind spots that they could hardly have
detected by themselves. Again, a preconceived and half-baked hy-
pothesis let them design ﬂawed experiments to collect heteroge-
neous data, which happened to support their half-way decent but
plausible hypothesis, much like a self-fulﬁlling prophecy. It was
true that their critics came up with all kinds of possible alternative
interpretations, but, to the best of my knowledge, the critics all
failed to detect or suggest possible sample heterogeneity. Further-
more, because of the possible sample heterogeneity, experiments
designed to falsify Tversky and Kahneman's hypothesis yielded
inconclusive results: the conjunction fallacy effect persisted. As a
consequence, those failed challenger experiments generated, by
virtue of repeating failure to debunk, an unintended consequence
of strengthening Tversky and Kahneman's hypothesis in appear-
ance, if not in substance.
Readers who are familiar with Roger Sperry's split-brain
research may recognize that the two modes of thinking, visual
and verbal, correspond to the function of the right cerebral hemi-
sphere and the left cerebral hemisphere, respectively. In fact, the
split-brain research is well known beyond the ﬁeld of neuroscience.
Thus, an obvious question to ask is: Why did Tversky and Kahne-
man adopt a primitive and less-than-satisfactory system of classi-
ﬁcation
of
thinking
styles?
Why
are
there
no
signiﬁcant
psychological theories of creativity based on the theory of cerebral
lateralization, as Sperry's discovery is often referred to? The answer
to these questions must then be linked to a larger issue of premature
falsiﬁcation in science.
According to Popper's falsiﬁability argument, a scientiﬁc theory
can be falsiﬁed (disproved) by a single counter-example or a single
piece of scientiﬁc fact that contradicts the theory's predictions. If so,
then many now-widely-accepted theories would have been elimi-
nated in their formative years when a new theory was being
actively debugged and revised, as some investigators once com-
plained. What investigators often overlook is that the scientiﬁc fact
that has been used to falsify a theory is itself a scientiﬁc statement,
which must also be subjected to falsiﬁcation tests for validation. It
is therefore possible to prematurely falsify a burgeoning theory in
its early stage of development by virtue of a ﬂawed piece of sci-
entiﬁc evidence. Premature falsiﬁcation thus poses a serious
problem of impeding scientiﬁc progress. The following account
serves as another cautionary tale.
In the wake of Sperry's experiment, the ﬁnding generated
tremendous enthusiasm and, in particular, it spurred the so-called
“right brain movement” in the education community because of
the belief that creative individuals preferred the use of the right
brain. Soon psychologists found that this belief was not true
because they found that creative people did not utilize their right
brain preferentially. The “right brain movement” was crushed to a
screeching halt.
In hindsight, the falsiﬁcation was premature because of a naïve
hypothesis based on simple-minded logical reasoning. It went like
this. If creative individuals prefer right-brain thinking, the two
hemispheres ought to exhibit objectively observable differences.
Superﬁcially, this reasoning is not unreasonable in view of present
understanding of neural plasticity. A brief overview of what it actu-
ally takes to solve aproblem, novelorroutine,reveals the complexity:
The underlying mental processes involve several hierarchical levels
of thinking. Instead of the vague term “thinking,” let us switch to the
language of artiﬁcial intelligence. Simon treated problem solving as
nothing but pattern recognition or just recognition [Simon, 1992].
There are two kinds of pattern recognition. Digital pattern recogni-
tion is related to the following: sequential processing, logical
reasoning, verbal thinking, and the left-brain function. Analog
pattern recognition is related to the following: parallel processing,
visual thinking, and the right-brain function. At the ﬁrst level of
problem solving, one invokes both digital and analog pattern
recognition to recognize words in the problem statement. At the
second level, one invokes both types of pattern recognition to un-
derstand the syntax (sequential processing) and semantics (parallel
processing). At the third level, one invokes either verbal thinking
(sequential processing) and/or visual thinking (parallel processing)
to search for plausible solutions. Finally, one invokes logical
reasoning to verify selected solutions, one by one, and then invokes
visual thinking to clean up remaining loose ends (see also, Sec. 7.3).
Obviously, the problem solver switches between the functions
of the two cerebral hemispheres, back and forth, constantly. Both
sides of the brain are utilized for information processing but in
different ways and at different hierarchies of problem solving. Even
if one utilizes the right brain preferentially in search of a solution,
one still utilizes the left brain for the purpose of veriﬁcation. The
notion that only one side of the brain is utilized in problem solving
so exclusively as to show anatomical or functional asymmetry is
naïve. My own investigation indicates that the major difference
between creative and non-creative individuals is limited to events
at the third level [Hong, 2013a; 2013b]. Furthermore, this difference
may not be present all the time. The possible difference may not be
reﬂected in morphological/structural changes but rather in tem-
porary dynamic changes. If the changes turn out to be dynamical,
the experimental methods of detection may not have sufﬁcient
time resolution to detect the difference. Even if a particular method
has the required time-resolving power, it would be extremely
technically challenging to differentiate separate information pro-
cessing taking place at three different levels, as mentioned above.
Such a technique is presently unavailable.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
717

Let us examine a latest attempt which was announced by a press
conference with the subject heading of “Researchers Debunk Myth
of ‘Right-brain’ and ‘Left-brain’ Personality Traits.” Jeffrey Anderson
and his colleagues [Nielsen et al., 2013] studied brain scans of 1,011
individuals with ages between 7 and 29, taken with a functional
connectivity MRI (magnetic resonance imaging) while their brains
were in a resting state for 5 to 10 min. They divided up the brain
into 7,000 regions, to see if any brain connections between regions
were left-lateralized or right-lateralized, i.e., to see if either side
was preferred, so to speak. They found no evidence of lateralization.
Without my additional explanation, I believe that most readers will
agree that the authors formulated a dubious hypothesis and
designed an erroneous experimental regime (resting instead of
solving problems), and utilized the wrong kind of equipment to
collect irrelevant data, which happened to conﬁrm their hypothe-
sis, much like a self-fulﬁlling prophecy. The ultimate verdict of their
investigation is correct, but it is also a no-brainer: both brains are
involved in creativity. Nevertheless, they concluded that the myth
of right-brain and left-brain personality traits is oversimpliﬁed.
Oversimpliﬁed? It is the reductionist science that disregards
phenomenology that is oversimpliﬁed. Without the help of a non-
existing objective method to eavesdrop on the content of infor-
mation crisscrossing the two cerebral hemispheres, how could an
investigator detect any preference of creative individuals, especially
when the individuals were instructed not to actively think about
some hard problems? There was another mistake at the common
sense level: absence of evidence is not evidence of absence, as
Altman and Bland aptly put it [Altman and Bland, 1995].
In conclusion, objective statistical methodology does not make a
ﬂawed inference scientiﬁc, nor can statistical methodology prescribe
a procedure to generate sound inferences. Excessive emphasis on
objectivity actually has the opposite effect of letting subjectivity
intrude imperceptibly so as to elude subsequent scrutiny.
7.3. Inference to the best explanation: the role of visual thinking
If objectivity is not so sacred after all, it loses its long-held status
of the sine qua non hallmark of scientiﬁc enterprise. If it is not ob-
jectivity, what other attribute makes science scientiﬁc? In order to
ﬁnd a more reliable replacement or replacements for objectivity,
one needs to go back to science history.
One of the reasons for invoking objectivity was to prevent au-
thority from making arbitrary or self-serving decisions about truth
and shoving it down our collective reluctant throats without even
bothering to tell us why. By virtue of objectivity, the decision about
truth was reassigned to the consenting public, at least to the con-
senting scientiﬁcally literate public, thus forcing authority to adopt
the practice of rational debate as an alternative route to win accep-
tance instead of threatening the dissidents with the burning stake to
enforce acceptance. But the effectiveness of rational debate has been
eroded by compartmentalization of specialties and fragmentation of
knowledge, which are unintended consequences of information ex-
plosion, which, in turn, is an unintended consequence of democra-
tization of scientiﬁc enterprise; the number of people who engage in
scientiﬁc investigations has soared spectacularlysince Newton's days.
The number alone is a positive factor for uncovering or discovering
any persistent inconsistencies, but this factor is offset by compart-
mentalization of specialties. Creativity research is a case in point.
Creativity is a topic shared among research in psychology, deci-
sion science and management science.30 Kahneman's book already
gave us a glimpse into the System 1 vs. System 2-based in-
terpretations of critical thinking in decision science. Let us compare
it with the mainstream view in management science. Together with
Dyer and Gregersen, innovation guru Clayton Christensen co-
authored a popular book The Innovator's DNA [Dyer et al., 2011].
They interviewed about thirty innovative entrepreneurs and a
similar number of senior executives in larger organizations. The
collective insights were crystallized into ﬁve innovative skills:
associating, questioning, observing, networking and experimenting.
While their conclusions appear to be reasonable, the authors'
methodology left a gaping hole. Had they also interviewed business
people with lesser talents or even mediocre ones, they might have
come up with similar conclusions. What they did not reveal was how
innovative business people executed these ﬁve skills differently
than mediocre ones. Needless to say, what was missing in their book
is the analysis of the mental habits of these business gurus in
invoking visual thinking vs. verbal thinking.
Psychologists' approach to creativity research is yet another
story. Needless to say, no resemblance can be detected in
comparing psychologists' theorization with what is being preached
in decision science and in management science. Readers interested
in how they approached the problem are recommended to read my
critiques that have appeared in two recent review articles [Hong,
2013a; Hong, 2013b].
The total lack of cross-fertilization between the above three
disciplines is mind-boggling; it appears that conclusions about the
same topic reached in three separate disciplines are not universal at
all, just like currencies. (Money may be universal but currencies are
not.) Yet the lack of objections from peers suggests a superﬁcial
appearance of objectivity without regard to universality. It appears
to me that what science aspires to may not be objectivity itself but
rather universality. The consideration of universality of natural laws
was one of the factors leading to the birth of special relativity;
universality requires natural laws to remain the same in all inertial
reference systems. If the requirement of objectivity does not
guarantee universality, what else can ﬁll the shoes left vacant by
objectivity?
The objectivity-based scientiﬁc enterprise has been around for
so long, and, in all good conscience, no one can deny the spec-
tacular success of reductionist sciences, including the spin-offs in
high-tech engineering. Now on the verge of breaking into more
complex aspects of science, bankruptcy suddenly looms so large
that reorganization seems to be the last but necessary resort. In
other words, when science of complex human experience en-
counters a “bottleneck,” a “bypass” surgery becomes a serious
alternative and may be the only hope for bringing it back to life.
Every possible criterion and every possible approach must be put
on the table for evaluation. Even the most basic and cherished
tenet should not escape scrutiny. This brings up the topic of
whether a scientiﬁc investigation begins with proposing hypoth-
eses, and whether logical reasoning is the best avenue leading to
discovery. Some of these topics have been examined in my pre-
vious article [Hong, 2013a]. We shall continue the exploration
now.
My analysis of psychological research revealed the predicament
confronting scientiﬁc investigations of complex phenomena: the
complexity of multiple factors. Forward-going syllogisms such as
logical deduction can only deal with one factor at a time, thus
running the risk of missing more important and more pertinent
factors, not to mention failure in deciphering the intricacy of
mutually dependent factors. Philosopher Charles Sanders Peirce
obviously recognized this pitfall. He initially proposed a backward-
going type of syllogism, known as abduction, as a remedy [Peirce,
1931e1958]. He claimed that abduction is the only route towards
inference to the best explanation. Inference to the best explanation is
30 Decision science focuses on critical thinking whereas management science is
preoccupied with innovation. Innovation is inseparable from creativity whereas
critical thinking and creativity are two sides of the same coin.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
718

obviously what we need in dealing with science of complex phe-
nomena, and it is particularly relevant to phenomenology.31 Peirce
obviously recognized the importance of Popper's falsiﬁability
argument and was one of the earliest voices in opposition to the
traditional claim of logical deduction as the route towards scientiﬁc
discovery. However, the concept of abduction underwent a
tortuous route of evolution. In response to criticism, Peirce
continued to revise the concept throughout his remaining career.
No matter how he revised the concept, he did not succeed in
silencing the critics, but the revisions actually bred more questions.
He eventually introduced an element of subjectivity. But syllogism
and subjectivity are not supposed to mix.
In hindsight, the culprit was the obsession with syllogism. Once
one decides to let it go, new possibilities open up. Syllogism,
whether it is deductive or abductive, always involves a step-by-step
algorithm-like procedure. As explained in Sec. 6.2.3, reasoning by
means of syllogism is ineffective and impractical except for the
simple cases of reasoning that involves a single step of deduction or
abduction. The requirement of multi-step syllogism for novel
problems with modest to advanced complexity makes deduction or
abduction impractical because of the enormous number of per-
mutations of logical steps. Thus, if one is willing to abandon syl-
logism, one quickly realizes that the concept of abduction as
delineated by Peirce's modiﬁcations is nothing but visual thinking
(see [Hong, 2013b] for detail).
I have explained, in Sec. 6.2.3, why visual thinking constitutes an
approach for heuristic searching for a relevant solution or solutions.
Now, when more than one plausible explanation is available, how
does one judge which one is the best explanation? What are the
criteria for a good explanation? A short list of qualities easily comes
to mind based on daily practices of scientiﬁc investigations: con-
sistency, rationality, coherence, and parsimony. What parsimony
implies is that a good theory presupposes the least to explain the
most. In other words, generality is an important attribute of a
theory. Parsimony denotes the same aspiration as the celebrated
Ockham's razor, which essentially proclaims, in plain English, that
unless there is overriding imperative, the simpler a theory is the
better. Einstein was quick to add, “Everything must be made as
simple as possible. But not simpler.” A theory can be too general to
be speciﬁc. The Eastern philosophy of advocating harmony be-
tween humans and Nature is a case in point. As an attitude of living
a life and of how we treat our environment, it is a great philo-
sophical idea, but, as a scientiﬁc imperative, it explains everything
but predicts almost nothing. This brings up the importance of
predictive power as a major virtue of a scientiﬁc theory. But there is
a problem or, rather, two problems. First, excessive emphasis on
predictability favors reductionist sciences d because of the relative
ease of mathematization d at the expense of sciences of complex
phenomena. Second, as a means for enhancing predictability it is
possible to fudge all the way through by invoking adjustable pa-
rameters d a.k.a. fudge factors d so as to artiﬁcially enhance the
predictive power of an otherwise mediocre or misleading mathe-
matical theory (e.g., van't Hoff's law, Ptolemy's epicycle theory). It
boils down to the reality: judging best explanations is as difﬁcult as
judging beauty. No single or a few itemizable criteria sufﬁce; only a
holistic judgment serves the purpose well.
This line of thinking reminds me of a ﬁctional television char-
acter of the 1970s and 1980s: Lieutenant Columbo, the absent-
minded detective of the L.A.P.D. Lt. Columbo often confessed to
having a terrible habit d an obsessive urge to “tie up loose ends.”
Columbo's allusion to loose ends reﬂected his uneasiness about
subtle incongruities detected only by means of visual thinking; he
felt that little insigniﬁcant details did not quite add up. His relent-
less pursuit of the loose ends and his obsessive quest for the best
explanation of all available criminal evidence often overturned the
verdict and exonerated a wrongly convicted suspect.
Kahneman's book mentioned the halo effect which affects
judgment [Kahneman, 2011]. It means that we all have a tendency
to be less critical in judging the work or opinion of someone with
reputation than in judging that of an average investigator. In this
case, the habit of tying up loose ends appears to be an effective
antidote to guard against uncritical acceptance of superﬁcially valid
but ultimately untenable scientiﬁc inferences. However, this critical
attitude sometimes backﬁres and it can, in principle, lead to un-
critical rejection of valid ideas because what we cannot understand
at ﬁrst glance may not necessarily be illogical or untenable. This
error in judgment is particularly common when experts judge non-
experts. It was no wonder that Einstein had the dubious distinction
of being openly ridiculed, for his introspection about visual
thinking, by an expert in psychology [Harris, 1988]. Here I shall
analyze a more recent case.
Frances Rauscher and coworkers reported an observation of
what is now known as the Mozart Effect [Rauscher et al.,1993,1995].
By exposing thirty-six college students to ten minutes of listening to
Mozart's Sonata for Two Pianos in D major (K488), their perfor-
mance on IQ (intelligence quotient) tests for spatial-temporal tasks
was enhanced. Needless to say, the report ignited a ﬁrestorm of
controversy. Daniel Levitin, a neuroscientist by training and a pro-
fessional musician, dismissed the report from an authoritative po-
sition [Letivin, 2006]. He further said, “Another problem with the
study was that there was no plausible mechanism proposed by
which this might work d how could music listening increase spatial
performance?” By simple logic, Levitin could not understand how
music, an art of time progression of tones, also affects cognitive
spatial performance rather than just temporal performance only.
In his letter to his admirer Baron von P., Mozart claimed to be able
to hold an entire music score in his memory so that “[he could]
survey it, like a ﬁne picture or a beautiful statue, at a glance”
([Holmes,1991], pp. 266270). Mozart further wrote, “Nor do I hear
in my imagination the parts successively, but I hear them, as it were,
all at once (gleich alles zusammen).” In computer science jargon,
Mozart pointed out that he was actually performing parallel pro-
cessing rather than sequential processing when he was composing all
in his head. By the same token, listeners of a piece of orchestral
music must perform parallel processing for the sake of enhanced
appreciation, which is exactly a cognitive spatial task, in addition to
a concurrent temporal task. The upshot is that Levitin had a serious
blind spot because he uncritically treated whatever he could not
comprehend as being wrong or untenable. Exposing Levitin's
blind spot does not prove that the Mozart Effect is valid, but the
work deserves additional follow-ups. The remedy to uncritical
rejection of others' ideas is to take thework of opponents seriously, if
not respectfully, and to make an effort to think from the opponent's
point of view so as to expose one's own blind spots. Before one
dismisses an opponent's work, it is a good practice to understand
why the opponent is wrong. This practice is not foolproof, but it
minimizes the likelihood of making a fool of oneself, at least.
In pursuit of a theory for complex phenomena of subjective
experience, we do not merely ask for self-consistency between the
hypothesis and the objectively collected data (local consistency).
We must also look for overall consistency of all concerned matters,
including investigators' attitudes and perhaps also unspoken body
language (global consistency). The best we can do is to be
constantly vigilant in search of subtle loose ends because they are
31 In view of Popper's falsiﬁability argument, it is impossible to ascertain which
one is the best inference. One can only strive for better and better explanations d
successive approximation in mathematics jargon. That is why science constantly
renews itself, as if it could not quite make up its mind.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
719

often subtle clues indicative of a lack of an overall inconsistency:
somehow things just do not quite add up. I suspect that one of the
reasons why Western science cherishes objectivity is to achieve
overall consistency. This is because subjectivity makes a scientist
vulnerable to blind spots, and blind spots are fertile ground for
breeding subtle inconsistencies. Tying up all detectable loose ends
seems to be the quickest way to make inferences to better and
better explanations, thus approaching the best explanation by a
recursive process of tying up remaining loose ends.32
Speaking about loose ends and overall consistency, free will
deniers have often betrayed their own deep-seated belief in the
existence of free will. Einstein, one of the best-known free will
deniers, did not believe that God throws dice, but he was quick to
add that this did not mean that he would not object to having lunch
with a murderer. If he consistently asserted that the world is strictly
deterministic, what made him think that he had the prerogative to
defy his destiny so as to avoid having lunch with a murderer?
Another eminent free will denier, Schr€odinger, did not fare any
better: He wrote a two-page Nature article to forcefully convince
the readers that free will is an illusion [Schr€odinger, 1936]. James
Rochford33 recognized this kind of behavioral inconsistency and he
wrote, “If freewill doesn't exist, then we should never try to argue
someone into determinism.” Rochford further wrote, “If freewill
doesn't exist, then we could never know if our knowledge of any-
thing is true.” After all, in a deterministic world, it is futile to wield
one's inﬂuence to divert the course of an event that is determined
to take place in a predetermined way anyway. It is also futile to
claim that one can resist brainwashing and make one's own judg-
ment, after rigorous and careful deliberation, to differentiate be-
tween truth and fallacy, and the perennial debate about whether
free will actually exists or not is nothing but a cruel practical joke.
This kind of overall inconsistency d inconsistency of one's
behavior with one's scientiﬁc conviction d is quite common among
contemporary free will deniers in the 21st century.34 Being a
contemporary free will denier, Wegner [Wegner, 2002] neverthe-
less took responsibility and morality seriously. Did he forget that,
without free will, morality is meaningless, and it makes no sense to
punish criminals for their crimes? Or, perhaps he was struggling
with two selves: The scientiﬁc self denied the existence of free will,
but the emotional self had the opposite belief. Wegner did confess,
“It's an illusion, but it's a very persistent illusion; it keeps coming
back … Even though you know it's a trick, you get fooled every
time. The feelings just don't go away.”35
Michael Gazzaniga is another contemporary free will denier. In
his book which propounded the non-existence of free will, he also
suggested that humans behave better if they believe in free will. He
cited an experiment of Vohs and Schooler ([Gazzaniga, 2011],
Chapter 4): Students who read about determinism cheated,
whereas those who read an uplifting book did not. He seemed to
cherish these observations, but his behavior puzzled me more than
the enigma of free will does.
Richard Dawkins, who is a contemporary hard determinist and
free will denier, had an enlightened second thought about pun-
ishing people who engaged in antisocial behavior. He wrote,
“Retribution as a moral principle is incompatible with a scientiﬁc
view of human behaviour.” He viewed these criminals as deter-
ministic machines like cars or man-made computers. He argued,
“When a computer malfunctions, we do not punish it. We track
down the problem and ﬁx it, usually by replacing a damaged
component, either in hardware or software.”36 It appeared that
Dawkins tacitly presumed that he alone had free will to decide to ﬁx
the malfunctioning computer by himself or to decide to have
someone else ﬁx it rather than just beat it up and totally demolish it
d whereas all criminals were mere machines or zombies devoid of
free will and they were passively waiting to be ﬁxed, just like the
malfunctioning computer. The absurdity of Dawkins' argument is
apparent. Perhaps Dawkins had been unconsciously engaged in a
word game of self-deception. Changing the word “punish” to “ﬁx”
d or even to a harmless word “neutralize” as some professional
killers were apt to put it d was merely a futile act of euphemism;
someone else with free will still had to initiate it. Besides, someone
else with free will to think differently may detect the trick beneath
the euphemism and manage to neutralize the cleverly crafted
deception. The substitution word game did not change the harsh
reality behind the deceptive façade.
Thus, in light of overall consistency, free will advocates appear
to fare better than free will deniers. By the same token, phenom-
enology can be made more scientiﬁc than behavioral science by
means of striving for universality and overall consistency.
Let us summarize what must be done to ensure scientiﬁc rigor in
phenomenological research.
 Instead of objectivity, we now pursue universality as the main
virtue of science, without abandoning existing criteria already in
practice, such as rationality, coherence, consistency, parsimony
and predictive power.
 In addition to local consistency commonly enforced within the
conﬁne of a speciﬁc investigation, global consistency with
existing knowledge and other pertinent matters must be
maintained.
 Since objective approaches alone do not guarantee scientiﬁc
rigor, we must invoke, in addition, visual thinking so as to
achieve inference to the best explanation. Of course, logical
reasoning is still required to safeguard overall consistency.
 In spite of all the above efforts, some loose ends may still
remain. We must then invoke visual thinking to uncover such
loose ends, and invoke logical reasoning to make sure all loose
ends have been tied up.
 Cross fertilization with other related disciplines and thinking
from the opponents' point of view are two of the quickest ways
of uncovering one's own blind spots.
8. Concluding summary
At the dawn of modern science, Rene Descartes' dualism
excluded the spiritual aspect of human experience from the legit-
imate topics of scientiﬁc investigations. Nowadays, few, if any,
practicing scientists or philosophers take dualism seriously, and
many of them consider dualism an erroneous dichotomy made by
32 One of the ultimate ironies is that “objectivity” becomes a major “loose end” in
Western science after it has delivered long and distinguished service to Western
science, including helping science rise from the ashes of the Dark Ages.
33 See an article “The Mind and the Brain: Is Freewill an Illusion?” by James M.
Rochford, which appeared in Evidence Unseen. http://www.evidenceunseen.com/
theology/anthropology/the-mind-and-the-brain-is-freewill-an-illusion/.
34 The lack of overall consistency is also symptomatic of antirealism and solipsism,
which both subscribe to absolute objectivity in reasoning. Bertrand Russell once
told the following hilarious story about a solipsist, whose remark happened to
contradict solipsism d the philosophical view that claims that the self can know
nothing but its own consciousness ([Russell, 1948], p. 180; [Sokal and Bricmont,
1998], p. 54): “I once received a letter from an eminent logician, Mrs. Christine
Ladd Franklin, saying that she was a solipsist, and was surprised that there were no
others.”
35 Quoted by Dennis Overbye in an article “Free Will: Now You Have it, Now You
Don't.” The New York Times, January 2, 2007.
36 See an article “Let's all stop beating Basil's car” by Richard Dawkins, which
appeared in Edge The World Question Center at http://edge.org/q2006/q06_9.html.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
720

Descartes. In hindsight, I have a different opinion. I regard Des-
cartes' dichotomy as a wise decision which has beneﬁted science
more than harmed it. In a previous article, I took the stance that
scientiﬁc theories are humans' mental constructs created in an
effort to comprehend how Nature works without resort to mysti-
cism [Hong, 2013a]. It is tantamount to recognizing Nature's
“behavioral” patterns. In pattern recognition, one naturally begins
with easy-to-recognize patterns ﬁrst. Dualism lets us focus on the
easy parts of science ﬁrst, and then work our way up the ladder of
complexity.
Reductionism
was
a
natural
and
well-justiﬁed
approach at least at the beginning. Descartes' dualism also made
peace with the then-Church Establishment so that scientists, with a
few exceptions, were not particularly targeted for persecution as
heretics.
All went well with the reductionist approach even in biological
research as long as the mind was excluded. Naturally, psychological
research began with the psychometrically measurable part d
behaviorism. The limitations of the reductionist approach began to
surface in the cognitive part of psychological research (cognitivism)
primarily owing to insistence upon objectivity. Cognitivism was
invigorated by its alliance with neuroscience (cognitive science),
but the shadow cast by objectivity did not vanish. Phenomenology
thus came into being as a consequence of dissatisfaction with the
reductionist approach to the investigation of the mind.
Initially, the founders of phenomenology, Heidegger and Hus-
serl, had no intention to make the discipline “scientiﬁc”; they just
wanted to make phenomenological analysis of the mind rigorous.
But their followers could never quite shed the stigma of subjectivity
in spite of the attempt to deploy the third-person perspective
[Dennett, 2001]. The astonishing progress of neuroscience also
lured or forced Heidegger and Husserl's followers to change course
and to consider naturalization of phenomenology as a desirable
research approach.
In attempting to contribute to the effort of naturalization of
phenomenology, I choose to examine the topic of free will. Free will
and creativity are two of the most enigmatic manifestations of
human consciousness. Having recently tackled the problem of
creativity, I ﬁnd that the two topics share the common issue of
origination. I also ﬁnd that the insights gained through inquiry into
creativity are potentially helpful in naturalizing phenomenology in
general. So far, free will advocates have encountered two of the
most formidable roadblocks posited by reductionist sciences d
physics and neuroscience. Here, the main conclusions are listed
below.
Physical determinism was and still is the most intractable
roadblock set up against the existence of free will. Although
determinism was often attributed to Newtonian mechanics, it was
Laplace who enunciated the doctrine of absolute physical deter-
minism. Therefore, the ﬁrst step towards reconciling free will
phenomenology with reductionist science is to re-examine the age-
old conﬂict between free will and absolute physical determinism.
Although philosophers who were more or less contemporaries of
Newton, such as Kant and Hume, deployed interpretive tricks to
become compatibilists, the conﬂict is real and irrevocable unless
one is willing to go so far as accepting vitalism or mysticism.
Superﬁcially, Laplace's doctrine appears to be self-evident and
capable of refuting any suggestion or evidence of the existence of
true noise. However, it is not as ironclad as we have been led to
believe. It is true that it is impossible to refute Laplace's doctrine.
But if Laplace's doctrine is not falsiﬁable, it is not a scientiﬁc
statement. Conversely, it is impossible to prove it, either. Therefore,
it is outside of the realm of scientiﬁc investigation. The only rational
verdict is that Laplace's doctrine belongs to the realm of episte-
mology. It was an epistemological choice made by Laplace. It was
misconstrued as scientiﬁc truth owing to a spectacular lack of
objections and a long list of success in reductionist sciences.
If Laplace's doctrine is no longer as sacred as it once seemed,
then everything related to determinism must be put on the table
for re-evaluation. In our revisiting of the topic, we have paid
attention to two pitfalls. First, I resort to mathematical concepts to
achieve sufﬁcient precision, and, second, I stay alert to combat the
deceptive power of natural language, such as the subtle difference
between the indeﬁnite article “a” and the deﬁnite article “the” as
well as assorted euphemisms that have an inherent tendency to
mislead scientiﬁc investigation.
It was found that one of the major culprits was the false di-
chotomy of determinism vs. indeterminism; indeterminism was
often misinterpreted as complete randomness. Therefore, the term
quasi-determinism is introduced to deal with the existence of a
gray scale between total determination and total randomness.
The main concept subsequently to be put on the table for
scrutiny is the concept of microscopic reversibility, which is math-
ematically equivalent to time-reversal symmetry implied by New-
tonian mechanics. We have found the time-honored and textbook-
endorsed argument of the so-called law of large numbers, which
was traditionally invoked to explain the compatibility of micro-
scopic reversibility and macroscopic irreversibility, is logically
ﬂawed and therefore untenable. The two historical paradoxes,
Loschmidt's velocity reversal paradox and Poincare's recurrence
paradox, are actually valid in spite of what physics textbooks have
led us to believe. These newly revealed ﬁndings indicate that
microscopic
reversibility
is
irreconcilably
incompatible
with
macroscopic irreversibility.
However,
the
most
direct
argument
against
the
alleged
compatibility can be formulated by considering the probability of
occurrence of a discrete event, with a continuous distribution,
before and after its occurrence in the theoretical framework of
Newtonian mechanics and Boltzmann's theory of statistical me-
chanics. Newtonian mechanics assigns the same absolute certainty
to the probabilities of both before and after the occurrence of a
particular discrete event; time-reversal symmetry is preserved. In
contrast, Boltzmann's theory assigns practically zero to the “before”
probability, but it assigns absolute certainty to the probablity of
occurrence of the discrete event after it actually happens; time-
reversal symmetry is broken. This conclusion happens to be the
elusive time-symmetry breaking that Prigogine had been arguing
about. This conclusion is also nearly identical to what Popper had in
mind: the determined past and the open future ([Popper, 1982], p.
48). The same conclusion can be generalized to any version of
deterministic physical causal law of the one-to-one type temporal
mapping. Of course, statistical mechanics seldom deals with
discrete events, much less the notion of absolute certainty. Never-
theless, time-reversal symmetry is routinely broken, and it is the
hallmark of statistical mechanics.
Superﬁcially, the new verdict judges that Boltzmann lost the
debate to his detractors. However, this new revelation would not
exactly be a tragedy for Boltzmann; it could be a blessing in
disguise, from the perspective of the 21st century physics when
Newtonian mechanics no longer commands gospel-like invinci-
bility. Instead of sending it down the drain irreversibly and beyond
rescue, the new verdict exalts Boltmann's statistical mechanics to
the new height of a genuine paradigm shift of great historical sig-
niﬁcance and of great real-life relevance. In other words, in
apparent failure, Boltzmann actually became even greater than he
himself had envisioned. Alas, the thought of surviving for posterity
probably never crossed Boltzmann's mind. In fact, near the end of
Boltzmann's life, the innovation of applying statistics to physics,
which he pioneered, had begun to catch ﬁre. Even Einstein applied
statistics to deduce what is known as B€ose-Einstein statistics. Ac-
cording to Popper [Popper, 1992], Boltzmann had no awareness of
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
721

this new trend. I even have good reason to suspect that Boltzmann
might have stumbled upon the simple and transparent proof,
which I just recapitulated, demonstrating unequivocally the in-
compatibility between his theory and Newtonian mechanics. In
despair and in dejection, Boltmann committed suicide in 1906. It is
an everlasting frustration for many of us, in hindsight, that Boltz-
man could not be informed of all subsequent scientiﬁc and philo-
sophical developments because the latter came more than a
hundred years too late to rescue him from his tragic end, precisely
because of Boltzmann's enduring legacy: Time is an arrow. What an
irony in real life!
In re-analyzing the concept of microscopic reversibility, an un-
expected spin-off was found: Deterministic chaos is not the only
reason why long-term weather forecasting is difﬁcult. On top of
that, the mechanical law of motion is quasi-deterministic, thus
contributing to quasi-deterministic chaos. This revelation also ex-
plains clearly why long-term predictions of solar eclipses and
recurrent returns of Halley's comet are so accurate, but long-term
weather forecasting is difﬁcult. Furthermore, deterministic chaos
does
not
explain
macroscopic
irreversibility,
but
quasi-
deterministic chaos does.
Since it is possible to build deterministic machines d e.g., digital
computers d in a quasi-deterministic world, it is necessary to
demonstrate that life processes are not absolutely deterministic in
order to give room for free will to exist. In the present article,
numerous
examples
of
biological
information
processing
at
different hierarchical levels d intracellular, membrane and inter-
cellular d are presented in order to demonstrate that the
phenomenological control laws in biological systems are at times
quasi-deterministic, and at other times more random, but never
absolutely deterministic.
Some types of noise are inevitable external interference, but
some other types of noise are actively involved in biological infor-
mation processing and must be considered endogenous noise. The
most dramatic event is ion channel ﬂuctuations, which are posi-
tioned at a key step of nerve excitation and which are therefore
directly linked to decision making. What is remarkable is that the
step of highly erratic ﬂuctuations at the membrane level is imme-
diately followed by the most deterministic step of nerve impulse
generation and propagation. This kind of abrupt transition is made
possible by the hierarchical organization of information processing,
which separates and shields intercellular dynamics from mem-
brane dynamics with an additional layer of constraints. There is
little doubt that the human brain is not a deterministic machine
that executes pre-programmed action. Normal humans are neither
robots nor zombies.37
The most formidable free will denier was none other than the
co-founder of quantum mechanics, Erwin Schr€odinger. Quantum
mechanics, which replaces Newtonian mechanics as the authori-
tative natural law for the microscopic world, offers a glimpse of
hope for free will to exist. But Schr€odinger ﬂatly denied that pos-
sibility and claimed that quantum mechanics plays no role in bio-
logical processes. I suspect that most investigators who cited
Schr€odinger for his famous claim never bothered to read his book
What is Life? for the original content. Schr€odinger admitted the role
of quantum mechanics in causing X-ray induced mutation, but he
dismissed it as rare occurrences, much like statisticians in dis-
missing the infamous “outlier” data points that happen to contra-
dict their expectations. Perhaps Schr€odinger did not expect that
man-made ozone depletion in the atmosphere would make skin
cancers more and more prevalent. Moreover, strange quantum
effects
are
sometime
visible
to
the
naked
eye.
Of
course,
Schr€odinger did not anticipate how quantum mechanics would
transform human understanding of strange events in daily occur-
rences. Apparently, commonplace occurrences elicit little attention
from experts, much less from average people.
Another factor that has bred conﬁdence in biological deter-
minism is the astonishing advances made in genetics and molec-
ular
biology.
The
genetic
codes
are
digital
in
nature
and
inheritance of traits is sometimes unnervingly striking if only one
has a chance to watch how a pair of identical twins talk, think and
behave. It was no wonder that Richard Dawkins, of The Selﬁsh Gene
fame, denied the existence of free will. However, it is common
knowledge that genetic determinism is not absolute; the pheno-
typical expression is subject to inﬂuences of external factors,
which regulate or modify gene expression without altering the
genetic codes (collectively known as epigenetic factors). That the
genetic codes do not dictate the phenotypes in absolute terms was
dramatically demonstrated by cloning of a cat (Felis domesticus)
named CC (acronym for Copy Cat or Carbon Copy), by means of
nuclear transplantation of a processed donor cumulus cell from an
adult multicolored cat [Shin et al., 2002]: the kitten's coat-
coloration pattern is similar to that of the nuclear-donor cat, but
it is not an exact carbon copy (compare ﬁgures in [Shin et al., 2002]
and [Holden, 2002]).
Regarding the phenomenology of free will, many eminent
philosophers have offered their thoughts. For general readers,
Henrik Walter's summary appears to be the most succinct
and
relevant:
alternativism,
intelligibility
and
origination.
We aspire to demonstrating that properly interpreted reduc-
tionist ﬁndings can be reconciled with these three aspects of
phenomenology.
It is self-evident that quasi-determinism resolves the issue of
alternativism. But, as Popper pointed out, indeterminism is not
enough. Popper's concern was how free will allows us to deliberately
and intelligently make decisions. For the two latter issues, we must
credit Donald Campbell for his two-fold contribution: a plausible
model of creativity, and the concept of downward causation.
Quasi-determinism bothered Popper and many others because of
the element of randomness, which is often construed as arbitrariness.
However, our understanding of creative problem solving does not
support the notion that rational decision making has to be deter-
ministic. Paradoxically, the supposedly deterministic approach of
logical deduction dwindles to aimless blind searches when one con-
fronts a novel problem with modest to high complexity. In contrast,
the intuition-inspired approach (visual thinking) is constrained to
search solutions in the most promising directions (heuristic search-
ing) whileretaining acertain degree offreedomforexploration d sort
of an optimal compromise between top-down constraints and
bottom-up explorations. Therefore, quasi-determinism is exactly
what is required for creative problem solving and rational decision
making. Elsewhere I have demonstrated that exclusive rule-based
reasoning (strictly logical deductions) often leads to globally absurd
conclusions [Hong, 2013a]. Thus, quasi-determinism is perfectly
compatible with intelligibility of decision making.
Donald Campbell originally proposed the concept of downward
causation to explain the seemingly teleological outcomes of evo-
lution. It became somewhat problematic when the concept of
downward causation was extended to explain the issue of origi-
nation in free will and that in creativity. If we take our subjective
feelings about free will seriously rather than treat it as an illusion,
then downward causation is a necessary condition. But the theory
of downward causation remains somewhat speculative. Since
downward causation originates from the whole and inﬂuences the
parts, various theoreticians, out of necessity demanded by the
theory, postulated several enabling scenarios: (a) existence of
37 A dramatic account of zombie-like behavior of living organisms can be found in
an article entitled Mindsuckers [Zimmer, 2014].
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
722

forces other than low-level forces, (b) these speculated forces can
overpower low-level forces, (c) these speculated forces can shield
the inﬂuence of low-level forces, (d) force-less causation enabled
by high-level organizational constraints.
Among these possibilities, speculation (b) sounds too spooky to
be considered scientiﬁc, in my opinion. As for speculation (c), it has
appeared to be an illusion; being present as the constant ﬁxtures
that hold higher-level structures in proper places, low-level forces
cease to attract attention when investigators have viewed the
whole picture from the higher-level perspective. Furthermore,
Schr€odinger's 1945 opinion exerted a powerful d almost deter-
ministic d brainwashing effect on investigators; it carries no
burden of proof to assume that quantum mechanics has no effect
on biology, in spite of the fact that quantum uncertainty enters the
ﬁrst step of photosynthesis and it compromises the integrity of
gene replications and translations. As for the speculation of force-
less causation, we note only a single example: the phenomenolog-
ical force due to gradient-induced ion diffusion. Empirical evidence
in support of the existence of downward causation is scarce.
However, the work of Schwartz's group on obsessive-compulsive
disorder and the work of Andersen's group on robotics appear to be
convincing. These two examples have demonstrated downward
causation in the intermediate steps. The nature of origination,
which pertains to the initial event that jumpstarts the chain of
events for fulﬁlling downward causation, remains unclear.
Although the notion of downward causation affecting low-level
forces appears to contradict established facts in physics, the
alternative notion of downward causation affecting dispersions of
low-level forces does not contradict any known physical laws.
Opportunities remain open for traditional experimental scientists
to search for evidence in support of the speculated effect of
downward causation modulating dispersions of low-level forces. It
is one of the research topics that offer opportunities for collabo-
ration between phenomenologists and neuroscientists as well as
physicists.
In the present article, I make an effort to treat phenomenology
and reductionist sciences in an even-handed fashion. In this way, I
managed to reconcile most of important phenomenological issues
with both reductionist physics and reductionist neuroscience. Curi-
ously, I found some insights gained into deciphering human crea-
tivity to be helpful in quest of naturalizing phenomenology. First, it
appears that objectivity alone has not been adequate in making
creativity research scientiﬁc. This revelation opens up a new possi-
bility of naturalizing phenomenology: Phenomenological research
must strive for universality instead of objectivity. Second, visual
thinking, which cultivates what is known as intuition, thus rises to
the occasion and offers an ideal reasoning tool to achieve overall
consistency in phenomenological research. It turns out that visual
thinking is also effective in combating natural language’s tendency to
cause confusion and mislead human judgments. It was indeed this
chosen approach that allowed us to narrow the gulf which separates
phenomenology of free will from reductionist sciences. In doing so, I
might have disappointed some phenomenologists, but I also found
some new opportunities for phenomenological research. Some past
interpretations of downward causationturnsout to be untenable, but
quasi-determinism breathes new life into the concept of downward
causation: a new possibility for downward causation to exert its in-
ﬂuence by shaping the dispersions of quasi-deterministic physical
laws remains open and promising. A perceived paradigm shift is
therefore not justiﬁed at this stage. That does not mean future
justiﬁcation can be precluded now.
In a previous publication, I pointed out a dilemma: parsimony or
paradigm shift [Hong, 2013a]. When the existing paradigm cannot
handle an intractable problem, the temptation is to launch a new
paradigm. False paradigm shifts are possible simply because one
fails to exhaust all possibilities of preserving the old paradigm.
Conversely, hanging onto a paradigm that has outlived its useful-
ness and preserving strained parsimony well beyond the point
where the latter scheme ceases to be parsimonious is just as pa-
thetic. As long as we have something constructive to suggest, we
still have not yet exhausted all conceivable approaches.
Although it is not possible to prove or disprove Laplace's doc-
trine, the doctrine serves a useful purpose. By presenting a receding
target, the doctrine has discouraged premature declaration that
true noise exists. In the same spirit, science continues to seek
rational explanations for presently mysterious phenomena and
push the frontier of the unknown as far back as possible, and never
concede defeat. In pursuing the topic of free will, we push the
unknown to as early a step as possible, and at present. I choose to
suspend my judgment at the step of jumpstarting origination. In
doing so, demystiﬁcation of free will remains unfulﬁlled. However,
any spurious claim of having resolved the issue of origination might
inadvertently mislead future investigators. I believe that no in-
vestigators should be forced to jump to conclusions about anything
if they feel that some loose ends remain to be tied up. Let us focus
on tying up the remaining loose ends.
Ironically, The Theory of Intelligent Design also plays a role
similar to that of Laplace's doctrine of absolute determinism. The
Theory of Intelligent Design is also neither provable nor falsiﬁable.
Curiously, the theory also presents receding targets to challenge
science. As our ignorance recedes “with the widening bounds of
knowledge,” the proponents will stop arguing over the same topics
and switch to other presently inexplicable phenomena and continue
to challenge science, as long as they have not exhausted a long list of
presently inexplicable phenomena. Inadvertently, they continue to
serve as a challenge and driving force to entice scientists to excel. But
the non-falsiﬁability of The Theory of Intelligent Design disqualiﬁes
it from presenting itself as an “alternative theory” to be taught in a
science class in school. Instead, it belongs in the curriculum of the-
ology departments or philosophy departments.
Previous investigations of human creativity unequivocally
demonstrated that phenomenology of creativity critically contrib-
uted to elucidation of human creativity. In the present article, it is
readily evident that phenomenology of free will may suggest clues
for scientists to design new experiments which may help elucidate
the enigma of free will. However, phenomenological investigations
of free will sometimes have been insufﬁciently scientiﬁc because of
occasional logical ﬂaws due to personal blind spots. As I pointed out
earlier, the quickest way to identify one's own blind spots is to
listen to the critics or opponents. These logical ﬂaws could have
been avoided if phenomenologists had thought from the point of
view of reductionists. On the other hand, phenomenology's chal-
lenge to reductionist sciences is deﬁnitely not a hostile takeover but
rather new inspiration for future research. I believe that the time is
ripe for genuine cross-fertilization between phenomenological
philosophy and reductionist sciences. Phenomenological philoso-
phy provides the top-down constraints for reductionist theories to
focus on more fruitful directions, whereas new ﬁndings resulting
from the bottom-up explorations in reductionist sciences keep
phenomenology research robust.
Phenomenologists who remain skeptical about this latter possi-
bility are recommended to think twice before launching a paradigm
shift: Just demonstrate, at least in principle, that phenomenological
approaches can suggest a radically new way of conducting scientiﬁc
research that can reproduce a few important reductionist achieve-
ments, such as rediscovering penicillin, reinventing the blue light-
emitting diode, the digital computer and the method of sequencing
human genomes. On the other hand, reductionists may feel their
hands are full, and there is no need and no room to accommodate
philosophy. For this, David Hawkins had the following advice:
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
723

“Philosophy may be ignored but not escaped; and those who most
ignore least escape” (quoted in [Rosen, 1985], p. 45).
Acknowledgments
The author is deeply indebted to two individuals for helpful
suggestions during his long process of learning the topic of free
will: Arran Gare and Koichiro Matsuno. The author also thanks
Linda Arrigo for meticulously copyediting the manuscript as well as
suggestions concerning organization of this manuscript.
Notes added in the proof
The author wishes to bring to the readers’ attention perhaps the
most recent article that describes a (compatibilist) view concerning
physical determinism and free will by contemporary physicists and
philosophers: Musser, G., 2015. Is the cosmos random? Sci. Am. 313
(3), 88e93.
References
Adam, G., Delbrück, M., 1968. Reduction of dimensionality in biological diffusion
processes. In: Rich, A., Davidson, N. (Eds.), Structural Chemistry and Molecular
Biology. W. H. Freeman, San Francisco, pp. 198e215.
Adleman, L.M., 1994. Molecular computation of solutions to combinatorial prob-
lems. Science 266, 1021e1024.
Altman, D.G., Bland, J.M., 1995. Absence of evidence is not evidence of absence. Br.
Med. J. 311, 485.
Anderson, F.H. (Ed.), 1960. Francis Bacon: the New Organon and Related Writings.
Prentice-Hall, Upper Saddle River, NJ.
Angelopoulos, A., Apostolakis, A., Aslanides, E., Backenstoss, G., Bargassa, P.,
Behnke, O., Benelli, A., Bertin, V., Blanc, F., Bloch, P., Carlson, P., Carroll, M.,
Cawley, E., Charalambous, S., Chertok, M.B., Danielsson, M., Dejardin, M.,
Derre, J., Ealet, A., Eleftheriadis, C., Faravel, L., Fetscher, W., Fidecaro, M.,
Filipcic, A., Francis, D., Fry, J., Gabathuler, E., Gamet, R., Gerber, H.-J., Go, A.,
Haselden, A., Hayman, P.J., Henry-Couannier, F., Hollander, R.W., Jon-And, K.,
Kettle, P.-R., Kokkas, P., Kreuger, R., Le Gac, R., Leimgruber, F., Mandic, I.,
Manthos, N., Marel, G., Mikuz, M., Miller, J., Montanet, F., Muller, A., Nakada, T.,
Pagels,
B.,
Papadopoulos,
I.,
Pavlopoulos,
P.,
Policarpo,
A.,
Polivka,
G.,
Rickenbach, R., Roberts, B.L., Ruf, T., Santoni, C., Sch€afer, M., Schaller, L.A.,
Schietinger,
T.,
Schopper,
A.,
Tauscher,
L.,
Thibault,
C.,
Touchard,
F.,
Touramanis, C., Van Eijk, C.W.E., Vlachos, S., Weber, P., Wigger, O., Wolter, M.,
Zavrtanik, D., Zimmerman, D., 1998. First direct observation of time-reversal
non-invariance in the neutral-kaon system. Phys. Lett. B444, 43e51.
Arnheim, R., 1969. Visual Thinking. University of California Press, Berkeley, Los
Angeles and London.
Astumian, R.D., 1997. Thermodynamics and kinetics of a Brownian motor. Science
276, 917e922.
Astumian, R.D., 2001. Making molecules into motors. Sci. Am. 285 (1), 56e64.
Astumian, R.D., Derenyi, I., 1998. Fluctuation driven transport and models of mo-
lecular motors and pumps. Eur. Biophys. J. 27, 474e489.
Autumn, K., Liang, Y.A., Hsieh, S.T., Zesch, W., Chan, W.P., Kenny, T.W., Fearing, R.,
Full, R.J., 2000. Adhesive force of a single gecko foot-hair. Nature 405, 681e685.
Baars, B.J., 1988. A Cognitive Theory of Consciousness. Cambridge University Press,
Cambridge, New York, New Rochelle, Melbourne and Sydney.
Balaguer, M., 2014. Free Will. MIT Press, Cambridge, MA, and London.
Baldwin, J.M., 1925. Mental Development in the Child and the Race: Methods and
Processes, third revised ed. Macmillan, New York and London. Reprinted, 1968.
Augustus M. Kelley Publishers, New York.
Barry, D.M., Nerbonne, J.M., 1996. Myocardial potassium channels: electrophysio-
logical and molecular diversity. Annu. Rev. Physiol. 58, 363e394.
Bastick, T., 1982. Intuition: How We Think and Act. John Wiley and Sons, Chichester,
New York, Brisbane, Toronto and Singapore.
Bell, E.T., 1986. Men of Mathematics: the Lives and Achievements of the Great
Mathematicians from Zeno to Poincare. Simon and Schuster, New York, London,
Toronto, Sydney, Tokyo and Singapore.
Bever, T.G., Chiarello, R.J., 1974. Cerebral dominance in musicians and nonmusicians.
Science 185, 537e539.
Bezrukov, S.M., Vodyanoy, I., 1995. Noise-induced enhancement of signal trans-
duction across voltage-dependent ion channels. Nature 378, 362e364.
Bohm, D., 1957. Causality and Chance in Modern Physics. University of Pennsylvania
Press, Philadelphia.
Bourret, R.B., Borkovich, K.A., Simon, M.I., 1991. Signal transduction pathways
involving protein phosphorylation in prokaryotes. Annu. Rev. Biochem. 60,
401e441.
Braich, R.S., Chelyapov, N., Johnson, C., Rothemund, P.W.K., Adleman, L., 2002. So-
lution of a 20-variable 3-SAT problem on a DNA computer. Science 296,
499e502.
Bratman, M.E., 1987. Intention, Plans, and Practical Reason. Harvard University
Press, Cambridge, MA, and London.
Bratman, M.E., Israel, D.J., Pollack, M.E., 1988. Plans and resource-bounded practical
reasoning. Comput. Intell. 4, 349e355.
Bricmont, J., 1996. Science of chaos or chaos in science? In: Gross, P.R., Levitt, N.,
Lewis, M.W. (Eds.), The Flight from Science and Reason, Annal. NY Acad. Sci., vol.
775. New York Academy of Sciences, New York, pp. 131e175.
Brush, S.G., 1966. Kinetic Theory. In: Irreversible Processes, vol. 2. Pergamon Press,
Oxford,
London,
Edinburgh,
New
York,
Toronto,
Sydney,
Paris
and
Braunschweig.
Brush, S.G., 1976a. The Kind of Motion We Call Heat: a History of the Kinetic Theory
of Gases in the 19th Century, Book 1: Physics and the Atomists. North-Holland
Publishing, Amsterdam, New York and Oxford.
Brush, S.G., 1976b. The Kind of Motion We Call Heat: a History of the Kinetic Theory
of Gases in the 19th Century, Book 2: Statistical Physics and Irreversible Pro-
cesses. North-Holland Publishing, Amsterdam, New York and Oxford.
Campbell, D.T., 1960. Blind variation and selective retention in creative thought as in
other knowledge processes. Psychol. Rev. 67, 380e400.
Campbell, D.T., 1974. Downward causation. In: Ayala, F.J., Dobzhansky, T. (Eds.),
Studies in the Philosophy of Biology: Reduction and Related Problems. Uni-
versity of California, Berkeley, CA, pp. 179e186.
Cannon, S.C., 1996. Sodium channel defects in myotonia and periodic paralysis.
Annu. Rev. Neurosci. 19, 141e164.
Chaudhury, M.K., Whitesides, G.M., 1992. How to make water run uphill. Science
256, 1539e1541.
Chinarov, V.A., Gaididei, Y.B., Kharkyanen, V.N., Sit'ko, S.P., 1992. Ion pores in bio-
logical
membranes
as
self-organized
bistable
systems.
Phys.
Rev.
A46,
5232e5241.
Churchland, P.M., Churchland, P.S., 1990. Could a machine think? Sci. Am. 262 (1),
32e37.
Cicchetti, D.V., 1991. The reliability of peer review for manuscript and grant sub-
missions: a cross-disciplinary investigation. Behav. Brain Sci. 14, 119e186.
Cohen, P.R., Levesque, H.J., 1990. Intention is choice with commitment. Artif. Intell.
42, 213e261.
Conrad, M., 1990. Molecular computing. In: Yovits, M.C. (Ed.), Advances in Com-
puters, vol. 31. Academic Press, Boston, San Diego, New York, London, Sydney,
Tokyo and Toronto, pp. 235e324.
Cottingham, J. (Ed.), 1996. Western Philosophy: an Anthology. Blackwell Publishers,
Oxford and Cambridge, MA.
Crutchﬁeld, R., 1962. Conformity and creative thinking. In: Gruber, H.E., Terrell, G.,
Wertheimer, M. (Eds.), Contemporary Approaches to Creative Thinking: a
Symposium Held at the University of Colorado. Atherton Press, New York,
pp. 120e140.
Damasio, A.R., 1994. Descartes' Errors: Emotion, Reason, and the Human Brain. G. P.
Putnam's Sons, New York.
Davies, P., 2002. That mysterious ﬂow. Sci. Am. 287 (3), 40e47.
De Waal, F., 1998. Chimpanzee Politics: Power and Sex Among Apes, revised ed.
Johns Hopkins University Press, Baltimore, MD, and London.
Deci, E.L., 1995. Why We Do What We Do: the Dynamics of Personal Autonomy. G. P.
Putnam's Sons, New York.
Deci, E.L., Koestner, R., Ryan, R.M., 1999. A meta-analytic review of experiments
examining the effects of extrinsic rewards on intrinsic motivation. Psychol. Bull.
125, 627e668.
Deci, E.L., Ryan, R.M., 1985. Intrinsic Motivation and Self-determination in Human
Behavior. Plenum, New York and London.
Dennett, D.C., 1991. Consciousness Explained. Little, Brown and Co., Boston, New
York, Toronto and London.
Dennett, D.C., 2001. The Fantasy of First-person Science. Nicod Lectures. Private
Circulation. At: http://ase.tufts.edu/cogstud/papers/chalmersdeb3dft.htm.
Dennett, D.C., 2007. Heterophenomenology reconsidered. Phenomenol. Cognitive
Sci. 6 (1e2), 247e270.
Dennett, D.C., Kinsbourne, M., 1992. Time and the observer: the where and when of
consciousness in the brain. Behav. Brain Sci. 15, 183e247.
Diggins, J., Ralph, J.F., Spiller, T.P., Clark, T.D., Prance, H., Prance, R.J., 1994. Chaotic
dynamics in the rf superconducting quantum-interference-device magnetom-
eter: a coupled quantum-classical system. Phys. Rev. E49, 1854e1859.
Drake, S., 1978. Galileo at Work: His Scientiﬁc Biography. University of Chicago
Press, Chicago.
Dyer, J., Gregersen, H., Christensen, C., 2011. The Innovation's DNA. Harvard Business
Review Press, Boston, MA.
Earman, J., 1986. A Primer on Determinism. D. Reidel Publishing, Dordrecht, Boston,
Lancaster and Tokyo.
Eccles, J.C., 1953. The Neurophysiological Basis of Mind: the Principles of Neuro-
physiology. Oxford University Press, Oxford.
Eccles, J.C., 1979. The Human Mystery. Springer-Verlag, Berlin, Heidelberg and New
York.
Ellis, G.F.R., 2009. Top-down causation and the human brain. In: Murphy, N.,
Ellis, G.F.R., O'Connor, T. (Eds.), Downward Causation and the Neurobiology of
Free Will. Springer-Verlag, Berlin and Heidelberg, pp. 63e81.
Evans, J.St.B.T., Frankish, K. (Eds.), 2009. In Two Minds: Dual Processes and Beyond.
Oxford Univesity Press, New York.
Evans, J.St.B.T., 2008. Dual-processing accounts of reasoning, judgment, and social
cognition. Annu. Rev. Psychol. 59, 255e278.
Fang, S.P., 2003. Are there differential word length effects in the two visual ﬁelds?
Brain Lang. 85, 467e485.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
724

Farber, S.L., 1981. Identical Twins Reared Apart: a Reanalysis. Basic Books, New York.
Fatt, P., Katz, B., 1952. Spontaneous subthreshold activity at motor nerve endings.
J. Physiol. 117, 109e128.
Feynman, R., 1967. The Character of Physical Law. MIT Press, Cambridge, MA, and
London.
Frankfurt, H.G., 2005. On Bullshit. Princeton University Press, Princeton and Oxford.
Frankfurt, H.G., 2006. On Truth. Alfred A. Knopf, New York.
Gabriel, B., Teissie, J., 1996. Proton long-range migration along protein monolayers
and its consequences on membrane coupling. Proc. Natl. Acad. Sci. U. S. A. 93,
14521e14525.
Galilei, G., 1610. Sidereus Nuncius (in Latin). English translation, 1989. The Sideral
Messenger. van Helden, A., Translator. University of Chicago Press, Chicago and
London.
Gallagher, S., Schmicking, D. (Eds.), 2010. Handbook of Phenomenology and
Cognitive Science. Springer, Dordrecht, New York, Heidelberg and London.
Gallagher, S., Zahavi, D., 2008. The Phenomenological Mind: an Introduction to
Philosophy of Mind and Cognitive Science. Routledge, London and New York.
Gazzaniga, M.S., 2011. Who's in Charge?: Free Will and the Science of the Brain.
HarperCollins, New York.
Gazzaniga, M.S., 2015. Tales from Both Sides of the Brain. HarperCollins, New York.
Georgeff, M., Rao, A., 1998. Rational software agents: from theory to practice. In:
Jennings, N.R., Wooldridge, M.J. (Eds.), Agent Technology: Foundations, Appli-
cations, and Markets. Springer-Verlag, Berlin, Heidelberg and New York,
pp. 139e160.
Gigerenzer, G., 1996. On narrow norms and vague heuristics: a reply to Kahneman
and Tversky. Psychol. Rev. 103, 592e596.
Giles, W.R., Imaizumi, Y., 1988. Comparison of potassium currents in rabbit atrial
and ventricular cells. J. Physiol. 405, 123e145.
Gilman, A.G., 1984. G proteins and dual control of adenylate cyclase. Cell 36,
577e579.
Goldman, A., 1990. Action and free will. In: Osherson, D.N., Kosslyn, S.M.,
Hollerbach, J.M. (Eds.), An Invitation to Cognitive Science, Visual Cognition and
Action, vol. 2. MIT Press, Cambridge, MA, and London, pp. 317e340.
Goldstein, S., 1996. Quantum philosophy: the ﬂight from reason in science. In:
Gross, P.R., Levitt, N., Lewis, M.W. (Eds.), The Flight from Science and Reason,
Annal. NY Acad. Sci., 775, pp. 119e125.
Gould, S.J., 1988. The Streak of Streaks. The New York Review of Books.
Gross, P.R., Levitt, N., Lewis, M.W. (Eds.), 1996. The Flight from Science and Reason.
Annal. NY Acad. Sci., vol. 775. New York Academy of Sciences, New York.
Gutzwiller, M.C., 1992. Quantum chaos. Sci. Am. 266 (1), 78e84.
Hadamard, J., 1945. The Psychology of Invention in the Mathematical Field.
Princeton University Press, Princeton, NJ. Reprinted, 1996. The Mathematician's
Mind: The Psychology of Invention in the Mathematical Field. Princeton Uni-
versity Press, Princeton, NJ.
Haken, H., 1983. Advanced Synergetics: Instability Hierarchies of Self-organizing
Systems and Devices. Springer-Verlag, Berlin, Heidelberg, New York and Tokyo.
Hanyu, Y., Matsumoto, G., 1991. Spatial long-range interactions in squid giant axons.
Physica D49, 198e213.
Hardyck, C., Tzeng, O.J.L., Wang, W.S.-Y., 1977. Cerebral lateralisation effects in visual
half-ﬁeld experiments. Nature 269, 705e707.
Harris, L.J., 1988. Right-brain training: some reﬂections on the application of
research on cerebral hemispheric specialization to education. In: Molfese, D.L.,
Segalowitz, S.J. (Eds.), Brain Lateralization in Children: Developmental Impli-
cations. Guilford Press, New York and London, pp. 207e235.
Harris, S., 2012. Free Will. Free Press.
Hatori, K., Honda, H., Shimada, K., Matsuno, K., 1998. Staggered movement of an
actin ﬁlament sliding on myosin molecules in the presence of ATP. Biophys.
Chem. 70, 241e245.
Hawking, S., 1993. Black Holes and Baby Universes and Other Essays. Bantam Books,
New York, Toronto, London, Sydney and Auckland.
Hayes, B., 2001. Randomness as a resource. Am. Sci. (Sigma Xi) 89, 300e304.
Haynes, J.-D., 2011. Decoding and predicting intentions. Ann. N. Y. Acad. Sci. 1224,
9e21.
Heiligenberg, W., 1991. The neural basis of behavior: a neuroethological view. Annu.
Rev. Neurosci. 14, 247e267.
Heisenberg, W., 1958. Physics and Philosophy: the Revolution in Modern Science.
Harper and Row, New York.
Herbert, A., Rich, A., 1999. RNA processing in evolution: the logic of soft-wired
genomes. In: Caporale, L.H. (Ed.), Molecular Strategies in Biological Evolution,
Annal. NY Acad. Sci., vol. 870. New York Academy of Sciences, New York,
pp. 119e132.
Herrnstein, R.J., Murray, C., 1996. Bell Curve: Intelligence and Class Structure in
American Life. Free Press, New York.
Hertwig, R., Gigerenzer, G., 1999. The ‘conjunction fallacy’ revisited: how intelligent
inferences look like reasoning errors. J. Behav. Decis. Mak. 12, 275e305.
Hilgemann, D.W., 1997. Cytoplasmic ATP-dependent regulation of ion transporters
and channels: mechanisms and messengers. Annu. Rev. Physiol. 59, 193e220.
Hille, B., 2001. Ion Channels of Excitable Membranes, third ed. Sinauer Associates,
Inc., Sunderland, MA.
Hodgkin, A.L., Huxley, A.F., 1952. A quantitative description of membrane current
and its application to conduction and excitation in nerve. J. Physiol. 117,
500e544.
Holden, C., 2002. Carbon-copy clone is the real thing. Science 295, 1443e1444.
Holmes, E., 1991. The life of Mozart: including his correspondence, Hogwood, C.
(Ed.). The Folio Society, London.
Hong, F.T., 1998a. A survival guide to cope with information explosion in the 21st
century: picture-based vs. rule-based learning, 21st Webzine 3(4), Speed Sec-
tion. http://www.vxm.com/FHong.html.
Hong, F.T., 1998b. Control laws in the mesoscopic processes of biocomputing. In:
Holcombe, M., Paton, R. (Eds.), Information Processing in Cells and Tissues.
Plenum, New York and London, pp. 227e242.
Hong, F.T., 2003a. The enigma of creative problem solving: a biocomputing
perspective. In: Barsanti, L., Evangelista, V., Gualtieri, P., Passarelli, V., Vestri, S.
(Eds.), Molecular Electronics: Bio-sensors and Bio-computers. Kluwer Academic
Publishers, Dordrecht, Boston and London, pp. 457e542.
Hong, F.T., 2003b. Towards physical dynamic tolerance: an approach to resolve the
conﬂict between free will and physical determinism. BioSystems 68, 85e105.
Hong, F.T., 2005a. A multi-disciplinary survey of biocomputing: 1. Molecular and
cellular levels. In: Bajic, V.B., Tan, T.W. (Eds.), Information Processing and Living
Systems. Imperial College Press, London, pp. 1e139.
Hong, F.T., 2005b. A multi-disciplinary survey of biocomputing: 2. Systems and
evolutionary levels, and technological applications. In: Bajic, V.B., Tan, T.W.
(Eds.), Information Processing and Living Systems. Imperial College Press,
London, pp. 141e573.
Hong, F.T., 2006. The enigma of human creativity: hidden messages from Nikola
Tesla's Moji Pronalasci. In: Halasi, R.J., Cosic, I.P., Halasi, T.J. (Eds.), Nas Tesla (Our
Tesla). University of Novi Sad Faculty of Technical Science, and Society for the
Promotion of Science, Novi Sad, Serbia, pp. 127e176 (in English). http://www.
med.wayne.edu/physiology/facultyproﬁle/hong/PDF%20ﬁles/Tesla_Novi_Sad_
viewing.pdf.
Hong, F.T., 2013a. The role of pattern recognition in creative problem solving: a case
study in search of new mathematics for biology. J. Prog. Biophys. Mol. Biol. 113,
181e215. http://dx.doi.org/10.1016/j.pbiomolbio.2013.03.017.
Hong, F.T., 2013b. Deciphering the enigma of human creativity: can a digital com-
puter think? J. Comput. Sci. Syst. Biol. 6, 228e261. http://dx.doi.org/10.4172/
jcsb.1000120.
Hume, D., 1993, Steinberg, E. (Ed.), An Enquiry Concerning Human Understanding,
second ed. Hackett Publishing, Indianapolis, IN.
James, W., 1937. The Will to Believe and Other Essays in Popular Philosophy.
Longmans, Green and Co., London, New York and Toronto.
Jennings, N.R., Wooldridge, M.J., 1998. Applications of intelligent agents. In:
Jennings, N.R., Wooldridge, M.J. (Eds.), Agent Technology: Foundations, Appli-
cations, and Markets. Springer-Verlag, Berlin, Heidelberg and New York,
pp. 3e28.
Juarrero, A., 1999. Dynamics in Action. MIT Press, Cambridge, MA, and London.
Juarrero, A., 2009. Top-down causation and autonomy in complex systems. In:
Murphy, N., Ellis, G.F.R., O'Connor, T. (Eds.), Downward Causation and the
Neurobiology of Free Will. Springer-Verlag, Berlin and Heidelberg, pp. 83e102.
Kagan,
Y.Y.,
Knopoff,
L., 1981.
Stochastic
synthesis
of
earthquake
catalogs.
J. Geophys. Res. 86, 2853e2862.
Kahneman, D., 2011. Thinking, Fast and Slow. Farrar, Straus and Giroux, New York.
Kant, I., 1786. Metaphysical foundations of natural science. In: Ellington, J. (Ed.),
Philosophy of Material Nature, Book 2. Hackett Publishing, Indianapolis, IN.
Kinsbourne, M., 1998. Taking the Project seriously: the unconscious in neuroscience
perspective. In: Bilder, R.M., LeFever, F.F. (Eds.), Neuroscience of the Mind on the
Centennial of Freud's Project for a Scientiﬁc Psychology, Annal. NY Acad. Sci.,
vol. 843. New York Academy of Sciences, New York, pp. 111e115.
Kitamura, K., Tokunaga, M., Iwane, A.H., Yanagida, T., 1999. A single myosin head
moves along an actin ﬁlament with regular steps of 5.3 nanometers. Nature 397,
129e134.
Kleppner, D., 1996. Physics and common nonsense. In: Gross, P.R., Levitt, N.,
Lewis, M.W. (Eds.), The Flight from Science and Reason, Annal. NY Acad. Sci., vol.
775. New York Academy of Sciences, New York, pp. 126e130.
Knoblich, G., Oellinger, M., 2006. The eureka moment. Sci. Am. Mind 17 (5), 38e43.
Koch, C., 2009. Free will, physics, biology, and the brain. In: Murphy, N., Ellis, G.F.R.,
O'Connor, T. (Eds.), Downward Causation and the Neurobiology of Free Will.
Springer-Verlag, Berlin and Heidelberg, pp. 31e52.
Korchev, Y.E., Bashford, C.L., Alder, G.M., Apel, P.Y., Edmonds, D.T., Lev, A.A.,
Nandi, K., Zima, A.V., Pasternak, C.A., 1997. A novel explanation for ﬂuctuations
of ion current through narrow pores. FASEB J. 11, 600e608.
Langley, P., Simon, H.A., Bradshaw, G.L., Zytkow, J.M., 1987. Scientiﬁc Discovery:
Computational Explorations of the Creative Processes. MIT Press, Cambridge,
MA, and London.
Laplace, P.S. (marquis de), 1814. Essai philosophique sur les probabilites. Gauthier-
Villars, Paris. English translation from the 6th French ed., 1902. A Philosophical
Essay on Probabilities. (Truscott, F.W., Emory, F.L. Translators), John Wiley and
Sons, New York. Reprinted, 1952. Dover Publications, New York.
Lawson, A.E., 2003. The Neurological Basis of Learning, Development and Discovery:
Implications for Science and Mathematics Instruction. Kluwer Academic Pub-
lishers, Dordrecht, Boston and London.
LeDoux, J.E., 1996. The Emotional Brain: the Mysterious Underpinnings of Emotional
Life. Simon and Schuster, New York.
Lepper, M.R., Henderlong, J., Gingras, I., 1999. Understanding the effects of extrinsic
rewards on intrinsic motivation d uses and abuses of meta-analysis: comment
on Deci, Koestner, and Ryan (1999). Psychol. Bull. 125, 669e676.
Letivin, D.J., 2006. This Is Your Brain on Music: the Science of a Human Obsession.
Dutton, Penguin Group, New York.
Lev,
A.A.,
Korchev,
Y.E.,
Rostovtseva,
T.K.,
Bashford,
C.L.,
Edmonds,
D.T.,
Pasternak, C.A., 1993. Rapid switching of ion current in narrow pores: impli-
cations for biological channels. Proc. R. Soc. Lond B252, 187e192.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
725

Levitan, I.B., 1994. Modulation of ion channels by protein phosphorylation and
dephosphorylation. Annu. Rev. Physiol. 56, 193e212.
Libet, B., 1985. Unconscious cerebral initiative and the role of conscious will in
voluntary action. Behav. Brain Sci. 8, 529e566.
Libet, B., Gleason, C.A., Wright, E.W., Pearl, D.K., 1983. Time of conscious intention to
act in relation to onset of cerebral activity (readiness-potential): the uncon-
scious initiation of a freely voluntary act. Brain 106, 623e642.
Lindsay, R., 1961. Physical Mechanics. Van Nostrand, Princeton, NJ.
Linetsky, B., 2013. Free Will: Sam Harris Has it (Wrong). Cognitive Consulting, Inc.
Lorenz, E.N., 1963. Deterministic nonperiodic ﬂow. J. Atmosph. Sci. 20, 130e141.
Lorenz, E., 1993. The Essence of Chaos. University of Washington Press, Seattle, WA.
MacCallum, M., 1975. News and views: the breakdown of physics? Nature 257, 362.
Mackey, M.C., 2001. Microscopic dynamics and the second law of thermodynamics.
In: Mugnai, C., Ranfagni, A., Schulman, L.S. (Eds.), Time's Arrows, Quantum
Measurement and Superluminal Behavior. Consiglio Nazionale Delle Ricerche,
Roma, pp. 49e65.
Matsuno, K., 1989. Protobiology: Physical Basis of Biology. CRC Press, Boca Raton, FL.
Medawar, P.B., 1967. The Art of the Soluble. Methuen, London.
Mellers, B., Hertwig, R., Kahneman, D., 2001. Do frequency representations elimi-
nate conjunction effects? An exercise in adversarial collaboration. Psychol. Sci.
12 (4), 269e275.
Moro, R., 2009. On the nature of the conjunction fallacy. Synthese 171, 1e24.
Moss, F., Wiesenfeld, K., 1995. The beneﬁts of background noise. Sci. Am. 273 (2),
66e69.
Murch, R., Johnson, T., 1999. Intelligent Software Agents. Prentice Hall PTR, Upper
Saddle River, NJ.
Murphy, N., 2009. Introduction and overview. In: Murphy, N., Ellis, G.F.R.,
O'Connor, T. (Eds.), Downward Causation and the Neurobiology of Free Will.
Springer-Verlag, Berlin and Heidelberg, pp. 1e28.
Musallam, S., Corneil, B.D., Greger, B., Scherberger, H., Andersen, R.A., 2004.
Cognitive control signals for neural prosthetics. Science 305, 258e262.
Musser, G., 2002. A hole at the heart of physics. Sci. Am. 287 (3), 48e49.
Neher, E., Sakmann, B., 1976. Single-channel currents recorded from membrane of
denervated frog muscle ﬁbres. Nature 260, 799e802.
Neher, E., Stevens, C.F., 1977. Conductance ﬂuctuations and ionic pores in mem-
branes. Annu. Rev. Biophys. Bioeng. 6, 345e381.
Newborn, M., 1997. Kasparov versus Deep Blue: Computer Chess Comes of Age.
Springer-Verlag, New York, Berlin and Heidelberg.
Newell, A., Shaw, J.C., Simon, H.A., 1962. The processes of creative thinking. In:
Gruber, H.E., Terrell, G., Wertheimer, M. (Eds.), Contemporary Approaches to
Creative Thinking: a Symposium Held at the University of Colorado. Atherton
Press, New York, pp. 63e119.
Nichols, C.G., Lopatin, A.N., 1997. Inward rectiﬁer potassium channels. Annu. Rev.
Physiol. 59, 171e191.
Nicolis, G., Prigogine, I., 1989. Exploring Complexity: an Introduction. W. H.
Freeman, New York.
Nielsen, J.A., Zielinski, B.A., Ferguson, M.A., Lainhart, J.E., Anderson, J.S., 2013. An
evaluation of the left-brain vs. right-brain hypothesis with resting state func-
tional connectivity magnetic resonance imaging. PLoS ONE 8 (8), e71275. http://
dx.doi.org/10.1371/journal.pone.0071275.
Pais, A., 1982. ‘Subtle is the Lord…’: the Science and the Life of Albert Einstein.
Oxford University Press, Oxford and New York.
Park, H., Park, J., Lim, A.K.L., Anderson, E.H., Alivisatos, A.P., McEuen, P.L., 2000.
Nanomechanical oscillations in a single-C60 transistor. Nature 407, 57e60.
Peirce, C., 1931e1958. Collected Papers of Charles Sanders Peirce, Vols. 1e6, Hart-
shorne, C., Weiss, P. (Eds.); Vols. 7e8, Burks, A.W. (Ed.). Harvard University
Press, Cambridge, MA.
Penrose, R., 1989. The Emperor's New Mind: Concerning Computers, Minds, and the
Laws of Physics. Oxford University Press, Oxford. Reprinted, 1991. Penguin
Books, New York and London.
Poincare, H., 1890. Sur le probleme des trois corps et les equations de la dynamique.
Acta Math. 13, 1e270.
Poincare, H., 1908. The Foundations of Science: Science and Hypothesis, the Value of
Science, Science and Method, Paris (in French). English translation, 1913.
(Halsted, G.B. Translator), Science Press, New York and Garrison, NY.
Popper, K.R., 1934. Logik der Forschung, Vienna (The Logic of Scientiﬁc Discovery).
English translation, 1968, rev. ed. Hutchinson, London. Reprinted, 1992, Rout-
ledge, London and New York.
Popper, K.R., 1950a. Indeterminism in quantum physics and in classical physics: part
I. Br. J. Phil. Sci. 1, 117e133.
Popper, K.R., 1950b. Indeterminism in quantum physics and in classical physics:
part II. Br. J. Phil. Sci. 1, 173e195.
Popper, K.R., 1982. The Open Universe: an Argument for Indeterminism. Rowman
and Littleﬁeld, Totowa, NJ. Reprinted, 1998, Routledge, London and New York.
Popper, K., 1992. Unended Quest: an Intellectual Autobiography, rev. ed. Routledge,
London.
Porter, M.A., Liboff, R.L., 2001. Chaos on the quantum scale. Am. Sci. (Sigma Xi) 89,
532e537.
Prigogine, I., 1989. The microscopic meaning of irreversibility. Z. Phys. Chem. Leipz.
270, 477e490.
Prigogine, I., 1997. The End of Certainty: Time, Chaos, and the New Laws of Nature.
Free Press, New York, London, Toronto, Sydney and Singapore.
Prigogine, I., Stengers, I., 1984. Order Out of Chaos: Man's Dialogue with Nature.
Bantam Books, New York, Toronto, London, Sydney and Auckland.
Proudfoot, D., 2015. What turing himself said about the Imitation Game. IEEE Spectr.
52 (7), 42e47.
Rao, A.S., Georgeff, M.P., 1991. Modeling rational agents within a BDI-architecture.
In: Allen, J., Fikes, R., Sandewall, E. (Eds.), Proceedings of the 2nd International
Conference on Principles of Knowledge Representation and Reasoning, April
22e25, 1991. Cambridge, MA. Morgan Kaufmann Publishers, San Mateo, CA,
pp. 473e484.
Rauscher, F.H., Shaw, G.L., Ky, K.N., 1993. Music and spatial task performance. Nature
365, 611.
Rauscher, F.H., Shaw, G.L., Ky, K.N., 1995. Listening to Mozart enhances spatial-
temporal reasoning: towards a neurophysiological basis. Neurosci. Lett. 185,
44e47.
Rosen, R., 1985. Anticipatory Systems: Philosophical, Mathematical and Methodo-
logical Foundations. Pergamon Press, Oxford, New York, Toronto, Sydney, Paris
and Frankfurt.
Rothberg, B.S., Magleby, K.L., 2001. Testing for detailed balance (microscopic
reversibility) in ion channel gating. Biophys. J. 80, 3025e3026.
Ruelle, D., 1991. Chance and Chaos. Princeton University Press, Princeton, NJ.
Russell, B., 1948. Human Knowledge: its Scope and Limits. Simon and Schuster, New
York.
Schr€odinger, E., 1936. Indeterminism and free will. Nature 13e14. July 4 Issue.
Schr€odinger, E., 1945. What is Life?: the Physical Aspect of the Living Cell. Cam-
bridge University Press, Cambridge, and Macmillan Company, New York.
Schulman, L.S., 1997. Time's Arrows and Quantum Measurement. Cambridge Uni-
versity Press, Cambridge, New York and Melbourne.
Schwartz, J.M., Beyette, M., 1996. Brain Lock: Free Yourself from Obsessive-
compulsive Behavior. ReganBooks, HarperCollins, New York.
Schwartz, J.M., Begley, S., 2002. The Mind and the Brain: Neuroplasticity and the
Power of Mental Force. ReganBooks/HarperCollins, New York.
Schwarz, F.P., Wasik, S.P., 1976. Fluorescence measurements of benzene, naphtha-
lene, anthracene, pyrene, ﬂuoranthene, and benzo[e]pyrene in water. Anal.
Chem. 48, 524e528.
Searle, J.R., 1990. Is the brain's mind a computer program? Sci. Am. 262 (1), 26e31.
Searle, J.R., 1999. Chinese room argument. In: Wilson, R.A., Keil, F.C. (Eds.), The MIT
Encyclopedia of the Cognitive Sciences. MIT Press, Cambridge, MA, and London,
pp. 115e116.
Shih, J.C., Chen, K., Ridd, M.J., 1999. Monoamine oxidase: from genes to behavior.
Annu. Rev. Neurosci. 22, 197e217.
Shin, T., Kraemer, D., Pryor, J., Uu, L., Ruglia, J., Howe, L., Buck, S., Murphy, K.,
Lyons, L., Westhusin, M., 2002. A cat cloned by nuclear transplantation. Nature
415, 859.
Sigworth, F.J., Neher, E., 1980. Single Naþ channel currents observed in cultured rat
muscle cells. Nature 287, 447e449.
Simon, H.A., 1977. Models of Discovery. D. Reidel Publishing Co., Dordrecht and
Boston.
Simon, H.A., 1979. Models of Thought. Yale University Press, New Haven, CT, and
London.
Simon, H.A., 1989. Models of Thought, vol. II. Yale University Press, New Haven, CT,
and London.
Simon, H.A., 1992. Scientiﬁc discovery as problem solving. In: Egidi, M., Marris, R.
(Eds.), Economics, Bounded Rationality and the Cognitive Revolution. Edward
Elgar Publishing, Hants, UK, and Brookﬁeld, VT, pp. 102e119.
Simon, H.A., Newell, A., 1958. Heuristic problem solving: the next advance in op-
erations research. Operations Research 6, 1e10. Reprinted in: Simon, H.A. (Ed.),
1982. Models of Bounded Rationality, Volume 1 Economic Analysis and Public
Policy 380e389. MIT Press Cambridge, MA, and London.
Skinner, B.F., 1953. Science and Human Behavior. Macmillan, New York.
Sokal, A., Bricmont, J., 1998. Fashionable Nonsense: Postmodern Intellectuals' Abuse
of Science. Picador USA/St. Martin's Press, New York.
Soon, C.S., Brass, M., Heinze, H.-J., Haynes, J.-D., 2008. Unconscious determinants of
free decisions in the human brain. Nat. Neurosci. 11 (5), 543e545.
Sperry, R.W., 1974. Lateral specialization in the surgically separated hemispheres.
In: Schmitt, F.O., Worden, F.G. (Eds.), The Neurosciences: Third Study Program.
MIT Press, Cambridge, MA, and London, pp. 5e19.
Sperry, R., 1982. Some effects of disconnecting the cerebral hemisphere. Science 217,
1223e1226.
Springer, S.P., Deutsch, G., 1989. Left Brain, Right Brain, third ed. W. H. Freeman,
New York.
Stanovich, K.E., West, R.F., 2000. Individual differences in reasoning: implications
for the rationality debate. Behav. Brain Sci. 23, 645e665.
Steinberg, I.Z., 1986. On the time reversal of noise signals. Biophys. J. 50, 171e179.
Strickland, E., 2015. Telekinesis made simple. IEEE Spectr. 52 (7), 11e14.
Suzuki, M., 2004. Actomyosin motor mechanism: afﬁnity gradient surface force
model. Prog. Colloid Polym. Sci. 125, 38e41.
Tan, L.H., Spinks, J.A., Gao, J.-H., Liu, H.-L., Perfetti, C.A., Xiong, J., Storfer, K.A., Pu, Y.,
Liu, Y., Fox, P.T., 2000. Brain activation in the processing of Chinese characters
and words: a functional MRI study. Hum. Brain Mapp. 10, 16e27.
Teller, E., 1998. Science and morality. Science 280, 1200e1201.
Tentori, K., Crupi, V., 2012. On the conjunction fallacy and the meaning of and, yet
again: a reply to Hertwig, Benz, and Krauss (2008). Cognition 122, 123e134.
Tesla, N., 1977. Moji Pronalasci/My Inventions. Skolska Knjiga, Zagreb (in both
Serbian and English).
Tolman, R.C., 1938. The Principles of Statistical Mechanics. Oxford University Press,
Glasgow, New York, Toronto, Melbourne and Wellington. Reprinted, 1967.
Tsien, R.W., Hess, P., McCleskey, E.W., Rosenberg, R.L., 1987. Calcium channels:
mechanisms of selectivity, permeation, and block. Annu. Rev. Biophys. Biophys.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
726

Chem. 16, 265e290.
Tsien, R.W., Lipscombe, D., Madison, D.V., Bley, K.R., Fox, A.P., 1988. Multiple types of
neuronal calcium channels and their selective modulation. Trends Neurosci. 11,
431e438.
Tsong, T.Y., Liu, D.-S., Chauvin, F., Gaigalas, A., Astumian, R.D., 1989. Electro-
conformational coupling (ECC): an electric ﬁeld induced enzyme oscillation for
cellular energy and signal transductions. Bioelectrochem. Bioenerg. 21, 319e331.
Turing, A.M., 1950. Computing Machinery and Intelligence. Mind Vol. LIX, No. 236.
Reprinted in: Feigenbaum, E.A., Feldman, J. (Eds.),1963. Computers and Thought.
McGraw-Hill Book Co., New York, San Francisco, Toronto and London, pp. 11e35.
Tversky, A., Kahneman, D., 1983. Extension versus intuitive reasoning: the
conjunction fallacy in probability judgment. Psychol. Rev. 90 (4), 293e315.
Tversky, A., Kahneman, D., 1974. Judgment under uncertainty: heuristics and biases.
Science 185, 1124e1131.
Tzeng, O.J.L., Hung, D.L., Cotton, B., Wang, W.S.-Y., 1979. Visual lateralization effect in
reading Chinese characters. Nature 282, 499e501.
Van Gluck, R., 1995. Who's in charge here? and who's doing all the work? In: Heil, J.,
Mele, A. (Eds.), Mental Causation. Clarendon Press, Oxford, pp. 233e256.
van Inwagen, P., 1983. An Essay on Free Will. Oxford University Press, Oxford.
Varo, Gy., Lanyi, J.K., 1991. Thermodynamics and energy coupling in the bacterio-
rhodopsin photocycle. Biochemistry 30, 5016e5022.
Velmans, M., 1991. Is human information processing conscious? Behav. Brain Sci. 14,
651e726.
Wallwork, E., 1997. Determinism, free will, compatibilism. J. Am. Psychoanal. Assoc.
45 (1), 307e314.
Walter, H., 1999. Neurophilosophie der Willensfreiheit, second ed. Mentis Verlag,
Paderborn (in German). English translation, 2001. Neurophilosophy of Free
Will: From Libertarian Illusions to a Concept of Natural Autonomy. (Klohr, C.,
Translator), MIT Press, Cambridge, MA, and London.
Wegner, D.M., 2002. The Illusion of Conscious Will. MIT Press, Cambridge, MA, and
London.
West, T.G., 1997. In the Mind's Eye, updated ed. Prometheus Books, New York.
Wiesenfeld, K., Moss, F., 1995. Stochastic resonance and the beneﬁts of noise: from
ice ages to crayﬁsh and SQUIDs. Nature 373, 33e36.
Wolfram, S., 2002. A New Kind of Science. Wolfram Media, Champaign, IL.
Wuketits, F.M., 2001. The philosophy of Donald T. Campbell: a short review and
critical appraisal. Biol. Philosophy 16, 171e188.
Yates, F.E., 1980. Physical causality and brain theories. Am. J. Physiol. 238,
R277eR290 (Regulatory Integrative Comp. Physiol. 7).
Zermelo,
E., 1896.
Ueber
einen
Satz
der
Dynamik
und
die
mechanische
W€armetheorie. Ann. Phys. 57, 485e494.
Zimmer, C., 2014. Mindsuckers. Natl. Geogr. 226 (5), 36e55.
F.T. Hong / Progress in Biophysics and Molecular Biology 119 (2015) 671e727
727

