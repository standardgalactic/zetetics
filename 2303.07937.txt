Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation
Junyoung Seo*1
Wooseok Jang*1
Min-Seop Kwak*1
Jaehoon Ko1
Hyeonsu Kim1
Junho Kim2
Jin-Hwa Kim‚Ä†2
Jiyoung Lee‚Ä†2
Seungryong Kim‚Ä†1
1Korea University
2NAVER AI Lab
Abstract
Text-to-3D generation has shown rapid progress in re-
cent days with the advent of score distillation, a methodol-
ogy of using pretrained text-to-2D diffusion models to op-
timize neural radiance Ô¨Åeld (NeRF) in the zero-shot set-
ting. However, the lack of 3D awareness in the 2D diffusion
models destabilizes score distillation-based methods from
reconstructing a plausible 3D scene. To address this is-
sue, we propose 3DFuse, a novel framework that incor-
porates 3D awareness into pretrained 2D diffusion mod-
els, enhancing the robustness and 3D consistency of score
distillation-based methods.
We realize this by Ô¨Årst con-
structing a coarse 3D structure of a given text prompt and
then utilizing projected, view-speciÔ¨Åc depth map as a con-
dition for the diffusion model. Additionally, we introduce a
training strategy that enables the 2D diffusion model learns
to handle the errors and sparsity within the coarse 3D struc-
ture for robust generation, as well as a method for ensur-
ing semantic consistency throughout all viewpoints of the
scene. Our framework surpasses the limitations of prior
arts, and has signiÔ¨Åcant implications for 3D consistent gen-
eration of 2D diffusion models. Project page is available at
https://ku-cvlab.github.io/3DFuse/.
1. Introduction
Text-to-3D generation, the task of generating realistic
3D models from texts [49, 26, 15], has rapidly grown in
recent years to become an essential problem in computer
vision and graphics. Recent approaches [38, 59, 25, 36]
incorporate generative models with neural radiance Ô¨Åeld
(NeRF) [32] to recover more plausible 3D scenes.
Re-
cent works along this line, such as DreamFusion [38] and
Score Jacobian Chaining (SJC) [59], utilize large-scale dif-
fusion models [45, 48] as a prior for optimizing NeRF. This
methodology, named score distillation [38], enables suc-
*Equal contribution.
‚Ä†Co-corresponding author.
Baseline
3DFuse (Ours)
‚Äúcute sheep with white fur‚Äù
‚Äúa fantastical wizard's tower with 
a spiral staircase and mysterious artifacts‚Äù
Figure 1: Teaser. 3DFuse enables view-consistent 3D gen-
eration given text prompt, while baseline (SJC [59]) often
fails to recover plausible 3D.
cessful generation of a realistic 3D scene from a given text
prompt.
However, 3D scenes generated by score distillation [38]
are often prone to suffer from distortions and artifacts and
sensitive to text prompts and random seeds, as shown in
‚ÄúBaseline‚Äù results of ‚Äúcute sheep with white fur‚Äù in Fig. 1.
One such failure is a 3D-incoherence problem, in which the
rendered 3D scenes produce geometric features that belong
to the frontal view (such as a face) multiple times at various
viewpoints, as described in (a) of Fig. 2. This failure occurs
due to the 2D diffusion model‚Äôs lack of awareness regard-
ing 3D information, especially the camera pose. Since the
diffusion model does not know from which direction the
object is viewed, it is easily biased to produce frontal ge-
ometric features at all viewpoints, including side and back
views, resulting in heavy distortions to the 3D scene.
An ideal solution to address this issue would be using a
3D aware diffusion model that can generate 3D-consistent
images for any given scene viewpoint. However, naively
training such a model from scratch would require extensive
3D data, whose size is minuscule compared to vast amounts
1
arXiv:2303.07937v3  [cs.CV]  16 Mar 2023

of 2D data [51]. In addition, this approach cannot beneÔ¨Åt
from the powerful generalization capabilities of diffusion
models trained on large 2D data. In light of this, a practical
solution would be a middle-ground approach that combines
the best of both worlds - a pretrained 2D diffusion model
imbued with 3D awareness suitable for 3D-consistent NeRF
optimization.
In this paper, we propose a novel framework, named
3DFuse, that effectively injects 3D awareness into pre-
trained 2D diffusion models. Given a text prompt, we Ô¨Årst
sample semantic code to fasten the semantic identity of the
generated scene. The semantic code consists of a gener-
ated 2D image and a prompt embedding optimized from
the pretrained diffusion model. Our consistency injection
module takes this semantic code and obtains a viewpoint-
speciÔ¨Åc depth map: a coarse 3D geometry constructed by
an off-the-shelf model [35, 41, 61] from the generated im-
age is projected to given viewpoint to build the depth map.
The module then leverages the coarse depth map and the
semantic code to inject 3D information into the diffusion
model. As the predicted 3D geometry is bound to have er-
rors, our module is able to handle the errors and coarseness
within the depth maps. To this end, we introduce a sparse
depth injector to implicitly correct erroneous depth infor-
mation and LoRA [14] adaptation to preserve the semantics
consistently. Our framework achieves signiÔ¨Åcant improve-
ment over previous works in generation quality and geomet-
ric consistency.
We summarize our contributions as follows:
‚Ä¢ We propose a novel framework, called 3DFuse, that
infuses 3D awareness into a pretrained 2D diffusion
model, preserving the original generalization capabil-
ity.
‚Ä¢ By distilling the score of the diffusion model that pro-
duces 3D-consistent image, 3DFuse stably optimizes
NeRF for view-consistent text-to-3D generation.
‚Ä¢ We demonstrate the effectiveness of our framework
qualitatively, and introduce a new metric for quanti-
tative evaluation of 3D-consistency.
2. Related work
Diffusion models.
Diffusion models [11, 54, 55] have
gained much attention as generative models due to their
stability, diversity, and scalability. Given these advantages,
diffusion models have been applied in various Ô¨Åelds, such
as image translation [45, 56, 53], image editing [29, 19],
and conditional generation [18, 45, 67]. Especially, text-to-
image generation has been highlighted with the introduction
of various guidance techniques [12, 2, 13]. GLIDE [34]
utilizes CLIP [39] guidance to enable text-to-image gener-
ation, followed by large-scale text-to-image diffusion mod-
els such as Imagen [48], DALL-E2 [40], and Stable Dif-
‚Äúa cute cat‚Äù
Diffusion
Model
Diffusion
Model
(a) Naive score distillation
‚Äúa cute cat‚Äù
Pose ùúã!
3DFuse
3DFuse
Pose ùúã"
Diffusion
Model
Diffusion
Model
(b) 3D-aware score distillation (Ours)
Figure 2: Motivation. (a) Previous methods [38, 55] only
use noisy rendered images and prompt itself for score dis-
tillation through diffusion model, resulting in poor 3D co-
herence. (b) Our 3DFuse addresses this issue and shows
robust performance in recovering 3D-consistent scene.
fusion [45]. The emergence of such models has led to the
widespread utilization of pretrained text-to-image models
for the tasks such as endowing additional conditions [60,
67] or performing manipulations [6, 23, 46].
3D shape synthesis.
3D shape synthesis is a task of con-
structing a 3D representation (e.g., voxel grid [58, 24],
mesh [8, 33, 10, 9, 7], point cloud [27, 69, 66], implicit
Ô¨Åelds [63, 28, 68, 62, 1]) from a given input, usually a
single image. Early approaches such as PrGAN [5] and
PointFlow [65] reconstruct single images into voxel and
point cloud, while some works address speciÔ¨Åc tasks such
as human mesh reconstruction [21, 17, 20, 16].
Recent
works [35, 61] employ large generative models trained on
large-scale data to achieve breakthroughs in 3D shape gen-
eration: Point-E [35] directly trains a diffusion model on
point cloud data for a conditional point cloud generation.
Another work, MCC [61], trains a masked autoencoding ar-
chitecture on a large-scale 3D dataset Co3D [43] for point
cloud construction from single images.
Text-to-3D generation.
Existing text-to-3D generation
methods generally employ pretrained vision-and-language
models, such as CLIP [39] to generate 3D shapes and scenes
from text prompts. DreamFields [15] incorporates CLIP
with neural radiance Ô¨Åelds (NeRF) [32] demonstrating the
potential for zero-shot NeRF optimization using only CLIP
as guidance. Recently, Dreamfusion [38] and SJC [59] have
2

demonstrated an impressive ability to generate NeRF with
frozen diffusion models instead of CLIP. Among its follow-
up works, Latent-NeRF [31] and Dream3D [64] are concur-
rent works and the most comparable to our work because
they both utilize explicit 3D shapes to provide additional
training signals in NeRF optimization. Dream3D directly
initializes NeRF using generated SDF [37], while Latent-
NeRF utilizes a user-provided mesh to give NeRF direct oc-
cupancy loss [30] for geometry optimization. Our approach
differs from theirs in that we leverage the 3D priors to im-
plicitly imbue the diffusion model itself with 3D awareness
for consistent and robust NeRF generation, while they apply
3D priors directly to facilitate NeRF optimization.
3. Preliminaries
Diffusion models.
Diffusion models are generative mod-
els that learn data distribution from a Gaussian distribution
by a gradual denoising process [11]. Diffusion models de-
Ô¨Åne a deterministic forward process q(¬∑) that adds noise
such that q(xt|x0) := N(xt; Œ±tx0, œÉ2
t I), where xt is a
noised sample with noise level t and x0 is a clean sam-
ple, e.g., original image, and Œ±t and œÉt are pre-deÔ¨Åned
variables that control a noise schedule. The reverse pro-
cess consists of denoising steps that progressively remove
noise by modeling a neural network ¬µŒ∏(xt, t) with param-
eters Œ∏ that predicts x0 given xt, and sampling from a pos-
terior function derived from the forward process such that
pŒ∏(xt‚àí1|xt) := q(xt‚àí1|xt, x0 = ¬µŒ∏(xt, t)). As shown in
DDPMs [11], noise approximation model œµŒ∏(xt, t) can be
used instead of ¬µŒ∏(xt, t) as follows:
œµŒ∏(xt, t) = xt ‚àíŒ±t¬µŒ∏(xt, t)
œÉt
.
(1)
For a conditional generation, for instance, text-to-image
diffusion models such as Stable Diffusion [45] receive a text
prompt as an additional condition. SpeciÔ¨Åcally, when a text
prompt c is given, a mapping model T(¬∑) maps the prompt
c into the embedding e = T(c). Then, the embedding e
is injected into the diffusion model. Formally, we denote
the text-to-image diffusion model as œµŒ∏(xt, t, T(c)). For the
sake of brevity, we shall omit the variable t and refer to
the function œµŒ∏(xt, T(c)). In this case, the loss function for
training the diffusion model is deÔ¨Åned as follows:
LdiÔ¨Ä(Œ∏, x) = Et,œµ
h
w(t)‚à•œµŒ∏
 xt, T(c)

‚àíœµ‚à•2
2
i
,
(2)
where œµ is a Gaussian noise and w(t) is a weighting func-
tion. Intuitively, this loss trains the model to predict the
added Gaussian noise in the data.
Score distillation to NeRF.
Optimizing NeRF with score
distillation was Ô¨Årst proposed in DreamFusion [38], which
optimizes NeRF parameters to follow the direction of the
score predicted by the diffusion model, i.e., the direction
of higher-density regions [38]. Concurrent works such as
SJC [59] and subsequent studies [25, 31] have been pro-
posed based on similar intuition.
SpeciÔ¨Åcally, let us denote Œò as parameters of NeRF, and
RŒò(œÄ) as a rendering function given a camera pose œÄ. In
DreamFusion and SJC, random camera pose œÄ is sampled,
and the diffusion model is utilized to infer the 2D score of
the rendered image, i.e., x = RŒò(œÄ). This score is used
to optimize the NeRF parameters Œò by letting the rendered
image move to the higher-density regions, i.e., to be realis-
tic. This can be explained as minimizing the loss function
introduced in Eq. 2 with respect to the NeRF parameters Œò
instead of the diffusion model‚Äôs parameters Œ∏ such that
Œò‚àó= argmin
Œò
EœÄ
h
LdiÔ¨Ä
 Œ∏, x = RŒò(œÄ)
i
.
(3)
In addition, the Jacobian term of the diffusion U-net
‚àÇœµŒ∏
 xt, T(c)

/‚àÇxt from the gradient of the loss function
‚àáŒòLdiÔ¨Äcan be omitted for efÔ¨Åciency, and the new gradient
can be formulated such that
‚àáŒòLSDS(Œ∏, x = RŒò(œÄ))
‚âúEt,œµ
h
Àúw
 t
 œµŒ∏
 xt, T(c)

‚àíœµ
 ‚àÇx
‚àÇŒò
i
,
(4)
where ‚àáŒòLSDS is a gradient of LdiÔ¨Äwith the U-net Jaco-
bian term omitted, and Àúw(t) is a weighting function.
4. Method
4.1. Motivation and overview
The score distillation-based text-to-3D methods [38, 25,
59] assume that maximizing the likelihood of images ren-
dered from arbitrary viewpoints of a NeRF can be translated
as maximizing the likelihood of the overall NeRF. Although
this is a reasonable assumption, 2D diffusion models lack
3D awareness, which leads to inconsistent and distorted ge-
ometry of generated NeRF. To overcome this challenge and
ensure NeRF‚Äôs 3D-consistency, we incorporate 3D aware-
ness into the diffusion model.
Previous works [38, 59] attempt this by using the text
prompts that roughly describe the camera viewpoint (e.g.,
‚Äúside view‚Äù). However, this ad-hoc approach is severely
limited: the ambiguity caused by the same text prompt rep-
resenting a wide range of different pose values leaves NeRF
generation vulnerable to geometric inconsistencies. A dif-
fusion model directly conditioned on camera pose value œÄ
would be an ideal solution; however, this is not feasible due
to the ambiguity in deÔ¨Åning a canonical space for each 3D
scene, and the difÔ¨Åculty of acquiring camera pose data.
To overcome these limitations, we present 3DFuse, our
novel framework for effectively incorporating 3D aware-
ness into pretrained text-to-image diffusion models [11, 45].
3

Prompt
(ùëê)
ùëí
Initial Image Generation
ùëí‚àó
ùëí‚àó
,
Semantic Code s
Sparse Depth 
Injector
Projection
Pivotal Tuning
LoRA
Layers
ùëí
Initial Prompt 
Embedding
ùëí
Pretrained Diffusion Model
ùëí‚àó
Optimized Prompt 
Embedding
Camera Pose (ùúã)
Coarse 3D
Diffusion U-Net
Neural
Rendering
Consistency Injection Module
ùëíOptimization
NeRF Optimization
3D
Model
Semantic Code Sampling
Score
Distillation
T
Figure 3: Overall architecture of 3DFuse. In the framework, semantic code is sampled to reduce the text prompt ambiguity
by generating an image based on the text prompt and then optimizing the prompt‚Äôs embedding to match the generated image.
Our consistency injection module receives this semantic code to synthesize view-speciÔ¨Åc depth maps as a condition to the
diffusion U-net. The module also consists of a sparse depth injector to implicitly incorporate 3D awareness by utilizing an
external 3D prior, and LoRA [14] layers to maintain semantic consistency.
Instead of having the diffusion model explicitly model cam-
era pose œÄ, our method constructs a coarse point cloud
of an initially generated image ÀÜx through an off-the-shelf
model [35, 61] and gives its viewpoint-speciÔ¨Åc depth map
as a condition for the diffusion model. As the sparse depth
map contains rich 3D information describing the scene from
a given viewpoint, this approach effectively enables the dif-
fusion model to generate NeRF in a 3D aware manner. In
addition, to ensure the semantic similarity of NeRF across
viewpoints, we introduce semantic code sampling, which
constrains the entire 3D scene to a single semantic identity.
In the following, we describe our proposed methodology
in detail. Our overall architecture is described in Fig. 3.
4.2. Semantic code sampling
The task of text-to-3D generation comes with a problem
of inherent ambiguity within the text prompt. For instance,
the text prompt ‚Äúa cute cat‚Äù has color ambiguity, as it could
refer to either a black or white cat. This ambiguity leads to
the freedom to generate any image within this range, which
harms the quality and coherence of the generated NeRF.
Each score distillation step may guide NeRF toward widely
different textures and semantics, which could result in a lack
of coherence in the generated output. In Sec. 5.5, we pro-
vide a detailed demonstration of this phenomenon.
We introduce a simple yet effective technique to counter
this text prompt ambiguity, which we call semantic code
sampling. To specify the semantic identity of the scene we
optimize and thus reduce ambiguity, we Ô¨Årst generate a 2D
image ÀÜx from the text prompt c. Then, we optimize the
text prompt embedding e to better Ô¨Åt the generated image,
similarly to the textual inversion [6]:
e‚àó= argmin
e
||œµŒ∏(ÀÜxt, e) ‚àíœµ||2
2,
(5)
where ÀÜxt is a noised image of the generated image ÀÜx with
the noise œµ and the noise level t.
We refer to the pair of the generated image ÀÜx and the op-
timized embedding e‚àóas semantic code s, i.e., s := (ÀÜx, e‚àó),
which are the inputs for our consistency injection module.
4.3. Incorporating a coarse 3D prior
Our approach aims to incorporate 3D awareness into pre-
trained 2D diffusion models, and to achieve this we con-
struct a coarse 3D representation of a given initial image
and project it to a target viewpoint to make a sparse depth
map. This sparse depth map is leveraged in our consistency
injection module as a condition for 3D awareness.
SpeciÔ¨Åcally, an off-the-shelf model D(¬∑) receives an im-
age as input and outputs a coarse 3D representation, which
is a sparse 3D point cloud in our architecture.
We can
choose D(¬∑) from a wide variety of models: it could be
a point cloud generative model such as Point-E [35] or a
single-image reconstruction model such as MCC [61]. Us-
ing them as 3D priors, we construct a sparse point cloud and
project it to get a sparse depth map P corresponding to the
4

camera pose œÄ:
P = P(D(ÀÜx), œÄ),
(6)
where P(¬∑) is a depth-projection function. Subsequently,
adopting the architecture of ControlNet [67], our sparse
depth injector EœÜ receives the sparse depth map P, and its
output features are added to the intermediate features within
diffusion U-net of œµŒ∏
 ÀÜxt, e‚àó
, which can be further formu-
lated such that œµŒ∏
 ÀÜxt, e‚àó; EœÜ(P)

.
This approach brings signiÔ¨Åcant advantages to score
distillation-based NeRF generation. Unlike previous meth-
ods that directly optimize NeRF using only global text
prompts, our 3DFuse conditions the 3D optimization ex-
plicitly on the semantic code and its view-speciÔ¨Åc depth
map. Not only does this enhance the 3D-consistency and
Ô¨Ådelity of NeRF as intended, but it also encourages the 3D
scene to be faithful to the semantic code, ensuring both ge-
ometric and semantic robustness of the generated 3D scene.
4.4. Training the sparse depth injector
The point cloud obtained by the off-the-shelf 3D model
inevitably contains errors and artifacts. It naturally causes
its depth map to also have artifacts, as shown in Fig. 4(a).
Therefore, our module must be able to handle both sparse
geometry and the errors of the projected depth map.
To this end, we employ two training strategies for our
sparse depth injector EœÜ(¬∑). First, we train our injector us-
ing sparse depth maps, acquired by projecting point clouds
from a point cloud dataset [43] to known viewpoints. By
training our module with sparse depth map‚Äìimage pairs,
our model learns to interpolate and infer dense structural
information from sparse depth. We impose strong augmen-
tations on the point cloud data by randomly subsampling
from it and adding randomly generated noisy points, which
increases the robustness of our model against errors and
noises present in the predicted sparse depth map. The texts
for the corresponding images are obtained using the image
caption model [4, 22].
Second, the injector EœÜ(¬∑) is also trained on predicted
dense depth maps of text-to-image pairs, acquired using Mi-
DaS [42]. This strengthens of model‚Äôs generalization capa-
bility, enabling it to infer structural information from cate-
gories that were not included in the 3D point cloud dataset
for sparse depth training.
Combining the two, given the depth map P along with
the corresponding image y and caption c, the training ob-
jective of the depth injector EœÜ is as follows:
Linject(œÜ) = Ey,c,P,t,œµ
h
||œµŒ∏
 yt, c; EœÜ(P)

‚àíœµ||2
2
i
,
(7)
which is similar to Eq. 2, but only tunes the depth injector
EœÜ while the diffusion model remains frozen. These train-
ing strategies enable our model to receive sparse and noisy
(a)
(b)
(c)
(d)
Figure 4: Qualitative results conditioned on the sparse
depth map. Given sparse depth maps in (a), (b) are the
results of depth-conditional Stable Diffusion, (c) are the re-
sults of ControlNet [67] trained on MiDaS [42] depths only,
and (d) are our 3DFuse results. The given text prompts are
‚Äúa front view of an owl‚Äù and ‚Äúa majestic eagle‚Äù.
depth maps directly as input and successfully infer dense
and robust structural information from them, without need-
ing any auxiliary depth completion network. Fig. 4 illus-
trates the effectiveness of our approach: our approach suc-
cessfully generates realistic results without being restricted
to the domain of the point cloud dataset. Note that the owl
and eagle used in the illustration of Fig. 4 are not included
in the category of the point cloud dataset [43] we use.
4.5. Pivotal tuning for semantic consistency
To accomplish our objective, the diffusion model should
produce a score that generates identical objects as much as
possible from different camera poses, based on a semantic
code. Although optimized embedding e‚àópreserves the se-
mantics, we further enhance this by adopting LoRA [14]
technique motivated by [47]. LoRA layers œà consist of lin-
ear layers, inserted into the residual path of the attention lay-
ers in the diffusion U-net. SpeciÔ¨Åcally, at test time, given an
image ÀÜx generated from text prompt c, we Ô¨Åx the optimized
embedding e‚àóand tune the LoRA layers œà [44]:
LLoRA(œà) = Eœµ,t
h
||œµŒ∏(ÀÜxt, e‚àó; œà) ‚àíœµ||2
2
i
.
(8)
Note that we only train the LoRA layers instead of the entire
diffusion model to avoid overÔ¨Åtting to a speciÔ¨Åc viewpoint.
5. Experiments
5.1. Implementation details
We conduct all experiments with the Stable Diffu-
sion [45] based on LDM [45], and train the sparse depth
injector using a subset of the Co3D dataset [43] from the
weights [67] trained image-text pairs along with depth maps
predicted by MiDaS [42]. We conduct experiments with
two off-the-shelf modules, mainly using Point-E [35] and in
5

Stable-DreamFusion
SJC
3DFuse (Ours)
‚Äúa photo of cute hippo‚Äù
‚Äúa photo of comfortable bed‚Äù
‚Äúa cute little kitten‚Äù
‚Äúa cozy cabin in woods with a chimney and a porch‚Äù
‚Äúa cute pig with a pink snout and curly tail‚Äù
‚Äúa playful zebra with black and white stripe‚Äù
Figure 5: Qualitative comparisons for text-to-3D generation. We compare our approach with Stable-DreamFusion [38, 57]
and SJC [59]. Notice that all the results are rendered with a Ô¨Åxed random seed for a fair comparison. The geometric
consistency of our approach can also be found at the video results provided in the project page.
6

Table 1: User study. The user study is conducted by sur-
veying 102 participants to evaluate 3D coherence, prompt
adherence, and rendering quality.
Method
3D
Prompt
Overall
coherence adherence quality
3DFuse (Ours)
60.1%
59.2%
60.9%
Stable-DreamFusion [38, 57]
23.4%
23.4%
22.7%
SJC [59]
16.5%
17.4%
16.4%
Table 2: Quantitative evaluation. We compare 3D consis-
tency with our proposed metric based on COLMAP [50].
Method
Variance ‚Üì
3DFuse (Ours)
0.0499
SJC [59]
0.0870
certain experiments also leveraging MCC [61] with MiDaS.
The details of score distillation and neural radiance Ô¨Åeld
representation follow the settings of SJC [59], our baseline.
In semantic code sampling, we adopt Karlo [3] based on
unCLIP [40] because we Ô¨Ånd that it tends to follow user
prompt more closely.
5.2. Text-to-3D generation
We compare 3DFuse to previous score distillation text-
to-3D frameworks, DreamFusion [38] and Score Jacobian
Chaining (SJC) [59]. All experiments including 3DFuse
use Stable Diffusion [45] as a backbone, which is a pub-
licly available large-scale text-to-image diffusion model.
Because DreamFusion [38]‚Äôs ofÔ¨Åcial implementation uses
publicly unavailable Imagen [48] model as its baseline, we
instead resort to using Stable Diffusion [45] for implemen-
tation, which we call Stable-DreamFusion [57].
Qualitative evaluation.
We present our qualitative eval-
uation in Fig. 5, which demonstrates that the neural radi-
ance Ô¨Åelds generated by our framework surpass previous
methods in both Ô¨Ådelity and geometric consistency. While
previous methods produce inconsistent, distorted geometry
in multiple regions, our method generates robust geome-
try at every viewpoint, as evidenced by the Ô¨Ågure as well
as video results provided in the project page. Furthermore,
we present the qualitative results of our approach when uti-
lizing MCC [61] for the 3D prior in Fig. 6, which shows
generating high-quality 3D content as well.
Quantitative evaluation.
It is difÔ¨Åcult to conduct a quan-
titative evaluation on a zero-shot text-to-3D generative
model due to the absence of ground truth 3D scenes cor-
responding to the text prompts. Existing works provide ad-
ditional user studies [25] or employ CLIP R-Precision [15,
‚Äúa toy bicycle‚Äù
‚Äúa toy plane‚Äù
‚Äúa backpack‚Äù
‚Äúa penguin‚Äù
Figure 6: Qualitative results of 3DFuse with MCC [61].
Our 3DFuse framework yields high-Ô¨Ådelity rendering re-
sults even with MCC [61].
38]. However, CLIP R-Precision only measures retrieval
accuracy through projected 2D image and text input, so it is
not suitable for quantifying the geometric consistency of a
3D scene.
Instead, we propose a new metric that utilizes COLMAP
[50] to measure the consistency of a generated 3D scene.
We based on the fact that the accuracy of COLMAP cam-
era pose optimization is dependent upon the robustness and
consistency of 3D surfaces, as stated in [52]. We sample
100 uniformly spaced camera poses from a hemisphere of
Ô¨Åxed radius, at identical elevation, all directed towards the
sphere‚Äôs center, and render 100 images of the 3D scene.
COLMAP predicts the camera pose of each image for re-
construction. We then measure the difference between the
predicted camera poses of two adjacent images at the ren-
dering stage. The variance of these values is used as our
metric for 3D-consistency evaluation. High variance indi-
cates inaccuracy in the predicted camera poses, which cor-
responds to 3D inconsistency that makes optimization for
COLMAP difÔ¨Åcult. Additional details of our metric are pro-
vided in the supplementary materials.
Using this metric, we compare the consistency of scenes
generated by our 3DFuse and SJC. In Table 2, we re-
port the average of variance scores over 42 generated 3D
scenes.
Our 3DFuse framework outperforms SJC by a
large margin, quantitatively demonstrating that our frame-
work achieves more geometrically consistent text-to-3D
generation.
User study.
We have conducted a user study with 102 par-
ticipants, which is shown in Table. 1. We have asked the
participants to choose their preferred result in terms of 3D
coherence, prompt adherence, and overall quality between
Stable-DreamFusion [38, 57], SJC [59], and our 3DFuse.
The results show that 3DFuse generates 3D scenes that the
majority of people judge as having higher Ô¨Ådelity and bet-
ter geometric consistency than previous methods. Further
details are described in the supplementary material.
7

‚ÄúA side view of‚Äù
‚ÄúA front view of‚Äù
‚ÄúA backside view of‚Äù
(a)
‚Äúa blue bird with a beak and feathered wings‚Äù
‚Äúa black horse‚Äù
¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑
(b)
(a)
(b)
¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑
Figure 7: Qualitative comparison of view-dependent im-
age generation results.
(a) Results of view augmented
prompting, and (b) our results with 3DFuse framework.
Reference
Generated 3D scene
Figure 8: Qualitative result for image-conditional 3D
generation. When a reference image is directly given as
input, 3DFuse successfully captures the image‚Äôs style and
appearance for coherent 3D scene generation.
5.3. View-dependent text-to-image generation
We conduct text-to-image generation experiments with
3DFuse to verify our framework‚Äôs capability to infuse 3D
awareness into 2D diffusion models. We observe whether
the images are generated in a 3D aware manner when view-
points are given as conditions through our 3DFuse frame-
work.
Fig. 7 well demonstrates the effectiveness of our
framework: it allows for highly precise control of the cam-
era pose in 2D images, with even small changes in view-
point being reÔ¨Çected well on the generated images. Our
approach shows superior performance to previous prompt-
based methods (e.g. ‚ÄúA front view of‚Äù) regarding both pre-
cision and controllability of injected 3D awareness.
Condition
‚Äúa fancy chair‚Äù
‚Äúa beach chair‚Äù
Condition
‚Äúa simple mug‚Äù
‚Äúa rusty mug‚Äù
Figure 9: 3D reconstruction with different prompts given
a single 3D structure.
The Ô¨Årst column represents the
given point clouds as conditions, and the second and third
columns show synthesized results.
Left side
Right side
Left side
Right side
w/o semantic code sampling
w/ semantic code sampling
Figure 10: Ablation study on semantic code sampling.
The given prompts are ‚Äúa round orange pumpkin with a
stem and a textured surface‚Äù (top) and ‚Äúa product photo of
a toy tank‚Äù (bottom).
5.4. Image-conditional 3D generation
In this experiment, instead of generating the initial image
from a text prompt, we directly give an input image as the
initial image ÀÜx of our semantic code, which effectively re-
conÔ¨Ågures our framework as an image-conditional setting.
Fig. 8 displays impressive results: it shows that our archi-
tecture successfully captures the overall structure and ap-
pearance of the input image for NeRF generation. This has
interesting implications, as it demonstrates that our frame-
work has a certain degree of 3D reconstruction capability.
5.5. Analysis and ablation study
Different prompts with single 3D representation.
3D
representation, i.e., point cloud obtained from off-the-shelf
models [35, 61] is typically coarse and sparse. To investi-
8

Stable-DreamFusion
SJC
3DFuse (Ours)
‚Äúa photo of comfortable bed‚Äù
‚Äúa black horse‚Äù
Seed 0
Seed 1
Seed 2
Seed 3
Seed 4
Seed 0
Seed 1
Seed 2
Seed 3
Seed 4
Figure 11: Qualitative comparison for robustness. When given a single prompt, 3DFuse demonstrates robust performance
even in random seed variations. We visualize the outputs generated from the Ô¨Åxed seed from 0 to 4.
9

gate how the diffusion model implicitly reÔ¨Ånes the coarse
3D structure, we conduct an ablation study using a Ô¨Åxed
point cloud and different text prompts instead of inferring
a point cloud from the initial image ÀÜx of semantic code s.
Fig. 9 shows that our 3DFuse is Ô¨Çexible in responding to
the error and sparsity inherent in the point cloud, robustly
interpreting the depth map in accordance with the semantics
of text input.
Semantic code sampling.
We conduct an ablation study
on semantic code sampling in our 3DFuse framework. The
results in the Ô¨Årst two columns of Fig. 10 show signiÔ¨Åcant
differences in geometry (pumpkin‚Äôs smooth left surface and
bumpy right surface) and semantic details (the tank‚Äôs ab-
sence of wheel on the left) without semantic code sampling,
depending on viewpoints. In contrast, using semantic code
sampling ensures both geometric and semantic consistency
across viewpoints, demonstrating its prominence in preserv-
ing the semantic identity of the 3D scene.
Robustness.
We experiment with Ô¨Åxed prompts and
changing random seeds to verify the robustness of our ap-
proach compared to previous works [38, 57, 59]. Fig. 11
demonstrates that our approach exhibits robust performance
and is far less sensitive to random seeds compared to previ-
ous works.
6. Conclusion
In this paper, we address the 3D-incoherence problem in
text-to-3D generation by score distillation. We propose a
novel framework, dubbed 3DFuse, that effectively incor-
porates 3D awareness into a pretrained 2D diffusion model.
Our method utilizes viewpoint-speciÔ¨Åc depth maps from
a coarse 3D structure, complemented with a sparse depth
injector and semantic code sampling for semantic consis-
tency. Our approach offers a practical solution for address-
ing the limitations of current text-to-3D generation tech-
niques and opens up possibilities for generating more re-
alistic 3D scenes from text prompts. Our experimental re-
sults demonstrate the effectiveness of our framework, out-
performing previous models in quantitative metric and qual-
itative human evaluation.
References
[1] Zezhou Cheng, Menglei Chai, Jian Ren, Hsin-Ying Lee,
Kyle Olszewski, Zeng Huang, Subhransu Maji, and Sergey
Tulyakov. Cross-modal 3d shape generation and manipula-
tion. In Computer Vision‚ÄìECCV 2022: 17th European Con-
ference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings,
Part III, pages 303‚Äì321. Springer, 2022.
[2] Prafulla Dhariwal and Alexander Nichol.
Diffusion mod-
els beat gans on image synthesis. NeurIPS, 34:8780‚Äì8794,
2021.
[3] Jiseob Kim Donghoon Lee, Jongmin Kim Jisu Choi, Woon-
hyuk Baek Minwoo Byeon, and Saehoon Kim.
Karlo-
v1.0.alpha on coyo-100m and cc15m. https://github.
com/kakaobrain/karlo, 2022.
[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR, 2020.
[5] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape
induction from 2d views of multiple objects. In 2017 In-
ternational Conference on 3D Vision (3DV), pages 402‚Äì411.
IEEE, 2017.
[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or.
An image is worth one word: Personalizing text-to-
image generation using textual inversion.
arXiv preprint
arXiv:2208.01618, 2022.
[7] Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun
Lai, and Hao Zhang. Tm-net: Deep generative networks for
textured meshes.
ACM Transactions on Graphics (TOG),
40(6):263:1‚Äì263:15, 2021.
[8] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-
Kun Lai, and Hao Zhang. Sdm-net: Deep generative net-
work for structured deformable mesh. ACM Transactions on
Graphics (TOG), 38(6):1‚Äì15, 2019.
[9] Kunal Gupta. Neural mesh Ô¨Çow: 3d manifold mesh gener-
ation via diffeomorphic Ô¨Çows. University of California, San
Diego, 2020.
[10] Paul Henderson, Vagia Tsiminaki, and Christoph H Lampert.
Leveraging 2d data to learn textured 3d mesh generation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 7498‚Äì7507, 2020.
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems, 33:6840‚Äì6851, 2020.
[12] Jonathan Ho and Tim Salimans.
ClassiÔ¨Åer-free diffusion
guidance. 2022.
[13] Susung Hong, Gyuseong Lee, Wooseok Jang, and Se-
ungryong Kim.
Improving sample quality of diffusion
models using self-attention guidance.
arXiv preprint
arXiv:2210.00939, 2022.
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021.
[15] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter
Abbeel, and Ben Poole. Zero-shot text-guided object genera-
tion with dream Ô¨Åelds. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
867‚Äì876, 2022.
[16] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-
emplar Ô¨Åne-tuning for 3d human model Ô¨Åtting towards in-
the-wild 3d human pose estimation. In 2021 International
Conference on 3D Vision (3DV), pages 42‚Äì52. IEEE, 2021.
[17] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
10

pose. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7122‚Äì7131, 2018.
[18] Yossi Kanizo, David Hay, and Isaac Keslassy. Palette: Dis-
tributing tables in software-deÔ¨Åned networks. In 2013 Pro-
ceedings IEEE INFOCOM, pages 545‚Äì549. IEEE, 2013.
[19] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2426‚Äì
2435, 2022.
[20] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-Ô¨Åtting in the loop. In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 2252‚Äì2261, 2019.
[21] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani-
ilidis. Convolutional mesh regression for single-image hu-
man shape reconstruction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 4501‚Äì4510, 2019.
[22] Ankur Kumar. The illustrated image captioning using trans-
formers. ankur3107.github.io, 2022.
[23] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion
models already have a semantic latent space. arXiv preprint
arXiv:2210.10960, 2022.
[24] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao
Zhang, and Leonidas Guibas. Grass: Generative recursive
autoencoders for shape structures.
ACM Transactions on
Graphics (TOG), 36(4):1‚Äì14, 2017.
[25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fi-
dler, Ming-Yu Liu, and Tsung-Yi Lin.
Magic3d: High-
resolution text-to-3d content creation.
arXiv preprint
arXiv:2211.10440, 2022.
[26] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-
Wing Fu.
Iss: Image as stetting stone for text-guided 3d
shape generation. arXiv preprint arXiv:2209.04145, 2022.
[27] Andrew Luo, Tianqin Li, Wen-Hao Zhang, and Tai Sing Lee.
Surfgen: Adversarial 3d shape synthesis with explicit surface
discriminators.
In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 16238‚Äì16248,
2021.
[28] Andrew Luo, Tianqin Li, Wen-Hao Zhang, and Tai Sing Lee.
Surfgen: Adversarial 3d shape synthesis with explicit surface
discriminators.
In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 16238‚Äì16248,
2021.
[29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. In International Conference on Learning Representa-
tions, 2021.
[30] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 4460‚Äì4470, 2019.
[31] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and
Daniel Cohen-Or. Latent-nerf for shape-guided generation
of 3d shapes and textures. arXiv preprint arXiv:2211.07600,
2022.
[32] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance Ô¨Åelds for view syn-
thesis.
volume 65, pages 99‚Äì106. ACM New York, NY,
USA, 2021.
[33] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter
Battaglia. Polygen: An autoregressive generative model of
3d meshes. pages 7220‚Äì7229, 2020.
[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021.
[35] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751, 2022.
[36] Roy
Or-El,
Xuan
Luo,
Mengyi
Shan,
Eli
Shecht-
man, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.
Stylesdf: High-resolution 3d-consistent image and geome-
try generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 13503‚Äì
13513, 2022.
[37] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
In Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 165‚Äì174, 2019.
[38] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall.
Dreamfusion: Text-to-3d using 2d diffusion.
arXiv
preprint arXiv:2209.14988, 2022.
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748‚Äì8763. PMLR, 2021.
[40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022.
[41] Ren¬¥e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12179‚Äì12188, 2021.
[42] Ren¬¥e Ranftl,
Katrin Lasinger,
David Hafner,
Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE transactions on pattern analysis and machine
intelligence, 44(3):1623‚Äì1637, 2020.
[43] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3d: Large-scale learning and evaluation of
11

real-life 3d category reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 10901‚Äì10911, 2021.
[44] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real im-
ages. ACM Transactions on Graphics (TOG), 42(1):1‚Äì13,
2022.
[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¬®orn Ommer.
High-resolution image
synthesis with latent diffusion models.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684‚Äì10695, 2022.
[46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and KÔ¨År Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242, 2022.
[47] Simo
Ryu.
Low-rank
adaptation
for
fast
text-to-
image diffusion Ô¨Åne-tuning.
https://github.com/
cloneofsimo/lora, 2022.
[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour,
Burcu Karagol Ayan,
S Sara Mahdavi,
Rapha Gontijo Lopes, et al.
Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022.
[49] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,
Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-
shan. Clip-forge: Towards zero-shot text-to-shape genera-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 18603‚Äì18613,
2022.
[50] Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited.
In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
4104‚Äì4113, 2016.
[51] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon,
Ross Wightman,
Mehdi Cherti,
Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al.
Laion-5b:
An open large-scale dataset for
training next generation image-text models. arXiv preprint
arXiv:2210.08402, 2022.
[52] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. Graf: Generative radiance Ô¨Åelds for 3d-aware im-
age synthesis. Advances in Neural Information Processing
Systems, 33:20154‚Äì20166, 2020.
[53] Junyoung Seo, Gyuseong Lee, Seokju Cho, Jiyoung Lee, and
Seungryong Kim. Midms: Matching interleaved diffusion
models for exemplar-based image translation. arXiv preprint
arXiv:2209.11047, 2022.
[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. 2020.
[55] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. 2020.
[56] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon.
Dual diffusion implicit bridges for image-to-image transla-
tion. In International Conference on Learning Representa-
tions, 2022.
[57] Jiaxiang Tang.
Stable-dreamfusion:
Text-to-3d with
stable-diffusion, 2022. https://github.com/ashawkey/stable-
dreamfusion.
[58] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.
Octree generating networks: EfÔ¨Åcient convolutional archi-
tectures for high-resolution 3d outputs. In Proceedings of
the IEEE international conference on computer vision, pages
2088‚Äì2096, 2017.
[59] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich.
Score jacobian chaining: Lift-
ing pretrained 2d diffusion models for 3d generation. arXiv
preprint arXiv:2212.00774, 2022.
[60] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong
Chen, Qifeng Chen, and Fang Wen.
Pretraining is all
you need for image-to-image translation.
arXiv preprint
arXiv:2205.12952, 2022.
[61] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph
Feichtenhofer, and Georgia Gkioxari.
Multiview com-
pressive coding for 3D reconstruction.
arXiv preprint
arXiv:2301.08247, 2023.
[62] Rundi Wu and Changxi Zheng.
Learning to gener-
ate 3d shapes from a single example.
arXiv preprint
arXiv:2208.02946, 2022.
[63] Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, and Bao-
quan Chen. Pq-net: A generative part seq2seq network for
3d shapes. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 829‚Äì838,
2020.
[64] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying
Shan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot
text-to-3d synthesis using 3d shape prior and text-to-image
diffusion models. arXiv preprint arXiv:2212.14704, 2022.
[65] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge
Belongie, and Bharath Hariharan. PointÔ¨Çow: 3d point cloud
generation with continuous normalizing Ô¨Çows. In Proceed-
ings of the IEEE/CVF international conference on computer
vision, pages 4541‚Äì4550, 2019.
[66] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis.
Lion: Latent
point diffusion models for 3d shape generation. 2022.
[67] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models.
arXiv preprint
arXiv:2302.05543, 2023.
[68] X Zheng, Yang Liu, P Wang, and Xin Tong. Sdf-stylegan:
Implicit sdf-based stylegan for 3d shape generation. In Com-
puter Graphics Forum, volume 41, pages 52‚Äì63. Wiley On-
line Library, 2022.
[69] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation
and completion through point-voxel diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision, pages 5826‚Äì5835, 2021.
12

