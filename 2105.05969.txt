SPUX Framework: a Scalable Package for Bayesian
Uncertainty Quantiï¬cation and Propagation
Jonas Å ukys and Marco Bacci
May 14, 2021
We present SPUX - a modular framework for Bayesian inference enabling uncertainty
quantiï¬cation and propagation in linear and nonlinear, deterministic and stochastic mod-
els, and supporting Bayesian model selection. SPUX can be coupled to any serial or
parallel application written in any programming language, (e.g. including Python, R, Ju-
lia, C/C++, Fortran, Java, or a binary executable), scales eï¬€ortlessly from serial runs on
a personal computer to parallel high performance computing clusters, and aims to provide
a platform particularly suited to support and foster reproducibility in computational sci-
ence. We illustrate SPUX capabilities for a simple yet representative random walk model,
describe how to couple diï¬€erent types of user applications, and showcase several readily
available examples from environmental sciences. In addition to available state-of-the-art
numerical inference algorithms including EMCEE, PMCMC (PF) and SABC, the open
source nature of the SPUX framework and the explicit description of the hierarchical par-
allel SPUX executors should also greatly simplify the implementation and usage of other
inference and optimization techniques.
1. Introduction
For centuries, human intuition and curiosity toward natural phenomena have been the main driving
forces behind the discovery of the fundamental laws of physics, and behind the formulations of
mathematical models capable of describing past and forecasting future behavior of various complex
systems. In evironmental sciences, in particular, there are strong justiï¬cations, and thus a strong
trend, for using stochastic models, such as stochastic diï¬€erential equations (SDEs) and individual
based models (IBMs), to simulate the dynamical systems of interest.
Indeed, these models are
especially useful when intrinsic uncertainties are present as in, for instance, the modeling of inter-
connected systems such as climate, weather, ocean and lake dynamics, subsurface ground water ï¬‚ows,
hydrological catchments, urban ï¬‚oods, and ecological communities, to name a few.
In recent years, as reviewed in [GDB+19], two additional important inï¬‚uencing factors have arisen
and have become available to scientists: considerable computational power and a massive increase
in data availability. The steady increase in computational power has allowed models to reduce their
level of approximation to reality by increasing complexity and/or by achieving faster convergence
towards the exact solution. Concurrently, the recent technological advances in sensing and imaging
have initiated the so-called era of big data, allowing one to complement mechanistic modeling based
on ï¬rst principles (e.g. conservation laws) and human ingenuity with observational data [KBBP16].
However, the opportunity to use high performance computing (HPC) infrastructures to enable an
eï¬ƒcient coupling of complex models and/or of large data-sets, is posing signiï¬cant challenges to the
so-called scientiï¬c programming and computing practices. Indeed, nowadays users are often required
to run their forward simulations on HPC clusters, while developers need to be able to exploit diï¬€erent
types of parallelism to ensure the feasibility of model calibration and uncertainty propagation. It is
also worth noting, that while for some complex models already a single forward simulation can be
computationally expensive, the statistical inference methodologies for assimilating datasets can be
extremely demanding already for models of intermediate complexity.
1
arXiv:2105.05969v1  [stat.CO]  12 May 2021

By building on these necessities, the focus of our contribution is on describing a new software
framework, called SPUX, which stands for "Scalable Package for Uncertainty Quantiï¬cation in X",
that aims to abstract and simplify the access to modern computing infrastructure for reproducible
uncertainty quantiï¬cation and propagation. The remainder of this section is dedicated to exposing
basic information about Bayesian inference, which is at the core of SPUX (a more detailed exposure
is provided in section 2), and to brieï¬‚y reviewing similar existing computational suites.
To advance the scientiï¬c understanding of complex systems, statistical inference techniques such
as Bayesian inference [GCS+14] can be used for probabilistic quantiï¬cation (i.e. including uncertain-
ties) of model parameters and (past, present and future) model states, and for comparing several
available models using Bayesian model selection. Bayesian inference conditions the prior distributions
of model parameters and (stochastic model) states (which probabilistically describe any prior infor-
mation regarding model parameter and output values) on the data to get the corresponding posterior
distributions. For instance, it can be based on the so-called likelihood function for a given model,
which formulates the model as a probability distribution of observations for given model parameter
values (to which modelâ€™s inputs and outputs are associated), and on the prior distribution of model
parameter values. Posterior probability distribution of model parameters can then be inferred from
combining such prior knowledge about the model and its parameters with the likelihood function (see
Bayes theorem and section 2 for more details).
Historically, the successful application of Bayesian inference for stochastic generative models with
realistic datasets has been hindered by the lack of eï¬ƒcient sampling techniques for posterior model
trajectories and for the computationally expensive evaluation of the likelihood (as a high-dimensional
integration).
The development of methodologies to address such challenges has been an active
research topic in recent years. Relevant methods include Particle Filter (PF) estimation (with op-
tional trajectory "smoothing" techniques [DJ09]) coupled with Markov Chain Monte Carlo (MCMC)
sampling - also known as Particle Markov Chain Monte Carlo (PMCMC) [ADH10], Gibbs sampling
- including Conditional Ornstein-Uhlenbeck Sampling (COUS) [RM09], the Approximate Bayesian
Computation (ABC) methodologies such as Simulated Annealing ABC (SABC) [AKS15], Hamilto-
nian Monte Carlo [AUS16], and stochastic variational (Bayesian) inference (SVI) methods [HBWP13].
In recent years, the number of computational inference frameworks implementing the above-
mentioned methodologies to study not only deterministic but also stochastic models has been growing
considerably. An attempt to provide a thorough review of those suites would certainly fall short, unless
carried out as a dedicated review, and is therefore beyond the scope of this contribution. Instead, we
brieï¬‚y mention the existing approaches that we are aware of to provide a context and to highlight their
speciï¬cities, referring the reader to the individual articles. In particular, the ï¬rst table overviews UQ
suites targeted at "static" stochastic models, where uncertainty is speciï¬ed by means of hierarchical
Bayesian networks, or incorporated into boundary (and initial) conditions or forcing terms:
Name
Language
Type
Type
Methodologies
Reference
BUGS
R/SAS
partial
specialized
Gibbs sampler
[LSTB09]
JAGS
Python/R
partial
specialized
Gibbs sampler
[Plu04]
MUQ
C++
partial
framework
optimization and UQ
[PCDM14]
Pest
proprietary
partial
program
optimization and UQ
[DMRT14]
STAN
Python
partial
framework
MCMC and HMC
[CGH+17]
emcee
Python
partial
specialized
EMCEE sampler
[FMHLG13]
PyMC3
Python
partial
framework
optimization and UQ
[SWF16]
UQpy
Python
partial
framework
optimization and UQ
[uqp20]
SPOTPY
Python
parallel
framework
optimization and UQ
[HKCCB15]
P4U
C/C++
parallel
framework
optimization and UQ
[HAPK15]
Dakota
C++
parallel
framework
optimization and UQ
[ABD+09]
Korali
Python/C++
parallel
framework
optimization and UQ
[WME+20]
The second table overviews UQ suites additionally supporting more complex "dynamic" stochastic
models (e.g. driven by SDEs and/or SPDEs):
2

Name
Language
Type
Type
Methodologies
Reference
PPF
Java
partial
specialized
Particle Filtering (PF)
[DSN+14]
timedeppar
R
partial
specialized
COUS
[RM09]
EasyABC
R
partial
specialized
ABC methods including SABC
[eas20]
pyro
Python
partial
framework
deep learning, UQ, SVI
[BCJ+18]
ABCpy
Python
parallel
specialized
ABC-type samplers
[DSU+17]
LibBi
C++
parallel
specialized
Particle Filtering (PF)
[Mur13]
PDAF
Fortran
parallel
specialized
Kalman/Particle Filtering (K/PF)
[NHS05]
SPUX
Python
parallel
framework
EMCEE, PF, PMCMC, SABC
[current]
In both tables, "partial" indicates only partial parallelization - either only the UQ algorithms (usually
only the outer loops over independent sampling tasks) are parallelized (manually or natively) or only
parallel external user applications are supported as models, in addition to standard serial models.
Full support (at the time of writing of this manuscript) for hierarchical parallelization of all, possibly
nested, algorithms (sampler, aggregator for multiple datasets, marginal likelihood estimator, model)
is indicated by plain "parallel".
As evinced by the large list of available frameworks, Bayesian inference is a ï¬eld that evolves fast,
especially when considering stochastic models.
Despite the strong commonalities in the naming,
the target applications of most suites fall into several insigniï¬cantly overlapping problem classes,
hindering the possibility of a direct comparison.
All of the established uncertainty quantiï¬cation
suites listed above provide users access to very sophisticated methodologies and are of great value
to the scientiï¬c community; however, all of them also have one or several shortcomings. Possible
deï¬ƒciencies include no support for "dynamic" stochastic models or parallel models, limited choice of
inference algorithms (for instance, either only Markov- or ABC-type), and user interfaces based on a
rather technical programming language (C/C++/Fortran). Unfortunately, among all suites reviewed
here and actually supporting full parallelism and "dynamic" stochastic models, none implement both
classes of inference algorithms (MCMC-type and ABC-type) and, at the same time, provide an
interface in high-level programming language (e.g. Python) for both programming use cases: coupling
a userâ€™s model and implementing a new inference algorithm. In addition, since plain embarrassingly
parallel Bayesian inference algorithms are being outcompeted by more eï¬ƒcient and adaptive, but also
more communication intensive methodologies, modern computing inference suites are required to
evolve to incorporate and oï¬€er these new functionalities despite their higher algorithmic complexity.
In contrast to the well-established suites for "static" stochastic models, a niche for UQ suites with a
focus on "dynamic" stochastic models, easy user interface, support for diï¬€erent classes of inference
methodologies, and full parallelization was so far relatively empty. In other words, the above overview
provides motivation and sets speciï¬c goals for any new uncertainty quantiï¬cation framework.
With the SPUX framework we aim to oï¬€er a framework that is open to a large class of model struc-
tures, and does not pose any limitations to the programming language. For instance, models can be
serial or parallel, and can be written in any language (e.g. Python, R, Julia, C/C++, Fortran, Java),
or can be available just as a binary executable. We choose Python as programming language for
SPUX as it is simple, popular, ï¬‚exible, and yet suited to exploit modern HPC architectures by means
of, for example, the mpi4py package [DPKC11]. More speciï¬cally, our framework mitigates high
computational costs by adaptively distributing model evaluations (for diï¬€erent parameters, data-sets,
and stochastic trajectories) over multiple computational units in a parallel compute environment. It
does so according to a multilevel parallel programming approach, which allows SPUX to overcome
the standard paradigm of map-reduce workï¬‚ows in favor of a more ï¬‚exible design tailored towards
algorithmic eï¬ƒciency, as it is suggested in a recent review [LLM19]. Indeed, the ï¬‚exible parallelization
paradigm used in SPUX is based on a continuous management of multiple parallel workers, while their
internal states are maintained remotely to signiï¬cantly improve the eï¬ƒciency of complex algorithms,
such as the Particle Filtering method.
SPUX can scale eï¬€ortlessly from laptops to large parallel
compute clusters, where it is particularly suited. SPUX already natively supports multiple inference
approaches, namely, Aï¬ƒne Invariant Markov chain Monte Carlo Ensemble with or without Particle
Filtering (with memory-eï¬ƒcient "rejection" particle smoothing [JMR15]), Simulated Annealing ABC,
3

and also standard Metropolis-Hastings MCMC. To the best of our knowledge, SPUX might be the
ï¬rst uncertainty quantiï¬cation framework that gathers all these capabilities in a single implemen-
tation. Finally, we want to mention that the open source nature of the SPUX framework and the
explicit description of the hierarchical parallel SPUX executors should allow to greatly simplify any ad-
ditional implementation and usage of other (existing or future) numerical inference and optimization
algorithms for deployment on parallel clusters.
The scope of the manuscript is to present the most recent version (1.0) of SPUX â€“ a prototype of
which was already introduced in the earlier publication [Å K17]. Theory and numerical methods for
Bayesian inference are brieï¬‚y reviewed in section 2. The purpose, design speciï¬cation, and available
modular components of SPUX are introduced in section 3, while section 4 showcases the framework
for a simple example model. A detailed overview of the most common use case â€“ coupling of a
scientiï¬c application to the SPUX framework, including a novel proposed adaptive sampling strategy,
is provided in section 5. Finally, section 6 describes the design of the SPUX framework based on the
parallel SPUX executors, and the outlook to future developments is provided in section 7.
2. Mathematical concepts and numerical algorithms
In this section, we start with a brief review of the mathematical concepts for the underlying scientiï¬c
problem addressed by our framework. In particular, we introduce generic and hidden Markov models
and Bayesian inference for them, followed by several widely used numerical techniques, and a brief
summary for subsequent uncertainty propagation (forecasting) of future model predictions.
2.1. Generic and hidden-Markov (state-space) models
Within the scope of this uncertainty quantiï¬cation framework, we will consider two classes of predictive
models: a wide class of "generic" models and a specialized class of hidden-Markov models.
state ğ‘šğ‘ ğ‘~ ğ‘€ğ‘ ğ‘(ğœƒ)
state ğ‘šğ‘¡0 âˆ¼ğ‘€ğ‘¡0(ğœƒ)
state ğ‘šğ‘ 1~ ğ‘€ğ‘ 1(ğœƒ)
ğ‘¡= ğ‘ 1
initial ğ‘€ğ‘¡0
parameters ğœƒ
output ğ‘¦ğ‘ 1 = â„(ğ‘šğ‘ 1, ğœƒ)
observation ğ‘œğ‘ 1~ ğ’ª(â‹…|ğ‘¦ğ‘ 1, ğœƒ)
ğ‘¡= ğ‘ ğ‘
output ğ‘¦ğ‘ ğ‘= â„(ğ‘šğ‘ ğ‘, ğœƒ)
observation ğ‘œğ‘ ğ‘~ ğ’ª(â‹…|ğ‘¦ğ‘ ğ‘, ğœƒ)
state ğ‘š~ ğ‘€(ğœƒ)
parameters ğœƒ
output ğ‘¦= â„(ğ‘š, ğœƒ)
observation ğ‘œ~ ğ’ª(â‹…|ğ‘¦, ğœƒ)
Figure 1: Scheme of a generic model (left) and a hidden-Markov model (right), mapping parameters
Î¸ to the state m, output y and observation o of the model M.
In a generic model M, a set of model parameters within a vector Î¸ is mapped to the model
"prediction", as depicted in the left part of Figure 1. We further categorize such model prediction into
the full model "state" m = M(Î¸), and its hidden part (deï¬ned by function h, for instance, extracting
only surface values from a three-dimensional lake model or accummulating only the number of adult
individuals in an ecological community) as model "output" y = h(m, Î¸). If the model M is stochastic
(e.g. driven by a stochastic process and hence attaining a non-unique state m) then, given a suitable
probability measure (denoted by P), its state is characterized by a probability distribution m âˆ¼M(Î¸),
as a shorthand notation for state m having a probability P(m|Î¸, M). The ensemble of all possible
"predictions" of such stochastic models are sometimes also referred to as "trajectories".
Many realistic models have an explicit temporal dimension (denoted in this manuscript by time t)
as depicted in the right part of Figure 1. In such case, for each time t, we denote the model by Mt,
a speciï¬c model state by mt, and the model output as yt = h(mt, Î¸). A stochastic time-dependent
model M is called a hidden-Markov model, if, for any increasing sequence of times s1 < Â· Â· Â· < sN,
4

its corresponding states mt âˆ¼Mt(Î¸) (but not necessarily its outputs yt) satisfy the Markov property
for all Î¸ and all 1 â‰¤k â‰¤N:
P(msk|mskâˆ’1, . . . , ms1, Î¸, M) = P(msk|mskâˆ’1, Î¸, M).
(1)
In a realistic scenario (to be modeled by model M), the value of the output y, which we could
refert to as the "exact" or "true" output, is often not measured completely accurately during the
observation process (independentent of a chosen model type). In particular, the corresponding data
"observations" o (see Figure 1) are instead assumed to follow a probabilistic distribution O(Â·|y, Î¸),
sometimes referred to as the observational error model, potentially also depending on some uncertain
parameters, included within the same vector Î¸ for simplicity and brevity of the exposition.
For time-dependent models, the corresponding data observations ot at "snapshots" t = s1, . . . , s Â¯
N
are assumed to be mutually independent and each follow a given O(Â·|ysn, Î¸) distribution.
Con-
sequently, the entire observation sequence (os1, . . . , osN ) follows a tensorized distribution with a
shorthand notation O(Â·|y, Î¸) = âŠ—N
n=1O(Â·|ysn, Î¸). In the following we refer to the observational error
model simply by "error".
Finally, hierarchical Bayesian networks (as examples of "static" probabilistic models introduced in
section 1) are supported as distributions (see subsection 5.6) for observational error and priors of
model parameters and intial model states (see subsection 2.2), are hence are not incorporated among
the "model" concept whithin manuscript.
2.2. Bayesian inference
Bayesian inference [GCS+14] can be used for statistical quantiï¬cation (including uncertainties) of
model parameters and (past, present and future) model states by conditioning the corresponding
prior distributions on the data to get the corresponding posterior distributions. In particular, for a
given model M mapping the parameters vector Î¸ to (possibly probabilistic) model state m âˆ¼M(Î¸),
the so-called likelihood L(D|Î¸, M) = P(D|Î¸, M) of the model M deï¬nes a probability distribution of
observations D for given model parameter values Î¸. In addition to the likelihood, initial information
about parameters Î¸ is described probabilistically by the so-called prior distribution Ï€(Î¸|M) = P(Î¸|M).
This prior knowledge on the model and its parameters is combined with the observed data D via the
likelihood L(D|Î¸, M) to obtain the posterior distribution P(Î¸|D, M) of model parameters Î¸:
P(Î¸|D, M) = L(D|Î¸, M)Ï€(Î¸|M)
P(D|M)
âˆL(D|Î¸, M)Ï€(Î¸|M),
(2)
where the Bayesian model evidence term P(D|M), useful for model selection, is independent of the
parameters Î¸.
For a deterministic model M, model output y = h(m, Î¸) = h(M(Î¸), Î¸) can be obtained for
any arbitrary Î¸, and hence the likelihood can be evaluated explicitly by applying the error, i.e.
L(D|Î¸, M) = O(D|y, Î¸).
In addition to the posterior distribution P(Î¸|D, M) of model parame-
ters Î¸ given by (2), the posterior distribution P(m|D, M) of model states m is given directly by
propagating P(Î¸|D, M) through model M using the procedures described in later sections.
For a stochastic model M, a conditional (on Î¸) prior distribution Ï€(m|Î¸, M) of the model states
m is also required (for simplicity, the same notation Ï€ is used for the prior distributions of model
parameters Î¸ and of model states m).
For instance, in a time-dependent stochastic model with
state mt âˆ¼Mt(Î¸), a prior distribution Mt0(Î¸) of the initial model state mt0 needs to be speciï¬ed
(possibly conditional on Î¸), and then the prior distribution of the later model states mt at t > t0
is determined by propagating Mt0(Î¸) through model M.
Given Ï€(m|Î¸, M), the conditional (on
Î¸) posterior distribution P(m|Î¸, D, M) of the (stochastic) model state m can be inferred jointly
with the posterior distribution P(Î¸|D, M) of model parameters Î¸ by evaluating their joint posterior
5

P(Î¸, m|D, M):
P(Î¸, m|D, M) = P(D|Î¸, m, M)Ï€(Î¸, m|M)
P(D|M)
âˆP(D|Î¸, m, M)Ï€(Î¸|M)Ï€(m|Î¸, M).
(3)
Note, that marginalization of (3) over stochastic model states m recovers (2), where the evaluation
of the likelihood L(D|Î¸, M) entails a marginalization over all possible model states m âˆ¼M(Î¸):
L(D|Î¸, M) =
Z
P(D|Î¸, m, M)Ï€(m|Î¸, M)dm.
(4)
In the remaining of this manuscript the dependence of L(D|Î¸, M) on prior model states distribution
Ï€(m|Î¸, M) will be understood implicitly via the dependency on model M which is assumed to provide
a prior distribution Mt0(Î¸) for the initial model state mt0. The information of the prior distribution for
later model states Ï€(mt>t0|Î¸, M) is usually incorporated as a speciï¬c evolution structure within the
model M and hence is also implicitly taken into account as a dependency for likelihood L(D|Î¸, M).
2.3. Numerical methods for Bayesian inference
Usually, Bayesian inference cannot be solved analytically for posteriors P(Î¸|D, M), and in the case
of stochastic model M, usually not even for likelihood L(D|Î¸, M) in Equation 4 and hence also not
for posterior of model states P(m|D, M). Therefore, numerical methodologies have been developed
to sample from the posterior distribution of model parameters Î¸ (and states m, if the model is
stochastic), obtained by running numerous corresponding simulations of the model M.
Existing
non-intrusive methods include the Metropolis or Metropolis-Hastings Markov chain Monte Carlo
(Markov-type for short) [GL06, GCS+14, Has70, MRRT53], Gibbs-type samplers for modifying one
parameter at a time [RM09], and the Approximate Bayesian Computation (ABC-type for short)
[AKS15].
Alterantive partially intrusive (but usually faster) methods include Hamiltonian Monte
Carlo (HMC-type for short) and Variational Bayesian Inference (SVI-type for short), based on exact
analytical solution to an approximation of the posterior. Sampling of posterior model states m in
stochastic models can be achieved, for instance, using Conditional Ornstein-Uhlenbeck Sampling
COUS [RM09] within Gibbs-type samplers, Particle Filtering [ADH10] within Markov-type samplers,
or by recasting model M to consider all its states m as parameters as well [AUS16] in HMC. ABC-
type methods require minimal model structure restrictions and are able to reliably sample from model
parameters posterior P(Î¸, |D, M), but are often very ineï¬ƒcient in sampling posterior of model states
P(m|Î¸, D, M), since the high-dimensional model output y is usually compressed to a low-dimensional
suï¬ƒcient summary statistic S(y). HMC and Variational Inference methodologies usually have the
best performance, but are intrusive (problem reformulations and/or derivatives are required). Next,
we brieï¬‚y describe Markov-type and ABC-type samplers, depicted by simpliï¬ed algorithm ï¬‚owcharts
in Figure 2 and already available within the SPUX framework.
2.3.1. Markov-type sampling
In the Markov-type sampling, samples from the posterior distribution of model parameters are gen-
erated iteratively. In each iteration, model parameters are proposed, for which the prior and the
likelihood are evaluated (up to an arbitrary factor) by considering model output (if needed). These
are then either accepted or rejected (in the latter case, the parameters from the previoius step are
kept). Acceptance or rejection is based on the ratio of current and previous posterior density estimates
(i.e. the product of likelihood and prior as evinced from Bayes theorem). In the ABC-type sampling,
the initial set of model parameters is drawn from the prior distribution. Then, an iterative procedure
consist of multiple tolerance steps (converging to zero), evaluating the distance metric between the
model output and the observations (data), re-drawing a subset of the model parameters and accepting
part of them based on the distance and the adjusted tolerance, this way gradually tuning the initial
6

Sampler
set initial (e.g. from prior ğœ‹) 
batch of parameters ğœƒ
if sampler is not converged to posterior
PF
for all observations at times ğ‘ 1, â€¦ , ğ‘ ğ‘:
evaluate obs. errors 
ğ’ªğ‘›
ğ‘= ğ’ªğ·ğ‘ ğ‘›|ğ‘¦ğ‘ ğ‘›
ğ‘, ğœƒ
(Markov)
(ABC)
compute likelihood ğ¿
of observed dataset ğ·
given parameters ğœƒ:
ğ‘š= ğ‘€ğœƒâ†’ğ¿= ğ’ªğ·|ğ‘¦, ğœƒ
accept or reject
ğœƒbased on ğ¿and ğœ‹
compute distance ğœŒ
between dataset ğ·
and model output ğ‘¦
of trajectory ğ‘šâˆ¼ğ‘€(ğœƒ)
resample ğœƒbased on 
ğœŒ, tolerance, and ğœ‹
initialize P trajectories
ğ‘šğ‘ 0
ğ‘=1,â€¦,ğ‘ƒâˆ¼ğ‘€ğ‘¡0(ğœƒ)
estimate likelihood
ğ¿â‰ˆà·‘
ğ‘›=1
ğ‘
1
ğ‘ƒà·
ğ‘=1
ğ‘ƒ
ğ’ªğ‘›
ğ‘
propose new ğœƒsamples
adjust tolerance
resample trajectories 
w.r.t. normalized ğ’ªğ‘›
ğ‘
evolve P trajectories
ğ‘šğ‘ ğ‘›
ğ‘âˆ¼â„™ğ‘›(â‹…|ğ‘šğ‘ ğ‘›âˆ’1
ğ‘
, ğœƒ, ğ‘€)
ğ‘šâˆ¼ğ‘€(ğœƒ)
Figure 2: Left: a simpliï¬ed ï¬‚owchart of the numerical posterior sampling algorithms of the Markov-
and ABC-types. Right: a simpliï¬ed ï¬‚owchart of the Particle Filter (PF) algorithm sampling
posterior trajectories (states) of a stachastic model by ï¬ltering them w.r.t. dataset and the
observation error model and estimating the likelihood deï¬ned as an integral in Equation 4.
Note, that ABC-type samplers estimate posterior of model parameters (but not states).
prior model parameter samples to the posterior model parameter samples.
2.3.2. Posterior trajectories sampling and likelihood estimation for stochastic models
For stochastic models, in addition to posterior distribution of the model parameters Î¸, sampling of
the posterior distribution of model states m is also required. In particular, the estimation of the
(marginalized) likelihood (4) required in the Markov-type samplers is most often estimated numeri-
cally. Nonlinear ï¬ltering numerical schemes, such as Particle Filter (PF) with or without smoothing
(also known as Particle Markov Chain Monte Carlo (PMCMC) technique [ADH10]) or a (Seam-
less) multi-level (Ensemble Transform) Particle Filter ((S)ML(ET)PF) [GC17] can be employed. Any
Markov-type sampler can be combined with the (ML)PF method for likelihood estimations, where
the hidden-Markov structure of the underlying stochastic model Mt is exploited for eï¬ƒcient marginal
likelihood approximations using time-series observations [ADH10, KR17]. In particular, for observa-
tions D consisting of time-series data D = {Dsn : n = 1, . . . , N} at time snapshots t = s1, . . . , sN,
the marginal likelihood in (4) can be rewritten (using the Markov structure, see [KR17]) as
L(D|Î¸, M) =
Z
Ï€ (ms0|Î¸, M)
N
Y
n=1
Pn
 msn|msnâˆ’1, Î¸, M

O(Dsn|ysn, Î¸)dms0 . . . dmsn.
(5)
Here, for given parameters Î¸, probability distributions Pn(msn|msnâˆ’1, Î¸) characterize random model
state vector msn given previous state msnâˆ’1, representing propagation of a given model state
msnâˆ’1 to the next state msn, The observational likelihood is evaluated using the (abbreviated)
error On(D|y, Î¸) = O(Dsn|ysn, Î¸) for model output ysn = h(msn, Î¸) and provides a probabilistic
model for the data observation process [KR17]. The PMCMC algorithm [ADH10, KR17] uses PF to
provide an unbiased statistical estimate of the proposal parameter marginal likelihood L(D|Î¸, M) with
structure given in equation 5. As depicted in Figure 2 (right), the numeric approximation of equation
5 involves sampling model trajectories ("particles") in terms of model states mp (with p = 1, . . . , P)
of the underlying model M with parameters Î¸. At each measurement time sn in the observations
time series, model simulations are paused and all particles are re-sampled (bootstrapped) according
to their (abbreviated) observational likelihoods Op
n(D|Î¸) = On(D|yp, Î¸) with yp
sn = h(mp
sn, Î¸). Such
periodic re-sampling increases algorithmic complexity due to the required destruction and replication
of existing particles, however, provides an eï¬ƒcient way of sampling "intermediate" posterior model
7

states (i.e. P(msn|Î¸, Ds1,...,sn, M) conditioned only on the partial dataset Ds1,...,sn up to the ï¬ltering
time sn). At the end of the PF, an unbiased estimate Ë†L(D|Î¸, M) of marginal likelihood L(D|Î¸, M)
as in equation 5 is evaluated by
L(D|Î¸, M) â‰ˆË†L(D|Î¸, M) =
N
Y
n=1
 
1
P
P
X
p=1
Op
n(D|y, Î¸)
!
=
N
Y
n=1
 
1
P
P
X
p=1
O(Dsn|yp
sn, Î¸)
!
.
(6)
In implementation, the evaluation fo Ë†L(D|Î¸, M) is performed in log-scale to mitigate numerical round-
oï¬€errors. The accuracy (namely, the variance) of the PF likelihood estimate clearly depends on the
number of used particles P. At the initial burn-in stage, the sampling acceptace procedure is often
dominated by the low likelihood values and hence the inaccuracy of the PF estimator is of secondary
importance. However, when sampler is converged towards the posterior, a larger number of particles
is prefered to ensure low relative approximation error in likelihood estimator. In supplementary Ap-
pendix G, we describe an adaptive procedure to automatically set the number of particles throughout
the sampling procedure based on the feedback containing historical estimator accuracies and param-
eters ï¬tness. To guarantee the convergence of the posterior, the particle adaptivity is "locked" after
the speciï¬ed period of sampling, which should be smaller or equal to the burn-in phase.
Additional methodologies, often refered to by "smoothing" [DJ09], are often employed to ob-
tain "smoothed" trajectories (conditioned on the entire dataset D) from posterior model states
P(m|Î¸, D, M). One way to achieve this is to resample already available "intermediate" posterior
trajectories (conditioned only on the partial dataset Ds1,...,sn). In particular, a simple yet very com-
putationally eï¬ƒcient (w.r.t. to both runtime and memory usage) "rejection" based smoothing (see
sections 2.3 and 5 [DJ09] and [JMR15]) sequentially iterates sn from s1 to sN to generate trajectories
from posteriors P(ms1, . . . , msn|Î¸, Ds1,...,sn, M) by an additional re-ï¬ltering step (after the main PF
ï¬lter step only for sn) for all preceeding snapshots s1, . . . , snâˆ’1 as well. Note, that such "rejection"
smoothing, unlike the PF ï¬lter for likelihood estimation, is prone to particle degeneracy (i.e. collapses
to a single trajectory for each sample of posterior parameter Î¸) for n â‰ªN [DJ09] and hence should
be used with care for non-illustrative purposes. More sophisticated (but also more computationally
expensive) techniques to prevent such particle degeneracy have been also reviewed in [DJ09].
2.3.3. Approximate Bayesian Computation
If the model M does not have a hidden-Markov structure, or if the error O(Â·|y, Î¸) is not explicitly
available as a probabilistic distribution (i.e. only a direct sampling of o = y = h(m, Î¸) by sampling m
from a probabilistic distribution M(Î¸) is possible), then the eï¬ƒcient numerical likelihood estimation
methods from subsubsection 2.3.2 cannot be directly applied. Note, that if data D consists only of a
single observation, there are obviously no eï¬ƒciency gains in using the (temporally) adaptive likelihood
estimation using Particle Filtering.
In such cases, a more general (but potentially less eï¬ƒcient due to the lack of adaptive temporal
ï¬ltering) Approximate Bayesian Computation (ABC) methodology [AKS15] can be used to sample
from the posterior distribution, without requiring the evaluation of the likelihood as in Equation 4.
In particular, in Boltzmann-type ABC methods, the joint posterior of model parameters and states is
approximated by the following family of distributions
PÏ„(Î¸, m|D, M) =
1
Z(Ï„)L(o|Î¸, M)Ï€(Î¸|M)eâˆ’Ï(D,o)
Ï„
where
o âˆ¼O(Â·|y, Î¸),
y = h(m, Î¸)
(7)
and where 1/Z(Ï„) is a normalization factor, Ï„ is the selected tolerance level, and Ï(D, o) measures
how close the observational dataset D is to the model observation o. If the error O is not available
explicitly, o = y = h(m, Î¸) is used instead. Given distance Ï, an initial tolerance level Ï„0, and an
initial distribution (usually prior Ï€(Î¸|M)), ABC-type samplers use a sequence of tolerances Ï„i â†’0
to generate a sequence of approximations to P(Î¸, m|D, M).
In the following, dependencies of a
8

particular distance value Ï(D, o) on the parameters Î¸ and the model M (and, if available, also of the
error O) are explicitly represented by using a supplementary notation Ï(D|Î¸, M) for Ï(D, o) with a
realization for o obtained from Î¸ and M as described above.
2.4. Uncertainty propagation and forecasting
For time-dependent models M(Î¸) = Mt(Î¸), once the joint posterior distribution P(Î¸, m[t0,T ](Î¸)|D, M)
of model parameters and states (up to the last snapshot time T = sN) is available, the distribution of
the "future" (forecast) model states P(m[T,âˆ)(Î¸)|D, M) is given by propagating P(Î¸, mT (Î¸)|D, M)
to the "future" times t > T using the model M. In practice, this is achieved with a Monte Carlo (MC)
sampler, where samples from P(m[T,TF ]|Î¸, D, M) are obtained by sampling from P(mT |Î¸, D, M) and
propagating them for t âˆˆ[T, TF ] for some future time TF > T.
Such an MC sampler can also be used for somewhat less diï¬ƒcult direct propagation of uncertainty
from prior distributions, when dataset is not available.
For very challenging priors, Markov-type
samplers from subsubsection 2.3.1 can be employed by using prior density instead of the likelihood.
3. SPUX framework
In this section we introduce the SPUX framework, focusing on the purpose and design speciï¬cation in
subsection 3.1, available modular components and built-in services in subsection 3.2, and paralleliza-
tion capabilities in subsection 3.3. A collection of continously updated current and past examples of
SPUX applications is illustrated at the end, in subsection 3.4.
3.1. SPUX purpose and design speciï¬cation
The purpose of the SPUX framework is to provide a seamless high-level interface to perform Bayesian
inference with a free choice of methodologies, algorithms, and computational environments.
To
achieve such ï¬‚exible customization and eï¬€ortless adaptivity, the SPUX framework harnesses the pow-
erful dynamic typing and runtime polymorphism oï¬€ered by the modern Python programming language,
which has recently become one of the most popular programming languages for scientiï¬c computing.
In essence, the SPUX framework is a collection of carefully selected, designed and optimized modu-
lar components. The modularity of SPUX components extends beyond the conventional restrictive
patterns and instead follows a "duck typing" design philosophy [duc18], namely, the suitability of an
object to perform a function is not determined by the objectâ€™s type, rather by the support of certain
methods and properties by the object itself. An overview of the basic key components currently
implemented in SPUX, each with the purpose of representing a particular mathematical concept as
introduced in section 2, together with available speciï¬c numerical methods for each component type,
is provided in the table below:
Concept
Component
Numerical method / algorithm
Description
P(Î¸, m|D, M)
Sampler
EMCEE, SABC, MCMC, MC
parameters sampling
L(D|Î¸, M)
Likelihood
Direct, PF
states sampling and likelihood
Ï(D|Î¸, M)
Distance
Norm, Regression
distance for ABC sampler
mt âˆ¼Mt(Î¸)
Model
Randomwalk, External, . . .
model for userâ€™s application
Î (Â·), Î£(Â·), etc.
Aggregator
Trajectories, Replicates
aggregator of components
3.2. SPUX component assignments and built-in services
All available SPUX components can be assigned to each other following the required dependencies.
An example scheme for such assignment of the components required by any Markov-type sampling
algorithm for Bayesian inference is provided in Figure 3 (the left part), together with the associated
9

mathematical objects introduced in section 2. The right part of this same scheme depicts an as-
signment of the components together with the associated mathematical objects for the a posteriori
forecast stage, introduced in subsection 2.4. Each SPUX component (for both stages: hindcast and
sampler
model
likelihood
ğœ‹(ğœƒ|ğ‘€)
ğ·
ğ’ª
â„™(ğœƒ|ğ·, ğ‘€)
â„™(ğ‘š[ğ‘¡0,ğ‘‡]|ğœƒ, ğ·, ğ‘€)
â€¦ infer uncertainty (hindcast)
propagate uncertainty (forecast)â€¦
sampler
model
â„™(ğœƒ|ğ·, ğ‘€)
â„™(ğ‘š[ğ‘‡,ğ‘‡ğ¹)|ğœƒ, ğ·, ğ‘€)
ğ‘€ğ‘‡ğœƒ= â„™(ğ‘šğ‘‡|ğœƒ, ğ·, ğ‘€)
ğ‘€ğ‘¡0(ğœƒ)
Figure 3: Component assignment and execution ï¬‚ow scheme for the SPUX framework using Markov-
type sampler and likelihood components for the inferenece (hindcast up to the last time
T in dataset D) and the resulting bootstrapped posterior parameters and model states
distributions for the forecast (uncertainty propagation beyond the last time T in dataset
D). Boxes with thin outlines indicate the associated mathematical objects introduced in
section 2, and boxes with thick outlines and thick arrows indicate SPUX components and
their internal assignments, respectively. Thin solid and dotted arrows represent component
inputs and outputs, respectively.
White background indicates built-in components and
inference outputs, whereas gray background indicates anticipated framework inputs.
forecast) is described in detail in section 4. Such modularity in SPUX allows easy implementation of
diï¬€erent numerical approaches for Bayesian inference. For instance, subsection 5.12 explain how to
use a structurally diï¬€erent ABC-type method (see subsubsection 2.3.3) within SPUX.
3.3. SPUX parallelization capabilities
One of the key advantage of SPUX is its very transparent yet very ï¬‚exible parallelization sub-system.
In particular, multiple parallel workers can be attached to each spux component listed in subsection 3.2
and depicted in Figure 3. An example of such parallelization scheme with only three components (sam-
pler, likelihood, and model) is provided in Figure 4, where sampler samples, likelihood particles, and
multiple tasks of the (optionally) parallel userâ€™s application/model (more details in subsection 5.10)
are distributed over available attached workers. Additional parallel components can be incorporated
when needed; for instance, the Replicates aggregator is designed to assimilate multiple independent
observational data sources in parallel. Note, that neither the model (i.e. the associated userâ€™s appli-
cation) nor any other SPUX component is strictly required to be parallelized; instead, any of these
components might also be serial (i.e. without any parallel workers attached). For a more detailed
description of parallel hierarchically stackable SPUX executors, refer to the technical section 6.
3.4. SPUX gallery
Inspired by the "Demo Data as Code" concept [Lim19], SPUX documentation website hosts a gallery
listing examples of userâ€™s applications, including source codes, authors, scientiï¬c ï¬elds, model pro-
gramming languages, used computational environment and conï¬guration, ï¬gures with representative
results, and associated scientiï¬c publications. At the time of this writing, example ï¬elds include
hydrology, aquatic ecology, urban hydrology, limnology, physics and data science, but the generality
of the SPUX framework does certainly extend beyond. In particular, SPUX is currently being actively
used for Bayesian inference in realistic individual-based modelling of riverine macro invertebrates,time-
dependent conceptual hydrological modeling of catchments,stochastic-input driven hydrological mod-
eling of the rainfall runoï¬€systems,and high resolution three-dimensional hydrological (and, eventually,
ecological) operational modeling of Lake Geneva.
10

sampler
parameters
parameters
parameters
parameters
likelihood
trajectory
trajectory
trajectory
trajectory
model
application
task
application
task
worker
worker
worker
worker
worker
worker
Figure 4: An example of parallelization capabilities for various compontents of the SPUX framework
described in subsection 3.2 and depicted in Figure 5. Three levels of hierarchical paralleliza-
tion are used here: parallelization over multiple model parameters samples of the EMCEE
sampler, over multiple particles of the PF likelihood, and over multiple independent tasks
of a parallel userâ€™s application. For each SPUX component (depicted using thick lines), the
thin solid arrows represent the required (possibly independent) tasks to be executed, the
thick solid arrows represent the internally called methods, and the dotted lines represent
the remaining ommited parts of the scheme.
4. SPUX framework showcase with a random walk model
This section guides the reader through an example model and usage pattern of SPUX. An overview of
diï¬€erent SPUX installation methods can be found on the SPUX documentation page, where we also
provide the links to access the source code, and a pre-conï¬gured SPUX Jupyter notebook (oï¬€ered on
a best-eï¬€ort basis only). After a brief overview of model types in subsection 4.1, an example model
of a random walk and its setup in SPUX are described in subsection 4.2. The serial model execution
procedure with a brief overview of resuts is described in subsection 4.3. In subsection 4.4, minor
auxiliary setup and execution steps needed to run SPUX in parallel on workstations and high perfor-
mance clusters are addressed. A detailed automatic PDF report compiling inference setup, results,
diagnostics and performance is introduced and interpreted in subsection 4.5. Finally, subsection 4.6
and subsection 4.7 describe procedures building upon already estimated posteriors for model parame-
ters and states: re-executing the best (or any other) model trajectory and forecasting to future times
or performing sequential Bayesian updating for a new dataset.
4.1. Deterministic and stochastic models
SPUX supports all types of models for Bayesian inference introduced in section 2: deterministic,
where model evaluation is uniquely determined by parameters Î¸, and stochastic, where model state
depends on random variable(s) (e.g. initial data) and/or is driven by stochastic process(es).
In Bayesian inference for deterministic models M, a simple Direct likelihood can be analytically
computed using the speciï¬ed error: L(D|Î¸, M) = O(D|y, Î¸) with y = h(m, Î¸) and m = M(Î¸). An
example of such model, Straightwalk, is available at examples/straightwalk.
For stochastic models M, in addition to uncertain model parameters Î¸, also the uncertain model
states m âˆ¼M(Î¸) need to be inferred. In such cases, the error O(D|y, Î¸) by itself is often not suï¬ƒcient
to analytically compute the likelihood L(D|Î¸, M) in (4) for given model parameters. Hence, additional
approximation techniques are required, as discussed in section 2. An example of a stochastic time-
independent model (left part of Figure 3) is provided in examples/gaussian-sabc. Another built-in
model in SPUX is a stochastic version of the Straightwalk model, called Randomwalk, where the
direction and size of each (time) step of the walker is a random variable. Since the SPUX framework
is tailored to Bayesian inference with stochastic time-dependent models, in the following section we
showcase the Randomwalk model. Files mentioned throughout next section are located in SPUX
repository at examples/randomwalk (assumed to be the current working directory).
11

4.2. Randomwalk model
The Randomwalk model describes a stochastic walk on a line (i.e. a set of real numbers). Its goal
is to provide the simplest possible conceptual model, which has just enough complexity to illustrate
most of the functionality in SPUX, while addressing the majority of requirements in the environmental
scientiï¬c modeling. Given a prescribed time step size âˆ†t (in seconds for example, to ï¬x the ideas) as a
numerical discretization parameter, together with an initial time t0 and an initial (possibly uncertain)
position mt0 âˆ¼Mt0 [m], the model iteratively takes random steps on a one-dimensional line every
âˆ†t time units. The direction and the size of each step depend on the time step size, on two models
parameters, the drift Âµ [m/s] and the volatility Ïƒ [msâˆ’1/2], and on an independent standard normal
random variable Nt:
mt+âˆ†t âˆ¼P(mt+âˆ†t|mt) = mt + âˆ†tÂµ +
âˆš
âˆ†tÏƒNt.
(8)
The Randomwalk model is a built-in SPUX model class with model initialization and evolution
(according to (8)) implemented in the corresponding class methods:
init(...)
initialize model with initial state mt0 and model parameters Î¸
run(...)
run model up to the speciï¬ed time t and return model prediction output yt
For implementation details of this demonstrational model, focused on simplicity and code readability
(no vectorization, explicit for-loops), refer to Listing 12 in Appendix D. In this particular model, the
initial argument contains the initial time t0 and the initial position mt0. Alternatively, initial
contents might as well be assigned in the model constructor directly, as for the time step size âˆ†t.
However, such explicit speciï¬cation of the model input (i.e., using initial) at the initialization
provides more ï¬‚exibility in the cases where the initial position Mt0 is uncertain (see subsection 4.3)
and/or when multiple observational datasets are available (see subsection 5.9). Finally, we assume
h(m, Î¸) = m, i.e. model state coincides with the model output.
4.3. Inference for Randomwalk model
As brieï¬‚y described in subsection 3.2, all available SPUX components can be assigned among each
other following the required dependencies. An extended version of the assignment scheme in Fig-
ure 3 for this example, together with the associated mathematical objects introduced in section 2, is
provided in Figure 5. The ï¬nal assigment assigns the top component, in this case (and also usually)
the sampler, to the built-in main SPUX framework component. The SPUX framework component
provides the hierarchical sandboxing (isolation to dedicated directories) and seeding (controlling the
independence of random number streams and ensuring reproducibility independently of the chosen
computational environment), manages runtime checkpointing, diagnostics, and framework setup op-
tions (see later subsection 5.1). In the following subsections we provide an elaborate description
of the remaining SPUX framework prerequisites, supplementary capabilities, and a general inference
execution workï¬‚ow, using the Randomwalk model (introduced in subsection 4.2) as the underlying
example.
4.3.1. SPUX conï¬guration prerequisites
To perform Bayesian inference for the given model, we also need to specify several conï¬guration ï¬les
for the remaining essential prerequisites (besides the sampler, the likelihood and the model). In
particular, for the Randomwalk example, these prerequisites are: the available observational data D,
a statistical error O(D|M(Î¸), Î¸), prior distributions Ï€(Î¸|M) for all the parameters that we intend
to infer, and the initial model state Mt0(Î¸). Note, that the parameters Î¸ contain both the parame-
ters of the model M and, if present, the hyper-parameters used for constructing the distributions of
the error O(Â·|ys, Î¸) and/or of the initial model states Mt0(Î¸). Prior initial model state distribution
Mt0(Î¸) is then attached to the speciï¬ed model component (representing M(Î¸)), error O(D|M(Î¸), Î¸)
12

sampler
spux
model
likelihood
prior
dataset
initial
error
application
parameters
sandbox
seed
aggregator
ğ‘šğ‘¡0 âˆ¼ğ‘€ğ‘¡0(ğœƒ)
ğ‘šğ‘ ğ‘›âˆ¼ğ‘€ğ‘ ğ‘›ğœƒ
ğ‘¦ğ‘ ğ‘›= â„(ğ‘šğ‘ ğ‘›, ğœƒ)
ğ‘€ğ‘¡0(ğœƒ)
parameters
time
ğ‘ ğ‘›
driver
ğœ‹(ğœƒ|ğ‘€)
ğ·
ğ’ª
ğ¿(ğ·|ğœƒ, ğ‘€)
â„™(ğœƒ|ğ·, ğ‘€)
ğœƒ
â„™(ğ·|ğ‘€)
posterior
evidence
â„™(ğ‘š|ğœƒ, ğ·, ğ‘€)
ensemble
report
output
ğœƒ
ğ‘€ğ‘¡ğœƒ
setup
ğ¿(ğ·|ğœƒ, ğ‘€)
Figure 5: Component assignment and execution ï¬‚ow scheme for the SPUX framework using Markov-
type sampler and likelihood components (an extension of the summary Figure 3). Boxes
with thin outlines indicate the associated mathematical objects introduced in section 2, and
boxes with thick outlines and thick arrows indicate SPUX components and their assignments
for internal evaluation, respectively.
For each SPUX component, thin solid and dotted
arrows represent component inputs and outputs, repspectively. White background indicates
built-in components and inference outputs, whereas gray background indicates anticipated
framework inputs. Dashed boxes indicate optional SPUX components for special sampler,
likelihood, model or dataset requirements and optional output or reporting capabilities.
is attached to likelihood component (representing L(D|Î¸, M)), and prior model parameters distri-
bution Ï€(Î¸|M) is attached to sampler component (representing P(Î¸, m|D, M)), see subsection 3.2
and Figure 5.
Marginal prior distributions Ï€Âµ, Ï€Ïƒ, Ï€Îµ for each parameter Î¸ = (Âµ, Ïƒ, Îµ) of the joint prior distribution
Ï€(Î¸|M), with Îµ being the standard deviation (as a hyper-parameter) of the (assumed) Gaussian error
(described below), are assumed to be independent and are given by (for plots, refer to Figure 6):
Ï€Âµ = U(âˆ’1, 1) [m/s],
Ï€Ïƒ = U(5, 15) [msâˆ’1/2],
Ï€Îµ = LN(1, 1) [m].
(9)
This multivariate prior distribution is deï¬ned in prior.py using a built-in Tensor SPUX distribution
(see subsection 5.6 for an overview of all SPUX distributions).
The Tensor combines multiple
statistically independent distributions (provided as a Python dictionary of the corresponding univariate
distributions - for instance, from the scipy.stats package) to a multivariate SPUX distribution.
The initial position mt0 of the randomwalk is also uncertain, and hence a prior distribution Mt0 for
the initial position mt0 at (deterministic) time t0 = 0 is deï¬ned in initial.py:
mt0 âˆ¼Mt0(Î¸) = N(10, 2) [m],
t0 âˆ¼Î´0 [s].
(10)
We note, that mt0 could alternatively be included as a model parameter in the prior deï¬ned in (9).
Actual dataset ï¬les are located in the datasets directory.
The default container for dataset
management in SPUX is a DataFrame of the pandas package [McK10], which is very similar to the
dataframe in the R programming language. The example script to load the dataset into a DataFrame,
is located in dataset.py. The dataset provides inaccurate observations Ds of the position ms at
(snapshot) times s = s1, . . . , sN. N/A values are allowed, however, no column (quantity of interest)
or row (snapshot) should contain only N/A values.
The inaccuracies in observational data are modeled with an error, which is a statistical distribution
of the observations (from the dataset) conditional on the speciï¬ed model output prediction (from
the simulation). In this example this distribution is assumed to be normally distributed with mean
equal to the position ms predicted by the model, and with standard deviation given by an additional
13

uncertain parameter Îµ used as a hyper-parameter:
Ds âˆ¼O(Â·|ys, Î¸) = N(ys, Îµ) [m].
(11)
All observations are assumed to be statistically independent.
The observational error is deï¬ned
in error.py as a function (or a collable object) which, given the model output prediction and
parameters, constructs the statistical distribution above as a SPUX Distribution instance.
An (optional) dictionary specifying units (as LaTeX-supported strings) for each parameter, model
output and time dimension is speciï¬ed in units.py. Its contents are used only in the SPUX report.
Within the context of this illustrative example, we also make use of the (optional) exact (loaded
in exact.py) parameter values available at datasets/exact.dat and the exact (synthetic) model
outputs (without the observational noise) available at datasets/predictions.dat.
4.3.2. SPUX conï¬guration and execution - user interface (UI)
The quickest way to run SPUX inference and post-processing (discussed in the following sections)
is using the SPUX UI conï¬guration ï¬le spux.cfg. There, arguments required for the construction,
conï¬guration, initialization, and execution of each SPUX component (either built-in or manually
imported) can be speciï¬ed, including any prerequisite script (see subsubsection 4.3.1) deï¬ning required
(mandatory or optional) options. All such components and options can be conï¬gured as depicted
in Listing 1, where an adaptive Particle Filter (PF with the default "rejection" smoothing) likelihood
with the speciï¬ed maximum number of particles is assigned to an Aï¬ƒne Invariant Ensemble sampler
(EMCEE) with the speciï¬ed number of concurrent chains.
Additional framework options (i.e. not
speciï¬c to any component) can also be speciï¬ed in spux.cfg to control various aspects of the SPUX
framework, as depicted in Listing 2. For instance, optional units of time, model parameters and
observations can be speciï¬ed (see also subsection 5.6). Built-in component and framework options
are described throughout the following sections with a summary available in subsection 5.1.
Listing 1: Example spux.cfg for components
model
Randomwalk
model.dt 0.1
likelihood PF
likelihood.particles
256
sampler
EMCEE
sampler.chains 32
sampler.samples
10000
Listing 2: Additional options in spux.cfg
prior
prior.py
error
error.py
dataset
dataset.py
initial
initial.py
burnin "half"
units
units.py
exact
exact.py
Using SPUX UI, inference and post-processing can be setup and performed by simply executing:
spux spux.cfg --execute --all
This automatically generates the required SPUX scripts (described in the following sections) and
automatically executes testing, synthesis, inference, reporting, and re-execution of the best trajectory.
The inference process can be terminated at any time, since the output is periodically checkpointed
(see subsection 5.1). Additional runtime arguments can be speciï¬ed to customize the inference:
--dry
"dry run" mode - inspect conï¬guration without actual sampling
--continue
continue the inference process starting from the latest checkpoint
--no-repro
disable reproducibility information (stored in randomwalk_reprozip.rpz)
The list of all built-in components and options is also retrievable by executing spux --help.
14

4.3.3. SPUX conï¬guration and execution - application programming interface (API)
The most ï¬‚exible way to conï¬gure SPUX is to use a conï¬guration script in which the prior, error
model, and dataset are explicitely imported and assigned (using the SPUX API) to the selected SPUX
components, such as likelihood (or distance) and sampler. A summary of an example configure.py
(excluding trivial module imports) is provided in Listing 3.
The mandatory spux.assign(...)
assigns all hierarchically ordered components to the SPUX framework.
Listing 3: Example configure.py (excluding imports)
model = Randomwalk (dt =0.1)
model.configure (initial)
likelihood = PF (particles =256)
likelihood.configure (dataset , error)
sampler = EMCEE (chains =32)
samper.configure (prior , samples =10000)
spux.configure (units)
spux.assign (model , likelihood , sampler)
Listing 4: Example infer.py
from
configure
import *
spux.setup ()
spux.init ()
sampler.init ()
sampler ()
spux.exit ()
The execution script infer.py provided in Listing 4 imports the components from configure.py,
setups the SPUX framework (mandatory spux.setup(...), see subsection 5.1), initializes the sam-
pler (mandatory sampler.init(...) for EMCEE), and performs the posterior sampling of the model
parameters and states for the speciï¬ed number of samples. The mandatory framework initialization
spux.init(...) and ï¬nalization spux.exit(...) methods manage the required computational
resources. For the EMCEE sampler, initial model parameters are drawn by default from the speciï¬ed
prior. The script can be executed by typing python infer.py in the console. Analogously to the
UI in subsubsection 4.3.2, the --dry, --continue and --no-repro runtime arguments can be used
for infer.py to enable "dry mode", continuation or disable reproducibility package.
4.3.4. SPUX results
The estimated marginal posteriors of model parameters are provided in Figure 6 and the estimated
marginal posteriors of model predictions are provided in Figure 7.
1.0
0.5
0.0
0.5
1.0
drift
0.0
0.5
1.0
1.5
2.0
2.5
pdf of drift
0
1
2
3
4
5
6
error
0
1
2
3
4
pdf of error
0.2
0.4
0.6
0.8
1.0
1.2
volatility
0.0
0.5
1.0
1.5
2.0
pdf of volatility
Figure 6: Marginal posterior (orange) and prior (blue) distributions of model parameters. The red
dashed line indicates the best found parameters values. The black dotted line represents
the exact parameter values.
For the inference results, the default burnin period (half of all samples) was selected to remove the
initial sampler bias. The adaptive number of particles described later in Appendix G is locked after
the speciï¬ed lock batches. By default the burnin is also set to this value to avoid any potential bias
due to the adaptivity process. For post-processing, only every thin-th sample (of each sampler chain)
is selected in order to obtain a sequence of statistically independent posterior samples. In particular,
15

0
5
10
15
20
25
time [s]
8
10
12
14
16
18
20
22
position [m]
best
exact model predictions
dataset
Figure 7: Posterior distribution of model predictions for the observational dataset. The shaded or-
ange regions indicate the log-density of the posterior model predictions distribution at the
respective time points, the red line represents the best found model prediction, the black
line represents the exact model prediction values.
the default "auto" value was used for the thin period, which uses the median of optimal thinning
periods obtained by estimating multivariate eï¬€ective posterior sample sizes for each sampler chain.
4.4. Parallel inference for Randomwalk model
With minimal eï¬€ort, the above example conï¬guration can be parallelized either for a local machine
or for a remote high performance computing (HPC) cluster. We emphasize, that no modiï¬cations
are needed for this particular "Randomwalk" model class. For HPC cluster, consider placing (see
subsection 5.1) the output directory in a parallel high performance "scratch" ï¬lesystem, if available.
For a more detailed discussion regarding models not written in pure Python, refer to section 5.
4.4.1. Attaching parallel workers
To enable parallel execution, a required number of parallel workers can be attached to each SPUX
component, as depicted in Figure 3. Examples are provided in Listing 5 (for the UI conï¬guration ï¬le
spux.cfg as in Listing 1) and in Listing 6 (for the API inference script infer.py as in Listing 4, in
this case these lines should be placed before the calls to framework setup and initialization).
Listing 5: Parallel workers in spux.cfg.
likelihood.workers 8
sampler.workers 16
Listing 6: Parallel workers in infer.py.
likelihood.attach (workers =8)
sampler.attach (workers =16)
A separate dedicated core is used for the manager process of each group of parallel workers. For an
advice regarding worker allocation strategies across multiple parallel executors, refer to Appendix B.
Advanced explicit attaching of SPUX executors to components is described in subsection 6.1.
4.4.2. Launching parallel SPUX
Assuming a library for the Message Passing Interface (MPI) [Mes15] is installed, parallel scripts need
to be launched through the Python mpi4py module. For the execution using the UI, specify --mpi
runtime argument. For infer.py using the API:
mpiexec -n 1 python -m mpi4py infer.py.
16

The required worker MPI processes will be spawned automatically (i.e. according to the resources
table).
For HPC systems not supporting dynamical spawning of new MPI processes, the required number
of MPI ranks (workers) needs to be explicitly speciï¬ed for mpiexec (after "-n"). The cumulative
number of required workers is indicated in the bottom right cell of the computational resources table
(see example in Table R9), which is printed to the console already during the "dry run" mode (for
which one core is suï¬ƒcient, i.e. no MPI is required). For convenience (e.g. to automate parallel job
submission process), this number is also written to the dedicated workers.txt ï¬le. Parallelization
can be temporarily disabled with --serial runtime argument, which ignores all workers attachments.
SPUX documentation outlines speciï¬cs regarding diï¬€erent MPI libraries and useful advice to address
any potential issues.
4.5. SPUX report
All tables and ï¬gures generated by the SPUX framework, such as previous Figure 6 and Figure 7,
are automatically included (see Appendix A for technical details) in a PDF report (A4 and "slides"
layouts), described in later sections. Such PDF report is also provided to support this section on
SPUX usage as Suplementary Material. All of the tables, ï¬gures, and even the conï¬guration scripts
(in section R8) referenced within this section are available in this report, hence it is strongly advisable
to have a separate copy of the Suplementary Material at hand.
Additionally, the "reproducible
package" spux.rpz is generated using the "reprozip" tool [CRSF16]. The SPUX report and the
spux.rpz provide the highest level of reproducibility (excluding containerization techniques) for an
inference or forecast run, independently of the chosen computational environment and/or hardware.
4.5.1. Conï¬guration and setup section
SPUX conï¬guration and setup is summarized in the ï¬rst section of the SPUX report (see section R1),
which is generated by executing the example report.py script and contains the following:
conï¬guration
Table R1
SPUX conï¬guration: component classes and their options
setup
Table R2
framework options from spux.setup(...), see subsection 5.1
units
Table R3
units for parameters, observations, and time
exact
Table R4
exact model parameters (if speciï¬ed)
evaluations
Table R5
total number of anticipated model evaluations across components
datasets
Figure R1
dataset(s) D and exact model predictions (if speciï¬ed)
errors
Figure R2
marginal error models O distributions for the speciï¬ed Î¸, y
prior
Figure R3
marginal prior distributions of model parameters - Ï€(Î¸, M)
initials
Figure R4
marginal prior distributions of initial model states - Mt0(Î¸)
In particular, for the Randomwalk example, at most 256 particles were used in the PF likelihood
(with adaptivity enabled, see Appendix G), and 32 chains were used in the EMCEE sampler. In total,
10â€™000 samples were requested, locking particle adaptivity after 75 sample batches (2â€™400 samples).
4.5.2. Results and diagnostics sections
To load and visualize inference results and diagnostics using the API, the report.py script is executed
with the additional --results runtime argument.
This report.py script uses built-in plotting
routines available in spux.reports.mpl module. The user can freely choose to use the reconstructed
results and diagnostics with other established data visualization libraries, including the specialized
pandas.plotting module and arviz [KCHM19] package.
The report script generates multiple
tables and ï¬gures of the results and diagnostics, and updates the SPUX report accordingly.
The inference results tables and ï¬gures included in section R2 of the SPUX report provide posterior
distributions for model parameters and predictions (i.e. model outputs y or even model states m):
17

best-parameters
Table R6
best found (e.g. maximum a posteriori) model parameters
parameters
Figure R5
marginal posteriors P(Î¸|D, M) of model parameters Î¸
predictions
Figure R6
marginal posteriors P(m|D, M) of model state m
dependencies2d
Figure R7
dependencies among pairs of posterior parameters Î¸ or states m
parameters2d
Figure R8
joint posterior for the most dependent model parameters Î¸ pair
In particular Figure R5 and Figure R6, are included here as Figure 6 and Figure 7, respectively.
Additional "diagnostics" tables and ï¬gures are included in section R3 of the SPUX report, providing
quality assesments of the inference results and the algorithmic technicalities for the Markov chain
sampling (EMCEE in this case), as well as the likelihood estimation (PF in this case):
status
Table R7
information about loaded SPUX status, see Appendix A
metrics
Table R8
metrics such as eï¬€ective sample size, thinning period, etc.
residuals
Figure R9
residuals (diï¬€erences between the dataset and outputs)
QQ
Figure R10
quantile-quantile comparison of residuals and O distributions
successfuls
Figure R11
tracking of the failed or skipped likelihood evaluations
samples
Figure R12
progress of model parameters sampling (including burnin)
samples-cutoï¬€
Figure R13
progress of model parameters sampling (excluding burnin)
acceptances
Figure R14
progress of the instantaneous sampler acceptance rate
resets
Figure R15
tracking likelihood re-estimations due to stuck chains
autocorrelations
Figure R16
autocorrelations of Markov chain parameters samples
likelihoods
Figure R17
progress of prior/likelihood/posterior (including burnin)
likelihoods-cutoï¬€
Figure R18
progress of prior/likelihood/posterior (excluding burnin)
ï¬tscores
Figure R19
progress of ï¬tscores as in subsection G.1 (including burnin)
accuracies
Figure R20
progress of likelihood accuracy as in subsection G.2
particles
Figure R21
progress of likelihood adaptivity as in subsection G.3
redraw
Figure R22
progress of the particle redraw fraction in PF (see Figure 2)
redraw-temporal
Figure R23
temporal progress of the redraw fraction in PF (see Figure 2)
From these diagnostic plots, also included in section R3 of the SPUX report, we determine that the
inference was relatively successful. In particular, the eï¬€ective sample size (computed using the esti-
mated autocorrelations as in Figure R16) is not much smaller than the actual request by the sampler,
the posterior residuals distribution is consistent with the theoretical distribution prescribed in the error
model, not many failed (NaN - where model did not return an output) or skipped (due to at least
one proposed parameter laying outside the support of its prior) likelihood evaluations, and converged
sampling of the parameters space due to the stationarity of the Markov chains. Additionally, the
average acceptance rate is relatively satisfactory (considering there were 3 model parameters), total
chain resets (likelihood re-estimations) due to stuck chains are negligible, and chain autocorrelations
lengths are relatively short. The adaptivity within the PF (described in Appendix G) is also successful:
ï¬tscores below the prescribed threshold, accuracies in the prescribed interval, the number of particles
steadily adapted within the speciï¬ed limits during the burnin stage, and the average redraw rate (the
fraction of unique particles in particle ï¬lter after each resampling) well above half the total number
of particles (indicating the absence of any critical degeneration, e.g., collapsing on a single particle,
of the PF resampling procedure).
Finally, various (approximate) criterions for model suitability [HWN18] are provided in Table R8:
with O
Bayesian Model Evidence (BME), Kashyap/Baysian Information Criterions (KIC/BIC)
w/o O
Bayesian Cross Validation (BCV), Deviance/Akaike Information Criterions (DIC/AIC)
Bayesian factors K(Ma, Mb) = P(D|Ma)/P(D|Mb) for models Ma/b from above metrics, determine
if model Ma (relative to model Mb) is strongly supported (K > 10) by the observational dataset(s).
4.5.3. Computational environment and performance sections
In section R4 of the SPUX report, the computational environment and attached computational re-
sources are provided:
18

environment
Table R9
computational environment (date, time, hardware, software versions)
resources
Table R10
required computational resources in terms of workers (e.g. cores)
In particular, for the Randomwalk example, we used 145 cores in total, with 16 parallel workers for
the EMCEE sampler, and 8 parallel workers for the PF likelihood.
In section R5, tables with measured runtimes of the entire inference run are included:
runtimes
Table R11
total inference runtimes (wall-clock and serial equivalent)
runtimes-latest
Table R12
latest inference runtimes (wall-clock and serial equivalent)
Optional computational performance plots for section R5 of the SPUX report, providing additional
insight into the computational and algorithmical eï¬ƒciency of the inference process, can be generated
by specifying --performance runtime argument for the report.py script. In particular, "runtimes"
of key SPUX routines are measured by default (see "performance" keyword for "informative" option in
subsection 5.1). This allows to generate the "runtimes" plot for the entire sampling progress or more
easily interpretable "runtime" plots for speciï¬c sampler batches. Optionally, if "timestamps" keyword
is requested for "informative" option, the respective "timestamps" plots can be generated, providing
an insight into the detailed SPUX performance proï¬les. Additionally, plots for parallel eï¬ƒciencies for
the entire sampling progress and strong scaling (multiple SPUX executions using diï¬€erent number of
parallel workers) can be generated (refer to the SPUX documentation). Since the current Randomwalk
example takes virtually no time to be executed, it is of little value to investigate the computational
performance of SPUX here; such detailed investigations will be included in the subsequent publications
focused on the application of the SPUX framework to realistic models and datasets.
It is, however, worthwhile to inspect the parallelization performance in terms of the measured
"traï¬ƒc" types and amounts within the adaptive PF resampling process, available in:
traï¬ƒc
Figure R24
progress of copied/moved particles and communication cost
traï¬ƒc-temporal
Figure R25
temporal progress of copied/moved particles and comm. cost
Note, that due to the communication-avoiding load balancing in the resampling parallelization routing
(see subsubsection 6.2.2), only a small fraction of all copied particles needs to be moved, and the
associated communication costs are even lower due to the exploitation of the node-level aï¬ƒnity of
the parallel workers (to optimize processing of the "stateï¬les", see subsubsection 5.5.1). The initial
period is dominated by the "move" traï¬ƒc, since the number of parallel workers equals the initial
number of particles, allowing only the exploitation of the node-level aï¬ƒnity (if "stateï¬les" are used).
4.6. Executing a selected (e.g. "best") model trajectory
If required, the best (or any other) model trajectory, corresponding to the best model parameters, and
the best model predictions (which, for stochastic models, are not determined only by the best model
parameters), can be explicitly executed using the auto-generated best.py script. Such a-posteriori
explicit execution of a speciï¬ed model trajectory allows to conï¬gure the model for richer output,
that is otherwise not required during the inference (i.e. for comparison with the datasets) or not
accessible using the functionality of the history option (see subsection 5.1). For instance, instead of
only the model output y, more of the hidden model state m could be returned as model predictions.
Additionally, an explicit trajectory directory is used for the modelâ€™s sandbox (see subsection 5.2)
instead of potentially inaccessible node-local ï¬lesystems (see subsubsection 5.3.5).
4.7. Forecast (uncertainty propagation) and sequential Bayesian updating
In many use cases, datasets could be structured into multiple time periods, allowing to perform the
Bayesian inference sequentially, and providing an optional future forecasts in-between such datasets.
Firstly, a forecast of the model predictions can be obtained by simply propagating the inferred
posterior distributions from a preceeding time period [t0, T], for which a dataset of observed data is
available, to a future time period [T, TF ], see subsection 2.4 and Figure 3. This can be achieved by
19

specifying, within the UI conï¬guration ï¬le of the SPUX framework, the location of the "past" inference
(with states=True, see subsection 5.1) root directory as pastdir and the list of "future" times as
timeset (see the example provided in Appendix C, Listing 9).
The UI automatically generates
the corresponding prior.py and initial.py scripts for the bootstrap distribution of samples from
posterior model parameters and the associated bootstrap distribution of samples from posterior model
predictions at the last snapshot of the dataset, respectively. These prior and initial prerequisites are
then assigned to an MC sampler, which is used to generate probabilistic (i.e. including uncertaitainty
quantiï¬cation) forecasts.
Additionally, if a validation dataset is also available (i.e. not used for the preceeding inference), it
can be used for the evaluation of the error in order to compute the predictive cross validation likeli-
hood or distance. For an example refer to examples/randomwalk-forecast. An analogous SPUX
conï¬guration could also be obtained (see subsection 2.4) by explicitly specifying prior distributions of
parameters and intial state instead of providing â€™pastdirâ€™.
Secondly, upon aquisition of an additional dataset (for a time period [Ti, Ti+1] following an already
inferred time period [Tiâˆ’1, Ti], as depicted in Figure 8), posterior distributions of model parameters
and ï¬nal model states at time Ti can be used as prior distributions of model parameters and initial
model state for the Bayesian inference within the succeeding time period [Ti, Ti+1], respectively. In
ğœ‹(ğœƒ|ğ‘€)
ğ·[ğ‘‡0,ğ‘‡1]
â„™[ğ‘‡0,ğ‘‡1](ğœƒ|ğ·, ğ‘€)
â„™(ğ‘š[ğ‘‡0,ğ‘‡1]|ğœƒ, ğ·, ğ‘€)
ğ‘€ğ‘‡1 ğœƒ= â„™(ğ‘šğ‘‡1|ğœƒ, ğ·, ğ‘€)
ğ‘€ğ‘‡0(ğœƒ)
ğ·[ğ‘‡1,ğ‘‡2]
dataset period
parameters
predictions
â„™[ğ‘‡0,ğ‘‡1](ğœƒ|ğ·, ğ‘€)
â„™(ğ‘š[ğ‘‡1,ğ‘‡2]|ğœƒ, ğ·, ğ‘€)
â„™[ğ‘‡0,ğ‘‡2](ğœƒ|ğ·, ğ‘€)
ğ‘‡0
ğ‘‡1
ğ‘‡2
Figure 8: Sequential Bayesian updating with SPUX for datasets split across multiple time periods.
Gray and white backgrounds indicate anticipated inputs and inferred outputs, respectively.
such a case, the location of the "past" inference and the additional dataset (for period [Ti, Ti+1])
need to be speciï¬ed; unless explicitly speciï¬ed otherwise, the error and units will be reused from the
conï¬guration of the speciï¬ed "past" inference. If model parameters are not expected to be inï¬‚uenced,
the model states for the next time period [Ti, Ti+1] can be inferred by using the PF likelihood while
keeping the model parameters distribution unchanged by using the MC sampler (see the example
provided in Appendix C (Listing 10) and examples/randomwalk-assimilate). Alternatively, if not
only the model state but also the model parameters need to be updated when processing the next
time period [Ti, Ti+1], a full sequential Bayesian update can be conï¬gured analogously to the example
provided in subsubsection 4.3.2, but with the pastdir speciï¬ed instead of the prior and initial.
For an example, please refer to examples/randomwalk-update.
5. SPUX framework usage and customization
This section starts with an overview of the available framework setup options in subsection 5.1 includ-
ing an overview of "sandboxing" strategies on subsection 5.2, and continues by describing key SPUX
customization guidelines for the most common necessities. Those are: how to couple an application
with the framework by deï¬ning a new model (subsection 5.3), including potential options to include
stochastic processes for model input/parameters (subsection 5.4); how to handle model state seri-
alization (subsection 5.5); how to specify a prior distribution for all (model and observational error)
parameters (subsection 5.6); how to deï¬ne an error for the available dataset(s) (subsection 5.7). In
addition, subsection 5.8 describes how optional "auxiliary" output and datasets, that are not in a
form of a pandas.DataFrame, can be incorporated into the model, error, and distribution classes.
Multiple independent datasets can be combined using the Replicates aggregator, introduced in
subsection 5.9. Instructions on possible built-in parallelization techniques for Python applications or,
alternatively, on executing existing parallel user applications within the SPUX model environment
20

are presented in subsection 5.10. Finally, subsection 5.11 provides some guidelines on advanced cus-
tomization options such as writing a custom SPUX component (e.g. sampler, aggregator, likelihood,
distance, etc.), with an example of SABC sampler described in subsection 5.12. SPUX documentation
might incorporate improvements made after the publication of this manuscript.
5.1. Options for framework setup, component conï¬guration and report
Multiple (non-mandatory) options to conï¬gure components, frameworkâ€™s "global" setup, and the
report can be speciï¬ed directly in UI conï¬guration ï¬le spux.cfg as indicated in Listing 1 and Listing 2.
In the following, we describe the corresponding API methods and list examples of such options.
In particular, an optional configure(...) method is available for each component, implementing
functionalities usually shared by all the components of a given component type. The following table
provides a brief overview of all available configure-options (excluding options already introduced in
the preceeding sections) for model, likelihood/distance, and sampler component types:
templatedir
None
directory with intial sandbox contents for the model
statefiles
None
sandbox ï¬les relevant to the model state (see subsubsection 5.5.1)
ignore
None
list of non-serializable model attribute names (see subsubsection 5.3.3)
timeset
4
integer: points in-between dataset snapshots; iterable: predictions times
auxset
None
auxiliary observational datasets (see subsection 5.8)
lock
None
batch index to lock samplerâ€™s feedback to likelihood or distance
An iterable (e.g. list or array) of times t0, . . . , t Â¯
N can be used for timeset to select corresponding
model outputs ytn for later post-processing, providing additional intermediate time points (among
dataset snapshots) for a larger temporal resolution (i.e. with Â¯N > N).
The framework itself can be customized by the optional spux.configure(...) (see Listing 3)
and the mandatory spux.setup(...) (see Listing 4) methods. The following basic arguments (with
default values and descriptions) are available:
seed
0
integer seed (for hierarchical seeding of RNG libraries)
verbosity
2
hierarchical verbosity level (integer) for SPUX components
informative
["performance"]
to save: "performance" "timestamps" "infos" "rejections"
sandboxdir
"sandbox"
directory for the "root" sandbox (see subsection 5.2)
trace
"none"
sandboxes to keep (if used): "none" "best" "posterior" "all"
outputdir
"output"
directory for SPUX output ï¬les (see Appendix A)
history
"none"
store "stateï¬les"/"auxiliary": "none" "best" "posterior" "all"
states
False
store ï¬nal model states for forecasting or sequential updating
checkpoint
600
minimal time period (in seconds) between checkpoints
Note, that including more keywords in informative will consequently also increase expected inference
runtime, required operational memory, and the total size of written output ï¬les (see Appendix A). To
keep diï¬€erent "stateï¬les" copies and archived auxiliary model predictions, a dictionary with respective
"statefiles" and "auxiliary" entries can be speciï¬ed for history. Refer to SPUX documen-
tation for the advanced options (redirect, cache, setupdir) and for the additional options of
functions within the test.py, synthesize.py, report.py, and best.py scripts.
5.2. Sandbox - ï¬lesystems and post-execution accessiblity
As indicated in subsection 5.1, the "root" sandbox directory sandboxdir contains nested sandboxes
for each (if used) batch, chain, replicate, and model (particle or trajectory). By default, the "root"
sandboxdir is placed in (node-local) fast virtual node-local RAM-based Linux ï¬lesystem called tmpfs
(by default mounted at /dev/shm) to avoid network and I/O overheads. If the amount of system
memory is a limiting factor, alternative (node-local) ï¬lesystems can be used to avoid network (but
not I/O) overhead. For inference runs on high performance computing clusters, if neither option
21

is possible, (shared) "scratch" ï¬le system can be used instead (with associated network and I/O
overheads).
If sandboxes located on node-local (not shared) ï¬lesystems are inaccessible and the
functionality of the history option is not suï¬ƒcient, the best (or any other) model trajectory can be
re-executed (even after inference) within a speciï¬ed accessible trajectorydir for modelâ€™s sandbox
(see subsection 4.6).
5.3. Adding a model
In the most common use case scenario of SPUX, a user will wrap an existing application as the SPUX
model either by conï¬guring an existing SPUX model or by implementing a new SPUX model as
Python class. To avoid confusion, we will refer to a userâ€™s existing application (in any programming
language) as the "application", and to the (built-in or custom) Python class coupling such application
to the SPUX framework as the "model". In this section we review available model testing routines
and discuss two scenarios in detail: using the built-in External model to manage the (appropriately
modiï¬ed) application and writing a new SPUX model class to explicitly wrap an (unmodiï¬ed) appli-
cation. In both cases, the incremental model execution must be possible, i.e. a corresponding output
is required for each speciï¬ed time from an increasing list of times.
5.3.1. Model testing and dataset synthesis
Automatically generated test.py (and synthesize.py) scripts (using the cleaned up SPUX UI
spux.cfg ï¬le to only deï¬ne the model component and its options) could be used to continuously
test the development of a new model class. In spux.cfg, model parameters can be speciï¬ed as the
parametersfile option, deï¬ning the path to a text ï¬le containing rows with parameter names and
values (separated by some white space), and an array of times (e.g. using utils.period(...)) for
model evaluation can be speciï¬ed as the snapshots option. The optional synthesize.py script uses
the speciï¬ed (or drawn from the prior) exact model parameters to generate exact model outputs and
selected observations (with error, if speciï¬ed) at the speciï¬ed snapshots, including the corresponding
exact.py and dataset.py scripts to load them (for instance, to generate the conï¬guration section
of the SPUX report). Such synthetically generated datasets provide an invaluable resource for making
sure the correctness of your implementation, especially because the posteriors for model parameters
(and outputs) obtained from the Bayesian inference can be compared to their exact values.
5.3.2. Using External model for userâ€™s application
The External SPUX model relies on an application execution command, with which users are already
familiar from using the console (shell), making this a good starting option for the ï¬rst coupling of the
application to the SPUX framework. In particular, the application execution command can be used to
conï¬gure SPUX with an External model, as indicated by examples in Listing 7 and Listing 8.
Listing 7: External model example in spux.cfg.
model
External
model.command r"./ myapp <TIME >"
Listing 8: External model example in infer.py.
command = r"./ myapp <TIME >"
model = External (command)
The External model automatically isolates the application to a unique sandbox directory, where
the corresponding initial model state is written to initial.txt ï¬le if the initial model state is
speciï¬ed. Subsequently, the command can be executed to evolve the current model state mtnâˆ’1 to the
next mtn by reading the automatically generated input ï¬les (parameters.txt for Î¸, time.txt for tn
and seed.txt for the seed), and writing output.txt (see subsubsection 5.3.8 for details). Optionally,
the contents of the corresponding ï¬les can be also passed to the application as runtime arguments
using <PARAMETERS>, <TIME> or <SEED> keywords (for substitution) within the application command.
An alternative "direct" mode of the External model is available (see later sections for reasoning) by
22

setting model attribute direct to True. In this mode, instead of executing the command sequentially
for each required time tn, the command is executed only once to evolve model from the initial state
mt0 to the ï¬nal state mT by reading automatically generated input ï¬les times.txt and seeds.txt
(or using corresponding <TIMES> and <SEEDS> keywords), and writing outputs.txt.
5.3.3. Implementing a new SPUX model class - execution control
The functionality of the simple External model might be insuï¬ƒcient; for instance, it might be
inadequate (binary executable), inconvenient (requires changes in application interface) or ineï¬ƒcient
(requires storing model state inbetween run(...) calls) to rely only on the application modiï¬cations.
A new SPUX model class can be written instead, allowing unrestricted capabilities for interfacing
the application to the SPUX model, possibly even without any modiï¬cations to the application itself.
Such model class can be speciï¬ed as the "model" component within respective SPUX conï¬guration
ï¬les in Listing 1 and Listing 3. New model classes need to be derived from the base Model class deï¬ned
in the spux.components.models.model module. The model execution ï¬‚ow scheme is provided in
Figure 9, where arguments and built-in internal variables (introduced in subsubsection 5.3.4) are
explicitly indicated for each model method.
init
seed
sandbox
run
run
run
save
state
load
exit
time
initial
parameters
sandbox
seed
time
time
seed
seed
output
templatedir
statefiles
statefiles
output
output
statefiles
statefiles
write
read
statefiles
Figure 9: Execution ï¬‚ow scheme for the SPUX model, controlling the userâ€™s application. The middle
row indicates order of model methods calls (with multiple run and save/load calls). The
top row indicates the arguments that are provided to these model methods, and the bottom
row indicates before which methods the built-in variables sandbox and seed are updated.
A new SPUX model class needs to have an appropriately implemented run(...) method:
run(self,time)
run model from current time (tnâˆ’1) until time tn, return model output ytn
In the method declaration above, self is a handle to the model class instance, time corresponds to
times in snapshots, timeset, dataset or auxset, and the model output (see subsubsection 5.3.8)
consists of a mandatory array of labelled values (and, if needed, an optional "auxiliary" object).
If run(...)
fails (invalid parameters, invalid trajectory evolution, etc.), nothing (None) can be
returned instead of raising an error, if the user would like the inference to continue. Optionally,
model initialization and ï¬nalization methods can be implemented:
init(self,initial,parameters)
initialize model with initial mt0 and parameters Î¸
exit(self)
ï¬nalize (cleanup) model after the last run(...) call
In some cases, performing the model evaluation m âˆ¼M(Î¸) for all timesteps in a single function
call (analogously to direct mode of the External model) instead of making incremental steps
mtn âˆ¼Pn(Â·|mtnâˆ’1, Î¸, M) for each time tn might be an easier way to wrap the user application
and/or a faster way to execute the model in cases where the incremental model execution is not
required (currently it is required only by the PF likelihood). This can be achieved by implementing a
custom __call__(...) method, returning model output for all times at once as a dataframe:
__call__(self,parameters,times)
given parameters and times, return model output y
23

The built-in implementation of __call__(...) relies on using init/run/exit methods and re-
turns model outputs dataframe as well associated info and (optional) timings dictionaries (see
Appendix A). Additionally, self.diagnostics attribute might be set (at runtime, by other compo-
nents) to a function returning a dictionary of model output (ytn at time tn) diagnostics (e.g. O(ytn|Î¸)):
diagnostics(time,prediction,parameters,rng)
return diagnostics of prediction ytn
The info in __call__(...) then needs to be updated with returned diagnostics (if not None).
The implementation of model methods could rely on direct imports of modules corresponding
to existing applications written in Python, could rely on customized versions of the corresponding
methods found in External or Randomwalk (or in the other models from the built-in examples), or
could be a combination thereof. Three choices are available to control the application execution:
basic
execute application command in a shell (an extension of the External model)
advanced
call application methods from Python using "drivers" (an eï¬ƒcient SPUXâ€™onic way)
custom
implement any custom interface between the model and the application
The "basic" execution control allows extending the capabilities of the External model (see List-
ing 11 in Appendix D), for instance, by storing model inputs in a diï¬€erent format, reading model
output from a custom output ï¬le, supporting parallel applications (see subsubsection 6.1.3), imple-
menting a custom init(...) method to account for the speciï¬ed initial model state.
The "advanced" and "custom" execution control allows the control of multiple execution stages
in the associated application directly from within the Python model methods, avoiding the unneces-
sary overhead of application initialization and ï¬nalization in-between consecutive calls of the modelâ€™s
run(...) method. The computational eï¬ƒciency gains are particularly large for a long time series
datasets, where run(...) needs to be called multiple times, and for models with small stochastic
volatility (including deterministic models), where model states ï¬ltering is infrequent (or absent) in-
between consecutive run(...) calls. In order to directly call routines within the userâ€™s non-Python
application, appropriate application-to-Python bindings can be used, with built-in examples including:
R
Python module "rpy2"
examples/runoff
Fortran
Python modules "f2py" or "ctypes" (C-ISO DLL)
examples/superflex
C/C++
Swig (swig.org) code wrapper
examples/hydro
Java
Python module "JPype1"
examples/IBM_2species
In addition to these examples, the most common practices of using the functionality of such Python
bindings for SPUX are combined into so-called "drivers" under spux/drivers directory. Note, that
loaded bindings or driver instances are usually non-serializable (see subsection 5.5), and hence their
names as model attributes (under self) need to be included in ignore, see subsection 5.1. Next,
we brieï¬‚y describe built-in framework functionalities that can be used in such a new model class.
5.3.4. Model scope attributes
Any model class instance has the following relevant internal attributes accessible in all methods:
self.sandbox()
path to an isolated sandbox directory (supports ï¬le name argument)
self.parameters
model parameters Î¸ (available if init(...) is not speciï¬ed)
self.seed()
list of integer seeds for each SPUX component (and iteration)
self.seed.one()
one integer seed, reduced from an array of hierarchical integer seeds
self.rng
numpy.random.RandomState as random_state for scipy.stats
self.verbosity
integer indicating verbosity level, e.g. for custom print(...) usage
self.print(string)
print string to standard output, taking into account the verbosity
self.replace(string)
substitute <PARAMETERS>, <SEED> and <TIME> keywords in string
self.shell(command)
execute the speciï¬ed command using shell in the sandbox directory
24

The sandboxing and seeding systems are described in more detail in the following sections. Sum-
mary implementations of methods for an external application model (External) and a Python
(Randomwalk) model are provided in Appendix D, Listing 11 and Listing 12, respectively. For an
alternative to self.shell(command) for parallel applications, see subsubsection 6.1.3.
5.3.5. Model sandbox
From within any method of the model, the path to the corresponding dedicated "local" sandbox can
be retrieved by executing self.sandbox(), which might be diï¬€erent for each run(...) call (e.g. if
PF likelihood is used). This is a path to a unique working directory for each model class instance,
isolating multiple models executed in parallel and preventing race conditions and conï¬‚icts for their
input/output ï¬les.
Any ï¬le access within the model class can be conveniently performed using
the ï¬le path returned by self.sandbox(filename). If a userâ€™s model requires certain common
ï¬les to be present in every "local" sandbox, a template sandbox directory templatedir can be
speciï¬ed in model.configure(...), see subsection 5.1 and Figure 9. The templatedir is best
placed in parallel high performance "scratch" ï¬lesystems, since contents of the template sandbox are
automatically copied (using eï¬ƒcient local caching) to each "local" sandbox.
5.3.6. Model seeding
Note, that self.seed(), self.seed.one() and self.rng can be diï¬€erent for the initialization call
init(...) and for each run(...) call. The reseed() method in the test.py script checks if the
model supports such frequent updates of the seeding and of the random number generator.
5.3.7. Initial model state (deterministic or stochastic)
Since the initial model state Mt0(Î¸) can depend on the model parameters, it is expected to be speciï¬ed
as a function initial(parameters), the return value of which is passed to modelâ€™s init(...) call.
Alternatively, initial(parameters) can return a prior SPUX distribution for the initial model state
Mt0(Î¸), a draw from which is passed to modelâ€™s init(...)
call.
In such case, the posterior
distribution of the initial model state is then also inferred.
5.3.8. Model output
The run(...) method of the model needs to return labelled model output ytn = h(mtn, Î¸) con-
structed by calling the built-in self.output(values,names,time) method. For the sake of sim-
plicity and to keep the amount of data manageable, only a list or an array of plain datatypes (e.g.
ï¬‚oats, integers, strings, etc.) is allowed to be speciï¬ed in the argument values, with corresponding
names in a list (or an array) of strings speciï¬ed in the argument names. Alternatively, values can
be a pandas.DataFrame with labels as index and values as the ï¬rst column. For inference, all
dataset (columns) labels must be present among the model output labels. For later reporting, the
(extended) model ouput can contain additional labelled quantities of interest from the full model state
mtn. Optional __call__(...) method of the model needs to return three objects: a dataframe for
predictions y, an info dictionary with supporting information (including diagnostics, if requested),
and timing information (or None, if not available). The predictions dataframe rows need to be
indexed by times and columns labeled by names - for examples, see Model and External model
classes. For the (extended) model output of a stochastic model, it could consist of multiple labelled
quantities to enable, for example, additional diagnostics plots to be included in the SPUX report (see
subsection 4.5):
predictions2d
summary of statistical dependencies among (extended) model output ytn pairs
prediction2d
joint posteriors for the most correlated (extended) model output ytn pair
25

In some complex models (for instance, in computational ï¬‚uid dynamics), even the output prediction
yt of the full state mt might consist of large multi-dimensional arrays (for instance, observed surface
values) instead of just a few scalar values. In order to beneï¬t from the built-in reporting capabilities
and at the same time have an eï¬ƒcient method for the evaluation of the error, a diverse handful of
important quantities of interest can be cherry-picked for the annotation and the (remaining or entire)
output prediction yt can be speciï¬ed as "auxiliary" (see subsection 5.8).
5.4. Processes (stochastic and deterministic)
Built-in processes for use within the SPUX model classes are available in spux.library.processes.
For instance, OrnsteinUhlenbeck (i.e. bounded standard Gaussian) process with a temporal correla-
tion length Ï„ is available, often used to facilitate inference of time-dependent input and/or parameters
for deterministic models [RM09]. In such case, the model input and/or parameters are stochastically
sampled from the process evaluate(time,rng) method with rng set to modelâ€™s self.rng.
5.5. Model state serialization
The "PF" likelihood estimator for stochastic models requires the userâ€™s model (and the underlying
application) to have the capability of cloning its state ms at any given time "snapshot" s available in
the speciï¬ed dataset. Additionally, the posterior forecast and sequential Bayesian updating described
in subsection 2.4 and subsection 4.7 also rely on loading the ï¬nal model state saved during the
preceding inference run. Such capabilities of the model are also tested by the automatically generated
test.py script mentioned earlier. In "clone" test, the script runs the speciï¬ed model up to the
speciï¬ed clone time and makes a clone of the original model by saving its state. Then, a second
model is created by loading the saved state of the original model and both models are run using
the same seed up to the speciï¬ed compare time. The outputs of both models must be identical
(up to numerical roundoï¬€errors). The "move" test checks if the model outputs are consistent in
case its sandbox is moved in-between consecutive model runs. Note, that a freshly cloned model,
resulting from the execution of the load method as depicted in Figure 9, does not execute the
init(...) method; in particular, any required corresponding application initialization procedures
need to be part of the load functionality.
Furthermore, application bindings or driver instances
(see subsubsection 5.3.3) are usually non-serializable and hence their names (if assigned as models
attributes) need to be speciï¬ed in ignore (see subsection 5.1).
In SPUX cloning is based on the concept of the model "state" serialization to a binary stream
(array). There are two potential sources for applicationâ€™s state information: the model class instance
and, if sandboxing is used, the contents of the associated sandbox directory.
5.5.1. Model state serialization using "stateï¬les"
If sandboxing is used and some ï¬les in sandboxes are relevant to the model state (often corresponding
to the "restart" or "pickup" ï¬les of the underlying application), then the list of all such "stateï¬les"
(wildcards such as "*" and directories are allowed) must to be speciï¬ed in model.configure(...),
see subsection 5.1 and Figure 9.
Such "stateï¬les" might be dynamically generated during the
init(...) and run(...) methods of the model, and hence are not necessarily already present
in the optional templatedir. If the model state is completely determined by such sandbox "state-
ï¬les" (i.e. those ï¬les are all that is required for a successful call of the run(...) method, in addition
to templatedir contents), then the built-in model state serialization functionality is already suf-
ï¬cient, independently of the applicationâ€™s programming language (including the External model
from subsubsection 5.3.2). However, writing relatively large "stateï¬les" (in every init(...) and
run(...)) as a strategy for model serialization might be ineï¬ƒcient due to the resulting I/O and
application initialization overheads if the model state is needed only very infrequently (or if PF is not
used - for instance, during forecast runs). To avoid such overhead, instead of writing and reading
26

"stateï¬les" in init(...) and run(...) methods, explicit model methods for "stateï¬les" writing
and reading can be implemented by the user:
write(self)
write "stateï¬les" representing model state mt to the model sandbox directory
read(self)
set model state mt by reading "stateï¬les" from the model sandbox directory
Note, that model sandbox is designed to contain only the "stateï¬les" corresponding to the current
model time tn. Any "stateï¬les" of the preceeding times (no longer required by run(...) method)
should be removed from the model sandbox. If "stateï¬les" are actually required for all times (e.g. in
post-processing), the history option needs to be set (see subsection 5.1) to enable automatic (within
self.output(...)) "stateï¬les" copies to the output directory (see Appendix A).
5.5.2. Model state serialization from model class instance
If, in addition to (or instead of) the sandbox "stateï¬les", the model state depends on the information
in the model class instance (or in the memory of the driven application), then the model serialization
requirements depend on the applicationâ€™s programming language.
In particular, for (pickleâ€™able)
applications written in pure Python or R (using r2py bindings), the built-in model state serialization
functionality is already suï¬ƒcient. For applications written in other programming languages, custom
methods need to be implemented to serialize the model into its binary representation (state) and to
de-serialize such state into the model again (including write() and read() for "stateï¬les", if used):
save(self)
return a bytearray (binary array) as the current model state mt
load(self,state)
load the speciï¬ed model state mt represented in the bytearray
For some of the most common programming languages, built-in driver modules described in subsub-
section 5.3.3 can be used to implement the above model state saving and loading.
5.6. Adding a distribution
SPUX requires all joint statistical distributions, such as model parameters prior Ï€(Â·|M), error O(Â·|M(Î¸), Î¸),
or model initial state prior Mt0(Î¸) to be deï¬ned as a SPUX Distribution. The mandatory func-
tionality of a Distribution X includes providing methods
(log)pdf(values)
evaluate the joint (log) probability density P(x) of values vector x âˆ¼X
If the provided values are invalid, 0 and âˆ’âˆshould be returned, respectively. Optional capabilities,
such as marginal probability density and quantile-quantile plots or initial sampler parameters drawn
from Ï€(Î¸, M), require implementation of additional Distribution methods:
(log)mpdf(label,value)
evaluate marginal (log) probability density of speciï¬c x[label]
intervals(alpha)
support intervals (mass alpha) of marginal probability densities
draw(rng)
draw random values vector x âˆ¼X using the provided rng
Each element of the vector x âˆ¼X has an associated label, i.e., a string, with supported La-
TeX syntax (for tables and ï¬gures), such as, for example, râ€™$\mu$â€™, and an associated type (by
default, random variables in Distribution are of type float). A dictionary of explicit types for
each random variable label (for instance, loaded from a text ï¬le with rows of "name type" using
loader.read_types(...) from spux.utils.io) can be speciï¬ed in configure(types=...) of
any distribution derived from the base Distribution class. Highly complex distributions represented
by hierarchical Bayesian networks, where each random variable can have conditional dependencies on
other variables via a directed acyclic graph, can be also constructed as a SPUX Distribution by
relying on already existing respective packages, such as PyMC3 [SWF16]. In the remaining of this
section we describe how to construct a Tensor distribution from univariate distributions of probabilis-
tic libraries (such as scipy.stats) and outline available built-in transformation methods for further
customization.
27

5.6.1. Tensor distribution
Already introduced in the Randomwalk model example in subsubsection 4.3.1, a Tensor distribution
class from spux.library.distributions.tensor module provides the easiest and by far the most
common way to specify a joint distribution of statistically independent univariate random variables.
Only a label-indexed dictionary of required univariate distributions, for example, constructed from
univariate distributions of the scipy.stats module, is needed to construct a Tensor distribution.
5.6.2. Distribution transformations
In some application scenarios, either truncated distributions might be needed (for instance, for non-
negative variables), or some observations (the outputs and/or the dataset) need to be transformed
before the density of the distribution from the error can be evaluated (for instance, in heteroscedastic
errors). For such purposes, the built-in transformation classes for univariate distributions are available
at spux.library.distributions.utils, with example usage in examples/hydro/error.py.
Truncate
truncate tail(s) of a probability density function at the speciï¬ed location(s)
Concentrate
concentrate tail(s) of probability density function to an atomic part
Transform
transform any continuous distribution by an invertible function
5.7. Adding an observational error model
In SPUX, an error is deï¬ned using a function (or a callable object) which takes model output
prediction y and parameters Î¸ as arguments and returns a corresponding SPUX distribution (see
subsection 5.6) D âˆ¼O(Â·|y, Î¸). Heteroscedastic errors can often be easily implemented using the
distribution transformation described in subsubsection 5.6.2. The SPUX framework aims at removing
systematic bias in the error by considering stochastic models instead of deterministic couterparts, and
hence by default temporally independent errors are assumed, covering most of the realistic use cases.
If required, temporally correlated errors can be setup by including past model outputs in output and
adding lagged dataframe columns in the dataset(s), see examples/superflex.
5.8. Adding auxiliary model outputs and observational datasets
Arbitrary (pandas.DataFrame-incompatible) model outputs and datasets, such as multi-dimensional
arrays or unstructured relational sets, can be also used in SPUX model, error, and distribution classes.
In particular, an arbitrary "auxiliary" object can be passed in run(...) as part of the model output
using output(...,auxiliary=...). Correspondingly, the prediction.auxiliary attribute will
be available, for instance, in the observational error model. There, the SPUX distribution for "stan-
dard" observations can be merged with a SPUX distribution (deï¬ned using prediction.auxiliary)
for "auxiliary" observations using the built-in Merge distribution from spux.distributions.merge
module. An auxset dictionary with "snapshots" as time points and a "loader" as function map-
ping a speciï¬ed snapshot to the corresponding dataset object (usually stored on a high-performance
ï¬lesystem, see subsubsection 4.4.2) needs to be speciï¬ed, see subsection 5.1. Note, that "auxiliary"
model outputs are not returned by the model __call__(...) method; if required internally, use the
model diagnostics attribute functionality instead. If history option is set (see subsection 5.1),
"auxiliary" model outputs are serialized and packed into archives in output directory (see Appendix A).
5.9. Aggregating multiple datasets
In some applications (for instance, examples/hydro), multiple observational datasets are available
for aggregation into the inference. Each dataset corresponds to the same model parameters Î¸, but is
a result of stochastic model evaluation with diï¬€erent initial model states and/or diï¬€erent (mutually
independent) stochastic trajectories. The Replicates aggregator can be used to aggregate datasets
by packing the corresponding datasets in a dictionary indexed by dataset names. The aggregator
28

can optionally conï¬gure initial model states from an analogously indexed dictionary of corresponding
initial functions, errors from a dictionary of corresponding error functions, and "auxsets" from a
dictionary of corresponding auxset dictionaries.
5.10. Parallel models and parallel user applications
The example Randomwalk model introduced in section 3 does not support parallelization, and hence
no executor was attached to it. However, an external user application might be very computationally
expensive and might be parallel (as a stand-alone executable or as a library) or might be splittable
into multiple independent tasks. In this section we introduce built-in executors for already parallel
user applications and review diï¬€erent ways to support direct model parallelization in SPUX. The
supplementary testing and synthesis scripts mentioned earlier, can also be launched for parallel models
(i.e., with multiple workers attached), as described in subsubsection 4.4.2.
Since the application processes might be running on a diï¬€erent compute node, in local ï¬lesystems
the sandbox directory can only be used for operations inside the SPUX model (not the coupled parallel
user application). If the sandbox directory is explicitly required inside parallel user application, then
the sandbox must be located on a shared ï¬lesystem, as described in subsection 5.2.
For applications parallelized using threads for shared memory architectures, such as multi-core
CPUs or GPUs, it is suï¬ƒcient (for any execution method from subsubsection 5.3.3) to properly
allocate enough computational resources and properly bind SPUX MPI workers (ranks), such that
each application has a dedicated part of a multi-core CPU or a GPU. If the (not yet parallelized)
algorithms within modelâ€™s init/run/__call__ methods can be split into independent tasks, consider
distribution across parallel workers of an attached "Pool" or "Ensemble" SPUX executor, see section 6.
In the next sections we outline available SPUX functionalities for applications parallelized using
MPI (distributed memory parallelism), where the built-in parallel model executor is attached to the
userâ€™s model with the speciï¬ed number of workers. In particular, "basic" mode replacing the built-in
self.shell(...) is described in subsubsection 5.10.1. Despite its generality and simplicity, the
"basic" model executor mode could be ineï¬ƒcient due to multiple application execution calls. Alter-
native model executor modes ("advanced" and "custom") are introduced in subsubsection 5.10.2 and
subsubsection 5.10.3 (with supplementary subsubsection 5.10.4), requiring only a single application
launch (e.g. loading a library or executing a process) and relying on application execution control
within init/run/__call__ methods. Such model executor modes allow one to avoid unnecessary
ï¬lesystem related operations and excessively large number of state saving/loading or "stateï¬les" writ-
ing/reading by implementing custom write/read or save/load methods (see subsection 5.5). A
guideline scheme for selecting the appropriate executor mode for the model based on the available
parallel features of the userâ€™s application and of the MPI library is available in Figure 10.
basic
driver to directly call
application methods
yes
no
spawning new MPI
processes is supported
application support to 
replace â€MPI_COMM_WORLDâ€
yes
no
advanced
custom
unsupported
yes
no
efficient model state
serialization needed
yes
no
Figure 10: A guideline scheme to select the appropriate parallel model executor mode based on the
characteristics of the application and of the MPI library.
29

5.10.1. Parallel model executor for parallel user applications - "basic" mode (execute)
As an extension of the "basic" execution mode in subsubsection 5.3.3, the "basic" parallel model
executor replaces the built-in self.shell(...) in init/run/__call__ methods with
self.executor.execute (râ€™applicationâ€™, args = [â€™arg1â€™, â€™arg2â€™])
This way, each application is executed (spawned) in parallel (in the respective sandbox directory),
analogously to the manual launch using MPI. For correct model serialization, model (and application)
state within init(...) and run(...) methods can be directly stored as model instance (self)
attributes and/or as ï¬les (e.g. "stateï¬les") in its sandbox (without explicit write/read methods).
5.10.2. Parallel model executor for parallel user applications - "advanced" mode (instruct)
As an extension of the "advanced" model execution method from subsubsection 5.3.3, the "advanced"
parallel model executor mode relies on direct (but minimally intrusive) application control. In partic-
ular, on each parallel application worker in the peers intra-communicator, a function to establish and
return (e.g. using drivers, see subsubsection 5.3.3 and Appendix E) an interface to the application
interface = method (manager, peers)
can be speciï¬ed for the built-in parallel model executor establish(...) method:
self.executor.establish (method)
User-deï¬ned instruction functions are required to call application method(s) on each parallel worker
(with the MPI intra-communicator peers instead of MPI_COMM_WORLD) using the binded interface:
result = instruction (interface, peers)
SPUX model acts as a "manager" to control application: specify parameters, retrieve output, and
save/load model state or write/read "stateï¬les". To achieve this, custom instruction functions
(see also Appendix E for a practical example) can be deï¬ned (even at runtime within model methods,
if speciï¬ed model parameters, time, seed, etc. need to be taken into account). An instruction
function can be dispatched from (any method of) the SPUX model (as the "manager") to all parallel
application processes (as "workers") for execution and retrieval of returned instruction results by
calling:
results = self.executor.instruct (instruction)
Application interface can be terminated by self.executor.demolish() in the model exit().
5.10.3. Parallel model executor for parallel user applications - "custom" mode (connect)
As an extension of the "custom" model execution mode introduced in subsubsection 5.3.3, a "custom"
parallel model executor mode can be used to launch (spawn) the application, e.g. in init(...), with
self.executor.launch (râ€™applicationâ€™, args = [â€™arg1â€™, â€™arg2â€™, â€™<PORT>â€™])
where strings <PORT> among the list of arguments will be replaced by the actual port, described in
later subsubsection 5.10.4. Afterwards, the manager-side (i.e. SPUX model) MPI inter-communicator
to the parallel workers (i.e. user application) can be obtained and released by, respectively:
workers = self.executor.connect() and self.executor.disconnect(workers)
executor method calls in any model method, as many times as required. Note, that the MPI commu-
nicators are not serializable and hence must be added to the ignore list (see subsection 5.1) if stored
as model (i.e. self) attributes. The manager-side MPI inter-communicator to workers can be used
for simulation control, parameters speciï¬cation, output retrieval, saving/loading of the model state
or writing/reading of "stateï¬les", and for termination of application execution in model exit().
30

5.10.4. Application-side communicators for parallel user application in "custom" mode
In the "custom" model executor mode, in addition to a standard intra-communicator among other
application workers (usually MPI_COMM_WORLD, as in a manual MPI run), each parallel worker (of the
userâ€™s application) also retrieves a corresponding worker-side inter-communicator with the "manager"
(the model) by calling MPI_Comm_Connect(port,...). The port can be passed as the command
line argument (or, alternatively, as a ï¬le in the model sandbox) to the application executable. The
worker-side MPI inter-communicator to "manager" can be used to manage the execution ï¬‚ow within
the userâ€™s application according to the requests from the manager side (parameters acquisition, output
reporting, saving/loading model state and writing/reading "stateï¬les", etc.).
5.11. Adding an arbitrary SPUX component
In this section we brieï¬‚y overview guidelines for adding new SPUX component algorithms (e.g. for
sampler, aggregator, likelihood or distance component types), or even new SPUX component types.
A new SPUX component algorithm class inherits the corresponding component type class, and
can potentially re-deï¬ne mandatory require dictionary and/or optional configure method (as in
configure.py), see Appendix F, Listing 15. The require dictionary contains "executor" with
an attachable executor type name (from subsection 6.1 with default being "Pool") and (optionally)
"tasks" with a list of assignable component type names (absent by default). Optional init and
mandatory __call__ methods of the component algorithm class are responsible for the actual com-
putations (as in infer.py) and can access self.sandbox, self.seed and self.rng instances.
A new SPUX component type class (i.e. a row in the components table, subsection 3.1) inherits the
Component class, deï¬nes the mandatory component attribute (a string for type name), and can re-
deï¬ne the require attribute and the optional configure method, see Appendix F, Listing 16. The
Component class from the spux.components.component module implements default attributes and
methods common among all SPUX component type classes, such as name, require, evaluations,
assign, attach, copy, setup, isolate, plant, spawn, consensus, feedback, feed, prepare.
5.12. Approximate Bayesian Computation type samplers
In ABC-type samplers (introduced in subsubsection 2.3.3), the (approximate-)likelihood based sam-
plers are replaced by approximate samplers based on an empirical distance between the appropriate
summary statistics S(y) of model output y and S(D) of the observational dataset D, as depicted in
the adapted SPUX framework scheme in Figure 11. If, in addition, the (optional) observation error
model is available, then it will be incorporated into the model output before the distance evaluations.
Alternatively, the userâ€™s SPUX model might already contain all required uncertainties arising from
both the generative (deterministic or stochastic) model and the stochastic error, in which case the
observational errors are also incorporated for the distance evaluations. However, the explicitly deï¬ned
error model (i.e., not part of the generative model) allows for future forecast of true model output,
whereas hiding the error within the generative model only allows for future forecast of observed (i.e.,
including observational noise) model output. For the ABC-type samplers, the number of required
Markov
ABC
sampler
model
distance
ğœ‹(ğœƒ|ğ‘€)
ğ’®(ğ·)
ğ’ª
â„™(ğœƒ|ğ·, ğ‘€)
â„™(ğ‘š[ğ‘¡0,ğ‘‡]|ğœƒ, ğ·, ğ‘€)
ğ‘€ğ‘¡0(ğœƒ)
sampler
model
likelihood
ğœ‹(ğœƒ|ğ‘€)
ğ·
ğ’ª
â„™(ğœƒ|ğ·, ğ‘€)
â„™(ğ‘š[ğ‘¡0,ğ‘‡]|ğœƒ, ğ·, ğ‘€)
ğ‘€ğ‘¡0(ğœƒ)
Figure 11: Diï¬€erences in the components conï¬guration for the SPUX framework using ABC-type
sampler (right part). The components that are diï¬€erent from the Markov-type samplers
(left part, refer to Figure 5 for a detailed description) are highlighted in red.
31

posterior samples can be set by specifying the batchsize for the sampler constructor, and the num-
ber of iterations for convergence can be set by specifying the batches (instead of the samples for
the Markov-type samplers) for the call sampler(...). For an example using the SABC sampler,
please refer to examples/randomwalk-sabc.
6. SPUX framework parallelization
In this section we describe three built-in executor types and their load balancing techniques. For the
design and implementation of the hierarchical parallelization sub-system supporting multiple nested
executors for simultanous parallelization of multiple nested SPUX components, see Appendix H.
6.1. SPUX executors - types
Any set of independent tasks within any of the SPUX components can be executed in parallel using
built-in SPUX executors. The parallel design of SPUX is centered on three main types of executors,
each supporting diï¬€erent functionality and (usually) meant to be used in diï¬€erent SPUX components:
pool
dynamically executes a set of independent tasks; task "states" are discarded
ensemble
adaptively executes a set of independent ensemble tasks (maintaining task "states")
model
dynamically executes a userâ€™s application (discarding or maintaining its "state")
Executors can be attached to SPUX components without any restrictions on the executor type, how-
ever, the list of available executor capabilities (methods) is checked at the attach(...) method
of the corresponding SPUX component. As described in the tutorial, the default executors of each
type are the serial executors with rather straight-forward implementations, namely SerialPool,
SerialEnsemble, SerialModel.
Currently, parallel executors of each type are implemented in
SPUX using the Message Passing Interface (MPI) library [Mes15] via the object-oriented Python
MPI bindings package mpi4py [DPKC11]. MPI dynamic process management for parallel workers is
employed for deploying nested parallelization. Pickle-based communication of arbitrary serializable
Python objects is used for execution workï¬‚ow management across managers and workers, as the
memory and processing time overheads are negligible. Additionally, for multiple subsequent executor
calls (e.g. for iterative sampling within sampler), executor task caching allows to perform task serial-
ization and communication only once. Finally, memory-eï¬ƒcient raw binary arrays (bytearray) are
used for eï¬ƒcient communication of model states.
6.1.1. SPUX "pool" executors
A "pool" type executor can be used by calling its map(functions,parameters) method, where
either argument (or both) can also be iterable (for instance, a list) over corresponding parallel tasks.
For lower communication overhead, consider providing lists of sandboxes and/or seeds as arguments
in map(...) instead of using lists of functions. An optional list of arguments can be speciï¬ed in
map(...) as args and will be passed to the execution of the function(s) by function(...,*args).
6.1.2. SPUX "ensemble" executors
An "ensemble" type executor does not accept tasks directly, but requires an instance derived from
the Ensemble base class deï¬ned in the spux.library.ensembles.ensemble module. Currently
the only implemented ensemble class is for a Particles ensemble of SPUX models (particles to
be used in the PF likelihood). For example, the "ensemble" executor, attached to an instance of
the PF likelihood, manages the parallel initialization, method execution and resampling of (indexed)
ensemble tasks. In particular, in-between the ensemble executor connect(ensemble,indices) and
disconnect() methods, call(method,args) and/or resample(indices) methods can be called
multiple times, each time executing the speciï¬ed ensemble method and/or performing ensemble
32

resampling, respectively. In the resample method, ensemble tasks can be cloned (duplicate indices)
and deleted (missing indices), including the load re-balancing across the resampled sub-ensembles. A
detailed description of the parallel resamplig procedure is available in the earlier manuscript [Å K17].
6.1.3. SPUX "model" executors
A "model" type executor is needed to execute a parallel user application. As described in subsec-
tion 5.10, three parallel model executor modes to manage parallel application workers are supported:
basic
execute(command) by spawning a new process (analogously to mpiexec)
advanced
instruct(...) to call instruction(...) with MPI intra-communicator peers
custom
launch(application) and connect()/disconnect() MPI inter-communicators
Options, such as args list of runtime arguments, can be speciï¬ed in execute(...) and launch(...)
methods. For serial executors (workers set to None or 0), both methods fall back to shell(...).
6.2. SPUX executors - load balancing
In all parallel executors, a collection of multiple tasks must be carefully dispatched to (or re-distributed
among) parallel workers to ensure the minimal needed wall-clock runtime to execute all tasks. In this
section we describe such load balancing algorithms for the pool and ensemble type executors. For
"model" type executors, the load balancing is federated to the userâ€™s application itself.
6.2.1. Load balancing for pool-type executors
All tasks requested in the "map" method of the "pool" type executor are executed dynamically, with
pending tasks being constantly dispatched to workers as they become available. For parallel runs with
multiple datasets (e.g. examples/hydro example), the Replicates aggregator performs guided load
balancing by evaluating the lengths of the associated datasets and sorting component evaluations,
taking into account the component adaptivity as well (for instance, the replicate-dependent adap-
tive number of particles in the PF likelihood). Higher priorities are assigned to components with
longer datasets and large number of particles (if applicable), and lower priorities are assigned to the
components with shorter datasets and smaller number of particles (if applicable).
6.2.2. Load balancing for ensemble-type executors
During routing, re-sampled tasks are instructed to be moved from over-utilized workers to the neigh-
boring under-utilized workers across their intra-communicator. Routing objects include worker-speciï¬c
information regarding task identiï¬cation, source (present worker address), destination (re-routed
worker address), re-identiï¬cation (for post-routing re-seeding), and aï¬ƒnity (local or remote node).
An empirical greedy algorithm is employed for constructing routings based on re-sampled task counts,
current task distribution across workers, and the maximal worker load. For each worker, resident re-
sampled tasks are kept in place provided the worker loads do not exceed the maximal worker load.
Remaining tasks are routed either to a under-utilized worker on the same compute node (determined
by aï¬ƒnity), or, in the worst case, to the closest (w.r.t. proximity of worker ranks) under-utilized
remote worker. To further reduce communication overhead, identical tasks are moved together by
moving only a single particle and then replicating it on the destination worker. Such load balancing
strategy, however, does not take into account possibly heterogeneous model runtimes for each task,
and hence there could be potential gains from dynamic balancing frameworks using task stealing in-
stead of an adaptive routing approach. Sandboxes, associated to the corresponding tasks, are assumed
to be isolated from each other, unless local aï¬ƒnity (determined automatically) is set in the routing
information. For local aï¬ƒnity, sandboxes exist on the same compute node (with access to the same
ï¬lesystem), and hence a simple and eï¬ƒcient sandbox renaming (internally consisting of stashing and
fetching) is suï¬ƒcient. Such "hybrid" parallelization, inspired by existing MPI + OpenMP paradigms,
33

harnesses the node-local connectivity to minimize the required sandbox-related communication. For
remote aï¬ƒnity, sandboxes are "pseudo-moved" from one remote ï¬lesystem to another, by copying
template sandbox on the destination, and, if sandbox "stateï¬les" are speciï¬ed, saving the sandbox
state, sending it to the destination, and loading it into the newly created sandbox. After removal of
all extinct particles, at the expense of memory usage the resampling process prioritizes overlapping
the communication synchronization overhead with any other pending task that does not require infor-
mation exchange with other parallel ensemble workers. In particular, after all asynchronous sends and
receives for task exchange according to routings are launched, all orphan particles (already sent, but
not to be kept) are removed, remaining particles are stashed, ensembles are synchronzed to prevent
race conditions, stashed particles are fetched using re-identiï¬cation, local particles are replicated, and
only in the last step received remote particles are stored and replicated, while still waiting in the back-
ground for all local particles (if any) to be sent. For a more detailed description and an illustrative
scheme of such adaptive ensemble task resampling, refer to the earlier manuscript [Å K17].
7. Future developments
In the near future, multi-level uncertainty quantiï¬cation methods (e.g. ML(ET)PF, MLCV) will be
investigated. These methods can greatly reduce the amount of required computational resources for
Bayesian inference and forecasting by conï¬guring applications for multiple diï¬€erent accuracies (e.g.
resolutions). These capabilities will be oï¬€ered in SPUX through the the foreseen MLCV aggregator
component. Furthermore, samplers optimized for multi-modal posteriors will be investigated. Addi-
tionally, shared memory optimizations for nodel-level model state communication, dynamic balancing
using task stealing in ensemble tasks resampling, and machine learning based dynamical process
scheduling for pool executors are foreseen as further improvements of an already very well performing
SPUX parallelization suite. Depending on the level of support for general purpose distributed tasking
libraries, new parallel executors could be added.
8. Author contributions and acknowledgments
Author contributions. JÅ  designed and implemented the prototype framework, plotting, automatic
report generation, compiled the documentation, contributed examples for built-in and "hydro" models,
and led the preparation of this manuscript. JÅ  and MB co-designed and implemented the paralleliza-
tion speciï¬cation, all main SPUX components, and SPUX interfaces (UI and API). MB also designed
and implemented the legacy MPI connector, distribution variable types, and contributed examples for
"superï¬‚ex" and "ibm" models.
Acknowledgments. Authors would like to thank Uwe Schmitt and MikoÅ‚aj RybiÅ„ski (ETH Zurich)
for support with CI/CD and unit testing on EULER and Daint clusters at the Swiss Supercomputing
Center (CSCS), Artur Saï¬n, Marco Dal Molin and Lorenz Amman (Eawag) for contributing exam-
ple models, Mira Kattwinkel (U Koblenz-Landau) for helping to design the initial SPUX framework
prototype, Alexander Madsen (PSI), Anthony Ebert (USI and Eawag), Simone Ulzega (ZHAW) for
contributing and testing SABC sampler, Peter Reichert (Eawag), Andreas Scheidegger (Eawag), Carlo
Albert (Eawag), Fabrizio Fenicia (Eawag), Vilma Å ukienË™e for providing feedback to this manuscript
and framework usage, Panagiotis Hadjidoukas (IBM Research Zurich) for advice regarding task-based
parallelism design, and Siddhartha Mishra (ETH Zurich) for access to EULER cluster.
Funding. We also acknowledge the Eawag Directorate Discretionary Funding for granting ï¬nan-
cial resources and Swiss Supercomputing Center (CSCS) for granting computational resources in the
development project d97.
34

References
[ABD+09]
Brian M Adams, WJ Bohnhoï¬€, KR Dalbey, JP Eddy, MS Eldred, DM Gay, K Haskell,
Patricia D Hough, and Laura P Swiler. Dakota, a multilevel parallel object-oriented
framework for design optimization, parameter estimation, uncertainty quantiï¬cation,
and sensitivity analysis: version 5.0 userâ€™s manual. Sandia National Laboratories, Tech.
Rep. SAND2010-2183, 2009.
[ADH10]
Christophe Andrieu, Arnaud Doucet, and Roman Holenstein.
Particle Markov chain
Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 72(3):269â€“342, 6 2010.
[AKS15]
Carlo Albert, Hans R KÃ¼nsch, and Andreas Scheidegger. A simulated annealing approach
to approximate bayes computations. Statistics and computing, 25(6):1217â€“1232, 2015.
[AUS16]
Carlo Albert, Simone Ulzega, and Ruedi Stoop. Boosting bayesian parameter inference of
nonlinear stochastic diï¬€erential equation models by hamiltonian scale separation. Phys.
Rev. E, 93:043313, Apr 2016.
[BCJ+18]
Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan,
Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman.
Pyro: Deep Universal Probabilistic Programming. Journal of Machine Learning Research,
2018.
[CGH+17]
Bob Carpenter, Andrew Gelman, Matthew D Hoï¬€man, Daniel Lee, Ben Goodrich,
Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan:
A probabilistic programming language. Journal of statistical software, 76(1), 2017.
[CRSF16]
Fernando Chirigati, RÃ©mi Rampin, Dennis Shasha, and Juliana Freire. Reprozip: Com-
putational reproducibility with ease. In Proceedings of the 2016 International Conference
on Management of Data, SIGMOD â€™16, pages 2085â€“2088. ACM, 2016.
[DJ09]
A. Doucet and A.M. Johansen. A tutorial on particle ï¬ltering and smoothing: Fifteen
years later. Handbook of nonlinear ï¬ltering, 12:656â€“704, 2009.
[DMRT14]
J Doherty, C Muï¬€els, J Rumbaugh, and M Tonkin. Pest, model independent parameter
estimation and uncertainty analysis, 2014.
[DPKC11]
Lisandro D. Dalcin, Rodrigo R. Paz, Pablo A. Kler, and Alejandro Cosimo. Parallel
distributed computing using Python. Advances in Water Resources, 34(9):1124â€“1139,
2011.
[DSN+14]
Omer Demirel, Ihor Smal, Wiro J Niessen, Erik Meijering, and Ivo F Sbalzarini. Ppf: A
parallel particle ï¬ltering library. 2014.
[DSU+17]
Ritabrata Dutta, Marcel Schoengens, Avinash Ummadisingu, Nicole Widmer, Jukka-
Pekka Onnela, and Antonietta Mira. Abcpy: A high-performance computing perspective
to approximate bayesian computation. arXiv preprint arXiv:1711.04694, 2017.
[duc18]
Glossary â€” python 3.7.1 documentation, 2018.
[eas20]
Easyabc, 2020.
[EMD17]
V. Elvira, J. MÃ­guez, and P. M. DjuriÄ‡. Adapting the number of particles in sequential
monte carlo methods through an online scheme for convergence assessment.
IEEE
Transactions on Signal Processing, 65(7):1781â€“1794, 2017.
35

[FMHLG13] Daniel Foreman-Mackey, David W Hogg, Dustin Lang, and Jonathan Goodman. em-
cee:
the mcmc hammer.
Publications of the Astronomical Society of the Paciï¬c,
125(925):306, 2013.
[GC17]
A. Gregory and C. J. Cotter. A seamless multilevel ensemble transform particle ï¬lter.
SIAM Journal on Scientiï¬c Computing, 39(6):A2684â€“A2701, 2017.
[GCS+14]
Andrew Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari, and D Rubin. Bayesian
Data Analysis, third ed. Chapman & Hall, 2014.
[GDB+19]
Carla Gomes, Thomas Dietterich, Christopher Barrett, Jon Conrad, Bistra Dilkina, Ste-
fano Ermon, Fei Fang, Andrew Farnsworth, Alan Fern, Xiaoli Fern, Daniel Fink, Douglas
Fisher, Alexander Flecker, Daniel Freund, Angela Fuller, John Gregoire, John Hopcroft,
Steve Kelling, Zico Kolter, Warren Powell, Nicole Sintov, John Selker, Bart Selman,
Daniel Sheldon, David Shmoys, Milind Tambe, Weng-Keen Wong, Christopher Wood,
Xiaojian Wu, Yexiang Xue, Amulya Yadav, Abdul-Aziz Yakubu, and Mary Lou Zeeman.
Computational sustainability: Computing for a better world and a sustainable future.
Commun. ACM, 62(9):56â€“65, August 2019.
[GL06]
D. Gamerman and H.F. Lopes. Markov Chain Monte Carlo: Stochastic Simulation for
Bayesian Inference. Taylor & Francis Inc., 2006.
[HAPK15]
Panagiotis E Hadjidoukas, Panagiotis Angelikopoulos, Costas Papadimitriou, and Petros
Koumoutsakos. Ï€4u: A high performance computing framework for bayesian uncertainty
quantiï¬cation of complex models. Journal of Computational Physics, 284:1â€“21, 2015.
[Has70]
W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applica-
tions. Biometrika, 57(1):97â€“109, 1970.
[HBWP13]
Matthew D. Hoï¬€man, David M. Blei, Chong Wang, and John Paisley. Stochastic vari-
ational inference. J. Mach. Learn. Res., 14(1):1303â€“1347, May 2013.
[HKCCB15] Tobias Houska, Philipp Kraft, Alejandro Chamorro-Chavez, and Lutz Breuer. Spotting
model parameters using a ready-made python package. PloS one, 10(12):e0145180,
2015.
[HWN18]
Marvin HÃ¶ge, Thomas WÃ¶hling, and Wolfgang Nowak. A primer for model selection:
The decisive role of model complexity. Water Resources Research, 54(3):1688â€“1715,
2018.
[JMR15]
P.E. Jacob, L.M. Murray, and S. Rubenthaler. Path storage in the particle ï¬lter. Statistics
and Computing, 25:487â€“496, 2015.
[KBBP16]
J Nathan Kutz, Steven L Brunton, Bingni W Brunton, and Joshua L Proctor. Dynamic
mode decomposition: data-driven modeling of complex systems. SIAM, 2016.
[KCHM19]
Ravin Kumar, Colin Carroll, Ari Hartikainen, and Osvaldo A. Martin. ArviZ a uniï¬ed
library for exploratory analysis of Bayesian models in Python. The Journal of Open
Source Software, 2019.
[KR17]
Mira Kattwinkel and Peter Reichert. Bayesian parameter inference for individual-based
models using a Particle Markov Chain Monte Carlo method. Environmental Modelling
& Software, 87:110â€“119, 2017.
[Lim19]
Thomas A. Limoncelli. Demo data as code. Commun. ACM, 62(10):39â€“41, September
2019.
36

[LLM19]
Adam Lev-Libfeld and Alexander Margolin. Fast data: Moving beyond from big dataâ€™s
map-reduce, 2019.
[LSTB09]
David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best. The bugs project:
Evolution, critique and future directions. Statistics in medicine, 28(25):3049â€“3067, 2009.
[McK10]
Wes McKinney. Data structures for statistical computing in python. In StÃ©fan van der
Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference,
pages 51 â€“ 56, 2010.
[Mes15]
Message Passing Interface Forum. MPI: Message-Passing Interface Standard. Version
3.1. Technical report, 2015.
[MRRT53]
Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, and Augusta H.
Teller. Equation of State Calculations by Fast Computing Machines. The Journal of
Chemical Physics, 21(6), 1953.
[Mur13]
Lawrence M Murray.
Bayesian state-space modelling on high-performance hardware
using libbi. arXiv preprint arXiv:1306.3277, 2013.
[NHS05]
Lars Nerger, Wolfgang Hiller, and Jens SchrÃ¶ter. Pdaf-the parallel data assimilation
framework: experiences with kalman ï¬ltering. In Use Of High Performance Computing
In Meteorology, pages 63â€“83. World Scientiï¬c, 2005.
[PCDM14]
Matthew Parno, P Conrad, A Davis, and YM Marzouk. Mit uncertainty quantiï¬cation
(muq) library, 2014.
[Plu04]
Martyn Plummer. Jags: Just another gibbs sampler, 2004.
[RM09]
P. Reichert and J. Mieleitner. Analyzing input and structural uncertainty of nonlinear
dynamic models with stochastic, time-dependent parameters. Water Resources Research,
45:W10402, doi:10.1029/2009WR007814, 2009.
[Å K17]
Jonas
Å ukys
and
Mira
Kattwinkel.
SPUX:
Scalable
Particle
Markov
Chain
Monte
Carlo
for
uncertainty
quantiï¬cation
in
stochastic
ecological
models.
http://arxiv.org/abs/1711.01410, 11 2017.
[SWF16]
John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic program-
ming in python using pymc3. PeerJ Computer Science, 2:e55, 2016.
[uqp20]
Uqpy, 2020.
[WME+20] D. WÃ¤lchli, S. Martin, A. Economides, L. Amoudruz, G. Arampatzis, X. Bian, and
P. Koumoutsakos. Load balancing in large scale bayesian inference. In [To be published
at] Proceedings of the Platform for Advanced Scientiï¬c Computing Conference PASC20,
2020.
A. SPUX setup and output
The setup directory contains generated raw (binary, text, and LaTeX) information regarding SPUX
framework conï¬guration and setup, corresponding to configure.py and infer.py, respectively. The
output directory is generated incrementally using checkpointing and contains multiple subdirectories:
37

samples
samples of posterior parameters, outputs, supporting "infos", timings
best
the "best" trajectory (parameters and output); also see Table R13
diagnostics
diagnostics information (e.g. metrics)
specifications
structure speciï¬cations of the outputs/infos ï¬les and the "best" trajectory
pickup
framework (e.g. sampler) pickup information (for --continue)
states
ï¬nal states samples of posterior model trajectories
statefiles
(only if used) copies of "stateï¬les" for all times
auxiliary
(only if used) auxiliary model outputs
Contents of the samples subdirectory can be loaded and used for any custom post-processing:
parameters-*.csv
a CSV ï¬le containing posterior samples of model parameters
predictions-*.dat
a binary ï¬le containing (posterior) of model output samples
infos-*.dat
a binary ï¬le containing supporting information (e.g. acceptance rates)
timings-*.dat
a binary ï¬le containing framework timings (runtimes and timestamps)
Refer to report.py for an example of loading all samples ï¬les into dataframes (for CSV) and
lists of dictionaries using loader.reconstruct(...) from spux.utils.io. The corresponding
speciï¬cations tables are also listed in the section R6 of the SPUX report as Table R14, Table R15, and
??. In particular, each model predictions (output) sample is a dictionary of either dataframes (default)
or Trajectories from spux.utils.trajectories module (using memory-eï¬ƒcient compressed
data structures [JMR15], e.g. for PF).
Infos contain additional (component-speciï¬c) information for "infos" keyword requested in informative
and/or for a manually speciï¬ed component (such as PF likelihood or Norm distance) diagnostics
list. Unless "rejections" keyword is requested in informative, model outputs and infos corresponding
to the rejected samples are excluded (set to None), and the estimates of prior and likelihood/distance
are overwritten by the values of the accepted samples.
To ensure consistency, we strongly advice to access any SPUX framework output (or conï¬guration
and setup options) using the SPUX status, which can be constructed using the Status class from
the spux.utils.status module, as is done in report.py. Attribute status.parameters is a
pandas.DataFrame loaded directly from parameters-*.csv, and additional status methods can
be called with any required batch/chain/time arguments:
info(key,...)
respective key value from loaded infos-*.dat
prediction(...)
respective model prediction from loaded predictions-*.dat
timing(...)
respective timing measurement from loaded timings-*.dat
sandbox(...)
respective model sandbox path (relative to the "root" sandbox)
auxiliary(...)
respective "auxiliary" model output (see subsection 5.8)
statefiles(...)
respective paths to directories storing all "stateï¬les" (see subsubsection 5.5.1)
Internally, these methods use "origins" stored in sampler "info" to locate the corresponding accepted
samples and functionality of the Trajectories class to follow trajectory ï¬ltering (if PF was used).
The figures directory contains all raw ï¬gure ï¬les in multiple formats (default: PDF (vector),
PNG (raster)) and with the associatiated caption ï¬les *.cap containing the description of the ï¬gure
contents. The contents of the setup and figures directories are used to generate the LaTeX source
ï¬les in the report directory, which are compiled into the SPUX report (A4 and "slides" layouts).
B. Remark on workers for parallel SPUX executors
The freedom to attach arbitrary many parallel workers for every SPUX component provides a lot of
ï¬‚exibility, but also leaves ample space for computationally ineï¬ƒcient parallel conï¬gurations. Hence,
for large production runs, we strongly advice to follow these two guidelines:
â€¢ Attach most workers to the outer-most SPUX component(s) (e.g. sampler).
38

â€¢ Avoid few parallel workers (less than 4) - replace them with the default workers = None.
SPUX will report the percentage of the number of manager cores w.r.t. the total number of cores.
Additionally, parallel performance and scaling plots could be used to investigate the model runtimes
homogeneity and the synchronization overheads. For highly heterogeneous model runtimes (for in-
stance, when model runtime strongly depends on the proposed model parameters), or for the cases
where sampler often proposes parameters outside the speciï¬ed prior parameters distribution, consider
attaching more workers to the PF likelihood instead in order to avoid very few tasks for each parallel
worker in sampler (and aggregator) SPUX components.
C. Example forecast and sequential assimilation scripts
Listing 9: Example spux.cfg for forecast
model
Randomwalk
model.dt 0.1
aggregator
Trajectories
aggregator.trajectories 64
sampler MC
sampler.chains 64
sampler.samples
500
pastdir "../ randomwalk"
timeset
utils.period (25, 30)
Listing 10: Example spux.cfg for assimilation
model
Randomwalk
model.dt 0.1
likelihood PF
likelihood.particles
256
sampler MC
sampler.chains 64
sampler.samples
1000
pastdir "../ randomwalk"
dataset
dataset -T_1 -T_2.py
D. Example model scripts
Listing 11: External model methods
from spux.utils.io import
loader
from spux.utils.io import
dumper
def run (self ,time ):
p = self.sandbox("parameters.txt")
t = self.sandbox("time.txt")
s = self.sandbox("seed.txt")
dumper.txt(p,self.parameters)
dumper.txt(t,self.time)
dumper.txt(s,self.seed ())
cmd = self.replace(self.command)
code = self.shell(cmd)
if code is None: return
None
o = self.sandbox("output.txt")
return
self.output(loader.txt(o))
Listing 12: Randomwalk model methods
def init (self ,initial ,parameters ):
self.d = parameters["drift"]
self.v = parameters["volatility"]
self.m = initial["position"]
self.time = initial["time"]
def run (self ,time ):
while
self.time < time:
dt = min(time -self.time ,self.dt)
n = self.rng.normal ()
self.m += dt*self.d
self.m += sqrt(dt)* self.v*n
self.time += dt
return
self.output ([ self.m],[â€™pâ€™])
E. Example parallel model executor scripts for Fortran
In Listing 13, the application module is obtained by compiling userâ€™s Fortran application with f2py.
On parallel application workers, the returned interface is passed to instruction in Listing 14.
39

Listing 13: Application interface example
def
interface (manager ,peers ):
import
application
interface = application
return
interface
Listing 14: Executor instruction example
def
instruction (interface ,peers ):
comm = peers.py2f ()
result = interface.main(comm)
return
result
F. Example SPUX component scripts
Listing 15: Component algorithm class example
class
NewAlgorithm (NewType ):
def init (self ,...):
...
def
__call__ (self ,...):
...
Listing 16: Component type class example
class
NewType (Component ):
component = "new_type"
requires = {"executor":"Pool"}
def
configure (self ,...):
...
G. Adaptivity in the Particle Filter
For the SPUX framework, we are also introducing an empirical adaptivity technique to dynamically
control the number of particles used for the PF likelihood.
The proposed technique, as already
common among complex adaptive methodologies such as [EMD17], relies on empirical metrics; in
this case, on "ï¬tscore", which describes the convergence progress of the posterior sampling, and on
"accuracy", which provides an estimate for the statistical error in the estimated likelihood.
G.1. Fitscores of model parameters
A ï¬tscore indicator is simply the likelihood normalized (purely for easier interpretation and gener-
alization) w.r.t. the dataset length, dimensionality of the observations, and the maximum of the
corresponding error distribution densities. As such, ï¬tscore measures (analogously to likelihood) how
consistent the model output is compared to the observational dataset. Recalling the notations
On(D|y, Î¸) = O(Dtn|ytn, Î¸),
Op
n(D|y, Î¸) = On(D|yp, Î¸),
On(D|y, Î¸) = 1
P
P
X
p=1
Op
n(D|y, Î¸),
(12)
we deï¬ne ï¬tscore as the average (over multiple dataset points N and multiple particles P of the
Particle Filter) logarithm of the normalized (with respect to probability density function value of
the model prediction and the dimensions d of the observations) residuals (observational model error
probability density of posterior model output):
r(Î¸, D, M) = 1
N
N
X
n=1
1
P
P
X
p=1
log Op
n(D|y, Î¸) âˆ’log Op
n(y|y, Î¸)
d
.
(13)
The numerical value of the ï¬tscore empirically tracks progress of the sampler convergence, with higher
values indicating that the sampler is most probably out of burn-in phase.
40

G.2. Accuracies of likelihood estimates
The variance Ïƒ2(log Ë†L) of the estimated marginal log-likelihood log Ë†L was found to be an important
indicator controlling the convergence of the Markov chain sampler [KR17]. It can be bounded (with
equality only for statistically independent On over all n = 1, . . . , N) by the sum of log-likelihood
variances from individual snapshots (for brevity, we drop (D|y, Î¸) notation from Op
n and On):
Ïƒ2(log Ë†L) = Ïƒ2
 
log
 N
Y
n=1
On
!!
= Ïƒ2
 N
X
n=1
log Op
n
!
â‰¤
N
X
n=1
Ïƒ2(log On).
(14)
Motivated by this upper bound, the empirical "accuracy" of the PF likelihood is deï¬ned as the average
(over multiple dataset points) standard deviations for the estimated observational log-errors (treated
as random statistical estimates depending on p):
Î´(Î¸, D, M) = 1
N
N
X
n=1
Ïƒ(log On).
(15)
The numerical value of the accuracy empirically measures the statistical accuracy of the marginal
likelihood estimator computed by the Particle Filter, normalized with respect to the length n of the
dataset. The standard deviations Ïƒ(log On) for the estimated observational log-errors of the current
snapshot, are dynamically (i.e., during runtime) estimated using 1st order Taylor-series approximation
and the central limit theorem. In particular, the variance of the logarithm of a random variable a can
be approximated using Taylor series around the mean Âµ(a):
Ïƒ2(log a) â‰ˆÏƒ2(a)
Âµ2(a),
provided
Ïƒ2(a) â‰ªÂµ2(a).
(16)
The summands Ïƒ(log On) in equation 15 can be approximated by such Taylor-series by setting a = On
and then applying a central limit theorem to approximate Âµ(On) and Ïƒ(On) by Ë†Âµ(On) and Ë†Ïƒ(On):
Âµ(On) = Âµ(Op
n) â‰ˆË†Âµ(Op
n) = 1
P
P
X
p=1
Op
n = On,
Ïƒ2(On) = Ïƒ2(Op
n)
P
â‰ˆË†Ïƒ2(Op
n)
P
.
(17)
In Equation 17, Ë†Âµ(Op
n) and Ë†Ïƒ(Op
n) are the empirical estimators for the mean and the variance as
computed from the available p samples O1
n, . . . , OP
n of the random variables On. Applying both
approximations, the ï¬nal estimate for the "accuracy" is then given by
Î´ â‰ˆ1
N
N
X
n=1
Ïƒ(On)
Âµ(On) â‰ˆ1
N
N
X
n=1
1
âˆš
P
Ë†Ïƒ(Op
n)
On
.
(18)
G.3. Adaptivity procedure and customization
Initially, all particle ï¬lters start with the minimal number of particles (default is 4 or the number
of parallel workers for PF). Fitscore values above the customizable threshold (with the default value
at -2) indicate that the sampler is most probably out of the initialization phase, and hence the
particle ï¬lter adaptivity is activated.
The number of particles is then halved or doubled in each
likelihood estimation step, depending on whether the accuracy is above or below the requested
accuracy envelope. Requested accuracy envelope is determined by the accuracy and margins speciï¬ed
within the adaptive PF likelihood. The maximum number of particles is given by particles option
of PF. To guarantee the convergence of the posterior, the particle adaptivity is suspended after the
user-speciï¬ed "lock" sample batches.
41

H. SPUX executors - design
The schemes outlining designs of the SPUX executor (the manager side) and the corresponding SPUX
contract (the worker side) are listed in Figure 12 and Figure 13, respectively. In Figure 12, during
init
rootcall
connector
peers
connector.init
root
prepare
task.executor.setup
bootup
connector.bootup
exit
shutdown
connector.disconnect
connector.shutdown
map [pool]
connect
call [ensemble]
disconnect
execute [model]
contract
setup
rootcall
owner
task
addresscall
address
resources
port
task(s)
results
Figure 12: Execution ï¬‚ow scheme for the SPUX executor base class (the manager side). The thick
solid arrows represent the sequence of the called methods (the gray arrow indicates a call
at the top-level executor only), the thin solid arrows represent the required inputs, and the
dotted arrows represent the outputs (or inputs provided to subsequently called methods).
The tasks-to-results pipeline can then be executed multiple times, with the ï¬nal executor
exit() deallocating all computational resources.
the conï¬guration stage when executors are attached to SPUX components, only the setup of each
executor is called and only with the owner argument. Such pre-setup executors are then used to
compute the total required resources in framework init(...). After the conï¬guration stage, the
init(...)
of the executor attached to the main component (sampler) is called by framework
init(...) and initializes the connector (i.e. allocates computational resources). The same executor
init(...) method then starts a chain of recursive hierarchical initializations for all other executors.
In particular, the setup(...) of the executor attached to the task of the owner of the current
executor is called, where the task of the executor owner becomes the owner of the task executor,
and the addresscall() of the executor is used for the rootcall of the task executor. During
the bootup(...), each parallel worker is issued a contract describing required worker behavior, as
explained in Figure 13. In Figure 13, the manager and peers communicators are provided as the
peers
executor.rootcall
executor
port
taskroot
executor.init
taskport
connector.connect
manager
task.executor.bind
task(s)
executor.exit
manager
results
manager.Disconnect
manager.Disconnect
Figure 13: Execution ï¬‚ow scheme for the SPUX contract (the worker side) required in SPUX execu-
tors. The boxes with dashed outline represent the communicators and the dashed arrows
represent the information exchange over the network. The thick solid arrows represent
the sequence of the called methods, the thin solid arrows represent the required inputs,
and the dotted arrows represent the outputs (or inputs provided to subsequently called
methods). The pipeline part highlighted with the gray outline can be executed multiple
times.
arguments of the contract. Initially, the port (to connect back to manager) and the executor (to be
42

attached to incoming tasks) for each workerâ€™s contract is received by the workers from the manager
communicator. The taskroot (integer or string) is determined by the rootcall(...) of the task
executor, which is set in task.executor.setup (...), see Figure 12. The taskport (integer or
string) is determined by calling init(...) of the received task executor, completing this way the next
recursive step of hierarchical executor initializations. The contract then waits for further instructions
from the manager, for instance, to execute some tasks. The determined taskroot and taskport
are unique among all other peer task executors (initialized in the contracts of parallel workers at the
same hierarchical level), and hence can be used for binding any requested task to the task executor
initialized speciï¬cally for this contract. The results obtained by executing the received tasks are then
sent back to the manager.
43

