Beyond Chain-of-Thought, Effective
Graph-of-Thought Reasoning in Large Language
Models
Yao Yao1,2, Zuchao Li3,‚àóand Hai Zhao1,2,‚àó
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University
3National Engineering Research Center for Multimedia Software,
School of Computer Science, Wuhan University, Wuhan, 430072, P. R. China
yaoyao27@sjtu.edu.cn, zcli-charlie@whu.edu.cn,
zhaohai@cs.sjtu.edu.cn
Abstract
With the widespread use of large language models (LLMs) in NLP tasks, re-
searchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs
in accomplishing complex reasoning tasks by generating intermediate steps. How-
ever, human thought processes are often non-linear, rather than simply sequential
chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning,
which models human thought processes not only as a chain but also as a graph. By
representing thought units as nodes and connections between them as edges, our
approach captures the non-sequential nature of human thinking and allows for a
more realistic modeling of thought processes. Similar to Multimodal-CoT [1], we
modeled GoT reasoning as a two-stage framework, generating rationales first and
then producing the final answer. Specifically, we employ an additional graph-of-
thoughts encoder for GoT representation learning and fuse the GoT representation
with the original input representation through a gated fusion mechanism. We
implement a GoT reasoning model on the T5 pre-trained model and evaluate its
performance on a text-only reasoning task (GSM8K) and a multimodal reasoning
task (ScienceQA). Our model achieves significant improvement over the strong
CoT baseline with 3.41% and 5.08% on the GSM8K test set with T5-base and
T5-large architectures, respectively. Additionally, our model boosts accuracy from
84.91% to 91.54% using the T5-base model and from 91.68% to 92.77% using the
T5-large model over the state-of-the-art Multimodal-CoT on the ScienceQA test
set. Experiments have shown that GoT achieves comparable results to Multimodal-
CoTlarge with over 700M parameters, despite having fewer than 250M backbone
model parameters, demonstrating the effectiveness of GoT.
1
Introduction
In the field of human cognition, it has long been recognized that the human thought process is far
more complex and non-linear than could be captured by a simple, sequential chain of thoughts [2].
Human thinking is often characterized by its ability to make sudden leaps and connections between
seemingly unrelated ideas, which can lead to novel insights and solutions. This non-linear, jumping
thought process is a hallmark of human creativity, reasoning, and problem-solving abilities. However,
it also poses a significant challenge for cognitive modeling and understanding.
‚àóCorresponding author. ‚Ä† Equal contribution.
Preprint. Under review.
arXiv:2305.16582v1  [cs.CL]  26 May 2023

Recently, Large Language Models (LLMs) have been advancing at an unprecedented pace. With the
emergence of breakthroughs such as GPT-3 [3], PaLM [4], and GPT-4 [5], the field of natural language
processing has entered a new era of possibilities. Recent studies [6‚Äì8] have shown that the reasoning
ability of LLMs can be unlocked by Chain-of-Thought (CoT) prompting. CoT prompting involves a
series of intermediate natural language rationales that lead to the final answer. In addition, Zhang
et al. [1] have introduced Multimodal-CoT, which combines both language and visual modalities to
help surpass the limitations of textual information. More detailed related works about CoT can be
found in Appendix A.1.
Previous works on Chain-of-Thought (CoT) prompting, which have been limited to textual and
visual information, often represented the human reasoning process as sequential thought chains. This
approach overlooks the modeling of humans‚Äô jumping thought process and neglects to incorporate
the complex structural information of reasoning thoughts into the model. To address this limitation,
we propose the Graph-of-Thought (GoT), a novel approach to modeling human thought processes
not only as a chain but also as a graph. Our method is based on the assumption that the human mind
works by connecting and recombining ideas in a non-sequential, graph fashion, rather than following
a strict sequential chain. By representing thought units as nodes and connections between thoughts as
edges, the Graph-of-Thought captures the rich, non-sequential nature of human thinking and allows
for a more realistic and logical modeling of reasoning processes.
Do ferns produce seeds?
Text Features
(A) Yes
(B) No
This diagram shows the life cycle of 
a fern.
Vision Features (Optional)
Graph-of-Thought Features
produce
seeds
ferns
shows
life 
cycle
of
diagram
Fern plants reproduce using both asexual reproduction 
and sexual reproduction ‚Ä¶ The heart-shaped plant 
begins the fern's sexual reproduction stage ‚Ä¶ The mature 
fern can make spores and begin the fern life cycle again.
Rationale
Ferns do not produce seeds. Mature ferns produce spores, 
and heart-shaped plants produce eggs and sperm.
Answer
The answer 
is (B)
Graph-of-Thought with Rationale
produce
seeds
ferns
shows
life 
cycle
of
diagram
has
sexual 
production
stage
Figure 1: An example of GoT reasoning. Vision features are optional and are only required in
multimodal reasoning task.
An example of GoT reasoning is shown in Figure 1. Inspired by Multimodal-CoT [1], we have
adopted a two-stage reasoning framework. It first generates rationales and then generates the final
answer based on the predicted rationales. In addition to text features, graph features of GoT are
integrated during the rationale generation and answer inference. Specifically, GoT is first constructed
with an Extract-Cluster-Coreference (ECC) process, which simulates the deductive process in human
reasoning. We have used T5 [9] pre-trained language model as our backbone model. GoT is encoded
with a graph attention network and then fused with the original representation via a gated fusion
network.
Furthermore, we have also presented a multimodal GoT, which integrates not only text features and
GoT features but also visual features. For our experiments, we have used both UnifiedQA (T5)-base
and UnifiedQA (T5)-large [10] as our backbone models.
We implement GoT as a two-stage framework and fine-tuning language models and integrating text,
thought graph, and vision features for a more realistic and accurate reasoning process. GoT demon-
strates exceptional performance on both text-only GSM8K [11] and multimodal ScienceQA [12]
benchmarks, surpassing the accuracy of online system ChatGPT [5] by 25.08%, 14.46%, strong
baseline Multimodal-CoT [1] by 6.63%, and even exceeding human performance, establishing a new
state-of-the-art on ScienceQA test set with far more less parameters.
2

2
Graph-of-Thought
Thought Graph
Image (Optional)
Graph-of-Thought 
Constructor
Input Text 
Question: Do ferns 
produce seeds?
Choices: (A) Yes (B) No
Context: This diagram 
shows the life cycle of 
a fern.
Predicted 
Rationales
Input 
Encoder
GoT
Encoder
Text
encoder
Vision 
encoder
Graph 
Attention 
Network 
Transformer
Encoder
Feature
Extractor
Cross
Attention
Cross
Attention
Gated
Fusion
Layer
Transformer
Decoder
Stage 1
Predict Rationales
LectureÔºöFern plants reproduce 
using both asexual reproduction 
and sexual reproduction‚Ä¶
Solution:  Ferns do not produce 
seeds. Mature ferns produce 
spores‚Ä¶
The answer is (B).
Decoder
Output 
Feature Fusion
Stage 2
Stage 2
Predict Answers
Figure 2: Graph-of-Thought framework overview
The overview of our proposed GoT can be seen in Figure 2. Inspired by Multimodal-CoT [1], GoT
also adopts a two-stage framework. (1) Rationale generation stage: In the first stage, the model
generates rationales based on the input text (including question, context, and choices) the vision
features, and the generated thought graph corresponding to the input text. GoT employs independent
encoders to encode input data for each modality. We use a Transformer encoder to encode input text,
a vision encoder to encode an image, and a graph attention network to encode the thought graph.
The encoded features are further passed into cross-attention to align text tokens with image patches
and graph nodes, respectively. We then use a gated fusion layer to fuse these three features further
and pass them into the Transformer decoder to predict the target rationales. (2) Answer generation
stage: The second stage aims at generating the final answer and is largely similar to the first stage.
The main difference is that the input text is concatenated with the predicted rationales from the first
stage. It is worth noting that the above process describes a general multimodal reasoning framework.
However, for text-only reasoning tasks, there are no image features, so the image encoding and vision
feature fusion processes mentioned above can be omitted. In the following section, we will provide a
detailed exposition of the two key steps of our GoT reasoning framework: GoT construction and GoT
encoding and integration.
2.1
GoT Construction
The word earthquake 
comes from the words 
earth and quake. The word 
earth means ground, and 
the word quake means to 
shake.
Earthquake
comes 
from
earth
quake
means
ground
shake
GoT
Rationales
Figure 3: Graph-of-Thought deduction example
3

GoT employs thought graphs to simulate human deductive reasoning, thereby modeling humans‚Äô
ability for leaps of thought. Our aim is to reflect the most fundamental deduction process by con-
structing a thought graph. If we have evidence that x ‚Üíy and y ‚Üíz, then it follows that x ‚Üíz. In
Figure 3, the deduction reasoning can be formulated as follows: Earthquake
comes from
‚àí‚Üí
{earth, quake},
{earth, quake}
means
‚àí‚Üí{ground, shake}. It is easy to reason that Earthquake‚àí‚Üí{ground, shake}.
We propose a novel Extract-Clustering-
Coreference (ECC) process to construct
thought graphs. ECC first extracts deductive
triplets T = {ti = (ti
x, ti
y, ti
z)} as the discrete
raw graph, where ti
x, ti
y, and ti
z are thought
units of the i-th triplet, and there exists an
edge ei
xy between ti
x and ti
y, and an edge ei
yz
between ti
y and ti
z. Then, ECC clusters the
nodes that refer to the same mentions to con-
duct coreference resolution. Specifically, we
replace every graph node that belongs to a
coreference cluster with the most representa-
tive mention in the cluster. By adopting this
technique, our model is better equipped with
denser thought graphs and the ability for de-
ductive reasoning. The detailed algorithm is
illustrated in Algorithm 1.
Algorithm 1 ECC process
Input: Input text S
Output: Thought graph G(N, E)
Extract deductive triplet set T from S
T = {t0, t1, ..., tn}, ti = (ti
x, ti
y, ti
z)
for every triplet ti ‚ààT do
Nr ‚ÜêNr ‚à™{ti
x, ti
y, ti
z}
Er ‚ÜêEr ‚à™{ei
xy, ei
yz}
end for
extract coreference clusters C for Nr
for every node ni ‚ààNr do
if ni ‚àà‚àÄcj ‚ààC then
n‚àó
j ‚Üêmost representative mention in cj
N ‚ÜêN ‚à™{n‚àó
j}
end if
end for
Reconnect N based on Er to construct E
return N , E
In GoT construction, during the rationale generation stage, the input text consists of concatenated
question, context, and choices. In multimodal GoT, image caption [12] is appended to the input text
for GoT to incorporate image information. During the answer inference stage, the predicted rationales
from the rationale generation stage are further concatenated with the input text for corresponding
GoT construction.
In our implementation of ECC process, inspired by [13], we utilize open information extraction (Ope-
nIE) systems 2 [14] to extract subject-verb-object triplets as thought unit nodes. We apply coreference
resolution to the extracted nodes using the Stanford CoreNLP system [15]. The constructed thought
graph is denoted as G(N, E), where N represents the nodes extracted by OpenIE and E represents
the adjacency matrix. Rows and columns correspond to the nodes in the graph, and if there is an edge
between two nodes, the corresponding matrix element is 1; otherwise, it is 0.
2.2
GoT Encoding and Integration
GoT reasoning utilizes separate encoders to encode input data for each modality. The thought graph
is encoded using a graph attention network, while the input text is encoded using a Transformer
encoder. In multimodal GoT reasoning, the image is encoded using an additional vision encoder.
2.2.1
Base Encoder
Text Encoder
For text representation, we use the Transformer encoder (e.g. T5 [9]) to encode the
input text. Given input sentence S = {w0, ..., wl}, we extract the hidden states from the last layer of
the Transformer encoder to obtain the text representation HT :
HT = {h0, h1, ..., hl} = Encodertext(S)
(1)
where hi is the hidden representation of token i and l represents the length of the text input.
Vision Encoder (Optional)
For multimodal reasoning where vision modality is required, follow-
ing [1], we extract patch-level features of image I using readily available vision extraction model
2https://github.com/philipperemy/Stanford-OpenIE-Python
4

as vision encoder Encodervision and then employ a trainable projection matrix WI to project the
extracted features into the vision representation HI which have the same shape with HT .
HI = WIEncodervision(I)
(2)
2.2.2
GoT Encoder
Node Embedding
We first use special tokens <s>
and </s> to highlight every thought graph node.
Specifically, for node set with j nodes N = {n0, ...nj}
, we construct the node input as p. we then feed the p
into the same text encoder and utilize the output repre-
sentation of the special token <s> as the initial node
representation. Formally,
p = [<s>, n0, </s>, ..., <s>, nj, </s>]
(3)
[hs
0, hn
0, he
0, ..., hs
j, hn
j , he
j] = Encodertext(p)
(4)
where the hs
i and he
i ‚ààRD are the representation of <s>
and </s> for node ni respectively, D is the dimension
of node embedding, and the hn
i = {hn
i,1, ..., hn
i,m} is
the representations of node ni with m tokens. we use
the hs
i to represent the node representation of ni.
GAT Encoder
We employ a graph attention network
(GAT) [16, 13] to encode the thought graph. For every
node ni in graph G(N, E), the graph attention layer
is designed as:
Dropout
GoT input
G ùëÅ, ùê∏
Graph 
Attention Layer
Graph 
Attention Layer
Concatenate
Dropout
Graph 
Attention Layer
FFNN
Layernorm
GoT representation
Multi-head 
attention
Residual connection
‚Ñéùëî‚Ä≤
‚Ñéùëî‚Ä≤
ùêªùê∫
‚Ä¶
Figure 4: Architecture of GoT encoder
aij = Attention(

Whs
i||Whs
j

);
qij = LeakyReLU (aij)
(5)
Œ±ij = Softmax(qij) =
exp (qij)
P
k‚ààKi exp (qik);
hg‚Ä≤
i = GELU
Ô£´
Ô£≠X
j‚ààKi
Œ±ijWhs
j
Ô£∂
Ô£∏
(6)
where || denotes concatenate operation, the W is a trainable weight and the set Ki contains the node
ni‚Äôs neighbours in thought graph G. Our graph attention layer first employed a shared attention
mechanism Attention(.) : RD‚Ä≤ √ó RD‚Ä≤ ‚ÜíR to compute the attention weights, where D‚Ä≤ is the
attention layer output dimension. The attention weights aij measures the importance of node ni‚Äôs
features to nj‚Äôs features. By only calculating the attention weights between nodes who are neighbours,
our graph attention layer demonstrates the ability to perceive structural information of graphs. In
our implementation, we adopt a single-layer feed-forward neural network (FFNN) as the attention
mechanism which is both simple and straight-forward.
The architecture of our GoT encoder can be seen in Figure 4. Our GoT encoder employs a multi-head
graph attention layer, following [16], we concatenate the output of each graph attention layer and
further pass it to a output graph attention layer with the same architecture:
hg‚Ä≤
i = ‚à•K
k=1GELU
Ô£´
Ô£≠X
j‚ààNi
Œ±k
ijWkhs
j
Ô£∂
Ô£∏;
hg‚Ä≤‚Ä≤
i
= GELU
Ô£´
Ô£≠X
j‚ààNi
Œ±ijWhg‚Ä≤
j
Ô£∂
Ô£∏
(7)
where K is the number of attention heads, || is the concatenate operation, and n is the number of
nodes in thought graph. We then use a single-layer feed-forward neural network (FFNN) to obtain
the final thought graph embedding HG:
hg‚Ä≤‚Ä≤ = [hg‚Ä≤‚Ä≤
0 , ..., hg‚Ä≤‚Ä≤
n ];
HG = FFNN(hg‚Ä≤‚Ä≤)
(8)
5

2.3
Feature Fusion
After obtaining the encoded features, we use a single head attention to align the text representation
HT with image representation HI and thought graph representation HG, respectively. The image
attention output HI and thought graph attention output HG are calculated by:
HI = Softmax
HT HI‚ä§
‚àö
d

HI;
HG = Softmax
HT HG‚ä§
‚àö
d

HG
(9)
where Q is HT and d is the dimension of HT . We take both KI and VI as HI and KG and VG as
HG. Please note that image representation is optional and is only required for multimodal dataset.
Next, a gated fusion mechanism [17, 1, 18, 19] is applied to combine the attention outputs HI and
HG with the text representation HT . The feature fusion output H can be calculated by:
Œª =
(
Sigmoid
 WT HT + WGHG
text-only reasoning
Sigmoid
 WT HT + WIHI + WGHG
multimodal reasoning
(10)
H =
(
(1 ‚àíŒª) ¬∑ HT + Œª ¬∑ HG
text-only reasoning
(1 ‚àíŒª) ¬∑ HT + Œª ¬∑ HI + Œª ¬∑ HG
multimodal reasoning
(11)
where WT ,WI and WG are all trainable weights. We then input the fused feature output H into the
decoder to predict the rationales or the final answer.
3
Experiments
Dataset
We evaluate our model on the text-only GSM8K [11] and multimodal ScienceQA bench-
mark [12]. GSM8K benchmark comprises 8.5K meticulously crafted grade school math problems
with annotated 2 to 8 problem solution steps. For GSM8K, the model is trained to reasoning through
the steps to generate the final answer. ScienceQA benchmark is the pioneering large-scale dataset
for multimodal science questions, equipped with comprehensive annotations for answers, including
detailed lectures and explanations. The dataset contains 21k questions covering three subjects: natural
science, language science, and social science. Each question is presented with a context in the form
of natural language or an optional image. The model is trained to elucidate the reasoning process in
natural language while choosing the answer from a set of options. The detailed dataset statistics are
shown in Appendix A.2.
Model Setup
In our experiments, we used T5 [9] as our basic model architecture, including both
T5-base and T5-large model sizes. Specifically, to ensure a fair comparison, we initialized our model
with the pre-trained T5 checkpoint - UnifiedQA [10] and used DETR [20] for the vision encoder,
following [7]. We fine-tuned the models for 50 epochs with a learning rate of 5e-5. The detailed
training parameters are available in Appendix A.3. We trained our models on four NVIDIA GeForce
RTX 4090 24G GPUs.
4
Results and Discussion
4.1
Main Results
Baselines
For GSM8K, our baselines include: (1) few-shot LLMs including GPT-3 [21], GPT-
3.5 [5], GPT-4 [5], and code-davinci-002 [22] (2) LLMs with CoT: To have a fair comparison we
also fine-tuned UnifiedQAbase and UnifiedQAlarge [10] on GSM8K with traditional two-stage CoT.
For ScienceQA, following [1, 12], our adopted baselines include: (1) Vision question answering
(VQA) baseline models [23‚Äì30]; (2) Text-to-text LLMs [31, 32] and (3) Text-to-text LLMs with
CoT prompting [12, 1]. Both UnifiedQA [12] and GPT-3.5 [12] use generated image captions to
incorporate vision semantics. Whereas, Mutimodal-CoT [1] injects generated image features into
traditional CoT reasoning.
6

Table 1: Rationale generation results (%). (*: we re-run the Mutimodal-CoTbase to report the full
rouge scores)
MODELS
ROUGE-L
ROUGE-1
ROUGE-2
ROUGE-LSUM
GSM8K
UnifiedQAbase [31]
70.61
75.32
51.13
70.24
UnifiedQAlarge [31]
72.83
76.91
54.25
72.34
GoT-T5base
71.08
75.46
51.85
70.61
GoT-T5large
72.91
76.93
54.57
72.45
ScienceQA
Mutimodal-CoT‚àó
base [1]
96.98
97.26
94.00
97.16
GoT-T5base
98.29
98.43
96.23
98.37
GoT-T5large
98.35
98.45
96.30
98.41
Results
The rationales generation results can be seen in Table 1. The overall results are reported in
Table 2 and Table 3. On the GSM8K dataset, for rationale generation in the first stage, our GoTbase
model achieves a 0.47 improvement in ROUGE-L compared to the UnifiedQAbase model that did
not incorporate GoT and the GoTlarge model achieves a 0.08 improvement. In the second stage of
answer generation, the GoT base model showed a 3.41% increase in accuracy, while the GoT large
model achieved a 5.08% improvement. GoT outperformed GPT-3 by 27.18% in accuracy while using
significantly fewer parameters than GPT-3. Although GPT-4 achieves a result of 92%, there is a high
probability it has over 175 billion parameters. Our model, compared to UnifiedQAlarge, reduces the
accuracy gap by 5%.
For ScienceQA dataset, in rationale generation stage, we can see from Table 1 that our model
achieves a ROUGE-L of 98.29 and outperforms the Mutimodal-CoTbase by 1.31. For the final answer
generation stage, our GoT achieves SOTA in all subjects and all grades. The most direct comparison
is that our model achieves an accuracy of 91.68% which is 6.77% higher than that of the Mutimodal-
CoTbase with the similar number of parameters and is competitive to the Mutimodal-CoTlarge with
738M parameters.
We can observe from Table 1 that the impact of GoT on rationale generation is limited. We attribute
this limitation to the fact that the input text for thought graph construction only includes questions
and choices. Consequently, the thought graph constructed from such limited information can only
facilitate constrained deductive reasoning. However, in the answer generation stage, when provided
with rationales, the model needs to possess stronger deductive reasoning capabilities to understand the
relationship between rationales, questions, and choices. Therefore, GoT demonstrates a significant
advantage over traditional CoT, elevating the accuracy from 62.70% to 66.11% in GSM8K and
from 84.91% to 91.54% in ScienceQA task. The results sufficiently suggest that utilizing thought
graph features for deductive reasoning is a more effective approach than the existing methods, which
only consider text or vision features by simply incorporating image captions or fusing generated
image features. In conclusion, our results confirm the effectiveness of utilizing two-dimensional
graph-of-thought and demonstrate the potential of incorporating GoT into reasoning for LLMs.
Table 2: Main test accuracy results (ACC%) of GSM8K. Size=backbone model size.
MODELS
TRAINING
SIZE
ACC(%)
GPT-3 [21]
train-set
175B
55.00
code-davinci-002 [22]
few-shot
175B
68.01
GPT-3.5 [5]
few-shot
-
57.10
GPT-4 [5]
few-shot
-
92.00
UnifiedQAbase [10]
train-set
223M
62.70
GoT-T5base
train-set
223M
66.11
UnifiedQAlarge [10]
train-set
738M
77.10
GoT-T5large
train-set
738M
82.18
7

Table 3: Main test accuracy results (%) of ScienceQA. SIZE=backbone model size. Question classes:
NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG =
image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12, AVG= average accuracy
scores
MODEL
TRAINING
SIZE
NAT
SOC
LAN
TXT
IMG
NO
G1-6
G7-12
AVG
Human
-
-
90.23
84.97
87.48
89.60
87.50
88.10
91.59
82.42
88.40
Vision question answering baselines
MCAN [23]
train-set
95M
56.08
46.23
58.09
59.43
51.17
55.40
51.65
59.72
54.54
Top-Down [24]
train-set
70M
59.50
54.33
61.82
62.90
54.88
59.79
57.27
62.16
59.02
BAN [25]
train-set
112M
60.88
46.57
66.64
62.61
52.60
65.51
56.83
63.94
59.37
DFAF [26]
train-set
74M
64.03
48.82
63.55
65.88
54.49
64.11
57.12
67.17
60.72
ViLT [27]
train-set
113M
60.48
63.89
60.27
63.20
61.38
57.00
60.72
61.90
61.14
Patch-TRM [28]
train-set
90M
65.19
46.79
65.55
66.96
55.28
64.95
58.04
67.50
61.42
VisualBERT [29, 30]
train-set
111M
59.33
69.18
61.18
62.71
62.17
58.54
62.96
59.92
61.87
Text-to-text LLMs
UnifiedQAbase [31]
zero-shot
223M
68.16
69.18
74.91
63.78
61.38
77.84
72.98
65.00
70.12
GPT-3.5 [32]
zero-shot
175B
74.64
69.74
76.00
74.44
67.28
77.42
76.80
68.89
73.97
Text-to-text LLMs with CoT
UnifiedQAbase (CoT) [12]
zero-shot
223M
71.00
76.04
78.91
66.42
66.53
81.81
77.06
68.82
74.11
GPT-3.5 (CoT) [12]
2-shot
175B
75.44
70.87
78.09
74.68
67.43
79.93
78.23
69.68
75.17
ChatGPT (CoT) [33]
few-shot
-
78.82
70.98
83.18
77.37
67.92
86.13
80.72
74.03
78.31
GPT-4 (CoT) [33]
few-shot
-
85.48
72.44
90.27
82.65
71.49
92.89
86.66
79.04
83.99
Mutimodal-CoTbase [1]
train-set
223M
87.52
77.17
85.82
87.88
82.90
86.83
84.65
85.37
84.91
GoT-T5base
train-set
223M
92.51
88.98
91.61
92.39
90.84
92.33
91.68
91.27
91.54
¬± 0.24
¬± 0.37
¬± 0.78
¬± 0.23
¬± 0.39
¬± 0.60
¬± 0.05
¬± 0.36
¬± 0.12
Mutimodal-CoTlarge [1]
train-set
738M
95.91
82.00
90.82
95.26
88.80
92.89
92.44
90.31
91.68
GoT-T5large
train-set
738M
96.51
82.26
93.61
96.56
89.56
94.29
93.83
90.86
92.77
¬± 0.25
¬± 0.21
¬± 0.19
¬± 0.26
¬± 0.29
¬± 0.10
¬± 0.18
¬± 0.38
¬± 0.18
4.2
Further Exploration
4.2.1
Ablation Study
In order to make sure that: (1) our GoT‚Äôs performance gain is not simply due to the increase of
parameters. We conduct an ablation study where we enlarge the number of parameters of Mutimodal-
CoTbase to the same size 233M with our model. The enlarged model is denoted as Mutimodal-
CoTbase(enlarged). (2) introducing thought graphs into GoT reasoning indeed boost the performance.
We construct a random thought graph by randomly select graph nodes. (3) the multi-head attention
mechanism in GoT encoder is necessary. We employ a single-head attention. The overall ablation
results can be found in Table 4.
Table 4: Ablation results of GoT.
MODEL
MODEL SIZE
G1-6
G7-12
AVG
‚àÜ
GoT-T5base
233M
91.68
91.27
91.54
-
w/ Random Connection
91.23
90.18
90.85
-0.69
w/ Single-head attention
91.08
90.77
90.97
-0.53
Mutimodal-CoTbase(enlarged)
233M
89.28
87.21
88.54
-3.00
From the table, we can see that our model significantly outperforms the enlarged Mutimodal-CoT by
an accuracy of 3.00%. The results sufficiently proved the importance of introducing thought graphs
into multimodal reasoning. When reducing the multi-head attention to single-head attention, GoT
suffers a loss of 0.53% accuracy, indicating the necessity of multi-head attention mechanism for GoT
encoder. By randomly construct thought graphs to disrupt the deductive reasoning process, our model
suffers a loss of 0.69%, indicating the effectiveness of GoT.
4.2.2
Analysis
Performance on Different Classes
In order to investigate the impact of GoT on the overall
model performance across different subjects , we calculated the accuracy for different subjects and
compared it with that of enlarged Mutimodal-CoT. We also compare the performance of two models
on different question classes.The radar Figure 5 shows the overall results for our base model. With
respect to various subjects and question classes, our model demonstrates superior performance over
8

the Mutimodal-CoTbase and attains a more consistent and enhanced outcome. Our model presents
outstanding advantages especially in the field of social science, with an accuracy improvement of
8.01%. For different question classes, our model demonstrates the largest improvement on questions
involving images. Our hypothesis is that by constructing a thought graph and integrating the three
features of text, image, and thought graph, we can better align the textual and visual information
for the model, thus maximizing the utilization of visual information and obtaining more accurate
answers.
75.00
80.00
85.00
90.00
95.00
100.00
NAT
SOC
LAN
TXT
IMG
NO
Mutimodal-CoT(enlarged)
Ours(base)
Figure 5: Performance on different question
classes
5
10
80
85
90
95
100
Grades
Accuracy(%)
Oursbase
Mutimodal-CoTbase(enlarged)
Figure 6: Performance on different grades
Performance on Different Grades
It can be seen from the Table 4 that the enlarged Mutimodal-
CoT experience a decrease in accuracy of 2.07 as the grade level of the given question increases
while GoT only has minor decrease of 0.41. We believe the main reason is that by incorporating
GoT, models acquires the ability for deductive reasoning and can better comprehend the relationships
between different entities and thus better understand the meaning of the problems. Through this
method, for higher-grade problems with greater complexity, the model can construct a thought graph
to help itself generate a more complete logical chain for deduction, thereby generating more accurate
answers. More detailed model performance on different grades can be found in Figure 6. We can see
from the figure that in the lower grade, two models achieves a similar performance. As the grade level
increases and the difficulty of the questions becomes more challenging, the gap between our model
and the Mutimodal-CoT model gradually widens. Due to the small number of questions (‚â§130)
available for each grade in grade 1 and grades 9-12, there is greater fluctuation in the accuracy of
both models. Nevertheless, it is evident from the table that our model exhibits stronger and more
stable advantages over Mutimodal-CoT in each grade.
Case Study and Limitation
In order to gain a deeper understanding of the performance of GoT, we
conduct a manual investigation of randomly selected examples generated by our approach which can
be found in Appendix A.4. We also visualize the attention weights aij in GoT encoder to demonstrate
how GoT performs deductive reasoning to generate more accurate answers in Appendix A.5
For the limitation of this work, compared to CoT, GoT may result in additional computational costs
and slightly slower training times. Detailed limitation analysis can be found in Appendix A.6.
5
Conclusion
We introduce a novel Graph-of-Thought (GoT) reasoning approach, which is an innovative method
for modeling the non-sequential nature of human thinking within large language models (LLMs).
GoT enhances LLMs with deductive reasoning abilities, providing a more realistic representation of
9

thought processes. Our experiments showcases the superiority of GoT on the text-only reasoning
dataset, achieving an accuracy of 82.18% on the GSM8K test set, outperforming GPT-3 significantly
while utilizing significantly fewer parameters. Furthermore, GoT establishes a new state-of-the-art
on the multimodal reasoning benchmark, ScienceQA, achieving an impressive accuracy of 92.77%
with fewer parameters. This performance surpasses strong ChatGPT and GPT-4 systems, as well as
human performance, demonstrating the efficacy of GoT. Through comprehensive case studies and
ablation studies, we provide substantial evidence of the effectiveness of GoT in reasoning tasks. If
you want it, you GoT it!
References
[1] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multi-
modal chain-of-thought reasoning in language models. CoRR, abs/2302.00923, 2023. doi: 10.
48550/arXiv.2302.00923. URL https://doi.org/10.48550/arXiv.2302.00923.
[2] Lawrence W Barsalou. Perceptual symbol systems. Behavioral and brain sciences, 22(4):
577‚Äì660, 1999.
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot learn-
ers. In Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,
Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,
James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,
Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant
Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek
Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,
Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor
Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,
Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language
modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL
https://doi.org/10.48550/arXiv.2204.02311.
[5] OpenAI. Gpt-4 technical report. 2023. URL https://cdn.openai.com/papers/
gpt-4.pdf.
[6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and
Denny Zhou. Chain of thought prompting elicits reasoning in large language models. CoRR,
abs/2201.11903, 2022. URL https://arxiv.org/abs/2201.11903.
[7] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. Self-
consistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171,
2022. doi: 10.48550/arXiv.2203.11171. URL https://doi.org/10.48550/arXiv.
2203.11171.
[8] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting
in large language models. CoRR, abs/2210.03493, 2022. doi: 10.48550/arXiv.2210.03493.
URL https://doi.org/10.48550/arXiv.2210.03493.
10

[9] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research, 21(140):1‚Äì67, 2020. URL
http://jmlr.org/papers/v21/20-074.html.
[10] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark,
and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single QA system. In
Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational
Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of
Findings of ACL, pages 1896‚Äì1907. Association for Computational Linguistics, 2020. doi:
10.18653/v1/2020.findings-emnlp.171. URL https://doi.org/10.18653/v1/2020.
findings-emnlp.171.
[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christo-
pher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR,
abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.
[12] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought
chains for science question answering. In The 36th Conference on Neural Information Process-
ing Systems (NeurIPS), 2022.
[13] Jiaao Chen and Diyi Yang. Structure-aware abstractive conversation summarization via discourse
and action graphs. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-
T√ºr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors,
Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-
11, 2021, pages 1380‚Äì1391. Association for Computational Linguistics, 2021. doi: 10.18653/v1/
2021.naacl-main.109. URL https://doi.org/10.18653/v1/2021.naacl-main.
109.
[14] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. Leveraging
linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pages 344‚Äì354, Beijing,
China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1034. URL
https://aclanthology.org/P15-1034.
[15] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David
McClosky. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computational Linguistics: System Demonstrations,
pages 55‚Äì60, Baltimore, Maryland, June 2014. Association for Computational Linguistics. doi:
10.3115/v1/P14-5010. URL https://aclanthology.org/P14-5010.
[16] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li√≤, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=rJXMpikCZ.
[17] Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, and Ben Kao. Good for misconceived reasons:
An empirical revisiting on the need for visual context in multimodal machine translation. In
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),
pages 6153‚Äì6166, Online, August 2021. Association for Computational Linguistics. doi: 10.
18653/v1/2021.acl-long.480. URL https://aclanthology.org/2021.acl-long.
480.
[18] Bei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong Xiao, Anxiang Ma, and JingBo Zhu. On
vision features in multimodal machine translation. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6327‚Äì6337,
Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.
acl-long.438. URL https://aclanthology.org/2022.acl-long.438.
11

[19] Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, and
Hai Zhao. Neural machine translation with universal visual representation. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net, 2020. URL https://openreview.net/forum?id=Byl8hhNYPS.
[20] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kir-
illov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Andrea
Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision
- ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceed-
ings, Part I, volume 12346 of Lecture Notes in Computer Science, pages 213‚Äì229. Springer,
2020.
doi: 10.1007/978-3-030-58452-8\_13.
URL https://doi.org/10.1007/
978-3-030-58452-8_13.
[21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. CoRR, abs/2205.11916, 2022. doi: 10.48550/arXiv.
2205.11916. URL https://doi.org/10.48550/arXiv.2205.11916.
[22] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Olivier Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting enables
complex reasoning in large language models. CoRR, abs/2205.10625, 2022. doi: 10.48550/
arXiv.2205.10625. URL https://doi.org/10.48550/arXiv.2205.10625.
[23] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
Deep modular co-attention
networks for visual question answering. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6281‚Äì6290.
Computer Vision Foundation / IEEE, 2019.
doi:
10.1109/CVPR.2019.00644.
URL
http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Deep_
Modular_Co-Attention_Networks_for_Visual_Question_Answering_
CVPR_2019_paper.html.
[24] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and vi-
sual question answering.
In 2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6077‚Äì6086.
Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.
00636.
URL http://openaccess.thecvf.com/content_cvpr_2018/html/
Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html.
[25] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. In Samy Ben-
gio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol√≤ Cesa-Bianchi, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al,
Canada, pages 1571‚Äì1581, 2018. URL https://proceedings.neurips.cc/paper/
2018/hash/96ea64f3a1aa2fd00c72faacf0cb8ac9-Abstract.html.
[26] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven C. H. Hoi, Xiaogang Wang, and
Hongsheng Li. Dynamic fusion with intra- and inter-modality attention flow for visual question
answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,
Long Beach, CA, USA, June 16-20, 2019, pages 6639‚Äì6648. Computer Vision Foundation
/ IEEE, 2019. doi: 10.1109/CVPR.2019.00680. URL http://openaccess.thecvf.
com/content_CVPR_2019/html/Gao_Dynamic_Fusion_With_Intra-_and_
Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.html.
[27] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without
convolution or region supervision. In Marina Meila and Tong Zhang, editors, Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event, volume 139 of Proceedings of Machine Learning Research, pages 5583‚Äì5594. PMLR,
2021. URL http://proceedings.mlr.press/v139/kim21k.html.
[28] Pan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan
Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understand-
ing and visual language reasoning.
In Joaquin Vanschoren and Sai-Kit Yeung, editors,
12

Proceedings of the Neural Information Processing Systems Track on Datasets and Bench-
marks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL
https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/
hash/d3d9446802a44259755d38e6d163e820-Abstract-round2.html.
[29] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A
simple and performant baseline for vision and language. CoRR, abs/1908.03557, 2019. URL
http://arxiv.org/abs/1908.03557.
[30] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. What does BERT
with vision look at? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020, pages 5265‚Äì5275. Association for Computational Linguistics,
2020. doi: 10.18653/v1/2020.acl-main.469. URL https://doi.org/10.18653/v1/
2020.acl-main.469.
[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67, 2020. URL http://jmlr.
org/papers/v21/20-074.html.
[32] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E.
Hinton.
Big self-supervised models are strong semi-supervised learners.
In Hugo
Larochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.
URL https://proceedings.neurips.cc/paper/2020/hash/
fcbc95ccdd551da181207c0c1400c655-Abstract.html.
[33] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-
Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large
language models. CoRR, abs/2304.09842, 2023. doi: 10.48550/arXiv.2304.09842. URL
https://doi.org/10.48550/arXiv.2304.09842.
13

A
Appendix
A.1
Related Works
In chain-of-thought reasoning, one idea leads to the next in a logical sequence and builds on previous
knowledge. Each idea is supported by evidence or reasoning, and the conclusions drawn from the
chain are logical and sound. Most CoT methods can be divided into two categories based on how to
generate the final answer: (1) prompting for CoT, including zero-shot CoT and few-shot CoT; and (2)
fine-tuning for CoT.
Zero-shot CoT Prompting
As large language models continue to advance rapidly, many re-
searchers are beginning to explore CoT reasoning for LLMs. The zero-shot CoT method proposed
by Kojima et al. [21] consists of two stages: (1) adding a "Let‚Äôs think step by step" prompt to generate
CoT, and (2) concatenating the generated CoT and adding the phrase "So the answer is" to obtain the
final answer.
Few-shot CoT Prompting
Few-shot CoT reasoning for LLMs, however, utilizes multiple input-
output pairs to prompt the LLMs to output CoT and obtain the final answer. Due to its ability to
provide better performance compared to Zero-shot CoT, Few-shot CoT has gained more attention in
research, particularly through effective demonstrations. Few-shot CoT prompting was first formally
explored by Wei et al. [6] and is a form of discrete prompt learning that involves context learning
in large models. Compared to traditional in-context learning, which prompts LLMs with a list of
input-output demonstration pairs along with a test input to allow the model to predict output, Few-shot
CoT prompting outputs additional logical reasoning procedures apart from the target output. Wang
et al. [7] proposed a follow-up method to [6]. The main improvement is that the model uses the
majority vote for the answers, which was found to significantly improve the performance of the
CoT. However, these few-shot CoT models depend on hand-crafted demonstrations. To solve this
problem, Zhang et al. [8] proposed Auto-CoT, which maintains the diversity of sampled questions
and generates reasoning chains to automatically construct demonstrations. Specifically, Auto-CoT
consists of two main stages: (1) Problem clustering: divide the given dataset of problems into several
clusters; (2) Demonstration sampling: select a representative problem from each cluster and use a
simple heuristic method to generate its reasoning chain. Furthermore, Lu et al. [33] also explores
few-shot CoT reasoning for recently popular LLMs ChatGPT and GPT-4 [5].
CoT Fine-tuning
In Zhang et al. [1], it was proposed to fine-tune smaller language models instead
of prompting them in LLMs. And this approach enabled the CoT to go beyond textual information
and incorporate visual (image) modalities using a gated fusion mechanism into a two-stage CoT. The
results demonstrated that CoT fine-tuning with fewer parameters has potential. Therefore, in this
work, we focus on fine-tuning for CoT to reduce the number of required model parameters and help
LLMs better comprehend different modalities. However, previous CoT research has been limited
to different modalities, such as textual and vision information, without considering the deduction
reasoning process. Therefore, in this work, we move beyond modeling the reasoning process solely
as a thought chain and elevate it to a thought graph. We provide a more comprehensive and nuanced
representation, enabling LLMs to perceive the deduction reasoning process accurately, resulting in
more precise answer generation.
14

A.2
Dataset statistics
Splits
#Problems
Train
7,473
Test
1,319
Table 5: GSM8K dataset statistics (# denotes
numbers)
Statistic
Number
Splits
#Train
12,726
#Dev
4,241
#Test
4,241
#Total
21,208
Attribute
#Subjects
3
#Topic
26
#Category
127
#Skill
379
Table 6: ScienceQA dataset statistics (# denotes
numbers)
A.3
Training Parameters
Parameters
Value
Epochs
50
Batch size for T5-base (per device)
4
Batch size for T5-large (per device)
2
Learning rate
5e-5
Weight decay
0.01
Max input length
512
Max number of nodes
150
Table 7: Training parameters for GoT
A.4
Case Study
To facilitate a more illustrative comparison between GoT and the CoT, we have selected several
representative examples. Figure 11 demonstrates examples for GSM8K dataset. Figure 8 to Figure
10 illustrates examples from ScienceQA dataset. From Figure 7 and Figure 8, we can see that GoT
can better understand the rationales and generate more accurate result. In Figure 9, we can see that
when provided with wrong rationale, our model is more robust to the noise and can focus on more
important key information. (We highlight the noisy wrong rationale in red and correct key rationale in
green). Figure 10 presents a language problem which have less context and requires a certain amount
of common sense knowledge. Hence, the impact of constructing a mind map on enhancing the model
is not significant. Therefore, both GoT and CoT predict wrong answers.
A.5
Representation Visualization
In order to demonstrate the deductive reasoning process of GoT more intuitively, we visualized the
attention weights of the GoT encoder. The visualization results can be found in Figure 12. We took
Figure 9 as an example. In Figure 9, even given a wrong rationale, GoT still manages to generate the
right answer. We select 14 representative thought nodes and found that "blue","color", and "common"
have the greatest weights which indicates that GoT guides the model to focus on more important
words and conduct correct deductive reasoning. For the disruptive node "a hard object," our model
can effectively discriminate against it and assign a lower attention weight to prevent the model from
selecting incorrect answers, as traditional CoT models often do due to erroneous rationales.
A.6
Limitation
Compared to Mutimodal-CoT [1], incorporating GoT may result in additional computational costs
and slightly slower training times. The training parameters and inference times of the different
models are presented in Table 8, which reveals that our model requires a 0.2% increase in parameters
compared to Mutimodal-CoT.
15

Table 8: The number of training parameters and inference time of different models (# denotes
numbers)
#Parameters
Inference time
(eval samples/per second)
Mutimodal-CoTbase [1]
227M
16.33
Ours
233M
13.38
16

Dataset
GoT Prediction
CoT Prediction
Question: Would you find the word pink on a dictionary page with the following guide words?
parrot ‚Äì property
Choices:
(A) yes
(B) no
Rationale: lecture: Guide words appear on each page of a dictionary. They tell you the first
word and last word on the page. The other words on the page come between the guide
words in alphabetical order. To put words in alphabetical order, put them in order by their
first letters. If the first letters are the same, look at the second letters. If the second letters
are the same, look at the third letters, and so on. If one word is shorter, and there are no
more letters to compare, then the shorter word comes first in alphabetical order. For
example, be comes before bed.
solution: Put the words in alphabetical order. Since pink is between the guide words parrot -
property, it would be found on that page.
Answer: The answer is (A)
Rationale: Solution: Guide words appear on each page of a dictionary. They tell you the first
word and last word on the page. The other words on the page come between the guide
words in alphabetical order. To put words in alphabetical order, put them in order by their
first letters. If the first letters are the same, look at the second letters. If the second letters
are the same, look at the third letters, and so on. If one word is shorter, and there are no
more letters to compare, then the shorter word comes first in alphabetical order. For
example, be comes before bed. Put the words in alphabetical order. Since pink is between
the guide words parrot - property, it would be found on that page.
Answer: The answer is (A)
Rationale: Solution: Guide words appear on each page of a dictionary. They tell you the first
word and last word on the page. The other words on the page come between the guide
words in alphabetical order. To put words in alphabetical order, put them in order by their
first letters. If the first letters are the same, look at the second letters. If the second letters
are the same, look at the third letters, and so on. If one word is shorter, and there are no
more letters to compare, then the shorter word comes first in alphabetical order. For
example, be comes before bed. Put the words in alphabetical order. Since pink is between
the guide words parrot - property, it would be found on that page.
Answer: The answer is (B)
right rationales right answer
right rationales wrong answer
Figure 7: Examples of ScienceQA
17

Dataset
GoT Prediction
CoT Prediction
Question: What is the name of the colony shown?
Choices:
(A) New Hampshire
(B) Vermont
(C) New York
Rationale: The colony is New York. During the
colonial era, New Hampshire and New York both
claimed the territory that would later become the
state of Vermont. Vermont was never its own
colony.
Answer: The answer is (C)
Rationale: Solution: The colony is New York. During the colonial era, New Hampshire and
New York both claimed the territory that would later become the state of Vermont.
Vermont was never its own colony.
Answer: The answer is (C)
Rationale: Solution: The colony is Delaware York. During the colonial era, New Hampshire
and New York both claimed the territory that would later become the state of Vermont.
Vermont was never its own colony.
Answer: The answer is (D)
(D) Delaware
right rationales right answer
wrong rationales wrong answer
Figure 8: Examples of ScienceQA
18

Dataset
GoT Prediction
CoT Prediction
Question: Which property do these three 
objects have in common?
Choices:
(A) blue
(B) hard
(C) sticky
Rationale: lecture: An object has different properties. A property of an object can tell you
how it looks, feels, tastes, or smells. Properties can also tell you how an object will behave
when something happens to it. Different objects can have properties in common. You can
use these properties to put objects into groups.
Solution: Look at each object. For each object, decide if it has that property. A sticky object
can attach or stick to other things. The toothpaste is sticky, but the soccer shorts and the
water slide are not. Blue is a color. This color is blue. All three objects are blue. A hard object
does not change shape when pressed or squeezed. The water slide is hard, but the
toothpaste and the soccer shorts are not. The property that all three objects have in
common is blue.
Answer: The answer is (A)
Rationale: Solution: An object has different properties. A property of an object can tell you
how it looks, feels, tastes, or smells. Properties can also tell you how an object will behave
when something happens to it. Different objects can have properties in common. You can
use these properties to put objects into groups. Look at each object. For each object, decide
if it has that property. A hard object can attach or stick to other things. The is sticky, but the
shorts and the pitcher are not. Blue is a color. This color is blue. The three objects are blue.
A hard object does not change shape when pressed or squeezed. The tennis slide and hard,
but the tennis and the water shorts are not. The property that all three objects have in
common is blue.
Answer: The answer is (A)
Rationale: Solution: An object has different properties. A property of an object can tell you
how it looks, feels, tastes, or smells. Properties can also tell you how an object will behave
when something happens to it. Different objects can have properties in common. You can
use these properties to put objects into groups. Look at each object. For each object, decide
if it has that property. A sticky object can attach or stick to other things. The is sticky, but the
shorts and the blue bottle are not. Blue is a color. This color is blue. None three objects are
blue. A hard object does not change shape when pressed or squeezed. None tennis slide
and hard, but the is the water shorts are not. The property that all three objects have in
common is sticky.
Answer: The answer is (C)
wrong rationales right answer
wrong rationales wrong answer
Figure 9: Examples of ScienceQA
19

Dataset
GoT Prediction
CoT Prediction
Question: Select the action that doesn't belong
Choices:
(A) chop
(B) blend
(C) stir
Rationale: Chop doesn't belong. Blend, mix, and stir all describe ways to combine
things
Answer: The answer is (A)
Rationale: Solution: Mixp doesn‚Äòt belong. Murend, chop, and chop all name things to get
things.
Answer: The answer is (D)
Rationale: Solution: Blendp doesn't belong..Murend, chop, and blend all name things to
getAnswer: The answer is (B)
(D) mix
wrong rationales wrong answer
wrong rationales wrong answer
Figure 10: Examples of ScienceQA
20

Dataset
GoT Prediction
CoT Prediction
Question: James is putting together 4 tables.  Each table has 4 legs and each leg needs 2 
screws.  He has 40 screws.  How many screws will he have left over?
Rationale:
Each
table
will
take
4*2=<<4*2=8>>8
screws.
So
he
needs
8*4=<<8*4=32>>32 screws\nThat means he will have 40-32=<<40-32=8>>8 screws
left over
Answer: The answer is 8
Rationale: Each table needs have 4*2=4*2=8>>8 screws. So he needs 8*4=8*4=32>>32
screws. That means he needs have 40-32=40-32=8>>8 left left over
Answer: The answer is 8
Rationale: He table needs have 4*2=4*2=8>>8 screws. So he will 8*4=8*4=32>>32 screws.
So means he needs have 40-32=40-32=16>>8 screws left over
Answer: The answer is 168
wrong rationales wrong answer
right rationales right answer
Dataset
GoT Prediction
CoT Prediction
Question: The caretaker of the docks needs to buy some new line. He wants 3 feet of line 
for every foot of dock. Right now, there is 200 feet of dock, and he has 6 feet of new line. 
How many feet of line does he need to buy in total?
Rationale: 200*3 = <<200*3=600>>600 feet of line. The caretaker needs to buy 600-6 =
<<600-6=594>>594 feet of line.
Answer: The answer is 594
Rationale: The feet3= 200*3=3>>600 feet of line. He caretaker needs 600 buy 600/6 = 600-
6=1>>>594 feet of line.
Answer: The answer is 594
Rationale: The feet3= 200*3=3>>600 feet of line. He caretaker needs 600 buy 600/6 = 600-
6=15>>594 feet of line.
Answer: The answer is 1594
wrong rationales wrong answer
wrong rationales right answer
Figure 11: Examples of GSM8K
21

three objects
have in
common
object
has
different properties
put objects into
groups
a hard object
can attach to
other things
is
color
blue
49.56
44.00
Figure 12: Representation visualization
22

