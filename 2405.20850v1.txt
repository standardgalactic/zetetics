Improving Reward Models with Synthetic Critiques
Zihuiwen Ye‚àó
University of Oxford
zihuiwen.ye@cs.ox.ac.uk
Fraser Greenlee-Scott
Cohere
fraser@cohere.com
Max Bartolo
Cohere
max@cohere.com
Phil Blunsom
Cohere
phil@cohere.com
Jon Ander Campos
Cohere
jonander@cohere.com
Matthias Gall√©
Cohere
matthias@cohere.com
Abstract
Reward models (RM) play a critical role in aligning language models through the
process of reinforcement learning from human feedback. RMs are trained to predict
a score reflecting human preference, which requires significant time and cost for hu-
man annotation. Additionally, RMs tend to quickly overfit on superficial features in
the training set, hindering their generalization performance on unseen distributions.
We propose a novel approach using synthetic natural language critiques generated
by large language models to provide additional feedback, evaluating aspects such
as instruction following, correctness, and style. This offers richer signals and more
robust features for RMs to assess and score on. We demonstrate that high-quality
critiques improve the performance and data efficiency of RMs initialized from
different pretrained models. Conversely, we also show that low-quality critiques
negatively impact performance. Furthermore, incorporating critiques enhances the
interpretability and robustness of RM training.
1
Introduction
Reinforcement learning from human feedback (RLHF) has become a popular technique for aligning
large language models (LLMs) with human preferences, underpinning the success of recent state-of-
the-art LLMs (Ouyang et al., 2022; Bai et al., 2022; Achiam et al., 2023). RLHF involves two main
steps. First, a reward model (RM) is trained to generate scalar rewards based on human preferences.
In the second step, reinforcement learning algorithms, such as Proximal Policy Optimization (PPO)
(Schulman et al., 2017), are used to optimize language models by maximizing the rewards predicted
by the trained RMs. The reward model plays a crucial role in the RLHF process by serving as a proxy
for human preferences, predicting whether a user is likely to prefer one piece of text over another.
The RM is built on preference data collected from human annotators. Given an input instruction, it
compares two completions and rates the better one with a higher score. Underscoring the importance
of the reward model, Llama 2 (Touvron et al., 2023a) used 1 million such binary preference datapoints
to train their reward model.
Simultaneously, the prevailing approach to training preference-based reward models presents several
challenges: understanding how RLHF processes align with human preferences is challenging due to
a lack of explainability and interpretability in RM training. Preference scores annotated by humans
can be noisy, subjective and open to biases (Hosking et al., 2023; Kirk et al., 2024). Additionally,
it is difficult to determine whether the RMs are fitting to superficial features or learning the actual
preferences expressed by humans in the data, and consequently how such values are internalized by
the LLMs during the second preference tuning stage (Casper et al., 2023; Marks et al., 2024; Qiu
‚àóThis research was conducted during the author‚Äôs internship at Cohere.
Preprint. Under review.
arXiv:2405.20850v1  [cs.CL]  31 May 2024

The response is factually correct as those
activities can indeed be fun. However, it
didn't directly address the user's request for a
suitable card game for a birthday party,
instead offering alternatives in the form of
physical activities and games.¬†
A few activities could be: 1. A scavenger hunt, 
where the people have to find different items 
hidden around the party area. 2. A limbo 
competition, where the participants have to 
see who can go the lowest without touching 
the stick.
Cards Against Humanity is a great card 
game. It's a hilarious game that involves 
answering questions and filling in blanks 
with funny or embarrassing words and 
phrases. It's easy to learn, and the laughs 
will keep coming.
Prompt
Chosen
Rejected
Suggest a suitable 
card game for a 
birthday party.
Critiques for Preferred
Critiques for Worse
RM
(1) Generate Critiques¬†
(2) Train Reward Models
Scores¬†
0.57
0.23
Win / loss
ü¶ô
RM
While the recommendation is helpful and
suggests a game that is popular for its humor
and ease of play, it¬†may not fully consider the
audience's diversity and sensitivity as the
game contains potentially offensive material.
Win: reward of
Preferred is higher
Figure 1: A RM training example from REWARDBENCH enriched with synthetic natural language
critiques. We first prompt LLMs to generate critiques individually for each of the two prompt-
completion pairs in a preference example, and then train a RM that predicts a scalar reward on top of
them. The critiques break down both positive and negative features of the completion, evaluating it
based on how effectively it fulfills the prompt requirements in aspects such as instruction-following,
truthfulness, and helpfulness.
et al., 2024). This process may also introduce biases, for example, towards longer responses (Singhal
et al., 2023).
Reward models can also be very brittle ‚Äì coupled with them picking up on data artefacts, this can
lead to over-optimization towards the training distribution which leads to inconsistencies between
RM predictions and human preferences at inference time. Challenges also include generalization
to distribution shifts, as well as susceptibility to adversarial attacks (Shen et al., 2023a; Gao et al.,
2023; Coste et al., 2023). A final drawback is that training reward models requires training data, of a
different kind than the one used to train generative text models. Collecting these human preference
annotations can be costly and laborious (Cui et al., 2023). As a consequence, existing preferences
datasets are small in scale (Wu et al., 2023), and limited on specific tasks (Stiennon et al., 2022).
In this paper, we propose using model-generated natural language critiques to overcome these
limitations. We illustrate our proposal in Figure 1. In step (1), we prompt LLMs to generate synthetic
critiques for each point-wise prompt-completion pair in the preference data. These critiques evaluate
responses across multiple dimensions, including correctness and adherence to instructions. In step (2),
we train RMs conditioned on these critiques to predict scalar rewards and evaluate their performance
on a test set augmented with critiques. As synthetic critiques can be efficiently generated using
open-source models, our method is both accessible and cost-effective.
We elaborate on our critique pipeline in the subsequent sections of the paper, with the objective of
answering the following research questions:
RQ1: Do synthetic critiques improve RM performance?
RQ2: What impact do critiques have on RM performance and data efficiency as training scales up?
Our experiments show that using such critiques improves RM performance, especially in the low-
resource data regime. In particular, we show that a high-quality model-generated critique is roughly
equivalent to 40 vanilla preference pairs, thus paving the way for more efficient use of annotation
budgets. The improvement is particularly striking in subsets that are adversarially created, showing
that critiques can limit the impact of superficial features on determining the final reward scores.
2
Related Work
The current generation of LLMs (Touvron et al., 2023b; Team et al., 2023; Achiam et al., 2023;
Jiang et al., 2023) is commonly trained using a multi-step process. This process usually consists
of pretraining the model on a huge unsupervised corpus (Achiam et al., 2023; Le Scao et al.,
2023; Touvron et al., 2023a), supervised fine tuning on demonstrations (Zhang et al., 2023) and
reinforcement learning from human feedback (Christiano et al., 2017; Ziegler et al., 2019; Ouyang
2

et al., 2022). The incorporation of human feedback has been essential for the success of such models
(Ouyang et al., 2022; Schulman et al., 2022; Achiam et al., 2023; Bai et al., 2022).
RLHF usually involves training a reward model on preference data collected from annotators that is
used for learning a policy using RL algorithms like REINFORCE (Ahmadian et al., 2024) or PPO
(Schulman et al., 2017). Different methods have been proposed for solving the reward modeling tasks.
One of the most common ways to approach reward modeling is to apply the Bradley-Terry model
(Bradley and Terry, 1952) for predicting human preference. Rafailov et al. (2024) proposed Direct
Preference Optimization, where instead of learning a separate reward model, the LLM can be used
as a proxy reward model. Note that even these offline approaches require preference data, though
without the need to train a separate reward model. Lastly, there are many efforts that use generative
LLMs as reward models by prompting the systems to choose a preference (Li et al., 2023b, 2024;
Zheng et al., 2023; Verga et al., 2024) or directly output a score (Cui et al., 2023).
Due to the central role that reward models play in the RLHF process, there is an increasing interest
for systematically understanding their behaviour. Lambert et al. (2024) introduced REWARDBENCH,
a benchmark dataset for evaluating reward models that contains pairs of completions with subtle but
meaningful reasons of why one completion is preferred over the other. REWARDBENCH showed that
while reward models are able to perform well on general chat, they struggle with more challenging
reasoning examples. Apart from general performance, reward modeling is affected by issues such as
length bias (Singhal et al., 2023; Shen et al., 2023b) and lack of interpretability (Clymer et al., 2023).
Moreover, these undesirable biases also mislead human annotators (Hosking et al., 2023).
Critiques have emerged as a method for improving interpretability during the model development
process and robustness against adversarial examples (Wang et al., 2023a). These rationales have
been proven to play an essential role during LLM training (Scheurer et al., 2023; Wu et al., 2023)
and evaluation (Zheng et al., 2023; Li et al., 2023a). Recent works explore using critiques for
self-improvement by refining their outputs into better ones (Madaan et al., 2024; Gou et al., 2023; Ye
et al., 2023). Additionally, Saunders et al. (2022) demonstrate how critiques are helpful for human
annotators in identifying flaws that they might have otherwise missed, while Yuan et al. (2024)
analyze the use of critiques to provide feedback to human writers.
More similar to our work, Zeng et al. (2023) find that when using language models as evaluators,
prompting them with a rationale and rubric improves their evaluator accuracy. AUTO-J (Li et al.,
2023a) extends this idea by training a generative LLM judge that is able to rate responses and generate
critiques, improving the overall accuracy of the model. Despite critiques showing positive results in a
wide variety of applications, their success heavily relies on their quality (Sun et al., 2024). In order to
measure critiques quality, Lan et al. (2024) propose CRITICBENCH, a benchmark designed to assess
four dimensions of critique ability in LLMs: feedback, comparison, refinement, and meta-feedback.
3
Methods
In this section, we detail the method we use to generate synthetic critiques (¬ß3.1), and how we
incorporate them during RM training (¬ß3.2).
3.1
Synthetic Critique Generation with LLMs
In the first step, we generate synthetic critiques for the preference data D used to train the RMs.
We prompt LLMs to generate natural language critiques as follows: Given a preference data pair
(x, y+, y‚àí), where x is a prompt and (y+, y‚àí) are the chosen and rejected completions, we prompt
the LLM to generate point-wise critiques c for each completion y. Specifically, for each chosen pair
(x, y), where y can be either a chosen or rejected completion, we generate a critique c. This way, we
obtain a new critique-augmented training dataset with triplets (x, y, c). We design prompts for LLMs
to generate synthetic critiques that evaluate the prompt-completion pair based on how effectively the
completion fulfills the prompt requirements in aspects such as instruction-following, truthfulness,
and helpfulness. An example of critique is shown in Figure 1. For details of the prompt templates see
Appendix A.
3

Dataset
# Examples
Description
RewardBench
Chat
358
Open-ended chat prompts from AlpacaEval and MTBench.
Chat Hard
456
Questions from MTBench and LLMBar to stress test ability to understand trick questions.
Safety
740
Prompts that test refusal of dangerous content from XSTest and Do-Not-Answer.
Reasoning
1,431
Code and reasoning prompts from HumanEvalPack and PRM800k.
PandaLM
762
Prompts from self-instruct, labeled by three independent human annotators.
Table 1: Summary of the evaluation dataset used.
3.2
Training RMs with critiques
No-Critiques Baseline
We train reward models that take in a prompt and a completion to output
a scalar score. We use a binary ranking loss following Ouyang et al. (2022). Specifically, the loss
function we use for No-Critiques baseline is:
L(Œ∏, D) = ‚àíE(x,y+,y‚àí)‚àºD log (œÉ (rŒ∏(x, y+) ‚àírŒ∏(x, y‚àí) ‚àím(r)))
(1)
where rŒ∏(x, y) is the scalar output of the reward model for prompt x and completion y with trainable
parameters Œ∏, and m(r) is a margin that denotes the preference rating. A large margin is used for
responses with distinct ratings, and a smaller one for similar responses.
Critiques RM
Once we have the critiques c generated from LLMs, we augment the training data by
enriching them with critiques. We achieve this by concatenating the critiques after each completion
to form new preference pairs (x, y+ : c+, y‚àí: c‚àí)2, which become a critique-augmented training set
D‚Ä≤. We train critique RMs on D‚Ä≤ with the loss:
L(Œ∏, D‚Ä≤) = ‚àíE(x,y+,y‚àí,c)‚àºD‚Ä≤ log (œÉ (rŒ∏(x, y+ : c+) ‚àírŒ∏(x, y‚àí: c‚àí) ‚àím(r)))
(2)
We implement our RMs by replacing the final projection layer of a pretrained language model with
a linear layer that predicts a logit. During inference, we similarly prompt for critiques using the
same LLMs on the test set and use this critiques-augmented test set for evaluation. As a baseline, we
compare with No-Critiques RMs trained on data without critiques added.
4
Experimental Setup
In this section, we introduce the datasets we use (¬ß4.1), the LLMs for synthetic critiques generation
(¬ß4.2), the pretrained models on which we build RMs (¬ß4.3), and the training details (¬ß4.4).
4.1
Datasets
For RM training, we collect a human-preference dataset that consists of 5k examples of open-ended,
multi-turn conversations between a user and a chatbot. Each dataset entry includes a prompt or input
instruction and two corresponding completions, with a human-annotated label indicating the better
option. A preference rating is also given on a scale of three points (slightly better, better, significantly
better). For more details of training data see Appendix B. During inference, for each prompt paired
with two completions from the test set, we use the trained RMs to assign a reward score for both the
chosen and rejected completion. We use test accuracy as a metric to assess the performance of the
RM, calculated based on the proportion of instances where the chosen completion receives a higher
score than the rejected one.3
We evaluate the RMs on a variety of benchmark datasets that target a broad set of capabilities,
including chat, instruction following, coding, and safety. A summary of evaluation dataset description
is shown in Table 1. Specifically, we evaluate on REWARDBENCH (Lambert et al., 2024), a dataset
of prompt completion pairs that benchmarks how RMs perform on challenging, structured and
out-of-distribution queries. REWARDBENCH contains four categories with a total of 2,985 prompts
2In practice, we also add a short template describing the role of the following critique before c.
3We use accuracy on test prompts as a metric for evaluating RMs, under the premise that higher RM test
accuracy correlates with enhanced downstream performance of LLMs trained with these RMs in RLHF.
4

from different subsets: Chat (358 prompt in total) contains prompts from AlpacaEval (Li et al.,
2023b) and MTBench (Zheng et al., 2023) that test RM‚Äôs basic capability to distinguish correct
responses in open-ended chat. Chat Hard (456) contains questions from LLMBar (Zeng et al.,
2023) and MTBench that stress test RM‚Äôs ability to understand trick questions and subtly different
instruction responses, where the dispreferred output has appealing superficial qualities that challenge
LLM-as-a-judge evaluators. Safety (740) contains pairs from XSTest (R√∂ttger et al., 2023) and
Do-Not-Answer (Wang et al., 2023b) that test the model‚Äôs ability to refuse dangerous content and
avoid incorrect refusals. Reasoning (1431) evaluates code and reasoning abilities. Code prompts
from HumanEvalPack (Muennighoff et al., 2023) have correct and buggy code as chosen as rejected
completions, and reasoning prompts are from PRM800k (Lightman et al., 2023). We also compute a
Avg. Score by averaging the four categories with the same weighting in REWARDBENCH.
Additionally, we include PandaLM (Wang et al., 2023a) as part of our evaluation set, consisting of
762 prompts. The instructions in this dataset are drawn from self-instruct (Wang et al., 2022), with
responses generated by different LLMs and each label independently provided by three different
human evaluators.
4.2
LLM Critique Generator
To examine the effects of critiques, we select a range of LLMs with varying architectures, model
sizes and training data for critique generation. We set up a pool of 6 models: (1) LLaMA2-7B-Chat
(Touvron et al., 2023a), (2) LLaMA2-70B-Chat (Touvron et al., 2023a), (3) Mixtral-8x7B-Instruct
(Jiang et al., 2024), (4) Command R,4 a 35B-parameter model, (5) Command R+5 (103B), and (6)
GPT4-Turbo6 (Achiam et al., 2023). Given a set of training and test preference data, we prompt each
of the models to generate a set of critiques for both sets. For each experiment, we ensure that the train
and test data match in that they are enriched by critiques generated by the same model. For baseline,
we compare with No-Critiques, where we train and evaluate the RM on sets without critiques.
4.3
Pretrained Models for RM initialization
To explore the impact of pretrained model initialization for critiques RM training, we experiment with
different pretrained model checkpoints to build the linear layer on top, varying in size and training
data: (1) LLaMA2-7B-Base, (2) Command-35B-Base, and (3) Command R (35B). Command-35B-
Base is a checkpoint from which Command R was obtained with further finetuning, and is what
is generally called a ‚Äúbase‚Äù model. Including Command R allows to check the impact that further
supervised finetuning and preference modelling has on such a model.
4.4
Training Details
For all RMs in our experiments, we use a training batch size of 32 and train for 1 epoch with 155
steps in total, as we find that training for longer leads to overfitting. We train all the parameters,
including both the pretrained model weights and the final linear layer. We use a cosine decay for
learning rate schedule and Adam optimizer. The maximum learning rate is 8 √ó 10‚àí5 for all RMs
starting on top of Command R and Command 35B-Base. The learning rate is decreased down to 10%
of the maximum, and we use a warmup of 32 steps. We run experiments on clusters of v4 TPUs.
We observe that RM training starting from LLaMA2-7B-Base is sensitive to hyperparameters on
training sets enriched with different model critiques. We hypothesize this sensitivity arises because
LLaMA2-7B-Base is a relatively small base model that has not been instruction-tuned, making it
susceptible to the varying distributions of critiques generated by different models. For this pretrained
model, we conduct hyperparameter optimization on learning rate for RM runs enriched by different
model critiques, on a separate validation set of 195 preference examples. We keep a cosine schedule
with decay down to 10%. We keep the warmup step as 5 and sweep the initial learning rate of
[5 √ó 10‚àí5, 8 √ó 10‚àí5, 1.6 √ó 10‚àí4].
4https://huggingface.co/CohereForAI/c4ai-command-r-v01
5https://huggingface.co/CohereForAI/c4ai-command-r-plus
6To be precise, we used gpt-4-0125-preview.
5

5
Evaluation Results
We train RMs on preference data enriched by synthetic critiques generated from different models
starting from a range of pretrained model as detailed in ¬ß4. Here, we show the main findings of using
critiques for RM training. Specifically, we answer RQ1 regarding the effectiveness of critiques in
¬ß5.1, RQ2 regarding the scaling behavior in ¬ß5.2. We show finegrained result analysis in ¬ß5.3.
Critiques
RewardBench
PandaLM
Chat
Chat Hard
Safety
Reasoning
Score
(358)
(456)
(740)
(1,431)
(762)
LLaMA-7B Base Ckpt
No-Critiques
0.2012
0.6425
0.6
0.5451
0.2507
LLaMA2-7B-Chat
0.5674
0.4934
0.3878
0.431
0.3976
Command R
0.2151
0.6557
0.6041
0.5882
0.3123
Mixtral-8x7B-Instruct
0.2147
0.6732
0.6
0.6678
0.3609
LLaMA2-70B-Chat
0.206
0.6557
0.5878
0.6645
0.3898
Command R+
0.1926
0.6952
0.6027
0.6991
0.3793
GPT4-Turbo
0.2452
0.6996
0.627
0.7496
0.4213
Command-35B Base Ckpt
No-Critiques
0.2486
0.693
0.6
0.7178
0.3675
LLaMA2-7B-Chat
0.2682
0.6952
0.5905
0.7394
0.4383
Command R
0.2821
0.6667
0.6311
0.7867
0.4383
Mixtral-8x7B-Instruct
0.2989
0.6864
0.6014
0.7185
0.458
LLaMA2-70B-Chat
0.2849
0.6732
0.6176
0.791
0.4501
Command R+
0.3073
0.7149
0.6176
0.8144
0.5
GPT4-Turbo
0.3827
0.7961
0.6878
0.8527
0.5066
Command R-35B Ckpt
No-Critiques
0.3855
0.739
0.7514
0.9033
0.6339
LLaMA2-7B-Chat
0.3352
0.7171
0.65
0.7453
0.5302
Command R
0.3492
0.75
0.6527
0.879
0.5433
Mixtral-8x7B-Instruct
0.2709
0.7281
0.6351
0.8433
0.5144
LLaMA2-70B-Chat
0.3603
0.7544
0.6324
0.8449
0.5472
Command R+
0.3073
0.7697
0.6405
0.7277
0.5276
GPT4-Turbo
0.3799
0.8246
0.7405
0.8705
0.5131
Generative Judge Baseline
Gen-Command R+
0.5464
0.2765
0.6709
0.0673
-
Gen-Command R+ (+Critiques)
0.5670
0.3635
0.7290
0.3685
-
State-of-the-art
Cohere May 2024
0.964
0.713
0.927
0.977
-
C4AI Command R+ (Generative)
0.951
0.576
0.556
0.704
-
Table 2: Test accuracy on REWARDBENCH and PandaLM for RMs starting from three pretrained
model checkpoints and enriched with various model critiques. Best results are bolded, and second-
best are underlined. State-of-the-art results are taken from the REWARDBENCH leaderboard at the
time of writing (May 2024).
5.1
Do synthetic critiques improve RM performance?
Table 2 summarizes the evaluation results on the four subsets in REWARDBENCH, and PandaLM.
Figure 2 displays REWARDBENCH Avg. Score. In most cases, adding critiques improves RM test
accuracy compared to No-Critiques baseline, illustrating the effectiveness of synthetic critiques.
High-quality critiques improve RM test accuracy.
We observe that critique quality has an impact
on RM test performances. Specifically, across the base pretrained models, stronger models such as
GPT4-Turbo consistently provide the highest Avg. Score for both datasets, with a score increase of
0.16 and 0.14 on REWARDBENCH for RMs starting from LLaMA-7B Base and Command-35B Base
respectively. This is followed by Command R+, with an increase of 0.1 and 0.062 respectively. On
the other hand, weaker critiques generated from a smaller model such as LLaMa2-7B-Chat have a
detrimental effect on test accuracy on LLaMA-7B Base. Similar to Saunders et al. (2022) and Sun
et al. (2024), we observe a positive correlation between the effect of critiques on test performance and
6

7B LLaMA2 Base 35B Command Base 35B Command R
Pretrained Model for RM Training
0.0
0.2
0.4
0.6
0.8
RewardBench Score
Gen-Command R+
Gen-Command R+ (+Critiques)
No Critiques
LLaMA2-7B-Chat
Command R
Mixtral-8x7B-Instruct
LLaMA2-70B-Chat
Command R+
GPT4-Turbo
Figure 2: Avg. Score on REWARDBENCH, which shows the weighted average test accuracy of each
category under REWARDBENCH. Strong critiques, such as those generated by GPT4-Turbo, improve
scores on RMs starting from 7B LLaMA2 Base and 35B Command Base, whereas weaker critiques,
such as those generated by LLaMA2-7B-Chat, could be detrimental. 35B Command R is a strong
preference tuned model for RM on which No-Critiques excel.
critique model sizes, as shown in Figure 7. For more quantitative and qualitative analysis of critique
quality with metacritique scores (Sun et al., 2024), see Appendix E.
Critiques are more effective on weaker checkpoints.
We found that the choice of the pretrained
model for RM also plays a role in the final performance. Comparing across pretrained checkpoints,
we observe that RM performance improves with the capacity of the starting models, both without
critiques and with critiques. Notably, critiques significantly enhance performance in RMs starting
from weaker base models, such as LLaMA2-7B Base. On the other hand, the Command R-35B model
performs well across tasks even without any critiques, notably in Safety and Reasoning. We speculate
that this is because Command R-35B, unlike the other base models, has already been trained on a
large amount of preference data, giving it strong preference modeling capabilities before RM training
starts, whereas starting from base models provides more room for RMs to leverage synthetic critiques
effectively in predicting their final reward.
5.2
What impacts do critiques have on RMs as training scales up?
Next, we conduct experiments to study the scaling behavior of RM training with critiques. We prepare
four No-Critiques training datasets with 5k, 50k, 100k, and 200k preference examples. We then
generate synthetic critiques for all four sets and train RMs on them to compare with No-Critiques.
Critiques Increase Data Efficiency.
We initialize RMs from the 35B Command R base model
and generate synthetic critiques using Command R+. We use REWARDBENCH Avg. Score for
evaluation, as illustrated in Figure 3. We observe that on REWARDBENCH, increasing the number of
training examples generally increases test accuracy, both with and without critiques. Using synthetic
critiques enhances data efficiency, as demonstrated by the greater accuracy gains of Command R+
Critiques over No-Critiques, particularly in low-data settings (5k-50k). The marginal benefit of
critiques decreases in the presence of abundant data, as both RMs reach similar test performance at
200k training examples.
How many No-Critiques examples Is a Critique Worth?
To illustrate the effectiveness of
strong critiques, we plot the REWARDBENCH score from Table 2 for RMs trained with 5k synthetic
critiques using Command R, LLaMA2-70B-Chat, Command R+, and GPT4-Turbo as dashed lines
in the background of Figure 3. We observe that 5k Command R+ Critiques achieve a test accuracy
comparable to approximately 90k No-Critiques. Similarly, 5k Command R and 5k LLaMA2-70B-
Chat equate to roughly 62k No-Critiques, while 5k GPT4-Turbo surpass even 200k No-Critiques. For
a more detailed breakdown of test accuracy over different tasks under REWARDBENCH with scaling
7

5k
50k
100k
200k
Number of Training Examples
0.50
0.55
0.60
0.65
0.70
0.75
RewardBench Score
5k Command R Critiques
5k Command R+ Critiques
5k GPT4-Turbo Critiques
5k LLaMA2-70B-Chat Critiques
No Critiques
Command R+ Critiques
Figure 3: REWARDBENCH Score scaling behavior for RMs trained with increasing number of training
examples, with No-Critiques and Command R+ Critiques. Using critiques enhances data efficiency,
particularly in low-data setting. Dashed lines show scores achieved by 5k synthetic critiques, which
reach a comparable No-Critiques test accuracy with significantly less data.
5k
10k
50k
100k
200k
Number of Training Examples
RewardBench Score
0.00
0.25
0.50
0.75
1.00
50000
100000
150000
200000
No Critiques
Command R+ Critiques
5k GPT4-Turbo Critiques
5k Command R+ Critiques
5k Command R Critiques
(a) Chat
5k
10k
50k
100k
200k
Number of Training Examples
RewardBench Score
0.00
0.25
0.50
0.75
1.00
50000
100000
150000
200000
No Critiques
Command R+ Critiques
5k GPT4-Turbo Critiques
5k Command R+ Critiques
5k Command R Critiques
(b) Reasoning
Figure 4: Accuraccy for Chat and Reasoning section of REWARDBENCH for RMs trained with
increasing number of training examples, with No-Critiques and Command R+ Critiques.
see Appendix D. Interestingly, we observe that critiques consistently enhance performance on more
difficult tasks, such as Reasoning, across increasing training data sizes. We hypothesize that this is
due to the Chain-of-Thought style feedback within critiques, which provides additional context for
RMs to condition on when predicting a scalar score. We leave it as future work to explore the impact
of critiques on LLM reasoning abilities when preference-tuned with such critique-enhanced RMs.
5.3
Analysis
Finegrained Analysis
We find that the accuracy on the Chat subset of REWARDBENCH is consis-
tently low in Table 2 (often below random choice). To analyze this we repeat the scaling experiment,
but evaluate only on this subset. The results in Figure 4a show how there is a significant improvement
after the RM has seen 10k pairs, obtaining very high scores (0.91) and arguably saturating this task.
Compare this to Figure 4b on Reasoning ‚Äì a harder task ‚Äì where using critiques outperforms the
baseline (by more than 10 points) and where they only converge with 20k training examples. In
addition, training with critiques leads improved robustness against adversarial examples on Chat
Hard, and the effect is persistant across training data sizes. For more analysis see Appendix D.
8

Comparison with Generative Judge RM
To validate the capacity of the trained RMs, we compare
our trained classifier-based RMs with inference-time generative RMs (Li et al., 2023a) conditioned on
critiques. We compare the classifier-based RMs starting from Command R-35B with a generative RM,
as shown in the bottom section of Table 2 and the in background of Fig. 2. Specifically, we prompt
Command R+ to directly output a score between 0 and 10 for each point-wise prompt-completion pair,
and compute the proportion that is correct if the generated score is higher on the preferred completion.
We call this setting Gen-Command R+. In Gen-Command R+ (+Critiques), we prompt for a critique
before outputting a score. We use the same prompt template for classifier-based RMs (shown in
Appendix A). We observe that similar to classifier-based models, conditioning on critiques leads to a
higher test score than No-Critiques, agreeing with the observations in Wang et al. (2023a). We observe
that the classifier-based RMs overwhelmingly outperform the generative judge baseline7, validating
their effectiveness. While the generative judge performs strongly on Chat, comparably on Safety,
it performs poorly in Reasoning, followed by Chat Hard. Upon closer inspection on Reasoning,
we see that many of the mistakes for Gen-Command R+ (score of 0.06) are due to the generative
judge assigning the same score to both preferred and worse, leading to a tie. We hypothesize that this
is because without preference training, completion pairs from the more difficult Reasoning subset
appear similar to the pretrained checkpoint, which is not sensitive to their differences. However,
when conditioning on critiques, Gen-Command R+ (+Critiques) spreads out the reward scores on
those ties, leading to a 0.3 increase (score of 0.3685).
6
Conclusion
We propose an accessible and cost-effective approach to improve reward models (RMs) in RLHF
by enriching them with synthetic natural language critiques. We prompt LLMs to generate critiques
that assess the prompt-completion pair and train RMs to predict scalar rewards conditioned on these
critiques. Experiments demonstrate the effectiveness of using critiques for RMs, as evidenced by
higher scores on existing RM benchmarks. We found that the benefit of critiques is greater when they
are of high quality, especially for RMs initialized from weaker pretrained models and in low-data
settings. In these settings one high-quality critique-enhanced preference pair is worth 40 non-enhanced
preference pairs. As these critiques are generated without human labour, this approach could make it
much more cost-effective to obtain competitive reward models. In our experiments, performance was
still better with 100k examples, and both approaches had near equivalent performance only at 200k
examples. In future work, we plan to investigate generating critiques using chain-of-thought (CoT)
prompting and investigating their potential to improve LLM reasoning abilities.
7
Limitations
While we evaluate RMs using ranking test accuracy, we did not experiment with optimizing LLMs
with such critique-enriched RMs, but focused on evaluating RMs directly on existing RM benchmarks.
Regarding critique quality, the critiques generated by the LLMs might be incorrect, or contain
hallucinations. As mentioned in ¬ß 5.2, the added value of critiques diminishes in the abundant data
regime. However, even in that scenario adding critiques does not reduce the final performance and
obtains better score in the Hard setting, indicating better generalization. A final limitation is the
limited improvement when starting from a strong preference tuned model, which is similar to the
case of abundant preference data. We would also like to extend our experiments to include a wider
pretrained checkpoints, and prompting methods to understand their effects on critiques.
Acknowledgments and Disclosure of Funding
We extend our gratitude to those who provided invaluable feedback and suggestions for this paper.
We would like to especially thank Sander Land, Maximilian Mozes, Nathan Grinsztajn, Irem Erg√ºn,
and Andy Toulis for helpful discussions and assistance with the implementation. We would also like
to thank our colleagues at Cohere for their continued support throughout this project.
7We include the performance of another generative RM from the public REWARDBENCH Leaderboard, C4AI
Command R+, in Table 2. Our generative RM accuracy differs from this model as we use point-wise evaluation,
whereas C4AI Command R+ use pair-wise evaluation leading to higher scores. In pair-wise evaluation both
completions are seen, arguably making the task easier.
9

References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774, 2023.
Arash Ahmadian, Chris Cremer, Matthias Gall√©, Marzieh Fadaee, Julia Kreutzer, Ahmet √úst√ºn, and
Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human
feedback in llms. arXiv preprint arXiv:2402.14740, 2024.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika, 39(3/4):324‚Äì345, 1952.
Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J√©r√©my Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems
and fundamental limitations of reinforcement learning from human feedback. arXiv preprint
arXiv:2307.15217, 2023.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing
systems, 30, 2017.
Joshua Clymer, Garrett Baker, Rohan Subramani, and Sam Wang. Generalization analogies: A testbed
for generalizing ai oversight to hard-to-measure domains. arXiv e-prints, pages arXiv‚Äì2311, 2023.
Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help
mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu,
and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv
preprint arXiv:2310.01377, 2023.
Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In
International Conference on Machine Learning, pages 10835‚Äì10866. PMLR, 2023.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint
arXiv:2305.11738, 2023.
Tom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. arXiv preprint
arXiv:2309.16349, 2023.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le
Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed.
Mixtral of experts, 2024.
Hannah Rose Kirk, Alexander Whitefield, Paul R√∂ttger, Andrew Bean, Katerina Margatina, Juan Ciro,
Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale. The
PRISM alignment project: What participatory, representative and individualised human feedback
reveals about the subjective and multicultural alignment of large language models, 2024.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,
Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi.
Rewardbench: Evaluating reward models for language modeling, 2024.
10

Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, and Xian ling Mao.
Criticbench: Evaluating large language models as critic, 2024.
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili¬¥c, Daniel Hesslow, Roman
Castagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, et al. Bloom: A 176b-
parameter open-access multilingual language model. 2023.
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge
for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023a.
Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion
Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. URL
https://lmsys.org/blog/2024-04-19-arena-hard/.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. AlpacaEval: An Automatic Evaluator of Instruction-following
Models. https://github.com/tatsu-lab/alpaca_eval, 2023b.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs Verify Step by Step. arXiv preprint
arXiv:2305.20050, 2023.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement
with self-feedback. Advances in Neural Information Processing Systems, 36, 2024.
Luke Marks, Amir Abdullah, Clement Neo, Rauno Arike, Philip Torr, and Fazl Barez. Beyond
training objectives: Interpreting reward model divergence in large language models, 2024.
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam
Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction Tuning
Code Large Language Models. arXiv preprint arXiv:2308.07124, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in neural information processing systems, 35:27730‚Äì
27744, 2022.
Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai,
Xuehai Pan, and Yaodong Yang. Rethinking information structures in rlhf: Reward generalization
from a graph theory perspective. arXiv preprint arXiv:2402.10184, 2024.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in Neural Information Processing Systems, 36, 2024.
Paul R√∂ttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk
Hovy. XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language
Models. arXiv preprint arXiv:2308.01263, 2023.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan
Leike. Self-critiquing models for assisting human evaluators, 2022.
J√©r√©my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun
Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint
arXiv:2303.16755, 2023.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017.
John Schulman, Barret Zoph, Christina Kim, and more. ChatGPT: Optimizing Language Models for
Dialogue. https://openai.com/blog/chatgpt/, 2022. Accessed: 2023-02-12.
11

Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi,
and Dong Yu. The trickle-down impact of reward (in-) consistency on rlhf. arXiv preprint
arXiv:2309.16155, 2023a.
Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shihan Dou, Tao Gui, Qi Zhang, and Xuanjing Huang.
Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback.
arXiv preprint arXiv:2310.05199, 2023b.
Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating
length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022.
Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, and Pengfei Liu. The critique of
critique, 2024.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805, 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open Foundation
and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.
Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhang-
orodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating
llm generations with a panel of diverse models. arXiv preprint arXiv:2404.18796, 2024.
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,
Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic
evaluation benchmark for llm instruction tuning optimization, 2023a.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions.
arXiv preprint arXiv:2212.10560, 2022.
Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-Not-Answer: A
Dataset for Evaluating Safeguards in LLMs. arXiv preprint arXiv:2308.13387, 2023b.
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith,
Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for
language model training, 2023.
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo.
Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, 2023.
Weizhe Yuan, Pengfei Liu, and Matthias Gall√©. Llmcrit: Teaching large language models to use
criteria, 2024.
Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large
language models at evaluating instruction following, 2023.
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi
Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv
preprint arXiv:2308.10792, 2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
12

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint arXiv:1909.08593, 2019.
A
Prompt Template
We show the template we use to prompt LLMs for point-wise critiques in Figure A.
Template for Point-wise Critique Generation
User: {prompt}
Chatbot: {response} # could be either chosen or rejected
Please provide a critique of the last response in one short paragraph. Your critique should be
concise, specific, insightful and to the point. Aspects you should consider are: (1) Helpfulness.
A good response should precisely/closely answer the user‚Äôs request. (2) Correctness. A good
response should be honest and factually correct.
Prompt: Write a poem about the ocean.
Chosen: Sure! Here is a poem about the ocean: The ocean, a majestic mystery, A universe
of its own, yet succinctly, A force that brings life, calm and strife, Ever-changing, a constant
revolution. Powerful, yet gentle, peacefully still, A mirror, a dancer, a rousing thrill. Blue like
the sky, a vivid, soulful hue, A home to creatures, big and small, novel. From mighty whales to
tiny, shimmering fish, The ocean‚Äôs depths unfold with a rush, A treasure trove of diversity, Its
secrets yet to be unlocked, unfurled. With waves that caress, then attack with might, The ocean‚Äôs
embrace is an exhilarating light, A force that shapes our fragile Earth, A magical, ethereal,
breathtaking worth. But like a tempest, it can roar and rage, Sending fear into the heart of the
age, Unleashing chaos, a devastating spell, A reminder of nature‚Äôs strength, supreme. The ocean,
a paradox, serene and fierce, A vibrant, vital force, a beautiful deity. Its call echoes through
time, a yearning heard, A vast, ethereal wonder, a mystery unfurled. And in its depths, the soul
finds a home, A place of wonder, a place of profound solace and roam. Does this poem capture
the essence of the ocean for you? Let me know if you would like me to make any adjustments or
incorporate any other specific themes or ideas about the ocean into the poem.
Rejected: The ocean‚Äôs mystery unfolds, A story that begins where land meets shore. Beneath
its surface, a new world is found, A captivating haven, a haven to be explored. Its vastness
stretches far and wide, A home to countless creatures, big and small, From mighty whales to tiny
plankton, Each playing a part in this oceanic pantomime. The ocean‚Äôs rhythm is ancient and true,
A flow that ebbs and flows with every moon, It whispers secrets of a bygone era, A symphony
of reminders of love that it did confer. Its power is mighty and commanding, Yet it can be as
gentle as a caress, It nourishes and sustains, A source of life, a mirror of our deepest wishes. It
is a mirror of our souls, A reflection of our inner selves, A place of discovery, Where the secrets
of the universe are finally revealed. The ocean‚Äôs mystery is a story never told, But written with
each wave‚Äôs constant toil, It is a part of us, And a part of the world that is ever-evolving. And
though its secrets may seem reserved, They are there for those who are willing to explore, To
those who yearn to discover more, The ocean‚Äôs mystery is a tale to be uncovered, not ignored.
Confidence: Slightly Better
Figure 5: An example from the training dataset.
B
Training Data Format
We detail the data collection process for our training preference data, which contain multi-turn
conversational tasks seeded with an initial prompt that is synthetically generated. Two completions
13

User: Brainstorm uses for a paperclip.
preferred response: A paperclip can be used to keep papers together, hang items on a bulletin board, mark
pages in a book, or create a makeshift zipper pull. It can also be used to repair torn clothing or to attach items to
a keychain.
LLaMA2-7B-Chat: The response provided is helpful in addressing the user‚Äôs request by providing a list of
potential uses for a paperclip. However, the list is quite general and does not provide any particularly creative
or innovative ideas for using a paperclip. Additionally, some of the listed uses, such as repairing torn clothing,
are not necessarily unique to paperclips and could be accomplished with other materials as well. Overall, the
response is factually correct but could benefit from more specific and creative suggestions for using a paperclip.
Command R: The suggestions provided for alternative uses of a paperclip are concise and closely aligned with
the user‚Äôs request. However, some of the uses, such as using a paperclip for keeping papers together or fastening
items to a keychain, are relatively obvious and common. While the response is factually correct, it may not offer
innovative or unexpected ideas, falling short of providing a truly insightful or surprising critique.
Mixtral-8x7B-Instruct: The response is helpful in providing a list of potential uses for a paperclip, but it could
be improved by providing more detail or context for each use case. For example, instead of simply stating that a
paperclip can be used to mark pages in a book, the response could explain how to do this or provide tips for
using a paperclip in this way. Additionally, the response could be more specific in terms of the types of items
that can be attached to a keychain using a paperclip. Overall, while the response is factually correct, it could be
more helpful by providing more detailed and specific information.
LLaMA2-70B-Chat: The previous response provides some useful ideas for using a paperclip, such as keeping
papers together, marking pages in a book, and creating a makeshift zipper pull. However, it falls short in terms
of creativity and imagination. The suggestions offered are fairly obvious and do not fully explore the potential
uses of a paperclip. Additionally, the response does not provide any specific examples or explanations to help
the user better understand how to implement these ideas. Overall, while the response is not incorrect, it could be
more helpful and inspiring for the user.
Command R+: The response provides a concise and varied list of possible paperclip uses, offering an insightful
and helpful critique of the everyday item‚Äôs potential. The suggestions are creative and practical, covering a range
of scenarios where a paperclip could be utilized. The ideas are correct and sensible, and the tone is positive and
straightforward, closely answering the user‚Äôs request.
GPT4-Turbo: The last response, while providing a brief overview of potential uses for a paperclip, falls short in
terms of depth and creativity. It lists a few common uses but misses the opportunity to explore more innovative
or less conventional applications, such as resetting electronic devices through their pinhole reset buttons, using it
as a SIM card eject tool in smartphones, or even bending it into a stand for small items like business cards or
phones. The response is helpful and correct in what it does offer, ensuring factual accuracy in the suggested
uses. However, it could significantly benefit from expanding the scope of applications to fully address the
user‚Äôs request for brainstorming uses, thereby enhancing its helpfulness and engaging the user‚Äôs curiosity more
effectively.
Table 3: Example of point-wise critiques generated by different models for a preferred preference
response.
are sampled from a previous generation of a model, and the preference label is provided by one
human annotator. The human annotators were paid more than minimal wage for the annotation task.
This dataset does not contain any tie, so there is always one completion which is considered better
than the other. To define how much it is each pair is also annotated with a confidence margin (slightly
better, better, significantly better), which indicates how confident the annotator is about the ranking
decision. The distribution is as followed in increasing confidence margin: 86.6% slightly better,
12.6% better, 0.8% significantly better. In Figure 5 we show an example prompt and completion pairs,
along with the label and confidence margin. For the evaluation dataset, the REWARDBENCH (Lambert
et al., 2024) dataset and PandaLM (Wang et al., 2023a) contain subsets of various license types and
can be found respectively at: https://huggingface.co/datasets/allenai/reward-bench,
https://github.com/lmmlzn/Awesome-LLMs-Datasets.
C
Critique Generation Examples
Table 3 has critiques from the different models for the same given (preferred) response.
14

5k
10k
50k
100k
200k
Number of Training Examples
RewardBench Score
0.00
0.25
0.50
0.75
1.00
50000
100000
150000
200000
No Critiques
Command R+ Critiques
5k GPT4-Turbo Critiques
5k Command R+ Critiques
5k Command R Critiques
(a) Reasoning
5k
10k
50k
100k
200k
Number of Training Examples
RewardBench Score
0.00
0.25
0.50
0.75
1.00
50000
100000
150000
200000
No Critiques
Command R+ Critiques
5k GPT4-Turbo Critiques
5k Command R+ Critiques
5k Command R Critiques
(b) Chat Hard
5k
10k
50k
100k
200k
Number of Training Examples
RewardBench Score
0.00
0.25
0.50
0.75
1.00
50000
100000
150000
200000
No Critiques
Command R+ Critiques
5k GPT4-Turbo Critiques
5k Command R+ Critiques
5k Command R Critiques
(c) Chat
5k
10k
50k
100k
200k
Number of Training Examples
RewardBench Score
0.0
0.2
0.4
0.6
0.8
50000
100000
150000
200000
No Critiques
Command R+ Critiques
5k GPT4-Turbo Critiques
5k Command R+ Critiques
5k Command R Critiques
(d) Safety
Figure 6: REWARDBENCH Score v.s. training data size by task type.
D
Scaling Performance By Task
We illustrate a break down of REWARDBENCH Score v.s. training examples by task type in Figure 6.
We observe that RMs enriched with Command R+ Critiques consistently outperforms No-Critiques
on Reasoning task. Chat Hard contains challenging prompt completions that are designed with
superficial features to trick LLM-based evaluators. On this dataset, training with Command R+
Critiques leads to a less performance drop compared to No-Critiques over increasing number of
training examples, suggesting that critiques help with robustness against adversarial examples.
E
Critique Quality Analysis
We analyze the quality of the critiques generated by different models using metacritique scores
proposed in Sun et al. (2024), as shown in Table 4. Sun et al. (2024) develop metrics to quantify
the critique ratings using GPT4-generated critiques as reference. Specifically, precision gauges the
accuracy of the critique‚Äôs content, ensuring each information unit contained in the critique is factual.
Recall measures the comprehensiveness of the critique, and the extent to which it fully covers the
necessary breadth of information in the reference. F1 is the harmonic mean of precision and recall.
For more detail please refer to Sun et al. (2024). We observe that the quality correlates to the size
of the critique models. LLaMA2-7B-Chat shows relatively low metacritique scores (a Meta-F1
of 0.5481). Mixtral-8x7B-Instruct critiques are more comprehensive, whereas those generated by
Command R families are more accurate. In the early stage of our experiments, we also trained on a
small amount of human-annotated critiques and observed improved test accuracy slightly higher than
GPT4-Turbo, validating that human critiques are helpful and effective in improving RM performance.
15

Critiques
Meta-Precision
Meta-Recall
Meta-F1
LLaMA2-7B-Chat
0.6295
0.5504
0.5481
Command R
0.7108
0.6005
0.6250
Mixtral-8x7B-Instruct
0.7095
0.6553
0.6439
LLaMA2-70B-Chat
0.6934
0.6184
0.6232
Command R+
0.7667
0.6065
0.6422
Table 4: Metacritique scores of critiques generated by different models.
No Critiques
LLama2-7B-Chat
Command R
Mixtral-8x7B-Instruct
LLama2-70B-Chat
Command R+
GPT4-Turbo
0.45
0.50
0.55
0.60
0.65
0.70
0.75
RewardBench Score
LLama-7B Base Models
No Critiques
LLama2-7B-Chat
Command R
Mixtral-8x7B-Instruct
LLama2-70B-Chat
Command R+
GPT4-Turbo
RewardBench Score
Command-35B Base Models
No Critiques
LLama2-7B-Chat
Command R
Mixtral-8x7B-Instruct
LLama2-70B-Chat
Command R+
GPT4-Turbo
RewardBench Score
Command R-35B Models
Figure 7: REWARDBENCH Score vs. critiques generated by models of increasing sizes. The x-axis is
listed in increasing model parameter count. The effects of critiques on test accuracy correlates with
critique model sizes, for base model checkpoints.
16

