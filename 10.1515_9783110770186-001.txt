Mathias Scharinger & Richard Wiese
Introduction: How to conceptualize
similarities between language and music
Music is the universal language of mankind, – poetry their universal pastime and delight.
Henry Wadsworth Longfellow (1835) [Longfellow, 1835]
Abstract: In this introduction, we first address the question of how to find the
appropriate level for the comparison between language and music. Secondly,
we argue that the appropriate level for this comparison is the one of prosody,
subsuming supra-segmental properties such as rhythm, meter and melody. We
then provide a bibliometric analysis of recent contributions on the topics most
central to this comparison. Finally, we introduce the contributions to the pres-
ent volume.
Keywords: language, music, prosody, bibliometric analysis
The famous quote by 19th century novelist Henry Wadsworth Longfellow and
many of its similar reformulations underscore the insight that music and lan-
guage are essential human cognitive abilities. In his influential book, Aniruddh
Patel goes as far as expressing these as identifying human features: “Language
and music define us as human.” (Patel, 2010, p. 3). Indeed, all modern humans
use language (e.g., Pagel, 2017 and references therein), and there is no culture
without music (Mehr et al., 2019). Longfellow’s quote has received direct sup-
port by Mehr et al. (2019) who were able to demonstrate that there are indeed
universal properties of music, e.g., its omnipresence, and most likely, its tonality.
Other properties, by contrast, vary, albeit in structured ways, e.g., along arousal
or religiosity. The ongoing quest for language universals, by comparison, has ap-
parently been less successful, or, put more positively, has revealed variability
along a vaster set of dimensions (e.g., Greenberg, 1978).
It is obvious, then, that music and language are related to each other in sev-
eral ways, displaying commonalities while also maintaining specific differentia-
tions. As briefly mentioned above, musicologists and linguists alike attempt to
Mathias Scharinger, Philipps-Universität Marburg, Pilgrimstein 16, D-35032, Marburg,
e-mail: mathias.scharinger@uni-marburg.de
Richard Wiese, Philipps-Universität Marburg, Pilgrimstein 16, D-35032, Marburg,
e-mail: wiese@uni-marburg.de
https://doi.org/10.1515/9783110770186-001

define universals in either domain. One such overarching universal feature seems
to be the expression of emotions and/or affect. In this respect, music has been
ascribed a direct link (Koelsch, 2011; Scherer, 1995; Timmers & Loui, 2019), while
language shows rather variable and more or less direct links to emotions and/or
affects. In language, emotions can be communicated by semantic features or
pragmatic conventions, or they can be expressed (more directly) by prosody (e.g.,
Liebenthal et al., 2016; Nygaard & Queen, 2008; Paulmann et al., 2012).
1 Prosody in language and music
In general, prosody as a system of suprasegmental linguistic information such as
stress, prominence, meter, rhythm, tone and intonation (e.g., Fox, 2002; Nespor
& Vogel, 1986; Selkirk, 1995) is a prime candidate for looking at the relation be-
tween language and music in a principled way. This claim is based on several
aspects, as elucidated in the following paragraphs.
First, prosody is concerned with the perceptual correlates “length”, “pitch”
and “perceived intensity” of the acoustic bases of language and music that are
directly comparable with each other by their physical properties duration, fre-
quency and intensity. Syllables in language, for instance, have a temporal extent
and their vocalic nucleus is characterized by fundamental frequency contours as
well as specific intensities. Tones in music also have a temporal extent and bear
specific tone heights. They may also differ in their intensities. Syllables and
tones are emitted as sound pressure waves in the air and perceived through the
same peripheral auditory system. When focusing on human singing, syllables
and tones are merged objects produced by the same tripartite human language
production system: respiratory source, phonatory larynx and filtering vocal tract.
Second, prosodic accounts, most prominently, Nespor & Vogel (1986), sug-
gest a hierarchical organization of prosodic units, the so-called “prosodic hierar-
chy”, that not only resembles a syntactic hierarchy, but is viewed as (part of) an
interface to syntax (Bennett & Elfner, 2019). Hierarchical structures in music,
akin to syntactic (or prosodic) structures in language, have been proposed and
formalized by Lerdahl and Jackendoff (Lerdahl & Jackendoff, 1977; 1983). Impor-
tantly, the model relates prosodic phrases from language to phrases from music.
The hierarchical phrase structure contains heads and complements and accounts
for the multi-tiered aspects of prominences on more local (individual tones or syl-
lables) or more global levels (groups of individual tones or syllables, cadences or
“feet”).
2
Mathias Scharinger & Richard Wiese

Third, prosody provides a very promising ground for evolutionary accounts
of language and music. Scientific approaches to the evolution of language and
music have sparked interest for decades in many different fields. The probably
most influential evolutionary theory of the last millennium also includes impor-
tant hypotheses about the role of music. To this end, Darwin suggested a song-
like communication system that pre-dates human language (Darwin, 1981 [1871]).
This view has been expressed in many subsequent accounts in which a common
pre-cursor for both language and music is assumed (e.g., Masataka, 2007; 2009).
A similar pre-cursor is discussed in a more language-oriented framework (Bicker-
ton, 1990). Accounts hence differ as to whether this pre-cursor rather resembled
music or language. Further, there are ongoing discussions about the evidence for
a co-evolution of language and music. In this respect, there are different opinions
on how to account for the apparent differences in adaptive benefits between lan-
guage and music. On an extreme side, music is ascribed no such direct benefit or
evolutionary advantage at all, expressed in the “auditory cheesecake hypothesis”
(Pinker, 1997). According to Pinker, music is a recent, rather “ornamental” devel-
opment in human evolution with strong emphasis on aspects of pleasure and aes-
thetics. Others maintain the hypothesis that even though no direct evolutionary
advantages of music may be identified, clearly some aspects co-evolved, e.g., with
conspecific calls and vocalizations, with social bonding or with credibility signal-
ing (Mehr et al., 2020; Savage et al., 2020). Fitch presents a comparative account
that also discusses aspects of co-evolution (Fitch, 2005; 2006). For instance, he
critically assesses the “spandrel” hypothesis, according to which music evolved
as a byproduct during the selection for features and mechanisms enabling human
language. An interesting argument is made regarding articulatory control. Accord-
ing to this argument, singing requires finer respiratory control than speech. This
supports the hypothesis that singing may have evolved earlier or simultaneously
with speaking (Fitch, 2006). A similar stance is taken by Wray (2002), suggesting
that complex signals (e.g., spoken or sung sounds) evolve first, and only later,
(compositional) meaning is added (for discussion, see also Fitch, 2005). Impor-
tantly, arguments in favor of co-evolution or shared proto-stages for both lan-
guage and music crucially involve prosodic aspects, based on a definition of
prosody that may be applied to both language and music. Some approaches, fol-
lowing the “musi-language” hypothesis (Mithen, 2005), therefore assume joint
prosodic sources for music and language (Brown, 2000; 2017). Similar views are
expressed by Heffner & Slevc (2015) and Fenk-Oczlon & Fenk (2009), although the
latter two accounts are not primarily of evolutionary nature, but rather describe
structural parallels between language and music that are most prominently seen
on the level of prosody. Fenk-Oczlon & Fenk (2009) speculate about all three pos-
sibilities of the sequential evolution of music language, considering a “music
Introduction: How to conceptualize similarities between language and music
3

before language”, “language before music” and a precursor/simultaneous sce-
nario, while arguing that either the “music before language” or the “musical pre-
cursor” scenario is the most likely one. They attribute a special role to singing
and consider vowels as essential units of a music-language precursor that was es-
sentially of prosodic nature (Fenk-Oczlon, 2017). Besides approaches with a focus
on frequency/pitch properties, such as provided by the aforementioned authors,
there are also very intriguing accounts with an emphasis on rhythm. In their re-
view on the evolution of rhythm processing, Kotz et al. (2018) argue that, in es-
sence, rhythm serves the social synchronization across domains and species.
Rhythm thus seems to fulfill a similar social bonding role than music as a whole
(Savage et al., 2020). The role and importance of synchronization by means of a
(common) rhythm is also expressed in Filippi et al. (2019). The authors argue that
temporal modulations aid the expression of emotions. For instance, slower articu-
lation rates are associated with sadness, while faster articulation rates are associ-
ated with joy. Temporal modulations are also assumed to support the delineation
of basic units in either language or music. The latter aspect is very prominently ex-
pressed in oscillatory approaches to language and music (e.g., Ding et al., 2017;
Ghitza & Greenberg, 2009). Basically, these approaches assume that the processing
of basic (prosodic) units in music and language is based on a successful parsing
(“chunking”) of the continuous acoustic signal by means of the brain’s oscillatory
mechanisms. More precisely, it is suggested that cortical rhythms, such as the os-
cillation in the theta-range (between 4 and 8 Hz), are beneficial in tracking and
thereby processing syllabic or tonal information by “quantizing” the acoustic infor-
mation into packages that correspond to the respective units, i.e., syllables or
tones. The exact role of cortical rhythms for segmenting and recognizing the re-
spective acoustic input is currently still discussed.
The fourth reason of why prosody is particularly well-suited for approach-
ing the relation of language and music concerns the shared neural substrates of
linguistic and musical prosody. Many studies have shown that music and lan-
guage share cortical and subcortical resources (e.g., Koelsch et al., 2002; Patel,
2003; Peretz & Zatorre, 2003), while others have identified specific biases for
language- versus music-processing, and thus suggested rather separate and
specialized processing resources (Bever & Chiarello, 1974). The latter authors
established (and confirmed) the special role of the left cerebral hemisphere for
language processing (cf. Broca, 1863), and the special role of the right cerebral
hemisphere for music processing. Later studies have provided evidence for a
more general hemispheric bias that depends on the level of processing of musi-
cal and linguistic input. In sum, it is assumed that initial processing of acoustic
input (both musical and linguistic), after the stages of the peripheral auditory
system, occurs bilateral, up to primary auditory cortex around Heschl’s Gyrus
4
Mathias Scharinger & Richard Wiese

in the temporal cortices. Later processing stages differentiate between rapid
and rather temporal event processing (in the left) and slower, rather spectral
event processing (in the right) hemisphere (e.g., Albouy et al., 2020; Poeppel,
2003; Zatorre & Belin, 2001). Importantly, though, right-temporal areas have
been identified to support the processing of both linguistic and musical pros-
ody, especially when focusing on frequency/pitch and intonation (e.g., Samm-
ler et al., 2015). The processing of rhythm or meter, on the other hand, may be
biased towards the left hemisphere, at least for trained musicians (Vuust et al.,
2005). Higher processing levels of language and music also seem to share neu-
ral resources. Here, the emphasis is on structural and syntactic processing that
is supported by left inferior frontal structures (e.g., Kunert et al., 2015; Sammler
et al., 2011), ascribing them rather domain-independent roles in sequencing, co-
ordinating or combining information from smaller constituent units (syllables,
words or tones). The involvement of these structures in both language and
music also supports the aforementioned generative model of musical structure
by Lerdahl and Jackendoff (Lerdahl & Jackendoff, 1977; 1983).
The fifth (but certainly not last) reason to focus on prosody when illustrating
the relation between language and music relates to transfer effects between the
two domains. This transfer can go in either direction and is best illustrated on
the level of prosody. The language-to-music transfer can be exemplified by abso-
lute pitch, the rare skill to assign arbitrary tone heights the correct (musical)
label (Levitin & Rogers, 2005). Deutsch and colleagues have shown that speakers
of a tone language are more likely to show absolute pitch than speakers of a non-
tone language (Deutsch et al., 2006). This suggests that language experience in
assigning labels to fundamental frequency/pitch in speech sounds transfers to
music. A recent study (Maggu et al., 2021) provides further evidence for a com-
mon mechanism underlying fundamental frequency/pitch encoding in music
and speech. The music-to-language transfer, on the other hand, seems to have
received far more attention. Again, the level of prosody is prominently focused at
when investigating these transfer effects. For instance, trained musicians have
been found to show greater sensitivity to fundamental frequency in the native
language (Schön et al., 2004) as well as in foreign languages (Marques et al.,
2007). Musicians also show greater sensitivity to metric structure in language
(Marie et al., 2011). Positive influences of musical training on general phonologi-
cal skills have been attested by numerous researchers (Anvari et al., 2002; Jones
et al., 2009; Moreno et al., 2009), for more discussion see Besson et al. (2011).
Thompson et al. (2004) provide evidence that musically trained persons are bet-
ter in decoding emotional speech prosody than untrained persons. This exem-
plary and by no means exhaustive list of studies shows that music and language
do not only show overlap in neural structures (Peretz et al., 2015) but apparently
Introduction: How to conceptualize similarities between language and music
5

use partially identical mechanisms such that the training of these mechanisms in
one domain has direct effects for the respective other domain.
The discussion above emphasizes that it is a fruitful endeavor to use prosody
for a principled comparison of language and music, an endeavor that we at-
tempt to pursue in this book. Prosody, in very broad terms, refers to the sound
structure of communicative systems and may be considered a “meta”-language
that formalizes the way of “how music speaks to language and vice versa”.
Prosody is firmly established within linguistic theory, particularly, phonology,
but is also applied in the musical domain (e.g., Glaser, 2000; Palmer & Hutch-
ins, 2006; Palmer et al., 2001; Palmer & Kelly, 1992). Therefore, prosody is not
just a field of inquiry that shares elements or features between music and lan-
guage (e.g., sound/tone durations and frequency/tone height), but may provide
a common conceptual ground.
A final argument that prosody is a very fruitful approach to study the rela-
tions between language and music stems from a bibliometric analysis introduc-
ing and framing the specific approaches to the prosodic link between the two
domains made by the authors in this volume.
2 A bibliometric analysis on prosody in music
and language
Bibliometric analyses are an emerging statistical and visual technique to de-
scribe and quantify a scientific landscape around a certain topic as well as its
impact on science as a whole (Cobo et al., 2011; Pritchard, 1969). A topic-search
in “Web of Science” (WoS, https://apps.webofknowledge.com, retrieved on
March 11th, 2021) with the three keywords “language”, “music”, and “prosod✶”
(the star being a place holder for either the noun or adjective ending), combined
with logical “and” yielded a total of 268 documents (among them 234 journal ar-
ticles) published between 1993 and 2021. The output of the search is essentially a
bibliographic list in which each document is fully referenced, including title, au-
thor, co-authors, affiliations, keywords and cited references. The search was ana-
lyzed by the bibliometric package “bibliometrix” (Aria & Cuccurullo, 2017). The
results showed documents of a total of 664 authors, 57 of which produced single-
authored documents and 607 of which participated in multi-authored docu-
ments. A total of 11 618 references were cited in all retrieved documents matching
the search terms.
A closer look at the development of the scientific landscape between 1993
and 2021 revealed a number of citations peak in 2017 (Figure 1 A), mainly driven
6
Mathias Scharinger & Richard Wiese

Figure 1: Illustration of the bibliometric analysis on the search terms “language”, “music”
and “prosod✶”. A: Number of articles containing the search terms as a function of the years of their
appearance. B: The twenty most-cited journals in the documents found by the WoS search. C: Co-
occurrence network of the key words (total: 788) found in the 268 documents of the WoS search.
Absolute number of occurrences are coded in font size, with higher numbers corresponding to
larger font sizes, co-occurrences are indexed by the number of connecting lines. Clustering was
done in VosViewer and is based on co-citation measures described in more detail in van Eck &
Waltman (2010). Depicted are the 50 most frequent keywords with at least 7 occurrences.
Introduction: How to conceptualize similarities between language and music
7

by publications on language and in the journal family “frontiers”. A more de-
tailed analysis on the journals that were cited in the 234 articles shows a quite
representative picture with acoustic-based journals leading the citation count
(Figure 1 B, Journal of the Acoustical Society of America). Notably, next to highly-
ranked neuroscientific journals (Cortex, Journal of Neuroscience) and journals
within music cognition (Music Perception), there are journals of broadest scope
and highest impact (Science, Nature Neuroscience, Proceedings of the National
Academy of Sciences of the United States of America), illustrating the scientific
interest and impact of the research field defined by the key words “language”,
“music” and “prosody”. Most importantly for the current book, however, is the
visualization of keyword co-occurrences in the network depicted in Figure 1 C
(created with VosViewer, van Eck & Waltman, 2010). Not surprisingly, the three
search items have a prominent position, with their font size positively correlated
with their number of occurrence in the search results. The keyword are organized
into color-coded clusters (see van Eck & Waltman, 2010 for details on the cluster-
ing technique), based on citation co-occurrences. A prominent red cluster is dom-
inated by key words on language and speech prosody, including terms such as
lexical stress, meter and rhythm. A green cluster subsumes terms on language,
emotion(s) and evolution. A small and distributed yellow cluster is based on
song, melody and pitch, while a blue cluster shows a neuroscientific charac-
ter, with emphasis on perception, auditory cortex and event-related potentials
(ERPs). It is perhaps worth noting that perception and related concepts play a
much more prominent role than does production as another fundamental per-
spective in cognition. Key words that correspond to key topics within this volume
are marked by rectangles and illustrate how well the present book covers the sci-
entific landscape defined by the three search terms “language”, “music” and
“prosody”. It is also discernible that the green cluster is not covered in this vol-
ume. This very plausibly illustrates that our attempt was not to provide an evolu-
tionary prosodic account for language and music, but rather to reveal fruitful
prosodic links between the two domains in a synchronic perspective.
3 Contributions to the present volume
The current volume thus takes prosody (in its wider sense) as an overarching
framework for investigating how each domain may profit from insights from
the other domain. Since most contributions stem from authors with a linguistic
background, it is natural that the focus is on the language side. There are,
8
Mathias Scharinger & Richard Wiese

however, also outlooks on how both directions of the transfer between lan-
guage and music may be explored in a fruitful way.
The first part of the book is concerned with underlying prosodic units
based on fundamental frequency/pitch. What is the appropriate granularity of
these units, and how can they be quantified, particularly with respect to the
recurrence structure in melodic contours?
The first aspect defines the core of the chapter by Pauline Larrouy-Maestri,
David Poeppel and Peter Q. Pfordresher. They start out by describing units
which occur in both language and music and acknowledge that even though these
units may differ in the respective domains, the operations applied to them may be
similar. The focus is then on so-called pitch “scoops”, i.e., deviations in frequency,
or small-scale frequency sweeps, which are introduced as a musical phenomenon.
Based on the observation that pitch deviations are tolerated to certain degrees in
music, scoops illustrate that the alleged discreteness of tone height does not hold
across the board. The magnitude of scoops rather depends on context and musical
skills of the performers. Therefore, pitch scoops provide counter-evidence to the
widely assumed difference between frequency/pitch in language (more variable,
less discrete) and frequency/pitch in music (less variable, discrete, cf. Zatorre &
Baum, 2012). Importantly, the authors elaborate on the evaluation of musical
scoops and distinguish between aesthetic and auditory evaluations. However,
their empirical data from experiments reported in this chapter suggest that audi-
tory correctness ratings (“in-tune”) correspond to (aesthetic) preferences.
Mathias Scharinger, on the other hand, argues that syllable-pitch in speech is
interpreted in a more discrete way than the aforementioned pitch differences be-
tween speech and music might suggest (Zatorre & Baum, 2012). Starting out from
the assumption that fundamental frequency in syllables can be quantized in a
discrete way, he defines speech melody as consisting of structured sequences of
syllable pitches, possibly dependent on the intrinsic pitch of the syllables’ nuclei.
Melodic contours are then proposed to be statistically describable by autocorrela-
tions of mean fundamental frequency (equated with pitch), the degree of which
seems to correlate with aesthetic judgments. The autocorrelation structure of
speech melody can also be seen in relation to a 1/f power law that is reminiscent
of one of the universal features of music discussed in Mehr et al. (2019). In fact,
when using a derived measure based on pitch autocorrelation, melodic structure
in language and music seems to follow a gradient, with poetry occupying a par-
ticular middle-ground between the two domains, as has been assumed since an-
tiquity (Winn, 1984).
The second part of the book is concerned with rhythm and stress and pur-
sues the question of how they compare across the domains. This part also
Introduction: How to conceptualize similarities between language and music
9

discusses data from electrophysiological experiments that highlight the role of
shared mechanisms (rather than shared units) between language and music
with respect to rhythm and stress.
First, Dicky Gilbers and Teja Rebernik pursue the general approach of as-
suming that language experience structures the way the corresponding musical
system is structured. They therefore emphasize the role of transfer effects, specif-
ically transfer from language to music, and argue that phonological and prosodic
properties take on a key role in this transfer. The structural similarities between
language and music are modelled within an Optimality Theory framework, in
which surface patterns are derived through the interaction of different con-
straints and their respective relative ranking to each other (Prince & Smolensky,
2004). They capitalize on the alleged universal status of these constraints and
their language-specific rankings, that in turn are thought to structure music.
With their exemplary analyses on different levels of description, including seg-
ments, syllables, tones, chords and rhythmic structure, they attempt to arrive at
a comparative typology. Their innovative approach subsumes aspects of univer-
sals in language and music (Greenberg, 1978; Mehr et al., 2019) and generative
structure building in both domains (Lerdahl & Jackendoff, 1977; 1983).
Jasmin Pfeifer and Silke Hamann are also interested in shared mechanisms
accounting for observations in music and language. They focus on a transfer
from music to language. Their general approach is to study relationships be-
tween the two domains by looking at impairments in one domain (music) and
the respective consequences for the other domain (language). To this end, they
look at amusia as a specific musical disability that essentially describes the lack
of musicality with many different consequences (Ayotte et al., 2002). More recent
research, as illustrated in the chapter, supports the view that amusia is accompa-
nied by a disruption of more domain-general mechanisms pertaining not only to
pitch processing in music, but also to rhythm and stress processing in language.
The hypothesis that persons with amusia are worse in processing linguistic stress
is tested with a behavioral study, while an electrophysiological study provides
complementary data on subconscious processing of linguistic stress patterns.
Their most intriguing finding is that persons with amusia seem to rely more on
duration cues than on pitch cues, illustrating not only transfer effects from music
to language, but also effects of between-domain compensation.
Richard Wiese provides an overview of rhythmic similarities between language
and music. His claim is that the two domains are more similar on the background
of rhythm and stress than previously assumed. He sets out to illustrate similarities
in the structural notations of prominence in music and language while acknowl-
edging that complete regular rhythm in language is impeded by higher-order,
10
Mathias Scharinger & Richard Wiese

syntactic and morphological constraints. Only when these constraints are lifted –
as, for instance, in poetry – can rhythmic regularity prevail. Wiese argues that the
functions of rhythm may be similar in the two domains and be best described by
referring to predictive mechanisms as currently discussed in Predictive Coding
frameworks applied to both language and music (Lewis & Bastiaansen, 2015;
Vuust et al., 2009). Five studies from a neurolinguistic and a corpus-linguistic
paradigm support the claim that regularity and predictiveness, while less
strong in language than in music, are still essential principles in organizing
events in time.
The third part of this volume is concerned with a topic at the true middle
ground between language and music: text-setting. Text-setting, either under-
stood as setting words to music or composing music to words illustrates the in-
tricacy of the spoken and the sung utterance. It has previously been established
that linguistic and musical prosodic properties (e.g., meter) align in text-setting
(Temperley & Temperley, 2013), but principled accounts with an emphasis on
prosody are relatively rare.
Heini Arjava and Gerrit Kentner are the first in this book to fill the aforemen-
tioned gap. They focus on text-setting in Finnish, a quantity-based language. The
overall topic is the alignment of prosodic information between music and lan-
guage. In music, prosodic properties are thought to subsume note duration,
while in speech, the equivalence is assumed to be prosodic weight. The study
they present highlights possible ways in which composers may be affected by
the prosodic system of the language that they attempt to set to music. Based on a
Finnish song collection, the prosodic relationship between language and music
is analyzed in symmetric and asymmetric music duration contexts. Detailed anal-
yses reveal that the correspondence of long notes and prosodic prominence is
driven by the syllabic nucleus, not the coda. Their analyses further suggest that
text-setting is also based on relative durations, i.e., durations in context, rather
than absolute prosodic unit characteristics.
Elena Girardi and Ingo Plag also examine text-setting from a prosodic perspec-
tive but put more emphasis on the musical structure, namely, musical meter,
note value and note height, relating this to poetic meter and syllabic prominence
on the linguistic side. They model text-setting within an Optimality-Theory frame-
work (Prince & Smolensky, 2004) and thereby attribute grammatical aspects to the
process of text-setting. They attempt to answer the question of whether the map-
ping from text to music is direct or mediated through metrical structure. The exam-
ple of classical English songs composed by Joseph Haydn reveals a systematic
mapping of text meter to musical meter, with prominence positions in the songs
almost categorically aligning with the text. The authors further show that the
Introduction: How to conceptualize similarities between language and music
11

strongest metrical positions co-occur with longer notes (replicating the findings of
Heini Arjava and Gerrit Kentner for Finnish). Pitch, on the other hand, seems to
play only a minor role for the mapping between text and music, i.e., metrically
prominent positions do not consistently align with pitch prominences in music.
The authors thus observe a general and intriguing trend according to which it is
mainly higher-order prosodic features which are matched between language and
music in text-settings! This view is complemented by the modeling results from
their Optimality Theory approach.
Christina Domene Moreno and Barış Kabak, finally, extend and complement
the chapter by Elena Girardi and Ingo Plag in at least two ways. First, they
focus on the psychological reality of the prosodic linguistic properties under in-
vestigation, and second, they compare languages and musical genres, enabling
to draw conclusions about universal, language- and culture-specific mapping
principles. They attempt a classification of languages according to the degree
to with which they incorporate pitch into the marking of prominence, thereby
distinguishing tone, stress, and pitch accent languages. The empirical basis for
their claims is a corpus of 42 children’s songs in Turkish, about half of which
composed in the Makam tradition and the other half composed in the Western
music tradition. The Turkish songs are furthermore compared to 18 English
children’s songs. Their main findings show that word stress corresponds to met-
rical prominence in Turkish, replicating findings from English in the chapter by
Elena Girardi and Ingo Plag. This was not true for Makam music. A further find-
ing is that linguistic stress is also aligned with melodic prominence (i.e., pitch-
based prominence) in Turkish, but not in English, again replicating the findings
by Elena Girardi and Ingo Plag. The authors conclude that the alignment of pro-
sodic structure in text-setting is based on the most informative cue for stress in
the respective language. This provides another important example for the fruit-
ful transfer effects between language and music.
Finally, the book concludes with a perspective paper by Sonia Kotz. The au-
thor argues that “an integrative cognitive and neural perspective” on prosody in
language and music is crucial for further progress in elucidating the human abil-
ity of processing acoustic events of all types. Particular emphasis is put on the
role of different cortical and subcortical components for handling increasingly
complex aspects of the structure of sounds.
12
Mathias Scharinger & Richard Wiese

References
Albouy, Philippe, Lucas Benjamin, Benjamin Morillon & Robert J. Zatorre. 2020. Distinct
sensitivity to spectrotemporal modulation supports brain asymmetry for speech and
melody. Science 367(6481). 1043–1047.
Anvari, Sima H., Laurel J. Trainor, Jennifer Woodside & Betty Ann Levy. 2002. Relations among
musical skills, phonological processing, and early reading ability in preschool children.
Journal of Experimental Child Psychology 83(2). 111–130.
Aria, Massimo & Corrado Cuccurullo. 2017. Bibliometrix: An R-tool for comprehensive science
mapping analysis. Journal of Informetrics 11(4). 959–975.
Ayotte, Julie, Isabelle Peretz & Krista Hyde. 2002. Congenital amusia: A group study of adults
afflicted with a music‐specific disorder. Brain 125(2). 238–251.
Bennett, Ryan & Emily Elfner. 2019. The syntax-prosody interface. Annual Review of Linguistics
5. 151–171.
Besson, Mireille, Julie Chobert & Céline Marie. 2011. Transfer of training between music and
speech: Common processing, attention, and memory. Frontiers in Psychology 2. 94.
Bever, Thomas G. & Robert J. Chiarello. 1974. Cerebral dominance in musicians and
nonmusicians. Science 185(4150). 537–539.
Bickerton, Derek. 1990. Language and species. Chicago, IL: The University of Chiacgo Press.
Broca, Paul. 1863. Localisations des fonctions cérébrales. Siège de la faculté du langage
articulé. Bulletin de la Société D’anthropologie 4. 200–208.
Brown, Steven. 2000. The “musilanguage” model of music evolution. In: Nils L. Wallin, Bjorn
Merker, Steven Brown, editors. The origins of music, 271–300. Cambridge, Mass.: MIT
Press Ltd.
Brown, Steven. 2017. A joint prosodic origin of language and music. Frontiers in Psychology 8. 1894.
Cobo, M. J., A. G. López‐Herrera, E. Herrera‐Viedma & F. Herrera. 2011. Science mapping
software tools: Review, analysis, and cooperative study among tools. Journal of the
American Society for Information Science and Technology 62(7). 1382–1402.
Darwin, Charles. 1981 [1871]. The descent of man and selection in relation to sex [originally
published in 1871]. Princeton: Princeton University Press.
Deutsch, Diana, Trevor Henthorn, Elizabeth Marvin & HongShuai Xu. 2006. Absolute pitch
among American and Chinese conservatory students: Prevalence differences, and
evidence for a speech-related critical period. The Journal of the Acoustical Society of
America 119(2). 719–722.
Ding, Nai, Aniruddh D. Patel, Lin Chen, Henry Butler, Cheng Luo & David Poeppel. 2017.
Temporal modulations in speech and music. Neuroscience and Biobehavioral Reviews
81(Pt B). 181–187.
Fenk-Oczlon, Gertraud. 2017. What vowels can tell us about the evolution of music. Frontiers
in Psychology 8. 1581.
Fenk-Oczlon, Gertraud & August Fenk. 2009. Some parallels between language and music
from a cognitive and evolutionary perspective. Musicae Scientiae 13(2). 201–226.
Filippi, Piera, Marisa Hoeschele, Michelle Spierings & Daniel L. Bowling. 2019. Temporal
modulation in speech, music, and animal vocal communication: Evidence of conserved
function. Annals of the New York Academy of Sciences 1453(1). 99–113.
Fitch, W. Tecumseh. 2005. The evolution of language: A comparative review. Biology &
Philosophy 2–3(20). 193–203.
Introduction: How to conceptualize similarities between language and music
13

Fitch, W. Tecumseh. 2006. The biology and evolution of music: A comparative perspective.
Cognition 100(1). 173–215.
Fox, Anthony. 2002. Prosodic features and prosodic structure: The phonology of
suprasegmentals. Oxford: Oxford University Press.
Ghitza, Oded & Steven Greenberg. 2009. On the possible role of brain rhythms in speech
perception: Intelligibility of time-compressed speech with periodic and aperiodic
insertions of silence. Phonetica 66(1–2). 113–126.
Glaser, Susan. 2000. The missing link: Connections between musical and linguistic prosody.
Contemporary Music Review 19(3). 129–154.
Greenberg, Joseph H. 1978. Universals of human language. Stanford, CA: Stanford University
Press.
Heffner, Christopher C. & L. Robert Slevc. 2015. Prosodic structure as a parallel to musical
structure. Frontiers in Psychology 6. 1962.
Jones, Jennifer L., Jay Lucker, Christopher Zalewski, Carmen Brewer & Dennis Drayna. 2009.
Phonological processing in adults with deficits in musical pitch recognition. Journal of
Communication Disorders 42(3). 226–234.
Koelsch, Stefan. 2011. Toward a neural basis of music perception – a review and updated
model. Frontiers in Psychology 2. 110.
Koelsch, Stefan, Thomas C. Gunter, D. Yves v Cramon, Stefan Zysset, Gabriele Lohmann &
Angela D. Friederici. 2002. Bach speaks: a cortical “language-network” serves the
processing of music. NeuroImage 17(2). 956–966.
Kotz, Sonja A., Andrea Ravignani & W. Tecumseh Fitch. 2018. The evolution of rhythm
processing. Trends in Cognitive Sciences 22(10). 896–910.
Kunert, Richard, Roel M. Willems, Daniel Casasanto, Aniruddh D. Patel & Peter Hagoort. 2015.
Music and language syntax interact in Broca’s Area: An fMRI study. Plos One 10 (11).
e0141069.
Lerdahl, Fred & Ray Jackendoff. 1977. Toward a formal theory of tonal music. Journal of Music
Theory 21. 111–171.
Lerdahl, Fred & Ray Jackendoff. 1983. A generative theory of tonal music. Cambridge, MA: The
MIT Press.
Levitin, Daniel J. & Susan E. Rogers. 2005. Absolute pitch: Perception, coding, and
controversies. Trends in Cognitive Sciences 9(1). 26–33.
Lewis, Ashley Glen & M. C. Bastiaansen. 2015. A predictive coding framework for rapid neural
dynamics during sentence-level language comprehension. Cortex 68. 155–168.
Liebenthal, Einat, David A. Silbersweig & Emily Stern. 2016. The language, tone and prosody
of emotions: Neural substrates and dynamics of spoken-word emotion perception.
Frontiers in Neuroscience 10(506).
Longfellow, Henry Wadsworth. 1835. Outre-Mer: A pilgrimage beyond the sea. New York:
Harper & Brothers.
Maggu, Akshay R., Joseph C. Y. Lau, Mary M. Y. Waye & Patrick C. M. Wong. 2021. Combination
of absolute pitch and tone language experience enhances lexical tone perception.
Scientific Reports 11(1). 1485.
Marie, Céline, Cyrille Magne & Mireille Besson. 2011. Musicians and the metric structure of
words. Journal of Cognitive Neuroscience 23(2). 294–305.
Marques, Carlos, Sylvain Moreno, São Luis Castro & Mireille Besson. 2007. Musicians detect
pitch violation in a foreign language better than nonmusicians: Behavioral and
electrophysiological evidence. Journal of Cognitive Neuroscience 19(9). 1453–1463.
14
Mathias Scharinger & Richard Wiese

Masataka, Nobuo. 2007. Music, evolution and language. Developmental Science 10(1). 35–39.
Masataka, Nobuo. 2009. The origins of language and the evolution of music: A comparative
perspective. Physics of Life Reviews 6(1). 11–22.
Mehr, Samuel A., Max M. Krasnow, Gregory A. Bryant & Edward H. Hagen. 2020. Origins of
music in credible signaling. Behavioral and Brain Sciences 44. e60.
Mehr, Samuel A., Manvir Singh, Dean Knox, Daniel M. Ketter, Daniel Pickens-Jones, S. Atwood,
Christopher Lucas, Nori Jacoby, Alena A. Egner, Erin J. Hopkins, Rhea M. Howard, Joshua
K. Hartshorne, Mariela V. Jennings, Jan Simson, Constance M. Bainbridge, Steven Pinker,
Timothy J. O’Donnell, Max M. Krasnow & Luke Glowacki. 2019. Universality and diversity
in human song. Science 366(6468). 1–17.
Mithen, Steven. 2005. The singing Neanderthals: The origins of music, language, mind and
body. London: Weidenfeld and Nicolson.
Moreno, Sylvain, Carlos Marques, Andreia Santos, Manuela Santos, São Luis Castro & Mireille
Besson. 2009. Musical training influences linguistic abilities in 8-year-old children: More
evidence for brain plasticity. Cerebral Cortex 19(3). 712–723.
Nespor, Marina & Irene Vogel. 1986. Prosodic phonology. Dordrecht: Foris Publications.
Nygaard, Lynne C. & Jennifer S. Queen. 2008. Communicating emotion: Linking affective
prosody and word meaning. Journal of Experimental Psychology: Human Perception and
Performance 34(4). 1017–1030.
Pagel, Mark. 2017. What is human language, when did it evolve and why should we care? BMC
Biology 15(64). 1–6.
Palmer, Caroline & Sean Hutchins. 2006. What is musical prosody? Psychology of Learning
and Motivation 46. 245–278.
Palmer, Caroline, Melissa K. Jungers & Peter W. Jusczyk. 2001. Episodic memory for musical
prosody. Journal of Memory and Language 45(4). 526–545.
Palmer, Caroline & Michael H. Kelly. 1992. Linguistic prosody and musical meter in song.
Journal of Memory and Language 31(4). 525–542.
Patel, Aniruddh D. 2003. Language, music, syntax and the brain. Nature Neuroscience 6.
674–681.
Patel, Aniruddh D. 2010. Music, language, and the brain. New York: Oxford University Press.
Paulmann, Silke, Debra Titone & Marc D. Pell. 2012. How emotional prosody guides your way:
Evidence from eye movements. Speech Communication 54(1). 92–107.
Peretz, I., D. Vuvan, M. E. Lagrois & J. L. Armony. 2015. Neural overlap in processing music and
speech. Philosophical Transactions of the Royal Society B-Biological Sciences 370(1664).
68–75.
Peretz, Isabelle & Robert J. Zatorre. 2003. The cognitive neuroscience of music. Oxford: Oxford
University Press.
Pinker, Steven. 1997. Words and rules in the human brain. Nature 378. 547–548.
Poeppel, David. 2003. The analysis of speech in different temporal integration windows:
Cerebral lateralization as ‘asymmetric sampling in time’. Speech Communication 41.
245–255.
Prince, Alan & Paul Smolensky. 2004. Optimality Theory – Constraint interaction in Generative
Grammar. Malden, MA: Blackwell Publishing.
Pritchard, Alan. 1969. Statistical bibliography or bibliometrics. Journal of Documentation
25. 348–349.
Introduction: How to conceptualize similarities between language and music
15

Sammler, Daniela, Marie-Hélène Grosbras, Alfred Anwander, Patricia E. G. Bestelmeyer &
Pascal Belin. 2015. Dorsal and ventral pathways for prosody. Current Biology 25(23).
3079–3085.
Sammler, Daniela, Stefan Koelsch & Angela D. Friederici. 2011. Are left fronto-temporal brain
areas a prerequisite for normal music-syntactic processing? Cortex 47(6). 659–673.
Savage, Patrick E., Psyche Loui, Bronwyn Tarr, Adena Schachner, Luke Glowacki, Steven Mithen
& W. Tecumseh Fitch. 2020. Music as a coevolved system for social bonding.
Behavioral and Brain Sciences 44. e59.
Scherer, Klaus R. 1995. Expression of emotion in voice and music. Journal of Voice 9(3).
235–248.
Schön, Daniele, Cyrille Magne & Mireille Besson. 2004. The music of speech: Music training
facilitates pitch processing in both music and language. Psychophysiology 41(3).
341–349.
Selkirk, Elizabeth. 1995. Sentence prosody: Intonation, stress, and phrasing. In: John
Goldsmith, editor. The handbook of phonological theory, 550–569. Cambridge, MA:
Blackwell Publishers.
Temperley, Nicholas & David Temperley. 2013. Stress-meter alignment in French vocal music.
The Journal of the Acoustical Society of America 134(1). 520–527.
Thompson, William Forde, E. Glenn Schellenberg & Gabriela Husain. 2004. Decoding speech
prosody: Do music lessons help? Emotion 4(1). 46–64.
Timmers, R. & P. Loui. 2019. Music and Emotion. Foundations in music psychology: Theory and
research, 783–825.
van Eck, Nees Jan & Ludo Waltman. 2010. Software survey: Vosviewer, a computer program for
bibliometric mapping. Scientometrics 84(2). 523–538.
Vuust, Peter, Leif Ostergaard, Karen J. Pallesen, Christopher Bailey & Andreas Roepstorff.
2009. Predictive coding of music – brain responses to rhythmic incongruity. Cortex 45(1).
80–92.
Vuust, Peter, Karen Johanne Pallesen, Christopher Bailey, Titia L. van Zuijen, Albert Gjedde,
Andreas Roepstorff & Leif Østergaard. 2005. To musicians, the message is in the meter:
Pre-attentive neuronal responses to incongruent rhythm are left-lateralized in musicians.
NeuroImage 24(2). 560–564.
Winn, James Anderson. 1984. Unsuspected eloquence: History of the relations between poetry
and music. New Haven, CT: Yale University Press.
Wray, Alison. 2002. Formulaic language and the lexicon. Cambridge: Cambridge University
Press.
Zatorre, Robert J. & Shari R. Baum. 2012. Musical melody and speech intonation: Singing a
different tune. PLoS Biology 10(7). e1001372.
Zatorre, Robert J. & Pascal Belin. 2001. Spectral and temporal processing in human auditory
cortex. Cerebral Cortex 11(10). 946–953.
16
Mathias Scharinger & Richard Wiese

