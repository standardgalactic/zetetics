A Bayesian Approach to Factor Analysis via
Comparing Posterior and Prior Concentration
by
Yun Cao
A thesis submitted in conformity with the requirements
for the degree of Doctor of Philosophy
Department of Statistics
University of Toronto
c⃝Copyright by Yun Cao 2010

A Bayesian Approach to Factor Analysis via Comparing
Posterior and Prior Concentration
Yun Cao
Degree of Doctor of Philosophy
Department of Statistics
University of Toronto
2010
Abstract
We consider a factor analysis model that arises as some distribution form known up
to ﬁrst and second moments. We propose a new Bayesian approach to determine if
any latent factors exist and the number of factors. As opposed to current Bayesian
methodology for factor analysis, our approach only requires the speciﬁcation of a
prior for the mean vector and the variance matrix for the manifest variables. We
compare the concentration of the prior and posterior about the various subsets of
parameter space speciﬁed by the hypothesized factor structures. We consider two
priors here, one is conjugate type and the other is based on the correlation factoriza-
tion of the covariance matrix. A computational problem associated with the use of
the second prior is solved by the use of importance sampling for the posterior anal-
ysis. If the data does not lead to a substantial increase in the concentration about
the relevant subset, of the posterior compared to the prior, then we have evidence
against the hypothesized factor structure. The hypothesis is assessed by computing
the observed relative surprise. This results in a considerable simpliﬁcation of the
problem, especially with respect to the elicitation of the prior.
ii

Acknowledgements
First of all, my utmost gratitude goes to my supervisor, Professor Mike Evans,
for his continuous support of my graduate study and research with his motivation,
enthusiasm, patience and most of all, his immense knowledge and rich experience.
He has guided me through all the stages of my research, so is this thesis. He helped
me to achieve more than what I expected of myself. I could not have imagined a
better advisor for my graduate study and a mentor for life.
Second, I would like to express my heartfelt gratitude to Professor Irwin Guttman,
a respectable, responsible and resourceful scholar, who meticulously read the the-
sis, spotted and corrected any error in it. Without his consistent and illuminating
instructions, this thesis could not have reached its present form.
I am greatly thankful to my committee members: Professor Nancy Reid and
Professor Jeremy Brunner for their insightful comments and their encouragement.
I am grateful for all the faculty members at Department of Statistics, especially,
Professor Don Fraser, Professor Andrey Feueuverger, Professor Radu Craiu, Profes-
sor Jeﬀrey Rosenthal for teaching me various courses and prepared me well for my
research work.
My sincere thanks go to Professor Aaron Childs, Professor Peter MacDonald,
Professor Shui Feng, Professor Angelo Canty at McMaster Univeristy, who brought
me to the realm of statistics.
I am also grateful for all the staﬀat Department of Statistics, Ram Mohabir,
Dermot Whelan, Laura Kerr, Andrea Carter, Sarah Johns for all of their support
and help through these years.
I would like to thank my fellow students colleagues at Department of Statistics
for oﬀering me great advice and also simulating discussions, Elif Acar, Yan Bai,
Daryl Caldwell, Meng Du, Gunho Jang, Zi Jin, Victor Leung, Longhai Li, Melissa
iii

Marcus, Hadas Moshonov, Shuying Sun, Angelo Valov, Tao Wang, Lizhen Xu, Chao
Yang, Jianguo Zhang. I thank all my friends who are always supportive and bring
laughter to my life.
Last but not least, I would like to thank my parents, Xianjun Kong and Fuli Cao,
for their unconditional love. They are always being there to encourage me, believe
in me and guide me through all the hard times. I am also grateful to Wenbin Kong
for his constant support throughout all these years.
iv

Contents
1
Introduction
1
1.1
Historical Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Factor Analysis Model . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2.1
Relevance of the model . . . . . . . . . . . . . . . . . . . . . .
6
1.2.2
Identiﬁcation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.2.3
Determination of q . . . . . . . . . . . . . . . . . . . . . . . .
12
1.2.4
Interpretations of factor loadings and factor scores . . . . . . .
13
1.3
Classical Factor Analysis . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.3.1
Principle factor analysis
. . . . . . . . . . . . . . . . . . . . .
14
1.3.2
Maximum likelihood factor analysis . . . . . . . . . . . . . . .
15
1.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2
Bayesian Inference
23
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2
Bayesian Estimation and Credible Regions . . . . . . . . . . . . . . .
26
2.3
Bayesian Hypothesis Assessment . . . . . . . . . . . . . . . . . . . . .
27
2.4
Relative Surprise Inferences
. . . . . . . . . . . . . . . . . . . . . . .
30
2.5
Prior elicitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.6
Bayesian Factor analysis . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.6.1
The Model Selection Approach
. . . . . . . . . . . . . . . . .
46
2.6.1.1
Prior for (µ, Γq, Ψ) . . . . . . . . . . . . . . . . . . .
47
v

2.6.1.2
Noninformative prior for (µ, Γq, Ψ) . . . . . . . . . .
51
2.6.1.3
Computing the Bayes factor (as in Lee)
. . . . . . .
54
2.6.2
Priors and Posteriors on (µ, Σ)
. . . . . . . . . . . . . . . . .
56
2.6.2.1
Conjugate prior for (µ, Σ) . . . . . . . . . . . . . . .
57
2.6.2.2
Prior for Σ based on correlation factorization . . . .
58
3
Methodology for Factor Analysis
67
3.1
Measure of concentration . . . . . . . . . . . . . . . . . . . . . . . . .
67
3.2
Concentration for the Factor Analysis Model . . . . . . . . . . . . . .
70
3.3
Minimizing Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.3.1
Constrained Univariate Quadratic Minimization . . . . . . . .
72
3.3.2
Constrained Row-wise Quadratic Minimization . . . . . . . . .
78
3.4
Examples of Computing the Distance . . . . . . . . . . . . . . . . . .
81
3.4.1
Example where R ∈R2×2
. . . . . . . . . . . . . . . . . . . .
81
3.4.2
The Independent Case (R = I)
. . . . . . . . . . . . . . . . .
83
3.4.3
One Factor
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.4.4
Two Factor
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.4.4.1
Univariate Minimization Approach . . . . . . . . . .
86
3.4.4.2
Row-wise Minimization Approach . . . . . . . . . . .
87
3.4.5
Counterexample to Exact Degree of Freedom Argument . . . .
88
3.5
The Prior and Posterior Distribution of d . . . . . . . . . . . . . . . .
88
3.5.1
One Factor Example . . . . . . . . . . . . . . . . . . . . . . .
89
3.5.2
Two Factor Example . . . . . . . . . . . . . . . . . . . . . . .
94
3.6
An Investigation of the Eﬀect of the Prior
. . . . . . . . . . . . . . .
98
3.6.1
An Arbitrary Prior of Σ . . . . . . . . . . . . . . . . . . . . .
98
3.6.2
A Marginally Uniform Prior on R . . . . . . . . . . . . . . . . 100
3.6.3
A Jointly Uniform Prior on R . . . . . . . . . . . . . . . . . . 101
vi

3.7
Choosing the Number of Factors . . . . . . . . . . . . . . . . . . . . . 102
3.7.1
Computing the ORS . . . . . . . . . . . . . . . . . . . . . . . 102
3.7.2
Inference of the Number of Factors . . . . . . . . . . . . . . . 104
3.8
Inference about R when Hq
0 is true . . . . . . . . . . . . . . . . . . . 112
4
Importance Sampling for Factor Analysis
124
4.1
Some Distribution Theory . . . . . . . . . . . . . . . . . . . . . . . . 125
4.2
Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
4.3
Implementation of Importance Sampling . . . . . . . . . . . . . . . . 133
4.3.1
One Factor Example . . . . . . . . . . . . . . . . . . . . . . . 133
4.3.1.1
Inverse Gamma Priors on the Scalings . . . . . . . . 133
4.3.1.2
Log-Normal Priors on the Scalings
. . . . . . . . . . 136
4.3.1.3
Computing the ORS . . . . . . . . . . . . . . . . . . 138
4.3.2
Two Factor Example . . . . . . . . . . . . . . . . . . . . . . . 140
4.3.2.1
Inverse Gamma Priors on the Scalings . . . . . . . . 140
4.3.2.2
Log-Normal Priors on the Scalings
. . . . . . . . . . 142
4.3.2.3
Computing the ORS . . . . . . . . . . . . . . . . . . 144
4.3.3
Further Comments . . . . . . . . . . . . . . . . . . . . . . . . 146
5
Example
148
5.1
International Currency Exchange
. . . . . . . . . . . . . . . . . . . . 148
5.2
Customer Satisfaction Data
. . . . . . . . . . . . . . . . . . . . . . . 156
Bibliography
162
vii

List of Tables
1.1
Correlation matrix of Attitude data . . . . . . . . . . . . . . . . . . .
19
1.2
Signiﬁcant tests based on 100 observations . . . . . . . . . . . . . . .
19
1.3
Factor method: Maximum Likelihood . . . . . . . . . . . . . . . . . .
20
4.1
Estimation of Standard Errors of Diﬀerent Importance Estimators . . 147
5.1
International Exchange in Six Currencies . . . . . . . . . . . . . . . . 149
5.2
Variable Descriptions of Customer Satisfaction Data . . . . . . . . . . 156
5.3
Means/Variances and Correlation of Customer Satisfaction Data . . . 157
List of Figures
2.1
The elicited prior density of 1/σ2 in Example 2.5.3. . . . . . . . . . .
45
2.2
Plot of density of ρ12 . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.3
Plot of density of ρ12 when p = 3, 6, 10
. . . . . . . . . . . . . . . . .
63
3.1
Density histogram of 103 distances of Example 3.4.3 . . . . . . . . . .
84
3.2
Histogram of 103 number of iterations of Example 3.4.3 . . . . . . . .
84
3.3
Histogram of 103 number of iterations of Example 3.4.4 . . . . . . . .
87
3.4
Plot of prior density of d . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.5
Plot of smoothed prior density of d . . . . . . . . . . . . . . . . . . .
90
viii

3.6
Plot of posterior distribution of d when n = 100: unsmoothed and
smoothed versions when H0 is true
. . . . . . . . . . . . . . . . . . .
92
3.7
Plot of Posterior distribution of d when n = 200: unsmoothed and
smoothed version when H0 is true . . . . . . . . . . . . . . . . . . . .
92
3.8
Comparison of Prior and Posterior distributions with diﬀerent sample
sizes when H0 is true . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
3.9
Comparison of prior and posterior densities when H0 is false . . . . .
94
3.10 Plot of prior density of d . . . . . . . . . . . . . . . . . . . . . . . . .
95
3.11 Plot of prior and posterior densities of d when H0 is true . . . . . . .
96
3.12 Plot of prior and posterior densities of d when H0 is false . . . . . . .
97
3.13 Plot of prior and posterior densities of d when H0 is true and false . .
98
3.14 Plot of prior density of d when p = 6 and q = 0 (top left), p = 6 and
p = 1 (top right), p = 6 and q = 2 (bottom left), p = 6 and q = 3
(bottom right).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.15 Plot of prior density of d when p = 6 and q = 0 (top left), p = 6 and
p = 1 (top right), p = 6 and q = 2 (bottom left), p = 6 and q = 3
(bottom right).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.16 Plot of prior density of d when p = 6 and q = 0 (top left), p = 6 and
p = 1 (top right), p = 6 and q = 2 (bottom left), p = 6 and q = 3
(bottom right).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
3.17 Plot of relative belief ratio when H0 is true . . . . . . . . . . . . . . . 103
3.18 Plot of relative belief ratio when H0 is false . . . . . . . . . . . . . . . 104
3.19 Comparison of prior and posterior densities of d when q = 0
. . . . . 105
3.20 Comparison of prior and posterior densities of d when q = 1
. . . . . 106
3.21 Comparison of prior and posterior densities of d when q = 2
. . . . . 107
3.22 Comparison of prior and posterior densities of d when q = 3
. . . . . 108
ix

3.23 Comparison of prior and posterior densities of d when q = 0
. . . . . 109
3.24 Comparison of prior and posterior densities of d when q = 1
. . . . . 110
3.25 Comparison of prior and posterior densities of d when q = 2
. . . . . 111
3.26 Comparison of prior and posterior densities of d when q = 3
. . . . . 112
3.27 Plot of posterior distribution of distance
. . . . . . . . . . . . . . . . 118
3.28 Plot of posterior distribution of distance
. . . . . . . . . . . . . . . . 121
4.1
Plot of posterior density of d when H0 is true
. . . . . . . . . . . . . 134
4.2
Comparison of prior and posterior densities of d when H0 is true . . . 134
4.3
Comparison of prior and posterior densities of d when H0 is false . . . 135
4.4
Plot of posterior density of d when H0 is true
. . . . . . . . . . . . . 136
4.5
Comparison of prior and posterior densities of d when H0 is true . . . 137
4.6
Comparison of prior and posterior densities of d when H0 is false . . . 138
4.7
Plot of relative belief ratio when H0 is true when n = 100 . . . . . . . 139
4.8
Plot of relative belief ratio when H0 is false when n = 100
. . . . . . 140
4.9
Comparison of prior and posterior densities of d when H0 is true . . . 141
4.10 Comparison of prior and posterior densities of d when H0 is false . . . 142
4.11 Comparison of prior and posterior densities of d when H0 is true . . . 143
4.12 Comparison of prior and posterior densities of d when H0 is false . . . 143
4.13 Comparison of prior and posterior densities of d with diﬀerent sample
sizes when H0 is false . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
4.14 Plot of relative belief ratio when H0 is true when n = 100 . . . . . . . 145
5.1
Comparison of prior and posterior densities of d . . . . . . . . . . . . 152
5.2
Comparison of prior and posterior densities of d . . . . . . . . . . . . 153
5.3
Comparison of prior and posterior densities of d . . . . . . . . . . . . 154
5.4
Comparison of prior and posterior densities of d . . . . . . . . . . . . 158
x

5.5
Comparison of prior and posterior densities of d . . . . . . . . . . . . 159
xi

Chapter 1
Introduction
1.1
Historical Overview
Factor analysis originated from psychometrics back in the late twentieth century,
when people actively worked on measurement of human intelligence due to Spearman
[36], Thurstone [38], Kendall [23] and one can also see the literature review by
Harman [17].
Typically intelligence is supposedly measured by administering a multi-question
test to a respondent. The ability to obtain correct answers is taken as evidence
of diﬀerent aspects of mental abilities, depending on the type of questions being
asked. While there was not a precise deﬁnition of mental abilities, these were felt
to exist and express themselves in an individual via the answers on the tests. To
obtain evidence on the existence of these mental abilities or “factors”, investigators
looked at the correlations existing among responses to diﬀerent questions. Early
study of such mental tests show—and it is certainly not surprising that most of the
correlations are positive, that is, people who score high on one kind of test are likely
to score highly on the other tests as well.
1

CHAPTER 1.
INTRODUCTION
2
Correlations in two dimensions are easily calculated, yet are diﬃcult to interpret
as most of these signify a noncausal relationship between the variables. However,
one implication of the strong correlation of two measures is that it may potentially
simplify two dimensions to one, i.e., we may be able to conclude that both variables
highly depend upon yet another variable. This variable, referred to as a factor, is
typically hidden or latent, i.e., we do not observe its values directly.
When more measures are involved, it leads to a more complex correlation sys-
tem, of which the underlying structure may convey something real and fundamental.
Factor analysis is one technique used to reduce a high-dimensional correlation co-
eﬃcient matrix, by mathematically “factoring” out hidden variables with minimal
loss of information. The residual variance of each measurement then contains some
speciﬁc information supplied by the measurement itself. Factor analysis is a method-
ology to determine if the correlation matrix could be reduced to a single abstract
factor, or to several such factors.
One of most common applications of factor analysis in psychology is to measure
so called “general intelligence” as proposed by Charles Spearman in the early twenti-
eth century (Spearman’s g). Spearman is a pioneer in psychology and is considered
to be the inventor of factor analysis. The purpose of Spearman’s invention is to
model the correlation of mental tests in order to probe some unambiguous “truth”
about human intelligence. For example, when two attributes of a person always
have a positive correlation, there might be some higher generality the correlation
implies, i.e., the tests are not measuring two independent attributes of mental abil-
ity. Spearman’s g was proposed as a single underlying factor in mental tests. It was
used as a quantiﬁcation of a human’s general mental worth. Although Spearman’s
g has been a controversial issue in the history of psychometrics, it provides us with
a real world application of factor analysis.

CHAPTER 1.
INTRODUCTION
3
Nowadays, factor analysis is ubiquitous in a variety of academic and industrial
ﬁelds, e.g., social studies, political science, education, marketing, and ﬁnance.
In section 1.2, we will give a more formal statistical deﬁnition of the factor
analysis model.
1.2
Factor Analysis Model
Factor analysis is a multivariate analysis technique that describes the covariance
relationships among many variables in terms of few unobserved, hypothetically ex-
isting “latent factors”.
Such “factors” cannot be observed directly, rather, they
are inferred from directly measured variables, through analysis of the covariance
structure of observed variables.
A q-factor model is written in the following form
y = Γqf + µ + e
(1.1)
where y(p × 1) is a random vector, Γ(p × q) is factor loading matrix with γij, the
loading of the ith variable on the jth factor, f(q × 1) is a random vector of latent
(common) factors, µ(p × 1) is the mean vector of y, and e(p × 1) is the error term,
also known as the unique factors. Here, we assume that
E(f) = 0,
Var(f) = I,
E(e) = 0,
Cov(ek, el) = 0,
k̸=l,
Var(e) = Ψ = diag(ψ1, · · · , ψp),
Cov(e, f) = 0.
In the factor analysis model, y is measurable and thus observable. If most of the
variation of any subgroup of y is caused by a common factor, then this subgroup of

CHAPTER 1.
INTRODUCTION
4
manifest variables behaves like one variable, and can be essentially reduced to one
dimension.
A factor analysis model is analogous to the linear regression model, except that
f are independent variables, and unobserved. The response y is a linear combina-
tions of latent variables f plus an error term. The “error” in linear regression is
often referred to as ﬂuctuations due to measurement on diﬀerent individuals and
is assumed random. The factor loadings act as regression coeﬃcients in a linear
regression model, hence γij represents how the ith variable relates to the jth factor.
The portion of the variance of the ith manifest variable due to the common factors,
is called the ith communality and is given by
h2
i =
q
X
j=1
γ2
ij,
since σii = var(yi) = h2
i + ψi. The leftover part of the variance of the ith variable is
called the unique, or speciﬁc, variance simply because this part of the variance isn’t
contributed by any common factors. The speciﬁc variance is equal to the variance
of the ith residual, namely,
Var(yi −
q
X
j=1
γijfj) = Var(ei) = ψi = σii −h2
i .
The model (1.1) implies that the variance-covariance matrix of y can be decom-
posed as:
Var(y) = Σ = ΓqΓ
′
q + Ψ,
(1.2)
and so Cov(yi, yj) = Pq
k=1 γikγjk when i ̸= j. Note that the covariances do not
depend on Ψ. This makes sense as a factor analysis model is used to explore the

CHAPTER 1.
INTRODUCTION
5
factor pattern via learning about the interdependency among the manifest variables.
We note, however, that covariances are not independent of the scale on which the
variables are measured, and so it is commonly recommended that we standardize the
variables and work instead with the correlations. With the model (1.1), however,
the correlations depend on both Γ and Ψ.
Accordingly, we can work with a diﬀerent parameterization of the factor model
given by
D−1
2(y −µ) = Γqf + e
(1.3)
where D = diag(Σ) and µ, f and e are as before. We then have that R = Corr(y) =
D−1
2ΣD−1
2 = ΓqΓ
′
q + Ψ and this immediately implies that the γij satisﬁes h2
i =
Pq
l=1 γ2
il ≤1 and ψi = 1 −h2
i . Note that h2
i is now the proportion of variation in the
standardized ith variable explained by the common factors and ψi is the residual
proportion of unexplained variance. So we will write
Corr(y)
= R = D−1
2ΣD−1
2
= ΓqΓ′
q + Ψ(Γq)
(1.4)
where Ψ(Γq) = diag(1 −h2
1, · · · , 1 −h2
p). Therefore,
ρij = Corr(yi, yj) =
q
X
l=1
γil
p
1 −h2
i
γjl
q
1 −h2
j
and the correlations depend only on Γ.
It is more sensible to parametrize the
correlation matrices rather than covariance matrices as the factor patterns do not
depend upon the scales of the manifest variables. For this reason, we prefer (1.3) to
(1.1), but we will discuss both in this thesis.
In a factor analysis, there are several aspects to be considered as discussed in

CHAPTER 1.
INTRODUCTION
6
Anderson and Rubin [1], namely, i) existence of the model ii) identiﬁcation iii)
determination of structure iv) estimation of parameters v) test of hypothesis vi)
determination of the number of factors vii) estimation of factor scores. We now
discuss these and other aspects of factor analysis in the following sections.
1.2.1
Relevance of the model
In a statistical analysis, we assume our data were generated from a distribution in
a statistical model {Pθ : θ ∈Θ}. Suppose y in (1.1) or (1.3) arises from some
distribution Pθ, where the distribution form is know up to its ﬁrst and second-
order moments, so our model is parametrized by θ = (µ, Σ). As an example, we
could postulate that y is distributed as p-dimensional multivariate normal, that is
y ∼Np(µ, Σ), where µ ∈Rp and Σ ∈Rp×p.
The fundamental problem in factor analysis is to see if we can explain rela-
tionships amongst observed variables by fewer latent factors.
Let us start with
something relevant and familiar. For example, in ordinary linear regression analysis
we ﬁt an linear model for dependent and independent variables. We can ask if this
model is correct or adequate, i.e., could the observed data be a realization of the
model speciﬁed. There are two aspects of this in the regression context. First we
must check whether or not the particular regression relationship is appropriate, and
secondly we must check whether or not any of the distributional assumptions made
hold.
With a factor analysis model, we know that for some choices of q the model
speciﬁed will always hold. For example, when q = p, we can take Γp = Σ1/2 and
Ψ = 0 and (1.2) holds. In this particular case, the only thing left to check is any
distributional assumptions we may make about f and e. Similarly, in (1.3) we can
take Γp = R1/2 for (1.4) to hold.

CHAPTER 1.
INTRODUCTION
7
Ignoring the distribution issue, we can say that the basic problem of factor
analysis is to determine the minimum number of factors q for which the equation
(1.2) or (1.4) can be solved. For q < p and a given Σ, one must determine if there
is a solution (Γq, Ψ) to
σii = ψii +
q
X
k=1
γ2
ik,
and
σij =
q
X
k=1
γikγjk
i ̸= j
for (1.2) or a solution Γq to
ρij =
q
X
l=1
γilγjl
with
q
X
j=1
γ2
ij ≤1
for (1.4).
First we consider model (1.2). There are p(p + 1)/2 elements in Σ, p diagonal
elements in Ψ and the factor loading matrix Γq has pq elements. Moreover, any
Γq can be replaced by ΓqQ, where Q is a orthogonal q × q matrix with q(q −1)/2
independent elements, since if (Γq, Ψ) is a solution, so is (ΓqQ, Ψ). To see this lack
of identiﬁability, let Q be an q ×q orthogonal matrix, Γ∗
q = ΓqQ and f∗= Q′f, then
(1.1) becomes
y = Γ∗
qf∗+ µ + e = ΓqQQ′f + µ + e = Γqf + µ + e,
since QQ′ = I, and
Γ∗
q(Γ∗
q)′ + Ψ = Γq(QQ′)Γ′
q + Ψ = ΓqΓ′
q + Ψ = Σ.
Therefore, there are only pq −q(q −1)/2 degrees of freedom in Γq, so the degrees of
freedom on the right of (1.2) is pq −q(q −1)/2 + p. Now consider the diﬀerence in

CHAPTER 1.
INTRODUCTION
8
degrees of freedom between those for Σ (i.e., p(p + 1)/2) and those on the right of
(1.2). This diﬀerence is
1
2p(p + 1) −1
2q(q + 1) −p −pq = 1
2[(p −q)2 −(p + q)].
(1.5)
When (1.5) is less than or equal to 0, then we expect that there is always at least
one solution for an arbitrary Σ. When (1.5) is greater than 0, then we expect that
there is only a solution for some Σ matrices, the factor model oﬀering a simpler
structure than the full variance-covariance matrix.
For (1.4) the situation is similar as R has p(p −1)/2 degrees of freedom, Γq has
pq −q(q −1)/2 degrees of freedom and thus we have
1
2p(p −1) −1
2q(q + 1) −pq = 1
2[(p −q)2 −(p + q)]
degrees of freedom just as given in (1.5) for (1.2).
Let us now consider a simple example when p = 2.
Example 1.1
Suppose that p = 2 and we want to solve (1.2) or (1.4) for Γq when q = 0, 1, 2.
Notice that when q = 0, then Γ0 = 0 and we only have a solution when the yi
variables are independent.
When q = 1, then we need to solve the system of equations
σ11 = ψ1 + γ2
11,
σ22 = ψ2 + γ2
21,
σ21 = γ11γ21,
for γ11, γ21, ψ1, and ψ2. Notice that (1.5) is less than 0 in this case. Therefore, given

CHAPTER 1.
INTRODUCTION
9
an arbitrary Σ, any Γ1 satisfying the constraint γ11γ21 = σ21 produces a solution
for Ψ by taking ψ1 = σ11 −γ2
11 and ψ2 = σ22 −γ2
21 provided ψ1, ψ2 ≥0.
Let γ∗
11 = γ11/√σ11 and γ∗
21 = γ21/√σ22, then an equivalent equation to γ11γ21 =
σ21 is given by
γ11γ21 = σ12
i.e.,
γ∗
11γ∗
21 = ρ12
(1.6)
We see that γ∗
11 =
p
|ρ12|, γ∗
22 = sign(ρ12)
p
|ρ12| satisfy (1.6) and gives the solution
γ11 =
p
|ρ12|, γ22 = sign(ρ21)√σ22
p
|ρ21|.
Indeed γ2
11 = σ11|ρ12| ≤σ11, γ2
12 = σ22|ρ12| ≤σ22 since |ρ12| ≤1 and
ψ = σ11 −γ2
11 = σ11(1 −|ρ12|)
ψ = σ22 −γ2
21 = σ22(1 −|ρ12|).
Further, we get a family of solutions γ∗
11 = a
p
|ρ12| and γ∗
22 = a−1sign(ρ21)
p
|ρ12|, so
γ11 = a
p
|ρ12|√σ11 and γ21 = a−1sign(ρ21)
p
|ρ12|√σ22, for all a such that a2|ρ12| ≤1
and a−2|ρ12| ≤1 or equivalent for all a satisfying |ρ12| ≤a2 ≤|ρ12|−1 with ψ1 =
σ11(1 −a2|ρ12| and ψ2 = σ22(1 −a−2|ρ12|). When |ρ12| = 0, either γ11 or γ22 equals
to 0.
As previously mentioned when q = 2, there always exists a solution by taking
Γ2 = Σ1/2 and Ψ = 0. Indeed again there are inﬁnitely many solutions in this case
given by Σ1/2Q for any orthogonal matrix Q ∈R2×2.
We can also determine Γq for q = 0, 1, 2 for model (1.3) by solving (1.4). When
q = 0 then Γ0 = 0 is a solution and there is a solution only when the yi are

CHAPTER 1.
INTRODUCTION
10
independent. When q = 1, then the previous analysis applies with σ11 = σ22 = 1.
When q = 2, there always exists a solution by taking Γ2 = R1/2 and RQ is also a
solution for all orthogonal Q ∈R2×2.
1.2.2
Identiﬁcation
Suppose that we obtain one solution such that the equation (1.2) holds. The next
question we ask is whether this solution is unique. As previously noted, we see that
Γq is only identiﬁed up to an orthogonal rotation Q.
This lack of uniqueness for Γq is sometimes regarded as a virtue as some useful
factor patterns are often not revealed until the factor loadings are rotated. In many
contexts, it has been proposed to choose an orthogonal matrix Q that results in ΓqQ
having as many zeros as possible. Choosing this rotation cannot be addressed via
statistical methods, rather it depends on the particular application. Accordingly,
we do not address the issue of choosing a rotation further in this thesis.
Rather, we will standardize Γq so that it is always lower triangular whether we
deal with (1.1) or (1.3). We apply Gram-Schmidt decomposition to the rows of
Γq and write Γq = Γ∗
qU where U ∈Rq×q is orthogonal and Γ∗
q ∈Rp×q is a lower
triangular with positive diagonal elements. Hence, we can assume that Γq is in the
set of all p × q lower triangular matrices with positive diagonal elements.
Even with this standardization, however, this does not guarantee a unique solu-
tion. Note that in Example 1.1 with p = 2, Γ1 is lower triangular and we can force
γ11 ≥0 and still we will generally obtain inﬁnitely many solutions. Let us consider
another example.
Example 1.2
Suppose p = 3 and we determine a standardized solution Γq for (1.1) and (1.3).

CHAPTER 1.
INTRODUCTION
11
Note that for q = 0 and q = 3, the results are as in Example 1.1.
When q = 1 then (1.5) equals 0, but this does not guarantee a solution for an
arbitrary Σ. We will show this via simulation results presented in Chapter 3.
When q = 2, we have the set of equations
σ21 = γ11γ21,
σ31 = γ11γ31,
σ32 = γ21γ31 + γ22γ32,
ψ11 = σ11 −γ2
11,
ψ22 = σ22 −(γ2
21 + γ2
22),
ψ33 = σ33 −(γ2
31 + γ2
32).
Notice that with this standardization, we do not need to estimate γij when i < j as
they all equal to 0. This implies that γ12 = 0.
For model (1.3), when q = 2 we have the following equations:
ρ21 = γ11γ21,
ρ31 = γ11γ31,
ρ32 = γ21γ31 + γ22γ32,
0 ≤γ2
11 ≤1,
γ2
21 + γ2
22 ≤1,
γ2
31 + γ2
32 ≤1.
Simulation results in Chapter 3 show that we do not have an exact solution when
q = 1, even though (1.5) is 0.

CHAPTER 1.
INTRODUCTION
12
1.2.3
Determination of q
As mentioned earlier, the fundamental problem in factor analysis is to determine
the smallest number of common factors. One approach postulates some q and then
estimates the model parameters, e.g., via maximum likelihood estimation, given that
the model is valid and then performs hypothesis tests based on those estimators to
determine q.
One approach is called “sequential testing”.
For this we test the hypothesis
q = q0 against q > q0. If the null hypothesis is rejected, then we proceed with
testing q = q0 + 1 versus q > q0 + 1, etc., until a hypothesis is accepted.
The
situation is analogous to carrying out a number of statistical tests when comparing
multiple means in analysis of variance. If we perform multiple testing in hypothesis
testing, then it is often argued that we must control the desired signiﬁcance level
of the entire procedure. Another sequential procedure is to test the hypothesis of
q = q0 against q = q0 + 1, if it is rejected test q = q0 + 1 against q = q0 + 2, if it’s
rejected, etc., until H0 is accepted.
An alternative approach to determining the smallest number of factors is via
principal factor analysis, which we will introduce explicitly in the next section. This
is also known as an ad hoc procedure and it has the advantage of much simpler
computations. Currently no statistical theory justiﬁes this approach. A subjec-
tive assessment is made of when a “suﬃcient” amount of total variation has been
explained by the q factors.
In this thesis, we will pursue the approach of considering the simplest model, i.e.,
start with q = 0, and assess whether or not we have evidence against this model. If
we do, then we will proceed to assess the evidence against q = 1, etc.. Our assess-
ment criterion is via the computation of a P-value that we will subsequently discuss.
There is no attempt to state and control some overall error. In eﬀect we proceed via

CHAPTER 1.
INTRODUCTION
13
looking for surprising results rather than a decision-theoretic formulation. What is
surprising, e.g., when is a P-value is small, depends upon context and, as such, this
seems like a more appropriate approach to us.
1.2.4
Interpretations of factor loadings and factor scores
Perhaps one of the most challenging tasks in factor analysis is to interpret factors and
factor loadings. The interpretation of the factor loadings is relatively straightforward
as they are similar to the regression coeﬃcients in regression analysis. However, we
are more interested in the implications from the factor loadings, namely, the factor
scores.
The statistical meaning of each element in a factor loading matrix is the strength
of relation of some manifest variable with some latent variable. Suppose some of
the manifest variables are highly loaded on one factor, while the others have near
zero factor loadings with respect to this factor. In such a case we try to interpret
the corresponding factor score in terms of these manifest variables.
Often practitioners rotate the factor loadings in an attempt to obtain some
intuitively meaningful factor scores. This sometimes is viewed as a practical beneﬁt
since it oﬀers ﬂexibility when interpreting without aﬀecting the validity of the factor
model. This can be criticized for its subjectivity since one may choose the rotation
freely.
One commonly used rotation strategy is the varimax rotation, which can result in
a “simple structure” for a factor model. It seeks a rotation, i.e., a linear combination
of the original factors such that the variance of the factor loadings is maximized,
namely, we maximize
X
(γ2
ij −¯γ2
ij)2.
with γ2
ij being the squared factor loading of ith variable on jth factor and ¯γ2
ij being

CHAPTER 1.
INTRODUCTION
14
the squared mean of the factor loadings. Usually, each loading matrix is scaled to
length one to reach computational stability. The goal is to attain a factor loading
matrix with as many near zero loadings as possible. There are some other rotation
techniques such as oblimin and quartimin (Thurstone [37], Neuhaus and Wrigley
[31]). However, we will not address this issue further in this thesis.
1.3
Classical Factor Analysis
The two most common classical factor analysis methods are principal factor analysis
(Hotelling [15]) and maximum likelihood factor analysis (Bartlett [3], Lawley [24],
Lawley [25], Madia, Kent and Bibby [29], Rao [34]).
1.3.1
Principle factor analysis
Factor analysis and principal component analysis share some common features yet
are diﬀerent in various ways. They both attempt to explain the original data by fewer
dimensions. Principle component tries to account for as much of the total variances
of the data as possible and comprises a linear transformation of the original variables.
Factor analysis tries to accountable for the covariances among the variables and
transform the latent variables to observed variables. Principal factor analysis follows
the rationale of principal component analysis and often serves as an ad hoc procedure
for determining the number of factors. Principal factor analysis is used when models
(1.1) and (1.3) are well deﬁned, namely, when (1.5) is greater than 0.
Usually, people work with standardized variables to avoid scaling issues. One
starts with some preliminary estimates of the communalities ˆh2
i , so the diagonal
elements of R −Ψ can be estimated by ˆh2
i . Then R −ˆΨ is written by the spectral

CHAPTER 1.
INTRODUCTION
15
decomposition theorem as:
R −ˆΨ =
p
X
i=1
λiviv′
i
(1.7)
where λi’s are the eigenvalues with λ1 ≥· · · ≥λp of R −ˆΨ and vi’s are the
corresponding eigenvectors. One then chooses k(< p) nonnegative eigenvalues and
estimates Γk by
ˆΓk = VΛ1/2
where Λ = diag(λ1, · · · , λk) and V = (v1, · · · , vk). Then, we update the estimates
for communality h2
i , namely, ˜h2
i = Pk
j=1 ˆγ2
ij. Finally we obtain the estimates for Ψ
by
˜Ψ = diag(1 −˜h2
i )
One can repeat this process by substituting ˜Ψ into (1.7) until the satisfactory results
are acquired.
1.3.2
Maximum likelihood factor analysis
In general, if we can assume the data were generated from a distribution in the
statistical model {fθ : θ ∈Θ}, then the model parameter θ can be estimated by
maximizing the likelihood function L(θ|y), where y is the observed data. In a factor
analysis context, we often assume that data come from a p dimensional multivariate
normal distribution, i.e., y ∼Np(µ, Σ) and suppose Y = (y1, · · · , yn) is a sample.
The likelihood function for (µ, Σ) is then
L(µ, Σ|Y) = f(y1, · · · , yn|µ, Σ) =
1
|2πΣ|n/2
n
Y
i=1
exp[−1
2(yi −µ)
′Σ−1(yi −µ)],

CHAPTER 1.
INTRODUCTION
16
and the log-likelihood function is
l(µ, Σ|Y) = −n
2log|2πΣ| −1
2
n
X
i=1
[(yi −µ)
′Σ−1(yi −µ)].
The maximum likelihood estimate of µ is ¯y. The log-likelihood function becomes
l(¯y, Σ|Y) = −n
2log|2πΣ| −1
2tr[nΣ−1S],
(1.8)
where S =
1
n−1
Pn
i=1(yi −¯y)
′(yi −¯y). It is easy to show that S is the MLE of Σ.
Now substituting ΓqΓ
′
q + Ψ for Σ then (1.8) becomes
l(¯y, Γq, Ψ|Y) = −np
2 log(2π) −n
2log|ΓqΓ
′
q + Ψ| −1
2tr[(ΓqΓ
′
q + Ψ)−1S].
(1.9)
We want to maximize (1.9) with respect to Γq and Ψ to obtain their MLE’s. Due
to nonidentiﬁability issues it is customary to add a constraint such as requiring that
Γ
′
qΨ−1Γq be diagonal, although it is not clear how this is to be interpreted.
Minimizing l(¯y, Γq, Ψ|Y) is equivalent to maximizing
log|ΓqΓ
′
q + Ψ| + tr[(S1/2ΓqΓ
′
q + Ψ)−1S1/2].
(1.10)
for Γq and Ψ. Let B = (ΓqΓ
′
q + Ψ)−1, and note that (1.10) can be written as
tr(S1/2BS1/2) + log|B−1| = tr(S1/2BS1/2) −log|S1/2BS1/2| + log|S|.
Note that tr(S1/2BS1/2) is the summation of the eigenvalues of S1/2BS1/2, namely,
Pn
i=1 λi and log|S1/2BS1/2| is the logarithm the product of the eigenvalues, namely,
log(Qn
i=1 λi). The next step is to take the derivatives with respect to λi’s and set

CHAPTER 1.
INTRODUCTION
17
them to zeros. Now we have a set of equations
1 −1
λi
= 0
and so ˆλi = 1. Therefore, the log likelihood function (1.9) is maximized at (Γq, Ψ)
satisfying
S1/2(ˆΓq ˆΓ
′
q + ˆΨ)−1S1/2 = I
(1.11)
since S1/2(ΓqΓ
′
q + Ψ)−1S1/2 is symmetrical.
Notice that (1.11) is equivalent to
ˆΓq ˆΓ
′
q + ˆΨ = S and since ˆΓ
′
q ˆΨ−1ˆΓq = D for some diagonal matrix D, we have
ˆΓq(I + D) = S ˆΨ−1ˆΓq
Thus, we want to solve for ˆΓq, ˆΨ given the following algebraic equations
ˆΓ(I + D) = S ˆΨ−1ˆΓq
D = ˆΓ
′
q ˆΨ−1ˆΓq
diagonal
ˆΨ = S −ˆΓq ˆΓ
′
q
diagonal.
The above equations are impossible to solve algebraically.
Lawley suggested an
iterative procedure which involves approximating ˆΨ, then solve for ˆΓq, then use
ˆΨ = S −ˆΓq ˆΓ
′
q to obtain a new approximation for ˆΨ, etc.
We notice that the maximum likelihood procedure can fail as evident in the fol-
lowing example in Lattin, Green and Carroll [26].
Example 1.3 Examination data
A researcher is interested in examining two components of attitude (denote A1 and
A2). She measures the ﬁrst component of attitude using three diﬀerent measurement
methods, and then measures the second component using the same three measure-

CHAPTER 1.
INTRODUCTION
18
ment methods.
In a survey of 100 individuals (all of whom returned completed
questionnaires), she collects information from each respondent on the following six
variables.
Y1
A measurement of A1 using measurement method 1
Y2
A measurement of A1 using measurement method 2
Y3
A measurement of A1 using measurement method 3
Y4
A measurement of A2 using measurement method 1
Y5
A measurement of A2 using measurement method 2
Y6
A measurement of A2 using measurement method 3
The researcher wants to test to be sure that her measures do in fact discriminate
between two diﬀerent components of attitude (i.e., that A1 and A2 are distinctly
separate constructs). In formulating her test, she wants to account for the fact that
there may be some correlation between variables that use the same measurement
method. For example, one may assume that the variance of X1 is attributable to
variation in the underlying component A1 plus measurement error.
However, if
some of this measurement error is attributable to the method (and is common to
all variables measured using this method), then the measurement for Y1 and Y4 are
not independent. The correlation matrix of this dataset is given in Table 1.1.
We analyze the dataset by proc factor in SAS. First, we ﬁt the 1-factor model
and the relevant SAS outputs are shown in Table 1.2. From the output, we reject
H0: no factors needed and also reject H0: only 1-factor needed. The 1-factor model
is not suﬃcient via the goodness of ﬁt test procedure. Plus, this one factor only
explains 36% of the total variation.
When the maximum likelihood algorithm attempts to ﬁt 2 factors, it fails because
one of the estimated communalities becomes greater than 1. The heywood option is

CHAPTER 1.
INTRODUCTION
19
Table 1.1: Correlation matrix of Attitude data
Y1
Y2
Y3
Y4
Y5
Y6
Y1
1
Y2
0.61
1
Y3
0.477
0.542
1
Y4
0.498
0.195
0.108
1
Y5
0.194
0.534
0.118
0.473
1
Y6
0.164
0.135
0.446
0.490
0.342
1
Table 1.2: Signiﬁcant tests based on 100 observations
Test
DF
Chi-square
Pr>Chisq
H0: No common factors
15
274.0091
< .0001
HA: At least one commom factor
H0: 1 Factor is suﬃcient
9
149.4555
< .0001
HA: More factors are needed
used to force such estimate to be 1, so the algorithm can proceed and it produces
the following results in Table 1.3.
For this real data example, the ﬁrst and second eigenvalues are inﬁnite, this is
a clear indication that something is wrong. Notice that in the “Factor Pattern”
of Table 1.3, the second variable and the factor one is perfectly correlated and the
fourth variable and the factor 2 is almost perfectly correlated. The variables Y2 and
Y4 have communality over 1, therefore, their weights are assigned as missing/inﬁnite/
values. More explicitly, P2
j=1 γ2
2j and P2
j=1 γ2
4j are expected to lie between 0 and
1 since we used a correlation matrix. When they are equal or greater than 1, the
unique variance ψ2 and ψ4 become less or equal to 0, this renders an invalid solution.
This situation is referred as a Heywood case, which the maximum likelihood method
is susceptible to, since the iteration algorithm assigns a high weight to the variable

CHAPTER 1.
INTRODUCTION
20
Table 1.3: Factor method: Maximum Likelihood
Eigenvalues of the Correlation Matrix
Eigenvalues
Diﬀerence
1
Infty
Infty
2
Infty
Infty
3
0.72187984
0.26001945
4
0.46186003
0.99134698
5
-0.52948694
0.12476562
6
-0.65425257
Factor Pattern
Factor1
Factor2
Y1
0.61000
0.38647
Y2
1.00000
-0.00000
Y3
0.54200
0.00236
Y4
0.19500
0.98080
Y5
0.53400
0.37609
Y6
0.13500
0.47275
Variance Explained by Each Factor
Weighted
Unweighted
Factor1
1.71487256
2.00727000
Factor2
0.85352957
1.47627508
Final Communality Estimates and Variable Weights
Communality
Weight
Y1
0.52145825
2.08968183
Y2
1.00000000
Infty
Y3
0.29376955
1.41596839
Y4
1.00000000
Infty
Y5
0.42659946
1.74398163
Y6
0.24171782
1.31877028

CHAPTER 1.
INTRODUCTION
21
with high communality and this tends to increase its communality, which increases
its weight, and so on.
The maximum likelihood algorithm does not seem work
properly in this example. In fact, such a situation occurs quite often in practice,
especially when the correlations are large.
1.4
Summary
In this thesis we present a new Bayesian approach to factor analysis. In particu-
lar, we are interested in assessing whether there are any latent factors as well as
the number of such factors. As opposed to current Bayesian methodology for fac-
tor analysis, our approach only requires the speciﬁcation of a prior for the mean
vector and the variance matrix for the manifest variables. This results in a consid-
erable simpliﬁcation of the problem, especially with respect to the elicitation of the
prior. In particular, the usual Bayesian approach requires a quite complicated prior
speciﬁcation.
In Chapter 1 we introduce the factor analysis model and discuss its character-
istics. Furthermore, we discuss the classical analysis of this model, via maximum
likelihood methods, and point to some deﬁciencies in this approach.
In Chapter 2 we provide a general discussion of Bayesian analysis and of relative
surprise inferences and their properties. We thoroughly discuss the traditional ap-
proach to a Bayesian factor analysis and extend this to include a prior on the mean.
Furthermore, we present a number of diﬀerent priors that can be used for the mean
and variance matrix of the manifest variables.
In Chapter 3 we present the essence of our approach to factor analysis. This
entails comparing the concentration of the prior about a hypothesized factor model
with the concentration of the posterior about this model. When the posterior does

CHAPTER 1.
INTRODUCTION
22
not concentrate suﬃciently about this model, when compared to the prior concen-
tration, we have evidence against the model. Accordingly, we discuss the general
concept of measuring the concentration of a probability distribution about a set
and apply this to the factor analysis model. Several algorithms are presented for
the implementation of this approach and thoroughly analyzed.
Also, estimation
procedures are derived for a standardized factor loading matrix, when a particular
factor model is to be used.
In Chapter 4 we consider the choice of a prior on the variance matrix when
this is speciﬁed by a prior on the individual variances of the manifest variables
and a proper uniform prior on their correlation matrix. Accordingly, we need only
elicit priors for the means and variances of the manifest variables and this can be
easily implemented, e.g., using the elicitation procedure discussed in Chapter 2.
By contrast, elicitation for correlations is very diﬃcult; a step that is completely
avoided with this prior. A computational problem associated with the use of this
prior is solved by the use of importance sampling for the posterior analysis.
In Chapter 5 we consider some applications of our approach and compare our
results to those obtained by others.

Chapter 2
Bayesian Inference
2.1
Introduction
In sampling theory, inferences are made based on a statistical model {Pθ : θ ∈Θ}
for the observed the data X. Bayesian inference, however, introduces some prior
knowledge about θ into inference. In the Bayesian framework, one can make prob-
ability statements about the unknown parameters, via the principle of conditional
probability.
For this we introduce a marginal distribution for θ, namely, a prior Π, which
represents what is known about the unknown parameter θ before data is obtained.
This speciﬁes a joint probability distribution for (X, θ), given by
π(θ)pθ(X)
where π is the prior density of θ, with respect to support measure ν on parameter
space Θ, and pθ is the density of X with respect to support measure υ on the sample
space X .
If Π is absolutely continuous, we can make probability statements about X, prior
23

CHAPTER 2.
BAYESIAN INFERENCE
24
to observing X, via the prior predictive distribution with density
m(X) =
Z
Θ
π(θ)pθ(X)dθ.
(2.1)
If Π is discrete, then the prior predictive probability function for X is
m(X) =
X
θ∈Θ
π(θ)pθ(X)
The prior is updated, after observing the data, to obtain the posterior distribu-
tion Π(·|X) with the density given by
π(θ|X) = π(θ)pθ(X)
m(X)
.
(2.2)
Notice that
π(θ|X) = Cπ(θ)pθ(X),
where C is equal to 1/m(X). Accordingly, if we recognize the functional form of
π(θ)pθ(X) as a function of θ, we may not need to evaluate m(X) to obtain the
posterior.
The ingredients to a Bayesian analysis comprise the sampling model {Pθ : θ ∈
Θ}, a prior Π and the observed X. A prior distribution Π is used to represent prior
knowledge and plays an important role in the analysis. The challenge is how to
specify a prior that reﬂects our beliefs and will not dominate in a way to distort the
truth (what the data is trying to say). In fact, for suﬃciently large n, the data will
tend to overwhelm the contribution of the prior.
Consider the following example.
Example 2.1.1 Bernoulli (θ)
A coin of unknown probability of heads θ, is tossed n times, obtaining X = (x1, · · · , xn)

CHAPTER 2.
BAYESIAN INFERENCE
25
where
xi =





1
head
0
tail
If an investigator believes that θ ∈(0.4, 0.6), then one can specify a prior that
places most of its mass on this interval, e.g., θ ∼Beta(α, β), where α and β are cho-
sen to reﬂect this belief. The resulting posterior distribution is Beta(α + P
i xi, β +
n −P
i xi). On the other hand, if no prior knowledge is known, one may specify
a uniform prior on the support region of θ, e.g., θ ∼Uniform (0, 1). The resulting
posterior distribution under the uniform prior is Beta(P
i xi + 1, n −P
i xi + 1).
An unacceptable inference may result from inappropriate model assumptions. In
the Bayesian framework, other than data X, each input of the inferential system is
exposed to criticism and those inputs need to be justiﬁed in light of the data. For
example, if X is “surprising” for each θ ∈Θ, then we have evidence that the model
is not correct. There are many frequentist methods for checking the model. If T(X)
is the minimum suﬃcient statistic of {Pθ : θ ∈Θ} then P(·|T(X)) is distributed
independently of θ.
Accordingly, we can compare x to P(·|T(X)) to assess the
model. Consider the following example.
Example 2.1.2 Location-Scale Normal
Suppose that X = (x1, · · · , xn) ∼N(µ, 1) where µ ∈R. Then T(X) = ¯x is a
minimal suﬃcient statistic, and X|T(X) is given by the distribution of X −¯x1,
which is Nn(0, I −11
′
n ). So we compare X −¯x1 to this distribution to check the
sampling model.
If the sampling model is appropriate, the Bayesian model may fail because the
prior is placing its mass primarily on distributions in the sampling model for which

CHAPTER 2.
BAYESIAN INFERENCE
26
the observed data is surprising. This is referred to as a prior-data conﬂict. One
resolution to prior-data conﬂict is to increase the amount of data, as the eﬀect of
the prior will vanish in the limit. A good discussion of methods for checking for
prior-data conﬂicts is presented in Evans and Moshonov ( [10], [11]).
2.2
Bayesian Estimation and Credible Regions
For any statistical analyses, we need to specify estimation methods, how to con-
struct credible regions and how to make hypotheses assessments. In the Bayesian
framework, all probabilities concerning the parameter of interest are based on the
posterior. The principle of conditional probability asserts that the posterior distri-
bution π(θ|X) summarizes the current state of knowledge about θ.
Suppose we are interested in making inference about a characteristic of interest,
namely, τ = T(θ) where T : Θ →T . Any probability statements about τ must be
conditional on X and inferences about τ are determined in the same way as inferences
about θ. In general, we need to determine the marginal posterior distribution of τ.
The posterior probability measure of τ is given by
ΠT(·|X) = Π(T −1(·)|X)
(2.3)
Sometimes, it is diﬃcult to obtain the closed form for the density of posterior distri-
bution of τ. In such a situation, we can use computational techniques to approximate
the expectations of parameters of interest.
A common estimation procedure is maximum a posteriori (MAP). For this we
obtain the mode of the posterior density of τ, namely,
ˆτ = argmaxτπT(·|X).

CHAPTER 2.
BAYESIAN INFERENCE
27
Notice that the MAP estimate coincides with Maximum Likelihood estimate when
the prior is uniform. Also it can be shown to be a Bayes rule when using the 0 −1
loss function.
Alternatively, the posterior mean, if it exists, is used, namely,
ˆτ = E(T(θ)|X).
This is a Bayes rule when the quadratic loss function is used.
We can also summarize the information in the posterior distribution by means
of highest posterior density regions (HPD regions). A γ-HPD region is deﬁned by
Dγ(X) = {τ : πT(τ|X) ≥c} where c is the smallest value such that
Π(Dγ(X)|X) ≥γ.
(2.4)
Such a region has smallest possible volume among all subsets containing γ of the
posterior probability when the parameter space is a subset of Rk. Notice that when
the mode of ˆτ is unique, then Dγ(X) ↓{ˆτ} as γ ↓0.
A major defect of HPD-based inferences, such as the mode, is that they are not
invariant under smooth reparameterizations. Suppose that ψ = Ψ(τ) and Ψ is a
1-1, smooth transformation. Then if Dψ
γ (X) is a γ-HPD region for ψ, it is generally
not the case that Dγ(X) = Ψ−1Dψ
γ (X). This is a clear inconsistency, as whether we
make inference about ψ or τ should not lead to fundamentally diﬀerent inferences.
2.3
Bayesian Hypothesis Assessment
We require a procedure to assess the null hypothesis H0 ⊂Θ. When H0 consists
of a single value θ0, namely, H0 = {θ0}, this is referred to as a problem of point

CHAPTER 2.
BAYESIAN INFERENCE
28
hypothesis assessment.
There are several approaches to hypothesis assessment.
The null hypothesis
H0 could be judged to be incompatible with the observed data x if H0 has a low
posterior probability given by Π(H0|X).
Consider the following example.
Example 2.3.1
Suppose τ = T(θ) and H0 = T −1{τ0}. To assess H0, we can compute the posterior
probability
Π(T(θ) = τ0|X) = Π(T −1{τ0}|X)
(2.5)
and we have evidence against H0 if (2.5) is small.
Now consider C ⊆Θ and we want to assess H0 : θ ∈C versus Ha : θ /∈C.
Let T(θ) = IC, where IC = 1 if θ ∈C and IC = 0 if θ ∈Cc. Then we can assess
H0 : T(θ) = 1 versus Ha : T(θ) = 0 and calculate the posterior probability
Π(T(θ) = 1|X) = Π(C|X).
When Π is continuous and H0 is a lower dimensional subset of the parameter
space, then Π(H0) will be 0. Thus the posterior probability Π(H0|X) = 0, no matter
what data are observed and so we will always reject H0. This result is clearly not
sensible and one resolution is to compute the Bayesian P-value instead. For this
suppose we want to assess H0 : τ = τ0 and πT(·|X) is the posterior density of τ.
Then we compute
Π(π(τ|X) ≤π(τ0|X)|X).
(2.6)
and if this is small conclude that we have evidence against H0. The Bayesian P-
value is based on the hpd principle as (2.6) is equal to 1 −inf{γ|τ0 ∈Dγ(X)}. Here

CHAPTER 2.
BAYESIAN INFERENCE
29
we calibrate whether what we observed is a surprising value compared to all other
possible values for the given model, so we also call this the observed surprise. Notice
that again this inference is invariant under 1-1 smooth reparameterizations.
Alternatively, we can put a discrete mass to H0 and place a prior within Hc
0 to
resolve this diﬃculty in point hypothesis assessment. It may actually be appropriate
in many cases as it does reﬂect people’s belief that H0 deserves sizable amount
of prior probability. This approach encounters other diﬃculties as shown in the
following example.
Example 2.3.2 Lindley’s Paradox
Suppose X = (x1, · · · , xn) is a sample from N(θ, 1) where θ ∈R1, and we want
to assess H0 : θ = 0. A natural prior for θ is Π1 = N(0, σ2) as it is conjugate
for location normal. By taking σ2 large enough, we place a diﬀuse prior reﬂecting
ignorance of knowledge about the location. We have to place a positive mass p to
0 to proceed as discussed above. Thus a prior Π2 mixes Π1 with δ0 a degenerate
probability measure at 0, namely, Π2 = pδ0 + (1 −p)Π1. The posterior distribution
of θ under Π1 is N((n + 1/σ)−1n ¯X, (n + 1/σ)−1). Under the mixture prior Π2, we
have
Π2(H0|X) =
p
(1 −p)(1 + nσ2)−1
2exp[n2
2 (n + 1
σ2)−1 ¯X2] + p
.
and the Bayes factor (ratio of posterior odds to prior odds) in favor of H0 equals
BFH0 =
√
1 + nσ2 · exp[n2
2 (n + 1
σ2)−1 ¯X2].
If we ﬁx √n ¯X and let σ2 →∞, then Π2(H0|X) →1 and BFH0 →∞. Therefore,
with a prior suﬃciently diﬀuse on Hc
0, we will always accept H0. However, suppose
√n ¯X = 10 is ﬁxed for each n. The frequentist approach, namely, the Z-test will
categorically reject H0, but the Bayesian approach will accept it under a suﬃciently

CHAPTER 2.
BAYESIAN INFERENCE
30
large σ2. We call this conﬂict Lindley’s Paradox as we expect the two approaches
would agree when we use a diﬀuse prior.
Bayes factors comprise a method of hypothesis assessment in Bayesian inference.
This involves computing the posterior odds in favor of H0 versus the prior odds in
favor of H0, as shown in the above example. Calibrating Bayes factor is a necessary
step but it does not seem obvious, e.g., whether a Bayes factor of 3 is large or small?
2.4
Relative Surprise Inferences
Let us assume Λ is a measure on T with density λ with respect to νT. Note that
Λ can be volume measure or counting measure. If ΠT(·|X) is absolute continuous
with respect to Λ, then
dΠT(·|X)
dΛ
(τ) = πT(τ|X)
λ(τ)
.
In hypothesis assessment contexts we have a hypothesized true value τ0 ∈T for
T(θ) and we are required to assess the hypothesis using the evidence provided by
the data. We may compute
ΠT
πT(τ|X)
λ(τ)
> πT(τ0|X)
λ(τ0)
|X

(2.7)
as a measure of surprise of H0 : τ = τ0. We call (2.7) the observed relative surprise
at τ0 with respect to Λ. For example, if Λ is volume measure and λ(τ) = 1, then
(2.7) is the complement of the Bayesian P-value (2.6).
An obvious question here is now we should choose the measure Λ. For a variety
of reasons that we will subsequently discuss, we take Λ = ΠT, namely, the marginal
prior probability measure of τ.
The relative surprise principle, as discussed in Evans [6] makes use of the follow-

CHAPTER 2.
BAYESIAN INFERENCE
31
ing preference ordering on the possible values of τ. We totally order the elements
of T so that τ1 is strictly preferred to τ2 if the relative increase in belief for τ1, from
a priori to a posteriori, is greater than the corresponding increase for τ2. This can
be translated mathematically into strictly preferring τ1 to τ2 whenever
πT(τ1|X)
πT(τ1)
> πT(τ2|X)
πT(τ2)
(2.8)
We call πT(τ|X)/πT(τ) the relative belief ratio of τ. Notice that (2.8) may seem
unusual, but it is determined in a similar way as (2.7) by taking the support measure
Λ to be the prior. We can use (2.8) to determine inferences and note that (2.8) is
invariant under smooth transformations of τ as the Jacobian factor cancels out in
both ratios.
For point estimations, we select a value τ ∈T that maximizes π(τ|X)/π(τ),
i.e., a value in T that has the greatest relative increase in belief from a priori to a
posteriori. So we compute the estimator by maximizing this ratio as a function of
τ, i.e.,
ˆτ = argmaxτ
π(τ|X)
π(τ)
(2.9)
and we call ˆτ a least relative surprise estimate (LRSE).
Suppose we have a hypothesized value τ0 ∈T for T(θ) and we need to assess
the hypothesis based on the evidence provided by the data. Then (2.8) leads to
comparing the relative increase in belief for τ0, from a priori to a posteriori, with
this increase for each of other values in T . If the increase for τ0 is small compared
to the increases of other values, then the data suggests that τ0 is surprising and we
have evidence against the hypothesis. We use the posterior probability of obtaining a
relative increase larger than the observed for τ0 and refer to this now as the observed

CHAPTER 2.
BAYESIAN INFERENCE
32
relative surprise (ORS) for τ0, namely,
ΠT
πT(τ|X)
πT(τ)
> πT(τ0|X)
πT(τ0) |X

(2.10)
The ORS can be interpreted as the probability of a greater change in beliefs for some
other value in T . If this is large, then we obtain evidence against the hypothesized
value τ0. Notice that the LRSE is a value in T that minimizes the ORS. The LRSE
is the most supported value by the data, thus it is least surprising.
Also, the ORS can be viewed as a kind of calibration of the Bayes factor as
evident in the following example.
Example 2.4.1
Suppose we want to assess H0 ∈Θ where 0 < Π(H0) < 1 and let
T(θ) = τ =





H0
θ ∈H0
Hc
0
θ /∈H0
Therefore,
ORS(H0)
= ΠT( π({τ}|X)
π({τ}) > π(H0|X)
π(H0) |X)
=





0
BFH0 ≥1
Π(Hc
0|X)
BFH0 < 1
where
BFH0 = Π(H0|X)
Π(Hc
0|X)/Π(H0)
Π(Hc
0).
(2.11)
is the Bayes factor in favor of H0.
Now we may compute the posterior probability of obtaining a value that is larger
than (2.11), that is
Π(BFT(θ) > BFH0|X).
(2.12)

CHAPTER 2.
BAYESIAN INFERENCE
33
A large value of (2.12) indicates that H0 is surprising. Notice that (2.12) agrees
exactly with ORS. More generally, if Hǫ
0 ↓{τ0} as ǫ ↓0, then
Π(Hǫ
0|X)
1 −Π(Hǫ
0|X)/
Π(Hǫ
0)
1 −Π(Hǫ
0) →πT(τ0|X)
πT(τ0) ,
and we may now view ORS as a calibration of the Bayes factor.
We can determine a γ-relative surprise region for τ ∈T in the exact same way
as the HPD regions (2.4). A γ-relative surprise region for τ is given by
Cγ(X) =

τ0 ∈T : ΠT(πT(τ|X)
πT(τ)
> πT(τ0|X)
πT(τ0) |X) ≤γ

.
(2.13)
Notice that the γ-relative surprise region is invariant under smooth reparameteriza-
tions.
A general version of such a credible region Bγ(X) generated from Λ is deﬁned
by
Bγ(X) =

τ0 ∈T : ΠT(πT(τ|X)
λT(τ)
> πT(τ0|X)
λT(τ0) |X) ≤γ

.
We refer to such inferences as hpd-like inferences. This means that hpd regions are
determined using densities taken with respect to the volume measure (i.e., Λ=volume
measure). We now can establish optimality results for general hpd-like credible re-
gions and so for γ-relative surprise regions as well. These provide further justiﬁca-
tions for using such inferences.
Lemma 2.4.1. ΠT(Bγ(X)|X) ≥γ with equality whenever the distribution of πT(·|X)/λ(·)
has no atoms.
Proof. Evans, Guttman and Swartz [7].
Theorem 2.4.1. The set Bγ(X) minimizes Λ(C) among all measurable sets C ⊂T

CHAPTER 2.
BAYESIAN INFERENCE
34
satisfying ΠT(C|X) ≥ΠT(Bγ(X)|X). When the distribution of πT(·|X)/λ(·) has no
atoms, then Bγ(X) minimizes Λ(C) among all measurable sets C ⊂T satisfying
ΠT(C|X) ≥γ.
Proof. Evans, Guttman and Swartz [7].
By taking Λ = ΠT, we have the following corollary.
Corollary 2.4.1. The γ-relative surprise region Cγ(X) has smallest prior measure
among all measurable sets C ⊂T satisfying ΠT(C|X) ≥ΠT(Cγ(X)|X) and
ΠT(Cγ(X)|X) = γ whenever the posterior distribution of πT(·|X)/λ(·) has no atoms.
Recent developments of optimality properties due to Evans and Shakhatreh [12]
are concerned with how regions perform at the relevant values of θ ∈Θ as deter-
mined by the prior. Note that any γ-credible region Bγ(X) ⊂T satisﬁes
γ ≤
R
X ΠT(Bγ(X)|X)M(dx) =
R
X
R
T IBγ(X)(t)ΠT(dt|X)M(dx)
=
R
Θ
R
X IBγ(X)(T(θ))Π(dθ|X)M(dx) =
R
Θ
R
X IBγ(X)(t)Pθ(dx)Π(dθ)
=
R
Θ Pθ(T(θ) ∈Bγ(X))Π(dθ),
(2.14)
where M is the prior predictive measure with density (2.1).
Notice that (2.14)
asserts that the prior probability that Bγ(X) contains the true value of τ is at least
γ on average. Suppose now we want to choose Bγ(X) among the γ-credible region
such that Λ(Bγ(X)) is minimized for each x. Such a Bγ also minimizes
R
X Λ(Bγ(X))M(dx)
=
R
Θ
R
X
R
T IBγ(X)(t)Λ(dt)Pθ(dx)Π(dθ)
=
R
Θ
R
T Pθ(t ∈Bγ(X))Λ(dt)Π(dθ).
(2.15)
Note that (2.15), when Λ is a probability measure, is precisely the prior probability
that Bγ covers a false value, i.e., a value that is drawn from Λ independently of
θ ∼Π and x ∼Pθ. By taking Λ = ΠT, we have the following corollary.

CHAPTER 2.
BAYESIAN INFERENCE
35
Corollary 2.4.2. A γ relative surprise region Cγ minimizes the prior probability
of covering a false value τ ∼ΠT independently of θ ∼Π and x ∼Pθ among all
γ-credible regions.
As we mentioned earlier, we may view the ORS as a kind of calibration for Bayes
factors. The Bayes factor in favor of a set C ⊂T is given by (2.11), replacing H0
by C. It measures the change in our belief that C contains the true value from a
priori to a posteriori. Perhaps a more straightforward and intuitive measure of this
change in belief in C is what we called relative surprise ratio, namely,
RBC(X) = ΠT(C|X)
ΠT(C)
Note that as C →{τ0} ∈T , RBC(X) converges to πT(τ0|X)/πT(τ0). Let us consider
Cγ(X) for t as in (2.13), notice that ΠT(πT(τ|X)/πT(τ) > k|X) is right continuous
in k. Then
Cγ(X) =

τ : πT(τ|X)
πT(τ)
> kγ(X)

,
where kγ(X) = inf{k : ΠT(πT(τ|X)/πT(τ) > k|X) ≥γ}. Then we have the following
properties for relative surprise regions.
Lemma 2.4.2. The relative surprise region Cγ(X) satisﬁes RBCγ(X)(X) > kγ(X)
Proof. Evans and Shakhatreh [12].
Lemma 2.4.3. The relative surprise region Cγ(X) satisﬁes BFCγ(X) > 1 and
RBCγ(X)(X) > 1.
Proof. Evans and Shakhatreh [12].
From the above lemma, we always observe an increase in belief in the set Cγ(X)
from a priori to a posteriori according to the Bayes factor.
In particular, the
posterior probability content of Cγ(X) is always greater than its prior content.

CHAPTER 2.
BAYESIAN INFERENCE
36
Theorem 2.4.2. A relative surprise region Cγ(X) has the maximal Bayes factor and
maximal relative belief ratio among all sets C satisfying ΠT(C|X) = ΠT(Cγ(X)|X)
Proof. Evans and Shakhatreh [12].
Finally, we have the following satisfying property for relative surprise regions.
Theorem 2.4.3. A γ-relative surprise region has the property that the prior proba-
bility of containing the true value is always greater than or equal to the prior prob-
ability of covering a false value, i.e., the relative surprise regions are unbiased.
Proof. Evans and Shakhatreh [12].
2.5
Prior elicitation
Elicitation is the process of formulating a person’s knowledge and beliefs about
one or more uncertain quantities into a (joint) probability distribution for these
quantities. Elicitation is generally conceived of as part of the process of statistical
modelling. It is a process for a statistician to recognize the meaning of the quantities
being formulated and translate knowledge about parameters into probabilistic form.
A good elicitation requires accuracy and eﬀectiveness of the translation. It can be
used as a method of specifying a prior distribution in the Bayesian context.
Bayesian inference is quite often criticized for its subjectivity, namely, diﬀerent
statisticians use diﬀerent priors for the same problem. Elicitation plays a role in
mediating subjectivity and helps to incorporate useful prior information into an
analysis.
It is extremely useful for making inference in statistical contexts as it
makes the statistician think about what is being elicited and what is reasonable
to believe about it.
Elicitation can turn posterior distributions into meaningful
quantities.
However, elicitation is by no means a straightforward process.
For

CHAPTER 2.
BAYESIAN INFERENCE
37
example, if we want to elicit a probability distribution for a continuous parameter
θ, then a statistician is required to elicit an inﬁnite collection of probabilities. This
task could necessarily require an enormous eﬀort, but it is important to remember
that the aim of elicitation is to capture the “big messages” in an expert’s opinion. For
example, if we estimate the average height of a human being, we strongly believe
that it’s not judicious to put more mass around 8 feet than 6 feet. For another
example, when a parameter is believed to be in a particular set, we may elicit a
prior distribution with support on that set.
A good reference for elicitation is given by Garthwaite, Kadane and O’Hagan
[14]. They summarize the elicitation process in four steps. First, the setup stage
involves selecting the expert(s), training the experts and identifying what aspects
of the problem to elicit, etc. The next stage is to elicit speciﬁc summaries of the
experts’ distributions for these aspects. Thirdly, one has to ﬁt a (joint) probability
distribution to those summaries. The last step is to assess the adequacy of elicitation
and identify if it’s necessary to return to the eliciting stages.
There are some fundamental psychological challenges when conducting elicita-
tions in practice.
For example, human judgements can be biased by hindsight,
conservatism and people expect a sample from a population to represent all the the
essential characteristics of the population, even if the sample is small. Researchers
have been attempting to counteract these deﬁciencies via various experimental pro-
cedures.
It is complex when conducting elicitation for the multivariate case. Since one
has to elicit joint probability distributions for two or more unknown variables, the
primary task is to determine the dependency among the variables. If, for sake of
convenience or other reasons, the variables are considered independent, the joint
probability distribution of those variables is just the product of their marginal

CHAPTER 2.
BAYESIAN INFERENCE
38
distributions.
Under the assumption of independence, we can view multivariate
elicitation as the aggregation of univariate elicitations. Alternatively, Bayesian hier-
archical models can be used to structure dependent variables in terms of conditional
independence. When there is no obvious way to formulate the problems in terms of
independent variables, one cannot avoid multivariate elicitations by modeling the
dependence structure via eliciting covariances or correlation coeﬃcients.
In Bayesian analysis, the prior distribution is used to convey the expert’s opinions
about unknown quantities, so some structure needs to be imposed on the unknown
quantities. In other words, we ﬁt a parametric distribution to the unknown quanti-
ties. One has to assume that his/her knowledge is well represented by a member of
a speciﬁed family of distributions.
When a parameter is believed to lie in [a, b], a simple elicited distribution is the
uniform distribution. This is often criticized for its simplicity as people would not
believe that the unknown quantity is as likely to be at the two end points as to be
near the center. Moreover, if a and b are not the absolute physical limits of the
unknown quantity, it is typically unreasonable to assume the probability outside
[a, b] is zero. Usually, more general distributions are chosen to quantify an expert’s
knowledge. The task is then reduced to eliciting appropriate hyperparameters.
For many sampling models, natural prior choices can be probability distribu-
tions from the conjugate family. The conjugate prior distributions are (computa-
tionally) convenient and often quite ﬂexible in representing a broad range of prior
beliefs. Modern computational methods, however, such as Markov Chain Monte
Carlo (MCMC), have freed Bayesian analysis from this computational constraint.
This provides maximal ﬂexibility and freedom to statisticians to elicit sensible priors.
Consider the following important example.

CHAPTER 2.
BAYESIAN INFERENCE
39
Example 2.5.1 Multiple linear regression
Suppose we have a regression model Y = xβ + ǫ where x = (x1, · · · , xk), β =
(β1, · · · , βk) and ǫ ∼N(0, σ2). The common conjugate priors for (σ2, β) is
β|σ ∼N(β0, σ2R0)
σ−2 ∼ω0χ2(δ0)
where ω0, β0 and R0 are hyperparameters needed to be elicited from an expert.
For example, Let us consider one of the hyperparameters. Several approaches
have been proposed to elicit β0 and they can be summarized as direct and indirect
methods. Direct methods require expert’s opinions about the unobserved quantities,
such as regression coeﬃcients, which do not seem appealing. Indirect methods are
advocated as people are asked questions about the observable quantities, such as
Y . The scheme of indirect methods is then to ask the expert’s prediction of Y at
each point of X = (x1, · · · , xn), such as the median of Y , the mean of Y , etc. If the
assessment of Y is denoted by y, then β0 is equated to (X
′X)−1X
′y.
Now let’s consider an example using four diﬀerent basic elicitation methods dis-
cussed in Garthwaite, Kadane and O’Hagan [14].
Example 2.5.2 Bernoulli (θ)
Suppose that θ is the proportion of students at the University of Toronto who are
male.
1. Quantile method: The expert is asked to specify his/her median estimate of θ and
also one or more quantiles (usually at least two) of his/her subjective distribution of
θ. It commonly can be assumed that the expert’s opinion can be well represented by
a beta distribution (conjugate distribution of Bernoulli distribution). A beta distri-

CHAPTER 2.
BAYESIAN INFERENCE
40
bution can then be selected from a table, see Winkler [40], listing several quantiles
with corresponding parameters.
2. Hypothetical future sample (HFS) method: The expert ﬁrst gives an estimate for
the proportion θ and then reassesses his/her opinion about θ in light of hypothetical
samples. A hypothetical sample can be in a form of a question, such as “Suppose a
random sample of 100 students were taken and 40 of them are male. What is the
probability that one additional student, chosen at random, is a male?”. In general,
there are quite a few such hypothetical samples ( hypothetical questions). Suppose a
Beta distribution is used to represent the expert’s belief about θ. For each hypothet-
ical sample, the hyperparamters will be uniquely determined via the pre-assessment
and post-assessment of θ. Then by averaging those hyperparameters, the desired
hyperparameter estimates are obtained.
3. Equivalent prior sample (EPS) method: The EPS method expresses an expert’s
opinion via an equivalent prior sample. Suppose the expert has some knowledge
concerning the students at University of Toronto. He/She is asked to determine two
numbers s and n that are roughly equal to having observed s male students in a
random sample of n students, assuming that he/she had very little knowledge about
the gender of University of Toronto students before seeing this sample. The prior
distribution is again a Beta distribution with hyperparameters s and n −s.
4. Probability density function (PDF) method: The expert is asked to specify a most
likely value for θ, say ˆθ and other points of the p.d.f. for θ. For example, the expert
may determine two values which are half as likely as ˆθ (at each side of ˆθ). Notice
that this method is analoguous to the quantile method.
Our goal is to translate our knowledge a priori about the data into meaningful
parameters. Almost all elicitation methods for prior distributions of scalar parame-

CHAPTER 2.
BAYESIAN INFERENCE
41
ters involve estimating simple quantities, namely, means and variances. Eliciting a
location parameter is more intuitive and straightforward than eliciting a variance.
Here we provide an example explicitly describing how to elicit the prior for the mean
and variance.
Example 2.5.3
Suppose we want to elicit a model for the mean and variance of the height of male
students at University of Toronto. Let X denote the height of a randomly selected
student. A N(µ, σ2) distribution seems to be a natural choice for human heights
(X). Based on a sample X1, . . . , Xn we want to make inferences about µ and σ2.
Before we do this, however, we need to choose a prior for (µ, σ2). We suppose that
our beliefs can be expressed by taking µ ∼N(µ0, σ2
0σ2) and 1/σ2 ∼Gamma(α0, β0),
i.e., the conjugate prior, but other choices could be made.
Our purpose here is
to illustrate how we propose to elicit relevant prior distributions for the mean and
variance. Other approaches can be taken.
Basically we want our choice of the prior to be based on what we know about the
possible distributions for the distribution of heights. For example, this information
certainly takes the form of intervals that we specify that will contain virtually all the
possible X values and intervals that will almost certainly contain the mean height.
As we now show, specifying such intervals leads to a choice of prior.
To elicit µ0 we specify an interval (m1, m2) that we believe with probability at
least 0.999 (we will interpret this value as virtual certainty for this application)
contains the true value of µ. Of course, we will choose the interval (m1, m2) to be
as short as possible. Then we put µ0 = (m1 + m2) /2 and since
0.999 ≤Φ
m2 −µ0
σ0σ

−Φ
m1 −µ0
σ0σ

= Φ
m2 −m1
2σ0σ

−Φ

−m2 −m1
2σ0σ

= 2Φ
m2 −m1
2σ0σ

−1,

CHAPTER 2.
BAYESIAN INFERENCE
42
we have that
σ2 ≤
m2 −m1
2
2  Φ−1 ((1 + 0.999)/2)
−2 σ−2
0 .
(2.16)
Now an interval that will contain all the possible X values with virtual certainty
is given by µ±σΦ−1 ((1 + 0.999)/2) . The length of this interval cannot be too short
or unrealistically long. So let s1 and s2 be lower and upper bounds, respectively, on
the half-length of this interval, implying
s2
1
 Φ−1 ((1 + 0.999)/2)
−2 ≤σ2 ≤s2
2
 Φ−1 ((1 + 0.999)/2)
−2 .
(2.17)
Then, combining (2.16) and (2.17), we can write
s2
2
 Φ−1 ((1 + 0.999)/2)
−2 =
m2 −m1
2
2  Φ−1 ((1 + 0.999)/2)
−2 σ−2
0
so
σ2
0 =
m2 −m1
2
2
s−2
2 .
Thus, the prior knowledge about possible values for X and µ combine to give us
reasonable choices for µ0 and σ2
0.
Also, from (2.17), we have that
s−2
2
 Φ−1 ((1 + 0.999)/2)
2 ≤1/σ2 ≤s−2
1
 Φ−1 ((1 + 0.999)/2)
2 .
(2.18)
Letting G(α0, β0, ·) denote the Gamma(α0, β0) cdf, we have from (4.9) that α0 and
β0 can then be determined by
G−1(α0, β0, (1 + 0.999)/2) = s−2
1 (Φ−1 ((1 + 0.999)/2))2
G−1(α0, β0, 1 −(1 + 0.999)/2) = s−2
2 (Φ−1 ((1 + 0.999)/2))2 .
(2.19)

CHAPTER 2.
BAYESIAN INFERENCE
43
Now supposing that we have parameterized the Gamma(α, β) density as
Γ−1(α)βαxα−1e−βx we have that
G(α0, β0, x) = P(X ≤x) = P(β0X ≤β0x) = G(α0, 1, β0x).
Therefore we can express (2.19) as
G(α0, 1, β0s−2
1
 Φ−1 ((1 + 0.999)/2)
2) = 0.9995
(2.20)
G(α0, 1, β0s−2
2
 Φ−1 ((1 + 0.999)/2)
2) = 0.0005.
(2.21)
We must solve (2.20) and (2.21) iteratively for α0 and β0. For this choose α1
to start and calculate x.9995 = G−1(α1, 1, (1 + 0.999)/2) which implies, using (2.20),
that β1 = x.9995s2
1/ (Φ−1 ((1 + 0.999)/2))2 . Then evaluate
G(α1, 1, β1s−2
2
 Φ−1 ((1 + 0.999)/2)
2).
(2.22)
If (2.22) is smaller than 0.0005 then decrease α1 to α2 and repeat the previous step.
If (2.22) is greater than 0.0005 then increase α1 to α2 and repeat the previous step.
Continue iterating like this until (2.21) is solved to a reasonable accuracy.
Consider now a numerical example and note that Φ−1 ((1 + 0.999)/2) = 3.29053.
So, if we measure heights in inches, then realistic choices are µ0 = 68 and m1 =
65, m2 = 71. Sensible choices for s1 and s2 are given by s1 = 3.29053 and s2 =
5(3.29053) since the interval 68 ± 3.29053 = (64.709, 71.291) seems unlikely to con-
tain all the heights and 68±5(3.29053) = (51.547, 84.453) deﬁnitely does. Therefore,

CHAPTER 2.
BAYESIAN INFERENCE
44
σ2
0 = 9(5(3.29053))−2 = 3.3248 × 10−2 and
s−2
1
 Φ−1 ((1 + 0.999)/2)
2 = 1
s−2
2
 Φ−1 ((1 + 0.999)/2)
2 = 25,
which says that the true value of σ lies in (1, 5) with virtual certainty and this seems
quite reasonable.
We iterated as described above and obtained the following results, where (2.22)
close to .0005 is used for the stopping criterion.
iteration
α
β
(2.22)
1
2
9.9987
0.0615
2
10
23.7492
0.0000
3
4
13.9340
0.0026
4
6
17.4106
0.0001
5
5
15.7099
0.0005
Accordingly, we set α0 = 5, β0 = 15.7099. In Figure 2.1 we have plotted the prior
density of 1/σ2.
Another natural prior to use here would be µ ∼N(µ0, σ2
0) and 1/σ2 ∼Gamma(α0, β0),
namely, µ and σ2 are independent.
Once we have selected (m1, m2) then µ0 =
(m1 + m2) /2 and (m1, m2) = µ0 ± σ0Φ−1 ((1 + 0.999)/2) speciﬁes
σ0 =
m2 −m1
2
  Φ−1 ((1 + 0.999)/2)
−1 .
Further, specifying s1 and s2 we can obtain α0 and β0 in exactly the same way as
with the previous prior.

CHAPTER 2.
BAYESIAN INFERENCE
45
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Sigma^(−2)
density
Figure 2.1: The elicited prior density of 1/σ2 in Example 2.5.3.
Regarding the prior choices in our thesis, we want to discuss the prior choices for
the mean and variance-covariance matrix. Eliciting appropriate priors for variance-
covariance matrices is by no means an easy task. For example, in Example 2.5.1 we
need to elicit σ2R0. A method for eliciting the variance-covariance matrix, σ2R0 is
proposed by Kadane, Dickey, Winkler, Smith and Peter [22]. This method involves
elicitation of a variance-covariance matrix for a multivariate t-distribution and it is
quite complicated.
For convenience sake, a common conjugate prior for variance-covariance matri-
ces when sampling from a multivariate normal is often adopted by statisticians,
namely, the Inverse Wishart. This also requires the elicitation of the hyperparame-
ters. Alternatively, one may decompose the variance-covariance matrices into several
components and elicit each component seperately. In this thesis, we separate the
variance-covariance matrices into scale and correlation components as discussed in
the subsequent section. With this separation, we may elicit the scale components
via one of the methods we have discussed, as in Example 2.5.3. Eliciting individual

CHAPTER 2.
BAYESIAN INFERENCE
46
correlations seems an hopeless task even with a moderate dimension, e.g., we must
ensure that the correlation matrix is positive deﬁnite. One possible prior for corre-
lation matrices is a uniform prior as the set of all correlation matrices is compact.
This particular choice of prior is discussed further in Section 2.6.4.
2.6
Bayesian Factor analysis
We consider two approaches to carrying out a Bayesian factor analysis. We will refer
to these as the model selection approach and the posterior concentration approach.
The model selection approach could be viewed as the standard and is described in
Lee [27] which, with some modiﬁcations, is the primary source for our presentation.
The model selection approach requires that we put a prior on (µ, Γq, Ψ) as well
as a prior on q ∈{0, · · · , k}. The posterior concentration approach requires a prior
on (µ, Σ).
We now describe relevant priors for these two approaches as well as aspects of the
posterior analysis. Most of the discussion for the posterior concentration approach
is deferred until Chapter 3.
2.6.1
The Model Selection Approach
For this approach, one has to specify p + 1 prior probabilities that the model with
i common factors is correct, namely, Π(Mi) for i = 0, · · · , p where
Mi = {P{µ,Σi}, µ ∈R1, Σi = ΓiΓ
′
i + Ψ}
(2.23)
as well as a prior on (µ, Γi, Ψ). Note that Γ0 = 0. We are then required to select
the appropriate model (2.23). A measure of plausibility of two models Mi and Mj is
provided by taking the ratio of the posterior probabilities ΠMi(·|Y)/ΠMj(·|Y) and

CHAPTER 2.
BAYESIAN INFERENCE
47
the MAP estimator selects the model with the largest posterior probability. If the
prior probabilities satisfy Π(M0) = Π(M1) = · · · = Π(Mp) then this is equivalent to
selecting the model with the largest Bayes factor.
BFMi =
Π(Mi|Y)
1 −Π(Mi|Y)/
Π(Mi)
1 −Π(Mi).
Note that in general the ratio of Bayes factors BFMi/BFMj is equal to the ratio of
the prior predictive densities mi(Y)/mj(Y).
2.6.1.1
Prior for (µ, Γq, Ψ)
Once we have determined the submodel Mq, we want to make inferences about µ, Γq
and Ψ and for this we need the posterior of these quantities. Suppose then that
we have the standard assumptions on the factor analysis model y = µ + Γqf + e,
namely, f ∼Nq(0, I) independent of e ∼Np(0, Ψ). In Theorem 2.6.1 we derive a
Gibbs sampling algorithm for a prior similar to a prior speciﬁed in Lee [27]. However,
our prior diﬀers from Lee’s development in two ways. First we place a ﬂat prior on
µ while Lee seems to ﬁx µ at the MLE (¯y). There appears to be little reason to do
this. Further Lee allows f ∼Nq(0, Φ) and an inverse Wishart prior is placed on Φ.
While this is possible we prefer to focus an factor analysis model where the common
factors are independent and identically distributed.
We then have the following
result, where Γk:q denote the vector given by the kth row of Γq and ψkk denote the
kth diagonal element of Ψ.
Theorem 2.6.1. Suppose the prior distribution for Θ = (µ, Γq, Ψ) is given by
µ ∼1,
which is independent of (Γq, Ψ) where the (Γk:q, ψkk) are independent for k = 1, · · · , p

CHAPTER 2.
BAYESIAN INFERENCE
48
with
Γk,q|ψ−1
kk ∼Nq(Γ0k, ψ−1
kk H0k)
ψ−1
kk ∼Gamma(α0kk, β0kk),
(2.24)
where µ0, Σ0, α0k, β0k, Γ0k, H0k are hyperparameters. Then we have that
(1) f1, · · · , fn are conditionally independent given Y, µ, Γq, Ψ with
fi|(Y, µ, Ψ−1, Γq) ∼Nq(bi, B),
(2)
µ|(Y, F, Ψ−1, Γq) ∼Np(µy, Ψ/n),
(3) (ψ11, Γ1:q), · · · , (ψpp, Γp:q) are conditionally independent given (Y, µ, F) and
Γk,q|(Y, µ, F, ψ−1
kk ) ∼Nq(ak, ψkkA)
ψ−1
kk |(Y, µ, F, Γq) ∼Gamma(n/2 + α0kk, βkk)
where µy = ¯y −Γq¯f, ak = A[H−1
0k Γ0k + F(Yk −µk1)], A = (H−1
0k + FF
′)−1, bi =
BΓ
′Ψ−1(yi −µ), B = (I + Γ
′Ψ−1Γ)−1 and βkk = β0kk + 1
2[(Yk −µk1)
′(Yk −µk1) −
a
′
kA−1ak + Γ0kH−1
0k Γ0k] with Y T
k being the kth row of Y, 1 being n × 1 vector of 1’s
and µk being the kth element of µ.
Proof.
p(F|Y, Θ) =
n
Y
i=1
p(fi|yi, Θ) ∝
n
Y
i=1
p(fi, yi|Θ) =
n
Y
i=1
p(fi)p(yi|fi, Θ)
(2.25)
where
p(fi) ∝exp[−1
2f
′
ifi]

CHAPTER 2.
BAYESIAN INFERENCE
49
and
p(yi|fi, Θ) ∝|Ψ|−1/2exp[−1
2(yi −Γqfi −µ)
′Ψ−1(yi −Γqfi −µ)]
(2.26)
Therefore,
p(fi|yi, Θ) ∝|Ψ|−1/2exp{−1
2[(yi −Γqfi −µ)
′Ψ−1(yi −Γqfi −µ) + f
′
ifi]}
Let B = (I+Γ
′
qΨ−1Γq)−1 and bi = BΓ
′
qΨ−1(yi −µ), then b
′
iB−1 = (yi −µ)
′Ψ−1Γq,
the exponential term of p(fi|yi, Θ) can be expressed as
−1
2[(yi −Γqfi −µ)
′Ψ−1(yi −Γqfi −µ) + f
′
ifi]
=
−1
2[y
′
iΨ−1yi + f
′
iΓ
′
qΨ−1Γfi + f
′
iΓ
′
qΨ−1µ + µ
′Ψ−1
i Γqfi + µ
′Ψ−1µ
−y
′
iΨ−1Γqfi −y
′
iΨ−1µ −f
′
iΓ
′
qΨ−1yi −µ
′Ψ−1yi + f
′
ifi]
=
−1
2{f
′
i(I + Γ
′
qΨ−1Γq)fi −(yi −µ)
′Ψ−1Γqfi −[(yi −µ)
′Ψ−1Γqfi]
′ + y
′
iΨ−1yi
+µ
′Ψ−1µi −y
′
iΨ−1µ −µ
′Ψ−1yi}
=
−1
2[(fi −bi)
′B−1(fi −bi)] + c
where c is a constant in yi. So the conditional distribution of fi given yi and Θ is
fi|(yi, Θ) ∼Nq(bi, B).
Further, the prior density for µ is proportion to 1. The joint prior density for
(Γq, Ψ−1) is given by p(Γq, Ψ−1) = p(Ψ−1)p(Γq|Ψ−1) and (Γj, ψjj) is independent
of (Γl, ψll) given (Y, µ) for 1 ≤j, l ≤p, j ̸= l from (2.26), therefore,
p(Γq, Ψ−1|Y, µ, F) =
p
Y
k=1
p(Γk, ψ−1
kk |Y, µ, F) ∝
n
Y
i=1
p
Y
k=1
p(ψ−1
kk )p(Γk|ψ−1
kk )p(yi|Θ, fi).
(2.27)

CHAPTER 2.
BAYESIAN INFERENCE
50
The conditional density of µ given (Y, F, Γ, Ψ) is proportional to
exp{−1
2[Pn
i=1(yi −Γqfi −µ)
′Ψ−1(yi −Γqfi −µ)]}
=
exp{−1
2µ
′(nΨ−1)µ −[n¯y
′Ψ−1 −Pn
i=1(Γqfi)
′Ψ−1]µ
−µ
′[Ψ−1n¯y −Pn
i=1 Ψ−1Γqfi] + Pn
i=1 y
′
iΨ−1yi −Pn
i=1 y
′
iΨ−1Γqfi
−Pn
i=1(Γqfi)
′Ψ−1yi + Pn
i=1(Γqfi)
′Ψ−1Γqfi}
∝
exp{−1
2[(µ −µy)
′nΨ−1(µ −µy)]},
where µy = ¯y −Γq¯f. Therefore,
µ|(Y, F, Γq, Ψ) ∼Np(µy, 1
nΨ).
Let Y
′
k being the kth row of Y, µk be the kth element of µ, 1 be the n × 1 vector
of 1’s and yik be the ith component of Y
′
k. By (2.27), the conditional density of
(Γk, ψkk) given (Y, µ, F) is proportional to
ψ
−n
2
kk exp{−1
2ψ−1
kk [Pn
i=1(yik −µk −Γ
′
kfi)2}(ψkk)−α0kk−1 exp{−βkk
ψkk }×
ψ
−q
2
kk exp{−1
2ψ−1
kk (Γk −Γ0k)
′H0k(Γk −Γ0k)}
=
ψ−(n/2+α0kk+1+q/2)
kk
exp{−1
2ψ−1
kk [(Γk −ck)
′(FF
′)(Γk −ck)
+(Γk −Γ0k)
′H−1
0k (Γk −Γ0k)] −ψ−1
kk ( 1
2dk + β0kk + Γ
′
0kH−1
0k Γ0k)}
=
{(ψ−1
kk )n/2+α0kk+1exp[−ψ−1
kk (β0kk + 1
2Γ
′
0kH−1
0k Γ0k + 1
2dk)]}
×{(ψ−1
kk )q/2exp{−1
2ψ−1
k [Γ
′
k(FF
′ + H−1
0k )Γk −c
′
k(FF
′)Γk −Γ
′
k(FF
′)ck
−Γ
′
kH−1
0k Γ0k −Γ
′
0kH−1
0k Γk]}}
=
{(ψ−1
kk )n/2+α0kk+1exp[−ψ−1
kk (β0kk + 1
2Γ
′
0kH−1
0k Γ0k + 1
2dk)]}
×{(ψ−1
kk )q/2exp{−1
2ψ−1
k {[Γk −(H−1
0k + FF
′)−1(H−1
0k Γ0k + (FF
′)ck)]
′
(H−1
0k + FF
′)[Γk −(H−1
0k + FF
′)−1(H−1
0k Γ0k + (FF
′)ck)]
−[(H−1
0k Γ0k + (FF
′)ck)
′(H−1
0k + FF
′)−1(H−1
0k Γ0k + (FF
′)ck]}

CHAPTER 2.
BAYESIAN INFERENCE
51
where ck = (FF
′)−1F(Yk −µk1) and dk = c
′
k(FF
′)ck = (Yk −µk1)
′F
′(FF
′)−1F(Yk −
µk1) = (Yk −µk1)
′(Yk −µk1). Hence,
p(Γk, ψ−1
kk |Y, µ, F) ∝[(ψ−1
kk )n/2+α0kk+1e−βkkψ−1
kk ][(ψ−1
kk )q/2e−1
2(Γk−ak)T (H−1
0k +FF
′)(Γk−ak)ψ−1
kk ]
where ak = A(H−1
0k Γ0k + F(Yk −µk1)), A = (H−1
0k + FF
′)−1, βkk = β0kk + 1
2[(Yk −
µk1)
′(Yk −µk1) −a
′
kA−1ak + Γ0kH−1
0k Γ0k]. Therefore,
ψ−1
kk |(Y, µ, F) ∼Gamma(n/2 + α0kk, βkk)
Γk,q|(Y, µ, F, ψ−1
kk ) ∼Nq(ak, ψkkA).
To implement Gibbs sampling algorithm, we iteratively sample one compo-
nent from its conditional distribution on other components. More speciﬁcally, by
choosing proper starting values for (µ, Γq, Ψ, F), say, µ(0), Γ(0)
q , Ψ(0) and F(0) =
(f(0)
1 , · · · , f(0)
n ), µ(1), Γ(1)
q , Ψ(1) and F(1) can be generated from using steps (1), (2)
and (3) of Theorem 2.6.1.
2.6.1.2
Noninformative prior for (µ, Γq, Ψ)
A common choice of prior is to use a noninformative prior. People do not always
have strong prior beliefs about the value of a parameter θ. How to choose a prior
to characterize “little is known a priori” has a long history of dispute. By “little is
known a priori”, we mean that we are almost equally willing to accept one value for
the parameter as another.
Example 2.6.1
Suppose that x = (x1, · · · , xn) is a sample from a N(µ, 1) distribution where µ ∈R1
is unknown. A possible prior for µ is N(µ0, σ2) distribution where µ0 is chosen by

CHAPTER 2.
BAYESIAN INFERENCE
52
elicitation. Taking σ very large reﬂects diﬀuse prior knowledge concerning the true
value of µ.
In Box and Tiao [5], they implicitly deﬁned noninformative priors as follows:
“Suppose it is possible to express the unknown parameter θ in terms of a metric
T(θ), so the the corresponding likelihood is data translated. This means that the
likelihood curve for T(θ) is completely determined a priori except for its location
which depends on data yet to be observed”. By taking T(θ) to be locally uniform,
and the resulting prior distribution is called noninformative for T(θ) with respect
to the data.
Sir Harold Jeﬀrey [18] proposed what is now called Jeﬀrey’s rule to determine a
noninformative prior on the grounds of its invariance under parameter transforma-
tions. The Jeﬀrey’s rule stated that the prior distribution for a single parameter θ is
noninformative if it is taken proportional to the square root of Fisher’s information
measure.
Example 2.6.2
Suppose that x = (x1, · · · , xn) is a sample from a N(µ, σ) distribution and σ is
known, the likelihood function of µ is
L(µ|x) ∝exp[−n
2σ2(µ −¯x)2]
(2.28)
where ¯x is the average of the observations. The likelihood curve for µ is completely
known a priori except for its location which is determined by ¯x. Therefore, (2.28)
is data translated in ψ(µ) = µ and a noninformative prior is given by
p(µ|σ) ∝c

CHAPTER 2.
BAYESIAN INFERENCE
53
where c is a constant. This prior also arises via Jeﬀrey’s rule.
Notice that this prior is improper and sometimes improper priors can produce
improper posteriors.
In the following we place a noninformative prior on µ, Γq and Ψ and then derive
a Gibbs sampler for the posterior.
Theorem 2.6.2. Suppose the prior distribution for Θ = (µ, Γq, Ψ) is given by
µ ∼1,
which is independent of (Γq, Ψ) where the (Γk:q, ψkk) are independent for k = 1, · · · , p
with
Γk:q|ψkk ∼1
ψkk ∼ψ−1
kk
The full conditional posterior distribution of (µ, Γq, Ψ, F) is given by
(1) f1, · · · , fn are conditionally independent given Y, µ, Γq, Ψ with
fi|(Y, µ, Ψ−1, Γq) ∼Nq(bi, B),
(2)
µ|(Y, F, Ψ−1, Γq) ∼Np(µy, Ψ/n),
(3) (ψ11, Γ1:q), · · · , (ψpp, Γp:q) are conditionally independent given (Y, µ, F) and
Γk,q|(Y, µ, F, ψ−1
kk ) ∼Nq(ck, ψkk(FFT)−1)
ψ−1
kk |(Y, µ, F, Γq) ∼χ2
n−q
where µy = ¯y −Γq¯fi, bi = BΓ
′
qΨ−1(yi −µ), B = (I + Γ
′
qΨ−1Γq)−1and ck =

CHAPTER 2.
BAYESIAN INFERENCE
54
(FF
′)−1F(Yk −µk1)
′ with Y
′
k being the kth row of Y, µk being the kth element of µ
and 1 being the n × 1 vector of 1’s.
Proof. The derivations of posterior distributions for µ and fi are shown in the proof
of Theorem 2.6.1.
Let Y
′
k being the kth row of Y, µk be the kth element of µ, 1 be the n×1 vector
of 1’s and yik be the ith component of Y
′
k. By (2.27), the conditional density of
(Γk, ψkk) given (Y, µ, F) is proportional to
ψ−n/2−1
kk
exp{−1
2ψ−1
kk [Pn
i=1(yik −µk −Γ
′
kfi)2}
=
ψ−(n/2+1)
kk
exp{−1
2ψ−1
kk [(Γk −ck)(FF
′)(Γk −ck)]}
=
[(ψ−1
kk )n/2−q/2+1e−1
2 ψ−1
kk ][ψ−q/2
kk
e−1
2ψ−1
kk (Γk−ck)
′FF
′(Γk−ck)],
where ck = (FF
′)−1F(Yk −µk1)
′.
Therefore,
ψ−1
kk |(Y, µ, F, Γq) ∼χ2
n−q
Γk,q|(Y, µ, F, Ψ) ∼Nq(ck, ψkk(FF
′)−1)
The sampling procedure can be carried out by using the Gibbs sampler speciﬁed
by Theorem 2.6.3.
2.6.1.3
Computing the Bayes factor (as in Lee)
To fully implement the model selection approach we need to select q and for this we
need the posterior probabilities for Mq (or at least their ratios) or the Bayes factors
in favor of Mq (or at least their ratios) for q = 0, · · · , p.
Suppose we introduce the latent variable u that selects the model Mi with prior
probability θi for i = 0, · · · , p, e.g., θi = 1/(p+1). Then adding this latent variable to

CHAPTER 2.
BAYESIAN INFERENCE
55
Theorem 2.6.1 and 2.6.2 does not change the sampling algorithms for the remaining
variables given that u = q. Given that dimension of the parameter space changes
as we change q, then a reversible jump Monte Carlo algorithm seems in order, see,
for example Green [21] and Lopes and West [28]. As our approach is diﬀerent, we
do not pursue this further here.
Lee (2007) advocates the use of path sampling to compute the ratio of normaliz-
ing constants to obtain the Bayes factors. This involves extensive computation and
we brieﬂy summarize the computing steps as follows.
For two models, e.g., M0 and M1, the importance sampling technique is recom-
mended if two models are close. Basically, we compute
m(k)
= p(Y|Mk) =
R
p(Y, F, Θ|Mk)dFdΘ,
k = 0, 1
=
R p(Y,F,Θ|Mk)
p(F,Θ|Y,Mk)p(F, Θ|Y, Mk)dFdΘ
= E( p(Y,F,Θ|Mk)
p(F,Θ|Y,Mk)),
(2.29)
where the expectation is taken with respect to p(F, Θ|Y, Mk). Then (2.29) can be
approximate by
m(k) = 1
I
I
X
i=1
p(Y, F(i), Θ(i)|Mk)
p(F(i), Θ(i)|Y, Mk),
where F(i), Θ(i) is drawn from the target distribution p(F(i), Θ(i)|Y, Mk) via simu-
lation methods. The ratio of the Bayes factor of M1 and M0, namely, B10 is given
by
B10 = m(1)
m(0).
Bridge sampling, see Meng and Wong [30], is preferable for a situation that M0
and M1 are not so close to each other. Instead of evaluating m(0) and m(1) directly,
an intermediate model M 1
2 is constructed so both of M0 and M1 are closer to M 1
2
than to each other. Then B10 is obtained by taking the products of two ratios. It

CHAPTER 2.
BAYESIAN INFERENCE
56
improves the eﬃciency and accuracy of the sampling procedure. Path sampling,
Gelman and Meng [19], is an extension of bridge sampling as path sampling uses
many bridges rather than one. If M0 and M1 are far apart, a series intermediate
models between M0 and M1 is constructed, say L −1 models. The Bayes factor is
equated to the product of the series of ratios. Taking the logarithm of the product
and letting L →∞, B10 is obtained. As our approach is somewhat diﬀerent, we do
not pursue this further here.
2.6.2
Priors and Posteriors on (µ, Σ)
In the posterior concentration approach to factor analysis, we place a prior on (µ, Σ)
only. This requires far less prior speciﬁcation than the model selection approach and
accordingly far less elicitations.
It is worth noting that the model selection approach also induces a prior on
(µ, Σ) by taking a mixture of the priors induced on (µ, Σ) by the priors for each Mi
where the mixture probabilities are the prior probabilities on the models (Press [32],
Press and Shigemasu [33]). It seems hopeless to determine any analytical properties
of this prior and relate these to anything we really know a priori about µ and Σ.
We consider two priors on (µ, Σ). The ﬁrst is conjugate and is commonly used,
perhaps more convenient than anything else. The second doesn’t seem to be used,
because of problems with posterior computations, but seems much more useful. We
will address the computational issue later in this thesis.

CHAPTER 2.
BAYESIAN INFERENCE
57
2.6.2.1
Conjugate prior for (µ, Σ)
Theorem 2.6.3. Suppose we observe Y = (y1, · · · , yn) ∼N(µ, Σ) and place a prior
on (µ, Σ) given by
µ|Σ ∼Np(µ0, σ2
0Σ)
Σ−1 ∼Wishartp(k0, A0),
(2.30)
where µ0, σ2
0, k0, A0 are the hyperparameters. The posterior is then given by
µ|(Y, Σ) ∼N(µy, Σy)
Σ−1|Y ∼Wishartp(n + k0, B),
(2.31)
where µy = (n +
1
σ2
0 )−1(n¯y +
1
σ2
0 µ0), Σy = (n +
1
σ2
0 )−1Σ, B−1 = (n −1)S + n¯y¯y
′ +
A0 +( 1
σ2
0 )µ0µ
′
0 −(n+ 1
σ2
0 )µyµ
′
y and S =
1
n−1(Y −1¯y)
′(Y −1¯y). Therefore, the priors
given in (2.30) are conjugate.
Proof. The joint likelihood function for Y is given by:
Qn
i=1 p(yi|µ, Σ) = Qn
i=1 |Σ|−1/2exp[−1
2(yi −µ)
′Σ−1(yi −µ)]
=
Qn
i=1 |Σ|−1/2exp[−1
2(µ −yi)
′Σ−1(µ −yi)]
=
Qn
i=1 |Σ|−1/2exp[−1
2(µ
′Σ−1µ −µ
′Σ−1yi −y
′
iΣ−1µ + y
′
iΣ−1yi)]
=
|Σ|−n/2exp[−1
2
Pn
i=1(µ
′Σ−1µ −µ
′Σ−1yi −y
′
iΣ−1µ + y
′
iΣ−1yi)]
=
|Σ|−n/2exp[−1
2(nµ
′Σ−1µ −Pn
i=1 yiµ
′Σ−1 −Pn
i=1 y
′
iΣ−1µ + Pn
i=1 y
′
iΣ−1yi)].
Now
p(µ, Σ−1|Y) ∝
n
Y
i=1
p(yi|µ, Σ−1)]p(µ|Σ−1)p(Σ−1)
(2.32)

CHAPTER 2.
BAYESIAN INFERENCE
58
and (2.32) is proportional to
|Σ|−n
2 −k0
2 −1exp{−1
2[nµ
′Σ−1µ −Pn
i=1 yiµ
′Σ−1 −Pn
i=1 y
′
iΣ−1µ
+ Pn
i=1 y
′
iΣ−1yi + 1
σ2
0 (µ −µ0)
′Σ−1(µ −µ0)] + tr[−1
2(A0Σ−1)]}
=
|Σ|−n
2 −k0
2 −1exp{−1
2{(µ −µy)
′[(n + 1
σ2
0 )Σ−1](µ −µy)
+ Pn
i=1(yi −¯y)
′Σ−1(yi −¯y) + n¯y
′Σ−1¯y + 1
σ2
0 (µ
′
0Σ−1µ0)
−µ
′
y[(n + 1
σ2
0 )Σ−1]µy} + [tr[−1
2(A0Σ−1)]}
=
|Σ|−n
2 −k0
2 −1exp{−1
2{(µ −µy)
′[(n + 1
σ2
0 )Σ−1](µ −µy)
+{tr[(n −1)S + n¯y¯y
′ + 1
σ2
0 µ0µ
′
0 −(n + 1
σ2
0 )µyµ
′
y + A0]Σ−1}}}
where µy=(n + 1
σ2
0 )−1(n¯y + 1
σ2
0 µ0) and S =
1
n−1(Y −1¯y
′)
′(Y −1¯y
′).
Therefore,
p(µ|Y, Σ−1) ∝exp{−1
2{(µ −µy)
′[(n + 1
σ2
0 )Σ−1](µ −µy)}}
p(Σ−1|Y) ∝|Σ|−(n+k0)
2
−1exp{tr[(n −1)S + n¯y¯y
′ + 1
σ2
0 µ0µ
′
0 −(n + 1
σ2
0 )µyµ
′
y + A0]Σ−1}
2.6.2.2
Prior for Σ based on correlation factorization
Suppose we take the prior on µ to be as speciﬁed in section 2.6.2.1, now another
approach to placing a prior on Σ is to decompose the variance matrix into sev-
eral components in order to model each eﬀectively. This can provide mathematical
simpliﬁcations, but also provide ﬂexibility for various statistical considerations, par-
ticularly with respect to elicitation. In statistical applications, it is best to use a
decomposition where components of the decomposition have direct statistical inter-
pretations. For example, if we are interested in learning about principal variance
components, it is desirable to work with spectral decomposition of a variance matrix.
We ﬁnd that the most natural and intuitive way to model the variance matrix for

CHAPTER 2.
BAYESIAN INFERENCE
59
a factor analysis is to decompose it into scale and correlation components. This is
one of the most direct and simplest way of decomposing a variance matrix, namely,
we write
Σ = D1/2RD1/2,
(2.33)
where D= Diag(δ1, · · · , δp) is the diagonal matrix of standard deviations, and R is
the correlation matrix. Recall that in Chapter 1 we noted that it is preferable to
use model (1.3) and parametrize the correlation matrix as
R = ΓqΓ
′
q + Ψ(Γq).
We will assume that the correlations are independent of the scales of the vari-
ables. This assumption is especially meaningful in a factor analysis context. More-
over, it is more convenient to implement the elicitation procedure if we consider
them seperately. With this assumption, the joint prior distribution of (D, R) is
p(D, R) = p(D)p(R)
(2.34)
There are several common choices that can be made for the prior on D. For
example, we could take the δi to be independent inverse gamma distributions or
place a multivariate normal distribution on log δ1, · · · , log δp. We then need to elicit
priors for the scale quantities as discussed in Section 2.5.
The main task is to specify a sensible prior for the correlation matrix R. There
are some discussions of the prior choices for the correlation matrices in the litera-
ture, e.g., Barnard, Mcculloch and Meng [4], and indeed there has been much work
that seeks a joint distribution for {R : R ∈Rp×p}, where Rp×p is p-dimensional
correlation matrix space. Notice that the p-dimensional correlation matrix space is

CHAPTER 2.
BAYESIAN INFERENCE
60
a compact subspace of a p(p−1)/2 dimensional unit cube. When we have very little
knowledge about the correlation matrix, we will want to use a noninformative prior.
The prior for Σ we discussed in subsection 2.6.2.1 is widely used as a conjugate
prior. In fact we can choose this prior so that each ρij(i ̸= j) is uniformly distributed
over [-1, 1] as proved in the following.
Theorem 2.6.4. Suppose that Σ is distributed as Inverse Wishartp(I, ν) and Σ =
D1/2RD1/2 where D = diag(δ1, · · · , δp) is the scaling matrix and R is the correlation
matrix. The marginal probability density for R is given by
fp(R) ∝|R|
1
2(ν−1)(p−1)−1 Y
i
(|Rii|)−ν
2 ,
(2.35)
where Rii is the ith principal sub-matrix of R.
Proof. Since the Jacobian of the transformation Σ−1 →Σ is |Σ|−(p+1), we have that
Σ has density function
fp(Σ|ν) ∝|Σ|−1
2(ν+p+1)exp[−1
2tr(Σ−1)]
(2.36)
Now making the change of variable Σ →(D, R) with the Jacobian is 2p|D|p/2, the
density in terms of (D, R) is
fp(D, R)
∝|D|
−ν−1
2 |R|−1
2(ν+p+1)exp[−1
2tr(D1/2RD1/2)−1]
∝|D|
−ν−1
2 |R|−1
2(ν+p+1)exp[−1
2
P
i
ρii
2δi]
(2.37)
where ρii =
|Rii|
|R|
with |Rii| being the ith principal sub-matrix.
To obtain the
marginal density for R, we integrate (2.37) with respect to D, namely,
fp(R) ∝|R|−1
2 (ν+p+1) Y
i
Z ∞
0
δ
−ν−1
2
i
exp[−ρii
2δi
]dµ(δi).
(2.38)

CHAPTER 2.
BAYESIAN INFERENCE
61
Let ηi = ρii/2δi and multiply by the Jacobian, (2.38) becomes
fp(R)
∝|R|−1
2(ν+p+1) Q
i(ρii)−ν
2 R ∞
0 η
ν−2
2
i
exp(−ηi)dµ(ηi)
∝|R|−1
2(ν+p+1) Q
i(ρii)−ν
2
= |R|
1
2(ν−1)(p−1)−1 Q
i(|Rii|)−ν
2
Corollary 2.6.1. If Σ is distributed as Inverse Wishartp(I, ν) and Σ = D1/2RD1/2
where D = diag(δ1, · · · , δp) is the scaling matrix and R is the correlation matrix,
then the marginal distribution of an individual correlation is
ρij ∼Uniform[−1, 1]
for i ̸= j when ν = p + 1.
Proof. Any principle sub-matrix of an Inverse Wishart matrix is an Inverse Wishart
distribution. For example, a p1 × p1 sub-matrix of Inverse Wishart matrix is dis-
tributed as Inverse Wishartp1(I1, ν −(p −p1)) where I1 is a p1 × p1 identity matrix.
Using this, we can derive the marginal distribution for ρij(i ̸= j). In particular if
we take p1 = 2, then we replace ν by ν1 = ν −(p −p1) and replace p by p1 = 2
in (2.35). The determinant of the 1st principal sub-matrix of any 2 × 2 correlation
matrix is 1, and therefore (2.35) becomes
f(ρij) = (1 −ρ2
ij)
ν−p−1
2
−1 ≤ρij ≤1.
(2.39)
Notice that ρij is an extended Beta distribution on [−1, 1] with parameters ν−p+1
2
and ν−p+1
2
. By taking ν = p + 1, then ρij is uniformly distributed on [−1, 1].

CHAPTER 2.
BAYESIAN INFERENCE
62
Given that the correlation matrix space Rp×p is a compact subspace of the p(p−
1)/2 dimensional cube [−1, 1]p(p−1)/2, we can put a jointly uniform prior on Rp×p,
e.g.,
p(R) ∝1
R ∈Rp×p.
(2.40)
One restriction is that correlation matrix has to be positive deﬁnite, i.e., |Rii| > 0 for
i = 1, · · · , p. Due to this restriction, we note that the resulting marginal distribution
of individual correlations ρij is not uniform for p > 2.
Consider the following example.
Example 2.6.3
Suppose p = 3 then we determine the marginal probability density for one of the
p(p −1)/2 = 3 individual correlations of R when R is distributed as in (2.40) as
p(ρ12) ∝
q
1 −ρ2
12,
where ρ12 ∈[−1, 1]. Figure 2.2 shows the marginal distribution of ρ12 when p = 3.
−1.0
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
rho_12
density
Figure 2.2: Plot of density of ρ12

CHAPTER 2.
BAYESIAN INFERENCE
63
In fact, as p increases, the marginal prior on the individual correlations under
the jointly uniform prior (2.40) tightens up around zero as evident in Figure 2.3.
−1.0
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
1.2
rho_12
density
p=3
p=6
p=10
Figure 2.3: Plot of density of ρ12 when p = 3, 6, 10
We are in favor of a jointly uniform distribution because the individual correla-
tions of a correlation matrix shrift away from ±1 as p increases and it seems more
appropriate when we lack information about the dependencies among the variables.
Further, one virtue of using a jointly uniform prior is that it greatly simpliﬁes the
prior elicitation process in factor analysis, namely, by choosing a jointly uniform
prior for correlation matrices, our task is reduced to elicit location and scale param-
eters. A variety of methods of eliciting location and scale parameters are documented
in Section 2.5. We will also present an algorithm to handle integration with respect
to the posterior in Chapter 4.
Fortunately, we can eﬃciently sample from the uniform distribution (2.40). One
method is called the onion’s method due to Ghosh and Henderson [20]. For this let

CHAPTER 2.
BAYESIAN INFERENCE
64
p(Rk) be the marginal density of Rk, where Rk is the k × k dimensional principal
leading minor of R and let q = (qk−1,1, · · · , qk−1,k−1)t be the completion of Rk in
Rk as in
Rk =


Rk−1
q
q
′
1

.
The onion method starts with one-dimensional matrix, namely, R1 and then
grows out to the desirable dimension by successively adding an extra row and an
extra column (the completion vector a.k.a. the kth column and kth row) generated
from some distribution. To be more precise, the iterative process can be displayed
in the following steps.
R1 = 1,
R2 =


1
q11
q11
1

,
R3 =


1
q11
q21
q11
1
q22
q21
q22
1


,
etc.
where the (qk−1,1, · · · , qk−1,k−1) are generated from some distribution.
Since the
algorithm grows out by adding the completion vectors, our task is to sample q from
some appropriate distribution.
First, let’s consider the following proposition regarding the marginal distribution
of Rk, then derive a distribution for q.

CHAPTER 2.
BAYESIAN INFERENCE
65
Proposition 2.6.1. Let p(Rk) be the marginal density of Rk, where Rk is the k ×k
dimensional principal leading minor of R. If R is distributed as in (2.40), then
p(Rk) ∝|Rk|
p−k
2
Rk ∈Rp
2 ≤k ≤p.
(2.41)
Proof. Ghosh and Henderson (2003).
Notice that p(Rk) is the joint density of Rk−1 and its corresponding completion
vector q, namely,
p(Rk)
= p(Rk−1, q)
= p(Rk−1)p(q|Rk−1).
In the sampling algorithm, we generate the completion vector q for a given Rk−1
from the conditional probability density
ωk(q) = p(q|Rk−1) ∝

Rk−1
q
q
′
1

p−k
2
Thus,
ωk(q)
= |Rk−1|
p−k
2 · (1 −q
′R−1
k−1q)
p−k
2
∝(1 −q
′R−1
k−1q)
p−k
2 ,
(2.42)
where q ∈Rp,
q
′R−1
k−1q ≤1.
Now we need to generate from (2.42).
First, we need to make a change of
variables. Putting ζ = R
−1
2
k−1q, we have that
ωk(ζ) ∝(1 −ζ
′ζ)
p−k
2
ζ ∈Bk−1
(2.43)
where Bk−1 in the unit ball in dimension k −1 as q
′R−1
k−1q ≤1. So we can sample

CHAPTER 2.
BAYESIAN INFERENCE
66
ζ and make a linear transformation to q.
To generate ζ from (2.43), we essentially generate uniformly over the surface of
Bk−1 and then sample the radius r appropriately.
By making the transformation ζ →(r, ϑ), so that ϑ = ζ/||ζ|| and r = ||ζ||, we
obtain
ωk(ζ)dζ ∝(1 −r2)
p−k
2 rk−2dr
(2.44)
By a further change of variable r →x where x = r2, we obtain the standard
univariate beta distribution density
x
k−1
2 −1(1 −x)
p−k
2 −1.
(2.45)
It is clear that we can generate the radius from Beta (α1, α2) distribution with
α1 = (k −1)/2 and α2 = (p −k)/2. To uniformly generate ϑ on a surface of a
hyperball Bk−1, we generate z ∼Nk−1(0, I) and set ϑ = z/||z||.
We summarize the steps for generating q as follows:
1) sample ϑ uniformly on the surface of Bk−1,
2) sample x from Beta ((k −1)/2, (p −k)/2) and set r = √x,
3) ζ = ϑ · r and q = R
1
2
k−1ζ.
It will be noted that we have not yet discussed sampling from the posterior for
Σ in this section. As it turns out, the method of this thesis requires that we be
able to sample from the prior, so these developments are by no means wasted. Still
these methods do not help us in generating from the posterior or even in developing
a Gibbs sampler. We develop an alternative approach in Chapter 4.

Chapter 3
Methodology for Factor Analysis
3.1
Measure of concentration
What is concentration? Let’s ﬁrst start with a simple example.
Example 3.1.2
A coin with unknown probability θ ∈R1 was tossed n times, X1, · · · , Xn ∈{0, 1}.
Then the sampling distribution of ¯X becomes more and more concentrated around
θ as n increases.
Suppose we have a probability measure P on X and we want to know to what
extent P concentrates about a certain set C ⊂X. We cannot use P(C) because,
for two sets in X with same probability content, the remaining probability can be
more widely spread out for one than the other. We need a method for measuring to
what extent the probability mass, outside of C is concentrated around C.
We choose to measure the concentration of P around C via the distribution of
d(x, C) = inf{d(x, y) : y ∈C}.
(3.1)
67

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
68
for some distance measure d on X . In particular, by choosing C = {Ep(X)} and
d(x, y) = ||x −y||2, where || · || is Euclidean distance on a subset X of a Euclidean
space, the expectation of d(x, C) is the variance, so we can view our measure of
concentration as a generalization of the concept of variance. Note that Ep[d(x, C)] =
Ep[d(x, C)ICc(x)] so that this measure really only depends on how the probability
is distributed on Cc relative to C.
While the expectation of d(x, C) seems like
a relevant quantity, more generally we want to measure the extent to which the
distribution of d(x, C) concentrates around 0.
Now suppose that we want to assess the hypothesis that the true value of θ ∈
H0 ∈Θ, after observing the data X. Perhaps the most natural method of assessing
this hypothesis is to compute the posterior probability Π(H0|X) and regard this as
evidence in favor of H0 when it is large. A diﬃculty with this approach is that, when
Π(H0) is small, then a large amount of data may be needed to make Π(H0|X) large
enough to be convincing. In fact, if Π assigns 0 probability to H0, simply because it
is a lower dimensional set of Θ, then Π(H0|X) = 0 no matter what data is obtained.
If we are making inference about a marginal parameter Ψ deﬁned on Θ, then
H0 = Ψ−1{ψ0} for some speciﬁed value ψ0, and various approach can be taken to
deal with the problem caused by Π(H0), e.g., see (2.10). When no Ψ naturally
exists, as in the factor analysis model problem we are discussing, then we have to
choose one and taking Ψ(θ) = d(θ, H0) seems like a reasonable choice. Clearly, the
degree to which Π (or Π(·|X)) concentrates about H0, is a measure of our belief
in H0 a priori (or a posteriori) and the prior (or posterior) concentration of the
distribution of d(θ, H0) about 0 is measuring this.
Accordingly, rather than be forced to modify a prior to ensure that it assigns
positive mass on H0, which indeed can create other diﬃculties, we look at comparing
the concentrations of the prior and the posterior probability measures about H0. If

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
69
the null hypothesis is true, i.e., the true value θ∗∈H0, then we expect the data will
lead to a greater concentration of the posterior distribution about H0 than the prior
distribution. In fact, under weak conditions, the posterior distribution of d(θ, H0)
will converge to d(θ∗, H0) = 0 when θ∗∈H0, and converge to a nonzero value
otherwise. So no matter how we choose d, for large amounts of data we can expect
the posterior distribution of d to indicate whether or not H0 holds.
To calibrate to what degree the posterior distribution of d(θ, H0) is concentrating
about 0, we compare its posterior distribution to its prior distribution. This makes
it necessary to decide how we will compare the prior and posterior distribution of
d. For this we look at the ratio of the posterior density to the prior density of d at
0, namely, πd(0|X)/πd(0), which measures the change in belief in H0 from a priori
to a posteriori. To calibrate this we compute the observed relative surprise (ORS),
namely,
Π
πd(d(θ, H0)|X)
πd(d(θ, H0))
> πd(0|X)
πd(0) |X

(3.2)
which is the posterior probability of an increase in belief, from a priori to a pos-
teriori, in the distance of the true value of θ from H0, that is greater than that
obtained when hypothesis is true. So when (3.2) is near 1 the increase in belief is
much greater for values of θ /∈H0 and we interpret this as evidence that H0 is false.
As discussed in Section 2.4, the value (3.2) was developed in Evans [6] as a ba-
sis for Bayesian inferences that would satisfy invariance under reparameterizations.
For example, if we were to change d to e(d), where e is any 1 −1, smooth trans-
formation, then it is clear that (3.2) does not change its value. Further, we have
discussed various optimality properties for relative surprise inferences in the class of
all Bayesian inferences in Section 2.4. Note that (3.2) also leads to credible regions
and estimates.
Comparing the concentration of the prior and posterior measures about a hy-

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
70
pothesis of interest, as a method for assessing this hypothesis, has been previously
discussed in the literature. For example, Evans, Gilula and Guttman [8] and Evans,
Gilula, Guttman and Swartz [9] used this idea in the context of contingency tables.
In those papers, however, we simply compared the concentration via graphing the
prior and posterior densities of d. In this thesis we use a more precise comparison
via (3.20).
3.2
Concentration for the Factor Analysis Model
Now let us move on to factor analysis model problems.
In Chapter 1, we have
mentioned that the essential problem in the factor analysis model is to determine
the minimum number of q such that (1.2) holds. So let
Hq
0 =
n
ΓqΓ
′
q + Ψ : (Γq, Ψ) ∈LT +(p, q) × Diag+(p)
o
,
i.e., Hq
0 denotes the subset of the set of variance matrices where q factors produce the
variance matrix. Note that, LT +(p, q) × Diag+(p) has an nonempty interior when
q > 0. For any point (Γq, Ψ) in this interior, then ΓqΓ
′
q + Ψ is positive deﬁnite,
and the map from this set to the space of positive deﬁnite matrices is continuously
diﬀerentiable there. Also, the concentration of the prior and posterior around Hq
0
is measured by the concentration of the prior and posterior distribution of Σ about
Hq
0, i.e., it does not involve the prior and posterior of µ.
For the basic distance measure d, we choose to use d(Σ(1), Σ(2)) = ||Σ(1)−Σ(2)||2
F,
where ||·||F is the Frobenius norm deﬁned on the space of positive semideﬁnite p×p
matrices by
||Σ(1) −Σ(2)||2
F =
p
X
i=1

σ(1)
ii −σ(2)
ii
2
+ 2
X
i<j

σ(1)
ij −σ(2)
ij
2
.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
71
For a given Σ, generated from the prior and the posterior, we want to compute its
Frobenius distance from Hq
0. so we need to be able to solve the following optimization
problem. For a given positive deﬁnite matrix Σ we need to ﬁnd the minimum, as a
function of (Γq, Ψ), of
||Σ −(ΓqΓ
′
q + Ψ)||2
F
=
Pp
i=1

σii −Pmin(i,q)
k=1
γ2
ik −ψii
2
+ 2 P
i<j

σij −Pmin(i,q)
k=1
γikγjk
2
.
(3.3)
Note that the ﬁrst sum in (3.3) is concerned with ﬁnding a (Γq, Ψ) that approximates
the individual variances of the observed variables, while the second sum is concerned
with ﬁnding a Γq that approximates the covariances among the observed variables.
In a factor analysis, it is the latter term that is of the key interest.
Conceivably, it is possible ﬁnd an optimal (Γq, Ψ) ∈LT +(p, q) × Diag+(p) such
that Pmin(i,q)
k=1
γ2
ik + ψii > σii for some i. But this says that our approximating factor
model exhibits greater variability for a manifest (observed) variable than that is
speciﬁed by Σ and this seems somewhat unnatural. Accordingly, let
Hq
0(Σ) =





ΓqΓ
′
q + Ψ : (Γq, Ψ) ∈LT +(p, q) × Diag+(p)
Pmin(i,q)
k=1
γ2
ik + ψii ≤σii





⊂Hq
0,
and note that Hq
0(Σ) depends on Σ only through the variances not correlations.The
set of (Γq, Ψ) parameterizing Hq
0(Σ) also has a nonempty interior whenever Σ is
positive deﬁnite. We then modify our distace measure to use
d(Σ, Hq
0) = inf

||Σ −(ΓqΓ′
q + Ψ)||2 : ΓqΓ′
q + Ψ ∈Hq
0(Σ)
	
,
(3.4)
i.e., we do not allow the diagonal terms of ΓqΓ′
q + Ψ to be greater than the corre-
sponding diagonal terms of Σ. This restriction seems very natural and, as we explain

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
72
below, will typically give the same result as if we minimized Frobenius distance over
Hq
0. An additional beneﬁt is that this restriction simpliﬁes the optimization con-
siderably. In fact, because LT +(p, q) × Diag+(p) is unbounded, it is not clear that
minimizing (3.3) over this set will always lead to a solution. Note that the set of
(Γq, Ψ) parameterizing Hq
0(Σ) is compact and so there always exists a point (Γ∗
q, Ψ∗)
in this set such that d(Σ, Hq
0) = ||Σ −(Γ∗
qΓ∗′
q + Ψ∗)||2
F.
Also, the prior and posterior distributions of the variances of the manifest vari-
ables, obtained from the minimizing values in Hq
0(Σ) will clearly be the same as
those obtained from the prior and posterior distributions placed on Σ. Of course
when q < p the distributions of the correlations will be diﬀerent.
3.3
Minimizing Algorithms
3.3.1
Constrained Univariate Quadratic Minimization
Now we need an eﬃcient algorithm to compute (3.4).
Consider ﬁrst the simple
situation where q = 0, as then Γ0 = 0 and ψii = σii gives the optimal solution. The
minimized distance measure is given by d(Σ, Hq
0) = 2 P
i<j σ2
ij. Further, we get the
same result when we minimize over Hq
0.
Suppose that the point minimizing (3.3) is in the interior of Hq
0. Then all the
partial derivatives must be equal to 0 at such a point and in particular this must
hold for the partial derivatives with respect to ψii. This implies that ψii = σii −
Pmin(i,q)
k=1
γ2
ik < σii. From this we get the following result.
Proposition 3.3.1. When the minimum of (3.3) occurs at an interior point of
LT +(p, q) × Diag+(p), then this point is also in the set of (Γq, Ψ) parameterizing
Hq
0(Σ) and so (3.3) at this value equals to d(Σ, Hq
0).

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
73
Proposition 3.3.1 suggests that minimizing over Hq
0 or Hq
0(Σ) will lead to the
same answer for many Σ. It does not seem possible to characterize those Σ for which
the minimum of (3.3) is not an interior point, but computing experience suggests
that these are extreme cases. In any case, as we have already argued, we feel that it
is more appropriate to minimize over Hq
0(Σ) to obtain our measure of concentration.
Now when (Γq, Ψ) is in the set parameterizing Hq
0(Σ), we have Pmin(i,q)
k=1
γ2
ik ≤σii
and so we can always make the ﬁrst sum of (3.3) equal to 0 by setting ψii =
σii −Pmin(i,q)
k=1
γ2
ik for every i. Therefore, since the second sum in (3.3) does not
involve ψii, we need only ﬁnd Γq ∈LT +(p, q), satisfying Pmin(i,q)
k=1
γ2
ik ≤σii for
each i, that minimizes this sum to compute d(Σ, Hq
0). As this has an important
interpretation we explicitly state this result.
Proposition 3.3.2. A value (Γ∗
q, Ψ∗) in the set of (Γq, Ψ) parameterizing Hq
0(Σ)
and such that d(Σ, Hq
0) = ||Σ −(Γ∗
qΓ∗′
q + Ψ∗)||2
F satisﬁes ψii = σii −Pmin(i,q)
k=1
γ2
ik for
each i.
The interpretation of this result is that the best approximation to Σ in the set
Hq
0(Σ), gives the same variance to each of the manifest variable as Σ.
We consider now the computation of d(Σ, H1
0). To compute d(Σ, H1
0) we need
to ﬁnd (γi1, · · · , γp1)′ with γ11 ≥0 and γ2
i1 ≤σii for each i, that minimizes
X
i<j
(σij −γi1γj1)2
(3.5)
Note that (γ11, · · · , γp1) lies in the box [0, σ1/2
11 ] × [−σ1/2
22 , σ1/2
22 ] × · · · × [−σ1/2
pp , σ1/2
pp ].
Also (3.5) is invariant under the transformation that multiplies (γ11, · · · , γp1)′ by
−1 and so it is equivalent to minimize (3.5) over
B1(Σ) = [−σ1/2
11 , σ1/2
11 ] × [−σ1/2
22 , σ1/2
22 ] × · · · × [−σ1/2
pp , σ1/2
pp ],

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
74
and we will see (Section 3.4.1) that minimizing over B1(Σ) improves things slightly
as the minimization algorithm that we use tends to converge faster in this case.
Now suppose that we proceed iteratively. First we start with (γ11, · · · , γp1) ∈
B1(Σ). The value of (3.5) at this point equals
P
i<j (σij −γi1(0)γj1(0))2
=
(Pp
j=2 γ2
j1(0))γ2
11(0) −2(Pp
j=2 σ1jγj1(0))γ11(0) + c
(3.6)
where c is a constant not depending on γ11(0). Now treat γ11(0) as a variable. If
Pp
j=2 γ2
j1(0) ̸= 0, then the quadratic in γ11(0) is minimized by γ11(0) equal to
p
X
j=2
σ1jγj1(0)/
p
X
j=2
γ2
j1(0)
(3.7)
If (3.7) is not in [−σ1/2
11 , σ1/2
11 ], then the quadratic is minimized over this interval by
setting γ11(0) = −σ1/2
11 , when (3.7) is less than −σ1/2
11 , or setting γ1/2
11
when (3.7)
is greater than σ1/2
11 . If Pp
j=2 γ2
j1(0) = 0, then Pp
j=2 σ1jγj1(0) = 0, and there is no
dependence on γ11(0) so we set γ11(1) equal to the value of γ11(0) that minimizes
(3.6). In any case we replace γ11(0) by the value γ11(1) that minimizes the quadratic
over [−σ1/2
11 , σ1/2
11 ] or we don’t change its value. After updating γ11 we next proceed
to replace γ21(0) by the value γ21(1) that minimizes the quadratic in γ21(0) over
[−σ1/2
22 , σ1/2
22 ] based on the same argument. We continue cycling through the variables
in this way.
We see immediately that at each step of the iteration, starting from some ini-
tial (γ11(0), · · · , γp1(0)) ∈B1(Σ), the value of (3.5) never increases. Since (3.5) is
bounded below by 0, this implies that the above iterative procedure converges to a
minimum value. The convergence is typically very fast.
This minimum value, as we will see, depends on the starting value and so we

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
75
are not guaranteed to obtain the absolute minimum from a given starting value.
Accordingly, we proceed as follows. We select m i.i.d. starting points from the
uniform distribution on B1(Σ) and compute the m minima d1, · · · , dm via the itera-
tive procedure applied to each starting value. We then estimate d(Σ, H1
0) by d(1):m,
i.e., the smallest order statistic. Computational experience indicates that there are
typically a very small number of minima for a given Σ. In fact, often there is only
1. So this represents an eﬃcient method for computing d(Σ, H1
0).
The case when q > 1 proceeds like the q = 1 case. For this we need to ﬁnd
Γq ∈LT +(p, q) that minimizes
X
i<j

σij −
min(i,q)
X
k=1
γikγjk


2
.
(3.8)
We note that this is a quadratic in each of the elements of Γq and so we can apply
the iterative procedure to each element of Γq. The i-th row of Γq is now constrained
to lie in a ball, centered at the appropriate origin, of squared radius σii, i.e., we
must minimize (3.8) over the compact set


Γq ∈LT +(p, q) :
min(i,q)
X
k=1
γ2
ik ≤σii and γii ≥0 for i = 1, · · · , p



which is the Cartesian product of half-balls and balls. In this case we see that (3.8)
is invariant under transformations of Γq that multiply a column by −1 and so it is
equivalent to minimize (3.8) over the compact set
Bq(Σ) =


Γq ∈LT +(p, q) :
min(i,q)
X
k=1
γ2
ik ≤σii, i = 1, · · · , p



which is a Cartesian product of balls. Referring to the iteration procedure described

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
76
previously as constrained univariate quadratic iteration, we prove the following re-
sult.
Theorem 3.3.1. Constrained univariate quadratic iteration starting from Γq(0) ∈
Bq(Σ), always gives a nonincreasing sequence of values of (3.8) and as such con-
verges. Moreover, the sequence Γq(k)Γ
′
q(k) + Ψ(k) converges to a matrix in the
closure of Hq
0(Σ) as k →∞.
Proof. We note ﬁrst that if ax2 + bx + c is such that a > 0, then the minimum of
the quadratic occurs at −b/2a and if this value is not in an interval [l, u] then the
minimum of the quadratic over the interval occurs at l or u.
Note that when r < s, then γrs = 0 and so we need only consider the case r ≥s.
We see that γrs occurs in terms of (3.8) only when i = r or j = r, so we can write
(3.8) as
p
X
r+1

σrj −
min(r,q)
X
k=1
γrkγjk


2
+
r−1
X
i=1

σir −
min(i,q)
X
k=1
γikγrk


2
+ c
(3.9)
where c is some constant not involving γrs. Now when s = r ≤q, then the second
term in (3.9) does not involve γrr and so we need only consider the ﬁrst term. we
see immediately that, as a function of γrr, the ﬁrst term can be written as
 
p
X
j=r+1
γ2
jr
!
γ2
rr −2
" X
j=r+1
 
σrj −
r−1
X
k=1
γrkγjk
!
γjr
#
γrr + c
(3.10)
where c is a constant. Note that if Pp
j=r+1 γ2
jr = 0, then the coeﬃcient of γrr in
(3.10) is also 0. When s < r, then note that s ≤q and (3.10) can be written as
Pp
j=r+1 γ2
js + Pr−1
i=s γ2
is

γ2
rs −2[P
j=r+1

σrj −Pmin(r,q)
k=1,k̸=s γrkγjk

γjs+
Pr−1
i=s

σir −Pmin(r,q)
k=1,k≤s γikγrk

γik]γrs + c
(3.11)

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
77
where c is a constant. Again if the coeﬃcient of γ2
rs in (3.11) is 0, then the coeﬃcient
of γrs is 0. Now for 1 ≤r ≤q, then
γrr ∈

−
 
σrr −
r−1
X
k=1
γ2
rk
!1/2
,
 
σrr −
r−1
X
k=1
γ2
rk
!1/2

(3.12)
and
γrr ∈

−

σrr −
min(r,q)
X
k=1,k̸=s
γ2
rk


1/2
,

σrr −
min(r,q)
X
k=1,k̸=s
γ2
rk


1/2

(3.13)
otherwise.
Now just as in the q = 1 case we can select a starting Γq(0) and then iterate
using (3.10) and (3.11), to minimize each quadratic over the relevant interval as
determined by (3.12) and (3.13).
So we might start with γ11(0) replacing it by
γ11(1), and then, with this new value for γ11, replace γ21(0) by γ21(1), etc. At each
step the distance (3.8) does not increase and so the sequence of distances converges
to a minimum. This proves the ﬁrst statement in Theorem 3.3.1.
Now let (Γ∗
q, Ψ∗) be such that d(Σ, Hq
0) = ||Σ −(Γ∗
qΓ∗′
q + Ψ∗)||2
F and consider
the sequence Γq(k)Γq(k) + Ψ(k). We have that
||[Γq(k1)Γ
′
q(k1) + Ψ(k1)] −[Γq(k2)Γ
′
q(k2) + Ψ(k2)]||F
≤
||[Γq(k1)Γ
′
q(k1) + Ψ(k1)] −[Γ∗
qΓ∗′
q + Ψ∗]||F+
||[Γ∗
qΓ∗′
q + Ψ∗] −[Γq(k2)Γ
′
q(k2) + Ψ(k2)]||F
and since ||[Γq(ki)Γ
′
q(ki) + Ψ(ki)] −[Γ∗
qΓ∗′
q + Ψ∗]||F →0 as ki →∞we see that the
sequence Γq(ki)Γ
′
q(ki)+Ψ(ki) is Cauchy and so converges to a matrix in the closure
of Hq
0(Σ). This establishes Theorem 3.3.1.
The convergence of Γq(k)Γ
′
q(k) + Ψ(k) is not necessary for the assessment of Hq
0

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
78
but has some utility that we will subsequently discuss.
The limiting distance obtained in Proposition 3.3.3 depends, in general, on the
starting value Γq(0). Accordingly we generate m independent starting values from
the uniform distribution on Bq(Σ), then obtain the distance d1, · · · , dm via con-
strained univariate quadratic iteration applied to each starting value, and estimate
d(Σ, Hq
0) by d(1):m.
To generate Γq(0) uniformly in Bq(Σ) we proceed as follows.
First we note
that we can generate ν uniformly in the ball in Rk of radius r by generating z ∼
Nk(0, I) independent of s ∼Beta(k, 1) and the putting ν = rsz/||z||. Then, for
i = 1, · · · , q generate (γi1(0), · · · , γii(0))
′ uniformly in the ball of radius σ1/2
ii
and for
i = q + 1, · · · , p generate (γi1(0), · · · , γiq(0))
′ uniformly in the ball of radius σ1/2
ii .
In many situations we would prefer to work with the correlation matrix R rather
than the variance matrix Σ. The only change in our discussion is that now we
compare the concentration of the prior and posterior distribution of R about
Hq
0(R) =





ΓqΓ
′
q + Ψ : (Γq, Ψ) ∈LT +(p, q) × Diag+(p)
Pmin(i,q)
k=1
γ2
ik + ψii ≤1 for each i





⊂Hq
0,
We do this by comparing the prior and posterior distribution of d(R, Hq
0). Note that
the algorithm for computing the minimum distance changes only by setting σii = 1
for each i. We will focus hereafter on carrying out a factor analysis based on the
correlation matrix.
3.3.2
Constrained Row-wise Quadratic Minimization
The algorithm of Section 3.3.1 proceeded iteratively by varying one variable at a
time. But there is no reason to minimize with respect to one variable at it is also
possible to minimize simultaneously with respect to all variables in a row. To see this

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
79
note that we want to minimize (3.8) subject to the constraints Pmin(i,q)
k=1
γ2
ik ≤σii.
We can write (3.8) as
X
i<j

σij −γ
′
(i)γ(j)
2
=
X
i<j

σ2
ij −2σijγ
′
(i)γ(i)
(j) + γ
′
(i)γ(i)
(j)γ(i)′
(j) γ(i)

where γ(i) = (γi1, · · · , γi,min(i,q))′ and γ(i)
(j) = (γi1, · · · , γj,min(i,q))′. Now ﬁxing i we see
that we get a quadratic form in γ(i), namely,
c −2
 X
j̸=i
σijγ(i)
(j)
!′
γ(i) + γ
′
(i)
 X
j̸=i
γ(i)
(j)γ(i)′
(j)
!
γ(i),
(3.14)
where c is a constant.
For the quadratic form x′Ax + b′x to have a unique minimum we must have
that A is positive deﬁnite and then the minimum occurs when x is the solution to
2Ax+b = 0 or x = −1
2A−1b. In the case of (3.14), we have that A = P
j̸=i γ(i)
(j)γ(i)′
(j)
and note that x′Ax = P
j̸=i

x′γ(i)
(j)
2
and we see that this is zero if and only if
x ∈L⊥n
γ1, · · · , γ(i)
i−1, γ(i)
i+1, · · · , γ(i)
p
o
.
Since L⊥n
γ1, · · · , γ(i)
i−1, γ(i)
i+1, · · · , γ(i)
p
o
is a
lower-dimensional subset of Ri, we have that A is positive deﬁnite with probability
1 for each randomly generated Γ. Accordingly the minimizing γ(i) for (3.14) is given
by
γ(i) =
 X
j̸=i
γ(i)
(j)γ(i)′
(j)
!−1  X
j̸=i
σijγ(i)
(j)
!
(3.15)
It may be, however, that (3.15) does not satisfy the constraint γ
′
(i)γ(i) ≤σii. In
such a case we must have that the minimum of (3.14) occurs at a point γ(i) where
γ
′
(i)γ(i) = σii.
So we must solve the Lagrange multiplier problem of minimizing

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
80
x′Ax + b′x + c + λ(x′x −σii) subject to x′x = σii. The partial derivatives satisfy
∂
∂x = 2(A + λI)x + b
∂
∂λ = x′x −σii
and when set equal to 0 leads to the system of equations
x = −1
2(A + λI)−1b
λ = −1
σii(x′Ax + x′b).
(3.16)
Note that (3.16) does not lead to a closed form expression for x. We could, however,
proceed iteratively to solve (3.16) by proposing a starting x satisfying ||x||2 = σii,
solving for λ, etc.
We only need to use the iterative process to solve (3.16) when (3.15) doesn’t
satisfy the constraints. A starting value x can be obtained as follows. Let x∗∗be
equal to (3.15) and let xk be the current value of γ(i). Then put x∗= αxk+(1−α)x∗∗
where α ∈[0, 1] is chosen so that σii = ||x∗||2 = α2||xk||2 + 2α(1 −α)x
′
kx∗∗+ (1 −
α)2||x∗∗||2, i.e., α is a root of
α2(||xk||2 −2x
′
kx∗∗+ ||x∗∗||2) + 2(xkx∗∗−||x∗∗||2)α + (||x∗∗−σii)
(3.17)
The quadratic given by (3.17) has roots
 ||x∗∗||2 −x
′
kx∗∗

±
p
(x
′
∗∗(xk −x∗∗))2 −||xk −x∗∗||2(||x∗∗||2 −σii)
||xk −x∗∗||2
and we choose the root in [0, 1]. Note that one of these roots must be in [0, 1] since
||xk||2 ≤σii while ||x∗∗|| > σii.
Actually given that the quadratic form (3.14) is convex we must have that the

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
81
quadratic form evaluated at x∗will be less than or equal to this function evaluated
at xk. So we can either put xk+1 equal to the result of iteration (3.16) or xk+1 = x∗
and we have the following result with the same proof as Theorem 3.3.1.
Theorem 3.3.2. Constrained row-wise quadratic iteration starting from Γq(0) ∈
Bq(Σ), always gives a nonincreasing sequence of values of (3.14) and as such con-
verges. Moreover, the sequence Γq(k)Γ
′
q(k) + Ψ(k) converges to a matrix in the
closure of Hq
0(Σ) as k →∞.
Note that with this approach we need to invert a matrix. We will consider an
example in Section 3.4.4.
3.4
Examples of Computing the Distance
We now consider some examples of using constrained univariate quadratic iteration
to compute the distance. First we consider an example where a closed form solution
can be obtained.
3.4.1
Example where R ∈R2×2
In this example we show that the algorithm converges in one step. Suppose that
p = 2 and q = 1 and we want to compute d(R, H1
0).
Then we must minimize
(ρ12 −γ11γ21)2 over γ11, γ21 where −1 ≤γ11 ≤1 and −1 ≤γ21 ≤1. Since [(p −
q)2 −(p + q)] < 0, this suggests there are multiple points that make this distance
measure exactly 0. Indeed, any point on the hyperbola γ11γ21 = ρ12 and satisfying
the constraints, gives the minimum distance.
Suppose we start our iteration procedure at (γ11(0), γ21(0)) ∈[−1, 1]2 such that
neither coordinates is 0 as this occurs with probability 1. If γ21(0) > ρ12 > 0, then

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
82
applying the iteration procedure we obtain
γ11(1) = ρ12γ21(0)
γ2
21(0)
=
ρ12
γ21(0), γ21(1) = ρ12γ11(1)
γ2
11(1)
= γ21(0),
(3.18)
and so γ11(1)γ21(1) = ρ21 and the distance is 0. If ρ12 > γ21(0) > 0, then
γ11(1) = 1, γ21(1) = ρ21
and so γ11(1)γ21(1) = ρ21 and the distance is 0. If 0 > −ρ12 > γ21(0), then (3.18)
holds. If 0 > γ21(0) > −ρ12, then
γ11(1) = −1, γ21(1) = −ρ21
and again γ11(1)γ21(1) = ρ21 and the distance is 0. Similarly, we can work out the
iteration procedure when ρ12 ≤0 and we get convergence to the minimum distance
after one iteration step. Notice that the value γ11(k) = γ11(1), γ21(k) = γ21(1) for
all k ≥1, so these are ﬁxed points. Also, there are many points that lead to the
minimum distance, namely, all those lying on the hyperbola γ11γ21 = ρ21 and in the
box [−1, 1]2 and which one the process converges to, depends on the starting value.
While it would seem to make more sense to minimize the quadratic in γ11(0)
over [0, 1], an analysis of that algorithm leads to two limit points for the distance,
so we prefer to minimize over [−1, 1]. The symmetrized algorithm seems to have
better convergence properties and is easier to implement and analyze.
In this example, we could work out the behavior of the minimization algorithm
directly.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
83
3.4.2
The Independent Case (R = I)
For a more general example, suppose that R = I ∈Rp and Γ1(0) ∈B1(R). Then
since all the ρij = 0 when i ̸= j we have immediately from (3.7) that Γ1(1) =
(0, · · · , 0)′ and so Ψ = I and again we have convergence after one iteration.
For most examples, however, we cannot carry out the computation by hand and
require software. In the next few subsections, we consider some situations where we
know the minimum distance but use software to carry out the iterative algorithm
to compute the minimum distance to verify that the algorithm gives the correct
answer.
3.4.3
One Factor
Now let’s consider a correlation matrix of the form R = Γ1Γ
′
1+Ψ, i.e., the correlation
matrix arising from a 1-factor model. The minimization algorithm should converge
to the actual distance 0.
Suppose we choose p = 6, Γ1 = (1, 1, 1, 1, 1, 1)′ and
Ψ = I. We ﬁrst uniformly generated 103 starting values Γ(0)
1
uniformly in B1(R)
and applied the minimization algorithm to each case. For stopping rule, we iterated
until the squared distance between two successive Γ(i)
1 was less than 10−5(precision).
In Figure 3.1 we give a density histogram of the 103 minimum distances obtained.
We see that these are all eﬀectively equal to 0 since the largest distance obtained
was 5.780201×10−6. In Figure 3.2, we provide a frequency histogram of the number
of iterative steps required for convergence and the mean number of iteration is 5.7
in each case. we see that the maximum number required is 7 and so the convergence
is very fast.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
84
distance
Density
0e+00
1e−06
2e−06
3e−06
4e−06
5e−06
6e−06
0e+00
2e+05
4e+05
6e+05
8e+05
1e+06
Figure 3.1: Density histogram of 103 distances of Example 3.4.3
number of iterations
Frequency
3
4
5
6
7
8
0
100
200
300
400
500
600
700
Figure 3.2: Histogram of 103 number of iterations of Example 3.4.3

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
85
In general, we do not have a proof that d(1):m converges in probability to the
absolute minimum. This will be the case, however, whenever there is an open subset
U ∈Bq(R) such that Γ(0)
q
∈U implies that the distance computed from this starting
value is equal to the minimum distance. In fact, extensive experience indicates that,
in the cases where more than one distance arises from diﬀerent starting values, then
there are ﬁnitely many distinct distances that arise via the iterative procedure. As a
diagnostic, however we can verify in certain cases, that such points are local minima
by checking that the gradient is close is 0 and the smallest eigenvalue of the Hessian
matrix at such point is greater than 0. In general, we rely on the multiple starting
values to ensure that the global minimum is attained.
3.4.4
Two Factor
We consider now a case where the correlation matrix possesses the two factor struc-
ture, namely, R = Γ2Γ
′
2 + Ψ. The correct distance in this case is again 0. We take
p = 8,
Γ2 =


0.8
0
0.8
0.8
0
0
0.8
0.6
0
0.8
0
0
0.8
0.8
0
0.6


′
,

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
86
Ψ =


0.36
0
0
0
0
0
0
0
0
0.36
0
0
0
0
0
0
0
0
0.36
0
0
0
0
0
0
0
0
0.36
0
0
0
0
0
0
0
0
0.36
0
0
0
0
0
0
0
0
0.36
0
0
0
0
0
0
0
0
0.36
0
0
0
0
0
0
0
0
0.28


.
3.4.4.1
Univariate Minimization Approach
We used 103 starting values generated uniformly from B1(R) and stopped the it-
eration until the precision 10−5 is reached. For a two factor model, the iterative
process takes longer with a mean number of 20 iterations but still it converges re-
ally fast. Almost all 103 iterations converge to the minimum distance 0 with one
exception, that maximum distance equal to 1.459. While the minimum distance
d(1):m = 8.678564 × 10−7.
In Figure 3.3 we present a frequency histogram of the number of iterations re-
quired for convergence.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
87
number of iterations
Frequency
0
10
20
30
40
50
0
50
100
150
200
250
300
350
Figure 3.3: Histogram of 103 number of iterations of Example 3.4.4
3.4.4.2
Row-wise Minimization Approach
Again we used 103 starting values generated uniformly from B1(R) and stopped the
iteration until the precision 10−5 is reached. The computing time for this approach
is as half as that for the univariate approach. If the minimized Γq is determined
by x∗= αxk + (1 −α)x∗∗where α ∈[0, 1] (Section 3.3.2), then the mean number
of iterations was 22 with the maximum distance equal to 2.411 and the minimum
distance was d(1):m = 1.21 × 10−6. IF the minimizing Γq is solved iteratively by
(3.16), then the mean number of iterations was 20.6 with maximum distance to
equal 3.658 and the minimum distance was d(1):m = 9.21 × 10−8. Note that both
methods give an absolute minimum distance eﬀectively equal to 0.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
88
3.4.5
Counterexample to Exact Degree of Freedom Argu-
ment
We have mentioned that degree of freedom in Chapter 1. When (1.5) is less or equal
to 0, namely, (p −q)2 ≤(p + q), then we expect there is at least one solution for
(1.2). For example, suppose we have that p = 3 and q = 1, then (p −q)2 = (p + q),
and so we think there exists at least one solution. However, when we generated
random correlation matrices R ∈R3×3, we found that not all such matrices lead to
an exact solution to (1.2) although many of them do. By applying the algorithm,
we computed d(1):m for 104 randomly generated correlation matrices. The minimum
distance obtained was 1.321341×10−13 which is eﬀectively 0. More than 90% of the
correlation matrices had distance eﬀectively equal to 0. However, there are quite a
few that do not result in distance equal to 0. For example, the maximum distance
obtained was 0.28.
3.5
The Prior and Posterior Distribution of d
The prior and posterior distribution of d cannot be obtained in closed form, rather
we must proceed via simulations. For example, for this we generate a sample of Σ
matrices from the prior distribution and compute the minimum distance for each
of the Σ matrices to Hq
0, namely, d(Σ, Hq
0). We use this sample to estimate the
prior distribution of d. For the posterior of d, we proceed similarly. Of course, this
assumes that we sample exactly, or approximately, from the prior and posterior of
Σ. In this Chapter, we restrict our attention to cases where this is possible and
discuss a situation where we can’t in Chapter 4.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
89
3.5.1
One Factor Example
For a ﬁrst example we consider some simulated data. Suppose now we want to
assess the hypothesized model, e.g. H1
0={Σ : Σ = Γ1Γ
′
1 + Ψ : Γ1 ∈R6×1}, i.e. 1-
factor model. We placed a conjugate prior Π on (µ, Σ) (Section 2.6.2.1) with p = 6,
namely,
µ|Σ ∼Normal(µ0, τ 2
0Σ)
Σ−1 ∼Wishart(k0, A0)
(3.19)
where k0 = 2×6 = 12, A0 = I6, µ0 = 06, and τ0 = 1. By applying ﬁtting algorithm,
we generated a prior distribution for d=min{∥Σ−Σ0∥2
F : Σ0 ∈H1
0(Σ)} with Monte
Carlo size N = 105. We plot this prior density histogram in Figure 3.4 with bin
width equal to 0.004.
As one may notice, there is some jiggling around the peak due to random noise.
A running-mean smoother is adopted to smooth the density curve by averaging
3 consecutive points.
Figure 3.5 shows a smoothed version of the prior density
estimate.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
90
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
2.5
d
Figure 3.4: Plot of prior density of d
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
2.5
d
Figure 3.5: Plot of smoothed prior density of d
To assess the eﬀectiveness of our approach, we generated data Y from the

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
91
N6(0, Γ1Γ
′
1 + Ψ) distribution with n = 100 where
Γ2 =

1
1
1
1
1
1
′
and Ψ = I6, i.e., H1
0 is true. The corresponding posterior π(µ, Σ|Y) (Theorem
2.6.3) is
µ|Σ ∼N6(µx, (n + 1/σ2
0)Σ)
Σ−1 ∼Wishart6(n + k0, B)
where µy = (n + 1
σ2
0 )−1(n¯y + 1
σ2
0 µ0), B−1 = (n −1)S + n¯y¯y
′ + A0 + ( 1
σ2
0 )µ0µ
′
0 −(n +
1
σ2
0 )µyµ
′
y with S =
1
n−1(Y −1¯y)
′(Y −1¯y).
In Figure 3.6, we present a plot of the posterior density histogram of d with bin
width equal to 0.001 and its smoothed version based on a running mean smoother
using 7 values, based on a Monte Carlo sample size of 105. The posterior has concen-
trated much more closely around zero than the prior. We repeated this procedure
with sample size n = 200 and have plotted the unsmoothed and smoothed posterior
density estimate of d in Figure 3.7. We see that, as expected, the posterior is much
more closely concentrate around zero when n = 200 than when n = 100. This is
also reﬂected in Figure 3.8 where we have plotted the prior and posterior of diﬀerent
sample sizes on the same plot.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
92
0.0
0.1
0.2
0.3
0.4
0
5
10
15
d
0.0
0.1
0.2
0.3
0.4
0
2
4
6
8
10
12
14
d
Figure 3.6: Plot of posterior distribution of d when n = 100: unsmoothed and
smoothed versions when H0 is true
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035
0
20
40
60
80
100
120
d
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035
0
20
40
60
80
100
120
d
Figure 3.7: Plot of Posterior distribution of d when n = 200: unsmoothed and
smoothed version when H0 is true

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
93
0.0
0.5
1.0
1.5
0
20
40
60
80
100
120
distance
prior
posterior when n=100
posterior when n=200
Figure 3.8: Comparison of Prior and Posterior distributions with diﬀerent sample
sizes when H0 is true
Now let’s consider the case when H0 is false, i.e., the data is generated from a
multivariate normal with variance matrix not in H1
0. We took the variance matrix
equal to Γ2Γ
′
2 + Ψ where
Γ2 =


1
0
1
0
0
0
0
1
0
1
0
0


′
and Ψ = I6.
Figure 3.9 shows the posterior density histogram of d with n = 100 based on a
Monte Carlo sample size 105 when H0 is false. The posterior density shifts away
from 0 thus concentrates less than the prior around the origin.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
94
0.0
0.5
1.0
1.5
0
1
2
3
4
5
6
distance
prior
posterior
Figure 3.9: Comparison of prior and posterior densities when H0 is false
3.5.2
Two Factor Example
Now let’s consider the case when data are generated from an 8-dimensional mul-
tivariate normal with a variance-covariance matrix produced via a 2-factor model,
namely, H2
0={Σ : Σ = Γ2Γ
′
2 + Ψ : Γ2 ∈R8×2}. Again, we place a conjugate prior
on (µ, Σ) as in Section 3.5.1. By applying the ﬁtting algorithm, we generated a
sample from the prior distribution of d = min{||Σ −Σ0||2
F : Σ0 ∈H2
0(Σ)} with
Monte Carlo size 105. We plot a smoothed prior density estimate in Figure 3.7,
based on taking the average of 7 consecutive points.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
95
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
distance
Figure 3.10: Plot of prior density of d
Now we consider a data set Y generated from N8(0, Γ2Γ
′
2 + Ψ) where Γ2 and
Ψ take the values as in Section 2.4.4, i.e., H2
0 is true. We present the prior and
posterior when H0 is true in Figure 3.11. We see that the posterior distribution has
a much greater concentration around 0 than the prior distribution.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
96
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
30
distance
prior
posterior
Figure 3.11: Plot of prior and posterior densities of d when H0 is true
Next we consider the case when H0 is false, i.e., the data are generated from
N(0, Σ∗) where Σ∗/∈H2
0, e.g., Σ∗= Γ3Γ
′
3 + Ψ, where
Γ3 =


1
0
1
0
1
0
0
0
0
1
0
0
1
0
0
0
0
0
1
0
0
1
0
0


′
and Ψ = I8. The prior and posterior density estimates are shown in Figure 3.12.
The posterior distribution leads to less concentration around 0 than the prior. We
see that the posterior is less concentrated around 0 than the prior.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
97
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
7
distance
prior
posterior
Figure 3.12: Plot of prior and posterior densities of d when H0 is false
For comparison purposes we present the prior and posterior density estimates
when H0 is true and H0 is false on same plot in Figure 3.13.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
98
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
30
distance
prior
posterior when H0 is true
posterior when H0 is false
Figure 3.13: Plot of prior and posterior densities of d when H0 is true and false
3.6
An Investigation of the Eﬀect of the Prior
In this section, we present the plots of the prior distribution of the distance based
on diﬀerent priors. Since the independent model is nested in the 1-factor model and
the 1-factor model is nested in the 2-factor model, etc., we expect to observe the
distribution of the distance becomes more and more concentrated around 0 as we
increase the number of factors, regardless of the prior used. This is indeed what we
observe.
3.6.1
An Arbitrary Prior of Σ
First we place a conjugate prior (3.19) on (µ, Σ).
In fact, we chose this prior
arbitrarily and we present the plot of prior density of d when q = 0, 1, 2, 3 in Figure

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
99
3.14. So we are able to observe the behavior of the prior distribution of d as we
increase the number of factors. All the simulations are based on a Monte Carlo
sample size of 105 with precision 10−5.
0
2
4
6
8
0.0
0.1
0.2
0.3
0.4
0.5
distance
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
2.5
distance
0.0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
12
distance
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0
50
100
150
200
distance
Figure 3.14: Plot of prior density of d when p = 6 and q = 0 (top left), p = 6 and
p = 1 (top right), p = 6 and q = 2 (bottom left), p = 6 and q = 3 (bottom right).

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
100
3.6.2
A Marginally Uniform Prior on R
Now we plot the distribution of distance for p = 6 when we put a marginally uniform
prior on R by choosing k0 = p + 1 for q = 0, 1, 2, 3.
0
5
10
15
0.00
0.05
0.10
0.15
distance
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
distance
0.0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
distance
0.0
0.2
0.4
0.6
0.8
0
10
20
30
40
50
distance
Figure 3.15: Plot of prior density of d when p = 6 and q = 0 (top left), p = 6 and
p = 1 (top right), p = 6 and q = 2 (bottom left), p = 6 and q = 3 (bottom right).

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
101
3.6.3
A Jointly Uniform Prior on R
In Section 2.6.2.2 we introduced a prior based on the correlation factorization. We
might choose a noninformative prior on R for several reasons in a factor analysis
context, see the discussion in Section 2.6.2.2.
We also introduced the sampling
algorithm to sample from this prior, namely, the “onion method”. Now we present
the distribution of the distance when we put a jointly uniform distribution on R,
namely, p(R) ∝1.
0
1
2
3
4
5
6
7
0.0
0.1
0.2
0.3
0.4
0.5
0.6
distance
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
distance
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
1
2
3
4
distance
0.0
0.1
0.2
0.3
0.4
0
5
10
15
20
25
30
35
distance
Figure 3.16: Plot of prior density of d when p = 6 and q = 0 (top left), p = 6 and
p = 1 (top right), p = 6 and q = 2 (bottom left), p = 6 and q = 3 (bottom right).

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
102
3.7
Choosing the Number of Factors
3.7.1
Computing the ORS
As discussed in Section 3.1, our approach to assessing the hypothesis Hq
0 is to com-
pute the ORS as given by
Πq
πq(d|X)
πq(d)
> πq(0|X)
πd(0) |X

(3.20)
where Πq is the marginal prior of d = d(Hq
0, Σ) with density πq. Note that Hq
0 is
true (i.e., Σ ∈Hq
0) if and only if d = 0 and so (3.20) amounts to computing the
posterior probability of obtaining a change in belief, from a priori to a posteriori,
greater than that observed at Hq
0. If this posterior probability is large, then the
data are clearly providing evidence against Hq
0.
A numerical problem arises when computing (3.20). This is because we need
to evaluate πq(0|X)/π(0) and typically πq(0) will be very small simply because Hq
0
is a lower dimensional subset of the parameter space. We note, however, that if
dα denotes a α-quantile of Πq then π(dα|X)/π(dα) →πd(0|X)/πd(0) as α →0.
Accordingly we take α small, and such that we can estimate π(dα) with reasonable
accuracy, and then use the approximation
πq(0|X)
πq(0)
≈πq(dα|X)
πq(dα)
The choice of α will of course be inﬂuenced by the Monte Carlo sample size we use
to estimate Πq. For the following simulations we took the sample size to be 105 and
choose α = 0.01.
Example 3.7.1 One factor example in Section 3.5.1
In Section 3.5.1 we plotted the prior and posterior density estimates when H1
0 is true.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
103
We observed that the posterior distribution is more concentrated around 0 than the
prior distribution of distance when H1
0 is true. We have plotted the relative belief
ratio π1(d|X)/π1(d) in Figure 3.17.
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0
5
10
15
20
25
30
distance
ratio
Figure 3.17: Plot of relative belief ratio when H0 is true
The relative ratio at 0 is approximated by π(dα|X)/π(dα) where dα = 0.0526
with α = 1%, i.e., the 1st percentile of the prior distribution. In this case we obtain
the approximated value 31. The ORS is then computed via (3.20) and it equals
Π(π1(d|X)
π1(d)
> 31|X) = 0
Accordingly we have no evidence against H1
0.
Next we consider the case when H0 is false from this section and the plot of
relative belief ratio is shown in Figure 3.18.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
104
0.2
0.4
0.6
0.8
0.0
0.5
1.0
1.5
2.0
2.5
distance
ratio
Figure 3.18: Plot of relative belief ratio when H0 is false
Again the relative belief ratio at 0 is approximated by π(dα|X)/π(dα) where
dα = 0.0526 with α = 1%, i.e., the 1st percentile of the prior distribution and it is
approximately equal to 0. The ORS is thus approximately equal to 1 and we have
strong evidence against the null hypothesis.
3.7.2
Inference of the Number of Factors
Our method to determine the number of factors, is to start with then independence
model, i.e., q = 0, and assess whether there is evidence against the model. If q = 0
is rejected, then we proceed with the one factor model, i.e., q = 1, and proceed
accordingly (Section 1.2.3).
Example 3.7.2 Example 1.3 where likelihood approach fails
We now revisit the example in Section 1.3.2 where the likelihood approach does not
work properly. We choose the conjugate prior (3.19) and start with the q = 0 case.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
105
We have plotted the prior and posterior densities when q = 0 in Figure 3.19.
0
2
4
6
8
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
distance
prior
posterior
Figure 3.19: Comparison of prior and posterior densities of d when q = 0
The relative belief ratio at 0 is approximated by π(dα|X)/π(dα) where dα = 0.53
with α = 1%. We have that the ORS is equal to 1 thus we have very strong evidence
against H0
0.
In Figure 3.20 we plotted the prior and posterior densities when q = 1. Again
the relative belief ratio is approximated by 0, so the computed ORS is equal to 1.
So we have strong evidence against the null hypothesis that it is 1-factor model and
proceed with q = 2.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
106
0.0
0.5
1.0
1.5
0
1
2
3
4
distance
prior
posterior
Figure 3.20: Comparison of prior and posterior densities of d when q = 1
In Figure 3.21 we plotted the prior and posterior densities when q = 2. Because
π2(0) > 0 we obtained the relative surprise ratio at 0 exactly and it is equal to 0.
The ORS is equal to 1, so we reject the null hypothesis and proceed with q = 3.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
107
0.0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
12
distance
prior
posterior
Figure 3.21: Comparison of prior and posterior densities of d when q = 2
In Figure 3.22 we plotted the prior and posterior densities when q = 3. We obtain
π3(0|X)/π3(0) exactly and it is equal to 0.00384. The ORS is equal to 0.9982. It
suggests that a model with more than 3 factors is needed.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
108
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0
50
100
150
200
distance
Figure 3.22: Comparison of prior and posterior densities of d when q = 3
When q = 4 we obtained π4(0|X)/π4(0) = 1.01 exactly and the ORS is equal to
0.0355. Therefore, we do not have evidence against H4
0 and we conclude that it is a
4-factor model.
Now we consider a diﬀerent prior for the covariance matrix such that we put a
marginally uniform prior on R (Corollary 2.6.1). Again we start with q = 0 and
plot the prior and posterior density in Figure 3.23. The relative belief ratio at 0 is
approximated by π4(0|X)/π4(0) = 0.0105 where dα = 1.165 with α = 0.01. We see
that the ORS is equal to 0.9985 and we have strong evidence against H0
0.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
109
0
5
10
15
0.0
0.2
0.4
0.6
0.8
1.0
1.2
distance
prior
posterior
Figure 3.23: Comparison of prior and posterior densities of d when q = 0
In Figure 3.24 we plotted the prior and posterior densities when q = 1. Because
π1(0) > 0 we obtained the relative surprise ratio at 0 exactly and it is equal to 0.
The ORS is equal to 1, so we reject the null hypothesis and proceed with q = 2.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
110
0.0
0.5
1.0
1.5
2.0
0
1
2
3
4
distance
prior
posterior
Figure 3.24: Comparison of prior and posterior densities of d when q = 1
In Figure 3.25 we plotted the prior and posterior densities when q = 2. We
obtained the relative surprise ratio at 0 exactly and it is equal to 0. The ORS is
equal to 1, so we reject the null hypothesis and proceed with q = 3.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
111
0.0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
distance
prior
posterior
Figure 3.25: Comparison of prior and posterior densities of d when q = 2
In Figure 3.26 we plotted the prior and posterior densities when q = 3. We
obtained the relative surprise ratio at 0 exactly and it is equal to 0.008. The ORS
is equal to 0.993, so we reject the null hypothesis and proceed with q = 4.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
112
0.0
0.2
0.4
0.6
0.8
0
10
20
30
40
50
distance
prior
posterior
Figure 3.26: Comparison of prior and posterior densities of d when q = 3
When q = 4 we obtained π4(0|X)/π4(0) = 1.42 exactly and the ORS is equal
to 0. Therefore, we do not have evidence against H4
0 and we conclude that it is a
4-factor model.
Note that we draw the same conclusion by choosing two diﬀerent priors. The
measure of surprise is robust to prior choice.
3.8
Inference about R when Hq
0 is true
Suppose we have tested Hq
0 and have found no evidence against this hypothesis.
Further suppose we are now willing to proceed as if Hq
0 is true.
Consider ﬁrst the general situation where we have a statistical model {fθ : θ ∈Θ},
a prior Π with respect to support measure ν and a parameter of interest ψ = Ψ(θ)
where Ψ : Θ →T . Suppose the prior distribution of ψ has density πΨ with respect
to support measure νT on T . If we test H0 : {θ : Ψ(θ) = ψ0}, and have no evidence

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
113
against H0, then presumably we are willing to proceed as if ψ = ψ0. If Ψ is not
1 −1 then we are still uncertain about the true value of θ. Based on our a priori
beliefs, the appropriate prior for θ, given that ψ = ψ0, is the conditional prior with
density
π(θ)JΨ(θ)
πΨ(ψ0)
(3.21)
with respect to support measure νψ0 on Ψ−1(ψ0) = {θ : Ψ(θ) = ψ0} and where
JΨ(θ) = (det(dΨ ◦dΨ
′)(θ))−1/2, dΨ is the diﬀerential of Ψ, and
πΨ(ψ0) =
Z
Ψ−1{ψ0}
π(θ)JΨ(θ)νψ0(dθ).
Note that we are implicitly assuming here that all relevant sets are manifolds with
the support measures the analogs of Euclidean volume on these spaces and Ψ is
smooth.
Now based on (3.21) the posterior of θ, given ψ = ψ0, equals
fθ(X)π(θ)JΨ(θ)π−1
Ψ (ψ0)
R
Ψ−1 fθ(X)π(θ)JΨ(θ)π−1
Ψ (ψ0)νψ0(dθ)
=
fθ(X)π(θ)JΨ(θ)
R
Ψ−1 fθ(X)π(θ)JΨ(θ)νψ0(dθ)
(3.22)
The overall posterior of θ is given by
π(θ|X) = fθ(X)π(θ)
m(X)
When we condition on Ψ−1{ψ0} we obtain that the conditional posterior has density
given by
fθ(X)π(θ)
m(X)
JΨ(θ)
πΨ(ψ0|X)
(3.23)

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
114
with respect to νψ0 and where
πΨ(ψ0|X) =
Z
Ψ−1{ψ0}
fθ(X)π(θ)
m(X)
JΨ(θ)νψ0(dθ).
This implies that (3.23) equals (3.22), so we have proved the following.
Proposition 3.8.1. When conditioning the posterior to a subset H0 = Ψ−1{ψ0}, if
we obtain the conditional posterior from the conditional prior or directly condition
the posterior, we obtain the same result.
This is a natural and satisfying result.
Now note that, given ψ = ψ0, we can obtain relative surprise inferences about
θ ∈Ψ−1{ψ0} based on the relative belief ratio.

fθ(X)π(θ)JΨ(θ)
R
Ψ−1 fθ(X)π(θ)JΨ(θ)νψ0(dθ)
 
π(θ)JΨ(θ)
R
Ψ−1 π(θ)JΨ(θ)νψ0(dθ)
−1
=
fθ(X)
m(X)
 πΨ(ψ0|X)
πΨ(ψ0)
−1
(3.24)
for θ ∈Ψ−1{ψ0}. Note that as a function of θ ∈Ψ−1{ψ0} the relative surprise
ratio (3.24) is proportional to fθ(X). This implies that the conditional LRSE for θ
is equal to the restricted MLE. Further the conditional ORS at θ0 is given by
Π
 fθ(X) > fθ0(X)|Ψ−1{ψ0}

and so relative surprise regions for θ are restricted likelihood regions.
The situation for inferences about R, conditioned on Hq
0, is more complicated.
For we have that θ = (µ, σ1, · · · , σp, R) = (θ1, θ2) ∈Θ1 × Θ2 where θ2 = R. In this
case we integrate out θ1 ﬁrst replacing fθ(X) by the integrated likelihood f ∗
θ2(X) =
R
fθ1,θ2(X)π(θ1|θ2)ν1(dθ1), and replacing π by π2(θ2) =
R
Ψ1 π(θ1, θ2)ν1(dθ1). So in

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
115
this case the relative surprise ratio is proportional to f ∗
θ2(X). This implies that the
conditional LRSE given Hq
0 is the restricted MLE based on the restricted integrated
likelihood f ∗
θ2(X) for θ2 ∈Hq
0. Further the conditional ORS at θ20 is
Π(f ∗
θ2(X) > f ∗
θ20(X)|Ψ−1{ψ0}).
We see that ﬁnding the conditional LRSE of R is a computationally diﬃcult
problem, which at this time remains unresolved. We can, however, propose a con-
venient approximation.
For this let ˆR denote the correlation matrix obtained from the MLE (bµ, bΣ), i.e.,
bR is the plug-in MLE of R. Now apply the minimization algorithm to bR and let bΓq
denote the Γq matrix corresponding to the minimum obtained. Then we estimate
R by
ebR = bΓqbΓ
′
q + bΨ,
where bΨ is determined from bΓq.
The matrix ebR is an approximation to the conditional LRSE in the following sense.
Suppose that Hq
0 is true. Then the MLE bR will converge to the true correlation
matrix in Hq
0 as we increase the amount of data. Accordingly we can expect that
ebR and the conditional LRSE will be similar for large data sets.
We can quantify the uncertainty in ebR as follows. Let R be generated from the
posterior and ﬁnd R∗∈Hq
0 closest to R, then determine the distance between R∗
and ebR. A plot of the posterior distribution of this distance gives some indication of
how much uncertainty there is in ebR.
We note that the estimation procedure for R, given that Hq
0 is true, also produces
an estimate of Γq, the standardized factor loading matrix. It should be remembered,
however, that this estimate is somewhat arbitrary, not only due to the arbitrariness

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
116
that arises by post-multiplication by an orthogonal matrix, but because we can have
two lower-triangular Γq matrices that produce the same R matrix.
We now consider some examples of this inference procedure.
Example 3.8.1 Estimation of R and the Factor Loadings for One-factor Model
For this example we have generated a sample of size n = 50 as described in Section
3.5.1, i.e., we generated data Y from the N6(0, Γ1Γ
′
1 + Ψ) distribution with n = 50
where
Γ1 =

1
1
1
1
1
1
′
and Ψ = I6, i.e., H1
0 is true. The true correlation matrix is
Rtrue =


1
0.5
0.5
0.5
0.5
0.5
0.5
1
0.5
0.5
0.5
0.5
0.5
0.5
1
0.5
0.5
0.5
0.5
0.5
0.5
1
0.5
0.5
0.5
0.5
0.5
0.5
1
0.5
0.5
0.5
0.5
0.5
0.5
1


.
The plug-in MLE for R is
bR =


1
0.577717
0.4724463
0.4745304
0.4716781
0.5501953
0.577717
1
0.5053433
0.5716013
0.5065358
0.5949968
0.4724463
0.5053433
1
0.4389522
0.4215333
0.4310375
0.4745304
0.5716013
0.4389522
1
0.5598629
0.3738716
0.4716781
0.5065358
0.4215333
0.5598629
1
0.4882861
0.5501953
0.5949968
0.4310375
0.3738716
0.4882861
1


.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
117
Now we apply the minimization algorithm with p = 6 and q = 1 with 103 starting
values from B1(R), we obtain d(1):m = 0.026163 with the minimizing Γ1 equal to
bΓ1 =


0.7268181
0.8053636
0.6310471
0.6827278
0.6891239
0.6926690


and so we may estimate Ψ by (1.4) and obtain
bΨ =


0.4724314
0
0
0
0
0
0
0.3515886
0
0
0
0
0
0
0.6016787
0
0
0
0
0
0
0.5336003
0
0
0
0
0
0
0.5248902
0
0
0
0
0
0
0.5201791


.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
118
Therefore R is estimated to be
ebR =


1
0.5853528
0.4586565
0.4962189
0.5008677
0.5034444
0.5853528
1
0.5082224
0.5498441
0.5549953
0.5578504
0.4586565
0.5082224
1
0.4308334
0.4348697
0.4371068
0.4962189
0.5498441
0.4308334
1
0.470484
0.4729044
0.5008677
0.5549953
0.4348697
0.470484
1
0.4773348
0.5034444
0.5578504
0.4371068
0.4729044
0.4773348
1


.
We note that ebR does better at estimating ρ46, where bR is seriously oﬀ, and the
remaining estimates are comparable.
To quantify the uncertainty of this estimate, we plot the posterior distribution
of the distance ebR to the R∗∈H1
0 that is closest to R generated from the posterior.
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
2.5
distance
Figure 3.27: Plot of posterior distribution of distance

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
119
Example 3.8.2 Estimations of R and the Factor Loadings for Two-factor model
Now we generated data Y by taking the variance matrix equal to Γ2Γ
′
2 + Ψ with
n = 50 where
Γ2 =


1
0
1
0
0
0
0
1
0
1
0
0


′
and Ψ = I6, i.e., H2
0 is true. So the true correlation matrix is
Rtrue =


1
0
0.5
0
0
0
0
1
0
0.5
0
0
0.5
0
1
0
0
0
0
0.5
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1


.
The plug-in MLE for R is
bR =


1
0.0519667
0.4693124
−0.048804
−0.124992
0.069185
0.0519667
1
−0.007978
0.5927194
−0.015378
0.1249498
0.4693124
−0.007978
1
−0.065858
−0.138574
−0.054588
−0.048804
0.5927194
−0.065858
1
0.1101113
−0.131696
−0.124992
−0.015378
−0.138574
0.1101113
1
−0.012076
0.069185
0.1249498
−0.054588
−0.131696
−0.012076
1


.
Now we apply the minimization algorithm with p = 6 and q = 2 with 103 starting

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
120
values from B2(R), we obtain d(1):m = 0.0419852 with the minimization Γ2 equal to
bΓ2 =


−0.745229
0
−0.064828
0.589925
−0.625703
−0.038945
0.0640407
0.9979473
0.1930417
0.0718414
−0.033954
−0.040195


and so we may estimate Ψ by (1.4) and obtain
bΨ =


0.4446343
0
0
0
0
0
0
0.6477858
0
0
0
0
0
0
0.6069789
0
0
0
0
0
0
0
0
0
0
0
0
0
0.9575737
0
0
0
0
0
0
0.9972315


.
And ﬁnally the estimate of R is
ebR =


1
0.0483116
0.4662919
−0.047725
−0.14386
0.0253034
0.0483116
1
0.0175887
0.5845625
0.0298665
−0.021511
0.4662919
0.0175887
1
−0.078935
−0.123585
0.0228104
−0.047725
0.5845625
−0.078935
1
0.0840564
−0.042287
−0.14386
0.0298665
−0.123585
0.0840564
1
−0.009442
0.0253034
−0.021511
0.0228104
−0.042287
−0.009442
1


.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
121
In this example, ebR seems to do a much better job at estimating ρ26 and ρ46 than bR
with the remaining estimates comparable.
To quantify the uncertainty of this estimation, we plot the posterior distribution
of the distance of ebR to R∗∈H2
0 that is closet to R generated from the posterior.
0.0
0.5
1.0
1.5
0.0
0.5
1.0
1.5
2.0
distance
Figure 3.28: Plot of posterior distribution of distance
Example 3.8.3 Estimation of R and the Factor Loadings for Example 3.7.3
In Example 3.7.3, we have concluded that H4
0 is true by computing the ORS. By
the aforementioned approximation method we can estimate R and also Γq. The
sample correlation matrix is shown in Table 1.1 with n = 100. Then by applying
the minimization algorithm for p = 6 and q = 4 with 103 starting values from B4(R),

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
122
we obtain the minimum distance to be 4.0716 × 10−6 with minimization Γ4.
bΓ4 =


−0.991378
0
0
0
−0.599391
−0.794777
0
0
−0.474698
−0.394635
0.7847409
0
−0.588632
0.0696786
−0.140438
−0.642289
−0.249872
−0.557906
−0.308327
−0.548667
−0.32307
−0.302464
0.3667937
−0.545788


and Ψ is estimated by
bΨ =


0.0171694
0
0
0
0
0
0
0.0090602
0
0
0
0
0
0
0.0031067
0
0
0
0
0
0
0.2163999
0
0
0
0
0
0
0.2302039
0
0
0
0
0
0
0.371719


.
And ﬁnally the estimate of R is
ebR =


1
0.5942228
0.4706053
0.5835569
0.2477181
0.3202844
0.5942228
1
0.5981765
0.2974416
0.5931818
0.4340362
0.4706053
0.5981765
1
0.1417174
0.0968262
0.5605614
0.5835569
0.2974416
0.1417174
1
0.5039123
0.4681359
0.2477181
0.5931818
0.0968262
0.5039123
1
0.4358361
0.3202844
0.4340362
0.5605614
0.4681359
0.4358361
1


.

CHAPTER 3.
METHODOLOGY FOR FACTOR ANALYSIS
123

Chapter 4
Importance Sampling for Factor
Analysis
The results in Chapter 3 depended on being able to sample from the prior and the
posterior of (µ, Σ). For this reason we relied on a commonly used conjugate prior.
Suppose, however, we want to use the decomposition Σ = D1/2RD1/2, where R is
the correlation matrix, and put a uniform prior on R and an independent prior on
D = diag(d1, · · · , dp) (Section 2.6.2.2). In particular, we want to use the prior
µ|Σ ∼Np(µ0, σ2
0Σ),
R ∼Uniform,
di ∼Inverse Gamma(α0i, β0i)
(4.1)
A real advantage of this prior is that we need only elicit priors for the location
parameters µ1, · · · , µp and the scale parameters d1 = σ11, · · · , dp = σpp, e.g., see
Section 2.6.2.2 where this is discussed. Certainly, it is much more diﬃcult to elicit
correlations and we avoid that here by using a noninformative prior. Note that we
avoid the use of improper noninfomative priors.
124

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
125
While it is natural to use this prior, there is also a signiﬁcant disadvantage. This
arises because an exact sampler isn’t available for the posterior. Moreover, not only
is there not an exact sampler, it seems very diﬃcult to come up with an eﬀective
MCMC algorithm for the posterior.
We discuss the diﬃculties associated with
obtaining a Gibbs Sampler in Section 4.1. In Section 4.2 we show that computational
diﬃculties associated with using this prior can be avoided by using an importance
sampling algorithm that arises very naturally. We note that, as discussed in Section
2.6.2.2, we have an algorithm for exact generation from the prior.
4.1
Some Distribution Theory
Theorem 4.1.1. Suppose we observe Y = (y1, · · · , yn) ∼Np(µ, Σ) and place the
prior given by (4.1) on (µ, D, R). Then the marginal posterior density of R, namely,
p(D|Y) is given by
(2π)−n/2|R|−n
2
Y
i
[βα0i
0i /(2π)1/2Γ(α0i)](
X
k
Aikρik
2
+ β0i)−α0i−n
2 + p
2
(4.2)
where Aik is the (i, k)-th element of
A = (n −1)S + [n/(nσ2
0 + 1)][(¯y −µ0)(¯y −µ0)
′].
with S =
1
n−1(Y −1¯y
′)
′(Y −1¯y
′) and ρik = (−1)i+k |Rik|
|R|
with Rik being the sub-
matrix of R by deleting ith row and kth column.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
126
Proof. The prior takes the form
p(µ, D, R) = p(µ|D, R)p(D)p(R)
=
Q
i[βα0i
0i /(2π)1/2Γ(α0i)]|D|−1
2|R|−1
2(Q
i d−1−α0i
i
)exp{−1
2[ 1
σ2
0 (µ −µ0)
′(D1/2RD1/2)−1
(µ −µ0)] −P
i β0id−1
i }
=
Q
i[βα0i
0i /(2π)1/2Γ(α0i)]|R|−1
2(Q
i d
−3
2−α0i
i
)exp{−1
2[ 1
σ2
0 (µ −µ0)
′(D1/2RD1/2)−1
(µ −µ0)] −P
i β0id−1
i }.
(4.3)
Multiplying (4.3) by the likelihood we see that
p(µ, D, R|Y)
= Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|D|−n
2 + p
2|R|−1
2−n
2 (Q
i d
−3
2−α0i
i
)exp{−1
2[Pn
i=1(yi −µ)
′
(D1/2RD1/2)−1(yi −µ) + 1
σ2
0 (µ −µ0)
′(D1/2RD1/2)−1(µ −µ0)] −P
i β0id−1
i }
= Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−1
2−n
2 (Q
i d
−3
2−α0i−n
2 + p
2
i
)exp{−1
2[Pn
i=1(yi −µ)
′
(D1/2RD1/2)−1(yi −µ) + 1
σ2
0 (µ −µ0)
′(D1/2RD1/2)−1(µ −µ0)] −P
i β0id−1
i }.
Integrate out µ, we have the following
p(D, R|Y) = Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−n
2 (Q
i d
−1−α0i−n
2 + p
2
i
)
exp{−1
2tr[A(D1/2RD1/2)−1] −P
i β0id−1
i }
(4.4)
where
A = (n −1)S + [n/(nσ2
0 + 1)][(¯y −µ0)(¯y −µ0)
′],
S =
1
n−1(Y −1¯y
′)
′(Y −1¯y
′).

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
127
Now (4.4) can also be written as
p(D, R|Y)
=
Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−n
2 (Q
i d
−1−α0i−n
2 + p
2
i
)
exp(−P
i
P
k
Aikρik
2d1/2
i
d1/2
k
−P
i
β0i
di )
=
Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−n
2 (Q
i d
−1−α0i−n
2 + p
2
i
)
Q
i exp(−P
k
Aikρik
2d1/2
i
d1/2
k
−β0i
di )
where Aik is the (i, k)-th element of A and ρik = (−1)i+k |Rik|
|R| , and Rik is the sub-
matrix of R obtained by deleting ith row and kth column. So we have immediately
that
p(R|D, Y) =
Y
i
[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−n
2
Y
i
exp(−
X
k
Aikρik
2d1/2
i
d1/2
k
). (4.5)
To obtain the marginal density for R, we need to integrate out D from p(D, R|Y).
Let Bi = P
k
Aikρik
2
+ β0i, so we can write
p(R|Y) = Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−n
2 Q
i
R ∞
0 (d
−1−α0i−n
2 + p
2
i
)exp(−Bi
di )dµ(di).
(4.6)
Putting ηi = Bi
di , and multiplying by the Jacobian, we obtain
p(R|Y)
=
Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−n
2 Q
i B
−α0i−n
2
i
R ∞
0 (η
1+α0i+ n
2 −p
2
i
)
exp(−ηi)dµ(ηi)
=
Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2|R|−n
2 Q
i B
−α0i−n
2
i
Γ(α0i + n−p
2 )
=
Q
i[βα0i
0i /(2π)1/2Γ(α0i)](2π)−n/2Γ(α0i + n−p
2 )|R|−n
2 Q
i(P
k
Aikρik
2
+ β0i)−α0i−n
2 + p
2.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
128
Since p(D|R, Y) = p(D,R|Y)
p(R|Y) , we have
p(D|R, Y) ∝Q
i(P
k
Aikρik
2
+ β0i)α0i+ n
2 −p
2(d
−1−α0i−n
2 + p
2
i
)exp(−P
k
Aikρik
2d1/2
i
d1/2
k
−β0i
di )
so the conditional distribution of di given (R, Y) is Inverse Gamma (α0i + n
2 −
p
2, P
k
Aikρik
2
+ β0i). So we can sample from the full conditionals of the δi but (4.5)
does not seem to lead to an easy algorithm for generating from the full conditionals
of ρij.
4.2
Importance Sampling
Suppose µ has the prior in (4.1) and let Π1 denote a prior on D and Π2 a prior
on R with D and R be a priori independent.
Then since the Jacobian of the
transformation (D, R) →Σ is given by |diag(Σ)|
(p−1)
2 , the joint prior on (µ, Σ) is
proportional to
|Σ|−1/2exp[−1
2τ 2
0 (µ −µ0)
′Σ−1(µ −µ0)]π1(diagΣ)×
π2(diag(Σ)−1/2Σdiag(Σ)−1/2)|diag(Σ)|−(p−1)
2 .
(4.7)
Making a change of variable Σ →J = Σ−1, which has Jacobian |Σ|(p+1), the prior
density of (µ, J ) is
|J |−(p+ 1
2)etr[−1
2τ 2
0 J (µ −µ0)(µ −µ0)
′]π1(diag(J −1))|diag(J −1)|−(p−1)
2 ×
π2((diagJ −1)−1/2J −1(diagJ −1)−1/2).
The likelihood function is proportional to
|Σ|−n
2 etr[−n
2 Σ−1(¯y −µ)(¯y −µ)
′ −n −1
2
Σ−1S],

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
129
so the joint posterior of (µ, J ) is proportional to
|J |
n
2 −(p+ 1
2)etr[−1
2σ2
0 J (µ −µ0)(µ −µ0)
′ −n
2J (¯y −µ)(¯y −µ)
′]×
etr[−(n−1)
2
J S]π1(diag(J −1))|diag(J −1)|−(p−1)
2 π2((diagJ −1)−1/2J −1(diagJ −1)−1/2).
(4.8)
Now we have that
1
τ 2
0 (µ −µ0)
′J (µ −µ0) + n(µ −¯y)
′J (µ −¯y)
=
1
τ 2
0 (µ
′J µ −2µ
′J µ0 + µ
′
0J µ0) + n(µ
′J µ −2µ′J ¯y + ¯y
′J ¯y)
=
( 1
τ 2
0 + n)µ′J µ −2( 1
τ 2
0 µ0 + n¯y)′J µ + 1
τ 2
0 µ
′
0J µ0 + n¯y
′J ¯y
=
( 1
τ 2
0 + n)[(µ −( 1
τ 2
0 + n)−1( 1
τ 2
0 µ0 + n¯y))′J (µ −( 1
τ 2
0 + n)−1( 1
τ 2
0 µ0 + n¯y))]
−( 1
τ 2
0 + n)−1( 1
τ 2
0 µ0 + n¯y)′J ( 1
τ 2
0 µ0 + n¯y) + 1
τ 2
0 µ
′
0J µ0 + n¯y
′J ¯y.
Integrating out µ of the posterior gives that the posterior of J is proportional to
|J |
n
2 −petr(−A(Y)J
2
)π1(diag(J −1))|diag(J −1)|−(p−1)
2 ×
π2((diagJ −1)−1/2J −1(diagJ −1)−1/2)
where
A = (n −1)S −[n/(nσ2
0 + 1)][(¯y −µ0)(¯y −µ0)
′].
Therefore, we have proved the following result.
Theorem 4.2.1. Suppose we put the prior (4.7) on (µ, Σ). Then the posterior
density of J is proportional to a Wishartp(A−1(Y), n −p + 1) density times
k(J ) = π1(diag(J −1))|diag(J −1)|−(p−1)
2 π2((diagJ −1)−1/2J −1(diagJ −1)−1/2).

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
130
It is important to note the following implication of Theorem 4.2.1. We have
proved that the posterior distribution of Σ−1 factors as a Wishart density times a
function of J −1 = Σ that does not involve the data Y. This function only depends
on the prior Π1 on D and the prior Π2 on R. Depending on, of course, how we choose
Π1 and Π2, this can lead to a simple simulation approach to calculating integrals
with respect to the posterior.
To see this let us consider the importance sampling approach to evaluating inte-
grals. Suppose we have X|Y ∼fX|Y and we want to evaluate E[g(X)|Y] for some
function g. Note that
E[g(X)|Y] =
Z
g(x)fX|Y(x)dx =
Z g(x)fX|Y(x)
h(x)
h(x)dx = Eh[g(x)fX|Y(x)
h(x)
]
for any density h with supp(f) ⊆supp(h). If we can only have the unnormalized
density of X|Y say qX|Y(x), then we can write
E[g(X)|Y] =
R
g(x)qX|Y(x)dx
R
qX|Y(x)dx
=
R
[g(x)qX|Y(x)/h(x)]h(x)dx
R
[qX|Y(x)/h(x)]h(x)dx
.
Provided that we have a simple algorithm for generating from h, we can then gen-
erate x1, · · · , xN from density h and estimate E[g(X)|Y] by
PN
i=1 g(xi)r(xi)
PN
i=1 r(xi)
(4.9)
where r(xi) =
qX|Y(xi)
h(xi) . Note that by the SLLN, (4.9) converges almost surely to
E[g(X)|Y].
The approximation (4.9) will not work well, however, unless h(x) is chosen care-
fully. In particular, we want Varh(
g(x)fX|Y(x)
h(x)
) < ∞so we can apply the Central
Limit Theorem to the numerator and denominator in (4.9). Applying the delta the-

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
131
orem to this ratio then gives the asymptotic normality of the ratio and we can use
the standard error of (4.9) to assess the accuracy of the approximation, see Evans
and Swartz [13] where this is discussed.
Naturally we would like Varh(
g(x)fX|Y(x)
h(x)
) to be as small as possible The general
idea, provided the functions g of interest are not too irregular, is to choose h to be
as close as possible to fX|Y. For our application we have a very natural choice of h,
namely, take h to be the Wp(A−1(Y), n −p + 1) density. This is the “hard” part of
the density as it contains all the dependence on the data.
Corollary 4.2.1. Suppose we take Π2 to be the uniform prior on R, i.e., π2(R) ≡1,
whenever R is a correlation matrix and is 0 otherwise. Further take π1 to be the
product of Inverse Gamma (αi, βi) densities, then
k(J ) =
p
Y
i=1
(σii)−αi−(p+1)/2 exp(−βi/σii).
Proof. By (4.7), the joint prior on (µ, Σ) is proportional to
|Σ|−1/2exp[−1
2τ 2
0
(µ −µ0)
′Σ−1(µ −µ0)]π1(diagΣ)|diag(Σ)|−(p−1)
2 .
Making the change of variable of Σ →J = Σ−1, which has Jacobian |Σ|p+1, the
joint posterior of (µ, J ) by (4.8) is proportional to
|J |
n
2 −petr(−A(Y)J
2
)π1(diag(J −1))|diag(J −1)|−(p−1)
2 .
(4.10)

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
132
By (4.1), δi ∼Inverse Gamma (α0i, β0i), then (4.10) equals
|J |
n
2 −petr(−A(Y)J
2
) Q
i π1(δi)δ
−(p−1)
2
i
∝
|J |
n
2 −petr(−A(Y)J
2
) Q
i(1/δi)α0i+1 exp(−β0i/δi)(δi)−(p−1)
2
=
|J |
n
2 −petr(−A(Y)J
2
) Q
i(1/δi)−α0i−p/2−1/2 exp(−β0i/δi)
(4.11)
According to Theorem 4.2.1, k(J ) equals
p
Y
i=1
(σii)−α0i−(p+1)/2 exp(−βi/σii).
Note that k(J ) is a fairly simple function of the σii. In a similar fashion we obtain
the following result.
Corollary 4.2.2. Suppose we take Π2 to be the uniform prior on R, i.e., π2(R) ≡1,
whenever R is a correlation matrix and is 0 otherwise. Further take π1 to be the
product of Log-Normal (µi, σi) densities, then
k(J ) =
p
Y
i=1
σ−1
i σ−1−(p+1)/2
ii
exp[−(log σii −µi)2/2σ2
i ].
In our case h(J ) is the density of an Wishart distribution.
So we generate
J1, · · · , JN ∼Wishartp(A−1(Y), n −p + 1). Then for each Ji, calculate Σi = J −1
i
,
obtain Di = diag(Σi) and calculate Ri = D−1/2
i
ΣiD−1/2
i
. Suppose π1 is as speciﬁed
in Corollary 4.2.1, the estimate of E[g(R)|Y] is then given by
ˆE[g(R)|Y]
=
PN
i=1 g(Ri) Qp
j=1(σjji)−αj −(p+1)
2 exp(−βj/σjji)
PN
i=1
Qp
j=1(σjji)−αj −(p+1)
2 exp(−βj/σjji)
= PN
i=1 g(Ri)wi,N

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
133
with wi,N =
Qp
j=1(σjji)−αj −(p+1)
2 exp(−βj/σjji)
PN
i=1
Qp
j=1(σjji)−αj −(p+1)
2 exp(−βj/σjji).
4.3
Implementation of Importance Sampling
In Section 2.6.2.2, we introduced the sampling method to exactly sample from a
jointly uniform prior. We also presented several marginal prior densities of d, when
using this joint prior, in Section 3.6.3. Now we implement importance sampling to
estimate the posterior density of d.
4.3.1
One Factor Example
4.3.1.1
Inverse Gamma Priors on the Scalings
We consider a one factor model, e.g. H1
0 = {Σ : Σ = Γ1Γ
′
1 + Ψ, Γ1 ∈R6×1}. We
generate data Y from the N6(0, Γ1Γ
′
1 + Ψ) distribution with n = 100, and where
Γ1 =

1
1
1
1
1
1
′
,
and Ψ = I6.
First we place prior (4.1) on (µ, D, R) so the weights are Inverse
Gamma (1, 1). We plot the estimated posterior density of d in Figure 4.1 based on a
Monte Carlo sample size N = 105 when H0 is true. Note that for this computation
we used 100 g functions corresponding to indictors for subintervals.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
134
0.0
0.1
0.2
0.3
0.4
0
2
4
6
8
10
12
distance
Figure 4.1: Plot of posterior density of d when H0 is true
Then we plot the prior and posterior densities in one plot in Figure 4.2.
0.0
0.5
1.0
1.5
2.0
0
2
4
6
8
10
12
distance
Prior
Posterior
Figure 4.2: Comparison of prior and posterior densities of d when H0 is true

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
135
From this we see that the posterior distribution has become much more concentrated
around 0.
Now let’s consider the case when H0 is false, i.e., the data is generated from a
multivariate normal with Σ∗/∈H1
0. In this case we used Σ∗= Γ2Γ
′
2 + Ψ where
Γ2 =


0.99
0
0.99
0
0.99
0.99
0
0.95
0
0.95
0
0


′
,
and Ψ = diag(0.02, 0.19, 0.02, 0.19, 0.02, 0.02).
Figure 4.3 shows the posterior density of d with n = 100 based on a Monte Carlo
sample size 106 when H0 is false.
0.0
0.5
1.0
1.5
2.0
0
1
2
3
4
5
6
7
distance
prior
posterior
Figure 4.3: Comparison of prior and posterior densities of d when H0 is false
In this case the posterior distribution has concentrated much less about 0 as evident
in the above ﬁgure.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
136
4.3.1.2
Log-Normal Priors on the Scalings
We use the data Y as in Section 4.3.1.2 but use the prior given by δi ∼Log-Normal
(0, 1). We plot the estimated posterior density of d, obtained via the importance
sampling, in Figure 4.4 based on a Monte Carlo size of 105.
0.0
0.1
0.2
0.3
0.4
0
2
4
6
8
10
12
distance
Figure 4.4: Plot of posterior density of d when H0 is true
Now we present the prior and the posterior densities in one plot in Figure 4.5.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
137
0.0
0.5
1.0
1.5
2.0
0
2
4
6
8
10
12
distance
Prior
Posterior
Figure 4.5: Comparison of prior and posterior densities of d when H0 is true
We see that the posterior has concentrated much more tightly around 0.
Again let’s consider the case when H0 is false, i.e., the data is generated from a
multivariate normal (0, Σ∗) with Σ∗/∈H1
0, e.g., Σ∗= Γ2Γ
′
2 + Ψ where
Γ2 =


0.99
0
0.99
0
0.99
0.99
0
0.95
0
0.95
0
0


′
,
and Ψ = diag(0.02, 0.19, 0.02, 0.19, 0.02, 0.02).
Figure 4.6 shows the posterior density histogram of d when n = 100 based on a
Monte Carlo sample size 105 when H0 is false.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
138
0.0
0.5
1.0
1.5
2.0
0
1
2
3
4
5
6
7
distance
prior
posterior
Figure 4.6: Comparison of prior and posterior densities of d when H0 is false
Again we see that the posterior has much less concentration around 0.
4.3.1.3
Computing the ORS
Now we compute the ORS for the example in Section 4.3.1.2 when H0 is true. The
ORS is computed by (3.20) and πq(0|X)/πq(0) is approximated by
πq(d0.01|X)
πq(d0.01)
where d0.01 is the 1st percentile of Πq (Section 3.7.1). In Figure 4.5 we observed
that the posterior distribution of d is more concentrated around 0 than the prior
distribution of d when H1
0 is true. Figure 4.7 is a plot of the relative belief ratios
π1(d|X)/π1(d).

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
139
0.20
0.25
0.30
0.35
0.0
0.5
1.0
1.5
2.0
distance
ratio
Figure 4.7: Plot of relative belief ratio when H0 is true when n = 100
The relative belief ratio at 0 is approximated by 1.96. The ORS is then computed
via (3.20) and it equals
Π(π1(d|X)
π1(d)
> 1.96|X) = 0.
Accordingly we have no evidence against H1
0.
Next we consider the case when H0 is false from Section 4.3.1.2. The relative
belief ratios are plotted in Figure 4.8.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
140
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
distance
ratio
Figure 4.8: Plot of relative belief ratio when H0 is false when n = 100
The relative belief ratio at 0 is approximated by 0 and the ORS is thus approx-
imately equal to 1. Thus we have strong evidence against the null hypothesis.
4.3.2
Two Factor Example
4.3.2.1
Inverse Gamma Priors on the Scalings
Now we consider a two factor model with p = 8, e.g., the data are generated from
an 8-dimensional multivariate normal with a variance-covariance matrix produced
via a 2-factor model, i.e., H2
0 is true. Again we ﬁrst place prior (4.1) on (µ, D, R)
and a data set Y is generated from N8(0, Γ2Γ
′
2 +Ψ) with Γ2 and Ψ taking the same
values as in Section 3.4.4. We present the prior and posterior densities with n = 100
when H0 is true in Figure 4.9.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
141
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
0
5
10
15
20
distance
Prior
Posterior
Figure 4.9: Comparison of prior and posterior densities of d when H0 is true
Then we consider the case when H0 is false, i.e., the data is generated from
N8(0, Σ∗) with Σ∗/∈H2
0, e.g., Σ∗= Γ3Γ
′
3 + Ψ where
Γ3 =


0.99
0
0
0.99
0.99
0
0
0
0
0.95
0
0
0
0.95
0.95
0
0
0
0.9
0
0
0
0
0.9


′
,
and Ψ = diag(0.02, 0.19, 0.36, 0.02, 0.02, 0.19, 0.19, 0.36).
We plot the prior and
posterior density estimates in Figure 4.10 based on a sample size 100.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
142
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
0
1
2
3
4
5
distance
prior
posterior
Figure 4.10: Comparison of prior and posterior densities of d when H0 is false
4.3.2.2
Log-Normal Priors on the Scalings
Now we place the prior δi ∼logN(0, 1), as we did in Section 4.3.1.2 and plot the
prior and posterior estimates of d in Figure 4.11.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
143
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
0
5
10
15
20
distance
Prior
Posterior
Figure 4.11: Comparison of prior and posterior densities of d when H0 is true
Then we consider the case when H0 is false and we plot the prior and posterior
density estimates in Figure 4.12 when n = 100.
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
0
1
2
3
4
5
distance
prior
posterior
Figure 4.12: Comparison of prior and posterior densities of d when H0 is false

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
144
For comparison purpose we also considered the sample size n = 500. We plot
the prior and posterior density histograms with diﬀerent sample sizes. The posterior
should concentrate more and more around the true value d∗̸= 0. Indeed this is what
we see has happened as is illustrated in Figure 4.13.
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
10
12
distance
prior
posterior when n=100
posterior when n=500
Figure 4.13: Comparison of prior and posterior densities of d with diﬀerent sample
sizes when H0 is false
4.3.2.3
Computing the ORS
Now we compute the ORS for the example in Section 4.3.2.2 when H0 is true. The
ORS is computed by (3.20) and πq(0|X)/πq(0) can be approximated by
πq(d0.01|X)
πq(d0.01)

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
145
where d0.01 is the 1st percentile of Πq (Section 3.7.1). In Figure 4.11 we observed
that the posterior distribution of d is more concentrated around 0 than the prior
distribution of d when H1
0 is true. Now we plot the relative belief raio π1(d|X)/π1(d)
in Figure 4.14.
0.18
0.20
0.22
0.24
0.26
0.28
0.30
0.000
0.005
0.010
0.015
distance
ratio
Figure 4.14: Plot of relative belief ratio when H0 is true when n = 100
The relative belief ratio at 0 is approximated by 0.0158. The ORS equals
Π(π2(d|X)
π2(d)
> 0.0158|X) = 0.
Accordingly we have no evidence against H1
0.
Next we consider the case when H0 is false from Section 4.3.2.2 and we start
with a sample size 100. We have observed that though the posterior density has
shifted away from 0 compared with the prior, the computed ORS is 0.936. and we
do not have deﬁnitive results. This arises due to the small sample size relative to
the number of parameters. However, when we increased the sample size to 500, the

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
146
relative belief ratio at 0 is approximately equal to 0 and the ORS is approximately
equal to 1. Thus we obtain strong evidence against the null hypothesis.
4.3.3
Further Comments
Though we placed diﬀerent priors on δi, the posterior distributions only diﬀer sightly.
As such the posterior density estimate seems somewhat robust to diﬀerent choices
of priors.
To assess the accuracy of our importance sampling, we estimate the coeﬃcient
of variation of the importance sampling estimator of the normalizing constant for
the posterior. In this case g(J ) = k(J ) and
w∗i =
k(Ji)
PN
j=1 k(Ji)
.
In Table 4.1 we present the values of PN
i=1 w2
∗i for each of the computations
carried out in this chapter. A value of PN
i=1 w2
∗i close to 1/N indicates that the im-
portance sampling is working while a value near 1 indicates a failure. The coeﬃcient
of variation is given by
1
N (N PN
i=1 w2
∗i −1), see Evans and Swartz [13]. The results
illustrate that the importance sampling is working very well.

CHAPTER 4.
IMPORTANCE SAMPLING FOR FACTOR ANALYSIS
147
Table 4.1: Estimation of Standard Errors of Diﬀerent Importance Estimators
PN
i=1 w2
∗i
CV
One Factor model
H0 true
Inverse Gamma
1.58 × 10−5
5.80 × 10−6
Log-Normal
3.59 × 10−5
2.59 × 10−5
H0 false
Inverse Gamma
1.74 × 10−5
7.434 × 10−6
Log-Normal
5.34 × 10−5
4.34 × 10−5
Two Factor model
H0 true
Inverse Gamma
3.48 × 10−5
2.48 × 10−5
Log-Normal
1.55 × 10−4
1.45 × 10−4
H0 false
Inverse Gamma
3.95 × 10−5
2.95 × 10−5
Log-Normal (n = 100)
1.96 × 10−4
1.86 × 10−4
Log-Normal (n = 500)
1.78 × 10−5
7.77 × 10−6

Chapter 5
Example
5.1
International Currency Exchange
The ﬁrst data set we consider is monthly international exchanges rates (n = 144)
studied in West and Harrison (1997). These time series are the exchange rates in
British pounds of the following p = 6 currencies: US dollar (US), Canadian dollar
(CAN), Japanese yen (JAP), French franc (FRA), Italian lira (ITA) and the (West)
German (Deutsch) mark (GER). The data span the period from January of 1975
to December of 1986 inclusive. Table 5.1 provides a partial data set from West and
Harrison [39].
In Lopes and West [28], they introduced and developed a reversible jump MCMC
method that treats the number of factors as a parameter and builds on prior work
on MCMC methods given the number of factors. The algorithm moves between
diﬀerent parameter spaces.
They also discussed a range of alternative methods
based on bridge sampling (see Section 2.6.1.3) as proposed by Meng and Wong [30].
They compared a range of methods for computing marginal data densities and Bayes
factors. In West and Harrison [30], it was determined that up to three principal
148

CHAPTER 5.
EXAMPLE
149
Table 5.1: International Exchange in Six Currencies
Year
Month
US
CAD
Yen
Franc
Lira
Mark
1975
1
2.36
2.35
707.8
10.31
1522
5.58
2
2.39
2.40
698.3
10.24
1527
5.57
3
2.42
2.42
695.1
10.15
1526
5.60
4
2.37
2.40
692.4
9.95
1502
5.63
5
2.32
2.39
676.3
9.41
1456
5.45
6
2.28
2.28
668.9
9.13
1426
5.34
7
2.19
2.19
647.5
9.23
1419
5.39
8
2.12
2.12
630.1
9.25
1413
5.44
9
2.08
2.09
624.8
9.13
1413
5.45
10
2.06
2.06
621.9
9.06
1395
5.31
11
2.05
2.05
620.2
9.03
1391
5.30
12
2.02
2.02
618.0
9.01
1381
5.30
1976
1
2.03
2.04
617.8
9.08
1424
5.28
2
2.03
2.01
611.0
9.06
1554
5.19
3
1.94
1.92
584.2
8.99
1605
4.98
4
1.85
1.82
552.3
8.62
1622
4.69
5
1.81
1.77
540.5
8.50
1549
4.63
6
1.76
1.72
527.9
8.36
1498
4.55
7
1.79
1.74
526.4
8.64
1495
4.60
8
1.78
1.76
518.6
8.86
1493
4.51
9
1.73
1.69
497.1
8.51
1460
4.31
10
1.64
1.60
477.1
8.16
1401
3.98
11
1.64
1.61
483.0
8.16
1416
3.95
12
1.68
1.71
494.3
8.37
1455
4.00
1977
1
1.71
1.73
498.3
8.52
1506
4.10
2
1.71
1.76
487.2
8.52
1509
4.11
3
1.72
1.81
481.7
8.55
1523
4.11
4
1.72
1.81
473.5
8.54
1525
4.08
5
1.72
1.80
476.7
8.51
1523
4.05
6
1.72
1.82
468.6
8.49
1522
4.05
7
1.72
1.83
456.1
8.36
1520
3.93
8
1.74
1.87
463.8
8.52
1535
4.03
9
1.74
1.87
465.2
8.58
1540
4.05
10
1.77
1.95
450.9
8.60
1559
4.03
11
1.82
2.02
444.8
8.81
1596
4.07
12
1.85
2.04
446.6
8.88
1623
3.99
components are needed by using various principal component analysis. We did a
principal factor analysis by proc factor of SAS and this indicates that three factors

CHAPTER 5.
EXAMPLE
150
are relevant. Although the dimension has been reduced to half as before, it oﬀers
no simpliﬁcation of the original assumption that Var(Y) = Σ based on the degree
of freedom argument. In other words, the factor model contains as many parameter
as Σ, thus the factor model provides no simpler explanation for the co-dependence
structure of Y than the full covariance matrix. The maximum likelihood approach
simply failed, which is analogous to the problem investigated in Chapter 1.
The analyses in Lopes and West [28] are based on very diﬀuse but proper pri-
ors. They chose a multivariate normal prior for the oﬀ-diagonal elements of factor
loadings and a normal prior to positive values on the diagonal elements centered at
0. A large scaling parameter is speciﬁed for the factor loadings. They assume inde-
pendent inverse gamma priors for the unique variances with a low value for shape
parameter to ensure a diﬀuse prior. All Bayesian methods in factor analysis try
to eschew using improper priors, e.g., Jeﬀery’s prior, as they lead to the Bayesian
analogue of the so called Heywood case. They concluded that that two factors are
needed to explain the co-dependence structure.
Our analyses are primarily based on the prior speciﬁed in Section 2.6.2.2 for
two reasons. First, the prior for Σ based on the correlation factorization is fully
interpretable in a factor analysis context as we essentially examine the correlation
among the variables. Second, the factorization greatly simpliﬁes the prior elicitation
process.
Our analysis ﬁrst assesses the number of factors and provides estimates for
the factor loadings and unique variances.
Our method requires only one prior
for location parameters and one prior for scaling parameters.
A jointly uniform
prior is speciﬁed for the correlation matrix. We chose a multivariate normal for
location parameters centered at their sample means, namely, N6(µ0, τ0Σ) where
µ0 = (1.8, 2.1, 430.6, 10.1, 1984, 12.4)
′ and τ0 = 100 to reﬂect ignorance about the

CHAPTER 5.
EXAMPLE
151
locations parameters. We note that, in Lopes and West [28] and all other Bayesian
factor analyses, people simply set µ = ¯x.
For the scaling parameters, we use independent inverse gammas with a small
value for shape hyperparameter so that the prior is diﬀuse but proper. For example,
R ∼Uniform
σii ∼Inverse Gamma(2.2, 0.1)
was used in our computations. These choices are those made in Lopes and West [28]
and are made here for comparison purposes.
By Theorem 4.2.1 and Corollary 4.2.1, the posterior density of Σ−1 = J is
proportional to a Wishart density times
k(J ) =
p
Y
i=1
(σii)−5.7 exp(−0.1/σii)
Our approach is via sequential testing. For this we test the hypothesis q = 0
against q > 0, i.e., H0
0 = {Σ : Σ = Γ0Γ
′
0 + Ψ}. We present the posterior and prior
density comparison in Figure 5.1.

CHAPTER 5.
EXAMPLE
152
0
1
2
3
4
5
6
7
0.0
0.2
0.4
0.6
0.8
1.0
1.2
distance
prior
posterior
Figure 5.1: Comparison of prior and posterior densities of d
The plot shows that posterior leads to much less concentration around 0 than the
prior. The computed ORS equals 1. Thus we have strong evidence against the null
hypothesis and conclude that q > 0, i.e., it is not an independence model.
We move on with assessing q = 1 against q > 1, i.e., H1
0 = {Σ : Σ = Γ1Γ
′
1 + Ψ}.
The posterior and prior density comparison is presented in Figure 5.2.

CHAPTER 5.
EXAMPLE
153
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
distance
prior
posterior
Figure 5.2: Comparison of prior and posterior densities of d
The plot again shows that posterior leads to much less concentration around 0 than
the prior. The computed ORS equals 0.9999. Thus we have strong evidence against
the null hypothesis and conclude that q > 1, i.e., a model with more than 1 factor
is needed.
Thus we continue with q = 2 and plot the prior and posterior densities in Figure
5.3.

CHAPTER 5.
EXAMPLE
154
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
10
20
30
40
distance
prior
posterior
Figure 5.3: Comparison of prior and posterior densities of d
Now the plot shows that posterior concentrates much more around 0 than the prior
does. The relative belief ratio at 0 is approximated by π(dα|X)/π(dα) = 17.2 where
dα = 0.018 with α = 0.01. The ORS equals 0.533, thus we do not have evidence
against the null hypothesis. We conclude that a 2-factor model is reasonable, which
agrees with Lopes and West [28].
It is also of interest to make inference about R when Hq
0 is true. In this example,
we proceed with q = 2. The inference procedure is described in Section 3.8 where
an approximation rather than an analytical solution is presented. By applying the
minimization algorithm for p = 6 and q = 2 with 103 starting values from B2(R),

CHAPTER 5.
EXAMPLE
155
we obtain the minimizing Γ2, namely,
bΓ2 =


−1
0
−0.823451
0.3754051
−0.726837
−0.275134
0.4542326
0.7446418
0.508424
0.8611069
−0.145522
0.1754735


and Ψ is estimated by
bΨ =


0
0
0
0
0
0
0
0.1809999
0
0
0
0
0
0
0.3960089
0
0
0
0
0
0
0.2391814
0
0
0
0
0
0
0
0
0
0
0
0
0
0.9480323


.
And ﬁnally the estimate of R is
ebR =


1
0.8234507
0.7268371
−0.454233
−0.508424
0.1455222
0.8234507
1
0.4952277
−0.094496
−0.095398
0.185704
0.7268371
0.4952277
1
−0.53503
−0.606462
0.0574921
−0.454233
−0.094496
−0.53503
1
0.8721589
0.064564
−0.508424
−0.095398
−0.606462
0.8721589
1
0.0771145
0.1455222
0.185704
0.0574921
0.064564
0.0771145
1


.

CHAPTER 5.
EXAMPLE
156
5.2
Customer Satisfaction Data
In the second example, we apply our method to customer satisfaction data (Rossi,
Gilula, Allenby [35]). The data set contains responses to a satisfaction survey for a
Yellow Pages advertised product regarding satisfaction concerning various aspects
of the advertisement (Table 5.2). The responses are based on a 10 point rating scale
and they are treated as continuous. The number of observations is n = 1811 and
the dimension is p = 10. Table 5.3 provides the means/variances and correlations
for the survey data.
Table 5.2: Variable Descriptions of Customer Satisfaction Data
Variable
Customer Satisfaction
1
Overall satisfaction
2
Setting competitive prices
3
Holding price increase to a minimum
4
Appropriate pricing given volume
5
Demonstrating eﬀectiveness of purchase
6
Reach a large number of customers
7
Reach of advertising
8
Long-term exposure
9
Distribution
10
Distribution to right geographic areas
Our analysis ﬁrst assesses the number of factors and then provides estimates
for the factor loadings and unique variances. Our method requires only one prior
for location parameters and one prior for scaling parameters.
A jointly uniform
prior is chosen for the correlation matrix.
We chose a multivariate normal for
location parameters centered at their sample means, namely, N10(µ0, τ0Σ) where
µ0 = (6.06, 5.88, 6.27, 5.55, 6.13, 6.05, 7.25, 7.46, 7.89, 7.77)
′ and τ0 = 100 to reﬂect

CHAPTER 5.
EXAMPLE
157
Table 5.3: Means/Variances and Correlation of Customer Satisfaction Data
mean
1
2
3
4
5
6
7
8
9
10
1
6.06
6.50
0.65
0.62
0.78
0.65
0.74
0.59
0.56
0.44
0.45
2
5.88
7.00
0.77
0.76
0.55
0.49
0.42
0.43
0.35
0.35
3
6.27
7.06
0.72
0.52
0.46
0.43
0.46
0.38
0.40
4
5.55
7.37
0.64
0.67
0.52
0.52
0.41
0.40
5
6.13
6.84
0.69
0.58
0.59
0.49
0.46
6
6.05
6.49
0.59
0.59
0.45
0.44
7
7.25
5.85
0.65
0.62
0.60
8
7.46
5.21
0.62
0.62
9
7.89
4.57
0.75
10
7.77
4.89
ignorance about the locations parameters.
For the scaling parameters, we use independent log-normals with the medians
being around
√
6.2 and 6.2 is the average of the sample variances. The prior that
we chose for this application is thus
µ ∼N10(µ0, 100Σ)
R ∼Uniform
σii ∼Log-normal(µi, σi)
where µ0 = (6.06, 5.88, 6.27, 5.55, 6.13, 6.05, 7.25, 7.46, 7.89, 7.77)
′, µi = ln
√
6.2 ≈1
and σi = 1.
By Theorem 4.2.1 and Corollary 4.2.1, the posterior density of Σ−1 = J is
proportional to a Wishart density times
k(J ) =
p
Y
i=1
(σii)−6.5 exp[−(log σii −1)2/2].

CHAPTER 5.
EXAMPLE
158
We start with testing for an independence model. The following is the plot of
the prior and posterior densities.
0
5
10
15
0.0
0.1
0.2
0.3
0.4
0.5
distance
prior
posterior
Figure 5.4: Comparison of prior and posterior densities of d
The posterior concentrates much less around 0 than the prior. Then the relative
belief ratio π(0|X)/π(0) is approximated by 0 and the ORS is equal to 1. Thus
we have strong evidence against the null hypothesis, i.e., it is not an independence
model.
We continue with testing whether q = 1. The plot of prior and posterior com-
parison is provided in Figure 5.5.

CHAPTER 5.
EXAMPLE
159
0
1
2
3
4
0
1
2
3
4
5
6
7
distance
prior
posterior
Figure 5.5: Comparison of prior and posterior densities of d
The posterior concentrates much more around 0 than the prior. The relative belief
ratio π(0|X)/π(0) is approximated by 0 and the ORS is equal to 0. Thus we have
no evidence against the null hypothesis, i.e., a 1-factor model is not contradicted by
the data.
In Ando [2], it is assumed that this data was generated from a multivariate t-
distribution and so was the latent factors. They gave a closed form for the posterior
distribution by evaluating the marginal likelihood analytically. The number of fac-
tors q is optimized by maximizing the posterior model probability. They concluded
that a 3-factor model explains the data and compared the results with traditional
approaches, e.g., AIC and BIC criteria, which select even more complex models.
Note that the correlations are very high. With such a high dimension, this indicates
an enormous redundancy among the variables. Our ﬁndings provide a great reduc-

CHAPTER 5.
EXAMPLE
160
tion in the complexity of the data structure, i.e., the data is eﬀectively arises from
1-dimensional structure, instead of 10.
It is also of interest to make inference about R when Hq
0 is true. In this example,
we proceed with q = 1. The inference procedure is described in Section 3.8 where
an approximation rather than an analytical solution is presented. By applying the
minimization algorithm for p = 10 and q = 1 with 103 starting values from B1(R),
we obtain the minimizing Γ1, namely,
bΓ1 =


−0.839556
−0.69752
−0.70663
−0.806131
−0.779599
−0.773248
−0.745421
−0.764139
−0.670724
−0.640161



CHAPTER 5.
EXAMPLE
161
and then Ψ is estimated by
bΨ =


0.295
0
0
0
0
0
0
0
0
0
0
0.513
0
0
0
0
0
0
0
0
0
0
0.501
0
0
0
0
0
0
0
0
0
0
0.350
0
0
0
0
0
0
0
0
0
0
0.392
0
0
0
0
0
0
0
0
0
0
0.402
0
0
0
0
0
0
0
0
0
0
0.444
0
0
0
0
0
0
0
0
0
0
0.416
0
0
0
0
0
0
0
0
0
0
0.550
0
0
0
0
0
0
0
0
0
0
0.590


.
The estimate of R is then
ebR =


1
0.585
0.592
0.677
0.655
0.649
0.626
0.642
0.563
0.537
0.586
1
0.493
0.562
0.544
0.539
0.520
0.533
0.468
0.447
0.593
0.493
1
0.570
0.551
0.546
0.527
0.540
0.474
0.452
0.677
0.562
0.570
1
0.628
0.623
0.601
0.616
0.541
0.516
0.655
0.544
0.551
0.628
1
0.603
0.581
0.596
0.523
0.499
0.649
0.539
0.546
0.623
0.603
1
0.576
0.591
0.519
0.495
0.626
0.520
0.527
0.601
0.581
0.576
1
0.570
0.500
0.477
0.642
0.533
0.540
0.616
0.596
0.591
0.570
1
0.513
0.489
0.563
0.468
0.474
0.541
0.523
0.519
0.500
0.513
1
0.429
0.537
0.447
0.452
0.516
0.499
0.495
0.477
0.489
0.429
1


.

Bibliography
[1] Anderson, T. W. and Rubin, H. (1955), Statistical analysis in factor analysis.
Stanford University California Applied Mathematics and Statistics Labs. 25
May 1955. 84p. Report: TR30.
[2] Ando, T. (2009), Bayesian factor analysis with fat-tailed factors and its exact
marginal likelihood. Journal of Multivariate Analysis, 100, 1717-1726.
[3] Bartlett, M. S. (1950), Test of signiﬁcance in factor analysis. Brit. J. Psychol.,
Statist. Sect., 1950, 77-85
[4] Bernard, J., McCulloch, R. and Meng, X. L. (2000), Modeling covariance
matrices in terms of standard deviations and correlations with application to
shrinkage. Statistica Sinica 10, 1281-1311.
[5] Box, G. E. P. and Tiao, G. C. (1973), Bayesian inference in statistical analysis.
Addison-Wesley, Reading, MA.
[6] Evans, M. (1997), Bayesian Inference procedures derived via the concept of
relative surprise. Communications in Statistics-Theory and Methods, Vol. 26,
No. 5, 1125-1143.
162

BIBLIOGRAPHY
163
[7] Evans, M., Guttman, I. and Swartz, T. (2006), Optimality and computations
for relative surprise inferences. Canadian Journal of Statistics, Vol. 34, No. 1,
113-129.
[8] Evans, M., Gilula, Z. and Guttman, I. (1993), Computational issues in the
Bayesian analysis of categorial data: Log-linear and Goodman’s RC model.
Statistica Sinica, 3, 391-406.
[9] Evans, M., Gilula, Z., Guttman, I. and Swartz, T.B. (1997), Bayesian analysis
of stochastically ordered distributions of categorial variables. Journal of the
American Statistical Association, 92, 208-214.
[10] Evans, M., Moshonov, H. (2006), Checking for prior-data conﬂict. Bayesian
Analysis, Vol. 1, No. 4, 893-914.
[11] Evans, M., Moshonov, H. (2007), Checking for prior-data conﬂict with hier-
archically speciﬁed priors, Bayesian Statistics and its Applications, eds. A.K.
Upadhyay, U. Singh, D. Dey, Anamaya Publishers, New Delhi, 145-159.
[12] Evans, M., Shakhatreh, M. (2008), Optimal properties of some Bayesian in-
ferences. Electronic Journal of Statistics, Vol. 2, 1268-1280.
[13] Evans, M. and Swartz, T. (2000), Approximating Integrals via Monte Carlo
and Deterministic Methods. Oxford University Press.
[14] Garthwaite, P. H., Kadane, J. B. and O’Hagan, A. (2005), Statistical meth-
ods for eliciting probability distributions, Journal of the American Statistical
Association, Vol. 100, 680-701.
[15] Hotelling, H. (1933), Analysis of a complex of variables into principal compo-
nents. Journal of Educational Psychology, 24, 417-441, 498-520.

BIBLIOGRAPHY
164
[16] Gelman, A. E. and Meng, X. L. (1998), Computing normalizing constants:
from Importance sampling to bridge sampling to path sampling, Statistical
Science, 13, 163-185
[17] Harman, H. H. (1976). Modern Factor Analysis (3rd edition), University of
Chicago Press: Chicago
[18] Jeﬀreys, H. (1961), Theory of Probability. Oxford University Press.
[19] Gelman, A. and Meng, X. L. (1998), Simulating normalizing constants: from
importance sampling to bridge sampling to path sampling. Statistical Science,
13, 163-185.
[20] Ghosh, S and Henderson, S. G. (2003), Behavior of the NORTA method for
correlated random vector generation as the dimension increases. ACM Trans-
actions on Modeling and Computer Simulation (TOMACS), Vol. 13, 276-294.
[21] Green, P. J. (1995), Reversible jump Markov chain Monte Carlo computation
and Bayesian model determination, Biometrika 82(4): 711-732.
[22] Kadane, J. B., Dickey, J., Winkler, R., Smith, W. and Peters, S. (1980),
Interactive elicitation of opinion for a normal linear model, Journal of the
American Statistical Association, Vol. 75, 845-854.
[23] Kendall, M. G. (1950), Factor Analysis, Journal of the Royal Statistical Soci-
ety, 12, 60-73.
[24] Lawley, D. N. (1940), The estimation of factor loadings by the method of
maximum likelihood. Proceedings of The Royal Society of Edinburgh, 60, 64-
82.

BIBLIOGRAPHY
165
[25] Lawley, D. N. (1941), Further investigation in factor estimation. Proceedings
of The Royal Society of Edinburgh, Section A 61, 176-185.
[26] Lattin, J., Carroll, D. and Green, P. (2002), Analyzing Multivariate Data,
Thomson Brooks/Cole, CA.
[27] Lee, S. Y. (2007), Structural Equation Modeling: a Bayesian Approach, John
Wiley&Sons, Chichester, UK.
[28] Lopes, H. F. and West, M. (2004), Bayesian model assessment in factor anal-
ysis. Statistica Sinica, 14, 41-67.
[29] Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979), Multivariate Analysis,
Academic Press, London.
[30] Meng and Wong (1996), Simulating ratios of normalizing constants via a sim-
ple Identity: a theoretical exploration, Statistica Sinica, 6, 831-860
[31] Neuhaus, J. O. and Wrigley, C. F. (1941), Further investigations in factor
estimation. Proceedings of The Royal Society of Edinburgh, 61, 176-185.
[32] Press, S. J. (1985), Applied Multivariate Analysis: Using Bayesian and Fre-
quentist Methods of Inference, Melbourne, FL: Krieger.
[33] Press, S. J., and Shigemasu, K. (1989), Bayesian Inference in factor analysis, in
Contributions to Probability and Statistics: Essays in Honor of Ingram Olkin,
eds. L. J. Gleser, M. D. Perlman, S. J. Press, and A. R. Sampson, New York:
Springer-Verlag, 271-287.
[34] Rao, C. R. (1955), Estimation and tests of signiﬁcance in factor analysis,
Psychometrika, Vol. 20, No. 2, 93-111.

BIBLIOGRAPHY
166
[35] Rossi, P. E. , Gilula, Z. and Allenby, G. M. (2001), Overcoming scale us-
age heterogeneity: a Bayesian hierarchical approach. Journal of the American
Statistical Association, 6(453), 20-31.
[36] Spearman, C. (1904), General intelligence, objectively determined and mea-
sured,” American Journal of Psychology, 15, 201-293.
[37] Thurstone, L. L. (1938), A new rotational method in factor analysis. Psy-
chometrika, 3, 199-218.
[38] Thurstone, L. L. (1935). The Veclors of Mind, University of Chicago Press:
Chicago
[39] West, M. and Harrison, J. (1997), Bayesian Forecasting and Dynamic Models
(2nd edition). Springer Series in Statistics.
[40] Winkler, R. L. (1972) Introduction to Bayesian Inference and Decision, Holt,
Rinehart&Winston, New York.

