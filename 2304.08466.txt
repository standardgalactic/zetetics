Synthetic Data from Diffusion Models
Improves ImageNet Classiﬁcation
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia*, Mohammad Norouzi
*, David J. Fleet
Google Research, Brain Team †
Abstract
Deep generative models are becoming increasingly pow-
erful, now generating diverse high ﬁdelity photo-realistic
samples given text prompts. Have they reached the point
where models of natural images can be used for gener-
ative data augmentation, helping to improve challenging
discriminative tasks?
We show that large-scale text-to-
image diffusion models can be ﬁne-tuned to produce class-
conditional models with SOTA FID (1.76 at 256×256 reso-
lution) and Inception Score (239 at 256 × 256). The model
also yields a new SOTA in Classiﬁcation Accuracy Scores
(64.96 for 256×256 generative samples, improving to 69.24
for 1024×1024 samples). Augmenting the ImageNet train-
ing set with samples from the resulting models yields sig-
niﬁcant improvements in ImageNet classiﬁcation accuracy
over strong ResNet and Vision Transformer baselines.
1. Introduction
Deep generative models are becoming increasingly ma-
ture to the point that they can generate high ﬁdelity photo-
realistic samples [12, 25,59]. Most recently, denoising dif-
fusion probabilistic models (DDPMs) [25,59] have emerged
as a new category of generative techniques that are capable
of generating images comparable to generative adversar-
ial networks (GANs) in quality while introducing greater
stability during training [12, 26].
This has been shown
both for class-conditional generative models on classiﬁca-
tion datasets, and for open vocabulary text-to-image gener-
ation [40,44,47,50].
It is therefore natural to ask whether current models are
powerful enough to generate natural image data that are ef-
fective for challenging discriminative tasks; i.e., generative
data augmentation. Speciﬁcally, are diffusion models capa-
ble of producing image samples of sufﬁcient quality and di-
versity to improve performance on well-studied benchmark
tasks like ImageNet classiﬁcation? Such tasks set a high
bar, since existing architectures, augmentation strategies,
and training recipes have been heavily tuned. A closely
*Work done at Google Research.
†{shekazizi, skornblith, davidﬂeet}@google.com
 
40
45
50
55
60
65
70
75
Classification Accuracy Score
Real Accuracy: 73.59
BigGAN
42.65
VQ-VAE-2
54.83
  CDM
63.02
Ours-256x
64.96
Ours-1024x
69.24
22
36
45
64
87
307
Parameters (M)
76
78
80
82
84
Top-1 Accuracy (%)
+1.52
DeiT-S
+1.78
R-50
+1.59
R-101
+1.56
R-152
+1.05
DeiT-B
+0.83
DeiT-L
Real
Real + Generated
Figure 1.
Top: Classiﬁcation Accuracy Scores [45] show that
models trained on generated data are approaching those trained on
real data. Bottom: Augmenting real training data with generated
images from our ImageNet model boosts classiﬁcation accuracy
for ResNet and Transformer models.
related question is, to what extent large-scale text-to-image
models can serve as good representation learners or founda-
tion models for downstream tasks? We explore this issue in
the context of generative data augmentation, showing that
these models can be ﬁne-tuned to produce state-of-the-art
class-conditional generative models on ImageNet.
To this end, we demonstrate three key ﬁndings. First,
we show that an Imagen model ﬁne-tuned on ImageNet
training data produces state-of-the-art class-conditional Im-
ageNet models at multiple resolutions, according to their
Fr´echet Inception Distance (FID) [22] and Inception Score
(IS) [52]; e.g,, we obtain an FID of 1.76 and IS of 239 on
256 × 256 image samples. These models outperform ex-
arXiv:2304.08466v1  [cs.CV]  17 Apr 2023

harvestman
Leonberger
Schipperke
bittern bird
brussels griffon
Figure 2. Example 1024×1024 images from the ﬁne-tuned Imagen (left) model vs. vanilla Imagen (right). Fine-tuning and careful choice
of guidance weights and other sampling parameters help to improve the alignment of images with class labels and sample diversity. More
samples are provide in the Appendix.
isting state-of-the-art models, with or without the use of
guidance to improve model sampling. We further establish
that data from such ﬁne-tuned class-conditional models also
provide new state-of-the-art Classiﬁcation Accuracy Scores
(CAS) [45], computed by training ResNet-50 models on
synthetic data and then evaluating them on the real Ima-
geNet validation set (Fig. 1 - top). Finally, we show that
performance of models trained on generative data further
improves by combining synthetic data with real data, with
larger amounts of synthetic data, and with longer training
times. These results hold across a host of convolutional and
Transformer-based architectures (Fig. 1 - bottom).
2. Related Work
Synthetic Data. The use of synthetic data has been widely
explored for generating large amounts of labeled data for
vision tasks that require extensive annotation. Examples
include tasks like semantic image segmentation [3, 10, 36,
37,54,66], optical ﬂow estimation [14,32,63], human mo-
tion understanding [19, 29, 38, 67], and other dense predic-
tion tasks [3,70]. Previous work has explored 3D-rendered
datasets [18, 73] and simulation environments with phys-
ically realistic engines [11, 15, 16]. Unlike methods that
use model-based rendering, here we focus on the use of
data-driven generative models of natural images, for which
GANs have remained the predominant approach to date
[6, 17, 36]. Very recent work has also explored the use of
publicly available text-to-image diffusion models to gener-
ate synthetic data. We discuss this work further below.
Distillation and Transfer. In our work, we use a diffu-
sion model that has been pretrained on a large multimodal
dataset and ﬁne-tuned on ImageNet to provide synthetic
data for a classiﬁcation model.
This setup has connec-
tions to previous work that has directly trained classiﬁcation
models on large-scale datasets and then ﬁne-tuned them on
ImageNet [34,39,43,62,72]. It is also related to knowledge
distillation [7, 23] in that we transfer knowledge from the
diffusion model to the classiﬁer, although it differs from the
traditional distillation setup in that we transfer this knowl-
edge through generated data rather than labels. Our goal in
this work is to show the viability of this kind of generative
knowledge transfer with modern diffusion models.

Diffusion Model Applications.
Diffusion models have
been successfully applied to image generation [25–27],
speech generation [8,35], and video generation [24,58,68],
and have found applications in various image processing ar-
eas, including image colorization, super-resolution, inpaint-
ing, and semantic editing [49,51,61,69]. One notable appli-
cation of diffusion models is large-scale text-to-image gen-
eration. Several text-to-image models including Stable Dif-
fusion [47], DALL-E 2 [44], Imagen [50], eDiff [1], and
GLIDE [40] have produced evocative high-resolution im-
ages. However, the use of large-scale diffusion models to
support downstream tasks is still in its infancy.
Very recently, large-scale text-to-image models have
been used to augment training data. He et al. [21] show
that synthetic data generated with GLIDE [40] improves
zero-shot and few-shot image classiﬁcation performance.
They further demonstrate that a synthetic dataset generated
by ﬁne-tuning GLIDE on CIFAR-100 images can provide
a substantial boost to CIFAR-100 classiﬁcation accuracy.
Trabucco et al. [65] explore strategies to augment individ-
ual images using a pretrained diffusion model, demonstrat-
ing improvements in few-shot settings. Most closely related
to our work, two recent papers train ImageNet classiﬁers on
images generated by diffusion models, although they ex-
plore only the pretrained Stable Diffusion model and do
not ﬁne-tune it [2, 56]. They ﬁnd that images generated
in this fashion do not improve accuracy on the clean Im-
ageNet validation set. Here, we show that the Imagen text-
to-image model can be ﬁne-tuned for class-conditional Im-
ageNet, yielding SOTA models.
3. Background
Diffusion. Diffusion models work by gradually destroy-
ing the data through the successive addition of Gaussian
noise, and then learning to recover the data by reversing
this noising process [25, 59]. In broad terms, in a forward
process random noise is slowly added to the data as time
t increases from 0 to T. A learned reverse process inverts
the forward process, gradually reﬁning a sample of noise
into an image.
To this end, samples at the current time
step, xt−1 are drawn from a learned Gaussian distribution
N(xt−1; µθ(xt, t), Σθ(xt, t)) where the mean of the distri-
bution µθ(xt, t), is conditioned on the sample at the pre-
vious time step. The variance of the distribution Σθ(xt, t)
follows a ﬁxed schedule. In conditional diffusion models,
the reverse process is associated with a conditioning signal,
such as a class label in class-conditional models [26].
Diffusion models have been the subject of many recent
papers including important innovations in architectures and
training (e.g., [1, 26, 41, 50]). Important below, [26] pro-
pose cascades of diffusion models at increasing image res-
olutions for high resolution images. Other work has ex-
plored the importance of the generative sampling process,
introducing new noise schedules, guidance mechanisms to
trade-off diversity with image quality, distillation for efﬁ-
ciency, and different parameterizations of the denoising ob-
jective (e.g., [28,31,50,53]).
Classiﬁcation Accuracy Score. It is a standard practice to
use FID [22] and Inception Score [52] to evaluate the visual
quality of generative models. Due to their relatively low
computation cost, these metrics are widely used as proxies
for generative model training and tuning. However, both
methods tend to penalize non-GAN models harshly, and In-
ception Score produces overly optimistic scores in methods
with sampling modiﬁcations [27, 45]. More importantly,
Ravuri and Vinyals [45] argued that these metrics do not
show a consistent correlation with metrics that assess per-
formance on downstream tasks like classiﬁcation accuracy.
An alternative way to evaluate the quality of samples
from generative models is to examine the performance of a
classiﬁer that is trained on generated data and evaluated on
real data [55,71]. To this end, Ravuri and Vinyals [45] pro-
pose classiﬁcation accuracy score (CAS), which measures
classiﬁcation performance on the ImageNet validation set
for ResNet-50 models [20] trained on generated data. It is
an intriguing proxy, as it requires generative models to pro-
duce high ﬁdelity images across a broad range of categories,
competing directly with models trained on real data.
To date, CAS performance has not been particularly
compelling. Models trained exclusively on generated sam-
ples underperform those trained on real data. Moreover,
performance drops when even relatively small amounts of
synthetic data are added to real data during training [45].
This performance drop may arise from issues with the qual-
ity of generated sample, the diversity (e.g., due to mode col-
lapse in GAN models), or both. Cascaded diffusion mod-
els [26] have recently been shown to outperform BigGAN-
deep [6] VQ-VAE-2 [46] on CAS (and other metrics). That
said, there remains a sizeable gap in ImageNet test per-
formance between models trained on real data and those
trained on synthetic data [26]. Here, we explore the use
of diffusion models in greater depth, with much stronger
results, demonstrating the advantage of large-scale models
and ﬁne-tuning.
4. Generative Model Training and Sampling
In what follows we address two main questions: whether
large-scale text-to-image models can be ﬁne-tuned as class-
conditional ImageNet models, and to what extent such mod-
els are useful for generative data augmentation. For this pur-
pose, we undertake a series of experiments to construct and
evaluate such models, focused primarily on data sampling
for use in training ImageNet classiﬁers. ImageNet classiﬁ-
cation accuracy is a high bar as a domain for generative data
augmentation as the task is widely studied, and existing ar-
chitectures and training recipes are very well-honed.

1.0
1.25
1.5
1.75
2.0
5.0
Guidance Weight
6
8
10
12
14
16
18
FID@50K
logvar coeff = 1.0
logvar coeff = 0.2
logvar coeff = 0.3
logvar coeff = 0.4
logvar coeff = 0.0
50
100
150
200
Inception Score
5
10
15
20
FID@1.2M
1.0
1.25
1.5
1.75
2.0
5.0
1.0
1.25
1.5
1.75
2.0
5.0
Guidance Weight
10
20
30
40
50
60
Classification Accuracy Score (%)
Top-5 Accuracy (%)
Top-1 Accuracy (%)
Figure 3. Sampling reﬁnement for 64×64 base model. Left: Validation set FID vs. guidance weights for different values of log-variance.
Center: Pareto frontiers for training set FID and IS at different values of the guidance weight. Right: Dependence of CAS on guidance
weight.
The ImageNet ILSVRC 2012 dataset [48] (ImageNet-
1K) comprises 1.28 million labeled training images and
50K validation images spanning 1000 categories. We adopt
ImageNet-1K as our benchmark to assess the efﬁcacy of the
generated data, as this is one of the most widely and thor-
oughly studied benchmarks for which there is an extensive
literature on architectures and training procedures, making
it challenging to improve performance. Since the images
of ImageNet-1K dataset vary in dimensions and resolution
with the average image resolution of 469×387 [48], we ex-
amine synthetic data generation at different resolutions, in-
cluding 64×64, 256×256, and 1024×1024.
In contrast to previous work that trains diffusion mod-
els directly on ImageNet data (e.g., [12, 26, 28]), here we
leverage a large-scale text-to-image diffusion model [50] as
a foundation, in part to explore the potential beneﬁts of pre-
training on a larger, generic corpus. A key challenge in do-
ing so concerns the alignment of the text-to-image model
with ImageNet classes. If, for example, one naively uses
short text descriptors like those produced for CLIP by [43]
as text prompts for each ImageNet class, the data generated
by the Imagen models is easily shown to produce very poor
ImageNet classiﬁer. One problem is that a given text label
may be associated with multiple visual concepts in the wild,
or visual concepts that differ systematically from ImageNet
(see Figure 2). This poor performance may also be a con-
sequence of the high guidance weights used by Imagen, ef-
fectively sacriﬁcing generative diversity for sample quality.
While there are several ways in which one might re-purpose
a text-to-image model as a class-conditional model, e.g., op-
timizing prompts in order to minimize the distribution shift,
here we ﬁx the prompts to be the one or two words class
names from [43], and ﬁne-tune the weights and sampling
parameters of the diffusion-based generative model.
4.1. Imagen Fine-tuning
We leverage the large-scale Imagen text-to-image model
described in detail in [50] as the backbone text-to-image
generator that we ﬁne-tune using the ImageNet training set.
It includes a pretrained text encoder that maps text to con-
textualized embeddings, and a cascade of conditional diffu-
sion models that map these embeddings to images of in-
creasing resolution.
Imagen uses a frozen T5-XXL en-
coder as a semantic text encoder to capture the complex-
ity and compositionality of text inputs. The cascade begins
with a 2B parameter 64×64 text-to-image base model. Its
outputs are then fed to a 600M parameter super-resolution
model to upsample from 64×64 to 256×256, followed by
a 400M parameter model to upsample from 256×256 to
1024×1024. The base 64×64 model is conditioned on text
embeddings via a pooled embedding vector added to the dif-
fusion time-step embedding, like previous class-conditional
diffusion models [26]. All three stages of the diffusion cas-
cade include text cross-attention layers [50].
Given the relative paucity of high resolution im-
ages in ImageNet, we ﬁne-tune only the 64×64 base
model and 64×64→256×256 super-resolution model on
the ImageNet-1K train split, keeping the ﬁnal super-
resolution module and text-encoder unchanged.
The
64×64 base model is ﬁne-tuned for 210K steps and the
64×64→256×256 super-resolution model is ﬁne-tuned for
490K steps, on 256 TPU-v4 chips with a batch size of
2048. As suggested in the original Imagen training pro-
cess, Adafactor [57] is used to ﬁne-tune the base 64×64
model because it had a smaller memory footprint compared
to Adam [33]. For the 256×256 super-resolution model,
we used Adam for better sample quality. Throughout ﬁne-
tuning experiments, we select models based on FID score
calculated over 10K samples from the default Imagen sam-
pler and the ImageNet-1K validation set.
4.2. Sampling Parameters
The quality, diversity, and speed of text-conditioned dif-
fusion model sampling are strongly affected by multiple
factors including the number of diffusion steps, noise condi-
tion augmentation [50], guidance weights for classiﬁer-free
guidance [27, 40], and the log-variance mixing coefﬁcient
used for prediction (Eq. 15 in [41]), described in further de-

59
60
61
62
63
64
65
CAS: Top-1 Accuracy (%)
1.8
2.0
2.2
2.4
2.6
2.8
3.0
FID
0.0
0.0
0.1
0.1
0.2
0.2
0.3
0.3
0.4
0.4
0.5
0.5
logvar coeff = 0.3
logvar coeff = 0.1
Figure 4. Training set FID vs. CAS Pareto curves under varying
noise conditions when the guidance weight is set to 1.0 for reso-
lution 256×256. These curves depict the joint inﬂuence of the
log-variance mixing coefﬁcient [41] and noise conditioning aug-
mentation [26] on FID and CAS.
tail in Appendix A.1. We conduct a thorough analysis of the
dependence of FID, IS and classiﬁcation accuracy scores
(CAS) in order to select good sampling parameters for the
downstream classiﬁcation task.
The sampling parameters for the 64 × 64 based model
establish the overall quality and diversity of image sam-
ples. We ﬁrst sweep over guidance weight, log-variance,
and number of sampling steps, to identify good hyperpa-
rameters based on FID-50K (vs. the ImageNet validation
set). Using the DDPM sampler [25] for the base model, we
sweep over guidance values of [1.0, 1.25, 1.5, 1.75, 2.0, 5.0]
and log-variance of [0.0, 0.2, 0.3, 0.4, 1.0], and denoise for
128, 500, or 1000 steps. The results of this sweep, sum-
marized in Figure 3, suggest that optimal FID is obtained
with a log-variance of 0 and 1000 denoising steps. Given
these parameter choices we then complete a more compute
intensive sweep, sampling 1.2M images from the ﬁne-tuned
base model for different values of the guidance weights. We
measure FID, IS and CAS for these samples on the valida-
tion set in order to select the guidance weight for the model.
Figure 3 shows the Pareto frontiers for FID vs. IS across dif-
ferent guidance weights, as well as the dependence of CAS
on guidance weight, suggesting that optimal FID and CAS
are obtained at a guidance weight of 1.25.
Given 64 × 64 samples obtained with the optimal hyper-
parameters, we then analyze the impact of guidance weight,
noise augmentation, and log-variance to select sampling pa-
rameters for the super-resolution models. The noise aug-
mentation value speciﬁes the level of noise augmentation
applied to the input to super-resolution stages in the Imagen
cascade to regulate sample diversity (and improve robust-
ness during model training). Here, we sweep over guid-
ance values of [1.0, 2.0, 5.0, 10.0, 30.0], noise condition-
ing augmentation values of [0.0, 0.1, 0.2, 0.3, 0.4], and log-
variance mixing coefﬁcients of [0.1, 0.3], and denoise for
128, 500, or 1000 steps. Figure 4 shows Pareto curves of
FID vs. CAS for the 64×64 →256×256 super-resolution
module across different noise conditioning augmentation
values using a guidance weight of 1.0. These curves demon-
strate the combined impact of the log-variance mixing co-
efﬁcient and condition noise augmentation in achieving an
optimal balance between FID and CAS.
Overall, the results suggest that FID and CAS are highly
correlated, with smaller guidance weights leading to bet-
ter CAS but negatively affecting Inception Score. We ob-
serve that using noise augmentation of 0 yields the low-
est FID score for all values of guidance weights for super-
resolution models.
Nevertheless, it is worth noting that
while larger amounts of noise augmentation tend to increase
FID, they also produce more diverse samples, as also ob-
served by [50]. Results of these studies are available in the
Appendix.
Based on these sweeps, taking FID and CAS into ac-
count, we selected guidance of 1.25 when sampling from
the base model, and 1.0 for other resolutions.
We use
DDPM sampler [25] log-variance mixing coefﬁcients of 0.0
and 0.1 for 64 × 64 samples and 256 × 256 samples respec-
tively, with 1000 denoising steps. At resolution 1024×1024
we use a DDIM sampler [60] with 32 steps, as in [50]. We
do not use noise conditioning augmentation for sampling.
4.3. Generation Protocol
We use the ﬁne-tuned Imagen model with the optimized
sampling hyper-parameters to generate synthetic data re-
sembling the training split of ImageNet dataset. This means
that we aim to produce the same quantity of images for
each class as found in the real ImageNet dataset while keep-
ing the same class balance as the original dataset. We then
constructed large-scale training datasets with ranging from
1.2M to 12M images, i.e., between 1× to 10× the size of
the original ImageNet training set.
5. Results
5.1. Sample Quality: FID and IS
Despite the shortcomings described in Sec. 3, FID [22]
and Inception Score [52] remain standard metrics for evalu-
ating generative models. Table 1 reports FID and IS for our
approach and existing class-conditional and guidance-based
approaches. Our ﬁne-tuned model outperforms all the ex-
isting methods, including state-of-the-art methods that use
U-Nets [26] and larger U-ViT models trained solely on Im-
ageNet data [28]. This suggests that large-scale pretraining
followed by ﬁne-tuning on domain-speciﬁc target data is an
effective strategy to achieve better visual quality with dif-
fusion models, as measured by FID and IS. Figure 2 shows
imaage samples from the ﬁne-tuned model (see Appendix
for more). Note that our state-of-the-art FID and IS on Ima-
geNet are obtained without any design changes, i.e., by sim-
ply adapting an off-the-shelf, diffusion-based text-to-image
model to new data through ﬁne-tuning. This is a promising

Model
FID train
FID validation
IS
64x64 resolution
BigGAN-deep (Dhariwal & Nichol, 2021) [12]
4.06
-
-
Improved DDPM (Nichol & Dhariwal, 2021) [41]
2.92
-
-
ADM (Dhariwal & Nichol, 2021) [12]
2.07
-
-
CDM (Ho et al, 2022) [26]
1.48
2.48
67.95 ± 1.97
RIN (Jabri et al., 2022) [30]
1.23
-
66.5
RIN + noise schedule (Chen, 2023) [9]
2.04
-
55.8
Ours (Fine-tuned Imagen)
1.21
2.51
85.77 ± 0.06
256x256 resolution
BigGAN-deep (Brock et al., 2019) [6]
6.9
-
171.4 ± 2.00
VQ-VAE-2 (Razavi et al., 2019) [46]
31.11
-
-
SR3 (Saharia et al., 2021) [49]
11.30
-
-
LDM-4 (Rombach et al., 2022) [47]
10.56
-
103.49
DiT-XL/2 (Peebles & Xie, 2022) [42]
9.62
-
121.5
ADM (Dhariwal & Nichol, 2021) [12]
10.94
-
100.98
ADM+upsampling (Dhariwal & Nichol, 2021) [12]
7.49
-
127.49
CDM (Ho et al, 2022) [26]
4.88
3.76
158.71 ± 2.26
RIN (Jabri et al., 2022) [30]
4.51
4.51
161.0
RIN + noise schedule (Chen, 2023) [9]
3.52
-
186.2
Simple Diffusion (U-Net) (Hoogeboom et al., 2023) [28]
3.76
2.88
171.6 ± 3.07
Simple Diffusion (U-ViT L) (Hoogeboom et al., 2023) [28]
2.77
3.23
211.8 ± 2.93
Ours (Fine-tuned Imagen)
1.76
2.81
239.18 ± 1.14
Table 1. Comparison of sample quality of synthetic ImageNet datasets measured by FID and Inception Score (IS) between our ﬁne-tuned
Imagen model and generative models in the literature. We achieve SOTA FID and IS on ImageNet generation among other existing models,
including class-conditional and guidance-based sampling without any design changes.
result indicating that in a resource-limited setting, one can
improve the performance of diffusion models by ﬁne-tuning
model weights and adjusting sampling parameters.
5.2. Classiﬁcation Accuracy Score
As noted above, classiﬁcation accuracy score (CAS) [45]
is a better proxy than FID and IS for performance of down-
stream training on generated data. CAS measures ImageNet
classiﬁcation accuracy on the real test data for a model
trained solely on synthetic samples. In keeping with the
CAS protocol [45], we train a standard ResNet-50 architec-
ture on a single crop from each training image. Models are
trained for 90 epochs with a batch size of 1024 using SGD
with momentum (see Appendix A.4 for details). Regardless
of the resolution of the generated data, for CAS training
and evaluation, we resize images to 256×256 (or, for real
images, to 256 pixels on the shorter side) and then take a
224×224 pixel center crop.
Table 2 reports CAS for samples from our ﬁne-tuned
models at resolutions 256×256 and 1024×1024. CAS for
real data and for other models are taken from [45] and [26].
The results indicate that our ﬁne-tuned class-conditional
models outperform the previous methods in the literature
at 256 × 256 resolution by a good margin, for both Top-1
and Top-5 accuracy. Interestingly, results are markedly bet-
ter for 1024×1024 samples, even though these samples are
down-sampled to 256×256 during classiﬁer training. As re-
ported in Table 2, we achieve the SOTA Top-1 classiﬁcation
accuracy score of 69.24% at resolution 1024×1024. This
greatly narrows the gap with the ResNet-50 model trained
on real data.
Figure 5 shows the accuracy of models trained on gener-
ative data (red) compared to a model trained on real data
(blue) for each of the 1000 ImageNet classes (cf. Fig. 2
in [45]). From Figure 5 (left) one can see that the ResNet-50
trained on CDM samples is weaker than the model trained
on real data, as most red points fall below the blue points.
For our ﬁne-tuned Imagen models (Figure 5 middle and
right), however, there are more classes for which the models
trained on generated data outperform the model trained on
real data. This is particularly clear at 1024×1024.
5.3. Classiﬁcation Accuracy with Different Models
To further evaluate the discriminative power of the syn-
thetic data, compared to the real ImageNet-1K data, we
analyze the classiﬁcation accuracy of models with differ-

0
200
400
600
800
1000
Class ID
0.0
0.2
0.4
0.6
0.8
1.0
Classification Accuracy
Synthetic (CDM)
Real Training Data
0
200
400
600
800
1000
Class ID
0.0
0.2
0.4
0.6
0.8
1.0
Classification Accuracy
Synthetic (Ours - 256x256)
Real Training Data
0
200
400
600
800
1000
Class ID
0.0
0.2
0.4
0.6
0.8
1.0
Classification Accuracy
Synthetic (Ours - 1024x1024)
Real Training Data
Figure 5. Class-wise classiﬁcation accuracy comparison accuracy of models trained on real data (blue) and generated data (red). Left: The
256 × 256 CDM model [26]. Middle and right: Our ﬁne-tuned Imagen model at 256 × 256 and 1024 × 1024.
Model
Top-1 Accuracy (%)
Top-5 Accuracy(%)
Real
73.09
91.47
BigGAN-deep (Brock et al., 2019) [6]
42.65
65.92
VQ-VAE-2 (Razavi et al, 2019) [46]
54.83
77.59
CDM (Ho et al, 2022) [26]
63.02
84.06
Ours (256×256 resolution)
64.96
85.66
Ours (1024×1024 resolution)
69.24
88.10
Table 2. Classiﬁcation Accuracy Scores (CAS) for 256×256 and 1024×1024 generated samples. CAS for real data and other models
are obtained from [45] and [26]. Our results indicate that the ﬁne-tuned generative diffusion model outperforms the previous methods by a
substantial margin.
ent architectures, input resolutions, and model capacities.
We consider multiple ResNet-based and Vision Transform-
ers (ViT)-based [13] classiﬁers including ResNet-50 [20],
ResNet-RS-50, ResNet-RS-152x2, ResNet-RS-350x2 [4],
ViT-S/16 [5], and DeiT-B [64]. The models trained on real,
synthetic, and the combination of real and synthetic data
are all trained in the same way, consistent with the training
recipes speciﬁed by authors of these models on ImageNet-
1K, and our results on real data agree with the published
results. The Appendix has more details on model training.
Table 3 reports the Top-1 validation accuracy of multi-
ple ConvNet and Transformer models when trained with the
1.2M real ImageNet training images, with 1.2M generated
images, and when the generative samples are used to aug-
ment the real data. As one might expect, models trained
solely on generated samples perform worse than models
trained on real data. Nevertheless, augmenting real data
with synthetic images from the diffusion model yields a
substantial boost in performance across all classiﬁers tested.
5.4. Merging Real and Synthetic Data at Scale
We next consider how performance of a ResNet-50 clas-
siﬁer depends on the amount of generated data that is used
to augment the real data. Here we follow conventional train-
ing recipe and train with random crops for 130 epochs, re-
sulting in a higher ResNet-50 accuracy here than in the CAS
0
2
4
6
8
10
Generated Data Size (M)
69
70
71
72
73
Top-1 Accuracy (%)
+3.35
+4.01
+4.35
Figure 6. Improved classiﬁcation accuracy of ResNet-50 with in-
creasing numbers of synthetic images added to real training data
at resolution 64×64.
results in Table 2. The Appendix provides training details.
Ravuri and Vinyals [45] (Fig. 5) found that for almost
all models tested, mixing generated samples with real data
degrades Top-5 classiﬁer accuracy. For Big-GAN-deep [6]
with low truncation values (sacriﬁcing diversity for sample
quality), accuracy increases marginally with small amounts
of generated data, but then drops below models trained
solely on real data when the amount of generated data ap-
proaches the size of the real train set.
Figure 6 shows that, for 64 × 64 images, performance
continues to improve as the amount of generated data in-

Model
Input Size
Params (M)
Real Only
Generated Only
Real + Generated
Performance ∆
ConvNets
ResNet-50
224×224
36
76.39
69.24
78.17
+1.78
ResNet-101
224×224
45
78.15
71.31
79.74
+1.59
ResNet-152
224×224
64
78.59
72.38
80.15
+1.56
ResNet-RS-50
160×160
36
79.10
70.72
79.97
+0.87
ResNet-RS-101
160×160
64
80.11
72.73
80.89
+0.78
ResNet-RS-101
190×190
64
81.29
73.63
81.80
+0.51
ResNet-RS-152
224×224
87
82.81
74.46
83.10
+0.29
Transformers
ViT-S/16
224×224
22
79.89
71.88
81.00
+1.11
DeiT-S
224×224
22
78.97
72.26
80.49
+1.52
DeiT-B
224×224
87
81.79
74.55
82.84
+1.04
DeiT-B
384×384
87
83.16
75.45
83.75
+0.59
DeiT-L
224×224
307
82.22
74.60
83.05
+0.83
Table 3. Comparison of classiﬁer Top-1 Accuracy (%) performance when 1.2M generated images are used for generative data augmentation.
Models trained solely on generated samples perform worse than models trained on real data. Nevertheless, augmenting the real data with
data generated from the ﬁne-tuned diffusion model provides a substantial boost in performance across many different classiﬁers.
Train Set (M)
256×256
1024×1024
1.2
76.39 ± 0.21
76.39 ± 0.21
2.4
77.61 ± 0.08 (+1.22)
78.12 ± 0.05 (+1.73)
3.6
77.16 ± 0.04 (+0.77)
77.48 ± 0.04 (+1.09)
4.8
76.52 ± 0.04 (+0.13)
76.75 ± 0.07 (+0.36)
6.0
76.09 ± 0.08 (-0.30)
76.34 ± 0.13 (-0.05)
7.2
75.81 ± 0.08 (-0.58)
75.87 ± 0.09 (-0.52)
8.4
75.44 ± 0.06 (-0.95)
75.49 ± 0.07 (-0.90)
9.6
75.28 ± 0.10 (-1.11)
74.72 ± 0.20 (-1.67)
10.8
75.11 ± 0.12 (-1.28)
74.14 ± 0.13 (-2.25)
12.0
75.04 ± 0.05 (-1.35)
73.70 ± 0.09 (-2.69)
Table 4. Scaling the training dataset by adding synthetic images,
at resolutions 256 × 256 and 1024 × 1024. The baseline Top-1
accuracy of the classiﬁer trained on real data is 76.39 ± 0.21. The
number in parenthesis shows the change obtained over baseline
with the addition of generated data.
creases up to nine times the amount of real data, to a total
dataset size of 12M images. Performance with higher reso-
lution images, however, does not continue to improve with
similarly large amounts of generative data augmentation.
Table 4 reports performance as the amount of generated data
increased over the same range, up to 9× the amount of real
data, at resolutions 256×256 and 1024×1024. The perfor-
mance boost remains signiﬁcant with ﬁne-tuned diffusion
models for synthetic data up to a factor of 4 or 5 times the
size of the real ImageNet training set, a signiﬁcant improve-
ment over results reported in [45].
6. Conclusion
This paper asks to what extent generative data augmen-
tation is effective with current diffusion models. We do so
in the context of ImageNet classiﬁcation, a challenging do-
main as it is extensively explored with highly tuned archi-
tectures and training recipes. Here we show that large-scale
text-to-image diffusion models can be ﬁne-tuned to produce
class-conditional models with SOTA FID (1.76 at 256×256
resolution) and Inception Score (239 at 256 × 256). The
resulting generative model also yields a new SOTA in Clas-
siﬁcation Accuracy Scores (64.96 for 256×256 models, im-
proving to 69.24 for 1024×1024 generated samples). And
we have shown improvements to ImageNet classiﬁcation
accuracy extend to large amounts of generated data, across
a range of ResNet and Transformer-based models.
While these results are encouraging, many questions
remain.
One concerns the boost in CAS at resolution
1024×1024, suggesting that the larger images capture more
useful image structure than those at 256×256, even though
the 1024×1024 images are downsampled to 256×256 before
being center-cropped to 224×224 for input to ResNet-50.
Another concerns the sustained gains in classiﬁcation accu-
racy with large amounts of synthetic data at 64×64 (Fig. 6);
there is less information at low resolutions for training, and
hence a greater opportunity for augmentation with synthetic
images. At high resolutions (Tab. 4) performance drops for
synthetic datasets larger than 1M images, which may indi-
cate bias in the generative model, and the need for more
sophisticated training methods with synthetic data. These
issues remain topics of on-going research.

Acknowledgments
We thank Jason Baldridge and Ting Chen for their valu-
able feedback. We also extend thanks to William Chan,
Saurabh Saxena, and Lala Li for helpful discussions, feed-
back, and their support with the Imagen code.
References
[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu
Liu. ediff-i: Text-to-image diffusion models with an ensem-
ble of expert denoisers. preprint arxiv.2211.01324,, 2022.
[2] Hritik Bansal and Aditya Grover. Leaving reality to imag-
ination: Robust classiﬁcation via generated datasets. arXiv
preprint arXiv:2302.02503, 2023.
[3] Dmitry
Baranchuk,
Ivan
Rubachev,
Andrey
Voynov,
Valentin Khrulkov, and Artem Babenko. Label-efﬁcient se-
mantic segmentation with diffusion models. arXiv preprint
arXiv:2112.03126, 2021.
[4] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus
Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens,
and Barret Zoph. Revisiting resnets: Improved training and
scaling strategies. Advances in Neural Information Process-
ing Systems, 34:22614–22627, 2021.
[5] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet-
ter plain vit baselines for imagenet-1k.
arXiv preprint
arXiv:2205.01580, 2022.
[6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthe-
sis. International Conference on Learning Representations,
2019.
[7] Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-
Mizil. Model compression. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 535–541, 2006.
[8] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mo-
hammad Norouzi, and William Chan.
Wavegrad: Esti-
mating gradients for waveform generation. arXiv preprint
arXiv:2009.00713, 2020.
[9] Ting Chen. On the importance of noise scheduling for diffu-
sion models. arXiv preprint arXiv:2301.10972, 2023.
[10] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.
Learning semantic segmentation from synthetic data: A geo-
metrically guided input-output adaptation approach. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 1841–1850, 2019.
[11] Celso M de Melo, Antonio Torralba, Leonidas Guibas, James
DiCarlo, Rama Chellappa, and Jessica Hodgins.
Next-
generation deep learning based on simulators and synthetic
data. Trends in cognitive sciences, 2021.
[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. Advances in Neural Infor-
mation Processing Systems, 34:8780–8794, 2021.
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020.
[14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical ﬂow with convolutional networks. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 2758–2766, 2015.
[15] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-
nio Lopez, and Vladlen Koltun. Carla: An open urban driv-
ing simulator. In Conference on robot learning, pages 1–16.
PMLR, 2017.
[16] C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, J De Fre-
itas, J Kubilius, A Bhandwaldar, N Haber, M Sano, et al.
Threedworld: A platform for interactive multi-modal physi-
cal simulation. Advances in Neural Information Processing
Systems (NeurIPS), 2021.
[17] Sven Gowal, Sylvestre-Alvise Rebufﬁ, Olivia Wiles, Florian
Stimberg, Dan Andrei Calian, and Timothy A Mann. Im-
proving robustness using generated data. Advances in Neural
Information Processing Systems, 34:4218–4233, 2021.
[18] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch,
Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra-
gasam, Florian Golemo, Charles Herrmann, Thomas Kipf,
Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-
Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek
Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad-
wan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi,
Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun,
Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi,
Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: A scal-
able dataset generator. 2022.
[19] Xi Guo, Wei Wu, Dongliang Wang, Jing Su, Haisheng Su,
Weihao Gan, Jian Huang, and Qin Yang. Learning video rep-
resentations of human motion from synthetic data. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 20197–20207, 2022.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.
[21] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing
Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic
data from generative models ready for image recognition?
arXiv preprint arXiv:2210.07574, 2022.
[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems,
30, 2017.
[23] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distill-
ing the knowledge in a neural network.
arXiv preprint
arXiv:1503.02531, 2015.
[24] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben

Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High deﬁnition video generation with diffusion mod-
els. arXiv preprint arXiv:2210.02303, 2022.
[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems, 33:6840–6851, 2020.
[26] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high ﬁdelity image generation. J. Mach. Learn.
Res., 23(47):1–33, 2022.
[27] Jonathan Ho and Tim Salimans.
Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598, 2022.
[28] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. sim-
ple diffusion: End-to-end diffusion for high resolution im-
ages. arXiv preprint arXiv:2301.11093, 2023.
[29] Shahram
Izadi,
David
Kim,
Otmar
Hilliges,
David
Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology, pages 559–568, 2011.
[30] Allan Jabri, David Fleet, and Ting Chen.
Scalable adap-
tive computation for iterative generation.
arXiv preprint
arXiv:2212.11972, 2022.
[31] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. NeurIPS, 2022.
[32] Yo-whan Kim. How Transferable are Video Representations
Based on Synthetic Data? PhD thesis, Massachusetts Insti-
tute of Technology, 2022.
[33] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arxiv:1412.6980,
2014.
[34] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
Big transfer (bit): General visual representation learning. In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16,
pages 491–507. Springer, 2020.
[35] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
Bryan Catanzaro. Diffwave: A versatile diffusion model for
audio synthesis. arXiv preprint arXiv:2009.09761, 2020.
[36] Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis,
Sanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthe-
sizing imagenet with pixel-wise annotations. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 21330–21340, 2022.
[37] Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba,
and Sanja Fidler.
Semantic segmentation with generative
models: Semi-supervised learning and strong out-of-domain
generalization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 8300–
8311, 2021.
[38] Jianxin Ma, Shuai Bai, and Chang Zhou. Pretrained diffusion
models for uniﬁed human motion synthesis. arXiv preprint
arXiv:2212.02837, 2022.
[39] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,
Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,
and Laurens Van Der Maaten. Exploring the limits of weakly
supervised pretraining. In Proceedings of the European con-
ference on computer vision (ECCV), pages 181–196, 2018.
[40] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021.
[41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models.
In International
Conference on Machine Learning, pages 8162–8171. PMLR,
2021.
[42] William Peebles, Jun-Yan Zhu, Richard Zhang, Antonio Tor-
ralba, Alexei A Efros, and Eli Shechtman. Gan-supervised
dense visual alignment. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 13470–13481, 2022.
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748–8763. PMLR, 2021.
[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022.
[45] Suman Ravuri and Oriol Vinyals.
Classiﬁcation accuracy
score for conditional generative models. Advances in neural
information processing systems, 32, 2019.
[46] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-
ating diverse high-ﬁdelity images with vq-vae-2. Advances
in neural information processing systems, 32, 2019.
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer.
High-resolution image
synthesis with latent diffusion models.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684–10695, 2022.
[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International journal of
computer vision, 115:211–252, 2015.
[49] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi.
Palette: Image-to-image diffusion models.
In
ACM SIGGRAPH 2022 Conference Proceedings, pages 1–
10, 2022.
[50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour,
Burcu Karagol Ayan,
S Sara Mahdavi,
Rapha Gontijo Lopes, et al.
Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022.
[51] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-

resolution via iterative reﬁnement.
IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2022.
[52] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in neural information processing
systems, 29, 2016.
[53] Tim Salimans and Jonathan Ho.
Progressive distillation
for fast sampling of diffusion models.
arXiv preprint
arXiv:2202.00512, 2022.
[54] Swami Sankaranarayanan,
Yogesh Balaji,
Arpit Jain,
Ser Nam Lim, and Rama Chellappa. Learning from synthetic
data: Addressing domain shift for semantic segmentation. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 3752–3761, 2018.
[55] Shibani Santurkar, Ludwig Schmidt, and Aleksander Madry.
A classiﬁcation-based study of covariate shift in gan distri-
butions. In International Conference on Machine Learning,
pages 4480–4489. PMLR, 2018.
[56] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus,
and Yannis Kalantidis.
Fake it till you make it: Learn-
ing (s) from a synthetic imagenet clone.
arXiv preprint
arXiv:2212.08420, 2022.
[57] Noam Shazeer and Mitchell Stern.
Adafactor: Adaptive
learning rates with sublinear memory cost. arXiv preprint,
arxiv:1804.04235, 2018.
[58] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792,
2022.
[59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning, pages 2256–2265. PMLR, 2015.
[60] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. International Confernece on
Learning Representations, 2021.
[61] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456, 2020.
[62] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-
nav Gupta. Revisiting unreasonable effectiveness of data in
deep learning era. In Proceedings of the IEEE international
conference on computer vision, pages 843–852, 2017.
[63] Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun
Jampani, Michael Krainin, Huiwen Chang, Ramin Zabih,
William T Freeman, and Ce Liu. Autoﬂow: Learning a better
training set for optical ﬂow. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 10093–10102, 2021.
[64] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv´e J´egou. Training
data-efﬁcient image transformers & distillation through at-
tention. In International conference on machine learning,
pages 10347–10357. PMLR, 2021.
[65] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan
Salakhutdinov. Effective data augmentation with diffusion
models. arXiv preprint arXiv:2302.07944, 2023.
[66] Nontawat Tritrong, Pitchaporn Rewatbowornwong, and Su-
pasorn Suwajanakorn.
Repurposing gans for one-shot se-
mantic part segmentation. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pages 4475–4485, 2021.
[67] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah-
mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.
Learning from synthetic humans.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 109–117, 2017.
[68] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-
dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi
Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.
Phenaki: Variable length video generation from open domain
textual description. arXiv preprint arXiv:2210.02399, 2022.
[69] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-
Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah
Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor
and editbench: Advancing and evaluating text-guided image
inpainting. arXiv preprint arXiv:2212.06909, 2022.
[70] Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, and
Bolei Zhou. Generative hierarchical features from synthe-
sizing images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 4432–
4442, 2021.
[71] Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi
Parikh. LR-GAN: Layered recursive generative adversarial
networks for image generation. In International Conference
on Learning Representations, 2017.
[72] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling vision transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 12104–12113, 2022.
[73] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao,
and Zihan Zhou.
Structured3d:
A large photo-realistic
dataset for structured 3d modeling.
In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part IX 16, pages 519–535.
Springer, 2020.

Appendix
A.1. Hyper-parameters for Imagen ﬁne-tuning and sample generation.
The quality, diversity, and speed of text-conditioned diffusion model sampling are strongly affected by multiple hyper-
parameters. These include the number of diffusion steps, where larger numbers of diffusion steps are often associated with
higher quality images and lower FID. Another hyper-parameter is the amount of noise-conditioning augmentation [50], which
adds Gaussian noise to the output of one stage of the Imagen cascade at training time, prior to it being input to the subsequent
super-resolution stage. We considered noise levels between 0 and 0.5 (with images in the range [0,1]), where adding more
noise during training degrades more ﬁne-scale structure, thereby forcing the subsequent super-resolution stage to be more
robust to variability in the images generated from the previous stage.
During sampling, we use classiﬁer-free guidance [27, 40], but with smaller guidance weights than Imagen, favoring di-
versity over image ﬁdelity to some degree. With smaller guidance weights, one does not require dynamic thresholding [50]
during inference; instead we opt for a static threshold to clip large pixel values at each step of denoising. Ho et al. [27] iden-
tify upper and lower bounds on the predictive variance, Σθ(xt, t), used for sampling at each denoising step. Following [41]
(Eq. 15) we use a linear (convex) combination of the log upper and lower bounds, the mixing parameter for which is referred
to as the logvar parameter. Figures 3 and 4 show the dependence of FID, IS and Classiﬁcation Accuracy Scores on guidance
weight and logvar mixing coefﬁcient for the base model at resolution 64×64 and the 64→256 super-resolution model. These
were used to help choose model hyyper-parameters for large-scale sample generation.
Below are further results relate to hyperparameter selection and its impact on model metrics.
50
100
150
200
Inception Score
10
20
30
40
50
60
Classification Accuracy Score (%)
1.0
1.0
1.25
1.25
1.5
1.5
1.75
1.75
2.0
2.0
5.0
5.0
Top-5 Accuracy (%)
Top-1 Accuracy (%)
50
100
150
200
Inception Score
5
10
15
20
FID@1.2M
1.0
1.25
1.5
1.75
2.0
5.0
Figure A.1. Left: CAS vs IS Pareto curves for train set resolution of 64×64 showing the impact of guidance weights. Right: Train set
FID vs IS Pareto curves for resolution of 64x64 showing the impact of guidance weights.
1.0
1.25
1.5
1.75
2.0
5.0
Guidance Weight
6
8
10
12
14
16
18
FID@50K
logvar coeff = 1.0
logvar coeff = 0.2
logvar coeff = 0.3
logvar coeff = 0.4
logvar coeff = 0.0
60
80
100
120
140
160
180
Inception Score
6
8
10
12
14
16
18
FID@50K
logvar coeff = 1.0
logvar coeff = 0.2
logvar coeff = 0.3
logvar coeff = 0.4
logvar coeff = 0.0
Figure A.2. Sampling reﬁnement for 64×64 base model. Left: Validation set FID vs. guidance weights for different values of log-variance.
Right: Validation set FID vs. Inception score (IS) when increasing guidance from 1.0 to 5.0.

10
20
30
40
50
60
CAS: Top-1 Accuracy (%)
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
FID
1
2
5
10
20
30
DDPM Steps: 128
0.0
0.1
0.2
0.3
0.4
0.5
10
20
30
40
50
60
CAS: Top-1 Accuracy (%)
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
FID
1
2
5
10
20
30
DDPM Steps: 500
0.0
0.1
0.2
0.3
0.4
0.5
10
20
30
40
50
60
CAS: Top-1 Accuracy (%)
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
FID
1
2
5
10
20
30
DDPM Steps: 1000
0.0
0.1
0.2
0.3
0.4
0.5
20
30
40
50
60
70
80
CAS: Top-5 Accuracy (%)
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
FID
1
2
5
10
20
30
DDPM Steps: 128
0.0
0.1
0.2
0.3
0.4
0.5
20
30
40
50
60
70
80
CAS: Top-5 Accuracy (%)
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
FID
1
2
5
10
20
30
DDPM Steps: 500
0.0
0.1
0.2
0.3
0.4
0.5
20
30
40
50
60
70
80
CAS: Top-5 Accuracy (%)
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
FID
1
2
5
10
20
30
DDPM Steps: 1000
0.0
0.1
0.2
0.3
0.4
0.5
Figure A.3. Top-1 and Top-5 classiﬁcation accuracy score (CAS) vs train FID Pareto curves (sweeping over guidance weight) showing the
impact of conditioning noise augmentation at 256×256 when sampling with different number of steps. As indicated by number overlaid
on each trend line, guidance weight is decreasing from 30 to 1.
20
30
40
50
60
CAS: Top-1 Accuracy (%)
2
3
4
5
6
7
8
9
FID
1
2
5
10
20
30
Augmentation Noise: 0.0
128
500
1000
20
30
40
50
60
CAS: Top-1 Accuracy (%)
4
6
8
10
12
FID
1
2
5
10
20
30
Augmentation Noise: 0.1
128
500
1000
20
30
40
50
60
CAS: Top-1 Accuracy (%)
4
6
8
10
12
14
16
FID
1
2
5
10
20
30
Augmentation Noise: 0.2
128
500
1000
10
20
30
40
50
CAS: Top-1 Accuracy (%)
4
6
8
10
12
14
16
18
FID
1
2
5
10
20
30
Augmentation Noise: 0.3
128
500
1000
10
20
30
40
50
CAS: Top-1 Accuracy (%)
6
8
10
12
14
16
18
20
FID
1
2
5
10
20
30
Augmentation Noise: 0.4
128
500
1000
10
20
30
40
50
CAS: Top-1 Accuracy (%)
6
8
10
12
14
16
18
20
FID
1
2
5
10
20
30
Augmentation Noise: 0.5
128
500
1000
Figure A.4. Top-1 and Top-5 classiﬁcation accuracy score (CAS) vs train FID Pareto curves (sweeping over guidance weight) showing the
impact of conditioning noise augmentation at 256×256 when sampling with different number of steps at a ﬁxed noise level. As indicated
by number overlaid on each trend line guidance weight is decreasing from 30 to 1. At highest noise level (0.5) lowering number sampling
step and decreasing guidance can lead to a better joint FID and CAS values. At lowest noise level (0.0) this effect is subtle and increasing
sampling steps and lower guidance weight can help to improve CAS.

20
30
40
50
60
CAS: Top-1 Accuracy (%)
2
4
6
8
FID
1
2
5
10
20
30
Vanilla Imagen
Finetuned Imagen
40
50
60
70
80
CAS: Top-5 Accuracy (%)
2
4
6
8
FID
1
2
5
10
20
30
Vanilla Imagen
Finetuned Imagen
Figure A.5. Fine-tuning of SR model helps to jointly improve classiﬁcation accuracy as well as FID of the vanilla Imagen.
2
4
6
8
10
Guidance Weight
40
45
50
55
60
65
70
CAS: Top-1 Accuracy (%)
0.0
0.1
0.2
0.3
0.4
230
235
240
245
250
Inception Score
40
45
50
55
60
65
70
75
CAS: Top-1 Accuracy (%)
5
8
2
10
1
0.0
0.1
0.2
0.3
0.4
Figure A.6. Sampling reﬁnement for 1024×2014 super resolution model. Left: CAS vs. guidance weights under varying noise conditions.
Right: CAS vs. Inception score (IS) when increasing guidance from 1.0 to 5.0 under varying noise conditions.
59
60
61
62
63
64
65
CAS: Top-1 Accuracy (%)
1.8
2.0
2.2
2.4
2.6
2.8
3.0
FID
0.0
0.0
0.1
0.1
0.2
0.2
0.3
0.3
0.4
0.4
0.5
0.5
logvar coeff = 0.3
logvar coeff = 0.1
81
82
83
84
85
CAS: Top-5 Accuracy (%)
1.8
2.0
2.2
2.4
2.6
2.8
3.0
FID
0.0
0.0
0.1
0.1
0.2
0.2
0.3
0.3
0.4
0.4
0.5
0.5
logvar coeff = 0.3
logvar coeff = 0.1
Figure A.7. Training set FID vs. classiﬁcation top-1 and top-5 accuracy Pareto curves under varying noise conditions when the guidance
weight is set to 1.0 for resolution 256×256. These curves depict the joint inﬂuence of the log-variance mixing coefﬁcient [41] and noise
conditioning augmentation [26] on FID and CAS.
A.2. Class Alignment of Imagen vs. Fine-Tuned Imagen
What follows are more samples to compare our ﬁne-tuned model vs. the Imagen model are provided in Figure A.8, A.9,
and A.10. In this comparison we sample our ﬁne-tuned model using two strategies. First, we sample using the proposed
vanilla Imagen hyper-parameters which use a guidance weight of 10 for the sampling of the base 64×64 model and subsequent
super-resolution (SR) models are sampled with guidance weights of 20 and 8, respectively. This is called the high guidance
strategy in these ﬁgures. Second, we use the proposed sampling hyper-parameters as explained in the paper which includes
sampling the based model with a guidance weight of 1.25 and the subsequent SR models with a guidance weight of 1.0. This
is called the low guidance weight strategy in these ﬁgures.

bittern bird
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
Cardoon
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
brussels griffon
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
Figure A.8. Example 1024×1024 images from vanilla Imagen (ﬁrst row) vs. ﬁne-tuned Imagen sampled with Imagen hyper-parameters
(high guidance, second row) vs. ﬁne-tuned Imagen sampled with our proposed hyper-parameter (low guidance, third row). Fine-tuning
and careful choice of sampling parameters help to improve the alignment of images with class labels, and also improve sample diversity.
Sampling with higher guidance weight can improve photorealism, but lessens diversity.

Schipperke
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
harvestman
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
Leonberger
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
Figure A.9. Example 1024×1024 images from vanilla Imagen (ﬁrst row) vs. ﬁne-tuned Imagen sampled with Imagen hyper-parameters
(high guidance, second row) vs. ﬁne-tuned Imagen sampled with our proposed hyper-parameter (low guidance, third row). Fine-tuning
and careful choice of sampling parameters help to improve the alignment of images with class labels, and also improve sample diversity.
Sampling with higher guidance weight can improve photorealism, but lessens diversity.

dowitcher
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
kuvasz
Imagen
Fine-tuned 
(High Guidance)
Fine-tuned 
(Low Guidance)
Figure A.10. Example 1024×1024 images from vanilla Imagen (ﬁrst row) vs. ﬁne-tuned Imagen sampled with Imagen hyper-parameters
(high guidance, second row) vs. ﬁne-tuned Imagen sampled with our proposed hyper-parameter (low guidance, third row). Fine-tuning
and careful choice of sampling parameters help to improve the alignment of images with class labels, and also improve sample diversity.
Sampling with higher guidance weight can improve photorealism, but lessens diversity.

A.3. High Resolution Random Samples from the ImageNet Model
Figure A.11. Random samples at 1024×1024 resolution generated by our ﬁne-tuned model. The classes are snail (113), panda (388),
orange (950), badger (362), indigo bunting (14), steam locomotive (820), carved pumpkin (607), lion (291), loggerhead sea turtle (33),
golden retriever (207), tree frog (31), clownﬁsh (393), dowitcher (142), lorikeet (90), school bus (779), macaw (88), marmot (336), green
mamba (64).

A.4. Hyper-parameters and model selection for ImageNet classiﬁers.
This section details all the hyper-parameters used in training our ResNet-based model for CAS calculation, as well as
the other ResNet-based, ResNet-RS-based, and Transformer-based models, used to report classiﬁer accuracy in Table 3.
Table A.1 and Table A.2 summarize the hyper-parameters used to train the ConvNet architectures and vision transformer
architectures, respectively.
For classiﬁcation accuracy (CAS) calculation, as discussed before we follow the protocol suggested in [45]. Our CAS
ResNet-50 classiﬁer is trained using a single crop. To train the classiﬁer, we employ an SGD momentum optimizer and run
it for 90 epochs. The learning rate is scheduled to linearly increase from 0.0 to 0.4 for the ﬁrst ﬁve epochs and then decrease
by a factor of 10 at epochs 30, 60, and 80. For other ResNet-based classiﬁers we employ more advanced mechanisms such
as using a cosine schedule instead of step-wise learning rate decay, larger batch size, random augmentation, dropout, and
label smoothing to reach competitive performance [62]. It is also important to emphasize that ResNet-RS achieved higher
performance than ResNet models through a combination of enhanced scaling strategies, improved training methodologies,
and the implementation of techniques like the Squeeze-Excitation module [4]. We follow the training strategy and hyper-
parameter suggested in [4] to train our ResNet-RS-based models.
For vision transformer architectures we mainly follow the recipe provided in [5] to train a competitive ViT-S/16 model
and [64] to train DeiT family models. In all cases we re-implemented and train all of our models from scratch using real only,
real + generated data, and generated only data until convergence.
Model
ResNet-50 (CAS)
ResNet-50
ResNet-101
ResNet-152
ResNet-RS-50
ResNet-RS-101
ResNet-RS-152
Epochs
90
130
200
200
350
350
350
Batch size
1024
4096
4096
4096
4096
4096
4096
Optimizer
Momentum
Momentum
Momentum
Momentum
Momentum
Momentum
Momentum
Learning rate
0.4
1.6
1.6
1.6
1.6
1.6
1.6
Decay method
Stepwise
Cosine
Cosine
Cosine
Cosine
Cosine
Cosine
Weight decay
1e-4
1e-4
1e-4
1e-4
4e-5
4e-5
4e-5
Warmup epochs
5
5
5
5
5
5
5
Label smoothing
-
0.1
0.1
0.1
0.1
0.1
0.1
Dropout rate
-
0.25
0.25
0.25
0.25
0.25
0.25
Rand Augment
-
10
15
15
10
15
15
Table A.1. Hyper-parameters used to train ConvNet architectures including ResNet-50 (CAS) [45], ResNet-50, ResNet-101, ResNet-152,
ResNet-RS-50, ResNet-RS-101, and ResNet-RS-152 [4].
Model
ViT-S/16
DeiT-S
DeiT-B
DeiT-L
Epochs
300
300
300
300
Batch size
1024
4096
4096
4096
Optimizer
AdamW
AdamW
AdamW
AdamW
Learning rate
0.001
0.004
0.004
0.004
Learning rate decay
Cosine
Cosine
Cosine
Cosine
Weight decay
0.0001
-
-
-
Warmup eepochs
10
5
5
5
Label dmoothing
-
0.1
0.1
0.1
Rand Augment
10
9
9
9
Mixup prob.
0.2
0.8
0.8
0.8
Cutmix prob.
-
1.0
1.0
1.0
Table A.2. Hyper-parameters used to train the vision transformer architectures, i.e., ViT-S/16 [5], DeiT-S [64], DeiT-B [64], and DeiT-
L [64].

