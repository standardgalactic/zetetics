LLaVA-Med: Training a Large Language-and-Vision
Assistant for Biomedicine in One Day
Chunyuan Li∗, Cliff Wong∗, Sheng Zhang∗, Naoto Usuyama, Haotian Liu, Jianwei Yang
Tristan Naumann, Hoifung Poon, Jianfeng Gao
Microsoft
https://aka.ms/llava-med
Abstract
Conversational generative AI has demonstrated remarkable promise for empow-
ering biomedical practitioners, but current investigations focus on unimodal text.
Multimodal conversational AI has seen rapid progress by leveraging billions of
image-text pairs from the public web, but such general-domain vision-language
models still lack sophistication in understanding and conversing about biomedical
images. In this paper, we propose a cost-efficient approach for training a vision-
language conversational assistant that can answer open-ended research questions
of biomedical images. The key idea is to leverage a large-scale, broad-coverage
biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to
self-instruct open-ended instruction-following data from the captions, and then
fine-tune a large general-domain vision-language model using a novel curriculum
learning method. Specifically, the model first learns to align biomedical vocabulary
using the figure-caption pairs as is, then learns to master open-ended conversational
semantics using GPT-4 generated instruction-following data, broadly mimicking
how a layperson gradually acquires biomedical knowledge. This enables us to train
a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less
than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal con-
versational capability and can follow open-ended instruction to assist with inquiries
about a biomedical image. On three standard biomedical visual question answering
datasets, fine-tuning LLaVA-Med outperforms previous supervised state-of-the-art
on certain metrics. To facilitate biomedical multimodal research, we will release
our instruction-following data and the LLaVA-Med model.
1
Introduction
Parallel image-text data is abundantly available in the general domain, such as web images and their
associated captions. Generative pretraining has proven effective to leverage this parallel data for
self-supervised vision-language modeling, as demonstrated by multimodal GPT-4 [32] and open-
sourced efforts such as LLaVA [24]. By instruction-tuning models to align with human intents
based on multimodal inputs, the resulting large multimodal models (LMMs) exhibit strong zero-shot
task completion performance on a variety of user-oriented vision-language tasks such as image
understanding and reasoning, paving the way to develop general-purpose multimodal conversational
assistants [2, 21, 9].
While successful in the general domains, such LMMs are less effective for biomedical scenarios
because biomedical image-text pairs are drastically different from general web content. As a result,
general-domain visual assistants may behave like a layperson, who would refrain from answering
∗Equal Contribution
Preprint. Work in progress
arXiv:2306.00890v1  [cs.CV]  1 Jun 2023

biomedical questions, or worse, produce incorrect responses or complete hallucinations. Much
progress has been made in biomedical visual question answering (VQA), but prior methods typically
formulate the problem as classification (e.g., among distinct answers observed in the training set) and
are not well equipped for open-ended instruction-following. Consequently, although conversational
generative AI has demonstrated great potential for biomedical applications [19, 30, 18], current
investigations are often limited to unimodal text.
In this paper, we present Large Language and Vision Assistant for BioMedicine (LLaVA-Med), a
first attempt to extend multimodal instruction-tuning to the biomedical domain for end-to-end training
of a biomedical multimodal conversational assistant. Domain-specific pretraining has been shown
to be effective for biomedical natural language processing (NLP) applications [17, 14, 10, 28] and
biomedical vision-language (VL) tasks [15, 7, 38, 49, 8]. Most recently, large-scale biomedical VL
learning has been made possible by the creation of PMC-15M [49], a broad-coverage dataset with 15
million biomedical image-text pairs extracted from PubMed Central1. This dataset is two orders of
magnitude larger than the next largest public dataset, MIMIC-CXR [15], and covers a diverse image
types. Inspired by recent work in instruction-tuning [34, 24], LLaVA-Med uses GPT-4 to generate
diverse biomedical multimodal instruction-following data using image-text pairs from PMC-15M,
and fine-tune a large biomedical-domain VL model [24] using a novel curriculum learning method.
Specifically, our paper makes the following contributions:
• Biomedical multimodal instruction-following data. We present a novel data generation pipeline
to create diverse (image, instruction, output) instances, by sampling biomedical image-text pairs
from PMC-15M and using GPT-4 to create instructions from the text alone (which becomes
the intended output). This requires zero manual annotations and creates an extremely diverse
visual instruction-following dataset by piggybacking on PMC-15 that covers the full spectrum
of research findings over biomedical images.
• LLaVA-Med. We propose a novel curriculum learning method for adapting LLaVA [24] to
the biomedical domain using our self-generated biomedical multi-modal instruction-following
dataset. Specifically, we first fine-tune LLaVA to align biomedical vocabulary using the image-
text pairs as is (with the generic instruction that simply asks for a description of the image). We
then continue training the model using our self-generated instruction-following data to learn
open-ended conversational semantics. In this way, we were able to train LLaVA-Med in less than
15 hours with eight A100s. Our empirical study validates the effectiveness of domain-specific
instruction-tuning, and reveals best practice and interesting findings for adapting multimodal
conversational assistant to high-value verticals. On well-established biomedical VQA datasets,
fine-tuning LLaVA-Med often outperforms supervised state-of-the-art (SoTA).
• Open-source. To facilitate research in biomedical multimodal learning, we will release the
following assets to the public: the biomedical multimodal instruction-following dataset and the
codebase for data generation and model training.
2
Related Work
Biomedical Chatbots.
Inspired by ChatGPT [31]/GPT-4 [32] and the success of open-sourced
instruction-tuned large language models (LLMs) in the general domain, several biomedical LLM
chatbots have been developed, including ChatDoctor [47], Med-Alpaca [12], PMC-LLaMA [45],
Clinical Camel [1], DoctorGLM [46], and Huatuo [44]. They are initialized with open-sourced LLM
and fine-tuned on customized sets of biomedical instruction-following data. The resulting LLMs
emerge with great potential to offer assistance in a variety of biomedical-related fields/settings, such
as understanding patients’ needs and providing informed advice.
To our knowledge, Visual Med-Alpaca [39] is the only existing multimodal biomedical chatbot that
accepts image inputs. Though Visual Med-Alpaca and the proposed LLaVA-Med share a similar
input-output data format, they differ in key aspects: (i) Model architectures. LLaVA-Med is an
end-to-end neural model and Visual Med-Alpaca is a system that connect multiple image captioning
models with a LLM, using a classifier to determine if or which biomedical captioning model is
responsible for the image. The text prompt subsequently merges the converted visual information
with the textual query, enabling Med-Alpaca to generate an appropriate response. (ii) Biomedical
instruction-following data. While Visual Med-Alpaca is trained on 54K samples from limited
biomedical subject domains, LLaVA-Med is trained a more diverse set.
1https://www.ncbi.nlm.nih.gov/pmc/
2

Biomedical Visual Question Answering.
An automated approach to building models that can
answer questions based on biomedical images stands to support clinicians and patients. To describe
existing biomedical VQA methods, we make a distinction between discriminative and generative
methods. For discriminative methods, VQA is treated a classification problem: models make
predictions from a predefined set of answers. While discriminative methods yield good performance,
they deal with closed-set predictions [13], and require mitigation when a customized answer set
is provided in at inference [22, 49, 8]. The discriminative formulation is suboptimal towards the
goal of developing a general-purpose biomedical assistant that can answer open questions in the
wild. To this end, generative methods have been developed to predict answers as a free-form text
sequence [5, 26, 41]. Generative methods are more versatile because they naturally cast the close-set
questions as as special case where candidate answers are in language instructions.
Model Architecture.
LLaVA-Med is similar to prefix tuning of language models (LMs) in [41] in
that a new trainable module connects frozen image encoder and causal LM. In [41], a three-layer
MLP network is used to map the visual features into a visual prefix, and the pre-trained LM are
GPT2-XL [37], BioMedLM [42] and BioGPT [28], with size varying from 1.5B to 2.7B. By contrast,
LLaVA-Med uses a linear projection and a 7B LM [43, 40]. Most importantly, [41] only considers
standard supervised fine-tuning and focuses efforts on exploring various modeling choices. Our
main contributions instead comprise proposing a novel data generation method that uses GPT-4 to
self-instruct biomedical multimodal instruction-following data using freely-available broad-coverage
biomedical image-text pairs extracted from PubMed Central [49].
3
Biomedical Visual Instruction-Following Data
There are a lack of multimodal biomedical datasets to train an instruction-following assistant. To
fill this gap, we create the first dataset of its kind from widely existing biomedical image-text pairs,
through a machine-human co-curation procedure. It consists of two sets, concept alignment and
instruction-following, which are used at different training stages, described in Section 4.
Biomedical Concept Alignment Data.
For a biomedical image Xv and its associated caption Xc,
we sample a question Xq, which asks to describe the biomedical image. With (Xv, Xc, Xq), we
create a single-round instruction-following example:
Human : Xq Xv<STOP>\n Assistant : Xc<STOP>\n
Depending on the length of caption, the question that is sampled either asks to describe the image
concisely or in detail. Two lists of questions are provided in Appendix A. In practice, 25% of captions
have length less than 30 words in PMC-15M [49], and thus 30 words is used as the cutoff point to
determine which list to choose. We sample 600K image-text pairs from PMC-15M. Though this
dataset only presents one-single task instructions, i.e., image captioning, it contains a diverse and
representative set of biomedical concept samples from the original PMC-15M [49].
Biomedical Instruction-Tuning Data.
To align the model to follow a variety of instructions,
we present and curate diverse instruction-following data with multi-round conversations about the
provided biomedical images, by prompting language-only GPT-4. Specifically, given an image
caption, we design instructions in a prompt that asks GPT-4 to generate multi-round questions and
answers in a tone as if it could see the image (even though it only has access to the text). Sometimes
the image caption is too short for GPT-4 to generate meaningful questions and answers. To provide
more context regarding the image, we also create a prompt that includes not only captions but also
sentences from the original PubMed paper that mentions the image. We also manually curate few-shot
examples in the prompt to demonstrate how to generate high-quality conversations based on the
provided caption and context. See Appendix B for the prompt and few-shot examples. To collect
image captions and their context, we filter PMC-15M to retain the images that only contain a single
plot. From them, we sample 60K image-text pairs from the five most common imaging modalities:
CXR (chest X-ray), CT (computed tomography), MRI (magnetic resonance imaging), histopathology,
and gross (i.e., macroscopic) pathology. We then extract sentences that mention the image from the
original PubMed paper as additional context to the caption, inspired by the observations that external
knowledge helps generalization [20, 25].
An example of instruction-following data is shown in Figure 1 shows, and the data statistics is shown
Figure 2. We have produced three versions of instruct data when iteratively improving the data
3

Biomedical Visual Instruction-Following Example
Caption:
Figure 4: Contrast-enhanced CT scan of the chest for patient #1. A
large cavitating lesion was identified in the right upper lobe with pleu-
ral involvement and likely extension into the chest wall. In addition,
right hilar lymphadenopathy was suspected. This was the primary
malignant lesion in this patient.
Sentences with in-line figure mentions:
- Computerized tomography ( CT ) scans of the chest ( Figure 4 )
were obtained.
- ( Figure 4 ) and demonstrated a large cavitating lesion in the pos-
terior aspect of the right upper lobe with probable pleural involve-
ment likely representing the primary lesion.
Conversation:
User: Where is the lesion located?
Assistant: The lesion is located in the posterior aspect of the right upper lobe of the lung.
User: Is it extending into the chest wall?
Assistant: Yes, the CT scan suggests that the large cavitating lesion in the right upper lobe is
likely extending into the chest wall.
User: Can you explain more about the cavitating lesion?
Assistant: Certainly. A cavitating lesion refers to an abnormal mass or growth within the lung
tissue that has a cavity or hollow space inside it. This cavity can be formed due to the necrosis
(death) of the tissue inside the lesion, leading to the breakdown and disintegration of the
affected tissue. Cavitating lesions can be caused by various factors, such as infections (like
tuberculosis or fungal infections), autoimmune diseases, or malignancies (cancer). In the context
of the CT scan provided, the large cavitating lesion in the right upper lobe of the lung is
suggestive of a malignant (cancerous) growth. The cavity within the lesion may be a result of the
rapid and uncontrolled growth of cancer cells, which can lead to the destruction of surrounding
healthy tissue. It’s important to note that further diagnostic tests, such as a biopsy, would be
required to confirm the malignancy and determine the specific type of cancer involved.
Figure 1: An instance of our GPT-4 generated instruction-following data. Top: The figure and caption
were extracted from a PubMed Central full-text article [35], along with the corresponding citances
(mentions of the given figure in the article). Bottom: The instruction-following data generated by
GPT-4 using the text only (caption and citances). Note that the image is not used to prompt GPT-4;
we only show it here as a reference.
quality: (i) 60K-IM. The aforemenioned dataset that considers inline mentions (IM) as the context.
(ii) 60K. A dataset of similar size (60K samples) without IM in self-instruct generation. (iii) 10K. A
smaller dataset (10 samples) without IM. They are used to ablate our data generation strategies and
their impact on trained LLaVA-Med in experiments.
4
Adapting Multimodal Conversational Models to the Biomedical Domain
We employ LLaVA,a general-domain multimodal conversation model [24], as the initial general-
domain LM, and continuously train the model to the biomedical domain. The same network architec-
ture is utilized, where a linear projection layer connects the vision encoder and the language model.
For LLaVA-Med model training, we use a two-stage procedure, illustrated in Figure 3.
Stage 1: Biomedical Concept Feature Alignment.
To balance between concept coverage and
training efficiency, we filter PMC-15M to 600K image-text pairs. These pairs are converted to
instruction-following data using a naive expansion method: instructions simply presents the task of
describing the image. For each sample, given the language instruction and image input, we ask the
model to predict the original caption. In training, we keep both the visual encoder and LM weights
frozen, and only update the projection matrix. In this way, the image features of vast novel biomedical
4

describe
have
show
affect
see
receive
provide
tell
explain
line
undergo
interpret
surround
infiltrate
cause
appearance
difference
finding
size
type
effect
accuracy
rate
improvement
uptake
volume
lung
number
structure
patient
tube
pacemaker
difference
improvement
treatment
kind
type
information
size
difference
cyst
luman
kind
procedure
performance
nodule
vessel
tissue
compression
show
indicate
represent
use
provide
reveal
have
play
suggest
include
compare
section
lesion
tissue
cell
area
presence
condition
abnormality
micrometer
area
region
value
hematoxylin
stain
staining
imaging
information
representation
view
detail
presence
lesion
mass
finding
appearance
cause
magnification
shape
role
presence
image
tomography
score
(a) Instruction
(b) Responses
Chest X-Ray
MRI
Histology
Gross pathology
CT
(c) Frequencies of images and QA pairs on the five domains.
Figure 2: The data statistics of biomedical multimodal instruction-following data: (a,b) The root
verb-noun pairs of instruction and responses, where the inner circle of the plot represents the root
verb of the output response, and the outer circle represents the direct nouns. (c) The distribution of
images and QA pairs on the five domains, one image is shown per domain. The domain example
images are from [3, 33, 4, 29, 48].
Stage 1
Stage 2
LLaVA
LLaVA-Med
Medical Concept Alignment
Medical Instruction Tuning
1 epoch on 600K samples
3 epochs on 60K samples
7 Hours
8 Hours
Downstream
q Medical VQA
o
VQA-Radiology
o
SLAKE
o
Pathology-VQA
q Medical Visual Chat
Figure 3: LLaVA-Med was initialized with the general-domain LLaVA and then continuously trained
in a curriculum learning fashion (first biomedical concept alignment then full-blown instruction-
tuning). We evaluated LLaVA-Med on standard visual conversation and question answering tasks.
visual concepts can be aligned to their textual word embeddings in the pre-trained LM. This stage can
be understood as expanding the vocabulary of aligned image-text tokens to the biomedical domain.
Stage 2: End-to-End Instruction-Tuning.
We only keep the visual encoder weights frozen, and
continue to update both the pre-trained weights of the projection layer and LM. To train the model to
follow various instructions and complete tasks in a conversational manner, we develop a biomedical
chatbot by fine-tuning our model on the biomedical language-image instruction-following data
collected in Section 3. As demonstrated in the experiments to be described later, the LLaVA-Med
model at this stage is able to not only be served as a biomedical visual assistant to interact with
users, but also achieve good zero-shot task transfer performance when evaluated on well-established
biomedical VQA datasets.
Fine-tuning to Downstream Datasets.
For some specific biomedical scenarios, there is a need of
developing highly accurate and dataset-specific models to improve the service quality of the assistant.
We fine-tune LLaVA-Med after the two-stage training on three biomedical VQA datasets [27],
covering varied dataset sizes and diverse biomedical subjects. Given a biomedical image as context,
5

multiple natural language questions are provided, the assistant responds in free-form text for both the
close-set and open-set questions, with a list of candidate answers constructed in the prompt for each
close-set question.
Discussion.
We discuss three favorable properties/implications of LLaVA-Med: (i) Affordable
development cost. Instead of scaling up data/model for the best performance, we aim to provide
affordable and reasonable solutions with low development cost: it takes 7 and 8 hours for stage 1 and 2
on 8 40G A100 GPUs, respectively (see Table 5 for detailed numbers). (ii) A recipe for many domains.
Though this paper focuses on biomedical domains, the proposed adaptation procedure is generalizable
to other vertical domains such as gaming and education, where novel concepts and domain knowledge
are needed to build a helpful assistant. Similar to the don’t stop pre-training argument in [11], we
consider a scalable pipeline to create domain-specific instruct data from large unlabelled data, and
advocate don’t stop instruction-tuning to build customized LMM. (iii) Low serving cost. While the
model size of general LMM can be giant and serving cost can be prohibitively high, customized LMM
has its unique advantages in low serving cost. (iv) Smooth Model Adaptation. Alternatively, the
network architecture allows us to initialize the vision encoder from BioMedCLIP [49], or initialize
the language model from Vicuna [43], which may lead to higher performance. However, adapting
from LLaVA smooth adaptation as a chatbot, where model’s behaviors transit from layperson to a
professional assistant that is able to provide helpful domain-specific response.
5
Experiments
We conduct experiments to study two key components, the quality of the produced multimodal
biomedical instruction-following data, and performance of LLaVA-Med. We consider two research
evaluation settings: (1) What is the performance of LLaVA-Med as an open-ended biomedcal visual
chatbot? (2) How does LLaVA-Med compare to existing methods on standard benchmarks? To
clarify, throughout the entire experiments, we only utilize the language-only GPT-4.
5.1
Biomedical Visual Chatbot
To evaluate the performance of LLaVA-Med on biomedical multimodal conversation, we construct
an evaluation dataset with 193 novel questions. For this test dataset, we randomly selected 50 unseen
image and caption pairs from PMC-15M, and generate two types of questions: conversation and
detailed description. The conversation data is collected using the same self-instruct data generation
pipeline as for the 2nd stage. Detailed description questions were randomly selected from a fixed set
[24] of questions to elicit detailed description responses.
We leverage GPT-4 to quantify the correctness of the model answer to a question when given the
image context and caption. GPT-4 makes a reference prediction, setting the upper bound answer
for the teacher model. We then generate response to the same question from another LMM. Given
responses from the two assistants (the candidate LMM and GPT-4), the question, figure caption, and
figure context, we ask GPT-4 to score the helpfulness, relevance, accuracy, and level of details of the
responses from the two assistants, and give an overall score on a scale of 1 to 10, where a higher score
indicates better overall performance. GPT-4 is also asked to provide a comprehensive explanation the
evaluation, for us to better understand the models. We then compute the relative score using GPT-4
reference score for normalization.
Question Types
Domains
Overall
Conversation
Description
CXR
MRI
Histology
Gross
CT
(Question Count)
(143)
(50)
(37)
(38)
(44)
(34)
(40)
(193)
LLaVA
39.4
26.2
41.6
33.4
38.4
32.9
33.4
36.1
LLaVA-Med
Stage 1
22.6
25.2
25.8
19.0
24.8
24.7
22.2
23.3
10K
42.4
32.5
46.1
36.7
43.5
34.7
37.5
39.9
60K
53.7
36.9
57.3
39.8
49.8
47.4
52.4
49.4
60K-IM
55.1
36.4
56.2
40.4
52.7
51.8
50.1
50.2
Table 1: Performance comparison of mulitmodal chat instruction-following abilities, measured by the
relative score via language GPT-4 evaluation.
6

Visual input example, Biomedical image from [6]
Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7398722/
User
What is shown in this image?
LLaVA-Med
The image is a chest X-ray (CXR) that shows bilateral patchy infiltrates, which are areas of increased
opacity in the lungs. These infiltrates can be indicative of various lung conditions, such as infections,
inflammation, or other lung diseases.
User
Are there any devices implanted in this patient?
LLaVA-Med
Yes, it appears that the patient has several devices implanted. These devices may include endotracheal
tubes, central venous catheters, and/or pulmonary artery catheters. These devices are typically used for
monitoring and managing various medical conditions, as well as for providing support during surgical
procedures.
User
What is shown in this image?
LLaVA
The image shows a ribcage with wires coming out of it, indicating that it is likely a medical illustration or
diagram.
User
Are there any devices implanted in this patient?
LLaVA
Yes, there are several devices protruding from the patient’s chest, which are likely medical devices used
for treatment or monitoring.
User
What is shown in this image?
Language GPT-4
The image shows a chest x-ray taken on Day 2 of a patient’s admission after intubation. You can see the
endotracheal tube marked by a yellow line, and rapidly worsening ground-glass opacities throughout the
lungs. There is also relative subpleural sparing, which is indicated by red arrows.
User
Are there any devices implanted in this patient?
Language GPT-4
Yes, there is an endotracheal tube implanted in the patient, as indicated by the yellow line in the image.
Table 2: Example comparison of medical visual chat and reasoning capabilities. The language-only
GPT-4 is considered as the performance upper bound, as the golden captions and inline mentions
are fed into GPT-4 as the context, without requiring the model to understand the raw image.
The results are reported in Table 1. LLaVA-Med with Stage-1 training alone is insufficient as a
chatbot, as it loses its ability to follow diverse instructions, though biomedical concept coverage is
improved. LLaVA-Med with the full two-stage training consistently outperforms the general domain
LLaVA, and training with larger instruct data (from 10K to 60K samples) leads to higher performance.
When inline mentions are considered in self-instruct, the generated data 60K-IM slightly improves
the chat ability. The results demonstrate the effectiveness of the strategies in biomedical instruction-
following data collection as well as the value of dataset assets. Overall, for the best LLaVA-Med,
it matches the 50.2% performance of GPT-4. Note that GPT-4 generates response by considering
ground-truth caption and golden inline mentions, without understanding the images. Though not a
fair comparison between LMMs and GPT-4, GPT-4 is a consistent and reliable evaluation tool.
In Table 2, we provide examples on the biomed visual conversations of different chatbots. LLaVA-
Med precisely answers the questions with biomedical knowledge, while LLaVA behaves like a
layperson, who hallucinate based on commonsense. Since the multimodal GPT-4 is not publicly
available, we resort to language-only GPT-4 for comparison. We feed golden captions and inline
mentions into GPT-4 as the context, it generates knowledgeable response through re-organizing the
information in the conversational manner.
5.2
Performance on Established Benchmarks
Dataset Description.
We train and evaluate LLaVA-Med on three biomedical VQA datasets. The
detailed data statistics are summarized in Table 3.
• VQA-RAD [16] contains 3515 QA pairs generated by clinicians and 315 radiology images that
are evenly distributed over the head, chest, and abdomen. Each image is associated with multiple
questions. Questions are categorized into 11 categories: abnormality, attribute, modality, organ
system, color, counting, object/condition presence, size, plane, positional reasoning, and other.
Half of the answers are closed-ended (i.e., yes/no type), while the rest are open- ended with either
one-word or short phrase answers.
7

VQA-RAD
SLAKE
PathVQA
Dataset
Train
Test
Train
Val
Test
Train
Val
Test
# Images
313
203
450
96
96
2599
858
858
# QA Pairs
1797
451
4919
1053
1061
19,755
6279
6761
# Open
770
179
2976
631
645
9949
3144
3370
# Closed
1027
272
1943
422
416
9806
3135
3391
Table 3: Dataset statistics. For SLAKE, only the English subset is considered for head-to-head
comparison with existing methods.
• SLAKE [23] is a Semantically-Labeled Knowledge-Enhanced dataset for medical VQA. It
consists of 642 radiology images and over 7000 diverse QA pairs annotated by experienced
physicians, where the questions may involve external medical knowledge (solved by provided
medical knowledge graph), and the images are associated with rich visual annotations, including
semantic segmentation masks and object detection bounding boxes. Besides, SLAKE includes
richer modalities and covers more human body parts than the currently available dataset, including
brain, neck, chest, abdomen, and pelvic cavity. Note SLAKE is bilingual dataset with English
and Chinese. When compared with existing methods, we only consider the English subset.
• PathVQA [13] is a dataset of pathology images. It contains a total of 4998 pathology images
with 32,799 QA pairs. Every image has several questions that relate to multiple aspects such as
location, shape, color, appearance, etc. The questions are categorized into two types, with several
varieties: open-ended questions such as why, what, how, where, etc., and closed-ended questions.
Evaluation Metrics.
For the closed-set questions, we report the accuracy. For open-set questions,
we use recall to evaluate the ratio that ground-truth tokens appear in the generated sequences. In
the literature, the unique answers in the training set are considered as the answer candidates, from
which the models can select to predict answers for testing questions. Since we do not provide any
constraint for the responses to open-set questions, our formulation is closer to open-set nature, but is
intrinsically harder.
Comparisons with SoTA.
We compare LLaVA-Med with the general domain LLaVA and existing
representative methods in Table 4 (a). First, All LLaVA-Med variants outperform LLaVA. While
the difference of language model initialization from LLaVA or Vicuna is minor, the initialization of
vision encoder from BioMed CLIP is slightly better than from general-domain CLIP. Second, the
fine-tuning performance of LLaVA-Med is higher than supervised SoTA on the closed-set questions
on VQA-RAD and PathVQA. This validates LLaVA-Med’s strong ability in following instruction
to complete biomedical tasks, when clear instructions are provided (e.g., , yes or no). Third, for
open-set questions, LLaVA-Med achieves SoTA on SLAKE, while its performance is limited on
other datasets, especially compared with existing methods. This is perhaps because the open-set
biomedical questions can be ambiguous without constraining their excepted answer options.
Ablation Studies.
To study the impact of our curated instruction data and hyper-parameters in the
training pipeline, we report the performance of different model variants in Table 4 (b). Several findings
are confirmed: (i) LLaVA-Med consistently outperforms LLaVA by a large margin, indicating the
effectiveness of our biomedical domain-specific adaptation. The performance gaps on zero-shot
are larger than that in fine-tuned settings, showing that LLaVA-Med is clearly a better option than
LLaVA when deploying one model for various scenarios in the wild. (ii) Training longer in Stage 1
improves zero-shot transfer, but Stage 1 alone is not sufficient, because the single image captioning
instruction in Stage 1 may encourage the model to lose its ability in follow diverse instructions. (iii)
Instruction-following data in Stage 2 is critical, and the performance is generally improved, when
the instruct data amount increases from 10K to 60K. The 60K-IM data provides the best averaged
zero-shot and fine-tuned performance, respectively, validating the effectiveness of considering inline
mention as external knowledge in data creation. (iv) Fine-tuning longer on downstream datasets
till 9 epochs benefits the performance, especially on checkpoints with 3-epoch training in Stage 2.
Increasing language model size from 7B to 13B improves the overall zero-shot performance and
fine-tuned performance. We suggest practitioners to choose the appropriate quality-cost trade-off, by
referring to the running time in Table 5.
8

VQA-RAD
SLAKE
PathVQA
Method
Ref
Open Closed
Ref
Open Closed
Ref
Open Closed
Supervised finet-tuning results with our own experiment runs
LLaVA
50.00
65.07
78.18
63.22
7.74
63.20
LLaVA-Med (From LLaVA)
61.52
84.19
83.08
85.34
37.95
91.21
LLaVA-Med (From Vicuna)
64.39
81.98
84.71
83.17
38.87
91.65
LLaVA-Med (BioMed CLIP)
64.75
83.09
87.11
86.78
39.60
91.09
Representative & SoTA methods with numbers reported in the literature
VL Encoder–Decoder [5]
71.49
82.47
71.49
85.61
Q2ATransformer [26]
79.19
81.20
54.85
88.85
Prefix T. Medical LM [41]
84.30
82.01
40.00
87.00
PubMedCLIP [8]
60.10
80.00
78.40
82.50
BiomedCLIP [49]
67.60
79.80
82.05
89.70
M2I2 [22]
66.50
83.50
74.70
91.10
36.30
88.00
(a) Comparison with prior state-of-the-art supervised methods. For open-ended questions, prior methods still
formulate the problem as classification among distinct answers in the training set, which may overestimate their
generalizability as these datasets are unusual in that the test answers are almost always present in training.
LLaVA-Med Model Variants
VQA-RAD
SLAKE
PathVQA
Average
Instruct
Stage 1 Stage 2 FT Open Closed Open Closed Open Closed
CLIP Vision Encoder [36], 7B Language Model
0
1
0
0
15.27
12.50 18.55
13.46
6.26
13.51
13.26
0
3
0
0
15.33
15.44 23.61
15.38
6.35
14.74
15.14
10K
1
3
0
25.79
57.35 31.50
51.68
8.49
59.66
39.08
10K
3
3
0
28.44
59.56 22.63
43.99
5.40
52.67
35.45
10K
1
3
1
36.39
55.88 71.64
56.49 25.50
82.87
54.79
10K
1
3
3
18.59
55.51 78.60
63.46 34.02
86.94
56.19
60K
1
1
0
29.80
55.15 38.08
50.00 11.70
59.66
40.73
60K
1
3
0
29.67
60.29 35.53
53.85 11.76
53.20
40.72
60K
1
3
1
22.63
58.09 72.75
54.33 24.19
71.60
50.60
60K
1
3
3
54.12
64.71 79.33
64.90 17.18
71.37
58.60
60K-IM
1
1
0
29.67
61.40 38.44
52.40 11.41
56.24
41.59
60K-IM
1
3
0
28.23
61.40 39.17
52.16 12.30
54.05
41.22
60K-IM
1
3
1
28.61
56.25 70.58
54.57 11.17
59.19
46.73
60K-IM
1
3
3
55.50
66.54 80.57
64.18 35.88
89.15
65.30
60K-IM
1
3
9
66.26
80.88 82.30
84.86 37.59
91.54
73.90
60K-IM
1
3
15 61.53
84.19 83.08
85.34 37.95
91.21
73.88
60K-IM
1
3
18 61.37
81.25 84.24
83.17 37.88
91.39
73.22
CLIP Vision Encoder [36], 13B Language Model
60K-IM
1
3
0
31.66
61.40 37.71
49.76 11.34
49.63
40.25
60K-IM
1
3
9
64.58
77.94 84.97
85.58 38.82
92.39
74.05
BioMed CLIP Vision Encoder [49], 7B Language Model
60K-IM
1
3
0
37.84
60.66 39.73
54.33 11.65
49.07
42.21
60K-IM
1
3
9
64.75
83.09 87.11
86.78 39.60
91.09
75.40
LLaVA
0
0
0
20.74
59.19 26.82
50.24
8.74
45.65
35.23
(b) Ablation studies with varying number of training epochs at different stages. “FT” is Fine-Tuning. 60K-IM
indicates the instruct data generated with inline mentions. The gray rows are zero-shot performance of LLaVA-
Med trained with different instruct data, they are selected to show in subtable (a).
Table 4: Quantitative results on three established biomedical VQA datasets. For open-set questions,
we report the recall for our free-form text generation method in column Open. For closed-set
questions, we report the accuracy in column Closed. Bold indicates LLaVA-Med achieves new SoTA.
9

Stage 1
Stage 2
VQA-RAD
SLAKE
PathVQA
1
3
Instruct
1
3
1
3
1
3
1
3
6.8
19.4
10K
0.6
1.8
0.3
0.6
0.6
1.0
1.0
2.5
60K
2.6
8.0
Table 5: Running time (hours) for 1 and 3-epoch training at each stage, with batch size 128 on eight
A100 GPUs.
Biomedical image from the SLAKE Bilingual dataset. [23]
User
这张图片的成像方式是什么?
LLaVA-Med
The image is an abdominal computed tomography (CT)
scan, which is a cross-sectional view of the patient’s ab-
domen. The image is taken in the portal phase, which
means it is captured after the administration of contrast ma-
terial to enhance the visibility of certain structures within
the abdomen.
Answer
CT
User
这张图片展示的是核磁共振的哪种类型?
LLaVA
The image shows a typical appearance of a nodular lesion
on T1-weighted MR images. The lesion appears hyper-
intense, which means it is brighter than the surrounding
tissue. This can be indicative of various types of lesions,
and further evaluation would be needed to determine the
specific nature and cause of the lesion.
Answer
T1
Table 6: Examples on zero-shot on Chinese questions. Though LLaVA-Med training data itself does
not include Chinese, it is able to correctly understand the Chinese questions.
Case Study I: Zero-shot on Chinese Questions.
For the LLaVA-Med trained on 60K-IM data, we
provide Chinese questions on SLAKE dataset. Though LLaVA-Med training does not include Chinese
instruction-following data, we show in Table 6 that LLaVA-Med is able to correctly understand the
Chinese questions and respond the correct answers, probably due to the multilingual knowledge
learned in LLaMA/Vicuna. Existing models will fail when zero-shot transfer cross languages.
6
Conclusions
We present LLaVA-Med, a large language-and-vision model for the biomedical domain. To create
this model, we create high-quality biomedical language-image instruction-following dataset using
a self-instruct approach to build a data curation pipeline using language-only GPT-4 and external
knowledge. LLaVA-Med demonstrates strong excellent chat abilities with domain knowledge, and
outperforms previous supervised SoTA on three VQA datasets on certain metrics with subsequent
fine-tuning.
While we believe that LLaVA-Med represents a significant step towards building a useful biomedical
visual assistant, we note that LLaVA-Med is limited by hallucinations and weak in-depth reasoning
common to many LMMs. Future work is directed toward improving quality and reliability.
References
[1] Clinical Camel. https://wanglab.ml/clinical_camel.html, 2023. 2
[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy
Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a
laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. 1
[3] Malek Ayoub, Megan Quamme, Abdul-Rahman K Abdel-Reheem, Poe Lwin, and Megan K
Quamme. Covid or not covid? a great mimicker behind the smoke screen. Cureus, 13(11),
2021. 5
[4] Bappy Basak, Alexander Haragan, Michael Shackcloth, and Joyce Thekkudan. Chondromyxoid
fibroma of the rib: A rare benign tumor with potential for local recurrence. Cureus, 13(10),
2021. 5
10

[5] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair. Vision–
language model for visual question answering in medical imagery. Bioengineering, 2023. 3,
9
[6] Anchit Bharat, Nikita Jain, Belaal Sheikh, Hafiz Jeelani, and Maryna Shayuk. Vaping-induced
lung injury: An uncharted territory. Cureus, 12, 07 2020. 7
[7] Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer,
Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle,
et al. Making the most of text semantics to improve biomedical vision–language processing. In
ECCV. Springer, 2022. 2
[8] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip
benefit visual question answering in the medical domain? In Findings of the Association for
Computational Linguistics: EACL 2023, pages 1151–1163, 2023. 2, 3, 9
[9] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-
language pre-training: Basics, recent advances, and future trends. Foundations and Trends® in
Computer Graphics and Vision, 2022. 1
[10] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan
Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining
for biomedical natural language processing. ACM Transactions on Computing for Healthcare
(HEALTH), 3(1):1–23, 2021. 2
[11] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. arXiv
preprint arXiv:2004.10964, 2020. 6
[12] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser,
Alexander Löser, Daniel Truhn, and Keno K Bressem. Medalpaca–an open-source collection of
medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. 2
[13] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+
questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. 3, 8
[14] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and
predicting hospital readmission. arXiv preprint arXiv:1904.05342, 2019. 2
[15] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P
Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified
publicly available database of chest radiographs with free-text reports. Scientific data, page 317,
2019. 2
[16] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of
clinically generated visual questions and answers about radiology images. Scientific data, 2018.
7
[17] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and
Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical
text mining. Bioinformatics, 36(4):1234–1240, 2020. 2
[18] Peter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai
chatbot for medicine. New England Journal of Medicine, 388(13):1233–1239, 2023. 2
[19] Peter Lee, Carey Goldberg, and Isaac Kohane. The ai revolution in medicine: Gpt-4 and beyond.
2023. 2
[20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented
generation for knowledge-intensive NLP tasks. NeurIPS, 2020. 3
11

[21] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang,
Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEVATER: A bench-
mark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on
Datasets and Benchmarks, 2022. 1
[22] Pengfei Li, Gang Liu, Lin Tan, Jinying Liao, and Shenjun Zhong. Self-supervised vision-
language pretraining for medical visual question answering. arXiv preprint arXiv:2211.13594,
2022. 3, 9
[23] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-
labeled knowledge-enhanced dataset for medical visual question answering. In International
Symposium on Biomedical Imaging (ISBI). IEEE, 2021. 8, 10
[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485, 2023. 1, 2, 4, 6
[25] Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan
Li. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint
arXiv:2301.07094, 2023. 3
[26] Yunyi Liu, Zhanyu Wang, Dong Xu, and Luping Zhou. Q2atransformer: Improving medical
vqa via an answer querying decoder. arXiv preprint arXiv:2304.01611, 2023. 3, 9
[27] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought
chains for science question answering. Advances in Neural Information Processing Systems,
2022. 5
[28] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.
Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings
in Bioinformatics, 2022. 2, 3
[29] Hassan Mirmohammad Sadeghi, Abbas Karimi, Samira Derakhshan, Pouyan Aminishakib, and
Kiarash Parchami. Conventional osteosarcoma of the mandible: Report of a rare case. Clinical
Case Reports, 9(9):e04843, 2021. 5
[30] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Ca-
pabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
2
[31] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2022. 2
[32] OpenAI. GPT-4 technical report. https://arxiv.org/abs/2303.08774, 2023. 1, 2
[33] Kyriakos A Papavasiliou, Dimitrios Stamiris, Stavros Stamiris, Antonia Bintoudi, and Elefthe-
rios Tsiridis. Quadratus femoris partial tear secondary to occult ischiofemoral impingement.
Journal of Orthopaedic Case Reports, 11(9):7, 2021. 5
[34] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning
with GPT-4. arXiv preprint arXiv:2304.03277, 2023. 2
[35] Roger Kevin Pringle and Lawrence H Wyatt. The appropriate use of radiography in clinical
practice: a report of two cases of biomechanical versus malignant spine pain. Chiropractic &
Osteopathy, 14(1):1–8, 2006. 4
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 9
[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 2019. 3
12

[38] George Shih, Carol C Wu, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, Tessa S
Cook, Arjun Sharma, Judith K Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg, et al.
Augmenting the national institutes of health chest radiograph dataset with expert annotations of
possible pneumonia. Radiology: Artificial Intelligence, 2019. 2
[39] Chang Shu, Baian Chen, Fangyu Liu, Zihao Fu, Ehsan Shareghi, and Nigel Collier. Visual
med-alpaca: A parameter-efficient biomedical llm with visual capabilities. 2023. 2
[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3
[41] Tom van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees GM Snoek,
and Marcel Worring. Open-ended medical visual question answering through prefix tuning of
language models. arXiv preprint arXiv:2303.05977, 2023. 3, 9
[42] A Venigalla, J Frankle, and M Carbin. BiomedLM: a domain-specific large language model for
biomedical text. MosaicML. Accessed: Dec, 23, 2022. 3
[43] Vicuna. Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatgpt quality. https:
//vicuna.lmsys.org/, 2023. 3, 6
[44] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu.
Huatuo: Tuning llama model with chinese medical knowledge, 2023. 2
[45] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further
finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023. 2
[46] Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Qian Wang, and Dinggang
Shen. Doctorglm: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint
arXiv:2304.01097, 2023. 2
[47] Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical
chat model fine-tuned on llama model using medical domain knowledge. arXiv preprint
arXiv:2303.14070, 2023. 2
[48] Mansoor Zafar, Abdul Wahab Paracha, Muteeb Ashraf, Tila Muhammad, Mark Whitehead,
Muhammad Toqeer, and Abdul Paracha. Delayed spontaneous regression of metastatic gastric
cancer: A case report of a rare finding. Cureus, 13(12), 2021. 5
[49] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh
Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for
biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2023. 2, 3, 6, 9
13

A
Data
Instructions for brief image description.
The list of instructions used to briefly describe the image
content are shown in Table 7. They present the same meaning with natural language variance.
• "Describe the image concisely."
• "Provide a brief description of the given image."
• "Offer a succinct explanation of the picture presented."
• "Summarize the visual content of the image."
• "Give a short and clear explanation of the subsequent image."
• "Share a concise interpretation of the image provided."
• "Present a compact description of the photo’s key features."
• "Relay a brief, clear account of the picture shown."
• "Render a clear and concise summary of the photo."
• "Write a terse but informative summary of the picture."
• "Create a compact narrative representing the image presented."
Table 7: The list of instructions for brief image description.
Instructions for detailed image description.
The list of instructions used to describe the image
content in detail are shown in Table 8. They present the same meaning with natural language variance.
• "Describe the following image in detail"
• "Provide a detailed description of the given image"
• "Give an elaborate explanation of the image you see"
• "Share a comprehensive rundown of the presented image"
• "Offer a thorough analysis of the image"
• "Explain the various aspects of the image before you"
• "Clarify the contents of the displayed image with great detail"
• "Characterize the image using a well-detailed description"
• "Break down the elements of the image in a detailed manner"
• "Walk through the important details of the image"
• "Portray the image with a rich, descriptive narrative"
• "Narrate the contents of the image with precision"
• "Analyze the image in a comprehensive and detailed manner"
• "Illustrate the image through a descriptive explanation"
• "Examine the image closely and share its details"
• "Write an exhaustive depiction of the given image"
Table 8: The list of instructions for detailed image description.
14

B
Prompts
Prompting GPT-4 to generate medical visual instruction-following data
messages = [ {"role":"system", "content": """You are an AI assistant specialized in
biomedical topics.
You are provided with a text description (Figure Caption) of a figure image from a biomedical
research paper. In some cases, you may have additional text (Figure Context) that mentions
the image. Unfortunately, you don’t have access to the actual image.
Below are requirements for generating the questions and answers in the conversation:
- Avoid quoting or referring to specific facts, terms, abbreviations, dates, numbers, or
names, as these may reveal the conversation is based on the text information, rather than
the image itself. Focus on the visual aspects of the image that can be inferred without
the text information.
- Do not use phrases like "mentioned", "caption", "context" in the conversation. Instead,
refer to the information as being "in the image."
- Ensure that questions are diverse and cover a range of visual aspects of the image.
- The conversation should include at least 2-3 turns of questions and answers about the
visual aspects of the image.
- Answer responsibly, avoiding overconfidence, and do not provide medical advice or
diagnostic information. Encourage the user to consult a healthcare professional for
advice."""}
]
for sample in fewshot_samples:
messages.append({"role":"user", "content":sample[‘context’]})
messages.append({"role":"assistant", "content":sample[‘response’]}
)
messages.append({"role":"user", "content":query})
Figure 4: messages we use to prompt GPT-4 to generate medical visual instruction-following
data. Manually curated few-shot examples are included in the prompt, where each example has
input sample[‘context’] and output sample[‘response’]. Please see Figure 5 for one of the
few-shot examples.
15

One of the few-shot examples used in the prompt
Example input:
Figure Caption:
Figure 2: Chest X-ray. Cardiomegaly with diffuse bilateral interstitial infiltrates and a
right-sided pleural effusion.
Figure Context:
- His respiratory status continued to worsen and on hospital day number 14 , he required
intubation and mechanical ventilation for hypoxemic respiratory failure ( Figure 2 ) .
Example output:
User: Which side of the lung has a pleural effusion?
Assistant: The right side of the lung has a pleural effusion.
User: According to the CT image, does the patient has cardiomegaly?
Assistant: The image appeares to be a chest x-ray. However, it
indicates that the patient has cardiomegaly, which means an enlarged
heart. Additionally, there are diffuse bilateral interstitial
infiltrates and a right-sided pleural effusion present in the image.
User: Additional Info: The patient’s respiratory status continued
to worsen and on hospital day number 14, requiring intubation and
mechanical ventilation for hypoxemic respiratory failure.
Assistant: Thank you for providing additional information about
the patient’s condition. Based on the information you provided, it
appears that the patient’s respiratory status continued to worsen and
on hospital day number 14, they required intubation and mechanical
ventilation for hypoxemic respiratory failure. This suggests that
the patient’s lung function has significantly declined, and they
may be experiencing respiratory distress. It is important for the
medical team to closely monitor the patient’s condition and provide
appropriate treatment to manage their symptoms and improve their lung
function.
Figure 5: One of the few-shot examples used in our prompt to construct medical visual instruction-
following data. “Figure Context“ in the input provides additional context by including sentences from
the corresponding PubMed paper that mention the figure.
16

Visual input example, Biomedical image:
User
What is this image about?
LLaVA-Med
The image is a coronal view of a computed tomography (CT) scan of the abdomen and
pelvis. It provides a detailed view of the internal organs and structures within the abdominal
and pelvic cavities.
User
What is abnormal about this image?
LLaVA-Med
The abnormality in this image is the presence of a large left-sided pelvic mass. This mass
could be due to various reasons, such as a tumor, cyst, or other growth. Further evaluation
and tests would be needed to determine the exact cause and appropriate treatment for this
abnormality.
User
What is this image about?
LLaVA
The image is a medical illustration showing a person’s internal anatomy, especially focusing
on their large and small intestines, stomach, liver and kidneys. The organs are highlighted
in different colors, making it easy to identify each organ
User
What is abnormal about this image?
LLaVA
As a visual AI assistant, I cannot determine the specific abnormality on the chest X-ray
based on the provided image description. The image description mainly focuses on the
organs and their colors...
Table 9: Example comparison of medical visual chat and reasoning capabilities.
17

