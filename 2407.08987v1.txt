Parameter inference from a non-stationary unknown process
Parameter inference from a non-stationary unknown process
Kieran S. Owens1, 2 and Ben D. Fulcher1, 2
1)School of Physics, The University of Sydney, Camperdown NSW 2006, Australia
2)Centre for Complex Systems, The University of Sydney, Camperdown NSW 2006, Australia.
(*Electronic mail: ben.fulcher@sydney.edu.au)
(Dated: 15 July 2024)
Non-stationary systems are found throughout the world, from climate patterns under the influence of variation in car-
bon dioxide concentration, to brain dynamics driven by ascending neuromodulation. Accordingly, there is a need
for methods to analyze non-stationary processes, and yet most time-series analysis methods that are used in practice,
on important problems across science and industry, make the simplifying assumption of stationarity. One important
problem in the analysis of non-stationary systems is the problem class that we refer to as Parameter Inference from a
Non-stationary Unknown Process (PINUP). Given an observed time series, this involves inferring the parameters that
drive non-stationarity of the time series, without requiring knowledge or inference of a mathematical model of the un-
derlying system. Here we review and unify a diverse literature of algorithms for PINUP. We formulate the problem, and
categorize the various algorithmic contributions into those based on: (1) dimension reduction; (2) statistical time-series
features; (3) recurrence quantification analysis; (4) prediction error; (5) phase-space partitioning; and (6) Bayesian
inference. This synthesis will allow researches to identify gaps in the literature and will enable systematic comparisons
of different methods. We also demonstrate that the most common systems that existing methods are tested on—notably
the non-stationary Lorenz process and logistic map—are surprisingly easy to perform well on using simple statistical
features like windowed mean and variance, undermining the practice of using good performance on these systems as
evidence of algorithmic performance. We then identify more challenging problems that many existing methods per-
form poorly on and which can be used to drive methodological advances in the field. Our results unify disjoint scientific
contributions to analyzing non-stationary systems and suggest new directions for progress on the PINUP problem and
the broader study of non-stationary phenomena.
The analysis of time-series data is important for a range
of fields, including physics, neuroscience, ecology, and fi-
nance. Time-series analysis methods commonly assume
that the measured process is stationary, i.e., that certain
properties of the process, such as mean and variance, are
constant across time. And yet many important processes
are non-stationary on relevant dynamical timescales. For
example, time series measured from electrical activity of
the human brain during sleep exhibit considerable varia-
tion in statistical properties across wakefulness and differ-
ent stages of sleep. Thus there is a need for methods that
can quantify non-stationary dynamical variation. Here we
consider the specific problem of inferring the parameters
that drive non-stationarity of a time series, without requir-
ing knowledge or inference of a mathematical model of the
underlying system. We call this problem Parameter Infer-
ence from a Non-stationary Unknown Process (PINUP).
We make two main contributions. First, we review and
categorize the algorithmic approaches to PINUP across
a fragmented literature, consolidating a diverse body of
methodological research for the first time. Unifying the
terminology of the problem and providing an overview of
the scientific methods that have been developed to date
will allow researchers to take a unified approach to target-
ing gaps in the interdisciplinary literature, thus facilitat-
ing progress on PINUP. Second, we show that the systems
most commonly used for benchmarking—the Lorenz pro-
cess and logistic map—admit trivial algorithmic solutions,
undermining the practice of presenting good performance
on these problems as evidence of algorithmic progress. We
present more challenging test problems, which many ex-
isting methods perform poorly on, and that can be used
to drive improvements in PINUP algorithms.
In turn,
this will foster the development of new approaches to the
broader study of non-stationary phenomena.
I.
INTRODUCTION
Time-series data are ubiquitous across science and indus-
try, making the tools of time-series analysis essential for con-
temporary analytic practice. To make analyses tractable, time
series are commonly assumed to be stationary. The assump-
tion of weak stationarity entails that the first and second mo-
ments of the distribution of a time series are constant, whereas
strong stationarity means that all conditional probabilities are
constant in time.1 However, many studied systems exhibit
non-stationary behavior that violates one or both of these as-
sumptions. Representative examples of non-stationary sys-
tems can be found in physics,2 engineering,3 ecology,4 cli-
mate science,5 neuroscience,6 and finance.7 Moreover, for
many studied systems the pattern of non-stationary variation
in dynamical behavior is of interest. For example, in the neu-
roscientific context it is useful to ask whether instances of
non-stationary brain dynamics are driven by patterns of ac-
tivity in certain brain regions.8 The non-stationarity of such
systems can be conceptualized in terms of one or more pro-
cesses that vary the conditional probabilities of the system by
modulating parameters of a hypothetical model or set of gen-
erative equations. Further, if we can reconstruct a time series
of the parameter variation underlying non-stationarity, we can
arXiv:2407.08987v1  [physics.data-an]  12 Jul 2024

Parameter inference from a non-stationary unknown process
2
then seek a correspondence between this parameter time series
and some part of the system or its environment. In some cases,
assuming stationarity may not only be incorrect, but can blind
us to important sources of variation that are crucial to under-
standing the system. For example, a recent study suggests
that the time-series features that best discriminate whether a
subject is an experienced meditator using electroencephalog-
raphy (EEG) are those that index non-stationarity, with tradi-
tional spectral properties showing no difference.9 In summary,
there is a need to develop methods that can be used to analyze
non-stationary systems—e.g., by identifying and quantifying
non-stationarity, and characterizing the dynamics of processes
that drive the statistical variation.
A pragmatic definition of stationarity is needed for data-
driven analyses because, as theoretically defined for stochastic
processes, stationarity is a property of processes rather than
of finite time series, and can only be evaluated in the large
data limit—in the univariate case, xt (t = 1,··· ,T), as the
number of samples T →∞. When stationarity is discussed
in relation to deterministic dynamical systems, an appeal is
often made to the concept of ergodicity, which is also prob-
lematic for finite data.10 Additionally, there is no consensus
theoretical definition of stationarity for deterministic dynami-
cal systems: some authors require the existence of an invari-
ant ergodic measure,11–13 some require that the system be au-
tonomous (i.e., with equations and parameters that are fixed
across time),14–18 while others require that time-series statis-
tics do not vary over the duration of measurement.19,20 Similar
to Schreiber 19, and Aguirre and Letellier 20, here we adopt a
definition of non-stationarity as the variation over time in the
joint probability distribution of a time series, for which varia-
tion in dynamical properties may be used as a surrogate mea-
sure. In practice, measuring variation in joint probabilities or
certain statistics necessitates specifying some timescale, i.e.,
some temporal window length over which this variation is
evaluated, and in general stationarity is timescale dependent.
For example, a process with a constant mean over long time
windows can have a wandering mean over short windows.
What timescale to consider is best decided by a practitioner
on the basis of domain knowledge. For example, given an
EEG time series measured during sleep, the relevant timescale
will be much shorter if 200 Hz hippocampal ripples21 are the
object of study, as compared to sleep stages, which by con-
vention are rated using 30 s epochs22.
In this work we consider the problem of Parameter Infer-
ence from a Non-stationary Unknown Process (PINUP), for
which we adopt the following definition (which we develop
more formally in Sec. II). Starting with time-series observa-
tions of a non-stationary process, and in the absence of a
known mathematical model of the process, PINUP involves
inferring one or more time-varying parameters (TVPs) that
drive the non-stationarity. A PINUP method is any algorith-
mic procedure for solving this problem.
The specification
that the generative process be unknown a priori is important.
Firstly, there are many real-world settings where there is con-
siderable uncertainty about the functional form of a process.
For example, for complex systems such as the brain, where we
often have large amounts of noisy, time-series data, finding
relevant time-varying patterns within the data is often more
feasible than developing an accurate model of the entire sys-
tem. Secondly, it is important to distinguish PINUP from the
problem of inferring static or time-varying parameters for a
known model, an example of which is the inverse problem for
differential equations.23 The Kalman filter, particle filter, and
other Bayesian and statistical inferential methods have been
developed for this purpose.24–28 In some cases it may be pos-
sible to reduce PINUP to this latter problem class if a genera-
tive model can first be learned via data-driven discovery.29–32
PINUP is an important problem that has been studied
widely across different disciplinary contexts, resulting in a
correspondingly fragmented literature on approaches to tack-
ling it. Existing theory and algorithms that have been devel-
oped for it are broad, from dimension reduction to Bayesian
inference. These myriad theoretical and algorithmic contribu-
tions are often disjoint, with overall minimal referencing of
approaches developed in different fields, suggesting a lack of
familiarity with the interdisciplinary literature. There is no
agreed problem formulation, and variation in terminology—
e.g., inferring a “drifting parameter",33 or a “driving force",34
versus a “causal driver"35—further compounds this. Further-
more, many papers only demonstrate the usefulness of their
individual algorithm on a single problem, most commonly the
logistic map or the Lorenz process.33,34,36–45 In addition to
the challenge of determining the current state of the interdis-
ciplinary scientific literature on this topic, this lack of bench-
marking standards or systematic comparison of algorithm per-
formance across a diverse range of challenging systems (as is
common practice for other problem classes) makes it difficult
for a reader to discern whether progress is being made. Here
we unify the disjoint literature on PINUP by clearly defin-
ing the problem class (Sec. II) and reviewing and categoriz-
ing the diverse, algorithmic, scientific literatures (Sec. III).
We then demonstrate, using new numerical experiments, the
trivial nature of problems commonly used for benchmark-
ing PINUP algorithms—including the logistic map—and in-
troduce more challenging problems on which PINUP perfor-
mance may more appropriately be tested, and on which many
conventional methods fail (in Sec. IV).
II.
PROBLEM FORMULATION
The basic procedure of PINUP is illustrated in Fig. 1.
PINUP aims to infer the time series of one or more sources of
non-stationary variation directly from a measured (univariate
or multivariate) time series. Taking an example from Fig. 1,
suppose that the dynamics of a brain were to be deliberately
manipulated by modulating brain dopamine activity over time.
Given a time series of brain activity, such as EEG data, one
could use PINUP to try to infer a time series of dopamine ac-
tivity. Similarly, if a chaotic flow such as the Lorenz process
has a time-varying parameter (TVP), then we can analyze the
Lorenz process time series to try to infer the time series of the
TVP.
We consider a time series x(t) of a non-stationary process
to be generated by a model M , the joint probability distri-

Parameter inference from a non-stationary unknown process
3
FIG. 1: Parameter Inference from a Non-stationary Unknown Process (PINUP). a. A non-stationary process is generated
by an unknown model M under the influence of some time-varying parameter(s) (TVPs) θ(t). A selection of example
processes are depicted with TVPs denoted in red: (i) brain activity under the time-varying influence of ascending
neuromodulation by dopamine; (ii) climate (e.g., mean temperature) under the influence of changing CO2; and (iii) the Lorenz
process with a time-varying parameter, ρ(t) [see Eq. (A4)]. b. A time-series realization X of the non-stationary process is
observed. c. A PINUP algorithm is used to infer the time-varying parameter(s) ˆθ(t) directly from the time series X.
bution of which depends on one or more time-varying pa-
rameters, θ(t), across some time interval t ∈[0,Tmax]. By
framing PINUP in terms of an general model M , we allow
for a range of generative processes. For example, M could
represent a system of ODEs
d
dt x = f(x,θ(t)), or an itera-
tive map xt+1 = f(xt,θt), among other possibilities includ-
ing noise processes, stochastic differential equations (SDEs),
or even a process governed by rules that could not be writ-
ten down in closed form. Stationarity then corresponds to the
fixed-parameter case, θ(t) = θ(0), ∀t. In practice, regardless
of whether M is defined in continuous or discrete time, to
obtain a time series of length T, the process must be sam-
pled discretely, for example, at a uniform sampling rate as
t = 0,∆t,2∆t,...,(T −1)∆t. We denote a time-series realiza-
tion from M (θ(t)) as X ∈RT×D, where D is the number of
time-series variables, and where the value at time t of vari-
able i is xt,i. Given an ascending sequence of sampling times
ti ∈[0,Tmax], we let xi ≡x(ti) and can write X in terms of row
vectors:
X =


x1
x2
...
xT


(1)
The corresponding discretization in time of θ(t) is θt. More
generally, if there is some observation function g and noise
process η, then our observed time series is Y ∈RT×D, where
yt = g(xt) + ηt. We obtain Y = X if observations are noise-
free and g is the identity function.
PINUP can be summarized as follows: given an observed
time-series dataset X (or Y), resulting from some unknown
process M under the influence of TVP(s) θ(t), our goal is to
infer an approximation of the TVP(s), denoted ˆθt.
Although many of the papers in the PINUP literature re-
fer to the inference of a time-varying parameter, other ter-
minology includes inference of a “driving force",38,39,41,46 a
“driving signal",37 a “drifting parameter",33,43,47 a “dynami-
cal parameter",48 or a “causal driver".35 In what follows we
treat these terms as synonyms and use the term TVP for con-
sistency.
III.
REVIEW OF EXISTING METHODS
How might one approach PINUP? Starting from the con-
cept of non-stationarity, one could proceed by developing a
method that quantifies variation in the joint probability dis-
tribution and then uses this information to infer one or more
TVPs. For example, some statistics (or ‘features’) of the dy-
namics will be sensitive to changes in the joint probability
distribution induced by θt (and in the optimal case will be
sufficient statistics with respect to θt). Therefore, one could
measure a set of time-series statistics across sliding time-
series windows and then examine the resulting time series
of statistics to obtain an estimate of the TVP(s).49 Similarly,
one could compute windowed empirical probability distribu-
tions, say, through naive histogram binning, in order to esti-
mate variation in the underlying probability density function
(for a stochastic process) or invariant measure (for a deter-
ministic process).50 Moreover, if a pattern of non-stationary
variation manifests jointly across several time-series variables
(or statistical features) then dimension reduction could be
used to project the data onto a lower-dimensional space that
captures shared variation that is driven by the underlying
parameter(s).39 Indeed, many such PINUP algorithms have
been proposed but they have never been systematically stud-

Parameter inference from a non-stationary unknown process
4
ied or compared.
In this work we organize the literature of theory and al-
gorithms for PINUP into six categories of conceptually dis-
tinct approaches, illustrated in Fig. 2, which we explain in
turn through this section. In particular, we organize existing
methods into those based on:
• Dimension reduction (Sec. III A, Fig. 2a). These meth-
ods project time-series data onto lower-dimensional
spaces that best capture some property (such as vari-
ance, autocorrelation, or predictability) that is sensitive
to underlying TVPs.
• Statistical time-series features (Sec. III B, Fig. 2b).
These methods track parameter variation using statistics
that may be sensitive to variation in the joint probability
distribution of the underlying process.
• Recurrence quantification analysis (RQA) (Sec. III C,
Fig. 2c). These methods infer TVPs from recurrence
plots which are computed based on the patterns of re-
currence in time-series data.
• Prediction error (Sec. III D, Fig. 2d). These methods
construct a time-series prediction/forecasting model
and then infer TVPs from the time series of prediction
error.
• Phase-space partitioning (Sec. III E, Fig. 2e). These
methods infer TVPs from multivariate time series of
metrics, such as prediction error, that are measured
across different regions of abstract space.
• Bayesian inference (Sec. III F, Fig. 2f). These methods
start from some prior distribution and then infer TVPs
from the data using a probability model.
A.
Dimension reduction
Dimension reduction involves mapping data to a lower-
dimensional space in a way that optimizes or preserves
some property of the data.51 For example, principal com-
ponent analysis (PCA), learns linear projections that maxi-
mize variance or, equivalently, minimize squared reconstruc-
tion error.52,53 The structure of time-series data, namely, that
the data are ordered and will in general display autocorrela-
tion, poses both challenges and opportunities for dimension
reduction methods. For example, time-series autocorrelation
violates the assumption of independent and identically dis-
tributed (i.i.d.) data used by many methods, but can also form
the basis of alternative methods that exploit this structure (as
seen below). For PINUP, if non-stationary variation manifests
across a multivariate time series X—say, by modulating vari-
ance or autocorrelation—then a dimension-reduction method
that is sensitive to such changes could be used to project onto
one or more dimensions that track the TVPs. This idea is de-
picted schematically in Fig. 2a, which depicts a linear dimen-
sion reduction from three dimensions to a two-dimensional
subspace.
Dimension reduction was used by Wiskott 39 to infer pa-
rameter variation from non-stationary time series. To do this,
Wiskott used Slow Feature Analysis (SFA), which extracts
slowly varying features for which the first temporal deriva-
tive is minimized.54 In contrast to PCA, the first dimension
of which identifies the direction of greatest time-series vari-
ance, SFA tracks the direction of slowest variation in the time
series. The inductive bias of SFA towards slowness makes it
sensitive to non-stationary dynamical variation when fast ob-
served dynamics are modulated by a slower TVP. When SFA
is used for PINUP, time-delay embedding55 (discussed fur-
ther in Sec. III D) and polynomial basis expansion are first
applied54 (noting that the method is termed ‘SFA2’ when
a second-order polynomial basis expansion is used).
The
noise-robustness of SFA for parameter inference was recently
examined by Wang et al. 56, and numerous SFA variants
have been applied to a range of problems over the past two
decades.57 A closely related dimension reduction method is
Time-lagged Independent Component Analysis (TICA) which
finds components that have the largest autocorrelation at a
given time-lag.58,59 TICA is equivalent to SFA, without TDE
or basis expansion, in the case where one-step autocorrela-
tion is considered.60 Several papers have explored the mathe-
matical relationships between techniques such as PCA, SFA,
TICA, Smooth Orthogonal Decomposition (SOD, detailed in
Sec. III E), and other methods.60–62 Nonlinear neural-network
extensions of these and similar dimension-reduction tech-
niques have also been proposed, such as deep-TICA, Time-
lagged Auto-Encoders (TAEs), and Variational Approach for
Markov Process networks (VAMPnets), many of which have
been applied to discover slow collective variables underlying
molecular dynamics.63–65 In contrast to SFA2, which applies
a polynomial basis expansion by default,54 deep-TICA learns
and applies a nonlinear basis expansion followed by TICA de-
composition using a differentiable network layer.65 The linear
methods, such as PCA, SFA, and TICA, trade expressivity for
interpretability, noting that linear transformation weights can
be interpreted as indicating the relative importance of different
variables or features for each dimensional projection. In con-
trast, the nonlinear methods can flexibly learn more complex
mappings but pose a risk of overfitting, especially for small
datasets or when the signal-to-noise ratio is low.66
Several dimension reduction PINUP methods have been ap-
plied to non-stationary time series to infer TVPs as a part
of scientific discovery. For example, SFA has been used in
climate science to infer TVPs that drive climate change and
other variation related to greenhouse and aerosol emissions,67
the North Atlantic Oscillation,68 the El Niño-Southern Oscil-
lation and Hale sunspot cycles,69 the Atlantic Multi-decade
Oscillation,70,71 and the East Asian trough.72 Additionally,
there is an active literature using methods such as TICA,
TAEs, and VAMPnets to discover reduced-order models of the
patterns of variation underlying molecular activity.63,65,73–75

Parameter inference from a non-stationary unknown process
5
a. Dimension reduction
b. Statistical time-series features
c. Recurrence quantification analysis
d. Prediction error
e. Phase-space partitioning
f. Bayesian inference
FIG. 2: Schematic depiction of six categories of PINUP methods, each of which tracks non-stationarity through some
aspect of parameter-driven variation in the joint probability distribution of a process. a. Dimension reduction methods
project time-series data onto lower-dimensional spaces that optimize with respect to some statistical property (such as variance,
slowness, autocorrelation, or predictability). The image shows data being mapped to a 2-dimensional subspace via a linear
transformation. b. Statistical time-series feature methods infer parameter variation from statistical variation. The figure depicts
mean values of a single time-series variable from the Lorenz process computed using sliding windows. c. Recurrence
quantification analysis methods infer parameter variation from recurrence plots which are computed based on the patterns of
recurrence in time-series data. The recurrence plot of a Lorenz process driven by a sinusoidal parameter ρ is shown. d.
Prediction error methods construct a time-series prediction/forecasting model and then infer parameter variation from the
resulting prediction error time series. The figure depicts one-step prediction error between the true trajectory and a prediction
using some function. e. Phase space partitioning methods infer parameter variation from multivariate time series of metrics,
such as prediction error, that are measured across different regions. The figure depicts a partition of the phase space of a Lorenz
process. f. Bayesian inference methods start from some prior distribution over models and TVP values and then infer parameter
variation from the data on the basis of a likelihood model. In the figure, parameter values θ(t) and a model M are drawn from
prior distributions, then data realizations generated from a process M (θ(t)) are compared to the observed time series X.
B.
Statistical time-series features
A statistical time-series feature is the scalar output of a
function that has been applied to a time series, mapping a
time series x ∈RT →R.76 Thousands of such time-series
features have been developed across a wide range of scien-
tific disciplines, with simple examples including mean and
variance, and more complex examples including entropy mea-
sures and nonlinear methods such as correlation dimension.77
A feature vector can be computed by applying multiple algo-
rithms to a time series, thereby providing a multifaceted statis-
tical description of the time series. Applying such algorithms
across sliding time-series windows results in a time series of
statistical features, which describes the variation in statisti-
cal properties over time. Provided the chosen statistics are
sensitive to TVP-driven statistical variation, time-series fea-
tures can be used individually or in aggregate to infer TVPs.
When multiple time-series features are used, a natural step is
to apply some form of dimension reduction in order to obtain
the TVP estimate ˆθt. As an example, consider a hypotheti-
cal process in which a single TVP θ(t) varies the mean off-
set of the process. Simply computing a moving average will
track θ(t) accurately. Alternatively, if θ(t) were to modulate
the frequency of an oscillatory component of a time series,
then θ(t) could be tracked by computing variation in the peak
location of the power spectrum over time. Crucially, such
windowed, feature-based inference of TVPs assumes that the
dynamics within each window are pseudostationary, so that
within-window parameter variation can be neglected. This
approach is illustrated in Fig. 2b as the computation of mean
time-series values across windows.
Statistical features of a time series—such as mean, vari-
ance, and autocorrelation—computed over sliding time-series
windows were used by Güttler, Kantz, and Olbrich 49 to suc-

Parameter inference from a non-stationary unknown process
6
cessfully reconstruct the space of TVPs of dynamical systems.
However, their method requires manual selection of features
that are sensitive to the dynamical changes induced by the
TVPs relevant to any given problem; it does not provide a
feature set that is sensitive to parameter variation in general.
In a similar vein, other authors have performed PINUP us-
ing one or more hand-selected features. For example, Yanson
et al. 78 showed that a slow TVP driving a Rössler attractor
can be reconstructed using variance, and Bollt et al. 17 pro-
posed a complexity measure termed ‘control entropy’ with the
aim of tracking TVPs. Addressing the related problem of pa-
rameter variation across a set of time series (each generated
with some fixed set of parameter values) rather than over time,
Niknazar, Nasrabadi, and Shamsollahi 44 assessed the perfor-
mance of individual nonlinear time-series features and volu-
metric features, which measure occupied space and geometric
trajectory properties, for inferring parameter settings of the
Lorenz and Mackey–Glass systems. Inferring parameter vari-
ation across time series was similarly undertaken by Alao, Lu,
and Soljacic 48, who applied PCA dimension reduction to fea-
ture vectors of regression coefficients from the output layer
of an echo state network, and by Fulcher et al. 79, who ad-
dressed the issue of manual feature selection by applying PCA
to a comprehensive set of over 7000 time-series features from
the hctsa package77. Harris and Fulcher80 demonstrated that
feature-based inference of a TVP can be biased by feature–
feature redundancy, but that this issue can be ameliorated by
applying PCA to appropriately chosen baseline data to find
suitable orthogonal coordinates.
C.
Recurrence quantification analysis
Introduced by Eckmann, Kamphorst, and Ruelle 81, recur-
rence plots (RP) represent the times at which states in a phase
space recur over the course of a time series. Given a time se-
ries X, a threshold distance ε, a norm ∥·∥, and the Heaviside
function Θ(·), a RP is constructed as a two-dimensional ma-
trix R according to Ri,j = Θ(ε −∥xi −x j∥).82 Accordingly, a
value of 1 at Ri,j means that the time-series states at times i
and j are within distance ε. An example RP matrix for a non-
stationary Lorenz process driven by a sinusoidal ρ parameter
is visualized in Fig. 2c. Note that some form of phase-space
reconstruction, typically time-delay embedding, is performed
prior to constructing the RP, requiring the selection of addi-
tional hyperparameters.82 In this section we focus on methods
that analyze the RP directly as a matrix or graph, whereas
analyzing time series of RQA statistics would form a subset
of the times-series feature-based methods grouped together in
Sec. III B. When applied to PINUP, the intuition is that the RP
structure is an informative signature of the (possibly nonlin-
ear) statistical properties of a process, and so non-stationarity
should be reflected in the RP. For example, if similar values
of θ(t) result in similar dynamical behavior, e.g., so that the
process evolves on the same attractor, then the probability of
recurrences will be higher, and so the similarity between rows
or columns of the RP can be used as the basis for TVP recon-
struction.
Casdagli 36 observed that, for appropriate TDE parameters,
the RP of a TVP-driven non-stationary process approximates
the RP of the TVP itself. Tanio, Hirata, and Suzuki 46 later
showed how to more reliably reconstruct a TVP from a RP
by using combinatorial optimization to permute the RP so
as to order the time-series points according to the similarity
of their dynamics, thereby obtaining an approximate, mono-
tonic transformation of the original TVP. Hirata, Horai, and
Aihara 83 developed another method for reconstructing time
series from RPs by first creating a weighted graph based
on neighborhood overlap, then computing all-pairs shortest
paths using Dijkstra’s algorithm,84 and then applying mul-
tidimensional scaling85 to reconstitute the original time se-
ries. This method also works when infinite-dimensional TDE
is used for RP construction, and can be used for PINUP if
the RP is coarse-grained prior to applying the reconstruc-
tion procedure.86,87 Recently, Gilpin 35 proposed inferring a
causal, driving process by first constructing a distance-based
adjacency matrix representation, i.e., RP, of a multivariate
time series and then computing the subleading eigenvector the
the graph Laplacian of this matrix.
D.
Prediction error
Consider the simple case of a non-stationary process
M (θ(t)) where the effect of the scalar-valued TVP θ(t) is
to translate the mean of the process. For example, picture
the Lorenz attractor (see Fig. 1a) translating up and down
the z-axis with θ(t). Suppose that a mean-sensitive predic-
tive model is then trained using a portion of the associated
time series, e.g., a single-step nearest-neighbor predictor (see
Fig. 2d). If the predictive model is then applied across time-
series windows, one would expect the prediction error to vary
according to changes in the mean of the process under the in-
fluence of θ(t). Moreover, since the prediction error varies
with ∆θ, one could use the time series of prediction errors to
infer θ(t). More generally, the methods we group here un-
der the category ‘prediction error’ involve training a predic-
tive model using part or all of a measured time series and then
inferring a TVP based on prediction error computed locally
across time-series windows, typically by taking ˆθt to be the
time series of prediction errors.
Many of these methods start with phase-space reconstruc-
tion (where the phase space is the space of all possible states
of a dynamical system55). The most common approach is
to use a scalar-valued time series to construct a time-delay
embedding (TDE), i.e., a vector time series where each vec-
tor contains dimension d entries from the original time se-
ries, each separated by a time delay of τ.55 The appeal of
TDE stems from Taken’s theorem,88,89 which provides con-
ditions under which there exists a diffeomorphism (a differ-
entiable, invertible bijection) between the manifolds of the re-
constructed and true dynamics, noting that a diffeomorphism
preserves topologically invariant properties. For the purpose
of PINUP, with a phase-space reconstruction in hand, one can
then fit prediction models and compute prediction errors with
the goal of tracking TVPs.

Parameter inference from a non-stationary unknown process
7
Schreiber 19 proposed testing for the presence of non-
stationarity using ‘cross-prediction error’, which quantifies
how well a nearest neighbors predictor for one time-series
segment performs on another time-series segment, similar to
our example above. Schreiber and Schmitz 1 then used cross-
prediction error to perform PINUP. They partition a time
series into windows and construct a dissimilarity matrix of
cross-prediction errors between windows. Clustering is then
performed and a TVP is inferred based on computed distances
from clusters over time.
Verdes et al. 34 provided a more
general formulation of the prediction error approach. Start-
ing with a TDE X, they consider a predictive model f such
that xt+1 ≈f(xt,θt) and show that θt can be expressed ana-
lytically in terms of prediction error, given: (i) a local linear
approximation; and (ii) a smooth dependence of f with θ.
Szeliga et al. 37 later proposed that f and ˆθt be learned jointly
using a multilayer perceptron neural network. The network
is trained to perform one-step time-series prediction using a
mean squared error (MSE) loss function, and ˆθt is treated as
a trainable vector where smoothness is enforced via an addi-
tional loss function of MSE across adjacent values ˆθt and ˆθt+1.
In follow up papers, Széliga et al. 38 and Verdes, Granitto, and
Ceccatto 40 showed how to use over-embedding and validation
data to cope with noisy time series and to select hyperparam-
eters during model training. An advantage of the formula-
tion of Verdes et al. 34 is that the form of f is not specified,
allowing f to be constructed using a wide range of predic-
tive modeling techniques. Subsequent authors have used pre-
diction models based on echo state networks41 and support
vector machines.43 However, an important limitation is that
most of the methods in this category of methods yield only
a scalar-valued prediction error, so they can resolve a single
TVP ˆθt, but in order to resolve multiple TVPs ˆθt some addi-
tional demixing technique is required, such as single-channel
blind source separation.90
E.
Phase-space partitioning
Instead of examining a process globally, phase-space par-
titioning methods analyze local regions of phase space, most
commonly by partitioning phase space into hypercubes, as il-
lustrated in Fig. 2e. Phase space is either reconstructed, e.g.,
using the method of TDE (see Sec. III D), or else a multi-
variate time series is taken to define a phase space, with each
variable treated as a separate dimension. The key intuition is
that local analyses, e.g., in contiguous regions of phase space,
may be more sensitive to subtle fluctuations in statistical prop-
erties that only occur in certain regions of phase space. These
methods output a multivariate time series to which some form
of dimension reduction is applied, making it possible to re-
solve multiple TVPs underlying a single time series. Note
that some of the methods listed here compute prediction error
locally, and so they are conceptually closely aligned with pre-
diction error methods (Sec. III D). A key distinction is that, as
noted, the methods in this section analyze multivariate time
series of locally computed metrics, allowing for the inference
of multiple TVPs, in contrast to the scalar-valued prediction
errors used by methods from the previous section. Addition-
ally, the methods in this section represent a historical thread
of PINUP research, mostly from the field of mechanical engi-
neering, and forms a coherent body of work.
Chelidze et al. developed a PINUP method called phase
space warping (PSW)33,47,91–96 that partitions phase space
and computes prediction error locally in each region, yield-
ing an error vector that evolves over time windows, followed
by dimension reduction via smooth orthogonal decomposi-
tion (SOD). Hyperparameter optimization is a challenge for
this method, and is discussed at length in Li and Chelidze 96.
Note that SOD and SFA are equivalent, sharing the general-
ized eigenvalue problem formulation: ˙CU = CUΛ, where C
and ˙C are the autocovariance matrices of a time series and its
first temporal derivative, respectively, and U and Λ are eigen-
vectors and eigenvalues.92,97 However, there can be subtle dif-
ferences in the algorithmic implementation of each method;
for example, SFA incorporates a PCA whitening step during
which near-zero eigenvalues can be optionally discarded ac-
cording to some threshold.98
Epureanu et al. proposed the method of sensitivity vector
fields (SVF)42,99–103 which first partitions phase space then
computes a vector of local divergence between trajectories in
each time window compared to a reference window, followed
by dimension reduction via PCA. Although the PSW and SVF
methods are similar, key differences are that the former con-
structs local predictive models and applies SOD dimension re-
duction, whereas the latter compares trajectories directly and
uses PCA dimension reduction.
Nguyen and Chelidze 104 noted the computationally inten-
sive nature of PSW and SVF, and so proposed a fast method
called characteristic distance (CD) which is is based on the
Birkhoff ergodic theorem.105 CD selects a set of random
points in phase space and then computes vectors of mean dis-
tance from each point, yielding a time series of mean distance
vectors to which SOD dimension reduction is then applied.
Sloboda et al.106–109 observed that PSW and SVF require var-
ious modeling and hyperparameter choices, and in response
proposed the boundary transformation vector (BTV) method
as a way to directly measure local attractor deformation. BTV
infers parameter variation by computing 2D Poincaré sections
for a number of planes through phase space and then com-
puting vectors of boundary deformation metrics across time
windows. Sloboda et al. were also influenced by Carroll 50
who proposed a density-based method which partitions phase
space into bins in order to compute an empirical probability
density, after which feature vectors are produced through pro-
jection onto a polynomial basis, followed by TVP reconstruc-
tion using the Euclidean distance between the feature vector
for each time window compared to a reference window.
Many of the phase-space partitioning PINUP methods have
been used in applied research on fault detection and prognos-
tication of the condition of industrial assets.47,110 The idea is
that for some non-stationary systems there exists a TVP that
tracks a condition such as the gradual mechanical or electrical
failure of a component. In such cases, PINUP methods can be
used to anticipate deteriorations in performance and eventual
failure of the system. Applications that have been examined

Parameter inference from a non-stationary unknown process
8
include tracking the discharge of a battery,47,91 crack growth
in an oscillating system,92–94,96,111 voltage input to a magneto-
elastic oscillator,104 behavior of an aeroelastic system,102 the
position of additive mass on a vibrating beam,101 variation in
resistance in a chaotic Chua circuit,42,106 in addition to the bi-
ological case of tracking muscle fatigue in high-performance
settings.95,112,113
F.
Bayesian inference
When used to infer parameters, Bayesian inference starts
with a prior probability distribution over possible parameter
values (and/or models) and then computes a posterior distri-
bution based on a probabilistic model of the conditional prob-
ability of the observed data given specific parameter values.114
The posterior distribution can then be used to calculate the ex-
pected parameter value(s) and to quantify uncertainty in this
estimate. Bayesian methods have been used extensively to fit
parameters to known models, as in the case of the ODE inverse
problem, and have been extended to handle TVPs.24–28,115
Bayesian methods have also been used for data-driven dis-
covery, i.e., system identification, where the functional form
of an ODE, PDE, or similar, is learned along with the corre-
sponding static coefficients for each equation term.31,116–118
A Bayesian approach to PINUP is illustrated in Fig. 2f.
Bayesian inference was used by Smelyanskiy et al. 119
to perform system identification for dynamical processes
with an underlying SDE (or ODE) model.
Importantly,
they developed an analytical, variational inference approach
that converges quickly compared to Markov Chain Monte
Carlo (MCMC) sampling. The same research group subse-
quently showed that their technique can be applied to sepa-
rate time-series windows in order to simultaneously resolve
TVPs.29,120,121 An important limitation of this method is that
it assumes dense sampling and minimal measurement noise,
which may not apply in practice. Moreover, model selection
is a problem, since the number of possible combinations of
basis functions for the SDE model scales exponentially; for
example, if a polynomial basis is used, there are 2d+1 possi-
ble combinations of polynomial terms up to order d for a sin-
gle variable. Model selection was addressed through heuristic
application of the Bayesian information criterion (BIC) and
beam search.122 The restriction that the underlying model take
the form of an SDE corresponds to only a subset of our PINUP
formulation, but nonetheless includes a wide range of impor-
tant processes. Additionally, achieving simultaneous system
identification and PINUP is notable, and is an important di-
rection for further research.
IV.
TESTING PINUP BENCHMARK PROBLEMS
The relative performance of different PINUP algorithms is
largely unknown, owing to a lack of comparative analyses
across the diverse methodological literature. In most studies,
the performance of a given method is evaluated by compar-
ing the TVP(s) ˆθt inferred using a PINUP method against the
ground truth θt from a numerical simulation of a process. The
advantage of simulated data is that the ground truth is known,
allowing performance of the TVP inference to be quanti-
fied using a metric such as Pearson correlation123 or nor-
malized mean squared error (NMSE).34 Many authors have
focused the testing of their algorithms on simulations using
well-characterized chaotic maps and flows, such as the Baker
map,1 the Tent map,39,46,49 the Hénon map,40,49 the Mackey–
Glass system,44,49 and the Rössler system.83,104 Notably, the
logistic map and the Lorenz process are the most commonly
used processes for evaluating PINUP methods across the pub-
lications that we surveyed.33,34,36–45 The common assumption
is that these canonical, nonlinear, chaotic systems offer chal-
lenging cases that are useful for assessing the performance of
PINUP algorithms. However, since so many PINUP methods
perform well on the same test problems, it raises suspicion
about whether these are truly challenging problems on which
strong performance is meaningful, or whether similar perfor-
mance could be achieved by simpler baseline methods (which
have not yet been adopted as standard practice within the lit-
erature).
To examine the suitability of the logistic map and Lorenz
processes as PINUP benchmarking problems, we performed
numerical experiments comparing the performance of several
representative PINUP methods to a simple baseline method
that estimates parameter variation as a time series of statis-
tics, either mean or standard deviation, computed over slid-
ing windows. Undermining their use as meaningful test prob-
lems, we find that good PINUP performance in the logistic
map and Lorenz process cases can be trivially achieved us-
ing our baseline method (Sec. IV A). Accordingly, we then
introduce new, more challenging problems on which the same
PINUP methods exhibit weaker performance, but for which
we can still find statistical time-series features that perform
well (Sec. IV B).
A.
Testing non-stationary logistic map and Lorenz systems
We aimed to test whether strong PINUP performance on
the non-stationary logistic map and Lorenz processes could
be achieved by a simple, baseline method, relative to that of
several existing PINUP methods. To do this we first generated
time series from each of the non-stationary logistic map and
the Lorenz processes. To demonstrate the comparative perfor-
mance of several existing methods, PINUP was conducted us-
ing methods chosen from the dimension reduction, prediction
error, and phase-space partitioning categories: quadratic SFA
(SFA2, a dimension reduction method);39 smoothed mean
prediction errors from a bank of echo state networks (ESN
error, a prediction error method);41 and characteristic distance
(CD, a phase-space partitioning method).104 We chose these
methods on the basis of ease of implementation, as well as to
achieve a spread of methods across the categories described
in Sec. III. A more detailed description of how we imple-
mented each method is in Appendix A 2. In order to evalu-
ate the performance of existing algorithms relative to a sim-
ple baseline method for tracking non-stationary variation, we

Parameter inference from a non-stationary unknown process
9
x
z
SFA2
ESN error
CD
mean
PINUP method
0.0
0.2
0.4
0.6
0.8
1.0
R2
Noise-free
SNR = 20 dB
SNR = 0 dB
0.0
0.5
1.0
Time
×105
25
30
p
a
b
c
x
Time
SFA2
ESN error
CD
std
PINUP method
0.0
0.2
0.4
0.6
0.8
1.0
R2
0.0
0.5
1.0
Time
×105
3.25
3.50
3.75
p
d
e
f
x
z
SFA2
ESN error
CD
acf timescale
PINUP method
0.0
0.2
0.4
0.6
0.8
1.0
R2
0.0
0.5
1.0
Time
×105
3.25
3.50
3.75
p
g
h
i
x
Time
SFA2
ESN error
CD
centroid freq
PINUP method
0.0
0.2
0.4
0.6
0.8
1.0
R2
0.0
0.5
1.0
Time
×105
2.75
3.00
3.25
p
j
k
l
[H]
FIG. 3: Individual time-series features match or exceed the TVP reconstruction accuracy of several PINUP methods
across four chaotic systems. The processes (and varied parameters) are: a-c. the Lorenz process (parameter ρ); d-f. the
logistic map (parameter r); g-i. the Langford process (parameter ω); and j-l. the sine map (parameter r); each of which displays
results from a parameter inference experiment. A noise-free example of each process is visualized (in panels a, d, g, and j) and
colored according to the corresponding TVP value at each point, noting that the flows are visualized as 2-dimensional spatial
projections, whereas the 1-dimensional maps are visualized using space and time. The time course of the associated sinusoidal
TVP is visualized for each system (b, e, h, and k) with color corresponding to the TVP value for comparison to the
corresponding process plots. The numerical experiment result panels (c, f, i, and l) show mean and standard deviation of TVP
reconstruction accuracy (Pearson R2) for each method across multiple trials using two levels of signal-to-noise-ratio (SNR) (0
and 20 dB) and a noise-free condition. The PINUP methods were quadratic slow feature analysis (SFA2), a
prediction-error-based method using echo state networks (ESN error), and characteristic distance (CD). The single time-series
features that were used were mean, standard deviation (std), the first time lag at which autocorrelation function falls below 1/e
(feature name: acf_timescale124), and the centroid of the power spectrum (feature name: centroid_freq124).
used an approach belonging to the ‘time-series features’ cate-
gory, comprising single time-series features—mean and stan-
dard deviation—computed across sliding windows. The re-
sulting time series of statistical features is taken to be the es-
timate ˆθt. We were primarily interested in demonstrating the
existence of statistics capable of tracking TVPs, so in each
case we selected the best-performing example, of mean or
standard deviation applied to a single variable, for compari-
son with the other methods. Since three of the methods—ESN
error, CD, and our baseline—utilize time-series windows for
the purpose of either smoothing or computing statistics, we
adopted the same window length 103 for each method, judg-
ing that this was sufficiently small relative to the slow TVPs
to resolve parameter variation while being sufficiently large to
confer robustness to noise.
We simulated each non-stationary process using a slow, si-
nusoidal TVP with period T/2, where T was the total Lorenz
process integration time or the number of logistic map iter-
ations, respectively. The amplitude of sinusoidal parameter
variation was ±10% relative to the default value of the pa-
rameter that was varied for each system. We note that quali-
tatively similar results are obtained for other functional forms
for the slow parameter and across a range of values of low
amplitude parameter variation. We then evaluated the per-
formance of each PINUP method by computing Pearson R2
between the ground truth and inferred TVPs. Mean R2 was
computed across 20 trials for each of three different levels of
additive, Gaussian, observation noise: at signal-to-noise ratios
(SNRs) of 0 and 20 dB, in addition to a noise-free condition.
In Appendix A 1 we detail the equations, parameter settings,
initial conditions, and functional form of the TVPs that were
used for each simulated process.
The results of our experiments are shown in Figs 3a-f,
which for the Lorenz and logistic map systems visualize the
process, the TVP, and the TVP reconstruction accuracy of
each PINUP method. Of the PINUP methods, ESN error per-
formed well on the non-stationary Lorenz process (Fig. 3c)
and SFA2 and CD performed well on the non-stationary lo-
gistic map (Fig. 3f). SFA2 also achieved near-perfect per-
formance on the Lorenz process in the noise-free condition
but failed under both noise conditions. In contrast, our sim-
ple baseline approach, which tracks either the mean or stan-

Parameter inference from a non-stationary unknown process
10
dard deviation across sliding windows, achieved high accu-
racy (R2 > 0.9) across all noise conditions.
To understand why a simple windowed distributional fea-
ture can track the non-stationary variation underlying each of
these nonlinear chaotic systems, first observe that when pa-
rameter values are mapped onto a 2-dimensional projection of
the Lorenz phase space (Fig. 3a) we see that the TVP induces
a translation of the attractor along the z-axis. Such transla-
tion in space is easily tracked by computing a mean statistic.
Similarly, it is visually evident that the variance of the logis-
tic map is modulated by the corresponding sinusoidal TVP
(Fig. 3d). The success of this simple baseline method un-
dermines the apparent difficulty of these classical benchmark
problems, which are ubiquitous in the PINUP literature and
which are often (and incorrectly) used to evidence the use-
fulness of a given PINUP algorithm. Even though chaotic
processes like the Lorenz process and logistic map under the
influence of a TVP may exhibit variation in nonlinear proper-
ties, our experiments show that using nonlinear statistics is not
required for strong PINUP performance. For these two sys-
tems this is because the parameter variation doesn’t uniquely
affect nonlinear structure, but also varies simple properties—
namely, the first two moments of the distribution.
B.
Finding challenging problems on which to evaluate
PINUP methods
Given that the Lorenz process and logistic map PINUP
problems can be trivially solved by tracking mean and vari-
ance, respectively, we next aimed to find more challeng-
ing alternative test problems that may be more suitable for
demonstrating impressive PINUP performance. Specifically,
we aimed to demonstrate the existence of non-stationary pro-
cesses for which accurate TVP reconstruction is possible us-
ing some PINUP methods, but for which the simple base-
line method of windowed computation of mean or variance
fails. To do this we manually searched through the chaotic
processes collated by Sprott 125 (comprising 29 maps and
33 flows) and Gilpin 126 (comprising 131 flows). For each
such system, we followed the comparative methodology of
Sec. IV A, i.e., varying a single parameter sinusoidally by
±10% of the default value, then quantifying the performance
of our selected PINUP methods using Pearson R2. To discover
whether there exist problems for which other time-series fea-
tures beyond mean and standard deviation can track TVPs,
we also performed the same sliding-window approach using
22 additional time-series features from the CAnonical Time-
series CHaracteristics (catch22) feature set.124
Here we present two processes, the Langford process and
sine map, for which we found TVP inference to fail using
sliding-window mean or standard variation, but for which at
least one other method performs well (among SFA2, ESN
error, CD, or one of the sliding-window catch22 features).
Our comparative results for these two processes are shown in
Figs 3g-l. Weak performance on both the Langford and sine
map processes was seen for all three of SFA2, ESN error, and
CD (Figs 3i,l), in addition to mean and standard variation (not
shown). The only exception to this was that SFA2 performed
very well on the Langford process in the noise-free condi-
tion, similar to what was seen for the Lorenz process. Ob-
serve that neither process exhibits obvious translation under
the influence of the TVP (see Figs 3g, j), consistent with the
weak performance of sliding-window mean. These are prob-
lems that require more algorithmic sophistication than simply
computing mean and variance. But these are not unsolvable
problems—indeed, in each case we identified a single statis-
tic from the catch22 feature set that could track the statisti-
cal changes induced by underlying parameter variation with
reasonable accuracy (i.e., R2 > 0.8) over two or more noise
levels. For the non-stationary Langford process, a statistical
measure of timescale based on the decay of the autocorrelation
function (catch22 feature name: acf_timescale) success-
fully tracked parameter variation, albeit with a steep decline in
performance with increasing noise, and for the non-stationary
sine map a measure of the centroid of the power spectrum
(catch22 feature name:
centroid_freq) performed well
over all three noise levels. A benefit of using simple time-
series statistics is that they point towards an interpretable the-
ory of what is changing in either process (i.e., autocorrelation
and the power spectrum, respectively).
Taken together, these findings demonstrate the existence of
more challenging PINUP problems, for which accurate TVP
reconstruction is possible but is not achieved by the first two
moments of the distribution or by a set of conventional meth-
ods. If new PINUP methods can be developed that are capable
of handling a range of such challenging problems, this will en-
able the inference of parameter variation for a wider range of
non-stationary phenomena.
V.
DISCUSSION
Progress on the PINUP problem has been hindered by the
fragmentation of the methodological literature across multiple
disciplinary settings and journals, to the extent that many of
the published PINUP methods reviewed here were developed
with minimal knowledge of the progress made on the same
problem in different fields. We have made headway on this is-
sue by providing the first overview of this interdisciplinary lit-
erature: formulating the PINUP problem class (in Sec. II) and
organizing the literature into different categories of PINUP
methods (in Sec. III). This shared terminology and set of ref-
erence algorithms will enable greater interaction and sharing
of ideas between disciplines, while also fostering a system-
atic, comparative, methodological approach (across methods
and problems) in future research. In turn, we hope that such
interdisciplinary, comparative work will enable progress on
the PINUP problem.
A similar lack of systematic comparison has been faced
by other algorithmic fields, such as computer vision,127
time-series classification,128 forecasting,129,130 and clinical
biomarker discovery,131,132 and has been addressed in a range
of ways. For example, progress in time-series classification
has been assisted by: (1) developing libraries of time-series
classification methods;133 (2) curating repositories of bench-

Parameter inference from a non-stationary unknown process
11
mark problems with agreed performance metrics;134 and (3)
conducting systematic comparisons of methods on these par-
ticular problems.135,136 We echo the ‘call to arms’ issued by
Keogh and Kasetty 128 that the culture of reviewers and scien-
tists needs to move towards testing new methods on a broad
set of challenging problems with comparison to a range of
other methods, including simple, baseline approaches.
The PINUP literature has been in need of simple, baseline
methods, to serve as a benchmark against which the perfor-
mance of new methods can be compared. The claimed use-
fulness of a new algorithm is justified by its superior per-
formance relative to simpler baseline alternatives, and the
field can only reasonably claim to be working on challeng-
ing problems when such simple methods fail. We propose
that mean or standard deviation computed across sliding time-
series windows provides such a baseline. Crucially, we found
that this simple approach is comparable or even superior to
several existing methods applied to the non-stationary Lorenz
and logistic map processes (Sec. IV A). Indeed, for many of
the non-stationary chaotic systems that we searched through
in Sec. IV B, either mean or standard deviation performed
well. A similar phenomenon has been observed in the time-
series classification literature, where complicated algorithms
sometimes fail to outperform classification using only mean
and variance, even though these distributional properties are
insensitive to the time-ordering of the data.137 Benchmark-
ing on problems that can be solved using mean or variance
has the downside of being unable to distinguish algorithms
that track simple, distributional properties from those that can
track more subtle forms of non-stationary variation. By eval-
uating performance on new, more challenging problems, re-
searchers in the PINUP field can more convincingly demon-
strate advances in algorithmic performance.
Addressing this need, we performed a wide search across
time-varying systems and demonstrated the existence of diffi-
cult problems for which TVP inference is possible, but where
several existing methods (in addition to sliding-window mean
and standard deviation) fail (Sec. IV B). In particular, we high-
light two such problems: the non-stationary Langford and
sine map processes. Additional problems of this type could
be found by applying our comparative methodology across a
range of non-stationary processes. More challenging PINUP
problems such as these provide a valuable starting point for
future research. Harking back to the definitions that opened
this paper, we speculate that an outstanding challenge for the
PINUP field may be to achieve accurate TVP inference for
processes that exhibit weak but not strong stationarity, i.e.,
where non-stationarity manifests through variation in joint
probabilities but not through variation in mean or variance.
For each of these two, more challenging PINUP problems,
we identified a single time-series statistic from the catch22
feature set that achieved accurate TVP reconstruction using
the same sliding-window approach that we used for mean and
variance (Sec. IV B). This simple sliding-window time-series
feature-based approach is straightforward to implement, par-
simonious, and enables the interpretation of non-stationary
dynamics in terms of the variation of statistical properties over
time. The fact that this simple sliding-window approach can
achieve strong performance where other methods fail presents
a promising direction for future research; for example, one
could refine feature selection or apply dimension reduction in
the space of statistical features. A downside of using statisti-
cal features is that a highly-sampled time series is needed to
enable robust statistical estimates. We expect the performance
of PINUP methods to depend on a range of factors including
sampling rate, the timescale of variation of the TVP relative
to the observed process, the amplitude of parameter variation,
and the contribution of dynamical and measurement noise.
Future work could explore the strengths and weaknesses of
different PINUP methods in the presence of variation in these
factors.
By adopting a systematic approach and testing on appro-
priately difficult problems, PINUP researchers can both drive
progress and help readers to discern genuine advances in
the field. Improved performance on tracking non-stationarity
from time-series data may enable advances on a range of re-
lated time-series analysis problems for which conventional
methods assume stationarity, including forecasting,138,139 and
system identification.29,31 Taken together, our review and nu-
merical results lay a foundation for interdisciplinary progress
on the PINUP problem and the broader study of non-
stationary phenomena.
AUTHOR DECLARATIONS
Conflict of Interest
The authors have no conflicts to disclose.
DATA AVAILABILITY STATEMENT
The
data
that
support
the
findings
of
this
study
are
openly
available
at
https://github.com/
DynamicsAndNeuralSystems/pinup_paper.
APPENDIX
Here we detail the non-stationary processes and methods
that were used for the numerical experiments described in
Sec. IV and shown in Fig. 3.
1.
Non-stationary processes
Non-stationary time series were generated, either through
integration of a flow
d
dt x = f(x,θ), given some initial state
x0, or iteration of a map xt+1 = f(xt,θt). In either case, the
functional form of the TVP was a two-period sinusoid,
θ(t) = p

1+α sin
4πt
T

,

Parameter inference from a non-stationary unknown process
12
where p is the default parameter value, α reflects proportional
variation, and T was the total integration time or number of it-
erations, respectively. For all systems we used α = 0.1, corre-
sponding to sinusoidal variation of ±10% around the default
parameter value p. For ODE integration, the Runge–Kutta–
Fehlberg (RKF45) method was used.140 The models that were
used are described below.
a.
Iterative Maps
For the logistic map141 with equation:
xn+1 = rxn(1−xn),
(A1)
we used a parameter value of r = 3.6 and initial condition
x0 = 0.6. For the TVP we varied r.
For the sine map125 with equation:
xn+1 = rsin(πxn),
(A2)
we used a parameter value of r = 3.0 and the initial condition
x0 = 0.6. For the TVP we varied r.
For both systems, we iterated to simulate time series of
length 105 samples.
b.
Flows
The
Lorenz
process
is
a
model
of
atmospheric
convection142 with equations:
˙x = σ(y−x),
(A3)
˙y = x(ρ −z)−y,
(A4)
˙z = xy−βz,
(A5)
where parameters were set to values σ = 10, ρ = 28, and
β = 8/3 and we used the initial condition (x0,y0,z0) =
(−9.79,−15.04,20.53) from Gilpin.126 For the TVP we var-
ied ρ.
The Langford process yields a torus-like attractor143 with
equations:
˙x = (z−β)x−ωy,
(A6)
˙y = ωx+(z−β)y,
(A7)
˙z = λ +αz−z3
3 −(x2 +y2)(1+ρz)+εzx3 ,
(A8)
where parameters were set to values α = 0.95,β = 0.7,λ =
0.6,ω = 3.5,ρ = 0.25, and ε = 0.1 and we used the
initial condition (x0,y0,z0) = (−0.78,−0.63,−0.18) from
Gilpin.126 For the TVP we varied ω.
For both Lorenz and Langford systems, integration was per-
formed over 1000 time steps, sampling at intervals of 0.01
time units.
2.
Methods
For each system, PINUP was performed using each of the
below methods (SFA2, ESN, CD, and statistical time-series
features) for two levels of additive Gaussian noise, yielding
SNRs of 0 and 20 dB, in addition to a noise-free condition.
SNR was calculated as 10log10(σ2
s /σ2
n ), where σ2
s is the vari-
ance of the signal and σ2
n is the variance of the noise.
a.
Quadratic slow feature analysis (SFA2)
This method is an example of a dimension reduction ap-
proach to PINUP (see Sec. III A). Following Wiskott,39 given
a time series, we first applied time-delay embedding (TDE)
with dimension m and delay τ, then applied a quadratic poly-
nomial basis expansion, followed by single-component SFA,
to obtain a parameter estimate ˆθt. We set τ to the time de-
lay corresponding to the first zero-crossing of the autocorre-
lation function (ACF), and in the setting of multivariate data
selected the minimum such value computed across time-series
variables. To set m, using Wiskott’s heuristic,39 we performed
a search over integers m ∈{1,...,20} and selected the value
of m for which the mean squared value of the first temporal
derivative of the TVP was minimized.
b.
Echo state network prediction error (ESN)
This method is an example of a prediction-error approach
to PINUP (see Sec. III D). Since no out-of-the-box algorithm
was available, we implemented a method inspired by the ap-
proach of Güntürkün 41: given a time series we train 50 echo
state networks of internal dimension 30 and then obtain the
mean prediction error across the 50 networks. The mean pre-
diction error time series was smoothed to obtain a TVP es-
timate. Rather than using adaptive filtering, we performed
smoothing using a Gaussian convolutional filter to obtain a
TVP estimate.
We used σ = 166 for the Gaussian kernel
standard deviation hyperparameter, yielding a filter window
of approximately 1000 steps between ±3σ, to maintain con-
sistency with the window sizes used in the CD and statistical
time-series feature methods.
c.
Characteristic distance (CD)
This method is an example of a phase-space partitioning
approach to PINUP (see Sec. III E). Following the general ap-
proach of Nguyen and Chelidze,104 given a time series xt of
length 105, 50 random points were uniformly sampled from
a phase space hypercube bounding the points in xt but with
boundaries extended by ±10% in each dimension to allow
for sampling of points ‘outside’ the distribution A vector of
Euclidean distances from each of the 50 random points was
computed for each of the points in xt. A vector of 50 mean
distances was then computed for contiguous, non-overlapping

Parameter inference from a non-stationary unknown process
13
windows of length 103. Single-component SFA was then ap-
plied to obtain a TVP estimate.
d.
Statistical time-series features
This simple baseline method is an example of a statistical
time-series feature approach to PINUP (see Sec. III B), noting
that it is not yet standard practice within the PINUP literature
to use this method as a baseline comparison. For each vari-
able of each process, the time-series features from the catch22
library124 were computed over sliding windows. Specifically,
each time series variable of length 105 was partitioned into
contiguous, non-overlapping windows of length 103 and the
features were computed for each window yielding feature time
series of length 100.
1T. Schreiber and A. Schmitz, “Classification of Time Series Data with
Nonlinear Similarity Measures,” Physical Review Letters 79, 1475–1478
(1997).
2B. Buˇca, J. Tindall, and D. Jaksch, “Non-stationary coherent quantum
many-body dynamics through dissipation,” Nature Communications 10,
1730 (2019).
3D. Chelidze and M. Liu, “Dynamical systems approach to fatigue damage
identification,” Journal of Sound and Vibration 281, 887–904 (2005).
4D. Summers, J. G. Cranford, and B. P. Healey, “Chaos in periodically
forced discrete-time ecosystem models,” Chaos, Solitons & Fractals 11,
2331–2342 (2000).
5F. Zhang, P. Yang, K. Fraedrich, X. Zhou, G. Wang, and J. Li, “Recon-
struction of driving forces from nonstationary time series including sta-
tionary regions and application to climate change,” Physica A: Statistical
Mechanics and its Applications 473, 337–343 (2017).
6J. Galadí, S. Silva Pereira, Y. Sanz Perl, M. Kringelbach, I. Gayte,
H. Laufs, E. Tagliazucchi, J. Langa, and G. Deco, “Capturing the non-
stationarity of whole-brain dynamics underlying human brain states,” Neu-
roImage 244, 118551 (2021).
7T. A. Schmitt, D. Chetalova, R. Schäfer, and T. Guhr, “Non-stationarity
in financial time series: Generic features and tail behavior,” Europhysics
Letters 103, 58003 (2013).
8J. M. Shine, “Neuromodulatory control of complex adaptive dynamics in
the brain,” Interface Focus 13, 20220079 (2023).
9N. W. Bailey, B. D. Fulcher, B. Caldwell, A. T. Hill, B. Fitzgibbon, H. van
Dijk, and P. B. Fitzgerald, “Uncovering a stability signature of brain dy-
namics associated with meditation experience using massive time-series
feature extraction,” Neural Networks 171, 171–185 (2024).
10D. Ruelle, Chaotic Evolution and Strange Attractors, 1st ed. (Cambridge
University Press, Cambridge ; New York, 1989).
11M. B. Kennel, “Statistical test for dynamical nonstationarity in observed
time-series data,” Physical Review E 56, 316–321 (1997).
12A. Witt, J. Kurths, and A. Pikovsky, “Testing stationarity in time series,”
Physical Review E 58, 1800–1810 (1998).
13R. Hegger, H. Kantz, L. Matassini, and T. Schreiber, “Coping with Non-
stationarity by Overembedding,” Physical Review Letters 84, 4092–4095
(2000).
14R. Manuca and R. Savit, “Stationarity and nonstationarity in time series
analysis,” Physica D: Nonlinear Phenomena 99, 134–161 (1996).
15C. Rieke, K. Sternickel, R. G. Andrzejak, C. E. Elger, P. David,
and
K. Lehnertz, “Measuring Nonstationarity by Analyzing the Loss of Recur-
rence in Dynamical Systems,” Physical Review Letters 88, 244102 (2002).
16R. Serquina, Y.-C. Lai, and Q. Chen, “Characterization of nonstationary
chaotic systems,” Physical Review E 77, 026208 (2008).
17E. M. Bollt, J. D. Skufca, S. J. McGregor, E. M. Bollt, J. D. Skufca, and
S. J. McGregor, “Control entropy: A complexity measure for nonstation-
ary signals,” Mathematical Biosciences and Engineering 6, 1–25 (2009).
18A. Ray and A. Roy Chowdhury, “On the characterization of non-stationary
chaotic systems: Autonomous and non-autonomous cases,” Physica A:
Statistical Mechanics and its Applications 389, 5077–5083 (2010).
19T. Schreiber, “Detecting and analysing nonstationarity in a time series with
nonlinear cross-predictions,” Physical Review Letters 78, 843–846 (1997),
arxiv:chao-dyn/9909044.
20L. A. Aguirre and C. Letellier, “Nonstationarity signatures in the dynam-
ics of global nonlinear models,” Chaos: An Interdisciplinary Journal of
Nonlinear Science 22, 033136 (2012).
21A. G. Siapas and M. A. Wilson, “Coordinated interactions between hip-
pocampal ripples and cortical spindles during slow-wave sleep,” Neuron
21, 1123–1128 (1998).
22D. Moser, P. Anderer, G. Gruber, S. Parapatics, E. Loretz, M. Boeck,
G. Kloesch, E. Heller, A. Schmidt, H. Danker-Hopfe, B. Saletu, J. Zeitl-
hofer, and G. Dorffner, “Sleep Classification According to AASM and
Rechtschaffen & Kales: Effects on Sleep Scoring Parameters,” Sleep 32,
139–149 (2009).
23A. Tarantola, Inverse Problem Theory and Methods for Model Parameter
Estimation, Other Titles in Applied Mathematics (Society for Industrial
and Applied Mathematics, 2005).
24S. Särkkä and L. Svensson, Bayesian Filtering and Smoothing, 2nd ed.
(Cambridge University Press, 2023).
25U. Güntürkün, J. P. Reilly, T. Kirubarajan, and H. deBruin, “Recursive hid-
den input estimation in nonlinear dynamic systems with varying amounts
of a priori knowledge,” Signal Processing 99, 171–184 (2014).
26A. Arnold and A. L. Lloyd, “An approach to periodic, time-varying pa-
rameter estimation using nonlinear filtering,” Inverse Problems 34, 105005
(2018).
27N. Hauzenberger, F. Huber, G. Koop, and L. Onorante, “Fast and Flex-
ible Bayesian Inference in Time-varying Parameter Regression Models,”
Journal of Business & Economic Statistics 40, 1904–1918 (2022).
28A. Arnold, “When Artificial Parameter Evolution Gets Real: Particle Fil-
tering for Time-Varying Parameter Estimation in Deterministic Dynamical
Systems,” Inverse Problems 39, 014002 (2023), arxiv:2204.00074 [math,
stat].
29D. G. Luchinsky, V. N. Smelyanskiy, A. Duggento, and P. V. E. McClin-
tock, “Inferential framework for nonstationary dynamics. I. Theory,” Phys-
ical Review E 77, 061105 (2008).
30S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equa-
tions from data by sparse identification of nonlinear dynamical systems,”
Proceedings of the National Academy of Sciences 113, 3932–3937 (2016).
31J. S. North, C. K. Wikle, and E. M. Schliep, “A Review of Data-Driven
Discovery for Dynamic Systems,” International Statistical Review 91,
464–492 (2023).
32Z. G. Nicolaou, G. Huo, Y. Chen, S. L. Brunton, and J. N. Kutz, “Data-
driven discovery and extrapolation of parameterized pattern-forming dy-
namics,” Physical Review Research 5, L042017 (2023).
33A. Chatterjee, J. Cusumano, and D. Chelidze, “Optimal tracking of param-
eter drif in a chaotic system: Experiment and theory,” Journal of Sound and
Vibration 250, 877–901 (2002).
34P. F. Verdes, P. M. Granitto, H. D. Navone, and H. A. Ceccatto, “Non-
stationary Time-Series Analysis:
Accurate Reconstruction of Driving
Forces,” Physical Review Letters 87, 124101 (2001).
35W. Gilpin, “Recurrences reveal shared causal drivers of complex time se-
ries,” (2023), arxiv:2301.13516 [nlin].
36M. C. Casdagli, “Recurrence plots revisited,” Physica D: Nonlinear Phe-
nomena 108, 12–44 (1997).
37M. Szeliga, P. Verdes, P. Granitto, and H. Ceccatto, “Extracting driving
signals from non-stationary time series,” in VII Brazilian Symposium on
Neural Networks, 2002. SBRN 2002. Proceedings. (IEEE Comput. Soc,
Pernambuco, Brazil, 2002) pp. 104–108.
38M. Széliga, P. Verdes, P. Granitto, and H. Ceccatto, “Modelling nonsta-
tionary dynamics,” Physica A: Statistical Mechanics and its Applications
327, 190–194 (2003).
39L. Wiskott, “Estimating Driving Forces of Nonstationary Time Series with
Slow Feature Analysis,” (2003), arxiv:cond-mat/0312317.
40P. F. Verdes, P. M. Granitto,
and H. A. Ceccatto, “Overembedding
Method for Modeling Nonstationary Systems,” Physical Review Letters
96, 118701 (2006).
41U. Güntürkün, “Sequential reconstruction of driving-forces from nonlinear
nonstationary dynamics,” Physica D: Nonlinear Phenomena 239, 1095–
1107 (2010).

Parameter inference from a non-stationary unknown process
14
42A. R. Sloboda and B. I. Epureanu, “Sensitivity vector fields in time-delay
coordinate embeddings: Theory and experiment,” Physical Review E 87,
022903 (2013).
43G. L. Grinblat, L. C. Uzal, P. F. Verdes, and P. M. Granitto, “Nonsta-
tionary regression with support vector machines,” Neural Computing and
Applications 26, 641–649 (2015).
44H. Niknazar, A. M. Nasrabadi, and M. B. Shamsollahi, “Volumetric be-
havior quantification to characterize trajectory in phase space,” Chaos,
Solitons & Fractals 103, 294–306 (2017).
45S. Wang, Y. Chen, Y. Mei, and W. He, “The robustness of driving force
signals extracted by slow feature analysis,” Chaos, Solitons & Fractals 171,
113447 (2023).
46M. Tanio, Y. Hirata, and H. Suzuki, “Reconstruction of driving forces
through recurrence plots,” Physics Letters A 373, 2031–2040 (2009).
47D. Chelidze, J. P. Cusumano, and A. Chatterjee, “A Dynamical Systems
Approach to Damage Evolution Tracking, Part 1: Description and Exper-
imental Application,” Journal of Vibration and Acoustics 124, 250–257
(2002).
48O. Alao, P. Y. Lu, and M. Soljacic, “Discovering Dynamical Parameters
by Interpreting Echo State Networks,” in NeurIPS 2021 AI for Science
Workshop (2021).
49S. Güttler, H. Kantz, and E. Olbrich, “Reconstruction of the parameter
spaces of dynamical systems,” Physical Review. E, Statistical, Nonlinear,
and Soft Matter Physics 63, 056215 (2001).
50T. L. Carroll, “Attractor comparisons based on density,” Chaos: An Inter-
disciplinary Journal of Nonlinear Science 25, 013111 (2015).
51C. J. C. Burges, “Dimension Reduction: A Guided Tour,” Foundations and
Trends® in Machine Learning 2, 275–364 (2009).
52I. Jolliffe, Principal Component Analysis, Springer Series in Statistics
(Springer-Verlag, New York, 2002).
53M. Udell, C. Horn, R. Zadeh, and S. Boyd, “Generalized Low Rank Mod-
els,” Foundations and Trends® in Machine Learning 9, 1–118 (2016).
54L. Wiskott and T. J. Sejnowski, “Slow Feature Analysis: Unsupervised
Learning of Invariances,” Neural Computation 14, 715–770 (2002).
55H. Kantz and T. Schreiber, Nonlinear Time Series Analysis, 2nd ed. (Cam-
bridge University Press, Cambridge, 2003).
56S. Wang, Y. Chen, Y. Mei, and W. He, “The robustness of driving force
signals extracted by slow feature analysis,” Chaos, Solitons & Fractals 171,
113447 (2023).
57P. Song and C. Zhao, “Slow Down to Go Better: A Survey on Slow Feature
Analysis,” IEEE Transactions on Neural Networks and Learning Systems
, 1–21 (2022).
58L. Molgedey and H. G. Schuster, “Separation of a mixture of indepen-
dent signals using time delayed correlations,” Physical Review Letters 72,
3634–3637 (1994).
59S. Schultze and H. Grubmüller, “Time-Lagged Independent Component
Analysis of Random Walks and Protein Dynamics,” Journal of Chemical
Theory and Computation 17, 5766–5776 (2021).
60T. Blaschke, P. Berkes, and L. Wiskott, “What is the relation between slow
feature analysis and independent component analysis?” Neural Computa-
tion 18, 2495–2508 (2006).
61S. Klus, F. Nüske, P. Koltai, H. Wu, I. Kevrekidis, C. Schütte, and F. Noé,
“Data-Driven Model Reduction and Transfer Operator Approximation,”
Journal of Nonlinear Science 28, 985–1010 (2018).
62A. A. Khan, J. Kuehl,
and D. Chelidze, “Toward a unified interpreta-
tion of the “proper”/“smooth” orthogonal decompositions and “state vari-
able”/“dynamic mode” decompositions with application to fluid dynam-
ics,” AIP Advances 10, 035225 (2020).
63A. Mardt, L. Pasquali, H. Wu, and F. Noé, “VAMPnets for deep learning
of molecular kinetics,” Nature Communications 9, 5 (2018).
64C. Wehmeyer and F. Noé, “Time-lagged autoencoders: Deep learning of
slow collective variables for molecular kinetics,” The Journal of Chemical
Physics 148, 241703 (2018).
65L. Bonati, G. Piccini, and M. Parrinello, “Deep learning the slow modes
for rare events sampling,” Proceedings of the National Academy of Sci-
ences 118, e2113533118 (2021).
66T. Hastie, R. Tibshirani,
and J. Friedman, The Elements of Statistical
Learning: Data Mining, Inference, and Prediction, 2nd ed. (SPRINGER
MIHE, New York, NY, 2017).
67P. F. Verdes, “Global warming is driven by anthropogenic emissions:
A time series analysis approach,” Physical Review Letters 99, 048501
(2007).
68G. Wang, P. Yang, and X. Zhou, “Extracting the driving force from ozone
data using slow feature analysis,” Theoretical and Applied Climatology
124, 985–989 (2016).
69G. Wang, P. Yang, and X. Zhou, “Identification of the driving forces of cli-
mate change using the longest instrumental temperature record,” Scientific
Reports 7, 46091 (2017).
70P. Yang, G. Wang, F. Zhang, and X. Zhou, “Causality of global warming
seen from observations: A scale analysis of driving force of the surface air
temperature time series in the Northern Hemisphere,” Climate Dynamics
46, 3197–3204 (2016).
71F. Zhang, P. Yang, K. Fraedrich, X. Zhou, G. Wang, and J. Li, “Recon-
struction of driving forces from nonstationary time series including sta-
tionary regions and application to climate change,” Physica A: Statistical
Mechanics and its Applications 473, 337–343 (2017).
72W. Lu, M. Duan, and G. Wang, “Case studies on driving factor with differ-
ent scales: A modified Lorenz system and 500-hPa geopotential height,”
Theoretical and Applied Climatology 141, 455–463 (2020).
73Y. Naritomi and S. Fuchigami, “Slow dynamics in protein fluctuations re-
vealed by time-structure based independent component analysis: The case
of domain motions,” The Journal of Chemical Physics 134, 065101 (2011).
74G. Pérez-Hernández, F. Paul, T. Giorgino, G. De Fabritiis, and F. Noé,
“Identification of slow molecular order parameters for Markov model con-
struction,” The Journal of Chemical Physics 139, 015102 (2013).
75F. Paul, H. Wu, M. Vossel, B. L. De Groot, and F. Noé, “Identification
of kinetic order parameters for non-equilibrium dynamics,” The Journal of
Chemical Physics 150, 164120 (2019).
76B. D. Fulcher, “Feature-Based Time-Series Analysis,” in Feature Engi-
neering for Machine Learning and Data Analytics (CRC Press, 2018).
77B. D. Fulcher and N. S. Jones, “Hctsa: A Computational Framework for
Automated Time-Series Phenotyping Using Massive Feature Extraction,”
Cell Systems 5, 527–531.e3 (2017).
78N. B. Yanson, A. N. Pavlov, T. Kapitaniak,
and V. S. Anishchenko,
“Global reconstruction from nonstationary data,” Technical Physics Let-
ters 25, 412–414 (1999).
79B. D. Fulcher, C. H. Lubba, B. Harris, K. S. Owens, G. F. Gilestro, S. R.
Schultz, and N. S. Jones, “Inferring low-dimensional parametric variation
underlying time-series datasets using comprehensive time-series feature
extraction,” (Unpublished), unpublished work.
80B. Harris, Inferring Parametric Variation across Non-Stationary Time
Series, Bachelor’s thesis, School of Physics, The University of Sydney
(2021).
81J.-P. Eckmann, S. O. Kamphorst,
and D. Ruelle, “Recurrence Plots of
Dynamical Systems,” Europhysics Letters 4, 973 (1987).
82C. L. Webber and N. Marwan, eds., Recurrence Quantification Analysis:
Theory and Best Practices, Understanding Complex Systems (Springer In-
ternational Publishing, Cham, 2015).
83Y. Hirata, S. Horai, and K. Aihara, “Reproduction of distance matrices
and original time series from recurrence plots and their applications,” The
European Physical Journal Special Topics 164, 13–22 (2008).
84E. W. Dijkstra, “A note on two problems in connexion with graphs,” Nu-
merische Mathematik 1, 269–271 (1959).
85A. Mead, “Review of the Development of Multidimensional Scaling Meth-
ods,” Journal of the Royal Statistical Society Series D: The Statistician 41,
27–39 (1992).
86Y. Hirata, T. Takeuchi, S. Horai, H. Suzuki, and K. Aihara, “Parsimonious
description for predicting high-dimensional dynamics,” Scientific Reports
5, 15736 (2015).
87Y. Hirata and K. Aihara, “Dimensionless embedding for nonlinear time
series analysis,” Physical Review E 96, 032219 (2017).
88F. Takens, “Detecting strange attractors in turbulence,” in Dynamical Sys-
tems and Turbulence, Warwick 1980, Vol. 898, edited by D. Rand and L.-S.
Young (Springer Berlin Heidelberg, Berlin, Heidelberg, 1981) pp. 366–
381.
89T. Sauer, J. A. Yorke, and M. Casdagli, “Embedology,” Journal of Statis-
tical Physics 65, 579–616 (1991).
90G.-j. Jang and T.-W. Lee, “A Probabilistic Approach to Single Channel
Blind Signal Separation,” in Advances in Neural Information Processing

Parameter inference from a non-stationary unknown process
15
Systems, Vol. 15 (MIT Press, 2002).
91D. Chelidze, “Identifying Multidimensional Damage in a Hierarchical Dy-
namical System,” Nonlinear Dynamics 37, 307–322 (2004).
92D. Chelidze and M. Liu, “Dynamical systems approach to fatigue damage
identification,” Journal of Sound and Vibration 281, 887–904 (2005).
93D. Chelidze and J. Cusumano, “Phase space warping: Nonlinear time-
series analysis for slowly drifting systems,” Philosophical Transactions of
the Royal Society A: Mathematical, Physical and Engineering Sciences
364, 2495–2513 (2006).
94D. Chelidze and M. Liu, “Reconstructing slow-time dynamics from fast-
time measurements,” Philosophical Transactions of the Royal Society A:
Mathematical, Physical and Engineering Sciences 366, 729–745 (2008).
95M. Song, D. B. Segala, J. B. Dingwell,
and D. Chelidze, “Slow-Time
Changes in Human EMG Muscle Fatigue States Are Fully Represented
in Movement Kinematics,” Journal of Biomechanical Engineering 131,
021004 (2009).
96H.-W.-X. Li and D. Chelidze, “Geometry-informed phase space warping
for reliable fatigue damage monitoring,” Structural Health Monitoring ,
14759217231174894 (2023).
97P. Berkes and L. Wiskott, “Slow feature analysis yields a rich repertoire of
complex cell properties,” Journal of Vision 5, 9 (2005).
98W. Konen, “On the numeric stability of the SFA implementation sfa-tk,”
(2009), arxiv:0912.1064 [stat].
99B. I. Epureanu and A. Hashmi, “Parameter Reconstruction Based on Sen-
sitivity Vector Fields,” Journal of Vibration and Acoustics 128, 732–740
(2006).
100A. Hashmi and B. Epureanu, “Sensitivity Resonance and Attractor Morph-
ing Quantified by Sensitivity Vector Fields for Parameter Reconstruction,”
Nonlinear Dynamics 45, 319–335 (2006).
101S.-H. Yin and B. I. Epureanu, “Experimental Enhanced Nonlinear Dynam-
ics and Identification of Attractor Morphing Modes for Damage Detec-
tion,” Journal of Vibration and Acoustics 129, 763–770 (2007).
102S.-H. Yin and B. I. Epureanu, “Structural health monitoring based on sen-
sitivity vector fields and attractor morphing,” Philosophical Transactions
of the Royal Society A: Mathematical, Physical and Engineering Sciences
364, 2515–2538 (2006).
103A. R. Sloboda and B. I. Epureanu, “Maximizing Sensitivity Vector Fields:
A Parametric Study,” Journal of Computational and Nonlinear Dynamics
9, 021018 (2014).
104S. H. Nguyen and D. Chelidze, “New invariant measures to track slow pa-
rameter drifts in fast dynamical systems,” Nonlinear Dynamics 79, 1207–
1216 (2015).
105J. Hawkins, Ergodic Dynamics: From Basic Theory to Applications, Grad-
uate Texts in Mathematics, Vol. 289 (Springer International Publishing,
Cham, 2021).
106A. R. Sloboda, “Boundary transformation representation of attractor shape
deformation,” Chaos: An Interdisciplinary Journal of Nonlinear Science
31, 083133 (2021).
107A. R. Sloboda and C. T. Kong, “Boundary Transformation Vectors: A
Geometric Method of Quantifying Attractor Deformation for Structural
Health Monitoring,” Journal of Computational and Nonlinear Dynamics
17, 121004 (2022).
108A. R. Sloboda and C. T. Kong, “Damage Assessment From Attractor
Boundary Deformation,” in ASME 2022 Conference on Smart Materials,
Adaptive Structures and Intelligent Systems (American Society of Me-
chanical Engineers Digital Collection, 2022).
109A. R. Sloboda and R. S. Sloboda, “Refinements to the boundary trans-
formation vector representation of attractor shape deformation to enhance
system parameter identification,” Chaos: An Interdisciplinary Journal of
Nonlinear Science 32 (2022), 10.1063/5.0102072.
110N. Gorjian Jolfaei, R. Rameezdeen, N. Gorjian, B. Jin, and C. W. K. Chow,
“Prognostic modelling for industrial asset health management,” Safety and
Reliability 41, 45–97 (2022).
111J. P. Cusumano, D. Chelidze, and A. Chatterjee, “A Dynamical Systems
Approach to Damage Evolution Tracking, Part 2: Model-Based Validation
and Physical Interpretation,” Journal of Vibration and Acoustics 124, 258–
264 (2002).
112D. B. Segala, D. H. Gates, J. B. Dingwell, and D. Chelidze, “Nonlinear
Smooth Orthogonal Decomposition of Kinematic Features of Sawing Re-
constructs Muscle Fatigue Evolution as Indicated by Electromyography,”
Journal of Biomechanical Engineering 133, 031009 (2011).
113K. Bajelani, A. R. Arshi, and A. N. Akhavan, “Influence of compression
garments on fatigue behaviour during running based on nonlinear dynam-
ical analysis,” Sports Biomechanics 0, 1–14 (2022).
114A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B.
Rubin, Bayesian Data Analysis, 3rd ed. (Chapman and Hall/CRC, Boca
Raton, 2013).
115Z. Zhang, Q. Shen, and X. Wang, “Parameter identification framework
of nonlinear dynamical systems with Markovian switching,” Chaos: An
Interdisciplinary Journal of Nonlinear Science 33, 123117 (2023).
116S. M. Hirsh, D. A. Barajas-Solano, and J. N. Kutz, “Sparsifying priors
for Bayesian uncertainty quantification in model discovery,” Royal Society
Open Science 9, 211823 (2022).
117K. Course and P. B. Nair, “State estimation of a physical system with un-
known governing equations,” Nature 622, 261–267 (2023).
118Y. Yuan, X. Li, L. Li, F. J. Jiang, X. Tang, F. Zhang, J. Goncalves,
H. U. Voss, H. Ding, and J. Kurths, “Machine discovery of partial dif-
ferential equations from spatiotemporal data: A sparse Bayesian learning
framework,” Chaos: An Interdisciplinary Journal of Nonlinear Science 33,
113122 (2023).
119V. Smelyanskiy, D. Luchinsky, D. Timuçin, and A. Bandrivskyy, “Recon-
struction of stochastic nonlinear dynamical models from trajectory mea-
surements,” Physical Review E 72, 026202 (2005).
120A. Duggento, D. G. Luchinsky, V. N. Smelyanskiy, I. Khovanov,
and
P. V. E. McClintock, “Inferential framework for nonstationary dynamics.
II. Application to a model of physiological signaling,” Physical Review E
77, 061106 (2008).
121D. G. Luchinsky, V. N. Smelyanskiy, M. Millonas, and P. V. E. McClin-
tock, “Dynamical inference of hidden biological populations,” The Euro-
pean Physical Journal B 65, 369–377 (2008).
122R. D. Morris, V. N. Smelyanskiy, and M. Millonas, “Parameter and Struc-
ture Inference for Nonlinear Dynamical Systems,” in AIP Conference Pro-
ceedings, Vol. 803 (AIP, San Jose, California (USA), 2005) pp. 112–120.
123J. L. Rodgers and W. A. Nicewander, “Thirteen Ways to Look at the Cor-
relation Coefficient,” The American Statistician (1988).
124C. H. Lubba, S. S. Sethi, P. Knaute, S. R. Schultz, B. D. Fulcher, and N. S.
Jones, “Catch22: CAnonical Time-series CHaracteristics,” Data Mining
and Knowledge Discovery 33, 1821–1852 (2019).
125J. C. Sprott, Chaos and Time-Series Analysis, illustrated edition ed. (Ox-
ford University Press, Oxford ; New York, 2001).
126W. Gilpin, “Chaos as an interpretable benchmark for forecasting and
data-driven modelling,” pre-print
(2023), 10.48550/arXiv.2110.05266,
arxiv:2110.05266 [nlin].
127J. Deng, R. Socher, L. Fei-Fei, W. Dong, K. Li, and L.-J. Li, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE Conference on
Computer Vision and Pattern Recognition(CVPR), Vol. 00 (2009) pp. 248–
255.
128E. Keogh and S. Kasetty, “On the Need for Time Series Data Mining
Benchmarks: A Survey and Empirical Demonstration,” Data Mining and
Knowledge Discovery 7, 349–371 (2003).
129S. Makridakis, M. Hibon, and C. Moser, “Accuracy of Forecasting: An
Empirical Investigation,” Journal of the Royal Statistical Society. Series A
(General) 142, 97–145 (1979), 2345077.
130S. Makridakis, E. Spiliotis, and V. Assimakopoulos, “Statistical and Ma-
chine Learning forecasting methods: Concerns and ways forward,” PLoS
ONE 13, e0194889 (2018).
131E. E. Bron, S. Klein, A. Reinke, J. M. Papma, L. Maier-Hein, D. C.
Alexander, and N. P. Oxtoby, “Ten years of image analysis and machine
learning competitions in dementia,” NeuroImage 253, 119083 (2022).
132N. Traut, K. Heuer, G. Lemaître, A. Beggiato, D. Germanaud, M. El-
maleh, A. Bethegnies, L. Bonnasse-Gahot, W. Cai, S. Chambon, F. Cli-
quet, A. Ghriss, N. Guigui, A. de Pierrefeu, M. Wang, V. Zantedeschi,
A. Boucaud, J. van den Bossche, B. Kegl, R. Delorme, T. Bourgeron,
R. Toro, and G. Varoquaux, “Insights from an autism imaging biomarker
challenge: Promises and threats to biomarker discovery,” NeuroImage 255,
119171 (2022).
133M. Löning, A. Bagnall, S. Ganesh, V. Kazakov, J. Lines, and F. J. Király,
“Sktime: A Unified Interface for Machine Learning with Time Series,”
https://arxiv.org/abs/1909.07872v1 (2019).

Parameter inference from a non-stationary unknown process
16
134A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom,
P. Southam, and E. Keogh, “The UEA multivariate time series classifi-
cation archive, 2018,” (2018), arxiv:1811.00075 [cs, stat].
135A. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh, “The great
time series classification bake off: A review and experimental evaluation
of recent algorithmic advances,” Data Mining and Knowledge Discovery
31, 606–660 (2017).
136A. P. Ruiz, M. Flynn, J. Large, M. Middlehurst, and A. Bagnall, “The great
multivariate time series classification bake off: A review and experimental
evaluation of recent algorithmic advances,” Data Mining and Knowledge
Discovery 35, 401–449 (2021).
137T. Henderson, A. G. Bryant, and B. D. Fulcher, “Never a Dull Moment:
Distributional Properties as a Baseline for Time-Series Classification,”
(2023), arxiv:2303.17809 [cs, stat].
138G. Wang, P. Yang, and X. Zhou, “Nonstationary time series prediction
by incorporating external forces,” Advances in Atmospheric Sciences 30,
1601–1607 (2013).
139G. Wang and X. Chen, “Nonstationary time series prediction combined
with slow feature analysis,” Nonlinear Processes in Geophysics 22, 377–
382 (2015).
140E. Fehlberg, “Low-order classical Runge-Kutta formulas with stepsize
control and their application to some heat transfer problems,” Tech. Rep.
NASA-TR-R-315 (NASA, 1969).
141R. M. May, “Simple mathematical models with very complicated dynam-
ics,” Nature 261, 459–467 (1976).
142E. N. Lorenz, “Deterministic Nonperiodic Flow,” Journal of the Atmo-
spheric Sciences 20, 130–141 (1963).
143W. F. Langford, “Numerical Studies of Torus Bifurcations,” in Numerical
Methods for Bifurcation Problems: Proceedings of the Conference at the
University of Dortmund, August 22–26, 1983, International Series of Nu-
merical Mathematics, edited by T. Küpper, H. D. Mittelmann, and H. We-
ber (Birkhäuser, Basel, 1984) pp. 285–295.

