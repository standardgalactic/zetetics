arXiv:2303.17276v1  [cs.AI]  30 Mar 2023
Submitted to:
TARK 2023
© Philipp Koralus, Vincent Wang-Ma´scianica
This work is licensed under the
Creative Commons Attribution License.
Humans in Humans Out:
On GPT Converging Toward Common Sense in both Success
and Failure
Philipp Koralus
Faculty of Philosophy
University of Oxford
Institute for Ethics in AI
University of Oxford
St. Catherine’s College, Oxford
philipp.koralus@stcatz.ox.ac.uk
Vincent Wang-Ma´scianica
Department of Computer Science
University of Oxford
St. Catherine’s College, Oxford
vincent.wang@stcatz.ox.ac.uk
Increase in computational scale and ﬁne-tuning has seen a dramatic improvement in the quality of
outputs of large language models (LLMs) like GPT [19, 3]. Given that both GPT-3 and GPT-4 were
trained on large quantities of human-generated text, we might ask to what extent their outputs reﬂect
patterns of human thinking, both for correct and incorrect cases. The Erotetic Theory of Reason
(ETR) provides a symbolic generative model of both human success and failure in thinking, across
propositional [12], quantiﬁed [18], and probabilistic reasoning [17], as well as decision-making [16].
We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from
a recent book-length presentation of ETR [13], consisting of experimentally veriﬁed data-points on
human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as
well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wason’s
card task, illusory inferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3
showed evidence of ETR-predicted outputs for 59% of these examples, rising to 77% in GPT-3.5 and
75% in GPT-4. Remarkably, the production of human-like fallacious judgments increased from 18%
in GPT-3 to 33% in GPT-3.5 and 34% in GPT-4. This suggests that larger and more advanced LLMs
may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent
in human-produced training data. According to ETR, the same fundamental patterns are involved
both in successful and unsuccessful ordinary reasoning, so that the ”bad” cases could paradoxically
be learned from the ”good” cases. We further present preliminary evidence that ETR-inspired prompt
engineering could reduce instances of these mistakes.
1
Introduction
Large language models (LLMs) like GPT-4 have become increasingly important due to their wide-
ranging capabilities and are already having a transformative impact in many applications. LLMs are de-
signed to generate human-like text, as well as other types of outputs, as reasonable responses to prompts.
While exact ﬁgures for GPT-4 have not been released, GPT-3 was trained on hundreds of billions of
words from diverse sources, including websites, books, and articles. Besides very large data-sets of
human-generated text, GPT-3.5 and GPT-4 both also beneﬁted from a human-guided ﬁne-tuning training
regime [24]. Given that these systems have been trained on human input rather than formal knowledge
representations and their outputs are determined by extrapolation of patterns found in those inputs (es-
sentially a form of next-word prediction), rather than by formal deductions, we may ask whether these
systems approximate human outputs in failure, as well as in success. To give structure to this investiga-
tion, we use an empirically motivated model of human reason, the erotetic theory.

2
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure
1.1
Success and failure in human reason through the lens of the erotetic theory
Human reason has a two-faced nature. On the one hand, it has an unparalleled range of successes to
its credit, notably science and modern civilisation. On the other hand, humans suffer from a variety of
systematic failures of reason. Many such experimentally documented failures have been widely popu-
larised in the literature [8, 1]. According to the erotetic (or “question-based”) theory of reason (ETR),
the two-faced nature of human reason is the result of one set of basic principles. We treat incoming
information, as well as choice scenarios, as raising questions and we aim to resolve these questions as
directly as possible. In many situations, the dynamics of resolving questions as directly as possible yields
correct judgments, but it can also lead us astray. Luckily, the processes proposed by ETR are guaranteed
to produce correct judgments, by the relevant standards of classical validity, probabilistic coherence, and
rational choice theory, provided that the agent is in erotetic equilibrium, e.g. in a state from which the
judgment could be reached regardless of what further questions might be raised. Unfortunately, such
equilibrium is often too costly to achieve. A key tenet of the theory is that common-sense judgment is
following the same core principles, regardless of whether it is succeeding or leading us astray; the only
difference is whether we are robust with respect to further questions. We will illustrate the basic idea
with a simple example.
(1) You have a hand of several cards. There is at least an ace and a queen in the hand or at least a king
and a jack. There is an ace in the hand. What if anything follows?
A majority of participants in a variety of studies of this type of pattern freely conclude that it follows
that there is a queen in the hand [21, 15]. A moment’s reﬂection shows that this is not the case: the
premises are compatible with a situation in which there is an ace, no queen, but a king and a jack.
The erotetic theory diagnoses the human tendency toward this fallacy as follows: we take the premise
on board as if it was a question, ‘‘Am I in an ace and queen situation, or in a king and
jack situation?’’ We then treat the next premise as a maximally strong answer, ‘‘You’re in an
ace situation!’’ Your ace situation has a queen in it as well, so you conclude that there is a queen.
ETR holds that the same question/answer dynamics are behind ordinary judgment more broadly, in-
cluding cases in which we draw correct inferences. For example, consider modus ponens. From ‘‘If
there is an ace in the hand, then there is king in the hand’’ and ‘‘There is an ace
in the hand’’, virtually everybody readily infers ‘‘There is a king in the hand’’. ETR also
treats this as a case of the same question/answer dynamic we have already considered: we take the con-
ditional premise on board as a question, ‘‘Am I in an ace and king situation, or am I in a
non-ace situation?’’ We then take the second premise as, ‘‘You are in an ace situation!’’,
and, again, treat it as a maximally strong answer, yielding the conclusion that there is a king. In other
words, the same basic dynamics involved in correct reasoning are also involved in fallacious reasoning.
The key difference, according to ETR, is that fallacious judgments are not in equilibrium with respect
to further questions that might be asked. Returning to the ﬁrst question, suppose I have taken on board:
‘‘Am I in an ace and queen situation, or in a king and jack situation?’’ If I now
further ask myself, e.g. ‘‘What about further situations with respect to the question
of whether there is an ace?’’, I end up with, ‘‘Am I in an ace and a queen situation,
a king, jack, and ace situation, or a king, jack, and no ace situation?’’ If I now
take on board ‘‘You are in an ace situation!’’ as a maximally strong answer as before, I no
longer get the conclusion that there is a king. The fallacious inference was not an erotetic equilibrium
inference. However, if we gave the modus ponens inference the same treatment of asking further ques-
tions, we would still arrive at the conclusion that there is a king. This is because correct inferences are
erotetic equilibrium inferences; they are robust to further questions.

Philipp Koralus, Vincent Wang-Ma´scianica
3
The patterns of judgment errors that can be explained erotetically are not limited to logical inference.
Similar patterns can be identiﬁed in reasoning about probability. For example, it is well-known that
humans have a tendency in certain cases to rank the probability of a conjunction as higher than the
probability of individual conjuncts, violating the axioms of probability [20]. If we are told that someone
is a math genius as well as an athletic outdoorswoman, we might na¨ıvely judge it to be more likely
for this person to be a computer scientist and active in the climbing community, than for this person
to be active in the climbing community. The ETR diagnosis is that ‘‘You’re in a math genius
and athletic outdoorswoman situation!’’ more strongly answers in favour of the conjunction
(provided no further questions are asked).
To consider another example from the domain of decision-making, it is known that we suffer from ir-
rational opportunity-cost neglect [6]. If we are told that we have saved some money for fun in a hypothet-
ical scenario, there is a greater chance we will say ‘‘Buy!’’ to ‘‘Buy an entertaining video or
don’t buy an entertaining video?’’ than if we are asked ‘‘Buy an entertaining video
or save your money for other purchases?’’ The ETR diagnosis is that ‘‘Fun!’’ more strongly
answers in favour of buying the video than in favour of not buying the video – unless you remind yourself
that not buying the video is a situation that likely has other potential fun purchases down the line, and
make that part of what your decision-question considers.
The question/answer dynamic proposed by ETR is represented set-theoretically, treating questions
fundamentally as sets of alternative states, with the dynamics of inference and judgment treated as up-
date rules on sets. ETR to-date covers propositional reasoning [12, 14], ﬁrst-order quantiﬁed relational
reasoning [18], reasoning with uncertainty [17] and decision-making [16]. ETR is symbolic, in the sense
that its outputs are determined by sets of states and the atoms that occur within those states. ETR is
a generative theory, in the sense that it produces concrete inferences from a given sequence of inputs.
In other words, we can ask it what if anything follows from a sequence of premises, or what the deci-
sion is for a given decision-question and a given a set of priorities. The theory has been motivated by
a large number of data-points from the empirical literature on human reasoning and judgment, and has
also been corroborated by experiments on ETR-speciﬁc predictions. For example, answering a question
is an inherently ordered process, since you cannot meaningfully treat something as an answer without
having ﬁrst taken on board a question. ETR correctly predicts that the extent to which people fall for our
opening example (1) of a fallacious inference is lessened if the order of premises is reversed [15]. After
all, if we are ﬁrst given ‘‘You are in an ace situation!’’ and then ‘‘Am I in an ace and
queen situation, or in a king and jack situation?’’ it is harder to treat the former as an
answer to the latter.
1.2
Predicting failure patterns
According to ETR, the same dynamics of reason underly common sense thinking in both success and
failure. This would lead us to expect that if we were to train a system to approximate the dynamics of
common sense thinking based on human-produced text, the familiar fallacious judgment patterns should
emerge as well. On the ETR view, this would be the case even if the text used as a training basis
vastly over-represents cases of correct common-sense judgment. This is relevant since we expect that
GPT would have been trained somewhat judiciously to approximate high-quality output and to solve
problems with the hope of objective correctness. Yet, on the ETR view, it would not be a a surprise
if a better approximation of good common-sense reasoning also yields a better approximation of bad
common-sense reasoning. The ETR view of how the successes and failures of reason are linked contrasts
with views that might take mistakes to be the result of special purpose heuristics. In addition, insofar as

4
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure
judgment errors are made as a result of approximating patterns of common-sense judgment as envisaged
by ETR, we would further expect that prompts to take on board broader questions should reduce the
occurrence of fallacious judgments.
To investigate these hypotheses, we studied the outputs of GPT with key examples from a recent
book-length treatment of ETR as inputs. We then examined whether ETR-inspired prompt engineering
can reduce the incidence of certain fallacious judgments that the GPT systems produced.
2
Study 1: Reasoning patterns in GPT with the ETR61 benchmark
We logged the outputs of GPT-3, GPT-3.5, and GPT-4 for a set of 61 inference and decision-making
problems for which ETR makes predictions about common-sense responses. We will refer to this set as
the ETR61 benchmark. ETR61 was created by compiling the main examples throughout a book-length
presentation of ETR [13], including some novel content variations. The examples covered a variety of
types of problems, across logical and probabilistic inference, as well as decision-making. Many examples
were variants of cases that have been shown experimentally to elicit incorrect or incoherent responses in
humans. Some additional examples are reproduced below.
(2) Some blue cards are textured. All square cards are blue. What if anything follows?
Human participants tend to fallaciously conclude that some square cards are textured [9].
(3) There are several cards on the table, which have a letter on one side and a number on the other
side. One card shows and E, one card shows a C, one card shows a 4, and one card shows a 5.
Which cards do you have to turn over to determine if the following statement is true? If a card has
an E on one side then it has a four on the other side.
In this type of problem the majority of human participants omit to turn over the ”5” card, even though
it is necessary to do so to determine whether the conditional is falsiﬁed [22].
(4) [Abbreviated] Linda is thirty-one years old. She majored in philosophy. As a student, she was
deeply concerned with issues of discrimination and social justice. Please rank order by probability
(highest to lowest) the following: Linda is a bank teller. Linda is a bank teller and is active in the
feminist moment.
Human participants have a tendency to rank the probability of the conjunction higher than the prob-
ability of the conjuncts [20], violating the axioms of probability theory.
(5) Which of the following subscription would you be most likely to purchase. 1. Economist.com
subscription - US $59.00. One-year subscription to Economist.com. Includes online access to
all articles from The Economist since 1997. [2. Print subscription - US $ 125.00. One-year
subscription to the print edition of The Economist.] 3. Print & web subscription - US $125.00.
One-year subscription to the print edition of The Economist and online access to all articles from
The Economist since 1997.
Human participants have been reported as being more likely to pick option 3 if the dominated option
2 (which nobody chose) is included in the menu [1], violating the axioms of rational choice theory.
In constructing the ETR61 benchmark, no systematic commitment was made to replicate the vi-
gnettes from human experiments exactly (though most of them were exact replications). We changed the
wording and parameters of various problems to overcome the resistance of GPT to make recommenda-
tions, and to deal with the possibility of some problems having been strongly present in the training data.
We only varied examples in ways that left the ETR predictions on those problems unchanged.

Philipp Koralus, Vincent Wang-Ma´scianica
5
2.1
Methodology
The GPT-3 model used was accessed at https://platform.openai.com/playground, with param-
eters text-davinci-003 at temperature 0.7, max-output 256 tokens, top P = 1, 0 frequency and presence
penalities, best-of-1. We refer to ChatGPT as GPT-3.5 and ChatGPT with GPT-4 backend as just GPT-4.
Both GPT-3.5 and GPT-4 models used were accessed at https://chat.openai.com/chat in March
2023, using the default 3.5 and 4 models.
For each version of GPT, we used ETR61 as prompts under two conditions: production and query.
Each question in each condition was entered into a refreshed context-free instance of the model. In the
production condition, questions were asked in the form of premises ended by ‘‘What, if anything,
follows?". Answers that that did not mention the main ETR-predicted conclusion but included state-
ments that imply it were further prompted with a topical query. For GPT-3, which had a relatively limited
max-output length, submission was repeated whenever answers were cut midway.
In the query condition, questions were asked in the form of premises ended by ‘‘Does it follow
that X?" in place of ‘‘What, if anything follows?" where X was the ETR-predicted response.
No follow-up questions were applied in the query condition.
For both cases, appropriate semantic
variants of ‘‘What, if anything follows?" or ‘‘Does it follow that X?" were assigned for
questions involving probability rankings and decision-making.
For every question in both conditions, correctness was recorded and the transcript logged. Correct-
ness was judged by the following criteria. A correct declaration of the answer anywhere was marked
correct regardless of the ensuing justiﬁcation. This gave GPT-3 an inﬂated correctness rate, as it often
produced nonsense justiﬁcations. Mindful of poor native numerical ability in language models [7] (and
workarounds [23]), for questions concerning probability, correctness was awarded for either correct an-
swers or correct procedure e.g. correct application of Bayes’ rule. For rankings of subjective probability,
correctness was awarded for probabilistic consistency e.g. P(A) ≥P(A&B) anywhere is inconsistent. For
decision questions, correctness was awarded for consistent decision-making across materially equivalent
framings of the choice scenarios.
2.2
Results
We adopt the convention of statistical signiﬁcance at p ≤0.05. All signiﬁcance tests are Wilcoxon
signed-rank as implemented in Mathematica version 13.0.1.0. by the SignedRankTest method. Since
it is known [2] that different computer packages for rank-sum statistics may yield different judgements
of signiﬁcance at the p = 0.05 level, we report fair signiﬁcance (0.01 ≤p ≤0.05) in orange and strong
signiﬁcance (p ≤0.01) in red. We introduce each test condition with a gloss, we report the percentage
rate for each test across the GPTs, and we provide the statistical signiﬁcance of the change in rates; we
tabulate the change in rate from GPT-3 to GPT-3.5 as “v3-v3.5”, GPT-3.5 to GPT-4 as “v3.5-v4”, and
GPT-3 to GPT-4 as “v3-v4”.
We found that GPT-3.5 produced fewer correct answers than either GPT-3 or GPT-4. In Table 1 we
observe a dip-then-rise in the rate of production of correct answers across generations, and a steady-
then-rise in the endorsement of correct answers. We observe a steady-then-sharp-rise in correctness and
consistency, taken to be both the production and endorsement of correct answers.
More advanced models exhibit more common sense. In Table 2 we observe upward trends in rates of
answers produced and answers endorsed that are predicted by ETR as common-sense judgments. There
is a rise-then-steady pattern when we consider any instance of common-sense reasoning in either produc-
tion or endorsement. While these trends are not statistically signiﬁcant stepwise between generations,

6
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure
How correct are GPTs as productive reasoners in ETR61?
Correct answer produced
Percentage
GPT-3
GPT-3.5
GPT-4
56%
48%
64%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.3248
0.0346
0.3752
How often do GPTs endorse correct conclusions in ETR61?
Correct answer endorsed
Percentage
GPT-3
GPT-3.5
GPT-4
48%
52%
72%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.6701
0.0150
0.0045
How often is GPT consistently correct in ETR61?
Correct production and endorsement
Percentage
GPT-3
GPT-3.5
GPT-4
30%
30%
59%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
1
0.0004
0.0017
Table 1: Cross-generational tests for correctness in reasoning across GPTs.
there is strong evidence that GPT-4 is overall more common-sense congruent than GPT-3.
How often do GPTs produce ETR-predicted common-sense answers?
Production predicted by ETR
Percentage
GPT-3
GPT-3.5
GPT-4
49%
61%
72%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.1134
0.1316
0.0030
How often do GPTs endorse ETR-predicted common-sense answers?
Endorsement predicted by ETR
Percentage
GPT-3
GPT-3.5
GPT-4
36%
49%
54%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.1060
0.5255
0.0354
How often do GPTs exhibit ETR-predicted common-sense responses?
Either above predicted by ETR
Percentage
GPT-3
GPT-3.5
GPT-4
59%
77%
75%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.0083
0.8213
0.0268
Table 2: Cross-generational tests for common-sense reasoning across GPTs.
More advanced models are more prone to ETR-predicted fallacies. We consider fallacies to be ”mis-
takes of common sense” that correspond to incorrect answers that are predicted by the default reasoning
procedure in ETR. We note in Table 3 that production and endorsement of fallacies rises from GPT-3 to
GPT-3.5, and either holds steady or drops slightly from GPT-3.5 to GPT-4. Remarkably, in every case,
the more advanced GPT-4 is more fallacy-prone than GPT-3.
GPT-4 produces signiﬁcantly more fallacies than it endorses. We performed several intra-generational
comparisons, reported in Table 4. First, we know on reasoning problems, human participants often pro-
duce different results depending on whether they are asked to produce a conclusion or whether they are

Philipp Koralus, Vincent Wang-Ma´scianica
7
How often do GPTs produce ETR-predicted human-like errors?
Production fallacious
Percentage
GPT-3
GPT-3.5
GPT-4
18%
33%
34%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.0140
0.8121
0.0135
How often do GPTs endorse ETR-predicted human-like errors?
Fallacy endorsed
Percentage
GPT-3
GPT-3.5
GPT-4
18%
23%
20%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.4281
0.5941
0.8213
How often do GPTs fall prey to ETR-predicted human-like errors?
Fallacy produced or endorsed
Percentage
GPT-3
GPT-3.5
GPT-4
26%
43%
36%
Cross-generational
statistical signiﬁcance
v3-v3.5
v3.5-v4
v3-v4
0.0135
0.2669
0.1647
Table 3: Cross-generational tests for fallacious reasoning across GPTs.
asked to whether a given conclusion follows [9]. We observe that later generations appeared to produce
more fallacies than they endorsed, and we report that GPT-4 produces signiﬁcantly more fallacies than it
endorses.
Do GPTs produce answers other than what they endorse?
ETR predicts production vs. ETR predicts endorsement
GPT-3
GPT-3.5
GPT-4
0.6552
0.3248
0.1169
Do GPTs differ in production vs. veriﬁcation performance?
Production correct vs. endorsement correct
GPT-3
GPT-3.5
GPT-4
0.3429
0.5588
0.1450
Is there a difference in rate of production vs. endorsement of fallacies?
Fallacy produced vs. fallacy endorsed
GPT-3
GPT-3.5
GPT-4
1
0.2610
0.0134
Table 4: Intra-generational tests across GPTs.
2.3
Discussion
We observe that while we see improvements in correct reasoning with later generations of GPT, we
also see an increase in the susceptibility to systematic fallacies. Remarkably, GPT-4 did not produce
fewer ETR-predicted fallacious conclusions than GPT-3.5; while GPT-3.5 in fact produced more of these
fallacious conclusions than GPT-3. This is would be expected if a mixture of more training (including
reinforcement learning from human feedback) and increased model-size increase the ﬁdelity with which
human common-sense reasoning patterns (as conceived of by ETR) are approximated. Unfortunately,
exact details of the model differences have not been made public, making it difﬁcult to theorise more
speciﬁcally about the mechanisms responsible for the observed differences in model behaviour.
Some of the relevant outputs produced to GPT 4 were in fact reminiscent of the dynamics of reason-

8
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure
ing described by ETR. Though these observations have to be interpreted with caution, they are potentially
suggestive.
Prompt: Either Jane is kneeling by the ﬁre and she is looking at the TV or else Mark is standing
at the window and he is peering into the garden. Jane is kneeling by the ﬁre. What if anything
follows?
GPT4: Since Jane is kneeling by the ﬁre, it follows that the ﬁrst part of the statement is true. [...]
So, what follows is that Jane is kneeling by the ﬁre and looking at the TV.
This is reminiscent of the pattern described in section 1.1, according to which entire disjuncts are
treated as alternatives in a question, and the subsequent premise is used as a maximally strong answer,
effectively selecting the alternative with the greatest overlap, instead of engaging with the logic of dis-
junction. We also found some evidence of the ETR-predicted order effect discussed in section 1.1:
GPT4 produced the relevant fallacious conclusion for "There is an ace and a queen, or else
a king and a ten.
There is a king." but not for "There is a king.
There is an ace
and a queen, or else a king and a ten."
3
Study 2: Prompt engineering to mitigate fallacies
As in the case of human judgment, the presence of systematic mistakes raises the question of whether
we can mitigate these mistakes by following a better reasoning strategy. Adding global instructions like
“reason step-by-step” can improve the accuracy of LLM outputs [10]. According to ETR, mistakes are
due to not having taken on board sufﬁciently broad questions before treating further information as an
answer. We examined whether a simplistic version of this idea could improve GPT performance.
3.1
Method
For each of GPT-3, 3.5, and 4, we ran augmented prompts based on a subset of ETR61, following
a similar procedure that described previously. For each version of GPT, we only used those original
prompts that had produced incorrect judgments that were predicted by the basic step in the ETR default
reasoning procedure. In other words, we only examined those cases that yielded mistakes and where
those mistakes were most directly generated by the question/answer dynamic at the core of ETR.
To construct the augmented prompts we used the following templates:
(Control) Reason step-by-step for the following problem. [Original prompt inserted here]
(ETR) Answer the following question according to this procedure: First, list the premises. Second,
turn each premise into a question to make a new list of questions; treat questions as possible
alternatives. Third, reason step-by-step using both lists, keeping track of alternatives. [Original
prompt inserted here]
3.2
Results
We found that both the control prompt template and the ETR prompt template reduced the incidence of
incorrect judgment outputs. In the case of GPT3.5, the ETR prompt was better than the control prompt
at a level that was statistically signiﬁcant.

Philipp Koralus, Vincent Wang-Ma´scianica
9
Are fallacies blocked by erotetic prompting?
Model
GPT-3
GPT-3.5
GPT-4
Fallacies blocked by control prompt
4/8
4/17
8/19
Fallacies blocked by ETR prompt
4/8
14/17
7/19
Stat. sig.
1
0.0045
0.7656
Table 5: Prompting GPT-3.5 to reason erotetically was signiﬁcantly better at blocking fallacies compared
to step-by-step prompting. On other models both prompts had comparable effect on blocking fallacies.
4
Conclusion and future work
The results of Study 1 suggest that GPT-3.5 and GPT-4 outputs are signiﬁcantly more similar to common-
sense judgment than GPT-3 outputs. Remarkably, this also applies to patterns of fallacious judgments.
In fact, the production of fallacious judgments from ETR61, including deductive reasoning, judgments
about probability, and decision-making, increases from GPT-3 to GPT-4. On the view of human rea-
soning proposed by ETR, this is not surprising. ETR holds that both successful and unsuccessful uses
of common sense are based on the same principles, so that a better approximation of the dynamics of
common-sense, as provided by more training and larger models, would also generate more common-
sense fallacies, even if the LLMs are primarily learning from success cases. Study 2 suggests that some
of the insights from ETR could inform strategies for prompt engineering to lessen the incidence of
common-sense fallacies in LLMs like GPT.
Study 1 bears on the question of how future improvements of LLMs relate to data scale. Given
that the later generation LLMs examined were in fact more prone to human-like fallacies, even though
they involved more processing on larger data sets, we speculate that the relevant problems are not easily
remedied by more generic training on human-generated text, which will have the relevant thought pat-
terns ingrained in it. However, ETR suggests a possible solution. With an implementation of ETR, it
would be possible to generate synthetic datasets of arbitrary size and complexity of fallacy-prone prob-
lems and their correct solutions that could be used to train against them. Similarly, an implementation of
ETR, could be used to construct a dynamic generative benchmark to test LLM performance. This may
help further pave the way toward LLMs as cleaned-up common-sense approximators.
References
[1] D. Ariely (2010): Predictably irrational : the hidden forces that shape our decisions. New York : Harper
Perennial.
[2] R. Bergmann et al. (2000): Different Outcomes of the Wilcoxon—Mann—Whitney Test from Different Statis-
tics Packages. The American Statistician 54(1), doi:10.1080/00031305.2000.10474513.
[3] T.B. Brown et al. (2020): Language Models are Few-Shot Learners, doi:10.48550/arXiv.2005.14165.
[4] S. Bubeck et al. (2023): Sparks of Artiﬁcial General Intelligence: Early experiments with GPT-4, doi:10.
48550/arXiv.2303.12712.
[5] A. Chowdhery et al. (2022): PaLM: Scaling Language Modeling with Pathways, doi:10.48550/arXiv.
2204.02311.
[6] S. Frederick, N. Novemsky, J. Wang, R. Dhar & S. Nowlis (2009): Opportunity Cost Neglect. Journal of
Consumer Research 36, pp. 553–561, doi:10.1086/599764.

10
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure
[7] S. Frieder, L. Pinchetti, R.-R. Grifﬁths, T. Salvatori, T. Lukasiewicz, P.C. Petersen, A. Chevalier & J. Berner
(2023): Mathematical Capabilities of ChatGPT, doi:10.48550/arXiv.2301.13867.
[8] D. Kahneman (2011): Thinking, Fast and Slow. Farrar, Straus and Giroux.
[9] S. Khemlani & P.N. Johnson-Laird (2012): Theories of the syllogism: a meta-analysis. Psychological Bul-
letin 138, pp. 427–457, doi:10.1037/a0026841.
[10] T. Kojima, S.S. Gu, M. Reid, Y. Matsuo & Y. Iwasawa (2023): Large Language Models are Zero-Shot
Reasoners, doi:10.48550/arXiv.2205.11916.
[11] P. Koralus (2023): Conditionals and Information Source Selection. In: Reason and Inquiry: The Erotetic
Theory, Oxford University Press, doi:10.1093/oso/9780198823766.003.0003.
[12] P. Koralus (2023): Core Reasoning and Erotetic Equilibrium. In: Reason and Inquiry: The Erotetic Theory,
Oxford University Press, doi:10.1093/oso/9780198823766.003.0002.
[13] P. Koralus (2023): Reason and Inquiry: The Erotetic Theory. Oxford University Press, doi:10.1093/oso/
9780198823766.001.0001.
[14] P. Koralus & S. Mascarenhas (2013): The Erotetic Theory of Reasoning: Bridges between Formal Semantics
and the Psychology of Deductive Inference. Philosophical Perspectives 27(1), pp. 312–365, doi:https://
doi.org/10.1111/phpe.12029.
[15] P. Koralus & S. Mascarenhas (2018): Illusory Inferences in a Question-Based Theory of Reasoning. Brill,
doi:10.1163/9789004365445-011. Pp. 300-322.
[16] P. Koralus & S. Moss (2023): Decision and Practical Reasoning. In: Reason and Inquiry: The Erotetic
Theory, Oxford University Press, doi:10.1093/oso/9780198823766.003.0006.
[17] P. Koralus & S. Moss (2023): Reasoning with Uncertainty. In: Reason and Inquiry: The Erotetic Theory,
Oxford University Press, doi:10.1093/oso/9780198823766.003.0005.
[18] P. Koralus, V. Wang, S. Moss & B. Mount (2023): Predicate Reasoning. In: Reason and Inquiry: The Erotetic
Theory, Oxford University Press, doi:10.1093/oso/9780198823766.003.0004.
[19] OpenAI (2023): GPT-4 Technical Report, doi:10.48550/arXiv.2303.08774.
[20] A. Tversky & D. Kahneman (1983): Extensional versus intuitive reasoning: the conjunction fallacy in prob-
ability judgment. Psychological Review 90, pp. 293–315, doi:10.1037/0033-295X.90.4.293.
[21] C. Walsh & P.N. Johnson-Laird (2004): Coreference and reasoning. Memory and Cognition 32, pp. 96–106,
doi:10.3758/BF03195823.
[22] P.C. Wason (1966): Reasoning. In B. Foss, editor: New Horizons in Psychology, Harmondsworth: Penguin
Books, pp. 135–151.
[23] S. Wolfram (2023): ChatGPT Gets Its “Wolfram Superpowers”!
Available at https://writings.
stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/. Accessed March 2023.
[24] D.M. Ziegler, N. Stiennon, J. Wu, T.B. Brown, A. Radford, D. Amodei, P. Christiano & G. Irving (2020):
Fine-Tuning Language Models from Human Preferences, doi:10.48550/arXiv:1909.08593v2.

